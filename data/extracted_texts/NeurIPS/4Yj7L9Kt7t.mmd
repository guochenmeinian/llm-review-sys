# Taming Heavy-Tailed Losses in Adversarial Bandits

and the Best-of-Both-Worlds Setting

 Duo Cheng

Virginia Tech

duocheng@vt.edu &Xingyu Zhou

Wayne State University

xingyu.zhou@wayne.edu &Bo Ji

Virginia Tech

boji@vt.edu

###### Abstract

In this paper, we study the multi-armed bandits problem in the best-of-both-worlds (BOBW) setting with heavy-tailed losses, where the losses can be negative and unbounded but have \((1+v)\)-th raw moments bounded by \(u^{1+v}\) for some known \(u>0\) and \(v(0,1]\). Specifically, we consider the BOBW setting where the underlying environment can be either (oblivious) adversarial (i.e., the loss distribution can change arbitrarily over time) or stochastic (i.e., the loss distribution is fixed over time), which is unknown to the decision-maker a prior. We propose an algorithm and prove that it achieves a \(T^{}\)-type worst-case (pseudo-)regret in the adversarial regime and a \( T\)-type gap-dependent regret in the stochastic regime, where \(T\) is the time horizon. Compared to the state-of-the-art results, our algorithm offers stronger _high-probability_ regret guarantees (vs. expected regret guarantees), and more importantly, relaxes a strong technical assumption on the loss distribution, which is generally hard to verify in practice. As a byproduct, relaxing this assumption leads to the first near-optimal regret result for heavy-tailed bandits with Huber contamination in the adversarial regime (vs. the easier stochastic regime studied in all previous works). Our result also implies a high-probability BOBW regret guarantee when the bounded true losses are protected with pure Local Differential Privacy (LDP), while the existing work ensures the (weaker) _approximate_ LDP with the regret bounds in expectation only.

## 1 Introduction

Consider the multi-armed bandits (MAB) problem (Auer et al., 2002a,b), which is a useful framework for sequential decision-making under uncertainty and can be formulated as repeated interactions between the environment and a (learning) algorithm. In each of the total \(T\) rounds indexed by \(t\), the algorithm plays an action \(a_{t}\) from a fixed set of \(K\) actions (assuming \(K T\)). Simultaneously, the environment determines the losses of all actions \(_{t}^{K}\). The algorithm observes and suffers the loss associated with \(a_{t}\) (denoted by \(_{t,a_{t}}\)). The goal of the algorithm is to minimize the cumulative loss over \(T\) rounds, or equivalently, to minimize the _regret_, defined as the difference between its cumulative loss and that incurred by playing the best-fixed action (in hindsight) all the time. Without observing the losses of the other actions, the algorithm must "infer" the optimal action through interactions on the fly, facing the well-known trade-off between exploration and exploitation.

Depending on how the losses are determined, MAB problem is typically studied in two regimes: 1) the stochastic regime, where the loss of each action is drawn from a fixed (but unknown) distribution over time; 2) the adversarial regime, where the losses can be arbitrary (within some known class). Typically, the losses are assumed to have a support on a bounded interval (e.g., \(\)), and the fundamental limits have been well understood: 1) in the stochastic regime, a number of optimismbased algorithms achieve the \(()\)1 worst-case (pseudo-)regret and \((_{i:_{i}>0}(1/_{i}) T)\) gap-dependent regret, where \(_{i}\) (formally defined in Section 2) is the sub-optimality gap between action \(i\) and the optimal action (Lai and Robbins, 1985; Auer et al., 2002a; Agrawal and Goyal, 2017); 2) in the adversarial regime, the \(()\) worst-case regret can be achieved by classic Online Learning algorithms, such as Follow-the-Regularized-Leader (FTRL), Online-Mirror-Descent (OMD), and Follow-the-Perturbed-Leader (FTPL) (Audibert and Bubeck, 2009; Lee et al., 2024).

Despite these progresses, the optimal algorithms in the two regimes fall into different frameworks (i.e., optimism-based algorithms for the stochastic regime vs. Online Learning algorithms for the adversarial regime). Moreover, while the former ones enjoy a logarithmic regret (i.e., \(O( T)\)) in the stochastic regime, even their \(()\) worst-case guarantees no longer hold when the environment deviates from the stochastic regime with empirical evidence provided in Zimmert and Seldin (2021). On the other hand, while Online Learning algorithms always preserve worst-case optimality, they could be too "conservative" to enjoy logarithmic regrets in stochastic environments.

These performance discrepancies motivated the study of the _Best-of-Both-Worlds (BOBW)_ setting. That is, one single algorithm preserves the optimal worst-case regret in the adversarial regime and adapts to the stochastic regime with a logarithmic regret, _without knowing the type of the regime_ in advance. Bubeck and Slivkins (2012) initiated the study by proposing a _detect-switch_ framework, which preserves the optimal \(()\) regret in the adversarial regime and enjoys \(O(( T)^{2}K/)\) regret in the stochastic regime, where \(:=_{i:_{i}>0}_{i}\) is the smallest sub-optimality gap. Under this framework, Auer and Chiang (2016) improved the gap-dependent term from \(K/\) to \(_{i:_{i}>0}(1/_{i})\).

Another line of work showed that without explicit detect-switch, OMD, originally designed for the adversarial regime, can automatically adapt to stochastic environments with a logarithmic regret (Wei and Luo, 2018; Zimmert and Seldin, 2021). In particular, Zimmert and Seldin (2021) achieved optimal regrets in the BOBW setting. Following these works, the power of Online Learning algorithms towards BOBW has been extended to various setups (Ito, 2021; Ito et al., 2022; Kong et al., 2023; Ito and Takemura, 2023; Dann et al., 2023; Jin et al., 2024; Lee et al., 2024; Tsuchiya et al., 2024).

While the aforementioned works require bounded losses, real-world data from application domains such as finance (Cont, 2001) and imaging (Hamza and Krim, 2001) often exhibits a heavy-tailed distribution. Intuitively, heavy-tailed losses make learning problems harder (compared to bounded losses) as they are "noisier" and "less informative" (Zhang et al., 2020). When losses are unbounded but have \((1+v)\)-th raw moment bounded by \(u^{1+v}\) for some known \(u>0\) and \(v(0,1]\), Bubeck et al. (2013) showed \((uK^{}T^{})\) worst-case regret and \((_{i:_{i}>0}(u^{1+1/v}/_{i})^{1/v} T)\) gap-dependent regret2 in the stochastic regime by integrating robust mean estimators (e.g., trimmed mean and median-of-means) into optimism-based algorithms.

To address heavy-tailed losses in the adversarial regime and BOBW setting, Huang et al. (2022) showed that with calibrated adaptive loss trimming thresholds, FTRL with Tsallis entropy regularizer (Audibert and Bubeck, 2009) enjoys the optimal BOBW expected regrets under the "truncated non-negative losses" assumption (see Assumption 2). Without this strong assumption, it is unclear whether the near-optimal worst-case regret can still be achieved in the adversarial regime, let alone the BOBW setting. The key technical challenge here is that heavy-tailed losses can be both negative and unbounded, which is known to break the regret guarantees of the Online Learning algorithms. Extensive discussions and insights for our solution are provided in Section 3.

Given this challenge, the gap raises an interesting question: _In heavy-tailed MAB, are there any fundamental barriers to the worst-case optimality in the adversarial regime and the BOBW guarantees?_

We offer a positive answer to the above question, which implies that there is no such barrier. Our main contributions are as follows:

* In the adversarial regime, we propose an OMD-based algorithm achieving the (near-)optimal \((uK^{}T^{})\) pseudo-regret _with high probability_. Our approach relaxes the undesired "truncated non-negative losses" assumption, which is needed in the state-of-the-art resultseven for the weaker expected regret guarantee (Huang et al., 2022). Relaxing it also allows us to obtain the near-optimal worst-case guarantee against the Huber contamination, which, to our best knowledge, was only studied for the stochastic regime in the literature. This suggests broader implications of our approach.
* On top of the above advance in the adversarial regime, by leveraging the detect-switch framework, we further extend the (near-)optimal regret guarantees to the BOBW setting. Specifically, our algorithm preserves the optimal \((uK^{}T^{})\) regret in the adversarial regime and enjoys \(O(K(u^{1+1/v}/)^{1/v}(K)( T)^{4})\) gap-dependent logarithmic regret in the stochastic regime, both _with high probability_, which implies that _there is no fundamental barrier to achieving the BOBW guarantees when the loss distributions are heavy-tailed_. This result also immediately imply the first high-probability BOBW regret guarantees with _pure_ Local Differential Privacy (LDP) protection on the true losses, while the existing result ensures the weaker _approximate_ LDP protection with expected regret guarantees only.
* Technique-wise, we leverage the inherent stronger stability of log-barrier to relax a strong technical assumption made in previous works and utilize the _increasing-learning-rates_ trick (Lee et al., 2020) to obtain the stronger high-probability guarantee in the adversarial regime. Moreover, we adapt the detect-switch framework by Bubeck and Slivkins (2012), originally designed for BOBW in the bounded-loss case, to the heavy-tailed setup. The adaptation introduces non-trivial challenges in the analysis due to the history-dependent trimmed estimator. In particular, to obtain the desired concentration rate in the adversarial regime, the proof does not follow its existing counterpart in the stochastic regime. Beyond addressing these challenges, we identify a novel use (i.e., handling history-dependent trimming in martingale concentrations) of an adaptive variant of Freedman's inequality (originally proposed by Lee et al. (2020) and improved by Zimmert and Lattimore (2022) for high-probability regret in adversarial bandits), which may be of independent interest.

We refer the readers to Table 1 for a summary of the most relevant results, in which we also include adaptive results on the case when \(u,v\) are unknown. We also present a comprehensive discussion about related work, which is deferred to Appendix A due to the page limit.

 

Problem Setup

In this section, we formally introduce the problem setup and define needed notations. To formulate heavy-tailed losses, we let \(_{t,i}\) denote the loss of action \(i\) in round \(t\), which is drawn from distribution \(P_{t,i}\) satisfying the following assumption:

**Assumption 1**.: The \((1+v)\)-th (raw) moments of losses (which have potentially unbounded support in \(\)) are bounded by \(u^{1+v}\) for some constants \(u>0\) and \(v(0,1]\), i.e., \(_{_{t,i} P_{t,i}}[|_{t,i}|^{1+v}]  u^{1+v}, t[T],i[K]\), where \([n]\) denotes set \(\{1,,n\}\) for any integer \(n 1\).

In the heavy-tailed MAB problem, the (learning) algorithm and environment perform the following interactions repeatedly in round \(t=1,,T\):

1. The algorithm samples action \(a_{t}\) from \([K]\) via \(a_{t} w_{t}:=(w_{t,1},,w_{t,K})\) in the probability simplex \(:=\{x^{K}\ \ _{i=1}^{K}x(i)=1\}\), i.e., action \(i[K]\) is sampled with probability \(w_{t,i}\). The environment draws loss \(_{t,i} P_{t,i}\) for every action \(i[K]\).
2. The algorithm observes \(_{t,a_{t}}\) only; the losses of all other actions are unrevealed.
3. The algorithm determines \(w_{t+1}\) based on the history \((w_{1},a_{1},_{1,a_{1}},,w_{t},a_{t},_{t,a_{t}})\).

_Remark 1_.: All of our algorithms and their regret bounds allow moment order \((1+v)(1,2]\) only. That is, one may not obtain any regret guarantee by running our algorithms with \(v>1\). If the losses have higher-order moments (\(v>1\)), one can simply run our algorithms with \(v=1\) and obtaining the corresponding regret bounds (since bounded higher-order moments imply lower-order ones). Note that Bubeck et al. (2013) showed that for all \(v 1\), the lower bounds in terms of both worst-case regret and gap-dependent regret are the same as the case of \(v=1\).

We assume that heavy tail parameters \(u\) and \(v\) and time horizon \(T\) are known to the algorithm a priori.3 The objective of the algorithm is to minimize the _pseudo-regret_\(R_{T}\), defined as

\[R_{T}:=_{t=1}^{T}(_{t,a_{t}}-_{t,i^{*}}),\] (1)

where \(_{t,i}:=_{_{t,i} P_{t,i}}[_{t,i}]\) denotes the mean loss of action \(i[K]\) in round \(t[T]\), and \(i^{*}*{argmin}_{i[K]}_{t=1}^{T}_{t,i}\) denotes any best-fixed action in hindsight.

Depending on how loss distributions are determined, we further define the following two regimes:

* **Stochastic regime:** For every action \(i[K]\), the loss distributions are identical in all rounds. That is, we have \(P_{1,i}==P_{T,i}=P(i),\) implying \(_{1,i}==_{T,i}=(i)\) and \(i^{*}*{argmin}_{i[K]}(i)\). We also define \(_{i}:=(i)-(i^{*})\) and \(:=_{i:_{i}>0}_{i}\).
* **(Oblivious) Adversarial regime:** All loss distributions are chosen arbitrarily (by some adversary, with the full knowledge of the algorithm) before the interaction begins. Our regret definition and adversarial model are also considered in Huang et al. (2022).

_Remark 2_.: It is not hard to see that the stochastic regime is a special (and "easy") case of the adversarial regime. There are two differences between our heavy-tailed setup and the bounded-loss setup in the adversarial bandits literature: 1) Typically in the (bounded-loss) adversarial regime, the losses are considered to be deterministic rather than randomized (and this difference also leads to some new challenges in the analysis when we adopt the detect-switch framework from the bounded case (Bubeck & Slivkins, 2012) as we will show later in Section 4.2), and 2) a stronger notion of regret, defined as \(_{T}:=_{t=1}^{T}_{t,a_{t}}-_{i[K]}_{t=1}^{T} _{t,i}\), is considered; this is stronger than pseudo-regret since \([_{T}][R_{T}]\). However, when adapted to the heavy-tailed case, it is natural to still consider randomized losses and pseudo-regret as in the stochastic regime: The "easiness" of the regime boils down to how heavy-tailed distributions are chosen. Moreover, a low _stronger regret_ depends not only on playing good actions (with low loss means), but also on the realization of potentially unbounded losses, which loses the standard meaning in evaluating the algorithm. Therefore, when switching from stochastic to adversarial regime in heavy-tailed bandits, we keep pseudo-regret as the metric.4Our goal is to design one single algorithm that can achieve the near-optimal \((uK^{}T^{})\) worst-case regret in the adversarial regime and enjoys \( T\)-type regret when the regime is stochastic, without being informed of the regime type in advance.

A closely related work by Huang et al. (2022) studied the same BOBW setup (i.e., achieving BOBW guarantee when \(u,v\) are known). They proposed an FTRL-based algorithm with Tsallis entropy regularizer and carefully-chosen history-dependent trimming threshold for loss magnitude control and showed \(O(_{i:_{j} 0}(u^{1+1/v}/_{i})^{1/v} T)\) regret in the stochastic regime and \(O(uK^{}T^{})\) regret in the adversarial regime, both of which are in _expectation_ and optimal. However, their regret guarantees rely heavily on a strong technical assumption:

**Assumption 2** (Truncated non-negative losses (Huang et al., 2022)).: Given any fixed \(M>0\), the loss distributions of the optimal action \(i^{*}\) satisfy \(_{_{t,i^{*}} P_{t,i^{*}}}[_{t,i^{*}}\{| _{t,i^{*}}|>M\}] 0, t[T]\).

In the following sections, we first show that by resorting to the log-barrier regularizer, we obtain the near-optimal regret bound in the adversarial regime without Assumption 2 and naturally extend it to the stronger high-probability guarantee (Section 3). On top of that, we further adapt the detect-switch framework in Bubeck and Slivkins (2012) and obtain high-probability bounds in BOBW (Section 4).

**Additional Notations.** For any round \(t\) and action \(i\), we let \(I_{t,i}:=\{a_{t}=i\}\) denote whether action \(i\) is pulled in round \(t\) and \(N_{t,i}:=_{s=1}^{t}I_{s,i}\) denote the number of times when action \(i\) is pulled before the end of round \(t\). We use \(_{t,i}:=_{t,i}I_{t,i}\{|_{t,i}| M_{t,i}\}/w_{t,i}\) to denote the (trimmed) IW estimate with respect to some threshold \(M_{t,i}\) and \(_{t,i}:=_{s=1}^{t}_{s,i}I_{s,i}\{|_{s,i}|  B_{s,i}\}/N_{t,i}\) to denote the (trimmed) empirical average with respect to some threshold \(B_{t,i}\).5 We also use \(^{}_{t,i}:=_{_{t,i} P_{t,i}}[_{t,i} \{|_{t,i}| M_{t,i}\}]\) to denote the mean of the trimmed loss and \(_{t,i}:=_{s=1}^{t}_{s,i}\) to denote the cumulative IW estimate.

## 3 High-probability Near-optimal Regret in the Adversarial Regime

This section is dedicated to high-probability regret in the adversarial regime. We first present detailed discussions on why Assumption 2 is needed in Huang et al. (2022) and how we get rid of it (Section 3.1), followed by the description of our algorithm design and regret guarantee (Section 3.2).

### Technical Challenges and Insights

The main technical challenge we encounter comes from the potentially-unbounded negative losses (even for regret bounds _in expectation_ only). We illustrate this challenge using the previous work of Huang et al. (2022) as an example. Their algorithm is running standard FTRL with \((1+v)^{-1}\)-Tsallis entropy regularizer over the trimmed loss estimate sequence \(_{1},,_{T}\) with respect to some trimming threshold \((M_{t,i})_{t[T],i[K]}\) to be introduced later in this subsection.

To bound the expected regret, Huang et al. (2022) first rewrite it as

\[[R_{T}] =[_{t=1}^{T} w_{t}-y^{i^{*}},_{t}- ^{}_{t}]+[_{t=1}^{T} w_{t}-y ^{i^{*}},^{}_{t}]\] \[=[_{t=1}^{T}_{i=1}^{K}w_{t,i} (_{t,i}-^{}_{t,i})]}_{}+[_{t=1}^{T} (^{}_{t,i^{*}}-_{t,i^{*}})]}_{}+[_{t=1}^{T} w_{t}-y^{i^{*}},_{t} ]}_{},\] (2)

where \(y^{i}\) is the \(K\)-dim vector such that the \(i\)-th entry is one and all the others are zero.

The analyses begin with Part III. The desired upper bound on Part III holds only under the well-known "stability condition" associated with Tsallis entropy (Jin et al., 2024, Lemma C.5.3):

\[_{t,i}(w_{t,i})^{1-}_{t,i}=_{t,i}(w_{t,i})^ {1-}(w_{t,i})^{-1}_{t,i}I_{t,i}\{|_{t,i}|  M_{t,i}\}-C(u,v),\] (3)

where constant \(C(u,v)>0\) depends on \(u\) and \(v\) only and the learning rate \(_{t,i}\) is chosen as \(u^{-1}t^{}\). While this condition is trivially satisfied when losses are non-negative, due to potentially-unboundednegative losses, threshold \(M_{t,i}\) here is chosen to be \(((t w_{t,i})^{})\) (in particular, to fully "cancel" the \((w_{t,i})^{-1}\) from \(_{t,i}\)). Otherwise, negative losses break this condition whenever \(w_{t,a_{t}}\) is very small.

While such a threshold suffices to bound Part I and Part III with worst-case optimality (and even in the BOBW setting), applying the analysis for Part I to Part II leads to an upper bound of form \(_{t=1}^{T}(w_{t,i^{*}})^{}\) on Part II, which is potentially unbounded since \(w_{t,i^{*}}\) can be very close to zero. However, with the help of Assumption 2, Part II itself is non-positive and hence can be ignored.

**Summary.** The key issue above is that, due to unbounded and negative losses, Eq. (3) results in a threshold \(M_{t,i}\) which scales with \((w_{t,i})^{}\) (in particular, for \(i=i^{*}\)), rendering Part II hard to bound.

_Remark 3_.: This issue may not be fixed by simply shifting all loss estimates to become positive and satisfy Eq. (3). Roughly speaking, the reason is that obtaining desired regret bounds relies heavily on the well-bounded \((1+v)\)-th moment of the losses (before shifting). However, to ensure positive losses, the needed shift is too "significant" and breaks the "nice" moment conditions.

To handle this issue and relax Assumption 2, we resort to the log-barrier regularizer, which offers the standard regret guarantee with the following stability condition (Agarwal et al., 2017):

\[_{t,i}w_{t,i}_{t,i}-0.5.\] (4)

Importantly, this condition itself already provides a \(w_{t,i}\) (rather than the previous \((w_{t,i})^{1-}\) in Eq. (3)) to "cancel" the \((w_{t,i})^{-1}\) from \(_{t,i}\), meaning that we do not need any additional \(w_{t,i}\) contributed from threshold \(M_{t,i}\). As a result, we can choose a different \(M_{t,i}\) that scales with \(K^{}\) rather than the previous \((w_{t,i})^{}\) such that all three parts are bounded by \((uK^{}T^{})\)_without Assumption 2_.

We further adopt the increasing-learning-rates trick (Lee et al., 2020) together with an adaptive variant of Freedman's inequality (stated in Lemma 12) to obtain the stronger _high-probability_ regret.

### Algorithm Overview and Regret Guarantee

We now discuss the algorithm design and present the full pseudo-code in Algorithm 1. At a high level, our algorithm is running standard OMD over loss sequence \(_{1},,_{T}\) with respect to some fixed threshold \(M_{t,i}=u(T/K)^{}\). The key ingredients (to obtain high-probability regret) are the special learning rate schedule and probability simplex truncation (in OMD update) (Wei and Luo, 2018; Lee et al., 2020), which we briefly introduce below.

**Increasing Learning Rates.** We use vectors \(\{_{t}\}_{t[T]}\) to keep track of smallest sampling probabilities throughout the interaction. To be more specific, for every action \(i\), if \(w_{t+1,i}\) is so small that \(1/w_{t+1,i}>_{t,i}\) holds, we increase the learning rate by a factor of \(>1\) and set \(_{t+1,i}=2/w_{t+1,i}\) (Line 9). Otherwise, we keep the learning rate unchanged and set \(_{t+1,i}=_{t,i}\) (Line 10). In the analysis, such a schedule introduces some negative term in the upper bound, which cancels out a positive term that could potentially be very large due to the variance of loss estimates (Lee et al., 2020).

**Probability Simplex Truncation.** We perform the OMD update over the _truncated_ probability simplex \(^{}:=\{x:x(i)/K, i[K]\}\) (where \(\) controls the degree of truncation), of which the purpose is to ensure that \(w_{t,i}\) is always at least \(/K\) and hence to control the variance of the loss estimates. The value of \(\) here is \(T^{}K^{}\) adapted to the heavy-tailed case and differs from the original value \(1/T\) for the bounded-loss case in Lee et al. (2020).

The regret guarantee of Algorithm 1 is formally stated below with full proofs presented in Appendix B.

**Theorem 1**.: _In the adversarial regime, for any failure probability \(\), by choosing initial learning rate \(=(40M(T)(8KT/))^{-1}\) and trimming threshold \(M_{t,i}=u(T/K)^{}\), Algorithm 1 ensures that with probability at least \(1-\), \(R_{T}=O(uK^{}T^{}( T)^{2}(T/))\). By further choosing \(=1/T\), Algorithm 1 ensures that \([R_{T}]=O(uK^{}T^{}( T)^{3})\)._

_Remark 4_.: Removing Assumption 2 is crucial to obtain _the first and near-optimal_ worst-case regret in heavy-tailed MAB when the feedback could be contaminated by the Huber model _in the adversarial regime_, in contrast to all previous works that study the (easier) stochastic regime (Guan et al., 2020; Agrawal et al., 2024; Wu et al., 2024). We provide all details in Appendix D.

## 4 High-probability Regrets in the Best-of-Both-Worlds Setting

With Algorithm 1 achieving high-probability optimal regret in the adversarial regime, we further leverage the detect-switch framework proposed by Bubeck and Slivkins (2012) named SAO to achieve high-probability bounds in the BOBW setting. We first present the BOBW guarantee, followed by an algorithm overview and analysis sketch. The complete proofs are provided in Appendix C.

**Theorem 2**.: _In the adversarial regime, for any failure probability \(\), by choosing constant \(c_{1}=6\), Algorithm 2 ensures that with probability at least \(1-\),_

\[R_{T}=O(uK^{}T^{}(K)( T)^{2}(( /))^{2}),\]

_which (by further choosing \(=1/T\)) implies that \([R_{T}]=O(uK^{}T^{}( K)( T)^{4}).\) In the stochastic regime, Algorithm 2 ensures that with probability at least \(1-\),_

\[R_{T}=O(K(K)(u^{1+1/v}/)^{1/v}(T)((T/)) ^{3}),\]

_which implies that \([R_{T}]=O(K(K)(u^{1+1/v}/)^{1/v}( T)^ {4}).\)_

_Remark 5_.: High-probability bounds also are powerful tools to handle adaptive adversaries (who determine the current distribution \(P_{t,i}\) based on past actions \(a_{1},,a_{t-1}\)). Following the literature (Audibert and Bubeck, 2010; Lee et al., 2020; Zimmer and Lattimore, 2022), based on our high-probability bounds against oblivious adversaries, one may further derive both high-probability and expected regret bounds against adaptive adversaries, which we leave as future investigation.

_Remark 6_.: This theorem immediately implies a (high-probability) BOBW regret guarantee when the losses are bounded (in \(\)) and protected with \(\)-Local Differential Privacy (LDP) via showing that the privatized losses (with Laplacian noise) have second moments bounded by \(O(^{-1})\)(Agarwal and Singh, 2017; Tossou and Dimitrakakis, 2017; Zheng et al., 2020; Ren et al., 2020). To the best of our knowledge, this is the first result showing (high probability) BOBW regret guarantee with _pure_ LDP protection, while the state-of-the-art result (Zheng et al., 2020) ensures _(the weaker) approximate_ LDP protection, and only expected regret bounds are provided. Full details are given in Appendix E.

### Algorithm Overview

Our algorithm design follows from the detect-switch framework by Bubeck and Slivkins (2012). The high-level idea is to keep performing statistical tests (to "identify" the environment) and carefully maintain the sampling distribution over all arms. Once some certain test fails (which implies that the environment is unlikely stochastic), we switch to Algorithm 1 and run it over the remaining rounds.

In each round after playing action \(a_{t}\) sampled from distribution \(w_{t}\), if any active action \(i A_{t}\) satisfies the test in Eq. (5), it is deactivated, and we use \(_{i}\) and \(q_{i}\) to store the round and the sampling probability when it is deactivated, respectively (Line 8). After that, the algorithm performs tests in Eqs. (6)-(8) for environment identification. If any of them is satisfied, the procedure is terminated, and we instead run Algorithm 1 over the remaining rounds. We use \(t_{}\) to denote the round when the algorithm switch happens. Otherwise, we update the distribution \(w_{t+1}\) for the next round as in Line 14. To make notations and analyses well-defined, we deactivate all remaining arms in \(A_{t_{}}\) when the algorithm switch happens.

```
1:Input: failure probability \(\); constant \(c_{1} 6\)
2:Define:\(=12T^{2}K(T)\); \(M_{t,i}=u}}{(((T/))^{}}\); \(B_{t,i}=u(}{(2T/)})^{}\); \((t)=12uK^{}t^{}((/) )^{}\)
3:Initialization: Play each action once; for every \(i[K]\), let \(w_{K+1,i}=,_{i}=T\); set \(A_{K}=[K]\).
4:for\(t=K+1,,T\)do
5: Sample and play action \(a_{t} w_{t}\); observe \(_{t,a_{t}}\)
6:for\(i A_{t-1}\)do
7:if \[_{t,i}-_{j A_{t-1}}_{t,j}>c_{1}(t)\] (5)
8:then\(A_{t}=A_{t-1}\{i\}\), \(_{i}=t\), and \(q_{i}=w_{t,i}\)
9:endif
10:endfor
11:if any of the three conditions holds then run Algorithm 1 for the remaining rounds, let \(t_{}=t\), and let \(q_{i}=w_{t,i}\), \(_{i}=t\) for every \(i A_{t}\):
12: \[ i[K]|_{t,i}/t-_{t,i}| >9u(})^{}+ \{i A_{t}\}(t)}{t}\] \[+\{i A_{t}\}(t)}{_{i}},\] (6) \[ i A_{t}(_{t,i}-_{j A_{t}} _{t,j})/t >(c_{1}+4)(t)/(_{i}-1),\] (7) \[ i A_{t}(_{t,i}-_{j A_{t}} _{t,j})/t (c_{1}-4)(t)/_{i}\] (8)
13:endif
14:\(w_{t+1,i}=\{i A_{t}\}q_{i}_{i}/(t+1)+\{i A_{t }\}(1-_{j A_{t}}q_{i}_{i}/(t+1))/|A_{t}|,  i[K]\)
15:endfor
16:\(q_{i}=w_{T,i}\), \(_{i}=T, i A_{T}\) ```

**Algorithm 2** SAO for heavy-tailed MABs (SAO-HT)

### Analysis Sketch

In this subsection, we present the key steps of the regret analysis in the proof of Theorem 2.

Before diving into the specific regime, we first derive a set of concentration results (good events). Informally, all of the following hold simultaneously with high probability in all rounds \(t[T]\):

\[|_{t,i}-_{s=1}^{t}_{s,i}|/t \{i A_{t}\}(t)}{t}+ \{i A_{t}\}(t)}{_{i}},\] (9) \[|_{t,i}-^{t}_{s,i}I_{s,i}}{N _{t,i}}| =()^{}}),\] (10) \[N_{t,i} =(q_{i}_{i}(1+ t)),\] (11)where \((t):=12uK^{t}((/))^{ }\). All analyses below are conditioned on these good events, and we omit "with high probability" in the arguments.

_Remark 7_.: Our main non-trivial adaptation deviating from the case with bounded losses in Bubeck and Slivkins (2012) is the good event in Eq. (10) due to _jointly randomized and heavy-tailed_ losses. In particular, we need the concentration result of trimmed mean \(_{t,i}\) specified in Eq. (10) _in both regimes_. In the stochastic regime, it been shown in Bubeck et al. (2013). We need Eq. (10) in adversarial regime (while Bubeck and Slivkins (2012) does not) as losses are _deterministic_ therein. However, with heavy tails, the proof of Eq. (10) in the adversarial regime does not follow straightforwardly from the stochastic regime. More technical details are presented later in Remark 8. Nonetheless, this matter is specific to the nature of the trimmed estimator we use, which essentially could be replaced with any other estimator, as long as the concentration rate is preserved. However, it is unclear whether other estimators can handle non-identical distributions.

#### 4.2.1 Analysis Overview in the Stochastic Regime

**Step 1: Showing that tests in Eqs. (6)-(8) are never satisfied.** We show by good events that tests in Eqs. (6)-(8) are never satisfied, implying that we never switch to Algorithm 1.

**Step 2: Building the connection between \(_{i}\) and \(_{i}\).** From tests in Eqs. (7) and (8), we can show that for every suboptimal action \(i\) with \(_{i}>0\), its sub-optimality gap \(_{i}=((K/_{i})^{})\). Intuitively, an action with a smaller sub-optimality gap stays active for a longer time.

**Step 3: Bounding the total regret.** By the definition of pseudo-regret, we now have

\[R_{T}=_{i:_{i}>0}_{i}N_{T,i}=(_{i:_{i} >0}_{i}q_{i}_{i})=(_{i:_{i}>0}_{i}q_{i }K(_{i})^{-1-}).\] (12)

We complete the proof by showing \(_{i:_{i}>0}q_{i}_{i=1}^{K}(1/i)=O( K)\).

#### 4.2.2 Analysis Overview in the Adversarial Regime

In the adversarial regime, whenever we switch to Algorithm 1, it provides \((u(T-t_{})^{}K^{})\) (high-probability) regret guarantee for the remaining \((T-t_{})\) rounds. Therefore, it suffices to show that the cumulative regret before the switch is \((u(t_{})^{}K^{})\).

In our analysis, we trivially bound the regret in the single round \(t_{}\) by \(2u\), and it remains to show that the regret in the first \((t_{}-1)\) rounds is \((u(t_{}-1)^{}K^{})\), which is explained in the following.

**Step 1: Regret decomposition.** We first get an regret upper bound in terms of \(i_{t}^{*}:=*{argmin}_{i[K]}_{s=1}^{t}_{s,i}\) for \(t=t_{}-1\):

\[_{s=1}^{t_{}-1}_{s,a_{t}}-_{s=1}^{t_{}-1}_{s,i^{*}}_{i=1}^{K}N_{t_{}-1,i}(^{t_{}-1}_{s,i}I_{s,i}}_{}}-_{t_{}-1,i}}{t_{}-1}}_{}}- _{t_{}-1,i}}{t_{}-1}+_{t_{}-1,i}-_{t_{}-1,i_{} ^{*}-1}}{t_{}-1}}_{}})\] \[+_{t_{}-1,i_{}^{*}-1}- _{s=1}^{t_{}-1}_{s,i_{}^{*}-1}}_{}},\] (13)

and then we bound Parts A, B, and C, separately.

**Step 2: Bounding Part A.** We rewrite Part A as \(=(^{t_{}-1}_{s,i}I_{s,i}}{N_{t_{ }-1,i}}-_{t_{}-1,i})+( _{t_{}-1,i}-_{t_{}-1,i}}{t_{}-1} ),\) where the first term on the right-hand side is \(O(u(}-1,i}})^{})\) due to Eq. (10) and the second term is \(O(u(}-1,i}})^{}+ (t_{}-1)}{_{i}-1})\) due to test in Eq. (6).

_Remark 8_.: Due to heavy tails, the losses are trimmed by some _history-dependent_ threshold \(B_{t,i}\) (which depends on the number of pulls \(N_{t,i}\)) for a rate-optimal concentration in good event (10). Toshow this in the stochastic regime, one can treat the observed losses from one action as _i.i.d._ samples via the "reward tape/table" argument (Slivkins, 2019) and apply Bernstein's inequality for every fixed \(N_{t,i}\) (and set the uniform upper bound as \(B_{t,i}\) associated with it) as in Bubeck et al. (2013). However, one cannot simply follow the same path in the adversarial regime, since the distributions are _no longer identical_, and has to follow the martingale-based analysis (Agarwal et al., 2021, Lemma 6.2).

Now one may readily see the issue: The desired uniform upper bound \(B_{t,i}\) is determined _on-the-fly_, while the standard Freedman's inequality for martingales (e.g., Lemma 11) requires a _fixed_ uniform upper bound. To close this gap, we again exploit an adaptive variant of Freedman's inequality by Zimmert & Lattimore (2022) (Lemma 12; which was originally proposed for a totally different use, namely, obtaining high-probability bounds in adversarial bandits), in which we can replace the fixed uniform upper bound with the largest _realization_, satisfying our need perfectly.

**Step 3: Bounding Part B.** By considering two disjoint cases (i.e., action \(i A_{t_{}-1}\) or not), we can show Part \(=O((t_{}-1)}{_{i}-1})\) using the tests in Eqs. (5) and (7).

**Step 4: Bounding Part C.** The good event in Eq. (9) simply implies Part \(=O((t_{}-1))\).

**Step 5: Putting all pieces together.** Combing Steps 1-4 and the good event in Eq. (11) yields

\[_{s=1}^{t_{}-1}_{s,a_{t}}-_{s=1}^{t_{}-1}_{s,i^{*}} =O(_{i=1}^{K}N_{t_{}-1,i}(u(N_{t_{ }-1,i})^{}+(t_{}-1)}{ _{i}-1})+(t_{}-1))\] \[=(u_{i=1}^{K}(N_{t_{}-1,i} )^{}+u_{i=1}^{K}q_{i}K^{}(t_{}-1)^ {})\] \[+(u_{i=1}^{K}}(t _{}-1)^{}}{_{i}-1}+(t_{}-1))\] \[=(uK^{}(t_{}-1)^{ }).\] (14)

## 5 Conclusion

In this paper, we show that there is indeed no fundamental barrier to achieving the BOBW guarantee in heavy-tailed MAB by relaxing a strong, hard-to-verify technical assumption on the loss distributions of the optimal action needed for the state-of-the-art results. We further leverage the increasing-learning-rates trick and the detect-switch framework to achieve the stronger high-probability guarantees. Our results also imply the first and near-optimal regret in the adversarial regime where the feedback could be contaminated in the Huber model, and the high-probability BOBW regret guarantee when losses are bounded and protected with pure LDP, while the state-of-the-art result only ensures the weaker approximate LDP protection with regret guarantees in expectation.

One follow-up question is whether the gap-dependent term \(K()^{-1/v}\) can be improved to the refined \(_{i:_{i}>0}(_{i})^{-1/v}\), which is achieved only in the stochastic regime (Bubeck et al., 2013). Under the detect-switch framework, we tend to believe this is possible by adapting more sophisticated tests designed in Auer & Chiang (2016) for the bounded-loss case, where the gap dependency is improved from \(K/\) to \(_{i:_{i}>0}(1/)\), although a higher computational complexity is expected.

It will also be interesting to understand whether canonical Online Learning algorithms (i.e., without explicit detection and switch) provably enjoy BOBW guarantees in heavy-tailed MAB (and if so, whether a refined gap dependency can be achieved). In other words, do heavy tails break the implicit adaption of Online Learning algorithms to the stochastic regime (in the worst case)? One promising direction is to still utilize log-barrier regularizer together with potentially more advance learning rate and/or trimming threshold design. Notably, one unique advantage of it over the detect-switch framework is that it typically directly extends the BOBW regret guarantee to the corrupted regime, which is an intermediate regime that smoothly extrapolates between the purely adversarial and stochastic regime (Zimmert & Seldin, 2021).