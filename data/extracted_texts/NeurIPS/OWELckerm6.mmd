# Laughing Hyena Distillery

Extracting Compact Recurrences From Convolutions

 Stefano Massaroli\({}^{,1}\), Michael Poli\({}^{,2}\), Daniel Y. Fu\({}^{,2}\),

**Hermann Kumbong\({}^{2}\)**, **Rom N. Parnichkun\({}^{3}\)**, **Aman Timalsina\({}^{4}\)**,

**David W. Romero\({}^{5}\)**, **Quinn McIntyre\({}^{2}\)**, **Beidi Chen\({}^{6}\)**, **Atri Rudra\({}^{7}\)**, **Ce Zhang\({}^{8}\)**,

**Christopher Re\({}^{2,}\), **Stefano Ermon\({}^{2,}\)**, **Yoshua Bengio\({}^{1,}\)**

\({}^{}\)Equal contribution. \(\) Equal senior authorship. \({}^{1}\)Mila and Universite de Montreal. \({}^{2}\)Stanford University.

\({}^{3}\)The University of Tokyo. \({}^{4}\)Purdue University. \({}^{5}\)Virje Universiteit Amsterdam. \({}^{6}\)Camegie Mellon University

and Meta AI (FAIR). \({}^{7}\)University of Buffalo, SUNY. \({}^{8}\)University of Chicago and Together Computer.

###### Abstract

Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, _long_ convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads - naively requiring a full pass (or _caching_ of activations) over the input sequence for each generated token - similarly to attention-based models. In this paper, we seek to enable \((1)\) compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into _heads_, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves \(10\) higher throughput than Transformers and \(1.5\) higher than Hyena at \(1.3\)B parameters, without any loss in quality after distillation.

## 1 Introduction

Attention-free approaches such as _long convolution sequence models_ (LCSMs), e.g., H3 , Hyena , have shown promise in matching Transformer [3; 4] performance across a wide range of tasks, with sub-quadratic complexity with respect to sequence length. Despite the improved efficiency during training on long sequences, unless the convolution filters are either _short_ or admit a _low_-dimensional state-state-space realization, LCSMs still need to process the entire growing sequence at every step of auto-regressive generation, similarly to Transformers.

In this work, we seek to refine LCSMs in both **efficiency** and **quality**. First, we study the inference stage, and propose methods to enable a _recurrent_ mode for auto-regressive generation. Recurrent modes prescribe the existence of a _state_ encoding the past information of the process in a fixed-dimension memory, enabling **constant per-step time** and **constant-memory** in generation. Then, we draw upon an analysis of pre-trained models to develop architectural enhancements for the Hyena block, simultaneously improving model quality and efficiency of the distillation procedure.

Distilling fast recurrencesWe introduce LaughingHvena, the first distillation approach for LCSMs that enables recurrent inference without impacting downstream quality. LaughingHvena seeks compact recurrences in the form of _state-space models_ (SSMs) [5; 6] as the solution of a nonlinear interpolation problem involving the convolution filters of a pre-trained model. Since the total memory cost of SSMs grows linearly in the state dimension \(d\), our distillation procedure enables high throughput by enabling processing of large batches during generation. We identify and address three core challenges related to distillation, including the identification of:* **Target state dimension:** we identify candidate state dimensions of our distilled SSMs by analyzing the spectrum of the Hankel operator associated with each convolution .
* **Parametrization:** we address issues with naive parametrizations by introducing a factorized _modal_ form, inspired by barycentric  and Prony-like  methods.
* **Approximation metric:** to ensure compatibility with any downstream task, we choose discrepancy metrics on the convolution filter, rather than model outputs.

In auto-regressive workloads, LaughingHyena-distilled models with state dimension \(d\) can generate \(K\) tokens in \((dK)\) time and with constant \((d)\) memory - improving over the \((K^{2})\) time and \((K)\) memory usage of _kv-cached_ Transformers and naively executed long convolutions. At model sizes above one billion parameters, LaughingHyena achieves \(10\) higher peak throughput over comparable Transformers (Figure 1.1), and can process larger batch sizes. Constant memory generation enables larger \(K\) for a given a memory constraint e.g., generating \(512\) tokens with LaughingHyena requires \(3\) less memory than with a Transformer. At smaller batch sizes, latency of LaughingHyena is also competitive with Transformers, reaching \( 2\) speedups at longer prompt lengths.

Improving pre-training qualityWe leverage our analysis of the distillation process to open up new avenues of improvement for LCSM architectures. Indeed, the high compression rates achievable through LaughingHyena hint at sub-utilization of the convolution. We revisit the multi-headed design of H3 ; tying weights across channels pushes long convolution filters towards larger effective dimension, and as an additional advantage reduces the runtime of post-training distillation and inference memory footprint. Further, multi-head Hyena models improve on pre-training perplexity over regular Hyena and GPT  architectures on the language dataset The Pile .

## 2 Preliminaries and Related Work

We discuss convolutions, state spaces and auto-regressive generation workloads for sequence models.

ConvolutionsLet \(*\) denote the convolution operator. It is defined as the dual operation to pointwise multiplication under Fourier transform. In signal processing and deep learning alike, one often encounters the causal linear convolution of a filter \(h\) (which may extend indefinitely) with an input \(u\) of length \(L\):

\[(h*u)_{t}=_{j=0}^{t}h_{t-j}u_{j}.\] (2.1)

Generally, \(u_{t}^{D}\) where \(D\) is the width of the signal - or in deep learning parlance - the number of _channels_. Without loss of generality, we specialize our analysis to _single input single output_ layers, i.e. with \(D=1\). For the input-output relations of type (2.1), we use the terms _convolution layer_ and _linear system_ interchangeably. Similarly, the function \(t h_{t}\) is referred to as both the _filter_ and the _impulse response_ of a linear system. Existing convolution sequence models can be classified in terms of the parametrization used for their filters. The class of _implicit_ convolutions represent the filter as a parametric function \(_{}:t h_{t}\).

State-space realizationOne option is to select \(_{}\) as the _impulse response_ function of a discrete linear time-invariant system,

\[ x_{t+1}&=x_{t}+u_ {t}\\ y_{t}&=x_{t}+h_{0}u_{t},  t h_{t}=h_{0}&t=0\\ ^{t-1}&t>0\] (2.2)

with _state_\(x_{t}^{d}\), _input_\(u_{t}\), and _output_\(y_{t}\). The matrices \(^{d d}\), \(^{d 1}\), \(^{1 d}\), and \(h_{0}\) are the learnable parameters of the model while the initial state \(x_{0}\) is usually set to zero such that \(u y\) is a pure convolution. While linear systems (2.2) are the staple of signal processes and control theory, their use as implicit parametrization of convolution filters in deep neural networks have only recently emerged [12; 6]. Other parametrizations [13; 14; 2] select \(_{}(t)\) as different flavors

Figure 1.1: Throughput (in generated tokens) of Transformers, H3 and Hyena models. LaughingHyena is a recurrent model distilled from a pre-trained Hyena. Workload involves generating \(256\) tokens given a prompt of length \(512\).

of implicit representation neural networks [15; 16]. The latter are generally more powerful in terms of the class of filters they can represent and flexibility during training, at the cost of losing a fixed state dimension.

### Long Convolution Sequence Models

The \(\)-family of convolution sequence models \(-\) and \(\) - relies on a combination of long convolutions and data-controlled gating to replace attention with sub-quadratic scaling in sequence length1. We use the deep learning convention of naming different projections as _query_\(q\), _key_\(k\) and _value_\(v\). Let \(_{q}\) and \(_{k}\) be the \(L\)-by-\(L\) diagonal matrices whose respective main diagonal entries are the respective entries of length-\(L\) sequences \(q\) and \(k\). A \(\)-block realizes a surrogate attention matrix with a data-controlled, parameterized decomposition in three terms:

\[(q,k,v)(q,k)v,(q,k)=_{q}_{h}_{k}\] (2.3)

where \(_{h}{}^{L L}\) is the Toeplitz matrix constructed from the learnable long convolution filter \(h\), i.e., \(_{h}{=}(h_{i-j})_{i,j=0}^{L-1}\). The \(qkv\)-projections are themselves the output of a convolution between the input sequence and three distinct _short_ filters. The degrees of freedom in \(\)-block design are the three short filters2 and the _long_ filter \(h\). The long filter can be parameterized using an implicit neural representation , state-space model , or explicit values . The threefold decomposition of the attention operator, allows evaluation of (2.3) in just \(}(L)(L_{2}L)\) time (two convolutions3 and two element-wise products), \(y_{t}{=}q_{t}(h kv)_{t}\). The overall operator acts on an input \(u\) by constructing a third-order multi-variate polynomial of \(u\) whose coefficients are controlled (nonlinearly) by parameters of the block.

### Auto-Regressive Generation

A typical workload for sequence models is auto-regressive generation. Given a length-\(T\)_prompt_\(u^{T}\), the model is tasked with producing the following \(K\) additional outputs - one at a time - for a resulting output sequence \(y\) of length \(L{=}T{+}K\).

Convolution sequence modelsAfter processing the initial prompt in \(}(T)\) time and obtaining a length-\(T\) output \(u y_{0},,y_{T-1}\), a generic convolution layer can _cache_ the output sequence and generate any additional outputs using (2.1) auto-regressively, i.e. \(y_{t+1}{=}_{j=0}^{t}h_{t-j}y_{j}\) for \(t{=}T{-}1,,T{+}K{-}1\). It is important to note that auto-regressive generation with generic long convolutions is expensive. It comes with a **quadratic** cost in the number \(K\) of tokens to be generated and require storing a cache of length up to \(L\).

**Lemma 2.1**.: _Generating \(K\) tokens with a long convolution layer (2.1) from a length-\(T\) prompt has time complexity \((T_{2}T+TK+K^{2})\) and requires \((L)\) memory._

State-space modelsWhen the linear system admits a state space realization (2.2), i.e. it is able to switch between convolution and recurrent mode, the cost of auto-regressive generation can be dramatically reduced. The memory footprint is \((d)\): all we need to cache is the state \(x_{t}\), a \(d\)-dimensional vector. With some further machinery that we develop in next section, we can retain \(}(T)\) time and \((T)\) memory to process the prompt4 and initialize the state \(x_{T-1}\). Each additional generation step only requires \((d)\) time.

**Lemma 2.2**.: _Generating \(K\) tokens with a state-space model (2.2) from a length-\(T\) prompt has time complexity \((T_{2}T{+}dK)\) and requires \((T+d)\) memory._

Note that long filters \(h\) truncated to length \(d\) (i.e. \(h_{t}{=}0\) for \(t>d-1\)) can also be interpreted as \(d\)-dimensional SSMs (see Appendix A.7) where the state (a cache) coincides with the last \(d\) inputs.

TransformersSelf-attention is certainly less efficient than long convolutions in processing the prompt, coming with a hefty \((T^{2})\) time complexity. However, Transformers can achieve a similar

Figure 2.1: \(\)-block. \(^{(q)}\), \(^{(k)}\), \(^{(v)}\) are _short_-convolution operators.

efficiency in auto-regressive generation by **caching** the sequences of past keys \(\{k_{t}\}\) and values \(\{v_{t}\}\). Specifically, from \(t{=}T{-}1\) onward, the new projections \((q_{t{+}1},k_{t{+}1},v_{t{+}1})\) are evaluated from the current output \(y_{t}\), and the new output \(y_{t+1}\) can be computed in linear time with two reductions

\[y_{t+1}=^{t+1}(q_{t{+}1}k_{j})v_{j}}{_{i=0}^{t+1} (q_{t{+}1}k_{j})}:(x)=e^{x}.\]

**Lemma 2.3**.: _Generating \(K\) tokens with self-attention from a length-\(T\) prompt has time complexity \((T^{2}{+}TK{+}K^{2})\) and requires \((L)\) memory._

## 3 The Laughing Hyena Distillery

In this section, we introduce our distillation method. We discuss choosing an approximation objective, a parametrization for the approximant and setting a target state dimension.

Given any pre-trained LCSM, the objective of the distillation procedure is to convert each pre-trained convolution filter into a distinct state-space model (2.2). This should be achieved with the smallest state dimension \(d\) which preserves, up to a certain tolerance, the input-output characteristics of the convolution layer. Formally, given a filter \(h\) the **distillation problem** is defined as follows.

Given the sequence \(h_{1},,h_{L}\), find a state-space model (2.2) of dimension \(d L\), whose input-output behavior _approximates_ the one of the convolution with \(h\) over the largest class of input sequences.

The choice of approximation metrics and assumptions on the input sequences yield different _distillation objectives_. A _distillation algorithm_ constitutes a systematic procedure for optimally choosing the systems matrices with respect to a particular objective. In instances where the original filter \(h\) is itself the impulse response of a finite-dimensional state-space model, e.g., when attempting distillation of H3 or S4 filters, the term distillation becomes analogous to _model-order reduction_. Hence, in such cases, the distillation algorithm should yield a state-space representation of a lower order state-dimension.

There exist several algebraic solutions to the model reduction problem [18; 19; 20], typically seeking low-rank structures of the state space by inspecting some invariant of the system, e.g. the _Gramians_ in _balanced truncation_[19; Ch. 7]. The lower-order system is then obtained as a projection of the system dynamics onto the found subspace where the system retains desired characteristics, e.g., input-output behavior, stability, etc.

Truncated filtersIn theory, implicitly parameterized convolution filters can represent arbitrarily long signals. In practice, these filters are trained on a fixed _maximum length_\(L\). At inference time the model can then be evaluated for sequences longer than \(L\). During distillation it is nonetheless reasonable to treat the pre-trained filters as potentially very long (even beyond \(L\)) but _finite_ impulse response functions [21; 22; 23; 24]. We show how this choice is supported by empirical evidence displaying how pre-trained filters typically decay to zero in finite time (see Appendix D).

Transfer function representationAn alternative description of the system (2.2) is its _transfer function_\(H\), defined as the \(z\)-transform of the impulse response \(H(z){=}_{t=0}^{}h_{t}z^{-t}\) for all \(z{}\) where the sum converges. The transfer function is a _proper rational function_ of \(z\)

\[H(z)=h_{0}+(z{ l}-)^{-1}=h_{0}+z^{ -1}+\ \ +b_{d}z^{-d}}{1+a_{1}z^{-1}+\ \ +a_{d}z^{-d}}.\] (3.1)

In the \(z\)-domain, the transfer function defines the input-output map as \(Y(z)=H(z)U(z)\). Here, \(H(z)\) is defined outside the \(\)-plane circle of radius \(()\), \(_{()}{:=}\{z:|z|>()\}\) where \(()\) is the spectral radius of \(\), i.e. the amplitude of its largest eigenvalue. We can recover all characteristics of a given system equivalently from either its transfer function or state-space representations (see Appendix A.3 for further details and derivations). Notably, the transfer function is an _invariant_ of the system: if we apply a change of variables to the state, the transfer function remains unchanged (Lemma A.3). This alone should discourage attempts at modeling filters by learning _dense_ state-space matrices \(,,\) as such: there are infinitely many equivalent state-space realizations that map to the same system. Starting from coefficients \((a_{i})\) and \((b_{i})\) of the rational transfer function (3.1), we can compute the impulse response in \(}(L)\) time (Lemma A.6). Moreover, we can map back the transfer function to a special state-space realization - the _companion_ canonical form - whose recurrence has time complexity \((d)\) (Lemma A.7), compared to the \((d^{2})\) of dense state-space matrices. From Lemmas A.3 and A.7 we can also prove that any stable state-space model can be converted by _canonicalization_ into its companion form, and thus can be equipped with an efficient recurrence (Thm. A.8).

The distillation problem presents several challenges:

1. **Defining the distillation objective.** A primary decision involves selecting a distillation objective. We are primarily interested in metrics of pure discrepancy between each filter of a pre-trained deep model and its approximator, rather than the expected input-output loss over a distribution of inputs.
2. **Choosing a state-space parametrization.** It is crucial to determine a suitable parametrization of the distilled state-space realization. Once this is decided, the task is to identify the parameters that minimize the distillation desiderata, which can involve challenging optimization problems in itself.
3. **Selecting the target state dimension.** Lastly, a challenge is to estimate the degree to which the model's order can be reduced. In other words, we must select the target state dimension of the distillation process to identify the right trade-off between efficiency and accuracy.

In the following, we address each of these challenges, and provide a comprehensive approach (summarized in Figure 3.1) to distill recurrences from convolution-based architectures.

### Data-Free Distillation Objectives

We focus on distillation objectives that are independent of the training data and the overall architecture of the neural network under consideration. The distillation loss should be chosen as a pure measure of discrepancy between each convolution filter \(h_{t}\) of the model and their finite-dimensional approximations \(_{t}=^{t-1}\). This approach ensures that we do not require a full sequential inference pass over the pre-trained model at each step of distillation procedure and the distilled model can be more broadly applied to downstream tasks. This choice is supported by Young's convolution inequality [25; 26], which indicates that the output approximation error has a bound \(\|y-\|_{r}\|h-\|_{q}\|u\|_{p}\) for properly chosen norms5. For maximum numerical stability and freedom of parametrization for the approximants, we favor modern unconstrained gradient-based approaches to then solve the resulting distillation program6. We design distillation algorithms which either match filters in _time domain_ minimizing the \(_{2}\) error (\(\|h\|_{2}[_{t}|h_{t}|^{2}]^{1/2}\)) or match their transfer functions optimally with respect to the \(_{2}\) norm (\(\|H\|_{2}[(1/2)_{-}^{}|H(e^{i})|^{2} ^{1/2})^{}\). As the distillation is carried out via gradient methods, \(_{2}\) is a natural candidate. \(_{2}\) error minimization can instead be used to uniformy bound the worst-case discrepancy as \(\|h-\|_{}\|H-\|_{2}\) (see Appendix A.2 for further details). 

### Making Hyena Laugh with Modal Interpolation

Our degrees of freedoms to solve the distillation problem are the matrices \(\), \(\), and \(\) of the state-space realization, which determine the filter for all \(t>0\). In distilled SSMs, the passthrough (residual) term cannot be freely assigned: it is simply \(h_{0}\), the value of the original filter at zero. Alternatively, given its

Figure 3.1: The LaughingHyena long convolution sequence model distillation blueprint.

appealing invariance properties, we can parametrize a proper rational function \((z)\) (3.1) and fit it to the (truncated) transfer function8 of the original filter \(H_{L}(z){:=}_{t=0}^{L}h_{t}z^{-t}\) (see Appendix B.2).

Modal canonical formOptimizing the full transfer function can be numerically challenging for several reasons e.g., ensuring stability9, and ill-posedness for high-order polynomials. A natural solution, inspired by barycentric approaches to rational function approximation , is to assume \(d\) distinct roots \(_{n}\) in the denominator's polynomial, \(_{n}((a))\).

**Proposition 3.1** ().: _If \((a)\) has distinct roots \(\{_{n}\}\), then the transfer function of the system can be factorized as \((z){=}_{n=1}^{d}R_{n}/(z-_{n}), z_{ ()}\) where \(\{R_{n}\}\) is the residue associated with the pole \(_{n}\)._

Computing the inverse transform of the expanded transfer function via, e.g., the _Cauchy residue theorem_, shows that the resulting impulse response \(\) corresponds to a truncated basis of exponentially decaying complex sinusoids

\[_{t}=_{n=1}^{d}R_{n}_{n}^{t-1}, R_{n},_{n} ,t>0.\] (3.2)

In practice, this corresponds to the impulse response of state-space model with diagonal matrix \(=(_{1},,_{d})\) and such that \(_{i}_{i}=R_{i}\) for all \(i=1,,d\). The distillation problem can be then defined in terms of the \(L\)-point nonlinear least squares interpolation error (squared \(_{2}\)) between \(h_{1},,h_{L}\) and (3.2) evaluated for \(t{=}1,,L\): \(_{\{_{n},R_{n}\}}\|-h\|_{2}^{2}\). Note that in case of the target filter \(h\) being real-valued, the objective can be replaced by \(\|[]-h\|_{2}^{2}\).

Although we find solutions of the distillation (interpolation) problem via modern gradient-based optimization techniques, it is worth mentioning that Prony showed how the nonlinear least square solution can be computed solving two linear problems . However, similar to Pade's method for rational approximation , these techniques can be numerically unstable. We opt for a parametrization similar to  where each eigenvalue is parameterized in polar form \(_{n}{:=}A_{n}e^{i_{n}}\) and the residues in cartesian form10. Note that, with this parametrization we have \([_{t}]=_{n}A_{n}^{t-1}[(R_{n})( _{n}(t-1))-(R_{n})(_{n}(t-1))]\). We can also solve the distillation problem in the \(_{2}\) sense by evaluating \(_{t}\) and \(h_{t}\) at \(t=0,,L-1\) and taking their respective (discrete) Fourier transform before computing the objective. Efficient evaluation of (3.2) is crucial for distillation. In particular we show the following:

**Lemma 3.1**.: _Evaluation of \((_{t})_{t=0}^{L-1}\) (3.2) can be done in \((dL)\) time from its modal form and in \(}(L)\) time from its proper rational form._

### Minimal Distillation Orders

Distilling into lower-dimensional systems is always desirable as they require fewer parameters to be optimized and they yield recurrences that are (linearly) more efficient in terms of time and memory complexity in post-distillation auto-regressive inference workloads. The dimension of _the smallest possible state-space model with impulse response exactly \(\{h_{t}\}_{t}\)_ is the so-called _McMillan degree_:

\[d^{*}=_{d}d\ :\ ^{d d}, ^{d 1},^{1 d}h_{t}=^{t-1},\  t>0\] (3.3)

**Theorem 3.1** (Ho-Kalman [35, Theorem 2, Corollary]).: _Let \(\) be the (infinite) Hankel matrix constructed with \(h\), i.e. \((h_{i+j})_{i,j=1}^{}\). Then, \(d^{*}=()\)._

Figure 3.2: Example of modal interpolation. The approximant is a linear combination of exponentially-decaying complex exponential basis functions with learned decay rate.

A lower bound for \(d^{*}\) can be estimated from a truncated filter of length \(L\) by constructing the \(L L\) principal sub-matrix \(_{L}\) and using the fact that \(()(_{L})\). Inspecting how fast the Hankel singular values \((_{n})_{n=1}^{L}\) decay in pre-trained convolution models can be predictive of the approximation quality at a fixed dimension. As a rule of thumb, \(d\) needs to be sufficiently large for \(_{d+1}\) to be sufficiently small11. Specifically, we can prove that the _last_ singular value \(_{d}\) determines the upper bound of distillation quality with a SSM of dimension \(d\), in terms of the Hankel norm . This is a direct consequence of Adamjan-Arov-Krein theorem  and can be informally stated as follows.

**Theorem 3.2** (Informal).: _Let \(h\) be a length-\(L\) filter, \(\) a distilled filter of order \(d<L\) and let \(_{L},}_{L}\) be the respective Hankel matrices. Then \(_{}_{L}}\|_{L}-}_{L}\|_{2}=_ {d}\)._

### Deploying the Recurrence

Once all the filters of a pre-trained model have been distilled with the proposed modal interpolation technique described above, the model unlocks a _recurrent mode_ which allocates a state \(x_{t}^{d}\) for each filter and enables fast auto-regressive inference. Deployment of distilled model involves two critical steps: the _pre-filling_ and the recurrent update rule itself.

Fast pre-fillingDuring auto-regressive generation, when a length-\(T\) prompt is fed to the model, we need to compute the state \(x_{T}\) to start generating new tokens. Using the recurrence, the time complexity of initializing \(x_{T}\) would be \((dT)\) with a \((d)\) memory footprint. One can alternatively distribute the computation on \(d\) processors with a _parallel scan_ operation [37; 38] to reach a parallel time complexity \((d_{2}T)\) while incurring in an increased memory requirement of \((dT)\)12. A third option is to use a single FFT convolution to obtain \(x_{T}\) in \(}(T)\) time and \((T)\) memory.

**Proposition 3.2**.: \(x_{T}=(_{T},,_{T-d})\) _where \(_{t}=(g*u)_{t}\) and \(g\) is the filter whose transfer function is \(1/()(z)\) and can be evaluated in \(}(T)\)._

Note that, the fast pre-filling algorithm established by this result requires evaluating the denominator polynomial of \(\) from its roots before deployment. This is equivalent to converting the transfer function from its factorized representation to its rational form (3.1).

Recurrent stepThe update rule is diagonal, thus efficiently evaluated in \((d)\) time and memory:

**Proposition 3.3**.: _The filter (3.2) has a state space matrices \(=(_{1},,_{d})^{d  d},\ =(1,,1)^{}^{d 1},\ =(R_{1},,R_{d}) ^{1 d}\) whose step can be evaluated in \((d)\) time and memory._

As we generally want the output \(y_{t}\) to be real-valued, we can simply update the complex state \(x_{t+1}=x_{t}+u_{t}\) and then take the real part of the output, \(y_{t}=[x_{t}]+h_{0}u_{t}\).

## 4 Multi-head Long Convolutions

We can leverage the Hankel spectrum analysis discussed in Section 3.3 to study the dynamics of the effective dimensionality of each convolution filter during LCSMs pre-training. We find that, at initialization, filters correspond to high-dimensional SSMs, and gradually converge to lower-dimensional representations during training. See Appendix E.2 for examples on Hyena and H3 models.

This observation leads to the question: _is it advantageous to perform independent long convolutions on each channel, or can we reduce the total number of filters without loss in quality?_ To answer this, we adapt the multi-head layer design proposed by H3 to Hyena:

1. Given the projections \(q,k,v^{L D}\), we split them into \(M\) chunks of size \(N{=}D/M\), \(q^{m},k^{m},v^{m}^{L N}\).
2. Each chunk is processed by a modified Hyena operator: first, we perform the outer product of \(k^{m}\) and \(v^{m}\) along the spatial dimension, \(z^{m}{:=}k^{m} v^{m}^{L N N}\), apply a long convolution with filter \(h^{m}\) to all \(N{}N\) elements independently, then compute \(y_{t}^{m}{=}(h^{m}*z^{m})_{t}q_{t}^{m}\), \(y^{m}^{L N}\) as shown in Figure 4.
3. Finally, we compose \(y^{1},,y^{m}\) into a single output \(y^{L D}\) via concatenation.

An instance of a MultiJena is equipped with \(M<D\) distinct long convolution filters, which leads to (\(a\)) faster distillation, with less filters to approximate, (\(b\)) lower memory footprint, via a total reduction of the states to cache during generation and (\(c\)) faster filter generation, by tying the weights of filter parameters. We note that tying weights of key-value projections has also been shown to be an effective technique to reduce memory cost in Transformers [39; 40].

Crucially, the multi-head structure of MultiJena enables us to prove favorable scaling in the _associative recall_ synthetic task, which was shown in  to be predictive of performance at scale. In associative recall, the model is given a sequence of key-value pairs and a query, and is tasked with matching the query to a key in the sequence by returning its associated value. The difficulty of the task grows with the vocabulary size \(s\): larger vocabularies necessitate wider models.

**Theorem 4.1**.: _The MultiJena layer, with \(( s)\) heads and model size \(( s)\) can solve the associative recall problem, where \(s\) denotes the vocabulary size._

In Appendix E.1, we empirically verify improved scaling in vocabulary size with multiple heads.

## 5 Experiments

* **Pretraining:** We pretrain a suite of MultiJena language models on The Pile , investigating scaling of perplexity with different amounts of total tokens (5, 10, 15 billion), as well as larger training runs for 300 billion tokens. MultiJena outperforms Transformers and Hylena.
* **Distillation analysis:** We investigate the relation between optimal distillation orders, Hankel spectrum, and errors on the logits of distilled models.
* **Post-distillation downstreams:** We evaluate the downstream impact of distilling long convolutional language models, reporting HELM  and LM-Eval-Harness  results.
* **Benchmarking:** We benchmark latency, throughput and memory along the different axes of batch size, sequence length, number of generated tokens. We include base models, distilled models and equivalent Transformers.

### Pre-training

To validate the multi-head formulation, we train \(150\) and \(350\) million parameter MultiJena models on The Pile  using \(8\) heads and otherwise the same architecture as equivalent Hylena models, following the setup of . Via the multi-head structure introduced in \(4\), MultiJena outperforms both Hylena and Transformers, including on data scaling runs with increasing numbers of tokens and full \(300\)B tokens runs (Table 5.1).

### Distillation Analysis

Next, we verify whether Hankel singular values are predictive of downstream errors, and whether large models can be distilled without loss in quality. We apply LaughingHylena distillation to pre-trained MultiJena, Hylena and H3 of different sizes. Concretely, for each layer and channel of a model, we parametrize the poles \(\{_{n}\}\) of the modal canonical forms (Section 3.2) at different orders \(d\), and solve for each \(_{2}\) approximation problem.

Approximation errors and spectrumWe investigate the magnitude of approximation errors introduced by LaughingHylena distillation. Given a pretrained MultiJena model, we compute the errors between original and distilled filters at each layer, averaged across channels. We repeat this process for different distillation orders (state dimension of the model form of Section 3.2). Figure 5.2 visualizes

Figure 4.1: A single head of a multi-head Hylena.

   Model & Perp. & Model & 5B & 10B & 15B \\  GPT & 9.3 & GPT (125M) & 13.3 & 11.9 & 11.2 \\ Hylena & 9.3 & Hylena (153M) & 13.3 & 11.8 & 11.1 \\ MultiJena & **8.7** & MultiJena (153M) & **12.1** & **11.0** & **10.6** \\    
   Model & 5B & 10B & 15B \\  GPT (355M) & 11.4 & 9.8 & 9.1 \\ Hylena (355M) & 11.3 & 9.8 & 9.2 \\ MultiJena (355M) & **10.6** & **9.4** & **8.9** \\   

Table 5.1: **[Left]** Perplexity of small models on The Pile, after pre-training for \(300\) billion tokens. **[Center and Right]** Perplexity on The Pile for models trained until a total number of tokens e.g., 5 billion (different runs for each token total).

minimum, maximum and average errors, per-layer errors and the distribution of the singular values of the Hankel operator associated to each filter. We observe distillation orders (\(>16\)) that yield smalls errors to be predicted by the distribution of singular values. Thus, analysis of the Hankel operator's spectrum is verified to be an effective approach to direct estimation of the optimal distillation order. We also note that the optimal order changes across layers, offering options for further optimization.

Output errorsNext, we compute relative \(_{1}\) error between output logits of pre-trained and distilled models to ensure LaughingHyena can be used in generation workloads. The optimal minimal distillation order estimated via Hankel operators (16) is sufficient to keep the output distribution over the vocabulary (\(>50\)k entries) close to the pre-trained model, as shown in Figure 5.2. Inspecting the error profile over logits sorted by magnitude reveals our approach to be robust to different sampling strategies for generation, including greedy decoding, top-\(k\), top-\(p\). Indeed, the relative errors are \(<\!10^{-2}\) up to and including the \(99.99\%\) percentile of the distribution, meaning e.g., a top-\(p\) sampling strategy with large \(p\) can be used on a distilled model without drift in outputs (mis-classified tokens). We note that the relative errors are maximum on small-norm logits, which are not required by most sampling strategies. In Appendix D.2, we provide a similar distillation error analysis for Hyena and H3 models. We find that Hyena and can be distilled with less than \(32\) orders and H3 with less than \(8\).

### Downstream Evaluation

We check how distillation affects downstream performance on language benchmarks. We apply distillation of order \(8\), \(16\) and \(32\) to our The Pile-pretrained MultiJyena language model and benchmark (Table 5.3) its performance on a suite of canonical (zero shot) tasks from LM-Eval-Harness  and HELM . The results are consistent with our error analysis: distillation orders equal or greater to \(16\) introduce little-to-no quality degradation.

Figure 5.1: Errors between logits of pretrained and distilled MultiJyena. In blue, we plot (ordered) logits, in light blue the cumulative distribution function, and in black the relative errors. The green dotted line indicates the 99.99% percentile. As the errors grow slowly as function of the percentiles, model outputs do not diverge from the base model.

   Model & LAMBADA & Winogrande & PIQA & HellaSwag & OpenbookQA \\  & acc & acc & acc & acc norm. & acc norm. \\  Pythia (160M) & 32.8 & **53.1** & 61.6 & 31.6 & **29.2** \\  MultiJyena (154M) & **43.2** & 52.7 & **64.6** & **34.1** & 29.0 \\ LaughingHyena-16 & 43.1 & 52.6 & 64.7 & 34.1 & 28.9 \\ LaughingHyena-8 & 0.0 & 51.8 & 51.5 & 32.7 & 28.2 \\ LaughingHyena-4 & 0.0 & 49.6 & 53.7 & 26.4 & 26.4 \\   

Table 5.2: Evaluation of LaughingHyena-distilled models pre and post modal distillation. We test on LM-Eval-Harness  and HELM  tasks, reporting Pythia  performance as a Transformer baseline trained on the same data. LaughingHyena-\(d\) is a MultiJyena model with each filter distilled of order \(d\).

### Benchmarking

We measure throughput, latency and memory usage of LaughingHyena for auto-regressive generation workloads, with initial prompt length \(T\) and number of generated tokens \(K\). The throughput is computed as number of generated tokens over latency. For each setting (and additional benchmarks), we provide details in Appendix D.4.

Peak throughputDistilled models do not need \(kv\)-caches. This reduces memory requirement during generation, enabling higher peak throughput in large-batch workloads. We achieve \(10\) higher throughput than Transformers at size \(1.3\) billion parameters (Figure 1). Throughput is higher than Transformers even at fixed batch sizes, indicating lower latency.

SSM state dimension and throughputFor typical distillation orders (\(<100\)), peak throughput is not greatly affected. We measure a \(2\%\) reduction in throughput from \(32\) to \(64\).

Prompt lengthThe throughput of LaughingHyena-distilled models is \(4\) larger than Transformers at fixed batch size \(64\) and prompt length \(1536\) (Figure 3). As prompt length increases, the runtime gap between pre-filling via convolutions in LCSMs and pre-filling in Transformers widens (e.g., \(}(T)\) as detailed in Section 3.4, compared to \((T^{2})\)).

Memory footprintRecurrent models do not require \(kv\)-caches and use constant memory for generation of an arbitrary number of tokens (Figure 4).

## 6 Conclusion

We study the efficiency and quality of state-of-the-art long convolutional sequence models. First, we introduce LaughingHyena, a novel distillation method inspired by rational function approximation and model-order reduction techniques. LaughingHyena can be applied after training to extract compact state-space models from each convolutional filter, without loss of quality. Distilled models achieve higher throughput than equivalently-sized Transformers, and can perform auto-regressive generation in constant memory by sidestepping the need to cache previous outputs. We theoretically and empirically investigate the trade-offs of different strategies for fast inference of recurrent models, and introduce architectural improvements to Hyena that improve pretraining quality.

Figure 4: Peak GPU memory for generation.

Figure 3: Scaling in prompt \(T\).

## Broader Impact

In this work, we focus on advances related to efficient models for long sequences.

EfficiencyOur distillation methods for constant-memory, high throughput inference in _long convolution sequence models_ (\(\)) can lead to energy savings during model deployement, enabling processing of longer-form content at a fraction of the cost and reducing environmental impact. Improved efficiency may also affect other aspects of AI safety, as it may make it easier produce malicious or harmful content.

AccessibilityBy improving the efficiency of training and generation,\(\) and \(\) may contribute to increased accessibility of large language models, lowering the hardware barrier to entry for individuals and organizations with limited resources.

SteerabilityNew method based on \(\) enable sequence models to process long-form prompts previously inaccessible by Transformers, which may lead to increased control over models via e.g., conditioning on additional instructions .