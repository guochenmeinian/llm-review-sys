# Collaboration! Towards Robust Neural Methods

for Routing Problems

 Jianan Zhou

Nanyang Technological University

jianan004@e.ntu.edu.sg

&Yaoxin Wu

Eindhoven University of Technology

y.wu2@tue.nl

&Zhiguang Cao

Singapore Management University

zgcao@smu.edu.sg

&Wen Song

Shandong University

wensong@email.sdu.edu.cn

&Jie Zhang, &Zhiqi Shen

Nanyang Technological University

{zhangj,zqshen}@ntu.edu.sg

Yaoxin Wu is the corresponding author.

###### Abstract

Despite enjoying desirable efficiency and reduced reliance on domain expertise, existing neural methods for vehicle routing problems (VRPs) suffer from severe robustness issues - their performance significantly deteriorates on clean instances with crafted perturbations. To enhance robustness, we propose an ensemble-based _Collaborative Neural Framework (CNF)_ w.r.t. the defense of neural VRP methods, which is crucial yet underexplored in the literature. Given a neural VRP method, we adversarially train multiple models in a collaborative manner to synergistically promote robustness against attacks, while boosting standard generalization on clean instances. A neural router is designed to adeptly distribute training instances among models, enhancing overall load balancing and collaborative efficacy. Extensive experiments verify the effectiveness and versatility of CNF in defending against various attacks across different neural VRP methods. Notably, our approach also achieves impressive out-of-distribution generalization on benchmark instances.

## 1 Introduction

Combinatorial optimization problems (COPs) are crucial yet challenging to solve due to the NP-hardness. Neural combinatorial optimization (NCO) aims to leverage machine learning (ML) to automatically learn powerful heuristics for solving COPs, and has attracted considerable attention recently . Among them, a large number of NCO works develop neural methods for _vehicle routing problems_ (VRPs) - one of the most classic COPs with broad applications in transportation , logistics , planning and scheduling , etc. With various training paradigms (e.g., reinforcement learning (RL)), the neural methods learn construction or improvement heuristics, which achieve competitive or even superior performance to the conventional algorithms. However, recent studies show that these neural methods are plagued by severe robustness issues , where their performance drops devastatingly on clean instances (sampled from the training distribution) with crafted perturbations.

Although the robustness issue has been investigated in a couple of recent works [87; 20; 42], the defensive methods on how to help forge sufficiently robust neural VRP methods are still underexplored.

In particular, existing endeavours mainly focus on the _attack2_ side, where they propose different perturbation models to generate adversarial instances. On the _defense_ side, they simply follow the vanilla adversarial training (AT) . Concretely, treated as a _min-max_ optimization problem, it first generates adversarial instances that maximally degrade the current model performance, and then minimizes the empirical losses of these adversarial variants. However, vanilla AT is known to face an undesirable trade-off [66; 85] between standard generalization (on clean instances) and adversarial robustness (against adversarial instances). As demonstrated in Fig. 1, vanilla AT improves adversarial robustness of the neural VRP method at the expense of standard generalization (e.g., POMO (1) vs. POMO_AT (1)). One key reason is that the training model is not sufficiently expressive . We empirically justify this viewpoint by increasing the model capacity through ensembling multiple models, which partially alleviates the trade-off. However, it is still an open question on how to effectively synergize multiple models to achieve favorable overall performance on both clean and adversarial instances within a reasonable computational budget.

In this paper, we focus on the _defense_ of neural VRP methods, aiming to concurrently enhance both the standard generalization and adversarial robustness. We resort to the ensemble-based AT method to achieve this objective. Instead of separately training multiple models, we propose a _Collaborative Neural Framework (CNF)_ to exert AT on multiple models in a collaborative manner. Specifically, in the inner maximization optimization of CNF, we synergize multiple models to further generate the _global_ adversarial instance for each clean instance by attacking the best-performing model, rather than only leveraging each model to independently generate their own _local_ adversarial instances. In doing so, the generated adversarial instances are diverse and strong in benefiting the policy exploration and attacking the models, respectively (see Section 4). In the outer minimization optimization of CNF, we train an attention-based neural router to forward instances to models for effective training, which helps achieve satisfactory load balancing and collaborative efficacy.

Our contributions are outlined as follows. 1) In contrast to the recent endeavors on the attack side, we concentrate on the defense of neural VRP methods, which is crucial yet underexplored in the literature. We empirically observe that the defense through vanilla AT may lead to the undesirable trade-off between standard generalization and adversarial robustness in VRPs. 2) We propose an ensemble-based collaborative neural framework to concurrently enhance the performance on both clean and adversarial instances. Specifically, we propose to further generate global adversarial instances, and design an attention-based neural router to distribute instances to each model for effective training. 3) We evaluate the effectiveness and versatility of our method against various attacks on different VRPs, such as the symmetric and asymmetric traveling salesman problem (TSP, ATSP) and capacitated vehicle routing problem (CVRP). Results show that our framework can greatly improve the adversarial robustness of neural methods while even boosting the standard generalization. Beyond the expectation, we also observe the improved out-of-distribution (OOD) generalization on both synthetic and benchmark instances, which may suggest the favorable potential of our method in promoting various types of generalization of neural VRP methods.

Figure 1: (a-b) Performance of POMO  on TSP100 against the attacker in . The value in brackets denotes the number of trained models. We report the average optimality (opt.) gap over 1000 test instances. (c) Solution visualizations on an adversarial instance. These results reveal the vulnerability of existing neural methods to adversarial attacks, and the existence of undesirable trade-off between standard generalization (a) and adversarial robustness (b) in VRPs. Details of the attacker and experimental setups can be found in Appendix B.1 and Section 5, respectively.

Related Work

**Neural VRP Methods.** Most neural VRP methods learn construction heuristics, which are mainly divided into two categories, i.e., autoregressive and non-autoregressive ones. Autoregressive methods sequentially construct the solution by adding one feasible node at each step.  proposes the Pointer Network (Ptr-Net) to solve TSP with supervised learning. Subsequent works trainPtr-Net with RL to solve TSP  and CVRP .  introduces the attention model (AM) based on the Transformer architecture  to solve a wide range of COPs including TSP and CVRP.  further proposes the policy optimization with multiple optima (POMO), which improves upon AM by exploiting solution symmetries. Further advancements [39; 34; 3; 24; 13; 44; 27; 45; 21; 12] are often developed on top of AM and POMO. Regarding non-autoregressive methods, the solution is typically constructed in a one-shot manner without iterative forward passing through the model.  leverages the graph convolutional network to predict the probability of each edge appearing on the optimal tour (i.e., heat-map) using supervised learning. Recent works [17; 36; 56; 64; 49; 82; 33; 77] further improve its performance and scalability by using advanced models, training paradigms, and search strategies. We refer to [40; 28; 83; 88] for scalability studies, to [30; 5; 90; 18; 41; 16; 89; 4] for generalization studies, and to [11; 59; 84; 61] for other COP studies. On the other hand, some neural methods learn improvement heuristics to refine an initial feasible solution iteratively, until a termination condition is satisfied. In this line of research, the classic local search methods and specialized heuristic solvers for VRPs are usually exploited [8; 43; 10; 76; 47; 78; 46]. In general, the improvement heuristics can achieve better performance than the construction ones, but at the expense of much longer inference time. In this paper, we focused on autoregressive construction methods.

**Robustness of Neural VRP Methods.** There is a recent research trend on the robustness of neural methods for COPs [68; 20; 42], with only a few works on VRPs [87; 20; 42]. In general, they primarily focus on attacking neural construction heuristics by introducing effective perturbation models to generate adversarial instances that are underperformed by the current model. Following the AT paradigm,  perturbs node coordinates of TSP instances by solving an inner maximization problem (similar to the fast gradient sign method ), and trains the model with a hardness-aware instance-reweighted loss function.  proposes an efficient and sound perturbation model, which ensures the optimal solution to the perturbed TSP instance can be directly derived. It adversarially inserts several nodes into the clean instance by maximizing the cross-entropy over the edges, so that the predicted route is maximally different from the derived optimal one.  leverages a no-worse optimal cost guarantee (i.e., by lowering the cost of a partial problem) to generate adversarial instances for asymmetric TSP. However, existing methods mainly follow vanilla AT  to deploy the defense, leaving a considerable gap to further consolidate robustness.

**Robustness in Other Domains.** Deep neural networks are vulnerable to adversarial examples , spurring the development of numerous attack and defensive methods to mitigate the arisen security issue across various domains. 1) **Vision:** Early research on adversarial robustness mainly focus on the continuous image domain (e.g., at the granularity of pixels). The vanilla AT, as formulated by  through min-max optimization, has inspired significant advancements in the field [65; 60; 85; 7; 86]. 2) **Language:** This domain investigates how malicious inputs (e.g., characters and words) can deceive (large) language models into making incorrect decisions or producing unintended outcomes [14; 72; 50; 91; 74]. Challenges include the discrete nature of natural languages and the complexity of linguistic structures, necessitating sophisticated techniques for generating and defending against adversarial attacks. 3) **Graph:** Graph neural networks are also susceptible to adversarial perturbations in the underlying graph structures , prompting research to enhance their robustness [29; 15; 19; 23; 63]. Similar to the language domain, challenges stem from the discrete nature of graphs and the interconnected nature of graph data. Although various defensive methods have been proposed for these specific domains, most are not adaptable to the VRP (or COP) domain due to their needs for ground-truth labels, reliance on the imperceptible perturbation model, and unique challenges inherent in combinatorial optimization.

## 3 Preliminaries

### Neural VRP Methods

**Problem Definition.** Without loss of generality, we define a VRP instance \(x\) over a graph \(=\{,\}\), where \(=\{v_{i}\}_{i=1}^{n}\) represents the node set, and \((v_{i},v_{j})\) represents the edge set with \(v_{i} v_{j}\).

The solution \(\) to a VRP instance is a tour, i.e., a sequence of nodes in \(\). The cost function \(c()\) computes the total length of a given tour. The objective is to seek an optimal tour \(^{*}\) with the minimal cost: \(^{*}=_{}c(|x)\), where \(\) is the set of all feasible tours which obey the problem-specific constraints. For example, a feasible tour in TSP should visit each node exactly once, and return to the starting node in the end. For CVRP, each customer node in \(\) is associated with a demand \(_{i}\), and a depot node \(v_{0}\) is additionally added into \(\) with \(_{0}=0\). Given the capacity \(Q\) for each vehicle, a tour in CVRP consists of multiple sub-tours, each of which represents a vehicle starting from \(v_{0}\), visiting a subset of nodes in \(\) and returning to \(v_{0}\). It is feasible if each customer node in \(\) is visited exactly once, and the total demand in each sub-tour is upper bounded by \(Q\). The optimality gap \()}{c(^{*})} 100\%\) is used to measure how far a solution is from the optimal solution.

**Autoregressive Construction Methods.** Popular neural methods [37; 38] construct a solution to a VRP instance following Markov Decision Process (MDP), where the policy is parameterized by a neural network with parameters \(\). The policy takes the states as inputs, which are instantiated by features of the instance and the partially constructed solution. Then, it outputs the probability distribution of valid nodes to be visited next, from which an action is taken by either greedy rollout or sampling. After a complete tour \(\) is constructed, the probability of the tour can be factorized via the chain rule as \(p_{}(|x)=_{s=1}^{S}p_{}(_{}^{s}|_{}^{<s},x)\), where \(_{}^{s}\) and \(_{}^{<s}\) represent the selected node and the partial solution at the \(s_{}\) step, and \(S\) is the number of total steps. Typically, the reward is defined as the negative length of a tour \(-c(|x)\). The policy network is commonly trained with REINFORCE . With a baseline function \(b()\) to reduce the gradient variance and stabilize the training, it estimates the gradient of the expected reward as:

\[_{}(|x)=_{p_{}(|x)}[(c( )-b(x))_{} p_{}(|x)].\] (1)

### Adversarial Training

Adversarial training is one of the most effective and practical techniques to equip deep learning models with adversarial robustness against crafted perturbations on the clean instance. In the supervised fashion, where the clean instance \(x\) and ground truth (GT) label \(y\) are given, AT is commonly formulated as a min-max optimization problem:

\[_{}_{(x,y)}[(y,f_{}())],=_{_{i}_{}[x]}[(y,f_{ }(_{i}))],\] (2)

where \(\) is the data distribution; \(\) is the loss function; \(f_{}()\) is the model prediction with parameters \(\); \(_{}[x]\) is the neighborhood around \(x\), with its size constrained by the attack budget \(\). The solution to the inner maximization is typically approximated by projected gradient descent:

\[x^{(t+1)}=_{_{}[x]}[x^{(t)}+( _{x^{(t)}}(y,f_{}(x^{(t)})))],\] (3)

where \(\) is the step size; \(\) is the projection operator that projects the adversarial instance back to the neighborhood \(_{}[x]\); \(x^{(t)}\) is the adversarial instance found at step \(t\); and the sign operator is used to take the gradient direction and carefully control the attack budget. Typically, \(x^{(0)}\) is initialized by the clean instance or randomly perturbed instance with small Gaussian or Uniform noises. The adversarial instance is updated iteratively towards loss maximization until a stop criterion is satisfied.

**AT for VRPs.** Most ML research on adversarial robustness focuses on the continuous image domain [22; 48]. We would like to highlight two main differences in the context of discrete VRPs (or COPs). 1) _Imperceptible perturbation:_ The adversarial instance \(\) is typically generated within a small neighborhood of the clean instance \(x\), so that the adversarial perturbation is imperceptible to human eyes. For example, the adversarial instance in image related tasks is typically bounded by \(_{}[x]:\|x-\|_{p}\) under the \(l_{p}\) norm threat model. When the attack budget \(\) is small enough, \(\) retains the GT label of \(x\). However, it is not the case for VRPs due to the nature of discreteness. The optimal solution can be significantly changed even if only a small part of the instance is modified. Therefore, the subjective imperceptible perturbation is not a realistic goal in VRPs, and we do not exert such an explicit imperceptible constraint on the perturbation model (see Appendix A for further discussions). In this paper, we set the attack budget within a reasonable range based on the attack methods. 2) _Accuracy-robustness trade-off:_ The standard generalization and adversarial robustness seem to be conflicting goals in image related tasks. With increasing adversarial robustness the standard generalization tends to decrease, and a number of works intend to mitigate such a trade-off in the image domain [66; 85; 73; 57; 80]. By contrast, a recent work  claims the existence of neural solvers with high accuracy and robustness in COPs. They state that a _sufficiently expressive_ model does not suffer from the trade-off given the problem-specific _efficient and sound_ perturbation model, which guarantees the correct GT label of the perturbed instance. However, by following the vanilla AT, we empirically observe that the undesirable trade-off may still exist in VRPs (as shown in Fig. 1), which is mainly due to the insufficient model capacity under the specific perturbation model. Furthermore, similar to combinatorial optimization, although the language and graph domains are also discrete, their methods cannot be trivially adapted to VRPs (see Appendix A).

## 4 Collaborative Neural Framework

In this section, we first present the motivation and overview of the proposed framework, and then introduce the technical details. Overall, we propose a collaborative neural framework to synergistically promote adversarial robustness among multiple models, while boosting standard generalization. Since conducting AT for deep learning models from scratch is computationally expensive due to the extra inner maximization steps, we use the model pretrained on clean instances as a warm-start for subsequent AT steps. An overview of the proposed method is illustrated in Fig. 2.

**Motivation.** Motivated by the empirical observations that 1) existing neural VRP methods suffer from severe adversarial robustness issue; 2) undesirable trade-off between adversarial robustness and standard generalization may exist when following the vanilla AT, we propose to adversarially train multiple models in a _collaborative_ manner to mitigate the above-mentioned issues within a reasonable computational budget. It then raises the research question on how to effectively and efficiently train multiple models under the AT framework, involving a pair of inner maximization and outer minimization, which will be detailed in the following parts. Note that despite the accuracy-robustness trade-off being a well-known research problem in the literature of adversarial ML, most works focus on the image domain. Due to the needs for GT labels or the dependence on the imperceptible perturbation model, their methods (e.g., TRADES , Robust Self-Training with AT ) cannot be directly leveraged to solve this trade-off in VRPs. We refer to Appendix A for further discussions.

**Overview.** Given a pretrained model \(_{p}\), CNF deploys the AT on its \(M\) copies (i.e., \(^{(0)}=\{_{p}^{(0)}\}_{j=1}^{M}_{p}\)) in a collaborative manner. Concretely, at each training step, it first solves the inner maximization optimization to synergistically generate the local (\(\)) and global (\(\)) adversarial instances, on which the current models underperform. Then, in the outer minimization optimization, we jointly train an attention-based neural router \(_{r}\) with all models \(\) by RL in an end-to-end manner. By adaptively distributing instances to different models for training, the neural router can reasonably exploit the overall capacity of models and thus achieve satisfactory load balancing and collaborative efficacy. During inference, we discard the neural router \(_{r}\) and use the trained models \(\) to solve each

Figure 2: The overview of CNF. Suppose we train \(M=3\) models (\(=\{_{1},_{2},_{3}\}\)) on a batch (\(B=3\)) of clean instances. The inner maximization generates local (\(\)) and global (\(\)) adversarial instances within \(T\) steps. In the outer minimization, a neural router \(_{r}\) is jointly trained to distribute instances to the \(M\) models for training. Specifically, based on the logit matrix \(\) predicted by the neural router, each model selects the instances with Top\(\)-largest logits (e.g., red ones). The neural router is optimized to maximize the improvement of collaborative performance after each training step of \(\). For simplicity, we omit the superscripts of instances in the outer minimization.

instance. The best solution among them is returned to reflect the final _collaborative performance_. We present the pseudocode of CNF in Algorithm 1, and elaborate each part in the following subsections.

```
0: training steps: \(E\), number of models: \(M\), attack steps: \(T\), batch size: \(B\), pretrained model: \(_{p}\);
0: robust model set \(^{(E)}=\{^{(E)}_{j}\}_{j=1}^{M}\);
1: Initialize \(^{(0)}=\{^{(0)}_{1},,^{(0)}_{M}\}_{p}\), \(^{(0)}_{r}\)
2:for\(=1\),..., \(E\)do
3:\(\{x_{i}\}_{i=1}^{B}\) Sample a batch of clean instances
4: Initialize \(^{(0)}_{i,j},^{(0)}_{i} x_{i},\ \  i[1,B],\  j[1,M]\)
5:for\(=1,,T\)do\(\) Inner Maximization
6:\(^{(t)}_{i,j}\) Approximate solutions to \((^{(t-1)}_{i,j};^{(e-1)}_{j}),\ \  i[1,B],\  j[1,M]\)
7:\(^{(e-1)}_{b_{i}}\) Choose the best-performing model for \(^{(t-1)}_{i}\) from \(^{(e-1)},\ \  i[1,B]\)
8:\(^{(t)}_{i}\) Approximate solutions to \((^{(t-1)}_{i};^{(e-1)}_{b_{i}}),\ \  i[1,B]\)
9:endfor
10:\(\{x_{i},^{(T)}_{i,j},^{(T)}_{i }\},\  i[1,B],\  j[1,M]}\)\(\) Outer Minimization
11:\(\) Evaluate \(\) on \(^{(e-1)}\)
12:\(}\) Softmax(\(f_{^{(e-1)}_{r}}(,)\))
13:\(^{(e)}\) Train \(^{(e-1)}_{j}^{(e-1)}\) on Top\((}_{ j})\) instances, \(\  j[1,M]\)
14:\(^{}\) Evaluate \(\) on \(^{(e)}\)
15:\(^{(e)}_{r}\) Update neural router \(^{(e-1)}_{r}\) with the gradient \(_{^{(e-1)}_{r}}\) using Eq. (6)
16:endfor ```

**Algorithm 1** Collaborative Neural Framework for VRPs

### Inner Maximization

The inner maximization aims to generate adversarial instances for the training in the outer minimization, which should be 1) effective in attacking the framework; 2) diverse to benefit the policy exploration for VRPs. Typically, an iterative attack method generates local adversarial instances for each model only based on its own parameter (e.g., the same \(\) in Eq. (3) is repetitively used throughout the generation). Such _local attack_ (line 6) only focuses on degrading each individual model, failing to consider the ensemble effect of multiple models. Due to the existence of multiple models in CNF, we are motivated to further develop a general form of local attack - _global attack_ (line 7-8), where each adversarial instance can be generated using different model parameters within \(T\) generation steps. Concretely, given each clean instance \(x\), we generate the global adversarial instance \(\) by attacking the corresponding _best-performing model_ in each iteration of the inner maximization. In doing so, compared with the sole local attack, the generated adversarial instances are more diverse to successfully attack the models (\(\) (see Appendix A for further discussions). Without loss of generality, we take the attacker from  as an example, which directly maximizes the variant of the reinforcement loss as follows:

\[(x;)= p_{}(|x),\] (4)

where \(b()\) is the baseline function (as shown in Eq. (1)). On top of it, we generate the global adversarial instance \(\) such that:

\[^{(t+1)}=_{}[^{(t)}+_{^{( t)}}(^{(t)};^{(t)}_{b})],^{(t)}_{b}=_{ }c(|^{(t)};),\] (5)

where \(^{(t)}\) is the global adversarial instance and \(^{(t)}_{b}\) is the best-performing model (w.r.t. \(^{(t)}\)) at step \(t\). If \(^{(t)}\) is out of the range, it would be projected back to the valid domain \(\) by \(\), such as the min-max normalization for continuous variables (e.g., node coordinates) or the rounding operation for discrete variables (e.g., node demands). We discard the sign operator in Eq. (3) to relax the imperceptible constraint. More details are presented in Appendix B.1.

In summary, the local attack is a special case of the global attack, where the same model is chosen as \(_{b}\) in each iteration. While the local attack aims to degrade a single model \(\), the global attack can be viewed as explicitly attacking the collaborative performance of all models \(\), which takes into consideration the ensemble effect by attacking \(_{b}\). In CNF, we involve adversarial instances that are generated by both the local and global attacks to pursue better adversarial robustness, while also including clean instances to preserve standard generalization.

### Outer Minimization

After the adversarial instances are generated by the inner maximization, we collect a set of instances \(\) with \(||=N\), which includes clean instances \(x\), local adversarial instances \(\) and global adversarial instances \(\), to train \(M\) models. Here a key problem is that _how are the instances distributed to models for their training, so as to achieve satisfactory load balancing (training efficiency) and collaborative performance (effectiveness)?_ To solve this, we design an attention-based neural router, and jointly train it with all models \(\) to maximize the improvement of collaborative performance.

Concretely, we first evaluate each model on \(\) to obtain a cost matrix \(^{N M}\). The attention-based neural router \(_{r}\) takes as inputs the instances \(\) and \(\), and outputs a logit matrix \(f_{_{r}}(,)=^{N M}\), where \(f\) is the decision function. Then, we apply Softmax function along the first dimension of \(\) to obtain the probability matrix \(}\), where the entity \(}_{ij}\) represents the probability of the \(i_{}\) instance being selected for the outer minimization of the \(j_{}\) model. For each model, the neural router distributes the instances with Top\(\)-largest predicted probabilities as a batch for its training (line 10-13). In doing so, all models have the same amount (\(\)) of training instances, which explicitly ensures the _load balancing_ (see Appendix A). We also discuss other strategies of instance distributing, such as sampling, instance-based choice, etc. More details can be found in Section 5.2.

After all models \(\) are trained with the distributed instances, we further evaluate each model on \(\), obtaining a new cost matrix \(^{}^{N M}\). To pursue desirable _collaborative performance_, it is expected that the neural router \(_{r}\) can reasonably exploit the overall capacity of models. Since the action space of \(_{r}\) is huge and the models \(\) are changing throughout the training, we resort to the reinforcement learning (based on trial-and-error) to optimize parameters of the neural router \(_{r}\) (line 14-15). Specifically, we set \((-^{})\) as the reward signal, and update \(_{r}\) by gradient ascent to maximize the expected return with the following approximation:

\[_{_{r}}(_{r}|)=_{j[1,M ],i(}_{j}),}}[( -^{})_{i}_{_{r}}}_{ij}],\] (6)

where the \(\) operator is applied along the last dimension of \(\) and \(^{}\), since we would like to maximize the improvement of collaborative performance after training with the selected instances. Intuitively, if an entity in \((-^{})\) is positive, it means that, after training with the selected instances, the collaborative performance of all models on the corresponding instance is increased. Thus, the corresponding action taken by the neural router should be reinforced, and vice versa. For example, in Fig. 2, if the reward entity for the first instance \(x_{1}\) is positive, the probability of this action (i.e., the red one in the first row of \(\)) will be reinforced after optimization. Note that the unselected (e.g., black ones) will be masked out in Eq. (6). In doing so, the neural router learns to effectively distribute instances and reasonably exploit the overall model capacity, such that the collaborative performance of all models can be enhanced after optimization. Further details on the neural router structure and the in-depth analysis of the learned routing policy are presented in Appendix C.

## 5 Experiments

In this section, we empirically verify the effectiveness and versatility of CNF against attacks specialized for VRPs, and conduct further analyses to provide the underlying insights. Specifically, our experiments focus on two attack methods [87; 42], since the accuracy-robustness trade-off exists when conducting vanilla AT to defend against them. We conduct the main experiments on POMO  with the attacker in , and further demonstrate the versatility of the proposed framework on MatNet  with the attacker in . More details on the experimental setups, data generation and additional empirical results (e.g., evaluation on large-scale instances) are presented in Appendix D. All experiments are conducted on a machine with NVIDIA V100S-PCIE cards and Intel Xeon Gold 6226 CPU at 2.70GHz. The source code is available at https://github.com/RoyalSkye/Routing-CNF.

**Baselines.** 1) _Traditional methods:_ we solve TSP instances by Concorde and LKH3 , and CVRP instances by hybrid genetic search (HGS)  and LKH3. 2) _Neural methods:_ we compare our method with the pretrained base model POMO (\(\)1M parameters), and its variants training with various defensive methods, such as the vanilla adversarial training (POMO_AT), the defensive method 

[MISSING_PAGE_FAIL:8]

over, for neural methods with multiple (\(M\)) models (e.g., CNF (3)), we develop an implementation of parallel evaluation on multiple GPUs, which can further reduce their inference time by almost \(M\) times. From the results, we observe that 1) traditional VRP methods are relatively more robust than neural methods against crafted perturbations, demonstrating the importance and necessity of improving adversarial robustness for neural methods; 2) the evaluation metrics of Fixed Adv. and Adv. are almost consistent in the context of VRPs; 3) our method consistently outperforms baselines, and achieves high standard generalization and adversarial robustness concurrently. For CNF, we show the performance of each model on TSP100 in Fig. 5. Although not all models excel well on both clean and adversarial instances, the collaborative performance is quite good, demonstrating diverse expertise of models and the capability of CNF in reasonably exploiting the overall model capacity.

### Ablation Study

We conduct extensive ablation studies on TSP100 to demonstrate the effectiveness and sensitivity of our method. Note that the setups are slightly different from the training ones (e.g., half training instances). The detailed results and setups are presented in Fig. 3 and Appendix D.1, respectively.

**Ablation on Components.** We investigate the role of each component in CNF by removing them separately. As demonstrated in Fig. 3(a), despite both components contribute to the collaborative performance, the neural router exhibits a bigger effect due to its favorable potentials to elegantly exploit training instances and model capacity, especially in the presence of multiple models.

**Ablation on Hyperparameters.** We investigate the effect of the number of trained models, which is a key hyperparameter of our method, on the collaborative performance. The results are shown in Fig. 3(b), where we observe that increasing the number of models can further improve the collaborative performance. However, we use \(M=3\) in the main experiments due to the trade-off between empirical performance and computational complexity. We refer to Appendix D.4 for more results.

**Ablation on Routing Strategies.** We further discuss different routing strategies, including neural and heuristic ones. Specifically, given the logit matrix \(\) predicted by the neural router, there are various ways to distribute instances: 1) _Model choice with Top\(\) (M-Top\(\)):_ each model chooses potential instances with Top\(\)-largest logits, which is the default strategy (\(=B\)) in CNF; 2) _Model choice with sampling (M-Sample):_ each model chooses potential instances by sampling from the probability distribution (i.e., scaled logits); 3-4) _Instance choice with Top\(\)/sampling (I-Top\(\)/I-Sample):_ in contrast to the model choice, each instance chooses potential model(s) either by Top\(\) or sampling. The probability matrix \(}\) is obtained by taking Softmax along the first and last dimension of \(\) for model choice and instance choice, respectively. Unlike the model choice, instance choice cannot guarantee load balancing. For example, the majority of instances may choose a dominant model (if exists), leaving the remaining models underfitting and therefore weakening the ensemble effect and collaborative performance; 5) _Random:_ instances are randomly distributed to each model; 6) _Self-training:_ each model is trained on adversarial instances generated by itself without instance distributing. The results in Fig. 3(c) show that M-Top\(\) performs the best.

### Out-of-Distribution Generalization

In contrast to other domains (e.g., vision), the set of valid problems is not just a low-dimensional manifold in a high-dimensional space, and hence the manifold hypothesis  does not apply to VRPs (or COPs). Therefore, it is critical for neural methods to perform well on adversarial instances when striving for a broader out-of-distriburion (OOD) generalization in VRPs. In this section, we

Figure 3: Ablation studies on TSP100. The metrics of _Uniform_ and _Fixed Adv._ are reported.

further evaluate the OOD generalization performance on unseen instances from both synthetic and benchmark datasets. The empirical results demonstrate that raising robustness against attacks through CNF can favorably promotes various forms of generalization, indicating the potential existence of neural VRP solvers with high generalization and robustness concurrently. The data generation and comprehensive results can be found in Appendix D.

**Synthetic Datasets.** We consider three generalization settings, i.e., cross-distribution, cross-size, and cross-size & distribution. The results are shown in Table 2, from which we observe that simply conducting the vanilla AT somewhat hurts the OOD generalization, while CNF can significantly improve it. Since adversarial robustness is known as a kind of local generalization property [22; 48], the improvements in OOD generalization can be viewed as a byproduct of defending against adversarial attacks and balancing the accuracy-robustness trade-off.

**Benchmark Datasets.** We further evaluate all neural methods on the real-world benchmark datasets, such as TSPLIB  and CVRPLIB . We choose representative instances within the range of \(n\). The results, presented in Tables 9 and 10, demonstrate that our method performs well across most instances. Note that all neural methods are only trained on \(n=100\).

### Versatility Study

To demonstrate the versatility of CNF, we extend its application to MatNet  to defend against another attacker in . Specifically, it constructs the adversarial instance by lowering the cost of a partial clean asymmetric TSP (ATSP) instance. When adhering to the vanilla AT, the undesirable trade-off is also observed in the empirical results of . In contrast, our method enables models to achieve both high standard generalization and adversarial robustness. The detailed attack method, training setups, and empirical results are presented in Appendix B.3, D.1 and D.3, respectively.

## 6 Conclusion

This paper studies the crucial yet underexplored adversarial defense of neural VRP methods, filling the gap in the current literature on this topic. We propose an ensemble-based collaborative neural framework to concurrently enhance performance on clean and adversarial instances. Extensive experiments demonstrate the effectiveness and versatility of our method, highlighting its potential to defend against attacks while promoting various forms of generalization of neural VRP methods. Additionally, our work can be viewed as advancing the generalization of neural methods through the lens of adversarial robustness, shedding light on the possibility of building more robust and generalizable neural VRP methods in practice.

The limitation of this work is the increased computational complexity due to the need to synergistically train multiple models. Fortunately, based on experimental results, CNF with just three models has already achieved commendable performance. It even surpasses vanilla AT trained with nine models, demonstrating a better trade-off between empirical performance and computational complexity.

Interesting future research directions may include: 1) designing efficient and effective attack or defense methods for other COPs; 2) pursuing better robustness with fewer computation, such as through conditional computation and parameter sharing; 3) theoretically analyzing the robustness of neural VRP methods, such as certified robustness; 4) investigating the potential of large language models to robustly approximate optimal solutions to COP instances.

    &  &  &  \\  & Rotation (100) & Explosion (100) & Uniform (50) & Uniform (200) &  &  &  \\  & Gap & Time & Gap & Time & Gap & Time & Gap & Time & Gap & Time & Gap & Time \\  Concorde & 0.000\% & 0.3m & 0.000\% & 0.3m & 0.000\% & 0.2m & 0.000\% & 0.6m & 0.000\% & 0.6m \\ LKIFs & 0.000\% & 1.2m & 0.000\% & 1.2m & 0.000\% & 0.4m & 0.000\% & 3.9m & 0.000\% & 3.3m & 0.000\% & 3.5m \\  POMO (1) & 0.471\% & 0.1m & 0.238\% & 0.1m & 0.064\% & 0.1m & 1.658\% & 0.5m & 2.936\% & 0.5m & 2.587\% & 0.5m \\ POMO\_AT (1) & 0.640\% & 0.1m & 0.364\% & 0.1m & 0.151\% & 0.1m & 2.667\% & 0.5m & 3.462\% & 0.5m & 2.989\% & 0.5m \\ POMO\_AT (3) & 0.508\% & 0.3m & 0.263\% & 0.3m & 0.085\% & 0.1m & 2.362\% & 1.5m & 3.176\% & 1.5m & 2.688\% & 1.5m \\ POMO\_TA (3) & 0.204\% & 0.3m & 0.107\% & 0.3m & 0.038\% & 0.1m & 1.414\% & 1.5m & 2.184\% & 1.5m & 1.718\% & 1.5m \\ POMO\_DivTrain (3) & 0.502\% & 0.3m & 0.255\% & 0.3m & 0.078\% & 0.1m & 2.356\% & 1.5m & 3.176\% & 1.5m & 2.707\% & 1.5m \\  CNF (3) & **0.193\%** & 0.3m & **0.084\%** & 0.3m & **0.036\%** & 0.1m & **1.383\%** & 1.5m & **2.055\%** & 1.5m & **1.672\%** & 1.5m \\   

Table 2: Generalization evaluation on synthetic TSP datasets. Models are only trained on \(n=100\).