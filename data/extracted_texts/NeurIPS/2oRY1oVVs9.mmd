# Incentivized Exploration in Two-sided Matching Markets

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We study _incentivized exploration_ (IE) in centralized two-sided matching markets where all agents and arms are myopic human decision-subjects with preferences over their potential matches. The platform can leverage information asymmetry to encourage all sequentially arriving agents and arms to explore alternative options. In particular, we use inverse-gap weighting, a technique studied in reinforcement learning and contextual bandits, as the theoretical underpinning for our novel recommendation policy. We obtain the first set of results for incentivized exploration in two-sided matching markets with dual incentive-compatibility constraints and asymptotically match the regret guarantee for combinatorial semi-bandits.

## 1 Introduction

Consider an online job market where job applicants seek to get matched with employers in a one-to-one format, i.e., each job opening only accepts a single candidate. Each job applicant has their preference over which position they want to work in to utilize their skill set best. Similarly, employers want to match with candidates with well-documented track records who they can trust to perform well in the new job. This is a canonical example of the one-to-one matching problem studied by Gale and Shapley (1962). While preference matching is ubiquitous, it may lead to self-imposed bias where job applicants only seek out employers they know beforehand, ignoring other options on the market. At the same time, employers also suffer from a lack of exploration as they are more favorable to prominent job applicants instead of expanding their search for the most suitable candidates. Moreover, in a large market, it is improbable that an employer can form an accurate preference ordering over job applicants without interacting with them first. Our goal is to _incentivize exploration_ in a centralized matching market, where the platform provides recommendations for either the job applicants or the employees to explore alternative options. Such exploration is crucial to any learning algorithm that seeks to find the optimal matching in two-sided markets.

Overview of results.Our main contributions are as follows:

1. Prior work in incentivized exploration only considers the agents' incentives. Instead, this work considers the incentive-aware exploration problem in an online matching market from the perspectives of both agents and arms. See Appendix B for a detailed motivation.
2. We provide an end-to-end BIC algorithm with two components: 'warm-start' and accelerated exploration. Particularly, we develop a novel recommendation policy based on the inverse-gap weighting technique to accelerate exploration with near-optimal regret guarantees.
3. We provide numerical simulation on synthetic data and show that our end-to-end algorithm is both 1) incentive-compatible and 2) efficient in terms of regret minimization.

Preliminary

Notation.We write \([K]=\{1,2,,K\}\) for \(K^{+}\). We use subscripts \(i,j\) to denote different agents or arms, and superscript \(t[T]\) to denote different time-steps.

We focus on an online two-sided matching market with time horizon \(T\). At time-step \(t[T]\), a fresh batch of \(N\) agents and \(N\) arms arrive and form \(N\) one-to-one matches. If they successfully match with some arms, the agents (and arms) report their shared utility to the platform and leave.

Reward formulation and Bayesian priors.We assume that the reward of each successful match is a bilinear function of the agent and the arm's profiles. Concretely, at time-step \(t\), each agent of type \(i\) has their user profile \(x_{i}^{(t)}^{d}\). Similarly, each arm of type \(j\) has profile vector \(a_{j}^{(t)}^{d}\). Let \(^{d d}\) be a latent matrix with rank \(r<d\). Then, the realized reward of a match (type \(i\) agent, type \(j\) arm) is:

\[r_{i,j}^{(t)}=r^{(t)}(x_{i}^{(t)},a_{j}^{(t)})(x_{i}^{(t)})^{}  a_{j}^{(t)}+_{i,j}^{(t)}\] (1)

where \(_{i,j}^{(t)}()\). We write \(_{i.j}=x_{i}^{} a_{j}\) to denote the expected reward of a match between agents of type \(i\) and arms of type \(j\), and \(_{i,j}^{(0)}\) to denote the prior-mean reward. Wlog, we assume that \( i,j:_{i,j}\). Henceforth, we write \(x_{i}\) and \(a_{j}\) to refer to agents of type \(i\) and arms of type \(j\).

Preferences.We focus on the stylized setting with two types of agents and arms. Let \(i,j\) denote the type of agents and arms, respectively. We are interested in two sets of preferences: agent-to-arm and arm-to-agent. In our motivating example, job applicants want to be matched with compatible employers and employers prefer to be matched with applicants who can perform well. Wlog, we assume that the initial preference ordering is \(_{1,1}^{(0)}_{1,2}^{(0)}_{2,2}^{(0)}\) and \(_{1,1}^{(0)}_{2,1}^{(0)}_{2,2}^{(0)}\). That is, all agents prefer type \(1\) arms to type \(2\) arms, and all arms prefer type \(1\) agents to type \(2\) agents.

Incentive-compatibility.Absent incentives and coordination from the platform, the agents and arms match each other using their initial preferences. However, the platform wants to incentivize both the agents and the arms to explore different options to find the optimal matching and maximize the cumulative reward. In particular, at each time step \(t\), the platform can broadcast a signal \(^{(t)}\) as a recommendation to all agents and arms. By _direct revelation principle_, this signal is equivalent to directly telling the agents which arm to match with, and vice versa.

**Definition 2.1** (Two-sided Bayesian Incentive-Compatible Condition).: \( t[T]\)_, the platform's recommendation is \(-\)two-sided Bayesian Incentive-Compatible (\(\)-BIC) for some \(>0\) if it satisfies:_

\[[r_{i,j}^{(t)}|=(x_{i}^{(t)},a_{j}^{(t)})]- _{[N]}[r_{i,}^{(t)}|=(x_{i}^{(t)},a_{j}^ {(t)})]\] (2) \[[r_{i,j}^{(t)}|=(x_{i}^{(t)},a_{j}^{(t)})]- _{[N]}[r_{,j}^{(t)}|=(x_{i}^{(t)},a_{j}^ {(t)})]\] (3)

**Assumption 2.2** (Behavioral Assumption).: _Agents and arms follow recommendations for any \(_{0}\)-BIC policy, for some fixed \(_{0}>0\). If one side rejects the recommendation, then both sides of the recommended (agent, arm) pair do have a match for that time-step and the platform receives a reward of \(0\) for that recommended pair. Both the agents and the arms are assumed to be myopic, i.e., they will choose the posterior best arms (agents) at the current time-step to match with._

Reduction to combinatorial semi-bandits.Our first insight is to reduce the two-sided matching problem to a combinatorial semi-bandits problem. Consider the following mapping: at each time-step, the set of all feasible matches between agents and arms constitutes the action space \(^{N N}\). An _atom_\((x_{i}^{(t)},a_{j}^{(t)})\) is a match between \(x_{i}^{(t)}\) and \(a_{j}^{(t)}\), and there are \(N^{2}\) total atoms. An _action_\(A^{(t)}\) at time-step \(t\) is the combination of matches at that round, where \(\|A^{(t)}\|_{1} N\). At each time-step \(t\), a learner arrives at the platform, receives a recommendation for an action \(A\), and chooses an action \(A^{(t)}\). The platform and the learner both observe the reward of each atom in this arm (and nothing else). The algorithm's reward in this time-step is the total reward of these atoms.

Under this reduction, a few technical challenges differentiate our result from that of combinatorial semi-bandits. Particularly, it is unclear how to collect the 'warm-start' samples, which are input to any efficient incentivized exploration algorithm. For a detailed explanation, see Appendix B.

Incentivized exploration for two agents and two arms

In this section, we focus on the fundamental special case of incentivized exploration with two types of agents and arms to show the salient points of our analysis. In essence, the platform first incentivizes all agents and arms to match each other and collect samples from these matches. Then, the platform use these 'warm-start' samples to accelerate exploration and quickly converge to the optimal matching.

### Initial exploration with Hidden Exploration

We present our first contribution, a BIC algorithm to collect the 'warm-start' samples, where the objective is to sample each atom, i.e., match between an agent and an arm, at least once and completes in \(T_{0}\) time-steps for some \(T_{0}\) determined by the prior. In the following algorithm, we show that in the 'worst case' with one 'explorable' atom initially, we can incentivize both the agents and the arms to explore different matches. Intuitively, given enough samples of the 'explorable' atom, we can split the remaining time-steps into phases such that in each phase, a new atom, i.e., a match between an agent and an arm that was previously not explorable, can be chosen by the learner upon receiving the principal's recommendation. The incentivized exploration technique within each phase builds on the approach from Mansour et al. (2015), which is defined for multi-armed bandits. However, the reward priors are highly correlated in two-sided matching markets, and the set of 'explorable' atoms can initially be of size \(1\). Furthermore, the intricate incentive interplay between agents and arms requires a more careful notion of which action to explore. Our technical contribution here is to provide a sequence of actions and prove that it is possible to incentivize both the agents and the arms to explore given some mild conditions on the posterior distribution of the reward for each atom.

We make the following non-degeneracy assumption: any action \(A_{}\) can be the posterior best action with a margin \(_{}\) and probability at least \(_{}\) after seeing at least \(n_{}\) samples of the previous actions.

**Assumption 3.1** (Fighting chance assumption).: _There exists number \(n_{}\) and \(_{},_{}(0,1)\) determined by the prior \(\) such that: for a sequence of actions \(A_{}^{1},,A_{}^{N^{2}}\) defined by \((,,)\). Let \(\) be the dataset containing exactly \(k\) samples of each arm, then_

\[[X_{i}^{k}_{}]_{} i k n_{},\] (4)

_where \(X_{i}^{k}=_{A A_{}}[_{A_{ }}-_{A}|]\)_

We state our initial sampling algorithm in Algorithm 1 and its theoretical guarantees in Theorem 3.2.

```
0: Batch size \(L\), target number of samples \(k\), gap \(C(0,1)\).
1: Initialize dataset \(=\);
2: The first \(k\) learners choose \(A=\{(x_{1},a_{1})\}\) without recommendations. Let \(_{1,1}^{k}\) be the sample average of these rewards. Add these \(k\) samples to \(\);
3:for each phase \(=1\) to \(N^{2}\)do
4:\(A_{}^{()}=(,, )\);
5:if\(_{1,1}^{k}_{A_{}^{(0)}}^{(0)}-C\)then
6: 'Exploit' action \(A^{*}=A_{}^{}\).
7:else
8: 'Exploit' action \(A^{*}=\{(x_{1},a_{1})\}\).
9: From the set \(P\) of the next \(L k\) learners, pick a set \(Q\) of \(k\) learners uniformly at random;
10: Every learner \(p P-Q\) is recommended the 'exploit' action \(A^{*}\);
11: Every learner \(p Q\) is recommended action \(A_{}\). Add the reward from all \(p Q\) to \(\). ```

**Algorithm 1**Initial sampling: Hidden Exploration

**Theorem 3.2**.: _Assuming Assumption 3.1 holds with constants \(n_{},_{},_{}\). Then, Algorithm 1 is two-sided \(\)-BIC as long as the batch size \(L\) is at least_

\[L 1+\{}_{}-2 },^{(0)}+_{2,1}^{(0)}-_{2,2}^{(0)}+ [_{A^{0},A_{2,2}}^{k}|_{3}][_{3}]-2}\}\] (5)

_and completes in \(T_{0}=N^{2} n_{}}{_{} _{}}\) time-steps. All actions are sampled at least \(n_{}\) times._

### Accelerated Exploration with Inverse Gap Weighting

Given the data collected by Algorithm 1, the platform wants to accelerate exploration and converge to the optimal matching. The platform has to balance _exploitation_, i.e., recommending the empirical best match to minimize regret, and _exploration_, i.e., ensuring that the two-sided BIC condition holds. The theoretical underpinning of our recommendation policy at this stage is _inverse gap weighting_, i.e., recommending a match with probability inversely proportional to the reward gap between that match and the empirical best match. Formally, we let \(b^{(t)}=*{argmax}_{A}_{A}^{(t)}\) denote the empirical best action at time-step \(t\). Then, the probability of an action \(A\) being recommended at time-step \(t\) is: \(p_{A}^{(t)}=+(_{b^{(t)}}^{(t)}-_ {A}^{(t)})}&A b^{(t)}\\ 1-_{A b^{(t)}}p_{A}^{(t)}&\), where the hyperparameter \(>0\) shows the tradeoff between exploration and exploitation. A smaller \(\) leads to more exploration, while a larger \(\) induces more exploitation. To ensure that \(\) is adaptive to the samples collected, we set \(=C_{0} N}\), where \(^{(t)}\) is the mean squared error of the prediction at time-step \(t\). Similar to Foster and Rakhlin (2020), we assume there exists an efficient regression-oracle that accurately compute \(^{(t)}\) at time-step \(t\). With this recommendation policy, we state the theoretical guarantee for accelerated exploration:

**Theorem 3.3** (Informal).: _Given sufficiently many 'warm-start' samples of all atoms, the inverse gap weighting recommendation policy is two-sided \(\)-BIC. The total regret during this stage is \(O(N)\), which asymptotically matches the optimal regret of combinatorial semi-bandits._

## 4 Numerical Simulations

In this section, we complement our theoretical results with an experiment (Figure 1) to show incentive compatibility and regret minimization of our combined algorithm. For details, see Appendix D.

## 5 Conclusion and Future Work

In this work, we present the first results for incentivized exploration in two-sided matching markets, where the agents and arms are individuals with preferences over their matches. We characterize the incentive-compatibility constraints and provide a reduction to combinatorial semi-bandits. With this reduction, we present a BIC algorithm that collects 'warm-start' samples and accelerates exploration to minimize regret. In the future, we want to extend this work in several directions. First, we want to analyze the setting with more than two types of agents and arms. Moreover, we are working on experiments using synthetic and real-world datasets to support our theoretical findings.

Figure 1: Regret using Algorithm 1 and Inverse Gap Weighting with time horizon \(T=20000\). Results are averaged over 10 runs, with the shaded region representing one standard error.