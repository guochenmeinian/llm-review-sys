# Nearly Tight Black-Box Auditing of Differentially Private Machine Learning

Meenachi Sundaram Muthu Selva Annamalai

University College London

meenatchi.annamalai.22@ucl.ac.uk

Emiliano De Cristofaro

University of California, Riverside

emilianodec@cs.ucr.edu

###### Abstract

This paper presents an auditing procedure for the Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm in the black-box threat model that is substantially tighter than prior work. The main intuition is to craft worst-case initial model parameters, as DP-SGD's privacy analysis is agnostic to the choice of the initial model parameters. For models trained on MNIST and CIFAR-10 at theoretical \(=10.0\), our auditing procedure yields empirical estimates of \(_{emp}=7.21\) and \(6.95\), respectively, on a 1,000-record sample and \(_{emp}=6.48\) and \(4.96\) on the full datasets. By contrast, previous audits were only (relatively) tight in stronger white-box models, where the adversary can access the model's inner parameters and insert arbitrary gradients. Overall, our auditing procedure can offer valuable insight into how the privacy analysis of DP-SGD could be improved and detect bugs and DP violations in real-world implementations. The source code needed to reproduce our experiments is available from https://github.com/spalabucr/bb-audit-dpsgd.

## 1 Introduction

Differentially Private Stochastic Gradient Descent (DP-SGD)  allows training machine learning models while providing formal Differential Privacy (DP) guarantees . DP-SGD is a popular algorithm supported in many open-source libraries like Opacus , TensorFlow , and JAX . Progress in privacy accounting and careful hyper-parameter tuning has helped reduce the gap in model performance compared to non-private training . Furthermore, pre-training models on large public datasets and then fine-tuning them on private datasets using DP-SGD can also be used to improve utility .

Implementing DP-SGD correctly can be challenging - as with most DP algorithms [5; 26] - and bugs that lead to DP violations have been found in several DP-SGD implementations [7; 22]. These violations do not only break the formal DP guarantees but can also result in realistic privacy threats [17; 33]. This prompts the need to audit DP-SGD implementations to verify whether the _theoretical_ DP guarantees hold in practice. This usually involves running membership inference attacks , whereby an adversary infers whether or not a sample was used to train the model. The adversary's success estimates the _empirical_ privacy leakage from DP-SGD, and that is compared to the theoretical DP bound. If the former is _"far"_ from the latter, it means that the auditing procedure is _loose_ and may not be exploiting the maximum possible privacy leakage. Whereas, when empirical and theoretical estimates are _"close,"_ auditing is _tight_ and can be used to identify bugs and violations .

Although techniques to tightly audit DP-SGD exist in literature [28; 29], they do so only in _active white-box_ threat models, where the adversary can observe and insert arbitrary gradients into the intermediate DP-SGD steps. By contrast, in the more restrictive _black-box_ threat model, where an adversary can only insert an input canary and observe the final trained model, prior work has only achieved loose audits [20; 28; 29]. In fact, prior work has suggested that the privacy analysis of DP-SGD in the black-box threat model can be tightened but only in limited settings (i.e., strongly convex smooth loss functions) [3; 9; 39]. Therefore, it has so far remained an open research question  whether or not it is possible to tightly audit DP-SGD in the black-box model.

In this paper, we introduce a novel auditing procedure that achieves substantially tighter black-box audits of DP-SGD than prior work, even for _natural_ (i.e., not adversarially crafted) datasets. The main intuition behind our auditing procedure is to craft worst-case _initial model parameters_, as DP-SGD's privacy analysis is agnostic to the choice of initial model parameters. We show that the gap between worst-case empirical and theoretical privacy leakage in the black-box threat model is much smaller than previously thought [20; 28; 29]. When auditing a convolutional neural network trained on MNIST and CIFAR-10 at theoretical \(=10\), our audits achieve an empirical privacy leakage estimate of \(_{emp}=6.48\) and \(4.96\), respectively, compared to \(_{emp}=3.41\) and \(0.69\) when the models were initialized to average-case initial model parameters, as done in prior work [11; 20; 28].

In the process, we identify and analyze the key factors affecting the tightness of black-box auditing: 1) dataset size and 2) gradient clipping norm. Specifically, we find that the audits are looser for larger datasets and larger gradient clipping norms. We believe this is due to more noise, which reduces the empirical privacy leakage. When considering smaller datasets with 1,000 samples, our audits are tighter - for \(=10.0\), we obtain \(_{emp}=7.21\) and \(6.95\), respectively, for MNIST and CIFAR-10.

Finally, we present tight audits for models with only the last layer fine-tuned using DP-SGD. Specifically, we audit a 28-layer Wide-ResNet model pre-trained on ImageNet-32  with the last layer privately fine-tuned on CIFAR-10. With \(=10.0\), we achieve an empirical privacy leakage estimate of \(_{emp}=8.30\) compared to \(_{emp}=7.69\) when the models are initialized to average-case initial parameters (as in previous work [11; 20; 28]).

Overall, the ability to perform rigorous audits of differentially private machine learning techniques allows shedding light on the tightness of theoretical guarantees in different settings. At the same time, it provides critical diagnostic tools to verify the correctness of implementations and libraries.

## 2 Related Work

**DP-SGD.** Abadi et al.  introduce DP-SGD and use it to train a feed-forward neural network model classifying digits on the MNIST dataset. On the more complex CIFAR-10 dataset, they instead fine-tune a pre-trained convolutional neural network, incurring a significant performance loss. Tramer and Boneh  improve on  using hand-crafted feature extraction techniques but also suggest that end-to-end differentially private training may be inherently difficult without additional data. Papernot et al.  and Dormann et al.  also present improvements considering different activation functions and larger batch sizes. Finally, De et al.  achieve state-of-the-art model performance on CIFAR-10 both while training from scratch and in fine-tuning regimes; they do so by using large batch sizes, augmenting training data, and parameter averaging. Our work focuses on auditing realistic models with high utility that might be used in practice; thus, we audit models with utility similar to  and the same models pre-trained in  in the fine-tuning regime.

**Membership Inference Attacks (MIAs).** Shokri et al.  propose the first black-box MIAs against machine learning models using shadow models. Carlini et al.  improve by fitting a Gaussian distribution on the loss of the target sample and using fewer models to achieve equivalent attack accuracies. Sablayrolles et al.  consider per-example thresholds to improve the performance of the attacks in a low false-positive rate setting . By factoring in various uncertainties in the MIA "game," Ye et al.  construct an optimal generic attack. Concurrent work [16; 25; 37] use poisoned pre-trained models, which is somewhat similar to our approach of using worst-case initial model parameters, but with a different focus. Specifically, Liu et al.  and Wen et al.  show that privacy leakage from MIAs is amplified when pre-trained models are poisoned but, crucially, do not consider DP auditing. Additionally, while Feng and Tramer  achieve tight audits with poisoned pre-trained models, their strategy is geared towards the last-layer only fine-tuning regime. By contrast,we focus on auditing DP-SGD with full model training to estimate the empirical worst-case privacy leakage in the black-box setting.

**Auditing DP-SGD.** Jayaraman and Evans  derive empirical privacy guarantees from DP-SGD, observing a large gap from the theoretical upper bounds. Jagielski et al.  use data poisoning in the black-box model to audit Logistic Regression and Fully Connected Neural Network models and produce tighter audits than using MIAs. They initialize models to fixed (average-case) initial parameters, but the resulting empirical privacy guarantees are still far from the theoretical bounds. They also adapt their procedure to the fine-tuning regime, again producing loose empirical estimates.

Nasr et al.  are the first to audit DP-SGD tightly; they do so by using adversarially crafted datasets and _active white-box_ adversaries that insert canary gradients into the intermediate steps of DP-SGD. In follow-up work,  presents a tight auditing procedure also for natural datasets, again in the _active white-box_ model. Zanella-Beguelin et al.  present tighter auditing procedures using Bayesian techniques, needing fewer models to audit larger privacy levels. Finally, De et al.  perform black-box auditing of their implementation of DP-SGD in the JAX framework ; they also use fixed (average-case) initialization of models as in , similarly achieving loose audits.

Overall, while prior work [11; 20; 29] showed that reducing the randomness in the initial model parameters results in tighter empirical privacy leakage estimates, the audits remained loose as the fixed model parameters were still initialized _randomly_. Specifically, designing an effective adversarial strategy that provides tight audits in the black-box setting without destroying utility has remained, to the best of our knowledge, an open problem.

## 3 Background

### Differential Privacy (DP)

**Definition 1** (Differential Privacy (DP) ).: A randomized mechanism \(:\) is \((,)\)-differentially private if, for any two neighboring datasets \(D,D^{}\) and \(S\), it holds:

\[[(D) S] e^{}[(D^{}) S]+\]

Put simply, DP guarantees a formal upper bound (constrained by the privacy parameter \(\)) on the probability that any adversary observing the output of the mechanism \(\) can distinguish between two _neighboring_ inputs to \(\) - i.e., two datasets differing in only one record.

### Differentially Private Machine Learning (DPML)

**DP-SGD.** Differentially Private Stochastic Gradient Descent (DP-SGD)  is a popular algorithm for training machine learning models while satisfying DP. In this paper, we consider classification tasks on samples from the domain \(\), where \(\) is the features domain and \(\) is the labels domain.

DP-SGD, reviewed in Algorithm 1, introduces two hyper-parameters: gradient clipping norm \(C\) and noise multiplier \(\). Typically, \(C\) is set to 1, while \(\) is calibrated to the required privacy level \((,)\) based on the number of iterations, \(T\), and batch size, \(B\). Early DP-SGD work  set batch sizes to be small and similar to those of non-private SGD; however, recent work has highlighted the advantage of setting large batch sizes [11; 14] by training state-of-the-art models with them. Following this trend, and to ease auditing, we set \(B=N\), as also done in prior work . Finally, while the initial set of model parameters \(_{0}\) is typically sampled randomly (e.g., using Xavier initialization ), it can also be set arbitrarily (and even adversarially), without affecting the privacy guarantees provided by DP-SGD.

**Fine-tuning regime.** In this setting, one first pre-trains a model on a (large) public dataset and then fine-tunes it on the private data. When a suitable public dataset is available, prior work has shown that fine-tuning can achieve much higher utility than training from scratch using DP-SGD .

There are two methods for fine-tuning: doing it for all layers or only the last one. In both cases, the initial model parameters, \(_{0}\), are set to the pre-trained model parameters (rather than randomly) _before_ running DP-SGD. However, while the former updates all model parameters during DP-SGD, with the latter, the model parameters until the last (fully connected) layer are "frozen" and not updated during DP-SGD. This setting treats the pre-trained model as a feature extractor and trains a LogisticRegression model using DP-SGD based on the extracted features. As prior work shows  that this setting produces models with accuracy closer to the non-private state of the art (while also incurring smaller computational costs), we also do fine-tuning of just the last layer.

### DP Auditing

As mentioned above, DP provides a theoretical limit on the adversary's ability - bounded by the privacy parameter \(\) - to distinguish between \((D)\) and \((D^{})\). With auditing, one assesses this limit empirically to verify whether the theoretical guarantees provided by \(\) are also met in practice. There may be several reasons this does not happen, e.g., the privacy analysis of \(\) is not tight  or because of bugs in the implementation of \(\)[28; 36]. Therefore, with DP mechanisms increasingly deployed in real-world settings, auditing them becomes increasingly important.

**Deriving \(_{emp}\).** When auditing a DP mechanism \(\), one runs \(\) repeatedly on two neighboring inputs, \(D\) and \(D^{}\), \(R\) times each and the outputs \((O=\{o_{1},...,o_{R}\}\) and \(O^{}=\{o^{}_{1},...,o^{}_{R}\})\) are given to an adversary \(\). Next, \(\) tries to distinguish between \(O\) and \(O^{}\), which determines a false positive rate (FPR) and false negative rate (FNR). For a given number of outputs \(2R\) and confidence level \(\), empirical upper bounds \(}\) and \(}\) can be computed using Clopper-Pearson confidence intervals [29; 28]. Finally, the empirical upper bounds are converted to an empirical lower bound \(_{emp}\) using the _privacy region_ of \(\) (see below) and \(_{emp}\) can be compared to the theoretical \(\). We consider an audit _tight_ if \(_{emp}\). To ease presentation, we abstract away the details and use \(_{emp}(,,2R,,)\) to denote the process of estimating an \(_{emp}\) from a given FPR and FNR.1

**Privacy region of DP-SGD.** Given a mechanism \(\), the privacy region of the mechanism, \(_{}(,)=\{(,)|.\}\) defines the FPR and FNR values attainable for any adversary aiming to distinguish between \((D)\) and \((D^{})\). Nasr et al.  note that the privacy region of DP-SGD corresponds (tightly) to that of \(\)-Gaussian Differential Privacy , which can be used to first compute a lower bound on \(\):

\[_{emp}=^{-1}(1-})-^{-1}(})\] (1)

Then, a lower bound on \(\) corresponds to a lower bound on \(\) by means of the following theorem:

**Theorem 1** (\(\)-Gpp to \((,)\)-DP conversion ).: A mechanism is \(\)-GDP iff it is \((,())\)-DP for all \( 0\), where:

\[()=(-+)-e ^{}(--)\] (2)

## 4 Auditing Procedure

In this section, we present our auditing procedure. We discuss the threat model in which we operate, the key intuition for using _worst-case initial parameters_, and how we craft them to maximize the empirical privacy leakage.

**Threat model.** We consider a simple _black-box_ model where the adversary only sees the final model parameters, \(_{T}\), from DP-SGD. Specifically, unlike prior work on tight audits [28; 29], the adversary cannot observe intermediate model parameters or insert canary gradients. We do so to consider more realistic adversaries that may exist in practice. Additionally, we assume that the adversary can choose a worst-case target sample as is standard for auditing DP mechanisms . Finally, we assume that the adversary can choose (possibly adversarial) initial model parameters. Note that this is not only an assumption allowed by DP-SGD's privacy analysis, but, in the context of the popular public pre-training, private fine-tuning regime , this is also a reasonable assumption.

**Auditing.** Our DP-SGD auditing procedure is outlined in Algorithm 2. As we operate in the black-box threat model, the procedure only makes a call to the DP-SGD algorithm--once again, they cannot insert canary gradients, etc. As mentioned, the adversary can set the initial model parameters (\(_{0}\)), which is fixed for all models. Finally, we assume that the final model parameters are given to the adversary, who calculates the loss of the target sample as "observations." These are then thresholded to compute FPRs and FNRs and estimate the \(_{emp}\) using the EstimateEps process (see Section 3.3). This threshold, which we denote as \(\), must be computed on a separate set of observations (e.g., a _validation_ set) for the \(_{emp}\) to constitute a technically valid lower bound. However, as any decision threshold for GDP is equally likely to maximize \(_{emp}\), we follow common practice [27; 29; 28] to find the threshold that maximizes the value of \(_{emp}\) from \(O O^{}\).

**Crafting worst-case initial parameters.** Recall that DP-SGD's privacy analysis holds not only for randomly initialized models but also for _arbitrary_ fixed parameters. Indeed, this was noted in prior work [11; 20; 28] and led to using initial model parameters that are fixed for all models. In our work, we consider _worst-case_, rather than _average-case_, initial parameters that minimize the gradients of normal samples in the dataset. This makes the target sample's gradient much more distinguishable. To do so, we pre-train the model using non-private SGD on an auxiliary dataset that follows the same distribution as the target dataset. Specifically, for MNIST, we pre-train the model on half of the full dataset (reserving the other half to be used for DP-SGD) for 5 epochs with a batch size of 32 and a learning rate of 0.01. For CIFAR-10, we first pre-train the model on the CIFAR-100 dataset for 300 epochs with batch size 128 and a learning rate of 0.1. Then, we (non-privately) fine-tune the model on half of the full dataset for 100 epochs, with a batch size of 256 and a learning rate of 0.1. Ultimately, we show that the gradients of normal samples are indeed minimized, achieving significantly tighter audits of DP-SGD in the black-box model.

```
0: Data distribution, \(\). Number of samples, \(n\). Initial model parameters, \(_{0}\). Loss function, \(\). Number of repetitions, \(2R\). Decision threshold, \(\). Confidence level, \(\).
1: Sample \(D^{n-1}\)
2: Craft target sample \((x_{T},y_{T})\)
3:\(D^{}=D\{(x_{T},y_{T})\}\)
4: Observations \(O\{\}\), \(O^{}\{\}\)
5:for\(r[R]\)do
6:\(_{r}\) DP-SGD\((D;_{0},)\)
7:\(^{}_{r}\) DP-SGD\((D^{};_{0},)\)
8:\(O[t](^{}_{r};(x_{T},y_{T}))\)
9:\(O^{}[t](^{}_{r};(x_{T},y_{T}))\)
10:endfor
11:FPR \(|\{o|o O,o\}|\)
12:FNR \(|\{o|o O^{},o<\}|\)
13:\(_{emp}\) EstimateEps(FPR, FNR, \(2R,,\))
14:return\(_{emp}\) ```

**Algorithm 2** Auditing DP-SGD

## 5 Experiments

In this section, we evaluate our auditing procedure using image datasets commonly used to benchmark differentially private classifiers and CNNs that achieve reasonable utility on these datasets.

Due to computational constraints, we only train \(2R=200\) models (100 with the target sample and 100 without) to determine the empirical privacy estimates \(_{emp}\) in the full model training setting and train \(2R=1000\) models for the last-layer only fine-tuning setting. In theory, we could achieve tighter audits by increasing the number of models, however, this would require significantly more computational resources and time (see below). Since our work focuses on the (broader) research questions of whether tight audits are possible in the black-box setting and how various factors affect this tightness, we believe \(2R=200\) provides a reasonable tradeoff between tightness and computational overhead.

In our evaluation, we report lower bounds with 95% confidence (Clopper-Pearson ) and report the mean and standard deviation values of \(_{emp}\) over five independent runs.2

### Experimental Setup

**Datasets.** We experiment with the MNIST  and CIFAR-10  datasets. The former includes 60,000 training and 10,000 testing 28x28 grayscale images in one of ten classes (hand-written digits), Often referred to as a "toy dataset," is commonly used to benchmark DP machine learning models. CIFAR-10 is a more complex dataset containing 50,000 training and 10,000 testing 32x32 RGB images, also consisting of ten classes. Note that the accuracy of DP models trained on CIFAR-10 has only recently started to approach non-private model performance [11; 35]. Whether on MNIST or CIFAR-10, prior black-box audits (using average-case initial parameters) have not been tight. More precisely, for theoretical \(=8.0\), De et al.  and Nasr et al.  only achieve empirical estimates of \(_{emp} 2\) and \(_{emp} 1.6\), respectively, on MNIST and CIFAR-10.

To ensure a fair comparison between the average-case and worst-case initial parameter settings, we split the training data in two and privately train on only half of each dataset (30,000 images for MNIST and 25,000 for CIFAR-10). The other half of the training dataset is used as the auxiliary dataset to non-privately pre-train the models and craft the worst-case initial parameters (see Section 4). We note that for CIFAR-10, we additionally augment the pre-training dataset with the CIFAR-100 dataset to boost the model accuracy further (see Appendix A for further details).

**Models.** When training the model fully, we use a (shallow) _Convolutional Neural Network (CNN)_ for both datasets. We do so as we find accuracy to be poor with models like Logistic Regression and Fully Connected Neural Networks, and training hundreds of deep neural networks is impractical due to large computational cost. Therefore, we draw on prior work by Dormann et al.  and train shallow CNNs that achieve acceptable model accuracy while being relatively fast to train. (We report exact model architectures in Appendix A). Whereas, for the last layer-only fine-tuning setting, we follow  and use the _Wide-ResNet (WRN-28-10)_ model pre-trained on ImageNet-32 .

**Experimental Testbed.** All our experiments are run on a cluster using 4 NVIDIA A100 GPUs, 64 CPU cores, and 100GB of RAM. Auditing just one model took approximately 16 GPU hours on MNIST and 50 GPU hours on CIFAR-10.

### Full Model Training

To train models with an acceptable accuracy on MNIST and CIFAR-10, we first tune the hyper-parameters (i.e., learning rate \(\) and number of iterations \(T\)); due to space limitations, we defer discussion to Appendix A.

**Model Accuracy.** In Table 1, we report the accuracy of the final models for the different values of \(\)-s we experiment with. While accuracy on MNIST is very good (\( 95\%\) for all \(\)), for CIFAR-10,

  
**Dataset** & \(=1.0\) & \(=2.0\) & \(=4.0\) & \(=10.0\) \\  MNIST & 95.4 & 95.7 & 95.9 & 95.9 \\ CIFAR-10 & 46.0 & 51.2 & 52.2 & 53.6 \\   

Table 1: Accuracy of models for average-case initial model parameters (\(_{0}\)).

we are relatively far from the state of the art (\(\)\(50\%\) at \(=10.0\) vs. \(\)\(80\%\) at \(=8.0\) in ). This is due to two main reasons. First, as we need to train hundreds of models, we are computationally limited to using shallow CNNs rather than deep neural networks. Second, we need to consider a much larger batch size to ease auditing ; thus, the models converge much more slowly. In Appendix B, we show that, given enough iterations (\(T=1,\)\(000\)), the model reaches an accuracy of \(\)\(70\%\) at \(=10.0\), almost approaching state-of-the-art. As a result, we audit models striking a reasonable trade-off between utility and computational efficiency, as discussed above.

**Impact of initialization.** In Figure 1, we compare the difference in \(_{emp}\) between worst-case initial parameters and average-case parameters (as done in prior work ). In the latter, all models are initialized to the same, randomly chosen \(_{0}\) (using Xavier initialization ), while, in the former, to a \(_{0}\) from a model pre-trained on an auxiliary dataset. In the rest of this section, the target sample is set as the blank sample, which is known to produce the tightest audits in previous work .

With worst-case initial parameters, we achieve significantly tighter audits for both MNIST and CIFAR-10. Specifically, at \(=1.0,2.0,4.0,10.0\), the empirical privacy leakage estimates reach \(_{emp}=0.08,0.54,1.89,6.48\) and \(_{emp}=0.00,0.18,1.43,4.96\) for MNIST and CIFAR-10, respectively. In comparison, with average-case initial parameters as in prior work, the empirical privacy leakage estimates were significantly looser, reaching only \(_{emp}=0.00,0.06,0.61,3.41\) and \(_{emp}=0.07,0.02,0.20,0.69\) for MNIST and CIFAR-10, respectively. Notably, at larger privacy levels \(=4.0\) and \(=10.0\), for both MNIST and CIFAR-10, the empirical privacy leakage estimates when auditing with worst-case initial parameters are not only larger than the average-case, but are outside of the \(\) s.d. range. This shows that our auditing procedure not only produces significantly tighter audits than prior work, especially at larger \(\)s, but also that, at lower \(\)-s, our evaluations are currently limited by the number of models used to audit (resulting in large s.d.).

Figure 1: Auditing models with average-case vs worst-case initial parameters at various levels of \(\).

Figure 2: Comparing the average gradient norms to empirical privacy leakage, \(_{emp}\), for models trained on MNIST at \(=10.0\) with increasing number of pre-training epochs.

To shed light on why worst-case initial parameters improve tightness, we vary the amount of pre-training done to craft \(_{0}\) and, in Figure 2, plot the corresponding \(_{emp}\)-s and the average gradient norm of all samples (after clipping) in the first iteration of DP-SGD. For computational efficiency, we focus on MNIST and set the theoretical \(\) to \(10.0\). Recall that, regardless of the initial model parameters, all models achieve \( 95\%\) accuracy on the test set. As the number of pre-training epochs increases from 0 (no pre-training) to 5, the average gradient norm steadily decreases from \(1.00\) to \(0.51\), as expected. On the other hand, the empirical privacy leakage increases from \(3.41\) to \(6.48\). In other words, with more pre-training, the impact of the _other_ samples in the dataset reduces, making the _target_ sample much more distinguishable, thus yielding tighter audits. Although there appears to be an anomaly in the \(_{emp}\) obtained when the model is pre-trained for 3 epochs, this is within the standard deviation.

_Remarks._ With lower \(\) values, the empirical \(_{emp}\) estimates are still relatively far away from the theoretical bounds. This is because, in each run, we train a small number of models and report the _average_\(_{emp}\) across all five runs. In fact, the _maximum_\(_{emp}\) obtained over the five runs amount to \(_{emp}=0.27,1.48,3.23,8.28\) and \(_{emp}=0.00,0.54,2.33,6.70\), respectively, on MNIST and CIFAR-10, for \(=1.0,2.0,4.0,10.0\). Nevertheless, even on the averages, our audits are still appreciably tighter than prior work--approximately by a factor of 3 for MNIST  and CIFAR-10 . Overall, this confirms that the tightness of the audits can vary depending on the datasets, possibly due to differences in the difficulty of the classification task on the datasets. For instance, MNIST classification is known to be easy, with DP-SGD already reaching close to non-private model performance, even at low \(\)-s. By contrast, CIFAR-10 is much more challenging, with a considerable gap still existing between private and non-private model accuracy .

**Impact of size of dataset.** In the auditing literature, empirical privacy leakage is often commonly estimated on (smaller) dataset samples both to achieve tighter audits and for computational efficiency reasons . In Figure 3, we evaluate the impact of the dataset size (\(n\)) on the tightness of auditing. While smaller datasets generally yield tighter audits, the size's impact also depends on the dataset itself. For instance, at \(=10.0\), auditing with \(n=100\) results in \(_{emp}=7.41\) and \(7.01\), for MNIST and CIFAR-10, respectively, compared to \(_{emp}=6.48\) and \(4.96\) on the full dataset. In other words, with MNIST, the dataset size does not significantly affect tightness. However, for CIFAR-10, smaller datasets significantly improve the audit's tightness significantly.

**Sensitivity to clipping norm.** Finally, we evaluate the impact of the hyper-parameters on the tightness of black-box audits. In the black-box setting, unlike in white-box, the adversary cannot arbitrarily insert gradients that are of the same scale as the gradient clipping norm, \(C\). Thus, they are restricted to gradients that naturally arise from samples. In Figure 4, we report the \(_{emp}\) values obtained with varying clipping norms, \(C\). For computational efficiency, the results on CIFAR-10 are for models trained on datasets of size \(n=1\),\(000\).

When the gradient clipping norm is small (\(C=0.1,1.0\)), the audits are tighter. This is because the gradient norms are typically very small in the worst-case initial parameter setting, leading to a much higher "signal-to-noise" ratio. While this might suggest that black-box audits may not be as

Figure 3: Auditing models trained on varying dataset sizes (\(n\)) at different values of \(\). The full dataset size \(||\) is 30,000 for MNIST and 25,000 for CIFAR-10.

effective with larger gradient clipping norms, we observe that, with \(C=10.0\), model accuracy is poor too--more precisely, with \(=1.0,2.0,4.0,10.0,\) accuracy reaches \(85.6\%,93.1\%,95.0\%,96.1\%\), respectively, compared to \(97.2\%,97.4\%,97.5\%,97.5\%\) with \(C=1.0\) on MNIST. This indicates that audits are looser with \(C=10.0\) because the model is not learning effectively, as there is too much noise. In fact, this could suggest that the privacy analysis for the \(C=10.0\) setting could be further tightened in the black-box threat model, which we leave to future work.

### Fine-Tuning the Last Layer Only

We now experiment with the setting where a model is first non-privately pre-trained on a large public dataset, and _only the last layer_ is fine-tuned on the private dataset. For this setting, we choose the WRN-28-10 model pre-trained on ImageNet-32 and fine-tune the last layer on CIFAR-10 using DP-SGD, as done in . At \(=1.0,2.0,4.0,10.0\), fine-tuning the last layer yields model accuracies of \(91.3\%,91.4\%,91.4\%,91.5\%\) and \(93.1\%,93.2\%,93.3\%,93.2\%\) for average-case and worst-case initial model parameters, respectively. In this section, we run the ClipBKD procedure from  to craft a target sample, as it has been shown to produce tighter empirical privacy guarantees for Logistic Regression models.

Once again, we compare the tightness of the audits with respect to the initial model parameters. In Figure 5, we report the empirical privacy leakage estimates obtained at various levels of \(\) when the last layer is initialized to average- and worst-case (through pre-training) initial model parameters. Similar to the full model training setting, model initialization can impact audit tightness, albeit not as significantly. Specifically, at \(=10.0\), we obtain \(_{emp}=9.19\) and \(9.33\), respectively, when the last layers are initialized to average- and worst-case initial model parameters.

Overall, the audits in this setting are considerably tight, and increasingly so for larger \(\), as we obtain empirical privacy leakage estimates of \(_{emp}=0.67,1.62,3.64,9.33\) for \(=1.0,2.0,4.0,10.0\)

Figure 4: Auditing models trained with varying gradient clipping norm (\(C\)) at \(=1.0,2.0,4.0,10.0\).

Figure 5: Auditing models when fine-tuning only the last layer using average-case and worst-case initialization of models at various levels of \(\).

respectively. The maximum \(_{emp}\) across five independent runs is also much tighter, namely, \(_{emp}=1.37,2.10,4.28,9.94\) for \(=1.0,2.0,4.0,10.0\), respectively. Although maximum \(_{emp}\) exceeds the theoretical \(\) at 1.0, 2.0, and 4.0 suggests a DP violation, this is, in fact, within the standard deviation of \( 0.40,0.36,\) and \(0.38\), respectively.

We also evaluated the impact of the dataset size and clipping norm in this setting as well; however, these factors do not significantly impact audit tightness, as this is a much simpler setting compared to training a CNN model fully. For more details, please see Appendix C.

## 6 Conclusion

This paper presented a novel auditing procedure for Differentially Private Stochastic Gradient Descent (DP-SGD). By crafting worst-case initial model parameters, we achieved empirical privacy leakage estimates substantially tighter than prior work for DP-SGD in the black-box model and for natural (i.e., not adversarially crafted) datasets. At \(=10.0\), we achieve nearly tight estimates of \(_{emp}=7.21\) and \(6.95\) for datasets consisting of 1,000 samples from MNIST and CIFAR-10, respectively. While we achieve slightly weaker estimates of \(_{emp}=6.48\) and \(4.96\) on the full datasets, these still represent a roughly 3x improvement over the estimates achieved in prior work .

Naturally, our work is not without limitations - the main one being the computational cost of auditing. Black-box auditing typically requires training hundreds of models to empirically estimate FPRs/FNRs with good accuracy and confidence. This can take hundreds of GPU hours, even for the shallow CNNs we audit in our work. Thus, auditing deep neural networks (e.g., WideResNet) trained on large datasets (e.g., ImageNet) may be computationally challenging for entities that are not huge corporations. However, as our main objective is addressing the open research question of whether tight DP-SGD audits are feasible in the black-box model, we focus on shallow models. Our results are very promising as, thanks to the intuition of using worst-case initial parameters, we do achieve nearly tight audits. Nevertheless, we leave it to future work to reduce the computational cost of the auditing procedure, e.g., by training significantly fewer models.

Relatedly, recent work  has begun to focus on auditing within a single training run, although it has thus far achieved only loose empirical estimates. One interesting direction would be to combine the insights from this paper (i.e., using worst-case initial model parameters) with one-shot auditing techniques. However, this may not be trivial as these techniques typically employ a large number of canary samples, each with large gradient norms, which can potentially interfere with our goal to reduce the gradient norm of "other" samples.

Furthermore, we only consider full batch gradient descent (\(B=n\)), i.e., without sub-sampling. Note that this is standard practice and eases auditing by enabling GDP auditing, which requires fewer models and is much less computationally intensive compared to auditing DP-SGD with sub-sampling using Privacy Loss Distributions (PLDs) . Nevertheless, when auditing DP-SGD with sub-sampling in the "hidden-state" threat model where the adversary can only observe the final trained model but can insert _gradient canaries_, recent work has shown that there is a significant gap between the theoretical upper bound and the empirical lower bound privacy leakage achieved . Even though this suggests a privacy amplification phenomenon in the hidden-state setting (which would extend to black-box as well), for general non-convex loss functions, prior work has also shown that the privacy analysis of DP-SGD with sub-sampling is tight even in the hidden-state . Therefore, whether audits can be tight in the black-box threat model for DP-SGD with sub-sampling under realistic loss functions and datasets remains an open question for future work.