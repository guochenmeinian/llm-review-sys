# Flow Matching for Scalable Simulation-Based Inference

Jonas Wildberger\({}^{*}\)

Max Planck Institute for Intelligent Systems

Tubingen, Germany

wildberger.jonas@tuebingen.mpg.de

&Maximilian Dax\({}^{*}\)

Max Planck Institute for Intelligent Systems

Tubingen, Germany

maximilian.dax@tuebingen.mpg.de

&Simon Buchholz\({}^{*}\)

Max Planck Institute for Intelligent Systems

Tubingen, Germany

sbuchholz@tue.mpg.de

&Stephen R. Green

University of Nottingham

Nottingham, United Kingdom

&Jakob H. Macke

Max Planck Institute for Intelligent Systems &

Machine Learning in Science,

University of Tubingen

Tubingen, Germany

Equal contribution

###### Abstract

Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures--making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30% with substantially improved accuracy. Our work underscores the potential of FMPE to enhance performance in challenging inference scenarios, thereby paving the way for more advanced applications to scientific problems.

## 1 Introduction

The ability to readily represent Bayesian posteriors of arbitrary complexity using neural networks would herald a revolution in scientific data analysis. Such networks could be trained using simulated data and used for amortized inference across observations--bringing tractable inference and speed to a myriad of scientific models. Thanks to innovative architectures such as normalizing flows [1; 2], approaches to neural simulation-based inference (SBI)  have seen remarkable progress in recent years. Here, we show that modern approaches to deep generative modeling (particularly flow matching) deliver substantial improvements in simplicity, flexibility and scaling when adapted to SBI.

The Bayesian approach to data analysis is to compare observations to models via the posterior distribution \(p(|x)\). This gives our degree of belief that model parameters \(\) gave rise to an observation \(x\), and is proportional to the model likelihood \(p(x|)\) times the prior \(p()\). One is typically interested in representing the posterior in terms of a collection of samples, however obtaining these through standard likelihood-based algorithms can be challenging for intractable or expensive likelihoods. In such cases, SBI offers an alternative based instead on _data simulations_\(x p(x|)\). Combined with deep generative modeling, SBI becomes a powerful paradigm for scientific inference . Neural posterior estimation (NPE) [4; 5; 6], for instance, trains a conditional density estimator \(q(|x)\) to approximate the posterior, allowing for rapid sampling and density estimation for any \(x\) consistent with the training distribution.

The NPE density estimator \(q(|x)\) is commonly taken to be a (discrete) normalizing flow [1; 2], an approach that has brought state-of-the-art performance in challenging problems such as gravitational-wave inference . Naturally, performance hinges on the expressiveness of \(q(|x)\). Normalizing flows transform noise to samples through a discrete sequence of basic transforms. These have been carefully engineered to be invertible with simple Jacobian determinant, enabling efficient maximum likelihood training, while at the same time producing expressive \(q(|x)\). Although many such discrete flows are universal density approximators , in practice, they can be challenging to scale to very large networks, which are needed for big-data experiments.

Recent studies [8; 9] propose neural posterior score estimation (NPSE), a rather different approach that models the posterior distribution with score-matching (or diffusion) networks. These techniques were originally developed for generative modeling [10; 11; 12], achieving state-of-the-art results in many domains, including image generation [13; 14]. Like discrete normalizing flows, diffusion models transform noise into samples, but with trajectories parametrized by a _continuous_ "time" parameter \(t\). The trajectories solve a stochastic differential equation  (SDE) defined in terms of a vector field \(v_{t}\), which is trained to match the score of the intermediate distributions \(p_{t}\). NPSE has several advantages compared to NPE, including the ability to combine multiple observations at inference time  and, importantly, the freedom to use unconstrained network architectures.

We here propose to use flow matching, another recent technique for generative modeling, for Bayesian inference, an approach we refer to as flow-matching posterior estimation (FMPE). Flow matching is also based on a vector field \(v_{t}\) and thereby also admits flexible network architectures (Fig. 1). For flow

Figure 1: Comparison of network architectures (left) and flow trajectories (right). Discrete flows (NPE, top) require a specialized architecture for the density estimator. Continuous flows (FMPE, bottom) are based on a vector field parametrized with an unconstrained architecture. FMPE uses this additional flexibility to put an enhanced emphasis on the conditioning data \(x\), which in the SBI context is typically high dimensional and in a complex domain. Further, the optimal transport path produces simple flow trajectories from the base distribution (inset) to the target.

matching, however, \(v_{t}\) directly defines the velocity field of sample trajectories, which solve ordinary differential equations (ODEs) and are deterministic. As a consequence, flow matching allows for additional freedom in designing non-diffusion paths such as optimal transport, and provides direct access to the density . These differences are summarized in Tab. 1.

Our contributions are as follows:

* We adapt flow-matching to Bayesian inference, proposing FMPE. In general, the modeling requirements of SBI are different from generative modeling. For the latter, sample quality is critical, i.e., that samples lie in the support of a complex distribution (e.g., images). In contrast, for SBI, \(p(|x)\) is typically less complex for fixed \(x\), but \(x\) itself can be complex and high-dimensional. We therefore consider pyramid-like architectures from \(x\) to \(v_{t}\), with gated linear units to incorporate \((,t)\) dependence, rather than the typical U-Net used for images (Fig. 1). We also propose an alternative \(t\)-weighting in the loss, which improves performance in many tasks.
* Under certain regularity assumptions, we prove an upper bound on the KL divergence between the model and target posterior. This implies that estimated posteriors are mass-covering, i.e., that their support includes all \(\) consistent with observed \(x\), which is highly desirable for scientific applications .
* We perform a number of experiments to investigate the performance of FMPE.2 Our two-pronged approach, which involves a set of benchmark tests and a real-world problem, is designed to probe complementary aspects of the method, covering breadth and depth of applications. First, on an established suite of SBI benchmarks, we show that FMPE performs comparably--or better--than NPE across most tasks, and in particular exhibits mass-covering posteriors in all cases (Sec. 4). We then push the performance limits of FMPE on a challenging real-world problem by turning to gravitational-wave inference (Sec. 5). We show that FMPE substantially outperforms an NPE baseline in terms of training time, posterior accuracy, and scaling to larger networks.

## 2 Preliminaries

Normalizing flows.A normalizing flow [1; 2] defines a probability distribution \(q(|x)\) over parameters \(^{n}\) in terms of an invertible mapping \(_{x}:^{n}^{n}\) from a simple base distribution \(q_{0}()\),

\[q(|x)=(_{x})_{*}q_{0}()=q_{0}(_{x}^{-1}()) |^{-1}()}{}|\,,\] (1)

where \(()_{*}\) denotes the pushforward operator, and for generality we have conditioned on additional context \(x^{m}\). Unless otherwise specified, a normalizing flow refers to a _discrete_ flow, where \(_{x}\) is given by a composition of simpler mappings with triangular Jacobians, interspersed with shuffling of the \(\). This construction results in expressive \(q(|x)\) and also efficient density evaluation .

Continuous normalizing flows.A continuous flow  also maps from base to target distribution, but is parametrized by a continuous "time" \(t\), where \(q_{0}(|x)=q_{0}()\) and \(q_{1}(|x)=q(|x)\). For each \(t\), the flow is defined by a vector field \(v_{t,x}\) on the sample space.3 This corresponds to the velocity of the sample trajectories,

\[_{t,x}()=v_{t,x}(_{t,x}()),_{0,x}( )=.\] (2)

    & NPE & NPSE & **FMPE (Ours)** \\  Tractable posterior density & Yes & No & Yes \\ Unconstrained network architecture & No & Yes & Yes \\ Network passes for sampling & Single & Many & Many \\   

Table 1: Comparison of posterior-estimation methods.

We obtain the trajectories \(_{t}_{t,x}()\) by integrating this ODE. The final density is given by

\[q(|x)=(_{1,x})_{*}q_{0}()=q_{0}()(-_{0}^{1} v_{t,x}(_{t})\!t),\] (3)

which is obtained by solving the transport equation \(_{t}q_{t}+(q_{t}v_{t,x})=0\).

The advantage of the continuous flow is that \(v_{t,x}()\) can be simply specified by a neural network taking \(^{n+m+1}^{n}\), in which case (2) is referred to as a _neural ODE_. Since the density is tractable via (3), it is in principle possible to train the flow by maximizing the (log-)likelihood. However, this is often not feasible in practice, since both sampling and density estimation require many network passes to numerically solve the ODE (2).

Flow matching.An alternative training objective for continuous normalizing flows is provided by flow matching . This directly regresses \(v_{t,x}\) on a vector field \(u_{t,x}\) that generates a target probability path \(p_{t,x}\). It has the advantage that training does not require integration of ODEs, however it is not immediately clear how to choose \((u_{t,x},p_{t,x})\). The key insight of  is that, if the path is chosen on a _sample-conditional_ basis,4 then the training objective becomes extremely simple. Indeed, given a sample-conditional probability path \(p_{t}(|_{1})\) and a corresponding vector field \(u_{t}(|_{1})\), we specify the sample-conditional flow matching loss as

\[_{}=_{t,\,x p(x),\, _{1} p(|x),\,_{t} p_{t}(_{t}|_{1})}\| v_{t,x}(_{t})-u_{t}(_{t}|_{1})\|^{2}.\] (4)

Remarkably, minimization of this loss is equivalent to regressing \(v_{t,x}()\) on the _marginal_ vector field \(u_{t,x}()\) that generates \(p_{t}(|x)\). Note that in this expression, the \(x\)-dependence of \(v_{t,x}()\) is picked up via the expectation value, with the sample-conditional vector field independent of \(x\).

There exists considerable freedom in choosing a sample-conditional path. Ref.  introduces the family of Gaussian paths

\[p_{t}(|_{1})=(|_{t}(_{1}),_{t}( _{1})^{2}I_{n}),\] (5)

where the time-dependent means \(_{t}(_{1})\) and standard deviations \(_{t}(_{1})\) can be freely specified (subject to boundary conditions5). For our experiments, we focus on the optimal transport paths defined by \(_{t}(_{1})=t_{1}\) and \(_{t}(_{1})=1-(1-_{})t\) (also introduced in ). The sample-conditional vector field then has the simple form

\[u_{t}(|_{1})=-(1-_{})}{1-(1- _{})t}.\] (6)

Neural posterior estimation (NPE).NPE is an SBI method that directly fits a density estimator \(q(|x)\) (usually a normalizing flow) to the posterior \(p(|x)\). NPE trains with the maximum likelihood objective \(_{}=-_{p()p(x|)} q(|x)\), using Bayes' theorem to simplify the expectation value with \(_{p(x)p(|x)}_{p()p(x|)}\). During training, \(_{}\) is estimated based on an empirical distribution consisting of samples \((,x) p()p(x|)\). Once trained, NPE can perform inference for every new observation using \(q(|x)\), thereby _amortizing_ the computational cost of simulation and training across all observations. NPE further provides exact density evaluations of \(q(|x)\). Both of these properties are crucial for the physics application in section 5, so we aim to retain these properties with FMPE.

#### Related work

Flow matching  has been developed as a technique for generative modeling, and similar techniques are discussed in  and extended in . Flow matching encompasses the deterministic ODE version of diffusion models  as a special instance. Although to our knowledge flow matching has not previously been applied to Bayesian inference, score-matching diffusion models have been proposed for SBI in  with impressive results. These studies, however, use stochastic formulations via SDEs  or Langevin steps and are therefore not directly applicable when evaluations of the posterior density are desired (see Tab. 1). It should be noted that score modeling can also be used to parameterize continuous normalizing flows via an ODE. Extension of [8; 9] to the deterministic formulation could thereby be seen as a special case of flow matching. Many of our analyses and the practical guidance provided in Section 3 therefore also apply to score matching.

We here focus on comparisons of FMPE against NPE [4; 5; 6], as it best matches the requirements of the application in section 5. Other SBI methods include approximate Bayesian computation [24; 25; 26; 27; 28], neural likelihood estimation [29; 30; 31; 32] and neural ratio estimation [33; 34; 35; 36; 37; 38; 39]. Many of these approaches have sequential versions, where the estimator networks are specifically tuned to a specific observation \(x_{}\). FMPE has a tractable density, so it is straightforward to apply the sequential NPE [4; 5; 6] approaches to FMPE. In this case, inference is no longer amortized, so we leave this extension to future work.

## 3 Flow matching posterior estimation

To apply flow matching to SBI we use Bayes' theorem to make the usual replacement \(_{p(x)p(|x)}_{p()p(x|)}\) in the loss function (4), eliminating the intractable expectation values. This gives the FMPE loss

\[_{}=_{t p(t),\;_{1} p(),x  p(x|_{1}),\,_{t} p_{t}(_{t}|_{1})}\|v_{t,x}(_{t})-u_{t}(_{t}|_{1})\|^{2},\] (7)

which we minimize using empirical risk minimization over samples \((,x) p()p(x|)\). In other words, training data is generated by sampling \(\) from the prior, and then simulating data \(x\) corresponding to \(\). This is in close analogy to NPE training, but replaces the log likelihood maximization with the sample-conditional flow matching objective. Note that in this expression we also sample \(t p(t)\), \(t\) (see Sec. 3.3), which generalizes the uniform distribution in (4). This provides additional freedom to improve learning in our experiments.

### Probability mass coverage

As we show in our examples, trained FMPE models \(q(|x)\) can achieve excellent results in approximating the true posterior \(p(|x)\). However, it is not generally possible to achieve _exact_ agreement due to limitations in training budget and network capacity. It is therefore important to understand how inaccuracies manifest. Whereas sample quality is the main criterion for generative modeling, for scientific applications one is often interested in the overall shape of the distribution. In particular, an important question is whether \(q(|x)\) is _mass-covering_, i.e., whether it contains the full support of \(p(|x)\). This minimizes the risk to falsely rule out possible explanations of the data. It also allows us to use importance sampling if the likelihood \(p(x|)\) of the forward model can be evaluated, which can be used for precise estimation of the posterior [40; 41].

Consider first the mass-covering property for NPE. NPE directly minimizes the forward KL divergence \(}(p(|x)||q(|x))\), and thereby provides probability-mass covering results. Therefore, even if NPE is not accurately trained, the estimate \(q(|x)\) should cover the entire support of the posterior \(p(|x)\) and the failure to do so can be observed in the validation loss. As an illustration in an unconditional setting, we observe that a unimodal Gaussian \(q\) fitted to a bimodal target distribution \(p\) captures both modes when using the forward KL divergence \(}(p||q)\), but only a single mode when using the backwards direction \(}(q||p)\) (Fig. 2).

For FMPE, we can fit a Gaussian flow-matching model \(q()=(,^{2})\) to the same bimodal target, in this case, parametrizing the vector field as

\[v_{t}()=^{2}+(t)^{2}-_{t})_{t }+t_{t}}{t(_{t}^{2}+(t)^{2})}\] (8)

(see Appendix A), we also obtain a mass-covering distribution when fitting the learnable parameters \((,)\) via (4). This provides some indication that the flow matching objective induces mass-covering behavior, and leads us to investigate the more general question of whether the mean squared error

Figure 2: A Gaussian (blue) fitted to a bimodal distribution (gray) with various objectives.

between vector fields \(u_{t}\) and \(v_{t}\) bounds the forward KL divergence. Indeed, the former agrees up to constant with the sample-conditional loss (4) (see Sec. 2).

We denote the flows of \(u_{t}\), \(v_{t}\), by \(_{t}\), \(_{t}\), respectively, and we set \(q_{t}=(_{t})_{*}q_{0}\), \(p_{t}=(_{t})_{*}q_{0}\). The precise question then is whether we can bound \(_{}(p_{1}||q_{1})\) by \(_{p}(u,v)^{}\) for some positive power \(\). It was already observed in  that this is not true in general, and we provide a simple example to that effect in Lemma 1 in Appendix B. Indeed, it was found in  that to bound the forward KL divergence we also need to control the Fisher divergence, \( p_{t}()( p_{t}()- q_{t}())^{2}\).

Here we show instead that a bound can be obtained under sufficiently strong regularity assumptions on \(p_{0}\), \(u_{t}\), and \(v_{t}\). The following statement is slightly informal, and we refer to the supplement for the complete version.

**Theorem 1**.: _Let \(p_{0}=q_{0}\) and assume \(u_{t}\) and \(v_{t}\) are two vector fields whose flows satisfy \(p_{1}=(_{1})_{*}p_{0}\) and \(q_{1}=(_{1})_{*}q_{0}\). Assume that \(p_{0}\) is square integrable and satisfies \(| p_{0}()| c(1+||)\) and \(u_{t}\) and \(v_{t}\) have bounded second derivatives. Then there is a constant \(C>0\) such that (for \(_{p}(u,v)<1\)))_

\[_{}(p_{1}||q_{1}) C\,_{p}(u,v)^{}.\] (9)

_The proof of this result can be found in appendix B._ While the regularity assumptions are not guaranteed to hold in practice when \(v_{t}\) is parametrized by a neural net, the theorem nevertheless gives some indication that the flow-matching objective encourages mass coverage. In Section 4 and 5, this is complemented with extensive empirical evidence that flow matching indeed provides mass-covering estimates.

We remark that it was shown in  that the KL divergence of SDE solutions can be bounded by the MSE of the estimated score function. Thus, the smoothing effect of the noise ensures mass coverage, an aspect that was further studied using the Fokker-Planck equation in . For flow matching, imposing the regularity assumption plays a similar role.

### Network architecture

Generative diffusion or flow matching models typically operate on complicated and high dimensional data in the \(\) space (e.g., images with millions of pixels). One typically uses U-Net  like architectures, as they provide a natural mapping from \(\) to a vector field \(v()\) of the same dimension. The dependence on \(t\) and an (optional) conditioning vector \(x\) is then added on top of this architecture.

For SBI, the data \(x\) is often associated with a complicated domain, such as image or time series data, whereas parameters \(\) are typically low dimensional. In this context, it is therefore useful to build the architecture starting as a mapping from \(x\) to \(v(x)\) and then add conditioning on \(\) and \(t\). In practice, one can therefore use any established feature extraction architecture for data in the domain of \(x\), and adjust the dimension of the feature vector to \(n=()\). In our experiments, we found that the \((t,)\)-conditioning is best achieved using gated linear units  to the hidden layers of the network (see also Fig. 1); these are also commonly used for conditioning discrete flows on \(x\).

### Re-scaling the time prior

The time prior \(\) in (4) distributes the training capacity uniformly across \(t\). We observed that this is not always optimal in practice, as the complexity of the vector field may depend on \(t\). For FMPE we therefore sample \(t\) in (7) from a power-law distribution \(p_{}(t) t^{1/(1+)},\ t\), introducing an additional hyperparameter \(\). This includes the uniform distribution for \(=0\), but for \(>0\), assigns greater importance to the vector field for larger values of \(t\). We empirically found this to improve learning for distributions with sharp bounds (e.g., Two Moons in Section 4).

## 4 SBI benchmark

We now evaluate FMPE on ten tasks included in the benchmark presented in , ranging from simple Gaussian toy models to more challenging SBI problems from epidemiology and ecology, with varying dimensions for parameters (\(()\)) and observations (\((x)\)). For each task, we train three separate FMPE models with simulation budgets \(N\{10^{3},10^{4},10^{5}\}\). Weuse a simple network architecture consisting of fully connected residual blocks  to parameterize the conditional vector field. For the two tasks with \((x)=100\) (B-GLM-Raw, SLCP-D), we condition on \((t,)\) via gated linear units as described in Section 3.2 (Fig. 8 in Appendix C shows the corresponding performance gain). For the remaining tasks with \((x) 10\) we concatenate \((t,,x)\) instead. We reserve 5% of the simulations for validation. See Appendix C for details.

For each task and simulation budget, we evaluate the model with the lowest validation loss by comparing \(q(|x)\) to the reference posteriors \(p(|x)\) provided in  for ten different observations \(x\) in terms of the C2ST score . This performance metric is computed by training a classifier to discriminate inferred samples \( q(|x)\) from reference samples \( p(|x)\). The C2ST score is then the test accuracy of this classifier, ranging from 0.5 (best) to 1.0. We observe that FMPE exhibits comparable performance to an NPE baseline model for most tasks and outperforms on several (Fig. 4). In terms of the MMD metric (Fig. 6 in the Appendix), FMPE clearly outperforms NPE (but MMD can be sensitive to its hyperparameters ). As NPE is one of the highest ranking methods for many tasks in the benchmark, these results show that FMPE indeed performs competitively with other existing SBI methods. We report an additional baseline for score matching in Fig. 7 in the Appendix.

As NPE and FMPE both directly target the posterior with a density estimator (in contrast to most other SBI methods), observed differences can be primarily attributed to their different approaches for density estimation. Interestingly, a great performance improvement of FMPE over NPE is observed for SLCP with a large simulation budget (\(N=10^{5}\)). The SLCP task is specifically designed to have a simple likelihood but a complex posterior, and the FMPE performance underscores the enhanced flexibility of the FMPE density estimator.

Finally, we empirically investigate the mass coverage suggested by our theoretical analysis in Section 3.1. We display the density \( q(|x)\) of the reference samples \( p(|x)\) under our FMPE model \(q\) as a histogram (Fig. 3). All samples \( p(|x)\) fall into the support from \(q(|x)\). This becomes apparent when comparing to the density \( q(|x)\) for samples \( q(|x)\) from \(q\) itself. This FMPE result is therefore mass covering. Note that this does not necessarily imply conservative posteriors (which is also not generally true for the forward KL divergence ), and some parts of \(p(|x)\) may still be undersampled. Probability mass coverage, however, implies that no part is entirely missed (compare Fig. 2), even for multimodal distributions such as Two Moons. Fig. 9 in the Appendix confirms the mass coverage for the other benchmark tasks.

## 5 Gravitational-wave inference

### Background

Gravitational waves (GWs) are ripples of spacetime predicted by Einstein and produced by cataclysmic cosmic events such as the mergers of binary black holes (BBHs). GWs propagate across the universe to Earth, where the LIGO-Virgo-KAGRA observatories measure faint time-series signals embedded in noise. To-date, roughly 90 detections of merging black holes and neutron stars have been made , all of which have been characterized using Bayesian inference to compare against theoretical models.6 These have yielded insights into the origin and evolution of black holes , fundamental properties of matter and gravity , and even the expansion rate of the universe . Under reasonable assumptions on detector noise, the GW likelihood is tractable,7 and inference is

Figure 3: Histogram of FMPE densities \( q(|x)\) for reference samples \( p(|x)\) (Two Moons task, \(N=10^{3}\)). The estimate \(q(|x)\) clearly covers \(p(|x)\) entirely.

typically performed using tools [57; 58; 59; 60] based on Markov chain Monte Carlo [61; 62] or nested sampling  algorithms. This can take from hours to months, depending on the nature of the event and the complexity of the signal model, with a typical analysis requiring up to \( 10^{8}\) likelihood evaluations. The ever-increasing rate of detections means that these analysis times risk becoming a bottleneck. SBI offers a promising solution for this challenge that has thus been actively studied in the literature [64; 65; 66; 67; 68; 7; 41]. A fully amortized NPE-based method called DINGO recently achieved accuracies comparable to stochastic samplers with inference times of less than a minute per event . To achieve accurate results, however, DINGO uses group-equivariant NPE [7; 69] (GNPE), an NPE extension that integrates known conditional symmetries. GNPE, therefore, does not provide a tractable density, which is problematic when verifying and correcting inference results using importance sampling .

### Experiments

We here apply FMPE to GW inference. As a baseline, we train an NPE network with the settings described in  with a few minor changes (see Appendix D).8 This uses an embedding network  to compress \(x\) to a 128-dimensional feature vector, which is then used to condition a neural spline flow . The embedding network consists of a learnable linear layer initialized with principal components of GW simulations followed by a series of dense residual blocks . This architecture is a powerful feature extractor for GW measurements . As pointed out in Section 3.2, it is straightforward to reuse such architectures for FMPE, with the following three modifications: (1) we provide the conditioning on \((t,)\) to the network via gated linear units in each hidden layer; (2) we change the dimension of the final feature vector to the dimension of \(\) so that the network parameterizes the conditional vector field \((t,x,) v_{t,x}()\); (3) we increase the number and width of the hidden layers to use the capacity freed up by removing the discrete normalizing flow.

We train the NPE and FMPE networks with \(5 10^{6}\) simulations for 400 epochs using a batch size of 4096 on an A100 GPU. The FMPE network (\(1.9 10^{8}\) learnable parameters, training takes \( 2\) days) is larger than the NPE network (\(1.3 10^{8}\) learnable parameters, training takes \( 3\) days), but trains substantially faster. We evaluate both networks on GW150914 , the first detected GW. We generate a reference posterior using the method described in . Fig. 5 compares the inferred posterior distributions qualitatively and quantitatively in terms of the Jensen-Shannon divergence (JSD) to the reference.9

FMPE substantially outperforms NPE in terms of accuracy, with a mean JSD of \(0.5\) mnat (NPE: \(3.6\) mnat), and max JSD \(<2.0\) mnat, an indistinguishability criterion for GW posteriors . Remarkably, FMPE accuracy is even comparable to GNPE, which leverages physical symmetries

Figure 4: Comparison of FMPE with NPE, a standard SBI method, across 10 benchmark tasks .

to simplify data. Finally, we find that the Bayesian evidences inferred with NPE (\( p(x)=-7667.958 0.006\)) and FMPE (\( p(x)=-7667.969 0.005\)) are consistent within their statistical uncertainties. A correct evidence is only obtained in importance sampling when the inferred posterior \(q(|x)\) covers the entire posterior \(p(|x)\), so this is another indication that FMPE indeed induces mass-covering posteriors.

### Discussion

Our results for GW150914 show that FMPE substantially outperforms NPE on this challenging problem. We believe that this is related to the network structure as follows. The NPE network allocates roughly two thirds of its parameters to the discrete normalizing flow and only one third to the embedding network (i.e., the feature extractor for \(x\)). Since FMPE parameterizes just a vector field (rather than a collection of splines in the normalizing flow) it can devote its network capacity to the interpretation of the high-dimensional \(x^{15744}\). Hence, it scales better to larger networks and achieves higher accuracy. Remarkably, the performance iscomparable to GNPE, which involves a much simpler learning task with likelihood symmetries integrated by construction. This enhanced performance, comes in part at the cost of increased inference times, typically requiring hundreds of network forward passes. See Appendix D for further details.

In future work we plan to carry out a more complete analysis of GW inference using FMPE. Indeed, GW150914 is a loud event with good data quality, where NPE already performs quite well. DINGO with GNPE has been validated in a variety of settings [7; 69; 41; 74] including events with a larger performance gap between NPE and GNPE . Since FMPE (like NPE) does not integrate physical symmetries, it would likely need further enhancements to fully compete with GNPE. This may require a symmetry-aware architecture , or simply further scaling to larger networks. A straightforward application of the GNPE mechanism to FMPE--GFMPE--is also possible, but less practical due to the higher inference costs of FMPE. Nevertheless, our results demonstrate that FMPE is a promising direction for future research in this field.

Figure 5: Results for GW150914 . Left: Corner plot showing 1D marginals on the diagonal and 2D 50% credible regions. We display four GW parameters (distance \(d_{}\), time of arrival \(t_{}\), and sky coordinates \(,\)); these represent the least accurate NPE parameters. Right: Deviation between inferred posteriors and the reference, quantified by the Jensen-Shannon divergence (JSD). The FMPE posterior matches the reference more accurately than NPE, and performs similarly to symmetry-enhanced GNPE. (We do not display GNPE results on the left due to different data conditioning settings in available networks.)

Conclusions

We introduced flow matching posterior estimation, a new simulation-based inference technique based on continuous normalizing flows. In contrast to existing neural posterior estimation methods, it does not rely on restricted density estimation architectures such as discrete normalizing flows, and instead parametrizes a distribution in terms of a conditional vector field. Besides enabling flexible path specifications, while maintaining direct access to the posterior density, we empirically found that regressing on a vector field rather than an entire distribution improves the scalability of FMPE compared to existing approaches. Indeed, fewer parameters are needed to learn this vector field, allowing for larger networks, ultimately enabling to solve more complex problems. Furthermore, our architecture for FMPE (a straightforward ResNet with GLU conditioning) facilitates parallelization and allows for cheap forward/backward passes.

We evaluated FMPE on a set of 10 benchmark tasks and found competitive or better performance compared to other simulation-based inference methods. On the challenging task of gravitational-wave inference, FMPE substantially outperformed comparable discrete flows, producing samples on par with a method that explicitly leverages symmetries to simplify training. Additionally, flow matching latent spaces are more naturally structured than those of discrete flows, particularly when using paths such as optimal transport. Looking forward, it would be interesting to exploit such structure in designing learning algorithms. This performance and flexibilty underscores the capability of continuous normalizing flows to efficiently solve inverse problems.