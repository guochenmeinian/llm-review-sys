# Unscrambling disease progression at scale: fast inference of event permutations with optimal transport

Unscrambling disease progression at scale: fast inference of event permutations with optimal transport

 Peter A. Wijeratne

Sussex AI Centre

Department of Informatics

University of Sussex

Brighton, BN1 9RH

United Kingdom

p.wijeratne@sussex.ac.uk

&Daniel C. Alexander

Hawkes Institute

Department of Computer Science

University College London

London, WC1E 6BT

United Kingdom

d.alexander@ucl.ac.uk

###### Abstract

Disease progression models infer group-level temporal trajectories of change in patients' features as a chronic degenerative condition plays out. They provide unique insight into disease biology and staging systems with individual-level clinical utility. Discrete models consider disease progression as a latent permutation of events, where each event corresponds to a feature becoming measurably abnormal. However, permutation inference using traditional maximum likelihood approaches becomes prohibitive due to combinatoric explosion, severely limiting model dimensionality and utility. Here we leverage ideas from optimal transport to model disease progression as a latent permutation matrix of events belonging to the Birkhoff polytope, facilitating fast inference via optimisation of the variational lower bound. This enables a factor of 1000 times faster inference than the current state of the art and, correspondingly, supports models with several orders of magnitude more features than the current state of the art can consider. Experiments demonstrate the increase in speed, accuracy and robustness to noise in simulation. Further experiments with real-world imaging data from two separate datasets, one from Alzheimer's disease patients, the other age-related macular degeneration, showcase, for the first time, pixel-level disease progression events in the brain and eye, respectively. Our method is low compute, interpretable and applicable to any progressive condition and data modality, giving it broad potential clinical utility.

## 1 Introduction

The main aim of disease progression modelling is to learn a hidden underlying disease trajectory from'snapshots' (sets of observations at a single time) of individuals at hidden points along the trajectory. The classical approach is to treat the problem dynamically, using either discrete  or continuous  models with latent variables to describe the hidden disease stage or time. An abundance of such models have been proposed (see  for a comprehensive review) and have found extensive success in providing unique interpretability and utility across a wide range of progressive diseases, including Alzheimer's disease (AD) , Huntington's disease , multiple sclerosis , Parkinson's disease , prion disease , amyotrophic lateral sclerosis , and chronic obstructive pulmonary disorder .

However all previous approaches make a compromise: they are either i) interpretable in feature space but sacrifice computational tractability ; or ii) are made computationally tractable by encoding to a latent space but sacrifice direct interpretability . Models of type (i) often require preprocessing or dimensionality reduction to extract a modest number of interpretable features from high dimensional data, e.g., deriving features of anatomical regions from medical images, because computation time scales super-linearly with the number of features. The preprocessing introduces uncertainty and is often computationally burdensome in itself.

Here we introduce the variational event-based model (vEBM), which enables high dimensional interpretable models through a new computationally efficient approach that avoids the need for dimensionality reduction or manual feature extraction. For example, with image-based models, it enables models that express progression at the pixel-level rather than regional level. To achieve this we reformulate disease progression modelling as the 'transport' of latent disease events to their 'optimal' location in a continuous latent permutation, unlocking benefits from recent advancements in the field of computational optimal transport . Our approach generalises discrete generative models of disease progression, e.g., [1; 36; 37; 38; 39; 43], which it obtains as a limit; and it directly infers a continuous probability over events, while the others require costly sampling methods. Crucially, it also facilitates variational inference of the posterior, allowing for substantial gains in computational tractability and hence larger models.

Related work.The closest direct comparisons to the model we propose here are the sequence-based models proposed by [33; 1]. These models underpin both a range of direct applications [20; 5; 23; 24; 12; 21; 25; 27; 29], as well as providing components in higher level models [5; 36; 38; 39]. Like ours, these models require data with only a single time-point per individual. They assume monotonic progression in order to learn a latent sequence of events from such cross-sectional data. However, the models in [33; 1] are severely limited by their computational tractability, and can typically only use a few 100 features at most. In contrast, our new formulation enables this type of model to include several orders of magnitude more features enabling, for example, pixel-level temporal models as we demonstrate here.

Deep-learning based sequence models, using e.g. transformer architectures have recently become popular for models of high dimensional temporal sequences e.g., . However, as with other deep state-space models, e.g., [44; 6] these approaches require vast amounts of data with multiple time points to train, unlike our approach which can be trained on modest datasets (order 100 subjects with observations from a single time-point). Furthermore, the computational power required to train the upstream foundation model, plus the downstream model itself, is order of magnitudes higher than our model, which can run on a single CPU in a matter of minutes.

### Contributions

Here we address the problem of how to learn interpretable high dimensional disease progression models efficiently, which is longstanding in the machine learning community.

* We leverage ideas from optimal transport to derive a new generative latent variable model of disease progression, the variational event-based model (vEBM). The vEBM characterises the disease process by a continuous latent permutation of event probabilities, permitting direct inference of event distributions and model uncertainty from mixed feature datasets.
* We define a differentiable variational evidence lower bound (ELBO) and devise a suitable inference scheme to learn the vEBM efficiently from high dimensional data.
* We use synthetic data to demonstrate that the vEBM achieves a factor of 1000\(\) faster inference than baselines, provides better inference accuracy, and is robust to noise.
* We use the vEBM with data from Alzheimer's disease (AD) and age-related macular degeneration (AMD) to obtain, for the first time, pixel-level disease progression events in the brain and eye, and mixed-feature models combining imaging and clinical test score data.

## 2 Variational event-based model

To derive the variational event-based model (vEBM), we first derive a generative latent variable model of disease progression in terms of a latent permutation matrix of events (Section 2.1). Our key methodological contribution is reformulating the generative model in the context of optimal transport; we introduce the relevant mathematical tools to do this in Section 2.2, which we use to derive the limit relationship between the classical EBM and the vEBM in Appendix Section A.3. We then define our model in a variational inference setting (Section 2.3), devise a suitable inference scheme (Section 2.4), and provide a method for probabilistic individual-level staging using the trained model (Section 2.5). Figure 11 provides schematic overview of the vEBM.

### Model of disease progression

Consider a generative latent variable model with observed data \(Y\) and latent variables \(Z=\{S,k,\}\), where \(S_{+}^{N N}\) is a latent permutation matrix of \(N\) events; \(k_{+}^{N}\) is the latent state of an individual; and \(=\{(1),(2),(3),...,(J)\}\) are additional model parameters. We write the joint probability as a hierarchical Bayesian model using the chain rule (see Appendix Figure 9 for the graphical model),

\[P(Z,Y)=P(Z) P(Y|Z)=P(S) P() P(k|S) P(Y|S,k,).\] (1)

Each element in the permutation matrix, \(S\), defines an 'event', which corresponds to a feature becoming measurably abnormal with respect to a reference distribution. Following , we parameterise the data likelihood by probability distributions of 'abnormality' (typically from patients, \(p\)), and 'normality' (typically from controls, \(c\)) for each feature, and choose to model the distributions for feature \(j\) using univariate Gaussian mixture models with mean, \(_{j}\), standard deviation, \(_{j}\), and mixture weights, \(w_{j}\), so that

\[P(Y_{j}|_{j}^{p,c})(_{j}^{p,c},_{j}^{p,c},w_{j} ^{p,c}).\] (2)

However any probabilistic characterisation that defines a reference group to anchor the progression is permissible. To enable \(S\) to be inferred using data from different individuals at a single time-point ('snapshots'), we make two key assumptions: \(i)\) monotonic progression of events at the group level; and \(ii)\) a consistent event permutation across the whole population. We note that assumption \(ii)\) could be relaxed to permit multiple event permutations (i.e., clusters) within the same population. Then for individuals \(i=\{1,2,3,...,I\}\) with observed features \(j=\{1,2,3,..,J\}\), the model likelihood can be written as (see Appendix A.2 for a full derivation),

\[P(Y|S;)=_{i=1}^{I}[_{k_{i}=0}^{N}P(k_{i}|S)_{j=1}^{k _{i}}P(Y_{i,j}|S,k_{i},_{s(j)}^{p})_{j=k_{i}+1}^{J}P(Y_{i,j}|S,k_{ i},_{s(j)}^{c})].\] (3)

Figure 1: Schematic of the variational event-based model for a toy 4-feature dataset. **A**. The dataset contains snapshots from \(I\) individuals, with \(j,k=\{1,2,3,4\}\) features and latent events; the features can be of any type and can be incomplete. **B**. Before inference, probabilistic models of normality and abnormality are fit to the dataset, giving the likelihood look-up tables \(P(Y|^{p,c})\) (Section 2.1); these are fixed throughout inference, as denoted by the inner box outside the training loop. To infer the permutation matrix \(S\) (Section 2.2), the ELBO (Section 2.3) is optimised and \(S\) is updated each iteration using the Sinkhorn-Knopp algorithm (Section 2.4). **C**. The resulting hard permutation, \(s\), i.e., the disease event sequence, is obtained from \(S\), using the Hungarian algorithm. We note that \(S\) represents the full distribution of event probabilities, which can be sampled to obtain uncertainty.

Here \(s(N)\) is a discrete permutation of \(N\) events, corresponding to the hard permutation obtained from \(S\) (see the next section); and \(_{s(j)}=_{s(j)}^{p}_{s(j)}^{c}\) are the patient, \(p\), and control, \(c\), distribution parameters generating the data for feature \(j\) at position \(s(j)\) in the permutation. Note that if data are missing, the two likelihoods on the RHS of Equation 3 can be set equal and factorised, i.e., the data can be treated as missing at random. In order to impose no prior information on the permutation ordering, we chose the prior over latent stages to be uniform, \(P(k_{i}|S)(0,k)\).

### Optimal transport for permutations

Under the definition of disease progression given by Equation 3 and using Bayes rule, our posterior is a probability distribution over the sequence of events. Given that permutations are factorial in \(N\), the challenge is to make inference of \(S\) computationally tractable when the number of features - and hence the number of events - is large (of order \(N>100\)). To address this, we propose to frame the learning problem in terms of the 'transport' of disease events to their 'optimal' location in the disease event sequence, \(s\), defined by the permutation matrix \(S\). Our key methodological contribution is the translation of the model likelihood (Equation 3) to the context of optimal transport; here we provide the necessary background theory to enable us to derive the relationship between \(s\) and \(S\) in our model.

Optimal transport aims to identify the mass-conserving coupling between two distributions ('transport plan') that minimises the cost required to move (or transform) one into the other . The minimum cost defines a distance between distributions (the Wasserstein distance) and induces a rich underlying geometry on the space of distributions, providing benefits over classical learning techniques such as maximum likelihood. While optimal transport requires solving a computationally expensive linear problem, recent advances have resolved this by substituting the original problem with an entropy regularised version , paving the way for its use in learning generative models.

When the couplings are restricted to be permutation matrices, we can leverage the machinery of entropy regularised optimal transport to provide computationally tractable solutions to inferring latent permutations . Here we are interested in learning a latent permutation matrix, \(S\), with a corresponding discrete permutation, \(s\), such that,

\[(i,j)_{+},\;S_{i,j}=1/n,&j=s_{i}\\ 0,&\] (4)

In the context of permutation matrices as couplings, \(S\) belongs to the Birkhoff polytope,

\[_{N}=\{S:S_{i,j} 0,\;_{j}^{N}S_{i,j}=1,\;_{i}^{N}S_{i,j} =1\}.\] (5)

The Birkhoff-von Neumann theorem states that \(_{N}\) is the convex hull of the set of doubly stochastic (soft) permutation matrices, and that its vertices are the (hard) permutation matrices . The row-column normalisation equality constraints in Equation 5 demand efficient algorithms to solve for \(S\). Following , we use the Sinkhorn-Knopp algorithm with an entropy regularisation term, \(H(S)=-_{i,j}S_{i,j}(S_{i,j})\), as an approximation to solving the optimal transport problem,

\[K(X/)=*{argmax}_{S_{N}} S,X_{F}+  H(S).\] (6)

Here \(K()\) is the Sinkhorn-Knopp operator, which maps the positive orthant on to \(B_{N}\) by iteratively normalising rows and columns [49; 50]; \(X\) is the unnormalised assignment probability (transport cost) matrix; and \(\) is a temperature parameter, analogous to the temperature-dependent softmax function for discrete categories . In our context, \(X\) corresponds to the event likelihood distributions given by Equation 2; as such, we are looking to find the permutation matrix, \(S\), that transports event probabilities to their optimal location in the event sequence, \(s\). Alternatively, we can think of the relationship as \(S\) being the transport plan that permutes event likelihoods in \(X\) to their optimal position in the latent event sequence.

To obtain a hard permutation from \(S\), we use a result from , who showed that \(M(X)\), the hard permutation matrix of discrete matches (i.e., the matrix of basis vectors corresponding to the vertices of the Birkhoff polytope), can be obtained as the limit \( 0\) of the Sinkhorn-Knopp operator,

\[M(X)=e_{s(0)}\\ \\ e_{s(N)}=_{ 0}K(X/).\] (7)Here \(e_{n}\) are basis (one-hot) vectors of size \(N\) with a value of 1 in the \(n\)-th position and 0 everywhere else. In practice we compute the hard permutation matrix \(M(X)\) from \(S\) using the Hungarian algorithm [52; 53], which solves the minimum bipartite matching problem in cubic time. We use the relation in Equation 7 to show that the original EBM can be obtained as the temperature limit of the vEBM (see Appendix A.3). To facilitate inference, the value of \(\) must be chosen to balance between the limit of a hard permutation, where the gradients are discontinuous (and hence non-differentiable), and a uniform soft permutation, where the gradients are flat (and hence non-informative). A parametric analysis of \(\), \(_{}\), and the number of Sinkhorn-Knopp iterations, \(n_{s}\), is presented in Appendix A.8.

### Variational permutation inference

We approximate the posterior probability, obtained from applying Bayes rule to Equation 3, using variational inference , and define the evidence lower bound (ELBO). To enable differentiability of the ELBO, we parameterise our variational prior and posteriors using the Gumbel-Sinkhorn distribution, \(G(X,)\), with a matrix, \(\), of i.i.d. Gumbel noise,

\[G(X,) K((X+)/).\] (8)

The Gumbel-Sinkhorn distribution effectively implements the reparameterisation trick  for permutations, and in the limit \( 0\) it has been shown to converge to the Gumbel-Matching distribution, the equivalent of the Gumbel-Sinkhorn distribution for hard matchings . We choose a uniform prior over permutations, \(G(X=0,_{})\), and for the posterior, \(G(X,;)\), with parameters \(\). We seek to optimise the corresponding ELBO,

\[P(Y)(;) =_{q_{}(Z|Y)}[P_{0}(Y|Z)-(q_{ }(Z|Y)\|P(Z))]\] (9) \[=_{q_{}(Z|Y)}[P_{}(Y|Z)-( G_{}(X,)\|G(X=0,_{}))].\]

The Kullback-Leibler (KL) divergence term on the RHS of Equation 9 is intractable, but can be rewritten as \(((X+)/\|/_{})\) by substituting \((X+)/\) for \(Z\) and estimated using random sampling . For completeness, we restate the full term derived by  in Appendix A.4.

### Inference scheme

To optimise the ELBO we use Adam  with \(n_{}=200\) iterations and a learning rate of 0.1. Temperature hyperparameters \(\) and \(_{}\) were set to 1 for all experiments, except the mixed events (Section 3.3.3), where \(=1E3\). We found setting \(=0\) during inference gave the fastest and most accurate estimate of \(S\), at the expense of not allowing for direct propagation of uncertainty. While uncertainty estimation is not the focus of this paper, we do provide some examples of setting \(\) non-zero in Appendix A.7. Pseudo-code for the full inference scheme is given in Appendix Algorithm 1.

### Probabilistic staging

We can use the trained model to obtain an individual-level likelihood distribution over stages, i.e., the likelihood at each state \(k\) given by Equation 3, where stage \(k=n\) corresponds to the first \(n\) events occurred and the remaining \(N-n\) events not occurred. Here we simply take the maximum likelihood stage for each individual, but alternative summary statistics could be used.

## 3 Experiments

### Baselines

We consider two baselines; i) the original EBM ; and the Alzheimer's Disease Probabilistic Cascades (ALPACA) model , both of which use maximum likelihood to estimate the ordered sequence, \(s\). The EBM learns \(s\) using gradient descent and Markov Chain Monte Carlo (MCMC) sampling. The ALPACA model instead defines \(s\) as the central permutation of a Mallows model , with a density over permutations given by \(p(s)(- d(s,s_{0}))\), where \(\) scales the spread around the central ordering, \(s_{0}\), and \(d(s,s_{0})=_{i=1}^{N} s(i)-s_{0}(i)\) is the distance between permutations. The ALPACA model learns \(s\) using expectation-maximisation (EM) and Gibbs sampling. We use the default parameters; for the EBM, \(10^{3}\) gradient descent iterations with \(10\) initial seeds, and \(10^{6}\) MCMC samples; for ALPACA, 10 EM iterations and 100 Gibbs samples.

### Synthetic data

To enable comparison between our model and the baselines, we simulate data generated by an ordered sequence, \(s\), according to the limit version of Equation 3 (see Appendix A.3). In brief, \(s\) is randomly initialised and individuals are assigned a stage with uniform probability, reflecting that individuals can be observed at any disease stage. Individuals are assigned as either controls or patients according to an arbitrary threshold on the disease stage (here we choose the lowest 20% stages as controls). Feature data for each individual are then generated from the Gaussian models of normal and abnormal feature values, depending on their stage, with zero means for the control distributions, random uniform means for the case distributions, and variable standard deviations for both controls and cases set to achieve a desired level of noise. Exact parameter values and code to generate synthetic data is given in the GitHub repository. Here we repeat performance evaluation over 10 simulated datasets for each experiment to support statistical significance tests.

#### 3.2.1 Faster inference

Figure 2 shows the runtime for the vEBM and baselines for three experiments (\(I=100\), \(J=10\); \(I=1000\), \(J=100\); \(I=200\), \(J=200\)). We do not consider \(J>200\) here, as the baselines become intractable, but Figure 2 clearly illustrates the unique computational ability of the vEBM to work with much larger \(J\), as we demonstrate throughout this section. The vEBM is a factor of 1000 times faster for the \(J=200\) experiment; this factor would only increase for larger models.

#### 3.2.2 Improved accuracy and robustness to noise

Figure 3 shows the effect of increasing aleatoric (measurement) noise levels on inference accuracy, as measured by the Kendall's tau  between the true and inferred sequences. The vEBM outperforms or is comparable to the baselines in all datasets and noise settings, except for \(I=100,J=1000\) and \(=0.1\); this is expected, because at low noise and smaller numbers of features the EBM's MCMC sampling should find the global minimum, while the vEBM will always have some uncertainty due to its variational approximation. Statistical significance was obtained at \(p<0.001\) using unpaired t-tests (note that only one datapoint is shown for ALPACA at \(J=100\), and none at \(J=200\), due to computational intractability). We highlight that the metric is sensitive to any departure from the correct ordering, even by a single sequence position; accordingly the visual correlation between the true and inferred sequence remains high, e.g., even at the highest noise (\(=1\), corresponding to a 1:1 signal:noise ratio), as the relationship is still approximately diagonal. Additional examples in other datasets and when setting the Gumbel noise, \(\), non-zero are given in Appendix A.6, A.7.

### Alzheimer's disease data

We use pre-processed tensor-based morphometry (TBM) data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study, a longitudinal observational study of AD. TBM data are derived from structural magnetic resonance imaging (MRI) data and represent voxel-level maps of intensity gradients with respect to a reference healthy brain template, providing a standardised measure of voxel-level volume loss (or gain) between individuals. The TBM dataset we use here is comprised of cross-sectional TBM maps from 816 individuals (299 controls, 399 mild cognitive impairment, 188 AD) . In Section 3.3.3 we also use three cognitive test scores - Mini-Mental State Examination (MMSE); Clinical Rating Dementia scale Sum of Boxes (CDRSB); Rey Auditory Verbal Learning Test (RAVLT). Both datasets are available to download for users with an ADNI account

Figure 2: Speed of inference as a function of model size, for the vEBM and baselines. Note that there is no datapoint for ALPACA at \(J=200\) due to computational intractability.

(https://adni.loni.usc.edu/data-samples/access-data/, data collections: "TBM Jacobian Maps MDT-SC"; "Tadpole Challenge").

#### 3.3.1 Pixel-level disease progression events in AD

We apply the vEBM to TBM data from ADNI to reveal the first pixel-level sequence of disease events in AD (Figure 4). We do not include the baselines here due to computational intractability (as demonstrated in Section 3.2). The pattern of change represented by the vEBM sequence recapitulates known large-scale changes due to AD; initial change in the ventricles, followed by other sub-cortical changes, then changes across the cortex . Moreover, the vEBM finds a detailed pattern of grey and white matter changes throughout the sequence, providing new small-scale insights into AD aetiology that have not previously been possible, which we explore further in the next section.

Figure 4: Pixel-level disease progression sequence in AD obtained by the vEBM. White pixels correspond to events that have occurred by the corresponding point of the sequence. The figure shows 10 sequence positions at uniform steps of 100 across the total of 1344, with the top left figure corresponding to position 50 (the first 50 events have occurred) and the bottom right to position 950. Images were made from the vEBM output using 3D Slicer (https://www.slicer.org/).

Figure 3: Accuracy of inference as a function of aleatoric noise, obtained by the vEBM from synthetic data with low, medium, and high noise levels (left: \(=0.1\); middle: \(=0.5\); right: \(=1\)). Top row: Kendallâ€™s tau distance between the inferred and true sequences as a function of model size (number of features). Standard errors of the mean are shown, from 10 repeats per experiment. Bottom row: example positional variance diagrams. The vertical axis lists the sequence of events inferred by the vEBM with the earliest event (order position 1) at the top. The true sequence is overlaid as red squares. Datasets have \(I=2000\) individuals and \(J=200\) features.

#### 3.3.2 Segmentation-based interpretation of pixel-level events

To evaluate our ADNI pixel-level model with respect to previous analyses that have used segmented regional brain areas, we map the vEBM pixel-level events post hoc to pixel-level labels obtained from the FreeSurfer segmentation of the reference template (Figure 5). Note that the regions shown are a subset of the total regions available from the FreeSurfer segmentation tool, which were chosen according to those that were sufficiently represented in terms of number of pixels in the 2D slice that was used to train the model (\(N>10\)). Also note that the number of points on each trajectory / line corresponds to the number of pixels available in each region; e.g., there are few pixels in "Putamen" (\(N=11\)), an intermediary number in "Thalamus-Proper" (\(N=43\)), and many in "Cerebral-Cortex" (\(N=299\)), corresponding to their relative sizes (areas) in the 2D slice.

Our findings are in broad agreement with previous results; sub-cortical changes (Thalamus-Proper, Putamen, Hippocampus) are earliest, followed by cortical (Cerebral-Cortex) and white matter (Cerebral-White-Matter), and finally ventricular change (Lateral-Ventric, VentralDC). However, our model provides much more fine-grained insights than, e.g., , we now obtain continuous trajectories of change, which capture interesting non-linearities, e.g., in the Thalamus-Proper, Brain-Stem, and Lateral-Ventric; this contrasts with the more linear changes in the Hippocampus, Cerebral-Cortex, and Cerebral-White-Matter.

#### 3.3.3 Mixed feature disease progression events in AD

The vEBM is not limited to modelling only image-based features, which we demonstrate by including three cognitive test score features (MMSE, CDRSB, RAVLT) and re-training the model. Figure 6 represents the spatio-(pseudo)-temporal pixel event topology obtained by the vEBM as a 2D histogram, and shows the position of the cognitive events by vertical lines. We calculate the spatial distribution of pixel events according to their Euclidean distance from the centre of the image. The colour denotes the number of pixel events in each histogram bin, e.g., in the first bin of events (the first column), we can see the density of pixel events occurring as a function of the distance from the centre. The pixel event topology shows the earliest events near the centre of the brain, as expected , before spreading out across the brain; these events are interleaved with cognitive events, which occur across the latter two thirds of the progression. This interleaving suggests that the vEBM could be used to provide fine-grained staging in between cognitive events, e.g., for stratification in clinical trials. Interestingly, the pixel event topology is asymmetric about the central axis of the brain in the

Figure 5: Trajectories of regional brain areas in our ADNI cohort, obtained by mapping the vEBM pixel-level events to pixel-level labels obtained from the FreeSurfer segmentation of the reference template. The horizontal axis shows the event number (from \(0-1344\)), and the vertical axis shows the fraction of pixel-events that have occurred in each regional brain area at the corresponding event number, as defined by the vEBM event sequence.

early stages, suggesting that the vEBM can identify subgroups of individuals who display asymmetric progression, which has previously been reported in small groups of people with AD, e.g., .

### Age-related macular degeneration data

We use pre-processed optical coherence tomography (OCT) data from the Duke University (DU) Ophthalmology 2013 dataset, a cross-sectional study of AMD . The OCT data represent thickness maps for retinal pigment epithelium and drusen complex (RPEDC), a marker of AMD progression. The OCT dataset is comprised of 384 individuals (115 controls, 269 AMD), and is publicly available to download: https://duke.app.box.com/s/180j6ziooeyylee07edy0il32zbyyzbg. To select only pixels with disease signal, we remove pixels with an effect size less than 4 on a pixel-wise t-test between the control and AMD groups.

#### 3.4.1 Pixel-level disease progression events in AMD

We apply the vEBM to OCT data from the DU cohort to reveal the first pixel-level sequence of disease events in AMD (Figure 7). The density of RPEDC spread around the centre of the eye reflects previous observations , and the vEBM provides a much finer-detailed progression pattern.

### Prediction of AD and AMD stage

Figure 8 shows the stage distribution for individuals in the ADNI and DU cohorts using the vEBM trained on mixed and pixel-only data, respectively. We find a fine-grained distribution of individual-level stages that reflects the clinical labels, demonstrating the utility of the vEBM for stratification tasks, e.g., to select cohorts for clinical trials .

Figure 6: Mixed pixel and cognitive feature event topology in AD found by the vEBM. The vertical axis shows the distance from the centre of the image to the pixel event, and the horizontal axis shows the event ordering obtained by the vEBM. Note that the cognitive events, denoted by vertical lines, are assigned an arbitrary distance of zero.

Figure 7: Pixel-level disease progression sequence in AMD obtained by the vEBM. White pixels correspond to events that have occurred by the corresponding point of the sequence. We have selected 10 sequence positions at uniform steps of 50 across the total of 537 in the full sequence, with the top left figure corresponding to position 80 and the bottom right to position 530. Images were made from the vEBM output using 3D Slicer (https://www.slicer.org/).

## 4 Discussion

We introduced the vEBM, a novel optimal transport formulation for discrete disease progression models that scales to hitherto impossible numbers of events. It allows pixel-level visualisation of the order of pathology appearance in chronic disease, as demonstrated for the first time in AD and AMD.

### Limitations

Here we did not fully explore model uncertainty, which as a Bayesian model, the vEBM can estimate directly; we reserve this for future work. We acknowledge that a model ordering several thousand events is not fully identifiable with only a few hundred snapshots, but the orderings we obtain are still highly meaningful, as demonstrated by our results in AD and AMD. Moreover, our optimal transport formulation naturally lends itself to feature-sparsification , allowing redundant events to be grouped together. While the vEBM can in principal be directly applied to raw image data, pre-processing of images to a common reference frame (i.e., image registration) is necessary to facilitate comparison between individuals; however pre-processing is necessary for any data type to be in a common reference frame (or scale). We do not explicitly account for feature-wise covariance in the model, e.g., in image-based data we would expect a high degree of collinearity between neighbouring pixels; this could be addressed by including an additional term that imposes local structure, e.g., a Markov random field . Finally, we note that the main limitation on the current formulation is not computational tractability but computer memory due to dense matrix operations, which could be alleviated using, e.g., sparse matrix representation.

### Broader impacts

The vEBM enables disease progression modelling at scale in multiple areas of medical imaging, not only the modalities demonstrated in this paper; such as other MRI modalities e.g., diffusion weighted imaging, microstructure modelling, connectivity; other imaging modalities, e.g., positron emission tomography, computed tomography, X-rays, ultrasound; and non-radiological imaging modalities, e.g., microscopy. Furthermore, the vEBM can run quickly on a relatively low-spec computer without the need for GPU infrastructure, making it accessible to research labs - and potentially clinics - that have limited resources, while further minimising its carbon impact by reducing compute time. In addition, it provides a new, more powerful, model for each component of mixture subtype models, e.g., , which currently uses a variant of the basic EBM. Such models are highly influential in stratifying patients into disease subgroups for more precise clinical trials and treatment deployment.