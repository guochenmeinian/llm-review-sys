# Energy Transformer

Benjamin Hoover

IBM Research

Georgia Tech

benjamin.hoover@ibm.com

&Yuchen Liang

Department of CS

RPI

liangy7@rpi.edu

&Bao Pham

Department of CS

RPI

phamb@rpi.edu

&Rameswar Panda

MIT-IBM Watson AI Lab

IBM Research

rpanda@ibm.com

&Hendrik Strobelt

MIT-IBM Watson AI Lab

IBM Research

hendrik.strobelt@ibm.com

&Duen Horng Chau

College of Computing

Georgia Tech

polo@gatech.edu

&Mohammed J. Zaki

Department of CS

RPI

zaki@cs.rpi.edu

&Dmitry Krotov

MIT-IBM Watson AI Lab

IBM Research

krotov@ibm.com

B.Hoover, Y.Liang, and B.Pham equally contributed to this work.

###### Abstract

Our work combines aspects of three promising paradigms in machine learning, namely, attention mechanism, energy-based models, and associative memory. Attention is the power-house driving modern deep learning successes, but it lacks clear theoretical foundations. Energy-based models allow a principled approach to discriminative and generative tasks, but the design of the energy functional is not straightforward. At the same time, Dense Associative Memory models or Modern Hopfield Networks have a well-established theoretical foundation, and allow an intuitive design of the energy function. We propose a novel architecture, called the Energy Transformer (or ET for short), that uses a sequence of attention layers that are purposely designed to minimize a specifically engineered energy function, which is responsible for representing the relationships between the tokens. In this work, we introduce the theoretical foundations of ET, explore its empirical capabilities using the image completion task, and obtain strong quantitative results on the graph anomaly detection and graph classification tasks.

## 1 Introduction

Transformers have become pervasive models in various domains of machine learning, including language, vision, and audio processing. Every transformer block uses four fundamental operations: attention, feed-forward multi-layer perceptron (MLP), residual connection, and layer normalization. Different variations of transformers result from combining these four operations in various ways. For instance,  proposes to frontload additional attention operations and backload additional MLP layers in a sandwich-like manner instead of interleaving them,  prepends an MLP layer before the attention in each transformer block,  uses neural architecture search methods to evolve even more sophisticated transformer blocks, and so on. Various methods exist to approximate the attention operation, multiple modifications of the norm operation, and connectivity of the block; see, for example,  for a taxonomy of different models. At present, however, the search for new transformerarchitectures is driven mostly by empirical evaluations, and the theoretical principles behind this growing list of architectural variations is missing.

Additionally, the computational role of the four elements remains the subject of discussions. Originally,  emphasized attention as the most important part of the transformer block, arguing that the learnable long-range dependencies are more powerful than the local inductive biases of convolutional networks. On the other hand more recent investigations  argue that the entire transformer block is important. The "correct" way to combine the four basic operations inside the block remains unclear, as does an understanding of the core computational function of the entire block and each of its four elements.

On the other hand, Associative Memory models, also known as Hopfield Networks , have been gaining popularity in the machine learning community thanks to theoretical advancements pertaining to their memory storage capacity and novel architectural modifications . Specifically, it has been shown that increasing the sharpness of the activation functions can lead to super-linear  and even exponential  memory storage capacity for these models, which is important for machine learning applications. This new class of Hopfield Networks is called Dense Associative Memories or Modern Hopfield Networks. Study  additionally describes how the attention mechanism in transformers is closely related to a special model of this family with the \(\) activation function.

There are high-level conceptual similarities between transformers and Dense Associative Memories, since both architectures are designed for some form of denoising of the input. Transformers are typically pre-trained on a masked-token task, e.g., in the domain of Natural Language Processing (NLP), certain tokens in the sentence are masked and the model predicts the masked tokens. Dense Associative Memory models are designed for completing the incomplete patterns. They can be trained in a self-supervised way by predicting the masked parts of the pattern, or denoising the pattern.

There are also high-level differences between the two approaches. Associative Memories are recurrent networks with a global energy function so that the network dynamics converges to a fixed point attractor state corresponding to a local minimum of the energy function. Transformers are typically not described as dynamical systems at all. Rather, they are thought of as feed-forward networks built from the four computational elements discussed above. Even if one thinks about them as dynamical systems with tied weights, e.g., , there is no reason to expect that their dynamics converge to a fixed point attractor (see the discussion in ).

Additionally, a recent study  uses a form of Majorization-Minimization algorithms  to interpret the forward path in the transformer block as an optimization process. This interpretation requires imposing certain constraints on the operations inside the block, and attempting to find an energy function that describes the constrained block. We take a complementary approach by using the intuition developed in Associative Memory models to _start_ with an energy function that is engineered for the problem of interest. The optimization process and the resulting architecture of the transformer block in our approach is a _consequence_ of this specifically chosen energy function.

Concretely, we use the recent theoretical advancements and architectural developments in Dense Associative Memories to design an energy function tailored to route the information between the tokens. The goal of this energy function is to represent the relationships between the semantic contents of tokens describing a given data point (e.g., the relationships between the contents of the image patches in the vision domain, or relationships between the nodes' attributes in the graph domain). The core mathematical idea of our approach is that the sequence of these unusual transformer blocks, which we call the Energy Transformer (ET), minimizes this global energy function. Thus, the sequence of conventional transformer blocks is replaced with a single ET block, which iterates the token representations until they converge to a fixed point attractor state. In the image domain, this fixed point corresponds to the completed image with masked tokens replaced by plausible auto-completions of the occluded image patches. In the graph domain, the fixed point reveals the anomaly status of a node given its neighbors, see Figure 1, or the graph label. The energy function in our ET block is designed with the goal to describe the _relationships between the tokens_. Examples of relationships in the image domain are: straight lines tend to continue through multiple patches, given a face with one eye being masked the network should inpaint the missing eye, etc. In the graph domain, these are the relationships between the features of nodes; or features of nodes and graph label in graph classification.

The core mathematical principle of the ET block - the existence of the global energy function - dictates strong constraints on the possible operations inside the block, the order in which these operations are executed in the inference pass, and the symmetries of the weights in the network. As a corollary of this theoretical principle, the attention mechanism of ET is different from the attention mechanism commonly used in feed-forward transformers . Lastly, our network may be viewed as an example of a broader class of Energy-Based Models  frequently discussed in the AI community. The proposed model is defined through the specific choice of the energy function, which, on the one hand, is suitable for the computational task of interest, and, on the other hand, results in optimization equations closely related to the forward pass in feed-forward transformers.

## 2 Energy Transformer Block

We now introduce the theoretical framework of the ET network. For clarity of presentation, we use language associated with the image domain. For the graph domain, one should think about "image patches" as nodes on the graph.

The overall pipeline is similar to the Vision Transformer networks (ViTs)  and is shown in Figure 1. An input image is split into non-overlapping patches. After passing these patches through the encoder and adding the positional information, the semantic content of each patch and its position is encoded in the token \(x_{iA}\). In the following the indices \(i,j,k=1...D\) are used to denote the token vector's elements, indices \(A,B,C=1...N\) are used to enumerate the patches and their corresponding tokens. It is helpful to think about each image patch as a physical particle, which has a complicated internal state described by a \(D\)-dimensional vector \(_{A}\). This internal state describes the identity of the particle (representing the pixels of each patch), and the particle's positional embedding (the patch's

Figure 1: Overview of the Energy Transformer (ET). Instead of a sequence of conventional transformer blocks, a single recurrent ET block is used. The operation of this block is dictated by the global energy function. The token representations are updated according to a continuous time differential equation with the time-discretized update step \(=dt/\). On the image domain, images are split into non-overlapping patches that are linearly encoded into tokens with added learnable positional embeddings (POS). Some patches are randomly masked. These tokens are recurrently passed through ET, and each iteration reduces the energy of the set of tokens. The token representations at or near the fixed point are then decoded using the decoder network to obtain the reconstructed image. The network is trained by minimizing the mean squared error loss between the reconstructed image and the original image. On the graph domain, the same general pipeline is used. Each token represents a node, and each node has its own positional encoding. The token representations at or near the fixed point are used for the prediction of the anomaly status of each node, or the graph label.

location within the image). The ET block is described by a continuous time differential equation, which describes interactions between these particles. Initially, at \(t=1\) the network is given a set containing two groups of particles corresponding to open and masked patches. The "open" particles know their identity and location in the image. The "masked" particles only know where in the image they are located, but are not provided the information about what image patch they represent. The goal of ET's non-linear dynamics is to allow the masked particles to find an identity consistent with their locations and the identities of open particles. This dynamical evolution is designed so that it minimizes a global energy function, and is guaranteed to arrive at a fixed point attractor state. The identities of the masked particles are considered to be revealed when the dynamical trajectory reaches the fixed point. Thus, the central question is: how can we design the energy function that accurately captures the task that the Energy Transformer needs to solve?

The masked particles' search for identity is guided by two pieces of information: identities of the open particles, and the general knowledge about what patches are in principle possible in the space of all possible images. These two pieces of information are described by two contributions to the ET's energy function: the energy based attention and the Hopfield Network, respectively, for reasons that will become clear in the next sections. Below we define each element of the ET block in the order they appear in Figure 2.

#### Layer Norm

Each token, or a particle, is represented by a vector \(^{D}\). At the same time, most of the operations inside the ET block are defined using a layer-normalized token representation

\[g_{i}=-}{_{j}(x_{j}- )^{2}+}}+_{i},= _{k=1}^{D}x_{k}\] (1)

The scalar \(\) and the vector elements \(_{i}\) are learnable parameters, \(\) is a small regularization constant. Importantly, this operation can be viewed as an activation function for the neurons and can be defined as a partial derivative of the Lagrangian function (see  for the discussion of this property)

\[L=D_{j}(x_{j}-)^{2}+ }\ +\ _{j}_{j}x_{j}, g_{i}=}\] (2)

Figure 2: **Left**: Inside the ET block. The input token \(^{(t)}\) passes through a sequence of operations and gets updated to produce the output token \(^{(t+1)}\). The operations inside the ET block are carefully engineered so that the entire network has a global energy function, which decreases with time and is bounded from below. In contrast to conventional transformers, the ET-based analogs of the attention module and the feed-forward MLP module are applied in parallel as opposed to consecutively. **Center**: The cosine similarity between the learned position embedding of each patch and every other patch. In each cell, the brightest patch indicates the cell of consideration. **Right**: 100 selected memories stored in the HN memory matrix, visualized by the decoder as 16x16 RGB image patches. This visualization is unique to our model, as traditional Transformers cannot guarantee image representations in the learned weights.

### Multi-Head Energy Attention

The first contribution to the ET's energy function is responsible for exchanging information between the particles (tokens). Similarly to the conventional attention mechanism, each token generates a pair of queries and keys (ET does not have a separate value matrix; instead the value matrix is a function of keys and queries). The goal of the energy based attention is to evolve the tokens in such a way that the keys of the open patches are aligned with the queries of the masked patches in the internal space of the attention operation. Below we use index \(=1...Y\) to denote elements of this internal space, and index \(h=1...H\) to denote different heads of this operation. With these notations the energy-based attention operation is described by the following energy function:

\[E^{}=-_{h=1}^{H}_{C=1}^{N}( _{B C}( A_{hBC}))\] (3)

where the attention matrix \(A_{hBC}\) is computed from query and key tensors as follows:

\[ A_{hBC}&=_{}K_{ hB}\ Q_{  hC}, 14.226378pt^{H N N}\\ K_{ hB}&=_{j}W_{ hj}^{K}\ g_{jB},  28.452756pt^{Y H N}\\ Q_{ hC}&=_{j}W_{ hj}^{Q}\ g_{jC},  28.452756pt^{Y H N}\] (4)

and the tensors \(^{K}^{Y H D}\) and \(^{Q}^{Y H D}\) are learnable parameters.

From the computational perspective each patch generates two representations: query (given the position of the patch and its current content, where in the image should it look for the prompts on how to evolve in time?), and key (given the current content of the patch and its position, what should be the contents of the patches that attend to it?). The log-sum energy function (3) is minimal when for every patch in the image its queries are aligned with the keys of a small number of other patches connected by the attention map. Different heads (index \(h\)) contribute to the energy additively.

### Hopfield Network Module

The next step of the ET block, which we call the Hopfield Network (HN), is responsible for ensuring that the token representations are consistent with what one expects to see in realistic images. The energy of this sub-block is defined as:

\[E^{}=-_{B=1}^{N}_{=1}^{K}G_{j=1}^{D}_{ j} \ g_{jB}, 28.452756pt^{K D}\] (5)

where \(_{ j}\) is a set of learnable weights (memories in the Hopfield Network), and \(G()\) is an integral of the activation function \(r()\), so that \(G()^{}=r()\). Depending on the choice of the activation function this step can be viewed either as a classical continuous Hopfield Network  if the activation function grows slowly (e.g., \(r()=\)ReLU), or as a modern continuous Hopfield Network  if the activation function is sharply peaked around the memories (e.g., \(r()=\) power, or softmax). The HN sub-block is analogous to the feed-forward MLP step in the conventional transformer block but requires that the weights of the projection from the token space to the hidden neurons' space to be the same (transposed matrix) as the weights of the subsequent projection from the hidden space to the token space. Thus, the HN module here is an MLP with shared weights that is _applied recurrently_. The energy contribution of this block is low when the token representations are aligned with some rows of the matrix \(\), which represent memories, and high otherwise.

### Dynamics of Token Updates

The inference pass of the ET network is described by the continuous time differential equation, which minimizes the sum of the two energies described above

\[}{dt}=-}, 14.226378pt  14.226378ptE=E^{}+E^{}\] (6)Here \(x_{iA}\) is the token representation (input and output from the ET block), and \(g_{iA}\) is its layer-normalized version. The first energy is low when each patch's queries are aligned with the keys of its neighbors. The second energy is low when each patch has content consistent with the general expectations about what an image patch should look like (memory slots of the matrix \(\)). The dynamical system (6) finds a trade-off between these two desirable properties of each token's representation. For numerical evaluations, equation (6) is discretized in time.

To demonstrate that the dynamical system (6) minimizes the energy, consider the temporal derivative

\[=_{i,j,A}}\ }{  x_{jA}}\ }{dt}=-_{i,j,A}}\ M^{A}_{ij}\ } 0\] (7)

The last inequality sign holds if the symmetric part of the matrix

\[M^{A}_{ij}=}{ x_{jA}}=L}{  x_{iA} x_{jA}}\] (8)

is positive semi-definite (for each value of index \(A\)). The Lagrangian (2) satisfies this condition.

#### Relationship to Modern Hopfield Networks and Conventional Attention

One of the theoretical contributions of our work is the design of the energy attention mechanism and the corresponding energy function (3). Although heavily inspired by prior work on Modern Hopfield Networks, our approach is fundamentally different from it. Our energy function (3) may look somewhat similar to the energy function of a continuous Hopfield Network with the \(\) activation function. The main difference, however, is that in order to use Modern Hopfield Networks recurrently (as opposed to applying their update rule only once) the keys must be constant parameters (called memories in the Hopfield language). In contrast, in our energy attention network the keys are _dynamical variables_ that evolve in time with the queries.

To emphasize this further, it is instructive to write explicitly the ET attention contribution to the update dynamics (6). It is given by (for clarity, assume only one head of attention):

\[-}}{ g_{iA}}=_{C A}_{}W^ {Q}_{ i}\ K_{ C}\ }_{}K_{ C}\ Q_{ A} +W^{K}_{ i}\ Q_{ C}\ }_{}K_{ A}\ Q_{  C}\]

In both terms the \(\) normalization is done over the token index of the keys, which is indicated by the subscript in the equation. The first term in this formula is the conventional attention mechanism  with the value matrix equal to \(=(^{Q})^{T}=_{}W^{Q}_{ i}K_{  C}\). The second term is the brand new contribution that is missing in the original attention mechanism. The presence of this second term is crucial to make sure that the dynamical system (6) minimizes the energy function if applied recurrently. This second term is the main difference of our approach compared to the Modern Hopfield Networks. The same difference applies compared to the other recent proposals .

Lastly, we want to emphasize that our ET block contains two different kinds of Hopfield Networks acting in parallel, see Figure 2. The first one is the energy attention module, which is inspired by, but not identical to, Modern Hopfield Networks. The second one is the "Hopfield Network" module, which can be either a classical or Modern Hopfield Network. These two should not be confused.

For completeness, the contribution of the "Hopfield Network" module to the update equation (6) can be written as

\[-}}{ g_{iA}}=_{=1}^{K}_{ i}G^ {}_{j=1}^{D}_{ j}g_{jA}\] (9)

which is applied to every token individually (there no mixing of different tokens).

## 3 Qualitative Inspection of the ET framework on ImageNet

We trained* the ET network on the masked image completion task using ImageNet-1k dataset . Each image was broken into non-overlapping patches of 16x16 RGB pixels, which were projectedwith a single affine encoder into the token space. Half of these tokens were "masked", by replacing them with a learnable MASK token. A distinct learnable position encoding vector was added to each token. Our ET block then processes all tokens recurrently for \(T\) steps. The token representations after \(T\) steps are passed to a simple linear decoder (consisting of a layer norm and an affine transformation). The loss function is the standard MSE loss on the occluded patches. See more details on the implementation and the hyperparameters in Appendix B.

Examples of occluded/reconstructed images (unseen during training) are shown in Figure 3. In general, our model learns to perform the task very well, capturing the texture in dog fur (column 3) and understanding meaningful boundaries of objects. However, we observe that our single ET block struggles to understand some global structure, e.g., failing to capture both eyes of the white dog (column 5) and completing irregular brick patterns in the name of extending the un-occluded borders (last column). We additionally inspect the positional encoding vectors associated with every token, Figure 2, where the model learns a locality structure in the image plane that is very similar to the original ViT . The positional embedding of each image patch has learned higher similarity values for neighboring tokens than for distant tokens.

Our network is unique compared to standard ViTs in that the iterative dynamics only _move_ tokens around in the same space from which the final fixed point representation can be decoded back into the image plane. This functionality makes it possible to visualize essentially any _token representation_, _weight_, or _gradient of the energy_ directly in the image plane. This feature is highly desirable from the perspective of interpretability, since it makes it possible to track the updates performed by the network directly in the image plane as the computation unfolds in time. In Figure 2 this functionality is used for inspecting the learned weights of the HN module directly in the image plane. According to our theory, these weights should represent basis vectors in the space of all possible image patches. These learned representations look qualitatively similar to the representations typically found in networks trained on image datasets, e.g., .

Figure 4: Token representations and gradients are visualized using the decoder at different times during the dynamics. The Energy Attention (ATTN) block contributes general structure information to the masked patches at _earlier_ time steps, whereas the Hopfield Network (HN) significantly sharpens the quality of the masked patches at _later_ time steps.

Figure 3: Reconstruction examples of our Energy Transformer using images from the ImageNet-1k validation set. _Top row:_ input images where 50% of the patches are masked with the learned MASK token. _Middle row:_ output reconstructions after 12 time steps. _Bottom row:_ original images.

We additionally visualize the gradients of the energy function (which are equal to the token updates, see Equation 6) of both ATTN block and the HN block, see Figure 4. Early in time, almost all signal to the masked tokens comes from the ATTN block, which routes information from the open patches to the masked ones; no meaningful signal comes from the HN block to the masked patch dynamics. Later in time we observe a different phenomenon: almost all signal to masked tokens comes from the HN module while ATTN contributes a blurry and uninformative signal. Thus, the attention module is crucial early in the network dynamics, feeding signal to masked patches from the visible patches, whereas the HN is crucial later in the dynamics as the model approaches the final reconstruction, sharpening the masked patches. All the qualitative findings presented in this section are in accord with the core computational strategy of the ET block as it was designed theoretically in section 2.

## 4 Graph Anomaly Detection

Having gained empirical insights in the image domain, we turn to quantitatively evaluating ET's performance on the graph anomaly detection problem*, a task with plenty of strong and recently published baselines. Graph Convolutional Networks (GCN)  have been widely used for this task due to their capability of learning high level representations of graph structures and node attributes [26; 27]. However, vanilla GCNs suffer from the over-smoothing problem . In each layer of the forward pass, the outlier node aggregates information from its neighbors. This averaging makes the features of anomalies less distinguishable from the features of benign or normal nodes. Our approach does not suffer from this problem, since the routing of the information between the nodes is done through the energy based attention, and the information aggregation is based on the attention scores.

Footnote *: The code is available: https://github.com/zhuergou/Energy-Transformer-for-Graph-Anomaly-Detection/.

For anomaly detection on graphs in the ET framework, consider an undirected graph with \(N\) nodes. Every node has a vector of raw attributes \(_{A}^{F}\), where \(F\) is the number of node's features. Every node also has a binary label \(l_{A}\) indicating whether the node is benign or anomalous. We focus on node anomaly and assume that all edges are trusted. The task is to predict the label of a node given the graph structure and the node's features. Since there are far more benign nodes in the graph than anomalous ones, anomaly detection can be regarded as an imbalanced node classification task.

First, similarly to images, the feature vectors for every node are converted to a token representation using a linear embedding \(\) and adding a learnable positional embedding \(_{A}\)

\[_{A}^{t=1}=\;_{A}+_{A}\] (10)

where the superscript \(t=1\) indicates the time of the update of the ET dynamics. This token representation is iterated through the ET block for \(T\) iterations. When the retrieval dynamics becomes

    & **Datasets** & **Split** & **GraphConsis** & **CAREGN** & **PC-GNN** & **BWGNN** & **MLP** & **GT** & **ET (Ours)** \\   &  & \(1\%\) & \(56.8_{ 2.8}\) & \(62.1_{ 1.3}\) & \(59.8_{ 1.4}\) & \(61.1_{ 0.4}\) & \(53.9_{ 0.2}\) & \(61.7_{ 0.4}\) & \(_{ 0.6}\) \\  & & \(40\%\) & \(58.7_{ 2.0}\) & \(63.3_{ 0.9}\) & \(63.0_{ 2.3}\) & \(71.0_{ 0.9}\) & \(57.5_{ 0.8}\) & \(68.7_{ 0.4}\) & \(_{ 0.1}\) \\  & & \(1\%\) & \(68.5_{ 3.4}\) & \(68.7_{ 1.6}\) & \(79.8_{ 5.6}\) & \(_{ 0.7}\) & \(47.6_{ 1.2}\) & \(88.6_{ 0.5}\) & \(89.3_{ 0.7}\) \\  & & \(40\%\) & \(75.1_{ 3.2}\) & \(86.3_{ 1.7}\) & \(80.5_{ 0.7}\) & \(92.2_{ 0.4}\) & \(79.1_{ 1.2}\) & \(91.7_{ 0.8}\) & \(_{ 0.3}\) \\  & & \(1\%\) & \(71.7\) & \(73.3\) & \(62.0\) & \(84.8\) & \(61.0\) & \(81.5\) & \(_{ 1.0}\) \\  & & \(40\%\) & \(73.4\) & \(77.5\) & \(63.1\) & \(86.8\) & \(70.5\) & \(83.6\) & \(_{ 1.0}\) \\  & & \(1\%\) & \(52.4\) & \(55.8\) & \(51.1\) & \(75.9\) & \(50.0\) & \(64.3\) & \(_{ 0.7}\) \\   &  & \(1\%\) & \(66.4_{ 3.4}\) & \(75.0_{ 3.8}\) & \(_{ 0.9}\) & \(72.0_{ 0.5}\) & \(59.8_{ 0.4}\) & \(72.5_{ 0.6}\) & \(73.2_{ 0.8}\) \\  & & \(40\%\) & \(69.8_{ 3.0}\) & \(76.1_{ 2.9}\) & \(79.8_{ 0.1}\) & \(84.0_{ 0.9}\) & \(66.5_{ 1.0}\) & \(81.9_{ 0.5}\) & \(_{ 0.3}\) \\   & & \(1\%\) & \(74.1_{ 3.5}\) & \(88.6_{ 3.5}\) & \(90.4_{ 2.0}\) & \(89.4_{ 0.3}\) & \(83.6_{ 1.7}\) & \(89.0_{ 1.2}\) & \(_{ 1.0}\) \\   & & \(40\%\) & \(87.4_{ 3.3}\) & \(90.5_{ 1.6}\) & \(95.8_{ 0.1}\) & \(_{ 0.4}\) & \(89.8_{ 1.0}\) & \(95.4_{ 0.6}\) & \(97.3_{ 0.4}\) \\   & & \(1\%\) & \(90.2\) & \(90.5\) & \(90.7\) & \(91.1\) & \(82.9\) & \(90.0\) & \(_{ 1.1}\) \\   & & \(40\%\) & \(91.4\) & \(92.1\) & \(91.2\) & \(94.3\) & \(87.1\) & \(88.2\) & \(_{ 3.0}\) \\   & & \(1\%\) & \(65.2\) & \(71.2\) & \(59.8\) & \(88.0\) & \(56.3\) & \(81.4\) & \(_{ 0.6}\) \\   & & \(40\%\) & \(71.2\) & \(71.8\) & \(68.4\) & \(\) & \(56.9\) & \(82.5\) & \(93.9_{ 0.2}\) \\   

Table 1: Performance on Yelp, Amazon, T-Finance, and T-Social datasets with different training ratios. Following , mean and standard deviation over 5 runs with different train/dev/test split are reported (standard deviations are only included if they are available in the prior work). Best results are in **bold**. Our model is state of the art or near state of the art on every category.

stable, we have the final representation for each node \(_{A}^{t=T}\) (or more precisely \(_{A}^{t=T}\), since the outputs are additionally passed through a layer norm operation after the final ET update). This output is concatenated with the initial (layer normalized) token to form the final output

\[_{A}^{}=_{A}^{t=1}_{A}^{t=T}\] (11)

where \(||\) denotes concatenation. Following , the node representation \(_{A}^{}\) is fed into an MLP with a sigmoid activation function to compute the anomaly probabilities \(p_{A}\). The weighted cross entropy

\[=_{A}\ l_{A}(p_{A})+(1-l_{A})(1-p_{A}) \] (12)

is used to train the network. Above, \(\) is the ratio of the benign labels (\(l_{A}\) = 0) to anomalous ones (\(l_{A}\) = 1). Attention is restricted to 1-hop neighbors of the target node.

### Experimental Evaluation

Four datasets are used for the experiments. YelpChi dataset  aims at opinion spam detection in Yelp reviews. Amazon dataset is used to detect anomalous users under the Musical Instrument Category on _amazon.com_. T-Finance and T-Social datasets  are used for anomalous account detection in transactions and social networks, respectively. For these four datasets, the graph is treated as a homogeneous graph (i.e., all the edges are of the same type), and a feature vector is associated with each node. The task is to predict the label (anomaly status) of the nodes. For each dataset, either \(1\%\) or \(40\%\) of the nodes are used for training, and the remaining \(99\%\) or \(60\%\) are split \(1:2\) into validation and testing sets, see Appendix C for details.

We compare with state-of-the-art approaches for graph anomaly detection, which include GraphConsis , CAREGNN , PC-GNN  and BWGNN . Additionally, multi-layer perceptrons (MLP) and Graph Transformer (GT)  are included in the baselines for completeness. Following previous work, macro-F1 score (unweighted mean of F1 score) and the Area Under the Curve (AUC) are used as the evaluation metrics on the test datasets . See Appendix C for more details on training protocols and the hyperparameters choices. The results are reported in Table 1. Our ET network demonstrates very strong results across all the datasets.

    &  &  &  &  &  &  &  &  \\    & & & & & & & & \\    & \(78.5_{ 0.4}\) & \(87.5_{ 0.3}\) & \(85.9_{ 0.4}\) & \((1.5)\) & \(82.0_{ 0.3}\) & \((13.7)\) - & & \(85.8_{ 12.3}\) & \((14.2)\) - & - \\    & \(73.2_{ 0.4}\) & \((9.7)\) & \(84.5_{ 0.3}\) & \((3.0)\) & \((3.0)\) & \((15.4)\) & \((15.4)\) & \((11.7)\) - & - \\    & & & & & & & & & \\    & & & & & & & & & \\    & \(75.8_{ 0.6}\) & \((9.1)\) & - & - & - & \(71.3_{ 0.1}\) & \((7.1)\) - & - & \(78.9_{ 0.3}\) \\    & \(84.9_{ 0.6}\) & \(78.5_{ 0.8}\) & \((9.1)\) & \(80.7_{ 12.3}\) & \((6.7)\) & \(81.0_{ 13.1}\) & \((14.7)\) & \(68.5_{ 12.1}\) & \((9.6)\) - & \((82.2_{ 0.6}\)\) - \\    & \(77.3_{ 0.4}\) & \((7.6)\) & - & - & \(78.4_{ 0.4}\) & - & - & - \\    & \(80.0_{ 12.2}\) & \((4.9)\) & - & - & \(86.7_{ 0.5}\) & - & \(88.5_{ 1.7}\) & \((11.5)\) - & - \\    & \(73.4_{ 1.1}\) & \(71.5_{ 0.7}\) & \((13.3)\) & - & \(72.8_{ 1.4}\) & \((22.9)\) & \(44.5_{ 0.7}\) & \((34.9)\) & \(87.9_{ 1.7}\) & \((12.1)\) & \(77.9_{ 1.4}\) & \((4.3)\) - \\    & \(74.2_{ 0.8}\) & \((10.7)\) & \(71.5_{ 0.6}\) & \((16.0)\) & \(71.0_{ 0.6}\) & \((17.3)\) & \(76.9_{ 0.5}\) & \((18.8)\) - & - & \(66.3_{ 0.5}\) & \((12.6)\) \\    & - & - & - & - & \(55.7\) & \((22.7)\) & \(\) & - & - \\   & \(\) & \((5.4)\) & \(}\) & \((2.6)\) & \(}\) & \((3.1)\) & \(}\) & \((21.4)\) & \(}\) & \((3.4)\) & \(}\) & \((16.5)\) & \(}\) & \((20.9)\) \\   

Table 2: Graph classification performance on eight datasets of TUDataset. Following , mean and standard deviation obtained from 100 runs of 10-fold cross validation are reported. For baselines standard deviations are only included if they are available in prior work. If the entry is unavailable in prior literature it is denoted by ‘-’; best results are in **bold**. The performance difference between non-baseline approaches (including ours) and the baselines (specified by their gray cell) is indicated by \(\) (decrease), \(\)(increase), and \(\)(no change within the error bars) along with the value.

Graph Classification with ET

To fully explore the efficacy of ET, we further evaluate its performance on the graph classification problem1. Unlike the anomaly detection task, where we predict a label for every node, in the graph classification task, a single label is predicted for the entire graph.

Consider a graph \(G\), where each node has a raw feature vector \(_{A}^{F}\) with \(F\) feature-dimension. Each vector is projected to the token space yielding \(_{A}^{D}\), where \(D\) is the token dimension. A learnable CLS token \(_{}\) is concatenated to the set of tokens resulting in the token matrix \(^{(N+1) D}\), and a learnable linear projection of the top \(k\) smallest eigen-vectors of the normalized Laplacian matrix, following , is added to \(\). Graph structural information is provided to ET within the attention operation. We also use a stacked version of the Energy Transformer consisting of \(S\) ET blocks, where each block has the same number of temporal unfolding steps \(T\) and its own LayerNorm. The final representation of the CLS token \(_{}^{=S,\ t=T}\) is projected via a linear embedding into the predictor space. A softmax over the number of classes is applied to this predictor representation, and the cross-entropy loss is used to train the network. See Appendix D for full details.

### Experimental Evaluation

Eight graph datasets from the TUDataset  collection are used for experimentation. NCI1, NCI109, MUTAG, MUTAGENICITY, and FRANKENSTEIN are a common class of graph datasets consisting of small molecules with class labels representing toxicity or biological activity determined in drug discovery projects . Meanwhile, DD, ENZYMES, and PROTEINS represent macromolecules. The task for both DD and PROTEINS is to classify whether a protein is an enzyme. Lastly, for ENZYMES, the task is to assign enzymes to one of the six classes, which reflect the catalyzed chemical reaction . See Table 8 for more details of the datasets.

We compare ET with the current state-of-the-art approaches for the mentioned datasets, which include WKPI-kmeans , WKPI-kcenters , DSGCN , HGP-SL , U2GNN , and EvoG . Additionally, approaches [39; 43; 44; 45; 46], which are close to the baselines, are included to further contrast the performance of our model. Following the 10-fold cross validation process delineated in , accuracy score is used as the evaluation metric and reported in Table 2. In general, we have observed that the modified ET demonstrates strong performance across the eight datasets. With the exception of MUTAG, ET beats other methods on all the baselines by a substantial margin.

## 6 Discussion and Conclusions

A lot of recent research has been dedicated to understanding the striking analogy between Hopfield Networks and the attention mechanism in transformers. At a high level, the main message of our work is that the _entire_ transformer block (including feed-forward MLP, layer normalization, and residual connections) can be viewed as a single large Hopfield Network, not just attention alone. At a deeper level, we use recent advances in the field of Hopfield Networks to design a novel energy function that is tailored for dynamical information routing between the tokens; and representation of a large number of relationships between those tokens. We have tested the ET network qualitatively on the image completion task, and quantitatively on node anomaly detection and graphs classification. The qualitative investigation reveals the perfect alignment between the theoretical design principles of our network and its empirical computation. The quantitative evaluation demonstrates strong results, which stand in line or exceed the methods recently developed specifically for these tasks. We believe that the proposed network will be useful for other tasks and domains (e.g., NLP, audio, and video), which serve as future directions for a comprehensive investigation.

There are two metrics describing computational costs of our architecture: memory footprint of the model and the number of flops. In terms of the memory footprint our model wins (is smaller) compared to feedforward transformers with independent weights and even ALBERT-style shared-weight transformers (see Appendix G and Table 15). Regarding the number of flops, our model has the same parametric scaling as the conventional transformers (quadratic in the number of tokens), but the energy attention has a constant factor of two more flops (due to the second term in the attention-induced updates of the tokens).

Finally, we have developed an auto-grad framework called HAMUX  that makes building flexible energy-based architectures, including ET, convenient and efficient (see HAMUX).