# Hardness of Learning Neural Networks under the Manifold Hypothesis

Bobak T. Kiani

John A. Paulson School of Engineering and Applied Sciences, Harvard University; e-mail: bkiani@g.harvard.edu.

Jason Wang

Harvard College, Harvard University; e-mail: jasonwang1@college.harvard.edu.

Melanie Weber

John A. Paulson School of Engineering and Applied Sciences, Harvard University; e-mail: mweber@g.harvard.edu

Code is made public at https://github.com/Weber-GeoML/manifold-learning-complexity

###### Abstract

The _manifold hypothesis_ presumes that high-dimensional data lies on or near a low-dimensional manifold. While the utility of encoding geometric structure has been demonstrated empirically, rigorous analysis of its impact on the learnability of neural networks is largely missing. Several recent results have established hardness results for learning feedforward and equivariant neural networks under i.i.d. Gaussian or uniform Boolean data distributions. In this paper, we investigate the hardness of learning under the manifold hypothesis. We ask which minimal assumptions on the curvature and regularity of the manifold, if any, render the learning problem efficiently learnable. We prove that learning is hard under input manifolds of bounded curvature by extending proofs of hardness in the SQ and cryptographic settings for Boolean data inputs to the geometric setting. On the other hand, we show that additional assumptions on the volume of the data manifold alleviate these fundamental limitations and guarantee learnability via a simple interpolation argument. Notable instances of this regime are manifolds which can be reliably reconstructed via manifold learning. Looking forward, we comment on and empirically explore intermediate regimes of manifolds, which have heterogeneous features commonly found in real world data.4

## 1 Introduction

High-dimensional data is often thought to have low-dimensional structure, which may stem, for instance, from symmetries in the underlying system. This observation has given rise to the _manifold hypothesis_, which presumes that high-dimensional data lies on or near a low-dimensional manifold. Empirical studies have confirmed this hypothesis across domains [20; 77; 37; 65]. A plethora of algorithms for geometric data analysis [44; 8; 80; 21] and, more recently, machine learning [30; 15; 23] seek to leverage such structure. While these methods have shown empirical promise, few formal results on the benefits of encoding geometric structure have been established. Here, we study the impact of the manifold hypothesis on the computational complexity of learning algorithms for neural networks. Specifically, we investigate, under which geometric assumptions feedforward neural networks are efficiently learnable.

The computational hardness of learning shallow, fully-connected neural networks under i.i.d. Gaussian data distributions has been established by [36; 26; 48; 32], who show that the complexity of learning even single hidden layer neural networks grows at least exponentially with the input size. Some of this analysis utilizes the correlational statistical query (CSQ) framework, of which learnability via gradient decent is a notable instance. Recently,  have shown similar hardness resultsfor equivariant neural networks, a class of geometric architectures that explicitly encode symmetries. Both lines of work indicate that additional assumptions on the neural network architecture or the data geometry are needed to establish learnability. Here, we focus on the latter.

A separate body of literature investigates nonlinear, low-dimensional structure in data. Approaches for estimating intrinsic dimension [44; 63; 87] and curvature [1; 86; 46] from data seek to characterize the geometry of low-dimensional manifolds. _Manifold Learning_ aims to identify low-dimensional structure by reconstructing low-dimensional manifolds from data. Methods such as Multi-Dimensional Scaling , Isomap  or Diffusion Maps  have shown empirical success in learning low-dimensional data manifolds. For some of these approaches, formal guarantees on their reliability with respect to the geometric characteristics and sample size of the data are known . More generally, the manifold learning problem and its complexity have been formally studied in [43; 42; 3]. However, the impact of data geometry on the complexity of learning neural networks in downstream tasks remains open.

In this paper, we ask: _Which assumptions on the data geometry guarantee learnability of neural networks?_ We show that the manifold hypothesis on its own does not guarantee learnability. In particular, we give hardness results for a class of low-dimensional manifolds in the statistical query (SQ) model or under cryptographic hardness assumptions. Our work follows an established line of proof techniques in the SQ literature [32; 48; 26], extending hardness results for neural network training in the Boolean and Gaussian input models to more general geometries. We further show that additional assumptions on the volume and curvature of the data manifold alleviate the fundamental limitations and guarantee learnability through a rather simple interpolation argument. In particular, manifolds which can be reliably reconstructed via manifold learning are in this regime. We further discuss geometric regimes in which our results do not directly apply, and in which provable learnability remains an open question. We illustrate our learnability results through computational experiments on neural network training in the learnable and provably hard regimes. We further complement our theoretical analysis with a brief computational study of the intrinsic dimension of image data manifolds, with the goal of testing the geometric assumptions in our framework.

## 2 Background

### Basic Notation

We denote scalars, vectors, and matrices as \(v,\), and \(\) respectively. We consider ambient spaces \(^{n}\) equipped with the usual inner product \(,\) and associated \(_{2}\) norm \(\|\|\). Submanifolds \(^{n}\) considered in this study have intrinsic dimension \(d n\) and are compact and connected (unless otherwise stated). The tangent space \(T_{}\) at a point \(\) denotes the \(d\)-dimensional linear subspace of \(^{n}\) spanned by velocity vectors of smooth curves incident at \(\). Given a subset \(S^{n}\), we denote by \(d(,S)=_{ S}\|-\|\) the distance of a point \(\) to \(S\). We denote \(_{d}\) as the \(d\)-dimensional volume measure inherited from the \(d\)-dimensional Hausdorff measure and denote \(_{d}(r)\) and \(_{d}(r)\) as the volume of the \(d\)-dimensional ball and \(d\)-dimensional sphere of radius \(r\) respectively. For a point \(\) in a given manifold \(\), \(_{}(,r)\) denotes the volume of the ball of radius \(r\) around the point \(\) with respect to the Riemannian distance metric.

### Learning Setting

We consider the task of learning feedforward neural networks \(f:^{n}\) composed of \(L\) hidden layers \(f^{()}:^{d_{}}^{d_{-1}}\) taking the form

\[f() =^{}f^{(L)}(f^{(L-1)}((f^{(1)} ()))),\] (1) \[f^{()}() =(_{})+_{},\]

where \(^{d_{L}}\), \(_{}^{d_{} d_{-1}}\) and \(^{d_{}}\) are trainable weights. The input dimension \(d_{0}\) is set to the ambient dimension \(n\) and the output is scalar-valued. Throughout we will consider the setting where the weight entries and hidden widths are bounded by \(O((n))\). When guaranteeing that a class of networks is learnable, we will assume that the number of layers \(L=O(1)\) is constant with respect to the input dimension. Our formal hardness results will apply for single hidden layer networks (\(L=1\)).

We consider learnability in the distribution-specific probably approximately correct (PAC) setting where the goal is to produce an algorithm which can learn a target function given polynomial time and samples .

**Definition 2.1** (Efficiently PAC Learnable).: A concept class \(\) consisting of functions \(c:\) is _efficiently PAC-learnable_ over distribution \(\) on \(^{n}\), if there exists an algorithm such that for any \(>0\) and \(>0\), and for any target concept \(c^{*}\), the algorithm takes at most \(m=(1/,1/,n)\) samples drawn i.i.d. from \((,c^{*}())\) with \(\), and returns a function \(f\) satisfying \(\|f-c^{*}\|_{}_{}[(f()-c^{*}())^{2}]}\) with probability at least \(1-\) in time at most \((1/,1/,n)\).

Note, that the above is a distribution specific instance of PAC learning as we require the algorithm to work only for a given distribution and not all distributions. In Section 3.2, we will also show a hard class of functions which is likely not efficiently PAC learnable. Hardness results are proven in the restricted **statistical query (SQ)** setting, a query complexity based model for proving hardness capturing most algorithms in practice . Given a joint distribution \(\) on input/output space \(\), any SQ algorithm is composed of a set of queries. Each query takes as input a function \(g:[-1,1]\) and tolerance parameter \(>0\), and returns a value \((g,)\) in the range:

\[|_{(,y)}[g(,y) ]-(g,)|.\] (2)

Gradient based algorithms can be queried by, for example, setting \(g(,y)=C(_{ }()-y)^{2}\) to estimate the gradient of a neural network \(_{}\) with respect to parameter \(\) for the MSE loss (constant \(C\) chosen so that outputs of \(g\) are bounded in \([-1,1]\) forming a valid query). Hardness is quantified in the number of queries needed to learn a function \(c^{*}\) drawn from function class \(\).

Manifold smoothness restrictions.To conform to practical settings where input features are normalized (e.g. image pixel values in the range \(\)), input distributions are assumed to be supported on smooth \(d\)-dimensional sub-manifolds \(^{n}\) of the \(n\)-dimensional hypercube. For any given manifold \(\), we will assume that data distributions \(_{}\) supported on that manifold have a smooth density \(f\) with respect to the volume measure and are appropriately bounded such that \(_{}}f()}{_{}f()}=O((n))\).

We will also place curvature restrictions on the manifold by bounding its reach, a global smoothness quantity introduced by  and commonly studied in the manifold learning community . Informally, it is the largest number \(D\), such that any point in ambient space at distance less than \(D\) has a unique nearest neighbor in the manifold \(\). It is defined more formally from descriptions of the medial axis (Figure 1).

**Definition 2.2** (Reach from medial axis).: Given a closed subset \(S^{n}\), the medial axis \((S)\) of \(S\) consists of the set of points with no unique nearest neighbor:

\[(S)=\{^{n}:  S,\|-\|=\|- \|=d(,S)\}.\] (3)

The reach \((S)\) of \(S\) is the minimal distance from \(S\) to \((S)\):

\[(S)=_{(S)}(,S).\] (4)

Bounds on the reach imply corresponding bounds on the radius of curvature (related to the geodesic and normal curvature) at any point in the manifold since \(()^{-1}\|^{}_{ (t),^{}(t)}(t)\|\) for any unit-speed geodesic \(:\) where \(\|^{}_{(t)}(t)\|=1\) for all \(t\). We will encounter a second classical curvature notion, _Ricci curvature_, which is a local, intrinsic curvature notion that characterizes the volume growth of geodesic balls (see sec. B.1 for a more formal definition). Positive lower bounds on the Ricci curvature imply that the manifold has a bounded diameter, a fact that we will use below.

Figure 1: Example of a one-dimensional manifold and its medial axis. Its reach is given by the minimum distance of the medial axis to the manifold.

### Related works

Here, we briefly summarize the most relevant prior work to our study and reserve Appendix A for a more detailed discussion. Our study lies at the intersection of research in manifold learning complexity and neural network learnability. In manifold learning, various works have analyzed the complexity of learning tasks over input manifolds. In one line of work, estimation of smooth manifolds in Hausdorff loss has been shown to require sample complexity of \(O(^{-d/2}(1/))\), which is independent of the ambient dimension \(n\)[14; 47; 58]; later works also provided algorithms that run in time linear in \(n\)[4; 38]. In a learning setting similar to our work,  provide upper and lower bounds on the complexity of manifold reconstruction in SQ settings. In a separate context, [71; 43] show that the sample complexity for determining whether a dataset is within a class of manifolds of specified intrinsic dimension, curvature, and volume bounds grows exponentially with the intrinsic dimension and polynomially in the reach and volume.  also categorizes the sample complexity for binary classification over smooth cuts on a data manifold where smoothness is defined by the condition number of the classification boundary of the manifold (a quantity closely related to reach).

The hardness of learning neural networks has a long history that we detail further in Appendix A. Under the i.i.d. Gaussian input model, superpolynomial  and exponential  lower bounds for learning single hidden layer networks in CSQ settings have been shown. For uniformly random Boolean inputs,  reduce the problem of learning single hidden layer neural networks to a cryptographically hard problem, a technique, which we also use in Appendix C.3. They also show how to "Booleanize" Gaussian inputs to show hardness for three hidden layer networks, which was later extended to SQ and cryptographic hardness of learning two hidden layer networks in .  used similar techniques to give hardness results for equivariant neural networks. To the best of our knowledge, the hardness of learning feedforward neural networks under more general data geometries, such as under the manifold hypothesis, has not been studied previously. Various works have found efficient learning algorithms under i.i.d. input assumptions when the weights of the networks are restricted in their rank, condition number, positivity, and other criteria [90; 27; 7; 36]. We consider the generic setting where weight matrices are bounded in width and magnitude by \(O((n))\), but otherwise unrestricted.

## 3 Learnability results

Our main results prove the existence of a learnable and hard to learn class of input data manifolds illustrated in Figure 2. The _learnable_ setting is the class of efficiently sampleable manifolds, which form the basis for algorithms that can provably reconstruct manifolds [9; 28; 70; 42]. As expected, we find that a simple interpolation argument guarantees learnability of neural networks over these manifolds. Below and in Appendix B.3, we comment on instances of data geometries commonly assumed in machine learning and data science applications that place manifolds within this regime.

The _provably hard_ setting are manifolds without bounds on volume but with bounds on curvature quantified globally by the reach. When the bound on the reach grows no faster than \(o()\) (\(n\) denoting the input size), we construct input data manifolds that feature curvature no larger than the stated bounds but for which learning neural networks is exponentially hard. Our proofs construct curves that cover exponentially many quadrants of the Boolean cube, which allows us to extend classical hardness results for learning Boolean functions expressible by neural networks.

Within the manifold regimes we study, our results are relatively tight with respect to bounds on the reach, since whenever the reach grows as \(()\), such manifolds fall within the first class of efficiently sampleable manifolds. We leave as an open question the learnability of manifolds whose reach grows exactly as \(()\) (see Appendix C.4 for more details). Looking beyond our setting, real-world data manifolds feature heterogeneous properties that may not conform to the global bounds on curvature, intrinsic dimension, etc. that we set in our study. Better characterizing these heteregeneous features is a first step to extending our learnability results to more realistic settings. We conduct some preliminary empirical analysis in analyzing this heterogeneity in our experiments (Section 4.2).

### Sampleable regime

In the manifold reconstruction literature, the goal is to draw samples from a distribution \(\) (typically supported on a given manifold \(\)) and find a manifold \(^{}\) which closely approximates the sample distribution or target manifold in some appropriate error metric such as the Haussdorf distance [89; 9; 2; 43; 28; 70; 42]. Runtime and sample complexity for these algorithms are generally at least linear in the volume of the manifold, polynomial in smoothness parameters such as the reach, and exponential in the intrinsic dimension, though the dependence on the ambient dimension can often be removed . Manifold reconstruction algorithms offer a direct means for learning data from a target neural network \(f^{*}\) as one can apply such algorithms to the graph of the function consisting of points \((,f^{*}())\). As we will show here, guarantees of learning in the manifold reconstruction literature directly imply guarantees for learning neural networks via a simple interpolation argument. However, caution must be taken in assuming this situation holds in practice. For this procedure to be efficient, the volume of the manifold should be at worst polynomial in the intrinsic dimension \(d\) and not growing significantly with the ambient dimension \(n\). Some empirical evidence suggests that manifolds of real-world data may not be in this regime [74; 16].

Essential to many of the manifold reconstruction algorithms is the requirement that one can efficiently cover the manifold with samples [28; 43; 9]. This requirement comes in various technical forms and we frame our results here assuming the ability to form an epsilon net with samples.

**Definition 3.1** (\((,)\)-net).: Given a distribution \(_{}\) over a manifold \(\), a subset \(\) of points forms an \((,)\)-net if with probability at least \(1-\) over draws \(_{}\), there exists a point \(^{}\) where \(\|-^{}\|\).

We denote a sequence of manifolds indexed by ambient dimensions as efficiently sampleable if for a fixed intrinsic dimension \(d\), one can draw polynomially many samples in \(n\) and error \(1/\) to form an epsilon net over the manifold \(\).

**Definition 3.2** (Efficiently sampleable manifold).: Let \(\{_{n}\}\) denote a sequence of manifolds in ambient dimension \(n\) with fixed intrinsic dimension \(d=O(1)\). Let \(\{_{_{n}}\}\) be a corresponding sequence of distributions over points on the manifolds. We denote this sequence of manifolds as _efficiently sampleable_ if with at most \(((n,1/))\) samples drawn i.i.d. from \(_{_{n}}\), one can form an \((,)\)-net of the manifold \(_{n}\) for any \(=((n)^{-1})\).

Note that classical manifold learning algorithms typically implicitly assume efficiently sampleable manifolds as they assume that the algorithms run efficiently or have runtimes that depend polynomially on the manifold's volume \(()\) (see Example B.8 for details). With standard regularity assumptions on the manifold, such a volume assumption typically implies that an epsilon net can be formed efficiently with \((()/^{d})\) samples by a coupon collector argument (see, e.g., Proposition 3.5).

_Remark 3.3_.: Given a target function \(f^{*}\) with inputs on a manifold \(\), we could treat the graph of the function \(_{f^{*}}=\{(,f^{*}()):\}\) as a manifold in \(\). Then, one can apply manifold reconstruction algorithms directly to the graph \(_{f^{*}}\), which lies in an ambient space of

Figure 2: Learnability of neural networks depends on the regularity and smoothness properties of the input data manifold. In the efficiently sampleable regime corresponding to manifolds which can be approximated well with samples, neural networks are learnable via simple interpolation arguments. In the regime where manifolds are bounded solely by their curvature and intrinsic dimension, we show classes of manifolds that obstruct the learnability of algorithms. Real-world data likely lives in an intermediate regime with heterogeneous properties (e.g. manifolds with varying intrinsic dimension; see Section 4.2).

dimension \(n+1\). This likely works in practice, though it runs into some technical issues due to discontinuities introduced by ReLU activations. We apply a more direct method here.

**Proposition 3.4**.: _Let \(n\)-dimensional inputs be drawn from a sequence of efficiently sampleable manifolds \(_{n}\) with intrinsic dimension \(d=O(1)\) and distributions \(_{_{n}}\) over the manifold. Denote \(_{n}\) as the function class of constant depth ReLU networks on \(n\) inputs with weights bounded in magnitude by \(B=O((n))\) and \(O((n))\) width. Then, one can learn \(f^{*}_{n}\) up to error \(\) in runtime and sample complexity \(O((n,B,1/))\)._

Proof.: From the efficiently sampleable property of the manifold, we form an \((^{},)\)-net using \(((n,1/))\) many samples where for all but a \(\)-fraction of the manifold any point is within \(^{}\) of a given point within the net. Any network of \(L\) layers and \(O((n))\) bounded width and weight magnitude has the Lipschitz property that 

\[\|f^{*}()-f^{*}(^{})\| \|-^{}\|_{=1}^{L}\|W^{()}\|_{2}\] (5) \[\|-^{}\|_{=1}^{L}\|W^{()}\|_{F}\] \[ O(B^{}n^{L})\|-^{}\|,\]

where \(B^{}=O((n,B))\) is a bound on the Frobenius norm of the weight matrices. Therefore, we can interpolate the value of \(f^{*}\) for any \(_{n}\) from a sample by setting \(^{}=O( B^{-1}n^{-L})\). For the \(\)-fraction that is not in this net, note that \(|f()| O((n))\) for all \(\) in the hypercube so setting \(=o(|f()|^{-1})\) will guarantee that the contribution from these points decays. Finally, since \(L=O(1)\) and \(B=O((n))\) by assumption, we have the resulting polynomial complexity. 

Restrictions on the curvature or convexity properties of a manifold can often render the manifold efficiently sampleable. To illustrate that many manifolds implicitly fall within the regime of efficiently sampleable manifolds, we give an example below. Additional examples can be found in Appendix B.3.

**Proposition 3.5** (Bounded Ricci curvature (Isoperimetric setting, see  for motivation) ).: _The set of distributions \(_{}\) over complete manifolds with bounded Ricci curvature \(()(d-1)K\) for a constant \(K>0\)5 are efficiently sampleable._

Proof.: The bound on the Ricci curvature guarantees that the diameter of the manifold is at most \(2/\) and the manifold is contained within a \(d\)-dimensional ball of radius \(1/\) (see also Theorem 6.3.3 of ). Thus, we can form an \((,)\)-net over the manifold \(\) by inducing it from an \(\)-cover on the ball of radius \(1/\) (see Definition B.1 for definition). To achieve an \(\)-cover of such a ball in \(d\) dimensions, it suffices to have \(N_{}=O(1/^{d})\) points (see, e.g., Lemma 5.2 of ). We denote the balls in the cover as \(b_{1},,b_{N_{}}\). Let \(\) denote a sampling ratio where we will take \(^{-2}=O((n))\) samples to form the \((,)\)-net. By a coupon-collector argument (see Lemma B.5), \(^{-2}\) samples suffices to guarantee with high probability that any ball \(_{i}\) with probability at least \(_{}(b_{i})\) has a sample within it. \(\) then is at most the probability that a randomly drawn sample falls within a ball \(b_{i}\) with probability \(_{}(b_{i})<\). Therefore,

\[_{i:_{}(b_{i})<}_{}(b_{i}) N_{}.\] (6)

Thus, setting \(=/N_{}\) suffices to achieve \(=((n)^{-1})\).

We remark that the efficiently sampleable setting does not necessarily require that the manifold be fully connected. In fact, it is straightforward to extend proofs of learnability such as in Proposition 3.5 to settings where there are multiple such disconnected manifolds as long as the number of disconnected components does not grow superpolynomially in \(n\).

### Hard regime with bounded curvature

The learnability of the networks in the manifold learning setting relied crucially on the fact that such settings implicitly place bounds on the volume of such manifolds. Here, we lift that restriction and consider a setting where manifolds are smooth and bounded only in their curvature and intrinsic dimension. More formally, we consider studying inputs drawn from manifolds with bounded reach (see Definition 2.2).

In this section, we construct a sequence of manifolds \(\{_{n}\}\) with sufficiently bounded reach that are exponentially hard to learn in the SQ model. These manifolds resemble space filling curves which wrap around exponentially many quadrants of the Boolean cube (see Appendix C.1). Given a bound on the reach \((_{n})=O(n^{})\) for \(<0.5\), the space filling curve wraps around \(2^{n^{1-2}}=2^{n^{(1)}}\) quadrants allowing one to extend standard hardness results for learning Boolean functions to data supported on this manifold.

**Theorem 3.6**.: _Let \(_{_{n}}\) denote the uniform distribution over manifolds \(_{n}\) constructed in Definition C.4 where \((_{n})=O(n^{})\) for \(<0.5\). Any SQ algorithm \(\) capable of learning the class of linear width single hidden layer \(\) neural networks under this sequence of distributions up to mean squared error sufficiently small (\(/8\) suffices) with queries of tolerance \(\) must use at least \((^{2}2^{n^{(1)}})\) queries._

Proof sketch.: We first form manifolds conforming to the stated bounds on the reach and intrinsic dimension while also "looping" around exponentially many quadrants of the hypercube. This is done by forming space-filling curves whose paths follow the indexing of a Gray code . Inputs on this manifold are real-valued, but by carefully selecting a portion of the input dimensions, we obtain inputs that with high probability are approximately distributed as uniform over Boolean inputs \(\{0,1\}^{n_{b}}\). We then extend previous proofs of the hardness of learning networks approximating parity functions under i.i.d. Boolean inputs proven in  to our setting. See Appendix C for complete proofs. 

_Remark 3.7_.: In Theorem C.12, we show an equivalent proof of hardness in the cryptographic setting following the techniques in . Namely, in Appendix C.3, we show that the class of functions in Theorem 3.6 is also hard to learn conditional on cryptographic assumptions related to the hardness of learning a class of pseudorandom functions.

The above result is relatively tight with respect to the bound on the reach. Namely, whenever the reach \((_{n})=(n^{})\) is growing faster than \(\), one can show that the volume of such manifolds is at most \(O((n))\) and fitting within the regime of efficiently sampleable manifolds studied in the previous subsection.

**Proposition 3.8**.: _Given a sequence of manifolds \(\{_{n}\}\) of intrinsic dimension \(d\) (fixed and independent of \(n\)) and reach bounded by \((_{n})=(n^{0.5})\), the volume of the manifolds grows at most \(_{d}(_{n})=O((n))\)._

We refer the reader to Appendix C.4 for further details and formal proofs of this tightness.

## 4 Experiments

### Empirical verification of main findings

To verify that neural networks are learnable in the sampleable regime (Section 3.1) and hard to learn in the bounded curvature regime (Section 3.2), we train neural networks over input data manifolds taken from these regimes to confirm the formal theoretical results. We consider target networks, which are single hidden layer neural networks of \(O(n)\) width and train overparameterized neural networks of larger width. For the sampleable regime, we draw inputs from a \(d\)-dimensional hypersphere supported over \(d\) orthogonal dimensions in the \(n\)-dimensional ambient space. This is an instance of a complete manifold of bounded curvature as in Proposition 3.5. For the hard to learn regime, we draw inputs from the 1-dimensional manifold constructed in Appendix C.1 with reach \(R=0.5\). Target functions in this hard regime correspond to the parity functions described in our proofs in Appendix C.

The results in Figure 3 empirically confirm our main findings. When inputs are drawn from the \(d\)-dimensional hypersphere (Figure 2(a)), the trained neural network achieves low test error with a fixed training set of size \(1000\) for all \(n\). Here, target functions are drawn from those in  who provided a class of hard to learn functions in the i.i.d. Gaussian input model. Once the input model is changed to the \(d\)-dimensional hypersphere, this class of functions is no longer hard to learn (see Appendix E for similar results over random targets).

In contrast, when inputs are drawn from the hard to learn manifold (Figure 2(b)), learning is only possible when the ambient dimension is small. Here, target functions are randomly chosen parity functions over the inputs (see Appendix E). At each step of training, we provide the algorithm with a fresh batch of i.i.d. samples. In Figure 8, we replicate these results when training networks overparameterized in depth (i.e. three hidden layers as opposed to one) as well. We refer the reader to Appendix E for further details.

### Empirical study of geometry of data manifolds

We empirically investigate the intrinsic dimension of real-world data manifolds as a first step towards testing the manifold regularity assumptions of our framework on real data. Largely, our results corroborate findings in other works highlighting the heterogeneous nature of real-world data manifolds [77; 87; 16].

Experimental Setup.We estimate intrinsic dimension using samples generated by a diffusion model, closely following the approach of . The use of a diffusion model allows us to generate arbitrarily dense samples in a neighborhood of a given point from the data manifold. Given these samples, we perform PCA on the collection of score vectors \(\{_{i}\}\) of the diffusion model at each of these samples, which point towards the direction of de-noising and hence towards the manifold itself. Estimating the rank of such a collection of score vectors recovers an estimate of the normal and intrinsic dimensions. We defer a detailed description of our approach, its implementation, and hyperparameter choices to Appendix E.1.

  
**Data set** & **\#0** & **\#1** & **\#2** & **\#3** & **\#4** & **\#5** & **\#6** & **\#7** & **\#8** & **\#9** \\  MNIST (28 x 28) & 102 & 66 & 120 & 109 & 73 & 101 & 109 & 72 & 114 & 104 \\ KMNIST (28 x 28) & 180 & 199 & 134 & 169 & 135 & 128 & 149 & 191 & 205 & 199 \\ FMNIST (28 x 28) & 429 & 177 & 596 & 275 & 225 & 233 & 312 & 125 & 201 & 418 \\   

Table 1: Estimated intrinsic dimension shown for each of the ten classes in MNIST, KMNIST, FMNIST.

Figure 3: (a) Learning is successful when inputs are drawn from a \(d=10\) intrinsic dimensional hypersphere living in ambient space of dimension \(n\) – an instance of the bounded positive curvature model in Proposition 3.5. Target functions are single hidden layer networks taken from the class of hard to learn functions in the Gaussian i.i.d. input model , which are no longer hard to learn in the input distribution considered here. (b) When the ambient dimension is large, learning algorithm struggles to learn a single hidden layer neural network drawn from the class of functions in the setting of Theorem 3.6 where the input data manifold has intrinsic dimension \(d=1\) and reach \(R=0.5\). The network trained to learn this target function is over-parameterized with respect to the target. Data is aggregated over five random realizations.

Results.Estimated intrinsic dimensions for three image data manifolds are given in Table 3. Score spectra for the three manifolds are shown in Figure 4 confirming that the score vectors do indeed cluster in a linear subspace spanning the normal dimensions. Furthermore, our approach gives a similar estimate for the intrinsic dimension of MNIST as reported in . To the best of our knowledge, FMNIST and KMNIST were not investigated in prior work. Comparing estimated intrinsic dimension across classes indicates significant heterogeneity of the real data manifolds, which would place them in the _potentially learnable_ regime in our proposed framework. This may seem surprising as images drawn from different classes can have different properties and symmetries. However, it corroborates our initial hypothesis that real-world data may not conform to global bounds on intrinsic dimension. Furthermore, we find that upon re-sizing the images to a smaller scale, the ambient dimension decreases faster in comparison to the intrinsic dimension for some datasets (see Table 3 in Appendix) lending some limited evidence to the underlying assumption in the manifold hypothesis that bounds on intrinsic dimension do not grow in tandem with ambient dimension. Additional experimental results and further discussion can be found in Appendix E.3.

## 5 Discussion

In this paper we have investigated whether regularity assumptions on the data geometry can render feedforward neural networks efficiently learnable. We show that bounding curvature in low-dimensional data manifolds, a common assumption in high-dimensional learning, does not suffice to alleviate the fundamental hardness of learning such architectures. However, we establish learning guarantees under geometric assumptions common in the manifold reconstruction literature that allow manifolds to be efficiently covered with \(O((n))\) samples. Our results contribute to a recent body of literature that seeks to establish learnability guarantees and provable hardness results for neural network architectures [12; 26; 36; 57; 48; 90].

While our results establish guarantees for a wide range of data manifolds, including manifolds that can be provably reconstructed via manifold learning, there are geometric regimes in which the question of learnability remains open. We hypothesize that the heterogeneity of real data manifolds might place them in this more uncertain regime and find some evidence of this heterogeneity in our experiments. A further theoretical and empirical investigation of this class of data manifolds is an important direction for future work.

The experimental results on estimating the intrinsic dimension of data manifolds are merely a starting point for understanding realistic assumption on data geometry in machine learning architectures. In future work we hope to look at a wider range of synthetic and real data sets, as well as a wider range of geometric characteristics and estimation techniques.

Broadly, our results indicate that assumptions beyond global smoothness of data manifolds are needed to guarantee learnability of neural networks in real-world settings. To circumvent hardness results, the particular form of the networks and their weights can be changed to for example incorporate symmetries via equivariant architectures or place restrictions on weights such as bounds on condition number or rank [27; 7]. Furthermore, some datasets live in a discrete input space (e.g., language data) which does not conform directly to Riemannian analysis. In these settings, adaptations of the manifold hypothesis to analysis via graphs and other combinatorial objects presents an interesting direction for future work [81; 15].

Figure 4: Sample spectrum of the singular values of a collection of score vectors given by a trained diffusion model pointing towards the direction of de-noising an image.