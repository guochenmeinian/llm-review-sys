# Alternating Updates for Efficient Transformers

Cenk Baykal

Google Research

&Dylan Cutler

Google Research

&Nishanth Dikkala

Google Research

&Nikhil Ghosh

UC Berkeley

&Rina Panigrahy

Google Research

&Xin Wang

Google Research

Correspondence to baykalc@google.com.Work done as an intern at Google Research

###### Abstract

It has been well established that increasing scale in deep transformer networks leads to improved quality and performance. However, this increase in scale often comes with prohibitive increases in compute cost and inference latency. We introduce Alternating Updates (AltUp), a simple-to-implement method to increase a model's capacity without the computational burden. AltUp enables the widening of the learned representation, i.e., the token embedding, while only incurring a negligible increase in latency. AltUp achieves this by working on a subblock of the widened representation at each layer and using a predict-and-correct mechanism to update the inactivated blocks. We present extensions of AltUp, such as its applicability to the sequence dimension, and demonstrate how AltUp can be synergistically combined with existing approaches, such as Sparse Mixture-of-Experts models, to obtain efficient models with even higher capacity. Our experiments on benchmark transformer models and language tasks demonstrate the consistent effectiveness of AltUp on a diverse set of scenarios. Notably, on SuperGLUE and SQuAD benchmarks, AltUp enables up to \(87\%\) speedup relative to the dense baselines at the same accuracy.

## 1 Introduction

Contemporary machine learning models have been remarkably successful in many domains, ranging from natural language  to computer vision . Many of these successes have come in part through sheer scale. A vast amount of empirical studies justify the conventional wisdom that bigger (models and data sets) is better . Accordingly, state-of-the-art Transformer  models often contain billions of parameters and are trained for weeks on enormously large data sets using thousands of AI accelerators. Their immense size leads to prohibitive compute and energy costs  and prevents their deployment to resource-constrained applications .

To alleviate these costs and enable scalability of modern Transformers, a recent line of works have proposed techniques to increase the capacity of models without drastically increasing the computational costs via conditional computation. A notable paradigm is sparsely-activated networks, such as Mixture-of-Experts (MoE) models . The main idea of MoE is to effectively _widen_ each network layer by accessing dynamically invoked parameters, i.e., experts, where each expert corresponds to a small subset of disjoint parameters that can be acted on by the input. During training and inference, a given input to the network is routed to a small subset of experts (parameters) to compute the output. As a result, the computation cost remains small relative to the total number of parameters. This scheme enables models with higher capacity with only a relatively small increase in computation.

While prior approaches in conditional computation have predominantly focused on the _processing power_ of transformers, there is a research gap in efficiently incorporating _widened learned representations_. Recent works have empirically and theoretically established that a wider token representation (i.e., a larger model dimension) helps in learning more complicated functions by enabling more information to be packed in the representation vectors [19; 24; 54]. This phenomenon is also evident in modern architectures of increasing capability. For instance, the representation dimension grows from 512 (small) to 768 (base) and 1024 (large, 3B, and 11B) in T5 models , and from 4096 (8B) to 8192 (64B) and 18432 (540B) in PaLM models . A widened representation dimension also significantly improves performance for dual encoder retrieval models [33; 34]. However, naively widening the learned representation requires accordingly increasing the model dimension (see Fig. 1), which quadratically increases the amount of computation in the feedforward computation. In light of the above, a natural question arises: can we leverage the benefit of wider representations without incurring the additional cost of wider transformer layers?

In this paper, we address this research gap by introducing _Alternating Updates_ (AltUp), a technique to incorporate wider representations in a simple and efficient way. AltUp operates by partitioning the widened representation vector into blocks, processing only a single block at each layer, and using an efficient prediction mechanism to infer the outputs of the other blocks (see Fig. 1). Processing a single block in each transformer layer enables AltUp to simultaneously keep the model dimension, hence the computation cost, constant and take advantage of using an increased token dimension. Unlike prior approaches, e.g., Sparse Mixture of Experts, AltUp is easy to implement, requires minimal hyperparameter tuning, and does not necessitate sharding. Moreover, since AltUp focuses on increasing the representation dimension, it can be applied synergistically with orthogonal techniques like MoE  to obtain complementary performance gains.

In particular, our contributions are:

1. We introduce _Alternating Updates_ (AltUp) to bridge the research gap in efficiency techniques by enabling wider representations with little additional computation cost. AltUp is simple-to-implement, requires minimal hyperparameter tuning, and does not necessitate sharding.
2. We develop two notable extensions of AltUp: (i) _Recycled-AltUp_, a faster variant of AltUp that requires virtually no additional learnable parameters and (ii) _Sequence-AltUp_, an extension of the AltUp idea to the sequence dimension.

Figure 1: An illustration of widening the token representation without (left) and with Alternating Updates (right). This widening causes a near-quadratic increase in computation in a vanilla transformer due to the increased layer width. In contrast, Alternating Updates keeps the layer width constant and efficiently computes the output by operating on a sub-block of the representation at each layer.

3. We present an extensive evaluation of AltUp on T5 models on various benchmark language tasks. Our experimental results show that AltUp and its variants uniformly lead to models with improved speed-accuracy trade-offs. Notably, on SuperGLUE and SQuAD benchmarks, AltUp enables up to \(87\%\) speedup relative to the dense baselines at the same accuracy.

## 2 Related Work

Prior work is rich with a diverse set of techniques to increase the efficiency of contemporary transformer models. Here, we cover the most relevant subset of state-of-the-art techniques and refer the interested reader to  for a more comprehensive survey.

Recent works have introduced extremely large, yet scalable models with the use of conditional routing of inputs to a learnable subset of parameters. These sparsely-activated models have achieved state-of-the-art performance on various benchmarks  and exhibit favorable theoretical properties [4; 1]. Notably, the Sparse Mixture of Experts (SMoE) [45; 57; 22] family of models use a learned softmax probability distribution to conditionally direct the computation to _experts_, i.e., subsets of network parameters. By routing the computation to a small subset of parameters on an input-dependent basis, SMoE leads to higher capacity models with a relatively small and controllable increase in computation. Switch Transformers  show that routing to a single expert on an input-dependent basis reduces computation and outperforms prior SMoE approaches on language tasks. Deja Vu  centers around selecting subsets of the attention and MLP parameters to apply for each input (contextual sparsity). Our work is orthogonal to Deja Vu and synergistic with these approaches at large as it focuses on conditional computation.

Follow-up work on SMoE include those that improve the load balancing of experts [60; 29], use reinforcement learning to learn the routing function , and leverage smooth top-\(k\) expert selection  (see  for a survey). Other choices for the routing function include non-learnable ones such as Locality Sensitivity Hashing (LSH)  which generally maps similar inputs to the same expert, Hash Layers that use token-based hashing , and language-specific deterministic routing . Residual Mixture of Experts  separates the expert weights into input-independent and input-dependent components.

Conditionally accessing _external memory_ is another related approach to vastly increase model capacity at the cost of a relatively small increase in computation [15; 14]. For examples, Memorizing Transformers , Memformer , and Product key memory  leverage dynamic memory to encode and retrieve relevant information. Additional works include those that use an immensely large untrainable corpus, such as Wikipedia, REALM , or a 2 trillion token database, RETRO . These prior works that focus on routing(expert)-based mechanisms often necessitate complicated, sharded implementations due to the sheer number of additional parameters that they introduce -- often on the order of billions. Our work, on the other hand, is simple-to-implement and requires virtually no hyperparameter tuning. Moreover, AltUp can be synergistically combined with sparsely-activated models like MoE to obtain complementary improvements in efficiency.

Additional relevant works in the realm of efficient transformers include Funnel transformers , Reformers , Performers , Big-Bird , and LongT5 , among others. These works notably present methods to reduce the quadratic cost of the attention mechanism of transformers. Another flavor of methods complementary to our work is that of adaptive computation, e.g., CALM , DynaBERT  CascadeBERT  and DeCap , where different amounts of computational power is allotted on an example-specific basis via some type of early-exit strategy. AltUp achieves its efficiency via the orthogonal direction of conditionally leveraging wider token representations, and hence can be easily combined with these efficiency techniques and others (e.g., quantization , FlashAttention ).

## 3 Alternating Updates

In this section, we introduce the method of _Alternating Updates_ (AltUp), an approach to enable increased token dimension with little additional computation cost.

### Background

At a high level, a standard transformer with \(L\) layers generates a \(d\)-dimensional representation by applying a sequence of layers transformer layers \(_{1},,_{L}\) as follows. For a particular input token within a sequence of length \(N\), the initial token representation \(x_{1}^{d}\) is computed by an embedding table lookup. Subsequently, this \(d\)-dimensional representation is refined across the transformer layers by iteratively computing \(x_{i+1}=_{i}(x_{i})\) for each layer \(i[L]\); here and throughout \([N]\) denotes the set \(\{1,,N\}\) for \(N\). Each transformer layer \(_{i}:^{d_{}}^{d_{ }}\) has width \(d_{}\) (with \(d_{}=d\) in the standard setting) and contains an attention block and a FeedForward (FFN) block. The width of the layer \(d_{}\) controls the dimensions of the matrices involved in the attention and FFN blocks. Consequently, the computation cost of attention and FFN scales with \((N^{2}d_{})\) and \((Nd_{}^{2})\), respectively. The output, \(x_{L+1}\) is the output token representation generated by the transformer. This computation is usually followed by a linear layer operation that maps from the \(d\)-dimensional representation \(x_{L+1}\) to \(||\)-dimensional logits (in \((||d)\) time), followed by a softmax non-linearity to generate the probabilities over the vocabulary \(\).

Increasing the representation dimension \(d\) is a way to enhance the capacity of the transformer model, as a wider representation enables the transformer to store richer information about the input and helps in learning more complicated functions [19; 24; 54]. Naively widening the token representation \(d\) requires widening each layer as well, since \(d_{}\) must match \(d\) in a standard transformer model. However, the computation time of each transformer layer grows roughly quadratically with \(d_{}\), notably for relatively short input sequences. This means that, growing the token dimension from \(d\) to \(2d\), for example, leads to a model that is at least 2 times (and closer to \(4\) times for small \(N\)) slower than the original model with a \(d\)-dimensional representation.

### Alternating Updates

The core idea of Alternating Updates is to _widen the representation vector, but perform computation with a \(d\)-dimensional sub-block_, and estimate the updated representation using a Predict-Compute-Correct algorithm, as illustrated in Figure 1, right. More specifically, AltUp expands the representation width from \(d\) to \(Kd\), for integers \(K>1,d>0\) (for example, \(K=2\) in Fig. 1), but uses layers of width \(d_{}=d\) (_not_\(d_{}=Kd\)) to transform the representation vector. By keeping the width of each transformer layer constant, AltUp avoids incurring the quadratic increase in computation cost that would otherwise be present with a naive expansion of the representation.

Alg. 1 depicts the details of the per-layer computation involved in a transformer with AltUp with a \(Kd\)-dimensional representation vector. The input to the AltUp layer is assumed to be the concatenation of \(d\)-dimensional contiguous subblocks \(x_{}=(x_{old}^{1},x_{old}^{2},...,x_{old}^{K}) ^{dK}\). Inspired by predictor-corrector methods used to solve ordinary differential equations , AltUp first generates a prediction \(^{i}\) for each of the subblocks \(i[K]\) (Line 1). This prediction takes the form of a mixture of subblocks \(^{i}=_{j=1}^{K}p_{i,j}x_{old}^{j}\), where \(p_{i,j}\) for \(i,j[K]\) are learnable scalars. Subsequently, one of the \(K\) sub-blocks is chosen and the computation with the unexpanded transformer layer of width \(d_{}=d\) is performed on this sub-block (Line 2). Finally, the result of this computation is used in the correction step to generate the updated representation for each sub-block (Line 3).

Computation timeWe see from Alg. 1 that AltUp introduces negligible amount of additional computation per layer, as the prediction and correction steps involve only vector addition and scalar-vector multiplications (\((d)\) operations). Thus, relative to the computation cost of a transformer layer with width \(d\) (which we incur on Line 2 in AltUp), the cost of AltUp is only an additional \((dK^{2})\) per token, where \(d\) is the original model dimension and \(K\) is the factor of increase in the representation dimension (typically \(K=2\) or \(K=4\), see Sec. 5). This additional \((dK^{2})\) cost per token is a factor of \(d\) smaller than the \((d^{2}K^{2})\) per token cost of the FFN block alone in a \(K\)-times wider transformer layer. In fact, an AltUp layer does not lead to an increased computation time relative to a \(d\)-width transformer layer asymptotically, since the \((dK^{2})\) additional cost per token per layer is dominated by the cost of the FFN block as \(K d\) in practice. At a higher level, AltUp requires using an embedding table with width \(Kd\) and invoking the final linear operation with \(Kd\)-dimensional vectors. The initial embedding lookup using a wider table and the linear + softmax operation with \(Kd\) (instead of \(d\)) dimensional vectors may lead to a perceptible increase in computation time. However, since we only incur this additional cost in the beginning and the end, these factors are often inconsequential,and increasingly so for deeper transformers. Nevertheless, we present an extension to AltUp in Sec. 4 that avoids this slowdown altogether for specialized applications.

Parameter countAltUp introduces \(K^{2}+K\) additional learnable parameters per layer, where \(K^{2}\) is due to \(p_{i,j},i,j[K]\) and \(K\) is a result of \(g_{i},i[K]\). Since \(K d\), this is an imperceptible amount of additional parameters per layer in practice. Zooming out, AltUp with an expansion factor of \(K\) requires a \(Kd\)-width embedding table, and consequently requires \((K-1)||d\) additional parameters, where \(||\) is the vocabulary size. In Sec. 4, we present a variant that requires no additional embedding parameters to be added to the model.

Memory footprintDue to the increase in the representation dimension, AltUp incurs a slightly higher activation memory footprint compared to the baseline model _during training_. In particular, a transformer model with \(L\) layers, model dimension \(h\), batch size \(b\), sequence length \(s\), and number of attention heads \(a\) has an activation memory footprint of \(sbhL(34+5as/h)\). Adding AltUp (with \(K=2\)) to this model leads to an additional activation memory of \((sbh+2sbh)L=3sbhL\), which is less than \(10\%\) of the vanilla transformer's. Moreover, a significant portion of memory is used by the weight parameters which only increase marginally with AltUp. Model parallelism and techniques such as gradient checkpointing, sequence parallelism, or selective activation recomputation can mitigate the memory impact of a wider activation vector even further. During inference, the memory footprint of activations is dominated by the Key-Value cache, which is unaffected by the additional activations introduced by AltUp.

Selection of sub-blocksThe selection of the sub-block \(j^{*}\) for the computation step in Algorithm 1 can be any user-specified technique. We consider two simple, deterministic selection methods in this paper and leave more sophisticated methods for future work: (i) **same**: choose the same sub-block for all the layers in a neural network and (ii) **alternating** (default method): for a sequence of layers, alternating through the sub-blocks, that is, if the sub-blocks are indexed with zero-based index, then sub-block \(\) mod \(K\) is selected for the computation step for layer \([L]\). This alternating selection is the default for Algorithm 1 (hence the name Alternating Updates). We compare the two selection methods empirically in the supplementary material and find that using alternating blocks performs better empirically.

## 4 AltUp Extensions

In this section, we present extensions of the core AltUp idea introduced in the previous section.

### Recycled-AltUp: Faster AltUp via embedding recycling

The AltUp formulation presented in Sec. 3 adds an insignificant amount of per-layer computation, however, it does require using a \(K\)-times wider embedding table. In certain scenarios where the vocabulary \(\) is very large, this may lead to a non-trivial amount of added computation for the initial embedding lookup and the final linear + softmax operation. A colossal vocabulary may also lead to an undesirable amount of added embedding parameters. _Recycled-AltUp_ is an extension of AltUp that avoids these computational and parameter costs by keeping the embedding table's width \(d\)-dimensional.

Figure 2 depicts an example application of Recycled AltUp with \(K=2\). The general idea is to _recycle_ the initial \(d\)-dimensional lookup by replicating the \(d\)-dimensional lookup \(K\) times. Hence, Recycled-AltUp virtually adds no additional parameters relative to the baseline width \(d\) model. Subsequently, the regular AltUp layers (Alg. 1) are applied until the last linear + softmax operation. To avoid the computational cost of this final operation, Recycled AltUp downprojects the \(Kd\)-dimensional representation vector \(x_{L+1}^{dK}\) to a \(d\)-dimensional representation by simply elementwise-adding the \(d\)-dimensional contiguous sub-blocks in \((Kd)\) time. Applying the linear + softmax operation on this down-projected vector implies that the computation cost of this operation is now \((||d)\) rather than \((K||d)\), effectively reducing the amount of computation by \(((K-1)||d)\). Our results in Sec. 5 show that Recycle-AltUp's improved speed and reduced parameter count may make it more appealing for certain applications.

### Sequence-AltUp: Extension of AltUp to the sequence dimension

Here, we introduce _Sequence-AltUp_, a natural extension of Alternating Updates to reduce the apparent sequence length. This extension is motivated by the computation cost associated with the cost of the attention mechanism for long input sequence lengths. Our approach is similar in its goal to that of prior techniques focused on designing efficient attention mechanisms to reduce the quadratic dependency of attention cost on the sequence length: Funnel transformers , Reformers , Performers , Big-Bird , and LongT5 .

Similar to the Funnel transformer , Sequence-AltUp uses a simple striding operation to reduce the sequence length. Only sampled tokens are processed by the transformer layer while the rest of the tokens require little computation, leading to a computation cost reduction by a factor of \(k\), where \(k\) is the stride parameter. In this way, Sequence-AltUp is similar to AltUp in that it applies the predict-compute-correct algorithm (Algorithm 1) to the sequence dimension, but it is different from AltUp in that it does not increase the sequence dimension.

Figure 3 depicts the baseline stride-and-skip technique (left) and the proposed Sequence-AltUp method (right). Given an input sequence of vectors \((x_{0},x_{1},...,x_{T-1})\) to a transformer layer \(\), we propose the following extension of Algorithm 1 to reduce the effective sequence length by a factor of \(k\). First, we apply a lightweight predictor on the full sequence to obtain a predicted output \(=(_{0},...,_{T-1})\). Next, we subsample the input with a fixed stride \(k\) and apply \(\) on the subsampled input and get computed output \(=(_{0},_{k},_{2k},...,_{(T- 1)/k*k})=(x_{0},x_{k},...,x_{(T-1)/k*k})\). Finally, we use a lightweight corrector to combine \(\) and \(\) to form the final output sequence. This design allows unsampled token vectors to obtain contextual information, even though they are not processed by the transformer layer directly--analogous to the inactivated sub-blocks in original AltUp. In contrast, a simple stride-and-skip approach (Figure 3, left) lacks the ability to bring contextual information to the skipped tokens. We present the full algorithm pseudocode and implementation details of Sequence-AltUp in the supplementary material.

Figure 2: Recycled-AltUp with \(K=2\).

## 5 Results

In this section, we apply AltUp and its variants to benchmark language models and tasks. We proceed by outlining the experimental setting below. In Secs. 5.1 and 5.2 we present the performance of AltUp on standard benchmarks with varying configurations and model sizes; in Secs. 5.3 and 5.4 we evaluate the performance of AltUp extensions and demonstrate their effectiveness. We present the full details of our evaluations and additional experimental results in the supplementary; namely, the supplementary contains additional evaluations that demonstrate the synergistic combination of AltUp with other conditional compute techniques, additional finetune results, and complementary ablation studies. Overall, our results consistently show that AltUp and its variants enable sizeable performance gains, e.g., up to \(87\%\) faster models, across all evaluations on standard benchmarks.

SettingWe performed all of our experiments using T5-model architectures  of varying sizes (small, base, large, and 3B) which we pretrained on the C4 dataset for \(500,000\) steps with a batch size of \(256\). The pretrained models were then finetuned on either the GLUE , SuperGLUE (SG) , SQuAD  or Trivia-QA (closed-book) [23; 42] benchmark tasks for a further \(50,000\) steps with a batch-size of \(256\). The pretraining task is to predict corrupted text spans, and the finetuning tasks are re-cast into text generation tasks. We report both pretraining and finetuning metrics: for pretraining, we report span prediction accuracy on a hold-out validation set, and for finetuning, we follow the same recipe as the T5 models, see  for more details. The supplementary contains the full details of our evaluations and hyperparameters.

### Alternating updates on benchmarks

First, we investigate whether incorporating AltUp on a baseline model leads to an unambiguously better model when we consider the predictive performance and _actual observed latency_ (not theoretical FLOPS). To this end, we compare the dense T5-Base/Large/XL models to models augmented with AltUp with \(K=2\) on GLUE, SuperGLUE, SQuAD, and TriviaQA (closed-book) finetuning tasks. Figure 4 plots the performance and normalized speed of the evaluated models.

As the figure depicts, models augmented with AltUp are uniformly faster than the extrapolated dense models at the same accuracy. For example, we observe that a T5 large model augmented with AltUp leads to a \(27\%\), \(39\%\), \(87\%\), and \(29\%\) speedup on GLUE, SuperGLUE, SQuAD, and Trivia-QA benchmarks, respectively. Moreover, we see that AltUp's relative performance improves as we apply it to larger models (compare relative speedup of T5 Base + AltUp to that of T5 Large + AltUp). This demonstrates the scalability of AltUp to and its improved performance on even larger models. Overall, **AltUp consistently leads to models with better predictive performance than the corresponding baseline models with the same speed on all model sizes and benchmarks.**

Figure 3: An illustration of Sequence-AltUp (right) and the baseline Stride-and-skip method (left). Sequence-AltUp has virtually the same computation cost as Stride-and-skip, but enables contextual information passing to the skipped tokens.

### AltUp with varying representation size

In the previous subsection, we had used a value of \(K=2\) for the runs with AltUp. As discussed in Sec. 3, \(K\) controls the width of the representation vector, and is the only hyperparameter required by AltUp. Can we obtain even more performant models by using a larger expansion factor \(K\)? Here, we compare the performance of AltUp with \(K=2\) to AltUp with a larger expansion factor \(K=4\).

Table 1 summarizes the results with AltUp instantiated on T5 small, base, and large sized models with hyperparameter \(K=2\) and \(K=4\). We observe that a larger value of \(K=4\) leads to strict improvements in pretrain accuracy over AltUp with \(K=2\) for all models (Table 1, column 2). This is perhaps intuitive, as a wider representation vector enables more information to be learned during the pretraining stage. Interestingly, however, a larger \(K\) does not always lead to better finetune performance, especially for smaller models. For example, despite having a worse pretrain accuracy, AltUp with \(K=2\) is better than AltUp with \(K=4\) on all finetune tasks GLUE, SuperGLUE, and SQuAD. We see a similar phenomenon occur for the Base model, but here \(K=4\) is better on GLUE;

  
**Model** & **Pretrain Accuracy** & **GLUE** & **SG** & **SQuAD (EM/F1)** & **TriviaQA (EM/F1)** \\  S & \(61.21\) & \(75.83\) & \(59.52\) & \(76.44/84.97\) & \(19.03/22.83\) \\ S + AltUp (K=2) & \(61.86\) & \(\) & \(\) & \(/\) & \(/\) \\ S + AltUp (K=4) & \(\) & \(76.40\) & \(\) & \(76.38/84.86\) & \(19.07/22.84\) \\  B & \(66.42\) & \(84.25\) & \(73.56\) & \(83.78/91.19\) & \(23.1/27.56\) \\ B + AltUp (K=2) & \(66.96\) & \(\) & \(75.80\) & \(/\) & \(24.35/28.78\) \\ B + AltUp (K=4) & \(\) & \(84.95\) & \(\) & \(84.82/92.07\) & \(/\) \\  L & \(69.13\) & \(87.23\) & \(81.21\) & \(86.77/93.56\) & \(26.15/30.76\) \\ L + AltUp (K=2) & \(69.32\) & \(88.20\) & \(82.75\) & \(/\) & \(27.10/32.04\) \\ L + AltUp (K=4) & \(\) & \(\) & \(\) & \(87.59/94.02\) & \(/\) \\   

Table 1: Performance of AltUp with varying representation dimension scaling parameter \(K\) on T5.

Figure 4: Evaluations of AltUp on T5 models of various sizes and popular benchmarks. AltUp consistently leads to sizeable speedups relative to baselines at the same accuracy. Latency is measured on TPUv3 with 8 cores. Relative speedup is defined as latency delta divided by AltUp latency.

and on Large, the trend reverses: \(K=4\) is better on every metric except for SQuAD. Our results indicate that a larger value of \(K\) has potential to increase the performance of models on pretrain and fine-tune metrics when AltUp is applied to larger models. _We note that there is an inherent trade-off between a larger factor \(K\) and trainability, however, as a larger value of \(K\) leads to less frequent activation of each sub-block which may impair performance_. We envision that practitioners can pick a value of \(K\) other than the default \(K=2\) to optimize performance on an application-specific basis.

### Recycled-AltUp

Next, we consider the performance of the lightweight extension of AltUp, Recycled-AltUp, introduced in Sec. 4. We apply Recycled-AltUp with \(K=2\) to T5 base, large, and XL models and compare its pretrain accuracy and speed to those of baselines. We record both the training speed and inference speed of the resulting models. Since Recycled-AltUp does not require an expansion in the embedding table dimension (see Sec. 4), we remark that the models augmented with it have virtually the same number of trainable parameters as the baseline models.

The results of our experiments are shown in Fig. 5. The figures for both the training and inference speed show that models with Recycled-AltUp clearly improve over baselines in pretrain accuracy, without any perceptible slowdown. While Recycled-AltUp's predictive strength generally falls below that of standard AltUp (cf., pretrain values for AltUp in Table 1), its improved speed and reduced parameter count may make it more suitable for certain applications - see Recycled-AltUp Fine-tune evaluations in the Appendix (Sec. G). We present additional fine-tuning results with Recycled-AltUp in the supplementary material; overall, our results demonstrate that Recycled-AltUp is similarly effective on fine-tuning tasks.

### Sequence-AltUp

Here, we evaluate Sequence-AltUp (from Sec. 4) to reduce the apparent sequence length for the T5 base model. In particular, we apply average pooling, stride-and-skip, and Sequence-AltUp to the encoder layers to reduce the apparent input sequence length. We apply stride-and-skip and Sequence-AltUp to layers \(2,,L-1\) of the encoder, rather than all the layers, with stride length \(4\) as we found that this configuration results in a better accuracy/speed trade-off for both techniques. For average pooling, the sequence length is immutably reduced from the start according to the method.

  
**Model** & **Pretrain Accuracy** & **Finetune GLUE** & **Finetune SG** & **Speed** \\  S & \(61.21\) & \(59.52\) & \(76.44/84.97\) & \(166.1\) \\ B (Baseline) & \(66.42\) & \(73.56\) & \(83.78/91.19\) & \(52.4\) \\  Average pooling & \(63.89\) & \(57.85\) & \(71.37/81.87\) & \(91.9\) \\ Stride-and-Skip & \(65.02\) & \(65.98\) & \(79.72/87.64\) & \(79.4\) \\ Sequence-AltUp & \(\) & \(\) & \(\) & \(74.9\) \\   

Table 2: Performance and pretrain speed of different methods for sequence length reduction on T5.

Figure 5: Recycled-AltUp on T5-B/L/XL compared to baselines. Recycled-AltUp leads to strict improvements in pretrain performance without incurring any perceptible slowdown.

Table 2 presents the comparisons on pretrain and finetune metrics (GLUE and SuperGLUE) and pretrain speed (measured by the number of sequences per second per core). The table additionally lists the relevant metrics for T5 Base (which is the baseline model) and T5 Small as reference points in the table. We observe that average pooling gives a large speed-up, but suffers from severe quality degradation, especially on the finetune metrics where it performs even worse than T5 small. Stride-and-skip and Sequence-AltUp, on the other hand, offer an improved quality and speed trade-off relative to T5 Base. In particular, Sequence-AltUp is only slightly slower than stride-and-skip (yet, still \( 40\%\) faster than the baseline), but is much closer to the baseline model's quality.

## 6 Conclusion

We propose the method of _Alternating Updates_ (AltUp) to increase the capacity of modern transformer models without incurring a significant increase in latency. Our approach bridges the research gap in efficient transformers by enabling the use of wider token representations without widening the transformer layers. AltUp utilizes lightweight prediction and correction steps to update a wider representation vector without increasing the transformer layer's computation cost. As a result, we achieve strong performance improvements on language modeling and language understanding benchmarks. We present extensions of AltUp that enable additional gains in efficiency. Given its orthogonal scope, AltUp can be synergistically applied with existing techniques like MoE. On popular language understanding and QA benchmarks, AltUp enables up to \(87\%\) speedup relative to the dense baselines at the same accuracy.

Limitations and future workA current limitation of the technique we propose is the lack of a deep theoretical understanding of its properties due to the complicated nature of rigorously analyzing transformer models. An interesting open question is whether it would be possible to analyze AltUp by relating its performance to a block compressed layer, and transitively relating that to a wide layer without block compression. A deeper understanding of AltUp may also shed light on the optimal hyperparameter \(K\) on an application-specific basis. In future work, we plan to conduct a theoretical investigation of alternating updates to develop a deeper understanding of its effectiveness across differing applications. We also plan to experiment with the use of a very large expansion factor \(K\).

Broader ImpactTraining and deploying modern neural network models consumes colossal amounts of resources. This leads to detrimental effects on the environment and hampers the widespread applicability and democratization of AI. We envision that AltUp can serve as a valuable component of efficient architectures of the future and help alleviate these negative impacts.