# BIGOS V2 Benchmark for Polish ASR: Curated Datasets and Tools for Reproducible Evaluation

Michal Junczyk

Adam Mickiewicz University, Poznan, Poland, michal.junczyk@amu.edu.pl Allegro, Poznan, Poland, michal.junczyk@allegro.com

###### Abstract

Speech datasets available in the public domain are often underutilized because of challenges in accessibility and interoperability. To address this, a system to survey, catalog, and curate existing speech datasets was developed, enabling reproducible evaluation of automatic speech recognition (ASR) systems. The system was applied to curate over 24 datasets and evaluate 25 ASR models, with a specific focus on Polish. This research represents the most extensive comparison to date of commercial and free ASR systems for the Polish language, drawing insights from 600 system-model-test set evaluations across 8 analysis scenarios. Curated datasets and benchmark results are available publicly. 1 The evaluation tools are open-sourced to support reproducibility of the benchmark, encourage community-driven improvements, and facilitate adaptation for other languages.2

## 1 Introduction

### Background

The Polish language is spoken by more than 50 million people worldwide. The number of available ASR systems and services, as well as speech data resources that support Polish, is systematically growing. However, the community has insufficient resources to methodically evaluate and track progress of ASR (Automatic Speech Recognition) technology for Polish. First, the available data assets are underutilized due to challenges such as accessibility, licensing, and interoperability. Secondly, there is no standardized ASR benchmark dataset. Finally, the tooling to reproduce or systematically extend evaluation scope is missing. As a result Polish ASR systems benchmarks performed so far cover limited number systems and datasets (see Appendix B.1).  These limitations may slow the development of new systems and applications, as reliable evaluations and publicly available _leaderboards_ drive research progress and inform the public about the capabilities of AI technology.  The international ASR community has recognized the importance of evaluation methodologies for consistent and comparative performance assessments in ASR specifically.  and ML field in general  This calls for innovations in the management of ASR datasets and evaluation methods. 

### Research gap

Current data curation and ASR benchmarking methods for low-resource languages such as Polish exhibit several shortcomings:* **Data utilization:** Speech datasets are often underutilized due to limited awareness or restricted accessibility.
* **Data quality:** Insufficient understanding of test sets can lead to an inaccurate representation of state-of-the-art performance.
* **Evaluation reproducibility:** Limited adoption of common benchmark sets impedes the validation and replication of research results.
* **Evaluation scope:** Ecologically valid ASR evaluations require consideration of a broader range of datasets, systems, and performance metrics to ensure comprehensive assessment.

### Contributions

1. **Benchmark datasets curation:** To address the lack of standardized ASR evaluation resources for Polish, a benchmark dataset was curated from 24 openly available sources.3 Diverse samples of both read4 and spontaneous speech5 are included. 2. **Benchmark toolchain development:** A benchmark toolchain was developed to ensure consistent ASR evaluation through standardized protocols, with flexible support for incorporating new datasets, systems, and metrics.6 3. **ASR systems evaluation:** Using the curated dataset, nine ASR systems and twenty-five models, including both commercial and freely available solutions, were evaluated. Variations in performance across different systems, datasets, and speaker demographics were observed. Results are available publicly on the Polish ASR leaderboard.7 4. **Open resources sharing:** All datasets, tools, and evaluation results are made freely available to the research community. This promotes transparency, reproducibility, and collaboration, allowing researchers to leverage the resources for further Polish ASR development or adapt them to other languages.

## 2 Methodology

### System overview

The system developed for data curation and ASR benchmarking encompasses three main processes:

* **ASR speech datasets survey**: Involves analyzing speech data catalogs and taxonomies, creating a dashboard that summarizes and categorizes existing speech datasets.
* **Curation of ASR benchmark dataset**: Includes processing, formatting, and analyzing datasets to create a standardized set for benchmarking ASR systems. BIGOS (Benchmark Intended Grouping of Open Speech) format was used. 
* **Evaluation of ASR systems**: Involves managing the evaluation process, generating results, and presenting performance metrics through a public dashboard for system comparison and analysis.

Figure 1 illustrates the system architecture and the core open tools used for development. The subsequent sections provide a detailed description of the specific processes and tools.

### Survey of datasets

A keyword-based literature review was used to identify and document relevant datasets.  The datasets were manually analyzed and annotated. The final methodology included:

1. Conducting keyword searches in relevant sources
2. Manually analyzing and annotating documentation
3. Cross-checking multiple sources for consistency and accuracy
4. Validating and analyzing downloadable datasets
5. Analyzing metadata to derive insights on Polish ASR speech datasets
6. Making the catalog and insights publicly available

The survey sources include language data repositories, scientific community platforms, and public domain documentation. The attributes considered include creator, funding, license, publication date, quality assurance, and content characteristics such as the format of the audio file and the number of speakers.  Resulting catalog and survey insights are shared on GitHub8 and Hugging Face.9

### Dataset curation

#### 2.3.1 Design considerations

A curated benchmark dataset for Polish ASR systems is intended to have the following features:

* **Task-appropriate:** Relevant and practical for the intended ASR task.
* **Accessible:** Available online under a license allowing the free use and derivative works.
* **Discoverable:** Easy to find and acquire (no registration or other access barriers).
* **Diverse and challenging:** Containing various examples to test the adaptability of the model, as well as complex cases to encourage community participation and minimize the risk of benchmark saturation.
* **Annotated**: With metadata about speakers and recordings allowing nuanced analysis and interpretation of the results.
* **Optimally sized:** Large enough to be representative, but manageable to download and use.
* **Clean yet realistic:** Free of major errors, but noisy enough to represent the complexity of the real world.

Figure 1: Architecture of data curation and ASR evaluation system.

* **Well-documented:** Provided with documentation that is understandable to users without technical skills.
* **Well-explained:** Provided with evaluation baselines and how-to-use script examples.

#### 2.3.2 Leveraging speech data catalog for sourcing open datasets

The Polish ASR speech dataset catalog was used to select datasets for curation.  Following criteria were considered:

* Datasets are available online under a license allowing free use for noncommercial purposes.
* Transcriptions are aligned with the recordings.
* Recording sampling rate is at least 8 kHz.
* Audio files are encoded using at least 16 bits per sample.

Twenty-four source datasets were curated as two new datasets: _BIGOS V2_ and _PELCRA for BIGOS_. Named after the Polish dish _bigos_, a traditional cabbage-based stew -- **BIGOS V2** builds upon its predecessor, _BIGOS (Benchmark Intended Grouping of Open Speech)_10 and offers expanded selection of metadata and recordings from the following source corpora:

* **The Common Voice dataset (_mozilla-common_voice_15-23_)  covers over 60 languages and many underrepresented groups. Available under CC-0 license.
* **The Multilingual LibriSpeech (MLS) dataset (fair-mls-20)** is a large multilingual corpus made by Facebook AI Research (FAIR) . Derived from audiobooks, it covers eight languages, with 44,000 hours of English and 6,000 hours for other languages. The Polish data includes 137 hours from 25 books by 16 speakers. Available under CC-BY license.
* **The Clarin Studio dataset (_clarin-pjatk-studio-15)_ by CLARIN-PL includes 13,802 short utterances (56 hours) from 554 sessions by 317 speakers. Each session has 20-31 audio files, all recorded in a studio for clear audio. Available under CC-BY-SA license.
* **The Clarin Mobile dataset (_clarin-pjatk-mobile-15)_ is a Polish speech corpus of read speech recorded on a telephone. It includes many speakers reading several dozen sentences and words with rare phonemes. Available under CC-BY-SA license.
* **The Jerzy Sas PWR datasets*
* (Politechnika Wroclawska) comprise three legacy sets of recordings available in the public domain:
* Male speaker speech set _(pwr-maleset-unk)_ - single male speaker recordings.
* Utterances containing short words (_pwr-shortwords-unk)_ - single-phoneme conjunctions and prepositions likely to be misrecognized.
* Spoken commands as very important utterances (VIUs) _(pwr-viu-unk)_ - editor control commands and domain-specific utterances.
* **The M-AI Labs Speech corpus (mailabs-19)** created from audiobooks as _MLS_. Intended for training speech recognition and synthesis systems in nine languages, with nearly a thousand hours of audio, including 53.5 hours for Polish. Available under proprietary license.
* **The AZON Read and Spontaneous Speech datasets (_pwr-azon_spont-20, pwr-azon_read-20)_ contain recordings from academic staff in the physical chemistry domain, including both supervised readings and unsupervised spontaneous recordings such as interviews and presentations. Available under a CC-BY-SA license.11 
Compared to predecessor, _BIGOS V2_ contains curated recordings and metadata from the following source corpora:

* **Google FLEURS (_google-fleurs-22)_ is a parallel speech benchmark dataset in 102 languages, based on the FLORes-101 machine translation benchmark.  Hosted on Hugging Face12 and available under a CC-BY license.

* **PolyAI Minds14** (_polyai-minds14-21_) is a dataset for training and evaluating intent recognition systems using spoken data. Covers spoken samples in the commercial e-banking domain in 14 language variations.  Hosted on Hugging Face13 and available under a CC-BY license. Additionally, _PELCRA for BIGOS_ dataset contains recordings and metadata from the following source corpora: * **PolEval 22 Diabiz sample** (_ul-diabiz_poleval-22_) was used for a punctuation restoration task in the 2022 PolEval competition. It is a subset of the _DiaBiz homepage14_ dialog corpus of phone-based customer-agent interactions by the PELCRA group of the University of Lodz. Available publicly under CC-BY-SA-NC-ND and curated for Polish ASR systems benchmarking purposes with the consent of the author. * **SpokesMix15** is a corpus of conversational Polish by the PELCRA group.  It includes speech recordings and word-by-word transcriptions with non-speech events. Available under the CC-BY-NC-ND license and curated with permission of the authors. * **SpokesBiz16** is a corpus of conversational Polish from the CLARIN-BIZ project, featuring over 650 hours of recordings from nearly 600 speakers.  Transcriptions are diarized and manually annotated. Includes eight diverse subsets, e.g. biographical interviews, job interviews, podcasts, and student presentations. Available under the CC-BY-NC-ND license and curated with the authors permission.

Datasheets of curated datasets can be found in Appendices C.9, C.10, C.11, C.12 and Hugging Face.17

#### 2.3.3 Curation process

* **Dataset structure curation:*
* Downloading and manually inspecting format and contents
* Creating train/dev/test splits if not available
* Assigning standard IDs to speakers and files
* **Audio file curation:*
* Removal of invalid and duplicated audio files
* Unifying audio format to WAV 16 bits/16 kHz
* Normalizing audio amplitude to -3 dBFS
* Splitting long audio files into shorter segments based on time-alignment annotations
* **Text files (transcripts and metadata) curation:*
* Converting text encoding to UTF8
* Extracting original transcription and removing redundant characters
* Removal of audio files for utterances containing offensive content
* Extracting and unifying metadata contents
* Generating metadata from text and audio content
* Saving in the standard tabular format
* **Dataset distribution*
* Uploading to the HF dataset hub
* Referencing the original license and authors in the README file

The resulting _BIGOS utterance data object_ with a description of the standard metadata fields is available in Table 24 in the Appendix.

### ASR evaluation

#### 2.4.1 System design considerations

Below is an overview of the main design considerations. Established tools and platforms for data management and evaluation were used whenever feasible (see Appendices C.1 and C.2 for details).

* **Metrics**: Support for well-established metrics for ASR evaluation.
* **Extensibility**: Easy integration of new datasets, normalization methods, metrics, and systems.
* **Availability**: Publicly accessible and intuitive presentation of results.
* **Comprehensiveness**: Performance analysis across scenarios, system parameters, and user groups.

#### 2.4.2 Overview of the evaluation process

The process is presented on figure 2. Currently four evaluation metrics are supported: Sentence Error Rate (SER), Word Error Rate (WER), Match Error Rate (MER) and Character Error Rate (CER).  The definitions are provided in Appendix C.3. The same pipeline was used to normalize both references and hypotheses. The impact of normalization is discussed in section 3.1 and normalization steps are described in Appendix C.5. Python scripts used for the evaluation are available on GitHub.18

In total twenty-five models of nine ASR systems were evaluated: Google STT V1 and V2, Azure STT, Whisper local and cloud, AssemblyAI, NeMo, MMS and Wav2Vec2. The references and details are available in the Appendix C.4. The complete list of evaluated systems and models is presented in table 17.

## 3 Evaluation results

Eight evaluation scenarios encompassing several key dimensions are supported. The _All System Variants_ scenario considers different system-model variants across the entire dataset, while _Subset Analysis_ focuses on evaluating specific subsets of the test data. The _System Type Comparison_ scenario contrasts free versus commercial systems, highlighting differences in performance. _Model Size Evaluation_ assesses variants by their respective model sizes, and _Audio Duration Analysis_ provides insight on the best and worst performing systems for different ranges of audio lengths. _Speaking Rate Evaluation_ examines system performance across varying speech rates, while _Speaker Age Group Analysis_ and _Speaker Gender Analysis_ evaluate system variants based on speaker age and gender demographics, respectively.

All benchmark results can be accessed through the public interactive dashboard.19 Users can display the evaluation results for a specific scenario or perform custom analysis for specific datasets, systems, metrics, normalization techniques, and diagram types. The results of selected scenarios are analyzed in the subsequent sections.

Figure 2: ASR evaluation process data flow.

### Impact of normalization on error rates

Table 1 presents the individual and average error rate reductions, measured in percentage points, for each normalization method applied. Corresponding results for the _PELCRA for BIGOS_ dataset can be found in the Appendix C.6 and online.

### Overall accuracy of available ASR systems and models

Figure 3 show the WER box plot for the systems evaluated using the _BIGOS V2 dataset_. The 3 best ASR models in terms of accuracy are _Whisper Large V3_, _Whisper Cloud_ and _Assembly AI best_. Corresponding results for the _PELCRA for BIGOS_ dataset can be found in the Appendix C.6 and online.

### Subset analysis

Figure4 presents performance across subsets of the _BIGOS V2 dataset_, sorted by median WER. The _CommonVoice_ and _PWR_ subsets are the least challenging overall, though the _pwr-viu-unk_ subset shows high WER for many systems. As revealed by manual inspection, this is caused by hallucinations for unnaturally slow speech rates. The most challenging subsets are _pwr-azon_read20_, _pwr-azon_spont20_ and _polyai-minds14-21_, containing specialized terminology, spontaneous speech and varied accents, respectively. These factors contribute to increased difficulty for ASR systems, leading to significant performance variation across different models.

  
**Method** & **SER [p.p.]** & **WER [p.p.]** & **MER [p.p.]** & **CER [p.p.]** & **Average [p.p.]** \\  blanks & -1.79 & 0.00 & 0.00 & -0.85 & -0.66 \\ lowercase & -2.65 & -6.06 & -6.27 & -1.40 & -4.10 \\ punctuation & -1.40 & -7.61 & -7.95 & -1.67 & -4.66 \\ all & -24.90 & -14.63 & -15.22 & -4.04 & -14.70 \\   

Table 1: Reduction of error rates caused by normalization of references and hypothesis for _BIGOS V2 dataset_

Figure 3: Box plot showing Word Error Rate (WER) distributions for systems evaluated on the _BIGOS V2 dataset_. Lower values indicate better performance, while narrower boxes and whisker ranges demonstrate more consistent performance across the 12 source datasets.

### Comparison of accuracy of commercial and freely available ASR systems

Table 2 compares the WER of commercial and free ASR systems. Commercial systems achieved lower minimum and median WER for BIGOS V2 and PELCRA datasets by approximately 2.5 p.p. and 4.5 p.p., respectively. Furthermore, both commercial and free systems obtained better recognition accuracy for read speech (BIGOS V2) than conversational speech (PELCRA) by approximately 17 and 19 p.p., respectively.

### Accuracy as a function of model size

Figures 4(a) and 5(a) present the relationship between model size and WER for BIGOS and PELCRA datasets, respectively. The figures show that as model size increases, WER generally decreases, indicating improved performance for larger models.

### Accuracy as a function of speech rate

Figures 4(b) and 5(b) illustrate the relationship between WER and speech rate, defined as the average number of words spoken per second.

## 4 Discussion

### Analysis of findings

#### 4.1.1 Impact of normalization

Normalization techniques resulted in significant reductions in error rates for all types of metrics (SER, WER, MER, CER). Applying all methods reduced WER by 15.78 p.p. for the PELCRA dataset and

  
**Dataset** & **Speech** & **Systems** & **Med. WER** & **Mean WER** & **Std. WER** & **Min. WER** \\  BIGOS V2 & read & paid & 12.96 & 17.26 & 24.98 & 0.00 \\ BIGOS V2 & read & free & 15.47 & 21.93 & 19.29 & 2.10 \\ PELCRA & spontaneous & paid & 29.90 & 31.34 & 14.72 & 5.27 \\ PELCRA & spontaneous & free & 34.18 & 37.45 & 19.43 & 8.74 \\   

Table 2: WER statistics for freely available and commercial ASR systems.

Figure 4: Boxplot of Word Error Rate (WER) per subset of _BIGOS V2 dataset_. Each box represents the WER distribution for a subset, with individual ASR systems indicated by unique colors and markers. Lower values indicate better performance.

15.22 p.p. for the _BIGOS V2 dataset_, highlighting the sensitivity of lexical metrics to spelling and formatting variations.

#### 4.1.2 Determining the best systems among free and commercial

Conversational speech (PELCRA) has higher error rates due to its spontaneous nature, with greater variability in style, speed, and pauses. The read speech (BIGOS V2) contains more structured speech, resulting in lower WER.

#### 4.1.3 Impact of model size on WER

Figure 4(a) shows that as the model size increases, the WER generally decreases, with larger models consistently achieving better performance. This trend is clear for models like the _Whisper_ series, although there are significant variations between models of similar sizes, particularly those trained on different datasets, such as _MMS_ and _Wav2vec2_. The _Whisper Large_ models achieve the lowest overall WER, while smaller models such as _Nemo Multilang_ and _Nemo Conformer_ still manage competitive results relative to larger models, demonstrating efficiency.

Figure 5(a) shows that the WER in all models for the PELCRA data set is higher. The trend of decreasing WER with larger models holds for both datasets, but the gains from larger models are more pronounced for conversational, noisy speech from PELCRA dataset, especially for _Whisper Large v1, v2, v3_. While _MMS_ models performed well on BIGOS, they show increased WER for PELCRA, indicating challenges with spontaneous interactions. The efficiency of _Nemo Multilang_ and _Nemo Conformer_ is also notable, though their advantage is reduced for conversational speech.

Figure 5: Example of evaluation scenario results.

Figure 6: Example of evaluation scenario results.

Overall, larger models are particularly beneficial for handling the variability of conversational datasets like PELCRA compared to read speech.

#### 4.1.4 Impact of speech rate on WER

Figures 4(b) and 4(b) show the relationship between Word Error Rate (WER) and speech rate for the BIGOS (read speech) and PELCRA (conversational speech) datasets. In both, WER decreases at moderate rates and rises for extreme speeds.

For BIGOS, WER is lowest between 1-2 words per second, with _assembly_best_ slightly outperforming _whisper_large_v3_. PELCRA shows a broader range up to 8 words per second, with WER lowest between 2-4 words per second, but both models struggle at extreme rates, particularly _whisper_cloud_. Manual inspection revealed a stronger tendency for the _whisper_ model to produce _hallucinations_ in either short recordings with high speech rates or long recordings with slowly pronounced words. Overall, conversational speech presents higher WER due to variability, while moderate speech rates yield optimal performance in both datasets.

### Implications

The developed data curation and evaluation system offers the following benefits for the research community:

* Establishes a standard for evaluating Polish ASR systems, enhancing reproducibility.
* Facilitates better use of datasets, promoting focused research. As of October 30th 2024, _BIGOS V2_ dataset had over 6,500 downloads, while _PELCRA for BIGOS_ had over 1,500 downloads.
* Encourages data sharing and collaboration, improving resources and progress.
* Identifies gaps, such as the need for detailed metadata and semantic metrics, guiding future studies.

Advantages for industry include:

* Informs public about strengths and weaknesses of available ASR system.
* Proposes a standard evaluation procedure to increase evaluation efficiency.
* Showcases the importance of normalization and utilization of metadata for analysis.
* Provides incentive to companies to showcase superior performance on a public benchmark for marketing purposes.

### Limitations and challenges

The reliability of results may be affected if recordings from popular datasets, such as _Common Voice_ and _MLS_, were included in training of the evaluated systems. To address this, new, non-public test recordings should be added to the benchmark dataset. Future research should also include manual transcriptions and annotations to ensure test data quality. Manual and automatic error classification and correction  can also be explored. Adding semantically informed metrics could offer additional insights into task-specific accuracy. [37; 35] Incorporating recordings that represent diverse usage conditions and Polish speaker demographics should improve reliability of assessing ASR systems robustness  and sociodemographic bias. [2; 1] Lastly, newly released systems and model updates could be systematically evaluated and compared with longitudinal studies in other languages .

## 5 Conclusion

The research addresses the issue of limited dataset usage for Polish benchmarking by offering a curated benchmark set derived from 24 publicly available datasets. The evaluation of 9 ASR systems and 25 models revealed notable performance differences between model sizes and speech types. This work improves reproducibility and directs future ASR advancements by providing public access to data catalogs, curated datasets, evaluation tools, and dashboards with comprehensive benchmarking results covering 8 scenarios. Specific methods and tools has potential to be reused for other low-resource languages.

Acknowledgments

The author gratefully acknowledges the original dataset creators for sharing their work openly and allowing its curation in this resource. BibTeX citations for the original authors are provided on curated datasets web pages, and the author hopes that users of datasets curated in _BIGOS_ format will include references to the original sources. 20 The author also extends thanks to anonymous reviewers for their insightful feedback on the first version of this manuscript.