# Learning from Highly Sparse Spatio-temporal Data

Leyan Deng\({}^{}\), Defu Lian \({}^{@sectionsign}\), Chenwang Wu\({}^{}\), Enhong Chen\({}^{@sectionsign}\)

\({}^{}\) School of Artificial Intelligence and Data Science, University of Science and Technology of China

\({}^{@sectionsign}\) School of Computer Science and Technology, University of Science and Technology of China

{dleyan, wcw1996}@mail.ustc.edu.cn, {liandefu, cheneh}@ustc.edu.cn

###### Abstract

Incomplete spatio-temporal data in the real world has spawned much research. However, existing methods often utilize iterative message-passing across temporal and spatial dimensions, resulting in substantial information loss and high computational cost. We provide a theoretical analysis revealing that such iterative models are susceptible to data and graph sparsity, causing unstable performances on different datasets. To overcome these limitations, we introduce a novel method named One-step Propagation and Confidence-based Refinement (OPCR). In the first stage, OPCR leverages inherent spatial and temporal relationships by employing a sparse attention mechanism. These modules propagate limited observations directly to the global context through one-step imputation, which is theoretically affected only by data sparsity. Following this, we assign confidence levels to the initial imputations by correlating missing data with valid data. This confidence-based propagation refines the separate spatial and temporal imputation results through spatio-temporal dependencies. We evaluate the proposed model across various downstream tasks involving highly sparse spatio-temporal data. Empirical results indicate that our model outperforms state-of-the-art imputation methods, demonstrating its effectiveness and robustness.

## 1 Introduction

Spatio-temporal data, encompassing information distributed across both spatial and temporal dimensions, is prevalent in various domains . In real world, the acquisition of spatio-temporal data often faces practical constraints, leading to missing observations. Incomplete data undermines the reliability and effectiveness of subsequent tasks, necessitating robust imputation techniques. However, the existing spatio-temporal works effort addresses temporal missing (i.e., point-level missing), which is usually caused by inevitable device failure. Beyond that, spatial missing (i.e., node-level missing) becomes a burning issue. For example, in a traffic scenario, the management will not deploy dense traffic sensors to reduce costs. Therefore, exploring spatially sparse data not only makes the best use of available data but also lowers the barriers to implementing spatio-temporal models in real scenarios. In view of the gap, Traffic4cast 2022 (T4C22) competition  presented sparse traffic data collected in real urban road networks. Specifically, the competition considered three cities, where sensors are deployed sparsely, i.e. spatial and point missing co-exist. T4C22 aims at generalizing limited data to entire city to predict global traffic dynamics.

We first provide a theoretical analysis for general spatio-temporal iterative imputation model from PAC-learnability perspective. Then we find that iterative methods are constrained by the sparsity and structure of data. As the data scale and missing rate increase, the model requires more iterations. Additionally, with an increasing number of iterations, the model requires more training samples. Therefore, for highly sparse large-scale data, iterative methods suffer from information loss and error accumulation. To address these issues, we propose a sparse attention-based one-step imputation and confidence-based refinement approach. In the first stage, we propagate information from observeddata to all missing data directly, leveraging inherent spatial and temporal correlations. In the second stage, we assign confidence-based propagation weights to the imputed results. Through confidence-based refinement, we eliminate the bias introduced by imputations from separate perspectives.

This paper makes the following contributions.

* We provide an limitation analysis for the popular iterative imputation from the PAC-learnability perspective, which explains the error accumulation caused by multiple iterations.
* Motivated by theoretical results, we propose a one-step propagation strategy to efficiently recover global information from limited observations. Then we assign confidence-based propagation weights to spatio-temporal interactions, refining the imputation results.
* We apply our method to various downstream tasks, the experiments show that our model can learn sufficient information from limited observations than state-of-the-art baselines.

## 2 Related Works

There are many works exploring how to impute different types of sparse data. For tabular data, matrix factorization-based methods are widely popular. A recent work transformed tabular data imputation into link prediction in a bipartite graph, which benefited from the expressive power of graph neural networks (GNNs). For graph data, a simple idea is propagating known features to missing data through the graph structure. For example Feature propagation (FP)  iteratively aggregates neighboring node features and PCFI  further considered pseudo-confidence to propagate messages more accurately. It is clear that GNN can capture spatial relations better. In addition to GNN-based imputation model, SAT  and ITR  both additionally considered structural representations and structure reconstruction objective. However, these graph imputation problems focus on static graph and fixed missing data sets, which is incompatible with dynamic sparse spatio-temporal data.

For time-series data, the existing approaches investigated the imputation performances of different sequence models. For example, BRITS  proposed a bidirectional Recurrent Neural Networks (RNN). SAITS  designed self-attention based model and jointly trained imputation and reconstruction objective. CSDI  proposed a conditional score-based diffusion model. However, these time-series imputation methods ignore intrinsic spatial dependencies in spatio-temporal data.

For spatio-temporal data, existing research leverages topological information effectively. IGNNK  introduced a scalable inductive method trained by reconstructing random sub-graphs. GRIN  was the first to apply Graph Neural Networks (GNNs) to multivariate time-series imputation and proposed a graph-based recurrent neural network. Subsequently, SPIN  identified that auto-regressive models like GRIN are prone to error propagation. SPIN developed an end-to-end architecture utilizing intra-node and inter-node attention mechanisms to address this. However, when dealing with highly sparse large-scale data, propagating valid information through graph structures requires multiple iterations, leading to error accumulation and information loss. In addition, PriSTI  proposed a diffusion-based model. Nonetheless, PriSTI employs two separate attention modules to incrementally aggregate temporal and spatial dependencies, which leads to decoupling the spatio-temporal context. Additionally, PriSTI uses linear interpolation for coarse conditional information, which is inadequate for spatially missing data. Beyond attribute imputation, PoGeVon  also addresses the challenge of missing structural information in spatio-temporal data.

## 3 Preliminaries

**Definition 3.1** (**Spatio-temporal Series**).: Given a fixed graph \(G=(,,)\), where \(\) and \(\) denote the set of \(N\) nodes and \(E\) edges, respectively; \(^{N N}\) is the corresponding adjacency matrix. A spatio-temporal series of length \(T\) is represented by \(^{N T F_{x}}\), where \(F_{x}\) denotes the number of feature dimensions. We represent each spatio-temporal point (ST point) with a tuple \(z=(v,t)\) and \(_{v,t}^{F_{x}}\) denotes the collected data of node \(v\) at time \(t\).

**Definition 3.2** (**Spatio-temporal Mask**).: For spatio-temporal series \(\), we use a binary mask \(\) to indicate missing ST points, where \(m_{v,t}=0\) if \(_{v,t}\) is missing, otherwise \(m_{v,t}=1\). Note that, since a faulty device often miss all records, we ignore the availability of data in feature dimension. Thus,we can define a unique sparse spatio-temporal series \(}\) in terms of \(\) and \(\), i.e.,

\[}=+NaN(-),\] (1)

**Definition 3.3** (Sparse Spatio-temporal Data Learning).: The goal is to accomplish downstream tasks based on the incomplete data \(}\). Without loss of generality, we consider the following objective functions for node-level task.

\[(,})=_{v}l(_{v},}_{v}),\] (2)

where \(l(,)\) is an element-wise loss function that depends on specific downstream tasks. Here \(_{v}\) and \(}_{v}\) are the ground truth and model's prediction for node \(v\), respectively.

For imputation task, the model aims to minimize the reconstruction error as follows.

\[(},})=-|}_{v }_{t}(1-m_{v,t}) l(_{v,t},} _{v,t}),\] (3)

where \(\) denotes the set of time steps and \(l(,)\) is an element-wise loss function.

## 4 Theoretical Analysis

As we stated above, the iterative spatio-temporal imputation methods will suffer from information loss and error accumulation on extremely sparse large-scale data. In order to expose the limitations of iterative models, we provide a theoretical analysis in terms of PAC-learnability for node-level tasks.

We use \(\) and \(\) to denote complete spatio-temporal series and node-level labels, and use \(\) to denote mask distribution. Let \(\) be the fixed but unknown distribution over \(\). We define the mixture distribution \(}\) of \(\) and \(\) using Eq. 1. Given a training set \(=\{(}_{i},_{i})\}^{m}\), we assume that all samples in \(\) are i.i.d. according to \(}\), denoted as \(}^{m}\). Let \(f:\) be a sparse spatio-temporal data learning model, the empirical risk over \(\) and the corresponding generalization risk is defined as follows,

\[:}_{}(f)= {m}_{i=1}^{m}l(f(}_{i}),_{i}),\] \[:_{}(f)=_{( },)}[l(f(}),)]\]

Similarly, we denote empirical and generalization risk under complete data as \(}_{S}(f)\) and \(_{}(f)\).

**Definition 4.1** (PAC-Learnability).: A concept class \(\) is said to be probably approximately correct (PAC) learnable if there exist a learning algorithm \(\) and a polynomial function \(poly(,,,)\) satisfying: for any \(,>0\) and any distribution \(\), the following holds for any sample size \(m poly(1/,1/,n,size(c))\):

\[_{S^{m}}[R_{}(h_{S}) ] 1-.\]

Most of sparse spatio-temporal data learning models have the following encoder-decoder architecture. The encoder \(f_{e}\) learns all ST point representations from sparse data and the decoder \(f_{d}\) outputs node-level predictions for specific downstream task.

\[f_{d}(f_{e}(}))=(_{t}f_{e}( })_{t}_{d}),\]

where \(_{d}\) represents the trainable weight matrix in decoder. In the encoder, the iterative models to learn the missing ST point representations work by continuously extracting valid information from its neighbors. The key differences between iterative models lie in how they define neighbors and how they aggregate messages from these neighbors.

Let \(=\{(v,t)|v,t\}\). For any ST point \(z\), we denote the set of its neighboring ST points at the \(k\)-th iteration by \(_{z}^{k}\). According to the definition of neighbors, we can infer the path between any two ST points. Without loss of generality, for any \(z\), we can formulate the \(k\)-th iteration as follows.

\[_{z}^{k}=(_{z^{}_{z}^{k}}p_{z^{}  z}^{k}_{z^{}}^{k-1}),\]

where \(_{z}^{k}\) denotes the learned representation for ST point \(z\) after the \(k\)-th iteration. For message-passing, the iterative models usually assign weight \(p_{z^{} z}^{k}\) to messages from \(z^{}\) to \(z\). We denote the distance of the path from ST point \(z\) to \(z^{}\) as \(d_{z z^{}}\). Let \(_{o}\) to be the set of all observed ST points and \(_{m}=_{o}\). To ensure that the model has ability to recover all ST points, the number of iterations must satisfies \(K_{z^{}_{m}}_{z_ {o}}d_{z z^{}}\). Therefore, multiple iterations are inevitable in highly sparse large-scale spatio-temporal data.

We then establish the PAC-learnable guarantee for the iterative spatio-temporal imputation methods. First, let us immediately mask some mild assumptions that are easy to implement.

**Assumption 4.2**.: For the weight matrix in decoder, we assume its spectral norm satisfies \(\|_{d}\|_{2} B_{d}\), for any ST point \(z\), we assume its feature vector satisfies \(\|_{z}\|_{2} B_{x}\). For the loss function, we assume \(l\) is an \(C_{l}\)-lipschitz continuous and bounded by \(\). For the activation function, we assume \(\) is \(C_{}\)-lipschitz continuous with \((0)=0\). For the mask distribution \(\), we assume that all ST points are randomly masked with a probability of \(\).

**Proposition 4.3**.: _Let \(\) be a \(K\)-iterations imputation model class, we assume its learned propagation weights are bounded by \([0,]\). If we draw a sample \(\) of size \(m\), for any \(f\), the following inequality holds._

\[_{}^{m}}[R_{} }(h_{})] 1-\]

_with \(m ^{}-4C_{1}}_{m}()}\), where \(=C_{l}^{K} C_{}^{K+1}(d_{})^{K} B _{x}B_{d}\)._

_Remark 4.4_.: Here \(0<<1\), \(d_{}=_{z}|_{z}|\) and \(=_{z}|\{d_{z^{} z}<K|z^{} \}|\). Consider PAC-learnability under complete data, number of required samples needs to satisfy \(m}_{m}())}\). Since \(_{m}()\) represents Rademacher complexity of model class \(\), it is encouraging that the learnability of the model on sparse data is related to the one on complete data. Thus, we can estimate the impact of the missing data on the model performance. Obviously, the required sample size \(m\) is positively correlated with the missing ratio \(\) and the number of model iterations \(K\). Nevertheless, the structural sparsity in the spatio-temporal data itself limits the learnability of the iterative model in addition to the data sparsity. Specifically, \(\) is related to the number of \(k\)-hop neighbors of each ST point. Therefore, the sparser the spatio-temporal structure (i.e., smaller \(\)), the greater the number of samples required. This theoretical result provides an interpretation for iterative model-induced error accumulation. Note that although we assume that all ST points are masked at random (MAR), this proof strategy can be generalized to other mask distribution \(\).

## 5 Methodology

This section presents our proposed method, One-step Propagation and Confidence-based Refinement (OPCR), as illustrated in Fig. 1. In the first stage, we independently learn intrinsic representations for ST points in both spatial and temporal dimensions. We then employ a sparse attention-based one-step propagation strategy to obtain two separate imputation results. In the second stage, we derive imputation confidence based on the learned correlations between ST points to address potential biases in these results. Finally, we perform confidence-based spatio-temporal propagation to refine the final predictions.

### Sparse Attention-based One-step Propagation

Motivated by the aforementioned theoretical findings, we propose a one-step propagation method to avoid error accumulation and information loss commonly found in iterative methods. Specifically, as illustrated in Fig. 1, we begin by independently learning intrinsic spatial and temporal representations for ST points. We capture correlations among ST points across different dimensions by adopting a sparse attention mechanism. This approach enables the limited information to be propagated directly to the global context as efficiently as possible.

#### 5.1.1 Sparse Spatial Attention

We first address the significant spatial sparsity present in spatio-temporal data. To fully utilize limited observation, we learn the correlations between nodes using static spatial information. Leveraging these intrinsic dependencies, messages from all observations are propagated directly to the global context through a sparse attention mechanism. We utilize a \(L_{s}\)-layers Graph Neural Network (GNN)  to learn the spatial representations of nodes from the graph structure. We initialize \(_{0}\) using the inherent static attributes of all nodes; then the layer-wise updation can be summarized as follows.

\[_{l}=(_{l-1}_{1}^{l}+^{-1}_{l-1}_ {2}^{l}),\]

where \(_{1}^{l}\) and \(_{2}^{l}\) are the parameter matrix in the \(l\)-layer; \(\) is the degree matrix of \(\).

In order learn the spatial associations between nodes, we employ a self-attention mechanism . We use node embeddings \(=_{L_{s}}\) as both the key and query. We denote the set of available nodes by \(}\). For any node \(u,v}\), the spatial self-attention is formulated as:

\[^{s},^{s},_{t}^{s}=_{Q}^{s},_{K}^ {s},_{t}_{}}^{s},\] (4) \[_{u,v}^{s}=_{u}^{s},_{v}^{s} )}{_{v^{}}}(_{u}^{s}, _{v^{}}^{s})},\]

where \(^{s},^{s},_{t}^{s}^{N F}\) represent the query, key, value of spatial self-attention; \(_{i}^{s}\), \(_{i}^{s}\) and \(_{i,t}^{s}\) stand for their \(i\)-the row; \(_{Q}^{s},_{K}^{s},_{V}^{s}^{F F}\) are parameter matrices. For any ST point \((v,t)\), we aggregate spatial messages coming from all observations weighted by learned attentions. The spatial dependencies-based imputation result can be computed as:

\[_{v,t}^{s}=(_{v^{}}}_{v,v^{}}^{s}_{v^{},t}^{s}).\] (5)

#### 5.1.2 Sparse Temporal Attention

Another component is designed to learn temporal correlations. We adopt a temporal attention mechanism for the time-series data associated with each node. Unlike recurrent sequence models such as RNN or LSTM, Transformers are not inherently capable of learning sequence information. We first introduce vanilla positional encoding  to capture this information. If real-world timestamps are available, we use a learnable embedding layer to encode these timestamps .

Figure 1: The framework of the proposed OPCR.

Combining positional encoding with timestamp encoding, for any node \(v\), the input to the temporal sparse attention module can be formulated as follows.

\[}_{v}=_{v}+PE(_{v})+MLP(),\] \[PE_{pos,2i}=sin(pos/10000^{2i/d_{model}})\] \[PE_{pos,2i+1}=cos(pos/10000^{2i/d_{model}})\]

where \(_{v}\) is associated time-series of node \(v\); \(\) is the available real-world time information, such as the hour of the day.

For any node \(v\), \(}_{v}\) serves as the key, query, and value in the temporal self-attention mechanism. For node \(v\), similar to the spatial module, we denote the available time steps by \(}_{v}\). Then the temporal attention score and the temporal aggregation are formulated as:

\[^{t}_{v},^{t}_{v},^{t}_{v}=}_{v }^{t}_{Q},}_{v}^{t}_{K},}_{v}^{t}_{V}\] (6) \[(^{t}_{t_{1},t_{2}})_{v}=^{t}_{v, t_{1}},^{t}_{v,t_{2}})}{_{t^{}_{v}_{v}} ((^{t}_{v,t_{1}},^{t}_{v,t^{}_{2}}))},\] \[^{t}_{v,t}=(_{t^{} _{v}}(^{t}_{t,t^{}})_{v}^{t}_{v,t^{}}),\]

where \(^{t}_{v},^{t}_{v},^{t}_{v}^{T F}\) represent the query, key, value of spatial self-attention; \(^{t}_{v,i}\), \(^{t}_{v,i}\) and \(^{t}_{v,i}\) stand for their \(i\)-the row; \(^{t}_{Q}\), \(^{t}_{K},^{t}_{V}^{F F}\) are parameter matrices.

Based on these two sparse attention modules, we propagate all available information to the global missing data in one step. We also give a PAC-Learnability analysis for the proposed method.

**Proposition 5.1**.: _Let \(\) be a sparse attention-based imputation model class. If we draw a sample \(\) of size \(m\), for any \(f\), the following inequality holds._

\[_{}^{m}}[R_{}} (h_{})] 1-\]

_with \(m-2C_{l}_{m}( )|}\), where \(=C_{l} C_{} B_{d} B_{v}\)._

_Remark 5.2_.: It can be seen that the number of required samples is positively correlated with the sparsity \(\). Unlike iteration-based models, sparsity is the sole factor that constrains the performance of the sparse attention-based model when dealing with incomplete data. Consequently, the proposed one-step strategy is scalable to various spatio-temporal data of different sizes and structures.

### Confidence-based Iterative Refinement

In the first stage, we reconstructed the missing representations based on inherent spatial and temporal information. However, these two independent modules decompose the global context. Integrating these results and refining them through spatio-temporal structure is a straightforward idea. To further propagate reliable information, we propose a confidence-based message-passing mechanism. Intuitively, there are correlations between any pairwise spatio-temporal points. Thus, the missing data will remove some dependencies. The larger the correlations between an ST point and other observations, the more plausible its recovery. Therefore, for any ST point \((u,t)\), we can define the confidences of its spatial and temporal imputation results as:

\[_{u,t}=}}(^{s}_{u}, ^{s}_{v})}{_{v}(^{s}_{u},^{s}_{v})}+_{v}}(^{t}_{ t},^{t}_{t^{}})}{_{t^{}}( ^{t}_{t},^{t}_{t^{}})}\] (7)

We extract highly confident parts from separate imputation results in temporal and spatial dimensions. However, simple weighting ignores the global context in spatio-temporal data. Therefore, we propose to employ the learned confidences as propagation weights through the spatio-temporal dimension. For any ST point \((u,t)\), we first define the set of its neighbors as

\[_{u,t}=\{(v,t)|v_{u}\}\{(v,t^{ })|t^{} t\},\]where \(_{u}\) denotes the set of neighboring nodes of node \(u\) in the fixed graph \(G\). Then the layer-wise message-passing for ST point \((u,t)\) can be formulated as

\[_{u,t}^{l+1}=(_{u,t}^{l}\|_{(v,t^{ })_{u,t}}_{v,t^{}}_{v,t^{}}^{l} ),\]

where \(_{u,t}^{0}=_{u,t}^{s}+_{u,t}^{t}\). During \(L\) layers, we finally recover representations for all ST points. Then the decoder is designed depend on different downstream tasks.

## 6 Experiments

We evaluate our approach on three sets of real-world datasets for different downstream tasks, including imputation and prediction. More experimental details and time complexity analysis are provided in the appendix. The source code and datasets are available at https://github.com/dleyan/OPCR.

### Datasets

We consider three sets of spatio-temporal datasets and summarize their statistics in Table 1. It is evident that these datasets vary in terms of topology size. The T4C22 dataset primarily comprises spatially missing data, exhibiting an exceptionally high sparsity of up to 90%.

To simulate incomplete data in realistic scenario, we design two policies to inject missing data into original datasets: 1) Point Missing, in which we follow the same setup of [12; 11], randomly dropping \(\) of the available data. 2) Spatial missing, in which we are inspired by the T4C22 dataset, randomly dropping 25% of the available data, then mask out \(\) of the devices.

### Baselines

We compare the proposed OPCR with various baselines designed for different data types.

* **Tabular Imputation.** We consider three statistical methods for matrix imputation: 1) Mean, imputing missing data using the mean value at each time step. 2) Matrix Factorization (MF)  with rank = 10. 3) GRAPE , a graph-based feature imputation method, which regards observations and features as two types of nodes in a bipartite graph, and the observed feature values as edges.
* **Graph Imputation.** For spatial missing, we consider some node attributes imputation methods: 1) Feature Propagation (FP) , iteratively propagating observed messages through graph structure. 2) PCFI , further measuring the propagation confidence.
* **Time-series Imputation.** To impute multivariate time-series, we consider: 1) BRITS , a bidirectional RNN-based model. 2) SAITS , a transformer-based model. 2) CSDI , a diffusion-based model.
* **Spatio-temporal Data Imputation.** We also consider some state-of-the-art methods for spatio-temporal data imputation. 1) IGNNK , an inductive GNN-based model. 3) SPIN-H , an efficient version of spatio-temporal attention based method. 3) PriSTI , a conditional diffusion model. 4) PoGeVon , solving a specific issue in which spatio-temporal data contains missing values in both node time series features and graph structures.

### Spatio-temporal Data Imputation Task

For the imputation task, we consider mean absolute error (MAE) as evaluation metrics. All the experiments are run with 5 different random seeds. In each round, we inject \(\) of missing data into

    &  &  &  \\  & PEMS-BAY & METR-LA & PV-US & CER-E & LOND & MADRID & MELBOURNE \\  \# Nodes & 325 & 207 & 5166 & 6435 & 59110 & 63397 & 49510 \\ \# edges & 2369 & 1515 & 71446 & 51428 & 132414 & 121902 & 94871 \\ \# steps & 52128 & 34272 & 8688 & 8688 & 10560 & 10464 & 10176 \\   

Table 1: Statistics of the datasets.

the entire dataset using different masking policies. All models are trained on the sparse training set and evaluated on the corresponding sparse testing set.

#### 6.3.1 Highly Sparse Data Imputation

The experimental results of all models are summarized in Table 2, where the missing rate \(\) is set as \(95\%\). There is a clear gap in the imputation performance on the traffic dataset between spatial-missing data and point-missing data, indicating that recovering spatial-missing data is a challenging issue. For the tabular and graph imputation baselines, their performance remains relatively consistent across datasets with spatial and point missing. This can be easily understood, as these baselines only utilize or fail to utilize spatial structure. The time-series imputation baselines are not suitable for spatial missing, as they solely rely on temporal information. In addition, it is difficult for them to model such a high-dimensional time series on large-scale datasets.

For state-of-the-art (SOTA) spatio-temporal series imputation baseline SPIN-H, they are most suitable for small-scale data with point missing. Since they only propagate partial observation iteratively, larger topology and higher spatial missing rate imply a need for more iterations. Therefore, the error accumulation and information loss during iterative process lead to their poor performance. For the inductive baseline IGNNK, it converts the original task to smaller-scale spatio-temporal series via sampling sub-graphs. The effectiveness of IGNNK proves that there are correlations between all nodes, which is consistent with our motivation. However, this sampling strategy still ignores some of the valid information. Therefore, considering the problem of error accumulation in autoregressive methods, some diffusion-based models have been proposed, such as CSDI  and PriSTI . It is still challenging for CSDI to model a high-dimensional time series in the large-scale dataset. However, it show performance improvement on the traffic dataset compared to other temporal baselines (i.e., BRITS and SAITS). For the spatio-temporal method PriSTI, it achieves better performances than SPIN-H on the traffic dataset with spatial missing. However, its performances are unstable on the large-scale datasets, especially the spatially missing data. PoGeVon  is proposed to deal with specific data where attribute-missing and structure-missing co-exist. Therefore, it show worse imputation performances than other spatio-temporal baselines. In addition, PoGeVon does not apply to the large-scale dataset because of its high computational complexity.

As a result, our proposed OPCR outperforms all the baselines in most cases. Especially, on traffic dataset with spatial missing, OPCR exceeds the best baseline by 26% and 37%. on large-scale PVUS dataset, OPCR exceeds the best baseline by 11% and 24% under two settings. These experiments demonstrate the effectiveness and scalability of our proposed OPCR.

#### 6.3.2 Imputation with Increasing Missing Rate

To evaluate the effect of data sparsity on imputation performance, we simulate sparse data at various rate, ranging from 25% to 95%. Figures 2 and 3 report imputation performances on point missing data and spatially missing data, respectively. On the traffic dataset, it is clear that the imputation errors increase with the missing rate. Similar to the results in Table 2, spatial missing poses a greater

    &  &  \\   & PEMS-BAY & METR-LA & PV-US & CER-E & PEMS-BAY & METR-LA & PV-US & CER-E \\  Mean & \(5.00 0.00\) & \(10.28 0.02\) & \(8.34 0.00\) & \(0.56 0.00\) & \(5.08 0.13\) & \(10.37 0.21\) & \(8.45 0.28\) & \(0.56 0.01\) \\ MF & \(5.25 0.00\) & \(7.55 0.04\) & \(4.53 0.01\) & \(0.50 0.00\) & \(5.71 0.07\) & \(7.61 0.11\) & \(12.47 0.24\) & \(0.57 0.01\) \\ GRAPE & \(4.12 0.02\) & \(6.75 0.03\) & \(10.69 0.22\) & \(0.34 0.00\) & \(4.72 0.02\) & \(6.78 0.03\) & \(11.67 0.03\) & \(0.36 0.00\) \\  FP & \(4.92 0.00\) & \(9.22 0.02\) & \(7.60 0.01\) & \(0.52 0.00\) & \(5.05 0.05\) & \(9.25 0.16\) & \(7.87 0.13\) & \(0.52 0.02\) \\ PCFI & \(5.14 0.01\) & \(12.94 0.03\) & \(8.79 0.02\) & \(0.52 0.01\) & \(6.75 0.59\) & \(14.95 1.86\) & \(12.31 0.30\) & \(0.50 0.04\) \\  BRITS & \(2.58 0.00\) & \(5.25 0.10\) & \(30.48 4.75\) & \(1.38 0.99\) & \(5.64 0.16\) & \(8.88 0.52\) & \(25.77 9.00\) & \(1.15 0.28\) \\ SAITS & \(2.44 0.01\) & \(5.21 0.03\) & \(6.40 4.56\) & \(0.67 0.57\) & \(61.42 0.24\) & \(9.51 0.63\) & \(2.73 0.08\) & \(0.37 0.11\) \\ CSDI & \(2.16 0.04\) & \(3.48 0.09\) & \(7.51 2.30\) & \(0.49 0.02\) & \(37.86 0.75\) & \(4.51 0.45\) & \(8.97 1.93\) & \(0.47 0.02\) \\  IGNNK & \(2.61 0.01\) & \(4.37 0.02\) & \(7.86 0.02\) & \(0.38 0.00\) & \(4.70 0.13\) & \(6.85 0.27\) & \(11.43 0.06\) & \(0.46 0.00\) \\ SPIN-H & \(1.84 0.02\) & \(2.99 0.03\) & \(1.94 0.06\) & \(0.33 0.09\) & \(4.93 0.04\) & \(7.62 0.06\) & \(2.63 0.06\) & \(0.29 0.00\) \\ PoGeVon & \(5.68 0.01\) & \(8.86 0.01\) & _/_ & _/_ & \(5.73 0.14\) & \(8.82 0.56\) & _/_ & _/_ \\ PRSTI & \(2.05 0.02\) & \(3.85 0.18\) & \(11.93 2.84\) & \(0.63 0.12\) & \(3.05 0.20\) & \(5.04 0.75\) & \(11.75 7.54\) & \(0.59 0.14\) \\ 
**OPCR** & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Imputation performances (in terms of MAE) on Traffic dataset and Large-scale dataset.

challenge for all models. On large-scale datasets, temporal baselines struggle to model extremely high-dimensional data and do not exhibit the same trends as observed in the traffic dataset. In addition, these models usually terminates training process early then result extremely poor imputation performances. For the inductive spatio-temporal baseline IGNNK, its overall performance show a significant improvement when the missing rate is reduced from 95% to 75%. This empirical results are due to IGNNK can not sample enough sub-graph data for training in highly-sparse data. For SPIN-H, its imputation error on the CER-E dataset is not exactly positively correlated with the missing rate, which is consistent with our theoretical findings. The diffusion-based method PriSTI achieves significant improvements over other baselines on the traffic dataset with spatial missing. However, PriSTI struggle to deal with large-scale dataset. For the proposed OPCR, we can observe that it outperforms baselines in most cases. Furthermore, the imputation performances of our method steadily improves with decreasing missing rates. This experimental result is consistent with the theoretical findings, confirming that the PAC learnability of the proposed OPCR is only constrained by data sparsity.

### Traffic Prediction Task

For the T4C22 dataset, we follow the competition rules. We set the evaluation metric to a weighted cross-entropy loss for the congestion classification task, and MAE for the prediction task. All the models are equipped with the same downstream network and use the same training strategy. Table 3 presents the traffic performance comparison for two downstream tasks. Since we designed strong downstream models for both prediction tasks, the performance gap is insignificant. It can be seen that our proposed OPCR outperforms baselines across all experiments. There are also some findings in the traffic prediction experiments. SPIN-H performs poorly compared to graph imputation baselines (i.e., FP and PCFI). This is because the T4C22 dataset only considers short-term (i.e., 4 time steps) historical series, and its highly spatial sparsity results in slow propagation of valid information.

### Ablation Study

To evaluate the effectiveness of different components of our proposed OPCR, we make comparisons between some model variants. To demonstrate the robustness of our model, we also conduct sensitivity

    &  &  \\   & LONDON & MADRID & MELBOURNE & LONDON & MADRID & MELBOURNE \\  FP & 0.8126 & 0.8624 & 0.8635 & 84.56 & 68.17 & 31.48 \\ PCFI & 0.8159 & 0.8514 & 0.8552 & 105.61 & 61.06 & 31.43 \\ SPIN-H & 0.8335 & 0.8768 & 0.8795 & 77.21 & 59.24 & **31.36** \\ 
**OPCR** & **0.8114** & **0.8444** & **0.8519** & **74.44** & **55.39** & 31.54 \\   

Table 3: Traffic Prediction performances.

Figure 2: Imputation performance (in terms of MAE) with increasing point missing rate.

Figure 3: Imputation performance (in terms of MAE) with increasing spatial missing rate.

analysis with respect to model's hyper-parameters. All models here are trained on the traffic datasets and large-scale datasets with 95% missing rate.

Effectiveness of Each Component.We consider the following variants. 1) Spatial module (S): Only use spatial module as encoder. 2) Temporal module (T): Only use temporal module as encoder. 3) W/O refinement (W/O R): Ignore the confidence-based refinement.

The results of the ablation study are shown in Fig. 4. First, we compare the spatial module and temporal module. Obviously, it is challenging for both variants to handle spatially missing data. On the large-scale dataset, the temporal module presents poorer performances compared to the spatial module. This result is easy to understand because the temporal module utilizes a narrower range of valid information. Second, combining temporal-based and spatial-based results yields superior performances compared to a single module. Finally, when comparing the complete model with all variants, it can be seen that confidence-based propagation also plays a vital role.

Sensitivity w.r.t. Hyper-parameters.First, we consider the effect of hidden states, as shown in Fig. 5. As the hidden states go from 16 to 256, the overall performance slowly increases and then fluctuates slightly. Therefore, we set the number of hidden states to 128. This result reflects the robustness of the proposed model. Then, we consider the sensitivity w.r.t. the number of layers in stage 2, as shown in Fig. 5. The approximate best level can be reached when the number of layers is only 2. These experiments prove that our design motivation is that a shallow iterative propagation is enough to refine the complete recovery by the first stage.

## 7 Conclusion

For the highly sparse spatio-temporal data learning problem prevalent in the real-world, the existing common scheme is iterative propagation-base imputation. However, such methods empirically suffer from error accumulation and high computational costs in large-scale data. Therefore, we first provide a PAC-learnability analysis for iterative imputation methods from the perspective of PAC-learnability, which theoretically demonstrates their limitations. Motivated by theoretical findings, we propose one-step propagation and confidence-based refinement (OPCR). In the first stage, we rapidly propagate useful information to all missing data through a sparse attention mechanism that makes full use of limited observations. To eliminate the bias caused by independent temporal and spatial modeling, we propose assigning confidence to imputation results and achieving more accurate spatio-temporal message-passing. We evaluate the imputation and prediction performances of the proposed OPCR on a dataset of different scales. The experimental results illustrate our approach outperforms the state-of-the-art imputation methods. There are several interesting future directions. First, it would be interesting to explore some downstream tasks without supervision. The second is to extend our theoretical analysis to non-random missing scenarios.

## 8 Acknowledgment

The work was supported by grants from the National Natural Science Foundation of China (No. 92367110).