# Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets

Zhang-Wei Hong\({}^{1}\), Aviral Kumar\({}^{2}\), Sathwik Karnik\({}^{1}\), Abhishek Bhandwaldar\({}^{3}\),

**Akash Srivastava\({}^{3}\), Joni Pajarinen\({}^{4}\), Romain Laroche\({}^{6}\), Abhishek Gupta\({}^{5}\), Pulkit Agrawal\({}^{1}\)**

Correspondence: zwhong@mit.edu, ImprobableAI Lab, Massachusetts Institute of Technology\({}^{1}\), RAIL Lab, UC Berkeley\({}^{2}\), MIT-IBM Lab\({}^{3}\), Aalto University\({}^{4}\), University of Washington\({}^{5}\), and independent researcher\({}^{6}\).

###### Abstract

Offline policy learning is aimed at learning decision-making policies using existing datasets of trajectories without collecting additional data. The primary motivation for using reinforcement learning (RL) instead of supervised learning techniques such as behavior cloning is to find a policy that achieves a higher average return than the trajectories constituting the dataset. However, we empirically find that when a dataset is dominated by suboptimal trajectories, state-of-the-art offline RL algorithms do not substantially improve over the average return of trajectories in the dataset. We argue this is due to an assumption made by current offline RL algorithms of staying close to the trajectories in the dataset. If the dataset primarily consists of sub-optimal trajectories, this assumption forces the policy to mimic the suboptimal actions. We overcome this issue by proposing a sampling strategy that enables the policy to only be constrained to "good data" rather than all actions in the dataset (i.e., uniform sampling). We present a realization of the sampling strategy and an algorithm that can be used as a plug-and-play module in standard offline RL algorithms. Our evaluation demonstrates significant performance gains in 72 imbalanced datasets, D4RL dataset, and across three different offline RL algorithms. Code is available at https://github.com/Improbable-AI/dw-offline-rl.

## 1 Introduction

Offline reinforcement learning (RL) [23; 27] aims to learn a decision-making policy that maximizes the expected return (i.e., the sum of rewards over time) using a pre-collected dataset of trajectories, making it appealing for applications where data collection is infeasible or expensive (e.g., recommendation systems ). Without loss of generality, it can be assumed that the dataset is generated from an _unknown_ policy \(_{}(a|s)\), also known as the _behavior_ policy . The goal in offline RL is to learn a policy, \(_{}(a|s)\) with parameters \(\), that exceeds the performance of the behavior policy. In offline RL, a widely recognized issue is the overestimation of \(Q\)-values for out-of-distribution state-action pairs, leading to suboptimal policies [8; 20; 22]. This stems from incomplete coverage of the state-action space in the dataset, causing the learning algorithm to consider absent states and actions during optimization.

Most state-of-the-art offline RL algorithms [7; 9; 20; 22; 25] mitigate the issue of OOD Q-values by constraining the distribution of actions of the learned policy \(_{}(a|s)\), to be close to the distribution of actions in the dataset. This results in a generic objective with the following form:

\[_{_{}}J(_{})-_{(s,a)} [(s,a)],\]

where \(J(_{})\) denotes the expected return of the policy \(_{}\), \(\) denotes the dataset, \(\) is a regularization term that penalizes the policy \(_{}\) for deviating from the state-action pairs in the dataset, and \(\) is thehyper-parameter balancing the conflicting objectives of maximizing returns while also staying close to the data distribution. This prevents offline RL algorithms from learning behaviors that produce action distributions that diverge significantly from the behavior policy.

An easy-to-understand example of choice for \(\) is the squared distance between the policy and the data , \((s,a):=\|_{}(s)-a\|_{2}^{2}\), where \(_{}(s)\) denotes the mean of the action distribution \(_{}(.|s)\) and \(a\) is an action sampled from the dataset. When the collected dataset is good, i.e., mostly comprising high-return trajectories, staying close to the data distribution is aligned with the objective of maximizing return, and existing offline RL algorithms work well. However, in scenarios where the dataset is skewed or imbalanced, i.e., contains only a few high-return trajectories and many low-return trajectories, staying close to the data distribution amounts to primarily imitating low-performing actions and is, therefore, detrimental. Offline RL algorithms struggle to learn high-return policies in such scenarios . We present a method that overcomes this fundamental limitation of offline RL. Our method is _plug-and-play_ in the sense that it is agnostic to the choice of the offline RL algorithm.

Our key insight stems from the observation that current methods are _unnecessarily conservative_ by forcing the policy \(_{}\) to stay close to _all_ the data. Instead, we would ideally want the policy \(_{}\) to be close to the _best_ parts of the offline dataset. This suggests that we should constrain \(_{}\) to only be close to state-action pairs that would be generated from a policy that achieves high returns, for instance, the (nearly) optimal policy \(^{*}\). In offline scenarios, where collecting additional data is prohibited, to mirror the data distribution of \(^{*}\) as much as possible, we can re-weight existing data (i.e., importance sampling ). We instantiate this insight in the following way: Represent the distribution induced by a better policy \(_{_{w}}\) (initially unknown) by re-weighting data points in the dataset with importance weights \(w(s,a)\) and denote this distribution as \(_{w}(s,a)\). Under this weighting, the offline RL algorithm's training objective can be written as:

\[_{_{}}\,J(_{})-_{(s,a)} [w(s,a)(s,a)],\]

Solving for \(_{}\) using the re-weighted objective constrains the policy to be close to the better policy \(_{}\), and, therefore, allows learning of performant policies. The key challenge is determining the weights \(w\) since the state-action distribution of the better policy \(_{_{w}}\) is initially unknown. To address this, we employ off-policy evaluation techniques  to connect the data distribution \(D_{w}\) with the expected return of the policy \(_{_{w}}\) that would generate it. This allows one to optimize the importance weights with respect to its expected return as follows:

\[_{w}J(_{_{w}})=_{(s,a)_{w}}[r (s,a)]_{(s,a)}[w(s,a)r(s,a)].\]

Here \(r(s,a)\) denotes reward of state-action pair \((s,a)\). By exploiting this connection, we optimize the importance weights \(w\) to maximize the expected return \(J(_{_{w}})\) of its corresponding policy \(_{_{w}}\) subject to necessary constraints (i.e., Bellman flow conservation constraint ). This enables us to obtain a better importance weights \(w\).

We evaluate our method with state-of-the-art offline RL algorithms  and demonstrate performance gain on \(72\) imbalanced datasets . Our method significantly outperforms prior work 

Figure 1: The dots represent actions in the dataset, where imbalanced datasets have more low-return actions. **(a)** Regularized offline RL algorithms  equally regularize the policy \(_{}\) on each action, leading to imitation of low-return actions and a low-performing \(_{}\). The color under the curves shows the policy’s performance \(J(_{})\), with red indicating higher performance and blue indicating lower performance. **(b)** Re-weighting the dataset based on actions’ returns allows the algorithm to only regularize on actions with high returns, enabling the policy \(_{}\) to imitate high-return actions while ignoring low-return actions.

in challenging datasets with more diverse initial states and fewer trajectories (\(20\) smaller than existing datasets). These datasets pose greater challenges, yet they are crucial for practical applications since real-world datasets are often small and exhibit diverse initial states (e.g., robots with different starting positions, where data comes from human teleoperation).

## 2 Preliminaries

**Typical (online) RL.** Reinforcement learning is a formalism that enables us to optimize an agent's policy in a Markov decision process (MDP ). The agent (i.e., decision-maker) starts from an initial state \(s_{0}\) sampled from an initial state distribution \(_{0}(.)\). At each timestep \(t\), the agent perceives the state \(s_{t}\), takes an action \(a_{t}(.|s_{t})\) with its policy \(\), receives a reward \(r_{t}=r(s_{t},a_{t})\) from the environment, and transitions to a next state \(s_{t+1}\) sampled from the environment dynamics \((.|s_{t},a_{t})\) until reaching terminal states. The goal in RL is to learn a policy \(\) to maximize the \(\)_-discounted_ expected, infinite-horizon return \(J^{}()=_{s_{0}_{0},a_{t}(.|s_{t}),s_{t+1} (.|s_{t},a_{t})}_{t=0}^{}^{t}r(s_{t},a_{t}) \). Typical (on-line) RL algorithms estimate the policy \(\)'s expected return (policy evaluation) from trajectories \(=(s_{0},a_{0},r_{0},s_{1},a_{1},r_{1})\) generated by rolling out \(\), and update the policy \(\) toward increasing \(J^{}()\) (policy improvement), and repeat the processing by performing rollouts with the updated policy.

**Offline RL.** With no interaction with the environment allowed during the course of learning, offline RL algorithms aim to learn a policy \(\) that maximizes return, entirely using a fixed dataset \(\) that was collected by an arbitrary and unknown "behavior policy" \(_{}\) (e.g., humans or pre-programmed controllers). These methods typically aim to estimate the return of a policy \(\) via techniques such as Q-learning or actor-critic, only using batches of state-action pairs \((s_{t},a_{t})\) uniformly drawn from \(\). We will denote this estimate value of the return as \(^{}_{}()\). The dataset \(\) consists of \(N\) trajectories rolled out by \(_{}\):

\[:=\{_{i}=(s_{0},a_{0},r_{0},s_{1},a_{1},r_{1} s _{T_{i}})_{i}\}_{i=1}^{N},\] (1)

where \(T_{i}\) denotes the length of \(_{i}\). In practice, a limit on trajectory length is required since we can only collect finite-length trajectories . When the states that the policy \(\) would encounter and the actions that \(\) would take are not representative in the dataset \(\), the estimated return \(^{}_{}()\) is typically inaccurate [8; 20; 22]. Thus, most offline RL algorithms learn the policy \(\) with pessimistic or conservative regularization that penalizes shift of \(\) from the behavior policy \(_{}\) that collected the dataset. Typically, implicitly or explicitly, the policy \(\) learned by most offline RL algorithms can be thought of as optimizing the following regularized objective:

\[_{}^{}_{}()-_{(s_{t},a_{t})}[(s_{t},a_{t})],\] (2)

where \(\) measures some kind of divergence (e.g., Kullback-Leibler divergence ) between \(\) and \(_{}\), and \(^{+}\) denotes the strength of regularization.

## 3 Problem Statement: Unnecessary Conservativeness in Imbalanced Datasets

In this section, we describe the issue of offline RL in imbalanced datasets. While algorithms derived from the regularized offline RL objective (Equation 2) attain good performance on several standard benchmarks , recent work  showed that it leads to _"unnecessary conservativeness"_ on imbalanced datasets  due to the use of constant regularization weight on each state-action pairs \((s,a)\) in Equation 2. To illustrate why, we start by defining _imbalance_ of a dataset \(\) using the positive-sided variance of the returns of the dataset (RPSV  defined in Definition 3.1). In essence, RPSV measures the dispersion of trajectory returns in the dataset. It indicates the room for improvement of the dataset. Figure 2 illustrates the distribution of trajectory returns in imbalanced datasets with high and low RPSV. Datasets with a low RPSV exhibit a pronounced concentration of returns around the mean value, whereas datasets with a high RPSV display a return distribution that extends away from the mean, towards higher returns. Intuitively, a dataset with high RPSV has trajectories with far higher returns than the average return of the dataset, indicating high chances of finding better data distribution through reweighting. Throughout this paper, we will use the term _imbalanced datasets_ to denote datasets with high RPSV.

**Definition 3.1** (Dataset imbalance).: RPSV of a dataset, \(_{+}[G(_{i})]\), corresponds to the second-order moment of the positive component of the difference between trajectory return: \(G(_{i}):=_{t=0}^{T_{i}-1}^{t}r(s^{i}_{t},a^{i}_{t})\) and its expectation, where \(_{i}\) denote trajectory in the dataset:

\[_{+}[G(_{i})]_{_{i}}[ (G(_{i})-_{_{i}}[G(_{i})])_{+} ^{2}] x_{+}=\{x,0\},\] (3)

Imbalanced datasets are common in real-world scenarios, as collecting high-return trajectories is often more costly than collecting low-return ones. An example of an imbalanced offline dataset is autonomous driving, where most trajectories are from average drivers, with limited data from very good drivers. Due to the dominance of low-return trajectories, state-action pairs \((s,a)\) from these trajectories are oversampled in Equation 2. Consequently, optimizing the regularized objective (Equation 2) would result in a policy that closely imitates the actions from low-performing trajectories that constitute the majority of dataset \(\), but ideally, we want the policy to imitate actions only on state-action pairs from high-performing trajectories. However, current offline RL algorithms [21; 18; 7] use constant regularization weight \(\) (Equation 2). As a result, each state-action pairs is weighted equally, which leads the algorithm to be unnecessarily conservative on all data (i.e., imitating all actions of state-action pairs in the dataset). Further analysis of imbalanced datasets can be found in Appendix A.1.

## 4 Mitigating Unnecessary Conservativeness By Weighting Samples

In this section, we seek to develop an approach to address the unnecessary conservativeness issue (Section 3) of regularized offline RL algorithms in imbalanced datasets. Adding more experiences from high-performing policies to the dataset would regularize the policy to keep close to high-performing policies and hence easily mitigate the unnecessary conservativeness issue. Though collecting additional experiences (i.e., state-action pairs \((s,a)\)) from the environment is prohibited in offline RL, importance sampling  can emulate sampling from another dataset \(_{w}\) since the weighting can be regarded as the density ratio shown below:

\[w(s,a)=_{w}(s,a)}{(s,a)},\] (4)

where \(_{w}(s,a)\) and \((s,a)\) denote the probability density of state-action pairs \((s,a)\) in dataset \(_{w}\) and \(\). Note that \(_{w}\) is unknown but implicitly defined through a given weighting function \(w\). This allows us to adjust the sampling distribution that we train the policy \(\) to, as suggested in the following equivalence:

\[_{}_{}^{}()-_{(s_{t},a _{t})}[w(s,a)(s_{t},a_{t})] _{}_{}^{}()-_{(s_{t},a_ {t})_{w}}[(s_{t},a_{t})].\] (5)

The remaining question is: _how can we determine the weighting \(w(s,a)\) so that we emulate sampling from a better dataset \(_{w}\) collected by a policy that achieves higher return than the behavior policy \(_{}\) that collected the original dataset \(\)?_.

### Optimizing the Weightings: Emulating Sampling from High-Performing Policies

Our goal is to discover a weighting function \(w\) that can emulate drawing state-action samples from a better dataset \(_{w}\) that is collected by an _alternative behavior policy_\(_{_{w}}\) with higher return than the behavior policy \(_{}\) that collected the original dataset \(\) (i.e., \(J^{}(_{_{w}}) J^{}(_{})\)). We make use of density-ratio-based off-policy evaluation methods [32; 29; 44] to determine if a weighting function corresponds to a high-return policy. Note that we do not propose a new off-policy evaluation approach but rather apply the existing off-policy evaluation technique in our problem. By using these

Figure 2: Return distribution of datasets with high and low RPSV. Low RPSV datasets have returns centered at the mean, while high RPSV datasets have a wider distribution extending towards higher returns. See Appendix A.4 for details.

techniques, we can relate the weighting \(w\) to the expected return of the alternative behavior policy \(J(_{_{w}})\) via importance sampling formulation as follows:

\[J^{}(_{_{w}})_{(s,a)_{w}} [r(s,a)]=_{(s,a)}[w(s,a)r(s,a)].\] (6)

In Equation 6, \(J^{}(_{_{w}})\) evaluates the quality of a given weighting function \(w\). It also provides a feasible objective to optimize \(w\), since it only requires obtaining samples from the original dataset \(\). However, it is important to note that Equation 6 measures \(\)-discounted return only when the the dataset \(_{w}\) represents a stationary state-action distribution that satisfies Bellman flow conservation constraint  in the MDP, as shown in the following equation:

\[_{w}(s^{})=(1-)_{0}(s^{})+_{s,a} (s^{}|s,a)_{w}(s,a)\; s^{},\;\;\;_{w}(s^{}):=_{a^{}}_{w} (s^{},a^{})\] (7)

where \(\) and \(\) denote the state and action spaces, respectively, and the discount factor \(\) determines what discount factor corresponds to in \(J^{}(_{_{w}})\) in Equation 6. We slightly abuse the notation, denoting state marginal as \(_{w}(s^{})\).

To estimate \(J^{}(_{_{w}})\) from the weighting function \(w\), it is required to impose Bellman flow conservation constraint (Equation 7) on \(w\). However, it is difficult to impose this constraint due to the dependence of initial state distribution \(_{0}\) in Equaton 7. Estimating \(_{0}\) from the first state of each trajectory in the dataset is an option, but it is infeasible when the trajectories do not consistently start from initial states sampled from the distribution \(_{0}\). While we could make the assumption that all trajectories begin from initial states sampled from \(_{0}\), it would limit the applicability of our method to datasets where trajectories start from arbitrary states. We thus choose not to make this assumption since current offline RL algorithms do not require it.

Instead, since the Bellman flow conservation constraint (Equation 7) only depends on the initial state distribution \(_{0}\) when \( 1\), it is possible to bypass this dependence, if we maximize the undiscounted return \(J(_{_{w}})=J^{=1}(_{_{w}})\) (i.e., setting \(=1\) in Equation 6) of the alternative behavior policy \(_{_{w}}\). While it deviates from the RL objective presented in Equation 2, undiscounted return is often more aligned with the true objective in various RL applications, as suggested in . Many RL algorithms resort to employing discounted return as an approximation of undiscounted return instead due to the risk of divergence when estimating the undiscounted return using Q-learning . Thus, we constrain the weighting function \(w\) to satisfy the Bellman flow conservation constraint with \(=1\) as shown below:

\[_{w}(s^{})=_{s,a}(s^{}|s,a)_ {w}(s,a)\; s^{}.\] (8)

To connect the constraint in Equation 8 to the objective in Equation 6, we rewrite Equation 8 in terms of weightings \(w\) according to 2, as shown below:

\[(s^{})w(s^{})=_{s,a}(s^{}|s,a)w(s, a)\; s^{}, w(s):=_{a} _{w}(s,a)}{(s,a)}\] (9)

where \(w(s)\) denotes state marginal weighting. Putting the objective (Equation 6) and the constraint (Equation 9) together, we optimize \(w\) to maximize the undiscounted expected return of the corresponding alternative behavior policy \(_{_{w}}\), as shown in the following:

\[_{w}J(_{_{w}})=_{(s,a)} [w(s,a)r(s,a)]\] (10) subject to \[_{(s,a,s^{})}[w(s^{})-w(s,a) \;|\;s^{}]=0\;\; s^{}.\]

As the weightings \(w\) can be viewed as the density ratio (Equation 4), we call our method as **Density-ratio Weighting (DW)**. We then re-weight offline RL algorithm, as shown in Equation 5. Note that while these weights correspond to \((s,a)\) in the dataset, this is sufficient to reweight policy optimization for offline RL.

### Practical Implementation

**Optimizing weightings.** We begin by addressing the parameterization of the weighting function \(w(s,a)\) and its state marginal \(w(s^{})\) in Equation 10. Though state marginal \(w(s^{})\) can derived from summing \(w(s,a)\) over action space \(\), as defined in Equation 9, it can difficult to take summation over a continuous or infinite action space. Thus we opt to parameterize the weightings \(w(s,a)\) and its state marginal \(w(s^{})\) separately. By using the identities \(_{w}(s,a)=_{w}(s)_{_{w}}(a|s)\) and \((s,a)=(s)_{}(a|s)\), we can represent \(w(s,a)\) as the product of two ratios:

\[w(s,a)_{w}(s,a)}{(s,a)}=_{ w}(s)_{_{w}}(a|s)}{(s)_{}(a|s)}=_{w}(s)}{(s)}_{w}}(a|s)}{ _{}(a|s)}.\] (11)

Michel et al.  showed that ratios can be parameterized by neural networks with exponential output. Thus, we represent state-action weighting \(w(s,a)\) as \(w_{,}(s,a)\) and its state marginal as \(w_{}(s)\), as shown below:

\[w_{,}(s,a)=(s)(s,a), w_{}(s)=(s)\] (12)

where \(\) and \(\) are neural networks. Next, we present how to train both neural network models. As the dataset often has limited coverage on state-action space, it is preferable to add a KL-divergence regularization \(D_{KL}(_{w}||)\) to the objective in Equation 10, as proposed in Zhan et al. . This regularization keeps the state-action distribution \(_{w}\) induced by the learned weighting \(w\) close to the original dataset \(\), preventing \(w_{,}(s,a)\) from overfitting to a few rare state-action pairs in \(\). Note that this does not prevent the learned weightings to provide a better data distribution for regularized offline RL algorithms. See Appendix A.2 for the detailed discussion. Another technical difficulty on training \(w\) is that it is difficult to impose Bellman flow conservation constraint in Equation 10 at every state in the state space since only limited coverage of states are available in the dataset. Thus, we instead use penalty method  to penalize the solution of \(w_{,}\) on violating this constraint in expectation. As a result, we optimize \(w_{,}\) for Equation 10 using stochastic gradient ascent to optimize the following objective (details can be found in Appendix A.3):

\[_{,}_{(s,a,s^{})}[ {w_{,}(s,a)r(s,a)}_{}-_{F}(s^{ })-w_{,}(s,a))^{2}}_{} ]-_{K}(_{w}||)}_{},\] (13)

where \(s^{}\) denotes the next state observed after taking action \(a\) at state \(s\), and \(_{F},_{K}^{+}\) denote the strength of both penalty terms. Note that the goal of our work is not to propose a new off-policy evaluation method, but to motivate ours in the specific objective to optimize the importance weighting for training offline RL algorithms. Importantly, our approach differs from previous off-policy evaluation methods [32; 26; 44], as further discussed in the related works (Section 6).

**Applying the weighting to offline RL.** The weighing function \(w_{,}\) could be pre-trained before training the policy, but this would introduce another hyperparameter: the number of pretraining iterations. As a consequence, we opt to train \(w_{,}\) in parallel with the offline RL algorithm (i.e., value functions and policy). In our experiments, we perform one iteration of offline RL update pairs with one iteration of weighting function update. We also found that weighting both \(_{}^{}()\) and \((s,a)\) at each state-action pairs sampled from the dataset \(\) with \(w_{,}(s,a)\) performs better than solely weighting the regularization term \((s,a)\). For example, when weighting the training objective of implicit Q-learning (IQL)  (an offline RL method), the weighted objective \(J_{_{w}}()\) is: \(_{(s,a)}[w_{,}(s,a)A(s,a)(a|s)]\), where \(A(s,a)\) denotes advantage values. Please, see Appendix A.3 for implementation details. We hypothesize that weighting both the policy optimization objective \(_{}^{}()\) and regularization \((s,a)\) in the same distribution (i.e., same importance weights) is needed to prevent policy \(\) increasing \(_{}^{}()\) by exploiting out-of-distribution actions on states with lower weights \(w_{,}(s,a)\), which could lead to poor performance . Appendix A.5.5 compares weighting both and only one objective. The training procedure is outlined in Algorithm 1.

## 5 Experimental Evaluation

Our experiments aim to answer whether our density-ratio weighting (DW) (Section 4) approach can improve the performance of offline RL algorithms with different types of imbalanced datasets (Section 3). Prior work on imbalanced datasets  focused exclusively on imbalanced datasets with trajectories originating from a similar initial state. However, in real-world scenarios, trajectories can be collected from diverse initial states. For instance, when collecting datasets for self-driving cars, it is likely that drivers initiate the recording of trajectories from drastically different initial locations. Wefound that imbalanced datasets with diverse initial states exhibit a long-tailed distribution of trajectory returns, while those with similar initial states show a bimodal distribution (see Appendix A.4 for details). As diversity of initial states affects the type of imbalance, we focus our experimentation on the two types of datasets: _(i) Trajectories with similar initial states_ and _(ii) Trajectories with diverse initial states_.

Following the protocol in prior offline RL benchmarking , we develop representative datasets of each type using the locomotion tasks from the D4RL Gym suite. Our datasets are generated by combining \(1-\%\) of trajectories from the random-v2 dataset (low-performing) and \(\%\) of trajectories from the medium-v2 or expert-v2 dataset (high-performing) for each locomotion environment in the D4RL benchmark. For instance, a dataset that combines \(1-\%\) of random and \(\%\) of medium trajectories is denoted as random-medium-\(\%\). We evaluate our method and the baselines on these imbalanced datasets across four \(\{1,5,10,50\}\), four environments. Both types of datasets are briefly illustrated below and detailed in Appendix A.4. Additionally, we present the results on the rest of original D4RL datasets in Appendix A.5.

**(i) Trajectories with similar initial states.** This type of datasets was proposed in , mixing trajectories gathered by high- and low-performing policies, as described in Section 3. Each trajectory is collected by rolling out a policy starting from similar initial states until reaching timelimit or terminal states. We consider a variant of smaller versions of these datasets that have small number of trajectories, where each dataset contains \(50,000\) state-action pairs, which is \(20\) times smaller. These smaller datasets can test if a method overfits to small amounts of data from high-performing policies.

**(ii) Trajectories with diverse initial states.** Trajectories in this type of dataset start from a wider range of initial states and have varying lengths. One real-world example of this type of dataset is a collection of driving behaviors obtained from a fleet of self-driving cars. The dataset might encompass partial trajectories capturing diverse driving behaviors, although not every trajectory accomplishes the desired driving task of going from one specific location to the other. As not all kinds of driving behaviors occur with equal frequency, such a dataset is likely to be imbalanced, with certain driving behaviors being underrepresented

### Evaluation Setup

**Baselines and prior methods.** We consider uniform sampling (denoted as Uniform) as the primary baseline for comparison. In addition, we compare our method with two existing approaches for improving offline RL performance on imbalanced datasets: advantage-weighting (AW), proposed in the recent work by  and percentage-filtering (PF) . Both AW and PF sample state-action pairs with probabilities determined by the trajectory's return to which they belong. The sampling probabilities for AW and PF are given as follows:

\[_{}(s^{i}_{t},a^{i}_{t}) ((G(_{i})-V_{0}(s^{i}_{0}))/)\] (Advantage-weighting) (14) \[_{}(s^{i}_{t},a^{i}_{t}) [G(_{i}) G_{K\%}]\] (Percentage-filtering), (15)

where \((s^{i}_{t},a^{i}_{t})\) denotes the state-action pair at timestep \(t\) of trajectory \(_{i}\). \(G_{K\%}\) represents a threshold for selecting the top-\(K\%\) of trajectories, with \(K\) chosen from \(\{10,20,50\}\) as practiced in . \(V(s^{i}_{0})\) denotes the value of the initial state \(s^{i}_{0}\) in trajectory \(_{i}\), and the coefficient \(\) represents the temperature coefficient in a Boltzmann distribution. We consider three levels of \(\): low (L), medium (M), and high (H) in our experiments. Further details of the hyperparameter setup can be found in Appendix A.4. For the following experiments, we implement our DW and the above baselines on the top of state-of-the-art offline RL algorithms: Conservative Q-Learning (CQL) , ImplicitQ-Learning (IQL) , and TD3BC . Note that as AW can provide a better initial sampling distribution to train the weighting function in DW, we initialize training DW with AW sampling (denoted as _DW-AW_) and initialize training DW with uniform sampling (denoted as _DW-Uniform_) in the following experiments. We refer the readers to Appendix A.3 for the implementation details.

**Evaluation metrics.** Following the settings of , we train all algorithm for one million gradient steps with three random seeds in each dataset. We evaluate the performance of policies acquired through each method in the environment corresponding to the dataset by conducting \(20\) episodes every \(1000\) gradient step. To determine the policy's performance at a given random seed, we compute the average returns over \(20\) episodes during the final \(10\) evaluation rounds, each round separated by 1000 gradient steps. We chose to average over the performance at the last \(10\) rounds rather than solely at the last evaluation round because we observed that the performance of offline RL algorithms oscillates during gradient steps. The main performance metric reported is the interquartile mean (IQM)  of the normalized performance across multiple datasets, along with its \(95\)% confidence interval calculated using the bootstrapping method. As suggested in , IQM is a robust measure of central tendency by discarding the top and bottom \(25\)% of samples, making it less sensitive to outliers.

### Scenario (i): Trajectories with Similar Initial States

Figure 2(a) shows IQM of the normalized return for thirty-two different datasets where trajectories start from similar initial states (Section 5). For all the datasets, we use the best hyperparameters for AW and PF found in  and the hyperparameters for DW-AW and DW-Uniform are presented in Appendix A.4. The results demonstrate that both DW-AW and DW-Uniform outperform the uniform sampling approach confirming the effectiveness of our method. Moreover, combining DW with AW enhances the performance of DW, indicating that DW can benefit from the advantages of AW. This is likely because AW can provide a good initial sampling distribution to start training the weighting function in DW. While DW-Uniform did not exceed the performance of AW in this experiment, it should be noted that our method can be applied when datasets are not curated with trajectories such as reset-free or play style datasets where data is not collected in an episodic manner. This is useful for continuing tasks, where an agent (data curator) performs a task infinitely without termination (i.e., locomotion).

**Limited size datasets.** Figure 2(b) presents the results on smaller versions of \(8\) of these datasets used in Figure 2(a). Note that as we observe the higher temperature \(\) enables AW with CQL to perform better in this type of dataset, we additionally consider AW-XH (extra high temperature) for comparison to provide AW with as fair a comparison point as possible. Further details on the hyperparameter settings can be found in Appendix A.4. Our methods consistently achieve significantly higher returns compared to AW and PF when combined with CQL. This suggests that our methods effectively utilize scarce data in smaller datasets better than weighted sampling approaches (AW and PF) that rely on episodic trajectory-based returns rather than purely transition level optimization like DW. In the case of IQL and TD3BC, we see a clear performance improvement of our methods over uniform sampling and PF while our methods perform on par with AW. For IQL, we hypothesize that this is because IQL is less prone to overfitting on small amounts of data due to its weighted behavior cloning objective , which always uses the in-distribution actions. However, it is worth noting that IQL falls short in performance compared to CQL with DW-Uniform. This suggests that IQL may primarily focus on replicating behaviors from high-return trajectories instead of surpassing them, as it lacks the explicit dynamic programming used in CQL.

**Takeaway.** Since CQL with DW-AW outperforms the other two offline RL algorithms in both dataset types, our suggestion is to opt for CQL with DW-AW, especially when dealing with datasets that might exhibit an imbalance and include trajectories originating from comparable initial states.

### Scenario (ii): Trajectories with Diverse Initial States

Figure 4 presents the results on thirty-two datasets of trajectories with diverse initial states (Section 5). We observe that uniform sampling's performance drops significantly in these datasets compared to trajectories with similar initial states, indicating that the presence of diverse initial states exacerbates the impact of imbalance. Both of our methods consistently outperform all other approaches considered in Section 5.2, including AW and PF methods. Notably, even the best-performing variant of AW

(AW-M) falls short of matching the performance of our DW-AW, demonstrating the effectiveness of DW in leveraging the initial sampling distribution provided by AW and furthering its performance. The performance degradation of AW can be attributed to the presence of diverse initial states and varying trajectory lengths in these datasets. In such cases, state-action pairs in trajectories with high returns are not necessarily generated by high-performing policies. For instance, a sub-optimal policy can also easily reach the goal and achieve a high return if it starts close to the goal (i.e., lucky initial states). Consequently, over-sampling state-action pairs from high-return trajectories can introduce bias towards data in trajectories starting from lucky initial states. Although AW attempts to address this issue by subtracting the expected return of initial states (see Section 5.1), our results show that AW has limited success in addressing this issue. This is because the estimated expected returns of initial states can be inaccurate since AW uses the trajectories' returns in the dataset to estimate initial states' returns (i.e., Monte Carlo estimates). The trajectories in the dataset are finite in length, which makes the Monte Carlo estimates of expected returns inaccurate. To conclude, when an imbalanced dataset consists of trajectories starting from diverse initial states, we recommend using DW-AW to re-weight the training objectives in offline RL algorithms.

## 6 Related Work

Our approach builds upon recent advances in off-policy evaluation techniques, specifically density-ratio importance correction estimation (DiCE) . DiCE has been primarily used for policy evaluation [29; 32; 10], while our method make use DiCE (i.e., the learned importance weights) to re-weight samples for offline RL algorithms. Recent works [43; 37; 33; 26] optimize the policy using DiCE via re-weighting behavior cloning with DiCE, while we found it fails to match offline RL algorithms' performance even in datasets with plenty of expert demonstration (Appendix A.5).

Offline imitation learning approaches [15; 30; 41] also consider imbalanced datasets similar to ours. However, these methods assume prior knowledge of which data points are generated by experts,

Figure 3: **(a)** Our methods, DW-AW and DW-Uniform, achieve higher return than Uniform, indicating that DW can enhance the performance of offline RL algorithms on imbalanced datasets. Note that our methods in IQL, although not surpassing AW and PF-10% in performance, ours can be applied to offline RL dataset that are not curated with trajectories. **(b)** Our methods outperform Uniform in CQL, IQL, and TD3BC, indicating no significant overfitting in smaller datasets. DW-AW demonstrates superior returns compared to AW and PF, particularly in CQL, indicating our method effectively leverages limited data. IQL shows limited gains likely due to its difficulties in utilizing data from the rest of low-return trajectories in the dataset (see Section 5.2).

while our approach does not rely on such information. Furthermore, our method can effectively handle datasets that include a mixture of medium-level policies and low-performing policies, whereas existing approaches often rely on expert-labeled data.

Multi-task offline RL algorithms [42; 14] filter data relevant to the current task of interest from datasets collected from multiple task. For example, Yu et al.  employ task relevance estimation based on Q-value differences between tasks. While our motivation aligns with data filtering, our problem setting differs as we do not assume knowledge of task identifiers associated with the data points. Additionally, our dataset comprises varying levels of performance within the same task, while existing works mix data from different tasks.

Support constraints [20; 38; 2; 40] have been proposed as an alternative approach to prevent offline RL algorithms from exploiting out-of-distribution actions, distinct from distributional constraints used in state-of-the-art methods [22; 7; 18]. While support constraints theoretically suit imbalanced data, the prior work  found that support constraints have not shown significant improvements beyond distributional constraint-based algorithms. Note that our method is independent of the constraint used in offline RL algorithms. Thus support constraints is orthogonal to our approach.

## 7 Conclusion, Future Directions, and Limitations

Our method, density-ratio weighting (DW) improves the performance of state-of-the-art offline RL algorithms [22; 18] over 72 imbalanced datasets with varying difficulties. In particular, our method exhibits substantial improvements in more challenging and practical datasets where the trajectories in the dataset start from diverse initial states and only limited amount of data are available. Future works can explore other optimization techniques to better address the Bellman flow conservation constraint in importance weights optimization (e.g., Augmented Lagrangian method ). Additionally, it would be valuable to study the impact of violating this constraint on the effectiveness of importance-weighted offline RL algorithms.

**Limitations.** Although our method improves performance by optimizing sample weights, we lack theoretical guarantees due to the absence of a unified theoretical analysis on the dependence of state-of-the-art offline RL algorithms on imbalanced data distribution. While some theoretical works [4; 36] have analyzed the interplay between data distribution and offline RL algorithm performance, they primarily focus on specific algorithms that differ significantly from the practical state-of-the-art offline RL algorithms.

Figure 4: Results on imbalanced datasets with trajectories starting from diverse initial states (Section 5.3). Compared to Figure 2(a), the performance of uniform sampling and AW decrease, showing that diverse initial states exacerbate the issue of imbalance. Our methods, DW-AW and DW-Uniform, achieve higher return than all the baselines, which suggests DW is advantageous in broader types of imbalanced datasets.

## Author Contributions

* **Zhang-Wei Hong:** Led the project and the writing of the paper, implemented the method, and conducted the experiments.
* **Aviral Kumar:** Advised the project in terms of theory, algorithm development, and experiment design. Revised the paper and positioned the paper in the field.
* **Sathwik Karnik:** Prepared the datasets and proofread the paper.
* **Abhishek Bhandwaldar:** Helpd scaling up experiments in the cluster.
* **Akash Srivastava:** Advised the project in the details of the practical and theoretical algorithm design and coordinated the compute.
* **Joni Pajarinen:** Advised the project in the details of the practical and theoretical algorithm design and experiment designs.
* **Romain Laroche:** Advised the project in the theory of the algorithms and dataset designs.
* **Abhishek Gupta:** Advised the project in terms of theory, algorithm development, and experiment design. Revised the paper and positioned the paper in the field.
* **Pulkit Agrawal:** Coordinated the project, revised the paper, and positioned the paper in the field.