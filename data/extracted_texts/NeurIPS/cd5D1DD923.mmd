# DeepACO: Neural-enhanced Ant Systems for Combinatorial Optimization

Haoran Ye1, Jiarui Wang1, Zhiguang Cao2,*, Helan Liang1,*, Yong Li3

1 School of Computer Science & Technology, Soochow University

2 School of Computing and Information Systems, Singapore Management University

3 Department of Electronic Engineering, Tsinghua University

{hrye, jrwangfurffico}@stu.suda.edu.cn, zgcao@smu.edu.sg, hlliang@suda.edu.cn, liyong07@tsinghua.edu.cn

* Corresponding authors: Zhiguang Cao and Helan Liang.

###### Abstract

Ant Colony Optimization (ACO) is a meta-heuristic algorithm that has been successfully applied to various Combinatorial Optimization Problems (COPs). Traditionally, customizing ACO for a specific problem requires the expert design of knowledge-driven heuristics. In this paper, we propose DeepACO, a generic framework that leverages deep reinforcement learning to automate heuristic designs. DeepACO serves to strengthen the heuristic measures of existing ACO algorithms and dispense with laborious manual design in future ACO applications. As a neural-enhanced meta-heuristic, DeepACO consistently outperforms its ACO counterparts on eight COPs using a single neural model and a single set of hyperparameters. As a Neural Combinatorial Optimization method, DeepACO performs better than or on par with problem-specific methods on canonical routing problems. Our code is publicly available at https://github.com/henry-yeh/DeepACO.

## 1 Introduction

Ant systems in nature are self-learners. They utilize chemical signals and environmental cues to locate and return food to the colony. The pheromone trails deposited by ants indicate the quality and distance of a food source. The intensity of pheromone trails increases as more ants visit and decreases due to evaporation, creating a self-learning foraging system.

Inspired by the ant systems in nature, researchers propose and develop Ant Colony Optimization (ACO) meta-heuristics for (but not limited to) Combinatorial Optimization Problems (COPs) . ACO deploys a population of artificial ants to explore the solution space through repeated solution constructions and pheromone updates. The exploration is biased toward more promising areas through instance-specific pheromone trails and problem-specific heuristic measures. Both the pheromone trail and the heuristic measure indicate how promising a solution component is. Typically, pheromone trails are initialized uniformly for all solution components and learned while solving an instance. On the contrary, heuristic measures are predefined based on prior knowledge of a problem, and devising proper heuristic measures for complicated COPs is quite challenging (an example is ).

Over the past decades, research and practice efforts have been dedicated to a careful design of heuristic measures in pursuit of knowledge-driven performance enhancement . However, this routine of algorithm customization exhibits certain deficiencies: 1) it requires extra effort and makes ACO less flexible; 2) the effectiveness of the heuristic measure heavily relies on expert knowledge and manual tuning; and 3) designing a heuristic measure for less-studied problems can be particularly challenging, given the paucity of available expert knowledge.

This paper proposes DeepACO, a generic neural-enhanced ACO meta-heuristic, and a solution to the above limitations. DeepACO serves to strengthen the heuristic measures of existing ACO algorithms and dispense with laborious manual design in future ACO applications. It mainly involves two learning stages. The first stage learns a problem-specific mapping from an instance to its heuristic measures by training neural models across COP instances. Guided by the learned measures, the second stage learns instance-specific pheromone trails while solving an instance with ACO. The heuristic measures learned in the first stage are incorporated into ACO (the second learning stage) by biasing the solution constructions and leading Local Search (LS) to escape local optima.

DeepACO is also along the line of recent progress in Neural Combinatorial Optimization (NCO) [12; 61; 7; 34]. Within the realm of NCO, DeepACO is more related to the methods that utilize heatmaps for algorithmic enhancement [32; 42; 90; 55; 42], but it is superior in its flexibility: DeepACO provides effective neural enhancement across eight COPs covering routing, assignment, scheduling, and subset problems, being the most broadly evaluated NCO technique to our knowledge. In addition, we propose three extended implementations for better balancing between exploitation and exploration: one featuring a multihead decoder, one trained with an additional top-\(k\) entropy loss, and one trained with an additional imitation loss. They can be generally applied to heatmap-based NCO methods.

As a neural-enhanced version of ACO, DeepACO consistently outperforms its counterparts on eight COPs using a single neural model and a single set of hyperparameters after only minutes of training. As an NCO method, DeepACO performs better than or competitively against the state-of-the-art (SOTA) and problem-specific NCO methods on canonical routing problems while being more generalizable to other COPs. To the best of our knowledge, we are the first to exploit deep reinforcement learning (DRL) to guide the evolution of ACO meta-heuristics. Such a coupling allows NCO techniques to benefit from decades of ACO research (notably regarding theoretical guarantees [7; 26]), while simultaneously offering ACO researchers and practitioners a promising avenue for algorithmic enhancement and design automation.

In summary, we outline our **contributions** as follows:

* We propose DeepACO, a neural-enhanced ACO meta-heuristic. It strengthens existing ACO algorithms and dispenses with laborious manual design in future ACO applications.
* We propose three extended implementations of DeepACO to balance exploration and exploitation, which can generally be applied to heatmap-based NCO methods.
* We verify that DeepACO consistently outperforms its ACO counterparts across eight COPs while performing better than or on par with problem-specific NCO methods.

## 2 Related work

### Neural Combinatorial Optimization

Neural Combinatorial Optimization (NCO) is an interdisciplinary field that tackles COPs with deep learning techniques. In general, existing NCO methods can be categorized into end-to-end and hybrid methods, and DeepACO belongs to the latter methodological category.

End-to-end methods in the former category learn autoregressive solution constructions or heatmap generation for subsequent sampling-based decoding. Within this realm, recent developments include better-aligned neural architectures [82; 64; 54; 50; 89; 13; 46], improved training paradigms [6; 56; 52; 83; 10; 66; 40; 45], advanced solution pipelines [47; 51; 48; 20; 78; 18; 65; 19], and broader applications [88; 95; 16; 29; 75; 9]. End-to-end methods are admirably efficient, but their constructed solutions can be further improved with iterative refinement and algorithmic hybridization.

Therefore, the hybrid methods in the latter category incorporate neural learners to make decisions within/for heuristics or generate heatmaps to assist heuristics. These methods either let neural learners make decisions within algorithmic loops [60; 87; 21; 17; 94; 86; 59], or generate heatmaps in one shot to assist subsequent algorithms. In the latter case, Xin et al.  propose to learn edge scores and node penalties to guide the searching process of LKH-3 , a highly optimized solver for routing problems. Kool et al.  train neural models to predict promising edges in routing problems, providing a neural boost for Dynamic Programming. Fu et al.  train small-scale GNN to build heatmaps for large TSP instances and feed the heatmaps into Monte Carlo Tree Search for solution improvements. Hudson et al.  utilize neural models to generate regret in Guided Local Search for TSP. DeepACO is along the line of these works, but superior in terms of its flexibility, i.e., it provides effective neural enhancement across eight COPs covering routing, assignment, scheduling, and subset problems while the existing hybridized methods mostly focus on a limited set of routing problems.

### Ant Colony Optimization

Ant Colony Optimization (ACO) is a meta-heuristic and evolutionary algorithm (EA) [4; 73] inspired by the behavior of ants in finding the shortest path between their colony and food sources .

The representative ACO meta-heuristics such as Ant System (AS) , Elitist AS , and MAX-MIN AS  provide general-purpose frameworks allowing for problem-specific customization. Such customization may involve designs of heuristic measures [49; 33], the incorporation of local search operators [3; 93], and the hybridization of different algorithms [44; 5]. DeepACO is not in competition with the most SOTA ACO algorithms for a specific COP. Instead, DeepACO can strengthen them with stronger heuristic measures and can in turn benefit from their designs.

ACO can utilize a group of techniques named hyper-heuristics  that are conceptually related to DeepACO. But hyper-heuristics mostly involve expert-designed heuristics to select from (heuristic selection hyper-heuristics)  or problem-specific and manually-defined components to evolve heuristics (heuristic generation hyper-heuristics) . By comparison, DeepACO is more generic, requiring little prior knowledge of a COP. Its aim is not to compete with any hyper-heuristics; instead, for example, we can utilize hyper-heuristics to improve LS components in DeepACO.

Unlike the knowledge-driven ACO adaptations above, a recent method, namely ML-ACO , boosts the performance of ACO via Machine Learning (ML) for solution prediction. While ML-ACO provides an initial insight into coupling ML with ACO, it is preliminary and limited. ML-ACO tailors five features for Orienteering Problem and trains ML classifiers in a supervised manner. It entails high-demanding expert knowledge for feature designs and specialized solvers for optimal solutions, making itself inflexible and hard to scale. By comparison, DeepACO leverages DRL and demands little expert knowledge to apply across COPs.

## 3 Preliminary on Ant Colony Optimization

The overall ACO pipeline is depicted in Fig. 1. We begin with defining a COP model and a pheromone model. They are prerequisites for implementing ACO algorithms.

COP modelGenerally, a COP model consists of, 1) a search space \(\) defined over a set of discrete decision variables \(X_{i}\), \(i=1,,n\), where each decision variable \(X_{i}\) takes a value from a finite set \(_{i}=\{v_{i}^{1},,v_{i}^{|_{i}|}\}\); 2) a set of constraints \(\) that the decision variables must satisfy; and 3) an objective function \(f:_{0}^{+}\) to minimize. A feasible solution \(\) to a COP is a complete assignment of all decision variables that satisfies all the constraints in \(\).

Pheromone modelA COP model defines a pheromone model in the context of ACO. Without loss of generality, a pheromone model is a construction graph that includes decision variables as nodes and solution components as edges . Each solution component \(c_{ij}\), that represents the assignment of value \(v_{i}^{j}\) to decision variable \(X_{i}\), is associated with its pheromone trial \(_{ij}\) and heuristic measure \(_{ij}\). Both \(_{ij}\) and \(_{ij}\) indicate how promising it is to include \(c_{ij}\) in a solution. Typically, ACO uniformly initializes and iteratively updates pheromone trails, but predefines and fixes heuristic measures.

As a motivating example, for TSP, a instance can be directly converted into a fully connected construction graph, where city \(i\) becomes node \(i\) and solution component \(c_{ij}\) represents visiting city \(j\) immediately after city \(i\). Then, \(_{ij}\) is typically set to the inverse of the distance between city \(i\) and \(j\). After defining a COP model and a pheromone model, we introduce an ACO iteration, usually consisting of solution constructions, optional LS refinement, and pheromone updates.

Solution construction and local search (optional)Biased by \(_{ij}\) and \(_{ij}\), an artificial ant constructs a solution \(=\{s_{t}\}_{t=1}^{n}\) by traversing the construction graph. If an ant is located in node \(i\) at the \(t\)-th construction step (\(s_{t-1}=i\)) and has constructed a partial solution \(_{<t}=\{s_{t}\}_{t=1}^{t-1}\), the probability of selecting node \(j\) as its next destination (\(s_{t}=j\)) is typically given by

\[P(s_{t}|_{<t},)=\{^{}_{ij}^{}}{_{c_{ii}(_{<t})} _{il}^{}_{il}^{}}&c_{ij}(_{<t}),\\ 0&.\] (1)

Here, \(\) is a COP instance, \((_{<t})\) is the set of feasible solution components given the partial solution, and \(\) and \(\) are the control parameters, which are consistently set to 1 in this work unless otherwise stated. To simplify the notations, we omit the dependence on \(\) for \(\), \(\), \(c\), and \(\). Based on Eq. (1), constructing a complete solution requires an \(n\)-step graph traversal. The probability of generating \(\) can be factorized as

\[P(|)=_{t=1}^{n}P(s_{t}|_{<t},).\] (2)

After solution constructions, local search (LS) is optionally applied to refine the solutions.

Pheromone updateAfter solution constructions, the pheromone update process evaluates solutions and adjusts the pheromone trails accordingly, i.e., it increases the pheromone trails of components in the superior solutions while decreasing those in the inferior ones. The detailed update rules can differ depending on the ACO variation used.

ACO intelligently explores the solution space by iterating the above process, eventually converging on (sub)optimal solutions. We refer the readers to  for more details.

## 4 Methodology

DeepACO is schematically presented in Fig. 1 wherein a comparison is made with ACO. It dispenses with expert knowledge and learns a set of stronger heuristic measures to guide the ACO evolution. DeepACO involves parameterizing the heuristic space (Section 4.1), optionally interleaving LS with neural-guided perturbation (Section 4.2), and training a heuristic learner across instances (Section 4.3). Additionally, we introduce three extended designs (Section 4.4) to boost exploration.

### Parameterizing heuristic space

We introduce a heuristic learner, defined by a graph neural network (GNN) with trainable parameters \(\), to parameterize the heuristic space. The heuristic learner maps an input COP instance \(\) to its

Figure 1: The schematic diagrams of ACO and DeepACO. DeepACO 1) additionally trains a heuristic learner across instances, 2) applies the well-trained heuristic learner to generate heuristic measures during inference, and 3) optionally leverages the learned heuristic measures to conduct local search interleaved with neural-guided perturbation.

heuristic measures \(}()\), where we rewrite it as \(}\) for notation simplicity. It contains non-negative real values \(_{ij;}\) associated with each solution component \(c_{ij}\), \( i\{1,,n\}, j\{1,,|}|\}\). DeepACO constructs solutions following Eq. (2) but biased by \(}\):

\[P_{}}(|)=_{t=1}^{n}P_{}}( s_{t}|_{<t},).\] (3)

In particular, we exploited the neural models recommended by Joshi et al.  and Qiu et al. . It consists of a GNN backbone relying on anisotropic message passing and an edge gating mechanism, and a Multi-Layer Perceptron (MLP) decoder mapping the extracted edge features into heuristic measures. We defer the full details of this neural architecture to Appendix A.

### Local search interleaved with neural-guided perturbation

In ACO, local search (LS) is optionally applied to refine the constructed solutions. However, LS is a myopic procedure in that it greedily accepts any altered solution with a lower objective value and easily gets trapped in local optima. In DeepACO, we intend the well-learned heuristic measures to indicate the global optimality of solution components. Leveraging such indicators and including solution components with greater global optimality can lead to better solutions eventually, if not immediately. Nevertheless, it is unrealistic to solely rely on the learned heuristic measures due to the inherent complexity of COPs.

Based on these considerations, we propose LS interleaved with neural-guided perturbation (NLS for short) in Algorithm 1. NLS interleaves LS aiming for a lower objective value and neural-guided perturbation biasing the learned optima. In each iteration, the first stage utilizes LS to repeatedly refine a solution until (potentially) reaching local optima. The second stage utilizes LS to slightly perturb the locally optimal solution toward gaining higher cumulative heuristic measures.

```
1:Input: A solution \(\); an objective function \(f\); well-learned heuristic measures \(}\); a local search operator \(LS\) that takes three inputs: a solution to refine, the targeted objective function, and the maximum iterations before encountering local optima; the number of perturbation moves \(T_{p}\); the number of NLS iterations \(T_{NLS}\)
2:Output: The best improved solution \(^{*}\)
3:\(=LS(,f,+)\)  // Improve \(\) until reaching a local optimum
4:\(^{*}=copy()\)
5:for\(iter=1 T_{NLS}\)do
6:\(=LS(,},T_{p})\) // Perturb \(\) toward higher cumulative heuristic measures with \(T_{p}\) moves
7:\(=LS(,f,+)\)
8:\(^{*}=(f(^{*}),f())\)
9:endfor ```

**Algorithm 1** NLS

### Training heuristic learner

We train the heuristic learner across COP instances. The heuristic learner \(\) maps each instance \(\) to its heuristic measures \(}\). Then, we minimize the expected objective value of both constructed solutions and NLS-refined constructed solutions:

\[(|)=_{  P_{}}(|)}[f()+Wf(NLS(,f,+ ))],\] (4)

where \(W\) is a coefficient for balancing two terms. Intuitively, the first loss term encourages directly constructing optimal solutions, which is, however, often held off by the complexity of COP. The second term encourages constructing solutions most fit for NLS, and it could be easier to learn high-quality solutions if coupling end-to-end construction with NLS. Even so, solely relying on the second term leads to inefficient training due to the small quality variance of the NLS-refined solutions. As a result, we find it useful to implement both terms for training. Note that the NLS process itself does not involve gradient flow.

In practice, we deploy Ant Systems to construct solutions stochastically following Eq. (3) for estimating Eq. (4). The pheromone trials are fixed to 1 to ensure an unbiased estimation. We apply a REINFORCE-based  gradient estimator:

\[(|)=_{ P_{ }(|)}[((f()-b())+W(f(NLS(,f,+ ))-b_{NLS}()))_{} P_{ }(|)],\] (5)

where \(b()\) and \(b_{NLS}()\) are the average objective value of the constructed solutions and that of the NLS-refined constructed solutions, respectively.

### Toward better exploration

The vanilla DeepACO fully exploits the underlying pattern of a problem and learns a set of aggressive heuristic measures (visualized in Appendix C.6) according to Eq. (4). Still, preserving exploration is of critical importance since COPs often feature many local optima . To that end, we further present three extended designs to enable a better balance of exploration and exploitation. Note that they can generally be applied to heatmap-based NCO methods.

#### 4.4.1 Multihead decoder

Multihead DeepACO implements \(m\) MLP decoders on the top of the GNN backbone. It aims to generate diverse heuristic measures to encourage exploring different optima in the solution space. A similar multihead design has been utilized for autoregressive solution construction . We extend this idea to the non-autoregressive heuristic generation here.

For training, Multihead DeepACO utilizes Eq. (4) calculated individually and independently for \(m\) decoders and an extra Kullback-Leibler (KL) divergence loss expressed as

\[_{KL}(|)=-n}_{k=1}^{m}_{ l=1}^{m}_{i=1}^{n}_{j=1}^{|_{i}|}}_{ij;} ^{k}}_{ij;}^{k}}{}_{ij; }^{l}},\] (6)

where \(}_{ij;}^{k}\) is the heuristic measure of the solution component \(c_{ij}\) output by the \(k\)-th decoder after row-wise normalization.

In the inference phase, Multihead DeepACO deploys \(m\) ant groups guided by their respective MLP decoders while the whole ant population shares the same set of pheromone trails.

#### 4.4.2 Top-\(k\) entropy loss

Entropy loss (reward) incentivizes agents to maintain the diversity of their actions. It is often used as a regularizer to encourage exploration and prevent premature convergence to suboptimal policies [68; 37; 51]. In the context of COP, however, most solution components are far from a reasonable choice for the next-step solution construction. Given this, we implement a top-\(k\) entropy loss while optimizing Eq. (4), promoting greater uniformity only among the top \(k\) largest heuristic measures:

\[_{H}(|)=_{i=1}^{n}_{j _{i}}}_{ij;}(}_{ij;}).\] (7)

Here, \(_{i}\) is a set containing top \(k\) solution components of decision variable \(X_{i}\), and \(}_{ij;}\) are the heuristic measures normalized within the set.

#### 4.4.3 Imitation loss

If expert-designed heuristic measures are available, we can additionally incorporate an imitation loss, enabling the heuristic learner to mimic expert-designed heuristics while optimizing Eq. (4). It holds two primary merits. First, the expert-designed heuristic measures are less aggressive than the learned ones. So, it can function as a regularization to maintain exploration. Second, the heuristic learner can acquire expert knowledge. Perfecting the imitation first can guarantee that the learned heuristics are at least not inferior to the expert-designed counterparts.

Accordingly, the imitation loss is formulated as

\[_{I}(|)=_{i=1}^{n}_{j=1}^{| _{i}|}}_{ij}^{*}}_{ij}^{*}}{ }_{ij;}},\] (8)where \(^{*}_{ij}\) and \(_{ij;}\) are expert-designed and learned heuristic measures, respectively, both after row-wise normalization.

## 5 Experimentation

### Experimental setup

BenchmarksWe evaluate DeepACO on eight representative COPs, including the Traveling Salesman Problem (TSP), Capacitated Vehicle Routing Problem (CVRP), Orienteering Problem (OP), Prize Collecting Traveling Salesman Problem (PCTSP), Sequential Ordering Problem (SOP), Single Machine Total Weighted Tardiness Problem (SMTWTP), Resource-Constrained Project Scheduling Problem (RCPSP), and Multiple Knapsack Problem (MKP). They cover routing, assignment, scheduling, and subset COP types. Their definitions and setups are given in Appendix D.

HardwareUnless otherwise stated, we conduct experiments on 48-core Intel(R) Xeon(R) Platinum 8350C CPU and an NVIDIA GeForce RTX 3090 Graphics Card.

### DeepACO as an enhanced ACO algorithm

In this part, we do not apply NLS and set \(W\) in Eq. (4) to 0 for training. It only entails minutes of training to provide substantial neural enhancement as shown in Fig. 2 (also see Appendix C.4). On the other hand, the extra inference time for DeepACO is negligible; for example, it takes less than 0.001 seconds for a TSP100.

DeepACO for fundamental ACO algorithmsWe implement DeepACO based on three fundamental ACO meta-heuristics: Ant System , Elitist Ant System , and MAX-MIN Ant System . They are widely recognized as the basis for many SOTA ACO variations [44; 2; 5; 3; 93]. We compare the heuristic measures learned by DeepACO to the expert-designed ones (detailed separately for eight COPs in Appendix D) on 100 held-out test instances for each benchmark COP. The results in Fig. 2 show that DeepACO can consistently outperform its ACO counterparts, verifying the universal neural enhancement DeepACO can bring. We defer the results on more COP scales to Appendix C.1.

DeepACO for advanced ACO algorithmsWe also apply DeepACO to Adaptive Elitist Ant System (AEAS) , a recent ACO algorithm with problem-specific adaptations. In Fig. 3, we plot the evolution curves of AEAS based on the original and learned heuristic measures, respectively, and DeepACO shows clearly better performance. In light of this, we believe that it is promising to exploit DeepACO for designing new ACO SOTAs.

Figure 2: Evolution curves of fundamental DeepACO and ACO algorithms on eight different COPs. For each COP, we plot the best-so-far objective value (averaged over 100 held-out test instances and 3 runs) w.r.t. the number of used evaluations along the ACO iterations.

DeepACO for different pheromone modelsThe pheromone model used in ACO can vary, even for the same problem . Following the naming convention in , we use \(PH_{suc}\) to denote the pheromone model that captures the successively chosen items and \(PH_{items}\) the one that directly captures how good each item is. We extend DeepACO from \(PH_{suc}\) to \(PH_{items}\) using a Transformer  encoder equipped with an MLP decoder. We showcase such flexibility of DeepACO on MKP in Fig. 6, where DeepACO using \(PH_{items}\) can still outperform its ACO counterparts. It validates that DeepACO can be readily extended to variations of pheromone models using properly aligned neural models. We defer further discussions to Appendix A and C.5.

DeepACO for better robustness to hyperparameter choiceIncreasing the robustness of ACO to hyperparameter choices has long been an important research topic . In Fig. 5, we adjust two important hyperparameters, i.e., _Alpha_ that controls the transition probability of solution construction and _Decay_ that controls the pheromone updates, evaluate DeepACO (AS) and ACO (AS) on the TSP100 test dataset, and plot their best objective values within the evolution of 4K evaluations. The results indicate that DeepACO is more robust to the hyperparameter choice given its much lower color variance. We thereby argue that data-driven training can also dispense with high-demanding expert knowledge for hyperparameter tuning.

DeepACO for real-world instancesWe draw all 49 real-world symmetric TSP instances featuring _EUC_2\(D\) and containing less than 1K nodes from TSPLIB. We infer instances with \(n<50\) nodes using the model trained on TSP20, those with \(50 n<200\) nodes using the model trained on TSP100, and the rest using the model trained on TSP500. The evolution curves of DeepACO and ACO are shown in Fig. 5, suggesting that DeepACO can consistently outperform its ACO counterparts even when generalizing across scales and distributions.

### DeepACO as an NCO method

Comparison on routing problemsAs shown in Table 1, DeepACO demonstrates competitive performance against the SOTA NCO methods for TSP. Here, we apply the generic 2-opt for NLS (\(T_{NLS}=10,T_{p}=20\)) and set \(W\) in Eq. (4) to 9. Note that most NCO baselines are specialized for TSP or routing problems, while DeepACO is a general-purpose meta-heuristic and validated across eight different COPs. In addition, the results of ablation studies in Table 2 further validate our designs; that is, both neural-guided perturbation and training with LS effectively strengthen DeepACO. More results on TSP100 and CVRP are given in Appendix C.2.

Figure 4: Sensitivity of DeepACO and ACO to the hyperparameter choice. Figure 5: Evolution curves of DeepACO and ACO averaged over TSPLIB instances.

[MISSING_PAGE_FAIL:9]

consistently better than its ACO counterparts and is on par with the specialized NCO methods. For the Operations Research (OR) community, DeepACO demonstrates a promising avenue toward leveraging deep reinforcement learning for algorithmic enhancement and design automation. For the Machine Learning (ML) community, DeepACO presents a versatile and adaptable NCO framework that can seamlessly integrate with SOTA ACO and EA techniques, including improved rules for solution construction and pheromone update , algorithmic hybridizations , and incorporation of sophisticated local search operators .

However, DeepACO is potentially limited by compressing all learned heuristic information into an \(n n\) matrix of heuristic measures. Due to the complexity of COPs and this restricted way of expressing problem patterns, DeepACO may fail to produce close-to-optimal solutions when not incorporating LS components. To address such limitations, we plan to investigate 3D or dynamic heuristic measures in our future work.