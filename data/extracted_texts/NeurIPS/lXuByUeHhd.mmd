# DoReMi: Optimizing Data Mixtures

Speeds Up Language Model Pretraining

Sang Michael Xie

Hieu Pham

Google DeepMind

Xuanyi Dong

Google DeepMind

Nan Du

Google DeepMind

Hanxiao Liu

Google DeepMind

Yifeng Lu

Google DeepMind

Percy Liang

Stanford University

Quoc V. Le

Google DeepMind

Tengyu Ma

Stanford University

Adams Wei Yu

Google DeepMind

###### Abstract

The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to set the domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across _all_ domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks.

## 1 Introduction

Datasets for training language models (LMs) are typically sampled from a mixture of many domains [17; 13; 10; 9]. For example, The Pile , a large publicly available dataset, is composed of 24% web data, 9% Wikipedia, 4% GitHub, etc.1 The composition of the pretraining data greatly affects the effectiveness of an LM [13; 20; 55]. However, it is unclear how much of each domain to include to produce a model that performs well for a wide variety of downstream tasks.

Existing works determine domain weights (the sampling probabilities for each domain) by using intuition or a set of downstream tasks. For example, The Pile uses heuristically-chosen domain weights, which could be suboptimal. On the other hand, existing LMs such as PaLM  and GLaM  tune the domain weights based on a set of downstream tasks, but requires training potentially thousands of LMs on different domain weights and risks overfitting to the particular set of downstream tasks.

Instead of optimizing domain weights based on a set of downstream tasks, our approach aims to find domain weights which lead to models that perform well on all domains by minimizing the worst-case _excess loss_ over domains, following Oren et al. ; Mindermann et al. . The excess loss is the loss gap between the model being evaluated and a pretrained reference model.

This motivates our algorithm, **D**omain **R**eweighting with **M**inimax Optimization (DoReMi), which leverages distributionally robust optimization (DRO) to tune the domain weights without knowledge of downstream tasks (Figure 1). First, DoReMi trains a small reference model (e.g., 280M parameters)in a standard way. Second, DoReMi trains a small distributionally robust language model (DRO-LM) , which minimizes the worst-case excess loss (relative to the reference's model's loss) across all domains. Notably, _rather than using the robust LM_, we take the domain weights produced by DRO training. Finally, we train a large (8B) LM on a new dataset defined by these domain weights.

Our approach adapts the DRO-LM framework  to optimize domain weights instead of producing a robust model. To do this, DoReMi uses the online learning-based optimizer from Group DRO [43; 34], which dynamically updates domain weights according to the loss on each domain for rescaling the training objective, instead of sub-selecting examples from a minibatch as in Oren et al. ; Mindermann et al. . Finally, DoReMi takes the averaged domain weights over DRO training steps.

In Section 3, we run DoReMi on 280M proxy and reference models to optimize domain weights on The Pile  and the GLaM dataset  (used in PaLM ). The DoReMi domain weights are used to train an 8B parameter LM (over 30x larger). On The Pile, DoReMi reduces perplexity on _all_ domains over baseline domain weights, even when it downweights a domain. DoReMi improves average downstream accuracy over a baseline model trained on The Pile's default domain weights by 6.5% points on generative few-shot tasks and achieves the baseline downstream accuracy 2.6x faster (Figure 2). In Section 4, we find that DoReMi consistently improves LM training when varying the sizes of the proxy model and the main model trained with optimized domain weights. On the

Figure 1: Given a dataset with a set of domains, Domain Reweighting with Minimax Optimization (DoReMi) optimizes the domain weights to improve language models trained on the dataset. First, DoReMi uses some initial reference domain weights to train a reference model (Step 1). The reference model is used to guide the training of a small proxy model using group distributionally robust optimization (Group DRO) over domains [35; 43; 34], which we adapt to output domain weights instead of a robust model (Step 2). We then use the tuned domain weights to train a large model (Step 3).

Figure 2: DoReMi optimizes domain weights with a small model (280M params) and uses these domain weights to train a much larger model (8B params, 30x larger). Here, optimizing the domain weights (training a small model twice) takes 8% of the compute of training the large model. DoReMi improves average one-shot downstream accuracy by 6.5% points and reaches the baseline accuracy 2.6x faster when pretraining on The Pile.

GLaM dataset where domain weights tuned on downstream tasks are available, DoReMi even performs comparably to tuning domain weights on downstream task performance.2

## 2 Domain Reweighting with Minimax Optimization (DoReMi)

In this section we define DoReMi, an algorithm for using a small proxy model to optimize the domain weights of a language modeling dataset, which then improves the training of a large model.

Setup.Suppose that we have \(k\) domains (e.g., Wikipedia, GitHub), where for each domain \(i\), we have a set of examples \(D_{i}\). Domain weights \(\!\!^{k}\) specify a probability distribution over the \(k\) domains, and consequently a distribution over the training data: \(P_{}\!=\!_{i=1}^{k}\!_{i}(D_{i})\) where \((D)=\!_{x D}\!_{x}\) is the uniform distribution over the examples in \(D\) and \(_{x}(x^{})\) is 1 if \(x^{}\!=\!x\) and 0 otherwise.

DoReMi.The inputs of DoReMi are the data \(D_{1},...,D_{k}\), reference domain weights \(_{}\) (e.g., uniform or based on raw token count of each domain), and training hyperparameters for the large, full-size model (number of training steps \(T\) and batch size \(b\)). DoReMi returns optimized domain weights \(\) and ultimately, a large model trained on \(P_{}\).

Step 1: Obtain a small reference model.We first train a model \(p_{}\) on some reference domain weights \(_{}\) (e.g., based on raw token count as a default) for \(T\) steps, batch size \(b\). This model serves as the reference model for step 2 and captures a baseline level of difficulty of each example/domain. The reference model can be a relatively small model (280M parameters in our experiments).

Step 2: Train proxy model with Group DRO to obtain domain weights.To obtain domain weights, we train a small _proxy model_\(p_{}\) in the distributionally robust language modeling (DRO-LM)  framework with the Group DRO optimizer , where \(\) are the weights of the proxy model. This framework trains a robust model by optimizing the worst-case loss over domains, which is equivalent to the following minimax objective:

\[_{}_{^{k}}L(,\!)\!:=\!_{i=1}^{k} \!_{i}\!\![}|x|}\!_{x D_{i}}\! _{}(x)\!-\!_{}(x)]\] (1)

where the losses \(_{}(x)=-\ p_{}(x)\) and \(_{}(x)=-\ p_{}(x)\) are the negative log-likelihoods of the proxy and reference models respectively in this paper, and \(|x|\) is the number of tokens in an example \(x\). The objective aims to minimize the worst-case excess loss across domains because the inner maximization over \(\) puts all the weight on the domain with the highest excess loss.

Intuitively, the excess loss (\(_{}(x)-_{}(x)\)) measures the headroom for the proxy model to improve, with respect to the reference model, on example \(x\). Examples with higher excess loss are those where the reference model achieves low loss (such that the example is "learnable") but the proxy model still has high loss. Examples with low excess loss may be very high entropy (i.e. optimal loss is high, and thus the reference loss is high) or very low entropy (i.e., easy to learn, and thus the proxy loss is low). The Group DRO optimizer works by interleaving exponentiated gradient ascent updates on domain weights \(_{t}\) with gradient updates on the proxy model weights \(_{t}\) over training steps \(t\). The optimizer updates \(_{t}\) to upweight domains with high excess loss, which scales up the proxy model's gradient update on examples from these domains. Following Nemirovski et al. , we return the average weights over the training trajectory \(\!=\!\!_{i=1}^{T}\!_{t}\) as the optimized domain weights to use in step 3.

Step 3: Train large model with new domain weights.The tuned domain weights \(\) define a new training distribution \(P_{}\). We resample the data from this new distribution to train a main model (larger than the reference/proxy models), using a standard training procedure.

Details for Step 2.Algorithm 1 provides the pseudocode for Step 2. The main structure of Algorithm 1 is a training loop which updates the proxy model over \(T\) steps. At each step, we follow Sagawa et al.  and sample a minibatch with uniform domain weights (regardless of the reference domain weights \(_{}\), which only affects the reference model). We then compute the per-domain excess losses, normalized by the total number of tokens in each domain, and use them to update the domain weights \(_{t}\) at each step. We first compute the per-domain excess loss at a per-token level and then aggregate, where the token-level losses at index \(j\) are \(_{_{t-1},j}(x)=- p_{_{t-1}}(x_{j} x_{1},...,x_{j-1})\) and \(_{,j}(x)=- p_{}(x_{j} x_{1},...,x_{j-1})\). Since the Group DRO optimizer  requires a non-negative loss, we clip the per-token excess loss at 0. Finally, we update the proxy model for the objective \(L(_{t-1},_{t})\) using a standard optimizer such as Adam  or Adafactor . All experiments in this paper use Adafactor. We set the domain weight update step size to \(=1\) and the smoothing parameter to \(c=1\)e-3 in all our experiments and did not extensively tune these hyperparameters.

Iterated DoReMi.We extend DoReMi by running it for multiple rounds, setting the reference domain weights \(_{}\) for the next round to be \(\) from the previous round. We call this _iterated DoReMi_. The entire iterated process still only uses small models for tuning domain weights. We stop iterating when the domain weights converge, which we define as when maximum change in any domain weight \(\|-_{}\|_{}\) is less than 1e-3. Empirically, this takes only 3 rounds on the GLaM dataset (Section 3.2).

## 3 DoReMi Improves LM Training Efficiency and Performance

In this section, we use DoReMi domain weights optimized with a 280M-parameter proxy model to train a 8B-parameter main model (30x larger). We consider two datasets, The Pile  and the GLaM dataset . On The Pile, DoReMi reduces perplexity significantly on every domain, improves average downstream accuracy on generative one-shot tasks by 6.5%, and achieves the baseline accuracy 2.6x faster. On the GLaM dataset where domain weights tuned on downstream datasets are available, DoReMi finds domain weights with comparable performance to downstream-tuned domain weights.

### Experimental setup

The Pile dataset.The Pile  is a 800GB text dataset with 22 domains (Table 1). The default domain weights were determined heuristically. We use the default domain weights from The Pile dataset to train the baseline and as the reference domain weights \(_{}\) in DoReMi (see Appendix C).

GLaM dataset.The GLaM dataset  (also used in training PaLM ) includes text from 8 domains (Table 2). For comparison, the GLaM domain weights (downstream-tuned) were tuned according to the downstream performance of models trained on each domain and the size of each domain . We consider this an oracle comparison, since these domain weights are tuned on downstream tasks that are in our evaluation set. We use uniform domain weights both for training the baseline and the reference domain weights \(_{}\) for DoReMi.

Training setup.We train Transformer  decoder-only LMs with the standard next-token language modeling loss. We conduct a controlled comparison by equalizing the amount of compute, measured by the number of tokens processed during training. For The Pile, we train each model for 200k steps; for the GLaM dataset, we train each model for 300k steps. All models use a batch size of 512 and maximum token length of 1024. The proxy and reference models have 280M parameters. All models are trained from scratch (other hyperparameters are in Appendix C).

Evaluation.We use held-out validation data to measure the perplexity on each domain. For downstream evaluation, we use the generative one-shot tasks from the GPT-3 paper : TriviaQA , NaturalQuestions , WebQuestions , SQuADv2 , and LAMBADA . We use the standard exact-match accuracy metric for the these datasets. The performance on these datasets (particularly TriviaQA) has been shown to correlate well with model scale even at the 100M-1B range .

Compute used for optimizing domain weights.We train two 280M models (the reference and proxy models) to optimize the domain weights. This is 8% of the FLOPs required to train the main 8B model. All FLOPs come from standard forward and backward passes.

Notation for model sizes in DoReMi.We denote the size of the reference/proxy models (which are always the same size in our experiments) and the size of the main model trained with DoReMi domain weights as "DoReMi (size of reference/proxy\(\)size of main model)": for example, DoReMi (280M\(\)8B). When we are discussing the optimized domain weights independently of the main model, we only include one number (e.g., DoReMi (280M)) which refers to the reference/proxy model size.

Figure 4: Per-domain log-perplexity of 8B models on The Pile. Despite downweighting some domains, DoReMi improves log-perplexity on all domains.

Figure 3: Average one-shot downstream accuracy (exact match) on 5 tasks, with 8B parameter models trained on The Pile (left) and the GLaM dataset (right). On The Pile, DoReMi improves downstream accuracy by 6.5% points and achieves the baseline accuracy 2.6x faster (same plot as Figure 2). On the GLaM dataset, iterated DoReMi (round 2) attains comparable performance to oracle domain weights tuned with downstream tasks that are in our evaluation set.

### DoReMi improves perplexity and downstream accuracy

We show that DoReMi significantly improves both the perplexity and downstream accuracy of 8B models trained on The Pile and the GLaM dataset over their respective baseline domain weights.

Downstream accuracy improves on The Pile.Figure 3 (left) shows the average downstream performance for baseline and DoReMi (280M\(\)8B) models on The Pile. DoReMi improves the downstream accuracy by 6.5% points and achieves the baseline accuracy within 75k steps -- 2.6x faster than the baseline (200k steps). Thus, DoReMi can dramatically speed up training and improve downstream performance.

DoReMi can reduce perplexity across all domains without a tradeoff.Figure 4 shows the per-domain log-perplexity of the 8B models on The Pile. DoReMi significantly reduces the perplexity over the baseline across _all_ domains, despite allocating lower weight to some domains. How can this occur? One hypothesis is that the domains with the lowest and highest entropy can be downweighted without impacting the perplexity much. The lowest entropy domains statistically require few samples to learn. The highest entropy domains have token distributions that are close to common uniform priors -- for example, models at random initialization tend to output a uniform next token distribution. Thus, we need less samples to fit these domains. Positive transfer from allocating more samples to medium entropy domains can then improve perplexity on all domains. In Appendix D, we provide a simple example where reweighting domains can improve perplexity on all domains and DoReMi finds such domain weights in simulations.

Iterated DoReMi achieves performance of downstream-tuned weights on the GLaM dataset.We employ iterated DoReMi on the GLaM dataset over 3 rounds. We find that the second and third round domain weights are almost identical (Table 2). Figure 3 (right) shows one-shot results for the first two rounds of iterated DoReMi. After the first round, the DoReMi main model has comparable downstream accuracy to the baseline (uniform domain weights). After the second round, the DoReMi main model achieves comparable downstream accuracy to oracle domain weights tuned on downstream tasks in our evaluation set. Overall, domain reweighting has a smaller effect on GLaM, possibly because there are only 8 domains compared to 22 in The Pile.

Inspecting the DoReMi domain weights.Tables 1 and 2 present the DoReMi domain weights for The Pile and the GLaM dataset. When running DoReMi on a 280M proxy model (DoReMi (280M)), most weight is put on the diverse Pile-CC web text domain. Note that Wikipedia is downweighted in comparison to the baseline, but DoReMi still improves the downstream accuracy on tasks derived from Wikipedia (e.g., TriviaQA, Appendix Table 5). Domain weights for a 1B proxy model (Appendix 8)

   Domain & Baseline & DoReMi (280M) & Difference & Domain & Baseline & DoReMi (280M) & Difference \\  Pile-CC & 0.1121 & 0.6057 & -0.4936 & DualMatchers & 0.0198 & 0.0018 & -0.0180 \\ YouTubeSubtitles & 0.0042 & 0.0502 & +0.0460 & Wikipedia (en) & 0.0199 & 0.0699 & -0.0220 \\ PhilPersp & 0.0027 & 0.0274 & +0.0247 & OpenWebText2 & 0.1247 & 0.1019 & -0.0228 \\ HackerNews & 0.0075 & 0.0134 & +0.0059 & Graph0 & 0.0427 & 0.0179 & -0.0248 \\ EnronEmails & 0.0030 & 0.0070 & +0.0040 & FreeLaw & 0.0386 & 0.0043 & -0.0343 \\ EnroPall & 0.0043 & 0.0062 & +0.0019 & USPTO Backgrounds & 0.0420 & 0.0036 & -0.0384 \\ Ubuntu IRC & 0.0074 & 0.0093 & +0.0019 & Books3 & 0.0867 & 0.0224 & -0.0452 \\ BookCorpus2 & 0.0044 & 0.0061 & +0.0017 & PubMed Abstracts & 0.0845 & 0.0113 & -0.0732 \\ NIH EnPutter & 0.0052 & 0.0063 & +0.0011 & StackExchange & 0.0029 & 0.0153 & -0.0776 \\ OpenSubtitles & 0.0124 & 0.0047 & -0.0077 & ArXiv & 0.1052 & 0.0036 & -0.1016 \\ Gutenberg (PG-19) & 0.0199 & 0.0072 & -0.0127 & PubMed Central & 0.1071 & 0.0046 & -0.1025 \\   

Table 1: Domain weights on The Pile. Baseline domain weights are computed from the default Pile dataset. DoReMi (280M) uses a 280M proxy model to optimize the domain weights.

    & Round 1 & Round 2 & Round 3 & Downstream-tuned \\  Wikipedia & 0.09 & 0.05 & 0.05 & 0.06 \\ Filtered webpages & 0.44 & 0.51 & 0.51 & 0.42 \\ Conversations & 0.10 & 0.22 & 0.22 & 0.27 \\ Forums & 0.16 & 0.04 & 0.04 & 0.02 \\ Books & 0.11 & 0.17 & 0.17 & 0.20 \\ News & 0.10 & 0.02 & 0.02 & 0.02 \\   

Table 2: Domain weights in the GLaM dataset. Iterated DoReMi (280M) converges within 3 rounds, with a similar overall pattern to domain weights tuned on downstream tasks.

shows a different trend, where OpenWebText is the mostly upweighted instead of Pile-CC. This suggests that there may be multiple possible local minima in the domain weight space. On the GLaM dataset, the DoReMi weights have the same general pattern as the downstream-tuned domain weights. DoReMi is able to recover a similar set of domain weights by starting from uniform initial reference domain weights, without any use of downstream data.

## 4 Ablations and Analysis Across Scales

Previously in Section 3, we showed that DoReMi finds domain weights using 280M models that can improve training of 8B models. In this section, we conduct an analysis of DoReMi where we vary the scale of the proxy model in relation to the main model and ablate the components of the excess loss objective.

DoReMi improves LMs consistently across scales.We consider using proxy and main models of the same size to analyze DoReMi's behavior in a simple setting, without the need for the domain weights to generalize across scales. Note that this is just for scientific purposes since this does not save compute in practice. In particular, we run DoReMi (X\(\)X) where X is 280M, 510M, 760M, or 1B on The Pile. Figure 5 shows that DoReMi consistently improves downstream accuracy over the baseline by 2% and achieves the baseline accuracy 4x faster on average across scales, and this improvement does not shrink with larger model size. DoReMi improves the worst-case perplexity on all scales and improves 18 of 22 individual domain perplexities on average across scales (Appendix Table 6). These experiments give a rough picture of how much is lost when using a smaller proxy model; our DoReMi (280M\(\)8B) model achieves the baseline accuracy 2.6x faster, while matching the proxy and main model sizes results in a 4x average speedup.

Proxy model underperforms main model, especially at larger sizes.Recall that DoReMi uses Group DRO to train a proxy model, which reweights the objective with the domain weights. In contrast, the main model is trained by resampling on the domain weights from DoReMi. When the proxy model and the main model are the same size, which one is the better model? Table (b) shows that the proxy

Figure 5: Average one-shot downstream accuracy across 4 model scales (280M, 510M, 760M, 1B) where the reference/proxy models for DoReMi are the same size as the main model trained with DoReMi domain weights. DoReMi consistently improves downstream accuracy across scales, with a similar 3% accuracy gap at 200k steps at most scales (except for 510M). DoReMi achieves the baseline accuracy 4x faster on average across scales.

model typically underperforms the main model in this case. The gap between the proxy and main model increases with scale, as the 1B proxy model not only underperforms the 1B main model but also the 1B baseline model, while the 280M proxy model achieves better perplexity than the 280M baseline model on 19/22 domains. Despite the relatively poor quality of the 1B proxy model, the domain weights still allow the 1B main model to achieve the baseline performance over 2x faster. This suggests that DoReMi can succeed even if the proxy model is not trained well. However, we hypothesize that the mismatch between the proxy and main model training (loss reweighting vs. resampling) explains their performance difference and therefore a resampling-based Group DRO optimizer may improve DoReMi for larger proxy models.

Effect of proxy model scale on larger main model's performance.We consider 70M, 150M, 280M, and 1B scales for the DoReMi proxy model while fixing the main model size at 8B (DoReMi (X\(\)8B)). From 70M to 280M, increasing the proxy model size improves downstream accuracy at 8B (Figure 6 left). We hypothesize that this trend does not continue for the 1B proxy model because the Group DRO optimizer is worse at larger scales (Table 2(b)). While DoReMi (280M\(\)8B) results in the most improvement at 8B, DoReMi (150M\(\)8B) and DoReMi (1B\(\)8B) still achieve the baseline accuracy almost 2x faster. This suggests that DoReMi is robust to the proxy model scale. In practice, we suggest choosing a relatively small proxy model size (280M) to save compute.

Choosing the easiest or hardest domains do not suffice.We ablate the components of the excess loss metric \(_{}(x)-_{}(x)\) by running DoReMi using only the loss of the proxy model \(p_{}\) on example \(x\), i.e. \(_{}(x)\) (prefer hardest domains for the proxy model) or only the negative loss of the reference \(-_{}(x)\) (prefer easiest domains for the reference model). Figure 6 (right) shows that neither of the components of the excess loss alone are sufficient to achieve the gains of DoReMi.

Table 3: Summary of per-domain log-perplexities on The Pile (22 total domains). Average log-perplexity is an unweighted average of the per-domain log-perplexities.

Figure 6: Average downstream accuracy for models trained on The Pile. **(Left)** Increasing the size of the reference/proxy models from 70M to 280M in DoReMi improves downstream accuracy for a 8B main model, but the trend does not continue for the 1B proxy model. We hypothesize that the Group DRO optimizer is worse for larger proxy models. **Right**) Optimizing for the hardest or easiest domains rather than excess loss (which combines both) do not achieve the same average downstream accuracy as DoReMi (280M models).

Related Work

Curating pretraining data for LMs.Most closely related is the GLaM dataset  (also used for training PaLM ), which has domain weights that are tuned using downstream data. Optimizing domain weights for downstream tasks can be expensive and could require search/zero-order optimization , RL , or heuristic assumptions on how positive/negative transfer between domains work. Example-level filtering also brings benefits for LM training. The C4 dataset  shows gains over CommonCrawl via heuristic data cleaning methods. Du et al. ; Xie et al.  show that filtering the data at an example level for high-quality text that look like Wikipedia and books can significantly improve downstream performance for LMs. In contrast to these works, DoReMi sets domain weights automatically with only two small LM training runs and does not make assumptions about the type of data to prefer (Wikipedia-like, etc.).

General data selection methods.Moore-Lewis selection [32; 3; 15] selects examples with high cross-entropy difference (similar to excess log-perplexity) between language models trained on target and raw data. In contrast, DoReMi reweights the data without a target distribution. Coleman et al.  select examples based on the uncertainty of a small proxy model for active learning, while DoReMi uses DRO on the excess loss with respect to a reference model, and focuses on data mixture reweighting. Mindermann et al.  select examples in an online fashion by taking the top \(k\) examples in a minibatch according to excess loss. DoReMi optimizes the data mixture before training, allowing the larger main model to train in a standard way. Many other works on data selection are in vision [49; 22; 24; 23; 25; 53; 54; 38; 31; 45] and mainly focus on example-level subset selection with metrics such as gradient matching. Overall, these methods do not address data selection for pretraining, where the downstream data distribution may be very different from the pretraining distribution. DoReMi aims to address the pretraining/downstream distribution shift with a robust optimization approach. To the best of our knowledge, we are the first to show that reweighting the data according to losses of a small proxy LM can improve the training efficiency of much larger LM.

Distributionally robust optimization.Within DRO methods for deep learning [4; 47; 35; 43], we target a restricted form of shift called group shifts [14; 35; 43], where the test distribution can be an unknown mixture of groups (domains). We follow DRO-LM , which employs DRO for LMs in the group shift setting. DRO-LM also uses a baseline loss, but with a simple bigram reference model. DoReMi uses a reference model of the same size and architecture as the proxy model to ensure that the losses are on a similar scale. During optimization, DRO-LM takes a worst-case subset of each minibatch to update the model on, while we use the Group DRO optimizer  which doesn't require online subselection. If we equalize the number of examples in each minibatch used for gradient updates, online subselecltion is more expensive than Group DRO since it requires running forward passes on a larger minibatch (e.g., double the minibatch size) before selecting a subset to update the model with. In comparison, the Group DRO optimizer updates the model on all examples in a weighted fashion. Overall, in contrast to these DRO methods which aim to produce robust **models**, we use DRO to optimize the **data** for training larger models more efficiently.

Data-centric AI.Large-scale datasets and benchmarks have driven much of the recent progress in AI, including vision, NLP, and multimodal models [12; 42; 52; 40; 39; 17; 44; 16]. However, most datasets are still painstakingly created with human-generated data, manual work, and heuristics [12; 39; 17; 44; 16]. DoReMi is a principled data-centric method that aims to improve language model training efficiency. We hope that DoReMi can provide a starting point for a general data-centric framework for language modeling via robust optimization.

## 6 Discussion and Limitations

Saving compute in DoReMi with extrapolation.In Section 2, we run DoReMi for the number of training steps that will be used to train the final model, which could be unnecessarily expensive. A future direction for saving compute would be to stop running DoReMi at an early step and extrapolate the domain weights for the desired number of steps, since we found that most of the variation in the domain weights during a DoReMi run seems to occur in the beginning of training (Appendix Figure 8).

Choice of reference model.The choice of reference model can affect the domain weights found by DoReMi. For example, iterated DoReMi (Section 3) improves performance by using a reference model trained on the tuned domain weights from a previous round of DoReMi. Further directions include varying the reference model size and using specialized reference models to optimize domain weights for a specific application area.

What is a domain?We define a domain by data provenance in our experiments, but this only enables coarse-grained control. Using fine-grained domains could improve the gains from DoReMi. For example, DoReMi is more effective on The Pile (22 domains) than the GLaM dataset (8 domains). Open directions include automatically finding fine-grained domains (e.g., via clustering as in DRO-LM ) and reweighting the data at an example level. When domains are very fine-grained, it will be important to control the pessimism of DRO (e.g., DRO can put all the weight on a small set of worst-case examples).

Transferability of domain weights across scales.We optimized the domain weights with a small proxy model (280M) and directly used these domain weights to improve training at a larger scale (8B). Understanding why the domain weights can be transferred across scales and the limits of how far these domain weights transfer are important questions to answer in future work.

Broader impacts.Large language models are We hope to improve training efficiency and reduce the environmental impact of training large LMs [50; 28; 37; 29]. In particular, by reducing the training time by 2x, we can halve the cost and energy consumption of training large language models. Since such efficiency improvements may be used to develop even larger models, there may be no absolute improvement in energy consumption. Ultimately, we hope to improve the training efficiency and cost of developing future language models relative to existing methods.

Large LMs have also been well-documented to have risks and biases [1; 33; 7; 6; 18]. For example, GPT-3 tends to have an anti-Muslim bias, where Muslims are frequently related to violence or terrorism in analogy and completion tasks . As large language models are increasingly relied upon in applications, the magnitude of the risks increases . Distributionally robust optimization (DRO), which is used in DoReMi to optimize the data mixture, can have a favorable impact on fairness . While the standard approach of minimizing the average loss can lead to disparate performance on minority subgroups that do not contribute heavily to the loss , DRO promotes good performance on all groups via a worst-case loss. In this way, DRO-style data-centric methods such as DoReMi can improve the representation disparity between majority and minority subgroups in a dataset.

## 7 Conclusion

We introduced DoReMi, an algorithm reweighting data domains for training language models. DoReMi is able to run on small models and transfer the benefits to 30x larger models, resulting in a 2.6x speedup in training on the Pile just by changing the sampling probabilities on domains. We hope to instigate more research on data-centric approaches for improving language model training efficiency.