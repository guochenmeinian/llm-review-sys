# RADAR: Robust AI-Text Detection via Adversarial Learning

Xiaomeng Hu

The Chinese University of Hong Kong

Sha Tin, Hong Kong

xmhu23@cse.cuhk.edu.hk

Pin-Yu Chen

IBM Research

New York, USA

pin-yu.chen@ibm.com

Tsung-Yi Ho

The Chinese University of Hong Kong

Sha Tin, Hong Kong

tyho@cse.cuhk.edu.hk

###### Abstract

Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo.

**Project Page and Demos:**https://radar.vizhub.ai

IBM demo is developed by Hendrik Strobelt and Benjamin Hoover at IBM Research

HuggingFace demo is developed by Xiaomeng Hu

## 1 Introduction

Large language models (LLMs) are high-capacity neural networks that are pretrained at web-scale datasets. They are foundation models achieving state-of-the-art performance in a wide range of natural language processing tasks (e.g. document completion, question answering, machine translation, and content creation with text prompts) with advanced capabilities such as in-context learning and reasoning (e.g. chain of thoughts). In particular, LLMs are the backbone of many ChatGPT-like conversational bots that enable text generation with high fluency and accuracy. However, while LLMs and their derived applications are expected to become ubiquitous in our future technology and society,new risks in failing to distinguish the so-called "AI text" generated by LLMs have emerged and gained considerable attention due to various reasons. The problem of reliable AI-text detection is motivated by realistic socio-technological challenges such as fake content generation, AI plagiarism (e.g. using LLMs for writing tests), and false accusations of innocent writers. According to a report released by OpenAI1, their latest AI-text detector is admittedly not fully reliable. In the reported evaluation of some challenging cases for English texts, their classifier only correctly identifies 26% of AI-text (true positives) while incorrectly classifying 9% of human-written text (false positives). Moreover, a recent study  found that state-of-the-art AI-text detectors demonstrated severely degraded performance when encountering texts written by non-native English speakers.

What can be even more challenging in AI-text detection is that existing AI-text detectors are prone to be manipulated. The authors in [27; 17] showed that using LLMs as a paraphraser can easily evade several AI-text detection methods, even in the scenario when the original AI-text had been watermarked. These findings sparked a heated debate about whether and how we can successfully design a reliable AI-text detector. While  theoretically quantifies the best detector's performance with respect to the total variation distance between AI-text and human-text distributions and argues that AI-text is difficult to detect, another work  proves that it is possible to obtain a reliable AI-text detector unless the human-text distribution is exactly the same as the AI-text distribution, based on an information-theoretical analysis (i.e., the sample complexity of Chernoff information and likelihood-ratio-based detectors).

To improve AI-text detection, we propose **RADAR**, a framework for training a robust AI-text detector using adversarial learning. An overview of RADAR is illustrated in Figure 1. Our proposal draws inspiration from adversarial machine learning techniques that train a high-quality generator by introducing a discriminator to form a two-player game, such as generative adversarial networks (GANs) . In RADAR, we introduce a paraphraser and a detector as two players with opposite objectives. The paraphraser's goal is to generate realistic content that can evade AI-text detection, while the detector's goal is to enhance AI-text detectability. In our framework, both the paraphraser and the detector are parametrized by separate LLMs. During training, the paraphraser learns to rewrite the text from a training corpus (generated by a target LLM from a human-text corpus) with the

Figure 1: Overview of RADAR. An AI-text corpus is first generated from a target (frozen) language model from a human-text corpus. In RADAR, we introduce a paraphraser (a tunable language model) and a detector (a separate tunable language model). In the training stage, the detector aims to discern human-text v.s. AI-text, while the paraphraser aims to rewrite AI-text to evade detection. The model parameters of the paraphraser and the detector are updated in an adversarial learning manner as described in Section 3. In the evaluation stage, the trained detector is deployed to predict the likelihood of AI-generated content for any input instance.

aim of decreasing the likelihood of AI-text prediction by the detector, whereas the detector aims to enhance the detection performance by learning to compare human-text v.s. AI-text from the training data and the paraphraser's output. These two players iteratively update their model parameters until their respective validation loss becomes stable. Specifically, the paraphraser treats the prediction of the detector as a reward and uses Proximal Policy Optimization (PPO)  for updates. The detector updates its parameters based on a logistic loss function evaluated on the human-text and AI-text corpora (including the texts generated by the paraphraser). In the evaluation phase, the trained detector is deployed to predict the likelihood of AI-written content for any input instance. When compared with 6 existing detectors, our experimental results on 8 different LLMs and 4 datasets show that RADAR attains similar detection performance on the original AI-generated texts (a relatively easy task) and simultaneously improves the AI-text detectability when facing an "unseen" paraphraser (i.e. this paraphraser is not used in RADAR). The result is summarized in Figure 2. When facing an unseen paraphraser (GPT-3.5-Turbo), the area under the receiver operating characteristic (AUROC) score of RADAR is improved by 31.64% compared to the best existing detector, suggesting a significant improvement and reliable AI-text detection power enabled by RADAR.

We summarize **our main contributions** as follows:

* To the best of our knowledge, RADAR is the first study that leverages the idea of adversarial learning between a paraphraser and a detector for training a robust AI-text detector.
* The experiments on 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) and 4 datasets show that unlike the six existing supervised and unsupervised AI-text detection methods, RADAR is the only robust detector that attains consistently high detection performance. RADAR's detector is not weakened by paraphrasing, as shown in Figure 2.
* We also find the strong transferability of RADAR's detection capability. The detectors of RADAR obtained from instruction-tuned first-class LLMs (e.g., Vicuna-7B) are also effective on other LLMs, suggesting the possibility of training a universal AI-text detector based on the state-of-the-art LLMs.

## 2 Related Work

**AI-Text Detection.** The research in AI-text detection can be divided into three approaches. (i) Statistical methods: some statistics such as entropy , n-gram frequency, and perplexity are used as a threshold to discern AI-text. A typical example is GLTR , which exploits entropy, probability, and probability rank for detection. A more recent work is DetectGPT , which assumes that the machine-generated text always lies in the negative curvature region of the log probability of the LLM of interest. Based on this hypothesis, DetectGPT perturbs the input text with a mask-filling language

Figure 2: Performance evaluation (AUROC) of 8 LLMs over 4 human-text datasets. _w/o paraphraser_ means the evaluation with the original AI-text corpora (the yellow bin \(\) in Figure 1). _RADAR-Unseen paraphraser_ means the evaluation with the paraphrased AI-text (the green bin \(\) in Figure 1) generated from an independent paraphraser (OpenAIâ€™s GPT-3.5-Turbo API) that is not used in RADAR. The black error bar represents the standard deviation of the detection AUROCs across 8 LLMs. Please refer to Section 4.2 for more implementation details.

model, such as T5 . Then, AI-text detection is performed by comparing the log probability of the text and its infilled variants. (ii) _Classification methods_: AI-text detection is formulated as a binary classification task, and a classifier is trained for a target language model [37; 29; 26; 13]. For example, OpenAI trains its AI-text classifier with a RoBERTa-based model .

The developers collected samples from the WebText dataset2 and labeled them as human-generated. Then, for each target GPT-2 model, they collected the generated samples and labeled them as machine-generated. Finally, they fine-tuned the pretrained RoBERTa-based model  for AI-text classification. More recently, with the appearance of CharGPT, OpenAI tuned a GPT model called AI-Classifier1 using data from several sources. The human-written text comes from three sources: a new Wikipedia dataset, the WebText dataset collected in 2019, and a set of human demonstrations collected as part of training InstructGPT . To collect machine-generated text, for the Wikipedia and WebText datasets, they truncated the articles sampled from the original corpus and used 34 models to generate article completion, pairing each generated text with the original article. For the demonstrations, they used a model to generate responses for each prompt and paired them with the corresponding human demonstrations. This detector was only accessible via a web interface since its release in January 2023, and it has been taken down since July 2023. (iii) _Watermark methods_: post-hoc watermarking techniques, such as rule-based methods [1; 15; 31] and deep-learning-based methods [6; 32], can be applied to an LLM. At inference time,  proposed a soft watermarking scheme to embed a watermark in each word of the generated sentence by dividing the vocabulary into different lists and sampling the next token in a differentiated manner. However, many existing AI-text detectors are shown to be significantly weakened by paraphrasing in .

**Adversarial Learning for Natural Language Generation.** The success of GAN  in the computer vision domain has motivated many studies in natural language generation. However, since text generation is a sequential sampling process that occurs in a discrete vocabulary space, it is difficult to directly train a text generator using back-propagation in an end-to-end manner [36; 7; 5; 35]. There are two common approaches to tackle this problem. The first one is to replace the discrete sampling operation with continuous approximation techniques [35; 5], such as Gumbel-Softmax [14; 22]. The second one is to view text generation as a decision-making process and cast the generator as a policy [36; 34; 7; 33]. A typical example is SeqGAN . During generation, SeqGAN considers the generated tokens as the state and the next token to be generated as the action, and it adopts Monte Carlo search to collect reward signals from the discriminator. Instead of using a classifier as the discriminator, the Diversity-Promoting GAN  uses a unidirectional LSTM as the discriminator and combines both word-level and sentence-level rewards into training. TextGAIL  proposed an imitation learning paradigm in which the rewards of the human-written text are regarded as a constant value. Then, both the rewards from human-text and AI-text are used to optimize the generator with PPO. These works all used warm-up training for the generator with maximum likelihood estimation (MLE) on the probability of the generated text sequence. On the other hand,  trained a language GAN from scratch. Our proposed RADAR differs from these works in that we focus on training a robust AI-text detector with a tunable paraphraser. Another line of work, such as [20; 4], uses paraphrasing techniques to find adversarial examples for natural language processing tasks and for training a robust language model via adversarial training. Their focus is on the correctness of natural language understanding, which is beyond our scope of AI-text detection.

## 3 RADAR: Methodology and Algorithms

We start this section by giving an overview and mathematical notations of our proposed RADAR framework in Figure 1. Then, in Sections 3.1 and 3.2, we provide the details on the design and training of the paraphraser and detector, respectively. Finally, we will summarise the entire training process into an algorithmic procedure in Section 3.3.

**High-Level Methodology.** Our RADAR framework consists of three neural-network-based language models (LMs): the target LM \(_{}\), the detector \(_{}\) and the paraphraser \(_{}\), parameterized with \(\), \(\) and \(\), respectively. We note that \(_{}\) is frozen (no updates on \(\)) in the entire process. We summarize RADAR into three key steps:

* **Step 1 (Data preparation):** Before training, we build \(\), the corpus of AI-text, by applying document completion based on the prefix span of text in the human-text corpus \(\) using \(_{}\).

* **Step 2 (Paraphraser update):** We collect AI-text samples \(x_{m}\) from \(\) and use \(_{}\) to do paraphrasing on \(x_{m}\) to generate paraphrased AI-text \(x_{p}\) to form a corpus \(\). Then, we use the reward of \(x_{p}\) returned by the detector \(_{}\) to update the paraphraser \(_{}\) using PPO.
* **Step 3 (Dectector update):** We use the human-text samples \(x_{h}\) from \(\), the original AI-text samples \(x_{m}\) from \(\), and the paraphrased AI-text samples \(x_{p}\) from \(\) in step 2 to update the detector \(_{}\) with a logistic loss function.
* **Step 4 (Performance Validation and Evaluation):** During training, we use the test set of WebText as the validation dataset to estimate RADAR's performance. For evaluation, we use \(_{}\) to generate AI-text for the evaluation dataset and to calculate RADAR's detection AUROC.

Step 2 to Step 3 can be repeated until there is no improvement in the AUROC evaluated on the validation dataset. The nature of rivalry in adversarial learning and the introduced competition helps the detector to learn to be robust in detecting both original and paraphrased AI-text.

### Training Paraphraser via Clipped PPO with Entropy Penalty

In RADAR, the goal of the paraphraser \(_{}\) is to paraphrase the input machine-generated text \(x_{m}\). We model the generation of paraphrased text as a decision-making process, taking \(x_{m}\) as the state and the output text \(x_{p}\) as the action. In particular, we optimize \(_{}\) using the reward feedback from the detector \(_{}\) with PPO. The output of \(_{}(x_{p})\) is the predicted likelihood of \(x_{p}\) being Human-text. The reward returned by \(x_{p}\) and the log probability of the text \(x_{p}\) are defined in Eq. 1:

\[R(x_{p},)=_{}(x_{p});\ \  P_{_{}}(x_{p}|x_{m})= _{i=1}^{N} P_{_{}}(x_{p}^{i}|x_{m},x_{p}^{1:i-1}),\] (1)

where \(x_{p}^{i}\) means the \(i\)-th token in the sentence \(x_{p}\) of length \(N\) and \(x_{p}^{1:i-1}\) represents the first \(i-1\) tokens in \(x_{p}\) (\(x_{p}^{1:0}\) means the default starting token).

We propose Clipped PPO with Entropy Penalty (cppo-ep) in RADAR to optimize \(_{}\). Let \((,a,b)\) denote a value-clipping operation with a lower limit \(a\) and an upper limit \(b\), \(r(,x_{m},x_{p})\) be the importance sampling ratio between a new policy \(_{}\) and an old policy \(_{^{}}\), and \((x_{m},x_{p}) P_{_{^{}}}\) be a state-action pair sampled from \(_{^{}}\). The loss of cppo-ep is defined as:

\[L_{}()=_{(x_{m},x_{p}) P_{_{^ {}}}}(r(,x_{m},x_{p}),1-,1+ ),r(,x_{m},x_{p})\} A(x_{p},)}_{L_{}} _{L_{}}\] (2)

where \(\) denotes expectation, \(\) is a parameter used in clipping to avoid the importance ratio \(r\) from being too large, \(A(x_{p},)\) is the advantage item of the paraphrased text \(x_{p}\) obtained by applying normalization to \(R(x_{p},)\) across the entire PPO sample buffer \(\). \(S()=_{(x_{m},x_{p}) P_{_{}}}-P_{_{}}(x_{p}|x_{m}) P_{_{}}(x_{p}|x_{m})\), which is an entropy term introduced to encourage \(_{}\) to explore more diverse generation policy. \(\) is a coefficient to control the ratio between \(L_{}\) and \(L_{}\), in order to make a balance between advantage (\(L_{}\)) and diversity (\(L_{}\)) when paraphrasing.

### Training Detector via Reweighted Logistic Loss

In a typical GAN training process, the discriminator receives an equal amount of positive and negative samples in each step, assuring an in-batch sample balance. However, in RADAR, by construction, the number of AI-text samples is twice the number of human-text samples, because each \(x_{h}\) from the human-text corpus \(\) is paired with a sample \(x_{m}\) from the original AI-text corpus \(\) as well as a paraphrased sample \(x_{p}\) generated by the paraphraser \(_{}\). To handle this in-batch imbalance problem, we use a reweighted logistic loss function to optimize the detector \(D_{}\), as described in Eq. 3:

\[L_{}()=_{x_{h}} _{}(x_{h})}_{L_{}:}+_{x_{m}}- (1-_{}(x_{m}))}_{L_{}^{1}:}+_{x_{m}\)
3:Select a target language model \(_{}\) to perform document completion on \(\) to build the corresponding AI-text corpus \(\)
4:Build a replay buffer \(\) to store samples temporarily collected for training
5:Build a validation dataset \(\) from \(\) and \(\)
6:Model initialization:
7:Detector \(_{}_{}\) (a pretrained language model)
8:Paraphraser \(_{}_{}\) (a pretrained language model)
9:Model training:
10:for\(i=1\) : maximum step do
11:Sample \(x_{n}\) and its corresponding \(x_{m}\) from \(\) and \(\) respectively
12:Use \(_{}\) to paraphrase \(x_{m}\) and generate \(x_{p}\)
13:Collect reward \(R(x_{p},)\) as in Eq. 1
14:Normalize \(R(x_{p},)\) to compute the advantage function \(A(x_{p},)\) used in Eq. 2
15: Fill \(\) with \((x_{h},x_{m},x_{p},A(x_{p},))\)
16:\(^{}\) # initialize the old policy \(^{}\) as the current policy \(\)
17:for\((x_{h},x_{m},x_{p},A(x_{p},))\)do
18:Compute the log probability \( P_{_{}}(x_{p}|x_{m})\) and \( P_{_{}^{}}(x_{p}|x_{m})\) using Eq. 1
19: Update \(_{}\) using Eq. 2
20:endfor
21:for\((x_{h},x_{m},x_{p},A(x_{p},))\)do
22: Update \(_{}\) using Eq. 3
23:endfor
24:Clear \(\)
25:Evaluate AUROC of \(_{}\) on the validation dataset \(\)
26:endfor
27:Detector \(_{}_{}\) (the detector model with the best AUROC on the validation dataset)
28:Paraphraser \(_{}_{}\) (the paraphraser model which pairs with \(_{}\))
29:Return \(_{}\) and \(_{}\) ```

**Algorithm 1** RADAR: Robust AI-Text Detection via Adversarial Learning

## 4 Experiments

### Experiment Setup

**Datasets and Metrics.** For training, we sampled 160K documents from WebText  to build the human-text corpus \(\). Then, we build the original AI-text corpus \(\) from \(\) using a target language model \(_{}\), which performs text completion using the first 30 tokens as the prompt and limits the sentence length to be 200 tokens. For evaluation, we select four human-text datasets covering different domains. Following , we use Xsum, SQuAD, and Reddit WritingPrompts (WP) to test a detector's ability to detect fake news, avoid academic fraud, and identify machine-generated literature innovation, respectively. In addition, we also use the non-native-authored TOEFL dataset (TOFEL)  to evaluate a detector's bias when encountering non-native-authored English text. Please see Appendix A for more details about the evaluation datasets. Following existing works, we report the area under the receiver operating characteristic curve (AUROC) score by varying the detector's threshold as the performance measure (higher is better), which captures the relationship between the true positive rate and the false positive rate.

**Comparisons.** We compare RADAR with various detection methods. These methods include the OpenAI (RoBERTa) model which is fine-tuned on WebText  and GPT-2  generations, as well as the statistical approaches including log probability, rank, log rank, entropy, and DetectGPT [8; 19; 23].

Specifically, we implemented DetectGPT using the trained T5-large model as the mask-filling model and performed 10 perturbations for each sentence to be detected.

**Large Language Models.** For the target LLM \(_{}\), we select 4 pairs of LLMs and summarize them in Table 1. Each pair contains an open-source LLM and its fine-tuned version via instruction-tuning.

**Paraphrase Configurations.** We consider two settings: _without (w/o) paraphrasing_ and _with paraphrasing_. To prepare the machine-generated text for evaluation, for the w/o paraphrasing setting, we use the original AI-text corpus \(\) generated by a target LLM based on an evaluation dataset. For the with paraphrasing setting, we define two types of paraphrasing: _seen paraphraser_ and _unseen paraphraser_. The seen paraphraser refers to the paraphraser \(_{}\) returned by RADAR. The unseen paraphraser means a new paraphraser that has not participated in training the detector of RADAR. We used the OpenAI API service of GPT-3.5-Turbo as the default unseen paraphraser. The prompt we used for paraphrasing is "Enhance word choices to make the sentence sound more like a human", as inspired by .

**Implementation Details.** We provide the detailed setups when implementing Algorithm 1. We build a PPO buffer \(\) that can temporarily store 256 pairs of data for subsequent training. We use the pre-trained T5-large and RoBERTa-large models as the initialization of \(_{}\) and \(_{}\) respectively. During training, we set the batch size to 32 and train the models until the validation loss converges. We use AdamW as the optimizer with the initial learning rate set to 1e-5 and use linear decay for both \(_{}\) and \(_{}\). We set \(=0.5\) for sample balancing in Eq. 3 and set \(=0.01\) in Eq. 2. We follow the same construction principle of the training dataset to create the 4 evaluation datasets based on Xsum, SQuAD, WP, and TOFEL. Experiments were run on 2 GPUS (NVIDIA Tesla V100 32GB).

### Performance Evaluation and Comparison with Existing Methods

We run three groups of experiments (w/o paraphraser, seen paraphraser, and unseen paraphraser) and report the overall results of RADAR and the compared methods on all 4 datasets in Table 2. The reported AUROC scores are averaged over the 8 considered LLMs. In the relatively easy case of without paraphrasing, most detectors attain good AUROC scores. RADAR attains a comparable performance (0.856) to the best existing detector (log rank, 0.904). The slightly worse performance of RADAR can be explained by the tradeoff in enhancing AI-text detection against paraphrasing.

When facing paraphrasing, all existing methods except entropy show significant performance degradation. The drop in AUROC compared to the w/o paraphrasing case ranges from \(10.4\%\) to \(81.7\%\). While entropy is shown to be more robust to paraphrasing, its AUROC score can be quite low. On the contrary, RADAR demonstrates robust and superior detection power, attaining the best performance on every dataset. As shown in Figure 2, the average AUROC score of RADAR (0.857) improves the best existing method (entropy, 0.651) by 31.64% against the unseen paraphraser. On average, RADAR is more robust to the seen paraphraser than the unseen paraphraser, because the seen paraphraser is what is used to train the detector in RADAR. More importantly, the detection performance of RADAR is stable across different paraphrasing schema, suggesting that RADAR can successfully mitigate the performance drop in AI-text detection.

 
**Parameter Count** & **Model Name** & **Organization** & **Pretrain Data** & **Instruction Fine-tune Data** \\   & Pythia-2.8B & EleutherAI &  & \)} \\    & Dolly-V2-3B & Dharicks & & \\   & Palmrya-base & Writer & & \\    & Camet-5B & Writer & & \\   & GPT-3.6B & EleutherAI &  &  \\    & Dolly-V1-6B & Dharicks & & \\   & LiMA-7B & Meta & & \\    & Vicuna-7B & LMSys & & \\  

Table 1: Summary of the studied large language models

### AI-Text Detection Transferability of RADAR

We explore the AI-text detection transferability of RADAR between the 8 LLMs and report the ratio F(A,B)=AUROC(A,B)/AUROC(B,B) for each LLM pair (A,B), where AUROC(A,B) means using the RADAR's detector trained on model A to evaluate the AI-text generated by model B. A larger ratio means better transferability from A to B. Figure 3 shows the matrix of pairwise detection transferability and the bar chart of the holistic detection transferability to all the 8 LLMs in the without and unseen paraphrasing settings. We highlight two key observations as follows.

**(I) Instruction-tuned models have better detection transferability.** Partitioning the LLMs into two groups, we can find that the detector targeting an instruction-tuned LLM (top 4 rows) generally transfers better than the detector targeting the corresponding LLM without instruction-tuning (bottom 4 rows). Take the pair (Vicuna-7B, LLaMA-7B) as an example, we can see that without paraphrasing, F(Vicuna-7B,LLaMA) can reach up to \(95.0\%\). On the other hand, F(LLaMA-7B,Vicuna-7B) can only

   &  &  &  &  &  &  \\   & log p & 0.882 & 0.868 & 0.967\(\) & 0.832 & 0.887 \\  & rank & 0.722 & 0.752 & 0.814 & 0.731 & 0.755 \\  & log rank & 0.902 & 0.893\(\) & 0.975\(\) & 0.847\(\) & 0.904\(\) \\ w/o Paraphraser & entropy & 0.536 & 0.521 & 0.296 & 0.534 & 0.472 \\  & DetectGPT & 0.874 & 0.790 & 0.883 & 0.919\(\) & 0.867 \\  & OpenAI (RoBERTa) & 0.953\(\) & 0.914\(\) & 0.924 & 0.810 & 0.900\(\) \\  & RADAR & 0.934\(\) & 0.825 & 0.847 & 0.820 & 0.856 \\   & log p & 0.230 & 0.156 & 0.275 & 0.130 & 0.198 \\  & rank & 0.334 & 0.282 & 0.357 & 0.163 & 0.284 \\  & log rank & 0.245 & 0.175 & 0.281 & 0.134 & 0.209 \\ RADAR-Seen Paraphraser & entropy & 0.796 & 0.845\(\) & 0.763 & 0.876\(\) & 0.820\(\) \\  & DetectGPT & 0.191 & 0.105 & 0.117 & 0.177 & 0.159 \\  & OpenAI (RoBERTa) & 0.821\(\) & 0.842 & 0.892\(\) & 0.670 & 0.806 \\  & RADAR & 0.920\(\) & 0.927\(\) & 0.908\(\) & 0.932\(\) & 0.922\(\) \\   & log p & 0.266 & 0.343 & 0.641 & 0.438 & 0.422 \\  & rank & 0.433 & 0.436 & 0.632 & 0.342 & 0.461 \\  & log rank & 0.282 & 0.371 & 0.632 & 0.421 & 0.426 \\ RADAR-Unseen Paraphraser & entropy & 0.779 & 0.710\(\) & 0.499 & 0.618 & 0.651\(\) \\  & DetectGPT & 0.360 & 0.384 & 0.609 & 0.630\(\) & 0.434 \\  & OpenAI (RoBERTa) & 0.789\(\) & 0.629 & 0.726\(\) & 0.364 & 0.627 \\  & RADAR & 0.955\(\) & 0.861\(\) & 0.851\(\) & 0.763\(\) & 0.857\(\) \\  

Table 2: AUROC score averaged over 8 target LLMs. RADAR-Seen Paraphraser means the paraphraser used in RADAR (\(_{}\)), RADAR-Unseen Paraphraser is OpenAIâ€™s GPT-3.5-Turbo API. The notations \(\{ for \(68.2\%\). Sorting the detectors according to the holistic detection transferablbility (which is presented in the bar chart), we can see the top-3 detectors are all trained with the instruction-tuned LLMs. A similar conclusion can be made for the with paraphrasing setting. Moreover, there is no obvious trend between the target LLM size and the resulting detection performance. The effect of instruction tuning on transferability is more prominent than model size.

**(II) RADAR achieves better detection transferability against paraphrasing.** Another interesting finding is that RADAR's transferability is generally improved when paraphrasing is in place. Comparing the two bar charts in Fig. 2(a) and Fig. 2(b), the average holistic detection transferability (over all LLMs) is increased by \(11.6\%\). Except for LLaMA-7B (3.8% drop) and GPT-J-6B (1.4% drop), all other LLMs' holistic transferability scores are improved from 2.4% (Palmyra-base) to 47.6% (Camel-5B).

**Transfer detection on AI-text generated by GPT-4.** We also test RADAR detectors on the texts generated by GPT-4. The results show that 5 out of 8 RADAR models can outperform the OpenAI (RoBERTa), and three of them can achieve more than 0.8 detection AUROC. For example, RADAR trained on Camel-5B can achieve 0.915 detection AUROC on GPT-4 generations. The results show that the RADAR can achieve good transfer detection for GPT-4. The details are given in Appendix K.

**Ensemble detection.** We also explored whether and how ensemble learning benefits detection by combining the outputs of detectors. The results show that the detection performance can be lifted by carefully tuning the ensemble ratio and the model to be combined. Please see Appendix G for the exact experiment results.

To sum up, we believe our findings suggest promising results for training a universal robust AI-text detector by leveraging state-of-the-art LLMs, and RADAR can use a smaller-sized and weaker LLM to achieve good detection performance on texts generated from top-notching LLMs (such as GPT-4).

### Variants of Paraphrasing

In addition to paraphrasing the original LLM-generated texts, we also evaluate the detection performance when paraphrasing human texts (the output is labeled as AI-text). We also allow paraphrasing multiple times in our analysis. We conduct our experiments on the Xsum dataset using the detector trained with Camel-5B. The paraphraser for evaluation is GPT-3.5-Turbo. As shown in Figure 3(a), we find that RADAR is the only detector robust to multi-round paraphrasing. On paraphrased AI-text, all existing methods suffer from a notable performance drop. On paraphrased human-text, RADAR remains effective, along with two existing methods (OpenAI (RoBERTa) and entropy). In general, multi-round paraphrasing does not seem to increase the difficulty of AI-text detection. We also find RADAR is robust to Dipper , another paraphrase model. Please see Appendix I for details.

### Evaluation on RADAR's Paraphraser

Although our focus is on training a robust AI-text detector via RADAR, as a by-product, we expect to obtain a better paraphraser through adversarial learning. To verify this hypothesis, we compare the quality of the initial paraphraser (a pretrained LLM) and the final paraphraser returned by RADAR

Figure 4: Detection AUROC of RADAR against multiple paraphrasing. The experiments are conducted on Xsum using the detector trained for Camel-5B.

using GPT-3.5-Turbo's response. We select 100 documents from WebText  and use 4 different paraphrasers from RADAR to paraphrase the documents. Then, we ask GPT-3.5-Turbo to rate sentences generated by these paraphrasers versus their initial version (T5-large). Figure 4(a) shows that RADAR also improves the quality of paraphrasing. Figure 4(b) shows that the RADAR's paraphraser can score higher if it is trained with a larger target LLM with instruction tuning. Following [11; 12], we also evaluate RADAR's paraphrasers on Quora Question Pairs (QQP8) and use iBLEU (\(=0.8\))  as the metric (higher is better). Figure 4(c) shows that the paraphrasing performance can be improved via RADAR as all the RADAR-paraphrasers can achieve a larger iBLEU score than T5-large.

### Balancing the Detection Performance in the with and without Paraphrasing Settings

From Figure 2, we can observe that though RADAR can achieve robust detection under paraphrasing, it is (slightly) worse than some of the existing baselines when AI-text data are unperturbed (i.e., w/o paraphrasing). We run a trade-off analysis on the weight coefficient \(\) in Equation (3) to study whether RADAR can be further tuned to achieve competitive performance on unperturbed data while still being robust to paraphrasing. We use Vicuna-7B as the target model to train 10 RADAR detectors by varying \(\) from 0.1 to 1.0 with 0.1 increment, and then evaluate these detectors as well as other detection baselines on the evaluation datasets. The results in Appendix J show that we can promote RADAR's performance on unperturbed data while still preserving high detection AUROC on paraphrased data. Take \(=0.6\) as an example. When we change \(\) from \(0.5\) (the default value of \(\)) to \(0.6\), the AUROC of w/o paraphrasing increases from \(0.906\) to \(0.937\), while the AUROC of unseen-paraphrasing also increases from \(0.892\) to \(0.920\). The result suggests that the detection performance of RADAR in the with and without paraphrasing settings can be simultaneously improved or better balanced with careful tuning of the hyperparameter \(\) during training.

## 5 Conclusion

In this paper, we presented a robust AI-text detector training framework called RADAR, which adopts adversarial learning to jointly train a detector and a paraphraser. RADAR addresses the shortcoming of existing detectors when facing LLM-paraphrased texts. Our extensive experiments on 8 LLMs and 4 datasets validated the effectiveness of RADAR and demonstrated its strong transferability across LLMs. We believe our results shed new light on improving AI-text detection.

## 6 Limitations and Ethical Considerations

While RADAR is more robust to paraphrasing than existing baselines measured on 4 datasets, sometimes it may show degraded detection performance against native LLM-generated texts (without paraphrasing) when compared to the best existing detection method. Moreover, like every existing AI-text detector, we acknowledge that our detector is not perfect and will likely give incorrect predictions in some cases. In terms of ethical considerations, we suggest users use our tool to assist with identifying AI-written content at scale and with discretion. If the detection result is to be used as evidence, further validation steps are necessary as RADAR cannot always make correct predictions.

Figure 5: Evaluation of RADARâ€™s paraphraser versus its initial version (T5-large).