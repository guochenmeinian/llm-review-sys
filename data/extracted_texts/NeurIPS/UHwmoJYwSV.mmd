# Attacks on Online Learners:

a Teacher-Student Analysis

Riccardo G. Margiotta

R. G. Margiotta (rimargi@sissa.it) is the corresponding author.

Sebastian Goldt

Guido Sanguinetti

_International School for Advanced Studies, Trieste, Italy_

R. G. Margiotta (rimargi@sissa.it) is the corresponding author.

###### Abstract

Machine learning models are famously vulnerable to adversarial attacks: small ad-hoc perturbations of the data that can catastrophically alter the model predictions. While a large literature has studied the case of test-time attacks on pre-trained models, the important case of attacks in an online learning setting has received little attention so far. In this work, we use a control-theoretical perspective to study the scenario where an attacker may perturb data labels to manipulate the learning dynamics of an online learner. We perform a theoretical analysis of the problem in a teacher-student setup, considering different attack strategies, and obtaining analytical results for the steady state of simple linear learners. These results enable us to prove that a discontinuous transition in the learner's accuracy occurs when the attack strength exceeds a critical threshold. We then study empirically attacks on learners with complex architectures using real data, confirming the insights of our theoretical analysis. Our findings show that greedy attacks can be extremely efficient, especially when data stream in small batches.

## 1 Introduction

Adversarial attacks pose a remarkable threat to modern machine learning (ML), as models based on deep networks have proven highly vulnerable to such attacks. Understanding adversarial attacks is therefore of paramount importance, and much work has been done in this direction; see  for reviews. The standard setup of adversarial attacks aims to change the predictions of a trained model by applying a minimal perturbation to its inputs. In the _data poisoning_ scenario, instead, inputs and/or outputs of the training data are corrupted by an attacker whose goal is to force the learner to get as close as possible to a "neafarious" target model, for example to include a backdoor . Data poisoning has also received considerable attention that has focused on the offline setting, where models are trained on fixed datasets . An increasing number of real-world applications instead require that machine learning models are continuously updated using streams of data, often relying on transfer-learning techniques. In many cases, the streaming data can be subject to adversarial manipulations. Examples include learning from user-generated data (GPT-based models) and collecting information from the environment, as in e-commerce applications. Federated learning constitutes yet another important example where machine learning models are updated over time, and where malicious nodes in the federation have private access to a subset of the data used for training.

In the online setting, the attacker intercepts the data stream and modifies it to move the learner towards the nefarious target model, see Fig. 1-A for a schematic where attacks change the labels. In this streaming scenario, the attacker needs to predict both the future data stream and model states, accounting for the effect of its own perturbations, so as to decide how to poison the current data batch. Finding the best possible attack policy for a given data batch and learner state can be formalized as a stochastic optimal control problem, with opposing costs given by the magnitude of perturbationsand the distance between learner and attacker's target . The ensuing dynamics have surprising features. Take for example Fig. 1-B, where we show the accuracy of an attacked model (a logistic regression classifier classifying MNIST images) as a function of the attack strength (we define this quantity in Sec. 2.2). Even in such a simple model, we observe a strong nonlinear behavior where the accuracy drops dramatically when crossing a critical attack strength. This observation raises several questions about the vulnerability of learning algorithms to online data poisoning. What is the effect of the perturbations on the learning dynamics and long-term behavior of the model under attack? Is there a minimum perturbation strength required for the attacker to achieve a predetermined goal? And how do different attack strategies compare in terms of efficacy? We investigate these questions by analyzing the attacker's control problem from a theoretical standpoint in the teacher-student setup, a popular framework for studying machine learning models in a controlled way . We obtain analytical results characterizing the steady state of the attacked model in a linear regression setting, and we show that a simple greedy attack strategy can perform near optimally. This observation is empirically replicated across more complex learners in the teacher-student setup, and the fundamental aspects of our theoretical results are also reflected in real-data experiments.

#### Main contributions

* We present a theoretical analysis of online data poisoning in the teacher-student setup, providing analytical results for the linear regression case [Result 1-5]. In particular, we demonstrate that a phase transition takes place when the batch size approaches infinity, signaled by a discontinuity in the accuracy of the student against the attacker's strength [Result 2].
* We provide a quantitative comparison between different attack strategies. Surprisingly, we find that properly calibrated _greedy_ attacks can be as effective as attacks with full knowledge of the incoming data stream. Greedy attacks are also computationally efficient, thus constituting a remarkable threat to online learners.
* We empirically study online data poisoning on real datasets (MNIST, CIFAR10), using architectures of varying complexities including LeNet, ResNet, and VGG. We observe qualitative features that are consistent with our theoretical predictions, providing validation for our findings.

### Further related work

Data poisoning attacks have been the subject of a wide range of studies. Most of this research focused on the offline (or batch) setting, where the attacker interacts with the data only once before training starts . Mei et al. notably proposed an optimization approach for offline data poisoning . Wang and Chaudhuri addressed a streaming-data setting, but in the idealized scenario where the attacker has access to the full (future) data stream (the so-called _clairvoyant_ setting), which effectively reduces the online problem to an offline optimization . So far, the case of online data poisoning has received only a few contributions with a genuine online perspective. In , Pang and coauthors investigated attacks on online learners with poisoned batches that aim for an immediate disruptive impact rather than manipulating the long-term learning dynamics. Zhang et al. proposed an optimal control formulation of online data poisoning by following a parallel line of research, that of online teaching , focussing then on practical attack strategies that perturb the input . Our optimal control perspective is similar to that of Zhang and co-authors, though our contribution differs from theirs in various aspects. First, we consider a supervised learning setting where attacks involve data labels, and not input data. We note that attacks on the labels have a reduced computational cost compared to high-dimensional adversarial perturbations of the inputs, which makes label poisoning more convenient in a streaming scenario. Second, we provide a theoretical analysis and explicit results for the dynamics of the attacked models, and we consider non-linear predictors in our experiments. We observe that our setup is reminiscent of continual learning, which has also recently been analyzed in a teacher-student setup . However, in online data poisoning the labels are dynamically changed by the attacker to drive the learner towards a desired target. In continual learning there is no entity that actively modifies the labels, and the learner simply switches from one task to another. Finally, we note that standard (static, test-time) adversarial attacks have been studied in a teacher-student framework .

**Reproducibility**. The code and details for implementing our experiments are available here.

## 2 Problem setup

### The teacher, the student, and the attacker

The problem setup is depicted in Fig. 1-A. There are three entities: the teacher, the student, and the attacker. The _teacher_ is a model of the environment, which generates a stream of clean input-output data batches. The inputs \(x^{D}\) have elements sampled from a distribution \(_{x}\), while clean labels are provided by the scalar teacher function: \(y^{}=^{}(x)\). The \(^{}\) batch of the stream is \(B^{P}_{}=\{(x_{},y^{}_{})_{P}\}_{p=1}^{P}\), with \(P\) the batch size. The _attacker_ intercepts the clean batch \(B^{P}_{}\) and perturbs the labels so as to make the student learn the _target_ function \(^{*}\). The _student_\(^{}_{}\) represents the learning model, and it updates its weights \(^{}_{}\) on the stream of perturbed batches via SGD, using each (perturbed) batch only once. We will consider teacher, attacker's target, and student with the same architecture \((x;)\); they only differ by the value of their parameters, respectively \(^{}\), \(^{*}\), \(^{}_{}\). We will refer to this Teacher-Student-Attacker setup as the _TSA problem_.

If the student trained directly on the input-label pairs provided by the teacher, we would recover the standard teacher-student setup that has been analyzed extensively in the literature [7; 8; 9; 10]. The key novelty of this paper is that we consider a setup where each new batch of data is instead _poisoned_ by the attacker with the attempt of deviating the student towards a different target function.

### The attack

The goal of the attacker is for the student to predict a target label \(y^{*}=^{*}(x)\) for each given input \(x\). Target label and clean label can coincide for specific inputs, however they won't generally do so. The attacker performs an action \(a_{}\) that produces perturbed labels as

\[y^{}_{}=y^{}(1-a_{})+y^{*}a_{}, a_{}[a_{ },a_{}].\] (1)

Note that \(a_{}=0\) implies no perturbation, while for \(a_{}=1\) perturbed and target labels coincide. We allow \(a_{}<0\) and \(a_{}>1\) to model attackers that under/overshoot. The attacker's action \(a_{}\) thus constitutes the _control variable_ of the TSA problem. We primarily consider the case where the same value of \(a_{}\) is used for all labels in each batch, but can be different for different batches. We also explore settings where this assumption is relaxed, including the case of sample-specific perturbations, and the scenario where only a fraction of the labels in each batch is perturbed.

The aim of the attacker is to sway the student towards its target, and so minimize the _refarious cost_

\[g^{}_{}=_{p=1}^{P}(^{}_{}(x_{  p})-^{*}(x_{ p}))^{2},\] (2)

Figure 1: _Problem setup and key experimental observations_. A: The environment (teacher) generates a sequence of clean data with \(^{}\) input \(x_{}\) and label \(y^{}_{}\). The attacker perturbs the labels towards a nefarious target \(y^{*}_{}=^{*}(x_{})\). At each training step, the predictions \(y^{}_{}\) of the learner (student) and the perturbed labels \(y^{}_{}\) are used to compute the loss and update the learner parameters. B: accuracy vs attack strength of a logistic regression classifying MNIST digits 1 and 7 under online label-flipping attacks. C: relative distance \(d_{}\) of the logistic learner vs SGD step \(\) during the clean training phase, and for training under attacks of increasing attack strength.

using small perturbations. For this reason, it also tries to minimize the _perturbation cost_

\[g_{}^{ per}=a_{}^{2},=( ^{*})C,\] (3)

with \(C\) a parameter expressing the cost of actions, and \((^{*})=_{x}[(^{*}(x)-^{}(x))^{2}]\) the expected squared error of the attacker's target function; \(_{x}\) indicates the average over the input distribution. We use the pre-factor \((^{*})\) in \(g_{}^{ per}\) to set aside the effect of the teacher-target distance on the balance between the two costs. Note that \(C\) is inversely proportional to the intuitive notion of _attack strength_: low \(C\) implies a low cost for the action and therefore results in larger perturbations; the \(x\)-axis in Fig. 1-B shows the inverse of \(C\). The attacker's goal is to find the _sequence_ of actions \(a_{}^{ opt}\) that minimize the _total expected cost_:

\[\{a_{}^{ opt}\}=*{argmin}_{\{a_{}\}}\,_{ fut }[_{T}G(\{a_{}\},T)], G=_{=1}^{T} ^{}(g_{}^{ per}+g_{}^{ nef}),\] (4)

where \(_{ fut}\) represents the average over all possible realizations of the future data stream, and \((0,1)\) is the future discounting factor. In relation to \(\), we make the crucial assumption that \(G\) is dominated by the steady-state running costs; we use \(=0.995\) in our experiments.

### SGD dynamics under attack

The perturbed batch \(B_{}^{P}=\{(x_{},y_{}^{})_{p}\}_{p=1}^{P}\) is used to update the model parameters \(_{}^{*}\) following the standard (stochastic) gradient descent algorithm using the MSE loss function:

\[_{+1}^{*}=_{}^{*}-_{_{}^{*}}_ {}(B_{}^{P}),_{}= _{p=1}^{P}(_{}^{*}(x_{ p})-y_{ p}^{})^{2},\] (5)

where \(\) is the learning rate. We characterize the system's dynamics in terms of the relative teacher-student distance, defined as

\[d_{}(C,P)=((_{}^{*})/(^{*}))^{ 1/2}.\] (6)

Note that \(d=0\) (resp. \(d=1\)) for a student \(^{ s}\) that perfectly predicts the clean (resp. target) labels, on average. We will split the training process into two phases: first, the student trains on clean data, while the attacker calibrates its policy. Then, after convergence of the student, attacks start and training continues on perturbed data. A typical realization of the dynamics is depicted in Fig. 1-C, which shows \(d_{}\) for a logistic regression classifying MNIST digits during the clean-labels phase, and under attacks of increasing strength (decreasing values of the cost \(C\)). The sharp transition observed in Fig. 1-B occurs for \(d_{} 0.5\), when the student is equally distant from the teacher and target.

### Attack strategies

Finding the optimal actions sequence that solves the control problem (4) involves performing the average \(_{ fut}\), and so it is only possible if the attacker knows the data generating process. Even then, however, optimal solutions can be found only for simple cases, and realistic settings require approximate solutions. We consider the following approaches:

* **Constant attacks**. The action is observation-independent and remains constant for all SGD steps: \(a_{}=a^{c}\). This simple baseline can be optimized by sampling data streams using past observations, simulating the dynamics via Eq. (5), and obtaining numerically the value of \(a^{c}\) that minimizes the simulated total expected cost.
* **Greedy attacks**. A greedy attacker maximizes the immediate reward. Since the attacker actions have no instantaneous effect on the learner, the greedy objective has to include the next training step. The greedy action is then given by \[a_{}^{ G}(x_{},_{}^{*};)=*{ argmin}_{a_{}}_{x_{+1}}[g_{}^{ per}+g_{+1}^{ nef}].\] (7) The average \(_{x_{+1}}\) can be estimated with input data sampled from past observations, and using (5) to obtain the next student state. Similarly to constant attacks, the greedy future weight \(\) can be calibrated by minimizing the simulated total expected cost using past observations.

* **Reinforcement learning attacks**. The attacker can use a parametrized policy function \(a(x,^{})\) and optimize it via reinforcement learning (RL). This is a _model free_ approach, and it is suitable for _grey box_ settings. Tuning deep policies is computationally costly, however, and it requires careful calibrations. We utilize TD3 , a powerful RL agent that improves the deep deterministic policy gradient algorithm . We employ TD3 as implemented in Stable Baselines3 .
* **Clairvoyant attacks**. A clairvoyant attacker has full knowledge of the future data stream, so it simply looks for the sequence \(\{a_{}\}\) that minimizes \(G(\{a_{}\},T 1)\). Although the clairvoyant setting is not realistically plausible, it provides an upper bound for the attacker's performance in our experiments. We use Opty  to cast the clairvoyant problem into a standard nonlinear program, which is then solved by an interior-point solver (IPOPT) .

The above attack strategies are summarized as pseudocode in Appendix A.

## 3 Theoretical analysis

We first analyze online label attacks with synthetic data to establish our theoretical results. We will confirm our findings with real data experiments in Sec. 4.2. Solving the stochastic optimal control problem (4) is very challenging, even for simple architectures \((x,)\) and assuming that \(_{x}\) is known. We discuss an analytical treatment in two scenarios for the _linear_ TSA problem with unbounded action space, where

\[(x;w)=w^{}x/.\] (8)

### Large batch size: deterministic limit

In the limit of very large batches, batch averages of the nefarious cost \(g_{}^{}\) and loss \(_{}\) in Eqs. (2, 5) can be approximated with averages over the input distribution. For standardized and i.i.d. input elements, the resulting equations are

\[g_{}^{}=|w_{}^{}-w^{}|^{2}, _{w^{}}_{}=((w_{}^{} -w^{})+(w_{}^{}-w^{})a_{}).\] (9)

In this setting, the attacker faces a deterministic optimal control problem, which can be solved using a Lagrangian-based method. Following this procedure, we obtain explicit solutions for the steady-state student weights \(w_{}^{}\) and optimal action \(a_{}^{}\). The relative distance \(d_{}^{}\) between student and teacher then follows from Eq. (6). We find

[Result 1] \[w_{}^{}=w^{}+d_{}^{}(w^{}-w^ {}), d_{}^{}=a_{}^{}=(C+ 1)^{-1}.\] (10)

The above result is intuitive: the student learns a function that approaches the attacker's target as the cost of actions \(C\) tends to zero, and \(d_{}^{}(C=0)=1\). Vice versa, for increasing values of \(C\), the attacker's optimal action decreases, and the student approaches the teacher function.

In the context of a classification task, we can characterize the system dynamics in terms of the accuracy of the student \(A_{}(C,P)=_{x}[S_{}(x)],\) with \(S_{}(x)=((_{}^{}(x))+( ^{}(x)))/2\). For label-flipping attacks, where \(^{}=-^{}\), a direct consequence of Eq. (10) is that the steady-state accuracy of the student exhibits a discontinuous transition in \(C=1\). More precisely, we find

[Result 2] \[A_{}(C)=1-H(C-1),\] (11)

with \(H()\) the Heaviside step function. This simple result explains the behaviour observed in Fig. 1-B, where the collapse in accuracy occurs for attack strength \(C^{-1} 1\). We refer to Appendix B for a detailed derivation of results (10, 11).

### Finite batch size: greedy attacks

When dealing with batches of finite size, the attacker's optimal control problem is stochastic and has no straightforward solution, even for the simple linear TSA problem. To make progress, we consider greedy attacks, which are computationally convenient and analytically tractable. While an explicit solution to (7) is generally hard to find, it is possible for the linear case of Eq. (8).

We find the _greedy action policy_

\[a^{ G}=DP}_{p=1}^{P}{(w^{*}-w^{*})^{ T}x_{p}(w^{ t }-w^{*})^{ T}x_{p}}+O().\] (12)

Note that \(a^{ G}\) decreases with the cost of actions, and \(a^{ G} 0\) for \(\). With a policy function at hand, we can easily investigate the steady state of the system by averaging over multiple realizations of the data stream. For input elements sampled i.i.d. from \(_{x}=(0,1)\), we find the _average_ steady-state weights \(_{}^{*}\) as

\[                       \] (13)

where \(_{}^{ G}\) is the distance (6) of the average steady-state student; we refer to Appendix C for the derivation of the above result. The expression of \(_{}^{ G}\) includes a surprising batch-size effect: for a fixed value of the cost \(C\), greedy attacks become more effective for batches of decreasing sizes, as the weights get closer to \(w^{*}\) while perturbations become smaller (see Eq. (45)). This effect is displayed in Fig. 2-A, which shows \(d_{}\) vs SGD step for greedy attacks, and for \(P=1,10\). Note that only the perturbed training phase is shown, so \(d_{0}=0\) as the student converges to the teacher function during the preceding clean training phase. The black dashed lines correspond to the steady-state values given by (13), which are in very good agreement with the empirical averages (up to a finite-\(\) effect)2. We also observe that \(d_{}\) is characterized by fluctuations with amplitude that decreases as the batch size increases. A consequence of such fluctuations is that the average steady-state accuracy of the student undergoes a smooth transition. The transition becomes sharper as the batch size increases, and in the limit \(P\) it converges to the prediction (11); see Fig. 2-B, which shows numerical evaluations of \(A_{}(C,P)\) averaged over multiple data streams. In our experiments, we draw the teacher weights elements as i.i.d. samples from \((0,1)\) and normalize them so that \(|w^{*}|^{2}=D\), while \(w^{*}=-w^{ t}\).

**Remark.** In order to derive the result of Eq. (13), we have set \(=D/\) so that \(_{}^{ G} d_{}^{ opt}\) for \(P\) (see Appendix B.1). This guarantees the steady-state optimality of the greedy action policy in the large batch limit. Note that \(\) coincides with the timescale of the linear TSA problem, and it lends itself to a nice interpretation: in the greedy setting, where the horizon ends after the second training step, the future is optimally weighted by a factor proportional to the decorrelation time of the weights. Optimality for \(P\) finite is not guaranteed, though we observe numerically that \(\) approximately matches our choice even for \(P=1,10\). We use again \(=D/\) to obtain the theoretical results of the next two sections.

Figure 2: _Greedy and clairvoyant attacks in the linear TSA problem_. A: relative distance vs SGD step \(\) for greedy attacks on linear models for two different batch sizes \(P\). Solid lines display the average \(_{}\), shaded areas cover 1std of \(d_{}\). Dashed lines show the theoretical estimate of Eqs. (13). B: steady-state average accuracy of the student vs cost of action. The dashed line shows the theoretical prediction of Eq. (11). C: scatterplot of greedy (G) vs clairvoyant (CV) actions used on the same data stream with \(P=1\). D: student-teacher (s-t, green) and student-target (s-\(*\), coral) overlap vs SGD step, for the clairvoyant (dotted line) and greedy (continuous line) attacks of panel C. Parameters: \(C=1\), \(a[-2,3]\), \(D=10\), \(=0.02 D\). Input elements sampled i.i.d. from \(_{x}=(0,1)\).

#### 3.2.1 Sample-specific perturbations

The batch-size effect observed for greedy attacks can be explained as a signature of the batch average appearing in the policy function (12). When dealing with a single data point at a time, the attacker has precise control over the data labels, designing sample-specific perturbations. For batches with multiple samples, instead, perturbations are designed to perform well on average and do not exploit the fluctuations within the data stream. As a result, attacks become less effective, increasing in magnitude and leading to a smaller average steady-state distance between the student and the teacher function. A straightforward solution to this problem consists of applying sample-specific perturbations, thus using a multi-dimensional control \(a^{P}\), which, however, results in a higher computational cost. This can be achieved by using the policy function (12) independently for each sample, so that the \(p\)-th element of \(a\) is given by

\[a_{p}^{}=D}(w^{s}-w^{*})^{}x_{p}(w^{}-w^{*})^{}x_{p}.\] (14)

In Appendix C.1, we show that this strategy coincides with the optimal greedy solution for multi-dimensional, sample-specific control, up to corrections of order \(\). We also show that the resulting average steady state reached by the student, for normal and i.i.d. input data, is

[Result 4] \[_{}^{s}=w^{}-_{}^{}  w^{*},_{}^{}=(C/3+1)^ {-1}.\] (15)

Remarkably, there is no batch-size dependence in the above solution, and the average steady-state distance coincides with (13) for \(P=1\). This result demonstrates that precise, sample-specific greedy attacks remain effective independently of the batch size of the streaming data.

#### 3.2.2 Mixing clean and perturbed data

In the previous section, we addressed a case of _enhanced_ control, where batch perturbations are replaced by sample-specific ones. Here, instead, we consider a case of _reduced_ control, where the attacker can apply the same perturbation to a fraction of the samples in each batch. This scenario can arise when the attacker faces computational limitations or environmental constraints. For example, the streaming frequency may be too high for the attacker to contaminate all samples at each time step, or, in a poisoned federated learning setting, the central server could have access to a source of clean data, beyond the reach of the attackers. Concretely, we assume that the attacker has access to a fraction \(\) of the batch samples only. Therefore, training involves \( P\) perturbed and \((1-)P\) clean samples. Perturbations follow the policy function

\[a_{}^{}=D P}_{p=1}^{ P} w_{} ^{*}x_{ p} w^{*}x_{ p}+O().\] (16)

In Appendix C.2, we find the following average steady-state solution for normal and i.i.d. input data:

[Result 5] \[_{}^{}=w^{}-_{}^{ } w^{*},_{}^{}=( ()C+1)^{-1};\] (17)

see Fig. 7 for a comparison with empirical evaluations. Note that for large batches \(_{}^{}(C/+1)^{-1}\), indicating that the cost of actions effectively increases by a factor \(^{-1}\). We remark that the above result is derived under the assumption that the attacker ignores the number of clean samples used during training, so it cannot compensate for the reduced effectiveness of the attacks.

## 4 Empirical results

### Experiments on synthetic data

Having derived analytical results for the infinite batch and finite batch linear TSA problem, we now present empirical evidence as to how different strategies compare on different architectures. As a first question, we investigate the efficacy of greedy attacks for data streaming in batches of small size. By choosing an appropriate future weight \(\), we know the greedy solution is optimal as the batch size tends to infinity. To investigate optimality in the finite batch setting, we compare greedy actions with clairvoyant actions on the same data stream, optimizing \(\) over a grid of possible values. Bydefinition, clairvoyant attacks have access to all the future information and therefore produce the best actions sequence. The scatterplot in Fig. 2-C shows greedy vs clairvoyant actions, respectively \(a^{ G}\) and \(a^{ CV}\), on the same stream. Note that greedy actions are typically larger than their clairvoyant counterparts, except when \(a^{ CV}\) is very large. This is a distinctive feature of greedy strategies as they look for local optima. The clairvoyant vision allows the attacker to allocate its resources more wisely, for example by investing and paying extra perturbation costs for higher future rewards. This is not possible for the greedy attacker, as its vision is limited to the following training step. Substantially, though, the scattered points in Fig. 2-C lie along the diagonal, indicating that greedy attacks are nearly clairvoyant efficient, even for \(P=1\). The match between \(a^{ G}\) and \(a^{ CV}\) corresponds to almost identical greedy and clairvoyant trajectories of the student-teacher and student-target weights overlap, defined respectively as \(O^{ st}_{}={w^{ s}_{}}^{ T}{w^{ t}}/D\) and \(O^{ s*}_{}={w^{ s}_{}}^{ T}{w^{ s}}/D\) (see Fig. 2-D).

Next, we investigate the steady state reached by the student under greedy attacks and the performance of the various attack strategies for three different architectures: the linear regression of Eq. (8), the sigmoidal perceptron \((x;w)={ Erf}(z/)\), with \({ Erf}()\) the error function and \(z={w^{ T}}x/\), and the 2-layer neural network (NN)

\[(x;v,w)=}_{m=1}^{M}v_{m}{ Erf}(z_{m}/ ), z_{m}={w^{ T}_{m}}x/,\] (18)

with parameters \(w_{m}^{D}\) and \(v^{M}\). We continue to draw all teacher weights from the standard normal distribution, normalizing them to have unitary self-overlap. For the perceptron we set the target weights as \(w^{*}=-w^{ t}\), while for the neural network \(w^{*}_{m}=w^{ t}_{m}\) and \(v^{*}=-v^{ t}\). The top row in Fig. 3 shows \(_{}=_{} d_{}\), i.e. the average steady-state distance reached by the student versus the cost of actions \(C\) for greedy attacks. The solid lines show the results obtained from simulations3. The yellow shaded area shows \(_{}^{ G}\) form Eq. (13) comprised between values of batch size \(P=1\) (upper contour) and \(P=10\) (lower contour). Note that the three cases show very similar behavior of \(_{}(C,P)\) and are in excellent agreement with the linear TSA prediction.

Figure 3: _Empirical results for the TSA problem with synthetic data_. A, top panel: average steady-state distance vs cost of action \(C\) for the linear TSA problem and greedy attacks. The orange area represents the range of solutions of \(_{}^{ G}\) (13) for \(P\). Bottom panel: average steady-state running cost of different attack strategies relative to the largest one (constant attacks), for \(P=1\), and \(C=1\). B, C: same quantities as shown in A for the perceptron and NN architectures; RL\({}^{*}\) indicates an agent that sees the final layer only. Averages were performed over \(10\) data streams of \(10^{5}\) batches and over the last \(10^{3}\) steps for each stream. Parameters: \(D=10\), \(=0.02 D\), \(a[-2,3]\) (A, B), \(M=100\), \(=0.02 D\), \(a\) (C). Input elements sampled i.i.d. from \(_{x}=(0,1)\).

In order to compare the efficacy of all the attack strategies presented in Sec. 2.4, we consider the _running cost_, defined as \(g_{}=g_{}^{}+g_{}^{}\). More precisely, we compute the steady-state average \(_{}\) over multiple data streams. The bottom row in Fig. 3 shows this quantity for the corresponding architecture of the top row. The evaluation of the clairvoyant attack is missing for the NN architecture, as this approach becomes unfeasible for complex models due to the high non-convexity of the associated minimization problem. Similarly, reinforcement learning attacks are impractical for large architectures. This is because the observation space of the RL agent, given by the input \(x\) and the parameters of the model under attack, becomes too large to handle for neural networks. Therefore, for the NN model we used a TD3 agent that only observes the read-out weights \(v\) of the student network; hence the asterisk in \(^{*}\). We observe that the constant and clairvoyant strategies have respectively the highest and lowest running costs, as expected. Greedy attacks represent the second-best strategy, with an average running cost just above that of clairvoyant attacks. While we do not explicitly show it, it should also be emphasized that the computational costs of the different strategies vary widely; in particular, while greedy attacks are relatively cheap, RL attacks require longer data streams and larger computational resources, since they ignore the learning dynamics. We refer to Fig. 8, 9 in Appendix D for a comparison between greedy and RL action policies, and for a visual representation of the convergence in performance of the RL algorithm vs number of training episodes.

### Experiments on real data

In this section, we explore the behaviour of online data poisoning in an uncontrolled scenario, where input data elements are not i.i.d. and are correlated with the teacher weights. Specifically, we consider two binary image classification tasks, using classes '1' and '7' from MNIST, and 'cats' and 'dogs' from CIFAR10, while attacks are performed using the greedy algorithm. We explore two different setups. First, we experiment on the MNIST task with simple architectures where all parameters are updated during the perturbed training phase. For this purpose, we consider a logistic regression (LogReg) and a simple convolutional neural network, the LeNet of . Then, we address the case of _transfer learning_ for more complex image-recognition architectures, where only the last layer is trained during the attacks, which constitutes a typical application of large pre-trained models. Specifically, we consider the visual geometry group (VGG11)  and residual networks (ResNet18) , with weights trained on the ImageNet-1k classification task. For all architectures, we fix the last linear layer to have 10 input and 1 output features, followed by the Erf activation function.

**Preprocessing**. For LogReg, we reduce the dimensionality of input data to \(10\) via PCA projection. For LeNet, images are resized from 28x28 to 32x32 pixels, while for VGG11 and ResNet18 images are resized from 32x32 to 224x224 pixels. Features are normalized to have zero mean and unit variance, and small random rotations of maximum 10 degrees are applied to each image, thus providing new samples at every step of training.

**Learning**. The training process follows the TSA paradigm: we train a teacher network on the given classification task, and we then use it to provide (soft) labels to a student network with the same architecture. As before, we consider two learning phases: first, the attacker is silent, and it calibrates \(\). Then attacks start and the student receives perturbed data. For consistency with our teacher-student analysis, we consider the SGD optimizer with MSE loss; see Fig. 10 in Appendix D for a comparison between SGD and Adam-based learning dynamics. To keep the VGG11 and ResNet18 models in a regime where online learning is possible, we use the optimizer with positive weight decay.

The findings are presented in Fig. 4, where panels A and B represent the outcomes of the full training and transfer learning experiments, respectively. Each panel consists of two rows: the top row displays the dependence of the relative student-teacher distance on the cost of action \(C\) and batch size \(P\). The orange area depicts \(_{}^{}\) from Eq. (13) for \(P\), which we included for a visual comparison. The bottom row shows the corresponding average steady-state accuracy of the student. In general, all learners exhibit comparable behavior to the synthetic teacher-student setup in terms of the relationship between the average steady-state distance \(_{}\) and \(C\). Additionally, all experiments demonstrate a catastrophic transition in classification accuracy when the value of \(C\) surpasses a critical threshold. The impact of batch size \(P\) on the \(_{}\) is also evident across all experiments. The effect of the weight decay used in the transfer learning setting is also visible: the value of \(_{}\) remains larger than zero even when \(C\) is very large (and attacks are effectively absent). Similarly, for very low values of \(C\), the relative distance remains below 1. We note that calibrating \(\) takes a long time for complex models updating all parameters, so we provide single-run evaluations for LeNet.

## 5 Conclusions

Understanding the robustness of learning algorithms is key to their safe deployment, and the online learning case, which we analyzed here, is of fundamental relevance for any interactive application. In this paper, we explored the case of online attacks that target data labels. Using the teacher-student framework, we derived analytical insights that shed light on the behavior of learning models exposed to such attacks. In particular, we proved that a discontinuous transition in the model's accuracy occurs when the strength of attacks exceeds a critical value, as observed consistently in all of our experiments. Our analysis also revealed that greedy attack strategies can be highly efficient in this context, especially when applying sample-specific perturbations, which are straightforward to implement when data stream in small batches.

A key assumption of our investigation is that the attacker tries to manipulate the long-run behavior of the learner. We note that, for complex architectures, this assumption may represent a computational challenge, as the attacker's action induces a slowdown in the dynamics. Moreover, depending on the context, attacks may be subject to time constraints and therefore involve transient dynamics. While we have not covered such a scenario in our analysis, the optimal control approach that we proposed could be recast in a finite-time horizon setting. We also note that our treatment of online data poisoning did not involve defense mechanisms implemented by the learning model. Designing optimal control attacks that can evade detection of defense strategies represents an important avenue for future research.

Further questions are suggested by our results. While the qualitative features of our theoretical predictions are replicated across all real-data experiments that we performed, we observed differences in robustness between different algorithms. For standard, test-time adversarial attacks, model complexity can aggravate vulnerability [30; 31], and whether this happens in the context of online attacks represents an important open question. Another topic of interest is the interaction of the poisoning dynamics with the structure of the data, and with the complexity of the task. Finally, we remark that our analysis involved sequences of i.i.d. data samples: taking into account temporal correlations and covariate shifts within the data stream constitutes yet another intriguing challenge.

**Compute**. We used a single NVIDIA Quadro RTX 4000 graphics card for all our experiments.

Figure 4: **Empirical results for the TSA problem with real data**. A, top row: average steady-state distance vs cost of action \(C\), for greedy attacks on fully trained logistic regression (LogReg) and LeNet architectures. The orange area represents the range of solutions of \(_{}^{}\) (13) for \(P\). Bottom row: average steady-state accuracy vs \(C\) for the architectures of the top row. B: same quantities as shown in A for transfer learning experiments on VGG11 and ResNet18. Averages were performed over \(10\) data streams of \(10^{5}\) batches and over the last \(10^{3}\) steps for each data stream. Parameters: \(D=10\), \(=0.02 D\) (LogReg, VGG11, ResNet18), \(=0.01\) (LeNet), \(a\).