# Amor: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback

Jian Guan\({}^{1,2}\), Wei Wu\({}^{2}\), Zujie Wen\({}^{2}\), Peng Xu\({}^{2}\), Hongning Wang\({}^{1}\), Minlie Huang\({}^{1}\)

\({}^{1}\)The CoAI group, DCST, Institute for Artificial Intelligence,

\({}^{1}\)State Key Lab of Intelligent Technology and Systems,

\({}^{1}\)Beijing National Research Center for Information Science and Technology,

\({}^{1}\)Tsinghua University, Beijing 100084, China. \({}^{2}\)Ant Group.

{jianguanthu, wuwei19850318,wang.hongn}@gmain.com,

{zujie.wzj,peng.x}@antgroup.com, aihuang@tsinghua.edu.cn.

 Corresponding authors.

###### Abstract

The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present Amor, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. Amor builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop Amor through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets, enabling Amor to generalize across different knowledge environments, while the latter tailors Amor to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of Amor to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism. The code and data are publicly available at https://github.com/JianGuanTHU/AMOR.

## 1 Introduction

LLMs, with astounding performance over general natural language processing (NLP) problems , have spurred great interest in building LLM-based agents to solve complex tasks by interacting with external resources such as web knowledge , specialized tools , etc.

We focus on developing agents for knowledge-intensive tasks, where the agent completes users' information-seeking requests by interacting with specific knowledge bases . To address the complexity of such tasks, we posit the desiderata for a qualifying agent as follows: Firstly, the agent should possess a robust _reasoning logic_ about the task to solve individual problems with precise pathways. Secondly, the agent should maintain an _adaptive mechanism_ to adjust to specific environments, rather than staying static. Thirdly, the reasoning process should be amenable to human interventions, enabling humans to steer the agent's behavior through direct _feedback_ to the process rather than only to the outcome. This ability can significantly facilitate alignment between agent behavior and human intent .

Although extensive studies have been conducted on building language agents, few, if any, can fulfill all the required criteria due to their uncontrollable reasoning logic, static model capability, orsparse/missing feedback signals, as detailed in Table 1. Consequently, it is still challenging for users to critique, and thus guide existing agents to follow targeted manners, especially when the agents are built upon less powerful LLMs .

We introduce **Amor**, an **A**daptable **m**odula**R** knowledge agent that can reason and adapt, with the reasoning process amenable to human supervision, based on open-source LLMs. Amor's reasoning logic is formalized as a finite state machine (FSM) [7; 21] that solves problems via a series of executions and transitions over a set of modules (Figure 1). This naturally enables the desired process-based supervision mechanism, allowing users to give feedback to each LLM-controlled module. Amor supports flexible forms of feedback, either binary judgments regarding the correctness or refinement of the outputs. The reasoning logic and process feedback mechanism together frame how Amor thinks, acts, and interacts with users and task environments.

We build Amor upon an LLM equipped with distinct parameters for different modules to efficiently handle multiple tasks. The training in Amor happens in two stages: **(1) Warm-up:** the modular design enables us to construct training data separately for each disentangled module without requiring complete trajectories for specific tasks. As a result, we create a large dataset of 50k examples covering multiple distinct tasks, simply using public datasets. We fine-tune Amor on this data for generalization over various knowledge-seeking scenarios. **(2) Adaptation:** when deployed, we tailor Amor to the target domain by letting it autonomously address user tasks (i.e., exploration), collecting process feedback for each LLM output, and evolving through further fine-tuning on the exploration trajectories with feedback (i.e., exploitation). Our contributions are summarized as follows:

**I.** We propose a general framework for building knowledge agents, featuring FSM-based reasoning logic and a process feedback mechanism. We focus on text corpora as knowledge bases, but the approach can be flexibly extended to other knowledge types and user tasks by customizing the modules and dependencies within the FSM framework.

    &  &  &  \\    & **Step** & & **Inter-Step Dependency** & \\ 
**WebGPT** & Tool Invoking & _Undefined_ & Imitation Learning from Humans & Outcome \\
**CoT** & Reasoning & _Undefined_ & Prompting & _Undefined_ \\
**ToT** & Reasoning & _Undefined_ & Prompting & _Process_ \\
**ReAct** & Reasoning\&T Invoking & _Undefined_ & Prompting & _Undefined_ \\
**Reflexion** & Reasoning\&T Invoking & _Undefined_ & Prompting & Process \\
**AgemathM** & Reasoning\&T Invoking & _Undefined_ & Imitation Learning from LLMs & Outcome \\
**MetaGPT** & Specialized Module & Sequential Pipeline & Prompting & Process \\
**Lumps** & Specialized Module & Sequential Pipeline & Imitation Learning from Humans & _Undefined_ \\ 
**Amor** & Specialized Module & Finite State Machine & Exploration\&Exploitation & Process \\   

Table 1: Comparison between Amor and representative methods for building agents. Appendix A.1 provides a more comprehensive discussion in detail.

Figure 1: Amor’s state transition diagram. Each box represents a state and the corresponding module that is executed when entering the state. There may be multiple categories of execution results distinguished by special branch tokens such as “[Next].” Then Amor determines the next state based on the branch tokens.

**II.** Experiments across multiple domains show the strong advantage of the FSM-based reasoning logic with \(30\)%-40% improvements over baselines when based on off-the-shelf LLMs (e.g., GPT-42). Switching to fine-tuned LLMs, the warm-up stage empowers Amor to generalize to multiple domains and surpass strong baselines. After we adapt Amor to specific domains, subsequent domain-specific adaptations reveal that process feedback is significantly more effective in improving the reasoning process than outcome feedback.

## 2 Related work

Language agents.Interest is surging in building agents for tasks necessitating multi-step reasoning. Existing work falls into two groups. The first group focuses on designing agent architectures, such as CoT's step-by-step reasoning , ReAct's integration of reasoning, action, and observation to allow tool use , and CodePlan's two-stage reasoning framework that first generates a code-form plan and then realizes low-level reasoning steps . Nevertheless, such free-form reasoning constraints human intervention. In contrast, modular agents follow a pipeline to execute specialized modules [19; 14; 11; 3; 52], improving the ease of intervention. The second group aims to design adaptive mechanisms for adapting agents to specific scenarios. ToT  and Reflexion  use environment feedback for multi-path pruning and iterative single-path refinement, respectively, but suffer from poor inference efficiency and need for real-time feedback. As a fine-tuning approach, recent work equipped open-source LLMs with specific agent abilities by learning from examples synthesized based on human priors , or expert trajectories from humans  or GPT-4 [53; 4] with correctness validation through outcome feedback. In contrast, our modular agent Amor employs FSM-based reasoning with a stronger capacity for handling complex tasks than simple pipelines and adapts effectively to specific environments via process feedback.

Retrieval-augmented generation (RAG).The RAG paradigm augments the inputs of LLMs with retrieved passages to enhance factuality [12; 22; 10]. Recent studies have developed interleaved reasoning-retrieval for better information recall than one-step retrieval [38; 16; 28]. However, retrieval may introduce noise that leads to low-quality answers . To tackle this, Self-RAG  trained LLMs to selectively perform retrieval and utilize retrieved passages. Unlike RAG approaches, Amor emphasizes an explainable reasoning process for proactively decomposing questions and seeking evidence for grounded generation, and allows for process feedback from humans. Nevertheless, RAG mainly focuses on integrating parametric factual knowledge in LLMs and retrieved non-parametric knowledge, which is less explainable and intervenable.

## 3 Amor agent

Amor relies on three key techniques: FSM-based reasoning logic, a process feedback mechanism, and a two-stage fine-tuning strategy. We detail the definition of the reasoning logic and its specification assuming the knowledge base is a text corpus in SS3.1, the method for fine-tuning open-source LLMs as a warm-up stage in SS3.2, and the adaptation stage driven by process feedback in SS3.3.

### Reasoning logic

Algorithm 1 outlines how to deduce the answer \(A\) for an input question \(Q\) with a reasoning process \(R\) using FSM-based reasoning logic, which can be defined by a quadruple: \(\{,,,\}\), where

* \(=\{s_{0},,s_{N-1}\}\) is a set of states with \(s_{0}\) as the initial state and \(s_{N-1}\) as the final state. Each state holds variables to track context information.
* \(=\{m_{0},,m_{N-1}\}\) is a set of modules with \(m_{k}\) triggered when the reasoning flow reaches state \(s_{k}\). The modules are categorized into two types: (a) Tool modules (\(_{}\)) for invoking tools, and (b) LLM modules (\(_{}\)) for calling LLMs.

* \(\) is the set of all possible outputs of \(\).
* \(:\) is the transition function that determines the next state of the reasoning flow given the current state and the execution result of the corresponding module.

When the external knowledge base is a text corpus, an instantiation of the reasoning logic can be represented by the state transition diagram in Figure 1. In this case, \(_{}\) perform document and passage retrieval using external retrievers; while \(_{}\) leverage the LLM to analyze and digest the question, documents, and passages to deduce the final answer. To distinguish different types of outputs from a module that requires different subsequent modules, we employ a set of special branch tokens such as "[Next]" to guide \(\) to determine the next state. In summary, Amor answers question \(Q\) by **(1)** iteratively decomposing \(Q\) to a sub-query \(q\) at state \(s_{0}\), and finding the answer \(a\) to \(q\) and the evidence passage \(e\) through iterative knowledge retrieval, relevance evaluation, retrieval refinement (i.e., "Passage Retrieval"), and answer extraction, until no more knowledge is needed; and **(2)** deducing the final answer \(A\) based on the collected evidence passages at the final state.

Defining reasoning logic as an FSM offers three advantages: **(1) Structured Thinking.** FSM makes specifications of inter-step dependencies (e.g., prioritization, branch selection) easy, and thus enables narrowing down the exploration space. **(2) Skill Disentanglement.** By decomposing complex tasks into modular steps, one can independently construct training data for each module, which significantly reduces the difficulty of implementing Amor with open-source LLMs (cf., SS3.2). This feature also allows Amor to focus on single steps, thereby mitigating the weakness of LLMs in reasoning over long context formed by task-solving trajectories . **(3) Intervenable Workflow.** The structured reasoning process enables users to easily diagnose the agent's mistakes and provide process feedback for improving the reasoning capability of the agent (SS3.3).

### Warming-up open-source LLMs

Open-source LLMs are observed to fall short in complex agent tasks [46; 25]. Recent studies have improved their reasoning abilities through imitation learning using trajectories from advanced LLMs such as GPT-4 [53; 4]. However, even GPT-4 can struggle with producing high-quality reasoning trajectories .

Amor's modular design enables us to construct training data for each module separately from existing datasets without simulating the whole trajectories, thus greatly alleviating the above issue. Formally, given a sample question \(Q\) with annotations of the final answer \(\), all sub-queries and answers \(=[(_{0},_{0}),(_{1},_{1}),]\), and all evidence passages \(=[_{0},_{1},]\), we can directly

Figure 2: On the top left is a sample question from Musique , providing ample information (in **green**) for constructing training examples for four LLM modules of Amor (bottom). We augment extra knowledge (in blue) for the Judge and Answer module by invoking the SearchDoc and SearchPg tools (top right). In each example, we highlight the prompt in purple to format the current state (before “Output:”) and output (after “Output:”), and use “\(||\)” to separate different examples for training.

transform these annotations into a suitable format to serve as training data for Decompose and Complete in Figure 1. Since Judge and Answer require multiple types of retrieved knowledge (e.g., _relevant_ or not), we employ retrieval tools to augment the input. Figure 2 exemplifies the construction pipeline, which can be easily extended to other knowledge-intensive datasets and specific domains. Appendix A.4 shows more details.

When fine-tuning open-source LLMs to handle multiple tasks defined by different modules, we are inspired by the Mixture-of-Experts approach  to learn distinct Feed-Forward Network (FFN) parameters in the final quarter of the Transformer blocks to balance the trade-off between performance and inference efficiency. These module-specific parameters are initialized using the original model's FFN layers. We call the proposed architecture Module-Aware Mixture-of-Experts (MA-MoE)3. Then, we fine-tune the MA-MoE model with the standard language modeling loss:

\[_{1}=-_{m_{},(,) _{m}}_{m}_{_{m}}(|),\] (1)

where \(\) refers to the policy model MA-MoE that maps the state \(\) to an action \(\), \(_{m}\) denotes the parameter for the module \(m_{}\), \(_{m}\) is the corresponding collection of training examples, \((,)\) is a state-output pair from \(_{m}\), and \(\{_{m}\}\) are tunable hyper-parameters.

### Adaptation through process feedback

Feedback is crucial for adapting language agents to specific environments , especially when dealing with unseen, long-tail, or ever-changing domain knowledge. Prior agents commonly used outcome feedback for adaptation which assesses the correctness of intermediate steps based on the success or failure of the outcome [53; 4]. However, outcome feedback is too sparse to improve intermediate reasoning . Recent studies also highlighted that LLMs' reasoning steps are likely to contradict the outcome , which means that outcome feedback may inevitably introduce noise during training (see examples in Appendix B.8). In contrast, Amor's process feedback mechanism can effectively alleviate these issues.

Algorithm 2 describes the adaptation mechanism of Amor parameterized by \(\), specifically as three steps: **(1) Exploration.**A more answers the input question \(Q\) by interacting with a knowledge base. **(2) Feedback Collection.**A more's reasoning process for \(Q\) is evaluated with feedback \(f_{k}\) for the output \(y_{k}\) of the LLM at each step during reasoning, which is either "right/wrong" or a refined version of \(y\). We convert \(y\) into a feedback-refined target output \(\) based on the feedback \(f_{k}\) and determine the immediate reward \(o_{k}\) as follows:

\[_{k},o_{k}=\{y_{k},1&f_{k}=,\\ y_{k},0&f_{k}=,\\ f_{k},1&f_{k}..\] (2)

**(3) Exploitation.** Every \(T\) steps of the former exploration and feedback collection, we optimize the initial policy based on the resulting trajectories and corresponding feedback :

\[_{2}=-_{m_{},(_{k}, _{k},o_{k})_{m}}_{m}[o_{k}- _{_{m}}(_{k}|s_{k})/_{_{m}}^{}(_{k}|s_{k})],\] (3)where \(_{m}\) denotes the training examples for module \(m\), \(_{}^{}\) refers to the initial warm-up policy. Notably, this loss function is non-differentiable, necessitating the use of a specialized optimization technique. We use a recently proposed alignment algorithm KTO  with an MLE regularization  for optimization, which optimizes the policy without requiring paired human preferences. Crucially, when optimizing a particular module \(m\), the gradient induced by the feedback signal propagates through the entire MA-MoE model, except for the FFN layers corresponding to other modules. This targeted optimization approach enables Amor to effectively align its outputs with the desired intermediate results and final answers, leveraging the fine-grained process feedback provided by human supervisors.

## 4 Experiments

### Experimental setup

Tools modules.We construct retrievers for both SearchDoc and SearchPsg using Contiver-MS MARCO . SearchDoc retrieves a single document snippet per query, while SearchPsg fetches the top three relevant passages from a given document. By invoking NextDoc, at most nine more document snippets are returned. Appendix B.1 presents more details.

Warm-up datasets.We employ four question-answering (QA) datasets to warm up open-source LLMs, including 2WikiMultiHopQA , Musique , NaturalQuestions  and BoolQ . They vary in levels of question complexity (single- or multi-hop), answer types (phrase spans or yes/no), and types of dependency structures between sub-queries (e.g., serial or parallel), etc. Appendix A.4 shows the statistics in detail.

Adaptation & evaluation datasets.We consider three benchmarks, by which we simulate different deployment scenarios: **(1) HotpotQA**: a challenging multi-hop QA dataset built on Wikipedia articles. We use the Wikipedia dump provided in  as the knowledge base. **(2) PubMedQA**: a biomedical QA dataset that requires answering a question by "yes/no" given a PubMed abstract. We adapt the data to retrieval-based QA by piling all 274k abstracts provided in the paper as a knowledge base, where each document comprises one abstract passage. **(3) Qasper**: answering questions in free form based on a long NLP paper. For each question, we regard the corresponding paper as a knowledge base and each section of the paper as a document with several passages. We use the training and validation sets for adaptation fine-tuning and the test sets for evaluation. For evaluation metrics, we use exact match (EM) and F1 scores for HotpotQA and Qasper; and the accuracy (ACC) of "yes/no" for PubMedQA. More details are in Appendix B.2.

Feedback annotation.Considering limited resources, we simulate human behavior and provide silver feedback to Amor's reasoning processes based on the gold answer \(\) and gold evidence passages \(=[_{0},_{1},]\) for each target question \(Q\), which are already included in the training and validation data of the three benchmarks. Table 2 shows how we annotate the feedback for each LLM output \(y\). Note that Amor is applicable for gold feedback from humans in realistic applications. Appendix B.3 discusses the accuracy of the silver feedback through human evaluation.

Implementation details.We set \(_{m}\) in Eq. 1 and Eq. 3 to \(1\) for all modules, \(I=1\) in Algorithm 2, and \(T\) to the size of the training set for each dataset, and fine-tune LLAMA-2-7B/13B-Chat for two epochs with a learning rate of \(2^{-5}\) using 8 NVIDIA 80GB A100 GPUs. While applying

  
**Module \(m\)** & **Output \(y\)** & **Silver Process Feedback \(f\)** \\ 
**Decompose\((Q,I)\)** & **[NetNet] \(q\)** & **right**, if the retrieved documents using \(q\) overlap the documents corresponding to \(\); **[Wang]**, otherwise. \\  & **[Finisi]** & **right**, if \( E\) (i.e, evidence passages collected by Autox); **[Wang]**, otherwise. \\ 
**Judge\((Q,I,q,d)\)** & **[RLEVENT]** & **[RLEVENT]**, if one of passages in \(\) comes from the same document as \(d\); **[LRRECEVENT]**, otherwise. \\ 
**Answer\((Q,H,q,P)\)** & **[Answer] \(a\)** & **right**, if \( E\); **[Wang]**, otherwise \\  & **[Unanswerable]** & **right**, if \(P=\); **[Wang]**, otherwise \\ 
**Complete\((Q,E)\)** & \(A\)** & **right**, if \( E\); **[Wang]**, otherwise. \\   

Table 2: Automatic annotation strategy for silver process feedback for different LLM modules.

Amor for inference, we use greedy decoding for all generations. Besides, we set the maximum number of decomposed sub-queries to the maximum count of gold evidence passages, i.e., \(2/1/1\) for HopotQA/PubMedQA/Qasper, respectively. Once the maximum number is reached, Amor is transited to state \(s_{6}\) ("Task Completion") to finalize the answer.

Baselines.We compare Amor to various baselines with or without fine-tuning: **(1) CoT**: it prompts an off-the-shelf LLM to generate the answer through step-by-step reasoning. **(2) RAG**: One-Step Retrieval (OneR) uses the question as a query to retrieve top-\(K\) document snippets with the SearchDoc module to augment the input. We set \(K\) as the maximum number of gold evidence passages in each dataset. Under the fine-tuning setting, we use the gold evidence passages for training. Self-RAG  selectively performs retrieval and utilizes retrieved passages while does not explicitly introduce question decomposition. They can be viewed as simplifications of Amor. **(3) ReAct**: it interleaves thought, action, and observation steps. An action can be either invoking the retrieval tools or finalizing an answer. We also compare Amor with fine-tuned ReAct-style agents including AgentLM  and FireAct. We set the maximum number of action steps to \(20\). **(4) Modular Agents:** ReWoo** follows a pipeline that plans all sub-goals, generates actions, and then executes, while Lumos applies this pipeline iteratively, tackling one sub-goal at a time with each interaction. Both agents utilize GPT-3.5 as a supplementary QA tool during action generation. Similar to Amor, they modularize language agents; however, they lack explicit mechanisms for assessing the relevance of retrieved information. Under the setting without fine-tuning, we provide in-context examples for the baselines following their official implementations.

Furthermore, we also conduct ablation studies to investigate the influence of different components, resulting in two more baselines: **(1) Amor\({}_{}\)**: Amor with only warm-up fine-tuning, without further adaptation; and **(2) Amor\({}_{}\)**: outcome feedback instead of process feedback is utilized in adaptation after Amor is warmed-up. Specifically, we determine the target output and corresponding immediate reward for an LLM module as detailed in Table 3, and then adapt Amor using Eq. 3. For clarity, we denote our final method as Amor\({}_{}\).

### Main results

Table 4 reports the evaluation results of Amor and baselines on three datasets, revealing three key findings: **(1) The FSM paradigm is clearly advantageous to prior agent frameworks.**\(_{}\) delivers strong performance by improving \(41.9\%\), \(32.1\%\), and \(41.2\%\) over ReAct on average when built on top of off-the-shelf LLMs, including L-7B, GPT-3.5, and GPT-4, respectively. This indicates that our proposed FSM paradigm is more effective in leveraging LLMs for complex reasoning. **(2) Warm-up fine-tuning generally enhances Amor in downstream tasks.** When based on L-7B, Amor\({}_{}\) outperforms Amor\({}_{}\) across all datasets. Furthermore, Amor\({}_{}\) also surpasses other fine-tuned ReAct-style and modular agents, even including FireAct that is fine-tuned with in-domain HotpotQA trajectories from GPT-4. This suggests the potential of utilizing existing datasets for developing powerful agents with well-defined reasoning logic. **(3) Process feedback is more effective than outcome feedback in facilitating the adaptation of agents.** The order that Amor\({}_{}>_{}>_{}\) indicates the impact of feedback in terms of tailoring agent behavior to specific domains, and process feedback is more helpful than outcome feedback for leading to the correct final answers.

Additionally, Table 5 presents the results from employing various model architectures (including our proposed MA-MoE model, the standard MoE model, and the Transformer model) as well as optimization algorithms (namely KTO and Supervised Fine-Tuning, i.e., SFT). The MoE model is identical to

  
**Module \(m\)** & **Target Output \(_{k}\) and Immediate Reward \(o_{k}\)** \\ 
**Decompose\((Q,H)\)** & \(_{k} y\) and \(o_{k}=1\) if \(f_{o}=\); \(_{k}\) and \(o_{k}=0\), otherwise. \\
**Judge\((Q,H,q,d)\)** & \(_{k} y\) and \(o_{k}=1\), if \(f_{o}=\); \(_{k}\) and \(o_{k}=1\), otherwise. \\
**Answer\((Q,H,q,P)\)** & \(_{k} y\) and \(o_{k}=1\) if \(f_{o}=\); \(_{k}\) and \(o_{k}=0\), otherwise. \\
**Complete\((Q,E)\)** & \(_{k}\) and \(o_{k}=1\) if \( E\); \(_{k}\) and \(o_{k}=0\), otherwise. \\   

Table 3: Refining each module output \(y\) to \(\) based on the outcome feedback \(f_{o}\) to adapt Amor, where \( y\) denotes converting the binary output \(y\) to its opposite label.

the MA-MoE model except it lacks module-specific awareness. SFT refers to optimizing the model only for outputs \(_{k}\) that receive an immediate reward \(o_{k}=1\), using standard language modeling loss. The results show: (1) The standard MoE architecture struggles to effectively differentiate its experts for multitasking scenarios, leading to performance comparable to the Transformer model. In contrast, the MA-MoE's module-specific awareness enables it to handle diverse tasks within the agent more adeptly. (2) KTO outperforms SFT in aligning the agent's performance with external feedback, owing to its exploitation of negative samples.

### Discussions

The main results have substantiated the benefits of different components of Amor for successfully completing tasks. Nonetheless, we are still curious about four key research questions: **(1) RQ1:**

    &  &  &  &  \\   & & **EM** & **F1** & **ACC** & **EM** & **F1** \\   \\ 
**ReAct** & **L-7B** & 12.2 & 16.6 & 61.8 & 6.0 & 19.2 \\  \(_{}\) & **L-7B** & 26.0 & 34.6 & 62.9 & 4.5 & 21.3 \\ 
**CoT** & **GPT-3.5** & 28.0\({}^{}\) & - & _N/A_ & _N/A_ & _N/A_ \\
**OneR** & **GPT-3.5** & 33.4 & 42.1 & 72.6 & 6.8 & 23.3 \\
**ReAct** & **GPT-3.5** & 30.8 & 38.8 & 58.2 & 5.8 & 27.0 \\
**ReWoo** & **GPT-3.5** & 30.4\({}^{}\) & 40.1\({}^{}\) & - & - & - \\  \(_{}\) & **GPT-3.5** & 39.6 & 49.3 & 68.8 & 10.0 & 30.8 \\ 
**CoT** & **GPT-4** & 45.0\({}^{}\) & - & _N/A_ & _N/A_ & _N/A_ \\
**ReAct** & **GPT-4** & 42.0\({}^{}\) & - & 62.1 & 7.1 & 26.2 \\  \(_{}\) & **GPT-4** & **55.2** & **65.2** & **80.0** & **11.5** & **37.4** \\   \\ 
**OneR\({}^{}\)** & **L-7B** & 34.8 & 43.8 & 75.3 & 11.0 & 25.5 \\ Self-RAG & **L-7B** & 22.4\({}^{}\) & 32.9 & 62.6 & 2.1 & 17.9 \\ AgentLM & **L-7B** & 22.0\({}^{}\) & - & 64.9 & 4.2 & 20.2 \\ FireAct & **L-7B** & 26.2\({}^{}\) & - & 66.1 & 6.5 & 18.4 \\ Lumos & **L-7B** & 29.4\({}^{}\) & - & 70.3 & 7.1 & 19.5 \\  \(_{}\)\({}^{}\)** & **L-7B** & 45.8 & 54.9 & 81.1 & **19.1** & 35.3 \\ \(_{}\) & **L-7B** & 33.6 & 41.9 & 73.4 & 11.1 & 23.6 \\ \(_{}\)\({}^{}\)** & **L-7B** & 40.8 & 49.3 & 77.5 & 9.4 & 25.4 \\  AgentLM & **L-13B** & 29.6\({}^{}\) & - & 67.9 & 7.1 & 24.4 \\ \(_{}\)\({}^{}\)** & **L-13B** & **48.6** & **55.3** & **82.2** & 18.1 & **38.0** \\ \(_{}\) & **L-13B** & 36.8 & 44.1 & 74.6 & 15.2 & 27.3 \\ \(_{}\)\({}^{}\)** & **L-13B** & 42.4 & 51.6 & 80.1 & 9.9 & 26.5 \\   

Table 4: Results of Amor and baselines. “L-7/13B” is short for “LLAMA-2-7/13B-Chat.” We highlight the best results in **bold** and underline the second best. Models marked with \({}^{}\) are fine-tuned on the target datasets. Results marked with \({}^{}\) are reported in the original paper and those marked with \({}^{}\) are reported in . _N/A_ means the method does not apply to the datasets. \(_{}\) outperforms baselines under the same setting significantly (\(p<0.01\), sign test).

    &  &  &  &  \\   & & **EM** & **F1** & **ACC** & **EM** & **F1** \\ 
**MA-MoE\({}^{}\)** & **KTO\({}^{}\)** & **45.8** & **54.9** & **81.1** & **19.1** & **35.3** \\ 
**MA-MoE** & **SFT** & 43.2 & 53.1 & 79.3 & 18.6 & 34.2 \\
**MoE** & **SFT** & 41.6 & 51.1 & 78.8 & 17.5 & 33.5 \\
**Transformer** & **SFT** & 41.4 & 50.9 & 78.2 & 17.8 & 33.2 \\   

Table 5: Results of Amor\({}_{}\) based on L-7B with different architectures and optimization algorithms. The architecture setting is also applied on the warm-up fine-tuning stage. \({}\) refers to our final method.

How do the Amor variants differ in the ability to collect evidence? **(2) RQ2:** Is process feedback more data-efficient than outcome feedback for adaptation? **(3) RQ3:** What if using human feedback for adaptation of Amor? **(4) RQ4:** To what extent does feedback-driven adaptation enhance the Amor's reasoning process? Besides, Appendix B.6 and B.7 further demonstrate the efficient token usage of Amor and the flexibility of Amor's reasoning framework, respectively.

Rq1: Evidence collection comparison.We use recall of gold evidence passages (\(\)) among those collected by Amor (\(E\)) to assess Amor's ability to collect evidence, formally as \(}{\#\{E\}}\).

As shown in Table 6, we observe: **(1)** Warm-up fine-tuning consistently enhances evidence collection, with Amor\({}_{}\) achieving higher recall than Amor\({}_{}\) across all datasets. **(2)** Adaptation through outcome feedback (Amor\({}_{}\)) exerts a negligible impact on the recall results compared with Amor\({}_{}\), suggesting the superiority of Amor\({}_{}\) to Amor\({}_{}\) in final answers (see Table 4) may stem from the improvement of Complete. **(3)** Process feedback is crucial to improve the evidence collection ability, with Amor\({}_{}\) substantially outperforming the other variants.

Rq2: Data efficiency for adaptation.We aim to compare the data efficiency of different feedback types for adaptation in terms of the number of exploratory instances required. To this end, we adjust the exploration steps \(T\) in Algorithm 2, selecting values at intervals of 200, ranging up to 2,000 steps on HotpotQA. Appendix B.5 further discusses the cases with \(I>1\) in Algorithm 2 where Amor is optimized over multiple rounds.

Figure 3 shows the post-adaptation performance of Amor varying with the number of exploratory instances (i.e., \(T\)). Compared to Amor\({}_{}\), Amor\({}_{}\) requires significantly fewer exploration steps to achieve comparable performance. Notably, Amor\({}_{}\) shows a marked decline in performance when exposed to a limited number of exploratory instances (\(<800\)), suggesting a reduced adaptability in exploration-limited scenarios. Conversely, Amor\({}_{}\)'s robust performance under such constraints highlights its superior adaptability and efficiency with minimal interaction.

Rq3: Adaptation through human feedbackDue to limited resources, we use automatically annotated silver feedback as a proxy for gold human feedback in our main experiments. We would like to emphasize that our experimental framework is inherently designed to seamlessly incorporate human feedback in place of its automated counterpart. To illustrate this, we carry out a human study to demonstrate how Amor is adapted through human feedback on HotpotQA. For this study, we hire an NLP expert to provide human feedback for each module within Amor\({}_{}\) on 2,000 exploratory

  
**Method** & **Base LLM** & **HotpotQA** & **PubMedQA** & **Qasper** \\ 
**OneR** & **N/A** & 31.1 & 67.6 & 24.9 \\  \(_{}\) & **L-7B** & 24.1 & 54.2 & 24.3 \\  \(_{}\) & **L-7B** & 53.5 & 79.8 & 41.9 \\ \(_{}\) & **L-7B** & 41.1 & 69.2 & 27.5 \\ \(_{}\) & **L-7B** & 40.2 & 70.0 & 27.5 \\  \(_{}\) & **L-13B** & **53.7** & **80.5** & **42.4** \\ \(_{}\) & **L-13B** & 43.0 & 69.4 & 27.0 \\ \(_{}\) & **L-13B** & 41.5 & 68.1 & 27.7 \\   

Table 6: Recall scores under different settings.

Figure 3: EM/F1 on HotpotQA varying with the number of exploratory instances for adaptation.

instances following the annotation strategy in Appendix B.3. Table 7 shows the adaptation results using the collected human feedback.

The results distinctly suggest: human feedback more effectively adapts Amor to specific knowledge environments than automatic feedback. Our study lays a robust groundwork for the practical deployment and real-world utilization of the Amor framework.

RQ4: Reasoning process assessment.To measure the accuracy of Amor's reasoning process, we performed a human study on HotpotQA, which involved: (1) selecting 50 random questions; (2) manually annotating the gold feedback \(f_{}\) for each LLM module output the following instructions using the same annotation protocol in Appendix B.3; and (3) calculating the accuracy of each LLM module output based on \(f_{}\) (1/0 indicating "right/wrong").

Table 8 presents the accuracy of Amor variants, affirming RQ1's findings: process feedback significantly improves the reasoning process over Amor\({}_{}\) that lacks adaptation, while outcome feedback has a negligible effect. Moreover, Amor\({}_{}\) relatively lags in the Decompose and Complete modules, hinting that future enhancements could focus on including more corresponding data during two fine-tuning stages.

### Case study

Appendix B.8 presents several examples to further illustrate Amor's strengths in reasoning logic and intervenability, as well as the limitations of relying on outcome feedback for adaptation, emphasizing the crucial role of process feedback.

## 5 Conclusion

In this work, we develop Amor, an adaptable modular agent designed for knowledge-intensive tasks, featuring FSM-based reasoning logic and a process feedback mechanism. Based on open-source LLMs, Amor undergoes a two-stage fine-tuning: initial warm-up to generalize across task environments and subsequent domain-specific adaptation through process feedback. Extensive experiments demonstrate Amor's advantages over strong baselines across multiple domains. Further discussions highlight the effectiveness and efficiency of process feedback in adaptation. compared to previous agents. Future work will explore extending our paradigm to more knowledge types (e.g., structured knowledge bases) and broader agent tasks, ultimately empowering LLMs to autonomously design FSM-based reasoning logic on top of our paradigm.

## 6 Acknowledgements

We thank the anonymous reviewers and area chairs for their valuable feedback and insightful comments that helped improve this work. This work was supported by the NSFC projects(Key project with No. 61936010). This work was supported by the National Science Foundation for Distinguished Young Scholars (with No. 62125604).

  
**Agents** & **Feedback Type** & **EM** & **F1** \\  \(_{}\) & Automatic Feedback & 45.8 & 54.9 \\ \(_{}\) & Human Feedback & 50.8 & 59.2 \\   

Table 7: Adaptation results of Amor through human feedback on HotpotQA based on L-7B.

  
**Method** & **Decompose** & **Judge** & **Answer** & **Complete** \\  \(_{}\) & **73.0** & **97.2** & **82.5** & **50.0** \\ \(_{}\) & 59.5 & 95.3 & 77.2 & 32.0 \\ \(_{}\) & 61.2 & 96.0 & 75.1 & 44.0 \\   

Table 8: Accuracy of four LLM modules. All Amor variants are based on L-7B.