# Transfer Learning for Latent Variable Network Models

Akhil Jalan

Department of Computer Science

UT Austin

akhiljalan@utexas.edu

&Arya Mazumdar

Halicioglu Data Science Institute & Dept of CSE

UC San Diego

arya@ucsd.edu

Soumendu Sundar Mukherjee

Statistics and Mathematics Unit (SMU)

Indian Statistical Institute, Kolkata

ssmukherjee@isical.ac.in

&Purnamrita Sarkar

Department of Statistics and Data Sciences

UT Austin

purna.sarkar@austin.utexas.edu

###### Abstract

We study transfer learning for estimation in latent variable network models. In our setting, the conditional edge probability matrices given the latent variables are represented by \(P\) for the source and \(Q\) for the target. We wish to estimate \(Q\) given two kinds of data: (1) edge data from a subgraph induced by an \(o(1)\) fraction of the nodes of \(Q\), and (2) edge data from all of \(P\). If the source \(P\) has no relation to the target \(Q\), the estimation error must be \((1)\). However, we show that if the latent variables are shared, then vanishing error is possible. We give an efficient algorithm that utilizes the ordering of a suitably defined graph distance. Our algorithm achieves \(o(1)\) error and does not assume a parametric form on the source or target networks. Next, for the specific case of Stochastic Block Models we prove a minimax lower bound and show that a simple algorithm achieves this rate. Finally, we empirically demonstrate our algorithm's use on real-world and simulated network estimation problems.

## 1 Introduction

Within machine learning and statistics, the paradigm of _transfer learning_ describes a setup where data from a source distribution \(P\) is exploited to improve estimation of a target distribution \(Q\) for which a small amount of data is available. Transfer learning is quite well-studied in learning theory, starting with works such as Ben-David et al. (2006); Cortes et al. (2008); Crammer et al. (2008), and at the same time has found applications in areas such as computer vision (Tzeng et al., 2017) and speech recognition (Huang et al., 2013). A fairly large body of work in transfer learning considers different types of relations that may exist between \(P\) and \(Q\), for example, Mansour et al. (2009); Hanneke and Kpotufe (2019, 2022), with emphasis on model selection, multitask learning and domain adaptation. On the other hand, optimal nonparametric rates for transfer learning have very recently been studied, both for regression and classification problems (Cai and Wei, 2021; Cai and Pu, 2024).

In this paper, we study transfer learning in the context of _random network/graph models_. In our setting, we observe Bernoulli samples from the full \(n n\) edge probability matrix for the source \(P\) and only a \(n_{Q} n_{Q}\) submatrix of \(Q\) for \(n_{Q} n\). We would like to estimate the full \(n n\) probability matrix \(Q\), using the full source data and limited target data, i.e., we are interested in the task of estimating \(Q\) in the partially observed target network, utilizing information from the fully observed source network. This is a natural extension of the transfer learning problem in classification/regressionto a network context. However, it is to be noted that network transfer is a genuinely different problem owing to the presence of edge correlations.

While transfer learning in graphs seems to be a fundamental enough problem to warrant attention by itself, we are also motivated by potential applications. For example, metabolic networks model the chemical interactions related to the release and utilization of energy within an organism (Christensen and Nielsen, 2000). Existing algorithms for metabolic network estimation (Sen et al., 2018; Baranwal et al., 2020) and biological network estimation more broadly (Fan et al., 2019; Li et al., 2022) typically assume that some edges are observed for every node in the target network. One exception is Kshirsagar et al. (2013), who leverage side information for host-pathogen protein interaction networks. For the case of metabolic networks, determining interactions _in vivo_1 requires metabolite balancing and labeling experiments, so only the edges whose endpoints are _both_ incident to the experimentally chosen metabolites are observed (Christensen and Nielsen, 2000). For a non-model organism, the experimentally tested metabolites may be a small fraction of all metabolites believed to affect metabolism. However, data for a larger set of metabolites might be available for a model organism.

To study transfer learning on networks, one needs to fix a general enough class of networks that is appropriate for the applications (such as the biological networks mentioned above) and also suitable to capture the transfer phenomenon. The latent variable models defined below appear to be a natural candidate for that.

**Latent Variable Models.** Latent variable network models consist of a large class of models whose edge probabilities are governed by the latent positions of nodes. This includes latent distance models, stochastic block models, random dot product graphs and mixed membership block models (Hoff et al., 2002; Hoff, 2007; Handcock et al., 2007; Holland et al., 1983; Rubin-Delanchy et al., 2022; Airoldi et al., 2008). They can also be unified under graph limits or graphons (Lovasz, 2012; Bickel and Chen, 2009), which provide a natural representation of vertex exchangeable graphs (Aldous, 1981; Hoover, 1979). In addition to their theoretical breadth and usefulness, latent variable models are relevant and applicable to real-world settings such as neuroscience Ren et al. (2023), ecology Trifonova et al. (2015), international relations Cao and Ward (2014), political psychology Barbera et al. (2015), and education research Sweet et al. (2013).

For unseen latent variables \(_{1},,_{n}^{d}\) and unknown function \(f_{Q}:\) where \(\) is a compact set and \(d\) an arbitrary fixed dimension, the edge probabilities are:

\[Q_{ij}=f_{Q}(_{i},_{j}).\] (1)

Typically, in network estimation, one observes adjacency matrix \(\{A_{ij}\}\) distributed as \(\{(Q_{ij})\}\), and either has to learn \(_{i}\) or directly estimate \(f_{Q}\). There has been much work in the statistics community on estimating \(_{i}\) for specific models (usually up to rotation). For stochastic block models, see the excellent survey in Abbe (2017).

Estimating \(f_{Q}\) can be done with some additional assumptions (Chatterjee, 2015). When \(f_{Q}\) has appropriate smoothness properties, one can estimate it by a histogram approximation (Olhede and Wolfe, 2014; Chan and Airoldi, 2014). This setting has also been compared to nonparametric regression with an unknown design (Gao et al., 2015). Methods for network estimation include Universal Singular Value Thresholding (Chatterjee, 2015; Xu, 2018), combinatorial optimization (Gao et al., 2015; Klopp et al., 2017), and neighborhood smoothing (Zhang et al., 2017; Mukherjee and Chakrabarti, 2019).

**Transfer Learning on Networks.** We wish to estimate the target network \(Q\). However, we only observe \(f_{Q}\) on \(}{2}\) pairs of nodes, for a uniformly random subset of variables \(S\{1,2,,n\}\). We assume \(S\) is vanishingly small, so \(n_{Q}:=|S|=o(n)\).

Absent additional information, we cannot hope to achieve \(o(1)\) mean-squared error. To see this, suppose \(f_{Q}\) is a stochastic block model with 2 communities of equal size. For a node \(i S\), no edges incident to \(i\) are observed, so its community cannot be learned. Since \(n_{Q} n\), we will attain \((1)\) error overall. To attain error \(o(1)\), we hope to leverage transfer learning from a source \(P\) if available. In fact, we give an efficient algorithm to achieve \(o(1)\) error, formally stated in Section 2.

**Theorem 1.1** (Theorem 2.3, Informal).: _There exists an efficient algorithm such that, if given source data \(A_{P}\{0,1\}^{n n}\) and target data \(A_{Q}\{0,1\}^{n_{Q} n_{Q}}\) coming from an appropriate pair \((f_{P},f_{Q})\)of latent variable models, outputs \(^{n n}\) such that_

\[[}\|Q-\|_{F}^{2} o(1)] 1- o(1).\]

There must be a relationship between P and Q for them to be an _appropriate_ pair for transfer learning. We formalize this relationship below.

**Relationship Between Source and Target.** It is natural to consider pairs \((f_{P},f_{Q})\) such that for all \(,\), the difference \((f_{P}(,)-f_{Q}(,))\) is small. For example, Cai and Pu (2024) study transfer learning for nonparametric regression when \(f_{P}-f_{Q}\) is close to a polynomial in \(,\). But, requiring \(f_{P}-f_{Q}\) to be pointwise small does not capture a broad class of pairs in the network setting. For example, if \(f_{P}= f_{Q}\). Then \(f_{P}-f_{Q}=(-1)f_{Q}\) can be far from all polynomials if \(f_{Q}\) is, e.g. a Holder-smooth graphon.2 However, under the network model, this means \(A_{P}\) and \(A_{Q}\) are stochastically identical modulo one being \(\) times denser than the other.

We will therefore consider pairs \((f_{P},f_{Q})\) that are close in some measure of local graph structure. With this in mind, we use a graph distance introduced in Mao et al. (2021) for a different inference problem.

**Definition 1.2** (Graph Distance).: _Let \(P^{n n}\) be the probability matrix of a graph. For \(i,j[n],i j\), we define the graph distance between them as follows:_

\[d_{P}(i,j):=\|(_{i}-_{j})^{T}P^{2}(I-_{i}_{i}^{T}-_{j}_{j}^{T})\|_{2}^{2},\]

_where \(_{i},_{j}^{n}\) are standard basis vectors._

Intuitively, this first computes the matrix \(P^{2}\) of common neighbors, and then computes the distance between two rows of the same (ignoring the diagonal elements). We will require that \(f_{P},f_{Q}\) satisfy a local similarity condition on the relative rankings of nodes with respect to this graph distance. Since we only estimate the probability matrix of \(Q\), the condition is on the latent variables \(_{1},,_{n}\) of interest. The hope is that the proximity in graph distance reflects the proximity in latent positions.

**Definition 1.3** (Rankings Assumption at Quantile \(h_{n}\)).: _Let \((P,Q)\) be a pair of graphs evaluated on \(n\) latent positions. We say \((P,Q)\) satisfy the rankings assumption at quantile \(h_{n} 1\) if there exists constant \(C>0\) such that for all \(i[n]\) and all \(j i\), if \(j\) belongs to the bottom \(h_{n}\)-quantile of \(d_{P}(i,)\), then \(j\) belongs to the bottom \(Ch_{n}\)-quantile of \(d_{Q}(i,)\)._

To further motivate Definition 1.3, recall our motivating example of biological network estimation. Previous works require some form of similarity between networks to enable transfer Sen et al. (2018); Fan et al. (2019); Baranwal et al. (2020). For example, Kshirsagar et al. (2013) require a _commonality hypothesis_: if pathogens A, B target the same neighborhoods in a protein interaction network, one can transfer from A to B. Our rankings assumption similarly posits that to transfer knowledge from A to B, A and B have similar 2-hop neighborhood structures.

Note that Definition 1.3 involves quantiles of graph distances; therefore it is a _relative_ condition, because it depends on a rank-ordering within both graphs \(P,Q\) before comparison. On the other hand, an _absolute_ condition would require that for nodes \(i,j[n]\), if e.g. \(d_{P}(i,j)<100\) then \(d_{Q}(i,j)<C 100\). Our condition is more flexible and will hold for a larger set of graph pairs \((P,Q)\), such as pairs where one graph is much more dense than the other.

Finally, to illustrate Definition 1.3, consider stochastic block models \(f_{P},f_{Q}\) with \(k_{P} k_{Q}\) communities respectively. If nodes \(i,j\) are in the same communities then \(P_{i}=P_{j}\), so \(d_{P}(i,j)=0\). We require that \(j\) minimizes \(d_{Q}(i,)\). This occurs if and only if \(d_{Q}(i,j)=0\). Hence if \(i,j\) belong to the same community in \(P\), they are in the same community in \(Q\). Note that the converse is not necessary; we could have \(Q\) with \(1\) community and \(P\) with arbitrarily many communities.

With the relationship between the source and target defined by the rankings assumption, our contributions are as follows.

**(1) Algorithm for Latent Variable Models.** We provide an efficient Algorithm 1 for latent variable models with Holder-smooth \(f_{P},f_{Q}\). The benefit of this algorithm is that it does not assume a parametric form of \(f_{P}\) and \(f_{Q}\). We prove a guarantee on its error in Theorem 2.3.

**(2) Minimax Rates.** We prove a minimax lower bound for Stochastic Block Models (SBMs) in Theorem 3.2. Moreover, we provide a simple Algorithm 2 that attains the minimax rate for this class (Proposition 3.4).

**(3) Experimental Results on Real-World Data.** We test both of our algorithms on real-world metabolic networks and dynamic email networks, as well as synthetic data (Section 4).

All proofs are deferred to the Appendix.

### Other Related work

Transfer learning has recently drawn a lot of interest both in applied and theoretical communities. The notion of transferring knowledge from one domain with a lot of data to another with less available data has seen applications in epidemiology Apostolopoulos and Bessiana (2020), computer vision Long et al. (2015); Tzeng et al. (2017); Huh et al. (2016); Donahue et al. (2014); Neyshabur et al. (2020), natural language processing Daume (2007), etc. For a comprehensive survey see Zhuang et al. (2019); Weiss et al. (2016); Kim et al. (2022). Recently, there have also been advances in the theory of transfer learning Yang et al. (2013); Tripuraneni et al. (2020); Agarwal et al. (2023); Cai and Wei (2021); Cai and Pu (2024); Cody and Beling (2023).

In the context of networks, transfer learning is particularly useful since labeled data is typically hard to obtain. Tang et al. (2016) develop an algorithmic framework to transfer knowledge obtained using available labeled connections from a source network to do link prediction in a target network. Lee et al. (2017) proposes a deep learning framework for graph-structured data that incorporates transfer learning. They transfer geometric information from the source domain to enhance performance on related tasks in a target domain without the need for extensive new data or model training. The SGDA method Qiao et al. (2023) introduce adaptive shift parameters to mitigate domain shifts and propose pseudo-labeling of unlabeled nodes to alleviate label scarcity. Zou et al. (2021) proposes to transfer features from the previous network to the next one in the dynamic community detection problem. Simchowitz et al. (2023) work on combinatorial distribution shift for matrix completion, where only some rows and columns are given. A similar setting is used for link prediction in egocentrically sampled networks in Wu et al. (2018). Zhu et al. (2021) train a graph neural network for transfer based on an ego-graph-based loss function. Learning from observations of the full network and additional information from a game played on the network Leng et al. (2020); Rossi et al. (2022). Wu et al. (2024) study graph transfer learning for node regression in the Gaussian process setting, where the source and target networks are fully observed.

Levin et al. (2022) proposes an inference method from multiple networks all with the same mean but different variances. While our work is related, we do not assume \([P_{ij}]=[Q_{ij}]\). Cao et al. (2010) do joint link prediction on a collection of networks with the same link function but different parameters.

Another line of related but different work deals with multiplex networks (Lee et al., 2014, 2015; Iacovacci and Bianconi, 2016; Cozzo et al., 2018) and dynamic networks Sarkar and Moore (2005); Kim et al. (2018); Sewell and Chen (2015); Sarkar et al. (2012); Chang et al. (2024); Wang et al. (2023). One can think of transfer learning in clustering as clustering with side information. Prior works consider stochastic block models with noisy label information (Mossel and Xu, 2016; Mazumdar and Saha, 2017) or oracle access to the latent structure (Mazumdar and Saha, 2017).

**Notation.** We use lowercase letters \(a,b,c\) to denote (real) scalars, boldface \(,,\) to denote vectors, and uppercase \(A,B,C\) to denote matrices. Let \(a b:=\{a,b\}\) and \(a b:=\{a,b\}\). For integer \(n>0\), let \([n]:=\{1,2,,n\}\). For a subset \(S[n]\) and \(A^{n n}\), let \(A[S,S]^{|S||S|}\) be the principal submatrix with row and column indices in \(S\). We denote the \(_{2}\) vector norm as \(\|\|=\|\|_{2}\), dot product as \(,\), and Frobenius norm as \(\|A\|=\|A\|_{F}\). For functions \(f,g:\) we let \(f g\) denote \(f=O(g)\) and \(f g\) denote \(f=(g)\). All asymptotics \(O(),o(),(),()\) are with respect to \(n_{Q}\) unless specified otherwise.

## 2 Estimating Latent Variable Models with Rankings

In this section, we present a computationally efficient transfer learning algorithm for latent variable models. Algorithm 1 learns the local structure of \(P\) based on graph distances (Definition 1.2). Foreach node \(i\) of \(P\), it ranks the nodes in \(S\) with respect to the graph distance \(d_{P}(i,)\). For most nodes \(i,j[n]\), none of the edges incident to \(i\) or \(j\) are observed in \(Q\). Therefore, we estimate \(_{ij}\) by using the edge information about nodes \(r,s S\) such that \(d_{P}(i,r)\) and \(d_{P}(j,s)\) are small.

Formally, we consider a model as in Eq. (1) with a compact latent space \(^{d}\) and latent variables sampled i.i.d. from the normalized Lebesgue measure on \(\). We set \(=^{d}\) without loss of generality and assume that functions \(f:\) are \(\)-Holder-smooth.

**Definition 2.1**.: _Let \(f:\) and \(>0\). We say \(f\) is \(\)-Holder-smooth if there exists \(C_{}>0\) such that for all \(,^{},\),_

\[_{^{d}:_{i}_{i}=}| _{i}}f}{_{x_{1}}^{_{1}} _{x_{d}}^{_{d}}}(,)-_ {i}}f}{_{x_{1}}^{_{1}}_{x_{d}}^{_{d}}}(^{},)| C_{}\|-^{}\|_{2}^{  1}.\]

To exclude degenerate cases where a node may not have enough neighbors in latent space, we require the following assumption.

**Assumption 2.2** (Assumption 3.2 of Mao et al. (2021)).: _Let \(G\) be a graph on \(_{1},,_{n}\). There exist \(c_{2}>c_{1}>0\) and \(_{n}=o(1)\) such that for all \(_{i},_{j}\),_

\[c_{1}\|_{i}-_{j}\|^{ 1}-_{n}}d _{G}(i,j) c_{2}\|_{i}-_{j}\|^{ 1}.\]

The second inequality follows directly from Holder-smoothness, and the first is shown to hold for e.g. Generalized Random Dot Product Graphs, among others (Mao et al., 2021).

We establish the rate of estimation for Algorithm 1 below.

**Theorem 2.3**.: _Let \(\) be as in Algorithm 1. Let \(f_{P}\) be \(\)-Holder-smooth and \(f_{Q}\) be \(\)-Holder-smooth for \(>0\), and let \(c\) be an absolute constant. Suppose \((P,Q)\) satisfy Definition 1.3 at \(h_{n}=c}{n_{Q}}}\) and \(P\) satisfies Assumption 2.2 with \(_{n}=O((})^{})\). Then there exists an absolute constant \(C>0\) such that_

\[[}\|-Q\|_{F}^{2}()^{}(})^{}] 1-n_{Q}^{-C}.\]To parse Theorem 2.3, consider the effect of various parameter choices. First, observe that our upper bound scales quite slowly with \(n\). Even if \(n\) is superpolynomial in \(n_{Q}\), e.g. \(n=n_{Q}^{ n_{Q}}\), then \( n=O(( n_{Q})^{2})=n_{Q}^{o(1)}\), so the overall effect on the error is dominated by the \(n_{Q}\) term.

Second, the bound is worse in large dimensions, and scales exponentially in \(\). This kind of scaling also occurs in minimax lower bounds for nonparametric regression (Tsybakov, 2009), and upper bounds for smooth graphon estimation (Xu, 2018). However, we caution that nonparametric regression can be quite different from network estimation; it would be very interesting to know the dependence of dimension on minimax lower bounds for network estimation, but to the best of our knowledge this is an open problem. Finally notice that a greater smoothness \(\) results in a smaller error, up to \(=1\), exactly as in (Gao et al., 2015; Klopp et al., 2017; Xu, 2018).

## 3 Minimax Rates for Stochastic Block Models

In this section, we will show matching lower and upper bounds for a very structured class of latent variable models, namely, Stochastic Block Models (SBMs).

**Definition 3.1** (Sbm).: _Let \(P^{n n}\). We say \(P\) is an \((n,k)\)-SBM if there exist \(B^{k k}\) and \(z:[n][k]\) such that for all \(i,j\), \(P_{ij}=B_{z(i)z(j)}\). We refer to \(z^{-1}(\{j\})\) as community \(j[k]\)._

We first state a minimax lower bound, proved via Fano's method.

**Theorem 3.2** (Minimax Lower Bound for SBMs).: _Let \(k_{P} k_{Q} 1\) with \(k_{Q}\) dividing \(k_{P}\). Let \(\) be the family of pairs \((P,Q)\) where \(P\) is an \((n,k_{P})\)-SBM, \(Q\) is an \((n,k_{Q})\)-SBM, and \((P,Q)\) satisfy Definition 1.3 at \(h_{n}=1/k_{P}\). Moreover, suppose \(S[n]\) is restricted to contain an equal number of nodes from communities \(1,2,,k_{P}\) of \(P\). Then the minimax rate of estimation is:_

\[_{^{n n}}_{(P,Q)} [}\|-Q\|_{F}^{2}]^{2}}{ n_{Q}^{2}}.\]

Note that Definition 1.3 at \(h_{n}=1/k_{P}\) implies that the true community structure of \(Q\) coarsens that of \(P\). The condition that \(k_{Q}\) divides \(k_{P}\) is merely a technical one that we assume for simplicity.

We remark that minimax lower bounds for smooth graphon estimation are established by first showing lower bounds for SBMs, and then constructing a graphon with the same block structure using smooth mollifiers (Gao et al., 2015). Therefore, we expect that Theorem 3.2 can also be extended to the graphon setting, using the same techniques. However, sharp lower bounds for other classes such as Random Dot Product Graphs will likely require different techniques (Xie and Xu, 2020; Yan and Levin, 2023).

**Remark 3.3** (Clustering Regime).: _In Appendix A.4 we also prove a minimax lower bound of \(}{n_{Q}}\) in the regime where the error of recovering the true clustering \(z\) dominates. This matches the rate of Gao et al. (2015), but for estimating all \(n^{2}\) entries of \(Q\), rather than just the \(n_{Q}^{2}\) observed entries._

Theorem 3.2 suggests that a very simple algorithm might achieve the minimax rate. Namely, use both \(A_{P},A_{Q}\) to learn communities, and then use only \(A_{Q}\) to learn inter-community edge probabilities. If \((P,Q)\) are in the nonparametric regime where regression error dominates clustering error (called the _weak consistency_ or _almost exact recovery_ regime), then the overall error will hopefully match the minimax rate.

We formalize this approach in Algorithm 2, and prove that it does achieve the minimax error rate in the weak consistency regime. To this end, we define the signal-to-noise ratio of an SBM with parameter \(B^{k k}\) as follows:

\[s:=},\]

where \(p=_{i}B_{ii},q=_{i j}B_{ij}\).

**Proposition 3.4** (Error Rate of Algorithm 2).: _Suppose \(P,Q^{n n}\) are \((n,k_{P}),(n,k_{Q})\)-SBMs with minimum community sizes \(n_{}^{(P)},n_{}^{(Q)}\) respectively. Suppose also that \((P,Q)\) satisfy 

**Definition 1.3**: _at \(h_{n}=n_{}^{(P)}/n\). Then if the signal-to-noise ratios are such that: \(s_{P} C(}{n_{}^{(P)}}(n)}{ ^{(P)}}})\) and \(s_{Q} C(}}{n_{}(Q)}(n_{Q})}{^{(Q)}}})\) for large enough constant \(C>0\), Algorithm 2 returns \(\) such that_

\[[}\|-Q\|_{F}^{2} ^{2}(n_{}^{(Q)})}{n_{Q}^{2}}] 1-O }.\]

## 4 Experiments

In this section, we test Algorithm 1 against several classes of simulated and real-world networks. We use quantile cutoff of \(h_{n}=}{n_{Q}}}\) for Algorithm 1 in all experiments.

**Baselines.** To the best of our knowledge, our exact transfer formulation has not been considered before in the literature. Therefore, we implement two algorithms as alternatives to Algorithm 1.

_(1) Algorithm 2_. Given \(A_{P}\{0,1\}^{n n},A_{Q}\{0,1\}^{n_{Q} n_{Q}}\), let \(k_{P}=,k_{Q}=}\). Compute spectral clusterings \(_{P},_{Q}\) with \(k_{P},k_{Q}\) clusters respectively. Let \(J_{S}\{0,1\}^{n_{Q} n}\) is such that \(J_{S;ij}=1\) if and only if \(i=j\) and \(i S\). The projection \(^{k_{P} k_{Q}}\) solves the least-squares problem \(_{^{k_{P} k_{Q}}}\|J_{S}_{P} -_{Q}\|_{F}^{2}\). We compute the \(\) differently from steps 4-7 in Algorithm 2 to account for cases where \(Q\) is not a true coarsening of \(P\). When \(Q\) is a true coarsening of \(P\), this reduces to the procedure in steps 4-7. Given \(_{P},\) we return \(\) as in Algorithm 2.

_(2) Oracle._ Suppose that an oracle can access data for \(Q\) on _all_\(n n_{Q}\) nodes as follows. Fix an error probability \(p(0,1)\). The oracle is given symmetric \(A_{Q}^{}\{0,1\}^{n n}\) with independent entries following a mixture distribution. For all \(i,j[n]\) with \(i<j\) let \(X_{ij}(p)\) and \(Y_{ij}(Q(_{i},_{j}))\). Then:

\[A_{Q;ij}^{}=_{i S,j S}Y_{ij}+(1-_{i S,j S })((1-X_{ij})Y_{ij}+X_{ij}(1-Y_{ij})).\]

Given \(A_{Q}^{}\), the oracle returns the estimate from Universal Value Thresholding on \(A_{Q}^{}\) Chatterjee (2015). As \(p 0\), the error will approach \(O(n^{})\) for a \(\)-smooth network on on \(d\)-dimensional latent variables (Xu, 2018), so the oracle will outperform any transfer algorithm.

**Simulations.** We first test on several classes of simulated networks. For \(n_{Q}=50,n=200\), we run \(50\) independent trials for each setting. We report results for each setting in Table 1, and visualize estimates for stylized examples in Figure 1.

At a glance, Figure 1 shows that Algorithms 1 and 2 both work well on Stochastic Block Models (first row), that only Algorithm 1 works well on graphons (second and third rows), and that the Oracle performs well in all cases.

_Smooth Graphons._ The latent space is \(=\). We consider graphons of the form \(f_{}(x,y)=+y^{}}{2}\) where \(P,Q\) have different \(\). We denote this the \(\)-Smooth Graphon.

_Mixed-Membership Stochastic Block Model._ Set \(k_{P}=,k_{Q}=}\). The latent space \(\) is the probability simplex \(=_{k_{P}}:=\{x^{k_{P}}:_{i}x_{i}=1\}^{k_{P}}\). The latent variables \(_{1},,_{n}\) are iid-Dirichlet distributed with equal weights \(},,}\). Then \(P_{ij}=_{i}^{T}B_{P}_{j}\) and \(Q_{ij}=(_{i})^{T}B_{Q}(_{j})\), for connectivity matrices \(B_{P}^{k_{P} k_{P}},B_{Q}^{k_{Q} k_{Q}}\), and projection \(:_{k_{P}}_{k_{Q}}\) for a fixed subset of \([k_{P}]\). For parameters \(a,b,\) we generate \(B^{k k}\) by sampling \(E(-,)^{k k}\) and set \(B=((a-b)I+b^{T}+E,0,1)\). We call this Noisy-MMSB\((a,b,)\).

_Latent Distance Model._ The latent space is the unit sphere \(=^{d-1}^{d}\). For scale parameter \(s>0\), we call \(f_{s}(,)=(-s\|-\|_{2})\) the \(^{d}\)-Latent\((s)\) model.

**Discussion.** When the latent dimension is larger than \(1\) (the Noisy MMSB and Latent Variable Models), our Algorithm 1 is better than both Algorithm 2 and the Oracle with \(p=0.1\). Note that Algorithms 1 and 2 use \(^{2}}{n^{2}} 0.06\) unbiased edge observations from \(Q\), while the Oracle with \(p=0.1\) observes \((1-p)-n_{Q}^{2}}{n^{2}} 0.9\) unbiased edge observations in expectation.

**Real-World Data.** Next, we test on two classes of real-world networks. We summarize our dataset characteristics in Table 2. See Appendix C for further details.

**Transfer Across Species in Metabolic Networks.** For a fixed organism, a metabolic network has a node for each metabolite, and an edge exists if and only if two metabolites co-occur in a metabolic

   Source & Target & Alg. 1 & Alg. 2 & Oracle & Oracle & Oracle \\  & & & & (\(p=0.1\)) & (\(p=0.3\)) & (\(p=0.5\)) \\  Noisy- & Noisy- & **0.7473\(\)** & \(1.3761\ \) & _0.9556 \(\)_ & \(2.2568\ \) & \(4.2212\ \) \\ MMSB & MMSB & **0.0648** & \(1.1586\) & _0.0633_ & \(0.3107\) & \(0.2825\) \\ \((0.7,0.3,0.01)\) & \((0.9,0.1,0.01)\) & & & & \\ \(0.1\)-Smooth & \(0.5\)-Smooth & _1.7656 \(\)_ & \(4.5033\ \) & \(4.5033\ \) & **0.5016 \(\)** & \(2.4423\ \) & \(5.7774\ \) \\ Graphon & Graphon & _0.7494_ & \(1.5613\) & **0.0562** & \(0.4574\) & \(0.7126\) \\ \(^{10}\) & \(^{10}\) & **0.5744 \(\)** & \(1.1773\ \) & _0.7715 \(\)_ & \(2.1822\ \) & \(4.3335\ \) \\ Latent\((2.5)\) & Latent\((1.0)\) & **0.1086** & \(1.0481\) & _0.0456_ & \(0.2741\) & \(0.3476\) \\   

Table 1: Comparison of different algorithms on simulated networks. Each cell reports \( 2\) of the mean-squared error over 50 independent trials. Error numbers are all scaled by \(1e2\) for ease of reading. Bold: Best algorithm. Emphasis: Second-best algorithm.

   Name & \(n\) & Median Degree & Type \\  BiGG Model iWFL1372 & \(251\) & \(15.00\) & Source \\ BiGG Model iPC815 & \(251\) & \(12.00\) & Source \\ BiGG Model iJN1463 & \(251\) & \(14.00\) & Target \\ Email-EU Days 1-80 & \(1005\) & \(6.92\) & Source \\ Email-EU Days 81-160 & \(1005\) & \(7.35\) & Target \\ Email-EU Days 561-640 & \(1005\) & \(7.66\) & Target \\   

Table 2: Dataset Characteristicsreaction in that organism. We obtain the unweighted metabolic networks for multiple gram-negative bacteria from the BiGG genome-scale metabolic model dataset (King et al., 2016; Norsigian et al., 2020). In the left half of Figure 2, we compare two choices of source organism in estimating the network for BiGG model iJN1463 (_Pseudomonas putida_). For a good choice of source, Algorithm 1 is competitive with the Oracle at \(p=0.1\).

**Transfer Across Time in the Email Interaction Networks.** We use the Email-EU interaction network between \(n=1005\) members of a European research institution across \(803\) days Leskovec and Krevl (2014); Paranjape et al. (2017). The source graph \(A_{P}\) is the network from day \(1\) to \( 80\) (\(\)). In Figure 2 we simulate transfer with targets \(\) (left) and \(\) (right). We visualize results for arbitrary target periods; similar results hold for other targets. Unlike metabolic networks, Algorithm 2 has comparable performance to both our Algorithm 1 and the oracle algorithm with \(p\{0.01,0.05\}\). Compared to the metabolic networks, this indicates that the email interaction networks are relatively well-approximated by SBMs, although Algorithm 1 is still the best.

**Additional Experiments and Baseline.** In Appendix B.1, we present additional ablation experiments that test the dependence of Algorithms 1 and 2 on all relevant parameters. We compare their performance to the Oracle baseline with \(p=0.0\) (the non-transfer setting), and an additional baseline adapted from Levin et al. (2022). We find that our Algorithms outperform this new baseline but are worse than the Oracle with \(p=0.0\), as expected. Further, in Appendix B.2, we test our Algorithms and original baselines on a link prediction task in the setting of Figure 2. We find that the relative accuracy of the methods for link prediction is qualitatively similar to that of Figure 2, and the Oracle performs even better with sparsity tuning.

Figure 1: Comparison of algorithms on three source-target pairs (\(n=2000,n_{Q}=500\)). Each row corresponds to a different source/target pair \((P,Q)\). For a fixed row, the upper triangular part on columns 2, 3, 4 corresponds a \(\) for a different algorithm. The upper triangular part of column 1 shows the true \(P\). The lower triangular part of columns 1, 2, 3, and 4 is identical for a fixed row, and shows the true \(Q\). In each heatmap, the lower triangle is the target \(Q\). Algorithm 2 performs best when \((P,Q)\) are SBMs (top), while Algorithm 1 is better for smooth graphons (2nd and 3rd rows).

## 5 Conclusion

In this paper, we study transfer learning for network estimation in latent variable models. We show that there exists an efficient Algorithm 1 that achieves vanishing error even when \(n n_{Q}^{(1)}\), and a simpler Algorithm 2 for SBMs that achieves the minimax rate.

There are several interesting directions for future work.

First, we believe that Algorithm 1 works for moderately sparse networks with population edge density \((})\). This is because the concentration of empirical graph distance (Algorithm 1 line 3) requires expected edge density \((n^{-1/2})\) Mao et al. (2021). It would be interesting to see if a similar approach can work for edge density \(()\). For example, in the aforementioned paper it is shown that a variation of the graph distance of Definition 1.2 concentrates at expected edge density \((n^{-2/3})\). While is this still far from the \(()\) regime, it suggests that variations on the graph distance might ensure our Algorithm 1 works for sparser graphs.

Second, the case of multiple sources is also interesting. We have focused on the case of one source distribution, as in Cai and Wei (2021); Cai and Pu (2024), but expect that our algorithms can be extended to multiple sources as long as they satisfy Definition 1.3.