# Long-Term Fairness with Unknown Dynamics

Tongxin Yin

Electrical and Computer Engineering

University of Michigan

Ann Arbor, MI 48109

tyin@umich.edu

&Reilly Raab

Computer Science and Engineering

University of California, Santa Cruz

Santa Cruz, CA 95064

reilly@ucsc.edu

&Mingyan Liu

Electrical and Computer Engineering

University of Michigan

Ann Arbor, MI 48109

mingyan@umich.edu

&Yang Liu

University of California, Santa Cruz

ByteDance Research

Santa Cruz, CA 95064

yangliu@ucsc.edu

These authors contributed equally to this work.

###### Abstract

While machine learning can myopically reinforce social inequalities, it may also be used to dynamically seek equitable outcomes. In this paper, we formalize long-term fairness as an online reinforcement learning problem for a policy affecting human populations. This formulation accommodates dynamical control objectives, such as achieving equitable population _states_, that cannot be incorporated into static formulations of fairness. We demonstrate that algorithmic solutions to the proposed fairness problem can adapt to unknown dynamics and, by sacrificing short-term incentives, drive the policy-population system towards more desirable equilibria. For the proposed setting, we develop an algorithm that adapts recent work in online learning and prove that this algorithm achieves simultaneous probabilistic bounds on cumulative loss and cumulative violations of fairness. In the classification setting subject to group fairness, we compare our proposed algorithm to several baselines, including the repeated retraining of myopic or distributionally robust classifiers, and to a deep reinforcement learning algorithm that lacks fairness guarantees. Our experiments model human populations according to evolutionary game theory and integrate real-world datasets.

## 1 Introduction

As machine learning (ML) algorithms are deployed for tasks with real-world social consequences (e.g., school admissions, loan approval, medical interventions, etc.), the possibility exists for runaway social inequalities (Crawford and Calo, 2016; Chaney et al., 2018; Fuster et al., 2018; Ensign et al., 2018). While "fairness" has become a salient ethical concern in contemporary research, the closed-loop dynamics of real-world systems with feedback, i.e., comprising ML-driven decisions and populations that mutually adapt to each other (Fig. 5 in the supplementary material) remain poorly understood. We assert that realistic scenarios are often described by fundamentally **unknown dynamics**: Even with models of human behavior based on rational behavior or evolutionary game theory, utilities and risk-preferences are generally unknown or uncertain. For this reason, we advocate for dynamics-agnostic formulations of fairness, for which reinforcement learning is a natural fit.

In this paper, our **primary contribution** is to consider the problem of _long-term fairness_, or algorithmic fairness in the context of a dynamically responsive population, as a reinforcement learning (RL)problem subject to constraint. In our formulation, the central learning task is to develop a policy that minimizes cumulative loss (e.g., financial risk, negative educational outcomes, misdiagnoses, etc.) incurred by an ML agent interacting with a human population up to a finite time horizon, subject to constraints on cumulative "violations of fairness", which we refer to in a single time step as _disparity_ and cumulatively as _distortion_.

Our central hypothesis is that an RL formulation of long-term fairness may allow an agent to learn to **sacrificer short-term utility in order to drive the system towards more desirable equilibria**. The core practical difficulties posed by our general problem formulation, however, are the potentially unknown dynamics of the system under control, which must be determined by the RL agent _online_ (i.e., during actual deployment), and the general non-convexity of the losses or constraints considered. Additionally, our problem setting generally requires continuous state and action spaces for the RL agent, which preclude familiar methods that yield performance guarantees in discrete settings.

Our **secondary contributions** are 1) to show that long-term fairness can be solved within asymptotic, probabilistic bounds under certain dynamical assumptions and 2) to demonstrate that the problem can also be addressed more flexibly with existing RL methods. For theoretical guarantees, we develop L-UCBFair, an online RL method, and prove sublinear bounds on regret (suboptimiality of cumulative loss) and distortion (suboptimality of cumulative disparity) with high probability (Section3.1). To demonstrate practical solutions without strong safety guarantees, we apply R-TD3, an adaptation of a well-known deep reinforcement learning method (viz., TD3) to a time-dependent Lagrangian relaxation of the central problem. We compare L-UCBFair and R-TD3 to several baselines (Section3.3), including myopic policies, in interaction with simulated populations initialized with synthetic or real-world data and updated according to evolutionary game theory (Section4).

Finally, this paper considers fairness in terms of statistical regularities across (ideally) socioculturally meaningful _groups_. While internal conflict exists between different statistical measures of fairness (Corbett-Davies and Goel, 2018), we show that an RL approach to long-term fairness can mitigate trade-offs between immediately-fair policy decision _outcomes_ (e.g., acceptance rate disparities (Dwork et al., 2012; Zemel et al., 2013; Feldman et al., 2015)) and causally-fair population states (e.g., qualification rate (Raab and Liu, 2021; Zhang et al., 2020)). In particular, we show that our proposed solutions can learn to avoid the familiar trap of pursuing short-term fairness metrics only to widen underlying disparities that demand escalating interventions at the expense of utility (Section5.2).

### Related Work

Our formulation of long-term fairness bridges recent work on "fairness in machine learning", which has developed in response to the proliferation of data-driven methods in society, and "safe reinforcement learning", which seeks theoretical safety guarantees in the control of uncertain dynamical systems.

**Dynamics of Fairness in Machine Learning** We distinguish long-term fairness from the dynamics of fair allocation problems (Joseph et al., 2016; Jabbari et al., 2017; Tang et al., 2021; Liu et al., 2017) and emphasize side-effects of algorithmic decisions affecting future decision problems. By formalizing long-term fairness in terms of cumulative losses and disparities, we iterate on a developing research trend that accounts for the dynamical response of a human population to deployed algorithmic prediction: both as a singular reaction (Liu et al., 2018; Hu et al., 2019; Hu and Zhang, 2022) or as a sequence of mutual updates between the population and the algorithm (Coate and Loury, 1993; D'Amour et al., 2020; Zhang et al., 2020; Heidari et al., 2019; Wen et al., 2019; Liu et al., 2020; Hu and Chen, 2018; Mouzannar et al., 2019; Williams and Kolter, 2019; Raab and Liu, 2021).

Several prior works have considered the long-term fairness implications in the context of performative stability or optimality (Perdomo et al., 2020) with known, stateless dynamical transitions: Hu and Zhang (2022) characterize the convergence of systems typified by sequential, myopic policies while Hashimoto et al. (2018) contrast myopic policy with a modified objective satisfying distributional robustness. While Mouzannar et al. (2019) and Raab and Liu (2021) address stateful dynamical transitions, the cited works only treat myopic classifiers that optimize immediate utility (subject to fairness constraints) rather than learning to anticipate dynamical population responses. Finally, while Wen et al. (2019) explore reinforcement learning for long-term fairness, the discussion is limited to a tabular explore-then-commit approach over discrete state and action spaces. This approach does not generalize to continuous spaces, where we provide separate and tighter bounds for both utility and disparity.

**Safe Reinforcement Learning** L-UCBFair furthers recent efforts in safe RL. While _model-based_ approaches, in which the algorithm learns an explicit dynamical model of the environment, constitute one thread of prior work (Efroni et al., 2020; Singh et al., 2020; Brantley et al., 2020; Zheng and Ratliff, 2020; Kalagarla et al., 2021; Liu et al., 2021; Ding et al., 2021), leading algorithms in this domain are characterized by significant time and space complexity. Among _model-free_ algorithms, the unknown dynamics of our setting preclude the use of a simulator that allows algorithms to search over arbitrary state-action pairs (Xu et al., 2021; Ding et al., 2020; Bai et al., 2022).

While Wei et al. (2022) introduce a model-free, simulator-free algorithm, the tabular approach they consider is only applicable to discrete state and action spaces, while our setting requires continuous state and action spaces. To tackle continuous _state_ space, Ding et al. (2021) and Ghosh et al. (2022) consider linear dynamics: Ding et al. (2021) develop a primal-dual algorithm with safe exploration, and Ghosh et al. (2022) use a softmax policy design. Both algorithms are based on the work of Jin et al. (2020), which proposed a least squares value iteration (LSVI) method, using an Upper Confidence Bound (UCB) (Auer et al., 2002) to estimate a state-action "\(Q\)" function. In addition to continuous state spaces, L-UCBFair also addresses continuous _action_ spaces. To our knowledge, L-UCBFair is the first model-free, simulator-free RL algorithm to provides theoretical safety guarantees for both discrete and **continuous state _and_ action spaces**. Moreover, L-UCBFair achieves bounds on regret and distortion as tight as any algorithm thus far on discrete action space (Ghosh et al., 2022).

## 2 Problem Formulation

We motivate our formulation of long-term fairness with a binary classification task, though the general formulation we propose is more widely applicable. Given this initial task, we sequentially incorporate _fairness constraints_, then _population dynamics_, and then _cumulative loss and disparity_, before fully formalizing the central problem of optimizing cumulative loss subject to expected dynamics and constraints on cumulative disparity.

For our initial binary classification task, suppose that a random individual, sampled i.i.d. from a fixed population, has _features_\(X^{d}\), a _label_\(Y\{-1,1\}\), and a demographic _group_\(G\) (where \(=[n]\) for \(n 2\)). Denote the joint distribution of these variables in the population as \(s(X,Y,G)\) such that \(X,Y,G s\). The task is to predict \(Y\) (as \(\)) from \(X\) and \(G\) by choosing a classifier \(a\), where \( a(X,G)\), to minimize the empirical loss \((s,a)[L(Y,)],\) where \(L\) denotes some individual loss such as zero-one-loss. In general, we allow arbitrary, (unit-interval) bounded loss functions \((s,a)\) such that the **basic classification task** is \(_{a}(s,a).\)

The **standard, "fair" classification task** (_without_ a dynamically responsive population) incorporates constraints on the choice of classifier \(a\), such that the _disparity_\(\) induced on distribution \(s\) by \(a\) is bounded by some value \(c\). Formally, the task is \(_{a}(s,a)\) subject to \((s,a) c\). A standard measure of disparity \(\) is the violation of "demographic parity" (Dwork et al., 2012). For example, when \(=\{g_{1},g_{2}\}\), \((s,a)|_{s,a}(g_{1})-_{s,a}(g_{2})|^ {2}\), where \(_{s,a}(g)(1 G{=}g).\)

In this paper, we also wish to consider measures of fairness inherent to the distribution \(s\) (e.g., parity of group _qualification_ rates \((Y{=}1 G{=}g)\)). Such measures of fairness can only be affected by changes to \(s\) and thus require dynamics, which are not captured by the above formulation (Raab and Liu, 2021; Zhang et al., 2020). We will see that such notions of disparity are well-suited to an RL formulation of long-term fairness.

State, action, and policyAdopting the semantics of _sequence_ of dependent classification tasks, we identify the time-dependent distribution \(s_{}\) of individuals in the population as a _state_ and the chosen classifier \(a_{}\) as an _action_ of some algorithmic _agent_ at time \(\). While state space \(\) is arbitrary, we assume that action space \(\) admits a Euclidean metric, under which it is closed (i.e., \(\) is isomorphic to \(^{m},m_{>0}\)). We specify that \(a_{}\) is sampled stochastically according to the agent's current _policy_\(_{}\), that is, \(a_{}_{}(s_{}).\) Additionally, we assume \(s_{}\) is fully observable at time \(\{1,2,...\}\). In practice, \(s_{}\) must be approximated from finitely many empirical samples, though this caveat introduces well-understood errors that vanish in the limit of infinitely many samples.

DynamicsMoving beyond the one-shot "fair" classification task above, let us now assume that a population may react to classification, inducing the distribution \(s\) to change. In practice, such "distribution shift" is a well-known, real-world phenomenon that can increase realized loss and disparity when classifiers are fixed (Chen et al., 2022). For classifiers that free to update in response to a mutating distribution \(s\), subsequent classification tasks depend on the (stochastic) predictions made in previous tasks. In our formulation, we assume the existence of dynamical kernel \(\) that maps a state \(s\) and action \(a\) at time \(\) to a _distribution over_ possible states at time \(+1\). That is, \(s_{+1}(s_{},a_{})\).

We stipulate that \(\) may be initially unknown, but we assume that it does not explicitly depend on time and may be reasonably approximated or learned "online". While real-world dynamics may depend on information other than the current distribution of classification-specific variables \((X,Y,G)\) -- e.g., exogenous parameters, history, or additional variables of state -- we have identified the dynamical state \(s\) with the current, fully-observed distribution for simplicity and tractability.

Reward and utility, value and quality functionsBecause the standard RL convention is to _maximize reward_ rather than _minimize loss_, for an RL agent, we define the instantaneous reward \(r(s_{},a_{}) 1-(s_{},a_{})\) and a separate "utility" \(g(s_{},a_{}) 1-(s_{},a_{})\), where \(r\) and \(g\) do not explicitly depend on time \(\). We next propose to optimize anticipated _cumulative_ reward subject to constraints on anticipated _cumulative_ utility. Let the index \(j\) represent either reward \(r\) or utility \(g\). We use the letter \(V\) (for "value") to denote the future expected accumulation of \(j\) over steps \([h,...,H]\) (without time-discounting) starting from state \(s\), using policy \(\). Likewise, we denote the "quality" of an action \(a\) in initial state \(s\) with the letter \(Q\). For \(j\{r,g\}\), we define

\[V^{}_{j,h}(s)[_{=h}^{H}js_{},a_ {}|s_{h}=s]; Q^{}_{j,h}(s,a) [_{=h}^{H}js_{},a_{}\ ]\,|\,s_{h}=s,a_{h}=a].\]

Note that \(V\) and \(Q\) marginalize over the stochasticity of state transitions \(s_{+1}(s_{},a_{})\) and the sampling of actions \(a_{}_{}(s_{})\). By the boundedness of \(r\) and, \(g\), the values of \(V\) and \(Q\) belong to the interval \([0,H-h+1]\).

"Long-term fairness" via reinforcement learningThe central problem explored in this paper is

\[_{} V^{}_{r,1}(s) V^{}_{g,1}(s) .\] (1)

We emphasize that this construction considers a finite time horizon of \(H\) steps, and we denote the optimal value of \(\) as \(^{}\). We first assume that a solution to the problem exists bounded within the interior of the feasible set (i.e., _strict primal feasibility_ or _Slater's condition_), as in Efroni et al. (2020), Ding et al. (2021), and Ghosh et al. (2022):

**Assumption 2.1** (Slater's Condition).: \(\,>0,\,\), such that \(V^{}_{g,1}(s)+\).

The Online SettingGiven initially unknown dynamics, we formulate long-term fairness for the _online_ setting (in which learning is only possible through actual "live" deployment of policy, rather than through simulation). As it is not possible to unconditionally guarantee constraint satisfaction in Eq. (1) over a finite number of online steps, we instead measure two types of _regret_: the suboptimality of a policy with respect to cumulative incurred loss, denoted \(\), and the suboptimality of a policy with respect to cumulative induced disparity, denoted denoted \(\) for "distortion".

\[(,s_{1}) V^{^{}}_{r,1}(s_{1})-V ^{}_{r,1}(s_{1});(,s_{1}) [0,-V^{}_{g,1}(s_{1})].\] (2)

## 3 Algorithms and Analysis

We show that it is possible to provide guarantees for long-term fairness in the online setting. To do this, we develop L-UCBFair, the first model-free, simulator-free algorithm to provide such guarantees with continuous state and action spaces. Specifically, we prove probabilistic, sublinear bounds for regret and distortion under a linear MDP assumption (Assumption 3.1) and properly chosen parameters (Appendix B.1, in the supplementary material). We defer proofs to the supplementary material.

### L-UcbFair

Episodic MDPL-UCBFair inherits from a family of algorithms that treat an episodic Markov decision process (MDP) Jin et al. (2020). Therefore, we first map the long-term fairness problem to the episodic MDP setting, which we denote as \((,,H,,,)\). The algorithm runs for \(K\)_episodes_, each consisting of \(H\) time steps. At the beginning of each episode, which we index with \(k\), the agent commits to a sequence of policies \(^{k}=(_{1}^{k},_{2}^{k},...,_{H}^{k})\) for the next \(H\) steps. At each step \(h\) within an episode, an action \(a_{h}^{k}\) is sampled according to policy \(_{h}^{k}\), then the state \(s_{h+1}^{k}\) is sampled according to the transition kernel \((s_{h}^{k},a_{h}^{k})\). \(s_{1}^{k}\) is sampled arbitrarily for each episode.

Episodic RegretBecause L-UCBFair predetermines its policy for an entire episode, we amend our definitions of regret and distortion over all \(HK\) time steps by breaking them into a sum over \(K\) episodes of length \(H\):

\[(K)=_{k=1}^{K}(V_{r,1}^{^{*}}(s_{1}^{k}) -V_{r,1}^{^{k}}(s_{1}^{k}))\;\;\;;\;\;(K)= [0,_{k=1}^{K}(-V_{g,1}^{^{k}}(s_{1}^{k} ))].\] (3)

The LagrangianConsider the Lagrangian \(\) associated with Eq.1, with dual variable \( 0\):

\[(,):=V_{r,1}^{}(s)+(V_{g,1}^{}( s)-).\] (4)

L-UCBFair approximately solves the primal problem, \(_{}_{}(,)\), which is non-trivial, since the objective function is seldom concave in practical parameterizations of \(\). Let \(^{*}\) denote the optimal value of \(\); L-UCBFair assumes bound \(^{*}\), given parameter \(\), as described in the supplementary material (Assumption B.1).

**Assumption 3.1** (Linear MDP).: We assume \((,,H,,,)\) is a linear MDP with feature map \(:^{d}\). That is, for any \(h\), there exist \(d\) signed measures \(_{h}=\{_{h}^{1},,_{h}^{d}\}\) over \(\) and vectors \(_{r,h},_{g,h}^{d}\) such that, for any \((s,a,s^{})\),

\[_{h}(s^{} s,a)=(s,a),_{h} (s^{});\;\;\;\;r(s,a)= (s,a),_{r,h};\;\;\;g(s,a)=(s,a),_ {g,h}.\]

Assumption 3.1 addresses the curse of dimensionality when state space \(\) is the space of distributions over \(X,Y,G\). This assumption is also used by Jin et al. (2020) and Ghosh et al. (2022), and a similar assumption is made by Ding et al. (2021). This assumption is well-justified in continuous time if we allow for infinite-dimensional Hilbert spaces Brunton et al. (2021), but in practice we require limited dimensionality \(d\) for the codomain of \(\). In our experiments, we use a neural network to estimate a feature map \(\) offline which approximately satisfies the linear MDP assumption (Appendix D.1).

#### 3.1.1 Construction

L-UCBFair, or "LSVI-UCB for Fairness" (Algorithm1) is based on a Least-Squares Value Iteration with an optimistic Upper-Confidence Bound, as in LSVI-UCB Jin et al. (2020). For each \(H\)-step episode \(k\), L-UCBFair maintains estimates for \(Q_{r}^{k}\), \(Q_{g}^{k}\) and a time-indexed policy \(^{k}\). In each episode, L-UCBFair updates \(Q_{r}^{k},Q_{g}^{k}\), interacts with the environment, and updates the dual variable \(_{k}\), which is constant over episode \(k\).

LSVI-UCB Jin et al. (2020)The estimation of \(Q\) is challenging, as it is impossible to iterate over all \(s,a\) pairs when \(\) and \(\) are continuous and \(\) is unknown. LSVI parameterizes \(Q_{h}^{}(s,a)\) by the linear form \(_{h}^{}(s,a)\), as used by Jin et al. (2020), and updates

\[_{h}*{argmin}_{^{d}} _{=1}^{k-1}r_{h}(s_{h}^{},a_{h}^{})+_{a }Q_{h+1}(s_{h+1}^{},a)-^{} (s_{h}^{},a_{h}^{})^{2}+\|\|^{2}.\]

In addition, a "bonus term" \((^{}_{h}^{-1})^{1/2}\) is added to the estimate of \(Q\) in Algorithm1 to encourage exploration.

Adaptive Search PolicyCompared to the works of Ding et al. (2021) and Ghosh et al. (2022), the major challenge we face for long-term fairness is a continuous action space \(\), which renders theindependent computation of \(Q_{x}^{k}\), \(Q_{q}^{k}\) for each action impossible. To handle this issue, we propose an adaptive search policy: Instead of drawing an action directly from a distribution over continuous values, \(\) represents a distribution over finitely many \((M)\), Voronoi partitions of \(\). After sampling a region with a softmax scheme, the agent draws action \(a\) uniformly at random from it. In addition to defining a Voronoi partitioning of the action space for the adaptive search policy of L-UCBFair (in the supplementary material), we make the following smoothness assumption, which we use to bound the error introduced by this sampling method:

**Assumption 3.2** (Lipschitz).: There exists \(>0\), such that \(\|(s,a)-(s,a^{})\|_{2}\|a-a^{}\|_{2}\).

**Dual Update** For L-UCBFair, the update method for estimating \(\) in Eq.4 is also essential. Since \(V_{r,1}^{}(s)\) and \(V_{g,1}^{}(s)\) are unknown, we use \(V_{r,1}^{k}(s)\) and \(V_{g,1}^{k}(s)\) to estimate them. An estimate for \(\) is iteratively updated by performing gradient ascent in the Lagrangian (Eq.4) with step-size \(\), subject to the assumed upper bound \(\) for \(\)(AssumptionB.1). A similar method is also used in Ding et al. (2021); Ghosh et al. (2022).

#### 3.1.2 Analysis

We next bound the regret and distortion, defined in Eq.3, realizable by L-UCBFair. We then compare L-UCBFair with existing algorithms for discrete action spaces and discuss the importance of the number of regions \(M\) and the maximum distance \(_{I}\) from any action to the center of its corresponding Voronoi partition.

**Theorem 3.3** (Boundedness).: _With probability \(1-p\), there exists a constant \(b\) such that L-UCBFair (Algorithm1) achieves_

\[(K)b H^{2}}+(+1)H ;(K))H^{2}}}{},\] (5)

_for parameter values \(1\)\(_{I})KH}\), \(=((M)4dHK/p)\), and \(=(dH)\)._

For a detailed proof of Theorem3.3, refer to AppendixB.1. This theorem indicates that L-UCBFair achieves \((H^{2}K})\) bounds for both regret and distortion with high probability. Compared to the algorithms introduced by Ding et al. (2021) and Ghosh et al. (2022), which work with discrete action space, L-UCBFair guarantees the same asymptotic bounds on regret and distortion.

### R-Td3

Because assumptions in Section3.1 (e.g., the linear MDP assumption) are often violated in the real world, we also consider deep reinforcement learning methods as a flexible alternative. Concretely, we apply "Twin-Delayed Deep Deterministic Policy Gradient" (TD3) Fujimoto et al. (2018), with an implementation and default parameters provided by the open-source package "Stable Baselines 3" Raffin et al. (2021), on a Lagrangian relaxation of the long-term fairness problem with time-dependent multipliers for the reward and utility terms (Eq.6). We term this specific algorithm for long-term fairness R-TD3. While such methods lack provable safety guarantees, they may still confirm our hypothesis that agents trained via RL can learn to sacrifice short-term utility in order to drive dynamics towards preferable long-term states and explicitly incorporate dynamical control objectives provided as functions of state.

To treat the long-term fairness problem (Eq.1) using unconstrained optimization techniques (i.e., methods like TD3), we consider a time-dependent Lagrangian relaxation: We train R-TD3 to optimize

\[_{}(s_{})}{}[ _{=1}^{H}(1-_{})(s_{},a_{})+ _{}(s_{},a_{})],\] (6)

where \(s_{+1}(s_{},a_{})\), \(_{}=/H\).

Strictly applied, myopic fairness constraints can lead to undesirable dynamics and equilibria Raab and Liu (2021). Eq.6 relaxes these constraints (hard \(\) soft) for the near future while emphasizing them long-term. Thus, we hope to develop classifiers that learn to transition to more favorable equilibria.

### Baselines

We compare L-UCBFair and R-TD3 to a greedy agent as a proxy for a myopic status quo in which policy is repeatedly determined by optimizing for immediate utility, without regard for the population dynamics. This standard is known as "Repeated Risk Minimization" (Perdomo et al., 2020; Hu and Zhang, 2022), and we implement it using simple gradient descent on the different classes of (\(\)-parameterized) objective functions \(f\) we consider (Eq. (7)). Having adopted a notion of fairness that relies on "groups", we presuppose different groups of agents indexed by \(g\), and denote group-conditioned loss as \(_{g}\). The objectives that are approximately minimized for each baseline are

\[ f() =*{}_{a}[(s,a) ].\] (7a) Myopic-Fair: \[f_{}() =*{}_{a}[(1-) (s,a)+(s,a)],(0,1).\] (7b) Maxmin: \[f() =*{}_{a}[_{g}(_{g}(s,a))].\] (7c) Maxmin-Fair: \[f_{}() =*{}_{a}[(1-)_{g} (_{g}(s,a))+(s,a)],( 0,1).\] (7d)

The two "Maxmin" objectives are related to distributionally robust optimization, which has been previously explored in the context of fairness (Hashimoto et al., 2018), while the two "Myopic" objectives are more straight-forward. While our baselines do not guarantee constraint satisfaction, the two objectives labelled "-Fair" are nonetheless "constraint aware" in precisely the same way as a firm that (probabilistically) incurs penalties for violating constraints.

## 4 Simulated Environments

To evaluate the proposed algorithms and baselines, we consider a series of binary (\(Y\{-1,1\}\)) classification tasks on a population of two groups \(=\{g_{1},g_{2}\}\) modeled according to evolutionarygame theory (using replicator dynamics, as described in Appendix A.1, in the supplementary material). We consider two families of distributions of real-valued features for the population: One that is purely synthetic, for which \(X(Y,1)\), independent of group \(G\), and one that is based on logistic regressions to real-world data, described in Appendix A.2 (in the supplementary material). Both families of distributions over \(X\) are parameterized by the joint distribution \((Y,G)\). RL agents are trained on episodes of length \(H\) initialized with randomly sampled states.

In order to better handle continuous state space, we make the following assumption, which has been used to simplify similar synthetic environments (Raab and Liu, 2021):

**Assumption 4.1** (Well-behaved feature).: For the purely synthetic setting, we require \(X\) to be a "well-behaved" real-valued feature within each group. That is, for each group \(g\), \((Y{=}1 G{=}g,X{=}x)\) strictly increases in \(x\).

As an intuitive example of Assumption 4.1, if \(Y\) represents qualification for a fixed loan and \(X\) represents credit-score, we require higher credit scores to imply higher likelihood that an individual is qualified for the loan.

**Theorem 4.2** (Threshold Bayes-optimality).: _For each group \(g\), when Assumption 4.1 is satisfied, the Bayes-optimal, deterministic binary classifier is a threshold policy described by a feature threshold \(a_{g}\) for group \(g\). That is, if \(X a_{g}\) then \(=1\); otherwise, \(=-1\)._

As a result of Theorem 4.2, we consider our action space to be the space of group-specific thresholds, and denote an individual action as the vector \(a(a_{1},a_{2})\). Nonetheless, we note that Assumption 4.1 is often violated in practice, as it is in our semi-synthetic setting.

## 5 Experimental Results

Do RL agents learn to seek favorable equilibria against short-term utility? Is Lagrangian relaxation Eq.1 sufficient to encourage this behavior? We give positive demonstrations for both questions.

### Losses and Disparities Considered

Our experiments consider losses \(\) which combine true-positive and true-negative rates, i.e.,

\[(s,a)=1-(s,a)-(s,a); r (s,a)=(s,a)+(s,a),\] (8)

where \(,\); \((s,a)=_{s,a}(1,Y{=}1)\); and \((s,a)=_{s,a}(1,Y{=}{-}1)\).

For disparity \(\), we consider functions of the form \((s,a)=\|_{s,a}(g_{1}){-}_{s,a}(g_{2})\|^{2}\) that measure violations of demographic parity (DP) (Dwork et al., 2012), equal opportunity (EOp) (Hardt et al., 2016), equal odds (EO), and qualification rate parity (QR), described in Table1. We note that QR is inconsequential for the baseline agents, which ignore the mutability of the population state.

### Results

Our experiments show that algorithms trained with an RL formulation of long-term fairness can drive a reactive population toward states with higher utility and fairness than myopic policies. In Fig.1, the baseline policies (i.e., subfigures (a) and (b)), which focus on short-term incentives drive disparate qualification rates and attain lower long-term utility than the RL agents, indicating that short-term utility is _misaligned_ with desirable dynamics in this example. In subfigure (b) in particular, the baseline policy increasingly violates the static fairness condition with time, agreeing with previous results by Raab and Liu (2021). Meanwhile, the RL algorithms (subfigures (c) and (d)) learn to drive universally high qualification rates, thus allowing policies that capture higher utility with time (subfigure (e)), Fig.3.

Our central hypothesis, that long-term fairness via RL may induce an algorithm to sacrifice short-term utility for better long-term outcomes, is clearly demonstrated in the purely synthetic environment depicted by subfigures (a1-a3) of Fig. 2, in which the environment provides higher immediate utility (true-positive rates) but lower long-term utility when a policy chooses initially high acceptance rates. In this case, the RL algorithm (subfigure (a2)) drives the system towards high qualification rates by giving up immediate utility maximized by the myopic agent (subfigure (a1)).

With subfigures (b1-b3) of Fig. 2, we demonstrate the capability of RL to incorporate notions of fairness (e.g., qualification rate parity QR), that are impossible to formulate in the myopic setting. In subfigures (b1-b3), both RL agents learn to satisfy qualification rate parity by driving the state of the population towards equal qualification rates by group.

Figure 1: Using a modeled population with scalar features fit to the “Adult” dataset (Dua and Graff, 2017) at each time-step to mirror the evolving qualification rates (Appendix A.2), we compare our baseline algorithms to L-UCBFair and R-TD3 on the same set of 16 initial states with the task of maximizing true positive rates (tp) subject to demographic parity (DP). Plots (a-d) depict evolving group qualification rates under each algorithm with streamlines (_red arrows indicate the author’s emphasis_), while shading indicating immediate violations of demographic parity. We remark that the corresponding plots for the non“-Fair” baselines are qualitatively indistinct and omit them for space. In subplot (e), we visualize mean episode loss where \(H{=}150\) for each algorithm.

Figure 2: Subfigures (a1–a3) compare a baseline policy to L-UCBFair in a purely synthetic environment on the task maximizing the fraction of true-positive classifications (tp) subject to bounded demographic parity violation (DP). Episode length in this environment is \(H=100\). Subfigures (b1-b3) compare L-UCBFair to R-TD3 on a similar task subject to bounded qualification rate disparity instead. In both environments, modeled agent utilities translate higher acceptance rates to lower qualification rates. Figures (a1, a2) and (b1, b2) depict evolving qualification rates with streamlines (_red arrows indicate the author’s emphasis_) and use shading to indicate fairness violations.

Finally, our experiments also show that RL algorithms without theoretical guarantees may be applicable to long-term fairness. In Fig. 2 subfigure (b2), R-TD3 achieves similar qualitative behavior as L-UCBFair (subfigure (b1)), that is, driving high qualification at the expense of short-term utility) while achieving lower episodic mean loss (subfigure (b3)).

LimitationsDespite the potential, highlighted by our experiments, for RL-formulation of fairness to drive positive long-term social outcomes, it is not possible to truly validate such approaches without deployment on actual populations, which may be practically and ethically fraught. In addition, violations of fairness or decreased utility may be difficult to justify to affected populations and stakeholders, especially when the bounds provided by L-UCBFair, while as tight as any known, rely on assumptions that may be violated in practice.

Concluding remarksMachine learning techniques are frequently deployed in settings in which affected populations will _react_ to the resulting policy, closing a feedback loop that must be accounted for. In such settings, algorithms that prioritize immediate utility or static notions of fairness may yield dynamics that are _misaligned_ with these objectives long-term.

In this paper, we have reformulated long-term fairness as an online reinforcement learning problem (Eq. (1)) to address the importance of dynamics. We have shown that algorithmic solutions to this problem (e.g., L-UCBFair) are capable of simultaneous theoretical guarantees regarding cumulative loss and disparity (violations of fairness). We have also shown that these guarantees can be relaxed in practice to accommodate a wider class of RL algorithms, such as R-TD3. Finally, we emphasize again that the RL framework of long-term fairness allows notions of disparity inherent to the _state_ of a population to be explicitly treated, while such definitions are inoperable in the standard, myopic framework of fairness. We hope that our contributions spur interest in long-term mechanisms and incentive structures for machine learning to be a driver of positive social change.

AcknowledgementsThis work is partially supported by the National Science Foundation (NSF) under grants IIS-2143895, IIS-2112471, IIS-2040800, and CCF-2023495.

Figure 4: Comparison of total **accumulated** disparity (blue) and loss (orange) over a single episode of \(H=150\) steps as functions of initial state (initial qualification rates), for R-TD3 and TD3 (no constraint; \(=0\)) in the semi-synthetic environment (Adult dataset). This figure indicates that the increasingly weighted fairness term in the objective of R-TD3 can play a role in more rapidly reducing cumulative disparity, in this case by driving the system towards the line of equal qualification rates, at the cost of increased cumulative loss.

Figure 3: The mean episodic loss (left) and disparity (right) within a 20-step sliding window obtained by L-UCBFair for the (tp, DP) setting of Fig. 1 (a2). We emphasize that both loss and disparity decrease in time, as required for the sublinear regret and distortion guaranteed by (Theorem 3.3).