# Active Learning-Based Species Range Estimation

Christian Lange\({}^{1}\) Elijah Cole\({}^{2,3}\) Grant Van Horn\({}^{4}\) Oisin Mac Aodha\({}^{1}\)

\({}^{1}\)University of Edinburgh \({}^{2}\)Altos Labs \({}^{3}\)Caltech \({}^{4}\)UMass Amherst

###### Abstract

We propose a new active learning approach for efficiently estimating the geographic range of a species from a limited number of on the ground observations. We model the range of an unmapped species of interest as the weighted combination of estimated ranges obtained from a set of different species. We show that it is possible to generate this candidate set of ranges by using models that have been trained on large weakly supervised community collected observation data. From this, we develop a new active querying approach that sequentially selects geographic locations to visit that best reduce our uncertainty over an unmapped species' range. We conduct a detailed evaluation of our approach and compare it to existing active learning methods using an evaluation dataset containing expert-derived ranges for one thousand species. Our results demonstrate that our method outperforms alternative active learning methods and approaches the performance of end-to-end trained models, even when only using a fraction of the data. This highlights the utility of active learning via transfer learned spatial representations for species range estimation. It also emphasizes the value of leveraging emerging large-scale crowdsourced datasets, not only for modeling a species' range, but also for actively discovering them.

## 1 Introduction

Understanding the geographic range that a biological species occupies is a fundamental piece of information that drives models for ascertaining how vulnerable a species is to extinction threats , for quantifying how they respond to climate induced habitat change , in addition to being important for assessing biodiversity loss. Estimated range maps are typically generated from statistical or machine learning-based models that are parameterized from sparsely collected in situ observations . Such in situ observations can be obtained through various means, e.g., via experts conducting detailed field surveys to record the presence or absence of a particular species in a defined geographic area , from community scientists that record incidental observations in a less structured manner , or from autonomous monitoring solutions such as camera traps . One of the major limiting factors in scaling up the generation of reliable range maps to hundreds of thousands of species is the underlying collection of data.

Recently, community science-based platforms such as iNaturalist , eBird , and PlantNet  have proven to be an appealing and scalable way to collect species observation data by distributing the effort across potentially millions of participants around the world. However, due to the incidental nature of the observations collected on these platforms (i.e., users are not directed towards a specific geographic location and asked to survey it exhaustively for the presence of a species of interest), there can be strong biases with respect to the places people go and the types of species they report . Additionally, a conflict exists in that the ranges of rare species are the hardest to characterize owing to the lack of data, but estimating their ranges would actually provide the most useful data for conservation purposes . In this work, we explore an alternative collection paradigm for efficiently estimating geographic ranges of previously unmapped species inspired by ideas fromactive learning . Our objective is to minimize the number of geographic locations that need to be sampled to accurately estimate a species' geographic range. To achieve this, we leverage recent advances in deep learning methods for joint species range estimation .

Existing work in species range estimation typically focuses on the offline setting whereby the entire set of observations is assumed to be available at training time [18; 12; 13]. As a result, the range estimation model is trained once offline and not updated. There have been attempts to use the trained models to predict which geographic location to investigate next to determine if a species might be present [38; 32]. However, these methods have typically not been developed, or quantified, in a fully online setting where new data is requested sequentially to update the model. Our approach instead makes the assumption that the geographic range of an unmapped species can be represented by a weighted combination of existing ranges. Through iterative active querying guided by these existing ranges, we efficiently estimate these weights online using only a small number of observations. An overview of this process is illustrated in Fig 1.

Our core contribution consists of a novel active learning-based solution for efficient species range estimation. We quantitatively evaluate the performance of our approach and compare it to alternative active learning methods on two challenging datasets containing expert-derived evaluation range data from one thousand species. We obtain a 32% improvement in mean average precision after ten time steps compared to a conventional active learning baseline on our globally evaluated test set. This translates to a significant reduction in the number of locations that need to be sampled to obtain a reliable estimate of a species range. Code for reproducing the experiments in our paper can be found at https://github.com/Chris-lange/SDM_active_sampling.

## 2 Related work

### Species distribution modeling

Species distribution modeling refers to a family of methods that avail of species observation and geospatial data to estimate the geographic range of a species . These models typically take some encoding of a spatial region of interest as input and output a numerical score indicating how likely it is for a particular species to be found there. There is a large body of work on this topic, but existing methods can be broadly categorized based on the type of data they train on (e.g., presence-absence or presence-only data) and the output quantity they attempt to predict (e.g., species presence or abundance) .

In the case of presence-absence data, at training time we have access to observations containing a set of locations where a species of interest is confirmed to be present and also locations where it has been confirmed to be absent. The advantage of this setting is that it is easy to fit standard supervised classification methods to the training data, e.g., decision trees . This type of data

Figure 1: Our goal is to estimate the geographic range of a species (i.e., the locations where the species can be found during its lifetime) from a small number of actively selected in situ observations. (Left) We assume we have access to a hypothesis set of candidate species range estimation models, where each member of the set encodes the range for the species it was trained on. Intuitively, models that are close in this space encode similar spatial ranges (e.g., \(h_{i}\) and \(h_{j}\)). (Right) At each time step in our active learning-based species range estimation paradigm, we sample a geographic location, a human then goes there to determine if the species is present or not, and then we update our predicted range based on the new data. The hypothesis space on the left is used to guide the selection of locations via our active querying approach.

can be obtained from skilled observers that fill out detailed checklists recording the species they observe in a given area and time . In contrast, in the presence-only setting, we do not have access to confirmed absencies. Existing methods either factor this into the model  or synthetically generate _pseudo_-absence data for training . Presence observations are easier to collect than presence-absence data as they are gathered opportunistically by observers . Like all data-driven methods, these approaches have difficulties in estimating accurate ranges in the low data regime .

While traditional machine learning solutions have been shown to be effective for range estimation , recently we have observed a growing interest in the application of deep learning methods to the task [12; 8; 31; 16; 45; 13; 15]. In contrast to the majority of existing single species methods, these deep approaches jointly model multiple species in the same model, thus exploiting the fact that some species share similar range characteristics. In this work, we do not innovate on the underlying range estimation models used, but instead we show that existing methods that are trained on readily available weakly supervised presence-only data can provide transferable representations that can be used by active learning-based methods.

### Active learning and species range estimation

Motivated by the fact that obtaining sufficiently diverse and informative training data can be one of the main bottlenecks when applying machine learning solutions to many real-world problems, active learning attempts to frame the data collection process as an optimization problem . Through interaction with a user, the goal is to minimize the number of datapoints that need to be labeled while still obtaining high performance on the task of interest, e.g., in our case, estimating a species' range. A variety of active querying strategies have been proposed in the literature, including uncertainty sampling , committee-based methods , and methods that seek to label the datapoint that would have the largest impact on the model's parameters , among others.

The optimal design of spatial surveys for sampling a geographic region of interest when estimating spatially varying quantities is a central question in geostatistics  and has applications in tasks such as sensor placement . Both non-adaptive and sequential methods have been explored for related tasks in the context of tractable parametric models . In the case of deep learning , active sampling methods have also been shown to be effective for predicting spatially varying quantities in the context of remote sensing data . However, the application of active sampling-based methods for large-scale species range estimation remains under explored. While we focus on estimating species ranges, our approach is applicable to any spatially varying quantity where it is possible to obtain a set of possible related predictions. Potential alternative applications include sensor placement, geographic priors for image classification, and disease modeling, in addition to related tasks such as active data collection for training image classifiers.

Xue et al.  explored different methods for guiding observers to under sampled locations to address the issue of spatial data bias in crowd collected datasets. However, their focus is not on estimating ranges, but instead to attempt to counteract data sampling biases. There have been attempts to use species distribution models to help guide the search for under observed species [25; 47; 23]. Rosner-Katz et al.  proposed a spatial sampling approach that involves stacking the outputs from a small number of range estimation models in order to test if locations that are estimated to contain rare species are good locations to check for other rare species. Marsh et al.  outlined an approach for defining a utility score for unsampled locations by measuring the difference in the output of a distribution model that is updated assuming the species of interest occurs there, or does not. This score can be used for suggesting locations to sample. However, it is expensive to compute as it requires updating the model for each possible location of interest. Different from these existing works, we propose a new efficient approach for the sequential acquisition of species presence or absence observations to efficiently parameterize models for range estimation.

## 3 Active species range estimation

### Species range estimation

The goal of species range estimation is to predict the geographic range that a species occupies given a sparse set of observations at training time. Specifically, in the presence-absence setting , we are given a dataset \(=\{(_{i},y_{i})\}_{i=1}^{N}\), where each \(_{i}\) is a feature vector encoding a geographic location of interest (i.e., a specific latitude and longitude). In this work we focus on spatial observations, but it is possible to extend our models to spatio-temporal data. The corresponding \(y_{i}\{0,1\}\) is a binary label indicating if the species has been observed to be present (\(y_{i}=1\)) or found to be absent (\(y_{i}=0\)) at the geographic location represented by \(_{i}\).

The input features \(\) can simply represent transformed latitude and longitude values (i.e., \(=[lat,lon]\)), a set of environmental features that encode information about the climate and/or habitat present at the location (i.e., \(=[e_{1},e_{2},,e_{d}]\)) , or could be a set of latent features extracted from a geospatially aware deep neural network (i.e., \(=f([lat,lon])\)) . In our binary classification setting, the goal is to learn the parameters of a model \(h\) such that it correctly classifies the observed data. This can be achieved by minimizing the cross-entropy loss with respect to the training data:

\[L_{CE}=-_{(,y)}y(h())+(1-y)(1-h()).\] (1)

Here, we represent a prediction from the model for one species as \(h()\). In the simplest case, \(h\) could be a logistic regressor with \(h()=(^{})\), where \(\) is the sigmoid function and \(\) is the parameter (i.e., weight) vector that is learned for each species. Once trained, the model can be densely evaluated across all locations of interest (e.g., the entire globe) to generate a predicted range map for one species.

### Active location sampling

In the previous section, we assumed we had access to a labeled training set \(\) to train the range estimation model \(h\). As noted earlier, obtaining this data on a global scale for rare or difficult to observe species can be extremely challenging as well as time consuming, even with a spatially distributed set of observers. Given this, it would be highly desirable to target limited observation resources to geographic locations that would result in observations that would be most informative for estimating the parameters of the model.

By framing our task as an active learning problem , our goal is to efficiently select and prioritize geographic locations for observation in order to improve the performance of the range estimation model for a given species while minimizing the total number of required observations. The active learning process can be broken down into three main steps; _querying/sampling_: determining the next location to observe, _labeling_: obtaining the observation label for the sampled location (i.e., is a species present or absent over a specified temporal interval), and _updating_: changing the model parameters based on the newly acquired data. Greedy active learning approaches simply query the next datapoint to be sampled one instance at a time. In this greedy setting, after a datapoint is observed and labeled, the model is updated. This process continues until some termination criteria is met, e.g., a maximum number of iterations or only a minor change in the model parameters is obtained.

One of the most commonly used strategies for greedy active learning is uncertainty sampling . This involves sampling the feature that represents the location \(\) that the current model is most uncertain about:

\[^{*}=*{arg\,min}_{}|0.5-P(y=1|,^{t} )|.\] (2)

Here, \(P(y=1|,^{t})=h^{t}()\) indicates the output of the model at the time step \(t\), and \(^{t}=\{(_{1},y_{1}),(_{2},y_{2}),,(_{t},y_{t })\}\) is the set of training observations acquired up to and including time \(t\). Once sampled, \(^{*}\) is labeled (i.e., an individual goes to the location represented by \(^{*}\) and determines if the species is present there, or not) and then added to the updated set of sampled observations \(^{t+1}=^{t}(^{*},y^{*})\). Finally, the current model is updated by fitting it to the expanded set of sampled observations. Later in our experiments, we compare to uncertainty sampling, in addition to other baseline active learning methods, and show that they are not as effective for the task of species range estimation in early time steps.

### Leveraging candidate range estimation models

Unlike conventional classification problems found in tasks such as binary image classification, the ranges of different species are not necessarily distinct, but can instead have a potentially large amount of overlap (see Fig. 1 (left)). Inspired by this observation, we propose a new active sampling strategy that makes use of a set of candidate range estimation models.

We assume that we have access to a set of candidate pretrained models \(=\{h_{1},h_{2},,h_{k}\}\). Each individual model encodes the spatial range for a different species, with one model per species. In practice, these can be a set of linear models (e.g., logistic regressors) that operate on features from a geospatially aware deep neural network, where each model has an associated weight vector \(_{k}\). Having access to a large and diverse \(\) may initially seem like a strong assumption, however in practice, it is now feasible to generate plausible range maps for thousands of species thanks to the availability of large-scale crowd collected species observation datasets [45; 13]. Importantly, we assume the species that we are trying to model using active learning is _not_ already present in \(\).

Assuming the candidate models are linear classifiers, we can represent our range estimation model \(h\), for an unseen species, as a weighted combination of the candidate models' parameters:

\[^{t}=_{k=1}^{||}P(h_{k}|^{t})_ {k}.\] (3)

By averaging the parameters of our candidate models we generate a compact linear classifier that can be used to predict the presence of the new species, without requiring access to all models in \(\) during inference. When a new observation is added to \(^{t}\), the posterior \(P(h_{k}|^{t})\) is updated based on the agreement between a model \(h_{k}\) and the observed data in \(^{t}\). Assuming a uniform prior over the candidate models in \(\), we model the posterior as \(P(h_{k}|^{t}) P(^{t}|h_{k})\), and the resulting likelihood is represented as:

\[P(^{t}|h_{k})=_{(,y)^{t}}yh_{k}()+(1 -y)(1-h_{k}()).\] (4)

This is related to the query-by-committee-setting in active learning , where our candidate models represent the individual 'committee' members. However, instead of being trained on subsets of the sampled data online during active learning, our candidate models are generated offline from a completely different set of observation data from a disjoint set of species.

#### 3.3.1 Active learning with candidate range estimation models

Given our estimate of the model parameters from Eqn. 3, we can apply any number of active learning query selection methods from the literature to perform sample (i.e., location) selection. However, while our candidate models in \(\) provide us with an effective way to estimate the model parameters online, we can also use them to aid in the sample selection step. Specifically, given the likelihood of an observation from a model, weighted by \(P(h_{k}|^{t})\), we choose the location \(^{*}\) to query that is the most uncertain:

\[^{*}=*{arg\,min}_{}|0.5-|} _{h_{k}}h_{k}()P(h_{k}|^{t})|.\] (5)

The intuition here is that we would like to sample the location that is currently most uncertain across all the candidate models. Our approach is motivated by the desire to make use of existing well characterized ranges when modeling the range of a less well described species, where observations may be very limited, to reduce the sampling effort needed.

The success of our approach hinges on the hypothesis set \(\) containing a sufficiently diverse and representative set of candidate models. If the target test species' range is very different from any combination of the ones in \(\), our ability to represent its range will be hindered. Additionally, even if the test species' range is broadly similar to some weighted combination of the models in \(\), some finer-grained nuances of the range might be different. To address this, we extend the hypothesis set by adding an additional model \(h_{}\). \(h_{}\) is also a logistic regressor, but it is trained on the progressively accumulated observations from the test species that are obtained during active learning using a cross-entropy objective from Eqn. 1. Adding \(h_{}\) ensures that we also have access to the maximum likelihood model according to the observed data.

We refer to our approach as WA_HSS, which indicates that it uses a Weighted Average of the hypothesis set to represent the model weights, and performs Hypothesis Set Selection during active querying. Similarly, WA_HSS+ is our approach with the inclusion of \(h_{}\).

Experiments

In this section we quantitatively evaluate our active learning approach on two existing benchmark datasets which we adapt to our online learning setting.

### Implementation Details

**Experimental paradigm.** Our experiments are designed to compare different active learning strategies using synthetic presence-absence data drawn from expert-derived range maps. We begin by discretizing the world into a set of valid explorable locations which correspond to the centroids of all land overlapping resolution five H3 cells . Here, each cell encompasses an area of 252 \(^{2}\) on average. At each time step, and for each active learning strategy, we select one location \(^{*}\) to query from any valid location that has not yet been sampled for the presence (or absence) of the species of interest. The corresponding species presence or absence label \(y^{*}\) for \(^{*}\) is then generated according to the "ground truth" range map. In the real world, this step represents an observer visiting the queried location and searching for the species of interest. While our model could evaluate any continuous location, using H3 cell centroids ensures that queryable locations are equally spaced across the surface of the globe. We commence each experiment by randomly sampling one presence and one absence observation for each species. Experiments are performed independently for each species in the test set, and there is no interaction between species during the observation process. This is realistic, as in practice it would be unreasonable to assume that a single observer would be able to confirm the presence of more than a small number of species owing to their finite knowledge of different biological organisms. We repeat each experiment three times using different initial labelled examples and show standard deviations as error bars. We provide additional implementation details and a description of each of the baseline methods in the appendices.

**Training.** For our feature representation \(\) (discussed in Sec. 3.1), we use a learned spatial implicit neural representation obtained from a pretrained neural network. Specifically, we train a fully connected network \(f\) with presence-only data using the \(_{-}\) loss proposed in . We use the same original set of publicly available data from iNaturalist  but retrain the model from scratch after removing species that are in our test sets. This results in 44,181 total species. The linear classifiers corresponding to these species form the set of candidate models \(\) used by our approach. Each model in \(\) has a dimensionality of \(256\), i.e., each is a logistic regressor with \(256\) weights. During the active learning phase, every time a labeled observation is obtained, for models that require a linear model to be estimated (e.g., LR models or models with \(h_{}\)), we update the classifier parameters \(\) for the species in question using the cross-entropy objective in Eqn. 1. We do not update the parameters of the feature extractor \(f\), which are kept frozen. In later experiments, we quantify the impact of different feature representations by changing the feature extractor, e.g., by comparing models trained with environmental features as input or randomly initialized networks.

**Datasets.** We make use of two expert curated sources of range maps for evaluation and to generate labels for observations during the active learning process: International Union for Conservation of Nature (_IUCN_)  and eBird Status and Trends (_S&T_) . We use the procedure outlined in  to preprocess the data and randomly select 500 species from each data source for evaluation. We apply an ocean mask to remove any cell that overlaps with the ocean to make the active learning procedure more "realistic" as we do not expect naturalists interested in improving a land based species range to explore the ocean to ensure that the species is not present there. This data represents our expert-grade presence-absence evaluation range map data. Unlike the _IUCN_ data which has a confirmed presence or absence state for each location on the earth, the _S&T_ data does not cover the entire globe. In the case of _S&T_, the valid region is different on a species by species case and is biased towards the Americas. As a result, during _S&T_ evaluation, cells which are not included within the valid region for each species are excluded from sampling and evaluation. The distribution of each test set is illustrated in the appendices.

**Evaluation metrics.** Each experiment consists of comparisons between different active learning strategies where we start with the same initial set of two randomly selected observations (i.e., one absence and one presence). We report performance using mean average precision (MAP), where a model is evaluated at the centroid of each valid H3 cell (i.e., limited to cells that only overlap with land) and is compared to "ground truth" presence-absence obtained from the test set expert-derivedrange maps. During active learning, we measure average precision across all species in the test set at each time step and average them to obtain a single MAP score.

**Baselines.** In addition to evaluating our WA_HSS+ approach, we compare to several baseline active learning strategies. These include random and uncertainty sampling. For each strategy, we evaluate models resulting from a weighted combination from the candidate set (WA) and conventional logistic regressors (LR). We also compare to AN_FULL_E2E, the end-to-end trained implicit network method from  using their \(_{}\) loss trained with all available data for each species. While this model is trained only on presence-only data using pseudo-negatives, it has access to many more observations per species (i.e., a minimum of 50 presence observations per species) and is not restricted to only sampling data from land. Additional comparisons are provided in the appendices.

### Results

**Comparison of active learning strategies.** In Fig. 2 we present quantitative results on the _IUCN_ and _S&T_ datasets, and show qualitative results in Fig. 4. We compare our WA_HSS+ approach to multiple different baseline active learning methods and observe superior performance compared to each. The performance on individual species can vary a lot, but the average MAP across all test species is similar from run to run.

There is a clear advantage when using our active sample selection method (i.e., comparing WA_HSS+ to random sampling in WA_random) on the _IUCN_ test set. The addition of the online model estimate also is beneficial (i.e., comparing WA_HSS+ to WA_HSS). Estimating model parameters as a weighted combination of the hypothesis set (i.e., WA methods) is significantly better than conventional logistic regression (i.e., LR methods). The performance improvement on the _S&T_ test set is less dramatic. Here, even the random selection baseline performs well. This can possibly be attributed to the limited valid locations to explore for the species, the relative larger sizes, and the American focus of its ranges compared to the more globally distributed _IUCN_ data. However, we still observe an advantage when using WA methods in the early time steps. As an additional baseline, we also compare to an end-to-end trained presence-only model from  which has access to significantly more observation data at training time. We can see that our WA_HSS+ method approaches, and then exceeds, the performance of this baseline in fewer than 40 time steps for the _IUCN_ species.

**Impact of the feature space.** Our approach makes use of a weakly supervised pretrained feature extractor \(f\) to generate the input features \(\). Here, we explore different features spaces: nn loc feats are the standard deep network features from  that uses location only inputs, nn env feats are deep network features that use location and \(20\) additional environmental covariates (i.e., features that describe local properties such as amount of rainfall and elevation) from , and nn rand feats are from a randomly initialized and untrained version of the standard network. We also

Figure 2: Comparison of different active learning querying methods for the task of species range estimation. For each of the two datasets, we display the performance of different active learning methods over time, where higher values are better. The results depict the average of three runs for each method. We observe that methods that use a weighted combination of the candidate models result in the best performance (e.g., WA versus LR) particularly in early timesteps. Our WA_HSS+ approach performs best overall in both datasets. (Left) _IUCN_ results, where we observe a large difference between methods. (Right) _S&T_ results, where the performance difference is smaller but our approach is superior in the earlier timesteps.

perform experiments using the same input features but without a deep feature extractor (i.e., loc feats and env feats). In this setting, the candidate models are just logistic regression classifiers operating directly on the input features. In Fig. 3 (top left) we observe that the deep feature extractors significantly outperform non-deep ones. This highlights the power of transfer learning from disjoint species in the context of range estimation. This has not been explored in the exiting literature, where conventional approaches typically train models on environmental covariate input features . Interestingly, the fixed random higher dimensional projection of nn rand feats performs better than using only the low dimensional location or environmental features, loc feats and env feats. This is likely because the low dimensional features are not sufficiently linearly separable.

**Impact of number of candidate ranges.** One of the strengths of our approach is that it can leverage representations learned from disjoint species. In this experiment we quantify the impact of reducing the number of species available when training the backbone feature extractor which also results in a reduction of the size of the candidate set \(\). Specifically, we train different feature extractors from scratch, where each model has access to a different subset of the data during training, e.g., WA_HSS, 1000 sp is trained on data from only 1,000 species. In Fig. 3 (top right) we unsurprisingly observe a drop in MAP when the size of \(\) is small (e.g., < 1,000 species). As the size increases, the model performance improves. This illustrates the benefit of larger and more diverse candidate sets. This reduction has a larger impact on WA_HSS compared to WA_HSS+ due to the inclusion of \(h_{}\) in the case of the latter.

**Impact of observation noise.** Here we simulate the impact of observation noise by exploring the setting where the species is actually present at the location but the observer misses it. Thus we obtain false negative observations, but not false positives. We simulate different noise rates where the observer misses the species at each time step with a probability sampled from {0%, 10%, 25%, 50%}. In Fig. 3 (bottom left) we observe that our approach is surprisingly robust to non-trivial amounts of observation noise, and only starts to fail when the noise level becomes very large (i.e., 50%). In contrast, the best logistic regressor baseline is much more impacted by such label noise.

**Impact of hypothesis weighting.** Unlike traditional query by committee methods where each committee member is trained on a subset of the available data , our hypotheses represent models

Figure 3: Ablations of our approach on the _IUCN_ dataset. (Top Left) Impact of the feature space used to represent \(\). (Top Right) Impact of the number of species in the candidate set \(\). (Bottom Left) Impact of observation noise by introducing false negatives, where higher label noise amounts indicate that the observer is more likely to incorrectly predict that a species is not present at a location. (Bottom Right) Impact of candidate model weighting during active query selection.

trained on _different_ species. Thus, no individual model will necessarily have the same range as the test species. To compensate for this, we weight each \(h\) by the likelihood of that hypothesis, \(P(h|^{t})\), as indicated in Eqn. 5. In Fig. 3 (bottom right) we observe that removing this weighting term (no weighting) significantly reduces the performance of WA_HSS+, particularly at later time steps where \(^{t}\) contains enough samples to effectively identify and downweight hypotheses that are no longer consistent with the data. On the other hand, averaging the "soft votes" of each hypothesis (i.e., multiplying by \(h_{k}()\) as in Eqn. 5) (WA_HSS+), or averaging the "hard votes" (i.e., \(h_{k}()>0.5\) used in WA_HSS+ hard labels) as in traditional query by committee methods , results in similar performance. The uncertainty of any one model is not particularly important, potentially due to the large number of hypotheses that can contribute to sample selection.

## 5 Limitations

In spite of the significant improvement in performance our approach provides, it does have some limitations which we leave for future work. Firstly, all active learning strategies we consider query single points rather than diverse batches . This might limit the use of our strategies in situations where there are multiple observers available to gather data simultaneously. The geographic accessibility of specific locations is another limitation as we currently do not consider the practical difficulties associated with reaching certain geographic areas, e.g., remote regions. We also do not explore other forms of observation noise outside of failing to detect the species of interest when it is present. However, errors associated with misidentifying species are mitigated on platforms such as iNaturalist  who use a community consensus mechanism to assign labels to observations.

The performance of our of approach is limited by the expressiveness of the provided learned feature space and the diversity of species present in the candidate hypothesis set. We are also impacted by any inaccuracies in the candidate ranges that may result from their inability to capture very high spatial resolution range characteristics, in addition to any class imbalance or geographic sampling biases in the data used to train the underlying models. However, our method will further benefit from advances in handling noise and bias in the context of training joint range estimation models , in addition to improvements in architectures and location encodings . Finally, the expert-derived evaluation data we use represents the best available source, but it is still not guaranteed to be free from errors as characterizing a species' true range is a challenging problem.

Figure 4: Predicted range maps for the Yellow-footed Green Pigeon for three different active learning strategies, illustrated over three different time steps. We also display the expert range map centered on India and Southeast Asia, inset in the top left. The queried locations are marked with \(\), with green representing present locations and red representing absent ones. WA_HSS+ quickly identifies both the Indian and Southeast Asian part of the species’ range.

**Broader impacts.** The models trained as part of this work have not been validated beyond the use cases explored in the paper. Thus care should be taken when using them for making any decisions about the spatial distribution of species. We only performed experiments on publicly available species observation data, and caution should be exercised when using these models to generate range maps for threatened species.

## 6 Conclusion

Obtaining a sufficient number of on the ground observations to train species range estimation models is one of the main limiting factors in generating detailed range maps for currently unmapped species. To address this problem, we presented one of the first investigations of active learning in the context of efficiently estimating range maps from point observation data. We also proposed a new approach that makes use of a pre-existing learned environmental feature representation and a set of range maps from disjoint species to enable efficient querying of locations for observation. Even though this feature representation is derived from a model trained on weakly labeled presence-only data, we show that it is still very effective at transferring to new species. Our approach is computationally efficient as all of our operations are linear with respect to the input features which ensures that we can easily scale to tens of thousands of species. Through extensive quantitative evaluation, we demonstrated that it is possible to obtain a significant improvement in the quality of the resulting range maps with much fewer observations when compared to existing active learning methods.

**Acknowledgements.** This project was funded by the Climate Change AI Innovation Grants program, hosted by Climate Change AI with the support of the Quadrature Climate Foundation, Schmidt Futures, and the Canada Hub of Future Earth. Funding was also provided by the Cornell-Edinburgh Global Strategic Collaboration Awards.