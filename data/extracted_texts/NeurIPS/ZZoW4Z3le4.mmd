# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

to the input graph and capable of capturing the unique properties of graph-structured data, such as degree differences or size changes. This adaptivity ensures that the activation function can effectively leverage the structural information present in the graph data, potentially leading to improved performance in graph tasks.

Recent work in graph learning has investigated the impact of activation functions specifically designed for graphs, such as Iancu et al.  that proposes graph-adaptive max and median activation filters, and Zhang et al.  that introduces GReLU, which learns piecewise linear activation functions with a graph-adaptive mechanism. Despite the potential demonstrated by these approaches, the proposed activation functions still have predefined fixed structures (max and median functions in Iancu et al.  and piecewise linear in Zhang et al. ), restricting the flexibility of the activation functions that can be learned. Additionally, in the case of GReLU, the learned activation functions inherit the drawback of points of non-differentiability, which are undesirable according to the properties mentioned above. As a consequence, to the best of our knowledge, none of the existing activation functions prove to be consistently beneficial across different graph datasets and tasks. Therefore, _our objective is to design a flexible activation function tailored for graph data, offering consistent performance gains_. This activation function should possess many, if not all, of the properties recognized as beneficial for activation functions, with an emphasis on blueprint flexibility, as well as task and input adaptivity.

Our Approach: DiGRAF.In this paper, we leverage the success of learning diffeomorphisms, particularly through Continuous Piecewise-Affine Based transformations (CPAB) , to devise an activation function tailored for graph-structured data. Diffeomorphisms, characterized as bijective, differentiable, and invertible mappings with a differentiable inverse, inherently possess many desirable properties of activation functions, like differentiability, boundedness within the input-output domain, and stability to input perturbations. To augment our activation function with graph-adaptivity, we employ an additional GNN to derive the parameters of the learned diffeomorphism. This integration yields our node permutation equivariant activation function, dubbed DiGRAF - **D**Iffeomorphism-based **GR**aph **A**ctivation **F**unction, illustrated in Figure 1, that dynamically adapts to different graphs, providing a flexible framework capable of learning activation functions for specific tasks and datasets in an end-to-end manner. This comprehensive set of characteristics positions DiGRAF as a promising approach for designing activation functions for GNNs.

To evaluate the efficacy of DiGRAF, we conduct an extensive set of experiments on a diverse set of datasets across various tasks, including node classification, graph classification, and regression. Our evaluation compares the performance of DiGRAF with three types of baselines: traditional activation functions, activation functions with trainable parameters, and graph activation functions. Our experimental results demonstrate that DiGRAF repeatedly exhibits better downstream performance than other approaches, reflecting the theoretical understanding and rationale underlying its design and the properties it possesses. Importantly, while existing activation functions offer different behavior in different datasets, DiGRAF maintains consistent performance across diverse experimental evaluations, further highlighting its effectiveness.

Main contributions.The contributions of this work are summarized as follows: (1) We introduce a learnable graph-adaptive activation function based on flexible and efficient diffeomorphisms - DiGRAF, which we show to have properties advocated in literature; (2) an analysis of such properties, reasoning about the design choices of our method; and, (3) a comprehensive experimental evaluation of DiGRAF and other activation functions.

Figure 1: Illustration of DiGRAF. Node features \(^{(l-1)}\) and adjacency matrix \(\) are fed to a GNN\({}_{}^{(l)}\) to obtain updated intermediate node features \(}^{(l)}\), which are passed to our activation function layer, DiGRAF. First, an additional GNN network GNN\({}_{}\) takes \(}^{(l)}\) and \(\) as input to determine the activation function parameters \(^{(l)}\). These are used to parameterize the transformation \(T^{(l)}\), which operates on \(}^{(l)}\) to produce the activated node features \(^{(l)}\).

## 2 Related Work

Diffeomorphisms in Neural Networks.A bijection mapping function \(f:\), given two differentiable manifolds \(\) and \(\), is termed a _diffeomorphism_ if its inverse \(f^{-1}:\) is also differentiable. The challenge in learning diffeomorphisms arises from their computational complexity: early research is often based on complicated infinite dimensional spaces , and later advancements have turned to Markov Chain Monte Carlo methods, which still suffer from large computational complexity [1; 2; 90]. To address these drawbacks, Freifeld et al. [24; 25] introduced the Continuous Piecewise-Affine Based transformation (CPAB) approach, offering a more pragmatic solution to learning diffeomorphisms by starting from a finite-dimensional space, and allowing for exact diffeomorphism computations in the case of 1D diffeomorphisms - an essential trait in our case, given that activation functions are 1D functions. CPAB has linear complexity and is parallelizable, which can lead to sub-linear complexity in practice . Originally designed for alignment and regression tasks by learning diffeomorphisms, in recent years, CPAB was found to be effective in addressing numerous applications using neural networks, posing it as a suitable framework for learning transformation. For instance, Detlefsen et al.  learns CPAB transformations to improve the flexibility of spatial transformer layers, Martinez et al.  combines CPAB with neural networks for temporal alignment, Weber and Freifeld  introduces a novel loss function that eliminates the need for CPAB deformation regularization in time-series analysis, and Wang et al.  utilizes CPAB to model complex spatial transformation for image animation and motion modeling.

General-Purpose Activation Functions.In the last decades, the design of activation functions has seen extensive exploration, resulting in the introduction of numerous high-performing approaches, as summarized in Dubey et al. , Kunc and Klema . The focus has gradually shifted from traditional, static activation functions such as ReLU , Sigmoid , Tanh , and ELU , to learnable functions. In the landscape of learnable activation functions, the Maxout  unit selects the maximum output from learnable linear functions, and PReLU  extends ReLU by learning a negative slope. Additionally, the Swish function  augments the SiLU function , a Sigmoid-weighted linear unit, with a learnable parameter controlling the amount of non-linearity. The recently proposed AdAct  learns a weighted combination of several activation functions, and DiTAC  learns a diffeomorphic activation function for CNNs. However, these activation functions are not input-adaptive, a desirable property in GNNs.

Graph Activation Functions.Typically, GNNs are coupled with conventional activation functions [43; 78; 84], which were not originally tailored for graph data, graph tasks, or GNN models. This implies that these activation functions do not inherently adapt to the structure of the input graph, which was found to be an important property in other GNN components, such as graph normalization . Recent works have suggested various approaches to bridge this gap. Early works such as Scardapane et al.  propose learning activation functions based on graph kernels, and Iancu et al.  introduces Max and Median filters, which operate on local neighborhoods in the graph, thereby offering adaptivity to the input graphs. A notable advancement in graph-adaptive activation functions is GReLU , a parametric piecewise affine activation function achieving graph adaptivity by learning parameters through a hyperfunction that takes into account the node features and the connectivity of the graph. While these approaches demonstrate the potential to enhance GNN performance compared to standard activation functions, they are constrained by their blueprint, often relying on piecewise ReLU composition, which can be performance-limiting . Moreover, a fixed blueprint limits flexibility, i.e., the ability to express a variety of functions. As we show in Figure 2, attempts to approximate traditional activation functions such as ELU and Tanh using piecewise ReLU composition with different segment counts (\(K=1\), \(2\), and \(3\)), reveal limited approximation power. On the contrary, our DiGRAF, which leverages CPAB, yields significantly better approximations. Furthermore, we demonstrate the approximation power of activations learned with the CPAB framework in our DiGRAF in Appendix E.1.

Figure 2: Approximation of traditional activation functions using CPAB and Piecewise ReLU with varying segment counts \(K\{1,2,3\}\) on a closed interval \(=[-5,5]\), demonstrating the advantage of utilizing CPAB and its flexibility to model various activation functions.

## 3 Mathematical Background and Notations

In this paper, we utilize the definitions from CPAB -- a framework for efficiently learning flexible diffeomorphisms , alongside basic graph learning notations, to develop activation functions for GNNs. Consequently, this section outlines the essential details needed to understand the foundations of our DiGRAF.

### CPAB Diffeomorphisms

Let \(=[a,b]\) be a closed interval, where \(a<b\). We discretize \(\) using a tessellation \(\) with \(_{}\) intervals, which, in practice, is oftentimes an equispaced 1D meshgrid with \(_{}\) segments  (see Appendix C for a formal definition of tessellation). Our goal in this paper is to learn a diffeomorphism \(f:\) that we will use as an activation function. Formally, a diffeomorphism is defined as follows:

**Definition 3.1** (Diffeomorphism on a closed interval \(\)).: A diffeomorphism on a closed interval \(\) is any function \(f:\) that is (1) bijective, (2) differentiable, and (3) has a differentiable inverse \(f^{-1}\).

To instantiate a CPAB diffeomorphism \(f\), we define a continuous piecewise-affine (CPA) velocity field \(v^{}\) parameterized by \(^{_{}-1}\). We display examples of velocity fields \(v^{}\) for various instances of \(\) in Figure 2(a) to demonstrate the distinct influence of \(\) on \(v^{}\). Formally, a velocity field \(v^{}\) is defined as follows:

**Definition 3.2** (CPA velocity field \(v^{}\) on \(\)).: Given a tessellation \(\) with \(_{}\) intervals on a closed domain \(\), any velocity field \(v^{}:\) is termed continuous and piecewise-affine if (1) \(v^{}\) is continuous, and (2) \(v^{}\) is an affine transformation on each interval of \(\).

The CPA velocity field \(v^{}\) defines a differentiable trajectory \(^{}(x,t):\) for each \(x\). The trajectories are computed by integrating the velocity field \(v^{}\) to time \(t\), and are used to construct the CPAB diffeomorphism. We visualize the resulting diffeomorphism in Figure 2(b) with matching colors denoting corresponding pairs of \(v^{}\) and \(f^{}(x)\). Mathematically,

**Definition 3.3** (CPAB Diffeomorphism).: Given a CPA velocity field \(v^{}\), the CPAB diffeomorphism \(f\) at point \(x\), is defined as:

\[f^{}(x)^{}(x,t=1)\] (1)

such that \(^{}(x,t=1)\) solves the integral equation:

\[^{}(x,t)=x+_{0}^{t}v^{} (^{}(x,))d.\] (2)

In arbitrary dimensions, computing Definition 3.3 required using an ordinary differential equation solver and can be expensive. However, for 1D diffeomorphisms, as in our DiGRAF, there are closed-form solutions to the CPAB diffeomorphism and its gradients , offering an efficient framework for learning activation functions.

### Graph Learning Notations

Consider a graph \(G=(V,E)\) with \(N\) nodes, where \(V=\{1,,N\}\) is the set of nodes and \(E V V\) is the set of edges. Let \(\{0,1\}^{N N}\) be the adjacency matrix of \(G\), and \(^{N F}\) the node feature matrix,

Figure 3: An example of CPA velocity fields \(v^{}\) defined on the interval \(=[-5,5]\) with a tessellation \(\) consisting of five subintervals. The three different parameters, \(_{1}\), \(_{2}\), and \(_{3}\) define three distinct CPA velocity fields (Figure 2(a)) resulting in separate CPAB diffeomorphisms \(f^{}(x)\) (Figure 2(b)).

where \(F\) is the number of input features. We denote the feature vector of node \(v V\) as \(_{v}^{P}\), which corresponds to the \(v\)-th row of \(\). The input node features \(\) are transformed into the initial node representations \(^{(0)}^{N C}\), using an embedding function \(:^{P}^{C}\) to \(\), where \(C\) is the hidden dimension, that is

\[^{(0)}=().\] (3)

The initial features \(^{(0)}\) are fed to a GNN comprised of \(L\) layers, where each layer \(l\{1,,L\}\) is followed by an activation function \(^{(l)}(;^{(l)}):\), and \(^{(l)}\) is a set of possibly learnable parameters of \(^{(l)}\). Specifically, the intermediate output of the \(l\)-th GNN layer is denoted as:

\[}^{(l)}=^{(l)}_{}(^{(l-1)},)\] (4)

where \(}^{(l)}^{N C}\). The activation function \(^{(l)}\) is then applied _element-wise_ to \(}^{(l)}\), yielding node features \(h^{(l)}_{u,c}=^{(l)}(^{(l)}_{u,c};^{(l)})\)\( u V, c[C]\). Therefore, the application of \(^{(l)}\) can be equivalently written as:

\[^{(l)}=^{(l)}(}^{(l)};^{(l)}).\] (5)

In the following section, we will show how this abstraction is translated to our DiGRAF.

## 4 DiGRAF

In this section, we formalize our approach, DiGRAF, illustrated in Figure 1, which leverages diffeomorphisms to learn adaptive and flexible graph activation functions.

### A CPAB Blueprint for Graph Activation Functions

Our approach builds on the highly flexible CPAB framework  and extends it by incorporating Graph Neural Networks (GNNs) to enable the learning of adaptive graph activation functions. While the original CPAB framework was designed for grid deformation and alignment tasks, typically in 1D, 2D, or 3D spaces, we propose a novel application of CPAB in the context of learning activation functions, as described below.

In DiGRAF, we treat a node feature (single channel) as a one-dimensional (1D) point. Given the node features matrix \(}^{N C}\), we apply DiGRAF per entry in \(}\), in accordance with the typical element-wise computation of activation functions. We mention that, while CPAB was originally designed to learn grid deformations, it can be utilized as an activation function blueprint by considering a conceptual shift that we demonstrate in Figure 4. Given an input function (shown in red in the figure), CPAB deforms grid coordinates, i.e., it transforms it along the horizontal axis, as shown in the blue curve. In contrast, DiGRAF transforms the original data points along the vertical axis, resulting in the green curve. This conceptual shift can be seen visually from the arrows showing the different dimensions of transformations. We therefore refer to the vertical transformation of the data as their activations. Formally, we define the transformation function \(T^{(l)}\) as the element-wise application of the diffeomorphism \(f^{}\) from Equation (1):

\[T^{(l)}(^{(l)}_{u,c};^{(l)}) f^{^{(l)}} (^{(l)}_{u,c}),\] (6)

where \(^{(l)}\) denotes learnable parameters of the transformation function \(T^{(l)}\), that parameterize the underlying CPA velocity field as discussed in Section 3. In Section 4.2, we discuss the learning of \(^{(l)}\) in DiGRAF.

The transformation \(T^{(l)}:\) described in Equation (6) is based on CPAB and therefore takes as input values within a domain \(=[a,b]\), and outputs a value within that domain, where \(a<b\) are hyperparameters. In practice, we take \(a=-b\), such that the activation function can be symmetric and centered around 0, a property known to be desirable for activation functions . For any entry in the intermediate node features \(}^{(l)}\)(Equation (4)) that is outside the domain \(\), we use the identity function. Therefore, a DiGRAF activation function reads:

\[(^{(l)}_{u,c},^{(l)})=T^{(l)}( {h}^{(l)}_{u,c};^{(l)}),&^{(l)}_{u,c}\\ ^{(l)}_{u,c},&\] (7)

Figure 4: Different transformation strategies. The input function (red), CPAB transformation (blue), and DiGRAF transformation (green), within \(=[-5,5]\) using the same \(\). While CPAB stretches the input, DiGRAF stretches the output, showcasing the distinctive impact of each approach.

In practice, DiGRAF is applied element-wise in parallel over all entries, and we use the following notation, which yields the output features post the activation of the \(l\)-th GNN layer:

\[^{(l)}=(}^{(l)},^{(l )}).\] (8)

### Learning Diffeomorphic Velocity Fields

DiGRAF, defined in Equation (7), introduces graph-adaptivity into the transformation function \(T^{(l)}\) by employing an additional GNN, denoted as \(_{}\), that returns the diffeomorphism parameters \(^{(l)}\):

\[^{(l)}(}^{(l)},)= (_{}(}^{(l)},)),\] (9)

where Pool is a graph-wise pooling operation, such as max or mean pooling. The resulting vector \(^{(l)}^{N_{}-1}\), which is dependent on the tessellation size \(}}\), is then used to compute the output of the \(l\)-th layer, \(^{(l)}\), as described in Equation (8). We note that Equation (9) yields a different \(^{(l)}\) for every input graph and features pair \((}^{(l)},)\), which implies the graph-adaptivity of DiGRAF. Furthermore, since \(_{}\) is trained with the other network parameters in an end-to-end fashion, DiGRAF is also adaptive to the task of interest. In Appendix B, we provide and discuss the implementation details of \(_{}\) and Pool.

Variants of DiGRAF.Equation (9) describes an approach to introduce graph-adaptivity to \(^{(l)}\) using \(_{}\). An alternative approach is to directly optimize the parameters \(^{(l)}^{N_{}-1}\), without using an additional GNN. Note that in this case, input and graph-adaptivity are sacrificed in favor of a computationally lighter solution. We denote this variant of our method by DiGRAF (W/O Adap.). Considering this variant is important because it allows us to: (i) offer a middle-ground solution in terms of computational effort, and (ii) it allows us to directly quantify the contribution of graph-adaptivity in DiGRAF. In Section 5, we compare the performance of the methods.

Velocity Field Regularization.To ensure the smoothness of the velocity field, which will encourage training stability , we incorporate a regularization term in the learning procedure of \(^{(l)}\). Namely, we follow the Gaussian smoothness prior on the CPA velocity field from Freifeld et al. , which was shown to be effective in maintaining smooth transformations. The regularization term is defined as follows:

\[(\{^{(l)}\}_{l=1}^{L})=_{l=1}^{L} {}^{(l)}{}^{}_{}^{-1}^{(l)},\] (10)

where \(_{}\) represents the covariance of a zero-mean Gaussian smoothness prior defined as in Freifeld et al. . We further maintain the boundedness of \(^{(l)}\) by employing a hyperbolic tangent function (Tanh). In this way, \(^{(l)}\) remains in \([-1,1]\) when applied in \(T^{(l)}\) in Equation (7), ensuring that the velocity field parameters remain bounded, encouraging the overall training stability of the model.

### Properties of DiGRAF

In this section, we focus on understanding the theoretical properties of DiGRAF, highlighting the compelling attributes that establish it as a performant activation function for GNNs.

**DiGRAF yields differentiable activations.** By construction, DiGRAF learns a diffeomorphism, which is differentiable by definition. Being differentiable everywhere is considered beneficial as it allows for smooth weight updates during backpropagation, preventing the zigzagging effect in the optimization process .

**DiGRAF is bounded within the input-output domain \(\).** We point out in Remark D.3 that the diffeomorphism \(T^{(l)}(;^{(l)})\) is a \(\) transformation. Any diffeomorphism is continuous, and by the extreme value theorem, \(T^{(l)}(;^{(l)})\) is bounded in \(\). This prevents the activation values from becoming excessively large, a property linked to faster convergence .

**DiGRAF can learn to be zero-centered.** Benefiting from its flexibility, DiGRAF has the capacity to learn activation functions that are inherently zero-centered. As an input-adaptive activation function governed by a parameters vector \(^{(l)}\), DiGRAF can be adjusted through \(^{(l)}\) to maintain a zero-centered nature. This property is associated with accelerated convergence in neural network training .

**DiGRAF is efficient.** DiGRAF exhibits linear computational complexity, and can further achieve sub-linear running times via parallelization in practice . Moreover, with the existence of a closed-form solution for \(f^{^{(l)}}\) and its gradient in the 1D case , the computations of CPAB can be done efficiently. Additionally, the measured runtimes, detailed in Appendix H, underscore the complexity comparability of DiGRAF with other graph activation functions.

In addition to the above properties, which follow from our design choice of learning diffeomorphisms through the CPAB framework, we briefly present the following properties, which are formalized and proven in Appendix D.

**DiGRAF is permutation equivariant.** We demonstrate in Proposition D.4 that DiGRAF exhibits permutation equivariance to node numbering, ensuring that its behavior remains consistent regardless of the ordering of the graph nodes, which is a key desired property in designing GNN components .

**DiGRAF is Lipschitz continuous.** We show in Proposition D.2 that DiGRAF is Lipschitz continuous and derive its Lipschitz constant. Since it is also bounded, we can combine the two results, which leads us to the following proposition:

**Proposition 4.1** (The boundedness of \(T(;^{(l)})\) in DiGRAF).: _Given a bounded domain \(=[a,b]\) where \(a<b\), and any two arbitrary points \(x,y\), the maximal difference of a diffeomorphism \(T(;^{(l)})\) with parameter \(^{(l)}\) in DiGRAF is bounded as follows:_

\[|T(x;^{(l)})-T(y;^{(l)})|(|b-a|,|x-y|(C_{ ^{(l)}}))\] (11)

_where \(C_{^{^{(l)}}}\) is the Lipschitz constant of the CPA velocity field \(^{^{(l)}}\)._

**DiGRAF extends commonly used activation functions.** CPAB , which is used as a framework to learn the diffeomorphism in DiGRAF, is capable of learning and representing a wide range of diffeomorphic functions. When used as an activation function, the transformation \(T^{(l)}(;^{(l)})\) in DiGRAF adapts to the specific graph and task by learning different \(^{(l)}\) parameters, rather than having a fixed diffeomorphism. Examples of popular and commonly used diffeomorphisms utilized as activations include Sigmoid, Tanh, Softplus, and ELU, as we show in Appendix D. Extending this approach is our DiGRAF that learns the diffeomorphism during training rather than selecting a pre-defined function.

## 5 Experiments

In this section, we conduct an extensive set of experiments to demonstrate the effectiveness of DiGRAF as a graph activation function. Our experiments seek to address the following questions:

1. Does DiGRAF consistently improve the performance of GNNs compared to existing activation functions on a broad set of downstream tasks?
2. To what extent is graph-adaptivity in DiGRAF beneficial when compared to our baseline of DiGRAF (W/O Adap.) and existing activation functions that lack adaptivity?
3. Compared with other graph-adaptive activation functions, how does the added flexibility offered by DiGRAF impact downstream performance?
4. How do the considered activation functions compare in terms of training convergence?

**Baselines.** We compare DiGRAF with three categories of relevant and competitive baselines: (1) _Standard Activation Functions_, namely Identity, Sigmoid , ReLU , LeakyReLU , Tanh , GeLU , and ELU  to estimate the benefit of learning activation functions parameters; (2) _Learnable Activation Functions_, specifically PReLU , Maxout  and Swish , to assess the value of graph-adaptivity; and (3) _Graph Activation Functions_, such as Max , Median  and GReLU , to evaluate the effectiveness of DiGRAF's design in capturing graph structure and the blueprint flexibility of DiGRAF as discussed in Section 4.

All baselines are integrated into GCN  for node tasks and GIN  (GINE  where edge features are available) for graph tasks, to ensure fair and meaningful comparisons, isolating the impact of other design choices. We provide additional details on the experimental settings and datasets in Appendix G, as well as additional experiments, including ablation studies, in Appendix E.

### Node Classification

Our results are summarized in Table 1, where we consider the BlogCatalog , Flickr , CiteSeer , Cora , and PubMed  datasets. As can be seen from the Table, DiGRAF consistently outperforms all standard activation functions, as well as all the learnable activation functions. Additionally, DiGRAF outperforms other graph-adaptive activation functions. We attribute this positive performance gap to the ability of DiGRAF to learn complex non-linearities due to its diffeomorphism-based blueprint, compared to piecewise linear or pre-defined functions as in other methods. Finally, we compare the performance of DiGRAF and DiGRAF (W/O Adap.). We remark that in this experiment, we are operating in a transductive setting, as the data consists of a single graph, implying that both DiGRAF and DiGRAF (W/O Adap.) are adaptive in this case. Still, we see that DiGRAF slightly outperforms the DiGRAF (W/O Adap.) and we attribute this performance gain to the GNN layers within DiGRAF  that are (i) explicitly graph-aware, and (ii) can facilitate the learning of better diffeomorphism parameters \(^{(i)}\) (Equation (9)) due to the added complexity.

### Graph Classification and Regression

**ZINC-12k.** In Table 2 we present results on the ZINC-12K  dataset for the regression of constrained solubility of molecules. We note that DiGRAF achieves an MAE of \(0.1302\), surpassing the best-performing activation on this dataset, Maxout, by \(0.0285\), which translates to a relative improvement of \( 18\%\).

**OGB.** We evaluate DiGRAF on 4 datasets from the OGB benchmark , namely, molesol, moltox21, molbace, and moltiv. The results are reported in Table 3, where it is noted that DiGRAF achieves significant improvements compared to standard, learnable, and graph-adaptive activation functions. For instance, DiGRAF obtains a ROC-AUC score of \(80.28\%\) on Moltiv, an absolute improvement of \(4.7\%\) over the best performing activation function (ReLU).

**TUDatasets.** In addition to the aforementioned datasets, we evaluate DiGRAF on the popular TUDatasets . We present results on MUTAG, PTC, PROTEINS, NCI1 and NCI109 in Table 5 in Appendix E. The results show that DiGRAF is always within the top-three performing activations across all datasets. As an example, on PROTEINS dataset, we see an absolute improvement of \(1.1\%\) over the best-performing activation functions (Maxout and GReLU).

### Convergence Analysis

Besides improved downstream performance, another important aspect of activation functions is their contribution to training convergence . We therefore present the training curves of DiGRAF as well as the rest of the considered baselines to gain insights into their training convergence. Results for representative datasets are presented in Figure 5, where DiGRAF achieves similar or better training convergence than other methods, while also demonstrating better generalization abilities due to its better performance.

### Discussion

Our extensive experiments span across 15 different datasets and benchmarks, consisting of both node- and graph-level tasks. Our key takeaways are as follows:

   Method \(\)/Dataset \(\) &  & Flickr & CtiSear & Cora & PumMed \\ 
**Standard Activations** & & & & & \\ GCN + Identity & 74.8\(\)0.5 & 53.5\(\)1.1 & **69.1\(\)1.6** & 80.5\(\)1.2 & 77.6\(\)2.1 \\ GCN + Signal  & 39.7\(\)4.5 & 18.3\(\)1.2 & 27.9\(\)2.1 & 32.1\(\)2.3 & 52.8\(\)6.6 \\ GCN + ReLU  & 72.1\(\)1.9 & 50.7\(\)2.3 & 67.7\(\)2.3 & 79.2\(\)1.4 & 77.6\(\)2.2 \\ GCN + LazgReLU  & 72.6\(\)2.1 & 51.0\(\)2.0 & 68.4\(\)1.8 & 79.4\(\)1.6 & 76.8\(\)1.6 \\ GCN + Tanh  & 73.9\(\)0.5 & 51.3\(\)1.5 & **69.1\(\)1.4** & 80.5\(\)1.3 & **77.9\(\)2.1** \\ GCN + GReLU  & 75.8\(\)0.5 & **56.1\(\)1.3** & 67.8\(\)1.7 & 79.3\(\)1.9 & 77.1\(\)2.7 \\ GCN + ELU  & 74.8\(\)0.5 & 53.4\(\)1.1 & **69.1\(\)1.7** & **80.7\(\)1.2** & 77.5\(\)2.2 \\ 
**Learnable Activations** & & & & & \\ GCN + PReLU  & 74.8\(\)0.4 & 53.2\(\)1.5 & **69.2\(\)1.5** & 80.5\(\)1.2 & 77.6\(\)2.1 \\ GCN + Maxout  & 72.4\(\)1.4 & 54.0\(\)1.8 & **68.5\(\)**2.2 & 79.8\(\)1.5 & 77.3\(\)2.9 \\ GCN + swish  & **76.0\(\)0.7** & 55.7\(\)1.4 & 67.7\(\)1.8 & 79.2\(\)1.1 & 77.3\(\)2.8 \\ 
**GRAPH Activations** & & & & & \\ GCN + Macau  & 72.0\(\)1.0 & 47.5\(\)0.9 & 59.7\(\)2.9 & 76.0\(\)1.8 & 75.0\(\)1.4 \\ GCN + Median  & 77.7\(\)0.7 & **58.3\(\)0.6** & 61.3\(\)2.7 & 77.1\(\)1.1 & 75.7\(\)2.5 \\ GCN + GReLU  & 73.7\(\)1.2 & 54.4\(\)1.6 & 68.5\(\)1.9 & **81.8\(\)1.8** & **78.9\(\)1.7** \\  GCN + DiGRAF (W/O Adap.) & 80.8\(\)0.6 & 68.6\(\)1.8 & 69.2\(\)2.1 & 81.5\(\)1.1 & 78.3\(\)1.6 \\ GCN + DiGRAF & **81.6\(\)0.8** & **69.6\(\)0.6** & **69.5\(\)1.4** & **82.8\(\)1.1** & **79.3\(\)1.4** \\   

Table 1: Comparison of node classification accuracy (%) \(\) on different datasets using various baselines with DiGRAF. The top three methods are marked by **First**, **Second**, **Third**.

   Method & ZINC (MAE \(\)) \\ 
**Standard Activations** & \\ GIN + Identity & 0.2460\(\)0.0214 \\ GIN + Signal  & 0.3839\(\)0.0058 \\ GIN + ReLU  & **0.1630\(\)0.0040** \\ GIN + LeakyReLU  & 0.1718\(\)0.0042 \\ GIN + Tanh  & 0.1797\(\)0.0064 \\ GIN + GReLU  & 0.1896\(\)0.0023 \\ GIN + ELU  & 0.1741* **Overall Performance of DiGRAF:** The performance offered by DiGRAF is consistent and on par with or better than other activation functions, across all datasets. These results establish DiGRAF as a highly effective approach for learning graph activation functions.
* **Benefit of Graph-Adaptivity:** DiGRAF outperforms the learnable (although not graph-adaptive) activation functions such as PReLU, Maxout, and Swish, as well as our non-graph adaptive baseline DiGRAF (W/O Adap.), on all considered datasets. This observation highlights the crucial role of graph-adaptivity in activation functions for GNNs.
* **The Benefit of Blueprint Flexibility:** DiGRAF consistently outperforms other graph-adaptive activation functions like Max, Median, and GReLU. We tie this positive performance gap to the ability of DiGRAF to model complex non-linearities due to its diffeomorphism-based blueprint, compared to piecewise linear or pre-defined functions as in other methods.
* **Convergence of DiGRAF:** As shown in Section 5.3, in addition to overall better downstream performance, DiGRAF allows to achieve better training convergence.

In summary, compared with 12 well-known activation functions used in GNNs, and across multiple datasets and benchmarks, DiGRAF demonstrates a superior, learnable, flexible, and versatile graph-adaptive activation function, highlighting it as a strong approach for designing and learning graph activation functions.

## 6 Conclusions

In this work, we introduced DiGRAF, a novel activation function designed for graph-structured data. Our approach leverages Continuous Piecewise-Affine Based (CPAB) transformations to integrate a graph-adaptive mechanism, allowing DiGRAF to adapt to the unique structural features of input graphs. We show that DiGRAF

Figure 5: Convergence analysis of DiGRAF compared to baseline activation functions. The plot illustrates the training loss over epochs, showcasing the overall faster convergence of DiGRAF.

    &  molesol \\ RMSE \(\) \\  &  moltx21 \\ ROC-AUC \(\) \\  &  moltx20 \\ ROC-AUC \(\) \\  & 
 moltx \\ ROC-AUC \(\) \\  \\   \\ GIN + Identity & 1.402\(\)0.036 & 74.51\(\)0.44 & 72.69\(\)2.93 & 75.12\(\)0.77 \\ GIN + Siemol  & 0.884\(\)0.043 & 69.15\(\)0.52 & 68.70\(\)3.68 & 73.87\(\)0.80 \\ GIN + ReLU  & 1.173\(\)0.057 & 74.91\(\)0.51 & 72.97\(\)0.40 & **75.58\(\)1.40** \\ GIN + LeakyReLU  & 1.219\(\)0.055 & 74.69\(\)1.10 & 73.40\(\)3.19 & 74.75\(\)1.20 \\ GIN + Tanh  & 1.190\(\)0.044 & 74.93\(\)0.61 & 74.92\(\)2.47 & **75.22\(\)**0.23 \\ GIN + GeLU  & 1.147\(\)0.050 & 74.29\(\)0.59 & 75.59\(\)3.32 & 74.15\(\)0.79 \\ GIN + ELU  & 1.104\(\)0.038 & 75.08\(\)0.62 & 76.10\(\)3.29 & 75.09\(\)0.65 \\   \\ GIN + PReLU  & **1.098\(\)0.062** & 74.51\(\)0.92 & 76.16\(\)2.28 & 73.56\(\)1.63 \\ GIN + Maxout  & 1.109\(\)0.045 & 75.14\(\)0.87 & 76.83\(\)3.88 & 72.75\(\)2.10 \\ GIN + Swish  & 1.113\(\)0.066 & 73.31\(\)1.01 & **77.23\(\)2.35** & 72.95\(\)0.64 \\   \\ GIN + Max  & 1.199\(\)0.070 & **75.50\(\)0.77** & 77.04\(\)2.81 & 73.44\(\)2.08 \\ GIN + Median  & **1.049\(\)0.038** & 74.39\(\)0.90 & **77.26\(\)2.74** & 72.80\(\)2.21 \\ GIN + GeLU  & 1.108\(\)0.066 & **75.33\(\)0.51** & 75.17\(\)2.60 & 73.45\(\)1.62 \\  GIN + DiGRAF (W/O Adap.) & 0.901\(\)0.047 & 76.37\(\)0.49 & 78.90\(\)1.41 & 79.19\(\)1.36 \\ GIN + DiGRAF & **0.8196\(\)0.051** & **77.03\(\)0.59** & **80.37\(\)1.37** & **80.28\(\)1.44** \\   

Table 3: A comparison of DiGRAF to natural baselines, standard, and graph activation layers on OGB datasets, demonstrating the advantage of our approach. The top three methods are marked by **First**, **Second**, **Third**.

exhibits several desirable properties for an activation function, including differentiability, boundedness within a defined interval, and computational efficiency. Furthermore, we demonstrated that DiGRAF maintains stability under input perturbations and is permutation equivariant, therefore suitable for graph-based applications. Our extensive experiments on diverse datasets and tasks demonstrate that DiGRAF consistently outperforms traditional, learnable, and existing graph-specific activation functions.

Limitations and Broader Impact.While DiGRAF demonstrates consistent superior performance compared to existing activation functions, there remain areas for potential improvement. For instance, the current formulation is limited to learning activation functions that belong to the class of diffeomorphisms, which, despite encompassing a wide range of functions, might not be optimal. By improving the performance on real-world tasks like molecule property prediction, and offering faster training convergence, we envision a positive societal impact by DiGRAF in drug discovery and in achieving a lower carbon footprint.