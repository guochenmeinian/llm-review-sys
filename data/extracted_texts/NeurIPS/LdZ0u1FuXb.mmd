# A Kernel Perspective on

Distillation-based Collaborative Learning

 Sejun Park   Kihun Hong   Ganguk Hwang

Department of Mathematical Sciences

Korea Advanced Institute of Science and Technology

{sejunpark, nuri9911, guhwang}@kaist.ac.kr

Corresponding author

###### Abstract

Over the past decade, there is a growing interest in collaborative learning that can enhance AI models of multiple parties. However, it is still challenging to enhance performance them without sharing private data and models from individual parties. One recent promising approach is to develop distillation-based algorithms that exploit unlabeled public data but the results are still unsatisfactory in both theory and practice. To tackle this problem, we rigorously analyze a representative distillation-based algorithm in the view of kernel regression. This work provides the first theoretical results to prove the (nearly) minimax optimality of the nonparametric collaborative learning algorithm that does not directly share local data or models in massively distributed statistically heterogeneous environments. Inspired by our theoretical results, we also propose a practical distillation-based collaborative learning algorithm based on neural network architecture. Our algorithm successfully bridges the gap between our theoretical assumptions and practical settings with neural networks through feature kernel matching. We simulate various regression tasks to verify our theory and demonstrate the practical feasibility of our proposed algorithm.

## 1 Introduction

Collaborative learning of AI models in decentralized settings is an important problem covered in various fields of machine learning such as distributed learning , Federated Learning (FL) , peer-to-peer learning , and miscellaneous collaborative learning . In particular, this theme has been most actively discussed in the context of FL . In this context, each local party is typically viewed as a subordinate entity within the collective learning system. For example, most FL algorithms mandate the exchange of local AI model information among participating local parties. Under this scheme, local AI models are usually subjected to restrictions in their architecture. However, from the perspective of collaboration, each local party may have to be regarded as an independent learning agent, meaning they are not obligated to fully share their model information. In short, the model (or parameter) exchange in FL algorithms can emerge as a critical issue in collaborative learning.

Fundamentally, addressing this issue necessitates an alternative medium for sharing learning information distinct from model exchange. Indeed, Distillation-based Collaborative Learning (DCL)  provides a good answer. In these algorithms, local training information is shared via the outcomes of AI models on additional unlabeled public data. The collected information is then utilized for knowledge distillation  to each local AI model. As mentioned in , this procedure is agnostic to model heterogeneity and avoids the direct sharing of local AI model information. This is a key advantage that distinguishes DCL from traditional FL.

Despite its pioneering nature and potential utility, DCL has not been sufficiently explored. A significant reason for this is the lack of theoretical understanding regarding knowledge distillation and its effectiveness in massively distributed statistically heterogeneous environments. Our work stems from the fundamental question of whether DCL algorithms can be theoretically effective in these settings. Inspired by [48; 58], we analyze FedMD [30; 41], the most standard DCL algorithm from a nonparametric perspective. Specifically, we adopt an operator-theoretic approach [5; 15; 37; 53; 65] to obtain an upper rate of convergence for the nonparametric version of FedMD (named **DCL-KR**) in the expected sense. Remarkably, our analysis reveals that DCL-KR achieves a nearly minimax optimal convergence rate, where the prefactor is independent of the number of participating local parties. It is worth noting that DCL-KR is the first nearly minimax optimal collaborative learning algorithm that does not directly share local data or models in massively distributed statistically heterogeneous environments. The novelty of our theoretical results and their comparison to prior works are provided in Section 2 and 3.

Nevertheless, our theoretical analysis does not fully demonstrate the efficacy of DCL algorithms based on neural network architectures. Instead, our theoretical results serve as inspiration for designing a novel DCL algorithm for regression that refines existing approaches. Consequently, we propose a Distillation-based Collaborative Learning algorithm over heterogeneous Neural Networks (named **DCL-NN**) for regression tasks. DCL-NN leverages kernel matching to align the feature kernels from the last hidden layer of each local AI model with an ensemble kernel. This procedure brings heterogeneous neural networks into the regime of DCL-KR.

Finally, we conduct experiments on DCL-KR and DCL-NN. To illustrate the superiority of our algorithms, we compare them with several baselines on various regression tasks. Experimental results show that DCL-KR achieves the same performance as the centralized model, even beyond the theoretical results. We also observe that DCL-NN significantly outperforms previous DCL frameworks in most settings.

In summary, our contributions are as follows:

1. In Section 3, we theoretically prove that a nonparametric version of the most standard distillation-based collaborative learning algorithm (named DCL-KR) is nearly minimax optimal in massively distributed statistically heterogeneous environments.
2. Inspired by the results provided in Section 3, we propose a distillation-based collaborative learning algorithm with heterogeneous neural networks (named DCL-NN) in Section 4.
3. In Section 5, we conduct experiments to empirically confirm our theoretical results and show the practical feasibility of our proposed algorithms.

## 2 Related Work

Federated LearningMost FL algorithms  communicate model parameters for collaboration. This approach has been extensively studied under various constraints, including data privacy , statistical heterogeneity [24; 31], communication efficiency , personalization [13; 59], and robustness . While it has been successful both theoretically and experimentally, this type of FL is limited in terms of the privacy and flexibility of local AI models, as the algorithms directly access the structures and parameters of the local models. Our study focuses on distillation-based collaborative learning, where the privacy and flexibility of local AI models are fully guaranteed.

Distillation-based Collaborative (or Federated) LearningThe type of algorithms we investigate operates by communicating the functional information of local AI models. These algorithms typically assume the availability of additional public data points. In this case, the outcomes of local models on the public dataset are used for collaboration. For instance, Li and Wang , Lin et al. , Park et al.  iteratively collect predictions of local models on the public dataset and then aggregate them into a naive ensemble (with or without a fixed linear transformation) to distribute. On the other hand, Cho et al. , Zhang et al. , Fan et al.  apply personalized ensemble strategies by additionally learning the mutual trust between models. Makhija et al.  propose FedHeNN, which distills training information in the form of matching feature kernels instead of the predictions of local AI models on the public data. Both FedHeNN and DCL-NN utilize centered kernel alignment  to match feature kernels of local models, but DCL-NN uses the ensemble distillation for predictions as well. Thus, DCL-NN enables parties to learn from the entire input space.

Decentralized Learning with Kernel RegressionA number of studies have investigated the minimax optimal rate of regularized kernel regression algorithms such as kernel ridge regression and gradient descent-based kernel regression with early stopping [5; 15; 37; 65]. In particular, over the past decade, the growing interest in decentralized learning has led to active research in the generalization analysis of decentralized kernel regression. While divide-and-conquer algorithms [34; 38; 66; 70] play a significant role in this research flow, most of them fail to account for statistical heterogeneity and massively distributed cases, along with privacy preservation, which has received a lot of attention recently. On the other hand, decentralized kernel regression algorithms with multiple communication rounds [40; 43; 48; 58; 67] achieve superior theoretical results compared to the divide-and-conquer algorithms. However, the discussions of these algorithms primarily focus on the efficiency of resource costs [40; 43; 67], while research on relaxing environmental constraints has been scarce. For example, most of these works assume a limited number of parties to prove the optimality in a minimax sense.

To the best of our knowledge, [48; 58] stand as the only investigations that consider general decentralized environments. Similar to our work, Park et al.  study the convergence rate of distillation-based collaborative learning with kernel regression. However, their results demonstrate a weaker version of minimax optimality and do not cover statistically heterogeneous environments. In this regard, Su et al.  offer a promising methodology. They analyze nonparametric versions of FedAvg  and FedProx , representative FL algorithms involving model exchange, in general decentralized environments such as statistically heterogeneous and massively distributed scenarios. In this work, we extend their methodology to analyze FedMD [30; 41] from a nonparametric perspective in massively distributed statistically heterogeneous environments. We summarize the comparison between our work and prior studies in Table 1. Note that algorithms that do not employ Nystrom scheme (including nonparametric FedAvg ) fail to preserve local data privacy due to the inherent characteristics of kernel regression. On the other hand, DC-NY  and DKRR-NY-CM  can achieve the local data privacy preservation by utilizing the public data as Nystrom centers.

## 3 DCL-KR: A Nonparametric View of FedMD

In this section, we establish the theory of a nonparametric version of FedMD [30; 41], the most standard distillation-based collaborative learning algorithm.

### Preliminaries

Let \(_{,y}=_{}_{y|}\) be a Borel probability measure on \(\) where \(\) is a compact subset of \(^{d}\) and we assume the support of \(_{}\) is \(\). The goal of the regression problem is to find a minimizer of the population risk, i.e.,

\[_{h:}(h),(h) :=\ _{(,y)_{,y}}|y-h()|^{2}.\]

Then, the function \(f_{0}^{*}:\) defined by \(_{0}_{y_{y|}(|_{ 0})}[y]\), \(_{0}\) is a target function.

    & interaction & local data & massively & non-i.i.d.\& \\ Methods & method & privacy & distributed & unbalanced \\  DKRR  & divide-and-conquer & & & & \\ DC-NY  & divide-and-conquer & ✓ & & & \\  DKRR-CM  & model exchange & & & & \\ DKRR-RF-CM  & model exchange & & & & \\ DKRR-NY-CM  & model exchange & ✓ & & \\ nFedAvg  & model exchange & & ✓ & ✓ \\  IED*  & knowledge distillation & ✓ & ✓ & \\ DCL-KR (ours) & knowledge distillation & ✓ & ✓ & ✓ \\   

Table 1: Comparative analysis of decentralized environments for (nearly) minimax optimality of representative collaborative learning algorithms with kernel regression. nFedAvg indicates the nonparametric version of FedAvg in . Note that IED  achieves a weaker version of minimax optimality.

Let \(k:\) be a Mercer kernel  where \(:=(_{}k(,))^{1/2}<\) and \(_{k}\) be a reproducing kernel Hilbert space associated to \(k\). We set \(k_{}:=k(,)\) and the covariance operator \(T_{k,}:_{k}_{k}\) with respect to any Borel probability measure \(\) on \(\) defined as

\[T_{k,}h=_{}h()k_{}\ d().\]

Then we can see that \(T_{k,}=_{}^{}_{}\) where \(_{}:_{k} L_{}^{2}\) is a natural embedding, \(L_{}^{2}=L^{2}(,)\) denotes the \(L^{2}\) space, and a superscript \({}^{}\) denotes the adjoint operator of a given operator. We also define the sampling operator \(S_{D}:_{k}^{n}\) by \(h[h(^{1}),,h(^{n})]^{}\) and \(T_{k,X}:=S_{D}^{}S_{D}\) when \(D=\{(^{1},y^{1}),,(^{n},y^{n})\}\) with \(X=\{^{1},,^{n}\}\) is given. Since \(S_{D}\) depends only on data inputs \(X\), we can define the sampling operator for unlabeled datasets in the same way. See Appendix A.1 for further details.

#### 3.1.1 Kernel Gradient Descent with Early Stopping

Given a dataset \(D=\{(^{1},y^{1}),,(^{n},y^{n})\}\) generated from \(_{,y}\), consider the empirical risk \(}_{D}:_{k}\) given by

\[}_{D}(h)=\|S_{D}h-\|_{2}^{2}\]

where \(=[y^{1},,y^{n}]^{}\). Here, \(\|\|_{2}\) denotes a scaled Euclidean norm \(\|\|_{2}=(_{i=1}^{n}_{i}^{2})^{1/2}\). From the functional derivative \(}_{D}(h)=S_{D}^{}(S_{D}h-)\), the gradient descent scheme becomes

\[_{1}=0,_{t+1}=_{t}-_{t}S_{D}^{}(S_{D}_{t}- )(t=1,2,)\]

where \(\{_{t}\}_{t}\) is a set of learning rates. In this work, we set \(_{t}=\), \(t\) for a fixed \((0,1/^{2})\). Then, a simple calculation gives \(_{t} S_{D}^{}(S_{D}S_{D}^{})^{-1}\) as \(t\) provided that the operator \(S_{D}S_{D}^{}\) is invertible. The limit is known as the minimum norm interpolation  of \(D\). Since the interpolation regressor generalizes poorly unless there is no noise [32; 39], early stopping strategies are usually applied to avoid the overfitting issue. With adequate stopping rules, gradient descent-based kernel regression has an optimal rate in a minimax sense [36; 37; 65].

### Dcl-Kr Algorithm

From now on, we consider the setting that there are \(m\) parties and the \(i\)th party has a private local data \(D_{i}=\{(_{i}^{j},y_{i}^{j}):j=1,,n_{i}\}\) for \(i=1,,m\). Assume that all data \(D=_{i=1}^{m}D_{i}\) are i.i.d. with the distribution \(_{,y}\) but each local dataset does not need to have the same distribution. Let \(Z=\{^{1},,^{n_{0}}\}\) be the additional public inputs. The goal of all parties is to have their models that perform well on the distribution \(_{}\). In other words, each party expects to be able to make good predictions not only for its local data distribution but also for unseen data distribution through collaborative learning.

Similar to , we construct a nonparametric version of FedMD (called **DCL-KR**), which is presented in Algorithm 1. In Algorithm 1, \(_{i}\) is a one-step local gradient descent update on \(}_{D_{i}}\), i.e., \(_{i}h=h- S_{D_{i}}^{}(S_{D_{i}}h-_{i})\) where \(_{i}=[y_{i}^{1},,y_{i}^{n_{i}}]^{}\). Similarly, \(}_{t}\) is a one-step gradient descent update on \(}_{(Z,_{p,t})}\), i.e., \(}_{t}h=h- S_{Z}^{}(S_{Z}h-_{p,t})\).

### Theoretical Results

In this subsection, we show the nearly minimax optimality of DCL-KR. To derive theoretical results, we assume the following conditions regarding regularity of noise, the kernel \(k\), and the target function \(f_{0}^{*}\) as below.

**Assumption 3.1**.: We assume \(_{y_{y}}y^{2}<\) and

\[((^{*}()|}{M})-^{* }()|}{M}-1)\ d_{y|}(y|)}{2M^{2}},\]

where \(M\) and \(\) are positive constants.

**Assumption 3.2**.: Let \(_{1}_{2}>0\) be eigenvalues of \(T_{k,_{}}\). There are fixed positive constants \(C_{s}\) and \(c_{s}\) such that

\[c_{s}i^{-1/s}_{i} C_{s}i^{-1/s},\  i\]

for some \(s(0,1)\).

**Assumption 3.3**.: The target function \(f_{0}^{*}\) satisfies

\[f_{0}^{*}\{h_{k}:h=T_{k,_{}}^{r-1/2}g\ \|g\|_{_{k}} R\}\]

for some \(r[,1]\) where \(T_{k,_{}}^{r-1/2}\) is the \((r-1/2)\) power of operator \(T_{k,_{}}\) and \(R>0\) is a fixed constant. In particular, \(f_{0}^{*}_{k}\).

The above assumptions determine the minimax lower rate  and are standard assumptions in many prior works . In detail,

* Assumption 3.1 implies that the noise is not excessively large. This assumption is a general noise condition that encompasses a wide range of cases. For instance, noise with Bernstein condition such as sub-Gaussian noise satisfies Assumption 3.1.
* Assumption 3.2 is about the eigenvalue decay of \(T_{k,_{}}\). From this assumption, one can derive bounds on the effective dimension that is related to covering and entropy number conditions .
* Assumption 3.3 is related to the regularity of the target function, specifically how well the RKHS induced by the kernel \(k\) represents the target function.

Under these assumptions, we can theoretically show the performance guarantee of DCL-KR. The proof is provided in Appendix A.2. Note that \((h)-(f_{0}^{*})=\|_{_{}}( h-f_{0}^{*})\|_{L^{2}_{_{}}}^{2}\) is the excess risk of a regressor \(h\) and so the quantity \(\|_{_{}}(h-f_{0}^{*})\|_{L^{2}_{_{}}}\) indicates the generalization ability of \(h\).

**Theorem 3.4**.: _Under Assumption 3.1, 3.2, and 3.3, with \(n_{0} n^{}( n)^{3}\) public inputs independently generated from \(_{}\) such that the Radon-Nikodym derivative \(}}{d_{}}\) satisfies_

\[0}}{d_{}} BB[1,),\] (2)

_DCL-KR gives the performance guarantee_

\[\|_{_{_{}}}(f_{i,T}-f_{0}^{*})\|_{L_{_{ }}^{2}} C B^{r}n^{-} n\]

_for all \(i=1,,m\) where \((0,1/^{2})\) is a fixed learning rate, \(T\) is an adequate stopping rule, and the prefactor \(C\) does not depend on \(B\), \(m\), and \(n\)._

Since the convergence rate \(n^{-}\) is the minimax lower rate under Assumption 3.1, 3.2, and 3.3, Theorem 3.4 implies that DCL-KR has an almost same convergence rate as the minimax optimal central training when there are sufficiently many public inputs. To the best of our knowledge, this is the first work to prove the (nearly) minimax optimality of a collaborative learning algorithm that does not directly share local data or models in massively distributed statistically heterogeneous environments. For example, divide-and-conquer algorithms work for limited \(m\). Specifically, DCL-NY  assumes \(m O(n^{})\) and DKRR-NY-CM  assumes \(m O(n^{})\). However, Theorem 3.4 does not require any condition on \(m\). Moreover, Theorem 3.4 deals us more general setting than the theory in . For example, Su et al.  only cover \(r=\) of Assumption 3.3. On the other hand, Park et al.  do not consider Assumption 3.2 which gives a finer result. Compared with , we also reduce the required size of public inputs and drop the statistical homogeneity condition.

The convergence rate in Theorem 3.4 has an additional factor \( n\) compared with a minimax lower rate , but this logarithm term grows slower than any polynomial. Note that an additional logarithm term commonly appears in the context of gradient descent-based kernel regression with Nystrom scheme .

Theorem 3.4 allows that the public input distribution \(_{}\) can be different from the local input distribution \(_{}\). It is natural that the condition (2) is required since \(_{}\) should cover \(_{}\) for fully distilling training information. We can see that the discrepancy between \(_{}\) and \(_{}\) affects the upper bound in Theorem 3.4 as the multiplication of \(B^{r}\). We can remove \(B^{r}\) in the upper bound by increasing public inputs. See Appendix A.3 for details.

#### 3.3.1 Proof Sketch of Theorem 3.4 and Comments

In the proof of Theorem 3.4, we decompose the term \(_{_{}}(f_{i,T}-f_{0}^{*})\) into four parts, say (I), (II), (III), and (IV) (see Eq. (7)). The proof is to bound the norms of these terms. Note that DCL-KR can also be understood as a Nystrom version of nonparametric FedAvg  from the recurrence relation (6).

(I) and (II) appear similarly in , except that (I) and (II) incorporate projections. To handle these terms, we reinterpret the proof presented in  in operator form instead of matrix form and extend it to our setting. We obtain a norm bound of (II) containing a quantity linked to the local Rademacher complexity. (Appendix A.2.2 and A.2.3)

Comparing with , (III) and (IV) are additional terms induced by the procedure that distills functional information from the local regressors. We apply techniques used in  to bound (III) and (IV). (Appendix A.2.4)

Note that previous works applying local Rademacher complexity-based stopping rule  deal with the case of \(r=\) only. In this work, we set a new stopping rule \(T\) which is an extension of previous works  and prove an extended version (Lemma A.6) of a well-known property . As a result, our theory covers \(r[,1]\) which affects the minimax lower rate. (Appendix A.2.5)

## 4 DCL-NN Algorithm

In this section, we retain the problem setting from Section 3 but employ heterogeneous neural networks as the local models. Based on the theoretical results in Section 3, we propose a noveldistillation-based collaborative learning algorithm **DCL-NN** across heterogeneous neural networks in a decentralized setting.

A key factor contributing to the successful theoretical guarantee of DCL-KR lies not only in the linearity of kernel regression but also in the equality of kernels across local models. In fact, the public data predictions can vary in different directions, even if the same training data points are used when kernels differ (See Appendix B). Therefore, we match the kernels of local AI models. Specifically, we use linear feature kernels [18; 64] induced by the features from the last hidden layers of local AI models for kernel matching. For example, for a neural network \(f:\) where \(f()=^{}g()+b\), \(g:^{c}\), \(^{c}\), and \(b\) we use

\[k_{f}(^{1},^{2})=g(^{1})^{}g(^{2}),^{1},^{2}\] (3)

as the feature kernel of \(f\). Through this idea, we can bring the setting closer to the regime of DCL-KR. Note that our theoretical results suggest that the target kernel should be a good kernel. Indeed, we observe that the naive ensemble

\[k=_{i=1}^{m}}{n}k_{f_{i}}.\] (4)

has a significantly better performance than individual feature kernels \(k_{f_{1}},,k_{f_{m}}\) (See Section 5 and Appendix B). Here, \(f_{i}\) is the local model of the \(i\)th party with its local feature kernel \(k_{f_{i}}\) obtained by (3) (\(i=1,,m\)). Therefore, we align local feature kernels \(k_{f_{1}},,k_{f_{m}}\) in a kernel distillation manner with the ensemble kernel \(k\) obtained by (4).

For this purpose, we introduce Centered Kernel Alignment (CKA)  as a kernel similarity measure. CKA is a typical measure associated with the similarity of two representations of neural networks  and is often used for kernel matching in neural networks . To compute empirical CKA between two kernels \(k_{1}\) and \(k_{2}\) on inputs \(\{^{},,^{p}\}\), we first calculate the Gram matrices \(K_{1}=[k_{1}(^{j_{1}},^{j_{2}})]_{1 j_{1},j_{2} p}\) and \(K_{2}=[k_{2}(^{j_{1}},^{j_{2}})]_{1 j_{1},j_{2} p}\). We then compute the empirical CKA via

\[}(k_{1},k_{2})=}(K_{1},K_{2})}{ }(K_{1},K_{1})}(K_{2},K_{2})}}.\]

Here, \(}\) is an estimator of the Hilbert-Schmidt Independence Criterion (HSIC) defined as

\[}(K_{1},K_{2})=}(K_{1}HK_{2}H)\]

where \(H:=I_{p}-^{}\) is the centering matrix. In the kernel distillation procedure, the \(i\)th local party maximizes \(}(k_{f_{i}},k)\) on public inputs \(Z\) (\(i=1,,m\)). Notably, this procedure requires only a single communication round for exchanging pairwise feature kernel values on public inputs, ensuring that our algorithm operates exclusively within the function space.

After the kernel distillation procedure, all local AI models have similar feature kernels up to constants. So we can follow an analogous process as in DCL-KR. Note that we perform learning rate scaling described in Appendix B to compensate the kernel scale difference. It makes the impact of local iterations consistent. We also provide the complete algorithm (Algorithm 2) and further details for Section 4 in Appendix B.

## 5 Experiments

In this section, we evaluate the performance of DCL-KR and DCL-NN. We compare them with baselines on various regression tasks.

DatasetsWe use the following six regression datasets to evaluate the performance. Target variables are one-dimensional in all datasets. (1) **Toy-1D** and (2) **Toy-3D** are synthetic datasets with one-dimensional and three-dimensional inputs, respectively. (3) **Energy** is a tabular dataset from the UCI database  to predict appliances energy use with 28 features. (4) **RotatedMNIST** is an image dataset where it aims to predict the rotation angles for given rotated images of the MNIST  images. (5) **UTKFace** and (6) **IMDB-WIKI**[42; 52] are image datasets for age estimation.

We compare kernel machine-based collaborative learning algorithms on two datasets Toy-1D and Toy-3D. On the other hand, we compare neural network-based collaborative learning algorithms on five datasets Toy-3D, Energy, RotatedMNIST, UTKFace, and IMDB-WIKI.

BaselinesWe compare DCL-KR with two central kernel regression models to verify our theoretical results. These two central models have the minimax optimal convergence rate. We also utilize existing decentralized kernel regression algorithms that does not directly share local data and models (DC-NY , DKRR-NY-CM , IED ) as baselines for DCL-KR. On the other hand, we adopt FedMD with unlabeled public inputs , FedHeNN , and KT-pFL  as baselines for DCL-NN.

SetupThe number of parties ranges from 10 to 100 for kernel machine-based algorithms and is 50 for neural network-based algorithms. We construct statistically heterogeneous decentralized environments with Algorithm 3. For neural network-based algorithms, we use 4 different neural network architectures for local models in all settings. For instance, we use ResNet-18, ResNet-34, ResNet-50 , and MobileNetv2  for large-scale image datasets. We utilize the average of Root Mean Squared Errors (RMSEs) of the local AI models on a test dataset as a performance metric. The test data points have the same distribution as the whole local data distribution. We apply FedMD with a few communication rounds for pretraining of DCL-NN. See Appendix C for detailed experimental configurations.

### Results on Kernel Machine-based Algorithms

The performance of DCL-KR and its baselines is presented in Figure 1. We set the number of parties \(m=10,20,,100\), the number of private data points \(n=50m\), and the number of public inputs \(n_{0}=n^{}(_{10}n)^{3}\). We first set \(_{}=_{}\), i.e., the public data distribution is the same as the entire local input distribution. As shown in Figure 1, DCL-KR outperforms the baselines in all

Figure 1: Performance of central Kernel Ridge Regression (centralKRR), central Kernel Regression with Gradient Descent (centralKRGD), DC-NY, DKRR-NY-CM, IED, and DCL-KR on Toy-1D and Toy-3D

Figure 2: Performance of IED and DCL-KR with \(n_{0} n^{}(_{10}n)^{3}\) on Toy-3D

experimental settings and achieves comparable performance to the central models. This result implies that DCL-KR has not only the nearly optimal convergence rate but also the same performance as central kernel regression models. In contrast, DC-NY and DKRR-NY-CM exhibit significantly lower performance compared with DCL-KR in massively distributed environments where their theory does not cover. IED does not show a significant performance drop in massively distributed environments even though its theory is built on the statistical homogeneity condition of local data distributions.

To further compare the performance of DCL-KR and IED, which show similar results to central models, we analyze the effect of \(n_{0}\) and \(_{}\) on their performance (Figure 2 and 3). Figure 2 illustrates that, as expected from the theoretical results, IED requires more public inputs than DCL-KR to achieve good performance. Moreover, when there is a public distribution shift, DCL-KR maintains its convergence rate, whereas the convergence rate of IED deteriorates. (See Appendix C.3.3 for experimental details.) Overall, our experiments validate the theoretical results of DCL-KR and demonstrate its superiority over previous results. For additional experimental results and analyses, please refer to Appendix C.3.

### Results on Neural Network-based Algorithms

Table 2 shows the performance of DCL-NN and baselines on five regression tasks. We also present the performance of standalone models and centralized models to assess the performance of the collaborative algorithms. For some cases exhibiting training instability, we report the best test error (marked with asterisks) observed across all communication rounds, while relying on a fixed number of communication rounds for the other cases.

As can be seen in Table 2, DCL-NN outperforms the baselines on all regression tasks. Note that FedHeNN employs kernel matching similar to DCL-NN, but it lacks supervision of label prediction through collaboration, resulting in insufficient performance improvement compared to standalone models. Given the superior performance of DCL-NN, it is evident that incorporating supervised learning for label prediction alongside kernel matching is desirable. On the other hand, while FedMD performs significantly better than standalone models, the performance of DCL-NN is consistently better. Considering that we utilize FedMD for pretraining of DCL-NN, we can see that it performs better than FedMD-only collaborative learning by first training local models with FedMD and then using DCL-NN. In conclusion, the experimental results support the practical effectiveness and superiority of DCL-NN over baselines.

Kernel Distillation ProcedureTo verify the necessity of kernel distillation, we examine the changes in the performance of local feature kernels and the CKA between them during the kernel distillation procedure. We conduct this experiment on UTKFace. We utilize the RMSE of a kernel linear regression model trained on all local data as a kernel performance measure. The results are presented in Figure 4. As shown in Figure 4, both kernel performance and CKA undergo a temporary degradation due to the change of the objective function at the initial stages. However, as training progresses, both metrics recover and kernel performance surpasses its initial level. Since kernel distillation aims to ensure that all local feature kernels are similar with high performance, the experimental results verify the effectiveness of kernel distillation.

For additional experimental results, please refer to Appendix C.4.

## 6 Conclusions

In this work, we analyze distillation-based collaborative learning from a nonparametric perspective and propose DCL-NN, a practical algorithm as an extension. We demonstrate that DCL-KR, a nonparametric version of FedMD, has a nearly minimax optimal convergence rate in massively distributed statistically heterogeneous environments. Inspired by DCL-KR, we propose DCL-NN, a novel distillation-based collaborative learning algorithm for heterogeneous neural networks. Our experiments confirm the theoretical results of DCL-KR and demonstrate the practical effectiveness of DCL-NN. For a discussion of the limitations of our work, please refer to Appendix D.

Broader ImpactOur work explores the methodologies of collaborative learning under data and model privacy preservation. In this regard, our research holds the potential to positively impact the facilitation of collaboration among AI models without raising concerns about information disclosure. On the other hand, our work does not pose any particularly noteworthy negative consequences, given its aim to contribute to the advancement of the general field of machine learning.