# Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations

Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations

Hao Chen\({}^{1}\)

haoc3@andrew.cmu.edu

Ankit Shah\({}^{1}\)

Jindong Wang\({}^{2}\)

Ran Tao\({}^{1}\)

Yidong Wang\({}^{3}\)

Xiang Li\({}^{1}\)

Xing Xie\({}^{2}\)

Masashi Sugiyama\({}^{4,5}\)

Rita Singh\({}^{1}\)

Bhiksha Raj\({}^{1,6}\)

\({}^{1}\)Carnegie Mellon University

\({}^{2}\)Microsoft Research

\({}^{3}\)Peking University

\({}^{4}\) RIKEN AIP

\({}^{5}\)The University of Tokyo

\({}^{6}\)MBZUAI

###### Abstract

Learning with reduced labeling standards, such as noisy label, partial label, and supplementary unlabeled data, which we generically refer to as _imprecise_ label, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables. Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings, with closed-form learning objectives derived from the unified EM modeling. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first practical and unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain. Code is available at: [https://github.com/Hhhhhhao/General-Framework-Weak-Supervision](https://github.com/Hhhhhhao/General-Framework-Weak-Supervision).

## 1 Introduction

One of the critical challenges in machine learning is the collection of annotated data for model training . Ideally, every data instance would be fully annotated with precise labels. However, collecting such data can be expensive, time-consuming, and error-prone. Often, the labels can be intrinsically difficult to ascertain precisely. Factors such as a lack of annotator expertise and privacy concerns can also negatively affect the quality and completeness of the annotations.

In an attempt to circumvent this limitation, several methods have been proposed to permit model learning from the data annotated with reduced labeling standards, which are generally easier to obtain. We will refer to such labels as _imprecise_. Fig. 1 illustrates some typical mechanisms of label imprecision that are commonly addressed in the literature. Label imprecision requires a modification of the standard supervised training mechanism to build models for each specific case. For instance, _partial label learning_ (PLL)  allows instances to have a set of candidate labels, instead of a single definitive one. _Semi-supervised Learning_ (SSL)  seeks to enhance the generalization ability when only a small set of labeled data is available, supplemented by a larger unlabeled set.

_Noisy label learning_ (NLL)  deals with noisy scenarios where the labels are corrupted or incorrect. There is a greater variety of other forms of label imprecision, including crowd-sourcing , programmable weak supervision , and bag-level supervision , among others.

While prior arts have demonstrated success in handling individual configurations of label imprecision, their approaches often differ substantially. They are tailored to a _specific_ form of imprecision, as depicted in Fig. 2. Such specificity not only imposes the necessity of devising a solution for emerging types of label imprecision scenarios, but also complicates the deployment in practical settings, where the annotations can be highly complex and may involve _multiple coexisting and interleaved_ imprecision configurations. For instance, considering a scenario where both noisy labels and partial labels appear together, it might be challenging to adapt previous methods in NLL or PLL to this scenario since they either rely on the assumption of definite labels  or the existence of the correct label among label candidates , thus requiring additional algorithmic design. In fact, a few recent works have attempted to address the combinations of imprecise labels in this way, such as partial noisy label  and semi-supervised partial label learning . However, simply utilizing a more sophisticated or ad-hoc design can hardly scale to other settings. In addition, most of these approaches attempt to infer the correct labels given the imprecise information (_e.g._ through consistency with adjacent data , iterative refinement , average over given labels , etc., to train the model, which inevitably accumulates error during training.

In this paper, we formulate the problem from a different perspective: rather than taking the imprecise label information provided as a potentially noisy or incomplete attempt at assigning labels to instances, we treat it generically as the information that imposes a deterministic or statistical restriction of the actual applicable true labels. We then train the model over the distribution of all possible labeling entailed by the given imprecise information. More specifically, for a dataset with samples \(X\) and imprecise label information \(I\), we treat the inaccessible full and precise labels \(Y\) as a latent variable. The model is then trained to maximize the likelihood of the provided information \(I\). Since the likelihood computed over the joint probability \(P(X,I;)=_{Y}P(X,I,Y;)\) must marginalize out \(Y\), the actual information \(I\) provided could permit a potentially exponential number of labeling. To deal with the resulting challenge of maximizing the logarithm of an expectation, we use the common approach of _expectation-maximization_ (EM) , where the E-step computes the expectation of \(P(X,I,Y;)\) given the posterior of current belief \(P(Y|X,I;^{t})\) at time step \(t\) and the M-step maximizes the tight variational lower bound over \(P(X,I;)\). The overall framework is thus largely agnostic to the various nature of label imprecision, with the imprecise label only affecting the manner in which the posterior \(P(Y|X,I;^{t})\) is computed. In fact, current approaches designed for various imprecise label scenarios can be treated as specific instances of our framework. Our approach can serve as a solution towards a _unified and generalized_ view for learning with _various_ imprecise labels.

While there exist earlier attempts on generalized or EM solutions for different (other) imprecise supervisions or fuzzy observations , they usually require additional assumptions and approximations on the imprecise information for learnablility , thus presenting limited scalability on practical settings . On the contrary, the unified framework we propose subsumes all of these and naturally extends to the more practical "mixed" style of data, where different types of imprecise labels coexist. Moreover, for noisy labels, our framework inherently enables the learning of a _noise model_, as we will show in Section 3.2. Through comprehensive experiments, we demonstrate that the proposed imprecise label learning (ILL) framework not only outperforms previous methods

Figure 1: Illustration of the full label and imprecise label configurations. We use an example dataset of 4 training instances and 3 classes. (a) Full label, the annotation is a single true label; (b) Partial label, the annotation is a label candidate set containing true label; (c) Semi-supervised, only part of the dataset is labeled, and the others are unlabeled; (d) Noisy label, the annotation is mislabeled.

for dealing with single imprecise labels of PLL, NLL, and SSL, but also presents robustness and effectiveness for mixed imprecise label learning (MILL) settings, leveraging the full potential to more challenging scenarios. Our contributions are summarized as follows:

* We propose an EM framework towards the unification of learning from _various_ imprecise labels.
* We establish scalable and consistent state-of-the-art (SOTA) performance with the proposed method on partial label learning, semi-supervised learning, and noisy label learning, demonstrating our method's robustness in more diverse, complex label noise scenarios.
* To the best of our knowledge, our work is the first to show the robustness and effectiveness of a single unified method for handling the mixture of various imprecise labels.

## 2 Preliminary

In this section, we illustrate the notations and baselines from different imprecise label settings that adopt various solutions. We will show later how our proposed method generalize and subsumes these prior arts. Let \(\) denote the input space, and \(=[C]:=\{1,,C\}\) represent the label space with \(C\) distinct labels. A fully annotated training dataset of size \(N\) is represented as \(=\{(_{i},y_{i})\}_{i[N]}\). Learning with imprecise labels involves approximating the mapping function \(f g:\) from a training dataset where the true label \(y\) is not fully revealed from the annotation process. Here \(f\) is the backbone for feature extraction, \(g\) refers to the classifier built on top of the features, and the output from \(f g\) is the predicted probability \((y|;)\), where \(\) is the learnable parameter for \(f g\). In this study, we primarily consider three imprecise label configurations (as illustrated in Fig. 1) and their corresponding representative learning paradigms (as shown in Fig. 2), namely partial label learning, semi-supervised learning, and noisy label learning.

**Partial label learning (PLL)**. PLL aims to learn with a candidate label set \(\), where the ground truth label \(y\) is concealed in \(\). The training data for partial labels thus becomes \(_{}=\{(_{i},_{i})\}_{i[N]}\). PICO  is a recent contrastive method that employs class prototypes to enhance label disambiguation (as shown in Fig. 2(a)). It optimizes the cross-entropy (CE)2 loss between the prediction of the augmented training sample \(_{w}()\) and the disambiguated labels \(}\). PiCO learns a set of class prototypes from the features associated with the same pseudo-targets. A contrastive loss, based on MOCO , is employed to better learn the feature space, drawing the projected and normalized features \(_{}\) and \(_{}\) of the two augmented versions of data \(_{}()\) and \(_{}()\)3 closer. The objective of PiCO is formulated as:

\[_{}=_{}((y| _{}();),})+ _{}(_{},_{},). \]

**Semi-supervised learning (SSL)**. For SSL, we can define the labeled dataset as \(_{}^{}=\{(_{i}^{},y_{i}^{ })\}_{i[N^{};}\), and the unlabeled dataset as \(^{}=\{_{j}^{}\}_{j[N^{} +1,N^{}+N^{}]}\), with \(N^{} N^{}\). A general confidence-thresholding based self-training [53; 18] pipeline for SSL is shown in Fig. 2(b). Consider FixMatch  as an example; there are usually two loss components: the supervised CE loss on labeled data and the unsupervised CE loss on unlabeled data. For the unsupervised objective, the pseudo-labels \(^{}\) from the network itself are used to train on the unlabeled data. A "strong-weak" augmentation  is commonly adopted. To ensure the quality of the pseudo-labels,

Figure 2: Baseline model pipelines for various imprecise label configurations. (a) PiCO  for partial label learning. (b) FixMatch  for semi-supervised learning. (c) SOP  for noisy label learning. (d) The proposed unified framework. It accommodates _any_ imprecise label configurations and also mixed imprecise labels with an EM formulation.

only the pseudo-labels whose confidence scores \(^{}\) are greater than a threshold \(\) are selected to participate in training:

\[_{}=_{}((y|_{}(^{});),y^{})+(^{})_{}( (y|_{}(^{});), ^{}). \]

**Noisy label learning (NLL)**. NLL aims at learning with a dataset of corrupted labels, \(_{}=\{(_{i},_{i})\}_{i[N]}\). We illustrate the NLL pipeline (in Fig. 2(c)) with the recent sparse over-parameterization (SOP) model , where a sparse _noise model_ consisting of parameters \(_{i},_{i}[-1,1]^{C}\) for each sample is adopted. The noise model transforms the network prediction from the true label distribution into the noisy label distribution. A CE loss and a mean-squared-error (MSE) loss optimize parameter \(\{_{i}\}\) and \(\{_{i}\}\) respectively:

\[_{}=_{}((( y|_{}();)+), )+_{}((y|_{}( );)+,), \]

where \(\) denotes the \(L_{}\) normalization and \(_{i}=_{i}_{i}}_{i}^{ }-_{i}_{i}(1-}_{i }^{})\), with \(}_{i}^{}\) referring to the one-hot version of \(y_{i}\). Consistency regularization with strong-weak augmentation and entropy class-balance regularization are additionally utilized for better performance in SOP .

## 3 Imprecise Label Learning

Although current techniques demonstrate potential in addressing particular forms of imprecise labels, they frequently fall short in adaptability and transferability to more complicated and more realistic scenarios where multiple imprecise label types coexist and interleave. This section first defines the proposed expectation-maximization (EM) formulation for learning with various imprecise labels. Then, we demonstrate that our unified framework seamlessly extends to partial label learning, semi-supervised label learning, noisy label learning, and the more challenging setting of mixed imprecise label learning. Connections and generalization to previous pipelines can also be drawn clearly under the proposed EM framework.

### A Unified Framework for Learning with Imprecise Labels

**Exploiting information from imprecise labels**. The challenge of learning with imprecise labels lies in learning effectively with inaccurate or incomplete annotation information. Per the analysis above, prior works catering to specific individual imprecise labels either explicitly or implicitly attempt to infer the precise labels from the imprecise label information. For example, partial label learning concentrates on the disambiguation of the ground truth label from the label candidates [13; 71; 50] or averaging equally over the label candidates . In semi-supervised learning, after the model initially learns from the labeled data, the pseudo-labels are treated as correct labels and utilized to conduct self-training on the unlabeled data [73; 18]. Similarly, for noisy label learning, an integral part that helps mitigate overfitting to random noise is the implementation of an accurate noise model capable of identifying and rectifying the incorrect labels [33; 69], thereby ensuring the reliability of the learning process. However, inferring the correct labels from the imprecise labels or utilizing the imprecise labels directly can be very challenging and usually leads to errors accumulated during training [73; 74], which is also known as the confirmation bias. In this work, we take a different approach: we consider all possible labeling along with their likelihood that the imprecise labels fulfill to train the model, rather than using a single rectified label from the imprecise information. Such an approach also eliminates the requirements for designing different methods for various imprecise labels and provides a unified formulation instead, where closed-form solutions can be derived.

**A unified framework for learning with imprecise labels (ILL)**. Let \(\{_{i}\}_{i[N]}\) represent the features as realizations from \(X\) and \(\{y_{i}\}_{i[N]}\) represent their precise labels as realizations from \(Y\) for the training data. Ideally, \(Y\) would be fully specified for \(X\). In the imprecise label scenario, however, \(Y\) is not provided; instead we obtain imprecise label information \(I\). We view \(I\) not as _labels_, but more abstractly as a variable representing the _information_ about the labels. From this perspective, the actual labels \(Y\) would have a distribution \(P(Y|I)\), and \(I\) can present in various forms. When the information \(I\) provided is the precise true label of the data, \(P(Y|I)\) would be a delta distribution, taking a value \(1\) at the true label, and \(0\) elsewhere. If \(I\) represents partial labels, then \(P(Y|I)\) would have non-zero value over the candidate labels, and be \(0\) elsewhere. When \(I\) represents a set of noisy labels, \(P(Y|I)\) would represent the distribution of the true labels, given the noisy labels. When \(I\) does not contain any information, i.e., unlabeled data, \(Y\) can take any value.

By the maximum likelihood estimation (MLE) principle, we must estimate the model to maximize the likelihood of the datainformation we have been provided, namely \(X\) and \(I\). Let \(P(X,I;)\) represent a parametric form for the joint distribution of \(X\) and \(I\)4 Explicitly considering the labels \(Y\), we have \(P(X,I;)=_{Y}P(X,Y,I;)\). The maximum likelihood principle requires us to find:

\[^{*}=*{arg\,max}_{}\  P(X,I;)=*{ arg\,max}_{}\ _{Y}P(X,Y,I;), \]

with \(^{*}\) denotes the optimal value of \(\). Eq. (4) features the log of an expectation and cannot generally be solved in closed-form, and requires iterative hill-climbing solutions. Of these, arguably the most popular is the expectation-maximization (EM) algorithm , which iteratively maximizes a tight variational lower bound on the log-likelihood. In our case, applying it becomes:

\[^{t+1}&=*{arg\, max}_{}*{}_{Y|X,I;^{t}}[ P(X,Y,I; )]\\ &=*{arg\,max}_{}*{}_{Y| X,I;^{t}}[ P(Y|X;)+ P(I|X,Y;)], \]

where \(^{t}\) is the \(t^{}\) estimate of the optimal \(\). Note that \(P(X;)\) is omitted from Eq. (5) since \(P(X)\) does not rely on \(\). The detailed derivation of the variational lower bound is shown in Appendix C.1. There are several implications from Eq. (5). (i) The expectation over the posterior \(P(Y|X,I;^{t})\) equates to considering _all_ labeling entailed by the imprecise label information \(I\), rather than any single (possibly corrected) choice of label. For independent instances setting mostly studied in this paper, we can derive closed-form training objectives from this formulation as shown in Section 3.2. (ii) The property of the second term \( P(I|X,Y;)\) is dependent on the nature of imprecise label \(I\). If \(I\) is derivable from true labels \(Y\), such as the actual labels or the label candidates, it can be reduced to \(P(I|Y)\), _i.e._, the probability of \(I\) is no longer dependent on \(X\) or \(\) and thus can be ignored from Eq. (5). If \(I\) represents the noisy labels, \(P(I|X,Y;)\) instead includes a potentially learnable noise model. (iii) It is a general framework towards the unification of any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc. In this work, we specialize the proposed EM framework to PLL, SSL, NLL, and the mixture of them in the following.

### Instantiating the Unified EM Formulation

We illustrate how to seamlessly expand the formulation from Eq. (5) to partial label learning, semi-supervised learning, noisy label learning, and mixture settings, with derived closed-form loss function5 for each setting here. The actual imprecise labels only affect the manner in which the posterior \(P(Y|X,I;^{t})\) is computed for each setting. We show that all learning objectives derived from Eq. (5) naturally include a consistency term with the posterior as the soft target. We also demonstrate that the proposed unified EM framework closely connects with the prior arts, which reveals the potential reason behind the success of these techniques. Note that while we only demonstrate the application of the proposed framework to four settings here, it can also be flexibly extended to other settings. More details of derivation below are shown in Appendix C.

**Partial label learning (PLL)**. The imprecise label \(I\) for partial labels is defined as the label candidate sets \(S\) containing the true labels. These partial labels indicate that the posterior \(P(Y|X,S;^{t})\) can only assign its masses on the candidate labels. Since \(S\) can be derived from true labels \(Y\), \(P(S|X,Y;)\) reduces to \(P(S|Y)\), and thus can be ignored. We also demonstrate with instance dependent partial labels that maintains \(P(S|X,Y;)\) in Appendix D.2.2. Defining the label candidates as \(\{_{i}\}_{i[N]}\) and substituting it in Eq. (5), we have the loss function of PLL derived using ILL framework:

\[_{}^{}=-_{Y[C]}P(Y|X,S;^{t})  P(Y|X;)_{}((y|_{}();),(y|_{}( ),;^{t})), \]

where \((y|_{}(),;^{t})\) is the normalized probability that \(_{k C}p_{k}=1\), and \(p_{k}=0, k\). Eq. (6) corresponds exactly to consistency regularization , with the normalized predicted probability as the soft pseudo-targets. We use \(_{s}\) and \(_{w}\) to denote the strong and weak augmentation as stated earlier. This realization on PLL shares similar insights as  which exploits a gradually induced loss weight for PLL on multiple augmentations of the data. However, our framework is much simpler and more concise as shown in Appendix D.2.2, which does not require additional techniques.

**Semi-supervised learning (SSL)** In SSL, the input \(X\) consists of the labeled data \(X^{ L}\) and the unlabeled data \(X^{ U}\). The imprecise label for SSL is realized as the limited number of full labels \(Y^{ L}\) for \(X^{ L}\). The labels \(Y^{ U}\) for unlabeled \(X^{ U}\) are unknown and become the latent variable. Interestingly, for the unlabeled data, there is no constraint on possible labels it can take. The posterior \(P(Y^{ U}|X^{ L},X^{ U},Y^{ L};)\), which is the actual prediction from the network, can be directly utilized as soft targets for self-training. Since \(Y^{ L}\) is conditionally independent with \(Y^{ U}\) given \(X\), the second term of Eq. (5): \(P(Y^{ L}|X^{ L},X^{ U},Y^{ U};)\), is reduced to \(P(Y^{ L}|X^{ L};)\), which corresponds to the supervised objective on labeled data. The loss function for SSL thus becomes:

\[^{ SSL}_{ ILLL}&=- _{Y[C]}P(Y^{ U}|X^{ U},X^{ L},Y^{ L};^{t}) P(Y^{ U }|X^{ U},X^{ L};)- P(Y^{ L}|X^{ L};)\\ &_{ CE}((y|_{ s}( ^{ u});),(y|_{ w}(^{ u });^{t}))+_{ CE}((y|_{ w }(^{ l});),y^{ l}) \]

The first term corresponds to the unsupervised consistency regularization usually employed in SSL, and the second term refers to the supervised CE loss only on labeled data. Eq. (7) has several advantages over the previous methods. It adopts the prediction as soft-targets of all possible labeling on unlabeled data, potentially circumventing the confirmation bias caused by pseudo-labeling and naturally utilizing all unlabeled data which resolves the quantity-quality trade-off commonly existing in SSL . It also indicates that previous pseudo-labeling with confidence threshold implicitly conducts the EM optimization, where the maximal probable prediction approximates the expectation, and the degree of the approximation is determined by the threshold \(\), rationalizing the effectiveness of dynamic thresholding.

**Noisy label learning (NLL)**. Things become more complicated here since the noisy labels \(\) do not directly reveal the true information about \(Y\), thus \(P(|Y,X;)\) inherently involves a noise model that needs to be learned. We define a simplified instance-independent6 noise transition model \((|Y;)\) with parameters \(\), and take a slightly different way to formulate the loss function for NLL from the ILL framework:

\[^{ NLL}_{ ILLL}&=- _{Y[C]}P(Y|X,;^{t},^{t}) P(Y|X,;, ^{t})- P(|X;,)\\ &_{ CE}((y|_{ s }(),;,^{t}),(y|_{ w}( ),;^{t},^{t}))+_{ CE}( (|_{ w}();,), ), \]

where the parameters \(\) and \(\) are learned end-to-end. The first term corresponds to the consistency regularization of prediction conditioned on noisy labels and the second term corresponds to the supervised loss on noisy predictions that are converted from the ground truth predictions. Both quantities are computed using the noise transition model given the noisy label \(\):

\[(y|,;,^{t})(y|;)(|y;^{t}),\;(| ;,)=_{y[C]}(y|;) (|y;). \]

**Mixture imprecise label learning (MILL)**. We additionally consider a more practical setting, mixture of imprecise label learning, with partial labels, noisy labels, and unlabeled data interleaved together. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of SSL as shown in Eq. (7). The labeled data here present partial and noisy labels \(}\). Thus the noisy supervised objective in Eq. (9) becomes the supervised consistency regularization as in Eq. (6) of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels. Thus we have the loss function for MILL derived as:

\[^{ MILL}_{ ILLL}&= _{ CE}((y_{ s}( ^{ l}),}^{ l};,),(y _{ w}(^{ l}),}^{ l};^{t },^{t}))\\ &+_{ CE}((_{ w}(^{ l});,),}^{ l} )\\ &+_{ CE}((y|_{ s}( ^{ n});),(y|_{ w}(^{ u });^{t})) \]We can compute both quantity through the noise transition model:

\[(y|,};,^{t})(y| ;)_{}}(y|; ^{t}),\;(|;,)=_{y[C]} (y|;)(|y;). \]

## 4 Experiments

In this section, we conduct extensive experiments to evaluate ILL. Albeit simple, the ILL framework achieves comparable state-of-the-art performance regarding previous methods on partial label learning, semi-supervised learning, and noisy label learning. Moreover, our experiments show that ILL could be easily extended to a more practical setting with a mixture of various imprecise label configurations. For all settings, we additionally adopt an entropy loss for balancing learned cluster sizes [75; 76], similarly as [69; 22]. Experiments are conducted with three runs using NVIDIA V100 GPUs.

### Partial Label Learning

**Setup**. Following , we evaluate our method on partial label learning setting using CIFAR-10 , CIFAR-100 , and CUB-200 . We generate partially labeled datasets by flipping negative labels to false positive labels with a probability \(q\), denoted as a partial ratio. The \(C-1\) negative labels are then uniformly aggregated into the ground truth label to form a set of label candidates. We consider \(q\{0.1,0.3,0.5\}\) for CIFAR-10, \(q\{0.01,0.05,0.1\}\) for CIFAR-100, and \(q=0.05\) for CUB-200. We choose six baselines for PLL using ResNet-18 : LWS , PRODEN , CC , MSE and EXP , and PiCO . The detailed hyper-parameters, comparison with the more recent method R-CR  that utilizes a different training recipe and model , and comparison with instance-dependent partial labels  are shown in Appendix D.2.2.

**Results**. The results for PLL are shown in Table 1. Our method achieves the best performance compared to the baseline methods. Perhaps more surprisingly, on CIFAR-10 and CIFAR-100, our method even outperforms the fully-supervised reference, indicating the potential better generalization capability using the proposed framework, sharing similar insights as in Wu et al. . While PiCO adopts a contrastive learning objective, our method still surpasses PiCO by an average of \(\%\) on CIFAR-10 and \(\%\) on CIFAR-100. Our approach can be further enhanced by incorporating contrastive learning objectives, potentially leading to more significant performance.

### Semi-Supervised Learning

**Setup**. For experiments of SSL, we follow the training and evaluation protocols of USB  on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select \(l/C\) samples from each class and treat the remaining samples as the unlabeled dataset. We present the results on CIFAR-100 and STL-10  for image classification, and IMDB  and Amazon Review  for text classification. We compare with the current methods with confidence thresholding, such as FixMatch , AdaMatch , FlexMatch , FreeMatch , and SoftMatch . We also compare with methods with the contrastive loss, CoMatch  and SimMatch . A full comparison of the USB datasets and hyper-parameters is shown in Appendix D.3.

**Results**. We present the results for SSL on Table 2. Although no individual SSL algorithm dominates the USB benchmark , our method still shows competitive performance. Notably, our method

   Dataset &  &  &  \\  Partial Ratio \(q\) & 0.1 & 0.3 & 0.5 & 0.01 & 0.05 & 0.1 & 0.05 \\  Fully-Supervised &  &  &  \\  LWS  & 90.30\(\)0.80 & 88.99\(\)1.43 & 86.16\(\)0.85 & 65.78\(\)0.02 & 59.56\(\)0.33 & 53.53\(\)0.08 & 39.74\(\)0.47 \\ PRODEN  & 90.24\(\)0.32 & 89.38\(\)0.31 & 87.78\(\)0.07 & 62.60\(\)0.02 & 60.73\(\)0.03 & 56.80\(\)0.29 & 62.56\(\)0.10 \\ CC  & 82.30\(\)0.21 & 79.08\(\)0.47 & 74.05\(\)0.35 & 49.76\(\)0.45 & 47.62\(\)0.08 & 35.72\(\)0.42 & 55.51\(\)0.02 \\ MSE  & 79.97\(\)0.48 & 75.65\(\)0.26 & 67.09\(\)0.06 & 49.17\(\)0.40 & 66.02\(\)1.42 & 43.81\(\)0.19 & 22.07\(\)2.36 \\ EXP  & 79.23\(\)0.10 & 75.79\(\)0.21 & 70.34\(\)0.32 & 44.45\(\)1.50 & 41.05\(\)1.40 & 29.27\(\)2.81 & 9.44\(\)2.32 \\ PiCO  & 94.39\(\)0.18 & 94.18\(\)0.12 & 92.58\(\)0.06 & 73.09\(\)0.34 & 72.74\(\)0.30 & 69.91\(\)0.24 & **72.17\(\)0.72** \\  Ours & **96.37\(\)0.08** & **96.26\(\)0.03** & **95.16\(\)0.05** & **75.31\(\)0.19** & **74.58\(\)0.03** & **74.00\(\)0.02** & 70.77\(\)0.29 \\   

Table 1: Accuracy of different partial ratio \(q\) on CIFAR-10, CIFAR-100, and CUB-200 for **partial label learning**. The best and the second best results are indicated in **bold** and underline respectively.

performs best on STL-10 with 40 labels and Amazon Review with 250 labels, outperforming the previous best by \(\%\) and \(\%\). In the other settings, the performance of our method is also very close to the best-performing methods. More remarkably, our method does not employ any thresholding, re-weighting, or contrastive techniques to achieve current results, demonstrating a significant potential to be further explored.

### Noisy Label Learning

Setup.We conduct the experiments of NLL following SOP  on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M , and WebVision . To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \(\) into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. The introduced noise is then treated as ground truth labels to train the model. We mainly select three previous best methods as baselines: DivideMix ; ELR ; and SOP . We also include the normal cross-entropy (CE) training and mixup  as baselines. More comparisons of other methods [91; 28] and on CIFAR-10N  with training details and more baselines [93; 28] are shown in Appendix D.4.

Results.We present the noisy label learning results in Table 3. The proposed method is comparable to the previous best methods. On synthetic noise of CIFAR-10, our method demonstrates the best performance on both symmetric noise and asymmetric noise. On CIFAR-100, our method generally produces similar results comparable to SOP. One may notice that our method shows inferior performance on asymmetric noise of CIFAR-100; we argue this is mainly due to the oversimplification of the noise transition model. Our method also achieves the best results on WebVision, outperforming the previous best by \(\%\). On Clothing1M, our results are also very close to DivideMix, which trains for 80 epochs compared to 10 epochs in ours.

### Mixed Imprecise Label Learning

Setup.We evaluate on CIFAR-10 and CIFAR-100 in a more challenging and realistic setting, the mixture of various imprecise label configurations, with unlabeled, partially labeled, and noisy labeled data existing simultaneously. We first sample the labeled dataset and treat other samples as the unlabeled. On the labeled dataset, we generate partial labels and randomly corrupt the true label of the partial labels. We set \(l\{1000,5000,50000\}\) for CIFAR-10, and \(l\{5000,10000,50000\}\) for

   Datasets & CIFAR-100 & STL-10 & IMDB & Amazon Review \\  \# Labels \(l\) & 200 & 400 & 40 & 100 & 20 & 100 & 250 & 1000 \\  AdaMatch  & 22.32±1.73 & 16.66±0.62 & 13.64±2.49 & 7.62±1.90 & 8.09±0.99 & 7.11±0.20 & 45.40±0.96 & **40.16±0.49** \\ FixMatch  & 29.60±0.90 & 19.56±0.52 & 16.15±1.89 & 8.11±0.68 & 7.22±3.33 & 7.33±0.13 & 47.61±0.83 & 43.05±0.54 \\ FlexMatch  & 26.76±1.12 & 18.24±0.36 & 14.40±3.11 & 8.17±0.78 & 7.82±0.77 & 7.41±0.38 & 45.73±1.60 & 42.25±0.33 \\ CoMatch  & 35.08±0.69 & 25.35±0.50 & 15.12±1.88 & 9.56±1.35 & 44.40±3.70 & 7.72±1.14 & 48.76±0.90 & 43.63±0.21 \\ SimMatch  & 23.78±1.08 & 17.06±0.18 & 17.74±3.20 & **7.55±1.52** & 7.93±0.55 & **7.08±0.33** & 45.91±0.95 & 42.21±0.30 \\ FreeMatch  & **21.40±0.30** & **15.65±0.26** & 12.73±3.22 & 8.52±0.53 & 8.94±0.21 & 7.59±0.45 & 46.41±0.60 & 42.64±0.06 \\ SoftMatch  & 22.67±1.32 & 16.84±0.66 & 13.55±3.16 & 7.84±1.72 & 7.76±0.58 & 7.97±0.72 & 45.29±0.92 & 42.21±0.30 \\  Ours & 22.06±1.06 & 16.40±0.54 & **11.09±0.71** & 8.10±1.02 & **7.32±0.12** & 7.64±0.67 & **43.96±0.32** & 42.32±0.02 \\   

Table 2: Error rate of different number of labels \(l\) on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for **semi-supervised learning**.

   Dataset &  &  &  & WebVision \\  Noise Type &  &  &  &  &  &  \\  Noise Ratio \(\) & 0.2 & 0.5 & 0.8 & 0.4 & 0.2 & 0.5 & 0.8 & 0.4 & - & - \\  CE & 87.20 & 80.70 & 65.80 & 82.20 & 58.10 & 47.10 & 23.80 & 43.30 & 69.10 & - \\ Mixup  & 93.50 & 87.90 & 72.30 & & 69.90 & 57.30 & 33.60 & 33.60 & - \\ DivideMix  & 96.10 & 94.60 & 93.20 & 93.40 & 77.10 & 74.60 & 60.20 & 72.10 & **74.26** & 77.32 \\ ELR  & 95.80 & 94.80 & 93.30 & 93.00 & 77.20 & 73.80 & 60.80 & 77.50 & 72.90 & 76.20 \\ SOP  & 96.30 & 95.50 & 94.00 & 93.80 & **7.88±0** & **7.59** & 63.30 & **78.00** & 73.50 & 76.60 \\  Ours & **96.78±1.01** & **96.60±1.35** & **94.31±0.87** & **94.75±0CIFAR-100. For partial labels, we set \(q\{0.1,0.3,0.5\}\) for CIFAR-10, and \(q\{0.01,0.05,0.1\}\) for CIFAR-100. For noisy labels, we set \(\{0,0.1,0.2,0.3\}\) for both datasets. Since there is no prior work that can handle all settings all at once, we compare on partial noisy label learning with PiCO+ , IRNet , and DALI . Although there are also prior efforts on partial semi-supervised learning [51; 52], they do not scale on simple dataset even on CIFAR-10. Thus, we did not include them in comparison. We conduct additional validation of our method on more complex settings for partial noisy labels with unlabeled data to demonstrate its robustness to various imprecise labels.

**Results**. We report the comparison with partial noisy label learning methods in Table 4. Compared to previous methods, the proposed method achieves the best performance. Despite the simplicity, our method outperforms PiCO+ and DALI with mixup, showing the effectiveness of dealing with mixed imprecise labels. We also report the results of our methods on more mixed imprecise label configurations in Table 5. Our method demonstrates significant robustness against various settings of the size of labeled data, partial ratio, and noise ratio. Note that this is the first work that naturally deals with all three imprecise label configurations simultaneously, with superior performance than previous methods handling specific types or combinations of label configurations. This indicates the enormous potential of our work in realistic applications for handling more practical and complicated data annotations common in real world applications.

## 5 Conclusion

We present the imprecise label learning (ILL) framework, a unified and consolidated solution for learning from all types of imprecise labels. ILL effectively employs an expectation-maximization (EM) algorithm for maximum likelihood estimation (MLE) of the distribution over the latent ground truth labels \(Y\), imprecise label information \(I\), and data \(X\). It naturally extends and encompasses previous formulations for various imprecise label settings, achieving promising results. Notably, in scenarios where mixed configurations of imprecise labels coexist, our method exhibits substantial robustness against diverse forms of label imprecision. The potential **broader impact** of the ILL framework is substantial. It stands poised to transform domains where obtaining precise labels poses a challenge, offering a simple, unified, and effective approach to such contexts. Beyond the three imprecise label configurations we have demonstrated in this study, the ILL framework shows promise for an extension to more intricate scenarios such as multi-instance learning  and multi-label crowd-sourcing learning . However, it is also crucial to acknowledge the **limitations** of the ILL

    &  &  &  &  \\  & & \(\)=0.1 & \(\)=0.2 & \(\)=0.3 & & \(\)=0.1 & \(\)=0.2 & \(\)=0.3 \\  PiCO+  & & 93.64 & 93.13 & 92.18 & & 71.42 & 70.22 & 66.14 \\ IRNet  & & 93.44 & 92.57 & 92.38 & & 71.17 & 70.10 & 68.77 \\ DALI  & & 94.15 & 94.04 & 93.77 & & 72.26 & 71.98 & 71.04 \\ PiCO+ Mixup  & & 94.58 & 94.74 & 94.43 & & 70.54 & 74.31 & 71.79 \\ DALI Mixup  & & 95.83 & 95.86 & 95.75 & & 76.52 & 76.55 & 76.09 \\ Ours & & **96.47fa.1** & **96.09fa.20** & **95.83s.84*** & **77.53a.24** & **76.96a.04** & **76.43a.02** \\  PiCO+  & & 92.32 & 92.22 & 89.95 & & 69.40 & 66.67 & 62.24 \\ IRNet  & & 92.81 & 92.18 & 91.35 & & 70.73 & 69.33 & 68.09 \\ DALI  & & 93.44 & 93.25 & 92.42 & & 72.28 & 71.35 & 70.05 \\ PiCO+ Mixup  & & 94.02 & 94.03 & 92.94 & & 70.5 & 73.33 & 67.56 \\ DALI Mixup  & & 95.52 & 95.41 & 94.67 & & 76.87 & 75.23 & 74.49 \\ Ours & & **96.2a.02** & **95.87fa.14** & **95.22a.06** & & **77.07fa.16** & **76.34a.08** & **75.13a.63** \\   

Table 4: Accuracy comparison of **mixture of different imprecise labels**. We report results of full labels, partial ratio \(q\) of 0.1 (0.01) and 0.3 (0.05) for CIFAR-10 (CIFAR-100), and noise ratio \(\) of 0.1, 0.2, and 0.3 for CIFAR-10 and CIFAR-100.

    &  &  &  &  &  \\  & & & & & & & & \(\)=0.0 & \(\)=0.1 & \(\)=0.2 & \(\)=0.3 \\   & 0.1 & 95.29fa.08 & 93.90fa.01 & 92.02a.02a.08 & 89.02a.08 & 0.01 & 69.90fa.23 & 68.74fa.163 & 66.87fa.34 & 65.34fa.02 \\  & 0.3 & 95.13a.06 & 92.95a.07 & 90.14a.06 & 87.31a.02 & 10,000 & 0.05 & 69.58fa.50.23 & 68.08fa.02 & 66.78fa.40 & 64.83a.17 \\  & 0.5 & 95.04a.01 & 92.18a.52 & 88.39fa.08 & 83.09fa.08 & & 0.10 & 68.92a.04 & 67.15a.05 & 64.44a.12 & 60.26fa.36 \\   & 0.1 & 94.48a.09 & 91.68a.07 & 87.17a.05 & 81.04a.13 & & 0.01 & 65.66a.02 & 63.13a.207 & 60.93fa.56 & 53.86a.06 \\  & 0.3 & 94.35a.08 & 99.84a.00 & 82.66a.15 & 69.02a.16 & 5,000 & 0.05 & 65.60fa.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0.00.00.0.00.00.0.00.00.00.0.00.00.00.00.00.0.00.00.0.00.00.00.0.00.00.0.00.0.00.00.0.00.00.0.00.00.0.00.00.00.00.00.0.00.00.00.0.00.00.

framework. Although its effectiveness has been substantiated on relatively smaller-scale datasets, additional empirical validation is necessary to assess its scalability to larger datasets. Furthermore, our study only considers balanced datasets; thus, the performance of the ILL framework when dealing with imbalanced data and open-set data still remains an open area for future exploration. We hope that our study will constitute a significant stride towards a comprehensive solution for imprecise label learning and catalyze further research in this crucial field.

## Acknowledge

Masashi Sugiyama was supported by the Institute for AI and Beyond, UTokyo.