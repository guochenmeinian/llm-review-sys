# Elo Uncovered: Robustness and Best Practices in Language Model Evaluation

Meriem Boubdir

Cohere For AI

meri.boubdir@gmail.com

&Edward Kim

Cohere

edward@cohere.com

&Beyza Ermis

Cohere For AI

beyza@cohere.com

&Sara Hooker

Cohere For AI

sarahooker@cohere.com

&Marzieh Fadaee

Cohere For AI

marzieh@cohere.com

###### Abstract

In Natural Language Processing (NLP), the Elo rating system, originally designed for ranking players in dynamic games such as chess, is increasingly being used to evaluate Large Language Models (LLMs) through "A vs B" paired comparisons. However, while popular, the system's suitability for assessing entities with constant skill levels, such as LLMs, remains relatively unexplored. We study two fundamental axioms that evaluation methods should adhere to: **reliability** and **transitivity**. We conduct an extensive evaluation of Elo behavior across simulated and real-world scenarios, demonstrating that individual Elo computations can exhibit significant volatility. We show that both axioms are not always satisfied, raising questions about the reliability of current comparative evaluations of LLMs. If the current use of Elo scores is intended to substitute the costly head-to-head comparison of LLMs, it is crucial to ensure the ranking is as robust as possible. Guided by the axioms, our findings offer concrete guidelines for enhancing the reliability of LLM evaluation methods, suggesting a need for reassessment of existing comparative approaches.

## 1 Introduction

In the rapidly evolving field of Natural Language Processing (NLP), the task of accurately and reliably evaluating LLMs has become increasingly challenging [32; 10; 50; 26; 43]. Human feedback has emerged as an indispensable tool in this performance assessment process, serving as a qualitative metric that captures nuances that automated scoring mechanisms often fail to address [2; 3; 4; 50; 13; 12]. These human-centered evaluations, highly valuable to the overall progress of the NLP field, typically adopt an _"A vs B"_ comparative setup, turning evaluations into a zero-sum game between language models. Pairwise comparisons, however, are fundamentally difficult to scale for large pools of models, due to the quadratic growth of comparisons required. Fortunately, this paired feedback structure  naturally lends itself to the Elo rating system, originally designed for ranking chess players (including those who have never before played each other) for better matchmaking .

Under the Elo rating system, players' skills are indicated by an _Elo rating_, where higher ratings indicate higher skill, and all players can be ranked best to worst using this scalar Elo rating. In the standard formulation (see Section 2), a player rated at 1800 has \(10\!:\!1\) odds of winning against a player rated at 1400. After a match, the winner takes rating points from the loser in a zero-sum fashion . Thus, with the Elo rating system, we can efficiently integrate subjective human feedback on paired _"A vs B"_ language model completions into a structured and unified rating system to assess the performance of language models.

The core principles of Elo rating have proven to be resilient and adaptable due to its dynamic adjustments, relative rating focus, consistency across skill levels, and simplicity and transparency. As a result, the Elo rating system has found diverse applications, from predicting sports events outcomes , and facilitating matchmaking in massively multiplayer online games like StarCraft ii and Dota , to its recent use in the evaluation of LLMs . However, to-date there has not been a comprehensive examination of the compatibility of Elo scores and LLMs evaluation.

Unlike dynamic competitors that evolve over time, LLMs have static capabilities and operate in a time-agnostic context. In this setting, evaluations of LLMs are not constrained by a preset number of turns, as is the case with tournament timelines or predefined match sequences. Moreover, the ordering of matches can significantly influence the final Elo scores and, consequently, model rankings. This oversight is particularly concerning, given the direct impact of Elo system rankings on both research directions and real-world applications in NLP as well as its widespread adoption .

This study aims to close this research gap by adopting an axiomatic approach and scrutinizing both the reliability and limitations of the Elo rating system when applied to LLMs. We study two fundamental axioms that evaluation methods should adhere to: **reliability** and **transitivity**. Through theoretical and empirical analyses grounded in collected human feedback data, our contributions provide a comprehensive understanding of when and how to reliably employ the Elo system for LLM evaluation, thus offering valuable guidelines for researchers and practitioners in the NLP field.

We find that Elo ratings for LLMs are highly sensitive to the _order of comparisons_ and the choice of hyperparameters. Moreover, desirable properties such as transitivity are not always guaranteed and can be unreliable unless there is comprehensive human feedback data for all _unique pairwise comparisons_ among models in the feedback pool. The sensitivity of Elo ratings becomes more pronounced when dealing with models that exhibit _similar_ performance levels. We illustrate the best practices for addressing Elo rating sensitivities by offering guidelines for hyperparameter selection and matchmaking scenarios.

Figure 1: **Impact of win probabilities and permutation sampling on Elo ratings**: Comparing Model A and Model B across three different win probabilities (\(Prob(A\ \ B)=\{0.6,0.55,0.51\}\)) with two levels of permutation sampling (\(N_{}=1\) and \(N_{}=100\)). The top row displays the observed win rates, the middle one the Elo ratings with a single permutation, and the bottom one the mean and standard error of the mean (SEM) of Elo ratings across 100 permutations.

**Implications of our work** As LLMs rapidly advance, evaluation leaderboards are gaining popularity to assess the performance of newly introduced models using Elo scores. Elo can also be used in the learning framework of LLMs to produce a ranking of models and their outputs for preference training. No research has explored the nuances of using Elo scores to compare LLMs, which, unlike chess, exhibit static capabilities and operate in a time-agnostic manner. We show that Elo rating does not always satisfy two critical axioms--reliability and transitivity--leading to rankings of models that are not accurate. Our research offers guidelines for reliable and robust implementation of Elo scores when comparing LLMs. Deviation from our recommendations could result in inaccuracies when ranking LLMs, particularly in situations where model performances are closely matched, and Elo score differences are minimal (a common occurrence in many real-world scenarios).

## 2 Elo Algorithm Explained

We provide the standard mathematical formulation of the Elo algorithm , contextualized to the setting of LLM evaluation. In this formulation, let \(\) be a set of models, and each model \(i\) is assigned an initial numerical Elo rating \(R_{i}\). For each match between two models, we calculate the _expected score_, then update the _ratings_ of both models as follows:

### Expected Score Computation

For a given paired zero-sum match-up between two models \(A\) and \(B\) (\(A,B\)), each with respective pre-match ratings \(R_{A}\) and \(R_{B}\), the expected scores \(E_{A}\) and \(E_{B}\) (i.e., match outcomes) are computed as:

\[E_{A}=-R_{A})/400}} E_{B}=-R_{B})/400}}\] (1)

In this context, the factor of \(400\) precisely adjusts the sensitivity of the expected score to differences in ratings. A \(400\)-point advantage in ratings translates to a \(10\!:\!1\) odds in favor of the higher-rated model, providing an interpretable metric for performance comparison. For evenly matched models (\(R_{A}=R_{B}\)), both \(E_{A}\) and \(E_{B}\) equate to \(0.5\), reflecting a \(50\!:\!50\) win probability for both models.

### Rating Update Mechanism

Following each match, the Elo ratings are updated based on the observed win-loss outcome. The rating adjustment for each model is dictated by the equation:

\[R^{}_{A}=R_{A}+K(S_{A}-E_{A})\] (2)

Here, \(S_{A}\) represents the actual score achieved by model \(A\), which can take on either the value 0 for a loss or 1 for a win. Model B's Elo rating is updated via the same method. The \(K\)-factor serves as a variable hyperparameter to adapt the rate of change in rating to different scenarios. A higher \(K\)-factor results in larger changes in the Elo score after each match-up, making the scoring more sensitive to individual results. A lower \(K\)-factor, in contrast, makes the Elo ratings more stable, with smaller changes after each match. In chess, the \(K\)-factor is usually set to 16 for masters and to 32 for novice players.

## 3 Desirable Properties of Elo

The objective of using Elo scores to rank models is to establish a comparative understanding of the performance hierarchy among them. When incorporating a new model into an already ranked list, only a limited number of pairwise annotations are required to determine its position in the ranking. The ability to infer a model's relative performance compared to all previous models in the list relies on the robustness of the scoring method and the transitive property of the ranking system. We describe these desirable properties through two axioms: _transitivity_ and _reliability_.

### Axiom 1: Transitivity

A desirable property of any rating system is transitivity because it ensures consistency and logical coherence in how entities are ranked or rated. Transitivity in this context means that if player \(A\) beats player \(B\), and player \(B\) beats player \(C\), then player \(A\) is expected to beat player \(C\). If the ranking of large language models exhibits transitivity, we can deduce their comparative performance without the need for direct head-to-head evaluations between every pair of models. The central assumption in developing various leaderboards for comparing language models is that the rankings adhere to the principle of transitivity .

While Elo's design inherently assumes transitivity, our synthetic data which are derived from realistic scenarios, uncovers certain circumstances that violate this assumption. Such anomalies can affect the final ranking of language models and their relative performance assessments.

### Axiom 2: Reliability

We consider two aspects of reliability:

**Sensitivity to ordering:** Unlike chess or time-bound sports where match sequences are structured, in LLM evaluations all matches can occur independently and in parallel, amplifying the sequence's influence on final model ranking. In this context, each match represents the performance comparison between two models on a specific prompt. If the prompts are presented in a specific order, and one model happens to perform better on the initial set of prompts, it may gain an advantage in subsequent comparisons due to the cumulative effect of its early success. This inherent variability prompts us to investigate the extent to which match-up ordering affects the robustness of Elo ratings.

**Sensitivity to hyperparameters:** The sensitivity of hyperparameters can compromise the robustness of Elo scores leading to inconsistent rankings. Evaluating and understanding this sensitivity is crucial for building evaluation frameworks that maintain consistency across diverse models. In this work, we evaluate the sensitivity of Elo performance to one key hyperparameter, the \(K\)-factor. This factor acts as a scaling constant in the Elo rating system, pivotal for updating ratings after each matching. It essentially determines how quickly a model's rating converges to what can be considered its "true" skill. While conventional applications like chess use standard \(K\)-factor values, these may not be directly applicable in the context of evaluating LLMs due to the unique characteristics and requirements of this domain.

## 4 Synthetic Human Feedback

Given the costly and time-consuming nature of human evaluations, studying the Elo system's behavior under various scenarios becomes challenging. To circumvent these limitations, we first validate the properties of Elo using synthetic data generation via Bernoulli processes to simulate various human feedback scenarios. In Section 6 we extend these evaluations to include real-world human feedback. This time-agnostic and independent setup of LLM evaluations resembles a Bernoulli process , a sequence of independent experiments, each yielding a simple "win" or "loss" outcome, representing one model outperforming another. We use this setting to control the characteristics of the distribution and evaluate the different desirable properties of a rating system.

In this controlled setting, our primary objectives include testing the **transitivity** axiom--whether a consistently higher-rated model outperforms those with lower ratings in all scenarios. Additionally, in studying the **reliability** axiom, we explore how the Elo scores are affected by the _order in which models are compared_ and the sensitivity to _hyperparameter adjustments_, particularly the \(K\)-factor. This synthetic setup offers a robust platform to dissect and understand the dynamics of the Elo rating system in the context of LLM evaluations, without the constraints and limitations of relying solely on real-world human feedback.

### The Bernoulli Analogy

Pairwise comparisons in LLM evaluation draw parallels with the foundational principles of the Bernoulli experiment in probability theory. This section studies the similarity between human feedback-based evaluations and the Bernoulli experiment's principles.

PreliminariesA Bernoulli trial is a random experiment with exactly two possible outcomes, "success" or "failure". The outcomes adhere to the probability condition:

\[P()+P()=1\] (3)

Here, the random variable \(\) denotes the outcome, where \(=1\) implies success, and \(=0\) signifies failure. The probabilities associated with these outcomes are given by:

\[P(=1)=p, P(=0)=1-p\] (4)

with \(0 p 1\), the "success" probability.

Mapping to Human FeedbackWhen comparing two models, \(A\) and \(B\), across \(N\) pairwise evaluations, the setup aligns with a Bernoulli process. This process comprises a sequence of independent and identically distributed (_i.i.d_) Bernoulli trials. To frame this analogy, we designate a win probability, \(P(A_{})\), to model \(A\). Leveraging a Bernoulli random variable, \(\), as a means to simulate synthetic human feedback, we proceed as follows:

1. A sample is drawn from \(\) using \(P(A_{})\).
2. If \(=1\), feedback suggests a preference for model \(A\).
3. Otherwise, model \(B\) is favored.

Extending to Multiple PlayersGiven a finite set of \(n\) distinct models \(\), their pairwise comparisons can be formulated as:

\[==\] (5)

This formula yields \(\) unique pairs \((A,B)\) where \(A,B\) and \(A B\). For each pair, a Bernoulli process comprising multiple Bernoulli experiments is conducted to discern which model performs better over a sequence of trials.

### Synthetic Data Generation

Building upon the Bernoulli process analogy, when conducting multiple independent evaluations between two models, the distribution of the number of times one model is preferred over the other naturally follows a binomial distribution. For \(N\) pairwise comparisons, the relation is:

\[P(k;N,p)=p^{k}(1-p)^{N-k}\] (6)

where \(P(k;N,p)\) is the probability of one model being preferred \(k\) times out of \(N\) evaluations. \(p\) is the success probability and \(\) is the binomial coefficient, representing the number of ways to choose \(k\) successes from \(N\) trials.

## 5 How Robust Are Elo Scores?

This section describes rigorous stress tests designed to investigate whether the two axioms, presented in Section 3, are satisfied in this evaluation framework. We focus on critical desirable properties of a ranking mechanism - that it should (1) be insensitive to match-up ordering, (2) not be overly sensitive to hyperparameters like the \(K\)-factor, and (3) preserve properties of transitivity. Subsequently, we provide empirically grounded guidelines for a safe and interpretable application of Elo ratings.

### Impact of Ordering on Elo Ratings

Experimental SetupTo quantify the effect of match-up ordering, we generate a baseline sequence of \(N_{}=1000\) match outcomes between models \(A\) and \(B\) (see Equation 6), reflecting the scale typical of LLM evaluations via human feedback. We hold \(N_{}\) constant for the entirety of our study to maintain consistency. From this baseline, we derive \(N_{}\) distinct permutations, each involving a complete reshuffling of the initial sequence to simulate various chronological orders in which the games might unfold. It is important to note that we are not generating new match outcomes for each permutation; instead, we simply reorder the existing data to explore the potential impact of different match-up sequences. For each reordered sequence, we update the Elo ratings \(R_{A}\) and \(R_{B}\) according to equation 2, resetting both ratings to an initial value of 1400 at the start of each permutation. Finally, we compute average Elo ratings per match across all \(N_{}\) permutations, ensuring a robust analysis that takes into account the full range of possible match-up orders.

We repeat this process to generate baseline sequences and their respective reorderings for a set of selected winning probabilities enabling us to inspect ratings' behavior under various real-world scenarios. \(N_{}\) is varied from a minimum of 1 to a maximum of 10k, providing a robust sample size for statistical analysis (see Figure 3). Subsequently, we compute the average Elo ratings per match across all permutations. These averages, \(_{A}\) and \(_{B}\). particularly for \(N_{}=1\) and \(N_{}=100\), are visualized to offer insights into the stability of the ratings, as shown in Figure 1.

Key FindingsOur analysis underscores the interplay between winning probability \(P(A_{})\) and the number of different orderings \(N_{}\) on the stability of Elo ratings after each update. For \(P(A_{}) 0.6\), Elo ratings demonstrate high stability; additional results for \(P(A_{})=0.65\) and beyond are available in Appendix B. On the other hand, for \(P(A_{}) 0.5\), ratings exhibit significant instability for a single sequence. As depicted in Figure 1, when both models have win probabilities around \(0.5\), Elo ratings frequently intertwine, making it challenging to discern a clear performance difference between the two. The instability plateaus as \(N_{}\) exceeds 100, resulting in stabilized Elo ratings that align closely with the preset winning probabilities. For instance, at \(P(A_{})=0.55\), the average Elo rating for Model \(A\), \(_{A}\), consistently exceeds that for Model \(B\), \(_{B}\), when averaged across multiple permutations, reflecting an accurate performance-based ranking of these models. These observations validate our concerns highlighted earlier, emphasizing the critical role of \(N_{}\) for a reliable interpretation of Elo ratings in LLM evaluations. In Elo-based evaluations, the sequence of model comparisons can significantly influence the final Elo scores, particularly in scenarios with models of similar quality, where this effect is magnified.

### Sensitivity to Hyperparameters

Experimental SetupWe extend our previous approach by conducting tests across a range of winning probabilities and multiple \(K\)-factor values (\(1,8,16,32,64\)). We compute and compare the

Figure 3: Variation of Model A’s average Elo score with increasing number of permutations \(N_{}\) for different probabilities of Model A winning (\(P(AB)\)). Error bars indicate standard errors of the mean.

Figure 2: Final Elo scores difference (\(S_{A}-S_{B}\)) as a function of \(K\)-factor and \(N_{}\). Positive values reflect the expected ranking where Model \(A\) is superior to Model \(B\), while negative values indicate a discrepancy, falsely suggesting that Model \(B\) has a higher Elo score than Model \(A\). We compare between a single sequence of outcomes and averages over \(N_{}=100\) unique permutations.

average Elo scores \(_{A}\) and \(_{B}\) for \(N_{}=1000\) and \(N_{}=\{1,100\}\). The differences between these final averages for Model \(A\) and Model \(B\) are summarized in Figure 2 to assess the stability and expected ranking between the two models.

Key FindingsAs shown in Figure 2, notable instability is observed in model rankings based on the final Elo scores when we consider a single sequence of paired comparisons (i.e., \(N_{}=1\)), especially for winning probabilities nearing 0.5. This instability is markedly exacerbated at higher \(K\)-factors. In contrast, the picture changes when coupling higher \(K\)-factors with raising the number of permutations to at least 100. Higher \(K\)-factors, in this multi-permutation scenario, speed up the differentiation between models' Elo scores, enabling faster convergence to their true skill levels. This yields much more stable and reliable model rankings. It is noteworthy that this faster convergence is observed to be more reliable for higher winning probabilities, which corresponds to skewed win rates in a real-world scenario.

### Transitive Properties of Elo Scores

Experimental SetupThe transitivity property of the Elo scores is defined as:

\[A>B B>C A>C\] (7)

To test the transitivity property, we design four distinct scenarios that model real-world conditions:

* Model \(A\) beats model \(B\) and model \(B\) beats model \(C\) both with high win probabilities (\(P_{}=0.75\)).
* Model \(A\) beats model \(B\) with a high win probability (\(P_{}=0.75\)), model \(B\) beats model \(C\) with a win probability close to 0.5 (\(P_{}=0.51\)).
* Model \(A\) beats model \(B\) with a win probability close to 0.5 (\(P_{}=0.51\)), model \(B\) beats model \(C\) with a high win probability (\(P_{}=0.75\)).
* Model \(A\) beats model \(B\) with a win probability of 0.54, model \(B\) beats model \(C\) with a win probability of 0.51.

In each of these scenarios, we simulate matches for paired comparisons "\(A\) vs. \(B\)" and "\(B\) vs. \(C\)" and then rearrange these matches in an arbitrary order to form our baseline sequence. This approach mimics how Elo ratings are computed for online leaderboards in the evaluation of large language models . We then analyze whether Elo scores maintain the expected model hierarchies.

Key FindingsThe outcomes from all four scenarios, detailed in Table 1, demonstrate the performance of Elo-based rankings across various configurations. In scenarios where there is a clear disparity between models (e.g.,\(@sectionsign\)), Elo ratings accurately reflect the expected hierarchy. However, in

   &  &  \\   & & \(N=1,K=1\) & \(N=100,K=1\) & \(N=1,K=16\) & \(N=100,K=16\) \\  \(@sectionsign\) & \(A\) & 1539.43 & 1528.50 \(\) 0.35 & 1650.93 & 1584.78 \(\) 3.09 \\ \(A B\) & \(B\) & 1390.47 & 1410.33 \(\) 0.54 & 1381.17 & 1406.48 \(\) 3.23 \\ \(B C\) & \(C\) & 1270.10 & 1261.17 \(\) 0.33 & 1167.90 & 1208.74 \(\) 2.71 \\  \(@sectionsign\) & \(A\) & 1502.09 & 1495.92 \(\) 0.36 & 1509.08 & 1526.04 \(\) 3.03 \\ \(A B\) & \(B\) & 1337.48 & **1342.70*** \(\)** 0.53 & 1379.00 & 1340.83 \(\) 2.83 \\ \(B C\) & \(C\) & 1360.42 & **1361.38*** \(\)** 0.38 & 1311.92 & 1333.13 \(\) 2.68 \\  \(@sectionsign\) & \(A\) & 1437.97 & **1433.84*** \(\)** 0.41 & 1440.31 & 1460.22 \(\) 2.90 \\ \(A B\) & \(B\) & 1455.10 & **1453.84*** \(\)** 0.61 & 1481.04 & 1452.87 \(\) 3.25 \\ \(B C\) & \(C\) & 1306.93 & 1312.32 \(\) 0.34 & 1278.65 & 1286.91 \(\) 2.72 \\  \(@sectionsign\) & \(A\) & 1426.33 & 1419.73 \(\) 0.36 & 1407.44 & 1432.26 \(\) 2.93 \\ \(A B\) & \(B\) & 1390.47 & 1393.29 \(\) 0.59 & 1386.17 & 1392.75 \(\) 3.04 \\ \(B C\) & \(C\) & 1383.20 & 1386.99 \(\) 0.41 & 1406.39 & 1374.99 \(\) 3.12 \\  

Table 1: Investigation of Elo score reliability in capturing true model hierarchies across varying configurations. Scenarios explore the transitive relationship \(A>B\) and \(B>C A>C\). The star (*) indicates cases where the Elo score fails to accurately reflect the expected hierarchy of models. \(\) represents models with similar performance; \(\) indicates that a model significantly outperforms the other one.

more complex cases such as \(}\) and \(}\), where one model significantly outperforms a second, which in turn is closely matched with a third, the rankings become less stable, challenging the assumption of transitivity. We observe once again that varying the number of permutations (\(N_{}=1\) vs. \(N_{}=100\)) and the \(K\)-factor plays a critical role in stability. In the \(}\) and \(}\), scenarios, with \(N_{}=100\) and \(K=1\), we notice discrepancies in the models' rankings. This contrasts with \(K=16\), where rankings are more consistent and accurate. The slower updates from \(K=1\) suggest this setting may be too conservative to capture transitive relations quickly, leading to inconsistencies.

## 6 Validation on Real-World Human Feedback

Building on the insights gained from synthetic data experiments, this section extends the validation of the Elo rating system to real-world human feedback. Our objectives are twofold: first, to ascertain how the properties demonstrated using synthetic data generalize to real human annotations, and second, to evaluate the Elo rating system's utility for assessing LLMs in practical settings.

Experimental SetupWe use the _LMSYS - Chatbot Arena_ dataset , an open-source collection of human preference data derived from unique users' interactions with two distinct models responding to a set of user-defined prompts. To align with our methodology from synthetic data analysis, tie outcomes have been excluded from this analysis to focus specifically on the implications of win-loss dynamics. We select pairs of models (A vs. B) from the initial dataset that feature at least 300 non-tie comparisons. This threshold ensures statistical robustness and allows us to include cases where win rates are closely contested, which can lead to more sensitive ratings. These pairs predominantly involve models from the GPT-4 family  and the Claude family . A comprehensive list of model pairs is included in Appendix C under Table 3, and a subset discussed here is shown in Table 2. The recorded win rates primarily exhibit skewed preferences, with the exception of the GPT-4-0314 vs. GPT-4-0613 pairing, indicating comparable performance levels (see Table 2). Given the variable number of evaluations per pair in the original dataset, we standardize this by sampling a fixed number, \(N_{}\), for each pair to align with the controlled conditions used in synthetic analyses. When sampling to \(N_{}\), we ensure that the resulting win rates accurately represent the original dataset's findings, providing a faithful evaluation of recorded model performance. This standardization facilitates a more reliable comparison and assessment of the Elo rating

  Experiment & Win Rates \\  GPT-4-0314 & 0.51 \\ GPT-4-0613 & 0.49 \\  Claude-1 & 0.59 \\ Claude-2.1 & 0.41 \\  GPT-4-0314 & 0.65 \\ Claude-2.1 & 0.35 \\  GPT-4-0613 & 0.61 \\ Claude-2.1 & 0.39 \\  GPT-4-1106-preview & 0.86 \\ GPT-4-0613 & 0.33 \\  

Table 2: Win rates per evaluated model across selected paired comparison experiments.

Figure 4: Elo score differences (\(S_{A}-S_{B}\)) across varying K-factors and \(N_{}\). Positive values in the heatmap indicate that the expected ranking is maintained (Model A outperforming Model B), while negative values suggest a ranking inversion, where Model B appears to outperform Model A, contrary to the actual win rates. Each cell’s label indicates the model with the higher Elo score.

system under real-world conditions. In line with our previous analyses, we continue to explore the influence of variations in \(N_{}=\{1,100,10000\}\) and the \(K\)-factor (ranging from 1 to 36) on Elo score robustness and reliability. We examine scenarios where one model decisively outperforms another (e.g., Claude-1 vs. Claude-2.1) and cases where models are nearly evenly matched (e.g., GPT-4-0314 vs. GPT-4-0613).

Key FindingsOur analysis of real-world human feedback data confirms that the stability of Elo ratings is influenced by disparities in win rates, analogous to win probabilities in synthetic data, and by the choice of hyperparameters \(K\)-factor and \(N_{}\). In cases where the models show a clear difference in performance as indicated by their win rates, such as in the Claude-1 vs. Claude-2.1 experiment, Elo ratings remain notably consistent across different \(K\)-factors and \(N_{}\) configurations (see Figure 3(a)). On the other hand, in cases like the GPT-4-0314 vs. GPT-4-0613 experiment where win rates are closely matched, the Elo rating system exhibits higher volatility at \(N_{}=1\) but gains stability with larger \(N_{}\) settings (100 and 10000), especially at lower \(K\)-factors (see Figure 3(b)). The magnitude of Elo score differences in these experiments illustrates that larger \(K\)-factor and \(N_{}\) values can amplify or reduce the perceived performance gap between models, reflecting the critical role of these parameters in evaluation sensitivity.

Regarding the conservation of transitivity, our findings indicate that this property is not universally maintained across real-world human evaluations and synthetic scenarios (see Section 5). The relative rankings of models with similar performance levels are particularly sensitive to the choice of hyperparameters. Consequently, one should exercise caution in drawing conclusions from the Elo scores, especially in the absence of extensive paired comparison data as required by the combination formula 5. These observations are consistent with the trends from our synthetic data experiments.

## 7 Related Work

Several works have proposed improvements to the Elo rating system. Variants such as Glicko  and TrueSkill(tm)  have incorporated more complex statistical methods into the original Elo framework, to address some of the limitations of the Elo rating system, particularly in the context of games with more than two players or teams, or games with more complex outcomes

Figure 5: Elo scores (\(S_{A}\), \(S_{B}\) and \(S_{C}\)) for three models at different configurations of \(N_{perms}=\{1,100,10000\}\) and \(K\)-factor = \(\{1,8,16,32\}\). The intersections of score lines in (a)a indicate fluctuating relative rankings, highlighting inconsistency especially pronounced among models with close performance levels. In contrast, (b)b demonstrates more stable relative rankings in conditions where win rates are more skewed.

than just win or loss. There is also ongoing research into the efficacy of these systems in diverse and dynamic environments [11; 7]. Prior work has demonstrated some limitations of Elo in maintaining transitivity, especially in non-transitive cyclic games such as rock-paper-scissors and StarCraft ii[7; 52]. However, our work diverges by focusing on the reliability of Elo applied to large language model systems. To date, there has not been a comprehensive evaluation in this context.

Independent from Elo, numerous studies have explored how sensitivity to hyperparameters can undermine the generalization of findings [41; 36; 23; 27; 9] in machine learning. This forms part of a wider body of work that considers which factors influence reliability and reproducibility [21; 22; 5; 14]. Notable directions includes studies on the impact of random seeds [40; 37; 51], model design choices [46; 48; 43; 28; 47], the use of data parallelism , hardware  and test set construction [49; 30; 38]. Our work is complementary to these efforts, providing a rigorous evaluation of the impact of key hyperparameters and experimental settings on Elo performance.

## 8 Empirical Guidelines for Robust Elo-based Evaluation of LLMs

In this section, we distill essential practices for enhancing the reliability of Elo-based evaluation of language models. These guidelines, derived from our empirical findings, differ notably from some conventional Elo settings and have significant implications for current real-world applications:

* **Achieving Score Stability**: To obtain stable and reliable Elo ratings, it's recommended to run numerous permutations, ideally with \(N_{} 100\). This approach significantly improves the consistency of outcomes over single or fewer permutations commonly used.
* **Adjusting the \(K\)-factor**: A smaller K-factor may reduce significant rating fluctuations when models have closely matched win rates.
* **Rapid Convergence for Clear Winners**: When there is a clear performance disparity between models, a higher K-factor accelerates the alignment of Elo ratings with the models' "true" performance levels. This is in stark contrast to traditional uses of Elo ratings, where a one-size-fits-all K-factor is frequently applied.
* **Transitivity is not guaranteed**: The assumption that (\(A\) beats \(B\) and \(B\) beats \(C\) implies \(A>C\)) is not consistently valid in Elo ratings. This is particularly invalid when models have similar performance levels, challenging a common assumption in many Elo-based evaluations.

These guidelines serve as empirically grounded recommendations to improve the robustness and interpretability of Elo-based evaluations for LLMs. Following these best practices will help in yielding more reliable conclusions on models' performance via human judgment.

## 9 Conclusion and Limitations

This paper presents a comprehensive study on the reliability of the Elo rating system for evaluating LLMs through human feedback within an axiomatic framework. We identify various factors that influence the robustness of Elo ratings and provide guidelines for their effective application in real-world scenarios. While our findings establish an essential foundation, they are by no means exhaustive. Future work could extend the present study by considering tie outcomes and adopting multi-category Bernoulli synthetic data to more closely simulate the varied landscape of human feedback. Such extensions could yield additional insights into the convergence properties of the Elo rating system in the fast-evolving field of language models.

## 10 Impact Statement

The implications of our work are significant in fields relying on LLMs for decision-making, content generation, and more. Improving the evaluation methods of LLMs contributes to the development of AI systems that are more reliable and trustworthy. This research also holds the potential to influence evaluation practices in other sectors that employ the Elo rating system, broadening its relevance and utility. However, it also emphasizes the need for cautious, informed application of Elo ratings to prevent misinterpretation or reliance on Elo-based rankings, particularly when the performance of models is comparable. As LLMs become more integrated into societal frameworks, ensuring the robustness and reliability of their evaluation mechanisms is paramount to fostering ethical, beneficial AI advancements.