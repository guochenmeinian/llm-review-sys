# Learning to Balance Altruism and Self-interest Based on Empathy in Mixed-Motive Games

Fanqi Kong\({}^{2,1}\)   Yizhe Huang\({}^{2,1}\)   Song-Chun Zhu\({}^{1,2,3}\)   Siyuan Qi\({}^{1}\)   Xue Feng\({}^{}}}}}}}}}}}}}\)

\({}^{1}\)State Key Laboratory of General Artificial Intelligence, BIGAI

\({}^{2}\)Institute for Artificial Intelligence, Peking University

\({}^{3}\)Department of Automation, Tsinghua University

kfq20@stu.pku.edu.cn, fengxue@bigai.ai

Corresponding author.

###### Abstract

Real-world multi-agent scenarios often involve mixed motives, demanding altruistic agents capable of self-protection against potential exploitation. However, existing approaches often struggle to achieve both objectives. In this paper, based on that empathic responses are modulated by inferred social relationships between agents, we propose LASE (**L**earning to balance **A**ltruism and **S**elf-interest based on **E**mpathy), a distributed multi-agent reinforcement learning algorithm that fosters altruistic cooperation through gifting while avoiding exploitation by other agents in mixed-motive games. LASE allocates a portion of its rewards to co-players as gifts, with this allocation adapting dynamically based on the social relationship -- a metric evaluating the friendliness of co-players estimated by counterfactual reasoning. In particular, social relationship measures each co-player by comparing the estimated \(Q\)-function of current joint action to a counterfactual baseline which marginalizes the co-player's action, with its action distribution inferred by a perspective-taking module. Comprehensive experiments are performed in spatially and temporally extended mixed-motive games, demonstrating LASE's ability to promote group collaboration without compromising fairness and its capacity to adapt policies to various types of interactive co-players.

## 1 Introduction

Multi-agent reinforcement learning (MARL) has exhibited impressive performance in numerous collaborative tasks and zero-sum games such as MPE, StarCraft, and Google Research Football [19; 25; 38]. These environments involve a predefined competitive or cooperative relationship between agents. Besides, mixed-motive games are prevalent, in which the relationships between agents are non-deterministic and dynamic. That is, agents could cooperate with some co-players and simultaneously compete with someone else. Furthermore, along with interactions, friends may turn into foes, and vice versa. In such games, to maximize self-interest, agents need to cooperate altruistically in some relationships while keep self-interested to avoid being exploited in some others. Consequently, in mixed-motive environments, the ability to balance altruism and self-interest according to social relationships is crucial for agent performance.

The commonly used CTDE (Centralized Training and Decentralized Execution) methods [31; 29] in MARL focus on global optimization goals and necessitate individual information sharing with centralized controllers, which is impractical for self-interest agents in mixed-motive games. On the other hand, simply training self-interest agents in a decentralized way may converge to localoptima, failing to maximize individual interests. For example, in Iterated Prisoner's Dilemma (IPD), decentralized A2C agents converge to defection, getting the minimal reward \(0\) (see details in Fig. 4).

Within the framework of decentralized learning, the gifting mechanism has been used to address the decision-making problems in mixed-motive games by enabling agents to transfer a portion of their rewards to others [20; 4]. Agents independently select gift recipients and determine gift amount, which complies with the decentralized requirement. Gifting can potentially shape co-players' policies and even incentivize them to behave more altruistically by influencing co-players' reward structure. Previous work has studied handcrafted gifting scheme [20; 34] and has learned end-to-end neural networks to determine the reward transfer scheme [36; 37]. However, a consideration of the correlation between social relationships and response strategies is absent, which is crucial for decision-making in mixed-motive games.

To balance the dilemma between altruism and self-interest caused by the non-deterministic and dynamic relationship between agents, it is a feasible way that adaptively modulating the gift amount to others according to the social relationships. This process is called (cognitive) empathy in developmental psychology. What's more, previous studies and human behavioral experiments have shown that empathy can promote the emergence of altruism among self-interested individuals [1; 2; 30]. To the best of our knowledge, there has been a lack of computational models of empathy and the study of empathy-based decision-making. In this work, we propose a computational model of empathy, in which social relationship is measured by a continuous variable, capturing the influence of co-players' behavior on the focal agent's reward and guiding gift scheme. On that basis, we provide a distributed MARL algorithm LASE (**L**earning to balance **A**ltruism and **S**elf-interest based on **E**mpathy) to address the dilemma between altruism and self-interest in mixed-motive games.

LASE uses counterfactual reasoning to infer the different social relationships with different co-players separately by comparing the estimated \(Q\)-value for the joint action to a counterfactual baseline established for each other agent. The counterfactual baseline marginalizes a single agent's action while keeping the other agents' actions fixed. This computational approach to social relationships enables LASE to explicitly decompose the value contributions of other agents to it, thereby providing clearer guidance for gift allocation. That is, gift more to the co-players who contribute more. Additionally, to deal with the challenge of inferring co-players in partially observable and decentralized environments, LASE is equipped with a perspectivec taking module to predict others' policies by converting LASE's local observation to a simulated observation of others.

To verify the effectiveness of LASE, we theoretically analyze its dynamics of decision-making in iterated mixed-motive games and conduct comprehensive experiments in spatially and temporally extended mixed-motive games. The results demonstrate LASE's ability to promote group cooperation without compromising fairness and its capability of self-protection against potential exploitation.

This paper makes three main contributions. **(1)** To our best knowledge, we are the first to computationally model empathy which modulates the response based on inferred social relationships. **(2)** We present LASE, a decentralized MARL algorithm that balances altruism and self-interest in mixed-motive games. It flexibly adapts strategy to promote cooperation while mitigating exploitation by others. **(3)** We provide a theoretical analysis of decision dynamics in iterated matrix games and experimentally verify that LASE outperforms baselines in a variety of sequential social dilemmas.

## 2 Related work

In multi-agent learning, the game-theoretic notion of social dilemmas has been generalized from the classic two-player matrix-form games Tab. 1 to sequential social dilemmas, a spatial-temporally-extended complex behavior learning setting [15; 4]. Various approaches have been proposed to foster cooperative behavior among agents to advance societal welfare. One approach incorporates the rewards of others as intrinsic rewards into one's own optimization objectives, with the ratio of intrinsic to extrinsic rewards determined by different methods, such as pre-defined social value orientations like altruistic or prosocial [21; 23], introducing the concept of inequity aversion , or learning in a model-free way . However, this approach depends on direct access to others' reward functions, which may not be feasible in realistic mixed-motive games. Another line of work dispenses with this assumption, allowing agents to model others and influence them through their actions [6; 17; 13; 8; 10]. Here, we employ a more direct form of opponent shaping named gifting.

As a peer rewarding mechanism that allows agents to reward other agents as a part of their action space , gifting can be viewed as a process of redistributing rewards among agents [12; 9; 7], but the key difference is that in our work, gifting does not require a powerful centralized controller to decide on the allocation. Instead, individuals make their own decisions about gifting, which is more in line with the setting of decentralized training. [34; 35] study the theoretical basis of how gifting promotes cooperative behavior in simple social dilemmas, while LIO  independently learns an incentive function to gift others. However, the rewards used for gifting in LIO are determined by the incentive function rather than split from its own reward. LIO doesn't adhere to zero-sum conditions, which to some extent alters the original game-theoretic nature. LToS  models the optimization problem of gifting (sharing) weights with the zero-sum setting as a bi-level problem and uses an end-to-end approach to train weights and policies jointly. MOCA  introduces contracts to restrict gifting recipients to the agents that fulfill certain behavioral patterns.

Our perspective taking module simulates others' behavior by adopting their perspective, a technique akin to Self-Other Modeling (SOM) . The difference is that instead of inferring the agent's goal which may not necessarily be well defined in some environments, we directly imagine their observations, and decouple the agent's network for inferring others' actions from its own real policy network utilized for execution. In MARL, some prior studies have employed counterfactual reasoning to deduce the impact of individual actions on others  or the whole team . In contrast, our focus lies on assessing others' influence on the focal agent.

## 3 Preliminaries

### Partially observable Markov games

We consider an \(N\)-player partially observable Markov game (POMG) [27; 18], \(= N,,\{^{i}\}, \{^{i}\},,\{^{i}\}\), where \(N\) represents the number of agents, \(s\) represents the state of the environment. In the partially observable setting, agent \(i\) only obtains the local observation \(o^{i}^{i}\) based on the current state \(s\). Each agent \(i\) learns an independent policy \(^{i}(a^{i}|o^{i})\) to select actions and form a joint action \(=(a^{1},...,a^{N})^{1}^{N}\), resulting in the state change from \(s\) to \(s^{}\) according to the transition function \(:^{1}^{N }()\), where \(()\) represents a probability distribution over the set \(\). Agent \(i\) receives an individual extrinsic reward \(r^{i}=^{i}(s,a^{1},,a^{N})\) and tries to maximize a long-term return:

\[V_{i}^{}(s_{0})=_{_{t},s_{t+1} P(s_{ t},_{t})}[_{t=0}^{}^{t}^{i}(s_{ t},_{t})],\] (1)

where the variables in bold represent the joint information of all agents, and \(\) is the discount factor.

### Policy Gradient Learning

In decentralized MARL, each self-interest agent \(i\) learns an independent policy \(^{i}\) parameterized by \(^{i}\). The optimization objective is to maximize the expected return in Eq. 1. We use the policy-based Actor-Critic method as the learning algorithm for our agents. The gradient for actor is \(_{}J()=_{_{}}[_{t=0}^{T}_{t} _{}\) log \(_{}(a_{t}|o_{t})]\), where \(_{t}\) represents the critic's evaluation of the actor. The most popular form of \(_{t}\) is TD-error: \(_{t}=r_{t}+ V^{_{}}(s_{t+1})-V^{_{}}(s_{t})\).

## 4 Methodology

To balance altruism and self-interest in mixed-motive games, we propose a distributed MARL algorithm LASE which empathically shares rewards with co-players based on inferred social relationships. The architecture of LASE is illustrated in Fig. 1, composed of two main modules: Social Relationship Inference (SRI) and Gifting. SRI conducts counterfactual reasoning to get the social relationships with co-players, which reflects the impact of co-players' actions on LASE's return. SRI compares the \(Q\)-value (estimated by the SR value network) of the current joint action to a counterfactual baseline which marginalizes the co-player's action, with its action distribution inferred by a perspective-taking (PT) module. PT is provided to address the challenge of predicting co-players' policies in partially observable and decentralized environments. In particular, PT consists of an observation conversion network, simulating the co-player \(j\)'s observation \(^{j}\) from the local observation \(o^{i}\), and an SR policy network, learning a function from \(^{j}\) to the inferred \(j\)'s policy.

The Gifting module, according to the inferred social relationships, determines the amount of reward sharing with others. Meanwhile, agents receive others' gifts and get the final reward \(r^{i,}\), which serves as an optimization target to guide the training of policy \(^{i}\). Section 4.1 and Section 4.2 will introduce our algorithm in detail. LASE's pseudocode is given as Algorithm 1.

### Zero-Sum Gifting

Here, we use the zero-sum gifting mechanism which adheres to the principle of "what I give is what I lose" , indicating that the overall rewards for the group remain constant. Notably, gifting is an autonomous decision, wherein any agent \(i\) holds a gifting weight vector at time step \(t\): \(_{t}}=[w^{ij}_{t}]_{j=1}^{N}\), where \(w^{ij}_{t}\) and \(_{j=1}^{N}w^{ij}_{t}=1\). \(w^{ij}_{t}\) is the fraction of agent \(i\)'s reward to gift \(j\). It is exactly the social relationship computed as Eq. 3. For an \(N\)-player group using the zero-sum gifting mechanism, \(}\) denotes the extrinsic rewards vector obtained through interactions with the environment at timestep \(t\). Agent \(i\)'s total reward is computed by

\[r^{i,}_{t}(},})=_{j=1}^{N}w^{ji}_{t}r ^{j}_{t}.\] (2)

The policy \(^{i}(a^{i}_{t}|o^{i}_{t})\) is trained to maxmize \(_{^{i}}[_{t=0}^{T}^{t}r^{i,}_{t}]\) using the TD-error introduced in Section 3.2 by replacing \(r_{t}\) with \(r^{}_{t}\).

### Social Relationships Inference

The social relationship \(w^{ij}\), modeled as a continuous variable, measures \(i\)'s inference of \(j\)'s friendliness to him. Based on counterfactual reasoning, \(w^{ij}\) is inferred as:

\[w^{ij}=_{}(o^{i}_{t},})-_{a^{j^{}}_{t} }^{i}_{}(a^{j^{}}_{t}|^{j}_{t})Q^{i}_{}(o^{i }_{t},(_{t}},a^{j^{}}_{t}))}{},\] (3)

where \(Q^{i}_{}(o^{i}_{t},})\), estimated by the SR value network, is \(i\)'s \(Q\)-value of its local observation and the joint action. Eq. 3 compares \(Q^{i}_{}(o^{i}_{t},})\) with a counterfactual baseline, which is the weighted sum of \(Q\)-values, with \(j\) taking all possible actions \(a^{j^{}}_{t}\) while the other agents' actions \(^{-j}\) fixed. The weight \(^{i}_{}(a^{j^{}}_{t}|^{j}_{t})\) is \(j\)'s policy inferred by \(i\). The denominator is for normalization, \(=(N-1)(_{a^{j^{}}_{t}}\,Q^{i}_{}(o^{i}_{t },(_{t}},a^{j^{}}_{t})-_{a^{j^{}}_{t}}\,Q^{i} _{}(o^{i}_{t},(_{t}},a^{j^{}}_{t})))\), where \((N-1)\) ensures \(w^{ij}\). After gifting, LASE keeps the remaining reward for itself, \(w^{ii}=1-_{j=1,j i}^{N}w^{ij}\).

Due to partial observability and decentralized learning, \(i\) is unable to accurately obtain \(j\)'s policy \(^{j}\) and its observation \(o^{j}_{t}\). So we utilize PT module to estimate \(j\)'s policy, denoted as \(^{i}_{}(a^{j^{}}_{t}|^{j}_{t})\). \(^{i}_{}\) is the SR policy network parameterized by \(^{i}\) which predicts \(j\)'s actions conditioned on the simulated observation \(^{j}_{t}\) learned by the observation conversion network. The observation conversion network, parameterized by \(^{i}\), enables LASE to adopt the perspective of others and generate a simulated observation of them. At timestep \(t\), LASE processes its own observation \(o^{i}_{t}\) along with another agent

Figure 1: Architecture of LASE. It consists of Social Relationships Inference (SRI) and Gifting. SRI conducts counterfactual reasoning to get the social relationships with co-players. The social relationship is measured by comparing the \(Q\)-value (estimated by the SR value network) of the current joint action to a counterfactual baseline which marginalizes the co-player’s action, with its action distribution inferred by a perspective-taking (PT) module. PT is provided to address the challenge of predicting co-players’ policies in partially observable and decentralized environments. The Gifting module, according to the inferred social relationships, determines the amount of reward to share.

\(j\)'s ID (represented as a one-hot vector), yielding the output \(_{t}^{j}\) which is of the same size as \(o_{t}^{i}\). To update \(^{i}\) and get a more accurate \(_{t}^{j}\), the loss function is

\[(^{i})=_{t}_{j=1,j i}^{N}(1- )CE(\{a_{t}^{j}\},_{}^{i}(a_{t}^{j}|_{t}^{j} ))+\|_{t}^{j}-o_{t}^{i}\|_{1}.\] (4)

The first term aims to ensure that the predicted policy \(_{}^{i}(a_{t}^{j}|_{t}^{j})\) aligns with co-player \(j\)'s actual actions. The second term aims to minimize the deviation of the simulated observation \(_{t}^{j}\) from \(i\)'s true observation \(o_{t}^{i}\), so that some common features in the environment can be reconstructed. The hyperparameter \(\) balances the two goals.

Since SR policy network predicts actions from an agent's self-perspective and SR value network estimates the agent's own returns, we integrated the training processes of the two networks within an actor-critic framework. In training, \(_{}^{i}\) takes one agent's observation as input and outputs a probability distribution over his action space. \(Q_{}^{i}\) computes the \(Q\)-value of the joint action under the current observation. Both networks are updated based on the individual extrinsic rewards obtained in the environment, and the TD-error defined as \(_{t}^{i}=r_{t}^{i}+ Q_{}^{i}(o_{t+1}^{i},})-Q_ {}^{i}(o_{t}^{i},})\). Specifically, the SR policy network and SR value network, parameterized by \(^{i}\) and \(^{i}\), are updated by

\[^{i}=^{i}+_{^{i}}_{t}_{t}^{i}_{^{ i}}_{}^{i}(a_{t}^{i}|o_{t}^{i}),^{i}=^{i}+ _{^{i}}_{t}_{t}^{i}_{^{i}}Q_{ }^{i}(o_{t}^{i},}).\] (5)

We can further intuitively comprehend Eq. 3: when \(w_{t}^{ij}\) attains the maximum value of \(1/(N-1)\), \(j\) has taken the best action that maximized \(Q_{}^{i}\) and \(i\) predicts with a probability of 1 that \(j\) will select the action that minimizes \(Q_{}^{i}\). At this point, the value \(j\) brings to \(i\) far exceeds \(i\)'s psychological expectation, which corresponds to the real-world scenario where people feel particularly happy when they are helped by someone they might have thought was unkind to them. So the amount of \(i\)'s gifting to \(j\) reaches the maximum. It is worth noting that Eq. 3 may yield a negative value, indicating that the agent is required to acquire rewards from other agents. Dealing with this scenario becomes intricate, particularly when the other agent is uncooperative. As this type of competition is not the primary focus of our research, we assign \(w^{ij}=0\) when \(w^{ij} 0\).

### Analysis in Iterated Matrix Game

We use iterated matrix games to theoretically analyze LASE's learning process. The iterative matrix game is to play multiple rounds of a single game with the payoff matrix shown in Tab. 1, where both players get a payoff of \(R\) by mutual cooperation (C) and \(P\) by mutual defection (D). If one player defects and the other cooperates, the defector receives a reward of \(T\), while the cooperator receives a reward of \(S\). We normalize \(R\) to 1 and \(S\) to 0, and let \(0 T 2,-1 S 1\) which is shown to be sufficient to characterize the three typical kinds of dilemmas in Tab. 3.

We carry out a closed-form gradient descent analysis on LASE in the two-player iterated matrix games and derive the policy update rule Eq. 15 and Eq. 16, where each agent \(i\) optimizes the reward after gifting \(r^{i,}\). The detailed deduction is provided in Appendix A. Then we simulate the policy update iteratively with random initial value and plot LASE's cooperation probability after convergence under various game parameters as illustrated in Fig. 2.

The results demonstrate that LASE converges to pure cooperation in Harmony and SH. In the more intense games, SG and PD, LASE stabilizes at cooperating with a probability greater than 0.5, successfully escaping from non-efficient Nash equilibria.

## 5 Experimental setup

### Environments

**Iterated Prisoner's Dilemma (IPD).** Here, we use iterated prisoner's dilemma (IPD) as an illustration to validate the theoretical analysis of LASE conducted in Section 4.3 and Appendix A. The specific game parameters are set as \([R,S,T,P]=[1,-0.2,1.2,0]\), represented

 P1/P2 & C & D \\  C & \((R,R)\) & \((S,T)\) \\ D & \((T,S)\) & \((P,P)\) \\ 

Table 1: Matrix-form game

Figure 2: The cooperation probability of LASE agents after convergence under different matrix-game parameters. The X-axis and Y-axis represent two parameters \(T\) and \(S\) respectively, where \(T[0:0.02:2]\) and \(S[-1:0.02:1]\).

as the red dot in Fig. 2. We employ the memory-1 IPD introduced in , with the state \(s=[,,,,s_{0}]\) comprising the joint action in the previous round and the initial state \(s_{0}\). The action space consists of two discrete actions - cooperation (C) and defection (D). Each episode lasts for 100 timesteps.

To evaluate the ability of LASE to address more complex environments, we study its performance in partially observable SSDs. SSDs extend matrix-form games in terms of space, time, and number of agents. Here, we study four specific SSDs: Coingame, Cleanup, Sequential Stag-Hunt (SSH), and Sequential Snowdrift Game (SSG) (Fig. 3). Schelling diagrams (see Fig. 10) of the four environments validate that they are appropriate extensions of representative game paradigms (a detailed analysis is given in Appendix B). Below is a detailed description of the four environments.

**Coingame.** The Coingame was originally introduced in  as a higher dimensional alternative to the IPD with multi-step actions. In this environment, two agents,'red' and 'blue', collect coins in a \(5 5\) map. A coin is either red or blue and there is only one coin on the map at any timestep. Agents will get a reward of \(1\) by picking up a coin of any color. However, when an agent collects a coin of a different color, the other agent will lose \(2\) points. After a pickup, a new coin with a random color and random location appears immediately. Therefore, if each agent greedily picks up all the coins, the sum of their expected scores will be 0.

**Cleanup.** In Cleanup, the goal of the agent is to gather as many apples as possible, with each apple carrying a reward of +1. However, the accumulation of waste in the river steadily approaches a depletion threshold, causing a linear decline in the apple growth rate to 0. At the beginning of each episode, the waste level exceeds the threshold and there are no apples in the map. This places the agent in a social dilemma: while individually focusing on collecting apples under the map leads to higher rewards, if all agents opt to refuse waste cleanup, no rewards are obtained. To maintain consistency with other environments, we partially modify the setting of cleanup from . We eliminate the actions of firing beams (cleaning and zapping) and require the agent to move to the waste's position to clean it. This does not alter the nature of the dilemma but makes it more challenging because the cleaning beam could have helped the agent clean from a distance.

**Sequential Stag-Hunt (SSH).** This environment is inspired by Markov Stag Hunt in . Each agent can get a reward of +1 by hunting a haze while hunting stags is more challenging and requires two or more agents. Each stag can bring a reward of +10, which is divided equally among the agents that jointly hunted it. The agent is immediately removed from the environment after successfully hunting once. Therefore, if an agent chooses to hunt stags, it must contend with the risk of no one cooperating with it. In contrast, hunting rabbits is a safer choice.

**Sequential Snowdrift Game (SSG).** In SSG, there are 6 piles of snowdrifts which can be removed by the agent. A pile of removed snowdrifts brings a +6 reward for each agent, but the remover incurs a cost of 4. So the agent waiting for others to remove the snowdrift (free-rider) can obtain a higher return. However, if no one chooses to remove it, no rewards can be obtained by anyone.

### Implementations

We employ fully decentralized training and execution for the agents, where all network parameters are independent. The policy structure of the agent comprises 2 convolutional layers for encoding observations, an LSTM layer to capture temporal information, and several fully connected layers activated by ReLU. The input is a multi-channel binary tensor, with the specific number of channels determined by the characteristics of different environments. For example, in Cleanup, 7 channels are

Figure 3: Graphic representations of four SSDs: (a) Coingame (5\(\)5 map), (b) Cleanup (8\(\)8 map), (c) SSH (8\(\)8 map), (d) SSG (8\(\)8 map).

incorporated, wherein the first four channels denote the positions of the four agents, the fifth and sixth channels signify waste and apples, and the last channel distinguishes between the inside and outside of the map through masking. To ensure adequate exploration, we let \((a|o)=(1-)(a|o)+/||\), with \(\) decaying linearly from \(_{}\) to \(_{}\) over \(_{}\) episodes.

The SR policy network and SR value network of the SRI module share two convolutional layers, followed by their own linear layers activated by ReLU function. The difference is that the inputs of the linear layers in SR value network include an additional concatenated one-hot vector of the joint action. The observation conversion network uses a single convolutional layer to encode observation, which is then concatenated with a one-hot vector representing the ID of other agents. This concatenated input is then processed through two linear layers, with the resulting output normalized using the sigmoid function.

In IPD, we modify the implementation by removing convolutional layers, reducing the parameters of FC layers, and appropriately increasing the learning rate to adapt the algorithm to this environment. See Appendix C for more details about the implementations of LASE.

### Baselines

Independent advantage actor-critic labeled **A2C** is a classical gradient-based RL algorithm. **LIO** learns an incentive function through the learning updates of reward recipients. **LOLA** considers the learning process of other agents when updating its own policy parameters. **IA**[11; 32] modifies the individual reward function by introducing inequity aversion. **SI**[13; 32] achieves coordination by rewarding agents for having causal influence over other agents' actions. We also show the approximate upper bound on performance by training the group optimal (**GO**) agents to maximize the collective reward. And we conduct ablation experiments **LASE w/o**, by removing the observation conversion network and replacing \(^{i}_{}(a^{j^{}}_{t}|^{j}_{t})\) in Eq. 3 with \(1/|^{}|\).

## 6 Results

### LASE promotes cooperation in social dilemmas

In **IPD**, as shown in Fig. 4, LASE successfully escapes the dilemma of non-efficient Nash Equilibrium (D, D). Both LASE agents converge to cooperate with a high probability, around \(0.93\) (see Fig. 3(a)), accompanied by a high collective reward. This is consistent with the theoretical results shown in Fig. 2, validating the effectiveness of LASE in dealing with social dilemmas. LASE is better at convergence speed and stability than LOLA, which also achieves a high collective reward. Unsurprisingly, GO, aiming to maximize group reward, reaches the upper bound. On the other hand, A2C, optimizing for one's own return, easily falls into the Nash equilibrium, where everyone defects and the group reward reduces to the minimum.

In **SSH** and **SSG**, LASE nearly reaches the upper bound of the total rewards as shown in Fig. 4(a) and Fig. 4(b): a total reward of 20 for successfully hunting two stages in SSH and a total reward of 120 in SSG for removing all the six snowdrifts. In **Coingame** (Fig. 4(c)) and **Cleanup** (Fig. 4(d)), LASE exhibits commendable performance by effectively avoiding the sub-optimal equilibrium, where everyone defects. GO outperforms LASE in SSG, Coingame, and Cleanup, because its optimization objective of maximizing the collective reward of all the agents and a strong assumption of the accessibility of everyone's reward function can help it avoid the dilemma.

On the other hand, it's worth emphasizing that the architecture of GO also gets it into the lazy problem , shown as its underperformance in SSH (Fig. 4(a)). In SSH, early hunting leads to moving

Figure 4: Results in IPD. (a) The learning path of two LASE agents. They start from the lower left and converge to the upper right of the phase diagram, where both agents cooperate with a probability around \(0.93\). (b) The collective reward of LASE and baselines. Five seeds are randomly selected for the experiment. The solid line represents the mean performance, while the shaded area indicates the standard deviation.

out of the environment and failing to obtain the group rewards of others' later hunting. Thus, GO may not hunt until the last few steps, and likely misses the the opportunity of cooperating to hunt stags.

A2C and SI hardly improve the collective return in Coingame and Cleanup, and fail to outperform LASE in SSH and SSG, where the dilemma is less intense. Although IA can directly access the rewards of others and adjust its own intrinsic rewards accordingly, it is still unable to get a higher collective return than LASE. Also as a gifting algorithm, LIO struggles to perform well in the four environments. Meanwhile, the need to manually specify the hyperparameters used to scale the amount of gifting also creates challenges for LIO to apply to different environments. In contrast, LASE breaks the limitations and achieves significantly better performance than LIO.

LASE w/o's convergence speed and performance are affected to a certain extent but not significantly. This is because LASE w/o differs from LASE by replacing the policy predictions of others with uniform policy to compute counterfactual baselines. It means that the gifting will continue as long as the first term of the numerator is substantial in Eq. 3. On the other hand, LASE builds a counterfactual baseline dynamically through perspective taking, and only gifts when the co-player's real action is superior to the baseline. Tab. 2 shows the mean of the gifting weights to other agents over the last 1e4 episodes of training, showing that LASE's gifting weights are lower than those of LASE w/o. Regarding gifting as a form of communication, LASE is valuable in reducing communication costs. Furthermore, to test LASE's scalability, we implement LASE in the extended Cleanup and SSG with 8 agents and a larger map. Results show that LASE is able to deal with more complex environments (see details in Section D.1).

### LASE promotes fairness

Considering the fact that the division of labor in Cleanup is more pronounced than in other environments and that cooperation in Cleanup is purely altruistic, making the dilemma more challenging, we take Cleanup as an example to show LASE's ability to promote fairness. Fig. 5(a) and Fig. 5(b) show the extrinsic rewards of each agent and the amount of waste cleaned by each in Cleanup. We find Agent 4 is the only one to clean and does not receive any extrinsic reward.

    & Coingame & Cleanup & SSG & SSH \\  LASE & **0.222** & **0.346** & **0.234** & **0.072** \\ LASE w/o & \(0.343\) & \(0.457\) & \(0.354\) & \(0.081\) \\   

Table 2: Mean of the gifting weights for LASE and LASE w/o during the last 10000 episodes of self-play training.

Figure 5: Learning curves in four SSDs. Shown is the collective reward. All the curves are plotted using 5 training runs with different random seeds, where the solid line is the mean and the shadowed area indicates the standard deviation.

Figure 6: Learning curves of each agent in Cleanup. Since the division of labor for different random seeds is not the same, only the results under one seed are shown to distinguish individual performance.

Fig. 5(c) illustrates the gifting weights each agent received from the other three agents. Agent 4 gets the most gifts, indicating that its cleaning contribution to the team is recognized and rewarded by the other three agents. Fig. 5(d) shows reward curves after gifting, where the reward gap between Agent 4 and the other three agents shrinks, implying fairness within the group is improved. Agents 1-3 always collect apples without cleaning, but this is not a free-rider behavior. Both cleaning and collecting are indispensable to get rewards in Cleanup. Although Agent 4 does not obtain any reward directly from the environment, the acquired reward is redistributed through gifting, narrowing the gap of reward between the cleaner and collectors. In contrast, although GO obtains higher collective returns, it sacrifices the individual interests of Agent 1 and Agent 2, while only Agent 3 and Agent 4 can get rewards Fig. 7.

We use _Equality (E)_ given by \(E=1-^{N}_{j=1}^{N}|R_{i}-R_{j}|}{2N_{i=1}^{N}R_{i}}\) to quantify the fairness, where the second term is the Gini inequality index. The greater the value of \(E\), the fairer it is. We can get \(E() 0.802\), \(E() 0.496\), showing that LASE achieves a higher level of fairness.

### LASE distinguishes co-players and responds adaptively

To evaluate LASE's adaptation ability to interact with various types of agents, we conduct an experiment in which a focal LASE agent interacts with three rule-based agents: cooperator (always clean up waste), defector (always try to collect apples), and a random agent. The gifting weights of LASE to the other agents are shown in Fig. 9. LASE can explicitly distinguish between different types of co-players. Moreover, it responds in a manner that aligns with human values: preferring to share rewards with cooperators rather than defectors.

To study how LASE responds dynamically and how it affects collective behavior, we conduct an experiment where one focal LASE agent interacts with three A2C agents (Background agents, Bgs). A GO agent is trained in the same way for comparison with LASE. Fig. 8(a) displays each agent's rewards after training for 30k episodes, whereas the LASE group shows the reward after gifting.

LASE and GO improve the group reward to a similar level, while four A2C agents will converge to the equilibrium of defection and gain almost no reward (see Fig. 4(d)). With gifting, LASE incentives _Bg3_ to clean as shown in Fig. 8(b). Thus, LASE and the other two _Bgs_ can get rewards by gathering apples. But for GO, the focal agent sacrifices itself to undertake all the cleaning tasks and cannot get any reward.

GO and LASE represent two different methods to foster cooperation. GO sacrifices its own interest to promote collective reward. LASE attempts to alleviate social dilemmas by incentivizing others to cooperate. When such an incentive mechanism fails, LASE will no longer gift those agents who constantly exploit it, like the defector in Fig. 8. Overall, we believe that LASE is a more efficient and secure policy for SSDs, as it can promote cooperation as well as avoid potential exploitation by others.

## 7 Conclusion

We introduce LASE, a decentralized MARL algorithm that fosters cooperation through gifting while safeguarding individual interests in mixed-motive games. LASE uses counterfactual reasoning to

Figure 8: LASE’s gifting weights to three rule-based co-players.

Figure 7: Four GOs’ rewards in Cleanup.

Figure 9: An LASE (or GO) agent interacts with three A2C agents in Cleanup. (a) The average reward for LASE and GO groups after training 30k episodes. The LASE group shows the reward after gifting. The first two bars show whole groups’ rewards. The remaining bars show the average reward for each agent. (b) The amount of the waste cleaned by each agent in the LASE group.

infer the social relationships with others which captures the influences of others' actions on LASE and modulates the gifting strategy empathetically. In particular, to empower LASE with the ability to infer others' policies in partially observable and decentralized environments, we establish a perspective taking module for LASE. Both theoretical analyses in matrix-form games and experimental results across diverse SSDs show that LASE can effectively promote cooperative behavior while ensuring relative fairness within the group. Furthermore, LASE is also able to recognize various types of co-players and adjust its gifting strategy adaptively to avoid being exploited, enabling broad applicability in complex real-world multi-agent interactions, such as automated negotiations in E-commerce and decision-making in autonomous driving. Whilst LASE exhibits superior abilities, there are some limitations of our method. What would be the consequence of giving agents the ability to refuse gifts? How to extend the algorithm LASE to continuous action space? These problems will illuminate our future work.