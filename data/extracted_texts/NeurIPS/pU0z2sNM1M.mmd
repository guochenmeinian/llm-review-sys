# Causal Dependence Plots

Joshua R. Loftus

Department of Statistics

London School of Economics

London, England, UK

j.r.loftus@lse.ac.uk

&Lucius E. J. Bynum

Center for Data Science

New York University

New York, NY, USA

lucius@nyu.edu

&Sakina Hansen

Department of Statistics

London School of Economics

London, England, UK

s.a.hansen1@lse.ac.uk

###### Abstract

To use artificial intelligence and machine learning models wisely we must understand how they interact with the world, including how they depend causally on data inputs. In this work we develop Causal Dependence Plots (CDPs) to visualize how a model's predicted outcome depends on changes in a given predictor _along with consequent causal changes in other predictor variables_. Crucially, this differs from standard methods based on independence or holding other predictors constant, such as regression coefficients or Partial Dependence Plots (PDPs). Our explanatory framework generalizes PDPs, including them as a special case, as well as a variety of other interpretive plots that show, for example, the total, direct, and indirect effects of causal mediation. We demonstrate with simulations and real data experiments how CDPs can be combined in a modular way with methods for causal learning or sensitivity analysis. Since people often think causally about input-output dependence, CDPs can be powerful tools in the xAI or interpretable machine learning toolkit and contribute to applications like scientific machine learning and algorithmic fairness.

## 1 Introduction

This paper develops Causal Dependence Plots (CDPs) to visualize relationships between input variables and a predicted outcome. Motivated by explaining or interpreting AI or machine learning models , for simplicity we consider supervised learning, i.e. regression or classification. We also focus on the model-agnostic or "black-box" setting, where the interpreter can query the model but not access its internal structure. Model-agnostic interpretation methods are functionally limited to observing how the model responds to variation in the inputs. While this initial application forms our practical motivation, we emphasize that CDPs are more general.

Simple explanations that focus on one input variable at a time can be powerful tools for human understanding. However, just as with the interpretation of linear regression model coefficients, these simple relationships can be misleading. When varying one input variable, _we must make some choice about what values to use for the other inputs_. CDPs make this choice using an explicit causal model, and to our knowledge this is the first work that does so. We compare CDPs to other state-of-the-art, non-causal explanation methods like the Partial Dependence Plot (PDP) , Individual Conditional Expectation (ICE) , Accumulated Local Effect (ALE) , and Shapley Additive Explanation (SHAP) feature plot . Explanation methods may respect existing causal dependencies between predictors or break them.

**Problem statement.** If there are causal relationships between predictors but our visualization, interpretation, or explanation method does not respect them the resulting model explanation may be irrelevant or misleading . Such explanations could lead to incorrect decisions for regulating or aligning algorithmic systems, sub-optimal allocations of resources based on model predictions, a breakdown between human feedback and reinforcement learning systems, or other forms of error andharm. In scientific machine learning--where explanations can be used to generate hypotheses for follow-up investigation--a flawed interpretation may support spurious hypotheses. For these reasons, _the causal validity of model explanations should be a top priority_.

**High level proposal.** We wish to interpret or explain a given supervised machine learning model \(()\) with \(p\) input features \(=(x_{1},,x_{p})\). Specifically, we want to understand how the predictions \(=()\) of this model depend on feature \(x_{j}\) for a given \(j\), \(1 j p\). PDPs and ICE plots do this by varying \(x_{j}\) and holding the other features \(_{ j}\) constant, where \(_{ j}\) is the \((p-1)\)-tuple containing all features except for \(x_{j}\). This implicitly assumes independence between \(x_{j}\) and the other inputs. Our method replaces this independence assumption with an Explanatory Causal Model (ECM) for the input features. This auxiliary ECM is a tool we use to help explain \(\), it determines how other inputs \(_{ j}\) vary when \(x_{j}\) is changed.

**CDP pseudo-algorithm.** To construct a CDP showing how \(()\) depends on \(x_{j}\), a user specifies an ECM \(\) containing the predictors \(\), and an intervention \(I(x_{j})\) in \(\). The intervention changes \(x_{j}\), and may change other features if they are caused by \(x_{j}\) in \(\). The type of intervention is chosen based on the type of causal explanation desired, with several example options demonstrated later. An explanatory dataset \(=\{_{i}:i=1,,n\}\) can be given or, if unavailable, generated by \(\). Note that we use the notational convention where \(i\) indexes observations or examples while \(j\) indexes features. The horizontal axis of the plot is specified by a grid \(\{_{j,k}:k=1,,K\}\) of possible values for \(x_{j}\), with \(k\) indexing grid points. For each observation \(_{i}\) in \(\), and at each grid point \(_{j,k}\):

1. Use the ECM to simulate counterfactual values \(^{*}_{i,k}\) for all features of observation \(i\) under the intervention \(I(_{j,k})\).
2. Input counterfactual features to the prediction function \(\), and store the resulting counterfactual prediction \(^{*}_{i,k}=(^{*}_{i,k})\) in an array indexed by \((i,k)\).

For each observation \(i\) in \(\), construct the individual counterfactual prediction curve \((_{j,k},^{*}_{i,k})\) by connecting points that are adjacent on the plot grid. Plot the empirical average of these curves, which is the main output of the CDP. The individual curves can be shown or suppressed as desired. _The resulting CDP shows how the model's predictions \(\) causally depend on \(x_{j}\) when this predictor is varied by the intervention \(I(x_{j})\) in ECM \(\)_.

**PDP and ICE algorithm.** Start with the notation and setup as above but without any ECM or intervention. For each observation \(_{i}\) in \(\), and at each grid point \(_{j,k}\):

1. Define \(^{}_{i,k}\) by setting feature \(x_{ij}\) to the grid point \(_{j,k}\) and keeping other features \(_{ j}\) fixed at their original values in \(_{i}\) from \(\), that is \[^{}_{i,k}:=(x_{i1},,x_{ij}_{j,k}, ,x_{ip}).\] (1)
2. Compute prediction \(^{}_{i,k}=(^{}_{i,k})\) and store in an array indexed by \((i,k)\).

Plotting \((_{j,k},^{}_{i,k})\) generates an ICE curve for each \(i\), and the empirical average of these is the PDP.

**Motivating example.** Consider a model for parental income \(P\), school funding \(F\), and graduates' average starting salary \(S\), with ECM shown in the bottom row of Figure 1. In the top row, the ECM functions are plotted in the left panel, and the remaining panels show visual explanations of supervised models that predict \(=(P,F)\). In this example the training data for black-box models was generated by the ECM, but later we will see real data examples where this is not the case. Blue curves show how \(\) depends on \(P\) when \(P\) is causally manipulated _without_ holding \(F\) constant, i.e. under the intervention \((P=p)\). Orange curves show the dependence of \(\) on \(P\) when \(F\) is held constant at its observed value, and coincide exactly with standard PDPs. Full definitions of these are given in Section 2. Several key takeaways:

* Comparing the direct (or partial) dependence curves and total dependence curves we see _there can be qualitative differences depending on the type of explanation desired_, for example one can be increasing while the other is decreasing. This is a consequential fact when considering how interventions may change (predicted) outcomes. Increasing \(P\) causes larger values of \(\), but if the increase in \(P\) is done while holding \(F\) constant then it could cause a decrease in \(\) (or a smaller increase). **An intervention which does _not_ hold other 

**predictors constant--arguably the canonical causal operation--can be shown by our TDP**.
* _Our framework includes some existing model explanation plots like ICE and PDPs as special cases_. In panels (c-d), and later in Theorem 2.10, we see that \(+=\). A practitioner seeing only the PDP in panel (d) may conclude that "dependence" of \(\) on \(P\) is weak, especially if \(P 1\). The TDP in panel (c) shows a stronger increasing relationship closer to the true total dependence and a more holistic view of how \(\) depends on \(P\). Our work clarifies that the weaker form of dependence shown by PDPs is natural direct dependence. We also see the same weak dependence empirically for SHAP and ALE plots in Figure 4.
* _Explanations of models can be qualitatively different from the underlying causal relationships_. For example, the random forest in panel (c) shows a direct dependence of \(\) on \(P\) that is increasing when the true direct dependence of \(S\) on \(P\) is decreasing. Predictive machine learning models may fail to capture causal dependence, and in this case studying the black-box would not necessarily help us learn about the real world. As another example, panel (b) shows that the total dependence of a linear model on a predictor can be non-linear, in this case because the mediator \(F\) depends non-linearly on \(P\).

### Applications

Different combinations of the predictive setting and choice of ECM generate many uses for CDPs. In general, ECMs can be designed based on a particular desired explanation, make use of prior domain knowledge, or be learned and estimated from data using causal learning methods in a modular fashion. Importantly, the ECM does not need to contain the outcome variable \(y\) except in one special case--residual plots--to be discussed later.

**Causal bridge.** In one special case we may wish to understand how \(\) depends on a variable \(z\)_which is not one of \(\)'s input features_ but is causally related to them. Other methods cannot do this, but CDPs can provided the ECM also contains \(z\). For simplicity we choose notation in the rest of the paper to reflect the case where the explanatory variable is an input, but this is not a loss of generality

Figure 1: Motivating example. Causal Dependence Plots (top row) and the Explanatory Causal Model (bottom) for the motivating example. Points show the explanatory dataset, which in this example is also the training data for the predictive models. Counterfactual curves for individual points are shown as thin, light lines, with averages displayed as thick, dark lines. Total Dependence (TDP) is represented in blue and Natural Direct Dependence in orange. Panel (a) shows the relationships of the ECM. Panels (b-c) show CDPs for a linear model and random forest (RF) model, respectively. Panel (d) shows PDP and ICE curves for the RF model from a standard software library. This is identical to our NDDP in panel (c). We show this holds true in general: PDP/ICE are a special case of CDPs.

since we can simply define \(_{}(,z):=()\) and apply CDPs to \(_{}\). Since the ECM may vary \(\) when \(z\) is changed, we can see how \(\) depends on \(z\). _This could be useful to probe a predictive model for fairness with respect to a sensitive attribute that the model does not use directly_.

**Incomplete causal knowledge.** There are various applications where an ECM does not need to be a fully specified or "correct" model for all features. First, CDPs only use the predicted--and not actual--outcome. This is useful for semi-supervised or anticausal learning: given causal structure _among predictors only_ and a supervised learning model, attempt causal inference for the outcome [52; 63]. Second, we may only require explanations or plots for one or a small number of features. In such cases _we only need information from the ECM about interventions on the features of interest and their causal descendants_, and not other predictors. Finally, predictive models often use features that are known transformations or representations of inputs, and these transformations can be used to construct an ECM. For example, if the features are \((x,x^{2},z)\), an ECM can simply encode the fact that \(x^{2}\) depends causally on \(x\). Even if we do not know the dependency between \(x\) and \(z\), we can make use of our partial knowledge about the features.

**Multiparty auditing, e.g. for fairness**. An owner of a predictive model may not have causal knowledge or incentives to use such knowledge. Predictive accuracy is their only concern. But a separate party, like a regulator, may audit that model. This party may have more causal knowledge due to specializing in auditing, or may be legally obliged to make certain causal assumptions for the purpose of the audit, e.g. to allow disparities only along certain causal pathways and not others. Previous work applied causality to fairness [5; 10; 27; 28; 29; 32; 34; 38; 48; 60; 62], recourse [7; 26; 44; 57], and other desiderata. _Existing methods like PDPs are limited to only showing direct dependence, and this may hide the full extent of unfairness or discrimination_. CDPs can be used to probe a black-box for unfairness in the form of total dependence or partially controlled dependence.

**Explanations under covariate shift.** Often a pre-trained model is used for predictions on data from a different data generating process than the training DGP. We can use ECMs and CDPs to visualize how the model will behave out-of-distribution. ECMs could even be chosen adversarially.

**Scientific theory development.** Large and complex models may be fit to data where underlying structure is largely unknown. In such settings, relatively simple ECMs can be used to formulate simple hypotheses relating some predictors and plot causal dependencies to check these hypotheses or generate new ones. This can also be done hypothetically, assuming an ECM for exploration.

### Contributions

After defining the CDP framework we demonstrate CDPs on synthetic and real datasets in Section 3 and Appendix B, including in conjunction with structural causal learning in B.2. We compare CDPs with other state of the art competitor visualization methods, for example in Figure 4. Theorem 2.10 establishes the first universally valid causal interpretation of CDP and ICE plots. Finally, in Section 2.7 we illustrate how to visualize uncertainty about the choice of ECM.

## 2 Methodology

### Supervised learning models

We are given a predictive model \(\), possibly estimated or learned using empirical risk minimization (ERM) \(=_{h}_{i=1}^{n}(h(_{i}), y_{i})\) with some loss function \(\), pre-specified function class \(\), and an independent and identically distributed training sample \(\{(y_{i},_{i}):i=1,,n\}\) with feature vectors \(_{i}^{T}^{p}\). In Section 2.6 we focus on simple mediation analysis and partition the predictor variables into subsets so that \(X\) and \(M\) both notate predictors, \(M\) being a mediator.

### Fundamental problem of univariate explanations

To create an explanation of model dependence on a single feature, like a plot with \(x_{j}\) on the horizontal axis and \(\) on the vertical axis, _we must decide what to do with the other features_ when varying \(x_{j}\) along the plot axis. Most explanation methods use the same approach as the PDP and ICE plots: they _hold other features fixed_ at values in a (auxiliary, explanatory) dataset. This may be unrealistic if other features depend on \(x_{j}\) causally, or even mathematically undefined if features are mutually constitutive, e.g. \((x,x^{2})\) or the set \(\{,,\}\).

### Structural Causal Models

Our notational conventions and definitions are influenced by [6; 41; 43]. Let \(\) be a set of exogeneous noise variables, \(\) a set of \(p=||\) observable variables, and \(\) a set of functions such that for each \(j\) we have \(V_{j}=g_{j}(_{j},U_{j})\), where \(_{j}\) and \(U_{j}\) are the observable and exogeneous parents, respectively, of variable \(V_{j}\). Let the directed acyclic graph (DAG) \(\) have vertices given by variables and, for each \(V_{j}\) and each of the parent variables in \(_{j}\) and \(U_{j}\), a directed edge oriented from the parents to \(V_{j}\).

**Definition 2.1** (Structural Causal Model (SCM)).: A (probabilistic) SCM \(\) is a tuple \(,,,P_{}\) where \(P_{}\) is the joint distribution of the exogeneous variables. This distribution and the functions \(\) determine the joint distribution \(P^{}\) over all the variables in \(\). Finally, causality in this model is represented by additional assumptions that \(\) admits the modeling of interventions and/or counterfactuals as defined below.

**Definition 2.2** (Interventions).: For the SCM \(\), an intervention \(I\) produces a modified SCM denoted \(^{(I)}\) which may have different structural equations \(^{I}\). Correspondingly, some variables may have different parent sets, so the DAG representation \(^{(I)}\) may also change. We denote the new, interventional distribution as \(P^{;(I)}\). A simple class of interventions involves intervening on one variable, e.g.

\[I=(V_{j}(}_{j},_{j}) ),\]

which changes how \(V_{j}\) and all variables on directed paths from \(V_{j}\) in \(\) are generated. An even simpler sub-class of these are the atomic interventions setting one variable \(V_{j}\) to one constant value \(v\), which we denote \(I_{j,v}:=(V_{j}=v)\). Note that in this case \(V_{j}\) has no parents in the graph \(^{(I)}\); the source of the intervention itself is outside the world of the model.

Interventions are useful for modeling changes to a data generating process (DGP), for example, experiments that control a particular variable to see how its value changes other variables, or policy changes aimed at altering or removing existing causal relationships. In addition to generating new observations as a DGP, an SCM can also be used to model counterfactual values for observations that have already been determined. A counterfactual distribution is an interventional distribution defined over a specific dataset with information or constraints given by some of the observed values in that data, as we now describe.

**Definition 2.3** (Counterfactuals).: Let \(\) be the observed variables for observations in a given dataset, \(_{j}=\) and \(U_{j}\) the observed and exogeneous parents of variable \(V_{j}\), and \(I\) and intervention that modifies any of \(V_{j}\)'s parents. The intervention \(I\) may hold some or all of \(\) fixed and vary \(U_{j} u\), passing these through \(g_{j}(,u)\), or through \(_{j}(},u)\) if the intervention also changes any of \(}\). The counterfactuals \(V_{j}(},u)\) are values \(V_{j}\) would have taken if any of its observed and/or exogeneous parents had taken the different values \((},u)\). To define the counterfactual distribution \(P^{|=;(I)}\), we use the posterior or conditional (depending on our probability model approach) distribution \(P_{|=}\) to model uncertainty about \(\) while computing counterfactual values of variables for an observation in the modified SCM \(^{(I)}\).

_Remark 2.4_.: Note that if the desired causal explanation uses counterfactuals, then we likely obtain the observed values from an auxiliary explanatory dataset. But since an SCM can generate data, we may also use it to generate the initial observed values and then re-use these when computing counterfactuals for the explanation.

### Causal explanations

Our proposed solution to the fundamental problem highlighted for univariate explanations is to use an auxiliary ECM \(_{j}\) and let this causal model determine how other features vary as functions of \(x_{j}\). We denote these explanations as \(_{j}(;_{j})\) or \(_{j}()\) if the context is clear. In the deterministic or noiseless case, suppose we know continuous functions \(g_{kj}\) such that \(x_{k}=g_{kj}(x_{j})\), with \(g_{jj}\) the identity. In this case the model \(_{j}\) tells us \(g(x_{j}):=(g_{1j}(x_{j}),,x_{j},,g_{pj}(x_{j}))\) is a curve in \(^{p}\) parameterized by \(x_{j}\), and we generate the explanation \(_{j}()\) by plotting \((g(x_{j}))\) against \(x_{j}\).

To extend this strategy to non-deterministic causal models we use an ECM \(_{}\) for the predictor variables with \(_{}\) its associated DAG. We represent this graphically in Figure 2. The expressive power of SCMs allows us to pose various interpretive questions and compute various kinds of explanations by performing operations in \(_{}\).

### Causal Dependence Plots

For the following definitions, we assume predictor variables \(_{}\), an outcome of interest \(Y_{Y}\), and a black-box function \((x):_{}_{Y}\) with outputs that we may also denote \(\). A structural causal model \(\), either assumed or learned from data, specifies causal relationships for the predictors \(\), i.e. it need not involve the outcome \(Y\). Note that predictors may only be a subset of the variables in \(\) as in the "causal bridge" application discussed in Section 1.1.

**Definition 2.5** (Explanatory Causal Model (ECM)).: An ECM \(^{}\) augments the SCM containing predictors by including the predicted outcome \(\) as an additional variable with \(\) as its structural equation.

Generating causal explanations for \(\) involves performing abduction, action, and prediction with this ECM. In a large ECM graph we may suppress all arrows into \(\) except those from the explanatory feature and its descendants. This is to simplify the display, as in Figure 5.

**Additional notation and conventions.** We use the shorthand \((P^{})\), where \(\) takes a distribution \(P^{}\) as its argument, to denote using data from that distribution as the input to the black-box function \(\). For each type of causal explanation with a given _Named Effect_ based on intervention \(I\), we define the _Individual Counterfactual Named Effect_ curves as the set of counterfactual curves \((P^{|=;(I)})\) for each individual, the _Named Effect Function_ as their (empirical) expectation \(}[(P^{|=;(I )})]\), and the _Named Dependence Plot_ as a plot displaying all of these curves.

**Definition 2.6** (Causal Dependence Plot (CDP)).: Given a function \(\), explanatory dataset \(\), ECM \(\), and family of interventions \(I_{}\) parameterized by \(\), we construct a plot with \(\) as the horizontal axis and display Individual Counterfactual (IC) curves

\[()=(P^{|=;(I_ {})}).\] (2)

These show the effect of intervention \(I_{}\) on black-box output for each individual observation in the explanatory dataset as \(\) varies. The (empirical) average of these (over the explanatory data) is (an estimate of) the Causal Effect Function (CEF), and a plot showing the IC and CEF is a Causal Dependence Plot.

We typically apply this to create plots for one explanatory feature \(X_{s}\) at a time using interventions like \(I_{}=(X_{s}=)\). Horizontal axes for plots use a grid over the possible values of \(X_{s}\) given by its range in dataset \(\). Bar graphs can be used when the explanatory feature is categorical.

Figure 2: An ECM for predictors is used to produce an explanation \(()\) of the predictive model \(\). Solid arrows represent possible causal relationships in the ECM, and dotted arrows show dependence of the model explanation on predictors. In (a) \(_{}\) denotes the subgraph of the SCM for predictors. In the mediation example (b), predictor \(X\) causes \(Y\) directly and also through mediator \(M\), creating an important distinction between direct dependence (orange) and total dependence (blue). The reverse causality example (c) shows variables useful in predicting \(Y\) may be caused by \(Y\), and also be causes of prediction \(=(X,M)\) and the explanation of that prediction.

Next is perhaps the most straightforward and important named effect.

**Definition 2.7** (Total Dependence Plot (TDP)).: For an intervention \(I\), the Individual Counterfactual Total Effect (ICTE) curves

\[(I)=(P^{|=;(I)})\] (3)

show the total effect of intervention \(I\) on black-box output for each individual observation in the explanatory dataset. The (empirical) average of these (over the explanatory data) is (an estimate of) the Total Effect Function (TEF), and a plot showing the ICTE and TEF is a Total Dependence Plot (TDP). We compute the TDP following Algorithm 1.

Inputs: \(\) (ECM), \(\) (black-box predictor), \(\) (explanatory dataset), \(X_{s}\) (covariate of interest)

Let \(X\) be a grid of possible values of \(X_{s}\)

Set \(N\) to the number of observations in \(\)

Initialize \(N|X|\) matrix of predictions \(\)

**for**\(x\) in \(X\)**do

Define intervention \(I=(X_{s}=x)\)

Sample counterfactual dataset \(_{X_{s} x}\) entailed by \(P^{|D;(I)}\)

Set \([:,x]\) to \((_{X_{s} x})\)

**end for**

Plot \(N\) lines \((X,[i,:])\) (Individual Counterfactuals)

Plot average \((X,_{i}[i,:]/N)\) ((Causal Dependence))

**Algorithm 1** Total Dependence Plot (TDP)

_Remark 2.8_.: In the remaining definitions, we give notation only for the individual counterfactual curves and leave the other objects implicitly defined.

We often wish to decompose how much of the total effect of \(X\) on \(\) is attributable to different pathways between the variables. This can be explored via direct dependence below, as well as with other named CDPs described in Appendix A.

**Definition 2.9** (Natural Direct Dependence Plot (NDDP)).: Given intervention \(I\) define a corresponding intervention \(J\) that intervenes on all descendants of any variables that are changed by \(I\), except for \(\), and resets them to their observed values in dataset \(\). We then define the Individual Counterfactual Natural Direct Effect curves

\[(I)=(P^{|=;(I,J)}).\] (4)

This quantity represents the effect of intervention \(I\) on black-box output \(\) while all variables not directly intervened upon by \(I\) are fixed at their 'natural', i.e., pre-intervention values in \(\). Algorithm 4 demonstrates how to compute the NDDP.

From this construction of NDDP, we see by comparing it to the **PDP and ICE algorithm** that it is equivalent to these, confirming what we observed in Figure 1(d).

**Theorem 2.10** (Pdp + ICE = NDDP).: _When generating plots for the predictive model \(\) using the dataset \(\) and feature \(X_{s}\), the ICE plot curves and Individual Counterfactual Natural Direct Dependence curves are identical. Hence, the NDDP is identical to a PDP that includes ICE curves._

Proof.: We have implicitly assumed both plots will use the same range for their horizontal axes. This is natural as implementations use the range of the feature in the dataset, and both plots are constructed from the same feature \(X_{s}\) in the same dataset \(\). Since the PDP and NDDP both contain empirical averages of their respective Individual curves it suffices to show these are equal at each point \(\) in the plot grid.

Consider individual \(i\) in dataset \(\) with features \(_{i}=(x_{i1},,x_{ip})\). The value of the ICE curve at \(\) for this individual is, from (1), \((_{i,k}^{})\) where \(_{i,k}^{}:=(x_{i1},,x_{is},,x_{ ip})\), i.e. the original features \(_{i}\) but with entry \(s\) set to \(\). We must show this is the same as the value of the Individual Counterfactual Natural Direct Dependence curve at \(\) for individual \(i\). Applying Definition 2.9, we use interventions

\[I=(X_{s}=)J=(X_{j}=_{j}X_{s}X_{j}).\]he NDDP applies these in the order \(I\) followed by \(J\). First, \(I\) sets the value of feature \(X_{s}\) to \(\) for all individuals, and may modify other features if they are descendants of \(X_{s}\). Then \(J\) intervenes on each descendant \(X_{j}\) of \(X_{s}\) and resets it to its observed values in \(\), and in particular for individual \(i\) these are each reset to \(x_{ij}\). Hence, the value of the Individual Counterfactual Natural Direct Dependence curve at \(\) for individual \(i\) is also given by \((_{i,k}^{})\). 

_Remark 2.11_.: Note that there is some subtlety in the assumption of using the same dataset: a Bayesian probability modeling approach to SCMs may add more randomness when sampling counterfactuals. In this case, rather than the ICE and ICNDD curves being identical, the ICE will equal the expectation of the ICNDD curves over this additional source of randomness. The additional randomness is specified by priors over exogenous variables, and the expectation can be estimated by more sampling at a computational cost of a constant factor. Since the usage of these plots is visual and somewhat qualitative this subtlety is not an important limitation of the theorem, and it would only apply under particular modeling assumptions.

_Remark 2.12_.: To our knowledge this is the first result establishing a universally valid causal interpretation of PDPs. **Its most important limitation is that it applies to the model output \(\) and not necessarily the original outcome \(Y\)**.

Several other types of named CDPs are described in Appendix A.

### Mediation analysis

Many applications involve a causal structure we refer to as a mediation triangle, with examples shown in Figure 1 and Figure 1(b). In mediation analysis, we often wish to decompose how much of the total effect of \(X\) on \(Y\) is attributable to the pathway through \(M\) and how much of it is direct. CDPs allow us to visualize frequently studied quantities of interest in this setting including other special cases defined in Appendix A.1. Although mediation analysis motivates CDPs and helps build intuition, we emphasize that our definitions can be used _with any structural causal model_. See Section 3 for other, more complex examples.

### Uncertainty and sensitivity analysis

There are various ways to incorporate uncertainty about the ECM into CDPs. We explore a natural first extension of the CDP that shows a _range between possible effect functions_ induced by a _set of auxiliary ECMs_. The set of ECMs could be pre-specified or, for example, given by a Markov equivalence class output by a causal structure learning algorithm. Returning to our motivating example from Section 1, we might question whether parental income \(P\) impacts school funding \(F\), considering instead an SCM without mediation: \(P S F\). Figure 3 shows a range of possible effect functions interpolating between this ECM without the indirect effect and the original ECM in Section 1, for each of the TDP, NDDP, and NIDP. In this we have assumed the same structural equations for the edges that are common to both models. These plots show a range for how \(\) may depend on \(P\) when we are unsure how \(F\) depends on \(P\). Figure 9 in Appendix B.2 shows an example with real data where we use candidate ECMs discovered by the PC algorithm. These examples are not confidence regions, but any method for producing confidence sets in SCMs could also be used with CDPs to display uncertainty regions. Future work can develop additional methods for

Figure 3: TDP, NDDP, and NIDP uncertainty bands for the salary example using the random forest model in Figure 1. The range of curves is induced by two candidate ECMs described in Section 2.7.

visualizing uncertainty, for example by leveraging sensitivity analysis based on conformal prediction [9; 21; 31; 61].

## 3 Experiments

We demonstrate CDPs in a series of experiments with simulated and real datasets.

**Comparison with other explanatory plots.** Figure 4 shows accumulated local effect (ALE)  and Shapley Additive Explanation (SHAP)  feature plots for the salary example. These appear similar to the PDP, with a more weakly increasing relationship than that seen in the TDP. TDPs represent a significant and novel contribution to the existing model visualizations.

**Real data with domain knowledge.** An ECM may be constructed using domain expertise. Figure 5 shows an ECM and CDPs for the Sachs et al.  dataset of expression levels of proteins and phospholipids in human cells, for which data and a ground-truth DAG1 are publicly available in the Causal Discovery Toolbox . While the actual biology of the problem is not our focus here, there are meaningful takeaways from the figure. For this model, the TDP shows an increasing relationship, while the NDDP/PDP shows a decrease. _The overall direction of the trend in predictions based on PKA is reversed if we hold other predictors fixed_. This is an important lesson for using model explanations in scientific machine learning.

Figure 4: Comparison of CDP (a) with PDP (b), ALE (c), and SHAP plots (d) for the salary example in Figure 1. Our TDP stands out, and all other plots are qualitatively similar.

Figure 5: ECM for the Sachs et al.  dataset (left), CDPs for an MLP predictive model (center), ALE (line) and SHAP (points) plots (right). All plots visualize the effect of PKA on predicted p44/42. PKA and its descendants are bolded. The NDDP (i.e. PDP + ICE), ALE, and SHAP all show an overall decrease, while the TDP shows an increase. _Conclusions depend strongly, qualitatively, on the specific interpretive question we ask, and causal modeling allows us to formulate questions precisely_.

Additional experiments can be found in Appendix B. Notably, in Appendix B.2 we _learn an ECM from data with a causal structural learning algorithm_ and then use it to produce CDPs. The main takeaway of the real data experiments is that CDPs can be useful in practice.

In simulation experiments we know the true DGP, so we can compare its functional form to various black-box models and their explanatory plots. Results in Appendix B.1 show that CDPs are sensitive to whether the functional form assumptions of the black-box model fit the DGP. In other words, _if a black-box model is a poor fit to the DGP, then CDPs can accurately explain the black-box but will not reflect the true DGP_. This limitation is not specific to CDPs but applies to all explanation methods. Figure 6 also shows that different ECMs can produce different CDPs for the same black-box model, and _a misspecified ECM can produce misleading CDPs_.

## 4 Discussion

**Related work.** Recent work in recourse  uses contrastive or counterfactual explanations . Some of this focuses on causal dependence . Blobaum and Shimizu  identify the predictor with the largest total effect, which is most applicable when assuming linearity. Zhao and Hastie  investigated causal interpretations of PDP, aiming for causal inference about the underlying DGP, and showed that when the DGP satisfies the backdoor criterion  then a PDP visualizes the total effect (TE) of a predictor. Cox Jr  observed an association between partial dependence plots and NDE, an equivalence we formally establish in Theorem 2.10, to our knowledge the first such result. Lazzari et al.  weight observations when computing PDPs. There has been some recent work creating causal variants of SHAP , and in future work we will explore comparisons of appropriate special cases of CDPs with these. We are not aware of any previous causal explanation work with the generality of CDPs.

**Limitations.** Causal modeling always involves some limitations . For CDPs, full specification of an ECM can be a strong assumption. However, in Sections 1.1 and 2.7 we discussed some ways this can be relaxed. In general, _if a causal explanation is desired or necessary, then we cannot avoid making causal assumptions_. Model-agnostic explanation methods also always have certain limitations . For example, if the predictive model fails to fit the DGP, then any model explanation will also fail if our interpretive goal is to learn about the DGP . _CDPs may be misleading if the true DGP differs in important ways from the ECM_, as shown in Figure 6. However, standard PDPs and similar explanation methods also require auxiliary explanatory data, and that data may also differ from the target DGP. So this is not an additional limitation specific to our method.

**Conclusion.** Causal Dependence Plots use an explanatory causal model to create plots with causal interpretations. This allows us to use the powerful language of structural causal models to pose and answer a variety of meaningful questions. Our framework generalizes Partial Dependence Plots, which Theorem 2.10 shows it includes as a special case, and allows other kinds of causal interpretations we have not seen previously explored in the literature. Future work in this direction could expand on some canonical causal structures for useful applications, or interface with other kinds of models, for example extending to non-tabular data by applying causal representation learning. Relating explanation methods to Pearl's ladder of causation , most previous interpretable machine learning and explainable AI methods--like PDPs--concer associations and hence are confined to the first rung of the ladder. With CDPs we ascend the ladder, creating model interpretations intended to change the world. Interpretability provided the initial motivation for CDPs, but since plots are qualitative CDPs also open the door for future work on causal methodology that relaxes assumptions while maintaining _visual validity_.

**Broader Impacts.** There are many potential societal consequences of our work: essentially those shared by all tools for model interpretability and explainability. Model explanations can be misleading, either due to error or intentional deception. When a user is convinced by a flawed model explanation to reach misguided conclusions about a model, they may make harmful or sub-optimal decisions about how or whether to use that model. For example, if an explanation tool is used to assess the fairness of a model, a flawed explanation could lead to the conclusion that a discriminatory model is fair or that a fair model is discriminatory. In applications related to science, a flawed model explanation can lead to wasting resources pursuing a dead-end hypothesis or to missing out on an important discovery. Similarly, poor business decisions can be made on the basis of flawed explanations.