# Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering

Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering

 Yijun Dong

Courant Institute of Mathematical Sciences

New York University

New York, NY

yd1319@nyu.edu

&Kevin Miller

Oden Institute for Computational

Engineering & Science

University of Texas at Austin

Austin, TX

ksmiller@utexas.edu

&Qi Lei

Courant Institute of Mathematical Sciences

& Center of Data Science

New York University

New York, NY

q1518@nyu.edu

&Rachel Ward

Oden Institute for Computational

Engineering & Science

University of Texas at Austin

Austin, TX

rward@math.utexas.edu

Equal contribution.

###### Abstract

Despite the empirical success and practical significance of (relational) knowledge distillation that matches (the relations of) features between teacher and student models, the corresponding theoretical interpretations remain limited for various knowledge distillation paradigms. In this work, we take an initial step toward a theoretical understanding of relational knowledge distillation (RKD), with a focus on semi-supervised classification problems. We start by casting RKD as spectral clustering on a population-induced graph unveiled by a teacher model. Via a notion of clustering error that quantifies the discrepancy between the predicted and ground truth clusterings, we illustrate that RKD over the population provably leads to low clustering error. Moreover, we provide a sample complexity bound for RKD with limited unlabeled samples. For semi-supervised learning, we further demonstrate the label efficiency of RKD through a general framework of cluster-aware semi-supervised learning that assumes low clustering errors. Finally, by unifying data augmentation consistency regularization into this cluster-aware framework, we show that despite the common effect of learning accurate clusterings, RKD facilitates a "global" perspective through spectral clustering, whereas consistency regularization focuses on a "local" perspective via expansion.

## 1 Introduction

The immense volume of training data is a vital driving force behind the unparalleled power of modern deep neural networks. Nevertheless, data collection can be quite resource-intensive; in particular, data labeling may be prohibitively costly. With an aim to lessen the workload associated with data labeling, semi-supervised learning capitalizes on more easily accessible unlabeled data via diverse approaches of pseudo-labeling. For instance, data augmentation consistency regularization (Sohn et al., 2020; Berthelot et al., 2019) generates pseudo-labels from the current model predictions on carefully designed data augmentations. Alternatively, knowledge distillation (Hinton et al., 2015) can be leveraged to gauge representations of unlabeled samples via pretrained teacher models.

Meanwhile, as the scale of training state-of-the-art models explodes, computational costs and limited data are becoming significant obstacles to learning from scratch. In this context, (data-free) knowledge distillation (Liu et al., 2021; Wang and Yoon, 2021) (where pretrained models are taken as teacher oracles without access to their associated training data) is taking an increasingly crucial role in learning efficiently from powerful existing models.

Despite the substantial progresses in knowledge distillation (KD) in practice, its theoretical understanding remains limited, possibly due to the abundant diversity of KD strategies. Most existing theoretical analyses of KD (Ji and Zhu, 2020; Allen-Zhu and Li, 2020; Harutyunyan et al., 2023; Hsu et al., 2021; Wang et al., 2022) focus on feature matching between the teacher and student models (Hinton et al., 2015), while a much broader spectrum of KD algorithms (Liu et al., 2019; Park et al., 2019; Chen et al., 2021; Qian et al., 2022) demonstrates appealing performance in practice. Recently, _relational knowledge distillation_ (RKD) (Park et al., 2019; Liu et al., 2019) has achieved remarkable empirical successes by matching the inter-feature relationships (instead of features themselves) between the teacher and student models. This drives us to focus on RKD and motivates our analysis with the following question:

_What perspective of the data does relational knowledge distillation learn, and how efficiently?_

(RKD learns spectral clustering.)In light of the connection between RKD and instance relationship graphs (IRG) (Liu et al., 2019)2, intuitively, RKD learns the ground truth geometry through a graph induced by the teacher model. We formalize this intuition from a spectral clustering perspective (Section 4). Specifically, for a multi-class classification problem, we introduce a notion of _clustering error_ (Definition 3.1) that measures the difference between the predicted and ground truth class partitions. Through low clustering error guarantees, we illustrate that RKD over the population learns the ground truth partition (presumably unveiled by the teacher model) via _spectral clustering_ (Section 4.1). Moreover, RKD on sufficiently many unlabeled samples learns a similar partition with low clustering error (Section 4.2). By appealing to standard generalization analysis (Wainwright, 2019; Bartlett and Mendelson, 2003; Wei and Ma, 2019) for unbiased estimates of the parameterized RKD loss, we show that the _unlabeled sample complexity of RKD_ is dominated by a polynomial term in the model complexity (Theorem 4.2, Theorem 4.3).

As one of the most influential and classical unsupervised learning strategies, clustering remains an essential aspect of modern learning algorithms with weak/no supervision. For example, contrastive learning (Caron et al., 2020; Grill et al., 2020; Chen and He, 2021) has proven to learn good representations without any labels by encouraging (spectral) clustering (HaoChen et al., 2021; Lee et al., 2021; Parulekar et al., 2023). However, the effects of clustering in the related schemes of semi-supervised learning (SSL) are less well investigated. Therefore, we pose a follow-up question:

_How does cluster-aware semi-supervised learning improve label efficiency and generalization?_

Figure 1: The complementary perspectives of RKD and DAC regarding the data. RKD learns the pairwise relations among data (_e.g._, edge \(w_{^{}}\) that characterizes the similarity between \(\) and \(^{}\)) over the population “globally” via spectral clustering, as illustrated in Section 4. Alternatively, DAC discovers the “local” clustering structure through the expansion of neighborhoods based on overlapping augmentation sets (_e.g._, \(()(^{}) \)), as elaborated in Section 6.

(Cluster-aware SSL is label-efficient.)We formulate a cluster-aware SSL framework (Section 5) where assuming a low clustering error (Definition 3.1) endowed by learning from unlabeled data, the label complexity scales linearly in the number of clusters, up to a logarithmic factor (Theorem 5.1).

Since clustering is a rather generic notion that can be facilitated from various perspectives, to better understand spectral clustering as learned by RKD, we compare the clustering mechanism of RKD with that of another common example of cluster-aware SSL--data augmentation consistency (DAC) regularization, while exploring the following question:

_What are the similarities and discrepancies between different cluster-aware strategies?_

(RKD and DAC learn clustering from complementary perspectives.)We unify the existing analysis (Yang et al., 2023) tailored for DAC regularization (Sohn et al., 2020) into the cluster-aware SSL framework (Section 6). In particular, while facilitating clustering similarly to RKD, DAC reduces the clustering error through an expansion-based mechanism characterized by "local" neighborhoods induced by data augmentations (Theorem 6.2). This serves as a complementary perspective to RKD which approximates spectral clustering on a graph Laplacian that reflects the underlying "global" structure of the population (Figure 1).

## 2 Related Works

Relational knowledge distillation.Since the seminal work of Hinton et al. (2015), knowledge distillation has become a foundational method for time and memory-efficient deep learning. Recent years have seen different KD strategies. Some works learn to directly match the output (response) (Hinton et al., 2015; Chen et al., 2017; Ba and Caruana, 2014) or intermediate layers (features) (Romero et al., 2014; Zagoruyko and Komodakis, 2016; Kim et al., 2018) of the teacher network. More recently, Park et al. (2019) introduced the idea of relational knowledge distillation (RKD), and concurrent work (Liu et al., 2019) introduced the "instance relationship graph" (IRG). They essentially presented similar ideas to learn the inter-feature relationship between samples instead of matching models' responses or features individually. A comprehensive survey of relational (relation-based) KD can be found in (Gou et al., 2021).

Despite the empirical success stories, the theoretical understanding of KD remained limited. Prior theoretical work is mostly constrained to linear classifier/nets (Phuong and Lampert, 2019; Ji and Zhu, 2020) or neural tangent kernel (NTK) analysis (Harutyunyan et al., 2023). Hsu et al. (2021) leverages knowledge distillation to improve the sample complexity analysis of large neural networks with potentially vacuous generalization bounds.

Data augmentation consistency regularization.Data augmentation serves as an implicit or explicit regularization to improve sample efficiency and avoid overfitting. Initially, people applied simple transformations to labeled samples and add them to the training set (Krizhevsky et al., 2017; Simard et al., 2002; Simonyan and Zisserman, 2014; He et al., 2016; Cubuk et al., 2018). Traditional transformations preserve image semantics, including (random) perturbations, distortions, scales, crops, rotations, and horizontal flips. Subsequent augmentations may alter the labels jointly with the input samples, e.g. Mixup (Zhang et al., 2017), Cutout (DeVries and Taylor, 2017), and Cutmix (Yun et al., 2019). Theoretical works have demonstrated different functionalities of data augmentation such as incorporating invariances (Mei et al., 2021), amplifying strong features (Shen et al., 2022), or reducing variance during the learning procedure (Chen et al., 2020).

Recent practices explicitly add consistency regularization to enforce prediction or representation similarities (Laine and Aila, 2016; Bachman et al., 2014; Sohn et al., 2020). Not only does this way incorporates unlabeled samples, but Yang et al. (2023) also theoretically demonstrated other benefits: explicit consistency regularization reduces sample complexity more significantly and handles misspecification better than simply adding augmented data into the training set.

Graph-based learning.Although we leverage graphs to model the underlying clustering structure of the population, the RKD algorithm that we study applies to generic data distributions and does not involve any graph constructions over the training data. The latter is crucial for modern learning settings as graph construction can be prohibitively expensive on common large-scale datasets.

Nevertheless, assuming that explicit graph construction is affordable/available, there exists a rich history of nonparametric graph-based semi-supervised classification models for transductive learn ing (Zhu et al., 2003; Bertozzi and Merkurjev, 2019; Zhou et al., 2003; Belkin et al., 2004; Belkin and Niyogi, 2004). These methods have seen success in achieving high accuracy results in the low-label rate regime (Calder et al., 2020; Bertozzi and Merkurjev, 2019). Furthermore, assuming inherited graph structures of data, recent progress in graph neural networks (GNN) (Zhou et al., 2020) and graph convolutional networks (GCN) (Welling and Kipf, 2016) has provided a connection between these classical graph-based methods and powerful deep learning models for a variety of problem settings (Zhou et al., 2020).

## 3 Problem Setup

Notations.For any event \(e\), let \(\{e\}=1\) if \(e\) is true, and \(\{e\}=0\) otherwise. For any positive integers \(n,K\), we denote \([n]=\{1,,n\}\) and \(_{K}\{(p_{1},,p_{K})^{K}\, |\,_{k=1}^{K}p_{k}=1\}\). For a finite set \(\), \(||\) denotes the size of \(\), and \(()\) denotes the uniform distribution over \(\) (_i.e._, \(()=1/||\) for all \(\)). For a distribution \(P\) over \(\) and any \(n\), \(P^{n}\) represents the joint distribution over \(^{n}\) such that \(\{_{i}\}_{i[n]} P^{n}\) is a set of \(n\)_i.i.d._ samples. Given any matrix \(^{n k}\) (\(n k\) without loss of generality), let \(()=\{_{i}()\,|\,i [k]\}\) be the singular values of \(\) with \(_{1}()_{k}()\). While for any symmetric \(^{n n}\), let \(()=\{_{i}()\,|\,i [n]\}\) be the eigenvalues of \(\) such that \(_{1}()_{n}()\). Given any labeling function \(y:[K]\), let \(:\{0,1\}^{K}\) be its one-hot encoding.

### Spectral Clustering on Population-induced Graph

We consider a \(K\)-class classification problem over an unknown data distribution \(P:[K]\) (while overloading \(P\) for the probability measure over \(\) without ambiguity). Let \(y_{*}:[K]\) be the ground truth classification that introduced a natural partition \(\{_{k}\}_{k[K]}\) where \(_{k}\{\,|\,y_{*}( )=k\}\) such that \(_{k=1}^{K}_{k}=\) and \(_{k}_{k^{}}=\) for all \(k k^{}\). Meanwhile, we specify a function class \(\{f:^{K}\}\) to learn from where each \(f\) is associated with a prediction function \(y_{f}()*{argmax}_{k[K]}f( )_{k}\).

We characterize the geometry of the data population3\(\) with an undirected weighted graph \(G_{}=(,())\) such that: (i) For every vertex pair \(,^{}\), the edge weight \(w_{^{}} 0\) characterizes the similarity between \((,^{})\). For example, \(w_{^{}}\) between a pair of samples from the _same_ class is larger than that between a pair from _different_ classes (ii) \(P()=w_{}_{^{}} w_{^{}}\) such that \(_{}_{^{}}w_{ ^{}}=1\), intuitively implying that a more representative instance \(\) (corresponding to a larger \(w_{}\)) has a higher probability of being sampled. We remark that such population-induced graph \(G_{}\) is reminiscent of the population augmentation graph for studying contrastive learning4(HaoChen et al., 2021).

Let \(()\) with \(_{^{}}()=w_{ ^{}}\) be the weighted adjacency matrix of \(G_{}\); let \(()\) be the diagonal matrix of the weighted degrees \(\{w_{}\}_{}\). The (normalized) graph Laplacian takes the form \(()=-}( )\) where \(}()( )^{-1/2}()( )^{-1/2}\) is the normalized adjacency matrix. Then, the _spectral clustering_(von Luxburg, 2007) on \(G_{}\) can be expressed as a (rank-\(K\)) Nystrom approximation of \(}()\) in terms of the weighted outputs \(()^{}f()\):

\[_{()}*{ argmin}_{f}\{R(f)\|}()-()^{}f()f()^{}()^{} \|_{F}^{2}\}.\] (1)

To quantify the alignment between the ground truth class partition and the clustering predicted by \(f\), we introduce the notion of clustering error.

**Definition 3.1** (Clustering error).: _Given any \(f\), we define the majority labeling_

\[_{f}()*{argmax}_{k [K]}_{^{} P()} [y_{*}(^{})=k y_{f}(^{})=y_{f}( )],\] (2)

_along with the minority subsets associated with \(f\): \(M(f)\{_{f} () y_{*}()\}\) such that \(P(M(f))\) quantifies the clustering error of \(f\). For any \(^{}\), let \((^{})_{f^{}} P(M(f))\)._

Intuitively, \(M(f)\) characterizes the difference between \(K\)-partition of \(\) by the ground truth \(y_{*}\) and by the prediction function \(y_{f}\) associated with \(f\); while \((^{})\) quantifies the worse-case clustering error of all functions \(f^{}\). In Section 4.1, we will demonstrate that spectral clustering on \(G_{}\) (Equation (1)) leads to provably low clustering error \((_{()})\).

### Relational Knowledge Distillation

For RKD, we assume access to a proper teacher model \(:\) (for a latent feature space \(\)) that induces a graph-revealing kernel: \(k_{}(,^{})=^{}}}{w_{^{}}}\). Then, the spectral clustering on \(G_{}\) in Equation (1) can be interpreted as the _population RKD loss_:

\[R(f)=_{,^{} P()^{2}}[(k_{}(,^{})-f( )^{}f(^{}))^{2}].\]

While with only limited unlabeled samples \(^{u}=\{_{j}^{u} j[N]\} P ()^{N}\) in practice, we consider the analogous _empirical RKD loss_:

\[_{(^{u})}*{ argmin}_{f}\{_{^{u}}(f )_{i=1}^{N/2}(f(_{2i-1}^{u })^{}f(_{2i}^{u})-k_{}(_{2i -1}^{u},_{2i}^{u}))^{2}\},\] (3)

where \(_{^{u}}(f)\) serves as an unbiased estimate for \(R(f)\) (Proposition D.1).

Further, for _semi-supervised setting_ with a small set of labeled samples \((,)=\{(_{i},y_{i}) \}_{i[n]} P(,y)^{n}\) (usually \(n N\)) _independent of \(^{u}\)_, let \(:^{K}[K]\{0,1\}\) be the zero-one loss: \((f(),y)=\{y_{f}() y\}\). We denote \((f)_{(,y) P }[(f(),y)]\) and \(}(f)_{i=1}^{n} (f(_{i}),y_{i})\) as the population and empirical losses, respectively, and consider a proper learning setting with the ground truth \(f_{*}*{argmin}_{f}(f)\). Then, SSL with RKD aims to find \(_{f}}(f)+_{^{u}}(f)\). Alternatively, for an overparameterized setting5 with \((*{argmin}_{f}}(f ))_{(^{u})}\), SSL with RKD can be formulated as: \(_{f_{(^{u})}}}(f)\).

## 4 Relational Knowledge Distillation as Spectral Clustering

In this section, we show that minimizing either the population RKD loss (Section 4.1) or the empirical RKD loss (Section 4.2) leads to low clustering errors \((_{()})\) and \((_{(^{u})})\), respectively.

### Relational Knowledge Distillation over Population

Starting with minimization of the population RKD loss \(f_{()}=*{argmin}_{f }R(f)\) (Equation (1)), let \((())=(_{1},, _{||})\) be the eigenvalues of the graph Laplacian in the ascending order, \(0=_{1}_{||}\) with an arbitrary breaking of ties.

A key assumption of RKD is that the teacher model \(\) (with the corresponding graph-revealing kernel \(k_{}\)) is well aligned with the underlying ground truth partition, formalized as below:

**Assumption 4.1** (Approximate teacher models).: _For \(G_{}\) unveiled through the teacher model \(k_{}(,)\), we assume \(_{K+1}>0\) (i.e., \(G_{}\) has at most \(K\) disconnected components); while the ground truth classes \(\{_{k}\}_{k[K]}\) are well-separated by \(G_{}\) such that \(_{k_{k}}_{k ^{}}q_{_{k}}w_{^{}}}{2 _{}_{^{}}w_{ ^{}}} 1\) (i.e., the fraction of weights of inter-class edges is sufficiently small)._In particular, \(\) reflects the extent to which the ground truth classes \(\{_{k}\}_{k[K]}\) are separated from each other, as quantified by the fraction of edge weights between nodes of disparate classes. In the ideal case, \(=0\) when the ground truth classes are perfectly separated such that every \(_{k}\) is a connected component of \(G_{}\), while \(_{K+1}>_{K}=0\).

Meanwhile, to reduce the generic function class \(\) down to a class of reasonable prediction functions, we introduce the following regularity conditions on the boundedness and margin:

**Assumption 4.2** (\(\)-skeleton boundedness and \(\)-margin).: _For any \(f\) with \(P(M(f)_{k}) P(_{k})/2\) for all \(k[K]\), we assume there exists a skeleton subset \(=[_{1};;_{K}]^{K}\) with6\(_{k}=*{argmax}_{ M(f)}f()_{k}\) such that \(y_{f}(_{k})=k\) for every \(k[K]\) and \(*{rank}(f())=K\)._

1. _[label=()]_
2. _We say that_ \(f\) _is_ \(\)_-skeleton bounded if_ \(_{1}(f())\) _for some reasonably small_ \(\)_._
3. _Let the_ \(k\)_-th margin of_ \(f\) _be_ \(_{k} f(_{k})_{k}-_{ M(f) :y_{f}() k}f()_{k}\)_. We say that_ \(f\) _has a_ \(\)_-margin if_ \(_{k[K]}_{k}>\) _for some sufficiently large_ \(>0\)_._

Intuitively, \(\) can be viewed as a set of \(K\) samples where \(f\) makes the "most confident" prediction in each class. Assumption 4.2 ensures the boundedness of predictions made on such a skeleton subset \(\|f()\|_{2}\), as well as a margin \(\) by which the "most confident" prediction of \(f\) in each class \(_{k}\) can be separated from the minority samples predicted to lie in other classes \(\{ M(f)|y_{f}() k\}\). As a toy example, when \(y_{f}:[K]\) is surjective, and the columns in \(f()^{|| K}\) consist of identity vectors of the predicted clusters \(f()_{k}=\{y_{f}()=k\}\), Assumption 4.2 is satisfied with \(=1\) and \(=1\). Meanwhile, the following Remark 4.1 highlights that although Assumption 4.2 cannot be guaranteed with spectral clustering alone, it is generally satisfied in the semi-supervised learning setting with additional supervision/regularization (_cf._ Example C.1).

**Remark 4.1** (Limitation of spectral clustering alone).: _Notice that for a generic function class \(\), spectral clustering alone is not sufficient to guarantee Assumption 4.2, especially the existence of a skeleton subset \(\) or a large enough margin \(\). As counter-exemplified in Example C.1, Equation (1) can suffer from large clustering error \((_{()})\) when applied alone and failing to satisfy Assumption 4.2._

_To learn predictions with accurate clustering, RKD requires either (i) additional supervision/regularization in end-to-end settings like semi-supervised learning or (ii) further fine-tuning in unsupervised representation learning settings like contrastive learning . For end-to-end learning in practice, RKD is generally coupled with standard KD (i.e., feature matching) and weak supervision , both of which help the learned function \(f\) satisfy Assumption 4.2 with a reasonable margin \(\)._

Throughout this work, we assume \(\) is sufficiently regularized (with weak supervision in Section 5 and consistency regularization in Section 6) such that Assumption 4.2 always holds. Under Assumption 4.1 and Assumption 4.2, the clustering error of RKD over the population \((_{()})\) is guaranteed to be small given a good teacher model \(\) that leads to a negligible \(\):

**Theorem 4.1** (RKD over population, proof in Appendix C.1).: _Under Assumption 4.1 and Assumption 4.2 for every \(f_{()}\), the clustering error with the population (Equation (1)) satisfies_

\[(_{()}) 2(}{^{2}},1)}.\]

Theorem 4.1 suggests that the clustering error over the population is negligible under mild regularity assumptions (_i.e._, Assumption 4.2) when (i) the ground truth classes are well-separated by \(G_{}\) revealed through the teacher model \(k_{}(,)\) (_i.e._, \( 1\) in Assumption 4.1) and (ii) the \((K+1)\)th eigenvalue \(_{K+1}\) of the graph Laplacian \(()\) is not too small. As we review in Appendix C.3, the existing result Lemma C.4 [Louis and Makaychev, 2014] unveils the connection between \(_{K+1}\) and the sparsest \(K\)-partition (Definition C.1) of \(G_{}\). Intuitively, a reasonably large \(_{K+1}\) implies that the partition of the \(K\) ground truth classes is the "only" partition of \(G_{}\) into \(K\) components by removing a _sparse_ set of edges (Corollary C.5).

### Relational Knowledge Distillation on Unlabeled Samples

We now turn our attention to a more practical scenario with limited unlabeled samples and bound the clustering error \((_{(^{u})})\) granted by minimizing the empirical RKD loss (Equation (3)).

To cope with \( f:^{K}\), we recall the notion of Rademacher complexity for vector-valued functions from Maurer (2016). Let \(()\) be the Rademacher distribution (_i.e._, with uniform probability \(\) over \(\{-1,1\}\)) and \(^{N K}\) be a random matrix with _i.i.d._ Rademacher entries. Given any \(N\), with \(f()_{k}\) denoting the \(k\)-th entry of \(f()^{K}\), we define

\[_{N}()_{^{u} P()^{N}\\ ^{N K}}[_{f}_{i=1}^{N}_{k=1}^{K}_{ik} f(^{u}_{i} )_{k}]\] (4)

as the Rademacher complexity of the vector-valued function class \( f:^{K}\).

Since the empirical RKD loss is an unbiased estimate of its population correspondence (Proposition D.1), we can leverage the standard generalization analysis in terms of Rademacher complexity to provide an unlabeled sample complexity for RKD.

**Theorem 4.2** (Unlabeled sample complexity of RKD, proof in Appendix D.2).: _Assume there exist \(B_{f},B_{k_{}}>0\) such that \(\|f()\|_{2}^{2} B_{f}\) and \(k_{}(,^{}) B_{k_{}}\) for all \(,^{},\;f\). Given any \(f_{|^{u}}_{(^{u})}\), \(f_{|}_{()}\), \((0,1)\), with probability at least \(1-/2\) over \(^{u} P()^{N}\),_

\[R(f_{|^{u}})-R(f_{|}) 16}(B_{f}+B_{k_{}})_{N/2}() +2(B_{k_{}}+B_{f})^{2}}.\]

Theorem 4.2 implies that the unlabeled sample complexity of RKD is dominated by the Rademacher complexity of \(\). In Appendix D.3, we further concretize Theorem 4.2 by instantiating \(_{N/2}()\) via existing Rademacher complexity bounds for neural networks (Golowich et al., 2018).

With \( 0\) as \(N\) increasing (Theorem 4.2), the clustering error of minimizing the empirical RKD loss can be upper bounded as follows.

**Theorem 4.3** (RKD on unlabeled samples, proof in Appendix D.4).: _Under Assumption 4.1 and Assumption 4.2 for every \(f_{(^{u})}\), given any \((0,1)\), if there exists \(<(1-_{K})^{2}\) such that \(R(f_{|^{u}}) R(f_{|})+\) for all \(f_{|^{u}}_{(^{u})}\) and \(f_{|}_{()}\) with probability at least \(1-/2\) over \(^{u} P()^{N}\), then error of clustering with the empirical graph (Equation (3)) satisfies the follows: for any \(K_{0}[K]\) such that \(_{K_{0}}<_{K+1}\) and \(C_{K_{0}}})^{2}-(1-_{ K})^{2}}{}=O(1)\),_

\[(_{(^{u})}) 2( }{^{2}},1)(}+ )C_{K_{0}})}{(1-_{K_{0 }})^{2}-(1-_{K+1})^{2}}).\]

Theorem 4.3 ensures that given sufficient unlabeled samples (proportional to the Rademacher complexity of \(\)) such that \( 0\), the clustering error \((_{(^{u})})\) from empirical RKD is nearly as \((_{()})\) from population RKD, up to an additional error term that scales linearly in \(\).

## 5 Label Efficiency of Cluster-aware Semi-supervised Learning

In this section, we demonstrate the label efficiency of learning from a function class with low clustering error (Definition 3.1), like the ones endowed by RKD discussed in Section 4.

Specifically, given any cluster-aware function subclass \(^{}\) with low clustering error (_e.g._, \(_{()}\) and \(_{(^{u})}\)), for a set of \(n\)_i.i.d._ labeled samples \((,) P(,y)^{n}\), we have the following generalization guarantee:

**Theorem 5.1** (Label complexity, proof in Appendix B.1).: _Given any cluster-aware \(^{}\) with \((^{}) 1\), assuming that \((,)\) contains at least one sample per class, for any \((0,1)\), with probability at least \(1-/2\) over \((,) P(,y)^{n}\), \(*{argmin}_{f^{}}}(f)\) satisfies_

\[()-(f_{*}) 4+2(^{})}+},\] (5)

e.g., _conditioned on \(^{u}\), \(_{|^{u}}*{argmin}_{f_{ (^{u})}}}(f)\) satisfies Equation (5) with \((_{(^{u})})\)._Theorem 5.1 implies that with low clustering error (_e.g._, endowed by unsupervised methods like RKD in Theorem 4.1/Theorem 4.3, DAC in Theorem 6.2), the labeled sample complexity is as low as \((K)\), linear in the number of clusters and asymptotically optimal7 up to a logarithmic factor.

**Remark 5.1** (Class imbalance, elaborated in Appendix B.2).: _In Theorem 5.1, while the label complexity scales as \(n=(K)\) with i.i.d. sampling, we meanwhile assume that \((,)\) contains at least one labeled sample per class. Intuitively, when the \(K\) classes are balanced (i.e., \(|_{k}|=||/K\) for all \(k[K]\)), an analogy to the coupon collector problem implies that \(n=O(K(K))\) i.i.d. labeled samples are sufficient in expectation. Nevertheless, with class imbalance, the label complexity can be much worse (e.g., when \(|_{K}||_{k}|\) for all \(k[K-1]\), collecting one label \(K\) takes \(1/P(_{K}) K\) labeled samples). In Appendix B.2, we show that such label inefficiency can be circumvented by leveraging a cluster-aware prediction \(f^{}\) and drawing \(O((K))\) labeled samples uniformly from each of the \(K\) predicted clusters, instead of i.i.d. from the entire population._

**Remark 5.2** (Coreset selection).: _While Theorem 5.1 yields an asymptotically optimal label complexity guarantee with i.i.d. sampling, it is of significant practical value to be more judicious in selecting labeled samples, especially for the low-label-rate regime (e.g., \(n=O(K)\)). Therefore, in the experiments (Appendix A), we further investigate--alongside i.i.d. sampling8--a popular coreset selection method (Bilmes, 2022; Krause and Golovin, 2014), where the coreset \((,)\) is statistically more representative of the data distribution \(P\)._

_In particular, coreset selection can be recast as a facility location problem (Krause and Golovin, 2014) characterized by the teacher model: \(_{}\;_{^{}} _{}\;k_{}(,^{})\), whose optimizers can be approximated heuristically via the stochastic greedy (Mirzasoleiman et al., 2015) submodular optimization algorithm (Schreiber et al., 2020). Intuitively, the facility location objective encourages the coreset \(\) to be representative of the entire population \(\) in a pairwise similarity sense, and the coreset approximation identified by the stochastic greedy method is nearly optimal (in the facility location objective) up to a multiplicative constant (Mirzasoleiman et al., 2015)._

## 6 Data Augmentation Consistency Regularization as Clustering

In light of the broadness of the notion of clustering, investigating the discrepancy between different types of cluster-awareness is crucial for understanding the effect of spectral clustering brought by RKD beyond the low clustering error demonstrated in Section 4. In this section, we leverage the existing theoretical tools from Wei and Ma (2019); Cai et al. (2021) to unify DAC regularization into the cluster-aware semi-supervised learning framework in Theorem 5.1. Specifically, we demonstrate the effect of DAC as a "local" expansion-based clustering (Yang et al., 2023), which works in complement with the "global" spectral clustering facilitated by RKD.

Start by recalling the formal notion of expansion-based data augmentation (Definition 6.1) and data augmentation consistency (DAC) error (Definition 6.2) from Wei et al. (2021):

**Definition 6.1** (Expansion-based data augmentation ((Wei et al., 2021) Definition 3.1)).: _For any \(\), we consider a set of class-invariant data augmentations \(()\) such that \(\{\}() _{y_{*}(x)}\). We say \(,^{}\) lie in neighborhoods of each other if their augmentation sets have a non-empty intersection: \(()\{^{} \;|\;()(^{}) \}\) for \(\); and \((S)_{ S}()\) for \(S\). Then, we quantify strength of such data augmentations via the \(c\)-expansion property (\(c>1\)): for any \(S\) such that \(P(S_{k}) P(_{k})/2\;\;k[K]\),_

\[P((S)_{k})>\{c P(S _{k}),P(_{k})\}\;\;k[K].\]

**Definition 6.2** (DAC error (Wei et al., 2021) (3.3)).: _For any \(f\) and \(^{}\), let_

\[(f)_{ P()}[ \;^{}()y_{f}() y_{f}(^{})],( ^{})_{f^{}}(f).\]We adopt the theoretical framework in Wei et al. (2021) for generic neural networks with smooth activation function \(\) and consider

\[=\{f()=_{p}(_{2} (_{1}))_{} ^{d_{} d_{-1}}\,[p],\;d=_{=0, ,p}d_{}\}.\]

With such function class \(\) of \(p\)-layer neural networks with maximum width \(d\) and weights \(\{_{}\}_{=1}^{p}\), we recall the notion of _all-layer margin_ from Wei et al. (2021) Appendix C.2. By decomposing \(f=f_{2p-1} f_{1}\) such that \(f_{2-1}()=_{}\) for all \([p]\) and \(f_{2}()=()\) for all \([p-1]\), we consider a perturbation \(=(_{1},, _{2p-1})\) (where \(_{2-1},_{2}^{d_{ }}\)) to each layer of \(f\):

\[f_{1}(,) =f_{1}(,_{1})=f_{1}( )+_{1}\|\|_{2},\] \[f_{}(,) =f_{}(,_{1},, _{})=f_{}(f_{-1}(,))+_{}\|f_{-1} (,)\|_{2}\]

such that \(f(,)=f_{2p-1}(,)\). Then, the all-layer margin \(m:[K]_{ 0}\) is defined as the minimum norm of \(\) that is sufficient to perturb the classification of \(f()\):

\[m(f,,y)_{}^{2p-1}\|_{}\|_{2}^{2}}*{argmax}_{k[K]}f(, )_{k} y.\] (6)

Moreover, Wei et al. (2021) introduced the _robust margin_ for the expansion-based data augmentation \(\) (Definition 6.1), which intuitively quantifies the worst-case preservation of the label-relevant information in the augmentations of \(\), measured with respect to \(f\):

\[m_{}(f,)_{^{} ()}m(f,^{},y_{f}()).\] (7)

We say the expansion-based data augmentation \(\) is _margin-robust_ with respect to the ground truth \(f_{}\) if \(_{}m_{}(f_{}, )>0\) is reasonably large.

Then, we cast DAC regularization9 as reducing the function class \(\) via constraints on the robust margins at the \(N\) unlabeled samples \(^{u}\): for some \(0<<_{}m_{}(f_{},)\),

\[_{^{u}}\{f m_{ }(f,_{i}^{u})\;\;i[N]\}.\] (8)

Leverage the existing unlabeled sample complexity bound for DAC error from Wei et al. (2021):

**Proposition 6.1** (Unlabeled sample complexity of DAC ((Wei et al., 2021) Theorem 3.7)).: _Given any \((0,1)\), with probability at least \(1-/2\) over \(^{u}\),_

\[(_{^{u}}^{})( ^{p}\|_{}\|_{E}}{ })+O(}),\]

_where \(()\) suppresses polylogarithmic factors in \(N\) and \(d\)._

The clustering error of Equation (8) is as low as its DAC error, up to a constant factor that depends on the augmentation strength, characterized by the \(c\)-expansion (Definition 6.1):

**Theorem 6.2** (Expansion-based clustering, proof in Appendix E.3).: _For Equation (8) with \(\) admitting \(c\)-expansion (Definition 6.1), the clustering error is upper bounded by the DAC error in Proposition 6.1:_

\[(_{^{u}}^{})\{,2\}(_{^{u}}^{}).\]

In Theorem 6.2, (i) \(c\) characterizes the augmentation strength (_i.e._, the perturbation on label-irrelevant information), whereas (ii) \(\) quantifies the margin robustness (_i.e._, the preservation of label-relevant information) for the expansion-based data augmentation. Ideally, we would like both \(c\) and \(\) to be reasonably large, while there exist trade-offs between the augmentation strength and margin robustness (_e.g._, overly strong augmentations inevitably perturb label-relevant information).

**Remark 6.1** (Clustering with DAC v.s. RKD).: _As Figure 1 demonstrated, in contrast to RKD that unveils the spectral clustering of a population-induced graph from a "global" perspective, DAC alternatively learns a "local" clustering structure through the expansion of neighborhoods \(()\) characterized by sets of data augmentations \(()\) (Definition 6.1)._

_Therefore, DAC and RKD provide complementary perspectives for each other. On one hand, the "local" clustering structure revealed by DAC effectively ensures a reasonable margin in Assumption 4.2 (Remark 4.1). On the other hand, when the augmentation strength \(c\) is insufficient (i.e., \(()\) is not expansive enough) for DAC to achieve a low clustering error \((_{^{u}})\) (Theorem 6.2), the supplementary "global" perspective of RKD connects the non-overlapping neighborhoods in the same classes and brings better classification accuracies, as we empirically verified in Appendix A._

## 7 Discussions, Limitations, and Future Directions

This work provides a theoretical analysis of relational knowledge distillation (RKD) in the semi-supervised classification setting from a clustering perspective. Through a cluster-aware semi-supervised learning (SSL) framework characterized by a notion of low clustering error, we demonstrate that RKD achieves a nearly optimal label complexity (linear in the number of clusters) by learning the underlying geometry of the population as revealed by the teacher model via spectral clustering. With a unified view of data augmentation consistency (DAC) regularization in the cluster-aware SSL framework, we further illustrate the complementary "global" and "local" perspectives of clustering learned by RKD and DAC, respectively.

As an appealing future direction, domain adaptation in knowledge distillation is a common scenario in practice where the training data of the teacher and student models come from different distributions. Although distributional shifts can implicitly reflect the alignment between the ground truth and the teacher model (Assumption 4.1) in our analysis, an explicit and specialized study on RKD in the domain adaptation setting may lead to better theoretical guarantees and deeper insights.

Another avenue for future exploration involves RKD on different (hidden) layers of the neural network. In this work, we consider RKD on the output layer following the standard practice (Park et al., 2019), whereas RKD on additional hidden layers has been shown to further improve the performance (Liu et al., 2019). While the analysis in Section 4 remains valid for hidden-layer features of low dimensions (\( K\)), Assumption 4.1 tends to fail (_i.e., \(\)_ can be large) for high-dimensional features, which may require separate and careful consideration.