# Induction Base:

A Best-of-both-worlds Algorithm for Bandits with Delayed Feedback with Robustness to Excessive Delays

 Saeed Masoudian

Churney ApS, Denmark

saeed@churney.io

&Julian Zimmert

Google Research

zimmert@google.com

&Yevgeny Seldin

University of Copenhagen

seldin@di.ku.dk

The work was done during SM's employment at the University of Copenhagen.

###### Abstract

We propose a new best-of-both-worlds algorithm for bandits with variably delayed feedback. In contrast to prior work, which required prior knowledge of the maximal delay \(d_{}\) and had a linear dependence of the regret on it, our algorithm can tolerate arbitrary excessive delays up to order \(T\) (where \(T\) is the time horizon). The algorithm is based on three technical innovations, which may all be of independent interest: (1) We introduce the first implicit exploration scheme that works in best-of-both-worlds setting. (2) We introduce the first control of distribution drift that does not rely on boundedness of delays. The control is based on the implicit exploration scheme and adaptive skipping of observations with excessive delays. (3) We introduce a procedure relating standard regret with drifted regret that does not rely on boundedness of delays. At the conceptual level, we demonstrate that complexity of best-of-both-worlds bandits with delayed feedback is characterized by the amount of information missing at the time of decision making (measured by the number of outstanding observations) rather than the time that the information is missing (measured by the delays).

## 1 Introduction

Delayed feedback is an ubiquitous challenge in real-world applications. Study of multi-armed bandits with delayed feedback has started at least four decades ago in the context of adaptive clinical trials (Simon, 1977, Eick, 1988), the same problem that has earlier motivated introduction of the bandit model itself (Thompson, 1933). We focus on robustness to delay outliers and to the loss generation mechanism. In practice occasional delay outliers are common (e.g., observations that never arrive). Robustness to the loss generation mechanism implies that the algorithm does not need to know whether the losses are stochastic or adversarial, but still provides regret bounds that match the optimal stochastic rates if the losses happen to be stochastic, while guaranteeing the adversarial rates if they are not (so-called best-of-both-worlds regret bounds). Such algorithms are important from a practical viewpoint, because the loss generation mechanism can rarely assumed to be stochastic, but it is still desirable to have tighter regret bounds if it happens to be. From the theoretical perspective both forms of robustness are interesting and challenging, requiring novel analysis tools and yielding better understanding of the problems.

Joulani et al. (2013) have studied multi-armed bandits with delayed feedback under the assumption that the rewards are stochastic and the delays are sampled from a fixed distribution.

They provided a modification of the UCB1 algorithm for stochastic bandits with non-delayed feedback (Auer et al., 2002). They have shown that the regret of the modified algorithm is \(O(_{i:_{i}>0}(}+_{}_ {i}))\), where \(i\) indexes the arms, \(_{i}\) is the suboptimality gap of arm \(i\), \(T\) is the time horizon (unknown to the algorithm), and \(_{}\) is the maximal number of outstanding observations. (An observation is counted as outstanding at round \(t\) if it originates from round \(t\) or earlier, but due to delay it was not revealed to the algorithm by the end of round \(t\). The number of outstanding observations \(_{t}\) at round \(t\) is the number of actions that have already been played, but their outcome was not observed yet. We also call \(_{t}\) the [running] count of outstanding observations. The maximal number of outstanding observations \(_{}\) is the maximal value that \(_{t}\) takes and is unknown to the algorithm.) The result implies that in the stochastic setting the delays introduce an additive term in the regret bound, proportional to the maximal number of outstanding observation.

In the adversarial setting, multi-armed bandits with delayed feedback were first analyzed under the assumption of uniform delays (Neu et al., 2010, 2014). For this setting Cesa-Bianchi et al. (2019) have shown an \((+)\) lower bound and an almost matching upper bound, where \(K\) is the number of arms and \(d\) is a fixed delay. The algorithm of Cesa-Bianchi et al. is a modification of the EXP3 algorithm of Auer et al. (2002b). Cesa-Bianchi et al. used a fixed learning rate that is tuned based on the knowledge of \(d\). The analysis is based on control of the drift of the distribution over arms played by the algorithm from round \(t\) to round \(t+d\). Thune et al. (2019) and Bistritz et al. (2019) provided algorithms for variable adversarial delays, but under the assumption that the delays are known "at action time", meaning that the delay \(d_{t}\) is known at time \(t\), when the action is taken, rather that at time \(t+d_{t}\), when the observation arrives. The advanced knowledge of delays was used to tune the learning rate and control the drift of played distribution from round \(t\), when an action is played, to round \(t+d_{t}\), when the observation arrives. Alternatively, an advance knowledge of the cumulative delay up to the end of the game could be used for the same purpose. Finally, Zimmert and Seldin (2020) derived an algorithm for the adversarial setting that required no advance knowledge of delays and matched the lower bound of Cesa-Bianchi et al. (2019) within constants. The algorithm and analysis of Zimmert and Seldin avoid explicit control of the distribution drift and are parameterized

  Paper & Key results \\  Joulani et al. (2013) & Stochastic bound: \((_{i:_{i}>0}(}+_{ }_{i}))\) \\  Zimmert and Seldin (2020) & Adversarial bound \\  & without skipping: \((+)\) \\  & with skipping: \((+_{}(||+} K}))\) \\  & (Masoudian et al. (2022) provide a matching lower bound) \\  Masoudian et al. (2022) & Best-of-both-worlds bound, stochastic part \\  & \((_{i i^{*}}(}+}{_{i} K})+d_{}K^{1/3} K)\) \\ The results assume oracle & Best-of-both-worlds bound, adversarial part \\  & \((++d_{}K^{1/3} K)\) \\  Our paper & Best-of-both-worlds bound, stochastic part \\  & \((_{i i^{*}}(}+}{_{i} K})+K_{}+S^{*})\), where \\  & \(S^{*}=((d_{}K^{} K,_{} \{||+}K^{} K}\} ))\) \\  & Best-of-both-worlds bound, adversarial part \\  & \((+_{}\{||+} K}\}+S^{*}+K_{})\) \\  

Table 1: Comparison to state-of-the-art. The following notation is used: \(T\) is the time horizon, \(K\) is the number of arms, \(i\) indexes the arms, \(_{i}\) is the suboptimality gap or arm \(i\), \(_{}\) is the maximal number of outstanding observations, \(D=_{t=1}^{T}d_{t}\) is the total delay, \([T]\) is a set of skipped rounds, \(=[T]\) is the set of non-skipped rounds, \(D_{}=_{t}d_{t}\) is the total delay in the _non_-skipped rounds, and \(d_{}\) is the maximal delay. We have \(_{}(||+}})\) and \(_{} d_{}\), and in some cases \(_{}(||+}})\) and \(_{} d_{}\).

by running counts of the number of outstanding observations \(_{t}\), which is an empirical quantity that is observed at time \(t\) ("at the time of action").

Masoudian et al. (2022) attempted to extend the algorithm of Zimmert and Seldin (2020) to the best-of-both-worlds setting. The stochastic part of the analysis of Masoudian et al. is based on a direct control of the distribution drift. The control is achieved by damping the learning rate to make sure that the played distribution on arms is not changing too much from round \(t\), when an action is played, to round \(t+d_{t}\), when the loss is observed. Highly varying delays cannot be treated with this approach, because fast learning rates limit the range \(d_{t}\) for which the drift is under control, while slow learning rates prevent learning. Therefore, Masoudian et al. had to reintroduce the assumption that that the maximal delay \(d_{}\) is known, and used it to tune the learning rate. Unfortunately, damping of the learning rate to control the drift over \(d_{}\) rounds made \(d_{}\) show up additively in the bound, meaning that potential presence of even a single delay of order \(T\) made both the stochastic and the adversarial bounds linear in the time horizon. We emphasise that the linear dependence of the regret on \(d_{}\) is real and not an artefact of the analysis, because it comes from damped learning rate.

We introduce a different best-of-both-worlds modification of the algorithm of Zimmert and Seldin (2020) that is fully parameterized by the running count of outstanding observations and requires no advance knowledge of delays or the maximal delay \(d_{}\). Our algorithm is based on a careful augmentation of the algorithm of Zimmert and Seldin with implicit exploration (described below), followed by application of a skipping technique (also described below) as a tool to limit the time span over which we need to control the distribution shift.

Implicit exploration was introduced by Neu (2015) to control the variance of importance-weighted loss estimates in adversarial bandits. But the exploration parameters add up linearly to the regret bound, making it highly challenging to design a scheme for best-of-both-worlds setting. The implicit exploration schedule of Neu leads to \(()\) regret bound and, therefore, unsuitable for that. Jin et al. (2022) introduced a different schedule for adversarial Markov decision processes with delayed feedback. However, it is unknown whether their schedule can work in a stochastic analysis. We introduce a novel schedule and show that it works in best-of-both-worlds setting.

Skipping was introduced by Thune et al. (2019) as a way to limit the dependence of an algorithm on a small number of excessively large delays. The idea is that it is "cheaper" to skip a round with an excessively large delay and bound the regret in the corresponding round by 1, than to include it in the core analysis. Thune et al. have assumed prior knowledge of delays, but Zimmert and Seldin (2020) have perfected the technique by basing it on a running count of outstanding observations. In both works skipping was an optional add-on aimed to improve regret bounds in case of highly unbalanced delays. In our work skipping becomes an indispensable part of the algorithm, because, apart from making the algorithm robust to a few excessively large delays, it also limits the time span over which the control of distribution drift is needed.

In Table 1 we compare our results to state of the art. In a nutshell, we replace terms dependent on \(d_{}\) by terms dependent on \(_{}\), and terms dependent on the square root of the total cumulative delay \(D=_{t=1}^{T}d_{t}\), by terms dependent on the number of skipped rounds \(||\) and a square root of the cumulative delay \(D_{}}=_{t}}d_{t}\) in the non-skipped rounds \(}\) (those with the smaller delay). This yields robustness to excessive delays, because neither \(_{}\) nor \(_{}(||+}}})\) depend on the magnitude of delay outliers. By contrast, both the stochastic and the adversarial regret bounds of Masoudian et al. (2022) become linear in \(T\) in presence of a single delay of order \(T\).

There are also additional benefits. It has been shown that \(_{} d_{}\), and in some cases \(_{} d_{}\)(Joulani et al., 2013, Masoudian et al., 2022). For example, if the first observation has delay \(T\), and the remaining observations have zero delay, then \(d_{}=T\), but \(_{}=1\). We also have that \(_{}(||+}})\), because \(=\) is part of the minimization on the left, and in some cases \(_{}(||+}})\). For example, if the delays in the first \(\) rounds are of order \(T\), and the delays in the remaining rounds are zero, then \(_{}(||+}}})= ()\), but \(=(T^{3/4})\)(Thune et al., 2019). Therefore, bounds that exploit skipping are preferable over bounds that do not, and for some problem instances the improvement is significant. In Appendix F we show that bounds with an additive term \(d_{}\), including the results of Masoudian et al. (2022), cannot benefit from skipping, in contrast to ours.

The following list highlights our main contributions.

1. We provide the first best-of-both-worlds algorithm for bandits with delayed feedback that is robust to delay outliers. It improves both the stochastic and the adversarial regret bounds relative to the work of Masoudian et al. (2022), which lacks such robustness. For some problem instances the improvement is dramatic, e.g., in presence of a single delay of order \(T\) both the stochastic and the adversarial regret bounds of Masoudian et al. are of order \(T\), whereas our bounds are unaffected.
2. We provide an efficient technique to control the distribution drift under highly varying delays.
3. We provide the first implicit exploration scheme that works in best-of-both-worlds setting.
4. We provide a procedure relating drifted regret to normal regret in presence of delay outliers.
5. At the conceptual level, we show that best-of-both-worlds regret depends on the amount of information missing at the time of decision making (the number of outstanding observations) rather than the time that the information is missing (the delays). It was shown to be the case for the stochastic and adversarial regimes in isolation (Joulani et al., 2013; Zimmert and Seldin, 2020), but we are the first to show that it is also the case for best-of-both-worlds.

## 2 Problem setting

We study the problem of multi-armed bandit with variable delays. In each round \(t=1,2,\), the learner picks an action \(I_{t}\) from a set of \(K\) arms and immediately incurs a loss \(_{t,I_{t}}\) from a loss vector \(_{t}^{K}\). However, the incurred loss is observed by the learner only after a delay of \(d_{t}\), at the end of round \(t+d_{t}\). The delays are arbitrary and chosen by the environment. We use \(_{t}\) to denote the number of outstanding observations at time \(t\) defined as \(_{t}=_{s t}(s+d_{s}>t)\) and \(_{}=_{[T]}_{t}\) to be the maximal number of outstanding observations. We consider two regimes for generation of losses by the environment: oblivious adversarial and stochastic.

We use pseudo-regret to compare the expected total loss of the learner's strategy to that of the best fixed action in hindsight. Specifically, the pseudo-regret is defined as:

\[_{T}=[_{t=1}^{T}_{t,I_{t}}]-_{ i[K]}[_{t=1}^{T}_{t,i}]=[_{t=1} ^{T}(_{t,I_{t}}-_{t,i^{*}_{T}})],\]

where \(i^{*}_{T}=_{i[K]}[_{t=1}^{T}_{t,i}]\) is the best action in hindsight. In the oblivious adversarial setting, the losses are assumed to be deterministic and independent of the actions taken by the algorithm. As a result, the expectation in the definition of \(i^{*}_{T}\) can be omitted and the pseudo-regret definition coincides with the expected regret. Throughout the paper we assume that \(i^{*}_{T}\) is unique. This is a common simplifying assumption in best-of-both-worlds analysis (Zimmert and Seldin, 2021). Tools for elimination of this assumption can be found in Ito (2021).

## 3 Algorithm

The algorithm is a best-of-both-worlds modification of the adversarial FTRL algorithm with hybrid regularizer by Zimmert and Seldin (2020). It is provided in Algorithm 1 display. The modification includes biased loss estimators (implicit exploration) and adjusted skipping threshold. The algorithm maintains a set of skipped rounds \(_{t}\) (initially empty), a cumulative count of "active" outstanding observations (those that have not been skipped yet), and a vector of cumulative observed loss estimates \(_{t}^{obs}\) from non-skipped rounds. At round \(t\) the algorithm constructs an FTRL distribution \(x_{t}\) over arms using regularizer \(F_{t}\) defined in equation (2) below, and samples an arm according to \(x_{t}\). Then it receives the observations that arrive at round \(t\), except those that come from the skipped rounds, and updates the vector \(_{t}^{obs}\) of cumulative loss estimates. The loss estimates \(_{t}\) are defined below in equation (1). Then it counts the number of "active" outstanding observations \(_{t}\) (those that belong to non-skipped rounds), updates the cumulative count of outstanding observations \(_{t}\), and computes the skipping threshold \(d_{}^{t}=_{t}}{49K^{2/3} K}}\). Finally, it adds rounds \(s\) for which the observation has not arrived yet and the waiting time \((t-s)\) exceeds the skipping threshold \(d_{}^{t}\) to the set of skipped rounds \(_{t}\). Lemma 20, which is an adaptation of Zimmert and Seldin (2020, Lemma 5) to our skipping rule, shows that at most one round \(s\) is skipped at a time (at most one index \(s\) satisfies the if-condition for skipping in Line 15 of the algorithm for a given \(t\)).

We use implicit exploration to control importance-weighted loss estimates. The idea of using implicit exploration is inspired by the works of Neu (2015) and Jin et al. (2022), but its parametrization and purpose are different from prior work. To the best of our knowledge, it is the first time implicit exploration is used for best-of-both-worlds bounds. For any \(s,t[T]\) with \(s t\) we define implicit exploration terms \(_{s,t}=e^{-_{t}}{_{t}-_{s}}}\). Our biased importance-weighted loss estimators are defined by

\[_{t,i}=(I_{t}=i)}{\{x_{t,i}, _{t,t+_{t}}\}},\] (1)

where \(_{s}=(d_{s},\{(t-s):t-s d_{}^{t}\})\) denotes the time that the algorithm waits for the observation from round \(s\). It is the minimum of the delay \(d_{s}\), and the time \((t-s)\) to the first round when the waiting time exceeds the skipping threshold \(d_{}^{t}\).

We use a hybrid regularizer based on a combination of the negative Tsallis entropy and the negative entropy, with separate learning rates,

\[F_{t}(x)=-2_{t}^{-1}(_{i=1}^{K}})+_{t}^{ -1}(_{i=1}^{K}x_{i} x_{i}),\] (2)

where the learning rates are \(_{t}^{-1}=\) and \(_{t}^{-1}=_{t}}{ K}}\). The regularizer is the same as the one used by Zimmert and Seldin (2020). By inheriting their regularizer we inherit their adversarial regret bound, which is minimax optimal, with just a minor adjustment due to introduction of implicit exploration and a slight change in the learning rates and skipping threshold. The main contribution of our work is carrying out the stochastic analysis while staying within the algorithmic framework of Zimmert and Seldin and keeping the adversarial regret bound almost unscathed.

The update rule for \(x_{t}\) is

\[x_{t}=_{t}^{*}(-_{t}^{obs})=_{x^{K -1}}_{t}^{obs},x+F_{t}(x),\] (3)where \(_{t}^{obs}=_{s=1}^{t-1}_{s}(s+d_{s}<t) (s_{t-1})\) is the cumulative importance-weighted loss estimate of observations that have arrived by time \(t\) and have not been skipped. We use \(^{*}=_{T}\) to denote the final set of skipped rounds at time \(T\).

## 4 Regret Bounds

The following theorem provides best-of-both-worlds regret bounds for Algorithm 1. A proof is provided in Section 5 and a bound on \(S^{*}\) can be found in Appendix H.

**Theorem 1**.: _The pseudo-regret of Algorithm 1 for any sequence of delays and losses satisfies_

\[_{T}=+_{[ T]}||+_{}} K}}+S^{* }+K_{},\]

_where \(_{}=_{t[T]}\{_{t}\}\) is the maximal number of outstanding observations after skipping and_

\[S^{*}=((d_{}K^{1/3} K\,_{ [T]}||+_{}}K^{ } K}})).\]

_Furthermore, if the losses are stochastic, the pseudo-regret also satisfies_

\[_{T}=(_{i i^{*}}(}+_{}}{_{i} K})+K_{}+S^{*}).\]

Masoudian et al. (2022) provide an \((+_{[T]}||+_{}} K}})\) regret lower bound for adversarial environments with variable delays, which is matched within constants by the algorithm of (Zimmert and Seldin, 2020) for adversarial environments. Our algorithm matches the lower bound within a multiplicative factor of \(K^{}\) on the delay-dependent term, which is the price we pay for obtaining a best-of-both-worlds guarantee. The price comes from a reduction of the skipping threshold of Zimmert and Seldin (2020) that we had to make to control the distribution drift that is due to the loss shift (see Appendix B.2). It is an open question whether this factor can be reduced.

In the stochastic regime, assuming that the delays in the first \(_{}\) rounds are of order \(T\), and that the losses come from Bernoulli distributions with bias close to \(\), a trivial regret lower bound is \((_{}}_{i}}{K}+_{i i ^{*}}})\). This bound is almost matched by the algorithm of Joulani et al. (2013) for the stochastic regime only. Our bound has some extra terms, most notably \(_{i i^{*}}_{}}{_{i} K}\) and \(S^{*}\). It is an open question whether these terms are inevitable or can be reduced.

Theorem 1 provides three major improvements relative to the results of Masoudian et al. (2022): (1) it requires no advance knowledge of \(d_{}\); (2) it replaces terms dependent on \(d_{}\) by terms dependent on \(_{}\), which never exceeds \(d_{}\), and in some cases may be significantly smaller; and (3) it makes skipping possible and beneficial, making the algorithm robust to a small number of excessively large delays and replacing \(\) term with \(_{S[T]}||+_{}}K^{} K}}\), which is never much larger, but in some cases significantly smaller.

## 5 Analysis

In this section, we present a proof of Theorem 1. We begin with the stochastic part of the bound in Section 5.1, followed by the adversarial part in Section 5.2.

### Stochastic Analysis

We start by defining the drifted regret \(_{T}^{drift}=[_{t=1}^{T}( x_{t}, _{t}^{obs}-_{t,z_{t}^{*}}^{obs}) ],\) where \(_{t}^{obs}=_{s=1}^{t}_{s}(s+ _{s}=t)(s_{t})\) is the cumulative vector of losses received at time \(t\). Lemma 2 is the first major contribution establishing a relationship between \(_{T}^{drift}\) and the actual regret \(_{T}\).

**Lemma 2** (Drift of the Drifted Regret).: _Let \(_{}^{t}=_{s[t]}\{_{s}\}\). Then_

\[_{T}^{drift}_{T}-2K_{t=1}^{T} (_{t,t+_{t}}+_{t,t+_{t}+_{ }^{t}})-}{4}-S^{*},\]

_where \(S^{*}\) is the total number of rounds skipped by the algorithm._

The core of Lemma 2 is based on controlling the distribution drift using implicit exploration and skipping. In prior work on bounded delays the relation between \(_{T}^{drift}\) and \(_{T}\) was achieved by shifting all the arrivals by \(d_{}\), leading to an additive term of order \(d_{}\). This approach fails for unbounded delays, because a single delay of order \(T\) prevents shifting and leads to linear regret. We address the challenge by introducing a procedure to rearrange the arrivals (Algorithm 2 below) and advanced control of the drift (Lemma 3 below). A proof of Lemma 2 is provided at the end of the section.

```
1Initialize\(_{t}^{new}=0\) for all \(t=1,,T+d_{}^{T}\)for\(t=1,,T\)do
2for\(s=1,,t:s+_{s}=t\)do
3 Find the first round \((s)[t,t+d_{}^{t}]\) such that \(_{(s)}^{new}=0\)
4 Move the arrival from round \(s\) to round \((s)\) and update \(_{(s)}^{new}=1\) ```

**Algorithm 2**Greedy Rearrangement

The drift control lemma (Lemma 3) is the second major contribution of the paper. Prior work on bounded delays controlled the drift by slowing the learning rate in accordance with \(d_{}\). This does not work for highly varying delays, because slow learning rates prevent learning, whereas fast learning rates fail to control the drift. Lemma 3 relies on implicit exploration terms in the loss estimators in equation (1) and on skipping of excessive delays, leaving the learning rates intact.

**Lemma 3** (Drift Control Lemma).: _Let \(d_{}^{t}\) be the skipping threshold at time \(t\). Then, for any \(i[K]\) and \(s,t[T]\), where \(s t\) and \(t-s d_{}^{t}\), we have_

\[x_{t,i} 4(x_{s,i},_{s,t}).\]

The proof is based on introduction of an intermediate variable \(_{s}=_{s}^{*}(-_{t-1}^{obs})\), which is based on the regularizer from round \(s\) and the loss estimate from round \(t\). It exploits the implicit exploration term \(_{s,t}\) to show that \(}{(_{i},_{s,t})} 2\) and skipping to show that \(_{i}}{x_{s,i}} 2\). The latter implies that \(_{i},_{s,t})}{(x_{s,i},_{s,t})} 2\), and in combination with the former completes the proof. The details of the two steps are provided in Appendix B.

Given Lemmas 2 and Lemma 3, we apply standard FTRL analysis, similar to Masoudian et al. (2022), to obtain an upper bound for \(_{T}^{drift}\). Specifically, in Appendix A we show that

\[_{T}^{drift} a_{t=1}^{T}_{i i^{*}}_{t}x_{ t,i}^{1/2}+b_{t=1}^{T}_{i i^{*}}_{t+_{t}}( _{t+_{t}}-1)x_{t,i}_{i}+c_{t=2}^{T}_{i=1}^{K} _{t}_{i}x_{t,i}(1/x_{t,i})}{ K}\] \[+(K_{t=1}^{T}_{t,t+_{ t}}),\] (4)

where \(a,b,c 0\) are constants and \(_{t}=_{s=1}^{t}(s+_{s}=t)\) is the number of arrivals at time \(t\) (if a round \(s\) is skipped at time \(t\) it counts as an "empty" arrival with loss estimate set to zero). By combining (4) with Lemma 2, we obtain

\[_{T} 2a_{t=1}^{T}_{i i}_{t}x_{ t,i}^{1/2}+2b_{t=1}^{T}_{i i^{*}}_{t+_{t}}( _{t+_{t}}-1)x_{t,i}_{i}+2c_{t=2}^{T}_{i=1}^{ K}_{t}_{t}x_{t,i}(1/x_{t,i})}{ K}\] \[+(K_{t=1}^{T}(_{t,t+ _{t}}+_{t,t+_{t}+_{}^{t}})+ _{}+S^{*}).\] (5)Then we apply a self-bounding analysis, similar to Masoudian et al. (2022), and get

\[_{T}=_{i i^{*}}(}(T)+}{_{i} K})+_{}+K_{t=1}^{T} (_{t,t+_{t}}+_{t,t+_{t}+_{ }^{t}})+S^{*}.\]

The details of the self-bounding analysis are provided in Appendix C.

The stochastic analysis is completed by the following lemma, which bounds the sum of implicit exploration terms above. It constitutes the third key result of the paper and shows that the bias from implicit exploration does not deteriorate neither the stochastic nor the adversarial bound. The proof is based on a careful study of the evolution of \(_{t}\) throughout the game, and is deferred to Appendix D.

**Lemma 4** (Summation Bound).: _For all \(s[T]\), let \(_{s}=_{r=1}^{s}_{r}\) and \(_{s,t}=e^{-_{t}}{_{s}}}\), then_

\[_{t=1}^{T}(_{t,t+_{t}}+_{t,t+_{t }+_{}^{t}})=(_{}).\]

#### Proof of Lemma 2 (Drift of the Drifted Regret)

We start with the definition of the drifted regret.

\[_{T}^{drift}=[_{t=1}^{T}(  x_{t},_{t}^{obs}-_{t,i_{T}^{*}}^{ obs})] =_{t=1}^{T}_{s+_{s}=t\\ s_{t}}_{i=1}^{K}[x_{s,i}x_{t,i}}{\{x_{s,i},_{s,t}\}}-^{*}}x_{s,i_{T}^{*}}x_{t,i}}{\{x_{s,i_{T}^{*}},_{ s,t}\}}]\] \[_{t=1}^{T}_{s+_{s}=t \\ s_{t}}_{i=1}^{K}[x_{s,i}x_{t,i}}{\{x_{s,i},_{s,t}\}}-_{s,i _{T}^{*}}x_{t,i}]\] \[_{t=1}^{T}_{s+_{s}=t}_{i=1}^{K} x_{s,i}x_{t,i}}{\{x_{s, i},_{s,t}\}}}_{}-_{s,i_{T}^{*}}x_{t,i}-^{*}.\] (6)

Note that when taking the expectation, we rely on the fact that \(_{s}\) with \(s+_{s}=t\) does not affect \(x_{t}\). If \(\{x_{s,i},_{s,t}\}=x_{s,i}\), then \(=_{s,i}x_{t,i}\), otherwise

\[=_{s,i}x_{t,i}-x_{t,i}(_{s,t}-x_{s,i} )}{_{s,t}}_{s,i}x_{t,i}-(_{s, t}-x_{s,i})}{_{s,t}}_{s,i}x_{t,i}-4_{s,t},\] (7)

where the first inequality uses \(x_{t,i} 4(x_{s,i},_{s,t})=4_{s,t}\) by Lemma 3, and \(_{s,i} 1\), and the second inequality follows by \(x_{s,i} 0\). Plugging (7) into (6) gives

\[_{T}^{drift} _{t=1}^{T}_{s+_{s}=t}_{i=1}^{K} [(_{s,i}x_{t,i}-4_{s,t}-_{s,i_{T}^{*}}x_{t,i})]-S^{*}\] \[[_{t=1}^{T}_{s+_{s}=t}_{i=1}^{K}_{i}x_{t,i}]}_{$}}-4K_{t=1}^{T} _{s+_{s}=t}[_{s,t}]-S^{*}.\] (8)

It suffices to give a lower bound for \(R_{T}\) in terms of the actual regret \(_{T}\). The difference between \(R_{T}\) and \(_{T}\) is that \(_{T}=[_{t=1}^{T}_{i=1}^{K}_{i}x_{t,i}]\), whereas in \(R_{T}\) the sum \(_{i=1}^{K}_{i}x_{t,i}\) is multiplied by the number of arrivals \(_{t}=_{s=1}^{t}(s+_{s}=t)\) at time \(t\), and \(_{t}\) might be larger than one or zero due to delays.

Our main idea here is to leverage the drift control lemma to provide a lower bound for \(R_{T}\) in terms of \(_{T}\). Specifically, by Lemma 3 for all \(r[0,d_{}^{t}]\), we have \((x_{t,i},_{t,t+r})x_{t+r,i}\), which implies \(x_{t,i}x_{t+r,i}-_{t,t+r}\). Thus, we obtain the following bound for any \(r[0,d_{}^{t}]\)

\[_{i=1}^{K}_{i}x_{t,i}_{i=1}^{K}_{i}x_{t+r, i}-K_{t,t+r}.\] (9)

In Algorithm 2 we provide a greedy procedure to rearrange the arrivals by postponing some arrivals to future rounds to create a _hypothetical_ rearranged sequence with at most one arrival at each round. Colliding arrivals are postponed to the first available (unoccupied) slot in the future. In Lemma 5 below we show that arrival originally received at time \(t\) stays in the \([t,t+_{}^{t}]\) interval (note that \(_{}^{t} d_{}^{t}\)). When an observation from round \(s\) is postponed from arriving at round \(t\) to arriving at round \(t+r\) for \(r[0,d_{}^{t}]\), by (9) it is equivalent to replacing \(_{i=1}^{K}_{i}x_{t,i}\) by \(_{i=1}^{K}_{i}x_{t+r,i}-K_{t,t+r}\) in \(R_{T}\). Note that Algorithm 2 may push an arrival to a round larger than \(T\), which is equivalent to replacing \(_{i=1}^{K}_{i}x_{t,i}\) by zero.

Let \(v_{t}^{new}\) for all \(t[T+d_{}^{T}]\) be the total arrivals at time \(t\) after the rearrangement, and let \((t)\) be the round to which we have mapped round \(t\) for all \(t[T]\). Then for any rearrangement

\[R_{T}=[_{t=1}^{T}v_{t}_{i=1}^{K}_{i}x_{t,i} ][_{t=1}^{T}_{t}^{new}_{i =1}^{K}_{i}x_{t,i}-K_{t=1}^{T}_{t,(t)}].\] (10)

The following lemma provides properties of the rearrangement procedure.

**Lemma 5**.: _Let \(_{}^{t}=_{s[t]}\{_{s}\}\). Then Algorithm 2 ensures for any \(t[T+d_{}^{T}]\) that \(_{t}^{new}\{0,1\}\). Furthermore, for any round \(t[T]\) it keeps all the arrivals at time \(t\) in the interval \([t,t+_{}^{t}]\), such that \( s t:s+_{s}=t(s)-t_{}^{t}\)._

We provide a proof of the lemma in Appendix E. As a corollary, after the Greedy Rearrangement (Algorithm 2) the number of rounds with zero arrivals is at most \(_{}^{T}\). This is because there will be no arrivals after \(T+_{}^{T}\) and \(_{t=1}^{T+_{}^{T}}_{t}^{new}=_{t=1}^{T}v_{t}=T\), which implies there are at most \(_{}^{T}\) zero arrivals as each round receives at most one arrival. Therefore

\[_{t=1}^{T}_{t}^{new}_{i=1}^{K} _{i}x_{t,i} =_{T}-[_{t=1}^{T}(v_ {t}^{new}=0)_{i=1}^{K}_{i}x_{t,i}]\] (11) \[_{T}-[_{t=1}^{T}( _{t}^{new}=0)]_{T}-[_{ }^{T}]_{T}-_{},\]

where the first equality uses the definition of \(_{T}=[_{t=1}^{T}_{i=1}^{K}_{i}x_{t,i}]\) and that \( t[T]:_{t}^{new}\{0,1\}\).

Since \( t[T]:(t) t+_{t}+_{}^{t}\), we have \(_{t,(t)}_{t,t+_{t}+_{}^{t}}\). Together with (11), (10), and (8) it completes the proof.

### Adversarial Analysis

The adversarial analysis is similar to the analysis of Zimmert and Seldin (2020, Theorem 2). In Appendix G we show that

\[_{T}=(+_{[T]} \{||+_{} K}\}+S^{*}+K _{t=1}^{T}_{t,t+_{t}}),\]

where the first two terms originate from the analysis of Zimmert and Seldin due to structural similarity of the algorithm, \(S^{*}\) is due to adjusted skipping threshold, and \(K_{t=1}^{T}_{t,t+_{t}}\) is due to implicit exploration bias and is bounded by Lemma 4. The proof is completed by the following bound on \(S^{*}\), which is shown in Appendix H.

**Lemma 6**.: _We have \(S^{*}=((d_{}K^{} K\;,_{ [T]}\{||+_{}K^{ } K}\})).\)_Discussion

We have successfully addressed the challenge of handling varying and potentially unbounded delays in best-of-both-worlds setting. The success was based on three technical innovations, which may be interesting in their own right: (1) A relation between the drifted and the standard regret under unbounded delays (given by Lemma 2, Algorithm 2, and Lemma 5); (2) A novel control of distribution drift based on implicit exploration and skipping that does not alter the learning rates and exhibits efficiency under highly varying delays (Lemma 3); and (3) An implicit exploration scheme applicable in best-of-both-worlds setting (Lemma 4).

The work leads to several directions for future research. One question is whether the best-of-both-worlds bounds could be improved further. In particular, whether the \(K^{}\) term in the adversarial regret bound could be reduced or eliminated. The term arose due to the need to decrease the skipping threshold of Zimmert and Seldin (2020) to control the distribution drift. It would also be valuable to explore whether it is possible to reduce the \(S^{*}\) term and reduce or eliminate the \(_{i i^{*}}_{}}{_{i} K}\) term in the stochastic bound, or to derive lower bounds showing that these terms are unavoidable. Another interesting direction is to find more applications for implicit exploration and skipping in the context of best-of-both-worlds bounds.