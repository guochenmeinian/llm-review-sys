# SPRING: Studying the Paper and Reasoning to Play Games

Yue Wu\({}^{14}\), Shrimai Prabhumoye\({}^{2}\), So Yeon Min\({}^{1}\), Yonatan Bisk\({}^{1}\), Ruslan Salakhutdinov\({}^{1}\), Amos Azaria\({}^{3}\), Tom Mitchell\({}^{1}\), Yuanzhi Li\({}^{1,4}\)

\({}^{1}\)Carnegie Mellon University, \({}^{2}\)NVIDIA, \({}^{3}\)Ariel University, \({}^{4}\)Microsoft Research

Work done during internship at Microsoft. For correspondence, contact ywu5@andrew.cmu.edu

###### Abstract

Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Craster or Minecraft. We propose a novel approach, SPRING, to read Craster's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context "reasoning" induced by different forms of prompts under the setting of the Craster environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of Craster as a test bed for LLMs. Code at github.com/holmeswww/SPRING

## 1 Introduction

Open-world survival games like Minecraft (Fan et al., 2022) and Craster (Hafner, 2021) pose significant challenges for AI algorithms due to a combination of factors: procedural generation requires strong generalization; diverse action space requires multi-task capabilities; technology tree requires long-term planning and deep exploration; diverse and conflicting objectives requires goal prioritization. In particular, Craster is designed for efficient simulation and fast iteration. Similar to Minecraft, Craster features key challenges such as multi-tasking, exploration with a deep and wide tech-tree, requiring the agent to craft multiple tools and interact with multiple objects to survive in the game.

Reinforcement learning (RL) has been the go-to approach for game-based problems, with numerous successes in games like Go (Silver et al., 2017), robotics (Fu et al., 2020; Hafner et al., 2023) and various video games (Vinyals et al., 2019; Schrittwieser et al., 2020; Badia et al., 2020; Hafner et al., 2023). While RL demonstrated impressive performance, it still suffers from certain limitations, such as high sample complexity and difficulty in incorporating prior knowledge. Such drawbacks make it exceptionally challenging to apply RL to diverse and complex open-world benchmarks like Craster (Hafner, 2021) or Minecraft (Fan et al., 2022). Addressing the benefits and drawbacks of RL is therefore crucial for achieving a sample-efficient solution.

On the other hand, large language models (LLMs) (Brown et al., 2020; Smith et al., 2022; Chowdhery et al., 2022) have shown remarkable success when prompted for various tasks, including embodied planning and acting (Ahn et al., 2022; Du et al., 2023; Wang et al., 2023; Shinn et al., 2023), QA or dialogue (Ouyang et al., 2022; Bubeck et al., 2023), and general problem-solving (Brown et al., 2020; Bubeck et al., 2023). Their unique planning (Ahn et al., 2022), reasoning (Shinn et al., 2023), and problem-solving (Bubeck et al., 2023; Madaan et al., 2023) ability makes them a promising candidate for incorporating prior knowledge and in-context reasoning for game-based problems, particularly when it comes to addressing the aforementioned limitations of RL.

Hence, in this work, we study the possibility and reliability of LLMs for understanding and reasoning with human knowledge, in the setting of games. We consider a two staged approach SPRING (Figure 1): (1) **studying the paper**: the first stage reads the LaTeX of the paper of (Hafner, 2021) and (2) **reasoning**: the second stage involves reasoning about that knowledge through a QA framework to take an environment action. Note that the Crafter environment was released after the data collection date of GPT-3.5 and GPT 4 (OpenAI, 2023) models2, the environment is unseen to them. We first use LLM to extract prior knowledge from the LaTeX source code of the original paper by Hafner (2021). We then use a similar QA summarization framework as Wu et al. (2023) which produces QA dialogue on game mechanics. SPRING handles significantly more diverse contextual information than (Wu et al., 2023), making use of all 17 action/interaction types and even information about desirable behaviors documented in the paper.

We focus on reading the relevant academic paper in the first stage of SPRING, by first deciding the paragraphs that are relevant for playing the game. Then we extract key information through a series of questions such as "_Write all information helpful for the game in a numbered list._". In the second stage,we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games. The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges. For example, the question "_For each action, are the requirements met?_" depends on the question "_What are the top 5 actions?_", creating an edge from the latter to the former. For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.

Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in Crafter (Hafner, 2021). Quantitatively, SPRING's zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithmstrained for 1M steps (Table 2).

Our contributions are as follows:

* SPRING is the first to tackle a competitive RL benchmark by explicitly extracting multiple interactions and tech-tree dependencies directly from an academic paper.

Figure 1: Overview of SPRING. The context string, shown in the middle column, is obtained by parsing the LaTeX source code of Hafner (2021). The LLM-based agent then takes input from a visual game descriptor and the context string. The agent uses questions composed into a DAG for chain-of-thought reasoning, and the last node of the DAG is parsed into action.

* We are the first to show SOTA performance in a challenging open world game with a zero-shot LLM-based (GPT-4) policy
* We study the quality of in-context reasoning induced by different prompts and propose a controlled chain-of-thought prompting through a DAG of questions for decision making.

## 2 Method

This section is structured as follows. We first describe how we generate the context from the LaTeX source code of Hafner (2021) in Section 2.1. Then we describe our SPRING framework and how we compute the action in Section 2.2.

Problem SettingOur goal is to show that LLMs can plan and act reasonably well in an environment where control tasks are less required. In the setting of Crafter, we define the states, \(s\), as samples from state distribution \(S\). We are interested in creating a goal-conditioned policy \(\) which maps state \(s\) to action \(a A\), \(:S A\). Due to the use of LLM, we further break the policy down into two parts: a descriptor \(\) which describes key aspects the visual observation in plain text (\(d=(s)\)). And an LLM-based actor which takes state description \(d\) and outputs action \(a\).

In addition, we define \(^{j}_{}\) to be the \(j^{}\) paragraph in the LaTeX source of the environment paper (Hafner, 2021), and \(_{LLM}\) to be the LLM which takes a context string and a question string as input and outputs an answer to the question.

### Studying the paper: Context from LaTeX source

Similar to Wu et al. (2023), we compose gameplay specific questions and then compute LLM answer to the questions for each subsection in the latex files. Since a considerable amount of the paper is irrelevant to the gameplay, we use a set of 2 questions \(Q_{}\)=["Would this paragraph help me succeed in this game"]. "Does this paragraph contain information on the game mechanics, or game strategies?") to identify relevance, and a set of 4 questions \(Q_{}\)=["Write all information helpful for the game in a numbered list.", "In plain text. List all objects I need to interact/avoid to survive in the game. Use "I would like to X object Y" in each step. Replace Y by the actual object, X by the actual interaction.", "Write all game objectives numbered list. For each objective, list its requirements.", "Write all actions as a numbered list. For each action, list its requirements.") to summarize gameplay and action space relevant information. We add the prompt "do NOT answer in LaTeX." to all of \(Q_{}\) to prevent the LLM from outputting the list in LaTeX format.

For a specific gameplay specific question \(q Q_{}\), our goal is to compute \(C_{q}\), the answer to \(q\) conditioned on the paper. However, since the length of the paper exceeds input length constraints for most LLMs, we have to break the paper down into paragraphs individual \(^{j}_{}\). We provide an illustration of the process in Figure 2.

First, we filter the paragraphs for relevance and keep only paragraphs identified as relevant by at least one question from \(Q_{}\). We set \(P^{}_{q}\) to be the set of relevant paragraphs.

\[P^{}_{q}=\{^{j}_{}| q_{r} Q_{ }\ s.t.\ _{LLM}(^{j}_{},q_{r})=\}\] (1)

Figure 2: **Paper Studying Module. The 3-step approach for computing \(C_{q}\) from the LaTeXsource code of Hafner (2021). First, as shown in the left column, for each paragraph we compute LLM answer for all relevancy questions in \(Q_{}\), and keep only the relevant paragraphs. Second, as shown in the middle column, we compute paragraph-level LLM answer to \(q\). Third, we summarize the answer into \(C_{q}\) with a summary prompt; we concatenate \(C_{q}\) across \(q Q_{game}\) and obtain \(C\).**

Second, we compute the set, \(A_{q}\), of answers to \(q\) for each relevant paragraph from \(P_{q}^{}\), from the LaTeX source code.

\[A_{q}=\{_{LLM}(_{},q):_{} P_{q}^{}\}\] (2)

Third, to obtain the answer string \(C_{q}\) from the set \(A_{q}\), we query an LLM with a summarization prompt \(q_{}=\) "Remove duplicate items."

\[C_{q}=_{LLM}((A_{q}),q_{})\] (3)

Finally, we concatenate (with the linebreak character) all question-context pairs to form the context string \(C\) for SPRING.

\[C=(\{qC_{q}| q Q_{}\})\] (4)

### Reasoning: QA-DAG for SPRING

For LLMs to be able to understand the gameplay, we first follow Du et al. (2023); Wang et al. (2023) to define an visual descriptor \(_{}\) which converts state \(s S\) to textual description \(d\) (Figure 3 a).

To achieve consistent chain-of-thought reasoning (Wei et al., 2021) throughout hundreds of steps within one round of gameplay, we compose a fixed set of questions \(Q_{}=\{q_{1},,q_{a}\}\) to query

   Node &  \\  \(q_{1}\) & List objects in the current observation. For each object, briefly answer what resource it provides \\  & and its requirement. \\  \(q_{2}\) & What was the last action taken by the player? \\  \(q_{3}\) & For each object in the list, are the requirements met for the interaction? \\  \(q_{4}\) & Did the last player action succeed? If not, why? \\  \(q_{5}\) & List top 3 sub-tasks the player should follow. Indicate their priority out of 5. \\  \(q_{6}\) & What are the requirements for the top sub-task? What should the player do first? \\  \(q_{7}\) & List top 5 actions the player should take and the requirement for each action. Choose ONLY from \\  & the list of all actions. Indicate their priority out of 5. \\  \(q_{8}\) & For each action in the list, are the requirements met? \\  \(q_{a}\) & Choose the best executable action from above. \\   

Table 1: List of all 9 questions in \(Q_{}\). The questions are designed to promote consistent chain-of-thought. Experimentally, we find the LLM robust to different phrasing of the questions.

Figure 3: **Reasoning.****(a)** The visual descriptor takes the last two gameplay screens as input, and outputs their descriptions in language (\(d^{t},d^{t-1}\)). **(b)** SPRING traverses a DAG of questions from Table 1 in topological order. Answer to the final question \(q_{a}\) is mapped to environment action using sub-string matching. **(c)** The LLM answer for each question (node) is conditioned on the previous 2 steps of observation, the context \(C\), and answers to the immediate parents of the current node.

the LLM at every step of the game, with question-question dependencies as \(D=\{(q_{u},q_{v})|q_{u},q_{v} Q_{}q_{v}q_{u}\}\). Note that the above specification forms a directed acyclic graph (DAG) with nodes \(Q_{}\) edges \(D\) (Figure 3 b).

For any question (node) \(q_{v} Q_{}\), we compute the answer \(A^{t}_{q_{v}}\) for time step \(t\), conditioned on the gameplay context \(C\), most recent 2 steps of game description \(d^{t-1},d^{t}\), and answers to its dependencies (Figure 3 c).

\[A^{t}_{q_{v}}=_{LLM}((C,d^{t-1},d^{t}, \{A^{t}_{q_{u}}|(q_{u},q_{v}) D\}),q_{v})\] (5)

Experimentally, we find that prompting the LLM with only the direct parents of a question greatly reduces the context length, and helps LLM to focus on the most relevant contextual information.

We traverse the DAG using a modified topological sort algorithm to compute LLM answer for each question based on its topological order. Finally, we map the answer to the last question in the node \(q_{a}\) directly to one of the 17 named actions in the environment with sub-string matching (\(a=A^{t}_{a}\)). We take the default action "Do" on sub-string matching failure.3

## 3 Experiments and Results

We present our experiments as follows. First, we explain our experimental setup and baselines for our experiments. Then, we compare SPRING to popular RL methods on the Crafter benchmark. Finally, we conduct experiments and analysis on different pieces of our architecture to study the influence of each part over the in-context "reasoning" capabilities of the LLM.

### Experimental Details

The Crafter environment (Hafner, 2021) is a procedurally generated open-world survival game for benchmarking RL algorithms with 22 achievements in a tech tree of depth 7. The environment is a grid-world features top-down observation and discrete action space of size 17. The observation also shows the current inventory state of the player, including its health points, food, water, rest levels, and inventory. The game is inspired by Minecraft and features a similar get-to-diamond challenge. In comparison, Crafter captures many key research challenges of Minecraft in a simpler and faster environment, thus speeding up experiments and result collection.

Environment DescriptorThe gameplay screen (top left of Fig 3.) consists of a 9 \(\) 9 grid (\(\{(i,j) 1 i,j 9\}\)). The top 7 rows consist of the local view of the world; each cell \((i,j)\) is associated with a pre-defined background (e.g., "grass", "water", "none") and possibly with an object "asset" (e.g., "tree", "health", "player"). The bottom 2 rows represent agent status (e.g., "health") and item inventories, which include images of assets (e.g., "stone sword"), and the number of each ascent in the inventory.

Our environment descriptor accepts as input the gameplay screen and outputs a text description of the screen. We first create combinations of background and object (appearance) assets. Then we add number assets to recognize the quantity of inventory/ status. We match these combinations with the gameplay screen, using cv2.filters with a matching _threshold_ of \(0.9\). We disable the detector during nights when observations are unreliable. Finally, for each \((i,j)\), we filter the matched combinations, and select the one with the highest matching score. From this information, we can measure the distance and direction of each object relative to the player; simultaneously, we can count the agent status and inventory item.

The environment descriptor then obtains the set of objects in observation \(=\{(obj,dist,direction)\}\), the set of inventory items \(=\{(object,count)\}\), and the agent status \(=\{(attribute,value,max)\}\). Including only the closest object of each kind, we compose the observation description \(d\) as: "You see : - <obj> <dist> steps to your <direction>. Your status: <attribute>: <value>/ <max>. Your inventory: - <object>: <count>". We describe direction of objects using "north","south","east","west".

Evaluation MetricsAgents in Crafter are evaluated primarily based on two metrics: reward and score. The game assigns a sparse \(+1\) reward each time the agent unlocks a new achievement in an episode, and assigns reward of \(-0.1/0.1\) when the agent loses/gains one health point. The score metric (Hafner, 2021) is computed by aggregating the success rates for each achievement:

\[S=(_{i=1}^{N}(1+s_{i}))-1,\]

where \(s_{i}\) is the agent's success rate on achievement \(i\) and \(N=22\) is the number of achievements. Note that RL agents only train on the reward, and SPRING does not require any training.

RL BaselinesWe include results from popular actor-critic methods like PPO (Schulman et al., 2017); DQN variants like Rainbow (Hessel et al., 2018); intrinsically motivated methods like RND (Burda et al., 2018), Plan2Explore (Sekar et al., 2020), EDE (Jiang et al., 2022); LLM assisted solutions like ELLM Du et al. (2023); model-based methods like DreamerV2 (Hafner et al., 2020); DreamerV3 (Hafner et al., 2023), which currently holds the state-of-the-art.

LLMs.For LLM access, we use GPT-3.5-turbo (OpenAI, OpenAI) and GPT-4 (OpenAI, 2023) from OpenAI's API.

### Overall Results

We compare the performance of RL baselines to SPRING with GPT-4 conditioned on the environment paper (Hafner, 2021) in Table 2.

SPRING out-performs the previous SOTA, including previous attempts at using LLMs for Crafter by large margins, achieving an \(88\%\) relative improvement on game score and a \(5\%\) improvement in reward on the best performing RL method (Hafner et al., 2023). Since the model obtains knowledge from reading the paper, SPRING requires \(0\) training steps, while RL methods generally require millions of training steps4.

We include a plot of unlock rate by task, comparing our method to popular RL baselines in Figure 4. SPRING assisted by prior knowledge out-performs RL methods by more than 10x on achievements like "Make Stone Pickaxe", "Make Stone Sword", and "Collect Iron", which are up to depth 5 down in the tech tree and significantly harder to reach through random exploration. For achievements "Eat Cow" and "Collect Drink", SPRING achieves perfect performance, whereas model-based RL framework like Dreamer-V3 has more than 5x lower unlock rate for "eat cow" since cows are moving and harder to reach through random exploration. Finally, we note that SPRING did not take the action "Place Stone", which can be reached easily by random exploration, since placing a stone was not discussed as beneficial for the agent in the paper (Hafner, 2021).

   Method & Score & Reward & Training Steps\({}^{4}\) \\  Human Experts & \(50.5 6.8\%\) & \(14.3 2.3\) & N/A \\  SPRING + paper (Ours) & \(\) & \(\) & **0** \\ DreamerV3 (Hafner et al., 2023) & \(14.5 1.6\%\) & \(\) & 1M \\ ELLM (Du et al., 2023) & N/A & \(6.0 0.4\) & 5M \\ EDE (Jiang et al., 2022) & \(11.7 1.0\%\) & N/A & 1M \\ DreamerV2 (Hafner et al., 2020) & \(10.0 1.2\%\) & \(9.0 1.7\) & 1M \\ PPO (Schulman et al., 2017) & \(4.6 0.3\%\) & \(4.2 1.2\) & 1M \\ Rainbow (Hessel et al., 2018) & \(4.3 0.2\%\) & \(5.0 1.3\) & 1M \\ Plan2Explore (Sekar et al., 2020) & \(2.1 0.1\%\) & \(2.1 1.5\) & 1M \\ RND (Burda et al., 2018) & \(2.0 0.1\%\) & \(0.7 1.3\) & 1M \\ Random & \(1.6 0.0\%\) & \(2.1 1.3\) & 0 \\   

Table 2: Table comparing SPRING and popular RL algorithms in terms of game score, reward, and training steps. Results for SPRING is summarized over 5 independent trials. SPRING out-performs the previous SOTA in terms of all metrics. In addition, since SPRING gathers knowledge from reading the paper, it requires no training.

### Component Analysis

We study how the different aspects of the framework contribute to the behavior of the agent through a series of ablations as shown in Table 3.

Studying the LaTeX PaperIn the first 4 rows of Table 3, we investigate the contribution of game-play context from the LaTeX paper toward performance of the agent. We report the performance of SPRING with no contextual information (w/o \(C\)) (row 4); SPRING conditioned on only the action descriptions and dependencies from (Hafner, 2021) Table F.1 (only question 4 from \(Q_{}\)) (row 3); SPRING conditioned on the context manually modified to exclude the "crafting table" dependency for wooden_pickase by removing two corresponding lines from the context \(C\) (row 2); SPRING conditioned on the full context from the paper (row 1).

As expected, since Crafter environment is unseen for GPT, the agent achieves performance similar to random agent without any game context. When provided with only action descriptions and action dependencies, using only question 4 from \(Q_{}\) in section 2.1, SPRING achieves strong \(67\%\) performance comparable to DreamerV2 (Silver et al., 2017).

For the next piece of the experiment, we manually remove "near crafting table" dependency for wooden_pickase from it's context, which is required for 11 later achievements. SPRING with GPT-4 incurs a \(24\%\) performance drop. Interestingly, we find that the LLM has some ability to recover

Figure 4: Ability spectrum showing the unlocking percentages for all 22 achievements. Rainbow manages to drink water and forage for food. DreamerV3 collects coal, iron, stone, and forges more advanced tools and weapons. Since SPRING starts off with knowledge about the game, it achieves more than 10x higher unlock rate on previously hard-to-reach tasks like “Eat Plant”, “Make Stone Pickaxe”, “Make Stone Sword”, and “Collect Iron”.

   Method & Achievement Depth & Reward & Questions per Step \\  SPRING + Full Paper & 6 & \(\) & 9 \\ SPRING + Paper w/ modified \(C\) & 4 & \(9.4 1.8\) & 9 \\ SPRING + Action Description & 4 & \(8.2 0.2\) & 9 \\ SPRING + w/o \(C\) & 1 & \(0.5 0.2\) & 9 \\  SPRING + Full Paper & 6 & \(\) & 9 \\ Step-by-step prompt + Full Paper & 5 & \(7.3 4.4\) & 2 \\ QA w/o DAG + Full Paper & 4 & \(4.3 3.9\) & 9 \\ w/o QA + Full Paper & 2 & \(2.4 1.3\) & 1 \\  SPRING + Full Paper & 6 & \(\) & 9 \\ SPRING + Full Paper w/ GPT-3.5 & 2 & \(3.3 2.9\) & 9 \\   

Table 3: Analysis on how different parts of SPRING contribute to its performance, comparing the max achievement depth in the tech tree, the reward, and the number of human-written questions in the prompt. Results are summarized over 5 independent trials. The first 4 rows study the necessity of prior knowledge from the context string \(C\). The middle 4 rows study different chain-of-thought prompting techniques. The last 2 rows study the role of LLMs. All three aspects are important for SPRING to achieve best reported performance.

from the inaccurate context information. We observe that after failing to craft the wooden_pickaxe without a table, the agent instead tries to craft a wooden_sword first to maintain survival. Eventually, the agent was able to identify the missing requirement through guessing and trying after some unsuccessful trials, and craft the wooden_pickaxe. However, the confusion delayed the agent's progress and therefore causes the performance gap with the agent conditioned on the full context (row 5).

ReasoningIn the middle 4 rows of Table 3, we investigate the contribution of different prompting methods toward performance of the model. Conditioned on the full context from the LaTeX paper, we report the performance of GPT-4 directly prompted to output the action using the last question \(q_{a}\) only (row 8); GPT-4 prompted with all questions from \(Q_{}\) but in a list without the DAG dependencies \(D\) (row 7); GPT-4 prompted "Let's think step-by-step" (Kojima et al., 2022) about the next action, and prompted to choose a permissible action \(q_{a}\) with let's think step-by-step followed by \(q_{a}\) again (row 6); GPT-4 with SPRING (row 5).

Relative to our method, we observe that directly prompting the LLM for the action leads to a \(80\%\) performance drop, and therefore does not result in a meaningful agent. The popular chain-of-thought reasoning prompt "Let's think step-by-step" (Kojima et al., 2022) achieves reasonable reward with a \(40\%\) drop, but with a high \(60.27\%\) standard deviation. Qualitatively, we observe that the LLM produces inconsistent outputs across time steps, due to the fact that the model's chain-of-thought is not directed or controlled through the prompt. Therefore, LLMs prompted with "Let's think step-by-step" alone cannot reliably follow a good policy. Controlling the chain-of-thought with 9 questions from \(Q_{}\) (section 2.2) successfully controls the consistency of LLM outputs across time qualitatively. However, we observe that the LLM often ignores earlier questions at later stages of QA when all previous questions are presented in a list, leading to random disagreements in answers. For example, the LLM may correctly identify that it needs "wooden pickaxe" to mine the stone ahead in the first few questions, but forgets about the requirement later when it's prompted for actions. Quantitatively, the model performs \(65\%\) worse with \(90\%\) variance without the DAG. The introduction of DAG eliminates this problem by reducing the QA context length to only a question's immediate parents.

Overall, SPRING achieves the best performance and a small \(6\%\) performance standard deviation, due to more consistent reasoning over time steps with better focus and fewer distractions.

LlmIn the last two rows of Table 3, we show that the same architecture does not work well with GPT-3.5-turbo. We believe the observed \(73\%\) performance gap mainly comes from GPT-3.5-turbo's worse performance at following fine-grained instructions in each of the questions, which are required for chain-of-thought reasoning with SPRING.

### Cost for running SPRING

The number of queries per step is 9 (same as the number of questions). Each game could take around 300 steps, but can go up to 500 steps in the worst case. Therefore, the maximum number of queries per game can go up to 4500. According to the public price of GPT-4 API, each query costs around 0.065. The total cost should be less than 270 (USD) per game with GPT-4. Given the progress in chip-set development, we are hopeful that the inference costs will lower, making LLMs more accessible for the public.

### Potential for Benchmarking LLMs

In Table 4, we compare popular publicly available LLMs including GPT-4 (OpenAI, 2023), GPT-3.5 (text-davinci-003) (OpenAI, OpenAI), Bard (Manyika, Manyika), Claude (Anthropic, Anthropic), Alpaca-30b (Taori et al., 2023) under the same setting on Crafter, following the same step-by-step prompt as Section 3.3 and Table 3. We observe a clear separation in performance under our setting.

## 4 Related Work

RL v.s. LLMsComparing LLM-based agents against RL agents brings forth a intriguing discussion. RL algorithms do not require prior knowledge such as instruction manuals, and could continually improve given enough trials. However, RL algorithms are typically trained with reward functions deliberately engineered to cover all in-game achievements (Hafner, 2021; Hafner et al., 2023). Such reward functions often require a lot of expert knowledge and careful formulation.

On the other hand, LLM agents like SPRING does not need the reward (we report reward for comparison purpose, SPRING does not use the reward during inference), but instead uses external knowledge from the EIEX source code. In addition, current LLM agents lack the capabilities of improving from interactions.

We hope future works would be able to leverage the benefits of both paradigms in order to achieve efficient planning with fine-grained control.

Policy Informed by Natural Language InstructionsIn the instruction following setting, step-by-step instructions have been used to generate auxiliary rewards, when environment rewards are sparse. Goyal et al. (2019); Wang et al. (2019) use auxiliary reward-learning modules trained offline to predict whether trajectory segments correspond to natural language annotations of expert trajectories.

There has been many attempts to go beyond instruction following to learning from unstructured natural language (Branavan et al., 2012; Goldwasser and Roth, 2014; Zhong et al., 2021; Wang and Narasimhan, 2021). Zhong et al. (2021); Wang and Narasimhan (2021) make use of special architectures to learn reasoning on grid worlds with template-generated instructions. However, the model requires 200 million training samples from templates identical to the test environments. Such a training requirement limiting the generalization of the model and causes performance loss even on slightly bigger grid worlds with identical mechanics.

Wu et al. (2023) proposes a summary (Read) and reasoning (Reward) through a QA prompting framework with an open-source QA LLM (Tafjord and Clark, 2021). The framework demonstrates the possibility of an using real-world human-written manuals to improve RL performance on popular games, despite limiting the interaction types to only "hit". Our framework handles all 17 kinds of interactions available in the game. Moreover, our framework makes use of information on tech-tree dependencies, and suggestions on desired policies extracted from the academic paper.

LLMs for PlanningLLMs have shown promising results at high-level planning in indoor embodied manipulation environments. Huang et al. (2022); Ahn et al. (2022) primarily explores generating plans for embodied tasks, with limited actions space and trajectory length. Song et al. (2022); Wu et al. (2022) enhances Ahn et al. (2022) with greater action diversity and real-time re-planning. However, a lot of the high-level plans lack executability and has to be post-processed to meet specific task requirements, thus limiting the generalization to complex open world tasks. In addition, all prior works along this line operates on few-shot human/expert generated demonstrations containing up to 17 trajectories to provide context for LLMs, which requires more manual labor, and may limit the generalization to unseen scenarios. In comparison, our SPRING framework requires no demonstration.

LLMs for Open World GamesCompared to popular indoor manipulation tasks, planning in open-world game environments poses the following additional challenges. 1) **Long horizon.** Due to the nature how in-game achievement/technology progresses, a successful gameplay can easily go beyond 200 steps (Hafner, 2021). 2) **Parallel objectives.** Open-world environments contain objectives that can be pursued in parallel and often require prioritization (Wang et al., 2023). Therefore, open world games are significantly more challenging than current indoor embodied manipulation environments.

   Method & Achievement Depth & Reward & Questions per Step \\  Step-by-step prompt + GPT-4 & 5 & \(\) & 2 \\ Step-by-step prompt + text-davinci-003 & 4 & \(4.5 2.1\) & 2 \\ Step-by-step prompt + Bard & 0 & \(-0.9 0\) & 2 \\ Step-by-step prompt + Claude & 1 & \(0.1 0.1\) & 2 \\ Step-by-step prompt + Alpaca-30b & 1 & \(0.1 0.1\) & 2 \\ Random & 1 & \(2.1 1.3\) & 0 \\   

Table 4: Comparison of different LLMs under the same setting using the context \(C\) generated with text-davinci-003 following the same step-by-step prompt as Section 3.3 and Table 3.

Du et al. (2023) applies LLMs as high-level planners to assist RL exploration in Crafter. Wang et al. (2023); Yuan et al. (2023) use LLMs as high-level planner and goal selector to control a low level-policy in Minecraft. Tsai et al. (2023) studies the capabilities of ChatGPT on text games. Notably, all prior works require expert or human generated example trajectories as context for the LLMs. Since the example trajectories do not cover all scenarios, all prior works may encounter unseen situation during evaluation, leading to an overall performance inferior to state-of-the-art RL algorithms (Hessel et al., 2018; Guss et al., 2021; Hafner et al., 2023), trained without the use of LLMs. To our knowledge, we are the first to show an LLM (GPT-4) achieving performance surpassing the state-of-the-art RL algorithms in a challenging open world game.

## 5 Limitations and Future Work

A primary limitation in using an LLM to support interaction with the environment is the need for object recognition and grounding. However, these limitations do not exist in environments that offer accurate object information, such as contemporary games (Fan et al., 2022) and virtual reality worlds (Kolve et al., 2017). While pre-trained visual backbones (He et al., 2017) perform poorly on games, they have shown reasonable performance for environments closer to the real-world (Shridhar et al., 2020). In addition, with recent progress on visual-language models (Bubeck et al., 2023; Driess et al., 2023; Liu et al., 2023; Zou et al., 2023), we believe there will be reliable and generalizable solutions to visual-language understanding in the foreseeable future. Future works could focus on address the requirement for a separate visual descriptor with large visual-language models.

## 6 Conclusions

In this work, we explore solving the Crafter (Hafner, 2021) RL benchmark using the latest LLMs by reading the LaTeX source code of an academic paper about the benchmark. We study the quality of in-context "reasoning" and "planning" induced by different forms of prompts under the setting of the Crafter open-world environment. To enforce consistent planning and execution over hundreds of environment steps, we introduce SPRING, an innovative prompting framework for LLMs designed to enable in-context chain-of-thought planning and reasoning. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training.

Our work demonstrates the reliability of LLMs for understanding and reasoning with human knowledge. We hope that our work points to a new way of integrating human prior knowledge into RL training through intrinsic rewards (Wu et al., 2023), hierarchical RL (Shu et al., 2017), or sub-goal planning (Wang et al., 2023; Wu et al., 2023).

## Broader Impacts

Our research on LLM holds potential for both positive and negative impacts. The benefits include better understanding of the powers of LLM and enhanced integration of prior knowledge, which could lead to advancement in various AI topics. However, the risks may involve reliance on computationally demanding models, game cheating or exploitation, and reliance on prior knowledge.