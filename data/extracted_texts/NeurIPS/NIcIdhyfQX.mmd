# Gradient of the prior \(\hat{x}_{T_{i}}\).

Q-Distribution guided Q-learning for offline reinforcement learning: Uncertainty penalized Q-value via consistency model

 Jing Zhang

HKUST

jzhanggy@connect.ust.hk

&Linjiajie Fang

HKUST

lfangad@connect.ust.hk

&Kexin Shi

HKUST

kshiaf@connect.ust.hk

&Wenjia Wang

HKUST (GZ)

wenjiawang@hkust-gz.edu.cn

&Bing-Yi Jing

SUSTech

jingby@sustech.edu.cn

Corresponding authors: Bing-Yi Jing and Wenjia Wang.

###### Abstract

"Distribution shift" is the main obstacle to the success of offline reinforcement learning. A learning policy may take actions beyond the behavior policy's knowledge, referred to as Out-of-Distribution (OOD) actions. The Q-values for these OOD actions can be easily overestimated. As a result, the learning policy is biased by using incorrect Q-value estimates. One common approach to avoid Q-value overestimation is to make a pessimistic adjustment. Our key idea is to penalize the Q-values of OOD actions associated with high uncertainty. In this work, we propose Q-Distribution Guided Q-Learning (QDQ), which applies a pessimistic adjustment to Q-values in OOD regions based on uncertainty estimation. This uncertainty measure relies on the conditional Q-value distribution, learned through a high-fidelity and efficient consistency model. Additionally, to prevent overly conservative estimates, we introduce an uncertainty-aware optimization objective for updating the Q-value function. The proposed QDQ demonstrates solid theoretical guarantees for the accuracy of Q-value distribution learning and uncertainty measurement, as well as the performance of the learning policy. QDQ consistently shows strong performance on the D4RL benchmark and achieves significant improvements across many tasks.

## 1 Introduction

Reinforcement learning (RL) has seen remarkable success by using expressive deep neural networks to estimate the value function or policy function . However, in deep RL optimization, updating the Q-value function or policy value function can be unstable and introduce significant bias . Since the learning policy is influenced by the Q-value function, any bias in the Q-values affects the learning policy. In online RL, the agent's interaction with the environment helps mitigate this bias through reward feedback for biased actions. However, in offline RL, the learning relies solely on data from a behavior policy, making information about rewards for states and actions outside the dataset's distribution unavailable.

It is commonly observed that during offline RL training, backups using OOD actions often lead to target Q-values being _overestimated_ (see Figure 1(a)). As a result, the learning policy tends to prioritize these risky actions during policy improvement. This false prioritization accumulates with each training step, ultimately leading to failure in the offline training process [4; 5; 6]. Therefore, addressing Q-value overestimation for OOD actions is crucial for the effective implementation of offline reinforcement learning.

Since any bias or error in the Q-value will propagate to the learning policy, it's crucial to evaluate whether the Q-value is assigned to OOD actions and to apply a pessimistic adjustment to address overestimation. Ideally, this adjustment should only target OOD actions. One common way to identify whether the Q-value function is updated by OOD actions is by estimating the uncertainty of the Q-value  in the action space. However, estimating uncertainty presents significant challenges, especially with high-capacity Q-value function approximators like neural networks . If Q-value uncertainty is not accurately estimated, a penalty may be uniformly applied across most actions , hindering the optimality of the Q-value function.

While various methods [7; 8; 9; 10; 11; 12; 3; 13; 14] attempt to make pessimistic estimates of the Q-value function, most have not effectively determined which Q-values need constraining or how to pessimistically estimate them with reliable and efficient uncertainty estimates. As a result, previous methods often end up being overly conservative in their Q-value estimations  or fail to achieve a tight lower confidence bound of the optimal Q-value function. Moreover, some in-sample training [16; 17; 18; 19] of the Q-value function may lead it to closely mimic the Q-value of the behavior policy (see Figure 1(b)), rendering it unable to surpass the performance of the behavior policy, especially when the behavior policy is sub-optimal. Therefore, in balancing Q safety for learning and not hindering the recovery of the most optimal Q-value, current methods tend to prioritize safe optimization of the Q-value function.

In this study, we introduce Q-Distribution guided Q-Learning (QDQ) for offline RL 2. The core concept focuses on estimating Q-value uncertainty by directly computing this uncertainty through bootstrap sampling from the behavior policy's Q-value distribution. By approximating the behavior policy's Q-values using the dataset, we train a high-fidelity and efficient distribution learner-consistency model . This ensures the quality of the learned Q-value distribution.

Figure 1: (a) The maximum of the estimated Q-value often occurs in OOD actions due to the instability of the offline RL backup process and the “distribution shift” problem, so the Q-value of the learning policy (yellow line) will diverge from the behavior policy’s action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value.

Since the behavior and learning policies share the same set of high-uncertainty actions , we can sample from the learned Q-value distribution to estimate uncertainty, identify risky actions, and make the Q target values for these actions more pessimistic. We then create an uncertainty-aware optimization objective to carefully penalize Q-values that may be OOD, ensuring that the constraints are appropriately pessimistic without hindering the Q-value function's exploration in the in-distribution region. QDO aims to find the optimal Q-value that exceeds the behavior policy's optimal Q-value while remaining as pessimistic as possible in the OOD region. Moreover, our pessimistic approach is robust against errors in uncertainty estimation. Our main contributions are as follows:

* **Utilization of trajectory-level data with a sliding window**: We use trajectory-level data with a sliding window approach to create the real truncated Q dataset. Our theoretical analysis (Theorem 4.1) confirms that the generated data has a distribution similar to true Q-values. Additionally, distributions learned from this dataset tend to favor high-reward actions.
* **Introduction of a consistency model as a distribution learner**: QDQ introduces the consistency model  as the distribution learner for the Q-value. Similar to the diffusion model, the consistency model demonstrates strong capabilities in distribution learning. Our theoretical analysis (Theorem 4.2) highlights its consistency and one-step sampling properties, making it an ideal choice for uncertainty estimation.
* **Risk estimation of Q-values through uncertainty assessment**: QDQ estimates the risk set of Q-values by evaluating the uncertainty of actions. For Q-values likely to be overestimated and associated with high uncertainty, a pessimistic penalty is applied. For safer Q-values, a mild adjustment based on uncertainty error enhances their robustness.
* **Uncertainty-aware optimization objective to address conservatism**: To reduce the overly conservative nature of pessimistic Q-learning in offline RL, QDQ introduces an uncertainty-aware optimization objective. This involves simultaneous optimistic and pessimistic learning of the Q-value. Theoretical (Theorem 4.3 and Theorem 4.4) and experimental analyses show that this approach effectively mitigates conservatism issues.

## 2 Background

Our approach aims to temper the Q-values in OOD areas to mitigate the risk of unpredictable extrapolation errors, leveraging uncertainty estimation. We estimate the uncertainty of Q-values across actions visited by the learning policy using samples from a learned conditional Q-distribution via the consistency model. In this section, we provide a concise overview of the problem settings in offline RL and introduce the consistency model.

### Fundamentals in offline RL

The online RL process is shaped by an infinite-horizon Markov decision process (MDP): \(=\{,,,r,_{0},\}\). The state space is \(\), and \(\) is the action space. The transition dynamic among the state is determined by \(:()\), where \(()\) is the support of \(\). The reward determined on the whole state and action space is \(r:,r<\), and can either be deterministic or random. \(_{0}(s_{0})\) is the distribution of the initial states \(s_{0}\), \((0,1)\) is the discount factor. The goal of RL is to find the optimal policy \(:(a)\) that yields the highest long-term average return:

\[J()=_{s_{0}_{0}}V(s_{0})=_{s_{0}_{0},a_{ 0}}Q(s_{0},a_{0})=_{s_{0}_{0},a_{0}}[ _{}[_{k=1}^{}^{k-1}r_{k}|s_{k},a_{k}] ],\] (1)

where \(Q(s,a)\) is the Q-value function under policy \(\). The process of obtaining the optimal policy is generally to recover the optimal Q-value function, which maximizes the Q-value function over the whole space \(\), and then to obtain either an implicit policy (Q-Learning algorithm [21; 22; 23]), or a parameterized policy (Actor-Critic algorithm [24; 25; 26; 27; 28]).

The optimal Q-value function \(Q^{*}(s,a)\) can be obtained by minimizing the Bellman residual:

\[_{s,a}[Q(s,a)-Q(s,a)]^{2},\] (2)where \(\) is the Bellman operator defined as

\[Q(s,a):=r(s,a)+_{s^{}}[_{a^{ }}Q(s^{},a^{})].\]

However, the whole paradigm needs to be adjusted in the offline RL setting, as MDP is only determined from a dataset \(\), which is generated by behavior policy \(_{}\). Hence, the state and action space is constraint by the distribution support of \(\). We redefine the MDP in the offline RL setting as: \(_{}=\{_{},_{},_{},r,_{0},\}\), where \(_{}=\{s|s(s^{})\},_{ }=\{a|a(_{})\}\). Then the transition dynamic is determined by \(_{D}:_{}_{} (_{})\). Therefore, the well-known "distribution shift" problem occurs when solving the Bellman equation Eq.2. The Bellman residual is taking expectation in \(_{}_{}\), while the target Q-value is calculated based on the actions from the learning policy.

### Consistency model

The consistency model is an enhanced generative model compared to the diffusion model. The diffusion model gradually adds noise to transform the target distribution into a Gaussian distribution and by estimating the random noise to achieve the reverse process, i.e., sampling a priori sample from a Gaussian distribution and denoise to the target sample iteratively (forms the sample generation trajectory). The consistency model is proposed to ensures each step in a sample generation trajectory of the diffusion process aligns with the target sample (we call consistency). Specifically, the consistency model  try to overcome the slow generation and inconsistency over sampling trajectory generated by the Probability Flow (PF) ODE during training process of the diffusion model [29; 30; 31; 32; 33].

Let \(p_{data}()\) denote the data distribution, we start by diffuse the original data distribution by the PF ODE:

\[d}=[(},t)-(t)^{2}(p _{t}(}))]dt,\] (3)

where \((,)\) is the drift coefficient, \(()\) is the diffusion coefficient, \(p_{t}(})\) is the distribution of \(}\), \(p_{0}() p_{data}()\), and \(\{},t[,T]\}\) is the solution trajectory of the above PF ODE.

Consistency model aims to learn a consistency function \(f_{}(},t)\) that maps each point in the same PF ODE trajectory to its start point, i.e., \(f_{}(},t)=}, t[,T]\). Therefore, \( t,t^{}[,T]\), we have \(f_{}(},t)=f_{}(}},t^{})\), which is the "self-consistency" property of consistency model.

Here, \(f_{}(},t)\) is defined as:

\[f_{}(},t)=c_{skip}(t)}+c_{out}(t)F_{}( },t),\] (4)

where \(c_{skip}(t)\) and \(c_{out}(t)\) are differentiable functions, and \(c_{skip}()=1,c_{out}()=0\) such that they satisfy the boundary condition \(f_{}(},)=x_{}\). In (4), \(F_{}(},t)\) can be free-form deep neural network with output that has the same dimension as \(}\).

Consistency function \(f_{}(},t)\) can be optimized by minimizing the difference of points in the same PF ODE trajectory. If we use a pretrained diffusion model to generate such PF ODE trajectory, then utilize it to train a consistency model, this process is called consistency distillation. We use the consistency distillation method to learn a consistency model in this work and optimize the consistency distillation loss as the Definition 1 in .

With a well-trained consistency model \(f_{}(},t)\), we can generate samples by sampling from the initial Gaussian distribution \(_{T}}(,T^{2})\) and then evaluating the consistency model for \(_{}}=f_{}(_{T}},T)\). This involves only one forward pass through the consistency model and therefore generates samples in a single step. This is the one-step sampling process of the consistency model.

## 3 Q-Distribution guided Q-learning via Consistency Model

In this work, we present a novel method for offline RL called Q-Distribution Guided Q-Learning (QDQ). First, we quantify the uncertainty of Q-values by learning the Q-distribution using the consistency model. Next, we propose a strategy to identify risky actions and penalize their Q-values based on uncertainty estimation, helping to mitigate the associated risks. To tackle the excessive conservatism seen in previous approaches, we introduce uncertainty-aware Q optimization within the Actor-Critic learning framework. This mechanism allows the Q-value function to perform both optimistic and pessimistic optimization, fostering a balanced approach to learning.

### Learn Uncertainty of Q-value by Q-distribution

Estimating the uncertainty of the Q function is a significant challenge, especially with deep neural network Q estimators. A practical indicator of uncertainty is the presence of large variances in the estimates. Techniques such as bootstrapping multiple Q-values and estimating variance  have been used to address this issue. However, these ensemble methods often lack diversity in Q-values  and fail to accurately represent the true Q-value distribution. They may require tens or hundreds of Q-values to improve accuracy, which is computationally inefficient [9; 5].

Other approaches involve estimating the Q-value distribution and determining the lower confidence bound [10; 12; 16], or engaging in in-distribution learning of the Q-value function [3; 13; 16; 10; 34]. However, these methods often struggle to provide precise uncertainty estimations for the Q-value . Stabilization methods can still lead to Q-value overestimation , while inaccurate variance estimation can worsen this problem. Furthermore, even if the Q-value is not overestimated, there is still a risk of it being overly pessimistic or constrained by the performance of the behavior policy when using in-distribution-only training.

In this subsection, we elucidate the process of learning the distribution of Q-values based on the consistency model, and outline the technique for estimating the uncertainty of actions and identifying risky actions. We have give a further demonstration on the performance of the consistency model and efficiency of the uncertainty estimation in Appendix G.2 and Appendix G.3.

**Trajectory-level truncated Q-value.** We chose to estimate the Q-value distribution of the behavior policy instead of the learning policy because they share a similar set of high-uncertainty actions . Using the behavior policy's Q-value distribution has several advantages. First, the behavior policy's Q-value dataset comes from the true dataset, ensuring high-quality distribution learning. In contrast, the learning policy's Q-value is unknown, counterfactually learned, and often noisy and biased, leading to poor data quality and biased distribution learning. Second, using the behavior policy's Q-value distribution to identify high-uncertainty actions does not force the learning policy's target Q-value to align with that of the behavior policy.

To gain insights into the Q-value distribution of the behavior policy, we first need the raw Q-value data. The calculation of the Q-value operates at the trajectory level, represented as \(=(s_{0},a_{0},s_{1},a_{1},...)\), with an infinite horizon (see Eq.1). In the context of offline RL, our training relies on the dataset \(\) produced by the behavior policy. This dataset consists of trajectories generated by the behavior policy, which is the only available trajectory-level data. However, the trajectory-level data from the behavior policy often faces a significant challenge: sparsity. This issue becomes even more pronounced when dealing with low-quality behavior policies, as the generated trajectories tend to be sporadic and do not adequately cover the entire state-action space \(\), especially the high reward region.

To address this pervasive issue of sparsity, as well as the infinite summation in Eq.1, we present a novel approach aimed at enhancing sample efficiency. Our proposed solution involves the utilization of truncated trajectories to ameliorate the sparsity conundrum and avoid infinite summation. By employing a \(k\)- step sliding window of width \(\), we systematically traverse the original trajectories, isolating segments within the window to compute the truncated Q-value (as depicted in Figure A.1). For instance, considering the initiation point of the \(i\)-th step sliding window as \((s_{i},a_{i})\), by setting \(=i+k\), we derive the truncated Q-value of this starting point as follows:

\[Q_{}^{_{g}}(s_{i},a_{i})=_{m=i}^{}^{m-1}r( s_{m},a_{m}) t(s_{m},a_{m}),t(s_{m},a_{m})=0,&terminal,\\ 1,&otherwise.\] (5)

The truncation of Q-values can occur either through sliding window mechanisms or task terminations. When truncation happens due to termination, the Q-value from Eq.5 is equivalent to the true Q-value, \(Q^{_{g}}(,) Q^{_{g}}(,)\). In contrast, if truncation results from window blocking, our theoretical analysis in Theorem 4.1 confirms that the distribution of truncated Q-values has properties similar to those of the true Q-value distribution.

Using a \(k\)-step sliding window does not compromise the consistency of the trajectory, owing to the inherent memory-less Markov property in RL. This strategic truncation allows for the extraction of truncated Q-values, which can improve sample efficiency, especially for long trajectories. Moreover, this approach highlights actions with potential high Q-values, as actions from lengthy trajectories--those with many successful interactions--are encountered more often during Q-distributiontraining. Consequently, the uncertainty of these actions is lower, reducing the likelihood of them being overly pessimistic.

**Learn the distribution of Q-value.** In distributional RL, the learning of Q-value distributions is typically achieved through Gaussian neural networks [35; 36], Gaussian processes [37; 38], or categorical parameterization . However, these methods often suffer from low precision representation of Q-value distributions, particularly in high-dimensional spaces. Moreover, straightforward replacement of true Q-value distributions with ensembles or bootstraps can lead to reduced accuracy in uncertainty estimation(a critical aspect in offline reinforcement learning ), or impose significant computational burdens [8; 7].

The idea of diffusing the original distribution using random noise has rendered the diffusion model a potent and high-fidelity distribution learner. However, it has limitations when estimating uncertainty. Sampling with a diffusion model requires a multi-step forward diffusion process to ensure sample quality. Unfortunately, this iterative process can compromise the accuracy of uncertainty estimates by introducing significant fluctuations and noise into the Q-value uncertainty. For a detailed discussion, see Appendix A.2.

To address this issue, we suggest using the consistency model  to learn the Q-value distribution. The consistency model allows for one-step sampling, like other generative models, which reduces the randomness found in the multi-step sampling of diffusion models. This results in a more robust uncertainty estimation. Furthermore, the consistency feature, as explained in Theorem 4.2, accurately captures how changes in actions affect the variance of the final bootstrap samples, making Q-value uncertainty more sensitive to out-of-distribution (OOD) actions compared to the diffusion model. Additionally, the fast-sampling process of the consistency model improves QDQ's efficiency. While there may be some quality loss in restoring real samples, this is negligible for QDQ since it only calculates uncertainty based on the variance of the bootstrap samples, not the absolute Q-value of the sampled samples. Overall, the consistency model is an ideal distribution learner for uncertainty estimation due to its reliability, high-fidelity, ease of training, and faster sampling.

Once we derive the truncated Q dataset \(_{Q}\), we train a conditional consistency model, denoted by \(f_{}(x_{T},T|(s,a))\), which approximates the distribution of Q-values. Since the consistency model aligns with one-step sampling, we can easily sample multiple Q-values for each action using the consistency model. Suppose we draw \(n\) prior noise \(\{_{T_{1}},_{T_{2}},,_{T_{n}}\}\) from the initial noise distribution \((0,T^{2})\), and denoise the prior samples by the consistency one-step forward process: \(_{_{i}}=f_{}(_{T_{i}},T_{i}|(s,a)),i=1,2,,n\). Then the variance of these Q-values, derived by

\[V(X_{}|(s,a))=_{i=1}^{n}[f_{}(_{T_ {i}},T|(s,a))-_{i=1}^{n}f_{}(_{T_{i}},T|(s,a)) ]^{2},\] (6)

can be used to gauge the uncertainty of \(Q(s,a)\).

### Q-distribution guided optimization in offline RL

**Recover Q-value function.** We propose an uncertainty-aware optimization objective \(_{uw}(Q)\) to penalize Q-value for OOD actions as well as to avoid too conservative Q-value learning for in-distribution areas. The uncertainty-aware learning objective for Q-value function \(Q_{}(s,a)\) is :

\[_{uw}(Q_{})=_{}\{(Q_{})_{H} +(1-)(Q_{})_{L}\}.\] (7)

In Eq.7, \((Q_{})H\) represents the classic Bellman residual defined in Eq.2. This residual is used in online RL and encourages optimistic optimization of the Q-value. In contrast, \((Q)_{L}\) is a pessimistic Bellman residual based on the uncertainty-penalized Q target \(Q_{L}(s^{},a^{})\), defined as

\[Q_{L}(s^{},a^{})=_{Q}(a^{}|s^{})}Q _{}(s^{},a^{})1_{(a^{}(Q))}+ Q_{ }(s^{},a^{})1_{(a^{}(Q))}.\] (8)

In Eq.8, \(_{Q}(a^{}|s^{})=|(s^{},a^{ }))}\) represents the uncertainty estimate of the Q-value for action \(a^{}\). The set \((Q)\) includes actions that may be out-of-distribution (OOD). We use the upper \(\)-quantile \(_{Q}^{}(a^{}|s^{})\) of the uncertainty estimate on actions taken by the learning policy as the threshold for forming \((Q)\). Additionally, we incorporate the quantile parameter \(\) as a robust weighting factor for the unpenalized Q-target value. This helps control the estimation error of uncertainty and enhances the robustness of the learning objective. We can also set a free weighting factor, but we use \(\) to reduce the number of hyperparameters.

**Improve the learning policy.** The optimization of learning policy follows the classic online RL paradigm:

\[_{}()=_{}~{}[_{s_{ }(s),a_{}(|s)}[Q_{}(s,a)]+_{a }[_{}(a)]].\] (9)

In Eq.9, an entropy term is introduced to further stabilize the volatile learning process of Q-value function. For datasets with a wide distribution, we can simply set the penalization factor \(\) to zero, which can further enhance performance. Furthermore, other policy learning objectives, such as the AWR policy objective , can also be flexibly used within the QDQ framework, especially for the goal conditioned task like Antmaze.

We outline the entire learning process of QDQ in Algorithm 1. In Section 4, Theorems 4.3 and 4.4 show that QDQ penalizes the OOD region based on uncertainty while ensuring that the Q-value function in the in-distribution region is close to the optimal Q-value. This alignment is the main goal of offline RL.

``` Initialize: target network update rate \(\), uncertainty-aware learning hyperparameter \(,\),policy training hyperparameters \(\). Consistency model \(f_{}\), Q networks \(\{Q_{_{1}},Q_{_{2}}\}\), actor \(_{}\), target networks \(\{Q_{_{1}^{}},Q_{_{2}^{}}\}\), target actor \(_{^{}}\). Q-distribution learning:  Calculate Q dataset \(_{Q}=\{Q_{}^{_{}}(s,a)\}\) scanning each trajectory \(\) by Eq.5. for each gradient step do  Sample minibatch of \(Q_{}^{_{}}(s,a)_{Q}\)  Update \(\) minimizing consistency distillation loss in Eq.(7)  endfor for each gradient step do  Sample mini-batch of transitions \((s,a,r,s^{})\)  Updating Q-function:  Update \(=(_{1},_{2})\) minimizing \(_{uw}(Q_{})\) in Eq.7  Updating policy:  Update \(\) minimizing \(_{}()\) in Eq.9  Update Target Networks: \(^{}+(1-)^{};_{i}^{} _{i}+(1-)_{i}^{},i=1,2\) endfor ```

**Algorithm 1** Q-Distribution guided Q-learning (QDQ)

## 4 Theoretical Analysis

In this section, we provide a theoretical analysis of QDQ. The first theorem states that if \(\) is sufficiently large, the distribution of \(Q^{_{}}\) does not significantly differ from the true distribution of \(Q^{_{}}\). This shows that our sliding window-based truncated Q-value distribution converges to the true Q-value distribution, ensuring accurate uncertainty estimation. A detailed proof can be found in Appendix B.

**Theorem 4.1** (Informal).: _Under some mildly condition, the truncated Q-value \(Q_{}^{_{}}\) converge distribution to the true true Q-value \(Q^{_{}}\)._

\[F_{Q_{}^{_{}}}(x) F_{Q^{_{}}}(x), +.\] (10)

In Theorem 4.2, we analyze why the consistency model is suitable for estimating uncertainty. Our analysis shows that Q-value uncertainty is more sensitive to actions. This sensitivity helps in detecting out-of-distribution (OOD) actions. A detailed statement of the theorem and its proof can be found in Appendix C.

**Theorem 4.2** (Informal).: _Following the assumptions as in , \(f_{}(x,T|(s,a))\) is \(L\)-Lipschitz. We also assume the truncated Q-value is bounded by \(\). The action \(a\) broadly influences \(V(X_{}|(s,a))\) by: \(|)}{ a}|=O(L^{2}T)\)._In Theorem 4.3, we give theoretical analysis that the uncertainty-aware learning objective in Eq.7 can converge and the details can be found in Appendix D.

**Theorem 4.3** (Informal).: _The Q-value function of QDQ can converge to a fixed point of the Bellman equation: \(Q(s,a)=Q(s,a)\), where the Bellman operator \(Q(s,a)\) is defined as:_

\[Q(s,a):=r(s,a)+_{s^{} P_{}(s^{ })}\{_{a^{}}[ Q(s^{},a^{})+(1-)Q_{L}( s^{},a^{})]\}.\] (11)

Theorem 4.4 shows that QDQ penalizes the OOD region by uncertainty while ensuring that the Q-value function in the in-distribution region is close to the optimal Q-value, which is the goal of offline RL.

**Theorem 4.4** (Informal).: _Under mild conditions, with probability \(1-\) we have_

\[\|Q^{}-Q^{*}\|_{},\] (12)

_where \(Q^{}\) is learned by the uncertainty-aware loss in Eq.7, \(\) is error rate related to the difference between the classical Bellman operator \(Q\) and the QDQ bellman operator \(Q\)._

The optimal Q-value, \(Q^{}\), derived by the QDQ algorithm can closely approximate the optimal Q-value function, \(Q^{*}\), benefiting from the balanced approach of the QDQ algorithm that avoids excessive pessimism for in-distribution areas. Both the value \(\) and \(\) are small and more details in Appendix E.

## 5 Experiments

In this section, we first delve into the experimental performance of QDQ using the D4RL benchmarks . Subsequently, we conduct a concise analysis of parameter settings, focusing on hyperparameter tuning across various tasks. For detailed implementation, we refer to Appendix G.

### Performance on D4RL benchmarks for Offline RL

We evaluate the proposed QDQ algorithm on the D4RL Gym-MuJoCo and AntMaze tasks. We compare it with several strong state-of-the-art (SOTA) model-free methods: behavioral cloning (BC), BCQ , DT , AWAC , Onestep RL , TD3+BC , CQL , and IQL . We also include UWAC , EDAC , and PBRL , which use uncertainty to pessimistically adjust the Q-value function, as well as MCQ , which introduces mild constraints to the Q-value function. The experimental results for the baselines reported in this paper are derived from the original experiments conducted by the authors or from replication of their official code. The reported values are normalized scores defined in D4RL .

Table 1 shows the performance comparison between QDQ and the baselines across Gym-MuJoCo tasks, highlighting QDQ's competitive edge in almost all tasks. Notably, QDQ excels on datasets with wide distributions, such as medium and medium-replay datasets. In th

   Dataset & BC & AWAC & DT & TD3+BC & CQL & IQL & UWAC & MCQ & EDAC & PBRL & QDQ(Ours) \\  ha-med & 42.6 & 43.5 & 42.6 & 48.3 & 44.0 & 47.4 & 42.2 & 64.3 & 65.9 & 57.9 & **74.1\(\)1.7** \\ ho-med & 52.9 & 57.0 & 67.6 & 59.3 & 58.5 & 66.2 & 50.9 & 78.4 & **101.6** & 75.3 & **99.0\(\)0.3** \\ wa-med & 75.3 & 72.4 & 74.0 & 83.7 & 72.5 & 78.3 & 75.4 & 91.0 & **92.5** & 89.6 & 86.9\(\)0.08 \\ ha-med-r & 36.6 & 40.5 & 36.6 & 44.6 & 45.5 & 44.2 & 35.9 & 56.8 & 61.3 & 45.1 & **63.7\(\)2.9** \\ ho-med-r & 18.1 & 37.2 & 82.7 & 60.9 & 95.0 & 94.7 & 25.3 & 101.6 & 101.0 & 100.6 & **102.4\(\)0.28** \\ wa-med-r & 26.0 & 27.0 & 66.6 & 81.8 & 77.2 & 73.8 & 23.6 & 91.3 & 87.1 & 77.7 & **93.2\(\)1.1** \\ ha-med-e & 55.2 & 42.8 & 86.8 & 90.7 & 91.6 & 86.7 & 42.7 & 87.5 & **106.3** & 92.3 & 99.3\(\)1.7 \\ ho-med-e & 52.5 & 55.8 & 107.6 & 98.0 & 105.4 & 91.5 & 44.9 & 112.3 & 110.7 & 110.8 & **113.5\(\)3.5** \\ wa-med-e & 107.5 & 74.5 & 108.1 & 110.1 & 108.8 & 109.6 & 96.5 & 114.2 & 114.7 & 110.1 & **115.9\(\)0.2** \\  Total & 466.7 & 450.7 & 672.6 & 684.6 & 677.4 & 698.5 & 437.4 & 797.4 & 841.1 & 759.4 & **848.0\(\)11.8** \\   

Table 1: Comparison of QDQ and the other baselines on the three Gym-MuJoCo tasks. All the experiment are performed on the MuJoCo “-v2” dataset. The results are calculated over 5 random seeds.med = medium, r = replay, e = expert, ha = halfcheetah, wa = walker2d, ho=hopperavoids the problem of over-penalizing Q-values. By balancing between being too conservative and actively exploring to find the optimal Q-value function through dynamic programming, QDQ gradually converges toward the optimal Q-value, as supported by Theorem 4.4.

Table 2 presents the performance comparison between QDQ and selected baselines3 across AntMaze tasks, highlighting QDQ's commendable performance. While QDQ focuses on reducing overly pessimistic estimations, it does not compromise its performance on narrow datasets. This is evident in its competitive results on the medium-expert dataset in Table 1, as well as its performance on AntMaze tasks. Notably, QDQ outperforms SOTA methods on several datasets. This success is due to the inherent flexibility of the QDQ algorithm. By allowing for flexible hyperparameter control and seamless integration with various policy optimization methods, QDQ achieves a synergistic performance enhancement.

### Parameter analysis

**The uncertainty-aware loss parameter \(\).** The parameter \(\) is crucial for balancing the dominance between optimistic and pessimistic updates of the Q-value (Eq.7). A higher \(\) value skews updates toward the optimistic side, and we choose a higher \(\) when the dataset or task is expected to be highly robust. However, the setting of \(\) is also influenced by the pessimism of the Q target defined in Eq.8. For a more pessimistic Q target value, we can choose a larger \(\). Interestingly, both the theoretical analyses in Theorem 4.3 and Theorem 4.4 and empirical parameter tuning suggest that variability in \(\) across tasks is minimal, with a typical value around 0.95.

**The uncertainty related parameter \(\).** The parameter \(\) influences both the partitioning of high uncertainty sets and acts as a relaxation variable to control uncertainty estimation errors. When dealing with a narrow action space or a sensitive task (such as the hopper task), the value of \(\) should be smaller. In these cases, the Q-value is more likely to select OOD actions, increasing the risk of overestimation. This means we face greater uncertainty (Eq.8) and need to minimize overestimation errors. Therefore, we require stricter criteria to ensure actions are in-distribution and penalize the Q-values of OOD points more heavily. A detailed analysis of how to determine the value of \(\) can be found in Appendix G.3.

**The entropy parameter \(\).** The \(\) term in Eq.9 stabilizes the learning of a simple Gaussian policy, especially for action-sensitive and narrower distribution tasks. When the dataset has a wide distribution or the task shows high robustness to actions (such as in the half-cheetah task), the Q-value function generalizes better across the action space. In these cases, we can set a more lenient requirement for actions, keeping the value of \(\) as small as possible or even at 0. However, when the dataset is narrow (e.g., in the AntMaze task) or when the task is sensitive to changes in actions (like in the hopper or maze tasks, where small deviations can lead to failure), a larger value of \(\) is necessary. For these tasks, a simple Gaussian policy can easily sample risky actions, as it fits a single-mode policy. Nonetheless, experimental results indicate that the sensitivity of the \(\) parameter is not very high. In fact, \(\) in Eq.9 is relatively small compared to the Q-value, primarily to stabilize training and prevent instability in Gaussian policy action sampling. See Appendix G.7 for more details.

   Dataset & BC & TD3+BC & DT & Onestep RL & AWAC & CQL & IQL & QDQ(Ours) \\  umaze & 54.6 & 78.6 & 59.2 & 64.3 & 56.7 & 74.0 & 87.5 & **98.6\(\)2.8** \\ umaze-diverse & 45.6 & 71.4 & 53.0 & 60.7 & 49.3 & **84.0** & 62.2 & 67.8\(\)2.5 \\ medium-play & 0.0 & 10.6 & 0.0 & 0.3 & 0.0 & 61.2 & 71.2 & **81.5\(\)3.6** \\ medium-diverse & 0.0 & 3.0 & 0.0 & 0.0 & 0.7 & 53.7 & 70.0 & **85.4\(\)4.2** \\ large-play & 0.0 & 0.2 & 0.0 & 0.0 & 0.0 & 15.8 & **39.6** & 35.6\(\)5.4 \\ large-diverse & 0.0 & 0.0 & 0.0 & 0.0 & 1.0 & 14.9 & **47.5** & 31.2\(\)4.5 \\  Total & 100.2 & 163.8 & 112.2 & 125.3 & 142.4 & 229.8 & 378 & **400.1\(\)23.0** \\   

Table 2: Comparison of QDQ and the other baselines on the Antmaze tasks. All the experiment are performed on the Antmaze “-v0” dataset for the comparison comfortable with previous baseline. The results are calculated over 5 random seeds.

Related Works

**Restrict policy deviate from OOD areas**. The distribution mismatch between the behavior policy and the learning policy can be overcome if the learning policy share the same support with the behavior policy. One approach involves explicit distribution matching constraints, where the learning policy is encouraged to align with the behavior policy by minimizing the distance between their distributions. This includes techniques based on KL-divergence [47; 48; 40; 44; 46], Jensen-Shannon divergence , and Wasserstein distance [47; 49]. Another line of research aims to alleviate the overly conservative nature of distribution matching constraints by incorporating distribution support constraints. These methods employ techniques such as Maximum Mean Discrepancy (MMD) distance , learning behavior density functions using implicit  or explicit  methods, or measuring the geometric distance between actions generated by the learning and behavior policies .In addition to explicit constraint methods, implicit constraints can also be implemented by learning a behavior policy sampler using techniques like Conditional Variational Autoencoders (CVAE) [42; 53; 50; 54], Autoregressive Generative Model , Generative Adversarial Networks (GAN) , normalized flow models , or diffusion models [57; 58; 11; 59; 11; 60].

**Pessimistic Q-value optimization**. Pessimistic Q-value methods offer a direct approach to address the issue of Q-value function overestimation, particularly when policy control fails despite the learning policy closely matching the behavior policy . A promising approach to pessimistic Q-value estimation involves estimating uncertainty over the action space, as OOD actions typically exhibit high uncertainty. However, accurately quantifying uncertainty poses a challenge, especially with high-capacity function approximators like neural networks . Techniques such as ensemble or bootstrap methods have been employed to estimate multiple Q-values, providing a proxy for uncertainty through Q-value variance [7; 9; 14], importance ratio [61; 62] or approximate Lower Confidence Bounds (LCB) for OOD regions [8; 9]. Other methods focus on estimating the LCB of Q-values through quantile regression [63; 34], expectile regression [10; 11], or tail risk measurement such as Conditional Value at Risk (cVAR) . Alternatively, some approaches seek to pessimistically estimate Q-values based on the behavior policy, aiming to underestimate Q-values under the learning policy distribution while maximizing Q-values under the behavior policy distribution [3; 13; 64; 65]. Another category of Q-value constraint methods involves learning Q-values only within the in-sample [16; 17; 18; 19], capturing only in-sample patterns and avoid OOD risk. Furthermore, Q-value functions can be replaced by safe planning methods used in model-based RL, such as planning with diffusion models  or trajectory-level prediction using Transformers . However, ensemble estimation of uncertainty may tend to underestimate true uncertainty, while quantile estimation methods are sensitive to Q-distribution recovery. In-sample methods may also be limited by the performance of the behavior policy.

## 7 Conclusion

We introduce QDQ, a novel framework rendering pessimistic Q-value in OOD areas by uncertainty estimation. Our approach leverages the consistency model to robustly estimate the uncertainty of Q-values. By employing this uncertainty information, QDQ can apply a judicious penalty to Q-values, mitigating the overly conservative nature encountered in previous pessimistic Q-value methods. Additionally, to enhance optimistic Q-learning within in-distribution areas, we introduce an uncertainty-aware learning objective for Q optimization. Both theoretical analyses and experimental evaluations demonstrate the effectiveness of QDQ. Several avenues for future research exist, including embedding QDQ into goal-conditioned tasks, enhancing exploration in online RL by efficient uncertainty estimation. We hope our work will inspire further advancements in offline reinforcement learning.