# HEST-1k: A Dataset for Spatial Transcriptomics and Histology Image Analysis

Guillaume Jaume\({}^{1,2,}\)1  Paul Doucet\({}^{1,3,}\)1  Andrew H. Song\({}^{1,2}\)  Ming Y. Lu\({}^{1,2,4}\)

Cristina Almagro-Perez\({}^{1,2}\)  Sophia J. Wagner\({}^{1,6,7}\)  Anurag J. Vaidya\({}^{1,2,5}\)

Richard J. Chen\({}^{1,2}\)  Drew F.K. Williamson\({}^{8}\)  Ahrong Kim\({}^{1,9}\)  Faisal Mahmood\({}^{1,2}\)

\({}^{1}\)Mass General Brigham, Boston, USA \({}^{2}\)Harvard Medical School, Boston, USA \({}^{3}\)ETH Zurich, Switzerland \({}^{4}\)EECS MIT, Cambridge, USA \({}^{5}\)HST MIT, Cambridge, USA \({}^{6}\)TUM, Munich, Germany \({}^{7}\)Helmholtz Munich, Munich, Germany \({}^{8}\)Emory School of Medicine, Atlanta, USA \({}^{9}\)Pusan National University, South Korea gjaume@bwh.harvard.edu  faisalmahmood@bwh.harvard.edu

Equal contribution

###### Abstract

Spatial transcriptomics enables interrogating the molecular composition of tissue with ever-increasing resolution and sensitivity. However, costs, rapidly evolving technology, and lack of standards have constrained computational methods in ST to narrow tasks and small cohorts. In addition, the underlying tissue morphology, as reflected by H&E-stained whole slide images (WSIs), encodes rich information often overlooked in ST studies. Here, we introduce HEST-1k, a collection of 1,229 spatial transcriptomic profiles, each linked to a WSI and extensive metadata. HEST-1k was assembled from 153 public and internal cohorts encompassing 26 organs, two species (_Homo Sapiens_ and _Mus Musculus_), and 367 cancer samples from 25 cancer types. HEST-1k processing enabled the identification of 2.1 million expression-morphology pairs and over 76 million nuclei. To support its development, we additionally introduce the HEST-Library, a Python package designed to perform a range of actions with HEST samples. We test HEST-1k and Library on three use cases: (1) benchmarking foundation models for pathology (HEST-Benchmark), (2) biomarker exploration, and (3) multimodal representation learning. HEST-1k, HEST-Library, and HEST-Benchmark can be freely accessed at https://github.com/mahmoodlab/hest.

## 1 Introduction

Advances in molecular profiling enable spatially-resolved gene expression analysis with increasingly large gene panels, enhanced spatial resolution, and greater sensitivity . From the early days of bulk RNA sequencing constrained by its coarse resolution and limited gene panels, spatially-resolved technologies have progressed to achieve whole-transcriptome sequencing at sub-cellular resolution . In cancer research, spatial transcriptomics (ST) holds particular promise for characterizing the tumor microenvironment, a key element in understanding disease progression and treatment response . With the large amount of transcriptomics data generated by a single ST sample (e.g., \(>\)10 million transcripts are detected in a typical 10x Genomics Xenium assay), computational methods are often used to uncover promising biomarkers, such as employing clustering methods for cell phenotyping .

However, high costs and rapidly evolving technology have constrained computational methods to narrow tasks and data cohorts of only a few patients . Consequently, we observe a lackof standardized resources and unified formats for handling ST, which limits the development of deep learning models on a large scale . In addition, the underlying tissue morphology, traditionally visualized in hematoxylin and eosin (H&E)-stained tissue sections (whole-slide images, WSIs), is often overlooked in ST studies, despite encoding valuable information. In particular, pairs of ST and WSI enable analyzing expression changes in their morphological context, which may facilitate the identification of morphological biomarkers (e.g., changes in nuclear shape) that correspond to gene regulation patterns. Alternatively, pairs of ST and WSI can enable multimodal tissue representation learning for joint modeling of the morphomolecular signature of tissue at a scale and resolution beyond bulk RNA sequencing . Finally, the development of "foundation models" for encoding histopathology images [147; 42; 29; 59; 88] has increased the need for new, diverse, and challenging benchmarks beyond diagnostic tasks. Using ST, new tasks can be defined to predict gene expression changes from histology.

Here, we introduce HEST-1k, a collection of paired ST and H&E-stained WSIs curated from public and internal cohorts (Figure 1.a). HEST-1k comprises 1,229 samples from 153 cohorts encompassing 26 organs, two species (_Homo Sapiens_ and _Mus Musculus_), and 367 cancer samples from 25 different subtypes. Processing all samples in HEST-1k resulted in 2.1 million expression-morphology pairs and 76 million detected nuclei. With new cohorts frequently made public, we also introduce the HEST-Library, a Python package for interacting with HEST-1k data and assembling new samples as they become available (Figure 1.b). We highlight the potential of HEST-1k through three use cases: (i) benchmarking foundation models for histology using the HEST-Benchmark, a set of nine tasks (eight human cancer types and nine organs) for gene expression prediction from histology and evaluated on eleven state-of-the-art models (Figure 1.c), (ii) a proof-of-concept demonstrating the use of HEST-1k for biomarker characterization (Figure 1.d), and (iii) a proof-of-concept for expression-guided fine-tuning of foundation models for histology (Figure 1.e).

Figure 1: **The HEST environment.****a.** Overview of HEST-1k, a dataset of \(n\)=1,229 paired spatial transcriptomics, H&E-stained whole-slide images and metadata. “Pathological” cases refer to non-tumor/non-cancer samples; “Tumor” refers to non-cancer samples. **b.** Overview of HEST-Library functionalities. **c., d., e.** Applications of HEST-1k include benchmarking foundation models for histology (**c.**), biomarker exploration (**d.**) and multimodal representation learning (**e.**).

Related work

**Libraries for ST analysis.** Libraries to process, visualize, and analyze ST have been built around two core pipelines: _Scanpy_ (and the Anndata format) in Python and _Seurat_ in R. _Scanpy_ has served as the foundation for several subsequent developments such as _Squidpy_ for spatial data exploration at cellular-, gene-, and morphological-level, _SpatialData_ for multi-technology integration and deep learning interfacing, _STlearn_ for cell-cell interactions and spatiotemporal trajectory analyses, and SOPA  for designing multistep pipelines. In R, _Seurat_ has been consolidated with packages such as _BayeSpace_ for clustering and spot super-resolution, and _Giotto suites_ for preprocessing, data integration and visualization of multiple ST technologies. 10x Genomics also includes proprietary software analytics through the Xenium and Visium Explorer pipelines for multimodal visualization, nuclear segmentation, and cell deconvolution. However, none of these pipelines were designed to handle the diversity of legacy data, where datasets can suffer from missing or incorrect data, such as alignment mismatches, incorrect pixel resolution, inconsistent image file formats, etc.

**Molecular profile prediction from H&E.** Molecular profiling from histology images has been explored both at (1) _slide-level_ to predict bulk molecular status/changes from a WSI and at (2) _patch-level_ to predict local molecular status/changes from regions-of-interest. (1) Slide-level profiling has been explored to predict the gene mutations , microsatellite instability , and gene expression changes , among others. The motivation is two-fold: First, patient screening to substitute or complement costly clinical molecular assays, and second, to identify morphological correlates of molecular alterations for discovering novel biomarkers. Such studies can be conducted on large patient cohorts as they mainly rely on data generated by the routine clinical workflow (e.g., using TCGA cohorts with >11,000 cases from 33 cancer types). (2) With ST, several works have explored predicting expression changes from regions-of-interest . Due to limited cohort sizes (typically one to ten patients), transfer learning has become the norm using pretrained models based on ConvNets  or Vision Transformers . Due to the inherent noise found in transcriptomic measurements, several methods have been developed for integrating context that can account for global and local information from surrounding ST spots . While recent technologies offer near-single-cell resolution (such as Visium HD and Xenium), legacy assays operate at a more coarse resolution, which can be upsampled using super-resolution techniques . The potential clinical and research implications of such methods are still being explored, with HEST-1k potentially catalyzing their large-scale development.

**Foundation models in pathology.** A fundamental task in computational pathology is to extract _general-purpose_ embeddings of image patches (typically 256\(\)256 to 512\(\)512-pixel regions) that can then be used for downstream tasks, such as diagnosis or prognosis prediction. To achieve this, self-supervised learning (SSL) has been extensively applied , such as based on the DINOv2 framework . General-purpose patch encoders are trained on increasingly large and diverse patient cohorts (e.g., UNI  uses a ViT-Large trained on 100k WSIs, Virchow  uses a ViT-Huge trained on more than 1.5M WSIs). Recently, vision-language encoders designed for pathology have also been proposed  and rely on large-scale paired data scraped from social media, textbooks, or publications. As the number of such models rapidly increases, new, diverse, and challenging benchmarks are needed to replace or complement well-established tasks where performance has saturated. HEST-Benchmark aims to address this by offering a set of nine patch-level tasks for gene expression prediction from histology.

**Patch-level benchmarks in histopathology.** Early task and dataset contributions in computational pathology revolved around classifying small regions of interest. Over the years, a variety of benchmarks have been established: In prostate cancer, Gleason grading at pixel- and patch-level has been widely explored, with public resources such as AGGC , DiagSet , and SICAPv2 . In colorectal cancer, datasets have been proposed for tissue classification, such as HunCRC , UniToPatho , MHIST , and CRC-100k . In breast cancer, morphological subtyping has been vastly explored (e.g., for atypical ductal hyperplasia detection), such as BACH , BRACS , and BreakHis , and for lymph node metastasis detection with Patch CAMELYON (pCAM) , respectively. However, the performance on many of these datasets has saturated; for instance, Gleason scoring reaches similar or better performance than pathologists , which limits objective comparisons of new methods and hinders well-informed model selection for developing better features.

Instead, HEST-Benchmark provides a collection of diverse and challenging tasks that enable assessing the predictive capabilities of foundation models for histology.

## 3 HEST-1k Dataset

We present HEST-1k, a dataset of paired ST, H&E-stained WSIs, and metadata (Figure 1.a). To this end, we extracted all publicly available cohorts that provide ST with H&E-stained whole-slide images. Specifically, we harvested data from 10x Genomics public datasets (TENX)1, Mendeley (MEND)2, Spatial-Research (SPA)3, Zenodo (ZEN)4, the National Center for Biotechnology Information (NCBI)5, GitHub6, the Human Cell Atlas7, BioStudies8, HTAN9, and internal data cohorts. A summary of all sources is provided in Appendix Table A1 with specifics in Appendix Table A2, A4, A6,A7,A8,A9, and A10.

### Metadata

As spatial transcriptomics experiments were not intended for large-scale computational research, they are provided in various formats (e.g., images can be in JPG or TIFF format, with or without cross-modal alignment files) and resolutions. We unified all data with comprehensive metadata with _generic-_, _histology-_, and _expression-_related descriptors for all samples. **Generic:** We provide the reference to the original publication, download link, year of publication, license, and sample species. Each sample is then categorized as either healthy, cancer, tumor (non-cancer), treated (which refers to a post-compound administration), genetically modified (mostly knock-out mouse samples), or pathological (i.e., non-tumorous with extra specification). All cancer samples were unified using the OncoTree code, a taxonomy of cancer types provided by the Memorial Sloan Kettering Cancer Center10. Finally, we provide the organ using the highest level of the OncoTree taxonomy as a reference. **Expression:** We report the number of genes and spots per sample, the spot resolution and spacing, the total number of reads, and the mean number of reads per spot. We additionally provide the transcriptomic technology (ST, Visum, Visium HD, or Xenium). **Histology:** We provide the image resolution (in \(\)m/pixel) and magnification as 10\(\) (1.15 to 0.8 \(\)m/px), 20\(\) (0.8 to 0.4 \(\)m/px) and 40\(\) (0.4 to 0.1 \(\)m/px). All images with a pixel size higher than 1.15 \(\)m/px were discarded to ensure an acceptable image quality. In addition, we provide the image size at the highest resolution and the tissue preparation protocol (frozen or formalin-fixed paraffin-embedded, FFPE).

### Histology

All tissue sections were normalized and transformed into a generic TIFF object, a pyramidal image that can easily be integrated into computational frameworks using Openslide or viewers such as QuPath. In addition, we provide a contour object that delineates all the tissue regions identified in the image. We developed a robust tissue _vs._ background detection method where we fine-tuned a DeepLabV3  model with an ImageNet-pretrained ResNet50 backbone on a set of annotated segmentation regions (including pen marks, fiducials, multiple stains, artifacts, etc.). From the tissue segmentation, we extracted 224\(\)224-pixel patches at 20\(\) magnification around each spot. For Xenium samples, we generated "pseudo-Visium" spots by pooling transcripts on 55 \(\) 55-\(\)m patches without spacing. This yielded 2.1 million valid patches for which a corresponding expression profile was derived. Such patching can readily be used for various downstream tasks, such as employed in the **HEST-Benchmark** or multimodal fine-tuning of foundation models for histology (Section 5 and 7).

### Nuclear segmentation and classification

In addition to patching, we include nuclear segmentation that delineates each nucleus identified in all slides from HEST-1k. We used CellViT , a state-of-the-art nuclear segmentation model that was trained on the PanNuke dataset . CellViT enables joint instance segmentation and classification of each nucleus into five classes: neoplastic epithelial, non-neoplastic epithelial, inflammatory, stromal, and necrotic. On average, we identified 62.1k nuclei per slide, for a total of 76.4 million nuclei identified across all samples. Among those, 17.6 million are classified as neoplastic, 21.5 million as stromal, 4.9 million as normal epithelial, 15.4 million as inflammatory, and 76 thousand as necrotic. The resulting nuclear segmentation and classification can easily be visualized using QuPath (using geoyion) or loaded as Python/R objects (using JSON). For all Xenium samples, we additionally provide the nuclear and cell segmentation derived from the DAPI staining finely aligned with the H&E slide.

### Gene expression

All expression data were unified in a Anndata object that can be loaded with _scampy_. Anndata encodes the gene names (as _var_) and number of spots (as _obs_). Each entry represents the raw transcript counts of a gene in a given spot. No additional normalization was conducted, and we let users explore various normalization strategies based on needs, e.g., using total count normalization, log-normalization, etc. In addition, we include metadata to specify the number of genes, the gene panel, and the tissue site. For all Xenium samples, we also provide the list of all measured transcripts with their exact 2D position in the tissue (aligned with the H&E slide).

To use the expression in tandem with the WSI, an alignment file describing the mapping between the image and the spots is needed. However, relying on publicly available alignment information brings three challenges: (1) most datasets report alignment with respect to a low-resolution version of the image, (2) they are not standardized, and (3) alignment quality can be low. To address these limitations, we re-aligned all samples under the same unified format between the WSI and the corresponding expression profile. For all Visium samples, we developed an automatic alignment pipeline based on fiducial detection (see Section 4) and embedded the alignment in the _scampy_ object. For all Xenium samples, we used the publicly available VALIS  pipeline for fine-grained image registration to align the DAPI image (aligned with the transcripts by design) and the H&E slide.

## 4 HEST-Library

The HEST-Library is built around _scampy_ and Anndata. At its core, the HEST-Library enables (1) assembling and querying HEST-1k, (2) visualizing and mitigating batch effects, and (3) running the HEST-Benchmark (Section 5). We describe its core functionalities, particularly for unifying legacy data.

**Conversion to generic TIFF.** We integrate functions to convert a WSI from common formats found in public ST datasets (e.g., OME.TIF, JPG, BigTIFF, etc.) to a pyramidal generic TIFF format. Pyramidal formats offer seamless integration with OpenSlide (commonly used in computational pathology pipelines) and QuPath (open-access software for WSI visualization and annotation).

**Automatic alignment in Visium.** Spot alignment is crucial to ensure an accurate match between the ST spots and the WSI. While software such as LoupeBrowser enables manual alignment using fiducials (i.e., reference markers placed at the corners of the capture area), the process remains time-consuming when processing large batches of samples. Instead, we implemented an automatic fiducial detection algorithm based on YOLOv8  for processing Visium samples (Appendix Figure 5). Specifically, we manually annotated 119 fiducial regions that we further augmented using tissue and fiducial mixing. We then fine-tuned YOLOv8 pretrained on the COCO dataset. In early versions that do not provide corner fiducials (e.g., STv1), we realigned using the provided spot position files. In Xenium, we use VALIS  to register the DAPI staining (aligned with the transcripts) with the H&E image.

**Automatic detection of image resolution.** From the alignment and the spot resolution, we can infer the exact pixel size. To this end, we compute the distance in pixel between two neighboring spots and leverage the known inter-spot distance in \(\)m to estimate the pixel width in \(\)m/px. For Xenium samples, we use the H&E alignment file provided as part of the assay, which provides an affine transformation from the DAPI-stained image (with known pixel size) to the H&E image. We then compared the self-reported image resolution and our re-estimations to manually inspect and correct discrepancies.

**Conversion to Anndata.** ST data is provided in multiple formats, such as CSV, MEX, TXT, h5, etc. We provide functions to unify a large set of existing formats into a Anndata object that stores the raw transcript counts as a matrix of genes by the number of spots, in addition to metadata about the samples (e.g., the (x,y) coordinates of each spot, the pixel resolution, etc.).

**Tissue segmentation and patching.** We provide a tissue segmentation pipeline optimized for Visium/Xenium images. The segmentation can then automatically tessellate the tissue into fixed-size image patches at a predefined resolution (expressed in \(\)m/px) around each spot.

**Automatic HEST-1k download.** To facilitate downloading part or all of the HEST-1k dataset (over \(>\)1TB), we implemented an easy download option where the user can specify entries of the metadata, for instance, to query all human invasive breast cancer cases.

**Batch effect visualization and mitigation.** We provide functions to help visualize batch effects using dimensionality reduction techniques with user-prompted stratification (e.g., tissue site, institution, disease, etc.). In addition, we provide a wrapper of well-established batch effect mitigation strategies (namely ComBat , Harmony  and matching mutual nearest neighbors ), which can be applied to a list of HEST samples.

## 5 HEST-Benchmark

From HEST-1k, we curated the HEST-Benchmark, a set of nine tasks for gene expression prediction from histology in human cancer samples. The goal is two-fold: (i) benchmarking foundation models for histology under a diverse and challenging benchmark and (ii) understanding the predictive capabilities of state-of-the-art models in predicting expression from morphology. Compared to existing tasks (e.g., Camelyon16 ), the HEST-Benchmark brings increased morphological diversity and more complex challenges, particularly with the inherent difficulty of expression prediction.

### Task definition

We define nine tasks with data from eight human cancers and nine organs (eight primary and one metastatic dataset), which include **invasive ductal carcinoma** (breast cancer, IDC, Task 1), **prostate adenocarcinoma** (prostate cancer, PRAD, Task 2), ** pancreatic adenocarcinoma** (pancreatic cancer, PAAD, Task 3), **skin cutaneous melanoma** (skin cancer, SKCM, Task 4), **colonic adenocarcinoma** (colon cancer, COAD, Task 5), **rectal adenocarcinoma** (rectum cancer, READ, Task 6), **clear cell renal cell carcinoma** (kidney cancer, ccRCC, Task 7), **lung adenocarcinoma** (lung cancer, LUAD, Task 8), and **axillary lymph nodes in IDC** (metastatic, LYMPH-IDC, Task 9). Additional information is provided in Appendix Table A11.

For each task, we predict the expression of the top 50 genes with the highest normalized variance across all samples from 112\(\)112 \(\)m H&E regions (equivalent to 224\(\)224-pixel patches at 20\(\)). To avoid train/test patient-level data leakage, we use patient-stratified splits, resulting in a \(k\)-fold cross-validation, where \(k\) is the number of patients. In ccRCC, we use \(k/2\)-fold cross-validation due to the large number of patients.

### Evaluating foundation model for pathology

We use the HEST-Benchmark to evaluate 11 foundation models for pathology. Namely, **ResNet50 (IN) ** (ImageNet pretrained), **CTransPath ** (adapted MoCov3 pretrained on TCGA and PAIP), **Remedis ** (SimCLR  pretrained on TCGA), **Phikon ** (iBOT pretrained on TCGA), **UNI ** (DINov2 ViT-Large pretrained on internal hospital data and GTEx), **CONCH ** (visual-language model using CoCa pretrained on captions from publications and educational resources), **GigaPath** (DINov2 ViT-giant pretrained on proprietary data), **Virchow** (DINov2 ViT-Huge pretrained on proprietary data), **H-Optimus-0** (DINov2 ViT-giant pretrained on proprietary data), and **UNIV1.5** (DINov2 ViT-giant pretrained on public and proprietary data). Additional information is provided in Table A12 and Appendix C.3.

We learn a regression model to map model-specific patch embeddings (512 to 2,048 dimensions) to the log1p-normalized expression of the top 50 highly variable genes. All tasks are evaluated using the Pearson correlation between the predicted and measured gene expression. We report mean and standard deviation across all folds (or patients). All experiments were run on a single NVIDIA 3090 GPU. We report performance using three downstream regression models: (i) PCA-reducedembeddings (with n=256 factors) followed by Ridge regression trained with adaptive regularization as shown in Table 1, (ii) Ridge regression model as shown in Appendix Table A13, and (iii) an XGBoost regression model with 100 estimators and a maximum depth of 3 as shown in Appendix Table A14. Our main results are reported using PCA+Ridge (i) and XGBoost (iii). Directly applying Ridge regression may unfairly penalize models with larger embedding dimensions. To guarantee a fairer and more objective comparison, we chose to utilize PCA reduction.

### Scaling laws in HEST-Benchmark

Overall, H-Optimus-0 brings the best average Pearson correlation in both PCA+Ridge and XGBoost evaluation, outperforming the second-best model, UNIV1.5, by 0.56% and 0.69%, respectively. ResNet50 (IN), the only model that was not pretrained on histology images, leads to the lowest performance in both PCA+Ridge and XGBoost. Legacy domain-specific models, such as CTransPath, are outperformed by all recent models, including UNIV1.5, UNI, GigaPath, Virchow, and H-Optimus-0. The disparity between the top and bottom domain-specific models is notable, showing an absolute improvement of 7.0% for PCA+Ridge and 4.8% for XGBoost. When inspecting individual performance, we observe large differences across tasks from 0.6432 Pearson correlation in SKCM to 0.2292 in READ for H-Optimus-0 evaluated using PCA+Ridge.

**Model scaling law.** By inspecting the number of trainable parameters within the vision encoder for each model, we can describe how model size influences performance (measured with average Pearson correlation across all tasks, Figure 2.a). Performance increases with model size following a logarithmic scaling law (Pearson correlation of R=0.81, P-value\(<\)0.01). Models considered "parameter-efficient" are represented on top of the log-transformed linear regression line (e.g., CONCH, UNIV1.5, and H-Optimus-0). This observation suggests a trade-off between downstream performance and model size. Despite the observation of a model scaling law, significant variations in performance among models of identical size persist, such as between H-Optimus-0 and GigaPath, both of which are ViT-giant models with over one billion parameters.

**Data scaling law.** We further explored how the number of training samples used for pretraining each model (i.e., the number of image patches) affects performance. We observe that increasing the number of patches moderately correlates with the average performance (Pearson correlation of R=0.48, P-value=0.13). This correlation is weaker than model size, which we hypothesize is due to this analysis overlooking both the absolute number of WSIs used for pretraining (image patches are not independently and identically distributed per WSI) and disparities of morphological variety among WSIs (e.g., staining variation, disease diversity, artifacts, etc.).

    & **IDC** & **PRAD** & **PAAD** & **SKCM** & **COAD** & **READ** & **ceRCC** & **LUAD** & **LYMPH IDC** & **Average** \\ 
**ResNet50 (IN)** & 0.4741 & 0.3075 & 0.3889 & 0.4822 & 0.2528 & 0.0812 & 0.2231 & 0.4917 & 0.2322 & 0.326 \\
**CTransPath** & 0.5011 & 0.3427 & 0.4378 & 0.5106 & 0.2285 & 0.11 & 0.2279 & 0.4985 & 0.2353 & 0.3447 \\  & \(\)0.0531 & \(\)0.0458 & \(\)0.0664 & \(\)0.0827 & \(\)0.0577 & \(\)0.0064 & \(\)0.0475 & \(\)0.0414 & \(\)0.0477 \\
**Phixon** & 0.5327 & 0.342 & 0.4432 & 0.5355 & 0.2585 & 0.1517 & 0.2423 & 0.5468 & 0.2373 & 0.3656 \\  & \(\)0.0914 & \(\)0.0877 & \(\)0.00884 & \(\)0.0589 & \(\)0.0056 & \(\)0.0022 & \(\)0.063 & \(\)0.0048 & \(\)0.0587 & \\
**CONCH** & 0.5363 & 0.3548 & 0.4475 & 0.5791 & 0.2533 & 0.1674 & 0.2179 & 0.5312 & 0.2507 & 0.3709 \\  & \(\)0.0842 & \(\)0.0099 & \(\)0.0729 & \(\)0.0524 & \(\)0.0057 & \(\)0.0076 & \(\)0.0353 & \(\)0.0017 & \(\)0.002 & \\
**REMDSIs** & 0.529 & 0.3471 & 0.4644 & 0.5818 & 0.2856 & 0.1145 & 0.2647 & 0.5336 & 0.2473 & 0.3742 \\  & \(\)0.069 & \(\)0.0074 & \(\)0.0722 & \(\)0.0421 & \(\)0.02 & \(\)0.0987 & \(\)0.0539 & \(\)0.0326 & \(\)0.0858 & \\
**GigaPath** & 0.5508 & 0.3708 & 0.4768 & 0.5533 & 0.001 & 0.186 & 0.2391 & 0.5399 & 0.2493 & 0.3853 \\  & \(\)0.0726 & \(\)0.041 & \(\)0.0898 & \(\)0.0386 & \(\)0.1453 & \(\)0.0094 & \(\)0.01964 & \(\)0.01969 & \(\)0.0523 & 0.3862 \\
**UNI** & 0.5702 & 0.314 & 0.4764 & 0.6254 & 0.263 & 0.1762 & 0.2427 & 0.5511 & 0.2565 & 0.3862 \\  & \(\)0.0833 & \(\)0.0173 & \(\)0.0887 & \(\)0.0338 & \(\)0.0311 & \(\)0.0685 & \(\)0.0085 & \(\)0.0198 & \(\)0.0048 & \\
**Virchow** & 0.5702 & 0.3309 & 0.4875 & 0.6088 & 0.0831 & 0.2019 & 0.2637 & 0.5459 & 0.2594 & 0.3977 \\  & \(\)0.0939 & \(\)0.0081 & \(\)0.0412 & \(\)0.0733 & \(\)0.0083 & \(\)0.0367 & \(\)0.039 & \(\)0.08262 & \(\)0.043 & \\
**Virchow2** & 0.5922 & 0.3465 & 0.4661 & 0.6174 & 0.2578 & 0.2084 & **0.2788** & **0.5605** & 0.2582 & 0.3984 \\  & \(\)0.0814 & \(\)0.0305 & \(\)0.0766 & \(\)0.0174 & \(\)0.0199 & \(\)0.0502 & \(\)0.0516 & \(\)0.0172 & \(, HEST-Benchmark brings new insights into the performance of foundation models for pathology. We observe that (1) Scaling the model size strongly correlates with average performance, but the gains grow logarithmically with the number of trainable parameters. (2) Scaling the number of training patches weakly correlates with a higher performance (also on a logarithmic scale). (3) Performance remains low for some tasks (e.g., READ and ccRCC), which suggests that (i) the morphology might not be as reflective of gene expression for some cancer types or (ii) some cohorts have more noise than others (e.g., due to batch effects, low sensitivity, dropout events, or spillover between adjacent spots).

## 6 HEST for biomarker exploration

HEST-1k also enables the analysis of interactions and correlations between tissue morphology (as seen in H&E) and local gene expression (as provided in ST). Here, we showcase the capabilities of HEST-1k (1) by studying morphological correlates of expression changes in invasive breast cancer and (2) by visualizing tumor heterogeneity both on the morphological and molecular sides. Specifically, we focus on invasive ductal carcinoma (IDC) samples imaged with Xenium. Using CellViT nuclear segmentation and classification, we identified neoplastic nuclei (exemplified in two samples: Figure 3.a with n=168,033 nuclei and Appendix Figure 6.a with n=342,018 nuclei). We then overlay the WSI with the expression of specific genes, such as _GATA3_, a known prognostic gene in breast cancer (Figure 3.b). This qualitatively shows that high _GATA3_ expression is associated with cancerous regions and reveals heterogeneity within invasive regions (e.g., the right-most region shows higher expression of _GATA3_ than the rest of the tumor, Figure 3.b). Using the nuclear segmentation, we can compute human-interpretable features related to nuclear size (area, perimeter, major axis length, minor axis length, and equivalent diameter), topology and shape (roundness, ellipticity, eccentricity, extent, and roughness), and cell distribution (cell density and crowdedness). A heatmap of the nuclear area of neoplastic cells also indicates morphological heterogeneity among neoplastic regions (Figure 3.c,d). Regions with a high nuclear area and elevated GATA3 expression notably overlap, suggesting that this tumor exhibits molecular heterogeneity, which to some degree is morphologically expressed.

To investigate this hypothesis, we measured the Pearson correlation between the expression of _GATA3_ and nuclear area in neoplastic cells (Figure 3.e). We observe a moderate correlation (R=0.47, P-value\(<10^{-4}\)), which is also observed in other genes and morphological features (Figure 3.f). Overall, out of the 12 human-interpretable features we analyzed, we found the highest association with gene expression for size-related features, while features involving topology, shape, and cell distribution had a lower correlation (R<0.2). A similar analysis in another IDC sample (Appendix Figure 6.b,c) further asserted these observations. In particular, we found the highest associations between nuclear size and expression for the genes _FLNB_ (R=0.45, P-value\(<10^{-4}\)) and _TPD52_ (R=0.47, P-value\(<10^{-4}\)), both

Figure 2: **Scaling laws in HEST-Benchmark.****a.** Model scaling law comparing the number of training parameters in the vision encoder (log-scale) and the average performance on the HEST-Benchmark. Pearson correlation between parameters and performance of R=0.81 (P-value < 0.01). **b.** Data scaling law comparing the number of image patches used for pretraining (log-scale) and the average performance on the HEST-Benchmark. Pearson correlation between number of patches and performance of R=0.48 (P-value=0.13).

involved in breast tumor growth and proliferation [13; 124], and _FOXA1_ (R=0.47, P-value\(<10^{-4}\)), a known prognostic factor associated with better survival [12; 150].

Such analysis highlights how HEST-1k can be used to identify fine-grained morphological correlates of expression. Similar approaches can be used to characterize morphological and molecular tumor heterogeneity at a larger scale.

## 7 HEST for multimodal representation learning

Access to spatially-resolved expression-morphology pairs unlocks new directions for multimodal representation learning. Several problem statements can be explored, such as cross-modal alignment and retrieval, multimodal fusion, etc. Here, we fine-tune CONCH  (ViT-Base model) on five Xenium invasive breast cancer cases (four ductal and one lobular case) using multimodal contrastive alignment. We hypothesize that the resulting breast cancer-specific patch encoder, termed CONCH-FT, can better encode the underlying molecular landscape associated with disease-specific morphologies. To validate the hypothesis, CONCH-FT is benchmarked on an independent breast cancer cohort for molecular subtyping against its non-finetuned version.

Specifically, for each Xenium sample, we extract 112\(\)112-\(\)m image patches centered around each spot at 20\(\) magnification (0.5\(\)m/px). This yields 47,051 pairs of 224\(\)224-pixel patches and corresponding expression profile (n=238 common genes in the panel, log1p normalized). We then embed the data using modality-specific encoders: the image patches using a pretrained CONCH model and the expression data using a 3-layer MLP (normalized expression data are encoded as tabular data). The modality-specific embeddings are then aligned using a contrastive objective, i.e., InfoNCE loss  by fine-tuning the image encoder and training the expression encoder from scratch. To mitigate over-fitting, we use the following training recipe: (1) Finetune only the last 3 layers of CONCH, (2) employ a layer-wise learning decay factor of 0.7, and (3) employ patch-level image augmentation. Additional details are provided in Appendix.

We evaluate the resulting CONCH-FT model to predict ER, PR, and HER2 expression status (binary) from WSIs in the BCNB dataset  (n=1,058 WSIs). To generate a slide representation for a

Figure 3: **HEST for biomarker exploration: Analysis of an invasive ductal carcinoma sample imaged with Xenium.****a**. IDC Xenium sample with neoplastic nuclei overlaid in red (\(n_{c}=168,033\) detected nuclei). Gray scale bar represents 2 mm. **b**. Heatmap of Xenium expression of gene _GATA3_. Blue and red values indicate above and below the mean (in white), respectively. **c**. Heatmap of neoplastic nuclear area. **d**. Four randomly selected regions with CellViT segmentation of the neoplastic nuclei. Black scale bar represents 30 \(\)m. **e., f**. Correlation between nuclear area and _GATA3_, and minor axis length and _MYBPC1_.

WSI, we take the average of the patch embedding in the WSI (mean pooling), which is subsequently mapped to the expression status using logistic regression (Table 2). The simple mean pooling approach to embedding the slide without additional fine-tuning on the downstream tasks highlights the expressivity of the learned latent space. We observe that CONCH-FT outperforms CONCH on most metrics, demonstrating that pan-tissue histology patch encoders can be further fine-tuned to obtain better tissue-specific patch encoders. This is further validated by the larger rank induced by the patch embedding space  for CONCH-FT, suggesting better expressivity of the patch embeddings. While these results are based on only five paired WSIs, we anticipate additional benefits when training with larger disease-specific cohorts.

## 8 Discussion

**Summary.** We assembled HEST-1k, a dataset comprising paired spatial transcriptomics, H&E-stained whole-slide images, and comprehensive metadata built from public and internal cohorts. HEST-1k includes 1,229 samples, encompassing 2.1 million spots and over 76 million cells. The scale and comprehensiveness of HEST-1k, supported by the HEST-Library, enable exploring directions such as biomarker exploration and multimodal representation learning. Additionally, motivated by the need for new, diverse, and challenging patch-level benchmarks, we curated the HEST-Benchmark, a set of nine tasks covering eight cancer types and nine organs for gene expression prediction from histology. The HEST-Benchmark revealed data and model scaling laws across 11 foundation models of different dimensions and pretraining scale .

**Limitations.** Our study includes a few limitations. First, research data, such as those generated in spatial transcriptomic, are inherently noisy. While we tried to minimize "label" noise (e.g., by re-estimating image magnification and alignment, and unifying cancer samples using oncotree code taxonomy), staining and compression artifacts, varying acquisition protocols, among others, can negatively impact the quality of HEST-1k. Second, batch effects (on both the imaging and transcriptomic sides) can be significant across samples, datasets, and technologies. While this study does not explore batch effects quantification or mitigation, we provide a set of helpers in HEST-Library to let users explore this direction. Lastly, although the HEST-Library was designed for versatility, it cannot cover all existing formats and should rather be viewed as a blueprint for processing ST data in a consistent and unified manner.

**Future work.** Spatial transcriptomics is rapidly evolving, with new datasets frequently published. As they become available, we will keep updating HEST-1k with new resources. This study merely starts to uncover the potential of HEST-1k for advancing translational research and biomarker exploration, and we plan to explore these capabilities further. Additionally, the prospects for multimodal representation learning with HEST-1k are promising and are expected to grow with the addition of more data.

    & **Rank** &  &  &  \\  & AUC & Bal.acc. & AUC & Bal.acc. & AUC & Bal.acc. \\ 
**CONCH** & 144.66 & 0.881 & 0.745 & 0.810 & 0.698 & 0.715 & **0.624** \\
**CONCH-FT** & **146.47** & **0.884** & **0.752** & **0.818** & **0.714** & **0.724** & 0.615 \\   

Table 2: **CONCH fine-tuning on invasive breast cancer. Logistic regression evaluation for ER/PR/HER2 status on BCNB (binary task, n=1,058 WSIs). A WSI is represented by the average of the patch embeddings within each WSI. We report the mean \(\) standard deviation computed over all folds (or patients) for ROC-AUC (AUC) and balanced accuracy (Bal.acc.). Best is bold.**

## Checklist

1. Do the main claims made in the abstract, and introduction accurately reflect the paper's contributions and scope? [Yes] Each claim: HEST-1k, HEST-Library, HEST-Benchmark, HEST for biomarker exploration, and multimodal fine-tuning are supported by dedicated sections in the main text, in addition to supplementary information provided in the appendix. In addition, all the code to reproduce results is made available.
2. Did you describe the limitations of your work? [Yes] We discuss limitations in the **Discussion**.
3. Did you discuss any potential negative societal impacts of your work? [Yes] We discuss potential negative societal impacts in the section **Ethical considerations, intended usage, and license**.
4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
5. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] In the abstract, we provide a link to access the HEST page on GitHub. HEST-Library includes a link to download all data and to run the HEST-Benchmark. Finally, we provide all metadata associated with HEST-1k in a CSV as part of the supplementary material.
6. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] When relevant, we provide training details, such as in the **HEST-Benchmark**.
7. Did you report error bars? [Yes] HEST-Benchmark results include standard deviation computed from cross-validation across all patients.
8. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]
9. If your work uses existing assets, did you cite the creators? [Yes] All public resources used in this study are cited in Appendix Table A2, A4, A6, A7, A8, A9 and A10.
10. Did you mention the license of the assets? [Yes] Metadata associated with HEST-1k includes the license under which data were originally published. We ensured that the reported license allowed the distribution and creation of derivatives of the data.
11. Did you include any new assets either in the supplemental material or as a URL? [Yes] As part of HEST-1k, we include internal datasets (see Appendix Table A9).
12. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No] We used public resources for which the license was allowing redistributing the work. Users are welcome to inspect the individual IRBs of each publicly available resource.
13. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] We manually ensured that none of the published and distributed data includes personally identifiable information or offensive content, such as personal health information.

## Appendix

* A Ethical considerations, intended usage and license
* B Background
* B.1 Computational pathology
* B.2 Spatial transcriptomics (ST)
* C HEST
* C.1 HEST-1k
* C.2 HEST-Library
* C.3 HEST-Benchmark
* D HEST for multimodal representation learning
* E HEST for discovery
* F Datasheet for HEST-1k
* F.1 Motivation for dataset creation
* F.2 Dataset composition
* F.3 Data collection process
* F.4 Data preprocessing
* F.5 Dataset distribution
* F.6 Legal and ethical considerations
* G Author statement

## Appendix A Ethical considerations, intended usage and license

All resources provided as part of this study are strictly for research purposes and must not be utilized to support any diagnostic procedures. Users are hereby notified that the nuclear segmentation and classification components are derived from a publicly available model. Consequently, this model should not be regarded as the definitive standard, and users should exercise particular caution when utilizing this part of the dataset. Despite our efforts to exclude sensitive information, such as patient names, addresses, and social security numbers, users are expressly prohibited from attempting to reverse engineer the data to extract confidential patient information. In the presumption that users will adhere to the aforementioned restrictions, we have not identified any potential adverse social impacts that could arise from the use of HEST-1k.

The dataset is hosted on the HuggingFace Dataset webpage. All instructions are provided in the main README of HEST-Library. From there, users can choose to download HEST-1k in its entirety or a subset (e.g., only breast cancer samples). The HEST-1k, HEST-Benchmark, and HEST library are released under the Attribution-NonCommercial-ShareAlike 4.0 International license (CC BY-NC-SA 4.0 Deed)11.

## Appendix B Background

This study connects two fields: (1) computational pathology, which primarily uses routinely acquired clinical data to determine outcomes such as disease diagnosis from H&E-stained digitized tissuesections, and (2) spatial transcriptomics, which so far has been confined to biological research and aims to identify new biomarkers predictive of disease progression or response to treatment, among others.

### Computational pathology

Research in computational pathology  has primarily focused on classifying digitized WSIs into clinical outcomes. Unlike natural image classification tasks such as ImageNet, a WSI may reach sizes of up to 150,000 \(\) 150,000 pixels at 20\(\) magnification (0.5\(\)m/pixel). The challenge of managing the large size of WSIs has been one the central themes of the field, primarily through the adoption of multiple instance learning (MIL) for weakly-supervised classification . MIL employs a two-step process: (1) Initially, the tissue is segmented from the background and then tessellated into patches, usually 256 \(\) 256 pixels, akin to an ImageNet sample, and each patch is compressed into a patch embedding using a pretrained patch encoder. (2) Subsequently, these patch embeddings are aggregated using a learnable neural network, such as an attention-based network, a graph neural network, or a Transformer, to produce a slide embedding . This slide embedding is then used to classify specific targets of interest, such as cancer histological subtyping, morphological subtyping, mutation prediction, or survival analysis.

Such frameworks have been shown to achieve better or similar performance than humans for Gleason grading in prostate cancer, metastasis detection in lymph nodes, determining the origin of a cancer of unknown primary, predicting heart transplant rejection, among others.

### Spatial transcriptomics (ST)

ST enables the measurement of gene activity and the mapping of its corresponding location in the tissue. In this study, we collected samples from two ST paradigms: sequencing-based (ST, Visium, Visium HD) and imaging-based (Xenium).

**Visium (HD) / Spatial transcriptomics:** Visium-HD and its predecessors Visium and Spatial Transcriptomics (STv1) refer to a family of sequencing-based products for spatially resolving large transcript panels, whose main difference lies in the resolution and spacing between the expression measurement, called a spot. These spots capture mRNA from tissue sections placed on the chip, and the location-specific barcodes contained in each of the spots bind to the RNA to retain spatial information. The RNA molecules are then washed off the slides and processed by a sequencing instrument. Using a sequencing-based method allows the reuse of existing sequencing instruments developed in the fields of single-cell and bulk transcriptomics, hence benefiting from existing technological advancements and allowing whole transcriptome analysis. A fundamental drawback of current sequencing-based methods is the inherent RNA resolution limitation imposed by the size of the spots (e.g., 55\(\)m in Visium).

**Xenium:** Xenium is an imaging-based spatial profiling technology that offers in situ RNA capturing on tissue sections by imaging fluorescent RNA markers derived from padlock probes and rolling circle amplification chemistry. This approach provides the exact 2D location of each measured transcript. As of 2024, Xenium cannot perform whole transcriptome measurements and is limited to gene panels of up to 5,000 genes.

## Appendix C Heat

### Heat-1k

We provide a comprehensive description of all publicly available and internal cohorts integrated into HEST-1k.

### HEST-Library

The HEST-Library helps transform unstructured spatial transcriptomics and histology data into a unified format. An overview of the HEST-Library is provided in Figure 4. An example of fiducial detection is presented in Figure 5.

### HEST-Benchmark

**Gene selection, XGBoost Forest, and Ridge regression models:** We learn a regression model that maps the patch embeddings of each encoder to its corresponding gene expression profile. The XGboost model uses 100 estimators, a 0.1 learning rate, a max depth of 3, 0.8 subsampling, gamma of 0.0, regression alpha of 0.0, and regression lambda of 1.0. Additional information can be found in the XGBoost API12. The Ridge regression uses a fixed \(L2\) regularization coefficient \(\) set to \(100/MC\), where \(M\) is the embedding dimension and \(C=50\) is the number of targets trained with the Regularized Least-Squares Routine solver (sklearn implementation). Both regression models are trained to predict a panel constituted of the 50 most variable genes of each task. Specifically, for each

Figure 4: **Overview of HEST-Library functionalities.** HEST was designed to transform legacy data scrapped in multiple public repositories, such as NCBI, into unified HEST objects that can easily be integrated into computational pipelines.

  
**Resource** & **Number of datasets** & **Number of samples** & **Size (GB, raw)** \\ 
10x Genomics & 87 & 112 & 275 \\ Mendeley & 9 & 118 & 181 \\ Spatial-Research & 4 & 139 & 18 \\ Zenodo & 4 & 21 & 18 \\ NCBI & 43 & 696 & 298 \\ Internal & 3 & 28 & 60 \\ Miscellaneous & 4 & 114 & 147 \\   

Table A1: **HEST-1k data overview.** All samples include a license that allows sharing and redistributing. National Center for Biotechnology Information: NCBI.

task, we select the 50 most variable genes across all spots and samples after excluding the genes that have non-zero counts in less than 10% of the spots.

**Benchmark task description:** We provide complementary information on each task introduced as part of the HEST-Benchmark.

**Task 1: Prediction of expression in invasive ductal carcinoma (breast cancer, IDC).** We used all publicly available Xenium samples available on 10x Genomics ("FFPE Human Breast using the Entire Sample Area", 2 patients) and two samples published in  (TENX95, TENX99, NCBI783, NCBI785). All samples are FFPE sections imaged with the Xenium pipeline v1.

**Task 2: Prediction of expression in prostate adenocarcinoma (prostate cancer, PRAD).** We used all 23 Visium samples (fresh frozen sections) from 2 patients published in  (MEND139 to MEND162). Both patients were diagnosed with prostatic acinar adenocarcinoma with a (4+3) Gleason score (ISUP group 4).

**Task 3: Prediction of expression in pancreatic adenocarcinoma (pancreatic cancer, PAAD).** We used 3 samples from 3 different patients from 10x Genomics ("FFPE Human Pancreas with Xenium Multimodal Cell Segmentation" and "Pancreatic Cancer with Xenium Human Multi-Tissue and Cancer Panel"). All samples are FFPE sections processed with Xenium pipeline v1 (TENX116, TENX126, TENX140).

**Task 4: Expression prediction in skin cutaneous melanoma (skin cancer, SKCM).** We used 2 samples from 2 different patients from 10x Genomics website ("Human Skin Data with Xenium Human Multi-Tissue and Cancer Panel"). All samples are FFPE sections processed with Xenium pipeline v1 (TENX115, TENX117).

**Task 5: Prediction of expression in colon adenocarcinoma (colon cancer, COAD).** We used 4 COAD samples from 2 different patients available on 10x Genomics (TENX111, TENX147, TENX148, TENX149). All samples are fresh frozen sections processed with Visium.

**Task 6: Prediction of expression in rectal adenocarcinoma (rectum cancer, READ).** We used 4 READ samples from 2 different patients published in . All samples are fresh frozen sections processed with Visium (ZEN36, ZEN40, ZEN48, ZEN49).

**Task 7: Prediction of expression in clear cell renal cell carcinoma (kidney cancer, ccRCC).** We used the 24 ccRCC samples of 24 different patients published in . All samples are fresh frozen sections processed with Visium (INT1 to INT24).

Figure 5: **Fiducial detection and automatic alignment in Visium.** Corner fiducials on 6.5\(\)6.5mm and 11mm\(\)11mm Visium slides are automatically detected with a finetuned Yolov8 model. The spot coordinates are then derived if at least 3 of the 4 corner fiducials are detected. This process enables automatically estimating the pixel resolution.

**Task 8: Prediction of expression in lung adenocarcinoma (lung cancer, LUAD).** We used 2 LUAD samples from 2 different patients from 10x genomics ("Preview Data: FFPE Human Lung Cancer with Xenium Multimodal Cell Segmentation"). All samples are fresh frozen sections processed with Xenium pipeline v1 (TENX118, TENX141).

**Task 9: Prediction of expression in axillary lymph nodes in IDC patients.** We used 4 axillary lymph node samples from 2 IDC patients published in . All samples are fresh frozen sections processed with Visium (NCBI681, NCBI682, NCBI683, NCBI684).

We provide a brief description of each patch encoder assessed with the HEST-Benchmark.

**ResNet50 (IN) :** This model uses a ResNet50 backbone  trained on ImageNet  (1.2 million natural images). Following prior work , the patch embeddings are extracted by taking the representation at the penultimate layer before final classification.

**CTransPath :** This model uses a "Tiny" Swin Transformer backbone  with a window size of 14 (Swin-T/14, 28 million parameters) pretrained on TCGA and PAIP datasets (17 million images) using MoCoV3 .

**Remedis :** This model uses a ResNet-152\(\)2 (232 million parameters) initialized with the "Big Transfer"-medium protocol  on ImageNet-22K and pretrained with SimCLR  on TCGA.

**Phikhon :** This model uses a Vision Transformer-Base (ViT-B, 86 million parameters)  trained on TCGA data using iBOT .

**UNI :** This model uses a ViT-Large (ViT-L, 307 million parameters)  trained on 100 million histology images (over 100,000 slides) from proprietary and public data using DINOv2 .

**CONCH :** This model uses a ViT-B (86 million parameters) trained on a smaller version of UNI using iBOT, and then fine-tuned on 1.17 million histology image-caption pairs extracted from online educational and research resources using CoCa .

**GigaPath :** This model uses a ViT-giant (1.13 billion parameters) trained on 1.3 billion image patches from 171,189 WSIs at 20\(\) magnification using DINOv2.

**Virchow :** This model uses a ViT-Huge (632M parameters) trained on 2 billion image patches and 1.5M WSIs at 20\(\) magnification using DINOv2.

**Virchow 2 :** This model uses a ViT-Huge (632M parameters) trained on 1.9B patches and 3.1M WSIs using DINOv2

**H-Optimus-0:** This model uses a ViT-giant (1.13B parameters) trained 273 million image patches from 500,000 WSIs at 20\(\) magnification using DINOv2.

**UNIV1.5:** This model uses a ViT-giant (1.13B parameters) trained on 432 million image patches from 350,000 WSIs using DINOv2.

## Appendix D HEST for multimodal representation learning

We provide additional information regarding CONCH fine-tuning using multimodal alignment. CONCH-FT model, a ViT-Base model initialized with CONCH weights, was fine-tuned for 50 epochs using a cosine learning rate scheduler, with a base learning rate of \(10^{-4}\) for the image encoder and \(10^{-5}\) for the expression encoder. Only the last 3 layers of the model were fine-tuned, with a layer-wise learning decay rate of 0.7. For training with the infoNCE loss, a contrastive temperature of \(10^{-2}\) and batch size of 1,024 pairs of patch and transcriptomics were used. A combination of random horizontal/vertical flip and color jittering was employed for image patch augmentation.

The rank of the embedding space (also referred to as smooth rank measure ) measures the quality of the embeddings produced from encoders trained in unsupervised or self-supervised manners. Given the patch embedding matrix \(H^{N d}\) and \(d<N\), where \(N\) is the number of patches and \(d\) is the feature dimension, we compute the rank as the entropy of the \(d\) L1-normalized singular values of \(H\).

HEST for discovery

Cells were segmented and classified using CellViT . To find the gene expression profile of each neoplastic cell, we matched each cell to its corresponding cell index in Xenium by assigning the index for which the distance between the cell centroids was the smallest. After matching all neoplastic cells, only those cells for which the assignment was unique were kept. After this filtering step, an average of 91% of the cells per sample were kept while 9% of the cells were discarded.

## Appendix F Datasheet for HEST-1k

We provide a DataSheet for HEST-1k that summarizes the contributions, analyses, and intended usages presented in the study.

### Motivation for dataset creation

* **Why was the dataset created?** HEST-1k was designed with three key applications: (1) multimodal representation learning of histology and transcriptomics, (2) biomarker exploration and characterization, and (3) benchmarking foundation models for pathology. Despite many publicly available resources, no existing unified and user-friendly formatting was available to bring ST into the world of deep learning.
* **What (other) tasks could the dataset be used for? Are there obvious tasks for which it should not be used?** Users are welcome to introduce new, creative ways to use the dataset. However, users are not allowed to try to retrieve patient information from the existing data. A dedicated section is provided to discuss ethical considerations and intended usage.
* **Has the dataset been used for any tasks already? If so, where are the results so others can compare (e.g., links to published papers)?** The metadata attached to HEST-1k reports all samples that were made public as part of a publication (peer-reviewed or not).

Figure 6: **HEST for biomarker discovery: Analysis of an invasive ductal carcinoma Xenium sample.****a.** IDC Xenium sample with neoplastic nuclei overlaid in red (\(n_{c}\)=342,018 detected nuclei). Six randomly selected regions with CellViT segmentation of the neoplastic nuclei. Black scale bar represents 30 \(\)m. **b.** Pearson correlation between the major axis length of neoplastic nuclei and the log1p-normalized expression of _TPD52_. **c.** Analogous analysis between nuclear area and _FLNB_ expression.

* **Who funded the creation of the dataset?** HEST is supported by the Brigham and Women's Hospital (BWH) President's Fund, Mass General Hospital (MGH) Pathology, and the National Institute of Health (NIH) National Institute of General Medical Sciences (NIGMS) through R35GM138216.

### Dataset composition

* **What are the instances?** The modalities used in this study are histopathology whole-slide images, gene expression data, and derivatives of these two modalities, such as nuclear segmentation and classification maps.
* **Are relationships between instances made explicit in the data** Each whole-slide image maps to a unique gene expression profile in an unequivocal way.
* **What data does each instance consist of?** Imaging data consists of Generic TIFF objects stored in a pyramidal format, and gene expression data consists of _scanpy_ objects. Derivatives are stored in JSON files, parquet files, and Hierarchical Data Format (HDF) files.
* **Is there a label/target associated with instances? If the instances are related to people, are subpopulations identified (e.g., by age, gender, etc.), and what is their distribution?** Each sample pair (slide and expression profile) is associated with comprehensive metadata. All metadata information is thoroughly described in the main paper. Age and gender are only reported in a subset of cases.
* **Is everything included or does the data rely on external resources? (e.g., websites, tweets, datasets) If external resources, a) are there guarantees that they will exist, and remain constant, over time; b) is there an official archival version. Are there licenses, fees or rights associated with any of the data?** We provide all data as part of the HEST-1k release. In addition, a link to the original data is provided in the metadata. Each sample is associated with a license as provided by the original publication, where we ensured that the reported license allowed for distributing and creating derivatives of the data.
* **Are there recommended data splits or evaluation measures?** HEST-1k comes with the HEST-Benchmark, a series of tasks for gene expression prediction from histology images. All patient-stratified splits are specified in the attached comma-separated values (CSV) files.
* **What experiments were initially run on this dataset? Have a summary of those results and, if available, provide the link to a paper with more information here.** All experiments run with HEST-1k are described in this study. The reader can refer to the main text for a thorough description of all experiments (see **HEST-Benchmark**, **HEST for biomarker exploration**, **HEST for multimodal representation learning**).

### Data collection process

* **How was the data collected?** The data were manually inspected and curated by the authors of the present study.
* **Who was involved in the data collection process?** All authors of the present study were involved in the data collection, inspection, and curation. The reader can refer to the original publication to understand how the data were originally acquired.
* **Over what time frame was the data collected? Does the collection time frame match the creation time frame?** The original data comprise publications from 2016 to 2024. As the dataset grows, more recent publications might be included in HEST-1k.
* **Does the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily random) from a larger set of instances?** All pairs of gene expression data and whole-slide images of the underlying studies were included and are unique.
* **Is there information missing from the dataset and why? (this does not include intentionally dropped instances; it might include, e.g., redacted text, and withheld documents) Is this data missing because it was unavailable?** Original publications may include some missing information, such as the alignment file between the slide and the expression profile. We developed computational tools to minimize missing information and reach near-complete metadata.

* **Are there any known errors, sources of noise, or redundancies in the data?** All whole-slide images have been manually inspected. The quality from one sample to another varies significantly, for instance, due to poor staining, compression artifact, lower resolution, etc. Gene expression data are inherently noisy. Users can decide to apply post-hoc normalization methods to reduce noise, e.g., stain normalization on the imaging side or batch effect mitigation on the transcriptomics side.

### Data preprocessing

* **What preprocessing/cleaning was done?** All whole-slide images were converted into pyramidal TIFF objects with re-estimated pixel resolution. All alignment files have been manually inspected and included if missing. All gene expression data have been transformed into _scampy_ objects following the same process.
* **Was the "raw" data saved in addition to the preprocessed/cleaned data? (e.g., to support unanticipated future uses)** Raw data are downloaded but not publicly shared. In the case of public samples, users can re-download them using the metadata provided as part of the dataset release.
* **Is the preprocessing software available?** Yes, the source code to preprocess HEST-1k is made publicly available as part of the HEST library.

### Dataset distribution

* **How is the dataset distributed?** HEST-1k is distributed using HuggingFace Datasets.
* **When will the dataset be released/first distributed?** The dataset is public and can be accessed through the HuggingFace Datasets interface.
* **What license (if any) is it distributed under? Are there any copyrights on the data?** The dataset is distributed under the Attribution-NonCommercial-ShareAlike 4.0 International license (CC BY-NC-SA 4.0 Deed).
* **Are there any fees or access/export restrictions?** No access/export restrictions unless they violate the terms of the above-mentioned license (CC BY-NC-SA 4.0 Deed).
* **Who is supporting/hosting/maintaining the dataset? How does one contact the owner/curator/manager of the dataset?** The dataset is maintained by the authors of the publication.
* **Will the dataset be updated? How often and by whom? How will updates/revisions be documented and communicated (e.g., mailing list, GitHub)? Is there an erratum?** The dataset might evolve as additional samples become publicly available. Dataset versioning will be put in place.
* **If the dataset becomes obsolete how will this be communicated?** The GitHub README will be updated.
* **Is there a repository to link to any/all papers/systems that use this dataset?** There is no repository to link papers that use HEST-1k. Users are required to cite HEST-1k if they use it in their own research.
* **If others want to extend/augment/build on this dataset, is there a mechanism for them to do so? If so, is there a process for tracking/assessing the quality of those contributions. What is the process for communicating/distributing these contributions to users?** Users are welcome to contact us if they would like to provide additional data that meets our standards. We do not have a dedicated system to communicate these contributions. Newly added data will be tracked in the versioning.

### Legal and ethical considerations

* **If the dataset relates to people (e.g., their attributes) or was generated by people, were they informed about the data collection? (e.g., datasets that collect writing, photos, interactions, transactions, etc.)** HEST-1k does not include patient information (such as name, address, etc.).

* **If it relates to other ethically protected subjects, have appropriate obligations been met? (e.g., medical data might include information collected from animals)** For animal samples (_Mus musculus_ tissue), we refer to the original publication for an in-depth analysis.
* **If it relates to people, were there any ethical review applications/reviews/approvals? (e.g. Institutional Review Board applications)** For human tissue, we refer to the original publication for an in-depth analysis. Internal cohorts were ethically reviewed and collected as part of dedicated IRBs.
* **If it relates to people, could this dataset expose people to harm or legal action? (e.g., financial, social or otherwise) What was done to mitigate or reduce the potential for harm?** No, patients cannot be linked to the corresponding histology and gene expression profile.
* **If it relates to people, does it unfairly advantage or disadvantage a particular social group? In what ways? How was this mitigated?** Most datasets do not include specific demographics. When reported, we include this information in the metadata associated with each sample. To our knowledge, the representation of HEST-1k does not unfairly advantage or disadvantage a particular social group.
* **Does the dataset contain information that might be considered sensitive or confidential? (e.g., personally identifying information)** No.
* **Does the dataset contain information that might be considered inappropriate or offensive?** No.

## Appendix G Author statement

The authors of this paper bear all responsibility in case of violation of rights associated with HEST-1k, HEST-Library, and HEST-Benchmark.

  
**Collection name** & **Organ** & **Technology** & \(n\) & 
 **Num.** \\ **genes** \\  \\  Adult Mouse Brain (FFPE) & Brain & Visium & 1 & 19,465 \\ Adult Mouse Brain Coronal Section (Fresh Frozen) 1 & Brain & Visium & 1 & 32,285 \\ Adult Mouse Brain Coronal Section (Fresh Frozen) 2 & Brain & Visium & 1 & 32,285 \\ Adult Mouse Kidney (FFPE) & Kidney & Visium & 1 & 19,465 \\ Adult Mouse Olfactory Bulb & Brain & Visium & 1 & 32,285 \\ Characterization of immune cell populations in the tumor microenvironment of colorectal cancer using high definition spatial profiling  & Bowel & Mixed & 8 & 18,085 \\ FFPE Human Breast using the Entire Sample Area FFPE Human Breast with Custom Add-on Panel FFPE Human Breast with Pre-designed Panel FFPE Human Pancreas with Xenium Multimodal Cell Segmentation & Breast & Xenium & 2 & 541 \\ FFPE Human Prostate Adenocarcinoma with 5K Human Pan Tissue and Pathways Panel FFPE Human Skin Primary Dermal Melanoma with 5K Human Pan Tissue and Pathways Panel Fresh Frozen Mouse Colon with Xenium Multimodal Cell Segmentation & Breast & Xenium & 1 & 541 \\ Fresh Frozen Mouse Brain Hemisphere with 5K Mouse Pan Tissue and Pathways Panel Fresh Frozen Visium on CytAssist: Human Breast Cancer, Probe-Based Whole Transcriptome Profiling & Breast & & 10,006 \\ Fresh Frozen Visium on CytAssist: Mouse Brain, Probe-Based Whole Transcriptome Profiling & Skin & & 10,017 \\ Human Bone and Bone Marrow Data with Custom Add-on Panel & Bone & Xenium & 1 & 541 \\ Human Brain Cancer, 11 mm Capture Area (FFPE) Human Breast Cancer (Block A Section 1) Human Breast Cancer (Block A Section 2) Human Breast Cancer (Block A Section 2) Human Breast Cancer: Ductal Carcinoma In Situ, Invasive Carcinoma (FFPE) Human Breast Cancer: Targeted, Immunology Panel Human Breast Cancer: Visium Fresh Frozen, Whole Transcriptome Human Cerebellum: Targeted, Neuroscience Panel Human Cerebellum: Whole Transcriptome Analysis Human Cervical Cancer (FFPE) Human Colon Preview Data (Xenium Human Colon Gene Expression Panel) Human Colorectal Cancer: Targeted, Gene Signature Panel Human Colorectal Cancer: Whole Transcriptome Human Glioblastoma: Targeted, Pan-Cancer Panel Human Glioblastoma: Whole Transcriptome Analysis Human Heart Human Heart Data with Xenium Human Multi-Tissue and Cancer Panel Human Intestine Cancer (FPPE) Human Kidney Preview Data (Xenium Human Multi-Tissue and Cancer Panel) Human Kidney 11 mm Capture Area (FFPE) Human Liver Data with Xenium Human Multi-Tissue and Cancer Panel Human Lung Cancer (FFPE) Human Lung Cancer, 11 mm Capture Area (FFPE) Human Lymph Node & \(n\): number of samples in the cohort. \\   

Table 2: **Datasets gathered from 10x Genomics portal. \(n\)**: number of samples in the cohort.

  
**Collection name** & **Organ** & **Technology** & \(n\) & 
 **Num.** \\ **genes** \\  \\  Human Ovarian Cancer (FFPE) & Ovary & Visium & 1 & 17,943 \\ Human Ovarian Cancer, 11 mm Capture Area (FFPE) & Ovary & Visium & 1 & 18,085 \\ Human Prostate Cancer, Acinar Cell Carcinoma (FFPE) & Prostate & Visium & 1 & 17,943 \\ Human Prostate & Prostate & Visium & 1 & 17,943 \\ Human Skin Data with Xenium Human Multi-Tissue and Cancer Panel & Skin & Xenium & 2 & 541 \\ Human Skin Preview Data (Xenium Human Skin Gene Expression Panel with Custom Add-On) & Skin & Xenium & 1 & 541 \\ Human Skin Preview Data (Xenium Human Skin Gene Expression Panel) & Skin & Xenium & 1 & 541 \\ Human Tonsil Data with Xenium Human Multi-Tissue and Cancer Panel & Lymph node & Xenium & 2 & 541 \\ Mouse Bone Data with Custom Add-on Panel & Bone & Xenium & 3 & 541 \\ Mouse Brain Coronal Section 1 (FFPE) & Brain & Visium & 1 & 19,465 \\ Mouse Brain Section 1 (Sagittal-Anterior) & Brain & Visium & 1 & 31,053 \\ Mouse Brain Serial Section 1 (Sagittal-Posterior) & Brain & Visium & 1 & 31,053 \\ Mouse Brain Serial Section 2 (Sagittal-Anterior) & Brain & Visium & 1 & 31,053 \\ Mouse Brain Serial Section 2 (Sagittal-Posterior) & Brain & Visium & 1 & 31,053 \\ Mouse Embryo, 11 mm Capture Area (FFPE) & Embryo & Visium & 1 & 19,465 \\ Mouse Kidney Section (Coronal) & Visium & 1 & 31,053 \\ Mouse Tissue Microarray in 3x3 Layout with 1 mm Edge to Edge Spacing (FFPE) & Lung/Brain & Visium & 1 & 19,465 \\ Mouse Tissue Microarray in 3x5 Layout with 1 mm Edge to Edge Spacing (FFPE) & Kidney/Brain Visium & 1 & 19,465 \\ Normal Human Prostate (FFPE) & Prostate & Visium & 1 & 17,943 \\ Pancreatic Cancer with Xenium Human Multi-Tissue and Cancer Panel & Princess & Xenium & 1 & 538 \\ Preservation Method Comparison on CytAssist: FFPE Mouse Brain (Sagittal), 11 mm Capture Area & Brain & Visium & 1 & 19,465 \\ Preservation Method Comparison on CytAssist: Fixed Frozen Mouse Brain (Sagittal), 11 mm Capture Area & Brain & Visium & 1 & 19,465 \\ Preservation Method Comparison on Visium CytAssist: Freß Frozen Mouse Brain (Sagittal), 11 mm Capture Area & Brain & Visium & 1 & 19,465 \\ Frozen Mouse Brain (Sagittal), 11 mm Capture Area & Brain & Visium & 1 & 19,465 \\ Preservation Method Comparison on Visium CytAssist: Fixed Frozen Mouse Brain (Sagittal), 11 mm Capture Area & Brain & Visium & 1 & 19,465 \\ Preview Data: FFPE Human Lung Cancer with Xenium Multimodal Cell Segmentation & Brain & Visium & 1 & 19,465 \\ Preview Data: FFPE Human Lymph Node with 5K Pan Tissue and Pathways Panel & Brain & Visium & 1 & 19,465 \\ Visium CytAssist Gene Expression Libraries of Post-Xenium Human Colon Cancer (FFPE) & Brain & Visium & 1 & 19,465 \\ Visium CytAssist Gene Expression Libraries of Post-Xenium Mouse Brain (FF) & Visium & 1 & 541 \\ Visium CDAssist, Mouse Embryo, 11 mm Capture Area (FFPE) & Visium HD Spatial Gene Expression Library, Mouse Kidney (FFPE) & Visium HD Spatial Gene Expression Library, Human Panel (FFPE) & Visium HD Spatial Gene Expression Library, Human Panel (FFPE) & & \\ Whole Mouse Pup Preview Data (Xenium Mouse Tissue Atlassing Panel) & Whole Mouse Patient & 1 & 541 \\   

Table 3: **Datasets gathered from 10x Genomics portal. Continuation.**

## Appendix A

 p{113.8pt} p{113.8pt} p{113.8pt}}  
**Publication** & **Organ** & **Technology** & \(n\) & **Num. genes** \\  A spatiotemporal organ-wide gene expression and cell atlas of the developing human heart  & Heart &   & 19 & 39,739 \\ Integrating spatial gene expression and breast tumour morphology via deep learning  & Breast &   & 68 & 16,744 \\ Spatial deconvolution of HER2-positive breast cancer delineates tumor-associated cell type interactions  & Breast &   & 36 & 15,045 \\ Visualization and analysis of gene expression in tissue sections by spatial transcriptomics  & Brain & 
  & 16 & 16,573 \\   

Table A5: **Datasets gathered from NCBI. Continuation.**

  
**Publication** & **Organ** & **Technology** & \(n\) &  **Num.** \\ **genes** \\  \\   Prostate ST Internal \\ Tertiary lymphoid structures generate and propagate \\ anti-tumor antibody-producing plasma cells in renal \\ cell cancer  \\  &  Prostate \\ Lymph \\ node \\  & 
 Visium \\  & 4 & 17,943 \\   

Table A9: **Internal datasets.**

    & **IDC** & **PRAD** & **PAAD** & **SKCM** & **COAD** & **READ** & **ccRCC** & **LUAD** & **LYMPH IDC** & **Average** \\ 
**REMEDIS** & 0.4936 & 0.2632 & 0.2881 & 0.4117 & 0.151 & 0.0776 & 0.2201 & 0.3114 & 0.1694 & 0.2651 \\  & \(\) 0.0725 & \(\) 0.0821 & \(\) 0.0544 & \(\)0.0384 & \(\) 0.0147 & \(\) 0.0684 & \(\) 0.0418 & \(\) 0.0432 & \(\) 0.0855 \\
**GigaPath** & 0.532 & 0.3035 & 0.3172 & 0.2231 & 0.163 & 0.1236 & 0.2172 & 0.3144 & 0.1925 & 0.2652 \\  & \(\) 0.0812 & \(\) 0.0279 & \(\) 0.0165 & \(\) 0.0871 & \(\) 0.0417 & \(\) 0.0379 & \(\) 0.0479 & \(\) 0.0871 & \(\) 0.0301 & \\
**UNlv1.5** & 0.5657 & 0.3065 & 0.3004 & 0.258 & 0.1982 & 0.1077 & 0.2023 & 0.3084 & 0.1998 & 0.2719 \\  & \(\) 0.0866 & \(\) 0.02 & 0.0199 & \(\) 0.0514 & \(\) 0.0026 & \(\) 0.0222 & \(\) 0.0695 & \(\) 0.0174 & \(\) 0.0129 & \\
**H-Optimus-0** & **0.5789** & 0.2561 & 0.3367 & 0.2778 & 0.1605 & 0.1228 & 0.2342 & 0.3143 & 0.1976 & 0.2754 \\  & \(\) 0.0898 & \(\) 0.0063 & \(\) 0.0428 & \(\) 0.0948 & \(\) 0.0822 & \(\) 0.0399 & \(\) 0.0737 & \(\) 0.083 & \(\) 0.0253 & \\
**Virchow2** & 0.5666 & 0.2972 & 0.2718 & 0.3038 & 0.1814 & 0.1208 & 0.2257 & 0.3017 & 0.2172 & 0.2762 \\  & \(\) 0.0848 & \(\) 0.037 & \(\) 0.0387 & \(\) 0.0184 & \(\) 0.0326 & \(\) 0.0526 & \(\) 0.0433 & \(\) 0.1199 & \(\) 0.019 & \\
**Virchow** & 0.5583 & 0.2744 & 0.3361 & 0.3389 & 0.1825 & 0.0955 & 0.2375 & 0.2897 & 0.2081 & 0.2801 \\  & \(\) 0.0876 & \(\) 0.0019 & \(\) 0.0377 & \(\) 0.0058 & \(\) 0.0087 & \(\) 0.0872 & \(\) 0.0731 & 0.4708 & \(\) 0.0234 & \\
**ResNet50** & 0.4453 & 0.2753 & 0.3432 & 0.413 & 0.2009 & 0.0669 & 0.2103 & 0.4001 & 0.203 & 0.2842 \\  & \(\) 0.0377 & \(\) 0.0682 & \(\) 0.0654 & \(\) 0.0814 & \(\) 0.0616 & \(\) 0.0646 & \(\) 0.0548 & \(\) 0.0637 & \(\) 0.0536 & \\
**CTransPath** & 0.4996 & 0.2895 & 0.3826 & 0.4038 & 0.1751 & 0.0909 & 0.2139 & 0.4026 & 0.2089 & 0.2963 \\  & \(\) 0.0594 & \(\) 0.0734 & \(\) 0.066 & \(\) 0.0043 & \(\) 0.0808 & \(\) 0.0438 & \(\) 0.0749 & \(\) 0.0867 & \(\) 0.0867 & \(\) 0.0867 \\
**Phikon** & 0.5259 & 0.2493 & 0.3594 & 0.3684 & 0.1697 & 0.1136 & **0.253** & 0.4224 & 0.2151 & 0.2974 \\  & \(\) 0.0791 & \(\) 0.1264 & \(\) 0.0707 & \(\) 0.1061 & \(\) 0.0562 & \(\) 0.0749 & \(\) 0.0483 & \(\) 0.0579 & \(\) 0.0416 & \\
**UNI** & 0.563 & 0.257 & 0.3768 & 0.3433 & 0.1839 & 0.1239 & 0.2395 & 0.3714 & 0.2236 & 0.2981 \\  & \(\) 0.0771 & \(\) 0.0819 & \(\) 0.0355 & \(\) 0.0556 & \(\) 0.0509 & \(\) 0.0534 & \(\) 0.0587 & \(\) 0.1098 & \(\) 0.0289 & \\
**CONCH** & 0.528 & **0.3604** & **0.4224** & **0.5079** & **0.2467** & **0.1443** & 0.2356 & **0.4957** & **0.2462** & **0.3541** \\  & \(\) 0.0794 & \(\) 0.0135 & \(\) 0.0773 & \(\) 0.0281 & \(\) 0.0045 & \(\) 0.0455 & \(\) 0.0837 & \(\) 0.0203 & \(\) 0.0349 & \\   

Table A11: **Overview of the HEST-Benchmark. Each task involves predicting the expression levels of the 50 most variable genes from 112\(\)112 \(\)m H&E-stained image patches centered on each spatial transcriptomics spot. The tasks are formulated as multivariate regression problems. The Oncotree code describes the cancer type diagnosed in samples, e.g., PAAD denotes pancreatic adenocarcinoma. Additional information is provided in the Appendix.**

    & **IDC** & **PRAD** & **PAAD** & **SKCM** & **COAD** & **READ** & **ccRCC** & **LUAD** & **LYMPH IDC** & **Average** \\ 
**ResNet50 (IN)** & 0.4646 & 0.3433 & 0.4017 & 0.4707 & 0.2892 & 0.0586 & 0.181 & 0.4967 & 0.2284 & 0.326 \\  & \(\) 0.0353 & \(\) 0.0168 & \(\) 0.0648 & \(\) 0.0834 & \(\) 0.0115 & \(\) 0.069 & \(\) 0.0502 & \(\) 0.01 & \(\) 0.0511 & \\
**CTransPath** & 0.4738 & 0.3514 & 0.4257 & 0.5304 & 0.2921 & 0.0996 & 0.2026 & 0.5225 & 0.234 & 0.348 \\  & \(\) 0.0394 & \(\) 0.0032 & \(\) 0.0701 & \(\) 0.073 & \(\) 0.0018 & \(\) 0.0766 & \(\) 0.0387 & \(\) 0.0063 & \(\) 0.0613 & \\
**Phikon** & 0.4704 & **0.3943** & 0.3988 & 0.5323 & 0.277 & 0.1451 & 0.213 & 0.542 & 0.2443 & 0.3575 \\  & \(\) 0.0672 & \(\) 0.0123 & \(\) 0.0598 & \(\) 0.0607 & \(\) 0.0098 & \(\) 0.0851 & \(\) 0.0062 & \(\) 0.0177 & \(\) 0.0632 & \\
**GigaPath** & 0.5222 & 0.3749 & 0.4415 & 0.5297 & 0.2876 & 0.1609 & 0.2207 & 0.5506 & 0.2464 & 0.3705 \\  & \(\) 0.0641 & \(\) 0.0103 & \(\) 0.058 & \(\) 0.0376 & \(\) 0.0099 & \(\) 0.0777 & \(\) 0.0402 & \(\) 0.0108 & \(\) 0.0526 & \\
**CONCH** & 0.5175 & 0.3784 & 0.4428 & 0.5766 & 0.3215 & 0.1431 & 0.1738 & 0.5581 & 0.2554 & 0.3742 \\  & \(\) 0.0602 & \(\) 0.0124 & \(\) 0.0657 & \(\) 0.0519 & \(\) 0.0062 & \(\) 0.0665 & \(\) 0.0544 & 0.0081 & \(\) 0.0605 & \\
**REMEDIS** & 0.5116 & 0.3526 & **0.4621** & 0.5885 & 0.319 & 0.1129 & 0.2303 & 0.562 & 0.2521 & 0.3768 \\  & \(\) 0.0594 & \(\) 0.0073 & \(\) 0.0555 & \(\) 0.0253 & \(\) 0.0101 & \(\) 0.0846 & \(\) 0.0393 & \(\) 0.0057 & \(\) 0.0601 & \\
**Virchow2** & 0.5378 & 0.3772 & 0.4237 & 0.5565 & 0.281 & **0.1779** & **0.2428** & 0.5641 & 0.2582 & 0.3799 \\  & \(\) 0.0685 & \(\) 0.007 & \(\) 0.0525 & \(\) 0.0152 & \(\) 0.0162 & \(\) 0.077 & \(\) 0.0361 & \(\) 0.0069 & \(\) 0.0504 & \\
**UNI** & 0.538 & 0.3513 & 0.451 & 0.6089 & 0.2921 & 0.1679 & 0.235 & 0.5357 & 0.2456 & 0.3806 \\  & \(\) 0.0603 & \(\) 0.0162 & \(\) 0.0387 & \(\) 0.0294 & \(\) 0.0191 & \(\) 0.0641 & \(\) 0.0381 & \(\) 0.0057 & \(\) 0.058 & \\
**Virchow** & 0.5309 & 0.3447 & 0.4448 & 0.6089 & 0.3275 & 0.1419 & 0.2307 & 0.5643 & **0.2617** & 0.3839 \\  & \(\) 0.0764 & \(\) 0.0117 & \(\) 0.0501 & \(\) 0.0165 & \(\) 0.00254 & \(\) 0.0669 & \(\) 0.0036 & \(\) 0.0091 & \(\) 0.0537 & \\
**UNIV1.5** & 0.555 & 0.3654 & 0.434 & 0.6025 & **0.336** & 0.1742 & 0.2166 & 0.5634 & 0.2515 & 0.3887 \\  & \(\) 0.0763 & \(\) 0.0098 & \(\) 0.0568 & \(\) 0.0385 & \(\) 0.0179 & \(\) 0.0568 & \(\) 0.0337 & \(\) 0.0054 & \(\) 0.0434 & \\
**H-Optimus-0** & **0.5564** & 0.3829 & 0.4445 & **0.6502** & 0.2922 & 0.1731 & 0.2402 & **0.5654** & 0.2555 & **0.3956** \\  & \(\) 0.0777 & \(\) 0.0049 & \(\) 0.0563 & \(\) 0.0326 & \(\) 0.0063 & \(\) 0.0777 & \(\) 0.0348 & \(\) 0.0084 & \(\) 0.0522 & \\   

Table A14: **HEST-Benchmark evaluated using XGBoost regression.** Model performance measured with Pearson correlation. Best is **bold**, second best is underlined.