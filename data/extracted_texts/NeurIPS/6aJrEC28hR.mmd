# Graph neural networks and non-commuting operators

Mauricio Velasco

Departamento de Informatica

Universidad Catolica del Uruguay

Montevideo, Uruguay

mauricio.velasco@ucu.edu.uy

&Kaiying O'Hare

Departament of Applied Mathematics and Statistics

Johns Hopkins University

Baltimore, Maryland

kohare3@jh.edu

Bernardo Rychtenberg

Departamento de Informatica

Universidad Catolica del Uruguay

Montevideo, Uruguay

bernardo.rychtenberg@ucu.edu.uy

&Soledad Villar

Departament of Applied Mathematics and Statistics

Johns Hopkins University

Baltimore, Maryland

svillar3@jhu.edu

###### Abstract

Graph neural networks (GNNs) provide state-of-the-art results in a wide variety of tasks which typically involve predicting features at the vertices of a graph. They are built from layers of graph convolutions which serve as a powerful inductive bias for describing the flow of information among the vertices. Often, more than one data modality is available. This work considers a setting in which several graphs have the same vertex set and a common vertex-level learning task. This generalizes standard GNN models to GNNs with several graph operators that do not commute. We may call this model graph-tuple neural networks (GtNN).

In this work, we develop the mathematical theory to address the stability and transferability of GtNNs using properties of non-commuting non-expansive operators. We develop a limit theory of graphon-tuple neural networks and use it to prove a universal transferability theorem that guarantees that all graph-tuple neural networks are transferable on convergent graph-tuple sequences. In particular, there is no non-transferable energy under the convergence we consider here. Our theoretical results extend well-known transferability theorems for GNNs to the case of several simultaneous graphs (GtNNs) and provide a strict improvement on what is currently known even in the GNN case.

We illustrate our theoretical results with simple experiments on synthetic and real-world data. To this end, we derive a training procedure that provably enforces the stability of the resulting model.

## 1 Introduction

Graph neural networks (GNNs)  are a widely-used and versatile machine learning tool to process different kinds of data from numerous applications, including chemistry , molecular geometry , combinatorial optimization , among many other. Such networks act on functions on the vertices of a graph (also called signals or vertex features) and use the structure of the graph as a powerful _inductive bias_ to describe the natural flow of information among vertices. One of the most common graph neural networks are based on graph convolutions , which generalize the notion of message passing. The typical architecture has building blocks which are polynomial functions of the adjacency matrix (or more generally of the shift operator) of a graph composed with componentwise non-linearities. Therefore, such networks implement the idea that the values of afunction at a vertex are related with the values at the immediate neighbors of the vertex and also with the values at the neighbors of its neighbors, etc.

Due to the significant practical success and diversity of applications of GNNs, there is a growing interest in understanding their mathematical properties. Researchers have delved into various theoretical aspects of MPNNs, including, for instance, expressivity [35; 47; 9; 1; 8], oversmoothing , multi-scale properties [18; 5], and model relaxations [12; 17]. One of the fundamental properties of graph neural networks is their remarkable _transferability property_, which intuitively refers to their ability to perform well in large networks when trained in smaller networks, thus _transfering_ knowledge from one to the other. This is in part possible because the number of parameters that defines a GNN is independent of the size of the input graphs. The idea is conceptually related to the algebraic notion of representation stability that has been recently studied in the context of machine learning models . More precisely, if two graphs describe similar phenomena, then a given GNN should have similar repercussions (i.e. similar effect on similar signals) on both graphs. In order to describe this property precisely, it is necessary to place signals and shift operators on different graphs (of potentially different sizes) in an equal footing to allow for meaningful comparisons and to characterize families of graphs describing "similar" phenomena. The seminal work  has used the theory of graphons to carry out these two steps, providing a solid theoretical foundation to the transferability properties of GNNs. The theory was further developed in [27; 41; 32; 23; 10], and was extended to other models in [6; 26; 43]. The transferability theory is very related to the stability or perturbation theory of GNNs that studies how GNN outputs change under small perturbations of the graph input or graph signal [42; 7; 13; 22], and conceptually related to the theory of generalization for GNNs [46; 11; 29; 33; 31] though the techniques are different.

In many practical situations a fixed collection of entities serves as common vertices to _several distinct graphs_ simultaneously that represent several modalities of the same underlying object. This occurs, for instance, in recommendation systems where the items can be considered as vertices of several distinct similarity graphs. It occurs in the analysis of social networks because individuals often participate in several distinct social/information networks simultaneously and in a wide array of multimodal settings.

The goal of this paper is to extend the mathematical theory of GNNs to account for multimodal graph settings. The most closely related existing work is the algebraic neural network theory of Parada-Mayorga, Butler and Ribeiro [38; 37; 4] who pioneer the use of algebras of non-commuting operators. The setting in this paper could be thought of as a special case of this theory. However, there is a crucial difference: whereas the main results in the articles above refer to the Hilbert-Schmidt norm, we define and analyze block-operator-norms on non-commutative algebras acting on function spaces. This choice allows us to prove stronger stability and transferability bounds that when restricted to classical GNNs improve upon or complement the state-of-the-art theory. In particular, we complement work in  by delivering bounds that do not exhibit no-transferable energy, and we complement results in  by providing stability bounds that do not require convergence. Our bounds are furthermore easily computable in terms of the networks' parameters improving on the results of  and in particular allow us to devise novel training algorithms with stability guarantees.

**Our contributions.** The main contribution of this work is a theoretical analysis for graph neural networks in the multimodal framework where each graph object (or graph tuple) can have several adjacency matrices on a fixed set of vertices. We call this model _graph-tuple neural networks (GtNNs)_. It generalizes GNNs and is naturally suited for taking into account information flows along paths traversing several distinct graphs. This architecture replaces the polynomials \(h(X)\) underlying graph convolutional neural networks with non-commutative polynomials \(h(X_{1},,X_{k})\) on the adjacency matrices of the \(k\) graphs in our tuple. More generally our approach via _operator networks_ gives a general and widely applicable parametrization for such networks. Our approach is motivated by the theory of switched dynamical systems, where recent algorithmic tools have improved our understanding of the iterative behaviour of non-commuting operators . Our main results are tight stability bounds for GtNNs and GNNs.

The second contribution of this article is the definition of graphon-tuple neural networks (WtNNs) which are the natural limits of (GtNNs) as the number of vertices grows to infinity. Graphon-tuple neural networks provide a good setting for understanding the phenomenon of transferability. Our main theoretical result is a _Universal transferability Theorem_ for graphon-graph transference which guarantees that _every_ graphon-tuple neural network (without any assumptions) is transferable over sequences of graph-tuples generated from a given graphon-tuple. This means that whatever a \(\) learns on a graph-tuple with sufficiently many vertices, instantaneously transfers with small error to all other graph-tuples of sufficiently large size provided the graph-tuples we are considering describe a "similar" phenomenon in the sense that they have a common graphon-tuple limit. Contrary to some prior results, under the convergence we consider in this paper, there is no no-transferable energy, meaning that the graphon-graph transferability error goes to zero as the size of the graph goes to infinity.

We show with simple numerical experiments that our theoretical bounds seem tight. In Section 7 we provide experiments on synthetic datasets and a real-world movie recommendation dataset where two graphs are extracted from incomplete tabular data. The stability bounds we obtain are within a small factor of the empirical stability errors. And remarkably, the bounds exhibit the same qualitative behavior as the empirical stability error. In order to perform this experiment we introduce a stable training procedure where linear constraints are imposed during GNN training. The stable training procedure could be considered of independent interest (see, for instance, ).

## 2 Preliminary definitions

For an integer \(n\) we let \([n]:=\{1,2,,n\}\). By a _graph \(G\) on a set \(V\)_ we mean an undirected, finite graph without self-loops with vertex set \(V(G):=V\) and edge set denoted \(E(G)\). A _shift matrix_ for \(G\) is any \(|V||V|\) symmetric matrix \(S\) with entries \(0 S_{ij} 1\) satisfying \(S_{ij}=0\) whenever \(i j\) and \((i,j) E(G)\).

Our main object of study will be signals (i.e. functions) on the common vertices \(V\) of a set of graphs so we introduce notation for describing them. We denote the _algebra of real-valued functions on the vertex set \(V\)_ by \([V]\). Any function \(f:V\) is completely determined by its vector of values so, as a vector space, \([V]^{|V|}\) however, as we will see later, thinking of this space as consisting of functions is key for understanding the neural networks we consider. Any shift matrix \(S\) for \(G\) defines a _shift operator_\(T_{G}:[V][V]\) by the formula \(T_{G}(f)(i)=_{j V}S_{ij}f(j)\).

The layers of graph neural networks (GNNs) are built from univariate polynomials \(h(x)\) evaluated on the shift operator \(T_{G}\) of a graph composed with componentwise non-linearities. If we have a \(k\)-tuple of graphs \(G_{1},,G_{k}\) with common vertex set \(V\) then it is natural to consider multivariate polynomials evaluated at their shift operators \(T_{G_{i}}\). Because shift operators of distinct graphs generally do not commute this forces us to design an architecture which is parametrized by _noncommutative polynomials_. The trainable parameters of such networks will be the coefficients of these polynomials.

**Noncommutative polynomials.** For a positive integer \(k\), let \( X_{1},,X_{k}\) be the _algebra of non-commutative polynomials in the variables \(X_{1},,X_{k}\)_. This is the vector space having as basis all finite length words on the alphabet \(X_{1},,X_{k}\) endowed with the bilinear product defined by concatenation on the basis elements. For example in \( X_{1},X_{2}\) we have \((X_{1}+X_{2})^{2}=X_{1}^{2}+X_{1}X_{2}+X_{2}X_{1}+X_{1}^{2} X_{1}^{2}+2X_{ 1}X_{2}+X_{2}^{2}\).

The basis elements appearing with nonzero coefficient in the unique expression of any element \(h(X_{1},,X_{k})\) are called the monomial words of \(h\). The degree of a monomial word is its length (i.e. number of letters). For example there are eight monomials of degree three in \( X_{1},X_{2}\), namely: \(X_{1}^{3},X_{1}^{2}X_{2},X_{1}X_{2}X_{1},X_{2}X_{1}^{2},X_{2}^{2}X_{1},X_{2}X_ {1}X_{2},X_{1}X_{2}^{2},X_{2}^{3}\). More generally there are exactly \(k^{d}\) monomial words of length \(d\) and \(-1}{k-1}\) monomial words of degree at most \(d\) in \( X_{1},,X_{k}\).

Noncommutative polynomials have a fundamental structural relationship with linear operators which makes them suitable for transference. If \(W\) is any vector space let \(End(W)\) denote the space of linear maps from \(W\) to itself. If \(T_{1},,T_{k} End(W)\) are any set of linear maps on \(W\) then the individual evaluations \(X_{i} T_{i}\) extend to a unique _evaluation homomorphism_\( X_{1},,X_{k} End(W)\), which sends the product of polynomials to the composition of linear maps. This relationship (known as universal freeness property) determines the algebra \( X_{1},,X_{k}\) uniquely. This proves that noncommutative polynomials are the only naturally transferable parametrization for our networks. For a polynomial \(h\) we denote the linear map obtained from evaluation as \(h(T_{1},,T_{k})\).

**Operator filters and non-commuting operator neural networks.** Using noncommutative polynomials we will define _operator networks_, an abstraction of both graph and graphon neural networks.

Operator networks will provide us with a uniform generalization to graph-tuple and graphon-tuple neural networks and allow us to describe transferability precisely.

The domain and range of our operators will be powers of a fixed vector space \(\) of signals. More formally, \(\) consists of real-valued functions on a fixed domain \(V\) endowed with a measure \(_{V}\). The measure turns \(\) into an inner product space (see [25, Chapter 2] for background) via the formula \( f,g:=_{V}fgd_{V}\) and in particular gives it a natural norm \(\|f\|:=( f,f)^{}\) which we will use throughout the article. In later sections the set \(\) will be either \([V]\) or the space \(L:=L^{2}()\) of square integrable functions in \(\) but operator networks apply much more generally, for instance to the spaces of functions on a manifold \(V\) used in geometric deep learning . By an _operator \(k\)-tuple_ on \(\) we mean a sequence \(:=(T_{1},,T_{k})\) of linear operators \(T_{j}:\). The tuple is _nonexpansive_ if each operator \(T_{j}\) has norm bounded above by one.

If \(h X_{1},,X_{k}\) is a noncommutative polynomial then the _operator filter defined by \(h=_{}c_{}X^{}\) and the operator tuple \(\)_ is the linear operator \((h,):\) given by the formula

\[h(T_{1},,T_{k})(f)=_{}c_{}X^{}(T_{1}, T_{k}) (f)\]

where \(X^{}(T_{1},,T_{k})\) is the composition of the \(T_{i}\) from left to right in the order of the word \(\). For instance if \(h(X_{1},X_{2}):=-5X_{1}X_{2}X_{1}+3X_{1}^{2}X_{2}\) then the graph-tuple filter defined by \(h\) applied to a signal \(f\) is \((h,T_{1},,T_{k})(f)=-5T_{1}(T_{2}(T_{1}(f)))+3T_{1}^{2}(T_{2}(f))\).

More generally, we would like to be able to manipulate several features simultaneously (i.e. to manipulate vector-valued signals) and do so by building block-linear maps of operators with blocks defined by polynomials. More precisely, if \(A,B\) are positive integers and \(H\) is a \(B A\) matrix whose entries are non-commutative polynomials \(h_{b,a} X_{1},,X_{k}\) we define the _operator filter determined by \(H\) and the operator tuple \(\)_ to be the linear map \((H,):^{A}^{B}\) which sends a vector \(x=(x_{a})_{a[A]}\) to a vector \((z_{b})_{b[B]}\) using the formula

\[z_{b}=_{a[A]}h_{b,a}(T_{1},,T_{k})(x_{a})\]

An _operator neural layer with ReLU activation_ is an operator filter composed with a pointwise non-linearity. This composition \((H,)\) yields a (nonlinear) map \((H,):^{A}^{B}\).

Finally an _operator neural network (ONN)_ is the result of composing several operator neural layers. More precisely if we are given positive integers \(_{0},,_{N}\) and \(N\) matrices \(H^{(j)}\) of noncommutative polynomials \(H^{(j)}_{b,a}:=h^{(j)}_{b,a}(X_{1},,X_{k})\) for \((b,a)[_{j+1}][_{j}]\) and \(j=0,,N-1\), the _operator neural network (ONN) determined by \(:=(H^{(j)})_{j=0}^{N-1}\) and the operator tuple \(\)_ is the composition \(^{_{0}}^{_{1}} ^{_{N}}\) where the \(j\)-th map in the sequence is the operator neural layer with ReLu activation \(_{j}(H^{(j)},):^{_{j}}^{_{j+1}}\). We write \((,):^{_{0}}^{_ {N}}\) to refer to the full composite function. See Appendix A for a discussion on the trainable parameters and the _transfer_ to other \(k\)-tuples. We conclude the Section with a key instance of operator networks:

An Example: Graph-tuple neural networks (GtNNs).Henceforth we fix a positive integer \(k\), a sequence \(G_{1},,G_{k}\) of graphs with common vertex set \(V\) and a given set of shift operators \(T_{G_{1}},,T_{G_{k}}\). We call this information a _graph-tuple \(:=(G_{1},,G_{k})\)_ on \(V\).

The _graph-tuple filter_ defined by a noncommutative polynomial \(h(X_{1},,X_{k}) X_{1},,X_{k}\) and \(\) is the operator filter defined by \(h\) evaluated at \(:=(T_{G_{1}},,T_{G_{k}})\) denoted \((h,):[V][V]\). Exactly as in Section 2 and using the notation introduced there, we define _graph-tuple filters_, _graph-tuple neural layers with ReLu activation_ and _graph-tuple neural networks (GtNN) on the graph-tuple \(\)_ as their operator versions when evaluated at the tuple \(\) above.

## 3 Perturbation inequalities

In this Section we introduce our main tools for the analysis of operator networks, namely _perturbation inequalities_. To speak about perturbations we endow the Cartesian products \(^{A}\) with max-norms

\[\|z\|_{}:=_{a[A]}\|z_{a}\|z=(z_{a})_{a[A]}^{A}.\]where the norm \(\|\|\) on the right-hand side denotes the standard \(L^{2}\)-norm on \(\) coming from the measure \(_{V}\) as defined in the previous section. Fix feature sizes \(_{0},,_{N}\) and matrices \(:=(H^{(j)})_{j=0,,N-1}\) of noncommutative polynomials in \(k\)-variables of dimensions \(_{j+1}_{j}\) for \(j=0,,N-1\) and consider the operator-tuple neural networks \((,):^{_{0}}^{_{n}}\) defined by evaluating this architecture on \(k\)-tuples \(\) of operators on the given function space \(\). A perturbation inequality for this network is an estimate on the sensitivity (absolute condition number) of the output when the operator-tuple and the input signal are perturbed in their respective norms, more precisely perturbation inequalities are upper bounds on the norm

\[\|(,)(f)-(,)(g) \|_{} \]

in terms of the input signal difference \(\|f-g\|_{}\) and the operator perturbation size as measured by the differences \(\|Z_{j}-W_{j}\|_{}\). The main result of this Section are perturbation inequalities that depend on easily computable constants, which we call _expansion constants_ of the polynomials appearing in the matrices \(\), allowing us to use them to obtain perturbation estimates for a given network and to devise training algorithms which come with stability guarantees. A key reason for the success of our approach is the introduction of appropriate norms for computations involving block-operators: If \(A,B\) are positive integers and \(z=(z_{a})_{a[A]}^{A}\) and \(R:^{A}^{B}\) is a linear operator then we define

\[\|R\|_{$}}:=_{z:\|z\|_{} 1} (\|R(z)\|_{}). \]

If \(h X_{1},,X_{k}\) is any noncommutative polynomial then it can be written uniquely as \(_{}c_{}x^{}\) where \(\) runs over a finite support set of sequences in the numbers \(1,,k\). For any such polynomial we define a set of \(k+1\)_expansion constants_ via the formulas

\[C(h):=_{}|c_{}| C_{j}(h):=_{}q_ {j}()|c_{}|j=1,,k\]

where \(q_{j}()\) equals the number of times the index \(j\) appears in \(\). Our main result is the following perturbation inequality, which proves that expansion constants estimate the perturbation stability of nonexpansive operator-tuple networks (i.e. those which satisfy \(\|T_{j}\|_{} 1\) for \(j=1,,k\)).

**Theorem 1**.: _Suppose \(\) and \(\) are two nonexpansive operator \(k\)-tuples. For positive integers \(A,B\) let \(H\) be any \(B A\) matrix with entries in \( X_{1},,X_{k}\). The operator-tuple neural layer with ReLu activation defined by \(H\) satisfies the following perturbation inequality: For any \(f,g^{A}\) and for \(m:=(\|f\|_{}\|g\|_{})\) we have_

\[\|(H,)(f)-(H,)(g)\|_ {}\\ \|f-g\|_{}_{b[B]}(_{a[A]}C(h_ {b,a}))+m_{b[B]}(_{a[A]}_{j=1}^{k}C_{j}(h_{b,a}) \|W_{j}-Z_{j}\|_{}). \]

The proof is in Appendix C. We apply the previous argument inductively to obtain a perturbation inequality for general graph-tuple neural networks by adding the effect of each new layer to the bound. More concretely if \(_{0},,_{N}\) denote the feature sizes of such a network and \(R_{W}\) and \(R_{Z}\) denote the network obtained by removing the last layer then

**Corollary 2**.: _Let \(m:=(\|R_{}(f)\|_{},\|R_{}(g)\|_{ })\). The end-to-end graph tuple neural network satisfies the following perturbation inequality:_

\[\|(,)(f)-(,)(g)\|_{}\\ \|R_{}(f)-R_{Z}(g)\|_{}_{b[ _{N}]}(_{a[_{N-1}]}C(h_{b,a}^{(N-1)}))+m_{b[ _{N}]}(_{a[_{N-1}]}_{j=1}^{k}C_{j}(h_{b,a}^{(N-1 )})\|W_{j}-Z_{j}\|_{}). \]Corollary 3 below shows that constraining expansion constants allows us to design operator-tuple networks of depth \(N\) whose perturbation stability scales _linearly_ with the network depth \(N\),

**Corollary 3**.: _Suppose \(\) and \(\) are two nonexpansive operator \(k\)-tuples. If the inequality_

\[_{b[_{j}+1]}(_{a[_{j}]}C(h^{(j)}_{b,a})) 1\]

_holds for \(j=0,,N-1\) then for \(m:=(\|f\|,\|g\|)\) we have:_

\[\|(,)(f)-(,)(g)\|\| f-g\|+m_{d=0}^{N-1}_{b[_{d}+1]}_{a[ _{d}]}_{j=1}^{k}C_{j}(h^{(d)}_{b,a})\|W_{j}-Z_{j}\|_{}. \]

## 4 Graphons and graphon-tuple neural networks (WtNNs).

In order to speak about transferability precisely, we have to address two basic theoretical challenges. On one hand we need to find a space which allows us to place signals and shift operators living on different graphs in equal footing in order to allow for meaningful comparisons. On the other hand objects that are close in the natural norm in this space should correspond to graphs describing "similar" phenomena. As shown in , both of these challenges can be solved simultaneously by the theory of graphons. A graphon is a continuous generalization of a graph having the real numbers in the interval \(\) as vertex set. The _graphon signals_ are the space \(L\) of square-integrable functions on \(\), that is \(L:=L^{2}()\). In this Section we give a brief introduction to graphons and define _graphon-tuple neural networks (WtNN)_, the graphon counterpart of graph-tuple neural networks. Our first result is Theorem 4 which clarifies the relationship between finite graphs and signals on them and their induced graphons and graphon signals respectively allowing us to make meaningful comparisons between signals on graphs with distinct numbers of vertices. The space of graphons has two essentially distinct natural norms which we define later in this Section and review in Appendix B. Converging sequences under such norms provide useful models for families of "similar phenomena" and Theorem 5 describes explicit sampling methods for using graphons as generative models for graph families converging in both norms.

**Comparisons via graphons.** A _graphon_ is a function \(W:\) which is measurable and symmetric (i.e. \(W(u,v)=W(v,u)\)). A _graphon signal_ is a function \(f L:=L^{2}()\). The _shift operator_ of the graphon \(W\) is the map \(T_{W}:L L\) given by the formula

\[T_{W}(f)(u)=_{0}^{1}W(u,v)f(v)dv\]

where \(dv=d(v)\) denotes the Lebesgue measure \(\) in the interval \(\).

A _graphon-tuple_\(W_{1},,W_{k}\) consists of a sequence of \(k\) graphons together with their shift operators \(T_{W_{i}}:L L\). Exactly as in Section 2 and using the notation introduced there, we define \((A,B)\)_graphon-tuple filters_, \((A,B)\)_graphon-tuple neural layers with ReLu activation_ and _graphon-tuple neural networks (WtNN)_ as their operator versions when evaluated at the \(k\)-tuple \(:=(T_{W_{1}},,T_{W_{k}})\).

For instance, if we are given positive integers \(_{0},,_{N}\) and matrices \(H^{(j)}\) with entries given by noncommutative polynomials \(H^{(j)}_{b,a}:=h^{(j)}_{b,a} X_{1},,X_{k}\) for \((b,a)[_{j+1}][_{j}]\) and \(j=0,,N-1\), the _graphon-tuple neural network (WtNN) defined by \(:=(H^{(j)})_{j=0}^{N-1}\) and \(\)_ will be denoted by \((,):L^{_{0}} L^{_{N}}\).

Next we focus on the relationship between (finite) graphs and graphons. Our main interest are signals (i.e. functions) on the common vertex set \(V\) of all the graphs which we think of as a discretization of the graphon vertex set \(\). More precisely, for every integer \(n\) we fix a collection of \(n\) intervals \(I^{(n)}_{j}:=[,)\) covering \([0,1)\) and \(n\) vertices \(v^{(n)}_{j}:= I_{j}\) which constitute the set \(V^{(n)}\).

To compare functions on different \(V^{(n)}\) we will use an _interpolation_ operator \(i_{n}\) and a _sampling_ operator \(p_{n}\). The _interpolation operator_\(i_{n}:[V^{(n)}] L\) extends a set of values at the points of \(V^{(n)}\)to a piecewise-constant function in \(\) via \(i_{n}(g)(u):=_{i=1}^{n}g(v_{i}^{(n)})1_{I_{i}^{(n)}}(u)\) where \(1_{Z}(x)\) denotes the \(\{0,1\}\) characteristic function of the set \(Z\). The _sampling operator_\(p_{n}:L[V^{(n)}]\) maps a function \(f\) to its conditional expectation with respect to the \(I_{j}\), namely the function \(g[V^{(n)}]\) given by the formula \(g(v_{j}):=_{I_{j}}f(v)dv/(I_{j})=n_{I_{j}}f(v)dv\). The sampling and interpolation operators satisfy the identities \(p_{n} i_{n}=id_{[V^{(n)}]}\), and \(i_{n}(p_{n}(f))\) is the piecewise function which on each interval \(I_{j}\) has constant value equal to the average of \(f\) on \(I_{j}\). Note that \(i_{n} p_{n}(f)\) approaches any continuous function \(f\) as \(n\).

Any graph \(G\) with \(n\) vertices and shift matrix \(S_{ij}\) induces a (piecewise constant) graphon \(W_{G}\). The _graphon induced by \(G\)_ is given by the formula

\[W_{G}(x,y)=_{i=1}^{n}_{j=1}^{n}S_{ij}1_{I_{i}^{(n)}}(x)1_{I_{j}^{(n)}} (y)\]

The following Theorem clarifies the relationship between the shift operator of a graph and that of its induced graphon and how this basic relationship extends to neural networks. Part \((2)\) will allow us to compare graph-tuple neural networks on different vertex sets by comparing their induced graphon-tuple networks (the proof is in Appendix C).

**Theorem 4**.: _For every graph-tuple \(G_{1},,G_{k}\) on vertex set \(V^{(n)}\) and their induced graphons \(W_{j}:=W_{G_{j}}\) the equality_

\[T_{W_{j}}=i_{n}}}{n} p_{n}\]

_holds. Moreover, this relationship extends to networks: given feature sizes \(_{0},,_{N}\) and matrices \(H^{(j)}\) of noncommutative polynomials having no constant term and of compatible dimensions \(_{j+1}_{j}\) for \(j=0,,N-1\) the graphon-tuple neural network \((,):L^{_{0}} L^{_{N}}\) and the normalized graph-tuple neural network \((,/n):[V]^{_{0}}[V]^{_{N}}\) satisfy the identity_

\[(H,)=i_{n}(H,}/n) p_{n}\]

_where \(p_{n}\) and \(i_{n}\) are applied to vectors componentwise._

**Graphon norms.** The space of graphons is infinite-dimensional and therefore allows for several norms. In infinite-dimensional spaces it is customary to speak about _equivalent norms_, meaning pairs that differ by multiplication by a constant, but also about the coarser relation of _topologically equivalent_ norms (two norms are topologically equivalent if a sequence converges in one if and only if it converges in the other). Here we describe two specific norms of interest and describe explicit mechanisms for producing converging sequences in the operator norm.

The most fundamental norm on graphons is \(\|W\|_{}:=|_{U,V}_{U V}W(u,v)dudv|\). Known as the cut norm, its importance stems from the fact that two graphons differing by a small cut norm must have similar induced subgraphs in the sense of the counting Lemma of Lovasz and Szegedi (see [30, Lemma 10.23] for details).

As an analytic object however, the cut norm is often unwieldy, so it is typically bounded via more easily computable norms. More precisely, the space of graphons admits two topologically inequivalent norms represented by the operator and Hilbert-Schmidt norms of graphon shift operators respectively (Example 6 shows that they are indeed inequivalent and why this is important in the present context).

Recall that the operator is defined by \(\|T_{W}\|_{}:=_{\|f\|,\|g\| 1}|_{0}^{1}_{0}^{1 }W(u,v)f(u)g(v)dudv|\) which is the induced norm of \(T_{W}\) as operator from \(L^{2}()\) to \(L^{2}()\). It is topologically equivalent to the cut norm (see Appendix B). The Hilbert-Schmidt (HS) norm of \(T_{W}\) is the \(^{2}\)-norm of the eigenvalues of \(T_{W}\) or equivalently the norm \(\|W\|_{L^{2}}\) thinking of \(W\) as a function in the square.

**Graphons as generative models.** Given a graphon \(W\) we explicitly construct families of graphs of increasing size which have \(W\) as limit. The family associated to a graphon provides a practical realization of the intuitive idea of a collection of graphs which "represent a common phenomenon".

Explicitly constructing such families is of considerable practical importance since they provide us with a controlled setting in which properties like transferability can be tested experimentally over artificially generated data.

Assume \(W(x,y)\) is a given graphon. For every integer \(n\) we fix a finite set of equispaced vertices as above and a collection of intervals \(I_{j}:=[v_{j},v_{j+1})\) for \(j=1,,n-1\) and \(I_{n}:=[0,v_{1})[v_{n},1]\). We will produce two kinds of undirected graphs with vertex set \(V^{(n)}:=\{v_{1},,v_{n}\}\):

1. A deterministic weighted graph, the _weighted template graph_\(H_{n}\) with vertex set \(V^{(n)}\) and shift operator \(S(v_{i},v_{j}):= I_{j}}W(x,y)dxdy}{(I_{i} I_{j})}\) on the edge \((v_{i},v_{j})\).
2. A random graph, the _graphon-Erdos-Renyi_ graph \(G_{n}\) with vertex set \(V^{(n)}\) and shift operator \(S(v_{i},v_{j})\{0,1\}\) sampled from a Bernoulli distribution with probability \(W(v_{i},v_{j})\), which is independent for distinct pairs of vertices.

The main result in this Section is that, under mild assumptions on the function \(W\), the weighted template graphs and the random graphon-Erdos-Renyi have induced shift operators converging to \(T_{W}\) in suitable norms (see Appendix C for a proof). Note that the second part of the theorem can be seen as a consequence of the analysis in  as well.

**Theorem 5**.: _For any positive integer \(n\) let \(^{(n)}\) and \(^{(n)}\) be the graphons induced by \(H_{n}\) and \(G_{n}\). The following statements hold_

1. _If_ \(W\) _is continuous then_ \(\|T_{W}-T_{^{(n)}}\|_{} 0\)__
2. _If_ \(W\) _is Lipschitz continuous then_ \(\|T_{W}-T_{^{(n)}}\|_{} 0\) _almost surely._

**Example 6**.: _Fix \(p(0,1)\) and let \(W(x,y)=p\) for \(x y\) and zero otherwise. The graphon Erdos-Renyi graphs \(G^{(n)}\) constructed from \(W\) as in \(()\) above are precisely the usual Erdos-Renyi graphs. Theorem 5 part \(()\) guarantees that \(\|T_{W}-T_{^{(n)}}\|_{} 0\) almost surely so this is a convergent graph family in the operator norm. By contrast we will show that the sequence of \(T_{^{(n)}}\) does not converge _to \(T_{W}\) in the Hilbert-Schmidt norm by proving that \(\|T_{^{(n)}}(x,y)-T_{W}(x,y)\|_{}>(p,1-p)>0\) almost surely. To this end note that for every \(n\) and every \((x,y)^{2}\) with \(x y\) the difference \(|W_{^{(n)}}(x,y)-W(x,y)|(p,1-p)\) since the term on the left is either \(0\) or \(1\). We conclude that \(\|T_{^{(n)}}(x,y)-T_{W}(x,y)\|_{}=\|W_{^{(n)}}(x,y)-W( x,y)\|_{L^{2}(^{2})}(p,1-p)>0\) for every \(n\) and therefore the sequence fails to converge to zero almost surely._

The previous example is important for two reasons. First it shows that the operator and Hilbert-Schmidt norm are not topologically equivalent in the space of graphons. Second, the simplicity of the example shows that for applications to transferability, we should focus on the operator norm. More strongly, it proves that transferability results that depend on the Hilbert-Schmidt norm are not applicable even to the simplest families of examples, namely Erdos-Renyi graphs.

## 5 Universal transferability

Our next result combines perturbation inequalities for graphon-tuple networks and Theorem 4 which compares graph-tuple networks and their induced graphon-tuple counterparts resulting in a _transferability_ inequality. As a corollary of this inequality we prove a universal transferability result which shows that _every_ architecture is transferable in a converging sequence of graphon-tuples, in the sense that the transferability error goes to zero as the index of the sequence goes to infinity. This result is interesting and novel even for the case of graphon-graph transferability (i.e. when \(k=1\)).

**Theorem 7**.: _Let \(\) be a graphon-tuple and let \(\) be a graph-tuple with equispaced vertex set \(V\). Let \(H\) be any \(B A\) matrix with entries in \((X_{1},,X_{k})\) and let \((H,):L^{A} L^{B}\) (resp \((H,):[V][V]\) ) denote the graphon-tuple neural layer (resp. graph-tuple neural layer) with ReLu activation defined by \(H\). If \(_{i}\) denotes the graphon induced \(y\)\(G_{i}\) then for every \(f L\)_the_ transferability error _for \(f\) satisfies_

\[\|(H,})(f)-i_{V}(H, })(p_{V}(f))\|_{}\] \[\|f-i_{V} p_{V}(f)\|_{}_{b[ B]}(_{a[A]}C(h_{b,a}))+\|f\|_{b[B]}(_{ a[A]}_{j=1}^{k}C_{j}(h_{b,a})\|T_{W_{j}}-T_{_{j}}\|_{} ).\]

We are now able to prove the following _Universal transferability_ result:

**Theorem 8**.: _Suppose \(G^{(N)}:=(G^{(N)}_{1},,G^{(N)}_{k})\) is a sequence of graph-tuples having vertex set \(V^{(N)}\). If the vertex set is equispaced for every \(N\) and the sequence converges to a graphon-tuple \(\) in the sense that \(\|T_{^{(N)}_{j}}-T_{W_{j}}\|_{} 0\) as \(N\) for \(j=1,,k\) then every graphon-tuple neural network on \(\) transfers. More precisely, for every neural network architecture \((,)\) and every essentially bounded function \(f L^{2}()\) the quantity_

\[\|(,})(f)-i_{V^{(N)}}(, |}})(p_{V^{(N)}}(f))\|_{}\]

_converges to zero as \(N\). Furthermore the convergence is uniform among functions \(f\) with a fixed Lipschitz constant._

## 6 Training with stability guarantees

Following our perturbation inequalities (i.e., Theorem 1 and Corollary 3) we propose a training algorithm to obtain a GtNN that enforces stability by constraining all the expansion constants \(C(h)\) and \(C_{j}(h)\). Consider a GtNN \((,_{G})\) and nonexpansive operator \(k\)-tuples \(_{G}\). Denote the set of \(k+1\) expansion constants for each layer \(d=0,,N-1\) as

\[C(H^{(d)}):=_{b[_{d+1}]}_{a[_{d}]}C(h^{(d)}_{b,a})  C_{j}(H^{(d)}):=_{b[_{d+1}]}_{a[ _{d}]}h^{(d)}_{b,a}j=1,,k,\]

and write \(()=(C(H^{(d)}))_{d=0}^{N-1}\) and \(_{j}()=(C_{j}(H^{(d)}))_{d=0}^{N-1}\) for \(j=1,,k\). Given \(k+1\) vectors of target bounds \(:=(C^{(d)})_{d=0}^{N-1}\) and \(_{j}:=(C^{(d)}_{j})_{d=0}^{N-1}\) for \(j=1,,k\), and training data \((x_{i},y_{i})^{_{0}}^{_{N}}\) for \(i I\), we train the network by a constrained minimization problem

\[_{c}_{i I}(((c),_{G})(x_{i }),y_{i})((c)),_{j} ((c))_{j}j=1,,k,\]

where \((,)\) is any nonnegative loss function depending on the task, and \(c\) denotes all the polynomial coefficients in the network. If we pick \(\) to be an all ones vector (or smaller), by Corollary 3, the perturbation stability is guaranteed to scale linearly with the number of layers \(N\).

To approximate the solution of the constrained minimization problem we use a penalty method,

\[_{c}_{i I}(((c),_{G})(x_{i }),y_{i})+[p(((c))-)+_{j=1}^{k}p(_{j}( (c))-_{j})], \]

where \(p()\) is a componentwise linear penalty function \(p()=(p(C^{(d)}))_{d=0}^{N-1}\) with \(p(C^{(d)})=(0,C^{(d)})\). The stable GtNN algorithm picks a fixed large enough penalty coefficient \(\) and trains the network with local optimization methods.

## 7 Experimental data and numerical results

We perform three experiments1: (1) we test the tightness of our theoretical bounds on a simple regression problem on a synthetic dataset consisting of two weighted circulant graphs (see Figure1 and Appendix D.1 for details) (2) we assess the transferability of the same model (Appendix D.2), and (3) we run experiments on a real-world dataset of a movie recommendation system where the information is summarized in two graphs via collaborative filtering approaches  and it is combined to infer ratings by new users via the GtNN model (see Figure 2 and Appendix D.3).

## 8 Conclusions

In this paper, we introduce graph-tuple networks (GtNNs), a way of extending GNNs to a multi-modal graph setting through the use tuples of non-commutative operators endowed with appropriate block-operator norms. We show that GtNNs have several desirable properties such as stability to perturbations and a universal transfer property on convergent graph-tuples, where the transferability error goes to zero as the graph size goes to infinity. Our transferability theorem improves upon the current state-of-the-art even for the GNN case. Furthermore, our error bounds are expressed in terms of computable quantities from the model. This motivates a novel algorithm to enforce stability during training. Experimental results show that our transferability error bounds are reasonably tight, and that our algorithm increases the stability with respect to graph perturbation. They also suggest that the transferability theorem holds for sparse graph tuples. Finally, the experiments on the movie recommendation system suggest that allowing for architectures based on GtNNs is of potential advantage in real-world applications.

Figure 1: We assess the tightness of our theoretical results on a regression problem on a synthetic data toy example consisting of two weighted circulant graphs. See Appendix D.1 for details. **(Left)** Numerical stability bound \(C(h)\) (dashed) and stability metrics \(\|h()\|_{}\) (solid) with respect to input signal perturbation as a function of the number of epochs for both the standard (1-layer) GtNN (orange) and (1-layer) stable GtNN (blue). **(Middle)** Similar plot for the stability metrics with respect to the graph perturbation \(\|h()-h()\|_{}\) and its upper bound (Lemma 12 part 2 and 3b). For this plot we take \(=\), and \(\) is a random perturbation from \(\) with \(\|Z_{1}-W_{1}\|_{}\|Z_{2}-W_{2}\|_{} 0.33\). **(Right)** For all four models, compute the 2-norm of the vector of output perturbations from Equation (1) over the test set for various sizes of graph perturbation \((\|T_{1}-W_{1}\|_{}+\|T_{2}-W_{2}\|_{})/2\), where the additive graph perturbation \(T_{1}-W_{1}\) and \(T_{2}-W_{2}\) are symmetric matrices with iid Gaussian entries. In addition, each \(T_{j}\) and \(W_{j}\) are normalized such that \(\|T_{j}\|_{} 1\) and \(\|W_{j}\|_{} 1\) for \(j=1,2\), so they are nonexpansive operator-tuple networks. **(All)** We observe that adding stability constraints does not affect the prediction performance: the testing R squared value for GtNN is \(0.6866\), while for stable GtNN is \(0.6543\).

Figure 2: This is an experiment on the MovieLens 100k database, a collection of movie ratings given by a set of 1000 users  to 1700 movies. Using collaborative filtering techniques  we extract two weighted graphs that we use to predict ratings of movies by user from a held out test set. See details in Appendix D.3. We report the mean squared error (MSE) in the test set as a function of the number of training iterations **(Left)** from \(0\) to \(500\) and **(Right)** from \(0\) to \(1500\) for the movie recommendation system experiments. We compare the two models GtNN on the tuple of two graphs (20NN) and GNN on the best single graph between those two (GNN) on various ridge-regularized versions (the legend contains the values of the chosen regularization constants).