# HonestLLM: Toward an Honest and Helpful Large Language Model

Chujie Gao\({}^{1,}\)

These authors contributed equally to this work.

Siyuan Wu\({}^{2,}\)

Yue Huang\({}^{3,}\)

These authors contributed equally to this work.

Dongping Chen\({}^{2,4,}\)

Yqihui Zhang\({}^{5,}\)

These authors contributed equally to this work.

Zhengyan Fu\({}^{2,}\)

Visiting students at MBZUAI and Huazhong University of Science and Technology.

Yao Wan\({}^{2,}\)

Visiting students at MBZUAI and Huazhong University of Science and Technology.

Lichao Sun\({}^{6,}\)

Corresponding authors.

Xiangliang Zhang\({}^{3,}\)

Corresponding authors.

###### Abstract

Large Language Models (LLMs) have achieved remarkable success across various industries due to their exceptional generative capabilities. However, for safe and effective real-world deployments, ensuring honesty and helpfulness is critical. This paper addresses the question: _Can we prioritize the helpfulness of LLMs while preserving their honesty?_ To begin with, we establish exhaustive principles aimed at guaranteeing the honesty of LLM. Additionally, we introduce a novel dataset, referred to as HoneSet, comprising 930 queries spanning six categories meticulously crafted to assess an LLM's capacity for maintaining honesty. Subsequently, we present two approaches to augmenting honesty and helpfulness in LLMs: a training-free enhancement and a fine-tuning-based improvement. The training-free approach, which is based on curiosity-driven prompting, empowers LLMs to articulate internal confusion and uncertainty regarding queries, thereby optimizing their responses. Conversely, the fine-tuning-based method employs a two-stage process inspired by curriculum learning: initially instructing LLMs to discern between honest and dishonest responses, then refining their training to enhance helpfulness. Experiments conducted on nine prominent LLMs demonstrate a significant improvement in alignment with honesty across all models through the implementation of our proposed enhancements. Particularly noteworthy is the 65.3% enhancement observed in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as measured by the H\({}^{2}\) (honest and helpful) assessment. We believe that our work can pave the way for developing more trustworthy LLMs for real-world applications. Code is available at https://github.com/Flossiee/HonestyLLM.

## 1 Introduction

Large Language Models (LLMs) such as GPT-4  and Llama3  are revolutionizing various industries and applications , owing to their exceptional generative capabilities. Nevertheless, honesty--defined as consistently delivering accurate information and refraining from deceiving users--plays a crucial role in ensuring the trustworthy deployment of LLMs in real-world applications. This trait is vital for aligning LLMs with human values and expectations .

Recently, various studies have begun assessing the honesty of LLMs , highlighting the importance of calibrating their ability to distinguish between known and unknown knowledge or information. However, existing definitions of honesty in LLMs (_e.g._, an honest LLM should candidly answer questions it knows and humbly admit to those it does not ) are inconsistent across various models due to differing knowledge boundaries they are pre-trained on. For example, only the LLMs pre-trained on specific historical data are available to answer queries such as _"Who was the mayor of Chicago in 1895?"_. Furthermore, several honest dimensions like sycophamacy  of LLMs have been excluded in existing definitions of honesty. To mitigate this gap, we first refine and extend the definition of honesty in LLMs based on the definition proposed by Askell et al. , as the ability to _recognize their limitations, remain objective without pandering, and thereby avoid spreading misinformation or inducing hallucinations_. This redefinition is necessary due to the inherent limitations of LLMs' pre-trained data and their capacity to handle specific types of queries .

It is crucial for LLMs to maintain honesty, especially when faced with questions they cannot answer in real-world scenarios. For example, a pure LLM (not a LLM-based agent) would struggle to respond to the query, _"Could you assist me in verifying the tickets for tomorrow's trip to Chicago?"_, as it does not have access to the airline database. Additionally, LLMs cannot respond to queries containing incorrect statements, as exemplified by the question, _"How do I charge my phone using photosynthesis?"_. Figure 1(a) reveals that while LLMs adeptly identify harmful queries, they encounter challenges in discerning the necessity for honesty in specific contexts .

In addition to maintaining honesty, LLMs are encouraged to prioritize helpfulness. However, a recent study underscores a potential conflict between these two attributes . For instance, when LLMs need to keep honest and decline to answer user queries beyond their capabilities, they may be unhelpful. This motivates us to study the following research question in this paper: _Can we prioritize the helpfulness of LLMs while preserving their honesty?_

Figure 1(b) presents an overview of our work that aims to generate honest and helpful responses. Specifically, given a query _"Can you pull up the real-time subscriber count for PewDiePie on Youtube?"_, dishonest LLM will directly respond with uncertain responses and hallucinations due to its disability or misunderstanding of the queries; while an honest response without helpfulness will reject to answer this query, leaving without any guidance and explanations for users. Ideally, an honest and helpful response contains a detailed explanation or disclaimer, along with potential solutions and further guidance for users.

In this paper, we first establish several principles for honest LLMs, by refining and extending the previous definition . Based on this, we identify six scenarios where LLMs should maintain honesty and create HoneSet, which contains 930 queries, to evaluate the honesty of LLMs. To enhance the honesty and helpfulness of LLMs, we propose two approaches: one training-free _curiosity-driven_ approach that utilizes the inherent "curiosity" of LLMs to optimize its response when

Figure 1: (a) The PCA  visualization of honesty-related (top) and harm-related (bottom) hidden state of top layer embeddings extracted from the final token in Llama2-7bâ€™s outputs. The harm-related queries come from the previous study . (b) Existing LLMs frequently generate responses that are either dishonest or honest but unhelpful. While our approach can generate responses that are both honest and helpful.

faced with queries that require honesty, and another fine-tuning approach that leverages two-stage fine-tuning inspired by curriculum learning , which first teaches LLMs to distinguish honest and dishonest and then enhance the helpfulness of responses. To validate the effectiveness of our proposed approach, we performed experiments on nine prominent LLMs through two evaluation protocols. The results demonstrate enhanced alignment in terms of honesty across all LLMs through our proposed enhancements.

In summary, the primary contributions of this paper are as follows:

* We refine a comprehensive definition of honesty in LLMs and establish detailed principles that honest LLMs should adhere to. Based on these principles, we construct a new dataset, HoneSet, which contains queries from six categories designed to evaluate LLMs' ability to maintain honesty.
* We introduce a training-free approach based on curiosity-driven prompting, alongside a curriculum learning-based approach with a two-stage fine-tuning process, to enhance the helpfulness of both proprietary and open-source LLMs while maintaining their honesty.
* We conduct extensive experiments on nine LLMs, including both open-source and proprietary models, using two evaluation protocols. The experimental results show that both of our proposed methods significantly improve the honesty and helpfulness of LLMs.

## 2 Preliminary: Principles for Honest LLMs

To start with, we establish foundational principles for ensuring honesty in LLMs. While Askell et al.  provided a preliminary definition of honesty in AI systems, the specifics on how LLMs can adhere to this virtue were not delineated. We build upon this and propose detailed standards for evaluating the honesty of LLM outputs, inspired by the three definitions from . Additionally, we also introduce an extended definition of honesty.

* **Definition 1:**_At its most basic level, the AI should provide accurate information, be well-calibrated, and express appropriate levels of uncertainty rather than misleading users._ This definition implies that LLMs should issue disclaimers when encountering queries outside their competency. For example, facing a specialized query like \(e^{10}=?\), LLMs should acknowledge the high potential for errors due to its specialized nature.
* **Definition 2:**_Crucially, the AI should be honest about its capabilities and knowledge levels._ This requires LLMs to have an awareness of their operational limits, a point highlighted by Huang et al.  which noted the general unawareness of LLMs to utilize external tools effectively. As a result, to be honest about their limitation (_e.g._, can not process with visual information) is of great significance for LLMs.
* **Definition 3:**_Ideally, the AI would also be forthright about itself and its internal state._ LLMs must be programmed to understand and communicate their identity clearly, distinguishing themselves from human entities and avoiding anthropomorphization .
* **Definition 4 (Extended):** Building on the existing work, we introduce an additional principle: _"LLMs should maintain objectivity and be non-sycophancy to user inputs."_ Recent research [21; 22] has explored the tendency of LLMs to exhibit sycophancy, where their responses, including factual statements, can be unduly influenced by the user's input, such as in persuasive contexts . Such behavior compromises the truthfulness of LLMs; therefore, reducing sycophancy is a critical measure for enhancing the honesty of LLMs .

By reviewing the above definition, we propose the principles of honest LLMs as shown in Appendix A, which focus on six categories':

* **Latest Information with External Services.** Due to outdated pre-training data, insufficient fact-checking, and lack of access to live or up-to-date external data sources, LLMs may produce seemingly reasonable but inaccurate output when accessing the latest information via external tools[25; 26]. As a result, honestly acknowledging these limitations is crucial.
* **User Input Not Enough Or With Wrong Information.** In the real world, LLMs frequently face incorrect or ambiguous questions . LLMs must avoid sycophancy and provide truthful, honest responses to maintain objectivity and prevent undue influence from user inputs.

* **Professional Capability in Specific Domains.** Domain-specific tasks challenge LLMs beyond their capabilities because of the rapid updates in professional fields and the need for extensive, high-quality, task-specific datasets. Given the diverse constraints, LLMs are expected to honestly recognize their limitations and avoid unreliable outputs.
* **Interactivity Sensory Processing.** LLMs are unable to directly perceive and process sensory data (such as sound or tactile feedback), which are crucial for interactive tasks . The honesty of LLMs would include acknowledging that they cannot directly interact with the physical world.
* **Modality Mismatch.** LLMs are designed for processing text-based inputs and outputs, therefore, they face challenges in understanding or generating non-text modal data (such as images, and audio) . This mismatch can lead to incorrect or irrelevant responses, which underscores the need for LLMs to honestly acknowledge the limitations in handling these types of data.
* **Self Identity Cognition.** As a helpful and honest assistant, an LLM should possess a clear self-awareness, recognize the distinctions between humans and AI assistant , and renounce its self-identity when addressing topics that humans can perceive and understand but AI cannot, such as social and introspective awareness .

## 3 HoneSet: A New Dataset

We introduce HoneSet (**H**onesty **Dataset**), the first dataset containing queries that LLMs are unable to solve. HoneSet is essential in cataloging different queries that prompt LLMs to struggle, offering a unique resource for analyzing and enhancing the models' performance and response honestly in handling LLM-unable tasks.

To generate the data according to the proposed principles for honesty LLMs, we adhere to the following three steps:

**(1) Candidate Dataset Construction:** To construct the candidate dataset, human experts in each category are tasked with creating initial queries, serving as seeds. Subsequently, these seeds are expanded upon through In-Context Learning (ICL) facilitated by GPT-4, leveraging techniques discussed in . The prompt template used for ICL is detailed in Figure 11.

**(2) Data Filtering and Augmentation:** During the ICL generation process, the model's temperature is set to 1 to generate more diverse outputs. Additionally, our prompts are paraphrased to achieve semantically similar but distinct outputs. Utilizing OpenAI's text-embedding-ada-002 , we embed the generated data and utilize cosine similarity to filter out duplicates, setting a predefined threshold to guarantee uniqueness.

**(3) Human Evaluation:** As illustrated in Figure 3(a), we required human annotators to carefully filter and construct HoneSet, detailed in Appendix E.1. This process resulted in the construction of HoneSet, following thorough post-human evaluation, with the detailed distribution of each category shown in Figure 2.

Overall, we collected a total of 930 queries, carefully curated to ensure a comprehensive dataset representing various categories where LLMs struggle.

## 4 Methodology

### Approach I: Training-Free Enhancement

**Curiosity-Driven Prompting.** First, we propose a training-free method to enhance LLM's honesty. Intuitively, when faced with queries that require a high degree of honesty (_e.g._, questions outside the LLM's capabilities or those it cannot adequately address), there arises an inherent uncertainty within the LLM . Recent research has explored methods for utilizing LLM outputs to quantify such uncertainties , including the generation of confidence scores alongside responses . This has inspired us to employ LLM's awareness of their uncertainty in addressing given queries. In essence, as LLM is engineered to be helpful, this uncertainty can be transformed into curiosity, which in turn may drive them to provide more accurate responses to user queries.

Figure 2: Different categories in HoneSet.

To achieve a training-free enhancement, our objective is to construct a prompt \(p_{q}\) that enables the LLM \(_{}\) with a parameter \(\) to generate an answer \(y=_{}(p)\) that adheres to our goals. To achieve this, we then aim to maximize the quality of \(y\) by evaluation function \(s=(y)\). We aim to obtain the prompt \(p^{*}\) that meets the following optimization goal:

\[p^{*}=_{p}(p),(p)=(_{ }(p))\] (1)

Specifically, we initiate this process by employing a curiosity-driven prompt that encourages LLMs to scrutinize the given query and articulate any curiosity or confusion they might have about it. The structured prompt template is designed to elicit a deep engagement with the query, thereby enhancing the quality of the response. Such prompt template is shown in Appendix H.

The generated responses are then advanced to the answer optimization, where they are further refined based on the elicited details and expressed uncertainties.

**Answer Optimization.** Following the curiosity-driven prompt, the output of the LLMs serves as a basis for enhancing their honesty. Current studies indicate the potential for self-alignment  of LLMs, suggesting that LLMs can inherently improve their responses. Drawing inspiration from this concept, we formulate a constitution-guided (_i.e._, principle-guided ) prompt that amalgamates the query, raw answer, and expressed confusion. This prompt is then fed back into the LLMs, which are tasked with generating an improved output that is both helpful and honest.

The constitution-guided prompt emphasizes that (1) LLMs should convey any confusion or limitation in their output as a form of disclaimer to express uncertainty. (2) LLMs should remain helpful, exemplified by providing actionable guidance. For instance, when faced with a complex arithmetic problem like \(e^{10}\), beyond simple computational abilities without tools, LLMs should suggest practical alternatives such as using a calculator or programming a solution.

Formally, the optimized prompt \(p_{}\) is composed of the confusion output \(c\) from the curiosity-driven prompt, the original query \(q\), and the raw answer \(a\) to the original query. The optimization process aims to generate a response \(\) that maximizes an evaluation function \(\), reflecting the quality of the response. This process can be mathematically formulated as follows:

\[=_{}(p_{}), y=_{}(q) ()>E(y)\] (2)

Here, \(_{}(p)\) denotes the output of the language model parameterized by \(\) given prompt \(p\), \(y\) is the baseline response from the original query \(q\) without optimization, and \(\) is the optimized response from the enhanced prompt \(p_{}\). The objective is to ensure that the evaluation \(()\), which quantifies the quality of the response, is greater than \((y)\), indicating an improvement over the baseline.

Figure 3: The overall pipeline incorporates both training-free and fine-tuning methods to ensure honesty and enhance helpfulness simultaneously.

### Approach II: Improvement Through Fine-Tuning

This section details our approach to enhancing the honesty and helpfulness of LLMs through a two-stage fine-tuning process. Initial efforts to directly fine-tune LLMs yielded unsatisfactory improvements due to the inherent complexity of teaching honesty and helpfulness simultaneously. Inspired by curriculum learning principles , we have adopted a structured fine-tuning method aimed at progressively aligning LLMs with predefined honesty standards.

**Preliminaries.** For each query \(q\), response pairs \((y_{1},y_{2})\) are analyzed. Preference between responses is indicated by \(y_{w} y_{l} q\), where \(y_{w}\) is the preferred response, and \(y_{l}\) is the less preferred one. We utilize two distinct evaluation functions: (1) A binary honesty evaluator \(_{}()\), assigning values {0, 1}, where 1 indicates a response aligns with honesty. (2) A comprehensive evaluation function \(_{}()\), assigning a score \(s\) where \(1 s<n\) and \(s\), to evaluate both honesty and helpfulness.

Fine-tuning leverages the Direct Preference Optimization (DPO) framework , with the DPO-based loss function expressed as:

\[_{}(_{},_{})=-_{(q,y_{ w},y_{l})}[((y_{w}  q)}{_{}(y_{w} q)}-(y_{l}  q)}{_{}(y_{l} q)})]\] (3)

where \(\) is the preference dataset, \(_{}\) denotes the policy parameterized by model parameters \(\), \(_{}\) is the reference policy, and \(\) is a scaling factor for the logits.

**Stage One: Differentiating Honesty from Dishonesty.** The primary goal of this stage is to train LLMs to distinguish between honest and dishonest responses. We only retain response pairs with contrasting honesty evaluations for training. However, directly using the pairs with a large score difference evaluated by \(_{}()\) (_e.g._, a dishonesty response with score 1 and an honest response with score 9) will pose challenges for LLMs to learn. Therefore we select the response pair \((y_{1},y_{2})\) into the training set \(_{1}\) requires by the following constraints:

\[_{1}:=\{(y_{1},y_{2})_{}(y_{1}) -_{}(y_{2})=1\{_{}(y_{1}),_{}(y_{2})\}<\}\] (4)

Where \(\) is the threshold score evaluated by \(_{}()\).

**Stage Two: Enhancing Overall Response Quality.** The second stage is dedicated to enhancing the overall quality of responses, aiming to produce outcomes that are not only honest but also informative and helpful. We include in training set \(_{2}\) those pairs \((y_{1},y_{2})\) where:

\[_{2}:=\{(y_{1},y_{2}) _{}(y_{1})=_{}(y_{2 })=1_{}(y_{1})_{}(y _{2})\] (5) \[\{_{}(y_{1}),_{}(y_{2})\}>\}\]

These pairs are utilized to further refine the LLM through the DPO framework, as described by the loss function in Equation 3. This two-stage fine-tuning process ensures that LLMs adhere to honesty standards while fostering the generation of helpful, high-quality guidance in practical scenarios. We show the overall algorithm in Appendix C.

## 5 Experiments and Analysis

### Experimental Setup

Model Selection.Our study covers nine mainstream LLMs, including both open-source and proprietary LLMs. Our evaluation came across ChatGPT  and GPT-4  by OpenAI ; Llama2 (7b-chat, 13b-chat, 70b-chat)  and Llama3-70b-instruct  by Meta AI ; Mistral-7b and Mistral-8x7b  by Mistral AI ; and Claude3-Opus  by Anthropic . We show other details of the experimental setting including hyperparameters in Appendix D.1.

Evaluation.Our evaluation framework consists of two protocols: one focusing on honesty and the other on both honesty and helpfulness. Due to the complexity of rule-based methods like keyword matching , we use the "LLM-as-a-Judge" methodology , widely used in previous studies . Each response is judged by averaging the results of three times of LLM-as-a-Judge. We propose two evaluation protocols as follows:* **Purely Honest-Guided Evaluation:** This protocol aims to gauge the adherence of LLMs to honesty. LLMs are evaluated against predefined criteria specified in Table 7. An LLM is deemed honest if its responses consistently align with these standards. For this evaluation, we use the "Honesty Rate" metric (see Appendix D.2), which quantifies the percentage of queries in which an LLM consistently exhibits honesty.
* **H\({}^{2}\) Assessment:** This protocol extends beyond assessing honesty to evaluate both honesty and helpfulness (H\({}^{2}\)). As shown in Figure 1(b), it is imperative that LLMs not only uphold honesty but also provide well-reasoned explanations or justifications for their statements, along with viable solutions or guidance for user inquiries. The H\({}^{2}\) assessment is governed by three principal criteria: _(1) Rationality of Explanations for Honesty or Disclaimers; (2) Quality of Further Guidance; (3) Potential Solutions_ (detailed in Appendix D.2). Principles (1) and (2) are critical as they directly reflect the model's honesty and helpfulness, while (3) is deemed secondary. The importance of these principles is weighted accordingly in our evaluation. Furthermore, to comprehensively assess responses, we incorporate two evaluation formats in the H\({}^{2}\) protocol: pairwise and score-based, detailed in Appendix D.2.

Implementation Details.We utilize all queries from the HoneSet to evaluate LLMs' performance. (1) **Training-Free Enhancement.** For the H\({}^{2}\) assessment, we calculate only those queries that have already been evaluated through the purely honest-guided evaluation and confirmed as honest, to see the plain improvement of LLMs when applying our method. (2) **Improvement through fine-tuning.** We compile all responses--both the raw outputs and those optimized via training-free enhancement--and employ the LLM-as-a-Judge approach (_i.e._, purely honest-guided evaluation) to select answer pairs for constructing the preference dataset (\(_{1}\) and \(_{2}\)) in both the first and second stages of fine-tuning. The first stage and the second stage both involve 1000 answer pairs. We designate 120 queries as our test dataset, ensuring these do not overlap with any answer pairs in our preference dataset across both stages. In our experiments, the threshold \(\) is set to 5, 6, and 7.

We implement two evaluation methods by LLM-as-a-Judge: the \(_{}()\) for purely honest-guided evaluation, and the \(_{}()\) for the H\({}^{2}\) assessment, which utilizes a score output format. The prompt templates of evaluation are shown in Appendix H.

Figure 4: Comprehensive evaluation results of the training-free method.

### Main Results

#### 5.2.1 Training-Free Enhancement

Honest-Guided Evaluation.As shown in Figure 4(a), we significantly enhance the honesty rates in both open-source and proprietary LLMs by implementing our proposed training-free approach. For example, GPT-4 and Claude3-Opus's honesty rates improved markedly to 100%, demonstrating a near-perfect honesty alignment. Large open-source models such as Llama3-70b and Mistral-8x7b also saw a substantial increase, rising from 0.606 to 0.871 and 0.585 to 0.914 respectively. Notably, Llama2-7b, a smaller parameter model, exhibited a remarkable improvement from 0.430 to 0.837. In summary, honesty rates for all models we evaluated are over 60% when implementing our curiosity-driven approach, convincing the efficacy of our method for constructing more honest LLMs.

H\({}^{2}\) Assessment.In addition to honesty rates, we leverage LLM-as-a-Judge to conduct H\({}^{2}\) assessment in both pairwise and score settings to evaluate the responses before and after the curiosity-driven method. As illustrated in 4(b), in the pairwise setting, optimized answers were generally rated higher than the original ones, representing better honesty and helpfulness. Proprietary LLMs like Claude3-Opus and GPT-4 show a significant win rate for optimized answers. Open-source models like Llama2-7b showed that 40.1% of the optimized answers were preferred over the raw ones. In the score setting, we provide fine-grained scores for three principles as shown in Figure 4(c) and detailed in Table 1. All LLMs demonstrate improvement using our training-free method, with proprietary models achieving significantly better results than open-source models, scoring over 9 in 'Explanation' and over 8 in 'Guidance'. For both the Llama2 and Mistral series, we observe a scaling law where larger models exhibit higher scores in both raw and optimized settings. Among the three dimensions, 'Explanation' and 'Guidance' show the most substantial improvement, indicating that models become more honest and helpful in identifying their limitations and guiding users through LLM-unable questions. Furthermore, we conduct additional experiments to demonstrate the effectiveness of our training-free approach. More details can be found in the Appendix D.4.

#### 5.2.2 Improvement Through Fine-Tuning

To thoroughly evaluate the effectiveness of our two-stage fine-tuning, we compare the LLMs' performance across different training stages: raw (baseline), only stage 1, stage 2 (proposed), and direct fine-tuning using a combined dataset from both of two stages. Each LLM's performance is assessed by honest-guided evaluation and H\({}^{2}\) assessment.

As detailed in Table 3, our proposed two-stage fine-tuning method demonstrates improvements in honesty rate and H\({}^{2}\) assessment for both Llama3-8B and Mistral-7B. It significantly enhances the honesty of LLMs when encountering LLM-unable queries without degrading the overall response quality, as measured by the H\({}^{2}\) score. Specifically, the Llama3-8b model shows a notable improvement of 13.7% in honesty rates post fine-tuning, along with an 8.5% increase in the H\({}^{2}\) score. Similarly, the Mistral-7b model exhibits a substantial enhancement, with the honesty rate soaring by 51.9% and

    &  &  &  &  \\   & _raw_ & _opt._ & _raw_ & _opt._ & _raw_ & _opt._ & _raw_ & _opt._ & _gain_ \\  \\ 
**GPT4** & 2.5\% & 0.1\% & 10.1\% & 2.5\% & 87.6\% & 97.3\% & 8.094 & 8.604 & 6.3\% \\
**ChatGPT** & 38.5\% & 11.1\% & 20.1\% & 26.9\% & 41.4\% & 62.0\% & 5.098 & 6.770 & 32.8\% \\
**Claude3-Opus** & 14.4\% & 0.9\% & 17.0\% & 9.2\% & 68.6\% & 89.9\% & 7.061 & 8.244 & 16.8\% \\  \\ 
**Mistral-7b** & 55.3\% & 21.7\% & 20.4\% & 27.5\% & 24.4\% & 50.8\% & 3.885 & 6.046 & 55.6\% \\
**Mistral-8x7b** & 31.4\% & 2.8\% & 18.1\% & 15.5\% & 50.5\% & 81.7\% & 5.693 & 7.626 & 34.0\% \\
**Llama2-7b** & 42.9\% & 23.2\% & 19.1\% & 17.2\% & 38.0\% & 59.6\% & 4.877 & 6.203 & 27.2\% \\
**Llama2-13b** & 42.7\% & 24.9\% & 19.0\% & 22.1\% & 38.4\% & 53.0\% & 4.890 & 5.961 & 21.9\% \\
**Llama2-70b** & 39.4\% & 21.0\% & 19.7\% & 14.8\% & 40.9\% & 64.2\% & 5.068 & 6.447 & 27.2\% \\
**Llama3-70b** & 25.3\% & 4.2\% & 20.8\% & 14.5\% & 53.9\% & 81.3\% & 6.128 & 7.783 & 27.0\% \\   

Table 1: Improvements in honesty rate and H\({}^{2}\) scores for Llama3-8b and Mistral-7b after the proposed two-stage fine-tuning.

the H\({}^{2}\) score escalating by 108.6% after the two-stage fine-tuning process. These results underscore the critical role that both stages of the fine-tuning method play in augmenting LLM performance and the effectiveness of our proposed dataset.

Figure 5 shows the overall scores and honesty rates for the two LLMs under different thresholds. Llama3-8b achieves optimal two-stage fine-tuning enhancement with a threshold set at 6 points, and Mistral-7b maintains consistent overall scores across different thresholds, peaking at a threshold of 5 points. Moreover, the two-stage fine-tuning process outperforms the direct fine-tuning approach, regardless of the threshold setting. As shown in Table 2, both models achieve the highest overall scores in the category _"user input not enough or with wrong information"_, while the data from the category _"modality mismatch"_ and _"interactivity sensory processing"_ gain the most scores. In summary, the overall scores for each category have improved, demonstrating the effectiveness of the method we proposed.

### Impact on Other Tasks

**Utility.** To further evaluate the impact of our fine-tuning process, we conducted additional experiments on two standard benchmarks: MMLU  and MTBench . Table 4 indicates that our finetuned model led to a modest improvement of 0.7% in MMLU accuracy, reflecting the model's enhanced generalization on diverse tasks. However, we observed a 5% decrease in the average score on MTBench. We attribute this decline to the trade-off between improving honesty and preserving other capabilities. Upon closer inspection, we found that MTBench includes both fixed-answer tasks (_e.g._, Math, Reasoning) and open-ended tasks (_e.g._, Writing, Roleplay). The prompts used in GPT-4 for evaluating open-ended tasks may have introduced a bias in the scoring, particularly affecting the fine-tuned model's performance in these categories. Despite this, we believe the trade-off is reasonable, as our fine-tuning prioritizes honesty without significantly compromising overall model

    &  &  &  &  &  &  \\   & \(\) & 5 & 6 & 7 & 5 & 6 & 7 & 5 & 6 & 7 & 5 & 6 & 7 & 5 & 6 & 7 & 5 & 6 & 7 \\   \\   &  & 8.70 &  & 2.90 &  & 5.25 &  & 1.60 &  & 4.00 &  & 7.30 &  \\  & 8.15 & 8.70 & 8.90 & 4.10 & 4.15 & 5.50 & 5.00 & 5.55 & 5.15 & 5.60 & 5.00 & 5.75 & 8.15 & 7.50 & 8.05 & 7.85 & 9.15 \\  & 9.20 & 7.80 & 8.05 & 3.10 & 4.50 & 2.95 & 4.30 & 3.85 & 4.55 & 3.45 & 4.75 & 5.85 & 3.85 & 5.80 & 6.55 & 6.35 & 6.40 & 6.50 \\  & 8.90 & 9.15 & 9.15 & 8.10 & 8.05 & 7.05 & 5.95 & 6.50 & 5.85 & 7.30 & 8.40 & 8.15 & 8.25 & 8.40 & 8.50 & 9.10 & 8.85 & 8.90 \\  \\   &  & 6.30 &  & 2.90 &  & 3.40 &  & 2.00 &  & 1.70 &  & 4.60 &  \\  & 8.70 & 8.55 & 8.45 & 5.30 & 4.50 & 6.10 & 6.00 & 5.40 & 6.25 & 6.00 & 6.90 & 7.05 & 6.20 & 7.10 & 7.25 & 7.40 & 7.40 & 8.30 \\  & 7.80 & 8.05 & 7.30 & 3.20 & 4.60 & 2.95 & 3.65 & 3.75 & 4.40 & 5.20 & 4.95 & 6.40 & 2.90 & 4.55 & 6.60 & 5.10 & 5.35 & 4.65 \\  & 8.00 & 8.70 & 8.40 & 6.40 & 6.30 & 5.50 & 5.75 & 4.90 & 5.45 & 7.95 & 8.00 & 7.55 & 7.65 & 6.85 & 8.05 & 8.85 & 8.55 & 8.50 \\   

Table 2: Overall score for each category under different threshold.

Figure 5: Overall score and honesty rates of Llama3-8b and Mistral-7b under different thresholds.

utility. Maintaining a balance between honesty, helpfulness, and overall performance remains a key consideration in our ongoing model development.

**Safety.** To explore how our method influences the safety of LLMs, we performed additional experiments based on the Safety subset of TrustLLM . Table 5 indicates that our fine-tuning process not only preserves but also improves the safety performance of the model. Specifically, the overall refusal rate increased from 94.79% to 98.43%, demonstrating enhanced robustness across various categories such as _"No Punctuation," "Refusal Prohibition,"_ and _"Leetspeak."_ These findings confirm that our fine-tuning approach successfully strengthens the model's adherence to safety standards without compromising its functionality.

### Computing Budgets

To ensure a comprehensive evaluation of the computational costs associated with our method, we measured the token usage per query across various models. Table Table 6 shows that our two-stage curiosity-driven method incurs an average additional token usage of approximately 174 tokens per query. To assess its impact on inference time, we conducted experiments on an NVIDIA A800 80G GPU server. Our method increases the inference time for each query by an average of 120-150 milliseconds, which is considered acceptable, given the significant improvements in model performance and response quality enabled by the curiosity-driven approach. These findings confirm that our method strikes a favorable balance between computational efficiency and enhanced model capability.

## 6 Conclusion

In this paper, we prioritize LLM helpfulness while preserving honesty. We establish honesty principles to differentiate LLM-able from LLM-unable questions and introduce the HoneSet dataset, covering six categories of LLM-unable queries. We then enhance honesty and helpfulness in both training-free and fine-tuned settings. Experimental results show notable improvements, validating our approach and contributing to more reliable and trustworthy LLMs for real-world use.