# Making Scalable Meta Learning Practical

Sang Keun Choe\({}^{1}\)1

Sanket Vaibhav Mehta\({}^{1}\)

Hwijeen Ahn\({}^{1}\)

Willie Neiswanger\({}^{2}\)

Pengtao Xie\({}^{3,5}\)

Emma Strubell\({}^{1,4}\)

Eric Xing\({}^{1,5}\)

\({}^{1}\)Carnegie Mellon University \({}^{2}\)Stanford University \({}^{3}\)UCSD \({}^{4}\)Allen Institute for AI \({}^{5}\)MBZUAI

###### Abstract

Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (_i.e._, learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8\(\) increase in throughput and 2.0/3.8\(\) decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.

## 1 Introduction

Meta learning aims to learn the _inductive biases_ (_e.g._ training data, neural architecture) of a machine learning program in such a way that a model trained with these inductive biases achieves optimal performance on user-specified _objectives_ (_e.g._ fairness, quick generalization). This concept of meta learning can naturally be formulated as bilevel optimization, where the upper (meta) level problem encodes inductive biases and objectives, and the lower (base) level optimization problem represents the main machine learning program of interest, such as image classification or language modeling. Depending on the design of inductive biases and objectives, meta learning has found many applications in machine learning, including hyperparameter optimization , data optimization , neural architecture search , learned optimizers , and few-shot learning .

Following its versatility, numerous algorithms have been proposed to solve meta learning. Among them, gradient-based meta learning (GBML) has in particular gained considerable attention, due to its capability to optimize a wide range of _high-dimensional_ inductive biases in an _efficient_ manner. For example, MAML  finds optimal initialization weights (inductive bias) that achieve quick generalization to new tasks (objective), and L2RW  optimizes training sample weights (inductive bias) to achieve robustness against label noise (objective). However, the above benefits of GBML oftentimes get overshadowed by its poor scalability in practice, especially under the recent trend of large models , which arises due to several factors. First, many GBML algorithms  require inversion of a large Jacobian matrix, which suffers from both algorithmic instability as wellas exorbitant compute/memory costs. Second, most GBML research assumes that lower (base) level optimization is performed with SGD, whereas most large models, exemplified by Transformers , are by default optimized with adaptive optimizers like Adam ; consequently, the applicability of SGD-based GBML methods to large models trained with adaptive optimizers remains unclear. Finally, most GBML research to date has been limited to the single-GPU setup due to the lack of distributed training support [9; 20], which is essential in large-scale learning.

In this work, we endeavor to resolve the aforementioned scalability issues in GBML by co-developing algorithms and systems, and explore the initial potential of scalable meta learning in diverse applications. Our main contributions can be summarized as follows:

1. We investigate the root causes of substantial memory/compute costs, algorithmic instability, and a lack of distributed training support in GBML, all of which significantly limit its scalability, through a technical analysis of implicit differentiation. In doing so, we identify three major factors, namely (i) base Jacobian inversion, (ii) a lack of algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of the backward pass of meta gradient computation, and discuss how each of them contributes negatively to the above limitations in depth.
2. Taking one step further, we propose an initial solution to each of the aforementioned issues by respectively (i) approximating the base Jacobian with an identity matrix, (ii) additionally expanding the meta Jacobian via the chain rule, and (iii) devising a novel communication strategy that efficiently uses the communication-computation overlap trick . Combining all these solutions, we develop SAMA, a holistic and practically **S**cal**Able **M**eta learning **A**lgorithm.
3. We evaluate the scalability and the overall performance of SAMA on a multitude of large-scale meta learning benchmarks involving large language models (_e.g._, BERT  and RoBERTa ) or large datasets (_e.g._, ImageNet-1k ). Notably, SAMA showcases up to 1.7/4.8\(\) increase in throughput and 2.0/3.8\(\) decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. In addition, we observe that SAMA-based data optimization consistently leads to improvements in text classification accuracy with large language models, and achieves state-of-the-art results in both small-/large-scale data pruning, demonstrating the initial potential of scalable meta learning.

Figure 1: **Top:** Table showing a scalability comparison. **Bottom left:** Plot of throughput vs memory of different GBML algorithms on the _noisy finetuning of BERT-base_ experiment. SAMA achieves better memory/compute efficiency overall given a fixed model, and the gap further widens by distributing compute across multiple GPUs with our efficient distributed training strategy. **Bottom right:** Plot of memory vs model size (_i.e._, # of parameters) of different GBML algorithms on the _continued pretraining of RoBERTa_ experiment. SAMA demonstrates the least significant increase in GPU memory usage with the increasing model size compared to baseline methods.

Background: Gradient-Based Meta Learning

We begin by reviewing the basics of (gradient-based) meta learning in order to establish the key aspects that have limited its scalability. Mathematically, meta learning is commonly formulated as bilevel optimization as follows:

\[^{*} =*{argmin}_{}\,L_{meta}(D_{meta};^{*}( ))\] \[s.t.\,\,\,^{*}() =*{argmin}_{}\,L_{base}(D_{base};,)\]

where \(\) (respectively, \(\)) are the parameters of meta (base) learners, \(D_{meta}\) (\(D_{base}\)) are meta (base) datasets, and \(L_{meta}\) (\(L_{base}\)) are meta (base) loss functions. An important implication of the above formulation is that meta learning changes the task of finding the optimal inductive biases from designing heuristics to designing meta optimization problems. As an example, consider the problem of finding the optimal inductive bias for fair classification given class-imbalanced training data. A traditional approach to this problem is to use a heuristic that reweights training samples inversely proportional to class frequencies. On the contrary, L2RW  designs a meta optimization problem by curating a small number of class-balanced data for the meta dataset \(D_{meta}\) and setting the meta learner \(\) to be importance weights for all training data. In short, unlike heuristic-based methods that explicitly specify "how to learn," meta learning methods only specify "what to learn" and let the meta learner automatically determine "how." From a programming paradigm perspective, such a difference can be understood as a transition from imperative to declarative programming.

While there are multiple approaches to solving meta learning, in this work we focus on _gradient-based_ approaches due to their ability to efficiently solve high-dimensional meta optimization (\(*{dim}() 1\)) problems. Such an ability is essential given that the search space for inductive biases (_e.g._ importance weights for training data) can increase exponentially with recent large models and datasets. Concretely, GBML computes a meta gradient composed of two terms--the best-response Jacobian and direct gradient--with the chain rule, as follows:

\[}{}=}{}}_{} }{^{*}}}_{}\] (1)

Since the direct gradient (teal) computation is straightforward with the underlying automatic differentiation library, the major challenge in GBML lies in computing the best-response Jacobian (purple), of which two common solutions are iterative differentiation [14; 15; 16; 42] and implicit differentiation [25; 40; 49; 51]. Between these two, in this paper we adopt implicit differentiation as our baseline solution to GBML, as it achieves better computation and memory efficiency than iterative differentiation , both of which are vital in accomplishing our goal of scalable GBML.

The gist of implicit differentiation is that it calculates the best-response Jacobian by leveraging Cauchy's Implicit Function Theorem (IFT) and re-interpreting the base optimization problem from the perspective of fixed-point iteration given an iterative solver \(u\), as follows:

\[}{}=-}}_{}( }}\\ )^{-1}\{ ^{*}=_{t}_{t}\\ _{t}=_{t-1}-u(_{t-1};).\] (2)

While exact implicit differentiation requires solving the base optimization problem to convergence \(^{*}\) by repeatedly applying an iterative solver \(u\) (_e.g._, SGD or Adam) to calculate base (blue) and meta (red) Jacobians, this is computationally impractical, especially in most large-scale learning settings. Therefore, researchers oftentimes approximate \(^{*}\) with a small number of unrolled update steps of \(u\). This results in a solution that alternates gradient descent between base and meta optimization problems, where the base gradient is calculated with standard backpropagation and the meta gradient with Eqs. (1) & (2). Noting that many techniques have been developed to efficiently perform and scale up standard backpropagation, we deduce that the major challenges in scaling GBML lie in meta gradient computation, which will be discussed in depth in the next section.

## 3 Scaling Meta Learning

It has long been recognized that meta gradient computation in GBML suffers from a substantial compute/memory cost [40; 51], algorithmic instability [2; 12], and a lack of efficient distributed training support [3; 6; 9], all of which can significantly limit its scalability. In this section, we first attempt to understand the above limitations at a technical level. Toward this end, we investigate three aspects in Eqs. (1) & (2), namely (i) base Jacobian inversion, (ii) algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of meta gradient computation, and discuss how they lead to the aforementioned limitations. Next, we propose initial solutions for each of these issues, based on which we build a holistic and ScalAble Meta Learning Algorithm, SAMA.

### Base Jacobian Inverse

ProblemDenoting the size of the base learner (_i.e._, \(()\)) as \(n_{b}\), the computational complexity of the naive base Jacobian (blue) inversion in Eq. (2) is \((n_{b}^{3})\). Since such cubic complexity is impractical even for small-sized base learners, practitioners typically utilize various linear systems techniques such as Neumann series  or conjugate gradient , and directly _approximate_\((})^{-1}} {^{*}}\). Given that the base Jacobian is a function of the Hessian matrix of the base optimization problem in GBML, these algorithms solve linear systems by iteratively performing Hessian-vector products. However, this Hessian-vector product computation contributes negatively to all three above limitations. First, while many automatic differentiation engines provide an efficient Hessian-vector product implementation like Pearlmutter's algorithm , its memory/compute cost is still prohibitive with larger models, as demonstrated in Fig. 1. Second, in most cases, we only have access to a stochastic estimation of the Hessian due to mini-batch sampling . Hence, meta gradients obtained with noisy Hessian-vector products can be biased, which may result in training instability. Finally, the most efficient distributed training features, such as communication-computation overlap , are designed for first-order gradients, rather than the higher-order gradients involved in Hessian-vector products.

SolutionA simple solution for avoiding the aforementioned issues stemming from base Jacobian inversion is to approximate the base Jacobian with an identity matrix as follows:

\[}{}=-(})^{-1} { L_{meta}}{^{*}}-}{^{*}}\] (3)

Under the deep equilibrium model setting, Jacobian-free backpropagation  shows that such an approximation can be understood as preconditioning the original meta gradient. Our approximation also resembles approximating the Hessian as an identity matrix in one-step unrolling techniques (_i.e._\(T_{1}-T_{2}\)) from [38; 41]. While this approximation is exact when the iterative solver \(u\) is naive SGD where \(u=}{}\), we note that the base Jacobian does not necessarily equate with the Hessian when an adaptive optimizer is used in base optimization (more detailed discussion on this issue is deferred to Sec. 3.2). Furthermore, their methods calculate the meta Jacobian at initialization \(\) instead of at convergence \(^{*}\) due to their close connection to iterative differentiation , and thereby inhibit unroll steps larger than 1, unlike our approach. Considering that a larger number of unroll steps allows for less frequent computations of expensive meta gradients, our implicit-differentiation-based derivation can lead to a further computational gain in large-scale meta learning. In Appendix E, we investigate the effect of this identity approximation in the "biased regression" setting where the closed-form solution can be analytically calculated, and empirically show that the identity approximation still allows for accurate estimation of the meta gradient \(}{}\) and the optimal meta solution \(^{*}\), even when the true base Jacobian is not an identity matrix.

### Algorithmic Adaptation for Adaptive Optimizers

ProblemMost existing implicit differentiation algorithms [21; 26; 40] compute the best-response Jacobian in Eq. (1) based on the assumption that the iterative solver \(u\) for base optimization is vanilla SGD, whereas recent large models, exemplified by Transformers [5; 62], are by default trained with adaptive optimizers, such as Adam . While the fixed point condition can be theoretically identical for any gradient-based optimizer at convergence (_i.e._, \(_{meta}}{^{*}}=0\)), researchers in practice approximate \(^{*}\) with a small number of gradient steps, at which the above fixed point condition would unlikely hold. Thus, the inconsistency between assumed and actual optimizers results in an incorrect meta gradient, which is a source of training instabilities and reduced performance in meta learning (as we demonstrate in Table 1).

SolutionTo take adaptive update rules into account, we propose to further expand a meta Jacobian term in Eq. (3) with the chain rule as follows:

\[=}{ }}=L_{ base}}{^{*}}}\] \[}{}-}{^{*} }= -L_{base}}{^{*}}}}{^{*}}\] (4)

where \(g_{base}\) is base-gradient computed at convergence (_i.e._\(}{^{*}}\)). In short, we accomplish the algorithmic adaptation for any adaptive optimizer with the update rule \(u\) through the middle term \(}\) in Eq. (4), which reduces to an identity matrix in the case of SGD. To analyze the adaptation cost, we note that parameter updates \(u\) in most optimizers are performed only with element-wise operations. Thus, the adaptation matrix is diagonal, for which computation/memory complexities are only \((n_{b})\). As a concrete example, we provide an adaptation matrix for the most popular Adam optimizer  in Appendix C.

Furthermore, to reduce computation/memory complexities involving the costly second-order derivative \(L_{base}}{^{*}}\), we instead perform the matrix-vector product with the central-difference method from DARTS  and the associative rule of matrix multiplication. All combined, we propose SAMA, a highly compute/memory efficient meta gradient algorithm for scalable meta learning, the formulation of which is as follows:

\[}{}-L_{base}}{ ^{*}}(}}{^{*}})-(^{+},)}{}-(^{-},)}{}}{2}\] (5)

where \(^{}=^{*} v\) with the perturbation vector \(v=}}{ ^{*}}\) and the step size \(=}\). We empirically observe that \(=1.0\) works well across a multitude of tasks without further tuning. Finally, we notice that previous work in penalty-based bilevel optimization (_e.g._ F2SA  and BOME ) further uses the direct gradient \(}{^{*}}\) explicitly in the base level optimization to maximize the performance of the final base parameter \(^{*}\) on the meta objective. Given that our perturbation vector \(v\) includes the direct gradient term, we also follow a similar strategy and update the base parameter \(\) in the direction of \(v\) (_i.e._\(_{t+1}=_{t}- v\)) every time the meta update is performed.

### Efficient Distributed Training & Implementation

ProblemIn large-scale learning, distributed data parallelism (DDP), which communicates and synchronizes local gradients from each device before the parameter update, is necessary to improve both compute and memory efficiency. However, most automatic differentiation libraries like PyTorch  only have native DDP support for their basic backward function, whereas SAMA, similar to other meta gradient algorithms, requires a custom implementation of the backward function, which consists of three basic backward passes as shown in Eq. (5).2 In addition, while a few meta learning

Figure 2: The overall workflow of meta gradient computation with SAMA in the distributed data parallel setting. In detail, SAMA consists of three _first-order_ backward passes performed with the underlying automatic differentiation engine, and one manual backward pass for algorithmic adaptation for the adaptive optimizer. Gradient synchronization is performed only once in the last backward pass with communication-computation overlap to minimize the communication bottleneck.

libraries, such as Betty , have preliminary DDP support, they do not offer further communication cost optimization. As a result, meta learning research to date has either been limited to a single-GPU setup or suffered from communication inefficiency.

SolutionSince we avoid any explicit computations of second-order gradient information in SAMA, we can utilize various efficient distributed training tricks that have been implemented for first-order gradients. More specifically, to enable efficient DDP in SAMA, we develop a novel communication strategy that performs first two backward passes locally on each device, and then overlaps computation in the final backward pass with communication. In PyTorch, this can be neatly achieved by implementing the first two backward passes with torch.autograd.grad and the last one with torch.autograd.backward. The overall workflow diagram of SAMA is presented in Figure 2. To facilitate research in scalable meta learning, we provide our implementation of SAMA with the above communication optimization in Betty3 that only requires a one-line change in the configuration.

## 4 Experiments

While few-shot learning has traditionally been the most popular application of meta learning, most recent large models such as GPT , ViT , and Whisper , provide few-shot generalization capability out of the box. Therefore, we in this work focus on another promising application of meta learning, _data optimization_, where we transform (_e.g._, reweight, label correct) downstream/pretraining data given the specific objectives in the meta optimization problem. Indeed, there is an increasing number of works originating in data-centric AI that empirically show that the quality of training data significantly affects the final performance of large models . Nevertheless, solutions proposed in these works to improve training data quality mostly rely on hand-designed heuristics, which typically result in suboptimal performance. Given that training data of large models serves as extremely high-dimensional inductive biases in machine learning, we expect that GBML's ability to efficiently optimize high-dimensional inductive biases can be fully unlocked in the data optimization application.

From a technical perspective, large-scale data optimization has a substantial compute/memory cost and frequently involves models that are trained with adaptive optimizers. Therefore, it serves as an ideal benchmark to (a) evaluate the scalability of SAMA compared to existing approaches, (b) study the effectiveness of each component in SAMA, and (c) investigate the practical usefulness of scalable meta learning across diverse domains. Specifically, in this section, we consider three data optimization applications, namely: Noisy finetuning of large language models (Sec. 4.1), continued pretraining of large language models (Sec. 4.2), and scale-agnostic data pruning (Sec. 4.3). While not an application of data optimization, we also include a preliminary analysis on the effect of model size on few-shot image classification accuracy in Appendix D. Finally, we note that all experiment details including baselines, hyperparameters, and compute resources are provided in Appendix B.

### Noisy Finetuning of Large Language Models

Weak supervision  proposes to achieve a significant reduction in the data labeling cost by letting users quickly generate labels for a large amount of data by exploiting multiple weak labeling functions, such as hand-designed rules and other neural networks. While increasing the _quantity_ of labeled data with weak supervision has led to noticeable improvements in multiple applications, a poor _quality_ of generated labels results in a degradation in test accuracy, leaving room for further improvement. Here, we attempt to alleviate this data quality issue in weak supervision by utilizing meta learning to automatically optimize noisy training data guided by a small amount of clean data in the meta level. In detail, we use data reweighting  and label correction  as our data optimization operations, of which the bilevel optimization formulation is as follows:

\[^{*}=(^{*}_{r},^{*}_{c})=*{argmin}_{ _{r},_{c}}(D_{clean};^{*}(_{r},_{c}))\]

\[^{*}(_{r},_{c})=*{argmin}_{} |}_{(x,y) D_{noisy}}w(;_{r}) (f(x;),c(x,y;_{c}))\]

where \(w(;_{r})\) and \(c(;_{c})\) are meta learners, respectively, for data reweighting and label correction. To evaluate the scaling efficiency of SAMA, we perform text classification with a BERT-base model with 110M parameters on multiple weak supervision datasets from the WRENCH benchmark . Furthermore, to study the effectiveness of our algorithmic adaptation strategy (Sec. 3.2), we conduct experiments with a variant of SAMA (_i.e._, SAMA-NA) that does not include algorithmic adaptation, and present the experiment results in Table 1.

In this experiment, we are able to make two observations. First, we notice that SAMA-based data reweighting and label correction both lead to noticeable improvements over finetuning and self-training (COSINE ) baselines in noisy text classification accuracy of the _large_ Transformer model across all benchmarks with the help of small additional clean data \(D_{clean}\) in the meta level. Second, given the superior performance of SAMA compared to SAMA-NA, we empirically verify the importance of algorithmic adaptation when an adaptive optimizer is used to train this Transformer base learner.

Additionally, we compare compute/memory efficiency of SAMA to that of two other implicit-differentiation-based meta learning algorithms, namely Neumann Series  and conjugate gradient . For fair comparison, we evaluate GPU memory usage (MB) and throughput (samples/sec) on the AGNews dataset from Wrench with a fixed _global_ batch size of 48, and summarize the result in Table 2. Three observations that can be made here are that (1) SAMA is generally more compute/memory efficient than the baselines, (2) the cost of algorithmic adaptation for adaptive optimizers is marginal as expected, and (3) the efficiency gap further widens as we distribute compute/memory across multiple GPUs with our efficient DDP communication strategy. Since we were only able to conduct experiments with up to 4 V100 GPUs due to the limited compute resources, exploring extremely large-scale GBML with larger GPU servers remains a promising future research direction. In Appendix F, we provide a more extensive ablation study for each component of SAMA, as well as compare test accuracy, GPU memory usage, and throughput of SAMA against various meta learning algorithms on IMDB and AGNews datasets from the Wrench benchmark.

### Continued Pretraining of Large Language Models

DAPT/TAPT  empirically demonstrate that additional pretraining (_i.e._, continued pretraining) of the generic language model on the domain or task-specific data can further improve downstream performance on diverse benchmarks. However, the inclusion of low-quality samples for continued pertaining tasks can potentially hinder pretraining by amplifying negative interference , which could lead to suboptimal downstream performance. Here, we attempt to minimize such negative transfer by reweighting samples from the continued pretraining task with meta learning. To this end, we adopt the auxiliary learning technique from TARTAN  and simplify the two-stage pretraining-finetuning pipeline into a one-stage multitask learning pipeline with the reweighting scheme applied to the pretraining loss. The bilevel optimization formulation is as follows:

\[^{*} =*{argmin}_{}\,_{ft}(D_{ft}; ^{*}())\] \[^{*}() =*{argmin}_{}\,_{ft}(D_{ft};) +|}_{x D_{pt}}w(x;)_{pt}(x;)\]

    &  &  &  & IMDB &  &  &  \\  Finetune (orig)  & - & 66.56 (2.31) & 83.93 (1.74) & 79.73 (2.60) & 56.09 (1.08) & 86.27 (0.53) & 82.26 (3.50) \\ COSINE  & - & 76.56 (0.08) & 86.80 (0.46) & 82.98 (0.05) & 58.47 (0.08) & 87.03 (0.00) & 89.22 (0.05) \\  Finetune (ours) & - & 67.93 (2.55) & 79.28 (1.78) & 78.16 (2.28) & 57.35 (1.43) & 85.79 (0.49) & 84.32 (2.55) \\  +R & SAMA-NA & 74.33 (2.34) & 87.11 (1.11) & 81.92 (1.74) & 60.88 (0.60) & 86.83 (0.19) & 80.96 (3.04) \\ +R \& C & SAMA-NA & 79.00 (2.62) & 87.67 (1.36) & 80.44 (0.97) & 64.05 (0.52) & 87.05 (0.39) & 80.73 (3.11) \\  +R & SAMA & 85.73 (0.81) & **89.67** (0.67) & 84.31 (1.86) & 76.89 (1.39) & 89.05 (0.34) & 93.64 (0.40) \\ +R \& C & SAMA & **87.93** (1.17) & 88.83 (2.32) & **85.71** (0.82) & **77.78** (0.59) & **89.79** (0.27) & **93.77** (0.08) \\   

Table 1: WRENCH results. R and C in the first column stand for data reweighting and label correction operations. The number in parentheses indicates standard deviation for each experiment over 3 runs.

    & GPUs & Memory & Throughput \\  Neumann & 1 & 26.0 & 82.9 \\ CG & 1 & 28.4 & 82.1 \\ SAMA-NA & 1 & 13.7 & 144.1 \\ SAMA & 1 & 14.3 & 142.0 \\ SAMA & 2 & 10.4 & 241.2 \\ SAMA & 4 & 7.4 & 396.7 \\   

Table 2: Memory and throughput analysis on AGNews with 4 V100 GPUs.

where \(_{ft}\)/\(_{ft}\) are finetuning/pretraining loss functions, \(D_{ft}\)/\(D_{pt}\) are finetuning/pretraining datasets, and \(w(;)\) is the data reweighting network. Following the experiment setup in TARTAN , we use task-specific data and a masked language modeling loss in our auxiliary task and perform experiments with RoBERTa-base on 4 datasets from the original DAPT/TAPT paper. We compare our SAMA-based data optimization against DAPT and TARTAN-MT. We exclude TAPT and TARTAN-Meta respectively because (1) TAPT consistently underperforms TARTAN-MT  and (2) TARTAN-Meta uses additional validation data in the meta level of the downstream tasks, making the comparison unfair. We report our experiment results in Table 3.

As shown above, SAMA-based data optimization leads to improvements in downstream performance on almost all datasets. This indirectly demonstrates that SAMA-based data reweighting can identify more/less relevant data in the auxiliary task and accordingly up-/down-weight them, unlike TARTAN-MT which allocates equal importance weights on all auxiliary data. Therefore, we expect that our method would likely benefit from additional auxiliary data by automatically figuring out and exploiting only relevant data, whereas TARTAN-MT is much more susceptible to negative transfer. While we only used task-specific data in our auxiliary task for the fair comparison with TARTAN-MT, extending auxiliary data to domain-specific or even general text data and comparing SAMA against DAPT or TARTAN-MT would be an intriguing future research direction. Finally, we analyze the GPU memory usage of different-sized RoBERTa in this experiment and present the result in Figure 1. The figure clearly shows the superior memory efficiency of SAMA with the increasing model size.

### Scale-Agnostic Efficient Data Pruning

Data pruning [47; 59; 61; 63] has recently received the limelight in the machine learning community as a means to both improve training efficiency and reduce (semantic) redundancy in training data. In particular, Sorscher et al.  showed both theoretically and experimentally that neural scaling laws can beaten by data pruning. Nevertheless, they point out that the optimal data pruning metric varies across different dataset scales and further research in scalable data pruning metrics is needed. Here, we propose to forgo hand-designed data pruning metrics, and rather automatically _meta-learn_ the importance weight of each training data following Meta-Weight-Net (MWN)  with four major modifications. First, we replace their iterative differentiation meta gradient algorithm with SAMA to achieve improved memory/compute efficiencies. Second, we further speed up meta learning by enabling distributed training with our efficient communication strategy. Third, we use the uncertainty of the prediction in addition to the loss value as an input to MWN to better estimate importance weight of each training data. Last, we use training data _both_ in the base and the meta levels, assuming no additional validation data. A bilevel optimization formulation of our method is as follows:

\[^{*} =*{argmin}_{}\ (D_{train};^{*}( ))\] \[\ ^{*}() =*{argmin}_{}\ |}_{(x,y) D_{train}}w( ,\,;)(x,y;)\]

where \(w(;)\) is MWN that takes the loss value \(\) and the uncertainty \(\) of the training sample \((x,y)\) as an input and outputs the importance weight. Under this setup, we run meta learning with SAMA for 30 / 50 epochs respectively for ImageNet-1k / CIFAR-10 and obtain the pruning metrics by averaging the importance weights of the last 5 epochs. We compare our method to several popular static/dynamic data pruning baselines, and present the results in Figure 3.

As expected, GBML-based data pruning with SAMA not only outperforms heuristics-based data pruning but also works well across different dataset scales. Surprisingly, we observe that GBML-based data pruning even leads to improvements in test accuracy at the pruning ratio of 0.1 and 0.2

    & ChemProt & HyperPartisan & ACL-ARC & SciERC & Average \\  Baseline & 82.70 (0.45) & 89.03 (2.25) & 68.17 (2.52) & 79.83 (0.89) & 79.93 \\ DAPT  & 84.17 (0.50) & 87.23 (0.65) & 71.84 (4.78) & 80.42 (1.57) & 80.92 \\ TARTAN-MT  & 84.18 (0.30) & 94.64 (0.91) & **72.41** (1.94) & 80.83 (0.71) & 83.02 \\ SAMA (ours) & **84.49** (0.13) & **95.18** (0.03) & 71.63 (1.68) & **81.84** (0.08) & **83.29** \\   

Table 3: Experiment results for auxiliary learning with the continued pretraining task. Following , we report test micro-F1 for ChemProt and macro-F1 for the other datasets. The number in parentheses indicates the standard deviation for each experiment over 3 runs.

on ImageNet-1k. The potential implication is that ImageNet-1k may have noisy labels or semantic redundancy and that GBML is able to automatically figure and filter out these samples. Further in-depth investigation of filtered data remains an interesting research direction. Considering that compute/memory inefficiency has traditionally been the major bottleneck in GBML applications, we also compare the relative search time for data pruning. Our result shows that SAMA demonstrates comparable or even shorter search time than heuristics-based methods. We also note that, while the original MWN  encounters the OOM error under our setup of batch_size=256, the throughput analysis with the reduced batch size reveals that efficient distributed training with SAMA on 4 GPUs achieves 15-20\(\) speed up compared to the original MWN that lacks distributed training support.

## 5 Related Work

AlgorithmsTwo major lines of research in gradient-based meta learning algorithms are iterative and implicit differentiation . Iterative differentiation [14; 15; 16; 42] computes meta gradients by differentiating through the optimization path and therefore requires saving all intermediate states on the path. This makes the memory/compute costs of iterative differentiation increase linearly with the number of unrolling steps. While the linear cost can be avoided with the use of techniques like truncated backpropagation , it is still more expensive than that of implicit differentiation, in which meta gradient computation is independent of the length of the optimization path. More specifically, meta gradient computation in implicit differentiation depends _only_ on the final state of the optimization path. To compute base Jacobian inversion in implicit differentiation, a multitude of variants have been proposed, each of which uses Neumann series [7; 40], conjugate gradient , Nystrom method , and more. While generally being more compute/memory efficient than iterative differentiation, most existing implicit differentiation algorithms have poor scalability due to the issues studied in Sec. 3.

ApplicationsMeta learning has found many applications in machine learning including few-shot learning [14; 51; 71], neural architecture search [38; 69], hyperparameter optimization [15; 16; 40; 42], data optimization [21; 54; 58; 70], and reinforcement learning [23; 29; 52], to name a few. Notably, most of these applications share the underlying mathematical formulation of bilevel optimization and are conceptually related to optimal design and inductive/declarative programming paradigms.

SystemsCompared to algorithms and systems research, there are relatively fewer research efforts in meta learning systems. In an attempt to facilitate research in few-shot image classification, _higher_, _learn2learn_, and _TorchMeta_ have been developed. However, due to their specific focus on few-shot image classification, these libraries have not been actively used in other meta learning tasks, such as data optimization or neural architecture search. Recently, software libraries

Figure 3: **Top Left:** ImageNet-1k data pruning results with ResNet-50. Reported numbers are relative accuracy compared to full training accuracy (_i.e._, pruned_acc/full_acc). Accuracy for other baseline methods is obtained from DynaMS . **Top Right:** CIFAR-10 data pruning results with ResNet-18. Accuracy for other baseline methods is obtained from Deepcore . **Bottom:** Relative time spent in finding data to prune compared to full ImageNet-1k training time.

for implicit differentiation including _JaxOpt_ and _Betty_, have been proposed. Given that Betty's software architecture is specifically designed to support various systems optimization for large-scale meta learning, we chose to implement SAMA in this framework.

## 6 Conclusion

In this paper, we strived to make scalable meta learning practical via both algorithmic and systems advancements. Towards this goal, we investigated diverse scaling bottlenecks in meta learning at a technical level and resolved them by developing SAMA. Tested on multiple benchmarks, SAMA empirically demonstrated its scaling efficiency as well as its capability to optimize a variety of high-dimensional inductive biases of large-scale learning. In future work, we plan to explore two directions. First, given that training extremely large models with 10B+ parameters require various systems techniques such as model/pipeline parallelism or optimizer sharding, extending SAMA to be compatible with these techniques would be highly important for further scalability. Second, we will also focus on large-scale meta learning application research, such as neural architecture search for Transformer-family models. Overall, we hope that our work can serve as a stepping stone for a lot of interesting scalable meta learning research to come.

Limitations & Broader ImpactsWhile SAMA has demonstrated significantly improved compute/memory efficiencies, and stably worked with fairly large models like BERT/RoBERTa, we could not test it on larger models with 1B+ parameters due to a lack of computational resources. Meta learning itself is mostly value-neutral, but there is a chance that practitioners amplify the bias or toxicity of the machine learning programs by enforcing them in the meta objective.