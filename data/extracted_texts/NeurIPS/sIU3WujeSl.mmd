# VOCE: Variational Optimization with Conservative Estimation for Offline Safe Reinforcement Learning

Jiayi Guan Tongji University

guanjiayi@tongji.edu.cn

&Guang Chen1 Tongji University

guangchen@tongji.edu.cn

&Jiaming Ji Peking University

jiamg.ji@gmail.com Long Yang Peking University

yanglong001@pku.edu.cn

&Ao Zhou Tongji University

2211107@tongji.edu.cn

&Zhijun Li Tongji University

zili@ieee.org

&Changjun Jiang Tongji University

cjjiang@tongji.edu.cn

###### Abstract

Offline safe reinforcement learning (RL) algorithms promise to learn policies that satisfy safety constraints directly in offline datasets without interacting with the environment. This arrangement is particularly important in scenarios with high sampling costs and potential dangers, such as autonomous driving and robotics. However, the influence of safety constraints and out-of-distribution (OOD) actions have made it challenging for previous methods to achieve high reward returns while ensuring safety. In this work, we propose a **V**ariational **O**ptimization with **C**onservative **E**stimation algorithm (VOCE) to solve the problem of optimizing safety policies in the offline dataset. Concretely, we reframe the problem of offline safe RL using probabilistic inference, which introduces variational distributions to make the optimization of policies more flexible. Subsequently, we utilize pessimistic estimation methods to estimate the Q-value of cost and reward, which mitigates the extrapolation errors induced by OOD actions. Finally, extensive experiments demonstrate that the VOCE algorithm achieves competitive performance across multiple experimental tasks, particularly outperforming state-of-the-art algorithms in terms of safety. Our code is available at github.VOCE.

## 1 Introduction

Reinforcement learning (RL) algorithms have made remarkable achievements in various fields such as robot control  and strategy games . However, limited by the online and trial-and-error nature, standard RL is challenging to apply to dangerous and expensive scenarios . Offline safe RL is a promising and potential approach to tackle the above problem, which learns policy satisfying safety constraints from pre-collected offline datasets without interacting with the environment .

Since offline safe RL methods aim to learn policies that satisfy safety constraints from offline datasets, which requires the agent to not only comply with safety constraints but also consider the influence of out-of-distribution (OOD) actions . This makes it difficult for the algorithm to learn policies that achieve high reward returns while satisfying safety constraints . There are two mainapproaches to deal with the aforementioned challenges: one is based on linear programming [17; 18], and the other is based on exploration and evaluation [19; 20; 21]. Linear programming provides a way to research offline safe RL problems, but it heavily depends on the F-divergence or KL-divergence to constrain the distance between the optimal distribution and the sample distribution . Therefore, it is difficult for linear programming to eliminate the extrapolation errors caused by OOD actions [10; 11]. The exploration evaluation approaches are to introduce conservative Q-value estimation under the actor-critic framework. Although exploration evaluation approaches avoid the overestimation issue through lower bound Q-values, they prematurely terminate trajectories that exceed the constraints during policy updates . The exploration evaluation method with strict constraints on the sample space makes it challenging to search for reward-maximizing policies. In conclusion, existing methods face challenges in learning policies from offline datasets that maximize reward while satisfying safety constraints, particularly when considering the impact of extrapolation errors.

In this work, to solve the constrained policy optimization problem and eliminate the extrapolation errors caused by OOD actions, we propose a variational optimization with conservative estimates for the offline safe RL algorithm. Concretely, we reframe the objective of offline safe RL using probabilistic inference, enhancing the flexibility of policy optimization by replacing parameterized policies with variational distributions. Furthermore, to avoid extrapolation errors caused by OOD actions, we derive upper and lower bounds for Q-values and estimate the Q-values of costs and rewards based on these bounds. Finally, we adopt a supervised learning approach to train a parameterized policy network according to the variational distribution of policies. The main contributions of this work are listed as follows:

* We utilize probabilistic inference to address the problem of offline safe RL, which introduces non-parametric variational distributions to replace parameterized policies, providing increased flexibility for optimizing safe policies in offline datasets.
* We derived upper and lower bounds for Q-value estimation using the pessimistic estimation approach. Subsequently, we utilized these bounds to estimate the Q-value of costs and rewards, respectively, to avoid extrapolation errors caused by OOD actions.
* We carry out extensive comparative experiments, and the results indicate that the VOCE algorithm outperforms state-of-the-art algorithms, especially in terms of safety.

## 2 Preliminaries

Constrained Markov decision processes (CMDP) provide a theoretical framework to solve safe RL problems , where the agent is cost-bounded by safety constraints. A CMDP is defined as a tuple \((,,C,P,r,_{0},)\), where \(^{n}\) is the state space, \(^{m}\) is the action space, \(P:\) is the transition kernel, which specifies the transition probability \(p(s_{t+1}|s_{t},a_{t})\) from state \(s_{t}\) to state \(s_{t+1}\) under the action \(a_{t}\), \(r:\) represents the reward function, \(C\) is the set of costs \(\{c_{i}:_{+},i=1,2,,m\}\) for violating \(m\) constraints, \((0,1]\) is the discount factor, and \(_{0}:\) is the distribution of initial states. A policy \(\) maps a probability distribution from state \(s_{t}\) to action \(a_{t}\). We utilize \(_{}\) to denote the parameterization of the policy with parameter \(\). In safe RL, the goal is to maximize the cumulative reward while satisfying safe constraints:

\[^{*}=_{}_{}[_{t=0}^{} ^{t}r(s_{t},a_{t})],_{} [_{t=0}^{}^{t}c_{i}(s_{t},a_{t})]_{i},\] (1)

where the \(=\{s_{0},a_{0},\}\) denotes the trajectory. \(_{i}\) is the \(i\)-th safe constraint limit.

In the offline RL setting, we learn a policy according to the offline dataset \(D\) collected by one or more data-collection policies, and without online interaction with the environment (\(D=\{(s_{t},a_{t},r_{t},c_{t})_{i}\}_{i=1}^{n}\)). Although this way of not interacting with the environment brings a lot of advantages, the offline dataset can not cover all action-state transitions, and the policy evaluation step actually utilizes the Bellman equation of a single sample . This makes the estimated Q-value vulnerable to OOD actions, which in turn severely affects the performance of the algorithm [24; 25]. As shown in Fig. 1, under the offline setting, off-policy safe RL methods neither learn a high-reward policy nor guarantee safety. Additionally, although offline RL methods obtain high rewards under safe expert data, it is difficult to guarantee safety because it directly ignores the cost. We can conclude that both offline RL and safe RL face challenges in learning policies that satisfy safety constraintsfrom offline datasets. Therefore, it is important to design an algorithm that learns high-reward and satisfies safety constraints, which is the focus of this paper.

## 3 Method

In this section, we present the details of Variationally Optimized with Conservatively Estimated for offline-safe RL algorithm (VOCE), which is the main contribution of this paper. We first reformulate the objective of offline-safe RL with probabilistic inference. Based on this, we derived upper and lower bounds for Q-value estimation, and adopted a pessimistic and conservative estimation approach to estimate the Q-value of costs and rewards respectively to eliminate the extrapolation error of OOD actions. Finally, we employ supervised learning to transform the variational distribution into a parameterized policy.

### Offline-safe RL as Inference

From the probabilistic inference perspective, the offline safe RL can be viewed as the problem of observing safe actions in states with high rewards. As the probabilistic inference model and notation in [26; 27], we introduce an optimal variable \(\) to represent the event of maximizing rewards. Assuming the likelihood of a given trajectory being optimal is proportional to the cumulative discounted reward, the infinite discounted reward formula is expressed as \((=1|)_{t}^{t}r(s_{t},a_{t})\). Since \((=1|) 0\), we further confirm that the likelihood of a given trajectory being optimal is proportional to the exponential of the accumulative discounted reward, then we rewrite the infinite discounted reward formula as \((=1|)^{t}r_{t}/)}\). The \(\) is a temperature parameter, and \(r_{t}\) is short for \(r(s_{t},a_{t})\). Let \(P_{}()\) be the probability of the trajectory under the policy \(\), then the log-likelihood of optimality under policy \(\) can be written as \(_{}(=1)=_{}(= 1|)P_{}()\). Furthermore, according to importance sampling and Jensen's inequality, we obtain a lower bound of the log-likelihood of optimality under policy \(\).

\[_{}(=1)=& _{}[( =1|)P_{}()}{()}]_{ }(=1|)P_{}()}{ ()}\\ &_{}_{t=0}^{ }^{t}r_{t}- D_{KL}(()||P_{}()), \] (2)

where the \(()\) is the auxiliary trajectory distribution. Since offline safe RL not only needs to consider maximizing cumulative rewards but also guaranteeing safety, we restrict \(()\) to the constrained feasible distribution space. According to the constraint threshold \(_{i}\), we define the feasible distribution space as:

\[^{_{i}}\{q(a_{t}|s_{t}):_{ q}[_{ t=0}^{}^{t}c_{i}(s_{t},a_{t})]_{i}\},\] (3)

where each \(q(a_{t}|s_{t})^{_{i}}\) indicates that the action distribution of the sate-condition satisfies the safety constraint. By factorizing the trajectory distribution , we express the trajectory distribution as:

\[()=(s_{0})_{t 0}p(s_{t+1}|s_{t},a_{t})q(a_{t}|s_{t}),q(a_{t}|s_{t})^{_{i}},\] (4)

\[P_{_{}}()=(s_{0})_{t 0}p(s_{t+1}|s_{t},a_{t})_{ }(a_{t}|s_{t}),\] (5)

Figure 1: Reward and cost curves of safe RL and off-policy RL under high-reward and safe datasets in _Point-Button_ task . The solid line is the mean of the performance, and the shaded line represents the variance of the performance. All performance curves employ more than 3 random seeds.

where \((s_{0})\) is the distribution of the initial state \(s_{0}\). Substituting Eq. (4) and (5) into Eq. (2) to eliminate the transitions \(p(s_{t+1}|s_{t},a_{t})\), and combining the feasible distribution space of Eq. (3), we obtain the object of offline safe RL problem as shown in Proposition 3.1.

**Proposition 3.1**.: _The objective of the offline safe RL problem can be defined through probabilistic inference as follows:_

\[(q,)=_{ q}_{t=0}^{}(^{ t}r_{t})- D_{KL}(q(|s_{t})||_{}(|s_{t})),\;q(a_{t} |s_{t})^{_{i}}.\] (6)

Proposition 3.1 provides a new objective for the problem of offline safe RL. The aforementioned probabilistic inference method has many optimizations over the previous dual methods in solving the problem of offline safe RL. This approach provides flexibility for optimizing the optimal policy by introducing a variational distribution \(q(a_{t}|s_{t})\) to connect the strong correlation between dual variable optimization and policy improvement. Furthermore, the approach decouples the optimal policy optimization problem into independent processes of optimal variational distribution optimization and parameterized policy update. We will introduce these two update processes in detail later.

### Variational Optimization with Conservative Estimates for Offline-safe RL

The previous standard RL and safe RL also employed the above-mentioned idea of probabilistic inference to solve the policy optimization problem [26; 28]. However, due to the influence of OOD actions in the offline setting, previous RL algorithms based on probabilistic inference are difficult to address the tasks of offline-safe RL. Therefore, we adopt a pessimistic conservative estimation method to improve the offline-safe RL algorithm based on probabilistic inference and obtain a **V**ariational **O**ptimization with **C**onservative **E**stimation algorithm (VOCE). We divide the VOCE into two steps: conservatively estimated variational optimization and parameterized policy update.

#### 3.2.1 Conservatively Estimated Variational Optimization

The goal of the conservatively estimated variational optimization is to optimize the optimal variational distribution \(q(a_{t}|s_{t})\) with a high reward while satisfying the safety constraints. In this step, we perform the maximization of the variational distribution \(q(a_{t}|s_{t})\) by fixing the policy \(_{}(a_{t}|s_{t})\). According to proposition 3.1, we write the objective of solving the optimal variational distribution as:

\[(q)\!=\!_{(s_{0})}_{q(a_{t}|s_{t})} Q^{}(s_{t},a_{t})\!-\! D_{KL}(q(|s_{t})||_{}(|s_{t})) ,_{(s_{0})}_{q(a_{t}| s_{t})}Q^{c_{i}}(s_{t},a_{t})\!\!_{i}.\] (7)

The optimization objective of the above variational distribution can be regarded as a KL-regularized constrained optimization problem. However, since the expected reward term \(_{q(a_{t}|s_{t})}Q^{r}(s_{t},a_{t})\) may be on different scales depending on the environment, it is difficult to set an appropriate penalty coefficient \(\) for different environments. Therefore, we introduce hard constraints to replace the regularization term of the KL-divergence and rewrite the optimization objective of Eq. (7) as:

\[&_{q}_{(s_{0})}_{a_{t}}q(a _{t}|s_{t})Q^{r}(s_{t},a_{t}),\;_{(s_{ 0})}_{a_{t}}q(a_{t}|s_{t})Q^{c_{i}}(s_{t},a_{t})_ {i},\\ &_{(s_{0})}D_{KL}(q(a_{t}|s_{t})||_{ }(a_{t}|s_{t})),_{a_{t}}q(a_{t}|s_{t})=1,  s_{t},\] (8)

where \(\) is the KL-divergence threshold between the variational distribution \(q(a_{t}|s_{t})\) and the parameterized policy \(_{}(a_{t}|s_{t})\). To solve the above-constrained problem (8), we replace the parametric \(q(a_{t}|s_{t})\) with the non-parametric to avoid performance degradation caused by approximation errors . Furthermore, we apply the Lagrange multiplier method to transform the above-constrained problem into an unconstrained problem. In the non-parametric form \(q(a_{t}|s_{t})\), since the objective function is linear and the constraints are convex, the constrained optimization problem shown in Eq. (8) is a convex optimization problem. Furthermore, through mild assumptions, we can obtain the strong dual form of Eq. (9).

**Assumption 3.2**.: _(Slater's condition). There exists a variational distribution that satisfies the safety constraints \(q(a_{t}|s_{t})^{_{i}}\) within the current policy trust region \(D_{KL}(q(a_{t}|s_{t})||_{}(a_{t}|s_{t}))<\)._

**Lemma 3.3**.: _Under Assumption 3.2, there exists a strong duality between the primal problem in Eq. (8) and the unconstrained problem in Eq. (9)._

\[(q,,,)&=_ { 0, 0,}_{q 0}_{(s_{0})} _{a_{t}}q(a_{t}|s_{t})Q^{r}(s_{t},a_{t})-^{T}_{a_{t}}q (a_{t}|s_{t})Q^{c_{i}}(s_{t},a_{t})-_{i}\\ &-_{a_{t}}q(a_{t}|s_{t})-1- D_{KL}q(a_{t}|s_{t})\|_{}(a_{t}|s_{t})- ,\] (9)

where \(,,\) are the Lagrange multipliers of the above multiple constraints. Based on the strong duality of the unconstrained problem in Eq. (9), we further derive a closed-form solution \(q^{*}(a_{t}|s_{t})\) for its internal maximization, as shown in Proposition 3.4. Proofs and discussions are in Appendix B.1.

**Proposition 3.4**.: _The closed-form solution of the optimal variational distribution that satisfies the safety constraint in Eq. (9) is given as:_

\[q^{*}(a_{t}|s_{t})=_{}(a_{t}|s_{t})(s_{t},a_{ t})}{}-,\] (10)

_where \(Q^{rc}(s_{t},a_{t}) Q^{r}(s_{t},a_{t})-^{T}Q^{c_{i}}(s_{t},a _{t})\)._

By substituting the closed-form solution (10) into Eq. (9) and eliminating similar terms, we obtain the convex minimization problem shown in Proposition 3.5. Subsequently, we solve for the dual variables \(\) and \(\) using Eq. (11). Proofs and discussions are in Appendix B.2.

**Proposition 3.5**.: _The \(,\) are dual variables and are solved via the convex optimization problem of the following formula._

\[(,)=_{ 0, 0}_{(s_{0})} _{a_{t}}_{}(a_{t}|s_{t}) (s_{t},a_{t})}{}+^{T}_{i}+.\] (11)

The Proposition 3.4 provides a proposal to solve a non-parametric variational distribution \(q(a_{t}|s_{t})\) for the given the Q-value \(Q^{r}(s_{t},a_{t})\) and \(Q^{c_{i}}(s_{t},a_{t})\). In addition, we provide the optimality and uniqueness of the above closed-form \(q^{*}(a_{t}|s_{t})\) solution under the premise of strong convexity in the Appendix B.2. Note that it can be seen from Eq. (10) that providing accurate \(Q^{r}(s_{t},a_{t})\) and \(Q^{c_{i}}(s_{t},a_{t})\) are the premise and guarantee for accurately computing the non-parametric variational distribution \(q(a_{t}|s_{t})\). We can find similar conclusions in the online safe RL algorithm CVPO , as described in Proposition 3.4 and 3.5. In the online setting, the empirical Bellman equation is directly applied to iteratively update the Q-value. However, in the offline setting, this approach would lead to clear extrapolation errors in \(Q^{c_{i}}(s_{t},a_{t})\) and \(Q^{r}(s_{t},a_{t})\) due to the OOD actions. Furthermore, it is difficult to accurately compute the variational distribution \(q(a_{t}|s_{t})\) according to Eq. (10).

To eliminate the extrapolation error caused by the OOD actions during the evaluation of the Q-value, we utilize the pessimistic conservative estimation approach to estimate \(Q^{r}\) and \(Q^{c_{i}}\), respectively. Specifically, to eliminate the impact of extrapolation error on \(Q^{r}\), we need to avoid overestimation of \(Q^{r}\). Therefore, similar to CQL  to learn a conservative lower bound of the Q-function by additionally minimizing the Q-values alongside a standard Bellman error objective, we choose the penalty that minimizes the expected Q-value under special action-state transitions for unknown actions produced by \(_{}(a_{t}|s_{t})\). Then, we define the Q-value of the iterative reward function as:

\[^{r}_{k+1}_{s_{t},a_{t} D} (Q^{r}(s_{t},a_{t}))-}^{}^{r}_{k}(s_{ t},a_{t})^{2}+_{s_{t} D,a_{t}_{}( |s_{t})}Q^{r}(s_{t},a_{t}),\] (12)

where the \(}^{}^{r}(s_{t},a_{t})=r(s_{t},a_{t})+^{r }(s_{t+1},a_{t+1})\). In Proposition 3.6, we show that \(^{r}(s_{t},a_{t})\) converges to a lower bound on \(^{r}(s_{t},a_{t})\). However, we can tighten this bound if we are interested in \(V^{r}(s_{t})\). We improve the bounds by introducing an additional maximization term under the sample distribution \(_{}\). Then the iterative update Eq. (12) of the reward Q-value can be rewritten as:

\[^{r}_{k+1} _{s_{t},a_{t} D}[(Q^{r}(s_{t},a_{t}))-}^{}^{r}_{k}(s_{t},a_{t})^{2}]+\\ [_{s_{t} D\\ a_{t}_{}(|s_{t})}Q^{r}(s_{t},a_{t})- _{s_{t} D\\ a_{t}_{}(|s_{t})}Q^{r}(s_{t},a_{t})], \] (13)

where \(\) is a tradeoff factor. Note that the Eq. (12) and (13) utilize the empirical Bellman operator \(}^{}\) instead of the actual Bellman Operator \(^{}\). Following the related work [23; 31], we employ theconcentration properties to compute the error. For any \( s_{t},a_{t} D\), with the probability \( 1-\), the sampling error can be written as:

\[|^{}(s_{t},a_{t})-}^{}(s_{t},a_{ t})|R_{max}}{(1-)(s_{t},a_{t})|}},\] (14)

where \(C_{r,p,}\) is a constant depending on the concentration properties of \(r(s_{t},a_{t})\), \(p(s_{t+1}|s_{t},a_{t})\) and the \(,(0,1)\). The \((s_{t},a_{t})|}}\) represents a vector of size \(||||\) containing the square root inverse count of each state action transition. Considering the sampling error as shown in Eq. (14), we can derive the condition for \(^{r}\) converging to the lower-bound \(Q^{r}\) at all \((s_{t},a_{t})\) through Eq. (12). The Proposition 3.6 provides the condition for \(^{r}\) to converge to the lower-bound of the \(Q^{r}\). Proofs and discussions are in Appendix B.3.

**Proposition 3.6**.: _For any \(_{}(a_{t}|s_{t})\) with supp \(_{}(a_{t}|s_{t})_{}\), \( s_{t},a_{t} D\), the Q-value function \(Q^{r}\) via iterating Eq. (12) satisfies:_

\[^{r}(s_{t},a_{t}) Q^{r}(s_{t},a_{t})-[(I- P^{r})^{ -1}}}{_{}}](s_{t},a_{t})+[(I-  P^{r})^{-1}R_{max}}{(1-)|} }](s_{t},a_{t}),\] (15)

_Thus, if \(R_{max}}{(1-)(s_{t},a_{ t})|}}[}(a_{t}|s_{t})}{_{}(a_{t}|s_{t})} ]^{-1}\), the iterative update Eq. (12) guarantees \(^{r} Q^{r}\)._

Next, when \(_{}(a_{t}|s_{t})=_{}(a_{t}|s_{t})\), we obtain a not lower-bound for the Q-values estimates pointwise. based on Eq. (13). We abuse the notation \(|}}\) to represent a vector of the inverse square root of state counts. Proofs and discussions are in Appendix B.4.

**Proposition 3.7**.: _When \(_{}(a_{t}|s_{t})=_{}(a_{t}|s_{t})\), according to Eq. (13), we obtain a lower bound for the true value of \(V^{r}=_{a_{t}_{}(a_{t}|s_{t})}[Q^{r}(s_{t},a_{t})]\) that satisfies the following inequality:_

\[^{r}(s_{t}) V^{r}(s_{t})-[(I- P^{r})^{-1}_{a_{t}_{}}[}{_{}}-1 ]](s_{t})+[(I- P^{r})^{-1}R_{max}} {(1-)|}}](s_{t}),\] (16)

_Thus, if \(R_{max}}{(1-)(s_{t})|}} [_{a_{t}_{}}[(a_{t}|s_{t}) }{_{}(a_{t}|s_{t})}-1]]^{-1}\), the Eq. (13) can guarantees \(^{r} V^{r}\)._

On the other hand, taking into account the need to satisfy the safety constraint \(_{(s_{0})}_{a}q(a_{t}|s_{t})Q^{c_{i}}(s_{t},a_{t}) _{i}\), the estimation of the cost Q-value must fulfill \(^{c_{i}} Q^{c_{i}}\). Based on the above analysis we choose the penalty that maximizes the expected Q-value under special action-state transitions for unknown actions produced by \(_{}(a|s)\). Therefore, we define the Q-value of the iterative reward function as:

\[^{c_{i}}_{k+1}\!\!_{s_{t},a_{t}  D}(Q^{c_{i}}(s_{t},a_{t}))\!-\!}^{} {Q}^{c_{i}}_{k}(s_{t},a_{t})^{2}\!-\!_{s D\\ a_{}(|s_{t})}Q^{c_{i}}(s_{t},a_{t}),\] (17)

where \(\) is the trade-off factor. The \(_{}\) denotes the marginal distribution corresponding to the unknown action. The Proposition 3.8 provides an upper bound on the convergence of the fixed point of \(^{c_{i}}\), and clarifies the conditions for \(^{c_{i}}\) to converge to the upper bound of \(Q^{c_{i}}\). Proofs and discussions are in Appendix B.5.

**Proposition 3.8**.: _For any \(_{}(a_{t}|s_{t})\) with supp \(_{}(a_{t}|s_{t})_{}\), \( s_{t},a_{t} D\), the Q-value function \(Q^{c_{i}}\) via iterating Eq. (17) satisfies:_

\[^{c_{i}}(s_{t},a_{t}) Q^{c_{i}}(s_{t},a_{t})\!+\![(I-  P^{})^{-1}}}{_{}}](s_{t},a_{ t})\!-\![(I- P^{})^{-1}C_{max}}{(1-)|}}](s_{t},a_{t}).\] (18)

_Thus, if \(C_{max}}{(1-)(s_{t},a_{t})|}} [}(a_{t}|s_{t})}{_{}(a_{t}|s_{t})} ]^{-1}\), the iterative update Eq. (17) can guarantee \(^{c_{i}} Q^{c_{i}}\)._

#### 3.2.2 Parametered Policy Update

After solving the optimal variational distribution of each state via Eq. (10), we need to obtain the policy parameters \(\). According to the solution target of Eq. (6), the optimization target can be obtained by eliminating the quantities irrelevant to \(\).

\[()\!=\!_{ q}[- D_{KL}(q(|s_{ t})\|_{}(|s_{t}))]=_{(s_{0})}_{q(a_{t}|s_{ t})}_{}(a_{t}|s_{t})\!-\! q(a_{t}|s_{t}),\] (19)where \( 0\) is the temperature parameters. In addition, the \(q(a_{t}|s_{t})\) is independent of \(_{}(a_{t}|s_{t})\). Therefore, the optimization objective of the above Eq. (19) can be rewritten as:

\[()=_{(s_{0})}_{q(a_{t}|s_{t})} _{}(a_{t}|s_{t}).\] (20)

## 4 Experimental Evaluation

In this section, we compare VOCE to previous offline safe RL methods in a range of domains and dataset compositions, including different action spaces and observation dimensions. To the best of my knowledge, there is currently no standardized dataset available in the field of offline safe RL. To facilitate further research and reproducibility of this work, we have collected a series of datasets using a trained online policy. The parameter settings of the dataset are in Appendix C.1.

### Task and Baseline

**Task.** We choose _Point-Goal_, _Car-Goal_, _Point-Button_ and _Car-Button_ four tasks widely adopted in the field of safe RL [26; 32; 33; 34; 29], as the experimental tasks for this work. A detailed description of each task is in Appendix C.2.

**Baselines.** BCQ-Lag is an offline safe RL algorithm that combines the Lagrange multiplier method with the BCQ  algorithm and employs adaptive penalty coefficients to implement offline constraints task. The C-CRR is an offline safe RL algorithm improved by the CRR  algorithm. It implements cost constraints by introducing cost evaluation Q-value function and Lagrange multipliers. Coptidice  is a policy optimization method based on the optimal stationary distribution space. The aforementioned three approaches are currently state-of-the-art algorithms in offline safe RL.

### Performance Comparison Experiment

To evaluate the performance of VOCE on diverse task and behavior samples, we collected sample data from three distinct behaviors within four different tasks. We introduce parameter \(\) to represent the proportion of trajectories in the sample data that satisfy the constraints. Then we employ \(\) to characterize samples to different behaviors. Fig. 2 illustrates the marginal distribution of rewards and costs for sample trajectories in the dataset at different values of \(\). The results from Fig. 2 reveal that as the value \(\) increases, the mean cost decreases, and the mean reward increases. Fig. 3 displays the reward and cost curves of VOCE and the state-of-the-art offline safe RL methods under different \(\) values for four tasks. The Fig. 3 results demonstrate that the VOCE achieves higher rewards across all tasks compared to the other three methods, while also satisfying or approaching the safety constraints threshold. Especially in the _Goal_ task, VOCE consistently meets the safety constraints across different \(\) values, while achieving higher reward returns. In the _Button_ task, when the parameter \(\) is small, VOCE struggles to ensure safety; however, the cost curve of VOCE remains lower than the other three methods. Based on the aforementioned results analysis, it can be concluded that the VOCE exhibits competitive performance across various combinations of samples from multiple tasks, particularly excelling in terms of safety compared to the state-of-the-art algorithms currently available.

### Ablation Experiment

**The parameter \(\) of the dataset.** Fig. 4 displays the boxplots of rewards and costs for the VOCE under different parameter values of \(\) in the _Point-Goal_ and _Point-Button_ tasks. The results from

Figure 2: Distribution of rewards and costs for samples collected by different behavioral policies.

Fig. 4 reveals an intriguing phenomenon: VOCE does not achieve the highest reward and lowest cost at \(\)=1.0 but instead attains the highest reward within the range of 0.7 to 0.9 (The range will vary depending on changes in the sampling method or policy.). This indicates that appropriately increasing the number of constraint-satisfying trajectories in the dataset benefits VOCE in improving its rewards and reducing costs. However, excessive augmentation of constraint-satisfying trajectories in the dataset may lead to a decrease in rewards and even an increase in costs.

**Conservative estimation of rewards and costs.** To assess the impact of conservative estimation on VOCE, we conducted ablation experiments on the conservative estimation of both reward and cost Q-values. Fig. 5 illustrates the rewards and costs of VOCE, VOCE-Qr, and VOCE-Qc across four tasks. VOCE-Qr stands for the VOCE method with the conservative estimation of Q-values removed for rewards. VOCE-Qc represents the VOCE method with the conservative estimation of Q-values removed for costs. The results presented in Fig. 5 demonstrate that the rewards of VOCE-Qr are notably lower than those of VOCE. This indicates that employing a lower-bound conservative estimation of reward Q-values helps eliminate extrapolation errors caused by OOD actions, thereby significantly improving the reward of VOCE. Furthermore, the results from Fig. 5 reveal that the

Figure 4: Ablation experiments with the same sampling policies and different \(\) coefficients.

Figure 3: The reward and cost curves of VOCE and baseline algorithms with different sample data under 4 tasks. The curve is averaged over 3 random seeds, where the solid line is the mean and the shaded area is the standard deviation.

rewards of VOCE-Qc are comparable to or even surpass those of VOCE. However, in some tasks, the costs of VOCE-Qc exceed the cost constraints. This suggests that utilizing an upper-bound conservative estimation of cost Q-values helps reduce the costs of VOCE, thereby enhancing the safety of VOCE.

## 5 Related Work

In this section, we elaborate on the work pertaining to offline safe RL and scrutinize three aspects of safe RL, offline RL, and offline safe RL.

**Safe RL.** Currently, Safe RL typically addresses optimization objectives with constraints using the primal-dual framework. The Lagrange version of PPO and TRPO [36; 37] widely used as the underlying baselines combines the constraint information with the original objective function via Lagrange multipliers. CPO  is the first general-purpose policy search algorithm for safe RL that guarantees that the near constraints are satisfied at each iteration. However, the second-order method [38; 39] requires second-order information and thus brings a higher computational burden. To address the approximation errors associated with Taylor approximations and inverting a high-dimensional Fisher information matrix, first-order methods such as CUP  and APPO  achieve better performance and implementation.

**Offline RL.** Offline RL, also known as batch RL, considers the problem of updating a policy from an offline dataset without interacting with the environment. There are mainly two approaches of policy constraints and Q-value regularization to solve the problem of OOD actions in offline RL [42; 43; 44]. The earliest proposed Batch-Constrained deep Q-learning (BCQ) algorithm  is a typical policy-constrained offline RL algorithm, which employs CVAE to learn behavioral policy generation models, and finds the optimal policy by maximizing the Q-value. On this basis, a series of policy-constrained algorithms [45; 46; 47; 48] are proposed, which mainly constrain the actions explored through the behavioral policy model. In addition, due to the obvious stability of the Q-value regularization approach, it has been extensively studied. A conservative Q-learning algorithm  proposed by Kumar et al., learns a conservative Q-value by enhancing the standard Bellman error objective with a simple Q-value regularizer. Subsequently, a series of Q-value regularization methods [49; 6; 50] were proposed which generally learn a conservative Q-value through regularization items or clipping Q-learning.

**Offline safe RL.** Offline safe RL is to solve the optimization problem of safety policy under the offline dataset. An offline safe RL algorithm with constraint penalty is proposed , which improves the objective function of the cost Q-value via using the constraint penalty item, and ensures the safety of the policy via terminating unsafe trajectory in advance. On the other hand, a constrained offline optimization algorithm (COPO)  defines the RL problem based on linear programming and constrains the distance between the final policy and offline sample behavior through regulation terms to solve the problem of offline safe RL algorithms. Additionally, this algorithm sets the discount factor \(=1\), and changes the maximizing discount reward objective to the maximizing mean reward objective. Similar to the COPO algorithm, an offline policy optimization via stationary distribution correction estimation (CoptiDICE)  also utilizes the linear programming method of RL to solve

Figure 5: Ablation study for conservative Q-value estimation of reward and cost, and constrained regularize of the policy update. The curve is averaged over 3 random seeds, where the solid line is the mean and the shaded area is the standard deviation.

the optimal stationary distribution instead of the gradient strategy, which expands the discount factor to \((0,1]\).

## 6 Conclusion

In this work, we propose a variational optimization with conservative estimation for offline safe RL algorithm to address the problem of optimizing safe policies using offline datasets. We introduce probabilistic inference to reframe the problem of offline safe RL, and we mitigate estimation errors caused by parameter approximation through employing nonparametric variational distributions instead of parameterized policies. Furthermore, we utilize the upper and lower bounds of Q-values to estimate the Q-values of cost and reward, thereby mitigating extrapolation errors caused by OOD actions. Finally, extensive comparisons and ablation experiments demonstrate that the VOCE algorithm outperforms state-of-the-art algorithms in terms of safety.