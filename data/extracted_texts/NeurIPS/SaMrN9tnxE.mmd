# ReMaX: Relaxing for Better Training on Efficient

Panoptic Segmentation

Shuyang Sun\({}^{1}\)1 Weijun Wang\({}^{2}\) Qihang Yu\({}^{}\) Andrew Howard\({}^{2}\) Philip Torr\({}^{1}\) Liang-Chieh Chen\({}^{}\)

\({}^{1}\)University of Oxford \({}^{2}\)Google Research

Work done during internship at Google Research. Correspondence to: kevinsun@robots.ox.ac.ukWork done while at Google.

###### Abstract

This paper presents a new mechanism to facilitate the training of mask transformers for efficient panoptic segmentation, democratizing its deployment. We observe that due to the high complexity in the training objective of panoptic segmentation, it will inevitably lead to much higher penalization on false positive. Such unbalanced loss makes the training process of the end-to-end mask-transformer based architectures difficult, especially for efficient models. In this paper, we present ReMaX that adds relaxation to mask predictions and class predictions during the training phase for panoptic segmentation. We demonstrate that via these simple relaxation techniques during training, our model can be consistently improved by a clear margin **without** any extra computational cost on inference. By combining our method with efficient backbones like MobileNetV3-Small, our method achieves new state-of-the-art results for efficient panoptic segmentation on COCO, ADE20K and Cityscapes. Code and pre-trained checkpoints will be available at https://github.com/google-research/deeplab2.

## 1 Introduction

Panoptic segmentation  aims to provide a holistic scene understanding  by unifying instance segmentation  and semantic segmentation . The comprehensive understanding of the scene is obtained by assigning each pixel a label, encoding both semantic class and instance identity. Prior works adopt separate segmentation modules, specific to instance and semantic segmentation, followed by another fusion module to resolve the discrepancy . More recently, thanks to the transformer architecture , mask transformers  are proposed for end-to-end panoptic segmentation by directly predicting class-labeled masks.

Although the definition of panoptic segmentation only permits each pixel to be associated with just one mask entity, some recent mask transformer-based methods  apply sigmoid cross-entropy loss (_i.e._, not enforcing a single prediction via softmax cross-entropy loss) for mask supervision. This allows each pixel to be associated with multiple mask predictions, leading to an extremely unbalanced loss during training. As shown in Figure 1, when using the sigmoid cross-entropy loss to supervise the mask branch, the false-positive (FP) loss can be even \(10^{3}\) larger than the false-negative (FN) loss. Surprisingly, such unbalanced loss leads to better results than using softmax cross-entropy, which indicates that the gradients produced by the FP loss are still helpful for better performance.

However, the radical imbalance in the losses makes it difficult for the network to produce confident predictions, especially for efficient backbones , as they tend to make more mistakes given the smaller model size. Meanwhile, the training process will also become unstable due to the large scale loss fluctuation. To address this issue, recent approaches  need to carefully clipthe gradients during training to a very small value like 0.01; otherwise, the loss would explode and the training would collapse. In this way, the convergence of the network will also be slower. A natural question thus emerges: _Is there a way to keep those positive gradients, while better stabilizing the training of the network?_

To deal with the aforementioned conflicts in the learning objectives, one naive solution is to apply _weighted_ sigmoid cross entropy loss during training. However, simply applying the hand-crafted weights would equivalently scale the losses for all data points, which means those positive and helpful gradients will be also scaled down. Therefore, in this paper, we present a way that can adaptively adjust the loss weights by only adding training-time relaxation to mask-transformers . In particular, we propose two types of relaxation: Relaxation on Masks (ReMask) and Relaxation on Classes (ReClass).

The proposed ReMask is motivated by the observation that semantic segmentation is a relatively easier task than panoptic segmentation, where only the predicted semantic class is required for each pixel without distinguishing between multiple instances of the same class. As a result, semantic segmentation prediction could serve as a coarse-grained task and guide the semantic learning of panoptic segmentation. Specifically, instead of directly learning to predict the panoptic masks, we add another auxiliary branch during training to predict the semantic segmentation outputs for the corresponding image. The panoptic prediction is then calibrated by the semantic segmentation outputs to avoid producing too many false positive predictions. In this way, the network can be penalized less by false positive losses.

The proposed ReClass is motivated by the observation that each predicted mask may potentially contain regions involving multiple classes, especially during the early training stage, although each ground-truth mask and final predicted mask should only contain one target in the mask transformer framework . To account for this discrepancy, we replace the original one-hot class label for each mask with a softened label, allowing the ground-truth labels to have multiple classes. The weights of each class is determined by the overlap of each predicted mask with all ground-truth masks.

By applying such simple techniques for relaxation to the state-of-the-art kMaX-DeepLab , our method, called ReMaX, can train the network stably without any gradient-clipping operation with a over \(10\) greater learning rate than the baseline. Experimental results have shown that our method not only speeds up the training by \(3\), but also leads to much better results for panoptic segmentation. Overall, ReMaX sets a new state-of-the-art record for efficient panoptic segmentation. Notably, for efficient backbones like MobileNetV3-Small and MobileNetV3-Large , our method can outperform the strong baseline by \(4.9\) and \(5.2\) in PQ on COCO panoptic for short schedule training; while achieves \(2.9\) and \(2.1\) improvement in PQ for the final results (_i.e._, long schedules). Meanwhile,

Figure 1: **The histogram shows the ratio of false positives to false negatives for the cross-entropy loss, on a logarithmic scale**. When using sigmoid as the activation function, the false positive loss is always over \(100\) greater than the false negative, making the total loss to be extremely unbalanced.

our model with a Axial-ResNet50 (MaX-S)  backbone outperforms all state-of-the-art methods with \(3\) larger backbones like ConvNeXt-L  on Cityscapes . Our model can also achieve the state-of-the-art performance when compared with the other state-of-the-art efficient panoptic segmentation architectures like YOSO  and MaskConverv  on COCO , ADE20K  and Cityscapes  for efficient panoptic segmentation.

## 2 Related Work

Mask Transformers for image segmentation.Recent advancements in image segmentation has proven that Mask Transformers , which predict class-labeled object masks through the Hungarian matching of predicted and ground truth masks using Transformers as task decoders [64; 4], outperform box-based methods [35; 70; 54] that decompose panoptic segmentation into multiple surrogate tasks, such as predicting masks for detected object bounding boxes  and fusing instance and semantic segmentation [48; 10] with merging modules [42; 53; 45; 71; 12; 41]. The Mask Transformer based methods rely on converting object queries to mask embedding vectors [32; 62; 67], which are then multiplied with pixel features to generate predicted masks. Other approaches such as Segmenter  and MaskFormer  have also used mask transformers for semantic segmentation. K-Net  proposes dynamic kernels for generating masks. CMT-DeepLab  suggests an additional clustering update term to improve transformer's cross-attention. Panoptic Segformer  enhances mask transformers with deformable attention . Mask2Former  adopts masked-attention, along with other technical improvements such as cascaded transformer decoders , deformable attention , and uncertainty-based point supervision , while kMaX-DeepLab  employs k-means cross-attention. OneFormer  extends Mask2Former with a multi-task train-once design. Our work builds on top of the modern mask transformer, kMaX-DeepLab , and adopts novel relaxation methods to improve model capacity.

The proposed Relaxation on Masks (ReMask) is similar to the masked-attention in Mask2Former  and the k-means attention in kMaX-DeepLab  in the sense that we also apply pixel-filtering operations to the predicted masks. However, our ReMask operation is fundamentally distinct from theirs in several ways: (1) we learn the threshold used to filter pixels in panoptic mask predictions through a semantic head during training, while both masked-attention  and k-means attention  use either hard thresholding or argmax operation on pixel-wise confidence for filtering; (2) our approach relaxes the training objective by applying a pixel-wise semantic loss on the semantic mask for ReMask, while they do not have explicit supervision for that purpose; and (3) we demonstrate that ReMask can complement k-means attention in Section 4.

Acceleration for Mask Transformers for efficient panoptic segmentation.DETR  successfully proves that Transformer-based approaches can be used as decoders for panoptic segmentation, however, it still suffer from the slow training problem which requires over 300 epochs for just one go. Recent works [14; 73; 79; 50] have found that applying locality-enhanced attention mechanism can help to boost the speed of training for instance and panoptic segmentation. Meanwhile, some other works [76; 43; 33] found that by removing the bi-partite matching for _stuff_ classes and applying a separate group of mask queries for _stuff_ classes can also help to speed up the convergence. Unlike them, which apply architectural level changes to the network, our method only applies training-time relaxation to the framework, which do not introduce any extra cost during testing. Apart from the training acceleration, recent works [26; 29; 12; 55; 51] focus on how to make the system for panoptic segmentation more efficient. However, all these works focus on the modulated architectural design while our approach focus on the training pipeline, which should be two orthogonal directions.

Coarse-to-fine refinement for image segmentation.In the field of computer vision, it is a common practice to learn representations from coarse to fine, particularly in image segmentation. For instance, DeepLab [6; 8] proposes a graph-based approach [38; 7] that gradually refines segmentation results. Recently, transformer-based methods for image and video segmentation such as [66; 14; 76; 69; 43; 20; 74; 78] have also adopted a multi-stage strategy to iteratively improve predicted segmentation outcomes in transformer decoders. The concept of using coarse-grained features (_e.g._, semantic segmentation) to adjust fine-grained predictions (_e.g._, instance segmentation) is present in certain existing works, including [11; 2; 3]. However, these approaches can lead to a substantial increase in model size and number of parameters during both training and inference.

By contrast, our ReMaX focuses solely on utilizing the coarse-fine hierarchy for relaxation _without_ introducing any additional parameters or computational costs during inference.

Regularization and relaxation techniques.The proposed Relaxation on Classes (ReClass) involves adjusting label weights based on the prior knowledge of mask overlaps, which is analogous to the re-labeling strategy employed in CutMix-based methods such as [75; 5; 60], as well as label smoothing  used in image classification. However, the problem that we are tackling is substantially different from the above label smoothing related methods in image classification. In image classification, especially for large-scale single-class image recognition benchmarks like ImageNet , it is unavoidable for images to cover some of the content for other similar classes, and label smoothing is proposed to alleviate such labelling noise into the training process. However, since our approach is designed for Mask Transformers [66; 13; 14; 73; 72] for panoptic segmentation, each image is precisely labelled to pixel-level, there is no such label noise in our dataset. We observe that other than the class prediction, the Mask Transformer approaches also introduce a primary class identification task. The proposal of ReClass operation reduces the complexity for the classification task in Mask Transformers. Prior to the emergence of Mask Transformers, earlier approaches did not encounter this issue as they predicted class labels directly on pixels instead of masks.

## 3 Method

Before delving into the details of our method, we briefly recap the framework of mask transformers  for end-to-end panoptic segmentation. Mask Transformers like [66; 14; 76; 69; 43] perform both semantic and instance segmentation on the entire image using a single Transformer-based model. These approaches basically divide the entire framework into 3 parts: a backbone for feature extraction, a pixel decoder with feature pyramid that fuses the feature generated by the backbone, and a transformer mask decoder that translates features from the pixel decoder into panoptic masks and their corresponding class categories.

In the transformer decoder, a set of mask queries is learnt to segment the image into a set of masks by a mask head and their corresponding categories by a classification head. These queries are updated within each transformer decoder (typically, there are at least 6 transformer decoders) by the cross-attention mechanism  so that the mask and class predictions are gradually refined. The set of predictions are matched with the ground truth via bipartite matching during training; while these queries will be filtered with different thresholds as post-processing during inference. We follow the same post-processing as kMaX-DeepLab .

Figure 4: **Demonstration on how ReClass works.** We utilize the mask rendered in blue as an example. Our ReClass operation aims to soften the class-wise ground-truth by considering the degree of overlap between the prediction and ground-truth mask. The blue mask intersects with both masks of “baseball glove” and “person”, so the final class weights contain both and the activation of ”person” in the prediction will no longer be regarded as a false positive case during training.

Figure 3: **ReMask Operation.** Modules, representations and operations rendered in gray are _not_ used in testing. \(\) and \(\) represent the matrix multiplication and Hadamard multiplication and + means element-wise sum. The \(\) symbol and “stop grad” mean that no gradient is flown to \(}\) from \(}\) during training.

### Relaxation on Masks (ReMask)

The proposed Relaxation on Masks (ReMask) aims to ease the training of panoptic segmentation models. Panoptic segmentation is commonly viewed as a more intricate task than semantic segmentation, since it requires the model to undertake two types of segmentation (namely, instance segmentation and semantic segmentation). In semantic segmentation, all pixels in an image are labeled with their respective class, without distinguishing between multiple instances (_things_) of the same class. As a result, semantic segmentation is regarded as a more coarse-grained task when compared to panoptic segmentation. Current trend in panoptic segmentation is to model _things_ and _stuff_ in a unified framework and resorts to train both the coarse-grained segmentation task on _stuff_ and the more fine-grained segmentation task on _things_ together using a stricter composite objective on _things_, which makes the model training more difficult. We thus propose ReMask to exploit an auxiliary semantic segmentation branch to facilitate the training.

Definition.Here we first define \(H,W\) as the height and width of the feature, \(N_{Q}\) as the number of mask queries. \(N_{C}\) denotes the number of semantic classes for the target dataset, \(d_{q}\) is the number of channels for the query representation, and \(d_{}\) is the number of channels for the input of semantic head. As shown in Figure 2 and 3, given a mask representation \(}}^{HW N_{Q}}\), we apply a panoptic mask head to generate panoptic mask logits \(}}^{HW N_{Q}}\). A mask classification head to generate the corresponding classification result \(^{N_{Q} N_{C}}\) is applied for each query representation \(^{N_{Q} d_{}}\). A semantic head is applied after the semantic feature \(}}^{HW d_{}}\) from the pixel decoder to produces a pixel-wise semantic segmentation map \(}}^{HW N_{C}}\) assigning a class label to each pixel. As for the structure for semantic head, we apply an ASPP module  and a \(1 1\) convolution layer afterwards to transform \(d_{}\) channels into \(N_{C}\) channels as the semantic prediction. Note that the whole auxiliary semantic branch will be skipped during inference as shown in Figure 3. Since the channel dimensionality between \(}}\) and \(}}\) is different, we map the semantic masks into the panoptic space by:

\[}_{}=(}})( ^{}),\] (1)

where \(()\) function represents the sigmoid function that normalizes the logits into interval \(\). Then we can generate the relaxed panoptic outputs \(}_{}\) in the semantic masking process as follows:

\[}_{}=_{}+(}_{}_{}),\] (2)

where the \(\) represents the Hadamard product operation. Through the ReMask operation, the false positive predictions in \(_{}\) can be suppressed by \(}_{}\), so that during training each relaxed mask query can quickly focus on areas of their corresponding classes. Here we apply identity mapping to keep the original magnitude of \(_{}\) so that we can remove the semantic branch during testing. This makes ReMask as a complete relaxation technique that does not incur any overhead cost during testing. The re-scaled panoptic outputs \(}_{}\) will be supervised by the losses \(_{}\).

Stop gradient for a simpler objective to \(}_{}\).In order to prevent the losses designed for panoptic segmentation from affecting the parameters in the semantic head, we halt the gradient flow to \(_{}\), as illustrated in Figure 3. This means that the semantic head is solely supervised by a semantic loss \(_{}\), so that it can focus on the objective of semantic segmentation, which is a less complex task.

How does ReMask work?As defined above, there are two factors that ReMask operation helps training, (1) the Hadamard product operation between the semantic outputs and the panoptic outputs that helps to suppress the false positive loss; and (2) the _relaxation_ on training objectives that trains the entire network simultaneously with consistent (_coarse-grained_) semantic predictions. Since the semantic masking can also enhance the locality of the transformer decoder like , we conducted experiments by replacing \(_{}\) with ground truth semantic masks to determine whether it is the training relaxation or the local enhancement that improves the training. When \(_{}\) is assigned with ground truth, there will be no \(_{}\) applied to each stage, so that \(_{}\) is applied with the most accurate local enhancement. In this way, there are large amount of false positive predictions masked by the ground truth semantic masks, so that the false positive gradient will be greatly reduced. The results will be reported in Section 4. The semantic masking can be viewed as local enhancement as it would suppress the extreme false-positive predictions via a simple masking operation.

### Relaxation on Classes (ReClass)

Mask Transformers [66; 14; 73; 43] operate under the assumption that each mask prediction corresponds to a single class, and therefore, the ground truth for the classification head are one-hot vectors. However, in practice, each imperfect mask predicted by the model during the training process may intersect with multiple ground truth masks, especially during the early stage of training. As shown in Figure 4, the blue mask, which is the mask prediction, actually covers two classes ("baseball glove" and "person") defined in the ground truth. If the class-wise ground truth only contains the class "baseball glove", the prediction for "person" will be regarded as a false positive case. However, the existence of features of other entities would bring over-penalization that makes the network predictions to be under-confident.

To resolve the above problem, we introduce another relaxation strategy on class logits, namely Class-wise Relaxation (ReClass), that re-assigns the class confidence for the label of each predicted mask according to the overlap between the predicted and ground truth semantic masks. We denote the one-hot class labels as \(\), the ground truth binary semantic masks as \(=[_{0},...,_{HW}]\{0,1\}^{HW N_{C}},\) the supplement class weights is calculated by:

\[_{m}=_{})^{}}{ _{i}^{HW}_{i}},\] (3)

where \(_{m}\) denotes the label weighted by the normalized intersections between the predicted and the ground truth masks. With \(_{m}\), we further define the final class weight \(}^{N_{C}}\) as follows:

\[}=_{m}+(1-_{m}),\] (4)

where the \(\) denotes the smooth factor for ReClass that controls the degree of the relaxation applying to the classification head.

## 4 Experimental Results

### Datasets and Evaluation Metric

Our study of ReMaX involves analyzing its performance on three commonly used image segmentation datasets. **COCO** supports semantic, instance, and panoptic segmentation with 80 "things" and 53 "stuff" categories; **Cityscapes** consists of 8 "things" and 11 "stuff" categories; and **ADE20K** contains 100 "things" and 50 "stuff" categories. We evaluate our method using the Panoptic Quality (PQ) metric defined in  (for panoptic segmentation), the Average Precision defined in  (for instance segmentation), and the mIoU  metric (for semantic segmentation).

  Method & Backbone & Resolution & FPS & PQ \\  Panoptic-DeepLab  & MNV3-L  & 641\(\)641 & 26.3 & 30.0 \\ Panoptic-DeepLab  & RS0  & 641\(\)641 & 20.3 & 35.1 \\ Real-time  & RS0  & 800\(\)1333 & 15.9 & 37.1 \\ MaskConvex  & MN-MH  & 640\(\)640 & 20.2 & 37.2 \\ MaskFormer  & RS0  & 800\(\)1333 & 17.6 & 46.5 \\ YOSO  & RS0  & 800\(\)1333 & 23.6 & 48.4 \\ YOSO  & RS0  & 512\(\)804 & 45.6 & 46.4 \\ kMax-DeepLab  & RS0  & 1281\(\)1281 & 16.3 & 53.0 \\  ReMax-\(^{}\) & MNV3-S  & 641\(\)641 & 108.7 & **40.4** \\ ReMaX-S\({}^{}\) & MNV3-L  & 641\(\)641 & 80.9 & **44.6** \\ ReMaX-M\({}^{}\) & RS0  & 641\(\)641 & 51.9 & **49.1** \\ ReMaX-B & RS0  & 1281\(\)1281 & 16.3 & **54.2** \\  

Table 1: Comparison with other state-of-the-art efficient models (\(\) 15 FPS) on COCO _val_ set. The Pareto curve is shown in Figure 6 (b). The FPS of all models are evaluated on a NVIDIA V100 GPU with batch size 1. \({}^{}\) represent the application of efficient pixel and transformer decoders. Please check the appendix for details.

Figure 5: Performance on COCO _val_ compared to the baseline kMaX-DeepLab . ReMaX can lead to \(3\) faster convergence compared to the baseline, and can improve the baselines by a clear margin. The performance of ResNet-50 can be further improved to 54.2 PQ when the model is trained for 200K iterations.

### Results on COCO Panoptic

Implementation details.The macro-architecture of ReMaX basically follows kMaX-DeepLab , while we incorporate our modules introduced in Section 3 into the corresponding heads. Concretely, we use the _key_ in each k-means cross-attention operation as \(}\) defined in Figure 3. The semantic head introduced during training consists of an ASPP module  and a \(1 1\) convolution that outputs \(N_{C}\) number of channels. The specification of models with different size is introduced in the appendix. For other details like post-processing and data preparation, we strictly follow kMaX-DeepLab .

Training details.We basically follow the training recipe proposed in kMaX-DeepLab  but make some changes to the hyper-parameters since we add more relaxation to the network. Here we high-light the necessary and the full training details and specification of our models can be also found in the appendix. The learning rate for the ImageNet-pretrained  backbone is multiplied with a smaller learning rate factor 0.1. For training augmentations, we adopt multi-scale training by randomly scaling the input images with a scaling ratio from 0.3 to 1.7 and then cropping it into resolution \(1281 1281\). Following , we further apply random color jittering , and panoptic copy-paste augmentation  to train the network. DropPath  is applied to the backbone, the transformer decoder. AdamW  optimizer is used with weight decay 0.005 for short schedule 50K and 100K with a batch size 64. For long schedule, we set the weight decay to 0.02. The initial learning rate is set to 0.006, which is multiplied by a decay factor of 0.1 when the training reaches 85% and 95% of the total iterations. The entire framework is implemented with DeepLab2  in TensorFlow . Following , we apply a PQ-style loss, a Mask-ID cross-entropy loss, and the instance discrimination loss to better learn the feature extracted from the backbone.

For all experiments if not specified, we default to use ResNet-50 as the backbone and apply ReMask to the first 4 stages of transformer decoder. The \(\) for ReClass operation is set to 0.1. All models are trained for 27 epochs (_i.e._, 50K iterations). The loss weight for the auxiliary semantic loss \(_{}\) applied to each stage in the transformer decoder is set to 0.5 and the weights for those loss terms in \(_{}\) are set the same as kMaX-Deeplab.

ReMaX significantly improves the training convergence and outperforms the baseline by a large margin.As shown in Figure 5, we can see that when training the model under different training schedules 50K, 100K and 150K, our method outperform the baselines by a clear margin for all different schedules. Concretely, ReMaX can outperform the state-of-the-art baseline kMaX-DeepLab by a significant 3.6 PQ when trained under a short-term schedule 50K iterations (27 epochs) for backbone ResNet-50. Notably, our model trained with only 50K iterations performs even better than kMaX-DeepLab  trained for the 100K iterations (54 epochs), which means that our model can speed up the training process by approximately \(2\). We kindly note that the performance of ResNet-50 can be further improved to 54.2 PQ for 200K iterations. ReMaX works very well with

Figure 6: **FPS-PQ Pareto curve** on (a) **COCO Panoptic _val_ set** and (b) **Cityscapes _val_ set**. Details of the corresponding data points can be found in Table 1 and 9. We compare our method with other state-of-the-art efficient pipelines for panoptic segmentation including kMaX-DeepLab , Mask2Former , YOSO , Panoptic-DeepLab , Real-time Panoptic Segmentation , UPSNet , LPSNet , MaskFormer , and MaskConver .

[MISSING_PAGE_FAIL:8]

**The relative contribution of loss relaxation and local enhancement in ReMask.** The auxiliary semantic loss term can be viewed as loss relaxation, while the semantic masking branch can be viewed as local enhancement. To disentangle the relative contribution of loss relaxation and local enhancement, we conducted another ablation study that removes the semantic masking branch (the concrete grey arrow right under "stop grad" in Figure 3), which would remove the local enhancement (semantic masking) but keep the auxiliary semantic loss term for loss relaxation. The results are reported in Table 8. The short training schedule of 50K iterations shows that the semantic loss relaxation leads to a 0.9 increase in PQ; while the semantic masking contributes to an additional 0.4 gain in PQ. The long-schedule training (i.e. 150K iterations) demonstrates that semantic masking is critical in ReMask because applying semantic loss relaxation alone without semantic masking does not result in any improvement. In other words, only using semantic loss relaxation may expedite the early stage of training (e.g, 50K iterations), but it fails to improve the ultimate convergence quality.

**The number of mask relaxation.** Table 3 shows the effect of the number of ReMask applied to each stage, from which we can observe that the performance gradually increases and reaches its peak at 52.4 PQ when the number of ReMask is 4, which is also our final setting for all other ablation studies. Using too many ReMask (\(>4\)) operations in the network may add too many relaxation to the framework, so that it cannot fit well to the final complex goal for panoptic segmentation.

**ReClass can also help improve the performance for ReMaX.** We investigate ReClass and its hyper-parameter \(\) in this part and report the results in Table 4. In Table 4, we ablate 5 different \(\) from 0 to 0.2 and find that ReClass performs the best when \(=0.1\), leading to a \(0.5\) gain compared to the strong baseline. The efficacy of ReClass validates our assumption that each mask may cover regions of multiple classes.

**Effect of the removing auxiliary semantic head for ReMask during testing.** The ReMask operation can be both applied and removed during testing. In Table 5, it shows that the accuracy is comparable under the two settings. In Table 5 we also show the necessity of applying identity mapping to \(_{}\) during training in order to remove the auxiliary semantic head during testing. Without the identity mapping at training, removing semantic head during testing would lead to \(0.5\) drop from \(52.4\) (the first row in Table 5) to \(51.9\).

### Results on Cityscapes

**Implementation details.** Our models are trained using a batch size of 32 on 32 TPUv3 cores, with a total of 60K iterations. The first 5K iterations constitute the warm-up stage, where the learning rate gradually increases from 0 to \(3 10^{-3}\). During training, the input images are padded to \(1025 2049\) pixels. In addition, we employ a multi-task loss function that includes four loss components with different weights. Specifically, the weights for the PQ-style loss (part of \(_{}\)), auxiliary semantic loss \(_{}\), mask-id cross-entropy loss (part of \(_{}\)), and instance discrimination loss are set to 3.0, 1.0, 0.3 and 1.0, respectively. To generate feature representations for our model, we use 256 cluster centers and incorporate an extra bottleneck block in the pixel decoder, which produces features with an output stride of 2. These design are basically proposed in kMaX-DeepLab  and we simply follow here for fair comparison.

**Results on Cityscapes.** As shown in Table 9 and Figure 6 (b), it shows that our method can achieve even better performance when using a smaller backbone MobileNetV3-Large (62.5 PQ) while the

   w/semantic masking & w/\(L_{sem}\) & w/\(\) & Iterations & PQ \\ (local enhancement) & (loss relaxation) &? & & \\  \(\) & \(\) & \(\) & 50K & 50.4 \\ \(\) & \(\) & \(\) & 150K & 53.0 \\  \(\) & ✓ & \(\) & 50K & 51.3 \\ \(\) & ✓ & \(\) & 150K & 53.0 \\  ✓ & ✓ & \(\) & 50K & 51.7 \\  ✓ & ✓ & ✓ & 50K & **52.4** \\ ✓ & ✓ & ✓ & 150K & **54.0** \\   

Table 8: The relative impact of _loss relaxation_ and _semantic masking_ (local enhancement) on COCO Panoptic _val_ set under short (50K) and long (150K) training schedule.

other methods are based on ResNet-50. Meanwhile, our model with Axial-ResNet-50 (_i.e._, MaX-S, 74M parameters) as the backbone can outperform the state-of-the-art models [31; 73] with a ConvNeXt-L backbone (> 220M parameters). The Pareto curve in Figure 6 (b) clearly demonstrates the efficacy of our method in terms of speed-accuracy trade-off.

### Results on ADE20K

Implementation details.We basically follow the same experimental setup as the COCO dataset, with the exception that we train our model for 100K iterations (54 epochs). In addition, we conduct experiments using input resolutions of \(1281 1281\) pixels and \(641 641\) respectively. During inference, we process the entire input image as a whole and resize longer side to target size then pad the shorter side. Previous approaches use a sliding window approach, which may require more computational resources, but it is expected to yield better performance in terms of accuracy and detection quality. As for the hyper-parameter for ReMask and ReClass, we used the same setting as what we propose on COCO.

Results on ADE20K.In Table 11, we compared the performance of ReMaX with other methods, using ResNet-50 as the backbone, and found that our model outperforms the baseline model by \(1.6\) in terms of mIOU, which is a clear margin compared to the baseline, since we do not require any additional computational cost but only the relaxation during training. We also find that our model can surpass the baseline model kMaX-DeepLab by \(1.1\) in terms of PQ. When comparing with other frameworks that also incorporate ResNet-50 as the backbone, we show that our model is significantly better than Mask2Former and MaskFormer by \(3.7\) and \(8.7\) PQ respectively.

## 5 Conclusion

This paper presents a novel approach called ReMaX, comprising two components, ReMask and ReClass, that leads to better training for panoptic segmentation with Mask Transformers. The proposed method is shown to have a significant impact on training speed and final performance, especially for efficient models. In principle, ReMaX has the potential to be generalized to other non-transformer-based panoptic segmentation frameworks as long as it has a panoptic mask representation and a semantic mask representation. In this paper, we mainly verify our method based on state-of-the-art mask transformers . We will further validate the generalization capability of our method in future work. We hope that our work will inspire further investigation in this direction, leading to more efficient and accurate panoptic segmentation models.

  Method & Backbone & FPS \& params & PQ \\  Mask2Former  & Swin-L  & - & 216M & 66.6 \\ kMaX-DeepLab  & Max-S  & 6.5 & 74M & 66.4 \\
6MaX-DeepLab  & ConvNeXt-L  & 3.1 & 232M & 68.4 \\ OneFormer  & ConvNeXt-L  & - & 220M & 68.5 \\  ReMaX &  & 6.5 & 74M & **68.7** \\  

Table 10: Cityscapes _val_ set results for larger backbones. \({}^{}\)Pre-trained on ImageNet-22k.

  Method & Backbone & FPS \& params \\  Mask2Former  & Swin-L  & - & 216M & 66.6 \\ kMaX-DeepLab  & Max-S’ & 6.5 & 74M & 66.4 \\
6MaX-DeepLab  & ConvNeXt-L  & 3.1 & 232M & 68.4 \\ OneFormer  & ConvNeXt-L  & - & 220M & 68.5 \\  ReMaX &  & 6.5 & 74M & **68.7** \\  

Table 11: ADE20K _val_ set results. Our FPS is evaluated on a NVIDIA V100 GPU under the corresponding resolution reported in the table.

  Method & Backbone & FPS \& params \\  Mask2Former  & Swin-L  & - & 216M & 66.6 \\ kMaX-DeepLab  & Max-S’ & 6.5 & 74M & 66.4 \\
6MaX-DeepLab  & ConvNeXt-L  & 3.1 & 232M & 68.4 \\ OneFormer  & ConvNeXt-L  & - & 220M & 68.5 \\  ReMaX &  & 6.5 & 74M & **68.7** \\  

Table 10: Cityscapes _val_ set results for larger backbones. \({}^{}\)Pre-trained on ImageNet-22k.

Acknowledgement.We would like to thank Yukun Zhu, Xuan Yang at Google Research for their kind help and discussion. Shuyang Sun and Philip Torr are supported by the UKRI grant: Turing AI Fellowship EP/W002981/1 and EPSRC/MURI grant: EP/N019474/1. We would also like to thank the Royal Academy of Engineering and FiveAI.