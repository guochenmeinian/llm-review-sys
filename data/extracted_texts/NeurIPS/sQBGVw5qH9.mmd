# Cocktail \(\overline{\mathbb{Y}}\) : Mixing Multi-Modality Controls for Text-Conditional Image Generation

Cocktail \(}\) : Mixing Multi-Modality Controls for Text-Conditional Image Generation

Minghui Hu\({}^{}\)

Jianbin Zheng\({}^{}\)

Daqing Liu\({}^{}\)

Chuanxia Zheng\({}^{@sectionsign}\)

Chaoyue Wang\({}^{@paragraphsign}\)

Dacheng Tao\({}^{@paragraphsign}\)

Tat-Jen Cham\({}^{}\)

\({}^{}\)_Nanyang Technological University, \({}^{}\)South China University of Technology, \({}^{@sectionsign}\)University of Oxford, \({}^{@paragraphsign}\)The University of Sydney, \({}^{}\)JD Explore Academy_

###### Abstract

Text-conditional diffusion models are able to generate high-fidelity images with diverse contents. However, linguistic representations frequently exhibit ambiguous descriptions of the envisioned objective imagery, requiring the incorporation of additional control signals to bolster the efficacy of text-guided diffusion models. In this work, we propose Cocktail, a pipeline to mix various modalities into one embedding, amalgamated with a generalized ControlNet (gControlNet), a controllable normalisation (ControlNorm), and a spatial guidance sampling method, to actualize multi-modal and spatially-refined control for text-conditional diffusion models. Specifically, we introduce a hyper-network gControlNet, dedicated to the alignment and infusion of the control signals from disparate modalities into the pre-trained diffusion model. gControlNet is capable of accepting flexible modality signals, encompassing the simultaneous reception of any combination of modality signals, or the supplementary fusion of multiple modality signals. The control signals are then fused and injected into the backbone model according to our proposed ControlNorm. Furthermore, our advanced spatial guidance sampling methodology proficiency incorporates the control signal into the designated region, thereby circumventing the manifestation of undesired objects within the generated image. We demonstrate the results of our method in controlling various modalities, proving high-quality synthesis and fidelity to multiple external signals. The codes are released at https://mhh0318.github.io/cocktail/.

## 1 Introduction

Text-conditional diffusion models  have actualized the capacity for high-quality generative capabilities. These models facilitate the generation of an array of high-calibre images through the utilization of concise textual prompts. However, linguistic representations pose inherent challenges in accurately encapsulating the precise imagery anticipated by the user, owing to the potential ambiguities and subjective interpretations of verbal descriptions within the context of visual synthesis. Moreover, minor alterations to the textual prompts also yield distinct visual outputs, underscoring the absence of refined control over the generative process.

Modifying the prior is a series of existing solutions for multi-modal control, _e.g._, the control of the entire prior space  as demonstrated in Fig. 1(a). These approaches centered on the whole prior lack the capacity for localised image modifications and the preservation of background elements. Moreover, these models typically require training from scratch, which demands a substantial amount of resources.

In response to the methods dealing with latent representation [29; 4; 27], an additional lightweight hyper-network is introduced in [53; 33], which is designed to encode external control signals into latent vectors and subsequently inject them directly into the backbone network. Such methods effectively handle control over the single additional modality; however, they exhibit limitations when confronted with multiple modalities, as shown in Fig 1(b). One issue is that each modality requires a unique network, leading to a computational overhead that escalates proportionally with the increase in the number of modalities. Furthermore, the impact of additive coefficients between different modes on the final imaging outcomes warrants consideration. The inherent imbalance among superimposed modes makes these additive coefficients a pivotal factor in determining the ultimate synthesized output. An additional challenge emerges during the sampling process when the model conducts an initial inference devoid of control signal injection. This preliminary inference step could result in object placement that potentially contradicts the control signals.

In this paper, we propose a novel pipeline, as shown in Fig. 1(c), termed Cocktail, which accomplishes multi-modality control through a text-conditional diffusion model. It encompasses three main components: **1)** a hyper-network capable of accommodating multi-modal input, **2)** a conditional normalisation method to mix the control features, and **3)** a sampling strategy designed to facilitate precise control over the generative process. As shown in Fig. 2, our method is proficient in generating images that meet all input conditions or any arbitrary subset thereof, _utilizing only one single model_.

To achieve this goal, we initially trained a branched network named gControlNet, which accepts multiple modalities. Upon training, the branched network can simultaneously accept arbitrary combinations of existing modalities. When multiple modalities coexist within the same region, the model is capable of automatically fusing input from different modalities, balancing the disparities between them. To leverage the features of the gControlNet, we further proposed controllable normalisation (ControlNorm). We can achieve better representation of control signals in terms of semantic and spatial aspects according to the decoupling provided by ControlNorm.

Moreover, we introduce a spatial guidance sampling method in order to facilitate spatial generation under the purview of multi-modal control signals. Specifically, we employ distinct textual prompts to differentiate entities from the background, incorporating entities into the background via a prompt editing approach. Our devised sampling approach demonstrates efficacy in circumventing the generation of undesired entities.

In summary, our main contributions are as follows:

* We introduced the Generalized ControlNet (gControlNet), a branched network capable of adaptively integrating multi-modal information, effectively addressing issues stemming from imbalances between modalities;
* We proposed the Controllable Normalisation (ControlNorm) to optimize the utilization of information within branched networks to yield more effective outcomes;
* We introduced a spatial guidance sampling method based on the operation within the attention map to generate relevant information tailored to regional contexts, preventing the inclusion of undesired objects outside the specified regions.

Figure 1: **Comparison of various control methods.** Our approach requires only _one generalized model_, unlike previous that needed multiple models for multiple modalities.

## 2 Related Work

Text-conditional Diffusion Models.Diffusion models [19; 46] has achieved great success in the area of text-to-image synthesis [34; 39; 41; 3; 14]. To reduce the computational cost, diffusion models typically function within the latent space  or produce low-resolution images that are later improved through super-resolution models [39; 3]. Fast sampling methods have also successfully reduced the number of generation steps required by diffusion models from hundreds down to merely a few [45; 32; 25; 30; 11]. The provision of classifier guidance during the sampling process can also significantly impact the outcomes, leading to substantial improvements in the results . In addition to the widely used classifier-free guidance , other types of guidance are also worth exploring [55; 13].

HyperNetworks for Pre-trained Models.Training a diffusion model is highly resource-intensive and environmentally unfriendly . Fine-tuning such a model can also be challenging due to the vast number of parameters involved . Therefore, introducing an additional tiny branched network to bias the output of the original network is a more reasonable choice [10; 2; 8]. Similar ideas have also proven to be effective in diffusion models: Hypernet  and LoRA  models are capable of altering the original diffusion model's sampling distribution by training a small branched network, which has become the most popular branched network in the current research community. More similar to our work is ControlNet , which is subsequently combined with different layers in the denoising U-Net to provide support for various task-specific guidance. It presents impressive results with various conditional inputs, _yet with only one modality for each model_. In contrast, our cocktail endows the ControlNet with multitasking capabilities using only _one single model_.

Conditional Normalisationapproaches have been employed across a range of vision tasks, including style transfer [24; 12], conditional generation [36; 7; 14; 21] and image-to-image translation . These techniques involve normalizing layer activations to zero mean and unit deviation, followed by denormalisation through an affine transformation derived from external data. External data can be in multiple formats, such as style images, semantic masks, or category labels.

Figure 2: **Examples of our model with the same prompt.** Given a text prompt along with various modality signals, our approach is able to synthesize images that satisfy _all input conditions_ or _any arbitrary subset_ of these conditions using _a single model_. The prompt is: _A girl holding a cat_.

Attention Map-based Prompt Tuning.Prompt-to-Prompt  is a method that adjusts local or global specifics in text-guided diffusion models by altering the cross-attention maps from source to target image, thereby maintaining spatial layout and geometry. Recently, numerous efforts have been made to improve the outcomes [35; 37]. However, such approaches are limited to synthesized images without an inversion technique. Unlike cross-attention, self-attention focuses on inter-pixel relationships within the same domain . Paint-with-words  is another method that allows users to specify the spatial locations of objects by selecting phrases from the text prompt.

## 3 Methods

In this work, our _main goal_ is to design a controllable generator that utilizes _various modalities_ of input, within a _single model_. To achieve this, we first propose a more general branched network, gControlNet, which can generate control signals from different modalities using a single network and adaptively weighted fuse them together. Furthermore, we propose ControlNorm to inject the signals from the gControlNet into the diffusion backbone, which solves the imbalanced problem within various modalities. Finally, we propose a spatial guidance sampling method to avoid the presence of extraneous objects in the generation process. By modifying the attention map, this approach effectively incorporates control signals into the backbone network. The whole pipeline is demonstrated in Fig. 3.

### Generalized ControlNet with Controllable Normalisation

Generalized ControlNet.ControlNet  is a method designed to influence and control the behavior of neural networks by adjusting the input conditions of specific network blocks. Instead of directly modifying the parameters of the primary network, ControlNet employs an auxiliary network to generate feature offsets. These offsets are then combined with different layers in the main network, such as a denoising U-Net, to support various task-specific guidance.

Given a trained backbone network block \((;)\) with parameter \(\), the input feature \(\) can be mapped to \(\). For the branched part, we duplicate the parameter \(\) to create a trainable copy \(_{t}\), which is

Figure 3: **The network architecture** of Generalized ControlNet (gControlNet) with Controllable Normalisation (ControlNorm). The parameters indicated by the yellow sections are sourced from the pre-trained model and stay constant, while only those in the blue sections are updated during training, with the gradient back-propagated along the blue arrows.

then trained using the supplementary modality. Preserving the original weights helps retain the information stored in the initial model after training on large-scale datasets, which ensures that the quality and diversity of the generated images do not degrade. Mathematically, the output from the trained network block can be expressed as:

\[=(;)+((+ (_{m});_{t}))(; ),\] (1)

where the control signal of a single modality \(_{m}\) is typically processed to obtain a format identity for \(\), for example, through a zero-initialized convolutional layer, and then added to \(\). \(()\) represents the zero-initialized layer. It is not only used in the process of handling \(_{m}\), but also serves to adjust the output of the branched network \((,_{t})\).

To accomplish the goals of accepting multiple external modalities as input and balancing signals from different modalities, we have devised a modified framework that adeptly merges these varied sources of information. At the top of our network, we adopt a simple downsampling network \(()\) to convert external conditional signals to the latent space, allowing the conditional signals to be directly injected into the latent space. It is worth noting that \(()\) is versatile and can adapt to different types of external signals. Given \(k\) different modalities, the converted conditional features are \(_{m}^{k}=(C^{k})\).

Controllable Normalisation.Instead of directly passing the sum of conditional features via a zero-initialized layer to the network block \((;_{t})\), _i.e._, \(}_{m}=(_{i}^{i})\), we introduce a _controllable normalisation (ControlNorm)_ method, which has an additional layer to generate two sets of learnable parameters, \((}_{m})\) and \((}_{m})\), conditioned on all \(k\) modalities. These two sets of parameters are used in the conditional normalisation layer to fuse the external conditional signals and the original signals. Specifically, the input to the trainable block \((;_{t})\) becomes:

\[(+((}_{m})))-_{c}()}{_{c}()}((}_{m}))+(_{m}),\] (2)

where \(_{c}()\) and \(_{c}()\) are the mean and standard deviation of the feature \(\) along the channel \(c\), \(\) is Hadamard product, and \((})\) and \((})\) are vectors that have the same dimension as \(\). With the help of zero-convolution, we can preserve the identity of \(x\) just before the start of the fine-tuning. It is worth noting that in the following layers, the internal feature in the branched network \(\) can be integrated with the latent feature \(\) from the original network in the same way:

\[(+(()))-_{c }()}{_{c}()}(()) +(_{m}),\] (3)

where \(\) is the intermediate features from the original network and \(\) is the intermediate features from the branched network. Specifically, we will have five sets of intermediate features from the Stable Diffusion  U-Net backbone and our generalized ControlNet, including four sets of features from encoder blocks and one from the middle block.

In fact, our controllable normalisation is a generalized version of conditional normalisation [36; 24]. After changing the mean and variance calculation dimension and replacing the external signal \(}\) by a mask image, real image, or class labels, we can derive the various forms of SPADE , AdaIN , CIN  and MoVQ . More interestingly, our controllable normalisation method not only enables the use of external signals as conditions, but also allows intermediate-layer signals to act as constraints.

As shown in Fig. 3, only the parameters of the gControlNet require updating. Given an initial latent \(_{0}\) and a time-step \(t\), the diffusion process progressively introduces noise \(\) to this latent, transforming the original latent into a noisy state \(_{t}\). The objective of the diffusion model is to estimate the noise \(\) adding to the \(\). Our proposed gControlNet shares the same objective function as the diffusion model, aiming to predict the noise added at time \(t\). The only distinction lies in the incorporation of multimodal information as conditional signals \(}_{m}=[_{m}^{0},_{m}^{1},,_{m}^{k}]\):

\[=_{_{0},t,_{p},_{m}, (0,1)}[\|-_{}(_{t},t, _{p},}_{m})\|_{2}^{2}]\] (4)

### Spatial Guidance Sampling

In order to leverage the control signals from generalized ControlNet and ensure that the generated objects appear within the areas of interest, we proposed a _spatial guidance_ sampling method. Here, we mainly focus on editing the cross-attention layers of the U-Net in Stable Diffusion.

Given a noisy latent \(\) at timestep \(t\) as the input to the pretrained U-Net with \(n\) blocks \(^{(n)}(;^{(n)})\) at different resolution, the latent vector \(\) will be down-sampled to various dimensional \(^{(n)}\) that map to the corresponding block \(^{(n)}\). The cross-attention maps \(^{(n)}^{(N_{i},N_{t})}\) in each block, parameterized by \(^{(n)}\), are associated with the linear projection of latent feature \(Q^{(n)}=f_{Q}((^{(n)}))\) and the prompt \(K=f_{K}(^{})\):

\[A^{(n)}_{ij|^{(n)}}= Q^{(n)}_{i},K_{j} }{_{k=1} Q^{(n)}_{i},K_{k}},\] (5)

where \(A^{(n)}_{ij}\) represents the attentional strength between the \(j\)-th prompt token (among \(N_{t}\) tokens) and the \(i\)-th latent feature (among \(N_{i}\) features). Intuitively, for each prompt token \(K_{j}\), there exists a corresponding latent feature map \(A^{(n)}_{j}\) with spatial information. Moreover, the corresponding feature map will contain spatial and shape information for the associated object. However, it is not only the object-describing tokens that contain information about the object's location and shape, some connecting words and padding tokens also convey spatial information for the overall scene .

We apply a masking strategy to the corresponding attention maps. In detail, we construct two sets of attention masks \(M^{(n)}\) and \(M^{(n)}^{(N_{i},N_{t})}\). Each column \(M^{(n)}_{j}\) and \(M^{(n)}_{j}\) is a flattened alpha mask, which is determined by the visibility of the corresponding text token \(K_{j}\). The values of \(M^{(n)}_{ij}\) and \(M^{(n)}_{ij}\) are determined based on the relationship between image token \(Q_{i}\) and text token \(K_{j}\). On the one hand, if image token \(Q_{i}\) corresponds to a region of the image that should be influenced by text token \(K_{j}\), \(M^{(n)}_{ij}\) is assigned the value of 1. On the other hand, if image token \(Q_{i}\) corresponds to a region of the image that should not be influenced by text token \(K_{j}\), \(M^{(n)}_{ij}\) is set to 1. It is worth noting that we consider the feature maps corresponding to most words as negative in order to avoid generating undesired objects. The mask components \(M^{(n)}\) and \(M^{(n)}\) are incorporated into the cross-attention computation process:

\[^{(n)}_{ij|^{(n)}}= Q^{(n)}_{i},K_{ j}+^{}M^{(n)}-^{}M^{(n)}}{_{k=1} Q^{(n)}_{i},K_{k}}.\] (6)

It is found that larger weights at higher noise levels  can lead to better results, thus \(^{}\) and \(^{}\) are noise-level sensitive parameters, defined by:

\[^{()}=^{}(1+)(^{(n)}),\] (7)

where \(^{}\) is a user-provided hyper-parameter.

We then substitute the feature map corresponding to the object description. Contrary to the image editing motivation behind conventional prompt tuning methods, it is not necessary for our method to provide a reference image and an amended prompt. Recalling the framework of our generalized ControlNet, the branch architecture is identical to the encoder portion of the backbone network. We found that the attention maps \(A^{(n)}_{j|^{(n)}_{i}}\) within the branch network also encompass object locations and shape information. Consequently, we opt for the attention map \(A^{(n)}_{j|^{(n)}_{i}}\) associated with the description \(K_{j}\) from the respective layer in ControlNet as the source for the original attention map \(A^{(n)}_{j|^{(n)}}\) substitution, ensuring that the information in the substituted attention map aligns more closely with the external input signal rather than the textual information derived from the original backbone network. Specifically, we replace the attention map generated by the original backbone network (based on the corresponding object description) with the attention map from the corresponding module in the branch network:

\[}^{(n)}=[^{(n)}_{0|^{(n)}};;A^{(n)}_{j|^{(n)}};;^{(n)}_{N_{t}|^{(n)}}][ ^{(n)}_{0|^{(n)}};;^{(n)}_{j|^{( n)}};;^{(n)}_{N_{t}|^{(n)}}].\] (8)

Subsequently, we can produce the spatially guided output from cross-attention layer by taking the product of \(}^{(n)}\) with \(V\).

## 4 Experiments

In this section, we delve into a comprehensive experimental analysis to validate the efficacy and superiority of the proposed method through ablation studies and application demonstrations. Subsequently, in Sec. 4.1, we put forth both quantitative and qualitative results, elucidating the comparative advantages of our approach. We also present an array of intriguing applications made possible by our gControlNet, showcasing its practical utility. Finally, Sec. 4.2 is dedicated to the discussion of ablation studies, scrutinising the impacts of varying injection and sampling methods. The experimental configurations, including the dataset specifications, implementation details, and evaluation metrics, can be found in the Appendix.

### Applications and Comparisions

Cocktail is proficient in seamlessly supporting multiple control inputs and autonomously fusing them, thereby eliminating the necessity for manual intervention to equilibrate diverse modalities. This unique property empowers users to easily incorporate a variety of modalities, resulting in more flexible multi-modal control. In Figure 4, we demonstrate how users can provide spatial information about multiple objects in different modalities to generate a complex scene. Notably, this entire process is accomplished by a single model without the need for additional branch networks.

We then compare Cocktail with two state-of-the-art methods: ControlNet  and T2I-Adapter , in the context of text-guided image-to-image translation within multiple modalities. We employ several evaluation methods, including LPIPS, mPA, mIoU, mAP and L2 distance for various modalities in this section. As depicted in Table 1, our method outperformed both ControlNet and T2I-Adapter across all evaluation metrics. This remarkable achievement signifies that our proposed cocktail can generate a structural image that closely resembles the ground truth image and aligns better with the input conditions, establishing its superiority. The visualization in Fig. 6 also illustrates the superior ability of our model to harmonize with the control signals.

We further present some samples from our method to examine its effectiveness on uni-modality translation. The visual comparison on the COCO validation set is showcased in Figure 5, highlighting the compelling performance. Benefiting from mixed training of multiple modalities, our method achieves remarkable generation quality, surpassing even models exclusively trained for a single control signal.

Our experiments show that Cocktail effectively leverages information from different modalities and exhibits outstanding multi-object generation abilities with consistent composition across various control signals.

  
**Method** & Similarity & Sketch Map & Segmentation Map & Segmentation Map & Pose Map \\  & (LPIPS \(\)) & (L2 Distance \(\)) & (mPA \(\)) & (mIoU \(\)) & (mAP \(\)) \\  Multi-Adapter & 0.7273 \(\)0.00120 & 7.93310 \(\)0.01392 & 26.30 \(\)0.242 & 13.98 \(\)0.177 & 40.02 \(\)0.761 \\ Multi-ControlNet & 0.6653 \(\)0.00145 & 7.59721 \(\)0.01516 & 36.59 \(\)0.273 & 22.70 \(\)0.229 & 38.19 \(\)0.761 \\ _Ours_ w/o ControlNorm & 0.4900 \(\)0.00144 & **7.18413**\(\)0.01453 & 48.26 \(\)0.287 & 32.66 \(\)0.272 & 61.93 \(\)0.775 \\ _Ours_\(\)\(\) & **0.4836**\(\)0.00133 & 7.28929 \(\)0.01385 & **49.20**\(\)0.289 & **33.27**\(\)0.271 & **61.99**\(\)0.778 \\   

Table 1: Quantitative comparison on the COCO5k validation set. The best result is highlighted.

Figure 4: Our model can generate images with the provided prompts and multi-modality information (e.g., edge, pose, and segmentation map) across various scales.

### Ablations

gControlNet and ControlNormPrevious methods  utilized a direct sum approach to fuse control signals from hyper-networks with latent variables from the original network. While this method conveys spatial information, it fails to consider semantic information, such as text or other modalities. Control signals decoupled through ControlNorm allow the preservation of semantic information while conveying spatial information. Another point is the need for normalization to address the imbalance of different modality signals. However, typical normalization methods lead to the loss of semantic information . Therefore, the control signals introduced through ControlNorm can better interpret conditional information. We present some generated images in Fig. 10 to substantiate the interpretative capability of ControlNorm.

Spatial Guidance SamplingThe spatial guidance sampling method not only ensures that objects are generated within controllable areas, but also minimizes the impact on other areas. An intuitive application is that when we modify certain objects or modalities, other parts of the generated image can remain unchanged. Figure 7 illustrates the contrast between employing spatial guidance sampling and its absence. A more significant variation in the overall tone of the resultant image is observed when spatial guidance is not utilized, leading to inconsistencies in the details of the attire. Conversely, the incorporation of new objects and modalities with the use of spatial guidance minimally impacts the original image.

Figure 5: Qualitative comparison of Uni-Modality on the COCO validation set.

## 5 Conclusion

In this work, we proposed Cocktail, a pipeline to achieve fine control by fusing multi-modal signals. Firstly, we introduced a generalized ControlNet capable of handling signals of different modalities using a single model. We also revealed that the semantic information of signals from the hyper-network may be lost through direct addition. However, a simple decoupling allows for a better interpretation of control signals. We introduced a controllable normalisation for decoupling and integration. Finally, we proposed a sampling scheme that can prevent the generation of unnecessary objects outside the focus area while also achieving a certain degree of editing capability and background protection.

Limitations.While we have achieved multi-modal fusion and control, there are still certain limitations in the handling of control signals from Cocktail that need to be addressed in future work. Firstly, although the current spatial guidance method can effectively prevent objects from being generated outside the focus area, it requires users to individually specify the area and corresponding object description during implementation. Secondly, spatial guidance can cause instability in the latent space in certain situations, leading to the generated images degrading and deviating from the existing control signals. Therefore, finding a stable anchor point as a reference during the generation process is also worth exploring.

Broader Impacts.The integration of multimodal control signals as inputs for synthesis greatly enhances user interaction flexibility and streamlines the utilization of text-conditional diffusion models. However, the process of fine-tuning large-scale generative models to accommodate diverse modalities requires a significant amount of energy, and we anticipate that the development of a universal model capable of accepting multiple modalities will help mitigate this impact. However, the growing capabilities in image generation also facilitate the production of manipulated images with malicious intent, such as the creation of counterfeit or deceitful information.

Figure 6: Cocktail can address the imbalance among various modalities. Here, the ”cross” symbol and the checkmark symbol denote the unmatched and matched modalities, respectively. It is important to note that our model accurately captures all modalities.