# Recursive Nested Filtering for Efficient Amortized Bayesian Experimental Design

Sahel Iqbal

Aalto University

&Hany Abdulsamad

Aalto University

&Sara Perez-Vieites

Aalto University

&Simo Sarkkai

Aalto University

&Adrien Corenlos

University of Warwick

Correspondence address: sahel.iqbal@aalto.fi

###### Abstract

This paper introduces the Inside-Out Nested Particle Filter (IO-NPF), a novel fully recursive algorithm for amortized sequential Bayesian experimental design in the non-exchangeable setting. We frame policy optimization as maximum likelihood estimation in a non-Markovian state-space model, achieving (at most) \((T^{2})\) computational complexity in the number of experiments. We provide theoretical convergence guarantees and introduce a backward sampling algorithm to reduce trajectory degeneracy. The IO-NPF offers a practical, extensible, and provably consistent approach to sequential Bayesian experimental design, demonstrating improved efficiency over existing methods.

## 1 Introduction

Bayesian experimental design (BED, Chaloner and Verdinelli, 1995; Lindley, 1956) is a mathematical framework for designing experiments to maximize information about the parameters of a statistical model. The primary objective of BED is to select a user-controllable input, known as the _design_, that is optimal in the sense that the resulting observation is, on average, maximally informative. This optimal design is obtained by solving a joint maximization and integration problem, which is typically intractable and thus requires approximation (Kueck et al., 2009). The difficulty is further compounded in sequential experiments, where the conventional _myopic_ strategy of optimizing one experiment at a time is both suboptimal and time-consuming (Huan, Jagalur, et al., 2024; Rainforth et al., 2024).

An alternative to the myopic approach for sequential BED is to learn a _policy_ instead of optimizing for individual designs (Foster et al., 2021; Huan and Marzouk, 2016; Ivanova et al., 2021). In this approach, we pay an upfront cost to learn the policy, but experiments can be performed in real-time. A recent addition to this literature is Iqbal et al. (2024), which leverages the duality between control and inference (Attias, 2003; Toussaint and Storkey, 2006) to rephrase policy optimization for BED as a maximum likelihood estimation (MLE) problem in a nonlinear, non-Markovian state-space model (SSM). Iqbal et al. (2024) also introduced the _Inside-Out (IO) SMC\({}^{2}\)_ algorithm, a nested particle filter targeting this non-standard SSM.

The IO-SMC\({}^{2}\) was shown to be a sample-efficient alternative to state-of-the-art BED amortization methods (Iqbal et al., 2024). However, this algorithm has two key limitations. First, it is not a fully recursive algorithm, as it requires reprocessing all past observations for each experiment within an inner particle filter, which incorporates a Markov chain Monte Carlo (MCMC) kernel. Second, the transition density of this MCMC kernel cannot be computed, and generating samples fromthe smoothing distribution is done via genealogy tracking (Kitagawa, 1996, Section 4.1), which degenerates for long experiment sequences \(T 1\). In view of this, our contributions are as follows:

1. We propose a novel, _fully recursive_, provably consistent, nested particle filter that performs sequential BED, which we refer to as the Inside-Out NPF (IO-NPF). The IO-NPF replaces the MCMC kernel in IO-SMC\({}^{2}\) with the jittering kernel from the _nested particle filter_ (NPF) of Crisan and Miguez (2017, 2018).
2. We propose a Rao-Blackwellized (Appendix A.2) backward sampling scheme (Godsill et al., 2004) for the IO-NPF to counter the degeneracy of the genealogy tracking smoother, thus improving the overall amortization performance.

## 2 Problem Setup

Let \(\) denote the parameters of interest, \(_{t}\) the design, and \(x_{t}\) the outcome of the \(t\)-th experiment, where \(t\{0,1,,T\}\). For brevity, we define the augmented state \(z_{t}\{x_{t},_{t-1}\}\) for \(t 1\) and \(z_{0}\{x_{0}\}\). We are interested in the non-exchangeable data setting where the outcomes, conditionally on \(\) and the designs \(_{0:T-1}\), follow Markovian dynamics,

\[p(x_{0:T}_{0:T-1},)=p(x_{0})_{t=1}^{T}f(x_{t} x_{t-1}, _{t-1},).\]

The designs \(_{t}\) are sampled from a stochastic policy \(_{}(_{t} z_{0:t})\) that depends on the history until experiment \(t\) and is parameterized by \(\). The joint density of the states, designs, and parameters can then be expressed as \(p_{}(z_{0:T},)=p()p(z_{0})_{t=1}^{T}p_{}(z_{t} z _{0:t-1},),\) where \(p(z_{0})=p(x_{0})\) and \(p_{}(z_{t} z_{0:t-1},)=f(x_{t} x_{t-1},_{t-1},)\, _{}(_{t-1} z_{0:t-1})\).

Our goal is to optimize the _expected information gain_ (EIG, Lindley, 1956) objective with respect to the policy parameters \(\). The EIG is defined as

\[()_{p_{}(z_{0:T})} p()-[p( z_{0:T})],\]

where \(\) denotes the entropy of a distribution. Hence, the EIG represents the expected reduction in entropy in the parameters \(\) under the policy \(_{}\), and the corresponding optimization problem is \(^{*}*{arg\,max}_{}\ \ ()\).

### The Dual Inference Problem

If the noise in the dynamics is static, then, equivalently (Iqbal et al., 2024, Proposition 1),

\[()_{p_{}(z_{0:T})}[_{t=1}^{T}r_{t }(z_{0:t})],\] (1)

where '\(\)' denotes equality up to an additive constant and

\[r_{t}(z_{0:t})- p( z_{0:t-1})f(x_{t} x_{t-1}, _{t-1},)\,.\]

Following Toussaint and Storkey (2006), we define a _potential function_\(g_{t}(z_{0:t})\,r_{t}(z_{0:t})}\), where \(>0\), and construct a non-Markovian Feynman-Kac model (Del Moral, 2004)\(_{t}\),

\[_{t}(z_{0:t};)=()}\,p(z_{0})_{s=1}^{t}p_{ }(z_{s} z_{0:s-1})\,g_{s}(z_{0:s}), 0 t T.\] (2)

Here, \(p_{}(z_{t} z_{0:t-1})= p_{}(z_{t} z_{0:t-1},)\,p(  z_{0:t-1})\,\) are the marginal dynamics under the filtered posterior \(p( z_{0:t-1})\) and \(Z_{t}()= g_{1:t}(z_{0:t})\,p_{}(z_{0:t})\,z_{0:t}\) is the normalizing constant, with \(g_{1:t}(z_{0:t})=_{s=1}^{t}g_{s}(z_{0:s})\).

Iqbal et al. (2024) showed that maximizing \(Z_{t}()\) is equivalent to maximizing a proxy EIG objective. The authors performed this maximization by evaluating the score \(()\) under samples from \(_{T}(;)\)

\[()_{} Z_{T}()=_{} p _{}(z_{0:T})\,_{T}(z_{0:T};)\,z_{0:T},\]and using a Markovian score climbing procedure (Naesseth et al., 2020) to perform policy optimization (see details in Appendix C). In Iqbal et al. (2024), sampling from \(_{T}(;)\) was done by constructing a _non-recursive_ particle filter, named Inside-Out SMC\({}^{2}\). The following section outlines a novel and efficient procedure to generate these samples in a _fully recursive_ manner.

## 3 The Inside-Out Nested Particle Filter (IO-NPF)

For a bootstrap particle filter (Chopin and Papaspiliopoulos, 2020, Chapter 10) targeting \(_{t}\), we need to (a) draw importance samples from \(p_{}(z_{t} z_{0:t-1})\), and (b) assign importance weights proportional to the potential function \(g_{t}\) to the samples. Both these operations require computing integrals with respect to \(p( z_{0:t-1})\), which is intractable in general. IO-SMC\({}^{2}\) provided a solution by nesting two particle filters: one to approximate \(_{t}\) and generating trajectories \(\{z_{0:t}^{n}\}_{1 n N}\) for \(t=0,,T\); and another inner particle filter to approximate \(p( z_{0:t}^{n})\) with an empirical distribution,

\[p( z_{0:t}^{n})( z_{0:t}^{n}) _{m=1}^{M}_{_{t}^{nm}}().\]

Here, \(_{}\) is the Dirac delta function centered at \(\) and \(\{_{t}^{nm}\}_{1 m M}\) is the set of particles associated with the trajectory \(z_{0:t}^{n}\). The marginal transition density \(p_{}\) and the potential \(g_{t}\) are then approximated by computing the necessary integrals with respect to this approximate filtering distribution \(( z_{0:t}^{n})\). This approximate distribution is obtained via a particle filter using a Markov kernel \(_{t}\) targeting \(p( z_{0:t}^{n})\). This means that the sequence of observations and designs has to be reprocessed from scratch, leading to a computational complexity of \((t)\) at every time step.

An alternative, which is used in the nested particle filter (NPF) of Crisan and Miguez (2018) to achieve a recursive algorithm, and which we borrow for the IO-NPF, is to jitter (Liu and West, 2001) the \(\)-particles using a Markov kernel \(_{M}\) that has \((1)\) cost. The jittered particles will no longer be samples from the true posterior, but by scaling the perturbation induced by the kernel as a function of the number of samples \(M\), we can control the error incurred through jittering (see Crisan and Miguez, 2018, Section 5.1).

The Inside-Out NPF algorithm, presented in Algorithm 1 (Appendix A), is a nested particle filter targeting \(_{t}\) with the same algorithmic structure as IO-SMC\({}^{2}\), but with the jittering kernel from the NPF used to rejuvenate the \(\)-particles. This seemingly small algorithmic change leads to two major consequences. First, it makes the algorithm fully recursive, with \((NMT)\) computational complexity against the \((NMT^{2})\) of IO-SMC\({}^{2}\). Second, it allows us to construct a backward sampling algorithm to generate less degenerate trajectories from \(_{t}\), which we describe in Section 3.1.

We adapt results from Crisan and Miguez (2018) to establish the following proposition, proved in Appendix B, for the consistency of the IO-NPF.

**Proposition 1** (Consistency).: _Let \(_{t}^{M}(z_{0:t})\) denote the marginal target distribution of Algorithm 1. Under technical conditions listed in Appendix B, for all bounded functions \(h\), we have_

\[_{M}_{_{t}^{M}}h(z_{0:t})=_{_{t}}h(z_{0:t}).\]

### Backward Sampling

The trajectories \(z_{0:T}^{n}\) generated by tracing particle genealogies are valid samples from \(_{T}\). However, for \(T 1\), very few unique paths for the first components of the trajectories \(z_{0:T}^{n}\) may remain, a problem due to resampling known as path degeneracy. The backward sampling algorithm (Godsill et al., 2004) is one way to circumvent the path degeneracy problem by simulating smoothing trajectories backward in time.

Before we explain the algorithm, we first note that at time \(t\), one "particle" of the IO-NPF is the set of random variables \(y_{t}^{n}z_{t}^{n},\{_{t}^{nm},B_{t}^{nm}\}_{1 m  M}}\), where \(B_{t}^{nm}\) is the ancestor index for \(_{t}^{nm}\). Given a partial backward trajectory \(_{t+1:T}\) which we have already simulated and partial forward trajectories \((y_{0:t}^{n})_{n=1}^{N}\), backward sampling selects \(_{t}\) from \((y_{t}^{n})_{n=1}^{N}\) with probability proportional to

\[W(y_{0:t}^{n},_{t+1:T})\,W_{z,t}^{n}^{M}(y_{0: t}^{n},_{t+1:T})}{_{t}^{M}(y_{0:t}^{n})}\,W_{z,t}^{n},t 0,\]where \(_{T}^{M}\) is the true target distribution of the IO-NPF (see Appendix A.1) and \(W_{z,t}^{n}\) are the filtering weights. This ratio can be computed by considering the transition densities of its constituent steps in Algorithm 1. In practice, implementing this naively would incur two drawbacks.

First, while correct, using \(W(y_{0:t}^{n},_{t+1:T})\)_as is_ would result in a degenerate algorithm: this is because the \(\)-particles \(\{_{t}^{nm},_{t+1}^{nm}\}\) are unlikely to be compatible pairwise, even when the populations \(\{(_{t}^{nm})_{m=1}^{M},(_{t+1}^{nm})\}_{m=1}^{M}\}\) are. In Appendix A.2 we show how this can be solved by marginalizing the weight over the ancestry of the \(\)-particles.

Second, the overall cost of backward sampling would be \((T^{2}N^{2}M^{2})\). This is prohibitive, and we prefer a cheaper'sparse' \((N)\) alternative consisting of sampling from a Markov kernel (again!) that keeps the distribution of the ancestors invariant (Bunch and Godsill, 2013; Dau and Chopin, 2023). This procedure is detailed in Appendix A.2.

## 4 Numerical Validation

To validate our method numerically, we consider the stochastic dynamics of a pendulum in conditionally linear-Gaussian form. The unknown parameters are a combination of its mass and length, while the measurement of the angular position and velocity constitutes the outcome of an experiment. The design is the torque applied to the pendulum at every experiment. We consider a series of \(50\) experiments, corresponding to \(50\) time steps of pendulum dynamics. The prior on the parameters is Gaussian.

Table 1 compares the _expected_ information gain and its surrogate, the _sequential prior contrastive estimate_ (sPCE, Foster et al., 2021), for a variety of amortized BED algorithms. The EIG estimate is computed by leveraging our nested sampling scheme to draw samples from the marginal \(p_{}(z_{0:T})\) and evaluate objective (1). The comparison includes policies trained using the IO-NPF both with and without backward sampling (BS), IO-SMC\({}^{2}\), implicit deep adaptive design (iDAD, Ivanova et al., 2021), and a random policy baseline. Furthermore, Figure 1 shows the _realized_ information gain achieved by policies trained using different algorithms over the number of experiments. In this particular conjugate setting, the \(\)-posteriors and the marginal transition densities can be computed in closed form, making the exact variant of IO-SMC\({}^{2}\) -- which substitutes the inner particle filter with exact posterior computation -- an ideal baseline. The results, both in Table 1 and Figure 1, highlight the benefit of backward sampling: IO-NPF _without_ backward sampling performs worse than IO-SMC\({}^{2}\), whereas IO-NPF _with_ backward sampling outperforms it.

Finally, Table 1 presents the runtime statistics of a single amortization iteration of the various inside-out particle filtering algorithms, emphasizing IO-NPF with backward sampling as the most

   Policy & EIG Estimate (1) & sPCE & Runtime [s] \\  Random & \(1.37 0.08\) & \(1.44 0.35\) & – \\ iDAD & \(2.58 0.17\) & \(2.53 0.35\) & – \\ IO-NPF & \(2.98 0.19\) & \(3.12 0.35\) & \(0.34 0.01\) \\ IO-SMC\({}^{2}\) & \(3.35 0.20\) & \(3.44 0.39\) & \(5.71 0.08\) \\ IO-NPF + BS & \(3.47 0.18\) & \(3.54 0.40\) & \(2.73 0.07\) \\ IO-SMC\({}^{2}\) Exact & \(3.54 0.20\) & \(3.64 0.38\) & \(1.36 0.04\) \\   

Table 1: EIG, sPCE and runtime for various BED schemes. Mean and standard deviation over \(25\) seeds.

Figure 1: Accumulation of the _realized_ information gain computed in closed form for different policies on the conditionally linear stochastic pendulum with a Gaussian parameter prior. We report the mean and standard deviation over \(1024\) realizations. Here IO–SMC\({}^{2}\) (Exact) — represents an ideal baseline where \(\)–posterior updates are closed-form, which is only possible in conjugate settings. Otherwise, our IO–NPF with backward sampling (BS) — outperforms all alternatives.

favorable option that offers the best balance between task performance, computational efficiency, and generality. For more details on this evaluation consult Appendix D. A public implementation in the Julia Programming Language is available at https://github.com/Sahel13/InsideOutNPF.jl.

## 5 Outlook and future work

We strongly believe that this new, fully recursive perspective on amortized Bayesian experimental design offers a promising, practical, extensible, and well-behaved approach both statistically and empirically. The main limitation of our approach (and of IO-SMC\({}^{2}\)) is the need to know the Markovian outcome-likelihood, which may not always be available. We hope to address this in future research. Additionally, we aim to derive stronger, non-asymptotic bounds for the error of the IO-NPF, similar to Miguez et al. (2013), in particular by incorporating recent results on backward sampling stability (Dau and Chopin, 2023), providing stronger guarantees on the learned policies.

## 6 Individual Contributions

AC and HA initially conceived the idea for this article, with AC and SI then developing the methodology. SI proved the consistency of the method, following AC's suggestions. The code implementation and experiments were carried out jointly by HA and SI. SI took the lead in writing the article, with valuable help from AC and HA. SPV and SS provided helpful discussions and feedback on the manuscript. All authors reviewed and validated the final version of the manuscript.