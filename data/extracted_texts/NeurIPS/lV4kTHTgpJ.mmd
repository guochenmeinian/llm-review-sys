# Model Fusion through Bayesian Optimization in Language Model Fine-Tuning

Chaeyun Jang

KAIST

jcy9911@kaist.ac.kr

&Hyungi Lee

KAIST

lhk2708@kaist.ac.kr

&Jungtaek Kim

University of Pittsburgh

jungateek.kim@pitt.edu

Co-first authors

Juho Lee

KAIST

juholee@kaist.ac.kr

###### Abstract

Fine-tuning pre-trained models for downstream tasks is a widely adopted technique known for its adaptability and reliability across various domains. Despite its conceptual simplicity, fine-tuning entails several troublesome engineering choices, such as selecting hyperparameters and determining checkpoints from an optimization trajectory. To tackle the difficulty of choosing the best model, one effective solution is _model fusion_, which combines multiple models in a parameter space. However, we observe a _large discrepancy between loss and metric landscapes_ during the fine-tuning of pre-trained language models. Building on this observation, we introduce a novel model fusion technique that optimizes both the desired metric and loss through _multi-objective Bayesian optimization_. In addition, to effectively select hyperparameters, we establish a two-stage procedure by integrating Bayesian optimization processes into our framework. Experiments across various downstream tasks show considerable performance improvements using our Bayesian optimization-guided method. Code will be available at: https://github.com/chaeyoon-jang/bomf.git.

## 1 Introduction

The field of Natural Language Processing (nlp) has significantly advanced with the pre-training of Transformer-based models on large amounts of texts without supervision. In general, these pre-trained networks are fine-tuned on supervised downstream tasks to solve particular tasks. The rise of Large Language Models (llms) such as gpt and llama has increased demands for huge memory and computing during fine-tuning on downstream tasks. In response, low rank approximation methods such as Low-Rank Adaptation (lora) and Quantized Low-Rank Adaptation (qlora) have been adopted recently to fine-tune the llm. However, the fine-tuning of Pretrained Language Models (plms) exhibits high sensitivity to marginal variations in hyperparameters such as learning rate and batch size, often leading to training failure and the performance drop of a fine-tuned model , while searching hyperparameters requires a vast amount of resources.

An effective strategy to seek an optimal model among multiple candidates is model ensembling, which yields impressive performance on generalization and robustness . However, traditional ensemble methods lead to several drawbacks including the space and computational costs that linearly scale with the number of models involved. These issues are particularly pertinent for llms, since individualmodels are costly to train and test. Therefore, we can alternatively utilize model fusion to aggregate multiple models into a single proficient model on a parameter space. One of its simplest forms, known as Stochastic Weight Averaging (swa) , involves taking the average of model parameters obtained during an optimization process. Despite its simplicity, swa and its variants have proven successful across various tasks, notably in computer vision [25; 42; 6; 46]. Recent advancement in this field is the concept of Model Soups, which has been introduced by Wortsman et al. . This approach weight-averages a set of models, obtained from multiple fine-tuning runs with different hyperparameters to create a powerful model that outperforms both individual and ensemble models.

The effectiveness of model fusion has predominantly been explored in the visual domain. For instance, while Model Soups have shown considerable improvements in image classification, they have not demonstrated superiority in the nlp tasks . The existing averaging methods like swa make use of their ability to encourage a fused model to locate on the center of the flatter area near local optima [25; 20], as loss landscapes are analogous to metric landscapes in computer vision tasks. Unfortunately, for plms, loss landscapes are substantially mismatched to metric landscapes, so that the flat loss minimum found by swa does not necessarily correspond to the flat metric minimum making a simple averaging method fail to find a superior model.

In this paper, we present a novel model fusion approach with an efficient hyperparameter selection strategy, denoted as Bayesian Optimization Model Fusion (bomf), specifically designed to fine-tune plms. To motivate our method, we start by illustrating two empirical analyses. Firstly, we demonstrate that the existing model fusion techniques are not suitable for plms. Secondly, we highlight that the optimal hyperparameters for plms exhibit consistency on varying the number of frozen layers or the rank used in the lora setting .

Based on these findings, we introduce our proposed method to build a single model, aggregated through the weighted combination of individual models. Supposing that evaluation metrics are non-differentiable, we employ Bayesian Optimization (bo) [5; 18], which is a black-box optimization technique, in developing our model fusion method. To the best of our knowledge, this is the first study that utilizes bo in the context of model fusion, in order to achieve the following objectives:

* **Utilization of Both Metrics and Loss Functions in Model Fusion.** Instead of running bo with an averaged target metric, we use Multi-Objective Bayesian Optimization (mobo) that considers both metrics and loss functions for model fusion. Despite low correlations between loss and metric values, we find that incorporating loss values still serves as useful guidance.
* **Two-Stage Model Fusion.** We devise our model fusion process as a two-stage bo procedure. One is for optimizing hyperparameters in fine-tuning and the other is dedicated to our model fusion method. The objective of the first stage is to maximize gains from the second stage to find hyperparameters leading to the optimal fused model after the bo of the second stage.

We demonstrate the effectiveness of bomf on several nlp tasks, including both Natural Language Understanding (nlu) and Natural Language Generation (nlg), with roberta, Text-to-Text Transfer Transformer (t5) and llama. Through these comprehensive experiments, we assess the performance of bomf in diverse nlp tasks and uncover the interesting properties of our approach through various ablation studies.

## 2 Preliminaries

Problem Setup.In this paper, we explore the process of fine-tuning plms using two types of datasets: a downstream training dataset \(_{}\) and a validation dataset \(_{}\). Assuming that we are given a _pre-trained_ set of weights \(_{}\) and a _trainable_ set of weights \(_{}\) for our plm denoted as \((,)\), \(_{}\) is either a subset of \(_{}\) or lora weights . Specifically, in the former case, \(_{}\) is deliberately selected from \(_{}\). As a special case, \(_{}\) will be identical to \(_{}\) if any layers or weights are not frozen. Meanwhile, if the lora method is employed during the fine-tuning of our model, \(_{}\) will be the lora weights.

We use \(K\) distinct metrics, denoted as \(f^{(k)}_{}(,)\) for \(k[K]\), to evaluate our model's performance on a given task. Each metric \(f^{(k)}_{}\) is typically non-differentiable, while a differentiable loss function \(f_{}\) is employed for training. Assuming that \(_{}\) is similar to the true data distribution, our goal is to find the optimal set of trainable weights \(^{}\) that minimizes the following:

\[^{}=*{arg\,min}_{}_{k=1}^ {K}^{(k)}_{}((_{}, ),_{}),\] (1)

where \(^{(k)}_{}\) is a normalized version of \(f^{(k)}_{}\) for all \(k[K]\), and \(\) is the space of trainable weights. However, due to the non-differentiability of the \(f^{(k)}_{}\) functions, conventional approaches resort to finding approximate solutions using gradient descent or its variants, as shown below:

\[}^{}=*{arg\,min}_{}f _{}((_{},), _{}).\] (2)

As will be discussed in the subsequent section, misalignment between loss and metric surfaces is more prominent in plms compared to computer vision models. To address this challenge, we propose a novel method to more adequately make use of \(\{f^{(k)}_{}\}_{k=1}^{K}\) and \(_{}\) by considering Equation 1.

Model Fusion.In the recent work , there has been a growing interest in the use of _weight averaging_ or _model fusion_ across diverse tasks. This line of research is an effective strategy to achieve superior performance in downstream tasks, all while managing computational costs by aggregation of multiple models. In this context, the aggregation of multiple models involves the identification of a set of \(N\) fine-tuned trainable weights, denoted as \(}=\{_{i}\}_{i=1}^{N}\). The objective is to derive a fused weight vector, \(}\), by utilizing \(\), such that \(}\) outperforms all other members in \(\). This can be expressed as \(L_{}(})*{arg\,min}_{ }L_{}()\), where \(L_{}()_{k=1}^{K}^{(k)}_{}((_{},), _{})\).

Model fusion approaches can be categorized into two main types: 1) uniform averaging and 2) weighted averaging. Uniform averaging methods, e.g., swa, Greedy Soups , involve the straightforward process of uniformly averaging weights within a subset \(}\) to obtain an improved performing weight vector \(}\), i.e. \(}=}|}_{}}\). Here, selecting a suitable subset \(}\) is an important strategy for each method. On the other hand, weighted averaging approaches, e.g., Learned Soups  and Rewarded Soups , aim to determine an optimized weight vector \(\) by forming a convex combination of parameters from \(\), expressed as \(}=_{i=1}^{N}_{i}_{i}\), where each averaging coefficient \(_{i}\) satisfies \(_{i} 0\), and \(_{i=1}^{N}_{i}=1\). While weighted averaging methods offer more flexibility compared to uniform averaging, they often require additional training to determine suitable values for the coefficient set \(\) through gradient descent updates based on the loss function \(f_{}\). However, in our proposed method, we suggest a weighted averaging technique that considers not only the loss function \(f_{}\) but also the metrics \(\{f^{(k)}_{}\}_{k=1}^{K}\).

Multi-Objective Bayesian Optimization.bo is a sample-efficient black-box optimization technique with probabilistic regression. Since we assume that an objective to optimize is unknown, a surrogate function, which is generally a probabilistic regression model, is estimated instead. The key desired properties of the surrogate function are attained by considering how a search space is exploited and explored through its outputs. Utilizing the surrogate function, bo eventually optimizes a specific form of optimizable function, called an acquisition function; see  for details.

On top of generic bo, mobo is used to solve an optimization problem, involved with \(K\) different competing objectives:

\[^{}=*{arg\,min}_{}(f_{1}(),f_{2}(),,f_{K}()).\] (3)

Supposing that we cannot directly access \(f_{1},f_{2},,f_{K}\), probabilistic surrogate models, which are alternatives to unknown objectives, should be used to determine a next point to evaluate. To find a solution candidate of Equation 3 using mobo, we can consider scalarization of either the realizations of surrogate models or acquisition functions corresponding to multiple objectives . In contrast to the scalarization method, the maximization of Expected HyperVolume Improvement (ehui), on a metric space  can be used:

\[^{}=*{arg\,max}_{}(;,),\] (4)

where a hypervolume is defined as the size of space between the Pareto frontier of \(n\) historical evaluations \(^{n K}\) and a reference point \(\). While the scalarization determines query pointsby aggregating \(K\) outputs with particular (potentially random) coefficients, the hypervolume improvement maximization chooses query points that widen the expected hypervolume, which is more robust to function scales without the sampling distributions of scalarization coefficients. As reported in the previous work [10; 3], compared to other existing mobo algorithms, \(q\)NEHVI which is a variant of the EHVI method that evaluates a batch of \(q\) points in a parallel manner. Building on the powerful mobo algorithm, our model fusion framework is capable of determining averaging coefficients efficiently reducing the number of evaluations required to find better fused plms.

## 3 Empirical Findings

In this section, we present empirical observations motivating our model fusion strategy. In SS 3.1, we initially illustrate distinct findings: unlike in computer vision tasks, in nlp tasks, there exists a significant misalignment between the loss and metric surfaces. This misalignment poses a challenge for straightforward model fusion methods when fine-tuning plms. In SS 3.2, we find that the optimal fine-tuned hyperparameters for plms analogously align across different architectural configurations varying the number of frozen layers or variations in rank in the lora setting.

### On Misalignment in Loss and Metric Landscapes

The well-known success of uniform averaging, e.g., swa and Model Soups, in image classification tasks, has been grounded on the flatness of a loss landscape. As one can see in Figure 0(a), the use of uniform averaging successfully explores minima on the flatter region of the loss landscape using individual weights close to the flatter region, resulting in enhanced generalization loss on a test dataset. This generalization effect is similarly observed in the case of the metric landscape, as illustrated in Figure 0(b), owing to the similarity between the loss and metric landscapes. This similarity is the consequence of the inherent similarity between the loss function and the metric . However, the

Figure 1: Visualization of the loss landscape over parameters (Figures 0(a) and 0(c)) and the metric landscape over parameters (Figures 0(b) and 0(d)) for both the vision task (Figures 0(a) and 0(b)) and the nlp task (Figures 0(c) and 0(d)). The metric is \(1-\)accuracy and F1 score for the vision task and the nlp task, respectively. In the vision task, we fine-tune the ResNet-50 model  pre-trained with ImageNet-21k  on the Caltech-101 dataset , while in the nlp task, fine-tuning was performed on the pre-trained roberta model on the mrpc dataset. The members of the swa for each figure are denoted as \(w_{1},w_{2},w_{3}\).

domain of language modeling, characterized by semantic, morphosyntactic, and pragmatic intricacies, requires the evaluation of generalization performance across a diverse array of tasks and metrics . It is unlikely to precisely align these metrics with a training loss function , leading to a misalignment that often results in more complex and less flat surfaces in language tasks compared to the loss function visually demonstrated in Figures 0(c) and 0(d).

In Figures 0(c) and 0(d), we find that while uniform averaging can reach high generalization performance based on the loss function, it poorly performs concerning the metric function compared to the best-performing weight in \(\). However, Figure 0(d) shows that even though the uniform averaging of three weight points degrades the metric performance, better points in terms of higher metric values exist in the convex set of the three weight points. The empirical results we observe above, which are caused by the complex and misaligned surface, motivate the need to utilize weighted averaging methods and seek the optimal combination of averaging weights based on the metric. This does not agree with the previous findings in vision tasks  and Figure 0(b) which argue minimal performance difference between the weighted averaging and the uniform averaging. Refer to Appendix C.1 for numerical results that show the discrepancy between the loss and metric landscapes in plms.

### On Hyperparameter Alignment

Discovering the optimal training hyperparameters incurs significant computational costs, particularly when fine-tuning extensive foundational models . This challenge arises since the ideal set of hyperparameters tends to vary in tandem with changes in both tasks and model structures.

Surprisingly, our empirical findings reveal a consistent alignment of optimal hyperparameters when fine-tuning plms, regardless of variations in the number of frozen layers or the rank of lora. As illustrated in Figure 2, the alterations in validation loss and metric resulting from changes in the learning rate or batch size exhibit a similar pattern across different numbers of frozen layers, except in the case when all pre-trained layers are frozen and only the classifier layer is trained. This proves that we can decrease computational cost for searching the optimal hyperparameters by tuning on smaller models with more frozen layers or lora with smaller ranks. Refer to Appendix C.2 to see the additional results when varying the adam beta, learning rate schedule, as well as the case of the lora.

Figure 2: Validation results on the MRPC dataset for roberta: loss (shown in left panels) and F1 score (in right panels) for varying learning rates, batch sizes, and frozen layers. Optimal hyperparameters align well across different frozen layers, except when all pre-trained layers are frozen.

Yang et al.  demonstrate that employing a particular model weight initialization method and learning rate scheduling method, referred to as \(\)-parametrization, enables the transferability of certain training hyperparameters (such as learning rate and momentum) varying the width of the model. However, it is important to note that these results specifically pertain to scenarios where models are trained from scratch. This distinction is noteworthy as our context involves the fine-tuning of plms. It would be a great future research direction to theoretically analyze this phenomenon.

## 4 Bayesian Optimization Model Fusion

In this section, bomf unfolds in three key steps. In SS 4.1, we present the process of constructing a set of fine-tuned trainable weights \(\), serving as components for model fusion. In SS 4.2, we introduce a method to identify optimal hyperparameters crucial in the construction of the set \(\) based on the findings explained in SS 3.2. Finally, we delve into how we conduct weighted averaging in SS 4.3, following the insights discussed in SS 3.1.

### Fusion Member Sampling

To improve the performance of our model through model fusion, it is crucial to carefully create the set \(\) by employing an appropriate weight sampling method. There are two main types of weight sampling methods: 1) sampling from multiple training trajectories  and 2) sampling from a single training trajectory with proper learning rate scheduling . However, Wortsman et al.  indicate that, when applying model fusion with samples from multiple training trajectories, the performance improvement becomes less significant during the fine-tuning of plms compared to vision tasks. This limitation in nlp tasks is attributed to the misalignment in loss and metric surfaces, as discussed in SS 3.1. Furthermore, when employing multiple training trajectories to sample fusion members, the training computation cost increases linearly in proportion to the number of fusion members. This poses a significant challenge, particularly in the context of fine-tuning plms. For these reasons, in our approach, we collect our fusion members from a single training trajectory. Since the fine-tuning process of plms involves a small number of training epochs and exhibits rapid convergence , we start gathering fusion members after 50% of the training epochs are completed. This timing is slightly quicker than the point described in the work , which begins collecting after 75% of the training epochs are concluded. Once we start collecting the fusion members, we proceed to uniformly sample 15 members throughout the remaining training epochs. Refer to Appendix A for more details on the process of collecting fusion members.

### Hyperparameter Search via Bayesian Optimization

In the construction of a set of fusion members \(\) from a single training trajectory, the effectiveness of the training trajectory significantly impacts the ultimate metric performance of the fused model weight \(}\). In this context, the effectiveness of a training trajectory refers to the model's metric performance using the best-performing weight within that trajectory on the validation dataset \(_{}\). The correlation in Figure 3 strongly indicates that the performance of the best-performing weight is positively correlated with the performance of the fused weight. Consequently, to achieve the best performance of the fused weight, it becomes crucial to identify the set of optimal hyperparameters \(\) that results in the most effective training trajectory. However, two primary challenges arise when searching for the optimal hyperparameters \(^{}\) that yield the best metric performance: 1) the metric functions \(\{f^{(k)}_{}\}_{k=1}^{}\) are non-differentiable and 2) we need to efficiently assign computational resources in finding better hyperparameters beyond naive methods such as grid search. To remedy these two issues simultaneously, in bomf, we employ bo to find the optimal set

Figure 3: Correlation between the performance of best-performing weights in a training trajectory and the performance of the fused model. We fine-tune the roberta model on the RTE dataset. Each point is obtained from the evaluation of a single trajectory with varying hyperparameters.

of hyperparameters:

\[^{*}=_{}_{k=1}^{K}_{}^{(k)}( (_{},()),_{ }),\] (5)

where \(()\) represents the best-performing weight within the training trajectory associated with the hyperparameter set \(\). Here, we utilize Gaussian process (gp) regression  and Log-Expected Improvement  as a surrogate function and an acquisition function, respectively. We employ three randomly initialized sets of hyperparameters as the starting point for bo, conducting 10 iterations of computations to determine the optimal set \(^{*}\).

The sequential nature of bo computations can lead to a substantial computational load, particularly in the context of fine-tuning plms. To address this issue and propose a more computationally efficient bo approach, we draw insights from the observations discussed in SS 3.2. The alignment of the best hyperparameters for fine-tuning between the full model and lightweight models (e.g., frozen layers model or reduced rank lora) allows us to utilize the lightweight model instead of the full model when seeking the optimal set \(^{*}\) as follows:

\[^{*}=_{}_{k=1}^{K}_{}^{(k)} ((_{},}()), _{}),\] (6)

where \(}\) is the trainable weight of the lightweight model. Refer to SS 6 to see how our computationally efficient method decreases computation time while maintaining performance.

### Multi-Objective Bayesian Optimization for Model Fusion

After completing the construction of the set \(\) with \(N\) individual models, the next stage involves selecting appropriate averaging coefficients \(^{N}\) to ensure the enhanced metric performance of a fused model. To achieve this, we can leverage metrics \(\{f_{}^{(k)}\}_{k=1}^{K}\) and apply a bo procedure to obtain optimal averaging coefficients \(^{*}\), similar to the optimization process for the hyperparameter set \(\). However, restricting the consideration to metric performance solely on \(_{}\) may result in our fused weights \(}\) overfitting to \(_{}\) and exhibiting poor generalization to the true data distribution, due to the complex and sharp nature of the metric landscape which is observed in SS 3.1. To tackle this challenge, when optimizing \(\), we propose to minimize both \(f_{}\) and \(\{f_{}^{(k)}\}_{k=1}^{K}\) by employing mobo identify a Pareto frontier defined as follows:

\[=^{*}^{*}=_{} l(),l_{1}(),,l_{K}() },\] (7)

where \(l()_{}((_{},}()),_{})\) and \(l_{k}()_{}^{(k)}((_{},}()),_{})\) for \(k[K]\). Note that \(}()\) denotes a fused set of weights with an averaging coefficient vector \(\), i.e., \(}()=_{i=1}^{N}_{i}_{i}\) where \(_{i}\) for \(i[N]\) and \(N\) is the number of models to fuse.

Here, we utilize the ehvi strategy, which is described in the work by Emmerich et al. . The hypervolume, in this context, is defined as a volume size between \(\) and a reference point \(\). We set the reference points as a zero vector. To enhance the optimization of the hypervolume improvement objective, we employ the logarithmic form of \(q\)NEHVI algorithm [10; 3], which is implemented with the BoTorch framework . As highlighted in SS 2, this algorithm has proven effective in practical multi-objective optimization scenarios. This makes it well-suited to handle the complex and sharp nature of our metric landscape, enabling it to successfully identify the optimal \(^{*}\). We run mobo for a total of \(5||=75\) iterations to find the optimal coefficients \(^{*}\).

In our case, additional constraints are in place for executing mobo, specifically 1) equality constraints and 2) inequality constraints for \(\). To address the inequality constraints (i.e., \(_{i} 0\)), we follow the work by Gardner et al.  to incorporate constraints into the acquisition function. To deal with the equality constraints \(_{i=1}^{N}_{i}=1\), we simply normalize the output of the acquisition function. Refer to Algorithm 1 in Appendix B for the summary of bomf.

## 5 Related Work

Model Fusion for Pre-Trained Language Models.Due to the increasing number of model parameters in recent plms, there has been a significant increase in both memory requirements and computational costs [73; 8; 63]. Consequently, there is growing attention on a research direction aimed at enhancing the performance of plms while simultaneously managing computational costs and memory requirements through the exploration of model fusion methods [54; 71; 9]. However, most of these studies have focused on fusing the models fine-tuned on different tasks, aiming to develop a single multi-task learner. In the context of a single-task fine-tuning scenario with plm, it has been observed that the previous simple weight-averaging approaches often yield marginal improvements [70; 27]; nevertheless, the exploration into the underlying rationale of this consequence remains limited. As mentioned in SS 3, we find that uniform weight averaging does not always align generalization on the loss surface with the optimal point on the metric surface, primarily due to the discrepancy between loss and metric landscapes. To address this issue, we develope a single-task model fusion method based on mobo, finding the optimal weight combination coefficients by considering both metrics and loss functions.

Bayesian Optimization.bo[5; 18] is a promising strategy to optimize a black-box function. In particular, if a target objective is costly in terms of function evaluations, Specifically, bo sequentially seeks solution candidates by modeling a surrogate function and maximizing an acquisition function. In the bo community, a gp is often employed as a surrogate function but diverse regression models such as Bayesian neural networks [61; 38] and tree-based models [23; 30] can be used. As a choice of acquisition function, expected improvement  and gp upper confidence bound  are often considered. Importantly, bo is more effective than other existing optimization strategies such as grid search and genetic algorithms . Its efficacy has been demonstrated in a wide variety of applications such as hyperparameter optimization , nanostructured device design , and chemical reaction optimization . Moreover, in the deep learning context, the necessity for efficient hyperparameter tuning via bo has risen following the increasing number of hyperparameters and parameters in models . Consequently, bo is applied for hyperparameter optimization in various deep learning tasks, such as image classification [28; 34] and nlp tasks [44; 7].

## 6 Experiments

In this section, we present empirical results demonstrating the effectiveness of bomf in various nlp tasks. We compare our method to five basic algorithms aimed at finding a high-performing solution. **Grid Fine-Tune** is a simple fine-tuning method that selects the best-performing checkpoint using grid search. **HPBO** utilizes optimal hyperparameters obtained by SS 4.2 for fine-tuning the baselines. **SWA** is an optimization technique that averages model parameters obtained during training. **Greedy SWA** is a modified version of swa inspired by Greedy Soups , sorting weights based on metric performance on \(_{}\) and including them in \(}\) only if they improve \(}\)'s performance. **Learned SWA**, inspired by Learned Soup , learns the coefficients \(\) based on the loss after fine-tuning. For medium-sized language models, we tested a variant of Transformer OTfusion , aligning pre-trained weights before averaging. Additionally, we experimented with **TWA**, a recent swa variant that reconstructs \(\) by finding weight space basis vectors and learns \(\) based on the loss.

    & &  &  \\  Method & RTE (Acc) & MRPC (F1) & SST-2 (Acc) & QNLI (Acc) & QQP (F1) & MNLI (Acc) & SQUAD2.0 (F1/EM) \\  Grid Fine-Tune & 77.78 & 92.39 & 94.87 & 92.62 & 88.16 & 87.41 & 78.18/72.83 \\  HPBO (Full) & 78.57 & 92.78 & 95.11 & 93.01 & 88.58 & 87.46 & 78.28/73.29 \\ SWA & 78.62 & 92.24 & 95.42 & 92.81 & 88.49 & 87.41 & 80.31/74.85 \\  Greedy SWA & 77.08 & 92.82 & 94.27 & 92.22 & 88.34 & 87.43 & 80.75/74.99 \\ Greedy SWA & 80.70 & 92.83 & 95.54 & 93.16 & 88.64 & 87.45 & 80.63/75.44 \\ Learned SWA & 81.40 & 92.81 & 95.31 & 92.94 & 88.38 & 87.41 & 80.65/74.23 \\ TWA & 81.23 & 91.58 & 95.54 & 93.00 & 87.85 & 87.42 & 80.29/74.79 \\  BOME\({}^{}\) (ours) & **81.75** & 93.37 & **95.65** & **94.83** & 88.66 & 87.51 & 80.82/75.79 \\ BOMF (ours) & 81.40 & **93.90** & 95.54 & 93.50 & **88.68** & **87.86** & **81.82/76.21** \\   

Table 1: **Results on Medium-Sized Language Models.** We conduct the text classification task using RoBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using t5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.

In all tables, the best performance is indicated with **boldfaced underline**, while the second-best value is represented with underline in each column. The final column 'Avg.' provides a summary of overall results for each method across various datasets or metrics. The terms 'Full' and 'Freeze' in Table 1 specify the exploration of optimal hyperparameters using either the entire model or a model with half of its weights frozen, as discussed in SS 4.2. Similarly, the terms 'Rank 64' and 'Rank 4' in Table 2(a) denote that we use the Rank 64 or the lightweight Rank 4 version of the lora model for the hyperparameter search, respectively. See Appendix A for the details of downstream datasets and hyperparameter selection.

### Empirical Analysis on Medium-Sized Language Models

We begin by evaluating the effectiveness of bomf on medium-sized language models using RoBERTa-base  and t5-base . For RoBERTa-base, we performed text classification tasks using the GLUE benchmark datasets . For t5-base, we carried out the question-answering task with the SQuAD2.0  dataset. For both models, we fine-tuned the weights directly on the downstream datasets.

Table 1 shows that bomf consistently outperforms other baselines across all model structure and datasets.3 Notably, the performance of HPBO, which uses hyperparameters obtained from SS 4.2 with the full model, surpasses Grid Fine-Tune for most datasets. These results demonstrate that our bo-based hyperparameter search framework effectively discovers optimal hyperparameters compared to grid search. Refer to Appendix C.4 for the performance of freeze HPBO, which uses a lightweight model for hyperparameter optimization. Freeze HPBO also clearly outperforms Grid Fine-Tune which proves the effectiveness of our bo-based hyperparameter search. Also, it is evident that model fusion methods, except bomf, lead to performance declines compared to HPBO, as discussed in SS 3.1, in certain datasets. On the contrary, bomf consistently betters the performance compared to HPBO, yielding that our method with mobo effectively finds optimal \(^{}\) even in complex and sharp metric landscapes. Refer to Table 16 for the complete results.

### Empirical Analysis on Large Language Models

We further validated the effectiveness of our proposed method by fine-tuning larger models using lora. Specifically, we experimented with llama2-7b and llama3-8b on tasks such as summarization using the SAMsum  dataset, Korean multi-choice medical question answering using the KorMedMCQA  dataset, and dialogue generation using the E2E  dataset. In the summarization task, while Learned SWA exhibited the best performance in terms of Rouge-2, bomf surpassed Learned SWA in average performance across all metrics, as illustrated in Table 2(a). Notably, for Rouge-L, only bomf improved over HPBO, highlighting the effectiveness of the multi-objective approach in bomf. Furthermore, as shown in Table 2(b), our model not only outperforms other baselines but also demonstrates that fine-tuning remains essential for specific tasks despite the rise of

Table 2: **Results on Large Language Models.** We compare the results of bomf and baseline methods using the SAMSum and KorMedMCQA datasets for summarization and medical multiple choice question-answering tasks with llama2-7b and llama3-8b. R1, R2, and RL denote Rouge-1, Rouge-2, and Rouge-L scores for summarization. Doctor, Nurse, and Pharm denote evaluation results for medical question answering in each respective field, using accuracy as the metric.

in-context learning (ICL) . This highlights the necessity of bomf, which efficiently identifies hyperparameters and provides an effective fine-tuning solution through model fusion. The results for E2E can be found in Appendix C.4.

### Ablation Study

Number of Frozen Layers.To analyze the efficiency of memory and compute when using a lightweight model in the bo procedure to find \(^{}\), we conduct a study using RoBERTa-base on the RTE and MRPC datasets. As presented in Table 3, the use of a lightweight model successfully identifies favorable hyperparameters that yield good performance while reducing the number of parameters by up to 25% and the computation time by up to 66%. This efficiency is achieved by caching outputs from the frozen layers. By systematically freezing layers from the tail of the model, we can cache the outputs from these frozen layers and reuse them during the training process.

Multiple Objectives.To validate the efficacy of using multiple objectives when determining optimal \(\), we compare bomf with single-objective baselines using t5-base on the SQuAD2.0 dataset. In this task, we consider two metrics: F1 score and Exact Match. Table 4 shows that relying on only one specific metric slightly increases the objective metric but results in a significant performance drop for the other metric. This outcome suggests that using single-objective bo is appropriate when aiming to find a model optimized for a specific metric, while the use of mobo is more suitable for discovering an optimal fused model that achieves high performance across various metrics. Refer to Appendix C.3 for further ablation studies.

## 7 Conclusion

In this paper, we empirically remarked two intriguing findings on loss and metric landscapes and hyperparameter alignment. Then, motivated by the observations mentioned above, we proposed a novel bo-based bomf algorithm for model fusion. Our method utilizes the bo and mobo frameworks to seek optimal fine-tuning hyperparameters and averaging coefficients, respectively. We validated that our proposed method exhibits improved performance on both nlu and nlg tasks on middle- and large-scale plms.

Limitations and Future Work.As discussed in SS 3.2, compelling future research involves the theoretical analysis of the hyperparameter alignment phenomenon. Moreover, we empirically observed that when utilizing quantization-based low-rank approximation methods [11; 37], traditional uniform averaging methods and weighted averaging methods face challenges in effectively aggregating models. These challenges arise from the quantized weight values in the models that behave differently with averaging weights. Another research direction is the development of averaging methods for the quantization-based low-rank approximation methods.