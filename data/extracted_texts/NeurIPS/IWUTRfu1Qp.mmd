# ## 1 Introduction

## 1 Introduction

Numerous large language models (LLMs), both closed-source and open-source (OpenAI, 2023; Touvron et al., 2023), are now available to the community. Evaluating their alignment with human preferences is crucial for selecting appropriate models in downstream applications (Ouyang et al., 2022). To meet this need, Chatbot Arena (Chiang et al., 2024) provides an open platform for evaluating LLMs based on human preferences. However, it typically takes weeks or even months for a newly released LLM to collect statistically enough human votes.

To reduce reliance on human annotations, automatic LLM benchmarks such as _AlpacaEval 2.0_(Dubois et al., 2024), _Arena-Hard-Auto_(Li et al., 2024b), and _MT-Bench_(Zheng et al., 2023) use LLM-based auto-annotators to evaluate language models. These automatic benchmarks are cheap, scalable, and have high Spearman correlations with Chatbot Arena (Li et al., 2023c). These advantages make them popular choices for providing timely assessments of newly released LLMs (Meng et al., 2024; Chen et al., 2024a), where high win rates can lead to significant _promotional benefits_.

While automatic benchmarks offer a valuable way for comparing LLMs, recent studies have revealed that auto-annotated win rates can be affected by biases related to output length and style (Dubois et al., 2024; Chen et al., 2024b; Zhang et al., 2024). In most cases, these biases are unintentional, stemming from the training data distribution; however, they can still game win rates, causing leaderboard results to deviate from actual human preferences. To mitigate this issue, several strategies have been introduced to control for output length and disentangle style from content, thereby reducing the potential for gameability (Dubois et al., 2024; Li et al., 2024a).

But, what if an adversary _intentionally_ cheats auto-annotators to achieve high win rates and capitalize on the resulting promotional benefits? In this study, we conduct stress tests on these benchmarks by submitting **"null models"** that, instead of responding to input instructions, generate **constant** outputs. Our initial experiments use ChatGPT to craft dozens of _persuasive_ responses (Zeng et al., 2024) expecting auto-annotators to favor them and gain high win rates. Note that persuasive responses do not respond to input instructions, so human annotators will assign them zero win rates.

We submit these persuasive responses to AlpacaEval 2.0 after wrapping them as null models. For instance, a null model NullModel("Pick me:") always returns the same output "Pick me!" for all the \(805\) input instructions in AlpacaEval 2.0, without providing any informative response. As seen in Figure 1(b), the AlpacaEval 2.0 auto-annotator (GPT-4-1106-preview) is robust to these persuasive responses, assigning win rates of less than \(1\%\).

Nevertheless, we find that **structured cheating responses** can cheat the auto-annotator by exploiting a weakness in LLMs, which may become confused during syntactic analysis when processing the evaluation templates, such as those used in AlpacaEval 2.0. A manually crafted cheating response that is structured can already achieve a \(76.8\%\) LC win rate, as seen in Figure 1(c).

We further modify this structured response by adding a prefix and optimizing it through random search based on querying results from GPT-4 (Andriushchenko et al., 2024; Zheng et al., 2024). To simulate more challenging scenarios, we assume that all input instructions of the automatic benchmarks are _private_. Thus, we craft a **transferable** prefix using a public set of instructions from UltraFeedback (Cui et al., 2023). We then evaluate this optimized prefix, concatenated with the structured cheating responses, by testing it on AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench as reported in Table 2. Additionally, we use open-source LLMs like Llama-3-Instruct (Meta, 2024; Touvron et al., 2023) as auto-annotators and conduct further ablation studies to verify our findings.

Anti-cheating has long been a critical consideration when designing the rules for leaderboards (Blum and Hardt, 2015), but this remains unexplored in the context of LLM benchmarks. While our experiments in this paper are primarily proof-of-concept, a determined adversary could leverage LLMs to generate more subtle and imperceptible cheating responses (Liu et al., 2023; Chao et al., 2023), unethically gaining high win rates and promotional advantages. Our findings highlight the urgent need to develop robust anti-cheating mechanisms to ensure reliable automatic LLM benchmarks.

## 2 Preliminaries

**LLM-based auto-annotators.** We focus on the problem of evaluating outputs from LLMs using auto-annotators. Formally, we define a model \(:^{*}^{*}\) as a function that transforms an input sequence of tokens into an output sequence of tokens, where \(\) is the vocabulary. Given an instruction \(I^{*}\), the LLM generates a response \((I)^{*}\). To evaluate these responses, we introduce an auto-annotator function \(:^{*}()\), where \(\) represents the evaluation output space, and \(()\) denotes the space of probability distributions over \(\). For instance, in _MT-Bench_, there is \(=\{1,2,...,10\}\), representing a score range; while in _AlpacaEval 2.0_, there is \(=\{,\}\), indicating binary judgments. The auto-annotator assesses the instruction \(I\), the response from the target model \(_{}(I)\), and optionally, the response from a reference model \(_{}(I)\). The output of the auto-annotator is either \((I||_{}(I))\), evaluating the target model alone, or \((I||_{}(I)||_{}(I))\), comparing the target and reference models to compute win rates.

**Threat model of cheating.** The cheater is assumed to have no direct access to the auto-annotator's parameters but can query the auto-annotator through an API provided by a service provider. Additionally, the cheater has no access to the test input instructions. The cheater's goal is to craft a _null model_ and manipulate the auto-annotator's evaluation to favor the **constant, non-informative** response outputs from the null model, rather than preferring the responses from the reference model.

**Experimental setup.** Our experiments utilize the official evaluation templates associated with different LLM-based evaluations unless stated otherwise. We evaluate our cheating method on AlpacaEval 2.0 (Li et al., 2023; Dubois et al., 2024), Arena-Hard-Auto (Li et al., 2024), and MT-Bench (Zheng et al., 2023) as detailed in Table 1. These benchmarks assess the models' ability to handle a wide range of conversational tasks across diverse query sets and have gained widespread adoption within the research community. We adhere to each benchmark's evaluation criteria when reporting our results. For AlpacaEval 2.0, we present both the raw win rate and the length-controlled (LC) win rate, with the LC one designed to mitigate bias from model verbosity. For Arena-Hard-Auto, we report the win rate against a reference model. Additionally, we provide the first-turn score for MT-Bench, using GPT-4-Preview-1106 as the auto-annotator model. The targeted auto-annotators include both open-source and closed-source LLMs: Llama-3-8B-Instruct,Llama-3-70B-Instruct(Meta, 2024; Touvron et al., 2023), and GPT-4-1106-Preview(OpenAI, 2023). Each LLM uses its default generation configuration with a temperature setting of \(0.0\). For Llama-3 auto-annotators, we use 4-bit quantized versions to reduce GPU memory usage.1 All experiments were conducted on \(8\) NVIDIA A100 (40G) GPUs within a few hours using vLLM as the inference engine, and the tokenization template was sourced from Hugging Face tokenizers.

## 3 Cheating strategies

Our initial experiments in Figure 2 indicate that using only an optimized adversarial suffix (without informative responses to input instructions) is ineffective on AlpacaEval 2.0 when GPT-4 acts as the auto-annotator. To address this limitation, our cheating strategies include: (1) constructing structured cheating responses to confuse widely used LLM auto-annotators, and (2) conducting token-level random search to craft the adversarial prefix, as outlined below:

**Structured cheating responses**. As shown in Figure 1, our cheating strategy involves replacing the original comparison with a misleading one, which disrupts the auto-annotator's syntactic analysis of the evaluation template and steers its judgment away from the intended outcomes. The response is carefully structured to be _resilient against swap operations_. For instance, on AlpacaEval 2.0, when

Figure 1: **Auto-annotator’s template of AlpacaEval 2.0**, which is fed into GPT-4-Preview-1106 to implement JUDGE. The placeholders {instruction} is filled in by each of the \(805\) input instructions \(I\), while in the _default_ setting, {output.1} is the reference model’s response \(_{}(I)\) and {output.2} is the target model’s response \(_{}(I)\). The _swap_ setting changes the order of outputs. In our experiments, the target model is instantiated by null models as NullModel(const_str), where const_str is either a **persuasive response** (**baseline**) or a **structed cheating response** (**ours**).

the submitted response is positioned last, the annotator predicts "M". Conversely, when it appears in the first position, the annotator predicts "m". The optimized response exhibits the following key properties: (1) It overrides the original instruction-output triplet with a fabricated one; (2) When positioned by default, it exploits the annotator's general preference for the last output, guiding it to predict "M"; (3) When swapped, it takes advantage of overwriting the output from model "M", causing the annotator to predict "m". The full template and final submission files are presented in Figures 7, 8 and 9. This structured response alone achieves a \(76.8\%\)_LC win rate on AlpacaEval 2.0_. Moreover, the response can be concatenated with an adversarial prefix to enhance the cheating effectiveness.

**Crafting adversarial prefix by random search (RS)**. To further improve the structured response, we incorporate an adversarial prefix and optimize it using an RS strategy based on GPT-4 query results. To emulate a more challenging scenario, we assume that the input instructions from the automatic benchmarks remain private. Therefore, we develop a transferable prefix, crafted using a publicly available instruction set. Our approach optimizes a single adversarial prefix by aggregating the losses over various instructions, ensuring that the prefix's impact is universal across different input instructions and positions. We utilize an RS algorithm to optimize the adversarial prefix (Zou et al., 2023; Andriushchenko et al., 2024; Zheng et al., 2024). The algorithm refines the prefix by sampling modifications and selecting the variant that minimizes the aggregated loss across multiple instructions. This process is detailed in Algorithm 1.

## 4 Cheating GPT-4 based automatic LLM benchmarks

GPT-4 models are the most widely used state-of-the-art auto-annotators, valued for their powerful evaluation capabilities. To assess the generality of our cheat, we applied it to a range of automatic LLM benchmarks, using the GPT-4-1106-Preview model as the auto-annotator. For RS, we set the number of training instructions \(N\) as \(10\), \(8\), and \(4\), the number of optimization steps \(T\) as \(384\), \(96\) and \(64\) for AlpacaEval 2.0, Arena-Hard-Auto and MT-Bench, respectively. The full templates and structured responses for Arena-Hard-Auto and MT-Bench are presented in Figures 10 and 11.

**The effectiveness of our structured response.** As mentioned in Section 3, we employ a structured response to facilitate the cheating, which provides a good initial point and could reduce the optimization cost. To further demonstrate the effectiveness of our structured cheating response, we evaluate \( p(=)\) on a sampled subset of the AlpacaEval 2.0 test instructions using different null responses. We compare our structured response with the other 16 persuasive responses, as shown in Figure 3. The results highlight the superiority of our structured response (marked as "Ours") because it achieves the lowest log probabilities. This demonstrates the effectiveness of our structured response in cheating the auto-annotator to favor our null model. Additionally, Figure 3 shows that the default configuration, where the baseline is placed second and the target model the last, tends to have lower losses, suggesting a preference for the second-position response. This highlights the position bias of the GPT-4-based auto-annotator, which often favors the last response.

**Empirical results.** The results of our experiments, summarized in Table 2, underscore the effectiveness of our method across various benchmarks. On AlpacaEval 2.0, our structured responses achieved a LC win rate of \(76.8\%\) and a raw win rate of \(59.5\%\). After integrating RS optimization, the LC win rate increased to \(86.5\%\), and the raw win rate improved to \(76.9\%\). These results represent significant improvements compared to the verified SOTA model, which achieves only \(57.5\%\)LC and \(51.3\%\) raw win rates. Our structured approach with random search outperforms the verified SOTA \(29.0\) percentage points in LC win rate and \(25.6\) in raw win rate. Compared to the community SOTA, our method achieves better performance in LC (\(86.5\%\) vs. \(78.5\%\)) and is comparable in raw win rates (\(76.9\%\) vs. \(77.6\%\)). Additionally, the LC win rates of our cheats are generally higher than the raw win rates because of their short length, which highlights that AlpacEval 2.0 is also not robust to length cheat. On the Arena-Hard-Auto, our structured approach achieves a win rate of \(67.2\%\), which increases to \(83.0\%\) after the random search. This is particularly notable because our final win rate matches the performance of the verified SOTA model, which stands at \(82.6\%\). For the MT-Bench, our structured responses initially achieve an average score of \(7.75\), which increases to \(9.55\) with random search optimization. This brings the score greatly outperforming the verified SOTA score of \(8.96\). In summary, our method achieves substantial gains over the state-of-the-art approaches, demonstrating its effectiveness across various benchmarks, and reinforcing the need for more robust automatic LLM benchmarks.

## 5 Ablation studies on open-source auto-annotators

To better understand the mechanism behind our method, we conduct extensive ablation studies on auto-annotators based on open-source LLMs. We focus on open-source LLma-3-instruct (8B, 70B

    & ^{}\)**} & ^{}\)**} & **MT-Bench\({}^{}\)** \\   & LC & Win Rate & Discrete & Win Rate & 95\% CI & avg \#tokens & Score \\  Verified SOTA & 57.5 & 51.3 & 53.8 & 82.6 & (-1.9, +2.0) & 662 & 8.96 \\ Community SOTA & 78.5 & 77.6 & 79.5 & - & - & - \\  Structured (**Ours**) & 76.8 & 59.5 & 64.2 & 67.2 & (-1.7, 1.2) & 198 & 7.75 \\ Structured+RS (**Ours**) & **86.5** & **76.9** & **84.0** & **83.0** & (-1.1, 1.5) & 205 & **9.55** \\   

* https://tatsu-lab.github.io/alpac_eval
* https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
* https://lmsys.org/blog/2023-06-22-leaderboard
* https://lmsys.org/blog/2023-06-22-leaderboard
* https://lmsys.org/blog/2023-06-22-leaderboard
* https://lmsys.org/blog/2023-06-22-leaderboard

Table 2: **Summary of our results**. We present win rates and scores of our cheat, comparing them to the state-of-the-art models (_recorded before October 1st, 2024_). The evaluation is conducted using GPT-4-1106-Preview as the auto-annotator. For pairwise comparison benchmarks, including AlpacEval 2.0 and Arena-Hard-Auto, the reference models are GPT-4-1106-Preview and GPT-4-0314, respectively. We report the LC win rates, raw win rates, discrete win rates, and rating scores. Our structured response combined with random search (Structured+RS) significantly improves performance across all benchmarks, achieving the highest win rates and scores.

Figure 3: **Boxplot of the \( p(=)\) using different null responses**. The response of each index can be found in Table 6. The target model’s responses are positioned in the second slot by “Default” and swapped to the first slot in “Swap”. Our structured response (marked as “Ours”) achieves the lowest log probabilities compared to the other 16 persuasive responses.

parameters) (Meta, 2024; Touvron et al., 2023). These models have been well-aligned by pair-wise preference data and show the ability to evaluate other LLMs.2 For RS, we set \(N=8\) and \(T=8192\).

**Sanity check**. Before we use Llama-3-Instruct models as our auto-annotator in the AlpacaEval framework, we conduct a sanity check to see whether they have such evaluation capability. We evaluate different automatic annotators on the AlpacaEval set by comparing 2.5K human annotations collected by Dubois et al. (2023). As shown in Table 3, both Llama-3-8B-Instruct and Llama-3-70B-Instruct show non-trivial human agreement and correlations. More concretely, Llama-3-8B-Instruct is comparable to ChatGPT, and Llama-3-70B-Instruct matches GPT-4 auto-annotator. Thus, it is reasonable to use them as the auto-annotators.

**Is the structured response useful on open-source auto-annotators?** We evaluate the \( p(=)\) on a subset of the AlpacaEval 2.0 test instructions using different null responses. As shown in Figure 4, the structured response has little effect on Llama-3 auto-annotators. In the case of Llama-3-8B-Instruct, the structured response does not exploit positional weaknesses in this model as the log probabilities for the default and swapped positions are generally similar to different persuasive responses. However, on Llama-3-70B-Instruct, we observe that under the swap setting, the structured response manages to reduce the log probability. Additionally, regarding the positional bias, the Llama-3-8B-Instruct shows little position bias as the probabilities for both default and swapped positions are fairly close. In contrast, Llama-3-70B-Instruct shows a clear positional bias under the swapped setting, with a higher log probability, indicating the model's strong preference for the last output ("M"). The larger Llama-3-70B-Instruct model behaves more similarly to the more advanced GPT-4, as it demonstrates a greater response to both the structured response and positional bias than the smaller 8B model. This suggests that model size may contribute to the susceptibility to our cheating techniques. Overall, the structured response is considerably less effective on the Llama-3 models compared to GPT-4. A possible explanation for this difference is that the instruction-following capabilities of the Llama-3 models, especially the smaller 8B variant, are not as powerful as those of GPT-4, making them less prone to cheating responses.

**Is random search effective on open-source auto-annotators?** The results shown in Table 5 demonstrate the effectiveness of random search on open-source auto-annotators like LLama-3-8B-Instruct and LLama-3-70B-Instruct. For Llama-3-8B-Instruct, without random search, the structured response achieves only a \(2.9\%\) LC win rate and \(1.4\%\) raw win rate. However, when the random search is applied, the win rates surge dramatically to \(95.4\%\) (LC) and \(86.3\%\) (raw), representing a gain of \(92.5\) percentage points in the LC win rate. For Llama-3-70B-Instruct, the structured response alone yields minimal success with a \(0.4\%\) LC win rate and \(0.2\%\) overall. Once random search is applied, these win rates leap to \(95.1\%\) (LC) and \(91.6\%\) (raw), showcasing improvements of \(94.7\) and \(91.4\) percentage points, respectively. These results indicate that random search is highly effective in improving the cheat's success on open-source auto-annotators, driving win rates close to \(100\%\).

    & **Human** & **Spearman** & **Pearson** &  &  &  \\  & **agreement** & **corr.** & & & & **prefer longer** \\  GPT-4\({}^{*}\) & 69.2 & 0.97 & 0.93 & 28.4 & 14.6 & 0.68 \\ CoT-GPT-4-Turbo\({}^{*}\) & 68.6 & 0.97 & 0.90 & 29.3 & 18.4 & 0.67 \\ GPT-4-Turbo\({}^{*}\) & 68.1 & 0.93 & 0.82 & 30.2 & 15.6 & 0.65 \\ Human\({}^{*}\) & 65.7 & 1.00 & 1.00 & 0.0 & 34.3 & 0.64 \\ ChatGPT\({}^{*}\) & 57.3 & 0.72 & 0.71 & 39.4 & 34.1 & 0.59 \\  Llama-3-8B-Instruct & 56.0 & 0.70 & 0.77 & 41.4 & 37.6 & 0.62 \\ Llama-3-70B-Instruct & 68.8 & 0.90 & 0.85 & 30.1 & 11.5 & 0.78 \\   ^{*}\) These results are taken from https://github.com/tatsu-lab/alpaca_eval.} \\   

Table 3: **Evaluation of auto-annotators vs. human annotations on AlpacaEval.** This table compares various auto-annotators to 2.5K human annotations. The human agreement metric measures how well each annotator aligns with the majority preferences of humans, based on approximately 650 examples with cross-annotations from four different human annotations per example. The spearman and pearson correlation metrics assess the correlation between the rankings generated by the auto-annotators and those produced by humans. Additionally, we report the annotators’ bias, variance, and the probability of preferring longer responses over shorter ones.

**Does searching on the test instructions directly help?** We also consider direct cheating. Direct cheating serves as an indicator of the upper bound of transfer cheating. The results shown in Table 4 clearly show that searching directly on the test instructions significantly boosts the cheat's performance. For the Llama-3-8B-Instruct model, using the structured response combined with random search without test instruction access achieves a strong LC win rate of \(95.4\%\) and an overall win rate of \(86.3\%\). However, when the adversarial prefix is optimized directly on the test instructions, the LC win rate jumps to an almost perfect \(99.8\%\), and the overall win rate increases to \(99.4\%\), representing gains of \(4.6\) and \(13.1\) percentage points, respectively. Similarly, for the Llama-3-70B-Instruct model, random search without access to test instructions results in an LC win rate of \(95.1\%\) and an overall win rate of \(91.6\%\). When the test instructions are used, these rates climb to \(99.4\%\) (LC) and \(98.2\%\) (raw), showing improvements of around \(4.3\) percentage points for LC and \(6.6\) for overall win rate. These results highlight that directly searching on the test instructions offers significant advantages, further optimizing the adversarial prefix and nearly achieving perfect performance.

**Can our method be combined with normal responses?** Our method can be combined with normal, informative responses by appending our cheating response to the original responses. As demonstrated in Figure 5, when combined with a more informative model like GPT-3.5-0613, we observe that the initial win rates are already high, even before significant optimization steps are taken. This is evident in Figure 4(b) and 4(d), where the performance (win rate and length-controlled win rate) increases steadily from a high baseline as optimization progresses. However, it is important to emphasize that our setting of using a null, non-informative model is far more challenging. In this setting (Figure 4(a) and 4(c)), the null model starts with much lower win rates because it offers no relevant information to the input queries, making it much harder to trick the auto-annotator. Despite this, as the optimization steps progress, the null model's performance steadily increases, ultimately achieving competitive win rates. This highlights the robustness of our method, showing that it can manipulate LLM-based benchmarks even in the most challenging scenario--where the model outputs irrelevant, non-informative responses. The success of our method under such difficult conditions makes it a valuable stress test of benchmark robustness.

Figure 4: **Boxplot of the \( p(=)\) using different null responses across different responses and auto-annotators. The structured response (index=0) is not as effective for the Llama models as for GPT-4-1106-Preview. An interesting observation is that, on Llama-3-70B-Instruct, the structured response successfully reduces the log probability under the swap setting. In contrast, the structured response is ineffective on Llama-3-8B-Instruct for both positions, implying that its effectiveness may be related to the model’s ability to follow instructions.**
## 6 Discussion

**Anti-cheating strategies and related work** Due to page limits, we provide the details of anti-cheating strategies and related work at Section A and B in Appendix.

**Conclusion**. In this paper, we uncover even null models can achieve high win rates by exploiting structural weaknesses in the evaluation process. These findings highlight the need for more robust automatic LLM benchmarks to ensure fair and reliable assessments of LLM performance. As the field of AI continues to evolve, we must address these vulnerabilities to maintain trust in the systems we use to evaluate language models. Failure to do so could lead to widespread manipulation of benchmarks, undermining the progress and credibility of AI research. In summary, while automatic LLM benchmarks provide a scalable and efficient way to evaluate models, they are not immune to cheating. The development of anti-cheating mechanisms and the reconsideration of benchmark design will be crucial steps toward ensuring the reliability and fairness of future LLM evaluations.

**Limitations and future work**. Despite the promising findings of our study, there are limitations that must be acknowledged. First, our work primarily focuses on specific benchmarks, and while our results generalize well across them, the cheat's effectiveness on other, less-studied benchmarks remains uncertain. Additionally, our approach relies heavily on the manual crafting of structured responses. Future work could explore more automated methods for generating adversarial outputs, which would allow adversaries to exploit these vulnerabilities on a larger scale. One important area for future research is the development of more robust anti-cheating mechanisms. Current efforts to mitigate cheating on LLM benchmarks have focused on controlling output length and style, but these measures have proven insufficient in the face of structured responses. New defenses will be crucial for maintaining the integrity of LLM benchmarks.

    &  &  &  &  \\  & & & & LC & Win Rate & Discrete \\   Llama-3 \\ 8B-Instruct \\ 390 \\  } & GPT-4 &  GPT 3.5 Turbo (06/13) \\  & - & 48.1 & 38.8 & 39.4 \\   & & & Structured & ✗ & 2.9 & 1.4 & 0.7 \\   & & Structured+RS & ✗ & **95.4** & **86.3** & **91.8** \\   & & Structured+RS & ✓ & 99.8 & 99.4 & 99.9 \\   Llama-3 \\ 70B-Instruct \\ 395 \\  } & GPT-4 & 
 GPT 3.5 Turbo (06/13) \\  & - & 30.5 & 19.7 & 19.8 \\   & & Structured & ✗ & 0.4 & 0.2 & 0.0 \\   & & Structured+RS & ✗ & **95.1** & **91.6** & **93.7** \\   & & Structured+RS & ✓ & 99.4 & 98.2 & 99.5 \\   

Table 4: **Win rates of the cheat against Llama-3-Instruct family.** We present the win rates of our cheat on AlpacaEval 2.0 when targeting models in the Llama-3-Instruct family. We evaluate different methods (Structured and Structured+Random Search) with and without access to test instructions. The results are measured using LC win rate, raw win rate, and discrete comparison metrics. We also explore the effect of different auto-annotators and random search optimization. The upper-bound win rates are approached by assuming the visibility of test instructions.

Figure 5: **Win rates along the number of steps across different models**. The win rates increase generally as the optimization steps grow. Notably, incorporating an informative model like GPT-3.5-0613 with our cheat has high initial win rates, indicating the challenge of our null model setting. Nonetheless, our cheat drives both models to over \(90\%\) win rates.