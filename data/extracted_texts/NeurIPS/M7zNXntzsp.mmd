# Building a stable classifier with the inflated argmax

Jake A. Soloff

Department of Statistics

University of Chicago

Chicago, IL 60637

soloff@uchicago.edu

&Rina Foygel Barber

Department of Statistics

University of Chicago

Chicago, IL 60637

rina@uchicago.edu

&Rebecca Willett

Departments of Statistics and Computer Science

NSF-Simons National Institute for Theory and Mathematics in Biology

University of Chicago

Chicago, IL 60637

willett@uchicago.edu

###### Abstract

We propose a new framework for algorithmic stability in the context of multiclass classification. In practice, classification algorithms often operate by first assigning a continuous score (for instance, an estimated probability) to each possible label, then taking the maximizer--i.e., selecting the class that has the highest score. A drawback of this type of approach is that it is inherently unstable, meaning that it is very sensitive to slight perturbations of the training data, since taking the maximizer is discontinuous. Motivated by this challenge, we propose a pipeline for constructing stable classifiers from data, using bagging (i.e., resampling and averaging) to produce stable continuous scores, and then using a stable relaxation of argmax, which we call the "inflated argmax", to convert these scores to a set of candidate labels. The resulting stability guarantee places no distributional assumptions on the data, does not depend on the number of classes or dimensionality of the covariates, and holds for any base classifier. Using a common benchmark data set, we demonstrate that the inflated argmax provides necessary protection against unstable classifiers, without loss of accuracy.

## 1 Introduction

An algorithm that learns from data is considered to be _stable_ if small perturbations of the training data do not lead to large changes in its output. Algorithmic stability is an important consideration in many statistical applications. Within the fairness literature, for instance, stability is one aspect of reliable decision-making systems . In interpretable machine learning, it similarly serves as a form of reproducibility .  relates stability to robustness, where a machine learning algorithm is robust if two samples with similar feature vectors have similar test error. In the context of generalization bounds,  and subsequent authors study stability of an algorithm's real-valued output--for instance, values of a regression function. In the setting of a multiclass classification problem, where the data consists of features \(X\) and labels \(Y[L]=\{1,,L\}\), the results of this literature can thus be applied to analyzing a classifier's predicted probabilities \(_{}(X)\)--i.e., our learned estimates of the conditional probabilities \(\{Y= X\}\), for each \([L]\). However, as we will see, stability of these predicted probabilities by no means implies stability of the predicted label itself, \(=*{argmax}_{[L]}_{}(x)\)--an arbitrarilysmall perturbation in \(_{}(x)\) might completely change the predicted label \(\). The distinction matters: for system trustworthiness, we care about the model's final output, on which users base their decisions.

In this paper, we propose a new framework for algorithmic stability in the context of multiclass classification, to define--and achieve--a meaningful notion of stability when the output of the algorithm consists of predicted labels, rather than predicted probabilities. Our work connects to other approaches to allowing for ambiguous classification, including set-valued classification  and conformal inference  (we will discuss related work further, below).

### Problem setting

In supervised classification, we take a data set \(=(X_{i},Y_{i})_{i[n]}\) consisting of \(n\) observations, and train a classifier that maps from the feature space \(\) to the set \([L]\) of possible labels.1 Typically, this map is constructed in two stages. First we run some form of regression to learn a map \(:_{L-1}\) from features to predicted probabilities, with \(_{}(x)\) denoting our learned estimate of the conditional label probability, \(\{Y= X=x\}\) (here \(_{L-1}=\{w^{L}:w_{i} 0,_{i}w_{i}=1\}\) denotes the probability simplex in \(L\) dimensions). We will write \(=()\), where \(\) denotes the learning algorithm mapping a data set \(\) (of any size) to the map \(\). Then the second step is to convert the predicted probabilities \(_{}(x)\) to a predicted label, which is typically done by taking the argmax, i.e., \(=*{argmax}_{}_{}(x)\) (with some mechanism for breaking ties). This two-stage procedure can be represented in the following diagram:

When predictions are ambiguous, meaning two or more classes nearly achieve the maximum predicted probability, the selected label becomes unstable and can change based on seemingly inconsequential perturbations to the training data. In other words, the above workflow is fundamentally incompatible with the goal of algorithmic stability. Consequently, in this paper we instead work with _set-valued classifiers_, which return a set of candidate labels--in practice, this typically leads to a singleton set for examples where we have high confidence in the label, but allows for a larger set in the case of ambiguous examples. While the idea of returning a set of candidate labels is not itself new, we will see that the novelty of our work lies in finding a construction that offers provable stability guarantees.

In this framework, a feature vector \(x\) is now mapped to a set of candidate labels \([L]\) (rather than a single label \(\)), via a selection rule \(s\), as illustrated here:

\[[width=142.26378pt]{images/142.eps}\]

Formally, given a test point \(x\), this two-stage procedure returns \(=s((x))[L]\), where \(=()\) is the output of the regression algorithm \(\) trained on data \(\). Here \(s:_{L-1}([L])\) denotes a _selection rule_, mapping a vector of estimated probabilities to the set of candidate labels, and \(([L])\) denotes the set of subsets of \([L]\). Of course, the earlier setting--where the procedure returns a single label \(=*{argmax}_{}_{}(x)\), rather than a subset--can be viewed as a special case by simply taking \(s\) to be the \(*{argmax}\) operator.

If we instead allow for \(\) to contain multiple candidate labels, a commonly used version of this framework is given by a top-5 procedure (or top-\(k\), for any fixed \(k\)). That is, after running a learning algorithm \(\) to estimate probabilities \((x)\), the selection rule \(s\) then returns the top 5 labels--the labels \(_{1},,_{5}[L]\) corresponding to the highest 5 values of \(_{}(x)\). This approach is more stable than a standard argmax. However, the choice of 5 in this setting is somewhat arbitrary, and the set \(\) always contains 5 candidate labels, regardless of the difficulty of the test sample--while intuitively, we might expect that it should be possible to return a smaller \(\) for test points \(x\) that are easier to classify (and a larger \(\) if \(x\) is more ambiguous). In contrast, in our work we seek a more principled approach, where we provide provable stability guarantees while also aiming for the set \(\) to be as small as possible.

### Overview of main results

In this work, we introduce _selection stability_, a new definition of algorithmic stability in the context of classification, which focuses on the stability of the predicted label. We reduce the problem of stabilizing a classifier to separately stabilizing the learning and selection stages, described above. For the selection rule \(s\) of the two-stage procedure, we propose the _inflated argmax_ operation:

**Definition 1** (Inflated argmax).: _For any \(w^{L}\), define the inflated argmax as_

\[*{argmax}^{}(w):=\{j[L]:*{dist}( w,R^{}_{j})<\},\] (1)

_where \(*{dist}(w,R^{}_{j})=_{v R^{}_{j}} \|w-v\|\), and where_

\[R^{}_{j}=\{w^{L}:w_{j}_{ j}w_{ }+/\}.\]

Our procedure will then return the set \(=*{argmax}^{}((x))\) of candidate labels--intuitively, \(j*{argmax}^{}((x))\) indicates that a small perturbation of the predicted probabilities, \((x)\), would lead to the \(j\)th label's predicted probability being largest by some margin. In particular, by construction, the inflated argmax will always include any maximal index--that is, if \((x)_{j}\) is the (possibly non-unique) largest estimated probability, then we must have \(j=*{argmax}^{}((x))\) (this fact, and other properties of the inflated argmax, will be established formally in Proposition 10 below).

In this work, we derive the stabilizing properties of the inflated argmax, and give an algorithm to compute it efficiently. We prove that combining this operation with bagging at the learning step will provably stabilize any classifier. In particular, our guarantee holds with no assumptions on the data, and no constraints on the dimensionality of the covariates nor on the number of classes.

## 2 Framework: stable classification

In this section, we propose a definition of algorithmic stability in the setting of multiclass classification. To begin, we formally define a _classification algorithm_ as a map2

\[:_{n 0}([L])^{n}\ \ \ \ ^{}([L]),\]

which maps a training data set \(\) of any size \(n\), together with a test feature vector \(x\), to a candidate set of labels \(=(,x)\). To relate this notation to our earlier terminology, the two-stage selection procedure described in Section 1.1 corresponds to the classification algorithm

\[(,x)=s((x))=( ).\]

Abusing notation, we will write this as \(=s\), indicating that \(\) is obtained by applying the selection rule \(s\) to the output of the learning algorithm \(\).

We now present our definition of algorithmic stability for a classification algorithm \(\). As is common in the algorithmic stability literature, we focus on the stability of the algorithm's output with respect to randomly dropping a data point: does the output of \(\) on a test point \(x\) change substantially if we drop a single data point from the training data \(\)? If the algorithm's output were a real-valued prediction \(\), we could assess this by measuring the real-valued change in \(\) when a single data point is dropped. For classification, however, we will need to take a different approach:

**Definition 2** (Selection stability).: _We say a classification algorithm \(\) has selection stability \(\) at sample size \(n\) if, for all datasets \(([L])^{n}\) and all test features \(x\),_

\[_{i=1}^{n}\{^{ i}= \},\]

_where \(=(,x)\) and where \(^{ i}=(^{ i},x)\), for each \(i[n]\)._

Here the notation \(^{ i}\) denotes the data set \(\) with \(i\)th data point removed--that is, for a data set \(=((X_{j},Y_{j}))_{j[n]}\), the leave-one-out data set is given by \(^{ i}=((X_{j},Y_{j}))_{j[n] i}\).

### Interpreting the definition

Intuitively, selection stability controls the probability that our algorithm makes wholly inconsistent claims when dropping a single data point at random from the training set. If we interpret \(\) as making the claim that the true label \(Y\) is equal to one of the labels in the candidate set \(\), and similarly for \(^{ i}\), then as soon as there is _even one single value_ that lies in both \(\) and \(^{ i}\), this means that the two claims are not contradictory--even if the sets are large and are mostly nonoverlapping.

At first glance, this condition appears to be too mild, in the sense that we require the two prediction sets only to have _some_ way of being compatible, and allows for substantial difference between the sets \(\) and \(^{ i}\). However, since standard classification algorithms always output a single label, they often cannot be said to be stable even in this basic sense. Thus, we can view this definition as providing a minimal notion of stability that we should require any interpretable method to satisfy.

### Connection to classical algorithmic stability

Most prior work on algorithmic stability concerns algorithms with continuous outputs--for example, in our notation above, the algorithm \(\) that returns estimated probabilities \((x)\). With such algorithms, there are more standard tools at our disposal for quantifying and ensuring stability. In this section, we connect classical algorithmic stability to our notion of selection stability (Definition 2). We first recall the following definition.

**Definition 3** (Tail stability).: _[_25_]_ _A learning algorithm \(\) has tail stability \((,)\) at sample size \(n\) if, for all datasets \(\) of size \(n\) and all test features \(x\),_

\[_{i=1}^{n}\{\|(x)-^{ i}( x)\|\},\]

_where \(=()\) and \(^{ i}=(^{ i})\), and where \(\|\|\) denotes the usual Euclidean norm on \(^{L}\)._

This intuitive notion of stability is achieved by many well-known algorithms \(\), such as nearest-neighbor type methods or methods based on bagging or ensembling (as established by [25; 26]--see Section 3.1 below for details).

### From stability to selection stability

To construct a classification algorithm using the two-stage procedure outlined in Section 1.1, we need to apply a selection rule \(s\) to the output of our learning algorithm \(\). We might expect that choosing a stable \(\) will lead to selection stability in the resulting classification algorithm--but in fact, this is not the case: even if the learning algorithm \(\) is itself extremely stable in the sense of Definition 3, the classification rule obtained by applying \(*{argmax}\) to the output of \(\) can still be extremely unstable. The underlying issue is that \(*{argmax}\) is extremely discontinuous--the perturbation \(\|(x)-^{ i}(x)\|\) in the predicted probabilities can be arbitrarily small but still lead to different predicted labels, i.e., \(*{argmax}_{}_{}(x)*{argmax}_{ }_{}^{ i}(x)\).

Since combining \(*{argmax}\) with a stable learning algorithm \(\) will not suffice, we instead seek a different selection rule \(s\)--one that will ensure selection stability (when paired with a stable learning algorithm \(\)). To formalize this aim, we introduce another definition:

**Definition 4** (\(\)-compatibility).: _A selection rule \(s:_{L-1}([L])\) is \(\)-compatible if, for any \(v,w_{L-1}\),_

\[\|v-w\|< s(v) s(w).\]

This notion of \(\)-compatibility allows us to construct classification algorithms with selection stability. Combining the above definitions leads immediately to the following result:

**Proposition 5**.: _Let \(\) be a learning algorithm with tail stability \((,)\) at sample size \(n\), and let \(s\) be a selection rule satisfying \(\)-compatibility. Then the classification algorithm \(=s\) has selection stability \(\) at sample size \(n\)._

Therefore, by pairing a stable learning algorithm \(\) with a compatible selection rule \(s\), we will automatically ensure selection stability of the resulting classification algorithm \(=s\).

Of course, \(\)-compatibility of the selection rule \(s\) might be achieved in a trivial way--for instance, \(s\) returns the entire set \([L]\) for any input. As is common in statistical settings (e.g., a tradeoff between Type I error and power, in hypothesis testing problems), our goal is to ensure selection stability while returning an output \(\) that is as informative as possible. In particular, later we will consider the specific aim of constructing \(\) to be a singleton set as often as possible.

## 3 Methodology: assumption-free stable classification

In this section, we formally define our pipeline for building a stable classification procedure using any base learning algorithm \(\). At a high level, we leverage Proposition 5 and separately address the learning and selection stages described in Section 2.

1. In Section 3.1, we construct a bagged (i.e., ensembled) version of the base learning algorithm \(\). The recent work of  ensures that the resulting bagged algorithm has tail stability \((,)\), with \(}\).
2. In Section 3.2, we introduce a new selection rule, the inflated argmax, and establish its \(\)-compatibility. Combined with the bagged algorithm, then, the resulting classification algorithm will be guaranteed to have selection stability \(\) (by Proposition 5).

Before describing these two steps in detail, we first present our main theorem that characterizes the selection stability guarantee of the resulting procedure. Given sample size \(n\) for the training data set \(\), the notation \(_{m}\) will denote a bagged version of the base algorithm \(\), obtained by averaging over subsamples of \(\) comprised of \(m\) data points sampled either with replacement ("bootstrapping", commonly run with \(m=n\)) or without replacement ("subbagging", commonly run with \(m=n/2\))--see below for details.

**Theorem 6**.: _Fix any sample size \(n\), any bag size \(m\), and any inflation parameter \(>0\). For any base learning algorithm \(\), the classification algorithm \(=*{argmax}^{}_{m}\), obtained by combining the bagged version of \(\) together with the inflated argmax, satisfies selection stability \(\) where_

\[=^{-2}}{1-p_{n,m}},\] (2)

_where \(p_{n,m}=1-(1-)^{m}\) for bootstrapping, and \(p_{n,m}=\) for subbagging._

The guarantee in Theorem 6 holds for any base learning algorithm \(\), and applies regardless of the complexity of the feature space \(\), and allows the test feature \(x\) to be chosen adversarially. The dependence on the number of classes \(L\) is mild--in fact, the tail stability parameter \(\) in (2) differs only by a factor of two for \(L=2\) versus \(L=\). Of course, the guarantee does depend on the choice of the bag size \(m\). In general, a smaller value of \(m\) leads to a stronger stability guarantee (since \(p_{n,m}\) increases with \(m\)), but this comes at the expense of accuracy since we are training the base algorithm \(\) on subsampled data sets \(^{r}\) of size \(m\) (rather than \(n\)). For the common choices of \(m=n\) for bootstrap or \(m=n/2\) for subbagging, we have \(}{1-p_{n,m}}=(1)\) for each.

### Bagging classifier weights

In this section, we formally define the construction of a bagged algorithm to recap the tail stability result that our framework leverages. We consider the two most common variants of this ensembling method: bagging (based on bootstrapping training points) and subbagging (based on subsampling). For any data set \(([L])^{n}\), we can define a subsampled data set of size \(m\) as follows: for a sequence \(r=(i_{1},,i_{m})[n]^{m}\) (which is called a _bag_), we define the corresponding subsampled data set \(^{r}=(X_{i},Y_{i})_{i r}([L])^ {m}\). Note that if the bag \(r\) contains repeated indices (i.e., \(i_{k}=i_{}\) for some \(k\)), then the same data point from the original data set \(\) will appear multiple times in \(^{r}\).

**Definition 7** (Bootstrapping or subbagging a base learning algorithm \(\)).: _Given a data set \(([L])^{n}\) and some \(x\), return the output \(}_{m}()(x)=_{r}[( ^{r})(x)]\), where the expected value is taken with respect to a bag \(r\) that is sampled as follows:_

* _Bootstrapping (sometimes simply referred to as bagging)_ _[_10_,_ 11_]_ _constructs each bag_ \(r\) _by sampling_ \(m\) _indices_ \(r=(i_{1},,i_{m})\) _uniformly with replacement from_ \([n]\)_._
* _Subbagging_ _[_1_]_ _constructs each bag_ \(r\) _by sampling_ \(m n\) _indices_ \(r=(i_{1},,i_{m})\) _uniformly without replacement from_ \([n]\)_._

The following result  ensures tail stability for any bootstrapped or subbagged algorithm:

**Theorem 8** ().: _For any base learning algorithm \(\) returning outputs in \(_{L-1}\), the bagged algorithm \(}_{m}\) has tail stability \((,)\) for any \(,>0\) satisfying (2)._

Computational cost of bagging.In this section, we have worked with the ideal, derandomized version of bagging for simplicity--that is, we assume that the expected value \(_{r}[(^{r})(x)]\) is calculated exactly with respect to the distribution of \(r\). In practice, of course, this is computationally infeasible (since for bootstrapping, say, there are \(n^{m}\) possible bags \(r\)), and so we typically resort to Monte Carlo sampling to approximate this expected value, defining \(}_{m}()(x)=_{b=1}^{B}(^{r_{b}})(x)\), for \(B\) i.i.d. bags \(r_{1},,r_{B}\) sampled via bootstrapping or subbagging. See Appendix C for extensions of our main result, Theorem 6, to the finite-\(B\) version of the method.

### The inflated argmax

We now return to the inflated argmax operator, as defined in Definition 1. This operator, in place of the standard argmax, allows for stability in classification.

While \(^{}\) is defined as an operator on \(^{L}\), in the context of classification algorithms, we only apply it to vectors \(w\) lying in the simplex \(_{L-1}\) (in particular, in the context of a two-stage classification procedure as in Section 1.1, we will apply the inflated argmax to the vector \(w=(x)\) of estimated probabilities). In Figure 1, we visualize the inflated argmax applied to the simplex in a setting with three possible labels, \(L=3\). Each different shaded area corresponds to the region of the simplex \(_{L-1}\) where \(^{}(w)\) returns a particular subset.

#### 3.2.1 Inflated argmax leads to selection stability

Unlike the standard argmax, the inflated argmax allows us to achieve stability in the context of classification. This is due to the following theorem, which shows that the inflated argmax is \(\)-compatible, as introduced in Definition 4. (The proofs of this result and all properties of the inflated argmax are deferred to Appendix A.)

**Theorem 9**.: _For any \(>0\) and any \(v,w^{L}\), if \(\|v-w\|<\) then \(^{}(v)^{}( w)\). In particular, viewing \(^{}\) as a map on the simplex \(_{L-1}\), \(^{}\) is \(\)-compatible._

To gain a more concrete understanding of this theorem, we can look again at Figure 1 to examine the case \(L=3\). Theorem 9 ensures that any two regions corresponding to disjoint subsets--e.g., the top corner region corresponding to \(\{2\}\), and the bottom center region corresponding to \(\{1,3\}\)--are distance at least \(\) apart. In particular, the region in the center, corresponding to vectors \(w\) that map to the full set \(\{1,2,3\}\), is a curved triangle of constant width \(\), also known as a _Reuleaux triangle_.

Proof of Theorem 6.With the above results in place, we can now see that the inflated argmax allows us to achieve selection stability, when combined with a bagged algorithm. In particular, combining Theorem 8, Theorem 9, and Proposition 5 immediately implies our main result, Theorem 6.

#### 3.2.2 Additional properties of the inflated argmax

The following result establishes some natural properties obeyed by the inflated argmax, and also compares to the standard argmax.

**Proposition 10** (Basic properties of the inflated argmax).: _Fix any \(>0\). The inflated argmax operator satisfies the following properties:_

1. _(Including the argmax.) For any_ \(w^{L}\)_,_ \((w)^{}(w)\)_._3 _Moreover_ \(_{>0}^{}(w)=(w)\)_._ 2. _(Monotonicity in_ \(\)_.) For any_ \(w^{L}\) _and any_ \(<^{}\)_,_ \(^{}(w)^{ ^{}}(w)\)_._ 3. _(Monotonicity in_ \(w\)_.) For any_ \(w^{L}\)_, if_ \(w_{j} w_{k}\)_, then_ \(j^{}(w) k^{}(w)\)_._ 
Next, while we have established that inflated argmax offers favorable stability properties, we have not yet asked whether it can be efficiently computed--in particular, it is not immediately clear how to verify the condition \((w,R^{}_{j})<\) in (1), in order to determine whether \(j^{}(w)\). The following result offers an efficient algorithm for computing the inflated argmax set.

**Proposition 11** (Computing the inflated argmax).: _Fix any \(w^{L}\) and \(>0\). Let \(w_{} w_{[L]}\) denote the order statistics of \(w\), and define_

\[(w)=k[L]:_{=1}^{k}(w_{[]}-w_{[k]}) ^{2}+_{=1}^{k}(w_{[]}-w_{[k]})^{2}^{2}}.\]

_Then_

\[^{}(w)=\{j[L]:w_{j}>}+_{1}(w)-(w)+1}}{ (w)}+(_{1}(w))^{2}-_{2}(w)}\},\]

_where \(_{1}(w)=++w_{[(w)]}}{(w)}\), and \(_{2}(w)=^{2}++w_{[(w)]}^{2}}{(w)}\)._

Figure 1: The left plot illustrates the inflated argmax (1) over the simplex \(_{L-1}\) when \(L=3\). The numbers in brackets correspond to the output of the inflated argmax, \(^{}(w)\), for various points \(w\) in the simplex. The right plot shows the same but for the standard argmax, which corresponds to the limit of \(^{}(w)\) as \( 0\).

Finally, thus far we have established that \(*{argmax}^{}\) enables us to achieve stable classification with a computationally efficient selection rule--but we do not yet know whether \(*{argmax}^{}\) is optimal, or whether some other choice of \(s\) might lead to smaller output sets \(\) (while still offering assumption-free stability). For instance, we might consider a fixed-margin rule,

\[s^{}_{}(w)=\{j:w_{j}>_{}w_{}-/ \},\] (3)

for which \(\)-compatibility also holds--might this simpler rule be better than the inflated argmax? Our next result establishes that--under some natural constraints--\(*{argmax}^{}\) is in fact the optimal choice of selection rule, in the sense of returning a singleton set (i.e., \(||=1\)) as often as possible.

**Proposition 12** (Optimality of the inflated argmax).: _Let \(s:_{L-1}([L])\) be any selection rule. Suppose \(s\) is \(\)-compatible (Definition 4), permutation invariant (in the sense of Proposition 10), and contains the argmax. Then for any \(w_{L-1}\) and any \(j[L]\),_

\[s(w)=\{j\}*{argmax}^{}(w)=\{j\}.\]

In other words, for any selection rule \(s\) satisfying the assumptions of the proposition (which includes the fixed-margin rule, \(s^{}_{}\)), if \(s(w)\) is a singleton set then so is \(*{argmax}^{}(w)\). (See also Appendix D for a closer comparison between the inflated argmax and the fixed-margin selection rule given in (3).)

## 4 Experiments

In this section, we evaluate our proposed pipeline, combining subbagging with the inflated argmax, with deep learning models and on a common benchmark data set.4

Data and models.We use Fashion-MNIST , which consists of \(n=60,000\) training pairs \((X_{i},Y_{i})\), \(N=10,000\) test pairs \((_{j},_{j})\), and \(L=10\) classes. For each data point \((X,Y)\), \(X\) is a \(28 28\) grayscale image that pictures a clothing item, and \(Y[L]\) indicates the type of item, e.g., a dress, a coat, etc. The base model we use is a variant of LeNet-5, implemented in PyTorch  tutorials as GarmentClassifier(). The base algorithm \(\) trains this classifier using \(5\) epochs of stochastic gradient descent.

Methods and evaluation.We compare four methods:

1. The argmax of the base learning algorithm \(\).

Figure 2: Results on the Fashion MNIST data set. The figure shows the instability \(_{j}\) (defined in (4)) over all test points \(j=1,,N\). The curves display the fraction of \(_{j}\)â€™s that exceed \(\), for each value \(\). The vertical axis is on a log scale. See Section 4 for details.

2. The \(\)-inflated argmax of the base learning algorithm \(\) with tolerance \(=.05\).
3. The argmax of the subbagged algorithm \(}_{m}\), with \(B=1,000\) bags of size \(m=n/2\).
4. The \(\)-inflated argmax of the subbagged algorithm \(}_{m}\), with \(B=1,000\) bags of size \(m=n/2\) and tolerance \(=.05\).

We evaluate each method based on several metrics. First, to assess selection stability, for each test point \(j=1,,N\) we compute its instability

\[_{j}:=_{k=1}^{500}\{s((_ {j})) s(^{ i_{k}}(_{j}))=\},\] (4)

where \(i_{1},,i_{500}\) are sampled uniformly without replacement from \([n]\) (i.e., we are sampling 500 leave-one-out models \(^{ i}\) to compare to the model \(\) trained on the full training set). Since our theory concerns worst-case instability over all possible test points, we evaluate the maximum instability

\[_{}:=_{j[N]}_{j}.\] (5)

Second, to evaluate how accurate each method is, we compute how often the method returns the correct label as a singleton \(_{}\) and set size \(_{}\) (the number of labels in the candidate set):

\[_{}:=_{j=1}^{N}\{s( (_{j}))=\{_{j}\}\},_{}:=_{j=1}^{N}s((_{j})).\] (6)

Ideally we would want a method to return the correct singleton as frequently as possible (a large value of \(_{}\) that is close to \(1\)) and small set size (a value of \(_{} 1\) that is close to \(1\)).

Results.In Figure 2, we present results for the instability of each method, plotting the survival function of the instability for all test points \((_{j})_{j[N]}\). The standard argmax applied to the base algorithm, \(*{argmax}\), has the longest tail, meaning \(_{j}\) is large for many test points \(j\). The inflated argmax applied to the base algorithm, \(*{argmax}\), offers only a very small improvement on the stability. By contrast, applying the standard argmax to the subbagged algorithm, \(*{argmax}}_{m}\), offers a much more substantial improvement, since the red dashed curve is much smaller than both of the solid curves. Still, some of the largest \(_{j}\) for this method are near \(1\), meaning for these test points the returned set is sensitive to dropping a single training instance. Combining the inflated argmax with subbagging, \(*{argmax}^{}}_{m}\), offers a dramatic improvement: in this case \(_{j}=0\) for every \(j=1,,N\).

In Table 1, we present the average measures, \(_{}\) and \(_{}\) and the worst-case measure \(_{}\). For both the base algorithm and the subbagged algorithm, applying the inflated argmax incurs a small cost both in terms of the frequency of returning the correct label as a singleton and the average size. The inflated argmax increases set size only very slightly, but when combined with subbagging, we improve upon the \(_{}\) of the original method and achieve an extremely high level of stability (\(_{}=0\)), while returning a singleton set in the vast majority of cases.

 
**Selection rule** & **Algo.** & \(_{}\) & \(_{}\) & \(_{}\) \\  \(*{argmax}\) & \(\) & 0.879 (0.003) & 1.000 (0.000) & 1.000 \\  \(*{argmax}^{}\) & \(}\) & 0.873 (0.003) & 1.015 (0.001) & 1.000 \\  \(*{argmax}\) & \(}_{m}\) & 0.893 (0.003) & 1.000 (0.000) & 0.966 \\  \(*{argmax}^{}\) & \(}_{m}\) & 0.886 (0.003) & 1.018 (0.001) & 0.000 \\  

Table 1: Results on the Fashion MNIST data set. We display the frequency of returning the correct label as a singleton, \(_{}\) and the average size, \(_{}\), both of which are defined in (6). Standard errors for these averages are shown in parentheses. The final column shows the worst-case instability over the test set, \(_{}\), defined in (5). For each metric, the symbol \(\) indicates that higher values are desirable, while \(\) indicates that lower values are desirable.

We extend our experiment in Appendix E. In particular, we compare the inflated argmax to some existing methods for set-valued classification, including simple thresholding and top-\(K\) rules as well as more involved methods, all of which are Bayes rules for various utility functions . We also evaluate each method using some additional metrics from set-valued classification, including utility-discounted predictive accuracy .

## 5 Discussion

Related work.There is an enormous literature on set-valued classification, so we only reference some of the most relevant works. Much prior work has considered the possibility of a more relaxed argmax to allow for uncertainty . The more recent papers in this line of work have focused on producing a sparse set of weights, but none of these works offer a formal stability guarantee for the support of the weights. Our work is the first to propose a version of the argmax that can provably control a notion of stability for the classification setting.

Our definition of selection stability relies on a notion of consistency between sets--two sets of candidate labels are consistent if they have at least one common element. This is similar in flavor to conformal classification , where a set of candidate values is valid if it contains the correct label; this method does not rely on any distributional assumptions, and consequently has been applied to complex and high-dimensional applications such as generative language models . These frameworks share the motivation of avoiding 'overconfidence in the assignment of a definite label' .

Overview, limitations and open questions.We prove a guarantee on the selection stability based on our methodology combining bagging with the inflated argmax. Theorem 6 does not place any assumptions on the learning algorithm nor on the data, including any distributional assumptions. In fact, the training data and test point may be chosen adversarially based on the algorithm, and the output will still be stable. Moreover, we do not assume that the sample size is large: the guarantee holds for any fixed training set size \(n\) and improves as \(n\) increases. Furthermore, the inflated argmax selection rule ensures that the returned sets of candidate labels are as small as possible (i.e., that there is as little ambiguity as possible about the predicted label for any given test point \(x\)).

While our theorem does not require assumptions, our method does require bagging the base learning algorithm. The main limitation of our work is that bagging is computationally intensive. However, training different bags is easily parallelizable, which is what allowed us to easily train a convolutional neural network on \(B=1,000\) total subsets of the training data. Moreover, while the original definition of bagging uses the conventional bootstrap, where each bag contains as many samples as the original data set, i.e. \(m=n\), in our framework we allow for arbitrary bag size \(m\), which could be much smaller than the sample size \(n\). Massively subsampling the data (\(m n\)) can actually help scale learning algorithms to large data sets . Moreover, bagging with \(m=n\) can be expensive, but there are still many areas of machine learning where it is used, notably in Random Forests. Finally, our experiments also show that a modest number of bags (\(B=1,000\)) is all that we really need to start seeing major gains in selection stability.

We leave open several avenues for future work. Practitioners may be interested in other forms of discrete stability, which are more strict than requiring one common element between \(\) and \(^{ i}\). For instance, one popular way to measure set similarity is the Jaccard index, given by \(^{ i}|}{|^{ i}|}\). Another open problem is to extend the stability framework and methods studied here to more structured discrete outputs, such as in clustering and variable selection, where nontrivial metrics on the output are more common.