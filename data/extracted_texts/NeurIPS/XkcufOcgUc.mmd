# Structure-free Graph Condensation: From Large-scale Graphs to Condensed Graph-free Data

Xin Zheng\({}^{1}\), Miao Zhang\({}^{2}\), Chunyang Chen\({}^{1}\), Quoc Viet Hung Nguyen\({}^{3}\), Xingquan Zhu\({}^{4}\), Shirui Pan\({}^{3}\)

\({}^{1}\)Monash University, Australia, \({}^{2}\)Harbin Institute of Technology (Shenzhen), China

\({}^{3}\)Griffith University, Australia, \({}^{4}\)Florida Atlantic University, USA

xin.zheng@monash.edu, zhangmiao@hit.edu.cn, chunyang.chen@monash.edu

henry.nguyen@griffith.edu.au, xzhu3@fau.edu, s.pan@griffith.edu.au

Corresponding authorCode is available at https://github.com/Amanda-Zheng/SFGC

###### Abstract

Graph condensation, which reduces the size of a large-scale graph by synthesizing a small-scale condensed graph as its substitution, has immediate benefits for various graph learning tasks. However, existing graph condensation methods rely on the joint optimization of nodes and structures in the condensed graph, and overlook critical issues in effectiveness and generalization ability. In this paper, we advocate a new Structure-Free Graph Condensation paradigm, named SFGC, to distill a large-scale graph into a small-scale graph node set without explicit graph structures, _i.e._, graph-free data. Our idea is to implicitly encode topology structure information into the node attributes in the synthesized graph-free data, whose topology is reduced to an identity matrix. Specifically, SFGC contains two collaborative components: (1) a training trajectory meta-matching scheme for effectively synthesizing small-scale graph-free data; (2) a graph neural feature score metric for dynamically evaluating the quality of the condensed data. Through training trajectory meta-matching, SFGC aligns the long-term GNN learning behaviors between the large-scale graph and the condensed small-scale graph-free data, ensuring comprehensive and compact transfer of informative knowledge to the graph-free data. Afterward, the underlying condensed graph-free data would be dynamically evaluated with the graph neural feature score, which is a closed-form metric for ensuring the excellent expressiveness of the condensed graph-free data. Extensive experiments verify the superiority of SFGC across different condensation ratios.3

## 1 Introduction

As prevalent graph data learning models, graph neural networks (GNNs) have attracted much attention and achieved great success . Various graph data in the real world comprises millions of nodes and edges, reflecting diverse node attributes and complex structural connections . Modeling such large-scale graphs brings serious challenges in both data storage and GNN model designs, hindering the applications of GNNs in many industrial scenarios . For instance, designing GNN models usually requires repeatedly training GNNs for adjusting proper hyper-parameters and constructing optimal model architectures. When taking large-scale graphs as training data, repeated training through message passing along complex graph structures, makes it highly computation-intensive and time-consuming through try-and-error.

To address these challenges brought by the scale of graph data, a natural data-centric solution  is graph size reduction, which transforms the real-world large-scale graph to a small-scalegraph, such as graph sampling [66; 6], graph coreset [47; 60], graph sparsification [1; 5], and graph coarsening [3; 28]. These conventional methods either extract representative nodes and edges or preserve specific graph properties from the large-scale graphs, resulting in severe limitations of the obtained small-scale graphs in the following two folds. First, the available information on derived small-scale graphs is significantly upper-bounded and limited within the range of large-scale graphs [66; 60]. Second, the preserved properties of small-scale graphs, _e.g._, spectrum and clustering, might not always be optimal for training GNNs for downstream tasks [1; 3; 28].

In light of these limitations of conventional methods, in this work, we mainly focus on graph condensation [27; 26], a new rising synthetic method for graph size reduction. Concretely, graph condensation aims to directly optimize and synthesize a small-scale condensed graph, so that the small-scale condensed graph could achieve comparable test performance as the large-scale graph when training the same GNN model. Therefore, the principal goal of graph condensation is to ensure consistent test results for GNNs when taking the large-scale graph and the small-scale condensed graph as training data.

However, due to the structural characteristic of graph data, nodes and edges are tightly coupled. This makes condensing graph data a complicated task since high-quality condensed graphs are required to jointly synthesize discriminative node attributes and topology structures. Some recent works have made initial explorations of graph condensation [27; 26]. For instance, GCOND  proposed the online gradient matching schema between the synthesized small-scale graph and the large-scale graph, followed by a condensed graph structure learning module for synthesizing both condensed nodes and structures. However, existing methods overlook two-fold critical issues regarding **effectiveness and generalization ability**. First, graph condensation requires a triple-level optimization to jointly learn three objectives: GNN parameters, distilled node attributes, and topology structures. Such complex optimization cannot guarantee optimal solutions for both nodes and edges in the condensed graph, significantly limiting its effectiveness as the representative of the large-scale graph. Furthermore, existing online GNN gradients [27; 26] are calculated with the short-range matching, leading to the short-sight issue of failing to imitate holistic GNN learning behaviors, limiting the quality of condensed graphs. Second, existing condensed graphs generally show poor generalization ability across different GNN models [27; 26], because different GNN models vary in their convolution operations along graph structures. As a result, existing methods are vulnerable to overfitting of specific GNN architectures by distilling convolutional information into condensed graph structures.

To deal with the above two-fold challenges, in this work, we propose a novel Structure-Free Graph Condensation paradigm, named SFGC, to distill large-scale real-world graphs into small-scale synthetic graph node sets without graph structures, _i.e._, condensed graph-free data. Different from conventional graph condensation that synthesizes both nodes and structures to derive a small-scale graph, as shown in Fig. 1, the proposed structure-free graph condensation only synthesizes a small-scaled node set to train a GNN/MLP, when it implicitly encodes topology structure information into the node attributes in the synthesized graph-free data, by simplifying the condensed topology to an identity matrix. Overall, the proposed SFGC contains two essential components: (1) a training trajectory meta-matching scheme for effectively synthesizing small-scale graph-free data; (2) a graph neural feature score metric for dynamically evaluating the quality of condensed graph-free data. To address the short-sight issue of existing online gradient matching, our training trajectory meta-matching scheme first trains a set of training trajectories of GNNs on the large-scale graph to acquire an expert parameter distribution, which serves as offline guidance for optimizing the condensed graph-free data. Then, the proposed SFGC conducts meta-matching to align the long-term GNN learning behaviors between the large-scale graph and condensed graph-free data by sampling from the training trajectory distribution, enabling the comprehensive and compact transfer of informative knowledge to the graph-free data. At each meta-matching step, we would obtain updated condensed graph-free data, which would be fed into the proposed graph neural feature score metric for dynamically evaluating its quality. This metric is derived based on the closed-form solutions of GNNs under the graph neural tangent kernel (GNTK) ridge regression, eliminating the iterative training

Figure 1: Comparisons of condensation _vs_ structure-free condensation on graphs.

of GNNs in the dynamic evaluation. Finally, the proposed SFGC selects the condensed graph-free data with the smallest score as the optimal representative of the large-scale graph. Our proposed structure-free graph condensation method could benefit many potential application scenarios, such as, _graph neural architecture search_[79; 81], _privacy protection_, _adversarial robustness_[67; 70], _continual learning_, and so on. We provide detailed demonstrations of how our method facilitates the development of these areas in Appendix B

In summary, the contributions of this work are listed as follows:

* We propose a novel Structure-Free Graph Condensation paradigm to effectively distill large-scale real-world graphs to small-scale synthetic graph-free data with superior expressiveness, to the best of our knowledge, for the first time.
* To explicitly imitate the holistic GNN training process, we propose the training trajectory meta-matching scheme, which aligns the long-term GNN learning behaviors between the large-scale graph and the condensed graph-free data, with the theoretical guarantee of eliminating graph structure constraints.
* To ensure the high quality of the condensed data, we derive a GNTK-based graph neural feature score metric, which dynamically evaluates the small-scale graph-free data at each meta-matching step and selects the optimal one. Extensive experiments verify the superiority of our method.

**Prior Works.** Our research falls into the research topic _dataset distillation (condensation)_[30; 59], which aims to synthesize a small typical dataset that distills the most important knowledge from a given large target dataset as its effective substitution. Considering most of the works condense image data [59; 39; 77; 76; 4], due to the complexity of graph structural data, only a few works [27; 26] address graph condensation, while our research designs a new structure-free graph condensation paradigm for addressing the effectiveness and generalization ability issues in existing graph condensation works. Our research also significantly differs from other general graph size reduction methods, for instance, graph coreset [47; 60], graph sparsification [1; 5] and so on. More detailed discussions about related works can be found in Appendix A.

## 2 Structure-Free Graph Condensation

### Preliminaries

**Notations.** Denote a large-scale graph dataset to be condensed by \(=(,,)\), where \(^{N d}\) denotes \(N\) number of nodes with \(d\)-dimensional features, \(^{N N}\) denotes the adjacency matrix indicating the edge connections, and \(^{N C}\) denotes the \(C\)-classes of node labels. In general, graph condensation synthesizes a small-scale graph dataset denoted as \(^{}=(^{},^{},^{ })\) with \(^{}^{N^{} d}\), \(^{}^{N^{} N^{}}\), and \(^{}^{N^{} C}\) when \(N^{} N\). In this work, we propose the structure-free graph condensation paradigm, which aims to synthesize a small-scale graph node set \(=(},})\) without explicitly condensing graph structures, _i.e._, the condensed graph-free data, as an effective substitution of the given large-scale graph. Hence, \(}\) contains joint node context attributes and topology structure information, which is a more compact representative compared with \((^{},^{})\).

**Graph Condensation.** Given a GNN model parameterized by \(\), graph condensation  is defined to solve the following triple-level optimization objective by taking \(=(,,)\) as input:

\[_{^{}}& [_{_{^{}}}(, ),]\\ s.t.&_{^{}}= *{arg\,min}_{}[_{ }(^{},^{}), ^{}],\\ &_{^{}}=*{arg\,min}_ {}[_{}( ^{})],\] (1)

where \(_{}\) is a submodule parameterized by \(\) to synthesize the graph structure \(^{}\). One of inner loops learns the optimal GNN parameters \(_{^{}}\), while another learns the optimal \(\) parameters \(_{^{}}\) to obtain the condensed \(^{}\), and the outer loop updates the condensed nodes \(^{}\). All these comprise the condensed small-scale graph \(^{}=(^{},^{},^{ })\), where \(^{}\) is pre-defined based on the class distribution of the label space \(\) in the large-scale graph.

Overall, the above optimization objective needs to solve the following variables iteratively: (1) condensed \(^{}\); (2) condensed \(^{}\) with \(_{_{^{}}}\); and (3) \(_{_{^{}}}\). Jointly learning these interdependent oach a complex and nested optimization process, resulting in the limited expressiveness of the condensed graph. This dilemma motivates us to reconsider the optimization objective of graph condensation to synthesize the condensed graph more effectively.

**Graph Neural Tangent Kernel (GNTK).** As a new class of graph kernels, graph neural tangent kernel (GNTK) is easy to train with provable theoretical guarantees, and meanwhile, enjoys the full expressive power of GNNs [10; 19; 21; 41]. In general, GNTK can be taken as the infinitely-wide multi-layer GNNs trained by gradient descent. It learns a class of smooth functions on graphs with close-form solutions. More specifically, let \(G=(V,E)\) denote a graph with nodes \(V\) and edges \(E\), where each node \(v V\) within its neighbor set \((v)\). Given two graphs \(G=(V,E)\) and \(G^{}=(V^{},E^{})\) with \(n\) and \(n^{}\) number of nodes, their covariance matrix between input features can be denoted as \(^{(0)}(G,G^{})^{n n^{}}\). Each element in \([^{(0)}(G,G^{})]_{uy^{}}\) is the inner product \(_{u}^{}_{u^{}}\), where \(_{u}\) and \(_{u^{}}\) are of input features of two nodes \(u V\) and \(u^{} V^{}\). Then, for each GNN layer \(\{0,1,,L\}\) that has \(\) fully-connected layers with ReLU activation, GNTK calculates \(}_{()}^{()} G,G^{}\) for each \([]\):

\[[}_{()}^{()} G,G^{}]_{uu^{}}&=[}_{(-1)}^{()} G,G^{} ]_{uu^{}}[}_{()}^{()}(G,G^{ })]_{uu^{}}\\ &+[_{()}^{()}(G,G^{}) ]_{uu^{}},\] (2)

where \(^{()}\) denotes the derivative _w.r.t._ the \(\)-th GNN layer of the covariance matrix, and the \((+1)\)-th layer's covariance matrix aggregates neighbors along graph structures as \([_{(0)}^{(+1)}(G,G^{})]_{uu^{ }}=_{v(u)\{u\}}_{v^{}(u^ {})\{u^{}\}}[_{()}^{()}(G,G^{})]_{vv^{}}\), ditto for the kernel \([}_{(0)}^{(+1)}(G,G^{})]_{uu^{ }}\). With the GNTK matrix \(}_{()}^{(L)} G,G^{} ^{n n^{}}\) at the node level, we use the graph kernel method to solve the equivalent GNN model for node classification with closed-form solutions. This would significantly benefit the efficiency of condensed data evaluation by eliminating iterative GNN training.

### Overview of SFGC Framework

The crux of achieving structure-free graph condensation is in determining discriminative node attribute contexts, which implicitly integrates topology structure information. We compare the paradigms between existing graph condensation (GC) and our new structure-free condensation SFGC as follows:

\[&=(,, )^{}=(^{},^{ },^{}),.\\ &=(,,) =(},,})==(},}), .\] (3)

Figure 2: Overall pipeline of the proposed Structure-Free Graph Condensation (SFGC) framework.

[MISSING_PAGE_FAIL:5]

distribution \(P_{_{}}\), which can be taken as a'meta' way to make the distilled dataset \(\) adapt different parameter initialization. That is why we call it'meta-matching'. In this way, when initializing \(_{}\) and \(_{}\) with the same model parameters, Eq. (5) contributes to aligning the learning behaviors of \(_{}\) that experiences \(p\)-steps optimization, to \(_{}\) that experiences \(q\)-steps optimization. In this way, the proposed training trajectory meta-matching schema could comprehensively imitate the long-term learning behavior of GNN training. As a result, the informative knowledge of the large-scale graph \(\) can be effectively transferred to the small-scale condensed graph-free data \(=(},})\) in the above outer-loop optimization objective of Eq. (4).

For the inner loop, we train \(_{}\) on the synthesized small-scale condensed graph-free data for optimizing its model parameter until the optimal \(}_{}^{s}\). Therefore, the final optimization objective of the proposed SFGC is

\[&_{}_{ _{}^{*} P_{_{}}}[_ {}(_{t}^{*}|_{t=t_{0}}^{p},}_{t}|_{t=t_{0}}^{q})],\\ & s.t.}_{}^{*}= *{arg\,min}_{}}_{}[_{}}( )],\] (6)

where \(_{}\) is the node classification loss calculated with the cross-entropy on graphs. Compared with the triple-level optimization in Eq. (1), the proposed SFGC directly replaces the learnable \(^{}\) in Eq. (1) with a fixed identity matrix \(\), resulting in the condensed structure-free graph data \(=(},,})\). Without synthesizing condensed graph structures with \(_{}\),the proposed SFGC refines the complex triple-level optimization to the bi-level one, ensuring effectiveness of the condensed graph-free data.

Hence, the advances of the training trajectory meta-matching schema in the proposed SFGC can be summarized as follows: (1) compared with the online gradient calculation, SFGC's offline parameter sampling avoids dynamically computing and storing gradients of both the large and condensed small graphs, reducing computation and memory costs during the condensation process; (2) compared with short-range matching, SFGC's long-term meta-matching avoids condensed data to short-sightedly fit certain optimization steps, contributing to a more holistic and comprehensive way to imitate GNN's learning behaviors.

### Graph Neural Feature Score

For each update of the outer loop in Eq. (6), we would synthesize the brand-new condensed graph-free data. However, evaluating the quality of the underlying condensed graph-free data in the dynamical meta-matching condensation process is quite challenging. That is because we cannot quantity a graph dataset's performance without blending it in a GNN model. And the condensed graph-free data itself cannot be measured by convergence or decision boundary. Generally, to evaluate the condensed graph-free data, we use it to train a GNN model. If the condensed data at a certain meta-matching step achieves better GNN test performance on node classification, it indicates the higher quality of the current condensed data. That means, evaluating condensed graph-free data needs an extra process of training a GNN model from scratch, leading to much more time and computation costs.

In light of this, we aim to derive a metric to dynamically evaluate the condensed graph-free data in the meta-matching process, and utilize it to select the optimal small-scale graph-free data. Meanwhile, the evaluation progress would not introduce extra GNN iterative training for saving computation and time. To achieve this goal, we first identify what characteristics such a metric should have: (1) closed-form solutions of GNNs to avoid iterative training in evaluation; (2) the ability to build strong connections between the large-scale graph and the small-scale synthesized graph-free data. In this case, the graph neural tangent kernel (GNTK) stands out, as a typical class of graph kernels, and has the full expressive power of GNNs with provable closed-form solutions. Moreover, as shown in Eq. (2), GNTK naturally builds connections between arbitrary two graphs even with different sizes.

Based on the graph kernel method with GNTK, we proposed a graph neural feature score metric \(_{}\) to dynamically evaluate and select the optimal condensed graph-free data as follows:

\[_{}()=\|_{}- }_{}, (},+)^{-1}}\|^{2},\] (7)

where \(}_{}, ^{N_{} N^{}}\) and \(}, ^{N^{} N^{}}\) denote the node-level GNTK matrices derived according to Eq. (2). And \(_{}\) is the validation sub-graph of the large-scale graph with \(N_{}\) numbers of nodes. Concretely, \(_{}()\) calculates the graph neural tangent kernel based ridge regression error. It measures that, given an infinitely-wide GNN trained on the condensed graph \(\) with ridge regression, how close such GNN's prediction on \(_{}\) to its ground truth labels \(_{}\). Note that Eq. (7) can be regarded as the Kernel Inducing Point (KIP) algorithm [39; 40] adapted to the GNTK kernel on GNN models.

Hence, the proposed graph neural feature score meets the above-mentioned characteristics as: (1) it calculates a closed-form GNTK-based ridge regression error for evaluation without iteratively training GNN models; (2) it strongly connects the condensed graph-free data with the large-scale validation graph; In summary, the overall algorithm of the proposed SFGC is presented in Algo. 1.

## 3 Experiments

### Experimental Settings

**Datasets.** Following , we evaluate the node classification performance of the proposed SFGC method on Cora, Citeseer , and Ogbn-arxiv  under the transductive setting, on Flickr  and Reddit  under the inductive setting. For all datasets under two settings, we use the public splits and setups for fair comparisons. We consider three condensation ratios (\(r\)) for each dataset. Concretely, \(r\) is the ratio of condensed node numbers \(rN(0<r<1)\) to large-scale node numbers \(N\). In the transductive setting, \(N\) represents the number of nodes in the entire large-scale graph, while in the inductive setting, \(N\) indicates the number of nodes in the training sub-graph of the whole large-scale graph. The dataset statistic details are shown in Appendix C.

**Baselines & Implementations.** We adopt the following baselines for comprehensive comparisons : graph coarsening method , graph coreset methods, _i.e._, Random, Herding , and K-Center , the graph-based variant DC-Graph of general dataset condensation method DC , which is introduced by , graph dataset condensation method GCOND  and its variant GCOND-X without utilizing the graph structure. The whole pipeline of our experimental evaluation can be divided into two stages: (1) the condensation stage: synthesizing condensed graph-free data, where we use the classical and commonly-used GCN model ; (2) the condensed graph-free data test stage: training a certain GNN model (default with GCN) by the obtained condensed graph-free data from the first stage and testing the GNN on the test set of the large-scale graph with repeated 10 times. We report the average transductive and inductive node classification accuracy (ACC%) with standard deviation (std). Following , we use the two-layer GNN with 256 hidden units as the defaulted setting. Besides, we adopt the K-center  features to initialize our condensed node attributes for stabilizing the training process. Additional hyper-parameter setting details are listed in Appendix E.

### Experimental Results

**Performance of SFGC on Node Classification.** We compare the node classification performance between SFGC and other graph size reduction methods, especially the graph condensation methods. The overall performance comparison is listed in Table 1. Generally, SFGC achieves the best performance on the node classification task with 13 of 15 cases (five datasets and three condensation ratios for each of them), compared with all other baseline methods, illustrating the high quality and expressiveness of the condensed graph-free data synthesized by our SFGC. More specifically, the better performance of SFGC than GCOND and its structure-free variant GCOND-X experimentally verifies the superiority of the proposed method. We attribute such superiority to the following two aspects regarding the condensation stage. First, the long-term parameter distribution matching of our SFGC works better than the short-term gradient matching in GCOND and GCOND-X. That means capturing the long-range GNN learning behaviors facilitates to holistically imitate GNN's training process, leading to comprehensive knowledge transfer from the large-scale graph to the small-scale condensed graph-free data. Second, the structure-free paradigm of our SFGC enables more compact small-scale graph-free data. For one thing, it liberates the optimization process from triple-level objectives, alleviating the complexity and difficulty of condensation. For another thing, the obtained optimal condensed graph-free data compactly integrates node attribute contexts and topology structure information. Furthermore, on Cora and Citeseer, SFGC synthesizes better condensed graph-free data that even exceeds the whole large-scale graph dataset. These results confirm that SFGC is able to break the information limitation under the large-scale graph and effectively synthesize new, small-scale graph-free data as an optimal representation of the large-scale graph.

**Effectiveness of Structure-free Paradigm in SFGC.** The proposed SFGC introduces the structure-free paradigm without condensing graph structures in graph condensation. To verify the effectiveness of the structure-free paradigm, we compare the proposed SFGC with its variants, which synthesize graph structures in the condensation process. Specifically, we evaluate the following three different methods of synthesizing graph structures with five variants of SFGC: (1) discrete \(k\)-nearest neighbor (\(k\)NN) structures calculated by condensed node features under \(k=(1,2,5)\), corresponding to the variants SFGC-d1, SFGC-d2, and SFGC-d5; (2) cosine similarity based continuous graph structures

    &  &  &  &  \\  & & & & & & & & & & \\   & 0.9\% & 52.2\(\)0.4 & 54.4\(\)4.4 & 57.1\(\)5.1 & 52.4\(\)0.8 & 66.8\(\)1.5 & 71.4\(\)0.8 & 70.5\(\)1.2 & **71.4\(\)0.5** & \\  & 1.8\% & 59.9\(\)0.5 & 64.2\(\)2.7 & 66.7\(\)1.6 & 64.3\(\)0.1 & 66.9\(\)0.9 & 68.1\(\)1.1 & 70.6\(\)0.5 & **72.4\(\)0.4** & 71.7\(\)0.1 \\  & 3.6\% & 65.3\(\)0.3 & 69.1\(\)0.1 & 69.0\(\)0.1 & 69.1\(\)1.1 & 66.3\(\)1.5 & 69.4\(\)1.4 & 68.5\(\)1.4 & **70.6\(\)0.7** & \\   & 1.3\% & 312.4\(\)0.2 & 63.6\(\)1.7 & 67.0\(\)1.3 & 64.0\(\)2.3 & 67.3\(\)1.0 & 75.9\(\)1.2 & 79.8\(\)1.3 & **80.1\(\)0.4** & \\  & 2.6\% & 65.2\(\)0.0 & 72.8\(\)1.3 & 75.4\(\)1.0 & 75.2\(\)1.2 & 67.6\(\)1.5 & 75.2\(\)0.0 & 80.1\(\)0.4 & **81.7\(\)0.5** & 81.2\(\)0.2 \\  & 5.2\% & 70.0\(\)0.1 & 76.8\(\)0.1 & 76.0\(\)0.1 & 67.2\(\)0.1 & 67.7\(\)0.2 & 76.0\(\)0.0 & 75.3\(\)0.3 & 83.6\(\)0.6 & \\   & 0.05\% & 35.4\(\)0.3 & 47.1\(\)1.9 & 52.4\(\)1.5 & 47.2\(\)1.0 & 58.6\(\)0.4 & 61.3\(\)0.5 & 59.2\(\)1.1 & **65.5\(\)0.7** & \\  & 0.25\% & 43.5\(\)0.2 & 57.3\(\)1.1 & 66.2\(\)1.5 & 58.6\(\)1.5 & 59.5\(\)0.5 & 64.2\(\)0.6 & 63.2\(\)0.3 & **66.3\(\)0.4** & 71.4\(\)0.1 \\  & 0.5\% & 50.4\(\)1.1 & 60.0\(\)0.0 & 60.0\(\)0.0 & 60.0\(\)0.0 & 39.4\(\)0.3 & 59.3\(\)0.3 & 63.0\(\)0.3 & 64.0\(\)0.4 & **66.8\(\)0.4** & \\   & 0.1\% & 41.9\(\)0.2 & 41.8\(\)0.2 & 42.5\(\)1.5 & 42.0\(\)1.0 & 46.3\(\)0.2 & 45.9\(\)1.0 & 45.0\(\)0.1 & 46.5\(\)0.4 & 46.0\(\)0.2 & \\  & 0.5\% & 44.5\(\)0.1 & 44.0\(\)0.1 & 43.9\(\)0.0 & 43.2\(\)1.0 & 45.9\(\)1.0 & 45.0\(\)0.2 & **47.1\(\)0.1** & 47.2\(\)0.1 & 47.2\(\)0.1 \\  & 1\% & 44.6\(\)0.1 & 44.6\(\)0.2 & 44.4\(\)0.0 & 44.1\(\)0.0 & 45.8\(\)0.1 & 45.0\(\)0.1 & **47.1\(\)0.1** & **47.1\(\)0.1 & \\   & 0.05\% & 40.9\(\)0.5 & 46.1\(\)1.4 & 53.1\(\)2.5 & 46.6\(\)2.3 & 88.2\(\)0.2 & 85.4\(\)0.4 & 80.8\(\)0.1 & **89.7\(\)0.2** & **89.7\(\)0.2 \\  & 0.1\% & 42.8\(\)0.8 & 58.0\(\)0.2 & 62.7\(\)1.0 & 53.0\(\)3.3 & 89.5\(\)0.1 & 89.3\(\)0.1 & 89.6\(\)0.7 & **90.0\(\)0.3** & 93.9\(\)0.4 \\  & 0.2\% & 47.4\(\)0.9 & 66.3\(\)1.9 & 71.0\(\)1.6 & 58.5\(\)1.2 & **90.5\(\)1.2** & 88.8\(\)0.0 & 90.1\(\)0.0 & 89.9\(\)0.4 & \\   

Table 1: Node classification performance (ACC%\(\)std) comparison between condensation methods and other graph size reduction methods with different condensation ratios. (Best results are in bold, and the second-sets are underlined.)

Figure 3: Comparisons between five variants of synthesizing graph structures _vs._ our structure-free SFGC (discrete \(k\)-nearest neighbor (\(k\)NN) structure variants: SFGC-d1 (\(k=1\)), SFGC-d2 (\(k=2\)), and SFGC-d5 (\(k=5\)), continuous graph structure variant: SFGC-c, parameterized graph structure variant: SFGC-p).

calculated by condensed node features, corresponding to the variant SFGC-c; (3) parameterized graph structure learning module with condensed node features adapted by , corresponding to the variant SFGC-p. We conduct experiments on three transductive datasets under nine condensation ratios for each graph structure synthesis variant and the proposed SFGC. The results are presented in Fig. 3. In general, the proposed SFGC achieves the best performance over various graph structure synthesis methods, and these results empirically verify the effectiveness of the proposed structure-free condensation paradigm. More specifically, for discrete \(k\)-nearest neighbor (\(k\)NN) structure variants, different datasets adapt different numbers of \(k\)-nearest neighbors under different condensation ratios, which means predefining the value of \(k\) can be very challenging. For example, Citeseer dataset has better performance with \(k=1\) in SFGC-d1 under \(r=0.9\%\) than SFGC-d2 and SFGC-d5, but under \(r=1.8\%\), \(k=2\) in SFGC-d2 performs better than others two. Besides, for continuous graph structure variant SFGC-c, it generally cannot exceed the discrete graph structure variants, except for Ogbn-arxiv dataset under \(r=0.05\%\). And the parameterized variant SFGC-p almost fails to synthesize satisfied condensed graphs under the training trajectory meta-matching scheme. The superior performance of SFGC to all structure-based methods demonstrates the effectiveness of its structure-free paradigm.

**Effectiveness of Graph Neural Feature Score in SFGC.** We compare the learning time between GNN iterative training _vs._ our proposed GNTK-based closed-form solutions of \(_{}\). Note that the iterative training evaluation strategy mandates the complete training of a GNN model from scratch at each meta-matching step, hence, we calculate its time that covers all training epochs under the best test performance for fair comparisons. Typically, for Flickr dataset (\(r=0.1\%\)), our proposed \(_{}\) based GNTK closed-form solutions takes only 0.015s for dynamic evaluation, which significantly outperforms the iterative training evaluation with 0.845s. The superior performance can also be observed in Ogbn-arxiv dataset (\(r=0.05\%\)) with 0.042s of our \(_{}\), compared with 4.264s of iterative training, illustrating our SFGC's high dynamic evaluation efficiency. More results and analysis of our proposed \(_{}\) in GNTK-based closed-form solutions can be found in Appendix.

**Generalization Ability of SFGC across Different GNNs.** We evaluate and compare the generalization ability of the proposed SFGC and other graph condensation methods. Concretely, we test the node classification performance of our synthesized graph-free data (condensed on GCN) with seven different GNN architectures: MLP, GAT , APPNP , Cheby , GCN , SAGE , and SGC . It can be generally observed that the proposed SFGC achieves outstanding performance over all tested GNN architectures, reflecting its excellent generalization ability. This is because our method reduces the graph structure to the identity matrix, so that the condensed graph node set can no longer be influenced by different convolution operations of GNNs along graph structures, enabling it consistent and good performance with various GNNs.

More experimental analysis and discussions, including the effects of different ranges of long-term meta-matching, the performance on downstream unsupervised graph clustering task, visualization of our condensed structure-free node set, as well as time complexity analysis, are detailed in Appendix E.

   Datasets (Ratio) & Methods & MLP & GAT  & APPNP  & Cheby (7) & GCN  & SAGE  & SGC  & Arg. \\   & DC-Graph  & 66.2 & - & 66.4 & 64.9 & 66.2 & 65.9 & 69.6 & 66.6 \\  & GCND-X  & 69.6 & - & 69.7 & 70.6 & 69.7 & 69.2 & 71.6 & 70.2 \\  & GCND  & 63.9 & 55.4 & 69.6 & 68.3 & 70.5 & 66.2 & 70.3 & 69.0 \\  & SPG (**curcurxiv**) & **71.3** & **72.1** & **70.5** & **71.8** & **71.6** & **71.7** & **71.8** & **71.5** \\   & DC-Graph  & 67.2 & - & 67.1 & 67.7 & 67.9 & 66.2 & 72.8 & 68.3 \\  & GCND-X  & 76.0 & - & 77.0 & 74.1 & 75.3 & 76.0 & 76.1 & 75.7 \\  & GCND  & 73.1 & 66.2 & 78.5 & 76.0 & 80.1 & 78.2 & 79.3 & 78.4 \\  & SFGC (**ours**) & **81.1** & **80.8** & **78.8** & **79.0** & **81.1** & **81.9** & **79.1** & **80.3** \\   & DC-Graph  & 59.9 & - & 60.0 & 55.7 & 59.8 & 60.0 & 60.4 & 59.2 \\  & GCND-X  & 64.1 & - & 61.5 & 59.5 & 64.2 & 64.4 & 64.7 & 62.9 \\  & GCND  & 62.2 & 60.0 & 63.4 & 54.9 & 63.2 & 62.6 & 63.7 & 61.6 \\  & SPG (**ours**) & **65.1** & **65.7** & **63.9** & **60.7** & **65.1** & **64.8** & **64.8** & **64.3** \\   & DC-Graph  & 43.1 & - & 45.7 & 43.8 & 45.9 & 45.8 & 45.6 & 45.4 \\  & GCND-X  & 42.1 & - & 44.6 & 42.3 & 45.0 & 44.7 & 44.4 & 44.2 \\  & GCND  & 44.8 & 40.1 & **45.9** & 42.8 & **47.1** & 46.2 & **46.1** & **45.6** \\  & SPG (**ours**) & **47.1** & **45.3** & - & **40.7** & **45.4** & **47.1** & **47.0** & 42.5 & 45.0 \\   & DC-Graph  & 50.3 & - & 81.2 & 77.5 & 89.5 & 89.7 & 90.5 & 85.7 \\  & GCND-X  & 40.1 & - & 78.7 & 74.0 & 89.3 & 89.3 & **91.0** & 84.5 \\   & GCND  & 42.5 & 60.2 & 87.8 & 75.5 & 89.4 & 89.1 & 89.6 & 86.3 \\   & SPG (**ours**) & **89.5** & **87.1** & **88.3** & **82.8** & **89.7** & **90.3** & 89.5 & **88.2** \\   

Table 2: Performance across different GNN architectures.

Conclusion

This work proposes a novel Structure-Free Graph Condensation paradigm, named SFGC, to distill the large-scale graph into the small-scale graph-free node set without graph structures. Under the structure-free learning paradigm, the training trajectory meta-matching scheme and the graph neural feature score measured dynamic evaluation work collaboratively to synthesize small-scale graph-free data with superior effectiveness and good generalization ability. Extensive experimental results and analysis under large condensation ratios confirm the superiority of the proposed SFGC method in synthesizing excellent small-scale graph-free data. It can be anticipated that our work would bridge the gap between academic GNNs and industrial MLPs by synthesizing small-scale, graph-free data to address graph data scalability, while retaining the expressive performance of graph learning. Our method works on condensing the number of nodes in a single graph at the node level, and we will explore extending it to condense the number of graphs in a graph set at the graph level in the future. We will also explore the potential of unifying graphs and large language models  for the graph condensation task.