# Scene Graph Generation with Role-Playing Large Language Models

Guikun Chen\({}^{1}\)1,  Jin Li\({}^{3*}\),  Wenguan Wang\({}^{1,2}\)2

\({}^{1}\)Zhejiang University

\({}^{2}\)National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Xi'an Jiaotong University

\({}^{3}\)Changsha University of Science & Technology

https://github.com/guikunchen/SDSGG

###### Abstract

Current approaches for open-vocabulary scene graph generation (OVSGG) use vision-language models such as CLIP and follow a standard zero-shot pipeline - computing similarity between the query image and the text embeddings for each category (, text classifiers). In this work, we argue that the text classifiers adopted by existing OVSGG methods,, category-/part-level prompts, are _scene-agnostic_ as they remain unchanged across contexts. Using such fixed text classifiers not only struggles to model visual relations with high variance, but also falls short in adapting to distinct contexts. To plug these intrinsic shortcomings, we devise SDSGG, a _scene-specific_ description based OVSGG framework where the weights of text classifiers are adaptively adjusted according to the visual content. In particular, to generate comprehensive and diverse descriptions oriented to the scene, an LLM is asked to play different roles (, biologist and engineer) to analyze and discuss the descriptive features of a given scene from different views. Unlike previous efforts simply treating the generated descriptions as _mutually equivalent_ text classifiers, SDSGG is equipped with an advanced _renormalization_ mechanism to adjust the influence of each text classifier based on its relevance to the presented scene (this is what the term "_specific_" means). Furthermore, to capture the complicated interplay between subjects and objects, we propose a new lightweight module called mutual visual adapter. It refines CLIP's ability to recognize relations by learning an interaction-aware semantic space. Extensive experiments on prevalent benchmarks show that SDSGG outperforms top-leading methods by a clear margin.

## 1 Introduction

SGG  aims to create a structured representation of an image by identifying objects as nodes and their relations as edges within a graph. The emerging field of OVSGG , which broadens the scope of SGG to identify and associate objects beyond a predefined set of categories, has become a research hotspot for its prospective to amplify the practicality of SGG in diverse real-world applications.

OVSGG has achieved notable progress due to the success of vision-language models (VLMs)  and prompt learning . Existing OVSGG methods adopt a standard zero-shot pipeline , which computes similarity between the visual embedding from query image and the text embedding from pre-defined text classifiers (. Fig. 1a). One straightforward direction for OVSGG is to use only the category name (, "riding")  as the text classifier and perform vision-languagealignment as in prompt learning to learn the underlying patterns. On the other hand, [11; 12] argue that such methods fail to utilize the rich context of additional information that language can provide. To address this,  decomposes relation detection into several _separate_ components, computing similarities by checking how well the visual features of the object match the part-level descriptions1. As shown in Fig. 1b, the object of relation "riding" should have four legs, a saddle, _etc_.

Despite these technological advances, we still observe current OVSGG systems lack in-depth inspection of the expressive range of the used text classifiers, which puts a performance cap on them. Concretely, OVSGG models that rely on _scene2-agnostic_ text classifiers have the following flaws.

**First**, methods based on category names [2; 3] struggle to model the large variance in visual relations. Using only category names as classifiers  does hold water when applied to object recognition . For instance, CLIP shares common visual features across diverse image-text pairs of tigers which encompasses a variety of tiger appearances and corresponding descriptions. Nonetheless, the scenario becomes far more complex when it comes to relation detection. The visual features that define the relation "on" can vary dramatically across scenes, _e.g._, "dog on bed" _vs._ "people on road". RECODE  proposes to decompose relation detection into recognizing part-level descriptions for both subject and object, hence partially easing the aforementioned difficulty. Yet, it computes similarities for the subject and object _separately_ and does not model the _interplay_ between subjects and objects. **Second**, part-level prompt based methods uniformly process all descriptions as affirmative classifiers [11; 12], overlooking the possibility that some text classifiers might be contrary to specific contexts. When querying LLMs for distinctive visual features of subjects and objects to distinguish the predicate "riding", with the subject as "human" and the object as "horse", LLMs provide part-level descriptions of the expected appearance of both entities. All generated descriptions are treated equally as definitive text classifiers. However, these descriptions could potentially be misleading, as LLMs produce them without considering the specific context, even resulting in some descriptions that are wholly irrelevant to the presented image. For example, LLMs typically associate the predicate "riding" with the animal

Figure 1: Illustration of the used text classifiers in OVSGG. (a) CLIP performs zero-shot classification by computing similarity between the query image and the text embeddings for each category, then choosing the highest. (b) To further utilize the learned semantic space of CLIP, one can compute similarities of multiple part-level prompts (_e.g._, the object of \(\)man, riding, horse\(\) may be described with “with four legs” and “with a saddle”). (c) Instead of using these _scene-agnostic_ text classifiers, SDSGG adopts comprehensive, _scene-specific_ descriptions generated by LLMs, which can adapt to specific contexts by using the proposed renormalization.

"with four legs". Nonetheless, such associations are indeed irrelevant, as an animal's legs are not always visible in the presented image, or the animal may have only two legs.

Filling the gaps identified above calls for a fundamental paradigm shift: moving from the fixed, _scene-agnostic_ (_i.e._, category-/part-level) text classifiers towards flexible, _scene-specific_ ones. In light of this, we develop SDSGG, a scene-specific description based OVSGG framework that utilizes text classifiers generated from LLMs, complemented by a renormalization technique, to understand scenes from different perspectives. **For the textual part**, given a scene with specified content, an LLM is assigned distinct roles, akin to experts specializing in biology, physics, and engineering, to analyze descriptive scene features comprehensively (_c.f._ Fig. 1c). Such a multi-person scheme is designed to improve the diversity of the generated scene descriptions as LLMs tend to generate repetitive content. LLM can be queried multiple times to obtain a large number of scene descriptions. Moreover, since not all descriptions are relevant to the presented image (_e.g._, some parts of the object may not appear), SDSGG is equipped with an advanced mechanism that renormalizes each scene description via opposite descriptions corresponding to the original descriptions. This involves evaluating two vision-language similarities: one for the original scene description and the other for its opposite. The _difference_ between the two similarities is viewed as the _self-normalized_ similarity of the scene description, allowing for flexible control over its influence. For instance, an irrelevant description would yield a _self-normalized_ similarity close to zero, as the two similarity scores of it and its opposite would be very close. By doing so, the generated scene-level descriptions become flexible, _scene-specific_ descriptions (SSDs). **For the visual part**, we propose a new adapter for relation detection, called mutual visual adapter, which consists of several lightweight learnable modules. The proposed adapter projects CLIP's semantic embeddings into another interaction-aware space, modeling the complicated interplay between the subject and object through cross-attention.

With the proposed adaptive SSDs, our SDSGG is capable of: **i)** adapting to the given context via evaluating the _self-normalized_ similarity of each SSD; **ii)** alleviating the overfitting problem in OVSGG models [2; 3] that use only one classifier; and **iii)** naturally generalizing to novel relations by associating them with SSDs. We validate SDSGG on two widely-used benchmarks, _i.e._ Visual Genome (VG)  and GQA . Experimental results show that SDSGG outperforms existing OVSGG methods  by a large margin. The strong generalization and promising performance of SDSGG evidence the great potential of _scene-specific_ description based relation detection.

## 2 Related Work

**Scene Graph Generation.** Since  introduces iterative message passing for SGG, research studies in structured visual scene understanding have witnessed phenomenal growth. Tremendous progress has been achieved and can be categorized into: **i)** Two-stage SGG [16; 17; 18; 19; 20], which first detects all objects in the images and then recognizes the pairwise relations between them; **ii)** Debiased SGG [21; 22; 23; 24; 25; 26; 27; 28], which focuses on the problem of long-tailed predicate distribution in the current dataset; **iii)** Weakly-supervised SGG [29; 30; 31; 32; 33], which investigates how to generate scene graph with only image-level supervision; **iv)** One-stage SGG [34; 35; 36; 37; 38; 39], which implements SGG within an end-to-end framework (also exemplified in other relation detection tasks [40; 41; 42]), discarding several hand-crafted procedures; **v)** Open-vocabulary SGG, which learns to recognize unseen categories during training by using category-level [2; 8; 9; 3; 10] or part-level prompts . **vi)** Few-show SGG, which learns to recognize relations given a few examples .

Existing OVSGG frameworks adopt a standard open-vocabulary learning paradigm, _i.e._, perform vision-language alignment in the pre-trained or random initialized semantic space with supervision of only the category names. One except  reformulates OVSGG from recognizing category-level prompts into recognizing part-level prompts, by decomposing SGG into several separate components and computing their similarities independently, in a training-free manner. SDSGG represents the best of both worlds. On the one hand, we point out the drawbacks of the commonly used _scene-agnostic_ text classifiers and introduce _scene-specific_ alternates to understand scenes from different perspectives. On the other hand, SDSGG incorporates a learnable mutual visual adapter to capture the underlying patterns in the dataset and proposes to renormalize text classifiers for adapting to specific contexts.

**Open-vocabulary Learning.** Most deep neural networks operate on the close-set assumption, which can only identify pre-defined categories that are present in the training set. Early zero-shot learning approaches [44; 45; 46] adopt word embedding projection to constitute the classifiers for unseen class classification. With the rapid progress of vision language pre-training [4; 5; 47], open vocabulary learning  has been proposed and demonstrates impressive capabilities by, for example, distilling knowledge from VLMs [49; 50; 51; 52; 53; 54], exploiting caption data [55; 56], generating pseudo labels [57; 58; 59; 60; 61], training without dense labels [62; 63; 64], joint learning of several tasks [65; 66; 67], and training with more balanced data [68; 69].

While sharing a very high-level idea of vision-language alignment in open-vocabulary methods, our SDSGG **i**) explicitly models the context-dependent scenarios and introduces _scene-specific_ text classifiers as the flexible learning targets, and **ii**) incoroperates a new mechanism for computing _self-normalized_ similarities, thereby renormalizing text classifiers according to the presented image.

**VLMs Meet LLMs .** The big win for VLMs has been all about getting the model to match up pictures and their descriptions closely while keeping the mismatched ones apart [4; 5; 47]. This trick, inspired by contrastive learning from self-supervised learning [71; 72; 73; 74; 75], helps VLMs get really good at figuring out what text goes with what image. Moreover, prompt learning acts as a flexible way to communicate with VLMs, giving them a nudge or context to apply their knowledge of images and text in specific ways [6; 76; 77; 78; 7]. In addition to hand-crafted or learnable prompts,  offers a fresh perspective, _i.e._, using LLMs to generate detailed, comprehensive prompts as the inputs of VLMs' text encoder. Many follow-up works [79; 80; 81; 82; 83; 84; 85] across various domains and tasks demonstrate the effectiveness of integrating VLMs and LLMs.

Category-/part-level prompts are _scene-agnostic_ and cannot adapt to specific contexts. To this end, SDSGG adopts _scene-specific_ descriptions, generated by LLMs in a multi-persona collaboration fashion, as the inputs of CLIP's text encoder. Different from part-level prompt based approaches [11; 12] which processes all part-level prompts as affirmative classifiers, SDSGG provides a flexible alternative via the association between classifiers (_i.e._, SSDs) and categories, and the renormalizing strategy _w.r.t._ each SSD. Since the learned semantic space of VLMs may not be sensitive to relations , we design a lightweight mutual visual adapter to project them into interaction-aware space for capturing the complicated interplay between the subject and object.

## 3 Methodology

**Task Setup and Notations.** Given an image \(\), SGG transforms it into a structured representation, _i.e._, a directed graph \(=\{,\}\), where \(\) represents localized (_i.e._, bounding box) objects with object category information and \(\) represents pairwise relations between objects. For a fair comparison, this work focuses on predicting \(\) given \(\), _i.e._, the predicate classification task which avoids the noise from object detection, as suggested by [1; 3; 12]. Our study delves into the intricacies of transitioning SGG from a traditional closed-set setting to an open vocabulary paradigm. This transition enables the system to recognize previously unseen predicate categories (_i.e._, novel split) by learning from observed predicate categories (_i.e._, base split) during training.

SDSGG follows the standard zero-shot pipeline with VLMs , which computes similarity between the visual embedding \(\) and the text embedding \(\) for each category, and the category with highest similarity is viewed as the final classification result (_c.f_. Fig. 2a). For each subject-object pair, \(\) can be derived by feeding cropped patches from the input image \(\) into the visual encoder. The text embedding \(\) used in existing OVSGG frameworks falls into two main settings: **i**) Each category consists of only one text classifier, _i.e._, the category name itself. **ii**) Each category consists of multiple text classifiers w.r.t. subject and object, _i.e._, part-level descriptions. SDSGG reformulates the text classifiers as _scene-specific_ descriptions which will be detailed in SS3.1.

**Algorithmic Overview.** SDSGG is a SSD based framework for OVSGG, supported by the cooperation of VLMs and LLMs. For the textual part (_c.f_. Fig. 2b), SDSSG enjoys the expressive range of the comprehensive SSDs generated by LLMs' multi-persona collaboration. This is complemented by a renormalizing mechanism to adjust the influence of each text classifier. For the visual part (_c.f_. Fig. 2c), SDSSG is equipped with a mutual visual adapter to aggregate visual features \(\) from \(\) for a given subject-object pair. After introducing how we generate and use SSDs for the text part (SS3.1) and the mutual visual adapter for interplay modeling of the subject and object (SS3.2), we will elaborate on SDSGG's training objective (SS3.3).

### Scene-specific Text Classifiers

**Scene-level Description Generation via Multi-persona Collaboration.** Using standard prompts to query LLMs is a direct way to generate scene descriptions. For instance, one could straightforwardly prompt LLM with a question like "Imagine there is an animal that is eating, what should the scene look like?". LLM's response would typically sketch out an envisioned scene based on its statistical training on large corpus. However, these responses may not fully capture the scene's complexity, often overlooking aspects such as the spatial arrangement of elements and the background environment.

To alleviate this, we draw inspiration from recent advances in LLMs' multi-persona capabilities [86; 87; 88; 89]. Specifically, LLM adopts three distinct roles, mirroring the expertise found in experts specializing in biology, physics, and engineering. This approach allows for a comprehensive discussion of what a given scene entails. Because each query to LLM usually only yields 3-5 sentences of description, we query LLM several times, each time giving LLM a different scene content to be discussed, thus obtaining a large number of scene descriptions. Since these initial descriptions may suffer from noise and semantic overlap, we ask LLM to streamline and combine these descriptions, ensuring more cohesive and distinct scene-level descriptions \(_{l}=\{d^{1},d^{2},,d^{N}\}\) and corresponding text embeddings \(=\{^{1},^{2},,^{N}\}\), where \(N\) denotes the number of SSDs and text embeddings \(\) are extracted by the text encoder of CLIP. Due to the limited space, we provide more details and prompts for generating scene descriptions in the appendix (SSD).

**Association between Scene-level Descriptions and Relation Categories.** So far, we have obtained various scene-level descriptions that have the ability to represent diverse scenes. A critical inquiry arises regarding their utility for relation detection, given their lack of explicit association with specific relation categories. To address this, we delineate three distinct scenarios characterizing the interplay between relation categories and scene descriptions: **i)** certain coexistence (\(C_{r}^{n}=1\)), where a direct correlation exists; **ii)** possible coexistence (\(C_{r}^{n}=0\)), indicating a potential but not guaranteed association; and **iii)** contradiction (\(C_{r}^{n}=-1\)), denoting an incompatibility between the scene description and relation category. Here \(C_{r}^{n}\) denotes the correlation between relation \(r\) and \(n_{th}\) scene description, and is generated by LLMs (prompts are shown in SSD). Such a categorization enables us to calculate the similarity for each relation category:

\[sim(,r)=_{n=1}^{N}C_{r}^{n}*,^{n},\] (1)

where \(,\) denotes the cosine similarity with temperature .

Figure 2: (a) Overview of SDSGG. (b) Each text classifier of SDSGG contains a raw description \(_{o}^{n}\) and an opposite description \(_{p}^{n}\). As such, the _self-normalized_ similarities can be computed with the association (\(C_{r}^{n}\)) between predicate categories and SSDs. (c) Given the visual features (_i.e._, \(_{s}^{img}\), \(_{o}^{cls}\), and \(_{o}^{img}\)) of both the subject and object extracted from CLIP’s visual encoder, our mutual visual adapter (MVA) projects them into interaction-aware space and models their complicated interplay with cross-attention.

**Scene-specific Descriptions = Scene-level Descriptions + Reweighing.** Upon examining text classifiers in depth, we noticed that certain classifiers are contextually bound, _e.g._, "two or more objects partially overlap each other" may not exist in all scenes. This observation underscores the necessity for a mechanism to evaluate the significance of each text classifier, rather than applying a uniform weight across the board. This is exactly what the term "_specific_" means. Recall that the similarity measurement between an image and the text "a photo of a cat" alone yields limited insight. However, when juxtaposed with multiple texts, such as "a photo of a cat/dog/tiger", the comparison of similarity scores across these categories reveals which category (cat, dog, or tiger) the image most closely resembles. Inspired by this, we propose the incorporation of an opposite description \(d_{y}^{m}\) (SSD) for each raw scene description \(d_{a}^{n}\) as a reference point (_e.g._, "two or more objects partially overlap each other" _vs._" each object is completely separate with clear space between them"), resulting in SSDs \(_{s}=\{(d_{a}^{1},d_{p}^{1}),(d_{a}^{2},d_{p}^{2}),,(d_{a}^{N},d_{p}^{N})\}\) and updated text embeddings \(=\{(_{a}^{1},_{p}^{1}),(_{a}^{2},_{p}^{2}), ,(_{a}^{N},_{p}^{N})\}\). Subsequently, the _self-normalized_ similarity is defined as:

\[sim(,r)=_{n=1}^{N}C_{r}^{n}*(,_{a}^{n} -,_{p}^{n}).\] (2)

The difference in similarity scores, _i.e._, \(,_{a}^{n}-,_{p}^{n}\), quantifies the relative contribution of that SSD. By such means, a SSD irrelevant to the presented context will have a minimal effect, as the similarity scores of it (\(,_{a}^{n}\)) and its opposite (\(,_{p}^{n}\)) would be nearly identical.

### Mutual Visual Adapter

After introducing how to obtain the text embeddings and how to compute vision-language similarity, one question remains at this point: how to obtain visual embeddings? When given a subject-object pair with bounding boxes from \(\), there exist various strategies for aggregating visual features for the subject and object within \(\). For example, traditional closed-set SGG frameworks [16; 17] employ Rol pooling to extract visual features for specified bounding boxes, subsequently fusing these features for further classification. In contrast, the more recent OVSGG framework  uses the visual encoder of CLIP to extract visual embeddings of both subject and object. Then, it processes two visual embeddings _independently_ through part-level descriptions. Such an independent approach, however, overlooks the informative interplay between the subject and object.

To address this oversight and capture the complicated interactions between subject and object, we introduce a new component: the mutual visual adapter (MVA). MVA is composed of several lightweight, learnable modules designed to fine-tune CLIP's visual encoder specifically for pairwise relation detection. This approach aims to enhance the model's ability to recognize the nuanced interactions that define relationships between subject and object in an image.

**Regional Encoder.** Given an image \(\) and a subject-object pair with bounding boxes (\(b_{s}\) and \(b_{o}\)) from \(\), the initial visual embeddings can be obtained from the visual encoder of CLIP:

\[_{s/o}\!=\![_{s/o}^{cls}]_{s/o}^{img}]\!=\![_{s/o}^{cls }]_{s/o}^{1},_{s/o}^{2},,_{s/o}^{M}]\!=\!_{v}\!(,b_{s/o}),\] (3)

where \(M\) denotes the number of patches, \(_{v}\) is CLIP's visual encoder that is kept frozen during training, and \(\) represents image cropping.

**Visual Aggregator.** Next, MVA is adopted to aggregate \(_{s}\) and \(_{o}\) by cross-attention and two lightweight projection modules. Let the subject part be the query, and the object part be the key and value. The patch embeddings of object \(_{o}^{img}\) are first projected into low-dimensional, semantic space:

\[_{o}^{img}=_{down}(_{o}^{img}),\] (4)

where \(\) denotes a standard fully connected layer. Afterwards, cross-attention is adopted to capture the complicated interplay between subject and object, resulting in an aggregated visual embedding for the given subject-object pair:

\[_{so}=_{up}((_ {o}^{cls}+(_{s}^{img},_{o}^{img}))),\] (5)

where \(\) is the average pooling. \(\) is the standard layer normalization. \((,)\) denotes the standard cross-attention operation. \(_{os}\) can be computed in a similar way by exchanging the query and key of cross-attention. Combining them together leads to the final visual embedding \(=(_{so}+_{os})/2\) for final similarity measurement. As such, MVA captures the interplay of subject and object in the projected, interaction-aware space.

**Directional Marker.** One may notice that the structure of MVA is _symmetric_ and has no information about which input branch is the subject/object. This has a relatively small effect on semantic relations, but a significant effect on geometric relations. For instance, after exchanging the location of the subject and object (image flipping), the relation "eating" remains unchanged, while the relation "on the left" would become "on the right". Here we simply incorporate two text embeddings (\(_{s}\) and \(_{o}\)) of "a photo of subject/object" into MVA and thus update the visual embedding as:

\[_{so}=((_{so},_{s})),\] (6)

where \(\) denotes concatenation. \(_{os}\) and \(\) can be updated accordingly. Further exploration of directional marker, _e.g._, incorporating more complex feature fusion modules, is left for future work.

### Training Objective

A typical training objective in open-vocabulary learning aims to bring representations of positive pairs closer and push negative pairs apart in the embedding space. In SDSGG, the term "positive/negative pairs" is not defined at the category level _but at the description level_, requiring losses tailored for different relation-description association types. Given a labeled relation triplet, one simplest contrastive loss can be defined as:

\[=|}_{^{n}} ,_{s}^{n})-,_{p}^{n}} _{similarity}-^{n}}_{target}^{2},\] (7)

where \(\) is a scaling factor. However, as for scene descriptions marked by possible coexistence (_i.e._, \(C_{r}^{n}=0\)), there is no direct target that can be used for training. Inspired by the identical mapping in residual learning , we make the prediction results of MVA close to those of CLIP. As such, MVA can learn the implicit knowledge embedded in CLIP's semantic space. In addition, this regularization term prevents MVA from overfitting to relations in the base split, which is a common problem in open-vocabulary learning. Hence, the loss is further reformulated as:

\[=|}_{^{n}} ,_{s}^{n})-,_{p}^{n}} _{similarity}-^{n}}_{target}-((,rel)-)}_{margin}^{2},\] (8)

where \(\) is another scaling factor, \(sim_{CLIP}(,rel)\) denotes the vision-language similarity derived from the original CLIP, and \(\) is a constant scalar and is empirically set to 3e-2.

## 4 Experiment

### Experimental Setup

**Dataset.** We evaluate our method on GQA  and VG  following [3; 12].

**Split.** Following previous work , VG is divided into two splits: base and novel split. The base split comprises 70% of the relation categories for training, while the novel split contains the remaining 30% categories invisible during training. For a more comprehensive comparison, we also conduct testing on the semantic set, encompassing 24 predicate categories [16; 12] with richer semantics. base and novel split of GQA  are obtained in a similar manner (SSB).

**Evaluation Metrics.** We report Recall@K (R@K) and Recall@K(mR@K) following [3; 22].

**Base Models and Competiors.** As for the base and novel split, we compare SDSGG with two baselines: **i)** CLS , which uses only the category-level prompts to compute the similarity between the image and text; and **ii)** Epic , a latest OVSGG method, which introduces an entangled cross-modal prompt approach and learns the cross-modal embeddings using contrastive learning. In terms of the semantic split, we compare our SDSGG with three baselines: **i)** CLS , which uses only the category-level prompts; **ii)** CLSDE , which uses prompts of relation class description; and **iii)** RECODE , which uses visual cues of several separate components. Since  has neither released the detailed split nor the code, it is not included in the comparisons for fairness.

**Implementation Details.** Due to limited space, implementation details are left in the appendix (SSB).

### Quantitative Comparison Result

We conduct quantitative experiments on VG  and GQA . To ensure the performance gain is reliable, each experiment is repeated three times. The average and standard deviation are reported.

[MISSING_PAGE_FAIL:8]

### Qualitative Comparison Result

Fig. 3 visualizes qualitative comparisons of SDSGG against CLIP  on VG . As seen, with the proposed SSDs, SDSGG can generate higher-quality relation predictions even in challenging scenarios. We respectfully refer the reviewer to the appendix (SSE) for more qualitative comparisons.

### Diagnostic Experiment

For thorough evaluation, we conduct a series of ablative studies on VG .

**Textual Part.** We first study the effectiveness of our multi-persona collaboration (MPC) for _scene-specific_ description generation (SS3.1) in Table 4. Here we use the standard prompts to query LLMs for generating descriptions. As seen, without MPC, the performance drops drastically, _e.g._, 11.8%/11.8% _vs_. 31.6%/30.0% R@100 on the base/novel split, respectively. This indicates the importance of the text classifiers used as they have impact on both training and testing.

While the effectiveness of MPC has been validated, one may wonder: **i)** Why use these three roles? **ii)** How to ensure the completeness and quality of generated classifiers? We want to highlight that: **i)** Different roles are used to increase the variety of descriptions. There is no word on exactly which roles should be used. **ii)** These open problems are beyond the scope of this work. We leave them for future work. **iii)** This work makes the first attempt to enhance classifier generation for OVSGG via MPC. **iv)** The experimental results suggest that the current generated SSDs are _good enough_ for commonly used relation categories. To provide more empirical results, we investigate the impact of the proposed _self-normalized_ similarities and the number of used SSDs in the appendix (SA).

**Visual Part.** Then, we examine the impact of mutual visual adapter (MVA, SS3.2) and directional marker (DM, SS3.2) in Table 5. The 1st row denotes a strong baseline, _i.e._, a multi-layer perceptron with comparable parameters to aggregate the visual features. Upon projecting the visual features into interaction-aware space and applying cross-attention, we observe consistent and substantial improvements for both R@K and mR@K on both the base and novel split. These results demonstrate the efficacy of our adapter and the necessity of incorporating cross-attention for capturing the complicated interplay between subjects and objects. Since DM is designed for geometric relations with massive annotations [21; 24], the improvements on R@K are considerable, while those on mR@K are relatively small. See SSA for more results.

## 5 Conclusion

This work presents SDSGG, a _scene-specific_ description based framework for OVSGG. Despite the previous works based upon category-/part-level prompts, we argue that these text classifiers are

   Method & Split & R@2050/100 & mR@2050/100 \\  
**Ours** & **base** & **18.7 / 26.6 / 31.6** & **9.1 / 12.3 / 14.7** \\ _w/o_ MPC & & 6.0 / 9.3 / 11.8 & 2.5 / 5.1 / 7.6 \\  
**Ours** & **novel** & **18.9 / 25.8 / 30.0** & **16.6 / 25.2 / 31.5** \\ _w/o_ MPC & & 5.2 / 8.7 / 11.8 & 4.9 / 11.0 / 16.2 \\   

Table 4: Ablation studies (§4.4) on MPC.

Figure 3: Visual results (§4.3) on VG .

   MVA & DM & Split & R@2050/100 & mR@2050/100 \\    & & 13.1 / 18.7 / 22.4 & 8.6 / 11.6 / 13.8 \\ ✓ & & 17.0 / 23.8 / 28.2 & **9.2 / 12.3 / 14.6** \\ ✓ & ✓ & & **18.7 / 26.6 / 31.6** & 9.1 / **12.3 / 14.7** \\   ✓ & & 17.4 / 24.7 / 28.9 & 17.2 / **26.5** / 30.9 \\ ✓ & & 18.6 / 25.1 / 28.7 & **17.4 / 25.0 / 31.0** \\ ✓ & ✓ & & **18.9 / 25.8 / 30.0** & 16.6 / 25.2 / **31.5** \\   

Table 5: Ablation studies (§4.4) on the visual part.

scene-agnostic_, which cannot adapt to specific contexts and may even be misleading. To address this, we carefully design a multi-persona collaboration strategy for generating flexible, context-aware SSDs, a _self-normalized_ similarity computation module for renormalizing the influence of each SSD, and a mutual visual adapter that consists of several trainable lightweight modules for learning interaction-aware space. Our approach distinguishes itself by using SSDs derived from LLMs, which are tailored to the content of the presented image. This is further enhanced by MVA, which captures the underlying interaction patterns based on the semantic space of VLMs. We expect the introduction of SDSGG will not only set a new benchmark for OVSGG, but also encourage the community to explore the potential of integrating LLMs with VLMs for deeper, contextual understanding of images.

**Acknowledgement.** This work was supported by the National Science and Technology Major Project (No. 2023ZD0121300), the National Natural Science Foundation of China (No. 62372405), the Fundamental Research Funds for the Central Universities 226-2024-00058, National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Xi'an Jiaotong University (No. HMHAI-202403), Bytedance Doubao Fund, and the Earth System Big Data Platform of the School of Earth Sciences, Zhejiang University.