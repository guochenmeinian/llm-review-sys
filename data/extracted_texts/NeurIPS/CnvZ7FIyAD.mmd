# Newton-Cotes Graph Neural Networks:

On the Time Evolution of Dynamic Systems

 Lingbing Guo\({}^{1,2,3}\), Weiqing Wang\({}^{4}\), Zhuo Chen\({}^{1,2,3}\), Ningyu Zhang\({}^{1,2}\),

**Zequn Sun\({}^{5}\), Yixuan Lai\({}^{1,2,3}\), Qiang Zhang\({}^{1,3}\) and Huajun Chen\({}^{1,2,3}\)\({}^{}\)**

\({}^{1}\)College of Computer Science and Technology, Zhejiang University

\({}^{2}\)Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph

\({}^{3}\)ZJU-Hangzhou Global Scientific and Technological Innovation Center

\({}^{4}\)Department of Data Science & AI, Monash University

\({}^{5}\)State Key Laboratory for Novel Software Technology, Nanjing University

Equal Contribution

###### Abstract

Reasoning system dynamics is one of the most important analytical approaches for many scientific studies. With the initial state of a system as input, the recent graph neural networks (GNNs)-based methods are capable of predicting the future state distant in time with high accuracy. Although these methods have diverse designs in modeling the coordinates and interacting forces of the system, we show that they actually share a common paradigm that learns the integration of the velocity over the interval between the initial and terminal coordinates. However, their integrand is constant w.r.t. time. Inspired by this observation, we propose a new approach to predict the integration based on several velocity estimations with Newton-Cotes formulas and prove its effectiveness theoretically. Extensive experiments on several benchmarks empirically demonstrate consistent and significant improvement compared with the state-of-the-art methods.

## 1 Introduction

Reasoning the time evolution of dynamic systems has been a long-term challenge for hundreds of years . Despite that the advances in computer manufacturing nowadays make it possible to simulate long trajectories of complicated systems constrained by multiple force laws, the computational cost still imposes a heavy burden on the scientific communities .

Recent graph neural networks (GNNs) -based methods provide an alternative solution to predict the future states directly with only the initial state as input . Take molecular dynamics (MD)  as an example, the atoms in a molecule can be regarded as nodes with different labels, and thus can be encoded by a GNN. As the input and the target are atomic coordinates, the learning problem becomes a complicated regression task of predicting the coordinate at each dimension of each atom. Furthermore, some directional information (e.g., velocities or forces) are also important features and cannot be directly leveraged especially considering the rotation and translation of molecules in the system. Therefore, the existing works put great effort into the reservation of physical symmetries, e.g., transformation equivariance and geometric constraints . The experimental results on a variety of benchmarks also demonstrate the advantages of their methods.

Without loss of generality, the main target of the existing works is to predict the future coordinate \(^{T}\) distant to the given initial coordinate \(^{0}\), with \(^{0}\) and the initial velocity \(^{0}\) as input. Then, thepredicted coordinate \(}^{T}\) can be written as:

\[}^{T}=^{0}+}^{0}T.\] (1)

For a complicated system comprising multiple particles, predicting the velocity term \(}^{0}\) rather than the coordinate \(}^{T}\) improves the robustness and performance. Specifically, by subtracting the initial coordinate \(^{0}\), the learning target can be regarded as the normalized future coordinate, i.e., \(}^{0}=(^{T}-^{0})/(T-0)\), which is why all state-of-the-art methods adopt this strategy [18; 19]. Nevertheless, the current strategy still has a significant drawback from the view of numerical integration.

As illustrated in Figure 1, supposed that the actual velocity of a particle is described by the curve \(v(t)\). To predict the future state \(^{T}\) at \(t=T\), the existing methods adopt a constant estimation approach, i.e., \(}^{T}=^{0}+}^{0}T=^{0}+ _{0}^{T}}^{0}dt\). Evidently, the prediction error could be significant as it purely relies on the fitness of neural models. If we alternatively choose a simple two-step estimation (i.e., Trapezoidal rule ), the prediction error may be reduced to only the blue area. In other words, the model only needs to _compensate_ for the blue area.

In this paper, we propose Newton-Cotes graph neural networks (abbr. NC) to estimate multiple velocities at different time points and compute the integration with Newton-Cotes formulas. Newton-Cotes formulas are a series of formulas for numerical integration in which the integrand (in our case, \(v(t)\)) are evaluated at equally spaced points. One most important characteristic of Newton-Cotes formulas is that the integration is computed by aggregating the values of \(v(t)\) at these points with a group of weights \(\{w^{0},...,w^{k}\}\), where \(k\) refers to the order of Newton-Cotes formulas and \(\{w^{0},...,w^{k}\}\) is irrelevant to the integrand \(v(t)\) and can be pre-computed by Lagrange interpolating polynomial .

We show that the existing works can be naturally derived to the basic version NC (\(k=0\)) and theoretically prove that the prediction error of this estimation as well as the learning difficulty will continually reduce as the increase of estimation step \(k\).

To better train a NC (k), we may also need the intermediate velocities as additional supervised data. Although these data can be readily obtained (as the sampling frequency is much higher than our requirement), we argue that the performance suffers slightly even if we do not use them. The model is capable of learning to generate promising velocities to better match the true integration. By contrast, if we do provide these data to train a NC (k), denoted by NC\({}^{+}\) (k), we will obtain a stronger model that not only achieves high prediction accuracy in conventional tasks, but also produces highly reliable results of long-term consecutive predictions.

We conduct experiments on several datasets ranging from N-body systems to molecular dynamics and human motions [27; 28; 29], with state-of-the-art methods as baselines [17; 18; 19]. The results show that NC improves all the baseline methods with a significant margin on all types of datasets, even without any additional training data. Particularly, the improvement for the method RF  is greater than \(30\%\) on almost all datasets.

## 2 Related Works

In this section, we first introduce the related works using GNNs to learn system dynamics and then discuss more complicated methods that possess the equivariance properties.

Graph Neural Networks for Reasoning DynamicsInteraction network  is perhaps the first GNN model that learns to capture system dynamics. It separates the states and correlations into two different parts and employs GNNs to learn their interactions. This progress is similar to some simulation tools where the coordinate and velocity/force are computed and updated in an alternative fashion. Many followers extend this idea, e.g., using hierarchical graph convolution [9; 10; 31; 32] or auto-encoder [33; 34; 35; 36] to encode the objects, or leveraging ordinary differential equations for energy

Figure 1: Illustration of different estimations. The rectangle, trapezoid, and striped areas denote the basic (used in the existing works), two-step, and true estimations, respectively. Blue denotes the error of two-step estimation that a model needs to compensate for, whereas blue \(+\) yellow denotes the error of the existing methods.

conservation . The above methods usually cannot process the interactions among particles in the original space (e.g., Euclidean space). They instead propose an auxiliary interaction graph to compute the interactions, which is out of our main focus.

Equivariant Graph Neural NetworksRecent methods considering physical symmetry and Euclidean equivariance have achieved state-of-the-art performance in modeling system dynamics [14; 15; 16; 17; 18; 19]. Specifically, [14; 15; 16] force the translation equivariance, but the rotation equivariance is overlooked. TFN  further leverages spherical filters to achieve rotation equivariance. SE(3) Transformer  proposes to use the Transformer  architecture to model 3D cloud points directly in Euclidean space. EGNN  simplifies the equivariant graph neural network and make it applicable in modeling system dynamics. GMN  proposes to consider the physical constraints (e.g., sticks and hinges sub-structures) widely existed in systems, which achieves the state-of-the-art performance while maintaining the same level of computational cost. SEGNN  extends EGNN with steerable vectors to model the covariant information among nodes and edges. Note that, not all EGNNs are designed to reason system dynamics, but the best-performing ones (e.g., EGNN  and GMN ) in this sub-area usually regard the velocity \(\) as an important feature. These methods can be set as the backbone model in NC, and the models themselves belong to the basic NC (0).

Neural Ordinary Differential EquationsThe neural ordinary differential equation (neural ODE) methods [40; 41] parameterize ODEs with neural layers and solve them by numerical solvers such as 4th order Runge-Kutta. These solvers are originally designed for numerical integration. Our work draws inspiration from numerical integration algorithms, where the integrand is known only at certain points. The goal is to efficiently approximate the integral to desired precision. Neural ODE methods repeatedly perform the numerical algorithms on small internals to obtain the numerical solution. Therefore, it may be feasible to iteratively perform our method to solve problems in neural ODEs.

## 3 Methodology

We start from preliminaries and then take the standard EGNN  as an example to illustrate how the current deep learning methods work. We show that the learning paradigm of the existing methods can be derived to the simplest form of numerical integration. Finally, we propose NC and theoretically prove its effectiveness in predicting future states.

### Preliminaries

We suppose that a system comprises \(N\) particles \(p_{1},p_{2},...,p_{N}\), with the velocities \(_{1},_{2},...,_{N}\) and the coordinates \(_{1},_{2},...,_{N}\) as states. The particles can also carry various scalar features (e.g., mass and charge for atoms) which will be encoded as embeddings. We follow the corresponding existing works [17; 18; 19] to process them and do not discuss the details in this paper.

Reasoning DynamicsReasoning system dynamics is a classical task with a very long history . One basic tool for reasoning or simulating a system is numerical integration. Given the dynamics (e.g., Langevin dynamics, well-used in MD) and initial information, the interaction force and acceleration for each particle in the system can be estimated. However, the force is closely related to the real-time coordinate, which means that one must re-calculate the system states for every very small time step (i.e., \(dt\)) to obtain a more accurate prediction for a long time interval. For example, in MD simulation, the time step \(dt\) is usually set to \(50\) or \(100\) fs (\(1\) fs \(=10^{-15}\) second), whereas the total simulation time can be \(1\) ms (\(10^{-3}\) second). Furthermore, pursuing highly accurate results usually demands more complicated dynamics, imposing heavy burdens even for super-computers. Therefore, leveraging neural models to directly predict the future state of the system has recently gained great attention.

### Egnn

Although the existing EGNN methods have great advantage in computational cost over the conventional simulation tools, the potential prediction error that a neural model needs to compensate for is also huge. Take the standard EGNN  as an example, the equivariant graph convolution can be written as follows:

\[_{ij}^{0}=_{e}(_{i},_{j},||_{i}^{ 0}-_{j}^{0}||^{2},e_{ij}),\] (2)where the aggregation function \(_{e}\) takes three types of information as input: the feature embeddings \(_{i}\) and \(_{j}\) for the input particle \(p_{i}\) and an arbitrary particle \(p_{j}\), respectively; the Euclidean distance \(||_{i}^{0}-_{j}^{0}||^{2}\) at time \(t=0\); and the edge attribute \(e_{ij}\). Then, the predicted velocity and future coordinate can be calculated by the following equation:

\[}_{i}^{0} =_{v}(_{i})_{i}^{0}+_{j  i}{(_{i}^{0}-_{j}^{0})_{ij}^{0}},\] (3) \[}_{i}^{T} =_{i}^{0}+}_{i}^{0}T,\] (4)

where \(_{v}:^{D}^{1}\) is a learnable function. From the above equations, we can find that the predicted velocity \(}_{i}^{0}\) is also correlated with three types of information: 1) the initial velocity \(_{i}^{0}\) as a basis; 2) the pair-wise Euclidean distance \(||_{i}^{0}-_{j}^{0}||^{2}\) (i.e., \(_{ij}^{0}\)) to determine the amount of force (analogous to Coulomb's law); and the pair-wise relative coordinate difference \((_{i}^{0}-_{j}^{0})\) to determine the direction. For multi-layer EGNN and other EGNN models, \(}_{i}^{0}\) is still determined by these three types of information, which we refer interested readers to Appendix A for details.

**Proposition 3.1** (Linear mapping).: _The existing methods learn a linear mapping from the initial velocity \(^{0}\) to the average velocity \(^{t^{*}}\) over the interval \([0,T]\)._

Proof.: Please see Appendix B.1 

As the model makes prediction only based on the state and velocity at \(t=0\), we assume that \(^{t^{*}}\) fluctuates around \(^{0}\) and follows a normal distribution denoted by \(_{NC(0)}=(^{0},_{NC(0)}^{2})\). Then, the variance term \(_{NC(0)}^{2}\) reflects how difficult to train a neural model on data sampled from this distribution. In other words, the larger the variance is, the worse the expected results are.

**Proposition 3.2** (Variance of \(_{NC(0)}\)).: _Assume that the average velocity \(^{t^{*}}\) over \([0,T]\) follows a normal distribution with \(=^{0}\), then the variance of the distribution is:_

\[_{NC(0)}^{2}=(T^{2})\] (5)

Proof.: According to the definition, \(_{NC(0)}^{2}\) can be written as follows:

\[_{NC(0)}^{2} =^{t^{*}}-^{0})^{2}}}{M}= ^{t^{*}}T-^{0}T)/T)}^{2}}{M}\] (6) \[=^{T}-^{0})-^{0}T )^{2}}}{MT^{2}}=^{T}((t)-^{0}T)dt) ^{2}}}{MT^{2}},\] (7)

where \(M N\) is the number of all samples. The term \(((t)-^{0}T)\) is a typical (degree \(0\)) polynomial interpolation error and can be defined as:

\[_{(0)}(t) =(t)-^{0}T=(t)-P_{k}(t)k=0\] (8) \[=^{(k+1)}()}{(k+1)!}(t-t_{0})(t-t_{1})...(t-t_ {k})k=0=()t,\] (9)

where \(0 T\), and the corresponding integration error is:

\[_{(0)}=_{0}^{T}_{(0)}(t)dt=( )_{0}^{T}tdt=()T^{2}=(T^{2}).\] (10)

Therefore, we obtain the final variance:

\[_{NC(0)}^{2}=(T^{4})}{MT^{2}}=(T^{2}),\] (11)

concluding the proof.

### Nc (k)

In comparison with the \(0\) degree polynomial approximation NC\((0)\), the high order NC\((k)\) is more accurate and yields only a little extra computational cost.

Suppose that we now have \(K+1\) (usually \(K 8\) due to the catastrophic Runge's phenomenon ) points \(_{i}^{0},_{i}^{1},...,_{k}^{K}\) equally spaced on time interval \([0,T]\), then the prediction for the future coordinate \(}_{i}^{T}\) in Equation (4) can be re-written as:

\[}_{i}^{T}=_{i}^{0}+_{k=0}^{K}w^{k} }_{i}^{k},\] (12)

where \(\{w^{k}\}\) is the coefficients of Newton-Cotes formulas and \(}_{i}^{k}\) denotes the predicted velocity at time \(t^{k}\). To obtain \(}_{i}^{k},k 1\), we simply regard EGNN as a recurrent model  and re-input the last output velocity and coordinate. Some popular sequence models like LSTM  or Transformer  may be also leveraged, which we leave to future work. Then, the equation of updating the predicted velocity \(}_{i}^{k}\) at \(t^{k}\) can be written as:

\[}_{i}^{k}=_{v}(_{i})_{i}^{k-1}+_{i}^{k-1}-_{j}^{k-1})_{ij}}}{N -1},\] (13)

where \(_{i}^{k-1},_{i}^{k-1}\) denote the velocity and coordinate at \(t^{k-1}\), respectively. Note that, Equation (13) is different from the form used in a multi-layer EGNN. The latter always uses the same input velocity and coordinate as constant features in different layers. By contrast, we first predict the next velocity and coordinate and then use them as input to get the new trainable velocity and coordinate. This process is more like training a language model .

From the interpolation theorem , there exists a unique polynomial \(_{k=0}^{K}C^{k}t^{(k)}\) of degree at most \(K\) that interpolates the \(K+1\) points, where \(C^{k}\) is the coefficients of the polynomial. To avoid ambiguity, we here use \(t^{(k)}\) to denote \(t\) raised to the power of \(k\), whereas \(t^{k}\) to denote the \(k\)-th time point. \(_{k=0}^{K}C^{k}t^{(k)}\) thus can be constructed by the following equation:

\[_{k}^{K}C^{k}t^{(k)}=[(t^{0})]+[(t^{0}),(t ^{1})](t-t^{0})+...+[(t^{0}),...,(t^{K})](t-t^{0})(t-t^{1} )...(t-t^{K}),\] (14)

where \([]\) denotes the divided difference. In our case, the \(K+1\) points are equally spaced over the interval \([0,T]\). According to Newton-Cotes formulas, we will have:

\[_{0}^{T}(t)dt_{0}^{T}_{k=0}^{K}C_{k}t^{(k)}dt= _{k=0}^{K}w^{k}^{k}.\] (15)

Particularly, \(\{w^{k}\}\) are Newton-Cotes coefficients that are irrelevant to \((t)\). With additional observable points, the estimation for the integration can be much more accurate. The objective thus can be written as follows:

\[*{argmin}_{\{^{k}\}}_{p}^{T}-}^{T}=_{p}^{T}-^{0}-_{k=0}^{ K}w^{k}}^{k}.\] (16)

**Proposition 3.3** (Variance of \(_{NC(k)}^{2}\)).: \( k\{0,1,...\},_{NC(k)}_{NC(k+1)}\)_, and consequently:_

\[_{NC(0)}^{2}_{NC(1)}^{2}..._{NC(k)}^{2}.\] (17)

Proof.: Please see Appendix B.2 for details. Briefly, we only need to prove that \(_{(0)}_{(1)}\), where NC (1) is associated with Trapezoidal rule. 

**Proposition 3.4** (Equivariance of NC).: _NC possesses the equivariance property if the backbone model \(\) (e.g., EGNN) in NC possesses this property._

Proof.: Please see Appendix B.3.

### NC (k) and NC\({}^{+}\) (k)

In the previous formulation, in addition to the initial and terminal points, we also need the other \(K-1\) points for supervision which yields a regularization loss:

\[_{r}=_{p}_{k=0}^{K}\|^{k}-}^{k}\|.\] (18)

We denote NC (k) with the above loss by NC\({}^{+}\) (k). Minimizing \(_{r}\) is equivalent to minimizing the difference between the true velocity and predicted velocity at each time point \(t^{k}\) for each particle. \(\|\|\) can be arbitrary distance measure, e.g., L2 distance. Although the \(K-1\) points of data can be easily accessed in practice, we argue that a loose version (without \(_{r}\)) NC (k) can also learn promising estimation of the integration \(_{0}^{T}(t)dt\). Specifically, we still use \(K+1\) points \(\{}^{k}\}\) to calculate the integration over \([0,T]\), except that the intermediate \(K-1\) points are unbounded. The model itself needs to determine the proper values of \(\{}^{k}\}\) to optimize the same objective defined in Equation 16. In Section 4.5, we design an experiment to investigate the difference between the true velocities \(\{^{k}\}\) and the predicted velocities \(\{}^{k}\}\).

### Computational Complexity and Limitations

In comparison with EGNN, NC (k) does not involve additional neural layers or parameters. Intuitively, we recurrently use the last output of EGNN to produce new predicted velocities at next time point, the corresponding computational cost should be \(K\) times that of the original EGNN. However, our experiment in Section 4.4 shows that the training time did not actually linearly increase w.r.t. \(K\). One reason may be the gradient in back-propagation is still computed only once rather than \(K\) times.

We present an implementation of NC\({}^{+}\) in Algorithm 1. We first initialize all variables in the model and then recurrently input the last output to the backbone model to collect a series of predicted velocities. We then calculate the final predicted integration and plus the initial coordinate as the final output (i.e., Equation (12)). Finally we compute the losses and update the model by back-propagation.

```
1:Input: the dataset \(\), the backbone model \(\), number of steps \(k\) for NC;
2:repeat
3:for each batched data in the training set \((\{^{0},...,^{k}\},\{^{0},..., ^{k}\})\)do
4:\(}^{0}^{0},} ^{0}^{0}\);
5:for\(i:=1\)to\(k\)do
6:\(}^{i},}^{i}(}^{i-1},}^{i-1})\);
7:endfor
8: Compute the final prediction \(}^{k}\) by Equation 12;
9: Compute the main prediction loss \(_{min}\) according to Equation (16);
10: Compute the velocity regularization loss \(_{r}\) according to Equation (18);
11: Minimize \(_{main}\), \(_{r}\);
12:endfor
13:until the loss on the validation set converges. ```

**Algorithm 1** Newton-Cotes Graph Neural Network

## 4 Experiment

We conducted experiments on three different tasks towards reasoning system dynamics. The source code and datasets are available at https://github.com/zjukg/NCGNN.

### Settings

We selected three state-of-the-art methods as our backbone models: RF , EGNN , and GMN . To ensure a fair comparison, we followed their best parameter settings (e.g., hidden size and the number of layers) to construct NC. In other words, the main settings of NC and the baselineswere identical. The results reported in the main experiments were based on a model of NC (2) for a balance of training cost and performance.

We reported the performance of NC w/ additional points (denoted by \(^{+}\)) and w/o additional points (the relaxed version, denoted by \(\)).

### Datasets

We leveraged three datasets towards different aspects of reasoning dynamics to exhaust the performance of NC. We used the N-body simulation benchmark proposed in . Compared with the previous ones, this benchmark has more different settings, e.g., number of particles and training data. Furthermore, it also considers some physical constraints (e.g., hinges and sticks) widely existing in the real world. For molecular dynamics, We used MD17  that consists of the molecular dynamics simulations of eight different molecules as a scenario. We followed  to construct the training/testing sets. We also considered reasoning human motion. Follow , the dataset was constructed based on \(23\) trials in CMU Motion Capture Database .

As NC\({}^{+}\) requires additional supervised data, we accordingly extracted more intermediate data points between the input and target from each dataset. It is worth noting that this operation was tractable as the sampling frequency of the datasets was much higher than our requirement.

### Main Results

We reported the main experimental results w.r.t. three datasets in Tables 1, 2, and 3, respectively. Overall, NC\({}^{+}\) significantly improved the performance of all baselines on almost all datasets and across all settings, which empirically demonstrates the generality as well as the effectiveness of the proposed method. Remarkably, NC (without extra data) achieved slightly low results but still had great advantages over performance in comparison with three baselines.

Specifically, The average improvement on N-body datasets was most significant, where we observe a near 20% decrease in prediction error for all three baseline methods. For example, NC (RF) and NC

    & =500\\ 0.1,0.2,0.1\)} & =1500\\ 0.1,3,2,1\)} \\  Linear & 8.23\(\)0.7 & 7.55\(\)0.00 & 9.76\(\)0.00 & 11.36\(\)0.00 & 8.22\(\)0.00 & 7.55\(\)0.00 & 9.76\(\)0.00 & 11.36\(\)0.00 & 11.62\(\)0.00 \\ GNN  & 5.33\(\)0.00 & 5.01\(\)0.00 & 7.58\(\)0.00 & 9.83\(\)0.00 & 9.77\(\)0.00 & 3.61\(\)0.13 & 3.23\(\)0.00 & 4.73\(\)0.11 & 7.97\(\)0.00 & 7.91\(\)0.31 \\ TFN  & 11.54\(\)0.00 & 9.87\(\)0.27 & 11.66\(\)0.00 & 13.43\(\)0.31 & 12.23\(\)0.12 & 5.86\(\)0.35 & 4.97\(\)0.23 & 8.51\(\)0.14 & 11.21\(\)0.21 & 10.75\(\)0.00 \\ SE(3)-Tr.  & 5.54\(\)0.00 & 5.14\(\)0.00 & 8.95\(\)0.00 & 11.42\(\)0.00 & 11.59\(\)0.00 & 5.02\(\)0.00 & 4.68\(\)0.00 & 8.39\(\)0.00 & 10.82\(\)0.00 & 10.85\(\)0.00 \\  RF  & 3.50\(\)0.01 & 3.07\(\)0.24 & 5.25\(\)0.44 & 7.59\(\)0.25 & 7.73\(\)0.39 & 2.97\(\)0.15 & 2.19\(\)0.11 & 3.80\(\)0.25 & 5.71\(\)0.31 & 5.66\(\)0.27 \\ NC (RF) & **2.84\(\)0.00** & **2.35\(\)**0.00 & **4.32\(\)**0.00 & **6.67\(\)**2.00 & 7.14\(\)0.23 & 2.91\(\)0.01 & **1.76\(\)0.01 & **3.23\(\)0.00** & 9.09\(\)0.00 & 5.34\(\)0.03 \\ NC\({}^{+}\) (RF) & 2.92\(\)0.00 & **2.34\(\)**0.00** & **4.28\(\)**0.00 & 7.02\(\)**0.01 & **7.06\(\)0.03** & **2.87\(\)0.00** & 1.86\(\)0.00 & 3.24\(\)0.00 & **5.06\(\)0.00** & **5.23\(\)0.29** \\  GNN  & 2.81\(\)0.27 & 2.27\(\)0.00 & 4.67\(\)0.75 & 4.75\(\)0.05 & 4.59\(\)0.20 & 2.59\(\)0.10 & 1.86\(\)0.00 & 2.54\(\)0.01 & 2.79\(\)0.00 & 3.25\(\)0.00 \\ NC (EGNN) & 2.14\(\)0.00 & 2.18\(\)0.00 & 2.53\(\)0.00 & 4.26\(\)0.00 & 4.13\(\)0.13 & 2.23\(\)0.13 & 1.91\(\)0.01 & 2.14\(\)0.00 & 2.36\(\)0.00 & **2.86\(\)0.00** \\ NC\({}^{+}\) (EGNN) & **2.25\(\)0.00** & **2.07\(\)**0.00 & **2.01\(\)**0.00** & **3.54\(\)**0.00 & **3.96\(\)**0.00 & 2.32\(\)0.00 & **1.86\(\)0.03** & 2.39\(\)0.01 & **2.35\(\)0.00** & 2.99\(\)0.02 \\  GMN  & 1.84\(\)0.00 & 2.02\(\)0.02 & 2.48\(\)0.00 & 2.92\(\)0.00 & 4.08\(\)0.00 & 1.68\(\)0.00 & 1.47\(\)0.03 & 2.10\(\)0.00 & 2.32\(\)0.02 & 2.86\(\)0.01 \\ NC (GMN) & **1.55\(\)0.00** & 1.58\(\)0.00 & 1.58\(\)0.02 & 2.07\(\)0.03 & 2.73\(\)0.00 & 2.99\(\)0.00 & 1.59\(\)0.00 & 1.18\(\)0.05 & 1.47\(\)0.01 & **1.66\(\)0.00** & 1.93\(\)0.03 \\ NC\({}^{+}\) (GMN) & 1.57\(\)0.00 & **1.43\(\)**0.02 & **2.03\(\)**0.00 & **2.57\(\)**0.04 & **2.72\(\)**0.03 & **1.49\(\)**0.00 & **1.17\(\)0.00** & **1.44\(\)**0.00** & 1.70\(\)0.06 & **1.91\(\)0.02** \\   

Table 1: Prediction error (\( 10^{-2}\)) on N-body dataset. The header of each column “\(p,s,h\)” denotes the scenario with \(p\) isolated particles, \(s\) sticks and \(h\) hinges. Results averaged across 3 runs.

    & \\ \)} & \\ \)} & \\ \)} & \\ \)} & \\ \)} \\  Raw & 10.94\(\)0.00 & 14.4\(\)1.17 & 10.14\(\)0.00 & 103.7\(\)1.25 & 62.4\(\)0.00 & 48.1\(\)0.00 & 46.4\(\)0.00 & 48.4\(\)0.00 & 13.9\(\)0.00 & 11.36\(\)(RF) greatly outperformed the original RF and even had better results than the original EGNN under some settings (e.g., 3,2,1). This phenomenon also happened on the molecular dynamics datasets, where we find that NC\({}^{+}\) (EGNN) not only defeated its original version, but also the original GMN and even NC\({}^{+}\) (GMN) on many settings.

### Impact of estimation step \(k\)

We conducted experiments to explore how the performance changes with respect to the estimation step \(k\). The experimental results are shown in Figure 2.

For all three baselines, the performance improved rapidly as the growth of step \(k\) from \(0\) to \(2\), but this advantage gradually vanished as we set larger \(k\). Interestingly, the curve of training time showed an inverse trend, where we find that the total training time of NC (5) with GMN was almost two times slower than the NC (1) version. Therefore, we set \(k=2\) in previous experiments to get the best balance between performance and training cost.

By comparing the prediction errors of different baselines, we find that GMN that considers the physical constraints (e.g., sticks and hinges) usually had better performance than the other two methods, but the gap significantly narrowed with the increase of step \(k\). For example, on MD17 and motion datasets, EGNN obtained similar prediction errors for \(k 2\) while demanding less training time. This phenomenon demonstrates that predicting the integration with multiple estimations lowers the requirement for model capacity.

Overall, learning with NC significantly reduced the prediction errors of different models. The additional training time was also acceptable in most cases.

### A Comparison between NC and NC\({}^{+}\)

The only difference between NC and NC\({}^{+}\) was the use of the regularization loss in NC\({}^{+}\) to learn the intermediate velocities. Therefore, we designed experiments to compare the intermediate results and the final predictions of NC and NC\({}^{+}\) with GMN as the backbone model.

   & GNN & TFN & SE(3)-Tr. & RF & EGNN & GMN \\  Raw & 67.3\({}_{ 1}\) & 66.9\({}_{ 2}\) & 60.9\({}_{ 0.9}\) & 197.0\({}_{ 10}\) & 59.1\({}_{ 2}\) & 43.9\({}_{ 1}\) \\ NC & N/A & N/A & N/A & 164.8\({}_{ 17}\) & 55.8\({}_{ 61}\) & 30.0\({}_{ 14}\) \\ NC\({}^{+}\) & N/A & N/A & N/A & **162.6\({}_{ 10}\)** & **51.3\({}_{ 4}\)** & **29.6\({}_{ 10}\)** \\  

Table 3: Prediction error (\( 10^{-2}\)) on motion capture. Results averaged across 3 runs.

Figure 2: A detailed comparison of NCGNN without extra data on \(5\) datasets, w.r.t. \(k\), average of \(3\) runs, conducted on a V100. The lines with dot, circle, and triangle denote the results of GMN, EGNN, and RF, respectively. The first, second, and third rows are the results of prediction error, training time of one epoch, and total training time, respectively.

We first tracked the average intermediate velocity prediction errors on valid datasets in terms of training epochs. The results are shown in Figure 3, from which we observe that the prediction errors were not huge for both two methods, especially NC\({}^{+}\) obtained highly accurate prediction for the intermediate velocities with different \(k\). For NC, we actually find that it implicitly learned to model the intermediate velocities more or less, even though its prediction errors gradually improved or dramatically fluctuated during training. This observation empirically demonstrates the effectiveness of the intermediate velocities in predicting the final state of the system. We also reported the results on MD17 and N-body datasets in Appendix C.

We then visualized the intermediate velocities and the final predictions in Figure 4. The intermediate velocities were visualized by adding the velocity with the corresponding coordinate at the intermediate time point. From Figure 4, we can find that NC\({}^{+}\) (green ones) learned very accurate predictions with only a few non-overlapped nodes to the target nodes (red ones). For the loose version NC, it learned good predictions for the backbone part, but failed on the limbs (especially the legs). The motion of the main backbone was usually stable, while that of the limbs involved swing and twisting. We also conducted experiments on the other two types of datasets, please see Figure 8 in Appendix C.

### On the Time Evolution of Dynamic Systems

In previous experiments, we show that NC\({}^{+}\) was capable of learning highly accurate predictions of intermediate velocities. This inspired us to realize the full potential of NC\({}^{+}\) by producing a series of predictions given only the initial state as input. For each time point (including the intermediate time points), we used the most recent \(k+1\) points of averaged data as input to predict the next velocity and

Figure 4: Visualization of the intermediate velocities w.r.t. \(k\). The red, blue, and green lines denote the target, prediction of NC, and prediction of NC\({}^{+}\), respectively.

Figure 5: Consecutive predictions on the Motion dataset. The red, blue, and green lines denote the target, the prediction of NC (0), and the prediction of NC\({}^{+}\) (2), respectively. The interval between two figures is identical to the setting in the main experiment.

Figure 3: The prediction errors of intermediate velocities on valid set, w.r.t. training epoch. The blue and green lines denote the prediction errors of NC and NC\({}^{+}\), respectively.

coordinate. This strategy produced much more stable predictions than the baseline NC (0) - directly using the last output as input to predict the next state.

The results are shown in Figure 5, from which we can find that both the baseline (blue lines) and NC\({}^{+}\) (green lines) had good predictions at the initial steps. However, due to the increasing acculturated errors, the baseline soon collapsed and only the main backbone was roughly fit to the target. By contrast, NC\({}^{+}\) was still able to produce promising results. The deviations mainly concentrated on the limbs. Therefore, we argue that NC\({}^{+}\) can produce not only the highly accurate one-step prediction, but also the relatively reliable consecutive predictions. We also uploaded the.gif files of examples on all three datasets in Supplementary Materials, which demonstrated the consistent conclusion.

We also compared our method with the EGNN methods for multi-step prediction [49; 50; 51; 52]. Specifically, multi-step prediction takes a sequence of states with fixed interval as input and predicts a sequence of future states with same length and interval. As recent multi-step prediction methods are still based on sequence or encoder-decoder architectures, it would be interesting to examine the applicability of NC to them. To this end, we adapted NC (\(k\)=2) to the source code of the best-performing method EqMotion  and used identical settings for the hyper-parameters.

The experimental results are shown in Table 4, where we can observe that NC (EqMotion) outperformed the base model EqMotion on all datasets and across all metrics. It is worth noting that the architecture and parameter-setting of the EGNN module were identical to EqMotion and NC (EqMotion), which clearly demonstrates the effectiveness of the proposed method. We have also released the source code of NC (EqMotion) in our GitHub repository.

### Ablation Study

We conducted ablation studies to investigate the effectiveness of the proposed Newton-Cotes integration. We implemented two alternative methods: NC (coeffs\(=1\)), where we set all the coefficients for summing the predicted velocities as 1; and depth-varying ODE, where we employed common neural ODE solvers from [40; 41] to address our problem. We used an identical GMN as the backend EGNN model and conducted a simple hyper-parameter search for the learning rate and ODE solver.

The results are presented in Table 5. The two alternative methods methods generally performed better than the baseline GMN. The performance of depth-varying GDE was comparable to that of NC (coeffs\(=1\)), but both were lower than that of the original NC. This observation further supports the effectiveness of our method.

## 5 Conclusion

In this paper, we propose NC to tackle the problem of reasoning system dynamics. We show that the existing state-of-the-art methods can be derived to the basic form of NC, and theoretically/empirically prove the effectiveness of high order NC. Furthermore, with a few additional data provided, NC\({}^{+}\) is capable of producing promising consecutive predictions. In future, we plan to study how to improve the ability of NC\({}^{+}\) on the consecutive prediction task.

    & N-body (Springs) & MD17 (Aspirin) & MD17 (Benzene) & MD17 (Ethanol) & MD17 (Malonaldehyde) \\   & ADE & Accuracy & ADE & FDE & ADE & FDE & ADE & FDE & ADE & FDE \\  LSTM  & - & 53.5 & 17.59 & 24.79 & 6.06 & 9.46 & 7.73 & 9.88 & 15.14 & 22.90 \\ NRI  & - & 93.0 & 12.60 & 18.50 & 1.89 & 2.58 & 6.69 & 8.78 & 12.79 & 19.86 \\ GroupNet  & - & - & 10.62 & 14.00 & 2.02 & 2.95 & 6.00 & 7.88 & 7.99 & 12.49 \\ EqMotion  & 16.8 & 97.6 & 5.95 & 8.38 & 1.18 & 1.73 & 5.05 & 7.02 & 5.85 & 9.02 \\  NC (EqMotion) & **16.2** & **98.9** & **4.74** & **6.76** & **1.15** & **1.63** & **5.02** & **6.87** & **5.19** & **8.07** \\   

Table 4: Multi-step prediction results (\( 10^{-2}\)) on the N-body and MD17 datasets. ADE and FDE denote average displacement error and final displacement error, respectively. The results of four baseline methods are from .

    & N-body (1,2,0) & MD17 (Aspirin) & Motion \\  GMN & 1.84 & 10.14 & 43.9 \\ NC (coeffs\(=1\)) & 1.78 & 9.64 & 48.4 \\ depth-varying ODE & 1.82 & 10.08 & 42.2 \\ NC & 1.55 & 9.50 & 30.0 \\   

Table 5: Ablation study results (\( 10^{-2}\)).