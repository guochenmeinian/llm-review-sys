# Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement

Hui Yuan

&Kaixuan Huang

Chengzhuo Ni

&Minshuo Chen

&Mengdi Wang

Department of Electrical and Computer Engineering, Princeton University. Authors' emails are: {huiyuan, kaixuanh, cn10, mc0750, mengdiw}@princeton.edu

###### Abstract

We explore the methodology and theory of reward-directed generation via conditional diffusion models. Directed generation aims to generate samples with desired properties as measured by a reward function, which has broad applications in generative AI, reinforcement learning, and computational biology. We consider the common learning scenario where the dataset consists of majorly unlabeled data and a small set of data with noisy reward labels. Our approach leverages a learned reward function on the smaller data set as a pseudolabeler to label the unlabelled data. After pseudo-labelling, a conditional diffusion model (CDM) is trained on the data and samples are generated by setting a target value \(a\) as the condition in CDM. From a theoretical standpoint, we show that this directed generator can effectively learn and sample from the reward-conditioned data distribution: 1. our model is capable of recovering the data's latent subspace representation. 2. the model generates samples moving closer to the user-specified target. The improvement in rewards of samples is influenced by a interplay between the strength of the reward signal, the distribution shift, and the cost of off-support extrapolation. We provide empirical results to validate our theory and highlight the relationship between the strength of extrapolation and the quality of generated samples. Our code is available at https://github.com/Kaffaljjidhmah2/RCGDM.

## 1 Introduction

Controlling the behavior of generative models towards desired properties is a major problem for deploying deep learning models for real-world usage. As large and powerful pre-trained generative models achieve steady improvements over the years, one increasingly important question is how to adopt generative models to fit the needs of a specific domain and to ensure the generation results satisfying certain constraints (e.g., safety, fairness, physical constraints) without sabotaging the power of the original pre-trained model .

In this paper, we focus on directing the generation of diffusion models , a family of score-matching generative models that have demonstrated the state-of-the-art performances in various domains, such as image generation  and audio generation, with fascinating potentials in broader domains, including text modeling , reinforcement learning  and protein structure modeling . Diffusion models are trained to predict a clean version of the noised input, and generate data by sequentially removing noises and trying to find a cleaner version of the input. The denoising network (a.k.a. score network) \(s(x,t)\) approximates the score function \( p_{t}(x)\), and controls the behavior of diffusion models. People can incorporate any control information \(y\) as an additional input to \(s(x,y,t)\) during the training and inference .

Here we abstract various control goals as a scalar reward \(y\), measuring how well the generated instance satisfies our desired properties. In this way, the directed generation problem becomes finding plausible instances with higher rewards and can be tackled via reward-conditioned diffusion models. The subtlety of this problem lies in that the two goals potentially conflict with each other: diffusion models are learned to generate instances _similar to_ the training distribution, while maximizing the rewards of the generation drives the model to _deviate from_ the training distribution. In other words, the model needs to "interpolate" and "extrapolate" at the same time. A higher value of \(y\) provides a stronger signal that guides the diffusion model towards higher rewards, while the increasing distribution shift may hurt the generated samples' quality. In the sequel, we provide theoretical guarantees for the reward-conditioned diffusion models, aiming to answer the following question:

_How to provably estimate the reward-conditioned distribution via diffusion? How to balance the reward signal and distribution-shift effect, and ensure reward improvement in generated samples?_

**Our Approach.** To answer both questions, we consider a semi-supervised learning setting, where we are given a small dataset \(_{}\) with annotated rewards and a massive unlabeled dataset \(_{}\). We estimate the reward function using \(_{}\) and then use the estimator for pseudo-labeling on \(_{}\). Then we train a reward-conditioned diffusion model using the pseudo-labeled data. Our approach is illustrated in Figure 1. In real-world applications, there are other ways to incorporate the knowledge from the massive dataset \(_{}\), e.g., finetuning from a pre-trained model [35; 54]. We focus on the pseudo-labeling approach, as it provides a cleaner formulation and exposes the error dependency on data size and distribution shift. The intuition behind and the message are applicable to other semi-supervised approaches; see experiments in Section 5.2.

From a theoretical standpoint, we consider data point \(x\) having a latent linear representation. Specifically, we assume \(x=Az\) for some matrix \(A\) with orthonormal columns and \(z\) being a latent variable. The latent variable often has a smaller dimension, reflecting the fact that practical data sets often exhibit intrinsic low-dimensional structures [13; 48; 37]. The representation matrix \(A\) should be learned to promote sample efficiency and generation quality . Our theoretical analysis reveals an intricate interplay between reward guidance, distribution shift, and implicit representation learning; see Figure 2 for illustration.

**Contributions.** Our results are summarized as follows.

**(1)** We show that the reward-conditioned diffusion model implicitly learns the latent subspace representation of \(x\). Consequently, the model provably generates high-fidelity data that stay close to the subspace (Theorem 4.5).

**(2)** Given a target reward value, we analyze the statistical error of reward-directed generation, measured by the difference between the target value and the average reward of the generated population. In the case of a linear reward model, we show that this error includes the suboptimality gap of linear off-policy bandits with full knowledge of the subspace feature, if taking the target to be the maximum possible. In addition, the other two components of this error correspond to the distribution shift in score matching and the cost of off-support extrapolation (Theorem 4.6).

**(3)** We further extend our theory to nonparametric reward and distribution configurations where reward prediction and score matching are approximated by general function class, which covers the wildly adopted ReLU Neural Networks in real-world implementation (Section 4.3 and Appendix F).

**(4)** We provide numerical experiments under both synthesized setting and more realistic settings such as text-to-image generation (stable diffusion) and reinforcement learning (decision-diffuser) to support our theory (Section 5 and Appendix I).

To our best knowledge, our results present the first statistical theory for conditioned diffusion models and provably reward improvement guarantees for reward-directed generation.

## 2 Related Work

Guided Diffusions.For image generations, guiding the backward diffusion process towards higher log probabilities predicted by a classifier (which can be viewed as the reward signal) leads to improved sample quality, where the classifier can either be separated trained, i.e., classifier-guided  or implicitly specified by the conditioned diffusion models, i.e., classifier-free . Classifier-free guidance has become a standard technique in the state-of-the-art text-to-image diffusion models [39; 38; 4]. Other types of guidance are also explored [33; 14]. Similar ideas have been explored in sequence modelling problems. In offline reinforcement learning, Decision Diffuser  is a diffusion model trained on offline trajectories and can be conditioned to generate new trajectories with high returns, satisfying certain safety constraints, or composing skills. For discrete generations, Diffusion LM  manages to train diffusion models on discrete text space with an additional embedding layer and a rounding step. The authors further show that gradients of any classifier can be incorporated to control and guide the text generation.

Theory of Diffusion ModelsA line of work studies diffusion models from a sampling perspective. When assuming access to a score function that can accurately approximate the ground truth score function in \(L^{}\) or \(L^{2}\) norm, [9; 25] provide polynomial convergence guarantees of score-based diffusion models. "Convergence of denoising diffusion models under the manifold hypothesis" by Valentin De Bortoli further studies diffusion models under the manifold hypothesis. Recently,  and  provide an end-to-end analysis of diffusion models. In particular, they develop score estimation and distribution estimation guarantees using the estimated score function. These results largely motivate our theory, whereas, we are the first to consider conditional score matching and statistical analysis of conditional diffusion models.

Connection to Offline Bandit/RLOur off-policy regret analysis of generated samples is related to offline bandit/RL theory [30; 29; 6; 12; 22; 32; 5]. In particular, our theory extensively deals with distribution shift in the offline data set by class restricted divergence measures, which are commonly adopted in offline RL. Moreover, our regret bound of generated samples consists of an error term that coincides with off-policy linear bandits. However, our analysis goes far beyond the scope of bandit/RL.

## 3 Reward-Directed Generation via Conditional Diffusion Models

In this section, we develop a conditioned diffusion model-based method to generate high-fidelity samples with desired properties. In real-world applications such as image/text generation and protein design, one often has access to abundant unlabeled data, but relatively limited number of labeled data. This motivates us to consider a semi-supervised learning setting.

Figure 1: **Overview of reward-directed generation via conditional diffusion model. We estimate the reward function from the labeled dataset. Then we compute the estimated reward for each instance of the unlabeled dataset. Finally, we train a reward-conditioned diffusion model using the pseudo-labeled data. Using the reward-conditioned diffusion model, we are able to generate high-reward samples.**

Figure 2: **Illustrations of distribution shifts in samples, reward, and encoder-decoder score network. When performing reward-directed conditional diffusion, (a) the distribution of the generated data shifts, but still stays close to the feasible data support; (b) the distribution of rewards for the next generation shifts and the mean reward improves. (c) (Adapted from ) the score network for reward-directed conditioned diffusion adopts an Encoder-Decoder structure.**

**Notation**:\(P_{xy}\) denotes ground truth joint distribution of \(x\) and its label \(y\), \(P_{x}\) is the marginal of \(x\). Any piece of data in \(_{}\) follows \(P_{xy}\) and any data in \(_{}\) follows \(P_{x}\). \(P\) is used to denote a distribution and \(p\) denotes its corresponding density. \(P(x y=a)\) and \(P(x,y=a)\) are the conditionals of \(P_{xy}\) Similarly, we also use notation \(P_{x}\), \(P(x=a)\) for the joint and conditional of \((x,)\), where \(\) is predicted by the learnt reward model. Also, denote a generated distribution using diffusion by \(\) (density \(\)) followed by the same argument in parentheses as the true distribution it approximates, e.g. \((x y=a)\) is generated as an approximation of \(P(x y=a)\).

### Problem Setup

Suppose we are given an unlabeled data set \(_{}=\{x_{j}\}_{j=1}^{n_{1}}\) and a labeled data set \(_{}=\{(x_{i},y_{i})\}_{i=1}^{n_{2}}\), where it is often the case that \(n_{1} n_{2}\). Assume without loss of generality that \(_{}\) and \(_{}\) are independent. In both datasets, suppose \(x\) is sampled from an unknown population distribution \(P_{x}\). In our subsequent analysis, we focus on the case where \(P_{x}\) is supported on a latent subspace, meaning that the raw data \(x\) admits a low-dimensional representation (see Assumption 4.1). We model \(y\) as a noisy measurement of a reward function determined by \(x\), given by

\[y=f^{*}(x)+(0,^{2})  1>>0.\]

A user can specify a target reward value, i.e., \(y=a\). Then the objective of directed generation is to sample from the conditional distribution \(P(x|y=a)\). Given \(f^{*},P_{x}\) or the low-dimensional support of \(P_{x}\) are unknown, we need to learn these unknowns explicitly and implicitly through reward-conditioned diffusion.

### Meta Algorithm

```
1:Input: Datasets \(_{}\), \(_{}\), target reward value \(a\), early-stopping time \(t_{0}\), noise level \(\). (Note: in the following psuedo-code, \(_{t}(x)\) is the Gaussian density and \(\) is the step size of discrete backward SDE, see SS3.3 for elaborations on conditional diffusion)
2:Reward Learning: Estimate the reward function by \[*{argmin}_{f}_{(x_{i},y_{i}) _{}}(f(x_{i}),y_{i}),\] (3.1) where \(\) is a loss and \(\) is a function class.
3:Pseudo labeling: Use the learned function \(\) to evaluate unlabeled data \(_{}\) and augment it with pseudo labels: \(}=\{(x_{j},_{j})=(x_{j})+_{j} \}_{j=1}^{n_{1}}\) for \(_{j}}}{{}}(0,^{2})\).
4:Conditional score matching: Minimize over \(s\) (\(\) constructed as 3.8) on data set \(}\) via \[*{argmin}_{s}_{t_{0}}^{T} {}_{(x,)}}_{x^{ }((t)x,h(t)D_{})}[\|_{x^{ }}_{t}(x^{}|x)-s(x^{},,t)\|_{2}^{2} ]t.\] (3.2)
5:Conditioned generation: Use the estimated score \((,a,)\) to sample from the backward SDE: \[_{t}^{t,}=[_{k }^{y,}+(_{k}^{y,},a,T-k )]t+_{t}t[k,(k+1)],k[.\] (3.3)
6:Return: Generated population \((|=a)\), learned subspace representation \(V\) contained in \(\). ```

**Algorithm 1** Reward-Conditioned Generation via Diffusion Model (RCGDM)

In order to generate novel samples with both high fidelity and high rewards, we propose Reward-Conditioned Generation via Diffusion Models (RCGDM); see Algorithm 1 for details. By using the labeled data \(_{}\), we approximately estimate the reward function \(f^{*}\) by regression, then we obtain an estimated reward function \(\). We then use \(\) to augment the unlabeled data \(_{}\) with "pseudo labeling" and additive noise, i.e., \(}=\{(x_{j},_{j}=(x_{j})+_{j}) \}_{j=1}^{n_{1}}\) with \(_{j}(0,^{2})\) of a small variance \(^{2}\). Here, we added noise \(_{j}\) merely for technical reasons in the proof. We denote thejoint distribution of \((x,)\) as \(P_{x}\). Next, we train a conditional diffusion model using the augmented dataset \(\). If we specify a target value of the reward, for example letting \(y=a\), we can generate conditioned samples from the distribution \((x|=a)\) by backward diffusion.

**Alternative approaches.** In Line 4, Algorithm 1 trains the conditional diffusion model via conditional score matching. This approach is suitable when we have access to the unlabeled dataset and need to train a brand-new diffusion model from scratch. Empirically, in order to realize conditional generation, we can utilize a pre-trained diffusion model and incorporate with reward signals to be conditioned on. Existing methods falling in to this category include classifier-based guidance , fine-tuning , and self-distillation . For theoretical cleanness, we focus on analysing Algorithm 1 as it shares the same core essence with other alternative methods, which is approximating of the conditional score \( p_{t}(x_{t}|y)\).

### Training of Conditional Diffusion Model

In this section, we provide details about the training and sampling of conditioned diffusion in Algorithm 1 (Line 4: conditional score matching and Line 5: conditional generation). In Algorithm 1, conditional diffusion model is learned with \(}=\{(x_{j},_{j}=(x_{j})+ _{j})\}_{j=1}^{n_{1}}\), where \((x,) P_{x}\). For simplicity, till the end of this section we use \(y\) instead of \(\) to denote the condition variable. The diffusion model is to approximate the conditional probability \(P(x)\).

**Conditional Score Matching.** The working flow of conditional diffusion models is nearly identical to that of unconditioned diffusion models reviewed in Appendix A. A major difference is we learn a conditional score \( p_{t}(x|y)\) instead of the unconditional one. Here \(p_{t}\) denotes the marginal density function at time \(t\) of the following forward O-U process,

\[X_{t}^{y}=-g(t)X_{t}^{y}t+ W_{t} X_{0}^{y} P_{0}(x|y)t(0,T],\] (3.4)

where similarly \(T\) is a terminal time, \((W_{t})_{t 0}\) is a Wiener process, and the initial distribution \(P_{0}(x|y)\) is induced by the \((x,)\)-pair distribution \(_{x}\). Note here the noise is only added on \(x\) but not on \(y\). Throughout the paper, we consider \(g(t)=1\) for simplicity. We denote by \(P_{t}(x_{t}|y)\) the distribution of \(X_{t}^{y}\) and let \(p_{t}(x_{t}|y)\) be its density and \(P_{t}(x_{t},y)\) be the corresponding joint, shorthanded as \(P_{t}\). A key step is to estimate the unknown \( p_{t}(x_{t}|y)\) through denoising score matching . A conceptual way is to minimize the following quadratic loss with \(\), a concept class.

\[*{argmin}_{s}_{0}^{T}_{(x_{t},y)  P_{t}}[\| p_{t}(x_{t}|y)-s(x_{t},y,t)\|_{2}^{2}] t,\] (3.5)

Unfortunately, the loss in (3.5) is intractable since \( p_{t}(x_{t}|y)\) is unknown. Inspired by Hyvarinen and Dayan  and Vincent , we choose a new objective (3.2) and show their equivalence in the following Proposition. The proof is provided in Appendix C.1.

**Proposition 3.1** (**Score Matching Objective for Implementation**).: For any \(t>0\) and score estimator \(s\), there exists a constant \(C_{t}\) independent of \(s\) such that

\[_{(x_{t},y) P_{t}}[\| p_{t}(x_{t}|y )-s(x_{t},y,t)\|_{2}^{2}]\] \[=_{(x,y) P_{x}}_{x^{ }((t)x,h(t)I_{D})}[\|_{x^{}} _{t}(x^{}|x)-s(x^{},y,t)\|_{2}^{2}]+C_{t},\] (3.6)

where \(_{x^{}}_{t}(x^{}|x)=--(t)x}{h( t)}\), where \(_{t}(x^{}|x)\) is the density of \(((t)x,h(t)I_{D})\) with \((t)=(-t/2)\) and \(h(t)=1-(-t)\).

Equation (3.6) allows an efficient implementation, since \(P_{x}\) can be approximated by the empirical data distribution in \(}\) and \(x^{}\) is easy to sample. Integrating (3.6) over time \(t\) leads to a practical conditional score matching object

\[*{argmin}_{s}_{t_{0}}^{T}}_{( x,y) P_{x}}_{x^{}((t)x,h(t)I_{D})} [\|_{x^{}}_{t}(x^{}|x)-s(x^{},y,t)\|_{2}^{ 2}]t,\] (3.7)

where \(t_{0}>0\) is an early-stopping time to stabilize the training [44; 50] and \(}\) denotes the empirical distribution.

[MISSING_PAGE_FAIL:6]

**Assumption 4.3**.: The latent variable \(z\) follows distribution \(P_{z}\) with density \(p_{z}\), such that there exists constants \(B,C_{1},C_{2}\) verifying \(p_{z}(z)(2)^{-(d+1)/2}C_{1}(-C_{2}\|z\|_{2}^{2}/2)\) whenever \(\|z\|_{2}>B\). And ground truth score is realizable: \( p_{t}(x)\).

**Assumption 4.4**.: Further assume \(z(0,)\) with its covariance matrix \(\) satisfying \(_{}I_{d}_{}I_{d}\) for \(0<_{}_{} 1\).

**Theorem 4.5** (**Subspace Fidelity of Generated Data**).: Under Assumption 4.1, if Assumption 4.3 holds with \(c_{0}I_{d}_{z P_{z}}[zz^{}]\), then with high probability on data,

\[(V,A)=}(}(,1/n_{1})D}{n_{1}}})\] (4.2)

with \((,1/n_{1})\) being the log covering number of function class \(\) as in (3.8). When Assumption 4.4 holds, \((,1/n_{1})=((d^{2}+Dd)(Ddn_{1}))\) and thus \((V,A)=}(}+D^{2}d)}{n_{1}}})\). Further under Assumption 4.2, it holds that

\[_{x_{a}}[\|x_{}\|_{2}]= (D}+}{\|^{*}\|_{ }}+d}),\] (4.3)

where \(^{*}\) is groundtruth parameter of linear model.

### Provable Reward Improvement via Conditional Generation

Let \(y^{*}\) be a target reward value and \(P\) be a generated distribution. Define the suboptimality of \(P\) as

\[(P;y^{*})=y^{*}-_{x P}[f^{*}(x)],\]

which measures the gap between the expected reward of \(x P\) and the target value \(y^{*}\). In the language of bandit learning, this gap can also be viewed as a form of _off-policy regret_. Given a target value \(y^{*}=a\), we want to derive guarantees for \((_{a};y^{*}=a)\), recall \(_{a}:=(|=a)\) denotes the generated distribution. In Theorem 4.6, we show \((_{a};y^{*}=a)\) comprises of three components: off-policy bandit regret which comes from the estimation error of \(\), on-support and off-support errors coming from approximating conditional distributions with diffusion.

**Theorem 4.6** (**Off-policy Regret of Generated Samples**).: Suppose Assumption 4.1, 4.2 and 4.4 hold. We choose \(=1\), \(t_{0}=((Dd^{2}+D^{2}d)/n_{1})^{1/6}\) and \(=1/\). With high probability, running Algorithm 1 with a target reward value \(a\) gives rise to

\[(_{a};y^{*}=a)\] \[(_{ }^{-1}_{P_{a}})}}_{_{1}}+ _{P_{a}}[g^{*}(x_{})]-_{ _{a}}[g^{*}(x_{})]|}_{_{2}}+_{_{a}}[h^{*}(x_{})]}_{_{3}},\] (4.4)

where \(_{}:=}(X^{}X+ I)\) where \(X\) is the stack matrix of \(_{}\) and \(_{P_{a}}=_{P_{a}}[xx^{}]\).

Implications and Discussions:**(1)** Equation (4.4) decomposes the suboptimality gap into two separate parts of error: error from reward learning (\(_{1}\)) and error coming from diffusion (\(_{2}\) and \(_{3}\)).

**(2)** It is also worth mentioning that \(_{2}\) and \(_{3}\) depend on \(t_{0}\) and that by taking \(t_{0}=((Dd^{2}+D^{2}d)/n_{1})^{1/6}\) one gets a good trade-off and small \(_{2}+_{3}\).

**(3)**\(_{1}\) suggests diffusion model is essentially doing representation learning, reducing \(D\) to smaller latent dimension \(d\). It can be seen from \((_{}^{-1}_{p_{q}})(}{\|^{*}\|_{}}+d)\) when \(n_{2}=(})\).

**(3)** If we ignore the diffusion errors when \(n_{1}\) is large enough, the suboptimatliy gap resembles the suboptimatliy of off-policy bandit learning in feature subspace [22, Section 3.2], . It showsthe major source of error occurs when moving towards the target distributions and the error behaves similar to bandit.

**(4)** On-support diffusion error entangles with distribution shift in complicated ways. We show

\[_{2}=((a)(d^{2}D+D^{2}d)^{1/ 6}{n_{1}}^{-1/6} a),\]

where \((a)\) quantifies the distribution shift depending on different reward values. In the special case of the latent covariance matrix \(\) is known, we can quantify the distribution shift as \((a)=(a d)\). We observe an interesting phase shift. When \(a<d\), the training data have a sufficient coverage with respect to the generated distribution \(_{a}\). Therefore, the on-support diffusion error has a lenient linear dependence on \(a\). However, when \(a>d\), the data coverage is very poor and \(_{2}\) becomes quadratic in \(a\), which quickly amplifies.

**(5)** When generated samples deviate away from the latent space, the reward may substantially degrade as determined by the nature of \(h\).

To the authors' best knowledge, this is a first theoretical attempt to understand reward improvement of conditional diffusion. These results imply a potential connection between diffusion theory and off-policy bandit learning, which is interesting for more future research. See proofs in Appendix D.3.

### Extension to Nonparametric Function Class

Our theoretical analysis, in its full generality, extends to using general nonparametric function approximation for both the reward and score functions. To keep our paper succinct, we refer to Appendix F and Theorem F.4 for details of our nonparametric theory for reward-conditioned generation. Informally, the regret of generated samples is bounded by

\[(_{a};y^{*}=a)=}( (a)(n_{2}^{-}+n_{1}^{-})}}))+_{_{a}}[h^{*}(x_{ })]\]

with high probability. Additionally, the nonparamtric generators is able to estimate the representation matrix \(A\) up to an error of \((V,A)=}(n_{1}^{-} }})\). Here the score is assumed to be Lipschitz continuous and \(\) is the smoothness parameter of the reward function, and \((a)\) is a class-restricted distribution shift measure. Our results on nonparametric function approximation covers the use of deep ReLU networks as special cases.

## 5 Numerical Experiments

### Simulation

We first perform the numerical simulation of Algorithm 1 following the setup in Assumption 4.1, 4.2 and 4.4. We choose \(d=16,D=64,g^{*}(x):=5\|x\|_{2}^{2}\), and generate \(^{*}\) by uniformly sampling from the unit sphere. The latent variable \(z\) is generated from \((0,_{})\), which is then used to construct \(x=Az\) with some randomly generated orthonormal matrix \(A\). We use the \(1\)-dimensional version of the UNet  to approximate the score function. More details are deferred to Appendix 1.

Figure 3 shows the average reward of the generated samples under different target reward values. We also plot the distribution shift and off-support deviation in terms of the \(2\)-norm distance from the support. For small target reward values, the generation average reward almost scales linearly with the target value, which is consistent with the theory as the distribution shift remains small for these target values. The generation reward begins to decrease as we further increase the target reward value, and the reason is two fold. Firstly, the off-support deviation of the generated samples becomes large in this case, which prevents the generation reward from further going up. Secondly, the distribution shift increases rapidly as we further increase the target value, making the theoretical guarantee no longer valid. In Figure 4, we show the distribution of the rewards in the generated samples. As we increase the target reward values, the generation rewards become less concentrated and are shifted to the left of the target value, which is also due to the distribution shift and off-support deviation.

### Directed Text-to-Image Generation

Next, we empirically verify our theory through directed text-to-image generation. Instead of training a diffusion model from scratch, we use Stable Diffusion v1.5 , pre-trained on LALION dataset . Stable Diffusion operates on the latent space of its Variational Auto-Encoder and can incorporate text conditions. We show that by training a reward model we can further guide the Stable Diffusion model to generate images of desired properties.

**Ground-truth Reward Model.** We start from an ImageNet  pre-trained ResNet-18  model and replace the final prediction layer with a randomly initialized linear layer of scalar outputs. Then we use this model as the ground-truth reward model. To investigate the meaning of this randomly-generated reward model, we generate random samples and manually inspect the images with high rewards and low rewards. The ground-truth reward model seems to favor colorful and vivid natural scenes against monochrome and dull images; see Appendix 1 for sample images.

**Labelled Dataset.** We use the ground-truth reward model to compute a scalar output for each instance in the CIFAR-10  training dataset and perturb the output by adding a Gaussian noise from \((0,0.01)\). We use the images and the corresponding outputs as the training dataset.

**Reward-network Training.** To avoid adding additional input to the diffusion model and tuning the new parameters, we introduce a new network \(_{}\) and approximate \(p_{t}(y|x_{t})\) by \((_{}(}),^{2})\). For simplicity, we set \(^{2}\) as a tunable hyperparameter. We share network parameters for different noise levels \(t\), so our \(_{}\) has no additional input of \(t\). We train \(_{}\) by minimizing the expected KL divergence between \(p_{t}(y|x_{t})\) and \((_{}(}),^{2})\):

\[_{}_{x_{t}}(p_{t}(y|x_{t}) (_{}(}),^{2}))=_{ }_{(_{t},y)}}-_{}(})\|_{2}^{2}}{2^{2}}+.\]

Equivalently, we train the reward model \(_{}\) to predict the noisy reward \(y\) from the noisy inputs \(x_{t}\). Also, notice that the minimizers of the objective do not depend on the choice of \(^{2}\).

**Reward-network-based Directed Diffusion.** To perform reward-directed conditional diffusion, observe that \(_{x} p_{t}(x|y)=_{x} p_{t}(x)+_{x} p_{t}(y|x)\), and \(p_{t}(y|x)-(x)\|_{2}^{2}}{2^{2} }\). Therefore,

\[_{x} p_{t}(y|x)=-1/^{2}_{x}\|y- _{}(x)\|_{2}^{2}.\]

In our implementation, we compute the gradient by back-propagation through \(_{}\) and incorporate this gradient guidance into each denoising step of the DDIM sampler  following  (equation (14)). We see that \(1/^{2}\) corresponds to the weights of the gradient with respect to unconditioned score. In the sequel, we refer to \(1/^{2}\) as the "guidance level", and \(y\) as the "target value".

Figure 4: **Shifting reward distribution of the generated population.**

Figure 3: **Quality of generated samples as target reward value increases. Left: Average reward of the generation; Middle: Distribution shift; Right: Off-support deviation. The errorbar is computed by \(2\) times the standard deviation over \(5\) runs.**

**Quantitative Results.** We vary \(1/^{2}\) in \(\{25,50,100,200,400\}\) and \(y\) in \(\{1,2,4,8,16\}\). For each combination, we generate 100 images with the text prompt "A nice photo" and calculate the mean and the standard variation of the predicted rewards and the ground-truth rewards. The results are plotted in Figure 5. From the plot, we see similar effects of increasing the target value \(y\) at different guidance levels \(1/^{2}\). A larger target value puts more weight on the guidance signals \(_{x}_{}(x)\), which successfully drives the generated images towards higher predicted rewards, but suffers more from the distribution-shift effects between the training distribution and the reward-conditioned distribution, which renders larger gaps between the predicted rewards and the ground-truth rewards. To optimally choose a target value, we must trade off between the two counteractive effects.

**Qualitative Results.** To qualitatively test the effects of the reward conditioning, we generate a set of images with increasing target values \(y\) under different text prompts and investigate the visual properties of the produced images. We isolate the effect of reward conditioning by fixing all the randomness during the generation processes, so the generated images have similar semantic layouts. After hyper-parameter tuning, we find that setting \(1/^{2}=100\) and \(y\{2,4,6,8,10\}\) achieves good results across different text prompts and random seeds. We pick out typical examples and summarized the results in Figure 6, which demonstrates that as we increase the target value, the generated images become more colorful at the expense of degradations of the image qualities.

## 6 Conclusion

In the paper, we study the problem of generating high-reward and high-quality samples using reward-directed conditional diffusion models, focusing on the semi-supervised setting where massive unlabeled data and limited labeled data are given. We provide theoretical results for subspace recovery and reward improvement, demonstrating the trade-off between the strength of the reward target and the distribution shift. Numerical results support our theory well.

Figure 5: **The predicted rewards and the ground-truth rewards of the generated images**. At each guidance level, increasing the target \(y\) successfully directs the generation towards higher predicted rewards, but also increases the error induced by the distribution shift. The reported baseline is the expected ground-truth reward for undirected generations.

Figure 6: **The effects of the reward-directed diffusion. Increasing the target value directs the images to be more colorful and vivid at the cost of degradation of the image qualities. Leftmost: without reward conditioning. Second-to-Last: target value \(y=2,4,6,8,10\). The guidance level \(1/^{2}\) is fixed to \(100\). The text prompts are ”A cat with a glass of water.”, ”An astronaut on the horseback”.**