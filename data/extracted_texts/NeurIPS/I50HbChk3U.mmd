# Provably Bounding Neural Network Preimages

Suhas Kotha\({}^{*}\)

Carnegie Mellon

suhask@andrew.cmu.edu &Christopher Brix\({}^{*}\)

RWTH Aachen

brix@cs.rwth-aachen.de &Zico Kolter

Carnegie Mellon

Bosch Center for AI

zkolter@cs.cmu.edu &Krishnamurthy (Dj) Dvijotham\({}^{}\)

Google DeepMind

dvijothamcs@gmail.com &Huan Zhang\({}^{}\)

UIUC

huan@huan-zhang.com

###### Abstract

Most work on the formal verification of neural networks has focused on bounding the set of outputs that correspond to a given set of inputs (for example, bounded perturbations of a nominal input). However, many use cases of neural network verification require solving the inverse problem, or over-approximating the set of inputs that lead to certain outputs. We present the INVPROP algorithm for verifying properties over the preimage of a linearly constrained output set, which can be combined with branch-and-bound to increase precision. Contrary to other approaches, our efficient algorithm is GPU-accelerated and does not require a linear programming solver. We demonstrate our algorithm for identifying safe control regions for a dynamical system via backward reachability analysis, verifying adversarial robustness, and detecting out-of-distribution inputs to a neural network. Our results show that in certain settings, we find over-approximations over \(2500\) tighter than prior work while being \(2.5\) faster. By strengthening robustness verification with output constraints, we consistently verify more properties than the previous state-of-the-art on multiple benchmarks, including a large model with 167k neurons in VNN-COMP 2023. Our algorithm has been incorporated into the \(\),\(\)-CROWN verifier, available at [https://abcrown.org](https://abcrown.org).

## 1 Introduction

Applying neural networks to safety-critical settings often requires reasoning about constraints on the inputs and outputs of a neural network. For example, for a physical system controlled by a neural network policy, it is of interest to understand which initial states will lead to an unsafe state such as colliding with an obstacle. Formal verification of neural networks seeks to provide provable guarantees demonstrating that networks satisfy formal specifications on their inputs and outputs. Most work to date has focused on developing algorithms that can bound the outputs of a neural network given constraints on the inputs, which can be used for applications such as analyzing the robustness of a neural network to perturbations of a given input (Wong and Kolter, 2018; Dvijotham et al., 2018; Zhang et al., 2018; Raghunathan et al., 2018; Gehr et al., 2018).

In this work, we address the inverse problem of over-approximating a neural network's preimage: given a set of outputs \(_{}\) (described by linear constraints on the network output), we seek to find a set that provably contains all inputs that lead to such outputs. For example, for our neural network policy, this would correspond to the states that collide with an obstacle one step in the future. Though the verification problem is already challenging due to non-convexity and high dimensionality, this new problem is even more difficult since neural networks are generally not invertible.

Specifically, representative efficient verifiers (such as state-of-the-art bound-propagation-based methods (Zhang et al., 2022)) can only compute bounds utilizing constraints on the input and critically depend on having tight bounds on intermediate activations. In the setting of this paper, however, the bounds derived from the input constraints are almost vacuous, since the only constraints on the input are that it should be from the valid input domain. We efficiently solve this problem by significantly generalizing the existing bound propagation-based verification framework, allowing one to leverage output constraints when tightening the intermediate activations. Our contributions are as follows:

* We develop an effective bound propagation framework, Inverse Propagation for Neural Network Verification (INVPROP), for the _inverse verification problem_ for neural networks, i.e., the problem of over-approximating the set of inputs that leads to a given set of outputs. Importantly, INVPROP requires no linear programming solver and can compute bounds on any intermediate layer.
* We unify INVPROP and traditional bound propagation into a more general verification framework, allowing us to connect our method to standard tools, such as the state-of-the-art bound propagation tool \(\),\(\)-CROWN (Zhang et al., 2018; Xu et al., 2021; Wang et al., 2021; Zhang et al., 2022; Brix et al., 2023; Muller et al., 2023). Our contribution allows \(\),\(\)-CROWN to tighten intermediate bounds with respect to output constraints, which could not be done by the original tool.
* We demonstrate that tight inverse verification requires multiple iterative refinements of intermediate bounds. While layer bounds in standard bound propagation only depend on the bounds of their predecessors, INVPROP incorporates bounds of _all_ layers in the network.
* We improve the state of the art on a control benchmark (Rober et al., 2022, 2022) by providing \(2500\) tighter bounds, \(2.5\) faster, for a Double Integrator and \(257\) tighter bounds, \(3.29\) faster, for a 6D Quadrotor. Furthermore, we demonstrate that INVPROP can strengthen robustness verification with output constraints and verify more robustness properties in less time compared to existing tools. Finally, we demonstrate its applicability in OOD detection.

## 2 Background and Problem Setup

### Notation

We use \([L]\) for \(L\) to refer to the set \(\{1,2,,L\}\), \(_{:,j}^{(i)}\) to refer to column \(j\) of the matrix \(^{(i)}\), \([]_{+}\) to refer to \((0,)\), and \([]_{-}\) to refer to \(-(0,)\). We use boldface symbols for vectors and matrices (such as \(^{(i)}\) and \(^{(i)}\)) and regular symbols for scalars (such as \(x_{j}^{(i)}\)). We use \(\) to denote element-wise multiplication of vectors \(,\).

We define an \(L\) layer ReLU neural network by its weight matrices \(^{(i)}\) and bias vectors \(^{(i)}\) for \(i[L]\). The output of the neural network for the input \(}^{(0)}\) from a bounded input domain \(\) is computed by alternately applying linear layers \(^{(i)}=^{(i)}}^{(i-1)}+^{(i)}\) and ReLU layers \(}^{(i)}=(,^{(i)})\) until we receive the output \(^{(L)}\) (which we refer to as the logits). Note that we treat softmax as a component of the loss function, not the neural network.

### Problem Statement

Given a neural network \(f:^{}^{}\) and an output constraint \(_{}^{}\), we want to compute \(f^{-1}(_{})\). Since precisely computing or expressing \(f^{-1}(_{})\) is an intractable problem in general, we strive to compute a tight over-approximation \(_{}\) such that \(f^{-1}(_{})_{}\). In particular, we target the convex hull of the preimage via a cutting-plane representation. \(_{}\) will be defined by a set of linear constraints parameterized by \(f()+\) in this work.

### Applications

Backward Reachability Analysis for Neural Feedback Loops.Establishing safety guarantees for neural network policies is a challenging task. One problem of interest is to find a set of initial states that does not reach a particular set of future states under the neural network policy. This can be helpful in collision avoidance or control with safety constraints. For example, consider a discrete-time double integrator controller (Hu et al., 2020) where the state at time \(t+1\) can be directly computed based on state at time \(t\) following the equation

\[_{t+1}=f(_{t})=1&1\\ 0&1_{t}+0.5\\ 1(_{t})\]with policy \(:^{2}\). If there is an obstacle in the room covering the region \([4.5,5.0][-0.25,0.25]\), it is of interest to understand which states will enter the unsafe region in the next time-step. We can represent this obstacle set with linear constraints that define \(_{}\).

\(f^{-1}(_{})\) denotes the set of states \(}\) such that \(}\) lies in the unsafe region given the control policy \(\). Overapproximating this set allows us to define the set of states to avoid one timestep in advance. We can compose \(f^{-1}\) with itself \(t\) times to obtain the set of initial states where \(\) would drive the system to the unsafe set after \(t\) steps.

Robustness VerificationAdversarial robustness is one classic problem for neural network verification where verifiers assess whether all perturbations of an input yield the same classification. INVPROP can be used to speed up this verification by exploiting an implicit output constraint. We follow Wang et al. (2021) and use the canonical form of verifying a property by proving that \(_{x}f(x)>0\). \(f(x)\) will be negative if and only if \(x\) is an adversarial example. This minimization problem can be rewritten as \(_{x X f(x) 0}f(x)\)(Yang et al., 2021). If no adversarial example exists, the infimum is computed over an empty set and returns positive infinity, indicating local robustness. If at least one adversarial example exists, the result is guaranteed to be negative. With the output constraint \(f(x) 0\), we only analyze inputs that yield an incorrect classification, reducing the search space and tightening verification bounds.

Determining what inputs lead to confident predictions.It is challenging to train a classifier that knows when it doesn't know. A simple and effective approach is to train with an additional logit representing OOD abstention. Labeled data for this additional class is generated via outlier exposure, or adding synthetic training data known to be OOD (Hendrycks et al., 2018; Chen et al., 2021).

Consider the logits \(y\) produced by a binary classifier trained in this manner. The classifier can classify in-distribution data by comparing \(y_{0}\) and \(y_{1}\), the logits corresponding to the two in-distribution labels. The quantity \((y_{0},y_{1})-y_{2}\), known as the logit gap, can be used to identify out-of-distribution data (Fig. 7 in Appendix A). Can the logit gap correctly identify data far from the training data as being OOD? To answer this question, we can compute the preimage of the set

\[_{}=\{:(y_{0},y_{1}) y_{2}\}\]

We detail how to model this set with linear constraints in Appendix F.1. INVPROP enables us to over-approximate \(f^{-1}(_{})\), answering whether the classifier learned to correctly identify OOD data.

## 3 Approach

### Convex over-approximation

Suppose we want to find a convex over-approximation of \(f^{-1}(_{})\). The tightest such set is its convex hull, which is the intersection of all half-spaces that contain \(f^{-1}(_{})\)(Boyd et al., 2004). This intersection is equivalent to \(_{^{n}}\{:^{}_{f(^{})_{}}^{}^{}\}\), which means that we can build an over-approximation by taking this intersection for finitely many \(\). Furthermore, replacing the minimization problem with a lower bound to its value still yields a valid half-space, where a tighter bound yields a tighter approximation.

We focus on convex over-approximations for two reasons: First, checking that the preimage satisfies a linear constraint \(^{}+d 0\) is equivalent to minimizing the linear function \(^{}\) over the preimage, which is in turn equivalent to minimizing the linear function over \((f^{-1}(_{}))\)(Boyd et al., 2004). Second, the convex hull and its tractable over-approximations are conveniently represented as intersections of linear constraints on the preimage, each of which can be quickly computed after a single precomputation phase to tighten bounds on intermediate neurons using the INVPROP algorithm outlined in the next section.

For the above reasons, we will focus on solving the following constrained optimization problem.

\[_{}^{};; f()_{} \]

Note that this differs from the widely studied forward verification problem, which can be phrased as

\[_{_{}}^{}f()\]

where \(_{}\) is a set representing constraints on the input, and the goal of the verification is to establish that the output \(f()\) satisfies a linear constraint of the form \(^{}f() d\) for all \(x_{}\).

### The INVPROP Algorithm

As a brief overview of our method, we first construct the Mixed Integer Linear Program (MILP) for solving this optimization problem, which generates the over-approximation \(_{}\) when solved for finitely many \(\). Then, we relax the MILP to a Linear Program (LP), which can construct the over-approximation \(_{}\). Finally, we relax the LP via its Lagrangian dual, which will be used to construct our over-approximation \(_{}\). This chain of relaxations is visualized in Figure 1.

The Mixed Integer Linear Programming (MILP) FormulationFor feed-forward ReLU neural networks, the non-linearities from the max operator can be encoded via integer variables, and this problem admits a MILP encoding similar to prior work in adversarial robustness (Tjeng et al., 2017). Problem (1) is equivalent to:

\[_{,}} ^{}\] (2a) s.t. \[;}^{(0)}=;^{(L)}+; \] \[^{(i)}=^{(i)}}^{(i-1)}+^{(i )} i[L];\] (2c) \[}^{(i)}=(0,^{(i)}); i[L-1] \]

The Linear Programming (LP) FormulationUnfortunately, finding an exact solution to this MILP is NP-complete (Katz et al., 2017). To sidestep the intractability of exact verification, we can compute lower bounds for this program via its convex relaxation. We consider bounds on the outputs of intermediate layers:

\[l_{j}^{(i)} x_{j}^{(i)} u_{j}^{(i)}\]

Based on these bounds, we can take the ReLU triangle relaxation (Ehlers, 2017) to get the LP

\[_{,} ^{}\] (3a) s.t. \[;^{(0)}^{(0)}; }^{(0)}=;^{(L)}+  \] \[^{(i)}=^{(i)}}^{(i-1)}+^{(i)}\] (3c) \[}^{(i)};^{(i)} }^{(i)};}^{(i)}^{(i)}}{^{(i)}-^{( i)}}(^{(i)}-^{(i)}); i[L] \]

where the bounds \(^{(0)}^{(0)}\) are either the bounds implicit in \(\), or a refinement of these obtained via previous rounds of bound propagation or input branching (see Algorithm 1 for details). Most efficient neural network verifiers do not solve an LP formulation of verification directly because LP solvers are often slow for problem instances involving neural networks. Instead, the bound propagation framework (Zhang et al., 2018; Wang et al., 2021; Zhang et al., 2022) is a practical and efficient way to lower bound the LP relaxation of the forward verification problem. However, there are _two major roadblocks_ to applying existing methods here: Typical bound-propagation cannot directly handle the output constraints (Eq. 3b) and the objective involving input layer variables (Eq. 3a). This is true for optimizing bounds on the input, intermediate layers, and the output layer, all of which need to be iteratively tightened as demonstrated later.

Figure 1: **Visualization of relaxations. The inner green region depicts the true \(f^{-1}(_{})\), the blue relaxation depicts the intersection of finite half-spaces solved via MILP, the red relaxation displays the same via LP, and the purple relaxation displays the same via INVPROP. Though this diagram displays looseness, we provide a comprehensive methodology to reduce the error in all three relaxations up to arbitrary precision (Section 3.3).**The Inverse Propagation (INVPROP) FormulationBy changing the LP above to optimize the quantity \(_{j}^{(0)}\) (input layer) or \(x_{j}^{(i)}\) (intermediate layers) for \(i[L-1]\), the bounds for the \(j\)-th neuron of layer \(i\) can be tightened in separate LP calls. However, this program is too expensive to be run multiple times for each neuron.

Inspired by the success of CROWN-family neural network verifiers , we efficiently lower bound the solution of the LP by optimizing its Lagrangian dual. This dual is highly structured , allowing us to bound input half-spaces \(^{}\) and intermediate bounds \(l_{j}^{(i)},u_{j}^{(i)}\) by closed-form expressions of the dual variables. Our main generalization is the ability to optimize input or intermediate layer bounds with the output constraints **after** them in the neural network, as shown in the following theorem.

**Theorem 1** (Bounding input half-spaces).: _Given an output set \(_{}=\{:+\}\) and vector \(\), \(g_{}(,)\) is a lower bound to the linear program in (3) for \(\), \(\), and \(g_{}\) defined via_

\[g_{}(,)= [^{}-^{(1)}^{(1)}]_{+} ^{(0)}-[^{}-^{(1)}^{(1)}]_{- }^{(0)}\] \[-_{i=1}^{L}^{(i)}^{(i)}+_{i=1}^{L-1} _{j^{(i)}}[^{(i)}l_{j}^{(i)}[_{j} ^{(i)}]_{+}}{u_{j}^{(i)}-l_{j}^{(i)}}]\]

_where every term can be directly recursively computed via_

\[^{-(i)} =\{j:u_{j}^{(i)} 0\};^{+(i)}=\{j:l_{j}^{(i)}  0\};^{(i)}=\{j:l_{j}^{(i)}<0<u_{j}^{(i)}\}\] \[^{(L)} =-;_{j}^{(i)}=^{(i+1)} _{:,j}^{(i+1)} i[L-1]\] \[_{j}^{(i)} =\{_{j}^{(i)},&j^{ +(i)}\\ 0,&j^{-(i)}\\ ^{(i)}}{u_{j}^{(i)}-l_{j}^{(i)}}[_{j}^{(i)}]_{+}-_{j }^{(i)}[_{j}^{(i)}]_{-},&j^{(i)}.\]

Proof.: Full proof is presented in Appendix D. 

In Appendix C, we show how to bound intermediate neurons in the network using a similar approach. Since the intermediate bounds might depend on bounds on neurons in subsequent layers (due to the output constraint), we cannot simply optimize bounds in a single forward pass layer by layer, unlike prior work such as \(\)-CROWN . Instead, we must iteratively tighten intermediate layer bounds with respect to the tightest bounds on all neurons computed thus far. This iterative approach can tighten the initially loose bounds by several orders of magnitude, as shown in Figure 2. After performing this procedure once, the intermediate bounds can be used to tightly lower bound \(^{}\) for any \(\) via Theorem 1. Therefore, this computation can be shared across all the constraints \(\) we use to describe \(_{}\). Our algorithm can be expressed in terms of forward/backward passes through layers of the neural network and implemented via standard deep learning modules in libraries like PyTorch . Since all of the operations are auto-differentiable, we can tighten our lower bound using standard gradient ascent (projected by the dual variable constraints).

Figure 2: **Necessity of Iterative Tightening.** Our approach enables us to iteratively tighten the bounds of all layers, with each iteration allowing for a smaller approximation ratio with respect to the true preimage. Green: Sum of bound intervals for all neurons in the third layer (second hidden layer); Blue: ratio between volumes of over-approximation and preimage. Measured for step \(t=10\) of the control benchmark defined in Section 4.1. Note that the improvement from iterative tightening is two orders of magnitude for intermediate bounds, and four orders of magnitude for the volume of the over-approximation.

**Algorithm 1** INVPROP. Can be applied to branches for non-convex overapproximations. Lower and upper bound of all neurons and all constraints are optimized using distinct \(\) and \(\) values in \(g\).

Initialize \(^{(i)},^{(i)}\) via cheap bound propagation methods for the forward verification problem while lower bounds for \(^{}}^{(0)}\) are improving do for\(i\{L-1,L-2,,1,0\}\)do for\(j\) layer \(i\) neurons, \(b\) {lower, upper} do  Optimize \(g^{b}_{ij}\) from Theorem 2 via gradient ascent to improve bound on neuron \(j\) in layer \(i\) with sense \(b\) (lower/upper) if\(b=\)upper then update \(u^{(i)}_{j}\) else update \(l^{(i)}_{j}\) Optimize \(g_{}\) from Theorem 2 via gradient ascent to improve lower bound on \(^{}\) for all \(\)

**Connection to forward verification** Our bound in Theorem 1 introduces the dual variable \(\), which enforces the output constraint during optimization. In fact, we can use this variable to get a better conceptual interpretation of our result. For the optimization problem in (1), taking the dual with respect to the constraint \(f()+\) yields the lower bound

\[_{}_{} ^{}+^{}(f()+)\] s.t. \[; 0\]

The objective can be represented as minimizing a linear function of the output of a residual neural network with a skip connection from the input to the output layer, subject to constraints on the input. Now, \(f()\) appears in the objective, similar to the standard forward verification problem with an augmented network architecture. Similarly, optimizing an intermediate bound is equivalent to a skip connection from layer \(i\) to the output (Appendix E). These connections allow us to implement our method using standard verification tools (Brix et al., 2023; Xu et al., 2021, 2020).

When \(f\) is a feedforward ReLU network, the lower bound described in this section is precisely the same as Theorem 1 (since both are solving the dual of the same linear program). This shows that the introduction of \(\) constitutes our generalization to the bound propagation framework.

Selection of \(\)The dependence of the optimization on \(\) is not a major limitation. The bounds of all intermediate neurons are optimized only with respect to the input and output constraints, not the hyperplanes used to describe the input half-space. Thus, their optimized bounds can be shared across the optimization of all input hyperplanes. For applications such as adversarial robustness, the box constraint of two hyperplanes per dimension could be chosen to scalably bound the input. The selection of appropriate \(\) depends on the application and is not our main focus.

### Branch and Bound

Though more rounds of iterative tightening will lower the gap in \(_{}_{}\), our current formulation still faces two sources of looseness: the gap in \(_{}_{}\) and the gap in \(_{} f^{-1}(_{})\). To overcome both of these issues, we can make use of branching (Bunel et al., 2020). While there are several possibilities here, we focus on input branching, which gave the biggest empirical gains in our experiments described in Section 4. More concretely, we divide the input domain \(=[^{(0)},^{(0)}]\) into several regions by choosing a coordinate \(i\) and computing

\[_{a}=\{:_{i} s_{i}\},_{b} =\{:_{i} s_{i}\}\]

so that \(=_{a}_{b}\). Doing this recursively, we obtain several regions and can compute an overapproximation of \(f^{-1}(_{})\) when the input is in each of those regions, and take a union of the resulting sets. The finer the input domain of our over-approximation, the tighter the approximation.

## 4 Results

We evaluate INVPROP on three different benchmarks. Across benchmarks, we find orders of magnitude improvement over prior work, and our methods work even for relatively large networks (167k neurons) and high dimensionality inputs (8112 dimensions). All implementation details are described in Appendix F and the utilized hardware is described in Appendix G.

[MISSING_PAGE_FAIL:7]

ReBreach-LP with 15625 output partitions. We run our algorithm with no branching. In Figure 4, we compare these three strategies, and we find that our over-approximation of the pre-image (in blue) is smaller than the state-of-the-art over-approximations by \(257\) times while being \(3.29\) times faster. Therefore, our algorithm scales well to higher dimensional control examples.

### Robustness Verification

Similar to us, Yang et al. (2021) encode the implicit output constraint of the local robustness verification query to tighten the bounds on neurons. However, they need an LP solver, which does not scale to large problems. The state-of-the-art verification toolkit \(\),\(\)-CROWN (Zhang et al., 2018; Xu et al., 2021; Wang et al., 2021; Zhang et al., 2022) is able to scale to large networks, but does not currently utilize the implicit output constraint. We demonstrate the benefit of encoding the output constraint by extending \(\),\(\)-CROWN and comparing the performance on the benchmark used by Yang et al. (2021) as well as benchmarks from the VNN-COMP 2023 (Brix et al., 2023).

MnistYang et al. (2021) provides four networks with ReLU activation functions and dense linear layers of 2/6/6/9 layers of 100/100/200/200 neurons, each trained on MNIST. For each network, 50 inputs were tested for local robustness. For the complete benchmark definition, we refer to Yang et al. (2021). The DeepSRGR results are taken from Yang et al. (2021), they do not report a timeout for their experiments. Both \(\),\(\)-CROWN and \(\),\(\)-CROWN+INVPROP were run with a per-input timeout of 5 minutes. Except for the input bounds, all bounds of all layers of each network are tightened using output constraints. We report the results in Table 3. Notably, we can verify more instances than the SOTA tool \(\),\(\)-CROWN, in sometimes less than one fifth the average runtime. We include a more detailed comparison between the results for \(\),\(\)-CROWN and \(\),\(\)-CROWN+INVPROP in Appendix H.

Vnn-Comp '23: YoloIn 2023, the VNN-COMP contained YOLO as an unsocored benchmark for object detection. It is a modified version of YOLOv2 (Redmon and Farhadi, 2016) with a network of 167k neurons and 5 residual layers, and is available at (Zhong, 2023; Brix, 2023). The network processes \(3 52 52\) images and uses convolutions, average pooling and ReLU activations. This benchmark is well suited for INVPROP, as the definition of the adversarial examples is a conjunction of constraints over the output. Therefore, a strong output constraint can be used to tighten the bounds of intermediate layers. The benchmark consists of 464 instances, of which 72 were randomly selected for the VNN-COMP. For our comparison, we remove those 348 instances that \(\),\(\)-CROWN verifies as robust without tightening the bounds of any intermediate layer. Those instances are verified within

   Method & ReBreach-LP & ReBreach-LP & INVPROP \\  Partitions & N/A & 15625 & N/A \\ Approx Vol \(\) & 64 & 0.064 & 0.0000249 \\ Time (sec) & 11.8 \(\) 2.2 & 662.2 \(\) 31.4 & 213.8 \(\) 11.1 \\   

Table 2: **Quadrotor Results.** We compare the tightness of the over-approximations and run time (mean and std taken over 5 runs).

Figure 4: **6D Quadrotor.** We visualize the three dimensions of the 6D quadrotor that specify physical position. We consider the initial state range of the black box and simulate a few one-step trajectories, indicated by the orange lines. Our obstacle is the red box. For the three images, the blue box represents the over-approximation computed by (a) ReBreach-LP with no partitioning, (b) ReBreach-LP with 15625 partitions, and (c) INVPROP, respectively. A tighter blue box is better.

less than four seconds each by both methods, with no room for improvement. We compare the performance of \(,\)-CROWN and \(,\)-CROWN + INVPROP on the remaining 116 instances in Figure 5. \(,\)-CROWN can verify 48 instances, all other instances reach the timeout of five minutes. After extending \(,\)-CROWN with INVPROP on the last two intermediate layers, almost all instances can be solved faster, and 6 previously timed out instances become verifiable.

### OOD Detection

Consider the calibrated OOD detector presented as discussed in Section 2.3, encoded by a four layer MLP with \(200\), \(200\), \(3\), and \(2\) neurons. We over-approximate the set of inputs which induce a sufficiently high in-distribution (ID) confidence (measured by \(\{y_{0},y_{1}\}>y_{2}\)) using 40 hyperplanes of equal slope, pictured in green in Figure 6. This set is non-convex, making the convex hull a poor over-approximation. With 4 input space branches, we get a much tighter over-approximation, as shown in the right plot. We compare the performance of our approach with and without branching over the input space with the MILP baseline (see Table 4). This demonstrates a simple proof-of-concept for how INVPROP can be used for verifying some calibration properties.

## 5 Related Work

Formally Verified Neural Networks.There has been a large body of work on the formal verification of neural networks, tackling the problem from the angles of Interval Bound Propagation , Convex Relaxations , Abstract Interpretation , LP , SDP , SMT , and MILP . However, most of this work is for forward verification (i.e., bounding the NN output given a

   Method & Input & Approx & \\ Branch. & Ratio & Time (sec) \\  MILP & no & 1.47 & 1562.26 \\ INVPROP & no & 4.39 & \(3.77 0.02\) \\ INVPROP & yes & 1.14 & \(12.02 0.06\) \\   

Table 4: **OOD Results.** Comparison of methods for over-approximating the preimage of the OOD region (Figure 6). A lower approximation ratio (Approx Rat.) is better.

Figure 5: **YOLO Results.** Runtime comparison of \(,\)-CROWN and \(,\)-CROWN+INVPROP on the YOLO benchmark (167k neurons and 5 residual layers). For the comparison, only those test instances were used that could not immediately be verified by \(,\)-CROWN without any iterative tightening of intermediate layer bounds. \(,\)-CROWN+INVPROP can verify more properties and is faster for almost all instances.

   Network &  &  &  \\  & verif. & avg. time [s] & verif. & avg. time [s] & verif. & avg. time [s] \\  FFN4 (3x100) & 35 & 781 & 45 & 7.4 & 45 & 6.4 \\ FFN5 (6x100) & 31 & 1689 & 38 & 6.7 & 39 & 8.7 \\ FFN6 (6x200) & 31 & 6178 & 29 & 5.4 & 30 & 1.0 \\ FFN7 (9x200) & 36 & 8960 & 37 & 3.2 & 40 & 1.9 \\   

Table 3: **MNIST Results.** Robustness verification with DeepSRGR (output constraints via LP solver), \(,\)-CROWN (GPU support) and \(,\)-CROWN+INVPROP (output constraints with GPU support)

Figure 6: **OOD Detection.** Green: Empirical ID inputs (1 million samples). Blue: border of the verified preimage. Branching is union of preimages for each of 4 quadrants.

set of inputs). Our work strictly generalizes bound propagation as presented in Xu et al. 2021 since setting \(=\) in Theorem 1 recovers its results.

Formal Analysis of the Input DomainPrior work has studied trying to determine the inverse of a neural network [Kindermann and Linden, 1990, Saad and Wunsch, 2007], though these methods are prohibitively slow. Zhang et al. [2018b] used an LP solver to compute changes to a given input that would lead to a change in the classification result while adhering to additional criteria. Yang et al. [2021b] encode the overapproximated network in an LP problem to tighten bounds by incorporating the output constraint. They require an LP solver and we compare our performance against theirs in Section 4.2. Zhong et al. , Wu et al.  similarly tighten intermediate bounds by using an LP solver. Dimitrov et al.  compute input regions that only consist of adversarial examples (i.e., adhere to a given output constraint) that are maximized using an LP solver. Finally, a concurrent work Zhang et al.  targets under- and over-approximating the preimage via LiRPA-based forward verification [Xu et al., 2020] with input-space and ReLU-space partitioning; since our work can be viewed as a generalization of LiPRA, their work may benefit from our results of tightening the intermediate layer bounds using output constraints during bound propagation.

Formal Verification for Neural Feedback Loops.Our control application was motivated by the growing body of work on backward reachability analysis and over-approximating states that result in a target set [Everett et al., 2021]. The original method solely utilized input constraints for deriving intermediate bounds. The later development of Everett et al. 2022 improved upon this by optimizing \(^{(0)}\) and \(^{()}\) with respect to output bounds but their implementation faces numerical instability when applied for many iterations. We also support the partitioning over the input space introduced by Rober et al. 2022a. The work of Vincent and Schwager 2021 explores utilizing an LP with complete neuron branching to verify control safety, which can be viewed as a domain-specific implementation of our MILP formulation of the inverse verification problem.

Certified OOD detection.There is a wide variety of OOD detection systems [Yang et al., 2021a, Salehi et al., 2022]. Commonly, they are evaluated empirically based on known OOD data [Wang et al., 2022, Liang et al., 2018, Chen et al., 2021]. Therefore, they fail to provide verified guarantees for their effectiveness. In fact, many OOD detection systems are susceptible to adversarial attacks [Sehwag et al., 2019, Chen et al., 2022]. Meinke et al.  show how to verify robustness to adversarial examples around given input samples. Berrada et al.  develop a general framework for certifying properties of output distributions of neural networks given constraints on the input distribution. However, this work is still constrained to verifying properties of the outputs given constraints (albeit probabilistic) on the inputs, and INVPROP is able to certify arbitrary regions of the input space that lead to confident predictions.

## 6 Discussion

We present the challenge of over-approximating neural network preimages and provide an efficient algorithm to solve it. By doing so, we demonstrate strong performance on multiple application areas. We believe there is a large scope for future investigation and new applications of INVPROP.

LimitationsFor higher-dimensional instances, our method would best work for box over-approximations (\(2d\) hyperplanes in \(d\) dimensions) and would struggle at more complicated shapes such as spheres. We expect more complex problems will benefit from a domain-specific strategy. Moreover, iteratively refining all of the intermediate bounds utilizing INVPROP faces a quadratic dependence on network depth, as opposed to a linear dependence in traditional forward verification. To mitigate this, output constraints could only be applied to layers close to the output, as we used for robustness. Finally, our branching strategy might not scale to higher dimensions, though this trade-off is well-studied [Bunel et al., 2020, Wang et al., 2021, Palma et al., 2023].

Potential Negative Social ImpactOur work improves reliable ML through facilitating systems that provably align with practitioner expectations and we expect INVPROP to have positive societal impact. We acknowledge that our improvements may be repurposed as model attacks, though we believe the positive use cases of our technique greatly outweigh current speculation of misusage.

AcknowledgementsWe thank Michael Everett and Nicholas Rober for helpful discussion and feedback on the paper. Huan Zhang acknowledges the support of the Schmidt Futures AI2050 Early Career Fellowship.