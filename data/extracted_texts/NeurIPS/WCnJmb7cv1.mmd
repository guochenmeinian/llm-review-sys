# Learning to Assist Humans without Inferring Rewards

Vivek Myers\({}^{1}\) &Evan Ellis\({}^{1}\)

Sergey Levine\({}^{1}\)  Benjamin Eysenbach\({}^{2}\)  Anca Dragan\({}^{1}\)

\({}^{1}\)UC Berkeley \({}^{2}\)Princeton University

###### Abstract

Assistive agents should make humans' lives easier. Classically, such assistance is studied through the lens of inverse reinforcement learning, where an assistive agent (e.g., a chatbot, a robot) infers a human's intention and then selects actions to help the human reach that goal. This approach requires inferring intentions, which can be difficult in high-dimensional settings. We build upon prior work that studies assistance through the lens of empowerment: an assistive agent aims to maximize the influence of the human's actions such that they exert a greater control over the environmental outcomes and can solve tasks in fewer steps. We lift the major limitation of prior work in this area -- scalability to high-dimensional settings -- with contrastive successor representations. We formally prove that these representations estimate a similar notion of empowerment to that studied by prior work and provide a mechanism for optimizing it. Empirically, our proposed method outperforms prior methods on synthetic benchmarks, and scales to Overcooked, a cooperative game setting. Theoretically, our work connects ideas from information theory, neuroscience, and reinforcement learning, and charts a path for representations to play a critical role in solving assistive problems.1

Website: https://empowering-humans.github.io

## 1 Introduction

AI agents deployed in the real world should be helpful to humans. When we know the utility function of the humans an agent could interact with, we can directly train assistive agents through reinforcement learning with the known human objective as the agent's reward. In practice, agents rarely have direct access to a scalar reward corresponding to human preferences (if such a consistent model even exists) , and must infer them from human behavior . This inference can be challenging, as humans may act suboptimally with respect to their stated goals, not know their goals, or have changing preferences . Optimizing a misspecified reward function can have poor consequences .

An alternative paradigm for assistance is to train agents that are _intrinsically_ motivated to assist humans, rather than directly optimizing a model of their preferences. An analogy can be drawn to a parent raising a child. A good parent will empower the child to make impactful decisions and flourish, rather than proscribing an "optimal" outcome for the child. Likewise, AI agents might seek to _empower_ the human agents they interact with, maximizing their capacity to change the environment . In practice, concrete notions of empowerment can be difficult to optimize as an objective, requiring extensive modeling assumptions that don't scale well to the high-dimensional settings deep reinforcement learning agents are deployed in.

What is a good intrinsic objective for assisting humans that doesn't require these assumptions? We propose a notion of assistance based on maximizing the influence of the human's actions on theenvironment. This approach only requires one structural assumption: the AI agent is interacting with an environment where there is a notion of actions taken by the human agent -- a more general setting than the case where we model the human actions as the outcome of some optimization procedure, as in inverse RL [7; 8] or preference-based RL .

Prior work has studied various objectives for empowerment. For instance, Du et al.  approximate human empowerment as the variance in the final states of random rollouts. Despite excellent results in certain settings, this approach can be challenging to scale to higher dimensional settings, and does not necessarily enable human users to achieve the goals the want to achieve. By contrast, our approach exclusively empowers the human with respect to the distribution of (useful) behaviors induced by their current policy (which we term the _effective empowerment_), and can be implemented through a simple objective derived from contrastive successor features, which can then be optimized with scalable deep reinforcement learning (Fig. 1). We provide a theoretical framework connecting our objective to prior work on empowerment and goal inference, and empirically show that agents trained with this objective can assist humans in the Overcooked environment  as well as more complex versions of the obstacle gridworld assistance benchmark proposed by Du et al. .

Our core contribution is an objective for training agents that are intrinsically motivated to assist humans without requiring a model of the human's reward function. The approach, Empowerment via Successor Representations (ESR), maximizes the influence of the human's actions on the environment, and, unlike past approaches for assistance without reward inference, is based on a scalable model-free objective that can be derived from learned successor features that encode which states the human may want to reach given their current action. By maximizing effective empowerment, our objective empowers the human to reach the desired states, not all states, without assuming a human model. We analyze this objective in terms of empowerment and goal inference, drawing novel mathematical connections between time-series representations, decision-making, and assistance. We empirically show that agents trained with our objective can assist humans in two benchmarks proposed by past work: the Overcooked environment  and an obstacle-avoidance gridworld .

## 2 Related Work

Our approach broadly connects ideas from contrastive representation learning and intrinsic motivation to the problem of assisting humans.

Assistive Agents.There are two lines of past work on assistive agents that are most relevant.

Figure 1: We propose an algorithm training assistive agents to empower human users - the assistant should take actions that enable human users to visit a wide range of future states, and the human’s actions should exert a high degree of influence over the future outcomes. Our algorithm scales to high-dimensional settings, opening the door to building assistive agents that need not directly reason about human intentions.

The first line of work focuses on the setting of an assistance game , where a robot (AI) agent tries to optimize a human reward of which it is initially unaware. Practically, inverse reinforcement learning (IRL) can be used in such a setting to infer the human's reward function and assist the human in achieving their goals . The key challenge with this approach is that it requires modeling the human's reward function. This can be difficult in practice, especially if the human's behavior is not well-modeled by the reward architecture. Slightly misspecified reward functions can lead to catastrophic outcomes (i.e., directly harmful behavior in the assistance context) . These concerns have prompted interest, under the umbrella term _AI Alignment_, in ensuring agents are safe and adhere to human values . Our approach avoids some of these issues by focusing on empowerment, a more general objective that does not require modeling the human's reward function.

The second line of work focuses on empowerment-like objectives for assistance and shared autonomy. Empowerment generally refers to a measure of an agent's ability to influence the environment . In the context of assistance, Du et al.  show one such approximation of empowerment (AvE) can be approximated in simple environments through random rollouts to assist humans. Meanwhile, empowerment-like objectives have been used in shared autonomy settings to assist humans with teleoperation  and general assistive interfaces . A key limitation of these approaches for general assistance is they only model empowerment over one time step. Our approach enables a more scalable notion of empowerment that can be computed over multiple time steps.

Intrinsic Motivation.Intrinsic motivation broadly refers to agents that accomplish behaviors in the absence of an externally-specified reward or task . Common applications of intrinsic motivation in single-agent reinforcement learning include exploration and skill discovery , empowerment , and surprise minimization . When applied to settings with humans, these objectives may lead to antisocial behavior . Our approach applies intrinsic motivation to the setting of assisting humans, where the agent's goal is an empowerment objective -- to maximize the human's ability to change the environment.

Information-theoretic Decision Making.Information-theoretic approaches have seen broad applicability across unsupervised reinforcement learning . These methods have been applied to goal-reaching , skill discovery , and exploration . In the context of assisting humans, information-theoretic methods have primarily been used to reason about the human's goals or rewards .

Our approach is made possible by advances in contrastive representation learning for efficient estimation of the mutual information of sequence data . While these methods have been widely used for representation learning  and reinforcement learning , to the best of our knowledge prior work has not used these contrastive techniques for learning assistive agents.

## 3 The Information Geometry of Empowerment

We will first state a general notion of an assistive setting, then show how an empowerment objective based on learned successor representations can be used to assist humans without making assumptions about the human following an underlying reward function. In Section 5, we provide empirical evidence supporting these claims.

### Preliminaries

Formally, we adapt the notation of Hadfield-Menell et al. , and assume a "robot" (\(\)) and "human" (\(\)) policy are training together in an MDP \(M=(,_{},_{},R,,)\). The states \(s\) consist of the joint states of the robot and the human; we do not have separate observations for the human and robot. At any state \(s\), the robot policy selects actions distributed according to \(_{}(a^{} s)\) for \(a^{}_{}\) and the human selects actions from \(_{}(a^{} s)\) for \(a^{}_{}\). The transition dynamics are defined by a distribution \((s^{} s,a^{},a^{})\) over the next state \(s^{}\) given the current state \(s\) and actions \(a^{}_{}\) and \(a^{}_{}\), as well as an initial state distribution \((s_{0})\). For notational convenience, we will additionally define random variables \(_{t}\) to represent the state at time \(t\), and \(_{t}^{}_{}(_{t})\) and \(_{t}^{}_{}(_{t})\) to represent the human and robot actions at time \(t\), respectively.

Effective Empowerment.Our work builds on a long line of prior methods that use information theoretic objectives for RL. Specifically, we adopt _empowerment_ as an objective for training an assistive agent . This section provides the mathematical foundations for empowerment,as developed in prior work. We build on the prior work by (1) providing an information geometric interpretation of what empowerment does (Section3.3) and (2) providing a scalable algorithm for estimating and optimizing a notion of empowerment, the _effective empowerment_ (Section4.1).

The idea behind empowerment is to think about the changes that an agent can effect on a world; an agent is more empowered if its actions effect a larger degree of change over future outcomes. Following prior work [18; 29; 46], we measure empowerment by looking at how much the actions taken _now_ affect outcomes _in the future_. An agent with a high degree of empowerment exerts a high degree of control of the future states by simply changing its current actions. Like prior work, we measure this degree of control through the mutual information \(I(^{+};^{})\) between the current action \(^{}\) and the future states \(^{+}\). Note that these future states might occur many time steps into the future.

Empowerment depends on several factors: the environment dynamics, the choice of future actions, the current state, and other agents in the environment. Different problem settings involve maximizing empowerment using these different factors. In this work, we study the setting where a "human" agent and a "robot" agent collaborate in an environment, with the robot aiming to maximize the empowerment of the human. This problem setting was introduced in prior work [2; 6]. Compared with other mathematical frameworks for learning assistive agents , framing the problem in terms of empowerment means that the assistive agent need not infer the human's underlying intention, an inference problem that is typically challenging [48; 49].

To define our empowerment objective, we introduce the random variable \(^{+}\), corresponding to a state sampled \(K(1-)\) steps into the future under the behavior policies \(_{}\) and \(_{}\). We will use \((^{+} s_{t})\) to denote the density of this random variable; this density is sometimes referred to as the discounted state occupancy measure. We will use mutual information to measure how much the action \(a_{t}\) at time \(t\) changes this distribution:

\[I(^{}_{t};^{+} s_{t}) _{s_{t},s_{t+k},^{}_{t},^{}_{t}} (_{t+K}=s_{t+k}_{t}=s_ {t},^{}_{t}=a_{t})}{(_{t+K}=s_{t+k} _{t}=s_{t})}.\] (1)

Our overall objective is _effective empowerment_, \((_{},_{})\): the mutual information between the human's actions and the future states \(^{+}\) while interacting with the robot:

\[(_{},_{})=_{t=0}^{ }^{t}I(^{}_{t};^{+}_{t}).\] (2)

We call this objective _effective empowerment_ rather than just _empowerment_ to highlight that the mutual information in Eq.2 is computed under the behavior policies \(_{}\) and \(_{}\). This is in contrast to much of the prior work on empowerment, which focuses on the highest possible empowerment that could be achieved under some policy, rather than the actual realized empowerment [6; 18; 46]. Focusing on the effective empowerment simplifies the problem of estimating empowerment since we only have access to samples from the current behavior policies, not arbitrary more powerful policies. In later sections, we will see this simplification is both empirically effective and theoretically justified.

Note that this objective resembles an RL objective: we do not just want to maximize this objective greedily at each time step, but rather want the assistive agents to take actions now that help the human agent reach states where it will have high empowerment in the future.

### Intuition and Geometry of Empowerment

Intuitively, the assistive agent should aim to maximize the number of future that can be realized by the human's actions. We will mathematically quantify this in terms of the discounted state occupancy measure, \(^{}(s^{+} s)\). An agent has a large empowerment if the future states for one action are very different from the future actions after taking a different action; i.e., when \((a_{t}=a_{1};^{+} s_{t})\) is quite different from \((a_{t} s_{2};^{+} s_{t})\) for actions \(a_{1} a_{2}\). The mutual information (Eq.1) quantifies this degree of control: \(I(_{t};^{+} s_{t})\).

One way of understanding this mutual information is through _information geometry_[50; 51; 52; 51]. For a fixed current state \(s_{t}\), assistant policy \(_{}\) and human policy \(_{}\), each potential action \(a_{t}\) that the human takes induces a different distribution over future states: \(^{_{},_{}}(^{+} s_{t},a_{t})\). We can think about the set of these possible distributions: \(\{^{_{},_{}}(^{+} s_{t},a_{t})  a_{t}\}\). Figure2_(Left)_ visualizes this distribution on a probability simplex for 6 choices of action \(a_{t}\). If we look at any possible distribution over actions, then this set of possible future distributions becomes a polytope (see orange polygon in Fig.2_(Center)_).

Intuitively, the mutual information \(I(_{t};^{+} s_{t})\) in our empowerment objective corresponds to the _size_ or _volume_ of this state marginal polytope. This intuition can be formalized by using results from information geometry . The human policy \(_{}(a_{t} s_{t})\) places probability mass on the different points in Figure 2 (Center). Maximizing the mutual information corresponds to "picking out" the state distributions that are maximally spread apart (see probabilities in Fig. 2 (_Center)_). To formalize this, define

\[(^{+} s_{t})_{(a_{t}|s_{t})}[( ^{+} s_{t},a_{t})]\] (3)

as the _average_ state distribution from taking the human's actions (see green square in Fig. 2 (_Center_)).

**Remark 3.1**.: _Mutual information corresponds to the distance between the average state distribution (Eq. 3) and the furthest achievable state distributions:_

\[I(_{t};^{+} s_{t})=_{a_{t}}D_{} (a_{t};^{+} s_{t})(^{+} s_{t} ) d_{}.\] (4)

This distance is visualized as the black lines in Fig. 2. When we talk about the "size" of the state marginal polytope, we are specifically referring to the length of these black lines (as measured with a KL divergence).

This sort of mutual information is a way for measuring the degree of control that an agent exerts on an environment. This measure is well defined for any agent/policy; that agent need not be maximizing mutual information, and could instead be maximizing some arbitrary reward function. This point is important in our setting: this means that the assistive agent can estimate and maximize the empowerment of the human user _without having to infer what reward function the human is trying to maximize_.

Finally, we come back to our empowerment objective, which is a discounted sum of the mutual information terms that we have been analyzing above. This empowerment objective says that the human is more empowered when this set has a larger size -- i.e., the human can visit a wider range of future state (distributions). The empowerment objective says that the assistive agent should act to try to maximize the size of this polytope. Importantly, this maximization problem is done sequentially: the assistive agent wants the size of this polytope to be large both at the current state and at future states; the human's actions should exert a high degree of influence over the future outcomes both now and in the future. Thus, our overall objective looks at a sum of these mutual informations.

Not only does this analysis provides a geometric picture for what empowerment is doing, it also lays the groundwork for formally relating empowerment to reward.

Figure 2: **The Information Geometry of Empowerment**, illustrating the analysis in Section 3.3. _(Left)_ For a given state \(s_{t}\) and assistant policy \(_{}\), we plot the distribution over future states for 6 choices of the human policy \(_{}\). In a 3-state MDP, we can represent each policy as a vector lying on the 2-dimensional probability simplex. We refer to the set of all possible state distributions as the _state marginal polytope_. _(Center)_ Mutual information corresponds to the distance between the center of the polytope and the vertices that are maximally far away. _(Right)_ Empowerment corresponds to maximizing the size of this polytope. For example, when an assistive agent moves an obstacle out of a human user’s way, the human user can spend more time at desired state.

### Relating Empowerment to Reward

In this section we take aim at the question: when humans are well-modeled as optimizing a reward function, when does maximizing effective empowerment help humans maximize their rewards? Answering this question is important because for empowerment to be a safe and effective assistive objective, it should enable the human to better achieve their goals. We show that under certain assumptions, empowerment yields a provable lower bound on the average-case reward achieved by the human for suffiently long-horizon empowerment (i.e., \( 1\)).

For constructing the formal bound, we suppose the human is Boltzmann-rational  with respect to some reward function \(R\) and rationality coefficient \(\). The distribution \(\) could be interpreted as a prior over the human's objective, a set of skills the human may try and carry out, or a population of humans with different objectives that the agent could be interacting with. Our quantity of interest, the average-case reward achieved by the human with our empowerment objective, is given by

\[_{_{}}^{}(_{})=_{ R\\ s_{0} p_{0}}V_{R,}^{_{},_{ }}(s_{0})\] (5)

where \(V_{R,}^{_{},_{}}(s_{0})\) is the value function of the human policy \(_{}\) under the reward function \(R\) when interacting with \(_{}\). Recalling Eq.2, we will express the overall effective empowerment objective we are trying to relate to Eq.5 as

\[_{}(_{},_{})=_{t=0}^{}^{t}I(^{+};_{t}^{} }_{t}).\] (6)

This notation is formalized in AppendixB.

The two key assumptions used in our analysis are creftype3.1, which states that the human will optimize for behaviors that uniformly cover the state space, and creftype3.2, which simply states that with infinite time, the human will be able to reach any state in the state space.

**Assumption 3.1** (Skill Coverage).: _The rewards \(R\) are uniformly distributed over the scaled \(||\)-simplex \(^{||}\) such that:_

\[R+|} ^{||}=(_{||}).\] (7)

**Assumption 3.2** (Ergodicity).: _For some \(_{},_{}\), we have_

\[^{_{},_{}}(^{+}=s s_{0})> 0s,(0,1).\] (8)

Our main theoretical result is creftype3.1, which shows that under these assumptions, maximizing effective empowerment yields a lower bound on the (squared) average-case reward achieved by the human for sufficiently large \(\). In other words, for a sufficiently long empowerment horizon, the empowerment objective Eq.2 is a meaningful proxy for reward maximization.

**Theorem 3.1**.: _Under creftype3.1 and creftype3.2, for sufficiently large \(\) and any \(>0\),_

\[_{}(_{},_{})^{1/2}(/e) \,_{_{}}^{}(_{}).\] (9)

The proof is in creftypeB.1 To the best of our knowledge, this result provides the first formal link between empowerment maximization and reward maximization. This motivates us to develop a scalable algorithm for empowerment maximization, which we introduce in the following section.

## 4 Estimating and Maximizing Empowerment with Contrastive Representations

Directly computing Eq.2 would require access to the human policy, which we don't have. Therefore, we want a tractable estimation that still performs well in large environments which are more difficult to model due to the exponentially increasing set of possible future states. To better-estimate empowerment, we learn contrastive representations that encode information about which future states are likely to be reached from the current state. These contrastive representations learn to model mutual information between the current state, action, and future state, which we then use to compute the empowerment objective.

### Estimating Effective Empowerment

To estimate this effective empowerment architecture, we need a way of learning the probability ratio inside the expectation. Prior methods such as Du et al.  and Salge et al.  rollout possible future states and compute a measure of their variance as a proxy for empowerment, however this doesn't scale when the environment becomes complex. Other methods learn a dynamics model, which also doesn't scale when dynamics become challenging to model . Modeling these probabilities directly is challenging in settings with high-dimensional states, so we opt for an indirect approach. Specifically, we will learn representations that encode two probability ratios. Then, we will be able to compute the desired probability ratio by combining these other probability ratios.

Our method learns three representations:

1. \((s,a^{},a^{})\) -- This representation can be understood as a sort of latent-space model, predicting the future representation given the current state \(s\) and the human's current action \(a^{}\) as well as the robot's current action \(a^{}\).
2. \(^{}(s,a^{})\) -- This representation can be understood as an uncontrolled model, predicting the representation of a future state without reference to the current human action \(a^{}\). This representation is analogous to a value function.
3. \((s^{+})\) -- This is a representation of a future state.

We will learn these three representations with two contrastive losses, one that aligns \((s,a^{R},a^{H})(s^{+})\) and one that aligns \(^{}(s,a^{})(s^{+})\)

\[_{,^{},}_{\{(s_{i},a_{i},s^{}_{i}) (_{1},_{1}^{},_{ _{_{_{_{_{ _{_{_{_{_{ }}}}}}}}}})\}_{i=1}^{N}}_{}\{ (s_{i},a_{i})\},\{(s^{}_{j})\}+_{} \{^{}(s_{i})\},\{(s^{}_{j})\},\]

where the contrastive loss \(_{}\) is the symmetrized infoNCE objective :

\[_{}(\{x_{i}\},\{y_{j}\})_{i=1}^{N} ^{T}y_{i}}}{_{j=1}^{N}e^{x_{i}^{T}y_{j}}} +^{T}y_{i}}}{_{j=1}^{N}e^{x_{j}^{T}y_{i}}} .\] (10)

We have colored the index \(j\) for clarity. At convergence, these representations encode two probability ratios , which we will ultimately be able to use to estimate empowerment (Eq. 2):

\[(s,a^{},a^{})^{T}(g) =(_{t+K}=g_{t}=s,_{t}^{}=a^{},_{t}^{ }=a^{})}{C_{1}\,(_{t+K}=g)}\] (11) \[^{}(s,a^{})^{T}(g) =(_{t+K}=s_{t+k} _{t}=s_{t},_{t}^{}=a^{})}{C_{2} \,(_{t+K}=g)}.\] (12)

Note that our definition of empowerment (Eq. 2) is defined in terms of similar probability ratios. The constants \(C_{1}\) and \(C_{2}\) will mean that our estimate of empowerment may be off by an additive constant, but that constant will not affect the solution to the empowerment maximization problem.

### Estimating Empowerment with the Learned Representations

To estimate empowerment, we will look at the difference between these two inner products:

\[(s_{t+K},a^{},a^{})^{T}(g)-(s_{t +K},a^{})^{T}(g)\] \[=(s_{t+K} s,a^{})- C_{1}- {(s_{t+K})}-(s_{t+K} s)+ C_{2}+(s_{t+K})}\] \[=(s_{t+K} s,a^{})}{(s _{t+K} s)}+}{C_{1}}.\]

Note that the expected value of the first term is the _conditional_ mutual information \(I(s_{t+K};a^{} s)\). Our empowerment objective corresponds to averaging this mutual information across all the visited states. In other words, our objective corresponds to an RL problem, where empowerment corresponds to the expected discounted sum of these log ratios:

\[(_{},_{}) =_{_{},_{}}_{t=0}^ {}^{t}I(s^{+};a_{t}^{} s_{t})\] \[_{_{},_{}} _{t=0}^{}^{t}((s_{t},a^{},a^{})-(s_ {t},a^{}))^{T}(g)-}{C_{1}}.\]The approximation above comes from function approximation in learning the Bayes optimal representations. Again, note that the constants \(C_{1}\) and \(C_{2}\) do not change the optimization problem. Thus, to maximize empowerment we will apply RL to the assistive agent \(_{}(a s)\) using a reward function

\[r(s,a^{})=((s_{t},a^{},a^{})- (s_{t},a^{}))^{T}(g).\] (13)

### Algorithm Summary

``` Input: Human policy \(_{}(a s)\) Randomly initialize assistive agent policy \(_{}(a s)\), and representations \((s,a^{},a^{})\), \((s,a^{T})\), and \((g)\). Initialize replay buffer \(\). while not converged do Collect a trajectory of experience with human policy and assistive agent policy, store in replay buffer \(\). Update representations \((s,a^{},a^{})\), \((s,a^{T})\), and \((g)\) with the contrastive losses in Eq. (10). Update \(_{}(a s)\) with RL using reward function \(r(s,a^{},a^{})=((s,a^{},a^{})- ^{}(s,a^{}))^{T}(g)\). Return: Assistive policy \(_{}(a s)\). ```

**Algorithm 1**Empowerment via Successor Representations (ESR)

We propose an actor-critic method for learning the assistive agent. Our method will alternate between updating these contrastive representations and using them to estimate a reward function (Eq. (13)) that is optimized via RL. We summarize the algorithm in Algorithm 1. In practice, we use SAC  as our RL algorithm. In our experiments, we will also study the setting where the human user updates their policy alongside the assistive agent.

Figure 3: We apply our method to the benchmark proposed in prior work , visualized in Fig. 3(a). The four subplots show variant tasks of increasing complexity (more blocks), (\( 1\) SE). We compare against AvE , the Goal Inference baseline from  which assumes access to a world model, and Reward Inference  where we recover the reward from a learned q-value. These prior approaches fail on all except the easiest task, highlighting the importance of scalability.

Experiments

We seek to answer two questions with our experiments. _First_, does our approach enable assistance in standard cooperation benchmarks? _Second_, does our approach scale to harder benchmarks where prior methods fail?

Our experiments will use two benchmarks designed by prior work to study assistance: the obstacle gridworld  and Overcooked . Our main **baseline** is AvE , a prior empowerment-based method. Our conjecture is that both methods will perform well on the lower-dimensional gridworld task, and that our method will scale more gracefully to the higher dimensional Overcooked environment. We will also compare against a naive baseline where the assistive agent acts randomly.

### Do contrastive successor representations effectively estimate empowerment?

We test our approach in the assistance benchmark suggested in Du et al. . The human (orange) is tasked with reaching a goal state (green) while avoiding the obstacles (purple). The AI assistant can move blocks one step at a time in any direction . While the original benchmark used \(N=2\) obstacles, we will additionally evaluate on harder versions of this task with \(N=5,7,10\) obstacles. We show results in Fig. 3. On the easiest task, both our method and AvE achieve similar asymptotic reward, though our method learns more slowly than AvE. However, on the tasks with moderate and high degrees of complexity, our approach (ESR) achieves significantly higher rewards than AvE, which performs worse than a random controller. These experiments support our claim that contrastive successor representations provide an effective means for estimating empowerment, and hint that ESR might be well suited for solving higher dimensional tasks.

Figure 4: _(a)_ The modified environment from Du et al.  scaled to \(N=7\) blocks, and _(b, c)_ the two layouts of the Overcooked environment .

Figure 5: In Coordination Ring, our ESR agent learns to wait for the human to add an onion to the pot, and then adds one itself. There is another pot at the top which is nearly full, but the empowerment agent takes actions to maximize the impact of the human’s actions, and so follows the lead of the human by filling the empty pot.

### Does our approach scale to tasks with image-based observations?

Our second set of experiments look at scaling ESR to the image-based Overcooked environment. Since contrastive learning is often applied to image domains, we conjectured that ESR would scale gracefully to this setting. We will evaluate our approach in assisting a human policy trained with behavioral cloning taken from Laidlaw and Dragan . The human prepares dishes by picking up ingredients and cooking them on a stove, while the AI assistant moves ingredients and dishes around the kitchen. We focus on two environments within this setting: a cramped room where the human must pass ingredients and dishes through a narrow corridor, and a coordination ring where the human must pass ingredients and dishes around a ring-shaped kitchen (Figs. 3(b) and 3(c)). As before, we compare with AvE as well as a naive random controller. We report results in Table 1. On both tasks, we observe that our approach achieves higher rewards than AvE baseline, which performs no better than a random controller. In Fig. 5, we show an example of one of the collaborative behaviors learned by ESR. Taken together with the results in the previous setting, these results highlight the scalability of ESR to higher dimensional problems.

## 6 Discussion

One of the most important problems in AI today is equipping AI agents with the capacity to assist humans achieve their goals. While much of the prior work in this area requires inferring the human's intention, our work builds on prior work in studying how an assistive agent can _empower_ a human user without inferring their intention. Relative to prior methods, we demonstrate how empowerment can be readily estimated using contrastive learning, paving the way for deploying these techniques on high-dimensional problems.

Limitations.One of the main limitations of our approach is the assumption that the assistive agent has access to the human's actions, which could be challenging to observe in practice. Automatically inferring the human's actions remains an important problem for future work. A second limitation is that the method is currently an on-policy method, in the sense that the assistive agent has to learn by trial and error. A third limitation is that the ESR formulation assumes that both agents share the same state space. In many cases the empowerment objective will still lead to desirable behavior, however, care must be taken in cases where the agent can restrict the information in its own observations, which could lead to reward hacking. Finally, our experiments do not test our method against real humans, whose policies may differ from the simulated policies. In the future, we plan to investigate techniques from off-policy evaluation and cooperative game theory to enable faster learning of assistive agents with fewer trials. We also plan to test the ESR objective in environments with partial observability over the human's state.

Safety risks.Perhaps the main risk involved with maximizing empowerment is that it may be at odds with a human's agents goal, especially in contexts where the pursuit of that goal limits the human's capacity to pursue other goals. For example, a family choosing to have a kid has many fewer options over where they can travel for vacation, yet we do not want assistive agents to stymie families from having children.

One key consideration is _whom_ should be empowered. The present paper assumes there is a single human agent. Equivalently, this can be seen as maximizing the empowerment of all exogenous agents. However, it is easy to adapt the proposed method to maximize the empowerment of a single target individual. Given historical inequities in the distribution of power, practitioners must take care when considering whose empowerment to maximize. Similarly, while we focused on _maximizing_ empowerment, it is trivial to change the sign so that an "assistive" agent minimizes empowerment. One could imagine using such a tool in policies to handicap one's political opponents.

   Layout & **ESR (Ours)** & Reward Inference & AvE & Random \\  Asymmetric Advantages & \(\) & \(60.33 0.26\) & \(36.71 1.71\) & \(59.36\) \\ Coordination Ring & \(\) & \(5.96 0.20\) & \(5.69 0.93\) & \(6.02\) \\ Cramped Room & \(\) & \(39.24 0.35\) & \(5.13 1.31\) & \(69.26\) \\   

Table 1: Overcooked ResultsAcknowledgments.We would like to thank Micah Carroll and Cameron Allen for their helpful feedback, as well as Niklas Lauffer for suggesting JaxMARL. We especially thank the fantastic NeurIPS reviewers for their constructive comments and suggestions. This research was partly supported by ARL DCIST CRA W911NF-17-2-0181 and ONR N00014-22-1-2773, as well as NSF HCC 2310757, the Jump Cocosys Center, Princeton Research Computing, and the DoD through the NDSEG Fellowship Program.