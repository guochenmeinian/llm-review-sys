# Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods

Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods

 Felix Dangel

Vector Institute

Toronto, Canada

fdangel@vectorinstitute.ai

###### Abstract

Despite their simple intuition, convolutions are more tedious to analyze than dense layers, which complicates the transfer of theoretical and algorithmic ideas to convolutions. We simplify convolutions by viewing them as tensor networks (TNs) that allow reasoning about the underlying tensor multiplications by drawing diagrams, manipulating them to perform function transformations like differentiation, and efficiently evaluating them with einsum. To demonstrate their simplicity and expressiveness, we derive diagrams of various autodiff operations and popular curvature approximations with full hyper-parameter support, batching, channel groups, and generalization to any convolution dimension. Further, we provide convolution-specific transformations based on the connectivity pattern which allow to simplify diagrams before evaluation. Finally, we probe performance. Our TN implementation accelerates a recently-proposed KFAC variant up to 4.5 x while removing the standard implementation's memory overhead, and enables new hardware-efficient tensor dropout for approximate backpropagation.

## 1 Introduction

Convolutional neural networks [CNNs, 39] mark a milestone in the development of deep learning architectures as their'sliding window' approach represents an important inductive bias for vision tasks. Their intuition is simple to explain with graphical illustrations [e.g. 21]. Yet, convolutions are more challenging to analyze than dense layers in multi-layer perceptrons (MLPs) or transformers . One reason is that they are hard to express in matrix notation and--even in index notation--compact expressions that are convenient to work with only exist for special hyper-parameters [e.g. 27, 2]. Many hyper-parameters (stride, padding,...) and additional features like channel groups  introduce even more complexity that is inherited by related routines, e.g. for autodiff. We observe a delay of analytic and algorithmic developments between MLPs vs. CNNs, e.g.

* Approximate Hessian diagonal: 1989 vs. 2024
* Hessian rank: 2021 vs. 2023
* Gradient descent learning dynamics: 2014 vs. 2023
* Neural tangent kernel (NTK): 2018 vs. 2019
* Kronecker-factored quasi-Newton methods: 2021 vs. 2022
* Kronecker-factored curvature (KFAC, KFRA, KFLR): (2015, 2017, 2017) vs. (2016, 2020, 2020)

The software support for less standard routines some of these methods require also reflects this gap. Some functions only support special dimensions . Others use less efficient workarounds (SS5.1) or are not provided at all (SSB.4). And they are hard to modify as the code is either closed-source  or written in a low-level language. This complicates the advance of existing, and the exploration of new, algorithmic ideas for convolutions.

Here, we seek to reduce this complexity gap by viewing convolutions as tensor networks [TNs, 53, 6, 9] which express the underlying tensor multiplications as diagrams. These diagrams are simpler to parse than mathematical equations and can seamlessly be (i) manipulated to take derivatives, add batching, or extract sub-tensors, (ii) merged with other diagrams, and (iii) evaluated with einsum. This yields simple, modifiable implementations that benefit from automated under-the-hood-optimizations for efficient TN contraction developed by the quantum simulation community [e.g. 66, 25, 74, 13], like finding a high-quality contraction order or distributing computations:

1. We use the TN format of convolution from Hayashi et al.  to derive diagrams and einsum formulas for autodiff and less standard routines for curvature approximations with support for all hyper-parameters, batching, groups, and any dimension (Table 1).
2. We present transformations based on the convolution's connectivity pattern to re-wire and symbolically simplify TNs before evaluation (example in Figure 1).
3. We compare default and TN implementations, demonstrating optimal peak memory reduction and run time improvements up to 4.5 x for a recent KFAC variant, and showcase their flexibility to impose hardware-efficient dropout for randomized backpropagation.

Our work not only provides simpler perspectives and implementations that facilitate the exploration of algorithmic ideas for convolutions, but also directly advances second-order methods like KFAC: It enables more frequent pre-conditioner updates, using larger batches without going out of memory, and extending KFAC to transpose convolution. These improvements are important for second-order optimization and other applications like Laplace approximations  and influence functions .

## 2 Preliminaries

We briefly review 2d convolution (SS2.1), tensor multiplication and einsum (SS2.2), then introduce the graphical TN notation and apply it to convolution (SS2.3). Bold lower-case (\(\)), upper-case (\(\)), and upper-case sans-serif (\(}\)) symbols indicate vectors, matrices, and tensors. Entries follow the same convention but use regular font weight; \([]\) denotes slicing \((\,||_{i,j}=A_{i,j})\). Parenthesized indices mean reshapes, e.g. \([]_{(i,j)}=[]_{i,j}\) with \(\) the flattened matrix \(\).

Figure 1: Many convolution-related routines can be expressed as TNs and evaluated with einsum. We illustrate this for the input-based factor of KFAC for convolutions [KFC, 27], whose standard implementation (_top_) requires unfolding the input (high memory). The TN (_middle_) enables internal optimizations inside einsum (e.g. with contraction path optimizers like opt_einsum ). (_Bottom_) In many cases, the TN further simplifies due to structures in the index pattern, which reduces cost.

### Convolution

2d convolutions process channels of 2d signals \(^{C_{} I_{1} I_{2}}\) with \(C_{}\) channels of spatial dimensions1\(I_{1},I_{2}\) by sliding a collection of \(C_{}\) filter banks, arranged in a kernel \(^{C_{} C_{} K_{1} K _{2}}\) with kernel size \(K_{1},K_{2}\), over the input. The sliding operation depends on various hyperparameters [padding, stride, \(\), see 21]. At each step, the filters are contracted with the overlapping area, yielding the channel values of a pixel in the output \(^{C_{} O_{1} O_{2}}\) with spatial dimensions \(O_{1},O_{2}\). Optionally, a bias from \(^{C_{}}\) is added per channel.

One way to implement convolution is via matrix multiplication , similar to fully-connected layers. First, one extracts the overlapping patches from the input for each output, then flattens and column-stacks them into a matrix \([]^{C_{}K_{1}K_{2} O_{1}O_{2}}\), called the _unfolded input_ (or im2col). Multiplying a matrix view \(^{C_{} C_{}K_{1}K_{2}}\) of the kernel onto the unfolded input then yields a matrix view \(\) of **Y** (the vector of ones, \(_{O_{1}O_{2}}\), copies the bias for each channel),

\[=[]+\:_{O_{1}O_{2}}^{}^{C_ {} O_{1}O_{2}}\,.\] (1)

We can also view convolution as an affine map of the flattened input \(^{C_{}I_{1}I_{2}}\) into a vector view \(\) of **Y** with a Toeplitz-structured matrix \(()^{C_{}O_{1}O_{2} C_{} I_{1}I_{2}}\),

\[=()+_{O_{1}O_{2}}^{C_{ }O_{1}O_{2}}\,.\] (2)

This perspective is uncommon in code, but used in theoretical works [e.g. 65] as it highlights the similarity between convolutions and dense layers.

### Tensor Multiplication

Tensor multiplication unifies outer (Kronecker), element-wise (Hadamard), and inner products and uses the input-output index relation to infer the multiplication type. We start with the binary case, then generalize to more inputs: Consider \(,,\) whose index names are described by the index tuples \(S_{1},S_{2},S_{3}\) where \(S_{3}(S_{1} S_{2})\) (converting tuples to sets if needed). Any product of **A** and **B** can be described by the multiplication operator \(*_{(S_{1},S_{2},S_{3})}\) with

\[=*_{(S_{1},S_{2},S_{3})}(,) []_{S_{3}}=_{(S_{1} S_{2}) S_{3}}[] _{S_{1}}[]_{S_{2}}\] (3)

summing over indices that are not present in the output. E.g., for two matrices \(,\), their product is \(=*_{(i,j),(j,k),(i,k))}(,)\) (see SS1.2), their Hadamard product \(=*_{((i,j),(i,j),(i,j))}(,)\), and their Kronecker product \(=*_{((i,j),(k,I),((i,k),(j,I)))}(,)\). Libraries support this functionality via einsum, which takes a string encoding of \(S_{1},S_{2},S_{3}\), followed by **A**, **B**. It also accepts longer sequences \(_{1},,_{N}\) with index tuples \(S_{1},S_{2},,S_{N}\) and output index tuple \(S_{N+1}\),

\[_{N+1}=*_{(S_{1},,S_{N},S_{N+1})}(_{1},, {A}_{N})\:\:[_{N+1}]_{S_{N+1}}=_{(_{n=1}^{N }S_{n}) S_{N+1}}\!\!\!(_{n=1}^{N}[_{n}]_{S_{n}} ).\] (4)

Figure 2: TNs of (a) 2d convolution and (b,c) connections to its matrix multiplication view. The connectivity along each dimension is explicit via an index pattern tensor \(}\).

[MISSING_PAGE_EMPTY:4]

## 3 TNs for Convolution Operations

We now demonstrate the elegance of TNs for computing derivatives (SS3.1), autodiff operations (SS3.2), and approximate second-order information (SS3.3) by graphical manipulation. For simplicity, we exclude batching (vmap-ing like in JAX ) and channel groups, and provide the diagrams with full support in SSB. Table 1 summarizes our derivations (with batching and groups). As a warm-up, we identify the unfolded input and kernel from the matrix-multiplication view (Equations (1) and (2)). They follow by contracting the index patterns with either the input or kernel (Figures 1(b) and 1(c)),

\[[]]_{(c_{},k_{1},k_{2}),(c_{},0_{1})} =_{i_{1},i_{2}}_{c_{},i_{1},i_{2}} ^{(1)}_{i_{1},o_{1},k_{1}}^{(2)}_{i_{2},o_{2},k_{2}},\] \[[()]_{(c_{},o_{1},o_{2}),(c_{},i_{1},i_{2})} =_{k_{1},k_{2}}^{(1)}_{i_{1},o_{1},k_{1}}^{(2)}_{i_ {2},o_{2},k_{2}}_{c_{},c_{},k_{1},k_{2}}\;.\]

### Tensor Network Differentiation

Derivatives play a crucial role in theoretical and practical ML. First, we show that differentiating a TN diagram amounts to a simple graphical manipulation. Then, we derive the Jacobians of convolution. Consider an arbitrary TN represented by the tensor multiplication from Equation (4). The Jacobian tensor \([_{_{j}}_{N+1}]_{S_{N+1},S^{}_{j}}=_{N+1}]_{S_{N+1}}}}{{[_{j}]_{S^{}}}}\) w.r.t. an input \(_{j}\) collects all partial derivatives and is addressed through indices \(S_{n+1} S^{}_{j}\) with \(S^{}_{j}\) an independent copy of \(S_{j}\). Assume that \(_{j}\) only enters once in the tensor multiplication. Then, taking the derivative of Equation (4) w.r.t. \([_{j}]_{S^{}_{j}}\) simply replaces the tensor by a Kronecker delta \(_{S_{j},S^{}_{j}}\),

\[_{N+1}]_{S_{N+1}}}{[_{j}]_{S^{ }_{j}}}=_{(_{n=1}^{N}S_{n}) S_{n+1}}[_{1} ]_{S_{1}}[_{j-1}]_{S_{j-1}}_{i S_{j}}_{i,i^{ }}[_{j+1}]_{S_{j+1}}[_{N}]_{S_{N}}\] (6)

If an index \(i S_{j}\) is summed, \(i S_{n+1}\), we can sum the Kronecker delta \(_{i,i^{}}\), effectively replacing all occurrences of \(i\) by \(i^{}\). If instead \(i\) is part of the output index, \(i S_{n+1}\), the Kronecker delta remains part of the Jacobian and imposes structure. Figure 2(a) illustrates this process in diagrams for differentiating a convolution w.r.t. its kernel. Equation (6) amounts to cutting out the argument of differentiation and assigning new indices to the resulting open legs. For the weight Jacobian \(_{}\), this introduces structure: If we re-interpret the two sub-diagrams in Figure 2(a) as matrices, compare with the Kronecker diagram from Equation (5) and use Figure 1(b), we find \([]^{}_{C_{}}\) for the Jacobian's matrix view [e.g. 16]. Figure 2(b) shows the input Jacobian \(_{}\) which is a tensor view of \(()\), as expected from the matrix-vector perspective of Equation (2).

Differentiating a TN is more convenient than using matrix calculus  as it amounts to a simple graphical manipulation, does not rely on a flattening convention, and therefore preserves the full index structure. The resulting TN can still be translated back to matrix language, if desired. It also simplifies the computation of higher-order derivatives (e.g. \(}}{{}}\)), since differentiation yields another TN and can thus be repeated. If a tensor occurs more than once in a TN, the product rule applies and the derivative is a sum of TNs with one occurrence removed.

Figure 3: TN differentiation as graphical manipulation. (a) Differentiating convolution w.r.t. **W** is cutting it out of the diagram and yields the weight Jacobian. (b) Same procedure applied to the Jacobian w.r.t. **X**. (c) VJP for the weight and (d) input Jacobian (transpose convolution). Jacobians are shaded, only their contraction with \(^{()}\) is highlighted.

### Autodiff & Connections to Transpose Convolution

Although Jacobians are useful, crucial routines for autodiff are vector-Jacobian and Jacobian-vector products (VJPs, JVPs). Both are simple to realize with TNs due to access to full Jacobians. VJPs are used in backpropagation to pull back a tensor \(^{()}^{C_{} C_{1} C_{2}}\) from the output to the input or weight space. The VJP results \(^{()}^{C_{} I_{1} I_{2}}\) and \(^{()}^{C_{} C_{}  K_{1} K_{2}}\) are

\[^{()}_{c_{}^{},c_ {1}^{},c_{2}^{}}=_{c_{},c_{1},c_{2}}V^{()}_ {c_{},c_{1},c_{2}}},c_{1},c_{2}}}{ \,X_{c_{}^{},c_{1}^{},c_{2}^{}}}\,, V ^{()}_{c_{}^{},c_{1}^{},c_{2}^{}}=_{ c_{},c_{1},c_{2}}V^{()}_{c_{},c_{1},c_{2}} },c_{1},c_{2}}}{\,W_{c_{}^{ },c_{}^{},c_{1}^{},c_{2}^{}}}\,.\]

Both are simply new TNs constructed from contracting the vector with the respective Jacobian, see Figures 2(c) and 2(d) (VJPs are analogous). The input VJP is often used to define transpose convolution . In the matrix-multiplication perspective (Equation (2)), this operation is defined relative to a convolution with kernel **W** by multiplication with \(()^{}\), i.e. using the same connectivity pattern but mapping from the convolution's output to input space. The TN in Figure 2(d) makes this sharing explicit and cleanly defines transpose convolution.4

### Kronecker-factored Approximate Curvature

The Jacobian diagrams allow us to construct the TNs of second-order information like the Fisher/generalized Gauss-Newton (GGN) matrix and sub-tensors like its diagonal (SSC). Here, we focus on the popular Kronecker-factored approximation of the GGN [47; 27; 23; 48] whose input-based Kronecker factor relies on the unfolded input \([]\) which requires large memory. State-of-the-art libraries that provide access to KFAC [17; 51] also use this approach. Using TNs, we can often avoid expanding \([]\) explicitly and save memory. Here, we describe the existing KFAC approximations and their TNs (see SS5.1 for their run time evaluation).

**KFC (KFAC-expand):** Grosse & Martens  introduce a Kronecker approximation for the kernel's GGN, \(\) where \(^{C_{} C_{}}\) and the input-based factor \(=[][]^{}^{C_{}K_{1}K_{2} C_{}K_{1}K_{2}}\) (Figure 3(a)), the unfolded input's self-inner product (averaged over a batch).

**KFAC-reduce:** Eschenhagen et al.  generalized KFAC to graph neural networks and transformers based on the concept of weight sharing, also present in convolutions. They identify two approximations: KFAC-expand and KFAC-reduce. The former corresponds to KFC . The latter shows similar performance in downstream tasks, but is cheaper to compute. It relies on the column-averaged unfolded input, i.e. the average over all patches sharing the same weights. KFAC-reduce approximates \(}}\) with \(}^{C_{} C_{}}\) and \(}=}{{({O_{1}O_{2}})^{2}}}\,}^{}[](1^{}_{O_{1}O_{2}}[])^{} ^{C_{}K_{1}K_{2} C_{}K_{1}K_{2}}\) (Figure 3(b); averaged over a batch). For convolutions, this is arguably a'more natural' approximation as it becomes exact in certain limits , in contrast to the expand approximation.

**KFAC for transpose convolution:** Our approach enables us to derive KFAC for transpose convolutions. We are not aware of previous works doing so. This seems surprising because, similar to SS2.1, transpose convolution can be seen as matrix multiplication between the kernel and an unfolded input. From this formulation we can immediately obtain KFAC through the weight sharing view of Eschenhagen et al. . The Kronecker factor requires unfolding the input similar to im2col, but for transpose convolutions. This operation is currently not provided by ML libraries. We can overcome this limitation, express the unfolding operation as TN, and--for the first time--establish KFAC (expand and reduce) for transpose convolutions (see SSB.4 for details).

Figure 4: TNs of input-based Kronecker factors for KFAC approximations of the Fisher/GGN (no batching, no groups). The unfolded input is shaded, only additional contractions are highlighted. (a) \(\) (KFC/KFAC-expand) from Grosse & Martens , (b) \(}\) (KFAC-reduce) from Eschenhagen et al.  (vectors of ones effectively amount to sums).

## 4 TN Simplifications & Implementation

Many convolutions in real-world CNNs use structured connectivity patterns that allow for simplifications which we describe here along with implementation aspects.

### Index Pattern Structure & Simplifications

The index pattern \(}\) encodes the connectivity of a convolution and depends on its hyper-parameters. Along one dimension, \(}=}(I,K,S,P,D)\) with input size \(I\), kernel size \(K\), stride \(S\), padding \(P\), and dilation \(D\). We provide pseudo-code for computing \(}\) in \(\) which is easy to implement efficiently with standard functions from any numerical library (Algorithm D1). Its entries are

\[[}(I,K,S,P,D)]_{i,o,k}=_{i,1+(k-1)D+(o -1)S-P}\,,\] (7)

with \(i=1,,I,o=1,,O,k=1,,K\) and output size \(O(I,K,S,P,D)=1+(I+2P-(K+(K-1)(D-1)))/S\). Since \(}\) is binary and has size linear in \(I,O,K\), it is cheap to pre-compute and cache. The index pattern's symmetries allow for re-wiring a TN. For instance, the symmetry of \((k,D)\) and \((o,S)\) in Equation (7) and \(O(I,K,S,P,D)\) permits a _kernel-output swap_, exchanging the role of kernel and output dimension (Figure 4(a)). Rochette et al.  used this to phrase the per-example gradient computation (Figure 4(c)) as convolution.

For many convolutions of real-world CNNs (see SSE for a hyper-parameter study) the index pattern possesses structure that simplifies its contraction with other tensors into either smaller contractions or reshapes: _Dense convolutions_ use a shared kernel size and stride, and thus process non-overlapping adjacent tiles of the input. Their index pattern's action can be expressed as a cheap reshape (Figure 4(b)). Such convolutions are common in DenseNets , MobileNets [31; 60], ResNets , and ConvNeXts . InceptionV3  has 2d _mixed-dense convolutions_ that are dense along one dimension. _Down-sampling convolutions_ use a larger stride than kernel size, hence only process a sub-set of their input, and are used in ResNet18 , ResNext101 , and WideResNet101 . Their pattern contracts with a tensor \(}\) like that of a dense convolution with a sub-tensor \(}}\) (Figure 4(c)). SS5.1 shows that those simplifications accelerate computations.

### Practical Benefits of the TN Abstraction & Limitations for Convolutions

**Contraction order optimization:** There exist various orders in which to carry out the summations in a TN and their performance can vary by orders of magnitude. One extreme approach is to carry out all summations via nested for-loops. This so-called Feynman path integral algorithm requires little memory, but many FLOPS since it does not re-cycle intermediate results. The other extreme is sequential pair-wise contraction. This builds up intermediate results and can greatly reduce FLOPS. The schedule is represented by a binary tree, but the underlying search is in general at least #P-hard . Fortunately, there exist heuristics to find high-quality contraction trees for TNs with hundreds of tensors [32; 25; 13], implemented in packages like opt_einsum.

**Index slicing:** A common problem with high-quality schedules is that intermediates exceed memory. Dynamic slicing  (e.g. cotengra) is a simple method to decompose a contraction until it becomes feasible by breaking it up into smaller identical sub-tasks whose aggregation adds a small overhead. This enables peak memory reduction and distribution.

**Sparsity:**\(}\) is sparse as only a small fraction of the input contributes to an output element. For a convolution with stride \(S<K\) and default parameters (\(P=0,D=1\)), for fixed output and kernel indices \(k,o\), there is exactly one non-zero entry in \([}]_{:,o,k}\). Hence \((})=OK\), which corresponds to a sparsity of \(}{{l}}\). Padding leads to more kernel elements that do not contribute to an

Figure 5: TN illustrations of index pattern simplifications and transformations. See § D.3 for the math formulation.

output pixel, and therefore a sparser \(}\). For down-sampling and dense convolutions, we showed how \(}\)'s algebraic structure allows to simplify its contraction. However, if that is not possible, \(}\) contains explicit zeros that add unnecessary FLOPS. One way to circumvent this is to match a TN with that of an operation with efficient implementation (like im2col, (transpose) convolution) using transformations like the _kernel-output swap_ or by introducing identity tensors to complete a template, as done in Rochette et al. , Dangel  for per-sample gradients and im2col.

**Approximate contraction & structured dropout:** TNs offer a principled approach for stochastic approximation via Monte-Carlo estimation to save memory and run time at the cost of accuracy. The basic idea is best explained on a matrix product \(:==_{n=1}^{N}[]_{:,n}[]_{n,:}\) with \(^{I N},^{N,O}\). To approximate the sum, we introduce a distribution over \(n\)'s range, then use column-row-sampling [CRS, 1] to form an unbiased Monte-Carlo approximation with sampled indices, which only requires the sub-matrices with active column-row pairs. Bernoulli-CRS samples without replacement by assigning a Bernoulli random variable \((_{n})\) with probability \(_{n}\) for column-row pair \(n\) to be included in the contraction. The Bernoulli estimator is \(}:=_{n=1}^{N}}}{{_{n}}}[ ]_{n,:}[]_{n,:}\) with \(z_{n}(_{n})\). With a shared keep probability, \(_{n}:=p\), this yields the unbiased estimator \(^{}=}{{p}}_{n=1,,N} ^{}^{}\) where \(^{}=\) and \(^{}=\) with \(=(z_{1},,z_{N})\) are the sub-matrices of \(,\) containing the active column-row pairs. CRS applies to a single contraction. For TNs with multiple sums, we can apply it individually, and also impose a distribution over the result indices, which computes a (scaled) sub-tensor.

## 5 Experiments

Here, we demonstrate computational benefits of TNs for less standard routines of second-order methods and showcase their flexibility to perform stochastic autodiff in novel ways.

### Run Time Evaluation

We implement the presented TNs' contraction strings and operands5 in PyTorch . The simplifications from SS4 can be applied on top and yield a modified einsum expression. To find a contraction schedule, we use opt_einsum with default settings. We extract the unique convolutions of 9 architectures for ImageNet and smaller data sets, then compare some operations from Table 1 with their standard implementation on an Nvidia Tesla T4 GPU (16 GB); see SSF for all details. Due to space constraints, we highlight important insights here and provide references to the corresponding material in the appendix. In general, the performance gap between standard and TN implementation decreases the less common an operation is (Figure F17); from forward pass (inference & training), to VJPs (training), to KFAC (training with a second-order method). This is intuitive as more frequently used routines have been optimized more aggressively.

**Impact of simplifications:** While general convolutions remain unaffected (Figure F18d) when applying the transformations of SS4, mixed dense, dense, and down-sampling convolutions consistently enjoy significant run time improvements (Figures F18a to F18c). As an example, we show the performance comparison for dense convolutions in Figure 6: The performance ratio's median between TN and standard forward and input VJP is close to 1, that is both require almost the same time. In the median, the TN even outperforms PyTorch's highly optimized weight VJP, also for down-sampling

Figure 6: Run time ratios of TN (w/o simplifications) vs. standard implementation for dense convolutions of 9 CNNs. With simplifications, convolution and input VJP achieve median ratios slightly above 1, and the TN implementation is faster for weight VJP, KFC & KFAC-reduce. The code in Figure 1 corresponds to default, TN, and simplified TN KFC implementation.

convolutions (Figure F21). For KFC, the median performance ratios are well below 1 for dense, mixed dense & sub-sampling convolutions (Figure F22).

**KFAC-reduce:** For all convolution types, the TN implementation achieves its largest improvements for \(}\) and consistently outperforms the PyTorch implementation in the median when simplifications are enabled (Figure F23). The standard implementation unfolds the input, takes the row-average, then forms its outer product. The TN does not need to expand \(\) in memory and instead averages the index pattern tensors, which reduces peak memory and run time. We observe performance ratios down to 0.22 x (speed-ups up to \( 4.5\) x, Table F9) and consistently lower memory consumption with savings up to 3 GiB (Figure 7). Hence, our approach not only significantly reduces the overhead of 2nd-order optimizers based on KFAC-reduce, but also allows them to operate on larger batches without exceeding memory (Eschenhagen et al.  specifically mention memory as important limitation of their method). Other examples for KFAC algorithms where computing the input-based Kronecker factor adds significant time and memory overhead are that of Petersen et al. , Benzing  which only use \(\) (setting \(\)), or Lin et al.  which remove matrix inversion.

**Downstream improvements with KFAC-reduce:** To demonstrate the speed-ups of KFAC-reduce in practical algorithms, we apply our work to the SINGD optimizer  and benchmark the impact of our TN implementation on its memory and run time in comparison to SGD without momentum. Concretely, we investigate SINGD with KFAC-reduce and diagonal pre-conditioners on ResNet18 and VGG19 on ImageNet-like synthetic data \((3,256,256)\) using a batch size of 128. We measured per-iteration time and peak memory on an NVIDIA A40 with 48 GiB of RAM. For SINGD, we compare computing the Kronecker factors with the standard approach ('SINGD') via input unfolding versus our TN implementation ('SINGD+TN'). Table 2 summarizes the results.

On both nets, our TN implementation halves SINGD's run time, and almost completely eliminates the memory, overhead compared to SGD. On VGG19, it dramatically lowers the memory overhead, cutting it down by a factor of 2 from 32 GiB to 16 GiB. This enables using larger batches or more frequently updating the pre-conditioner, underlining the utility of our approach for reducing the computational gap between approximate second-order and first-order methods.

### Randomized Autodiff via Approximate Contraction

CRS is an alternative to checkpointing  to lower memory consumption of backpropagation . Here, we focus on unbiased gradient approximations by applying the exact forward pass, but CRS when computing the weight VJP, which requires storing a sub-tensor of \(\). For convolutions, the approaches of existing works are limited by the supported functionality of ML libraries. Adelman et al.  restrict to sampling \(\) along \(c_{}\), which eliminates many gradient entries as the index is part of the gradient. The randomized gradient would thus only train a sub-tensor of the kernel per step. Oktay et al. , Chen et al.  apply unstructured dropout to \(\), store it in sparse form, and restore the sparsified tensor during the backward pass. This reduces memory, but not computation.

Our TN implementation is more flexible and can, for example, tackle spatial dimensions with CRS. This reduces memory to the same extent, but also run time due to fewer contractions. Importantly, it

    & **Optimizer** & **Per iter.[s]** & **Peak mem. [GiB]** \\     SGD \\ SINGD \\ SINGD+TN \\  } } \\  } & SGD & \(0.12\) (\(1.0\) x) & \(3.6\) (\(1.0\) x) \\  & SINGD & \(0.19\) (\(1.7\) x) & \(4.5\) (\(1.3\) x) \\  & SINGD+TN & \(0.16\) (\(1.3\) x) & \(3.6\) (\(1.0\) x) \\     SGD \\ SINGD+TN \\  } } \\  } & SGD & \(0.69\) (\(1.0\) x) & \(14\) (\(1.0\) x) \\  & SINGD & \(1.0\) (\(1.5\) x) & \(32\) (\(2.3\) x) \\  & SINGD+TN & \(0.80\) (\(1.2\) x) & \(16\) (\(1.1\) x) \\   

Table 2: Impact of our TN implementation on SINGD’s run time and peak memory compared to SGD.

Figure 7: Extra memory used by the standard versus our TN implementation (simplifications enabled) of KFAC-reduce. Each point represents a convolution from 9 CNNs, clipped below by 1 MiB. TNs consistently use less memory than the standard implementation (one exception), and often no extra memory at all. We observe memory savings up to 3 GiB.

does not zero out the gradient for entire filters. In Figure 8 we compare the gradient approximation errors of channel and spatial sub-sampling. For the same memory reduction, spatial sub-sampling yields a smaller approximation error on both real & synthetic data. E.g., instead of keeping 75 % of channels, we achieve the same approximation quality using only 35 % of pixels.

## 6 Related Work

**Structured convolutions:** We use the TN formulation of convolution from Hayashi et al.  who focus on connecting kernel factorizations to existing (depth-wise separable [31; 60], factored , bottleneck , flattened/CP decomposed, low-rank filter [67; 57; 70]) convolutions and explore new factorizations. Our work focuses on operations related to convolutions, diagram manipulations, the index pattern structure, and computational performance/flexibility. Structured convolutions integrate seamlessly with our framework by replacing the kernel with its factorized TN.

**Higher-order autodiff:** ML frameworks focus on differentiating scalar-valued objectives once. Recent works [37; 38; 43] developed a tensor calculus to compute higher-order derivatives of tensor-valued functions and compiler optimizations through linear algebra and common sub-expression elimination. Phrasing convolution as einsum, we allow it to be integrated into such frameworks, benefit from their optimizations, and complement them with our convolution-specific simplifications.

## 7 Conclusion

We used tensor networks (TNs), a diagrammatic representation of tensor multiplications, to simplify convolutions and many related operations. We derived the diagrams of autodiff and less standard routines for curvature approximations like KFAC with support for all hyper-parameters, channel groups, batching, and generalization to arbitrary dimensions. All amount to simple einsum expressions that can easily be modified--e.g. to perform stochastic backpropagation--and benefit from under-the-hood optimizations before evaluation. We complemented those by convolution-specific symbolic simplifications based on structure in the connectivity pattern and showed their effectiveness to advance second-order methods. Our TN implementation accelerates the computation of KFAC up to 4.5 x and uses significantly less memory. Beyond performance improvements, the simplifying perspective also allowed us to formulate KFAC for transpose convolution. More broadly, our work underlines the elegance of TNs for reasoning about tensor multiplications and function transformations (differentiation, batching, slicing, simplification) in terms of diagrams at less cognitive load without sacrificing rigour. We believe they are a powerful tool for the ML community that will open up new algorithmic possibilities due to their simplicity & flexibility.

Figure 8: Sampling spatial axes is more effective than channels on both (a) real-world and (b) synthetic data. We take the untrained All-CNN-C  for CIFAR-100 with cross-entropy loss, disable dropout, and modify the convolutions to use a fraction \(p\) of **X** when computing the weight gradient via Bernoulli-CRS. For mini-batches of size 128, we compute the deterministic gradients for all kernels, then flatten and concatenate them into a vector \(\); likewise for its proxy \(}\). CRS is described by \((p_{c_{i}},p_{i_{1}},p_{i_{2}})\), the keep rates along the channel and spatial dimensions. We compare channel and spatial sampling with same memory reduction, i.e. \((p,1,1)\) and \((1,,)\). To measure approximation quality, we use the normalized residual norm \(-\|_{2}}}{{\|\|_{2}}}\) and report mean and standard deviation of 10 different model and batch initializations.