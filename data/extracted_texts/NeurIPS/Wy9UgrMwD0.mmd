# No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO

Skander Moalla\({}^{1}\) Andrea Miele\({}^{1}\) Daniil Pyatko\({}^{1}\) Razvan Pascanu\({}^{2}\) Caglar Gulcehre\({}^{1}\)

\({}^{1}\) CLAIRE, EPFL Google DeepMind

Correspondence to skander.moalla@epfl.ch.

###### Abstract

Reinforcement learning (RL) is inherently rife with non-stationarity since the states and rewards the agent observes during training depend on its changing policy. Therefore, networks in deep RL must be capable of adapting to new observations and fitting new targets. However, previous works have observed that networks trained under non-stationarity exhibit an inability to continue learning, termed loss of plasticity, and eventually a collapse in performance. For off-policy deep value-based RL methods, this phenomenon has been correlated with a decrease in representation rank and the ability to fit random targets, termed capacity loss. Although this correlation has generally been attributed to neural network learning under non-stationarity, the connection to representation dynamics has not been carefully studied in on-policy policy optimization methods. In this work, we empirically study representation dynamics in Proximal Policy Optimization (PPO) on the Atari and MuJoCo environments, revealing that PPO agents are also affected by feature rank deterioration and capacity loss. We show that this is aggravated by stronger non-stationarity, ultimately driving the actor's performance to collapse, regardless of the performance of the critic. We ask why the trust region, specific to methods like PPO, cannot alleviate or prevent the collapse and find a connection between representation collapse and the degradation of the trust region, one exacerbating the other. Finally, we present Proximal Feature Optimization (PFO), a novel auxiliary loss that, along with other interventions, shows that regularizing the representation dynamics mitigates the performance collapse of PPO agents. Code and run histories are available at https://github.com/CLAIRE-Labo/no-representation-no-trust.

## 1 Introduction

Reinforcement learning (RL) agents are inherently subject to non-stationarity as the states and rewards they observe change during learning. Therefore, neural networks in deep RL must be capable of adapting to new inputs and fitting new targets. However, previous works have observed that networks trained under non-stationarity exhibit an inability to continue learning, termed loss of plasticity, and a collapse in performance (Dohare et al., 2021; Abbas et al., 2023; Kumar et al., 2023; Dohare et al., 2023, 2023). Kumar et al. (2021); Lyle et al. (2022) connect this phenomenon to representation dynamics and show that value networks in off-policy value-based RL algorithms exhibit a decrease in the rank of their representations, termed feature rank collapse, and a decrease in their ability to regress to arbitrary targets, called capacity loss. Although this deterioration in representation is more generally attributed to neural networks trained under non-stationarity (Lyle et al., 2023), the connection to representation dynamics has been overlooked in on-policy policy optimization methods. In particular, Proximal Policy Optimization (PPO) (Schulman et al., 2017), one of the most popular policy optimization methods, makes several minibatch updates over non-stationary data, unlike vanilla policy gradient methods, and optimizes a surrogate loss that depends on a moving oldpolicy. This raises the question of how much PPO agents are impacted by the same representation degradation attributed to non-stationarity. Dohare et al. (2021, 2023a, 2023) have shown that PPO agents lose plasticity throughout training but have only diagnosed it as a collapse in performance or as an Adam optimization issue. Igl et al. (2021) have shown that non-stationarity affects the generalization of PPO agents (learning speed when training episodes are very different otherwise performance at test time on novel episodes) but not necessarily training, and no connection was made with the feature rank and capacity measures used in the recent value-based works. One crucial outstanding question is why the trust region embedded in methods like PPO is unable to prevent the deterioration in policy by constraining its update. To address these gaps, we present the following contributions:

1. We provide the first study of feature rank and capacity loss in on-policy policy optimization, revealing that PPO agents in the Arcade Learning Environment (Bellemare et al., 2013) and MuJoCo (Todorov et al., 2012) environments are subject to representation collapse.
2. We draw connections between representation collapse, performance collapse, and trust region issues in PPO, showing that PPO's clipping becomes ineffective under poor representations and fails to prevent performance collapse, which is irrecoverable due to loss of capacity. We further isolate the breakdown of the trust region in a theoretical setting.
3. We corroborate these connections by performing interventions that regularize non-stationarity and representations and result in a better trust region and mitigation of performance collapse, incidentally giving insights on sharing an actor-critic trunk.
4. We propose _Proximal Feature Optimization_ (PFO), a new regularization on the representation of the policy that regularizes the change in pre-activations. PFO strengthens our analysis by addressing the representation issues and mitigating performance collapse.
5. We open source our code providing a comprehensive and reproducible codebase for studying representation dynamics in policy optimization and a large database of run histories with extensive logging for further investigation on this topic.

## 2 Background

#### 2.0.1 Reinforcement Learning (Sutton & Barto, 2018)

We formalize our RL setting with the finite-horizon undiscounted Markov decision process, describing the interaction between an agent and an environment with finite 2 sets of states \(\) and actions \(\), and a reward function \(r:}\). An initial state \(S_{0}\) is sampled from the environment, then at each time step \(t\{0,,t_{}-1\}\), the agent observes the state \(S_{t}\), picks an action \(A_{t}\) according to its policy \(:()\) with probability \((A_{t}|S_{t})\), 3 observes the next state \(S_{t+1} S\) sampled from the environment and receives a reward \(R_{t+1} r(S_{t},A_{t},S_{t+1})\). We denote by \(G_{t}_{k=t}^{t_{}-1}R_{k+1}\) the return after the action at time step \(t\). The goal of the agent is to maximize its expected return \(J()_{}_{t=0}^{t_{}-1}R_{t+1}= _{}[G_{0}]\) over the induced random trajectories. We discuss the choice of this setting in Appendix A.1.

#### 2.0.2 Actor-Critic Agent

We consider on-policy deep actor-critic agents which train a policy (or actor) network \((;)\) also denoted \(_{}\), and a value (or critic) network \((;)\) that approximates the return of \(_{}\) at every state. At every training stage, the agent collects a batch of samples, called rollout, with its current policy \(_{}\), and both networks are trained with gradient descent on this data. The critic is trained to minimize the Euclidean distance to an estimator of the returns (e.g., \(G_{t}\)). We use \(\)-returns computed with the Generalized Advantage Estimator (GAE) (Schulman et al., 2015). The actor is trained with the Proximal Policy Optimization (PPO) (Schulman et al., 2017).

#### 2.0.3 Proximal Policy Optimization

PPO-Clip, the most popular variant of PPO algorithms (Schulman et al., 2017), optimizes the actor by repeatedly maximizing the objective in Equation 1 at each rollout.

\[L^{CLIP}_{_{}}()=_{_{}}\!\!\! [_{t=0}^{t_{}-1}(}(A_{t}|S_{t})}{ _{}(A_{t}|S_{t})}_{t},(}(A_{t}|S_{t})}{_{}(A_{t}|S_{t})},1+,1-) _{t})]\] (1)The objective is defined for some small hyperparameter \(\); \(_{}\) is the last \(_{}\) of the previous optimization phase, used to collect a training batch (rollout) after each optimization phase; \(_{t}\) is an estimator of the advantage of \(_{}\) (e.g., \(_{t}=G_{t}-(S_{t};_{})\)); we use the GAE in our experiments. An optimization phase consists of maximizing the objective with minibatch gradient steps over multiple epochs on the training batch. We refer to PPO-Clip as PPO and provide a pseudocode in Algorithm 1.

Intuitively, PPO aims to maximize the policy advantage \(_{_{}}\![_{t=0}^{t_{}-1}}(A_{t}|S_{t})}{_{}(A_{t}|S_{t})}_{t}]\) defined by Kakade & Langford (2002), which participates in a lower bound. In the improvement of \(_{}\) given that it is close to \(_{}\)(Schulman et al., 2015, see Theorem 1). In this regard, a gradient step on \(L^{CLIP}_{_{}}()\) would increase (resp. decrease) the probability of actions at states yielding positive (resp. negative) advantage until the ratio between the policies for those actions reaches \(1+\) (resp. \(1-\)) at which point the gradient at those samples becomes null. This is a heuristic to ensure a trust region that keeps policies close to each other, resulting in policy improvement.

**Non-stationarity in deep RL and PPO** The actor and the critic networks are both subject to non-stationarity in deep RL. As the agent improves, it visits different states, shifting the distribution of states which makes the networks' input distribution non-stationary. This also holds for the targets to fit the critic, which change as the returns of the policy change. Unlike vanilla policy gradient (Sutton et al., 1999), and A2C (Mnih et al., 2016), PPO's objective is optimized by performing multiple epochs of minibatch gradient descent on the current collected batch, potentially making the networks more likely to be impacted by previous training rollouts. In this sense, increasing the number of epochs in PPO can cause the agent to "overfit" more to previous experience.

**Feature rank** As done in most works studying feature dynamics in deep RL (Lyle et al., 2022; Kumar et al., 2021), we refer to the activations of the last hidden layer of a network (the penultimate layer) as the features or representation learned by the network. On a batch of \(N\) samples, this gives a matrix of dimensions \(N D\) denoted by \(\), where \(D<N\) is the width of the penultimate layer. Several measures of the rank of this matrix have been used to quantify the "quality" of the representation (Kumar et al., 2021; Gulcehre et al., 2022; Lyle et al., 2022; Andriushchenko et al., 2023). Their absolute values differ significantly, but their dynamics are often correlated. We track all of the different rank metrics in our experiments, compare them in Appendix E, and use the _approximate rank_ in our main figures for its connection to principal component analysis (PCA). Given a threshold \(\) and the singular values \(_{i}(),,_{D}()\) of \(\) in decreasing order, the approximate rank of \(\) is \(_{k}\{^{k}_{j}^{2}()}{_{j=1}^{k} _{j}^{2}()}>1-\}\) which corresponds to the smallest dimension of the subspace recovering \((1-)\%\) of the variance of \(\). We use \(=0.01\) i.e. the reconstruction recovers \(99\%\) of the variance as done by Andriushchenko et al. (2023); Yang et al. (2020). We refer to this metric as _feature_ rank with reference to the rank of the _feature_ matrix when there is no ambiguity.

**Capacity loss** Target-fitting capacity (Lyle et al., 2022) is computed on checkpoints of a network undergoing some training to measure the evolution of its ability to fit some chosen target independent from its training. It is a concrete metric to evaluate plasticity. Given a fixed target (distribution over inputs and outputs) and a fixed optimization budget, a checkpoint's capacity loss is the loss from fitting the checkpoint to the target at the end of the optimization budget. Usually, the capacity of a deep RL agent is measured by its ability to fit the outputs of a model initialized randomly from the same distribution as the agent on a fixed rollout collected by this target random model (Lyle et al., 2022; Nikishin et al., 2023). We follow this practice. The data would in expectation be from the same distribution as the agent's initial checkpoint. To fit the critic, we use an \(L^{2}\) loss on the outputs of the models. To fit the actor, we use a KL divergence between the target and the checkpoint (forward KL).

## 3 Deteriorating representations, collapse, and loss of trust

It is well-known that non-stationarity in deep RL can be a factor causing issues in representation learning. However, most of the observations have been made in value-based methods showing that value networks are prone to rank collapse, harming their expressivity, and in turn, the performance of the agent (Lyle et al., 2022; Kumar et al., 2022). Non-stationarity has been shown to impact PPO's generalization Igl et al. (2021) and performance in the long run or in a continual learning setting (Dohare et al., 2021, 2023, 20), but no evidence of representation deterioration was shown. Our motivation is to reuse the tools that showed that value-based methods are prone to representation collapse but in policy optimization methods for the first time. We focus on PPO for its popularity and its non-stationarity which is impacted and can be controlled by multi-epoch optimization.

Furthermore, a crucial question for PPO, compared to most value-based alternatives, is how the regularization implicit in PPO through its trust region interacts with representation and performance collapse. Intuitively it should prevent rapid degradation of the policy.

**Experimental setup** We begin our experiments by training PPO agents on the Arcade Learning Environment (ALE)(Bellemare et al., 2013) for pixel-based observations with discrete actions and on MuJoCo (Todorov et al., 2012) for continuous observations with continuous actions. To keep our experiments tractable, we choose the Atari-5 subset recommended by Aitchison et al. (2023) and add Gravitar to include at least one sparse-reward hard-exploration game from the taxonomy presented by Bellemare et al. (2016). For MuJoCo, we train on Ants, Half-Cheetahs, Humanoids, and Hoppers, which have varying complexity and observation and output sizes. We use the same model architectures and hyperparameters as popular implementations of PPO on ALE and MuJoCo (Raffin et al., 2021; Huang et al., 2022b); these are also the architectures and hyperparameters used by Schulman et al. (2017) in the original implementation of PPO; they do not include normalization layers. For MuJoCo we further adopt a parameterization of the output action distribution using a TanhNormal4 with both its mean and variance depending on the state representation as done by Haarnoja et al. (2018); Andrychowicz et al. (2021). As we study the connection between performance and representation dynamics this is a more natural choice than using the commonly implemented state-independent variance which would be independent of representation dynamics. The ALE models use ReLU activations (Nair and Hinton, 2010) and the MuJoCo ones tanh; we also experiment with ReLU on MuJoCo. We use separate actor and critic models for both environments unless specified in Section 4. Details on the performance metrics and tables of all environment parameters, model architectures, and algorithm hyperparameters are presented in Appendix B. Observing that the previous findings on the feature dynamics of value-based approaches (Gulcehre et al., 2022; Lyle et al., 2022) apply to the critic of PPO as well since the loss function is the same, we focus on studying the feature dynamics of the actor unless stated otherwise in the text or figures.

We vary the number of epochs as a tool to control the effects of non-stationarity, which gives the agent a more significant number of optimization steps per rollout while not changing the optimal target it can reach due to clipping, as opposed to changing the value of \(\) in the trust region for example.5 We keep the learning rate constant throughout training and use the same learning rate for all the epoch configurations.6 To understand the feature dynamics, we measure different metrics that are proposed in the literature, including feature rank, number of dead neurons (Gulcehre et al., 2022), capacity loss (Lyle et al., 2022), and penultimate layer pre-activation norm. Previous work has monitored feature norm values as well (Abbas et al., 2023; Lyle et al., 2024); however, in our case, we found that as the neurons in the policy network die, the feature norm might be stable while the pre-activation norm blows up. All the metrics are computed on on-policy rollouts except for the capacity loss.

We run five seeds per hyperparameter configuration and report mean curves with min/max shaded regions unless specified otherwise. All curves, except for capacity loss, are smoothed using an exponentially weighted moving average with a coefficient of \(0.05\).

### PPO suffers from deteriorating representations

**Deteriorating representation** How do the representation metrics of a PPO agent such as the _feature rank_ and the _capacity loss_, evolve during training? Are they subject to the same decline observed by Kumar et al. (2021); Lyle et al. (2022) in value-based methods? Does it affect performance?

As illustrated in Figure 1 with ALE/Phoenix as an example, we observe a consistent increase in the norm of the pre-activations of the feature layer of the policy network. Learning curves for all the ALE games and MuJoCo tasks considered can be found in Appendix D. The increase in feature norm is present in all the games/tasks considered in both environments, that is, with the two different model architectures and activation functions in the case of MuJoCo. We associate the rapid growth in the norm of the pre-activations with an eventual decline in the policy network's feature rank. We observe a rank decline in five out of six ALE games and seven out of eight MuJoCo tasks (four with ReLU and three with tanh). The same observations about the increasing norm of the pre-activations can be made about the critic network. However, its rank varies more with the sparsity of the reward: in most environments, its rank experiences a significant deterioration after the policy's performance declines (not the policy's rank) and rewards become sparser, and in the sparse-reward game Gravitar, the critic's rank collapses before the policy. Furthermore, capacity loss is increasing for the critic, as observed in value-based plasticity studies (Lyle et al., 2022), and we also show that is the case for the actor, for which it explodes around rank collapse.

**Worse consequences** How does increasing the number of epochs per rollout to vary non-stationarity affect a PPO agent's representation? Does it degrade as observed in DQN and SAC agents when increasing the replay ratio (Nikishin et al., 2022; Kumar et al., 2022)?

Increasing the replay ratio in DQN and SAC deteriorates the agent's representation and, in turn, its performance (Kumar et al., 2022; D'Oro et al., 2023). This is commonly attributed to "overfitting" to previous experience (Nikishin et al., 2022). Increasing the number of epochs in PPO is analogous, and a natural hypothesis is that this would accelerate the deterioration of the policy's representation. Figure 1 shows that increasing the number of epochs accelerates the increase of pre-activations norm and the decrease of the policy's feature rank.7 In some cases, the rank eventually collapses, coinciding with the policy's performance collapse. We observe the performance collapse in three of the six ALE games and three of the four MuJoCo tasks.

**Characterizing the collapse** The collapse we observe is distinct from the typical entropy collapse. Figure 2 shows that the policy reaches a high entropy. A high overall entropy can come from an average of high-entropy states with different action distributions or trivially from the same high-entropy distribution in all states. Our analysis reveals the latter, a zero policy variance across states. This corresponds to a collapsed representation where most neurons are inactive. 8 The output thus relies solely on the bias term, as linear weights act on a null feature vector, making actions near uniform across all states and collapsing performance on complex tasks.

### Collapsed representations create trust issues and unrecoverable loss

Intuitively, the heuristic trust region set by PPO-Clip should prevent sudden catastrophic changes and limit the rank collapse, which induces worse performance. However, empirically, it seems the trust region cannot mitigate the collapse. In this section, we seek to understand the interaction between the rank collapse and the trust region. We argue that as rank collapses, the clipping constraint becomes unreliable and unable to restrict learning. This is in line with previous works that have pointed out

Figure 1: **Deteriorating performance and representation metrics** The policy network of a PPO-Clip agent on ALE/Phoenix-v5 is subject to a deteriorating representation. The norm of the pre-activations of the penultimate layer consistently increases, and its rank eventually decreases. Performing more optimization epochs per rollout to increase the effects of non-stationarity accelerates the growth of the norm of the pre-activations and the collapse of its rank. This ultimately leads to the collapse of the policy. This collapse is not driven by the value network, whose rank is still high. Both networkâ€™s ability to fit arbitrary targets (capacity loss) is also worsening.

Figure 2: **Rank collapse gives a high but trivial entropy** The rank collapse of the policy gives a policy with high entropy but zero variance across states. The network outputs the same high-entropy action distribution in all states, as all the neurons in the feature layer are dead. Its output only depends on the constant bias term.

that probability ratios during training can go beyond the clipping limits with PPO-Clip (Engstrom et al., 2020; Wang et al., 2020; Sun et al., 2022). We believe, however, that this behavior is systematic when rank collapses and does not merely happen occasionally.

Wang et al. (2020, Theorem 2) state that when the gradients of the unclipped samples align with the gradients of clipped samples, the clipped samples' ratios will have their probabilities continue to go beyond the clip limit. They claim this condition would hold in practice because of "optimization tricks" or optimizer accumulated moments; however, there is no evidence that these factors induce the gradient alignment or that the alignment is present in practice. Our intuition is that representation degradation leads to alignment in the gradients and, therefore, a breakdown of the trust region constraint. This can create a snowball effect, preventing PPO-Clip from preventing representation collapse. We summarize this in two observations:

Loss of trust is extreme around poor representationsThe average of probability ratios outside the clipping limits (below \(1-\) in Figure 3) significantly diverges from the clipping limit around the collapse of the agent's representation. This gives one more reason why the PPO trust region can be violated. We isolate this in a toy setting and analyze it formally in the next section. We further show in Figure 4 scatter plots of the lowest average probability ratios in runs with their associated representation metrics.9 We observe no significant correlation in the regions where the representation is rich (high rank, low pre-activation norm), but an apparent decrease of the average of probability ratios below \(1-\) is observed as the representation reaches poor values. Note that we characterize the collapsing regime by an extremely low rank, however, it is not straightforward to draw a line between low-rank representations beneficial for generalization and extremely low-rank representations causing aliasing as also acknowledged by Gulcehre et al. (2022), but for environments like Atari, our figures seem to draw the line at single-digit ranks, which can be related to the action space of dimension 8+.

Loss of plasticity makes performance collapse unrecoverableThe persistent decrease in performance overlaps with a monotonic decrease in policy variance and PPO objective. It appears that as

Figure 4: **Representation vs trust region** Samples from ALE/Phoenix-v5 training curves. Each point maps an average of the probability ratios below the clipping limit vs. its corresponding average representation metric (dead neurons, feature rank, feature norm). The average ratios are significantly lower around poor representations (high dead neurons, low policy rank, high feature norm) reflecting the failure of the trust region in this regime. Averages are over non-overlapping windows larger than episodes.

Figure 3: **Focusing on individual runs** Individual training curves on ALE/NameThisGame-v5 with different epochs per batch. Extremely low ratios are observed around the representation collapse of a PPO-Clip agent, implying that the heuristic trust region breaks down when representation power is lacking. The last-minibatch value of the PPO objective decreases towards 0 around the representation collapse, implying a reduction in the ability to improve the policy and recover, which is corroborated by the increase in capacity loss. (Ratios are trivially above \(1-\) after collapse as a collapsed model does not change much to have values below \(1-\).)

the policy loses its ability to distinguish state, it can also ascend the PPO objective less and less at each batch (recall: after collecting a batch, the loss starts around zero with a normalized advantage, and through minibatch updates, the clipped policy advantage is ascended). Intuitively, this is implied by a loss of plasticity or a collapse in entropy (no new actions to learn from). As seen in Section 2 the entropy does not collapse, and measuring the capacity loss in Figure 3 shows that the decrease in objective gain is associated with a significant increase in capacity loss, implying loss of plasticity.

**Connecting the dots** Hence, around collapse, the representation of the policy is getting so poor that it is impacting its ability to distinguish and act differently across states; the trust region cannot prevent this catastrophic change as it also breaks down with a poor representation; finally, the policy's plasticity is also becoming so poor that the agent cannot recover by optimizing the surrogate objective.

**Implications and discussion** The causal connection we draw between the representation dynamics, the trust region, and the performance primarily holds around the collapse regime and not necessarily throughout training. However, this does not mean that one should only be concerned about the link when performance is starting to deteriorate. The representations don't collapse all of a sudden; they deteriorate throughout training until they reach collapse. Thus, mitigating representation degradation should happen throughout training and not only when around the collapsing regime. In addition, the connection gives important insights into the failure mode of the popular PPO-Clip algorithm, whose trust region is highly dependent on the representation quality, and more generally about trust-region methods which only constrain the output probabilities.

#### 3.2.1 A toy setting to understand the effects of rank collapse on trust region

We present a toy example that illustrates how a collapsed representation bypasses the clipping set by PPO and cannot satisfy the trust region it seeks to set. PPO constructs a trust region around the policy \(_{}(|s)\) of the agent evaluated at a given state \(s\), enforcing (in an approximate way) that the update computed on state \(s\) can not move the policy \(_{}(|s)\) outside of the trust region. However, the constraint does not capture how updates computed on another state \(s^{}\) affect the policy's probability distribution over the current state \(s\). The underlying assumption is that updates computed on different states are, at least in expectation, approximately orthogonal to each other, and they do not interact. Therefore, restricting the update of the current state is sufficient to keep the policy within the region.

In our case, however, one can show that as the rank collapses or the neurons die, the representations corresponding to different states become more colinear.10 Therefore, the gradients also become more colinear. In the extreme case, when the rank collapses to 1, or there is only one neuron alive, all representations are exactly colinear; therefore, all gradients are also. This means that even though clipping prevents the policy \(_{}(|s)\) on the current state \(s\) from changing due to the update of that state \( L(_{}(|s))\), \(_{}(|s)\) will still change and move outside of the trust region due to the updates on other states \(s^{}\). Leading to the trust region constraint being ineffective and not constraining the learning process in any meaningful sense. This gives a clear situation where the theorem of Wang et al. (2020) holds and can easily be analyzed as below without resorting to the theorem for an end-to-end proof or to get a better intuition.

**Formal statement of the toy setting** Let us consider a batch containing two state-action pairs \((x,a_{1})\) and \((y,a_{1})\) with sampled probabilities \(_{}(a_{1}|x)\) and \(_{}(a_{1}|y)\) and positive estimated advantages \(A(x,a_{1}),A(y,a_{1})>0\). Let \((x),(y)\) be fixed 1-dimensional representations of \(x\), and \(y\) that can be seen as the output of the (frozen) penultimate layer of a policy network with collapsed representation (all but one dead neuron), and let \(\) such that \((y)=(x)\). Let \(=[_{1},_{2}]\), be the last layer of the network, computing the logits of two actions, \(a_{1}\) and \(a_{2}\), that are fed into a softmax to compute the probabilities. I.e., \(_{}(a_{i}|s)=(s)}}{e^{_{1}(s)}+ e^{_{2}(s)}}\). Consider PPO

Figure 5: **Simulation of the toy setting** Left (\(>0\)): a gradient on \((x,a_{1})\) takes the probability of \((y,a_{1})\) up and vice versa. When one is above the threshold and should not increase, the other still pushes it. Right (\(<0\)): a gradient on \((x,a_{1})\) takes the probability of \((y,a_{1})\) down and vice versa. Both slow each down, with one forcing the other to be lower than its initial value.

minibatch updates alternating between \((x,a_{1})\) and \((y,a_{1})\). Ideally, the PPO loss increases \(_{}(a_{1}|s)\) at gradients on \((x,a_{1})\) until it reaches the clip ratio and similarly on \((y,a_{1})\). However, we show in Appendix C that a gradient step in \((x,a_{1})\) also affects \(_{}(a_{1}|y)\) and depending on \(\) will increase it past its clipped ratio, or decrease it below its initial value. Essentially, when \( 0\), a gradient on \((x,a_{1})\) increases \(_{1}^{}\) therefore increasing both \(_{^{}}(a_{1}|x)\) and \(_{^{}}(a_{1}|y)\). The same holds for a gradient on \((y,a_{1})\), causing one state to reach the clip limit first depending on \(>1\) but still have the other keep pushing its probability upwards. However, when \( 0\), a gradient on \((x,a_{1})\) increases \(_{1}^{}\) therefore increasing \(_{^{}}(a_{1}|x)\) but decreasing \(_{^{}}(a_{1}|y)\). For a gradient on \((y,a_{1})\) it is the opposite: \(_{1}^{}\) decreases therefore \(_{^{}}(a_{1}|x)\) decreases and \(_{^{}}(a_{1}|y)\) increases, causing each state to reduce the probability of the other, and depending on \(<1\) one of the probabilities will dominate and push the other one down. Figure 5 shows the evolution of the probabilities when simulating the updates empirically.

## 4 Intervening to regularize representations and non-stationarity

Having observed that PPO is affected by a frequent representation degradation that impacts its trust region heuristic and causes its performance to collapse, we turn to study interventions that aim at regularizing the representation of the policy network or reducing the non-stationarity in the optimization. We investigate whether these interventions improve the representation metrics we track and if in turn, this affects performance. We choose simple interventions that do not apply modifications to the models during training (e.g., resetting or adding neurons) or require significantly more memory (e.g., maintaining separate copies of the models). We perform interventions on the games/tasks where the collapse is the most significant. We are interested in the state of the agent at the end of the training budget. We record the performance and representation metrics for each run as averages over the last 5% of training progress. We measure the excess ratio at a timestep as the average probability ratio above \(1+\) divided by the average probability ratio below \(1-\) at that timestep. This metric gives an idea of how much the policy exceeds the trust region. Its average value is computed over the last 5% of training progress where the ratios are non-trivial, giving the same window at the end of training as the other metrics when there is no collapse, otherwise a window before total collapse covering 5% of training progress, as after collapse, the model does not change anymore and the ratios are trivially within the \(1+\) and \(1-\) limits. We give additional details on the computation of these aggregate metrics and the interventions performed in Appendix B.

**PFO: Regularizing features to mitigate trust-region issues** The motivation for our first intervention and our proposed regularization method comes from our observation that the norm of the preactivation features is consistently increasing, which can be linked to the trust-region issues discussed in Section 3. We seek to mitigate this effect in a way that is analogous to the PPO trust region, by extending the trust region to the feature space. We apply an \(L^{2}\) loss on the difference between the pre-activated features of the optimized policy and the policy that collected the batch, as a way to keep the pre-activations of the network during an update within a trust region. We apply this regularization to the pre-activations and not the activations, as dead neurons cannot propagate gradients, and even when they do, depending on the activation function, do so with a low magnitude. The regularization is an additional loss/penalty added to the overall loss. We term this loss the Proximal Feature Optimization (PFO) loss. With \(_{}(s)\) as the pre-activation of the penultimate layer of the actor \(_{}\) given a state \(s\),

\[L_{_{}}^{PFO}()=_{_{}}\!\![ _{t=0}^{t_{}-1}_{}(S_{t})-_{_{}}(S_{t})_{2}^{2}].\] (2)

We apply two versions of PFO: one on only the penultimate layer's pre-activations and one on all the pre-activations until the penultimate layer. In the scope of this work, we do not tune the coefficient of PFO; we pick the closest power of 10 that sets the magnitude of this loss to a similar magnitude of the clipped PPO objective tracked on the experiments without intervention. This gives a coefficient of 1 for ALE, 1 for MuJoCo with tanh, and 10 with ReLU. The goal is not necessarily to obtain better performance but to see if PFO improves the representations learned by PPO and if, in turn, it affects its trust region and performance. As shown in Figure 6, the regularization of PFO effectively brings the norm of the preactivation down, the number of dead neurons down, the capacity loss down, and the rank up. This coincides with a significant decrease in the excess probability ratio, especially in the upper tail. More importantly, we also see a significant increase in the lower tail of the returns where no collapse in performance is observed anymore on ALE/NameThisGame and ALE/Phoenix, with a slight increase in the upper tail showing that PFO can increase performance. Among the interventions we have tried, PFO provided the most consistent improvements in representation and trust region.

Sharing the actor-critic trunkIn deep RL, the decision to use the same feature network trunk for both the actor and the critic is not trivial. Depending on the complexity of the environment, it can significantly change the performance of a PPO agent (Andrychowicz et al., 2021; Huang et al., 2022). We, therefore, attempt to draw a connection between sharing the feature trunk, the resulting representation, and its effects on the PPO objective. In this intervention, we make the actor and the critic share all the layers except their respective output layers and backpropagate the gradients from both the value and policy losses to the shared trunk. Figure 6 shows that the value loss acts as a regularizer, which decreases the feature rank and, depending on the reward's sparsity, gives two distinct effects. In dense-reward environments such as ALE/Phoenix and ALE/NameThisGame, the ranks are concentrated at low but non-trivial values: the upper tail significantly decreases compared to the baselines while the lower tail increases. This coincides with a lower feature norm, lower excess probability ratio, and, in turn, a high tail for the returns. It also increases performance in some cases. However, the opposite is true in the sparse-reward environment Gravitar: the rank completely collapses, and the feature norms and excess ratios are very high, collapsing the model's performance. This is consistent with the observations made in the plasticity works studying value-based methods: they show that sparse rewards deteriorate the rank of the value network, and we show that when shared in an actor-critic architecture they, in turn, deteriorate the policy. It is important to note that this distinction using the reward sparsity holds when comparing environments from the same family (e.g., ALE), but may not hold otherwise (e.g., comparing an ALE and a MuJoCo environment). We provide training curves showing the difference in the evolution of the feature rank when sharing the actor-critic trunk in Appendix D. To further strengthen this observation we run an intervention on ALE/Phoenix (a dense reward environment), with a reward mask randomly masking a reward with 90% chance, comparing the effects of sharing the actor-critic trunk. As expected, while with dense rewards, sharing the trunk is beneficial in ALE/Phoenix (Appendix Figure 21), with the sparse reward, the opposite is true: sharing the trunk is detrimental (Appendix Figure 35).

Adapting AdamAsadi et al. (2023) argue that as the targets of the value function change with the changing policy rollouts, the old moments accumulated by Adam become harmful to fit the new targets and find that resetting the moments of Adam helps performance in DQN-like algorithms. As the PPO objective creates a dependency on the previous policy, and more generally, in the policy gradient, the advantages change with the policy, the same argument about Adam moments can be made for PPO. Furthermore, Dohare et al. (2023); Lyle et al. (2023) advocate for decaying the second moment of Adam faster than its default decay of 0.999 when training under non-stationarity

Figure 6: **Effects of regularizing features and non-stationarity _Top & Middle: ALE/Phoenix-v5 & ALE/NameThisGame-v5_. Regularizing the difference between the features of consecutive policies with PFO results in better representations, a lower trust-region excess, and mitigates performance collapse. The same applies to sharing the actor-critic trunk. _Bottom: ALE/Gravitar_. Sharing the feature trunk between the actor and the critic results in a worse policy representation as the value network is subject to rank collapse due to reward sparsity. A boxplot includes 15 runs with different epochs.**

and set it to match the decay of the first moment. Therefore, we experiment with both resetting Adam's moments after each batch collection (to avoid tuning its frequency) and setting the second moment to decay at the (smaller) default decay of the first moment for both the actor and the critic; the moments are thus only accumulated over the epochs on the same batch in the former and over shorter batch sequences in the latter. We observe in Figure 6 and Appendix D that these interventions reduce the feature norm and increase the feature rank on ALE, which also reduces the excess probability ratio and, in some cases, improves performance; however, they are not sufficient to prevent collapse and, like sharing the actor-critic trunk, result in poor performance on ALE/Gravitar.

## 5 Related Work

Our work is complementary to various other works studying the plasticity and representation dynamics of neural networks trained under non-stationarity. Kumar et al. (2023) provide a comprehensive comparison and categorization of methods used to mitigate plasticity loss in continual supervised learning tasks and their effects on representations. Our work provides insights into the transferability of some of these solutions to RL and tools to evaluate their impact on trust region methods. Sokar et al. (2023) provide an alternative characterization of plasticity loss in RL using dormant neurons and observe an increase in dormant neurons for non-stationary objectives. Abbas et al. (2023) study representation metrics such as feature norms and observe a decrease of the norm due to dying neurons. Like in the work of Lyle et al. (2022), both studies only include value-based methods. In this work, we study dead units and capacity loss as Lyle et al. (2022) and provide corroboration of the dying units phenomenon in policy optimization methods and, taking the dying neurons out of the equation, find that the norm of preactivations actually blows up.

Other feature regularizations similar to PFO have been studied in value-based offline RL. Kumar et al. (2022) propose DR3, which counteracts an implicit regularization in TD learning by minimizing the dot product between the features of the estimated and target states. Ma et al. (2023) propose Representation Distinction (RD) which tries to avoid unwanted generalization by minimizing the dot product between the features of state-action pairs sampled from the learned policy and those sampled from the dataset or an OOD policy. Both are related to PFO as the methods directly tackle an undesired feature learning dynamic, but there is no motivation for DR3 or RD in online RL, and PFO is conceptually different. The implicit regularization that DR3 counteracts is not present in on-policy RL as shown by Kumar et al. (2022) in the SARSA experiment, and PFO differs from DR3 as it extends a trust region rather than counteracts an implicit bias. Similarly, the overestimation studied by Ma et al. (2023) in the vicious backup-generalization cycle is broken by on-policy data, and RD regularizes state features between the learned policy and the dataset policy, not consecutive policies.

## 6 Conclusion and Discussion

**Conclusion** In this work, we provide evidence that the representation deterioration under non-stationarity observed by previous work in value-based methods generalizes to PPO agents in ALE and MuJoCo with their common model architectures and is connected to performance collapse. This brings a novel perspective to previous works that showed that PPO agents lose plasticity throughout training. We show that this is particularly concerning for the heuristic trust region set by PPO-Clip, which fails to prevent collapse as it becomes less effective when the agent's representation becomes poor. Finally, we present Proximal Feature Optimization (PPO), a simple novel auxiliary loss based on regularizing the evolution of features that mitigates representation degradation and, along with other interventions, shows that controlling representation mitigates performance collapse.

**Limitations and open questions** In this work, we study the common architecture and optimizer of PPO agents in ALE and MuJoCo consisting of relatively small models without normalization layers, weight decay, or memory (e.g., not using Transformers and RNNs). Despite our best attempts, as with any other empirical machine learning work, the generalization of our results to other settings is not fully known. Still, this work should raise awareness about the representation collapse phenomenon observed in PPO and encourage future work to monitor representations when training PPO agents, as it can help diagnose performance collapse. We have focused on simple interventions that regularize non-stationarity and representations to highlight the effects of non-stationarity and the connection between representation, trust region, and collapse, but exploring interventions on plasticity is also valuable, as these may also influence the same dynamics. We believe further studies to analyze this problem, both empirically and particularly theoretically, to understand the reasons driving representation deterioration to be valuable. We hope that our study encourages work in this direction.