# Robust Fine-tuning of Zero-shot Models via Variance Reduction

Beier Zhu  Jiequan Cui  Hanwang Zhang

Nanyang Technological University

beier002@e.ntu.edu.sg, hanwangzhang@ntu.edu.sg

###### Abstract

When fine-tuning zero-shot models like CLIP, our desideratum is for the fine-tuned model to excel in both in-distribution (ID) and out-of-distribution (OOD). Recently, ensemble-based models (ESM) have been shown to offer significant robustness improvement, while preserving high ID accuracy. However, our study finds that ESMs do not solve the ID-OOD trade-offs: they achieve peak performance for ID and OOD accuracy at different mixing coefficients. When optimized for OOD accuracy, the ensemble model exhibits a noticeable decline in ID accuracy, and vice versa. In contrast, we propose a sample-wise ensembling technique that can simultaneously attain the best ID and OOD accuracy without the trade-offs. Specifically, we construct a Zero-Shot Failure (ZSF) set containing training samples incorrectly predicted by the zero-shot model. For each test sample, we calculate its distance to the ZSF set and assign a higher weight to the fine-tuned model in the ensemble if the distance is small. We term our method Variance Reduction Fine-tuning (VRF), as it effectively reduces the variance in ensemble predictions, thereby decreasing residual error. On ImageNet and five derived distribution shifts, our VRF further improves the OOD accuracy by 1.5 - 2.0 pp over the ensemble baselines while maintaining or increasing ID accuracy. VRF achieves similar large robustness gains (0.9 - 3.1 pp) on other distribution shifts benchmarks. Codes are available in https://github.com/BeierZhu/VRF.

## 1 Introduction

To ensure the reliability of machine learning systems, it is essential to develop models that can generalize to unseen, out-of-distribution environments. Large pre-trained models such as CLIP  and ALIGN  have recently shown remarkable robustness against challenging distribution shifts. However, it is widely acknowledged that these improvements in robustness are most pronounced in the zero-shot setting, while conventional fine-tuning on these models often compromises robustness when compared to zero-shot performance . This phenomenon is known as the ID-OOD trade-offs, _i.e._, improving performance on in-distribution (ID) data can sometimes lead to decreased performance on out-of-distribution (OOD) data .

In recent years, ensemble-based models (ESMs) have demonstrated significant success in addressing the ID-OOD dilemma . Specifically, denote the input as \(\), the zero-shot model as \(}(y|;_{})\) and the fine-tuned model as \(}(y|;_{t})\), existing ESMs typically employ the output-space ensemble (OSE) , which outputs \(}(y|;_{})=}(y |;_{t})+(1-)}(y|; _{s})\), and the weight-space ensemble (WSE) , which outputs \(}(y|;_{})=}(y| ;a_{t}+(1-)_{s}),\). Compared to fine-tuned models, ESMs offer significant accuracy enhancements under distribution shift, while maintaining high ID accuracy.

However, ESM cannot fully address the ID-OOD trade-offs. In Figure 1 (a), by varying the mixing coefficient \(\), we plot the ID-OOD frontier curves (pink line) for the CLIP ViT-B/16 model onImageNet  (ID) and five derived distribution-shifted datasets (OOD): ImageNet-V2 , ImageNet-R , ImageNet-A , ImageNet-Sketch  and ObjectNet . We find that the ensemble model achieves its optimal ID and OOD performance at different \(\) values: the best ID accuracy is achieved at \(=0.5\) and the best OOD accuracy is obtained at \(=0.3\). When the ensemble model reaches its optimal value for OOD, the performance on ID decreases by \(3.6\%\) relative to its peak. Similarly, when the ensemble model is optimized for ID, the performance on OOD decreases by \(1.6\%\) relative to its best value - the ID-OOD trade-offs still persist for ESMs. This raises a natural question:

#### 2.1.1 Can ensemble-based models simultaneously attain the best ID and OOD accuracy?

In this paper, we affirmatively answer this question by proposing a sample-wise ensembling technique, dubbed variance reduction fine-tuning (VRF). This method is motivated by an empirical finding illustrated in Fig 1 (b). For each sample in the training dataset, if the fine-tuned model correctly predicts the label while the zero-shot model fails, we collect its features representation in the fine-tuned model as the zero-shot failure (ZSF) set. We then measure the distance \(d()\) of each test sample \(\) to the ZSF set. Based on this distance, test samples are grouped into bins, and we compute the ratio of fine-tuned accuracy to zero-shot accuracy: \(_{_{_{_{_{_{ _{_{_{_{_{}}}}}}}}}}}}{ _{_{_{_{_{ _{_{}}}}}}}}{_{_{ _{_{}}}}}}\) monotonically decreases as \(d()\) increases. Intuitively, the closer a sample is to the ZSF set, the more likely it is that the zero-shot model makes incorrect predictions, whereas the fine-tuned model is more likely to be accurate, leading to a higher \(_{_{_{}}}}{_{_{_{}}}}\) ratio. Therefore, we use the distance to assign weights to the models: a smaller \(d()\) results in a higher weight for the fine-tuned model, and vice versa.

As depicted by the orange diamond in Fig. 1 (a), by leveraging the sample-wise weights, our VRF simultaneously attains the best ID and OOD accuracy. In Section 5, we show that on a variety of different models and tasks, our VRF approach consistently outperforms the existing fine-tuning and ensembling methods, including linear probing, end-to-end fine-tuning, LP-FT , OSE and WSE . In specific, on ImageNet and five derived distribution shifts, our VRF further improves the OOD accuracy by 1.5 - 2.0 pp over the ensemble baselines while maintaining or increasing ID accuracy. Furthermore, in Section 4, we justify our approach by demonstrating that it effectively minimizes the variance of the ensemble models, resulting in reduced residual error.

Figure 1: (a) ID-OOD frontier curves for the CLIP ViT-B/16 model on the ID (ImageNet) and OOD (IN-{V2, R, A, Sketch} and ObjectNet) datasets by varying the mixing coefficient \(\). The ensemble model achieves its best ID and OOD performance at different \(\) values. Our method VRF simultaneously attains the best ID and OOD accuracy, outperforming the ensemble by \(3.6\%\) on OOD and \(1.6\%\) on ID at its optimal performance points.(b) Relationship between the ratio of fine-tuned accuracy to zero-shot accuracy (\(_{_{}}}{_{_{_{_{_{}}}}}}\)) and the distance to the zero-shot failure set (\(d()\)). \(_{_{_{_{_{ _{_{}_{_{}_{_{ }_{_{}_{_{}_{}_{ _{}}}}}}}}}}}}{_{_{_{ _{_{}}}}}}}\) demonstrates a monotonic decrease as \(d()\) increases.

Related Work

**Mitigating ID-OOD trade-offs.** Improving performance on in-distribution data can sometimes lead to a decrease in performance on out-of-distribution data, and vice versa. This phenomenon is known as the ID-OOD trade-offs. Xie et al.  leverage auxiliary information as outputs of auxiliary tasks to pre-train a model to reduce OOD error. Khani and Liang  show that self-training on large amounts of unlabeled data can mitigate such trade-offs by removing spurious features. Tripuraneni et al.  tackle this problem by learning representations that are robust across diverse tasks. However, these methods usually necessitate additional unlabeled data or auxiliary information. In contrast, our VRF is a straightforward variation of fine-tuning that does not require any extra data.

**Robust fine-tuning of zero-shot models.** Vision-language models like CLIP  have demonstrated outstanding improvements in robustness. It is commonly acknowledged that conventional fine-tuning methods often compromise robustness when compared to zero-shot performance. Therefore, enhancing downstream robustness has been the focus of subsequent works [15; 28; 5; 19; 6; 30]. Kumar et al.  show that a two-process of linear probing followed by full fine-tuning can alleviate feature distortion, leading to stronger OOD performance without sacrificing ID accuracy. Wortsman et al.  propose a method of weight interpolation between the zero-shot and the fine-tuned models to improve both ID and OOD accuracy. Goyal et al.  demonstrate that mimicking the contrastive pre-training objectives to fine-tune the zero-shot models outperforms tuning via the traditional supervised cross-entropy loss. However, the ID-OOD trade-offs are still observed with these methods. In contrast, our method VRF can simultaneously achieve the best ID and OOD accuracy.

## 3 Methods

### Set Up

**Task:** Consider a classification setting where the goal is to map an instance \(\) to a label \(y=[K]\). We are provided with a zero-shot model \(f(;_{})\), a downstream dataset \(=\{_{i},y_{i}\}_{i=1}^{N}\), and a fine-tuned model \(f(;_{})\) which is trained on \(\). Below, we outline the implementation of the zero-shot and fine-tuned models:

* **Zero-shot models** (ZS): We investigate CLIP models  as our zero-shot models. CLIP models are pre-trained using image-text pairs \(\{(_{1},_{1}),...,(_{B},_{B})\}\) from the Internet. The objective of the CLIP models is to train a visual encoder \(_{}\) and a text encoder \(_{}\) such that the cosine similarity \(<_{}(_{i}),_{}(_{i})>\) is maximized relative to unmatched pairs. CLIP models perform zero-shot inference for \(K\) classes by matching \(\) with potential class names \(\{c_{1},...,c_{K}\}\). Concretely, by extending the class name \(\{c_{k}\}\) to a prompt "\(_{k}=\)a photo of a \(\{c_{k}\}\)", the zero-shot model outputs the score (logit) for class \(k\) as \(f(;_{})_{k}=<_{}(),_{ {t}}(_{k})>\). The predicted probabilities can be calculated using the softmax function, _i.e._, \(}(y|;_{})=(f(; _{}))_{y}\). The model outputs the label as \((f(;_{}))=_{i}\,f(;_{})_{i}\)
* **Linear classifiers** (LC): We learn a linear classifier on top of the visual embedding \(_{}()\) while freezing the visual encoder \(_{}\). The parameters of the linear classifier are optimized to minimize the cross-entropy loss on \(\).
* **End-to-end fine-tuning** (E2E-FT): We update both the linear classifier and the visual encoder by minimizing the cross-entropy loss on \(\).
* **Linear probing then full fine-tuning** (LP-FT): We employ a two-phase fine-tuning approach: initially training a linear classifier, followed by full fine-tuning starting from the solution derived from training the linear classifier.
* **Output-space ensemble** (OSE): We perform linear interpolation of the outputs between a zero-shot model and a fine-tuned model (_e.g._, E2E-FT or LP-FT): \[}(y|;_{})=}(y| ;_{})+(1-)}(y|; _{}),\] (1)
* **Weight-space ensemble** (WSE). We combine the weights through linear interpolation between a zero-shot model and a fine-tuned model: \[}(y|;_{})=}(y|;_{}+(1-)_{}),\] (2)

### Variance Reduction Fine-tuning

We now present our proposed method, VRF, which consists of three steps. First, before the inference stage, we collect the Zero-Shot Failure (ZSF) set. Second, for a given test sample, we calculate its distance to the ZSF set. Third, we assign weights to combine predictions from the zero-shot and fine-tuned models based on this distance.

**Step 1 (Identification).** For each \(_{i}\) in the training dataset \(\), if the fine-tuned model correctly predicts the label while the zero-shot model fails, we collect its feature representation \(_{i}=_{}(_{i};_{})\) from the fine-tuned model to form the zero-shot failure set \(\). Specifically, \(\) is defined as:

\[=\{_{i}y_{i}=(f_{}( _{i}))y_{i}(f_{}(_{i}))\}.\] (3)

Here, \(f_{}()\) and \(f_{}()\) are used to denote \(f(;_{})\) and \(f(;_{})\), respectively, for simplicity.

**Step 2 (Distance Calculation).** The key empirical observation underpinning VRF is that in the vicinity of the ZSF set, a test sample typically exhibits lower zero-shot accuracy (\(_{}\)) and higher fine-tuned accuracy (\(_{}\)). Consequently, the \(_{}}{_{}}\) ratio demonstrates a monotonic decrease as the distance from the sample to the ZSF set increases. In this paper, we adopt non-parametric density estimation using nearest neighbors  to measure the distance of a test sample to the dataset \(\). Specifically, during inference, we derive the feature representation \(\) of a test sample \(\), and compute the \(_{2}\) distances \(\|-_{i}\|_{2}\) w.r.t. \(_{i}\). We reorder \(\) according to the increasing \(_{2}\) distance and denote the ordered set in sequence as \(^{}=(_{(1)},_{(2)},...,_{(| |)})\). The distance of \(\) to \(\) is defined as the \(_{2}\) distance to the \(k\)-th nearest neighbor (\(k\)-NN), _i.e._,

\[d(;,k)=\|-_{(k)}\|_{2}.\] (4)

If there is no ambiguity, we use \(d()\) to denote \(d(;,k)\) for readability. Since the features in CLIP models are \(_{2}\) normalized, \(d()\) are bounded between \(\).

**Step 3 (Sample-Wise Ensembling).** We implement sample-wise out-space ensembling in the form:

\[}_{}(y|)=() }_{}(y|)+(1-()) }_{}(y|),\] (5)

where \(()(0,1)\). We use the distance to ZSF set \(d()\) to determine the weight \(\). As shown by the blue line in Fig 2, a smaller value of \(d()\) corresponds to a larger \(_{}}{_{}}\) ratio, and vice versa. Therefore, we set the weight \(\) to be inversely proportional to \(d()\). Given that \(\) is bounded between \(0\) and \(1\), we employ a sigmoid function \(()\) as:

\[()=(-(d()-a)/b),\] (6)

where \(a,b>0\) are two hyper-parameters sweeped using accuracy on ID validation set. We visualize the weight curve in green on Fig 2, setting \(a=1.5\) and \(b=0.6\). We summarize the whole process in Algorithm 1.

```
1:Given: Training dataset \(\), a zero-shot model \(f_{}\) and a fine-tuned model \(f_{}\).
2:Build zero-shot failure set \(\) using Eq. (3). \(\) Step 1: Identification
3:Inference Stage:
4:Given a test sample \(\), compute its feature representation \(\), zero-shot prediction \(}_{}(y|)\) and fine-tuned model prediction \(}_{}(y|)\).
5:Compute the \(k\)-NN distance to \(\) as \(d()\) using Eq. (4). \(\) Step 2: Distance Calculation
6:Compute the weight \(()\) using Eq. (6).
7:Return \(}_{}(y|)\) using Eq. (5) \(\) Step 3: Sample-Wise Ensembling ```

**Algorithm 1** Variation Reduction Fine-tuning

## 4 Justification

We now prove that our VRF can effectively reduce the variance of the combined model, resulting in lower errors compared to ensembling using a constant mixing coefficient.

### Background

The outputs of a well trained classifier are expected to approximate the _a posterior_ class distribution. Apart from the irreducible error (Bayes error), the residual error of a classifier can be broken down into bias and variance components. In specific, for a test sample \(\), the probability output of a classifier parameterized by \(\) can be expressed as:

\[}(y|;)=(y|)++_{y}()}_{},\] (7)

where \((y|)\) denotes the true _a posterior_ distribution, \(_{y}\) is the label bias of \(}(y|;)\) which is independent to the input \(\), and \(_{y}()\) is related to the given input \(\). In this study, we primarily attribute the residual error to the variance term (_i.e._, \(_{y}=0\)), as the label bias problem in foundation models has been effectively addressed by Zhu et al. . Tumer et al.  have proven that the expected residual error \(E\) is given by:

\[E=[_{y}()]}{s},\] (8)

where \(s\) is a constant factor related to the derivative of the true _a posterior_ distribution and is independent of the trained model, and \([_{y}()]\) is the variance.

### Variance Reduction Fine-tuning Leads to Lower Residual Error

Let us shift our focus to the effects of combining the zero-shot and fine-tuned models. Let \(g_{}()\) and \(g_{}()\) be two functions that produce weights for ensembling the models. Subject to the constraint that \(g_{}()+g_{}()=1\), the residual error of the combined classifier is obtained by:

\[}_{}(y|)=g_{}()}_{}(y|)+g_{}()}_{}(y|)=(y|)+}( )_{}()+g_{}() _{}()}_{_{}()},\] (9)

where we omit the subscript \(y\) of \(\) for readability. The variance of \(_{}()\) can be expressed as:

\[[_{}()]=g_{}()^{2} [_{}()]+g_{}()^{2} [_{}()].\] (10)

Here, we assume the residual errors are independent following the assumption of the previous studies of CLIP fine-tuning [14; 31]. We further explore the case of correlated residual errors in Section B. According to Eq. (8), the reduction in variance can be readily translated into a reduction in error rates. To obtain the smallest variance \([_{}()]\), we minimize Eq. (10) using Lagrange multiplier to enforce the constraint that \(g_{}()+g_{}()=1\), and obtain the optimal weight function \(g_{}\) as:

\[g_{}()=[_{}()]}{ [_{}()]+[_{}()]}=}}{E_{}+E_{}}=(1+}}{E_{ }})^{-1}_{}}{_{}}\] (11)

Since \(_{}}{_{}} d()^{-1}\) (a smaller distance \(d()\) is associated with a larger \(_{}}{_{}}\) as shown in Fig. 2), we design the weighting function \(g_{}()=() d()^{-1}\) as in Eq. (6).

## 5 Experiments

### Experimental Setup

**Datasets with distribution shifts.** We provide the results for ImageNet  and its five derived distribution shifts: (1) ImageNet-V2 (IN-V2) : Test images sampled after a decade of the original ImageNet. (2) ImageNet-R (IN-R) : Contains renditions (_e.g._, art, cartoons, graffiti). (3) ImageNet Sketch (IN-Sketch) : Consists of sketches rather than natural photos. (4) ImageNet-A (IN-A) : Collects real-world images that are misclassified by ResNet models. (5) ObjectNet , a test set featuring objects with diverse backgrounds, rotations, and imaging viewpoints. We extend our analysis to include a standard distribution shift benchmark [15; 14; 4]: CIFAR-10 \(\) STL-10, where the ID is CIFAR-10 , and the OOD is STL-10 . We removed the "monkey" class from STL-10, as it does not exist in CIFAR-10. In addition, we also consider subpopulation shifts, where the ID data contains a few sub-categories, and the OOD data comprises different sub-categories within thesame parent category. Following [15; 14], we adopt Entity30 dataset , which aims to categorize images into one of 30 entity categories, such as "vehicle" and "insect".

**Baselines.** We adopt two models: CLIP ViT-B/32 and a larger ViT-B/16 from OpenAI . The default model used in ablation studies is the CLIP ViT-B/16. In addition to the zero-shot models, we compare our approach against five standard methods for adapting pre-trained models: (1) linear classifier , (2) E2E-FT, (3) LP-FT , (4) OSE, and (5) WSE . The descriptions of these methods have been included in Section 3.1.

**Implementation details.** When fine-tuning E2E-FT models, we adhere to Wortsman et al. , employing the default PyTorch AdamW optimizer for 10 epochs with weight decay of 0.1 and a cosine-annealing learning rate schedule with 500 warm-up steps. Unless specified, we use a learning rate of \(3 10^{-5}\), gradient clipping at norm 1. When fine-tuning LP-FT, we first adopt the settings of Wortsman et al.  to train the linear classifier, then full fine-tune the models at a learning rate of \(1 10^{-5}\). For efficiently performing \(k\)-NN search, we use Faiss library . Denote the size of the ZSF set is \(||\), we scale the \(k\) according to a percentage \(p\%\) of the sample set, where \(k=(p\%||)\). In this paper, \(p\) is set to \(0.1\%\), a value consistent with the default setting proposed by Sun et al. . Note that all the hyperparameters, _e.g._, \(,a,b\), are searched using the accuracy on the in-distribution (ID) validation set. Derived distribution shift datasets are _only for evaluation and not for hyperparameter sweeps_. See Appendix C.1 for the details of experimental details.

    &  &  & Avg \\  & & IN-V2 & IN-Sketch & IN-A & IN-R & ObjectNet & shifts \\  Zero-shot  & 63.3 & 55.9 & 42.3 & 31.5 & 69.3 & 43.5 & 48.5 \\ Linear classifier  & 75.4 & 63.4 & 38.8 & 26.1 & 58.7 & 41.5 & 45.7 \\  E2E-FT  & 76.2 & 64.2 & 38.7 & 21.0 & 57.1 & 40.1 & 44.2 \\ + Weight-space ensemble  & 77.9 & 67.2 & 45.1 & 28.8 & 66.4 & 45.1 & 50.5 \\ + Output-space ensemble & 77.3 & 66.0 & 44.2 & 27.1 & 68.4 & 44.4 & 50.0 \\ + VRF (ours) & 77.6 & 66.7 & 47.0 & 29.2 & 70.9 & 46.3 & 52.0 \\ \(\) & +0.3 & +0.7 & +2.8 & +2.1 & +2.5 & +1.9 & +2.0 \\   LP-FT  & 76.9 & 64.8 & 39.9 & 25.7 & 69.9 & 42.6 & 48.6 \\ + Weight-space Ensemble  & 78.0 & 67.0 & 44.8 & 31.2 & 65.8 & 46.1 & 51.0 \\ + Output-space Ensemble & 77.8 & 66.3 & 44.0 & 29.5 & 66.2 & 45.5 & 50.3 \\ + VRF (ours) & 77.8 & 66.7 & 46.1 & 31.0 & 70.0 & 46.3 & 51.8 \\ \(\) & +0.0 & +0.4 & +2.1 & +1.5 & +3.8 & +0.8 & +1.5 \\   

Table 1: Accuracy of various methods on ImageNet and derived distribution shifts for CLIP ViT-B/32.

    &  &  & Avg \\  & & IN-V2 & IN-Sketch & IN-A & IN-R & ObjectNet & shifts \\  Zero-shot  & 68.3 & 61.9 & 48.3 & 50.1 & 77.6 & 54.2 & 58.4 \\ Linear classifier  & 79.3 & 69.1 & 44.8 & 44.3 & 66.7 & 51.1 & 55.2 \\  E2E-FT  & 81.3 & 70.6 & 45.1 & 36.6 & 65.6 & 50.5 & 53.7 \\ + Weight-space ensemble  & 82.5 & 73.1 & 51.6 & 47.6 & 75.1 & 55.7 & 60.6 \\ + Output-space ensemble & 82.2 & 72.0 & 50.6 & 46.8 & 76.7 & 54.9 & 60.2 \\ + VRF (ours) & 82.3 & 72.1 & 52.9 & 48.4 & 78.7 & 56.4 & 61.8 \\ \(\) & +0.1 & +0.1 & +2.3 & +1.6 & +2.0 & +1.5 & +1.6 \\  LP-FT  & 81.5 & 70.7 & 46.7 & 41.4 & 66.4 & 52.4 & 55.5 \\ + Weight-space ensemble  & 82.4 & 73.0 & 51.5 & 50.6 & 74.2 & 56.6 & 61.2 \\ + Output-space ensemble & 82.1 & 72.3 & 50.9 & 50.9 & 74.9 & 55.7 & 60.9 \\ + VRF (ours) & 82.1 & 72.3 & 52.9 & 51.2 & 78.8 & 57.2 & 62.4 \\ \(\) & +0.0 & +0.0 & +2.0 & +0.3 & +3.9 & +1.5 & +1.5 \\   

Table 2: Accuracy of various methods on ImageNet and derived distribution shifts for CLIP ViT-B/16.

### Results

**ImageNet and its five shifted distribution results.** In Table 1 and 2, we report the ID-OOD accuracies of fine-tuning baselines for CLIP ViT-32 and CLIP ViT-16 models, respectively. For OSE and WSE, we choose the mixing coefficient \(\) with the highest ID validation accuracy. To enhance clarity in the results, we denote the improvement over OSE as \(\) in Tables 1 and 2. We observe that our VRF boosts the accuracy of fine-tuned models, including ensembling baseline models, across five ImageNet distribution shifted datasets, while maintaining or improving the ImageNet in-distribution performance. For instance, in Table 1, when ensembling with the E2E-FT model, our VRF outperforms the OSE model by \(2.0\%\) on distribution shifts while increasing the ID accuracy by \(0.3\%\). Compared to WSE models, our VRF achieves a delta of \(1.2\%\) on distribution shifts, while maintaining ID performance within \(0.2\%\), as shown in E2E-FT part of Table 2.

**CIFAR-10 \(\) STL-10 and Entity-30 results.** We report the accuracy of various methods in Table 3 (a,b). We note that fine-tuning baselines can enhance the accuracy on CIFAR-10 compared to the zero-shot models. However, this improvement comes at the expense of reduced accuracy on STL-10. For instance, E2E-FT leads to a decrease of approximately \(3.6\%\) in STL-10 accuracy, as shown in Table 3(a). Previous ensemble methods can mitigate the degradation to some extent, but the STL-10 performance still lags behind the zero-shot performance, _e.g._, In Table 3(b), the accuracy of E2E-FT + WSE is \(97.8\%\) whereas the zero-shot performance is \(98.4\%\). In contrast, our VRF simultaneously improves accuracy on both CIFAR-10 and STL-10. Similarly, for Entity-30, our VRF can further improvement the OOD performance when compared to WSE and OSE methods.

In addition, we plot the ID-OOD frontier curves in Figure 3 (a.1&b.1), respectively. Similar to the results on ImageNet (Figure 1(a)), the ensemble model achieves its best ID and OOD performances at different \(\) values. For instance, on the CIFAR-10 benchmark, when the ensemble model attains its optimal ID value at \(=0.7\), the OOD performance decreases by \(2.0\%\) relative to its peak.

    &  &  \\  & ID & OOD & ID & OOD \\  Zero-shot  & 88.3 & 97.1 & 65.2 & 66.5 \\ Linear classifier & 95.0 & 96.6 & 93.3 & 68.1 \\  E2E-FT  & 97.9 & 93.5 & 94.4 & 65.1 \\  + WSE  & 98.2 & 95.7 & 94.6 & 68.8 \\  + OSE & 97.9 & 95.9 & 94.4 & 66.4 \\  + VRF (ours) & 97.8 & 97.3 & 94.5 & 69.5 \\ \(\) & -0.1 & +1.4 & +0.1 & +3.1 \\  LP-FT  & 97.9 & 95.0 & 94.6 & 67.7 \\  + WSE  & 98.1 & 96.4 & 94.8 & 68.8 \\  + OSE & 98.1 & 96.4 & 94.7 & 68.5 \\  + VRF (ours) & 98.1 & 97.5 & 94.8 & 70.1 \\ \(\) & +0.0 & +1.1 & +0.1 & +1.6 \\   

Table 3: Accuracy of various methods on CIFAR-10 \(\) STL-10 and Entity-30.

Figure 3: ID-OOD frontier curves by varying the mixing coefficient \(\) and \(_{}}{_{}}\) curves for the CLIP ViT-B/16. (a) CIFAR-10 (ID) and STL-10 (OOD) results. (b) Entity-30 results.

Conversely, when the optimal OOD value is reached at \(=0.3\), the performance on ID diminishes by \(2.7\%\) from its best. In contrast, our VRF simultaneously attains the ID and OOD performance.

We also analyze the relation between the ratio \(_{}}{_{}}\) and \(d()\) in Figure 3 (a.2&b.2). Consistent with the findings from ImageNet (Figure 1 (b)), we observe that the ratio decreases as \(d()\) increases, which further supports our design of assigning a higher weight to fine-tuned models if \(d()\) is smaller.

### Further Analysis and Ablation Studies

**VRF for linear-probed models.** A drawback of the proposed method is its doubled inference and storage cost compared to WSE and other single-model robust fine-tuning methods. To address concerns regarding space-time complexity, we apply our VRF method to linear-probed models and present the results in Table 4. We also compare with output-space ensembling, since the model is linear, it is equivalent to weight-space ensembling. We also compare it with output-space ensembling, which, given the linear nature of the model, is equivalent to weight-space ensembling. Consistent with the conclusions drawn from fully fine-tuned models, our VRF method further improves OOD performance while maintaining comparable ID performance to OSE/WSE ensembling.

**Using ZSF set \(\) or entire training set \(\)?** In Step 1 of our VRF, we define the zero-shot failure set \(\) and use it to compute distances. We aim to find out whether using the entire training set \(\) offers comparable performance. In Figure 4, we plot the \(_{}}{_{}}\) curves and report both ID and OOD accuracy using the two sets. We observe that the ratio curve using \(\) does not exhibit a monotonic trend with \(d()\): it initially increases and then decreases as \(d()\) increases. Furthermore, the ratio \(_{}}{_{}}\) using \(\) is less informative when \(d()\) is smaller than 1.2, as the curve relatively remains flat. As the zero-shot models can accurately predict a large proportion of the ID data (recall that the zero-shot accuracy is \(68.3\%\)), a smaller distance to entire training set \(\) does not reliably indicate whether the fine-tuned model can make more accurate predictions. In comparison, our ZSF set contains only the samples where zero-shot models fail but fine-tuned models succeed. When a sample is close to \(\), it is more likely that the accuracy ratio will be high. Consequently, the performances using \(\) are clearly outperformed by those using \(\).

**Comparison with selective prediction using OOD detector.** A simple approach to address the ID-OOD trade-offs is to use an OOD detector for selective prediction. The OOD detector is a binary classifier \(G_{}()\) to decide whether a sample is ID or OOD based on a threshold \(\). For a test sample, predictions are made with the fine-tuned model if classified as ID, and with the zero-shot model otherwise:

\[f_{}() =f_{}()&G_{}()=\\ f_{}()&G_{}()=,\] (12) \[G_{}() =&S()\\ &S()<,,\] (13)

    &  &  &  \\  & ID & OOD & ID & OOD & ID & OOD \\  Zero-shot classifier  & 68.3 & 58.4 & 90.1 & 98.4 & 68.3 & 68.2 \\ Linear classifier & 79.3 & 55.2 & 95.8 & 97.7 & 95.3 & 69.6 \\  WSE/OSE & 79.9 & 57.8 & 95.8 & 97.7 & 95.5 & 70.5 \\ VRF (ours) & 79.8 & 58.5 & 95.8 & 98.4 & 95.4 & 71.4 \\   

Table 4: Results of VRF for linear-probed models using CLIP ViT-B/16 models.

Figure 4: ZSF set \(\) vs. all data \(\)

where instances with higher scores \(S()\) are classified as ID and vice versa. \(\) is typically chosen to achieve achieve a \(95\%\) true positive rate for ID samples. We report the results of several implementations of \(S()\) in Table 5 (Details are in Section C.6). We note that selective prediction achieves comparable ID performance to E2E-FT models and similar OOD performance to zero-shot models. However, its accuracy still falls significantly short of our VRF. This is because traditional OOD detectors are designed for scenarios where the OOD data have a completely disjoint label space from the ID data, _i.e._, \(_{}_{}=\). However, in our setup, the zero-shot models show predictive power on ID data, and the fine-tuned models are effective on OOD data. Making binary selections may overlook the complementary knowledge from the other model. Instead, our weight function \(()\) intelligently selects the contribution of each model based on the distance to the ZSF set. Another reason why our method outperforms selective prediction is the effective use of the ZSF set, as illustrated in Figure 4. Directly using all ID data as traditional OOD detectors (e.g., kNN and MD) leads to a weak correlation between the accuracy ratio and the distance \(d()\) (or score \(S()\))

**Examination of the averaged weight for ID and OOD test sets.** Figure 5(a) shows the average weight (\(_{}[()]\)) of the E2E-FT model in ensembling for both ID and OOD test sets. As expected, higher average weights are observed in the ID test set, as the fine-tuned models excel in such domain.

**Logits-based ensembling.** In this paper, we implement OSE by linearly interpolating the probabilities of the two models. Another common strategy for ensembling, known as Logits-Space Ensembling (LSE), involves interpolating in the logits space: \(f(;_{})= f(;_{})+(1- )f(;_{})\). We aim to investigate whether our VRF can enhance the robustness of LSE without compromising the ID accuracy. The results depicted in Figure 5(b) confirm that our VRF can indeed generalize to LSE.

**Effect of \(k\) in \(k\)-NN distance.** In Figure 5(c), to compute \(k=(p||)\), we vary \(p\) across the range \(\{0.0001\%,0.01\%,0.05\%,0.1\%,10\%,50\%\}\). We note two observations: (1) Varying \(k\) slightly affect the ID performance: the fluctuations are less than \(0.1\%\). (2) The OOD accuracy declines as \(p\) increases, but the degradation is very slight for relative small values of \(p\) (_e.g._, when \(p<0.01\%\), the decline is smaller than \(0.2\%\)). In our implementation, we use the \(k\)-th nearest sample instead of the nearest one to reduce the potential impact of label noise. If the nearest sample is mislabeled, the distance may be unreliable. The \(k\)-th sample, being in a higher-density region, offers more stable distance estimates with lower variance, as it lies between the \((k-1)\)-th and \((k+1)\)-th samples. This makes the measure more robust to outliers. Additionally, prior research  shows that using the \(k\)-th nearest distance improves density estimation, which we adopt here.

Figure 5: (a) Averaged weight \(_{}[()]\) on different datasets. (b) VRF based on logit-space ensembling. (c) Comparison with the effect of different \(k\) in the \(k\)-NN distance.

Figure 6: (a) Effect of \(a\) and \(b\) on ImageNet ID accuracy. (b) Effect of \(a\) and \(b\) on ImageNet OOD accuracy. (c) Other designs of \(()\), hyper-parameters are searched on validation set.

**Inference speed of computing \(k\)-nearest neighbor distance.** Thanks to the Faiss library , the \(k\)-NN search can be efficiently implemented. When evaluated on ImageNet benchmarks using CLIP ViT/B-16 features, the inference speed is approximately \(1.8\) milliseconds per-image, which does not significantly improve the inference time. In Figure 8, we further present the per-image inference speed of the \(k\)-nearest neighbor distance computation for various \(k\) values. The inference speed is less than \(2\) ms when \(k<512\).

**Effect of \(a\) and \(b\) in \(\).** We demonstrate the effect of \(a\) and \(b\) in Figure 6 (a&b). We highlight three trends: (1) ID performance peaks at \(b 0.6\) across different values of \(a\). (2) OOD performance often improves as \(b\) increases across different values of \(a\). (3) When \(b\) is sufficiently large, _e.g._, \(b>10\) for ID and \(b>2\) for OOD, \(a\) has marginal effect on the performance of ID and OOD. In Appendix C.2, we further plot the trade-offs when tuning \(a\) and \(b\).

**Other designs of \(()\).** We further explore alternative designs of \(()\) beyond the sigmoid format in Eq. (6):

* Binary weight: \(_{}()=[d()<a]\), where \(a\) and \([]\) is the indicator function.
* Linear weight: \(_{}()=_{}(-b(d()-a))\), where \(a\), \(b>0\) and \(_{}()\) rectifies the weight within \(\).

We report the results on ImageNet in Table 6 and plot the weight curves with the value of hyperparameters in Figure 6(c). We find that the Linear and the Sigmoid weights show comparable performance and assign similar values of \(\) around \(d=1.5\).

**Visualization of samples \(\) according to \(d()\).** In Figure 7, we randomly sample testing images with the top-100 smallest \(d()\) values in the range \([0.40,0.62]\) and the top-100 largest \(d()\) values in the range \([1.59,1.63]\). Interesting, we observe that: (1) Samples with the smallest \(d()\) predominantly consist of fine-grained species, _e.g._, "Triturus vulgaris", "eff" and "lycaenid", where the fine-tuned models possess domain-specific knowledge, which is often lacking in the zero-shot models. (2) Images with the largest \(d()\) exhibit styles different from those of the fine-tuning samples, including tattoos, cartoons, and sketches, contrasting with the photos typically seen in fine-tuning. Zero-shot models are more skilled in non-real photo styles compared to fine-tuned models.

## 6 Impact, limitations and conclusion

**Impact.** Zero-shot models inherit the weaknesses from pre-training data to downstream tasks, such as noisy and malicious samples. Our VRF might propagate the negative impact.

**Limitations.** Our approach is built on the premise that zero-shot models posses predictive capabilities for downstream tasks. However, if the pre-training knowledge significantly differs from the downstream tasks, our algorithm might fail, which is also an open problem in transfer learning. In addition, the proposed method doubles inference cost compared to WSE and other fine-tuning methods, as it runs both the zero-shot and fine-tuned models. However, this overhead can be mitigated by parallel execution.

**Conclusion.** Inspired by the ID-OOD trade-offs in ensemble-based fine-tuning, we propose VRF to simultaneously optimize the best ID and OOD accuracy. By leveraging the distance to the ZSF set, we assign sample-wise weights to the two models. Despite its simplicity, our VRF demonstrates strong empirical performance, offering a promising technique for solving ID-OOD trade-offs.

   Design of \(\) & ID & OOD \\  Binary & 81.3 & 58.4 \\ Linear & 82.3 & 61.7 \\ Sigmoid & 82.3 & 61.8 \\   

Table 6: Accuracy of designs of \(\) on ImageNet.

Figure 8: Inference speed (per-image) using different \(k\).

Figure 7: Visualization the samples with the smallest/largest \(d()\).