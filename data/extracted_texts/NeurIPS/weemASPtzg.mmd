# Linear Causal Representation Learning from

Unknown Multi-node Interventions

Burak Varrici

Carnegie Mellon University

&Emre Acarturk

Rensselaer Polytechnic Institute

&Karthikeyan Shanmugam

Google DeepMind

&Ali Tajer

Rensselaer Polytechnic Institute

Work was done while BV was a Ph.D. student at Rensselaer Polytechnic Institute.

###### Abstract

Despite the multifaceted recent advances in interventional causal representation learning (CRL), they primarily focus on the stylized assumption of single-node interventions. This assumption is not valid in a wide range of applications, and generally, the subset of nodes intervened in an interventional environment is _fully unknown_. This paper focuses on interventional CRL under unknown multi-node (UMN) interventional environments and establishes the first identifiability results for _general_ latent causal models (parametric or nonparametric) under stochastic interventions (soft or hard) and linear transformation from the latent to observed space. Specifically, it is established that given sufficiently diverse interventional environments, (i) identifiability _up to ancestors_ is possible using only _soft_ interventions, and (ii) _perfect_ identifiability is possible using _hard_ interventions. Remarkably, these guarantees match the best-known results for more restrictive single-node interventions. Furthermore, CRL algorithms are also provided that achieve the identifiability guarantees. A central step in designing these algorithms is establishing the relationships between UMN interventional CRL and score functions associated with the statistical models of different interventional environments. Establishing these relationships also serves as constructive proof of the identifiability guarantees.

## 1 Introduction

Causal representation learning (CRL) is a major leap in causal inference, moving away from the conventional objective of discovering causal relationships among a set of variables and learning the variables themselves. By combining the strengths of causal inference and machine learning, CRL specifies data representations that facilitate reasoning and planning . CRL is motivated by the premise that in a wide range of applications, a lower-dimensional latent set of variables with causal interactions generates the usually high-dimensional observed data. Therefore, CRL's objective is to use the observed data and learn the latent causal generative factors, which include the causal latent variables and their causal relationships.

**CRL objectives.** Formally, consider a set of latent causal random variables \(Z^{n}\) and a directed acyclic graph (DAG) \(\) that encodes the causal relationships among \(Z\). The latent variables are transformed by an _unknown_ function \(g\) to generate the _observed_ random variables \(X^{d}\), where \(X g(Z)\). CRL aims to use \(X\) to recover the latent causal variables \(Z\) and the causal structure \(\).

Two central questions of CRL pertain to _identifiability_, which refers to determining the conditions under which \(Z\) and \(\) can be recovered, and _achievability_, which refers to designing CRL algorithms that can achieve the foreseen identifiability guarantees. Identifiability has been demonstrated tobe inherently under-constrained , prompting the development of diverse methodologies that incorporate inductive biases to enable identifiability. _Interventional_ CRL is one direction with significant recent advances in which interventions on latent causal variables are used to create statistical diversity in the observed data [1; 3; 4; 5; 6].

**Unknown multi-node interventions.** Despite covering many aspects of interventional CRL, such as parametric versus nonparametric causal models, parametric versus nonparametric transformations, and intervention types, the majority of the existing studies assume that the interventions are single-node, i.e., exactly one latent variable is intervened in each environment [3; 4; 5; 6; 7; 8; 9; 10]. This assumption, however, is restrictive in some of the application domains of CRL such as biology and robotics in which generally the subset of nodes intervened in an intervention environment can be _fully unknown_. For instance, biological perturbations in genomics are imperfect interventions with off-target effects on other genes [11; 12], or interventions on robotics applications are likely to affect multiple causal variables . Hence, realizing the promises of CRL critically hinges on dispensing with the assumption of single-node interventions.

In this paper, we address the open problem of using _unknown multi-node (UMN) stochastic_ interventions to recover the latent causal variables \(Z\) and their causal graph \(\), wherein each environment an unknown subset of nodes are intervened. We consider a general latent causal model (parametric or nonparametric) and focus on the _linear_ transformations as an important class of parametric transformation models. We establish identifiability results and design algorithms to achieve them under both soft and hard interventions. For this purpose, we delineate connections between UMN interventions and the properties of score functions, i.e., the gradients of the logarithm of density functions. This score-based framework is the UMN counterpart of the single-node framework proposed in [5; 7], albeit with significant technical differences. Our contributions are summarized below.

* We show that under sufficiently diverse interventional environments, UMN stochastic hard interventions suffice to guarantee perfect identifiability of the latent causal graph and the latent variables (up to permutations and element-wise scaling).
* transitive closure of the latent DAG is recovered, and latent variables are recovered up to a linear function of their ancestors. Remarkably, these guarantees match the best possible results in the literature of single-node interventions.
* We design score-based CRL algorithms for implementing CRL with UMN interventions with provable guarantees. These guarantees also serve as constructive proof steps of the identifiability results.

**Challenges of UMN interventions.** There are two broad challenges specific to addressing the UMN intervention setting that render it substantially distinct from the single-node (SN) intervention setting. First, in SN interventions, since the learner knows exactly one node is intervened in each environment, it can readily identify the intervention targets up to a permutation. In contrast, in UMN interventions, the learner does not know how many nodes are intervened in each environment. Therefore, the nature of resolving the uncertainty about the intervention targets becomes fundamentally different. An immediate impact of this is that it becomes more challenging to properly capitalize on the statistical diversity embedded in the interventional data. Secondly, in SN interventions, only one causal mechanism changes across the environments. Such sparse variations of the causal mechanisms are a core property leveraged by various existing CRL approaches, e.g., contrastive learning , and score-based framework [6; 7]. On the contrary, UMN interventions allow for many concurrent causal mechanism changes, which renders leveraging sparsity patterns in mechanism variations futile. Finally, since intervention targets are unknown, our central algorithmic idea is to properly aggregate the UMN interventional environments to create new distinct environments under which the inherent statistical diversity is more accessible.

### Related literature

**Single-node interventional CRL.** The majority of the studies on interventional CRL focus on SN interventions [3; 4; 5; 6; 7; 8; 9; 10], which can be categorized based on their assumptions on the latent causal model, transformation, and intervention model. Based on this taxonomy, it has been shown that SN hard interventions suffice for identifiability with general latent causal models and linear transformations (one intervention per node) , with linear Gaussian latent models and general transformations (one intervention per node) , and with general latent models and general transformations (two interventions per node) [6; 9]. For the less restrictive SN soft interventions, identifiability up to ancestors is shown for general latent models and linear transformations  and linear Gaussian latent models and general transformations . Furthermore, under additional assumptions such as sufficiently nonlinear latent models, the latent DAG is shown to be perfectly identifiable [7; 10; 14]. In a related study,  focuses on learning the latent DAG (but without learning latent causal variables) using SN hard interventions without parametric assumptions on the model.

**Multi-node interventional CRL.** The studies on MN intervention settings are sparser than the SN intervention settings. Table 1 summarizes the results closely related to the scope of this paper along with the identifiability results established in this paper. In summary, the existing studies either provide _partial_ identifiability or focus on non-stochastic do interventions. The study in  focuses on linear non-Gaussian latent models and linear transformations and uses soft interventions to establish identifiability up to surrounding variables by using multiple interventional mechanisms for each node. In a different study,  uses strongly separated multi-node do interventions and provides perfect identifiability results for general latent models and linear transformations. We also note the partially related study in  that applies soft interventions on a subset of nodes and aims to disentangle the non-intervened variables from the intervened ones. Distinct from all these studies, we address the open problem of perfect identifiability under UMN stochastic interventions.

**Other approaches to CRL.** We note that there exist other interesting settings that address CRL without interventions. Some examples include using multi-view data [18; 19; 20; 21], leveraging temporal sequences [22; 23], building on nonlinear independent component analysis (ICA) principles to identify polynomial latent causal models , and imposing sparsity constraints to obtain partial disentanglement [25; 26], and grouping of observational variables . We refer to  for a detailed literature review on various CRL problems.

## 2 CRL setting and preliminaries

**Notations.** Vectors are represented by lowercase bold letters, and element \(i\) of vector \(\) is denoted by \(_{i}\). Matrices are represented by uppercase bold letters, and we denote row \(i\) and column \(j\) of matrix \(\) by \(_{i}\) and by \(_{:,j}\), respectively, and \(_{i,j}\) denotes the entry at row \(i\) and column \(j\). We use \((\{_{1},,_{r}\})\) to denote the nullspace of the matrix consisting of the row vectors \(\{_{1},,_{r}\}\). For \(n\), we define \([n]\{1,,n\}\). The row permutation matrix associated with any permutation \(\) of \([n]\) is denoted by \(_{}\). We denote the indicator function by \(\). We use \((f)\) to denote the image of a function \(f\) and \(()\) to denote the dimension of a subspace \(\). Random variables and their realizations are presented by upper and lower case letters, respectively.

### Latent causal model

Consider a latent causal space consisting of \(n\) causal random variables \(Z[Z_{1},,Z_{n}]^{}\). An _unknown_ linear transformation \(^{d n}\) maps \(Z\) to the observed random variables denoted by \(X[X_{1},,X_{d}]^{}\) according to:

\[X= Z\,\] (1)

where \(d n\) and \(\) is full rank. The probability density functions (pdfs) of \(X\) and \(Z\) are denoted by \(p_{X}\) and \(p_{Z}\), respectively. We assume that \(p_{Z}\) has full support on \(^{n}\). Subsequently, \(p_{X}\) is supported on \(()\). The causal relationships among latent variables \(Z\) are represented by a DAG \(\) in which the \(i\)-th node corresponds to \(Z_{i}\). Hence, \(p_{Z}\) factorizes according to:

\[p_{Z}(z)=_{i=1}^{n}p_{i}(z_{i} z_{(i)})\,\] (2)

   Work & Latent model & Int. type & Main assumption on interventions & Identifiability (ID) \\ 
 & Linear & Soft & \(|(i)|\) independent int. mechanisms & ID up to surrounding \\
 & General & do & strongly separated interventions & perfect ID \\
**Theorem 1** & General & Hard & lin. indep. interv. (Assumption 1) & perfect ID \\
**Theorem 2** & General & Soft & lin. indep. interv. (Assumption 1) & ID up to ancestors \\   

Table 1: Comparison of the results to existing work in multi-node interventional CRL. We note that all studies assume linear transformation.

where \((i)\) denotes the set of parents of node \(i\) in \(\). The conditional pdfs \(\{p_{i}(z_{i} z_{(i)}):i[n]\}\) are assumed to be continuously differentiable with respect to all \(z\) variables. We use \((i)\), \((i)\), and \((i)\) to denote the children, ancestors, and descendants of node \(i\), respectively. We say that a permutation \((_{1},,_{n})\) of \([n]\) is a valid causal order if the membership \(_{i}(_{j})\) indicates that \(i<j\). Without loss of generality, we assume that \((1,,n)\) is a valid causal order. We will specialize some of our results for the latent causal models with additive noise 1 specified by

\[Z_{i}=f_{i}(Z_{(i)})+N_{i}\;,\] (3)

where functions \(\{f_{i}:i[n]\}\) capture the causal dependence of node \(i\) on its parents and the terms \(\{N_{i}:i[n]\}\) represent the exogenous noise variables.

### Unknown multi-node intervention models

In addition to the observational environment, we have \(M\) UMN _interventional_ environments denoted by \(\{^{m}:m[M]\}\). We assume that the set of nodes intervened in each environment is _unknown_, and denote the set intervened in environment \(^{m}\) by \(I^{m}[n]\). Accordingly, we define the _intervention signature matrix_\(_{}\{0,1\}^{n M}\) to compactly represent the intervention targets under various environments as

\[[_{}]_{i,m}=\{i I^{m}\}\;, i [n]\;,\;\; m[M]\;.\] (4)

The \(m\)-th column of \(_{}\) lists the indices of the nodes intervened in environment \(^{m}\), which we refer to as the _intervention vector_ of environment \(^{m}\). Ensuring identifiability inevitably imposes restrictions on the structure of \(_{}\). For instance, if the \(i\)-th row of \(_{}\) is a zero vector, it means that node \(i\) is not intervened in any environment, then the perfect identifiability is not possible 2. Therefore, to avoid such impossibility cases, we impose the mild condition that \(_{}\) has sufficiently diverse columns, formalized next.

**Assumption 1**.: _Intervention signature matrix \(_{}\) defined in (4) is full row rank, i.e., it contains \(n\) linearly independent intervention vectors._

In this paper, we consider UMN stochastic interventions and address identifiability results under both hard interventions as well as soft interventions as the most general form of intervention.

**Soft interventions.** A soft intervention on node \(i\) alters the _observational causal mechanism_\(p_{i}(z_{i} z_{(i)})\) to an _interventional causal mechanism_\(q_{i}(z_{i} z_{(i)})\). Such a change occurs in node \(i\) in all the environments \(^{m}\) that contain node \(i\), i.e., \(i I^{m}\). Subsequently, the pdf of the latent variables in environment \(^{m}\), denoted by \(p_{Z}^{m}\), factorizes according to:

\[p_{Z}^{m}(z)_{i I^{m}}q_{i}(z_{i} z_{(i)}) _{i I^{m}}p_{i}(z_{i} z_{(i)})\;, m [M]\;.\] (5)

**Hard interventions.** Under a _hard_ intervention on node \(i\), the functional dependence of node \(i\) on its parents is removed, and the observational causal mechanism \(p_{i}(z_{i} z_{(i)})\) is changed to an interventional causal mechanism \(q_{i}(z_{i})\), independent of parents of node \(i\).

To distinguish the observational and interventional data, we denote the latent and observed random variables in environment \(^{m}\) by \(Z^{m}\) and \(X^{m}\), respectively. We note that interventions do not affect the transformation \(\). Hence, in \(^{m}\) we have \(X^{m}= Z^{m}\) for all \(m[M]\).

**Score functions.** The score function of a pdf is defined as the gradient of its logarithm. We denote the score functions associated with the distributions of \(Z^{m}\) and \(X^{m}\) by

\[_{Z}^{m}(z) p_{Z}^{m}(z)\;,_{X}^{m}(x) p_{X}^{m}(x)\;, m[M]\;.\] (6)

Note that, using the factorization in (5), \(_{Z}^{m}\) decomposes as

\[_{Z}^{m}(z)=_{i I^{m}} q_{i}(z_{i} z_{(i )})+_{i I^{m}} p_{i}(z_{i} z_{(i)})\;.\] (7)

We denote the difference in score functions between interventional and observational environments by

\[_{Z}^{m}(z)_{Z}^{m}(z)-_{Z}(z)_{X}^{m}(x)_{X}^{m}(x)-_{X}(x)\;,  m[M]\;.\] (8)

### Identifiability criteria

In CRL, we use observed variables \(X\) to recover the true latent variables \(Z\) and the latent causal graph \(\). We denote a generic estimator of \(Z\) given \(X\) by \((X):^{d}^{n}\). We also consider a generic estimate of \(\) denoted by \(}\). To assess the fidelity of the estimates \((X)\) and \(}\) with respect to the ground truth \(Z\) and \(\), we provide the following well-known identifiability measures.

**Definition 1** (Identifiability).: _For CRL under linear transformations, we define:_

1. **Perfect identifiability:**__\(}\) _and_ \(\) _are isomorphic, and the estimator_ \((X)\) _satisfies that_ \[(X)=_{}_{} Z\, Z^{n}\,\] (9) _where_ \(_{}^{n n}\) _is a_ constant _diagonal matrix with nonzero diagonal entries and_ \(_{}\) _is a row permutation matrix._
2. **Identifiability up to ancestors:**__\(}\) _and transitive closure of_ \(\)_, denoted by_ \(_{}\)_, are isomorphic, and the estimator_ \((X)\) _satisfies that_ \[(X)=_{}_{} Z\,  Z^{n}\,\] (10) _where_ \(_{}^{n n}\) _is a_ constant _matrix with nonzero diagonal entries that satisfies_ \([_{}]_{i,j}=0\) _for all_ \(j\{(i)\{i\}\}\)_, and_ \(_{}\) _is a row permutation matrix._

In the algorithm we will design, estimating \((X)\) and \(}\) are facilitated by estimating the inverse of the transformation \(\), that is Moore-Penrose inverse \(^{}[^{}]^{-1} ^{}\), which we refer to as the _true encoder_. To formalize the process of estimating the true encoder, we define \(\) as the set of candidate encoders specified by \(\{^{n d}:( )=n^{} X=X\,\  X \}\). Corresponding to any pair of observation \(X\) and valid encoder \(\), we define \((X;)\) as an _auxiliary_ estimate of \(Z\) generated as \((X;) X=()  Z\).

## 3 Identifiability under UMN interventions

In this section, we present the main identifiability and achievability results for CRL with UMN interventions and interpret them in the context of the recent results in the literature. We start by specifying the regularity conditions on the statistical models, which are needed to ensure sufficient statistical diversity and establish identifiability results for hard and soft UMN interventions. The constructive proofs of the results are based on CRL algorithms, the details of which are presented in Section 4. Complete proofs are deferred to Appendix A.

We note that the UMN setting subsumes SN interventions. Similarly to all the existing identifiability results from SN interventions, it is necessary to have _sufficient_ statistical diversity created by the intervention models.3 These conditions can be generally presented in the form of regularity conditions on the probability distributions. Specifically, a commonly adopted regularity condition (or its variations) in the SN intervention setting is that for every possible pair \((i,j)\) where \(i[n],j(i)\), the following term cannot be a constant function in \(z\),

\[}((z_{i} z_{(i)})}{q_{i}(z_{i} z_{(i)})})[}(z_{i} z_{(i)})}{q_{i}(z_{i} z _{(i)})}]^{-1}\.\] (11)

We present a counterpart of these conditions for UMN interventions, which involves one additional term to account for the effect of intervening on multiple nodes simultaneously.

**Definition 2** (Intervention regularity).: _We say that an interventions are regular if for every possible triplet \((i,j,c)\) where \(i[n],j(i)\) and \(c\), the following ratio cannot be a constant function in \(z\)_

\[}((z_{i} z_{ (i)})}{q_{i}(z_{i} z_{(i)})}+c(z_{j} z_{ (j)})}{q_{j}(z_{j} z_{(j)})})[}(z_{i} z_{(i)})}{q_{i}(z _{i} z_{(i)})}]^{-1}\.\] (12)Essentially, \(}(z_{i}|z_{(i)})}{q_{i}(z_{i }|z_{(i)})}\) captures the effect of intervening on node \(i\) on the _score_ associated with node \(j\). In our method, we will use combinations of score differences of multi-node environments. This regularity condition ensures that the effect of a multi-node intervention is not the same on the scores associated with different nodes. Given these properties, we establish perfect identifiability for CRL with linear transformations using UMN stochastic hard interventions.

**Theorem 1** (Identifiability under UMN hard interventions).: _Under Assumption 1 and a latent model with additive noise,_

1. _perfect latent recovery is possible using regular UMN hard interventions; and_
2. _if the latent causal model satisfies adjacency-faithfulness, then perfect latent DAG recovery is possible using regular UMN hard interventions._

Theorem 1 is the first perfect identifiability result using UMN _stochastic_ hard interventions. In contrast,  establishes perfect latent recovery using highly more stringent do-interventions. Furthermore, Theorem 1 establishes the first perfect latent DAG recovery result (under any type of multi-node interventions) for nonparametric latent models. We note that the capability of handling nonparametric latent models stems from leveraging the score functions. Similar properties are demonstrated by the prior work on score-based CRL for SN interventions . It is noteworthy that we use a total of \(n+1\) environments whereas the study in  requires \(2_{2}n\) do interventions of strongly separating sets. However, we show that identifiability is _impossible_ using strongly separating sets of UMN stochastic hard interventions (see Appendix A.6).

Next, we consider UMN _soft_ interventions. Since soft interventions retain the ancestral dependence of the intervened node, in general, the identifiability guarantees for soft interventions are weaker than those of hard interventions. Next, we establish that UMN soft interventions guarantee identifiability up to ancestors for the general causal latent models and linear transformations.

**Theorem 2** (Identifiability under UMN soft interventions).: _Under Assumption 1, identifiability up to ancestors is possible using regular UMN soft interventions._

Identifiability up to ancestors has recently shown to be possible using SN soft interventions on general latent models . Theorem 2 establishes the same identifiability guarantees without the restrictive assumption of SN interventions. Furthermore, Theorem 2 is significantly different from existing results for UMN soft interventions. Specifically, the study in  focuses on linear non-Gaussian latent models and requires \(|(i)|+1\) distinct mechanisms for each node \(i\). In contrast, Theorem 2 does not make parametric assumptions on latent variables and works with sufficiently diverse interventions described by Assumption 1 instead of requiring multiple interventional mechanisms for the same node.

## 4 UMN interventional CRL algorithm

In this section, we design the **U**nknown **M**ulti-node **I**nterventional (UMNI)-CRL algorithm that achieves identifiability guarantees presented in Section 3. This algorithm falls in the category of score-based frameworks for CRL [5; 7] and incorporates novel components to this framework that facilitate UMN interventions with provable guarantees. Our score-based approach uses the structural properties of score functions and their variations across different interventional environments to find reliable estimates for the true encoder \(^{}\). The critical step involved is a process that can aggregate the score differences under the available interventional environments, which have entirely _unknown_ intervention targets, and reconstruct the score differences for any desired hypothetical set of intervention targets. In particular, we establish that such desired score differences can be computed by aggregating the score differences available under the given UMN interventions. The proposed UMNI-CRL algorithm consists of four stages for implementing CRL. The properties of these stages also serve as the steps of constructive proof for identifiability results. We present the key algorithmic stages and their properties in the remainder of this section and defer their proofs to Appendix A.

**Stage 1: Basis score differences.** In the first stage, we compute score differences for each interventional environment and construct the _basis score difference_ functions that are linearly independent. The purpose of these functions is to subsequently use them and reconstruct the score differences under any arbitrary hypothetical interventional environment. To this end, we use the following relationship between score functions of \(X\) and \(Z\).

```
1:Input: Samples of \(X\) from environment \(^{0}\) and interventional environments \(\{^{m}:m[M]\}\)
2:Stage 1: Choose basis score differences and construct \(_{X}\) using (17)
3:Stage 2: Identifiability up to a causal order
4:for\(t(1,,n)\)do
5:for\(\)do\(\)\(\) is specified in (18)
6:\(_{}(\{ _{i}^{*}\ :\,i[t-1]\})}(_{X})\)
7:if\(()=1\)then
8: pick \((_{X}) (\{_{i}^{*}:i[t-1]\})\)
9:\(_{i}^{*}/\|\|_{2}\) and \([]_{:,t}\)
10:break
11:Stage 2 outputs: \(^{*}\) and \(\) ```

**Algorithm 1** Unknown **M**ulti-node **I**terventional (UMNI)-CRL

```
1:Input: Samples of \(X\) from environment \(^{0}\) and interventional environments \(\{^{m}:m[M]\}\)
2:Stage 1: Choose basis score differences and construct \(_{X}\) using (17)
3:Stage 2: Identifiability up to a causal order
4:for\(t(1,,n)\)do
5:for\(\)do\(\)\(\) is specified in (18)
6:\(_{}(\{ _{i}^{*}\ :\,i[t-1]\})}(_{X})\)
7:if\(()=1\)then
8: pick \((_{X}) (\{_{i}^{*}:i[t-1]\})\)
9:\(_{i}^{*}/\|\|_{2}\) and \([]_{:,t}\)
10:break
11:Stage 2 outputs: \(^{*}\) and \(\) ```

**Algorithm 2**Stage 3: Identifiability up ancestors

```
1:Input: Samples of \(X\) from environment \(^{0}\) and interventional environments \(\{^{m}:m[M]\}\)
2:Stage 1: Choose basis score differences and construct \(_{X}\) using (17)
3:Stage 2: Identifiability up to a causal order
4:for\(t(1,,n)\)do
5:for\(\)do\(\)\(\) is specified in (18)
6:\(_{}(\{ _{i}^{*}\ :\,i[t-1]\})}(_{X})\)
7:if\(()=1\)then
8: pick \((_{X}) (\{_{i}^{*}\ :\,i[t-1]\})\)
9:\(_{i}^{*}/\|\|_{2}\) and \([]_{:,j}^{*}\)
10: Set \(\) False and break
11:if_parent is True then
12: Add \(t j\) and \(t u\) to \(}\) for all \(u}(j)\)\(\) edges to identified descendants
13:Stage 3 outputs: \(},^{*}\) and \(\) ```

**Algorithm 3** State 3 outputs: \(},^{*}\) and \(\)

```
1:Input: Samples of \(X\) from environment \(^{0}\) and interventional environments \(\{^{0}:m[M]\}\)
2:Stage 1: Choose basis score differences and construct \(_{X}\) using (17)
3:Stage 4: Identifiability up to a causal order
4:for\(t(1,,n)\)do
5:for\(\)do\(\)\(\) is specified in (18)
6:\(_{}(\{ _{i}^{*}\ :\,i[t-1]\})}(_{X})\)
7:if\(()=1\)then
8: pick \((_{X}) (\{_{i}^{*}:i[t-1]\})\)
9:\(_{i}^{*}/\|\|_{2}\) and \([]_{:,t}\)
10:break
11:\((X;^{*})\)
12:for\(t(1,,n)\)do
13:for\(j}(t)\)do
14:if\(_{t} 1.5mu }_{j}\{_{i}:i }(j)\{t\}\}\)then
15: Remove \(t j\) from \(}\)\(\) removing the edges from the nonparent ancestors
16:Return\(}\) and \(\) ```

**Algorithm 4** State 4: Unmixing for hard interventions

```
1:Input: Samples of \(X\) from environment \(^{0}\) and interventional environments \(\{^{0}:m[M]\}\)
2:Stage 1: Choose basis score differences and construct \(_{X}\) using (17)
3:Stage 2: Identifiability up to a causal order
4:for\(t(1,,n)\)do
5:for\(\)do\(\)\(\) is specified in (18)
6:\(_{}(\{ _{i}^{*}\ :\,i[t-1]\})}(_{X})\)
7:if\(()=1\)then
8: pick \((_

**Lemma 1** ([7, Corollary 2]).: _Latent and observational score functions are related via \(_{X}(x)=[^{}]^{}_{Z}(z)\), where \(x= z\)._

Using Lemma 1 for the scores and score differences defined in (7) and (8), respectively, we have

\[_{X}^{m}(x)=[^{}]^{}_{Z}^{m} (z))}}{{=}}[^{}]^{} _{i I^{m}}(z_{i} z_{(i)})}{q_ {i}(z_{i} z_{(i)})}\;.\] (13)

We compactly represent the summands in the right-hand side of (13) by defining the matrix-valued function \(:^{n}^{n n}\) with the entries

\[[(z)]_{j,i}}(z_{i} z_{(i)})}{q_{i}(z_{i} z_{(i)})}\;,  i,j[n]\;,\] (14)

based on which (13) can be restated as

\[_{X}^{m}(x)=[^{}]^{}(z) [_{}]_{:,m}\;.\] (15)

We note that \([(z)]_{i,j}\) is constantly zero for \(i\{(j)\{j\}\}\), and \(j\)-th column of \(\) is a function of the variables in \(\{z_{k}:k(j)\{j\}\}\) which implies that the columns of \(\) are linearly independent. Throughout the rest of the paper, we omit the arguments of the functions \(_{X}^{m}\) and \(\) when the dependence is clear from the context. Note that \(_{}\) has \(n\) linearly independent columns (Assumption 1). Denote the indices of the independent columns by \(\{b_{1},,b_{n}\}\) and define the _basis intervention matrix_\(^{n n}\) using these columns as

\[[]_{i,m}[_{}]_{i,b_{m}}= \{i I^{b_{m}}\}\;, i,m[n]\;.\] (16)

Subsequently, it can be readily verified that the score difference functions \(\{_{X}^{m}:m\{b_{1},,b_{n}\}\}\) are also linearly independent by leveraging (15) and linearly independent columns of \(\). Hence, these score difference functions are sufficient to reconstruct the remaining unavailable score difference functions. As such, \(\{_{X}^{m}:m\{b_{1},,b_{n}\}\}\) serve as _basis score difference_ functions. We stack these basis score differences, where each is a \(d\)-dimensional vector, to construct the matrix-valued function \(_{X}:^{d n}\), which is used as the basis score difference matrix in the subsequent stages.

\[_{X}[_{X}^{b_{1}},, {s}_{X}^{b_{n}}]=[^{}]^{} \;.\] (17)

Finally, we note that \(_{X}\) is directly estimated from samples of \(X\) via learning \(\{_{X}^{b_{m}}:m[n]\}\). Since \(\) encodes the score differences in latent space, it cannot be estimated directly. Furthermore, \(\) is unknown, and (17) is given to emphasize the relationship between observed and latent score differences.

**Stage 2: Identifiability up to an unknown causal order.** We design a process that aggregates score differences of the UMN interventions and obtains a _partial_ identifiability guarantee (identifiability up to an unknown causal order) as an intermediate step toward more accurate identifiability. Specifically, we linearly aggregate the columns of \(_{X}\) such that those aggregate scores facilitate identifiability up to an unknown causal order. Such mixing of the columns is facilitated by computing \(_{X}\), where the mixing matrix \(^{n n}\) should be learned. Given the decomposition of \(_{X}\) in (17), if we learn \(\) such that \(\) is upper triangular up to a row permutation, then we can subsequently learn an intermediate estimate \(^{*}\) using the image of \(_{X}\). This ensures identifiability up to an unknown causal order since rows of \(^{*}\) will be equal to combinations of rows of \(^{}\) up to a causal order.

We design an iterative process to sequentially learn the columns of \(\). Specifically, at each iteration of Stage 2, we learn an integer-valued vector \(\) such that the projection of \((_{X})\) onto the nullspace of the partially recovered encoder estimate becomes a one-dimensional subspace. To see why this procedure works, note that the function \(_{X}\) is essentially a combination of SN latent score differences via (17), and the SN score difference \( p_{i}(z_{i} z_{(i)})- q_{i}(z_{i} z_{ (i)})\) is a one-dimensional subspace if and only if the intervened node \(i\) has no parents. By taking the projection of the \((_{X})\) onto the nullspace of the partially learned encoder while searching for a desired \(\), we ensure that the final encoder estimate \(^{*}\) of this stage will be full-rank. Finally, we use \(\) to denote maximum determinant of a matrix in \(\{0,1\}^{(n-1)(n-1)}\), and show that the set

\[\{-,,+\}^{n}\] (18)

is guaranteed to contain such \(\) vectors. The following result summarizes the guarantees of this procedure.

**Lemma 2**.: _Under Assumption 1 and intervention regularity, outputs of Stage 2 of Algorithm 1 satisfy:_

1. \(\) _has nonzero diagonal entries and is upper triangular up to a row permutation._
2. \([^{*}]_{t}(\{[^{}]_{_{j}}:j[t]\})\) _for all_ \(t[n]\) _and_ \(\{[^{*}]_{t}:t[n]\}\) _are linearly independent._

Proof.: See Appendix A.1. 

**Stage 3: Identifiability up to ancestors.** Next, we refine the outcome of Stage 2 to ensure identifiability up to ancestors by updating the columns of \(\) such that the entries of \(\) can be nonzero only for the coordinates that correspond to ancestor-descendant node pairs. For this purpose, we design Stage 3 of UMNI-CRL that iteratively updates the columns of \(\). The key idea is that the edges in the transitive closure graph \(_{}\) can be determined by investigating the subspaces' dimensions similarly to Stage 2. Leveraging this property, for all node pairs that do not constitute an edge in \(_{}\), we aggregate the corresponding columns of \(\) such that the corresponding entry of \(\) will be zero. In Theorem 3, we show that the outputs of this stage achieve identifiability up to ancestors, that is \(}\) is isomorphic to \(_{}\) and \(_{i}\) is a linear function of \(\{Z_{_{j}}:j(i)\{i\}\}\) for all \(i[n]\).

**Theorem 3**.: _Under Assumption 1 and regular UMN soft interventions, outputs of Stage 3 of Algorithm 1 have the following properties._

1. _The estimate_ \((X;^{*})\) _satisfies identifiability up to ancestors._
2. \(}\) _and_ \(_{}\) _are related through a graph isomorphism._

Proof.: See Appendix A.2. 

**Stage 4: Perfect identifiability via hard interventions.** In the case of hard interventions, we apply an unmixing procedure to further refine our estimates and achieve perfect identifiability. This stage consists of two steps. The first step relies on the property that the intervened node becomes independent of its non-descendants and updates rows of the encoder estimate sequentially. In the second step, we leverage the knowledge of ancestral relationships and use a small number of conditional independence tests to refine the graph estimate from transitive closure \(_{}\) to the true latent DAG \(\). The following theorem summarizes the guarantees achieved by Algorithm 1.

**Theorem 4**.: _Under Assumption 1 and regular UMN hard interventions for an additive noise model, outputs of Stage 4 of Algorithm 1 have the following properties._

1. _Estimate_ \((X;^{*})\) _satisfies perfect latent variable recovery._
2. _If_ \(p_{Z}\) _is adjacency-faithful to_ \(\)_, then_ \(}\) _and_ \(\) _are related through a graph isomorphism._

Proof.: See Appendix A.3. 

Finally, we note that the computational cost of UMNI-CRL is dominated by the cardinality of the search space for aggregating the score differences, e.g., in the worst-case, Stage 2 has \(((2)^{n})\) complexity. Therefore, the structure of the UMN interventions determines the complexity via its determinant. We elaborate on the computational complexity of the algorithm and the range of \(\) in Appendix A.8.

## 5 Simulations

We empirically assess the performance of the UMNI-CRL algorithm for recovering the latent DAG \(\) and latent variables \(Z\). Implementation details and additional results are provided in Appendix B4.

**Data generation.** To generate \(\), we use Erdos-Renyi model with density \(0.5\) and \(n\{4,5,6,7,8\}\) nodes. For the causal models, we adopt linear structural equation models (SEMs) with Gaussian noise. The nonzero edge weights of the linear SEMs are sampled from \(([0.5,1.5])\), and the noise terms are zero-mean Gaussian variables with variances \(_{i}^{2}\) sampled from \(([0.5,1.5])\). For a soft intervention on node \(i\), the edge weight vector of node \(i\) is reduced by a factor of \(1/2\), and for a hard intervention, the edge weights are set to zero. The variance of the noise term is reduced to \(_{i}^{2}/4\) in both intervention types. We consider target dimensions \(d\{10,50\}\), generate 100 latent graphs for each \((n,d)\) pair, and generate \(n_{}=10^{5}\) samples of \(Z\) from e\(^{d n}\) is randomly sampled under full-rank constraint, and observed variables are generated as \(X= Z\). Finally, for each graph realization, intervention matrix \(\) is chosen randomly among column permutations of full-rank \(\{0,1\}^{n n}\) matrices, which satisfies Assumption 1.

**Score functions.** The algorithm uses score differences between environment pairs. Since we use a linear Gaussian model, \(X\) is also multivariate Gaussian, and its score function can be estimated by \(s_{X}(x)=- x\) in which \(\) is the sample estimate of the precision matrix of \(X\). We note that the design of UMNI-CRL is agnostic to the choice of the estimator and can adopt any reliable score estimator for nonparametric distributions [29; 30].

**Graph recovery.** To assess the graph recovery, we report the structural Hamming distance (SHD) between the true and estimated DAGs. Recall that UMN hard and soft interventions ensure different levels of identifiability guarantees. Hence, we report the SHD between (i) transitive closure \(_{}\) and \(}\) for soft interventions, and (ii) true DAG \(\) and \(}\) for hard interventions. Table 2 shows that latent graph recovery performance remains consistent for both soft and hard interventions when observed variables dimension \(d\) increases from \(10\) to \(50\), which conforms to our expectations due to theoretical results. Note that the expected number of edges is \(n(n-1)/4\) since we set the density of random graphs to \(0.5\). Hence, the increasing SHD is also unsurprising when the latent dimension \(n\) increases from \(4\) to \(8\), and the performance remains reasonable at \(n=8\). Finally, we note that \(n=8\) is the largest latent graph size considered among the closely related SN intervention studies [4; 8; 7; 9].

**Latent variable recovery.** The estimates are given by \((X;^{*})=(^{*})\). Hence, we scrutinize the effective mixing matrix \((^{*})\) and report the ratio of its _incorrect mixing entries_ to the number of zeros in constant matrices \(_{}\) and \(_{}\) according to Definition 1, denoted by

\[_{}([^ {*}]_{i,j} 0)}{n^{2}-n}\;,_{}}(i)}([ ^{*}]_{i,j} 0)}{n^{2}-_{i}|\,}(i)|}\;.\] (19)

We also report the mean correlation coefficient (MCC) , which measures linear correlations between the estimated and ground truth latent variables and is commonly used in related work. Table 2 shows that the UMNI-CRL algorithm achieves strong MCC performance (over \(0.90\)) in all cases. Furthermore, the ratio of incorrect mixing entries remains less than \(0.20\) for both soft and hard interventions This demonstrates a strong performance of the UMNI-CRL algorithm at recovering latent variables for as many as \(n=8\) latent variables even for observed variables dimension \(d=50\).

## 6 Discussion

In this paper, we established novel identifiability results using unknown multi-node (UMN) interventions for CRL under linear transformations. Specifically, we designed the provably correct UMNI-CRL algorithm, leveraging the structural properties of score functions across different environments. To facilitate identifiability, we introduced a sufficient condition for the set of UMN interventions, abstracted as having \(n\) sufficiently diverse interventional environments. Investigating the necessary conditions for UMN interventions to enable identifiability remains an open problem. The main limitation is the assumption of linear transformations. Given existing results for general transformations using two SN interventions per node [6; 9], a promising direction for future work is extending our results to general transformations using UMN interventions with multiple interventional mechanisms per node.

    & &  &  \\ \(n\) & \(d\) & \((_{},})\) & MCC & \(_{}\) & \((,})\) & MCC & \(_{}\) \\ 
4 & 10 & \(0.91 0.12\) & \(0.95 0.01\) & \(0.08 0.01\) & \(0.75 0.11\) & \(0.98 0.02\) & \(0.13 0.02\) \\
5 & 10 & \(1.67 0.20\) & \(0.93 0.01\) & \(0.09 0.01\) & \(1.65 0.11\) & \(0.97 0.02\) & \(0.13 0.02\) \\
6 & 10 & \(3.19 0.26\) & \(0.92 0.01\) & \(0.12 0.01\) & \(3.12 0.25\) & \(0.96 0.02\) & \(0.12 0.02\) \\
7 & 10 & \(5.44 0.34\) & \(0.90 0.01\) & \(0.15 0.01\) & \(5.36 0.35\) & \(0.93 0.03\) & \(0.15 0.03\) \\
8 & 10 & \(7.63 0.41\) & \(0.89 0.01\) & \(0.16 0.01\) & \(9.70 0.52\) & \(0.87 0.03\) & \(0.20 0.03\) \\ 
4 & 50 & \(0.77 0.12\) & \(0.96 0.01\) & \(0.06 0.01\) & \(0.66 0.10\) & \(0.98 0.01\) & \(0.13 0.02\) \\
5 & 50 & \(1.93 0.20\) & \(0.93 0.01\) & \(0.10 0.01\) & \(1.80 0.19\) & \(0.98 0.02\) & \(0.13 0.01\) \\
6 & 50 & \(3.39 0.27\) & \(0.92 0.01\) & \(0.13 0.01\) & \(3.05 0.25\) & \(0.95 0.03\) & \(0.13 0.01\) \\
7 & 50 & \(4.62 0.30\) & \(0.91 0.01\) & \(0.13 0.01\) & \(6.12 0.34\) & \(0.91 0.02\) & \(0.16 0.01\) \\
8 & 50 & \(8.26 0.49\) & \(0.90 0.01\) & \(0.14 0.01\) & \(9.01 0.53\) & \(0.88 0.03\) & \(0.28 0.02\) \\   

Table 2: UMNI-CRL for a linear causal model with UMN interventions (mean \(\) standard error)

#### Acknowledgements and disclosure of funding

This work was supported by IBM through the IBM-Rensselaer Future of Computing Research Collaboration.