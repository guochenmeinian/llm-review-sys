# Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs

Peng Jin\({}^{1,4}\) Yang Wu\({}^{3}\) Yanbo Fan\({}^{3}\) Zhongqian Sun\({}^{3}\) Yang Wei\({}^{3}\) Li Yuan\({}^{1,2,4}\)

\({}^{1}\) School of Electronic and Computer Engineering, Peking University, Shenzhen, China

\({}^{2}\) Peng Cheng Laboratory, Shenzhen, China \({}^{3}\) Tencent AI Lab, China

\({}^{4}\)AI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School, China

jp21@stu.pku.edu.cn dylan.yangwu@qq.com yuanli-ece@pku.edu.cn

Corresponding author: Yang Wu, Li Yuan.

###### Abstract

Most text-driven human motion generation methods employ sequential modeling approaches, e.g., transformer, to extract sentence-level text representations automatically and implicitly for human motion synthesis. However, these compact text representations may overemphasize the action names at the expense of other important properties and lack fine-grained details to guide the synthesis of subtly distinct motion. In this paper, we propose hierarchical semantic graphs for fine-grained control over motion generation. Specifically, we disentangle motion descriptions into hierarchical semantic graphs including three levels of motions, actions, and specifics. Such global-to-local structures facilitate a comprehensive understanding of motion description and fine-grained control of motion generation. Correspondingly, to leverage the coarse-to-fine topology of hierarchical semantic graphs, we decompose the text-to-motion diffusion process into three semantic levels, which correspond to capturing the overall motion, local actions, and action specifics. Extensive experiments on two benchmark human motion datasets, including HumanML3D and KIT, with superior performances, justify the efficacy of our method. More encouragingly, by modifying the edge weights of hierarchical semantic graphs, our method can continuously refine the generated motion, which may have a far-reaching impact on the community. Code and pre-trained weights are available at https://github.com/jpthu17/GraphMotion.

## 1 Introduction

Human motion generation is a fundamental task in computer animation [54; 4] and has many practical applications across various industries including gaming, film production, virtual reality, and robotics [61; 8]. With the progress made in recent years, text-driven human motion generation has allowed for the synthesis of a variety of human motion sequences based on natural language descriptions. Therefore, there is a growing interest in generating manipulable, plausible, diverse, and realistic sequences of human motion from flexible natural language descriptions.

Existing text-to-motion generation methods [42; 61; 8; 1; 63] mainly rely on sentence-level representations of texts and directly learn the mapping from the high-level language space to the motion sequences. Recently, some works [54; 8; 62] propose conditional diffusion models for human motion synthesis and further improve the synthesized quality and diversity. Although these methods have made encouraging progress, they are still deficient in the following two aspects. (i) **Imbalance.** The model, which directly uses the transformers  to extract text features automatically and implicitly, may overemphasize the action names at the expense of other important properties like direction andintensity. As a typical consequence of this unbalanced learning, the network is insensitive to the subtle changes in the input text and lacks fine-grained controllability. (ii) **Coarseness.** On the one hand, motion descriptions frequently refer to multiple actions and attributes. However, the compact sentence-level representations extracted by current works usually fail to convey the clarity and detail needed to fully understand the text, leading to a lack of fine-grained details to guide the synthesis of subtly distinct motion. On the other hand, mapping directly of existing works from the high-level language space to motion sequences further hinders the generation of fine-grained details. Therefore, we argue that it is time to seek a more precise and detailed text-driven human motion generation method to ensure an accurate synthesis of complex human motions.

To this end, we propose a more fine-grained control signal, hierarchical semantic graphs, to represent different intentions for controllable motion generation and design a coarse-to-fine motion diffusion model, called GraphMotion. As shown in Fig. 1, motion descriptions inherently possess hierarchical structures and can be represented as hierarchical graphs composed of three types of abstract nodes, namely motions, actions, and specifics. Concretely, the overall sentence describes the global motion involving multiple actions, e.g., "walk", "pick", and "stand" in Fig. 1, which occur in sequential order. Each action consists of different specifics that act as its attributes, such as the agent and patient of the action. Such global-to-local structures contribute to a reliable and comprehensive understanding of motion descriptions. Correspondingly, to take full advantage of this fine-grained control signal, we decompose the text-to-motion diffusion process into three semantic levels from coarse to fine, which are responsible for capturing the overall motion, local actions, and action specifics, respectively.

The proposed GraphMotion has three compelling advantages: **First**, the explicit factorization of the language embedding space enables us to build a fine-grained correspondence between textual data and motion sequences, which avoids the imbalanced learning of different textual components and coarse-grained control signal representation. **Second**, the hierarchical refinement property of GraphMotion allows the model to progressively enhance the generated results from coarse to fine, which avoids the coarse-grained generated results. **Third**, to further fine-tune the generated results for more fine-grained control, our method can continuously refine the generated motion by modifying the edge weights of the hierarchical semantic graph. Experimental results on two benchmark datasets for text-to-motion generation, including HumanML3D  and KIT , demonstrate the advantages of GraphMotion. The main contributions of this work are as follows:

* To the best of our knowledge, we are the first to propose hierarchical semantic graphs, a fine-grained control signal, for text-to-motion generation. It decomposes motion descriptions into global-to-local three types of abstract nodes, namely motions, actions, and specifics.
* Correspondingly, we decompose the text-to-motion diffusion process into three semantic levels. This allows the model to gradually refine results from coarse to fine. Experiments show that our method achieves new state-of-the-art results on two text-to-motion datasets.
* More encouragingly, by modifying the edge weights of hierarchical semantic graphs, our method can continuously refine the generated motion, which may have a far-reaching impact.

Figure 1: **We propose hierarchical semantic graphs, a fine-grained control signal, for text-to-motion generation and factorize text-to-motion generation into hierarchical levels including motions, actions, and specifics to form a coarse-to-fine structure. This approach enhances the fine-grained correspondence between textual data and motion sequences and achieves better controllability conditioning on hierarchical semantic graphs than carefully designed baselines.**

Related Work

Text-driven Human Motion Generation.Text-driven human motion generation aims to generate 3D human motion based on text descriptions. Due to the user-friendliness and convenience of natural language , text-driven human motion generation is gaining significant attention and has many applications. Recently, two categories of motion generation methods have emerged: joint-latent models [2; 42] and diffusion models [8; 62; 47]. Joint-latent models, e.g., TEMOS , typically learn a motion variational autoencoder and a text variational autoencoder. These models then constrain the text and motion encoders into a shared latent space using the Kullback-Leibler divergences  loss. The latter category of methods, e.g., MDM , proposes a conditional diffusion model for human motion generation to learn a powerful probabilistic mapping from the textual descriptors to human motion sequences. Although these methods have made encouraging progress, they still suffer from two major deficiencies: unbalanced text learning and coarse-grained generated results. In this paper, we propose to leverage the inherent structure of language [6; 58] for motion generation. Specifically, we introduce hierarchical semantic graphs as a more effective control signal for representing different intentions and design a coarse-to-fine motion diffusion model using this signal.

Diffusion Generative Models.Diffusion generative models [49; 18; 11; 21; 50] are a type of neural generative model that uses the stochastic diffusion process, which is based on thermodynamics. The process involves gradually adding noise to a sample from the data distribution, and then training a neural network to reverse this process by gradually removing the noise. In recent years, diffusion models have shown promise in a variety of tasks such as image generation [18; 50; 11; 19; 57], natural language generation [3; 36; 13], as well as visual tasks . Some other works [25; 32] have attempted to adapt diffusion models for cross-modal retrieval [33; 34; 35; 23; 24]. Inspired by the success of diffusion generative models, some works [62; 54; 8] have applied diffusion models to human motion generation. However, these methods typically learn a one-stage mapping from the high-level language space to motion sequences, which hinders the generation of fine-grained details. In this paper, we decompose the text-to-motion diffusion process into three semantic levels from coarse to fine. The resultant levels are responsible for capturing overall motion, local actions, and action specifics, which enhances the generated results progressively from coarse to fine.

Graph-based Reasoning.The graph convolutional network  is originally proposed to recognize graph data. It uses convolution on the neighborhoods of each node to produce outputs. Graph attention networks  further enhance graph-based reasoning by dynamically attending to the features of neighborhoods. The graph-based reasoning has shown great potential in many tasks, such as scene graph generation , visual question answering [20; 31; 30], natural language generation , and cross-modal retrieval . In this paper, we focus on reasoning over hierarchical semantic graphs on motion descriptions for fine-grained control of human motion generation.

## 3 Methodology

In this paper, we tackle the tasks of text-driven human motion generation. Concretely, given an arbitrary motion description, our goal is to synthesize a human motion \(x^{1:L}=\{x^{i}\}_{i=1}^{L}\) of length \(L\). The overview of the proposed GraphMotion is shown in Fig. 2.

### Hierarchical Semantic Graph Modeling

Existing works for text-driven human motion generation typically directly use the transformer  to extract text features automatically and implicitly. However, motion descriptions inherently possess hierarchical structures that can be divided into three sorts of abstract nodes, including motions, actions, and specifics. Compared with the sequential structure, such global-to-local structure contributes to a reliable and comprehensive understanding of the semantic meaning of motion descriptions and is a promising fine-grained control signal for text-to-motion generation.

Semantic Role Parsing.To obtain actions, attributes of action as well as the semantic role of each attribute to the corresponding action, we implement a semantic parser of motion descriptions based on a semantic role parsing toolkit [48; 7]. We extract three types (motions, actions, and specifics) of nodes and twelve types of edges to represent various associations among the nodes. For details about the types of nodes and edges, please refer to our supplementary material.

Specifically, given the motion description, the parser extracts verbs that appeared in the sentence and attribute phrases corresponding verb, and the semantic role of each attribute phrase. The overall sentence is treated as the global motion node in the hierarchical graph. The verbs are considered as action nodes and connected to the motion node with direct edges, allowing for implicit learning of the temporal relationships among various actions during graph reasoning. The attribute phrases are specific nodes that are connected with action nodes. The edge type between action and specific nodes is determined by the semantic role of the specifics in relation to the action.

Graph Node Representation.Given the motion description, we follow previous works  and leverage the text encoder of CLIP  to extract the text embedding. For the global motion node \(v^{m}\), we utilize the [CLS] token to summarize the salient event described in the sentence. For the action node \(v^{a}\), we use the token of the corresponding verb as the action node representation. For the specific node \(v^{s}\), we apply mean-pooling over tokens of each word in the attribute phrase.

Action-aware Graph Reasoning.The interactions across different levels in the constructed graph not only explain the properties of local actions and how local actions compose the global motion, but also reduce ambiguity at each node. For example, the verb "pick" in Fig. 1 can represent different actions without context, but the context "with both hands" constrains its semantics, so that it represents the action of "pick up with both hands" rather than "pick up with one hand." Therefore, we propose to reason over interactions in the graph to obtain hierarchical textual representations.

We utilize graph attention networks  (GAT) to model interactions in a graph. Specifically, given the initialized node embeddings \(v=\{v^{m},v^{a},v^{s}\}\), we first transform the input node embeddings into higher-level embeddings \(h=\{h^{m},h^{a},h^{s}\}\) by:

\[h^{m}=v^{m}, h^{a}=v^{a}, h^{s}=v^{s},\] (1)

where \(^{D D}\) is a shared linear transformation and \(D\) is the dimension of node representation. For each pair \(\{h_{i},h_{j}\}\) of connected nodes, we concatenate the node \(h_{i}^{D}\) with its neighbor node \(h_{j}^{D}\), generating the input data \(_{ij}=[h_{i},h_{j}]^{2D}\) of the graph attention module.

However, in a graph with multiple types of edges, the vanilla graph networks need to learn separate transformation matrices for each edge type. This can be inefficient when learning from limited motion data, and prone to over-fitting on rare edge types.

To this end, we propose to factorize multi-relational weights into two parts: a common transformation matrix \(^{2D 1}\) that is shared for all edge types and a relationship embedding matrix \(_{}^{2D N}\) that is specific for different edges, where \(N\) is the number of edge types. Following GAT , we apply LeakyReLU  in the calculation of attention coefficients and set the negative input slope to 0.2. The attention coefficient \(_{ij}\) is formulated as:

\[e_{ij}=(^{}_{ij})+(R_{ij}_{}^{}_{ij}),_{ij}= (e_{ij})}{_{k N_{i}}(e_{ik})},\] (2)

Figure 2: **The overview of the proposed GraphMotion for text-driven human motion generation.** We factorize motion descriptions into hierarchical semantic graphs including three levels of motions, actions, and specifics. Correspondingly, we decompose the text-to-motion diffusion process into three semantic levels, which correspond to capturing the overall motion, local actions, and action specifics.

where \(R_{ij}^{1 N}\) is a one-hot vector denoting the type of edge between node \(i\) and \(j\). \(_{i}\) is the set of neighborhood nodes of node \(i\). To alleviate over-smoothing  in graph networks, we apply skip connection when calculating output embeddings. The output embeddings \(\) are formulated as:

\[_{i}=_{j_{i}}_{ij}h_{j} +v_{i},\] (3)

where \(\) is a nonlinear function. Following GAT , we use ELU  as the nonlinear function \(\).

### Coarse-to-Fine Motion Diffusion Model for Graphs

To leverage the coarse-to-fine topology of hierarchical semantic graphs during generation, we also decompose the text-to-motion diffusion process into three semantic levels, which correspond to capturing the overall motion, local actions, and action specifics. During the reverse denoising process, the fine-grained semantic layer generates results based on the results from the coarse-grained semantic layer. This allows for a detailed and plausible representation of the intended motion.

**Motion Representation.** Following previous works [42; 8; 61], we first encode the motion into the latent space with a motion variational autoencoder  and then use diffusion models to learn the mapping from hierarchical semantic graphs to the motion latent space.

Specifically, we build the motion encoder \(\) and decoder \(\) based on the transformer [55; 41]. For the motion encoder \(\), we take \(C\) learnable query tokens and motion sequence \(x^{1:L}=\{x^{i}\}_{i=1}^{L}\) as inputs to generate motion latent embeddings \(z^{C D^{}}\), where \(D^{}\) is the dimension of latent representation. For the motion decoder \(\), we take the latent embeddings \(z^{C D^{}}\) and the motion query tokens as the input to generate a human motion sequence \(x^{1:L}=\{x^{i}\}_{i=1}^{L}\) with \(L\) frames.

The loss \(_{}\) of the motion variational autoencoder can be divided into two parts. First, we use the mean squared error (MSE) to reconstruct the original input. Second, we use the Kullback-Leibler divergences (KL) loss  to narrow the distance between the distribution of latent space \(q(z|x)\) and the standard Gaussian distribution \((0,)\). The full loss \(_{}\) is formulated as:

\[_{}=_{x q(x)}((x))\|_{2}^{2}}_{}+((0,)\|q(z|x))}_{},\] (4)

where \(\) is the trade-off hyper-parameter. \(q(z|x)=(_{z},_{z})\) is obtained by sampling based on the mean \(_{z}\) and variance \(_{z}\) estimated by the model. To generate motion from coarse to fine step by step, we encode motion independently into three latent representation spaces \(z^{m}^{C^{m} D^{}}\), \(z^{a}^{C^{a} D^{}}\) and \(z^{s}^{C^{s} D^{}}\), where the number of tokens gradually increases, i.e., \(C^{m} C^{a} C^{s}\).

**Hierarchical Graph-to-Motion Diffusion.** Corresponding to the three-level structure of the hierarchical semantic graphs, we decompose the diffusion process into three semantic levels and build three transformer-based denoising models, which correspond to motions, actions, and specifics.

For the motion level model \(_{m}\), our goal is to learn the diffusion process from global motion node \(^{m}\) to motion latent representation \(z^{m}\). In a forward diffusion process \(q(z_{t}^{m}|z_{t-1}^{m})\), noised sampled from Gaussian distribution is added to a ground truth data distribution \(z_{0}^{m}\) at every noise level \(t\):

\[q(z_{t}^{m}|z_{t-1}^{m})=(z_{t}^{m};}z_{t-1}^{m}, _{t}), q(z_{1:T}^{m}|z_{0}^{m})=_{t=1}^{T}q(z_{t}^{m}|z _{t-1}^{m}),\] (5)

where \(_{t}\) is the step size which gradually increases. \(T\) is the length of the Markov chain. We sample \(z_{t}^{m}\) by \(z_{t}^{m}=_{t}}z_{0}^{m}+_{t}}^{m}\), where \(_{t}=_{i=1}^{t}(1-_{i})\). \(^{m}\) is a noise sampled from \((0,1)\). We follow previous works [18; 8] and predict the noise component \(^{m}\), i.e., \(}=_{m}(z^{m},t^{m},^{m})\).

For the action level model \(_{a}\), to leverage the results generated by the motion level, we concatenate the action node \(^{a}\), the motion node \(^{m}\), and the result \(z^{m}\) generated by the motion level together as the input of the action level denoising network, i.e., \(^{a}=_{a}(z^{a},t^{a},[^{m},^{a},z ^{m}])\).

For the specific level model \(_{s}\), we leverage the results generated by the action level and nodes at all semantic levels to predict the noise component, i.e., \(^{s}=_{s}(z^{s},t^{s},[^{m},^{a}, ^{s},z^{a}])\).

[MISSING_PAGE_FAIL:6]

**HumanML3D** is currently the largest 3D human motion dataset that originates from and textually reannotates the HumanAct12  and AMASS  datasets. This dataset comprises 14,616 human motions and 44,970 text descriptions, with each motion accompanied by at least three precise descriptions. The lengths of these descriptions are around 12 words. **KIT** contains 3,911 human motion sequences and 6,278 textual annotations. Each motion sequence is accompanied by one to four sentences, with an average description length of 8 words.

_Metrics._ Following previous works, we use the following five metrics to measure the performance of the model. (1) **R-Precision.** Under the feature space of the pre-trained network in , given one motion sequence and 32 text descriptions (1 ground-truth and 31 randomly selected mismatched descriptions), motion-retrieval precision calculates the text and motion Top 1/2/3 matching accuracy. (2) **Frechet Inception Distance (FID).** We measure the distribution distance between the generated and real motion using FID  on the extracted motion features . (3) **Multimodal Distance (MM-Dist).** We calculate the average Euclidean distances between each text feature and the generated motion feature from that text. (4) **Diversity.** All generated motions are randomly sampled to two subsets of the same size. Then, we extract motion features  and compute the average Euclidean distances between the two subsets. (5) **Multimodality (MModality).** For each text description, we generate 20 motion sequences, forming 10 pairs of motions. We extract motion features and calculate the average Euclidean distance between each pair. We report the average of all text descriptions.

_Implementation Details._ For the motion variational autoencoder, motion encoder \(\) and decoder \(\) all consist of 9 layers and 4 heads with skip connection . Following MLD , we utilize a frozen text encoder of the CLIP-ViT-L-14  model for text representation. The dimension of node representation \(D\) is set to 768. The dimension of latent embedding \(D^{}\) is set to 256. We set the token sizes \(C^{m}\) to 2, \(C^{a}\) to 4, and \(C^{s}\) to 8. We set \(\) to 1e-4. All our models are trained with the AdamW  optimizer using a fixed learning rate of 1e-4. We use 4 Tesla V100 GPUs for the training, and there are 128 samples on each GPU, so the total batch size is 512. For the HumanML3D dataset, the model is trained for 6,000 epochs during the motion variational autoencoder stage and 3,000 epochs during the diffusion stage. The number of diffusion steps of each level is 1,000 during training, and the step sizes \(_{i}\) are scaled linearly from \(8.5 1\)e-4 to 0.012. For runtime, training tasks 16 hours for motion variational autoencoder and 24 hours for denoiser on 4 Tesla V100 GPUs.

**Comparisons to State-of-the-art.** We compare the proposed GraphMotion with other methods on two benchmarks. In Tab. 1, we show the results on the HumanML3D test set. Tab. 2 shows the results

Figure 3: **Qualitative comparison of the existing methods. We provide the motion results from three text prompts. The darker colors indicate the later in time. The generated results of our method better match the descriptions, while others have downgraded motions or improper semantics, demonstrating that our method achieves superior controllability compared to well-designed baseline models.**

[MISSING_PAGE_EMPTY:8]

Analysis of the coarse-to-fine motion diffusion model.In Tab. 4, we provide the ablation study of the coarse-to-fine motion diffusion model on the HumanML3D test set. These results prove that coarse-to-fine generation is beneficial to motion generation. In addition, we show the performance at each level in Fig. 4. Among the three levels, the performance of the specific level is the best, which confirms the effectiveness of the coarse-to-fine motion diffusion model.

Effect of the diffusion steps.In Tab. 5, we show the ablation study of the total number of diffusion steps on the HumanML3D test set. Following MLD , we adopt the denoising diffusion implicit models  (DDIM) during interference. As shown in Tab. 5, our method consistently outperforms the existing state-of-the-art methods with the same total number of diffusion steps, which demonstrates the efficiency of our method. With the increase of the total diffusion steps, the performance of our method is further improved, while the performance of MLD saturates. We find that the number of diffusion steps at the higher level (e.g., specific level) has a greater impact on the result. Therefore, in scenarios requiring high efficiency, we recommend allocating more diffusion steps to the higher level.

Quantitative and Qualitative Discussion._Quantitative experiment of the imbalance problem._ In this experiment, we mask the verbs and action names in the motion description to force the model to generate motion only from action specifics. For example, given the motion description "a person walks several steps forward in a straight line.", we would mask "walks". Transformer extracts text features automatically and implicitly. However, it may encourage the model to take shortcuts, such as overemphasizing the action name "walks" at the expense of other important properties. Therefore, when the verbs and action names are masked, the other models, which directly use the transformer

   Methods & FID \(\) & MM-Dist \(\) & Diversity \(\) & MModality \(\) \\  MDM & 5.622 & 7.163 & 8.713 & 3.578 \\ MLD & 3.492 & 5.632 & 8.874 & 3.596 \\ GraphMotion & \(\) & \(\) & \(\) & \(\) \\   

Table 6: **Quantitative experiment of the imbalance problem on the HumanML3D test set.** “\(\)” denotes that higher is better. “\(\)” denotes that lower is better.

   Methods Compared & Preference Rate \\  GraphMotion vs. MotionDiffuse & 64.10\% \\ GraphMotion vs. MLD & 56.41\% \\ GraphMotion vs. Ground Truth & 48.72\% \\   

Table 7: **User studies for quantitative comparison. We show the preference rate of GraphMotion over the compared model.**

Figure 5: **Qualitative analysis of refining motion results. The darker colors indicate the later in time. The trajectory of the human body is indicated by an arrow. The trajectory associated with the modified edge is highlighted in red, and other parts are identified in blue.**

to extract text features, fail to generate motion well. By contrast, the hierarchical semantic graph explicitly extracts the action specifics. The explicit factorization of the language embedding space facilitates a comprehensive understanding of motion description. It allows the model to infer from action specifics such as "several steps forward" and "in a straight line" that the overall motion is "walking forward". As shown in Tab. 6, our method can synthesize motion by relying only on action specifics, while other methods fail to generate motion well. These results indicate that our method avoids the imbalance problem of other methods.

_Human evaluation._ In our evaluation, we randomly selected 39 motion descriptions for the user study. As shown in Tab. 7, GraphMotion is preferred over the other models most of the time.

_Qualitative analysis of refining motion results._ To fine-tune the generated results for more fine-grained control, our method can continuously refine the generated motion by modifying the edge weights of the hierarchical semantic graph. As illustrated in Fig. 5, we can alter the action attributes by manipulating the weights of the edges of the action node and the specific node. For example, by increasing the weights of the edges of "zags" and "to left", the human body will move farther to the left. Moreover, by fine-tuning the weights of the edges of the global motion node and the action node, we can fine-tune the duration of the corresponding action in the whole motion. For example, by enhancing the weights of the edges of the global motion node and "walking", the length of the walk will be increased. In Fig. 6, we provide additional qualitative analysis of refining motion results. Specifically, we perform the additional operations on the hierarchical semantic graphs: (1) masking the node by replacing it with the MASK token; (2) modifying the node; (3) deleting nodes; (4) adding a new node. The qualitative results demonstrate that our approach provides a novel method of refining generated motions, which may have a far-reaching impact on the community.

## 5 Conclusion

In this paper, we focus on improving the controllability of text-driven human motion generation. To provide fine-grained control over motion details, we propose a novel control signal called the hierarchical semantic graph, which consists of three kinds of abstract nodes, namely motions, actions, and specifics. Correspondingly, to leverage the coarse-to-fine topology of hierarchical semantic graphs, we decompose the text-to-motion diffusion process into three semantic levels, which correspond to capturing the overall motion, local actions, and action specifics. Extensive experiments demonstrate that our method achieves better controllability than the existing state-of-the-art methods. More encouragingly, our method can continuously refine the generated motion by modifying the edge weights of hierarchical semantic graphs, which may have a far-reaching impact on the community.

**Acknowledgements.** This work was supported by the National Key R&D Program of China (2022ZD0118101), Nature Science Foundation of China (No.62202014), and Shenzhen Basic Research Program (No.JCYJ20220813151736001).

Figure 6: **Additional qualitative analysis of refining motion results. The qualitative results demonstrate that our approach provides a novel method of refining generated motions.**