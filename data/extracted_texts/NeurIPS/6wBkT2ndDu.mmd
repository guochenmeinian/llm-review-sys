# Weitzman's Rule for Pandora's Box with Correlations

Evangelia Gergatsouli

University of Wisconsin-Madison

evagerg@cs.wisc.edu

&Christos Tzamos

University of Wisconsin-Madison

& University of Athens

tzamos@wisc.edu

###### Abstract

Pandora's Box is a central problem in decision making under uncertainty that can model various real life scenarios. In this problem we are given \(n\) boxes, each with a fixed opening cost, and an unknown value drawn from a known distribution, only revealed if we pay the opening cost. Our goal is to find a strategy for opening boxes to minimize the sum of the value selected and the opening cost paid.

In this work we revisit Pandora's Box when the value distributions are correlated, first studied in Chawla et al. (2020). We show that the optimal algorithm for the independent case, given by Weitzman's rule, directly works for the correlated case. In fact, our algorithm results in significantly improved approximation guarantees compared to the previous work, while also being substantially simpler. We also show how to implement the rule given only sample access to the correlated distribution of values. Specifically, we find that a number of samples that is polynomial in the number of boxes is sufficient for the algorithm to work.

## 1 Introduction

In various minimization problems where uncertainty exists in the input, we are allowed to obtain information to remove this uncertainty by paying an extra price. Our goal is to sequentially decide which piece of information to acquire next, in order to minimize the sum of the search cost and the value of the option we chose.

This family of problems is naturally modeled by Pandora's Box, first formulated by Weitzman (1979) in an economics setting, with multiple application in consumer search, housing markets and job search (see McCall and McCall (2007) for more applications). In this problem where we are given \(n\) boxes, each containing a value drawn from a known distribution and each having a fixed known _opening cost_. We can only see the exact value realized in a box if we open it and pay the opening cost. Our goal is to minimize the sum of the value we select and the opening costs of the boxes we opened.

In the original work of Weitzman, an optimal solution was proposed when the distributions on the values of the boxes were independent (Weitzman, 1979). This algorithm was based on calculating a _reservation value_ (\(\)) for each box, and then choosing the box with the lowest reservation value to open at every step. Independence, however, is an unrealistic assumption in real life; in a housing market neighboring houses' price are affected the same way, or in a job search setting, candidates might share qualifications that affect them similarly. Wanting to tackle a more realistic setting, Chawla et al. (2020) first studied the problem where the distributions are correlated, and designed an algorithm giving a constant approximation guarantee. This algorithm is quite involved, it requires solving an LP to convert the Pandora's Box instance to a Min Sum Set Cover one, and then solving this instance to obtain an ordering of opening the boxes. Finally, it reduces the problem of deciding when to stop to an online algorithm question corresponding to Ski-Rental.

### Our Contribution

In this work we revisit Pandora's Box with correlations, and provide **simpler**, **learnable** algorithms with **better approximation guarantees**, that directly **generalize** Weitzman's reservation values. More specifically, our results are the following.

* **Generalizing**: we first show how the original reservation values given by Weitzman (1979) can be generalized to work in correlated distributions, thus allowing us to use a version of their initial greedy algorithm.
* **Better approximation**: we give two different variants of our main algorithm, that each uses different updates on the distribution \(\) after every step. 1. _Variant 1: partial updates_. We condition on the algorithm not having stopped yet. 2. _Variant 2: full updates_. We condition on the exact value \(v\) revealed in the box opened. Both variants improve the approximation given by Chawla et al. (2020) from \(9.22\) to \(4.428\) for Variant 1 and to \(5.828\) for Variant 2. It is worth noting that our result for Variant 1 is _almost tight_, since the best possible approximation factor we can obtain is \(4\), implied by Feige (1998). We include more details on the lower bound in Section A.4 of the Appendix.
* **Simplicity**: our algorithms are greedy and only rely on the generalized version of the reservation value, while the algorithms in previous work rely on solving a linear program, and reducing first to Min Sum Set Cover then to Ski-Rental, making them not straightforward to implement. A \(9.22\) approximation was also given in Gergatsouli and Tzamos (2022), which followed the same approach but bypassed the need to reduce to Min Sum Set Cover by directly rounding the linear program via randomized rounding.
* **Learnability**: we show how given sample access to the correlated distribution \(\) we are able to still maintain the approximation guarantees. Specifically, for Variant 1 only \((n,1/,(1/))\) samples are enough to obtain \(4.428+\) approximation with probability at least \(1-\). Variant 2 is however impossible to learn.

Our analysis is enabled by drawing similarities from Pandora's Box to Min Sum Set Cover, which corresponds to the special case of when the values inside the boxes are \(0\) or \(\). For Min Sum Set Cover a simple greedy algorithm was shown to achieve the optimal \(4\)-approximation (Feige et al., 2002). Surprisingly, Weitzman's algorithm can be seen as a direct generalization of that algorithm. Our analysis follows the histogram method introduced in Feige et al. (2002), for bounding the approximation ratio. However, we significantly generalize it to handle values in the boxes and work with tree-histograms required to handle the case with full-updates.

### Related Work

Since Weitzman's initial work (Weitzman, 1979) on Pandora's Box there has been a renewed interest in studying this problem in various settings. Specifically Doval (2018); Beyhaghi and Kleinberg (2019); Beyhaghi and Cai (2023); Fu et al. (2023) study Pandora's Box when we can select a box without paying for it (non-obligatory inspection), in Boodaghians et al. (2020) there are tree or line constraints on the order in which the boxes can be opened. In Chawla et al. (2020, 2021) the distributions on the values inside the boxes are correlated and the goal is to minimize the search and value cost, while finally in Bechtel et al. (2022) the task of searching over boxes is delegated by an agent to a principal, while the agent makes the final choice. The recent work of Chawla et al. (2020) is the first one that explores the correlated distributions variant and gives the first approximation guarantees. The recent survey by Beyhaghi and Cai (2023) summarizes the recent work on Pandora's Box and its variants.

This problem can be seen as being part of the "price of information" literature (Charikar et al., 2000; Gupta and Kumar, 2001; Chen et al., 2015, 2015), where we can remove part of the uncertainty of the problem at hand by paying a price. In this line of work, more recent papers study the structure of approximately optimal rules for combinatorial problems (Goel et al., 2006; Gupta and Nagarajan, 2013; Adamczyk et al., 2016; Gupta et al., 2016, 2017; Singla, 2018; Gupta et al., 2019).

For the special case of Min Sum Set Cover, since the original work of Feige et al. (2002), there has been many follow-ups and generalizations where every set has a requirement of how many elementscontained in it we need to choose (Azar et al., 2009; Bansal et al., 2010; Azar and Gamzu, 2011; Skutella and Williamson, 2011; Im et al., 2014).

Note also that multiple results on problems related to Pandora's box have been published in ML-related conferences, as this is a problem that encompasses both algorithmic and learning aspects (e.g. Esfandiari et al. (2019); Gergatsouli and Tzamos (2022); Bhaskara et al. (2020); Cesa-Bianchi et al. (2021); Guo et al. (2021).

## 2 Preliminaries

In Pandora's Box (\(\)) we are given a set of \(n\) boxes \(\), each with a known opening cost \(c_{b}^{+}\), and a distribution \(\) over a vector of unknown values \(=(v_{1},,v_{n})^{n}_{+}\) inside the boxes. Each box \(b\), once it is opened, reveals the value \(v_{b}\). The algorithm can open boxes sequentially, by paying the opening cost each time, and observe the value instantiated inside the box. The goal of the algorithm is to choose a box of small value, while spending as little cost as possible "opening" boxes. Formally, denoting by \(\) the set of opened boxes, we want to minimize

\[_{v}[_{b}v_{b}+_{b }c_{b}].\]

A _strategy_ for Pandora's Box is an algorithm that in every step decides which is the next box to open and when to stop. We measure the performance of our algorithm using the competitive (or approximation) ratio; a strategy \(\) is \(\)-approximation if \([]\)OPT, where OPT is the optimal online algorithm1

A strategy can pick any open box to select at any time. To model this, we assume without loss of generality that after a box is opened the opening cost becomes \(0\), allowing us to select the value without opening it again. In its full generality, a strategy can make decisions based on every box opened and value seen so far. We call this the _Fully-Adaptive_ (FA) strategy.

Different Benchmarks.As it was initially observed in Chawla et al. (2020), optimizing over the class of fully-adaptive strategies is intractable, therefore we consider the simpler benchmark of _partially-adaptive_ (PA) strategies. In this case, the algorithm has to fix the opening order of the boxes, while the stopping rule can arbitrarily depend on the values revealed.

### Weitzman's Algorithm

When the distributions of values in the boxes are independent, Weitzman (1979) described a greedy algorithm that is also the optimal strategy. In this algorithm, we first calculate an index for every box \(b\), called _reservation value_\(_{b}\), defined as the value that satisfies the following equation

\[_{}[(_{b}-v_{b})^{+} ]=c_{b}, \]

where \((a-b)^{+}=(0,a-b)\). Then, the boxes are ordered by increasing \(_{b}\) and opened until the minimum value revealed is less than the next box in the order. Observe that this is a _partially-adaptive_ strategy.

## 3 Competing with the Partially-Adaptive

We begin by showing how Weitzman's algorithm can be extended to correlated distributions. Our algorithm calculates a reservation value \(\) for every box at each step, and opens the box \(b\) with the minimum \(_{b}\). We stop if the value is less than the reservation value calculated, and proceed in making this box _free_; we can re-open this for no cost, to obtain the value just realized at any later point. The formal statement is shown in Algorithm 1.

```
1:Input: A set of boxes \(\), \( having stopped. On the other hand, for full updates we condition on the exact value that was instantiated in the box opened. Theorem 3.1 gives the approximation guarantees for both versions of this algorithm.

```
Input: Boxes with costs \(c_{i}\), distribution over scenarios \(\).
1 An unknown vector of values \(v\) is drawn repeat
2 Calculate \(_{b}\) for each box \(b\) by solving: \[_{}[(_{b}-v_{b})^ {+}]=c_{b}.\] Open box \(b=_{b}_{b}\) Stop if the value the observed \(V_{b}=v_{b}_{b}\)\(c_{b} 0\)// Box is always open now or can be reopened Update the prior distribution - Variant 1:\(|_{V_{b}>_{b}}\) (partial updates) - Variant 2:\(|_{V_{b}=v_{b}}\) (full updates) until termination;
```

**Algorithm 1**Weitzman's algorithm, for correlated \(\).

**Theorem 3.1**.: _Algorithm 1 is a \(4.428\)-approximation for Variant 1 and \(5.828\)-approximation for Variant 2 of Pandora's Box against the partially-adaptive optimal._

Proof.: We seperately show the two components of this theorem in Theorems 3.2 and 3.3. 

Observe that for independent distributions this algorithm is exactly the same as Weitzman's , since the product prior \(\) remains the same, regardless of the values realized. Therefore, the calculation of the reservation values does not change in every round, and suffices to calculate them only once at the beginning.

ScenariosTo proceed with the analysis of Theorem 3.1, we assume that \(\) is supported on a collection of \(m\) vectors, \((^{s})_{s}\), which we call scenarios, and sometimes abuse notation to say that a scenario is sampled from the distribution \(\). We assume that all scenarios have equal probability. The general case with unequal probabilities follows by creating more copies of the higher probability scenarios until the distribution is uniform.

A scenario is _covered_ when the algorithm decides to stop and choose a value from the opened boxes. For a specific scenario \(s\) we denote by \(c(s)\) the total opening cost paid by an algorithm before this scenario is covered and by \(v(s)\) the value chosen for this scenario.

Reservation ValuesTo analyze Theorem 3.1, we introduce a new way of defining the reservation values of the boxes that is equivalent to (1). For a box \(b\), we have that

\[_{b}=_{A}+_{s A}_{ }[s]v_{b}^{s}}{_{s A}_{} [s]}\]

The equivalence to (1), follows since \(_{b}\) is defined as the root of the expression

\[_{s}[(_{b}-v_{b}^{s}) ^{+}] -c_{b}=_{s}_{}[s ](_{b}-v_{b}^{s})^{+}-c_{b}\] \[=_{A}_{s A}_{}[s](_{b}-v_{b}^{s})-c_{b}.\]

If we divide the above expression by any positive number, the result will not be affected since we require the root of the equation; \(_{b}\) being the root is equivalent to \(_{b}\) being the root of the numerator.

Thus, dividing by \(_{s A}_{}[s]\) we get that \(_{b}\) is also the root of

\[_{A}_{}[s ](_{b}-v_{b}^{s})-c_{b}}{_{s A}_{} [s]}=_{b}-_{A}+_{s A }_{}[s]v_{b}^{s}}{_{s A}_{ }[s]}. \]

This, gives our formula for computing \(_{b}\), which we can further simplify using our assumption that all scenarios have equal probability. In this case, \(_{}[s]=1/||\) which implies that

\[_{b}=_{A}||+_{s A} v_{b}^{s}}{|A|}. \]

### Conditioning on \(V_{b}>_{b}\)

We start by describing the simpler variant of our algorithm where after opening each box we update the distribution by conditioning on the event \(V_{b}>_{b}\). This algorithm is _partially adaptive_, since the order for each scenario does not depend on the actual value that is realized every time. At every step the algorithm will either stop or continue opening boxes conditioned on the event "We have not stopped yet" which does not differentiate among the surviving scenarios.

**Theorem 3.2**.: _Algorithm 1 is a \(4.428\)-approximation for Pandora's Box against the partially-adaptive optimal, when conditioning on \(V_{b}>_{b}\)._

In this section we show a simpler proof for Theorem 3.2 that gives a \(3+2 5.828\)-approximation. The full proof for the \(4.428\)-approximation is given in section A.2 of the Appendix. Using the equivalent definition of the reservation value (Equation (3)) we can rewrite Algorithm 1 as follows.

```
Input: Boxes with costs \(c_{i}\), set of scenarios \(\).
1\(t 0\)
2\(R_{0}\) the set of scenarios still uncovered
3while\(R_{t}\)do
4 Let \(_{t}_{b,A R_{t}}|R_{t}|+ _{s A}v_{b}^{s}}{|A|}\)
5 Let \(b_{t}\) and \(A_{t}\) be the box and the set of scenarios that achieve the minimum
6 Open box \(b_{t}\) and pay \(c_{b_{t}}\)
7 Stop and choose the value \(v_{b_{t}}\) at box \(b_{t}\) if it is less than \(_{t}\) (see also Fact 3.2.1)
8 Set \(c_{b_{t}} 0\)
9\(R_{t} R_{t} A_{t}\)
10\(t t+1\)
11 end while
```

**Algorithm 2**Weitzman's rule for Partial Updates

Structure of the solution.An important property to note is that by the equivalent definition of the reservation value (3) the set of scenarios that stop at each step are the ones that give a value at most \(\) for the box opened, as we formally state in the following fact.

**Fact 3.2.1**.: _The value at box \(b_{t}\) is less than \(_{t}\) if and only if \(s A_{t}\)._

In equation (8) the set \(A_{t}\) that maximizes the expression contains all the scenarios with value at most \(_{b}\) for the box \(b\). Therefore, the set \(A_{t}\) are exactly the scenarios covered at each step \(t\) of the algorithm, and can be removed from consideration.

Before showing our result, observe that this algorithm is partially adaptive; the order of the boxes does not depend on the scenario realized. This holds since we only condition on "not having stopped" (i.e. \(_{V_{b}>_{b}}\)) and therefore each scenario either stops or uses the same updated prior as all other surviving scenarios to calculate the next reservation values. If we were to draw our solution, it would look like a line, (see also Figure 2 in Appendix A.2), which as we observe in Section 3.2 differs from Variant 2.

Moving on to show the proof, we first start by giving a bound on the cost of the algorithm. The cost can be broken down into opening cost plus the value obtained. Since at any time \(t\), all remaining

[MISSING_PAGE_FAIL:6]

\(b B_{L}\) and \(A=L_{b}\), we obtain \(_{s}|L_{b}| c_{b}|R_{t}|+_{s L_{b}}v_{s}^{}\), and by summing up the inequalities for all \(b B_{L}\) we get

\[_{s}|_{b B_{L}}c_{b}+_{s L}v_{s}^{}}{|L|}|c^{*}+_{s L}v_{s}^{}}{|L|} }{}+v_{s}^{}}{|L|} \]

where for the second inequality we used that the cost for covering the scenarios in \(L\) is at most \(c^{*}\) by construction, and in the last inequality that \(|L|=|R_{t}|/()\). We consider each term above separately, to show that the point \(p\) is within the histograms.

Bounding the opening cost.By the construction of \(c^{*}\), the point in the \(_{o}\) histogram that has cost at least \(c^{*}\) is at distance at least \((1-)|R_{t}|\) from the right hand side. This means that in the rescaled histogram, the point that has cost at least \(c^{*}/()\) is at distance at least \((1-)|R_{t}|/_{o}\) from the right hand side.

On the other hand, in the ALG histogram the distance of \(p\) from the right edge of the histogram is at most \(|R_{t}|\), therefore for the point \(p\) to be inside the \(_{o}\) histogram we require

\[_{o} 1-. \]

Observe that throughout the proof we did not use the fact that we change the opening cost to \(0\), therefore the bound on our cost works even if we re-pay the boxes that are reopened.

The fact that the opening cost becomes \(0\) is not directly used in the analysis (i.e. inequalities (4) and (5) ). Our analysis gives an upper bound on the cost of the algorithm, even if the algorithm never changes the cost of an opened box to \(0\). That is the reason in (4) and (5) the cost appears unchanged but the analysis still works for the algorithm since we just want an upper bound (and if we changed the cost to 0 this would only lower the cost of the algorithm).

Bounding the values cost.By the construction of \(v^{*}\), the point in the \(_{v}\) histogram that has value \(v^{*}\) is at distance at least \(|R_{t}|(1-)\) from the right hand side. This means that in the rescaled histogram, the point that has value at least \(v^{*}\) is at distance at least \((1-)|R_{t}|/_{v}\) from the right hand side.

On the other hand, in the ALG histogram the distance of \(p\) from the right edge of the histogram is at most \(|R_{t}|\), therefore for the point \(p\) to be inside the \(_{o}\) histogram we require

\[_{v}(1-). \]

We optimize the constants \(_{o},_{v},,\) by ensuring that inequalities (6) and (7) hold. We set \(_{o}=1-\) and \(_{v}=(1-)\), and obtain that \(_{o}/((1-))+_{v} /((1-))\). Requiring these to be equal we get \(=1/(2-)\), which is minimized for \(=1/\) and \(=2-\) for a value of \(3+2\).

### Conditioning on \(V_{b}=v\)

In this section we switch gears to our second variant of Algorithm 1, where in each step we update the prior \(\) conditioning on the event \(V_{b}=v\). We state our result in Theorem 3.3. In this case, the conditioning on \(\) implies that the algorithm at every step removes the scenarios that are _inconsistent_ with the value realized. In order to understand better the differences of the two variants and their conditioning we included an example and a discussion in section A.1 of the Appendix.

**Theorem 3.3**.: _Algorithm 1 is a \(3+2 5.828\)-approximation for Pandora's Box against the partially-adaptive optimal, when conditioning on \(V_{b}=v\)._

The main challenge was that the algorithm's solution is now a tree with respect to scenarios instead of a line as in the case of \(|_{V_{b}>_{b}}\). Specifically, in the \(D|_{V_{b}>_{b}}\) variant at every step all scenarios that had \(V_{b}_{b}\) were covered and removed from consideration. However in the \(D|_{V_{b}=v}\) variant the remaining scenarios are split into different cases, based on the realization of \(V\), as shown in the example of Figure 4, which is deferred to Section A.3 of the Appendix due to space constraints.

This results into the ALG histogram not being well defined, since there is no unique order of covering the scenarios. We overcome this by generalizing the histogram approach to trees.

Proof of Theorem 3.3.: The proof follows similar steps to that of Theorem 3.2, thus we only highlight the differences. The algorithm is presented below, the only change is line 5 where we remove the inconsistent with the value revealed scenarios, which also leads to our solution branching out for different scenarios and forming a tree.

```
0: Boxes with costs \(c_{i}\), set of scenarios \(\).
1 Define a root node \(u\) corresponding to the set \(\)
2\(R_{u}\) the set of scenarios still uncovered
3while\(R_{u}\)do
4 Let \(_{u}_{b,A R_{u}}|R_{u}|+ _{u A}v_{b}^{s}}{|A|}\)
5 Let \(b_{u}\) and \(A_{u}\) be the box and the set of scenarios that achieve the minimum
6 Open box \(b_{u}\) paying \(c_{b_{u}}\) and observe value \(v\)
7 Stop and choose the value at box \(b_{u}\) if it is less than \(_{u}\): this holds iff\(s A_{u}\)
8 Set \(c_{b_{u}} 0\)
9 Let \(u^{}\) be a vertex corresponding to the set of consistent scenarios with \(R_{u^{}} R_{u}(A_{u}\{s R_{u}:v_{b_{u}}^ {s} v\})\)// Remove inconsistent scenarios
10 Set \(u u^{}\)
11 end while
```

**Algorithm 3**Weitzman's rule for Full Updates

Bounding the opening costConsider the tree \(\) of ALG where at every node \(u\) a set \(A_{u}\) of scenarios is covered. We associate this tree with node weights, where at every node \(u\), we assign \(|A_{u}|\) weights \((_{u},...,_{u})\). Denote, the weighted tree by \(_{}\). As before, the total cost of ALG is equal to the sum of the weights of the tree.

We now consider two alternative ways of assigning weights to the the nodes, forming trees \(_{_{o}}\), \(_{_{v}}\) using the following process.

* \(_{_{o}}\). At every node \(u\) we create a vector of weights \(_{u}^{_{o}}=(c_{s}^{})_{s A_{u}}\) where each \(c_{s}^{}\) is the opening cost that scenario \(s A_{u}\) has in the optimal solution.
* \(_{_{v}}\). At every node \(u\) we create a vector of weights \(_{u}^{_{v}}=(v_{s}^{})_{s A_{u}}\) where each \(v_{s}^{}\) is the value the optimal uses to cover scenario \(s A_{u}\).

We denote by \((_{})\) the sum of all weights in every node of the tree \(\). We have that \(()\) is equal to the total cost of ALG, while \((_{_{o}})\) and \((_{_{v}})\) is equal to the optimal opening cost OPT\({}_{o}\) and optimal value OPT\({}_{v}\) respectively. Intuitively, the weighted trees correspond to the histograms in the previous analysis of Theorem 3.2.

We want to relate the cost of ALG, to that of \(_{_{o}}\) and \(_{_{v}}\). To do this, we define an operation similar to histogram scaling, which replaces the weights of every node \(u\) in a tree with the top \(\)-percentile of the weights in the subtree rooted at \(u\). As the following lemma shows, this changes the cost of a tree by a bounded multiplicative factor.

**Lemma 3.3.1**.: _Let \(\) be a tree with a vector of weights \(_{u}\) at each node \(u\), and let \(^{()}\) be the tree we get when we substitute the weights of every node with the top \(\)-percentile of all the weights in the subtree of \(\) rooted at \(u\). Then_

\[(^{()})().\]

We defer the proof of Lemma 3.3.1 to Section A.3 of the Appendix. To complete the proof of Theorem 3.3, and bound \((_{})\), we show as before that the weights at every node \(u\), are bounded by the weights of \(_{_{o}}^{(1-)}\) scaled by \(\) plus the weights of \(_{_{v}}^{((1-))}\), for the constants \(,(0,1)\) chosen in the proof of Theorem 3.2. This implies that

\[(_{_{o}}) (_{_{o}}^{(1 -)})+(_{_{v}}^{((1-))})\]\[ )}(_{_{o}})+)}(_{_{o}})\]

which gives \( 5.828\) OPT for the choice of \(\) and \(\). The details of the proof are similar to the one of Theorem 3.1, and are deferred to section A.3 of the Appendix.

Note on the approximation factors.Observe that Variant 2, where we condition on \(V_{b}=v\) has a worse approximation factor than Variant 1 where we only condition on \(V_{b}>_{b}\). Intuitively someone might expect that with more information the approximation factor will improve. However, it is challenging to argue about this formally. It is also plausible that such monotonicity may not hold as more information might lead the greedy algorithm to make wrong decisions. Instead of making any such claims, we analyze this case directly by showing that our proof approach extends to the full update variant with a generalization of the histogram method to work on trees. Our technique for improving the approximation for the partial updates variant could not be generalized however and thus we only obtain the worse approximation guarantee.

## 4 Learning from Samples

In this section we show that our algorithm also works when we are only given sample access to the correlated distribution \(\).

We will mainly focus on the first variant with partial updates \(|_{V>v}\). The second variant with full Bayesian updates \(|_{V=v}\) requires full knowledge of the underlying distribution and can only work with sample access if one can learn the full distribution. To see this consider for example an instance where the values are drawn uniformly from \(^{d}\). No matter how many samples one draws, it is impossible to know the conditional distribution \(|_{V=v}\) after opening the first box for fresh samples \(v\), and the Bayesian update is not well defined4.

Variant 1 does not face this problem and can be learned from samples if the costs of the boxes are polynomially bounded by \(n\), i.e. if there is a constant \(c>0\) such that for all \(b\), \(c_{b}[1,n^{c}]\). If the weights are unbounded, it is impossible to get a good approximation with few samples. To see this consider the following instance. Box 1 has cost \(1/H 0\), while every other box has cost \(H\) for a very large \(H>0\). Now consider a distribution where with probability \(1- 1\), the value in the first box is \(0\), and with probability \(1/H\) is \(+\). In this case, with a small number of samples we never observe any scenario where \(v_{1} 0\) and believe the overall cost is near \(0\). However, the true cost is at least \(H 1/H\) and is determined by how the order of boxes is chosen when the scenario has \(v_{1} 0\). Without any such samples it is impossible to pick a good order.

Therefore, we proceed to analyze Variant 1 with \(|_{V>}\) in the case when the box costs are similar. We show that polynomial, in the number of boxes, samples suffice to obtain an approximately-optimal algorithm, as we formally state in the following theorem. We present the case where all boxes have cost 1 but the case where the costs are polynomially bounded easily follows.

**Theorem 4.1**.: _Consider an instance of Pandora's Box with opening costs equal to 1. For any given parameters \(,>0\), using \(m=poly(n,1/,(1/))\) samples from \(\), Algorithm 1 (Variant 1) obtains a \(4.428+\) approximation policy against the partially-adaptive optimal, with probability at least \(1-\)._

To prove the theorem, we first note that variant 1 of Algorithm 1 takes a surprisingly simple form, which we call a threshold policy. It can be described by a permutation \(\) of visiting the boxes and a vector of thresholds \(\) that indicate when to stop. The threshold for every box corresponds to the reservation value the first time the box is opened. To analyze the sample complexity of Algorithm 1, we study a broader class of algorithms parameterized by a permutation and vector of thresholds given in Algorithm 4.

Our goal now is to show that polynomially many samples from the distribution \(\) suffice to learn good parameters for Algorithm 4. We first show a Lemma that bounds the cost of the algorithm calculated in the empirical \(}\) instead of the original \(\) (Lemma 4.1.1), and a Lemma 4.1.2 that shows how capping the reservation values by \(n/\) can also be done with negligible cost.

**Lemma 4.1.1**.: _Let \(,>0\) and let \(^{}\) be the empirical distribution obtained from \((n,1/,\)\((1/))\) samples from \(\). Then, with probability \(1-\), it holds that_

\[|_{}[(,)-_{b}v_{ b}]-_{D}[(,)-_{b}v_{b} ]|\]

_for any permutation \(\) and any vector of thresholds \([0,]^{n}\)_

We defer the proof of Lemmas 4.1.1, 4.1.2 and that of Theorem 4.1.1 to Section A.5 of the Appendix.

**Lemma 4.1.2**.: _Let \(\) be any distribution of values. Let \(>0\) and consider a permutation \(\) and thresholds \(\). Moreover, let \(^{}\) be the thresholds capped to \(n/\), i.e. setting \(^{}_{b}=\{_{b},n/\}\) for all boxes \(b\). Then,_

\[_{v D}[(,^{})](1+ )_{v D}[(,)].\]

Note on Continuous vs Discrete Distributions.The results of Section 4 apply for general distributions (discrete or continuous) and show that the partial updates variant leads to good approximation when run on the empirical distribution obtained just with polynomially many samples. In contrast, the full updates variant requires a complete description of the distribution. However, as the approximation factor does not depend on the support size, It can also apply even for continuous distributions with arbitrary large support by taking a limit over a very fine discretization

## 5 Conclusion

We present a summary of our results with a comparison to previous work on Table 1. Our main contribution was to improve the approximation factor for Pandora's Box with correlations given by Chawla et al. (2020), while also greatly simplifying their approach. Our algorithm also directly extends the independent case algorithm, giving us a unified way to solve this problem. An interesting open question is to try and improve their results for more complex combinatorial constraints, like selecting \(k\) boxes (instead of one) or for selecting a basis of size \(k\), when the boxes are part of a matroid.

Observe also that the more natural Variant 2 seems worse than Variant 1 even though the algorithm has more accurate information through the update of the prior. Intuitively we would expect a better factor, however since the algorithm is greedy approximation, and not the optimal, the factor may not necessarily be monotone on the amount of information given. We leave as an open problem whether our analysis in Variant 2 is tight or this greedy algorithm cannot perform better under full information.

  & **Approx. Factor** & **Learnable from Samples** \\  Algorithm of Chawla et al. (2020) & \(9.22\) & Yes \\ 
**Variant 1** (\(D_{V_{b}>_{b}}\)) & \(\) (Thm 3.2) & **Yes** (Thm 4.1 ) \\ 
**Variant 2** (\(D_{V_{b}=v}\)) & \(\) (Thm 3.3) & **No** (Sec. 4) \\ 

Table 1: Summary of our results (in bold) and comparison to previous work.