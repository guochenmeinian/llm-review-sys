# SPARKLE: A Unified Single-Loop Primal-Dual Framework for Decentralized Bilevel Optimization

Shuchen Zhu

Peking University

shuchenzhu@stu.pku.edu.cn

&Booo Kong

Peking University

kongboao@stu.pku.edu.cn

&Songtao Lu

IBM Research

songtao@ibm.com

&Xinmeng Huang

University of Pennsylvania

xinmengh@sas.upenn.edu

&Kun Yuan

Peking University

kunyuan@pku.edu.cn

Equal Contribution

Corresponding Author: Kun Yuan. Kun Yuan is also affiliated with National Engineering Laboratory for Big Data Analytics and Applications, and AI for Science Institute, Beijing, China.

###### Abstract

This paper studies decentralized bilevel optimization, in which multiple agents collaborate to solve problems involving nested optimization structures with neighborhood communications. Most existing literature primarily utilizes gradient tracking to mitigate the influence of data heterogeneity, without exploring other well-known heterogeneity-correction techniques such as EXTRA or Exact Diffusion. Additionally, these studies often employ identical decentralized strategies for both upper- and lower-level problems, neglecting to leverage distinct mechanisms across different levels. To address these limitations, this paper proposes **SPARKLE**, a unified **S**ingle-loop **P**imal-dual **A**go**R**ithm framework for decentra**L**ized bil**E**vel optimization. SPARKLE offers the flexibility to incorporate various heterogeneity-correction strategies into the algorithm. Moreover, SPARKLE allows for different strategies to solve upper- and lower-level problems. We present a unified convergence analysis for SPARKLE, applicable to all its variants, with state-of-the-art convergence rates compared to existing decentralized bilevel algorithms. Our results further reveal that EXTRA and Exact Diffusion are more suitable for decentralized bilevel optimization, and using mixed strategies in bilevel algorithms brings more benefits than relying solely on gradient tracking.

## 1 Introduction

Numerous modern machine learning tasks, such as reinforcement learning , meta-learning , adversarial learning , hyper-parameter optimization , and imitation learning , entail nested optimization formulations that extend beyond the traditional single-level paradigm. For instance, hyper-parameter optimization aims to identify the optimal hyper-parameters for a specific learning task in the upper level by minimizing the validation loss, achieved through training models in the lower-level process. This nested optimization structure has spurred significant attention towards Stochastic Bilevel Optimization (SBO). Since the size of data samples involved in bilevel problems has become increasingly large, this paper investigates decentralized algorithms over a network of \(n\) agents (nodes) that collaborate to solve the following distributed bilevel optimization problem:

\[_{x^{p}}\ \ (x)=f(x,y^{}(x)):=_{i=1}^{ n}f_{i}(x,y^{}(x)), 56.905512pt\] (1a)\[ y^{}(x)=*{argmin}_{y^{q}} g(x,y):=_{i=1}^{n}g_{i}(x,y)}.\] (1b)

In this formulation, each agent \(i\) holds a private upper-level objective function \(f_{i}:^{p}^{q}\) and a strongly convex lower-level objective function \(g_{i}:^{p}^{q}\) defined as:

\[f_{i}(x,y)=_{_{f_{i}}}[F_{i}(x,y;)], g_ {i}(x,y)=_{_{g_{i}}}[G_{i}(x,y;)],\] (2)

where \(_{f_{i}}\) and \(_{g_{i}}\) represent the local data distributions at agent \(i\). This paper does not make any assumptions about these data distributions, implying there might be data heterogeneity across agents.

Linear speedup and transient iteration complexity.A decentralized stochastic algorithm achieves _linear speedup_ if its iteration complexity decreases linearly with the network size \(n\). Additionally, the _transient iteration complexity_ refers to the number of transient iterations a decentralized algorithm must undergo to achieve the asymptotic linear speedup stage. The fewer the transient iterations, the faster the algorithm can achieve linear speedup. This paper aims to develop decentralized stochastic bilevel algorithms that can achieve linear speedup with as few transient iterations as possible.

**Limitations in previous works.** A significant challenge in decentralized bilevel optimization lies in accurately estimating the hyper-gradient \((x)\) through neighborhood communications. Several studies have emerged to effectively address this challenge, such as those by [9; 33; 52; 16; 21; 40; 57; 29]. However, existing works suffer from several critical limitations:

* **Stringent assumptions and inadequate convergence analysis.** Many existing studies rely on stringent assumptions to ensure convergence. For instance, references [9; 10; 33; 21; 52] assume bounded gradients, while reference  assumes bounded data heterogeneity (also known as bounded gradient dissimilarity). These restrictive assumptions do not arise in centralized bilevel optimization, implying their potential unnecessity. Moreover, some of these works suffer from inadequate convergence analysis, unable to clarify the transient iteration complexity [9; 33; 10] or provide a sharp estimation of the influence of network topologies [52; 21].
* **Limited exploration of various heterogeneity-correction techniques.** Several concurrent studies [16; 57; 40] have utilized Gradient Tracking (GT) [50; 13; 38] to remove the assumption of bounded data heterogeneity. However, it remains uncertain whether GT is the most suitable mechanism for decentralized bilevel optimization. Many other techniques are also useful for addressing data heterogeneity in single-level decentralized optimization, such as EXTRA  and Exact-Diffusion (ED) [56; 30; 54] (which is also known as D\({}^{2}\)). Even within GT, there are variants including Adapt-Then-Combine GT (ATC-GT) , non-ATC-GT , and semi-ATC-GT . It remains unexplored whether these techniques for mitigating data heterogeneity converge and even outperform GT when employed in decentralized bilevel algorithms.
* **Unknown effects of employing different upper- and lower-level update strategies.** In bilevel optimization, the challenges in solving the upper- and lower-level problems differ substantially. For instance, the upper-level problem (1a) is non-convex, whereas the lower-level problem (1b) is strongly convex. Moreover, estimating the gradient at the lower level is considerably simpler compared to estimating the hyper-gradient at the upper level. Understanding the roles of updates at each level is crucial to develop more efficient algorithms. However, most existing algorithms employ the same decentralized methods to solve both the upper- and lower-level problems. For example, references [9; 52; 29] utilize decentralized gradient descent (DGD) for updates at both levels while [57; 40; 16] leverage GT, overlooking the potential advantages of mixed strategies.

To address these limitations, several critical questions naturally arise: Should each heterogeneity-correction mechanism listed in [50; 13; 38; 56; 30; 54; 46] be explored one-by-one? Should we consider combining any two of these techniques to update the upper and lower-level problems, respectively? It is evident that examining each individual heterogeneity-correction technique, and even exploring their combinations, would involve an unbearable amount of effort.

**Main results and contributions.** This paper addresses all the aforementioned limitations without exhaustively exploring all heterogeneity-correction techniques. Our main results are as follows.

* **A unified decentralized bilevel framework.** To avoid examining each single heterogeneity-correction technique, we propose **SPARKLE**, a unified Single-loop Primal-dual \(\)ithmframework for decent_Lized_ bilEvel optimization. By specifying certain hyper-parameters, SPARKLE can be tailored to SPARKLE-EXTRA, SPARKLE-ED, and SPARKLE-GT, which employ EXTRA , ED , or multiple GT variants , respectively, to facilitate the upper and lower-level problems. Additionally, SPARKLE is the _first_ algorithm enabling distinct updating strategies across different levels; for example, one can utilize GT in the upper-level but ED in the lower-level, resulting in a brand new SPARKLE-GT-ED algorithm.
* **A unified and sharp analysis of various heterogeneity-correction schemes.** We provide a unified convergence analysis for SPARKLE, which immediately applies to all SPARKLE variants with distinct heterogeneity-correction techniques. The analysis does not require restrictive assumptions such as gradient boundedness used in  or data-heterogeneity bounded used in . Moreover, our analysis demonstrates the provable superiority of SPARKLE compared to existing algorithms, as evidenced by the convergence rates listed in Table 1. Most importantly, our analysis shows that both SPARKLE-EXTRA and SPARKLE-ED outperform SPARKLE-GT (see Table 1), implying that _GT is not the best_ scheme for decentralized bilevel optimization.
* **Mixing strategies outperform employing GT alone.** We demonstrate how optimization at different levels affects convergence rates. Our theoretical analysis suggests that the updating strategy at _the lower level is crucial in determining the overall performance_ in decentralized bilevel algorithms. Building upon this insight, we establish that incorporating the ED or EXTRA strategy in the lower-level update phase leads to better transient iteration complexity than relying solely on the GT mechanism in both levels as proposed in , see Table 2 for more details.
* **Comparable performance with single-level algorithms.** We elucidate the comparison between bilevel and single-level stochastic decentralized optimization. On one hand, we demonstrate that the convergence performance of all our proposed algorithms is not inferior to their single-level counterparts (see the bottom part in Table 1). On the other hand, by considering specific lower-level loss functions, our bilevel results directly yield the non-asymptotic convergence of corresponding

  
**Algorithms** & **Assumption** & **A. Rate.** & \({}^{}\) & **A. Comp.** & \({}^{}\) & **A. Comm.** & \({}^{}\) & **Tran. Iter.** & \({}^{}\) & **Loopless** \\  DSBO  & LC & \(}\) & \(}(p(z)+)+ )}\) & N.A. & No \\ MA-DSBO  & LC & \(}\) & \(}()\) & \(}()+\) & N.A. & No \\ SLAM  & LC & \(}\) & \(}()\) & \(}\) & N.A. & No \\ MDBO  & BG & \(}\) & \(}()\) & \(}\) & \(}{}\) & No \\ Gossip DSBO  & BG & \(}\) & \(}()\) & \(}()+}\) & \(}{(1-^{3})}\) & No \\ LoPA \({}^{*}\) & BGD & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & No \\ D-SOBA  & BGD & \(}\) & \(}\) & \(}\) & \(}\) & \(\{}{(1-^{3})^{3}}}{(1-^{3}) ^{3}}\}\) & Yes \\ 
**SPARKLE-GT (ours)** & **None** & \(}\) & \(}\) & \(}{}\) & \(}{}\) & **Yes** \\ 
**SPARKLE-EXTRA (ours)** & **None** & \(}\) & \(}\) & \(}{}\) & \(}{}\) & **Yes** \\ 
**SPARKLE-ED (ours)** & **None** & \(}\) & \(}\) & \(}{}\) & \(}{}\) & **Yes** \\  Single-level GT  & None & \(}\) & \(}\) & \(}\) & \(}\) & \(\{}{(1-^{3})^{3}})^ {3}}\}\) & Yes \\ Single-level EXTRA  & None & \(}\) & \(}\) & \(}\) & \(}{(1-^{3})^{3}}\) & Yes \\ Single-level ED  & None & \(}\) & \(}\) & \(}\) & \(}{(1-^{3})^{3}}\) & Yes \\    \({}^{}\) The convergence rate when \(K\) (smaller is better).

\({}^{}\) The number of gradient/Jacobian/Hessian evaluations per agent to achieve \(\)-accuracy when \( 0\) (smaller is better).

\({}^{}\) The communication costs per agent to achieve \(\)-stationarity when \( 0\) (smaller is better).

\({}^{}\) The transient iteration complexity to achieve linear speedup (smaller is better). \({}^{}\) N.A." means that the algorithm cannot achieve linear speedup or the transient time cannot be accessed from existing convergence analysis.

\({}^{}\) Additional assumptions beyond Assumption 1.

\({}^{*}\) LoPA solves the personalized problem, where the lower-level objectives are local to agents.

\({}^{}\)\(a>0\) measures the relative sparsity of the mixing weights \(_{},_{},_{}\), which can be very small in certain cases. Here \(1-\) in **Tran. Iter.** denotes the smallest spectral gap of \(_{},_{},_{}\). See more discussions in Appendix C.2.3.

Table 1: Comparison between different decentralized stochastic bilevel algorithms. \(K\) denotes the number of (upper-level) iterations; \(1-\) denotes the spectral gap of the mixing matrix (see Assumption 2); \(b^{2}\) bounds the gradient dissimilarity; \(\) is the target stationarity such that \(_{k=0}^{K-1}\|(x^{k})\|^{2}/K<\); \(p\) and \(q\) are the dimensions of the upper- and lower-level variables, reflecting per-round communication costs. Assumptions of bounded gradient, Lipschitz continuity, and bounded gradient dissimilarity are abbreviated as BG, LC, and BGD, respectively. We also list the best-known results of single-level GT, EXTRA, and ED at the bottom.

single-level algorithms. This is the _first_ result demonstrating bilevel optimization essentially subsumes the convergence of the single-level optimization.

Our main results are listed in Table 1. All SPARKLE variants achieve the state-of-the-art asymptotic rate, asymptotic gradient complexity, asymptotic communication cost, and transient iteration complexity under more relaxed assumptions compared to existing methods.

**Related works.** A significant challenge in decentralized bilevel optimization is accurately estimating the hyper-gradient \((x)\), necessitating solving global lower-level problems and estimating Hessian inversion. To this end, various decentralized techniques have been applied in bilevel optimization, including Neumann series in , JHIP oracle in , HIGP oracle in , and augmented Lagrangian-based communication in . Additionally, reference  proposes a single-loop algorithm utilizing decentralized SOBA. To enhance algorithmic robustness against data heterogeneity, recent studies have employed Gradient Tracking (GT) in both lower- and upper-level optimization. However, existing works built upon GT suffer from several limitations. Results of [16; 9] concentrate solely on deterministic cases, while reference  addresses personalized problems in the lower-level, which do not require achieving global consensus in the lower-level problem. Moreover, [9; 10] introduce computationally expensive inner loops for GT steps. None of these works can establish smaller transient iteration complexity than D-SOBA for decentralized SBO, even though the latter algorithm employs no heterogeneity-correction technique.

The unified framework for single-level decentralized optimization has been extensively studied in the literature. References [1; 49; 26] propose frameworks for decentralized composite optimization in deterministic settings, while  investigates a framework under stochastic settings. However, none of these works can be directly applied to decentralized bilevel algorithms. Several studies [21; 57] utilize variance reduction techniques to accelerate the convergence of stochastic decentralized bilevel algorithms. Our proposed SPARKLE framework is orthogonal to variance reduction; it can also incorporate variance-reduced gradient estimation to achieve improved convergence rates. More relevant works on decentralized optimization and bilevel optimization are discussed in Appendix A.

**Notations.** We use lowercase letters to represent vectors and uppercase letters to represent matrices. We introduce \(\{x_{1},...,x_{n}\}:=[x_{1}^{},...,x_{n}^{}]^{} ^{p}\) for brevity. Variables with overbar denote the average over all agents. For example, \(^{k}=_{i=1}^{n}x_{i}^{k}/n\). We denote \(=A-_{n}_{n}^{}\) for matrix \(A^{n n}\), where \(_{n}^{n}\) denotes the \(n\)-dimensional vector with all entries being one. For a function \(f(x,y):^{p}^{q}\), we use \(_{1}f(x,y)^{p}\), \(_{2}f(x,y)^{q}\) to represent its partial gradients with respect to \(x\) and \(y\), respectively. Similarly, \(_{12}f(x,y)^{p q}\), \(_{22}f(x,y)^{q q}\) represent the corresponding Jacobian and Hessian matrix. We use the notation \(\) to denote inequalities that hold up to constants related to the initialization of algorithms and smoothness constants.

## 2 SPARKLE: A unified framework for decentralized bilevel optimization

This section develops SPARKLE, a unified framework for decentralized bilevel optimization, and discusses its numerous variants by specifying certain hyper-parameters.

### Three pillar subproblems in decentralized bilevel optimization.

When solving the upper-level problem (1a), it is critical to obtain the hyper-gradient \((x)\), which can be expressed as 

\[(x)=_{1}f(x,y^{}(x))-_{12}^{2}g(x,y^{}(x)) [_{22}^{2}g(x,y^{}(x))]^{-1}_{2}f(x,y^{}(x)).\] (3)

 
**lowerupper** & **ED** & **EXTRA** & **GT** \\ 
**ED** & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) \\ 
**EXTRA** & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) \\ 
**GT** & \(\{}{(1-)^{2}},}\}\) & \(\{}{(1-)^{2}},}\}\) & \(\{}{(1-)^{2}},}\}\) \\  

Table 2: The transient iteration complexity of SPARKLE with mixed updating strategies at various levels. The smaller the transient iteration complexity is, the faster the algorithm will achieve its linear speedup stage. The first row and column respectively indicate the updating strategy for the upper- and lower-level problems. Please refer to Appendix B.3 for more implementation details and Appendix C.2.4 for proofs.

Evaluating this hyper-gradient is computationally expensive due to the inversion of the Hessian matrix. This evaluation becomes even more challenging over a decentralized network of collaborative agents. First, the inverse of the Hessian matrix cannot be obtained by simply averaging the local Hessian inverses due to \([_{i=1}^{n}_{22}^{2}g_{i}(x,y^{}(x))]^{-1} {n}_{i=1}^{n}[_{22}^{2}g_{i}(x,y^{}(x))]^{-1}\). Second, the global averaging operation cannot be realized through decentralized communication. To overcome these challenges, one can introduce an auxiliary variable \(z^{}(x)\!:=[_{22}^{2}g(x,y^{}(x))]^{-1}_{2}f(x,y^{} (x))\), which is the solution to a quadratic problem

\[z^{}(x)=*{argmin}_{z^{q}}\{z^{ }_{22}^{2}g(x,y^{}(x))z-z^{}_{2}f(x, y^{}(x))\}.\] (4)

Once \(z^{}(x)\) is derived by solving (4), we can substitute it into (3) to achieve \((x)\).

Following this idea, solving the distributed bilevel optimization problem (1) essentially involves solving three subproblems, where \(h_{i}(x,y^{}(x),z):=z^{}_{22}^{2}g_{i}(x,y^{}(x ))z-z^{}_{2}f_{i}(x,y^{}(x))\),

\[x^{} =*{argmin}_{x^{p}}_{i=1} ^{n}f_{i}(x,y^{}(x)), \] (5a) \[y^{}(x) =*{argmin}_{y^{q}}_{i=1} ^{n}g_{i}(x,y), \] (5b) \[z^{}(x) =*{argmin}_{z^{q}}_{i=1} ^{n}h_{i}(x,y^{}(x),z). \] (5c)

Given the variable \(x\), one can achieve \(y^{}(x)\) by solving the lower-level problem in (5b). With \(y^{}(x)\) determined, \(z^{}(x)\) can be obtained by solving the auxiliary-level problem in (5c). Subsequently, with \(z^{}(x)\) available, one can directly compute the hyper-gradient and solve the upper-level problem in (5a) using gradient descent. This constitutes the primary methodology to solve problem (1).

A bilevel algorithm essentially solves three subproblems listed in (5), each formulated as a single-level decentralized optimization problem. Nevertheless, primary approaches may suffer from nested loops in algorithmic development. A few recent studies [12; 11; 57; 29] propose to solve each problem in (5a)-(5c) approximately with _one single_ iteration, leading to practical single-loop bilevel algorithms. For example, applying a D-SGD step  to each of (5a)-(5c) yields the D-SOBA method , while further leveraging the GT technique leads to decentralized bilevel methods in [9; 16; 57; 21].

However, it is less explored whether numerous other heterogeneity-correction techniques [50; 13; 38; 56; 30; 54; 46] beyond GT can be incorporated into algorithmic design to achieve even better performance in bilevel optimization. To avoid exploring each case individually, we next introduce a general framework that unifies all these techniques for solving single-level problems.

### A unified framework for decentralized single-level optimization.

In this subsection, we consider solving the single-level problem \(_{x^{p}}_{i=1}^{n}f_{i}(x)\) over a network of \(n\) nodes. For each \(k\)-th (\(k 0\)) iteration, we let \(x_{i}^{k}\) denote the local \(x\)-variable maintained by the \(i\)-th agent. Furthermore, we associate the topology with a weight matrix \(W=[w_{ij}]_{i,j=1}^{n}^{n n}\) in which \(w_{ij}(0,1)\) if node \(j\) is connected to node \(i\) otherwise \(w_{ij}=0\). We use bold symbols to denote stacked vectors or matrices across agents. For example, \(^{k}=\{x_{1}^{k},...,x_{n}^{k}\}^{pn}\) and \(=W I_{p}\), where \(\) denotes the Kronecker product operator.

**A unified framework with moving average.** Building on the formulation in [1; 2], we develop a unified primal-dual framework with moving average for decentralized optimization:

\[^{k+1}=(1-)^{k}+^{k},^{k+1}=^{k}-^{k+1}- ^{k},^{k+1}=^{k}+^{k+1}.\] (6)

Here \(^{k}\) denotes the primal variable, \(^{k}\) denotes the dual variable introduced to mitigate the influence of data-heterogeneity, \(^{k}\) stacks all (stochastic) gradients evaluated at \(x_{i}^{k}\) for \(1 i n\), \(^{k}\) denotes the momentum introduced to boost training with coefficient \(\), and \(>0\) is the learning rate. Matrices \(,,^{pn pn}\) are adapted from the mixing matrix \(\), which determine how agents communicate with each other. See Appendix B.1 for more detailed motivations.

Framework (6) unifies various decentralized techniques in the literature. For instance, by letting \(=1\) and specifying \(,,\) delicately, framework (6) reduces to ED, EXTRA, and numerous GTvariants, see Table 3 and Appendix B.1 for more details. Framework (6) is closely related to the unified decentralized method developed in [1; 2]. The primary difference lies in the incorporation of the momentum variable \(^{k}\), which can help improve the transient iteration complexity of the framework (6) and relax the smoothness condition for bilevel algorithms . A detailed comparison between framework (6) and that proposed in [1; 2] is provided in Appendix B.2.

``` Initialize \(^{0}=^{0}=^{0}=^{0}=\), \(_{x}^{0}=_{y}^{0}=_{z}^{0}=\), learning rate \(_{k},_{k},_{k},_{k}\). for\(k=0,1,,K-1\)do \(^{k+1}=_{y}^{k}-_{k}_{y}^{k}-_{y}_{y}^{k}\), \(_{x}^{k+1}=_{y}^{k}+_{y}^{k+1}; \) lower-level update \(^{k+1}=_{z}^{k}-_{k}_{z} ^{k}-_{z}_{z}^{k}\), \(_{z}^{k+1}=_{z}^{k}+_{z}^{k+1}; \) auxiliary-level update \(^{k+1}=(1-_{k})^{k}+_{k}^{k}\); \(\) momentum update \(^{k+1}=_{x}^{k}-_{k}_{x} ^{k+1}-_{x}_{x}^{k}\), \(_{x}^{k+1}=_{x}^{k}+_{x}^{k+1}; \) upper-level update endfor ```

**Algorithm 1** SPARKLE: A unified framework for decentralized stochastic bilevel optimization

### A unified framework for decentralized bilevel optimization.

By utilizing the unified framework (6) to approximately solve each subproblem in (5) with only _one iteration_, we achieve SPARKLE, a unified single-loop framework for decentralized bilevel optimization. In particular, we independently sample data \(_{i}^{k}_{f_{i}}\), \(_{i}^{k}_{g_{i}}\) within each node at iteration \(k\), and evaluate stochastic gradients/Jacobians/Hessians as follows

\[l_{i}^{k} =_{1}F_{i}(x_{i}^{k},y_{i}^{k};_{i}^{k}), b_{i}^{k }=_{2}F_{i}(x_{i}^{k},y_{i}^{k};_{i}^{k}), v_{i}^{k}=_{2} G_{i}(x_{i}^{k},y_{i}^{k};_{i}^{k}),\] \[J_{i}^{k} =_{12}^{2}G_{i}(x_{i}^{k},y_{i}^{k};_{i}^{k}), H_{ i}^{k}=_{22}^{2}G_{i}(x_{i}^{k},y_{i}^{k};_{i}^{k}).\]

Next we stack the descent directions for variables of each level as follows

\[ ^{k}=\{v_{1}^{k},...,v_{n}^{k}\},\] \[ ^{k}=\{H_{1}^{k}z_{1}^{k}-b_{1}^{k},...,H_{n}^{k}z_{n}^{ k}-b_{n}^{k}\},\] \[ ^{k}=\{l_{1}^{k}-J_{1}^{k}z_{1}^{k+1},...,l_{n}^{k}-J_{n}^{ k}z_{n}^{k+1}\}.\]

The SPARKLE algorithm is detailed in Algorithm 1. In this algorithm, we utilize different dual variables \(_{s}\) and communication matrices \(_{s},_{s},_{s}\) for each variable \(s\{x,y,z\}\) to optimize their respective objective functions. We use momentum \(^{k}\) only for updating the upper-level variable, which is sufficient to enhance convergence of bilevel algorithms and relax the smoothness condition.

**Versatility in decentralized strategies.** SPARKLE is highly versatile, supporting various decentralized strategies by allowing the specification of different communication matrices \(_{s}\), \(_{s}\), and \(_{s}\). For example, by setting \(_{s}=\), \(_{s}=(-)^{1/2}\), and \(_{s}=\) for any \(s\{x,y,z\}\), SPARKLE will utilize EXTRA to update variables \(x,y\), and \(z\), resulting in the SPARKLE-EXTRA variant. Other variants can be achieved by setting \(_{s}\), \(_{s}\), and \(_{s}\) according to Table 3. These variants can be implemented more efficiently than listed in Algorithm 1, see Appendix B.3.

**Flexibility across optimization levels.** SPARKLE supports different optimization and communication mechanisms for each level of (5), which can be directly achieved by choosing different \(_{s}\), \(_{s}\), and \(_{s}\) matrices for each level \(s\{x,y,z\}\). For example, SPARKLE can utilize GT to update the upper-level variable \(x\) while employing ED to update the auxiliary- and lower-level variables \(y\) and \(z\). Throughout this paper, we denote SPARKLE using the decentralized mechanism \(\) for the lower-level and auxiliary variables, and \(\) for the upper-level in Algorithm 1, by SPARKLE-\(\)-\(\), or simply SPARKLE-\(\) if \(=\). In addition, SPARKLE even supports utilizing different mixing matrices \(_{x},_{y},_{z}\) across levels.

## 3 Convergence analysis

In this section, we establish the convergence properties of the SPARKLE framework and examine the influence of different decentralized techniques utilized across optimization levels.

### Assumptions

Before presenting the theoretical guarantees, we first introduce the following assumptions used throughout this paper.

**Assumption 1**.: _There exist constants \(_{g},L_{f,0},L_{f,1},L_{g,1},L_{g,2}\) such that for any \(1 i n\),_

1. \( f_{i}, g_{i},^{2}g_{i}\) _are_ \(L_{f,1},L_{g,1},L_{g,2}\) _Lipschitz continuous, respectively;_
2. \(\|_{2}f_{i}(x,y^{}(x))\| L_{f,0}\) _for any_ \(x^{p}\)_;_3__ 3. \(g_{i}(x,y)\) _is_ \(_{g}\)_-strongly convex with respect to_ \(y\) _for any fixed_ \(x^{p}\)_._

_Moreover, we define \(L:=\{L_{f,0},L_{f,1},L_{g,1},L_{g,2}\}\) and \(:=L/_{g}\)._

**Assumption 2**.: _For each \(s\{x,y,z\}\), the corresponding mixing matrix \(W_{s}^{n n}\) is non-negative, symmetric and doubly stochastic, i.e.,_

\[W_{s}=W_{s}^{}, W_{s}_{n}=_{n},(W_{s})_{ij}  0,\,1 i,j n,\]

_and the corresponding communication graph is strongly-connected, i.e., its eigenvalues satisfy \(1=_{1}(W_{s})>_{2}(W_{s})_{n}(W_{s})\) and \((W_{s}):=\{|_{2}(W_{s})|,|_{n}(W_{s} )|\}<1\)._

The value \(1-(W_{s})\) is referred to as the spectral gap in the literature  of \(W_{s}\), which measures the connectivity of the communication graph. It would approach \(0\) for sparse networks. For example, it holds that \(1-(W_{s})=(1/n^{2})\) for the matrix \(W_{s}\) induced by a ring graph.

**Assumption 3**.: _For any \(s\{x,y,z\}\), we assume the communication matrices \(A_{s},B_{s},C_{s}\) used in SPARKLE are polynomial functions of \(W_{s}\). Furthermore, we assume \(A_{s},C_{s}\) are doubly stochastic, and \((B_{s})=\{_{n}\}\). In addition, we assume all eigenvalues of the augmented matrix_

\[L_{s}:=[}-B_{s}^{2}&B_{s}\\ -B_{s}&}]\]

_are strictly less than one in magnitude, where \(} C_{s}-_{n}_{n}^{}\) and \(} I_{n}-_{n}_{n}^{}\)._

We remark that Assumption 3 is mild and is satisfied by all choices listed in Table 3. See more discussions in Appendix C.2.

**Assumption 4**.: _We assume \( F_{i}(x,y;), G_{i}(x,y;)\), and \(^{2}G_{i}(x,y;)\) to be unbiased estimates of \( f_{i}(x,y), g_{i}(x,y)\), and \(^{2}g_{i}(x,y)\) with bounded variances \(_{f,1}^{2},_{g,1}^{2},_{g,2}^{2}\), respectively._

### Convergence theorem

Under the above assumptions, we establish the convergence properties as follows. Proof details can be found in Appendix C.

   Algorithms & \(_{s}\) & \(_{s}\) & \(_{s}\) & The specific update rule at the \(k\)-th iteration. \\  ED & \(_{s}\) & \((-_{s})^{}\) & \(_{s}\) & \(^{k+2}=_{s}(2^{k+1}-^{k}- ((^{k+1})-(^{k})))\) \\ EXTRA & \(\) & \((-_{s})^{}\) & \(_{s}\) & \(^{k+2}=_{s}(2^{k+1}-^{k})- ((^{k+1})-(^{k}))\) \\ ATC-GT & \(_{s}^{2}\) & \(-_{s}\) & \(_{s}^{2}\) & \(^{k+1}=_{s}(^{k}-_{s}^{k} )\), \(_{s}^{k+1}=_{s}(_{s}^{k}+( ^{k+1})-(^{k}))\) \\ Semi-ATC-GT & \(_{s}\) & \(-_{s}\) & \(_{s}^{2}\) & \(^{k+1}=_{s}^{k}-_{s}^{k}\), \(_{s}^{k+1}=_{s}(_{s}^{k}+( ^{k+1})-(^{k}))\) \\ Non-ATC-GT & \(\) & \(-_{s}\) & \(_{s}^{2}\) & \(^{k+1}=_{s}^{k}-_{s}^{k}\), \(_{s}^{k+1}=_{s}_{s}^{k}+(^{k+1 })-(^{k})\) \\   

Table 3: SPARKLE facilitates different decentralized techniques by specifying \(_{s},_{s},_{s}\) for \(s\!\!\{x,y,z\}\). We denote the stacked local variables and the associate gradients estimates by \(\!\!\{,,\}\) and \(()\), respectively. The update rule refers to the specific algorithmic recursion for each level. See derivations in Appendix B.2.

**Theorem 1**.: _Under Assumptions 1 - 4, there exist proper constant step-sizes \(,\;,\,\) and momentum coefficient \(\), such that the SPARKLE framework listed in Algorithm 1 will converge as follow:_

\[_{k=0}^{K}[\|(^{k})\|^ {2}]}{}+^{}( _{y,1}+_{z,1})}}{K^{}}+ ^{}_{x,1}}}{K^{}}\] \[+(^{}_{y,2}+^{6}_ {z,2})}}{K^{}}+(^{}_{y,3}+^{}_{z,3}+^{} _{x,3})+( C_{}+^{4}C_{} ),\]

_where \(\{_{f,1},_{g,1},_{g,2}\}\), \(\{_{s,i}\}_{i=1}^{3}\) are constants depending only on \(_{s},_{s},_{s},_{s}\) for \(s\{x,y,z\}\), and \(C_{},C_{}\) are constants independent of \(K\). See Lemma 17 for their detailed values._

In the deterministic scenario with \(=0\), SPARKLE converges at the rate \((1/K)\), see the formal theorem and derivation in Appendix C.3. This recovers the rate in  under even milder assumptions. Unlike reference , which only considers GT in the deterministic setting, SPARKLE is a unified bilevel framework for the more general stochastic setting.

**Linear speedup.** According to Theorem 1, SPARKLE achieves an asymptotic linear speedup as \(K\) approaches infinity, which applies to all SPARKLE variants regardless of the decentralized strategies employed and whether they are utilized at different optimization levels. Furthermore, the asymptotically dominant term \(^{5}/()\) matches exactly with the single-node bilevel algorithm SOBA  when \(n=1\), implying the tightness of Theorem 1 in terms of the asymptotic rate.

**Remark 1**.: _We establish an upper bound for the consensus error \(_{k=0}^{K}[^{k}-}^{k}\|^{2}}{n}+^{k}-}^{k}\|^{2}}{ n}]\). Please refer to Lemma 19 in Appendix C.2.1 for more details._

### Transient iteration complexity

With the non-asymptotic rate established in Theorem 1, we can derive the transient iteration complexity of SPARKLE as follows. The proof is in Lemma 18.

**Corollary 1**.: _Under the same assumptions as in Theorem 1, the transient iteration complexity of SPARKLE--with the influence of \(\) and \(^{2}\) omitted for brevity--is on the order of_

\[\{n^{2}_{x},n^{3}_{y},n^{3}_{z},n_{x}, n_{y},n_{z}\},\] (8)

_where \(_{s},_{s}\) only depend \(_{s},_{s},_{s},_{s}\) for \(s\{x,y,z\}\). Their values are in Lemma 18._

We obtain the transient iteration complexity of each variant of SPARKLE by applying Corollary 1.

**Corollary 2**.: _For SPARKLE-ED and SPARKLE-EXTRA, if we choose \(_{y}=_{z}\), it holds that_

\[_{x}&=((1-( _{x}))^{-2}),_{y}=_{z}=((1- (_{y}))^{-2}),\\ _{x}&=((1-(_{x}))^{-}),_{y}=_{z}= ((1-(_{y}))^{-2}).\] (9)

_Furthermore, if we choose \(_{x}=_{y}=_{z}\) and denote \((_{x})\), the transient iteration complexity derived in (8) can be simplified as \(n^{3}/(1-)^{2}\)._

**Corollary 3**.: _For SPARKLE-GT and its variants with semi/non-ATC-GT, if we let \(_{y}=_{z}\),_

\[_{x}&=((1-( _{x}))^{-2}),_{y}=_{z}= ((1-(_{y}))^{-2}),\\ _{x}&=((1-(_{x}))^{-2}),_{y}=_{z}= ((1-(_{y}))^{-}).\]

_Furthermore, if we let \(_{x}=_{y}=_{z}\) and denote \((_{x})\), the transient iteration complexity derived in (8) can be simplified as \(\{n^{3}/(1-)^{2},n/(1-)^{8/3}\}\)._

**Remark 2** (_SOTA transient iterations_).: Comparing with algorithms listed in Table 1, all SPARKLE variants achieve smaller transient iteration complexity, implying that they can achieve linear speedup much faster than the other algorithms, especially over sparse network topologies with \(1- 0\).

**Remark 3** (_GT is not the best technique for decentralized SBO_).: While GT is widely adopted in the literature [16; 21; 57] to facilitate decentralized SBO, a comparison of Corollary 2 and 3 reveals that both SPARKLE-EXTRA and SPARKLE-ED outperform SPARKLE-GT in terms of transient iteration complexity. This implies that EXTRA and ED are better than GT for decentralized SBO.

### Different strategies across optimization levels

Corollary 1 clarifies how different update strategies for \(x\), \(y\), and \(z\) impact the transient iterations through constants \(\{_{s},_{s}\}\) for \(s\{x,y,z\}\). Since \(_{y}=_{z}\) and \(_{y}=_{z}\) when \(_{y}=_{z}\) (Lemma 18), we naturally employ the same strategy to update \(y\) and \(z\). The following corollary studies the utilization of both ED and GT in SPARKLE. See the transient iterations complexity of other mixed strategies in Appendix C.2.4 and Table 2.

**Corollary 4**.: _For SPARKLE-ED-GT which uses ED to update \(y\) and \(z\) and GT to update \(x\), if \(_{x}=_{y}=_{z}\) and we denote \(=(_{x})\), it then holds that_

\[_{x}=_{y}=_{z}=((1-)^{-2}), _{x}=_{y}=_{z}=((1-)^ {-2}),\]

_which implies that the transient iteration complexity in (8) can be simplified as \(n^{3}/(1-)^{2}\)._

**Remark 4** (_Mixed strategies outperform employing GT only)_.: Comparing Corollary 3 and 4, we find that using ED to update \(y\) and \(z\) will lead to smaller \(_{y}\) and \(_{z}\), which improves the transient iteration complexity compared to employing GT only in all optimization levels (see Corollary 3).

### Different topologies across optimization levels

In SPARKLE, we can utilize different topologies across levels. Theorem 1 and Corollary 1 have clarified the influence of using different topologies across levels through the constants \(\{_{s},_{s}\}\) for \(s\{x,y,z\}\). For instance, when substituting \(\{_{s},_{s}\}\) established in (9) into (8), SPARKLE-ED has the following transient iteration complexity:

\[\{n^{2}(1-(_{x}))^{-2},n^{3}(1-(_{y}))^{-2}\}\]

where \(_{x}\) is the mixing matrix for updating \(x\), while \(_{y}\) is for updating \(y\) and \(z\). As long as \((1-(_{x}))^{-1}(1-(_{y}))^{-1}\) holds, SPARKLE-ED retains the transient iteration complexity of \(n^{3}(1-(_{y}))^{-2}\), which allows for the utilization of a sparser network topology when updating \(x\), thereby reducing communication overheads. Consequently, the ratio \(a\) of the communication volume per round for the variables \(x\) and \(y\) can be significantly less than one. See Appendix C.2.3 for discussion on how to use different topologies across levels in other SPARKLE variants.

### Recovering single-level decentralized optimization

Previous works typically study single-level and bilevel optimization separately. By taking \(G_{i}(x,y,)|y|^{2}/2\) and \(F_{i}(x,y,)=F_{i}(x,)\) into (2), the decentralized SBO problem (1) reduces to stochastic single-level optimization. By setting \(^{k} 0\), \(^{k} 0\), \(u_{i}^{k}=_{1}f_{i}(x_{i}^{k},_{i}^{k})\), SPARKLE reduces to the single-level framework (6), whose convergence can be naturally guaranteed by Theorem 1. Please refer to Appendix C.4 for the detailed proof and results. This is the _first_ result demonstrating that bilevel optimization essentially subsumes the convergence of single-level optimization.

## 4 Numerical experiments

In this section, we present experiments to validate our theoretical findings. We first explore how update strategies and network structures influence the convergence of SPARKLE. Then we compare SPARKLE to the existing decentralized SBO algorithms. Additional experiments about a decentralized SBO problem with synthetic data are in Appendix D.1.

**Hyper-cleaning on FashionMNIST dataset.** We consider a data hyper-cleaning problem  on a corrupted FashionMNIST dataset . Problem formulations and experimental setups can be found in Appendix D.2. Firstly, we equip SPARKLE with different decentralized strategies in different optimization levels and then compare them with D-SOBA , MA-DSBO-GT , and MDBO  using the corruption rate \(p=0.1,0.2,0.3\), respectively. As is shown in Figure 1, all the SPARKLE-based algorithms generally achieve higher test accuracy than D-SOBA, while ED and EXTRA especially outperform GT. Meanwhile, using mixed strategies (_i.e._, SPARKLE-ED-GT and SPARKLE-EXTRA-GT) achieves similar test accuracy with SPARKLE-ED and SPARKLE-EXTRA and outperform SPARKLE-GT, respectively. These observations match with the theoretical results in Corollary 2-4 and Remark 3, 4.

Next, we test SPARKLE-EXTRA with two communication strategies including _fixed topology for updating \(x\) and varying topology for \(y,z\)_, and _fixed topology for updating \(y,z\) and varying topology for \(x\)_. As illustrated in Figure 2, maintaining a fixed topology for \(x\) while reducing the connectivity of the topology for \(y\) and \(z\) will deteriorate the algorithmic performance. Conversely, preserving the topology for \(y\) and \(z\) while decreasing the connectivity for \(x\) has little impact on the performance. This suggests that the influence of the network topology for \(y\) and \(z\) on the algorithm dominates over the topology for \(x\), which is consistent with our discussion in Section 3.5. We also numerically examine the influence of moving average on convergence, see discussions in Appendix D.2.

**Distributed policy evaluation in reinforcement learning.** We consider a multi-agent MDP problem in reinforcement learning on a distributed setting with \(n\{10,20\}\) agents respectively, which can be formulated as a decentralized SBO problems . Here, we compare SPARKLE with existing decentralized SBO approaches including MDBO  and the stochastic extension of SLDBO  over a Ring graph. Figure 3 illustrates that SPARKLE converges faster and achieves a lower sample complexity than the other baselines, especially when \(n=20\), which shows the empirical benefits of SPARKLE in decentralized SBO algorithms with a large number of agents and sparse communication modes. More experimental details are in Appendix D.3.

**Decentralized meta-learning.** We investigate decentralized meta-learning on miniImageNet  with multiple tasks , formulating it as a decentralized bilevel optimization problem. This approach minimizes the validation loss with respect to shared parameters as the upper-level loss, while the training loss is managed by task-specific parameters at the lower level. Additional details about the experiment can be found in Appendix D.4. Our method, SPARKLE, is benchmarked against D-SOBA  and MAML , demonstrating a significant improvement in training accuracy.

## 5 Conclusions and limitations

This paper proposes SPARKLE, a unified single-loop primal-dual framework for decentralized stochastic bilevel optimization. Being highly versatile, SPARKLE can support different decentralized mechanisms and topologies across optimization levels. Moreover, all SPARKLE variants have been demonstrated to achieve state-of-the-art convergence rate compared to existing algorithms. However, SPARKLE currently supports only strongly-convex problems in the lower-level optimization. Its compatibility with generally-convex lower-level problems remains unknown. Additionally, the condition number of the lower-level problem significantly impacts the performance, as is the case with existing bilevel algorithms. We aim to address these limitations in future work.

Acknowledgment

The work of Shuchen Zhu, Boao Kong, and Kun Yuan is supported by Natural Science Foundation of China under Grants 92370121, 12301392, and W2441021. This work is also supported by Open Project of Key Laboratory of Mathematics and Information Networks, Ministry of Education, China. No. KF202302.