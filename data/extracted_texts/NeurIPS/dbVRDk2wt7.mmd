# Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning

Berken Utku Demirel

Department of Computer Science

ETH Zurich, Switzerland

berken.demirel@inf.ethz.ch &Christian Holz

Department of Computer Science

ETH Zurich, Switzerland

christian.holz@inf.ethz.ch

###### Abstract

The success of contrastive learning is well known to be dependent on data augmentation. Although the degree of data augmentations has been well controlled by utilizing pre-defined techniques in some domains like vision, time-series data augmentation is less explored and remains a challenging problem due to the complexity of the data generation mechanism, such as the intricate mechanism involved in the cardiovascular system. Moreover, there is no widely recognized and general time-series augmentation method that can be applied across different tasks. In this paper, we propose a novel data augmentation method for quasi-periodic time-series tasks that aims to connect intra-class samples together, and thereby find order in the latent space. Our method builds upon the well-known mixup technique by incorporating a novel approach that accounts for the periodic nature of non-stationary time-series. Furthermore, by controlling the degree of chaos created by data augmentation, our method leads to improved feature representations and performance on downstream tasks. We evaluate our proposed method on three time-series tasks, including heart rate estimation, human activity recognition, and cardiovascular disease detection. Extensive experiments against state-of-the-art methods show that the proposed approach outperforms prior works on optimal data generation and known data augmentation techniques in the three tasks, reflecting the effectiveness of the presented method. Source code: [https://github.com/eth-siplab/Finding_Order_in_Chaos](https://github.com/eth-siplab/Finding_Order_in_Chaos).

## 1 Introduction

Self-supervised learning methods have gained significant attention as they enable the discovery of meaningful representations from raw data without explicit annotations. These self-supervised methods learn representations without labels by designing pretext tasks that transform the unsupervised representation learning problem into a supervised one such as predicting the rotation of images , or contexts . Among these methods, contrastive learning (CL), which learns to distinguish semantically similar examples over dissimilar ones, stands out as a powerful approach in self-supervised learning across various domains including computer vision , speech recognition , and natural language processing .

The success of contrastive learning relies on the creation of similar and dissimilar examples, which is typically achieved through the use of data augmentations . Recently, it was shown that data augmentations have a role to create a "_chaos_" between different intra-class samples such that they become more alike. For example, two different cars become very similar when they are both cropped to the wheels. . However, in time-series data, creating similar samples with augmentation techniques is more challenging due to the complexity of the dynamical data generation mechanisms . Moreover, research on contrastive learning for time series has demonstrated theabsence of a unique data augmentation technique that consistently performs better than others in different tasks [19; 20]. Instead, the choice of augmentation depends on the contextual characteristics of the signal, such as perturbing the high-frequency content of a signal that carries characteristic information in low frequencies does not generate useful data samples that are helpful for contrastive learning to learn class invariant features .

Considering these limitations, in this work, we first propose a novel data augmentation method for time series data by performing a tailored mixup while considering the phase and amplitude information as two separate features. Then, we perform specific operations for both features to generate positive samples by controlling the mixup coefficients for each feature to prevent aggressive augmentation. Specifically, our method employs a technique that controls the mixup ratio for each randomly chosen pair based on their distance in the latent space which is acquired through the use of a variational autoencoder (VAE), whose objective is to learn disentangled representations of data without labels. To this end, subjecting to the distance constraint in the latent space, the mixup tries to connect semantically closer samples together more aggressively while preventing the excessive interpolation of dissimilar samples that are likely to belong to different classes. Therefore, the purpose of our proposed method for quasi-periodic time-series data augmentation is to find an order in "chaos" between different samples such that they become more alike by interpolating them in a novel manner to prevent the loss of information. We summarize our contributions as follows:

* We propose a novel mixup method for non-stationary quasi-periodic time-series data by considering phase and magnitude as two separate features to generate samples that enhance intra-class similarity and help contrastive learning to learn class-separated representations.
* We present a novel approach for sampling mixup coefficients for each pair based on their similarity in the latent space, which is constructed without supervision while learning disentangled representations, to prevent aggressive augmentation between samples.
* We show that the tailored mixup with coefficient sampling consistently improves the performance of contrastive learning in three time-series tasks compared to prior mixup techniques and proposed augmentation methods that generate optimal/hard positives or negatives.

## 2 Preliminaries

### Notation

We use lowercase symbols \((x)\) to denote scalar quantities, bold lowercase symbols \(()\) for vector values, and capital letters \((X)\) for random variables. Functions with a parametric family of mappings are represented as \(f_{}(.)\) where \(\) is the parameters. The discrete Fourier transformation of a real-valued time series sample is denoted as \((.)\), yielding a complex variable as \(}\) where \(\) and \(k[0,f_{s}/2]\) is the frequency with the maximum value of Nyquist rate. The amplitude and phase of the \(()\) are represented as \(A()\) and \(P()\). The real and imaginary parts of a complex variable are shown as \((.)\) and \((.)\). The detailed calculations for operations are given in the Appendix A.1.

### Setup

We follow the common CL setup as follows. Given a dataset \(=\{(_{i})\}_{i=1}^{K}\) where each \(_{i}\) consists of real-valued sequences with length L and \(C\) channels. The objective is to train a learner \(f_{}\) which seeks to learn an invariant representation such that when it is fine-tuned on a dataset \(_{l}=\{(_{i},_{i})\}_{i=1}^{M}\) with \(M K\) and \(_{i}\{1,,N\}\), it can separate samples from different classes.

### Motivation

As stated by prior works, mixup-based methods have poor performance in domains where data has a non-fixed topology, such as trees, graphs, and languages [22; 23]. Here, we demonstrate how we derived our proposed method by revealing the limitations of mixup for time series theoretically while considering the temporal dependencies and non-stationarities.

**Assumption 2.1** (SNR Matters).: _There exist one or multiple bandlimited frequency ranges of interest \(f^{*}\), where the information that average raw time-series data conveys about the labels (i.e., \((;)\)) is directly proportional to normalized signal power in that frequency range as in Equation 1._\[(;)_{f^{*}}S_{x}(f)\ /_{-}^{ }S_{x}(f)\ \ \ \ S_{x}(f)=_{N}| _{n=-N}^{N}x_{n}e^{-j2 fn}|^{2} \]

Assumption 2.1 states that the information from a time series depends on its signal-to-noise ratio (SNR). Prior works showed that specific frequency bands hold inherent information about the characteristics of time series, which helps classification [21; 24].

**Assumption 2.2**.: _The true underlying generative process \(f(.)\), for a given data distribution \(=\{_{k}\}_{k=1}^{K}\), is quasiperiodic, i.e., \(f(+)=g(,f())\), where \(\) can be either fixed or varied._

Assumption 2.2 posits that the observed data samples from the distribution \(\) are generated by a quasiperiodic function. This is a minimal assumption since the quasiperiodicity is the relaxed version of the periodic functions. In simpler terms, quasiperiodicity can be described as the observed signals exhibiting periodicity on a small scale, while being unpredictable on a larger scale. And, several prior works showed that the data generation mechanism of time-series data for several applications in the real world are quasiperiodic [25; 26; 27; 28; 29; 30]. Therefore, Assumption 2.2 is realistic.

**Proposition 2.3** (Destructive Mixup).: _If Assumptions 2.1 and 2.2 hold, there exist \( Beta(,)\) or \( U(,1.0)\) with high values of \(\) such that when linear mixup techniques are utilized, the lower bound of the mutual information for the augmented sample distribution decreases to zero._

\[ 0(;^{+})< (;^{*}),&\ ^{*}\\ ^{+}=+(1-)}& \ _{f^{*}}S_{x^{*}}(f)=_{-}^{}S_{x^{*}}(f) \]

Proofs can be found in Appendix A. This proposition indicates that although the augmented samples are primarily derived from anchor samples (\(\)) with high ratios, the resulting instances may not contain any task-relevant information. In other words, the augmentation process can potentially discard the whole task-specific information. This destructive behavior of mixup for quasi-periodic data can be attributed to the _interference_ phenomenon in which two waves interact to form the resultant wave of the lower or higher amplitude according to the phase difference as shown in Proposition 2.3.

## 3 Method

We introduce a novel approach to overcome the limitations of mixup by treating the magnitude and phase of sinusoidal signals as two distinct features with separate behaviors. Subsequently, we apply tailored mixup operations to each feature, considering their specific characteristics and effects.

We perform the linear mixup for the magnitude of each sinusoidal. However, for the phase, we take a different approach and bring the phase components of the two coherent signals together by adding a small value to the anchor's phase in the direction of the other sample. The mixup operation performs the linear interpolation of features , however, interpolation of two complex variables can result in a complex variable whose phase and magnitude are completely different/far away from those two, i.e., mixup can be destructive extrapolation rather than the interpolation of features. Therefore, we mix the phase of two sinusoidal as follows. We start by calculating the shortest phase difference between the two samples, denoted as \(\), as described in Equation 31.

\[&[P()-P( })](\ 2)\\ &=-2,&\ >\\ ,& \]

The sign of the calculated phase difference provides information about the relative phase location of the other sample, in either a clockwise or counterclockwise direction in the phasor diagram. And, the absolute value of it represents the shortest angular difference between two samples in radians. Basedon the calculated phase difference between two samples, we perform mixup operation to generate diverse positive samples as in Equation 4 such that phase and magnitude of augmented instances are interpolated properly according to the anchor sample \(\), without causing any destructive interference.

\[^{+}=^{-1}(A(}) P(}))\ \ \ \ \ A(})=_{A}A()+(1-_{A})A(})\ \ \]

\[P(})=P()-||*(1-_{P}),& \ >0\\ P()+||*(1-_{P}),& \]

The proposed method which mixes the magnitude and phase of each frequency component with tailored operations, not only prevents destructive interference between time series, resulting in an increase in the lower bound of mutual information (as shown in Theorem 3.1), but also generates diverse augmented instances with the same two samples by using two different mixing coefficients.

**Theorem 3.1** (Guarantees for Mixing).: _Under assumptions 2.1 and 2.2, given any \((0,1]\), the mutual information for the augmented instance lower bounded by the sampled \(\) and anchor \(\)._

\[(;)(;})<(;})\ \ \ \ }=^{-1}(A(}) P(})) \]

We provide an intuitive demonstration in Figure 1, along with a detailed mathematical proof presented in Appendix A. Our approach also offers increased flexibility in selecting the mixing coefficients of phase and magnitude, based on their sensitivities to the mixing process as well as the augmentation degree for each randomly chosen pair. Since the degree of augmentations has crucial importance for contrastive learning, there can be cases where augmentations are either too weak (intra-class features cannot be clustered together) or too strong (inter-class features can also collapse to the same point) and lead to sub-optimal results . To mitigate this issue and find an order for augmentation degree, we search pairs of samples that are semantically closer, meaning they are more likely to belong to the same class. We then perform the proposed mixup more aggressively on these pairs, creating more closer and diverse samples while decreasing the augmentation strength for less similar pairs. To find similar samples without labels, we train a completely unsupervised \(\)-VAE  that maps data points to a latent space such that two random samples are semantically similar if they are close in the latent as shown in Proposition 3.2.

**Proposition 3.2** (Consistency in Latent Space ).: _Given a well-trained unconditional VAE with the encoder \(E(.)\) that produces distribution \(p_{E}(|)\), the decoder \(D(.)\) that produces distribution \(q_{D}(|)\) while the prior for \(\) is \(p()\), let \(}\) and \(}\) be two latent vectors of two different real samples \(}\) and \(}\), i.e., \(E(})=}\) and \(E(})=}\). if the distance \(d(},})\), then \(D(})\) and \(D(})\) will have a similar semantic label as in Equation 6._

\[|(D(});)-(D(}); )|, \]

Figure 1: The phasor representation of linear mixup \(\)), and proposed mixup \(\)). The anchor, randomly chosen sample, and generated instances are represented as \(\), \(}\), and \(}\), respectively.

_where \(\) stands for tolerable semantic difference, \(\) is the maximum distance to maintain semantic consistency, and \(d(.)\) is a distance measure such as cosine similarity between two vectors._

The above proposition with Theorem 3.1 motivates us to perform augmentation aggressively if two randomly chosen samples are semantically closer. Therefore, we sample the mixup coefficient for both phase and magnitude from a uniform distribution \(_{A},_{P} U(,1.0)\) with low values of \(\) if the distance between the latent vectors is below a threshold, otherwise, they are drawn from a truncated normal distribution \(_{A},_{P}^{t}(,,1.0)\) with a high mean and low standard deviation.

## 4 Experiments

We conduct experiments on the proposed approach and compare it with other mixup methods or optimal/hard positive sample generation in the contrastive learning setup. During our experiments, we use SimCLR  framework without specialized architectures or a memory bank for all baselines to have a fair comparison. Results with other CL frameworks can be found in Appendix E. Complete training details and hyper-parameter settings for datasets and baselines are provided in Appendix D.

### Datasets

We performed extensive experiments on eight datasets from three tasks that include activity recognition from inertial measurements (IMUs), heart rate prediction from photoplethysmography (PPG), and cardiovascular disease classification from electrocardiogram (ECG). We provided short descriptions of each dataset below, and further detailed information with metrics can be found in Appendix B.

Activity recognitionWe used UCIHAR , HHAR , and USC  for activity recognition. During the evaluation, we assess the cross-person generalization capability of the contrastive models, i.e., the model is evaluated on a previously unseen target domain. We follow the settings in GILE  to treat each person as a single domain while the fine-tuning dataset is much smaller than the unsupervised one.

Heart rate predictionWe used the IEEE Signal Processing Cup in 2015 (IEEE SPC) , and DaLia  for PPG-based heart rate prediction. The SPC provides two datasets, one smaller with lesser artifacts (referred to as SPC12)  and a bigger dataset with more participants including heavy motions (referred to as SPC22). In line with previous studies, we adopted the leave-one-session-out (LOSO) cross-validation, which involves evaluating methods on subjects or sessions that were not used for pre-training and fine-tuning.

Cardiovascular disease (CVD) classificationWe conducted our experiments on China Physiological Signal Challenge 2018 (CPSC2018)  and Chapman University, Shaoxing People's Hospital (Chapman) ECG dataset . We selected the same four specific leads as in  while treating each dataset as a single domain with a small portion of the remainder dataset used for fine-tuning the pre-trained model. We split the dataset for fine-tuning and testing based on patients (each patient's recordings appear in only one set).

### Baselines

Comparison with prior mixup techniquesWe evaluate the effectiveness of our proposed mixup by comparing it with other commonly used mixup methods, including Linear-Mixup , Binary-Mixup , Geometric-Mixup , Cut-Mix , Amplitude-Mix  and Spec-Mix . When we compare the performance of mixup techniques, we follow the same framework with  where the samples of mixture operation only happen in current batch samples. And, the mixup samples are paired with anchors, i.e., without applying mixup second times, for contrastive pre-training.

Comparison with methods for optimal sample generationWe evaluate the performance of our proposed method by comparing it with other data generation methods and baselines in contrastive learning setup while considering previously known augmentation techniques. Traditional data augmentations for time-series , such as resampling, flipping, adding noise, etc. InfoMin which leverages an adversarial training strategy to decrease the mutual information between sampleswhile maximizing the NCE loss . NNCLR , which uses nearest neighbors in the learned representation space as the positive samples. Positive feature extrapolation , which creates hard positives through feature extrapolation. GenRep which uses the latent space of a generative model to generate "views" of the same semantic content by sampling nearby latent vectors . Aug. Bank , which proposes an augmentation bank that manipulates frequency components of a sample with a limited budget. STAug , which combines spectral and time augmentation for generating samples using the empirical mode decomposition and linear mixup. DACL , which creates positive samples by mixing hidden representations. IDAA , which is an adversarial method by modifying the data to be hard positives without distorting the key information about their original identities using a VAE. More implementation details for each baseline are given in Appendix C.

### Implementation

We use a combination of convolutional with LSTM-based network, which shows superior performance in many time-series tasks , as backbones for the encoder \(f_{}(.)\) where the projector is two fully-connected layers. We use InfoNCE as the loss, which is optimized using Adam with a learning rate of \(0.003\). We train with a batch size of 256 for 120 epochs and decay the learning rate using the cosine decay schedule. After pre-training, we train a single linear layer classifier on features extracted from the frozen pre-trained network, i.e., linear evaluation, with the same hyperparameters. Reported results are mean and standard deviation values across three independent runs with different seeds on the same split. More details about the implementation, architectures, and hyperparameters with the trained VAEs are given in Appendix D.

## 5 Results and Discussion

Tables 1, 2, and 3 present the results of our proposed approach compared to state-of-the-art methods for optimal/hard positive sample generation in contrastive learning setup across the three tasks in eight datasets. Additionally, Figure 2 compares our approach with prior mixup methods (e.g., linear mixup, cutmix) without applying any other additional augmentation techniques. Overall, our proposed method has demonstrated superior performance compared to other methods in seven datasets, with the second-best performance in the remaining dataset, and a minor performance gap.

From these tables, we can see that our proposed method significantly outperforms DACL, which suggests creating a positive sample by mixing fixed hidden representations in an intermediate layer , by a large margin (up to 20.8% with a 10.1% on average in activity recognition). This suggests that when the representations are not yet linearly separable at the beginning of the contrastive training process, the interpolated representations using mixup may be dissimilar to the actual interpolated samples and may not capture their underlying features. One interesting result from our experiments is that IDAA  exhibits comparable performance to our method in some datasets, and even slightly outperforms our approach in the HHAR dataset for activity recognition. Despite using distinct methods to generate positive instances, i.e., adversarial and mixup, our approach and IDAA algorithm

    & UCHAR &  &  \\   & ACC\(\) & MF\(\) & ACC\(\) & MF\(\) & ACC\(\) & MF\(\) \\  _Supervised_ & & & & & & \\ DCL  & 77.63 & – & 51.27 & – & 60.35 & – \\ CoDATS  & 68.22 & – & 45.69 & – & – & – \\ G1LE  & 88.17 & – & 55.61 & – & – & – \\  _Self-Supervised_ & & & & & & \\ Traditional Augs. & 87.05 \(\) 1.07 & 86.13 \(\) 0.96 & 85.48 \(\) 1.16 & 84.31 \(\) 1.31 & 53.47 \(\) 1.10 & 52.09 \(\) 0.95 \\ NNCLR  & 85.31 \(\) 0.91 & 83.56 \(\) 1.25 & 83.16 \(\) 1.32 & 82.15 \(\) 1.25 & 55.41 \(\) 1.43 & 52.64 \(\) 1.37 \\ InfoMin  & 30.07 \(\) 8.15 & 30.66 \(\) 0.15share similarities in approaches for positive instance generation in CL setup. The IDAA algorithm aims to create hard positive samples that lie near class boundaries without changing the identity of the sample, while our method interpolates two samples to produce a positive instance that is similar to the original sample while adding noise to the phase and amplitude in the direction of a randomly chosen sample. In other words, both approaches try to keep the sample identity intact by taking special precautions while generating new positive instances, which may explain their similar performance in our experiments.

In contrast, approaches that do not prioritize preserving sample identity while generating samples or hidden representations often demonstrate suboptimal performance on average while exhibiting increased variability across tasks.

Examples of such methods include PosET , which generates hard positive samples to improve contrastive learning by extrapolating features, STAug , which uses empirical mode decomposition with linear mixup technique together, and InfoMin , which tries to minimize mutual information between two instances in an adversarial manner. The performance comparison of prior mixup techniques and our proposed one is shown in Figure 2. On average, our proposed method outperforms all other mixup techniques while reducing the variance across tasks. What is interesting about this figure is that while the linear  and amplitude mixup  reach our method in some datasets for _activity recognition_, the performance of the linear mixup decreases heavily for the other two tasks whereas the amplitude mixup gives reasonable performance. This empirical outcome supports our initial theorem about the destructive effect of mixup, which suggests linear mixup or other derivatives can discard the whole task-specific information in the generated positive sample for quasi-periodic signals even though the mixing coefficient is sampled from a distribution such that the generated samples are much closer to the anchor.

### Ablation Studies

Here, we present a comprehensive examination of our proposed method and the effect of its components on the performance. Mainly, we investigate the effect of the proposed mixup by applying the instance selection algorithm to the linear mixup (w/o Prop. Mixup). Then, we perform our proposed mixup with the constant \(_{A}\) and \(_{P}\) coefficients without investigating latent space distances between pairs (w/o Aug. Degree). Tables 4, 5 and 6 summarize the results. The second row in the tables shows the performance when the proposed mixup method is not applied while choosing mixup coefficients according to the distances in the latent space for the linear mixup. The last row illustrates

   Method &  &  &  \\   & MAE\(\) & RMSE\(\) & MAE\(\) & RMSE\(\) & MAE\(\) & RMSE\(\) \\  _Supervised_ & & & & & & \\ DCL & 22.02 & 28.44 & 28.10 & 32.45 & 6.58 & 11.30 \\ CNN Ensemble\({}^{*}\) & 3.89 & – & 8.74 & – & 8.58 & – \\  _Self-Supervised_ & & & & & & \\ Traditional Augs. & 20.67 \(\) 1.13 & 26.35 \(\) 0.98 & 16.84 \(\) 1.10 & 22.23 \(\) 0.72 & 12.01 \(\) 0.65 & 21.09 \(\) 0.86 \\ NNCLR  & 20.28 \(\) 2.21 & 28.23 \(\) 1.63 & 23.49 \(\) 1.54 & 28.75 \(\) 3.66 & 11.56 \(\) 0.63 & 19.95 \(\) 0.89 \\ InfoMin  & 36.84 \(\) 5.11 & 29.78 \(\) 7.31 & 31.58 \(\) 4.72 & 29.72 \(\) 4.83 & 45.89 \(\) 8.71 & 50.77 \(\) 9.72 \\ IDAA  & 19.02 \(\) 0.96 & 27.42 \(\) 1.11 & 15.37 \(\) 1.21 & 22.41 \(\) 1.42 & 11.12 \(\) 0.64 & 20.45 \(\) 0.69 \\ PosET  & 25.60 \(\) 1.93 & 33.80 \(\) 2.71 & 23.42 \(\) 1.50 & 31.51 \(\) 3.71 & 35.99 \(\) 3.95 & 39.92 \(\) 3.12 \\ STAug  & 27.44 \(\) 1.93 & 35.63 \(\) 3.10 & 19.86 \(\) 2.11 & 30.70 \(\) 3.54 & 18.70 \(\) 4.06 & 30.81 \(\) 3.61 \\ Aug. Bank  & 27.31 \(\) 2.17 & 37.93 \(\) 2.96 & 27.84 \(\) 2.03 & 36.41 \(\) 3.98 & 35.87 \(\) 4.18 & 40.61 \(\) 3.74 \\ GenRep  & 21.02 \(\) 1.41 & 28.42 \(\) 1.65 & 15.67 \(\) 1.23 & 22.33 \(\) 1.43 & 25.41 \(\) 1.62 & 36.83 \(\) 1.87 \\ DACL  & 21.85 \(\) 1.63 & 28.17 \(\) 1.75 & 14.67 \(\) 1.10 & 20.06 \(\) 1.21 & 18.44 \(\) 1.32 & 25.61 \(\) 1.45 \\ Ours & **16.26**\(\) 0.72 & **22.48**\(\) 0.95 & **12.25**\(\) 0.47 & **18.20**\(\) 0.61 & **10.57**\(\) 0.55 & **20.37**\(\) 0.73 \\   

* The entire dataset, excluding the test, is utilized with labels, while in DCM, the labeled data size matches that of the CL

Table 2: Performance comparison of ours with prior works in _Heart Rate Prediction_ datasets

   Method & CPSC 2018 & Chapman \\   & AUC\(\) & AUC\(\) \\  _Supervised_ & & \\ CNN  & — & 95.80 \\ Casual CNN  & — & 97.70 \\  _Self-Supervised_ & & \\ Traditional Augs. & 67.86 \(\) 3.41 & 74.69 \(\) 2.04 \\ NNCLR  & 70.06 \(\) 2.05 & 77.19 \(\) 2.41 \\ InfoMin  & 64.48 \(\) 6.15 & 56.34 \(\) 9.12 \\ IDAA  & 80.90 \(\) 0.73 & 93.63 \(\) 0.91 \\ PosET  & 72.58 \(\) 2.12 & 78.27 \(\) 2.34 \\ STAug  & 74.15 \(\) 1.15 & 93.88 \(\) 0.87 \\ Aug. Bank  & 81.78 \(\) 1.24 & 94.75 \(\) 0.90 \\ GenRep  & 52.49 \(\) 3.43 & 86.72 \(\) 1.13 \\ DACL  & 82.38 \(\) 0.84 & 92.28 \(\) 0.97 \\ Ours & **85.30**\(\) 0.45 & **95.90**\(\) 0.82 \\   

* The entire dataset, excluding the test, is utilized with labels, while in DCM, the labeled data size matches that of the CL

Table 3: Performance comparison between ours and prior work in _CVD_.

the performance change resulting from randomly sampling mixup coefficients without considering any relationship between the selected pair while applying tailored mixup for phase and magnitude.

The results obtained from the ablation study support the previous claims and outcomes. For example, when the linear mixup is applied instead of the proposed mixup technique for _heart rate prediction_ (Table 5, w/o Prop. Mixup), the performance decrease is significant compared to the case when coefficients are sampled without considering the distance in the latent space (Table 5, w/o Aug. Degree). This observation indicates that as the periodicity in data increases, linear mixup can lead to significant destructive interferences, whereas our method effectively prevents such issues.

While our mixup technique consistently enhances performance across datasets, we observe a decline when the mixing coefficients are sampled based on the distance in the latent space for two datasets. Also, the performance increase gained by sampling coefficients based on distance is relatively low compared to the proposed mixup. Several factors can explain this observation. First, the VAE might not be well trained due to the limited size of data in each class, i.e., the assumption in Proposition 3.2 does not hold. This can lead to inconsistencies in the semantic similarity of the latent space such that two close samples in the latent space might have different labels. Second, if the number of classes increases for a downstream task, the probability of sampling intra-class samples in a batch will decrease, leading to a lack of performance improvement. Therefore, in future investigations, it might be beneficial to use a different distance metric for quasi-periodic time-series data such that it can scale with the number of classes while considering the lack of big datasets.

More ablation studies about the sensitivity of mixing coefficients and performance in different self-supervised learning frameworks, like BYOL  can be found in Appendix E.1 and E.2. And, investigations regarding whether we still need known data augmentations are given in Appendix E.3. Examples that visually demonstrate the negative effects of linear mixup and our proposed mixup

    & CPSC 2018 & Chapman \\   & AUC\(\) & AUC\(\) \\  Ours & **85.30**\(\) 0.45 & **95.90**\(\) 0.82 \\ w/o Prop. Mixup & 81.20 (-4.10) & 86.30 (-9.60) \\ w/o Aug. Degree & 80.67 (-4.63) & 95.98 (+0.08) \\   

Table 6: Ablation on proposed mixup with coefficient selection in _CVD_.

    & UCIHAR &  &  &   }} \\    & ACC\(\) & MF\(\) & ACC\(\) & MF\(\) & ACC\(\) & MF\(\) \\  Ours & **91.60**\(\) 0.65 & **90.46**\(\) 0.53 & **88.05**\(\) 1.05 & **87.95**\(\) 1.10 & **60.13**\(\) 0.75 & **59.13**\(\) 0.69 \\ w/o Prop. Mixup & 83.09 (-8.51) & 81.65 (-8.81) & 85.89 (-2.16) & 86.01 (-1.94) & 45.10 (-15.03) & 43.64 (-15.49) \\ w/o Aug. Degree & 80.86 (-10.74) & 80.18 (-10.26) & 87.53 (-0.95) & 87.75 (-0.20) & 57.00 (-3.13) & 54.75 (-4.38) \\   

Table 4: Ablation on proposed mixup with coefficient selection in _Activity Recognition_ datasets

    & IEEE SPC12 &  &  \\   & MAE\(\) & RMSE\(\) & MAE\(\) & RMSE\(\) & MAE\(\) & RMSE\(\) \\  Ours & **16.26**\(\) 0.72 & **22.48**\(\) 0.95 & **12.25**\(\) 0.47 & **18.20**\(\) 0.61 & **10.57**\(\) 0.55 & **20.37**\(\) 0.73 \\ w/o Prop. Mixup & 20.45 (-44.19) & 28.51 (-4.63) & 15.29 (+3.04) & 24.08 (+5.88) & 24.11 (+13.54) & 35.45 (+15.18) \\ w/o Aug. Degree & 19.30 (+3.04) & 24.84 (+2.36) & 16.01 (+3.76) & 21.21 (+3.19) & 11.10 (+0.53) & 20.13 (-0.24) \\   

Table 5: Ablation on proposed mixup with coefficient selection in _Heart Rate Prediction_ datasets.

Figure 2: The comparison of mixup methods where the error bars represent the deviation across random seeds (explicit numbers are given in Appendix E). **a)** shows the performance in _activity recognition_, **b)** is for _heart rate prediction_, and finally **c)** shows the _CVD classification_. For the last two tasks, we excluded Geomix as its performance is extremely poor and distorts the y-axis scale.

technique to prevent this problem can be found in Appendix F. Comparative results regarding the performance of the tailored mixup in the supervised learning paradigm are given in Appendix G.

## 6 Related Work

The goal of contrastive learning is to contrast positive with negative pairs . In other words, the embedding space is governed by two forces, the attraction of positive pairs and the repellence of negatives, actualized through the contrastive loss . Since label information is unavailable during the training, positive pairs are generated using augmentation techniques on a single sample, while negative pairs are randomly sampled from the entire dataset. Therefore, the choice or generation of positive and negative samples plays a pivotal role in the success of contrastive learning  and both approaches, generation/selection of positive/negative pairs, have been investigated thoroughly in the literature , we limit our discussion about prior works related to data augmentation techniques that create optimal or hard samples without labels.

Adversarial based approachesA growing body of literature has investigated generating samples by using adversarial training for both positives and negatives . A seminal work about the importance of augmentations in CL, InfoMin, presented an adversarial training strategy where players try to minimize and maximize the mutual information using the NCE loss . CLAE, one of the first works that leveraged the adversarial approach, shows that adversarial training generates challenging positive and hard negative pairs . Another recent study proposed an adversarial approach that generates hard samples while retaining the original sample identity by leveraging the identity-disentangled feature of VAEs . However, adversarial augmentations may change the original sample identity due to excessive perturbations and it is infeasible to tune the attack strength for every sample to preserve the identity. In other words, these approaches do not consider the sample-specific features and use a constant perturbation coefficient for all samples whereas our proposed method considers the similarity between pairs and tunes the mixing coefficients accordingly.

Mixup based approachesMixup-based methods have been recently explored in contrastive learning . According to a recent theoretical work , mixup has implicit data-adaptive regularization effects that promote generalization better than adding Gaussian noise, which is a commonly used augmentation strategy in both time-series and vision data . Although, mixup-based approaches have shown success in different problems , such as domain adaptation, creating samples using mixup in the input space is infeasible in domains where data has a non-fixed topology, such as sequences, trees, and graphs . Therefore, recent works suggest mixing hidden representations of samples, similar to Manifold Mixup . However, this method claims that mixing fixed-length hidden representation via an intermediate layer "\(=+(1-)}\)" can be interpreted as adding noise to a given sample in the direction of another. However, it is an overly optimistic claim because early during training, where in most cases there is usually no linear separability among the representations, this synthesis may result in hidden representations that are completely different and far away from the samples . Therefore, in this work, we take a different approach and modify the mixup method considering its limitations for quasi-periodic non-stationary time-series data. Also, unlike most existing methods that aim to generate hard samples--samples that are close to class boundaries--using adversarial approaches  or feature extrapolation , our method seeks to connect semantically closer samples together using interpolation in a tailored way.

## 7 Conclusion

In this paper, we first demonstrate the destructive effect of linear mixup for quasi-periodic time-series data, then introduce a novel tailored mixup method to generate positive samples for the contrastive learning formulation while preventing this destructive effect and interpolating the samples appropriately. Theoretically, we show that our proposed method guarantees the interpolation of pairs without causing any loss of information while generating a diverse set of samples. Empirically, our method outperforms the prior approaches in three real-world tasks. By conducting experiments on contrastive and supervised learning settings, we show that our approach is agnostic to the choice of learning paradigm. Thus, it holds the potential for utilization in generating augmented data for different learning paradigms as well. We believe that the method proposed in this paper has the potential to significantly improve learning solutions for a diverse range of time series tasks.