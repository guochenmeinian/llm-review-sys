# Generalized Bayesian Inference for Scientific

Simulators via Amortized Cost Estimation

Richard Gao\({}^{*1,2}\)

r.dg.gao@gmail.com

Michael Deistler\({}^{*1,2}\)

michael.deistler@uni-tuebingen.de

Jakob H. Macke\({}^{1,2,3}\)

jakob.macke@uni-tuebingen.de

###### Abstract

Simulation-based inference (SBI) enables amortized Bayesian inference for simulators with implicit likelihoods. But when we are primarily interested in the quality of predictive simulations, or when the model cannot exactly reproduce the observed data (i.e., is misspecified), targeting the Bayesian posterior may be overly restrictive. Generalized Bayesian Inference (GBI) aims to robustify inference for (misspecified) simulator models, replacing the likelihood-function with a cost function that evaluates the goodness of parameters relative to data. However, GBI methods generally require running multiple simulations to estimate the cost function at each parameter value during inference, making the approach computationally infeasible for even moderately complex simulators. Here, we propose amortized cost estimation (ACE) for GBI to address this challenge: We train a neural network to approximate the cost function, which we define as the expected distance between simulations produced by a parameter and observed data. The trained network can then be used with MCMC to infer GBI posteriors for any observation without running additional simulations. We show that, on several benchmark tasks, ACE accurately predicts cost and provides predictive simulations that are closer to synthetic observations than other SBI methods, especially for misspecified simulators. Finally, we apply ACE to infer parameters of the Hodgkin-Huxley model given real intracellular recordings from the Allen Cell Types Database. ACE identifies better data-matching parameters while being an order of magnitude more simulation-efficient than a standard SBI method. In summary, ACE combines the strengths of SBI methods and GBI to perform robust and simulation-amortized inference for scientific simulators.

Machine Learning in Science, Excellence Cluster Machine Learning, University of Tubingen

\({}^{2}\)Tubingen AI Center

\({}^{3}\)Department Empirical Inference, Max Planck Institute for Intelligent Systems

Tubingen, Germany

\({}^{*}\)Equal contributions.

## 1 Introduction

Mechanistic models expressed as computer simulators are used in a wide range of scientific domains, from astronomy, geophysics, to neurobiology. The parameters of the simulator, \(\), encode mechanisms of interest, and simulating different parameter values produces different outputs, i.e., \((_{i})_{i}\), where each model-simulation \(_{i}\) can be compared to experimentally observed data, \(_{o}\). Using such simulators, we can quantitatively reason about the contribution of mechanisms behind experimentalmeasurements. But to do so, a key objective is often to find all those parameter values that can produce simulations consistent with observed data.

One fruitful approach towards this goal is simulation-based inference (SBI) , which makes it possible to perform Bayesian inference on such models by interpreting simulator outputs as samples from an implicit likelihood , \( p(|)\). Standard Bayesian inference targets the parameter posterior distribution given observed data, i.e., \(p(|_{o})=_{o}|)p()}{p|_{o})}\), where \(p()\) captures prior knowledge and constraints over model parameters, and the likelihood function \(p(_{o}|)\) is evaluated as a function of \(\) for a fixed \(_{o}\). SBI methods can differ in whether they aim to approximate the likelihood [3; 4; 5; 6] or the posterior directly [7; 8; 9], and can be _amortized_, i.e., do not require new simulations and retraining for new data [10; 11]. In the end, each method provides samples from the posterior, which are all, in theory, capable of producing simulations that are _identical_ to the observation we condition on. Furthermore, by definition, the posterior probability of drawing a sample scales as the product of its prior probability and, critically, the likelihood that this sample can produce a simulation that is _exactly equal_ to the observation.

However, targeting the exact posterior may be overly restrictive. In many inference scenarios, modelers are primarily interested in obtaining a diverse collection of parameter values that can explain the observed data. This desire is also reflected in the common usage of posterior predictive checks, where seeing predictive simulations that resemble the data closely (in some specific aspects) is used to gauge the success of the inference process. In particular, it is often clear that the scientific model is only a coarse approximation to the data-generating process, and in some cases even cannot generate data-matching simulations, i.e., is misspecified . For example, in the life-sciences, it is not uncommon to use idealized, theoretically motivated models with few parameters, and it would be unrealistic to expect that they _precisely_ capture observations of highly complex biological systems. In such cases, or in cases where the model is fully deterministic, it is nonsensical to use the probability of exactly reproducing the data. In contrast, it would still be useful to find parameter values that produce simulations which are 'close enough', or as close as possible to the data. Therefore, instead of sampling parameters according to _how often_ they produce simulations that match the data _exactly_, many use cases call for sampling parameters according to _how closely_ their corresponding simulations reproduce the observed data.

**Generalized Bayesian Inference (GBI)** offers a principled way to do so by replacing the (log) likelihood function with a cost function that simply scores a parameter given an observation, such as the expected distance between \(_{o}\) and all possible simulations \(\) produced by \(_{i}\) (Fig. 1). Several recent works have leveraged this framework to perform inference for models with implicit or intractable likelihoods, especially to tackle model misspecification: Matsubara et al.  use a Stein Discrepancy as the cost function (which requires the evaluation of an unnormalized likelihood and multiple i.i.d. data samples), and Cherieff-Abdellatif et al. and Dellaporta et al. [15; 16] use simulator samples to estimate maximum mean discrepancy and directly optimize over this cost function via stochastic gradient descent (which requires a differentiable simulator). More broadly, cost functions such as scoring rule estimators have been used to generalize approximate Bayesian computation (ABC)  and synthetic likelihood approaches [3; 18], where the Monte Carlo estimate requires (multiple) simulations from \(p(|)\).

Thus, existing GBI approaches for SBI either require many simulations to be run during MCMC sampling of the posterior (similar to classical ABC methods), or are limited to differentiable simulators. Moreover, performing inference for new observations requires re-running simulations, rendering such methods simulation-inefficient and expensive at inference-time, and ultimately impractical for scientific simulators with even moderate computational burden.

We here propose to perform GBI for scientific simulators with **amortized cost estimation (ACE)**, which inherits the flexibility of GBI but amortizes the overhead of simulations by training a neural

Figure 1: **Estimating cost from simulations.** Using the expected distance between simulated and target data as the cost function, GBI assigns high probability to parameter values that, on average, produce simulations that are close—but not necessarily equal—to the observation.

network to predict the cost function for any parameter-observation pair. We first outline the GBI formalism in Sec. 2, then introduce ACE in Sec. 3. In Sec. 4, we show that ACE provides GBI posterior predictive simulations that are close to synthetic observations for a variety of benchmark tasks, especially when the simulator is misspecified. We showcase its real-world applicability in Sec. 5: using experimental data from the Allen Cell Types Database, ACE successfully infers parameters of the Hodgkin-Huxley single-neuron simulator with superior predictive performance and an order of magnitude higher simulation efficiency compared to neural posterior estimation . Finally, we discuss benefits and limitations of GBI and ACE, and related work (Sec. 6).

## 2 Background

To construct the GBI posterior, the likelihood, \(p(_{o}|)\), is replaced by a 'generalized likelihood function', \(L(;_{o})\), which does not need to be a probabilistic model of the data-generating process, as long as it can be evaluated for any pair of \(\) and \(_{o}\). Following the convention in Bissiri et al. , we define \(L(;_{o}) e^{-(;_{o})}\), where \((;_{o})\) is a cost function that encodes the quality of \(\) relative to an observation \(_{o}\), and \(\) is a scalar inverse temperature hyperparameter that controls how much the posterior weighs the cost relative to the prior. Thus, the GBI posterior can be written as

\[p(|_{o}) e^{-(;_{o})}p().\] (1)

As noted previously , if we define \((;_{o})- p(_{o}| )\) (i.e., self-information) and \(=1\), then we recover the standard Bayesian posterior, and "tempered" or "power" posterior for \( 1\)[19; 20]. The advantage of GBI is that, instead of adhering strictly to the (implicit) likelihood, the user is allowed to choose arbitrary cost functions to rate the goodness of \(\) relative to an observation \(_{o}\), which is particularly useful when the simulator is misspecified. Previous works have referred to \((;_{o})\) as risk function , loss function , or (proper) scoring rules when they satisfy certain properties [22; 18] (further discussed in Section 6.1). Here we adopt 'cost' to avoid overloading the terms 'loss' and'score' in the deep learning context.

## 3 Amortized Cost Estimation for GBI

### Estimating cost function with neural networks

In this work, we consider cost functions that can be written as an expectation over the likelihood:

\[(;_{o})_{p(| )}[d(,_{o})]=_{}d( ,_{o})p(|)d.\] (2)

Many popular cost functions and scoring rules can be written in this form, including the average mean-squared error (MSE) , the maximum mean discrepancy (MMD\({}^{2}\)) , and the energy score (ES)  (details in Appendix A2). While \((;_{o})\) can be estimated via Monte Carlo sampling, doing so in an SBI setting is simulation-inefficient and time-intensive, since the inference procedure must be repeated for every observation \(_{o}\), and simulations must be run in real-time during MCMC sampling of the posterior. Furthermore, this does not take advantage of the structure in parameter-space or data-space around neighboring points that have been simulated.

We propose to overcome these limitations by training a regression neural network (NN) to learn \((;_{o})\). Our first insight is that cost functions of the form of Eq. 2 can be estimated from a dataset consisting of pairs of parameters and outputs--in particular, from _a single_ simulation run per \(\) for MSE, and finitely many simulation runs for MMD\({}^{2}\) and ES. We leverage the well-known property that NN regression converges to the conditional expectation of the data labels given the data: If we compute the distances \(d(,_{o})\) between a single observation \(_{o}\) and every \(\) in our dataset, then a neural network \(f_{}()\) trained to predict the distances given parameters \(\) will denoise the noisy distance labels \(d(,_{o})\) and converge onto the desired cost \(f_{}()(;_{o })=_{p(|)}[d(,_{o})]\), approximating the cost of any \(\) relative to \(_{o}\) (see Appendix A3.1 for formal statement and proof).

### Amortizing over observations

As outlined above, a regression NN will converge onto the cost function \((;_{o})\) for a particular observation \(_{o}\). However, naively applying this procedure would require retraining of the network for any new observation \(_{o}\), which prevents application of this method in time-critical or high-throughput scenarios. We therefore propose to _amortize_ cost estimation over a target distribution \(p(_{t})\). Specifically, a NN which receives as input a parameter \(\) and an independently sampled target datapoint \(_{t}\) will converge to \((;_{t})\) for all \(_{t}\) on the support of the target distribution (Fig. 2a,b), enabling estimation of the cost function for any pair of \((,_{t})\) (see Appendix A3.2 for formal statement and proof).

Naturally, we use the already simulated \( p()\) as target data during training, and therefore do not require further simulations. In order to have good accuracy on potentially misspecified observations, however, such datapoints should be within the support of the target distribution. Thus, in practice, we augment this target dataset with noisy simulations to broaden the support of \(p(_{t})\). Furthermore, if the set of observations (i.e., real data) is known upfront, they can also be appended to the target dataset during training. Lastly, to keep training efficient and avoid quadratic scaling in the number of simulations, we randomly subsample a small number of \(_{t}\) per \(\) in each training epoch (2 in our experiments), thus ensuring linear scaling as a function of simulation budget. Fig. 2a,b summarizes dataset construction and network training for ACE (details in Appendix A4.1).

### Sampling from the generalized posterior

Given a trained cost estimation network \(f_{}(,)\), an observed datapoint \(_{o}\), and a user-selected inverse temperature \(\), the generalized posterior probability (Eq. 1) can be computed up to proportionality for any \(\): \(p(|_{o})(- f_{}(, _{o}))p()\) and, thus, this term can be sampled with MCMC (Fig. 2c). The entire algorithm is summarized in Algorithm 1.

``` Inputs: prior \(p()\), simulator with implicit likelihood \(p(|)\), number of simulations \(N\), feedforward NN \(f_{}\) with parameters \(\), NN learning rate \(\), distance function \(d(,)\), noise level \(\), number of noise-augmented samples \(S\), inverse temperature \(\), number of target datapoints per \(\)\(N_{}\), \(K\) observations \(_{o}^{(1,,K)}\). Outputs:\(M\) samples from generalized posteriors given \(K\) observations. Generate dataset: \(,\{_{i} p(),_{i} p(|_{i})\}_{i:1 N}\) add noise and concatenate: \(_{}=[,\ _{1 S}+,\ _{o}],(,^{2})\) Training: whilenot convergeddo for\((,)\) in batchdo \(_{t}^{}\) sample \(N_{}\) datapoints from \(_{}\) for\(_{t}\) in \(_{t}^{}\)do \(+(f_{}(,_{t})-d( ,_{t}))^{2}\) \(-(_{})\) ; // and reset L to zero Sampling: for\(k[1,...,K]\)do  Draw \(M\) samples, with MCMC, from: \((- f_{}(,_{o}^{(k)}))\ p()\) ```

**Algorithm 1**Generalized Bayesian Inference with Amortized Cost Estimation (ACE)

### Considerations for choosing the value of \(\)

We note that the choice of value for \(\) is an important decision, though this is an issue not only for ACE but GBI methods in general . A good "baseline" value for \(\) is such that the average distance across a subset of the training data is scaled to be in the same range as the (log) prior probability, both of which can be computed on prior simulations. From there, increasing \(\) sacrifices sample diversity for predictive distance, and as \(\) approaches infinity, posterior samples converge onto the minimizer of the cost function. In practice, we recommend experimenting with a range of (roughly) log-spaced values since, as we show below, predictive sample quality tend to improve with increasing \(\).

## 4 Benchmark experiments

### Experiment setup

TasksWe first evaluated ACE on four benchmark tasks (modified from Lueckmann et al. ) with a variety of parameter- and data-dimensionality, as well as choice of distance measure: (1) **Uniform 1D**: 1D \(\) and \(\), the simulator implements an even polynomial with uniform noise likelihood, uniform prior (Fig. 2c); (2) **2 Moons**: 2D \(\) and \(\), simulator produces a half-circle with constant mean radius and radially uniform noise of constant width, translated as a function of \(\), uniform prior; (3) **Linear Gaussian**: 10D \(\) and \(\), Gaussian model with mean \(\) and fixed covariance, Gaussian prior; (4) **Gaussian Mixture**: 2D \(\) and \(\), simulator returns five i.i.d. samples from a mixture of two Gaussians, both with mean \(\), and fixed covariances, one with broader covariance than the other, uniform prior.

For the first three tasks, we use the mean-squared error between simulation and observation as the distance function. For the Gaussian Mixture task, we use maximum mean discrepancy (MMD\({}^{2}\)) to measure the statistical distance between two sets of five i.i.d. samples. Importantly, for each of the four tasks, we can compute the integral in Eq. 2 either analytically or accurately capture it with quadrature over \(\). Hence, we obtain the true cost \((;_{o})\) and subsequently the 'ground-truth' GBI posterior (with Eq. 1), and use that to draw, for each value of \(\) and \(_{o}\), 5000 samples (GT-GBI, black in Fig. 3). See Appendix A4.2 for more detailed descriptions of tasks and distance functions.

Training dataFor each task, we simulate 10,000 pairs of \((,)\) and construct the target dataset as in Fig. 2a, with 100 additional noise-augmented targets and 20 synthetic observations--10 well-specified and 10 misspecified--for a total of 10120 \(_{t}\) data points. Well-specified observations are additional prior predictive samples, while misspecified observations are created by moving prior predictive samples outside the boundaries defined by the minimum and maximum of 100,000 prior predictive simulations (e.g., by successively adding Gaussian noise). See Appendix A4.3 for details.

Test dataTo evaluate inference performance, we use ACE to sample approximate GBI posteriors conditioned on 40 different synthetic observations, 20 of which were included in the target dataset \(_{t}\), and 10 additional well-specified and misspecified observations which were not included in the target dataset. We emphasize that including observations in the target data is not a case of test data leakage, but represents a real use case where some experimental data which one wants to perform inference on are already available, while the network should also be amortized for unseen observations measured after training. Nevertheless, we report in Fig. 3 results for 'unseen' observations, i.e., not in the target dataset. Results are almost identical for those that were in the target dataset (Appendix A1). We drew 5000 posterior samples per observation, for 3 different \(\) values for each task.

MetricsWe are primarily interested in two aspects of performance: approximate posterior predictive distance and cost estimation accuracy. First, as motivated above, we want to find parameter

Figure 2: **Schematic of dataset construction, network training, and inference.****(a-b)** The neural network is trained to predict the distance between pairs of \(\) (red) and \(_{t}\) (green), as a noisy sample of the cost function (i.e., expected distance) evaluated on \(\) (grey) and \(_{t}\). **(c)** At inference time, the trained ACE network predicts the cost for any parameter \(\) given observation \(_{o}\) (top row), which is used to evaluate the GBI posterior under different \(\) (bottom row, darker for larger \(\)) for MCMC sampling without running additional simulations. The distance is well-defined and can be approximated even when the simulator is misspecified (dashed lines).

configurations which produce simulations that are as close as possible to the observation, as measured by the task-specific distance function. Therefore, we simulate using each of the 5000 ACE GBI posterior samples, and compute the average distance between predictive simulations and the observation. Mean and standard deviation are shown for well-specified and misspecified observations separately below (Fig. 3, 1st and 2nd columns). Second, we want to confirm that ACE accurately approximates \((;_{o})\), which is a prerequisite for correctly inferring the GBI posterior. Therefore, we compare the ACE-predicted and true cost across 5000 samples from each GBI posterior, as well as the classifier 2-sample test (C2ST, ) score between the ACE approximate and ground-truth GBI posterior (Fig. 3, 3rd and 4th columns). Note that cost estimation accuracy can be evaluated for parameter values sampled in any way (e.g., from the prior), but here we evaluate accuracy as samples become more concentrated around good parameter values, i.e., from GBI posteriors with increasing \(\). We expect that these tasks become increasingly challenging with higher values of \(\), since these settings require the cost estimation network to be highly accurate in tiny regions of parameter-space.

Other algorithmsAs a comparison against SBI methods that target the standard Bayesian posterior (but which nevertheless might produce good predictive samples), we also tested approximate Bayesian computation (ABC), neural posterior estimation (NPE, ), and neural likelihood estimation (NLE, ) on the same tasks. NPE and NLE were trained on the same 10,000 simulations, and 5000 approximate posterior samples were obtained for each \(_{o}\). We used the amortized (single-round) variants of both as a fair comparison against ACE. Additionally, to rule out the possibility that ACE is simply benefiting from the additional inverse temperature hyperparameter, we implemented a "tempered" version of NLE by scaling the NLE-approximated log-likelihood term (by the same \(\)s) during MCMC sampling. For ABC, we used the 10,000 training samples as a reference set, from which 50 were drawn as posterior samples with probability scaling inversely with the distance between their corresponding simulation and the observation, i.e., ABC with acceptance kernel .

Figure 3: **Performance on benchmark tasks.** ACE obtains posterior samples with low average distance to observations, and accurately estimates cost function. **Rows:** results for each task. **Columns:** average predictive distance compared to SBI methods and GT (1st and 2nd), cost estimation accuracy evaluated on ACE posterior samples for different \(\) (lighter blue shades are lower values of \(\)) (3rd), and C2ST accuracy relative to GT GBI posterior (4th, lower is better).

### Benchmark results

Overall, we see that for well-specified \(_{o}\) (i.e., observations for which the simulator is well-specified), ACE obtains GBI posterior samples that achieve low average posterior predictive simulation distance across all four tasks, especially at high values of \(\) (Fig. 3, 1st column). In comparison, ABC is worse for the Linear Gaussian task (which has a higher parameter dimensionality than all other tasks), whereas NPE, NLE, and tempered NLE achieve similarly low posterior predictive distances.

On misspecified observations, across all tasks and simulation-budgets (with the exception of Gaussian mixture on 10k simulations) we see that ACE achieves lower or equally low average posterior predictive simulation distance as neural SBI methods, even at moderate values of \(\) (Fig. 3, 2nd column, Figs. A4, A5). This is in line with our intuition that ACE returns a valid and accurate cost even if the simulator is incapable of producing data anywhere near the observation, while Bayesian likelihood and posterior probabilities estimated by NLE and NPE are in these cases nonsensical [28; 29; 30; 31]. Furthermore, simply concentrating the approximate Bayesian posterior via tempering does not lead to more competitive performance than ACE on such misspecified observations, and is sometimes even detrimental (e.g., tempered NLE at high \(\) on Uniform 1D and Linear Gaussian tasks). Therefore, we see that ACE can perform valid inference for a broad range of simulators, obtaining a distribution of posterior samples with predictive simulations close to observations, and is automatically robust against model-misspecification as a result of directly targeting the cost function.

For both well-specified and misspecified observations, ACE-GBI samples achieve posterior predictive distance very close to ground-truth (GT)-GBI samples, at all values of \(\) (Fig. 3, 1st and 2nd column), suggesting that ACE is able to accurately predict the expected distance. Indeed, especially for low to moderate values of \(\), the ACE-predicted cost closely matches the true cost (Fig. 3, 3rd column, light blue for well-specified \(_{o}\), Fig. A2 for misspecified). For higher values of \(\), ACE-predicted cost is still similar to true cost, although the error is, as expected, larger for very large \(\) (Fig. 3, 3rd column, dark blue).

As a result, highly concentrated generalized posteriors are estimated with larger (relative) discrepancies, which is reflected in the classifier 2-sample score (C2ST) between ACE and GT GBI posteriors (Fig. 3, 4th column): ACE posterior samples are indistinguishable from GT samples at low \(\), even for the 10D Linear Gaussian task, but becomes less accurate with increasing \(\). Nevertheless, predictive simulation distance dramatically increases with \(\) even when ACE is less accurate, suggesting that sampling to explicitly minimize a cost function which targets parameters with data-similar simulations is a productive goal. Relative performance results across algorithms are qualitatively similar when using a training simulation budget of 200 (Fig. A4) and 1000 (Fig. A5), but ABC required a sufficiently high simulation budget and performed poorly for 1000 training simulations or less.

## 5 Hodgkin-Huxley inference from Allen Cell Types Database recordings

Finally, we applied ACE to a commonly used scientific simulator and real data: we used a single-compartment Hodgkin-Huxley (HH) simulator from neuroscience and aimed to infer eight parameters of the simulator given electrophysiological recordings from the Allen Cell Types Database [32; 33; 34]. While this simulator can generate a broad range of voltage traces, it is still a crude approximation to _real_ neurons: it models only a subset of ion channels, it ignores the spatial structure of neurons, and it ignores many intracellular mechanisms . It has been demonstrated that parameters of the HH-model given _synthetic_ recordings can be efficiently estimated with standard NPE , but estimating parameters given _experimental_ recordings has been challenging  and has required ad-hoc changes to the inference procedure (e.g., Bernaerts et al.  added noise to the summary statistics, and Goncalves et al.  used a custom multi-round scheme with a particular choice of density estimator). We will demonstrate that ACE can successfully perform simulation-amortized inference given experimental recordings from the Allen Cell Types Database (Fig. 4a).

We trained NPE and ACE given 100K prior-sampled simulations (details in Appendix A4.4). After training, ACE accurately predicts the true cost of parameters given experimental observations (Fig. 4b). We then used slice sampling to draw samples from the GBI posterior for three different values of \(=\{25,50,100\}\) and for ten observations from the Allen Cell Types Database. Interestingly, the marginal distributions between NPE and ACE posteriors are very similar, especially for rather low values of \(\) (Fig. 4c, cornerplot in Appendix Fig. A7). The quality of posterior predictive samples, however, strongly differs between NPE and ACE: across the ten observations from the AllenCell Types database, only 35.6% of NPE posterior predictives produced more than five spikes (all observations have at least 12 spikes), whereas the ACE posterior predictives closely match the data, even for low values of \(\) (Fig. 4d, samples for all observations and all \(\) values in Figs. A8,A9,A10 and for NPE in Fig. A11. 66% (\(=25\)), 87% (\(=50\)), 96% (\(=100\)) of samples have more than five spikes). Indeed, across all ten observations, the average posterior predictive distance of ACE was significantly smaller than that of NPE, and for large values of \(\) the distance is even less than half (Fig. 4e). Finally, for rejection-ABC, only the top \(35\) samples (out of the full training budget of 100K simulations) had a distance that is less than the _average_ posterior predictive distance achieved by ACE.

To investigate these differences between NPE and ACE, we also evaluated NPE posterior predictive performance on synthetic data (prior predictives) and found that it had an average predictive distance of 0.189, which roughly matches the performance of ACE on the experimental observations (0.174 for \(\)=50). This suggest that, in line with previous results , NPE indeed struggles with _experimental_ observations, for which the simulator is inevitably imperfect. We then trained NPE with 10 times more simulations (1M in total). With this increased simulation budget, NPE performed significantly better than with 100K simulations, but still produced poorer predictive samples than ACE trained with 100K simulations (for \(=\{50,100\}\)), although the marginals were similar between NPE (1M) and ACE (100K) (Fig. A6, samples for all observations in Appendix Fig. A12).

Overall, these results demonstrate that ACE can successfully be applied to real-world simulators on which vanilla NPE fails. On the Hodgkin-Huxley simulator, ACE generates samples with improved predictive accuracy despite an order of magnitude fewer simulations and despite the marginal distributions being similar to those of NPE.

## 6 Discussion

We presented ACE, a method to perform distance-aware inference for scientific simulators within the Generalized Bayesian Inference (GBI) framework. Contrary to'standard' simulation-based inference (SBI), our method does not target the Bayesian posterior, but replaces the likelihood function with a cost function. For real-world simulators, doing so can provide practical advantages over standard Bayesian inference:

Figure 4: **Applicaton of ACE to Allen data.****(a)** Three observations from the Allen Cell Types Database. **(b)** True cost (evaluated as Monte-Carlo average over 10 simulations) per \(\) vs ACE-predicted cost. Colors are different observations. **(c)** Marginals of posterior distributions for NPE (orange) and ACE (shades of blue. Light blue: \(=25\), medium blue: \(=50\), dark blue: \(=100\)). **(d)** Top: Two GBI predictive samples for each observation. Bottom: Two NPE predictive samples. Additional samples in Appendix A8-A11. **(e)** Average predictive distance to observation for NPE and ACE with \(=\{25,50,100\}\).

First, the likelihood function quantifies the probability that a parameter generates data which _exactly_ matches the data. However, in cases where the model is a rather crude approximation to the real system being studied, scientists might well want to include parameters that can generate data that is sufficiently close (but not necessarily identical) in subsequent analyses. Our method makes this possible, and is advantageous over other GBI-based methods since it is amortized over observations and the inverse temperature \(\). Second, many simulators are formulated as noise-free models, and it can be hard to define appropriate stochastic extensions (e.g., ). In these cases, the likelihood function is ill-defined and, in practice, this setting would require'standard' SBI methods, whose density estimators are generally built to model continuous distributions, to model discrete jumps in the posterior density. In contrast, our method can systematically and easily deal with noise-free simulators, and in such situations more closely resembles parameter-fitting algorithms. Lastly, standard Bayesian inference is challenging when the model is misspecified, and the performance of neural network-based SBI methods can suffer drastically in this scenario .

### Related work

GBI for Approximate Bayesian ComputationSeveral studies have proposed methods that perform GBI on simulators with either an implicit (i.e., simulation-based) likelihood or an unnormalized likelihood. Wilkinson et al.  argued that rejection-ABC performs exact inference for a modified model (namely, one that appends an additive uniform error) instead of approximate inference for the original model. Furthermore, ABC with arbitrary probabilistic acceptance kernels can also be interpreted as having different error models, and Schmon et al.  integrate this view to introduce generalized posteriors for ABC, allowing the user to replace the hard-threshold kernel (i.e., \(\)-ball of acceptance) with an arbitrary loss function that measures the discrepancy between \(\) and \(_{o}\) for MCMC-sampling of the approximate generalized posterior.

Other recent GBI methods require a differentiable simulator [16; 15] or build tractable cost functions that can be sampled with MCMC [14; 18], but this still requires running simulations _at inference time_ (i.e., during MCMC) and does not amortize the cost of simulations and does not reuse already simulated datapoints.

Finally, Bayesian Optimization for Likelihood-free Inference (BOLFI, ) and error-guided LFI-MCMC  are not cast as generalized Bayesian inference approaches, but are related to ACE. Similarly as in ACE, they train models (for BOLFI, a Gaussian process and, for error-guided LFI-MCMC, a classifier) to estimate the discrepancy between observation and simulation. In BOLFI, the estimator is then used to iteratively select new locations at which to simulate. However, contrary to ACE, neither of these two methods amortizes the cost of simulations over observations.

Misspecification-aware SBISeveral other methods have been proposed to overcome the problem of misspecification in SBI: For example, Bernaerts et al.  add noise to the summary statistics in the training data, Ward et al.  use MCMC to make the misspecified data well-specified, and Kelly et al.  introduce auxiliary variables to shift the (misspecified) observation towards being well-specified. All of these methods, however, maintain that the inference result should be the 'as close as possible' version of the posterior distribution. Contrary to that, our method does _not_ aim to obtain the Bayesian posterior distribution (which, for misspecified models, can often be nonsensical or even undefined if the evidence is zero), but is specifically targeted towards parameter regions that are a specified distance from the observation. More broadly, recent advances in uncertainty quantification in deep neural networks, where standard mean-predicting regression networks are supplemented with a uncertainty- or variance-predicting network [44; 45], may serve to further connect loss-minimizing deep learning with (misspecification-aware) SBI.

### Limitations

While our method amortizes the cost of simulations and of training, it still requires another method to sample from the posterior distribution. We used multi-chain slice-sampling  for efficiency, but any other MCMC algorithm, as well as variational inference, could also be employed [47; 48]. While sampling incurs an additional cost, this cost is generally small in comparison to potentially expensive simulations.

In addition, our method can perform inference for distance functions which can be written as expectations over the likelihood. As we demonstrated, this applies to many popular and widely used distances. Our method can, however, not be applied to arbitrary distance functions (e.g., the minimum distance between all simulator samples and the observation). While the distances we investigated here are certainly useful to practitioners, they do not necessarily fulfill the criterion of being 'proper' scoring rules . Furthermore, we note that the cost functions considered here by default give rise to unnormalized generalized likelihoods. Therefore, depending on whether the user aims to approximate the generalized posterior given the normalized or unnormalized likelihood, different MCMC schemes should be used in conjunction with ACE (standard MCMC vs. doubly-intractable MCMC, e.g., the Exchange algorithm ).

Compared to'standard' SBI, GBI introduces an additional hyperparameter to the inference procedure, the inverse temperature \(\). This hyperparameter has to be set by the user and its choice strongly affects inference behaviour: low values of \(\) will include regions of parameter-space whose data do not necessarily match the observation closely, whereas high values of \(\) constrain the parameters to only the best-fitting parameter values. While we provide heuristics for selecting \(\), we acknowledge the inconvenience of an additional hyperparameter. However, our method is amortized over \(\), which makes exploration of different \(\) values possible, and which could simplify automated methods for setting \(\), similar to works where \(\) is taken as the exponent of the likelihood function .

Finally, as with any method leveraging deep neural networks, including neural density estimator-based SBI methods (such as NPE), sensitivity to the number of training samples and the dimensionality of the task should always be considered. As we demonstrate above, increasing simulation budget improves the performance of any algorithm, and a reasonable number of training simulations yielded improved performance on a real-world neuroscience application, while the amortization property shifts the cost of simulations up front. In addition, we consider tasks up to 10 dimensions here, as most existing SBI methods have been benchmarked and shown to perform adequately on such tasks , though it remains to be seen how ACE can extend to higher dimensional parameter-space and data-space and whether embedding networks will be similarly helpful.

## 7 Conclusion

We presented a method that performs generalized Bayesian inference with amortized cost estimation. Our method produces good predictive samples on several benchmark tasks, especially in the case of misspecified observations, and we showed that it allows amortized parameter estimation of Hodgkin-Huxley models given experimental recordings from the Allen Cell Types Database.

## 8 Acknowledgements

RG is supported by the European Union's Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement No. 101030918 (AutoMIND). MD is supported by the International Max Planck Research School for Intelligent Systems (IMPRS-IS). The authors are funded by the Machine Learning Cluster of Excellence, EXC number 2064/1-390727645. This work was supported by the Tubingen AI Center (Agile Research Funds), the German Federal Ministry of Education and Research (BMBF): Tubingen AI Center, FKZ: 01IS18039A, and the German Research Foundation (DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mechanisms, project number: 276693517.

We would like to thank Jan Boelts, Janne Lappalainen, and Auguste Schulz for feedback on the manuscript, Julius Vetter for feedback and discussion on proper scoring rules, Poormina Ramesh and Mackelab members for extensive discussions throughout the project, as well as Francois-Xavier Briol for suggestions on sampling doubly-intractable posteriors, and the reviewers for their constructive comments on the readability of the manuscript and suggestions for additional analyses.