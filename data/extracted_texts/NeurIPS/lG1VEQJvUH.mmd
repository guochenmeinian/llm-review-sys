# Unitary convolutions for learning on graphs and groups

Bobak T. Kiani

John A. Paulson School of Engineering and Applied Sciences, Harvard; e-mail: bkiani@g.harvard.edu

Lukas Fesser

John A. Paulson School of Engineering and Applied Sciences, Harvard; e-mail: lukas_fesser@fas.harvard.edu

Melanie Weber

John A. Paulson School of Engineering and Applied Sciences, Harvard; e-mail: mweber@g.harvard.edu

###### Abstract

Data with geometric structure is ubiquitous in machine learning often arising from fundamental symmetries in a domain, such as permutation-invariance in graphs and translation-invariance in images. Group-convolutional architectures, which encode symmetries as inductive bias, have shown great success in applications, but can suffer from instabilities as their depth increases and often struggle to learn long range dependencies in data. For instance, graph neural networks experience instability due to the convergence of node representations (over-smoothing), which can occur after only a few iterations of message-passing, reducing their effectiveness in downstream tasks. Here, we propose and study _unitary group convolutions_, which allow for deeper networks that are more stable during training. The main focus of the paper are graph neural networks, where we show that unitary graph convolutions provably avoid over-smoothing. Our experimental results confirm that unitary graph convolutional networks achieve competitive performance on benchmark datasets compared to state-of-the-art graph neural networks. We complement our analysis of the graph domain with the study of general unitary convolutions and analyze their role in enhancing stability in general group convolutional architectures.4

## 1 Introduction

In recent years, the design of specialized machine learning architectures for structured data has received a surge of interest. Of particular interest are architectures for data domains with inherent symmetries, such as permutation-invariance in graphs and sets, translation-invariance in images, and other symmetries that arise from fundamental laws of physics in scientific data.

Group-convolutional architectures allow for explicitly encoding symmetries as inductive biases, which has led to performance gains in scientific applications ; theoretical studies have analyzed the impact of geometric inductive biases on the complexity of the architecture . Graph Neural Networks are among the most popular architectures for graph machine learning and have found impactful applications in a wide range of disciplines, including in chemistry , drug discovery , particle physics , and recommender systems . However, despite these successes, several limitations remain. A notable difficulty is the design of stable, deep architectures. Many of the aforementioned applications require accurate learning of long-range dependencies in the data, which necessitates deeper networks. However, it has been widely observed that group-convolutional networks suffer from instabilities as their depths increases. On graph domains, these instabilities have been studied extensively in recent years, notably in the form of _over-smoothing_ effects , which characterizes the fast convergence of the representations of nearby nodes with depth. This effect can often be observed after only a few iterations of message-passing (i.e., small number of layers) and can significantly decrease the utility of the learned representations in downstream tasks. While interventions for mitigating over-smoothing have been proposed, including targeted perturbations of the input graph's connectivity (rewiring) and skip connections, a more principled architectural approach with theoretical guarantees is stilllacking. Similar effects, such as exploding or vanishing gradients, have also been studied in more general group-convolutional architectures, specifically in CNNs , for which architectural interventions (e.g., skip connections) have been proposed.

In this work, we take a different route. Inspired by a long line of work studying unitary recurrent neural networks , we propose to replace the standard group convolution operator with a _unitary_ group convolution. By construction, the unitarity ensures that the linear transformations are norm-preserving and invertible, which can significantly enhance the stability of the network and avoid convergence of representations to a fixed point as its depth increases. We introduce two unitary graph convolution operators, which vary in the way the message passing and feature transformation are parameterized. We then generalize this approach to cover more general group-convolutional architectures.

Our theoretical analysis of the proposed unitary graph convolutions shows that they enhance stability and prevent over-smoothing effects that decrease the performance of their vanilla counterparts. We further describe how generalized unitary convolutions avoid vanishing and exploding gradients, enhancing the stability of group-convolutional architectures without additional interventions, such as residual connections or batch normalization.

### Related work

We now provide a brief background into work that motivated and inspired this current study, deferring a more complete discussion to App. A. Unitary matrices have a long history of application in neural networks, specifically related to improving stability for deep networks  enhancing the learning of long-range dependencies in data .  implemented unitary matrices in recurrent neural networks to address issues with the challenge of vanishing and exploding gradients inherent to learning long sequences of data in RNNs . These original algorithms were later improved to be more expressive while still being efficient to implement in practice . For graph convolution,  discuss applications of the exponential map to linear convolutions; here, we use this same exponential map to explicitly apply unitary operators parameterized in the Lie algebra. For image data, various works  design and analyze variants of orthogonal or unitary convolution used in CNN layers. These can be viewed as a particular instance of the group convolution we study here over the cyclic group. More recently, proposals for unitary or orthogonal message passing have shown improvements in stability and performance compared to conventional message passing approaches . However, in contrast to our work, these methods do not always implement a unitary transformation across the whole input (e.g. only applying it in the feature transformation) and in the case of  can be computationally expensive to implement for large graphs (see App. A for more detail).

## 2 Background and Notation

We denote scalars, vectors, and matrices as \(c,\), and \(\) respectively. Given a matrix \(\), its conjugate transpose and transpose are denoted \(^{}\) and \(^{}\). Given two matrices, \(\) and \(\), we denote their tensor product (or Kronecker product) as \(\). Given a vector \(\), its standard Euclidean norm is denoted \(\|\|\). For a matrix \(\), we denote its operator norm as \(\|\|\) and Frobenius norm as \(\|\|_{F}\).

Group Theory BasicsSymmetries ("invariances") describe transformations, which leave properties of data unchanged ("invariant"), and as such characterize the inherent geometric structure of the data domain. Algebraically, symmetries can be characterized as groups. We say that a group is a matrix _Lie group_, if it is a differentiable manifold and a subgroup of the set of invertible \(n n\) matrices (see App. C). Lie groups are associated with a Lie algebra, a vector space, which is formed by its tangent space at the identity. A comprehensive introduction into Lie groups and Lie algebras can be found in . Throughout this work we will encounter the \(n\)-dimensional orthogonal \(O(n)\) and unitary \(U(n)\) Lie groups, which are defined as

\[O(n)=\{^{n n}:^{}=\}, U(n)=\{^{n n}:^{}=\}.\] (1)Lie algebras of \(O(n)\) and \(U(n)\) are the set of skew symmetric \((n)\) and skew Hermitian \((n)\) matrices respectively, i.e.,

\[(n)=\{^{n n}:+^{}= 0\},(n)=\{^{n n}: +^{}=0\}.\] (2)

Given a matrix \((n)\) (or \((n)\)), the matrix exponential maps the matrix to an element of the Lie group \(() O(n)\) (or \(U(n)\)). More details on unitary parametrizations can be found in App. C.2.

Graph Neural NetworksWe denote graphs by \(=(V,E)\) where \(V\) and \(E\) denote the set of nodes and edges respectively. For a graph on \(n\) nodes, unless otherwise specified, we let \(V=\{1,,n\}\) index the nodes and denote the adjacency matrix by \(^{n n}\) and node features \(^{n d}\). We also often use \(^{n n}\) to denote the diagonal degree matrix where diagonal entry \(i\) records the degree of node \(i\). The normalized adjacency matrix is defined as \(}=^{-1/2}^{-1/2}\). Given a node feature matrix \(^{n d_{}}\) where row \(i\) denotes the \(d_{}\)-dimensional feature vector of node \(i\), graph convolution operators take the general form 

\[f_{}(;)=_{0}+_{1}++ ^{k}_{k},\] (3)

where \(_{0},_{k}^{d_{} d_{}}\) are trainable parameters. For ease of presentation, we will often omit the adjacency matrix as an explicit input to the operation. Often, only a single "message passing" step is included and the operation takes the simple form \(f_{}()=\). Eq. (3) is equivariant under any permutation matrix \(_{}^{n n\,5}\) since

\[f_{}(_{};_{}_{}^{-1})=_{} f_{}(;).\] (4)

We aim to parameterize a particular subset of these operations which preserve unitarity properties.

Group-Convolutional Neural NetworksIn general, a linear convolution operation \(_{G}:^{n c}^{n c}\) takes the form of a weighted sum over linear transformations that are equivariant to a given group \(G\). For simplicity, we assume here that we are working with finite groups though this can be generalized to other settings . Given an input \(^{n c}\) consisting of \(c\) channels in a vector space of dimension \(n\), we study convolutions of the form

\[_{G}()=_{i=1}^{m}_{i}_{i},\] (5)

where \(_{1},,_{m}^{n n}\) are linear operators equivariant to the group \(G\) and \(_{1},,_{m}^{c c}\) are parameterized weight matrices. The graph setting is recovered by setting \(_{k}=^{k}\). Similarly, for cyclic convolution as in conventional CNNs, one sets \(_{k}\) to be the circulant matrices sending basis vector \(_{k}_{i}=_{i+k}\) where indexing is taken mod \(n\).

## 3 Unitary Group Convolutions

We first describe unitary convolution for data on graphs (equivariant to permutations) which is the main focus of our study and then detail general procedures for performing unitary convolutions equivariant to general finite groups. Implementing these operations often requires special considerations to handle nonlinearities, complex numbers, initialization, etc. which we discuss in App. E.

### Unitary graph convolution

We introduce two variants of unitary graph convolution, which we denote as _Separable unitary convolution_ (short _UnConv_) and _Lie orthogonal/unitary convolution_ (short _Lie UniConv_). UniConv is a simple adjustment to standard message passing and treats linear transformations over nodes and features separately. Lie UniConv, in contrast, parameterizes operations in the Lie algebra of the orthogonal/unitary groups. This operation is fully unitary, but does not have the tensor product nature of the separable UniConv.

By introducing complex numbers, we can enforce unitarity separately in the message passing and feature transformation.

**Definition 1** (Separable unitary graph convolution (UniConv)).: Given an undirected graph \(\) over \(n\) nodes with adjacency matrix \(^{n n}\), separable unitary graph convolution (UniConv) \(f_{}:^{n d}^{n d}\) takes the form

\[f_{}()=(it),^{ }=,\] (6)

where \( U(d)\) is a unitary operator and \(t\) controls the magnitude of the convolution.

One feature of the complexification of the adjacency matrix is that messages propogate as "waves" as observed for example in Fig. 1. Since \(\) is a symmetric matrix, \((it)\) is unitary for all values of \(t\) and corresponds to vanilla message passing up to first order: \((it)+it+O(t^{2})\).

**Remark 2**.: We observe that performance on real-world tasks is usually improved when enforcing unitarity in the node message passing where oversmoothing occurs, but not necessarily when enforcing unitarity in the feature transformation \(\). Thus, one can choose to leave \(\) in Eq. (6) as a fully parameterized (unconstrained) matrix as we often do in our experiments.

More generally, one can parameterize the operation in the Lie algebra by first forming a skew Hermitian convolution operation \(g_{}:^{n d}^{n d}\) and then applying the exponential map. This approach has the benefit that it can be fully implemented using real numbers to obtain an orthogonal operator by enforcing constraints in the real part of the weight matrix \(\) only.

**Definition 3** (Lie orthogonal/unitary graph convolution (Lie UniConv)).: Given an undirected graph \(\) over \(n\) nodes with adjacency matrix \(^{n n}\), Lie unitary/orthogonal graph convolution (OrthoConv) \(f_{}:^{n d}^{n d}\) takes the form

\[ f_{}()&=(g_{ })()=_{k=0}^{}}^{(k)}()}{k!}=+g_{}()+g_{}(g_{ }())+,\\ g_{}()&=, +^{}=0.\] (7)

The operation of \(g_{}\) can be represented as a vector-matrix operation \((g_{}())=^{}\,(X)\) where \(^{}\) belongs in the Lie algebra. If \(\) is real-valued, the above returns an orthogonal map since the exponential of a real-valued matrix is real-valued.

Implementing the exponential mapThe exponential map in Definitions 1 and 3 can be performed using accurate approximations with typically constant factor overhead in runtime. We use the simple \(K\)-th order Taylor approximation

\[(M)=_{k=0}^{K}^{k}}{k!}+O(\|^{K+1}}{(K+ 1)!}).\] (8)

For experiments, we find that setting \(K=10\) suffices in all cases as the error exponentially decreases with \(K\). Various other accurate and efficient approximations exist as detailed in App. C.2. We also

Figure 1: Comparison of standard linear message passing with iterates \(_{L+1}=c(_{L}+_{L})\) versus unitary message passing with iterates \(_{L+1}=(i)_{L}\) for a graph of 80 nodes connected as a ring. The unitary message passing has a wave-like nature which ensures messages “propagate” through the graph. In contrast, the standard message passing has a unique fixed point corresponding to the all ones vector which inherently causes oversmoothing in the features. Here, \(c\) is chosen to ensure the operator norm of the matrix \(+\) is bounded by one.

refer the reader to App. E for other implementation details associated to handling complex numbers, initialization, etc.

### Generalized unitary convolutions

In the more general setting, we are concerned with parameterizing unitary operations of the form

\[_{G}()=_{i=1}^{m}_{i}_{i},\] (9)

where \(_{1},,_{m}^{n n}\) are linear operators equivariant to the group \(G\) and \(_{1},,_{m}^{c c}\) are parameterized weight matrices (e.g., set \(_{k}=^{k-1}\) to recover graph convolution). One can enforce and parameterize unitary convolutions in Eq. (9) in the Lie algebra basis or in the Fourier domain as detailed in App. D.

```
0: equivariant linear operator \(^{n}^{n}\)
0: vector \(^{n}\)
1:\(}=(-^{})\) (skew symmetrize operator)
2:return\((})()\) (or approximation thereof) ```

**Algorithm 1** Unitary map from Lie algebra

In the Lie algebraic setting (Algorithm 1), one explicitly parameterizes operators in the Lie algebra or orthogonally projects arbitrary linear operators onto this basis by the mapping \((-^{})/2\). This parameterization is particularly simple to implement (it is a linear basis) and unitary operators are subsequently implemented by applying the exponential map. This setting covers previous implementations of unitary RNNs and CNNs  and is detailed in Sec. 3.1 for GNNs.

**Example 1** (Convolution on regular representation (Lie algebra)).: Given a group \(G\) and vector space \(\) of dimension \(|G|\) with basis \(\{_{g}:g G\}\), then the left action \(_{g}\) (right action \(_{g}\)) of any \(g G\) is a permutation \(_{g}_{h}=_{g^{-1}h}\) (\(_{g}_{h}=_{hg}\)). Let \(^{|G|}\) be the vectorized form of an input function \(x:G\) and \(m:G\) the filter for convolution6

\[(m x)(u)=_{v G}m(u^{-1}v)x(v)_{G}()=[_{g G}m(g)_{g}].\] (10)

Parameterizing operations on the Lie algebra simply requires that \(m(g)=m(g^{-1})^{*}\) since \(_{g}^{-1}=_{g}^{}\).

An example implementation of the above for a toy learning task on the dihedral group is in App. F.3. One can also generally implement convolutions in the (block diagonal) Fourier basis of the graph or group (see App. D and Algorithm 3). Here, one employs a Fourier operator which block diagonalizes the input into its irreducible representations or some spectral representation. Fourier representations often have the advantage of being faster to implement due to efficient Fourier transforms. Since we do not use this in our experiments, we defer the details to App. D.

## 4 Properties and theoretical guarantees

Unitary operators are now well studied in the context of neural networks and have various properties that are useful in naturally enhancing stability and performance of learning architectures . These properties and their theoretical guarantees are outlined here. We defer all proofs to App. B, many of which follow immediately from the definition of unitarity.

Throughout, we will assume that convolution operators act on a vector space \(\) and are built from a basis of linear operators that is equivariant to input and output representation \((g)\) of a group \(G\). We set input/output representations to be equal so that the exponential map of an equivariant operator is itself equivariant.

**Fact 1** (Basic properties).: _Any unitary convolution \(f_{}:\) built from Algorithms 1 and 3 meets the basic properties:_

\[&\;f_{ }^{-1}::f_{}^{-1}(f_{}())= ,\\ &: \;\|f_{}()\|=\|\|,\\ &(g) f_{}=f_{}(g).\] (11)

A simple corollary of the above isometry property leveraged in prior work on unitary CNNs  is that unitary matrices naturally provide robustness guarantees and a provable means to bound the effects of adversarial perturbations (see Corollary 9 in App. B). For graphs, we note that the properties above are generally impossible to obtain with graph convolution operations that perform a single message passing step as we show below.

**Proposition 4**.: _Let \(f_{}:^{n d}^{n d}\) be a graph convolution layer of the form_

\[f_{}(,)=_{0}+_{1},\] (12)

_where \(_{0},_{1}^{d d}\) are parameterized matrices. The linear map \(f(,):^{n d}^{n d}\) is orthogonal for all adjacency matrices \(\) of undirected graphs only if \(_{1}=\) and \(_{0} O(d)\) is orthogonal. Furthermore, denoting \(_{}^{nd nd}\) as the Jacobian matrix of the map \(f_{}(,)\), for any choice of \(_{0},_{1}\), there always exists a normalized adjacency matrix \(}\) such that_

\[\|_{}}^{}_{}}-\| _{1}\|_{F}^{2}}{2d},\] (13)

_where \(\|\|\) is the operator norm of matrix \(\)._

The above shows that one must apply higher powers of \(\) as in the exponential map to achieve a linear operator that is close to orthogonal.

OversmoothingDuring the training of GNNs we often observe that the features of neighboring nodes become more similar as the depth of the networks (i.e., the number of message-passing iterations) increases. This "oversmoothing" phenomenon has a strong connection to the spectral properties of graphs where convergence of a function on the graph is measured through the Dirichlet form7 or its normalized variant also termed the Rayleigh quotient.

**Definition 5** (Rayleigh quotient ).: Given an undirected graph \(=(V,E)\) on \(|V|=n\) nodes with adjacency matrix \(\{0,1\}^{n n}\), let \(^{n n}\) be a diagonal matrix where the \(i\)-th entry \(_{ii}=d_{i}\) and \(d_{i}\) is the degree of node \(i\). Let \(f:V^{d}\) be a function from nodes to features. Then the Rayleigh quotient \(R_{}(f)\) is equal to

\[R_{}(f)=,) E}\|)}{}}}-)}{}}}\|^{2}} {_{}\|f()\|^{2}}=( ^{}(-}}))}{\|\|_{F}^ {2}},\] (14)

where \(}}=^{-1/2}^{-1/2}\) is the normalized adjacency matrix and \(^{n d}\) is a matrix with the \(i\)-th row set to feature vector \(f(i)\). We will at times abuse notation and let \(\) be an input to \(R_{}()\).

Given an undirected graph \(\) on \(n\) nodes with normalized adjacency matrix \(}}=^{-1/2}^{-1/2}\), we compare the Rayleigh quotient of normalized vanilla and unitary convolution. First, it is straightforward to show that the Rayleigh quotient is invariant to unitary transformations giving a proof that unitary graph convolution avoids oversmoothing.

**Proposition 6** (Invariance of Rayleigh quotient).: _Given an undirected graph \(\) on \(n\) nodes with normalized adjacency matrix \(}}=^{-1/2}^{-1/2}\), the Rayleigh quotient \(R_{}()=R_{}(f_{}())\) is invariant under normalized unitary or orthogonal graph convolution (see Definitions 1 and 3)._

In contrast, oversmoothing commonly occurs with vanilla graph convolution and has been proven to occur in a variety of settings . To illustrate this, we exhibit a simple setting below commonly found at initialization where the parameterized matrix is set to be an orthogonal matrix and input features are random. Here, the magnitude of oversmoothing concentrates around its average and grows with the value of \((}}^{3})\) which corresponds to the (weighted) number of triangles in the graph.

**Proposition 7**.: _Given a simple undirected graph \(\) on \(n\) nodes with normalized adjacency matrix \(}=^{-1/2}^{-1/2}\) and node degree bounded by \(D\), let \(^{n d}\) have rows drawn i.i.d. from the uniform distribution on the hypersphere in dimension \(d\). Let \(f_{}()=}\) denote convolution with orthogonal feature transformation matrix \( O(d)\). Then, the event below holds with probability \(1-(-())\):_

\[R_{}() 1-O(})  R_{}(f_{}()) 1-( }^{3})}{(}^{2})}+O( {n^{1/4}}).\] (15)

Vanishing/Exploding gradientsA commonly observed issue in training deep neural networks, especially RNNs with evolving hidden states, is that gradients can exponentially grow or vanish with depth . In fact, one of the original motivations for using unitary matrices in RNNs is to directly avoid this vanishing/exploding gradient problem . From a theoretical view, prior work has shown that carefully initialized layers have Jacobians that meet variants of the dynamical isometry property commonly studied in the mean field theory literature characterizing the growth/decay over layers of the network . We analyze a version of this property here and discuss it in the context of our work.

**Definition 8** (Dynamical isometry).: Given functions \(f_{1},,f_{L}:^{n}^{n}\), let \(F_{i}=f_{i} f_{1}\). Let \(_{F_{i}}()\) be the Jacobian matrix of \(F_{i}\) at \(^{n_{S}}\). The function \(F_{L}=f_{L} f_{1}\) is dynamically isometric up to \(\) at \(^{n}\) if there exists orthogonal matrix \( O(n)\) such that \(\|_{i=1}^{L}_{F_{i}}()-\|\) where \(\|\|\) denotes the operator norm.

Network layers that meet the dynamical isometry property generally avoid vanishing/exploding gradients. The particular form we analyze is stricter than those studied in the mean field and Gaussian process literature which analyze the distribution of singular values over the randomness of the weights . Unitary convolution layers followed by isometric activations are examples of dynamical isometries that hold throughout training as exemplified below.

**Example 2**.: Compositions of the layer \((f_{}())\) consisting of the unitary convolution layer (Definition 1) followed by the Group Sort activation (Eq. (61)) are perfectly dynamically isometric (\(=0\)) at all \(^{n}\)9.

## 5 Experiments

Our experimental results here show that unitary/orthogonal variants of graph convolutional networks perform competitively on various graph learning tasks. Due to space constraints, we present experiments on additional datasets and architectures in App. F. This includes experiments on TU-Dataset  and an instance of unitary group convolutional networks on the dihedral group where the goal is to learn distances between pairs of elements in the dihedral group. Training procedures and hyperparameters are reported in App. G. Reported results in tables are over the mean plus/minus standard deviation.

Toy model: graph distanceTo analyze the ability of our unitary GNN to learn long-range dependencies, we consider a toy dataset where the aim is to learn the distance between two indicated nodes on a large graph connected as a ring. This task is inspired by similar toy models on ring graphs where prior work has shown that message passing architectures fail to learn long-range dependencies between distant nodes . The particular dataset we analyze consists of a training set of \(N=1000\) graphs on \(n=100\) nodes where each graph is connected as a ring (see Fig. 1(a)). Node features \(_{i}\) are a single number set to zero for all but two randomly chosen nodes whose features are set to one. The goal is to predict the distance between these two randomly chosen nodes. For a graph of \(n\) nodes, conventional message passing architectures require at least \(n/2\) sequential messages to fully learn this dataset. As shown in Fig. 1(b), conventional message passing networks fail to learn this task whereas the unitary convolutional architecture succeeds. We refer the reader to App. F.1 for further details and results for additional architectures.

Long Range Graph Benchmark (LRGB)We consider the Peptides, Coco, and Pascal datatsets from the Long Range Graph Benchmark (LRGB) . There are two tasks associated with Peptides, a peptide function classification task (Peptides-func) and a regression task (Peptides-struct). Coco and Pascal are node classification tasks.Table 1 shows that the Unitary GCN outperforms standard message passing architectures and is competitive with other state of the art architectures as well, many of which employ global attention mechanisms. This provides evidence that the unitary GCN is nearly as effective at learning long-range signals.

Heterophilous Graph DatasetFor node classification, we consider the Heterophilous Graph Dataset proposed by . The dataset contains the heterophilous graphs Roman-empire, Amazon-ratings, Minesweeper, Tolokers, and Questions, which are often considered as a benchmark for evaluating the performance of GNNs on graphs where connected nodes have dissimilar labels and features. Our results in Table 2 show that the unitary GCN outperforms the baseline message passing and graph transformer models. Given the heterophilous nature of this dataset, these findings reinforce the notion that unitary convolution enhances the ability of convolutional networks to capture long-range dependencies.

## 6 Discussion

In this paper we introduced unitary graph convolutions for enhancing stability in graph neural networks. We provided theoretical and empirical evidence for the effectiveness of our approach. We further introduce an extension to general groups (generalized unitary convolutions), which can be leveraged in group-convolutional architectures to enhance stability.

    &  &  &  &  \\  &  &  &  &  \\   GCN \({}^{}\) \\ GINE \\  } & \(0.6860 0.0050\) & \(0.2460 0.0007\) & \(0.1338 0.0007\) & \(0.2078 0.0031\) \\  & GINE \({}^{}\) & \(0.6621 0.0067\) & \(0.2473 0.0017\) & \(0.2125 0.0009\) & \(0.2718 0.0054\) \\  & GATEDGCN \({}^{}\) & \(0.6765 0.0047\) & \(0.2477 0.0009\) & \(0.2922 0.0018\) & \(0.3880 0.0040\) \\  & GUMP & \(0.6843 0.0037\) & \(0.2564 0.0023\) & - & - \\   GCN \({}^{}\) \\ GINE \\  } & \(0.6534 0.0091\) & \(0.2509 0.0014\) & \(\) & \(0.4440 0.0065\) \\  & DREW & \(0.7150 0.0044\) & \(0.2536 0.0015\) & - & \(0.3314 0.0024\) \\  & Explorer & \(0.6527 0.0043\) & \(0.2481 0.0007\) & \(0.3430 0.0008\) & \(0.3960 0.0027\) \\  & GRIT & \(0.6988 0.0082\) & \(0.2460 0.0012\) & - & - \\  & Graph Vit & \(0.6942 0.0075\) & \(0.2449 0.0016\) & - & - \\  & CRAWL & \(0.7074 0.0032\) & \(0.2506 0.0022\) & - & \(\) \\   ResNet \\  } & UniGCN & \(0.7072 0.0035\) & \(\) & \(0.2852 0.0016\) & \(0.3516 0.0070\) \\  & Lie UniGCN & \(\) & \(0.2460 0.0011\) & \(0.3153 0.0035\) & \(0.4005 0.0067\) \\   

Table 1: Unitary GCN with UniConv (Definition 1) and Lie UniConv (Definition 3) layers compared with other GNN architectures on LRGB datasets . Top performer bolded and second/third underlined. Networks are set to fit within a parameter budget of \(500,000\) parameters. Complex numbers are counted as two parameters each. See App. G for additional details.

Figure 2: (a) Example datapoint on \(n=25\) nodes; the target is \(y=5\) (distance between red nodes). (b) Results for the ring toy model problem with \(100\) nodes where the unitary GCN with UniConv or Lie UniConv layers is the only message passing architecture able to learn successfully. Best performance over networks with \(5\), \(10\), and \(20\) layers is plotted. Other architectures typically perform best with \(5\) layers and only learn shorter distances (see App. F.1).

LimitationsPerhaps the biggest challenge in working with unitary convolutions is the overhead associated with maintaining unitarity or orthogonality via approximations of the exponential map or diagonalizations in Fourier or spectral bases. For implementing unitary maps, working with complex numbers also requires different initialization and activation functions. We refer the reader to App. C.2 and E for methods to alleviate these challenges in practice. Separately, there may be target functions or problem instances where unitarity or orthogonality may not be appropriate. For example, one can envision node classification tasks where the target function is neither (approximately) invertible nor isometric. In such instances, non-unitary layers will be required to learn the task. More generally, deciding when to use unitary layers is problem-dependent. In some cases, such as applications with smaller input graphs, simple and more efficient interventions such as adding residual connections or including batch norm will likely suffice for addressing signal propagation problems.

Future DirectionsIn the graph domain, extensions of graph convolution to more advanced methods such as those that better incorporate edge features could widen the range of applications. Exploring hybrid models that combine unitary and non-unitary layers (e.g. global attention mechanisms) could potentially lead to more robust and versatile graph neural networks. Future work can also improve the efficiency of the parameterizations and implementations of the exponential map (see App. C.2). In a similar vein, it is likely that approximately unitary/orthogonal layers suffice in many settings to achieve the performance gains we see in our work. Methods that approximately enforce or regularize layers towards unitarity may be of interest in these instances due to their potential for improved efficiency. In this study, we mainly focused on applications to graph classification and regression tasks; however, the proposed methodology is much more general and could open up a wider range of applications to domains with more general symmetries or different data domains. For example, unitary matrices offer provable guarantees to adversarial attacks (see Corollary 9) and testing this robustness in practice on geometric data has yet to be conducted.