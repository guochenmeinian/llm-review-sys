# Category:

MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-object Demand-driven Navigation

Hongcheng Wang\({}^{1,3}\)1

Peiqi Liu\({}^{2}\)1

Wenzhe Cai\({}^{4}\) Mingdong Wu \({}^{1,3}\) Zhengyu Qian \({}^{2}\) Hao Dong \({}^{1,3}\)2

\({}^{1}\)CFCS, School of CS, PKU \({}^{2}\)School of EECS, PKU \({}^{3}\)PKU-Agibot Lab

\({}^{4}\)School of Automation, Southeast University

Equal contribution.Corresponding author.

###### Abstract

The process of satisfying daily demands is a fundamental aspect of humans' daily lives. With the advancement of embodied AI, robots are increasingly capable of satisfying human demands. Demand-driven navigation (DDN) is a task in which an agent must locate an object to satisfy a specified demand instruction, such as "I am thirsty." The previous study typically assumes that each demand instruction requires only one object to be fulfilled and does not consider individual preferences. However, the realistic human demand may involve multiple objects. In this paper, we introduce the Multi-object Demand-driven Navigation (MO-DDN) benchmark, which addresses these nuanced aspects, including multi-object search and personal preferences, thus making the MO-DDN task more reflective of real-life scenarios compared to DDN. Building upon previous work, we employ the concept of "attribute" to tackle this new task. However, instead of solely relying on attribute features in an end-to-end manner like DDN, we propose a modular method that involves constructing a coarse-to-fine attribute-based exploration agent (C2FAgent). Our experimental results illustrate that this coarse-to-fine exploration strategy capitalizes on the advantages of attributes at various decision-making levels, resulting in superior performance compared to baseline methods. Code and video can be found at https://sites.google.com/view/moddn.

## 1 Introduction

In the field of psychology, the creation of demands directs the motivation for human behavior in the real world, and behavior ultimately leads to the fulfillment of the demand [1; 2; 3; 4]. For example, an individual might crave sweet iced tea, prompting them to search for object such as tea, sugar, ice, and water, and then combine them to make sweet iced tea. Recently, with the rapid development of embodied AI and large language models, researchers are interested in using robots to meet various human demands [5; 6; 7; 8; 9; 10; 11; 12; 13]. Demand-driven Navigation (DDN)  is a variant of ObjectGoal Navigation (ON) [15; 16; 17; 18; 19; 20; 21], which requires an agent to find an object that satisfies a given demand instruction. For example, when a user gives the agent an instruction such as "I am thirsty," the robot has to search the entire environment for objects such as water, tea, coffee, etc., depending on what is available within the environment.

In previous work , demand instructions often exhibit a low level of complexity, lack consideration for user preferences, and typically require only one object to fulfill each instruction. However,real-world situations frequently involve more intricate instructions, necessitating the coordination of multiple objects to satisfy a demand. For example, in the BEHAVIOR-1K , daily tasks often involve the interaction of multiple objects. Furthermore, individual users may have distinct preferences. For instance, when presented with the basic demand "I am thirsty," one user may prefer sparkling water while another may favor juice. However, satisfying preferred demands is obviously more difficult than satisfying basic demands, so the agent needs to be flexible in prioritizing goals depending on both the situations of users and environments.

We introduce a new benchmark, Multi-object Demand-driven Navigation (MO-DDN), in which demand instructions are comprised of basic and preferred demand components. An agent must find a combination of objects (we call it a "**solution**" later) that satisfies the demand. The agent should satisfy the user's preferred demands as much as the situation allows. We use GPT-4 3 to automatically generate and modify tasks and perform manual checks. Our settings are similar to Multi-object Navigation (MON) [23; 24; 25], but we use a demand instruction to combine potentially multiple objects in a form that is consistent with common sense and personal preferences of humans rather than a list of object categories in MON. The number and category of objects in a solution are not known in advance but require an agent to reason on its own. Moreover, solutions that satisfy the same demand may have a different number of objects. We argue that MO-DDN has similar advantages over MON as DDN has over ON.

We consider MO-DDN to be a crucial preliminary step in task planning. Today, with sufficient meta-information provided (_e.g._, position, state, and category of all objects in the scene), large language models can provide satisfactory task planning results [26; 27; 28; 5]. **However, maintaining and providing accurate meta-information can be challenging and inconvenient in real life.** The user's interactions with the scene change the meta-information frequently. Moreover, since the user is not necessarily omniscient about the scene, especially in unfamiliar scenes, it is possible that if the user parses the demand himself and directly provides the desired objects, these desired objects may not exist in the scene. Even if the user can provide comprehensive meta-information, delivering the meta-information to the agent is time-consuming and inconvenient for the user. Therefore, the purpose of MO-DDN is to provide timely and accurate meta-information depending on **a convenient demand instruction** in natural language for later task planning or, more later, object manipulation.

Figure 1: **An Example of Multi-object Demand-driven Navigation.** A user plans to host a party at his new house and outlines some basic demands (highlighted in orange), along with specific preferences for different individuals (highlighted in red). The agent parses these demands and locates multiple objects in various locations in the scene to fulfill them. Despite not meeting the preferred “ice cream” demand, the agent successfully addresses basic demands, such as organizing lunch.

In the recently ObjectGoal Navigation work, benefiting from the development of Vision Language Models (VLMs) [30; 31; 32; 33; 34; 35; 36] and Large Language Models (LLMs) [37; 38; 39; 40; 41; 42], some ObjectGoal Navigation methods return to a modular manner [43; 44; 17; 45; 46; 47; 48; 49; 50] rather than an end-to-end manner [51; 52; 53; 54; 55; 56; 57; 58]. While maintaining the core concept of the "attribute" from the end-to-end agent of the previous work , we modify the training of the attribute model to make it applicable to multi-object search, and propose a coarse-to-fine attribute-based exploration modular agent, **C2FAgent**. In the coarse exploration phase, similar to the modular methods in ObjectGoal Navigation, we use a depth camera to reconstruct the environment point clouds and an object detection module, GLEE , to label the objects in the point clouds. Additionally, the point clouds are compressed and segmented into several 2D rectangular blocks, and thus each detected object belongs to a block. For each block, we calculate the similarity of the objects' attribute features and the instruction's attribute features as a metric for choosing a waypoint. We calculate basic demand similarity scores and preferred demand similarity scores separately and select blocks according to a weighted sum of the two scores. In the fine exploration phase (_i.e._, when the agent arrives at the waypoint), we train an end-to-end attribute exploration module to identify and report the target object to compose the solution.

The experimental results show that our proposed method outperforms the baselines, and the ablation study shows that the attribute model improves exploration efficiency in different phases. We argue that this coarse-to-fine design allows for the incorporation of prior knowledge from external foundation models in the coarse exploration phase and task-relevant world-grounding exploration in the fine exploration phase. Moreover, the ablation study on the weighted sum of the basic and preferred similarity scores demonstrates that increasing the preferred weights allows the agent to prioritize searching for the preferred solution and increasing the basic weights allows the agent to prioritize searching for the basic solutions. Therefore, users can adjust the weights to influence the agent's behavior freely, which is a flexible way to handle personal preferences.

Our main contributions are listed as follows:

* We propose a new benchmark, MO-DDN, which considers multi-object combinations as solutions and more complex and diverse demand instructions. MO-DDN can be regarded as a crucial preliminary step in task planning.
* We extend the training process of the attribute model, enabling the attribute features to work well in a multi-object setting. Based on the new version of attribute features, We design a coarse-to-fine attribute-based exploration agent, C2FAgent, for this benchmark, allowing the attribute features to play an important role in different exploration phases.
* The experimental results show that the attribute features do improve the efficiency of exploration, and the experimental results substantially surpass the baselines. Ablation study shows that attribute-based exploration is more efficient than frontier-based exploration  and LLM-based waypoint selection.

## 2 Related Work

### Visual Navigation

Goal DescriptionIn general terms, visual navigation involves the continuous generation of actions based on RGB-D and GPS+Compass inputs until a specified objective is reached . Visual navigation tasks vary in their goal description, such as step-by-step instructions in Vision-Language Navigation (VLN) [61; 62; 63; 64; 65; 66; 67; 68], audio in Audio-visual Navigation [69; 70; 71; 72; 73; 74], object categories in Object-Goal Navigation (ON) [15; 16; 17; 18; 19; 20; 21], object category lists in MultiObject Navigation (MON) [23; 24; 25; 26], and demand instructions in Demand-driven Navigation (DDN) . Our proposed benchmark, MO-DDN, can be viewed as a multi-object version of DDN. Although, like DDN, MO-DDN's inputs are demand instructions, MO-DDN's solutions are multi-object rather than single-object, as in DDN.

Previous Method OverviewIn ON, with the rise of object detection models [31; 30; 75; 76], object segmentation models [77; 78; 79; 80], and large language models, modular methods [43; 44; 17; 45; 46; 47; 48; 49; 50] are gradually being developed. They greatly improve navigation efficiency and success rate by building semantic maps and navigable maps and then planning paths on the maps. Meanwhile, end-to-end methods [51; 52; 53; 54; 55; 56; 57; 58] focus more on learning associations in object-object and object-scene to help reason about the potential location of target objects. In MON, previous work has focused on studying how to quickly construct , memorize [24; 43], and use semantic maps  or implicit representations  of scenes. In DDN, the concept of attributes is introduced as a way of expressing what an object shows when it fulfills a demand, _e.g._, quenching thirst is an attribute of water in the context of demand "I am thirsty". In this paper, we extend the concept of attributes and propose a new method to train the attribute model. We combine the advantages of modular and end-to-end methods and propose a coarse-to-fine attribute-based exploration modular agent.

### Foundation Large Model in Embodied Task

Foundation Large Models refer to self-supervised pre-trained models trained on large Internet-scale datasets, which have demonstrated promising capabilities across various embodied tasks. Large language models (LLMs) such as GPT-4 , LLaMA [83; 40], and Gemma  exhibit performance comparable to humans in task planning [26; 27; 28; 85; 86], common sense reasoning  and question answering [87; 88; 89; 90; 34; 91]. Furthermore, researchers use LLMs to synthesize task datasets . The CLIP model , which employs contrastive learning with image and text pairs, showcases strong semantic extraction abilities in navigation [14; 93; 21; 68; 94]. Visual Language Models offer understanding of images such as image descriptions [95; 96; 97; 98], object detection [31; 75; 76], and object segmentation [77; 78; 79; 80] for downstream embodied tasks. This paper uses LLMs, specifically GPT-4, to generate task instructions, solutions, and attribute examples for attribute learning. Furthermore, we employ GLEE , a state-of-the-art object detection model, to detect object categories within the field of view for attribute feature extraction.

## 3 Multi-object Demand-driven Navigation

Following the basic settings in DDN, let \(\) denote a set of demand instructions, \(e\) denotes a set of navigable scenes, and \(\) denotes a set of object categories. Let \(o\) denote a set of solutions for demand instructions. An element in \(o\) (_i.e._, a solution) is a subset of \(\). In each episode, similar to the DDN task, an agent is randomly initialized to a location within a mapless environment and receives a demand instruction \(DI\) in natural language. In this benchmark, a demand instruction consists of two parts: one is basic demand instruction \(DI_{b}\), and the other is preferred demand instruction \(DI_{p}\), _e.g._, " I need a comfortable place to play computer games, preferably with good lighting." Each \(DI\) has two sets of solutions, _i.e._, basic solution \(So_{b}\) and preferred solution \(So_{p}\) for \(DI_{b}\) and \(DI_{p}\), respectively. For example, {desk, soft chair} is an element in \(So_{b}\) for the above example demand instruction, and {desk, soft chair, table lamp} is an element in \(So_{p}\).

Then, at each time step, the agent should choose an action from \(\), \(\), \(\), \(\), \(\), \(\), and \(\). The action \(\) is similar in MON, which automatically reports the objects in the field of view. To reduce the difficulty of the MO-DDN task and focus it on navigation, if the agent chooses \(\), all objects in the field of view with distance below the threshold \(d_{find}\) will be recorded in a found list \(FL\), instead of requiring the bounding box of the target object as in DDN. The found list \(FL\) is used to calculate the basic and preferred success rate later. When the agent chooses \(\), or the number of choices \(\) reaches a threshold \(n_{find}\), or the number of steps reaches a threshold \(n_{step}\), the episode ends and the success rate is calculated. For a specific demand instruction \(DI\), We calculate the basic success rate as follows:

\[SR_{b}=_{i=1}^{N}_{s_{b} So_{b}} _{o s_{b}}}{Len(s_{b})}\] (1)

where \(N\) donates the number of testing episodes, \(s_{b}\) donates a basic solution in the solution set of \(DI\), \(FL\) donates the found list that the agent reports, \(o\) donates an object category. The preferred success rate \(SR_{p}\) is calculated similarly. To summarize, the success rate of an episode is the value with the highest percentage of satisfaction among all solutions. We also calculate the SPL  corresponding to \(SR_{b}\) and \(SR_{p}\). We generate 300 tasks, encompassing 358 object categories from the HSSD dataset . These tasks are referred to as HSSD's world-grounding tasks, indicating that objects in these tasks are in the HSSD dataset. We generate some language-grounding tasks to train the attribute model afterward. Language-grounding means that the objects in the solutions can be everything that makes sense rather than restricting objects in HSSD. Please see the supplementary material for details about task generation A.1.1, task metrics A.1.2, and task dataset statistics A.1.3.

Method

### Attribute Model Training

In this section, we describe how to train the MO-DDN's attribute model that extends from the attribute model in DDN. In DDN, the training of attribute features is constrained by the assumption that each instruction requires only one attribute that can be satisfied by a single object. Such attribute features may not work well under multi-object settings. This paper proposes directly mapping demand instructions and object categories into the same attribute feature space. Such a mapping can learn multiple attribute features simultaneously to address multi-object search. Concretely, the core function of the attribute model is to map a demand instruction in \(\) or an object category in \(\) to several attribute features in \(^{d}\) (\(\) is the set of real numbers, \(d\) is the dimension of the attribute features) that are in the shared attribute feature space. In order to enable the alignment of the attribute features of instructions and objects, we design a discrete codebook and five losses. The alignment means that for a demand instruction and an object in its solution, one of the attribute features of the instruction and one of the attribute features of the object have a high cosine similarity.

#### 4.1.1 Codebook and Its Initialization

The reason for using a discrete codebook is similar to VQ-VAE, _i.e._, different attributes are inherently discrete from each other. Then using a discrete codebook as an attribute feature space is a better way to represent the relationships between attributes than a continuous attribute feature space. The codebook is essentially some feature vectors, and in our experiments, we choose 128 as the number of vectors and 768 as the vector dimension (same with CLIP ViT-L/14's text dimension); thus, the codebook is a 128\(\)768 matrix. we use CLIP-Text-Encoder (ViT-L/14) to encode the language-grounding attributes generated in the supplementary material Sec A.2.1 into attribute features. These attribute features are then clustered using K-means  to get 128 clustering centers. The feature vectors of these 128 clustering centers are used to initialize the codebook, making the codebook as a subspace of the CLIP feature space.

#### 4.1.2 Definition of Losses

In our method, there are two attribute models that share the same architecture, the instruction attribute model \(AM_{ins}\) and the object attribute model \(AM_{obj}\), shown in Fig. 2. In both models, only MLP Encoder and CLIP Encoder will be used in **C2FAST**, Codebook and MLP Decoder are only used to train MLP Encoder. We obtain the \(k_{1}\) instruction attributes and \(k_{2}\) object attributes (the green square in Fig. 2) by prompting GPT-4 for "what attributes can satisfy this instruction?" and "what attributes does this object have?", respectively. For the five losses used to train the two attribute models, see the pseudo-code 1.

Figure 2: **Attribute Model.** This figure shows the architecture of the attribute model. Instructions and objects share the same model architecture. Instructions and items share the same model architecture. For parameters, they share only the parameters of the shared codebook, while the parameters of the MLP Encoder and Decoder are independent. Only the red with flames modules in the figure will be trained while the blue with snowflakes CLIP model parameters will be frozen.

The full loss function is shown below:

\[Loss=_{1} Attribute\ Loss+_{2} VQ\ Loss+\\ _{3} Commit\ Loss+_{4} Recon\ Loss+_{5}  Matching\ Loss\] (2)

where \(_{1}\) is 2.0, \(_{2}\) is 1.0, \(_{3}\) is 0.25, \(_{4}\) is 1.0, and \(_{5}\) is 1.0. Attribute Loss provides a direct loss for directing the MLP Encoder to learn the projections of IF and OF to IAF and OAF 4. The next three items VQ Loss, Commitment Loss and Reconstruction Loss can be referred to VQ-VAE Loss . Our motivation in using these three items is to provide indirect constraints that enable the IF and OF to be projected into the shared codebook's feature space. To provide a direct alignment of the attribute features of a given instruction and objects that satisfy the given instruction, we design Matching Loss. An instruction and an object in the instruction's solution theoretically have at least one attribute that matches, _i.e._, an attribute of the object that necessarily satisfies part of this instruction; otherwise, the object should not be part of the instruction's solution. We consider the most similar pair of attribute features among the \(k_{1}\) IAF and the \(k_{2}\) OAF to be a match, and therefore need to reduce the error of this pair of attribute features.

In the end, only the instruction and object MLP Encoders and CLIP-Text-Encoder are used in navigation, and other part like codebook and MLP Decoder are not used. Since this codebook is initialized by CLIP features and the ground-truth attribute features are also encoded by CLIP, the attribute feature space we get can actually be seen as a subspace of the CLIP semantic space. We hope that this design will improve the generalization of attribute features, since the CLIP semantic space shows good generalization and performance on many tasks [94; 93; 102].

### Coarse-to-fine Exploration Agent

In this section, we describe how attribute features work in navigation. The agent switches between coarse and fine exploration phases back and forth until the number of \(\) executions reaches the upper limit \(n_{find}\) or the number of steps reaches the upper limit \(n_{step}\). Fig. 3 illustrates a general overview of the entire navigation policy and Fig. 4 and Fig. 5 show the details of the coarse and fine exploration phase, respectively. In both the coarse and fine exploration phase, we load the parameters of Ins MLP Encoder and Obj MLP Encoder from the attribute training for waypoint (_i.e._, block) selection and end-to-end fine exploration. We argue that this coarse-to-fine design allows for the incorporation of prior knowledge from external foundation large models in the coarse exploration phase and task-relevant world-grounding exploration in the fine exploration phase.

#### 4.2.1 Coarse Exploration Phase

In this phase (see Fig. 4), the agent receives the RGB-D input, pose, and the demand instruction. We then use the camera parameters and depth map to compute partial point clouds of the current observation and merge them with the previously observed point clouds. We use an object detection model, GLEE, to detect objects in the RGB image and project them into the depth map, labeling the point clouds. We segment the point clouds into many rectangular blocks according to \(x\) and \(y\) coordinates, and each block is a \(b b\) square, where \(b\) is 2 in experiments. Thus, each detected object belongs to a block (according to the center of the object's point clouds). We use the instruction attribute features to query objects in each attribute block, and each block will then get a score. We use two different ways to generate instruction attribute features; see the LLM branch and MLP branch at the bottom in Fig. 4. **For the LLM branch** in Fig. 4, we use GPT-4 to generate language-level basic and preferred attributes separately and use CLIP-Text-Encoder to obtain basic and preferred attribute features. These two attribute features can be used to calculate two scores, basic and preferred scores. Adjusting the weights of these two scores can control whether to prioritize the search for basic or preferred solutions. The formula for calculating the score (_i.e._, the process of query) is as follows:

\[s=_{o block}(r_{p}_{i=1..k_{1};j=1..k_{2}}f^{i}_{pref\_ins}*f^ {j}_{o}+r_{b}_{i=1..k_{1};j=1..k_{2}}f^{i}_{basic\_ins}*f^{j}_{o})\] (3)

Where \(f^{j}_{o}\) denote \(j_{th}\) attribute features of the object \(o\), \(f^{i}_{basic\_ins}\) denote \(i_{th}\) basic attribute features of the instruction (so do \(f^{i}_{pref\_ins}\)), \(*\) denotes cosine similarity, and \(r_{b}\) and \(r_{p}\) are adjustable weights for whether to find basic or preferred solutions. If deployed in a real environment, these two weights

Figure 4: **Coarse Exploration. This figure presents the process of building and labeling the point clouds, segmenting the blocks, and calculating the scores for each block.**

Figure 3: **Navigation Policy. The agent continuously switches between a coarse exploration phase and a fine exploration phase until the \(\) count limit \(n_{find}\) is reached or the total number of steps \(n_{step}\) is reached. See Sec. 4.2.1 and Sec. 4.2.2 for details about the two phases. In each timestep, the GLEE model is used to identify and label objects in the RGB and project them to the point cloud.**can be freely adjusted by the user to apply to different situations. We conduct an ablation study on these two weights and discuss in detail how they affect the basic and preferred solutions in the experimental section. **For the MLP branch**, we use the Ins MLP Encoder from attribute training to map the instruction features into \(k_{1}\) attribute features, and we use a similar query process to compute scores. MLP branch is a lightweight alternative that neither requires remote LLM nor consumes computational resources to run local LLMs. If deployed in a real environment, the agent is free to choose one of the branches according to the current LLM availability. We report the results of the two branches separately in the experimental section. Finally, we randomly choose a point in the highest-scoring and never-visited block as the waypoint. Then, the agent navigates to the point by a path-planning algorithm. Please see supplementary material for details about the coarse exploration module, the path-planning algorithm and blocks' score visualizations A.3.1.

#### 4.2.2 Fine Exploration Phase

In this phase, we train an end-to-end module using imitation learning similar to DDN. The alignment capabilities of CLIP in the visual and textual domains allow Obj MLP Encoder to still extract object attribute features through CLIP-Visual-Encoder's object features. The self-attention mechanism in the Transformer Encoder learns the association between the attribute features of the objects in the current field of view and the attribute features of the instructions, which can implicitly determine whether these objects need to be reported. Following BERT  and ViT , we add a CLS token as the output of the feature fusion. For more training and hyperparameter details, see the supplementary material A.3.2.

## 5 Experiment

### Experimental Settings

We use habitat-sim and habitat-lab as our simulator and HSSD as our scene dataset. We randomly select 30 tasks as testing tasks and the remaining 270 tasks as training tasks(_i.e._, unseen task and seen task in Tab. 5.3, respectively). These training tasks are used to collect trajectories to train the fine exploration module, VTN and ZSON. HSSD splits the scenes into val scenes and train scenes (_i.e._, unseen scenes and seen scenes in Tab. 5.3, respectively). In all experimental settings, the judgment distance of the found list \(d_{find}\) is one meter, and a maximum number of \(\)\(n_{find}\) is five times, and the maximum step number \(n_{step}\) is 300. A single RTX 4090 is enough to run the experiments. More experimental settings are available in the supplementary material A.4.

Figure 5: **Fine Exploration. We employ imitation learning to train an end-to-end module in this phase. This module loads the Ins MLP Encoder and Obj MLP Encoder’s parameters as initialization from attribute training, along with a Transformer Encoder to integrate features. The output feature corresponding to the CLS token is combined with GPS+Compass features and a previous action embedding and passed through an LSTM to generate actions by an actor.**

[MISSING_PAGE_FAIL:9]

* **Q2**: Do attribute features also work in the end-to-end fine exploration modules? How about replacing the fine exploration module with VTN and ZSON?
* **Q3**: Do VQ-VAE losses and codebook initialization contribute to experimental results?
* **Q4**: Can adjusting the weights of basic and preferred scores affect agent behavior?

In the ablation study, we report the results in the seen tasks and seen scenes. Ours refers to C2FAgent (LLM branch). The experimental results for all four questions are in Tab. 5.4. For specific experimental setups, please see supplementary materials A.4.3.

**For Q1**, the experimental results demonstrate that attribute-based coarse exploration outperforms rule-based FBE, commonsense-based LLM and CLIP-based exploration. **For Q2**, the experimental results show that the fine exploration module utilizes the prior in the attribute model well, outperforming the VTN, which has larger model parameters, and the ZSON, which has been pre-trained on 36M total episodes and fine-tuned on the same trajectory dataset with Ours. In addition, we note that Coarse+VTN exceeds VTN, suggesting that the coarse exploration module can steer the agent to the region where it is more likely to find objects that satisfy the demand. **For Q3**, we find that the performance decreases after removing the VQ-VAE Loss or codebook initialization. Since the attribute model itself is trained on language-grounding tasks and thus agnostic to the task being evaluated, we can argue that the VQ-VAE Loss and the initialization contribute to the generalizability of attribute features. **For Q4**, adjusting the weights of two scores can indeed affect the agent's behavior. By tuning up \(r_{p}\), \(SR_{p}\) and \(SPL_{p}\) increase, while \(SR_{b}\) and \(SPL_{b}\) decrease; and vice versa. This characteristic allows the user to freely decide whether to prioritize the search for the basic or preferred solution in the current situation. For example, when a user who likes Coke is **very** thirsty, he can increase \(r_{b}\) to allow the agent to satisfy the basic demand with a higher success rate, while in the general case, \(r_{p}\) can be increased to allow the agent to try to satisfy the preferred demand.

## 6 Conclusion and Discussion

In this paper, we propose a new benchmark, MO-DDN, which can be regarded as a multi-object version of DDN. Moreover, MO-DDN can be considered a crucial preliminary step in task planning. We also propose a coarse-to-fine attribute-based exploration agent, extending the concept of "attribute" to a multi-object setting. The agent uses attribute features in different exploration phases. The experimental results show that our method outperforms the baselines, and the attribute features improve exploration efficiency. The ablation study also demonstrated the effectiveness of our method.

Limitations and Broader Societal Impacts In this paper, we assume that the number of attributes of both instructions and objects is fixed values (_i.e._, \(k_{1}\) and \(k_{2}\)). Though this facilitates training and its experimental results outperform baselines, there are still some gaps with real life. Future work could consider more flexible attribute features. Please see A.5 for more limitation discussion. To the best of our knowledge, there are no observable adverse effects on society.