# TSDS: Data Selection for Task-Specific Model Finetuning

Zifan Liu

University of Wisconsin-Madison

Madison, WI

zliu676@wisc.edu

&Amin Karbasi

Yale University

New Haven, CT

amin.karbasi@yale.edu

&Theodoros Rekatsinas

Apple

Zurich, Switzerland

trekatsinas@apple.com

###### Abstract

Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (**T**ask-**S**pecific **D**ata **S**election), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data. We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques. We evaluate our method on data selection for both continued pretraining and instruction tuning of language models. We show that instruction tuning using data selected by our method with a 1% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average. Our code is available at https://github.com/ZifanL/TSDS.

## 1 Introduction

Finetuning foundation models  is the de-facto paradigm for building machine learning applications that focus on specific tasks. Models such as BERT  and LLaMA  are large-scale models pretrained on massive unlabeled data across a wide range of domains. Those models can be specialized to downstream tasks through finetuning. Finetuning can take a variety of forms depending on the target task. For instance, _continued pretraining_ extends the pretraining stage of a model on a dataset that is more closely related to a target domain. As another setting, _instruction tuning_ trains a generative foundation model on instruction-response pairs to improve its performance in responding to task-specific instructions.

Finetuning foundation models can lead to significant improvement in downstream tasks, but the effectiveness heavily relies on the right choice of training data . However, the data repositories that one considers during training of generative models tend to be large--consider forexample the use of Common Crawl1, which contains 250 billion web pages, or The Pile --and hence, it is impractical to manually select the data that are distributed like the use cases in the target task. Therefore, automated task-specific data selection becomes critical.

In this paper, we propose TSDS (**T**ask-**S**pecific **D**ata **S**election), a framework to select data for task-specific model finetuning. We consider the scenario of finetuning a foundation model to customize it for a specific task characterized by a few representative examples. The input to our framework is the representative examples and a massive repository of candidate data. Guided by the representative examples, we select training data from the repository for task-specific finetuning. We identify the following requirements for our framework.

**(Distribution Alignment)** First, the distribution of the selected data should match the distribution of the representative examples from the target task. Distribution alignment is essential for a model to learn the target distribution and enable data-efficient finetuning for the target task . Many works  retrieve candidate examples that are most similar to the representative examples. Such heuristics do not ensure distribution alignment between the selected data and the representative examples. A recent work  selects data by importance resampling to match the target distribution but is limited to an n-gram feature space, which cannot capture high-level semantics.

**(Diversity)** Second, the selected data should be diverse so that the model can learn a wide range of related knowledge rather than overfitting to specific examples. In practice, data repositories created by web crawling may contain a large portion of near-duplicates  that can compromise diversity and negatively impact model performance . For example, a study  on several snapshots of ClueWeb2 and Common Crawl shows that 14% to 52% of the documents are near-duplicates. Previous works  on task-specific data selection overlook near-duplicates, leading to the over-representation of such examples in the selected data. We require our framework to ensure diversity in selection even when a large fraction of the candidate examples are near-duplicates.

**(Scalability)** Finally, the selection algorithm should be efficient, considering the increasing scale of modern data repositories. The high volume of candidate data (e.g., 250 billion pages in Common Crawl) poses a great challenge to efficient selection.

Our framework formulates task-specific data selection as an optimization problem that allows a smooth trade-off between two crucial objectives: distribution alignment and diversity. The solution to the optimization problem is a categorical distribution assigned to the candidates which we will sample from. In the optimization objective, we use optimal transport to measure the discrepancy between the distribution assigned to the candidates and the target distribution, encouraging the alignment between them. We show that the optimization problem admits efficient algorithms to compute the optimal solution. In addition, our framework supports distribution alignment in any metric space that supports efficient nearest-neighbor search, including model-agnostic semantic embedding and model-specific features such as gradients.

Our contributions: 1) We formulate data selection for task-specific finetuning as an optimization problem based on optimal transport for distribution alignment, with a regularization term that encourages diversity. 2) We make our framework robust to near-duplicates by incorporating kernel density estimation  into the regularization term. 3)We show the connection between the optimal solution to the optimization problem and nearest neighbor search, which allows us to develop efficient algorithms employing approximate nearest-neighbor search techniques .

We conduct extensive experiments to validate the effectiveness of our framework. We focus on natural language processing tasks where foundation models have shown great advancements. We show that our framework beats the state-of-the-art baseline  by \(1.5\) points in F1 score on average with a selection ratio of 1% on instruction tuning for two modern large language models on three tasks. In addition, continued pretraining using domain-specific data selected by our framework outperforms the other selection methods by up to 3 F1 points on four classification tasks from various domains. We also demonstrate that our framework is robust to near-duplicates in the data repository, maintaining consistent performance when 1% of the candidate examples are duplicate for up to 1,000 times. Our method is efficient, taking 28 hours to preprocess a corpus of 150M examples and less than 1 hour for each task-specific selection.

Background and Overview

In this section, we provide background information that is essential for the problem, followed by a formal statement of the problem and an overview of our proposed framework.

### Background

We introduce the notations that will be used throughout the paper and the optimal transport problem.

NotationWe use \(_{ 0}\) to represent the set of non-negative real numbers, and \(_{>0}\) to represent the set of positive real numbers. Let \(N\) be a positive integer and we use \([N]\) to denote the set of integers from \(1\) to \(N\). We use bold letters to denote matrices and the corresponding plain letters with subscripts to denote the entries in the matrix. For example, \(^{M N}\) is a matrix with size \(M N\), and \(_{ij}\) or \(_{i,j}\) is the entry in the \(i^{}\) row and the \(j^{}\) column (1-indexed).

Optimal Transport between Discrete DistributionsWe introduce the optimal transport problem, which forms the basis of our data selection framework. Let \((A,f)\) be a metric space where \(A\) is a finite set and \(f:A A_{ 0}\) is a distance function. Consider two discrete distributions \(\) on \(U A\) and \(\) on \(V A\), where both \(U\) and \(V\) are finite sets. Let \(u_{i}\) be the \(i^{}\) example in \(U\) and \(_{i}=(u_{i})\) be the probability of \(u_{i}\). Similarly, let \(v_{j}\) be the \(j^{}\) example in \(V\) and \(_{j}=(v_{j})\) be the probability of \(v_{j}\). Let \(_{ 0}^{|U||V|}\) be a transport of probability mass between \(\) and \(\), where \(_{ij}\) is amount of probability mass transported from \(u_{i}\) to \(v_{j}\). Assume that the cost of transporting one unit of probability mass from \(u_{i}\) to \(v_{j}\) is \(f(u_{i},v_{j})\), the distance between \(u_{i}\) and \(v_{j}\). Optimal transport is the problem of transporting all the probability mass from \(U\) to \(V\) with a minimal cost:

\[_{_{ 0}^{|U||V|}}_{i=1}^{|U|}_{j= 1}^{|V|}_{ij}f(u_{i},v_{j})_{j=1}^{|V|} _{ij}=_{i}, i[|U|],_{i=1}^{|U|}_{ij}=_{j},  j[|V|]\]

### Task-Specific Data Selection Problem Statement

We now introduce the problem of data selection for task-specific finetuning. We assume access to a set of \(M\) representative examples \(Q=\{q_{i}\}_{i=1}^{M}\) from the target task, which we call query examples. Consider a data repository \(D=\{x_{j}\}_{j=1}^{N}\) containing \(N\) candidate examples. Note that \(Q\) and \(D\) are multisets that may contain duplicates. We aim to select \(B\) examples from the repository guided by the query examples. The selected examples will be used to finetune a model to tailor it to the target task. We adopt the _model-agnostic_ formulation above for the generality of the solution. However, our framework can be applied to model-specific selection by using model-specific data representations; an example evaluation for model-specific instruction tuning is presented in Section 5.1.

### Framework Overview

Our framework takes the candidate examples and the query examples as inputs and outputs a set of task-specific examples by the following workflow. 1. (_Encoding_) We first encode the query examples and the candidate examples into the same metric space with a specified distance function. 2. (_Probability Assignment_) We determine the probability mass assigned to each candidate example by solving an optimization problem. 3. (_Sampling_) We take a random sample with replacement from the candidate examples following a categorical distribution where the probability is determined by the assignment in the previous step.

## 3 Data Selection and Optimal Transport

Data selection for task-specific finetuning can be expressed as an optimization problem for probability assignment to the candidates in the data repository. First, we discuss the formulation of the optimization problem and then show the existence of closed-form solutions. In addition, we propose a regularization term that addresses the problem of near-duplicates among the candidates. The proofs of the theorems in this section are provided in Appendix B.

### Optimization Problem

Consider the metric space \((Z,f)\) where \(Z=Q D\) contains all the examples and \(f:Z Z\) is a distance function. Let \(_{ 0}^{M N}\) be the distance matrix, where \(d_{ij}=f(q_{i},x_{j})\) is the distance between the \(i^{}\) query example and the \(j^{}\) candidate example.

We propose an optimization problem that transports probability mass from the query examples to the candidates. The objective is a linear combination of a probability transport cost for distribution alignment and a regularization term to encourage diversity. Formally, given \(_{ 0}^{M N}\), we consider the following optimization problem, which we refer to as Problem RT (regularized transport):

\[_{_{ 0}^{M N}}_{i=1 }^{M}_{j=1}^{N}_{ij}d_{ij}+(1-)G()_{j=1}^{N}_{ij}=, i [M]\]

where \(C>0\) is a scaling constant, \(\) is a hyper-parameter that controls the trade-off between distribution alignment and diversity, and \(G\) is a regularization function. The first term in Problem RT is the cost of probability transport where \(_{ij}\) is the mass transported from the \(i^{}\) query example to the \(j^{}\) candidate. Each query example has \(\) probability mass to transport, as stated in the constraint. The probability transport cost measures the cost of transforming one distribution to another by moving probability mass between them, providing a method to quantify probability alignment. The second is a regularization term that encourages the diversity of probability transport.

Let \(^{}\) be an optimal solution to Problem RT. We assign \(p_{j}^{}=_{i=1}^{M}_{ij}^{}\) probability to candidate example \(x_{j}\), which is the sum of the probability mass it receives from all the query examples. When we sample from the candidate examples in the subsequent step, \(x_{j}\) has probability \(p_{j}^{}\).

We propose two instantiations of the regularization term that encourage the diversity of probability transport by penalizing its discrepancy to the uniform transport:

* \(G_{}()=M_{i M,j N}|_{ij}-|\) captures the largest probability gap between \(\) and the uniform transport.
* \(G_{}()=_{i=1}^{M}_{j=1}^{N}|_{ij }-|\) is the total variation distance between \(\) and the uniform transport.

We use uniform transport as a reference point to encourage diversity as it represents the most diverse way of transporting the probability mass from one query example to all the candidates, assuming the candidates are distinct.

### Closed-Form Solution

When \(G=G_{}\), Problem RT can be solved by standard linear programming techniques, but they run in \(((MN)^{2})\) time, which is prohibitively expensive. Instead, we show the existence of a closed-form solution that can be computed in \(O(MN N)\) time (see Section 4 for the algorithm).

Using \(G_{}\) as the regularization function, we get an optimal solution by transporting the probability of each query example evenly to its \(K\)-nearest neighbors among the candidates, where \(K\) is determined by the tradeoff between distribution alignment and diversity:

**Theorem 3.1**.: _Given \(_{ 0}^{M N}\) where \(N>1\), consider Problem RT with \(G()=G_{}()=M_{i M,j N}|_{ij}- {1}{MN}|\). For all \(i[M]\), let \(j_{1}^{i},,j_{N}^{i}\) be a reordering of \([N]\) such that \(d_{ij_{1}^{i}} d_{ij_{k}^{i}}\). Consider \(^{}_{ 0}^{M N}\) whose entries are \(\) if \(j\{j_{1}^{i},,j_{K}^{i}\}\) and \(0\) otherwise, where \(K=\{k[N]|_{i=1}^{M}_{l=1}^{k-1}(d_{ij_{k}^{i}}- d_{ij_{l}^{i}})<(1-)M\}\). Assume \(K N/2\), and then \(^{}\) is a minimizer of Problem RT. \(^{}\) is the unique minimizer if \(_{i=1}^{M}_{l=1}^{K}(d_{ij_{K+1}^{i}}-d_{ij_{l}^{i}})>( 1-)M\) and \([M]\) such that \(d_{ij_{K+1}^{i}}=d_{ij_{K}^{i}}\)._

Similarly, there exists a closed-form solution that can be computed in \(O(MN N)\) time when \(G=G_{}\) (see Appendix A for the solution and the algorithm).

### Addressing Near-Duplicates via Kernel Density Estimation

When there exists a large fraction of near-duplicates among the candidates, \(G_{}\) fails to characterize the diversity of probability assignment since it treats near-duplicates as distinct examples. Consequently, the contents in the near-duplicates will be over-sampled. For example, if \(100\) of the \(K\)-nearest neighbors of a query example are duplicates and the others are distinct, the content in the duplicates will receive \(100\) times as much probability mass as any other example.

To address the near-duplicate problem, we propose a regularization function incorporating kernel density estimation (KDE) , which is a non-parametric method to estimate the probability density function from finite examples. We determine the duplication level of a point by the kernel density estimate at its position. We use the Epanechnikov kernel such that given \(D\), the density estimate at point \(x\) is \(_{x^{} D}(1-)^{2}}{h^{2}},0)\), where \(h>0\) is the kernel size and \(f\) is the distance function. For example, for a point \(x\) in \(D\) whose distance to any other point is larger than \(h\), the density estimate at \(x\) is \(1\). If we create two duplicates of \(x\) and add them to \(D\), the density estimate at \(x\) increases to \(3\).

Our KDE-based regularization function is \(G_{}()=M_{i[M],j[N]}_{j}|_{ij}-}{M_{j^{}[N]}1/_{j^{}}}|\) where \(_{j}=_{x^{} D}(1-f(x_{j},x^{})/h^{2})\) is the density estimate at \(x_{j}\). \(G_{}()\) compares \(\) to the probability assignment that is proportional to the inverse of the density, and penalizes the largest gap weighted by the density. Note that \(G_{}\) is a special case of \(G_{}()\) with \(_{j}=1\) for all \(j[N]\).

The optimal solution to Problem RT when \(G=G_{}\) can be obtained by assigning the probability mass of each query example to the nearest neighbors among the candidates, weighted by the inverse of their density estimate, as is shown by the following theorem.

**Theorem 3.2**.: _Given \(_{ 0}^{M N}\) and \(_{1},,_{N}_{>0}\), consider Problem RT with \(G()=G_{}()=M_{i[M],j[N]}_{j}| _{ij}-}{M_{j^{}[N]}1/_{j^{}}}|\). For all \(i[M]\), let \(j_{1}^{i},,j_{N}^{i}\) be a reordering of \([N]\) such that \(d_{ij_{1}^{i}} d_{ij_{i}^{i}}\). Let \(s_{k}^{i}=_{l=1}^{k}1/_{j_{l}^{i}}\), and \(s\) be a discrete variable that takes value from \(=\{s_{1}^{k}|k[M],k[N]\}\{0\}\). Let \(c(s)=_{i=1}^{M}c_{i}(s)\), where \(c_{i}(s)=0\) if \(s s_{1}^{i}\) and \(c_{i}(s)=_{l=1}^{k-1}^{i}}-d_{ij_{l}^{i}}}{_{j_{l}^{i}}}\) if \(s_{k-1}^{i}<s s_{k}^{i}\) for any \(k 2\). Let \(s^{*}=\{s|c(s)<(1-)M\}\), and \(K_{i}=\{k\{0,,N-1\}|s_{k}^{i} s^{*}\}\). Assume \(s^{*}_{j=1}^{N}1/_{j}\), and then \(^{*}\) is a minimizer of Problem RT where \( i[M],k[N]\)_

\[_{ij_{k}^{i}}^{*}=1/(Ms^{*}_{j_{k}^{i}}),&k K_{i}\\ -_{l=1}^{K_{j}}1/(Ms^{*}_{j_{l}^{i}}),&k=K_{i}+1\\ 0,&\]

\(^{*}\) _is the unique minimizer if \(\) such that \(c(s)=(1-)M\) and \( i[M]\) such that \(d_{ij_{K_{i}}^{i}}=d_{ij_{K_{i}+1}^{i}}\) or \(d_{ij_{K_{i}+1}^{i}}=d_{ij_{K_{i}+2}^{i}}\)._

Intuitively, we count candidate \(x_{j}\) as \(1/_{j}\) examples. For each query example, the optimal solution assigns probability mass to the candidates in its neighborhood proportional to their adjusted counts. The size of the neighborhood is determined by the limit \(s^{*}\) on the sum of the adjusted counts.

In Figure 1, we show an example comparing the optimal transport with \(G_{}\) and \(G_{}\). When \(G=G_{}\), the probability is transported uniformly to the candidates regardless of their relative positions. When \(G=G_{}\), the clustered candidates receive less probability due to their high density, and they will be less over-represented when we take samples according to the assigned probability.

## 4 Efficient Probability Assignment Algorithms for Data Selection

We propose efficient algorithms to assign probability mass to the candidates according to the optimal solutions to Problem RT. For \(G=G_{}\) and \(G=G_{}\), the corresponding algorithms are KNN-Uniform (Algorithm 1) and KNN-KDE (Algorithm 2). Each algorithm takes the query examples and the candidates as input and outputs the probability assigned to each candidate.

Both algorithms prefetch the \(L\) nearest neighbors of each query example from the candidates as the first step, where \(L\) is a limit on the neighborhood size. Specifically, \((,,L)\) returns theindices \(^{M L}\) of the nearest neighbors and the corresponding distances \(^{M L}\), where \(j_{ik}\) is the index of the \(k^{}\) nearest neighbor of \(q_{i}\) in \(\), and \(d_{ik}\) is the distance between \(q_{i}\) and \(x_{j_{ik}}\). Retrieving nearest neighbors exactly requires computing the distance between every query example and all the candidates, which is inefficient when the candidate size \(N\) is in the order of millions and billions. Alternatively, we can employ approximate nearest search techniques  to improve efficiency at the cost of lower accuracy.

Then the algorithms assign probability mass to the nearest neighbors of each example. KNN-Uniform determines \(K\) based on the tradeoff between distribution alignment and diversity. Then the algorithm assigns the probability mass of each query example evenly to its \(K\)-nearest neighbors. KNN-KDE assigns probability mass to the nearest neighbors proportional to the inverse of their kernel density estimates (Line 15-18). The sizes of the neighborhoods are determined by Line 7-12, where we increase the limit \(s\) on the sum of the inverse of the density estimates over the neighborhood until the condition on Line 9 is satisfied. We use a priority queue to store the possible values \(s\) can take and retrieve the smallest one in each iteration.

In KDE-KNN, we also precompute the kernel density estimate for the \(L\)-nearest neighbors of each query example. To estimate the kernel density of each candidate example, we need to compute the distance between it and all the other candidate examples. To reduce the computational cost, we use the \(I\)-nearest neighbors among the prefetched examples as the set to compute KDE for each candidate example. Let \(^{}\) be the set containing the \(L\)-nearest neighbors of all the query points and \(_{x}\) be the \(I\)-nearest neighbors of \(x\) in \(^{}\). We compute the KDE of example \(x\) as \(_{x^{}_{x}}(1-)^{2}}{h^{2}})\).

KNN-Uniform runs in \(O(ML+T_{1})\) time, and KNN-KDE runs in \(O(ML M+T_{2})\) time, where \(T_{1}\) is the runtime of GetKNN, and \(T_{2}\) is the runtime of ComputeKDE. With exact nearest neighbor search, \(T_{1}=O(MN N)\) and \(T_{2}=O(M^{2}L^{2}(ML))\). If we employ approximate nearest neighbor search techniques such as HNSW  for real vectors and \(l_{2}\) distance, we have \(T_{1}=O((M+N) N)\) and \(T_{2}=O(ML(ML))\).

Figure 1: An example of the optimal probability transports under different regularization terms. We consider 1 query example \(q\) and 5 candidates \(x_{1},,x_{5}\) embedded in a 2-dimensional space. Assume that the candidates that form a cluster (i.e., \(x_{3},x_{4},x_{5}\)) have a density estimate of \(\) each and the others have a density estimate of \(1\).

```
1Input: query examples \(=\{q_{i}\}_{i=1}^{M}\), candidate examples \(=\{x_{j}\}_{j=1}^{N}\), number of nearest neighbors to prefetch \(L>1\), \(\), \(C>0\); Output:\(p_{1},,p_{N}\);
2\(,(,,L)\);
3\((,)\)/*\(^{M L}\) and \(_{ik}\) is the density of \(x_{j_{ik}}\) */
4\(()\);
5for\(i[M]\)do
6\(K_{i} 0\); \(c_{i} 0\); \(.((1/_{i1},i))\);
7while\(\) is not emptydo
8\(s,i.()\); \(K_{i} K_{i}+1\); \(c_{i}_{k=1}^{K_{i}}(d_{i,K_{i}+1}-d_{ik})/_{ik}\);
9if\(_{i=1}^{M}c_{i}(1-)M\)then
10\(s^{*} s\); break;
11if\(K_{i}+1<L\)then
12\(.((s+1/_{i,K_{i}+1},i))\);
13for\(j[N]\)do
14\(p_{j} 0\);
15for\(i[M]\)do
16for\(k[K_{i}]\)do
17\(p_{j_{ik}} p_{j_{ik}}+1/(Ms^{*}_{ik})\);
18\(p_{j_{i,K_{i}+1}} p_{j_{i,K_{i}+1}}+-_{k=1}^{K_{i}}1/( Ms^{*}_{ik})\); ```

**Algorithm 2**KNN-KDE.

## 5 Experiments

We evaluate our framework on data selection for task-specific instruction tuning and domain-specific continued pretraining, using different encodings as needed. We show that 1) our framework outperforms the state-of-the-art methods on data selection for task-specific instruction tuning and domain-specific continued pretraining by up to 6 points and 3 points in F1 score respectively; 2) our framework is robust to duplicates, exhibiting consistent performance when 1% of the candidate examples are duplicated up to 1000 times, while baseline methods show a drop of 2 points in F1 score (see Appendix E.1); 3) our method is efficient, requiring 28 hours to preprocess 150 million candidate examples and less than 1 hour for each task-specific selection (see Appendix E.2).

### Evaluation on Task-Specific Instruction Tuning

We select training data to perform instruction tuning to tailor a model to specific downstream tasks. We assume access to several query examples that represent the use cases of the target task and a repository of instruction-response pairs to select from. The detailed setting is as follows.

Target Tasks, Model, and Data RepositoryWe consider three tasks from standard benchmarks for language model evaluation. The properties are shown in Table 1. We use two models: Llama-2-7b and Mistral-7B. We use a combination of Flan V2, CoT, Dolly, and Open Assistant as the data repository for selection, which contains 270K examples.

EncodingWe encode the examples using rescaled and randomly projected gradients from a Llama-2-7b model finetuned on a random 5% of the data repository. The encoding process follows Xia et al., who show that gradient-based encoding is essential to capture the utility of training examples in instruction tuning. We use \(l_{2}\) distance as the distance function. See Appendix C for the details.

   Dataset & Task & \# Test Instances & \# Query Examples & \# Shots* & Metric \\  TydiQA & Multilingual QA & 1,713 & 9 & 1 & F1 score \\ MMLU & Multiple choice & 18,721 & 285 & 5 & Accuracy \\ BBH & Reasoning & 920 & 81 & 3 & Accuracy \\   

* \# shots is the number of QA examples provided in the prompt when querying the model.

Table 1: Information of the target datasets for instruction tuning.

Methods1) **Rand** selects a random subset from the data repository; 2) **LESS** (the state-of-the-art method on data selection for task-specific instruction tuning) selects training data from the data repository based on their gradient similarity to the query examples; 3) **Ours** is the KNN-KDE instantiation of our framework with \(C=5\), \(=0.075\) and \(h=0.2\). We discuss how we choose the parameters in Appendix C. The implementation details of our method can also be found in Appendix C. Note that our method is not sensitive to the hyperparameters, as shown by the microbenchmarks in Appendix E.

Evaluation ProtocolFollowing Xia et al. , we finetune the base model on the selected data for \(4\) epochs. The dataset size is 0.5% / 1.0% / 5% of the data repository. Since our method is based on probabilistic sampling, we do not select a fixed training set. Instead, in each epoch we sample randomly from the data repository following the assigned probability. The hyperparameters for finetuning also follow Xia et al.  (see Appendix D). We repeat each experiment for three runs with different random seeds and report the mean and standard deviation.

ResultsThe results are shown in Table 2 where "Base" is the base model without finetuning and "Full" is the model finetuned on the full data repository. Our method consistently outperforms the baselines on TydiQA and BBH across different selection ratios, beating the state-of-the-art method (LESS) by up to 6 points. With a selection ratio of 1%, our method outperforms the full data repository on TydiQA and BBH. On MMLU, our methods show comparable results to LESS. Note that for Mistral-7B, finetuning on the full repository leads to worse performance than no finetuning, which highlights the importance of careful data selection for task-specific instruction tuning. We also notice that finetuning Mistral-7B on any selected set does not increase its accuracy on MMLU. The reason could be that the base Mistral-7B model has already been well-tuned for multiple-choice questions using high-quality data. We observe a drop in the performance of our method on TydiQA when the selection ratio increases from 1% to 5%, which may be caused by overfitting. We can early stop the training process to avoid overfitting in practice.

### Evaluation on Domain-Specific Continued Pretraining

In this experiment, we select data for domain-specific continued pretraining to adapt a model to a specific domain. We assume access to a set of annotated data for a domain-specific task that serves as query examples and a repository of unlabeled data to select from. We continue pretraining the base model on the selected data and then perform supervised finetuning using the annotated data.

Target Tasks and Data RepositoryWe consider four datasets focused on classification tasks across diverse domains. The properties are provided in Table 3. We select data for continued pertaining from a data repository consisting of 150M sequences crafted by Xie et al.  from The Pile .

   Model &  &  \\  Dataset & TydiQA & MMLU & BBH & TydiQA & MMLU & BBH \\  Base & \(40.6\) & \(45.7\) & \(39.1\) & \(49.6\) & \(62.4\) & \(56.5\) \\ Full & \(52.7\) & \(51.4\) & \(41.4\) & \(44.7\) & \(58.9\) & \(48.0\) \\   & Rand & \(49.8_{2.4}\) & \(45.0_{0.4}\) & \(38.3_{0.5}\) & \(57.0_{1.5}\) & \(59.5_{0.3}\) & \(49.7_{0.1}\) \\  & LESS & \(52.3_{1.4}\) & \(46.2_{0.7}\) & \(39.0_{0.6}\) & \(55.0_{3.0}\) & \(}\) & \(53.0_{0.9}\) \\  & Ours & \(}\) & \(}\) & \(}\) & \(}\) & \(60.3_{0.9}\) & \(}\) \\   & Rand & \(47.8_{1.7}\) & \(45.9_{0.5}\) & \(38.2_{0.7}\) & \(57.8_{0.4}\) & \(59.4_{0.2}\) & \(53.7_{1.0}\) \\  & LESS & \(54.0_{1.0}\) & \(}\) & \(40.2_{0.6}\) & \(59.0_{0.8}\) & \(}\) & \(53.7_{1.8}\) \\  & Ours & \(}\) & \(47.9_{0.2}\) & \(}\) & \(}\) & \(60.5_{0.8}\) & \(}\) \\   & Rand & \(49.5_{1.4}\) & \(46.0_{0.8}\) & \(40.8_{0.6}\) & \(57.6_{0.7}\) & \(60.2_{0.3}\) & \(54.8_{1.1}\) \\  & LESS & \(54.3_{0.7}\) & \(50.6_{0.0}\) & \(40.2_{1.8}\) & \(60.4_{1.3}\) & \(}\) & \(53.7_{0.6}\) \\   & Ours & \(}\) & \(}\) & \(}\) & \(}\) & \(59.9_{0.4}\) & \(}\) \\   

Table 2: Performance of instruction tuning with dataset selected by our method compared with the baselines. The subscripts represent the standard deviations.

**Target-Domain Data Accessibility** To simulate different levels of access to target-domain annotated data, we consider three settings with varying sizes of annotated data (1K, 3K, and 10K). When the size is set to \(M\) and the original target-domain training set is larger than \(M\), we sub-sample it by choosing \(M\) examples uniformly at random without replacement.

**Encoding** We encode the examples into \(^{512}\) using the Universal Sentence Encoder  to capture semantic meanings and use \(l_{2}\) distance as the distance function.

**Methods** 1) **Rand** selects a random subset from the data repository; 2) **DSIR** (the state-of-the-art method on data selection for domain-specific continued pretraining) selects examples by importance resampling to match the unigram and bigram distribution of the query examples.; 3) **Ours** is the KNN-KDE instantiation of our framework with \(C=5\), \(=0.6\) and \(h=0.1\).

**Evaluation Protocol** For each domain-specific task, we provide the annotated set to the selection methods as the query examples to guide the selection. We perform continued pretraining on 1M examples selected by each method from the data repository for one epoch (see Appendix E.4 for different selection sizes), starting from the base ALBERT  model. Then we finetune the model on the domain-specific annotated set and evaluate it on the test set. The hyperparameters for training follow previous works [17; 49; 48] (see Appendix D). The experiments are repeated five times with varying random seeds. We remove the best and the worst among the five runs to rule out outlier runs and report the mean and standard deviation.

**Results** The test F1 scores of the downstream classification tasks are reported in Table 4. As a reference point, we provide the performance of finetuning the model directly without continued pretraining (Base). Our method outperforms the baselines in most cases except ChemProt (3K) and AGNews (1K), with a gap of up to 3 points in F1 scores. On ChemProt (3K) and AGNews (1K), our method is comparable to DSIR. We also notice that our method shows an average improvement of 1.92 points over DSIR with an annotated set size of 1K and 0.38 points with an annotated set size of 3K. This indicates that our method is particularly effective with small annotated sets.

## 6 Related Works

**Task-Specific Data Selection** Similarity-based methods [39; 17; 2; 50] retrieves the top ones from the candidates, ranked by their similarity to the representative examples from the target task. The features used for similarity computation can be embeddings or ngrams for texts. Another line of works [35; 48] use two generative models where one learns the distribution of the target-task data and the other learns the general-purpose data. Model-specific data selection methods [12; 47] choose data to maximize the model performance on the target task. Given the high cost of actually training a model and evaluating it on the target task, these methods often estimate the model performance by approximation. DSDM  approximate the model performance using datamodels , a function

    &  &  &  \\  & ChemP. & IMDB & SCI. & AGNews & ChemP. & IMDB & SCI. & AGNews & IMDB & AGNews \\  Base & \(69.6_{1.8}\) & \(88.0_{0.4}\) & \(60.1_{2.3}\) & \(87.1_{1.0}\) & \(77.1_{1.1}\) & \(88.7_{0.4}\) & \(75.8_{1.1}\) & \(87.7_{0.3}\) & \(90.0_{0.0}\) & \(89.1_{0.1}\) \\ Rand & \(69.7_{1.5}\) & \(87.3_{0.1}\) & \(62.7_{2.9}\) & \(87.2_{0.3}\) & \(78.6_{0.2}\) & \(88.5_{0.1}\) & \(77.5_{1.6}\) & \(88.2_{0.1}\) & \(90.2_{0.1}\) & \(90.2_{0.1}\) \\ DSIR & \(74.8_{0.7}\) & \(87.7_{0.6}\) & \(68.5_{0.1}\) & \(}\) & \(}\) & \(89.4_{0.2}\) & \(78.9_{0.7}\) & \(89.1_{0.3}\) & \(90.8_{0.1}\) & \(90.1_{0.1}\) \\ Ours & \(}\) & \(}\) & \(}\) & \(87.3_{0.2}\) & \(81.9_{0.4}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\   

Table 4: F1 scores of the downstream tasks. Standard deviations are shown in the subscripts.

   Dataset & Domain & Train & Validation & Test & Classes & Metric \\  ChemProt  & Biomedical & 4,169 & 2,427 & 3,469 & 13 & micro-F1 score \\ IMDB  & Movie review & 20,000 & 5,000 & 25,000 & 2 & macro-F1 score \\ SCIERC  & Computer science & 3,219 & 455 & 974 & 7 & macro-F1 score \\ AGNews  & News & 114,947 & 4,999 & 7,596 & 4 & macro-F1 score \\   

Table 3: Training, validation, test sizes and the number of classes in the datasets.

that maps the training data membership (whether each candidate is included in the training set or not) to the model performance. LESS  employs the influence function  to approximate the marginal gain on the model performance when including a candidate into the training set. Specifically, LESS computes the gradient similarity between each candidate and all the query examples, and the maximum similarity is the score for ranking. Then the top-ranked candidates are selected. A major difference between our method and LESS is that our method matches the distributions, while LESS takes the top ones based on aggregated statistics.

#### Diversity Measurement for Data Selection

Measuring diversity is a critical aspect of data selection, as it ensures that the chosen dataset represents a wide range of examples rather than being overly concentrated on similar or redundant instances. DEITA  selects data in an iterative manner, where the contribution of a new example to the overall diversity is measured by the clipped cosine distance between the new example and the closest examples that have been selected. QDIT  measures the diversity of the selected data using the facility location function that quantifies how well each example in the full set is represented by the selected set. Wang et al.  measure the diversity using the log determinant distance between the selected set and a reference set that is maximally diverse.

#### Data Deduplication

Data deduplication removes duplicates or near-duplicates from a dataset. Exact duplicates can be detected using hash functions [11; 46], while the detection of near-duplicates is more challenging. Some works [37; 14] identify near-duplicates utilizing locality-sensitive hashing . Others [28; 6] compute edit distances between examples to find near-duplicates. Another line of works [1; 42] relies on learned embeddings of the examples to detect near-duplicates.

## 7 Conclusion

In this paper, we proposed a framework for data selection for task-specific model finetuning, based on optimal transport, which allows a smooth tradeoff between distribution alignment and diversity. We incorporated kernel density estimation to make the selection robust to near-duplicates. Experimentally we showed that our method is effective in both task-specific instruction tuning and domain-specific continued pretraining. A potential direction for future work is to incorporate more efficient variants of optimal transport, such as Sinkhorn distances , to further improve the computational efficiency. One limitation of our framework is the reliance on a set of representative examples to guide the selection, which may not be easy to craft. The representative examples may also contain biases that can be exaggerated through the selection process, leading to negative social impacts. In practice, additional effort must be allocated to ensure the quality of the representative examples and the size of the representative examples needs to be decided according to the budget of human effort.