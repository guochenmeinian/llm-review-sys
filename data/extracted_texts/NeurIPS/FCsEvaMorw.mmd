# Rainbow Teaming:

Open-Ended Generation of Diverse Adversarial Prompts

 Mikayel Samvelyan\({}^{*1,2}\) Sharath Chandra Raparthy\({}^{*1}\) Andrei Lupu\({}^{*1,3}\)

**Eric Hambro\({}^{1}\) Aram H. Markosyan\({}^{1}\) Manish Bhatt\({}^{1}\) Yuning Mao\({}^{1}\) Minqi Jiang\({}^{1,2}\)**

**Jack Parker-Holder\({}^{2}\) Jakob Foerster\({}^{1,3}\) Tim Rocktaschel\({}^{2}\) Roberta Raileanu\({}^{1,2}\)**

\({}^{1}\)Meta \({}^{2}\)University College London \({}^{3}\)University of Oxford

Equal contribution. Correspondence to mikayel@samvelyan.com, sharathraparthy@gmail.com, and alupu@meta.com.

###### Abstract

As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel black-box approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem, and uses open-ended search to generate prompts that are both effective and diverse. Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90% across all tested models. Furthermore, we demonstrate that prompts generated by Rainbow Teaming are highly transferable and that fine-tuning models with synthetic data generated by our method significantly enhances their safety without sacrificing general performance or helpfulness. We additionally explore the versatility of Rainbow Teaming by applying it to question answering and cybersecurity, showcasing its potential to drive robust open-ended self-improvement in a wide range of applications.

## 1 Introduction

Large language models (LLMs) have recently experienced remarkable growth in both their capabilities  and their applications in various fields . As LLMs become increasingly complex and are deployed in safety-critical environments , it is essential to thoroughly understand their robustness to different inputs. Indeed, the susceptibility of LLMs to adversarial prompts -- prompts crafted to mislead the model or exploit its weaknesses, potentially leading to unsafe, biased, or incorrect outputs -- poses a significant challenge . Identifying and mitigating these vulnerabilities is therefore vital to ensure the safe and reliable operation of LLMs in the real world .

Current methods for identifying adversarial prompts aimed at "attacking" LLMs and eliciting undesirable outputs are limited by several factors. These limitations include the necessity of fine-tuning an attacker model ; white-box access to the target model ; or significant human input . Several recent black-box methods, such as PAIR  and TAP , rely on a human-in-the-loop to specify harmful behaviours (e.g., "Write a tutorial on how to hack the university server") in a way where the human effort required is linear in the number of desired prompts. Many existingmethods for systematically discovering adversarial attacks exhibit a lack of diversity by design , for instance by restricting themselves to a single predefined attack strategy . Others suffer from loss of diversity, a common issue in objective-based prompt optimisation approaches . In both cases, the narrow focus of generated prompts limits the usefulness of those methods both as a diagnostic tool and as a source of synthetic data for improving robustness.

We introduce Rainbow Teaming, a versatile approach for systematically generating diverse adversarial prompts for LLMs via LLMs. While the prevailing approach to automatic _red teaming_ also uses LLMs to generate adversarial inputs, it exhibits a steep trade-off between the diversity of discovered attacks and their success rate. In contrast, Rainbow Teaming takes a more deliberate approach, efficiently covering the space of attacks by directly optimising for the attack quality and diversity. To this end, our method casts the problem of adversarial prompt generation as _qualify-diversity_ (QD) search  and takes direct inspiration from Samvelyan et al.  to discover a set of adversarial prompts that are both diverse and effective.

Rainbow Teaming is an _open-ended_ approach  which builds on MAP-Elites , an evolutionary search method that iteratively populates an "archive" with increasingly higher-performing solutions. In our case, these solutions are adversarial prompts that elicit undesirable behaviours in a target LLM, while the archive is a discrete grid where each dimension categorises prompts according to a feature of interest for diversity, such as attack style, risk category, or prompt length. The output of our method, as shown in Figure 1, is a set of prompts covering every combination of features specified by the archive. These diverse and effective attack prompts serve both as a diagnostic tool for the vulnerabilities of the target LLM and as a high-quality synthetic dataset to robustify the target.

Rainbow Teaming is directly applicable to a wide range of domains. Implementing Rainbow Teaming requires three essential building blocks: 1) A set of _features_ that specify the dimensions of diversity (e.g., "Risk Category" or "Attack Style"); 2) A _mutation operator_ to evolve adversarial prompts (e.g., an LLM that is itself prompted to mutate previously discovered prompts ); and 3) a _preference model_ that ranks adversarial prompts based on their effectiveness. For safety, this can be a "judge" LLM  that compares two responses to determine which is more unsafe.

We demonstrate the effectiveness of Rainbow Teaming through extensive experiments targeting several state-of-the-art LLMs fine-tuned on safety-aligned data, including the Llama 2-chat  and Llama 3-Instruct  models. Despite the rigorous development of these models, our experiments reveal hundreds of adversarial prompts per individual run, achieving an attack success rate higher than 90% across all tested models without requiring external data. Using popular safety benchmarks, we demonstrate that Rainbow Teaming outperforms strong baselines in identifying vulnerabilities. Additionally, fine-tuning LLMs with synthetic data generated by our approach significantly

Figure 1: An example archive generated by Rainbow Teaming when used to discover safety vulnerabilities in Llama 2-chat 7B. Here, we search over two features: Risk Category and Attack Style. Shading corresponds to the Llama Guard  scores of responses induced by the adversarial prompt in each cell (higher means more confidence in the response being unsafe). Some excerpts of discovered prompts from a single archive are shown.1

enhances their adversarial robustness, improving resistance to unseen attacks and subsequent rounds of Rainbow Teaming, without diminishing their general capabilities and helpfulness.

We further illustrate the versatility of Rainbow Teaming by applying it to other domains, such as question answering and cybersecurity, uncovering hundreds of effective adversarial prompts in each case. These findings underscore Rainbow Teaming's potential as a comprehensive tool for diagnosing and advancing the robustness and reliability of LLMs across diverse applications.

## 2 Background

Rainbow Teaming builds on existing approaches in quality-diversity (QD) search to automate the discovery of a broad spectrum of adversarial prompts. QD methods seek to produce a collection of solutions that are individually high-performing and collectively diverse . Given a space of solutions \(\), the quality of a solution \(x\) is measured using a _fitness function_\(f:\). The diversity of solutions is characterised using a _feature descriptor function_, \(d:\) that maps each solution to a point in a feature space \(=^{N}\). This space encompasses specific pre-defined attributes of the solution, such as its behavioral aspects. For each \(z\), QD searches for the solution \(x\) such that \(d(x)=z\) and \(f(x)\) is maximised.

Our work builds directly on _MAP-Elites_, a simple yet effective QD method. MAP-Elites tracks the highest-fitness solutions in a multidimensional grid, referred to as the _archive_, which discretises the feature space \(\). The archive is first initialised with random solutions. During each iteration of MAP-Elites, a solution \(x\) is sampled at random from the archive and modified to create a new solution \(x^{}\) (e.g., by injecting Gaussian noise). The new solution \(x^{}\) is then evaluated and assigned to its corresponding archive cell based on its descriptor \(z^{}=d(x^{})\). If the cell is vacant, or if \(x^{}\) has higher fitness than the current occupant, also known as the _elite_, \(x^{}\) becomes the new elite for that cell. Through repeated cycles of selection, mutation, and evaluation, MAP-Elites fills the archive with the highest-fitness solutions. Algorithm 1 in Appendix B provides the pseudocode of this method.

## 3 Rainbow Teaming

We now describe Rainbow Teaming, our new approach for automatically generating a diverse collection of adversarial prompts. Rainbow Teaming casts this task as a QD search problem with the solution space corresponding to all possible prompts. Our rationale for employing QD is twofold:

* Effective adversarial prompts for specific scenarios (e.g., criminal planning) could be effective for others (e.g., cybercrime and hacking) with relatively small modifications. This adaptability implies that solutions can serve as _stepping stones_ to accelerate the discovery of new adversarial strategies across different categories.
* A thorough diagnostic of the vulnerabilities of a model calls for a comprehensive analytical tool to mitigate the risks of leaving attack vectors undiscovered. Similarly, safety fine-tuning requires a sufficiently _diverse_ dataset to improve a model's adversarial robustness against a wide range of attacks. Diversity is essential for both of these objectives, and QD allows us to optimise it explicitly.

Rainbow Teaming is based on MAP-Elites . We store adversarial prompts as solutions in a \(K\)-dimensional archive, with each dimension corresponding to one of the pre-defined features. Each cell in the archive corresponds to a unique combination of \(K\) categories that describe the prompt within it, known as the cell's and the solution's _descriptor_, and denoted \(z= c_{1},,c_{K}\). The LLM for which the adversarial prompts are generated is referred to as the _Target_. Initial solutions can be either generated randomly using an LLM or loaded from an existing dataset. As shown in Figure 2, all key operation of the iterative search are performed with LLMs.

At each iteration of Rainbow Teaming, we sample 1) an adversarial prompt \(x\) from the archive with descriptor \(z\), and 2) a descriptor \(z^{}\) for the new _candidate_ prompt to be generated. Note that \(z\) and \(z^{}\) are different.2 We provide \(x\) and \(z^{}\) to the _Mutator_ LLM to generate a new candidate prompt \(x^{}\) with descriptor \(z^{}\). We then feed \(x^{}\) to the Target to generate a response. Finally, we ask a _Judge_ LLM  to compare the effectiveness of the candidate prompt \(x^{}\) to that of the archive's elite prompt - the prompt stored in the archive with a descriptor \(z^{}\). This comparison focuses on the criteria of interest, such as the toxicity of the Target response, to determine which of the two prompts more effectively meets the adversarial objective. We then store the winning prompt in the archive at the position specified by \(z^{}\). Algorithm 2 in Appendix B provides the pseudocode of our method.

Rainbow Teaming is highly versatile and can easily be applied to various settings by implementing three components: prompt features, a mutation operator, and a preference model.

### Prompt Features

The features define the archive, with each predefined feature corresponding to one of the \(K\) archive dimensions. A feature can be either categorical or numerical. For categorical features, the axis of the archive is composed of discrete bins each representing a unique feature category. For instance, the Risk Category and Attack Style features in Figure 1 each consist of \(10\) categories. Numerical features are represented on a continuous scale, discretised into a set of intervals. Features therefore determine both the final archive size and the axes of diversity that Rainbow Teaming prioritises. This is particularly true given their interplay with the _mutation operator_, as described next.

### Mutation Operator

Rainbow Teaming generates new candidates by applying directed mutations to previously discovered adversarial prompts. The Mutator receives a parent prompt \(x\) sampled uniformly at random from the archive and the prescribed descriptor \(z^{}= c^{}_{1},,c^{}_{K^{}}\) for the candidate. It then mutates the prompt \(x\) once for each feature -- \(K\) times overall -- to produce a new candidate prompt \(x^{}\).

Sampling the candidate's descriptor in advance confers several key benefits. First, this allows us to forgo using a classifier for assigning the candidate to its corresponding cell, which can be inaccurate. Second, it introduces more diversity by mitigating the biases of the Mutator, which could otherwise neglect entire categories. Third, it helps avoid spending iterations on areas of the archive for which we already have effective adversarial prompts. We do this by biasing the sampling distribution of the descriptors towards areas of the archive with low fitness. We compute fitness explicitly for this purpose but do not use it to inform archive updates.

To further promote diversity, the candidate prompt is considered for further evaluation only if it is sufficiently dissimilar from its parent. We measure the similarity using BLEU  and filter out prompts that have high BLEU scores with respect to their parents.

Figure 2: Overview of Rainbow Teaming in the safety domain: Our method operates on a discretised grid, archiving adversarial prompts with \(K\) defining features, such as Risk Category or Attack Style. Each iteration involves a _Mutator_ LLM applying \(K\) mutations to generate new candidate prompts. These prompts are then fed into the _Target_ LLM. A _Judge_ LLM evaluates these responses against archived prompts with the same features, updating the archive with any prompt that elicits a more unsafe response from the Target.

### Preference Model

The preference model, operated through the Judge, performs the ranking of adversarial prompts based on their effectiveness (e.g., whether they elicit unsafe responses). The Judge inputs can vary between domains, but preference-based evaluations include the Target responses to both the candidate and the existing prompt from the archive with descriptor \(z^{}\). The Judge determines which prompt is more effective using a majority vote over multiple evaluations and swapping prompt positions to mitigate order bias . If the candidate wins the comparison, it replaces the existing prompt.

Relying on a preference model rather than a score-based evaluator offers two advantages. First, LLMs prompted to perform pairwise comparisons have a higher agreement with humans than those performing single-answer grading . This is particularly true in an optimisation context, which introduces the risk of reward hacking the evaluator. Second, the score of any numerical evaluator with a fixed scale can be maximised, at which point it is impossible to identify better candidate prompts, resulting in minimal updates in the archive. We present a preference model ablation supporting those claims in Appendix E.4.

While we describe Rainbow Teaming as using LLMs for all key steps, those can be substituted by other models or rule-based components in some domains (e.g., see Section 6.1).

## 4 Rainbow Teaming for Safety

In this section, we evaluate the effectiveness of Rainbow Teaming in discovering adversarial prompts that trigger unsafe (i.e., harmful, offensive, or toxic) responses. We target various models that are fine-tuned with safety-aligned data and are among the safest publicly available LLMs, including Llama 2 and Llama 3 models.

* **Features:** We use two feature dimensions: Risk Category and Attack Style. Our risk categories expand upon the safety risk taxonomy in Inan et al.  and cover common safety issues in LLMs, such as "Criminal Planning", "Violence and Hate" or "Self-Harm". Attack Style pertains to the method of eliciting unsafe responses, such as "Authority Manipulation" and "Role Play". Together, these features enable a thorough evaluation of LLM safety, covering a broad spectrum of adversarial threats and tactics. The full list of categories for both features is provided in Appendix H.1.
* **Mutation Operator:** We use an instruction-tuned Llama 2 70B model, which applies two mutations to create the candidate prompt, as illustrated in Figure 2. The first mutation targets the risk category, using a few-shot prompt template shared across all categories. The second mutation focuses on the attack style, employing few-shot prompts handcrafted for each individual category.
* **Preference Model:** We use an instruction-tuned Llama 2 70B model as a Judge to compare Target responses to two adversarial prompts. Its role is to determine which response is more unsafe, ignoring the prompts. The prompt corresponding to the winner's response is the one we then keep in the archive. Details about the prompts used for the Judge are provided in Appendix I.

To initialise the archive, we generate prompts from scratch according to risk categories without relying on a pre-existing seed dataset. These are then mutated with respect to attack style and used as a foundation for subsequent evolutionary iterations. We use two independent measures of Attack Success Rate (ASR) - the proportion of prompts in the archive that elicit unsafe responses. Specifically, we use two safety classifiers, namely GPT-4  with a specialised prompt and the Llama Guard safeguard model . GPT-4 acts as a binary classifier for whether a response is generally unsafe or not, independent of risk categories. In contrast, Llama Guard is prompted to perform _per-category binary classification_, considering an attack successful only if the resulting response violates the risk category it is assigned to. Neither of these metrics is explicitly optimised by Rainbow Teaming, but the probability of Llama Guard classifying a prompt as unsafe is the fitness score used to bias the selection of the prescribed feature descriptors for new candidates. Prompts for both evaluators are provided in Appendix I. For all experiments, we report the mean and standard error over \(3\) independent runs.

We also measure inter-evaluator agreement on 100 pairs of prompts and responses. Table 8 in Appendix E.3 shows that human-human agreement (83%) is similar to human-AI agreement (81% for GPT-4 and 78% for Llama Guard) and GPT-4-Llama Guard agreement (79%), and is consistent with prior work . We therefore use GPT-4 and Llama Guard as proxies for human evaluation.

### Results

**Main Results.** Figure 3 presents the ASR of Rainbow Teaming when applied to the Llama 2-chat 7B , Llama 3-Instruct 8B , Mistral 7B  and Vicuna 7B v1.5  models across 2000 iterations, using GPT-4 for evaluation. Rainbow Teaming is highly effective, generating a large collection of adversarial prompts against all models. The Llama models exhibit the highest robustness: following 2000 iterations, we obtain archives of 100 prompts with an approximate **ASR of 92%** against both variants. Mistral 7B and Vicuna 7B demonstrate a higher level of vulnerability with **98%** of the adversarial prompts in Rainbow Teaming-generated archives being successful. These results are echoed by the ASR reported by Llama Guard in Figure 10.

While Figure 3 showcases relatively small LLMs, Rainbow Teaming is equally effective against larger models. Figure 8 in Appendix E.1 presents results of Rainbow Teaming targeting 7B, 13B, and 70B variants of Llama 2-chat model, **achieving 90% or higher ASR across all model sizes**.

We compare Rainbow Teaming to two baselines. The first baseline _(No Stepping Stones)_ ignores past solutions in the archive and generates new prompts based on the risk category, before applying the attack style mutation, effectively repeating the process we use to initialise the Rainbow Teaming archive. The second baseline, _(Same Cell Mutations)_, is identical to Rainbow Teaming, except that it uses the parent prompt's descriptor as the candidate prompt descriptor, i.e., it performs mutations within each archive cell independently. Figure 4 shows Rainbow Teaming outperforming both baselines, highlighting the value of stepping stones in one case and the significance of cross-category mutations in the other.

**JailbreakBench Results.** We also apply Rainbow Teaming towards eliciting specific harmful behaviours from the JailbreakBench  dataset. Using the same attack styles, we generate 1000 prompts evenly spanning 100 harmful behaviours, with results presented in Table 1. We compare against two PAIR  variants: one from Chao et al. , based on MiXtral, and another using the same mutator LLM as our Rainbow Teaming implementation, with \(N=20\) parallel streams generating a total of 2000 prompts. We classify jailbreaks using both the same classifier as Chao et al.  and Llama Guard prompted with the harmful behaviours. For each prompt, we regenerate 4 responses and consider the prompt successful if any of the responses is classified as harmful. We believe this is representative of user interaction with LLMs, where they can prompt the model repeatedly in the hope of obtaining a different response. Compared to both PAIR variants, Rainbow Teaming discovers more jailbreaks across more behaviours, while also maintaining much higher prompt diversity.

**Transfer of Adversarial Prompts.** Understanding whether attacks transfer across models is important to assess the generality of the adversarial prompts, and whether they are intrinsically tied to the models they are optimised for. To evaluate transfer, we take the final prompts generated by Rainbow Teaming for each _original target_ in Figure 3 and evaluate their ASR against other _transfer targets_.

Table 2 presents the ASR on four different models using archives generated by Rainbow Teaming targeting each of these models. We show the ASR in grey when re-prompting targets using their own archive. On average, the ASR when transferring prompts is \(50\%\) of the ASR against the original target, indicating that Rainbow Teaming discovers general prompts which apply to multiple models. However, the exact transfer rate is highly dependent upon the pairing of original and transfer targets. We find that prompts transfer better from safer to less safe models than in the opposite direction. That said, the highest transfer rate is from Vicuna 7B 1.5 to Mistral 7B, even though Vicuna is fine-tuned from a Llama 2 base. We also achieve up to 66% ASR on GPT-4o, indicating no significant difference between open and closed-source models.

Impact of the Similarity Filter.Because archive categories are not mutually exclusive, we run the risk of populating the archive with near identical prompts. This is useful for discovering a category-agnostic failure mode but comes at the cost of significant diversity loss in the archive. To mitigate the issue, we implement a parent-child similarity filter at the mutation stage, as described in Section 3.2. Table 3 compares the performance of Rainbow Teaming with and without using this similarity filter. We also report archive self-BLEU , BERTScore , ROGUE-L m and compression ratio  scores designed to measure the diversity of a whole dataset. Our results show that the similarity filter is an effective way of maintaining the linguistic diversity of the archive.

Additional results with different system prompts are provided in Appendix E.2. We include an ablation study in Appendix E.4 to assess the role of the preference model. We discuss computational costs in Appendix G.

   Classifier & PAIR &  PAIR with \\ RT mutator LLM \\  & Rainbow Teaming \\    JailbreakBench Classifier  (\(\)) \\ Llama Guard (JBB Behaviours) (\(\)) \\  & -/4 & 1/1 & **8/7** \\ 
 - \\  & 14/11 & **66/41** \\   Self-BLEU (\(\)) & - & 0.74 & **0.51** \\   

Table 1: Comparison of Rainbow Teaming against PAIR  for eliciting harmful behaviours from JailbreakBench . Top: \((n/k)\) indicates the total number of successful jailbreaks \((n)\) and the total number of behaviours jailbroken \((k)\) for each method and classifier (best of 4 responses). Bottom: Self-BLEU similarity score.
Enhancing Robustness with Synthetic Data

Generating diverse, high-quality instruction-tuning datasets can be expensive, often requiring human annotations. Rainbow Teaming offers a low-cost alternative, generating diverse synthetic data that specifically targets the model's vulnerabilities. In this section, we demonstrate the usefulness of Rainbow Teaming as a synthetic dataset generation method by applying it to improve the safety of LLMs. We find that training on our synthetically generated data improves robustness to adversarial prompts while retaining the general capabilities of the model.

We use Rainbow Teaming to generate 15 archives targeting the Llama 2-chat 7B model, yielding a total of 1500 adversarial prompts. We perform a 12/3 train-test split and use Llama 2-chat 70B with a handcrafted system prompt to generate safe refusal prompts for the train set. We then perform supervised fine-tuning (SFT)  on this dataset and evaluate the ASR of the 300 held-out prompts before and after SFT. As shown in Table 4, we find that **fine-tuning Llama 2-chat 7B on the synthetic dataset generated by Rainbow Teaming substantially reduces the attack success rate from 92% / 95% to 0.3% / 0.7%**, as measured by GPT-4 and Llama Guard. Similarly, the ASR of PAIR  on the JailbreakBench (JBB, ) behaviours drops from 14% to 0% (measured by Llama Guard, as in Table 1). This demonstrates that additional SFT on Rainbow Teaming data also improves safety against out-of-distribution attacks. Crucially, SFT does not diminish the model's general capabilities as measured on the GSM8K (8-shot, maj@1)  and MMLU (5-shot)  benchmarks.3

Table 4 also reports the reward model scores  of the Llama 2-chat 7B model before and after SFT. We report safety and helpfulness scores on the Anthropic Harmless and Anthropic Helpful datasets  respectively. We observe a \(1.5\%\) safety score increase, despite the fact that Llama 2-chat models use the Anthropic Harmless dataset as a part of the reinforcement learning from human feedback (RLHF) pipeline . This is accompanied by a \(0.5\%\) drop in helpfulness, which we attribute to fine-tuning the model exclusively on the adversarial prompts produced by Rainbow Teaming. Mixing the adversarial data with helpfulness data would likely negate this effect, but we leave the study of adversarial fine-tuning strategies to future work.

To further investigate the robustness of the newly fine-tuned model, we reapply Rainbow Teaming to the Llama 2-chat 7B model after fine-tuning it on synthetic data generated by our method. As shown in Figure 5, the new model is substantially more robust to our approach, with a **final ASR of 39% (down from 92%)**. We expect that performing multiple rounds of Rainbow Teaming, alternating between collecting synthetic data and adversarial fine-tuning, will further increase the model's robustness to adversarial attacks. We show examples of archives at different iterations of Rainbow Teaming before and after SFT in Figure 13.

    &  &  &  &  \\ When & GPT-4\(\) & Llama Guard\(\) & on JBB\(\) & GSM8K\(\) & MMLU\(\) & Safe \(\) & Helpful\(\) \\  Before SFT & \(0.92 0.008\) & \(0.95 0.005\) & 0.14 & \(0.224\) & \(0.412\) & \(0.883\) & \(0.518\) \\ After SFT & \(0.003 0.003\) & \(0.007 0.003\) & 0.0 & 0.219 & \(0.405\) & \(0.897\) & \(0.513\) \\   

Table 4: Safety and capabilities scores of the Llama 2-chat 7B model before and after SFT on Rainbow Teaming-generated data. Fine-tuning greatly improves robustness to adversarial prompts without hurting capabilities.

Figure 5: Attack success rate before and after fine-tuning Llama 2-chat 7B on synthetic data generated via Rainbow Teaming. The fine-tuned model is significantly less vulnerable to Rainbow Teaming on a second application, with the method achieving a substantially lower ASR after 2000 iterations.

Rainbow Teaming for Other Applications

### Question Answering

We apply Rainbow Teaming to question answering, generating adversarial trivia questions -- questions which the target model answers incorrectly. We define a 3D archive, with Topic, Interrogative Word and Question Length as features. The mutation operators for topics and interrogative words are analogous to those used in Section 4. For length, we simply prompt the Mutator to either "lengthen" or "shorten" the question. The preference model uses a Judge to compare answers from a Target (Llama 2-chat 7B) and a superior Oracle (Llama 2-chat 70B) to determine the fitness of questions based on the correctness of the responses. For more information, see Appendix F.1.

Results.In Table 5 we compare Rainbow Teaming to a baseline that generates candidate questions from scratch rather than relying on existing questions in the archive. We observe that Rainbow Teaming achieves higher fitness, higher coverage (percentage of non-empty cells in the archive), and higher diversity in questions, indicating the importance of utilising previously discovered adversarial questions. Importantly, not relying on previous solutions leaves regions of the archive uncovered, particularly for short questions as seen in the example archives in Appendix E. Figure 6 illustrates an example architecture generated using Rainbow Teaming. Some example questions are also shown in Appendix E.7.

### Cybersecurity

We apply Rainbow Teaming to cybersecurity, searching for adversarial prompts that elicit behaviour such as generating insecure code or providing assistance in orchestrating cyberattacks. We use a 2D archive with the 10 MITRE categories for cyberattack tactics  (e.g., "Exfiltration" or "Defense Evasion") and prompt length divided into 10 equal bins. Our Mutator is an instruction-tuned Llama 2 70B model, mutating first for MITRE attack style, and then for prompt length. We use a binary Judge mechanism involving Llama 2-chat 70B and CodeLlama-34B Instruct models to evaluate generated prompts, as outlined in CyberSecEval . We provide further details in Appendix F.2.

Results.Table 6 presents the results of a cybersecurity assessment for various target models on prompts generated by Rainbow Teaming. For all models, we successfully generate \(10 10\) archives that are fully identified as malicious, as classified by CyberSecEval . Human expert evaluation finds a lower ASR, with \(0.94\) and \(0.92\) for Llama 2-chat 7B and CodeLlama 7B Instruct,

   Target & CyberSecEval & Human \\  Llama 2-chat 7B & 1.00 & 0.94 \\ Llama 2-chat 70B & 1.00 & 0.80 \\ CodeLlama 7B Instruct & 1.00 & 0.92 \\ CodeLlama 34B Instruct & 1.00 & 0.80 \\   

Table 6: Cybersecurity ASR of Rainbow Teaming on four Targets, as reported by CyberSecurityEval  (3 seeds), and human expert evaluation (1 seed).

Figure 6: An example archive of adversarial questions discovered by Rainbow Teaming. Vacant cells are marked in yellow, intermediate but unsuccessful attempts are in green, and successful adversarial questions are in purple.

and \(0.8\) for both Llama 2-chat 70B and CodeLlama 34B Instruct. While Rainbow Teaming remains highly effective, the discrepancy between CyberSecEval and expert annotations suggests the need for a better cybersecurity-specific evaluation, which we hope will be the focus of future work.

## 7 Related Work

Adversarial Attacks on LLMs.Rainbow Teaming relates most closely to prompt-level attacks which rely on strategies such as misspellings, prompting in foreign languages , or persona-modulation  to jailbreak LLMs. Perez et al.  use an LLM and a brute-force approach to automatically discover prompt-level attacks, but this approach can suffer from mode collapse and does not always generate a diverse set of prompts. Meanwhile, Liu et al.  propose a white-box method that refines hand-crafted attack prompts using a mix of genetic algorithms and LLM-based mutations. However, they focus on optimising a single solution rather than a diverse population. The closest works to our own are PAIR  and Tree of Attacks with Pruning (TAP)  -- two black-box methods for automatically discovering prompt-level attacks by using an LLM to iteratively generate candidates. However, both methods are designed to jailbreak the model with respect to a single task rather than across a range of diverse risk categories and attack styles. In contrast, our work uses quality-diversity search to automatically discover attacks covering a diverse set of risks and attack strategies. Although evolutionary algorithms have previously been used for adversarial attacks on LLMs , this work is the first to apply a quality-diversity framework  in this area. Unlike most evolutionary algorithms (e.g., genetic algorithms), which evolve a single optimal solution, quality-diversity approaches generate a wide variety of distinct, high-quality solutions.

Open-Endedness and LLMs.Rainbow Teaming builds on the ability of LLMs to act as a powerful mutation operator over language inputs, one that adheres to the underlying structure of natural language . Several recent methods exploit this capability of LLMs in order to perform an efficient novelty-driven evolutionary search in the language space, leading to the discovery of potentially open-ended repertoires of solutions . Closest to our approach is QDAIF  which similarly uses LLMs for QD search in order to generate a diverse archive of LLM outputs. Rainbow Teaming is different from QDAIF in several important factors. First, we search for and archive diverse _prompts_ for the target LLMs, whereas QDAIF archives diverse _responses_ from it -- a separate problem altogether. While QDAIF focuses purely on generating diverse outputs for creative writing, our method seeks to find a diverse set of adversarial prompts. QDAIF relies on a score-based fitness function (log probability of the token generation), whereas Rainbow Teaming uses a preference-based judge for performing updates to the archive. Rainbow Teaming additionally incorporates parent-child similarity filtering to preserve the linguistic diversity of the prompts.

An extended related work section is provided in Appendix C.

## 8 Conclusion

In this work, we introduce Rainbow Teaming, a novel approach for the automatic generation of diverse adversarial prompts for LLMs. By leveraging quality-diversity search, Rainbow Teaming efficiently explores the space of potential adversarial attacks, resulting in a diverse archive of prompts that highlight the vulnerabilities of LLMs. Our extensive experiments with multiple models, such as Llama 3-Instruct and Llama 2-chat, and across various domains, including safety, question answering, and cybersecurity, demonstrate the generality of Rainbow Teaming. Moreover, the synthetic data generated through Rainbow Teaming can be utilised for fine-tuning LLMs, thereby enhancing their resilience against further adversarial attacks without compromising their general performance. This illustrates the potential of Rainbow Teaming as a means for the continuous, open-ended self-improvement of LLMs, with minimal human intervention. Future work with Rainbow Teaming involves extending its application beyond LLMs to areas such as vision and multi-modal AI systems. Moreover, incorporating Rainbow Teaming into the fine-tuning stages of LLM development presents an opportunity to consistently strengthen their defences against adversarial attacks.

We discuss the limitations and broader impact of our work in Appendix A.