# On Generalization Bounds for Projective Clustering

Maria Sofia Bucarelli\({}^{1}\)  Matilde Fjeldso Larsen\({}^{2}\) Chris Schwiegelshohn\({}^{2}\) Mads Bech Toftrup\({}^{2}\)

\({}^{1}\)Department of Computer, Control and Management Engineering Antonio Ruberti,

Sapienza University of Rome, Italy

\({}^{2}\)Department of Computer Science, Aarhus University, Denmark

mariasofia.bucarelli@uniroma1.it 201805091@post.au.dk

{schwiegelshohn,toftrup}@cs.au.dk

###### Abstract

Given a set of points, clustering consists of finding a partition of a point set into \(k\) clusters such that the center to which a point is assigned is as close as possible. Most commonly, centers are points themselves, which leads to the famous \(k\)-median and \(k\)-means objectives. One may also choose centers to be \(j\) dimensional subspaces, which gives rise to subspace clustering. In this paper, we consider learning bounds for these problems. That is, given a set of \(n\) samples \(P\) drawn independently from some unknown, but fixed distribution \(\), how quickly does a solution computed on \(P\) converge to the optimal clustering of \(\)? We give several near optimal results. In particular,

1. For center-based objectives, we show a convergence rate of \(()\). This matches the known optimal bounds of [Fefferman, Mitter, and Narayanan, Journal of the Mathematical Society 2016] and [Bartlett, Linder, and Lugosi, IEEE Trans. Inf. Theory 1998] for \(k\)-means and extends it to other important objectives such as \(k\)-median.
2. For subspace clustering with \(j\)-dimensional subspaces, we show a convergence rate of \(()\). These are the first provable bounds for most of these problems. For the specific case of projective clustering, which generalizes \(k\)-means, we show a convergence rate of \((}{{n}}})\) is necessary, thereby proving that the bounds from [Fefferman, Mitter, and Narayanan, Journal of the Mathematical Society 2016] are essentially optimal.

## 1 Introduction

Among the central questions in machine learning is, given a sample of \(n\) points \(P\) drawn from some unknown but fixed distribution \(\), how well does a classifier trained on \(P\) generalize to \(\)? The probably most popular way to formalize this question is, given a loss function \(L\) and optimal solutions \(_{P}\) and \(_{}\) for sample \(P\) and distribution \(\), respectively, how the empirical excess risk \(L(,_{P})-L(,_{})\) decreases as a function of \(n\). This paper focuses on loss functions associated with the clustering problem. Popular examples include \((k,z)\) clustering, which asks for a set of \(k\) centers \(^{d}\) minimizing the cost \(_{p P}_{s}\|p-s\|_{2}^{z}\) and more generally, \((k,j,z)\) subspace clustering which asks for a set of \(k\) subspaces \(:=\{U_{1},U_{2}, U_{k}\}\) minimizing \(_{p P}_{U_{i}}\|(I-U_{i}U_{i}^{T})p\|_{2}^{z}\). Special cases include \((k,1)\) clustering, known as \(k\)-median, \((k,2)\) clustering known as \(k\)-means and \((k,j,2)\) clustering known as projective clustering. Generally, there seems to be an interest in varying \(z\), as letting \(z\) tend towards \(1\) tends to result in outlier-robust clusterings. The problem is less widely explored for \(z>2\), although in particular for subspaceapproximation there is some recent work [27; 34; 79; 77]. Higher powers give more emphasis on outliers. For example, centralised moments with respect to the three and four norms are skewness and kurtosis, respectively, and are extensively employed in statistics, see Cohen-Addad et al.  for previous work on clustering with these measures. Fitting a mixture model with respect to skewness minimizes asymmetry around the target center. Studing the problems for \(z\) is very well motivated, as the \((1,)\) clustering is equivalent to the minimum enclosing ball problem. Unfortunately, one often requires additional assumptions, as the minimum enclosing ball problem suffers from the curse of dimensionality , is very prone to outliers [25; 38].

Despite a huge interest and a substantial amount of research, so far optimal risk bounds \(()\)1 for the \(k\)-means problem have been established, see the seminal paper by Fefferman et al.  for the upper bound and Bartlett et al.  for nearly matching lower bounds. For general \((k,z)\)-clustering problems, the best known results prove a risk bound of \(O(}{{n}}})\). For \((k,j,2)\) clustering, the best known bounds of \((}{{n}}})\) are due to Fefferman et al. . Thus, the following question naturally arises:

Is it possible to obtain optimal generalization bounds for all \((k,j,z)\)-clustering objectives?

We answer this question in the affirmative whenever \(j\) and \(z\) are constant, which seems to be the most relevant case in practise . Specifically, we show

* The excess risk bound for \((k,z)\)-clustering when given \(n\) independent samples from an unknown fixed distribution \(\) is bounded by \(()\), matching the lower bound of .
* The excess risk bound for \((k,j,z)\)-clustering when given \(n\) independent samples from an unknown fixed distribution \(\) is bounded by \((}}{{n}})\).
* There exists a distribution such that the excess risk for the \((k,j,2)\)-clustering problem is at least \((}}{{n}})\), matching the upper bound of Fefferman et al.  up to polylog factors.

We note that we assume \(z\) to be a constant, which is the case for projective clustering, \(k\)-means and \(k\)-median. For non-constant \(z\), the dependency on \(z\) is exponential.

### Related work

The most basic question one could answer is if the empirical estimation performed on \(P\) is consistent, i.e. as \(n\), whether the excess risk tends to \(0\). This was shown in a series of works by Pollard [64; 66], see also Abaya and Wise . Subsequent work then analyzed the convergence rate of the risk. The first works in this direction proved convergence rates of the order \((1/)\) without giving dependencies on other parameters [22; 65]. Linder et al.  gave an upper bound of \(O(d^{3/2})\). Linder  improved the upper bound to \(O(d)\). Bartlett et al.  showed an upper bound \(O()\) and gave a lower bound of \((/n})\). Motivated by applications of clustering for high dimensional kernel spaces [7; 18; 20; 21; 37; 39; 56; 58; 67; 78; 80; 81], research subsequently turned its efforts towards minimizing the dependency on the dimension. Biau et al.  presented an upper bound of \(O(k/)\), see also the work by Clemenccon . Fefferman et al.  gave a matching upper bound of the order \(O()\), which was later recovered by using techniques from Foster and Rakhlin  and Liu . Further improvements require additional assumptions of the distribution \(\), see Antos et al. , Levrard , Li and Liu . For subspace clustering, there have only been results published for the case \(z=2\)[39; 50; 72], for which the state of the art provides a \((}}{{n}})\) risk bound due to Fefferman et al. . A highly related line of research originated with the study of coresets for compression. For Euclidean \((k,z)\) clustering, coresets with space bounds of \((}{{z^{2+z}}})\) have been established [30; 32], which roughly corresponds to a error rate of \((}{{n}}})\) as a function of the size of the compression. For the specific case of \(k\)-median and \(k\)-means, coresets with space bounds of \((}}{{^{2}}})\) are known , which corresponds to a error rate of \((}}{{n}}})\). Both results are optimal for certain ranges of \(\) and \(k\) and while these bounds are worse than what we hope to achieve for generalization, many of the techniques such as terminal embeddings are relevant for both fields. For \((k,j,z)\) clustering, coresets are only known to exist under certain assumptions, where the provable size is \(((k,j,^{-1}))\)[40; 44].

## 2 Preliminaries

We use \(\|x\|_{p}:=[p]{|x_{i}|^{p}}\) to denote the \(_{p}\) norm of a vector \(x\). For \(p\), we define the limiting norm \(\|x\|_{}= x_{i}\). Further, we refer to the \(d\)-dimensional unit Euclidean ball by \(B_{2}^{d}\), i.e. \(x B_{2}^{d}\) is a vector in \(^{d}\) and \(\|x\|_{2}:=^{d}x_{i}^{2}} 1\). Let \(U\) be a \(d j\) orthogonal matrix, i.e., with pairwise and orthogonal columns with unit Euclidean norm. We say that \(UU^{T}\) is the projection matrix associated with \(U\). Let \(z\) be a positive integer. Given any set \(\) of \(k\) points in \(B_{2}^{d}\) we denote the \((k,z)\)-clustering cost for a point set \(P\) with respect to solution \(\) as

\[(P,):=_{p P}_{s}\|p-s\|_{2}^{z}.\]

Special cases include \(k\)-means (\(z=2\)) and \(k\)-median (\(z=1\)). Similarly, given a collection \(\) of \(k\) orthogonal matrices of rank at most \(j\), we denote the \((k,j,z)\)-clustering cost of a point set \(P\) as

\[(P,):=_{p P}_{U}\|(I-UU^{T})p\| _{2}^{z}.\]

The specific case \((k,j,2)\) is often known as projective clustering in literature. The cost vector \(v^{,P}^{|P|}\), respectively \(v^{,P}^{|P|}\) has entries \(v_{p}^{}=_{s}\|p-s\|_{2}^{z}\), respectively \(v_{p}^{}=_{U}\|(I-UU^{T})p\|_{2}^{z}\) for \(p P\). We will omit \(P\) from \(v^{,P}\) and \(v^{,P}\), if \(P\) is clear from context. The overall cost is \(\|v^{}\|_{1}=_{p P}_{s}\|p-s\|_{2}^{z}\) and \(\|v^{}\|_{1}=_{p P}_{U}\|(I-UU^{T})p\|_{2}^ {z}\). The set of all cost vectors is denoted by \(V\).

Let \(\) be an unknown but fixed distribution on \(B_{2}^{d}\) with probability density function \(\). For any solution \(\), respectively \(\), we define \((,):=_{p B_{2}^{d}}_{s}\|p-s\|^{z}[p]dp\) and \(OPT:=_{}(,)\) and respectively \((,):=_{p B_{2}^{d}}_{U}\|(I-UU^{T})p\|^{z}[p]dp\) and \(OPT:=_{}(,)\). Let \(P\) be a set of \(n\) points sampled independently from \(\). We denote the cost of the empirical risk minimizer on \(P\) by \(OPT_{P}:=_{}\|v^{}\|_{1}\), and respectively, \(OPT_{P}:=_{}\|v^{}\|_{1}\). The excess risk of \(P\) with respect to a set of cost vectors is denoted by

\[_{|P|}(V):=_{P}[OPT_{P}]-OPT.\]

Finally, we use the notion of a net. Let \((V,dist)\) be a metric space, \((V,dist,)\) is an \(\)-net of the set of vectors \(V\), if for all \(v V\)\(\)\(v^{}(V,dist,)\) such that \(dist(v,v^{})\). We will particularly focus on nets for cost vectors induced by \((k,z)\)-clustering and \((k,j,z)\)-clustering defined as follows, prior work has proposed similar nets for coresets and sublinear algorithms for \((k,z)\) clustering .

**Definition 2.1** (Clustering Nets).: A set \(_{}\) of \(|P|\)-dimensional vectors is an \(\)-clustering net if for every cost vector \(v\) obtained from a solution \(\) or \(\), there exists a vector \(v^{}_{}\) with \(\|v^{}-v\|_{}\)

A slightly weaker condition as required by these nets requiring only \(\|v^{}-v\|_{2}\) would also be sufficient. Nevertheless, we are not able to show better bounds when relaxing the condition and having a point-wise guarantee may be of independent interest.

## 3 Outline and technical contribution

Due to space restrictions, the full proofs are provided in the supplementary material. In this section, we endeavour to present a complete and accessible overview of the key ideas behind the theorems.

Let \(P\) be a set of \(n\) points sampled independently from some unknown but fixed distribution \(\). To show that the excessive risk with respect to clustering objectives is in \((f(n))\) for some function \(f\), it is sufficient to show two things. First, that for the optimal solution \(_{}\), the clustering cost estimated using \(P\) is close to the true cost. Second, any solution that is more expensive than \(_{}\) does not become too cheap when evaluated on \(P\). Both conditions are satisfied if for any solution \(\)

\[|(P,)-(,)|(f(n)).\]

Showing \(|(P,_{})-( ,_{})| O()\) with good probability is typically a straightforward application of concentration bounds such as Chernoff's bound. In fact, these concentration bounds show something even stronger. Given \(t\) solutions \(_{1},_{t}\), we have

\[_{P}_{_{t}}|(P,_{i})-(,_{i})| O(}).\] (1)

What remains is to bound the number of solutions \(t\).

Clustering nets and dimension reduction for center based clusteringUnfortunately, the total number of expensive clusterings in Euclidean space is infinite, making a straightforward application of 1 useless. Nets as per Definition 2.1 are now typically used to reduce the infinite number of solutions to a finite number. Specifically, one has to show that by preserving the costs of all solutions in the net, the cost of any other solution is also preserved. Using basic techniques from high dimensional computational geometry, it is readily possible to prove that a \(\)-net for \((k,j,z)\) clustering of size \((k j d^{-1})\) exists, where \(d\) is the dimension of the ambient space. Plugging this into Equation 1 and setting \(^{-1}=n^{2}\) then yields a generalization bound of the order \(O(})\). Unfortunately, this leads to a dependency on \(d\), which is suboptimal. To improve the upper bounds, we take inspiration from coreset research. For \((k,z)\)-clustering, a number of works have investigated dimension reduction techniques known as terminal embeddings, see . Given a set of points \(P^{d}\), a terminal embedding \(f:^{d}^{m}\) guarantees \(\|p-q\|_{2}=(1)\|f(p)-f(q)\|_{2}\) for any \(p P\) and \(q^{d}\). Terminal embeddings are very closely related to the Johnson-Lindenstrauss lemma, see  for applications to clustering, but more powerful in key regard: only one of the points is required to be in \(P\). The added guarantee extended to arbitrary \(q^{d}\) due to terminal embeddings allows us to capture all possible solutions. There are also even simpler proofs for \(k\)-mean that avoid this machinery entirely, see . Unfortunately, these arguments are heavily reliant on properties of inner products and are difficult to extend to other values of \(z\). The terminal embedding technique may be readily adapted to \((k,z)\)-clustering, though some care in the analysis must be made to avoid the worse dependencies on the sample size necessitated for the coreset guarantee, described as follows.

Improving the union bound via chaining:To illustrate the chaining technique, consider the simple application of the union bound for a terminal embedding with target dimension \(m=(^{-2} n)\), see the main result of Narayanan and Nelson . Replacing the dependency on \(d\) with an appropriately chosen parameters and plugging the resulting net \(N_{}\) of size \((k^{-2} n^{-1})\) yields a generalization bound of \(O(})\) for \((k,z)\) clustering. We improve on this using a chaining analysis, see  for its application to coresets for \((k,z)\) clustering and  for \((k,j,2)\) clusterings. Specifically, we use a nested sequence of nets \(N_{1/2},N_{1/4},N_{1/8},,N_{2^{-2 n}}\). Note that for every solution \(\), we may now write \((p,)\) for any \(p P\) as a telescoping sum

\[(p,)=_{h=0}^{}(p,_{2^{-(h +1)}})-(p,_{2^{-h}})\]

with \({}_{_{2^{-h}}} N_{h}\) and \((p,_{1})\) being set to \(0\). We use this as follows. Suppose for some solution \(\), we have solutions \(_{2^{-h}} N_{2^{-h}}\) and \(_{2^{-(h+1)}} N_{2^{-(h+1)}}\). Then \(|(p,_{2^{-h}})-(p,_{2^{-(h+1)}})|  O(2^{-h})|(p,_{2^{-h}})-(p, )|\) for all \(p P\). Instead of applying the union bound for a small set of solutions, we apply the union bound along every pair of solutions appearing in the telescoping sum. Using arguments similar to Equation 1, we then obtain

\[_{P}_{_{2^{-h}} _{2^{-(h+1)}}\\  N_{h} X_{h+1}}|(P,_{2^{-h}})-(P,_{2^{-(h+1)}})|\] \[=2^{-h}(||N_{h+1} |)}{n}})=2^{-h}((k/2^{h})}{n}})(})\]

This is the desired risk bound for \((k,z)\) clustering. To complete the argument in a rigorous fashion, we must now merely combine the decomposition of \((P,)\) into the telescoping sum with the learning rate that we just derived. Indeed, this already provides a simple way of obtaining a bound on the risk of the order \(()\), which turns out to be optimal. In summary, to apply the chaining technique successfully, the following two properties are sufficient: (i) the dependency on \(\) in the net size can be at most \(((^{-2}))\), as the increase in net size is then met with a corresponding decrease between successive estimates along the chain and (ii) the nets have to preserve the cost up to an additive \(\) for _every_ sample point \(p\). The second property is captured by Definition 2.1. Both properties impose restrictions on the dimension reductions that can be successfully integrated into the chaining analysis.

Dimension reduction for projective clustering:It turns out that extending this analysis \((k,j,z)\) clustering is a major obstacle. While the chaining method itself uses no particular properties of \((k,z)\) clustering, the terminal embeddings needed to obtain nets cannot be applied to subspaces. Indeed, terminal embeddings by the very nature of their guarantee, cannot be linear2, and hence a linear structure such as a subspace will not be preserved. At this stage, there are a number of initially promising candidates that can provide alternative dimension reduction methods. For example, the classic Johnson-Lindenstrauss lemma can be realized via a random embedding matrix and, moreover, preserves subspaces, see for example . Unfortunately, as remarked by , there is an inherent difficulty in applying Johnson-Lindenstrauss type embeddings even for \((k,z)\) clustering coresets and the same arguments also apply for generalization bounds.

An alternative dimension reduction method based on principal component analysis was initially proposed by  for \((k,j,2)\), see also  and most notably  for a different variant that applies to arbitrary \((k,j,z)\) objectives. For \((k,j,2)\) clustering, it states that a dimension reduction on the first \(O(D/)\) principal components preserves the projective cost of all subspaces of dimension \(D\). Since \((k,j,2)\) clustering is a special case of a \(k j\) dimensional projection, it implies that \(O(kj/)\) dimensions are sufficient. Given that these dimension reductions are based on PCA-type methods, they are linear and therefore seem promising initially. Unfortunately, this technique has serious drawbacks. It does not satisfy the requirements for Definition 2.1, only preserving the cost on aggregate rather then per individual point, and thus cannot be combined with the chaining technique3. Without the chaining technique, the best bound one can hope for is of the order \((j^{2}/n})\), which falls short of what we are aiming for.

Another important technique used to quantify optimal solutions of \((k,j,z)\) clustering initially proposed by  and subsequently explored by  and has frequently seen use in coreset literature . Succinctly, it states that a \((1+)\) approximate solution to the \((1,j,z)\) clustering problem of a point set \(P\) is contained in a subspace spanned by \((j^{2}/)\) input points of \(P\). While this result improves over PCA for large values of \(k\), applying it only yields a learning rate of the order \(O(/n})\). It turns out that this technique has the exact same limitations as PCA, namely that costs per point are not preserved, and thus only offers a different tradeoff in parameters.

Our new insight:Given the state of the art, designing a dimension reduction technique that would enable the application of the chaining technique might seem hopeless, and indeed, we were not able to prove such. The key insight that allows us to bypass these bottlenecks is to find a dimension reduction that applies not to all solutions \(\), but only to a certain subset of them. Indeed, we show that for any point set \(P\) contained in the unit ball and any subspace \(\) of dimension \(j\), there exists a subspace \(S\) spanned by \(O(j/^{2})\) points of \(P\) such that for every point \(p\): \(|(p,)-(p_{S},_{S})|\). This is similar to the guarantee provided by  but stronger in that it (i) applies to arbitrary subspaces, which is required for the chaining analysis, and (ii) applies to each point of \(P\) individually, rather than for the entire point set \(P\) on aggregate. We then augment the chaining analysis by applying a union bound over all \(}\) possible dimension reductions, thereby capturing all solutions \(\). We are unaware of any previously successful attempts at integrating multiple dimension reductions within a chaining analysis and believe that the technique may be of independent interest.

## 4 Useful results from learning theory

Our goal is to bound the rate with which the empirical risk decreases for clustering problems. For a fixed set of \(n\) points \(P\) and a set of functions \(F:P\), we define the Rademacher complexity (\(Rad_{n}\)) and the Gaussian complexity (\(G_{n}\)) wrt \(F\) respectively as

\[Rad_{n}(F)=_{r}_{f F}_{p P}f(p) r _{p} G_{n}(F)=_{g}_{f F}_{p P}f( p) g_{p}\]

where \(r_{p}\) are independent random variables following the Rademacher distribution, whereas \(g_{p}\) are independent Gaussian random variables. In our case, we can think of \(f\) as being associated to a solution \(\) (respectively a solution \(\)) and \(f(p)=(p,)=_{s}\|p-s\|_{2}^{z}\) (respectively \(f(p)=(p,)=_{U}\|(I-UU^{T})p\|_{2}^{z}\)). Since we associate every \(f\) with a cost vector \(v^{}\), we will use \(Rad_{n}(F)\) and \(Rad_{n}(V)\) as well as \(G_{n}(F)\) and \(G_{n}(V)\) interchangeably. The following theorem is due to Bartlett and Mendelson. .

**Theorem 4.1** (Simplified variant of Theorem 8 of Bartlett and Mendelson ).: _Consider a loss function \(L:A\). Let \(F\) be a class of functions mapping from \(X\) to \(A\) and let \((X_{i})_{i=1}^{n}\) be independent samples from \(\). Then, for any integer \(n\) and any \(>0\), with probability at least \(1-\) over samples of length \(n\), denoting by \(}_{n}\) the empirical risk, every \(f F\) satisfies_

\[L(f(X))}_{n}L(f(X))+Rad_{n}(F)+}.\]

Thus, in order to bound the excess risk, Theorem 4.1 shows that it is sufficient to bound the Rademacher complexity. It is well known (see, for example, B.3 of Rudra and Wootters ) that \(Rad_{n}(V)G_{n}(V)\). Thus we can alternatively bound the Gaussian complexity, which is sometimes more convenient. Note that if \(V\) is the set of all cost vectors, clustering nets are mere \((V,\|.\|_{},)\). Using these nets, we can bound the Rademacher and Gaussian complexity. Indeed the following lemma holds.

**Lemma 4.2**.: _Let \(\) be a distribution over \(B_{2}^{d}\) and let \(P\) a set of \(n\) points sampled from \(\). Suppose that for a set of \(n\)-dimensional vector \(V\), we have an absolute constant \(C,>0\) such that \(|(V,\|.\|_{},)| O(^{-2}^{ }(n^{-1})C)\). Then_

\[G_{n}(V) O(n}{n}}).\]

The specific types of nets used in our study and the size bounds for those nets will be the key to obtaining the desired upper bounds and will be detailed in the next section.

## 5 Generalization bounds for center-based clustering and subspace clustering

We start by giving our generalization bounds for center based clustering and subspace clustering problems. For subspace clustering problems, we first state the result for general \((k,j,z)\) clustering. An improvement for the special case \(z=2\) will be given later.

**Theorem 5.1**.: _Let \(\) be a distribution over \(B_{2}^{d}\) and let \(P\) be a set of \(n\) points sampled from \(\). For any set of \(k\) points \( B_{2}^{d}\), we denote by \(v^{}\) the \(n\)-dimensional cost vector of \(P\) in solution _with respect to the \((k,z)\)-clustering objective. Moreover we denote by \(v^{}\) the \(n\)-dimensional cost vector of \(P\) in solution \(\) with respect to the \((k,j,z)\)-clustering objective. Let \(V_{z}\) be the union of all cost vectors of \(P\) for the center-based clustering and \(V_{j,z}\) the union of all cost vectors for subspace clustering. Then with probability at least \(1-\)_

\[_{n}(V_{z}) O(n}{n}}+ })\] (2) \[_{n}(V_{j,z}) O  jn log^{3}n}{n}}+}.\] (3)

Following Theorem 4.1, it is sufficient to bound the Rademacher complexity in order to bound the excess risk. The Rademacher complexity is, up to lower order terms, equal to the Gaussian complexity, which, following Lemma 4.2 may be bounded by obtaining small nets with respect to the \(\|.\|_{}\) norm. We believe that the results on the bounds of the nets, may be of independent interest and we'll state these results in the following Lemma.

**Lemma 5.2**.: _Let \(\) be a distribution over \(B_{2}^{d}\) and let \(P\) a set of \(n\) points sampled from \(\), let \(V_{z}\) be defined as in Theorem 5.1 let \(V_{j,z}\) be defined as in Theorem 5.1. Then_

\[|(V_{z},\|.\|_{},)|(O(1)z^{3 } k^{-2} n((z)+(^{-1})))\] (4) \[|(V_{j,z},\|.\|_{},)|(O(1)(3 z)z^{+2} k j^{-2}( n+j(j^{-1})) ^{-1}).\] (5)

Combining Lemma 5.2 with Lemma 4.2 now yields the immediate bound on the Rademacher and Gaussian complexity. Following the discussion from Section 3, we use terminal embeddings to prove the part of Lemma 5.2 pertaining to \((k,z)\) clustering, see Appendix B. Unfortunately, the terminal embedding technique is not admissible for obtaining nets for subspace clustering as clarified in Section 3. Thus, we use an entirely different approach. We show the existence of a collection of dimension reducing maps with subspace preserving properties. Fortunately, the number of dimension reducing maps is small. Our desired net sizes then follow by enumerating over all of these dimension reducing maps, and for the candidate solutions covered by each such dimension reducing map, we can find an efficient net. First, we introduce a slightly different, but closely related notion to \((1,j,z)\)-nets.

**Definition 5.3** (Projective Nets).: Let \(P B_{2}^{d}\) be a set of points, and let \(z\) be a positive integer. For a \(d j\) matrix \(S\) with columns that have at most unit norm and any point \(p P\), define the projective cost as \(_{proj}(p,S)=\|S^{T}p\|_{2}\). Let \(V\) be the set of all projective cost vectors induced by such matrix \(S\). We call a \((V,\|.\|_{},)\) a \((,j)\)-projective net of \(P\).

On a high level, the proof largely relies on the following decomposition. Let \(U\) be a candidate subspace and let \(\) be a projection matrix used to approximate \(\|(I-UU^{T})p\|_{2}^{2}\) We have

\[\|(I-UU^{T})p\|^{2}\!=\!}_{(1)}\!-\! p\|^{2}}_{(2)}\!+\!}_{(3)}\!-\! {\|UU^{T}(I-)p\|^{2}}_{(4)}\!+\! UU^{T}(I-)p}_{(5)}\] (6)

Here, we wish to select \(\) such that \(\|U^{T}(I-)p\|_{2}\) is small for all \(p P\). Note that this implies that the terms \(2p^{T} UU^{T}(I-)p\) and \(\|UU^{T}(I-)p\|^{2}\) are small. For the term (2), we merely have to show that projective nets exist. If the number of \(\) is small, we can further construct good nets for the terms (1) and (3). We start by giving a bound for the projective nets. Our first Lemma 5.4 shows that if the points lie in a sufficiently low-dimensional space, such a net can be obtained by constructing a net \((B_{2}^{d},\|.\|_{2},^{})\) for a sufficiently small \(^{}\).

**Lemma 5.4**.: _Let \(P B_{2}^{d}\) be a set of points, and \(z\) be a positive integer. Then there exists an \((,j)\)-projective net of size \(|(V,\|.\|_{},)|(O(1) d j (j^{-1}))\)._

To reduce the dependency on the dimension, we now use the following lemma. Essentially, it shows that in order to retain the properties of \(U\), we can find a projection matrix \(\) of rank at most \(O(j^{-2})\).

**Lemma 5.5**.: _Let \(P B_{2}^{d}\). For any orthogonal matrix \(U^{j d}\), there exists \(M P\), with \(|M| O(j^{-2})\), such that \( p P,\|U^{T}(I-_{M})p\|\|(I-_{M})p\|\)._We now use this lemma as follows. We can efficiently enumerate over all candidate \(\), as Lemma 5.5 guarantees us that we only have to consider \(}(j^{-2} n)\) many different \(M\) inducing projection matrices. This immediately gives us \(0\)-nets for the terms (1) and (3). For each \(\), we then apply Lemma 5.4, which gives us a net for term (2). Finally, by choice of \(\), we can show that terms (4) and (5) are negligible.

### Tight generalization bounds for projective clustering

For the specific case of \((k,j,2)\) clustering, also known as projective clustering, we obtain an even better dependency on \(j\). A similar bound can likely also be derived using the seminal work of , though the dependencies on \( n\) and \( 1/\) are slightly weaker. The proof uses the main result by , itself heavily inspired by , and arguments related to bounding the Rademacher complexity of linear function classes. Crucially, it avoids the issue of obtaining an explicit dimension reduction entirely, but the approach cannot be extended to general \((k,j,z)\) clustering.

**Theorem 5.6**.: _Let \(\) be a distribution over \(B_{2}^{d}\) and let \(P\) a set of \(n\) points sampled from \(\). For any set \(\) of \(k\) orthogonal matrices of rank at most \(j\), we denote by \(v^{}\) the \(n\)-dimensional cost vector of \(P\) in solution \(\) with respect to the \((k,j,2)\)-clustering objective, i.e. \(v_{p}^{}=_{U}\|(I-UU^{T})p\|^{2}\). Let \(V_{j,2}\) be the union of all cost vectors of \(P\). Then with probability at least \(1-\) for any \(>0\)_

\[_{n}(V_{j,2}) O(^{3+}( )}+}).\]

Finally, we also show that the bounds from Theorem 5.6 and  are optimal up to polylogarithmic factors.

**Theorem 5.7**.: _There exists a distribution \(\) supported on \(B_{2}^{d}\) such that \(_{n}(V_{j,2})()\)._

The rough idea is to define a distribution \(\) supported on the nodes of a \(2kj\)-dimensional simplex with some points having more probability mass and some points having smaller mass. Using the tightness of Chernoff bounds, we may then show that the probability of fitting a subspace clustering to a good fraction of the lower mass points is always sufficiently large.

## 6 Experiments

Theoretical guarantees are often notoriously conservative compared to what is seen in practice. In this section, we present empirical findings detailing whether the risk bounds from the previous sections are also the risk bounds one can expect when dealing with real datasets. Indeed, for the related question of computing coresets, experimetal work by  seems to indicate that the worst case bounds by  are not what one has to expect in practise for center based clustering. Generally, two properties can determine the risk decrease. First, the clusters may be well separated . Indeed, making assumptions to this end, there is also some theoretical evidence that a rate of \(O(k/n)\) is possible . The other, somewhat related explanation is that if the ground truth consists of \(k^{}<k\) clusters , the dependency on \(k\) will point more towards the smaller, true number of clusters. We run the experiments both for center based clustering, as well as subspace clustering. While the focus of the paper is arguably more on subspace clustering, the experiments are important in both cases. Although both problems are hard to optimize exactly, center based clustering is significantly more tractable and thus may lend better insight into practical learning rates. For example, we have an abundance of approximation algorithms for \((k,z)\) clustering  whereas, even in the case of \((k,1,z)\) clustering in two dimensions  it is not possible to find any finite approximation in polynomial time.

In the main body, we focus on \((k,1,z)\) clustering, as there already exists a phase transition in terms of the computational complexity between the normal \(k\)-median and \(k\)-means problems and the \((k,1,1)\) and \((k,1,2)\) clustering objectives, while \(j=1\) still admits more positive results than other subspace clustering problems Agarwal et al. , Feldman et al. .

DatasetsWe use four publicly available real-world datasets: Mushroom , Skin-Nonskin , MNIST , and Covtype . Below, we show the results on the Covtype dataset, and the remainingexperiments are deferred to the supplementary material. Each dataset was normalized by the diameter, ensuring that all points lie in \(B_{2}^{d}\).

Problem parameters and algorithmsFor both center based clustering as well as subspace clustering, we focus on the powers \(z\{1,2,3,4\}\). \(z=2\) is arguably the most popular and also the most tractable variant. \(z=1\) is the objective with the least susceptibility to outliers. Finally, we consider the cases \(z=3\), due to it minimizing asymmetry and \(z=4\) as a tractable alternative to the coverage objective \(z\). The excess risk is evaluated for \(k\{10,20,30,50\}\) for both center based and subspace clustering. Expectation maximization (EM) type algorithms are used for both center-based and subspace clustering, though this is a severe computational challenge fo \((1,j,z)\) clustering, if \(z 2\), see . Given a solution \(\) we first assign every point to its closest center and subsequently recompute the center. For more details on initialization and concrete implementations, we refer to the supplementary material.

Experimental setup and resultsTo estimate the optimal cost \(OPT\) for the two objective functions, we run the corresponding appropriate algorithms mentioned above ten times on the entire dataset \(P\) and use the minimal objective value as an estimate for \(OPT\). We obtain a sample \(S_{i}\) of size \(n\) by sampling uniformly at random and estimate the optimal cost for that sample, \(OPT_{i}\). We repeat this 5 times. The empirical excess risk is calculated as \(_{n}=_{i=1}^{5}(P.OPT_{i})}{5}-OPT\). The excess risk for center-based clustering is evaluated on exponential-sized subset sizes \(n\{2^{6},2^{7},,2^{12}\}\).

   Dataset & Points & Dim & Labels \\  Mushrooms & 8,124 & 112 & 2 \\ MNIST & 60,000 & 784 & 10 \\ Skin\_Nonskin & 245,057 & 3 & 2 \\ Covtype & 581,012 & 54 & 7 \\   

Table 1: Datasets used for the experiments

Figure 1: Excess risk for line clustering on Covtyp. Shaded areas show max-min intervals.

We fit a line of the form \(c^{q_{1}}}{n^{q_{2}}}\) where \(c,q_{1},q_{2}\) are the optimizeable parameters. Let \(y_{i}\) be the excess risk in run \(i\). Let \(k_{i}\) and \(n_{i}\) be the values of \(k\) and \(n\) in run \(i\) and let \(r\) be the total number of times the excess risk was evaluated for each combination of algorithm and dataset. We use gradient descent on the following loss to optimize the parameters \(LSE=_{i=1}^{r}(y_{i}-c}}{n^{q_{2}}})^{2}.\)

The results in Figure 1 show that the excess risk for subspace clustering decreases quicker for higher values of \(z\), and we see a similar pattern for center-based clustering. The supplementary material contains more details on the empirical evaluations of center-based clustering. The best-fit lines shown in Tables 2 and 3 in the supplementary material indicate that the empirical excess risk values decrease slightly quicker than predicated by theory. The expected values are \(q_{1}=q_{2}=0.5\) and we observe \(q_{1},q_{2}\) around \(0.44,0.52\) respectively. For \(k\) this indicates a slightly favorable dependency in practice. For \(q_{2}\), we consider the difference to the theoretical bound of \(0.5\) negligible. The choice of \(z\) does not seem to have a significant impact on either finding. For subspace clustering, the dependency on \(k\) is a bit more pronounced and increases slightly towards the theoretical guarantees. Contrary to hopes that margin or stability conditions might occur on practical datasets, the results indicate that the theoretical guarantees of the learning rate are near-optimal even in practice. Moreover, the rates were not particularly affected by either the choice of \(z\) or by the dimension \(j\) when analyzing subspace clustering.

## 7 Conclusion and open problems

In this paper, we presented several new generalization bounds for clustering objectives such as \(k\)-median and subspace clustering. When the centers are points or constant dimensional subspaces, our upper bounds are optimal up to logarithmic terms. For projective clustering, we give a lower bound showing that the results obtained by  are nearly optimal. A key novel technique was using an ensemble of dimension reduction methods with very strong guarantees.

An immediate open question is to which degree ensembles of dimension reductions can improve learning rates over a single dimension reduction. Is it possible to find natural problems where there is a separation between the embeddability and the learnablity of a class of problems, or given the ensemble, is it always possible to find a single dimension reduction with the guarantees of the ensemble? Another open question is motivated by the recent treatment of clustering through the lens of computational social choice . Using current techniques from coresets  and learning theory , it seems difficult to improve over the learning rate of \(O(/n})\) for the fair clustering problem specifically. It it possible to match the bounds for unconstrained clustering?

## Disclosure of Funding Acknowledgements

Maria Sofia Bucarelli was partially supported by projects FAIR (PE0000013) and SERICS (PE00000014) under the MUR National Recovery and Resilience Plan funded by the European Union - NextGenerationEU. Supported also by the ERC Advanced Grant 788893 AMDROMA, EC H2020RIA project "SoBigData++" (871042), PNRR MUR project IR0000013-SoBigData.it.

Chris Schwiegelshohn was supported by the Independent Research Fund Denmark (DFF) under a Sapere Aude Research Leader grant No 1051-00106B.