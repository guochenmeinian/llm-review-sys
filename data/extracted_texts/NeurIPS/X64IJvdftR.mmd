# Certified Robustness for Deep Equilibrium Models

via Serialized Random Smoothing

Weizhi Gao\({}^{1}\)

Zhichao Hou\({}^{1}\)

Han Xu\({}^{2}\)

Xiaorui Liu\({}^{1*}\)

wgao23@ncsu.edu

xuhan2@arizona.edu

xliu96@ncsu.edu

###### Abstract

Implicit models such as Deep Equilibrium Models (DEQs) have emerged as promising alternative approaches for building deep neural networks. Their certified robustness has gained increasing research attention due to security concerns. Existing certified defenses for DEQs employing deterministic certification methods such as interval bound propagation and Lipschitz-bounds can not certify on large-scale datasets. Besides, they are also restricted to specific forms of DEQs. In this paper, we provide the first randomized smoothing certified defense for DEQs to solve these limitations. Our study reveals that simply applying randomized smoothing to certify DEQs provides certified robustness generalized to large-scale datasets but incurs extremely expensive computation costs. To reduce computational redundancy, we propose a novel Serialized Randomized Smoothing (SRS) approach that leverages historical information. Additionally, we derive a new certified radius estimation for SRS to theoretically ensure the correctness of our algorithm. Extensive experiments and ablation studies on image recognition demonstrate that our algorithm can significantly accelerate the certification of DEQs by up to 7x almost without sacrificing the certified accuracy. Our code is available at https://github.com/WeizhiGao/Serialized-Randomized-Smoothing.

## 1 Introduction

The recent development of implicit layers provides an alternative and promising perspective for neural network design (Amos and Kolter, 2017; Chen et al., 2018; Agrawal et al., 2019; Bai et al., 2019, 2020; El Ghaoui et al., 2021). Different from traditional deep neural networks (DNNs) that build standard explicit deep learning layers, implicit layers define the output as the solution to certain closed-form functions of the input. These implicit layers can represent infinitely deep neural networks using only one single layer that is defined implicitly. The unique definition endows implicit models with the capability to model continuous physical systems, a task that traditional DNNs cannot accomplish (Chen et al., 2018). Additionally, the implicit function theorem enhances memory efficiency by eliminating the need to store intermediate states during forward propagation (Chen et al., 2018; Bai et al., 2019). Furthermore, implicit models offer a valuable accuracy-efficiency trade-off, making them adaptable to varying application requirements (Chen et al., 2018). These advantages underscore the significant research value of implicit models.

Deep equilibrium models (DEQs) are one promising class of implicit models that construct the output as the solution to input-dependent fixed-point problems (Bai et al., 2019). With a fixed-point solver, DEQs can be seen as infinite-depth and weight-tied neural networks. The modern DEQ-based architectures have shown comparable or even surpassing performance compared with traditional explicit models (Bai et al., 2019, 2020; Gu et al., 2020; Chen et al., 2022). Due to the superiorperformance of DEQs, their adversarial robustness has gained increasing research interest. Recent research has revealed that DEQs also suffer from similar vulnerabilities as traditional DNNs, which raises security concerns (Gurumurthy et al., 2021; Yang et al., 2022). Multiple works propose empirical defenses to improve the adversarial robustness of DEQs using regularization methods (Chu et al., 2023; El Ghaoui et al., 2021) and adversarial training (Yang et al., 2023; Gurumurthy et al., 2021; Yang et al., 2022). These empirical defenses measure models' adversarial robustness by the robust performance against adversarial attacks. However, they do not provide rigorous security guarantees and often suffer from the risk of a false sense of security (Athalye et al., 2018), leading to tremendous challenges for reliable robustness evaluation.

As alternatives to empirical defenses, certified defenses aim to provide theoretical robustness guarantees. It is worth noting that certified defenses certify that no adversarial example can ever exist within a neighborhood of the test sample regardless of the attacks, providing reliable robustness measurements and avoiding the false sense of security caused by weak attacking algorithms. Recent works have explored interval bound propagation (IBP) (Wei and Kolter, 2021; Li et al., 2022) and Lipschitz bounding (LBEN) (Havens et al., 2023; Jafarpour et al., 2021) for certifiable DEQs. However, IBP usually estimates a loose certified radius due to the error accumulation in deep networks (Zhang et al., 2021) and the global Lipschitz constant tends to provide a conservative certified radius (Huang et al., 2021). Due to the conservative certification, IBP and LBEN generate trivial certified radii (namely, close to 0) in some cases such as deep networks, especially in large-scale datasets (e.g., ImageNet) (Zhang et al., 2021; Li et al., 2023). Moreover, the design of IBP and LBEN relies on specific forms of DEQs and can not be customized to various model architectures, restricting the application of these methods.

Given the inherent limitations of existing works, the objective of this paper is to explore the certified robustness of DEQs via randomized smoothing for the first time. Randomized smoothing approaches construct smoothed classifiers from arbitrary base classifiers and provide certified robustness via statistical arguments (Cohen et al., 2019) based on the Monte Carlo probability estimation. Therefore, compared with IBP and LBEN methods, randomized smoothing has better flexibility in certifying the robustness of various DEQs of different architectures. More importantly, the probabilistic certification radius provided by randomized smoothing can be larger and generalized to large-scale datasets.

Our study reveals that applying randomized smoothing to certify DEQs can indeed provide better certified accuracy but it incurs significant computation costs due to the expensive _fixed point solvers_ in DEQs and _Monte Carlo estimation_ in randomized smoothing. For instance, certifying the robustness of one \(256 256\) image in ImageNet dataset with a typical DEQ takes up to 88.33 seconds. This raises significant efficiency challenges for the application of certifiable DEQs in real-world applications. In this paper, we further delve into the computational efficiency of randomized smoothing certification of DEQs. Our analysis reveals the computation redundancy therein, and we propose an effective approach, named Serialized Random Smoothing (SRS). Importantly, the certified radius and theoretical guarantees of vanilla randomized smoothing can not be applied in SRS. Therefore, we develop a new certified radius with theoretical guarantees for the proposed SRS. Our method tremendously accelerates the certification of DEQs by leveraging their unique property and reducing the computation redundancy of randomized smoothing. The considerable acceleration of SRS-DEQ allows us to certify DEQs on large-scale datasets such as ImageNet, which is not possible in previous works. In a nutshell, our contributions are as follows:

* We provide the first exploration of randomized smoothing for certifiable DEQs. Our study reveals significant computation challenges in such certification, and we provide insightful computation redundancy analyses.
* We propose a novel Serialized Randomized Smoothing approach to significantly accelerate the randomized smoothing certification for DEQs and corresponding certified radius estimation with new theoretical guarantees.
* We conduct extensive experiments on CIFAR-10 and ImageNet to show the effectiveness of our SRS-DEQ. Our experiments indicate that SRS-DEQ can speed up the certification of DEQs up to \(\) almost without sacrificing the certified accuracy.

## 2 Background

In this section, we provide necessary technical background for DEQs and Randomized Smoothing.

### Deep Equilibrium Models

**Implicit formulation.** Traditional feedforward neural networks usually construct forward feature transformations using fixed-size computation graphs and explicit functions \(^{l+1}=f_{_{l}}(^{l})\), where \(^{l}\) and \(^{l+1}\) are the input and output of layer \(f_{_{l}}()\) with parameter \(_{l}\). DEQs, as an emerging class of implicit neural networks, define their output as the fixed point solutions of nonlinear equations:

\[^{*}=f_{}(^{*},),\] (1)

where \(^{*}\) is the output representation of implicit neural networks and \(\) is the input data. Therefore, the computation of DEQs for each input data point \(\) requires solving a fixed-point problem to obtain the representation \(^{*}\).

**Fixed-point solvers.** Multiple fixed-point solvers have been adopted for DEQs, including the naive solver, Anderson solver, and Broyden solver (Geng & Kolter, 2023). The naive solver directly repeats the fixed-point iteration until it converges:

\[^{l+1}=f(^{l},).\] (2)

More details about these solvers can be referred to in the work (Cohen et al., 2019). In this paper, we denote all solvers as follows:

\[=(f,,^{0}),\] (3)

where \(^{0}\) is the initial feature state that is taken as \(\) in DEQs. All the solvers end the iteration if the estimation error \(f()-\) of the fixed point reaches a given tolerance error or a maximum iteration threshold \(L\).

### Randomized Smoothing

Randomized smoothing (Cohen et al., 2019) is a certified defense technique that guarantees \(_{2}\)-norm certified robustness. Given an arbitrary base classifier \(f()\), we construct a smoothed classifier \(g()\):

\[g() =*{arg\,max}_{c}(f(+)=c),\] (4) \[(,^{2}),\] (5)

where \(\) is the label space, and \(^{2}\) is the variance of Gaussian distribution. Intuitively, the smoothed classifier outputs the most probable class over a Gaussian distribution. If we denote \(p_{A}\) and \(p_{B}\) as the probabilities of the most probable class \(c_{A}(x)\) and second probable class \(c_{B}(x)\), Neyman-Pearson theorem (Neyman & Pearson, 1933) provides a \(_{2}\)-norm certified radius \(R\) for the smoothed classifier \(g\):

\[g(+)=c_{A}(x)\|\|_{2}<R,\] (6) \[R=(^{-1}(})- ^{-1}(})).\] (7)

Here \(()\) is the inverse of the standard Gaussian cumulative distribution function, and \(},}\) satisfy:

\[(f(+)=c_{A}(x))} }_{c c_{A}(x)}(f(+)=c).\]

In practice, \(}\) and \(}\) are estimated using the Monte Carlo method. It is crucial to maintain the independence of each prediction in the simulation to ensure the correctness of randomized smoothing.

## 3 Serialized Randomized Smoothing

In this section, we begin by revealing the computation challenges associated with certifying DEQs using Randomized Smoothing. Subsequently, we propose Serialized Random Smoothing (SRS), a novel approach to remarkedly enhance the efficiency of DEQ certification. However, directly applying the estimated radius of the standard randomized smoothing to SRS breaks the theoretical guarantee of certified robustness. To address this challenge, we develop the correlation-eliminated certification technique to estimate the radius in SRS.

### Computation Challenges

According to Eq. (7), we need to estimate the lower bound probability \(}\) and upper bound probability \(}\). The appliance of Monte Carlo sampling rescues as follows:

\[(f(+)=c)_{i}^{N}\{f (+_{i})=c\},\] (8)

where \(\{\}\) is the indicator function, and \(_{i}(,^{2})\) is the \(i\)-th random perturbation sampled from Gaussian distribution. However, it introduces computation challenges due to the large sampling number \(N\). Our empirical study (Section 4.2) indicates that applying randomized smoothing to certify MDEQ (Bai et al., 2020) on one \(32 32\) image in CIFAR-10 takes 12.89 seconds, and 88.33 seconds for one \(256 256\) image in ImageNet. The computation challenges raise tremendous limitations in real-world applications.

We provide an analysis for the slow randomized smoothing certification of DEQs. First, each forward iteration of DEQs can be very expensive. This is because DEQs are typically weight-tied neural networks so one layer \(f(,)\) of DEQs needs to be complex to maintain the expressiveness. Second, the fixed-point solver needs many iterations for convergence. To maintain the best performance, the solvers usually set a small value for the error tolerance (e.g., \(0.001\)). Although second-order solvers like Broyden's method have faster convergence, their computation cost per iteration is higher. Third, the Monte Carlo estimation in randomized smoothing further exacerbates the expensive inference of DEQs, leading to significant computation challenges, as shown in Figure 0(a).

### Serialized Randomized Smoothing

As introduced in Section 3.1, the Monte Carlo method is employed in randomized smoothing to estimate the prediction probability, typically necessitating over \(10,000\) times inference computation for certifying one data point. Despite the independent sampling of Gaussian noises, these noises \(\{_{i}\}\) are added to the same certified data point \(\) to form noisy data samples \(\{+_{i}\}\). Notably, these samples are numerically and visually similar to each other as can be seen in Figure 1. Moreover, in randomized smoothing, the base classifier is trained on Gaussian-augmented data to be resistant to noises added to data points, yielding robust features and base classifiers. Therefore, the feature representation of these noisy samples computed in the forward computation of DEQs shares significant similarities, resulting in a substantial computation redundancy in the fixed-point solvers. The computation redundancy contributes to the inefficient DEQ certification with randomized smoothing as a primary factor. Consider a DEQ with 50 layers as an illustrative example. In the Monte Carlo estimation with \(N=10,000\), it requires the forward computation of \(50 10,000=500,000\) layers. However, if we can estimate the intermediate representation at the \(45^{th}\) layer, the required forward iterations reduce to \(5 10,000=50,000\) layers, bringing a \(10\) acceleration.

Figure 1: Illustrations of the standard DEQ and our SRS-DEQ. The representation for each sample goes through \(D\) layers in standard DEQ. Our SRS-DEQ uses the previous representation as the initialization and converges to the fixed point with a few layers (\(S D\)). After get all the predictions, SRS-DEQ makes use of correlation-eliminated certification to estimate the certified radius.

Motivated by the above challenges and analyses, we propose a novel solution, _Serialized Randomized Smoothing (SRS)_, to effectively reduce the computation redundancy in the certification of DEQs. The key idea of Serialized Randomized Smoothing is to accelerate the convergence of fixed-point solvers of DEQs by harnessing historical feature representation information \(\) computed from different noisy samples, thereby mitigating redundant calculations. While the hidden representation \(^{0}\) is initialized as \(\) in standard DEQs (Bai et al., 2021), we propose to choose a better initialization of \(^{0}\) to accelerate the convergence of DEQs (Bai et al., 2021), which can potentially reduce the number of fixed-point iteration \(S\) and computation cost. Specifically, our Serialized Randomized Smoothing approach leverages the representation \(^{S}_{i}\) computed from the previous noisy sample \(+_{i}\) as the initial state of the fixed-point solver of the next noisy sample:

\[^{S}_{i}=(f,+_{i},^{S}_{i- 1}),\] (9)

where \(i=1,2,,N\). As shown in Figure 0(b), due to the similarity between \(^{*}_{i-1}(^{S}_{i-1})\) and \(^{*}_{i}(^{S}_{i})\) as analyzed in the motivation, it only takes a few fixed-point iterations to adjust the feature representation from \(^{S}_{i-1}\) to \(^{*}_{i}(^{S}_{i})\), which significantly accelerates the prediction of DEQs.

Though a better initialization accelerates the inference of DEQs, it introduces an unnecessary correlation within the framework of randomized smoothing. In standard randomized smoothing, each prediction is made independently. However, the predictions are linked through previous fixed points as defined by \((f,+_{i},^{S}_{i-1})\). To exemplify this, consider an extreme case where the solver functions as an identity mapping. In such a case, all subsequent predictions merely replicate the first prediction. This pronounced correlation effectively reduces the process to an amplification of the first prediction, breaking the confidence estimation for Monte Carlo. Therefore, we develop a new estimation of the certified radius with theoretical guarantees in the next subsection.

### Correlation-Eliminated Certification

The primary challenge is to confirm how much the initialization of the fixed-point solver influences the final predictions. For different data samples \(+_{i}\) and initialization \(^{S}_{i}\), the cases can be different depending on the complex loss landscape of the fixed-point problem and the strength of the solver. Nonetheless, comparing all predictions from SRS with standard predictions, which necessitate numerous inference steps, is impractical. Such a comparison contradicts the fundamental requirement for efficiency in this process.

To maintain the theoretical guarantee of randomized smoothing, we propose correlation-elimination certification to obtain a conservative estimate of the certified radius. The core idea involves discarding those samples that are misclassified as the most probable class, \(c_{A}(x)\), during the Monte Carlo process. Let \(p_{m}\) represent the probability that a sample is predicted as class \(c_{A}(x)\) using SRS but falls into other classes with the standard DEQ. We can drop the misclassified samples as follows:

\[N^{E}_{A}=N_{A}-p_{m}N_{A},\] (10)

where \(N_{A}\) represents the count of samples predicted as class \(c_{A}(x)\) and \(N^{E}_{A}\) refers to the subset of these effective samples that are predicted as class \(c_{A}(x)\). Utilizing \(N^{E}_{A}\) and \(N\), we are ultimately able to estimate the certified radius. For the reason that \(p_{m}\) is intractable, we employ an additional hypothesis test using a limited number of samples to approximate its upper bound. During the Monte Carlo sampling of SRS, we randomly select \(K\) of samples (a small number compared to \(N\)) along with their corresponding predictions, which are then stored as \(_{m}\) and \(_{m}\), respectively. After the Monte Carlo sampling, these samples, \(_{m}\), are subjected to predictions using the standard DEQ to yield the labels \(_{g}\), which serve as the ground truth. Mathematically, we estimate \(}\) as follows:

\[N_{1}=_{i=1}^{K}\{Y_{m}=Y_{g}Y_{g}=c_{A}(x)\},\] (11)

\[N_{2}=_{i=1}^{K}\{Y_{m}=c_{A}(x)\},\] (12)

\[}=1-(N_{1},N_{2},1-),\] (13)

where \(=/2\) is for keeping the confidence level of the two-stage hypothesis test. Besides, \((k,n,1-)\) returns a one-sided \((1-)\) lower confidence interval for the Binomial parameter \(p\) given that \(k(n,p)\). In other words, it returns some number \(\) for which \( p\) with probability at least \(1-\) over the sampling of \(k(n,p)\). Intuitively, a smaller \(}\) indicates a higher consistency between the predictions of SRS and those of the standard DEQ, yielding a greater number of effective samples. To enhance comprehension, we include an example in Appendix I to demonstrate the workflow of correlation-eliminated certification. In the end, we estimate the certified radius with the following equation:

\[}=(N_{A}^{E},N,1-)\] (14)

\[R=^{-1}(})\] (15)

To implement SRS-DEQ efficiently, we stack the noisy samples into mini-batches for faster parallel computing as shown in Algorithm 1. Given a certified point, we sample batch-wise noisy data. After solving the fixed-point problem for the first batch, the subsequent fixed-point problem is initialized with the solution of the previous one. By counting the effective predictions using Eq. (10), the algorithm finally returns the certified radius as in standard randomized smoothing (Cohen et al., 2019). The following Theorem 3.1 theoretically guarantees the correctness of our algorithm (proof available in Appendix A):

**Theorem 3.1** (Correlation-Eliminated Certification).: _If Algorithm 1 returns a class \(_{A}(x)\) with a radius \(R\) calculated by equation 14 and 15, then the smoothed classifier \(g\) predicts \(_{A}(x)\) within radius \(R\) around \(\): \(g(+)=g()\) for all \(\|\|<R\), with probability at least \(1-\)._

## 4 Experiments

In this section, we conduct comprehensive experiments in the image classification tasks to demonstrate the effectiveness of the proposed SRS-MDEQ. First, we introduce the experimental settings in detail. Then we present certification on CIFAR-10 and ImageNet datasets to demonstrate the certified accuracy and efficiency. Finally, we provide comprehensive ablation studies to understand its effectiveness.

### Experiment Settings

**Datasets.** We use two classical datasets in image recognition, CIFAR-10 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015), to evaluate the certified robustness. It is crucial to emphasize that this is the first attempt to certify the robustness of DEQs on such a large-scale dataset.

**DEQ Architectures and Solvers.** We select MDEQ with Jacobian regularization (Bai et al., 2020), a type of DEQs specially designed for image recognition, to serve as the base classifier in randomized smoothing. Specifically, we choose MDEQ-SMALL and MDEQ-LARGE for CIFAR-10, and MDEQ-SMALL for ImageNet. To obtain a satisfactory level of certified accuracy, all the base classifiers are trained on the Gaussian augmented noise data with mean \(0\) and variance \(^{2}\). Detailed information regarding the model configuration and training strategy is available in Appendix B.

We closely follow the solver setting in MDEQ (Bai et al., 2020). For the standard MDEQ on CIFAR-10, we use the Anderson solver with the step of {1, 5, 30}. For the standard MDEQ on ImageNet, we use the Broyden solver with the step of {1, 5, 14}. We apply Anderson and Naive solvers on CIFAR-10 and Broyden solver on ImageNet for the proposed SRS-MDEQ with the step of {1, 3}. We adopt a warm-up technique, where we use multi-steps to solve the fixed-point problem for the first batch in Algorithm 1. The warm-up steps for our SRS-MDEQ are set as 30 and 14 steps for CIFAR-10 and ImageNet, respectively. The details of warm-up strategy are shown in Appendix K. For notation simplicity, we use a number after the algorithm name to represent the number of layers of the model, and we use "**N**", "**A**", and "**B**" to denote the Naive, Anderson, and Broyden solvers. For instance, SRS-MDEQ-3A denotes SRS-MDEQ method with 3 steps of Anderson iterations.

**Randomized smoothing.** Following the setting in randomized smoothing (Cohen et al., 2019), we use four noise levels to construct smoothed classifiers: {0.12, 0.25, 0.50, 1.00}. We report the _approximate certified accuracy_ as in (Cohen et al., 2019), which is defined as the fraction of the test data that is both correctly classified and certified with a \(_{2}\)-norm certified radius exceeding a radius threshold \(r\). In our experiments, we set the failure rate as \(=0.001\) and the sampling number as \(N=10,000\) in the Monte Carlo method, unless specified otherwise. All the experiments are conducted on one A100 GPU.

**Baselines.** We majorly use standard Randomized Smoothing for MDEQs as our baseline for comparison. It is also important to compare our method with state-of-the-art certified defenses. Note that the 

[MISSING_PAGE_EMPTY:7]

**Running Time.** The running time summarized in Table 1, Table 2, and Table 3 shows significant efficiency improvements of our SRS-DEQ method compared with standard randomized smoothing. In general, the time cost almost linearly increases with the number of layers. The standard MDEQ requires 30 layers to certify each image to a satisfactory extent, which costs 12.89 seconds per image for the large model and 3.08 seconds per image for the small model on CIFAR-10. This process leads to a heavy computational burden in the certification process. Fortunately, this process can be significantly accelerated by our SRS-MDEQ. To be specific, for CIFAR-10, large SRS-MDEQ-1N is near \(11\) faster than large MDEQ-30A with a 2.5% certified accuracy drop. Besides, small SRS-MDEQ-3A outperforms small MDEQ-30A in efficiency by \(7\) with only a 1% accuracy drop. On Imagenet, our SRS-MDEQ-1B can speed up the certification by \(6\) while enhancing certified robustness compared to MDEQ-14B.

### Ablation Study

In this section, we conduct comprehensive ablation studies to investigate the instance-level consistency and the effectiveness of our correlation-eliminated certification. We also provide more ablation studies on the hyperparameter of MDEQ solvers in Appdenix F and G. Finally, we show the empirical robustness performance of our method in Appendix L.

**Instance-level consistency.** Besides providing global measurements for the SRS-MDEQ with the certified accuracy in Section 4.2, we study how closely SRS-MDEQ matches accurate MDEQ at the instance level based on our proposed _Relative Radius Difference_ (RRD). RRD compares the relative difference between the certified radius of SRS-MDEQ and the accurate MDEQ for each instance \(_{i}\):

\[(_{i})=^{i}-r_{s}^{i}|}{r_{b}^{i}},\] (16)

where \(r_{b}^{i}\) and \(r_{s}^{i}\) represent the certified radius of \(_{i}\) with MDEQ-30A and SRS-MDEQ, respectively. We compute RRD over the instances with a positive certified radius to avoid the singular value.

We present the histograms of RRD in Figure 2. As shown in Section 4.3, only with one layer, the certified radius achieved by SRS-MDEQ-1A is quite close to the accurate MDEQ since these relative differences are mostly small and close to 0, and it significantly outperforms the standard MDEQ-1A with one layer. Moreover, with 3 layers as shown in Section 4.3, the RRD values become even more concentrated around 0, which shows a very consistent certified radius with the accurate MDEQ. The instance level measurement for other settings of MDEQs are shown in Appendix H.

**Power of correlation-eliminated certification.** The correctness of our method is based on estimating the upperbound of \(p_{m}\). In this ablation study, we investigate the effectiveness in the following two aspects. We provide additional analysis for this technique in Appendix I.

Figure 3: Gap histogram of MDEQ-LARGE and \(}\) histogram of MDEQ-LARGE with 10 bins.

Figure 2: RRD histogram with MDEQ-LARGE with 20 bins.

(1) The magnitude of the upperbound. A large \(}\) indicates that we need to drop many predictions in \(_{A}\), showcasing strong correlation in SRS-DEQ. We plot the histogram of \(}\) to show the magnitude. Figure 2(a) and 2(b) illustrates that the majority of \(}\) values fall within lower intervals, even with just a single step. This trend is more pronounced with three layers, as depicted in Figure 2(b). These observations suggest that an increase in the number of steps reduces the correlation in predictions, resulting in a certified radius calculated by our method closer to the one obtained through standard randomized smoothing for DEQs. The inner reason is that our approach does not necessitate the exclusion of a large number of samples for most certified points.

(2) The empirical correctness of the upperbound, i.e., if the estimated value is larger than the number of samples we should drop. For each certified point, we calculate the gap between those two values:

\[}-}_{n=1}^{N}\{y_{b}^{n} y_{s}^ {n}y_{s}^{n}=c_{A}\},\] (17)

where \(N_{A}\) is the number of samples classified as \(c_{A}\) with SRS. As shown in Figure 2(c) and 2(d), the histogram of the gap, the value is always larger than 0, meaning that the estimation effectively covers the samples that we should drop. Moreover, the gap distribution is notably skewed towards 0. For example, more than 95% of the gaps are less than 0.2, signifying that our estimation is not only effective but also tight. More results can be found in Appendix I.

**Compared to explicit neural networks.** To demonstrate the superior performance of certification with DEQs, we also compare our results against those of explicit neural networks. Despite surpassing the performance of explicit neural networks is not our target, we claim the performance of DEQs can catch up with them, as shown in Table 4. We provide a comparison between DEQs and ResNet-110 under the same training and evaluation setting, and the results are consistent with those reported in (Cohen et al., 2019).

**Results on Other Randomized Smoothing methods.** In addition to the standard version of randomized smoothing (Cohen et al., 2019), there are more advanced methods available. To demonstrate the general adaptability of our approach to randomized smoothing, we conduct experiments using SmoothAdv (Salman et al., 2019). For these experiments, we utilize PGD (Kurakin et al., 2016) as the adversarial attack method, setting the number of adversarial examples during training to 4. The results, presented in Table 5, show that SmoothAdv improves certified accuracy for both standard randomized smoothing and our SRS approach.

   Model\(\)Radius & \(0.0\) & \(0.25\) & \(0.5\) & \(0.75\) & \(1.0\) & \(1.25\) & \(1.5\) \\  ResNet-110 & 65\% & 54\% & 41\% & 32\% & 23\% & 15\% & 9\% \\  MDEQ-30A & **67\%** & **55\%** & 45\% & 33\% & 23\% & 16\% & 12\% \\ SRS-MDEQ-3A & 66\% & 54\% & **45\%** & **33\%** & **23\%** & **16\%** & 11\% \\   

Table 4: Comparison of certified accuracy for ResNet-110 and the MDEQ-LARGE architecture with \(=0.5\) on CIFAR-10. The best certified accuracy for each radius is in bold.

   Model \(\) Radius & \(0.0\) & \(0.25\) & \(0.5\) & \(0.75\) & \(1.0\) & \(1.25\) & \(1.5\) \\  MDEQ-1A (adv) & 23\% & 17\% & 12\% & 9\% & 6\% & 4\% & 2\% \\ MDEQ-5A (adv) & 52\% & 44\% & 32\% & 21\% & 17\% & 14\% & 10\% \\ MDEQ-30A (adv) & **62\%** & **54\%** & 43\% & **37\%** & **30\%** & **23\%** & 14\% \\ MDEQ-30A (standard) & 62\% & 50\% & 38\% & 30\% & 22\% & 13\% & 9\% \\  SRS-MDEQ-1A (adv) & 60\% & 43\% & 35\% & 27\% & 18\% & 14\% & 9\% \\ SRS-MDEQ-3A (adv) & 60\% & 52\% & **43\%** & 36\% & 29\% & 22\% & **14\%** \\   

Table 5: Certified accuracy for the MDEQ-SMALL architecture with \(=0.5\) on CIFAR-10 using SmoothAdv. The maximum norm \(\) of PGD is set as 0.5 and the number of steps \(T\) is set as 2.

## 5 Related Work

### Deep Equilibrium Models

Recently, there have been many works on deep implicit models that define the output by implicit functions (Amos and Kolter, 2017; Chen et al., 2018; Bai et al., 2019; Agrawal et al., 2019; El Ghaoui et al., 2021; Bai et al., 2020; Winston and Kolter, 2020). Among these, deep equilibrium model defines the implicit layer by solving a fixed-point problem (Bai et al., 2019, 2020). There are many fundamental works investigating the existence and the convergence of the fixed point (Winston and Kolter, 2020; Revay et al., 2020; Bai et al., 2021; Ling et al., 2023). With many advantages, DEQs achieve superior performance in many tasks, such as image recognition (Bai et al., 2020), image generation (Pokle et al., 2022), graph modeling (Gu et al., 2020; Chen et al., 2022), language modeling (Bai et al., 2019), and solving complex equations (Marwah et al., 2023). Though DEQs catch up with the performance of DNNs, the computation inefficiency borders the deployment of deep implicit models in practice (Chen et al., 2018; Dupont et al., 2019; Bai et al., 2019). Related works focus on reusing information from diffusion models and optical flows, demonstrating the effectiveness of reducing computational redundancy of DEQs (Bai and Melas-Kyriazi, 2024; Bai et al., 2022). However, this paper focuses on the certified robustness of DEQs and provides a theoretical analysis of our proposed method.

### Certified Robustness

Empirical defenses like adversarial training are well-known in deep learning (Goodfellow et al., 2014). Some existing works improve the robustness of DEQs by applying adversarial training (Gurumurthy et al., 2021; Yang et al., 2023, 2022). Different from the empirical defense like adversarial training, certified defenses theoretically guarantee the predictions in a small ball maintain as a constant (Wong and Kolter, 2018; Raghunathan et al., 2018; Gowal et al., 2018; Cohen et al., 2019). The most common way to certify robustness is to define a convex program, which lower bounds the worst-case perturbed output of the network (Raghunathan et al., 2018; Wong and Kolter, 2018). The increasing computation complexity in high-dimension optimization hinders the generalization of these methods. Interval bound propagation (IBP) is another certification method for neural networks, which computes an upper bound of the class margin through forward propagation (Gowal et al., 2018). However, the layer-by-layer computation mode brings a potentially loose certified radius. Recently, randomized smoothing has drawn much attention due to its flexibility (Cohen et al., 2019). Randomized smoothing certifies \(_{2}\)-norm robustness for arbitrary classifiers by constructing a smoothed version of the classifier. There are some existing works certifying robustness for DEQs. Most of them adapt IBP to DEQs by constructing a joint fixed-point problem (Wei and Kolter, 2021; Li et al., 2022). Others design specific forms of DEQs to control the Lipschitz constant of the models (Havens et al., 2023; Jafarpour et al., 2021). Yet, no existing work explores randomized smoothing for certifiable DEQs.

## 6 Conclusion

In this work, we provide the first exploration of randomized smoothing certification for DEQs. Our study shows that randomized smoothing for DEQs can certify more generalized architectures and be applied to large-scale datasets but it incurs significant computation costs. We delve into the computation bottleneck of this certified defense and point out the new insight of computation redundancy. We further propose a novel Serialized Random Smoothing approach to significantly reduce the computation cost by leveraging the computation redundancy. Finally, we propose a new estimation for the certified radius for our SRS. Our extensive experiments demonstrate that our algorithm significantly accelerates the randomized smoothing certification by up to \(7\) almost without sacrificing the certified accuracy. Our discoveries and algorithm provide valuable insight and a solid step toward efficient robustness certification of DEQs. Our work significantly improves the security of artificial intelligence, especially applicable in sensitive domains, enhancing the appliance of the models and maintaining the integrity of AI-driven decisions. Though our paper speeds up the certification of DEQs with randomized smoothing, it cannot be directly applied to other architecture. We regard the speedup for the general method as our future research.

## 7 Acknowledgements

Weizhi Gao, Zhichao Hou, and Dr. Xiaorui Liu are supported by the National Science Foundation (NSF) National AI Research Resource Pilot Award, Amazon Research Award, NCSU Data Science Academy Seed Grant Award, and NCSU Faculty Research and Professional Development Award.