# RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models

Maya Varma

Stanford University

mayavarma@cs.stanford.edu

&Jean-Benoit Delbrouck

Stanford University; Hugging Face

jbdel@stanford.edu

Zhihong Chen

Stanford University

zhihongc@stanford.edu

&Akshay Chaudhari

Stanford University

akshaysc@stanford.edu

&Curtis Langlotz

Stanford University

langlotz@stanford.edu

Equal senior authorship.Code: https://github.com/Stanford-AIMI/RaVL

###### Abstract

Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) primarily operate at the global image-level rather than intervening directly on fine-grained image features and (ii) are predominantly designed for unimodal settings. In this work, we present RaVL, which takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level. Given a fine-tuned VLM, RaVL first **discovers** spurious correlations by leveraging a region-level clustering approach to identify precise image features contributing to zero-shot classification errors. Then, RaVL **mitigates** the identified spurious correlation with a novel region-aware loss function that enables the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with various model architectures, data domains, and learned spurious correlations. Our results show that RaVL accurately discovers (191% improvement over the closest baseline) and mitigates (8.2% improvement on worst-group image classification accuracy) spurious correlations. Qualitative evaluations on general-domain and medical-domain VLMs confirm our findings.1

## 1 Introduction

Contrastive vision-language models (VLMs) (e.g., CLIP  and ALIGN ) are a powerful class of models that jointly learn relationships between images and text. VLMs are generally pretrained on web-scale datasets with millions of image-text pairs and have been shown to exhibit impressive capabilities on a wide range of downstream tasks. In particular, VLMs have the ability to perform tasks in a zero-shot manner without utilizing explicit task-specific training data; this is accomplished by modeling downstream tasks (e.g., image classification, text-to-image retrieval) as image-text matching tasks .

However, pretrained VLMs can exhibit poor zero-shot performance when compared to state-of-the-art task-specific models, particularly on challenging or out-of-domain downstream tasks . As a result, pretrained VLMs are often fine-tuned on domain-specific vision-language datasets in orderto improve zero-shot performance on tasks of interest. For instance, recent works have fine-tuned the CLIP VLM  on vision-language datasets consisting of (i) chest X-rays and paired physician reports , (ii) pathology data and paired text [17; 19], and (iii) product images and paired captions from online fashion retailers .

Domain-specific vision-language datasets used to fine-tune VLMs may be small in size, preventing VLMs from gaining the robustness benefits that come with training on diverse, web-scale data [6; 14]. As a result, fine-tuned VLMs may capture spurious correlations between image features and textual attributes . For instance, consider a VLM fine-tuned on an animal image-text dataset where the presence of butterflies is closely correlated with the presence of flowers (Figure 1). Consequently, the VLM may learn to incorrectly associate the image features corresponding to _flower_ with the textual attribute _butterfly_. At test time, the VLM is likely to exhibit degraded zero-shot classification performance on (i) images of butterflies without flowers and (ii) images of other animals with flowers.

Improving robustness of fine-tuned VLMs to spurious correlations is challenging for the following two reasons. First, existing automated approaches primarily discover and mitigate spurious correlations at the global image level rather than intervening directly on fine-grained image features. Such approaches discover spurious correlations by identifying coherent groups of misclassified images in an automated fashion [13; 43; 22; 42]; then, the identified spurious correlation can be mitigated during training using data augmentation or robust optimization [43; 39; 22; 56]. However, recent works have suggested that such global image-level strategies (i) discover spurious correlations that align poorly with human-interpretable attributes  and (ii) may not effectively enable models to ignore spurious correlations during training [15; 18]. Second, existing approaches for discovering and mitigating spurious correlations are predominantly designed to improve robustness of unimodal image classification models [39; 43] or pretrained VLMs [60; 49]. These settings differ substantially from the fine-tuned VLM setting, which presents several unique challenges such as the absence of class and subgroup labels in the training set and the inclusion of free-form text.

In this work, we address these challenges by introducing **R**egion-**a**ware **V**ision-**L**anguage learning (R**aVL), an approach for improving the robustness of fine-tuned VLMs to spurious correlations. R**aVL takes a _fine-grained_ perspective on VLM robustness by discovering and mitigating spurious correlations using local image features, rather than operating at the global image level. Our contributions are:

* First, given a fine-tuned VLM, R**aVL **discovers** learned spurious correlations between image features and textual attributes. Using a labeled classification dataset, we decompose images into candidate regions, utilize the VLM embedding space to group visually-similar regions into feature clusters, and quantitatively evaluate the effects of each feature on zero-shot classification errors.
* Second, given a ranked list of image features that the VLM has learned to spuriously correlate with one or more textual attributes, R**aVL **mitigates** the identified spurious correlations. Our key insight is that region-level information can be leveraged during VLM fine-tuning in order to improve model robustness. To this end, we introduce a novel region-aware loss function

Figure 1: _Region-aware Vision-Language learning (R**aVL). R**aVL takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features.

that encourages the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning.

In order to evaluate RaVL, we introduce a large-scale evaluation framework for controlled, fine-grained evaluations of VLM robustness on synthetic and real-world data. Our framework consists of 654 fine-tuned VLMs paired with annotations for the ground-truth spurious correlations learned by each VLM. Across these evaluation settings, (i) RaVL accurately discovers spurious correlations, achieving a 191% improvement over the closest baseline, and (ii) RaVL effectively mitigates spurious correlations, achieving up to an 8.2% improvement on worst-group image classification accuracy. Qualitative evaluations on general-domain and medical-domain VLMs confirm the utility of RaVL.

This paper is organized as follows. In Section 2, we introduce our problem setting. Then, in Section 3, we present Stage 1 of RaVL, including our proposed methodology for discovering spurious correlations, our large-scale evaluation framework, and experimental results. In Section 4, we introduce Stage 2 of RaVL, including our proposed methodology for mitigating spurious correlations as well as experimental results. Finally, we conclude in Section 5.

**Related Work**: Our work builds on several recent research directions for discovering and mitigating spurious correlations. We provide an analysis of related works in Appendix Section A.

## 2 Preliminaries

In this section, we formally describe our problem setting. Datasets used for fine-tuning VLMs can be expressed as \(_{F}=\{(I_{i},T_{i})\}_{i=1}^{m}\), where \(I_{i}\) represents image inputs and \(T_{i}\) represents paired free-form text. We do not assume access to any class or subgroup labels.

The performance of fine-tuned VLMs can be characterized with zero-shot classification tasks. In line with prior work [13; 22; 56], we assume that the zero-shot classification dataset includes a validation split \(_{V}=\{(I_{i},y_{i})\}_{i=1}^{n}\) with images \(I_{i}\) and known ground-truth class labels \(y_{i}\), where \(\) denotes the set of all possible class labels. At evaluation time, classification performance is computed by encoding class labels in \(\) as text and matching images to the closest class label using embedding similarity. We do not assume access to any subgroup labels.

Fine-tuned VLMs may learn spurious correlations between image features and textual attributes. Let \(_{a}\) represent the image features corresponding to a visual concept \(a\) (e.g., flowers in Figure 1) and \(y\) represent a class label (e.g., "butterfly" in Figure 1) such that \(_{a}\) and \(y\) share no causal relationship. Then, a fine-tuned VLM that has learned a spurious correlation will be unable to disentangle \(_{a}\) and \(y\) at evaluation time. This will manifest in low zero-shot classification performance on the following two subgroups of data: (i) images from class label \(y\) without the feature \(_{a}\) and (ii) images from other class labels \(\{y\}\) with the feature \(_{a}\).

However, since neither the fine-tuning dataset \(_{F}\) nor the evaluation dataset \(_{V}\) include subgroup labels corresponding to visual concepts \(a\), discovering and mitigating such spurious correlations poses a challenge. For instance, in Figure 1, there are no annotations for flowers in datasets \(_{F}\) and \(_{}\), making it challenging to identify and address the learned spurious correlation between image features corresponding to flowers and the textual attribute corresponding to "butterfly".

In the following sections, we will discuss our automated approach RaVL, which aims to address this challenge by employing fine-grained region-level information to discover (Section 3) and mitigate (Section 4) spurious correlations in fine-tuned vision-language models.

## 3 Discovering Spurious Correlations in Fine-Tuned Vision-Language Models

In this section, we present the first stage of RaVL, which aims to discover learned spurious correlations in VLMs. In Section 3.1, we discuss our region-aware approach for discovering fine-grained spurious correlations. Then, in order to quantitatively evaluate the efficacy of spurious feature discovery methods, we introduce a large-scale evaluation framework in Section 3.2. Finally, in Section 3.3, we use our evaluation framework to demonstrate that RaVL outperforms prior approaches in discovering fine-grained spurious correlations between image features and textual attributes.

### Our Approach: Discovering Spurious Correlations

The first stage of RaVL aims to identify spurious correlations between image features and textual attributes learned by a fine-tuned VLM \(\). In contrast to prior works that have incorporated humans in the loop in order to identify spurious correlations [56; 30], RaVL is a fully automated approach. Additionally, whereas previous automated methods for discovering spurious correlations focus predominantly on identifying groups of _images_ with high error rates [22; 13], our approach identifies specific _image features_ that model \(\) has learned to spuriously correlate with a textual attribute. Our goal is to discover precise spurious correlations that can be easily interpreted by humans.

As discussed in Section 2, a model \(\) that has learned a spurious correlation between an image feature \(_{a}\) and a textual attribute \(y\) will demonstrate low zero-shot performance on (i) images in \(_{V}\) with label \(y\) without the feature \(_{a}\) and (ii) images in \(_{V}\) with other labels \(\{y\}\) with the feature \(_{a}\). The key challenge lies in identifying such relationships when no annotations are provided for visual concepts \(a\). RaVL addresses this challenge by (1) obtaining candidate image features in \(_{V}\), (2) identifying the candidate image features that, when present in an image, directly contribute to classification errors, and (3) ranking the identified image features by degree of learned spurious correlations.

**Obtaining candidate image features.** RaVL first utilizes the zero-shot classification dataset \(_{V}\) to identify candidate image features. To this end, we use the fine-tuned VLM \(\) to extract an image embedding for each image \(I_{i}\) in \(_{V}\) and a text embedding for each class \(y\). Zero-shot classification is performed using the computed embeddings; this results in a softmax-normalized image score distribution vector \(_{I_{i}}^{||}\), where \(||\) represents the number of classes. Then, we decompose each image \(I_{i}\) in \(_{V}\) into a set of candidate _regions_\(_{i}\). There are a variety of ways in which an image can be decomposed into regions, such as dividing images into equal-sized segments (e.g., quadrants) or using region proposal networks (RPNs) . Ideally, regions should capture key features in the image; however, **we emphasize that RaVL does not require ground-truth region-level annotations**. We then apply RoIAlign [16; 63] to the image encoder of \(\) to extract embeddings for each region. Zero-shot classification is performed using the computed region embeddings, resulting in a softmax-normalized region score distribution matrix \(_{R_{i}}^{|_{i}|||}\).

Given region-level embeddings for all candidate regions in \(_{V}\), we next aim to identify coherent groups of image features that occur consistently throughout the dataset (e.g., features corresponding to "flower" or "butterfly" in Figure 1). To this end, we cluster the computed region-level embeddings using the K-Medoids algorithm with cosine distance. The optimal number of clusters is selected in an automated fashion using Silhouette distance. The resulting clusters (denoted as \(\)) capture key image features in \(_{V}\). For feature cluster \(c\), let \(_{c}\) denotes the set of features in cluster \(c\).

**Identifying candidate image features that directly contribute to classification errors.** We now seek to identify features that, when present in an image, are directly responsible for prediction errors.

Let \(_{c}\) represent the set of regions assigned to cluster \(c\) and let \(_{c}\) represent the set of images associated with the regions in cluster \(c\). We identify labels for images in \(_{c}\); we designate this label set as \(_{c}\). For each class label \(y_{c}\), we identify all images in \(_{c}\) with label \(y\), and we designate zero-shot classification accuracy on this subset of \(n_{in}^{y}\) images as \(p_{in}^{y}\). Then, we identify all images in \(_{v}\) with label \(y\) that do not have a region included in cluster \(c\), and we designate zero-shot classification accuracy on this subset of \(n_{out}^{y}\) images as \(p_{out}^{y}\).

We now introduce the _cluster influence score_, which evaluates the extent to which features \(_{c}\) contribute to mispredicted image classification labels. We restrict our evaluation to only include mispredicted images in \(_{c}\) with ground-truth labels \(y\) such that \(p_{in}^{y}<p_{out}^{y}\); we will refer to this subset as \(_{c}^{err}_{c}\). For each image \(I_{i}_{c}^{err}\), we extract (i) the image score distribution vector \(_{I_{i}}\) and (ii) the region score distribution matrix \(_{R_{i}}\). We use \(_{I_{i}}\) to identify the predicted image class \(\), and we then identify the region \(r_{i}^{max}\) in \(_{i}\) with the highest score for class \(\).

**Definition 1** (Cluster Influence Score).: For cluster \(c\) and label \(y\), the cluster influence score is the proportion of images \(I_{i}_{c}^{err}\) with label \(y\) where the identified highest-scoring region \(r_{i}^{max}\) is part of cluster \(c\) (i.e., \(r_{i}^{max}_{c}\)):

\[H_{c}^{y}=_{c}^{err}|y_{i}=y\}|}_{I_{i} _{c}^{err};y_{i}=y}[r_{i}^{max}_{c}]\] (1)The final cluster influence score for cluster \(c\) is computed as the maximum over all labels \(y\) as \(H_{c}=max_{y_{c}}H_{c}^{y}\). High values of \(H_{c}\) show that features \(_{c}\) are similar to the incorrect label in the vision-language embedding space; this suggests that for a given image with an incorrect prediction, feature \(_{c}\) is more likely to contribute to the misprediction than other features in the image. On the other hand, low values of \(H_{c}\) are likely to indicate that feature \(_{c}\) represents a core feature associated with the class label or a neutral feature that does not affect predictions.

Given \(H_{c}\) for each feature cluster, we prune all clusters with influence scores below a threshold of \(_{l}\), which we set to 0.25 in all experiments.

**Ranking image features by degree of learned spurious correlation.** For each remaining feature cluster, we next aim to determine the extent to which the presence or absence of features \(_{c}\) affects classification performance; we introduce the _cluster performance gap_ metric to this end.

_Definition 2 (Cluster Performance Gap)._ For cluster \(c\) and label \(y\), the cluster performance gap is the weighted difference between zero-shot classification accuracy on images with features \(_{c}\) and images without features \(_{c}\):

\[G_{c}^{y}=w_{y}(p_{in}^{y}-p_{out}^{y}),\] (2)

where \(w_{y}\) is a simple weighting factor computed as \(w_{y}=2[(n_{in}^{y},n_{out}^{y})/(n_{in}^{y}+n_{out}^{y})]\).

Since spurious correlations result in consistent errors as opposed to isolated misclassifications, the weighting factor is designed to prioritize stronger spurious correlations that result in a larger number of errors. \(G_{c}^{y}\) ranges between 0 and 1. The final performance gap metric for cluster \(c\) is computed across all labels as \(G_{c}=_{y_{c}}|G_{c}^{y}|\). A high value of \(G_{c}\) suggests that the presence or absence of features \(_{c}\) contribute to large class-level variations in image classification performance.

Given \(G_{c}\) for each feature cluster, we rank clusters in order from highest to lowest values. The output of this stage is a ranked list of image features that model \(\) has learned to spuriously correlate with one or more class labels in \(\).

### Experimental Setup: Designing a Large-Scale Evaluation Framework

We now discuss our approach for evaluating RAVL. Evaluating the accuracy of predicted spurious correlations is challenging because the ground-truth spurious correlations learned by a model \(\) are typically unknown. Previous works on VLM robustness evaluate discovered spurious correlations with qualitative experiments, human-in-the-loop evaluations, or small-scale datasets . Our aim in this section is to introduce a large-scale experimental setup where the ground-truth spurious correlations learned by VLMs are known and annotated in advance; this can then enable us to determine whether the features discovered by RAVL in Section 3.1 accurately align with the ground-truth. Our evaluation framework is motivated by prior work [26; 13]; however, in contrast to existing approaches, we introduce evaluation settings that are designed (i) for evaluating robustness approaches at the fine-grained region level rather than the global image-level, and (ii) for evaluating VLMs rather than unimodal models.

**Designing Controlled Evaluations:** Our evaluation framework artificially induces spurious correlations in the VLM fine-tuning data; then, given the known pre-defined spurious correlation and a VLM that learned the desired spurious correlation, we can quantitatively evaluate the extent to which RaVL discovers the correlation.

We create a set of _evaluation settings_ using data from two domains: (1) synthetic data (MNIST  and FashionMNIST ) and (2) real-world data (COCO ). Each evaluation setting consists of the following components:

1. _Predefined spurious correlation_: We define a spurious image feature and textual attribute pair (\(^{eval}\), \(a^{eval}\)). For MNIST and FashionMNIST, \(^{eval}\) represents a red rectangle; \(a^{eval}\) is generated from the set of class labels {zero, one, two, three, four five, six, seven, eight, nine} for MNIST and {t-shirt, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, ankle boot} for FashionMNIST. For COCO, we sample \(^{eval}\) and \(a^{eval}\) from the list of annotated attributes.
2. _Fine-tuning dataset_: We construct a vision-language fine-tuning dataset \(_{F}^{eval}=\{(I_{i},T_{i})\}_{i=1}^{m}\) with images \(I_{i}\) and text \(T_{i}\). Dataset \(_{F}^{eval}\) is sampled from the training sets of MNIST, FashionMNIST, or COCO such that the presence of image feature \(^{eval}\) is closely correlated with the presence of text attribute \(a^{eval}\) as measured by Cramer's V .

3. _Fine-tuned VLM_: A VLM \(\) is fine-tuned on \(_{F}^{eval}\).
4. _Evaluation dataset_: Model \(\) is evaluated using a zero-shot classification dataset \(_{V}^{eval}=\{(I_{i},y_{i},_{i},_{i})\}_{i=1}^ {n}\) with images \(I_{i}\), class labels \(y_{i}\), region bounding boxes \(_{i}\), and region-level labels \(_{i}\). In particular, \(a^{eval}\) must be included in the class label set, and \(^{eval}\) must be annotated in the region-level label set. Since \(_{V}^{val}\) is designed to reflect a real-world setting, we assume that a correlation between \(a^{eval}\) and \(^{eval}\) does not exist. Dataset \(_{V}^{eval}\) is constructed from the test sets of MNIST, FashionMNIST, or COCO.

Given the four components listed above, we classify an evaluation setting as valid if model \(\) learned the intended spurious correlation. In order to measure this, we first identify images with label \(a^{eval}\) in \(_{V}^{eval}\) and compute the performance difference between images with feature \(^{eval}\) and images without feature \(^{eval}\); we designate this value as \(_{1}\). Then, for labels \(y a^{eval}\), we compute the maximum performance difference between images without feature \(^{eval}\) and images with feature \(^{eval}\); we designate this value as \(_{2}\). Large values of \(_{1}\) and \(_{2}\) suggest that model \(\) has learned the desired spurious correlation between image feature \(^{eval}\) and textual attribute \(a^{eval}\), as defined in Section 2. We remove settings where \(_{1}\) or \(_{2}\) are below some predefined performance threshold \(_{eval}\). The performance threshold \(_{eval}\) serves as a quantitative indicator of learned correlation strength.

**Implementation Details:** In total, we generate 620 fine-tuning datasets \(D_{F}^{eval}\) (100 synthetic; 520 real-world). We then fine-tune model \(\) on each dataset with three random seeds, resulting in 1860 candidate evaluation settings. Finally, we filter out settings where model \(\) does not consistently learn the spurious correlation; to this end, we only retain settings where both \(_{1}\) and \(_{2}\) exceed \(_{eval}=10\) across all three random seeds. We repeat this procedure across various pretrained VLMs \(\), resulting in 654 valid experimental settings. Additional implementation details are provided in Appendix B.

    & \))**} \\   & 10 & 20 & 30 & 40 \\  Num. Eval Settings & 654 & 369 & 234 & 168 \\  Random & 21.2 & 18.2 & 15.5 & 12.5 \\ Distilling Failures & 20.1 & 16.2 & 8.5 & 1.5 \\ George & 19.3 & 15.9 & 10.9 & 7.7 \\ Domino & 17.1 & 15.0 & 11.7 & 9.0 \\ Spurious-Aware Detection & 20.0 & 25.3 & 32.1 & 42.0 \\ RaVL (Ours) & **61.8** & **76.2** & **84.2** & **91.1** \\  

Table 1: _Mean Precision@10 metrics demonstrate the efficacy of RaVL in discovering spurious correlations._ On average across 654 evaluation settings, RaVL consistently outperforms baselines.

    & \))**} \\   & 10 & 20 & 30 & 40 \\  Unweighted \(G_{c}\) Only & 21.2 & 30.0 & 36.2 & 55.0 \\ \(G_{c}\) Only & 40.9 & 51.7 & 63.8 & 66.7 \\ \(G_{c}\) \& \(H_{c}\) (RAVL) & **46.0** & **54.8** & **72.4** & **83.3** \\  

Table 2: _Ablations show the utility of the cluster performance gap and influence metrics._ We report Precision@10 metrics for a CLIP-RN50 model fine-tuned on real-world data (171 settings).

Figure 2: RaVL _accurately identifies spurious correlations._ Using our evaluation settings, we show that RaVL consistently outperforms prior methods in discovering learned spurious correlations between image features and textual attributes. Here, we provide Precision@10 metrics for a CLIP-RN50 model fine-tuned on synthetic data (129 settings) and real-world data (171 settings).

### Results: RaVL Effectively Discovers Spurious Correlations

**Comparisons to Prior Approaches**: Given an evaluation setting with a predefined spurious correlation \((^{eval},a^{eval})\), a fine-tuned VLM \(\), and an evaluation dataset \(_{V}^{val}\), our goal is to determine the extent to which RaVL can discover the correlation between \(^{eval}\) and \(a^{eval}\).

To this end, we use the labeled zero-shot classification dataset \(_{V}^{val}\), which includes ground-truth region bounding boxes and associated region labels. We provide the ground-truth bounding boxes as input to RaVL, which returns a single top-ranked cluster of regions likely to include spurious features. We rank regions within the cluster based on similarity to the cluster medoid, and we utilize the provided region-level labels in \(_{V}^{eval}\) to evaluate the proportion of top-\(K\) regions that contain the desired spurious feature \(^{eval}\). In line with prior work , we report performance with Precision@K metrics. We note that given an identified spurious feature \(^{eval}\), the correlated textual attribute \(a^{eval}\) can be detected by identifying the class label in \(_{V}^{eval}\) where the absence of feature \(^{eval}\) leads to degraded performance.

There are few existing approaches for performing automated detection of fine-grained spurious features learned by VLMs. Here, we compare RaVL with four previously-developed methods: Distilling Failures , George , Domino , and Spurious-Aware Detection . Distilling Failures, George, and Domino are state-of-the-art approaches for automatic identification of model failures resulting from spurious correlations; although these methods operate at the global image level and are designed for unimodal settings, we adapt these approaches for our setting by utilizing regions and zero-shot classification scores as input. Spurious-Aware Detection operates at the fine-grained region level by computing class-based performance gaps resulting from the presence or absence of particular features. To enable a fair comparison with RaVL, we provide the same set of regions and associated embeddings as input to all baselines. We also compare RaVL with a random baseline, where the ranked list of regions is shuffled randomly.

Table 1 summarizes mean Precision@10 metrics across all 654 evaluation settings. Results demonstrate that RaVL consistently outperforms prior approaches in discovering spurious correlations between image features and textual attributes, contributing to a 191% improvement over the closest baseline. In Table 1, we evaluate the effects of learned spurious correlation strength by varying the error threshold \(_{eval}\) from 10 to 40 and reporting performance for the subset of valid evaluation settings. Results show that RaVL is particularly effective when VLM \(\) learns a strong spurious correlation; as learned correlation strength increases, performance of RaVL increases by 47% whereas most baselines degrade in performance. We also observe that Domino, George, and Distilling Failures often achieve performance near or below the random baseline across our evaluation settings; this suggests that methods designed for detecting errors resulting from spurious correlations at the global image-level cannot be easily adapted for fine-grained region-level discovery. Figure 2 demonstrates that our findings hold for both synthetic and real-world data.

**Ablations**: Our ablation study evaluates the role of the cluster influence score \(H_{c}\) and the cluster performance gap metric \(G_{c}\) (Section 3.1) in enabling accurate discovery of spurious correlations between image features and textual attributes. We compare the following three metrics for ranking clusters: (1) an unweighted cluster performance gap metric where \(w_{y}\) is set to 1, (2) the cluster performance gap with \(w_{y}\) computed as in Section 3.1, and (3) a combination of the cluster performance gap and cluster influence metric as used in RaVL. As shown in Table 2, the metrics utilized by RaVL consistently demonstrate the best performance across various learned correlation strengths (\(_{eval}\)). Our results suggest the utility of both the performance gap metric and the influence score in identifying fine-grained spurious correlations.

**Evaluations in the Wild**: In addition to our controlled evaluations, we evaluate the ability of RaVL to surface spurious correlations learned by 12 off-the-shelf VLMs [12; 36; 20]; this presents a realistic and uncontrolled evaluation setting. We consider two zero-shot classification tasks \(_{V}\): (1) a 397-class scene classification task on SUN397  and (2) binary classification of cardiomegaly in chest X-rays from ObjectCXR . We use the cluster performance gap metric \(G_{c}\), introduced in Section 3.1, to quantify the degree of the learned spurious correlation.

Our results demonstrate that all evaluated models, which span a range of architecture, training data, and parameter counts, show evidence of having learned spurious correlations; this is demonstrated by nonzero values of the cluster performance gap metric \(G_{c}\). On average across the evaluated models, the top-ranked spurious feature cluster discovered by RAVL on SUN397 achieves a cluster performancegaps (\(G_{c}\)) of \(9.9_{ 3.2}\) (minimum = 5.1, maximum = 14.0). On ObjectCXR, the mean value of \(G_{c}\) is \(0.08_{ 0.04}\) (minimum = 0.04, maximum = 0.12).2 Our results support findings from previous work suggesting that _all_ models may learn spurious correlations .

In Figure 3, we provide qualitative examples of discovered spurious features for the CLIP ViT-B/16 model evaluated on SUN397 and the PubmedCLIP ResNet-50 model evaluated on ObjectCXR. For the CLIP ViT-B/16 model, RAVL surfaces a feature cluster consisting of text-based retail signage. We observe significant performance gaps between images containing the RaVL-identified feature and images that do not contain the feature. For instance, we note a 48.2 point difference in zero-shot classification accuracy for the class label fast food restaurant, suggesting that a CLIP ViT-B/16 model can better classify a scene of a fast food restaurant when a text-based retail sign is present. For the PubmedCLIP ResNet-50 model, RAVL discovers that the presence of metal clips (found in the patient's clothing) is spuriously correlated with cardiomegaly. We observe that the presence of clips improves zero-shot classification accuracy for the class label cardiomegaly by 15.3 points.

Our evaluations show that RaVL can surface fine-grained spurious correlations in realistic settings. Additional implementation details and qualitative examples are provided in Appendix D.

## 4 Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models

In this section, we present the second stage of RaVL, which aims to mitigate learned spurious correlations in VLMs. In Section 4.1, we discuss our methodology for mitigating fine-grained spurious correlations with a novel region-aware loss function. In Section 4.2, we use the evaluation framework previously introduced in Section 3.2 to demonstrate that RaVL substantially outperforms prior approaches in mitigating spurious correlations between image features and textual attributes.

  
**Method** &  &  \\   & **Img. Overall** & **Img. WG** & **Reg. Overall** & **Reg. WG** & **Img. Overall** & **Img. WG** & **Reg. Overall** & **Reg. WG** \\  Standard FT & 64.0 & 31.4 & 72.0 & 46.9 & 64.6 & 31.0 & 72.9 & 47.4 \\ Upsampled FT & 66.6 & 37.8 & 74.3 & 52.2 & 66.7 & 37.7 & 74.7 & 52.8 \\ VL-ERM & 68.8 & 32.2 & 75.6 & 50.3 & 68.7 & 30.9 & 75.9 & 50.6 \\ VL-GDR & 69.1 & 33.7 & 75.6 & 50.4 & 68.8 & 31.1 & 76.0 & 51.0 \\ Spurious-Aware & **69.8** & 33.6 & 76.5 & 50.6 & 69.2 & 30.7 & 76.8 & 50.5 \\ RaVL (Ours) & **69.8** & **39.1** & **78.9** & **57.8** & **70.2** & **40.8** & **79.5** & **58.5** \\   

Table 3: RaVL _effectively mitigates spurious correlations._ Here, we report mean Image Overall, Image Worst-Group (Img. WG), Region Overall, and Region Worst-Group (Reg. WG) metrics across our real-world evaluation settings. Since performance of mitigation methods is dependent on the results of Stage 1, we report metrics across settings where Stage 1 Precision@10\(>0.6\) and Stage 1 Precision@10\(>0.8\).

Figure 3: RaVL _surfaces spurious correlations in off-the-shelf VLMs._ RaVL identifies a spurious correlation learned by CLIP ViT-B/16 between the presence of text-based retail signage and the class label fast food restaurant in a scene classification task. RaVL also surfaces a spurious correlation learned by PubMedCLIP ResNet-50 between metal clips (found in clothing) and the class label cardiomegaly (a heart condition) on a chest X-ray classification task.

### Our Approach: Mitigating Spurious Correlations

As described in Section 3, Stage 1 of RaVL discovers image features that VLM \(\) has learned to spuriously correlate with textual attributes. We next aim to mitigate the spurious correlation. Motivated by prior work on fine-grained VLMs [58; 46], our key insight is that utilizing region-level information during VLM training can enable models to focus on relevant image-text relationships and ignore spurious correlations.

Since dataset \(_{F}\) exclusively consists of images and text, ground-truth subgroup and class labels are not available. As a result, we first assign plausible (i) region-level subgroup labels and (ii) image-level class labels to the vision-language fine-tuning dataset \(_{F}\). To assign subgroup labels, we decompose each image \(I_{i}\) in dataset \(_{F}\) into a set of candidate regions \(_{i}\). We then fit the trained K-Medoids clustering model from Section 3.1 on \(_{i}\) and identify all spurious regions associated with the top ranked cluster. We represent the identified spurious regions as \(_{i}^{s}\) and remaining non-spurious regions as \(_{i}^{r}\) such that \(_{i}^{s}_{i}^{r}=_{i}\). In order to assign plausible class labels, we parse the paired text \(T_{i}\) associated with each image to identify samples that reference the class labels included in the zero-shot classification label set \(\); we refer to the assigned class label for image \(I_{i}\) as \(_{i}\).

We now introduce a novel region-aware contrastive loss function for training VLM \(_{new}\). For batch \(\), we define \(_{}^{s}\) as the set of all spurious regions in the batch: \(_{}^{s}=_{I_{i}}_{i}^{s}\). For image \(I_{i}\), the first loss component \(L_{R}^{i}\) encourages high embedding similarity between non-spurious regions \(_{i}^{r}\) and assigned class label \(_{i}\) when compared to other class labels.

\[L_{R}^{i}=-(_{i}^{r},_{i})}{_{_{j}}_{m}(_{i}^{r},_{j})+P(_ {}^{s})}\] (3)

Here, for region embedding function \(f\) and text embedding function \(g\), \(_{m}(A,b)=(_{a A}( f(a),g(b)/))\) with temperature \(\). The term \(P(_{}^{s})\) is a penalty that enforces embedding-level dissimilarity between spurious regions and correlated class labels.

The second loss component \(L_{A}^{i}\) encourages high embedding similarity between non-spurious regions \(_{i}^{r}\) and assigned class label \(_{i}\) when compared to other regions. We define \((a,b)=( f(a),g(b)/)\) with temperature \(\).

\[L_{A}^{i}=-(_{i}^{r},_{i})}{_{m}( _{i}^{r},_{i})+_{j=1,_{j}_{i}}^{| |}_{m}(_{j}^{r},_{i})+_{r_{j}_{}^{s}}(r_{j},_{i})}.\] (4)

The final loss is expressed as \(L= L_{CL}+(1-)_{i=1}^{||}(L_{R}^{i}+L_{A}^{i})\). Here, \(\) is a hyperparameter and \(L_{CL}\) takes the form of the original loss function used for training \(\); in our experiments, \(L_{CL}\) is the CLIP objective . Extended formulations of our loss function are provided in Appendix C.

### Results: RaVL Effectively Mitigates Spurious Correlations

**Comparisons to Prior Approaches**: We use the evaluation framework previously introduced in Section 3.2 to compare RaVL with prior approaches. There are few existing approaches for mitigating spurious correlations in the setting of fine-tuned VLMs. Here, we compare RaVL with standard VLM fine-tuning, upsampled VLM fine-tuning, ERM, GDRO , and Spurious-Aware Mitigation . Since ERM and GDRO are traditionally used in unimodal classification settings, we adapt these approaches for our setting by adding a contrastive vision-language objective and using zero-shot classification scores during fine-tuning; we refer to these approaches as VL-ERM and VL-GDRO respectively.

Table 3 summarizes mean zero-shot classification results across our real-world evaluation settings. Since performance of mitigation methods is dependent on the accuracy of the discovered spurious correlations in Stage 1, Table 3 displays results for two evaluation categories: (i) the 192 settings where RaVL Stage 1 Precision@10 is greater than 0.6, and (ii) the 106 settings where RaVL Stage 1 Precision@10 is greater than 0.8. In line with prior works on robustness [39; 56], we report image overall performance and image worst-group performance. Additionally, in order to evaluate the extent to which the VLM understands fine-grained features, we introduce two new metrics: region overall performance and region worst-group performance. Region-level accuracies are computed by performing zero-shot classification with region embeddings and comparing predicted labels to the ground-truth region-level labels provided in the zero-shot classification dataset.

Results show that RAVL consistently outperforms prior approaches in mitigating spurious correlations. Across the two evaluation categories in Table 3, RAVL contributes to an improvement of up to 8.2% on image worst-group performance and 10.8% on region worst-group performance over the nearest baseline. Improvements in region worst-group performance are particularly notable, suggesting that RAVL can better interpret fine-grained features when compared to prior approaches. Additionally, as the accuracy of the discovered spurious correlations in Stage 1 increases, the performance of the RAVL mitigation approach increases proportionally. Our results demonstrate the efficacy of our fine-tuning procedure in mitigating spurious correlations when compared to prior approaches.

## 5 Conclusion

In this work, we introduced RAVL, a fine-grained region-aware approach for addressing spurious correlations in VLMs. We demonstrate through large-scale, controlled experiments as well as in-the-wild evaluations that RAVL can discover (191% improvement in identified correlations) and mitigate (8.2% improvement on worst-group performance) spurious correlations in VLMs. We hope that our work can help (i) diagnose and correct critical failure modes in VLMs prior to deployment and (ii) drive progress towards the development of fine-grained approaches for model robustness.