# On the Interplay between Social Welfare and Tractability of Equilibria

Ioannis Anagnostides

Carnegie Mellon University

ianagnos@cs.cmu.edu

&Tuomas Sandholm

Carnegie Mellon University

Strategic Machine, Inc.

Strategy Robot, Inc.

Optimized Markets, Inc.

sandholm@cs.cmu.edu

###### Abstract

Computational tractability and social welfare (aka. efficiency) of equilibria are two fundamental but in general orthogonal considerations in algorithmic game theory. Nevertheless, we show that when (approximate) _full efficiency_ can be guaranteed via a _smoothness_ argument a la Roughgarden, Nash equilibria are approachable under a family of no-regret learning algorithms, thereby enabling fast and decentralized computation. We leverage this connection to obtain new convergence results in _large games_--wherein the number of players \(n 1\)--under the well-documented property of full efficiency via smoothness in the limit. Surprisingly, our framework unifies equilibrium computation in disparate classes of problems including games with vanishing _strategic sensitivity_ and two-player zero-sum games, illuminating en route an immediate but overlooked equivalence between smoothness and a well-studied condition in the optimization literature known as the _Minty property_. Finally, we establish that a family of no-regret dynamics attains a welfare bound that improves over the smoothness framework while at the same time guaranteeing convergence to the set of coarse correlated equilibria. We show this by employing the _clairvoyant_ mirror descent algortihm recently introduced by Piliouras _et al._

## 1 Introduction

The _Nash equilibrium (NE)_ formalizes the notion of a _stable_ outcome in a multiagent strategic interaction, and has arguably served as the most influential solution concept in the development of game theory. Indeed, algorithms designed to approximate Nash equilibria in two-player zero-sum games have recently resolved major challenges in AI . Its prescriptive power, however, has been severely undermined in multi-player general-sum games by intrinsic computational barriers , limitations which also manifest in the inability of natural learning algorithms to converge . Another well-established but orthogonal critique is the _equilibrium selection_ problem : a general-sum game may have multiple Nash equilibria with widely different welfare. As a result, a modern research agenda in computational game theory has been to identify and characterize natural classes of games that circumvent those fundamental limitations.

In this paper, we uncover a new natural class of games for which the aforementioned caveats of Nash equilibria can be effectively addressed. In particular, our investigation originates from a natural question: when are _efficient_--in terms of social welfare--Nash equilibria easy to compute? The answer to this question is, at first glance, unsatisfactory: even if Nash equilibria are fully efficient, computational hardness still persists given that constant-sum multi-player games are hard. Indeed, efficiency and computational tractability are, in general, two orthogonal considerations. We show,however, an interesting twist, an unexpected interplay between efficiency--when viewed from a specific lens--and the behavior of a family of _no-regret_ learning algorithms.

### Our results

To elucidate the alluded connection that drives much of our results, we first have to recall that the canonical paradigm for establishing the efficiency of equilibria is Roughgarden's celebrated _smoothness_ framework  (exposed thoroughly in Section 2). In this context, we observe that _if full efficiency of equilibria can be guaranteed via a smoothness argument (with bounded parameters), then Nash equilibria are approachable under a family of no-regret learning algorithms_. In other words, full efficiency _via smoothness_ implies computational tractability of NE. This is surprising in that Roughgarden's smoothness framework was developed primarily in order to automatically extend _price of anarchy (PoA)_ bounds to more permissive equilibrium concepts, such as _coarse correlated equilibria (CCE)_; tractability of NE appears at first glance entirely unrelated. In fact, while applying the smoothness framework to (multi-player) constant-sum games appears to make little sense, given that PoA considerations are trivial in such games (all outcomes attain the same social welfare), it turns out that a certain regime of smoothness in (multi-player) constant-sum games is equivalent to a well-known condition in the optimization literature called the _Minty property_ (Observation 3.4).

The condition described above already captures--somewhat unexpectedly--well-studied settings, such as games that admit a minimax theorem , but we have found that the most fertile and novel ground to apply this theory revolves around _large games_, that is, games with a large number of players \(n 1\).1 The reason we focus on large games is a well-documented economic phenomenon: equilibria in large games approach--under natural conditions--full efficiency as \(n+\), a property often established via smoothness --a crucial ingredient in our framework. (One rough intuition for this is that in large games each player's influence on the outcome becomes negligible, making optimal behavior easier to characterize .) To state our first main result, we denote by \((,)\) a pair of bounded _smoothness_ parameters of a game \(\); the ratio \((,)\) circumscribes the (in)efficiency of equilibria of \(\), in that all equilibria will attain at least a \(\) fraction of the optimal welfare.

**Theorem 1.1** (Informal).: _Consider a sequence of \(n\)-player \((_{n},_{n})\)-smooth games \((_{n})_{n 1}\) such that \(_{n}}{1+_{n}} 1\) with a sufficiently fast rate. Then, there are decentralized and computationally efficient no-regret dynamics approaching an \((o_{n}(1),o_{n}(1))\)-weak Nash equilibrium._

A few remarks are in order. First, in Theorems 3.1 and A.2 we give a precise non-asymptotic characterization that quantifies the number of iterations as well as the approximation error. We also recall that in an \((o_{n}(1),o_{n}(1))\)-_weak_ Nash equilibrium _almost all_ players are _almost best responding_ (Definition 2.2); this is a well-studied relaxation for which hardness results carry over in general . A limitation of a weak Nash equilibrium is that it can prescribe a strategy profile in which a large numbers of players--albeit of vanishing fraction--can significantly benefit from deviating; this limitation is inherent in a certain regime of Theorem 1.1. Depending on the rate with which \(_{n}\) approaches full efficiency, Theorem 1.1 can also imply convergence to the usual notion of Nash equilibrium--wherein _all_ players are almost best responding (Corollary A.5). More generally, for a broad class of games that includes _graphical_ games with bounded degree, _polymatrix_ games, and games exhibiting small _strategic sensitivity_, the conclusion of Theorem 1.1 applies without imposing any restrictions on the rate of convergence of \(_{n}\); those are the most commonly studied classes of games when \(n 1\).

There are ample compelling aspects in connecting the convergence of no-regret learning algorithms with Roughgarden's smoothness framework. First, there has been a considerable interest in understanding smoothness, insights that can now be inherited to a seemingly entirely different but equally fundamental problem. For example, smoothness naturally extends to _Bayesian games_ (see also  for the related class of contextual games), a property that we leverage in Theorem 3.8 to expand our scope to mechanisms of incomplete information. Additional extensions based on _local smoothness_ and the refined primal-dual framework of Nadav and Roughgarden  are also described in Appendices A.3 and A.4. Finally, our criterion has a clear and natural economic interpretation, and is part of an ongoing research effort to identify tractable classes of variational inequalities (VIs) beyond the Minty property .

As a concrete example, our framework subsumes games wherein each player's effect on the outcome vanishes as \(n\); this captures, for example, simple voting settings , as well as general auction design problems where establishing that property turns out to be highly non-trivial and quite delicate . More concretely, for games with vanishing _sensitivity_\(_{n} 0\), in that unilateral deviations can only affect a player's utility by an additive \(_{n}\) (Definition 3.5), we show that the conclusion of Theorem 1.1 applies as long as \(_{n} o_{n}(1/)\) (Theorem 3.6). At the same time, the condition that \(_{n} 1\) goes much deeper than games with vanishing sensitivity, and, as we explained earlier, surprisingly applies to two-player zero-sum games as well. We find it conceptually appealing that a unifying framework can establish tractability of Nash equilibria in two seemingly disparate classes of problems such as two-player zero-sum games and games with vanishing strategic sensitivity.

Remaining on smooth games, but relaxing the assumption \( 1\), we next study conditions under which the efficiency guaranteed by the smoothness framework can be improved, while at the same time ensuring the no-regret property, thereby implying convergence to the set of CCE. The smoothness bound is known to be applicable to _any_ outcome of no-regret dynamics, but here we are instead interested in more refined guarantees when specific learning algorithms are in place. Building on a recent result , we show in Theorem 4.1 that the _clairvoyant_ variant of gradient descent, introduced by Piliouras et al. , enjoys an improved welfare bound _and_ ensures fast convergence to the set of CCE for the average correlated distribution of play. Crucially, compared to an earlier result , the clairvoyant algorithm manages to satisfy an appealing notion of per-player incentive compatibility, in the form of convergence to CCE. In other words, improving the welfare predicted by smoothness is not at odds with incentive compatibility (Corollary 4.3).

## 2 Background

NotationWe let \(=\{1,2,\}\) be the set of natural numbers. For \(n\), we denote by \( n\{1,2,,n\}\). For a vector \(^{d}\), we denote by \(\|\|_{2}\) its (Euclidean) \(_{2}\) norm. For a convex, compact and nonempty set \(\), we let \(_{}()\) represent the Euclidean projection operator with respect to \(\). We let \(D_{}\) be the \(_{2}\)-diameter of \(\). To simplify the exposition, we often use the \(O()\) notation in the main body to suppress the dependence on certain parameters; we also write \(O_{n}()\) to indicate the asymptotic growth solely as a function of \(n\), so as to lighten the exposition.

Multilinear gamesWe consider \(n\)-player _multilinear_ games. In a multilinear game \(\) each player \(i n\) has a convex, compact and nonempty set of feasible strategies \(_{i}^{d_{i}}\), for some dimension \(d_{i}\). Under a joint strategy profile \(=(_{1},,_{n})_{i=1}^{n}_{i} \), there is a continuous utility function \(u_{i}:(_{1},,_{n})\) such that \(u_{i}()=_{i},_{i}(_{-i})\), for some function \(_{i}:_{-i}^{d_{i}}\); here, we used for convenience the standard notation \(_{-i}(_{1},,_{i-1},_{i+1},,_ {n})\). This setup readily captures _normal-_ as well as _extensive-form_ games.

Specifically, in a normal-form game every player \(i n\) selects as strategy a probability distribution \(_{i}(_{i})\) over a finite set of available actions \(_{i}\). There is an arbitrary utility function \(u_{i}:_{i=1}^{n}_{i}[-1,1]\) that maps a joint action profile to a utility for Player \(i\); we will be making the standard assumption that the range of each player's utilities is bounded by an absolute constant, which is in particular independent of the number of players \(n\). In this setting, the mixed extension of the utility function indeed satisfies the multilinearity condition imposed above: for \(_{i=1}^{n}(_{i})\), \(u_{i}()_{}[u_{i}()]= _{i},_{_{i}}u_{i}()\), where we overloaded the notation \(u_{i}()\).

Returning to the general setting of multilinear games, we let \(F_{}:_{i=1}^{n}_{i}_{i=1}^{n}_ {i}\) denote the underlying _operator_ of the game \(\), defined as \(F_{}:(_{1},,_{n})(_{1}(_{-1}), ,_{n}(_{-n}))\), where function \(_{i}\) was introduced earlier. For notational simplicity, we will often omit the subscript \(\) when it is clear from the context. We will say that \(F\) is _L-Lipschitz continuous_ (w.r.t. the \(_{2}\) norm) if for any \(,^{}_{i=1}^{n}_{i}\) it holds that \(\|F()-F(^{})\|_{2} L\|-^{}\|_{2}\).

Welfare and the price of anarchyFor a joint strategy profile \(_{i=1}^{n}_{i}\), we define the _social welfare_ attained under \(\) as \(()_{i=1}^{n}u_{i}()\). The maximum possible social welfare attainable in a game \(\) will be denoted by \(_{}\). Without any essential loss of generality, it will be assumed that \(_{}>0\). We say that the game is _constant-sum_ if \(()=V_{>0}\) for any \(_{i=1}^{n}_{i}\). The _price of anarchy (PoA)_ of a game \(\) quantifies the loss in efficiency incurred on account of strategic players . Formally, if \(_{}\) is the nonempty set of _(mixed) Nash equilibria_ of \(\) (Definition 2.2) , we define \(_{}_{_{}} \{()}{_{}}\}\).

Smooth gamesWe are now ready to recall the seminal notion of a _smooth game_,2 conceived in the pioneering work of Roughgarden  as a technique to (lower) bound the price of anarchy.

**Definition 2.1** (Smooth game ).: An \(n\)-player game \(\) is called _\((,)\)-smooth_, where \(>0\) and \(>-1\), if there exists \(^{}_{i=1}^{n}_{i}\) with \((^{})=_{}\) such that for every \(_{i=1}^{n}_{i}\),

\[_{i=1}^{n}u_{i}(_{i}^{},_{-i})_{ }-().\] (1)

(In the definition above, and throughout this paper, we slightly abuse notation by parsing \(u_{i}(_{i}^{},_{-i})\) as \(u_{i}(_{1},,_{i-1},_{i}^{},_{i+1},, _{n})\).) Roughgarden  observed that in a \((,)\)-smooth game, in the sense of Definition 2.1, every Nash equilibrium attains at least a \((,)\) fraction of the optimal social welfare \(_{}\). Importantly, this efficiency guarantee immediately carries over to outcomes of no-regret learning algorithms as well. The _robust price of anarchy (\(_{}\))_ is the best (_i.e._, largest) price of anarchy bound provable via a smoothness argument, and can be defined as the solution to the linear program induced by the smoothness constraints given in (1) (see (24) in Appendix A.5). One delicate point here is that the value of \(_{}\) could be associated with unbounded smoothness parameters, which is a pathological and rather trivial manifestation of smoothness (Remark A.10 elaborates on this point). To be clear, when we say a game \(\) attains a certain value \(_{}\), we mean that there exists a finite pair of legitimate smoothness parameters \((,)\) such that \(_{}=\). With this convention, it might be the case that \(_{}(,)_{}\) under any finite pair of smoothness parameters \((,)\) (Remark A.10). It is also easy to see that \(_{}_{}\).

Nash equilibriumWe next recall the concept of a _weak_ Nash equilibrium, a natural generalization of the standard notion which is meaningful in multi-player games.

**Definition 2.2** (Weak Nash equilibrium ).: Let \([0,1)\) and \(_{ 0}\). A joint strategy profile \(_{i=1}^{n}_{i}\) is an _\((,)\)-weak Nash equilibrium_ if at least a \(1-\) fraction of the players are \(\)-best responding. An \((,0)\)-weak Nash equilibrium will be simply referred to as _\(\)-Nash equilibrium_.

In the definition above, we clarify that a player \(i n\) is said to be _\(\)-best responding_ if \(_{i}(_{1},,_{n})_{_{i}^{ }_{i}}_{i}^{},_{i}(_{-i}) -_{i},_{i}(_{-i})\). We also define the _Nash equilibrium gap_ as \(()_{1 i n}_{i}()\). It has been shown that hardness results in multi-player general-sum games persist under the weak Nash equilibrium concept introduced above, even when \(\) and \(\) are absolute constants bounded away from \(0\).

RegretWe are operating in the usual online learning setting. At every time \(t\) a player \(i n\) selects a strategy \(_{i}^{(t)}_{i}\), and then receives as feedback the linear utility function \(_{i}_{i},_{i}^{(t)}\), where we overload notation so that \(_{i}^{(t)}_{i}(_{-i}^{(t)})\). The _regret_ of player \(i n\) under a time horizon \(T\) is defined as

\[_{i}^{(T)}_{_{i}^{}_{i}} \{_{t=1}^{T}u_{i}(_{i}^{},_{-i}^{(t)})\}-_ {t=1}^{T}u_{i}(^{(t)}).\]

Optimistic gradient descentBy now, there are many online algorithms known to guarantee _sublinear_ regret, \(_{i}^{(T)}=o(T)\), even if the observed utilities are selected adversarially . The main no-regret learning algorithm we consider in this paper is _optimistic gradient descent_ (henceforth OGD) , which is known to yield improved regret guarantees in the setting of learning in games . For each player \(i n\), OGD is defined through the following update rule for \(t\).

\[_{i}^{(t)} _{_{i}}(}_{i}^{(t)}+ _{i}^{(t)}),\] (OGD) \[}_{i}^{(t+1)} _{_{i}}(}_{i}^{(t)}+ _{i}^{(t)}).\]Here, \(>0\) is the _learning rate_; \(_{i}^{(t)}^{d_{i}}\) is the _prediction_ vector; and \(}_{i}^{(1)}_{i}\) is the _initialization_. It is assumed that the strategy set \(_{i}\) is such that the Euclidean projection \(_{_{i}}()\) can be implemented efficiently. We will let \(_{i}^{(t)}_{i}^{(t-1)}\) for any \(t\), where \(_{i}^{(0)}_{i}(}_{-i}^{(1)})\).

## 3 Convergence to Nash equilibria via smoothness

In this section, we study the convergence of optimistic gradient descent (OGD) in large games, that is to say, in the regime \(n 1\). In our first main result, stated below as Theorem 3.1, we show that when \(_{n+}_{n}=1\) with a sufficiently fast rate, not only are Nash equilibria approaching the optimal social welfare, but they can also be computed efficiently in a decentralized fashion via OGD.

In the sequel, when considering a sequence of games \((_{n})_{n 1}\), with each game \(_{n}\) being parameterized by the number of players \(n\), we will use a subscript with variable \(n\) to index the \(n\)th game in the sequence; that notation will also be used to refer to the other underlying parameters of the game that depend on the number of players.

**Theorem 3.1**.: _Consider an \(n\)-player \((,)\)-smooth game \(_{n}\) such that the game operator \(F_{n}\) is \(L_{n}\)-Lipschitz continuous and \((1-_{n})(1+)\), with \(_{n} 0\). Suppose further that all players follow OGD with learning rate \(_{n}=1/(4L_{n})\). Then, for \((0,1)\) and a sufficiently large number of iterations \(T=O_{n}(nL_{n}^{2}/_{n})\), where \(_{n} L_{n}_{_{n}}_{n}\), there is time \(t^{} T\) such that \(^{(t^{})}(_{1}^{(t^{})},,_{n}^{(t^ {})})\) is a_

\[(}O_{n}(}{n}}), )-\] (2)

_In particular, if \(_{n}=O_{n}(n^{1-})\), for \((0,1)\), OGD yields an \((O_{n}(n^{-}),O_{n}(n^{-}))\)-weak NE._

We remark that if it further holds that \(_{n}=o_{n}(1)\), the above theorem establishes convergence to the standard notion of Nash equilibrium--wherein _all_ players are (almost) best responding (Corollary A.5). Parameter \(_{n}\), which is proportional to the error term \(_{n}\), controls both the number of iterations and the approximation guarantee in (2); we refer to Theorem A.2 (in Appendix A) for a more precise non-asymptotic characterization, which bounds the players' cumulative best response gap as a function of the growth of \(_{n}\). We also note that Theorem 3.1 can be strengthened so that (2) holds for _most_ iterates of OGD (say 99%), not just a single one (Remark A.4).

Now, to be more concrete regarding the preconditions of Theorem 3.1, we first observe that the growth of the Lipschitz constant \(L_{n}\) depends on the normalization assumptions as well as the structure of the underlying game \(_{n}\). In particular, let us assume--as is standard--that the range of the utility functions is independent of \(n\), in which case \(_{_{n}}=O_{n}(n)\). We then show that \(L_{n}=O_{n}(1)\) in each of the following cases: graphical games with bounded degree (Lemma A.7), games with \(O_{n}(1/n)\) strategic sensitivity per Definition 3.5 (Lemma A.8), and polymatrix (general-sum) games even with unbounded neighborhoods (Lemma A.9); the first two of the aforementioned classes are the most commonly studied classes in the literature under the regime \(n 1\). For all of those classes, applying Theorem 3.1 yields an \((o_{n}(1),o_{n}(1))\)-weak Nash equilibrium for any \(_{n} o_{n}(1)\). More generally, the Lipschitz constant \(L_{n}\) can grow with \(n\) (Lemma A.6), in which case \(_{n}\) must vanish with a faster rate for the conclusion of Theorem 3.1 to kick in. One important limitation of Theorem 3.1 is that, at least in a certain regime, it prescribes a strategy in which many players--albeit a vanishing fraction--may have a profitable deviation (in accordance with Definition 2.2); this is admittedly an inherent feature of our framework.

_Remark 3.2_.: In a decentralized environment, one question that arises from Theorem 3.1 concerns the identification of a time index \(t^{} T\) that satisfies (2), which can be viewed as the stopping condition of the algorithm. We suggest two possible approaches. First, if we accept that players have access to a common source of randomness, then all players can sample the same index \(t^{}\) uniformly at random from the set \( T\). As we point out in Remark A.4, this suffices to provide a guarantee with high probability with only a small degradation in the solution quality. The second approach, which does not rest any having a common source of randomness, involves a coordinator who can communicate with the players but possesses no information whatsoever about the underlying game. The coordinator sets a target solution quality parameterized by \((,)\) (per Definition 2.2), and after each iteration \(t\) elicits from each player \(i n\) a single bit, encoding whether \(\{_{i}(^{(t)})>\}\). (We note that each player can indeed determine its best response gap with only its local information--namely,the utility feedback.) The coordinator can then evaluate whether the fraction of the players with at most an \(\) best response gap matches the desired accuracy. While the second approach makes for a less decentralized protocol, the communication overhead described above is arguably very limited.

The key precondition of Theorem 3.1 pertains the behavior of the smoothness parameters, to be discussed next after we first sketch the proof of Theorem 3.1, which extends a recent technique .

Proof sketch of Theorem 3.1The proof is based on the fact that the sum of the players' regrets \(_{i=1}^{n}_{i}^{(T)}\)_cannot be too negative_, which in turn follows from the assumption that \(_{n} 1-_{n}\). The argument then proceeds by bounding the players' cumulative best response gap across the \(T\) iterations \(_{t=1}^{T}_{i=1}^{n}(_{i}(^{(t)}))^{2}\) as a function of \(_{n}\) and the time horizon \(T\), ultimately leading to the conclusion of Theorem 3.1.

Connection with the Minty propertyWhile we have stated Theorem 3.1 in the regime \(n 1\), under the premise that \(_{n}\) is sufficiently close to \(1\) (as a function of \(n\)), its conclusion is in fact interesting beyond that regime. Indeed, an important observation is that any two-player constant-sum game \((,)\), with \(_{1},_{2}+_{1}, {x}_{2}=V\) for any \((_{1},_{2})_{1}_{2}\), satisfies \(_{}=1\). This is indeed a consequence of Von Neumann's minimax theorem: \((_{1}^{},_{2}^{})_{1} _{2}\) such that \(u_{1}(_{1}^{},_{2})+u_{2}(_{2}^{},_{1})-V= _{1}^{},_{2}-_{1}, _{2}^{} 0\) for any \((_{1},_{2})_{1}_{2}\), in turn implying that \(u_{1}(_{1}^{},_{2})+u_{2}(_{2}^{},_{1})( 1+)_{}-()=V\); that is, any two-player constant-sum game is \((1+,)\)-smooth, thereby making the conclusion of Theorem 3.1 readily applicable (by taking \( 1-^{2}\) for any \(>0\); see the explicit statement of Theorem A.2). This captures and unifies earlier iteration complexity bounds under OGD and the extra-gradient method .

**Proposition 3.3**.: _Any two-player constant-sum game \(\) is \((1+,)\)-smooth for \(>-1\), implying that \(_{}=1\). Thus, \(O(1/^{2})\) iterations of OGD suffice to obtain an \(\)-Nash equilibrium, for any \(>0\)._

More broadly, there is a surprisingly overlooked but immediate connection between smooth games (per Definition 2.1) and the _Minty property_, a well-known condition in the literature on variational inequalities (VIs) . More precisely, the Minty property postulates the existence of a strategy profile \(^{}_{i=1}^{n}_{i}\) such that \(^{}-,F() 0\) for any \(_{i=1}^{n}_{i}\), where we recall that \(F\) is the operator of the game. (We caution that the last inequality is typically stated with the opposite sign since the operator \(F\) is defined oppositely.) The following connection is thus immediate from the fact that \(^{},F()=_{i=1}^{n}_{i}^{},_{i}(_{-i})=_{i=1}^{n}u_{i}(_{i}^{},_ {-i})\) and \(,F()=_{i=1}^{n}_{i},_{i}(_{-i})=()\) (by multilinearity).

**Observation 3.4**.: _For any (multi-player) constant-sum game \(\), the Minty property is equivalent to \(\) being \((1+,)\)-smooth for some \(>-1\)._

Indeed, the Minty property implies that \(_{i=1}^{n}u_{i}(_{i}^{},_{-i}) V=(1+)_{ }-()\), and the converse direction is also immediate. In fact, for (multi-player) zero-sum games, the Minty property is equivalent to \(\) satisfying (1) under some pair \((,)^{2}\). We stress that even if \(_{}=1\), traditional no-regret learning algorithms such as online mirror descent do not generally enjoy iterate convergence to Nash equilibria , which stands in stark contrast to the behavior of OGD (Proposition 3.3). In light of Observation 3.4, Theorem 3.1 should also be viewed as part of an ongoing effort to establish sufficient conditions of tractability that are more permissive than the Minty property (_e.g._, ). The criterion we furnish herein, based on the smoothness framework, has the important benefit of enjoying a natural economic interpretation, as well as having being extensively studied in the literature. Indeed, we will leverage insights from prior work to obtain several interesting extensions in the remainder of this section.

Games with vanishing sensitivityReturning to the regime \(n 1\), why should we expect \(_{n} 1\)? Indeed, if anything large games are more general than games with a small number of players since one can always incorporate "dummy" players into the game. Yet, the point is that large games oftentimes exhibit a structure that leads to more efficient outcomes. For example, one immediate implication of our framework relates to games with a _vanishing (strategic) sensitivity_ (see, _e.g._, ). There are various ways of defining sensitivity; here, we adopt the following standard definition.

**Definition 3.5**.: The strategic _sensitivity_\(_{>0}\) of an \(n\)-player game in normal form is defined as

\[_{1 i n}_{}_{1 i^{ } n}_{a_{i^{}}^{}_{i^{}}}|u_{i}(a_{ i^{}}^{},_{-i})-u_{i}()|.\]In words, a unilateral deviation can only impact a player's utility by an additive \(\). Now, as long as the sensitivity decays fast enough, a proof analogous to that of Theorem 3.1 implies the following.

**Theorem 3.6**.: _Consider an \(n\)-player game \(_{n}\) with sensitivity \(_{n}_{>0}\). Then, \(T=O_{n}(n)\) iterations of_ OGD _suffic to obtain a \((}O_{n}(_{n}),)\)-weak Nash equilibrium, for \((0,1)\)._

In particular, Theorem 3.6 yields an \((o_{n}(1),o_{n}(1))\)-weak Nash equilibrium as long as \(_{n}=o_{n}()\). Further, in the canonical regime where \(_{n}=O_{n}(1/n)\), Theorem 3.6 circumscribes the best response gap for all but a constant number of players (by taking \(=O_{n}(1/n)\)). There are many natural settings where we should expect results such as Theorem 3.6 to be applicable . We find it conceptually compelling that our framework can provide in a unifying way equilibrium guarantees for two seemingly disparate classes of games, namely two-player zero-sum games and games with vanishing strategic sensitivity.

In a similar vein, Feldman et al.  showed that \(_{n} 1\) with a rate of \(1/\) in a general auction design problem (see also ) under the relatively mild assumption that each player participates in the market with some constant probability (aka. probabilistic demand), thereby bypassing known barriers regarding the inefficiency of equilibria in general combinatorial domains. Their proof is based on the fact that each bidder's impact on the prices--under a simultaneous uniform-price auction format--becomes asymptotically negligible, in the spirit of Definition 3.5 introduced above. The difficulty that arises in that setting is that the natural representation of the utility functions violates our multilinearity assumption (postulated in Section 2). Instead, one would have to resort to some form of discretization before attempting to apply Theorem 3.1, which could be computationally prohibitive; the other prerequisite is that \(L_{n}=o_{n}()\), for which our approach in Lemma A.8 in conjunction with the insights of Feldman et al.  could be useful. Understanding whether our techniques can be applied in the combinatorial auction setting of Feldman et al.  is left as a challenging open question.

Efficiency of equilibria does not suffice for tractabilityA natural question arising from Theorem 3.1 is whether a similar statement applies under the assumption that \( 1\), that is, assuming that all Nash equilibria of \(\) are (approximately) fully efficient. This is clearly a weaker assumption, but it is unfortunately not sufficient to yield any non-trivial guarantees even in normal-form games:

**Proposition 3.7**.: _Even under the promise that \(_{}=1\), computing a \((1/())\)-Nash equilibrium in normal-form games in polynomial time is impossible when \(n 3\), unless \(\)._

This stands in contrast to the class of \((1+,)\)-smooth games, where a fully polynomial-time approximation scheme (\(\)) is implied by Theorem 3.1--assuming access to a utility and a projection oracle, both of which are available in, for example, most succinct normal-form games. Proposition 3.7 is a straightforward consequence of the fact that Nash equilibria are hard to compute even in constant-sum \(3\)-player games . Furthermore, in Example A.11 we identify a specific \(3\)-player game in which OGD fails (unconditionally) to converge to \(\)-Nash equilibria for a constant value of \(>0\), even though \(=1\). Our example is based on a variant of _Shapley's game_.

Another notable advantage of smoothness as a criterion of convergence is that, at least when the game is represented explicitly, it is easy to compute; this is in contrast to \(\), whose identification even in two-player games is \(\)-hard (Proposition A.12).

ExtensionsIt turns out that smoothness per Definition 2.1 can be further sharpened using a primal-dual framework . Such refined guarantees can also be translated into our setting (in the context of Theorem 3.1), as we elaborate on in Appendix A.3. The upshot is that the modification of Nadav and Roughgarden  necessitates analyzing the _weighted_ sum of the players' regrets \(_{i=1}^{n}z_{i}_{i}^{(T)}\) under a dual set of variables \(\{z_{i}\}_{i=1}^{n}\). Another interesting extension worth noting relies on the _local_ smoothness framework , as we explain in Appendix A.4; the key observation is that local smoothness per Nguyen  can be associated with a _linearized_ notion of regret, at which point the analysis of Theorem 3.1 readily carries over. Finally, we also expand our scope to Bayesian mechanisms, as we expound in the upcoming subsection.

Before we proceed, it is important to point out that, unsurprisingly, smoothness is not merely enough to guarantee convergence of OGD; see Example A.13. It is instead crucial to additionally ensure that \( 1\) in order to obtain interesting guarantees for the behavior of algorithms such as OGD.

### Bayesian mechanisms

Next, we leverage the connection between smoothness and convergence to Nash equilibria to extend the scope of our framework to Bayesian mechanisms. In particular, analogously to Definition 2.1, Syrgkanis and Tardos  have introduced the notion of a _smooth mechanism_ (Definition A.14); detailed background on Bayesian mechanisms and smoothness in that realm is provided in Appendix A.7. In this context, we leverage a reduction of Hartline et al.  from an incomplete-information to a complete-information mechanism to arrive at the following theorem. (The standard notion of a _Bayes-Nash equilibrium_ is analogous to Definition 2.2, and is recalled in Definition A.15 of Appendix A.7.)

**Theorem 3.8**.: _Consider a Bayesian mechanism \(\) such that \(_{}=1\). Then, for any \(>0\), \(T=O(1/^{2})\) iterations of \(\) suffice to obtain an \(\)-Bayes-Nash equilibrium of \(\)._

In particular, \(\) above is executed on the so-called _agent-form_ representation of \(\) (Appendix A.7). Analogously to Theorem 3.1, the above theorem can also be extended in the large \(n 1\) under the assumption that \(_{n} 1\). It is worth noting that Theorem 3.1 already can be applied to certain games of incomplete information (such as imperfect-information extensive-form games), but Theorem 3.8 additionally makes a connection with the literature on smoothness in mechanism design, which facilitates characterizing the smoothness parameters.

## 4 Improved welfare for no-regret dynamics

Roughgarden's seminal work  established that no-regret learning algorithms always attain asymptotically at least \(\) fraction of the optimal social welfare (on average). This guarantee is satisfactory for many classes of games where \(\) is close to \(1\) (emphatically those studied earlier in Section 3), but smoothness is certainly not a universal phenomenon: there are simple games in which the smoothness framework only provides vacuous guarantees; one such example is Shapley's game, discussed in Appendix A.10. As a result, one important question arising is whether it is possible to improve the efficiency bound predicted by smoothness when specific learning algorithms are in place, while at the same time still guaranteeing convergence to the set of _coarse correlated equilibria (CCE)_. We stress that optimizing over the set of CCE is typically NP-hard in succinct games [79; 6], making this question interesting also from a complexity-theoretic standpoint.

In this section, we show that it is indeed possible to obtain improved efficiency bounds under a generic condition, while at the same time guaranteeing the no-regret property for each player. A key ingredient in our improvement is the use of _clairvoyant_ mirror descent, an algorithm recently introduced by Piliouras et al. . More precisely, we will instantiate that algorithm with (squared) Euclidean regularization, which can be defined as follows. Let \(_{_{i}}(_{i})_{ _{i}^{}_{i}}\{_{i}^{}, _{i}-\|_{i}-_{i}^ {}\|_{2}^{2}\}\) be the induced _prox operator_, where \(_{i}_{i}\) and \(_{i}^{d_{i}}\). _Clairvoyant gradient descent_ (henceforth \(\)) at time \(t\) outputs \(^{(t)}=_{^{(t-1)}}( F(^{(t) }))(_{_{1}^{(t-1)}}(_{1}( _{-1}^{(t)})),,_{_{n}^{(t-1)}}( _{n}(_{-n}^{(t)})))\), where \(^{(t)}\) is any \(^{(t)}\)-approximate fixed point of the map \(_{i=1}^{n}_{i}_{^{(t-1 )}}( F())\), and \(^{(0)}\) is an arbitrary initialization. It turns out that for \(<1/L\), this map is a _contraction_[37; 85; 19], thereby making approximate fixed points easy to compute. Furthermore, there is also an uncoupled implementation of \(\), making the algorithm compelling from a decentralized standpoint as well, but we will not dwell on this issue here. We are now ready to state the main result of this section.

**Theorem 4.1**.: _Suppose that all players are updating their strategies using \(\) with \(^{(t)}D_{_{i}}}{t^{2}}\) and learning rate \(=\) in a \((,)\)-smooth game \(\), where \(L\) is the Lipschitz-continuity parameter of \(F\). Then, for any \(_{0}>0\) and \(TD_{}^{4}}{_{0}^{2}}\) iterations,_

1. _the average correlated distribution of play is a_ \(}^{2}}{T}-\)_;_
2. _there is a time_ \(t^{}[\![T]\!]\) _such that_ \[(^{(t^{})})_{ _{0}}\{_{}(,)_{}+ }{16(+1)LD_{}^{2}},_{}^ {}_{}\}.\] (3)

This is the first result that establishes simultaneously these properties under a computationally efficient algorithm, improving a recent work  (see also [42; 67] for related results) that failed toguarantee convergence to CCE. In particular, that earlier work was analyzing \(\), and as it turns out, under a time-invariant learning rate \(\) it is not even known whether \(\) ensures _sublinear_ per-player regret, let alone constant (as in Corollary 4.3). The basic ingredient to this improvement is a new property of \(\), which we explain below. Before we sketch the proof, we note that Item 2 above can be readily strengthened so that the improvement holds for the average welfare of most of the strategies, not just a single one (Remark A.22).

Proof sketch of Theorem 4.1The key step in the proof is showing (in Corollary A.21) that \(\) satisfies the remarkable per-player regret bound \(_{i}^{(T)}-_{t=1}^{T} _{i}(^{(t)})^{2}\), where \(>0\) depends on the approximation error of the fixed points of \(\)--and can be made time-invariant with only an \(O( T)\) per-iteration overhead--and \(>0\). To do this, we crucially rely on a certain property of the Euclidean regularizer (Lemma A.20), which we use in conjunction with the analysis of Farina et al.  who extended the original argument of Piliouras et al.  beyond entropic regularization.

It is worth noting that the above per-player regret bound (Corollary A.21) implies that a player with nonnegative regret will be almost always approximately best responding, a rather singular occurrence in the context of learning in games; this has interesting implications and goes well-beyond what is currently known for \(\). In particular, it is an open question whether Theorem 4.1 holds under \(\).

Next, we shall describe a concrete implication of Theorem 4.1 under a generic condition. To do so, let us denote by \(_{}^{c}\) the price of anarchy in \(\) with respect to the worst-case \(\)-Nash equilibrium (so that \(_{}^{0}_{}\)).

**Condition 4.2**.: _Consider a game \(\) and some game-dependent parameter \(C=C()>0\). There exists an \(_{0}>0\) such that \(_{}^{_{0}}>_{}+ _{0}^{2}C\)._

Naturally, it is always the case that \(_{}_{}\). Further, \(_{}\) is in general strictly smaller since it measures the worst-case welfare over a larger set than \(_{}\) (even broader than outcomes of no-regret learning); Figure 1 in Appendix A.8 further corroborates this premise in a sequence of random normal-form games. Now assuming that \(_{}>_{}\), Condition 4.2 is met if \(_{ 0}_{}^{}=_{ }\), a mild continuity condition (see, for example, the discussion by Roughgarden ).

**Corollary 4.3**.: _Consider a \((,)\)-smooth game \(\) that satisfies Condition 4.2 under some \(_{0}>0\). Then, \(\) after \(TD_{}^{4}}{_{0}^{2}}\) iterations and \(=\) satisfies the following:_

1. _the average correlated distribution of play is an_ \(O()\)_-CCE;_
2. _there is a time_ \(t^{}[\![T]\!]\) _and_ \(C^{}()>0\) _such that_ \((^{(t^{})})(_{}(,)+ _{0}^{2}C^{}())_{}\)_._

A fundamental question that arises from Theorem 4.1 is whether there exists a computationally efficient algorithm that determines a CCE with social welfare at least a \(\) fraction of the optimal welfare.3 In games where \(=\) this is clearly possible; in contrast, while Theorem 4.1 improves over the smoothness bound, it does not always guarantee welfare up to \(\). This is a central question in light of the intractability of Nash equilibria , which has indeed served as a primary critique to the literature quantifying the price of anarchy of Nash equilibria .

Another promising avenue to improving the welfare predicted by the smoothness framework revolves around eliminating certain strategy profiles by arguing that they are reached with negligible probability. For example, in Appendix A.10 we identify an example where iteratively eliminating strictly dominated actions can improve the predictive power of the smoothness framework.

## 5 Further related work

Large gamesThe study of non-cooperative games with many players (_i.e._, large games) has been a classical topic in economic theory , most recently revived in the context of _mean-field games_ (_e.g._, ). Indeed, many traditional motivating scenarios in algorithmic game theory, including markets and Internet routing, often feature a large number of players in practice. In particular, it has emerged that, under certain conditions, equilibria in large games exhibit certain remarkable robustness and stability properties; see, for example, the recent survey of Gradwohl and Kalai , as well as the older treatment of Kalai  on the subject. Furthermore, mechanism design in large games, along with privacy guarantees, is explored in the work of Kearns et al.  (see also ).

Efficiency in large gamesOf particular importance to our work, and specifically the precondition of Theorem 3.1, is the line of work uncovering the by now well-documented phenomenon in economics that large games exhibit, under certain relatively mild assumptions, fully efficient equilibria. Our framework additionally requires that the efficiency of equilibria can be derived via a smoothness argument, in the sense of Roughgarden ; we stress again that efficiency alone is of little use when it comes to equilibrium computation (Proposition 3.7). Fortunately, smoothness has emerged as the canonical paradigm for bounding the price of anarchy (_e.g._, see the survey of Roughgarden et al. ), albeit with some notable exceptions . In particular, Feldman et al.  quantify the price of anarchy in large games via the smoothness framework. They show that in a general combinatorial domain with simultaneous uniform-price auctions, it holds that \(_{n} 1\) with a rate of \(1/\) as long as there is _probabilistic demand_, meaning that every buyer abstains from the auction with a constant probability. Several other papers have studied the price of anarchy in large games . In particular, we highlight the work of Cole and Tao  which, as Feldman et al. , relies on a smoothness argument to establish full efficiency in the limit with a rate of \(1/\) in a Walrasian auction, while asymptotic full efficiency is also shown for Fisher markets under the gross substitutes condition. Further, Carmona et al.  provide sufficient conditions under which equilibria are fully efficient in a class of mean-field games; understanding thus whether our framework has new implications in such games is an interesting direction for the future. We finally point out that many other papers have focused on learning in auctions and markets; see , and references therein.

## 6 Conclusions and future work

In conclusion, we have furnished a new sufficient condition under which a family of no-regret learning algorithms, including optimistic gradient descent (OGD), approaches (weak) Nash equilibria. Our criterion has a natural economic interpretation, being intricately connected with Roughgarden's smoothness framework, and captures other well-studied conditions such as the Minty property. We have also shown that _clairvoyant_ gradient descent attains an improved welfare bound compared to that predicted by the smoothness framework, while ensuring at the same time fast convergence to the set of CCE.

There are many promising directions for future work. First, we have seen that under the condition \(=1\) there exists an algorithm that computes an \(\)-NE in time \((1/)\), leading to a _pseudo_ polynomial-time algorithm (under natural game representations); is there an algorithm that instead runs in time \(((1/))\)?

Convergence to Nash equilibria via computational hardness?Another promising approach for showing convergence to Nash equilibria is by harnessing computational hardness results for the underlying welfare maximization problem. To be specific, we consider the following condition.

**Condition 6.1**.: _Consider a multi-player \((,)\)-smooth game \(\) with \(_{}\) from a class of games \(\) with the polynomial expectation property . For any \(\), computing a joint strategy profile \(_{i=1}^{n}_{i}\) such that \(()_{}_{}+1/ ()\) is \(\)-hard, for any \(()\)._

Indeed, smoothness often circumscribes the welfare of polynomial algorithms, such as combinatorial auctions under XOS valuations--in fact, unconditionally under polynomial communication; see [34, Theorem 1.4] and [102, Appendix A.7]. Now, the role of Condition 6.1 is that (unless \(=\)) a polynomially-bounded algorithm such as OGD--which is efficiently implementable (for games with a polynomial number of actions) under the polynomial expectation property--will have the property that \(_{i=1}^{n}_{i}^{(T)}-1/()\), for any \(\) and \(()\), which in turn leads to the following.

**Theorem 6.2**.: _Consider a class \(\) satisfying Condition 6.1. For any \(\) and \(=1/()\), there is a polynomial-time algorithm for computing an \(\)-Nash equilibrium, unless \(=\)._

By virtue of Corollary 4.3, the same conclusion applies even under the weaker condition that computing a CCE with welfare improving over the smoothness bound is hard; this is related to the hardness result of Barman and Ligett , discussed in the full version of this paper.