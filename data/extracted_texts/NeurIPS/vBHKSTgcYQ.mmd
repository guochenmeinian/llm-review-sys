# An Exploration-by-Optimization Approach

to Best of Both Worlds in Linear Bandits

 Shinji Ito

NEC Corporation, RIKEN AIP

i-shinji@nec.com &Kei Takemura

NEC Corporation

kei_takemura@nec.com

###### Abstract

In this paper, we consider how to construct best-of-both-worlds linear bandit algorithms that achieve nearly optimal performance for both stochastic and adversarial environments. For this purpose, we show that a natural approach referred to as exploration by optimization (Lattimore and Szepesvari, 2020b) works well. Specifically, an algorithm constructed using this approach achieves \(O(d)\)-regret in adversarial environments and \(O( T}{_{}})\)-regret in stochastic environments. Symbols \(d\), \(T\) and \(_{}\) here represent the dimensionality of the action set, the time horizon, and the minimum sub-optimality gap, respectively. We also show that this algorithm has even better theoretical guarantees for important special cases including the multi-armed bandit problem and multitask bandits.

## 1 Introduction

The linear bandit problem (Abernethy et al., 2008) is a fundamental sequential decision-making problem in which a player chooses a \(d\)-dimensional vector \(a_{t}\) from a given _action set_\(^{d}\) and incurs loss \(f_{t}\). The expectation of \(f_{t}\) is assumed to be expressed as an inner product of \(_{t}\) and \(a_{t}\) with an unknown _loss vector_\(_{t}^{d}\) in each round \(t=1,2,,T\). This paper focuses on the (expected) regret defined as \(R_{T}=_{a^{*}}[_{t=1}^{T}_{t},a-a^{*}]\). It is known that the order of achievable regret can vary greatly depending on the structure of the environment that generates losses. For example, in stochastic environments, i.e., if the loss vector \(_{t}\) is fixed at \(^{*}\) for all \(t\), there exists an algorithm that achieves regret of \(O( T)\). By way of contrast, for adversarial environments in which the loss vector changes arbitrarily, the optimal regret bound is \(()\)(Abernethy et al., 2008; Bubeck et al., 2012). While this clearly suggests the importance of choosing an algorithm that is well-suited to the environment, in real-world applications it is often quite difficult, if not virtually impossible, to know the type of environment in advance.

One promising approach to this difficulty is to apply _best-of-both-worlds (BOBW)_ algorithms (Bubeck and Slivkins, 2012) that work nearly optimally both for stochastic and adversarial environments. The first BOBW algorithm for linear bandit problems was proposed by Lee et al. (2021); it achieves \(O(|)})\)-regret in adversarial environments and \(O(c^{*}(T||) T)\)-regret in stochastic environments, where \(c^{*}=c(,^{*})\) represents an instance-dependent quantity characterizing the instance-optimal asymptotic bound for stochastic environments (Lattimore and Szepesvari, 2017). This algorithm by Lee et al. (2021) also has the notable advantage that these regret guarantees hold with high probability. On the other hand, their regret bound for stochastic environments includes an extra \( T\) factor.1 This issue of extra \( T\) factors has been resolved in a recent study by Dann et al. (2023). They have proposed algorithms achieving regret bounds with optimal dependency on \(T\), which are summarized here in Table 1. Their regret bounds for stochastic environments dependon the _minimum sub-optimality gap_\(_{}\). Note that we have \(c^{*} O(d/_{})\)(Lee et al., 2021, Lemma 16) and that this inequality can be arbitrarily loose (Lattimore and Szepesvari, 2017). The study by Dann et al. (2023) involves the proposal of a general reduction technique, which has the remarkable advantage of being applicable to a variety of sequential decision-making problems in addition to linear bandit problems, though it does tend to complicate the structure and implementation of the algorithms.

The main contribution of this paper is to show that BOBW algorithms can be constructed via a (conceptally) simple approach referred to as _exploration by optimization_(EXO) (Lattimore and Szepesvari, 2020; Lattimore and Gyorgy, 2021; Foster et al., 2022). In this approach, we update the _reference distribution_\(q_{t}\) using the exponential weight method and then compute sampling distribution \(p_{t}\) for action \(a_{t}\) (by modifying \(q_{t}\) moderately) and loss estimator \(g_{t}\) by solving an optimization problem so that an upper bound on regret is minimized. As shown by Lattimore and Gyorgy (2021) and Foster et al. (2022), this natural approach has a connection to both the information ratio (Russo and Van Roy, 2014) and the decision-estimation coefficient (Foster et al., 2021), and it achieves nearly optimal worst-case regret bounds for some online decision-making problems. These existing studies on EXO, however, focus on adversarial environments, and not much has yet been shown about the potential of its working for stochastic environments. In this paper, we show that the EXO approach also works for stochastic environments to achieve \(O( T)\)-regret.

The regret bounds with the EXO approach and in existing studies are summarized in Table 1. This paper shows the regret bounds of \(O()\) in adversarial environments and of \(O( d T)\) in stochastic environments, where \(\) is a parameter that depends on \(\) and \(\) is a parameter depending on \(\) and \(^{*}\). Bounds on \(\) and \(\) shown in this paper are summarized in Table 2. For general action sets, the EXO approach reproduces the regret bound shown by Dann et al. (2023, Corollary 7). Further, for some special cases, such as multi-armed bandits and hypercube linear bandits, it achieves improved regret bounds that are tight in stochastic regimes parametrized by the sub-optimality gap \(_{}\).

To show BOBW regret bounds, we consider here the EXO approach combined with the _continuous_ exponential weights method (Hoeven et al., 2018), in which we update reference distributions \(q_{t}\) over a convex set. In our regret analysis of the exponential weight, the ratio of the variance of the sampling distribution \(p_{t}\) to the variance of \(q_{t}\) plays an important role: the larger the former, the better the guarantee obtained. To show \(O( T)\)-regret bounds for stochastic environments, we exploit the fact that \(q_{t}\) is a _log-concave distribution_(Lovasz and Vempala, 2007). Intuitively speaking, as any log-concave distribution has most of its weight concentrated around its mean (see, e.g., (Lovasz and Vempala, 2007, Lemma 5.17)), we may choose sampling distribution \(p_{t}\) so that it has a larger variance than \(q_{t}\) which leads to improved regret bounds.

   Reference & Adversarial & Stochastic \\  Lattimore and Szepesvari (2017) & & \(c^{*} T+o( T)\) \\ Bubeck et al. (2012) & \(O(|)})\) & \(O(c^{*}(T||) T)\) \\ Lee et al. (2021) & \(O(|)})\) & \(O(c^{*}(T||) T)\) \\ Dann et al. (2023, Cor.7) & \(O(T T})\) & \(O( T}{_{}})\) \\ Dann et al. (2023, Cor.12) & \(O(|)})\) & \(O(|) T}{_{}})\) \\
**[This work, Theorem 1]** & \(O()\) & \(O( d T)\) \\   

Table 1: Regret bounds for linear bandits

    & Setting & Action set & \(\) & \(\) \\  Corollary 1 & General & \(^{d}\) & \(O(d)\) & \(O(d/_{})\) \\ Corollary 2 & Multi-armed & \(=_{d}:=\{e_{1},,e_{d}\}\) & \(O(1)\) & \(O(1/_{})\) \\ Corollary 3 & Hypercube & \(=\{0,1\}^{d}\) & \(O(d)\) & \(O(1/_{})\) \\ Corollary 3 & Multitask & \(=_{d_{1}}_{d_{m}}\) & \(O(m)\) & \(O(1/_{})\) \\   

Table 2: Bounds on parameters \(\) and \(\)Our regret analysis uses the _self-bounding technique_(Wei and Luo, 2018; Zimmert and Seldin, 2021) to show regret bounds for stochastic environments. One advantage of this technique is that a regret bound also leads to _stochastic environments with adversarial corruptions_ or _corrupted stochastic environments_(Lykouris et al., 2018; Li et al., 2019). As with existing algorithms based on the self-bounding technique, our algorithm achieves a regret bound of \(O( dT T+)\) for corrupted stochastic environments with the corruption level \(C 0\). More generally, these regret bounds are achieved for environments in _adversarial regimes with self-bounding constraints_(Zimmert and Seldin, 2021), details of which are provided here in Section 3.

Another contribution of this paper is to establish a connection between the EXO approach and the SCRiBLe algorithm (Abernethy et al., 2008, 2012) (and its extensions (Dann et al., 2023; Ito and Takemura, 2023)). The SCRiBLe algorithm uses a follow-the-regularized-leader (FTRL) method with self-concordant barrier regularization to select a point in the action set, rather than a probability distribution. Abernethy et al. (2008) have shown that this approach achieves \(O(T})\)-regret bounds for adversarial environments. Recently, Dann et al. (2023) and Ito and Takemura (2023) have shown that it can also achieve \(O( T}{_{}})\)-regret in stochastic environments, through modification of the sampling distributions and loss estimators. To better understand the connection between these studies and the EXO approach, we propose a variant that we refer to as mean-oriented EXO and which includes EXO with the continuous exponential weight method as a special case. Given this correspondence, existing SCRiBLe-type algorithms can be interpreted as EXO-based methods in which the optimization is roughly solved with approximation ratios \((d)\).

The results of this paper establish a connection between the framework of exploration by optimization and best-of-both-worlds regret guarantees. We would like to emphasize that there is potential to extend this result to a broader class of sequential decision-making beyond linear bandits, such as partial monitoring problems and episodic MDPs. In fact, the EXO framework can be applied to a general model of sequential decision-making with structured observation (Foster et al., 2022) and is known to have a connection to several key concepts in sequential decision-making, such as the Decision-Estimation Coefficients (Foster et al., 2021) and information-directed sampling (Lattimore and Gyorgy, 2021; Russo and Van Roy, 2014), which has been extensively applied to partial monitoring (Lattimore and Szepesvari, 2020) and linear partial monitoring (Kirschner et al., 2020). Building further relationships between these generic frameworks and BOBW regret analysis will be an important topic to work on in the future.

## 2 Related work

For adversarial linear bandit problems, various approaches have been considered, including algorithms based on self-concordant barriers (SCRiBLe) (Abernethy et al., 2008, 2012; Rakhlin and Sridharan, 2013), discrete exponential weights (Geometric Hedge) (Dani et al., 2008; Bubeck et al., 2012; Cesa-Bianchi and Lugosi, 2012), and continuous exponential weights (continuous Geometric Hedge) (Hazan and Karnin, 2016; Bubeck and Eldan, 2019), all of which can be interpreted as methods in the FTRL framework. We can see that continuous exponential weight methods can be interpreted as an FTRL approach with an entropic barrier, a \(d\)-self-concordant barrier (Chewi, 2021), which implies a close relationship between the SCRiBLe algorithm and the continuous Geometric Hedge, as Bubeck and Eldan (2019) have noted. One key difference lies in how the sampling distribution \(p_{t}\) is chosen; the SCRiBLe algorithm employs a sampling scheme supported on the Dikin ellipsoid while the continuous Geometric Hedge designs \(p_{t}\) by mixing the reference distribution \(q_{t}\) with some exploration basis (e.g., a G-optimal design distribution). As a result of this difference, the latter can achieve an \(O(d^{-1/2})\)-times better regret bound than the former. Recent studies by Dann et al. (2023, Theorem 3) and Ito and Takemura (2023) have proposed new sampling schemes based on Dikin ellipsoid sampling to achieve BOBW regret bounds. This paper takes these approaches a step further, determining a sampling distribution itself on the basis of an optimization problem for minimizing regret, which leads to improved performance.

The self-bounding technique (Wei and Luo, 2018; Zimmert and Seldin, 2021) is known as a promising approach for designing and analyzing BOBW algorithms. The greatest impetus for the widespread use of this technique is considered to come from the Tsallis-INF algorithm (Zimmert et al., 2019) for the multi-armed bandit problem. Since the study on Tsallis-INF, FTRL methods with Tsallis-entropy regularizer (with some modification) have been used successfully in a variety of sequential decision-making problems with limited (structured) observation, including combinatorial semi-bandits (Zimmert et al., 2019), bandits with switching costs (Rouyer et al., 2021; Amir et al., 2022), bandits with delayed feedback (Masoudian et al., 2022), online learning with feedback graphs (Erez and Koren, 2021; Rouyer et al., 2022). dueling bandits (Saha and Gaillard, 2022), and episodic MDPs (Jin and Luo, 2020; Jin et al., 2021; Dann et al., 2023a). On the other hand, a few (rather limited number of) studies have constructed BOBW algorithms using regularizer functions other than the Tsallis entropy. For example, Ito (2021) proposed a BOBW multi-armed bandit algorithm by using log-barrier regularization equipped with adaptive learning rates. More recently, Shannon-entropy regularization with some specific adjustment methods for learning rates has been shown to provide BOBW regret guarantees for several problems including online learning with feedback graphs (Ito et al., 2022b), partial monitoring (Tsuchiya et al., 2023), and linear bandits (Kong et al., 2023). It seems that these Shannon-entropy-based methods have the strength of being versatile while these have the weakness that their regret bounds for stochastic environments include extra \(O( T)\)-factors. The approach in this paper may appear to be related to the previous studies using the Shannon entropy (Ito et al., 2022b; Tsuchiya et al., 2023; Kong et al., 2023) as it employs exponential weight methods, but it is more related to the studies using log-barrier regularization (Ito, 2021). In fact, both log barriers and continuous exponential weight can be captured in the category of self-concordant barriers as we will discuss in Sections 5 and 6. These also have in common the type of regret bounds: \(O( T)\) in stochastic environments and \(O()\) in adversarial environments.

## 3 Problem setting

In linear bandit problems, the player is given an _action set_\(^{d}\), which is a closed bounded set of \(d\)-dimensional vectors. Without loss of generality, we assume that \(\) spans all vectors in \(^{d}\). In each round \(t\), the environment chooses a _loss vector_\(_{t}^{d}\) while the player selects an action \(a_{t}\) without knowing \(_{t}\). The player then gets feedback of \(f_{t}[-1,1]\) that is drawn from a distribution \(M_{a_{t}}\), where the series of distributions \(M=(M_{a})_{a}\) is in \(_{_{t}}:=\{(M_{a})_{a}| a, \ _{f M_{a}}[f]=_{t},a\}\). The condition that the support of \(M_{a}\) is included in \([-1,1]\) for all \(a\) implies that \(_{t},a[-1,1]\) holds for all \(a\) and \(t[T]\). In other words, it is assumed that \(_{t}\) for any \(t\), where we define \(^{d}\) by \(=\{^{d} a,\ |\ ,a 1\}\). The performance of the player is evaluated by means of _regret_ defined as

\[R_{T}(a^{*})=[_{t=1}^{T}_{t},a_{t} -_{t=1}^{T}_{t},a^{*}], R_{T}= _{a^{*}}R_{T}(a^{*}).\] (1)

Regimes of environmentsIn _stochastic environments_, \(_{t}\) is assumed to be round-invariant, i.e., there exists \(^{*}\) such that \(_{t}=^{*}\) holds for all \(t\). In _adversarial environments_, \(_{t}\) can be chosen in an adversarial manner depending on the history so far \(((_{s},a_{s}))_{s=1}^{t-1}\). _Stochastic environments with adversarial corruptions_ correspond to an intermediate setting between these two types of environments. These environments are parametrized by the _corruption level_\(C 0\), in which the loss vectors \((_{t})\) are assumed to satisfy \([_{t=1}^{T}_{a}|_{t}- ^{*},a|] C\). To capture these settings of environments, we define an _adversarial regime with a \((^{*},a^{*},C,T)\)-self-bounding constraint_(Zimmert and Seldin, 2021) in which the environments choose losses so that

\[R_{T}(a^{*})[_{t=1}^{T}^{*},a_{t}-a^{* }]-C\] (2)

holds for any algorithms. This regime includes (corrupted) stochastic environments. In fact, any corrupted stochastic environments with the true loss \(^{*}\) and the corruption level \(C 0\) are included in an adversarial regime with a \((^{*},a^{*},C,T)\)-self-bounding constraint. Our regret analysis for (corrupted) stochastic environments relies only on the condition of (2), as in some existing studies (Zimmert and Seldin, 2021; Dann et al., 2023b).

Reduction to convex action setLet \(=()\) represent the convex hull of \(\). We may consider the action set \(\) instead of \(\), i.e., if we can construct an algorithm \(_{}\) for the problem with action set \(\), we can convert it to an algorithm \(_{}\) for the problem with \(\). In fact, for any output \(a_{t}^{}\) of \(_{}\), we can pick \(a_{t}\) so that \([a_{t}|a_{t}^{}]=a_{t}^{}\).2 When defining \(a_{t}\) as the output of \(_{}\), we have \([f_{t}|a_{t}^{}]=[_{t},a_{t} |a_{t}^{}]=_{t},a_{t}^{}\), which means that the expectation of the feedback \(f_{t}\) given \(a_{t}^{}\) is indeed a linear function in \(a_{t}^{}\) and that the regret for \(_{}\) coincides with that for \(_{}\).

## 4 Exploration by optimization

### Algorithm framework and regret analysis

Let \(()\) denote the set of all distributions over \(\). For any distribution \(p(^{d})\), denote \((p)=_{x p}[x{x}^{}]^{d}\), \(H(p)=_{x p}[x{x}^{}]^{d d}\), and \(V(p)=H(p)-(p)(p)^{}^{d d}\). We consider algorithms that choose \(a_{t}\) following a _sampling distribution_\(p_{t}()\) in each round \(t\). To determine \(p_{t}\), we compute an (approximately) unbiased estimator \(g_{t}(x;a_{t},f_{t})\) of the loss function \(_{t},x\). Let \(\) be the set of all estimators \(g\) that are linear in the first variable, i.e., \(=\{g:[-1,1]  h:[-1,1]^{d},\;g(x;a,f)= h(a,f),x\}\). For any distribution \(p()\), let \(^{}(p)\) represent the set of all unbiased estimators:

\[^{}(p)=\{g , x, M_{}, }{}[g(x;a,f)]=,x \}.\] (3)

A typical choice of \(g\) given \(p\) is the estimator defined as follows:

\[g(x;a,f)= h(a,f),x, h(a,f)=f (H(p))^{-1}a,\] (4)

where \(H(p)\) is assumed to be non-singular. This is an unbiased estimator, i.e., \(g\) defined by (4) is an element of \(^{}\). In fact, if \(M_{}\), we have \(_{a p,f M_{a}}[h(a,f)]=_{a p}[(H(p))^{-1}aa^{ }]=(H(p))^{-1}H(p)=\).

Using \(\{g_{s}\}_{s=1}^{t}\), we set a _reference distribution_\(q_{t}\) over \(\) by the continuous exponential weights method as follows: \(q_{t}(x) q_{0}(x)(-_{t}_{s=1}^{t-1}g_{s}(x;a_{s},f_{s}))\), where \(q_{0}\) is an arbitrary initial distribution over \(\) and \(_{t}\) is a learning rate such that \(_{1}_{2}_{T}>0\). We may choose \(_{t}\) depending on the historical actions and observations so far \(((a_{s},f_{s}))_{s=1}^{t-1}\). Let \(D_{}(q^{},q)\) represent the Kullback-Leibler divergence of \(q^{}\) from \(q\). Note that the exponential weights method can be interpreted as a follow-the-regularized-leader approach over \(\) with the regularizer function \(q D_{}(q,a_{0})\), i.e., the distribution \(q_{t}\) is a solution to the following optimization problem:

\[q_{t}*{arg\,min}_{q()}\{ }[_{s=1}^{t-1}g_{s}(x;a_{s},f_{s})] +}\,D_{}(q,q_{0})\}.\] (5)

We may determine the sampling distribution \(p_{t}\) on the basis of \(q_{t}\) in an arbitrary way. The regret is then bounded as follows:

**Lemma 1**.: _For any distribution \(p^{*}()\) such that \(D_{}(p^{*},q_{0})<\), the expected value of the regret \(R_{T}(a^{*})\) for \(a^{*} p^{*}\) is bounded by_

\[[_{t=1}^{T}(_{t},(p_{t})-(q_{t}) +(g_{t},p_{t},q_{t},_{t})+} (_{t}g_{t}(;a_{t},f_{t}),q_{t}))+}(p^{*},q_{0})}{_{T+1}}],\]

_where_ \[(g,p,q,)=_{a^{*},M_{}}\{}{}[ ,x-a^{*}-g(a^{*};a,f)+g(x;a,f)]\},\] \[(g,q)=_{q^{}()} \{}[g(x)]-}{ }[g(x)]-D_{}(q^{},q)\}.\] (6)

This follows from the standard analysis for exponential update methods or FTRL (see, e.g., , Exercise 28.12), and all omitted proofs can be found in the appendix. For the case of the exponential weights, the _stability term_\((g,q)\) can be bounded as

\[(g,q)_{} }[(g(x)-)],(y)=(-y)+y-1.\] (7)The _bias term_\((g,p,q,)\) corresponds to the bias of the estimator \(g(;a,f)\) for the fucntion \(,\), under the condition that \(a p\), and \([f|a]= a,\). We can easily see that \(g^{}(p)\) implies \((g,p,q,)=0\) for all \(q()\) and \(\).

For any reference distribution \(q()\) and learning rate \(>0\), we define \(_{q,}(p,g)\) by

\[_{q,}(p,g)=_{,M_{}}\{ ,(p)-(q)+(g,p,q,)+ }_{a p,f M_{a}}[( g(;a,f),q)]\}.\]

From Lemma 1, the regret is bounded as

\[}_{a^{*} p^{*}}[R_{T}(a^{*})]}[_{t=1}^{T}_{q_{t},_{t}}(p_{t},g_{t})+}\,D_{}(p^{*},q_{0})].\] (8)

In the _exploration-by-optimization (EXO)_ approach (Lattimore and Szepesvari, 2020, 2021), we choose \(g_{t}\) and \(p_{t}\) so that the value of \(_{q_{t},_{t}}(g_{t},p_{t})\) is as small as possible, i.e., we consider the following optimization problem:

\[_{q_{t},_{t}}(p,g) p(),\;g.\] (9)

We denote the optimal value of this problem by \(_{q_{t},_{t}}^{*}\).

**Remark 1**.: In most linear bandit algorithms based on exponential weights (Dani et al., 2008, Bubeck et al., 2012, Cesa-Bianchi and Lugosi, 2012, Hazan and Karnin, 2016, Besbes et al., 2019), \(p_{t}\) is defined as \(q_{t}\) mixed with a small component of _exploration basis_\(_{0}\) and \(g_{t}\) is given by (4) with \(p=p_{t}\). The exploration basis \(_{0}\) here is a distribution over \(\) such that \(G(_{0}):=_{a}a^{}H(_{0})a\) is bounded. It is known that any action set \(^{d}\) admits an exploration basis \(_{0}\) such that \(G(_{0}) d\), which is called the G-optimal design or the John's ellipsoid exploration basis. Given such a \(_{0}\), we set \(p_{t}=(1-_{t})q_{t}+_{t}_{0}\) with some \(_{t}(0,1)\). Under the condition of \(_{t}=(1/d)\) and \(_{t}=(d_{t})\), we have \(_{q_{t},_{t}}(p_{t},g_{t})=O(_{t}+_{t}d)=O(_{t}d)\), which implies that \(_{q_{t},_{t}}^{*}=O(_{t}d)\) holds for any \(q()\). Hence, from (8), if \(D_{}(p^{*},q_{0}) B\), by setting \(_{t}==()\), we obtain \(}_{a^{*} p^{*}}[R_{T}(a^{*})]=O()\).

### Sufficient condition for best-of-both-worlds regret guarantee

In this section, we show that a certain type of upper bounds on \(_{q,}^{*}\) that depend on reference distribution \(q\) will lead to best-of-both-worlds regret bounds. In the following, we set the initial distribution \(q_{0}\) to be a uniform distribution over \(\). Denote \(_{^{*}}(x)=^{*},x-a^{*}\) for any \(^{*}\). The following lemma provides a sufficient condition for best-of-both-worlds regret guarantees:

**Theorem 1**.: _Suppose that there exist \(>0,>0\) and \(_{0}>0\) such that_

\[_{q_{t},}^{*}\{,_{^{*}} ((q_{t}))\}\] (10)

_for all \(t\) and \(_{0}\). Then, an algorithm based on the EXO approach achieves the following regret bounds simultaneously: (i) In adversarial environments, we have \(R_{T}=O(+}).\) (ii) In environments satisfying (2), we have \(R_{T}=O((+})d T+)\)._

In the proof of this theorem, we consider the algorithm that chooses \((p_{t},g_{t})\) to be an optimal solution to (9) and that sets learning rates \(_{t}\) by \(_{t}=\{_{0},^{t-1}_ {s}^{*}_{q_{s},_{s}}^{*}}}\}\). A complete proof is given in the appendix.

**Remark 2**.: In the implementation of the algorithm, we do not necessarily need to solve the optimization problem (9) exactly. In fact, to achieve the regret upper bounds in Theorem 1, it is sufficient to compute \(p_{t},g_{t}\), and \(z_{t}>0\) that satisfy \(_{t}^{-1}_{q_{t},_{t}}(p_{t},g_{t}) z_{t}\{, _{^{*}}((q_{t}))\}\) for some \(>0\) and \(>0\), under the assumption of \(_{t}_{0}\). In this case, setting \(_{t}=\{_{0},/^{t-1}z_{s}}\}\) will work.

### Regret bounds for concrete examples

In this section, we will see how we can bound \(^{*}_{q_{t},_{t}}\). For \(q()\), define \((q),^{}(q) 0\) by

\[(q) =_{p():(p)=(q)}\{H(p)^{-1}  V(q)\},\] (11) \[^{}(q) =_{p():(p)=(q)}\{y>0 d  V(q) y H(p)\}.\] (12)

Note that \((q)^{}(q)\) holds. In fact, from the definition of \(^{}(q)\), there exists \(p()\) satisfying \((p)=(q)\) and \(d V(q)^{}(q) H(p)\). For such a distribution \(p\), we have \(H(p)^{-1} V(q)=(H(p)^{-1/2}V(q)H(p)^{-1/2}) (d^{-1}^{}(q) I)=^{}(q)\), which implies that \((q)^{}(q)\).

Using these, we can construct a bound \(^{*}_{q,}\) as the following lemma provides:

**Lemma 2**.: _Suppose that \(q\) is a log-concave distribution. If \( 1/d\), we then have \(^{*}_{q,}=O((q))\). If \( 1\), we then have \(^{*}_{q,}=O(^{}(q))\)._

Bounds on \(^{*}_{q,}\) in this lemma can be achieved by \(p=+(1-)_{0}\) and \(g\) defined by (4), where \(\) is the minimizer on the right-hand side of (11) or (12), \(_{0}\) is a G-optimal design for \(\), and some mixing weight parameter \((0,1)\). The proof of this lemma is given in the appendix.

General action setWe can provide an upper bound on \((q)\) by exploiting the property of _log-concave distributions_. A distribution \(q()\) is called a log-concave distribution if it has a distribution function that is proportional to \((-h(x))\) for some convex function \(h\). Note that the reference distribution \(q_{t}\) is a log-concave function.

**Lemma 3**.: _Suppose that \(q\) is a log-concave distribution. Let \(^{*}\) be an arbitrary loss vector such that \(a^{*}*{arg\,min}_{a}^{*},a\) is unique. Denote \(_{^{*},}=_{a\{a^{*}\}}_{^ {*}}(a)\). We then have \(^{}(q)=O(d\{1,_{^{*}}((q))/_ {^{*},}\})\)._

We here provide a proof sketch for this lemma. We consider a distribution \(p()\) of \(a\) generated by the following procedure: (i) Pick \(x\) following \(q\). (ii) Let \(\) be an arbitrary distribution over \(\) such that \(()=x\). In other words, compute \(:_{ 0}\) such that \(_{a^{}}(a^{})=1\) and \(_{a^{}}(a^{})a^{}=x\). (iii) Pick \(a\) following \(\). It is then clear that \((p)=(q)\) holds. Further, it follows from the assumption that \(q\) is a log-concave distribution that \((1-p(a^{*}))u^{}V(p)u=(u^{}V(q)u)\) holds for any \(a^{*}\) and any \(u^{d}\{0\}\), which implies \(d V(q) O((1-p(a^{*}))d) V(p) O((1-p(a^{*}))d)H(p)\). Finally, since we have \((1-p(a^{*}))_{^{*}}((p))/_{^{*},}\)- the bound on \(^{}(q)\) in Lemma 3 follows. A full proof can be found in the appendix.

From Lemmas 2 and 3, we obtain the following bounds on \(\) and \(\):

**Corollary 1**.: _For arbitrary action set \(\), the condition (10) in Theorem 1 holds with \(=O(d),=O(d/_{^{*},})\), and \(_{0}=O(1)\)._

Hence, there exists an algorithm that achieves \(R_{T}=O(d)\) in adversarial environments and \(R_{T}=O(}{_{^{*},}} T+}{_{ ^{*},}} T})\) in stochastically constrained adversarial environments.

Multi-armed banditsSuppose a problem setting in which the action set is the canonical bases: \(=_{d}:=\{e_{1},e_{2},,e_{d}\}=\{a\{0,1\}^{d} \|a\|_{1}=1\}\). For this action set, we have the following bound on \((q)\):

**Lemma 4**.: _If \(=_{d}\), for any log-concave distribution \(q\), \((q)\) defined in (11) is bounded as \((q) O(_{i=1}^{d}_{i}(q)(1-_{i}(q))) O( _{1 i d}\{1-_{i}(q)\})\)._

Let \(^{*}\) be an a loss vector such that \(e_{i^{*}}*{arg\,min}_{1 i d}^{*}_{i}\) is unique. We then have \(_{^{*}}()=_{i i^{*}}_{^{*}}(e_{i})_{i} _{^{*},}(1-_{i^{*}})\), which, combined with Lemma 4, implies \((q)_{^{*}}((q))/_{^{*},}\). From this and Lemma 2, we have the following bounds on \(\) and \(\):

**Corollary 2**.: _For the multi-armed bandit problem \(=_{d}\), the condition (10) in Theorem 1 holds with \(=O(1)\), \(=O(1/_{^{*},})\) and \(_{0}=O(1/d)\)._Product-space action setsSuppose that \(^{d}\) is expressed as a product space of \(m\) sets, i.e., \(=^{(1)}^{(2)}^{(m)}\), where \(^{(j)}^{d_{j}}\) for each \(j\) and \(_{j=1}^{m}d_{j}=d\). Denote \(^{(j)}=(^{(j)})\). This setting is referred to as the _multi-task bandit_ problem (Cesa-Bianchi and Lugosi, 2012), in which the player chooses \(a_{t}^{(j)}^{(j)}\) for each \(j\) in parallel, and then gets only a single feedback corresponding to the sum of the losses for all \(m\) actions. For such an action set, probability distribution \(q_{t}\) given by continuous exponential weights can be expressed as a product measure of \(m\) log-concave measures \(q_{t}^{(j)}(^{(j)})\). In fact, if a probability density function is expressed as \(q(x)=c( L,x)\) for some \(c>0\)\(>0\), and \(L=(L^{(1)},,L^{(m)})^{d}\), it can be decomposed as \(q(x)=c(_{j=1}^{m} L^{(j)},x^{(j)})=_{j}^{m}c^{ (j)}( L^{(j)},x^{(j)})=_{j}^{m}q^{(j)}(x^{(j)})\), where \(x^{(j)}^{(j)}\) and \(c^{(j)}=_{x^{(j)}}( L^{(j)},x^{(j)}) x\). This fact leads to the following bounds on \((q)\) and \(_{q,}^{*}\):

**Lemma 5**.: _If \(=^{(1)}^{(m)}\) and \(q()\) can be expressed as a product measure of \(q^{(j)}(^{(j)})\) for \(j=1,,m\), we have \((q)_{j=1}^{m}(q^{(j)})\)._

**Corollary 3**.: _If \(=_{d_{1}}_{d_{m}}\), the condition (10) in Theorem 1 holds with \(=O(m)\), \(=O(1/_{^{*},})\), and \(_{0}=O(1/d)\). If \(=\{0,1\}^{d}\), the condition (10) in Theorem 1 holds with \(=O(d)\), \(=O(1/_{^{*},})\), and \(_{0}=O(1/d)\)._

### Computational complexity

At this time, it is not known if there is an efficient algorithm to solve the optimization problems (9). However, it is worth noting that the optimization problem corresponding to the right-hand side of (11) is a convex optimization problem. Further, as the minimum can be achieved by \(p()\), we can reduce the problem into a convex optimization problem over \(||\)-dimensional space, which can be solved efficiently if \(||\) is not very large.

Note that it is not always necessary to solve the optimization problem (9) exactly, as mentioned in Remark 2. For example, we can implement an algorithm achieving regret bounds of Corollary 1 given a separation oracle (or equivalently, a linear optimization oracle) for \(cA\), without solving (9) exactly. Indeed, to achieve the regret bounds of Corollary 1, it suffices to find \(p\) and \(g\) such that \(_{q_{t},_{t}}(p,g)\) is bounded by the RHS of (10). The construction of such \(p\) and \(g\) is provided in the proof of Lemmas 2 and 3, which can be performed using a separation oracle for \(\). In fact, we can obtain samples from \(p\) by using the techniques for log-concave sampling (e.g., (Lovasz and Vempala, 2007)) and for computing convex combination expression (cf. Caratheodory's theorem for convex hull and (Schrijver, 1998, Corollary 14.1gL)). However, the analysis of log-concave sampling and calculations of \(H(p)\) (which is required for constructing \(g\)) including consideration of calculation errors can be highly complicated, and the computational cost can be very large, although on the order of polynomials (e.g., (Hazan and Karnin, 2016, Corollary 6.2)).

## 5 Mean-oriented FTRL and SCRiBLe

For linear bandit problems, we have an alternative approach that we referred to as _mean-oriented FTRL_, in which we compute a _reference mean_\(w_{t}\) given as follows:

\[w_{t}*{arg\,min}_{w}\{(_{s=1}^{t-1 }h_{s}(a_{s},f_{s}),w)+}(w)\},\] (13)

where \(h_{s}(a_{s},f_{s})\) is an estimator of \(_{s}\) and \(\) is a convex regularization function over \(\) such that \(_{x}(x)=0\). Let \(D_{}(x^{},x)\) denote the Bregman divergence associated with \(\): \(D_{}(x^{},x)=(x^{})-(x)-(x),x^{}-x\). This approach ensures the following regret bound:

**Lemma 6**.: _For any point \(x^{*}\), we have_

\[R_{T}(x^{*}) [_{t=1}^{T}(_{t},(p_{t}) -w_{t}+(h_{t},p_{t},w_{t},_{t})+_{ }(_{t}h_{t}(a_{t},f_{t}),w_{t})}{_{t}})+)}{ _{T+1}}],\] \[(h,p,w,)=_{a^{*} ,M_{t}}\{}{}[-h(a,f),w-a^{*}]\},\] \[_{}(,w) =_{w^{}}\{,w- w^{}-D_{}(w^{},w)\}.\] (14)

One example of linear bandit algorithms based on mean-oriented FTRL is the SCRiBLe algorithm (Abernethy et al., 2008, 2012), which employs _self-concordant barriers_ as regularizer functions.

We can also consider an EXO approach for mean-oriented FTRL. For any reference point \(w\) and learning rate \(>0\), define

\[_{w,}(p,h)=_{,M_{t}}\{ ,(p)-w+(h,p,w,)+ {a p,f M_{a}}{}[_{}( h(a,f),q)]\}\]

and \(^{*}_{w,}=_{p(),:^{d}}_{w,}(p,)\).

This achieves the following BOBW regret bounds, under assumptions on \(^{*}_{w,}\):

**Lemma 7**.: _Suppose that \(\) is a \(\)-self-concordant barrier. Suppose that there exist \(>0,>0\) and \(_{0}>0\) such that_

\[^{*}_{w_{t},}\{,_{^{*}}( w_{t})\}\] (15)

_for all \(t\) and \(_{0}\). Then an algorithm based on the EXO approach achieves the following regret bounds simultaneously: (i) In adversarial environments, we have \(R_{T}=O(+}).\) (ii) In environments satisfying (2), we have \(R_{T}(a^{*})=O((+}) T+).\)_

### Connection to the SCRiBLe algorithm and variants of it

Abernethy et al. (2008, 2012) have proposed a sampling scheme \(w_{t} p_{t}\) supported on the Dikin ellipsoid, which leads to \(_{w_{t},_{t}}(p_{t},h_{t})=O(_{t}d^{2})\) with an appropriately defined unbiased estimator \(h_{t}\). This algorithm referred to as SCRiBLe hence achieves \(R_{T}=O( T T})\), the adversarial regret bound in Lemma 7 with \(=O(d^{2})\). Recent studies by Dann et al. (2023, Theorem 3) and Ito and Takemura (2023) have shown that a modified sampling scheme based on the Dikin ellipsoid sampling achieves \(_{w_{t},_{t}}(p_{t},h_{t})=O(_{t}d^{2}\{1,_ {^{*}}(w_{t})/_{^{*},}\})\). Consequently, these modified algorithms achieve BOBW regret bounds in Lemma 7 with \(=O(d^{2})\) and \(=O(d^{2}/_{^{*},})\). These results suggest that the following lemma holds:

**Lemma 8** (Dann et al., 2023, Ito and Takemura, 2023).: _If \(\) is a self-concordant barrier, (15) holds with \(=O(d^{2})\), \(=O(d^{2}/_{^{*},})\) and \(_{0}=O(1/d)\)._

### Connection to EXO with continuous exponential weights

The entropic barrier is a \(\)-self-concordant barrier with \( d\)(Chewi, 2021), for which a definition and properties are given, e.g., in (Bubeck and Eldan, 2019). We can see that mean-oriented EXO with entropic-barrier regularization coincides with the EXO approach with exponential weights. In fact, if we compute \(w_{t}\) using (13) with \(\) the entropic barrier, from the definition of the entropic barrier, \(w_{t}\) is equal to the mean of the distribution \(q_{t}\) given by the continuous exponential weights. Given this correspondence, we can see that bounds on \(^{*}_{w_{t},}\) follow immediately from Corollary 1:

**Lemma 9**.: _If \(\) is the entropic barrier for \(\), (15) holds with \(=O(d)\), \(=O(d/_{^{*},})\) and \(_{0}=O(1)\)._

It is worth noting that these bounds are \(O(1/d)\)-times better than the results for Lemma 8 that follow from previous studies.

## 6 Limitations and future work

A limitation of this work is that the regret bounds for (corrupted) stochastic environments require the assumption that the optimal arm \(a^{*}*{arg\,min}_{a}<^{*},a>\) is unique. While this assumption is common in analyses of best-of-both-worlds algorithms based on the self-bounding technique (Zimmert and Seldin, 2021; Dam et al., 2023b), a limited number of studies on the multi-armed bandit problem (Ito, 2021; Jin et al., 2023) have removed this uniqueness assumption by careful analyses of regret bounds for follow-the-regularized-leader methods. As these analyses rely on structures specific to the multi-armed bandit problems, extending them to other problem settings does not seem trivial. Extending this analysis technique to linear bandits and removing the uniqueness assumptions will be an important future challenge.

Another future direction is to work toward a tighter regret bound of \(O(c^{*} T)\) in stochastic environments while satisfying BOBW regret guarantee, where \(c^{*}\) is an instance-dependent quantity that characterizes the optimal asymptotic bound (see, e.g., (Lattimore and Szepesvari, 2017; Corollary 2)). As \(c^{*} O(d/_{})\)(Lee et al., 2021, Lemma 16.) and the gap between these two quantities can be arbitrarily large (Lattimore and Szepesvari, 2017, Example 4), our bounds depending on \(1/_{}\) can be far from tight. In the multi-armed bandit problem, a special case of linear bandits, the quantity \(c^{*}\) can be expressed as \(c^{*}=_{a:_{^{*}}(a)>2}} (a)}\), and known FTRL-type BOBW algorithms with \(O(c^{*} T)\)-regret bounds employs the Tsallis entropy with \((1/)\)-learning rates: \(_{t}(w)=-_{i=1}^{d}}\)(Zimmert and Seldin, 2021) or logarithmic barriers with entry-wise adaptive learning rates: \(_{t}(w)=-_{i=1}^{d}} w_{i}\)(Ito, 2021; Ito et al., 2022). The latter would be more closely related to our results in this paper as logarithmic barriers are examples of self-concordant barriers. A major difference between the approach in this paper and the one by Ito (2021) is that the former only considers regularizer functions expressed as a scalar multiple of a fixed function, while the latter even changes the _shape_ of regularizer functions for each round. Such a more flexible adaptive regularization may be necessary for pursuing tighter regret bounds. Another promising approach is to extend the Tsallis-entropy approach to linear bandits while the connection between the Tsallis-INF (Zimmert and Seldin, 2021) and this paper may be somewhat tenuous as the Tsallis entropy is not a self-concordant barrier.