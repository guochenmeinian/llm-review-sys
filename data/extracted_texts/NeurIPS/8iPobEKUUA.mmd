# Efficient Minimum Bayes Risk Decoding using Low-Rank Matrix Completion Algorithms

Firas Trabelsi

Google

first@google.com &David Vilar

Google

vilar@google.com &Mara Finkelstein

Google

marafin@google.com &Markus Freitag

Google

freitag@google.com

###### Abstract

Minimum Bayes Risk (MBR) decoding is a powerful decoding strategy widely used for text generation tasks, but its quadratic computational complexity limits its practical application. This paper presents a novel approach for approximating MBR decoding using matrix completion techniques, focusing on the task of machine translation. We formulate MBR decoding as a matrix completion problem, where the utility metric scores between candidate hypotheses and pseudo-reference translations form a low-rank matrix. First, we empirically show that the scores matrices indeed have a low-rank structure. Then, we exploit this by only computing a random subset of the scores and efficiently recover the missing entries in the matrix by applying the Alternating Least Squares (ALS) algorithm, thereby enabling a fast approximation of the MBR decoding process. Our experimental results on machine translation tasks demonstrate that the proposed method requires 1/16 utility metric computations compared to vanilla MBR decoding while achieving equal translation quality measured by Comet22 on the WMT2 dataset (en \(\)de, en \(\)ru). We also benchmark our method against other approximation methods and we show gains in quality when comparing to them.

## 1 Introduction

The generation process in most conditional natural language processing tasks is usually guided by the maximum-a-posteriori (MAP) rule: given an input \(x\), generate the output \(\) that maximizes the posterior probability distribution: \(=*{argmax}_{y}p(y|x)\). It can be shown that MAP decoding is optimal under a 0-1 loss criterion. However for more nuanced tasks, where different outputs can be considered correct, and the quality of the output is not just a binary decision between "right" and "wrong", MAP decoding is no longer optimal. Neural Machine Translation (NMT) is a prominent example of these types of tasks. For NMT, a system is trained to generate a sentence in a target language given a source sentence in another language. For a given sentence, there exists a variety of possible translations, each of which has a different quality level. Eikema and Aziz (2020) demonstrated that MAP decoding methods are suboptimal for NMT, and showed that other generation strategies may be preferred. Furthermore, NMT models often assign human translations lower probabilities than their own beam search outputs, due to model calibration issues (Ott et al., 2018; Freitag et al., 2020).

As an alternative, Eikema and Aziz (2020, 2022) applied MBR decoding for NMT models. MBR decoding follows a two-step approach, where a model is used to generate a list of candidate translations and a list of pseudo-references (which may be the same as the list of candidates). The candidatesare then scored with a performance metric using the pseudo-references as an approximation of the true references, and the candidate with the maximum expected quality (or equivalently minimum risk) is then selected. In contrast to MAP decoding, MBR decoding is not designed to generate the translation with the highest estimated model probability; instead it aims to directly optimize a utility function. Subsequent research conducted by Freitag et al. (2022a) showed that MBR decoding with _neural_ utility metrics leads to significant improvements over beam search decoding. However, MBR is computationally expensive, with a time complexity of \(O(N^{2})\) for a candidate list containing \(N\) samples and \(N\) pseudo-references (usually the two lists coincide). According to Freitag et al. (2022a), ideally \(N\) ranges between \(100\) and \(1\,000\), which involves thousands to millions of utility function computations. Note than when using neural metrics, each of the \(O(N^{2})\) "computation steps" is itself expensive, requiring a forward pass through a large neural network.

In this work, we propose to reduce the number of metric computations by scoring only a subset of candidate-pseudo-reference pairs. We then proceed to use a matrix completion algorithm (ALS in our case) to estimate the remaining utility scores. For such completion algorithms to work, the full matrix has to fulfill some conditions, specifically to be a low-rank matrix. We empirically show that this is indeed the case for the utility matrices for MBR decoding. Intuitively, this makes sense: akin to recommendation systems, where similar users are expected to have similar book or movie preferences, similar pseudo-references are expected to have similar "translation preferences". Experimental results show the effectiveness of our method, where the performance of the full MBR algorithm can be matched with only a fraction of the computational cost. Compared to other approaches that also seek to reduce the number of computations, our method does not compromise translation quality, as confirmed by human evaluation.

Our scientific contributions are as follows:

1. We empirically show that the utility matrices for MBR decoding are low-rank.
2. We apply ALS to a subset of scores to approximate the full MBR matrix.
3. We show that using our method we can reduce the number of computations by a factor of 16, while maintaining the same translation quality level.

## 2 Related Work

While MT research has traditionally relied on MAP decoding or generating \(k\)-best lists through beam search for MBR decoding, Eikema and Aziz (2020) proposed an approximation of MBR decoding via unbiased sampling. Their method aims to address the limitations of MAP decoding (Eikema and Aziz, 2020; Muller and Sennrich, 2021; Eikema and Aziz, 2022) by demonstrating that samples drawn from the NMT model align more faithfully with training data statistics when compared to beam

Figure 1: **PMBR decoding only requires a subset of the utility computations to approximate the output of MBR decoding.** The method approximates the unknown values by running a matrix completion algorithm which exploits the low-rank nature of the MBR matrix. Once the full matrix is recovered, the method behaves similar to the vanilla MBR decoding method where the hypothesis with the highest average score is selected.

search. Freitag et al. (2022a) showed that using neural metrics results in significant improvements in translation quality. Freitag et al. (2023a) reported that the choice of sampling approach is important, and epsilon sampling (Hewitt et al., 2022) is ideal for MBR decoding and reranking.

While the improvements in translation quality afforded by MBR are widely acknowledged, its high computational cost limits its application in practice. Different approaches have been proposed to speed up MBR computation. Eikema and Aziz (2022) propose to decouple the candidate and pseudo-reference lists to allow for different sizes, and propose a coarse-to-fine refinement of the hypothesis space. Cheng and Vlachos (2023) speed up MBR decoding by gradually increasing the number of samples used to estimate the utility, while additionally pruning the hypothesis space. Jinnai and Ariu (2024) formulate MBR as a model identification problem, and apply approximate algorithms developed on this problem. Vamvas and Sennrich (2024) aggregate the set of pseudo-references, allowing for just one utility computation per candidate. This greatly accelerates the decoding process, but the utility metric needs to fulfill certain conditions to be applicable. Finkelstein et al. (2024) use MBR decoding in a knowledge-distillation framework to simulate MBR decoding with single-pass search. Tomani et al. (2024) train quality-aware translation models in order to reduce the size of the candidate list. Similar in spirit to MBR decoding, QE-rescoring approaches (Fernandes et al., 2022) also directly optimize a utility function, with linear-time cost.

Low-Rank Matrix completion is an active area of research and multiple algorithms have been developed to perform it. Nguyen et al. (2019) is an extensive survey for such methods. Some of the popular algorithms are: Singular Value Thresholding (Cai et al., 2008), Bayesian Probabilistic Matrix Factorization (Akulwar and Pardeshi, 2016), Maximum Margin Matrix Factorization (Srebro et al., 2004) and Alternating Least Squares (Zachariah et al., 2012), which is the one we chose for this work. To the best of our knowledge, this work is the first one to apply matrix completion algorithms for completing a partial MBR score matrix.

## 3 Preliminaries

We are given an NMT model \(P_{}(y|x)\) which serves to estimate the probability of a hypothesis segment \(y\), given a source segment \(x\), with \(\) being the learned parameters of the neural network. MAP decoding involves searching for the most probable translation under \(P_{}(y|x)\). However, determining the hypothesis with the maximum probability is computationally intractable due to the expansive and combinatorially complex search space. Consequently, approximations like beam search (Graves, 2012; Sutskever et al., 2014) are often employed.

If we want to generate diverse hypotheses, e.g. in generative tasks where creativity is desired instead of selecting the candidate with the highest probability (or an approximation thereof), we sample the output sentence following the probability distribution defined by the model. For NMT, this approach is used for generating a list of candidate translations. Specifically, epsilon sampling, as outlined by Hewitt et al. (2022), has emerged as the leading sampling technique for MBR. It was shown by Freitag et al. (2023a) to outperform other methods such as ancestral, top-\(k\) or nucleus sampling (Holtzman et al., 2020). Epsilon sampling prunes away any token with a probability lower than a threshold \(\), thereby guaranteeing that each token within a sample is allocated a fair probability mass.

### Minimum Bayes Risk Decoding

In MBR decoding (Bickel and Doksum, 1977; Berger, 1985), given a set of candidate hypotheses \(\), the goal is to select the optimal hypothesis based on its expected utility, measured by a function \(u\), with respect to the distribution over human references within the space of all references \(\).

Since the true distribution remains unknown, we resort to sampling from the model instead, which relies on the assumption that the model provides a reliable approximation for the true underlying distribution over human translations. Furthermore, the integration over the vast space of all possible references \(\) is computationally intractable. Therefore, MBR adopts a finite sample estimate by sampling a set of pseudo-references \(\) from \(P_{}(|x)\). This approximation can be expressed as:

\[h^{}=*{argmax}_{h}|}_{r}u(h,r)\] (1)Usual practice is to set \(=\), i.e. the same set of model hypotheses serves both as the candidate list \(\) as well as the pseudo-reference list \(\). The computational time complexity of MBR decoding is \(O(N^{2})\) with \(N\) the size of the candidate list.

Note that this quadratic expression refers to _each sentence_ to translate, i.e. for a corpus of size \(S\), the total cost will be \(O(S N^{2})\). Also there is a hidden (multiplicative) constant, namely the cost of the computation of the utility function. For surface level metrics (e.g. Bleu, ChrF), this cost is negligible, but for neural metrics it involves computing the forward pass of a large neural network. Therefore, any reduction in the number of metric computations has an important effect on the total running cost.

### Low-Rank Matrix Completion

Low-Rank Matrix Completion is a fundamental problem in machine learning and data analysis with popular application such as _Collaborative Filtering_(Rennie and Srebro, 2005) and _Image Denoising_(Candes and Recht, 2008). The goal of matrix completion is to estimate the missing entries of a partially observed matrix, under the assumption that the underlying matrix is low-rank. This assumption implies that the matrix can be well-approximated by a product of two smaller matrices, capturing the latent factors that explain the observed data. Candes and Recht (2008) proved that perfect approximations can be achieved if the number of observed entries is larger than \(CN^{1.2}r(N)\) for some positive numerical constant \(C\), for most \(N N\) matrices of rank \(r\) with very high probability.

One simple and efficient algorithm is Alternating Least Squares (ALS) (Zachariah et al., 2012). To recover any matrix \(M\), the algorithm approximates it by two smaller matrices \(M X^{T}Y\) and then minimizes the following equation given the observed entries.

\[_{X,Y}_{m_{ij}}(m_{ij}-x_{i}^{T}y_{j})^{2}+ (_{i}||x_{i}||^{2}+_{j}||y_{j}||^{2})\] (2)

The algorithm achieves this by alternatively solving for \(X\) and \(Y\) as shown in Algorithm 1 The algorithm has three hyperparameters: \(\) a regularization term, \(r\) the second dimension of the smaller matrices and \(n\) the number of alternating steps performed. The main motivation for picking this algorithm in our approach is its simple implementation.

```
0:\(\), \(r\) and \(N\)
1: Initialize \(X\), \(Y\) with shapes \(N r\) and \(r N\)
2:repeat
3:for\(i=1 n\)do
4:\(x_{i}=(_{m_{ij} m_{i}}\), \(y_{j}y_{j}^{}+ I_{k})^{-1}_{m_{ij} m_{i*}}m_{ij}y_{r}\)
5:endfor
6:for\(j=1 n\)do
7:\(y_{j}=(_{m_{ij} m_{*j}}x_{r}x_{r}^{}+ I_{k})^{-1 }_{m_{ij} m_{*j}}m_{ij}x_{i}\)
8:endfor
9:until convergence ```

**Algorithm 1** ALS for Matrix Completion

## 4 MBR Matrix

### Definition of MBR matrix

Given a source sentence, we use an NMT model to generate a set \(\) of hypotheses such that \(||=N\). As explained in the preliminaries section, the MBR method uses two different sets of hypotheses and pseudo-references, but in practice we use the same set of samples for both \(\) and \(\). The pairwise scores for all hypotheses in \(\) gives an \(N N\) matrix \(M\) such that \(M[i,j]=U(h_{i},h_{j})\) for all \((h_{i},h_{j})\) and a utility metric \(U\) that computes some similarity between two hypotheses.

With this matrix formulation, MBR decoding reduces to picking the row with the highest average (since each row maps to one sample in the hypotheses list).

### MBR matrices are low rank

Intuitively, the values within the MBR matrix are highly correlated, since by definition each value \(M[i,j]\) is computing a similarity score between two hypotheses given a utility metric. This is a key assumption that our work is built on since low-rank matrices have theoretical bounds on the number of entries required to recover the full matrix (Candes and Recht, 2008).

We verify this assumption empirically. We generated 1024 samples for each example in the WMT 2022 en \(\)de and en \(\)ru datasets. We then generated the \(N N\) matrices for different values of \(N\{64,128\}\) by only considering a random subset of the samples using two different utility metrics: MetricX (Freitag et al., 2022b) and ChrF (Popovic, 2015). We then perform singular value decomposition and look at the distribution of the singular values, shown in Figure 2 and Table 1. We observe that across utility metrics and matrix dimensions, \(_{1}_{2}\). On average across datasets we have \(_{2}/_{1}<0.05\), this means that most of the information within the matrix can be captured by a single dominant direction or component and thus can be approximated by a rank-1 matrix.

## 5 The PMBR Method

We propose an approximation method for MBR decoding that leverages the low-rank structure of the MBR matrix. The procedure is shown in Algorithm 2. Given an NMT model, we start by generating a set of hypotheses \(\) similar to the vanilla MBR method. Then, instead of computing all the pairwise scores in the utility matrix, we only compute a random subset of the scores that we denote with \(\). The size of \(\) depends on the _computation budget_ available. We define the budget as the ratio of

    &  &  \\   &  &  &  &  \\   & \(_{1}\) & \(_{2}\) & \(_{3}\) & \(_{1}\) & \(_{2}\) & \(_{3}\) & \(_{1}\) & \(_{2}\) & \(_{3}\) & \(_{1}\) & \(_{2}\) & \(_{3}\) \\  English\(\)German & 45.7 & 2.1 & 1.0 & 39.9 & 2.4 & 1.4 & 91.6 & 3.8 & 1.7 & 76.2 & 4.2 & 2.1 \\ German\(\)English & 47.4 & 2.1 & 1.4 & 47.5 & 1.5 & 1.4 & 94.6 & 3.7 & 2.1 & 93.7 & 3.0 & 2.0 \\ English\(\)Russian & 47.7 & 2.0 & 1.1 & 36.1 & 1.9 & 1.3 & 95.4 & 3.6 & 1.7 & 76.8 & 3.4 & 2.1 \\ Russian\(\)English & 46.1 & 2.2 & 1.0 & 40.5 & 2.8 & 1.3 & 92.0 & 4.1 & 1.7 & 81.8 & 5.7 & 2.1 \\  Average & 46.7 & 2.1 & 1.1 & 41.0 & 2.15 & 1.35 & 93.4 & 3.8 & 1.8 & 82.1 & 4.1 & 2.1 \\   

Table 1: Summary of the first three singular values of MBR matrices for the MetricX and chrF utility functions, with two different sizes and four different language pairs

Figure 2: Plot the singular values of an example 124x124 MBR matrix using logscale. We observe a sharp drop after the first singular value for the two utility metrics indicating that the matrix is rank-1.

computations performed with respect to the total amount of computations to compute the full matrix. Thus for any given budget \(1/r\), we end up with \(N^{2}/r\) entries observed in the matrix. The next step is to apply ALS on \(M[]\) as described in Algorithm 1, where \(M[]\) denotes the matrix of size \(N N\) where only the entries of \(\) are non-null. Finally, with all the pairwise scores recovered, we perform vanilla MBR decoding. We call this procedure PMBR for _Probabilistic MBR_ decoding.

```
0: List of hypotheses \(H\), reduction ratio \(r(0,1)\)
1:\(N|H|\)
2:\(S[|N|^{2} r]\)\(\) Number of utility computations
3:\(\)\(\) Sample \(S\) coordinate pairs \((i,j)\) from \(N N\)
4:\(M^{|N||N|}\)\(\) Initialize empty matrix
5:for\((i,j)\)do
6:\(M_{ij} U(i,j)\)
7:endfor
8:\(M\) ALS\((M,)\)
9:returnvanilla_MBR(M) ```

**Algorithm 2** PMBR: MBR Approximation using ALS

Time ComplexityThe time complexity of this algorithm is dominated by the utility metrics computations. The utility metrics are deep neural networks that require a number of floating-point operations in the order of millions while ALS requires only a few hundred operations. For reference, 30 steps of the ALS algorithm with \(r=10\) running on a CPU takes on average 0.2 seconds to run while the MetricX inference takes 3.4 seconds on a TPUv4 platform. Thus, the savings in run time achieved by our approximation is close to proportional to the savings in number of utility computations. Note that this analysis focuses only on the second stage of MBR decoding, i.e. we do not take the cost of generating the hypotheses into account.

## 6 Experimental Setup

### Metrics

We use MetricX (Juraska et al., 2023) as the utility function for all variants of MBR decoding as it has been shown that neural fine-tuned metrics outperform word-overlap metrics like Bleu (Papineni et al., 2002) and ChrF (Popovic, 2015) for MBR decoding (Freitag et al., 2022). MetricX is an extension of Bleurt(Sellam et al., 2020), showing higher correlation with human judgment (Freitag et al., 2023) and has been designed to also work on multi-sentence segments (Deutsch et al., 2023) and not only sentences in isolation. In addition, we report Comet22 (Rei et al., 2020, 2022) scores as there is a risk of overfitting (Amrhein and Sennrich, 2022) on MetricX. In addition, for one selected experiment we conducted expert-based human evaluations using MQM (Freitag et al., 2021), a human evaluation scheme centered on marking errors present in the translations. We report results by varying the budget available to the MBR methods. For each budget, we randomly sample from the full MBR matrix, and report the average results of 1000 trials.

### Datasets and Model

We run experiments using the WMT 2022 test sets for English\(\)German (en\(\)de) and English\(\)Russian (en\(\)ru). The official WMT test sets (Kocmi et al., 2022) are split into sentences but come with document information. We constructed multi-sentence (paragraph) level test sets with the following method: for each document, we concatenate sentences together as long as we do not exceed 500 sentence piece model (SPM) tokens (given the MetricX SPM model). We respect sentence boundaries and do not truncate sentences. In WMT22, there are four different domains. Some domains lack document context, so segments remain as single sentences, even within multi-sentence test sets. Test data statistics can be seen in Appendix 5. We use PaLM8B (Chowdhery et al., 2022) as translation model and sample 1024 examples for each sentence using epsilon sampling with \(=0.02\)(Freitag et al., 2023) and using 3-shot prompting with examples taken from the FLORES corpus (Guzman et al., 2019)

### Decoding Methods

We compare our approximation PMBR against three other decoding methods. To enable a fair comparison, we adapt each method so that the number of utility function computations is the same for each method. (Recall that for a given budget \(1/r\), we only observe \(N^{2}/r\) entries in the matrix when performing PMBR.) We compare PMBR with the following methods:

* FMBR: This is the full MBR method. This is the only method that is not affected by the budget i.e the full matrix is observed.
* \(N\!\!K\): This method was proposed by Eikema and Aziz (2022) and works by shrinking the pseudo-references list size. For a budget \(1/r\) the MBR matrix gets reduced to an \(N K\) matrix with \(K=N/r\). The \(K\) pseudo-references are randomly sampled.
* \(S\!\!\!S\): This method corresponds to FMBR, but reduces the total size of the utility matrix to a size of \(S S\), where the total number of entries corresponds to the available budget, i.e. \(S=/r}\). The \(S\) examples are randomly sampled.

### Hyperparameter Tuning

The ALS algorithm has three hyperparameters \(\), \(n\) and \(r\) as described in Algorithm 1. We perform a grid search to optimize these hyperparameters, setting our loss function to be the accuracy with respect to the vanilla MBR method. Concretely, for each example sentence we rank all samples by running the vanilla MBR. Let us denote with \(h_{}\) the hypothesis selected by the full MBR method, and let \(}}(h_{})\) the rank the position of \(h_{}\) after ordering the hypotheses according to the scores predicted by PMBR. The loss function is then just the sum of \(}}(h_{})\) for all the hypotheses in a subset of the data. We minimize this loss per language pair on 10 examples that we hold from the data generated with the WMT 2022 datasets with this search space \(\{\{0.1,0.15,0.2\}\}\{r\{5,6, 14,15\}\}\{n\{10,11, ,29,30\}\}\)

## 7 Results

The main experimental results are summarized in Table 2 and Table 3. In Table 2, we fix \(N=128\) and we study the behaviour of each approximation method by limiting their budget to a fraction of the

Figure 3: We scored WMT22 DeEn dataset 1000 times for each budget available. Each scoring picks without replacement 128 samples from the 1024 samples available for each sentence. The highlighted area shows the standard deviation of the scores.

    & budget &  &  &  &  &  \\   & & C & X & C & X & C & X & C & X & C & X \\   & FMBR & 83.52 & 77.01 & 83.52 & 77.01 & 83.52 & 77.02 & 83.52 & 77.02 & 83.52 & 77.01 \\  &  & PMBR & **83.53** & 75.94 & **83.63** & **76.50** & **83.63** & **76.81** & **83.60** & **76.96** & **83.56** & **77.01** \\  & & \(N K\) & 82.18 & 74.96 & 82.90 & 75.99 & 83.28 & 76.59 & 83.45 & 76.84 & 83.48 & 76.93 \\  & & \(S S\) & 83.39 & **76.12** & 83.52 & 76.43 & 83.57 & 76.63 & 83.59 & 76.79 & 83.57 & 76.91 \\   & FMBR & 79.97 & 73.52 & 79.97 & 73.52 & 79.97 & 73.52 & 79.97 & 73.52 & 79.97 & 73.52 \\  &  & PMBR & **80.00** & **73.32** & **80.01** & **73.44** & **80.03** & **73.52** & **80.02** & **73.53** & **80.00** & **73.54** \\  & & \(N K\) & 79.75 & 72.93 & 79.87 & 73.24 & 79.92 & 73.39 & 79.95 & 73.45 & 79.96 & 73.49 \\  & & \(S S\) & 79.87 & 73.17 & 79.91 & 73.29 & 79.94 & 73.37 & 79.96 & 73.44 & 79.97 & 73.49 \\   & FMBR & 83.52 & 77.01 & 83.52 & 77.01 & 83.52 & 77.02 & 83.52 & 77.02 & 83.52 & 77.01 \\  &  & PMBR & **83.53** & 75.94 & **83.63** & **76.50** & **83.63** & **76.81** & **83.60** & **76.96** & **83.56** & **77.01** \\  & & \(N K\) & 82.18 & 74.96 & 82.90 & 75.99 & 83.28 & 76.59 & 83.45 & 76.84 & 83.48 & 76.93 \\  &  & FMBR & **83.39** & **76.12** & 83.52 & 76.43 & 83.57 & 76.63 & 83.59 & 76.79 & 83.57 & 76.91 \\   & FMBR & 79.17 & 75.57 & 79.17 & 75.57 & 79.17 & 75.57 & 79.17 & 75.57 & 79.17 & 75.57 \\  & & \(P\)MBR & **79.15** & **75.15** & **79.23** & **75.40** & **79.22** & **75.48** & **79.19** & 75.51 & **79.18** & **75.56** \\  &  & N\( K\) & 78.71 & 74.68 & 78.99 & 75.21 & 79.08 & 75.41 & 79.14 & **75.52** & 79.15 & 75.54 \\  & & \(S S\) & 78.98 & 75.01 & 79.06 & 75.20 & 79.10 & 75.34 & 79.13 & 75.43 & 79.15 & 75.52 \\   

Table 2: Results on the four translation directions on the WMT22 data. Each number (except for FMBR) is the average of 1 000 runs with different random values taken from the full MBR matrix. \(N\) is set to 128, and the budget is allocated according to the description in Section 6.3. ‘C’ denotes Comet22 scores and ‘X’ MetricX scores.

    & budget &  &  &  &  &  \\   & & C & X & C & X & C & X & C & X & C & X \\   & FMBR & NA & NA & 79.91 & 73.29 & 79.91 & 73.29 & 79.91 & 73.29 & 79.91 & 73.29 \\  &  & PMBR & NA & NA & **79.87** & **73.01** & **79.97** & **73.26** & **79.99** & **73.36** & **79.95** & **73.34** \\  & & NxK & NA & NA & 79.46 & 72.27 & 79.74 & 72.84 & 79.84 & 73.09 & 79.88 & 73.22 \\  &  & SxS & NA & NA & 79.61 & 72.56 & 79.73 & 72.82 & 79.82 & 73.04 & 79.87 & 73.17 \\   & OFMBR & 79.97 & 73.45 & 79.97 & 73.45 & 79.97 & 73.45 & 79.96 & 73.45 & 79.97 & 73.45 \\  &  & PMBR & **79.90** & **73.07** & **79.98** & **73.30** & **80.02** & **73.44** & **80.02** & **73.47** & **80.00** & **73.47** \\  & & NxK & 79.43 & 72.25 & 79.76 & 72.91 & 79.88 & 73.20 & 79.93 & 73.35 & 79.96 & 73.40 \\  & & SxS & 79.73 & 72.80 & 79.82 & 73.04 & 79.88 & 73.19 & 79.92 & 73.30 & 79.95 & 73.38 \\   & FMBR & 79.97 & 73.52 & 79.97 & 73.52 & 79.97 & 73.52 & 79.97 & 73.52 & 79.97 & 73.52 \\  &  & PMBR & **80.00** & **73.32** & **80.01** & **73.44** & **80.03** & **73.52** & **80.02** & **73.53** & **80.00** & **73.54** \\  & & NxK & 79.75 & 72.93 & 79.87 & 73.24 & 79.92 & 73.39 & 79.95 & 73.45 & 79.96 & 73.49 \\  & & SxS & 79.87 & 73.17 & 79.91 & 73.29 & 79.94 & 73.37 & 79.96 & 73.44 & 79.97 & 73.49 \\   & FMBR & 79.96 & 73.60 & 79.96 & 73.60 & 79.96 & 73.60 & 79.96 & 73.60 & 79.96 & 73.60 \\  &  & PMBR & **80.02** & **73.44** & **80.03** & **73.53** & **80.02** & **73.55** & **80.01** & **73.58** & **80.00** & **73.6full computational cost on each language pair. The top row comprises the results obtained with the full MBR method (_FMBR_) running on the complete list of \(N\)=\(128\) candidates, and can be considered as an upper bound for the performance of each approximation method. The number of utility calls for FMBR is \(128^{2}=16\,384\). In Figure 3, we plot the data for the for de\(\)en from Table 2. In Table 3, we fix the language pair to (de\(\)en) and we set \(N\) to different values. This simulates the behavior of approximation methods as the candidate list grows. Similar results for (en\(\)de) are shown in Appendix 6. MQM human evaluation results are summarized in Table 4.

As measuring performance with the same metric we are optimizing for has the risk of overfitting, we mainly focus on Comet22 to assess translation quality. These are the main findings:

(1) PMBR outperforms all other tested approximation methodsPMBR outperforms both the \(N\)x\(K\) and \(S\)x\(S\) approximation methods across language pairs, sample sizes and budgets. The gap between the approximation methods closes as the budget increases. Moreover the results in Table 3 show that the same pattern holds when the size of the hypotheses list changes.

(2) PMBR is competitive to FMBRWe can reduce the computational cost by up to \(r=1/32\) with PMBR without any loss in translation quality as measured by Comet22. Interestingly, we observe that MetricX scores slightly drop when reducing the budget. As this does not affect the final translation quality as measured by Comet22, we argue that this is a good sign and PMBR acts as some kind of regularization.

(3) Human Evaluation confirms (1) and (2)To verify our findings based on Comet22, we do run a MQM human evaluation with professional translators. Results are summarized in Table 4. The results confirm our previous findings: (1) PMBR is the best approximation method when compared to \(N\)x\(K\) and \(S\)x\(S\), and (2) PMBR is getting close to the performance of FMBR.

## 8 Conclusions

In this paper we have shown the inherent low-rank structure of Minimum Bayes Risk (MBR) score matrices which we leveraged to develop an approximation method for MBR decoding that achieves competitive performance while significantly reducing computational complexity. Our empirical results demonstrate the efficacy of this approach across diverse language pairs and evaluation metrics, suggesting its potential for wider application in machine translation and other natural language generation tasks.

Future research could explore the efficacy of alternative matrix completion algorithms to further enhance the low-rank approximation. In addition, the observed low-rank property could be exploited to inform sampling strategies, potentially leading to more efficient and informative data collection for MBR decoding. Another promising avenue is to investigate the applicability of this work to domains beyond natural language generation tasks.

## 9 Limitations

While we have verified that the MBR matrices are low-rank, we did not conduct an empirical analysis on their coherence. A low-rank matrix is easier to complete if its energy spreads evenly across different coordinates. This property is captured by the notion of coherence (Candes and Recht, 2008).

    & Comet22 & MetricX & MQM \\  FMBR & 83.33 & 77.15 & 1.169 \\  PMBR & 83.51 & **76.95** & **1.370** \\ NxK & 83.11 & 76.75 & 1.746 \\ SxS & **83.59** & 76.79 & 1.566 \\   

Table 4: Summary of the average scores of the full EnDe WMT 2022 with N=256 and r=1/16 pairs scored 1000 times using MetricX and Comet22. The MQM scores are limited to 65 examples where all systems disagreed.

In this paper, we only run experiments with MetricX as utility function. The computational costs for computing all pairwise utility scores is expensive. However, we showed that the low-rank matrix structure holds for both MetricX and chrF which gives us confidence that PMBR will generalize regardless of the utility function.

Our human evaluation is limited in size because it is costly. With automatic metrics, we can simulate multiple runs of scoring the datasets but this is not feasible with human evaluations. Thus, we put less statistical significance on our human evaluation.