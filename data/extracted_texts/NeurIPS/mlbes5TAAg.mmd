# Unleashing the Power of Randomization in

Auditing Differentially Private ML

 Krishna Pillutla\({}^{1}\) & Galen Andrew\({}^{1}\) & Peter Kairouz\({}^{1}\)

**H. Brendan McMahan\({}^{1}\)** & **Alina Oprea\({}^{1,2}\)** & **Sweoong Oh\({}^{1,3}\)**

\({}^{1}\)Google Research & \({}^{2}\)Northeastern University & \({}^{3}\)University of Washington

###### Abstract

We present a rigorous methodology for auditing differentially private machine learning algorithms by adding multiple carefully designed examples called canaries. We take a first principles approach based on three key components. First, we introduce Lifted Differential Privacy (LiDP) which expands the definition of differential privacy to handle randomized datasets. This gives us the freedom to design randomized canaries. Second, we audit LiDP by trying to distinguish between the model trained with \(K\) canaries versus \(K-1\) canaries in the dataset, leaving one canary out. By drawing the canaries i.i.d., LiDAR can leverage the symmetry in the design and reuse each privately trained model to run multiple statistical tests, one for each canary. Third, we introduce novel confidence intervals that take advantage of the multiple test statistics by adapting to the empirical higher-order correlations. Together, this new recipe demonstrates significant improvements in sample complexity, both theoretically and empirically, using synthetic and real data. Further, recent advances in designing stronger canaries can be readily incorporated into the new framework.

## 1 Introduction

Differential privacy (DP), introduced in , has gained widespread adoption by governments, companies, and researchers by formally ensuring plausible deniability for participating individuals. This is achieved by guaranteeing that a curious observer of the output of a query cannot be confident in their answer to the following binary hypothesis test: _did a particular individual participate in the dataset or not?_ For example, introducing sufficient randomness when training a model on a certain dataset ensures a desired level of differential privacy. This in turn ensures that an individual's sensitive information cannot be inferred from the trained model with high confidence. However, calibrating the right amount of noise can be a challenging process. It is easy to make mistakes in implementing a DP mechanism due to intricacies involving micro-batching, sensitivity analysis, and privacy accounting. Even with correct implementation, there are several incidents of published DP algorithms with miscalculated privacy guarantees that falsely report higher levels of privacy . Data-driven approaches to auditing a mechanism for a violation of a claimed privacy guarantee can significantly mitigate the danger of unintentionally leaking sensitive data.

Popular approaches for auditing privacy share three common components [e.g. 31, 32, 45, 69, 33]. Conceptually, these approaches are founded on the definition of DP and involve producing counterexamples that potentially violate the DP condition. Algorithmically, this leads to the standard recipe of injecting a single carefully designed example, referred to as a _canary_, and running a statistical hypothesis test for its presence from the outcome of the mechanism. Analytically, a high-confidence bound on the DP condition is derived by calculating the confidence intervals of the corresponding Bernoulli random variables from \(n\) independent trials of the mechanism.

Recent advances adopt this standard approach and focus on designing stronger canaries to reduce the number of trials required to successfully audit DP [e.g., 31; 32; 33; 45; 69]. However, each independent trial can be as costly as training a model from scratch; refuting a false claim of \((,)\)-DP with a minimal number of samples is of utmost importance. In practice, standard auditing can require training on the range of thousands to hundreds of thousands of models [e.g., 60]. Unfortunately, under the standard recipe, we are fundamentally limited by the \(1/\) sample dependence of the Bernoulli confidence intervals.

**Contributions.** We break this \(1/\) barrier by rethinking auditing from first principles.

1. **Lifted DP**: We propose to audit an equivalent definition of DP, which we call Lifted DP in SS3.1. This gives an auditor the freedom to design a counter-example consisting of _random_ datasets and rejection sets. This enables adding random canaries, which is critical in the next step. Theorem 3 shows that violation of Lifted DP implies violation of DP, justifying our framework.
2. **Auditing Lifted DP with Multiple Random Canaries**: We propose adding \(K>1\) canaries under the alternative hypothesis and comparing it against a dataset with \(K-1\) canaries, leaving one canary out. If the canaries are deterministic, we need \(K\) separate null hypotheses; each hypothesis leaves one canary out. Our new recipe overcomes this inefficiency in SS3.2 by drawing _random_ canaries independently from the same distribution. This ensures the exchangeability of the test statistics, allowing us to reuse each privately trained model to run multiple hypothesis tests in a principled manner. This is critical in making the confidence interval sample efficient.
3. **Adaptive Confidence Intervals**: Due to the symmetry of our design, the test statistics follow a special distribution that we call _eXchangeable Bernoulli (XBern)_. Auditing privacy boils down to computing confidence intervals on the average test statistic over the \(K\) canaries included in the dataset. If the test statistics are independent, the resulting confidence interval scales as \(1/\). However, in practice, the dependence is non-zero and unknown. We propose a new and principled family of confidence intervals in SS3.3 that adapts to the empirical higher-order correlations between the test statistics. This gives significantly smaller confidence intervals when the actual dependence is small, both theoretically (Proposition 4) and empirically.
4. **Numerical Results**: We audit an unknown Gaussian mechanism with black-box access and demonstrate (up to) \(16\) improvement in the sample complexity. We also show how to seamlessly lift recently proposed canary designs in our recipe to improve the sample complexity on real data.

## 2 Background

We describe the standard recipe to audit DP. Formally, we adopt the so-called add/remove definition of differential privacy; our constructions also seamlessly extend to other choices of neighborhoods as we explain in SSB.2.

**Definition 1** (Differential privacy).: A pair of datasets, \((D_{0},D_{1})\), is said to be _neighboring_ if their sizes differ by one and the datasets differ in one entry: \(|D_{1} D_{0}|+|D_{0} D_{1}|=1\). A randomized mechanism \(:^{*}\) is said to be \((,)\)_-Differentially Private_ (DP) for some \( 0\) and \(\) if it satisfies \(_{}((D_{1}) R) e^{}\, _{}((D_{0}) R)+\), which is equivalent to

\[(((D_{1}) R)-)- ((D_{0}) R)\,,\] (1)

for all pairs of neighboring datasets, \((D_{0},D_{1})\), and all measurable sets, \(R\), of the output space \(\), where \(_{}\) is over the randomness internal to the mechanism \(\). Here, \(^{*}\) is a space of datasets.

For small \(\) and \(\), one cannot infer from the output whether a particular individual is in the dataset or not with a high success probability. For a formal connection, we refer to . This naturally leads to a standard procedure for auditing a mechanism \(\) claiming \((,)\)-DP: present \((D_{0},D_{1},R)^{*}^{*}\) that violates Eq. (1) as a piece of evidence. Such a counter-example confirms that an adversary attempting to test the participation of an individual will succeed with sufficient probability, thus removing the potential for plausible deniability for the participants.

**Standard Recipe: Adding a Single Canary.** When auditing DP model training (using e.g. DP-SGD [1; 54]), the following recipe is now standard for designing a counter-example \((D_{0},D_{1},R)\)[31; 32; 33; 45; 69]. A training dataset \(D_{0}\) is assumed to be given. This ensures that the model under scrutiny matches the use-case and is called a _null hypothesis_. Next, under a corresponding _alternative hypothesis_, a neighboring dataset \(D_{1}=D_{0}\{c\}\) is constructed by adding a single carefully-designedexample \(c\), known as a _canary_. Finally, Eq. (1) is evaluated with a choice of \(R\) called a _rejection set_. For example, one can reject the null hypothesis (and claim the presence of the canary) if the loss on the canary is smaller than a fixed threshold; \(R\) is a set of models satisfying this rejection rule.

**Bernoulli Confidence Intervals.** Once a counter-example \((D_{0},D_{1},R)\) is selected, we are left to evaluate the DP condition in Eq. (1). Since the two probabilities in the condition cannot be directly evaluated, we rely on the samples of the output from the mechanism, e.g., models trained with DP-SGD. This is equivalent to estimating the expectation, \(((D) R)\) for \(D\{D_{0},D_{1}\}\), of a Bernoulli random variable, \(((D) R)\), from \(n\) i.i.d. samples. Providing high confidence intervals for Bernoulli distributions is a well-studied problem with several off-the-shelf techniques, such as Clopper-Pearson, Jeffreys, Bernstein, and Wilson intervals. Concretely, let \(}_{n}((D) R)\) denote the empirical probability of the model falling in the rejection set in \(n\) independent runs. The standard intervals scale as \(|((D_{0}) R)-}_{n}((D_{0})  R)| C_{0}n^{-1/2}\) and \(|((D_{1}) R)-}_{n}((D_{1})  R)| C_{1}n^{-1/2}\) for constants \(C_{0}\) and \(C_{1}\) independent of \(n\). If \(\) satisfies a claimed \((,)\)-DP in Eq. (1), then the following finite-sample lower bound holds with high confidence:

\[_{n}=(}_{n}((D_{1}) R)-}{}-)-(}_{n}((D_{0}) R)+}{})\,.\] (2)

\((,)\)-DP amounts to testing the violation of this condition. This is fundamentally limited by the \(n^{-1/2}\) dependence of the Bernoulli confidence intervals. Our goal is to break this barrier.

**Notation.** While the DP condition is symmetric in \((D_{0},D_{1})\), we use \(D_{0},D_{1}\) to refer to specific hypotheses. For symmetry, we need to check both conditions: Eq. (1) and its counterpart with \(D_{0},D_{1}\) interchanged. We omit this second condition for notational convenience. We use the shorthand \([k]:=\{1,2,,k\}\). We refer to random variables by boldfaced letters (e.g. \(\) is a random dataset).

**Related Work.** We provide a detailed survey in Appendix A. A stronger canary (and its rejection set) can increase the RHS of (1). The resulting hypothesis test can tolerate larger confidence intervals, thus requiring fewer samples. This has been the focus of recent breakthroughs in privacy auditing in . They build upon membership inference attacks [e.g. 12, 53, 68] to measure memorization. Our aim is not to innovate on this front. Instead, our framework can seamlessly adopt recently designed canaries and inherit their strengths as demonstrated in SS5 and SS6.

Random canaries have been used in prior work, but for making the canary out-of-distribution in a computationally efficient manner. No variance reduction is achieved by such random canaries. Adding multiple (deterministic) canaries has been explored in literature but for different purposes.  include multiple copies of the same canary to make the canary easier to detect while paying for group privacy since the paired datasets differ in multiple entries (see SS3.2 for a detailed discussion).  propose adding multiple distinct canaries to reuse each trained model for multiple hypothesis tests. However, each canary is no stronger than a single canary case, and the resulting auditing suffers from group privacy. When computing the lower bound on \(\), however, group privacy is ignored and the test statistics are assumed to be independent without rigorous justification.  avoids group privacy in the federated scenario where the adversary has the freedom to return a canary gradient update of choice. The prescribed random gradient shows good empirical performance. The confidence interval is not rigorously derived. Our recipe for injecting multiple canaries _without_ a group privacy cost with _rigorous_ confidence intervals can be incorporated into these works to give provable lower bounds.

In an independent and concurrent work, Steinke et al.  also consider auditing with randomized canaries that are Poisson-sampled, i.e., each canary is included or excluded independently with equal probability. Their recipe involves computing an empirical lower bound by comparing the accuracy (rather than the full confusion matrix as in our case) from the possibly dependent guesses with the worst-case randomized response mechanism. This allows them to use multiple dependent observations from a single trial to give a high probability lower bound on \(\). Their confidence intervals, unlike the ones we give here, are non-adaptive and worst-case.

## 3 A New Framework for Auditing DP Mechanisms with Multiple Canaries

We define Lifted DP, a new definition of privacy that is equivalent to DP (SS3.1). This allows us to define a new recipe for auditing with multiple random canaries, as opposed to a single deterministic canary in the standard recipe, and reuse each trained model to run multiple correlated hypothesis testsin a principled manner (SS3.2). The resulting test statistics form a vector of _dependent but exchangeable_ indicators (which we call an eXchangeable Bernoulli or XBern distribution), as opposed to a single Bernoulli distribution. We leverage this exchangeability to give confidence intervals for the XBern distribution that can potentially improve with the number of injected canaries (SS3.3). The pseudocode of our approach is provided in Algorithm 1.

### From DP to Lifted DP

To enlarge the design space of counter-examples, we introduce an equivalent definition of DP.

**Definition 2** (Lifted differential privacy).: Let \(\) denote a joint probability distribution over \((_{0},_{1},)\) where \((_{0},_{1})^{*}^{*}\) is a pair of _random_ datasets that are neighboring (as in the standard definition of neighborhood in Definition 1) with probability one and let \(\) denote a _random_ rejection set. We say that a randomized mechanism \(:^{*}\) satisfies \((,)\)_-Lifted Differential Privacy_ (LiDP) for some \( 0\) and \(\) if, for all \(\) independent of \(\), we have

\[_{,}((_{1}))  e^{}\,_{,}((_{0}))+\,.\] (3)

In Appendix A.3, we discuss connections between Lifted DP and other existing extensions of DP, such as Bayesian DP and Pufferfish, that also consider randomized datasets. The following theorem shows that LiDP is equivalent to the standard DP, which justifies our framework of checking the above condition; if a mechanism \(\) violates the above condition then it violates \((,)\)-DP.

**Theorem 3**.: _A randomized algorithm \(\) is \((,)\)-LiDP iff \(\) is \((,)\)-DP._

A proof is provided in Appendix B. In contrast to DP, LiDP involves probabilities over both the internal randomness of the algorithm \(\) and the distribution \(\) over \((_{0},_{1},)\). This gives the auditor greater freedom to search over a _lifted_ space of joint distributions over the paired datasets and a rejection set; hence the name Lifted DP. Auditing LiDP amounts to constructing a _randomized_ (as emphasized by the boldface letters) counter-example \((_{0},_{1},)\) that violates (3) as evidence.

### From a Single Deterministic Canary to Multiple Random Canaries

Our strategy is to turn the LiDP condition in Eq. (3) into another condition in Eq. (4) below; this allows the auditor to reuse samples, running multiple hypothesis tests on each sample. This derivation critically relies on our carefully designed recipe that incorporates three crucial features: \((a)\) binary hypothesis tests between pairs of stochastically coupled datasets containing \(K\) canaries and \(K-1\) canaries, respectively, for some fixed integer \(K\), \((b)\) sampling those canaries i.i.d. from the same distribution, and (\(c\)) choice of rejection sets, where each rejection set only depends on a single left-out canary. We introduce the following recipe, also presented in Algorithm 1.

We fix a given training set \(D\) and a canary distribution \(P_{}\) over \(\). This ensures that the model under scrutiny is close to the use case. Under the alternative hypothesis, we train a model on a randomized training dataset \(_{1}=D\{_{1},,_{K}\}\), augmented with \(K\) random canaries drawn i.i.d. from \(P_{}\). Conceptually, this is to be tested against \(K\) leave-one-out (LOO) null hypotheses. Under the \(k^{}\) null hypothesis for each \(k[K]\), we construct a coupled dataset, \(_{0,k}=D\{_{1},,_{k-1},_{k+1},_{K}\}\), with \(K-1\) canaries, leaving the \(k^{}\) canary out. This coupling of \(K-1\) canaries ensures that \((_{0,k},_{1})\) is neighboring with probability one. For each left-out canary, the auditor runs a binary hypothesis test with a choice of a random rejection set \(_{k}\). We restrict \(_{k}\) to depend only on the canary \(_{k}\) that is being tested and not the index \(k\). For example, \(_{k}\) can be the set of models achieving a loss on the canary \(_{k}\) below a predefined threshold \(\).

The goal of this LOO construction is to reuse each trained private model to run multiple tests such that the averaged test statistic has a smaller variance for a _given number of models_. Under the standard definition of DP, one can still use the above LOO construction but with fixed and deterministic canaries. This gives no variance gain because evaluating \(((D_{0,k}) R_{k})\) in Eq. (1) or its averaged counterpart \((1/K)_{k=1}^{K}((D_{0,k}) R_{k})\) requires training one model to get one sample from the test statistic \(((D_{0,k}) R_{k})\). The key ingredient in _reusing trained models_ is randomization.

We build upon the LiDP condition in Eq. (3) by noting that the test statistics are exchangeable for i.i.d. canaries. Specifically, we have for any \(k[K]\) that \(((_{0,k})_{k})=(( {D}_{0,K})_{K})=((_{0,K})_{j}^{ })\) for any canary \(_{j}^{}\) drawn i.i.d. from \(P_{}\) and its corresponding rejection set \(^{}_{j}\) that are statistically independent of \(_{0,K}\). Therefore, we can rewrite the right side of Eq. (3) using \(m\) i.i.d. _test canaries_\(^{}_{1},,^{}_{m} P_{}\) and a single trained model \((_{0,K})\) as

\[_{k=1}^{K}_{,}(( {D}_{1})_{k})  }{m}_{j=1}^{m}_{, }((_{0,K})^{}_{j})+\,.\] (4)

Checking this condition is sufficient for auditing LiDP and, via Theorem 3, for auditing DP. For each model trained on \(_{1}\), we record the test statistics of \(K\) (correlated) binary hypothesis tests. This is denoted by a random vector \(=(((_{1})_{k}))_{k=1}^{K}\{0,1\}^ {K}\), where \(_{k}\) is a rejection set that checks for the presence of the \(k^{}\) canary. Similar to the standard recipe, we train \(n\) models to obtain \(n\) i.i.d. samples \(^{(1)},,^{(n)}\{0,1\}^{K}\) to estimate the left side of (4) using the empirical mean:

\[}_{1}:=_{i=1}^{n}_{k=1}^{K}^{(i)}_{k} \;,\] (5)

where the subscript one in \(}_{1}\) denotes that this is the empirical first moment. Ideally, if the \(K\) tests are independent, the corresponding confidence interval is smaller by a factor of \(\). In practice, the \(K\) tests are correlated and the size of the confidence interval depends on their correlation. We derive principled confidence intervals that leverage the empirically measured correlations in SS3.3. We can define \(\{0,1\}^{m}\) and its mean \(}_{1}\) analogously for the null hypothesis. We provide pseudocode in Algorithm 1 as an example guideline for applying our recipe to auditing DP training, where \(f_{}(z)\) is the loss evaluated on an example \(z\) for a model \(\). \(()\) and \(()\) respectively return the lower and upper adaptive confidence intervals from SS3.3. We instantiate Algorithm 1 with concrete examples of canary design in SS5.

```
0: Sample size \(n\), number of canaries \(K\), number of null tests \(m\), DP mechanism \(\), training set \(D\), canary generating distribution \(P_{}\), threshold \(\), failure probability \(\), privacy \(\).
1:for\(i=1,,n\)do
2: Randomly generate \(K+m\) canaries \(\{_{1},,_{K},^{}_{1},,^{}_{m}\}\) i.i.d. from \(P_{}\).
3:\(_{0} D\{_{1},,_{K-1}\}\), \(_{1} D\{_{1},,_{K}\}\)
4: Train two models \(_{0}(_{0})\) and \(_{1}(_{1})\)
5: Record test statistics \(^{(i)}(f_{_{1}}(_{k})<) _{k=1}^{K}\) and \(^{(i)}(f_{_{0}}(^{}_{ j})<)_{j=1}^{m}\)
6: Set \(_{1}\{^{(i)}\}_{i [n]},/2\) and \(}_{0}\{^{(i) }\}_{i[n]},/2\)
7:Return\(_{n}(_{1}-)/ }_{0}\) and a guarantee that \((<_{n})\). ```

**Algorithm 1** Auditing Lifted DP

**Our Recipe vs. Multiple Deterministic Canaries.** An alternative with deterministic canaries would be to test between \(D_{0}\) with no canaries and \(D_{1}\) with \(K\) canaries such that we can get \(K\) samples from a single trained model on \(D_{0}\), one for each of the \(K\) test statistics \(\{((D_{0}) R_{k})\}_{k=1}^{K}\). However, due to the fact that \(D_{1}\) and \(D_{0}\) are now at Hamming distance \(K\), this suffers from group privacy; we are required to audit for a much larger privacy leakage of \((K,((e^{K}-1)/(e^{}-1)))\)-DP. Under the (deterministic) LOO construction, this translates into \((1/K)_{k=1}^{K}((D_{1}) R_{k}) e^{K }(1/K)_{k=1}^{K}((D_{0}) R_{k})+((e^{K }-1)/(e^{}-1))\), where if one canary violates the group privacy condition then the average also violates it. We can reuse a single trained model to get \(K\) test statistics in the above condition, but each canary is distinct and not any stronger than the one from \((,)\)-DP auditing. One cannot obtain stronger counterexamples without sacrificing the sample gain. For example, we can repeat the same canary \(K\) times as proposed in . This makes it easier to detect the canary, making a stronger counter-example, but there is no sample gain as we only get one test statistic per trained model. With deterministic canaries, there is no way to avoid this group privacy cost while our recipe does not incur it.

### From Bernoulli Intervals to Higher-Order Exchangeable Bernoulli (XBern) Intervals

The LiDP condition in Eq. (4) critically relies on the canaries being sampled i.i.d. and the rejection set only depending on the corresponding canary. For such a symmetric design, auditing boils down to deriving a Confidence Interval (CI) for a special family of distributions that we call _Exchangeable Bernoulli (XBern)_. We derive the CI for the alternate hypothesis, i.e., the left side of Eq. (4). The CI under the null hypothesis is analogous.

Recall that \(_{k}:=((_{1})_{k})\) denotes the test statistic for the \(k^{}\) canary. By the symmetry of our design, \(\{0,1\}^{K}\) is an _exchangeable_ random vector that is distributed as an exponential family. Further, the distribution of \(\) is fully defined by a \(K\)-dimensional parameter \((_{1},,_{K})\) where \(_{}\) is the \(^{}\) moment of \(\). We call this family XBern. Specifically, this implies permutation invariance of the higher-order moments: \([_{j_{1}}_{j_{}}]=[_{k_{1}} _{k_{}}]\) for any distinct sets of indices \((j_{l})_{l[]}\) and \((k_{l})_{l[]}\) for any \( K\). For example, \(_{1}:=[(1/K)_{k=1}^{K}_{k}]\), which is the LHS of (4).

Using samples from this XBern, we aim to derive a CI on \(_{1}\) around the empirical mean \(}_{1}\) in (5). Bernstein's inequality applied to our test statistic \(_{1}:=(1/K)_{k=1}^{K}_{k}\) gives,

\[|}_{1}-_{1}|  (_{1})}+\,,\] (6)

w.p. at least \(1-\). Bounding \((_{1})_{1}(1-_{1})\) since \(_{1}\) a.s. and numerically solving the above inequality for \(_{1}\) gives a CI that scales as \(1/\) -- see Figure 2 (left). We call this the **1st-order Bernstein bound**, as it depends only on the 1st moment \(_{1}\). Our strategy is to measure the (higher-order) correlations between \(_{k}\)'s to derive a tighter CI that adapts to the given instance. This idea applies to any standard CI. We derive and analyze the higher order Bernstein intervals here, and experiments use Wilson intervals from Appendix C; see also Figure 2 (right).

Concretely, we can leverage the 2nd order correlation by expanding

\[(_{1})=(_{1}-_{2})+(_{2}-_{1}^{2}) _{2}:=_{k_{1}<k_{2}[K]} [_{k_{1}}_{k_{2}}].\]

Ideally, when the second order correlation \(_{2}-_{1}^{2}=[_{1}_{2}]-[_{1}] [_{2}]\) equals \(0\), we have \((_{1})=_{1}(1-_{1})/K\), a factor of \(K\) improvement over the worst-case. Our higher-order CIs adapt to the _actual level of correlation_ of \(\) by further estimating the 2nd moment \(_{2}\) from samples. Let \(}_{2}\) be the first-order Bernstein upper bound on \(_{2}\) such that \((_{2}}_{2}) 1-\). On this event,

\[(_{1})  (_{1}-}_{2})+( }_{2}-_{1}^{2})\,.\] (7)

Combining this with (6) gives us the **2nd-order Bernstein bound** on \(_{1}\), valid w.p. \(1-2\). Since \(}_{2}}_{2}+1/\) where \(}_{2}\) is the empirical estimate of \(_{2}\), the 2nd-order bound scales as

\[|_{1}-}_{1}|  }+|}_{2}-}_{1}^{2}|}+}\,,\] (8)

where constants and log factors are omitted. Thus, our 2nd-order CI can be as small as \(1/+1/n^{3/4}\) (when \(}_{1}^{2}_{2}\)) or as large as \(1/\) (in the worst-case). With small enough correlations of \(|}_{2}-}_{1}^{2}|=O(1/K)\), this suggests a choice of \(K=O()\) to get CI of \(1/n^{3/4}\). In practice, the correlation is controlled by the design of the canary. For the Gaussian mechanism with random canaries, the correlation indeed empirically decays as \(1/K\), as we see in SS4. Thus, the CI decreases monotonically with \(K\). This is also true for the CI under the null hypothesis with \(K\) replaced by \(m\).

However, there is a qualitative difference between the null and alternate hypotheses. Estimation under the alternate hypothesis incurs a larger bias as the number \(K\) of canaries increases -- this is due

Figure 1: **Bias illustration**: Consider the sum query \(z_{1}+z_{2}\) with 2 inputs. Its DP version produces a point in the blue circle w.h.p. due to the noise \((0,)\) scaled by \(\). When auditing with a random canary \(\), it contributes additional randomness (red disc) leading to a smaller effective privacy parameter \(\).

Figure 2: **Left**: The Bernstein CI \([},}]\) can be found by equating the two sides of Bernstein’s inequality in Eq. (6). **Right**: The asymptotic Wilson CI is a tightening of the Bernstein CI with a smaller \(1/\) coefficient (shown here) and no \(1/n\) term.

to the additional randomness from adding more canaries into the training process, as illustrated in Figure 1. The optimal choice of \(K\) balances the bias and variance. On the other hand, estimation under the null hypothesis incurs no bias and a larger number \(m\) of test canaries always helps. We return to the bias-variance tradeoff in SS4.

**Higher-order intervals.** We can recursively apply this method by expanding the variance of higher-order statistics, and derive higher-order Bernstein bounds. The next recursion uses empirical 3\({}^{}\) and 4\({}^{}\) moments \(}_{3}\) and \(}_{4}\) to get the **4\({}^{}\)-order Bernstein bound** which scales as

\[|_{1}-}_{1}|}+}\,| }_{2}-}_{1}^{2}|+}|}_{4 }-}_{2}^{2}|^{1/4}+}\,.\] (9)

Ideally, when the 4\({}^{}\)-order correlation is small enough, \(|}_{4}-}_{2}^{2}|=O(1/K)\), (along with the 2nd-order correlation, \(}_{2}-}_{1}^{2}\)) this 4th-order CI scales as \(1/n^{7/8}\) with a choice of \(K=O(n^{3/4})\) improving upon the 2nd-order CI of \(1/n^{3/4}\). We can recursively derive even higher-order CIs, but we find in SS4 that the gains diminish rapidly. In general, the \(^{}\) order Bernstein bounds achieve CIs scaling as \(1/n^{(2-1)/2}\) with a choice of \(K=O(n^{(-1)/})\). This shows that the higher-order CI decreases in the order \(\) of the correlations used; we refer to Appendix C for details.

**Proposition 4**.: _For any positive integer \(\) that is a power of two and \(K= n^{(-1)/}\), suppose we have \(n\) samples from a \(K\)-dimensional XBern distribution with parameters \((_{1},,_{K})\). If all \(^{}\)-order correlations scale as \(1/K\), i.e., \(|_{2^{}}-_{^{}}^{2}|=O(1/K)\), for all \(^{}\) and \(^{}\) is a power of two, then the \(^{}\)-order Bernstein bound is \(|_{1}-}_{1}|=O(1/n^{(2-1)/(2)})\)._

## 4 Simulations: Auditing the Gaussian Mechanism

**Setup.** We consider a simple sum query \(q(D)=_{z D}z\) over the unit sphere \(=\{z^{d}\,:\,\|z\|_{2}=1\}\). We want to audit a Gaussian mechanism that returns \(q(D)+\) with standard Gaussian \((0,_{d})\) and \(\) calibrated to ensure \((,)\)-DP. We assume black-box access, where we do not know what mechanism we are auditing and we only access it through samples of the outcomes. A white-box audit is discussed in SSA.1. We apply our new recipe with canaries sampled uniformly at random from \(\). Following standard methods [e.g. 23], we declare that a canary \(_{k}\) is present if \(_{k}^{}()>\) for a threshold \(\) learned from separate samples. For more details and additional results, see Appendix E. A broad range of values of \(K\) (between 32 and 256 in Figure 3 middle) leads to good performance in auditing LiDP. We use \(K=\) (as suggested by our analysis in (8)), \(m=K\) test canaries, and the 2\({}^{}\)-order Wilson estimator (as gains diminish rapidly afterward) as a reliable default setting.

**Sample Complexity Gains.** In Figure 3 (left), the proposed approach of injecting \(K\) canaries with the 2\({}^{}\) order Wilson interval (denoted "LiDP +2\({}^{}\)-Order Wilson") reduces the number of trials, \(n\), needed to reach the same empirical lower bound, \(\), by \(4\) to \(16\), compared to the baseline of injecting a single canary (denoted "DP+Wilson"). We achieve \(_{n}=0.85\) with \(n=4096\) (while the baseline requires \(n=65536\)) and \(_{n}=0.67\) with \(n=1024\) (while the baseline requires \(n=4096\)).

Figure 3: **Left**: For Gaussian mechanisms, the proposed LiDP-based auditing with \(K\) canaries provides a significant gain in the required number of trials to achieve a desired level of lower bound \(\) on the privacy. **Center**: Increasing the number of canaries trades off the bias and the variance, with our prescribed \(K=\) achieving a good performance. **Right**: Increasing the dimension makes the canaries less correlated, thus achieving smaller confidence intervals, and larger \(\). We shade the standard error over 25 repetitions.

**Number of Canaries and Bias-Variance Tradeoffs.** In Fig. 3 (middle), LiDP auditing with \(2^{}\)/\(4^{}\)-order CIs improve with increasing canaries up to a point and then decreases. This is due to a bias-variance tradeoff, which we investigate further in Figure 4 (left). Let \((K,)\) denote the empirical privacy lower bound with \(K\) canaries using an \(^{}\) order interval, e.g., the baseline is \((1,1)\). Let the bias from injecting \(K\) canaries and the variance gain from \(^{}\)-order interval respectively be

\[(K):=(K,1)-(1,1)\,, (K,):=(K,)-(K,1)\,.\] (10)

In Figure 4 (left), the gain \((K)\) from bias is negative and gets worse with increasing \(K\); when testing for each canary, the \(K-1\) other canaries introduce more randomness that makes the test more private and hence lowers the \(\) (see also Figure 1. The gain \((K)\) in variance is positive and increases with \(K\) before saturating. This improved "variance" of the estimate is a key benefit of our framework. The net improvement \((K,)-(1,1)\) is a sum of these two effects. This trade-off between bias and variance explains the concave shape of \(\) in \(K\).

Next, we see from Figure 5 that a larger number \(m\) of test canaries always helps, i.e., it does not incur a similar bias-variance tradeoff. Indeed, this is because larger \(m\) does not lead to any additional bias, as discussed in SS3.3. However, the gains quickly saturate, so \(m=K\) is a reliable default, especially for \(K 16\) or so; we refer to Appendix E for more plots and details.

**Correlation between Canaries.** Based on (8), this improvement in the variance can further be examined by looking at the term \(|_{2}-_{1}^{2}|\) that leads to a narrower \(2^{}\)-order Wilson interval. The log-log plot of this term in Figure 4 (middle) is nearly parallel to the dotted \(1/K\) line (slope = \(-0.93\)), meaning that it decays roughly as \(1/K\) (note that the log-log plot of \(y=cx^{}\) is a straight line with slope \(a\)). This indicates that we get close to a \(1/\) confidence interval as desired. Similarly, we get that \(|_{4}-_{2}^{2}|\) decays roughly as \(1/K\) (slope = \(-1.05\)). However, the \(4^{}\)-order estimator offers only marginal additional improvements in the small \(\) regime (see Appendix E). Thus, the gain diminishes rapidly in the order of our estimators.

Figure 4: **Left**: Separating the effects of bias and variance in auditing LiDP; cf. definition (10). **Center & Right**: The correlations between the test statistics of the canaries decrease with \(K\) and \(d\), achieving smaller CIs.

Figure 5: **Left**: There is no bias-variance tradeoff with the number \(m\) of test canaries: larger \(m\) always gives a better empirical \(\) but the results saturate quickly. This experiment is for the Gaussian mechanism. **Center & Right**: The proposed LiDP-based auditing procedure gives significant improvements similar to Figure 3 when auditing the Laplace mechanism with canaries sampled randomly from the unit \(_{1}\) ball.

**Effect of Dimension on the Bias and Variance.** As the dimension \(d\) increases, the LiDP-based lower bound becomes tighter monotonically as we show in Figure 3 (right). This is due to both the bias gain, as in (10), improving (less negative) and the variance gain improving (more positive). With increasing \(d\), the variance of our estimate reduces because the canaries become less correlated. Guided by Eq. (8,9), we measure the relevant correlation measures \(|}_{2}-}_{1}^{2}|\) and \(|}_{4}-}_{2}^{2}|\) in Figure 4 (right). Both decay approximate as \(1/d\) (slope = \(-1.06\) and \(-1.00\) respectively, ignoring the outlier at \(d=10^{6}\)). This suggests that, for Gaussian mechanisms, the corresponding XBern distribution resulting from our recipe behaves favorably as \(d\) increases.

**Auditing Other Mechanisms.** The proposed LiDP-based auditing recipe is agnostic to the actual privacy mechanism. We audit a Laplace mechanism with canaries drawn uniformly at random over the unit \(_{1}\) ball in Figure 4 (center and right). The results are qualitatively similar to those of the Gaussian mechanism, leading to an eight-fold improvement in the sample complexity over the baseline.

## 5 Lifting Existing Canary Designs

Several prior works (e.g., 32; 41; 45; 60), focus on designing stronger canaries to improve the lower bound on \(\). We provide two concrete examples of how to _lift_ these canary designs to be compatible with our framework while inheriting their strengths. We refer to Appendix D for further details.

We impose two criteria on the distribution \(P_{}\) over canaries for auditing LiDP. First, the injected canaries are easy to detect, so that the probabilities on the left side of (4) are large and those on the right side are small. Second, a canary \( P_{}\), if included in the training of a model \(\), is unlikely to change the membership of \( R_{^{}}\) for an independent canary \(^{} P_{}\). Existing canary designs already impose the first condition to audit DP using (1). The second condition ensures that the canaries are uncorrelated, allowing our adaptive CIs to be smaller, as we discussed in SS3.3.

**Data Poisoning via Tail Singular Vectors.** ClipBKD  adds as canaries the tail singular vector of the input data (e.g., images). This ensures that the canary is out of distribution, allowing for easy detection. We lift ClipBKD by defining the distribution \(P_{}\) as the uniform distribution over the \(p\) tail singular vectors of the input data. If \(p\) is small relative to the dimension \(d\) of the data, then they are still out of distribution, and hence, easy to detect. For the second condition, the orthogonality of the singular vectors ensures the interaction between canaries is minimal, as measured empirically.

**Random Gradients.** The approach of  samples random vectors (of the right norm) as canary gradients, assuming a grey-box access where we can inject gradients. Since random vectors are nearly orthogonal to any fixed vector in high dimensions, their presence is easy to detect with a dot product. Similarly, any two i.i.d. canary gradients are roughly orthogonal, leading to minimal interactions.

## 6 Experiments

We compare the proposed LiDP auditing recipe relative to the standard one for DP training of machine learning models. Our code is available online.1

**Setup.** We test with two classification tasks: FMNIST  is a 10-class grayscale image classification dataset, while Purchase-100 is a sparse tabular dataset with \(600\) binary features and \(100\) classes [19; 53]. We train a linear model and a multi-layer perceptron (MLP) with 2 hidden layers using DP-SGD  to achieve \((,10^{-5})\)-DP with varying values of \(\). The training is performed using cross-entropy for a fixed epoch budget and a batch size of \(100\). We refer to Appendix F for details.

**Auditing.** We audit the LiDP using the two types of canaries from SS5: data poisoning and random gradient canaries. We vary the number \(K\) of canaries and the number \(n\) of trials. We track the empirical lower bound obtained from the Wilson family of confidence intervals. We compare this with auditing DP, which coincides with auditing LiDP with \(K=1\) canary. We audit _only the final model_ in all the experiments.

**Sample Complexity Gain.** Table 1 shows the reduction in the sample complexity from auditing LiDP. For each canary type, auditing LiDP is better \(11\) out of the \(12\) settings considered. The average improvement (i.e., the harmonic mean over the table) for data poisoning is \(2.3\), while for randomgradients, it is \(3.0\). Since each trial is a full model training run, this improvement can be quite significant in practice. This improvement can also be seen visually in Figure 6 (left two).

**Number of Canaries.** We see from Figure 6 (right two) that LiDP auditing on real data behaves similarly to Figure 3 in SS4. We also observe the bias-variance tradeoff in the case of data poisoning (center right). The choice \(K=\) is competitive with the best value of \(K\), validating our heuristic. Overall, these results show that the insights from SS4 hold even with the significantly more complicated DP mechanism involved in training models privately.

**Additional Comparisons.** We show in SSF.4 that our LiDP auditing with generally outperforms the Bayesian auditing approach of Zanella-Beguelin et al. (2019). This also suggests an interesting future research direction: adapt Bayesian credible intervals for LiDP auditing to get the best of both worlds.

## 7 Conclusion

We introduce a new framework for auditing differentially private learning. Diverging from the standard practice of adding a single deterministic canary, we propose a new recipe of adding multiple i.i.d. random canaries. This is made rigorous by an expanded definition of privacy that we call LiDAR. We provide novel higher-order confidence intervals that can automatically adapt to the level of correlation in the data. We empirically demonstrate that there is a potentially significant gain in sample dependence of the confidence intervals, achieving a favorable bias-variance tradeoff.

Although any rigorous statistical auditing approach can benefit from our framework, it is not yet clear how other popular approaches (e.g. 12) for measuring memorization can be improved with randomization. Bridging this gap is an important practical direction for future research. It is also worth considering how our approach can be adapted to audit the diverse definitions of privacy in machine learning (e.g. 28).

**Broader Impact.** Auditing private training involves a trade-off between the computational cost and the tightness of the guarantee. This may not be appropriate for all practical settings. For deployment in production, it is worth further studying approaches with minimal computational overhead (e.g. 2; 12).

    &  &  &  \\   & **(Wilson)** & \(=2\) & \(=4\) & \(=8\) & \(=16\) & \(=2\) & \(=4\) & \(=8\) & \(=16\) \\   & 2nd-Ord. & \(0.68\) & \(3.31\) & \(2.55\) & \(4.38\) & \(2.69\) & \(3.34\) & \(5.98\) & \(5.06\) \\  & 4th-Ord. & \(0.42\) & \(2.68\) & \(2.29\) & \(3.76\) & \(2.66\) & \(2.70\) & \(7.75\) & \(4.19\) \\   & 2nd-Ord. & \(4.80\) & \(1.46\) & \(2.95\) & \(1.60\) & \(4.62\) & \(2.88\) & \(2.33\) & \(5.46\) \\  & 4th-Ord. & \(4.42\) & \(2.49\) & \(2.37\) & \(1.30\) & \(3.95\) & \(2.81\) & \(2.02\) & \(4.60\) \\   & 2nd-Ord. & \(3.14\) & \(1.06\) & \(1.41\) & \(9.28\) & \(1.41\) & \(0.71\) & \(4.30\) & \(2.84\) \\  & 4th-Ord. & \(2.84\) & \(1.09\) & \(1.36\) & \(6.99\) & \(1.29\) & \(0.42\) & \(4.35\) & \(2.38\) \\   

Table 1: The (multiplicative) improvement in the sample complexity from auditing LiDP with \(K=16\) canaries compared to auditing DP with \(n=1000\) trials. We determine this factor by linearly interpolating/extrapolating \(_{n}\); cf. Figure 6 (left) for a visual representation of these numbers. For instance, an improvement of \(3.31\) means LiDP needs \(n 1000/3.31 302\) trials to reach the same empirical lower bound that DP reaches at \(n=1000\).

Figure 6: **Left two**: LiDP-based auditing with \(K>1\) canaries achieves the same lower bound \(\) on the privacy loss with fewer trials. **Right two**: LiDP auditing is robust to \(K\); the prescribed \(K=\) is a reliable default.