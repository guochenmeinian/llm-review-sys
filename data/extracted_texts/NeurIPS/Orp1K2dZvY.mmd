# Weakly Supervised 3D Open-vocabulary Segmentation

Kunhao Liu\({}^{1}\)  Fangneng Zhan\({}^{2}\)  Jiahui Zhang\({}^{1}\)  Muyu Xu\({}^{1}\)  Yingchen Yu\({}^{1}\)

**Abdulmotaleb El Saddik\({}^{3,5}\)  Christian Theobalt\({}^{2}\)  Eric Xing\({}^{4,5}\)  Shijian Lu\({}^{1}\)\({}^{*}\)**

\({}^{1}\)Nanyang Technological University \({}^{2}\)Max Planck Institute for Informatics

\({}^{3}\)University of Ottawa \({}^{4}\)Carnegie Mellon University \({}^{5}\)MBZUAI

Corresponding author

###### Abstract

Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at https://github.com/Kunhao-Liu/3D-OVS.

## 1 Introduction

Semantic segmentation of 3D scenes holds significant research value due to its broad range of applications such as robot navigation , object localization , autonomous driving , 3D scene editing , augmented/virtual reality, etc. Given the super-rich semantics in 3D scenes, a crucial aspect of this task is achieving open-vocabulary segmentation that can handle regions and objects of various semantics including those with long-tail distributions. This is a grand challenge as it necessitates a comprehensive understanding of natural language and the corresponding objects in the 3D world.

The main challenge in open-vocabulary 3D scene segmentation is the lack of large-scale and diverse 3D segmentation datasets. Existing 3D segmentation datasets like ScanNet  primarily focus on restricted scenes with limited object classes, making them unsuitable for training open-vocabulary models. An alternative is to distill knowledge from pre-trained 2D open-vocabulary segmentation models to 3D representations as learned with NeRF  or point clouds, by fitting the feature maps or segmentation probability outputs from the 2D models . Though this approach circumvents the need for the 3D datasets, it inherits the limitations of the 2D models which are usually finetuned with close-vocabulary datasets of limited text labels , thereby compromising the open-vocabulary property, especially for text labels with long-tail distributions .

We achieve precise and annotation-free 3D open-vocabulary segmentation by distilling knowledge from two pre-trained foundation models into NeRF in a weakly supervised manner, supervised only by the open-vocabulary text descriptions of the objects in a scene, as illustrated in Fig. 1. One foundation model is CLIP  which is trained with Internet-scale text-image pairs  capturing extensive open-vocabulary multimodal knowledge. The other is DINO  which is trained with large-scale unlabelled images capturing superb scene layout and object boundary information. However, CLIP yields image-level features which are not suitable for pixel-level semantic segmentation. Thus certain mechanisms should be designed to extract pixel-level CLIP features without fine-tuning. Additionally, the image patches' CLIP features may have ambiguities for segmentation, which need to be regularized for accurate open-vocabulary segmentation. At the other end, DINO produces feature maps instead of explicit segmentation maps. Certain distillation techniques should be designed to extract the necessary information from DINO features to facilitate precise segmentation.

We construct a hierarchical set of image patches to extract pixel-level features from image-level CLIP features and design a 3D _Selection Volume_ to identify the appropriate hierarchical level for each 3D point, effectively aligning CLIP features with pixel-level features without fine-tuning. In addition, we introduce a _Relevancy-Distribution Alignment (RDA)_ loss to address CLIP feature ambiguities, aligning segmentation probability distribution with class relevancies that capture similarities between class text features and corresponding CLIP features. Moreover, we propose a novel _Feature-Distribution Alignment (FDA)_ loss to distill object boundary information from DINO features. The FDA loss encourages close segmentation probability distributions for points with similar DINO features and distant distributions for dissimilar features. To address the training instability due to diverse distribution shapes, we further re-balance weights associated with similar and dissimilar DINO features.

Our method enables weakly supervised open-vocabulary segmentation of 3D scenes with accurate object boundaries. By distilling knowledge from CLIP without fine-tuning, our approach preserves its open-vocabulary knowledge and effectively handles text labels with long-tail distributions. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Remarkably, our experiments demonstrate that our method surpasses fully supervised models trained with segmentation annotations in certain scenes, highlighting the possibility that 3D open-vocabulary segmentation can be effectively learned from large amounts of 2D images and text-image pairs.

In summary, the contributions of this work are three-fold. _Firstly_, we propose an innovative pipeline for weakly supervised 3D open-vocabulary segmentation by distilling knowledge from pre-trained foundation models into NeRF without requiring any annotations in training. _Secondly_, we introduce a Selection Volume to align image-level CLIP features with pixel-level features, supplemented by novel Relevancy-Distribution Alignment and Feature-Distribution Alignment losses that respectively resolve CLIP features' ambiguities and effectively distill DINO features for 3D scene segmentation. _Lastly_, extensive experiments demonstrate that our method effectively recognizes long-tail classes and produces accurate segmentation maps, even with limited input data.

Figure 1: **Weakly Supervised 3D Open-vocabulary Segmentation. Given the multi-view images of a 3D scene and the open-vocabulary text descriptions, our method distills open-vocabulary multimodal knowledge from CLIP and object reasoning ability from DINO into the reconstructed NeRF, producing accurate object boundaries for the 3D scene without requiring any segmentation annotations during training.**

Related Work

Open-vocabulary Segmentation.In recent years, the field of 2D open-vocabulary segmentation has garnered significant attention, driven by the availability of extensive text-image datasets and vast computational resources. Predominant approaches [8; 14; 15; 16; 17; 18; 19] typically distill knowledge from large-scale pre-trained models, such as image-text contrastive learning models [20; 21; 10; 22] and diffusion models . However, the distillation process requires fine-tuning on close-vocabulary datasets, contrasting with massive datasets used for large-scale pre-trained models . This leads to limited performance in recalling infrequent classes with long-tail distributions [2; 3], compromising the open-vocabulary property. OpenSeg  is not finetuned on a closed set of classes but is weakly supervised via image captions. However, OpenSeg has a smaller vocabulary and knowledge than CLIP as it is trained on a much smaller dataset. Our method, without fine-tuning CLIP, effectively handles such classes.

3D Scenes Segmentation.3D scene segmentation has been a long-standing challenge in computer vision. Traditional approaches focus on point clouds or voxels with limited class variety in datasets, restricting generalizability to unseen classes [25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36]. Recently, numerous point-cloud-based techniques have emerged to explore open-vocabulary 3D scene segmentation by encoding 2D open-vocabulary models' features into 3D scene points [37; 38; 3; 39; 40]. However, these methods are also mostly evaluated on datasets with restricted scenes and limited class ranges [27; 28; 41; 5], not fully exhibiting the open-vocabulary property. Moreover, point clouds have compromised geometric details, making them less suitable for precise segmentation compared to NeRF representations [42; 6]. Consequently, there has been a surge in NeRF-based 3D segmentation techniques that mainly address interactive segmentation [43; 44; 4], panoptic segmentation [45; 46], moving part segmentation , object part segmentation , object co-segmentation , unsupervised object segmentation [50; 51], etc. FFD  attempts to segment unseen text labels during training by fitting LSeg's  feature maps to a NeRF, but inherits LSeg's limitations, hindering generalization to long-tail distribution classes. Our method overcomes these challenges by directly using CLIP image features and distilling them into a NeRF representation  without fine-tuning on close-vocabulary datasets.

Foundation Models.Pre-trained foundation models [52; 53] have become a powerful paradigm in computer science due to their ability to capture general knowledge and adapt to various downstream tasks [20; 23; 54; 55; 56; 57; 12; 58]. These models are trained using various paradigms in natural language processing, such as masked language modeling [57; 58], denoising autoencoder , replaced token detection , and sentence prediction tasks, as well as in computer vision, including data generation [62; 63; 63], data reconstruction , and data contrastive learning [20; 10; 12; 21; 13; 22]. Foundation models acquire emergent capabilities for exceptional performance on downstream tasks, either in a zero-shot manner or with fine-tuning. In this work, we harness the capabilities of two prominent foundation models, CLIP  and DINO [12; 13]. CLIP learns associations between images and texts by mapping them to a shared space, facilitating applications in tasks like image classification, object detection, visual question-answering, and image generation [65; 10; 12; 62; 66; 9]. DINO, trained in a self-supervised manner, extracts scene layout information, particularly object boundaries, and has been successfully employed in tasks such as classification, detection, segmentation, keypoint estimation, depth estimation, and image editing [12; 13; 49; 67; 68; 69].

## 3 Method

We propose a novel method for weakly supervised open-vocabulary segmentation of reconstructed NeRF. Given the multi-view images of a scene and the open-vocabulary text description for each class, we aim to segment the reconstructed NeRF such that every 3D point is assigned a corresponding class label.

To achieve this, we exploit the CLIP model's multimodal knowledge by mapping each 3D point to a CLIP feature representing its semantic meaning. As CLIP only generates image-level features, we extract a hierarchy of CLIP features from image patches and learn a 3D _Selection Volume_ for pixel-level feature extraction, as described in Sec. 3.1.

[MISSING_PAGE_FAIL:4]

\[S()=(_{i}T_{i}_{i}S_{i})^{N_{ x}},\] (2)

where \(C_{i},F_{i},S_{i}\) are the color, feature, and selection vector of each sampled point along the ray, \(T_{i}=_{j=0}^{i-1}(1-_{i})\) is the accumulated transmittance and \(_{i}=1-(-_{i}_{i})\) is the opacity of the point. We apply a Softmax function to the selection vector of each ray such that the sum of the probability of each scale is equal to 1.

For a set of rays \(\) in each training batch, the supervision loss can then be formulated as the combination of the L2 distance between rendered and ground truth RGB values and the cosine similarities \(,\) between the rendered features and the selected multi-scale CLIP features:

\[_{supervision}=_{}(\| ()-C()\|_{2}^{2}-(),S( )F()).\] (3)

Given a set of text descriptions \(\{_{i}\}_{i=1}^{C}\) of \(C\) classes and the CLIP text encoder \(E_{t}\), we can get the classes' text features \(T=E_{t}()^{C D}\). Then we can get the segmentation logits \(z()\) of the ray **r** by computing the cosine similarities between the rendered CLIP feature and the classes' text features:

\[z()= T,()^{C}.\] (4)

We can then get the class label of the ray \(l()=(z())\).

### Relevancy-Distribution Alignment for Ambiguity Mitigation

To mitigate the ambiguities of the CLIP features, we propose to align the segmentation probability distribution with the spatially normalized relevancy maps of each class, enabling our method to identify specific image regions described by each class text, as illustrated in Fig. 2. The segmentation probability of each ray \(P()\) can be derived from the segmentation logits with a Softmax function:

\[P()=(z())^{C}.\] (5)

The relevancy of a given class is determined by the similarity between the class's text feature and the selected feature from the hierarchy of image patches' CLIP features. Given an image \(I\), we can get its multi-scale pixel-level CLIP feature \(F_{I}^{N_{x} D H W}\) using Alg. 1 and selection vector \(S_{I}^{N_{s} H W}\) using Eq. (2). And then we can get the image's relevancy map \(R_{I}^{C H W}\) as:

\[R_{I_{hw}}=S_{I_{hw}} T,F_{I_{hw}},\] (6)

where where \(h,w\) denotes the index in the \(H\) and \(W\) channel. We normalize each class's relevancy independently within an input view to \(\) to mitigate the ambiguities of CLIP features, making our method discern image regions described by each class text:

\[_{I}=(R_{I}-(R_{I}))/((R_{I})- (R_{I}))^{C H W},\] (7)

Figure 2: **Mitigating CLIP features’ ambiguities with normalized relevancy maps.** For original relevancy maps \(r_{a},r_{b}\) of classes \(a\) and \(b\), we note a higher relevancy for class \(b\) in Region 2 than in other image regions. Despite this, the ambiguities of CLIP features lead to Region 2’s classification as \(a\) due to the higher absolute relevancy of \(a\) in Region 2, even as \(a\) is located in Region 1. To rectify this, we normalize each class’s relevancy maps to a fixed range. These normalized relevancy maps, \(}\) and \(}\), reduce such ambiguities, facilitating accurate region-class assignments.

where \(()\) and \(()\) are the functions getting the lowest and highest values across the spatial dimensions (i.e. \(H\) and \(W\)). We apply a Softmax function to \(_{I}\) to make it a probability vector. Then we can assign each ray **r** its normalized relevancy with all the classes \(()^{C}\). We employ the Jensen-Shannon (JS) divergence to measure the discrepancy between the normalized relevancy \(()\) and the segmentation probability distribution \(P()\) of each ray, formulating the Relevancy-Distribution Alignment (RDA) loss:

\[_{RDA}=_{}_{c C}(P()_{c}()_{c}}{M_{P}()_{c}} )+()_{c}(()_{c}} {M_{P}()_{c}}))/2,\] (8)

where \(M_{P}()=(P()+())/2\) is the average of the two distributions, and the subscript \(c\) denotes the probability of the \(c\)th class. By aligning the normalized relevancies and the segmentation probability distributions, our method can effectively identify the specific region corresponding to the text description of each class.

### Feature-Distribution Alignment for Precise Object Boundary Segmentation

To ensure the segmentation exhibits precise object boundaries, we align the segmentation probability distribution with the images' DINO features, which have been shown to capture superb scene layouts and object boundary information [12; 13]. Following [49; 68], we extract the scene layout information with a DINO feature correlation tensor. Given a patch of size \(H_{p} W_{p}\), we can get the correlation tensor \(Corr\_F^{H_{p}W_{p} H_{p}W_{p}}\) as:

\[Corr\_F_{hwij}=(f_{hw},f_{ij}),\] (9)

whose entries represent the cosine similarity between the DINO features \(f\) at spatial positions \((h,w)\) and \((i,j)\) of the patch. In order to construct the correlation tensor for the segmentation probability distribution, we propose utilizing the JS divergence to assess the similarity between segmentation probabilities at two distinct spatial positions. The choice of JS divergence offers several advantages, including its symmetric nature and a bounded range of \(\), which contribute to improved numerical stability. However, since we only care about the class label of each point, i.e. the entry with the highest probability, we use a low temperature \(<1\) to get a sharper version of the segmentation probability distribution \(\) to let the model focus on the entry with the largest probability:

\[=\;(z/)^{C}.\] (10)

The distribution correlation tensor \(Corr\_D^{H_{p}W_{p} H_{p}W_{p}}\) can thus be computed with:

\[Corr\_D_{hwij}=_{c C}(_{hwc}( _{hwc}}{M_{c}})+_{ijc}(_{ijc}}{M_{c}}))/2,\] (11)

where \(_{hwc},_{ijc}\) are the segmentation probabilities of the \(c\)th class at spatial locations \((h,w)\) and \((i,j)\) of the patch, \(M_{}=(_{hw}+_{ij})/2\) is the average of the two distributions. Thus the correlation loss  can be expressed as:

\[_{corr}=_{hwij}(Corr\_F_{hwij}-b) Corr\_D_{hwij},\] (12)

Figure 3: **Difference between similar and distant distributions.** Distributions having large divergence from the target distribution exhibit significantly diverse shapes, increasing the training instability (left). Conversely, distributions displaying low divergence with the target distribution consistently demonstrate a similar shape (right).

where \(b\) is a hyper-parameter denoting that we consider the segmentation probabilities of two spatial locations \((h,w)\) and \((i,j)\) to be similar if their DINO features' similarity is larger than \(b\) and distant if less than \(b\). Nonetheless, the correlation loss \(_{corr}\) introduces significant instability due to the diverse shapes of distributions with large divergence from a target distribution, making the loss assign wrong labels to the segmented parts. Conversely, when a distribution displays a low JS divergence with the target distribution, it consistently demonstrates a similar shape to the target distribution, as shown in Fig. 3. Based on this observation, we propose re-balancing the weights associated with similar and dissimilar DINO features. Specifically, we allocate a much greater weight to the correlation loss arising from similar DINO features and a smaller weight to that of dissimilar DINO features, thereby mitigating the instability caused by the correlation loss. Thus the Feature-Distribution Alignment (FDA) loss can be formulated with:

\[pos\_F=(Corr\_F-b,=0), neg\_F=(Corr \_F-b,=0),\] (13)

\[_{FDA}=_{pos}_{hwij}(pos\_F_{hwij} Corr\_D_{hwij} )/(pos\_F)+\\ _{neg}_{hwij}(neg\_F_{hwij} Corr\_D_{hwij})/ (neg\_F),\] (14)

where \((,=0)\) is to clamp all the elements smaller/greater than 0 to 0, thus making \(pos\_F 0\) and \(neg\_F 0\), \(()\) is to count the number of non zero elements, and \(_{pos},_{neg}\) are the weights associated with similar and dissimilar DINO features, which are set as \(_{pos}=200_{neg}=0.2\) by default.

The total training loss is: \(=_{supervision}+_{RDA}+_{FDA}\), where \(_{supervision},_{RDA}\) are calculated with randomly sampled rays and \(_{FDA}\) is calculated with randomly sampled patches.

## 4 Experiments

We evaluate our method on 3D open-vocabulary segmentation, showing that our method can recognize long-tail classes and produce highly accurate object boundaries even with limited input data. We employ TensoRF  as the backbone and extract 3 scales of pixel-level CLIP features. More implementation details and experiments are in the appendix.

Dataset.Existing 3D segmentation datasets predominantly focus on either restricted scenes with a narrow range of object classes [70; 5], or individual objects [71; 72; 73], thereby limiting their capacity to fully assess the task of 3D open-vocabulary segmentation. Thus following , we create a dataset comprising 10 distinct scenes. Each scene features a set of long-tail objects situated in various poses and backgrounds. Ground truth masks for the test views are manually annotated, enabling both qualitative and quantitative evaluation of our segmentation methods. We also evaluate our method on more diverse datasets which include human body, human head, indoor scenes with low-quality images [70; 74], and a complex scene from LERF datasets , as shown in the appendix.

Baselines.We benchmark our method with three NeRF-based methods capable of 3D open-vocabulary segmentation: FFD , Semantic-NeRF (abbreviated as Sem) , and LERF .

  & Methods & _bed_\\ **mel** \\  } & _lown_ \\ **Accuracy** \\  } & _room_ \\ **mol** \\  } & _bench_ \\ **Accuracy** \\  } & _tableU. } & _techn_ \\ **accarray{} } & _tableU. } & _techn_ \\ **accarray{} } \\    & LSeg  & 56.0 & 87.6 & 04.5 & 16.5 & 17.5 & 77.5 & 19.2 & 46.1 & 06.0 & 42.7 & 07.6 & 29.9 \\  & ODISE  & 52.6 & 86.5 & 48.3 & 35.4 & 39.8 & 82.5 & 52.5 & 59.7 & 24.1 & 39.0 & 39.7 & 34.5 \\  & OV-Seg  & 79.8 & 40.4 & 66.1 & 69.6 & 81.2 & 92.1 & 72.4 & 49.1 & 88.9 & 89.2 & 80.6 & 65.3 \\   & FFD  & 56.6 & 86.9 & 03.7 & 09.5 & 42.9 & 82.6 & 25.1 & 51.4 & 06.1 & 42.8 & 07.9 & 30.1 \\  & Sem(ODISE)  & 50.3 & 86.5 & 27.7 & 22.2 & 24.2 & 80.5 & 29.5 & 61.5 & 25.6 & 56.4 & 18.4 & 30.8 \\   & Sem(OV-Seg)  & 59.3 & 96.7 & 66.5 & 89.0 & 87.6 & **95.4** & 53.8 & 81.9 & **94.2** & **98.5** & 83.8 & **94.6** \\   & LERF  & 73.5 & 86.9 & 27.0 & 43.8 & 73.7 & 93.5 & 46.6 & 79.8 & 53.2 & 79.7 & 33.4 & 41.0 \\   & **Ours** & 89.5 & 96.7 & 74.0 & 91.6 & 88.2 & **97.3** & 92.8 & 98.9 & 89.3 & 96.3 & 88.8 & **96.5** \\  

Table 1: **Quantitative comparisons. We report the mIoU(\(\)) scores and the Accuracy(\(\)) scores of the following methods in 6 scenes and highlight the best, second-best, and third-best scores. Our method outperforms both 2D and 3D methods without any segmentation annotations in training.**FFD, a pioneering effort in 3D open-vocabulary segmentation, applies the feature maps of the 2D open-vocabulary segmentation method, LSeg , to achieve its segmentation results. For Sem, we adapt it to the open-vocabulary segmentation context by distilling segmentation results from two state-of-the-art 2D open-vocabulary segmentation models: the diffusion-model-based ODISE , and the CLIP-based OV-Seg . LERF closely aligns with our proposed approach due to its use of knowledge distillation from CLIP and DINO. However, its primary focus is on object localization rather than segmentation. We use the same scale level number and patch sizes in LERF for fair comparisons. We also include results obtained by independently segmenting each test view using the aforementioned 2D models. Note that under our settings, FFD, Sem are fully supervised methods using segmentation annotations.

### Results

We present the qualitative results in Fig. 4 and quantitative results in Tab. 1. Our proposed method outperforms all other techniques, including those which heavily rely on extensive segmentation annotations, such as LSeg, ODISE, OV-Seg. In particular, ODISE and FFD underperform in our evaluation, as they are unable to identify many long-tail classes, suggesting that the fine-tuning process of LSeg and ODISE may have led to a significant loss of the open-vocabulary knowledge originally encapsulated by CLIP and Stable Diffusion . OV-Seg attempts to retain CLIP's knowledge by leveraging a mined dataset, however, it requires a mask proposal model which produces inconsistent segmentation across views, making Sem(OV-Seg) produce noisy and imprecise segmentation. LERF

Figure 4: **Qualitative comparisons. Visualization of the segmentation results in 3 scenes. Our method successfully recognizes long-tail classes and produces the most accurate segmentation maps.**

also fails to capture precise object boundaries due to its usage of a relatively naive regularization loss, which fails to fully exploit the object boundary information within the DINO features. In contrast, our method exhibits robust performance, successfully recognizing long-tail classes and generating accurate and well-defined boundaries for each class. However, LERF allows querying any object without the need for running optimization again, which is an advantage over our method.

### Studies

Ablations.We conduct ablation studies to evaluate the individual contributions of the RDA loss and the FDA loss to the overall performance of our proposed method. As shown in Tab. 2, both RDA loss and FDA loss are crucial to our method, without each of which can result in severe performance degradation. As illustrated in Fig. 5, without the RDA loss, the model does not resolve the ambiguities of the CLIP features, leading to misclassifications. For instance, it fails to distinguish between an _orange cat_ and a _Portuguese egg tart_, and confuses a _mini offroad car_ with _wood_. Without the FDA loss, although our method can correctly locate each class, it fails to segment precise object boundaries. When discarding the re-balancing in the FDA loss, i.e. using the correlation loss , the model produces accurate boundaries but assigns each cluster the wrong label due to the instability brought by diverse distribution shapes.

Limited input.Given the substantial computational and storage demands of extracting hierarchical CLIP features for each view (exceeding 1GB for 3 scales in the full model), we explore whether reducing input CLIP features would yield similar results, as shown in Tab. 2 and Fig. 5. We test two modifications: reducing views for feature extraction and using a single scale of CLIP features rather than three. Halving the input views for feature extraction leads to negligible performance degradation (\(<1\%\)) and minor visual differences compared to the full model. When reducing to only \(10\%\) of input views, equivalent to 2-3 views in our dataset, we observe a modest \(9\%\) drop in the mIoU score and a \(1\%\) decrease in the Accuracy score, while retaining accurate segmentation across most classes. Using a single scale of CLIP features also only incurs minimal degradation (\(<1\%\)). Even under extreme conditions, i.e., extracting a single scale of features from \(10\%\) of input views (total only \(}{{30}}\) input of the full model), performance degradation is limited to \(10\%\). This efficient approach even outperforms LERF  which utilizes all input views and scales, highlighting our method's robustness.

## 5 Limitations

The limitations of our method are twofold. First, unlike LERF , our method requires text labels before training. To perform segmentation with new text labels, our method needs to be retrained.

  & **mIoU** & **Accuracy** \\   w/o RDA loss & 57.2 & 79.4 \\ w/o FDA loss & 58.2 & 82.7 \\ w/o re-balance & 44.9 & 74.3 \\ 
50\% views & 85.7 & **95.7** \\
10\% views & 79.1 & 94.6 \\ single scale & 85.2 & 95.5 \\ single \& 10\% & 77.1 & 94.6 \\ 
**full model** & 86.2 & 95.8 \\ 

Table 2: **Studies.** We report the mean mIoU(\(\)) scores and the Accuracy(\(\)) scores in our studies.

Figure 5: **Studies.** Visualization of the studies on ablations and limited input.

Inferring accurate boundaries with open vocabularies is challenging for implicit representations like NeRF, as NeRF learns a continuous representation rather than a discrete one. It is promising to learn object-level discrete representation using NeRF in future work.

Second, since our method has never seen any segmentation maps during training (only weakly supervised by the text labels), it fails to segment complex scenes like indoor datasets  with high precision, as shown in the appendix. Our method distills pixel-level CLIP features in a patch-based fashion with a strong inductive bias for compact objects with an aspect ratio close to 1. For objects with large complex shapes and unobvious textures, our method would fail to recognize them.

## 6 Conclusion

In this study, we address the challenge of 3D open-vocabulary segmentation by distilling knowledge from the pre-trained foundation models CLIP and DINO into reconstructed NeRF in a weakly supervised manner. We distill the open-vocabulary multimodal knowledge from CLIP with a Selection Volume and a novel Relevancy-Distribution Alignment loss to mitigate the ambiguities of CLIP features. In addition, we introduce a novel Feature-Distribution Alignment loss to extract accurate object boundaries by leveraging the scene layout information within DINO features. Our method successfully recognizes long-tail classes and produces precise segmentation maps, even when supplied with limited input data, suggesting the possibility of learning 3D segmentation from 2D images and text-image pairs.