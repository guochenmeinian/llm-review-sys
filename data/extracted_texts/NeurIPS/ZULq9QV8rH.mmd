# Self-Supervised Learning with Lie Symmetries for Partial Differential Equations

Gregoire Mialon

Meta, FAIR

&Quentin Garrido

Meta, FAIR

Univ Gustave Eiffel, CNRS, LIGM

&Hannah Lawrence

Meta, FAIR

MIT

&Danyal Rehman

MIT

&Yann LeCun

Meta, FAIR

NYU

&Bobak T. Kiani

MIT

Correspondence to: gmialon@meta.com, garridoq@meta.com, and bkiani@mit.edu, \({}^{}\) Equal contribution

###### Abstract

Machine learning for differential equations paves the way for computationally efficient alternatives to numerical solvers, with potentially broad impacts in science and engineering. Though current algorithms typically require simulated training data tailored to a given setting, one may instead wish to learn useful information from heterogeneous sources, or from real dynamical systems observations that are messy or incomplete. In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision. Our representation outperforms baseline approaches to invariant tasks, such as regressing the coefficients of a PDE, while also improving the time-stepping performance of neural solvers. We hope that our proposed methodology will prove useful in the eventual development of general-purpose foundation models for PDEs.

## 1 Introduction

Dynamical systems governed by differential equations are ubiquitous in fluid dynamics, chemistry, astrophysics, and beyond. Accurately analyzing and predicting the evolution of such systems is of paramount importance, inspiring decades of innovation in algorithms for numerical methods. However, high-accuracy solvers are often computationally expensive. Machine learning has recently arisen as an alternative method for analyzing differential equations at a fraction of the cost [1; 2; 3]. Typically, the neural network for a given equation is trained on simulations of that same equation, generated by numerical solvers that are high-accuracy but comparatively slow . What if we instead wish to learn from heterogeneous data, e.g., data with missing information, or gathered from actual observation of varied physical systems rather than clean simulations?

For example, we may have access to a dataset of instances of time-evolution, stemming from a family of partial differential equations (PDEs) for which important characteristics of the problem, such as viscosity or initial conditions, vary or are unknown. In this case, representations learned from such a large, "unlabeled" dataset could still prove useful in learning to identify unknown characteristics, given only a small dataset "labeled" with viscosities or reaction constants. Alternatively, the "unlabeled" dataset may contain evolutions over very short periods of time, or with missing time intervals; possible goals are then to learn representations that could be useful in filling in these gaps, or regressing other quantities of interest.

To tackle these broader challenges, we take inspiration from the recent success of self-supervised learning (SSL) as a tool for learning rich representations from large, unlabeled datasets of text and images [5; 6]. Building such representations from and for scientific data is a natural next step in the development of machine learning for science . In the context of PDEs, this corresponds to learning representations from a large dataset of PDE realizations "unlabeled" with key information (such as kinematic viscosity for Burgers' equation), before applying these representations to solve downstream tasks with a limited amount of data (such as kinematic viscosity regression), as illustrated in Figure 1.

To do so, we leverage the joint embedding framework  for self-supervised learning, a popular paradigm for learning visual representations from unlabeled data [9; 10]. It consists of training an encoder to enforce similarity between embeddings of two augmented versions of a given sample to form useful representations. This is guided by the principle that representations suited to downstream tasks (such as image classification) should preserve the common information between the two augmented views. For example, changing the color of an image of a dog still preserves its semantic meaning and we thus want similar embeddings under this augmentation. Hence, the choice of augmentations is crucial. For visual data, SSL relies on human intuition to build hand-crafted augmentations (e.g. recoloring and cropping), whereas PDEs are endowed with a group of symmetries preserving the governing equations of the PDE [11; 12]. These symmetry groups are important because creating embeddings that are invariant under them would allow to capture the underlying dynamics of the PDE. For example, solutions to certain PDEs with periodic boundary conditions remain valid solutions after translations in time and space. There exist more elaborate equation-specific transformations as well, such as Galilean boosts and dilations (see Appendix E). Symmetry groups are well-studied for common PDE families, and can be derived systematically or calculated from computer algebra systems via tools from Lie theory [11; 13; 14].

Contributions:We present a general framework for performing SSL for PDEs using their corresponding symmetry groups. In particular, we show that by exploiting the analytic group transformations from one PDE solution to another, we can use joint embedding methods to generate useful representations from large, heterogeneous PDE datasets. We demonstrate the broad utility of these representations on downstream tasks, including regressing key parameters and time-stepping, on simulated physically-motivated datasets. Our approach is applicable to any family of PDEs, harnesses the well-understood mathematical structure of the equations governing PDE data -- a luxury not typically available in non-scientific domains -- and demonstrates more broadly the promise of

Figure 1: A high-level overview of the self-supervised learning pipeline, in the conventional setting of image data (top row) as well as our proposed setting of a PDE (bottom row). Given a large pool of unlabeled data, self-supervised learning uses augmentations (e.g. color-shifting for images, or Lie symmetries for PDEs) to train a network \(f_{}\) to produce useful representations from input images. Given a smaller set of labeled data, these representations can then be used as inputs to a supervised learning pipeline, performing tasks such as predicting class labels (images) or regressing the kinematic viscosity \(\) (Burgers’ equation). Trainable steps are shown with red arrows; importantly, the representation function learned via SSL is not altered during application to downstream tasks.

adapting self-supervision to the physical sciences. We hope this work will serve as a starting point for developing foundation models on more complex dynamical systems using our framework.

## 2 Methodology

We now describe our general framework for learning representations from and for diverse sources of PDE data, which can subsequently be used for a wide range of tasks, ranging from regressing characteristics of interest of a PDE sample to improving neural solvers. To this end, we adapt a popular paradigm for representation learning without labels: the joint-embedding self-supervised learning.

### Self-Supervised Learning (SSL)

Background:In the joint-embedding framework, input data is transformed into two separate "views", using augmentations that preserve the underlying information in the data. The augmented views are then fed through a learnable encoder, \(f_{}\), producing representations that can be used for downstream tasks. The SSL loss function is comprised of a similarity loss \(_{}\) between projections (through a projector \(h_{}\), which helps generalization ) of the pairs of views, to make their representations invariant to augmentations, and a regularization loss \(_{}\), to avoid trivial solutions (such as mapping all inputs to the same representation). The regularization term can consist of a repulsive force between points, or regularization on the covariance matrix of the embeddings. Both function similarly, as shown in . This pretraining procedure is illustrated in Fig. 2 (left) in the context of Burgers' equation.

In this work, we choose variance-invariance-covariance regularization (VICReg) as our self-supervised loss function . Concretely, let \(,^{}^{N}\) contain the \(D\)-dimensional representations of two batches of \(N\) inputs with \(D D\) centered covariance matrices, \(()\) and \((^{})\). Rows \(_{i,:}\) and \(_{i,:}^{}\) are two views of a shared input. The loss over this batch includes a term to enforce similarity (\(_{}\)) and a term to avoid collapse and regularize representations (\(_{}\)) by pushing elements of the encodings to be statistically identical:

\[(,^{})}{N}^{N}\|_{i,:}-_{i,:}^{}\|_{2}^{2}}_{_{ }(,^{})}+}{D} \|()-\|_{F}^{2}+\|(^{ })-\|_{F}^{2}}_{_{}()+_{}(^{})},\] (1)

Figure 2: Pretraining and evaluation frameworks, illustrated on Burgers’ equation. **(Left)** Self-supervised pretraining. We generate augmented solutions \(\) and \(^{}\) using Lie symmetries parametrized by \(g\) and \(g^{}\) before passing them through an encoder \(f_{}\), yielding representations \(\). The representations are then input to a projection head \(h_{}\), yielding embeddings \(\), on which the SSL loss is applied. **(Right)** Evaluation protocols for our pretrained representations \(\). On new data, we use the computed representations to either predict characteristics of interest, or to condition a neural network or operator to improve time-stepping performance.

where \(\|\|_{F}\) denotes the matrix Frobenius norm and \(_{inv}\), \(_{reg}^{+}\) are hyperparameters to weight the two terms. In practice, VICReg separates the regularization \(_{reg}()\) into two components to handle diagonal and non-diagonal entries \(()\) separately. For full details, see Appendix C.

Adapting VICReg to learn from PDE data:Numerical PDE solutions typically come in the form of a tensor of values, along with corresponding spatial and temporal grids. By treating the spatial and temporal information as supplementary channels, we can use existing methods developed for learning image representations. As an illustration, a numerical solution to Burgers consists of a velocity tensor with shape \((t,x)\): a vector of \(t\) time values, and a vector of \(x\) spatial values. We therefore process the sample to obtain a \((3,t,x)\) tensor with the last two channels encoding spatial and temporal discretization, which can be naturally fed to neural networks tailored for images such as ResNets . From these, we extract the representation before the classification layer (which is unused here). It is worth noting that convolutional neural networks have become ubiquitous in the literature [18; 12]. While the VICReg default hyper-parameters did not require substantial tuning, tuning was crucial to probe the quality of our learned representations to monitor the quality of the pre-training step. Indeed, SSL loss values are generally not predictive of the quality of the representation, and thus must be complemented by an evaluation task. In computer vision, this is done by freezing the encoder, and using the features to train a linear classifier on ImageNet. In our framework, we pick regression of a PDE coefficient, or regression of the initial conditions when there is no coefficient in the equation. The latter, commonly referred to as the inverse problem, has the advantage of being applicable to any PDE, and is often a challenging problem in the numerical methods community given the ill-posed nature of the problem . Our approach for a particular task, kinematic viscosity regression, is schematically illustrated in Fig. 2 (top right). More details on evaluation tasks are provided in Section 4.

### Augmentations and PDE Symmetry Groups

Background:PDEs formally define a systems of equations which depend on derivatives of input variables. Given input space \(\) and output space \(\), a PDE \(\) is a system of equations in independent variables \(\), dependent variables \(:\), and derivatives \((_{},_{},)\) of \(\) with respect to \(\). For example, the Kuramoto-Sivashinsky equation is given by

\[(x,t,u)=u_{t}+uu_{x}+u_{xx}+u_{xxxx}=0.\] (2)

Informally, a symmetry group of a PDE \(G\)2 acts on the total space via smooth maps \(G:\) taking solutions of \(\) to other solutions of \(\). More explicitly, \(G\) is contained in the symmetry group of \(\) if outputs of group operations acting on solutions are still a solution of the PDE:

\[(,t,u)=0[g(,)]=0,\ \ \  g G.\] (3)

Figure 3: One parameter Lie point symmetries for the Kuramoto-Sivashinsky (KS) PDE. The transformations (left to right) include the un-modified solution \((u)\), temporal shifts \((g_{1})\), spatial shifts \((g_{2})\), and Galilean boosts \((g_{3})\) with their corresponding infinitesimal transformations in the Lie algebra placed inside the figure. The shaded red square denotes the original \((x,t)\), while the dotted line represents the same points after the augmentation is applied.

For PDEs, these symmetry groups can be analytically derived  (see also Appendix A for more formal details). The types of symmetries we consider are so-called Lie point symmetries \(g:\), which act smoothly at any given point in the total space \(\). For the Kuramoto-Sivashinsky PDE, these symmetries take the form depicted in Fig. 3:

\[& g_{1}(): &(x,t,u)(x,t+,u)\\ & g_{2}():& (x,t,u)(x+,t,u)\\ & g_{3}():& (x,t,u)(x+ t,t,u+)\] (4)

As in this example, every Lie point transformation can be written as a one parameter transform of \(\) where the transformation at \(=0\) recovers the identity map and the magnitude of \(\) corresponds to the "strength" of the corresponding augmentation.3 Taking the derivative of the transformation at \(=0\) with respect to the set of all group transformations recovers the Lie algebra of the group (see Appendix A). Lie algebras are vector spaces with elegant properties (e.g., smooth transformations can be uniquely and exhaustively implemented), so we parameterize augmentations in the Lie algebra and implement the corresponding group operation via the exponential map from the algebra to the group. Details are contained in Appendix B.

**PDE symmetry groups as SSL augmentations, and associated challenges:** Symmetry groups of PDEs offer a technically sound basis for the implementation of augmentations; nevertheless, without proper considerations and careful tuning, SSL can fail to work successfully . Although we find the marriage of these PDE symmetries with SSL quite natural, there are several subtleties to the problem that make this task challenging. Consistent with the image setting, we find that, among the list of possible augmentations, crops are typically the most effective of the augmentations in building useful representations . Selecting a sensible subset of PDE symmetries requires some care; for example, if one has a particular invariant task in mind (such as regressing viscosity), the Lie symmetries used should neither depend on viscosity nor change the viscosity of the output solution. Moreover, there is no guarantee as to which Lie symmetries are the most "natural", _i.e._ most likely to produce solutions that are close to the original data distribution; this is also likely a confounding factor when evaluating their performance. Finally, precise derivations of Lie point symmetries require knowing the governing equation, though a subset of symmetries can usually be derived without knowing the exact form of the equation, as certain families of PDEs share Lie point symmetries and many symmetries arise from physical principles and conservation laws.

**Sampling symmetries:** We parameterize and sample from Lie point symmetries in the Lie algebra of the group, to ensure smoothness and universality of resulting maps in some small region around the identity. We use Trotter approximations of the exponential map, which are efficiently tunable to small errors, to apply the corresponding group operation to an element in the Lie algebra (see Appendix B) [22; 23]. In our experiments, we find that Lie point augmentations applied at relatively small strengths perform the best (see Appendix E), as they are enough to create informative distortions of the input when combined. Finally, boundary conditions further complicate the simplified picture of PDE symmetries, and from a practical perspective, many of the symmetry groups (such as the Galilean Boost in Fig. 3) require a careful rediscretization back to a regular grid of sampled points.

## 3 Related Work

In this section, we provide a concise summary of research related to our work, reserving Appendix D for more details. Our study derives inspiration from applications of Self-Supervised Learning (SSL) in building pre-trained foundational models . For physical data, models pre-trained with SSL have been implemented in areas such as weather and climate prediction  and protein tasks [25; 26], but none have previously used the Lie symmetries of the underlying system. The SSL techniques we use are inspired by similar techniques used in image and video analysis [9; 20], with the hopes of learning rich representations that can be used for diverse downstream tasks.

Symmetry groups of PDEs have a rich history of study [11; 13]. Most related to our work,  used Lie point symmetries of PDEs as a tool for augmenting PDE datasets in supervised tasks. For some PDEs, previous works have explicitly enforced symmetries or conservation laws by for example constructing networks equivariant to symmetries of the Navier Stokes equation , parameterizingnetworks to satisfy a continuity equation , or enforcing physical constraints in dynamic mode decomposition . For Hamiltonian systems, various works have designed algorithms that respect the symplectic structure or conservation laws of the Hamiltonian [30; 31].

## 4 Experiments

Equations considered:We focus on flow-related equations here as a testing ground for our methodology. In our experiments, we consider the four equations below, which are 1D evolution equations apart from the Navier-Stokes equation, which we consider in its 2D spatial form. For the 1D flow-related equations, we impose periodic boundary conditions with \(=[0,L][0,T]\). For Navier-Stokes, boundary conditions are Dirichlet (\(v=0\)) as in . Symmetries for all equations are listed in Appendix E.

1. The **viscous Burgers' Equation**, written in its "standard" form, is a nonlinear model of dissipative flow given by \[u_{t}+uu_{x}- u_{xx}=0,\] (5) where \(u(x,t)\) is the velocity and \(^{+}\) is the kinematic viscosity.
2. The **Korteweg-de Vries (KdV)** equation models waves on shallow water surfaces as \[u_{t}+uu_{x}+u_{xxx}=0,\] (6) where \(u(x,t)\) represents the wave amplitude.
3. The **Kuramoto-Sivashinsky (KS)** equation is a model of chaotic flow given by \[u_{t}+uu_{x}+u_{xx}+u_{xxxx}=0,\] (7) where \(u(x,t)\) is the dependent variable. The equation often shows up in reaction-diffusion systems, as well as flame propagation problems.
4. The **incompressible Navier-Stokes** equation in two spatial dimensions is given by \[_{t}=-- p+^{2}+,=0,\] (8) where \((,t)\) is the velocity vector, \(p(,t)\) is the pressure, \(\) is the fluid density, \(\) is the kinematic viscosity, and \(\) is an external added force (buoyancy force) that we aim to regress in our experiments.

Solution realizations are generated from analytical solutions in the case of Burgers' equation or pseudo-spectral methods used to generate PDE learning benchmarking data (see Appendix F) [12; 18; 32]. Burgers', KdV and KS's solutions are generated following the process of  while for Navier Stokes we use the conditioning dataset from . The respective characteristics of our datasets can be found in Table 1.

**Pretraining:** For each equation, we pretrain a ResNet18 with our SSL framework for 100 epochs using AdamW , a batch size of 32 (64 for Navier-Stokes) and a learning rate of 3e-4. We then freeze its weights. To evaluate the resulting representation, we (i) train a linear head on top of our features and on a new set of labeled realizations, and (ii) condition neural networks for time-stepping on our representation. Note that our encoder learns from heterogeneous data in the sense that for a given equation, we grouped time evolutions with different parameters and initial conditions.

### Equation parameter regression

We consider the task of regressing equation-related coefficients in Burgers' equation and the Navier-Stokes' equation from solutions to those PDEs. For KS and KdV we consider the inverse problem of regressing initial conditions. We train a linear model on top of the pretrained representation for the downstream regression task. For the baseline supervised model, we train the same architecture, _i.e._ a ResNet18, using the MSE loss on downstream labels. Unless stated otherwise, we train the linear model for \(30\) epochs using Adam. Further details are in Appendix F.

**Kinematic viscosity regression (Burgers):** We pretrain a ResNet18 on \(10,000\) unlabeled realizations of Burgers' equation, and use the resulting features to train a linear model on a smaller, labeleddataset of only \(2000\) samples. We compare to the same supervised model (encoder and linear head) trained on the same labeled dataset. The viscosities used range between \(0.001\) and \(0.007\) and are sampled uniformly. We can see in Table 1 that we are able to improve over the supervised baseline by leveraging our learned representations. This remains true even when also using Lie Point symmetries for the supervised baselines or when using comparable dataset sizes, as in Figure 4. We also clearly see the ability of our self-supervised approach to leverage larger dataset sizes, whereas we did not see any gain when going to bigger datasets in the supervised setting.

**Initial condition regression (inverse problem):** For the KS and KdV PDEs, we aim to solve the inverse problem by regressing initial condition parameters from a snapshot of future time evolutions of the solution. Following , for a domain \(=[0,L]\), a truncated Fourier series, parameterized by \(A_{k},_{k},_{k}\), is used to generate initial conditions:

\[u_{0}(x)=_{k=1}^{N}A_{k}(x}{L}+_{k}).\] (9)

Our task is to regress the set of \(2N\) coefficients \(\{A_{k},_{k}:k\{1,,N\}\}\) from a snapshot of the solution starting at \(t=20\) to \(t=T\). This way, the initial conditions and first-time steps are never seen during training, making the problem non-trivial. For all conducted tests, \(N=10\), \(A_{k}(-0.5,0.5)\), and \(_{k}(-0.4,0.4)\). By neglecting phase shifts, \(_{k}\), the inverse problem is invariant to Galilean boosts and spatial translations, which we use as augmentations for training our SSL method (see Appendix E). The datasets used for KdV and KS contains 10,000 training samples and 2,500 test samples. As shown in Table 1, the SSL trained network reduces NMSE by a factor of almost three compared to the supervised baseline. This demonstrates how pre-training via SSL can help to extract the underlying dynamics from a snapshot of a solution.

**Buoyancy magnitude regression:** Following , our dataset consists of solutions of Navier Stokes (Equation (8)) where the external buoyancy force, \(=(c_{x},c_{y})^{}\), is constant in the two spatial directions over the course of a given evolution, and our aim is to regress the magnitude of this force \(^{2}+c_{y}^{2}}\) given a solution to the PDE. We reuse the dataset generated in , where \(c_{x}=0\) and \(c_{y}(0.2,0.5)\). In practice this gives us 26,624 training samples that we used as our "unlabeled" dataset, 3,328 to train the downstream task on, and 6,592 to evaluate the models. As observed in Table 1, the self-supervised approach is able to significantly outperform the supervised baseline. Even when looking at the best supervised performance (over 60 runs), or in similar data regimes as the supervised baseline illustrated in Fig. 4, the self-supervised baseline consistently performs better and improves further when given larger unlabeled datasets.

   Equation & KdV & KS & Burgers & Navier-Stokes \\  SSL dataset size & 10,000 & 10,000 & 10,000 & 26,624 \\  Sample format (\(t,x,(y)\)) & 256\(\)128 & 256\(\)128 & 448\(\)224 & 56\(\)128\(\)128 \\  Characteristic of interest & Init. coeffs & Init. coeffs & Kinematic viscosity & Buoyancy \\ Regression metric & NMSE (\(\)) & NMSE (\(\)) & Relative error \%(\(\)) & MSE (\(\)) \\  Supervised & 0.102 \(\) 0.007 & 0.117 \(\) 0.009 & 1.18 \(\) 0.07 & 0.0078 \(\) 0.0018 \\ SSL repr. + linear head & **0.033 \(\) 0.004** & **0.042 \(\) 0.002** & **0.97 \(\) 0.04** & **0.0038 \(\) 0.0001** \\  Timestepping metric & NMSE (\(\)) & NMSE (\(\)) & NMSE (\(\)) & MSE \( 10^{-3}()\) \\  Baseline & 0.508 \(\) 0.102 & 0.549 \(\) 0.095 & 0.110 \(\) 0.008 & 2.37 \(\) 0.01 \\ + SSL repr. conditioning & **0.330 \(\) 0.081** & **0.381 \(\) 0.097** & 0.108 \(\) 0.011 & **2.35 \(\) 0.03** \\   

Table 1: Downstream evaluation of our learned representations for four classical PDEs (averaged over three runs, the lower the better (\(\))). The normalized mean squared error (NMSE) over a batch of \(N\) outputs \(}_{k}\) and targets \(_{k}\) is equal to \(=_{k=1}^{N}\|}_{k}-_{k}\|_{ 2}^{2}/\|}_{k}\|_{2}^{2}\). Relative error is similarly defined as \(=_{k=1}^{N}\|}_{k}-_{k}\|_{ 1}/\|}_{k}\|_{1}\) For regression tasks, the reported errors with supervised methods are the best performance across runs with Lie symmetry augmentations applied. For timestepping, we report NMSE for KdV, KS and Burgers as in , and MSE for Navier-Stokes for comparison with .

[MISSING_PAGE_FAIL:8]

the former outperforms the latter in the PDE domain we consider. A possible explanation is that enforcing similar representations for two different views of the same solution forces the network to learn the underlying dynamics, while the supervised objectives (such as regressing the buoyancy) may not be as informative of a signal to the network. Moreover, Fig. 4 illustrates how more pretraining data benefits our SSL setup, whereas in our experiments it did not help the supervised baselines.

**Cropping:** Cropping is a natural, effective, and popular augmentation in computer vision [21; 39; 40]. In the context of PDE samples, unless specified otherwise, we crop both in temporal and spatial domains finding such a procedure is necessary for the encoder to learn from the PDE data. Cropping also offers a typically weaker means of enforcing analogous space and time translation invariance. The exact size of the crops is generally domain dependent and requires tuning. We quantify its effect in Fig. 5 in the context of Navier-Stokes; here, crops must contain as much information as possible while making sure that pairs of crops have as little overlap as possible (to discourage the network from relying on spurious correlations). This explains the two modes appearing in Fig. 5. We make a similar observation for Burgers, while KdV and KS are less sensitive. Finally, crops help bias the network to learn features that are invariant to whether the input was taken near a boundary or not, thus alleviating the issue of boundary condition preservation during augmentations.

**Selecting Lie point augmentations:** Whereas cropping alone yields satisfactory representations, Lie point augmentations can enhance performance but require careful tuning. In order to choose which symmetries to include in our SSL pipeline and at what strengths to apply them, we study the effectiveness of each Lie augmentation separately. More precisely, given an equation and each possible Lie point augmentation, we train a SSL representation using this augmentation only and cropping. Then, we couple all Lie augmentations improving the representation over simply using crops. In order for this composition to stay in the stability/convergence radius of the Lie Symmetries, we reduce each augmentation's optimal strength by an order of magnitude. Fig. 5 illustrates this process in the context of Navier-Stokes.

## 5 Discussion

This work leverages Lie point symmetries for self-supervised representation learning from PDE data. Our preliminary experiments with the Burgers', KdV, KS, and Navier-Stokes equations demonstrate the usefulness of the resulting representation for sample or compute efficient estimation of characteristics and time-stepping. Nevertheless, a number of limitations are present in this work, which we hope can be addressed in the future. The methodology and experiments in this study were confined to a particular set of PDEs, but we believe they can be expanded beyond our setting.

Figure 4: Influence of dataset size on regression tasks. **(Left)** Kinematic regression on Burger’s equation. When using Lie point symmetries (LPS) during pretraining, we are able to improve performance over the supervised baselines, even when using an unlabled dataset size that is half the size of the labeled one. As we increase the amount of unlabeled data that we use, the performance improves, further reinforcing the usefulness of self-supervised representations. **(Right)** Buoyancy regression on Navier-Stokes’ equation. We notice a similar trend as in Burgers but found that the supervised approach was less stable than the self-supervised one. As such, SSL brings better performance as well as more stability here.

Learning equivariant representations:Another interesting direction is to expand our SSL framework to learning explicitly equivariant features . Learning _equivariant_ representations with SSL could be helpful for time-stepping, perhaps directly in the learned representation space.

Preserving boundary conditions and leveraging other symmetries:Theoretical insights can also help improve the results contained here. Symmetries are generally derived with respect to systems with infinite domain or periodic boundaries. Since boundary conditions violate such symmetries, we observed in our work that we are only able to implement group operations with small strengths. Finding ways to preserve boundary conditions during augmentation, even approximately, would help expand the scope of symmetries available for learning tasks. Moreover, the available symmetry group operations of a given PDE are not solely comprised of Lie point symmetries. Other types of symmetries, such as nonlocal symmetries or approximate symmetries like Lie-Backlund symmetries, may also be implemented as potential augmentations .

Towards foundation models for PDEs:A natural next step for our framework is to train a common representation on a mixture of data from different PDEs, such as Burgers, KdV and KS, that are all models of chaotic flow sharing many Lie point symmetries. Our preliminary experiments are encouraging yet suggest that work beyond the scope of this paper is needed to deal with the different time and length scales between PDEs.

Extension to other scientific data:In our study, utilizing the structure of PDE solutions as "exact" SSL augmentations for representation learning has shown significant efficacy over supervised methods. This approach's potential extends beyond the PDEs we study as many problems in mathematics, physics, and chemistry present inherent symmetries that can be harnessed for SSL. Future directions could include implementations of SSL for learning stochastic PDEs, or Hamiltonian systems. In the latter, the rich study of Noether's symmetries in relation to Poisson brackets could be a useful setting to study . Real-world data, as opposed to simulated data, may offer a nice application to the SSL setting we study. Here, the exact form of the equation may not be known and symmetries of the equations would have to be garnered from basic physical principles (e.g., flow equations have translational symmetries), derived from conservation laws, or potentially learned from data.