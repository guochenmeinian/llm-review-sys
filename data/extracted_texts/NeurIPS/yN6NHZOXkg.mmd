# Generalized Information-theoretic Multi-view Clustering

Weitian Huang

School of Computer Science and Engineering

South China University of Technology

Guangzhou, 510006, China

&Sirui Yang

School of Computer Science and Engineering

South China University of Technology

Guangzhou, 510006, China

&Hongmin Cai

School of Computer Science and Engineering

South China University of Technology

Guangzhou, 510006, China

Hongmin Cai is the Corresponding Author, whose E-mail is: hmcai@scut.edu.cn.

###### Abstract

In an era of more diverse data modalities, multi-view clustering has become a fundamental tool for comprehensive data analysis and exploration. However, existing multi-view unsupervised learning methods often rely on strict assumptions on semantic consistency among samples. In this paper, we reformulate the multi-view clustering problem from an information-theoretic perspective and propose a general theoretical model. In particular, we define three desiderata under multi-view unsupervised learning in terms of mutual information, namely, _comprehensiveness_, _concentration_, and _cross-diversity_. The multi-view variational lower bound is then obtained by approximating the samples' high-dimensional mutual information. The Kullback-Leibler divergence is utilized to deduce sample assignments. Ultimately the information-based multi-view clustering model leverages deep neural networks and Stochastic Gradient Variational Bayes to achieve representation learning and clustering simultaneously. Extensive experiments on both synthetic and real datasets with wide types demonstrate that the proposed method exhibits more stable and superior clustering performance than state-of-the-art algorithms.

## 1 Introduction

Multiple views of data are often collected by different measurement methods to represent various properties of objects in many practical applications. For example, for images or videos, various visual features such as HOG , SIFT , and LBP  can be acquired by traditional filters and form a kind of multi-view data. These features can be collected from different domains or generated from various sensors. In general, different views can complement each other due to limitations or biases in measurement methods that would result in insufficient information contained in individual views .

The ever-increasing network activity brings massive unlabeled data, which makes clustering, the traditional task of machine learning, one of the hot topics in unsupervised learning. Importantly, in clustering task, learning the comprehensive and discriminative representations is crucial for partitioning the samples. Compared to single-view clustering, multi-view clustering (MVC) is widely studied as it exploits complementary information between different views to improve the ultimate performance. Most multi-view clustering algorithms learn the consistent representation among multiple views to realize clustering. The benchmark techniques include view co-regularization ,Canonical Correlation Analysis (CCA) [6; 7] and low-rank tensor decomposition . Although different methods have achieved superior clustering performance, they lack a unified theoretical framework to help us further understand multi-view clustering.

Recently, Information Bottleneck (IB)  have attracted increasing attention in deep neural network research. The information bottleneck is an information theory principle that helps explain the puzzling success of today's artificial intelligence algorithms. This motivates us to explain multi-view learning using information bottleneck theory. In this paper, we extend from supervised information bottleneck theory to unsupervised learning, and construct a general information-theoretic multi-view clustering framework. The main contributions of our work are illustrated as follows,

* In unsupervised scheme, we define three requirements for multi-view representation learning from the view of information theory, _comprehensiveness_, _concentration_, and _cross-diversity_. Consequently, a general information-based multi-view clustering model (IMC) is proposed to achieve representation learning and clustering simultaneously.
* We present a multi-view VAE scheme to solve IMC by leveraging deep neural networks and Stochastic Gradient Variational Bayes (SGVB). Specifically, the multi-view mutual information is approximated by a variational lower bound, which can be expressed as a unified multi-view representation obtained through multiple variational autoencoders and sampling, followed by a self-training strategy for clustering.
* Experimental results show that the proposed method is effective in learning informative yet minimal representations and exhibits satisfactory generalization ability.

## 2 Variants of Information Bottleneck

### Information bottleneck

The information bottleneck principle  aims to obtain the most compressed representation of data while retaining task-relevant information. To achieve this, for given input data \(\) with label \(\), the corresponding codeword \(\) decided by \(p(|)\) can be obtained by optimizing the objective of the information bottleneck,

\[_{p(|):I(;) I^{*}}I(;)=_{}I(;)- I(; ):=_{IB}.\] (1)

where \(I(;)\) represents the mutual information between two random variables \(\) and \(\), which implicitly measures the level of compression in the representation \(\). \(_{IB}\) is to identify the optimal encoding \(p(|)\) that minimizes \(I(;)\) while ensuring \(I(;)\) exceeds a minimal threshold value \(I^{*}\). This constrained problem can then be transformed into a non-constraint problem by incorporating a trade-off factor \(\) that balances \(I(;)\) and \(I(;)\). Such scheme yields sufficient and minimal representations that maximize task-relevant information while minimizing task-irrelevant information, thus balancing prediction and generalization performance.

### Unsupervised information bottleneck

When the sample labels are unavailable, our aim is to compress each sample's description to the greatest extent possible while simultaneously maximizing the retention of the principal component information of the data. The unsupervised information bottleneck objective is described in ,

\[_{}I(;)- I(;i) {}_{p()}[_{p(|)} [ q(|)]- D_{KL}(p(|)||q())]:=_{UIB}.\] (2)

where \((a)\) forms the lower bound of \(_{UIB}\), which is achieved by substituting the intractable posterior \(p(|)\) and marginal \(p()\) with respective variational posterior \(q(|)\) and marginal \(q()\). Note that the lower bound is essentially identical to \(\)VAE . The detailed derivation process is shown in Appendix A.

In clustering tasks, one aims to discover the underlying structure of data and divide it into distinct clusters. The current deep clustering methods firstly extract deep low-dimensional embeddings and then apply standard clustering techniques. Theoretically, the deep clustering method is expected to follow the information bottleneck principle while preserving the original data clustering structure. Therefore, one can tailor Eq. (2) to realize deep clustering,

\[_{}I(;)- I(;i)\] \[ _{p()}[_{p(|)}[ q(|)]- D_{KL}(p(| )||q())-_{p(|)}[D_{KL }(p(|)||q(|))]]\] \[:= _{IBC}=_{UIB}-_{p()}[_{p(|)}[D_{KL}(p(|) ||q(|))]].\] (3)

Here we introduce a discrete variable \(\) to represent the cluster and denote \(p(|)\) as the cluster distribution implied by the data. We aim for the cluster inferred by embedding \(\) to be close to the original cluster distribution. To measure this, we add KL divergence term with the balance factor \(\). It is worth noting that the gap between \(_{IBC}\) and \(_{UIB}\) lies in the preservation of the clustering structure in the process of information compression.

**Connection to DEC**. Deep Embedding for Clustering (DEC)  begins with pre-training an auto-encoder to learn low-dimensional representations, and the soft cluster assignment computed by the embeddings (as \(q(|)\) in Eq. (3)) is determined by the Student's \(t\)-distribution. The clustering loss is designed via KL divergence to approach the auxiliary target distribution (as \(p(|)\) in Eq. (3)). By setting \(=0\) and \(=1\), and using the pre-training strategy, we obtain the DEC model.

**Connection to VaDE**. Variational Deep Embedding (VaDE)  combines VAEs and Gaussian mixture models and encodes the initial data distribution as a learned Gaussian mixture distribution in the latent variable space. By setting \(=1\) and \(=1\), we get the VaDE model.

## 3 Information-based Multi-view Clustering

Let \(}=\{^{(v)}\}_{v=1}^{m}\) be a dataset consisting of \(m\) views, where each view collection comprises \(n\) independent and identically distributed (i.i.d) samples, \(^{(v)}=\{_{1}^{(1)},_{2}^{(2)},...,_{ n}^{(v)}\}\). Consider \(}=\{^{(v)}\}_{v=1}^{m}\) presenting view-peculiar compressed representations for \(}\) and \(\{1,...,K\}\) designating a categorical random variable and represent the index of the actual cluster. For multi-view clustering, we first learn a unified latent representation \(\) across all views, which satisfies comprehensiveness, concentration and cross-diversity. Additionally, the multi-view representation is needed to preserve the original data structure information related to clusters followed by performing clustering.

### Unified multi-view representation learning

We first define the desiderata for multi-view representation as follows.

**Definition 3.1** (**Comprehensive, concentrative and cross-diverse multi-view representation)**.: Given the multi-view observations \(}\), a representation, \(\), is comprehensive if each view observation can be predicted by \(\),

\[^{*}=_{}I(;}).\] (4)

**Concentration**. For each view data \(^{(v)}\), their view-peculiar compressed representation \(^{(v)}\) is concentrative if the redundant information for each view is eliminated,

\[^{(v)*}=_{^{(v)}}I(^{(v)};^{(v )}).\] (5)

**Cross-diversity**. View-peculiar latent representations \(}\) can be cross-diverse if they avoid duplication of information within the view,

\[}^{*}=_{}}I(;}).\] (6)

To more intuitively grasp the definition of multi-view representations, we depict the three requirements for representation learning from the perspective of information theory. As illustrated in Fig 1, taking dual-view data as an example, the information carried by \(^{(1)}\) and \(^{(2)}\) are \(H(^{(1)})\) and \(H(^{(2)})\) respectively, which are fixed. To effectively convey the combination of the two views, the unified representation \(\) achieves informativeness by maximizing the mutual information between \(\) and \(^{(1)}^{(2)}\). Additionally, to remove redundant information from each view, we minimize \(I(^{(1)};^{(1)})\) and \(I(^{(2)};^{(2)})\) respectively for maximum compression of the respective view information. Ideally, semantically irrelevant information specific to each view is removed while complete semantically relevant information is retained.

To balance comprehensiveness and concentration, we introduce mutual information between \(\) and \(^{(1)}^{(2)}\) as a cross-diversity indicator,

\[I(;^{(1)},^{(2)})=I(;^{(1)}) +I(;^{(2)})-[I(^{(1)};^{(2)})-I (^{(1)};^{(2)}|)],\] (7)

where we denote \(D_{}(^{(1)};^{(2)})=I(^{(1)};^{(2)})-I(^{(1)};^{(2)}|)\), representing the duplicate information between views. As showed in Fig 1 (c), one can approach cross-diversity by maximizing \(I(;^{(1)})\), \(I(;^{(2)})\) and minimizing \(D_{}(^{(1)};^{(2)})\).

**Proposition 3.1** (**Cross-diversity is sufficient for comprehensiveness and concentration, respectively**).: _Given two Markov chains \(^{(1)}^{(1)}\) and \(^{(2)}^{(2)}\), maximization of \(I(;^{(1)},^{(2)})\) is sufficient for maximization of \(I(;^{(1)},^{(2)})\) when \(^{(1)},^{(2)}\) are fixed and minimization of \(_{v=1}^{2}I(^{(v)};^{(v)})\) when \(\) is fixed, respectively._

Proof.: Under the two Markov chains accompanied by loss of information, we have,

\[I(;^{(1)},^{(2)})I(^{(1)};^{(1)})+I(^{(2)};^{(2)}),\]

Next we rewrite the cross-diversity indicator,

\[I(;^{(1)},^{(2)}) =H(^{(1)},^{(2)})-H(^{(1)},^{(2)}|)\] \[=H(^{(1)})+H(^{(2)})-I(^{(1)}; ^{(2)})-H(^{(1)},^{(2)}|)\] \[ H(^{(1)})+H(^{(2)}),\]

Since \(I(;^{(1)},^{(2)})\) achieves the maximum value, i.e. \(I(;^{(1)},^{(2)})=H(^{(1)})+H(^{(2)})\). We denote \(I(;^{(1)},^{(2)})\) as \(\),

\[I(;^{(1)},^{(2)}) =I(;^{(1)},^{(2)})\] \[=H()-H(|^{(1)},^{(2)})\] \[H(^{(1)})+H(^{(2)})-H( ^{(1)},^{(2)}|^{(1)},^{(2)})\] \[=H(^{(1)})-H(^{(1)}|^{(1)})+H( ^{(2)})-H(^{(2)}|^{(2)})\] \[=I(^{(1)};^{(1)})+I(^{(2)}; ^{(2)}),\]

Figure 1: Illustration of three desiderata for learning multi-view representations.

To satisfy both inequalities \((b)\) and \((c)\), the equation holds, i.e., \(I(;^{(1)},^{(2)})=I(^{(1)};^{( 1)})+I(^{(2)};^{(2)})\). Therefore, when \(I(;^{(1)},^{(2)})\) is maximized, fixing \(^{(1)},^{(2)}\), \(I(;^{(1)},^{(2)})\) maximization holds, and conversely, fixing \(\), \(I(^{(1)};^{(1)})+I(^{(2)};^{(2)})\) minimization holds. 

With these desiderata, the unified multi-view representation learning can be formulated as follows,

\[_{,} I(;})-_{v}^{m}I(^{(v)}; ^{(v)})+ I(;})\] \[}{{}}_{p(})}[_{p(}|})}[ q(}|)]-_{v} ^{m}D_{KL}(p(^{(v)}|^{(v)})||q(^{(v)}))]+  I(;}),\] (8)

where \((d)\) leverages the variational technique to construct the lower bound of the first two terms (deviation can be found in Appendix B), the \(\) is the balance coefficient that controls the cross-diversity of representation and balances the comprehensiveness and concentration. Mutual information between \(\) and multiple representations \(^{(1)},...,^{(m)}\) can further be expressed by,

\[I(;})=_{v=1}^{m}I(;^ {(v)})-_{v=1}^{m}_{t v}^{m}D_{}(^{(v)}, ^{(t)}).\] (9)

### Generalized multi-view clustering framework

For the multi-view clustering task, previous works [14; 15] considered category-related or semantic-related information to be relevant only to the multi-view consensus part. As shown in Fig. 2 (a), the part where two views intersect contains all the information related to cluster \(\) in the multi-view data. However, this is only a special case. More generally, the respective views contain specific semantic information that can complement each other. This is why a comprehensive understanding of an object or event requires different views. As shown in Fig. 2 (b), we cannot determine if the view-common or view-peculiar parts of the views contain more semantic information, we can only assume that they all contain this information.

Next, we seek to learn the unified multi-view representation without losing the original clustering structure of the data. For examples, we assume that the data obeys a Gaussian mixture distribution, or that the data has a tight intra-class structure and a relaxed inter-class structure. We add a clustering term to Equation (8) for measuring the distance between the posterior distribution of a given original

Figure 2: (a) Cluster information is only related to common information between multiple views. (b) View-peculiar information also includes semantic features. (c) Multi-view variational autoencoders and a fusion network are incorporated to perform representation learning and clustering simultaneously.

data to clusters and the posterior distribution of a given multi-view representation to clusters. Then, the objective of information-based multi-view clustering can be formulated as,

\[_{IMC}= _{p(})}_{p( }|})}[ q(}| })]}_{}-^{m}D_{KL}(p(}^{( v)}|}^{(v)})||q(}^{(v)}))}_{}\] \[-_{p(}|} )}[D_{KL}(p(|})||q(|}))]}_ {}+};})}_{ }.\] (10)

Note that Equation (10) can be regarded as four items representing specific roles: data reconstruction, multi-regularization, information shift and clustering. The data reconstruction item maximizes the expectation of the log-likelihood to achieve comprehensive so that the multi-view representation retain more information of the data. The multi-regularization term makes the multi-view representation forget the information of the original data by minimizing the KL divergence of the posterior distribution and the variational distribution to meet the requirement of concentration. Information shift item adjusts the information relationship between multi-view representations to achieve cross-diverse. The clustering item measures the degree to which the data retains the original data structure after dimensionality reduction.

### Numerical scheme to solve IMC

Following VAE , we use a deep neural network to estimate the parameters of the unknown distribution to instantiate IMC, as shown in Fig. 2 (c). According to the Markov chain \(}^{(v)}\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\( \(_{^{(v)}}\) and \(_{^{(v)}}\) are the outputs of the \(v\)-th encoder, \(_{}\) and \(_{}\) are the outputs of the fusion module, respectively.

By using Monte-Carlo estimators, the objective of IMC can be further written as,

\[_{\{^{(v)}\},\{^{(v)}\},,}_ {}= _{i=1}^{n}[_{v=1}^{m}_{t=1 }^{T} q_{^{(v)}}(_{i}^{(v)}|_{it})- p_{ ^{(v)}}(_{it}^{(v)}|_{i}^{(v)}).\] \[+(,)-_{k=1}^{ K}p_{ik}}{q_{ik}}]+ I_{JS}(;}),\] (14)

where \(T\) denotes the number of Monte Carlo samples and is usually set to be 1. Additionally, variational distribution \(q(^{(v)})\) is set to multivariate standard normal distribution, i.e., \(q(^{(v)})=(,)\). The mutual information between representations \(I(;})\) can be maximized by using sample-based differentiable mutual information lower bound, Jensen-Shannon estimator . The partial derivatives of each parameter combination are then calculated for the stochastic backpropagation technique. The concrete optimization procedure of IMC is summarized in Algorithm 1.

```
0: Multi-view dataset \(}\); Setting \(T=1\) and dimensionality of the latent variables.
0: Initialize parameters \(\{^{(v)}\}\), \(\{^{(v)}\}\), \(\) with random values, cluster centers \(=\{_{1},...,_{K}\}\) by \(K\)-means.
1:while not reaching the maximal epochs do
2:for\(i\) in \(n\) samples do
3:for\(v\) in \(m\) views do
4: Calculate \((_{^{(v)}},\ _{^{(v)}})\) through \(v\)-th encoder and then sample \(_{it}^{(v)}\) by Equation (12);
5:endfor
6: Calculate \((_{},\ _{})\) through the fusion module and then sample \(_{it}\) by Equation (13);
7:for\(v\) in \(m\) views do
8: Generate \(^{(v)}\) by \(v\)-th decoders.
9:endfor
10:endfor
11: Update \(\{^{(v)}\}\), \(\{^{(v)}\}\), \(\), \(\) by maximizing Equation (14).
12:endwhile ```

**Algorithm 1** Optimization Procedure of IMC

## 4 Related Work

Recently, several works have been devoted to the problem of multi-view representation learning based on information theory. For supervised methods,  analyses the consistency and complementary characteristics of multi-view features from the information bottleneck theory, and strengthen the discriminative power of the encoder through the margin maximization method to balance the accuracy and complexity of the multi-view model.  seeks to fuse the features of each view to obtain a joint representation through a deep neural network, and maximize the mutual information between labels and joint representations to preserve the target information. Given a graph and its labels,  proposes a Graph Information Bottleneck (GIB) framework to efficiently infer the most informative yet compressed subgraph for recognition.

While in the unsupervised scheme,  proposes a multi-view representation learning method based on information bottleneck theory, Multi-view Information Bottleneck (MIB). MIB assumes that the shared part of the two views is the target information, and the part not shared by the two views is the superfluous information, followed by leveraging off-the-shelf information bottleneck technique to learn robust representations.  explores the potential relationship and intrinsic information between different views, and discarded redundant information from multi-view data with the help of the information bottleneck principle to balance the consistency and complementarity between multiple views. Under the framework of information theory, Completer  maximizes multi-view shared mutual information for consistent learning, and minimizes conditional entropy to recover missing view with available data.  regards each view source data as a self-supervised signal for learning latent representations, removing redundant information from the view itself while retaining consistent information for shared features across multiple views. A multivariate mutual information term is employed to decompose view-specific representations, which further ensures that the multi-view fusion self-expression matrix via view-common features is compact and informative. The aforementioned unsupervised multi-view representation learning methods aim to learn consistent information shared across multiple views, as shown in Fig. 2 (a). These methods either have limitations in practical applications or do not consider the duplicate information among views. It motivates us to construct a general information-theoretic-based unsupervised multi-view clustering framework and introduce three requirements for multi-view representation learning.

## 5 Experiments

### Experimental settings

**Model setup**: The architectures of \(p_{^{(v)}}(^{(v)}|^{(v)})\) and \(q_{^{(v)}}(^{(v)}|)\) are fully connected networks with with \(d_{v}\)-500-500-1204-256 and 256-1024-500-500-\(d_{v}\) neurons, respectively, where \(d_{v}\) is the dimensionality of each view. The fusion network \(p_{(v)}(|\{^{(v)}\}_{v=1}^{m})\) concatenates multiple view-peculiar latent representations, followed by a fully connected layer. Adam optimizer  is utilized to maximize the objective function, and set the learning rate to be 0.001 with a decay of 0.9 for every 10 epochs.

**Datasets**: We adopt four real-world datasets listed as follows, (1) **UCI-digits** contains 2000 examples of ten numerals from 0 to 9 with five views which are respectively extracted by Fourier coefficients, profile correlations, Karhunen-Love coefficient, Zernike moments, and pixel average extractors. (2) **Notting-Hill** is widely used video face dataset for clustering, which collects 4660 faces across 76 tracks of the 5 main actors from the movie 'Notting Hill'. We use the multi-view version provided in , consisting of 550 images with three kind of features, i.e., LBP, gray pixels, and Gabor features. (3) **BDGP** contains 2500 images in 5 categories, and each sample is described by a 1750-D image vector and a 79-D textual feature vector. (4) **Caltech20** is a subset of the object recognition dataset  containing 20 classes with six different views, including Gabor features, wavelet moments, CENTRIST features, histogram of oriented gradients, GIST features and local binary patterns.

**Compared algorithms**: Four state-of-the-art algorithms and two ablation models are used to compare the clustering performance, including: (1) **DMVAE** assumes that the data and multi-view latent representation obey a Gaussian mixture distribution, which can be considered as a special case of IMC, i.e., let \(=0\) and \(=1\). (2) **MIB** considers the parts shared by both views as target information and the view-specific parts as superfluous information. (3) **CMIB-Nets** proposes collaborative multi-view information bottlenecks to learn the information inherent within a view and the shared structure between views, while reducing redundant information. (4) **Completer** introduces maximization of mutual information between views and minimization of conditional entropy of different views to achieve dual prediction and representation learning. For fairness, we repeatedly perform all compared algorithms 10 times on four datasets and the mean and standard deviation were recorded.

**Ablation models**: For ablation study, we construct two variants of IMC as a comparison. (1) **IMC-v1** indicates that the objective function Equation (10) reduces the item of information shift, i.e., \(=0\). (2) **IMC-v2** sets \(=0\) to discard the KL divergence of the clustering item, and uses \(k\)-means to perform clustering on the unified multi-view representation.

**Evaluation metrics**: For a comprehensive analysis, we use three popular clustering metrics including Normalized Mutual Information (NMI), Accuracy (ACC) and adjusted rand index (ARI). The higher the values of these indicators, the better the clustering performance.

### Performance evaluation

We tested seven methods on four multi-view datasets. The experimental results are summarized in Table 1. It can be observed that i) combining the four datasets in terms of three indicators, IMC has obtained the best performance 8 times and the second-best performance 3 times, which is the most stable and accurate multi-view clustering model. ii) Although the Completer model has achieved the best NMI performance in caltech20, its NMI performance lags behind by 12.97% in BDGP. This instability can also be found in the MIB model. iii) The end-to-end model (IMC, IMC-v1, DMVAE) that considers the clustering loss in the process of learning multi-view representation will achieve better clustering results than the two-stage model (MIB, CMIB-nets, Completer). iv) Comparing IMC, IMC-v1 and IMC-v2, the performance of using KL divergence clustering term has been significantly improved, while the use of information shift term has a small improvement.

We therefore draw three conclusions: First, the IMC developed based on the general definition of multi-view information theory obtains more stable clustering results. Second, end-to-end models help to learn more discriminative representations. Third, in the process of learning multi-view representations, three requirements (comprehensiveness, concentration, cross-diversity) play a role in improving the clustering performance.

### Robustness Assessment

To evaluate the robustness of the model, we add different scales of sparse noise to the dataset UCI-digits. Specifically, we randomly select 10%, 30%, and 50% pixels of the original image size, and add pepper noise or salt noise to each pixel with probability 0.5, i.e., set 0 or 255.

As shown by the clustering results in Table 2, the performance of all algorithms decreases as the noise ratio increases, but the performance of our algorithm is always the best. It can be observed that the decline of IMC is the smallest, while the decline of the Completer model is obvious. This may be due to the assumption of multi-view semantic consistency is more sensitive as the noise ratio increases, the information shared by multiple views will also incorporate more noise. It is worth noting that the performance of IMC-v2 is better than that of IMC-v1, which is contrary to the clustering experiment in Table 1. It can be concluded that only pursuing inter-view consistent information leads to semantic bias when the dataset contains noise, while more informative representations can be obtained for robustness based on the general multi-view information bottleneck theory.

### Parameter analysis

The proposed method contains two balance parameters, the information shift balance coefficient \(\), and the clustering loss trade-off parameter \(\). Although IMC at fixed values of some parameters has

   Datasets & Metrics & DMVAE & MIB & CMIB-Nets & Completer & IMC-v1 & IMC-v2 & IMC \\   & ACC & 90.95\(\)0.62 & 83.30\(\)1.27 & 85.70\(\)1.15 & 91.28\(\)1.41 & 90.01\(\)0.60 & 84.01\(\)1.15 & **92.13\(\)0.55** \\  & NMI & 85.54\(\)1.06 & 75.43\(\)1.04 & 78.31\(\)1.41 & 86.34\(\)0.60 & 84.79\(\)0.32 & 79.01\(\)1.54 & **88.01\(\)0.73** \\  & ARI & 85.40\(\)1.54 & 76.16\(\)1.55 & 76.97\(\)1.64 & 86.67\(\)0.86 & 84.78\(\)0.43 & 78.18\(\)0.88 & **87.83\(\)0.25** \\   & ACC & 76.22\(\)1.21 & 81.65\(\)1.37 & 85.40\(\)2.36 & 80.17\(\)2.79 & 77.79\(\)2.00 & 84.83\(\)1.90 & **87.10\(\)1.35** \\  & NMI & 72.97\(\)0.96 & 75.95\(\)2.52 & 78.65\(\)2.57 & 76.11\(\)2.27 & 74.93\(\)1.94 & 78.79\(\)1.08 & **80.67\(\)1.42** \\  & ARI & 69.50\(\)0.77 & 71.91\(\)1.61 & **80.46\(\)1.92** & 71.48\(\)3.29 & 70.30\(\)2.15 & 79.10\(\)2.32 & 80.19\(\)1.74 \\   & ACC & 90.59\(\)1.45 & 86.82\(\)0.65 & 85.82\(\)0.58 & 79.31\(\)1.55 & 88.70\(\)1.42 & 80.46\(\)1.85 & **91.46\(\)0.82** \\  & NMI & **85.32\(\)0.53** & 80.82\(\)0.86 & 81.60\(\)0.66 & 74.25\(\)0.69 & 81.47\(\)1.12 & 73.31\(\)2.45 & 84.40\(\)1.20 \\  & ARI & 78.58\(\)2.54 & 73.90\(\)1.65 & 74.25\(\)2.19 & 71.44\(\)1.45 & 77.65\(\)2.15 & 69.31\(\)2.80 & **80.25\(\)1.62** \\   & ACC & 61.50\(\)0.54 & 56.12\(\)2.54 & 55.26\(\)3.18 & **62.31\(\)2.65** & 58.05\(\)1.28 & 52.42\(\)3.15 & 60.82\(\)1.66 \\  & NMI & 68.32\(\)1.23 & 63.28\(\)2.66 & 62.44\(\)2.56 & **70.25\(\)2.20** & 65.75\(\)2.20 & 61.76\(\)3.40 & 69.20\(\)1.48 \\   & ARI & 59.86\(\)1.46 & 58.10\(\)2.90 & 56.45\(\)2.74 & 61.14\(\)2.55 & 57.52\(\)2.56 & 53.30\(\)2.82 & **61.42\(\)2.12** \\   

Table 1: Clustering performance comparison on four datasets (mean\(\)standard deviation). The optimal and suboptimal results are in bold and underlined, respectively.

   Datasets & Noise & DMVAE & MIB & CMIB-Nets & Completer & IMC-v1 & IMC-v2 & IMC \\   & 10\% & 82.35\(\)1.24 & 74.60\(\)1.75 & 75.55\(\)2.35 & 85.28\(\)1.84 & 81.12\(\)2.26 & 78.46\(\)1.45 & **86.82\(\)1.05** \\  & 30\% & 75.84\(\)2.26 & 66.35\(\)3.45 & 65.50\(\)2.82 & 73.40\(\)2.56 & 69.90\(\)3.25 & 72.18\(\)2.48 & **78.56\(\)1.36** \\  & 50\% & 63.80\(\)1.58 & 58.46\(\)2.80 & 56.46\(\)1.64 & 60.76\(\)3.65 & 56.84\(\)3.40 & 65.30\(\)2.66 & **70.35\(\)2.28** \\   

Table 2: Comparison of NMI at noise ratios of 10%, 30%, and 50% (mean\(\)standard deviation). The optimal and suboptimal results are in bold and underlined, respectively.

shown satisfactory performance, it is still important to explore the effect of varying these parameters on the performance and to mine the inner mechanism of this model. We evaluate the parameter sensitivity of ACC, NMI, ARI metrics on Caltech20 dataset, as shown in Figure 3, we choose the values of \(\) and \(\) in the range of {0.01, 0.1, 1, 10, 100}. It can be seen from the results that our method is not sensitive to the coefficient of clustering loss, but how to adjust the parameter \(\) is the key to improving clustering performance. As expressed in Equation (8), \(\) controls the degree of cross-diversity of multi-view representation. Therefore, the selection strategy of the beta parameter can be based on prior knowledge of the dataset. In our experience, if there is a large degree of consistency in the semantics of multiple views, then the \(\) is set to small and vice versa.

## 6 Conclusion

In this paper, we propose an information-theoretic multi-view clustering framework that avoids the assumption of semantic consistency across multiple views. Specifically, we extend the information bottleneck theory to unsupervised multi-view learning through defining three requirements for multi-view representations via mutual information. By constructing a multi-view variational lower bound and introducing KL divergence as a clustering loss, the entire framework is finally optimized by deep neural network and stochastic gradient variational Bayes. Experiments verify the superiority and robustness of the generalized information-based multi-view clustering on four benchmark datasets and noisy data.

The practical limitations of the proposed model lie in the choice of parameters. The mathematical strategy for selecting the optimal parameters is a direction that can be studied in the future. In addition, constructing a tighter lower bound or estimator to approximate high-dimensional mutual information is also a place for further improvement in this work.