# Generalizability of Memorization Neural Networks

Lijia Yu\({}^{1}\), Xiao-Shan Gao\({}^{2,3,}\), Lijun Zhang\({}^{1,3}\), Yibo Miao\({}^{2,3}\)

\({}^{1}\) Key Laboratory of System Software (Chinese Academy of Sciences)

and State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences

\({}^{2}\)Academy of Mathematics and Systems Science, Chinese Academy of Sciences

Beijing 100190, China

\({}^{3}\)University of Chinese Academy of Sciences, Beijing 100049, China

Corresponding author.

###### Abstract

The neural network memorization problem is to study the expressive power of neural networks to interpolate a finite dataset. Although memorization is widely believed to have a close relationship with the strong generalizability of deep learning when using over-parameterized models, to the best of our knowledge, there exists no theoretical study on the generalizability of memorization neural networks. In this paper, we give the first theoretical analysis of this topic. Since using i.i.d. training data is a necessary condition for a learning algorithm to be generalizable, memorization and its generalization theory for i.i.d. datasets are developed under mild conditions on the data distribution. First, algorithms are given to construct memorization networks for an i.i.d. dataset, which have the smallest number of parameters and even a constant number of parameters. Second, we show that, in order for the memorization networks to be generalizable, the width of the network must be at least equal to the dimension of the data, which implies that the existing memorization networks with an optimal number of parameters are not generalizable. Third, a lower bound for the sample complexity of general memorization algorithms and the exact sample complexity for memorization algorithms with constant number of parameters are given. It is also shown that there exist data distributions such that, to be generalizable for them, the memorization network must have an exponential number of parameters in the data dimension. Finally, an efficient and generalizable memorization algorithm is given when the number of training samples is greater than the efficient memorization sample complexity of the data distribution.

## 1 Introduction

Memorization is to study the expressive power of neural networks to interpolate a finite dataset . The main focus of the existing work is to study how many parameters are needed to memorize. For any dataset \(_{tr}\) of size \(N\) and neural networks of the form \(:^{n}\), memorization networks with \((N)\) parameters have been given with various model structures and activation functions [31; 50; 30; 29; 26; 47; 56; 11; 65]. On the other hand, it is shown that in order to memorize an arbitrary dataset of size \(N\)[64; 56], the network must have at least \((N)\) parameters, so the above algorithms are approximately optimal. Under certain assumptions, it is shown that sublinear \((N^{2/3})\) parameters are sufficient to memorize \(_{tr}\). Furthermore, Vardi et al.  give a memorization network with optimal number of parameters: \(()\).

Recently, it is shown that memorization is closely related to one of the most surprising properties of deep learning, that is, over-parameterized neural networks are trained to nearly memorize noisy data and yet can still achieve a very nice generalization on the test data [45; 7; 4]. More precisely, thedouble descent phenomenon  indicates that when the networks reach the interpolation threshold, larger networks tend to have more generalizability . It is also noted that memorizing helps generalization in complex learning tasks, because data with the same label have quite diversified features and need to be nearly memorized . A line of research to harvest the help of memorization to generalization is _interpolation learning_. Most of recent work in interpolation learning shows generalizability of memorization models in linear regimes .

As far as we know, the generazability of memorization neural networks has not been studied theoretically, which is more challenging compared to the linear models, and this paper provides a systematic study of this topic. In this paper, we consider datasets that are sampled i.i.d. from a data distribution, because i.i.d. training dataset is a necessary condition for learning algorithms to have generalizability . More precisely, we consider binary data distributions \(\) over \(^{n}\{-1,1\}\) and use \(_{}^{N}\) to mean that \(_{tr}\) is sampled i.i.d. from \(\) and \(|_{tr}|=N\). All neural networks are of the form \(:^{n}\). The main contributions of this paper include four aspects.

First, we give the smallest number of parameters required for a network to memorize an i.i.d. dataset.

**Theorem 1.1** (Informal. Refer to Section 4).: _Under mild conditions on \(\), if \(_{tr}^{N}\), it holds_

_(1) There exists an algorithm to obtain a memorization network of \(_{tr}\) with width 6 and depth \(()\)._

_(2) There exists a constant \(N_{}_{+}\) depending on \(\) only, such that a memorization network of \(_{tr}\) with at most \(N_{}\) parameters can be obtained algorithmically._

\(N_{}\) is named as the **memorization parameter complexity** of \(\), which measures the complexity of \(\) under which a memorization network with \( N_{}\) parameters exists for almost all \(D_{}^{N}\).

Theorem 1.1 allows us to give the memorization network for i.i.d dataset with the optimal number of parameters. When \(N\) is small so that \( N_{}\), the memorization network needs at least \(()\) parameters as proved in  and (1) of Theorem 1.1 gives the optimal construction. When \(N\) is large, (2) of Theorem 1.1 shows that a constant number of parameters is enough to memorize.

Second, we give a necessary condition for the structure of the memorization networks to be generalizable, and shows that even if there is enough data, memorization network may not have generalizability.

**Theorem 1.2** (Informal. Refer to Section 5).: _Under mild conditions on \(\), if \(_{tr}^{N}\), it holds_

_(1) Let \(\) be a set of neural networks with width \(w\). Then, there exist an integer \(n>w\) and a data distribution \(\) over \(^{n}\{-1,1\}\) such that, any memorization network of \(_{tr}\) in \(\) is not generalizable._

_(2) For almost any \(\), there exists a memorization network of \(_{tr}\), which has \(()\) parameters and is not generalizable._

Theorem 1.2 indicates that memorization networks with the optimal number of parameters \(()\) may have poor generalizability, and commonly used algorithms for constructing fixed-width memorization networks have poor generalization for some distributions. These conclusions demonstrate that the commonly used network structures for memorization is not generalizable and new network structures are needed to achieve generalization.

Third, we give a lower bound for the sample complexity of general memorization networks and the exact sample complexity for certain memorization networks.

**Theorem 1.3** (Informal. Refer to Section 6).: _Let \(N_{}\) be the memorization parameter complexity defined in Theorem 1.1. Under mild conditions on \(\), we have_

_(1)_ **Lower bound**_. In order for a memorization network of any \(_{}^{N}\) to be generalizable, \(N\) must be \((}^{2}}{^{2}(N_{})})^{2}\)._

_(2)_ **Upper bound**_. For any memorization network with at most \(N_{}\) parameters for \(_{}^{N}\), if \(N=(N_{}^{2} N_{})\), then the network is generalizable._Notice that the lower bound is for general memorization networks and the upper bound is for memorization networks with \( N_{}\) parameters, which always exist by (2) of Theorem 1.1. In the latter case, the lower and upper bounds are approximately the same, which gives the exact sample complexity \((N_{}^{2})\) in this case. In other words, a necessary and sufficient condition for the memorization network in (2) of Theorem 1.1 to be generalizable is \(N=(N_{}^{2})\).

_Remark 1.4_.: Unfortunately, these generalizable memorization networks cannot be computed efficiently, as shown by the following results proved by us.

(1) If \(P NP\), then all networks in (2) of Theorem 1.3 cannot be computed in polynomial time.

(2) For some data distributions, an exponential (in the data dimension) number of samples is required for memorization networks to achieve generalization.

Finally, we want to know that does there exist a polynomial time memorization algorithm that can ensure generalization, and what is the sample complexity of such memorization algorithm? An answer is given in the following theorem.

**Theorem 1.5** (Informal. Refer to Section 7).: _There exists an \(S_{}_{+}\) depending on \(\) only such that, under mild conditions on \(\), if \(N=(S_{})\), then we can construct a generalizable memorization network with \(O(N^{2}n)\) parameters for any \(_{}^{N}\) in polynomial time._

\(S_{}\) is named as the **efficient memorization sample complexity** for \(\), which measures the complexity of \(\) so that the generalizable memorization network of any \(D_{}^{N}\) can be computed efficiently if \(N=(S_{})\).

The memorization network in Theorem 1.5 has more parameters than the optimal number \(()\) of parameters required for memorization. The main reason is that building memorization networks with \(()\) parameters requires special technical skill that may break the generalization. On the other hand, as mention in , over-parametrization is good for generalization, so it is reasonable for us to use more parameters for memorization to achieve generalization.

_Remark 1.6_.: We explain the relationship between our results and interpolation learning . Interpolation learning uses optimization to achieve memorization, which is a more practical approach, while our approach gives a theoretical foundation for memorization networks. Once an interpolation is achieved, Theorem 1.2, (1) of Theorem 1.3, and Theorem 1.5 are valid for interpolation learning. For example, according to (1) of Theorem 1.3, \((N_{}^{2})\) is a lower bound for the sample complexity of interpolation learning, and by Theorem 1.5, \((S_{})\) is an upper bound for the sample complexity of efficient interpolation learning.

**Main Contributions**. Under mild conditions for the data distribution \(\), we have

* We define the _memorization parameter complexity_\(N_{}_{+}\) of \(\) such that, a memorization network for any \(_{}^{N}\) can be constructed, which has \(()\) or \( N_{}\) parameters. Here, the memorization network has the optimal number of parameters.
* We give two necessary conditions for the construction of generalizable memorization networks for any \(_{}\) in terms of the width and number of parameters of the memorization network.
* We give a lower bound \((N_{}^{2})\) of the sample complexity for general memorization networks as well as the exact sample complexity \((N_{}^{2})\) for memorization networks with \( N_{}\) parameters. We also show that for some data distribution, an exponential number of samples in \(n\) is required to achieve generalization.
* We define the _efficient memorization sample complexity_\(S_{}_{+}\) for \(\), so that generalizable memorization network of any \(D_{}^{N}\) can be computed in polynomial time, if \(N=(S_{})\).

## 2 Related work

**Memorization**. The problem of memorization has a long history. In , it is shown that networks with depth 2 and \((N)\) parameters can memorize a binary dataset of size \(N\). In subsequent work,it is shown that networks with \((N)\) parameters can be a memorization for any dataset [31; 50; 11; 30; 65; 29; 64; 56; 26; 47] and such memorization networks are approximately optimal for generic dataset [64; 56]. Since the VC dimension of neural networks with \(N\) parameters and depth \(D\) and with ReLU as the activation function is at most \((ND)\)[24; 5; 6], memorizing some special datasets of size \(N\) requires at least \(()\) parameters and there exists a gap between this lower bound \(()\) and the upper bound \((N)\). Park et al.  show that a network with \((N^{2/3})\) parameters is enough for memorization under certain assumptions. Vardi et al.  further give the memorization network with optimal number of parameters \(()\). In , strengths of both generalization and memorization are combined in a single neural network. Recently, robust memorization has been studied [35; 62]. As far as we know, the generazability of memorization neural networks has not been studied theoretically.

**Interpolation Learning**. Another line of related research is interpolation learning, that is, leaning under the constraint of memorization, which can be traced back to . Most recent works establish various generalizability of interpolation learning in linear regimes [7; 12; 38; 53; 59; 66]. For instance, Bartlett et al.  prove that over-parametrization allows gradient methods to find generalizable interpolating solutions for the linear regime. In relation to this, how to achieve memorization via gradient descent is studied in [13; 14]. Results of this paper can be considered to give sample complexities for interpolation learning.

**Generalization Guarantee**. There exist several ways to ensure generalization of networks. The common way is to estimate the generalization bound or sample complexity of leaning algorithms. Generalization bounds for neural networks are given in terms of the VC dimension [24; 5; 6], under the normal training setting [27; 44; 8], under the differential privacy training setting , and under the adversarial training setting [60; 58]. In most cases, these generalization bounds imply that when the training set is large enough, a well-trained network with fixed structure has good generalizability. On the other hand, the relationship between memorization and generalization has also been extensively studied [45; 41; 10; 19; 20]. In , sample complexity of neural networks is given when the norm of the transition matrix is limited, in , sample complexity of shallow transformers is considered. This paper gives the lower bound and upper bound (in certain cases) of the sample complexities for interpolation learning.

## 3 Notation

In this paper, we use \(O(A)\) to mean a value not greater than \(cA\) for some constant \(c\), and \(\) to mean that small quantities, such as logarithm, are omitted. We use \((A)\) to mean a value not less than \(cA\) for some constant \(c\), and \(\) to mean that small quantities, such as logarithm, are omitted. We say for all \((x,y)\) there is event A stand means that \(P_{(x,y)}(A)=1\).

### Neural network

In this paper, we consider feedforward neural networks of the form \(:^{n}\) and the \(l\)-th hidden layer of \((x)\) can be written as

\[X_{l}=(W_{l}X_{l-1}+b_{l})^{n_{l}},\]

where \(=\) is the activation function, \(X_{0}=x\) and \(N_{0}=n\). The last layer of \(\) is \((x)=W_{L+1}X_{L}+b_{L+1}\), where \(L\) is the number of hidden layers in \(\). The depth of \(\) is \(()=L+1\), the width of \(\) is \(()=_{i=1}^{L}\{n_{i}\}\), the number of parameters of \(\) is \(()=_{i=0}^{L}n_{i}(n_{i+1}+1)\). Denote \((n)\) to be the set of all neural networks in the above form.

### Data distribution

In this paper, we consider binary classification problems and use \(\) to denote a joint distribution on \((n)=^{n}\{-1,1\}\). To avoid extreme cases, we focus mainly on a special kind of distribution to be defined in the following.

**Definition 3.1**.: For \(n_{+}\) and \(c_{+}\), \((n,c)\) is the set of distributions \(\) on \((n)\), which has a _positive separation bound_: \(_{(x,1),(x,-1)}||x-z||_{2} c\).

The accuracy of a network \(\) on a distribution \(\) is defined as

\[A_{}()=_{(x,y)}(( (x))=y).\]

We use \(_{tr}^{N}\) to mean that \(_{tr}\) is a set of \(N\) data sampled i.i.d. according to \(\). For convenience, dataset under distribution means that the dataset is i.i.d selected from a data distribution.

_Remark 3.2_.: We define the distribution with positive separation bound in for the following reasons. (1) If \(_{tr}^{N}\) and \((n,c)\), then \(x_{i} x_{j}\) when \(y_{i} y_{j}\). Such property ensures that \(_{tr}\) can be memorized. (2) Proposition 3.3 shows that there exists a \(\) such that any network is not generalizable over \(\), and this should be avoided. Therefore, distribution \(\) needs to meet certain requirements for a dataset sampled from \(\) to have generalizability. Proof of Proposition 3.3 is given in Appendix A. (3) Most commonly used classification distributions should have positive separation bound.

**Proposition 3.3**.: _There exists a distribution \(\) such that \(A_{}() 0.5\) for any neural network \(\)._

### Memorization neural network

**Definition 3.4**.: A neural network \((n)\) is a memorization of a dataset \(_{tr}\) over \((n)\), if \(((x))=y\) for any \((x,y)_{tr}\).

_Remark 3.5_.: Memorization networks can also be defined more strictly as \((x)=y\) for any \((x,y)_{tr}\). In Proposition 4.10 of , it is shown that these two types of memorization networks need essentially the same number of parameters.

To be more precise, we treat memorization as a learning algorithm in this paper, as defined below.

**Definition 3.6**.: \(:_{n_{+}}2^{(n)}_{n_{+}}(n)\) is called a _memorization algorithm_ if for any \(n\) and \(_{tr}(n)\), \((_{tr})\) is a memorization network of \(_{tr}\).

Furthermore, a memorization algorithm \(\) is called an _efficient memorization algorithm_ if there exists a polynomial \(:\) such that \((_{tr})\) can be computed in time \(((_{tr}))\), where \((_{tr})\) is the bit-size of \(_{tr}\).

_Remark 3.7_.: It is clear that if \(\) is an efficient memorization algorithm, then \(((_{tr}))\) is also polynomial in \((_{tr})\).

There exist many methods which can construct memorization networks in polynomial times, and all these memorization methods are efficient memorization algorithms, which are summarized in the following proposition.

**Proposition 3.8**.: _The methods given in [9; 62] are efficient memorization algorithms. The methods given in [55; 49] are probabilistic efficient memorization algorithms, which can be proved similar to that of Theorem 4.1. More precisely, they are Monte Carlo polynomial-time algorithms._

## 4 Optimal memorization network for dataset under distribution

By the term "dataset under distribution", we mean datasets that are sampled i.i.d. from a data distribution, and is denoted as \(_{tr}^{N}\). In this section, we show how to construct the memorization network with the optimal number of parameters for dataset under distribution.

### Memorization network with optimal number of parameters

To memorize \(N\) samples, \(()\) parameters are necessary . In , a memorization network is given which has \(()\) parameters under certain conditions, where \(\) means that some logarithm factors in \(N\) and polynomial factors of other values are omitted. Therefore, \(()\) is the optimal number of parameters for a network to memorize certain dataset. In the following theorem, we show that such a result can be extended to dataset under distribution.

**Theorem 4.1**.: _Let \((n,c)\) and \(_{tr}^{N}\). Then there exists a memorization algorithm \(\) such that \((_{tr})\) has width \(6\) and depth (equivalently, the number of parameters) \(O((Nn/c))\). Furthermore, for any \((0,1)\), \((_{tr})\) can be computed in time \(((_{tr}),(1/))\) with probability \( 1-\)._

**Proof Idea.**_This theorem can be proven using the idea from . Let \(_{tr}=\{(x_{i},y_{i})\}_{i=1}^{N}\). The mainly different is that in , it requires \(||x_{i}-x_{j}|| c\) for all \(i j\), which is no longer valid when \(_{tr}\) is sampled i.i.d. from distribution \(\). Since \(\) has separation bound \(c>0\), we have \(||x_{i}-x_{j}|| c\) for all \(i,j\) satisfying \(y_{i} y_{j}\), which is weaker. Despite this difference, the idea of  can still be modified to prove the theorem. In constructing such a memorization network, we need to randomly select a vector, and each selection has a probability of 0.5 to give the correct vector. So, repeat the selection \((1/)\) times, with probability \(1-\), we can get at least one correct vector. Then we can construct the memorization network based on this vector. Detailed proof is given in Appendix B._

_Remark 4.2_.: The algorithm in Theorem 4.1 is a Monte Carlo polynomial-time algorithm, that is, it gives a correct answer with arbitrarily high probability. The algorithm given in  is also a Monte Carlo algorithm.

### Memorization network with constant number of parameters

In this section, we prove an interesting fact of memorization for dataset under distribution. We show that for a distribution \((n,c)\), there exists a constant \(N_{}_{+}\) such that for all datasets sampled i.i.d. from \(\), there exists a memorization network with \(N_{}\) parameters.

**Theorem 4.3**.: _There exists a memorization algorithm \(\) such that for any \((n,c)\), there is an \(N^{}_{}_{+}\) satisfying that for any \(N>0\), with probability 1 of \(_{tr}^{N}\), we have \(((_{tr})) N^{}_{}\). The smallest \(N^{}_{}\) of the distribution \(\) is called the memorization parameter complexity of \(\), written as \(N_{}\)._

**Proof Idea.**_It suffices to show that we can find a memorization network of \(_{tr}\) with a constant number of parameters, which depends on \(\) only. The main idea is to take a subset \(^{}_{tr}\) of \(_{tr}\) such that \(_{tr}\) is contained in the neighborhood of \(^{}_{tr}\). It can be proven that the number of elements in this subset is limited. Then construct a robust memorization network of \(^{}_{tr}\) with certain budget , we obtain a memorization network of \(_{tr}\), which has a constant number of parameters. The proof is given in Appendix C._

Combining Theorems 4.1 and 4.3, we can give a memorization network with the optimal number of parameters.

_Remark 4.4_.: What we have proven in Theorem 4.3 is that a memorization algorithm with a constant number of parameters can be found, but in most of times, we have \(N^{}_{}>N_{}\). Furthermore, if \(N^{}_{}\) is large for the memorization algorithm, the algorithm can be efficient. Otherwise, if \(N^{}_{}\) is closed to \(N_{}\), the algorithm is usually not efficient.

_Remark 4.5_.: It is obvious that the memorization parameter compelxity \(N_{}\) is the minimum number of parameters required to memorize any dataset sampled i.i.d. from \(\). \(N_{}\) is mainly determined by the characteristic of \((n,c)\), so \(N_{}\) may be related to \(n\) and \(c\). It is an interesting problem to estimate \(N_{}\).

## 5 Condition on the network structure for generalizable memorization

In the preceding section, we show that for the dataset under distribution, there exists a memorization algorithm to generate memorization networks with the optimal number of parameters. In this section, we give some conditions for the generalizable memorization networks in terms of width and number of parameters of the network. As a consequence, we show that the commonly used memorization networks with fixed width is not generalizable.

First, we show that networks with fixed width do not have generazability in some situations. Reducing the width and increasing depth is a common way for parameter reduction, but it inevitably limits the network's power, making it unable to achieve good generalization for specific distributions, as shown in the following theorem.

**Theorem 5.1**.: _Let \(w_{+}\) and \(\) be a memorization algorithm such that \((_{tr})\) has width not more than \(w\) for all \(_{tr}\). Then, there exist an integer \(n>w\), \(c_{+}\), and a distribution \((n,c)\) such that, for any \(_{tr}^{N}\), it holds \(A_{}((_{tr})) 0.51\)._

**Proof Idea.**_As shown in , networks with small width are not dense in the space of measurable functions, but this is not enough to estimate the upper bound of the generalization. In order to further measure the upper bound of generalization, we define a special class of distributions. Then, we calculate the upper bound of the generalization of networks with fixed width on this class of distribution. Based on the calculation results, it is possible to find a specific distribution within this class of distributions, such that the fixed-width network exhibits a poor generalization of this distribution. The proof is given in Appendix D._

It is well known that width of the network is important for the network to be robust . Theorem 5.1 further shows that large width is a necessary condition for generalizabity.

Note that Theorem 5.1 is for a specific data distribution. We will show that for most distributions, providing enough data does not necessarily mean that the memorization algorithm has generalization ability. This highlights the importance of constructing appropriate memorization algorithms to ensure generalization. We need to introduce another parameter for data distribution.

**Definition 5.2**.: The distribution \(\) is said to have _density_\(r\), if \(_{x}(x A)/V(A) r\) for any closed set \(A^{n}\), where \(V(A)\) is the volume of \(A\).

Loosely speaking, the density of a distribution is the upper bound of the density function.

**Theorem 5.3**.: _For any \(n_{+},r,c_{+}\), if distribution \((n,c)\) has density \(r\), then for any \(N_{+}\) and \(_{tr}^{N}\), there exists a memorization network \(\) for \(_{tr}\) such that \(()=O(n+(Nnr/c))\) and \(A_{}() 0.51\)._

**Proof Idea.**_We refer to the classical memorization construction idea . The main body includes three parts. Firstly, compress the data in \(_{tr}\) into one dimension. Secondly, map the compressed data to some specific values. Finally, use such a value to get the label of input. Moreover, we will pay more attention to points outside the dataset. We use some skills to control the classification results of points that do not appear in the dataset \(_{tr}\), so that the memorization network will give the wrong label to the points that are not in \(_{tr}\) as much as possible to reduce its accuracy. The general approach is the following: (1) Find a set in which each point is not presented in \(_{tr}\) and has the same label under distribution \(\). Without loss of generality, let they have label \(1\). (2) In the second step mentioned in the previous step, ensure that the mapped results of the points in the set mentioned in (1) are similar to the samples with label \(-1\). This will cause the third step to output the label \(-1\), leading to an erroneous classification result for the points in the set. The proof is given in Appendix E._

_Remark 5.4_.: Theorem 5.1 shows that the width of the generazable memorization network needs to increase with the increase of the data dimension. Theorem 5.3 shows that when \(()=()\), the memorization network may have poor generalizability for most distributions. The above two theorems indicate that no matter how large the dataset is, there always exist memorization networks with poor generalization. In terms of sample complexity, it means that for the hypotheses of neural networks with fixed width or with optimal number of parameters, the sample complexity is infinite, contrary to the uniform generalization bound for feedforward neural networks [63, Lemma D.16].

_Remark 5.5_.: It is worth mentioning that the two theorems in this section cannot be obtained from the lower bound of the generalization gap , and more details are shown in Appendix E.

## 6 Sample complexity for memorization algorithm

As said in the preceding section, generalization of memorization inevitably requires certain conditions. In this section, we give the necessary and sufficient condition for generalization for the memorization algorithm in Section 4 in terms of sample complexity.

We first give a lower bound for the sample complexity for general memorization algorithms and then an upper bound for memorization algorithms which output networks with an optimal number of parameters. The lower and upper bounds are approximately the same, thus giving the exact sample complexity in this case.

### Lower bound for sample complexity of memorization algorithm

Roughly speaking, the sample complexity of a learning algorithm is the number of samples required to achieve generalizability . The following theorem gives a lower bound for the sample complexity of memorization algorithms based on \(N_{}\), which has been defined in Theorem 4.3.

**Theorem 6.1**.: _There exists_ **no** _memorization algorithm \(\) which satisfies that for any \(n_{+},c_{+},,(0,1)\), if \((n,c)\) and \(N v}^{2}}{^{n}(N_{})}(1-2-)\), it holds_

\[_{_{tr}^{N}}(A((_{tr })) 1-) 1-\]

_where \(v\) is an absolute constant which does not depend on \(N,n,c,,\)._

**Proof Idea.**_The mainly idea is that: for a dataset \(_{tr}^{n}\{-1,1\}\) with \(|_{tr}|=N\), we can find some distributions \(_{1},_{2},\), such that if \(_{tr,i}(_{i})^{N}\), then with a positive probability, it hold \(_{tr,i}=_{tr}\). In addition, each distribution has a certain degree of difference from the others. It is easy to see that \((_{tr})\) is a fixed network for a given \(\), so \((_{tr})\) cannot fit all \(_{i}\) well because \(_{i}\) are different to some degree. So, if a memorization algorithm \(\) satisfies the condition in the theorem, we try to construct some distributions \(\{_{i}\}_{i=1}^{n}\), and use the above idea to prove that \(\) cannot fit one of the distributions in \(\{_{i}\}_{i=1}^{n}\), and obtain contradictions. The proof of the theorem is given in Appendix F._

_Remark 6.2_.: In general, the sample complexity depends on the data distribution, hypothesis space, learning algorithms, and \(,\). Since \(N_{}\) is related to \(n\) and \(c\), the lower bound in Theorem 6.1 also depends on \(n\) and \(c\). Here, the hypothesis space is the memorization networks, which is implicitly reflected in \(N_{}\).

_Remark 6.3_.: Roughly strictly, if we consider interpolation learning, that is, training network under the constraint of memorizing the dataset, then Theorem 6.1 also provides a lower bound for the sample complexity.

This theorem shows that if we want memorization algorithms to have guaranteed generalization, then about \((N_{}^{2})\) samples are required. As a consequence, we show that, for some data distribution, it need an exponential number of samples to achieve generalization. The proof is also in Appendix F.

**Corollary 6.4**.: _For any memorization algorithm \(\) and any \(,(0,1)\), there exist \(n_{+},c>0\) and a distribution \((n,c)\), such that in order for \(\) to have generalizability on \(\), that is for all \(N N_{0}\), there is_

\[_{_{tr}^{N}}(A((_{tr })) 1-) 1-,\]

\(N_{0}\) _must be more than \(v(2^{2}})c^{4}(1-2-)/n^{2})\), where \(v\) is an absolute constant not depending on \(N,n,c,,\)._

### Exact sample complexity of memorization algorithm with \(N_{}\) parameters

In Theorem 6.1, it is shown that \((N_{}^{2})\) samples are necessary for generalizability of memorization. The following theorem shows that there exists a memorization algorithm that can reach generalization with \((N_{}^{2})\) samples.

**Theorem 6.5**.: _For all memorization algorithms \(\) satisfies that \((_{tr})\) has at most \(N_{}\) parameters, with probability 1 for \(_{tr} D^{N}\), we have_

**(1)**: _For any_ \(c,,(0,1),\)__\(n_{+}\)_, if_ \((n,c)\) _and_ \(N}^{2}(N_{}/(^{2}))}{ ^{2}}\)_, then_

\[_{_{tr}^{N}}(A((_{tr })) 1-) 1-,\]

_where_ \(v\) _is an absolute constant which does not depend on_ \(N,n,c,,\)_._
**(2)**: _If_ \(P NP\)_, then all such algorithms are not efficient._

**Proof Idea.**_For the proof of (1), we need to use the \(N_{}\) to calculate the VC-dimension , and take such a dimension in the generalization bound theorem  to obtain the result. For the proof of (2), we show that, if such algorithm is efficient, then we can solve the following reversible 6-SAT  problem, which is defined below and is an NPC problem. The proof of the theorem is given in Appendix G._

**Definition 6.6**.: Let \(\) be a Boolean formula and \(\) the formula obtained from \(\) by negating each variable. The Boolean formula \(\) is called _reversible_ if either both \(\) and \(\) are satisfiable or both are not satisfiable. The _reversible satisfiability problem_ is to recognize the satisfiability of reversible formulae in conjunctive normal form (CNF). By the _reversible 6-SAT_, we mean the reversible satisfiability problem for CNF formulae with six variables per clause. In , it is shown that the reversible 6-SAT is NPC.

Combining Theorems 6.1 and 6.5, we see that \(N=(N_{}^{2})\) is the necessary and sufficient condition for the memorization algorithm to generalize, and hence \((N_{}^{2})\) is the exact sample complexity for memorization algorithms with \(N_{}\) parameters over the distribution \((n,c)\).

Unfortunately, by (2) of Theorem 6.5, this memorization algorithm is not efficient when the memorization has no more than \(N_{}\) parameters. Furthermore, we conjecture that there exist no efficient memorization algorithms that can use \((N_{}^{2})\) samples to reach generalization in the general case, as shown in the following conjecture.

_Conjecture 6.7_.: If P\(\) NP, there exist no efficient memorization algorithms that can reach generalization with \((N_{}^{2})\) samples for all \((n,c)\).

_Remark 6.8_.: This result also provides certain theoretical explanation for the over-parameterization mystery [45; 7; 4]: for memorization algorithms with \(N_{}\) parameters, the exact sample complexity \((N_{}^{2})\) is greater than the number of parameters. Thus, the networks is under-parameterized and for such a network, even if it is generalizable, it cannot be computed efficiently.

## 7 Efficient memorization algorithm with guaranteed generalization

In the preceding section, we show that there exist memorization algorithms that are generalizable when \(N=(N_{}^{2})\), but such an algorithm is not efficient. In this section, we give an efficient memorization algorithm with guaranteed generalization.

First, we define the efficient memorization sample complexity of \(\).

**Definition 7.1**.: For \((x,y)\), let \(L_{(x,y)}=_{(z,-y)}||x-z||_{2}\) and \(B((x,y))=_{2}(x,L_{(x,y)}/3.1)=\{z^{n}:\|z-x\|_{2} L _{(x,y)}/3.1\}\). The nearby set \(S\) of \(\) is a subset of sample \((x,y)\) which is in distribution \(\) and satisfies: (1) for any \((x,y)\), \(x_{(z,w) S}B((z,w))\); (2) \(|S|\) is minimum.

Evidently, for any \((n,c)\), its nearby set is finite, as shown by Proposition 7.7. \(S_{}=|S|\) is called the _efficient memorization sample complexity_ of \(\), the meaning of which is given in Theorem 7.3.

_Remark 7.2_.: In the above definition, we use \(L_{(x,y)}/3.1\) to be the radius of \(B((x,y))\). In fact, when \(3.1\) is replaced by any real number greater than \(3\), the following theorem is still valid.

**Theorem 7.3**.: _There exists an efficient memorization algorithm \(\) such that for any \(c,,(0,1)\), \(n_{+}\), and \((n,c)\), if \(N}(S_{}/)}{}\), then_

\[_{_{tr}^{N}}(A((_{tr })) 1-) 1-.\]

_Moreover, for any \(_{tr}^{N}\), \((_{tr})\) has at most \(O(N^{2}n)\) parameters._

**Proof Idea.**_For a given dataset \(_{tr}^{n}\{-1,1\}\), we use the following two steps to construct a memorization network._

_Step 1. Find suitable convex sets \(\{C_{i}\}\) in \(^{n}\) such that each sample in \(_{tr}\) is in at least one of these convex sets. Furthermore, if \(x,z C_{i}\) and \((x,y_{x}),(z,y_{z})_{tr}\), then \(y_{x}=y_{z}\), and define \(y(C_{i})=y_{x}\)._

_Step 2. Construct a network \(\) such that for any \(x C_{i}\), \(}((x))=y(C_{i})\). This network must be a memorization of \(_{tr}\), because each sample in \(_{tr}\) is in at least one of \(\{C_{i}\}\). Hence, if \(x C_{i}\) and \((x,y_{x})_{tr}\), then \(}((x))=y(C_{i})=y_{x}\). The proof of the theorem is given in Appendix H._

_Remark 7.4_.: Theorem 7.3 shows that there exists an efficient and generalizable memorization algorithm when \(N=(S_{})\). Thus, \(S_{}\) is an intrinsic complexity measure of \(\) on whether it iseasy to learn and generalize. By Theorem 6.1, \(S_{} N_{}^{2}\) for some \(\), but for some "nice" \(\), \(S_{}\) could be small. It is an interesting problem to estimate \(S_{}\).

_Remark 7.5_.: Theorem 7.3 uses \(O(N^{2}n)\) parameters, highlight the importance of over-parameterization . Interestingly, Remark 6.8 shows that if the network has \(O()\) parameters, even if it is generalizable, it cannot be computed efficiently.

The experimental results of the memorization algorithm mentioned in Theorem 7.3 are given in Appendix I. Unfortunately, for commonly used datasets such as CIFAR-10, this algorithm cannot surpass the network obtained by training with SGD, in terms of test accuracy. Thus, the main purpose of the algorithm is theoretical, that is, it provides a polynomial-time memorization algorithm that can achieve generalization when the training dataset contains \((S_{})\) samples. In comparison of theoretical works, training networks is NP-hard for small networks  and the guarantee of generalization needs strong assumptions on the loss function .

Finally, we give an estimate for \(S_{}\). From Corollary 6.4 and Theorem 7.3, we obtain a lower bound for \(S_{}\).

**Corollary 7.6**.: _There exists a distribution \((n,c)\) such that \(S_{}(S_{}/)(}{ n^{2}}2^{2^{}}})\)._

We will give an upper bound for \(S_{}\) in the following proposition, and the proof is given in Appendix H.1. From the proposition, it is clear that \(S_{}\) is finite.

**Proposition 7.7**.: _For any \((n,c)\), we have \(S_{}([6.2n/c]+1)^{n}\)._

_Remark 7.8_.: The above proposition gives an upper bound of \(S_{}\) when \((n,c)\), and this does not mean that \(S_{}\) is exponential for all \((n,c)\). Determining the conditions under which \(S_{}\) is small for a given \(\) is a compelling problem.

## 8 Conclusion

Memorization originally focuses on theoretical study of the expressive power of neural networks. Recently, memorization is believed to be a key reason why over-parameterized deep learning models have excellent generalizability and thus the more practical interpolation learning approach has been extensively studied. But the generalizability theory of memorization algorithms is not yet given, and this paper fills this theoretical gap in several aspects.

We first show how to construct memorization networks for dataset sampled i.i.d from a data distribution, which have the optimal number of parameters, and then show that some commonly used memorization networks do not have generalizability even if the dataset is drawn i.i.d. from a data distribution and contains a sufficiently large number of samples. Furthermore, we establish the sample complexity of memorization algorithm in several situations, including a lower bound for the memorization sample complexity and an upper bound for the efficient memorization sample complexity.

**Limitation and future work** Two numerical complexities \(N_{}\) and \(S_{}\) for a data distribution \(\) are introduced in this paper, which are used to describe the size of the memorization networks and the efficient memorization sample complexity for any i.i.d. dataset of \(\). \(N_{}\) is also a lower bound for the sample complexity of memorization algorithms. However, we do not know how to compute \(N_{}\) and \(S_{}\), which is an interesting future work. Conjecture 6.7 tries to give a lower bound for the efficient memorization sample complexity. More generally, can we write \(N_{}\) and \(S_{}\) as functions of the probability density function \(p(x,y)\) of \(\)?

Corollary 6.4 indicates that even for the "nice" data distributions \((n,c)\), to achieve generalization for some data distribution requires an exponential number of parameters. This indicates that there exists **"data curse of dimensionality"**, that is, to achieve generalizability for certain data distribution, neural networks with exponential number of parameters are needed. Considering the practical success of deep learning and the double descent phenomenon , the data distributions used in practice should have better properties than \((n,c)\), and finding data distributions with polynomial size efficient memorization sample complexity \(E_{}\) is an important problem.

Finally, finding a memorization algorithm that can achieve SOTA results in solving practical image classification problems is also a challenge problem.