# Schema-learning and rebinding as mechanisms of in-context learning and emergence

Sivaramakrishnan Swaminathan Antoine Dedieu Rajkumar Vasudeva Raju

Murray Shanahan Miguel Lazaro-Gredilla Dileep George

Google DeepMind

{sivark,adedieu,rajvraju,mshanahan,lazarogredilla,dileepgeorge}@google.com

###### Abstract

In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by an alternative sequence prediction learning method, namely clone-structured causal graphs (CSCGs). A key property of CSCGs is that, unlike transformer-based LLMs, they are _interpretable_, which considerably simplifies the task of explaining how ICL works. We show that ICL in CSCG uses a combination of (a) learning template (schema) circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding novel tokens to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different capabilities emerge at different levels of overparameterization, suggesting that overparameterization helps in learning more complex template (schema) circuits. By showing how ICL can be achieved with small models and datasets, we open up a path to novel architectures, and take a vital step towards a more general understanding of the mechanics behind this important capability.

## 1 Introduction

In a pre-trained sequence model, _in-context learning_ (ICL), or _few-shot prompting_, is the ability to learn a new task from a small set of examples presented within the context (the prompt) at inference time. Surprisingly, large language models (LLMs) trained on sufficient data exhibit ICL, even though they are trained only with the objective of next token prediction [1; 2]. A good deal of the ongoing excitement surrounding LLMs arises from this unexpected capacity, since it dramatically enlarges their set of potential applications. Ongoing attempts to understand this capability take a variety of forms, including higher-level normative accounts using Bayesian inference , and mechanistic explanations involving implicit gradient descent  or induction heads . Despite this, the mechanisms that underlie ICL in LLMs remain somewhat mysterious.

We take an alternative approach, studying a sequence-learning model called a clone-structured causal graph (CSCG) [6; 7] to reveal the conditions that drive ICL. We show that ICL can be explained as a combination of (a) learning template circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens to appropriate slots in templates . Unlike n-gram models, CSCGs allow transitive generalization in the latent space: they assign semantically sensible non-zero probabilities to sequences never seen during training to ensure that the contexts (prompts) used for retrieval are not pure memorizations. In addition, the binding of novel tokens to slots in learned templates allows the same structural knowledge to be applied to entirely novel inputs. We hypothesize how similar mechanisms could exist in transformer-based LLMs. Byelucidating the principles that underpin the mechanics of ICL, we hope to pave the way for the design of novel architectures for abstraction and generalization, while the building blocks we identify guide the search for mechanistically interpretable  and editable  circuits in transformers .

## 2 Rebinding algorithm for clone-structured causal graphs

### Background on clone-structured causal graphs (CSCGs)

Consider an agent executing a series of discrete actions \(a_{1},,a_{N-1}\) with \(a_{n}\{1,,N_{}\}\), e.g. walking in a room. As a result of each action, the agent receives a perceptually aliased observation , resulting in the stream of random variables \(X_{1},,X_{N}\) with observed values \(x_{1},,x_{N}\), where each \(x_{n}\{1,,N_{}\}\). CSCG  is a probabilistic sequence learning model that introduces a latent explanatory variable \(Z_{n}\) at each timestep \(n\), with values \(z_{n}\{1,,N_{}\}\), to model the action-conditional stream of observations as

\[P(x_{1},,x_{N}|a_{1},,a_{N-1})=_{z_{1},,z_{n}}P(x_{1}|z _{1})P(z_{1})_{n=2}^{N}P(x_{n}|z_{n})P(z_{n}|z_{n-1},a_{n-1}).\]

A transition tensor \(T:\,T_{ijk}=P(Z_{n}=k|Z_{n-1}=j,a_{n-1}=i)\, n\) represents the action-conditional dynamics. \(T\) defines a directed multigraph, whose nodes correspond to the values of \(z\). Conditioned on an action, each entry of \(T\) is the weight of a directed edge between two nodes (from the row index to the column index of that entry). A CSCG can thus recover a graph that represents the latent causal structure  of the environment (see Fig. 1D for an example) which can then be used for planning.

An emission matrix \(E:\,E_{ij}=P(X_{n}=j|Z_{n}=i)\,\, n\) represents the observation probabilities. CSCGs have a deterministic observation model: for any latent value \(z\), the same observation \(x\) is always emitted. Multiple values of \(z\) can result in the same observed \(x\), making the model overcomplete . The restriction to deterministic \(E\) makes CSCGs less general than a hidden Markov model (HMM), but easier to learn . A CSCG can disambiguate multiple aliased percepts (same observation \(x\)) into distinct causes (different latent values \(z\)) given a sufficiently long context. If the observations correspond to word tokens, CSCGs can also be used as a language model, with a single action that accesses the next token1.

### Rebinding in CSCGs

On encountering a new environment with a similar structure but different observations, an agent can learn that environment faster by reusing the latent graph \(T\) from prior experience and relearning just the emission matrix, through a _rebinding_ process . Rebinding can be interpreted as a soft intervention on the agent's model [16; 17]. See Fig. 1D & F for examples of two rooms that share the same latent structure but different observations. When a new emission matrix binds to an existing schema, it has to respect the _clone structure_ of the original emission matrix (Fig. 1E). The clone structure function \(() 1,,N_{}\) partitions the latent state in \(N_{}\)_slots_: two latent states \(z=i\) and \(z=i^{}\) belong to the same slot iff \((i)=(i^{})\). An emission matrix respects the clone structure \(\) if \((i)=(i^{}) E_{ij}=E_{i^{}j}\,\, \,i,i^{},j.\) The 3-tuple \(\{T,,E\}\) defines a _grounded schema_, the tuple \(\{T,\}\) defines an _ungrounded schema with clone structure_ and \(T\) alone is a _schema_.

#### 2.2.1 Fast rebinding by attending to surprise

Often, environment changes are localized such that most of the latent structure and observation mapping is preserved while just a few observations need to be rebound: for example, just replacing the carpet in a room while the wall colors remain the same, or getting exposed to a new word in a familiar context. This insight can be utilized to derive an algorithm that focuses the update of the emission matrix only to those observations that were found surprising by the existing model.

Suppose that at test time, a grounded schema \(\{T,,E^{}\}\) is exposed to a sequence with novel observations. Algorithm 1 proposes a fast procedure to update the emission matrix to the new observations by only performing local updates, and to bind the updated emission matrix to the existing schema \(T\), defining a new grounded schema \(\{T,,E^{}\}\). We call this process _fast rebinding_.

Given a prompt \((x_{1},,x_{N})\) and a surprise threshold, Algorithm 1 proceeds by (a) identifying emission matrix entries that need updating then (b) updating these entries using the Expectation-Maximization (EM) algorithm . The conditional probability \(P(X_{n}=j x_{ n})\) of tokens at timestep \(n\) given all other tokens is used to identify timesteps and latent states that are surprising. Step 3 identifies _anchors_, i.e., latent states corresponding to observations that are correctly predicted with high confidence: anchors are not rebound. Step 4 identifies _candidates for rebinding_ as latent states (a) not among the anchor states and (b) corresponding to timesteps at which observations are incorrectly predicted with high confidence. Finally, instead of re-learning the whole emission matrix [15, Appendix A.2], Step 5 (detailed in Appendix A) _locally updates_ the emission matrix by only applying EM on the latent states and timesteps identified in Step 4. As a result, only a small subset of rows differ between \(E^{0}\) and \(E^{}\). Protected rows correspond to either (a) anchors in the current prompt or (b) slots not relevant to the current prompt but possibly relevant to future observations. Section 6 discusses how a similar mechanism could be implemented in transformers.

**Input:** Grounded schema \(\{T,,E^{0}\}\), pseudocount \(\), prompt \((x_{1},,x_{N})\), surprise probability threshold \(p_{}\).

**Output:** Rebound emission matrix \(E^{}\)

```
1: Define \(^{0} E^{0}+\), with normalized rows.
2: For timestep \(n\), use the emission matrix \(^{0}\) to compute \(P(X_{n}=j x_{ n})=P(X_{n}=j x_{1},,x_{n-1},x_{n+1}, ,x_{N}),\; j N_{}\)
3: Identify latent states and timesteps that can act as anchors: \(=\{(i,n)\;\;P(X_{n}=x_{n} x_{ n})>p_{},\;(i)=x_{n}\}\)
4: Identify latent states to be rebound (and their timesteps): \(=\{(i,n)\;\;P(X_{n}=j x_{ n})>p_{},\;j x_{n},\;(,n),\;(i,) (i)=j\}\)
5: Fix \(T\), and use EM to update the emission matrix (initialized with \(E^{0}\), and without using any pseudocount) by only using the beliefs for latent states \(i\) and timesteps \(n\) such that \((i,n)\). ```

**Algorithm 1** Fast rebinding algorithm

Figure 1: **A. Inducing room structure (_cognitive maps_) from sequential sensory observations is challenging due to perceptual aliasing – local observations do not identify locations uniquely. B. Cloned hidden Markov models (HMMs) . Each observation is mapped to multiple clone states in the latent space. C. Graphical model for CSCGs , extending cloned HMMs by incorporating actions. CSCGs utilize the latent space to overcome the perceptual aliasing problem. Different clones learn to represent different temporal contexts to recover the latent structure of the room. D. Learned CSCG for the room shown in panel A consists of a latent transition matrix and an emission matrix. We visualize the model in two ways: (i) stacking clone states for respective observations into columns, and (ii) clones as nodes in a transition graph, colored with their respective emissions. E. The emission matrix imposes a _slot_ structure – nodes within the same slot are constrained to bind to the same observation. A new environment with the same latent structure but different observation mapping (Room 2) can be learned quickly by freezing the transition graph and slot structure, and learning a new emission matrix by rebinding slots to a new set of observations. F. CSCG for a _different room_ learned purely through rebinding.

After rebinding, we complete the prompt by performing MAP inference conditioned on the provided prompt in the rebound CSCG. We run the max-product algorithm  forward (the backward messages are all uniform) thus generating a series of MAP observations for the tokens following the prompt. We stop once we generate a delimiter token. See Algorithm 2 in Appendix B for details.

## 3 Outline of the overall argument using CSCG

### Context-dependent latent representations and transitive generalization

The clone structure of CSCGs allows context-based separation and appropriate blending for language modeling. For example, the sense of the word "bank" in "bank robber" is different from the one in "river bank". CSCG learning disambiguates these contexts in the latent space by wiring them to different clones to improve predictive accuracy. In Fig. 2A, the sentences "river bank resort", and "one bank robber" use different clones of "bank". Sequences can have probabilistic branching: "one bank robber" can terminate at "\(\)n", or continue to "eating at river bank resort" or "eating bread and honey", or "eating bread and butter at river bank resort" (Fig. 2B). CSCGs also allow the merging of contexts that result in transitive generalization: even if training data has only the sequences "breach and butter", and "milk and honey", if they go through the same clone state "and", the model will generalize to "bread and honey" and "milk and butter", assigning non-zero probability to those sequences. Due to the combination of context-sensitive separation and transitivity, related topics, concepts, and algorithms get clustered into sub-networks that pass through the same clones. A prompt's context would activate its sub-network, and transitive generalization allows for prompts that are not exact memorizations. As we show in Section 4, the Bayesian inference perspective on ICL  corresponds to this context-sensitive and transitively generalizing storage and retrieval alone, and is insufficient to explain the ICL properties we consider in the next sections.

### Learning flexible schemas (template circuits) and rebinding

Just like learning room layouts, CSCG can learn automata circuits  for sequence-to-sequence (seq2seq) algorithms. See Fig. 2 for CSCG circuits for computing parity, copying a sequence, and reversing sequences of multiple lengths. The list reversal circuit in Fig. 2E is bound to the specific symbols \(A,B,C,D,E\) used in training. For use as a template, slots in this graph must be able to appropriately bind to contents (arbitrary symbols) that occur in context at test time [8; 21]. The rebinding mechanism (formalized in Algorithm 1) can intuitively be understood as operating based on prediction errors - when the latent context strongly predicts the latent state corresponding to a time instant, but the actual observation is mismatched, rebinding adjusts the emission matrix to wire all the clones of that latent state to the surprising observation. Such a mechanism to mix and gate previous knowledge with new content allows circuits learned during training to become flexible templates

Figure 2: **A.** CSCGs allow both separation of contexts and transitive generalization. The word “bank” is wired to multiple clones corresponding to the different contexts it is used in. If “milk and honey”, and “bread and butter” are seen in training, transitive generalization occurs if they get wired through the same “and” clone: “bread and honey” and “milk and butter” appear as valid sequences. **B.** Probabilistic branching & merging of sequences. **C – F.** Exemplar CSCG circuits for copying a sequence, parity operation, reversing a list with exactly five elements, reversing lists with a variable number of elements. **G.** Rebinding to new observations: dashed gray arrows correspond to old emissions while green arrows correspond to newly rebound emissions.

with slots that can dynamically bind to new inputs as required. For example, in the list reversal schema in Fig. 2F, tokens "[l], and "]" are prior contents that detect the beginning and end of the list - these act as anchors for grounding the schema in the observations. Probabilistic branching based on the end of list token "]" allows for length generalization, whereas absorbing arbitrary symbols into the slots corresponding to \(A,B,C,D,E\) allows the algorithm to generalize to new symbols. Fig. 2G illustrates the outcome of this rebinding mechanism where the slots emitting \(A,B,C,D,E\) are respectively rebound to symbols \(K,M,N,P,R\) from the input prompt. Similarly, in the sentence "I wrote in a notebook using a dax", rebinding can absorb the new token "dax" into the context by binding it to a clone corresponding to "pencil" or "pen", and use the new word in those contexts.

### Instruction-based or content-based retrieval and completion of tasks

Zero-shot task recognition as content-based retrieval using rebinding:Many striking examples of zero-shot learning involve recognizing tasks from prompts, and repeating them on new inputs. For example, given a prompt "Input: [p, q, r, s] Output: [p, p, q, q, r, r, s, s]; Input: [l, m, n, o] Output: [l, l, m, m, n, n, o]" LLMs can infer the task as repeating the elements of the sequence, and apply that to complete the output for a new input prompt even when the tokens "p, q, r, s, l, m, n, o" were not seen during training, in association with this task. The rebinding mechanism offers a natural explanation for this. Given the prompt, expectation maximization (EM)  simultaneously evaluates the different rebindings to multiple latent algorithm schemas to infer the best binding, which is then applied to complete the query prompt.

Instruction-based retrieval:When algorithms are trained with prefixed language instructions, CSCGs learn instruction sub-networks that directly point to the circuits that represent the algorithms (see Section 4.2). The algorithm can be retrieved by direct prompting with language instructions that can be significantly different from training instructions due to transitive generalization and rebinding.

### Emergence

We hypothesize and empirically demonstrate in Section 4, that emergence is explainable as the combined effects of the above properties (context-separation, transitive generalization, schema-formation, and rebinding), model capacity, and patterns in the data. Training on a bigger dataset results in the induction of more templates that might not have occurred in the smaller dataset. Learning the schematic circuits for more complex algorithms or more patterns in the data requires greater model capacity because overparameterization helps in the optimization process.

## 4 Results

We substantiate the above argument using empirical results on three datasets: (a) the GINC benchmark introduced in , (b) a suite of algorithm learning tasks that we introduce in our LIALT datasets, and (c) a zero-shot word usage induction task on a CSCG language model.

### Context-sensitive retrieval on GINC dataset matches Bayesian inference explanation

**Dataset:** The GINC dataset  introduced for studying ICL, is generated from a uniform mixture of five factorial HMMs . Each factorial HMM is referred to as a _concept_. A document is created by concatenating independent sentence samples from a concept. The in-context test prompts have examples of lengths \(k\{3,5,8,10\}\), varying in number from \(n=0\) to \(n=64\), with \(2500\) prompts for each setting \((k,n)\). Each prompt uniformly selects a concept, samples \(n-1\) examples \(x^{(1)}_{:k},,x^{(n-1)}_{:k}\) of length \(k\), and one example \(x^{(n)}_{:k-1}\) of length \(k-1\). The in-context task is to infer the most likely last token of the last example, i.e., \(*{argmax}_{x^{(n)}_{k-1}}p(x^{(n)}_{k-1}|x^{(1)}_{:k}, ,x^{(n-1)}_{:k},x^{(n)}_{:k-1})\). Since the vocabulary is shared among different latent concepts, observations in GINC are aliased like in natural language, and solving the task requires the model to disambiguate the aliased observations to correctly infer the latent concepts.

**Training:** We train a single CSCG with \(50\) clones on the GINC dataset for \(100\) full-batch EM iterations using a pseudocount  of \(=10^{-2}\). Given a test prompt, CSCG infers the most likely hidden sequence for that prompt, then predicts the next most likely observation.

**Results:** CSCG learns different latent sub-networks corresponding to the five latent concepts in the GINC dataset ( Fig. 3A), and inference on a supplied prompt retrieves the correct latent sub-network (Fig. 3C). Increasing the prompt length improves the localization of the sub-network and the particular states within the sub-network. Figure 3C visualizes the decoded latent state distribution for an example prompt in the zero-shot setting (\(n=0\)). The decoding starts out uncertain, and improves as the prompt gets longer. This localization (on the graph) results in effective schema retrieval, and hence accurate prompt completion. Figure 3B[left] reports the in-context accuracy--defined as the average ratio of correct predictions--for each \((k,n)\) pair of the GINC test set. CSCG in-context accuracy matches the patterns exhibited by LSTMs and transformers in , while slightly improving their performance. Fig. 3B also shows that a CSCG with larger capacity, i.e. with \(50\) clones per token, better separates the latent concepts and significantly outperforms a CSCG with only \(10\) clones per token. Fig. 9[left] in Appendix C displays the CSCG in-context confidence: for larger contexts, CSCG is better at disambiguating aliasing and the averaged predictions probabilities are higher. Finally, Fig. 9[right] shows that similarly to the transformer and LSTM in , CSCG fails at ICL when test prompts are sampled from concepts unseen during training. The GINC results match the context-based retrieval argument in Section 3.1: ICL in this setting is the retrieval of a shared latent concept between the prompt and the model. By using the long-range coherence of concepts in the training documents, the model learns to separate concepts into different latent representations. Despite the train and prompt distribution mismatch , CSCG succeeds at prompt completion because the representation allows transitive mixing.

### Learning schemas for seq2seq algorithms and generalization using rebinding

**Training dataset:** To test the ability of CSCG to learn _algorithms_ that generalize to novel inputs not seen during training, we construct the Language Instructed Algorithm Learning Tasks (LIALT) dataset. The LIALT training set contains demonstrations of \(13\) list and matrix algorithms displayed in Fig. 4A[top-left]. A demonstration consists of a multi-word language instruction--each algorithm has five different instructions--followed by \(10\) input-output examples of that algorithm. See Tables 2 & 3 in Appendix D.1 for the complete list of instructions used. For each instruction, the dataset contains \(20\) demonstrations. Within a demonstration, the language instruction and the examples are separated by a "/" delimiter. Demonstrations are separated by a "\(\)n" delimiter. The input lists and matrices values are created by uniformly sampling from a vocabulary of \(676\) tokens, created by random pairings of uppercase letters. List operations examples vary in length from \(3\) to \(6\), and the matrix operations are of sizes \(2 2\) or \(3 3\). Fig. 4A [bottom-left] shows the training data format.

Figure 3: **A.** Visualizing the transition graph of a CSCG with \(50\) clones trained on the GINC dataset from . The clones cluster into five groups – one per _concept_. **B.**[Left] In-context accuracy averaged over the GINC test dataset (with \(95\%\) confidence intervals (CIs) as in ), for the same model For contexts of \(8\) and \(10\) tokens, the model predicts the most likely next token at least \(95\%\) of the time—including in the zero-shot regime. [Right] In-context accuracy decreases when we reduce the number of clones to \(10\)—for \(k\{8,10\}\) it drops from above \(95\%\) to below \(75\%\). The numerical values are reported in Appendix C, Table 1. **C.** Decoded latent state distributions (increasing intensities of black for higher density) for the CSCG with 50 clones, for an \(n=0\) & \(k=10\) prompt “o y w r m r o y aj”, when truncated to different lengths (\(k=2,3,5,8,10\)). Longer prompts improve latent state estimation—resulting in better concept retrieval, and next token prediction.

Test dataset:LIALT has two test datasets, respectively containing: (a) instruction-based retrieval prompts, and (b) example-based retrieval prompts. An instruction-based retrieval test prompt consists of a natural language instruction followed by a single input. An example-based retrieval test prompt consists of a first input-output example of an algorithm, without any natural instruction, followed by a second input. All the lists and matrices in the two test datasets contain novel tokens. For both types of prompts, the in-context task is to predict the algorithm's output when applied to the (last) input. Note that for an example-based prompt, CSCG has to infer the algorithm used from the first example. Each test set contains \(100\) prompts, constructed by uniformly sampling instructions, and list or matrix tokens. Fig. 4A [right] shows the formats of these two test sets.

Training:For each token, a CSCG allocates its number of clones proportionally to the number of distinct contexts in the training data in which it occurs2. We parameterize CSCG capacity via this proportionality factor - the "overallocation ratio". We train CSCGs for an increasing sequence of overallocation ratios on the training data with \(500\) EM iterations and a pseudocount of \(=10^{-6}\). After running EM, we run \(10\) iterations of Viterbi training .

Figure 4: **A.** [Top-left] List and matrix algorithms used in the LIALT dataset. Format of the training set [bottom-left] and examples of the two LIALT test sets [right]. **B.** Example of a learned circuit for the “reverse” algorithm, displayed by stacking clones [left] or unrolling them [right].

Figure 5: [Left] In-context accuracy (ICA) with \(95\%\) CIs after a single EM iteration, as a function of the overallocation ratio for a CSCG trained on LIALT and averaged [top] on the instruction-based LIALT test set [bottom] on the example-based LIALT test set. ICA increases with model capacity. [Right] ICA with standard errors per task on the two LIALT test sets: for each task, overparametrization improves performance. Invisible bars indicate zero accuracy for the respective combinations of model and task. All the numerical values are in Appendix D.3. Figure 11 in the Appendix visualizes the same quantities after EM convergence; the similarity demonstrates that the fast rebinding algorithm is not just localized in its updates, but also rapid.

**Results:** CSCGs with sufficient model capacity successfully learn the algorithms in the training set, and rebinding generalizes those algorithms to novel tokens. Fig. 4B shows the extracted circuit for the list reversal algorithm. Fig. 5[left] presents the in-context accuracy of CSCGs (using \(=10^{-6}\) and \(p_{}=0.1\)) on the two LIALT test sets: the best performing CSCG (a) successfully rebinds the learned schemas to the test prompts' novel tokens and (b) correctly infers the algorithm from a single input-output pair for example-based prompts. Fig. 5 also shows that model size drives ICL performance [left] even when breaking down the performance by tasks [right].

The learned CSCG (initialized with an overallocation ratio of \(3\)) is visualized in Fig. 10 in the Appendix, using stacked clones. Fig. 6A shows the transition graph using the Kamada-Kawai algorithm . It reveals thirteen loosely connected clusters corresponding to the thirteen algorithms present in the LIALT dataset. Fig. 6B illustrates the rebinding process, with the decoded distributions over latent states of the learned CSCG model, for two different example-based prompts. Even before any rebinding, the identification of anchors and slots already restricts the decoding to schemas compatible with the prompt _structure_--in this case based on brackets & delimiters. However, the structure is insufficient to disambiguate completely between the compatible schemas (list operations corresponding to reversal, circular forward shift, and circular backward shift), and both the chosen prompts result in the same latent state distribution. Hence, the decoded distribution after the first E-step localizes to the three compatible schemas. In the M-step that follows, the slots in all three schemas will be rebound for this prompt. At the end of the first EM iteration, the new bindings for slots in the correct schema will be highly certain given the consistent evidence, while inconsistent evidence will lead to uncertain bindings for the other slots. In the E-step of the second iteration, the respective levels of certainty in the bindings then help promote the correct algorithm schema to become the most likely decoding--and complete the prompt appropriately. Note that a single EM step is sufficient to derive the correct rebinding in these examples. Compare Figs. 5 & 11, and the

Figure 6: **A.** Transition graph of the CSCG model learned on the LIALT dataset, visualized using the Kamada-Kawai algorithm. **B.** Visualizing the inferred probability of the observation at timestep \(n\), conditioned on observations at all other timesteps, before rebinding. This drives the identification of anchors and slots selected for rebinding. **C.** Decoded latent state distributions (and predicted prompt completions) for the two different example-based LIALT prompts specified in subfig. B: (top) before rebinding, and (bottom) after one iteration of EM. Fig. 12 in Appendix D.3.2 extends the same visualization to EM convergence. The left prompt corresponds to the operation of circularly shifting the list forward, and the right prompt corresponds to reversing the list.

tables in Appendix Sec. D.3 for how the in-context completion performance after the first EM step in the rebinding process is very similar to that at the end of EM convergence.

The LIALT results substantiate the arguments we made in Sections 3.2 and 3.3. Bayesian inference of the latent context based on long-term coherence (sufficient for the GINC results in Section 4.1) does not explain the remapping of a latent representation to completely new tokens as required for generalizing on the LIALT algorithms. Without rebinding, even a prompt containing a full-length example of an algorithm but with novel tokens does not retrieve the correct algorithm schema or produce the correct completion based on inference over the latent states alone (Fig. 6B, first row). By contrast, simultaneously inferring the rebindings and the latent states results in accurate retrieval of the algorithm schema and the correct prompt completion (Fig. 6B, second row). CSCGs are thus able to learn seq2seq algorithms and generalize those algorithms to novel tokens using rebinding.

**Emergence:** ICL performance of CSCG on the LIALT dataset shows characteristics attributed to emergence. In-context accuracy has a clear dependency on the level of overparameterization of CSCG, offering evidence in support of our hypothesis in Section 3.4.

### Dax test

In language, the "dax" test  is used to demonstrate the capability of a model to absorb the usage of an entirely new word from a single presentation. To test for this capability, we train a CSCG on the PreCo dataset , which is a large-scale English dataset for coreference resolution. We then test the model on five word-replaced query prompts, where certain words in the prompts do not appear in the training set. We use Algorithm 1 with \(=10^{-6}\) and \(p_{}=\) to rebind the emission matrix on each of these prompts, each time probing the model for completing a sentence by filling in the blanks (uncertain inputs) using MAP inference. Fig. 7 shows these results.

## 5 Related work

**In-context learning:** Similar to how humans learn by analogy  and how synaptic plasticity allows the brain to rapidly adapt to a new task , ICL capabilities  allows a pre-trained model to learn a new task given only a few examples. [29; 30] showed how demonstrations that explicitly guide the reasoning process improve the ICL performance of transformers on new complex tasks. We clarify below some concepts that should not be confused with ICL, and then discuss some works that aim at understanding ICL and the factors that influence it.

**Supervised learning (SL) and few-shot learning (FSL):** SL approaches learn a mapping that minimizes a loss on the training data: gradient methods are a popular paradigm [31; 32; 33]. In FSL, a model learns to rapidly adapt to a new task from a limited number of supervised examples [34; 35; 36], and performs this same task at inference. In contrast, ICL tasks are only revealed at inference. [37; 38] showed that finetuning transformers on ICL instructions improves their ICL performance.

**Meta-learning:** The meta-learning paradigm aims at learning to adapt to a new task with only a few examples [39; 40; 41] by using multiple learning experiences. In contrast, ICL directly emerges from the pre-trained model. [42; 43] proposed a meta-learning framework for ICL where the model is fine-tuned: it learns to leverage few-shot examples and to adapt to new tasks at inference time.

**How ICL works:** explained ICL as implicit Bayesian inference and constructed the GINC dataset (see Section 4.1) for demonstrating ICL.  abstracted ICL as an algorithm learning problem and found that a transformer can implicitly infer a hypothesis function. Similarly,  showed that a transformer can be trained to perform ICL of unseen linear functions, with performance

Figure 7: Examples of the dax test on a CSCG trained on the PreCo dataset. In each row, the novel word in red (e.g. “terras”) is absorbed by binding it to the clones of the corresponding word in blue (e.g. “planets”). The CSCG can then use the new token in similar contexts, as demonstrated by the fill-in-the-blanks probing.

comparable to the optimal least squares estimator.  showed that, in the linear case, transformers implicitly implement gradient descent and train an implicit linear model on the ICL examples.  proposed a dual between transformer attention and gradient methods and suggested pre-trained models as meta-optimizers. They presented ICL as implicit finetuning, where the forward pass on the demonstrative examples produces meta-gradients. Finally,  showed the existence of "induction heads" in transformers, that emerge during training, copy previous patterns, and drive ICL capacities.

**What influences ICL:**[1; 47] indicated that LLMs' ICL performance "emerges" then keeps improving when the model size increases.  proposed a substitute for the positional encoding, and demonstrated how transformers can learn schemas for algorithmic tasks and generalize to test sequences longer than any seen during training. Some works have highlighted the role of the training data in ICL.  showed that ICL emerge when the training data has a large number of rare classes and when examples appear in clusters. while  demonstrated that ICL emerge when a model is trained on a combination of multiple corpora, and that low perplexity and ICL performance do not always correlate. [51; 52] found that ICL is highly unstable and is influenced by the prompting template, the selection of in-context examples, and the order of the examples.  showed that the ICL performance is driven by the exposure to the label space, the input distribution, and the overall format of the sequence. Similarly,  found that selecting ICL examples with closer embeddings to ICL test sample improves ICL performance, and  showed that adding explanations in-context improves performance. Finally,  recently claimed that the sharp emergence of ICL in larger models might be an artifact of the metrics, not a fundamental property of the model.

## 6 Discussion

With ICL reconstructed into schema learning, schema retrieval, and slot rebinding, an interesting avenue for future work would be to probe various sequence models for how robustly each of these components are manifested - or even construct models around these principles. Here we consider how this framework might map to transformers, where the phenomenon of ICL was originally observed. Unlike CSCGs, transformers buffer the inputs and represent location as positional encoding, allowing attention to gate by the structure of the prompt, along with the contents. Prior explanations [3; 4] do not distinguish the role of sequence positions vis-a-vis contents; we argue that theories might need to emphasize this distinction (see Fig. 8A) to fully understand the inductive biases behind ICL. We conjecture (see Fig. 8B) that layers of the transformer implement multiple mixed templates of positions and content, evaluated at different offsets of a prompt. The template assembly that can auto-regressively match the prompt wins out the competition to gate the content. The rebinding mechanism requires only a few iterations of sparse updates to the emission matrix, and can be temporally "unrolled" into a forward pass, allowing ICL behavior with fixed weights since the slotting process lives in the space of activations.

Coming back to CSCGs, implementations can scale to larger models and datasets by exploiting sparsity and parallelizing computations in the EM steps. Allowing a factorized latent space and adding skip connections would also allow compositionality while enabling scalability. Further, while we have illustrated here the concept of rebinding to attach new symbols to existing slots, rebinding "through time" can also target connections between clones, enabling compositional behavior in-context. We leave these explorations for future research. Our goal here has been to elucidate a general framework for ICL behavior, leveraging the interpretability of CSCGs. We hope this demystifies the ICL behavior observed in LLMs by analogy, showcases avenues for further research on ICL capabilities, and provides broad impetus for interpretable methods.

Figure 8: **A**. Learned templates in a transformer could involve content, position, or a mix of both. **B**. Activations in the forward pass of a transformer could be selected among pre-learned templates that mix content and position to achieve ICL without weight changes.