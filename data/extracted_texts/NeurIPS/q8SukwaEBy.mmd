# Learning from Active Human Involvement through Proxy Value Propagation

Zhenghao Peng\({}^{@sectionsign}\), Wenjie Mo\({}^{@sectionsign}\), Chenda Duan\({}^{@sectionsign}\), Quanyi Li\({}^{}\), Bolei Zhou\({}^{@sectionsign}\)

\({}^{@sectionsign}\)University of California, Los Angeles, \({}^{}\)University of Edinburgh

###### Abstract

Learning from active human involvement enables the human subject to actively intervene and demonstrate to the AI agent during training. The interaction and corrective feedback from human brings safety and AI alignment to the learning process. In this work, we propose a new reward-free active human involvement method called _Proxy Value Propagation_ for policy optimization. Our key insight is that a proxy value function can be designed to express human intents, wherein state-action pairs in the human demonstration are labeled with high values, while those agents' actions that are intervened receive low values. Through the TD-learning framework, labeled values of demonstrated state-action pairs are further propagated to other unlabeled data generated from agents' exploration. The proxy value function thus induces a policy that faithfully emulates human behaviors. Human-in-the-loop experiments show the generality and efficiency of our method. With minimal modification to existing reinforcement learning algorithms, our method can learn to solve continuous and discrete control tasks with various human control devices, including the challenging task of driving in Grand Theft Auto V. Demo video and code are available at: https://metadriverse.github.io/pvp.

## 1 Introduction

Reinforcement learning (RL) has been successfully applied in many domains, ranging from board game Go , strategy game StarCraft II , autonomous driving , and even nuclear fusion . Existing RL methods assume the manually designed reward functions can fully express human intents and preferences. However, the resulting agents might exhibit biased, misguided, or undesired behaviors due to faulty reward functions . Moreover, the poor sample efficiency as well as the safety concern due to the trial-and-error exploration prevent the real-world deployment of RL.

Human-in-the-loop methods are promising to achieve alignment, learning efficiency, and safety. Human-in-the-loop policy learning relies on human subjects to oversee the learning process of the autonomous agents, thus it can better align the learned behaviors with the preferences of humans compared with handcrafted reward functions. Different forms of human involvement in human-in-the-loop policy learning have been studied over the years. Human subjects can advise actions upon the requests of the robots  or provide preference-based feedback to assess the relative value of the collected trajectories . These methods learn from passive human involvement, where the human subjects do not provide real-time feedback and intervention during data collection. For safety-critical tasks such as autonomous driving, safety is undoubtedly the first priority in human preference and the passive involvement methods yield unbounded risks in such settings. An increasing body of works focuses on active human involvement, where human subjects actively intervene and provide demonstrations during the execution time . With online correction and demonstration from human subjects, AI alignment and training-time safety of the system can be substantially enhanced.

In this work, we focus on learning from active human involvement and develop a simple yet effective method that can turn a common value-based RL method into a reward-free human-in-the-loop method with minimal modification. Our key insight is that we can learn a proxy value function from active human involvement, such that the proxy values encode human intents and guide policy learning to emulate human behaviors. Specifically, we propose the _Proxy Value Propagation (PVP)_ method which labels high Q values to human actions and low Q values to agent actions that are intervened by the human subjects. The proxy values are then propagated to unlabeled state-action pairs in the agent's exploration through TD-learning. Value-based RL methods soon learn policies that align with human intents because of the value-maximization nature. Experiments show that PVP can be successfully applied to both continuous and discrete action spaces, and achieve higher learning efficiency compared to baselines in various tasks, including driving in Grand Theft Auto V (GTA V). It is also compatible with different forms of human control devices, including gamepad, driving wheel, and keyboard. We summarize our main contributions as follows:

1. We propose a simple yet effective method, Proxy Value Propagation, that can be integrated into existing RL algorithms to learn from active human involvement. Our method is reward-free and can be generalized across various task settings and human control devices.
2. The experiments show that the proposed PVP method enables superior performance and high learning efficiency in various tasks from the MiniGrid, MetaDrive, CARLA, to GTA V environment. User study further shows that PVP achieves better performance and is more user-friendly compared to other human-in-the-loop baselines.

## 2 Related Work

AI alignment is one of the major issues in learning trustworthy intelligent agents for real-world applications. It is difficult to represent various human preferences into a scalar reward function in existing Reinforcement Learning (RL) methods [40; 6]. Meanwhile, the manually designed reward function, which might be misaligned with human preferences, often leads to undesired behaviors [23; 20]. As a promising complement to RL, Human-in-the-loop Learning (HL) can overcome costly reward engineering and convey human intents to the learning process directly through human involvement. Compared to imitation learning (IL) [14; 9], where the agent learns directly from high-quality human demonstration, HL methods benefit from interactive human involvement and feedback during the training, mitigating the possible distributional shift that usually happens when learning from offline data .

**Preference-based RL.** A large body of work focuses on learning human preference via ranking pair of trajectories generated by the learning agent [5; 11; 38; 52; 41; 36; 22; 51]. InstructGPT  aligns language models by first supervised learning in demonstration and then finetuning by the reward learned from human preference feedback. Preference learning can be applied to the tasks that human can not conduct, such as moving a six-legged Ant robot by assigning exact torque at each joint . For those tasks that human can demonstrate, these methods do not fully utilize real-time feedback from human subjects during agent-environment interaction.

**HL with Passive Human Involvement.** Different from preference-based RL, human subjects can provide direct feedback to the learning agent during training through passive human involvement. Some works learn policy from human-provided evaluative feedback, a Boolean flagging correct or wrong actions [19; 3; 32]. This is similar to the intervention in our framework. However, in , humans provide high-level instructions, e.g. pointing to the left/right, while in PVP humans provide intervention and low-level demonstrations. The other line of work allows the neural policy to operate the robot and the human subjects can provide demonstration upon the requests from the learning agents [28; 30; 16]. The expert policy will intervene when uncertainty is huge, where the agent uncertainty is estimated by the variance of actions . These methods reduce the cost of human resources but have potential risks to human subjects since they do not fully control the system. For example, when human subjects use these algorithms to train autopilot AI, they are exposed to significant risks if they are in a self-driving cars due to unpredictable agent behaviors.

**Learning from Active Human Involvement.** For safety-critical tasks such as autonomous driving, the safety of both the controlled vehicles and the human subjects is the top priority. There are many works that allow human subjects to proactively involve the agent-environment interactions based on their own judgment to ensure safety, which we call active human involvement. Human subjectscan terminate the episode if a near-accidental situation happens and such intervention policy can be learned [56; 1; 43; 35; 54; 50]. Recent studies explore active human involvement methods through intervention and demonstration in the human-agent shared autonomy [27; 30; 17; 47; 26; 16; 54]. However, previous methods do not fully utilize the power of human involvement. COACH  treats human labels as indications of advantage instead of simply as reward. Compared to COACH, our method accepts not only the feedback (the intervention signal) but also the human demonstration. Our method does not consider the time delay of human subjects explicitly as COACH does. Interactive imitation learning method (HG-DAgger)  does not leverage data collected by agents, while Intervention Weighted Regression (IWR)  does not suppress undesired actions likely intervened by human. Meanwhile, Expert Intervention Learning (EIL)  and IWR  focus on optimizing actions step-wise without considering the temporal correlation between steps. These drawbacks harm learning efficiency and thus incur more human involvement. Moreover, previous methods lack experiments to demonstrate the generalizability to different task settings and human control devices.

## 3 Problem Formulation

Policy learning aims at finding a policy to solve the sequential decision-making problem, which is usually modeled by a Markov decision process (MDP). MDP is defined by the tuple \(M=,,,r,,d_{0}\) consisting of a state space \(\), an action space \(\), a state transition function \(:\), a reward function \(r:[R_{},R_{}]\), a discount factor \((0,1)\), and an initial state distribution \(d_{0}:\). The goal of conventional reinforcement learning is to learn a _novice policy_\(_{n}(a|s):\) that can maximize the expected cumulative return: \(_{n}=_{_{n}}_{ P_{_{n}}}[_{t=0}^{T} ^{t}r(s_{t},a_{t})]\), wherein \(=(s_{0},a_{0},...,s_{T},a_{T})\) is the trajectory sampled from trajectory distribution \(P_{_{n}}\) induced by \(_{n}\), \(d_{0}\) and \(\). Here \(_{n}\) defines a stochastic policy, while deterministic policy can be denoted as \(_{n}(s):\) and its action distribution is a Dirac delta distribution \(_{n}(a|s)=(a-_{n}(s))\).

The reward function imposes an assumption that the reward can fully reflect the intentions of the users and incentivize desired behaviors. However, this assumption may not always hold and the learned agent may obtain biased behaviors or figure out the loophole to finish the task [23; 40]. Revisiting the primal goal when developing learning systems, we find the reward is not a necessity since what we really want to achieve is the realization of human preference in the learned behaviors and, as suggested by , the ultimate source of information about human preferences are human behaviors.

Imitation Learning (IL) methods directly learn \(_{n}\) from human behaviors. Assuming a human expert has a _human policy_\(_{h}(a_{h}|s):\), which outputs human action \(a_{h}\). Note that human action shares the same action space as novice action. IL learns from the trajectories generated by human policy \(_{h} P_{_{h}}\) and optimizes the novice policy to close the gap between \(_{n}~{} P_{_{n}}\) and \(_{h}\). Instead of generating an offline dataset and training novice policy against it [14; 9], we can incorporate a human subject into the loop of training for providing online data. This can mitigate the distributional shift since the data generated with human-in-the-loop has closer state distribution to that of the novice policy . This can be modeled by introducing an _intervention policy_\(I(|s,a_{n})\) to describe human subjects' intervention behaviors. In earlier methods such as DAgger , the intervention policy is a Bernoulli distribution and the control authority switches back and forth between the novice and the expert. It is unrealistic to invite a real human subject to be involved in such training. Later studies allow the human subjects to intervene and take full control [49; 43; 26; 54], which we call such setting as _learning from active human involvement_. During training, a human subject accompanies the novice policy and can intervene with the agent by taking over the control to demonstrate desired behaviors. The intervention policy can be considered as a deterministic policy denoted by \(I(s,a_{n}):\{0,1\}\) where \(a_{n}_{n}(|s)\) is agent's action. With notations above, the _behavior policy_\(_{b}\) that generates actions during training is:

\[_{b}(a|s)=(1-I(s,_{n}(s)))(a-_{n}(s))+I(s,_{n}(s))_{h}(a|s).\] (1)

With such a model of active human involvement, we can now formulate our objectives.

**Task-specified metrics.** Our primal goal is to find novice agents whose behaviors are well-aligned with human preferences. In this work, we inform the human subjects of the primal goal of the tasks, _e.g._ navigating to the destination in driving tasks. They are also aware of how task-specified metrics, such as success rate and route completion provided by the test environments, are computed.

These metrics serve as a proxy for human preferences in evaluating trained agents' performance. Unlike prior work where these metrics were used as rewards, our learning agent cannot access them. The only supervision sources in our method are human interventions, \(I(s,a)\), and demonstrations, \(a_{h}_{h}(|s)\).

**Preference Alignment.** In our method, humans can intervene at any time. Most interventions occur in near-accidental situations or when agents are performing poorly. Conversely, lack of intervention indicates alignment with human preferences. Hence, another goal is to develop a novice policy that minimizes human interventions during shared control. In the next section, we will discuss our insights and how we build a concise, general, and efficient learning method to achieve these objectives.

## 4 Method

We propose the _Proxy Value Propagation (PVP)_ method which can transform a value-based RL method into an efficient reward-free human-in-the-loop policy optimization method that learns from active human involvement. PVP is compatible with various task settings, such as continuous and discrete action spaces, as well as various human control devices. In this section, we first summarize the basic workflow of value-based RL before introducing the motivation and the design of PVP. We then describe the implementation details.

**Value-based RL:** The proposed human-in-the-loop method results from the minimum modification of existing reinforcement learning methods. Thus, we briefly introduce the background of related methods. Value-based RL optimizes the value function and policy iteratively. On the value function side, we denote the state-action value and state value of policy \(\) as \(Q(s,a)=[_{t=0}^{}^{t}r(s_{t},a_{t})]\) and \(V(s)=_{a(|s)}Q(s,a)\), respectively. A neural network is commonly used to estimate the value function with Bellman backup: \(Q(s,a) r(s,a)+_{a^{}}Q(s^{},a^{})\), where \(s^{}\) is the next state. To learn the value network \(Q_{}\) parameterized by \(\), stochastic gradient descent on the temporal difference (TD) loss is conducted \(J^{}()=_{(s,a,s^{})}\ |Q_{}(s,a)-(r(s,a)+_{a^{ }}Q_{}(s^{},a^{}))|^{2},\) where \(Q_{}\) can be a delay-updated target network. In this work, we adopt the TD learning in the **reward-free** setting. Remove the reward in the TD loss, the TD loss becomes:

\[J^{}()=_{(s,a,s^{})}\ |Q_{}(s,a)- _{a^{}}Q_{}(s^{},a^{})|^{2}.\] (2)

On the policy side, based on the learned value function, the deterministic policy \(_{n}\) parameterized by \(\) can be learned by maximizing the Q values: \(J()=_{s}\,Q(s,_{n}(s;))\). The optimal policy is expected to maximize Q values:

\[_{n}(s)=_{a}Q(s,a).\] (3)

### Proxy Value Propagation

We illustrate the active human involvement of PVP in Fig. 1. During training, the human subject supervises the agent-environment interactions (Fig. 1**A**). Those exploratory transitions by the agent are stored in the Novice Buffer \(_{n}=\{(s,a_{n},s^{})\}\). At any time, the human subject can intervene the free exploration of the agent by pressing a button in the control device (Fig. 1**B**). While pressing the button, the human takes over the control and provides a demonstration of how to behave. During human involvement, both human and novice actions will be recorded into the Human Buffer \(_{h}=\{(s,a_{n},a_{h},s^{})\}\). Concurrently with the human-agent shared control, our method keeps updating the novice policy by the novel Proxy Value Propagation mechanism (Fig. 1**C**), which will be discussed later.

In the shared human-agent control, human intervention serves as a distinct indicator of suboptimal agent performance, which could result from the agent executing perilous actions or exhibiting ineffective behaviors. Thus, the optimal policy learned by the agent should (1) strive to approximate the behaviors demonstrated by the human subjects and (2) avoid performing actions that are intervened by humans.

The key insight of this work is that we can manipulate the Q values to induce desired behaviors, given that value-based RL has the nature to seek value-maximizing policy as Eq. 3. As shown in Fig. 1**C**, for emulating human behavior and minimizing intervention, we sample data \((s,a_{n},a_{h})\) from the human buffer and label the Q value of the human action \(a_{h}\) with \(+1\) and the novice action \(a_{n}\) with \(-1\). This is achieved by fitting the Q network directly with PV loss:

\[J^{}()=,a_{h})}{}[|Q_{}(s,a_{h} )-1|^{2}+|Q_{}(s,a_{n})+1|^{2}]I(s,a_{n}).\] (4)

The transitions in the novice buffer are not intervened by the human subject, meaning they are aligned with human preferences. Meanwhile, those transitions also contain information of the forward dynamics [24; 55]. To exploit the information contained in these transitions, instead of discarding these data as in , we propagate the proxy values to these states via TD learning in Eq. 2 and use those transitions together with those human-involved transitions for the policy learning. The final value loss is evaluated as follows:

\[ J()=J^{}()+^{}()=&,a_{h})_{h}}{}[|Q_{}(s,a_{h})-1|^{2}+|Q_{}(s,a_{n})+1|^{2}]I(s,a_{n})\\ &+)_{h}}{}|Q_{ }(s,a)-_{a^{}}Q_{}(s^{},a^{}) |^{2}\] (5)

Then we follow the policy update process outlined in the base RL methods.

### Analysis

**Connection to CQL.** The proposed PVP method can be interpreted as adopting the Conservative Q-Learning (CQL)  objective for reward-free and online learning settings. It augments the CQL objective with an extra L2 regularization term imposed on the Q-values for human-involved transitions. In our online learning setting, Eq. 5 can be reformulated as:

\[J()=_{h},I(s,a_{n})=1}{}[Q_{}^{2}(s,a_{n})+Q_{}^{2}(s,a_{h})\]

\[\]

\[\]

CQL was originally proposed to mitigate the problem of overestimated Q-values in offline RL settings. These overestimations often lead to suboptimal policies due to the optimistic selection of actions with misleadingly high values. In our work, we deal with human actions and novice actions sampled from two different distributions, where overestimation might also occur. However, unlike CQL, PVP does not have access to a reward function, meaning the Q-values are not grounded in an estimation of true values. The additional L2 regularizer therefore serves to impose constraints on the Q-values, helping to prevent unbounded growth and potential overfitting. In Sec. 5.4, we compare the learned proxy Q-values under both CQL and PVP objectives. Our results indicate that human and agent actions are more distinguishable when learned through PVP.

Figure 1: Illustration of Proxy Value Propagation. **(A)** Human oversees the agent’s trial-and-error exploration with the environment. When the human subject does not intervene, the transitions will be recorded into the novice buffer \(_{n}\). **(B)** When the human intervenes, both novice action \(a_{n}\) and human action \(a_{h}\) will be recorded into the human buffer \(_{h}\) but only the human action will be applied to the environment. **(C)** In training, we use the human buffer to compute proxy value loss and propagate the human intent knowledge to all transitions via TD loss without access to the reward.

**Alternative to Reward Assignment.** On the other hand, a more straightforward idea than PVP is to assign a reward of +1 to human actions and -1 to agent actions during intervention. Unfortunately, it is not practical since the Bellman backup is conducted on the transition triplet \((s,a,s^{})\), where one has to use future states' values to estimate current values. Therefore, the reward must correspond to the action from the behavior policy, the action \(a\) causes the transitions from \(s\) to \(s^{}\). In our context, during involvement, the action \(a=a_{h}\) must come from human policy as the human subject is taking control. Though we can assign \(+1\) reward and compute value target in those human-involving transitions, we have no way to assign \(-1\) to the agent's actions because we don't know the next states caused by those actions and thus we can't compute the value target. It is also not practical to query the environment to get the next state \(s^{}(s,a_{n})\) as the \(a_{n}\) is a potentially danger or undesired action and replaying it in the real-world environment is not feasible. In our preliminary experiment, we find the policy fails to learn anything regardless of the amount of human involvement provided. This is because the reward will be \(+1\) for all the human-involving transitions and the learning agent will find a pitfall to maximize its rewards: it always demonstrates undesired behaviors so that humans will always take control, which yields a \(+1\) reward.

### Implementation Details

**Base RL Methods.** Our method can be implemented for both continuous and discrete action spaces by extending TD3  and DQN  with PV loss and the balanced buffer. While TD3 uses a deterministic policy, DQN adopts epsilon-greedy exploration that makes the policy stochastic. We remove the action noise in DQN and simply follow the argmax rule to select actions. Therefore, our method enjoys deterministic novice policy in both cases. The primary reason is that according to the feedback of human subjects, stochastic novice makes human subjects experience excessive fatigue due to the difficulty in monitoring and correcting agents' noisy actions. This design choice makes our method more user-friendly, as shown in the user study in Sec. 5.3.

**Balanced Buffers.** The intervention gradually becomes sparse as the agent learns to reduce human intervention. However, those sparse intervention signals contain even more important information on how to behave under critical situations. Previous method  stores agent data and human data into one buffer and samples them uniformly. Abusing the notations, the ratio between the transitions from the agent's exploration and from human involvement is \(|_{n}|:|_{h}|\) in each SGD batch. The human demonstrations are overwhelmed by the amount of agent-generated trajectories,

Figure 2: Evaluation of PVP under four different environments with human control devices. For each environment, we plot the test-time performance curve of the agent trained by the proposed PVP and the RL counterpart TD3. The x-coordinate is the total number of environment interactions, which indicates the time steps the training agent (in RL method) or the human-agent system (in our method) experiences during training. Compared to the RL counterpart, the proposed method achieves much higher performance with superior learning efficiency.

leading to inefficient learning of those critical human behaviors and even catastrophic forgetting. For example, the driving policy sometimes fails to master acceleration at the beginning of an episode, even though the human subject has already demonstrated the expected maneuver multiple times. This is because the demonstration of initial acceleration only lasts a short period of time and thus is scarce in the buffer. To address this issue, we balance the transitions coming from the human buffer and the novice buffer. In each training iteration, we sample two equally-sized batches \(b_{n}\) and \(b_{h}\) from \(_{n}\) and \(_{h}\) respectively, each has \(N/2\) samples. \(N\) is the batch size for the policy update. By concatenating \(b_{n}\) and \(b_{h}\), our method can balance the data from the human's demonstration and from the agent's exploration, and hence the ratio between two types of data in each SGD batch keeps \(1:1\). Therefore, in the initial acceleration example above, the balanced buffer recalls the acceleration behavior, preventing catastrophic forgetting.

## 5 Experiments

### Experimental Setting

**Tasks.** We conduct experiments on various control tasks with different observation and action spaces. For continuous action space, we use three driving environments, MetaDrive safety benchmark , CARLA Town01 , and a customized driving environment built upon Grand Theft Auto V (GTA V), a popular video game. In these tasks, the agent needs to steer the target vehicle with low-level acceleration, braking, and steering, to reach its destination. Specifically, in MetaDrive safety environments, the agent needs to avoid any crash in the heavy-traffic scene with normal vehicles, obstacles, and parked vehicles. In MetaDrive, there exists a split of training and test environments, and we present the performance of the learned agent in a held-out test environment. To examine our method with different observation modalities, we use the sensory state vector in MetaDrive and GTA V and the bird-eye view image in CARLA as observation. For discrete action space, we use MiniGrid Two Room task , which involves agent exploration such as moving toward a door and opening the door before reaching the destination. The observation of MiniGrid is the semantic map of the agent's local neighborhood. Please refer to Appendix E for more information about the environment setup.

**Evaluation Metrics.** In MetaDrive safety benchmark, we report _total safety cost_ as the number of crashes during training, which reflects the number of potential dangers exposed to the human subject during training. We also report _episodic return_, _episodic safety cost_, and _success rate_ as the test performance of the agents. Episodic safety cost is the average number of crashes in one

    &  &  \\  Method &  Human \\ Data \\ Usage \\  &  Total \\ Data \\ Usage \\  &  Total \\ Safety \\ Cost \\  &  Episodic \\ Return \\  &  Episodic \\ Safety \\ Cost \\  & 
 Success \\ Rate \\  \\  SAC & - & 1M & 2.76K \(\) 0.95K & 386.77 \(\)35.1 & 0.73 \(\)1.18 & 0.82 \(\)0.18 \\ PPO & - & 1M & 24.34K \(\) 3.56K & 335.39 \(\)12.41 & 3.41 \(\)1.11 & 0.69\(\)0.08 \\ TD3 & - & 1M & 1.74K \(\) 0.62K & 318.12 \(\)21.9 & 0.47 \(\)0.08 & 0.70 \(\)0.09 \\  SAC-Lag & - & 1M & 1.84K \(\) 0.49K & 351.96 \(\)101.88 & 0.72 \(\)0.49 & 0.73 \(\)0.29 \\ PPO-Lag & - & 1M & 11.64K \(\) 4.16K & 299.99 \(\)9.46 & 1.18 \(\)0.83 & 0.51 \(\)0.17 \\ CPO & - & 1M & 4.36K \(\) 2.22K & 194.06 \(\)0.86 & 1.71 \(\)1.02 & 0.21 \(\)0.29 \\  Human Demo. & 30K & - & 39 & 347.523 & 0.39 & 0.97 \\  BC & 30K (1.0) & - & - & 113.32 \(\)10.21 & 2.171 \(\)0.65 & 0.073 \(\)0.02 \\ GAIL & 30K (0.015) & 2M & 25.90K \(\) 8.15K & 81.51 \(\) 9.43 & 1.308 \(\) 0.23 & 0.0 \(\) 0.0 \\  HG-Dagger & 39.0K (0.76) & 51K & 56 & 116.393 & 1.979 & 0.045 \\ IWR & 35.8K (0.79) & 45K & 52 & 226.221 & 1.457 & 0.465 \\ HACO & 19.2K (0.48) & 40K & 130 & 143.287 & 1.645 & 0.139 \\  PVP w/o TD & 13.5K (0.34) & 40.5K & 70 & 252.447 & 1.277 & 0.220 \\ PVP w/ Reward & 12.8K (0.32) & 40K & 30 & 319.383 & 0.767 & 0.755 \\ PVP (Ours) & 14.6K (0.37) & 40K & 76.8 \(\)9.3 & 353.636 \(\)23.7 & 0.898 \(\)0.15 & 0.857 \(\)0.04 \\   

Table 1: Comparison of different approaches in MetaDrive-Keyboard. The overall intervention rate is given besides the human data usage.

episode. The success rate is the ratio of episodes in which agents reach the destination to the total test episodes. In CARLA, we report _route completion_ and _success rate_. Route completion is the ratio of the traveled distance to the length of the complete route. GTA V uses _route completion_ and MiniGrid uses _success rate_ to measure the performance. Except for total safety cost, the aforementioned metrics measure the test-time performance, which is tested when the agent runs independently without human involvement. For human-in-the-loop experiments, we also report the total number of human-involved transitions (_human data usage_) and the _overall intervention rate_, which is the ratio of human data usage to total data usage. These show how much effort humans make to teach the agents. We also design a user study to measure the experience of human subjects in Sec. 5.3.

**Human Interfaces.** To examine the generalizability of our method, we leverage multiple control devices: Xbox Wireless Controller (Gamepad), keyboard, and Logitech G29 Racing Wheel. We denote the MetaDrive tasks with three devices as MetaDrive-Gamepad/Keyboard/Wheel. As shown in Fig. 2, human subjects can takeover through control devices and monitor the training process through the visualization of environments on the screen. The Ethics statement is provided in Appendix A.

**Experimental Details.** We implement most of the code with Stable-Baselines3 . Training results of various baselines in MetaDrive tasks are obtained from the open-source code by . The RL baselines are repeated 5 times with different random seeds, while other human-in-the-loop methods are repeated fewer times due to limited human resources. In the training of the human-in-the-loop methods, a real human subject participates in each experiment and we do not use any simulated user input. During testing, there is no form of human involvement. For each experiment, we evaluate each checkpoint in the environment for multiple runs and report the average task-specified metrics as the performance of this checkpoint. We report the performance of the best checkpoint as the result of the experiment. We provide the standard deviation if the experiments are repeated multiple runs in tables and figures. All experiments with humans are conducted on a local computer with an Nvidia GeForce RTX 3080. The local computer can support real-time simulation and training. Hyper-parameters and other details are given in Appendix E and G.

**Baselines.** We test four native RL baselines: PPO , SAC , TD3  and DQN . We also test three safe RL baselines: Constraint Policy Optimization (CPO) , PPO-Lagrangian , SAC-Lagrangian . In all baselines above, the reward function and cost function (for MetaDrive Safety Benchmark) are defined by the environment and can be accessed by the agents. We also test IL methods Behavior Cloning (BC) and GAIL . Human-in-the-loop methods that learn from active human involvement are tested: Human-Gated DAgger (HG-DAgger) , Intervention Weighted Regression (IWR)  and Human-AI Copilot Optimization (HACO) .

### Baseline Comparison

**Comparing with RL Counterparts.** Fig. 2 shows the curves of test-time performance. In MetaDrive-Gamepad, our method achieves 350 returns in 37K steps. This takes about one hour in the real-world HL experiment. TD3 baseline fails to achieve comparable results even after 300K steps of training. In CARLA, PVP agents learn to drive within 30 minutes with our method, while TD3 cannot solve the task. In GTA V, PVP can solve the task with 1.2K human data usage and 20K total data usage. The whole experiment takes only 16 minutes. TD3 instead utilizes 300K steps to achieve similar

    & Human & Total & Route & Success \\  & Data & Data & Completion & Rate \\  PPO & - & 1M & 0.24 \(\) 0.013 & 0.0 \(\) 0.0 \\ TD3 & - & 1M & 0.11 \(\) 0.05 & 0.0 \(\) 0.0 \\  BC & 5K & - & 0.42 \(\) 0.08 & 0.20 \(\) 0.10 \\  HG-DAgger & 6.8K & 24K & 0.64 & 0.47 \\ IWR & 5.7K & 24K & 0.69 & 0.60 \\ HACO & 4.8K & 24K & 0.52 & 0.40 \\  PVP (Ours) & 6.6K & 24K & 0.92 \(\) 0.05 & 0.73 \(\) 0.08 \\   

Table 2: Results of different approaches in CARLA.

Figure 3: We visualize the action sequences generated by HACO and PVP agents in the same MetaDrive map who are trained to 40K steps. PVP has much smoother actions.

performance. In MiniGrid tasks, our method successfully solves the tasks while vanilla DQN fails, showing that PVP can learn an exploratory solution and can be incorporated into discrete action space. We also show experiments on one easier and one harder MiniGrid environment in Appendix F.2, where PVP greatly improves learning efficiency.

**Comparing with Human-in-the-loop Baselines.** Table 1 suggests all tested HL methods achieve extremely low safety violations in training compared to vanilla RL and Safe RL methods, empirically supporting the preference alignment of the active human involvement, if we consider human preference is to avoid safety violation. Compared to other human-in-the-loop methods, our method costs the lowest human efforts in terms of human data usage and overall intervention rate, while greatly outperforming baselines in testing performance. Since MetaDrive has a training and test environment split, the result suggests PVP can learn high-quality agents with generalizability. Similar results are shown in CARLA in Table 2. Compared to RL baselines, HL methods achieve decent success rates and route completion rates even with only 24K environmental interactions. Compared to HL baselines, PVP achieves the best route completion rate.

**Visualization.** In Fig. 3, we visualize the action sequences of the agents trained by PVP and a human-agent shared control baseline HACO . The angle and length of each arrow represent the steering and acceleration, respectively. The human subject's actions are marked with yellow. Compared to HACO method, PVP agent produces smoother actions, which explains its high user study scores shown in the next section.

### User Study

We design a user study questionnaire to assess the experience of human subjects. Details are provided in the human subject research protocol in Appendix B. Three aspects are considered: (1) **Compliance** measures whether the behaviors of the agent satisfy human intents. For example, a highly compliant agent behaves like human such that the human subjects feel like they are completing objectives by themselves. (2) **Performance** is the subjective evaluation from human subjects on whether the agent can solve the primal task, e.g. driving to the destination in navigation tasks. This score should be low if the agent cannot learn a particular behavior or forgets it even though human subjects have taught the agent multiple times. (3) **Stress** gauges the cognitive cost human subjects pay to train the agent. A typical source of stress is the annoying oscillation and jitter the agent demonstrates. Unexpected behavior that requires human's instant reaction also creates stress. A lower score means more stress.

Table 3 shows our method is the most user-friendly method. On the one hand, we use a deterministic novice policy that greatly alleviates the jitter and unexpected behaviors, reducing stress. On the other hand, our method masters human behaviors and suppresses undesired actions with the balanced buffer and proxy value, improving the user experience in compliance and performance.

### Ablation Studies

**TD learning:** As shown in Table 1 "PVP w/o TD", disabling TD learning via setting \(J^{}(Q)=0\) significantly damages the performance of PVP, suggesting that propagating information from human-involved states to other states is critical to the success of PVP.

**PVP with reward:** Both MetaDrive and CARLA results in Table 1 and 4 show that adding the environmental reward doesn't bring significant improvement in the learning performance, which might be caused by the fact that the native reward function might not be aligned with human preference.

    & HG-DAgger & IWR & HACO & PVP \\  Compliance & 3.0 \(\) 0.8 & 4.0 \(\) 0.8 & 3.0 \(\) 0.2 & 4.8 \(\) 0.5 \\ Performance & 2.2 \(\) 1.0 & 3.7 \(\) 0.9 & 3.3 \(\) 0.9 & 4.8 \(\) 0.5 \\ Stress & 3.2 \(\) 0.9 & 4.5 \(\) 0.5 & 2.3 \(\) 0.9 & 4.7 \(\) 0.6 \\   

Table 3: User study result. The maximum score for each item is 5.

**Balanced buffer:** We find that disabling balanced buffers (PVP w/o BB) makes the training unstable and leads to poor performance. This design avoids the catastrophic forgetting when the agent-generated data overwhelms the human demonstrations as in HACO .

**Novice buffer:** We find that PVP without the Novice buffer (PVP w/o NB) yields poor performance. The agent data stored in the novice buffer contains information on human preference and the forward dynamics of the environment. Thus, PVP does not discard the agent exploratory data as opposed to HG-DAgger .

**Stochastic policy:** We implement PVP based on Soft Actor-critic  so that the novice policy is now a stochastic policy. As shown in the "PVP w/ SP" in Table 4, introducing randomness in novice actions greatly reduces the performance. The human subjects report that the novice agents with stochastic policy oscillate frequently, making it hard to respond when the agents suddenly drive toward the side road. HACO  has similar human-AI shared control as PVP, but it adopts a stochastic policy. For comparison, we also implement HACO without a stochastic policy. "HACO w/o SP" suggests deterministic policy can not bring significant improvement to HACO.

**Regularization on Q values**: As discussed in Sec. 4.2, PVP objective can be interpreted as CQL with a newly introduced L2 regularization term on the Q values. We conduct the experiment to evaluate the performance of the vanilla CQL objective with other PVP designs in our reward-free online learning settings. As shown in "PVP w/ CQL" in Table 4, CQL objective yields worse performance. This experiment shows that the vanilla CQL doesn't work in this human active involvement setting. As shown in Fig. 4, the proxy value in the vanilla CQL method has a much larger magnitude which makes the values of behavior actions (the actions applied to the environment) and agent actions hard to distinguish. PVP has smoother proxy values with a clear margin between behavior and novice Q. CQL does not set a bound for the proxy value, thus proxy values in those extreme human actions are reinforced without a bound, making the novice policy rapidly learn those extreme actions, whereas PVP has bounded proxy values, leading to more stable training and better overall performance.

## 6 Conclusion

Learning through active human involvement is a promising approach enabling safe and efficient policy learning. In this work, we propose _Proxy Value Propagation (PVP)_ that can effectively learn from the intervention and the corrective feedback from active human involvement. PVP can be seamlessly integrated into existing value-based RL methods and achieves highly efficient reward-free policy learning, without offline pretraining and reward engineering. Human-in-the-loop experiments show the proposed method achieves superior performance and better user experience across diverse environments with different action spaces and human control devices, showing that the learning from active human involvement is a efficient policy learning method aligning human preference.

**Limitations.** (1) We only apply our method to two value-based RL methods. Advanced techniques such as exploration encouraging  and prioritized replay buffer  can be added to further improve the result. (2) Our method is not applicable to tasks where humans can not provide demonstrations. (3) We assume that human always demonstrates desired actions. We will show in Appendix D that suboptimal human behaviors will damage learning. In this case, we can define a sparse cost function in the training environment and utilize constrained optimization  to penalize bad demonstrations. (4) We assume that human subjects are available and attentive throughout the entire training. While our method is proven to be effective even under heavy traffic environments, we plan to further enhance its sample efficiency. We will achieve this goal by conducting offline RL training and policy evaluation in the background or passively involving human subjects whenever the model is uncertain about the environment.