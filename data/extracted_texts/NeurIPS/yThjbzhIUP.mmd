# PGDiff: Guiding Diffusion Models for Versatile

Face Restoration via Partial Guidance

Peiqing Yang\({}^{1}\)  Shangchen Zhou\({}^{1}\)  Qingyi Tao\({}^{2}\)  Chen Change Loy\({}^{1}\)

\({}^{1}\)S-Lab, Nanyang Technological University

\({}^{2}\)SenseTime Research, Singapore

https://github.com/pq-yang/PGDiff

###### Abstract

Exploiting pre-trained diffusion models for restoration has recently become a favored alternative to the traditional task-specific training approach. Previous works have achieved noteworthy success by limiting the solution space using explicit degradation models. However, these methods often fall short when faced with complex degradations as they generally cannot be precisely modeled. In this paper, we propose _PGDiff_ by introducing _partial guidance_, a fresh perspective that is more adaptable to real-world degradations compared to existing works. Rather than specifically defining the degradation process, our approach models the desired properties, such as image structure and color statistics of high-quality images, and applies this guidance during the reverse diffusion process. These properties are readily available and make no assumptions about the degradation process. When combined with a diffusion prior, this partial guidance can deliver appealing results across a range of restoration tasks. Additionally, _PGDiff_ can be extended to handle composite tasks by consolidating multiple high-quality image properties, achieved by integrating the guidance from respective tasks. Experimental results demonstrate that our method not only outperforms existing diffusion-prior-based approaches but also competes favorably with task-specific models.

## 1 Introduction

Recent years have seen diffusion models achieve outstanding results in synthesizing realistic details across various content . The rich generative prior inherent in these models opens up a vast array of possibilities for tasks like super-resolution, inpainting, and colorization. Consequently, there has been a growing interest in formulating efficient guidance strategies for pre-trained diffusion models, enabling their successful adaptation to various restoration tasks .

A common approach  is to constrain the solution space of intermediate outputs during the denoising process1. At each iteration, the intermediate output is modified such that its degraded counterpart is guided towards the input low-quality (LQ) image. Existing works achieve this goal either by using a closed-form solution  or back-propagating simple losses . These methods are versatile in the sense that the pre-trained diffusion model can be adapted to various tasks without fine-tuning, as long as the degradation process is known in advance.

While possessing great versatility, the aforementioned methods are inevitably limited in generalizability due to the need for prior knowledge of the degradation process. In particular, a closed-form solution generally does not exist except for special cases such as linear operators. In addition, back-propagating losses demand differentiability of the degradation process, which is violated for many degradations such as JPEG compression. Importantly, degradations in the wild often consistof a mixture of degradations , and hence, it is difficult, if not impossible, to model them accurately. As a result, existing works generally limit the scope to simplified cases, such as fixed-kernel downsampling. The generalization to real-world degradations remains a formidable challenge.

Motivated by the above, instead of modeling the degradation process, we propose to model the _desired properties_ of high-quality (HQ) images. The merit of such guidance is the agnosticity to the degradation process. However, it remains unclear what properties are desired and how appropriate guidance can be constructed. Through our extensive experiments, we find that with diffusion prior acting as a natural image regularization, one could simply guide the denoising process with easily accessible properties, such as image structure and color statistics. For example, as shown in Fig. 1, one could generate plausible outputs simply by providing guidance on the lightness and the statistics (_i.e._, mean and variance) of each color channel, without knowing the exact decolorization process. By constraining the HQ image space, our idea bypasses the difficulty of knowing the prior relation between LQ and HQ images, thus improving generalizability.

In this work, we devise a simple yet effective instantiation named _PGDiff_ by introducing _partial guidance_. PGDiff adopts classifier guidance  to constrain the denoising process. Each image property corresponds to a classifier, and the intermediate outputs are updated by back-propagating the gradient computed on the loss between the classifier output and the target property. Since our partial guidance is agnostic to the degradation process, it can be easily extended to complex tasks by compositing multiple properties. For instance, the task of old photo restoration can be regarded as a combination of restoration, inpainting, and colorization, and the resultant guidance is represented as a weighted sum of the guidance in the respective task. We also demonstrate that common losses such as perceptual loss [2; 16] and adversarial loss  can be incorporated for further performance gain.

**Contributions.** Our main contributions include **i)** a new concept of adapting diffusion models to restoration without presumptions of the degradation process. We show that it suffices to guide the denoising process with _easily accessible properties_ in the HQ image space, with diffusion prior acting as regularization, and **ii)**_partial guidance_, a versatile approach that is applicable to a broad range of image restoration and enhancement tasks. Furthermore, it allows flexible combinations of guidance for intricate tasks. We conduct extensive experiments to demonstrate the effectiveness of PGDiff on a variety of challenging tasks including blind face restoration and old photo restoration. We also demonstrate interesting applications, such as reference-based restoration. The results confirm the superiority of PGDiff over previous state-of-the-art methods.

## 2 Related Work

**Generative Prior for Restoration.** Generative prior has been widely adopted for a range of image restoration tasks, including super-resolution, inpainting, and colorization. One prominent approach in

Figure 1: **Overview of Our PGDiff Framework for Versatile Face Restoration**. Here, we take the colorization task as an example to illustrate our inference pipeline. One may refer to Table 1 for the corresponding details (_e.g._, property, classifier, and target) of other tasks. We show that our method can handle a wide range of tasks, including (a) blind face restoration, (b) face colorization, (c) face inpainting, and also composite tasks such as (d) old photo restoration.

this field is the use of pre-trained generative adversarial networks (GANs) [10; 18; 1]. For instance, GAN-inversion [26; 11; 28] inverts a corrupted image to a latent code, which is then used for generating a clean image. Another direction is to incorporate the prior into an encoder-decoder architecture [3; 4; 40; 44], bypassing the lengthy optimization during inference. VQVAE  is also commonly used as generative prior. Existing works [48; 13; 43; 47] generally first train a VQVAE with a reconstruction objective, followed by a fine-tuning stage to adapt to the subsequent restoration task. Recently, diffusion models have gained increasing attention due to their unprecedented performance in various generation tasks [33; 29; 7; 15; 34], and such attention has led to interest in leveraging them as a prior for restoration.

**Diffusion Prior.** There has been a growing interest in formulating efficient guidance strategies for pre-trained diffusion models, enabling their successful adaptation to various restoration tasks [9; 42; 20; 37; 39]. Among them, DDRM , DDNM , and GDP  adopt a zero-shot approach to adapt a pre-trained diffusion model for restoration without the need of task-specific training. At each iteration, the intermediate output is modified such that its degraded counterpart is guided towards the input low-quality image. This is achieved under an assumed degradation process, either in the form of a fixed linear matrix [42; 20] or a parameterized degradation model , with learnable parameters representing degradation extents. In this work, we also exploit the generative prior of a pre-trained diffusion model by formulating efficient guidance for it, but unlike existing works that limit the solution space using explicit degradations [9; 42; 20], we propose to model the desired properties of high-quality images. Such design is agnostic to the degradation process, circumventing the difficulty of modeling the degradation process.

## 3 Methodology

PGDiff is based on diffusion models. In this section, we first introduce the background related to our method in Sec. 3.1, and the details of our method are presented in Sec. 3.2.

### Preliminary

**Diffusion Models.** The diffusion model  is a class of generative models that learn to model a data distribution \(p(x)\). In particular, the forward process is a process that iteratively adds Gaussian noise to an input \(x_{0} p(x)\), and the reverse process progressively converts the data from the noise distribution back to the data distribution, often known as the denoising process.

For an unconditional diffusion model with \(T\) discrete steps, at each step \(t\), there exists a transition distribution \(q(x_{t}|x_{t-1})\) with variance schedule \(_{t}\):

\[q(x_{t}|x_{t-1})=(x_{t};}\,x_{t-1},_{t}).\] (1)

Under the reparameterization trick, \(x_{t}\) can be written as:

\[x_{t}=}\,x_{t-1}+}\,,\] (2)

where \(_{t}=1-_{t}\) and \((;,)\). Recursively, let \(_{t}=_{i=1}^{t}_{i}\), we have

\[x_{t}=_{t}}\,x_{0}+_{t}}\,.\] (3)

During sampling, the process starts with a pure Gaussian noise \(x_{T}(x_{T};,)\) and iteratively performs the denoising step. In practice, the ground-truth denoising step is approximated  by \(p_{}(x_{t-1}|x_{t})\) as:

\[p_{}(x_{t-1}|x_{t})=(_{}(x_{t},t),_{}(x_ {t},t)),\] (4)

where \(_{}(x_{t},t)\) is a constant depending on pre-defined \(_{t}\), and \(_{}(x_{t},t)\) is generally parameterized by a network \(_{}(x_{t},t)\):

\[_{}(x_{t},t)=}}(x_{t}-}{ {1-_{t}}}_{}(x_{t},t)).\] (5)

From Eq. (3), one can also directly approximate \(x_{0}\) from \(_{}\):

\[_{0}=_{t}}}x_{t}- _{t}}{_{t}}}_{}(x_{t},t).\] (6)

**Classifier Guidance.** Classifier guidance is used to guide an unconditional diffusion model so that conditional generation is achieved. Let \(y\) be the target and \(p_{}(y|x)\) be a classifier, the conditional distribution is approximated as a Gaussian similar to the unconditional counterpart, but with the mean shifted by \(_{}(x_{t},t)g\):

\[p_{,}(x_{t-1}|x_{t},y)(_{}(x_{t},t)+ _{}(x_{t},t)g,_{}(x_{t},t)),\] (7)

where \(g=_{x}\,p_{}(y|x)|_{x=_{}(x_{t},t)}\). The gradient \(g\) acts as a guidance that leads the unconditional sampling distribution towards the condition target \(y\).

### Partial Guidance

Our _partial guidance_ does not assume any prior knowledge of the degradation process. Instead, with diffusion prior acting as a regularization, we provide guidance only on the desired properties of high-quality images. The key to PGDiff is to construct proper guidance for each task. In this section, we will discuss the overall framework, and the formulation of the guidance for each task is presented in Sec. 4. The overview is summarized in Fig. 1 and Algorithm 1.

**Property and Classifier.** The first step of PGDiff is to determine the desired properties which the high-quality output possesses. As summarized in Table 1, each image property corresponds to a classifier \(p_{}(y|_{0})\), and the intermediate outputs \(x_{t}\) are updated by back-propagating the gradient computed on the loss between the classifier output and the target \(y\).

   & **Task** & **Property** & **Target: \(\)** & **Classifier: \((y|_{0})}\)** \\   & **Inpainting** & **Unmasked Region** & **Mask(\(y_{0}\))** & **Mask** \\   & **Colorization** & **Lightness** & **rgb2gray(\(y_{0}\))** & **rgb2gray** \\  & **Color Statistics** & **AdaI\(_{0}\)** & **Identity** \\   & **Restoration** & **Smooth Semantics** & **Clean(\(y_{0}\))** & **Identity** \\  & **Ref-Based Restoration** & **Smooth Semantics** & **Clean(\(y_{0}\))** & **Identity** \\  & **Identify Reference** & **ArcFace(\(y_{ref}\))** & **ArcFace ** \\    & **Task** & **Composition** & **Composition** \\  
**Composite Task** & **Old Photo Restoration (w/ scratches)** & **Restoration + Inpainting + Colorization** \\  

Table 1: **Examples of Partial Guidance. Each image property corresponds to a classifier, and each task involves one or multiple properties as guidance. The target value of each property is generally obtained either from the input image \(y_{0}\) or the denoised intermediate output \(_{0}\). For a composite task, we simply decompose it into multiple tasks and combine the respective guidance. Here, Clean denotes a pre-trained restorer detailed in Sec. 4.1, Identity refers to an identity mapping, and \(y_{ref}\) represents a reference image containing entity with the same identity as \(y_{0}\).**Given a specific property (_e.g._, lightness), we construct the corresponding classifier (_e.g._, rgb2gray), and apply classifier guidance during the reverse diffusion process as shown in Fig. 1. Although our PGDiff is conceptually similar to classifier guidance, we find that the conventional guidance scheme often leads to suboptimal performance. In this work, we borrow ideas from existing works  and adopt a _dynamic guidance scheme_, which introduces adjustments to the guidance weight and number of gradient steps for enhanced quality and controllability.

**Dynamic Guidance Scheme.** Our dynamic guidance scheme consists of two components. First, we observe that the conventional classifier guidance, which adopts a constant gradient scale \(s\), often fails in guiding the output towards the target value. This is especially unfavourable in tasks where high similarity to the target is desired, such as inpainting and colorization. To alleviate this problem, we calculate the gradient scale based on the magnitude change of the intermediate image :

\[s_{norm}=-x_{t-1}^{}\|_{2}}{\|g\|_{2 }} s,\] (8)

where \(x_{t-1}^{}(_{},_{})\). In this way, the dynamic guidance weight \(s_{norm}\) varies along iterations, more effectively guiding the output towards the target, thus improving the output quality.

Second, the conventional classifier guidance typically executes a single gradient step at each denoising step. However, a single gradient step may not sufficiently steer the output toward the intended target, particularly when the intermediate outputs are laden with noise in the early phases of the denoising process. To address this, we allow multiple gradient steps at each denoising step  to improve flexibility. Specifically, one can improve the guidance strength of a specific property by increasing the number of gradient steps. The process degenerates to the conventional classifier guidance when the number of gradient steps is set to \(1\). During inference, users have the flexibility to modulate the strength of guidance for each property as per their requirements, thus boosting overall controllability.

**Composite Guidance.** Our partial guidance controls only the properties of high-quality outputs, and therefore can be easily extended to complex degradations by stacking respective properties. This is achieved by compositing the classifiers and summing the loss corresponding to each property. An example of composite tasks is shown in Table 1. In addition, we also demonstrate that additional losses such as perceptual loss  and adversarial loss  can be incorporated for further quality improvement. Experiments demonstrate that our PGDiff achieves better performance than existing works in complex tasks, where accurate modeling of the degradation process is impossible.

Figure 2: **Comparison on Blind Face Restoration. Input faces are corrupted by real-world degradations. Our PGDiff produces high-quality faces with faithful details. (**Zoom in for best view.**)

## 4 Applications

By exploiting the diffusion prior, our PGDiff applies to a wide range of restoration tasks by selecting appropriate guidance. In this section, we will introduce the guidance formulation and provide experimental results.

### Blind Face Restoration

**Partial Guidance Formulation.** The objective of blind face restoration is to reconstruct a high-quality face image given a low-quality input corrupted by unknown degradations. In this task, the most straightforward approach is to train a network with the MSE loss using synthetic pairs. However, while these methods are able to remove the degradations in the input, it is well-known  that the MSE loss alone results in over-smoothed outputs. Therefore, extensive efforts have been devoted to improving the perceptual quality, such as incorporating addition losses (_e.g._, GAN loss) [22; 10; 16; 46; 8] and components (_e.g._, codebook [48; 13; 43; 47; 35] and dictionary [23; 24; 12; 8]). These approaches often require multi-stage training and experience training instability.

In our framework, we decompose a high-quality face image into _smooth semantics_ and _high-frequency details_, and provide guidance solely on the _smooth semantics_. In this way, the output \(_{0}\) in each diffusion step is guided towards a degradation-free solution space, and the diffusion prior is responsible for detail synthesis. Given an input low-quality image \(y_{0}\), we adopt a pre-trained face restoration model \(f\) to predict smooth semantics as partial guidance. Our approach alleviates the training pressure of the previous models by optimizing model \(f\) solely with the MSE loss. This is because our goal is to obtain _smooth semantics_ without hallucinating unnecessary high-frequency details. Nevertheless, one can also provide guidance of various forms by selecting different restorers, such as CodeFormer . The loss for classifier guidance is computed as: \(_{res}=||_{0}-f(y_{0})||_{2}^{2}\).

**Qualitative Results.** We evaluate the proposed PGDiff on three real-world datasets, namely LFW-Test , WebPhoto-Test , and WIDER-Test . We compare our method with both task-specific CNN/Transformer-based restoration models [48; 26; 40] and diffusion-prior-based models2[9; 42; 45]. As shown in Fig. 2, existing diffusion-prior-based methods such as GDP  and DDNM  are unable to generalize to real-world degradations, producing outputs with notable artifacts. In contrast, our PGDiff successfully removes the degradations and restores the facial details invisible in the input

    &  &  &  \\  & & GFP-GAN  & CodeFormer  & DiFace  & **Ours** \\   & FID\(\) & 72.45 & 74.10 & 67.98 & 71.62 \\  & NIQE\(\) & 3.90 & 4.52 & 5.47 & 4.15 \\   & FID\(\) & 91.43 & 86.19 & 90.58 & 86.18 \\  & NIQE\(\) & 4.13 & 4.65 & 4.48 & 4.34 \\   & FID\(\) & 40.93 & 40.26 & 38.54 & 39.17 \\  & NIQE\(\) & 3.77 & 4.12 & 4.44 & 3.93 \\   

Table 2: Quantitative comparison on the _real-world_**LFW-Test, WebPhoto-Test, and WIDER-Test datasets. Red and blue indicate the best and the second best performance, respectively.

    &  &  \\  & GFP-GAN  & CodeFormer  & DiFace  & **Ours (w/o ref)** & **Ours (w/ ref)** \\  FID\(\) & 186.88 & 129.17 & 123.18 & 119.98 & 121.25 \\ MUSIQ\(\) & 63.33 & 69.62 & 60.98 & 67.26 & 64.67 \\  LPIPS\(\) & 0.49 & 0.36 & 0.35 & 0.34 & 0.35 \\ IDS\(\) & 0.36 & 0.55 & 0.56 & 0.44 & 0.76 \\   

Table 3: Quantitative comparison on the _synthetic_**CelebRef-HQ** dataset. Red and blue indicate the best and the second best performance, respectively.

images. Moreover, our PGDiff performs favorably over task-specific methods even without extensive training on this task.

**Quantitative Results on Real-world Datasets.** To compare our performance with other methods quantitatively on real-world datasets, we adopt FID  and NIQE  as the evaluation metrics and test on three real-world datasets: LFW-Test , WebPhoto-Test , and WIDER-Test . LFW-Test consists of the first image from each person whose name starts with A in the LFW dataset , which are 431 images in total. WebPhoto-Test is a dataset comprising 407 images with medium degradations collected from the Internet. WIDER-Test contains 970 severely degraded images from the WIDER Face dataset . As shown in Table 2, our method achieves the best or second-best scores across all three datasets for both metrics. Although GFP-GAN achieves the best NIQE scores across datasets, notable artifacts can be observed, as shown in Fig. 2. Meanwhile, our method shows exceptional robustness and produces visually pleasing outputs without artifacts.

**Quantitative Results on Synthetic Dataset.** We present a quantitative evaluation on the synthetic CelebRef-HQ dataset  in Table 3. Considering the importance of identity-preserving in blind face restoration, we introduce reference-based restoration in Sec. 4.5 in addition to the general restoration in Sec. 4.1. Table 3 shows that our methods achieve best or second best scores across both no-reference (NR) metrics for image quality (_i.e._, FID and MUSIQ) and full-reference (FR) metrics for identity preservation (_i.e._, LPIPS and IDS). Since we employ heavy degradation settings when synthesizing CelebRef-HQ, it is noteworthy that identity features are largely distorted in severely corrupted input images. Thus, it is almost impossible to predict an identity-preserving face without any additional identity information. Nevertheless, with our reference-based restoration, we observe that a high-quality reference image of the same person helps generate personal characteristics that are highly similar to the ground truth. The large enhancement of identity preservation is also indicated in Table 3, where our reference-based method achieves the highest IDS, increasing by 0.32.

### Face Colorization

**Partial Guidance Formulation.** Motivated by color space decomposition (_e.g._, YCbCr, YUV), we decompose our guidance into _lightness_ and _color_, and provide respective guidance on the two aspects. For lightness, the input image acts as a natural target since it is a homogeneous-color image. Specifically, we guide the output lightness towards that of the input using the simple rgb2gray operation. Equivalently, the loss is formulated as follows: \(_{l}=||(_{0})-(y_{0})| |_{2}^{2}\). The lightness guidance can also be regarded as a dense structure guidance. This is essential in preserving image content.

With the lightness guidance constraining the structure of the output, we could guide the color synthesis process with a lenient constraint - color statistics (_i.e._, mean and variance of each color channel). In particular, we construct the target by applying AdaIN to \(_{0}\), using a pre-determined set of color statistics for each R, G, B channel. Then we push \(_{0}\) towards the color-normalized output: \(_{c}=||_{0}-((_{0},))||_{2}^{2}\), where \(\) refers to the set of color statistics and \(()\) denotes the stop-gradient operation . The overall loss is formulated as: \(_{color}=_{l}+_{c}\), where \(\) is a constant that controls the relative importance of the structure and color guidance. To construct a universal color tone, we compute the average color statistics from a selected subset of the CelebA-HQ dataset . We find that this simple strategy suffices to produce faithful results. Furthermore, our PGDiff can produce outputs with diverse color styles by computing the color statistics from different reference images.

**Experimental Results.** As shown in Fig. 3, GDP  and DDNM  lack the capability to produce vibrant colors. In contrast, our PGDiff produces colorized outputs simply by modeling the lightness and color statistics. Furthermore, our method is able to generate outputs with diverse color styles by calculating color statistics from various reference sets.

### Face Inpainting

**Partial Guidance Formulation.** Since diffusion models have demonstrated remarkable capability in synthesizing realistic content [33; 29; 7; 15; 34], we apply guidance only on the unmasked regions, and rely on the synthesizing power of diffusion models to generate details in the masked regions. Let \(B\) be a binary mask where \(0\) and \(1\) denote the masked and unmasked regions, respectively. We confine the solution by ensuring that the resulting image closely resembles the input image within the unmasked regions: \(_{inpaint}=\|B_{0}-B y_{0}\|_{2}^{2}\), where \(\) represents the pixel-wise multiplication.

**Experimental Results.** We conduct experiments on CelebRef-HQ . As depicted in Fig. 4, GPEN  and GDP  are unable to produce natural outputs, whereas CodeFormer  and DDNM  generate outputs with artifacts, such as color incoherence or visual flaws. In contrast, our PGDiff successfully generates outputs with pleasant details coherent to the unmasked regions.

### Old Photo Restoration

**Partial Guidance Formulation.** Quality degradations (_e.g._, blur, noise, downsampling, and JPEG compression), color homogeneity, and scratches are three commonly seen artifacts in old photos. Therefore, we cast this problem as a joint task of _restoration_, _colorization_, and _inpainting3_. Similar to face colorization that composites the loss for each property, we composite the respective loss in each task, and the overall loss is written as: \(_{old}=_{res}+_{color}_{color}+ _{inpaint}_{inpaint}\), where \(_{inpaint}\) and \(_{color}\) are constants controlling the relative importance of the different losses.

**Experimental Results.** We compare our PGDiff with BOPB , GFP-GAN  and DDNM . Among them, BOPB is a model specifically for old photo restoration, GFP-GAN (v1) is able to restore and colorize faces, and DDNM is a diffusion-prior-based method that also claims to restore old photos with scratches. As shown in Fig. 5, BOPB, GFP-GAN, and DDNM all fail to give natural color in such a composite task. While DDNM is able to complete scratches given a proper scratch map, it fails to give a high-quality face restoration result. On the contrary, PGDiff generates sharp colorized faces without scratches and artifacts.

Figure 4: **Comparison on Face Inpainting on Challenging Cases. Our PGDiff produces natural outputs with pleasant details coherent with the unmasked regions. Moreover, different random seeds give various contents of high quality.**

Figure 3: **Comparison on Face Colorization. Our PGDiff produces diverse colorized output with various color statistics given as guidance. The first column of our results is guided by the average color statistics of a subset of the CelebA-HQ dataset , and the guiding statistics for the remaining three columns are represented as an image in the top right corner.**

### Reference-Based Restoration

**Partial Guidance Formulation.** In reference-based restoration, a reference image from the same identity is given to improve the resemblance of personal details in the output image. Most existing works exploiting diffusion prior [9; 42; 20; 45] are not applicable to this task as there is no direct transformation between the reference and the target. In contrast, our partial guidance is extensible to more complex tasks simply by compositing multiple losses. In particular, our PGDiff can incorporate personal identity as a partial attribute of a facial image. By utilizing a reference image and incorporating the identity loss into the partial guidance, our framework can achieve improved personal details. We extract the identity features from the reference image using a pre-trained face recognition network, such as ArcFace . We then include the negative cosine similarity to the loss term \(_{res}\) in blind face restoration (Sec. 4.1): \(_{ref}=_{res}-(v_{_{0}},v_ {r})\), where \(\) controls the relative weight of the two losses. Here \(()\) represents the cosine similarity, and \(v_{_{0}}\) and \(v_{r}\) denote the ArcFace features of the predicted denoised image and the reference, respectively.

**Experimental Results.** We use the CelebRef-HQ dataset , which contains \(1,005\) entities and each person has \(3\) to \(21\) high-quality images. To build testing pairs, for each entity, we choose one image and apply heavy degradations as the input, and then we select another image from the same identity as the reference. In Fig. 6, we observe that without the identity loss term \((v_{_{0}},v_{r})\), some of the personal details such as facial wrinkles and eye color cannot be recovered from the distorted inputs. With the additional identity loss as guidance, such fine details can be restored. In addition, our PGDiff can be used to improve identity preservation of arbitrary face restorers. For instance, as shown in Fig. 7 (a), by using CodeFormer  as our restorer and incorporating the identity loss, the fine details that CodeFormer alone cannot restore can now be recovered. Quantitative results are included in Table 3 and discussed in Sec. 4.1.

### Quality Enhancement

**Partial Guidance Formulation.** Perceptual loss  and adversarial loss  are two common training losses used to improve quality. Motivated by this, we are interested in whether such losses can also be used as the guidance for additional quality gain. We demonstrate this possibility in the task of blind face restoration using the following loss: \(_{quality}=_{res}+_{per}||(_{0})-(y)||_{2}^{2}+_{GAN} D(_{0})\), where \(_{per}\) and \(_{GAN}\) are the relative weights. Here \(\) and \(D\) represent pre-trained VGG16  and the GAN discriminator , respectively.

**Experimental Results.** We demonstrate in Fig. 7 (b) that perceptual loss and adversarial loss can boost the blind restoration performance in terms of higher fidelity with photo-realism details.

Figure 5: **Comparison on Old Photo Restoration on Challenging Cases. For a severely damaged old photo, with one eye masked with scratch, while only DDNM  is able to complete the missing eye, its restoration quality is significantly low. In contrast, our PGDiff produces high-quality restored outputs with natural color and complete faces.**

Figure 6: **Reference-Based Face Restoration. Our PGDiff, using \(_{ref}\) with identity loss as guidance, produces personal characteristics that are hard to recover without reference, _i.e._, using \(_{res}\) only. (Zoom in for details)**

## 5 Ablation Studies

In this section, we perform ablation studies on the dynamic guidance scheme mentioned in Sec. 3.2 to verify its effectiveness over the conventional classifier guidance scheme.

**Effectiveness of Dynamic Guidance Weight.** We first investigate the effectiveness of dynamic guidance weight \(s_{norm}\) in the face inpainting task, where the unmasked regions of the output image should be of high similarity to that of the input. As shown in Fig. 8 (a), without the dynamic guidance weight, although plausible content can still be generated in the masked area, the similarity and sharpness of the unmasked regions are remarkably decreased compared with the input. With \(s_{norm}\) replacing the constant \(s\), the output is of high quality with unmasked regions nicely preserved. The results indicate that our dynamic guidance weight is the key to ensuring high similarity to the target during the guidance process.

**Effectiveness of Multiple Gradient Steps.** To verify the effectiveness of multiple gradient steps, we compare the blind restoration results with the number of guidance steps \(N\) set to be \(1\), \(2\), and \(3\). While \(N=1\) is just the conventional classifier guidance, we set \(N=2\) during the first \(0.5T\) steps and set \(N=3\) during the first \(0.3T\) steps. As shown in Fig. 8 (b), artifacts are removed and finer details are generated as \(N\) increases. These results suggest that multiple gradient steps serve to improve the strength of guiding the output toward the intended target, particularly when the intermediate outputs are laden with noise in the early phases of the denoising process.

## 6 Conclusion

The generalizability of existing diffusion-prior-based restoration approaches is limited by their reliance on prior knowledge of the degradation process. This study aims to offer a solution that alleviates this constraint, thereby broadening its applicability to the real-world degradations. We find that through directly modeling high-quality image properties, one can reconstruct faithful outputs without knowing the exact degradation process. We exploit the synthesizing power of diffusion models and provide guidance only on properties that are easily accessible. Our proposed _PGDiff_ with _partial guidance_ is not only effective but is also extensible to composite tasks through aggregating multiple properties. Experiments demonstrate that PGDiff outperforms diffusion-prior-based approaches in both homogeneous and composite tasks and matches the performance of task-specific methods.

**Acknowledgement.** This study is supported under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).

Figure 8: **Ablation Study of Dynamic Guidance.** The comparison results on the dynamic guidance scheme verify its effectiveness over the conventional classifier guidance scheme.

Figure 7: (a) Using CodeFormer as the restorer with our identity guidance improves the reconstruction of fine details similar to the ground truth. (b) The comparison results show that the quality enhancement loss is able to enhance fidelity with photo-realism details.