# Successor-Predecessor Intrinsic Exploration

Changmin Yu\({}^{1,2}\) Neil Burgess\({}^{1,*}\) Maneesh Sahani\({}^{2,*}\) Samuel J. Gershman\({}^{3,*}\)

\({}^{1}\)Institute of Cognitive Neuroscience; \({}^{2}\)Gatsby Computational Neuroscience Unit;

UCL, London, United Kingdom

\({}^{3}\)Department of Psychology, Harvard University, Cambridge, United States

\({}^{*}\) Joint senior authors

###### Abstract

Exploration is essential in reinforcement learning, particularly in environments where external rewards are sparse. Here we focus on exploration with _intrinsic rewards_, where the agent transiently augments the external rewards with self-generated intrinsic rewards. Although the study of intrinsic rewards has a long history, existing methods focus on composing the intrinsic reward based on measures of future prospects of states, ignoring the information contained in the retrospective structure of transition sequences. Here we argue that the agent can utilise retrospective information to generate explorative behaviour with structure-awareness, facilitating efficient exploration based on global instead of local information. We propose _Successor-Predecessor Intrinsic Exploration_ (SPIE), an exploration algorithm based on a novel intrinsic reward combining prospective and retrospective information. We show that SPIE yields more efficient and ethologically plausible exploratory behaviour in environments with sparse rewards and bottleneck states than competing methods. We also implement SPIE in deep reinforcement learning agents, and show that the resulting agent achieves stronger empirical performance than existing methods on sparse-reward Atari games.

## 1 Introduction

The study of exploration in reinforcement learning (RL) has produced a broad range of methods [1; 2], ranging from simple methods such as pure randomization [3; 1; 4], to more sophisticated methods such as targeted exploration towards states with high uncertainty [5; 6; 7] and implicit exploration with entropy maximization [8; 9]. Intrinsic exploration, a highly effective class of methods, uses intrinsic rewards based on the agent's current knowledge of the environment, hence informing targeted exploration towards states with high predictive uncertainty or state occupancy diversity [10; 11; 6; 12]. However, existing approaches define the intrinsic reward based solely on prospective or empirical marginal information about future states, ignoring retrospective information (e.g., does a given state always precedes the goal state, hence should be more frequently traversed?). We argue that the retrospective information contains useful signals about the connectivity structure of the environment, hence could facilitate more efficient targeted exploration. For example, consider a clustered environment with bottleneck states connecting the clusters (Figure 0(a)), exploration based on local information (e.g., visitation counts) would discourage the agent from traversing bottleneck states, despite the key roles these states play in connecting different clusters. Guiding the agents to visit such "bottleneck" states in the face of minimal local information gain is essential in driving efficient and biologically plausible exploration. Here we study the contribution of retrospective information for global exploration with intrinsic motivation.

One of the most successful recent intrinsic exploration algorithms  uses the successor representation (SR; [13; 14]) to generate intrinsic rewards. The SR represents each state in terms of successor states. The row norms of the SR can be used as an intrinsic reward that generalises count-basedexploration . As we discuss in Section 3, the SR contains not only prospective information, but also retrospective information about expected predecessors. This information can be utilised to construct a novel intrinsic reward which overcomes some of the problems associated with purely prospective intrinsic rewards, such as untargeted exploration, the augmented reward function is non-stationary, and asymptotic uniformity.

We provide a brief overview of background and relevant literature in Section 2, and formally introduce the novel intrinsic exploration method, _Successor-Predecessor Intrinsic Exploration_ (SPIE), in Section 3. We propose two instantiations of SPIE for discrete and continuous state spaces, with comprehensive empirical examinations of properties of SPIE in discrete state space. We show that SPIE facilitates more efficient exploration, in terms of improved sample efficiency of learning and higher asymptotic return, through empirical evaluations on both discrete and continuous environments in Section 4.

## 2 Background and related work

**Reinforcement Learning Preliminaries**. We consider the standard RL problem in Markov Decision Processes (MDP), defined by the tuple, \(,,,_{0},,\), where \(\) is the state space, \(\) is the action space, \(:()\) is the state transition distribution (where \(()\) is the probability simplex over \(\)), \(_{0}()\) is the initial state distribution, \(:\) is the reward function, and \((0,1)\) is the discount factor. The goal for an RL agent is to learn the optimal policy that maximises value (expected cumulative discounted reward): \(^{*}(a|s)=*{argmax}_{}q^{}(s,a),(s,a) \), where \(:()\) is the policy, and \(q^{}(s,a)\) is the state-action value function:

\[q^{}(s,a)=_{^{}}[_{=0}^{} ^{}(s_{},a_{})|s_{0}=s,a_{0}=a]=_{ ^{}}[(s,a)+ q^{}(s^{},a^{})]\,,\] (1)

where \(^{}(s^{}|s)=_{a}(a|s)(s^{}|s,a)\) is the marginal state transition distribution given \(\)1. The second equality is the recursive form of the action value function known as the _Bellman equation_, which underlies temporal difference learning :

\[^{}(s_{t},a_{t})^{}(s_{t},a_{t})+_{ t},_{t}=r_{t}+^{}(s_{t+1},a_{t+1})-^{}(s_{t},a _{t})\,,\] (2)

where \(^{}(s_{t},a_{t})\) is the current estimate of the action values (with respect to \(\)), \(_{t}\) is the (one-step) temporal difference (TD) error. We will study the effect of different intrinsic rewards on the performance of online TD learning (SARSA) in discrete state MDPs.

**The successor representation**. The SR is defined as the expected cumulative discounted future state occupancy under the policy2:

\[[s,s^{}]=_{^{}}[_{=0}^{ }^{}(s_{},s^{})|s_{0}=s]=_{^{}}[(s_{0},s^{})+(s_{1 },s^{})|s_{0}=s].\] (3)

Given the recursive formulation, it is possible to learn the SR matrix online with TD learning. Given the transition tuple, \((s_{t},a_{t},r_{t},s_{t+1},a_{t+1})\), the update is

\[}[s_{t},s^{}]}[s_{t},s^{} ]+_{t}^{},_{t}^{}=(s_{t}, s^{})+}[s_{t+1},s^{}]-}[s_{t},s^{ }]\,,\] (4)

Note that these equations are analogous to TD learning for value function estimation, except that in this case the function being learned is a vector-valued (one-hot) representation of future states.

**First-occupancy representation**. The SR captures the expected cumulative discounted state occupancy over all future steps. However, in many real-world and simulated tasks, it may be preferable to reach the goal state as quickly as possible instead of as frequently as possible. In this spirit, Moskovitz et al.  introduced the _First-occupancy Representation_ (FR). Formally, the FR matrix in a discrete MDP is defined by

\[[s,s^{}] =_{^{}}[_{=0}^{}^ {}(s_{}=s^{},s^{}\{s_{0:}\})|s_{0}=s]\] (5) \[=_{^{}}[(s_{t},s^{} )+(1-(s_{t},s^{}))[s_{t+1},s^{}]|s_{t}=s ]\,,\]where \(\{s_{0:}\}=\{s_{0},s_{1},,s_{-1}\}\). The recursive formulation implies that there is an efficient TD learning rule for online learning of the FR matrix. Given the transition tuple \((s_{t},a_{t},r_{t},s_{t+1},a_{t+1})\), the update rule is

\[}[s_{t},s^{}]}[s_{t},s^{}]+ _{t}^{},_{t}^{}= (s_{t},s^{})+(1-(s_{t},s^{}))}[s_{t+1},s^{}]-}[s_{t},s^{}]\,,\] (6)

**Intrinsic exploration in RL.** Here we focus on exploration with intrinsic motivation, where the agent augments the external rewards with self-constructed intrinsic rewards based on its current knowledge of the environment.

\[r^{}(s,a)=r^{}(s,a)+ r^{}(s,a)\,,\] (7)

where \(r^{}(s,a)\) denotes the extrinsic environmental reward, \(r^{}(s,a)\) denotes the (possibly non-stationary) intrinsic reward, and \(\) is a multiplicative scaling factor controlling the relative balance of \(r^{}(s,a)\) and \(r^{}(s,a)\). The intrinsic reward often operates by motivating the agent to move into under-explored parts of the state space in the absence of extrinsic reinforcement. Many types of intrinsic rewards have been proposed, including functions of state visitation counts [17; 18; 19], predictive uncertainty of value estimation , and predictive error of forward models [10; 6; 20]. In a closely related work, Zhang et al.  proposes NovelD, which constructs the episode-specific non-negative intrinsic reward based on the difference between the novelty measures of temporally adjacent states along a trajectory. However in contrast to SPIE (discussed later), the key difference is that NovelD does not explicitly utilise the retrospective information for exploration and the associated intrinsic reward is episode-dependent.

## 3 Successor-Predecessor Intrinsic Exploration

Existing intrinsic exploration methods construct intrinsic rewards based on either the predictive information in a temporally forward fashion (e.g., predictive error), or the empirical marginal distribution (e.g., count-based exploration). Here we argue that the retrospective information inherent in experienced trajectories, though having been largely overlooked in the literature, could also be utilised as a useful exploratory signal. Specifically, consider the environment in Figure 0(a) (_Cluster-simple_), where the discrete grid world is separated into two clusters connected by a "bottleneck" state. Whenever the starting and reward locations are in different clusters, the bottleneck state, \(s_{*}\), always precedes the goal state, regardless of the trajectory taken. Hence, the frequent predecessor state (e.g., \(s_{*}\)), to the goal state should be traversed despite the fact that immediate information gain by traversing the state is minimal. In the absence of extrinsic reward, if only utilising learned prospective information based on past experience (e.g., the norm of the online-learned SR ), the intrinsic motivation for exploration is merely local hence would discourage transitions into bottleneck states. However, the retrospective information can be utilised to identify the state transitions that connect different sub-regions of the state space, hence incorporating the connectivity information of the state space into guiding exploration, allowing the agent to escape local exploration and navigate towards bottleneck states to reach distant regions.

We develop _Successor-Predecessor Intrinsic Exploration_ (SPIE) algorithm utilising intrinsic rewards based on both prospective and retrospective information from past trajectories. Below we provide instantiations of SPIE based on the SR for discrete and continuous state spaces.

**SPIE in discrete state space.** We define the _SR-Relative_ (SR-R) intrinsic reward, which is defined as the SR of the future state from the current state minus the sum of the SRs of the future state from all states. Formally, given a transition tuple, \((s,a,r,s^{},a^{})\), we define the SR-R intrinsic reward as:

\[r_{}(s,a)=}[s,s^{}]-||}[:,s^{ }]||_{1}=-_{, s}}[ ,s^{}]\,,\] (8)

The above equation holds in deterministic MDPs (i.e., when \(s^{}\) is a function of \((s,a)\)). We note that the \(j\)-th column of the SR matrix represents the expected discounted occupancy to state \(j\), starting from every state, hence constituting a _temporally backward_ measure of the accessibility of state \(j\). Therefore, \(r_{}(s,a)\) consists of both a prospective measure (\(}[s,s^{}]\)) and a retrospective measure (\(||}[:,s^{}]||_{1}\)), and exploring with \(r_{}\) is an instantiation of SPIE in discrete MDPs. Intuitively, \(r_{}(s)\) can be interpreted as penalising transitions leading into states \(s^{}\) that are frequently reached from many states other than \(s\), hence providing an intrinsic motivation for guiding the agent towards states that are harder to reach in general, e.g, boundary states and bottleneck states. We thoroughly investigate the individual contribution of prospective and retrospective information through ablation studies in Appendix B.4, and we observe that prospective information alone does not yield optimal exploration performance, whereas utilising only the retrospective information does not degrade exploration efficiency, indicating the importance of global topological information contained in the retrospective information for intrinsic exploration.

In a closely related work, Machado et al.  showed that \(r_{}(s)=1/||}}[s,:]||_{1}\) can be used as an intrinsic reward that facilitates exploration in sparse reward settings. They additionally showed that the row norm of the online-learned SR matrix implicitly approximates the state visitation counts, so the resulting behaviour resembles count-based exploration. However, a key issue associated with \(r_{}\) is that the asymptotic exploratory behaviour is uniformly random across all states, i.e., \(||}}[s,:]||_{1} 1/(1-), s\). We note that exploration involves learning of both the environmental transition structure \(^{}\) and the reward structure \(\). Hence, were the SR matrix to be known _a priori_ (hence \(^{}\) could be implicitly derived), no intrinsic motivation would be introduced at any state and the resulting agent regresses back to random exploration, omitting further efficient exploration for learning \(\). Since \(r_{}\) contains the sum of columns of the SR matrix, the asymptotic uniformity in \(r_{}\) no longer holds, yielding non-trivial intrinsic exploration even when the SR matrix is known and fixed _a priori_, allowing continual exploration for learning the reward structure despite sparse extrinsic reinforcement.

**Analysis of \(r_{}\) with pure exploration in grid-worlds**. We examine exploration based on \(r_{}(s)\) in discrete grid-worlds with different topologies (Fig. 1a). We first consider pure exploration in the absence of extrinsic reward, and evaluate the exploratory behaviours of \(4\) RL agents with different intrinsic rewards, in terms of their state coverage. The agents we consider are: vanilla SARSA ; SARSA with \(r_{}\) (SARSA-SR; ); SARSA with \(r_{}(s)=||F|s,:]||_{1}\) (SARSA-FR; ); and SARSA with \(r_{}\) (SARSA-SRR); the pseudocode for SARSA-SRR can be found in Appendix). We consider \(4\) different grid-world environments with different configurations (Figure 1a), namely, \(10 10\) open-field grid (_OF-small_); \(10 10\) grid with two rooms (_Cluster-simple_); \(10 10\) grid with \(4\) rooms (_Cluster-hard_); and \(20 20\) open-field grid (_OF-large_).

Exploration efficiency is quantified as the number of timesteps taken to cover \(50\%\), \(90\%\) and \(99\%\) of the state space. The value estimates for all states are initialised to be \(0\). Due to the absence of extrinsic reward, the vanilla SARSA agent is equivalent to a random walk policy, which acts as a natural baseline. We observe from Figure 1b that SARSA-SRR yields the fastest coverage of the state space amongst all considered agents. The SARSA-FR agent yields similar state coverage efficiency as SARSA. SARSA-SR performs poorly in all 4 grid-worlds, failing to achieve \(50\%\) state coverage within \(8000\) timesteps in all environments other than the simplest one (_OF-small_). Moreover, we observe that SARSA-SRR performed consistently across the \(4\) considered grid configurations, whereas all other agents experienced significant degradation in exploration efficiency as the size and complexity of the environments increase.

We note that in addition to improved exploration efficiency, SARSA-SRR exhibits "cycling" behaviour in pure exploration in the \(20 20\) two-cluster environment (Figure 6e), spending the majority of its time exploring in one cluster and periodically traverses the "bottleneck" states to explore the opposing clusters upon sufficient coverage of the current cluster. Such "cycling" strategy exhibits short-term memory of recent states and consistent long-term planned exploration towards regions more distant in history. This is potentially advantageous for environments with non-stationary reward structures (), such as real-world foraging, which require continual exploration for identifying new rewards. We verify the capability of SARSA-SRR for dealing with non-stationary reward structure in Section 4 (Figure 3).

The complexity of analysing the properties of SARSA-SRR is two-fold: the online learning of the SR matrix and the online update of the Q-values. By assuming the SR matrix is known and fixed throughout training,3 we observe from Figure 1c that SARSA-SRR consistently outperforms all competing methods, similar to what we observed when the SR (FR) matrix is learned online. Additionally, we observe that the exploration efficiency for all three intrinsic exploration agents drops when using the intrinsic reward constructed with the fixed SR (FR), but SARSA-SRR yields minimal decrease comparing to the significant degradation with SARSA-SR and SARSA-FR. Hence, we have empirically confirmed that the improved exploration efficiency does not stem solely from the online learning of the SR matrix, but is a property of \(r_{}\). Another long-standing issue with many existingintrinsic exploration methods is the non-stationary nature of the associated intrinsic bonus. By fixing the SR (FR) matrix, the associated \(r_{}\) is stationary whilst still yielding high exploration efficiency, hence validating the utility of SPIE.

**SPIE in continuous state space with deep RL.** In order to generalise \(r_{}\) to continuous state space, we replace the SR with successor features (SF; ).

\[^{}(s,a)=[_{k=0}^{}^{k} _{t+k}|s_{t}=s,a_{t}=a]=(s_{t+1})+ [^{}(s_{t+1},(s_{t+1}))|s_{t}=s,a_{t}=a]\] (9)

where \((s,a)\) is a feature representation such that \(r(s,a)=(s,a)\), with weight parameter **w**. The recursive formulation for SF admits gradient-based learning of \(\) by minimising the following squared TD loss.

\[_{t}^{}=[((s_{t},a_{t})+(s_ {t+1},a_{t+1})-(s_{t},a_{t}))^{2}]\,,\] (10)

where the transition tuple \((s_{t},a_{t},s_{t+1},a_{t+1})\) can be taken from either online samples (SARSA-like) or sampled from offline trajectories (Q-learning-like). We previously noted that the column of the SR matrix provides a marginal retrospective accessibility of states, facilitating stronger exploration. However, there is no SF-analogue of the column of the SR matrix. We therefore construct the retrospective exploration objective with the _Predecessor Representation_ (PR), which was proposed to measure how often a given state is preceded by any other state given the expected cumulative discounted preceding occupancy . The formal definition for the PR matrix under discrete MDP,

Figure 1: **Evaluation of exploration efficiency in grid worlds.** (a) Grid worlds with varying size and complexity. ‘S’ and ‘G’ in _OF-small_ and _Cluster-hard_ represents the start and goal states in the goal-oriented reinforcement learning task; colored \(G_{1}\) and \(G_{2}\) in _OF-small_ and _Cluster-hard_ represent the changed goal locations (see the non-stationary reward experiment in Section 4), \(s_{*}\) in _Cluster-simple_ denote the bottleneck state. (b-c) Accumulated number of states visited against exploration timesteps, for all considered agents in all grid-worlds in with (a) online-learned SR matrix (b) and fixed SR matrix (c). All reported results are averaged over \(10\) random seeds (shaded area denotes mean \(\) 1 standard error). Hyperparameters can be found in Appendix.

\(^{||||}\), is defined as following.

\[[s,s^{}]=_{}^{}}[_{=0 }^{n}^{}(s,s_{n-})|s_{n}=s^{}]=_ {}^{}}[(s,s_{n})+[s,s_{n- 1}]],\] (11)

where the expectation is based on \(}^{}(s_{t}=s|s_{t+1}=s^{})=^{} (s,s^{})z(s)}{z(s^{})}\), the _retrospective_ transition model, and \(z(s)=_{t}_{^{}}[(s_{t }=s)]\), denotes the stationary distribution given policy \(\).

Utilising the recursive formulation for the PR matrix, we can again derive a TD-learning rule. Namely, given the transition tuple, \((s_{t},a_{t},r_{t},s_{t+1},a_{t+1})\), we have the following update rule.

\[}^{}[,s_{t+1}]=}[,s_{t+1}]+ _{t}^{},_{t}^{}=(s_{t+1 },)+}[,s_{t}]-}[,s_{t +1}].\] (12)

The SR and PR have a reciprocal relationship (proof in appendix):

\[()=()\,,\] (13)

where \(()^{||||}\) denotes the diagonal matrix whose diagonal elements corresponds to the discrete stationary distribution of the MDP under the current policy.

Similar to how SF generalises SR, we propose the "Predecessor Feature" (PF) that generalises PR.

\[^{}(s)=[_{k=0}^{}^{k}_{t -k}|s_{t+1}=s]=(s_{t+1})+[ ^{}(s_{t})|s_{t+1}=s,a_{t}=a].\] (14)

Similarly to the SF, the recursive definition of the PF again allows a simple expression of the TD error for gradient-based learning of the PF.

\[_{t}^{}=[((s_{t+1})+(s_{t})-(s_ {t+1}))]\,,\] (15)

We utilise the norms of SF and PF to replace the row sums in discrete settings for tractable approximation to \(r_{}\) in continuous state spaces. We use the same feature vector, \(\), for computing the SF and PF. In order to ensure the SF and PF are of similar scales across the state space, we normalise \((s)\) such that \(||(s)||_{2}=1\) for all \(s\). Contrary to how we define \(r_{}\) as the difference between the SR and the column sum of the SR in discrete MDPs4, we find that setting the intrinsic reward as the difference between the reciprocal of the norms of the SF and the PF yields better empirical performance. We hence define the continuous Successor-Predecessor intrinsic reward as follows.

\[r_{}=(s_{t+1})||_{1}}-(s_{t},a_{t})||_{1}}\] (16)

**Details of deep RL implementation of \(r_{}\)**. We instantiate \(r_{}\) based on a Deep Q Network (DQN; ), with auxiliary predictive reconstruction task (\(_{}=[(s_{t+1}-_{t+1})^ {2}|s_{t}]\), where \(_{t+1}\) is the predicted next state), and separate heads for computing the q-values, the SF, and the PF, respectively (Figure 2). We call this model DQN-SF-PF. Note that, following Machado et al. , the intermediate feature representation \(\) is trained given only the predictive reconstruction and value learning supervisions, and not updated given the TD error in the learning of the SF or the PF (the filled black circle in Figure 2 indicating the stop_gradient operation). We adopt the same set of hyperparameters and architecture for the DQN as reported in Oh et al. . To make the comparison consistent, we utilise the

Figure 2: Graphical illustration of the neural network architecture of DQN-SF-PF for Atari games. Note that the state feature vector is L2-normalised, \((s)=(s)}{||(s)||_{2}}\), where \((s)\) is the raw output of the convolutional encoder.

mixed Monte-Carlo return loss [28; 12], defined as following.

\[_{q}&=[((1- )_{}(s,a)+_{}(s,a))^{2}]\,,\\ _{}(s,a)&=_{t=0}^{ }^{t}(r(s_{t},a_{t})+ r_{}(s_{t},a_{t}; ^{-}))-q(s,a;)\,,\] (17)

where \(_{}\) denotes the standard TD error for q-values (Eq. 2), \(\) is the scaling factor controlling the contribution of the TD error and the Monte Carlo error, and \(\) and \(^{-}\) denote the parameters for the online and target DQN-SF-PF, respectively. Hence the overall loss objective for training DQN-SF-PF is as following.

\[_{}=w_{q}_{q}+w_{}^{ }+w_{}^{}+w_{}_{ }\,,\] (18)

where \(w_{q/}\) denotes the scaling factors for the respective loss terms. The complete set of hyperparameters for DQN-SF-PF can be found in the Appendix.

## 4 Experiments

**Classical hard exploration tasks**. We evaluate performance of the discrete SPIE agent (and other considered agents in Section 3: SARSA, SARSA-SR, SARSA-FR) on two classical hard exploration tasks commonly studied in the PAC-MDP literature, _RiverSwim_ and _SixArms_ (appendix Figure 5). In both tasks, environment transition dynamics induce a bias towards states with low rewards, leaving high rewards in states that are harder to reach. Evaluation of the agents is based on the cumulative reward collected within \(5000\) training steps.

We observe from Table 1 that SARSA-SRR significantly outperforms all other considered agents. Moreover, in order to further justify the utility of \(_{}\) in driving exploration, we run ablation studies by evaluating the performance of variants of SARSA-SRR (Appendix B.4). Ablation studies reveal the importance of combining both prospective and retrospective information for exploration, as well as the benefits of dynamic balancing exploring uncertain states and bottleneck states.

In order to validate the replacement of column norm of the SR with column norm of the PR in the construction of \(r_{}\), given the reciprocal relationship (Eq. 13), we empirically evaluate the performance of the SARSA agent with the alternative intrinsic reward, \(r_{}(s,a)=}[s,s^{}]-||}[:,s^{ }]||_{1}\). SARSA-SR-PR yields comparable performance as SARSA-SRR on both RiverSwim and SixArms (Table 1), empirically justifying the instantiation of SPIE with the PR for capturing the retrospective information.

**Goal-oriented / sparse-reward tasks**. We next evaluate the agents on grid world tasks with a single terminal goal state (Figure 0(a); _OF-small_ and _Cluster-hard_). All non-terminal transitions yield rewards of \(-1\), and transitions into the goal state generates a reward of \(0\). Such goal-directed or sparse-reward tasks require efficient exploration. We examine both open-field and clustered grid-worlds. In _OF-small_ and _Cluster-hard_ tasks, SARSA-SRR outperforms both vanilla SARSA and SARSA-SR in terms of sample efficiency (Figure 3). In addition, SARSA-SRR yields more stable training and performance is more robust across different random seeds. Note that the navigation performance of SARSA-SR during training is highly unstable, which might attribute to its equivalence to count-based exploration given that visitation count is only a local measure for exploration. Somewhat surprisingly, the improvement for SARSA-SRR is more significant in open-field grid world (_OF-small_) rather than

   & SARSA & SARSA-SR & SARSA-FR & SARSA-SRR & SARSA-SR-PR \\   RiverSwim & 25,075 & 1,197,075 & 1,547,243 & \(,,\) & \(,,\) \\  & (1,224) & (36,999) & (34,050) & (479,655) & (419,922) \\ SixArms & 376,655 & 1,025,750 & 119,149 & \(,,\) & \(,,\) \\  & (8,449) & (49,095) & (42,942) & (1,024,726) & (1,032,050) \\  

Table 1: Evaluations SARSA-SRR and related baseline agents on RiverSwim and SixArms (averaged over 100 seeds, numbers in the parentheses represents standard errors).

the clustered grid world (_Cluster-hard_), in contrast to the pure exploration experiments (Figure 0(b)). Nevertheless, the improvement is strong and consistent.

In many real-world tasks, the environment is inherently dynamic, requiring continual exploration for adapting the optimal policy with respect to the non-stationary task structure. One such example is random foraging, where foods are depleted upon consumption, and new rewards appear in new locations. As argued in Section 3, SARSA-SRR yields "cycling" exploratory behaviour (Figure 6), hence could facilitate continual exploration that is potentially suitable for such non-stationary environments. To empirically justify the hypothesis, we consider the Non-Markovian Reward Decision Process (NMRDP; ), where the reward changes dynamically given the visited state sequence. We instantiate the NMRDPs in the grid worlds, _OF-small_ and _Cluster-hard_, where there are three reward states (\(G\), \(G_{1}\), \(G_{2}\); Figure 0(a)) that are sequentially activated (and deactivated) every \(30\) episodes. As shown in Figure 2(c) and 2(d), we observe that SARSA-SRR consistently outperforms SARSA and SARSA-SR, reaching the new goal states in increasingly shorter timescales. This supports our idea that SPIE provides a more ethologically plausible exploration strategy for dealing with non-stationarity. However, we note that the main focus of the current paper is on improved exploration within a single task, instead of over a stream of inter-related tasks. Here we provide preliminary evidence of potential applicability of SPIE in such continual exploration setting, and we leave more rigorous investigation in this direction for future work.

**Linear function approximation for continuous state spaces**. We next evaluate SPIE with function approximation. As a first step, we consider the linear features before moving onto the deep RL setting. We consider the _MountainCar_ task (Figure 3(a); ), with sparse reward structure, where we set the reward to \(0\) for all transitions into non-terminal states (the terminal state is indicated by the flag on the top of the right hill). We utilise Q-learning with linear function approximation, where we define the linear features to be the \(128\)-dimensional random Fourier features (RFF ; Figure 3(b)). The SF and the PF are defined given the RFF, and are learned via standard TD-learning (Eq. 10; 15). The performance (over the first \(1000\) training episodes) of the resulting linear-Q agents with \(r_{}\) and

Figure 3: **Goal-oriented navigation in grid worlds. Evaluations of SARSA, SARSA-SR and SARSA-SRR on _OF-small_ (a) and _Cluster-hard_ (b) grid worlds (Figure 0(a)) with stationary reward structure, and on _OF-small_ (c) and _Cluster-hard_ (d) with _non-stationary_ reward structures. The red dashed horizontal line represents the shorted path distance. The black dashed vertical lines represent the time point at which the goal change occurs.**

\(r_{}\) is shown in Figure 3(c). The agent with \(r_{}\) outperforms the opposing agent significantly, empirically justifying the utility of SPIE in the linear function approximation regime.

**Deep RL instantiation of SPIE in Atari games**. We empirically evaluate \(_{}\) on \(6\) Atari games with sparse reward structures : Freeway, Gravitar, Montezuma's Revenge, Private Eye, Solaris, and Venture. We follow the evaluation protocol as stated in Machado et al. , where we report the averaged evaluation scores over \(10\) random seeds given \(10^{8}\) training steps. The agent takes (stacked) raw pixel observations as inputs. Across all \(4\) games, the \(\) values are set to \(0.07\) and the discounting factor \(=0.995\). We adopt the epsilon-annealing scheme as in , which linearly decreases \(\) from \(1.0\) to \(0.1\) over the first \(10^{6}\) frames. We train the network with RMSprop, with standard hyperparameters, learning rate \(0.00025\), \(=0.001\) and decay equals \(0.95\). The discounting factors for value learning and online learning of the SF and the PF are set to \(0.99\). The scaling factors in Eq. 18 are set such that the different losses are on roughly similar scales: \(w_{q}=1\), \(w_{}=1500\), \(w_{}=1500\), \(w_{}=0.001\). More implementation details can be found in Appendix.

We compare DQN-SF-PF with vanilla DQN trained with standard TD error, vanilla DQN trained with the MMC loss (\(_{q}\)), Random Network Distillation (RND; ), DQN-SR trained with the MMC loss  (Table 2). All agents are trained with the predictive reconstruction auxiliary task. By comparing with our main baseline, DQN-SR, we observe that DQN-SF-PF significantly outperforms DQN-SR on Four games (Gravitar, Montezuma's Revenge, Private Eye and Solaris), whilst yielding similar performance on the remaining two games (Freeway and Venture). Moreover, DQN-SF-PF outperforms RND, a state-of-the-art Deep RL algorithm for exploration, on all \(6\) games. The empirical difference is not only reflected in the asymptotic performance, but also in the sample efficiency of learning. Specifically, for Montezuma's Revenge, one of the hardest exploration games in the Atari suite, our agent achieves near asymptotic performance (defined as the score given \(10^{8}\) training steps) with only \( 8 10^{6}\) training frames, whereas the performance of DQN-SR saturates at \( 2.4 10^{7}\) training frames (with a lower score). We emphasise that the main aim of our empirical evaluations is to validate the utility of SPIE exploration objective as a simple modification to DQN. In principle, SPIE can be integrated with any state-of-the-art RL agent, and different instantiations of SPIE could be implemented to deal with the task at hand. We leave such investigation for future work.

   & DQN & DQN\({}^{}\) & RND & DQN\({}^{}\)-SR & DQN\({}^{}\)-SF-PF \\   Freeway & 32.4 (0.3) & 29.5 (0.1) & 28.2 (0.2) & 29.4 (0.1) & 27.5 (0.2) \\ Gravitar & 118.5 (22.0) & 1078.3 (254.1) & 714.1 (105.9) & 457.4 (120.3) & 1223.0 (408.9) \\ Mont. Rev. & 0.0 (0.0) & 0.0 (0.0) & 528 (314.0) & 1395.4 (1121.8) & 1530.0 (1072.1) \\ Private Eye & 1447.4 (2,567.9) & 113.4 (42.3) & 61.3 (53.7) & 104.4 (50.4) & 488.2 (390.9) \\ Solaris & 783.4 (55.3) & 2132.6 (394.8) & 1395.2 (401.7) & 1890.1 (163.1) & 2455.8 (262.0) \\ Venture & 4.4 (5.4) & 1220.1 (51.0) & 953.7 (167.3) & 1348.5 (56.5) & 1274.0 (133.2) \\  

Table 2: Evaluations of SPIE with deep RL implementation on hard-exploration Atari games (averaged over 10 random seeds, numbers in the parentheses are 1 standard errors).

Figure 4: **Evaluation of SPIE with linear features in MountainCar. (a) Graphical demonstration of MountainCar environment; (b); Example random Fourier features; (c) Evaluations of Q-learning with linear function approximation with intrinsic rewards \(r_{}\) and \(r_{}\) on MountainCar. Reported results are averaged over \(10\) random seeds.**

Conclusion

The development of more efficient exploration algorithms is essential for practical implementation of RL agents in real-world environment where sample efficiency and optimality are vital to success. Here, we propose a general intrinsically motivated exploration framework, SPIE, where we construct intrinsic rewards by combining both prospective and retrospective information contained in past trajectories. The retrospective component provides information about the connectivity structure of the environment, facilitating more efficient targeted exploration between sub-regions of state space given structure awareness (e.g., robust identification of the bottleneck states; Figure 0(a)). SPIE yields more sample efficient exploration in discrete MDPs under complete absence of external reinforcement. Moreover, a side benefit we observe empirically is that SPIE exhibits ethologically plausible exploratory behaviour during exploration in grid worlds (i.e., cycling between different clusters of states). In continuous state space, we developed a novel generalization of the predecessor representation, the predecessor features, for capturing retrospective information in continuous spaces. Empirical evaluations on both discrete and continuous MDPs demonstrate that SPIE yields improvements over existing intrinsic exploration methods, in terms of sample efficiency of learning and asymptotic performance, and for adapting to non-stationary reward structures.

We instantiate SPIE using the SR and the PR, but we note that SPIE is a general framework that can be implemented with other formulations (e.g., predictive error in a temporally backward direction ) and with more advanced neural architectures (including those currently unthought of). Although here we have examined the empirical properties of SPIE, the theoretical underpinnings for SPIE and the bottleneck seeking exploratory behavior bears further investigation. Specifically, more work needs to be done to probe the theoretical property of using SF and PF in continuous settings. Our definition of \(r_{}\) overlaps with the successor contingency , which has long been recognised for learning causal relationship between predictors and reward . An interesting venue for future work is to investigate the implications of SPIE for causally guided exploration in RL. Another interesting direction for future work is to investigate the implications of SPIE in human exploration, where we could utilise SPIE to investigate how human balance local (e.g., visitation counts) versus global (e.g., environment structure) information for exploration in sequential decision tasks .