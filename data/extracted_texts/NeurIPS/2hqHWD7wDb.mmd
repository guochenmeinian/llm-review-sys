# Quantitative Convergences of Lie Group Momentum Optimizers

Lingkai Kong

School of Mathematics

Georgia Institute of Technology

lkong75@gatech.edu

&Molei Tao

School of Mathematics

Georgia Institute of Technology

mtao@gatech.edu

###### Abstract

Explicit, momentum-based dynamics that optimize functions defined on Lie groups can be constructed via variational optimization and momentum trivialization. Structure preserving time discretizations can then turn this dynamics into optimization algorithms. This article investigates two types of discretization, Lie Heavy-Ball, which is a known splitting scheme, and Lie NAG-SC, which is newly proposed. Their convergence rates are explicitly quantified under \(L\)-smoothness and _local_ strong convexity assumptions. Lie NAG-SC provides acceleration over the momentumless case, i.e. Riemannian gradient descent, but Lie Heavy-Ball does not. When compared to existing accelerated optimizers for general manifolds, both Lie Heavy-Ball and Lie NAG-SC are computationally cheaper and easier to implement, thanks to their utilization of group structure. Only gradient oracle and exponential map are required, but not logarithm map or parallel transport which are computational costly.1

## 1 Introduction

First-order optimization, i.e., with the gradient of the potential (i.e. the objective function) given as the oracle, is ubiquitously employed in machine learning. Within this class of algorithms, momentum is often introduced to accelerate convergence; for example, in Euclidean setups, it has been proved to yield the optimal dimension-independent convergence order, for a large class of first-order optimizers, under strongly convex and \(L\)-smooth assumptions of the objective function[21, Sec. 2].

Gradient Descent (GD) without momentum can be generalized to Riemannian manifold by moving to the negative gradient direction using the exponential map with a small step size. This generalization is algorithmically straight forward, but quantifying the convergence rate in curved spaces needs nontrivial efforts . In comparison, generalizing momentum GD to manifold itself is nontrivial due to curved geometry; for example, the iteration must be kept on the manifold and the momentum must stay in the tangent space, which changes with the iteration, at the same time. It is even more challenging to quantify the convergence rate theoretically due to the loss of linearity in the space, leading to the lack of triangle inequality and cosine rule. Finally, there is not necessarily acceleration unless the generalization is done delicately.

Regardless, optimization on manifolds is an important task, for which the manifold structure can either naturally come from the problem setup or be artificially introduced. A simple but extremely important example is to compute the leading eigenvalues of a large matrix, which can be approached efficiently via optimization on the Stiefel manifold ; a smaller scale version can also be solved via optimization on \((n)\). More on the modern machine learning side, one can algorithmically add orthonormal constraints to deep learning models to improve their accuracy and robustness [e.g., 5, 9, 17, 19, 24]. Both examples involve \((n)\), which is an instance of an important type of curved spaces called Lie groups.

Lie groups are manifolds with additional group structure, and the nonlinearity of the space manifests through the non-commutative group multiplication. The group structure can help not only design momentum optimizer but also analyze its convergence. More precisely, this work will make the following contributions:

* Provide the first quantitative analysis of Lie group momentum optimizers. This is significant, partly because there is no nontrivial convex functions on many Lie groups (see Rmk. 1), so we have to analyze nonconvex optimization.
* Theoretically show an intuitively constructed momentum optimizer, namely Lie Heavy-Ball, may not yield accelerated convergence. Numerical evidence is also provided (Sec. 6).
* Generalize techniques from Euclidean optimization to propose a Lie group optimizer, Lie NAG-SC, that provably has acceleration.

Comparing to other optimizers that are designed for general manifolds, we bypass the requirements for costly operations such as parallel transport [e.g., 4], which is a way to move the momentum between tangent spaces, and the computation of geodesic [e.g., 1], which may be an issue due to not only high computational cost but also its possible non-uniqueness.

### Related work

In Euclidean space, Gradient Descent (GD) for \(\)-strongly convex and \(L\)-smooth objective function can converge as \(\|x_{k}-x_{*}\|(1-C)^{k}\|x_{k}-x_{0}\|\)2 upon appropriately chosen learning rate. Momentum can accelerate the convergence by softening the dependence on the condition number \(:=L/\). However, how momentum is introduced matters to achieving such an acceleration. For example, NAG-SC  has convergence rate 3\(1-C}\), but Heavy-Ball  still has linear dependence on the conditional number, similar to gradient descent without momentum (but it may work better for nonconvex cases).

Remarkable quantitative results also existed for manifold optimization. The momentum-less case is relatively simpler, and , for example, developed convergence theory for GD on Riemannian manifold under various assumptions on convexity and smoothness, which matched the classical Euclidean result -- for instance, Thm. 15 of their paper gave a convergence rate of \(1-C\{,K\}^{4}\) when the learning rate is \(1/L\), under geodesic-\(\)-strong-convexity and geodesic-\(L\)-smoothness. For the momentum case,  analyzed the convergence of a related dynamics in continuous time, namely an optimization ODE corresponding to momentum gradient flow, on Riemannian manifolds under both geodesically strongly and weakly convex potentials, based on a tool of modified cosine rule. However, numerical methods in discrete time that are easy and cheap to implement and provably convergent in an accelerated fashion under mild conditions are still under-developed. One existing idea is to transform a function on the manifold to a function on a Euclidean space by the logarithm function. More precisely, in the case where the logarithm is a one-to-one map from the manifold to the tangent space, it can be used to project the objective function on the manifold to a function on the tangent space ('pullback objective function'), enabling the usage of accelerated algorithms in Euclidean spaces [e.g., 11]. Although such analysis may relax the requirement of global convexity, it requires assumptions on the 'pullback objective function', which is hard to check in reality. Another series of seminal works include , which analyzed the convergence of a class of optimizers by extending Nesterov's technique of estimating sequence  to Riemannian manifolds. They managed to show a convergence rate between \(1-C}\) and \(1-C\), i.e., with conditional number dependence inbetween that of GD with and without momentum in the Euclidean cases. They further proved that, as the iterate gets closer to the minimum, the rate bound gets better because it converges to \(1-C}\). However, their algorithm requires the logarithm map (inverse of the exponential map), which may not be uniquely defined on many manifolds (e.g., sphere) and can be computationally expensive. In contrast, Lie NAG-SC, which we will construct, works only for Lie group manifolds, but they are more efficient when applicable, due to being based on only exponential map and gradient oracle. Acceleration of the same type will also be theoretically proved.

Momentum optimizers specializing in Lie groups have also been constructed before , where variational optimization  was generalized to the manifold setup and then left trivialization were employed to obtain ODEs with Euclidean momentum that perform optimization in continuous time. Our work is also based on those ODEs, whose time however has to be discretized so that an optimization algorithm can be constructed. Delicate discretizations have been proposed in  so that the optimization iterates stay exactly on the manifold, saving computational cost and reducing approximation errors. But we will further improve those discretizations. More precisely, note first that  slightly abused notation and called both the continuous dynamics and one discretization Lie NAG-SC. However, we find that their splitting discretization may not give the best optimizer - at least, a 1st-order version of their splitting scheme yields a linear condition number dependence in our convergence rate bound. Since this splitting-based optimizer almost degenerates to heavy-ball in the special case of Euclidean spaces (as Rmk. 28 will show), we refine the terminology and call it Lie Heavy-Ball. To remedy the lack of acceleration, we propose a new discretization that actually has square root condition number dependence and thus provable acceleration, and call it as (the true) Lie NAG-SC.

Finally, note there can be obstructions to accelerated optimization, for example roughly when curvature is negative [16; 10; 12]. That is, however, not a contradiction because our setup involves positive curvature.

### Main results

We consider the local minimization of a differentiable function \(U:\), i.e., \(_{g}U(g)\), where \(\) is a finite-dimensional compact Lie group, and the oracle allowed is the differential of \(U\).

Two optimizers we focus on are given in Alg. (1). Under assumptions of \(L\)-smoothness and locally geodesic-\(\)-strong convexity, we proved that Lie Heavy-Ball has convergence rate \((1+C)^{-1}\), which is approximately the convergence rate of \(1-C\{,K\}\) for Lie GD (Eq.1, which is identical to Riemannian GD applied to Lie groups); this is no acceleration. To accelerate, we propose a new Lie NAG-SC algorithm, with provable convergence rate \((1+C\{},K\})^{-1}\). Note the condition number dependence becomes \(\) instead of \(:=L/\), hence acceleration.

For a summary of our main results, please see Table 1.

```
Parameter :step size \(h>0\), friction \(>0\), number of iterations \(N\) Initialization :\(g_{0}\), \(_{0}=0\) Output :Local minimum of \(U\) for\(k=0,,N-1\)do ifHeavy-Ballthen \(_{k+1}=(1- h)_{k}-hT_{g_{k}}_{g_{k-1}} U(g_{k})\) ifNAG-SCthen \(_{k+1}=(1- h)_{k}-(1- h)h(T_{g_{k}}_{g_{k-1 }} U(g_{k})-T_{g_{k-1}}_{g_{k-1}} U(g_{k-1}))-\) \(hT_{g_{k}}_{g_{k-1}} U(g_{k})\) \(g_{k+1}=g_{k}(h_{k+1})\)  end if return\(g_{N}\)
```

**Algorithm 1**Momentum optimizer on Lie groups

**Remark 1** (Triviality of convex functions on Lie groups).: _We do not assume any global convexity of \(U\). In fact, \(U\) has to be nonconvex for any meaningful optimization to happen. This is because we are considering compact Lie groups5, and a convex function on a connected compact manifold could only be a constant function . An intuition for this is, a convex function on a closed geodesic must be constant. See [e.g., 18, Sec. B.3] for more discussions. Our analysis is, importantly, for nonconvex \(U\), and convexity is only required locally to ensure a quantitative rate estimate can be obtained._

## 2 Preliminaries and setup

### Lie group and Lie algebra

A **Lie group**, denoted by \(\), is a differentiable manifold with a group structure. A **Lie algebra** is a vector space with a bilinear, alternating binary operation that satisfies the Jacobi identity, known as Lie bracket. The tangent space at \(e\) (the identity element of the group) is a **Lie algebra**, denoted as \(:=T_{e}\). The dimension of the Lie group \(\) will be denoted by \(m\).

**Assumption 2** (general geometry).: _We assume the Lie group \(\) is finite-dimensional and compact._

One technique we will use to handle momentum is called **left-trivialization**: Left group multiplication \(L_{g}: g\) is a smooth map from the Lie group to itself and its tangent map \(T_{g}_{g}:T_{} T_{g}\) is a one-to-one map. As a result, for any \(g\), we can represent the vectors in \(T_{g}\) by \(T_{e}_{}\) for \( T_{e}\). This operation is the left-trivialization. It comes from the group structure and may not exist for a general manifold. If the group is represented via an embedding to matrix group, i.e., \(g,^{n n}\), then the left trivialization is simply given by \(T_{e}_{g}=g\) with the right-hand side given by matrix multiplication.

A Riemannian metric is required to take Riemannian gradient and we are considering a left-invariant metric: we first define an inner product \(,\) on \(\), which is a linear space, and then move it around by the differential of left multiplication, i.e., the inner product at \(T_{g}\) is for \(_{1},_{2} T_{g}\), \(_{1},_{2}T_{g}_{g^{-1}}_{1 },T_{g}_{g^{-1}}_{2}\).

### Optimization dynamics

Riemannian GD [e.g., 32] with iteration \(g_{k+1}=_{g_{k}}(-h U(g_{k}))\) can be employed to optimize \(U\) defined on \(\), where \(\) is Riemannian gradient, and \(: T_{g}\) is the exponential map. To see a connection to the common Euclidean GD, it means we start from \(g_{k}\) and go to the direction of negative gradient with step size \(h\) to get \(g_{k+1}\) by geodesic instead of straight line. Riemannian GD can be understood as a time discretization of the Riemannian gradient flow dynamics \(=- U(g)\).

In the Lie group case, it is identical to the following Lie GD obtained from left-trivialization :

\[g_{k+1}=g_{k}_{e}(hT_{g_{k}}_{g_{k}^{-1}} U(g_{k})), \]

where \(T_{g_{k}}_{g_{k}^{-1}} U(g_{k})\) is the left-trivialized gradient. \(_{e}\)7 is the exponential map staring at the group identity \(e\) following the Riemannian structure given by the left-invariant metric, and the operation between \(g_{k}\) and \((hT_{g_{k}}_{g_{k}^{-1}} U(g_{k}))\) is the group multiplication. To accelerate its convergence, momentum was introduced to the Riemannian gradient flow via variational optimization and left-trivialization , leading to the following dynamics:

\[=T_{e}_{g}\\ =-(t)+_{}^{*}-T_{g}_{g^{-1}}  U(g) \]

Here \(g(t)\) is the position variable. \(\) is the standard'momentum' variable even though it should really be called velocity. It lives \(T_{g(t)}\), which varies as \(g(t)\) changes in time, and we will utilize group structure to avoid this complication. More precisely, the dynamics lets the'momentum' \(\) be \(T_{e}_{g}\), and \(\) is therefore \(T_{g}_{g^{-1}}\) and it is our new, left-trivialized momentum. Intuitively, one can

   & Continuous dynamics & Heavy-Ball & NAG-SC \\  Scheme & Eq. (2) & Eq. (9) & Eq. (13) \\  Step size \(h\) & - & \(}{4L}\) & \(\{},\}\) \\  Convergence rate \(c\) & \(\) & \((1+)^{-1}\) & \((1+\{},_{g}\) being \(g\) is position times angular momentum, which is momentum. Similar to the Lie GD Eq. (1), we will not use \( U(g)\) directly, but its left-trivialization \(T_{g}_{g^{-1}}( U(g))\), to update the left-trivialized momentum.

This dynamics essentially models a damped mechanical system, and Tao and Ohsawa  proved this ODE converges to a local minimum of \(U\) using the fact that the total energy (kinetic energy \((,)\) plus potential energy \(U\)) is drained by the friction term \(-\). In general, \(\) can be a positive time-dependent function (e.g., for optimizing convex but not strongly-convex functions), but for simplicity, we will only consider locally strong-convex potentials, and constant \(\) is enough.

For curved space, an additional term \(_{}^{*}\,\) that vanishes in Euclidean space shows up in Eq. (2). It could be understood as a generalization of Coriolis force that accounts for curved geometry and is needed for free motion. The **adjoint operator**\(:\) is defined by \(_{X}\,Y:=X,Y\). Its dual, known as the **coadjoint operator**\(^{+}:\), is given by \((_{X}^{*}\,Y,Z)= Y,_{X}\,Z, Z \).

### Property of Lie groups with \(^{*}\) skew-adjoint

The term \(_{}^{*}\,\) in the optimization ODE (2) is a quadratic term and it will make the numerical discretization that will be considered later difficult. Another complication from this term is, it depends on the Riemannian metric, and indicates an inconsistency between the Riemannian structure and the group structure, i.e., the exponential map from the Riemannian structure is different from the exponential map from the group structure. Fortunately, on a compact Lie group, the following lemma shows a special metric on \(\) can be chosen to make the term \(_{}^{*}\,\) vanish.

**Lemma 3** (ad skew-adjoint ).: _Under Assumption 2, there exists an inner product on \(\) such that the operator \(\) is skew-adjoint, i.e., \(_{}^{*}=-\,_{}\) for any \(\)._

This special inner product will also give other properties useful in our technical proofs; see Sec. A.1.

### Assumption on potential function

To show convergence and quantify its rate for the discrete algorithm, some smoothness assumption is needed. We define the \(L\)-smoothness on a Lie group as the following.

**Definition 4** (\(L\)-smoothness).: _A function \(U:\) is \(L\)-smooth if and only if \( g, G\),_

\[T_{}_{^{-1}} U()-T_{g} _{g^{-1}} U(g) Ld(,g) \]

_where \(d\) is the geodesic distance._

Under the choice of metric in Lemma 3 that \(\) is skew-adjoint, Lemma 21 shows this is same as the commonly used geodesic-\(L\)-smoothness (Def. 20).

To provide an explicit convergence rate, some convex assumption on the objective function is usually needed. Under the assumption of unique geodesic on a geodesically convex set \(S\), the definition of strongly convex functions in Euclidean spaces can be generalized to Lie groups:

**Definition 5** (Locally geodesically strong convexity).: _A function \(U:\) is locally geodesic-\(\)-strongly convex at \(g_{*}\) if and only if there exists a geodesically convex neighbourhood of \(g_{*}\), denoted by \(S\), such that \( g, S\),_

\[U(g)-U()T_{}_{^{-1}} U (),^{-1}g+^{-1 }g^{2} \]

_where \(\) is well-defined due to the geodesic convexity of \(S\)._

## 3 Convergence of the optimization ODE in continuous time

To start, we provide a convergence analysis of the ODE (2), since our numerical scheme comes from its time discretization. We do not claim such convergence analysis for the ODE is new, and in fact, convergence for continuous dynamics has been provided on general manifolds [e.g., 3]. However, we will prove it using our technique to be self-contained and provide some insights for the convergence analysis of the discrete algorithm later.

Define the total energy \(E^{}:\) as

\[E^{}(g,):=U(g)+^{2} \]

i.e., the total energy is the sum of the potential energy and the kinetic energy. Thanks to the friction \(\), the total energy is monotonely decreasing, which provides global convergence to a stationary point.

**Theorem 6** (Monotonely decreasing of total energy ).: _Suppose the potential function \(U^{1}()\) and the trajectory \((g(t),(t))\) follows ODE (2). Then_

\[E^{ODE}(g(t),(t))=-\|\|^{2}\]

Thm. 6 provides the global convergence of ODE (2) to a stationary point under only \(^{1}\) smoothness: when the system converges, we have \(_{}=0\), which gives \(\| U(g_{})\|=0\).

Moreover, using the non-increasing property of total energy, the following corollary states that if the particle starts with small initial energy, it will be trapped in a sub-level set of \(U\). The local potential well can be defined using \(U\)'s sub-level set.

**Definition 7** (\(u\) sub-level set).: _Given \(u\), we define the \(u\) sub-level set of \(U\) as_

\[\{g:U(g) u\}:=_{i 0}S_{i}\]

_i.e. a disjoint union of connected components._

**Corollary 8**.: _Suppose \(U^{1}()\). Let \(u=E^{ODE}(g(0),(0))\). If the \(u\) sub-level set of \(U\) is \(_{i 0}S_{i}\) and \(g(0) S_{0}\), then we have \(g(t) S_{0}, t 0\)._

Under further assumption of local strong convexity on this sub-level set, convergence rate can be quantified via a Lyapunov analysis inspired by . More specifically, given a fixed local minimum \(g_{*}\), there is provably a local unique geodesic convex neighbourhood of \(g_{*}\). Denote it by \(S\), and we define \(^{}\) on \(S\) by

\[^{}(g,):=U(g)-U(g_{*})+\|\|^{2}+ g_{*}^{-1}g+^{2} \]

By assuming the local geodesic-\(\)-strong convexity of \(U\) on \(S\), we have the following quantification of Eq. (2).

**Theorem 9** (Convergence rate of the optimization ODE).: _If the initial condition \((g_{0},_{0})\) satisfies that \(g_{0} S\) for some geodesically convex set \(S\), \(U^{1}()\) is locally geodesic-\(\)-convex on \(S\), and the \(u\) sub-level set of \(U\) with \(u=E^{ODE}(g_{0},_{0})\) satisfies \(S_{0} S\), then we have_

\[U(g(t))-U(g_{*}) e^{-c_{}}t}^{}(g_{0}, _{0}) \]

_with \(c_{}}=\) by choosing \(=2\)._

**Remark 10**.: _This theorem alone is a local convergence result and a [Lie group + momentum] extension of an intuitive result for Euclidean gradient flow, which is, if the initial condition is close enough to a minimizer and the objective function has a positive definite Hessian at that minimizer, then gradient flow converges exponentially fast to that minimizer. However, Thm.6 already ensures global convergence, and if not stuck at a saddle point, the dynamics will eventually enter some local potential well. If that potential well is locally strongly convex at its minimizer, then the local convergence result (Thm.9) supersedes the global convergence result (which has no rate), and gives the asymptotic convergence rate. Note however that different initial conditions may lead to convergence to different potential wells (and hence minimizers), as usual._

## 4 Convergence of Lie Heavy-Ball/splitting discretization in discrete time

One way to obtain a manifold optimization algorithm by time discretization of the ODE (2) is to split its vector field as the sum of two, and use them respectively to generate two ODEs:

\[=T_{e}_{g}\\ =0=0\\ =--T_{g}_{g^{-1}} U(g) \]

Each ODE enjoys the feature that its solution stays exactly on \(\), and therefore if one alternatively evolves them for time \(h\), the result is a step-\(h\) time discretization that exactly respects the geometry (no projection needed). If one approximates \((- h)\) by \(1-h\), then the same property holds, and the resulting optimizer is

\[g_{k+1}=g_{k}(h_{k+1})\\ _{k+1}=(1- h)_{k}-hT_{g_{k}}_{g_{k}^{-1}} U(g_{k})  \]In Euclidean cases, such numerical scheme can be viewed as Polyak's Heavy-Ball algorithm after a change of variable (Rmk. 27), and will thus be referred to as Lie Heavy-Ball. It is also a 1st-order (in \(h\)) version of the '2nd-order Lie-NAG' optimizer in  (Rmk. 28).

To analyze Lie Heavy-Ball's convergence, we again seek some 'energy' function such that the iteration of the numerical scheme Eq. (9) will never escape a sub-level set of the potential, similar to the continuous case. Given fixed friction parameter \(\) and step size \(h\), we define the modified energy \(E^{}:\) as

\[E^{}(g,) U(g)+}{2}\|\|^{2} \]

**Theorem 11** (Monotonely decreasing of modified energy of Heavy Ball).: _Assume the potential \(U\) is globally \(L\)-smooth. When the step size satisfies \(h+L}\), we have the modified energy \(E^{}\) is monotonely decreasing, i.e.,_

\[E^{}(g_{k},_{k})-E^{}(g_{k-1},_{k-1})- h\| _{k}\|^{2}\]

Thm. 11 provides the global convergence of Heavy-Ball scheme Eq. (9) to a stationary point under only \(L\)-smoothness: Due to the monotonicity of the energy function \(E^{}\), the system will eventually converge. When it converges, since \(g\) is not moving, we have \(\|_{}\|=0\), leading to the fact that \(\| U(g_{})\|=0\). More importantly, the following corollary shows that the non-increasing property of the modified traps \(g\) in sub-level set of \(U\):

**Corollary 12**.: _Let \(u=E^{}(g_{0},_{0})\). If the \(u\) sub-level set of \(U\) satisfies \(g_{0} S_{0}\) and_

\[d(S_{0},_{i 1}S_{i})>h}(g_{0},_{0} )}+h^{2}_{S_{0}}\| U\| \]

_Then we have \(g_{k} S_{0}\) for any \(k\) for the Heavy-Ball scheme Eq. (9) when \(h+L}\)._

Under the further assumption of local strong convexity on this sub-level set, the convergence rate can be quantified via a Lyapunov analysis inspired by . More specifically, given a fixed local minimum \(g_{*}\), there is a local unique geodesic neighbourhood of \(g_{*}\), denoted by \(S\), and we define \(^{}\) on \(S\) by

\[^{}(g,)(U(g(-h ))-U(g_{*}))+\|\|^{2}+ g_{*}^{-1}g+^{2} \]

The exponential decay for the Lyapunov function (Lemma 32) helps us quantify of the convergence rate for Eq. (9) in the following theorem:

**Theorem 13** (Convergence rate of Heavy-Ball scheme).: _If the initial condition \((g_{0},_{0})\) satisfies that \(g_{0} S\) for some geodesically convex set \(S\), \(U\) is \(L\)-smooth and locally geodesic-\(\)-convex on \(S\), and the \(u\) sub-level set of \(U\) with \(u=E^{}(g_{0},_{0})\) satisfies \(S_{0} S\) and Eq. (11), then we have_

\[U(g_{k})-U(g_{*}) c_{}^{k}^{}(g_{0},_{0})\]

_with \(c_{}(1+)^{-1}\) by choosing \(=2\), \(h=}{4L}\)._

Note the rate is \((1+1/(16))^{-1}\). The condition number dependence is linear (\(\)) but not \(\). Similarly, the procedure of global convergence \(\) local potential well \(\) local minimum discussed in Rmk. 10 also applies the Heavy-Ball algorithm.

## 5 Convergence of Lie NAG-SC in discrete time

The motivation for NAG-SC is to improve the condition number dependence. The convergence rate of Heavy-Ball shown in Thm. 13 is the same as the momentumless case (e.g., Thm. 15) under the assumption of local strong convexity and \(L\)-smoothness. To improve the condition number dependence, inspired by , we define Lie NAG-SC as the following:

\[g_{k+1}=g_{k}(h_{k+1})\\ _{k+1}=(1- h)_{k}-(1- h)h(T_{g_{k}}_{g_{k^{ -1}}} U(g_{k})-T_{g_{k-1}}_{g_{k^{-1}}} U(g_{k-1}) )-hT_{g_{k}}_{g_{k^{-1}}} U(g_{k}) \]Comparing to Lie Heavy-Ball, an extra \((h^{2})\) term \(h(T_{g_{k}}_{g_{k^{-1}}} U(g_{k})-T_{g_{k-1}}_{g_{ k-1}^{-1}} U(g_{k-1}))\) is introduced (see [25, Sec. 2] for more details in the Euclidean space). Our technique of left-trivialized (and hence Euclidean) momentum allows this trick to transfer directly from Euclidean to the Lie group case.

For NAG-SC, we will only provide a local convergence with quantified convergence under \(L\)-smoothness and local geodesically convexity on a geodesically convex subset \(S\). The difficulty in designing a modified energy and proving the global convergence will be given later in Rmk. 36. We define the following Lyapunov function:

\[^{}(g,) (U(g(-h))-U(g_{*}))+^{2} \] \[++ g_{*}^{-1} g+h U(g(-h))^{2}-(2- h)}{4(1- h)}  U(g(-h))^{2}\]

where \(g_{*}\) is the minimum of \(U\) in \(S\). This Lyapunov function helps us to trap \(g\) in a local potential well and quantify the convergence rate:

**Theorem 14** (Convergence rate of NAG-SC).: _If the initial condition \((g_{0},_{0})\) satisfies that \(g_{0} S\) for some geodesically convex set \(S\) satisfying \(_{g S}d(g_{*},g)\) for some \(a<2\) and \(A:=_{\|X\|=1}_{X}_{}\), \(U\) is \(L\)-smooth and locally geodesic-\(\)-convex on \(S\), and the \(u\) sub-level set of \(U\) with \(u=(1- h)^{-1}^{}(g_{0},_{0})\) satisfies \(S_{0} S\) and_

\[d(S_{0},S-S_{0})>h^{}(g_{0},_{0})} \]

_then we have_

\[U(g_{k})-U(g_{*}) c_{}^{k}^{}(g_{0}, _{0})\]

_by choosing \(h=\{},\}\) and \(=2\), with \(c_{}:=(1+\{ },\})^{-1}\), where_

\[p(x):= \]

Unlike sampling ODE and Lie Heavy-Ball, monotonely decreasing modified energy is not provided for Lie NAG-SC. It is unclear whether such modified energy for NAG-SC exists, and an intuition is provided in the Rmk. 36.

Another fact in Thm. 14 that is worth noticing is, we have a term \(1/p(a)\) that depends on the curvature of the Lie group 8, while the Lie Heavy-Ball has the same convergence rate as the Euclidean case . It is unclear if the lost of convergence rate in Lie NAG-SC comparing to the Euclidean case is because of our proof technique or the curved space itself. However, we try to provide some insights in Rmk. 35.

## 6 Systematic numerical verification via the eigen decomposition problem

### Analytical estimation of property of eigenvalue decomposition potential

Given a symmetric matrix, its eigen decomposition problem can be approached via an optimization problem on \((n)\):

\[_{X^{n n},X^{}X=I}X^{}BXN\]

where \(N:=([1,,n])\). This problem is a hard non-convex problem on manifold, but some analytical estimation [e.g., 7, Thm. 4] can be helpful for us to choose optimizer hyperparameters (we don't have to have those to apply the optimizers, but in this section we'd like to verify our theoretical bounds and hence \(\) and \(L\) are needed).

This problem is non-convex with \(2^{n}n!\) stationary points corresponding to the elements in \(n\)-order symmetric group, including \(2^{n}\) local minima and \(2^{n}\) local maxima. We suppose \(B=R R^{}\) with \(=(0,1,,n-2,)\), where \(_{i}\)'s (the diagonal values of \(\)) are in ascend order. Given \(\) in the \(n\)-symmetric group, the corresponding local minimum is \(X_{}:=(X_{(i)})\), i.e., we switch the columns of \(X\) by \(\). The eigenvalues of its Hessian at the local minimum \(\) can be written as

\[_{ij}=(j-i)(_{(j)}-_{(i)}), 1 i<j n\]

The global minimum is given by \(_{*}=id\) with minimum value \(_{i=1}^{n}i_{i}\).

### Numerical Experiment

We use the eigenvalues at the global minimum to estimate the \(L\) and \(\) in its neighborhood. As a result, around the global minimum, \(Ln-1(_{n}-_{1}\), and \(_{i}\{_{i+1}-_{i}\}\), where we assume \(\)'s are sorted in the ascend order. Such estimation is used to choose our parameters (\(\) and \(h\)) in all experiments as stated in Table 1.

Given a conditional number \(\), we design \(A\) in the following way: we choose \(=0,1,,n-2,\) and \(R\) is uniformly sampled from \((n)\) using (22, Sec. 2.1.1). When the given \(\) satisfies \((n-1)(n-2)\), the condition number at global minimum is the given \(\).

The results are presented in Fig. 1 and 2. In all experiments, we set \(n=10\), and the computations are done on a MacBook Pro (M1 chip, 8GB memory).

## 7 Application to Vision Transformer

This section will demonstrate a practical modern machine learning application of our Lie NAG-SC optimizer. The setting is a highly non-convex optimization problem with stochastic gradients, due to being a real deep learning task, but empirical success is still observed. More specifically, it was discovered  that adding artificial orthogonal constraints to attention layers in transformer models can improve their performances, because orthogonality disallows linearly dependent correlations between tokens, so that the learned attentions can be more efficient and robust. We will apply our optimizer to solve this constrained optimization problem.

The setup is the following (using the notation of ): consider a Scaled Dot-Product Multi-head Attention given by \((Q,K,V)=(_{1},...,_{n})W^{O}\), where \(_{i}=(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})\), \((,,)=(^{}}{}})\). The trainable parameters are matrices \(W_{i}^{Q}^{d_{} d_{k}}\), \(W_{i}^{K}^{d_{} d_{k}}\), \(W_{i}^{V}^{d_{} d_{v}}\) and \(W^{O}^{n_{} d_{}}\). The

Figure 1: Fig. 1(a) shows that 1) Lie NAG-SC converges much faster than Lie Heavy-Ball on ill-conditioned problems; 2) The fitted dashed curve and the experimental results align well, showing our theoretical analysis of the convergence rate \(c_{}\) and \(c_{}\) is correct. Fig. 1(b) shows the performance of our algorithms on non-convex problems experimentally. In this specific experiment, Lie NAG-SC outperforms Lie Heavy-Ball and finds the global minimum successfully without being trapped in local minimums. However, we are not sure which is better in general optimization. One possible reason for the good performance on NAG-SC is it uses a larger learning rate and is better for jumping out of the local minimums. The values of Lyapunov function along the trajectory are not provided since it is not globally defined.

three input matrices \(Q\), \(K\) and \(V\) all have dimension \(sequence\_length d_{}\). \(d_{k}\) and \(d_{v}\) are usually smaller than \(d_{}\).

In the case \(d_{}=n_{}d_{k}\), which is satisfied in many popular models, we apply the constraint 'orthogonality across heads'  and require \((W_{i}^{Q},i=1...,n_{})\) and \((W_{i}^{K},i=1...,n_{})\) to be in \((d_{})\). We compare the performance of our newly proposed optimizer with the existing ones (the optimizer in  is identical to Lie Heavy-Ball on \((n)\)). Fig. 3 and Tab. 2 are the validation error when we train a vision transformer  with 6.3M parameters from scratch on CIFAR, showing an improvement of Lie NAG-SC comparing the state-of-the-art algorithm Lie Heavy-Ball. The computations are done on a single Nvidia V100 GPU. The model structures and hyperparameters are identical as Sec. 3.2 in . Each presented result is the average of 3 independent runs.

   & Euclidean SGD & Lie HB & Lie NAG-SC \\  CIFAR 10 & 9.84\% & 9.12\% & 8.77\% \\  CIFAR 100 & 32.68\% & 31.93\% & 31.38\% \\  

Table 2: Validation error rate of vision transformer trained by different algorithms on CIFAR, showing the performance is ordered by Euclidean GD < Lie Heavy-Ball < Lie NAG-SC for both CIFAR 10 and 100. The blue font means the lowest error rate.

Figure 3: Training curve when applying our Lie HB and Lie NAG-SC to vision transformers. Forcing the query matrices and the key matrices on \((n)\)[19, Orthogonality across heads] led to reduced training error. Moreover, validation error is also improved (Tab. 2).

Figure 2: Local convergence of Lie Heavy-Ball and Lie NAG-SC on eigenvalue decomposition problem with different condition numbers. The initialization is close to the global minimum. The dashed curves are the value of potential function along the trajectory and the solid curves are the values of the corresponding Lyapunov functions. Lie GD (Eq. 1) has \(h\) been chosen as \(1/L\)[32, Thm. 15]. We observe: 1. Lie NAG-SC converges much faster than Lie Heavy-Ball, especially on ill-conditioned problems. 2. Although the potential function is not monotonely decreasing, the Lyapunov is.