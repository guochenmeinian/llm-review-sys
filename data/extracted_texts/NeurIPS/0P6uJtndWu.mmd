# Efficient Diffusion Policies for Offline Reinforcement Learning

Bingyi Kang Xiao Ma Chao Du Tianyu Pang Shuicheng Yan

Sea AI Lab

{bingykang,yusufma555,duchao0726}@gmail.com {tianyupang,yansc}@sea.com

equal contribution

###### Abstract

Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked. Recently, Diffsuion-QL  significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers from two critical limitations. 1) It is computationally inefficient to forward and backward through the whole Markov chain during training. 2) It is incompatible with maximum likelihood-based RL algorithms (_e.g._, policy gradient methods) as the likelihood of diffusion models is intractable. Therefore, we propose _efficient diffusion policy_ (EDP) to overcome these two challenges. EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain. We conduct extensive experiments on the D4RL benchmark. The results show that EDP can reduce the diffusion policy training time **from 5 days to 5 hours** on gym-locomotion tasks. Moreover, we show that EDP is compatible with various offline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-the-art on D4RL by large margins over previous methods. Our code is available at https://github.com/sail-sg/edp.

## 1 Introduction

Offline reinforcement learning (RL) is much desired in real-world applications as it can extract knowledge from previous experiences, thus avoiding costly or risky online interactions. Extending online RL algorithms to the offline domain faces the distributional shift  problem. Existing methods mainly focus on addressing this issue by constraining a policy to stay close to the data-collecting policy , making conservative updates for Q-networks , or combining these two strategies . However, Offline RL can also be viewed as a state-conditional generative modeling problem of actions, where the parameterization of the policy network is important but largely overlooked. Most offline RL works follow the convention of parameterizing the policy as a diagonal Gaussian distribution with the learned mean and variance. This scheme might become inferior when the data distribution is complex, especially when offline

Figure 1: Efficiency and Generality. D-QL is Diffusion-QL. _Left_: The training time on the locomotion tasks in D4RL. _Right_: the performance of EDP and previous SOTA on each domain in D4RL. EDP is trained with TD3 on locomotion and IQL on the other three domains. (Best viewed in color.)data are collected from various sources and present strong multi-modalities . Therefore, more expressive models for the policy are strongly desired.

Recently, Diffusion-QL  made a successful attempt by replacing the diagonal Gaussian policy with a diffusion model, significantly boosting the performance of the TD3+BC  algorithm. Diffusion models [34; 10] have achieved the new state-of-the-art (SOTA) in image generation tasks [26; 5], demonstrating a superior ability to capture complex data distributions.

Despite the impressive improvement that Diffusion-QL has achieved, it has two critical drawbacks preventing it from practical applications. _First_, training a diffusion policy with offline RL is computationally inefficient. Consider a parameterized diffusion policy \(_{}(|)\), Diffusion-QL optimizes it by maximizing the Q value \(Q(,_{})\) of a state \(\) given a policy-generated action \(_{}_{}()\). However, sampling from a diffusion model relies on a long parameterized Markov chain (_e.g._, 1,000 steps), whose forward inference and gradient backpropagation are unaffordably expensive. _Second_, diffusion policy is not a generic policy class as it is restricted to TD3-style algorithms. As computing the sample likelihood \(_{}(a s)\) is intractable in diffusion models , diffusion policy is incompatible with a large family of policy gradient algorithms (_e.g._, V-Trace , AWR , IQL ), which require a tractable and differentiable log-likelihood \(_{}(|)\) for policy improvement.

In this work, we propose _efficient diffusion policy_ (EDP) to address the above two limitations of diffusion policies. Specifically, we base EDP on the denoising diffusion probabilistic model (DDPM) , which learns a noise-prediction network to predict the noise used to corrupt an example. In the forward diffusion process, a corrupted sample follows a predefined Gaussian distribution when the clean example and timestep are given. In turn, given a corrupted sample and predicted noise, we can approximate its clean version by leveraging the reparametrization trick. Based on this observation, to avoid the tedious sampling process, we propose _action approximation_ to build an action from a corrupted one, which can be easily constructed from the dataset. In this way, each training step only needs to pass through the noise-prediction network once, thus substantially reducing the training time. As experimented, by simply adding action approximation, we obtain **2x** speed-up without performance loss. Moreover, we apply DPM-Solver , a faster ODE-based sampler, to further accelerate both the training and sampling process. Finally, to support likelihood-based RL algorithms, we leverage the evidence lower bound for the likelihood developed in DDPM and approximate the policy likelihood from a constructed Gaussian distribution with variance fixed and mean obtained from action approximation.

We evaluate the efficiency and generality of our method on the popular D4RL benchmarking, as shown in Fig. 1. We first benchmark the efficiency of EDP on gym-locomotion tasks. By replacing the diffusion policy in Diffusion-QL with our EDP, the training time of Diffusion-QL is reduced substantially **from five days to five hours** (compared to their official code). Meanwhile, we observe slight to clear performance improvements on different tasks as the improved efficiency enables training DDPM with more timesteps than before. Moreover, we plug EDP into three different offline algorithms (including TD3+BC, CRR, and IQL), and the results justify its superiority over standard diagonal Gaussian policies. As a result, EDP set up new state-of-the-art on all four domains in D4RL.

## 2 Related Work

Offline RLDistributional shift between the learned and behavior policies is offline RL's biggest challenge. Existing research mitigates this problem by making modifications to policy evaluation [21; 17; 15; 24] or policy improvement [9; 39; 6; 33; 38; 42; 41]. For example, conservative Q-learning (CQL)  penalizes out-of-distribution actions for having higher Q-values, proving that this is equivalent to optimizing a lower bound of Q-values. Onestep RL  conducts policy evaluation on in-distribution data to avoid querying unseen actions. IQL  introduces expectile regression  to approximate dynamic programming with the Bellman optimality function. TD3+BC explicitly constrains the learned policy by adding a behavior cloning loss to mimic the behavior policy. Instead, CRR and AWR impose an implicit policy regularization by performing policy gradient-style policy updates. Despite their effectiveness, they ignore that the capacity of a policy representation plays a vital role in fitting the data distribution. This paper instead focuses on an orthogonal aspect (_i.e._, policy parameterization) that all the above methods can benefit. Another line of work tries to cast offline RL as a sequence-to-sequence model [3; 11], which is beyond the scope of this work.

Policy ParametrizationDifferent RL algorithms may pose different requirements for parameterizing a policy distribution. There are mainly two categories of requirements: 1) The sampling process is differentiable, such as the deterministic policy in DDPG  and TD3 . 2) The log-likelihood of samples is tractable. For example, policy gradient methods [30; 31; 38; 28] optimize a policy based on maximum likelihood estimation (MLE). Therefore, most works represent policy with a diagonal Gaussian distribution with mean and variance parameterized with a multi-layer perceptron (MLP). On the other hand, BCQ  and BEAR  choose to model policy with a conditional variational autoencoder (CVAE). Recently, Diffusion-QL  introduced diffusion models into offline RL and demonstrated that diffusion models are superior at modeling complex action distributions than CVAE and diagonal Gaussian. However, it takes tens to hundreds more time to train a diffusion policy than a diagonal Gaussian one. Moreover, diffusion policy only satisfies the first requirement, which means many other offline RL algorithms can not use it, including the current SOTA IQL.

Our method is motivated to solve the about two limitations. We first propose a more efficient way to train diffusion policies, which reduces training time to the level of a Gaussian policy. Then, we generalize the diffusion policy to be compatible with MLE-based RL methods.

## 3 Preliminaries

### Offline Reinforcement Learning

A decision-making problem in reinforcement learning is usually represented by a Markov Decision Process (MDP): \(=\{,,P,R,\}\). \(\) and \(\) are the state and action spaces respectively, \(P(^{}|,)\) measures the transition probability from state \(\) to state \(^{}\) after taking action \(\) while \(R(,,^{})\) gives the reward for the corresponding transition, \([0,1)\) is the discount factor. A policy \((|)^{2}\) describes how an agent interacts with the environment. The optimal policy \(^{*}(|)\) is the one achieves maximal cumulative discounted returns: \(^{*}=_{}[_{t=0}^{}^{t}r( _{t},_{t})]\). Reinforcement learning algorithms frequently rely on the definition of value function \(V()=_{}[_{t=0}^{}^{t}r( _{t},_{t})|_{0}=]\), and action value (Q) function \(Q(,)=_{}[_{t=0}^{} ^{t}r(_{t},_{t})|_{0}= ,_{0}=]\), which represents the expected cumulative discounted return of a policy \(\) given the initial state \(\) or state-action pair \((,)\).

In the offline RL setting, instead of learning from interactions with the environment, agents focus on learning an optimal policy from a previously collected dataset of transitions: \(=\{(_{t},_{t},_{t+1},r_ {t})\}\). Offline RL algorithms for continuous control are usually based on an actor-critic framework that alternates between policy evaluation and policy improvement. During policy evaluation, a parameterized Q network \(Q_{}(,)\) is optimized based on approximate dynamic programming to minimize the following temporal difference (TD) error \(L_{}()\): \(_{(,,^{}) }[(r(,)+_{ ^{}}Q_{}(^{},^ {})-Q_{}(,))^{2}],\) where \(Q_{}(,)\) denotes a target network. Then at the policy improvement step, knowledge in the Q network is distilled into the policy network in various ways. Offline RL methods address the distributional shift  problem induced by the offline dataset \(\) by either modifying the policy evaluation step to regularize Q learning or constraining the policy improvement directly. In the following, we will show that our diffusion policy design is compatible with any offline algorithms and can speed up policy evaluation and improvement.

### Diffusion Models

Consider a real data distribution \(q()\) and a sample \(^{0} q()\) drawn from it. The (forward) diffusion process fixed to a Markov chain gradually adds Gaussian noise to the sample in \(K\) steps, producing a sequence of noisy samples \(^{1},^{K}\). Note that we use superscript \(k\) to denote diffusion timestep to avoid conflicting with the RL timestep. The noise is controlled by a variance schedule \(^{1},,^{K}\):

\[q(^{k}|^{k-1})=(^{k}; }^{k-1},^{k}), q( ^{1:K}|^{0})=_{k=1}^{K}q(^{k}| ^{k-1}).\] (1)

When \(K\), \(^{K}\) distributes as an isotropic Gaussian distribution. Diffusion models learn a conditional distribution \(p_{}(^{t-1}|^{t})\) and generate new samples by reversing the above process:

\[p_{}(^{0:K})=p(^{K})_{k=1}^{K}p_{} (^{k-1}|^{k}), p_{}(^{k-1}| ^{k})=(^{k-1};_{}( ^{k},k),_{}(^{k},k)),\] (2)where \(p(^{K})=(,)\) under the condition that \(_{k=1}^{K}(1-^{k}) 0\). The training is performed by maximizing the evidence lower bound (ELBO):\(_{_{0}}[ p_{}(^{0})]_{q}[ (^{0,K})}{q(^{1:K}|^{0})}]\).

## 4 Efficient Diffusion Policy

In this section, we detail the design of our efficient diffusion policy (EDP). First, we formulate an RL policy with a diffusion model. Second, we present a novel algorithm that can train a diffusion policy efficiently, termed Reinforcement-Guided Diffusion Policy Learning (RGDPL). Then, we generalize the diffusion policy to work with arbitrary offline RL algorithms and compare our EDP with Diffusion-QL to highlight its superiority in efficiency and generality. Finally, we discuss several methods to sample from the diffusion policy during evaluation.

### Diffusion Policy

Following , we use the reverse process of a conditional diffusion model as a parametric policy:

\[_{}(|)=p_{}(^{0:K}|)=p(^{K}) _{k=1}^{K}p_{}(^{k-1}|^{k},),\] (3)

where \(^{K}(,)\). We choose to parameterize \(_{}\) based on Denoising Diffusion Probabilistic Models (DDPM) , which sets \(_{}(^{k},k;)=^{k}\) to fixed time-dependent constants, and constructs the mean \(_{}\) from a noise prediction model as: \(_{}(^{k},k;)=}}(^{k}-}{^{k}}}_{}( ^{k},k;))\), where \(^{k}=1-^{k}\), \(^{k}=_{s=1}^{k}\), and \(_{}\) is a parametric model.

To obtain an action from DDPM, we need to draw samples from \(K\) different Gaussian distributions sequentially, as illustrated in Eqn. (2)-(3). The sampling process can be reformulated as

\[^{k-1}=}}(^{k}-}{ ^{k}}}_{}(^{k},k;))+ },\] (4)

with the reparametrization trick, where \((,)\), \(k\) is the reverse timestep from \(K\) to \(0\).

Similar to DDPM, plugging in the conditional Gaussian distributions, the ELBO in Sec. 3.2 can be simplified to the following training objective \(L_{}()\):

\[_{k,,(^{0},)}[\|- _{}(^{k}}^{0}+^{k}},k;)\|^{2}],\] (5)

where \(k\) follows a uniform distribution over the discrete set \(\{1,,K\}\). It means the expectation is taken over all diffusion steps from clean action to pure noise. Moreover, \((,)\), and \((^{0},)\) are state-action pairs drawn from the offline dataset. Given a dataset, we can easily and efficiently train a diffusion policy in a behavior-cloning manner as we only need to forward and backward through the network once each iteration. As shown in Diffusion-QL , diffusion policies can greatly boost the performance when trained with TD3-based Q learning. However, it still faces two main drawbacks that limit its real-world application: 1) It is inefficient in sampling and training; 2) It is not generalizable to other strong offline reinforcement learning algorithms.

### Reinforcement-Guided Diffusion Policy Learning

To understand how a parametric policy \(_{}\) is trained with offline RL algorithms, we start with a typical Q-learning actor-critic framework for continuous control, which iterates between policy evaluation and policy improvement. Policy evaluation learns a Q network by minimizing the TD error \(L_{}()\):

\[_{(,,^{})}[(r(,)+ Q_{}(^{},^{})-Q_{}(,))^{2}],\] (6)

where the next action \(^{}_{}(|^{})\). The policy is optimized to maximize the expected Q values :

\[_{}_{,_{}(| )}[Q_{}(,)].\] (7)It is straightforward to optimize this objective when a Gaussian policy is used, but things get much more difficult when a diffusion policy is considered due to its complicated sampling process. Instead, we propose to view the offline RL problem from the perspective of generative modeling, where a diffusion policy can be easily learned in a supervised manner from a given dataset. However, unlike in computer vision, where the training data are usually perfect, offline RL datasets often contain suboptimal state-action pairs. Suppose we have a well-trained Q network \(Q_{}\), the question becomes how we can efficiently use \(Q_{}\) to guide diffusion policy training procedure. We now show that this can be achieved without sampling actions from diffusion policies.

Let's revisit the forward diffusion process in Eqn. 1. A notable property of it is that the distribution of noisy action \(^{k}\) at any step \(k\) can be written in closed form: \(q(^{k}|^{0})=(^{k};^{k}}^ {0},(1-^{k}))\). Using the reparametrization trick, we are able to connect \(^{k}\), \(^{0}\) and \(\) by:

\[^{k}=^{k}}^{0}+^{k}},(,).\] (8)

Recall that our diffusion policy is parameterized to predict \(\) with \(_{}(^{k},k;)\). By relaxing \(\) with \(_{}(^{k},k;)\) and rearranging Eqn. (8), we obtain the approximatied action:

\[}^{0}=^{k}}}^{k}-^{k}}}{^{k}}}_{}(^{k}, k;).\] (9)

In this way, instead of running the reverse diffusion process to sample an action \(^{0}\), we can cheaply construct \(}^{0}\) from a state-action pair \((,)\) in the dataset by first corrupting the action \(\) to \(^{k}\) then performing one-step denoising to it. We will refer to this technique as _action approximation_ in the following. Accordingly, the policy improvement for diffusion policies is modified as follows:

\[L_{}()=-_{,}^{0}}[Q_{ }(,}^{0})].\] (10)

To improve the efficiency of policy evaluation, we propose to replace the DDPM sampling in Eqn. (4) with DPM-Solver , which is an ODE-based sampler. The algorithm is defered to the appendix.

### Generalization to Various RL algorithms

There are mainly two types of approaches to realize the objective in Eqn. 7 for policy improvement.

_Direct policy optimization_: It maximizes Q values and directly backpropagate the gradients from Q network to policy network, _i.e._, \(_{}L_{}()=-(,)}{ }}{}\). This is only applicable to cases where \(}{}\) is tractable, _e.g._, when a deterministic policy \(=_{}()\) is used or when the sampling process can be reparameterized. Sample algorithms belonging to this category include TD3 , TD3+BC , and CQL . One can easily verify that both the expensive DDPM sampling in Eqn. (4) and our efficient approximation in Eqn. (9) can be used for direct policy optimization.

_Likelihood-based policy optimization_: It tries to distill the knowledge from the Q network into the policy network indirectly by performing weighted regression or weighted maximum likelihood:

\[_{}_{(,)}[f(Q_{}( ,))_{}(|)],\] (11)

where \(f(Q_{}(,))\) is a monotonically increasing function that assigns a weight to each state-action pair in the dataset. This objective requires the log-likelihood of the policy to be tractable and differentiable. AWR , CRR , and IQL  fall into this category but each has a unique design in terms of the weighting function \(f\). Since the likelihood of samples in Diffusion models is intractable, we propose the following two variants for realizing Eqn. 11.

First, instead of computing the likelihood, we turn to a lower bound for \(_{}(|)\) introduced in DDPM . By discarding the constant term that does not depend on \(\), we can have the objective:

\[_{k,,(,)}[ f(Q_{ }(,))}{2^{k}(1-^{k-1})}\| -_{}(^{k},k;)\|^{2}].\] (12)

Second, instead of directly optimizing \(_{}(|)\), we propose to replace it with an approximated policy \(_{}(|)(}^{0},)\), where \(}^{0}\) is from Eqn. (9). Then, we get the following objective:

\[_{k,,(,)}[f(Q_{}(,)) \|-}^{0}\|^{2}].\] (13)Empirically, we find these two choices perform similarly, but the latter is easier to implement. So we will report results mainly based on the second realization. In our experiments, we consider two offline RL algorithms under this category, _i.e._, CRR, and IQL. They use two weighting schemes: \(f_{}=Q_{}(s,a)-_{a^{} (a|s)}Q(s,a^{})_{}\) and \(f_{}=Q_{}(s,a)-V_{}(s)_{ }\), where \(\) refers to the temperature parameter and \(V_{}(s)\) is an additional value network parameterized by \(\). We defer the details of these two algorithms to Appendix A.

### Comparison to Diffusion-QL

Now we are ready to compare our method and Diffusion-QL comprehensively. Though our EDP shares the same policy parametrization as Diffusion-QL, it differs from Diffusion-QL significantly in the training algorithm. As a result, the computational efficiency and generality of diffusion policies have been improved substantially.

**Efficiency** The diffusion policy affects both policy evaluation (Eqn. (6)) and policy improvement (Eqn. (7)). First, calculating \(L_{}()\) in policy evaluation requires drawing the next action from it. Diffusion-QL uses DDPM sampling while EDP employs a DPM-Solver, which can reduce the sampling steps from 1000 to 15, thus accelerating the training. Second, in policy improvement, Diffusion-QL again applies DDPM for sampling. Then, it calculates the loss function based on sampled actions and backpropagates through the sampling process for network update. This means it needs to forward and backward a neural network for \(K\) times each training iteration. As a result, Diffusion-QL can only work with small \(K\), _e.g._, \(5 100\). In comparison, our training scheme only passes through the network once an iteration, no matter how big \(K\) is. This enables EDP to use a larger \(K\) (1000 in our experiments) to train diffusion policy on the more fine-grained scale. The results in Tab. 1 also show a larger \(K\) can give better performance.

**Generality** Diffusion-QL can only work with direct policy optimization, which contains only a small portion of algorithms. Moreover, thanks to their flexibility and high performance, the likelihood-based algorithms are preferred for some tasks (_e.g._, Antmaze). Our method successfully makes diffusion trainable with any RL algorithm.

### Controlled Sampling from Diffusion Policies

Traditionally, a continuous policy is represented with a state-conditional Gaussian distribution. During evaluation time, a policy executes deterministically to reduce variance by outputting the distribution mean as an action. However, with diffusion policies, we can only randomly draw a sample from the underlying distribution without access to its statistics. As a result, the sampling process is noisy, and the evaluation is of high variance. We consider the following method to reduce variance.

**Energy-based Action Selection (EAS)** Recall that the goal of (offline) RL is to learn a policy that can maximize the cumulative return or values. Though the policy \(_{}\) is stochastic, the learned \(Q_{}\) provides a deterministic critic for action evaluation. We can sample a few actions randomly, then use \(Q_{}\) for selection among them to eliminate randomness. EAS first samples \(N\) actions from \(_{}\) by using any samplers (_i.e._, DPM-Solver), then sample one of them with weights proportional to \(e^{Q(,)}\). This procedure can be understood as sampling from an improved policy \(p(|) e^{Q(,)}_{}(|)\). All results will be reported based on EAS. See Appendix. C.4 for the other two methods.

## 5 Experiments

We conduct extensive experiments on the D4RL benchmark  to verify the following assumptions: 1) Our diffusion policy is much more efficient than the previous one regarding training and evaluation costs. 2) Our diffusion policy is a generic policy class that can be learned through direct and likelihood-based policy learning methods. We also provide various ablation studies on the critical components for better understanding.

BaselinesWe evaluate our method on four domains in D4RL, including Gym-locomotion, AntMaze, Adroit, and Kitchen. For each domain, we consider extensive baselines to provide a thorough evaluation. The simplest method is the classifier behavior cloning (BC) baseline and 10% BC that performs behavior cloning on the best 10% data. TD3+BC  combines off-policy reinforcement learning algorithms with BC. OneStepRL  first conducts policy evaluation to obtain the Q-value of the behavior policy from the offline dataset, then use it for policy improvement. AWAC , AWR , and CRR  improve policy improvement by adding advantage-based weights to policy loss functions. CQL  and IQL  constrain the policy evaluation process by making conservative Q updates or replacing the max operator with expectile regression. We also consider the Decision Transformer (DT)  baseline that maps offline RL as a sequence-to-sequence translation problem.

Experimental SetupWe keep the backbone network architecture the same for all tasks and algorithms, which is a 3-layer MLP (hidden size 256) with Mish  activation function following Diffusion-QL . For the noise prediction network \(_{}(^{k},k;)\) in diffusion policy, we first encode timestep \(k\) with sinusoidal embedding , then concatenate it with the noisy action \(^{k}\) and the conditional state \(\). We use the Adam  to optimize both diffusion policy and the Q networks. The models are trained for 2000 epochs on Gym-locomotion and 1000 epochs on the other three domains. Each epoch consists of 1000 iterations of policy updates with batch size 256. For DPM-Solver , we use the third-order version and set the model call steps to 15. We reimplement DQL strictly following the official PyTorch code  for fair comparison and we refer to DQL (JAX) for all sample efficiency comparisons. We defer the complete list of all hyperparameters to the appendix due to space limits. Throughout this paper, the results are reported by averaging 5 random seeds.

Evaluation ProtocalWe consider two evaluation metrics in this paper. First, _online model selection_ (OMS), proposed by Diffusion-QL , selects the best-performing model throughout the whole training process. However, though OMS can reflect an algorithm's capacity, it is cheating, especially when the training procedure is volatile on some of the tasks. Therefore, we propose another metric to focus on the training stability and quality, which is _running average at training_ (RAT). RAT calculates the running average of evaluation performance for ten consecutive checkpoints during training and reports the last score as the final performance.

### Efficiency and Reproducibility

In this section, we focus on the training and evaluation efficiency of our efficient diffusion policy. We choose the OMS evaluation metric to make a fair comparison with the baseline method Diffusion-QL . We consider four variants of EDP to understand how each component contributes to the high efficiency of our method. 1) EDP is the complete version of our method. It uses the action approximation technique in Eqn. (9) for policy training and uses DPM-Solver for sampling. 2) _EDP w/o DPM_ modifies EDP by replacing DPM-Solver with the original DDPM sampling method in Eqn. 4. 3) _EDP w/o AP_ removes the action approximation technique. 4) DQL (JAX) is our Jax implementation of Diffusion-QL.

We first benchmark and compare the training/evaluation speed of the above three variants and Diffusion-QL. We choose walker2d-medium-expert-v2 as the testbed. For training speed, we run each algorithm for 10,000 iterations of policy updates and calculate the corresponding iterations-per-second (IPS). Similarly, we sample 10,000 transitions by interacting with the environment and calculate the corresponding steps-per-second (SPS) for evaluation speed. Based on the visualization in Fig. 2, by taking DQL (JAX) as the baseline, we are able to attribute the performance boost to specific techniques proposed. Specifically, we can observe that action approximation makes 2.3x training and 3.3x sampling faster, while using the DPM-Solver adds an additional 2.3x training speedup. We can observe that DQL (JAX) is \(5\) faster than Diffusion-QL, which means our Jax implementation is more computationally efficient than Diffusion-QL's PyTorch code. This demonstrates that both the action approximation technique and DPM-Solver play a critical role in making the training of diffusion policy efficient. However, this technique does not affect the sampling procedure; thus, _EDP w/o DPM_ and DQL (JAX) are on par with each other regarding sampling speed.

Figure 2: Training and evaluation speed comparison. The training IPS are: \(4.66,22.30,50.94,38.4\), and \(116.21\). The sampling SPS are: \(18.67,123.70\), \(123.06\), \(411.0\), \(411.79\).

[MISSING_PAGE_FAIL:8]

Evaluation MetricsTo reveal that evaluation metric is important, we train EDP with TD3 algorithms on three selected environments: walker2d-medium-expert-v2, hopper-medium-expert-v2, and antmaze-medium-diverse-v0. We then compare the scores for OMS (best) and RAT (average) by plotting the training curves in Fig. 3. On walker2d, the training is stable; thus, both OMS and RAT scores steadily grow and result in close final scores. A similar trend can be observed on the hopper but with a more significant gap between these two metrics. However, these two metrics diverge significantly when the training succeeds and then crashes on antmaze. Therefore, OMS is misleading and can not give a reliable evaluation of algorithms, which explains the necessity of using RAT in Sec. 5.2.

Energy-Based Action SelectionWe notice that energy-based action selection (EAS) is a general method and can also be used for arbitrary policies. We apply EAS to normal TD3+BC and find no improvement, which shows EAS is only necessary for diffusion sampling. The results are deferred to the Appendix. Moreover, set the number of actions used in EAS from 1 to 200, and report the performance on gym-locomotions tasks in Fig. 4. It shows the normalized score monotonically grows as the number of actions increases on 8 out of 9 tasks. In our main experiments, we set the number of actions to 10 by trading off the performance and computation efficiency.

DPM-SolverWe are using DPM-Solver to speed up the sampling process of diffusion policies. The number of models in DPM-Solver is an important hyper-parameter that affects sampling efficiency and quality. We vary this number from 3 to 30 and compare the performance on gym-locomotion tasks in Fig. 5. We can observe that the performance increases as more steps of model calls are used. The performance gradually plateaus after 15 model calls. Therefore, we use 15 in our main experiments.

## 6 Conclusion

Diffusion policy has emerged as an expressive policy class for offline reinforcement learning. Despite its effectiveness, diffusion policy is limited by two drawbacks, hindering it from wider applications. First, training a diffusion policy requires to forward and backward through a long parameterized Markov chain, which is computationally expensive. Second, the diffusion policy is a restricted policy class that can not work with likelihood-based RL algorithms, which are preferred in many scenarios. We propose efficient diffusion policy (EDP) to address these limitations and make diffusion policies faster, better, and more general. EDP relies on an action approximation to construct actions from corrupted ones, thus avoiding running the Markov chain for action sampling at training. Our benchmarking shows that EDP achieves 25\(\) speedup over Diffusion-QL at training time on the gym-locomotion tasks in D4RL. We conducted extensive experiments by training EDP with various offline RL algorithms, including TD3, CRR, and IQL, the results clearly justify the superiority of diffusion policies over Gaussian policies. As a result, EDP set new state-of-the-art on all four domains in D4RL.

Figure 4: Performance of different number of actions used in EAS. The experiments are conducted on the nine locomotion tasks.

Figure 5: Performance of DPM-Solver with varying steps. The experiments are conducted on the nine locomotion tasks.

Figure 3: Training curves for EDP +TD3 on three representative environments. Average represents RAT, Best represents OMS.