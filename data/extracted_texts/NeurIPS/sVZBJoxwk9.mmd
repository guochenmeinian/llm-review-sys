# Generalized Eigenvalue Problems with

Generative Priors

 Zhaoqiang Liu

University of Electronic Science and Technology of China

{zqliu12, liwenbnu}@gmail.com

&Wen Li

Corresponding author.

University of Electronic Science and Technology of China

{zqliu12, liwenbnu}@gmail.com

&Junren Chen

University of Hong Kong

chenjr58@connect.hku.hk

###### Abstract

Generalized eigenvalue problems (GEPs) find applications in various fields of science and engineering. For example, principal component analysis, Fisher's discriminant analysis, and canonical correlation analysis are specific instances of GEPs and are widely used in statistical data processing. In this work, we study GEPs under generative priors, assuming that the underlying leading generalized eigenvector lies within the range of a Lipschitz continuous generative model. Under appropriate conditions, we show that any optimal solution to the corresponding optimization problems attains the optimal statistical rate. Moreover, from a computational perspective, we propose an iterative algorithm called the Projected Rayleigh Flow Method (PRFM) to approximate the optimal solution. We theoretically demonstrate that under suitable assumptions, PRFM converges linearly to an estimated vector that achieves the optimal statistical rate. Numerical results are provided to demonstrate the effectiveness of the proposed method.

## 1 Introduction

The generalized eigenvalue problem (GEP) plays an important role in various fields of science and engineering . For instance, it underpins numerous machine learning and statistical methods, such as principal component analysis (PCA), Fisher's discriminant analysis (FDA), and canonical correlation analysis (CCA) . In particular, the GEP for a symmetric matrix \(^{n n}\) and a positive definite matrix \(^{n n}\) is defined as

\[_{i}=_{i}_{i}, i=1,2, ,n.\] (1)

Here, \(_{i}\) represent the generalized eigenvalues, and \(_{i}\) denote the corresponding generalized eigenvectors of the matrix pair \((,)\). The eigenvalues are ordered such that \(_{1}_{2}_{n}\). Throughout this paper, we focus on the setting of symmetric semi-definite GEPs.

According to the Rayleigh-Ritz theorem , the leading generalized eigenvector of \((,)\) is the optimal solution to the following optimization problem:

\[_{^{n}}^{} ^{}=1.\] (2)

Or, in an equivalent form with respect to the Rayleigh quotient (up to the transformation of the norm of the optimal solutions):

\[_{^{n}, 0}^{} }{^{}}.\] (3)Subsequent generalized eigenvectors of \((,)\) can be obtained through methods such as generalizations of Rayleigh quotient iteration [56; 19; 66], subspace style iterations [99; 25; 89], and the QZ method [64; 43; 45; 10].

In many practical applications, the matrices \(\) and \(\) may be contaminated by unknown perturbations, and we can only access approximate matrices \(}\) and \(}\) based on \(m\) independent observations. Specifically, we have

\[}=+,}=+ ,\] (4)

where \(\) and \(\) are the perturbation matrices. In addition, in recent years, there has been an increasing interest in studying the GEP in the high-dimensional setting where \(m n\), with the prominent assumption that the leading generalized eigenvector \(_{1}^{n}\) of the uncorrupted matrix pair \((,)\) (_cf._ Eq. (1)) is sparse. This leads to the sparse generalized eigenvalue problem (SGEP), for which we can estimate the underlying signal \(_{1}\) based on the approximate matrices \(}\) and \(}\) by solving the following constrained optimization problem:

\[_{^{n}}^{}} }{^{}}} ^{}} 0,\;\|\|_{0} s,\] (5)

where \(\|\|_{0}=|\{i\,:\,u_{i} 0\}|\) represents the number of non-zero entries of \(\) and \(s\) is a parameter for the sparsity level. As mentioned in , solving the non-convex optimization problem in Eq. (5) is challenging in the high-dimensional setting because \(}\) is singular and not invertible, preventing us from converting the SGEP to the sparse eigenvalue problem and making use of classical algorithms that require taking the inverse of \(}\).

Motivated by the remarkable success of deep generative models in a multitude of real-world applications, a new perspective on high-dimensional inverse problems has recently emerged. In this new perspective, the assumption that the underlying signal can be well-modeled by a pre-trained (deep) generative model replaces the common sparsity assumption. Specifically, in the seminal work , the authors explore linear inverse problems with generative models and provide sample complexity upper bounds for accurate signal recovery. Furthermore, they presented impressive numerical results on natural image datasets, demonstrating that the utilization of generative models can result in substantial reductions in the required number of measurements compared to sparsity-based methods. Numerous subsequent works have built upon , exploring various aspects of inverse problems under generative priors [86; 29; 42; 36; 67; 93; 35; 65].

In this work, we investigate the high-dimensional GEP under generative priors, which we refer to as the generative generalized eigenvalue problem (GGEP). The corresponding optimization problem becomes:

\[_{^{n}}^{}} }{^{}}} ^{}} 0,\;(G),\] (6)

where \((G)\) denotes the range of a pre-trained generative model \(G\,:\,^{k}^{n}\), with the condition that the input dimension \(k\) is much smaller than the output dimension \(n\) to enable accurate recovery of the signal using a small number of measurements or observations.

### Related Work

This subsection provides a summary of existing works related to Sparse Generalized Eigenvalue Problems (SGEP) and inverse problems with generative priors.

**SGEP:** There have been extensive studies aimed at developing algorithms and providing theoretical guarantees for specific instances of SGEP. For instance, sparse principal component analysis (SPCA) is one of the most widely studied instances of SGEP, with numerous notable solvers in the literature, including the truncated power method , Fantope-based convex relaxation method , iterative thresholding approach , regression-type method , and sparse orthogonal iteration pursuit , among others [63; 18; 59; 80; 30; 39; 1; 46; 68; 11; 90]. Additionally, significant developments for sparse FDA and sparse CCA, another two popular instances of SGEP, can be found in various works including [24; 95; 69; 57; 7; 15; 27; 79; 60; 13; 14; 44; 94; 20; 83; 96].

There are also many works that propose general approaches for the SGEP, including the projection-free method proposed in , the majorization-minimization approaches proposed in [82; 81], the decomposition algorithm proposed in , an inverse-free truncated Rayleigh-Ritz method in , the linear programming based sparse estimation method in , among others [76; 62; 84; 40; 8; 31].

Among the works related to SGEP,  is the most relevant to ours. In , the authors proposed a two-stage computational method for SGEP that achieves linear convergence to a point achieving the optimal statistical rate. Their method first computes an initialization vector via convex relaxation, and then refines the initial guess by truncated gradient ascent, with the method for the second stage being referred to as the truncated Rayleigh flow method. Our work generalizes the truncated Rayleigh flow method to the more complex scenario of using generative priors, and we also establish a linear convergence guarantee to an estimated vector with the optimal statistical error.

**Inverse problems with generative priors:** Since the seminal work  that investigated linear inverse problems with generative priors, various high-dimensional inverse problems have been studied using generative models. For example, under the generative modeling assumption, one-bit or more general single-index models have been investigated in [92; 38; 50; 52; 73; 41; 12], spiked matrix models have been investigated in [3; 16; 17], and phase retrieval problems have been investigated in [26; 33; 34; 92; 78; 2; 49; 54].

Among the works that study inverse problems using generative models, the recent work  is the most relevant to ours. Specifically, in , the authors considered generative model-based PCA (GPCA) and proposed a practical projected power (PPower) method. Furthermore, they showed that provided a good initial guess, PPower converges linearly to an estimated vector with the optimal statistical error. However, as pointed out in  for the case of sparse priors, due to the singular matrix \(}\) of the GEP, the corresponding methods for PCA generally cannot be directly applied to solve GEPs. Therefore, our work, which provides a unified treatment to GEP under generative priors, broadens the scope of .

A detailed discussion on the technical novelty of this work compared to  and  is provided in Section 2.2.

### Contributions

The main contributions of this paper can be summarized as follows:

* We provide theoretical guarantees regarding the optimal solutions to the GGEP in Eq. (6). Particularly, we demonstrate that under appropriate conditions, the distance between any optimal solution to Eq. (6) and the underlying signal is roughly of order \(O()\), assuming that the generative model \(G\) is \(L\)-Lipschitz continuous with bounded \(k\)-dimensional inputs. Such a statistical rate is naturally conjectured to be optimal based on the information-theoretic lower bound established in  for the simpler GPCA problem.
* We propose an iterative approach to approximately solve the non-convex optimization problem in Eq. (6), which we refer to as the projected Rayleigh flow method (PRFM). We show that PRFM converges linearly to a point achieving the optimal statistical rate under suitable assumptions.
* We have conducted simple proof-of-concept experiments to demonstrate the effectiveness of the proposed projected Rayleigh flow method.

### Notation

We use upper and lower case boldface letters to denote matrices and vectors, respectively. We write \([N]=\{1,2,,N\}\) for a positive integer \(N\), and we use \(_{N}\) to denote the identity matrix in \(^{N N}\). A _generative model_ is a function \(G\,:\,^{n}\), with latent dimension \(k\), ambient dimension \(n\), and input domain \(^{k}\). We focus on the setting where \(k n\). For a set \(S^{k}\) and a generative model \(G\,:\,^{k}^{n}\), we write \(G(S)=\{G()\,:\, S\}\). We define the radius-\(r\) ball in \(^{k}\) as \(B^{k}(r):=\{^{k}\,:\|\|_{2} r\}\). In addition, we use \((G):=G(B^{k}(r))\) to denote the range of \(G\). We use \(\|\|_{2 2}\) to denote the spectral norm of a matrix \(\). We use \(_{}()\) and \(_{}()\) to denote the minimum and maximum eigenvalues of \(\) respectively. \(^{n-1}:=\{^{n}:\|\|_{2}=1\}\) represents the unit sphere in \(^{n}\). The symbols \(C\), \(C^{}\), and \(C^{}\) are absolute constants whose values may differ from line to line. We use standard Landau symbols for asymptotic notations, with the 

[MISSING_PAGE_FAIL:4]

_Remark 2.5_.: Assumption 2.4 is closely related to classic assumptions about the Crawford number of the symmetric-definite matrix pair \((,)\)[84; 8]. More specifically, we have

\[(,):=_{^{n-1}}^{})^{2}+(^{} )^{2}}_{}()>0.\] (8)

Additionally, based on the \(L\)-Lipschitz continuity of \(G\) and [87, Lemma 5.2], for any \(>0\), there exists a \(\)-net \(M^{}\) of \((G)\) such that \(|M^{}| k\). Note that \(M^{}(G)^{n-1}\). Then, if

\[(M^{}):=,M^{})^{2}+(,M ^{})^{2}},\] (9)

with

\[(,M^{}):=_{_{1} M^{},_{ 2} M^{}}|_{1}^{}_{2}|,( ,M^{}):=_{_{1} M^{},_{2} M ^{}}|_{1}^{}_{2}|,\] (10)

it follows from Assumption 2.4 that

\[(M^{}) 2C}{m}}.\] (11)

Therefore, if \(m=k\) with a sufficiently large implied constant, the conditions that

\[)}{(,)} c (,M^{}) c^{}_{}( )\] (12)

naturally hold, where \(c,c^{}\) are certain positive constants.2 This leads to an assumption similar to [84, Assumption 1].

### Discussion on the Technical Novelty Compared to  and 

Our analysis builds on techniques from existing works such as  and , but these techniques are combined and extended in a non-trivial manner; we highlight some examples as follows:

* We present a recovery guarantee regarding the global optimal solutions of GGEP in Theorem 3.1. Such guarantees have not been provided for SGEP in . Although in [51, Theorem 1], the authors established a recovery guarantee regarding the global optimal solutions of GPCA, its proof is significantly simpler than ours and does not involve handling singular \(}\) or using the property of projection as in Eq. (78) in our proof.
* The convergence guarantee for Rifle in  hinges on the generalized eigenvalue decomposition of \((}_{F},}_{F})\), where \(F\) is a superset of the support of the underlying sparse signal, along with their Lemma 4, which follows directly from [98, Lemma 12] and characterizes the error induced by the truncation step. Additionally, the convergence guarantee of PPower in  only involves bounding the term related to the sample covariance matrix \(\) (as seen in their Eq. (83)). In contrast, in our proof of Theorem 3.3, we make use of the generalized eigenvalue decomposition of \((,)\), and we require the employment of significantly distinct techniques to manage the projection step (and bound a term involving both \(}\) and \(}\), see our Eq. (111)). This is evidenced in the three auxiliary lemmas we introduced in Appendix B.1 and the appropriate manipulation of the first and second terms on the right-hand side of Eq. (114).
* Unlike in GPCA studied in , where the underlying signal can be readily assumed to be a unit vector that is (approximately) within the range of the normalized generative model, in the formulation of the optimization problem for GGEP (refer to Eq. (6) and the corresponding analysis, we need to carefully address the distinct normalization requirements of the generative model and \(}\).

**Input**: \(}\), \(}\), \(G\), number of iterations \(T\), step size \(>0\), initial vector \(_{0}\)

**for**\(t=0,1,,T-1\)**do**

\[_{t} =_{t}^{}}_{t}}{ _{t}^{}}_{t}},\] (14) \[_{t+1} =_{G}(_{t}+(}-_ {t}})_{t}),\] (15)

**end for Output**: \(_{T}\)

**Algorithm 1** Projected Rayleigh Flow Method (PRFM)

## 3 Main Results

Firstly, we present the following theorem, which pertains to the recovery guarantees in relation to the globally optimal solution of the GCEP in Eq. (6). Similar to [51, Theorem 1], Theorem 3.1 can be easily extended to the case where there is representation error, i.e., the underlying signal \(^{*}(G)\). Here, we focus on the case where \(^{*}(G)\) (_cf._ Assumption 2.3) to avoid non-essential complications. The proof of Theorem 3.1 is deferred to Appendix B.

**Theorem 3.1**.: _Suppose that Assumptions 2.1, 2.3, and 2.4 hold for the GEP and generative model \(G\). Let \(}\) be a globally optimal solution to Eq. (6) for GCEP. Then, for any \((0,1)\) satisfying \(=O(k)/n\), when \(m=k\), we have_

\[\{\|}-^{*}\|_{2},\|}+^{* }\|_{2}\} C_{1}}{m}},\] (13)

_where \(C_{1}\) is a positive constant depending on \(\) and \(\)._

As noted in , an \(\)-layer neural network generative model is typically \(L\)-Lipschitz continuous with \(L=n^{()}\). Then, under the typical scaling of \(L=n^{()}\), \(r=n^{()}\), and \(=1/n^{()}\), the upper bound in Eq. (13) is of order \(O(),\) which is naturally conjectured to be optimal based on the algorithm-independent lower bound provided in  for the simpler GPCA problem.

Although Theorem 3.1 demonstrates that the estimator for the GCEP in Eq. (6) achieves the optimal statistical rate, in general, the optimization problem is highly non-convex, and obtaining the optimal solution is not feasible. To address this issue, we propose an iterative approach that can be regarded as a generative counterpart of the truncated Rayleigh flow method proposed in . This approach is used to find an estimated vector that approximates a globally optimal solution to Eq. (6). The corresponding algorithm, which we refer to as the projected Rayleigh flow method (PRFM), is presented in Algorithm 1.

In the iterative process of Algorithm 1, the following steps are performed:

* Calculate \(_{t}\) in Eq. (14) to approximate the largest generalized eigenvalue \(_{1}\). Note that from Lemma B.1 in Appendix B.1, we obtain \(_{t}^{}}_{t}>0\) for \(t>0\) under appropriate conditions.
* In Eq. 15, we perform a gradient ascent operation and a projection operation onto the range of the generative model, where \(_{G}()\) denotes the projection function. This step is essentially analogous to the corresponding step in [84, Algorithm 1]. However, instead of seeking the support for the sparse signal, our aim is to project onto the range of the generative model. Furthermore, we adopt a simpler choice for the step size \(\) in the gradient ascent operation.3

_Remark 3.2_.: Specifically, for any \(^{n}\), \(_{G}()_{(G)}\|-\|_{2}\). We will assume implicitly that the projection step can be performed accurately, as in , for the convenience of theoretical analysis. In practice, however, approximate methods might be necessary, such as gradient descent  or GAN-based projection methods .

We establish the following convergence guarantee for Algorithm 1. The proof of Theorem 3.3 can be found in Appendix C.

**Theorem 3.3**.: _Suppose that Assumptions 2.1, 2.3, and 2.4 hold for the GEP and generative model \(G\). Let \(_{1}=(_{1}-_{2})_{}()\) and \(_{2}=(_{1}-_{n})_{}()\). Suppose that_

\[_{1}+_{2}<2,\] (16)

_and \(_{0}:=_{0}^{}^{*}>0\) satisfies the condition that_

\[+)c_{0}}}{1-c_{0}}<1,\] (17)

_where_

\[b_{0}=(2-(_{1}+_{2}))+_{1}(2()-(1+_{0}))+ 3_{2}())}, c_{0}= -_{1}}{2}.\] (18)

_Then, for any \((0,1)\) satisfying \(=O(k)/n\), when \(m=k\), there exists a positive integer \(T_{0}=O}\), such that the sequence \(\{\|_{t}-^{*}\|_{2}\}_{t T_{0}}\) is monotonically decreasing, with the following inequality holds for all \(t T_{0}\):_

\[\|_{t}-^{*}\|_{2}(+)c_{ 0}}}{1-c_{0}})^{t}\|_{0}-^{*}\|_{2}+C_{2} {}{m}},\] (19)

_where \(C_{2}\) is a positive constant depending on \(\), \(\), and \(\). Additionally, we have for all \(t T_{0}\) that_

\[\|_{t}-^{*}\|_{2} C_{2}}{m}}.\] (20)

Theorem 3.3 establishes the conditions under which Algorithm 1 converges linearly to a point that achieves the statistical rate of order \(O()\).

_Remark 3.4_.: Both inequalities in Eqs. (16) and (17) can be satisfied under appropriate conditions. More specifically, first, under suitable conditions on the underlying matrix pair \((,)\) and the step size \(\), the condition in (16) can hold. Moreover, using of the inequality that \(2 1\) for any \(c[0,1)\), we obtain that when

\[+1}{2(1-c_{0})}<1,\] (21)

or equivalently,

\[_{2}+3_{1}-2_{1}(2()-(1+_{0}))-6_{ 2}())}>3,\] (22)

the condition in Eq. (17) holds. Then, for example, when \(():=_{}()/_{}()\) is close to \(1\), and \(_{0}\) is close to \(1\), (22) can be approximately simplified as

\[_{2}+3_{1}>3.\] (23)

Note that both the conditions \(_{1}+_{2}<2\) (in Eq. (16)) and \(_{2}+3_{1}>3\) can be satisfied for appropriate \(_{1}\) and \(_{2}\) (under suitable conditions for \(\), \(\), and \(\)), say \(_{1}=\) and \(_{2}=1.1\).

_Remark 3.5_.: In certain practical scenarios, we may assume that the data only contains non-negative vectors, for example, in the case of image datasets. Additionally, during pre-training, we can set the activation function of the final layer of the neural network generative model to be a non-negative function, such as ReLU or sigmoid, further restricting the range of the generative model to the non-negative orthant. Therefore, the assumption that \(_{0}:=_{0}^{}^{*}>0\) is mild (in experiments, we simply set the initial vector \(_{0}\) to be \(_{0}=[1,1,,1]^{}/^{n}\)), and similar assumptions have been made in prior works such as . As a result, we provide an upper bound on \(\|_{t}-^{*}\|_{2}\) instead of on \(\{\|_{t}-^{*}\|_{2},\|_{t}+^{*}\| _{2}\}\).

Experiments

In this section, we conduct proof-of-concept numerical experiments on the MNIST dataset  to showcase the effectiveness of the proposed Algorithm 1. Additional results for MNIST and CelebA  are provided in Appendices D and E.4 We note that these experiments are intended as a basic proof of concept rather than an exhaustive study, as our contributions are primarily theoretical in nature.

### Experiment Setup

The MNIST dataset consists of \(60,000\) images of handwritten digits, each measuring \(28 28\) pixels, resulting in an ambient dimension of \(n=784\). We choose a pre-trained variational autoencoder (VAE) model as the generative model \(G\) for the MNIST dataset, with a latent dimension of \(k=20\). Both the encoder and decoder of the VAE are fully connected neural networks with two hidden layers, having an architecture of \(20-500-500-784\). The VAE is trained using the Adam optimizer with a mini-batch size of 100 and a learning rate of 0.001 on the original MNIST training set. To approximately perform the projection step \(_{G}()\), we use a gradient descent method with the Adam optimizer, with a step size of \(100\) and a learning rate of \(0.1\). This approximation method has been used in several previous works, including [77; 71; 50; 51]. The reconstruction task is evaluated on a random subset of \(10\) images drawn from the testing set of the MNIST dataset, which is unseen by the pre-trained generative model.

We compare our Algorithm 1 (denoted as PRFM) with the projected power method (denoted as PPower) proposed in  and the Rifle method (e.g., denoted as Rifle20 when the cardinality parameter is set to \(20\)) proposed in .

To evaluate the performance of different algorithms, we employ the Cosine Similarity metric, which is calculated as \(CosSim(^{*},})=}^{} ^{*}\). Here, \(^{*}\) is the target signal that is contained in the unit sphere, and \(}\) denotes the normalized output vector of each algorithm. To mitigate the effect of local minima, we perform \(10\) random restarts and select the best result from these restarts. The average Cosine Similarity is calculated over the \(10\) test images and the \(10\) restarts. All experiments are carried out using Python 3.10.6 and PyTorch 2.0.0, with an NVIDIA RTX 3060 Laptop 6GB GPU.

### Results for GGEPs

Firstly, we adhere to the experimental setup employed in , where the underlying matrices \(\) and \(\) are set to be \(=4^{*}(^{*})^{}+_{n}\) and \(=_{n}\) respectively. We sample \(m\) data points following \((,)\), and another \(m\) data points following \((,)\). The approximate matrices \(}\) and \(}\) are constructed based on the sample covariances correspondingly. Specifically, we set

\[}=_{i=1}^{m}(2_{i}^{*}+_{i})(2_{i}^{*}+_{i})^{},}= _{i=1}^{m}_{i}_{i}^{}.\] (24)

Here, \(_{i}\) are independently and identically distributed (i.i.d.) realizations of the standard normal distribution \((0,1)\), \(_{i}\) are i.i.d. realizations of \((,_{n})\), and \(_{i}\) are also i.i.d. realizations of \((,_{n})\). \(_{i}\), \(_{i}\), and \(_{i}\) are independently generated. The leading generalized eigenvector \(^{*}\) is set to be the normalized version of the test image vector. Note that in this case, the generalized eigenvalues are \(_{1}=5\) and \(_{2}==_{n}=1\). To ensure that the conditions in Eqs. (16) and (23) are both satisfied, we set the step size \(\) for PRFM to be \(=\). The step size \(^{}\) for Rifle is set to \(35/32\) such that \(^{}/_{t-1} 7/32=\). For all the methods, the initialization vector \(_{0}\) for is set to be the normalized vector of all ones, namely \(_{0}=[1,1,,1]^{}/^{n}\), which naturally guarantees that \(_{0}=_{0}^{}^{*}>0\) since the image vectors in the MNIST dataset contain only non-negative entries. For PPower, the input matrix set to be \(}\) (ignoring the fact that \(}\) is not exactly the identity matrix; see [51, Algorithm 1]). We vary the number of measurements \(m\) in \(\{100,150,200,250,300,350\}\).

Figure 1(a) show the reconstructed images with different numbers of measurements. Additionally, Figure 2(a) presents the quantitative comparison results based on the Cosine Similarity metric. From

these figures, we can see that when the number of measurements \(m\) is relatively small compared to the ambient dimension \(n\), sparsity-based methods Rifle20 and Rifle100 lead to poor reconstructions, and PPower and PRFM can achieve reasonably good reconstructions. Moreover, PRFM generally yields significantly better results than PPower. This is not surprising as PPower is not compatible with the case where \(}\) is not the identity matrix.

Next, we also investigate the case where

\[}=_{i=1}^{m}y_{i}_{i}_{i}^{ },}=_{i=1}^{m}_{i}_ {i}^{},\] (25)

where \(_{i}^{n}\) are independent standard Gaussian vectors and \(y_{i}=(_{i}^{}^{*})^{2}\). The generation of \(}\) corresponds to the phase retrieval model. Other experimental settings remain the same as those in the previous case where \(}\) and \(}\) are generated from Eq. (24). The corresponding numerical results are presented in Figures 1(b) and 2(b). From these figures, we can see that PRFM also leads to best reconstructions in this case.

## 5 Conclusion and Future Work

GEP encompasses numerous significant eigenvalue problems, motivating this paper's examination of GEP using generative priors, referred to as Generative GEP and GGEP for brevity. Specifically, we assume that the desired signal lies within the range of a specific pre-trained Lipschitz generative model. Unlike prior works that typically addressed high-dimensional settings through sparsity, the generative prior enables the accurate characterization of more intricate signals. We have demonstrated that the exact solver of GGEP attains the optimal statistical rate. Furthermore, we have devised a computational method that converges linearly to a point achieving the optimal rate under appropriate assumptions. Experimental results are provided to demonstrate the efficacy of our method.

Figure 1: Reconstructed images of the MNIST dataset for \((},})\) generated from Eqs. (24) and (25).

Figure 2: Quantitative results of the performance of the methods on MNIST.

In the current work, for the sake of theoretical analysis, we adhere to the assumption made in prior works like  that the projection onto the range of the generative model can be executed accurately. However, in experiments, this projection step can only be approximated, consuming a substantial portion of the running time of our proposed method. Developing highly efficient projection methods with theoretical guarantees is of both theoretical and practical interest. Another intriguing area for future research is to provide guarantees for estimating multiple generalized eigenvectors (or the subspace they span) under generative modeling assumptions.