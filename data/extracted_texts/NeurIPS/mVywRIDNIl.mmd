# Reining Generalization in Offline Reinforcement Learning via Representation Distinction

Yi Ma

College of Intelligence and Computing

Tianjin University

mayi@tju.edu.cn

&Hongyao Tang

Universite de Montreal

Mila

tang.hongyao@mila.quebec

&Dong Li

Noah's Ark Lab, Huawei Technology

dongleecsu@gmail.com

&Zhaopeng Meng

College of Intelligence and Computing

Tianjin University

mengzp@tju.edu.cn

Corresponding author.

###### Abstract

Offline Reinforcement Learning (RL) aims to address the challenge of distribution shift between the dataset and the learned policy, where the value of out-of-distribution (OOD) data may be erroneously estimated due to overgeneralization. It has been observed that a considerable portion of the benefits derived from the conservative terms designed by existing offline RL approaches originates from their impact on the learned representation. This observation prompts us to scrutinize the learning dynamics of offline RL, formalize the process of generalization, and delve into the prevalent overgeneralization issue in offline RL. We then investigate the potential to rein the generalization from the representation perspective to enhance offline RL. Finally, we present Representation Distinction (RD), an innovative plug-in method for improving offline RL algorithm performance by explicitly differentiating between the representations of in-sample and OOD state-action pairs generated by the learning policy. Considering scenarios in which the learning policy mirrors the behavioral policy and similar samples may be erroneously distinguished, we suggest a dynamic adjustment mechanism for RD based on an OOD data generator to prevent data representation collapse and further enhance policy performance. We demonstrate the efficacy of our approach by applying RD to designed backbone algorithms and widely-used offline RL algorithms. The proposed RD method significantly improves their performance across various continuous control tasks on D4RL datasets, surpassing several state-of-the-art offline RL algorithms.

## 1 Introduction

Reinforcement learning (RL) is a machine learning paradigm centered on training intelligent agents to make decisions through interactions with an environment. In this learning process, an agent develops a policy--a mapping from states to actions--via trial and error, with the goal of maximizing cumulative rewards over time. However, acquiring interaction data in many real-world applications can be expensive, time-consuming, or even hazardous. This spurred the advancement of offline Reinforcement Learning, which has garnered considerable interest in the research community due to its ability to learn effective policies from pre-existing data collected by unknown behavioral policywithout necessitating online interactions with the environment. Offline RL can result in substantial savings in resources, time, and the risk associated with online exploration.

Despite its appealing potential, offline RL encounters challenges in learning optimal policies from limited and suboptimal datasets. The primary challenge in offline RL is the distribution shift between the dataset and the learned policy. As the agent updates its policy, it may come across out-of-distribution (OOD) state-action pairs absent from the limited support of the fixed offline dataset . Conventional RL algorithms, such as SAC  or TD3 , could yield overly optimistic Q-value estimations for these unseen state-action pairs due to overgeneralization, leading to catastrophic performance. Existing methods in offline RL seek to address this challenge by incorporating techniques like policy constraint [4; 5; 6; 7], conservative value estimates, [8; 9], and uncertainty estimation [10; 11; 12].

Recently,  elucidated that the ability of existing offline reinforcement learning (RL) methods to handle OOD actions is significantly attributed to their impact on the learned representations of state-action pairs. The learned representation can capture the underlying structure and essential aspects of the state-action space, enabling the agent to identify similarities and patterns across various state-action pairs, thus improving agent's learning efficacy and generalization capacity. Although there are a few works  also study representation learning in offline RL, they focus on how different kinds of pretrained representation matter in downstream policy learning, neglecting the understanding of the essential interplay between representation and policy during the offline co-learning process.

In this paper, we first present a view dubbed Backup-Generalization Cycle to gain a tangible comprehension of how a typical offline RL algorithm learns. We subsequently formalize how generalization happens, and discuss the overgeneralization issue prevalent in offline RL. Finally, we introduce a novel method called Representation Distinction (RD) to mitigate overgeneralization from in-sample state-action pairs to OOD ones. Specifically, we distinctly differentiate the data sourced from the dataset and the data generated by the learning policy from the representation perspective. In cases where the learning policy's performance aligns with the behavioral policy and the generated data largely overlaps with the dataset, we devise an OOD data generator to produce data with lower Q-values than the current policy. We then gradually shift our focus from differentiating in-sample data and data generated by the learning policy to differentiating data generated by the learning policy and data generated by the OOD actor in the representation space, preventing similar samples from being erroneously distinguished.

To demonstrate the efficacy of our proposed RD method in enhancing offline RL policy performance by reining the generalization, we apply it to two backbone algorithms, which are modified versions of the original SAC and TD3 algorithms by incorporating an ensemble of critics and an uncertainty regularization term in the policy update process. By integrating RD into these simple algorithms, our proposed method RD can improve the performance of the backbone agents on D4RL datasets across various continuous control tasks and outperforms several state-of-the-art offline RL algorithms. We also apply RD on two widely-used baselines TD3BC and CQL and improve their performance significantly. In-depth analysis based on visualization and statistical results that demonstrates the efficacy of RD in obtaining more differentiable representation compared to its variants is also provided.

The main contributions of this work can be summarized as follows:

* We introduce a view called Backup-Generalization Cycle to foster an understanding of typical offline value function learning, and highlights the necessity of reining generalization to enhance offline RL.
* We proposed a practical plug-in method Representation Distinction (RD) from the perspective of representation to enhance the performance of offline RL methods by inhibiting overgeneralization among state-action pairs sourced from different distributions.
* We evaluate the effectiveness of RD by applying it to designed backbone algorithms and existing widely-used algorithms and enhance their performance significantly.

## 2 Related Works

Offline RLOffline RL algorithms strive to train RL agents using pre-collected datasets. Nevertheless, the distribution shift between the behavior policy and the policy being learned may lead to issues, as OOD actions are sampled from the learned policy and incorporated into the learned critic. To address this challenge, various approaches have been proposed. Some earlier methods aim to constrain the learned policy to remain close to the behavior policy, which can be accomplished through explicit policy regularization [4; 5; 6], implicit policy constraints [15; 16; 17; 18; 19], or by employing auxiliary behavioral cloning losses . Alternative approaches penalize the Q-value of OOD actions to discourage their selection [8; 9; 10; 11; 12]. Moreover, model-based methods that train with conservative penalties have been suggested [20; 21; 22; 23; 24; 25].

Representation in Offline RL.Prior research has aimed to analyze various aspects of the representations induced by TD-based methods using function approximation, predominantly in the standard online RL setting [26; 27; 28; 29; 30]. More recently, this line of inquiry has emerged in the offline RL setting [31; 32; 33; 34; 13].  investigates which representations can result in stable convergence of TD in a linear setting. [32; 33] explore the learning dynamics of Q-learning in an overparameterized setting, observing excessively low-rank and aliased feature representations at the fixed points identified by TD-learning.  studies the extent of impact of different interventions on the causal link between the effective rank and offline RL agent performance and points out that there is no strong relationship between them.  highlights that a substantial portion of the benefits of existing offline RL approaches, which aim to avoid OOD actions, actually originates from their effects on the learned representations. The authors also identify specific metrics that facilitate effective evaluation of the quality of the learned representation.  pretrain the representation using different auxiliary loss and then fix the representation to apply it to downstream policy learning. Nevertheless, a thorough examination of the explicit representation distinction between in-sample and OOD state-action pairs in the context of offline RL is lacking. Our approach promotes orthogonality between the representation vectors of in-sample and OOD data, offering a more practical solution for inhibiting overgeneralization, thus enhancing offline RL.

## 3 Preliminary

Markov Decision Process (MDP) is a mathematical framework that models decision-making processes in stochastic environments. It is defined by a tuple \((S,A,P,R,)\), where \(S\) is the set of states, \(A\) is the set of actions, \(P\) is the state transition probability function, \(R\) is the reward function, and \(\) is the discount factor. Q-learning is the major approach for obtaining the optimal \((s)\), which learns a Q-value function \(Q(s,a)\) that represents the expected cumulative discounted rewards when starting from the state \(s\) taking the action \(a\) and executing the policy \(\) thereafter. the Q-value function is evaluated by iterating the Bellman operator as \(Q(s,a)=_{s^{} P(|s,a)}[r(s,a)+ _{a^{}(s^{})}Q(s^{},a^{})]\). The goal of RL is to find an optimal policy \((s)\) that maximizes the cumulative discounted rewards.

Offline Reinforcement Learning , aims to learn a policy from a fixed dataset of interaction samples, without further interaction with the environment. The dataset \(\) consists of transition tuples \((s,a,r,s^{})\) collected from interactions between one or multiple behavior policies and the environment. The goal is to learn an optimal policy using only this dataset.

To deal with large state-action space, deep RL resorts to deep neural networks for function approximation. Typically, a Q-network parameterized by \(\) can be viewed as \(Q_{}(s,a)=^{}(s,a)\), where \((s,a)^{d}\) is regarded as the representation obtained by the penultimate layer of Q-network while \(^{d}\) is the linear weight [35; 36; 33; 13]. In conventional online and offline RL learning process, the representation and the linear weight learn together. A few recent works[37; 38; 39; 40] also introduce auxiliary objectives to strengthen or regularize the representation with different purposes. We use this view for the presentation of our analysis and proposed method in the following sections.

## 4 Reining Generalization in Offline RL

In this section, we focus on the learning dynamics of offline RL. First, we present a view called Backup-Generalization Cycle (Sec. 4.1) to gain a tangible comprehension of how a typical offline RL algorithm learns. We then take a further step to formalize how generalization happens, and discuss the overgeneralization issue in offline RL (Sec. 4.2). Finally, we propose the idea of kernel control to address the overgeneralization issue (Sec. 4.3), with the practical implementation deferred to Sec. 5.

### Backup-Generalization Cycle in Offline RL

Offline RL learns from a static offline dataset with no access to online interaction over the course of policy learning. To gain a better understanding of the intricacies of offline deep RL, for the first, we introduce a view called Backup-Generalization Cycle. This view, as depicted in Fig. 1, fosters an understanding of typical offline value function learning via two key components: **Backup** and **Generalization**:

* Typical offline deep RL algorithms employ various forms of **backups** to learn the \(Q\)-function network. Here we use \(C()\) to denote a generic form of the target value such as Clipped Double \(Q\)-Learning . It is notable that only the values of state-action pairs in the offline dataset are _directly_ updated.
* Since the values of all state-action pairs that are not in the dataset will never be directly updated, changes to the values of such \((s,a)\) are solely _indirectly_ instigated by the backups on \((s,a)\), i.e., through **generalization**. Note that the state-action pairs for the target values of backup are highly likely to be absent from the dataset, i.e., \((s^{},(s^{}))\), as the learning of current policy \(\).

Hence, the typical offline learning process of value function can be regarded as a consequence of the complex interplay between backup and generalization. This dynamic interplay forms a cycle: (1) the backups on \((s,a)\) consistently influence the values of \((s,a)\) (highly likely including \((s^{},(s^{}))\)); (2) the consistently changing \(Q(s^{},(s^{}))\) participates in the backups on \((s,a)\); the two kinds of dynamics iterate and twine during the learning process.

There are also some similar opinions mentioned in a few recent works on offline RL [41; 29]. We utilize the Backup-Generalization Cycle to distinctly separate and better analyze these two contributing factors. In the next subsection, we concentrate on the overgeneralization issue with a formal characterization of value generalization among state-action pairs.

### Overgeneralization in Offline RL

According to the Backup-Generalization Cycle presented above, generalization plays a significant role for \((s,a)\). In contrast to backup, which is relatively explicit and controllable, generalization is implicit and intricate. In the following of this subsection, we analyze value generalization among state-action pairs during the offline RL learning process.

For a starting case, we consider how the \(Q\) function update caused by typical Temporal-Difference (TD) learning on a single state-action pair \((s,a)\) (denoted as \(^{}\)), affects the \(Q\)-value of an arbitrary state-action pair \((,)\). The post-update parameter \(^{}\) can be formalized as follows:

\[^{}=+(Q_{}(s,a)-Q_{}(s,a)) _{}Q_{}(s,a),\] (1)

where learning rate is omitted for convenience. We further formalize the post-update \(Q\)-value of \((,)\) by Taylor expansion at the pre-update parameter \(\):

\[Q_{^{}}(,)=Q_{}(,)+_{}Q_ {}(,)^{}(^{}-)+ (\|^{}-\|^{2}),\] (2)

Now, we can characterize the generalization from \((s,a)\) to \((,)\) by plugging Eq.1 into Eq.2:

\[Q_{^{}}(,)=Q_{}(,)+k_{}(,,s,a)(Q_{}(s,a)-Q_{}(s,a))+ (\|^{}-\|^{2})\] (3)

Figure 1: An illustration of Backup-Generalization Cycle. \(s_{},s_{0},s_{},s_{}\) denotes four states in the offline dataset \(\). Backups and generalizations are denoted by blue and red arrows respectively.

where \(k_{}(,,s,a)_{}Q_{}(,)^{} _{}Q_{}(s,a)\) and it is called Neural Tangent Kernel in . Eq.3 elucidates the change of the \(Q\)-value of any state-action pair \((,)\) can be mainly characterized by the kernel \(k_{}(,,s,a)\) and the TD-error \(Q_{}(s,a)-Q_{}(s,a)\). Apparently, the kernel controls the extent of generalization: for small values of \(k_{}(,,s,a)\), the generalization effect is minor and the classic tabular learning (i.e., no generalization) can be viewed as a special case when \(k_{}\) and the higher-order term are 0; for large values of \(k_{}(,,s,a)\), the generalization effect is prominent and \(Q_{^{}}(,)\) can be largely changed in either the same or the opposite direction of TD-error. Intriguingly, Eq. 3 indicates that we can control the generalization by mainly adjusting the kernel \(k_{}(,,s,a)\).

It is widely deemed that the crux of offline RL is the overestimation of OOD data, i.e., \((s,a)\). Recall the Backup-Generalization Cycle in Fig. 1. The overestimation propagates directly via backups and further spreads via generalization. Most existing works on offline RL focus on eliminating or suppressing overestimation by conservative value estimation [8; 10] or distribution constraint [4; 5], leading to various forms of \(C()\). These works can be categorized as overestimation-control backups. However, since the source of overestimation is improper generalization, i.e., **overgeneralization** issue, we argue that it is inadequate to only cut the _downstream_ while leave the _upstream_ uncontrolled. Naturally, the generalization should also be carefully controlled as a preposed complement to the overestimation-control backups. Therefore, we take a further step and propose a novel method to control the generalization in the next subsection.

### Reining Generalization via Kernel Control

Since the control of generalization (i.e., the _upstream_) is neglected in most previous offline RL works, we propose a novel method to address the overgeneralization issue by controlling the kernel in Eq. 3. The key idea is a two-stage generalization control: Policy-Dataset Generalization Inhibition and Policy-OOD Generalization Inhibition. Fig. 2 illustrates the concept of the two stages and an intermediate scene between them, which are detailed below.

As illustrated in the left of Fig. 1, the first stage of generalization control is to inhibit the generalization from behavior policy distribution (or dataset) to learning policy distribution, i.e., suppress \(Q(s,a)}Q(s,(s))\) for \((s,a)\). Such kind of generalization is called _extrapolation_ in the offline RL literature . Extrapolation is a main component of generalization as its consequence \(Q(s,(s))\) consistently participates in the backups. Thus, overestimation caused by improper extrapolation severely handicaps the learning. For a stable and effective learning process, we argue that extrapolation should be inhibited, especially at the early learning stage, where the learning policy is usually largely different with the behavior policy. This can be achieved by encouraging the kernel to 0, i.e., \(_{}|_{}Q_{}(s,a)^{}_{}Q_{}(s,(s))|\). Consequently, training on \((s,a)\) induces only a minor change to \(Q_{}(s,(s))\).

With effective suppression of overestimation achieved by the first stage of generalization inhibition, we further consider an intermediate scene where the learning policy evolves and resembles the behavioral policy as the training proceeds, leading to a distribution overlap. In this scene, the effect of the first stage could become over-inhibition. In another word, \(_{}|_{}Q_{}(s,a)^{}_{}Q_{}(s,(s))|\) for \(a(s)\) could lead to a sharp and bumpy \(Q\)-value landscape as shown in the middle of Fig. 2. It adversely impacts the robustness of \(Q\) function, further resulting in a brittle policy. Hence, it's unwise

Figure 2: A conceptual illustration of Reining Generalization via Kernel Control.

to impose the Policy-Dataset Generalization Inhibition all along the learning process. For some empirical evidence of this, we observe performance decay in some of our experiments in Table 3 and Fig.4. A natural remedy is to gradually remove the inhibition. However, it leaves value generalization uncontrolled once again, exposing the policy learning under the risk of collapse due to overestimation.

To address the dilemma above, we introduce the second stage, Policy-OOD Generalization Inhibition, as illustrated in the right of Fig. 2. In this stage, we make use of an additional policy \(_{}\) that outputs _OOD actions_. The OOD action here is defined as an action with a lower \(Q\)-value than the action selected by the learning policy for the same state. The actual effect of the second stage is suppression of the generalization between the learning policy distribution and OOD policy distribution. Such a means of kernel control can be viewed as a special way of increasing the action gap . It improves the robustness of value function learning by discouraging erroneous generalization on suboptimal actions. Formally, this is achieved by \(_{}|_{}Q_{}(s,(s))^{}_{}Q_{}(s, _{}(s))|\).

Overall, our kernel control method consists of the two stages of generalization inhibition, which effectively controls the generalization over the entire course of learning, providing a favorable _upstream_ for better offline RL.

## 5 Representation Distinction in Offline RL

In this section, we devise a practical algorithm to rein the generalization in offline RL following the design of two stages of generalization inhibition. A heuristic method to achieve smooth transition among them is also proposed. We term our algorithm of reining generalization as Representation Distinction (RD) and apply them to both existing and designed backbone algorithms.

### Practical Algorithm Design

We first focus on the phase depicted in the left of Fig.2. A natural approach is to directly minimize \(}_{s,a} k_{}(s,(s),s,a)\) across the dataset. However, due to the high dimensionality of \(_{}Q_{}(s,a)\), the direct computation of \(}_{s,a} k_{}(s,(s),s,a)\) involves the computation and backpropagation through per-example gradient dot products, which is computationally prohibitive. To mitigate this, we adopt the method proposed by  and approximate \(()\) with the contribution solely from the last layer parameters, i.e., \(}_{s,a}_{}Q_{}(s, (s))^{}_{}Q_{}(s,a)\). Consequently, we derive the following loss function:

\[_{1}=}_{s,a}(s,(s)) ^{}(s,a)\] (4)

where \((s,a)\) signifies the representation of state-action pairs as mentioned in Section 3. By minimizing \(_{1}\), we encourage the learned Q-function to yield representations that are as orthogonal as possible between data from dataset and \(\). In this manner, the representations of \((s,a)\) and \((s,(s))\) can be distinctly discerned.

To substantiate the efficacy of inhibiting generalization between in-sample data and data derived from the learning policy \(\), we conduct rudimentary experiments by directly minimizing \(_{1}\) during the training of the standard off-policy SAC algorithm  on both halfcheeta-medium and halfcheeta-medium-expert. The evaluation results depicted in Fig.3 demonstrate that enhancing the representation alone, devoid of explicit pessimism, can achieve commendable performance, substantially surpassing the baseline SAC. These outcomes underline the potency of inhibiting generalization to eliminate overestimation in offline setting.

Figure 3: Average results of SAC over 5 seeds with and w/o RD.

[MISSING_PAGE_EMPTY:7]

the addition of two steps to the conventional policy updating scheme, as illustrated in Algorithm 1 in the appendix. In each training iteration, the OOD data generator is first trained to generate data with lower Q-values than the current policy. Then, the Q-function is updated by minimizing the loss function \(=*_{}+_{}\), and the policy is updated based on \(_{}\). Here, \(\) represents a weighted factor, and \(_{}\) and \(_{}\) refer to the optimization objectives of any offline RL algorithm. A psedutocode using RD in offline RL is given in Algorothm 1. In the experimental section, we demonstrate the versatility of RD by integrating it into both backbone algorithms and existing offline RL algorithms.

## 6 Experiments

We evaluate RD by applying it on TD3BC , CQL , SAC-N-Unc and TD3-N-Unc through a series of experiments on D4RL  gym MuJoCo-v2 and Adroit-v1 datasets, where the former is a dataset commonly used in previous work for continuous control tasks, and the latter poses a significant challenge for most offline RL methods due to its sparse reward property. We compare our method with BC and several model-free offline RL algorithms, including DT , TD3BC , CQL , IQL , EDAC , and Diffusion-QL . We obtain the results of the baselines by re-running the official codes or directly extracting them from the original papers. In our experiments, all the re-run baselines and our algorithm are executed with five random seeds, and we report the average normalized results of the final ten evaluations. We report the performance at 1M gradient step for TD3BC and CQL and that at 3M gradient step for SAC-N-Unc and TD3-N-Unc on MuJoCo tasks. For Adroit tasks, we report results at 500K gradient step. Note that due to the space limitation, we abbreviate the names of the datasets from {RANDOM, MEDIUM, MEDIUM-REPLAY, MEDIUM-EXPERT, EXPERT} to {R, M, MR, ME, E} in all the tables.

### Main Results

   Dataset & **TD3-N-Unc** & **TD3-N-Unc** & **SC-N-Unc** & **SAC-N-Unc** & **TD3BC** & **TD3BC** & **CQL** & **CQL** \\  & **+RD** & **+RD** & **+RD** & **+RD** & **+RD** & **+RD** & **+RD** \\  HalfcCheetai-m & \(66.8 0.5\) & \(\) & \(65.9 1.0\) & \(\) & \(48.0 0.3\) & \(\) & \(47.1 0.2\) & \(\) \\ HalfcCheetai-m & \(53.4 3.9\) & \(\) & \(53.2 5.4\) & \(\) & \(44.6 0.3\) & \(44.6 0.5\) & \(45.2 0.6\) & \(\) \\ HalfcCheetai-m & \(97.7 2.2\) & \(\) & \(99.4 2.5\) & \(\) & \(90.5 6.6\) & \(\) & \(81.1 0.6\) & \(\) \\  Hopper-m & \(41.9 5.05\) & \(\) & \(45.7 0.1\) & \(\) & \(60.4 4.0\) & \(\) & \(\) & \(\) \\ Hopper-m & \(92.5 1.81\) & \(\) & \(104.7 0.9\) & \(104.6 0.4\) & \(61.2 0.5\) & \(\) & \(87.7 1.4\) & \(14.0 3.2\) \\ Hopper-m & \(100.3 2.26\) & \(\) & \(110.9 0.12\) & \(110.6 0.3\) & \(105.6 4.1\) & \(101.8 2.8\) & \(93.9 14.3\) & \(\) \\  WalkER2-m & \(69.3 35.2\) & \(\) & \(24.2 2.8\) & \(\) & \(82.7 5.5\) & \(\) & \(80.4 3.5\) & \(\) \\ walkER2D-m & \(91.6 2.7\) & \(\) & \(85.2 2.7\) & \(\) & \(82.1 2.5\) & \(\) & \(79.2 5.0\) & \(\) \\ walkER2D-m & \(90.6 45.0\) & \(\) & \(113.1 9.6\) & \(\) & \(110.2 0.5\) & \(110.1 0.5\) & \(109.7 0.5\) & \(\) \\   

Table 1: Results of different algorithms and the ones equipped with RD

   Dataset & BC & DT & TD3BC & CQL & IQL & EDAC & Diffusion-QL &   &   & 
  \\  HalfcCheetai-n & \(2.2\) & \(2.2\) & \(11.0\) & \(31.3\) & \(13.7\) & \(28.4\) & \(22.0\) & \(25.4\) & \(31.0\) \\ Hopper-R & \(3.7\) & \(5.4\) & \(8.4\) & \(5.3\) & \(8.4\) & \(25.3\) & \(18.3\) & \(\) & \(\) \\ WalkER2D-R & \(1.3\) & \(2.2\) & \(1.7\) & \(5.4\) & \(5.9\) & \(16.6\) & \(5.5\) & \(\) & \(

To demonstrate the applicability of RD, we integrate RD into the training regime of TD3BC, CQL, TD3-N-Unc and SAC-N-Unc. As observed from Table 1, both TD3BC and CQL exhibit improved overall performance when RD is applied. It is noteworthy as the core idea of CQL to increase the Q value of the data in the dataset and diminish the Q value of the data generated by a mixed policy comprising the learning policy and random policy, which incorporates the representation distinction of state-action pairs sourced from different distributions. Therefore in practice, we apply the insights of RD to gradually transfer the original Q value restriction to the Q-value differentiation between the mixed policy and the designed OOD policy. As a result, CQL's performance is significantly improved. Moreover, the backbone algorithms themselves yield substantial advantages from the application of RD. Table 2 further reveals that backbone algorithms equipped with RD can achieve the best overall performance and attains state-of-the-art performance on several datasets. Additional evidence supporting the efficacy of RD in enhancing convergence speed and its potential to reduce the quantity of Q ensembles is provided in the appendix.

### Comparison and discussion with techniques that inhibit overestimation

We compare our method RD with the existing techniques that inhibit potential overestimation including DR3  and Layer Norm . In addition to the regularizer, all the other parameters are kept the same across different methods to ensure fairness. As shown in Table 3, RD is more helpful in helping improving the backbone algorithm than DR3 and Layer Norm.

RD and DR3 differ mainly from two perspectives. From the angle of the framework derived from, DR3 is derived from the theoretical characterizing implicit regularization in TD-Learning, which is a generalization of the implicit regularization from Supervised Learning [52; 53] to TD-Learning in RL setting. In contrast, RD is derived from the proposed Backup-Generalization framework. From the angle of regularization effect, DR3 is proposed to directly counter the implicit regularization of TD-Learning while RD is to suppress the generalization between in-sample data and OOD data. With some heuristics, DR3 regularizer arrives at a similar form, i.e., to minimize the NTK between consecutive state-action pairs in backup, to RD regularizer. Apparently, DR3 regularizer can be a special case of RD regularizer in terms of the definition of out-of-sample actions. The regularization effect of DR3 is similar to the Policy-Dataset Generalization Inhibition shown by Figure 2. Such a regularization can induce over-inhibition of generalization when the distribution of current policy overlaps with the offline dataset as illustrated. We also show in the following that DR3 and Policy-Dataset Generalization Inhibition (i.e., PDD in the following part) achieve similar empirical results.

### Ablation Studies and In-depth Analysis

We design ablation experiments to elucidate the significance of RD's components. We compare RD against several variants including, including Policy-Dataset Distinction (PDD), Random-Dataset Distinction (RDD), Policy-OOD Distinction (POD), and Policy-Dataset Distinction with Dynamically Adjusted Weight (PDDDAW). PDD refers to the auxiliary representation loss calculated using data generated by the learning policy and that from the dataset. Likewise, RDD utilizes data generated by a random policy versus that from the dataset, while POD employs data produced by the learning policy against that by the OOD actor. As for PDDDAW, its PDD component is reweighted using the same heuristic \(w\) adjustment method as RD, but the POD component is persistently weighted by zero. We draw a comparison between TD3-N-Unc + RD and these variants, as tabulated in Table 3. The results demonstrate that RD, fundamentally a dynamic amalgamation of PDD and POD, attains superior overall performance, underscoring the indispensability of each RD component.

   Dataset &  TD3-N-Unc \\ **+PDD** \\  &  **TD3-N-Unc** \\ **+PDD** \\  &  **TD3-N-Unc** \\ **+RDD** \\  &  **TD3-N-Unc** \\ **+RDD** \\  &  **TD3-N-Unc** \\ **+DD3** \\  &  **TD3-N-Unc** \\ **+LVarNorm** \\  &  **TD3-N-Unc** \\ **+RDD** \\  \\   hindependent-m \\ hopper-m \\ walker2d-m \\  & \(66.8 0.5\) & \(66.1 0.9\) & \(66.6 0.7\) & \(65.9 0.8\) & \(63.1 2.4\) & \(64.4 1.7\) & \(63.2 0.8\) & \(\) \\   hindependent-m \\ hopper-m \\ walker2d-m \\  & \(69.9 50.5\) & \(99.9 6.4\) & \(100.7 6.2\) & \(77.4 38.6\) & \(82.8 40.5\) & \(\) & \(83.0 29.0\) & \(103.0 0.8\) \\   hindependent-m \\ hopper-m \\ walker2d-m \\  & \(69.9 35.2\) & \(94.9 17.7\) & \(66.7 38.1\) & \(80.2 31.9\) & \(92.4 0.7\) & \(92.1 2.0\) & \(65.8 20.7\) & \(\) \\   hindependent-m \\ hopper-m \\ walker2d-m \\  & \(94.6 11.5\) & \(96.3 13.2\) & \(102.9 1.2\) & \(99.9 4.5\) & \(93.7 13.0\) & \(100.0 3.7\) & \(\) & \(103.1 0.6\) \\   hindependent-m \\ hopper-m \\ walker2d-m \\  & \(110.9 0.5\) & \(108.4 0.4\) & \(108.4 0.4\) & \(108.2 0.7\) & \(108.0 0.5\) & \(88.4 42.8\) & \(108.8 0.3\) \\  
 hindependent-m \\ hopper-m \\ walker2d-m \\  & \(4.1 7.1\) & \(67.4 47.2\) & \(60.8 46.1\) & \(28.5 40.8\) & \(44.4 54.0\) & \(109.9 0.4\) & \(11.7 0.4\) & \(\) \\   

Table 3: Average normalized scores of TD3-N-Unc with RD or other variants, and DR3 and Layer Norm that inhibit potential overestimation It is worth noting that using PDD can help backbone algorithms achieve relatively satisfactory performance. However, as depicted in Fig 4, algorithm using PDD occasionally suffers performance degradation during the later stages of training, consequently impairing the overall efficacy of the approach. To elucidate this phenomenon, the final models trained via RD and PDD, as shown in Fig 4, are saved and the quality of the representations obtained by these models are compared. A total of 100 state-action pairs are sampled from the halfcheetah-expert, halfcheetah-medium, and halfcheetah-random datasets, respectively, and then consolidated. Using t-Distributed Stochastic Neighbor Embedding (t-SNE), the distribution of the representations of these 300 state-action pairs derived from both models is charted. Each sample is marked with distinctive symbols (stars for random data, squares for medium data, and circles for expert data) to differentiate samples from diverse datasets. Moreover, varying colors are assigned to individual samples corresponding to the Q-value estimated by the proficiently trained Q-value network acquired by RD, with brighter colors signifying higher Q-values. As depicted in Fig 5 and 6, the RD-derived representation exhibits superior differentiation between state-action pairs with high and low Q-values, whereas the PDD-derived representation frequently misclassifies expert and medium data.

To better demonstrate that the representations learned by RD are superior, we calculate which dataset the top five nearest samples to each sample belong to, and compute the mean over all samples from the same dataset in Table 4. For instance, the EXPERT-EXPERT metric of representation learned via RD indicates that 94% of the five nearest samples to the expert samples are also expert samples. We can conclude that representation learned via RD can better help samples from the same dataset to cluster closer. More results are provided in the appendix.

## 7 Conclusions and Limitations

This paper analyze the important role of generalization in offline RL and presents a novel plug-in method, Representation Distinction (RD), to enhance the performance of offline RL algorithms by reining the generalization. By explicitly differentiating between in-sample and OOD state-action pairs, the generalization across data could be properly inhibited. Our extensive experiments demonstrate the efficacy of the RD method when applied to several offline RL algorithms, significantly enhancing their performance across various continuous control tasks on D4RL datasets. In conclusion, our work contributes a valuable perspective to the field of offline RL by focusing on reining generalization from the representation perspective and providing a flexible solution that can be applied to a variety of existing algorithms.

However, our work is not without limitations. For example, currently RD primarily focus on continuous control tasks, and its applicability to discrete control tasks is yet to be explored. In addition, the transition between the two phases of RD is a heuristic method, which could be improved by designing adaptive adjustment methods. Future research directions may include addressing the aforementioned limitation.