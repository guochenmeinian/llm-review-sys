# Resfusion: Denoising Diffusion Probabilistic Models for Image Restoration Based on Prior Residual Noise

Resfusion: Denoising Diffusion Probabilistic Models for Image Restoration Based on Prior Residual Noise

Zhenning Shi \({}^{1}\)  Haoshuai Zheng\({}^{1}\)  Chen Xu\({}^{1}\)  Changsheng Dong\({}^{1}\)  Bin Pan\({}^{2}\)

Xueshuo Xie\({}^{3}\)  Along He\({}^{1}\)  Tao Li\({}^{1,3}\)  Huazhu Fu\({}^{4}\)

\({}^{1}\)College of Computer Science, Nankai University

\({}^{2}\)School of Statistics and Data Science, Nankai University

\({}^{3}\)Haihe Lab of ITAI

\({}^{4}\)Institute of High Performance Computing, A*STAR

Corresponding authors: litao@nankai.edu.cn, healong2020@163.com

###### Abstract

Recently, research on denoising diffusion models has expanded its application to the field of image restoration. Traditional diffusion-based image restoration methods utilize degraded images as conditional input to effectively guide the reverse generation process, without modifying the original denoising diffusion process. However, since the degraded images already include low-frequency information, starting from Gaussian white noise will result in increased sampling steps. We propose Resfusion, a general framework that incorporates the residual term into the diffusion forward process, starting the reverse process directly from the noisy degraded images. The form of our inference process is consistent with the DDPM. We introduced a weighted residual noise, named **resnoise**, as the prediction target and explicitly provide the quantitative relationship between the residual term and the noise term in resnoise. By leveraging a smooth equivalence transformation, Resfusion determine the optimal acceleration step and maintains the integrity of existing noise schedules, unifying the training and inference processes. The experimental results demonstrate that Resfusion exhibits competitive performance on ISTD dataset, LOL dataset and Raindrop dataset with only **five** sampling steps. Furthermore, Resfusion can be easily applied to image generation and emerges with strong versatility. Our code and model are available at [https://github.com/nkicsl/Resfusion](https://github.com/nkicsl/Resfusion).

## 1 Introduction

Denoising diffusion models  have emerged as powerful and effective conditional generative models, demonstrating remarkable success in synthesizing high-fidelity data for image generation. Saharia et al.  proved that these generative processes can be applied to image restoration by feeding degraded images as conditional input into the score network. SNIPS  combines annealed Langevin dynamics and Newton's method to arrive at a posterior sampling algorithm, exploring the generative diffusion processes to solve the general linear inverse problems. Based on these, many diffusion-based models were adapted for downstream image restoration tasks . For traditional diffusion-based models, the reverse process begins with Gaussian white noise, considering only the degraded images as the conditional input. This results in an increased number of sampling steps. Image restoration tasks often focus on restoring and editing specific high-frequency details while preserving crucial low-frequency information, such as the image structure. The degraded images used as conditional input inherently contain the low-frequency information. Therefore, initiating the reverse process from Gaussian white noise for image restoration tasks appears unnecessary and inefficient.

Consequently, some works have proposed to generate clean images directly from degraded images or noisy degraded images. InDI  restores clean images through the reverse process of direct iteration to degraded images; DDRM  reformulate the image restoration tasks as inverse problems when the mapping between clean and degraded images is available; IR-SDE  directly models the image degradation process using mean-reverting SDE (Stochastic Differential Equations); I\({}^{2}\)SB  constructs a Schrodinger bridge between clean and degraded data distributions; Resshift  shifts the residual term from degraded low-resolution images to high-resolution images, performing the recovery in the latent space. Liu et al.  introduced the Residual Denoising Diffusion Models (RDDM), generalizing the diffusion process of InDI and I\({}^{2}\)SB. RDDM points out that co-learning the residual term and the noise term can effectively improve the model performance. However, RDDM has some limitations. Firstly, RDDM predicts the residual term and the noise term separately, without explicitly specifying their quantitative relationship. Secondly, due to its forward process adopting an accumulation strategy for the residual term and the noise term, its forward and reverse processes are inconsistent with the DDPM , which results in poor generalization and interpretability. Thirdly, RDDM requires the design of a complex noise schedule, as utilizing existing noise schedules would result in performance loss.

To solve the problems mentioned above, we propose **Resfusion**, a general framework for image restoration and can be easily expand to image generation. By introducing the residual term into the diffusion forward process, we bridge the gap between the input image and the ground truth, starting the reverse process directly from the noisy degraded images. We calculate the quantitative relationship between the residual term and the noise term, naming their weighted sum as the resonise. Through the smooth equivalence transformation, we determine the optimal acceleration step and unify the training and inference processes. As a versatile methodology for image restoration, Ressfusion does not require any physical prior knowledge. Resfusion allows for the direct use of existing noise schedules, and the image restoration process can be completed in just five sampling steps.

Our contributions can be summarized as follows:

* First, by introducing the residual term into the diffusion forward process, our **resnoise-diffusion** process starts the reverse process directly from the noisy degraded images, closing the gap between the degraded input and the ground truth.
* Second, we explicitly provide the quantitative relationship between the residual term and the noise term in the loss function, and name the weighted residual noise as **resnoise**. Through transforming the learning of the noise term into the resonise term, the form of our reverse inference process is consistent with the DDPM.
* Third, through the **smooth equivalence transformation** in resonise-diffusion process, we determine the optimal acceleration step and unify the training and inference processes. The optimal acceleration step is non-trivial where the posterior probability distribution is

Figure 1: The proposed Resfusion is a general framework for image restoration and can be easily expand to image generation (setting \(_{0}=0\)). We introduce the residual term (\(R=_{0}-x_{0}\)) into the forward process, redefine \(q(x_{t}|x_{t-1})\) to \(q(x_{t}|x_{t-1},R)\) (as shown by the _orange_ arrow), and name this diffusion process as resonise diffusion. Through employing a novel technique called ”smooth equivalence transformation”, we can directly use the degraded image \(_{0}\) to obtain \(x_{T^{}}\) (as shown by the _blue_ arrow). We bridge the gap between the input image and ground truth, unifying the training and inference processes.

equivalent to the prior probability distribution at this step. Moreover, we can directly use the existing noise schedule instead of redesigning the noise schedule.

## 2 Methodology

### Learning the resonise

First, we denote the input degraded image and the ground truth as \(_{0}\) and \(x_{0}\). In order to extend the diffusion process of Denoising Diffusion Probabilistic Models (DDPM)  to image restoration, we define the residual term as Eq. (1).

\[R=_{0}-x_{0} \]

Then the forward process can be defined as Eq. (2) and Eq. (3). We provide a detailed explanation for why we introduce the residual term \(R\) to Eq. (3) in this way in Sec. 2.2 and Figure 2. Consistent with the definition in DDPM, we use the notation \(_{t}=1-_{t}\) and \(_{t}=_{s=1}^{t}_{s}\).

\[q(x_{1:T}|x_{0},R)=_{t=1}^{T}q(x_{t}|x_{t-1},R) \]

\[q(x_{t}|x_{t-1},R)=N(x_{t};}x_{t-1}+(1-})R,(1- _{t})I) \]

Then the redefined forward process can be formalized as Eq. (4), where \(\) represents the Gaussian white noise.

\[x_{t}=}x_{t-1}+(1-})R+} , N(0,I) \]

According to Eq. (4), \(x_{t}\) can be reparameterized as Eq. (5).

\[x_{t}=_{t}}x_{0}+(1-_{t}})R+ _{t}}, N(0,I) \]

We can easily incorporate this forward process into the vanilla DDPM. We introduce a residual noise, named as **resnoise** (symbolized as \(res\)), to describe the gap between the current estimate \(x_{t}\) and the ground truth \(x_{0}\), and the term to be minimized can be formulated as Eq. (6). Detailed proof can be found in Appendix A.1.

\[res=+})_{t}} }{_{t}}R,_{x_{0},,t}[||res-res_{ }(x_{t},t)||^{2}] \]

Through this process, we transform the learning of \(_{}(x_{t},t)\) into \(res_{}(x_{t},t)\). \(_{}(x_{t},t)\) represents the noise of the noisy ground truth, while \(res_{}(x_{t},t)\) represents the residual noise between the input degraded images and the ground truth. We name this process **resnoise-diffusion**.

### Smooth equivalence transformation

According to Eq. (5) and Eq. (1), we can derive Eq. (7). It is worth mentioning that \(x_{T}\) is uncomputable because the ground truth \(x_{0}\) is unavailable in Eq. (7) during the reverse process, so we can not initialize \(x_{T}\) directly.

\[x_{t}=(2_{t}}-1)x_{0}+(1-_{t}}) _{0}+_{t}}, N(0,I) \]

Fortunately, the weighted coefficient of \(x_{0}\), which is \((2_{t}}-1)\) in Eq. (7), can be very close to zero. Since the input degraded image \(_{0}\) is available, we can find a time step \(T^{}\) where \(x_{T^{}}\) is computable. When \(T\) is sufficiently large, the variation of \(_{t}}(t T)\) with respect to time \(t\) is smooth. We call this technique **smooth equivalence transformation**. Therefore, we can derive \(T^{}\) as Eq. (8) and obtain \(x_{T^{}}\) in Eq. (9) with a small bias. This bias can also be eliminated through the Truncated Schedule technique that we propose next.

\[T^{}=_{i=1}^{T}|_{i}}-| \]

\[x_{T^{}}(1-_{T^{}}})_{0}+ _{T^{}}}_{T^{}}}_{0}+_{T^{}}} \]

Thus we only need to minimize Eq. (6) when \(t T^{}\) since \(x_{T^{}}\) is available, as shown in Eq. (10).

\[P_{}(x_{0})=_{x_{1}:x_{T^{}}}P_{data}(x_{T^{}})_{t=1 }^{T^{}}P_{}(x_{t-1}|x_{t})dx_{1}:x_{T^{}} \]The resonise-diffusion reverse process can be formulated as Eq. (11) and Eq. (12). Consistent with the definition in DDPM, the \(_{}\) is taken fixed as \(_{t}=_{t-1}}{1-_{t}} _{t}\).

\[P_{}(x_{0:T^{}-1}|x_{T^{}})=_{t=1}^{T^{}}P_{ }(x_{t-1}|x_{t}) \]

\[P_{}(x_{t-1}|x_{t})=N(x_{t-1};_{}(x_{t},t),_{}(x_{t},t)), P_{}(x_{0}|x_{1})=N(x_{0};_{}(x_{1},1)) \]

The mean \(_{}(x_{t},t)\) of resonise-diffusion reverse process can be formalized as Eq. (13), which is demonstrated in Appendix A.1 from Eq. (19) to Eq. (23).

\[_{}(x_{t},t)=}}(x_{t}-}{ {1-_{t}}}res_{}) \]

Similar to previous work [5; 17], our method enhances the diffusion model by incorporating a conditioning function. This function integrates latent representation from both the current estimate \(x_{t}\) and the input degraded image \(_{0}\). Then Eq. (6) can be modified to Eq. (14).

\[_{x_{0},,t}[||res-res_{}((x_{t}, _{0},t)||^{2}] \]

Just like the vanilla DDPM, Resfusion gradually fit the current estimate \(x_{t}\) to the ground truth \(x_{0}\), implicitly reducing the residual term \(R\) between \(_{0}\) and \(x_{0}\) with the resonise. When we define \(_{t}\) as Eq. (15) with the same Gaussian noise \(\) in \(x_{t}\), \(_{T^{}}\) can be seen as an intermediate result in an implicit DDPM process with the input degraded image \(_{0}\) as the target distribution. We essentially quantitatively computed the accelerated step \(T^{}\), on which step \(x_{T^{}}\) is closed to \(_{T^{}}\). When T is large enough, the approximate equal sign will become an equal sign. The implicit DDPM reverse process (\(\) to \(_{0}\)) is deterministic because \(_{0}\) is available during the inference. The determination of step \(T^{}\) corresponds to the point where the posterior probability distribution (resnoise-diffusion reverse process) becomes consistent with the prior probability distribution (implicit DDPM reverse process).

\[_{t}=_{t}}_{0}+ _{t}},_{T^{}}} 1-_{T^{}}}, x_{T^{}} _{T^{}} \]

As shown in Fig. 2, the resonise-diffusion reverse process (\(R+\) to \(x_{0}\)) intersects with implicit DDPM reverse process (\(\) to \(_{0}\)), this intersection corresponds to step \(T^{}\). The intersection of two

Figure 2: The working principle of Resfusion. \(x_{0}\) represents the distribution of the ground truth, while \(_{0}\) represents the distribution of the degraded images. \(_{0}-x_{0}\) represents the gap between them, defined as the residual term \(R\) in Eq. (1). Resfusion does not explicitly guide \(_{0}\) to \(x_{0}\). Instead, it implicitly learns the distribution of \(R\) by doing resonise-diffusion reverse process from \(x_{t}\) to \(x_{0}\). The resonise-diffusion reverse process can be imagined as doing diffusion reverse process from \(R+\) to \(x_{0}\) (as shown by the _violet_ arrow), guiding \(x_{t}\) gradually towards \(x_{0}\) along this direction. Following the principles of similar triangles, the coefficient of \(R\) at step \(t\) is computed as \(1-_{t}}\). At any step \(t\) during the training process, \(x_{t}\) can be calculated based on \(x_{0}\) and \(R\) through Eq. (4).

diagonals of a parallelogram is the midpoint of them (corresponding to \(0.5\) in Eq. (11)), but due to the discrete nature of the diffusion process, the acceleration point actually falls on the point closest to the intersection, which is step \(T^{}\) as Eq. (8). Meanwhile, since \(x_{T^{}}\) is available, resonise-diffusion steps after step \(T^{}\) are not necessary according to Eq. (10). Therefore, Resfusion can directly start from step \(T^{}\) for both inference and training process. Because the \(\) coefficient of resonise-diffusion is exactly the same as vanilla DDPM, Resfusion can directly use any existing noise schedule. In practical implementation, we utilized a technique called **Truncated Schedule** to control the offset between \(x_{T^{}}\) and \(_{T^{}}\) when \(T\) is small. Further details can be found in Appendix A.7.

## 3 Experiments

To verify the performance of Resfusion, we conducted experiments on three image restoration tasks, including shadow removal, low-light enhancement and deraining. For fair comparisons, we use an U-net  structure which is the **same** as RDDM as the backbone. We simply concatenate \(x_{t}\) and \(_{0}\) in the channel dimension and feed them into the network. For all the tasks, we only employ one U-net to predict resonise. Furthermore, we simply utilize a truncated linear schedule  and perform only **five** sampling steps for all datasets. The experimental setting details are provided in the Appendix A.4.

**ISTD dataset ** is a dataset designed for shadow removal, comprising 1870 sets of image triplets consisting of shadow image, shadow mask, and shadow-free image. It consists of 1330 image triplets for training and 540 image triplets for quantitative evaluations. We compare the proposed method with the popular shadow removal methods, i.e., ST-CGAN , DSC , ARGAN , DHAN ,

    &  \\   &  &  &  &  &  \\  & & & PSNR \(\) & SSIM & MAE \(\) & PSNR \(\) & SSIM \(\) & MAE \(\) & PSNR \(\) & SSIM \(\) & MAE \(\) \\    } & Input Image & - & 22.40 & 0.936 & 32.11 & 27.32 & 0.976 & 6.83 & 20.56 & 0.893 & 10.97 \\  & ST-CGAN  & 31.8M & 33.74 & 0.981 & 9.99 & 29.51 & 0.958 & 6.05 & 27.44 & 0.929 & 6.65 \\  & DSC  & 22.3M & 34.64 & 0.984 & 8.72 & 31.26 & 0.969 & 5.04 & 29.00 & 0.944 & 5.59 \\  & DHAN  & 21.8M & 35.53 & 0.988 & 7.49 & 31.05 & 0.971 & 5.30 & 29.11 & 0.954 & 5.66 \\  & FusionNet  & 186.5M & 34.71 & 0.975 & 7.91 & 28.61 & 0.880 & 5.51 & 27.19 & 0.945 & 5.88 \\  & UnfoldingNet  & 10.1M & 36.65 & 0.987 & 8.29 & 31.54 & 0.978 & 4.55 & 29.85 & 0.960 & 5.09 \\  & DMTN  & 22.8M & 35.83 & 0.990 & 7.00 & 33.01 & 0.979 & 4.28 & 30.42 & 0.965 & 4.72 \\  & RDDM (SM-Res-N)  & 15.5M & 36.74 & 0.988 & 6.67 & 33.18 & **0.979** & **4.27** & 30.91 & 0.962 & **4.67** \\  & **Resfusion (ours)** & **7.7M** & **37.51** & **0.990** & **6.49** & **34.26** & 0.978 & 4.48 & **31.81** & **0.965** & 4.81 \\    } & Input Image & - & 22.34 & 0.935 & 33.23 & 26.45 & 0.947 & 7.25 & 20.33 & 0.874 & 11.35 \\  & ARGAN  & - & - & - & 9.21 & - & - & 6.27 & - & - & 6.63 \\  & DHAN  & 21.8M & 34.79 & 0.983 & 8.13 & 29.54 & 0.941 & 5.94 & 27.88 & 0.921 & 6.29 \\  & CAVNet  & 358.2M & - & - & - & 8.86 & - & - & 6.07 & - & - & 6.15 \\  & **Resfusion (ours)** & **7.7M** & **36.45** & **0.985** & **7.08** & **32.08** & **0.950** & **5.02** & **30.09** & **0.932** & **5.34** \\   

Table 1: Quantitative comparisons with other shadow removal methods. We report PSNR, SSIM  and MAE in the shadow region (S), the non-shadow region (NS) and all image (ALL). The best and second-best results are highlighted in **bold** and underlined. “\(\)” (resp. “\(\)”) means the larger (resp. smaller), the better. We use the symbol “-” to indicate models or results that are unavailable.

Figure 3: Visual comparisons of the restored results by different shadow-removal methods on the ISTD dataset.

FusionNet , CANet , UnfoldingNet , DMTN , and RDDM(SM-Res-N) . In order to ensure a fair comparison, we conducted experiments on two settings for the ISTD dataset, following the methods used in DMTN  and DHAN : (1) The results are evaluated at a resolution of \(256 256\) after being resized. (2) The original image resolutions (\(640 480\)) are maintained for evaluation.

**LOL dataset ** comprises 500 pairs of images, consisting of both low-light and normal-light versions, which are further divided into 485 training pairs and 15 evaluation pairs. The low-light images contain noise produced during the photo capture process. We compare the proposed method with the popular low-light enhancement methods, i.e., RetinexNet , KinD , KinD++ , Zero-DCE , EnlightenGAN , Restormer , LLFormer , RDDM (SM-Res-N) , and RDDM (SM-Res) . Some existing methods  calculate metrics by adjusting the overall brightness based on reference images (called as using GT-mean). However, this approach can introduce biases and potential unfairness. In accordance with LLFormer , we compute metrics without utilizing any reference information. To ensure a fair comparison, we conducted experiments on two settings for the LOL dataset, following the methods employed in RDDM  and LLFormer : (1) The results are evaluated at a resolution of \(256 256\) after being resized. PSNR and SSIM are evaluated based in YCbCr color space. (2) The original image resolutions (\(600 400\)) are maintained for evaluation. PSNR and SSIM are evaluated in RGB color space.

**Raindrop dataset ** is a dataset designed for deraining, comprising 861 training image pairs for training, and 58 image pairs dedicated for quantitative evaluations, denoted in  and  as Raindrop-A. We compare the proposed method with the popular deraining methods, i.e., pix2pix ,

    \\  Method & Params & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\   \\  Input Image & - & 9.30 & 0.377 & 0.513 \\ RDDM (SM-Res-N)  & 15.5M & 23.90 & 0.931 & 0.116 \\ RDDM (SM-Res)  & 2.7M & 25.99 & 0.937 & 0.116 \\
**Resfusion (ours)** & **7.7M** & **30.02** & **0.954** & **0.070** \\   \\  Input Image & - & 7.77 & 0.191 & 0.560 \\ RetinexNet  & 0.6M & 16.77 & 0.560 & 0.474 \\ KinD  & 8.0M & 20.87 & 0.790 & 0.170 \\ KinD++  & 9.6M & 21.30 & 0.820 & 0.160 \\ Zero-DCE  & **0.3M** & 14.86 & 0.562 & 0.335 \\ EnlightenGAN  & 8.6M & 17.48 & 0.652 & 0.322 \\ Restormer  & - & 22.37 & 0.816 & 0.141 \\ LLFormer  & 24.6M & 23.65 & 0.816 & 0.169 \\
**Resfusion (ours)** & 7.7M & **24.63** & **0.860** & **0.107** \\   

Table 2: Quantitative comparisons with other low-light enhancement methods. We report PSNR, SSIM and LPIPS . The best and second-best results are highlighted in **bold** and underlined. "\(\)" (resp. "\(\)") means the larger (resp. smaller), the better.

Figure 4: Visual comparisons of the restored results by different image restoration methods on the LOL dataset and the Raindrop dataset.

    \\  Method & Params & PSNR \(\) & SSIM \(\) \\   \\  Input Image & - & 25.81 & 0.887 \\ RDDM (SM-Res-N)  & 15.5M & 32.51 & 0.956 \\
**Resfusion (ours)** & **7.7M** & **34.40** & **0.975** \\   \\  Input Image & - & 25.40 & 0.882 \\ pix2pix  & - & 28.02 & 0.855 \\ AttentiveGAN  & **6.2M** & 31.59 & 0.917 \\ DqRN  & 10.2M & 31.24 & 0.926 \\ RaindropAttn  & - & 31.44 & 0.926 \\ All-in-One  & - & 31.12 & 0.927 \\ IDT  & 16.4M & 31.87 & 0.931 \\ WeatherDiffid  & 82.9M & 30.71 & 0.931 \\ RainDropDiff2s  & 109.7M & 32.43 & 0.933 \\
**Resfusion (ours)** & 7.7M & **32.61** & **0.938** \\   

Table 3: Quantitative comparisons with other deraining methods. We report PSNR and SSIM. The best and second-best results are highlighted in **bold** and underlined. "\(\)" means the larger, the better.

[MISSING_PAGE_FAIL:7]

in Fig 5, we qualitatively determine the functionalities of each component in the loss function of Resfusion. The weighted residual term are responsible for semantic shift, while the noise term handles detail reconstruction. Predicting resonoise \(rese_{}\) achieves both semantic shifts and detail reconstruction, bridging the gap between the input degraded image and the ground truth, enabling effective image restoration.

### Equivalent representations of the loss function

Based on Eq. (1), since \(_{0}\) is available, once we acquire either \(x_{0}\) or \(R\), we can determine the other. Moreover, according to Eq. (18), since \(x_{t}\) is obtainable and considering the equivalence between \(x_{0}\) and \(R\), we can also obtain the equivalent reverse process of resonoise-diffusion by predicting either \(x_{0}\) or \(R_{}\). Similar to DDPM, the fundamental purpose of Resfusion is also to predict \(x_{0}\) (which is equivalent to predicting \(R\)). In contrast, DDPM reconstructs the distribution of the original image \(x_{0}\) by incrementally reducing noise from the Gaussian white noise, whereas Resfusion skips the generation of low-frequency information by utilizing \(_{0}\) for initialization. Therefore, in addition to the noise removal, Resfusion also involves removing a weighted residual term to accomplish the semantic shifting. It is worth mentioning that, unlike RDDM, predicting the noise term \(_{}\) is **not** an equivalent loss function. Due to the truncated schedule we adopt, the starting point \(x_{T^{}}\) will only consist of weighted \(_{0}\) and \(\). As \(_{0}\) is obtainable and serves as a conditional input to the neural network, directly predicting the noise term would lead to the neural network learning a simple pattern, resulting in training failure.

We compare the quantitative performance obtained by using different equivalent prediction targets as loss functions on ISTD dataset, LOL dataset and Raindrop dataset. We use the same backbone as described in Sec. 3 and employ the same truncated linear schedule, performing five sampling steps for all datasets. The original image resolutions are maintained for evaluation. As shown in Table 4, predicting \(rese_{}\) outperformed predicting \(x_{0}\) and \(R_{}\) in terms of PSNR and SSIM on all three datasets. Predicting \(rese_{}\) resulted in better reconstruction of the fine details, as illustrated in Fig 6.

    &  &  &  \\  & PSNR \(\) & SSIM \(\) & MAE \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) \\  \(x_{0}\) & 29.67 & 0.927 & 5.35 & 23.10 & 0.813 & 0.150 & 32.51 & 0.935 \\ \(R_{}\) & 29.75 & 0.930 & **5.26** & 22.87 & 0.807 & 0.143 & 32.57 & 0.935 \\ \(rese_{}\) & **30.09** & **0.932** & 5.34 & **24.63

[MISSING_PAGE_EMPTY:9]

## 6 Conclusion

We propose the Resfusion, a general framework for image restoration. We explicitly provide the quantitative relationship between the residual term and the noise term, named as resonise. Through employing the smooth equivalence transformation, we unify the training and inference process. Resfusion does not require any prior physical knowledge and can directly utilize existing noise schedules. Experimental results shows that Resfusion exhibits competitive performance for shadow removal, low-light enhancement, and deraining tasks, with only five sampling steps. It is important to note that Resfusion is not limited to image restoration and can be applied to any image generation domain. The versatility of our framework lies in its ability to simultaneously model the residual term and the noise term. Our subsequent experiments have demonstrated that Resfusion can be easily applied to various image generation tasks and exhibits strong competitiveness.

## 7 Acknowledgements

This work is partially supported by the National Natural Science Foundation of China (62272248), the Natural Science Foundation of Tianjin (23JCZDJC01010, 23JCQNJC00010) and the Key Program of Science and Technology Foundation of Tianjin (24HHXCSS00004).

Figure 8: Visualization of the five sampling steps, where the \(blue\) arrow represents the smooth equivalence transformation, and the \(red\) box represents the resonise-diffusion reverse process. We select the LOL web Test dataset (which **does not** have ground truth) and the Raindrop-B test dataset (which is much more challenging than Raindrop-A) to showcase the effects of low-light enhancement and deraining. We directly use the pretrained models on LOL dataset and Raindrop dataset, demonstrating the strong robustness of Resfusion.