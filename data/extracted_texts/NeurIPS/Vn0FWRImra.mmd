# Nearly Minimax Optimal Submodular Maximization

with Bandit Feedback

 Artin Tajdini, Lalit Jain, Kevin Jamieson

University of Washington, Seattle, WA

{artin, jamieson}@cs.washington.edu, lalitj@uw.edu

###### Abstract

We consider maximizing an unknown monotonic, submodular set function \(f:2^{[n]}\) with cardinality constraint under stochastic bandit feedback. At each time \(t=1,,T\) the learner chooses a set \(S_{t}[n]\) with \(|S_{t}| k\) and receives reward \(f(S_{t})+_{t}\) where \(_{t}\) is mean-zero sub-Gaussian noise. The objective is to minimize the learner's regret with respect to an approximation of the maximum \(f(S_{*})\) with \(|S_{*}|=k\), obtained through robust greedy maximization of \(f\). To date, the best regret bound in the literature scales as \(kn^{1/3}T^{2/3}\). And by trivially treating every set as a unique arm one deduces that \(T}\) is also achievable using standard multi-armed bandit algorithms. In this work, we establish the first minimax lower bound for this setting that scales like \((_{L k}(L^{1/3}n^{1/3}T^{2/3}+T}))\). For a slightly restricted algorithm class, we prove a stronger regret lower bound of \((_{L k}(Ln^{1/3}T^{2/3}+T}))\). Moreover, we propose an algorithm Sub-UCB that achieves regret \(}(_{L k}(Ln^{1/3}T^{2/3}+T}))\) capable of matching the lower bound on regret for the restricted class up to logarithmic factors.

## 1 Introduction

Optimizing over sets of \(n\) ground items given noisy feedback is a common problem. For example, when a patient comes into the hospital with sepsis (bacterial infection of the blood), it is common for a cocktail of \(1<k n\) antibiotics to be prescribed. This can be attractive for reasons including 1) the set could be as effective (or more) than a single drug alone, but each unit of the cocktail could be administered at a far lower dosage to avoid toxicity, or 2) could be more robust to resistance by blocking a number of different pathways that would have to be overcome simultaneously, or 3) could cover a larger set of pathogens present in the population. In this setting the prescriber wants to balance exploration with exploitation over different subsets to maximize the number of patients that survive. As a second example, we consider factorial optimization of web-layouts: you have \(n\) pieces of content and \(k\) locations on the webpage to place them-how do you choose subsets to maximize metrics like click-through rate or engagement?

Given there are \( n^{k}\) ways to choose \(k\) items amongst a set of \(n\), this optimization problem is daunting. It is further complicated by the fact that for any set \(S_{t}\) that we evaluate at time \(t\), we only get to observe a noisy realization of \(f\), namely \(y_{t}=f(S_{t})+_{t}\) where \(_{t}\) is mean-zero, sub-Gaussian noise. In the antibiotics case, this could be a Bernoulli indicating whether the patient recovered or not, and in the web-layout case this could be a Bernoulli indicating a click or a (clipped) real number to represent the engagement time on the website. To make this problem more tractable, practitioners make structural assumptions about \(f\). A common assumption is to assume that higherorder interaction terms are negligible . For example, assuming only interactions up to the second degree would mean that there exist parameters \(^{(0)}\), \(^{(1)}^{n}\), and \(^{(2)}^{}\) such that

\[f(S)=^{(0)}+_{i S}^{(1)}_{i}+_{i,j S,i j}^{ (2)}_{i,j}.\] (1)

However, this model can be very restrictive and even if true, the number of unknowns scales like \(n^{2}\) which could still be intractably large.

An alternative strategy is to remain within a non-parametric class, but reduce our ambitions to measuring performance relative to a different benchmark which is easier to optimize. We say a set function \(f:2^{[n]}\) is _increasing and submodular_ if for all \(A B[n]\) we have \(f(A) f(B)\) and

\[f(A B)+f(A B) f(A)+f(B).\] (2)

Such a condition limits how quickly \(f\) can grow and captures some notion of diminishing returns. Diminishing returns is reasonable in both the antibiotics and webpage optimization examples. It is instructive to note that a sufficient condition for the parametric form of (1) to be submodular is for \(_{i,j}^{(2)}_{i,j} 0\). But in general, \(f\) still has \( n^{k}\) degrees of freedom even if it is monotonic and submodular. And it is known that for an unknown \(f\), identifying \(S^{*}:=_{S[n]:|S|=k}f(S)\) may require evaluating \(f\) as many as \(n^{k}\) times.

The power of submodularity is made apparent through the famous result of  which showed that the _greedy algorithm_ which grows a set one item at a time by adding the item with the highest marginal gain returns a solution that is within a \((1-e^{-1})\)-multiplicative factor of the optimal solution. That is, if we begin with and set \(S^{f}_{gr}_{i[n]:S^{f}_{gr}}f(S^{f}_{gr}\{i\})\) until \(|S^{f}_{gr}|=k\), then \(f(S^{f}_{gr})(1-1/e)f(S^{f}_{*})\) where \(S^{f}_{*}:=_{S[n]:|S| k}f(S)\) if \(f\) is increasing and submodular. This result is complemented by  which shows achieving any \((1-e^{-1}+)\)-approximation is NP-Hard. Under additional assumptions like curvature, this guarantee can be strengthened.

Due to the centrality of the greedily constructed set to the optimization of a submodular function, it is natural to define a performance measure relative to the greedily constructed set. However, as discussed at length in the next section, because we only observe noisy observations of the underlying function, recovering the set constructed greedily from noiseless evaluations is too much to hope for. Consequently, there is a more natural notion of regret against a noisy greedy solution, denoted \(R_{}\), that actually appears in the proofs of all upper bounds found in the literature for this setting (see the next section for a definition).

For this notion of regret, previous works have demonstrated that a regret bound of \(R_{}=O((k)n^{1/3}T^{2/3})\) is achievable (). This \(T^{2/3}\) rate is unusual in multi-armed bandits, where frequently we expect a regret bound to scale as \(T^{1/2}\). On the other hand, by treating each \(k\)-subset as a separate arm, one can easily adapt existing algorithms to achieve a regret bound of \(T}\). This leads to the following question:

_Does there exist an algorithm that obtains \(T}\) regret for \(r=o(k)\) on every instance? And if not, what is the optimal dependence on \(k\) and \(n\) for a bound scaling like \(T^{2/3}\)?_

To address these questions, we prove a minimax lower bound and complement the result with an algorithm achieving a matching upper bound. To be precise, the contributions of this paper include:

* A minimax lower bound demonstrating that \(R_{}=_{0 L k}(L^{1/3}n^{1/3}T^{2/ 3}+T})\). In words, for small \(T\), a \(T^{2/3}\) regret bound is inevitable, for large \(T\) the \(T}\) bound is optimal, with an interpolating regret bound for in between.
* For slightly restricted class of algorithms with non-adaptive greedy error threshold, we have the improved \(R_{}=(_{0 L k}(Ln^{1/3}T^{2/3}+ T})\).
* We propose an algorithm that for any increasing, submodular \(f\), we have \(R_{}=}_{0 L k}(Ln^{1/3}T^{2/3}+T})\). As this matches our lower bound, we conclude that this is the first provably tight algorithm for optimizing increasing, submodular functions with bandit feedback. Existing algorithms construct a set by greedily adding \(k\) items. Our main insight is that it is actually optimal to build up a set up to a size \(i^{*}\) and then for the remaining stages play sets of size \(k\) that include the initial set of size \(i^{*}\). Our choice of \(i^{*}\) is directly motivated by our lower bound.

In what remains, we will formally define the problem, discuss the related work, and then move on to the statement of the main theoretical results. Experiments and conclusions follow.

### Problem Statement

Let \([n]=\{1,,n\}\) denote the set of base arms, \(T\) be the time horizon, and \(k\) be a given cardinality constraint. At time \(t\), the agent selects a set \(S_{t}[n]\) where \(|S_{t}| k\), and observes reward \(f(S_{t})+_{t}\) where \(_{t}\) is i.i.d. mean-zero 1-sub-Gaussian noise, and \(f:2^{[n]}\) is an unknown monotone non-decreasing submodular function defined for all sets of cardinality at most \(k\).

Ideally, our goal would be to minimize the regret relative to pulling the best set \(S^{*}:=_{|S| k}f(S)\) at each time. In general, even if we had the ability to evaluate the true function \(f()\) (i.e. without noise), maximizing a submodular function with a cardinality constraint is NP-hard. However, greedy algorithms which sequentially add points, i.e. \(S^{(i+1)}=_{a S^{(i)}}f(S^{(i)} a),1 i k\) guarantee that \(f(S^{(k)}) f(S^{})\) with \( 1-1/e\) in worst-case. Unfortunately, since we do not know \(f()\) and instead only have access to noisy observations, running the greedy algorithm on any estimate \(()\) may not necessarily guarantee an \(=(1-1/e)\)-approximation to \(f(S^{*})\)1.

Consequently, a natural notion to address noisy observations is an \(\)-approximate greedy set for \(^{k}\). We define the following collection of sets of size \(k\)

\[^{k,} =\{S=S^{(k)} S^{(1)},|S^{(i)}|=i,\] \[_{a S^{(i)}}f(S^{(i)}\{a\})-f(S^{(i+1)}) _{i}\}.\]

Intuitively, any \(S^{k,}\) can be thought of as being constructed from a process that adds an element at stage \(i\) which is \(_{i}\)-optimal compared to the Greedy algorithm run on \(f\). Such a set naturally arises as the output of the Greedy algorithm run on an approximation \(\). This set enjoys the following guarantee.

**Lemma 1.1**.: _(Theorem 6 in ) For any \(^{k}\), and \(S^{k,}_{}^{k,}\), we have_

\[f(S^{k,}_{})+^{T} (1-e^{-1})f(S^{*}).\]

Lemma 1.1 is a noise-robust analogous result to the approximation ratio of the perfect greedy algorithm of  that says \(f(S^{k,0}_{})(1-e^{-1})f(S^{*})\). Note that \(|^{k,}|\) is non-decreasing in \(_{i}\) for all \(i[k]\), so identifying a set in \(^{k,}\) is in some sense easier for a larger \(^{T}\). Thus, to define an appropriate definition of regret, the measure must balance the facts that comparing with the noiseless greedy approximation in \(^{k,}\) may be impossible, but should account for identifying a set in \(^{k,}\) is strictly easier for larger \(^{T}\). Inspired by the above lemma we define _robust greedy regret_

\[R_{}:=_{,S^{k, }_{}^{k,}}R(S ^{k,}_{})+T^{T}\] (3)

where

\[R(S):=_{t=1}^{T}f(S)-f(S_{t}).\]This notion of regret captures the fact that if the algorithm plays a set in \(^{k,}\) then they may be incurring up to \(^{T}\) extra regret. Note that when \(=\) achieves the minimum (which can happen if the "gaps" between the greedily added element and all other elements at each stage is large) then this notion of regret is relative to the greedy set constructed in the noiseless setting.

The definition of regret in (3) is not novel to our paper. This notion is implicitly used in  in the proofs of Lemma 3 for the full-feedback setting and Theorem 13 for the bandit feedback setting,  in Theorem 4.1,  in Theorem 1,  in Theorem 2 for the full-feedback setting and Theorem 4 for bandit feedback, and  in Theorem 1. However, readers of these papers will note that they report their results not in terms of \(R_{}}\), but \(\)-Regret: for an \(\), define \(\)-regret by, \(R_{}:=_{t=1}^{T} f(S^{*})-f(S_{t})\) where \(S^{*}:=_{[S] k}f(S)\). Using Lemma 1, one immediately has that \(R_{} R_{}}\) for \(=(1-e^{-1})\). Thus, an upper bound on (3) immediately results in an upper bound on \(R_{}\), which is precisely what previous works exploit to obtain their upper bounds on \(R_{}\).

To summarize: all the analyses of these previous works concentrate on showing an upper bound on \(R_{}}\), and only at the last step argue that \(R_{} R_{}}\), and report an upper bound on \(R_{}\). But \(R_{}\) can be a very loose lower bound on \(R_{}}\)! For instance, when the function is modular (the inequalities of submodularity are tight), and the gap between the best set and worst set is equal to \(<e^{-1}\), then a random selection algorithm would get zero or even negative \(R_{}\) regret, while \(R_{}}\) would be linear \( T\), which is more natural. Thus, in studying regret against approximations attained by an offline step-wise greedy procedure, \(R_{gr}\) can be a more appropriate measure than \(R_{}\)

### Related Work

There has been several works on combinatorial multi-armed bandits with submodular assumptions and different feedback assumptions. Table 1 summarizes of the most relevant results as well as the results of this paper. For monotonic submodular maximization specifically, previous work use Lemma 1.1 with appropriate \(\) to prove an upper bound on expected \(R_{}\)-regret when the greedy result with perfect information gives an \(\)-approximation of the actual maximum value.

**Stochastic** In the stochastic setting, when the expected reward function is submodular and monotonic,  proposed an explore-then-commit algorithm with full-bandit feedback that achieves \(R_{}}=(k^{4/3}T^{2/3}n^{1/3})\)2. Recently,  showed with the same explore-then-commit algorithm with different parameters, \(R_{}}=(kn^{1/3}T^{2/3}+kn^{2/3}T^{1/3}d)\) is possible with delay feedback parameter of \(d\). Without the monotonicity,  achieves \(R_{}=(nT^{2/3})\) with bandit feedback for \(=1/2\). There have also been several works in the semi-bandit feedback setting (, ), and others such as getting the marginal gain of each element after each query.

**Adversarial** In the adversarial setting, the environment chooses an arbitrary sequence of monotone submodular functions \(\{f_{1},,f_{T}\}\), and the goal is to minimize regret against an approximation of the reward of the best set in hindsight (, , , ).  showed \((k)\)\(R_{(1-e^{-1})}\)-regret is possible with partially transparent feedback(where after each round, \(f(S^{(i)})\) for

  Function Assumptions & Stochastic & Regret & Upper Bound & Lower Bound \\  Submodular+monotone & ✓ & \(R_{}}\) & \(kn^{1/3}T^{4/3}\) & \(weak})}{}_{L}(L^{1/3}n^{1/3}T^{2/3}+ T})\) \\ Submodular+monotone & \(\) & \(R_{}}\) & \(kn^{1/3}T^{4/3}\) & \(weak})}{}_{L}(L^{1/3}n^{1/3}T^{2/3}+ T})\) \\ Degree d Polynomial & \(\) & \(R(S^{*})\) & \()}{}(T},T})\) & \()}{}(T},T})\) \\
**Submodular+monotone** & ✓ & \(R_{}}\) & \()}{}_{L}(^{1/3}T^{2/3}+T})\) & \()}{}_{L}(L^{1/3}n^{1/3}T^{2/3}+T})\) \\ (**This work**) & & & & \\  

Table 1: Best known regret bounds for combinatorial multiarmed bandits under different assumptions. By lemma 1.1 our upperbound can also be stated for \(R_{1-e^{-1}}\). We note that our lower bound proven for the stochastic setting immediately applies to the adversarial setting in the table.

all \(i\) is revealed instead of only \(f(S^{(k)}))\) and \((kn^{1/3}T^{2/3})\)\(R_{(1-e^{-1})}\)-regret for the bandit-feedback setting.  proposed a generalized algorithm with \(}(kn^{2/3}T^{2/3})\)\(R_{(1-e^{-1})}\)-regret with full bandit feedback, and showed all explore-then-commit greedy algorithms have \((T^{2/3})\) regret, when applied to our setting. Without the monotone assumption,  gets \((nT^{2/3})\)\(R_{(1/2)}\)-regret with bandit feedback. The upper-bound results in the adversarial setting doesn't naturally lead to results in the stochastic setting as the function is submodular and monotone only in expectation in the stochastic setting.

**Continuous Submodular** There are several works on online maximization of the continuous extensions of submodular set functions to a compact subspace such as Lovasz and multilinear extensions(, ). With a stronger assumption of DR-submodularity, it's possible to achieve higher approximation ratio guarantees and lower regret bounds (, , ).  uses multilinear extension to achieve \(O(T^{2/3})\)\(R_{(1-e^{-1})}\)-regret for adversarial submodular maximization with partition matroid constraint.

**Low-degree polynomial** In general reward functions without the submodular assumption,  showed if the reward function is a \(d\)-degree polynomial, \((T},T})\) regret is optimal.

## 2 Lower Bound

**Theorem 2.1**.: _For any \(n 4\), \(k n/3\), satisfying \(512k^{7}n T\), let \(\) denote the set of submodular functions that are non-decreasing and bounded by \(\) for sets of size \(k\) or less, with \(f()=0\). Then_

\[_{}_{f}[R_{}] {1}{16}(k-i^{*})^{1/3}T^{2/3}n^{1/3}e^{-8}+T^{1/2}}}e^{-2}\]

_where the infimum is over all randomized algorithms and the supremum is over the functions in \(\), and \(i^{*}[k]\) is the largest value satisfying \(k^{6}}}^{3} T\)._

The lowerbound is intuitively a mix of the greedy explore-then-commit algorithm for the first \(k-i^{*}\) arms, and then a standard MAB algorithm between all superarms of cardinality \(k\) that include those elements. For small \(T\) (i.e. \(T=(n^{4})\)) the regret would be \((k^{1/3}n^{1/3}T^{2/3})\), and for large \(T\)(i.e. \(T=(n^{3k-2})\)) the regret would be \((^{1/2}T^{1/2})\). This lowerbound also immediately gives a lower bound for the adversarial setting where \(f_{i}=f+(0,1)\) is the function chosen by the environment at time \(i\).

Proof SketchWe construct a hard instance so that at each cardinality a single set gives an elevated reward. Focusing on \(k=2\) for illustration, the instance would be the following:

\[_{0}:=f(\{i\})=1/2&i\{1\}\\ f(\{i\})=1/2-&i[n]\{1\}\\ f(\{i,j\})=3/4&(i,j)=(1,2)\\ f(\{i,j\})=3/4-&(i,j)\{(1,2)\} \]

where \(\) is the gap of the best set that we will tune based on \(T\). Pulling any arm of cardinality less than \(2\) would incur \((1)\) regret, however, since there are only \(n\) such sets (compared to \(\) sets of size \(2\)), pulling these simple arms give more information on the optimal set.

For a set of alternative instances, we choose a set of size \(k\) and elevate its reward by \(2\). We also elevate every prefix set of a permutation of this set by \(2\) so that the new set can be found by a greedy algorithm. Again, for \(k=2\), and any \(\{,\}[n]\{1,2\}\)

\[_{,}:=f(\{i\})=1/2&i\{1\}\\ f(\{i\})=1/2+&i\{\}\\ f(\{i\})=1/2-&i[n]\{1,\}\\ f(\{i,j\})=3/4&(i,j)=(1,2)\\ f(\{i,j\})=3/4+&(i,j)=(,)\\ f(\{i,j\})=3/4-&\]

Note that, if \(<\) for the \(k=2\) instance, All the functions are submodular, as \(f(\{a,b\})-f(\{b\})+2 1/2- f(\{a\})-f(\{\})\) for any \(a,b[n]\).

For \(_{0}\), if \(_{i}<\) for all \(i\), then \(f_{_{0}}(S_{gr}^{2,})=\) as the noisy greedy finds the best arm, and otherwise \(^{T}\), so \(_{}f_{_{0}}(S_{gr}^{2, })+^{T}=\). Similarly, \(_{}f_{_{, {j}}}(S_{gr}^{2,})+^{T}= +\). So for these instances \(R_{}}=R(S^{*})\).

We show that if the KL divergence between an alternate instance and \(_{0}\) is small, then the algorithm cannot distinguish between the two environments and the maximum regret of the two would be \(( T)\). Let \(_{,},_{,}\) be the probability and expectation under \(_{,}\), respectively when executing some fixed algorithm with observations being corrupted by standard Gaussian noise. Then \(KL(_{0}|_{,})=}{2} _{0}[T_{}]+4_{0}[T_{, }]\) for \(k=2\), where \(T_{S}\) is the number of pulls of set \(S\), and

\[_{0}[R_{}}]+_{, }[R_{}}]_{i=1}^{n} _{0}[T_{i}]+_{0}(T_{1,2} )+_{,}(T_{1,2}>)\] \[_{i=1}^{n}_{0}[T_{i}]+(-KL(_{0}|_{,}))= {2}_{i=1}^{n}_{0}[T_{i}]+-2^ {2}_{0}[T_{}]+_{0}[T_{, }].\]

Since \(,\) were arbitrary, the following Lemma shows that there exist a pair that are pulled for small number of times in expectation (see Lemma A.2 for general \(k\)).

**Lemma 2.2**.: _There exists a pair \(,\) such that_

\[_{0}[T_{}]+_{0}[T_{,}] _{0}[T_{i}]}{n-2}+}\]

Proof.: For a pair \((i,j)\), define \(Q_{(i,j)}:=_{0}[T_{i}]+_{0}[T_{i,j}]\). Then the sum of this term for all pairs not equal to \(1,2\) would be

\[Q:=_{(i,j)(1,2)}Q_{(i,j)}(n-3)_{i(1,2)}_{0}[T_{i }]+_{i,j 1,2}_{0}[T_{i,j}](n-3)_{i}_{0}[T_{i}]+T\]

Then by Pigeonhole principal there exist a pair \(,\) such that

\[Q_{(,)}} _{i}_{0}[T_{i}]+}\]

Using the lemma, for some \((,)\), we have

\[_{0}[R_{}}]+_{,}[R_{}}]_{i=1}^{n}_{0}[T_{ i}]+-2^{2}_{i} _{0}[T_{i}]+}\]

We choose an appropriate \(\) based on value of \(i^{*}\).

* If \(i^{*}=1\), then for \(=T^{-1/3}n^{1/3}\), we have \(T}{} 1\). So either the KL divergence is less than \(2\), then the regret is lowerbounded by \( Te^{-2}=T^{2/3}n^{1/3}e^{-2}\), or for KL divergence to be larger than \(2\) we would have \(_{i}_{0}[T_{i}]T^{2/3}n^{1/3}\), which from the above equation shows the regret is \((T^{2/3}n^{1/3})\). This can be extended for expected value of pulls of each cardinality lower than \(i^{*}+1\) for general \(k\).
* If \(i^{*}=2\), then it can be shown that the term \(_{i=1}^{n}_{0}[T_{i}]+-2 ^{2}_{i}_{0}[T_{i}]+(T-_{i=1}^{n} _{0}[T_{i}])/\) with \(=/T}\) minimizes when \(_{i=1}^{n}_{0}[T_{i}]=0\) i.e. zero single arm sets being pulled in expectation, so the regret would be \(T^{1/2}^{1/2}(-2)\).

This shows that the expected regret is \((_{i}(i^{1/3}n^{1/3}T^{2/3}+T}))\). The instance of general \(k\), and the detailed proof is in appendix A.1. 

We define an algorithm to be in non-adaptive greedy error-threshold class against \(R_{}\) regret, if it selects \(^{}_{1},,^{}_{k}\) at the start only dependent on parameters \(T,n,k\) before any arm pulls, and minimizes regret against \(f(S^{k,^{}}_{})+^{T}^{}\). All the algorithms from previous work in the literature fall within this restricted class, and with this extra assumption we can prove a stronger lower bound.

**Theorem 2.3**.: _For any \(n 4\), \(k n/3\), satisfying \(512k^{9}n T\), let \(\) denote the set of submodular functions that are non-decreasing and bounded by \(\) for sets of size \(k\) or less, with \(f()=0\). Then_

\[_{}_{f}[R_{ }](k-i^{*})T^{2/3}n^{1/3}e^{-10}+T^{1/2 }}}e^{-2}\]

_where the infimum is over all randomized algorithms with non-adaptive greedy error threshold selection, and the supremum is over the functions in \(\), and \(i^{*}[k]\) is the largest value satisfying \(k^{6}}}^{3} T\)._

## 3 Ucb Upper Bound

```
1:Input:\(T\), \(m\), greedy stop level \(l\)
2:Initialization:\(S^{(0)}=\), \(T_{A}=0\) for all \(A[n]\)
3:For each \(a[n]\), pull \(\{a\}\) exactly \(m\) times and update \(T_{\{a\}} m\). Update \(t mn\).
4:for\(i=1,2,,l\)do
5:\(U_{a}\) for all \(a S^{(i-1)}\)
6:while\(T_{S^{(i-1)}*{arg\,max}U_{a}}<m\)do
7: Pull arm \(S_{t}=S^{(i-1)}*{arg\,max}_{a}U_{a}\), observe \(r_{}\), and update \(T_{S_{t}} T_{S_{t}}+1\)
8:for each \(a S^{(i-1)}\)do
9:\(S_{a} S^{(i-1)}\{a\}\)
10:\(_{S_{a}}}}_{t:\,l_{t}=S_{a}}r_{t}\)
11: Compute UCB: \(U_{a}=_{S_{a}}+}}}\)
12:endfor
13:\(t t+1\)
14:endwhile
15: Update the base set: \(S^{(i)} S^{(i-1)}\{a_{i}\}\) where \(a_{i}:=*{arg\,max}_{a}U_{a}\)
16:endfor
17:while\(t<T\)do
18: Run UCB on all size \(k\) super-arms \(A\) where \(S^{(l)} A\).
19:endwhile ```

**Algorithm 1**Sub-UCB algorithm for set bandits with cardinality constraints

A natural approach to minimizing regret is to take an Explore-Then-Commit strategy motivated by the greedy algorithm. Such an algorithm would be the following - proceed in \(k\) rounds. Set \(S^{0}=\).

In round \(i\) pull each set in the collection \(\{S^{i-1}\{a\}:a[n] S^{i-1}\}\), \(m\) times. Use these samples to update our estimate \(\) of \(f\) on these sets, and set \(S^{(i)}_{a[n] S^{i-1}}(S^{i-1}\{a\})\). This approach has been pursued by existing works , and with an appropriate choice of \(m\) results in \(O(kn^{1/3}T^{2/3})\) regret.

The disadvantage of this approach is that it can not achieve the correct trade-off between \(T}\) and \(kn^{1/3}T^{2/3}\) exhibited by the lower bound. Motivated by the statement of the lower bound, our algorithm Sub-UCB attempts to interpolate between these different regret regimes. The critical quantity is \(i^{*}\). For the first \(k-i^{*}\) cardinalities, our algorithm plays a UCB style strategy which more or less follows the ETC strategy described in the previous paragraph. After that, it defaults to a UCB algorithm on all subsets containing \(S^{k-i^{*}}\), a total of \(}{i^{*}}\) possible arms.

**Theorem 3.1**.: _For any \(l k\), Sub-UCB guarantees_

\[[R_{}](1+4)lT^{2/3}n^{1/3}( T)^{1/3}+65  T}+\]

_when \(m=T^{2/3}n^{-2/3} T^{1/3}\)._

Proof SketchWe show that for \(:=2)/m}\), the greedy part of Sub-UCB with high probability adds an \(\)-optimal arm in each step. Defining event \(G\) to be \(|_{S}-f(S)|(2knT^{2})}\) for all iterations, we prove that this event is true with a probability of at least \(1-\).

On Event \(G\), We show that an \(\)-good arm is selected at each step of the greedy algorithm for \(=2)}{m}}\). Let \(a\) be a sub-optimal arm with expected reward value more than \(2)}{m}}\) from the best arm in the \(i\)-th step i.e. \(_{S^{(i)},a}:=_{a^{}}f(S^{(i)}\{a^{}\})-f(S^{(i)} \{a\}) 2)}{m}}\). Then if arm \(a\) is added in \(i\)-th step, we have \(U_{a}(t) U_{a^{*}}(t) f(S^{(i)})\{a^{*}\}\), and therefore,

\[U_{a}(t)-f(S^{(i)}\{a\})_{S^{(i)},a}>2 )}{m}},\]

so \(_{S^{(i)}\{a\}}-f(S^{(i)}\{a\})>)}{ m}}\). This is a contradiction with event \(G\), so on event \(G\) such an arm cannot be selected. Lastly, we expand the regret of two stages. As UCB in the second part of the algorithm has the regret of \(65 T}+\) against the best arm containing \(S^{(l)}\)(see ), it is an upper bound for the regret against the greedy solution were the first \(l\) steps select an \(\)-good arm, and the last \(k-l\) steps select the best arm, so on event \(G\) the regret can be written against a set in \(^{k,}\) where

\[^{T}=l+(k-l)0=2l)}{m}}.\]

Therefore, the expected regret \([R_{}]\) on event \(G\) can be written as

\[2Tl)}{m}}+mn(k-i^{*})+65 T }+,\]

for any choice of \(m\) and \(l\). So for \(m=T^{2/3}n^{-2/3}^{1/3}(2knT^{2})\) the above term becomes \(}(lT^{2/3}n^{1/3}+})\). The detailed proof is in Appendix B 

## 4 Experiments

For the experiments we compare Sub-UCB (\(l\)) for different greedy stop levels \(l\), Sub-UCB (\(k-i^{*}\)) which selects the best stop level based on the regret analysis, the ETCG (explore-then-commit greedy) algorithm from , and UCB on all size \(k\) arms. Each arm pull has a \(1\)-Gaussian noise, with \(50\) trials for each setting. The expected reward functions are the following.

**Functions:**

* The Unique greedy path hard instance i.e. \[f(S)=_{i=1}^{|S|}&S=\{1,,|S|\}\\ _{i=1}^{|S|}+&S=\{1,,|S|\}.\] This function is inspired by the hard instance in the proof of our lower-bound. Note that this particular parameterization is submodular when \(k 7\), not for general \(k\).
* Weighted set cover function i.e. \(f_{}(S)=_{C}w(C)\{S C\}\) for a partition \(\) of \([n]\) and weight function \(w\) on the partition. For \(n=15\) and \(k=4\), we use the partitions of size \(5,5,4,1\) with weights of \(1/10,1/10,2/10,6/10\) respectively.

**Results:** As illustrated in figure 1, we observe that our algorithm with the level selection of \(k-i^{*}\) outperforms both ETCG and naive UCB on all size \(k\) arms, as it combines the advantages of greedy approach for small \(T\)s and UCB on many super arms for large \(T\). For smaller \(T\)s compared to \(\), both Sub-UCB and ETCG outperform normal UCB as it doesn't have enough budget to find optimal sets of size \(k\), so it gets linear regret(as the other two get \(O(T^{2/3})\)). However, as \(T\) becomes larger the reverse happens as \(T^{1/2}\) becomes smaller than \(T^{2/3}\), but Sub-UCB adopts to \(T\) and continues to outperform the two until it converges with naive UCB for very large \(T\).

Figure 1: Regret comparison for weighted set cover with \(n=15\) and \(k=4\)

Figure 2: Comparison between all Sub-UCB greedy stop cardinality choices for the unique greedy path function with \(n=20\) and \(k=5\). The worst-case optimal stop cardinality \(l=k-i^{*}\) is highlightedIn figure 2, we compare the performance of Sub-UCB for different choices of greedy stop cardinality, and observe that the best choice gradually decreases from \(k\) to \(0\) as \(T\) gets larger, and \(k-i^{*}\) is a practical selection of the best stop cardinality before running the algorithm. Note that the defined stop level was chosen to minimize the worst-case bound on the regret, and if the gaps between arms on a particular instance are larger than the worst case, this stop level could be conservative. So \(k-i^{*}\) is near the optimal stop level, and not the exact one as seen in these figures. Also, the empirical standard derivation is much smaller than \((T^{1/2})\) due to the regret symmetry of non-optimal sets at each cardinality, and it's not visible in the plots.

## 5 Conclusion

In this paper we showed that \(_{L}(L^{1/3}T^{2/3}n^{1/3}+T})\), ignoring logarithmic factors, is a lower bound on the regret against robust greedy solutions of stochastic submodular functions, and a stronger lower bound if the algorithm class is slightly restricted. We also matched this bound with an algorithm. This work is the first minimax lower bound for submodular bandits, and beyond closing the \(k^{2/3}\) gap between the general lowerbound and upperbound, it remains open to prove similar minimax optimal bounds in settings with different types of constraint such as matroid, or in general, any offline-to-online greedy procedure that is robust to local noise (e.g. Non-monotonic submodular maximization where the greedy approach gets a \(1/2\)-approximation of the function, or DR-submodular optimization for the continuous setting which also has a \((1-e^{-1})\)-approximation).