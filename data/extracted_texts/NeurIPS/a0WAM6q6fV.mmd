# Croissant: A Metadata Format for ML-Ready Datasets

Mubashara Akhtar\({}^{1*}\), Omar Benjelloun\({}^{2*}\), Costanza Conforti\({}^{2*}\), Luca Foschini\({}^{3*}\), Pieter Gijsbers\({}^{4}\), Joan Giner-Miguelez\({}^{5,22*}\), Sujata Goswami\({}^{6}\), Nitsha Jain\({}^{1*}\), Michalis Karamousandakis\({}^{7}\), Satyapriya Krishna\({}^{8}\), Michael Kuchnik\({}^{8*}\), Sylvain Lesage\({}^{10*}\), Quentin Lhoest\({}^{10*}\), Pierre Marcenc\({}^{2*}\), Mani Maskey\({}^{11}\), Peter Mattsson\({}^{2}\), Luis Oala\({}^{12*}\), Hamidah Oderinwale\({}^{13}\), Pierre Ruyssen\({}^{2*}\), Tim Santos\({}^{14}\), Rajat Shinde\({}^{15*}\), Elena Simperl\({}^{1,16*}\), Arjun Suresh\({}^{17}\), Geoffry Thomas\({}^{2,18*}\), Slava Tykhonov\({}^{19*}\), Joaquin Vanschored\({}^{4*}\), Susheel Varma\({}^{3}\), Jos van der Velde\({}^{4*}\), Steffen Vogler\({}^{20}\), Carole-Jean Wu\({}^{9}\), Luyao Zhang\({}^{21}\)

Authors in alphabetical order

###### Abstract

Data is a critical resource for machine learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that creates a shared representation across ML tools, frameworks, and platforms. Croissant makes datasets more discoverable, portable, and interoperable, thereby addressing significant challenges in ML data management. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, enabling easy loading into the most commonly-used ML frameworks, regardless of where the data is stored. Our initial evaluation by human raters shows that Croissant metadata is readable, understandable, complete, yet concise.

## 1 Introduction

Recent machine learning (ML) advances highlight the critical role of data management in achieving technological breakthroughs. Yet, working with data remains time-consuming and painful due to a wide variety of data formats, the lack of interoperability between tools, and the difficulty of discovering and combining datasets . Data's prominent role in ML also leads to questions about its responsible use for training and evaluating ML models in areas such as licensing, privacy, or fairness, among others . New approaches are needed to make datasets easier to work with, while also addressing concerns around their responsible use.

This paper presents _Croissant_, a metadata format designed to improve ML datasets' discoverability, portability, reproducibility, and interoperability. Croissant makes datasets "ML-ready" by recording ML-specific metadata that enables them to be loaded directly into ML frameworks and tools (see Figure  for sample code). Croissant describes datasets' attributes, the resources they contain, and their structure and semantics. This uniform description streamlines their usage and sharing within the ML community and between ML platforms and tools while fostering responsible ML practices. Figure  gives an overview of the Croissant lifecycle and ecosystem.

Croissant can describe most types of data commonly used in ML workflows, such as images, text, audio, or tabular. While datasets come in a variety of data formats and layouts, Croissant exposes a unified "view" over these resources. It lets users add semantic descriptions and ML-specific information. The Croissant vocabulary  does not require changing the underlying data representation, and can thus be easily added to existing datasets, and adopted by dataset repositories.

To assess Croissant's usability, we conducted a preliminary usability evaluation on metadata creation for language, vision, audio, and multimodal datasets. Several practitioners annotated ten widely used ML datasets. We analyzed the consistency of their responses and collected their feedback on Croissant.

The remainder of the paper is structured as follows: in Section 2 we contextualize related work. In Section 3 we describe the Croissant format, its integrations, and the tools that support it. Section 4 comprises the user study and discusses its results and limitations.

## 2 Related Work

While there have been many prior efforts in standardizing dataset metadata, they typically lack ML-specific support, do not work with existing ML tools, or lag behind the demands of dynamically evolving requirements, such as responsible ML. We outline the state of the field below.

**Vocabularies for Dataset Documentation.** Dataset documentation is indispensable for effective data management and serves as a foundational element for training and evaluating ML models . Metadata descriptions of datasets enhance their discoverability, interoperability, and usability, which is critical for advancing research and data-driven applications. Ontologies and vocabularies are semantic web tools used to standardize dataset documentation. While vocabularies comprise sets of terms and their meanings to describe data consistently, ontologies provide a structured framework to define and relate these concepts within a domain. Ontologies and vocabularies are evaluated for their coverage (i.e., do they represent all relevant concepts), accuracy (correctness of definitions and relationships), consistency (no logical contradictions), and usability (ease of use and integration). This is done through methods like competency questions, expert validation, and use-case testing .

**Standards for Catalogs and Metadata.** With the increase of data availability online, various efforts have focused on making data both discoverable and user-friendly by supplementing datasets with comprehensive metadata. This metadata may include details about the dataset, such as authorship, format, and intended use, all structured consistently to support automated processing and retrieval. Key efforts towards documentation have led to the creation of standards like the Data Catalog Vocabulary (DCAT)  and the Dataset vocabulary in schema.org . DCAT facilitates interoperability among web-based data catalogs, enabling users to aggregate, classify, and filter datasets efficiently. Schema.org  acts as a de facto standard for metadata, helping search engines discover and index published web content, including datasets, thus enhancing dataset accessibility and understandability. This versatility allows schema.org to describe a wide array of content types effectively. Other frameworks, such as Data Packages  and CSV on the Web  support methods for describing and exchanging tabular data. The Global Alliance for Genomics and Health's Data Use Ontology (DUO)  refines data usage terms with optional modifiers, improving clarity in genomic data sharing agreements. Efforts towards integration of FAIR principles (Findability, Accessibility, Interoperability, and Reusability)  in metadata vocabularies are also noteworthy. Despite their utility for specific domains and formats, these standards do not entirely meet the specialized needs of data management within the ML domain. In this context, the compliance of ML-ready datasets with the FAIR principles is a primary need for improving discoverability, portability and reproducibility in the ML ecosystem. The adoption of standard metadata description practices across the broader community further enhances the interoperability of ML datasets from diverse domains.

Figure 1: The Croissant lifecycle and ecosystem.

**Operationalizing Responsible ML through Data Work.** Data-centric ML  is increasingly seen as critical to the development of trustworthy ML systems, including aspects such as fairness, accountability, transparency, data privacy and governance, safety, and robustness . Seminal works, such as Datasheets for Datasets  and Data Statements , have emphasized the importance of dataset documentation to assess and increase the trustworthiness of ML systems. Several related documentation efforts such as Data Cards  and Data Nutrition Labels  have been inspired them. ML data repositories, such as Kaggle , OpenML  and Hugging Face , have initiated their own metadata documentation efforts. Hugging Face, for example, provides Dataset Cards  that include summaries, fields, splits, potential social impacts, and biases inherent in the datasets.

These approaches typically rely on data documentation written in natural language, without a standard machine-readable representation, which makes data documentation challenging for machines to read and process. Croissant fills this gap by providing a standardized framework for data documentation that ensures semantic consistency and machine readability, thereby facilitating seamless integration with existing tools and frameworks used by the ML community.

## 3 The Croissant Format

The Croissant format is a community-driven metadata vocabulary for describing datasets that builds on Schema.org . Croissant is divided into four layers: _(i)_ The _Dataset Metadata Layer_, containing relevant information such as name, description, and version. _(ii)_ The _Resource Layer_ describes the source data used in the dataset. _(iii)_ The _Structure Layer_, describing and organizing the structure of the resources. _(iv)_ The _Semantic Layer_, which provides ML-specific data interpretation and semantics. A more detailed description of the Croissant format can be found in the official specification . Documentation and code is available online2

In the remainder of this section, we illustrate each layer with examples from popular ML datasets. Afterwards, we briefly describe the Croissant Responsible AI extension, and then provide an overview of ML frameworks, tools, and repositories that currently support Croissant.

### The Dataset Metadata Layer

Croissant dataset descriptions, illustrated in Figure 3 are based on schema.org/Dataset, a widely adopted vocabulary for datasets on the Web , hence ensuring interoperability with existing standards and tools. Croissant specifies constraints on which schema.org properties are required, recommended and optional, and adds additional properties, e.g., to represent snapshots, live datasets, and citation information.

Figure 2: Users can easily inspect datasets (e.g., Fashion MNIST ) and use them in data loaders with Croissant. See Supplementary material or visit [https://github.com/mlcommons/croissant](https://github.com/mlcommons/croissant) for more examples.

### The Resources Layer

This layer represents the data resources (e.g., files) of the dataset. Schema.org properties are insufficient to adequately describe dataset contents with complex layouts, which are common for ML datasets. This layer provides two primitive classes to address this limitation and describe dataset resources: FileObject to describe individual files and FileSet to describe sets of files.

Figure 3 shows an excerpt of the Croissant definition of the PASS dataset , where declarations of object names are highlighted in yellow, with references in orange. This distribution includes two FileObjects: a CSV file containing metadata about the dataset (line 13) and an archive file containing images (line 20). Moreover, FileSet (in line 27) is used to refer to a collection of images, videos, or text files that contain the (unlabeled) data used for training and inference. Since there can be numerous files, FileSets are specified with inclusion/exclusion filters (e.g., a pattern matching all files that should be included) as shown on line 30.

### The Structure Layer

While FileObject and FileSet describe a dataset's resources, they lack information on how the content of the resources is organized. This is addressed with RecordSet, which allows loading data of various formats into a standard representation, including structured (CSV and JSON) and

Figure 3: Dataset metadata and resources for the PASS dataset.

Figure 4: A RecordSet that joins images and structured metadata from the PASS dataset.

unstructured (text, audio, and video) data. Handling all data formatting information in one layer abstracts away format heterogeneity, addressing a key challenge in processing and loading ML data.

RecordSet provides a common structure description for records that may contain multiple fields, which can be used across different modalities. As an example, Figure 1 shows a RecordSet combining images from PASS with additional features from a metadata CSV file. Each Field in the RecordSet defines the source of its data, which may refer to the contents of elements in a FileSet. For instance, the Field images/image_content in line 9 refers to the image-files FileSet and also points to the specific property to extract in line 10.

Fields can be nested, as we can see in the images/coordinates field, which contains two subfields: images/coordinates/latitude and images/coordinates/longitude. Croissant supports nesting entire RecordSets, e.g., to add annotations (e.g. object bounding boxes) to images, where each image may correspond to multiple structured annotations. See Croissant's COCO  definition for a representative example. RecordSet also supports joining heterogeneous data and data manipulation methods, like JSON Path and regular expressions, for flexible data extraction and transformation.

### The Semantic Layer

The semantic layer introduces a number of useful features in the context of ML data. These are implemented using the primitives defined in the previous sections, generally as new classes or properties defined in the Croissant namespace. Semantic typing is used to describe important aspects of ML practice, such as the dataset splits (train, test, validation) as well as dataset labels. Additionally, semantic typing is used to describe commonly used data types, such as bounding boxes, categorical data, or segmentation masks. As an example, in Figure 1, the structured Field images/coordinates has the dataType GeoCoordinates from schema.org. The subFields images/coordinates/latitude and images/coordinates/longitude are implicitly mapped to the latitude and longitude properties associated with that class, because their names match by suffix.

### The Croissant-RAI Extension

Croissant-RAI  is an extension of the Croissant format that builds on existing responsible AI (RAI) dataset documentation approaches, such as Data Cards  and Datasheets for Datasets , making it easier to publish, discover, and reuse RAI metadata. The extension was developed around RAI use cases such as documenting the data life cycle, data labeling and participatory processes, information for AI safety, fairness assessments, and regulatory compliance. It was developed through a multi-step, iterative vocabulary engineering process. Based on the target use cases, a list of properties was defined through evaluation of related dataset documentation vocabularies and the Croissant vocabulary with an aim to detect overlaps and gaps. The resulting properties were evaluated by annotating example datasets to verify their usability and usefulness. For more details, see .

### Croissant Tools and Integrations

In parallel with the definition of the Croissant format, we have pursued a number of integrations, with the goals of 1) making Croissant immediately useful to users, and 2) grounding Croissant in the requirements of real-world datasets and tools. Figure 1 gives an overview of the Croissant ecosystem.

Data Repositories.Croissant has been integrated into three major dataset repositories: Hugging Face Datasets, Kaggle Datasets, and OpenML, which together describe over 400,000 datasets in the Croissant format. This integration has succeeded with minimal effort because Croissant is an extension of the widely adopted Schema.org/Dataset vocabulary and does not require changing the existing data layout. Supporting Croissant involved adding additional fields to existing metadata. Furthermore, most repositories offer normalized data representations (Hugging Face and OpenML convert most datasets to Parquet) and their own data types (such as relational schemas for tabular data). Consequently, the conversion to Croissant primarily focuses on managing these data formats and specifying associated data types as RecordSet definitions.

In addition to the support from individual data repositories, Croissant is also supported by Google Dataset Search . When a user searches for a query that returns Croissant datasets, a special filter allows them to restrict the results to only Croissant datasets. This functionality allows users to effectively search for Croissant datasets across data repositories and the entire web.

ML Frameworks.Croissant's reference implementation is a standalone Python library that supports the validation of Croissant dataset descriptions, their programmatic creation and manipulation, and serialization into JSON-LD. To consume data, the library provides an iterator abstraction that interoperates with existing data loaders. The TensorFlow Datasets  library provides a dataset builder that prepares the dataset on disk in a format compatible with JAX, TensorFlow and PyTorch loaders. Alternatively, frameworks such as PyTorch DataPipes  interface with the Croissant library by wrapping the iterator directly. We anticipate that additional optimization opportunities will arise with more varied and larger datasets, perhaps requiring distributed execution as well as more advanced operator scheduling.

**Croissant Editor.** Croissant is primarily a machine readable format (in JSON-LD), so users may find it hard to create dataset descriptions by hand. We developed the Croissant Editor  (also on GitHub ), a tool that lets users visually create and modify Croissant datasets. The Croissant Editor provides form-based editing and validation of Croissant metadata, and bootstraps the definition of resources and RecordSets by inferring them from the data uploaded by the user. The editor integrates the Croissant Responsible AI extension, and guides users in describing RAI aspects of their datasets.

### The Croissant Working Group

We designed the Croissant format in an open and participatory way. The MLCommons Croissant Working Group (WG  consists of diverse stakeholders and domain experts from academia, industry, research organizations, and collaborative networks such as the AI for Public Good network. Use cases were discussed and presented to WG members (including domain experts) as they were developed, ensuring that diverse views and priorities were covered. The schema is designed to be modular and extensible, allowing for domain-specific attributes and concerns to be integrated into the Core Croissant format. We continuously collect feedback from working group members and users and are committed to incorporating this feedback in future versions of Croissant. Additionally, Croissant is based on schema.org, a well-established vocabulary.

## 4 Croissant Evaluation: A User Study with ML Practitioners

This section describes the user study we conducted to evaluate the Croissant metadata format. We asked machine learning practitioners to annotate a variety of datasets commonly used in the ML community. Human annotators authored a subset of the Croissant and Croissant-RAI attributes and assessed them based on criteria commonly used in vocabulary evaluation .

### The User Study Process

Recruitment of Annotators and Annotation Process.We recruited nine volunteers from the Croissant development community who were all proficient in English with backgrounds in vocabulary and ontology engineering, dataset documentation, ML benchmarking, and responsible AI. We collected demographic information from all annotators, which we published in the user study report . For each one of the ten datasets, we collected metadata definitions from three annotators, resulting in thirty annotations. Each human annotator assessed approximately three datasets on average, with three annotating one dataset and one person annotating six datasets.

The instructions for the annotators were comprised of: \((i)\) a short introduction to the Croissant metadata format; \((ii)\) the purpose of the user study; \((iii)\) the definitions of the requested Croissant and Croissant-RAI attributes; \((iv)\) links to the format specifications, and \((v)\) a link to each datasetin the Hugging Face repository. Prior to starting the user study, we obtained ethical clearance and informed annotators about the data being collected and its purpose. For each dataset, annotators filled out a provided JSON template with the sixteen attributes to complete. Afterwards, annotators answered questions about their level of understanding of the datasets (see Table 1), and indicated their confidence in the annotations they provided on a Likert scale  between \(1\) and \(5\). We followed previous research  suggesting that confidence ratings can serve as a tool to understand potential annotation inconsistencies. The user study began in April \(2024\) and lasted approximately five weeks.

Selection of Croissant Attributes.We selected ten attributes from Croissant's Dataset Layer (see Section 3.1) and six Croissant-RAI attributes (Section 3.5). We selected attributes that \((i)\) require manual specification, \((ii)\) can be defined by dataset users using the following resources: the dataset itself, a publication describing the dataset, and the Hugging Face dataset card if available, and \((iii)\) support the discoverability and reproducibility of datasets, along the lines of previous literature on improving dataset usability via documentation. For example, missing or limited descriptions of datasets reduce their discoverability and hinder practitioners from using the dataset as intended . Moreover, lack of information on data reproducibility, e.g., about the data collection and curation process, also impacts the dataset's adoption in the ML community . Table 2 and Table 3 list the attributes selected for this study.

ML Datasets.We selected commonly used ML datasets from the language, vision, and audio modalities, based on their popularity on the Hugging Face (HF) Datasets repository. We further filtered datasets to require \((a)\) a pre-existing Croissant description, \((b)\) a dataset card in HF Datasets, and \((c)\) a publication that describes the dataset creation process. Table 4 lists all datasets.

Evaluation.To evaluate the collected attribute annotations, we studied the provided answers and assessed the agreement among annotators. To measure agreement for textual attributes (e.g., sc:description), we calculated BLEU scores between attribute annotations, which were in textual form and did not allow for inter-annotator agreement scores commonly used for measuring agreement based on categorical data. The BLEU metrics  measure text similarity based on overlapping \(4\)-grams in text pairs. The score can be between \(\) with \(1\) indicating perfect match between both compared texts. Hence, a score closer to one indicates higher agreement among all three annotations available for the respective attribute and dataset.

### Mapping Evaluation Criteria to Croissant

Previous literature proposes different criteria for vocabulary evaluation . Following prior work , we evaluate Croissant on the five criteria we outline below. We further discuss how the criteria translate to Croissant and specify questions to evaluate each criterion in the context of our user study.

  
**Criteria** & **Question** & **Answer Options** \\  Answer Confidence & How confident are you that your & **I** (no confidence) \\  & provided annotations are correct? & **5** (very confident that annotations are correct) \\   & How well did you understand the & **I** (I don’t understand the dataset at all) - \\  & dataset (e.g. the task, domain, & **5** (the dataset incl. its purpose, creation, etc. is very clear and understandable for me) \\   & Is there any (in your opinion) & **I** (yes, there is lots of critical information about the dataset that Croissant does not capture) - \\  & important) information about the & **5** (no, every important information about this \\  & dataset which you can’t define & dataset, which might be useful for ML users, \\  & using Croissant? & is capture in Croissant attributes) \\   & Did you find any attributes redundant & **I** (yes, there are lots of redundant attributes) - \\  & and not definable for this dataset? & **5** (no, none of the attributes is redundant) \\   & How intuitive are the attributes names & **I** (not intuitive at all, for each single attribute \\  & for you? A name is not intuitive if & checked the specification to understand it) - \\  & you need to check the specification & **5** (very intuitive, based on the name I could \\  & to understand the attribute’s name? & understand the attribute very well) \\  Understandability & Rate the ease of understanding & **I** (Understanding the spec. was very hard) - \\  & the Croissant specification. & **5** (the spec. is very easy to understand) \\   

Table 1: Post-annotation assessment: Criteria, corresponding questions, and answer scales.

\((1)\) **Consistency.** The criterion evaluates if a vocabulary is consistent and free of contradictions in its attribute definitions . To measure Croissant's consistency, we studied how well annotations by different annotators for the same attribute and dataset aligned, i.e. based on the agreement among annotators.

\((2)\) **Completeness.** A vocabulary is complete if it covers the specified intent. While Croissant is an ongoing effort and not fully complete, we evaluated during the user study if Croissant currently misses any attributes necessary to capture important information about commonly used ML datasets. We asked annotators to flag any important information about the datasets they annotated that could not be defined using Croissant.

\((3)\) **Conciseness.** The conciseness criterion assesses whether a vocabulary avoids useless definitions and is free of redundancies. We measured this by asking annotators if they found any Croissant attributes redundant or not definable for the studied ML datasets.

\((4)\) **Readability.** The readability criteria assess how intuitive the attribute names are. After completing the annotations, we asked annotators to indicate on a Likert scale of \(1\) to \(5\) how intuitive they found Croissant attribute names to be.

\((5)\) **Understandability.** The understandability criteria evaluates how easily user can understand Croissant attributes from the provided documentation. During our user study, we instructed annotators to use the Croissant specifications  and prompted them afterwards with questions.

### Results and Discussion

This section analyses data collected during the user study. First, we evaluate the answers to the questions listed in Table . Second, we study the annotation of Croissant and Croissant-RAI attributes.

Criteria Evaluation.Assessing annotators' ratings for the criteria in Table , we find that for over \(80\%\) of annotations (\(25\) out of \(30\) annotations), Croissant attributes capture important information about the datasets (see Figure 5). For the conciseness criteria, we found a higher variance in ratings. While most annotations state that none or few requested attributes are redundant for the dataset (approx. \(60\%\)), seven annotations (around \(23\%\)) state that some attributes are redundant

  
**Property** & **RAI Use Case** & **Dataset** & **Modality** \\  sc:description & rai:dataCollection & Data life cycle & MMLU  & Language \\ sc:license & rai:dataCollectionTimeframe & Data life cycle & Dolly-15k  & Language \\ sc:name & rai:dataAnnotationPlatform & Data labelling & FLORES  & Language \\ sc:url & rai:annotatorDemographics & Data labelling & CIFAR10  & Vision \\ sc:creator & rai:dataUseCases & AI safety and fairness evaluation & MSCOCO  & Vision \\ sc:publisher & rai:personalSensitiveInformation & Compliance & Visual Genome  & Vision \\  & rai:personalSensitiveInformation & Compliance & MMMU  & VL \\   & & MathVista  & VL \\   & & ML\_Eng  & Audio \\   & & librispeech\_asr  & Audio \\   

Table 2: Annotated Table 3: Annotated Croissant-RAI attributes.

or not definable. This was due to Croissant-RAI attributes, e.g., rai:annotatorDemographics and rai:personalSensitiveInformation, which was either missing from the dataset's documentation or difficult to extract it. The majority of annotations (around \(83\%\)) found Croissant and Croissant-RAI attribute names intuitive, resulting in high _readability_ scores (Figure 7). However, one attribute pair commonly confused annotators: sc:creator and sc:publisher. These attributes come from the schema.org vocabulary, which Croissant builds upon, and are already widely used to describe datasets on the Web. On the bright side, around \(90\%\) of annotations stated that the specifications were understandable(Figure 8).

In addition to criteria-related questions, we asked annotators how confident they were regarding the correctness of their annotations and their understanding of the datasets. For the majority of annotations (more than \(75\%\)), their annotators selected that they were very confident. We found that the annotations with moderate confidence were indeed the ones with lower agreement on attribute values. Moreover, for the majority of annotations, the annotators selected that they had a clear understanding of the datasets, with around \(80\%\) selecting four or five on the five point scale (see Table 1), which gave us strong confidence in the data collected through this study.

Attributes Evaluation.Table 5 provides BLEU scores  as a measure of agreement for annotated text attributes. Overall, the average BLEU scores for Croissant attributes (\(0.55\)) is higher than for Croissant-RAI attributes (\(0.41\)). This difference can be attributed to several factors. First, multiple RAI attributes require a free-form text answer, which is more likely to differ across annotations than categorical or short-answer attributes such as sc:name, sc:url, or sc:inLanguage. Second, Croissant attributes are more easily extractable from the dataset's page on Hugging Face or from the introduction of the corresponding publication, while Croissant-RAI attributes often require detailed studying of the publication to find relevant RAI information, such as demographic information.

Attributes' average BLEU scores also diverges based on their expected values. Attributes with Text as the expected value have an average BLEU score of \(0.4\) while Date/Datetime attributes have an average score of \(0.47\) Attributes with predefined values such as Language or Url, have an average score of \(0.59\), indicating higher agreement. For example, comparing annotations across attributes, we observe the highest BLEU scores for sc:license. This is largely attributable to the fact that, while being free-form text, there is less variety in the attribute's annotations and therefore more matching \(4\)-grams. The low BLEU score for the MathVisa dataset is due to one annotator providing the text of the license instead of its name, as instructed in the specification .

## 5 Limitations and Future Work

Croissant Format.While the Croissant metadata format provides a shared representation across various ML tools, platforms, and frameworks, certain challenges remain that should be addressed in future work. First, its structure may pose difficulties for users unfamiliar with the format, potentially hindering broader adoption. In the future, we plan to extend Croissant tools (e.g., the Croissant editor) and provide comprehensive documentation, as these are the primary means of making Croissant

[MISSING_PAGE_FAIL:10]