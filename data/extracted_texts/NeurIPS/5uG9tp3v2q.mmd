# Transformers on Markov data: Constant depth suffices

Nived Rajaraman * &Marco Bondaschi &Kannan Ramchandran &Michael Gastpar

UC Berkeley &Ashok Vardhan Makkuva

EPFL

Correspondence to nived.rajaraman@berkeley.edu.

###### Abstract

Attention-based transformers have been remarkably successful at modeling generative processes across various domains and modalities. In this paper, we study the behavior of transformers on data drawn from \(k^{}\)-order Markov processes, where the conditional distribution of the next symbol in a sequence depends on the previous \(k\) symbols observed. We observe a surprising phenomenon empirically which contradicts previous findings: when trained for sufficiently long, a transformer with a fixed depth and \(1\) head per layer is able to achieve low test loss on sequences drawn from \(k^{}\)-order Markov sources, even as \(k\) grows. Furthermore, this low test loss is achieved by the transformer's ability to represent and learn the in-context conditional empirical distribution. On the theoretical side, our main result is that a transformer with a single head and three layers can represent the in-context conditional empirical distribution for \(k^{}\)-order Markov sources, concurring with our empirical observations. Along the way, we prove that _attention-only_ transformers with \(O(_{2}(k))\) layers can represent the in-context conditional empirical distribution by composing induction heads to track the previous \(k\) symbols in the sequence. These results provide more insight into our current understanding of the mechanisms by which transformers learn to capture context, by understanding their behavior on Markov sources. Code is available at: https://github.com/Bond1995/Constant-depth-Transformers.

## 1 Introduction

Attention-based transformers have revolutionized the field of natural language processing (NLP)  and beyond , achieving significant performance gains across tasks like machine translation, text generation, and sentiment analysis. A key factor in their success is their ability to model sequences far more efficiently, and the ability to learn in-context .

To understand this capability, a canonical approach is to sample the input from a _\(k^{}\)-order Markov process_, where the next symbol's conditional distribution depends only on the previous \(k\) symbols. Recent studies  have investigated the ability of transformers to learn Markov processes and establish that learning happens in phases. The transformer eventually learns to represent the conditional \(k\)-gram model, which is the in-context MLE of the Markov process.

The results in  seem to suggest that for low depth transformers to learn Markov processes of order \(k\), it is essential that the number of heads scale linearly in \(k\). At first glance, this is a bit concerning - real world data generating processes often contain long-range dependencies. How is it that transformers succeed at capturing these kinds of long-range dependencies, while at the same time requiring so many heads to be able to capture the necessary context for \(k^{}\)-order Markov sources?To understand the nature of this phenomenon, we train low-depth transformers on \(k^{}\)-order Markov sources. These experiments result in two surprising empirical phenomena that seem to contradict previous findings: when trained for sufficiently long, \((i)\) a \(2\)-layer, \(1\)-head transformer can learn \(k^{}\)-order Markov processes for \(k\) as large as \(4\), \((ii)\) a \(3\)-layer, \(1\)-head transformer is able to achieve low test loss on sequences drawn from \(k^{}\)-order Markov sources, even as \(k\) grows to be as large as \(8\) (Fig. 3). In both cases, the values of \(k\) for which the models appear to learn \(k^{}\)-order Markov sources are much higher than those predicted in prior experiments . This discrepancy shows that our understanding of the mechanisms used by transformers to learn \(k^{}\)-order Markov processes is not complete and raises a broader question:

_What is the interplay between depth, number of heads and non-linearity in learning \(k^{}\)-order Markov processes?_

In this paper, we approach this question from the point of view of representation power, and provide some partial explanations toward the phenomena illustrated previously.

Our main contributions are as follows:

1. We show, rather surprisingly, that the standard transformer architecture with \(3\) layers and \(1\) head per layer is capable of representing the conditional \(k\)-gram model (Definition 1), and thereby learn \(k^{}\)-order Markov models in-context.
2. Along the way to building up to this result, we consider the simpler family of _attention-only transformers_ and show that they can represent the conditional \(k\)-gram model with \(_{2}(k+1)\) layers.
3. Under a natural assumption on the nature of the attention patterns learnt by the transformer, we then argue that for \(k 3\) attention-only transformers _need_ at least \( 1+_{2}(k-2)\) layers to represent a "\(k^{}\)-order induction head" (Definition 2). Empirically, transformers are observed to learn \(k^{}\)-order induction heads whenever they achieve small test error .

The last result is a consequence of a more general tradeoff between the number of layers, \(L\), and heads per layer, \(H\), an attention-only transformer requires to represent a \(k^{}\)-order induction head, under a natural assumption on the learnt attention patterns. In conjunction, these results also reveal the role of non-linearities (aside from the softmax in the attention) in the transformer architecture. In particular, it appears that layer normalization plays a critical role in the ability of constant-depth transformers to learn the conditional \(k\)-gram model. Together with the experimental results mentioned previously, these results paint a more comprehensive picture about the representation landscape of transformers in the context of \(k^{}\)-order Markov processes.

Notation.Scalars are denoted by italic lower case letters like \(x,y\) and Euclidean vectors and matrices in bold \(,,\), etc. The notation \(_{p q}\) (resp. \(_{p q}\)) refers to the all-zero (resp. all-one) matrix. When it is clear from the context, we omit the dimensions of a matrix. Define \( S\{1,2,,S\}\) for \(S\). \(()\) denotes the indicator function and \((S)\) denotes the uniform distribution over a set \(S\).

### Related work

There is a large body of active research focused on studying different aspects of transformer models . Our work closely relates to the aspects of understanding the representation power of

   &  & **Standard** \\  \(L\) & \(2\) & \(_{2}(k+1)\) & \(3\) \\  \(H\) & \(k\) & \(1\) & \(1\) \\  

Table 1: Each column in this table indicates that there is a transformer with \(L\) layers and \(H\) heads in the first layer which can represent the conditional \(k\)-gram model.2

Figure 1: \(k^{}\)-order Markov process for \(k=4\). The symbol \(X_{n+1}\) is sampled from the distribution \(P(|X_{n},X_{n-1},X_{n-2},X_{n-3})\) which only depends on the last \(4\) symbols (marked in red).

transformers, and in-context learning. [13; 14; 15] study the representation capabilities of transformers and show properties such as universal approximation and Turing-completeness. Viewing transformers as sequence to sequence models, [16; 17] study their ability to model formal languages and automata. Along more related lines to our work, [18; 19] present logarithmic depth transformer constructions for representing a \(k\)-hop generalization of the notion of an induction head . On the other hand the theoretical and mechanistic understanding of in-context learning  has received much attention lately [22; 23; 24; 25], focusing on different operating regimes and phases of learning. There are a few recent papers which study the behavior of transformers when trained on data generated from Markov processes, and generalizations thereof [5; 26]. In particular, [7; 8] study the optimization landscape of gradient descent in learning generalizations of Markov processes, and  present a study of how transformers learn to represent in-context \(k\)-gram models, focusing on different phases of learning.

## 2 Preliminaries

We provide the necessary background for Markov processes, the conditional \(k\)-gram model, and the transformer architecture.

### Markov processes

Markov processes are one of the widely used models in sequence modeling . The characterizing property of these processes is that at any time step, the future evolution is only influenced by the most recent states. More formally, a sequence \((X_{n})_{n 1}\) is a \(k^{}\)-order Markov process on a finite state space \([S]\) with the transition kernel \(P\), if surely,

\[PX_{n+1} X_{1},,X_{n}=PX_{n+1} X_{n-k+1}, ,X_{n}\]

This property allows us to capture the conditional distribution at any position using only its previous \(k\) symbols. This motivates the notion of a conditional \(k\)-gram, its empirical counterpart, defined for any sequence \((x_{1},,x_{n})\).

**Definition 1** (Conditional \(k\)-gram model).: _Given a sequence \((x_{1},,x_{n})\) of length \(n\) in \([S]^{n}\), the conditional \(k\)-gram model \(_{k}( x_{1},,x_{n})\) corresponds to the in-context estimate of the distribution over symbols conditioned on the last \(k\) symbols, i.e. for \(x[S]\),_

\[_{k}(x x_{1},,x_{n})^{n} (x_{i}=x,x_{i-1}=x_{n},,x_{i-k}=x_{n-k+1})}{_{i=k+1}^{n} (x_{i-1}=x_{n},,x_{i-k}=x_{n-k+1})},\]

which is defined only so long as the denominator is non-zero. This structure is illustrated in Figure 1(a). It is well known that the conditional \(k\)-gram in Eq. (1) with Laplace smoothing corresponds to the Bayes optimal estimate of the next symbol probability, when the data is drawn from fixed Markov process sampled from a prior distribution .

In our experiments, we will consider \(k^{}\)-order Markov kernels sampled from a Dirichlet prior with parameter \(\). Namely, the transition \(P(|X_{1}=i_{1},,X_{k}=i_{k})\) is sampled independently and uniformly on the \(S\)-dimensional simplex \(_{1}^{S}\), for each tuple \((i_{1},,i_{k})\).

### Transformer architecture

In this paper, we will consider variants of the standard transformer architecture in Figure 1(b) introduced in , with the goal to understand the role of depth and the non-linearities in the architecture. The simplest variant removes all the layer normalization and the (non-linear) feedforward layer, and is referred to as an _attention-only_ transformer. The \(L\)-layer \(1\)-head attention-only transformer with relative position encodings, operating on a sequence of length \(T\) is defined in Architecture 1.

The attention scores in layer \(\), \(\{_{n,i}^{()}:i n\}\), are computed as \(_{K}^{()}( _{j}^{()}+_{n-j}^{(),K}),_{Q}^{( )}_{j}^{()}:j[n]}\). The superscript \(()\) indicates the layer index, and the matrices \(_{K}^{()}\), \(_{Q}^{()},_{V}^{()}^{d d}\) capture the key, query and value matrices in layer \(\). Note that the attention-only transformer may include a feedforward layer with linear activations, i.e. a linear transformation. For representation purposes, this linear transformation can be combined with the projection matrix in the attention layer, allowing the feedforward layer to be omitted from the model. In the attention layer, we consider relative position encodings (the terms labeled in blue), which translates the key and value vectors depending on the relative position of the embedded symbol.

\[&\ n=1,2,,T;\ \ _{n}^{(1)}=(x_{n})^{d}.\] (Input embeddings) \[&}_{n}^{()}=_{i[n]} _{n,i}^{()}_{V}^{()}(_{i}^{( )}+}{{p_{n-i}}})^{d}, \] (Attention) \[&_{n}^{(+1)}=_{n}^{()}+}_{n}^{()},\] (Residual) \[&,\ _{n,i}^{()}=_{i}(\{_{K}^{()}(_{i}^{()}+p_{n-i}^{( ),K}),_{Q}^{()}_{n}^{()}:i[S]\} ).\] (Linear) \[&_{T}=_{T}^{(L+1)}+ ^{S},\] (Prediction)

The extension to \(H\) heads is straightforward, where in each transformer layer there are \(H\) attention layers in parallel, resulting in \(_{n}^{(,1)},,_{n}^{(,H)}^{d}\) for each \(n\). These vectors are concatenated and passed through a linear transformation \(_{O}^{()}:^{dH}^{d}\) which is the output of the attention layer. Finally, the output of the model after \(L\) layers is passed through a linear layer, which projects the \(d\)-dimensional embeddings back into \(^{S}\) and the resulting vector is passed through a non-linearity \(f\), usually a softmax, to result in the model's prediction of the next symbol probabilities. The theoretical results in this paper will choose \(f=()\).

## 3 Understanding the empirical behavior of transformers

The motivation for the present work comes from a series of experimental results, which challenge our current understanding of transformers in the context of learning Markov processes. Several works in the literature [7; 8; 6] have studied the ability of transformer models to learn \(k^{}\)-order Markov processes. The experimental results present in the literature suggest that in order for a \(2\) layer transformer model to be able to learn a randomly sampled Markov process of order \(k\), it is crucial for the number of heads in the first attention layer to scale linearly with the order, \(k\). In particular, the authors of  claim that in their experiments, "Single attention headed models could not achieve better performance than bigram (models)" in learning random \(k^{}\)-order Markov processes in-context. Similarly, the authors of  study a generalization of learning \(k\)th-order Markov processes to learning causal processes on degree \(k\) graphs. The theory and experiments pertain to \(2\)-layer \(k\) head transformers.

In Figure 3, we train \(2\) and \(3\)-layer transformers with a single head on data drawn from random Markov processes of various orders drawn from a Dirichlet prior. With \(2\) layers and a single head, we see that the model is able to learn even order-\(4\) Markov processes, and go beyond the simple order-\(1\) processes which were projected to be the limit of its ability to learn. Likewise, with \(3\) layers, transformers are able to go much further and learn order-\(8\) Markov processes, which was the largest value of \(k\) we evaluated on. These results contrast with our current understanding of how induction heads are realized in the parameter space [6; 8] - existing constructions which realize these attention patterns require \(k\) heads when the number of layers is \(2\), and it's unclear how to implement them with fewer heads. At a high level, each of the \(k\) heads play a critical role - where, loosely speaking, the \(i\)th-head looks back \(i\) positions in the sequence.

Building up to our main results, in the sequel, we study the simpler case of attention-only transformers where the feedforward layers and layer normalization are removed.

## 4 Warming up: Attention-only transformers

The study of attention-only transformers trained on Markov processes has garnered some attention in the prior literature. Notably, the authors of  study \(2\)-layer \(1\)-head attention-only transformers trained on data drawn from \(1^{}\)-order Markov processes whose parameters are drawn from a Dirichlet prior. The model is observed to learn a very specific behavior, known as an "induction head" , which in this setting is able to represent the conditional \(1\)-gram (Eq. (1)).

The induction head mechanism is composed of two layers where the first layer learns the attention pattern \(_{n,i}^{(1)}=(i=n-1)\), thereby allowing the model to capture information about the symbol at position \(n-1\) in the embedding vector at time \(n\). In the second layer, the attention layer picks out those indices \(n\) where \(x_{n-1}=x_{T}\), the final symbol in the sequence. At these positions, since \(x_{n-1}=x_{T}\), one would expect that the next symbol \(x_{n}\) is a good predictor of \(x_{T+1}\), and the model uses this information to predict the next symbol \(x_{T+1}\) according to its conditional empirical estimate, \(_{1}(x_{T+1}|x_{1},,x_{T})\), i.e. the conditional \(1\)-gram model.

**Theorem 1**.: _The conditional \(1\)-gram model can be represented by a \(2\)-layer and \(1\)-head attention-only transformer with embedding dimension \(d=3S+2\)._

Although a version of this result is proved in , we include a proof in Appendix A for completeness.

**Remark 1**.: _In Theorem 1 and other results to follow, we de-emphasize the role of the bit-precision to which the transformer is implemented. That said, note that when the constructions in Theorems 1 to 3 are implemented to \(O((T))\) bits of precision, the representation results are realized up to an additive \(O(1/T)\) error._

The ideas in Theorem 1 readily extend to representing the conditional \(k\)-gram model, by instead using \(k\) heads in the first layer. The \(j\)th head learns the attention pattern \(_{n,i}^{(1)}=(i=n-j)\) and

Figure 3: Gap with the optimal test loss for \((a)\) a \(2\)-layer, \(1\)-head transformer model (above), and \((b)\) a \(3\)-layer, \(1\)-head transformer (below), averaged over \(3\) runs for each \(k\). The models learn the conditional \(k\)-gram model for randomly sampled \(k\)-th order Markov processes, even for large \(k\).

concatenating the outputs of the heads, the model learns to aggregate information about \(x_{n},,x_{n-k}\) in the embedding vector at time \(n\). The second layer realizes what is best described as a "\(k^{}\)-order " induction head, where the model learns to pick out those positions \(n\) where for every \(j[k]\), \(x_{n-j}=x_{T-j+1}\), i.e. the history of length \(k\) at those positions match the final \(k\) symbols in the input sequence ( see Figure 4). This mechanism is also referred to as a long-prefix induction head .

**Definition 2** (Higher-order induction head).: _A \(1\)-head attention layer is said to realize a \(k^{}\)-order induction head if on any sequence \((x_{1},,x_{T})[S]^{T}\), for any fixed \(n T\), as a function of the input sequence, \(_{n,T}\) is maximized if and only if \(x_{n-j}=x_{T-j+1}\) for every \(j[k]\)._

\(k^{}\)-order induction heads generalize the concept of an induction head , and keep track of the positions \(i n\) where there is a perfect occurrence of the final \(k\) symbols in the sequence. Such attention patterns are immediately useful in representing the conditional \(k\)-gram - increasing the temperature within the softmax of this attention layer results in an attention pattern which converges to the uniform distribution over those positions where the final \(k\) symbols \(x_{T-k+1},,x_{T}\) are seen previously in the sequence. Loosely, this allows the model to "condition" on the last \(k\) symbols in the sequence. With \(k\) heads, the model can aggregate information from the previous \(k\) positions and implement a \(k^{}\)-order induction head, which leads to the following result. A full proof is discussed in Appendix A.1.

**Theorem 2**.: _The conditional \(k\)-gram model can be represented by an attention-only transformer with \(2\) layers, \(k\) heads and embedding dimension \(d=(k+2)S+k+1\)._

While this result is positive, it suggests that a \(2\)-layer transformer requires approximately \(k\) times as many parameters to be able to represent the conditional \(k\)-gram model. The first result we prove is that increasing the depth of the model is exponentially more beneficial, in that a transformer with \(O((k))\) depth can estimate in-context \(k\)-grams.

**Theorem 3**.: _The conditional \(k\)-gram model can be represented by an attention-only transformer with relative position encodings, with \(L=_{2}(k+1)\) layers and \(1\) head per layer. The embedding dimension is \( 2k(S+1)+S\)._

With \(2\) layers and \(k\) heads, the transformer aggregates information about each of the previous \(k\) positions one step at a time through the \(k\) heads. However, with \(((k))\) layers, the same task can be done far more efficiently. In the first attention layer, the model aggregates information about the current and previous position. Namely, using the relative position embeddings, \(_{n}^{(2)}\) is chosen as a linear combination of \(_{n}^{(1)}=(x_{n})\) and \(_{n-1}^{(1)}=(x_{n-1})\). This allows the embedding at position \(n\) to aggregate information about \(x_{n}\) and \(x_{n-1}\). In the same vein, in the second attention layer, the model aggregates information from \(_{n}^{(2)}\) and \(_{n-2}^{(2)}\) in \(_{n}^{(3)}\); the former has information about \(x_{n}\) and \(x_{n-1}\), and the latter has information about \(x_{n-2}\) and \(x_{n-3}\). This expands the "window" of \(x_{i}\)'s on which \(_{n}\) depends on to size \(4\). In the \(^{}\) layer, the model aggregates information from \(_{n}^{()}\) and \(_{n-2^{}}^{()}\) which allows \(_{n}^{(+1)}\) to effectively depend on the \(x_{i}\)'s in a window of size \(2^{+1}\) starting at position \(n\), namely \(x_{n}\), \(,x_{n-2^{+1}+1}\). In the final layer, the embedding at position \(i\), \(_{i}^{(L)}\) for \(L=_{2}(k+1)\) depends on \(x_{n},x_{n-1},,x_{n-k}\). In the last layer, the model can realize the dot-product \(_{K}^{(L)}_{n}^{(L)},_{Q}^{(L)}_{T}^{(L)} =_{j=1}^{k}(x_{n-j}=x_{T-j+1})\) by choosing the key and query vectors appropriately. By increasing the temperature in the attention softmax, the attention pattern realized is the uniform distribution on values of \(n\) such that \(x_{n-j}=x_{T-j+1}\) for every \(j[k]\), i.e., a \(k^{}\)-order induction head. The full proof of this result is provided in Appendix B.

Figure 4: \(k^{}\)-order induction head for \(k=2\). The attention pattern \(_{T,n}\) is maximized for those values of \(n\) at which \(x_{T-j+1}=x_{n-j}\) for all \(j[k]\). These are the positions where the \(k\)-length prefix at those positions matches with the last \(k\) symbols in the sequence.

While this is a promising step toward understanding the behavior transformers exhibit in Figure 3, showing that depth plays an important role in their ability to represent conditional \(k\)-gram models, the picture is still not complete. The experimental results in Section 3 do not preclude the possibility that a transformer might not even require logarithmic depth to be able to learn \(k^{}\)-order Markov processes approximately. In the next section, we will study constant-depth transformers and establish a rather surprising positive result about the representation power of this class of models in capturing conditional \(k\)-grams.

## 5 Understanding the role of non-linearity: Constant-depth constructions

In the previous section, we saw how the transformer uses the power of depth to learn conditional \(k\)-grams far more efficiently. In particular, every additional attention layer effectively doubles the window of positions \(i=n-1,n-2,\) which the model has access to information about at the current time \(n\). By composing \(L=((k))\) attention layers, the model is able to collect enough information within the output embedding \(_{n}^{(L+1)}\) to be able to realize a \(k^{}\)-order induction head in the next layer. In this section, we prove that adding non-linearity to the architecture, in the form of layer normalization, can significantly change the mechanism in which the transformer realizes this \(k^{}\)-order head. In particular, there are constant depth architectures which allow a \(k^{}\)-order induction head to be realized, surpassing the logarithmic depth attention-only constructions.

Modification to the standard transformer architecture.To simplify the proof of our main result, we will consider a subtle modification to the standard transformer architecture, which is presented in Architecture 2 and Figure 4(a). We will remove the first layer norm prior to the multi-head attention and move the second layer norm to after the feed-forward network. It is important to note that Theorem 4 holds even for the architecture presented in Figure 1(b), which is the architecture we evaluate empirically. The modification we present in Figure 4(a) allows the construction to be simpler and makes it much easier to convey the key intuition. The main difference compared to the attention-only design presented in Architecture 1 is the addition of layer normalization and a feedforward layer in the for-loop over \(n[T]\) for each transformer layer \(\). The differences between Architectures 2 and 1 are emphasized in blue.

**Theorem 4**.: _Conditional \(k\)-grams can be represented by a transformer with \(3\) layers, \(1\) head per layer, relative position encodings and layer normalization. The embedding dimension is \(O(S)\)._

**Remark 2**.: _Although the proof stated does not bound the approximation error arising from a finite bound on the bit precision of the transformer, in theory, it should suffice to have \(((T)+k)\) bits per parameter for the statement of Theorem 4 to go through with an \(O(1/T)\) additive approximation error. The main point is that none of the weights of the model exceed \((k)\) and with \((T)\) additional bits per parameter, the approximation error scales as \(O(1/T)\)._

Figure 5: _Disassembling the constant-depth construction. The first two layers are critical in the modelâ€™s ability to capture information from the previous \(k\) positions. Layer normalization plays a critical role in in the \(3^{}\) layer which realizes a \(k^{}\)-order induction head._

\[}_{n}^{()} =_{n}^{()}+_{i[n]}_{n,i}^{( )}_{V}^{()}(_{i}^{()}+_{n-i}^{(),V} )^{d},\] (Attention + Residual \[{}_{1}\] ) \[_{n}^{()} =_{2}^{()}(_{1}^{() }}_{n}^{()})^{d},\] (FFN) \[_{n}^{()} =_{n}^{()}-_{d 1}}{} ^{d},\] (LN) \[_{n}^{(+1)} =_{n}^{()}+}_{n}^{()}^{d},\] (Residual \[{}_{2}\] )

Architecture 2: Modified transformer architecture. The computations above are carried out for each \(n[T]\) in each layer \([L]\). In the layer normalization step (LN), the feature mean \(\) is defined as, \(_{i([d])}e_{i}^{d}_{n}^{ ()}\) and the feature variance \(^{2}=_{i([d])}e_{i}^{d}, _{n}^{()}^{2}-^{2}\).

### Proof sketch

In the attention-only transformer with \(2\) layers and \(k\) heads, the model is able to keep track of where the final \(k\) symbols in the sequence appeared previously (i.e., a \(k^{}\)-order induction head) by, loosely, using each head to keep track of the occurrences of one of the final \(k\) symbols. On the other hand, with the benefit of more depth, with \(L=((k))\) layers, the model is able to collect enough information within the output embedding \(_{n}^{(L+1)}\) to be able to realize the same behavior. However, neither of these constructions scale down to the case when the depth and number of heads of the transformer are both constants independent of \(k\). We provide a brief sketch of the construction below.

Recall that a \(k^{}\)-order induction head keeps track of the indices \(i\) such that \( j[k],\ x_{i-j}=x_{n-j+1}\). Defining \(_{i}_{j=1}^{k}2^{j}e_{x_{i-j+1}}\), notice that the condition \(\{ j[k],\ x_{i-j}=x_{n-j+1}\}\) can equivalently be captured by writing \(\{_{i-1}=_{n}\}\). This true because of the fact that the binary representation of any integer is unique. Furthermore, these vectors, up to scaling, can be realized by softmax attention (namely, \(_{n,n-i} 2^{i}\) for \(1 i k\)).

With this step, finding occurrences of the last \(k\) symbols in the input sequence boils down to realizing an attention pattern in the second layer, \(_{n,i}^{(2)}\), which is maximized whenever \(_{i-1}=_{n}\). While dot-product attention naively encourages those values of \(i\) for which \(_{i-1}\) and \(_{n}\) are "similar" to each other, a qualitative statement is lacking. In general, it will turn out to that a different measure of similarity is necessary within the softmax to be able to encourage those values of \(i\) for which these vectors match. This is where the role of layer-normalization comes in.

Instead of the usual dot-product, suppose the attention mechanism in the second layer was,

\[_{n,i}^{(2)}\!\!(-\|_{i-1}}{\|_{i-1}\|_{2}}-_{n}}{\|_{n}\|_{2}}\|_{2 }^{2}),\] (1)

where \(\) is the temperature parameter. Then, as the temperature \(\) grows, the attention pattern essentially focuses on those values of \(i\) for which \(_{i}/\|_{i-1}\|_{2}=_{n}/\|_{n}\|_{2}\). With this attention pattern, we are thus very close to the statement we wanted to check, (\(_{i-1}_{n}\)). As it turns out, for the special structure in the \(_{i}\)'s considered (dyadic sums of one-hot vectors), we may write down,

\[_{i-1}=_{n}_{i-1}/\|_{i-1}\|_{2}=_{n}/\|_{n}\|_{2}.\]

A quantifiable equivalence is provided in Lemma 1.

Realizing \(L_{2}\)-norm attention (eq. (1)).Observe the equivalence,

\[_{i-1}}{\|_{i-1}\|_{2}},_{n}}{\|_{n}\|_{2}}=1-\|_{i-1}}{\|_{i-1} \|_{2}}-_{n}}{\|_{n}\|_{2}}\|_{2}^{2}\] (2)

Taking a softmax on both sides, notice that the RHS (up to an additive constant) is the \(L_{2}\)-norm based attention, while the LHS is the usual dot-product attention between \(_{i-1}/\|_{i-1}\|_{2}\) and \(_{n}/\|_{n}\|_{2}\). Thus on unit-normalized vectors, \(L_{2}\)-norm attention and dot product attention are but the same.

While the first layer of the transformer computes the \(_{i}\)'s by a weighted summation, layer normalization fills in the last missing piece of the puzzle which is to normalize them to unit norm. This is a consequence of defining the embedding vectors appropriately, as we discuss more in Appendix C.1.

From this step, realizing the actual conditional \(k\)-gram model follows readily. In particular, as the temperature \(\) in the attention grows, the attention pattern zooms in on indices \(i_{n}\{k+1 i n: j[k],x_{i-j} =x_{n-j+1}\}\) in the last layer. The value vectors at this step are the one-hot encoding of \(x_{i}\); putting everything together, the logits realized by the transformer are,

\[_{T}(x_{T+1})=_{n}|}_{i_{ n}}(x_{i}=x_{T}),\] (3)

which is the conditional \(k\)-gram model (eq. (1)).

While the transformer construction described above only requires two layers, the actual construction we propose differs slightly and has an additional layer. The first two layers of the transformer respectively compute \(_{i}\) and \(_{i-1}\) which are added to the embedding vector at time \(i\). This is important because we need to test whether \(_{i-1}_{n}\) and not whether \(_{i}_{n}\) or \(_{i-1}_{n}\).

**Summary.** The construction can be summarized as follows: the first layer computes \(_{n}=_{j=1}^{k}2^{j-1} e_{x_{n-j}}\) by choosing appropriate value vectors and relative position embeddings to realize the attention pattern \(_{n,n-i} 2^{i}(1 i k)\). The layernorm that follows subsequently can be replaced by RMSnorm, by a simple trick which we discuss in Appendix C.1, resulting in \(_{n}/\|_{n}\|_{2}\) to be appended to the embedding at time \(n\). Using a very similar construction, layer \(2\) computes \(_{n-1}/\|_{n-1}\|_{2}\), which is added to the embedding at time \(n\). Finally, in the last layer, the dot-product \(_{i-1}/\|_{i-1}|_{2},_{n}/\|_{n}\|_{2}\) defines the attention score, and as the temperature \(\) grows, the pattern converges to \((_{n})\). Choosing the value vectors in this layer appropriately gives eq. (3).

## 6 Lower bounds on transformer size

In this section, we study the limits of how shallow a transformer can be made while still capturing conditional \(k\)-grams. The first result we establish in this vein is a lower bound against \(1\)-layer transformers showing that their expressive power is too limited unless the embedding dimension or number of heads scale near-linearly in \(T\).

**Theorem 5**.: _Consider any \(1\)-layer transformer with layer normalization and feedforward layers, where all the coordinates of the embedding vectors and unnormalized attention scores are computed with \(p\) bits of precision. If the transformer is able to compute the conditional \(3\)-gram on inputs drawn from \(\{0,1,2\}^{T}\) to within an additive error of \(1/3T\), then \(2pH+dp+2 T/3\)._

Choosing the bit precision to be \(p=O((T))\), this implies that for transformers with \(1\) layer, the sum of the number of heads and the embedding dimension must be at least \((T/(T))\), in order to represent conditional \(3\)-grams to within an additive error of \(1/3T\).

### Conditional lower bounds on attention-only transformers

While the previous section shows that \(1\)-layer transformers have fairly limited representation power, it is not immediately clear how whether any of these issues are present with transformers with more layers. Indeed, as we discussed in Section 4, an attention-only transformer with \(O(_{2}(k))\) layers and \(1\) head per layer can represent conditional \(k\)-grams on its input sequences. With the addition of non-linearities, Theorem 4 shows that the model can represent conditional \(k\)-grams using just a constant number of layers. In this section, we try to understand the gap between these two results and prove conditional lower bounds on the size of attention-only transformers which do not have non-linearities arising from layer normalization.

We prove conditional lower bounds under some natural assumptions on the nature of the attention patterns learnt by the transformer. To motivate these assumptions, consider the experiment in Figure 6, where we train an attention-only transformer with \(2\) layers and \(1\) head, on order-\(1\) Markov processes. At test-time, we plot the attention patterns learnt in the first layer of the model on test sequences. Notice that the attention pattern learnt by the model at layer \(1\) is largely independent of the input sequences themselves and only depends on the position.

**Assumption 1**.: _In an \(L\)-layer attention-only transformer with \(H\) heads per layer, assume that layers \(=1,2,,L-1\) and heads \(h[H]\) realize an attention pattern where \(_{n,i}^{(,h)}\) only depends on the positions \(n\) and \(i\) and on \(\) and \(h\), but not on the input sequence \(x_{1},,x_{T}\)._Rather than proving the size lower bound depending on the transformers ability to represent the conditional \(k\)-gram itself, we consider a simplification and assume that the goal of the model is to represent a \(k^{}\)-order induction head (Definition 2) in the last layer. Although learning a \(k^{}\)-order induction head is not strictly necessary for the transformer to be able to represent conditional \(k\)-grams, note that every construction we have considered so far (cf. Theorems 1 to 4) go through this mechanism to realize the conditional \(k\)-gram model. Likewise, for other related problems, such as the causal learning task in , the causal structure is captured by an extension of the \(k^{}\)-order induction head to general causal graphs. Our main lower bound is the following result.

**Theorem 6**.: _Consider an \(L\)-layer transformer with \(h_{}\) heads in layer \(L\). Assuming the transformer satisfies Assumption 1, if \(_{=1}^{L-1}(H_{}+1) k-2\), the attention pattern in layer \(L\) cannot represent a \(k^{}\)-order induction head._

While this lower bound is not unconditional, meaning that it does not directly imply that the transformer cannot represent conditional \(k\)-grams, it is important to understand the interpretation of this result: attention-only transformers which somehow break through this barrier need to use a significantly different mechanism to realize the conditional \(k\)-gram model.

Theorem 6 implies that under Assumption 1, a \(2\)-layer attention-only transformer with \(1\) head cannot realize a \(k^{}\)-order induction head for any \(k 4\). Likewise, under the same assumption, a \(3\)-layer attention-only transformer with \(1\) head cannot realize a \(k^{}\)-order induction head for any \(k 6\). These results give more weight to the experiment in Figure 3 where we observe that a \(2\)-layer transformer learns a \(k^{}\)-order Markov process for \(k=4\) and a \(3\)-layer transformer learns a \(k^{}\)-order Markov process for \(k=8\), and show that non-linearities in the architecture allow the transformer to break past the size barriers in Theorem 4.

## 7 Conclusion

We observe empirically that \(2\) and \(3\) layer transformers are able to learn \(k^{}\)-order Markov chains for much higher values of \(k\) than previously anticipated. We show there are \(O((k))\)-layer constructions of attention-only transformers which are able to learn the conditional \(k\)-gram model, which is the in-context MLE of the Markov model. With non-linearities in the model, we show that a \(3\)-layer \(1\)-head transformer is capable of representing the same. We show that \(1\)-layer transformers cannot represent conditional \(k\)-grams for any \(k 3\) unless the number of heads or embedding dimension scale almost linearly in \(T\). We also prove a conditional lower bound on the depth and number of heads of attention-only transformers to represent \(k^{}\)-order induction heads, under an assumption on the realized attention patterns.