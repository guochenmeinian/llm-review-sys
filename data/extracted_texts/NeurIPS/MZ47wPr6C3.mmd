# On Sparse Canonical Correlation Analysis

Yongchun Li

University of Tennessee

ycli@utk.edu

&Santanu S. Dey

Georgia Tech

santanu.dey@isye.gatech.edu

&Weijun Xie

Georgia Tech

wxie@gatech.edu

###### Abstract

The classical Canonical Correlation Analysis (CCA) identifies the correlations between two sets of multivariate variables based on their covariance, which has been widely applied in diverse fields such as computer vision, natural language processing, and speech analysis. Despite its popularity, CCA can encounter challenges in explaining correlations between two variable sets within high-dimensional data contexts. Thus, this paper studies Sparse Canonical Correlation Analysis (SCCA) that enhances the interpretability of CCA. We first show that SCCA generalizes three well-known sparse optimization problems, sparse PCA, sparse SVD, and sparse regression, which are all classified as NP-hard problems. This result motivates us to develop strong formulations and efficient algorithms. Our main contributions include (i) the introduction of a combinatorial formulation that captures the essence of SCCA and allows the development of approximation algorithms; (ii) the establishment of the complexity results for two low-rank special cases of SCCA; and (iii) the derivation of an equivalent mixed-integer semidefinite programming model that facilitates a specialized branch-and-cut algorithm with analytical cuts. The effectiveness of our proposed formulations and algorithms is validated through numerical experiments.

## 1 Introduction

The Canonical Correlation Analysis (CCA), proposed by H. Hotelling , aims to identify the correlations between two sets of multivariate variables based on their covariance. Since then, CCA has become a powerful statistical technique used for multivariate data analysis, with its applications across diverse fields such as computer vision , natural language processing , and speech analysis . Despite its popularity, CCA can encounter challenges in explaining correlations between two variable sets within high-dimensional data contexts, such as genomic datasets . In contrast, Sparse Canonical Correlation Analysis (SCCA), which seeks sparse linear combinations of these variable sets, offers substantially enhanced interpretability .

Formally, this paper studies the SCCA problem:

\[v^{*}:=_{^{n},^{m}}\{^{ }:^{} 1,^{}  1,\|\|_{0} s_{1},\|\|_{0} s_{2}\},\] (SCCA)

where \(s_{1} n\), \(s_{2} m\) are positive integers and \(&\\ ^{}&\) denotes a covariance matrix of \((n+m)\) random variables. Specifically, \(\) and \(\) are the covariance matrices of the \(n\) and \(m\) random variables, respectively, and \(^{n m}\) is the cross-covariance matrix between \(n\) and \(m\) random variables. Hence, \(&\\ ^{}&\), \(\), \(\) are positive semidefinite matrices of size \((n+m),n\), and \(m\), respectively. Here, matrices \(\), \(\) can be singular, i.e., some random variables may be dependent on others. In fact, the covariance matrices \(,\) are often low-rank, especially within the high-dimension low-sample size data context (see, e.g., the gene expression data in ).

The SCCA problem generalizes three widely-studied sparsity-constrained optimization problems as special cases, which are sparse PCA [2; 13; 27], sparse SVD [28; 41], and sparse regression [22; 3]. To be specific, when \(n=m\), \(s_{1}=s_{2}\), \(,\) are identity matrices, and \(\) is a positive semidefinite matrix, SCCA reduces to the classic sparse PCA problem; when \(,\) are identity matrices, SCCA becomes the sparse SVD problem; and when \(\) is rank-one, Section 3 shows that SCCA is equivalent to two sparse linear regression subproblems.

### Main contributions

SCCA is generally NP-hard, given that its special cases, sparse PCA, sparse SVD, and sparse regression are all classified as NP-hard problems. We are motivated to develop efficient formulations and algorithms for SCCA through a mixed-integer optimization lens. The main contributions, along with the structure of the remainder of this paper, are the following:

1. In Section 2, we present an exact semidefinite programming (SDP) reformulation and derive a closed-form optimal value of classic CCA problem. We also develop an equivalent combinatorial formulation of SCCA, which allows the development of approximation algorithms;
2. When the covariance matrix \(&\\ ^{}&\) is low-rank, Section 3 studies the complexity of two special cases of SCCA. This motivates us to develop a polynomial-time exact algorithm of complexity \((n^{3}+m^{3})\) for solving SCCA to global optimaality when the sparsity levels (i.e., \(s_{1}\) and \(s_{2}\)) meet or exceed the ranks of \(\) and \(\);
3. Section 4 derives an equivalent mixed-integer SDP (MISDP) reformulation for SCCA. When applying the Benders decomposition approach, instead of solving the large-scale SDPs, we design a customized branch-and-cut algorithm with closed-form cuts, which can successfully solve SCCA to optimality; and
4. Section 5 numerically test the proposed formulations and algorithms. It is noted that our polynomial-time exact algorithm can solve real-world instances with \(n=19,672\) and \(m=2,149\) variables in seconds, provided that both \(s_{1}\) and \(s_{2}\) are at least the ranks of \(\) and \(\).

Our analyses and results can be extended to SCCA with multiple pairs of basis vectors \((,)\), allowing for a more flexible and comprehensive exploration of correlations among data sets. The detailed formulations of multiple SCCA, along with the computational results, are provided in Appendix F.

### Relevant literature

_SCCA._ To the best of our knowledge, the work  was the first paper that introduced the concept of SCCA to select only small subsets of variables to better explain the relationship between many genetic loci and gene expression phenotypes. A handful subset of features enhances interpretability, a desirable property, especially in complex data analysis, which has been successfully demonstrated in Sparse PCA . To obtain sparse canonical loadings \((,)\),  first applied elastic net penalty to the classical CCA via an iterative regression procedure. In a seminal work on SCCA , the authors proposed a rigorous formulation by enforcing the \(_{1}\) constraints on variables \((,)\) and developed a penalized matrix decomposition method to solve the penalized CCA problem. Then, extensive research has focused on various penalty norm functions to obtain sparse canonical loadings (see, e.g., [20; 26; 39; 42; 10]). In particular,  penalized multiple canonical loadings by \(_{1}\) norm and computed the sparse solution by the linearized Bregman method. It should be noted that under the assumption that the leading canonical loadings are sparse, [7; 17; 18] established theoretical guarantees of iterative approaches for estimating sparse solutions. Another research direction in SCCA introduced penalty functions based on group structural information of input data and developed group SCCA methods [29; 30]. For a comprehensive overview of CCA and SCCA methods, we refer readers to the survey by  and the references therein. These approaches, however, do not strictly enforce the exact sparsity requirement but only approximate the sparsity requirement (i.e., the \(_{0}\) norm) by a convex function. Another relevant work  introduced binary variables to recast SCCA as a mixed-integer nonconvex program under the assumption of positive definite matrices \(,\), based on which they designed a branch-and-bound algorithm. Different from the literature, our work does not require positive definiteness assumption of matrices \(,\), and we are able to obtain mixed-integer conic and semidefinite programming reformulations, allowing for better exact and approximation algorithms.

_Connections to and differences with sparse PCA and sparse SVD._ Analogous to SCCA, both sparse PCA [13; 25] and sparse SVD  select small subsets of variables to improve the interpretability of dimensionality reduction methods: PCA and SVD. Considerable investigation has been conducted on solving sparse PCA and sparse SVD from three angles: convex relaxations [12; 13; 14], approximation algorithms [6; 9; 28], and exact algorithms [2; 27; 28]. As mentioned before, in sparse PCA and sparse SVD, the covariance matrices \(,\) are identity. Such a setting dramatically simplifies the subset selection problems of sparse PCA and sparse SVD compared to that of SCCA, as in these problems, it suffices to focus on the selection of a submatrix of the matrix \(\). Specifically, it is shown in [11; 27; 35] that sparse PCA reduces to selecting a principal submatrix of \(\) to maximize the largest eigenvalue(s) and sparse SVD reduces to selecting a possibly non-symmetric submatrix of \(\) to maximize the largest singular value(s) . Quite differently, the combinatorial reformulation (1) of SCCA aims to simultaneously select a sized-\((s_{1} s_{1})\) principal submatrix of \(\), a sized-\((s_{2} s_{2})\) principal submatrix of \(\), and a sized-\((s_{1} s_{2})\) submatrix of \(\). These fundamental differences in the underlying formulations of sparse PCA and sparse SVD preclude the direct application of their existing algorithms to the SCCA.

**Notations:** The following notation is used throughout the paper. We use bold lower-case letters (e.g., \(\)) and bold upper-case letters (e.g., \(\)) to denote vectors and matrices, respectively, and we use corresponding non-bold letters (e.g., \(x_{i}\)) to denote their components. We let \(^{n},^{n}_{+},^{n}_{++}\) denote the set of all the \(n n\) symmetric real matrices, the set of all the \(n n\) symmetric positive semidefinite matrices, and the set of all the \(n n\) symmetric positive definite matrices, respectively. We let \(\) denote the identity matrix and let \(\) denote the vector or matrix with all-zero entries. We let \(^{n}_{+}\) denote the set of all \(n\)-dimensional nonnegative vectors. We let \([n]=\{1,2,,n\}\), \([s,n]=\{s,s+1,,n\}\). Given a matrix \(^{n m}\) and two subsets \(S[n]\), \(T[m]\), we let \(^{}\) denote the pseudo inverse of matrix \(\), let \(_{S,T}\) denote a submatrix of \(\) with rows and columns indexed by sets \(S,T\), respectively, and let \((_{S,T})^{}\) denote the pseudo inverse of submatrix \(_{S,T}\). For a set \(S\) and an integer \(k\), we define the set \(S+k=\{i+k|i S\}\). Given a vector \(^{n}\) and a subset \(S[n]\), we let \(_{S}\) denote a subvector of \(\) in the subset \(S\). We define \([]_{+}=\{,0\}\). We let \(_{}()\) denote the largest singular value function and let \(_{}()\) denote the largest eigenvalue value function.

## 2 A combinatorial reformulation of SCCA

This section introduces an equivalent combinatorial optimization reformulation of SCCA. This reformulation serves as the foundation for developing two effective approximation algorithms.

### An exact semidefinite programming representation of CCA

To begin with, let us focus on the classic CCA problem, which refers to SCCA without zero-norm constraints, as defined below:

\[_{^{n},^{m}}\{^{} :^{} 1,^{} 1 \}.\] (CCA)

This formulation of CCA can be regarded as a quadratically constrained quadratic program concerning the variables \(\\ ^{n m}\). We next define three-block matrices of size \((n+m)\) below that aid in the presentation of our results.

\[}=&/2\\ ^{}/2&,\ \ }=& \\ &,\ \ }=&\\ &.\]

By introducing a size-\((n+m)\) matrix variable \(=\\ \\ ^{}\) and removing the rank-one constraint on \(\), we can obtain an SDP relaxation of (CCA), as described below

\[_{^{m+n}_{+}}\{(}):(}) 1, (}) 1\}.\] (SDP Relaxation)Next, let us present a key lemma regarding properties of block matrices being positive semidefinite, fundamental for reformulating the SCCA.

**Lemma 1** (): _For any symmetric matrix \((&\\ ^{}&)^{n+m}\), the followings are equivalent:_

1. _The block matrix is positive semidefinite;_
2. \(_{+}^{n}\)_,_ \((-^{})=\)_,_ \(-^{}^{}_{+}^{m}\)_; and_
3. \(_{+}^{m}\)_,_ \((-^{})^{}=\)_,_ \(-^{}^{}_{+}^{n}\)_._

Inspired by Lemma 1, we hereby establish the equivalence between CCA and its SDP Relaxation. Remarkably, both of these problems achieve the same optimal value, namely \(_{}(^{}}^{}})\).

**Proposition 1**: _For the CCA problem, we have the following results._

1. _Both CCA and its SDP Relaxation have an optimal value_ \(_{}(^{}}^{}})\)_;_
2. _A pair of optimal solutions_ \((^{*},^{*})\) _to CCA satisfies_ \[^{*}=^{}},\;\;^{*}=^{} },\] _where_ \(^{n},^{m}\) _denote a pair of leading singular vectors of matrix_ \(^{}}^{}}\)_; and_
3. _An optimal solution_ \(^{*}\) _to the SDP Relaxation is_ \[^{*}=^{*}\\ ^{*}^{*}\\ ^{*}^{}.\]

_Proof._ See Appendix A.1. \(\)

Proposition 1 motivates the following observation on the optimal values of CCA and SCCA.

**Observation 1**: _The optimal value of CCA is upper bounded by \(1\), so is the optimal value of SCCA._

It is noteworthy that the results presented in Proposition 1 are established through a distinct methodology. This methodology leverages the positive semidefinite condition of block matrices, as shown in Lemma 1, and incorporates duality theory. This approach differs from most prior research , which proved Part (i) of Proposition 1 by relying on the singular value decomposition and assuming that matrices \(\) and \(\) are positive definite (i.e., full rank). To the best of our knowledge,  showed parts (i) and (ii) of Proposition 1 for a special low-rank CCA problem, where the authors assumed that the covariance matrices are defined as \(=^{}\), \(=^{}\), and \(=^{}\). Remarkably, Proposition 1 extends this result to a more general scenario where \(\) and \(\) are not constrained to be strictly positive definite and \(\) is not constrained to directly depend on \(,\), allowing for rank deficiencies and flexible data structure.

### An equivalent formulation of SCCA

In this subsection, we transform SCCA into a combinatorial optimization problem, according to the insights provided by Proposition 1.

**Theorem 1**: _SCCA is equivalent to the following combinatorial optimization:_

\[v^{*}=_{S_{1}[m],|S_{1}| s_{1},S_{2}[n],|S_{2}|  s_{2}}\{_{}(_{S_{1},S_{1}})^{}} _{S_{1},S_{2}}_{S_{2},S_{2}})^{}})\}.\] (1)

_Proof._ See Appendix A.2. \(\)

The combinatorial formulation (1) presents significant computational difficulties when attempting to solve SCCA. The primary obstacles are two-fold: first, simultaneously selecting submatrices from the matrices \(,,\) requires a sophisticated optimization across multiple dimensions. Second,the selection criterion is particularly complex, as it involves optimizing the largest singular value of the product of the selected submatrix of \(\) and the square root of pseudo-inverse submatrices of \(\) and \(\). These complexities necessitate effective optimization solution procedures to address the high-dimensional and non-convex nature of the problem.

Motivated by Theorem 1, we customize the greedy and local search algorithms for SCCA (1) that has been widely used in the literature to solve special cases of SCCA, such as sparse PCA and sparse SVD in literature (see, e.g., [27; 28]). The detailed implementations can be found in Appendix B.

## 3 Low-rank SCCA

In practice, it is common that the sample covariance matrix \(&\\ ^{}&\) exhibits low-rank characteristics. This phenomenon is especially prominent when dealing with high-dimensional, low-sample size data, e.g., the real gene expression data in . In this section, we study two special cases of low-rank SCCA and their computational complexities. Specifically, we develop a polynomial-time exact algorithm of complexity \((n^{3}+m^{3})\) for solving SCCA to global optimality when sparsity levels (i.e., \(s_{1}\) and \(s_{2}\)) exceed or equal the ranks of \(\) and \(\). Besides, we recast SCCA into mixed-integer convex quadratic programming when matrix \(\) is rank-one.

### Special Case I: SCCA with low-rank covariance matrices

In this section, we show that the computational complexity of SCCA is contingent upon the ranks of the covariance matrices \(\) and \(\). To be more precise, when the sparsity level \(s_{1}\) (or \(s_{2}\)) is equal to or greater than the rank \(r\) (or \(\)) of the covariance matrix \(\) (or \(\)), the imposition of a zero-norm constraint over \(\) (or \(\)) in SCCA becomes redundant. Consequently, lower ranks in the covariance matrices correspond to better computational complexity in solving SCCA.

**Theorem 2**: _Suppose \(r=()\) and \(=()\), then SCCA takes a complexity of \((n^{r-1}m^{-1}+n^{r-1}+m^{-1})\). The following results hold:_

1. _When_ \(s_{1} r\) _and_ \(s_{2}\)_, the SCCA problem is equivalent to CCA, i.e.,_ \[v^{*}=_{^{n},^{m}}\{^{ }:^{} 1,^{}  1\};\] (2)
2. _When_ \(s_{1} r\) _and_ \(s_{2}<\)_, the SCCA problem can be reduced to_ \[v^{*}=_{^{n},^{m}}\{^{ }:^{} 1,^{}  1,\|\|_{0} s_{2}\};\] (3)
3. _When_ \(s_{1}<r\) _and_ \(s_{2}\)_, the SCCA problem can be reduced to_ \[v^{*}=_{^{n},^{m}}\{^{ }:^{} 1,^{}  1,\|\|_{0} s_{1}\}.\] (4)

_Proof._ See Appendix A.3. \(\)

The results in Theorem 2 build on the covariance structure of the data matrix. Specifically, if \(B\) and \(C\) are of rank \(r\) and \(\), respectively, there are only \(r\) and \(\) linearly independent vectors in the subspaces corresponding to \(B\) and \(C\). Thus, the cosine of the principal angle can always be represented by these \(r\) and \(\) vectors. As a result, the canonical directions of CCA consist of only \(r\) and \(\) nonzero elements. We further make the following remarks about Theorem 2:

1. Theorem 2 implies the complexity of solving SCCA, as summarized in the corollary below.
2. Inspired by Part (i) of Theorem 2, we also develop a polynomial-time exact algorithm yielding an optimal solution to SCCA (1) when \(s_{1} r\) and \(s_{2}\). The detailed implementation can be found in Algorithm 1, which successfully solves some large instances with up to \(n=19,672\) and \(m=2,149\) variables in seconds in our numerical experiments; and
3. The proof of Theorem 2 implies that CCA always admits an optimal sparse solution \((^{*},^{*})\) satisfying \(\|^{*}\|_{0} r\) and \(\|^{*}\|_{0}\). We show in Proposition 1 that the SDP Relaxation of CCA is exact. Therefore, as a side product, we provide the first-known sufficient condition about (i.e., \(s_{1} r\) and \(s_{2}\)) when the convex SDP Relaxation matches SCCA.

**Corollary 1**: _Suppose \(r=()\) and \(=()\). There exists an algorithm that can find an optimal solution to SCCA in \((n^{r-1}m^{-1})\) time complexity._

**Proposition 2**: _Suppose \(r=()\) and \(=()\). Then Algorithm 1 returns an optimal solution to SCCA (1) in \((n^{3}+m^{3})\) time complexity when \(s_{1} r\) and \(s_{2}\)._

_Proof._ Following the proof of Theorem 2, we can show that the output solution of Algorithm 1 is optimal to SCCA (1). In addition, the Step 2 of Algorithm 1 needs computing the eigendecomposition of matrix \(_{+}^{n}\), which takes a time of \((n^{3})\). Given a matrix \(^{n(n-r)}\), it also takes a time of \((n^{3})\) to find its \((n-r)\) linearly independent rows at Step 3 through the QR decomposition . Hence, Algorithm 1 takes a complexity of \((n^{3}+m^{3})\). \(\)

```
1:Input: Matrices \(^{m n}\), \(_{+}^{m}\), \(_{+}^{m}\) and integers \(s_{1}[r,n]\), \(s_{2}[,m]\)
2: Compute the eigenvectors \(^{n(n-r)}\) of \(\) that correspond to its \((n-r)\) zero eigenvalues,
3: Find \((n-r)\) linearly independent rows in \(\), and collect their indices into a subset \(T_{1}^{*}[n]\)
4: Perform the same procedure on matrix \(\) to obtain the subset \(T_{2}^{*}[m]\)
5: Define the subsets \(S_{1}^{*}=[n] T_{1}^{*}\) and \(S_{2}^{*}=[m] T_{2}^{*}\), and compute \[v^{*}=_{}(_{S_{1}^{*},S_{1}^{*}})^{ }}_{S_{1}^{*},S_{2}^{*}}_{S_{2}^{*},S_{2}^{*}} )^{}})\]
6:Output: An optimal solution \((S_{1}^{*},S_{2}^{*})\) and optimal value \(v^{*}\) ```

**Algorithm 1** An exact algorithm for SCCA (1) when \(s_{1} r\) and \(s_{2}\)

### Special Case II: SCCA with a rank-one cross-covariance matrix

In this subsection, we study the other interesting low-rank special case of SCCA where the cross-covariance matrix \(\) is rank-one. For this special case, we prove its NP-hardness with reduction to the sparse regression problem. We further demonstrate that rank-one SCCA can be simplified to solving two Mixed-Integer Convex Quadratic Programs (MICQPs), which can be more scalable than directly solving SCCA. Our numerical findings confirm this improved scalability.

We observe that SCCA can be separable over variables \(\) and \(\) for the rank-one \(\). In fact, suppose that \(=^{}\), then SCCA is equivalent to

\[v^{*}=_{^{n},^{m}}\{^{ }^{}:^{} 1,^{ } 1,\|\|_{0} s_{1},\|\|_{0} s_{2}\}\] (5)

which can be equivalently the product of the optimal values of the following two subproblems:

\[v_{x} =_{^{n}}\{^{}:^{ } 1,\|\|_{0} s_{1}\},\] (6) \[v_{y} =_{^{m}}\{^{}:^{ } 1,\|\|_{0} s_{2}\}.\]

That is, the identity \(v^{*}=v_{x}v_{y}\) holds. Next, we show that each subproblem in (6) can be reduced to the classic sparse regression problem  and is thus NP-hard as shown below.

**Theorem 3**: _When matrix \(=^{}\) is rank-one, each maximization problem in (6) is NP-hard._

_Proof._ See Appendix A.4. \(\)

Theorem 3 links the maximization problem (6) and the well-known sparse regression problem, implying that even solving the rank-one SCCA problem (5) is NP-hard. However, it also motivates us to adapt existing mixed-integer optimization techniques from sparse regression (see, e.g., ) to tackle each subproblem in (6). By introducing binary variables to model the zero-norm constraint, we derive equivalent MICQP formulations for subproblems (6) in Appendix C. There are two types of formulations depending on whether matrices \(\) and \(\) are positive definite, which build on the Big-M and perspective techniques, respectively.

## 4 Reformulating SCCA as a mixed-integer semidefinite program (MISDP)

While the combinatorial formulation (1) is elegant in its structure, it poses significant challenges when attempting to solve it to optimality using branch-and-bound based methods. To fill this gap, in this section, we derive an equivalent MISDP formulation for SCCA, amenable for developing exact methods.

First, it is noted that an optimal solution \((^{*},^{*})\) to SCCA is always bounded that satisfies \(\|^{*}\|_{2}^{2} M_{1}\) and \(\|^{*}\|_{2}^{2} M_{2}\), where we specify the construction of the coefficients \(M_{1}\) and \(M_{2}\) in Appendix C.1. Such bounds are essential to the derivation of the MISDP. It is convenient to introduce the following notation about \(\{M_{ii}\}_{i[n+m]}\):

\[M_{ii}=M_{1}, i[n],\ \ M_{ii}=M_{2}, i[n+1,n+m].\]

**Theorem 4**: _The SCCA is equivalent to the following MISDP:_

\[v^{=}_{_{+}^{n+m},}\{ {tr}(}):(}) 1, (}) 1,X_{ii} M_{ii}z_{i}, i [n+m]\}.\] (7)

_where the feasible set is defined as \(=\{\{0,1\}^{n+m}:_{i[n]}z_{i} s_{1},_{i[ n+1,n+m]}z_{i} s_{2}\}\)._

_Proof._ See Appendix A.5. \(\)

Note that the proposed MISDP formulation (7) is of size \((n+m)(n+m)\) since our matrix variable \(\) replaces \(\\ \\ ^{}\) in SCCA.

We have formulated SCCA as a mixed-integer convex optimization problem in Theorem 4. Unfortunately, no commercial solvers can efficiently solve MISDP problems. We derive an equivalent mixed-integer linear program of SCCA with exponentially many linear constraints and an efficient separation oracle based on the approach introduced by , which allows us to develop a tailored branch-and-cut algorithm. First, by separating the binary variables \(\), we rewrite the MISDP (7) as

\[v^{*}=_{,v}\{v:v f()\},\] (8)

where the function \(f()\) is defined as

\[f()=_{_{+}^{n+m}}\{( }):(}) 1, (}) 1,X_{ii} M_{ii}z_{i},  i[n+m]\}.\] (9)

For any feasible solution \(}\) of the problem (8), by leveraging the concavity of function \(f()\), the linear inequality

\[v f(})+ f(})^{}(-})\]

cuts off the solution \(}\) unless it happened to be optimal in (8), which paves the way for a delayed cut-generation procedure within a branch-and-bound framework. As the linear inequality of the above type needs to be added dynamically given different solutions \(}\) at each iteration, it calls for an efficient evaluation of function \(f(})\) and its subgradient. To speed up the computation, we derive the closed-form expression for both of them. The detailed derivations can be found in Appendix D.

**Strategies to improve computational speed in practice:** First, we provide a variable-fixing method that can identify some binary variables of the MISDP (7) being one at optimality. Removing these pre-selected variables from the feasible set reduces the problem size of SCCA. Second, we enhance the branch-and-cut algorithm with a high-quality warm start solution obtained from the local search algorithm. Third, by relaxing the binary variables in the MISDP (7) to be continuous or computing CCA, we can obtain an upper bound of SCCA, and the gap between this bound and the local search output gives an initial gap at the root node. Finally, at each iteration, the branching node is selected based on its potential to decrease the current upper bound instead of random branching.

## 5 Numerical results

This section tests the numerical performance of our formulations and algorithms on synthetic and real data. All the experiments are conducted in Python 3.6 with calls to Gurobi 9.5.2 and MOSEK 10.0.29 on a PC with 10-core CPU, 16-core GPU, and 16GB of memory. The codes and data used in our experiments are available at https://github.com/yongchunli-13/SCCA.git.

### Experimental setup

**Synthetic data generation:** Before we present the empirical results, we first describe the properties of the synthetic data which shall be used throughout this section. By following , given parameters \((n,m,s_{1},s_{2})\), we first synthetically generate positive definite matrices \(^{*}^{n}_{++}\) and \(^{*}^{n}_{++}\) by \(^{*}=}}^{}+\) and \(^{*}=}}^{}+\), respectively, where \(}\) and \(}\) consist of elements generated from a normal distribution \((0,1)\). Then, we let \(^{*}^{n m}=^{*}^{}^ {*}\), where we generate \(\) uniformly from \((0,1)\), and vectors \(,\) are generated from a normal distribution \((0,1)\) that satisfy \(\|\|_{0}=s_{1}\), \(\|\|_{0}=s_{2}\), \(^{}^{*}=1\) and \(^{}^{*}=1\). Next, we sample \(N=5,000\) data samples from a normal distribution \((,^{*}&^{*}\\ (^{*})^{}&^{*})\) and compute their sample covariance matrix to obtain the testing data \(&\\ ^{}&\).

**Real data:** To obtain a comprehensive understanding of the overall performance of our algorithms, we further conduct experiments on six UCI datasets  with sizes ranging from 34 to 385 variables. The dataset is split into the first \(n\) variables and the remaining \(m\) variables to construct the sample covariance matrices \(,,\). Besides, we examine the performance of the proposed algorithms on the real breast cancer dataset  that contain \(n=19,672\) and \(m=2,149\) variables. The information on each dataset is summarized in Appendix E.

Throughout, the computational time is in seconds, the time limit is one hour, and the dashed line "-" denotes the unsolved case within the time limit. Note that we let **LB** denote the lower bound obtained from the approximation algorithm, and we let **UB** denote the upper bound obtained from convex relaxations of SCCA. Besides, we define **gap(%)\(=100(-v^{*})/v^{*}\)** to be the optimality gap, and we replace \(v^{*}\) with the best lower bound when \(v^{*}\) is not available. We define **MIPGap(%)** to be the gap of exact algorithms at termination. Notably, the complexity analysis of the SCCA problems in Section 3 indicates that its solution process depends on the ranks of the data matrices. Therefore, we present the numerical results under both full-rank and low-rank cases for a comprehensive evaluation.

### Illustration of the impact sparsity levels on SCCA

In this subsection, we apply the local search algorithm to evaluating the performance of SCCA against different sparsity levels \(s_{1}\) and \(s_{2}\). Specifically, for a given dataset, we compute the ratio of correlations between SCCA and CCA for various \(s_{1},s_{2}\) parameters. We test the real UCI data and synthetic data, and the results are displayed in Figure 2 and Figure 2. This visualization provides insights for the maximum sparsity SCCA can achieve while maintaining the correlation of full data. For the real UCI data, SCCA almost recovers the correlation of CCA when \(s_{1} n/2\) and \(s_{2} m/2\).

### Solving SCCA with full-rank matrices

The numerical results on synthetic and real data are presented in Table 1 and Table 2, respectively, which include multiple instances with various parameters \((n,m,s_{1},s_{2})\). First, we observe that the greedy and local search algorithms are scalable, and their outputs match the optimal values for most solved testing cases. That is, they achieve zero optimality gaps on these cases. In the "Convex relaxation" column, we compute an upper bound by solving either the continuous relaxation of MISDP (7) or CCA, and we use CCA for \(n 40\) and \(m 40\) cases for efficiency. It is seen that the upper bound maintains an optimality gap at most \(2.78\%\). Then, we apply the branch-and-cut algorithm to solve SCCA to optimality, which can handle the case up to a size of \(n=m=120\) in

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]