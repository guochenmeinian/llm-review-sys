# Vitron: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing

Hao Fei\({}^{1,2}\) Shengqiong Wu\({}^{1,2}\) Hanwang Zhang\({}^{1,3}\) Tat-Seng Chua\({}^{2}\) Shuicheng Yan\({}^{1,}\)

\({}^{1,}\)*Skywork AI, Singapore \({}^{2}\) National University of Singapore \({}^{3}\) Nanyang Technological University

haofei37@nus.edu.sg swu@u.nus.edu hanwangzhang@ntu.edu.sg dcscts@nus.edu.sg shuicheng.yan@kunlun-inc.com

Shuicheng Yan is the corresponding author. This work was performed when Hao Fei was an Associate Member, and Shengqiong Wu was an Intern at Skywork AI.

###### Abstract

Recent developments of vision large language models (LLMs) have seen remarkable progress, yet still encounter challenges towards multimodal generalists, such as coarse-grained instance-level understanding, lack of unified support for both images and videos, and insufficient coverage across various vision tasks. In this paper,

Figure 1: Vitron supports four main task clusters of visions, spanning visual comprehension to visual generation, from low level to high level.

we present **Vitron**, a universal pixel-level vision LLM designed for comprehensive _understanding_, _generating_, and _editing_ of both static images and dynamic videos. Building on top of an LLM backbone, Vitron incorporates encoders for images, videos, and pixel-level regional visuals within its frontend modules, while employing state-of-the-art visual specialists as its backend, via which Vitron supports a spectrum of vision end tasks, spanning visual comprehension to visual generation, from low level to high level. To ensure an effective and precise message passing from LLM to backend modules for function invocation, we propose a novel hybrid method by simultaneously integrating discrete textual instructions and continuous signal embeddings. Further, we design various pixel-level spatiotemporal vision-language alignment learning for Vitron to reach the best fine-grained visual capability. Finally, a cross-task synergy module is advised to learn to maximize the task-invariant fine-grained visual features, enhancing the synergy between different visual tasks. Demonstrated over 12 visual tasks and evaluated across 22 datasets, Vitron showcases its extensive capabilities in the four main vision task clusters. Overall, this work illuminates the great potential of developing a more unified multimodal generalist.

## 1 Introduction

Recently, the field of multimodal large language models (MLLMs) has witnessed rapid and flourishing development across multiple communities. Extensive research efforts have been directed towards augmenting powerful, purely language-based LLMs with modules capable of visual perception, thereby extending their applicability to MLLMs . MLLMs, such as BLIP-2 , LLAVA , MiniGPT-4  and GPT-4V  etc., demonstrate a robust and exceptional capability in image understanding, paralleling the deep semantic comprehension of language. In the realm of vision, the ability to process and comprehend dynamic videos is equally critical. Concurrently, several MLLMs have emerged with a focus on video understanding, e.g., VideoChat  and Video-LLaMA , demonstrating significant advancements in video comprehension.

Subsequent studies have sought to further expand the capabilities of MLLMs, with efforts bifurcating into two primary dimensions. On one hand, there's a deepening of MLLMs' understanding of vision, transitioning from coarse, instance-level comprehension towards a pixel-level, fine-fined understanding of images, thereby achieving visual regional grounding capabilities, as seen in GLaMM , PixelLM , and MiniGPT-v2 , etc., alongside the counterparts in pixel-grounding video LLMs . On the other hand, there's an expansion in the breadth of functionalities MLLMs can support within the vision field. A portion of the research has already ventured into enabling MLLMs not just to comprehend input vision signals but also to support the generation and output of vision content, with systems like GILL , Emu , etc., flexibly generating image content, and GPT4Video  and NExT-GPT  achieving video generation.

We posit that the future trend of vision LLMs necessarily involves the enhancement of their capabilities towards a high degree of unification, i.e., multimodal generalists. However, our observations reveal that despite the diversity of existing vision LLMs developed by the community, there is still a clear lack of unification. **First**, almost all existing vision LLMs treat images and videos as separate entities, either supporting only images or videos . We argue for a unified vision MLLM framework that concurrently supports both images and videos, acknowledging that vision inherently comprises both static images and dynamic videos - both core components of our world and largely interchangeable in most scenarios. **Second**, the current support for vision functionalities in MLLMs is found wanting, with most models only capable of understanding , or at most generating images or videos . We contend that future MLLMs should embrace a broader spectrum of vision tasks and functionalities, enabling unified support for all vision-related tasks and achieving an _"one for all"_ capability, which is vital for real-world applications, especially in vision creation that often involves a series of iterative and interactive operations. For example, users typically start by _generating_ images from text, transforming an idea into visual content; and then refining this content through further fine-grained _editing_ to add more details; following, proceeding to create dynamic content by _generating_ videos from the images; and finally, engaging in several rounds of iterative interaction, such as video _editing_, to enhance and finalize their creation. **Last but not the least**, for a generalist integrated with various multimodal functionalities, one key lies in how to ensure that all tasks achieve their best performance as much as possible. This includes both that, 1) the instructionsfrom the LLM are precisely conveyed to the downstream decoders, and 2) different tasks do not undermine each other but rather cooperate.

To address all these gaps, this paper introduces **Vitron**, a pioneering universal pixel-level vision LLM, as shown in Fig. 2. First, Vitron leverages a backbone LLM for comprehending, reasoning, decision-making, and multi-round user interactions. To perceive both image and video modal signals and support fine-grained user visual inputs, Vitron incorporates encoders for images, videos, and regional box/sketch-specified inputs. On the backend, several state-of-the-art (SoTA) image and video modules are integrated for decoding and executing a wide range of vision tasks, spanning from lower to higher levels, such as visual understanding (perceiving and reasoning), generating, segmenting (grounding and tracking), editing (inpainting). To ensure that Vitron precisely conveys the LLM's decisions to various backend decoder modules for function invocation, we propose a novel hybrid method of instruction passing. Specifically, we enable the LLM to output not only discrete textual instructions, but also continuous signal feature embeddings passed to the modules. Finally, to maximize the functionalities of different modules within Vitron, we further devise a synergy module, where we fully maximize the task-persistent fine-grained visual features to be shared among different visual tasks.

The overall training for Vitron aims to equip it with robust and powerful vision understanding and manipulation capabilities. We first imbue Vitron basic MLLM skills by carrying out 1) vision-language alignment learning between the frontend encoders and central LLM, also 2) invocation-oriented instruction tuning, and 3) embedding-oriented alignment tuning between LLM and backend modules. Going beyond this, we further try to strengthen Vitron's capacities. On the one hand, we introduce fine-grained spatiotemporal vision grounding instruction tuning, training LLM on grounding predictions and pixel-aware perception for images and videos, such that Vitron sufficiently gains pixel-level visual perception. On the other hand, we utilize adversarial training  to decouple _task-specific features_ from _task-invariant fine-grained visual features_ in signal feature representations, thereby enhancing the synergy between different tasks.

Extensive experiments covering 12 tasks across 22 datasets are performed. Leveraging its advanced architecture as a multimodal generalist, Vitron demonstrates proficiency in a comprehensive range of vision tasks. Notably, the unified system's performance is on par with or even surpasses singleton state-of-the-art specialists on specific tasks. Further analyses reveal the efficacy of each design of the system. Our overall contributions are summarized as follows.

To our knowledge, we for the first time propose a grand unified vision MLLM, Vitron, capable of pixel-level understanding, generating, segmenting, editing of both images and videos.

Figure 2: Technical overview of the Vitron framework.

introduce a more effective LLM-to-decode instruction-passing mechanism over both discrete texts and continuous signal embeddings. 

We propose carrying out various pixel-level vision-language spatiotemporal alignment learning for MLLMs to reach the best fine-grained visual capability. 

We devise a synergy module to maximize the task-persistent fine-grained visual features shareable among all different visual tasks, via which Vitron surpasses existing SoTA specialists' performance.

## 2 Related Work

Achieving a profound understanding and comprehensive operational capabilities in vision, ranging from low-level visual pixel understanding [7; 65; 122; 46; 55; 102; 52; 53; 54] to high-level comprehension of overall semantics [19; 45; 70; 23; 31; 38; 39; 48; 115; 26; 56; 24], represents a significant topic. Recent years have seen the development of highly potent large-scale vision models, such as ViT  and CLIP , which have achieved remarkable vision understanding capabilities; models like SAM  and SEEM  have solved vision segmentation tasks; and diffusion-based models [34; 82; 73; 28; 112; 86; 25] have reached unprecedented performance in vision generation. Yet these models might lack an LLM as a central decision processor, unable to flexibly interpret user intent or execute tasks interactively [97; 47; 114]. The emergence of LLMs has exhibited unprecedented intelligence capability [99; 76; 16]. Extending the success of language understanding in LLMs, researchers have promptly investigated and developed various MLLMs, enabling LLMs to comprehend vision. By integrating high-performance vision encoders of images or videos into language-based LLMs, these models have been made capable of understanding vision signals [77; 1; 49; 81; 63]. Going beyond vision understanding, further research has aimed to enhance MLLMs, for instance, by endowing them with vision generation capabilities [96; 43] or supporting pixel-level understanding and grounding [130; 125; 85; 132; 110]. In Table 1 we summarize some existing popular vision MLLMs in terms of the vision function support.

However, we observe that current research on vision LLMs lacks depth in two critical aspects. Firstly, current vision LLMs tend to separate images and videos, supporting either one or the other. The construction of a unified MLLM is crucial, as vision inherently encompasses both static images and dynamic videos, both of which are core components of our visual world. Thus, covering both aspects simultaneously is essential for optimally adapting to practical applications. Although models like NExT-GPT  have relatively well-supported unification across various modalities, they fall short in supporting pixel-level in-depth vision understanding and comprehensive support for vision operation tasks. The second issue is the incomplete support for vision tasks by existing MLLMs. Most current MLLMs primarily support understanding images or videos [63; 138], with

    &  &  &  &  &  &  \\    & **Image** & & & **Video** & & & \\  Flamingo  & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ BLIP-2  & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ MiniGPT+  & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ LLVA  & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ GILL  & ✓ & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ \\ Emu  & ✓ & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ \\ MiniGPT-5  & ✓ & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ \\ DreamLLM  & ✓ & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ \\  GPT4RoI  & ✓ & ✗ & ✓ & ✓ & ✗ & ✗ & ✗ \\ NEXT-Clat  & ✓ & ✗ & ✓ & ✓ & ✗ & ✗ & ✗ \\ MiniGPT-v2  & ✓ & ✗ & ✓ & ✓ & ✗ & ✗ & ✗ \\ Shikra  & ✓ & ✗ & ✓ & ✓ & ✗ & ✗ & ✗ \\ Kosmos-2  & ✓ & ✗ & ✓ & ✓ & ✗ & ✗ & ✗ \\ GLAMM  & ✓ & ✗ & ✓ & ✓ & ✗ & ✗ \\ Osprey  & ✓ & ✗ & ✓ & ✓ & ✗ & ✗ & ✗ \\ PixelLM  & ✓ & ✗ & ✓ & ✓ & ✗ & ✗ \\ LLVAVA-Plus  & ✓ & ✗ & ✗ & ✓ & ✓ & ✓ & ✗ \\  VideoChatt  & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Video-LLAMa  & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Video-LLaVA  & ✓ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Video-ChaGTPT  & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ GPT4Video  & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ PGVIeo-LLaVA  & ✗ & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ \\  NExT-GPT  & ✓ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\  Vitron (Ours) & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparisons of existing (partially, imperfect coverage) representative vision MLLM.

only a few supporting generation  or editing/inpainting . Building a generalist that can handle (almost) all vision-related tasks and operations in an end-to-end architecture should be the next major trend for vision MLLMs. Yet simply integrating existing visual specialists into an LLM to form MLLMs is not sufficient enough, as genuine human-level AI should possess universal intelligence with robust cross-task generalizability . Thus, it is necessary to further consider how to enable synergy effects  among different task specialists within a generalist, for which goal, we have devised a synergy strategy in this work. Besides, compared to the multimodal comprehension capabilities of MLLM, endowing MLLM with strong multimodal generative abilities is even more challenging. The key lies in how to effectively and unbiasedly convey MLLM's semantic understanding signals to the backbone decoder modules. There are two mainstream approaches to LLM-to-decoder message passing within the MLLM community. One is based on discrete textual instructions , and the other on continuous signal embeddings . However, we find that these two methods are complementary. Specifically, the former allows the LLM to efficiently convey task execution commands to the backend modules through simple text, but it struggles to provide modality-specific signals; the latter can conveniently carry the features needed for tasks, but fails to accurately convey execution intention (especially for managing many modules). In this work, we propose a hybrid method by integrating them together.

## 3 Architecture of Vitro

Vitron takes most common '_encoder-LLM-decoder_' architecture paradigm, as in existing popular MLLMs . The overall framework is shown in Fig. 2, where three key blocks are included: 1) frontend vision&language encoders, 2) central LLM for semantics understanding and text generation, and 3) backend decoder modules for user responding and vision manipulation.

### Frontend Vision-Language Encoding

For both images and videos, we employ the CLIP ViT-L/14@336px  as the encoder, respectively. The video encoder independently processes each frame, further employing average pooling across the temporal dimension to yield overall temporal representation features. Then, we employ a regional pixel-aware visual extractor as the sketch encoder for user interaction, e.g., clicking, drawing boxes or polygons, and making scribbles. We mainly follow , and use the object-based representations of mask regions that come from user's inputs, which not only encode the pixel-level visual features but also gather the spatial position information of each region. The region features are pooled with also the binary mask of spatial geometry of the object region encoded, and the resulting embeddings are used. Then, the multimodal feature representations are passed to LLM via linear projection.

### Core LLM

In Vitro, an LLM serves as the pivotal agent. Following the most common practice , we utilize Vicuna (7B, version 1.5). The LLM processes inputs from both language and visual modalities to perform semantic understanding and reasoning, and then make decisions. For visual comprehension tasks, LLM directly outputs textual responses for users. On the other side, LLM also needs to transmit signals and instructions to backend modules, directing them to invocate more complex tasks that go beyond text generation, such as visual segmentation, generation, and editing. As emphasized earlier, the ability of LLMs to effectively and precisely convey messages is crucial to the performance of complex multimodal tasks. To this end, we propose fully integrating the advantages of the two common message-passing methods: _discrete textual instructions_ and _continuous signal embeddings_. The former aids in accurately invoking different backbone modules (thanks to the LLM's proficiency in task dispatching), while the latter supplements with richer modality-preserved visual features that cannot be directly described through discrete text. As depicted in Fig. 2, the LLM outputs 1) text responses for users, 2) text instructions for module invocation, and 3) feature embeddings of special tokens. The feature embeddings are split into the task-specific features and the task-invariant fine-grained visual-language features. Both the text instructions and feature embeddings are passed to backbone modules.

### Backend Visual Specialists

To enable our MLLM with various visual task abilities, we integrate an array of singleton vision specialists into LLM. For image generation and editing, we integrate the diffusion-based model GLIGEN . For image and video segmentation, we opt for SEEM . For video generation, ZeroScope  and I2VGen-XL  are utilized for text-to-video and image-to-video tasks, respectively. Lastly, for video editing functionality, we incorporate StableVideo . The text instructions from LLM first determine which task module to invoke; simultaneously, feature embeddings are fed into the corresponding module's feature encoder to assist with task execution. Specifically, we design a structured invocation template, including 1) Module name, 2) Invocation command, and 3) Region (optional) specifying a fine-grained vision feature needed for certain tasks. The feature embeddings include both _task-specific features_ and _task-invariant fine-grained features_. The purpose of this design is to achieve feature decoupling, during which we aim to have the task-invariant fine-grained features shared as widely as possible among all tasks to facilitate synergy between different tasks.

## 4 Pixel-aware Synergistic Vision-Language Understanding Tuning

With the Vitron framework, we now train the model with three stages of targets. First, we try to endow it with basic multimodal capabilities, i.e., comprehension and generation. Then, we engage in fine-grained vision grounding instruction tuning to further enhance the model's pixel-level perception abilities. Finally, we carry out cross-task synergy learning, maximizing the shared fine-grained features among all tasks.

### Basic Multimodal Comprehension and Generation Skill Training

In the first stage of training, the primary goal is to equip the MLLM with basic multimodal understanding and generation abilities, including the frontend alignment of encoder-LLM, as well as the backend alignment of LLM-decoder. Appendix SSB.1 details all the following three types of training.

**Overall Vision-Language Alignment Learning.** This is to ensure the input vision and language are mapped to a unified feature space. Following prior common practice, we utilize datasets comprising 'image-caption' pairs (CC3M ), 'video-caption' pairs (Webvid ), and'region-caption' pairs (RefCOCO ) drawn from existing established corpora and benchmarks. When provided with an image, video, or specific visual region, we engage the frozen LLM to generate a text description or caption that aligns with the reference caption.

**Text Invocation Instruction Tuning.** This step of training aims to equip the system with the precise capability to execute commands, allowing the LLM to generate appropriate and correct invocation text instructions. To accomplish this, we collect a total of 55,000+ instruction tuning samples.

**Embedding-oriented Decoder Alignment Tuning.** Besides using explicit textual instruction to invocate downstream modules, the signal feature embedding/representation (from LLM) should also be fed to the modules. Following , we align the feature embedding with all the visual modules' input encoders via the decoding-side projection layers, i.e., by minimizing their distances.

### Fine-grained Spatiotemporal Vision Grounding Instruction Tuning

A visual generalist should require a strong capability of pixel-aware vision understanding of both images and videos. Thus, we propose a fine-grained spatiotemporal vision grounding instruction tuning for Vitron. The core idea is to enable the LLM to ground the fine-grained spatiality of images and the detailed temporality of videos. Appendix SSB.2 extends more detailed descriptions of the following three learning aspects.

**Image Spatial Grounding.** Considering that the LLM alone can only output text, we design it to respond with the corresponding bounding box areas. We focus on two types of tasks: grounded image captioning  and referring image segmentation .

**Video Spatial-Temporal Grounding.** For videos, the LLM must identify spatial regions and ground them within the temporal context of the video, essentially achieving video tracking. Similarly, we explore tasks such as grounded video captioning  and referring video tracking .

**Grounding-aware Vision QA.** The grounding tasks mentioned above only touch upon the low-level aspects of vision perception. However, in many scenarios, it's essential for the LLM to possess high-level, in-depth vision reasoning capabilities, building upon the foundational low-level pixel grounding. Thus, we further introduce grounding-aware vision QA, including Image-QA  and Video-QA , enabling LLM to undertake semantic-level QA tasks based on the grounded results.

### Cross-task Synergy Learning

As a generalist, directly invoking different specialists leads to a critical issue: _how to ensure that the different modules (tasks) work together synergistically?_ Otherwise, without such collaboration,

Figure 3: Illustration of the synergy module.

[MISSING_PAGE_FAIL:7]

shown in Tables 5 illustrate that Vitro surpasses the best baseline across various datasets and metrics, proving its strong and accurate fine-grained semantic understanding of images.

The above two tasks focus solely on the model's ability to recognize at the region level. Taking a step further, we delve deeper into assessing the capability for image semantics understanding, particularly through image-based Visual Question Answering (VQA) tasks. These tasks effectively reflect the model's proficiency in comprehending the deeper semantic content of images. Table 6 displays the results across a series of six datasets for image-based VQA. We primarily compare two groups of models: those with and without pixel-wise vision grounding capabilities. The findings indicate that models equipped with fine-grained grounding abilities indeed show stronger task performance, suggesting that fine-grained grounding contributes to a more profound understanding of semantics. Notably, our Vitro achieves the highest performance among the models evaluated.

Region-level Video Understanding.Similarly, for videos, we evaluate the Region-level Video Understanding capability. Building on observations from images, we now directly engage in video QA tasks. Table 7 presents the results on video QA across four representative datasets. Interestingly, while PG-Video-LLaVA has video grounding capabilities, it does not show better results than Video-LLaVA, which lacks grounding. However, our Vitro achieves superior performance. This indirectly proves that our system possesses more accurate video grounding capabilities (as previously demonstrated in Table 8), along in better video semantics understanding.

### Results on Vision Generation

Next, we assess our system's capabilities in vision generation, focusing on three of the most representative types of generation tasks: text-to-image generation, text-to-video generation, and image-to-video generation. These tasks broadly cover the spectrum of image generation requirements. Tables 8, 9, and 10 showcase how our Vitro performs in comparison to other SoTA systems, including both MLLM and non-MLLM synthesizers. The results clearly demonstrate that Vitro outperforms on all three tasks. For instance, in both text-to-image and text-to-video generation tasks, Vitro shows more advanced performance compared to NExT-GPT. Similarly, in the image-to-video generation task, Vitro still outshines the SoTA baseline, VideoCrafter1, showcasing superior results.

### Results on Vision Editing

Image Editing.We use the MagicBrush dataset , which challenges models with an editing query that demands a series of complex edits to an image. These edits include removing, changing, inpainting, and adding elements. Since there are currently no MLLM systems that support image editing, our comparison is limited to non-LLM expert systems. In Table 11, we present the performance of different models across various metrics. Vitro demonstrates stronger performance on all metrics, indicating its stable image editing capabilities.

 
**Method** & **Ground?** & **OKVQA ** & **GQA ** \\  Flaminging  & ✗ & 44.7 & - \\ BLIP-2  & ✗ & 45.9 & 41.0 \\ InstructBLIP  & ✗ & - & 49.5 \\ MiniGPT-4  & ✗ & 37.5 & 30.8 \\ LLaVA  & ✗ & 54.4 & 41.3 \\ Shixra  & ✓ & 47.2 & - \\ MiniGPT-2  & ✓ & 57.8 & 60.1 \\ 
**Vitron** & ✗ & **59.4** & **62.1** \\  & ✓ & -2.0 & -1.7 \\  

Table 6: Results (accuracy) on image-based VQA.

 
**Method** & **FID (\(\))** & **CLIPSIM (\(\))** \\  CGIDE  & 12.24 \\ SD  & 11.21 \\ NExT-GPT  & 11.28 \\ Emu  & 11.66 \\ GILL  & 12.20 \\ DreamLLM  & 8.46 \\  \\  & **7.57** \\   
 
**Method** & **FID (\(\))** & **CLIPSIM (\(\))** \\  CGVideo  & 23.59 & 0.2631 \\ MakeVideo  & 13.17 & 0.3049 \\ Latent-VDM  & 14.25 & 0.2756 \\ Latent-Shift  & 15.23 & 0.2773 \\ CoDi  & — & 0.2890 \\ NExT-GPT  & 13.04 & 0.3085 \\  & **10.11** & **0.3682** \\ w/o syng. & +3.17 & -0.5672 \\  

Table 10: Image-to-Video generation on UCF101 .

 
**Method** & **FVD (\(\))** & **IS (\(\))** \\  AnimateAny  & 642.64 & 63.87 \\ DynamiCrafter  & 404.50 & 41.97 \\ SEINE  & 306.49 & 54.02 \\ VideoCrafter1  & 297.62 & 50.88 \\  & **175.46** & **56.89** \\ w/o syng. & +96.24 & -5.03 \\  

Table 10: Image-to-Video generation on UCF101 .

 
**Method** & **Ground?** & **OKVQA ** & **GQA ** \\  Flaming  & ✗ & 44.7 & - \\ BLIP-2  & ✗ & 45.9 & 41.0 \\ InstructBLIP  & ✗ & - & 49.5 \\ MiniGPT-4  & ✗ & 37.5 & 30.8 \\ LLaVA  & ✗ & 54.4 & 41.3 \\ Shixra  & ✓ & 47.2 & - \\ MiniGPT-2  & ✓ & 57.8 & 60.1 \\  & ✗ & **59.4** & **62.1** \\  & ✓ & -2.0 & -1.7 \\  

Table 7: Results (accuracy and confidence Score) on video QA.

 
**Method** & **FVD (\(\))** & **CSIPIM (\(\))** \\  AnimateAny  & 642.64 & 63.87 \\ DynamiCrafter  & 404.50 & 41.97 \\ SEINE  & 306.49 & 54.02 \\ VideoCrafter1  & 297.62 & 50.88 \\  & **175.46** & **56.89** \\ w/o syng. & +96.24 & -5.03 \\  

Table 10: Image-to-Video generation on UCF101 .

Video Editing.For video editing, the community currently lacks a standardized benchmark and evaluation method akin to those for image editing. Therefore, we opted for a manual evaluation approach. We asked different video editing systems to edit the same video based on the same query, after which five individuals were asked to score the edited videos. The evaluation focused on 1) the success of target content modifications and 2) the faithfulness/fidelity of non-target content. Table 12 presents the manual evaluation results for video editing. It is clear that Vitron outperforms the two baseline systems in both respects, showcasing superior video editing capabilities. Following this, we visualized the process of video editing by Vitron.

## 6 Discussions

Above we demonstrate the overall efficacy of Vitron via extensive quantitative comparison. Now we take one step further, exploring how and why the system advances via in-depth analyses.

\(\)**Discrete Textual Instruction or Continuous Signal Embedding, Which Better?Firstly, we explore different message-passing mechanisms to determine whether discrete textual instruction is more beneficial, or whether continuous signal embedding is better for building a multi-modal generalist. Also, we validate the pros and cons of the proposed hybrid method of message passing. We conduct tests on 6 tasks, where we compare the task performance of Vitron using the hybrid method (default setting), without signal embedding and without text instruction, as well as the successful execution rate of the backend task module. Fig. 4 presents the results. As can be observed, overall, the performance under scenarios utilizing both methods is consistently better, which confirms the effectiveness of our hybrid mode. Meanwhile, we find that the method of text instruction is more conducive to the successful execution of backend modules, but soft feature embedding seems to be more useful in terms of specific task performances.

\(\)**How Much Does Each Fine-grained Visual Grounding Learning Contribute?Next, we validate the specific contribution of the various fine-grained visual grounding learning strategies proposed in SS4.2. Fig. 5 (the top 4 relate to image tasks, and the bottom 4 to video tasks) shows the impact on performance when a particular learning strategy is removed. Generally, all these 3 types of fine-grained visual grounding learning strategies are vital for different downstream tasks. For instance, grounding and referring segmentation tasks directly influence fine-grained visual recognition tasks, whereas tuning for grounding-aware visual QA considerably boosts cognition level QA tasks. This verifies the efficacy of our proposed fine-grained visual grounding tuning strategies.

\(\)**Does Vitron Really Achieve Cross-task Synergy?Finally, we investigate if our system could adequately support cross-task synergy. Based on the results of the ablation item for the'synergy

Figure 4: The influences of using different strategies for message passing.

 
**Method** & **CLIP\({}_{dir}\) (\(\))** & **CLIP\({}_{img}\) (\(\))** & **CLIP\({}_{out}\) (\(\))** & **LI (\(\))** \\  InstructPixPix  & 0.115 & 0.837 & 0.245 & 0.093 \\ MagicBrush  & 0.123 & 0.883 & 0.261 & 0.058 \\ Pap  & 0.025 & 0.568 & 0.101 & 0.280 \\ NT-Inv  & 0.121 & 0.752 & 0.263 & 0.077 \\ Emu-Edit  & 0.135 & 0.897 & 0.261 & 0.052 \\ 
**Vitron** & **0.142** & **0.910** & **0.274** & **0.047** \\  w/o syng. & -0.012 & -0.104 & -0.078 & + 0.036 \\  

Table 11: Image editing results on MagicBrush .

module' in Table 2 to Table 12, we can observe that the synergy learning mechanism indeed positively influences overall performance. In Fig. 6 we further study whether there is synergy between different tasks and their collaborative relations. For ease of study, we considered a one-to-one mapping relationship, studying the cooperation between pairs of tasks one at a time. It is evident that the cooperative effects vary between different tasks. Tasks or backbone modules that rely more heavily on fine-grained visual features gained more significant improvements. This also demonstrates that our synergy learning module can successfully facilitate cross-task synergy.

## 7 Conclusion

In this work, we present Vitron, a grand unified pixel-level vision LLM for seamlessly understanding (perceiving and reasoning), generating, segmenting (grounding and tracking), and editing (inpainting) both images and videos. We further introduce a novel hybrid method of message passing that combines discrete textual instructions with continuous signal embeddings to ensure precise function invocation. Furthermore, Vitron employs pixel-level spatiotemporal vision-language alignment to enhance its fine-grained visual capabilities. A cross-task synergy module is also developed to optimize the use of task-invariant fine-grained visual features, boosting synergy across various visual tasks. On 12 visual tasks across 22 datasets, Vitron exhibits extensive capabilities in visual segmentation, fine-grained vision understanding, generation, and editing. Overall, this research showcases the great potential to build a vision-language generalist that can advance toward a more unified AI.