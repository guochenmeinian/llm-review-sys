# Multiclass Boosting:

Simple and Intuitive Weak Learning Criteria

 Nataly Brukhim

Department of Computer Science

Princeton University

&Amit Daniely

Department of Computer Science

Hebrew University

Google Research

&Yishay Mansour

Department of Computer Science

Tel Aviv University

Google Research

&Shay Moran

Faculty of Mathematics

Faculty of Computer Science

Faculty of Data and Decision Sciences

Technion

Google Research

###### Abstract

We study a generalization of boosting to the multiclass setting. We introduce a weak learning condition for multiclass classification that captures the original notion of weak learnability as being "slightly better than random guessing". We give a simple and efficient boosting algorithm, that does not require realizability assumptions and its sample and oracle complexity bounds are independent of the number of classes.

In addition, we utilize our new boosting technique in several theoretical applications within the context of List PAC Learning. First, we establish an equivalence to weak PAC learning. Furthermore, we present a new result on boosting for list learners, as well as provide a novel proof for the characterization of multiclass PAC learning and List PAC learning. Notably, our technique gives rise to a simplified analysis, and also implies an improved error bound for large list sizes, compared to previous results.

## 1 Introduction

Boosting is a powerful algorithmic approach used to boost the accuracy of weak learning models, transforming them into strong learners. Boosting was first studied in the context of binary classification in a line of seminal works which include the celebrated Adaboost algorithm, as well an many other algorithms with various applications (see e.g. ).

The fundamental assumption underlying boosting is that a method already exists for finding poor, yet not entirely trivial classifiers. Concretely, binary boosting assumes there exists a learning algorithm that, when presented with training examples, can find a classifier \(h:\{0,1\}\) that has classification error less than \(1/2\). That is, it performs slightly better than random guessing. The intuition is that this is the most minimal assumption one can make about a learning algorithm, without it being impractical. This assumption is called the _weak learning assumption_, and it is central to the study of boosting.

While binary boosting theory has been extensively studied, extending it to the multiclass setting has proven to be challenging. In particular, it turns out that the original notion of weak learnability as being "slightly better than a random guess", does not easily extend to the multiclass case. Forexample, perhaps the most natural extension is to assume that the learner has accuracy that is slightly better than \(1/k\), where \(=\{1,...,k\}\). However, this naive extension is in fact known to be too weak for boosting (see Section 2 below for a detailed discussion). Instead, previous works [19; 7; 23] have formulated various complex weak learning assumptions with respect to carefully tailored loss functions, and rely on restrictive realizability assumptions, making them less useful in practice.

A weak learning assumption.In this work, we generalize the classic formulation of boosting to the multiclass setting. We introduce a weak learning condition that captures the original intuition of weak learnability as "slightly better-than-random guessing". The key idea that renders this condition useful compared to previous attempts, is based on a "hint" given to the weak learner. The hint takes the form of a list of \(k\) labels per example, where \(k\) is possibly smaller than \(||\). Then, the assumption is that there exists a learner capable of producing not entirely trivial classifiers, if it was provided with a "good hint". In other words, if the list provided to the learner happens to contain the correct label, we expect the learner to perform slightly better than randomly guessing a label from the list. Specifically, the assumption is that for any \(k 2\), if the given lists of size \(k\) contain the true labels, the learner will output a classifier \(h:\) with error slightly better than random guessing among the \(k\) labels. Notice that this encompasses both the binary case when \(k=2\), as well as the naive extension mentioned above, when \(k=||\). We call this new condition the "better-than-random guess", or _BRG_ condition.

The BRG condition also generalizes the classic binary case condition in a practical sense. Previous methods on multiclass boosting are framed within the PAC (Probably Approximately Correct) setting, correspond to a _known_ hypothesis class \(^{}\), and assume that weak learning hold for every distribution \(\) over the _entire_ domain \(\). Practically, these requirements can be very difficult to check or guarantee. In contrast, as in binary boosting, the BRG condition can be relaxed to a more benign empirical weak learning assumption, that can be verified immediately in an actual learning setting.

Recursive boosting.Our main contribution is a new boosting algorithm that is founded on the BRG condition. Our boosting methodology yields a simple and efficient algorithm. It is based on the key observation that even a naive weak learner can produce a useful hint. Recall that when given no hint at all, the naive weak learner can still find a hypothesis with a slight edge (\(>0\)), over random guessing among \(||\) labels. Although this may result in a poor predictor, we prove that it effectively reduces the label space per example to approximately \(1/\) labels. This initial hint serves as the starting point for subsequent iterations of the boosting algorithm. The process continues recursively until the label list per example is reduced to size \(2\), at which point any classic binary boosting can yield a strong classifier. Unlike previous methods, our boosting algorithm and guarantees do not rely on realizability assumptions nor do they scale with \(||\). In fact, we show that the sample and oracle-call complexity of our algorithm are entirely independent of \(||\), which implies our approach is effective even in cases where the label space \(\) is possibly infinite. Moreover, the overall running time of our algorithm is polynomial in the size of its input.

An important insight that underlies our approach is the link between the naive weak learning condition, which we term _weak-BRG_ learning, to that of _list learning_. In list learning [5; 8; 18], rather than predicting a single outcome for a given unseen input, the goal is to provide a short list of predictions. Here we use this technique as an intermediate goal of the algorithm, by which effectively reducing the size of the label space in each round. The generalization analysis relies on sample compression arguments which result in efficient bounds on the sample and oracle complexities.

Perhaps surprisingly, the connection between weak learnability and list learnability is even more fundamental. We prove that there is an equivalence between these two notions. Specifically, we establish that a \(\)-weak learner is equivalent to an \((1/)\)-list learner.

Lastly, we demonstrate the strength of our boosting framework. First, we give a generalization of our boosting technique to hold for list PAC learning algorithms. Then, we showcase the effectiveness of the weak learning criteria in capturing learnability in two fundamental learning settings: PAC learning, and List PAC learning. Recently,  proved a characterization of multiclass PAC learning using the Daniely-Shwartz (DS) dimension. In a subsequent study,  gave a characterization of _list_ learnability using a natural extension of the DS dimension. Here we show that in both cases, assuming the appropriate dimension is bounded, one can devise a simple weak learning algorithm. Thus, it is also amenable to a boosting method similarly to our approach, leading to a novel and alternative proof of the characterization of learnability. We note that for cases where the dimension is much smaller than the list size, we have an improved result over previous bound. Moreover, our approach offers a simpler algorithm and analysis technique, potentially benefiting future applications as well.

### Main result

The main contributions in this work are as follows.

1. **Multiclass boosting framework.** Our main result is a boosting framework for the multiclass setting, which is a natural generalization of binary boosting theory. We give a simple weak learning assumption that retains the notion of weak learnability as "slightly-better-than-random-guess" from the binary case. Furthermore, we give an efficient multiclass boosting algorithm, as formally stated in Theorem 1 below. Our boosting algorithm is given in Section 3 (Algorithm 3).
2. **Applications: List PAC learning.** First, we establish an equivalence between List PAC learning and Weak PAC learning, demonstrating the strong ties between List PAC learning and multiclass boosting theory. Furthermore, we present a new result on boosting for list learners. Lastly, we give a novel and alternative proof for characterization of PAC learning and List PAC learning. In particular, the results imply a simplified algorithmic approach compared to previous works, and improved error bound for cases where the list size is larger than the appropriate dimension .

We will now introduce the main weak learning assumption, which we call the "better-than-random guess", or BRG condition, and state our main result in Theorem 1 below.

In its original form, the boosting question begins by assuming that a given hypothesis class \(\{0,1\}^{}\) is _weakly-PAC_ learnable. Similarly, here we present the BRG condition framed as weak (multiclass) PAC setting, followed by a relaxation to an _empirical_ variant of the assumption.

**Definition 1** (BRG condition).: _We say that an hypothesis \(h:\) satisfies the \(\)-BRG condition with respect a list function \(:^{k}\) on a distribution \(\) over examples if_

\[_{(x,y)}[h(x)=y](+)_{(x,y)}[y(x)].\] (1)

_We say that a learning rule \(\) satisfies the \(\)-BRG condition for a hypothesis class \(\) if for every \(\)-realizable distribution \(\), for every \(k 2\), for every list function \(:^{k}\), the output hypothesis \(h\) outputted by \(\) satisfies Equation (1) with probability \(1-\), when given \(m_{0}()\) i.i.d. examples from \(\), and given \(\)._

In words, the condition determines that if \(y\) belongs to the set \((x)\), then \(h\) has a higher probability of correctly classifying \(x\) by an additional factor of \(\), compared to a random guess from the list \((x)\).

However, requiring that the labels be deterministic according to a target function from a known class \(\), and that weak learning hold for every distribution \(\) over the entire domain \(\) are impractical, as they can be very difficult to check or guarantee.

Instead, as in the binary boosting setting, our condition can be relaxed to a more benign _empirical_ weak learning assumption, as given next.

**Definition 2** (Empirical BRG condition).: _Let \(S()^{m}\). We say that a learning rule \(\) satisfies the empirical \(\)-BRG condition for \(S\) if there is an integer \(m_{0}\) such that for every distribution \(p\) over \([m]\), for every \(k 2\), for every list function1\(:|_{S}^{k}\), when given \(m_{0}\) examples from \(S\) drawn i.i.d. according to \(p\), and given \(\), it outputs a hypothesis \(h\) such that,_

\[_{i=1}^{m}p_{i}[h(x_{i})=y_{i}](+ )_{i=1}^{m}p_{i}[y_{i}(x_{i})].\] (2)

Next, we give our main result of an efficient boosting algorithm, as stated in Theorem 1 below.

**Theorem 1** (Boosting (Informal)).: _There exists a multiclass boosting algorithm \(\) such that for any \(,>0\), and any distribution \(\) over \(\), when given a training set \(S^{m}\) and oracle access to a learning rule \(\) where2\(m=(}{^{3}})\) and applying \(\) with a total of \((1/^{3})\) oracle calls to \(\), it outputs a predictor \(:\) such that with probability at least \(1-\), we get that if \(\) satisfies the empirical \(\)-BRG condition for \(S\) then,_

\[_{(x,y)}[(x) y].\]

### Related work

Boosting theory has been extensively studied, originally designed for binary classification (e.g., AdaBoost and similar variants) . There are various extension of boosting to the multiclass setting.

The early extensions include AdaBoost.MH, AdaBoost.MR, and approaches based on Error-Correcting Output Codes (ECOC) [24; 1]. These works often reduce the \(k\)-class task into a single binary task. The binary reduction can have various problems, including increased complexity, and lack of guarantees of an optimal joint predictor.

Other works on multiclass boosting focus on practical considerations and demonstrate empirical performance improvements across various applications [29; 16; 15; 3; 6; 4; 21]. However, they lack a comprehensive theoretical framework for the multiclass boosting problem and often rely on earlier formulations such as one-versus-all reductions to the binary setting or multi-dimensional predictors and codewords.

Notably, a work by  established a theoretical framework for multiclass boosting, which generalizes previous learning conditions. However, this requires the assumption that the weak learner minimizes a complicated loss function, that is significantly different from simple classification error. Moreover, it is based on a restrictive realizability assumption with respect to a _known_ hypothesis class. In contrast, we do not require realizability, and only consider the standard classification loss.

More recently,  followed a formulation for multiclass boosting similar to that of . They proved a hardness result showing that a broad, yet restricted, class of boosting algorithms must incur a cost which scales polynomially with \(||\). Our approach does not fall in this class of algorithms. Moreover, our algorithm has sample and oracle complexity bounds that are entirely independent of \(||\).

## 2 Warmup: _too-weak_ weak learning

When there are only \(2\) labels, the weak learner must find a hypothesis that predicts the correct label a bit better than a random guess. That is, with a success probability that is slightly more than \(1/2\). When the number of labels \(k\) is more than \(2\), perhaps the most natural extension requires that the weak learner outputs hypotheses that predict the correct label a bit better than a random guess _among \(k\) labels_. That is, with a success probability that is slightly more than \(1/k\).

However, this is in fact known to be too weak for boosting (see e.g., , Chapter 10). Here we first give a simple example that demonstrates that fact. However, we also show that all is not yet lost for the "better-than-random-guess" intuition. Specifically, we describe how this condition can still allow us to extract valuable knowledge about which labels are _incorrect_. This observation will serve as a foundation for our main results, which we will elaborate on in the next section.

We start by defining the notion of better-than-random weak learner that we term weak-BRG learning.

**Definition 3** (weak-BRG learning).: _A learning algorithm \(\) is a weak-BRG learner for a hypothesis class \([k]^{}\) if there is \(>0\) and \(m_{0}:(0,1)\) such that for any \(_{0}>0\), and any \(\)-realizable distribution \(\) over \([k]\), when given \(m_{0} m_{0}(_{0})\) samples from \(\), it returns \(h:\) such that with probability \(1-_{0}\),_

\[_{(x,y) D}[h(x)=y]+.\] (3)

To get an intuition for why this definition is indeed too weak for boosting, consider the following simple example. Suppose that \(=\{a,b,c\}\), \(=\{1,2,3\}\), and that the training set consists of thethree labeled examples \((a,1),(b,2),\) and \((c,3)\). Further, we suppose that we are using a weak learner which chooses weak classifiers that never distinguish between \(a\) and \(b\). In particular, the weak learner always chooses one of two weak classifiers: \(h_{1}\) and \(h_{2}\), defined as follows. For \(x\{a,b\}\) then \(h_{1}\) always returns \(1\) and \(h_{2}\) always returns \(2\). For \(x=c\) they both return \(3\).

Then, notice that for any distribution over the training set, either \(h_{1}\) or \(h_{2}\) must achieve an accuracy of at least \(1/2\), which is significantly higher than the accuracy of \(1/k=1/3\). However, regardless of how weak classifiers are aggregated, any final classifier \(H\) that relies solely on the predictions of the weak hypotheses will unavoidably misclassify either \(a\) or \(b\). As a result, the training accuracy of \(H\) on the three examples can never exceed \(2/3\), making it impossible to achieve perfect accuracy through any boosting method.

Furthermore, we note that this simple example can also be extended to a case where the data is realizable by a hypothesis class which is not learnable by any learning algorithm (let alone boosting). For example, consider the hypothesis class \(=\{1,2,3\}^{}\) for \(=\). Then, \(\) is not PAC learnable (e.g., via No-Free-Lunch (, Theorem 5.1)). However, similarly as above, one can construct a learning rule that returns a hypothesis with accuracy \(1/2>1/k\) over an \(\)-realizable distribution.

Next, we will examine a useful observation that will form the basic building block of our algorithmic methodology. We demonstrate that the natural weak learner given in Definition 3, while weak, is nonetheless useful. This can be shown by examining the guarantees obtained through its application in boosting. Specifically, we consider the following classic variant of boosting via the Hedge algorithm.

```
0: Training data \(S([k])^{m}\), parameter \(>0\).
0: A predictor \(H:\).
1: Initialize: \(w_{1}(i)=1\) for all \(i=1,...,m\).
2:for\(t=1,,T\)do
3: Denote by \(_{t}\) the distribution over \([m]\) obtained by normalizing \(w_{t}\).
4: Draw \(m_{0}\) examples from \(_{t}\) and pass to the weak learner.
5: Get weak hypothesis \(h_{t}:\), and update for \(i=1,...,m\): \[w_{t+1}(i)=w_{t}(i)e^{- 1[h_{t}(x_{i})=y_{t}]}.\]
6:endfor
7: Output \(H\) such that for all \((x,y)[k]\), \[H(x,y)=_{t=1}^{T}[h_{t}(x)=y].\] ```

**Algorithm 1** Boosting via Hedge

Notice that the output of Algorithm 1 returns a predictor that is not a classifier, but a scoring function with the aim of predicting the likelihood of a given label candidate \(y[k]\) for some \(x\). Typically, boosting algorithms combine the weak hypothesis into such a scoring function yet their final output applies an _argmax_ over it, to yield a valid classifier. However, since the weak learning assumption is too weak as we have shown above, taking the argmax is useless in this setting.

Instead, the following lemma shows that by boosting the "too-weak" learner, we can guarantee to eliminate one label for each example in the data. Towards that end, we consider a relaxed variant of the weak-BRG learner, to be defined over a data set \(S\), which we term the _empirical weak-BRG_ learner. Specifically, we say that a learner satisfies the empirical weak-BRG condition if there is an integer \(m_{0}\) such that for any distribution over the training examples, when given \(m_{0}\) examples drawn i.i.d from it, the learner outputs a hypothesis that satisfies Equation (3).

Proofs are deferred to the appendix.

**Lemma 1** (Remove one label).: _Let \(S([k])^{m}\). Let \(\) be an empirical weak-BRG learner for \(S\) with respect to some \(\) and sample size \(m_{0}\). Then, the output \(H:([k])[0,T]\) obtained by running Algorithm 1 with \(T}\) and \(=}\), guarantees that for all \((x,y) S\)\(+\). Moreover, for all \((x,y) S\) the minimally scored label \(=_{[k]}H(x,)\) must be incorrect. That is \( y\)._

Notice that if we were to take the argmax of \(H\) as is typically done in boosting, the guarantees given in Lemma 1 do not suggest this will result in the correct prediction. In fact, this approach might yield a rather bad classifier even for the set \(S\) on which it was trained. In other words, for any \((x,y) S\) it may be that there is some incorrect label \(y^{} y\) with \(H(x,y^{})>H(x,y)\).

However, notice that the lemma does suggest a good classifier of _incorrect_ labels. That is, the lowest scored label will always be an incorrect one, over the training data. This property can be shown to generalize via compression arguments, as discussed in Section 5. This allows us to effectively reduce the size of the label space by one, and is used as the basic building of our algorithm, as detailed in the next section.

## 3 Multiclass boosting results

We start by introducing the notion of weak learnability that is assumed by our boosting algorithm. We note that it is a relaxation of the empirical BRG condition introduced in Definition 2 in the sense that it does not make any guarantees for the case that the given hint list does not contain the correct label. This may seem like significantly weakening the assumption, yet it turns out to be sufficient for our boosting approach to hold.

In the resulting fully relaxed framework, no assumptions at all are made about the data. Although the BRG condition is not explicitly assumed to hold, when this is the case, our final bound given in Theorem 2 implies a high generalization accuracy.

**Definition 4** (Relaxed Empirical \(\)-BRG learning).: _Let \(S()^{m}\), \(>0\), and integer \(m_{0}\). Let \(M\) be a set of list functions of the form 3\(:^{k}\) for any integer \(k\), such that for each \( M\) and \(i[m]\), then \(y_{i}(x_{i})\). A learning algorithm satisfies this condition with respect to \((S,,m_{0},M)\), if for any distribution \(p\) over \(S\) and any \(:^{k}\) such that \( M\), when given a sample \(S^{} p^{m_{0}}\) and access to \(\), it returns \(h:\) such that,_

\[_{(x,y) p}[h(x)=y]+.\] (4)

Notice that when the list \(\) returns the set of all possible labels \(\) and it is of size \(k\), this condition is essentially equivalent to the empirical weak-BRG condition, which as shown above is too weak for boosting. Requiring that the condition will hold for _any_ list size \(k||\) is sufficient to facilitate boosting, as shown in Theorem 2.

The starting point of our overall boosting algorithm (given in Algorirhm 3), is a simple learning procedure specified in Algorithm 4 that is used to effectively reduce the size of the label space. In particular, it is used to produce the initial "hint" function that is used by the boosting method.

```
0:\(S()^{m}\), parameters \(m_{0},p>0\).
0:A function \(:^{p}\).
1: Set \(S_{1}:=S\).
2:for\(j=1,...,p\)do
3: Let \(_{j}\) denote the uniform distribution over \(S_{j}\).
4: Draw \(m_{0}\) examples from \(_{j}\) and pass to the weak learner, with \(_{0}\).
5: Get weak hypothesis \(h_{j}:\).
6: Set \(S_{i+1}\) to be all the points in \(S_{i}\) which \(h_{j}\) predicts incorrectly.
7:endfor
8: Output \(\) defined by: \[(x)=h_{1}(x),,h_{p}(x)}.\] ```

**Algorithm 2** Initial hint We can now present our main boosting method in Algorithm 3, and state its guarantees in Theorem 2.

```
0: Training data \(S()^{m}\), edge \(>0\), parameters \(T,,p>0\).
0: A predictor \(:\).
1: Initialize: get \(_{1}\) by applying Algorithm 2 over \(S\).
2:for\(j=1,,p-1\)do
3: Call Hedge (Algorithm 1) with \(S\) and \(_{j}\), and parameters \(,T\) to get \(H_{j}:\). \(\)Modify Algorithm 1 to receive \(_{j}\) as input, and in line 4 pass it to the weak learner.
4: Construct \(_{j+1}:|_{S}^{p-j}\) such that for all \(x\), \[_{j+1}(x)=\{y\;:\;y_{j}(x)\;\;H_{j}(x,y)> \},\]
5:endfor
6: Output the final hypothesis \(:=_{p}\). ```

**Algorithm 3** Recursive Boosting

The following theorem is formally stating the main result given in Theorem 1.

**Theorem 2** (Boosting).: _Let \(\) denote a learning rule that when given any set of labeled examples and a list function, returns some hypothesis \(h:\). Let \(,,,m_{0}>0\), and let \(\) a distribution over \(\). Then, when given a sample \(S^{m}\) for \(m\;m_{0}\;(^{2}(m)())}{^{2} \;}\), oracle access to \(\) and \(T}\), \(p\), and \(=}\), Algorithm 3 outputs a predictor \(\) such that the following holds. Denote by \(M\) the sets of list functions on which \(\) was trained throughout Algorithm 3. Then, with probability at least \(1-\), we get that if \(\) satisfies the \(\)-BRG condition (as given in Definition 4) with respect to \((S,,m_{0},M)\) then,_

\[_{(x,y)}[(x) y].\]

Observe that Theorem 2 implicitly assumes that the sample complexity of the weak learner \(m_{0}\) is not strongly dependent on the overall sample size \(m\) and scales at most poly-logarithmically with \(m\). In other words, although the statement holds for any \(m_{0}\), the result becomes vacuous otherwise.

In addition, notice that Theorem 2 is quite agnostic in the sense that we have made no prior assumptions about the data distribution. Concretely, Theorem 2 tells us that the generalization error will be small _if_ the given oracle learner \(\) happens to satisfy the \(\)-BRG condition with respect to the particular inputs it receives throughout our boosting procedure.

Adaptive boostingBoosting algorithms typically do not assume knowing the value of \(\) and are adapted to it on the fly, as in the well-known Adaboost algorithm . However, the boosting algorithm given in Algorithm 1, as well as our boosting method as a whole, requires feeding the algorithm with a value estimating \(\). If the estimation of \(\) provided to the algorithm is too large, the algorithm may fail. This can be resolved by a simple preliminary binary-search-type procedure, in which we guess gamma, and possible halve it based on the observed outcome. This procedure only increases the overall runtime by a logarithmic factor of \(O((1/))\), and has no affect on the sample complexity bounds.

## 4 Applications to List PAC learning

The applications given in this section are based on the framework of _List PAC learning_[5; 8], and demonstrate that it is in fact closely related to the multiclass boosting theory. First, we establish an equivalence between list learnability and weak learnability in the context of the PAC model. Furthermore, we present a new result on boosting for list PAC learners. Lastly, we give a novel and alternative proof for characterization of PAC learnability and List PAC learnability. In particular, these imply a simplified algorithmic approach compared to previous works [5; 8].

We start with introducing list learning in Definition 5, followed by the definition of weak PAC learning, similarly to the weak-BRG learning definition we give in this work.

**Definition 5** (\(k\)-List PAC Learning).: _We say that a hypothesis class \(^{}\) is \(k\)-list PAC learnable, if there is an algorithm such that for every \(\)-realizable distribution \(\), and every \(,>0\), when given \(S^{m}\) for \(m m(,)\), it returns \(_{S}:^{k}\) such that with probability \(1-\),_

\[_{(x,y)}y_{S}(x) 1-.\]

**Definition 6** (\(\)-weak PAC Learning).: _We say that a hypothesis class \(^{}\) is \(\)-weak PAC learnable, if there is an algorithm such that for every \(\)-realizable distribution \(\), and every \(>0\), when given \(S^{m}\) for \(m m()\), it returns \(h_{S}:\) such that with probability \(1-\),_

\[_{(x,y)}y=h_{S}(x).\]

Next, in the following lemmas we show the strong connection between these two notions. Specifically, we give an explicit construction of a list learner given oracle access to a weak learner, and vice versa.

**Lemma 2** (Weak \(\) List Learning).: _Let \(^{}\) be a hypothesis class. Assume \(\) is a \(\)-weak PAC learner for \(\) with sample complexity \(m_{w}:(0,1)\). Let \(k\) be the smallest integer such that \(<\), and denote \(=-\). Then, there is an \((k-1)\)-List PAC learner with sample complexity \(m(,)=((/T)}{^{2}})\) where \(T=(})\) is the number of its oracle calls to \(\)._

**Lemma 3** (List \(\) Weak Learning).: _Let \(^{}\) be a hypothesis class. Assume \(\) is a \(k\)-List PAC learner for \(\) with sample complexity \(m_{}:(0,1)\). Then, for any \(>0\) there is an \(\)-Weak PAC learner where \(=\), with sample complexity \(m()=(m_{}(,1/2) k+(k/)^{2})\) where \(q=2k(2/)\) is the number of its oracle calls to \(\)._

Lastly, Theorem 3 concludes this section demonstrating the strong ties between weak and list learnability. Concretely, it combines the results of both Lemma 2 and Lemma 3 above to show that when the appropriate parameters \(\) and \(k\) are optimal, then \(\)-PAC learnability and \(k\)-list PAC learnability are in fact equivalent.

**Theorem 3** (Optimal accuracy \(\) Optimal list size).: _Let \(^{}\). Denote by \(k()\) the smallest integer \(k\) for which \(\) is \(k\)-list PAC learnable, assuming that \(k()<\). Denote by \(()\) the supremum over \(\) for which \(\) is \(\)-weak PAC learnable. Then, it holds that \(k()()=1\)._

### List boosting and conformal learning

List prediction rules naturally arise in the setting of _conformal learning_. In this model, algorithms make their predictions while also offering some indication of the level of reliable confidence in those predictions. For example in multiclass classification, given an unlabeled test point \(x\), the conformal learner might output a list of all possible classes along with scores which reflect the probability that \(x\) belongs to each class. This list can then be truncated to a shorter one which contains only the classes with the highest score. See the book by  and surveys by  for more details.

We now consider a closely related notion of List PAC learnability, that similarly to conformal learning allows the list size to depend on the desired confidence. This was also defined in , termed weak List PAC Learning, due to the dependence of the list size on the input parameter.

Indeed, it is natural to expect that the list size will increase when we require a more refined accuracy, and perhaps that this is a weaker notion of learnability than that of List PAC learning, which corresponds to a fixed list size.

Interestingly, it turns out that weak List PAC Learning is in fact equivalent to strong List PAC Learning. In other words, a list learner with a list size that varies with the desired accuracy parameter can be _boosted_ to a list learner with a fixed list size, and arbitrarily good accuracy. The proof is by way of a generalization of our boosting technique to lists, as stated in Theorem 4.

**Theorem 4** (List boosting).: _Let \(^{}\). Let \(_{0},_{0}>0\), and assume that there exists an algorithm such that for every \(\)-realizable distribution \(\), and for some integer \(k_{0}:=k_{0}(_{0})\), when given \(S^{m}\) for \(m m(_{0},_{0})\), it returns \(_{S}:^{k_{0}}\) such that with probability \(1-_{0}\),_

\[_{(x,y)}y_{S}(x) 1-_{0}.\]_Then, there is a \(k\)-List PAC learning algorithm for \(\) for a fixed list size \(k=}{1-2_{0}}\)._

Observe that Theorem 4 indeed generalizes classic boosting. Specifically, consider the binary setting and notice that when \(k_{0}=1\), and \(_{0}\) is slightly smaller than \(1/2\), Theorem 4 implies that weak learning with edge \(-_{0}\), is equivalent to strong learning with arbitrarily small error. The following corollary shows that weak List PAC Learning implies strong List PAC Learning.

**Corollary 1**.: _If a class \(^{}\) is weakly-List PAC learnable then it is also List PAC learnable._

### Characterization of List PAC learnability

We now focus on the characterization of List PAC learnability, which also implies the characterization of PAC learnability. Towards that end, we define the Daniely-Shwartz (DS) dimension . Specifically, we give the natural generalization of it to \(k\)-sized lists, called the \(k\)-DS dimension.

**Definition 7** (\(k\)-DS dimension ).: _Let \(^{}\) be a hypothesis class and let \(S^{d}\) be a sequence. We say that \(\)\(k\)-DS shatters \(S\) if there exists \(,\,||<\) such that \( f|_{S},\  i[d],\,f\) has at least \(k\)\(i\)-neighbors. The \(k\)-DS dimension of \(\), denoted as \(d^{k}_{DS}=d^{k}_{DS}()\), is the largest integer \(d\) such that \(\)\(k\)-DS shatters some sequence \(S^{d}\)._

We note that when \(k=1\), this captures the standard DS dimension . We show that when the \(k\)-DS dimension is bounded, one can construct a simple weak learner which satisfies our BRG condition. Thus, it is also amenable to our boosting method, leading to a qualitatively similar results for characterization of learnability as in . The result is given in the next theorem.

**Theorem 5** (PAC and List-PAC learnability).: _Let \(^{}\) be an hypothesis class with \(k\)-DS dimension \(d<\). Then, \(\) is List PAC learnable. Furthermore, there is a learning algorithm \(A\) for \(\) with the following guarantees. For every \(\)-realizable distribution \(\), every \(>0\) and every integer \(m\), given an input sample \(S^{m}\), the algorithm \(A\) outputs \(=A(S)\) such that4_

\[_{(x,y)}[(x) y]k ^{4}+(1/)}{m},\]

_with probability at least \(1-\) over \(S\). In particular, if \(k=1\), then \(\) is PAC learnable._

We remark that for cases where \(d k\) we have an improved result over the bound given by . For comparison, the error bound given by , Theorem 2 is \((k^{6}+(1/)}{m})\).

Thus, Theorem 5 demonstrates that our boosting-based approach gives rise to an alternative proof for the characterization of PAC learnability and List PAC learnability. Moreover, our approach offers a simpler algorithm and analysis technique, that can perhaps be of use in future applications as well.

## 5 Generalization via compression

This section is concerned with the analysis of our boosting method given in Algorithm 3, and the proof of our main result given in Theorem 1 (and formally in Theorem 2).

The boosting algorithm given in this work is best thought of as a _sample compression scheme_. A sample compression scheme (Definition 8) is an abstraction of a common property to many learning algorithms. It can be viewed as a two-party protocol between a _compresser_ and a _reconstructor_. The compressor gets as input a sample \(S\). The compressor picks a small subsample \(S^{}\) of \(S\) and sends it to the reconstructor. The reconstructor outputs an hypothesis \(h\). The correctness criteria is that \(h\) needs to correctly classify _all_ examples in the input sample \(S\). We formally define it next.

**Definition 8** (Sample Compression Scheme ).: _Let \(r m\) be integers. An \(m r\) sample compression scheme consists of a reconstruction function_

\[:()^{r}^{}\]

_such that for every \(S()^{m}\), there exists \(S^{} S\) of size \(r\), such that for all \((x,y) S\) it holds that \(h(x)=y\), where \(h=(S^{})\)._We are now ready to prove the main result, given in Theorem 2. The next paragraph highlights the assumptions that were made, followed by the proof of the theorem.

Specifically, we assume for simplicity that the learning algorithm does not employ internal randomization. Thus, it can be regarded as a fixed, deterministic mapping from a sequence of \(m_{0}\) unweighted examples, and a function \(:^{k}\), to a hypothesis \(h:\). We note that our results remain valid for a randomized learner as well, yet we assume the above for ease of exposition.

Proof of Theorem 2.: The proof is given via a sample compression scheme, demonstrating that if weak learnability holds, then the final predictor \(\) can be represented using a small number of training examples, and that it is consistent with the entire training set.

First, we fix the sample \(S\) and assume that the \(\)-BRG condition holds for \(S\) as in the theorem statement. We will then show for each \(j=1...p\) that \(_{j+1}\) satisfies the following 3 properties: (a) for each \(x\) it returns at most \(p-j\) labels, (b) for all \((x,y) S\), it holds that \(y_{j+1}(x)\), and (c) it can be represented using only a small number of training examples.

First, note that \(_{1}\) is indeed a mapping to at most \(p\) labels by its construction in Algorithm 2. Moreover, recall that by the above assumption, the weak learner is a deterministic mapping from its input to a hypothesis. Therefore, any hypothesis produced by the weak learner within Algorithm 2 can be represented simply by the sequence of \(m_{0}\) examples on which it was trained. Lemma 4 implies that there is a subset \(S^{} S\) of size at most \(m_{0} p\), where \(p=(m)/\) such that the following holds: There are \(p\) hypotheses \(h^{}_{i}:\), that comprise the list \(_{1}\), where each \(h^{}_{i}\) can be represented by \(m_{0}\) examples in \(S^{}\). It is also guaranteed by Lemma 4 that for all \((x,y) S\), it holds that \(y_{1}(x)\).

Next, we will show that the 3 properties (a)-(c) above holds for \(_{2}\) (and similarly for all \(j 2\)). Consider the first \(T\) weak hypotheses \(h^{(1)}_{1},...,h^{(1)}_{T}\) generated by Algorithm 3, within its first call to Algorithm 1. Notice that each \(h^{(1)}_{i}\) can now be represented by the sequence of \(m_{0}\) examples on which it was trained, as well as the same \(m_{0} p\) examples from above that correspond to \(_{1}\). Therefore, we can represent the mapping \(_{2}\) by a total of \(T m_{0}+m_{0} p\) examples. Next, we will show that (a) and (b) hold, by applying Lemma 1. Specifically, we use it to prove that for each \((x,y) S\), \(_{2}(x)\) returns \(p-1\) labels, and also that \(y_{2}(x)\).

We first show that the conditions of Lemma 1 are met, by considering a simple conversion of all the labels according to \(_{1}\). Specifically, since both Lemma 1 and Algorithm 1 assume the labels are in \([k]\), yet both Algorithm 3 and our weak learner assume the labels in \(S\) are in \(\), we can think of mapping each \(y\) to \([p+1]\) according to \(_{1}\), getting its corresponding label \([p+1]\) in the mapped space, and then remapping back to the \(\) space when returning to Algorithm 3.

Concretely, for each pair \((x,y) S\), convert it to \((x,)[p]\), such that the \(\)-th entry of \(_{1}(x)\) is \(y\), denoted \(_{1}(x)_{}=y\). By Definition 4 we obtain a hypothesis \(h:\). For its internal use in Algorithm 1, we convert it into a hypothesis \(h^{}:[p+1]\) such that if \(h(x)(x)\), set \(h^{}(x)=\) for \(\) that satisfies \(_{1}(x)_{}=h(x)\), or \(h^{}(x)=p+1\) if there is no such \([p]\). Finally, we set the output \(H_{1}\) of Algorithm 1 to be defined with respect to the original, remapped, weak hypotheses \(h\).

Now applying Lemma 1 with \(k:=p\), we get that for all \((x,y) S\), we have \(H_{1}(x,y)>T/p\). Therefore, it must hold that \(y_{2}(x)\). Moreover, since \(_{y^{}_{1}(x)}H_{1}(x,y^{})_{y^{} }H_{1}(x,y^{}) T\),

Lemma 1 implies that there must be a label \(y^{} y\) such that \(y^{}_{1}(x)\) for which \(H_{1}(x,y^{})<T/p\). Therefore, by construction of \(_{2}\) we get that \(y^{}_{2}(x)\), and \(|_{2}(x)||_{1}(x)\{y^{}\}|=p-1\).

Next, we continue in a similar fashion for all rounds \(j=3,...,p-1\). Namely, the same arguments as above show that by applying Lemma 1 with \(k:=p-j+2\), we get that \(_{j+1}\) satisfies the above conditions over \(S\). Moreover, to represent each weak hypotheses \(h^{(j)}_{i}\) generated by Algorithm 3 within its \(j\)-th call to Algorithm 1, we use the sequence of \(m_{0}\) examples on which it was trained, as well as the same \((j-1) T m_{0}+m_{0} p\) examples from above that correspond to \(_{j}\).

Overall, we have shown that if \(\) satisfies the \(\)-BRG condition (as given in Definition 4) with respect to \((S,,m_{0},M)\) then the final predictor \(:=_{p}\) is both consistent with the sample \(S\), and can be represented using only \(r\) examples, where,

\[r=(p-1) T m_{0}+m_{0} p=O((m)}{ ^{2}})=O(^{2}(m)}{^{3}}).\] (5)We can now apply a sample compression scheme bound to obtain the final result. Specifically, we apply Theorem 6 (for \(k=1\)), for a \(m r\) sample compression scheme algorithm \(\) equipped with a reconstruction function \(\) (see Definition 8). We denote \(_{}()=_{(x,y)}[(x) y]\). Then, by Theorem 6 we get that,

\[_{S^{m},}[S_{}()> ],\]

where the overall randomness of our algorithm is denoted by \(\). Plugging in \(r\) from Equation (5), and \(m\) given in the theorem statement, yields the desired bound.