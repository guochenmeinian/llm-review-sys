# Robust Reinforcement Learning from Corrupted Human Feedback+

Footnote â€ : The authors are listed in alphabetical order: \({}^{1}\)Georgia Tech, \({}^{2}\)Amazon.

Alexander Bukharin\({}^{1}\)  Ilgee Hong\({}^{1}\)  Haoming Jiang\({}^{2}\)  Zichong Li\({}^{1}\)  Qingru Zhang\({}^{1}\)

Zixuan Zhang\({}^{1}\) &Tuo Zhao\({}^{1}\)

###### Abstract

Reinforcement learning from human feedback (RLHF) provides a principled framework for aligning AI systems with human preference data. For various reasons, e.g., personal bias, context ambiguity, lack of training, etc, human annotators may give incorrect or inconsistent preference labels. To tackle this challenge, we propose a robust RLHF approach - \(R^{3}M\), which models the potentially corrupted preference label as sparse outliers. Accordingly, we formulate the robust reward learning as an \(_{1}\)-regularized maximum likelihood estimation problem. Computationally, we develop an efficient alternating optimization algorithm, which only incurs negligible computational overhead compared with the standard RLHF approach. Theoretically, we prove that under proper regularity conditions, \(R^{3}M\) can consistently learn the underlying reward and identify outliers, provided that the number of outlier labels scales sublinearly with the preference sample size. Furthermore, we remark that \(R^{3}M\) is versatile and can be extended to various preference optimization methods, including direct preference optimization (DPO). Our experiments on robotic control and natural language generation with large language models (LLMs) show that \(R^{3}M\) improves robustness of the reward against several types of perturbations to the preference data.

## 1 Introduction

As artificial intelligence (AI) systems continue to advance and become increasingly sophisticated, ensuring their alignment with human values and preferences has emerged as a paramount concern, particularly for recent large language models . One promising approach to achieving this alignment is Reinforcement Learning from Human Feedback (RLHF), which involves training AI systems through a process of reward modeling based on human-provided feedback and preferences .

A significant challenge in RLHF, however, arises from the inherent uncertainty present in the preference data provided by human evaluators . Since RLHF often targets highly complex scenarios where defining precise preference standards is difficult, if not impossible, annotators may provide undesirable or inconsistent preference labels, especially when they lack sufficient experience or training. In the case of a robotics system designed to assist with household tasks, an untrained annotator might label actions that complete the task efficiently but in a manner that could potentially cause property damage or compromise safety as preferable, overlooking the importance of safe and responsible operation.

An even more concerning scenario is that some human evaluators may maliciously assign incorrect preference labels . Personal prejudices, agendas, or lack of understanding about the true goals of the system could lead some annotators to intentionally mislabel examples, which could undermine the entire RLHF process and cause the model to learn undesirable or misaligned behaviors, posing asignificant risk to the robustness and reliability of the AI system. Despite its critical importance for AI system alignment, this issue has received limited attention in the existing literature. For example, when training an AI system for automated content moderation on social media platforms, malicious annotators could mislabel examples of hate speech, misinformation, or harmful content as desirable, leading the model to learn to allow the proliferation of toxic and dangerous online behaviors. Despite its critical importance for AI system alignment, the robustness of RLHF has only received limited attention in the existing literature .

To address this challenge, we propose a robust Reinforcement Learning from Human Feedback (RLHF) approach - \(R^{3}M\) (**R**obust **R**eward **M**odeling for **RL**HF) to handle partially corrupted preference labels. Specifically, we assume that a subset of incorrect (corrupted) labels exists as outliers in the preference data used for training the reward model.2 To model the label corruption, we introduce an instance-specific perturbation factor to the Bradley-Terry (BT) model for human preference . We then learn the reward model and perturbation factors simultaneously by maximizing an \(_{1}\)-regularized likelihood of the preference data. Theoretically, we prove that under proper regularity conditions, our approach can consistently learn the underlying ground truth reward and identify potential outliers, provided that the number of incorrect labels scales sublinearly with the preference sample size. Computationally, we show that the additional computational overhead of involving the perturbation factor in training is negligible: The log-likelihood is strictly convex and univariate with respect to each perturbation factor, and we can obtain its closed-form update at each iteration.

To demonstrate the effectiveness of our proposed method, we apply \(R^{3}M\) to robotic control . Specifically, we consider different types of corruptions to the preference data, including irrational flipping, stochastic flipping, and myopic flipping. We train robust reward models with \(R^{3}M\) and optimize the policy based on the learned reward. We observe that \(R^{3}M\) outperforms the standard RLHF method for all tasks under all preference models.

Moreover, \(R^{3}M\) can be further generalized to other preference optimization methods. For example, we incorporate \(R^{3}M\) into direct preference optimization (DPO)  and evaluate its performance on two natural language generation tasks - dialogue and summarization. We adopt Llama-2 7B  and use Claude 3 as the judge. We find that \(R^{3}M\)-DPO outperforms DPO in policy learning for both tasks, and our results suggest that the training data of both tasks are very likely to have a small percentage of corruptions. Besides, we also consider random flipping for corrupting the preference data, and the results also show that \(R^{3}M\)-DPO outperforms DPO.

## 2 Related works

**Robust reward modeling**. Research on robust reward modeling for RLHF remains limited, though some prior works have explored various types of robustness: The most relevant results are from  and . They consider partially corrupted preference labels and propose to filter corrupted data based on the label confidence;  address robustness to diverse human preferences by learning a mixture of reward models;  focus on robustness to sampling temperature and length bias, and develop an iterative version of DPO;  consider reward overoptimization and employ a reward model ensemble.

While not explicitly framed as robustness, several other methods relate to this challenge: The  approach modifies DPO's loss function to avoid overfitting from weak regularization.  also explore reward ensembles;  develop a nonconvex human-aware loss, which downweighs training samples for reward learning when preference labels cannot be correctly predicted by reward models.

**Robust classification**. The reward learning problem in RLHF is related to classification, as both involve learning functions that map inputs to class labels or preference labels. However, in classification, the goal is to accurately predict class labels, while in RLHF, the goal is to learn a reward function that assigns scalar rewards to inputs, which are then used to optimize a policy or model. Despite these differences, the robustness literature in classification offers valuable insights for robust reward learning in RLHF.

The existing literature on robust classification has explored several directions. For instance,  and follow-up works  have focused on developing nonconvex loss functions that are robust to outliers. Additionally,  and subsequent works  have investigated instance-dependent calibration of nonconvex loss functions, which requires some prior knowledge of label noise. Other works propose iteratively filtering data based on uncertainties of labels or losses , which can be viewed as a relaxation of some nonconvex loss functions.  and follow-up works  have concentrated on robustness to distribution shifts. More recently,  and subsequent works [24; 26; 49] have explored adversarial robustness against the worse-case perturbation to the input.

## 3 Robust reinforcement learning from corrupted human feedback

We first introduce the problem setup on corrupted preference data, and then present \(R^{3}M\) for robust reward modeling. Lastly, we develop an efficient optimization algorithm for \(R^{3}M\) and further extend \(R^{3}M\) to direct preference optimization.

### Corruption to human feedback

We consider a Markov Decision Process (MDP) \(=(,,P,r,)\) with state \(s\), action \(a\), state transition kernel \(P\), discount factor \(\), and the reward function \(r:\), which is assumed to be aligned with human preferences. To learn such a reward function, we collect a (potentially corrupted) human preference dataset \(_{0}\) by some behavior policy \(_{}\), which contains \(n\) pairs of trajectory segments \(_{0}=(z_{w,i},z_{,i})_{i=1}^{n}\). Here, a trajectory segment \(z\) of length \(m\) denotes a sequence of consecutive state and action pairs \(\{(s_{t},a_{t})\}_{t=1}^{m}\) sampled according to some behavior policy, and \(z_{w,i}\) and \(z_{,i}\) denote the trajectory segments preferred and dispreferred by the human annotators, respectively.

Different from the conventional RLHF approach, we assume that the human preference follows a distribution perturbed by potential corruption:

\[p(z_{w,i} z_{,i};r^{*},_{i}^{*})=(r^{*}(z_{w,i})-r^{*}(z_ {,i})+_{i}^{*}), \]

where \(r^{*}\) denotes the ground truth reward function when applied to the trajectory segment, \(r^{*}(z):=_{t=1}^{m}^{t}r^{*}(s_{t},a_{t})\) with discount factor \((0,1]\), \((x)=1/(1+(-x))\) denotes the sigmoid function, and \(_{i}^{*}\) is a deterministic perturbation modeling the annotator's bias. Note that when \(_{i}^{*}=0\), (3.1) is reduced to the standard Bradley-Terry (BT) model; when \(_{i}^{*} r^{*}(z_{,i})-r^{*}(z_{w,i})\), the annotator is very likely to give an incorrect preference. For notational simplicity, we denote \(^{*}=[_{1}^{*},...,_{n}^{*}]^{}^{n}\), and we consider the case where \(^{*}\) is a sparse vector, i.e., the annotators' biases and mistakes only happen to a fraction of the preference data.

**Remark 3.1**.: Note that the perturbation factors \(_{i}\)'s are assumed to be deterministic and arbitrary. They can be intentionally introduced to mislead or confuse the reward learning process. This is in general more challenging than the setting of stochastic outliers, where the labels are flipped according to certain distribution.

### Method

We next develop the estimators of the ground truth reward \(r^{*}\) and the sparse \(^{*}\). To encourage the sparsity of \(\), we propose to minimize an \(_{1}\)-regularized negative log-likelihood of \(r\) and \(_{i}\)'s over the preference data:

\[(,)=*{argmin}_{r,}_{ }(r,)=-_{i=1}^{n}[ p(z_{w,i} z _{,i};r,)]+\|\|_{1}, \]

where \((0,1)\) is a tuning parameter, and \(\|\|_{1}=_{i=1}^{n}|_{i}|\) denotes the \(_{1}\) norm of \(\). The \(_{1}\) regularizer has been widely used in the existing literature on sparse estimation, such as Lasso . It can be viewed as a convex relaxation of the \(_{0}\) norm of \(\), i.e., \(\|\|_{0}=_{i=1}^{n}(_{i} 0)\). By tuning \(\) from large to small, we can control the number of nonzero entries in \(\) from small to large.

**Remark 3.2**.: The standard preference loss function is more susceptible to the influence of outliers in the training data. Therefore, the model may exhibit underfitting on the inlier (clean) data points, as it attempts to minimize the impact of the outliers on the overall loss. This eventually can distort the decision boundary, leading to suboptimal performance on the majority of the inlier data.

Once the reward is learned, we further apply Proximal Policy Optimization (PPO, ) to find a policy \(\), which maximizes the expected sum of discounted rewards,

\[=*{argmax}_{}_{(s_{t},a_{t}) _{}}_{t=1}^{}^{t}(s_{t},a_{t} ),\]

where \(_{}\) denotes the stationary distribution of the state-action pair induced by \(\).

### Alternating optimization

We present an efficient alternating optimization algorithm for solving (3.2). Suppose we parameterize the reward model \(r\) as a neural network with parameter \(\). At the \(k\)-th iteration, we have the iterate \(^{(k)}\), and we sample a pair of trajectory segments \(z_{w,i}\) and \(z_{,i}\). We first fix \(^{(k)}\) and minimize the loss with respect to \(_{i}\) by

\[_{i}^{(k+1)}=*{argmin}_{_{i}}-((r(z_{w,i}; ^{(k)})-r(z_{,i};^{(k)})+_{i}))+|_{i}|. \]

By examining the optimality condition of (3.3),

\[(r(z_{w,i};^{(k)})-r(z_{,i};^{(k)})+_{i})-1+ _{i}=0,\]

where \(_{i}|_{i}^{(k+1)}|\), we can obtain a closed-form solution

\[_{i}^{(k+1)}=\{(1/-1)-r(z_{w,i};^{(k)})+r(z_{,i}; ^{(k)}),0\}. \]

Denote \(_{i}(,_{i})=-((r(z_{w,i};)-r(z_{,i};)+ _{i}))+|_{i}|,\) we update \(\) by a stochastic gradient descent step given \(_{i}^{(k+1)}\)

\[^{(k+1)}=^{(k)}-_{}_{}_{i}(^{(k)},_{i} ^{(k+1)}), \]

where \(_{}\) is the learning rate.

### Extension to direct preference optimization (DPO)

Our proposed \(R^{3}M\) approach is generic and can be extended to DPO , which is another popular method for policy learning from human preferences. DPO directly learns the policy in supervised manner using the preference data of state-action pairs \(_{0}=(s_{i},a_{w,i},a_{,i})_{i=1}^{n}\). This approach forgoes the need to learn the reward function explicitly by reparameterizing the reward function \(r\) with respect to its optimal policy \(_{r}\): Recall that \(_{}\) denotes the behavior policy, we have

\[r(s,a)=((a|s)}{_{}(a|s)})+  Z(s),Z(s)=_{a}_{}(a|s)( ), \]

\(>0\) is a tuning parameter controlling the KL divergence between \(_{}\) and \(_{}\). By plugging in (3.6) back into (3.2), we have the policy optimization problem

\[(,)=*{argmin}_{,}_{}()=-_{i=1}^{n}(( r_ {}(a_{w,i}|s)- r_{}(a_{,i}|s)+_{i}))+ \|\|_{1},\]

where \(r_{}(a|s)=((a|s)/_{}(a|s))\) denotes the log-probability ratio.

## 4 Theoretical analysis

We next establish the statistical guarantees for \(R^{3}M\) on the reward recovery. Specifically, we prove that the reward function learned by \(R^{3}M\) from corrupted human feedback can be as accurate as its counterpart without outliers.

To better convey our theoretical insights, we consider a bandit setting, i.e., MDP with a horizon of one, mirroring the setup in DPO (see Section 3.4). The preference data of state-action pairs are given as \(_{0}=\{(s_{i},a_{1,i},a_{2,i},y_{i})\}_{i=1}^{N}\), where \(y_{i}=(a_{1,i} a_{2,i})\) denotes whether \(a_{1,i}\) is preferred to \(a_{2,i}\). Such a setting is common in real-world LLM applications such as (single-turn) question-answering or text summarization task, where \(a_{1,i}\) and \(a_{2,i}\) denote two different responses corresponding to the same prompt \(s_{i}\).To ease the theoretical analysis, we consider a tabular setting, where the number of states \(||\) and the number of actions \(||\) are finite. For notational simplicity, we denote the true reward as a vector \(R^{*}=[r^{*}(s,a)]^{||||},\) which concatenates the rewards of all state-action pairs, \(r^{*}(s,a)\) with \(s\) and \(a\).

Before we proceed with our main results, we first present the statistical guarantees of standard RLHF on the reward recovery, when there is no outlier in preference data (\(^{*}=0\)). Specifically, we can adapt Lemma 3.1 in Zhu et al.  to our setting: The Maximum Likelihood Estimator (MLE) \(\) attains the following statistical rate of convergence:

\[\|-R^{*}\|_{_{0}}^{2}=(|| |}{n}), \]

with overwhelming probability. Here, \(_{0}=_{i=1}^{n}x_{i}x_{i}^{}\) is a positive semi-definite matrix depending on the training dataset \(_{0}\) with \(x_{i}=(s=s_{i},a=a_{1,i})-(s=s_{i},a=a_{2,i})^{||||}\), and \(\|\|_{_{0}}\) is the matrix norm defined as \(\|v\|_{_{0}}^{2}=v^{}_{0}v\) for any vector \(v^{||||}\).

**Remark 4.1**.: As has been shown in Zhu et al. , given (4.1), one can further prove the desirable regret bound for the learnt policy. Therefore, our theoretical analysis only focuses on the statistical guarantees on the reward recovery.

We then impose the following two assumptions on the problem.

**Assumption 4.2**.: The perturbation \(^{*}\) only has \(s 0\) non-zero entries, i.e. \(\|^{*}\|_{0} s\). Moreover, there exists a constant \(C>0\) such that \(\|^{*}\|_{} C\).

**Assumption 4.3**.: Let \(B>0\) be some constant. We have \(r^{*}_{B}\), where

\[_{B}=\{r:\ \ _{s,a}r(s,a)=0,\|R\|_{2}^{2}=_{s,a }(r(s,a))^{2} B\}.\]

Note that \(s\) is allowed to scale with \((n,|S|,|A|)\), but \(C\) and \(B\) are not. This is mainly due to technical reasons to ensure the model identifiability. Accordingly, we adopt a constrained MLE formulation:

\[(,)=*{argmin}_{r,}- _{i=1}^{n}[ p(s_{i},a_{1,i},a_{2,i},y_{i};r,)]+ \|\|_{1}r_{B}, \]

where \(p(s_{i},a_{1,i},a_{2,i},y_{i};r,)\) is defined under the bandit setting as follows:

\[p(s_{i},a_{1,i},a_{2,i},y_{i};r,) =(y_{i}=1)(r(s_{i},a_{1,i})-r(s_{i},a_{2,i})+ _{i})\] \[+(y_{i}=0)(r(s_{i},a_{2,i})-r(s_{i},a_{1,i}) +_{i}).\]

Note that we add the constraint \(r_{B}\) in (4.2) also due to the technical reason under the tabular setting. In practice, the reward model with function approximation is usually trained with proper regularization, and therefore \(r\) can be bounded without any constraint.

**Theorem 4.4**.: Suppose Assumptions 4.2 and 4.3 hold. Let \(=[(s,a)]\) and \(\) be the minimizer of (4.2). Given \(=1/n\), there exists universal constants \(C_{0}>0\) and \(\), such that we have

\[\|-R^{*}\|_{_{0}}^{2}+\|-^{ *}\|_{2}^{2}}(+| |||}{n})\]

with overwhelming probability.

Proof Sketch.: Due to space limit, we only present a proof sketch here. The technical proof of the lemmas can be found in Appendix A. For notational simplicity, we denote \( R=-R^{*}\) and \(=-^{*}\), and denote the negative log-likelihood function on \(_{0}\) as

\[(R,) =-_{i=1}^{n}[ p(s_{i},a_{1,i},a_{2,i},y_{i };r,)]\] \[=-_{i=1}^{n}[((y_{i}=1) ( x_{i},R+_{i})+(y_{i}=0)(- x _{i},R+_{i}))],\]

where we use \( x_{i},R=(s=s_{i},a=a_{1,i})-(s=s_{i },a=a_{2,i}),R=r(s_{i},a_{1,i})-r(s_{i},a_{2,i})\). Since \(_{B}\) and \(\) are the minimizers of (4.2) and \(r^{*}_{B}\) by Assumption 4.3, we have

\[(,)+\|\|_{1} (R^{*},^{*})+\|^{*}\|_{1}. \]

The next Lemma proves the strong convexity of \(\) in \(R\) and \(\) at \((R^{*},^{*})\) and \((,^{*})\), respectively.

**Lemma 4.5**.: Suppose Assumptions 4.2 and 4.3 hold. Let \(=1/(2+(-B-C)+(B+C))\). \(\) is strong convex with respect to \(R\) at \((R^{*},^{*})\) with parameter \(\),

\[(R^{*}+ R,^{*})-(R^{*},^{*})- _{R}(R^{*},^{*}), R\| R\| _{_{0}}^{2}. \]

Moreover, \(\) is \(/n\)-strong convex with respect to \(\) at \((,^{*})\),

\[(,^{*}+)-(, ^{*})-_{}(,^{*}), \|\|_{2}^{2}. \]

Given the index set \(=\{i\{1,2,,n\}|_{i}^{*} 0\}\) and \(^{c}=\{1,2,,n\}\), we can decompose any \(^{n}\) by the index set \(\) and \(^{c}\) as follows:

\[=_{}+_{^{c}}.\]

Here \(_{}\) has the same non-zero entries as \(^{*}\). Now we apply the strong convexity of \(\) to (4.3), use Cauchy-Schwartz inequality to bound the inner product, and use decomposability of \(\) to obtain the following result.

**Lemma 4.6**.: Given the strong convexity of \(\) in (4.4) and (4.5), let \(\|_{}(,^{*})\|_{}\), we have

\[\| R\|_{_{0}}^{2}+\|\|_{2}^{2} 2 \|_{}\|_{1}+\|_{R}(R^{*},^ {*})\|_{_{0}^{+}}\| R\|_{_{0}}. \]

The above inequality suggests that we can control the estimation error of \(\) by only \(_{}\), which can be regarded as the projection of \(\) onto the subspace \(\{^{n}|_{j}=0,j\}\). The next lemma bounds the gradient of \(\) with respect to \(\):

**Lemma 4.7**.: For any \(R^{||||}\) and \(^{n}\), we have \(\|_{}(,^{*})\|_{} 1/n\).

Therefore, it suffices to take \(=1/n\). Furthermore, in the proof of Lemma 3.1 of Zhu et al.  (See Section B.1 of Zhu et al. ), the gradient of \(\) with respect to \(R\) can be bounded as following:

**Lemma 4.8**.: There exists a universal constant \(C_{1}>0\), such that we have

\[\|_{R}(R^{*},^{*})\|_{_{0}^{+}} C_{1}|||+(1/)}{n}}\]

with probability at least \(1-\).

Finally, we combine Lemma 4.6 and the upper bounds of gradients in Lemma 4.7 and 4.8 to get

\[\| R\|_{_{0}}^{2}+\|\|_{2}^{2}}(+C_{1}^{2}|||+( 1/)}{n}),\]

which holds with probability at least \(1-\). 

We make the following remarks about Theorem 4.4:

**Remark 4.9**.: When the data perturbation is sufficiently sparse, i.e. \(s||||\), the convergence rate of estimating the reward under the presence of corrupted data is dominated by \(||||/n\). Notably, it is of the same order as that using clean data, which is presented in (4.1). In other words, even there is contamination in data, the learned reward can still be as accurate as its counterpart without outliers. However, if the ground-truth perturbation \(^{*}\) is not very sparse, i.e. \(s||||\), it can hurt the statistical rate of convergence.

**Remark 4.10**.: In our analysis, we estimate rewards for each state-action pair under the tabular bandit setting. However, our results can be extended to infinite-state and infinite-action case with reward function approximation, following Zhu et al. . Specifically, our results work for the scenario where reward functions can be linearly approximated . Moreover, when the reward function is smooth, it can be approximated by neural networks and our analysis for convergence rate of reward recovery under corrupted preference data can apply as well .

## 5 Experiment

In this section, we demonstrate the effectiveness of our proposed robust loss function through its application in robotic control and natural language generation tasks. Due to space limit, we defer some less important results and explanations to Appendix C.

### Robotic control

**Experiment setup.** We evaluate the robustness of \(R^{3}M\) across three robotic control tasks within the PyBullet  environments: _HalfCheetah_, _Ant_, and _Hopper_. To simulate noisy human preference, we consider three noise models of human preferences as follows:

1. **Stochastic noise model**: For a pair of trajectory segments \((z_{1},z_{2})\), we generate a preference label with the probability \(((r^{}(z_{1})-r^{}(z_{2}))/)\) where \(>0\) is the temperature. This model captures typical human behavior, where preferences are more likely to be corrupted when the true preference is unclear. We control the noise rate by tuning \(\) in \(\{1.0,2.0,3.0\}\). As the value of \(\) increases, the probability becomes closer to uniform, causing greater corruption.
2. **Myopic noise model**: For a pair of sequences of state-action pairs \(z_{1}=\{(s_{1,t},a_{1,t})\}_{t=1}^{m}\) and \(z_{2}=\{(s_{2,t},a_{2,t})\}_{t=1}^{m}\), we generate a preference label by

\[z_{1} z_{2}\ \ _{t=1}^{m}^{m-t}r^{}(s_{1,t},a_{1,t})>_{t=1}^{m}^{m-t}r^{}(s_{2,t},a_{2,t})\ \ z_{2} z_{1},\]

where \((0,1]\) is a discount factor. This model represents shortsighted human behavior, where people may place more weight on recent observations. We control the noise rate by tuning \(\)in \(\{0.3,0.5,0.7\}\). In general, as the value of \(\) decreases, the importance of initial observations diminishes, which leads to greater corruption.

3. **Irrational noise model**: For pairs of trajectory segments \(\{(z_{1,i},z_{2,i})\}_{i=1}^{||}\) in a mini-batch \(_{0}\) where \(r^{*}(z_{1,i})>r^{*}(z_{2,i})\) (i.e., \(z_{1,i}\) is preferred over \(z_{2,i}\) by the ground truth reward), we flip the preference labels of the top \(||^{p}/|| 100\)% pairs, ordered by the largest true reward difference \(r^{*}(z_{1,i})-r^{*}(z_{2,i})\). Here, \(p(0,1)\) represents a sublinear rate of label perturbation. This model considers extreme human errors, where people can make mistakes even on clear preference pairs. We control the noise rate by tuning \(p\) in \(\{1/3,1/2,2/3\}\). As the value of \(p\) increases, a larger number of preferences are corrupted.

For reward function, we use two-hidden-layer MLPs, with each hidden layer containing 64 units, which is consistent with the architecture used in both policy and value networks. Similarly with Christiano et al. , we repeat the following three steps for each stage: (i) We sample a set of trajectories using the policy \(\), and a reward function \(\) assigns a reward to each trajectory segment. We then update \(\) using proximal policy optimization (PPO, Schulman et al. ). (ii) We split the trajectory segments into a training set and a testing set. From the training set, we randomly sample pairs of segments, generate preference labels using a noise model, and construct \(_{0}\). For the testing set, we sample pairs of segments, generate preference labels using the ground truth rewards, and construct \(^{}_{0}\). (iii) We train \(\) on \(_{0}\) and use \(^{}_{0}\) to evaluate the preference prediction accuracy of \(\).

Note that we do not perturb the preferences in \(^{}_{0}\) to evaluate how effectively \(R^{3}M\) recovers the ground truth rewards. We set the budget to 2 million timesteps. Every 10,000 timesteps, we evaluate the performance of the policy \(\) over 20 test episodes and calculate the preference prediction accuracy of the reward function at each stage. We conduct training using 10 different random seeds. For hyperparameter tuning in both reward learning and policy optimization, we identify the best policy based on its performance (i.e., the highest return over timesteps) and then select the corresponding reward function. For evaluation metric, we follow Lee et al.  and use normalized returns with respect to the performance of RL using the ground truth reward:

\[=}{}.\]

Further details of implementation and hyperparameter tuning procedures are in Appendix B.1.

**Results.** We summarize the results on three PyBullet tasks as follows:

Figure 1 presents the results for the baseline (cross-entropy loss) and \(R^{3}M\) across three different tasks and noise models (stochastic, myopic, and irrational) with varying noise rates. As can be seen, \(R^{3}M\) consistently outperforms the baseline across all tasks, noise models, and noise rates, except for the case of \(p=1/3\) in the irrational noise model, where only 6.25% of the training data is corrupted. Although there is some overlap in performance variability, as indicated by the error bars, the results demonstrate that \(R^{3}M\) is more robust to noise in human preferences compared to

Figure 1: Normalized returns for the baseline (cross-entropy loss) and \(R^{3}M\) across all noise models and noise rates. Error bars represent the standard deviation across 10 different seeds. Learning curves and percentile plots are in Appendix C.1.

the standard cross-entropy loss. The improvements are particularly notable at higher noise rates. Additional details, including learning curves and percentile plots, are provided in Appendix C.1.

### Natural Language Generation

**Experiment setup.** We evaluate the proposed robust extension of DPO on two natural language generation tasks: _summarization_ and _single-turn dialogue_. In summarization, the policy generates sentences to summarize the main points from posts on Reddit. Following previous work , we conduct instruction tuning on the filtered TL;DR summarization dataset  to get the initial reference model. Then we use the human preferences gathered by Stiennon et al.  for preference optimization. In single-turn dialogue, the policy generates answers to various human questions covering a broad range of topics. We use the Anthropic Helpful and Harmless (HH) dialogue preferences dataset , which contains over 170k dialogues between human and automated-assistant. We conduct instruction tuning on the preferred responses in the dataset to get the reference model, and do the preference optimization using the original dataset. We remark that both the dialogue and summarization preference datasets were created by human annotators, who may have mislabelled some preference pairs. Therefore we apply \(R^{3}M\) directly to these datasets, investigating if popular RLHF datasets can gain from corruption-robust RLHF methods.

For all experiments we utilize Llama-2 7B  as the base model. We fine-tune the entire model in the instruction tuning stage, and apply LoRA fine-tuning in the alignment stage when testing all baselines due to computational efficiency concerns. We set the rank of the LoRA adaptor to \(64\).

**Baselines.** We consider several preference optimization baselines: DPO , IPO , SLiC-HF , KTO , and DPO with dropout . We use the Huggingface TRL implementation for all methods . We also consider a data filtering baseline which first trains an initial DPO model on the full dataset, and then filters the dataset based on the learned reward difference. Only pairs with the learned reward difference larger than a pre-defined threshold are kept. Finally, another DPO model is trained on the filtered dataset. This method has twice the computation cost of \(R^{3}M\).

**Evaluation.** As human evaluation is prohibitively expensive, we use Claude 3 Sonnet , to automatically evaluate responses based on summary quality and response helpfulness/harmlessness for the summarization and dialogue tasks, respectively. Prior work has shown that Claude 3 and GPT-4 can effectively measure a quantitative improvement over the instruction-tuned model . We split a small subset (800 prompts) from each instruction tuning dataset for testing and calculate the win rate against the instruction-tuned reference model as the evaluation metric. The percentage of instances where the response generated by policy A is preferred over policy B is referred to as the win rate of A against B. We also report winning score, which is calculated as \(-\#}{}+1\).

**Results on Non-Perturbed Datasets.** Table 1 presents the performance of all baseline methods on the dialogue and summarization tasks. As indicated, \(R^{3}M\) significantly outperforms all other baselines, with the exception of the Data Filtering method in the summarization task. However, it is important to note that the Data Filtering baseline incurs **double the training cost** compared to our method, which may be prohibitive in scenarios with limited computational resources. For the dialogue task, we find the sparsity rate to be 1.2%, while for summarization we find the sparsity rate to be 10.8%. Paired with the results, our findings suggest the datasets do contain noisy preferences, and that our method is effective in mitigating their negative effects. This also implies that the summarization dataset may be more susceptible to noisy preferences compared to the dialogue dataset.

To better understand our method, we conduct further analysis on the learned perturbation factor \(\) in Figure 2. We extract a subset from the training data and use Claude 3 to assess whether it agrees with the annotated preference labels. We can observe that Claude 3 exhibits a lower agreement rate for samples with a positive perturbation factor. This indicates that the perturbation factor effectively

    &  &  \\ 
**Method** & **Win Rate (\%)** & **Winning Score** & **Win Rate (\%)** & **Winning Score** \\  SLiC-HF & **62.58 (\(\) 1.46)** & **1.507 (\(\) 0.04)** & 59.5 (\(\) 0.45) & 1.488 (\(\) 0.01) \\ IPO & 53.62 (\(\) 2.01) & 1.335 (\(\) 0.02) & 51.91 (\(\) 1.63) & 1.31 (\(\) 0.02) \\ Data Filtering & 53.33 (\(\) 0.72) & 1.367 (\(\) 0.01) & **63.20 (\(\) 0.69)** & **1.5 (\(\) 0.02)** \\ DPO & 57.2 (\(\) 3.93) & 1.356 (\(\) 0.02) & 59.95 (\(\) 1.01) & 1.477 (\(\) 0.01) \\ R3M-DPO & **63.5 (\(\) 3.23)** & **1.506 (\(\) 0.05)** & **62.29 (\(\) 0.19)** & **1.504 (\(\) 0.01)** \\   

Table 1: Win rates and winning scores for dialogue and summarization tasks. Confidence intervals are over three seeds.

identifies outliers within the dataset, thereby enhancing the learning process. Figure 2 provides an example of a corrupted annotation identified in the data.

**Results on Perturbed Datasets.** To explore how our method handles increased noise, we manually perturbed the dataset by flipping a random portion of the training labels. We then compared the winning scores of \(R^{3}M\) with those of the DPO baseline. As depicted in Figure 3, our method consistently outperforms DPO. Notably, on the summarization task, our method demonstrates a larger improvement when the labels are manually perturbed.

**Ablation studies**. In Figure 4, we examine the sensitivity of the hyperparameter \(\). For robotic tasks, we use the myopic noise model with \(=0.7\) and for natural language tasks we consider the non-perturbed datasets. We can see that values of \(\) near the selected (best) ones also outperform the baseline.

## 6 Discussions

**Smooth Reward Modeling.** In real-world reinforcement learning applications, ground truth reward models are often assumed to be smooth [35; 8], enabling effective learning by neural networks. However, this assumption may not always hold, as certain applications can exhibit non-smoothness in specific regions of the state-action space. Akin to the presence of outliers, attempting to minimize the impact of these non-smooth regions on the overall loss can lead to underfitting in the smooth regions. Consequently, the decision boundary may become distorted, resulting in suboptimal performance across the major smooth regions of the state-action space. We remark that this fundamental difficulty in learning non-smooth reward models presents a challenge. Our proposed \(R^{3}M\) method can mitigate this issue by modeling data from the non-smooth regions as outliers. Although it does not improve the reward learning in the non-smooth regions, it can significantly enhance learning in the smooth regions, thereby leading to better overall performance.

**Assumption on Deterministic Perturbations**. The theoretical analysis underpinning our proposed \(R^{3}M\) method assumes deterministic perturbations to the preference data, a setting more challenging than specific distributional assumptions on the perturbations. Our extensive experiments further corroborate this claim, demonstrating the robustness of \(R^{3}M\) against a wide range of perturbation types (some may be not even sparse) introduced to the preference data. This empirical evidence

Figure 4: Sensitivity of the hyperparameter \(\) across Dialogue and Summarization tasks.

Figure 3: Comparison of winning scores between \(R^{3}M\) and the DPO baseline across different perturbation percentages on two tasks.

Figure 2: (a) Comparison of the Claude 3 agreement on the annotated labels between sample pairs with zero and positive learned perturbation factors. (b) An example of corrupted annotation in the HH dataset.

substantiates the efficacy of our approach in handling diverse forms of corruption, underscoring its practical utility in real-world reinforcement learning applications where the nature of perturbations may be unknown or difficult to characterize.