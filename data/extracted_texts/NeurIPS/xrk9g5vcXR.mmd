# QuIP: 2-Bit Quantization of

Large Language Models With Guarantees

 Jerry Chee

Cornell University

jerrychee@cs.cornell.edu

&Yaohui Cai

Cornell University

yc2632@cornell.edu

&Volodymyr Kuleshov

Cornell University

kuleshov@cornell.edu

&Christopher De Sa

Cornell University

cdesa@cs.cornell.edu

###### Abstract

This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from _incoherent_ weight and Hessian matrices, i.e., from the weights being even in magnitude and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/Cornell-RelaxML/QuIP.

## 1 Introduction

Large language models (LLMs) have enabled advances in text generation, few-shot learning, reasoning, protein sequence modeling, and other tasks . The massive size of these models--often reaching into hundreds of billions of parameters--requires sophisticated deployment methods and motivates research into efficient inference algorithms.

This work studies the post-training quantization of LLM parameters as a way to improve their runtime efficiency . Our key insight is that quantization can be most effective when weight and proxy Hessian matrices are _incoherent_--that the weights themselves are even in magnitude, and the directions in which it is important to have good rounding accuracy are not too large in any one coordinate. Intuitively, incoherence can be thought of as a principled form of outlier reduction, which makes it easier to adaptively round the weights to a finite set of compressed values. We use this intuition to develop theoretically sound two-bit quantization algorithms that scale to LLM-sized models.

Specifically, we introduce quantization with incoherence processing (QuIP), a new method motivated by the above insight. QuIP consists of two steps: (1) an adaptive rounding  procedure, which minimizes a quadratic proxy objective \(()=((-W)H(-W)^{T})\) of the error between the original weights \(W\) and the quantized weights \(\) using an estimate of the Hessian \(H\); (2) efficient pre- and post- processing that ensures that the weight and Hessian matrices are incoherent by multiplyingthem by a Kronecker product of random orthogonal matrices. We denote "incoherence processing" as both the pre- and post- processing steps of our procedure. Incoherence processing can be viewed as a form of outlier suppression across the weights and the activation space.

We complement our method with a theoretical analysis--the first for a quantization algorithm that scales to LLM-sized models--which analyzes the role of incoherence and shows that our quantization procedure is optimal within a general class of rounding methods. Interestingly, we find that QuIP without incoherence processing yields a more efficient implementation of an earlier algorithm, OPTQ ; our paper thus also provides the first theoretical analysis for that method.

Empirically, we find that incoherence processing greatly improves the quantization of large models, especially at higher compression rates, and yields the first LLM quantization method that produces viable results using only two bits per weight. For large LLM sizes (>2B parameters), we observe small gaps between 2-bit and 4-bit compression that further decrease with model size, hinting at the feasibility of accurate 2-bit inference in LLMs.

Contributions.In summary, this paper makes the following contributions: (1) we propose QuIP, a quantization method based on the insight that model parameters should ideally be incoherent; (2) we provide a theoretical analysis for a broad class of adaptive rounding methods that encompass QuIP and OPTQ; (3) we demonstrate that QuIP makes two-bit LLM compression viable for the first time.

## 2 Related Work

Adaptive rounding.Nagel et al.  are the first to motivate the "adaptive rounding" proxy objective (Eq. (1)) in a principled way. There are many quantization methods which quantize by optimizing this proxy objective [5; 6; 9; 12; 14; 20; 32]. Many require further retraining which can be expensive, and are not evaluated on the current largest open LLMs (OPT , BLOOM ). Lybrand and Saab  propose a greedy per-neuron quantization procedure that is similar to ours, except they do not consider arbitrary linear functions of the error correction. Their work bounds the proxy objective, albeit on the first layer only.

Post training quantization in large models.There is a growing body of work on PTQ in LLMs such as OPT and BLOOM. The size of these models make it difficult to apply previously developed methods. The majority of these methods make quantization easier by somehow reducing the range of weights or activations, but still use nearest rounding. SmoothQuant  rescales between activations and weights to remove outliers from the activations and make quantization overall easier. ZeroQuant  proposes a per-layer knowledge distillation method. LLM.int8(4) decompose matrix multiplications into a majority of 8 bit and a minority of 16 bit operations. LUT-GEMM  designs kernels to accelerate quantized matrix multiplications. RPTQ  reorders activations and quantizes them in groups, reducing the impact of range differences between channels.

OPTQ (Formerly known as GPTQ). OPTQ  is based on OBQ , and proposes a novel rounding method that can work on the largest OPT and BLOOM models. The method works iteratively over the weight columns in a fixed order: (1) quantize with nearest rounding and compute the error, (2) update the remaining weights with a scaled error, and (3) repeat.

Other quantization methods.There are other quantization procedures which do not round based on the proxy objective of , or are not designed for the largest language models [10; 11; 13; 19; 28; 29].

## 3 Quantization With Incoherence Processing: Adaptive Rounding Step

This section introduces quantization with incoherence processing (QuIP), a new method consisting of: (1) an adaptive rounding step; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence. We define and analyze step (1) in this section; the next section focuses on step (2).

Following existing state-of-the-art post-training quantization methods, we round weights per-layer by minimizing the "adaptive rounding" proxy objective, as in Nagel et al. ,

\[()=_{x}[\|(-W)x\|^{2}]= ((-W)H(-W)^{T}).\] (1)

Here, \(W^{m n}\) is the original weight matrix for a given linear layer, \(^{m n}\) are the quantized weights, \(x^{n}\) is an input vector drawn uniformly at random from a calibration set, and \(H\) is the second moment matrix of these vectors, interpreted as a proxy Hessian. Crucially, this formulation lets the quantization be run in parallel across neurons, which is tractable for large language models . For simplicity, we will focus in this section on rounding to the integers; subsequent sections will extend the analysis to finite grids.

### LDLQ: An Optimal Adaptive Rounding Method

Our strategy is to define a family of adaptive rounding methods for optimizing objective (1) and then define LDLQ, the optimal method within that class. Our defined methods iteratively perform the following update for \(k=1,2,...,n\):

\[_{k}=(W_{k}+(W_{1:(k-1)}-_{1:(k-1)})a_{k}),\]

where \(W_{k}\) denotes the \(k\)-th column, \(W_{1:(k-1)}\) denotes the first \(k-1\) columns, the subroutine \(\) denotes either nearest rounding or standard unbiased rounding to the integers (which rounds up or down such that \([(z)]=z\)), and \(a_{k}^{k-1}\) is some sequence of vectors. This scheme rounds columns one at a time; at each step, it adds a "correction" term that is a linear function of the residual from the rounding we have done so far. The final \(\) satisfies the following matrix equation:

\[=(W+(W-)U),\] (2)

where \(U\) is a strictly upper-triangular matrix whose columns are the vectors \(a_{k}\) and \(\) acts elementwise. Because \(U\) is upper-triangular, \(_{k}\) only depends on \(_{1:(k-1)}\).

If we let \(=(W+(W-)U)-(W+(W-)U)\) denote the quantization error of \(\), we find that \(-W=(U+I)^{-1}\) and we can rewrite objective (1) as

\[((-W)H(-W)^{T})=((U+I)^{- 1}H(U+I)^{-T}^{T}).\] (3)

The LDLQ MethodHow should we specify \(U\), the linear feedback from the quantization error of preceding columns in (2)? Equation 3 provides an answer. If we choose \(U\) such that the LDL decomposition of \(H\) is

\[H=(+I)D(+I)^{T},\] (4)

where \(D\) is a (non-negative) diagonal matrix and \(\) is upper unit triangular, then the terms \((U+I)\) in Eq. (3) cancel. We denote as LDLQ the rounding procedure in Eq. (2) with \(U\) as the LDL assignment from Eq. (4). We will now see that the LDL assignment of \(U\) is in fact optimal.

### Deriving the Optimality of the LDLQ Adaptive Rounding Procedure

In order to reason about optimality, we consider weights which are worst and average-case for the proxy loss. Let \(\) denote a rounding method, and let \((W,H)\) be the resulting quantized weights. Define the _worst-case_ (\(_{}\)) and _average_ (\(_{}\)) proxy losses with respect to the input weights as

\[_{}(,H) =_{W^{m n}}[(((W,H)-W)H((W,H)-W)^{T})]\] (5) \[_{}(,H) =_{W^{m n}}[ (((W,H)-W)H((W,H)-W)^{T}) ].\] (6)

**Theorem 1**.: LDLQ _is worst and average-case optimal amongst rounding methods which specify the linear feedback \(U\) as a function of \(H\) (not of \(W\)), and when rounding to the integers. That is, for all rounding methods \(\) in the class described by Eq. (2), for all positive semi-definite \(H\), and for \(\) as either nearest or stochastic rounding,_

\[(D)=_{}(,H) _{}(,H)\ \ (D)=_{}(,H) _{}(,H),\]

_where \(D\) is the matrix from the LDL decomposition of \(H\), and \(c=12\) for nearest, \(c=6\) for stochastic._

**Remarks.** The number of rows being quantized is \(m\), and each quantization method operates across the \(n\) entries of each row. For all rounding methods described by Eq. (2), and for all positive semi-definite \(H\), \(\) as nearest rounding achieves the same worst-case proxy loss as stochastic rounding, but achieves better average proxy loss.

Moving beyond a generic algorithm \(\) within our framework, we consider the common baselines of nearest and stochastic rounding. These methods are represented within our framework by choosing the appropriate \(\) subroutine, and setting all entries of the linear feedback to zero.

For these baseline methods, their optimality gap to LDLQ is governed by \((D)\) vs. \((H)\). For any non-diagonal \( 0\), LDLQ achieves strictly lower worst and average-case proxy loss because \((D)<()\). Let \(=\{,\}\). Then, \(_{}(,)<_{ }(,)\) and \(_{}(,)<_{ }(,)\). Across OPT models 125m to 2.7b, \((D)/(H) 0.65\)--empirically verifying that the gap is not insignificant. See Supplement C for full details.

### Incoherence: Optimality with a Spectral Bound

Theorem 1 gives exact expressions for the proxy loss, albeit with \((D)\), which can be difficult to reason about. In Figure 2, we empirically observe that \(H\) is approximately low-rank: we visualize the spectrum of several randomly chosen \(H\) from OPT-2.7b, and observe that the spectrum decays rapidly. In fact, across all layers of OPT-125m to 2.7b models, a vast majority of \(H\) matrices have fewer than a quarter of eigenvalues \(>1\%\) of the max eigenvalue; see Supplement C for full details. Given this observation about the low rank of \(H\), can we bound the behavior of LDLQ, and thus \((D)\), using the spectrum of \(H\)?

We do this building on a variant of the incoherence assumption that is specialized to our case .

**Definition 1**.: _We say a symmetric Hessian matrix \(H^{n n}\) is \(\)-incoherent if it has an eigender-composition \(H=Q Q^{T}\) such that for all \(i\) and \(j\), \(|Q_{ij}|=|e_{i}^{T}Qe_{j}|/\). By extension, we say a weight matrix \(W^{m n}\) is \(\)-incoherent if all \(i\) and \(j\), \(|W_{ij}|=|e_{i}^{T}We_{j}|\|W\|_{F}/\)._

Note that "most" \(n n\) matrices are incoherent with \(=()=}(1)\) because a random orthogonal matrix has entries with squared-magnitudes that concentrate around their mean of \(1/n\). Incoherence in \(W\) can be viewed as a form of outlier reduction: a small bound on the magnitude of its entries means that we do not need to scale it as much to make it fit in the finite range of representable low-precision numbers. Figures 2 and 3 plot the max absolute weight and hessian eigenvector entries before and after our incoherence processing, on all layers in OPT-2.7b. A line with slope=1 is drawn for reference. We see that \(W\) and \(H\) are more incoherent after our incoherence processing is applied. Making \(H\) incoherent is less intuitive, but its utility is motivated by the following lemma.

**Lemma 2**.: _Let \(H^{n n}\) be a \(\)-incoherent positive semi-definite symmetric matrix and let \(H=(+I)D(+I)^{T}\) be its LDL Cholesky decomposition, where \(\) is a strictly upper triangular matrix and \(D\) is a (non-negative) diagonal matrix. Then,_

\[(D)}{n}(H ^{1/2})^{2}.\]

To the best of our knowledge, this is a novel result using incoherence to obtain a bound on \((D)\) that depends only on the spectrum of \(H\). To help interpret this result, we derive explicit proxy losses for plain nearest and stochastic rounding, which we will then compare to what LDLQ gets via Lemma 2.

```
0:\(b\), \(H^{n n}\) SPD, original \(W^{m n}\), \(_{+}\), \(\)
1:seeded sample random two-factor orthogonal matrices \(U^{m m}\) and \(V^{n n}\)
2:\(H=H+*((H))I\)\(\) from OPTQ
3:\((H)/(W^{T}W)}\)\(\) applies element-wise
4:\(W W\); \(H^{-1}H^{-1}\)\(\) diagonal rescaling
5:\(W UWV^{T}\); \(H VHV^{T}\)\(\) incoherence
6:\(s\|W\|_{F}/;\ W(W+1)\)\(\) reduced quantization range due to incoherency
7:\(W(W*(2^{b}-1),0,2^{b}-1)\)\(\) rescale \(W\) to lie within \([0,2^{b}-1]\)
8:return\(W,H,s,\) ```

**Algorithm 2** QuIP - Incoherence Post-Processing

**Lemma 3**.: _Let \(H\) be symmetric positive definite. In the worst case stochastic rounding achieves \(_{}(,H)=(m/4)(H)\). In the average case nearest and stochastic rounding achieve \(_{}(\{,\},H)=(m/c) (H)\), where \(c=12\) for nearest, and \(c=6\) for stochastic._

To interpret this result, consider \(H\) rank-\(k\) with \(^{2}k<n\). By Cauchy-Schwarz, \((H^{1/2})^{2} k(H)\). Combining Lemma 2 with the LDLQ proxy losses of Theorem 1 and comparing with Lemma 3,

\[_{}(,H) }{4n}(H^{1/2})^{2} k}{4n}(H) (H)=_{}(,H)\] \[_{}(,H) }{cn}(H^{1/2})^{2} k}{cn}(H) (H)=_{}(,H),\]

where \(\{,\}\), and \(c\) is as given in Theorem 1. This shows that for sufficiently low-rank \(H\), LDLQ is asymptotically better than plain nearest and stochastic rounding by a factor of \(^{2}k/n\).

Without incoherence: no improvement with a spectral bound.By assuming incoherence, we were able to show LDLQ gets an asymptotically better bound in terms of just the spectrum of \(H\). We might ask: _was the incoherence assumption necessary to get this result?_ The following theorem answers this question in the affirmative by showing that without incoherence, the best spectral bound for LDLQ cannot differentiate it from the nearest and stochastic rounding baselines.

**Theorem 4**.: _Consider all \(\) with the same spectrum as \(H\). For any positive semi-definite \(H\), the following holds. On the worst-case loss \(\) achieves the same error as stochastic rounding,_

\[_{\,s.t.\ ()=(H)}_{ }(,)=_{}(,H)=(H).\]

_On the average-case loss \(\) achieves the same error as the corresponding rounding routine. Let \(=\{,\}\) and \(c=12\) for nearest, \(c=6\) for stochastic._

\[_{\,s.t.\ ()=(H)}_{ }(^{*},)=_{}( ,H)=(H).\]

Note that the worst case for comparing LDLQ against these baselines occurs when \(H\) is diagonal, see Theorem 1 and Lemma 3. Assuming incoherence as we do is a natural way to exclude such cases.

## 4 Quantization With Incoherence Processing: Incoherence Processing Step

Next, we leverage the above incoherence analysis to introduce _incoherence processing_, the second step of the QuIP algorithm. Our strategy will be to pre-process weight and Hessian matrices to ensure the favorable incoherence properties outlined above. One straightforward way to make a symmetric matrix incoherent is to conjugate it by a uniform random orthogonal matrix: this will result in each of its eigenvectors being a random unit vector, whose entries will concentrate around magnitude \(n^{-1/2}\).

Specifically, let \(U^{m m}\) and \(V^{n n}\) be two random orthogonal matrices. (Let's temporarily ignore how these matrices are generated, or how we would efficiently perform inference.) We ensure the weight and Hessian are incoherent with high probability through random orthogonal multiplications \( VHV^{T}\) and \( UWV^{T}\). Importantly, this transformation preserves the proxy quadratic form since \((H^{T})=((UWV^{T})(VHV^{T} )(VW^{T}U^{T}))=(WHW^{T})\).

### Incoherence via Efficient Orthogonal Multiplication

If all we wanted to do was to store or transmit the weights of the quantized neural network, the above procedure would introduce no overhead, since we can generate a random orthogonal matrix from a seed--making it essentially free to store. However, for running _inference_ on a DNN, we need to multiply by the weight matrix \(W\), and here the need to manifest and multiply by \(n n\) random orthogonal matrices \(U,V\) would be prohibitive.

To handle this, we propose to instead use a distribution over random orthogonal matrices for which multiplication is fast. Let \(n=pq\) be a factorization of \(n\) (where \(p q\)), and set \(U=U_{L} U_{R}\) where \(U_{L}\) is sampled uniformly from the \(p p\) orthogonal matrices and \(U_{R}\) is sampled uniformly from the \(q q\) orthogonal matrices. Multiplication of a vector \(x^{n}\) by the matrix \(U\) can be accomplished by reshaping to a \(p q\) matrix, multiplying on the left by \(U_{L}\) and the right by \(U_{R}^{T}\), and then reshaping back: this takes \(O(n(p+q))=o(n^{2})\) operations. Using more than two factors in this way is also possible, but using two suffices to make this preprocessing asymptotically non-dominant.

**Lemma 5**.: _Let \(H\) be a positive semi-definite matrix on \(^{n n}\) and \(W\) a matrix on \(^{m n}\), and suppose that \(m=p_{1} p_{2} p_{k}\) and \(n=q_{1} q_{2} q_{k}\). Let \(U_{1},U_{2},,U_{k},V_{1},V_{2},,V_{k}\) be independent random orthogonal matrices on \(^{p_{i} p_{i}}\) and \(^{q_{i} q_{i}}\) respectively. Set \(U\) as the Kronecker product \(U=U_{1} U_{2} U_{k}\) and \(V\) as \(V=V_{1} V_{2} V_{k}\) Then \(VHV^{T}\) is \(_{H}\)-incoherent with probability at least \(1-\), and \(UWV^{T}\) is \(_{W}\)-incoherent with probability at least \(1-\), where_

\[_{H}=A^{k/2}(}{})^{k/2}=}(1)\ \ \ \ _{W}=A^{k}()^{k}=} (1)\]

_for some global constants \(A\) and \(C\) independent of \(n\) and \(k\)._

**Remarks.** This lemma means that multiplying by a random matrix in this family suffices to make a matrix incoherent with parameter \(\) only poly-logarithmic in the matrix size. In our experiments we use \(k=2\) factors to construct the orthogonal matrices \(U,V\).

### Additional Heuristics

We outline QuIP pre-processing and post-processing in Algorithms 1 and 2, respectively. In line 5 of Algorithm 1, we apply the aforementioned fast orthogonal multiplication procedure to ensure \(W\) and \(H\) are incoherent. We also randomly permute entries at the fast matrix multiplication step to prevent any correlation between attention heads from worsening performance. We introduce a number of additional heuristic improvements that further improve performance.

**Incoherence-Based Heuristics.** Line 4 diagonally rescales \(W\) and \(H\) to minimize \(()(H)\|W\|_{F}^{2}\), effectively trading off the spectrum of these matrices to find a minimum. Motivated by the incoherence of \(W\), Line 6 computes the quantization range depending on the spectrum \(\|W\|_{F}\), instead of the typical \(_{i,j}|W_{ij}|\). Our full QuIP procedure is described in Algorithm 3, which contains calls to the pre- and post-processing sub-steps in Algorithms 1 and 2.

**Greedy local search.** Our basic procedure yields a good initial guess with error guarantees. We can further lower the proxy loss by running coordinate descent after LDLQ (but before post-processing), updating the weights in the same order as in the initial pass. See Supplement B for full details.

```
0:\(b\), \(H^{n n}\) SPD, \(W^{m n}\), \(\{,\}\), \(_{+}\), \(\)
1:\(,H,s,1(b,H,W,,)\)\(\) QuIP Incoherence Pre-Procesing
2:\(H=(+I)D(+I)^{-1}\)\(\) LDL decomposition
3:for\(k\{1,,n\}\)do\(_{k}((W_{k}+(W-) _{k}),0,2^{b}-1)\)\(\) LDLQ
4:return\(2(b,H,,s,)\)\(\) QuIP Incoherence Post-Processing ```

**Algorithm 3** QuIP: Quantization with Incoherence Processing

## 5 Extensions and Further Analyses

### OPTQ is a Special Case of LDLQ

We prove a novel theoretical insight: QuIP without incoherence processing (i.e., LDLQ) is equivalent to a more efficient version of the OPTQ algorithm. That is, OPTQ falls under our class of adaptive rounding procedures with linear feedback, and is within-class optimal.

**Theorem 6**.: _OTPQ  falls within the class of adaptive rounding procedures with linear feedback as described by Eq. (2), and is equivalent to LDLQ in Section 3._

**Remarks.** To the best of our knowledge, this equivalence yields the first theoretical analysis of OPTQ. Even though the two methods are equivalent, LDLQ is more efficient. OPTQ's implementation requires a matrix inversion of \(H\), and two Cholesky decompositions. Our implementation of LDLQ performs no matrix inversion, and only one Cholesky decomposition.

**Empirical Verification.** The quantized outputs of the OPTQ implementation  are shown to be exactly identical to the outputs of our LDLQ implementation. Synthetic random data was used, with \(W^{1000 1000}\). Full details can be found in Supplement C.

### A Bound for Rounding to a Finite Grid

In Section 3, we saw that LDLQ (equivalently, OPTQ) is optimal for minimizing the adaptive rounding objective. However, this analysis assumed rounding to the integers. In practice, we do not want to round \(W\) just to the integers, but instead to scale it, shift it, and round it a finite subset corresponding to a \(b\)-bit integer. To do this, the "real" LDLQ algorithm uses a clamp operation to restrict the range of quantized values. Is LDLQ still optimal when this small change is made? It turns out that the answer is _no_, as the following concrete example illustrates.

**Finite Grid Counterexample.** Figure 4 illustrates the behavior of LDLQ and other rounding methods--when restricted via clamping to a finite 4-bit grid \(\)--on a particular example where \(H\) is a (cleverly chosen) small perturbation of \((I_{n}+_{n n}-e_{n}e_{n}^{T})/n\), and \(W\) has \(m=16\) and is a small perturbation of \(_{m n}/2\). Details of the setup appear in Supplement C. The figure shows that clamped LDLQ with nearest rounding is asymptotically worse, and the clamping to the finite grid is what causes it to be worse in this case.

Note that in our experiments in practice, OPTQ has been shown to soundly beat nearest rounding. This clamping issue does not seem to arise in practice; however, since it is _possible_ we do need to take it into account to prove useful end-to-end bounds.

**A Procedure With a Bound.** In order to address the above issues in theory, here we describe a method that acts to restrict the value of \(|_{ij}-W_{ij}|\), so that the rounded weights will remain inside the grid if \(W\) is sufficiently far inside. We do this via the optimization problem with hyperparameter \(c\)

minimize: \[(HR^{T}R)\] over: \[R\] unit upper triangular (7) subject to: \[e_{i}^{T}R^{T}Re_{i} 1+c,\  i\{1,,n\}.\]

Figure 4: LDLQ underperforms.

Our "fixed" algorithm solves this convex problem (e.g. with ADMM), then runs QuIP using stochastic rounding and \(U=R^{-1}-I\) in place of the LDL decomposition. Observe that for sufficiently large \(c\), this is exactly equivalent to base QuIP, since the solution of that optimization problem is given by the LDL decomposition when the constraint is dropped. Doing this (the full algorithm is given in the supplemental) yields the following theorem.

**Theorem 7**.: _Suppose that we run Algorithm 5 (Supplement) to quantize a matrix \(W^{m n}\) by solving the objective (7). Then there exists an assignment of the algorithm's hyperparameters \(c\) and \(\) such that with probability at least \(1-\), all the quantized weights will be in range (no overflow or need for clipping) and_

\[((-W)H(-W)^{T})=} (4^{b}}(H^{1/2})^{2}\|W\|_{F}^{ 2}).\]

In practice, because clamping rarely causes issues, and because of the significant additional compute needed to solve this program, we always just use QuIP as described in the previous sections, which is equivalent to setting \(c\) large and using nearest rounding.

## 6 Experiments

**Overview.** We quantize the OPT  family of models (up to 66B parameters) and Llama 2 70B  using various quantization and processing methods. QuIP is superior to OPTQ and other baselines across all model sizes and evaluation tasks. Most interestingly, incoherence processing yields excellent performance using as little as two bits per weight when paired with any of the quantization methods we consider (including nearest rounding). Two-bit quantization with QuIP is viable at even moderate model sizes (1B parameters), a regime where other two-bit quantization methods fail. At the largest model sizes, the difference between 2-bit and 16-bit weight performance becomes small. We compare the throughput of QuIP with OPTQ's efficient implementation on language generation and show that it is not much slower. Additional results on the effectiveness of the proxy loss, unbiased rounding, and Algorithm 5 are presented in the Supplement C.

**Setup.** The experimental infrastructure is built on top of OPTQ's  repository which is implemented in PyTorch . We quantize the HuggingFace implementations of the OPT and Llama 2 model families. All models are quantized on a single GPU, with up to 48GB of memory. Our calibration set is the same as OPTQ; 128 random 2048 token segments from the C4 dataset  consisting of generic text data from crawled websites. Therefore, no task-specific data is viewed when quantizing. Following OPTQ, quantization is performed one Transformer block at a time: loaded into GPU memory, the Hessian computed, and then the weights quantized. The current block's inputs are then passed through the quantized block to produce inputs for the following block. The Hessian is computed from the quantized Transformer up to that point rather than from the full precision model; like OPTQ, we find this improves quantization. Further details on the setup can be found in Supplement C, including a description of the computational resources used to perform the experiments.

**Methods.** We evaluate compositions of several quantization and pre/post processing methods. For quantization methods, we evaluate nearest rounding, LDLQ (or OPTQ), and two variations. LDLQ-RG re-orders the weights based on \((H)\) to modify the quantization order and adds further greedy updates to the proxy. "Greedy" performs the greedy updates only. We evaluate the baseline preprocessing from OPTQ which adds \(H H+*((H))I\) for numerical stability. We also evaluate our incoherence processing in Algorithms 1 and 2, denoted as "IncP". With this notation QuIP = LDLQ + IncP, and QuIP-RG = LDLQ-RG + IncP.

**Datasets.** We evaluate on the following language generation tasks: WikiText2 , Penn Treebank (PTB) , and C4. We also evaluate on zero-shot tasks, including LAMBADA (LAMB) , ARC Easy (ArcE) , PiQA , and StoryCloze (SC) . See Supplement C for the full set of results.

**Main Results.** QuIP is the first PTQ procedure to achieve good quantization at two bits per weight, across a variety of LLM sizes and evaluation tasks. In Figure 5 we compare QuIP and OPTQ when quantizing to 2 and 3 bits per weight (4-bit quantization works equally well for both methods); we evaluate OPT models (up to 66B) on PTB, C4, ARC Easy, and LAMBADA. QuIP is superior to OPTQ across the model sizes and evaluation tasks. At three bits, QuIP matches the full precision model reasonably well. At two bits and for larger LLMs (>2B parameters), QuIP begins to approach the performance of the full precision model. As model size increases, so does the quality of QuIP's

  &  &  \\  WBits & Wiki\(\) & C4\(\) & ArcE\(\) & PiQA\(\) & SC\(\) & Wiki\(\) & C4\(\) & ArcE\(\) & PiQA\(\) & SC\(\) \\ 
16 & 3.319 & 5.709 & 59.72 & 80.90 & 79.95 & 3.319 & 5.709 & 59.72 & 80.90 & 79.95 \\ 
4 & 3.596 & 5.905 & 58.96 & 80.52 & 79.12 & 3.531 & 5.869 & 59.81 & 80.47 & 79.63 \\
3 & 4.907 & 7.099 & 54.38 & 78.56 & 77.72 & 3.853 & 6.135 & 59.81 & 80.25 & 79.31 \\
2 & 123.908 & 70.541 & 25.34 & 50.54 & 51.75 & **6.326** & **8.937** & **54.38** & **75.08** & **75.37** \\  

Table 1: Quantizing Llama 2 70B with QuIP and OPTQ, and evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits.

  &  &  \\  WBits & Wiki\(\) & PTB\(\) & C4\(\) & ArcE\(\) & LAMB\(\) & Wiki\(\) & PTB\(\) & C4\(\) & ArcE\(\) & LAMB\(\) \\ 
16 & 9.56 & 14.04 & 11.45 & 65.40 & 72.40 & 9.56 & 14.04 & 11.45 & 65.40 & 72.40 \\   &  \\
4 & 9.59 & 14.22 & 11.56 & 64.77 & 72.39 & 9.60 & 14.18 & 11.50 & 65.32 & 73.20 \\
3 & 10.32 & 15.36 & 12.23 & 60.19 & 68.89 & 9.79 & 14.37 & 11.66 & 65.28 & 72.68 \\
2 & 71.70 & 88.19 & 29.59 & 42.47 & 25.77 & **11.48** & 17.40 & 13.55 & 57.87 & **65.24** \\   &  \\
4 & 9.64 & 14.20 & 11.56 & 63.76 & 71.94 & 9.66 & 14.11 & 11.51 & 64.86 & 71.86 \\
3 & 10.31 & 15.15 & 12.15 & 63.43 & 69.78 & 9.75 & 14.44 & 11.68 & 63.51 & 71.53 \\
2 & 49.40 & 73.45 & 29.12 & 41.20 & 26.35 & 11.68 & **16.94** & 13.44 & **59.51** & 62.31 \\   &  &  \\
4 & 9.69 & 14.33 & 11.59 & 63.09 & 72.37 & 9.72 & 14.23 & 11.52 & 65.99 & 71.71 \\
3 & 13.63 & 23.05 & 16.30 & 50.51 & 56.76 & 9.92 & 14.45 & 11.71 & 63.80 & 71.38 \\
2 & 4816.6 & 3473.81 & 3183.2 & 26.30 & 0.00 & 11.59 & 17.39 & **13.30** & 58.80 & 64.47 \\   &  \\
4 & 10.77 & 15.41 & 13.52 & 61.28 & 70.42 & 9.77 & 14.16 & 11.53 & 64.06 & 71.41 \\
3 & 1564.9 & 1526.2 & 1808.2 & 34.47 & 1.73 & 9.89 & 14.49 & 11.74 & 64.06 & 71.41 \\
2 & 41547.8 & 34348.6 & 24815.7 & 25.80 & 0.00 & 12.04 & 18.12 & 14.11 & 56.36 & 60.64 \\  

Table 2: Quantizing OPT-30b with various quantization and processing methods, and evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.

Figure 5: Quantizing OPT models up to 66B parameters. Our method QuIP is the first PTQ procedure to achieve good quantization at 2 bits per weight, across a variety of model sizes and evaluation tasks.

2-bit quantization. We provide plots on the remaining datasets in Supplement C. Note that the dip in OPTQ on OPT-66B is documented in their paper.

Table 1 shows the results of quantizing Llama 2 70B using QuIP and OPTQ. Again, QuIP achieves good quantization at two bits while OPTQ does not.

**Incoherence Processing Ablation.** Table 2 shows all combinations of quantization and processing methods evaluated on OPT-30B. At lower weight bits, QuIP's incoherence processing dramatically improves the performance of all quantization methods, across all evaluation tasks. Remarkably, all quantization methods--even nearest--are viable at two bits with our incoherence processing. Our modifications in QuIP-RG sometimes give an improvement over QuIP, but further study is required to evaluate these modifications. Figures for OPT-125M to 13B are in Supplement C.

**Throughput Comparison.** We evaluate the additional overhead of our incoherence processing during model inference by modifying OPTQ's efficient forward pass. OPTQ's implementation contains a quantized-matrix full-precision-vector product kernel and was shown to offer speedups over a FP16 baseline. Our incoherence processing additions are performed in PyTorch. Table 4 shows that our QuIP implementation is about 1.5\(\) slower than OPTQ.

**Further Ablation.** QuIP's incoherence processing contains several sub-steps. Table 3 shows their relative contributions; all are necessary for the full improvement. Table 5 shows that the random permutation step within the fast orthogonal multiplication also significantly reduces perplexity.

## 7 Conclusion

This paper introduced quantization with incoherence processing (QuIP), an algorithm consisting of (1) an optimal adaptive rounding procedure which minimizes a quadratic proxy of the weight error, and (2) efficient pre- and post-processing to ensure the incoherence of the weight and Hessian matrices by multiplying them by a Kronecker product of random orthogonal matrices. We showed that QuIP quantization is optimal in a general class of adaptive rounding methods with linear feedback; this theoretical analysis is the first for any quantization algorithm that scales to LLM-sized models.

Empirically, QuIP achieves the first viable two-bit quantization results for LLMs, especially at large model sizes, hinting at the feasibility of accurate 2-bit inference in LLMs.