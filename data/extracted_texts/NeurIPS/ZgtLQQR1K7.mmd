# VMamba: Visual State Space Model

Yue Liu 1 Yunjie Tian1 Yuzhong Zhao1 Hongtian Yu1 Lingxi Xie2 Yaowei Wang3 Qixiang Ye1 Jianbin Jiao1 Yunfan Liu1\({}^{}\)

1 UCAS 2 Huawei Inc. 3 Pengcheng Lab.

{liuyue171,tianyunjie19,zhaoyuzhong20,yuhongtian17}@mails.ucas.ac.cn

198808xc@gmail.com, wangyw@pcl.ac.cn, {qxye,jiaojb,liuyunfan}@ucas.ac.cn

###### Abstract

Designing computationally efficient network architectures remains an ongoing necessity in computer vision. In this paper, we adapt Mamba, a state-space language model, into VMamba, a vision backbone with linear time complexity. At the core of VMamba is a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D bridges the gap between the ordered nature of 1D selective scan and the non-sequential structure of 2D vision data, which facilitates the collection of contextual information from various sources and perspectives. Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements. Extensive experiments demonstrate VMamba's promising performance across diverse visual perception tasks, highlighting its superior input scaling efficiency compared to existing benchmark models. Source code is available at https://github.com/MzeroMiko/VMamba

## 1 Introduction

Visual representation learning remains as a fundamental research area in computer vision that has witnessed remarkable progress in the era of deep learning. To represent complex patterns in vision data, two primary categories of backbone networks, _i.e._, Convolutional Neural Networks (CNNs)  and Vision Transformers (ViTs) , have been proposed and extensively utilized in a variety of visual tasks. Compared to CNNs, ViTs generally demonstrate superior learning capabilities on large-scale data due to their integration of the self-attention mechanism . However, the quadratic complexity of self-attention w.r.t. the number of tokens imposes substantial computational overhead in downstream tasks involving large spatial resolutions.

To address this challenge, significant efforts have been made to improve the efficiency of attention computation . However, existing approaches either restrict the size of the effective receptive field  or suffer from notable performance degradation across various tasks . This motivates us to develop a novel architecture for vision data, while maintaining the inherent advantages of the vanilla self-attention mechanism, _i.e._, global receptive fields and dynamic weighting parameters .

Recently, Mamba , a innovative State Space Model (SSM) , in the field of natural language processing (NLP), has emerged as a promising approach for long-sequence modeling with linear complexity. Drawing inspiration from this advancement, we introduce VMamba, a vision backbone that integrates SSM-based blocks to enable efficient visual representation learning. However, the core algorithm of Mamba, _i.e._, the parallelized selective scan operation, is essentially designed for processing one-dimensional sequential data. This presents a challenge when adapting it for processing vision data, which lacks an inherent sequential arrangement of visual components. To address this issue, we propose 2D Selective Scan (SS2D), a four-way scanning mechanism designed for spatial domain traversal. In contrast to the self-attention mechanism (Figure 1 (a)), SS2D ensuresthat each image patch acquires contextual knowledge exclusively through a compressed hidden state computed along its corresponding scanning path (Figure 1 (b)), thereby reducing the computational complexity from quadratic to linear.

Building on the VSS blocks, we develop a family of VMamba architectures (_i.e._, VMamba-Tiny/Small/Base) and enhance their performance through architectural improvements and implementation optimizations. Compared to benchmark vision models built on CNNs (ConvNeXt ), ViTs (Swin , HiViT ), and SSMs (S4ND , Vim ), VMamba consistently achieves higher image classification accuracy on ImageNet-1K  across various model scales. Specifically, VMamba-Base achieves a top-1 accuracy of \(83.9\%\), surpassing Swin by \(+0.4\%\), with a throughput exceeding Swin's by a substantial margin over \(40\%\) (\(646\)_vs._\(458\)). VMamba's superiority extends across multiple downstream tasks, with VMamba-Tiny/Small/Base achieving \(47.3\%\)/\(48.7\%\)/\(49.2\%\) mAP in object detection on COCO  (\(1\) training schedule). This outperforms Swin by \(4.6\%\)/\(3.9\%\)/\(2.3\%\) and ConvNeXt by \(3.1\%\)/\(3.3\%\)/\(2.2\%\), respectively. As for single-scale semantic segmentation on ADE20K , VMamba-Tiny/Small/Base achieves \(47.9\%\)/\(50.6\%\)/\(51.0\%\) mIoU, which surpasses Swin by \(3.4\%\)/\(3.0\%\)/\(2.9\%\) and ConvNeXt by \(1.9\%\)/\(1.9\%\)/\(1.9\%\), respectively. Furthermore, unlike ViT-based models, which experience quadratic growth in computational complexity with the number of input tokens, VMamba exhibits linear growth in FLOPs while maintaining comparable performance. This demonstrates its state-of-the-art input scalability.

The contributions of this study are summarized as follows:

* We propose VMamba, an SSM-based vision backbone for visual representation learning with linear time complexity. A series of architectural and implementation improvements are adopted to enhance the inference speed of VMamba.
* We introduce 2D Selective Scan (SS2D) to bridge 1D array scanning and 2D plane traversal, enabling the extension of selective SSMs to process vision data.
* VMamba achieves promising performance across various visual tasks, including image classification, object detection, and semantic segmentation. It also exhibits remarkable adaptability w.r.t. the length of the input sequence, showcasing linear growth in computational complexity.

## 2 Related Work

Convolutional Neural Networks (CNNs).Since AlexNet , considerable efforts have been devoted to enhancing the modeling capabilities [49; 52; 27; 29] and computational efficiency [28; 53; 64; 46] of CNN-based models across various visual tasks. Sophisticated operators like depth-wise convolution  and deformable convolution [5; 70] have been introduced to increase the flexibility and efficacy of CNNs. Recently, inspired by the success of Transformers , modern CNNs  have shown promising performance by integrating long-range dependencies [11; 47; 34] and dynamic weights  into their designs.

Vision Transformers (ViTs).As a pioneering work, ViT  explores the effectiveness of vision models based on vanilla Transformer architecture, highlighting the importance of large-scale

Figure 1: Comparison of the establishment of correlations between image patches through (a) self-attention and (b) the proposed 2D-Selective-Scan (SS2D). The red boxes indicate the query image patch, with its opacity representing the degree of information loss.

pre-training for image classification performance. To reduce ViT's dependence on large datasets, DeiT  introduces a teacher-student distillation strategy, transferring knowledge from CNNs to ViTs and emphasizing the importance of inductive bias in visual perception. Following this approach, subsequent studies propose hierarchical ViTs [36; 12; 61; 39; 66; 55; 6; 10; 67; 1].

Another research direction focuses on improving the computational efficiency of self-attention, which serves as the cornerstone of ViTs. Linear Attention  reformulates self-attention as a linear dot-product of kernel feature maps, using the associativity property of matrix products to reduce computational complexity from quadratic to linear. GLA  introduces a hardware-efficient variant of linear attention that balances memory movement with parallelizability. RWKV  also leverages the linear attention mechanism to combine parallelizable transformer training with the efficient inference of recurrent neural networks (RNNs). RetNet  adds a gating mechanism to enable a parallelizable computation path, offering an alternative to recurrence. RMT  further extends this for visual representation learning by applying the temporal decay mechanism to the spatial domain.

State Space Models (SSMs). Despite their widespread adoption in vision tasks, ViT architectures face significant challenges due to the quadratic complexity of self-attention, especially when handling long input sequences (_e.g._, high-resolution images). In efforts to improve scaling efficiency [8; 7; 45; 51; 41], SSMs have emerged as compelling alternatives to Transformers, attracting significant research attention. Gu _et al._ demonstrate the potential of SSM-based models in handling the long-range dependencies using the HiPPO initialization . To improve practical feasibility, S4  proposes normalizing the parameter matrices into a diagonal structure. Various structured SSM models have since emerged, each offering distinct architectural enhancements, such as complex-diagonal structures [22; 19], support for multiple-input multiple-output , diagonal plus low-rank decomposition , and selection mechanisms . These advancements have also been integrated into larger representation models [43; 41; 16], further highlighting the versatility and scalability of structured state space models in various applications. While these models primarily target long-range and sequential data such as text and speech, limited research has explored applying SSMs to vision data with two-dimensional structures.

## 3 Preliminaries

Formulation of SSMs.Originating from the Kalman filter , SSMs are linear time-invariant (LTI) systems that map the input signal \(u(t)\) to the output response \(y(t)\) via the hidden state \((t)^{N}\). Specifically, continuous-time SSMs can be expressed as linear ordinary differential equations (ODEs) as follows,

\[^{}(t)&= (t)+u(t),\\ y(t)&=(t)+Du(t),\] (1)

where \(^{N N},^{N 1}\), \(^{1 N}\), and \(D^{1}\) are the weighting parameters.

Discretization of SSM.To be integrated into deep models, continuous-time SSMs must undergo discretization in advance. Concretely, for the time interval \([t_{a},t_{b}]\), the analytic solution of the hidden state variable \((t)\) at \(t=t_{b}\) can be expressed as

\[(t_{b})=e^{(t_{b}-t_{a})}(t_{a})+e^{ (t_{b}-t_{a})}_{t_{a}}^{t_{b}}()u()e^{-(- t_{a})}\,d.\] (2)

By sampling with the time-scale parameter \(\) (_i.e._, \(d|_{t_{i}}^{t_{i+1}}=_{i}\)), \(h(t_{b})\) can be discretized by

\[_{b}=e^{(_{a}++_{b-1})}( _{a}+_{i=a}^{b-1}_{i}u_{i}e^{-(_{a}++ _{i})}_{i}),\] (3)

where \([a,b]\) is the corresponding discrete step interval. Notably, this formulation approximates the result obtained by the zero-order hold (ZOH) method, which is frequently utilized in the literature of SSM-based models (please refer to Appendix A for detailed proof).

Selective Scan Mechanism.To address the limitation of LTI SSMs (Eq. 1) in capturing the contextual information, Gu _et al_.  propose a novel parameterization method for SSMs, which incorporates an input-dependent selection mechanism (referred to as S6). However, for selective SSMs, the time-varying weighting parameters pose a challenge for efficient computation of hidden states, as convolutions cannot accommodate dynamic weights, making them inapplicable. Nevertheless, since the recurrence relation of \(h_{b}\) in Eq. 3 can be derived, the response \(y_{b}\) can still be efficiently computed using associative scan algorithms [2; 42; 50], which has linear complexity (see Appendix B for a detailed explanation).

## 4 VMamba: Visual State Space Model

### Network Architecture

We develop VMamba in three scales: Tiny, Small, and Base (referred to as VMamba-T, VMamba-S, and VMamba-B, respectively). An overview of the architecture of VMamba-T is illustrated in Figure 3 (a), and detailed configurations are provided in Appendix E. The input image \(^{H W 3}\) is first partitioned into patches by a stem module, resulting in a 2D feature map with spatial dimension of \(H/4 W/4\). Without incorporating additional positional embeddings, multiple network stages are employed to create hierarchical representations with resolutions of \(H/8 W/8\), \(H/16 W/16\), and \(H/32 W/32\). Specifically, each stage comprises a down-sampling layer (except for the first stage), followed by a stack of Visual State Space (VSS) blocks.

The VSS blocks serve as the visual counterparts to Mamba blocks  (Figure 3 (b)) for representation learning. The initial architecture of VSS blocks (referred to as the 'vanilla VSS Block' in Figure 3 (c)) is formulated by replacing the S6 module. S6 is the core of Mamba and achieves global receptive fields, dynamic weights (_i.e._, selectivity), and linear complexity. We substitute it with the newly proposed 2D-Selective-Scan (SS2D) module, and more details will be introduced in the following subsection. To further enhance computational efficiency, we remove the entire multiplicative branch (highlighted by the red box in Figure 3 (c)), as the effect of the gating mechanism has already been achieved by the selectivity of SS2D. As a result, the improved VSS block (shown in Figure 3 (d)) consists of a single network branch with two residual modules, mimicking the architecture of a vanilla Transformer block . All results in this paper are obtained using VMamba models built with VSS blocks in this architecture.

### 2D-Selective-Scan for Vision Data (SS2D)

While the sequential nature of the scanning operation in S6 aligns well with NLP tasks involving temporal data, it poses a significant challenge when applied to vision data, which is inherently non-sequential and encompasses spatial information (_e.g._, local texture and global structure). To address this issue, S4ND  reformulates SSM with convolutional operations, directly extending the kernel from 1D to 2D through the outer-product. However, such modification restricts the weights from being input-dependent, resulting in a limited capacity for capturing contextual information. Therefore, we adhere to the selective scan approach  for input processing and propose the 2D-Selective-Scan (SS2D) module to adapt S6 to vision data without compromising its advantages.

Figure 2: Illustration of 2D-Selective-Scan (SS2D). Input patches are traversed along four different scanning paths (_Cross-Scan_), with each sequence independently processed by separate S6 blocks. The results are then merged to construct a 2D feature map as the final output (_Cross-Merge_).

Figure 2 illustrates that data forwarding in SS2D consists of three steps: cross-scan, selective scanning with S6 blocks, and cross-merge. Specifically, SS2D first unfolds the input patches into sequences along four distinct traversal paths (_i.e._, Cross-Scan). Each patch sequence is then processed in parallel using a separate S6 block, and the resultant sequences are reshaped and merged to form the output map (_i.e._, Cross-Merge). Through the use of complementary 1D traversal paths, SS2D allows each pixel in the image to integrate information from all other pixels across different directions. This integration facilitates the establishment of global receptive fields in the 2D space.

### Accelerating VMamba

As shown in Figure 3 (e), the VMamba-T model with vanilla VSS blocks (referred to as 'Vanilla VMamba') achieves a throughput of \(426\) images/s and contains \(22.9\)M parameters with \(5.6\)G FLOPs. Despite achieving a state-of-the-art classification accuracy of 82.2% (outperforming Swin-T  by 0.9% at the tiny level), the low throughput and high memory overhead present significant challenges for the practical deployment of VMamba.

In this subsection, we outline our efforts to enhance its inference speed, primarily focusing on improvements in both implementation details and architectural design. We evaluate the models with image classification on ImageNet-1K. The impact of each progressive improvement is summarized as follows, where (\(\%\), img/s) denote the gains in top-1 accuracy on ImageNet-1K and inference throughput, respectively. Further discussion is provided in Appendix E.

1. (\(+0.0\%\), \(+41\) img/s) by re-implementing Cross-Scan and Cross-Merge in Triton.
2. (\(+0.0\%\), \(-3\) img/s) by adjusting the CUDA implementation of selective scan to accommodate float16 input and float32 output. This remarkably enhances the training efficiency (throughput from \(165\) to \(184\)), despite slight speed fluctuation at test time.
3. (\(+0.0\%\), \(+174\) img/s) by substituting the relatively slow einsum in selective scan with a linear transformation (_i.e._, torch.nn.functional.linear). We also adopt the tensor layout of (B, C, H, W) to eliminate unnecessary data permutations.

Figure 3: **Left:** Illustration of (a) the overall architecture of VMamba, and (b) - (d) the structure of Mamba and VSS blocks. **Right:** Comparison of VMamba variants and benchmark methods in terms of classification accuracy and computational efficiency.

* (\(-0.6\%\), \(+175\) img/s) by introducing MLP into VAMba due to its computational efficiency. We also discard the DWConv (depth-wise convolutional ) layers and change the layer configuration from  to  to lower FLOPs.
* (\(+0.6\%\), \(+366\) img/s) by reducing the parameter ssm-ratio (the feature expansion factor) from \(2.0\) to \(1.0\) (also referred to as Step (d.1)), raising the layer numbers to  (also referred to as Step (d.2)), and discarding the entire multiplicative branch as illustrated in Figure 3 (c).
* (\(+0.3\%\), \(+161\) img/s) by introducing the DWConv layers (also referred to as Step (e.1)) and reducing the parameter d_state (the SSM state dimension) from \(16.0\) to \(1.0\) (also referred to as Step (e.2)), together with raising ssm-ratio back to \(2.0\).
* (\(+0.1\%\), \(+346\) img/s) by reducing the ssm-ratio to \(1.0\) while changing the layer configuration from  to .

## 5 Experiments

In this section, we present a series of experiments to evaluate the performance of VAMba and compare it to popular benchmark models across various visual tasks. We also validate the effectiveness of the proposed 2D feature map traversal method by comparing it with alternative approaches. Additionally, we analyze the characteristics of VAMba by visualizing its effective receptive field (ERF) and activation map, and examining its scalability with longer input sequences. We primarily follow the hyperparameter settings and experimental configurations used in Swin . For detailed experiment settings, please refer to Appendix E and F, and for additional ablations, see Appendix H. All experiments were conducted on a server with 8 \(\) NVIDIA Tesla-A100 GPUs.

### Image Classification

We evaluate VAMba's performance in image classification on ImageNet-1K , with comparison results against benchmark methods summarized in Table 1. With similar FLOPs, VAMba-T achieves a top-1 accuracy of \(82.6\%\), outperforming DeiT-S by \(2.8\%\) and Swin-T by \(1.3\%\). Notably, VAMba maintains its performance advantage at both Small and Base scales. For example, VAMba-B achieves a top-1 accuracy of \(83.9\%\), surpassing DeiT-B by \(2.1\%\) and Swin-B by \(0.4\%\).

In terms of computational efficiency, VAMba-T achieves a throughput of 1,686 images/s, which is either superior or comparable to state-of-the-art methods. This advantage continues with VAMba-S and VAMba-B, achieving throughputs of \(877\) images/s and \(646\) images/s, respectively. Compared to SSM-based models, the throughput of VAMba-T is \(1.47\) higher than S4ND-Conv-T  and \(1.08\) higher than Vim-S , while maintaining a clear performance lead of \(0.4\%\) and \(2.1\%\) over these models, respectively.

   Model &  Params \\ (M) \\  &  FLOPs \\ (G) \\  &  TP. \\ (img/s) \\  & 
 Top-1 \\ (\%) \\  \\   \\  ConvNeXt-T  & 29M & 4.5G & 1198 & 82.1 \\ ConvNeXt-S  & 50M & 8.7G & 684 & 83.1 \\ ConvNeXt-B  & 89M & 15.4G & 436 & 83.8 \\   \\  S4ND-Conv-T  & 30M & 5.2G & 683 & 82.2 \\ S4ND-ViT-B  & 89M & 17.1G & 397 & 80.4 \\ Vim-S  & 26M & 5.3G & 811 & 80.5 \\  VAMba-T & 30M & 4.9G & 1686 & 82.6 \\ VAMba-S & 50M & 8.7G & 877 & 83.6 \\ VAMba-B & 89M & 15.4G & 646 & 83.9 \\   

Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100 GPU and an AMD EPYC 7542 CPU, using the toolkit released by , following the protocol proposed in . All images are of size \(224 224\).

### Downstream Tasks

In this sub-section, we evaluate the performance of VMamba on downstream tasks, including object detection and instance segmentation on MSCOCO2017 , and semantic segmentation on ADE20K . The training framework is based on the MMDetection  and MMSgmenation  libraries, following  in utilizing Mask R-CNN  and UperNet  as the detection and segmentation networks, respectively.

Object Detection and Instance Segmentation.The results on MSCOCO are presented in Table 2. VMamba demonstrates superior performance in both box and mask Average Precision (AP\({}^{}\) and AP\({}^{}\)) across different training schedules. Under the \(12\)-epoch fine-tuning schedule, V/Mamba-T/S/B achieves object detection mAPs of \(47.3\%/48.7\%/49.2\%\), outperforming Swin-T/S/B by \(4.6\%/3.9\%/2.3\%\) mAP and ConvNeXt-T/S/B by \(3.1\%/3.3\%/2.2\%\) mAP, respectively. V/Mamba-T/S/B achieves instance segmentation mAPs that exceed Swin-T/S/B by \(3.4\%/2.8\%/1.8\%\) mAP and ConvNeXt-T/S/B by \(2.6\%/1.9\%/1.4\%\) mAP, respectively. Furthermore, VMamba's advantages persist with the 36-epoch fine-tuning schedule using multi-scale training, highlighting its strong potential in downstream tasks requiring dense predictions.

Semantic Segmentation.Consistent with previous experiments, VMamba demonstrates superior performance in semantic segmentation on ADE20K with a comparable amount of parameters. As shown in Table 2, VMamba-T achieves \(3.4\%\) higher mIoU than Swin-T and \(1.9\%\) higher than ConvNeXt-T in the Single-Scale (SS) setting, and the advantage persists with Multi-Scale (MS) input. For models at the Small and Base levels, VMamba-S/B outperforms NAT-S/B  by \(2.6\%\)/\(2.5\%\) mIoU in the SS setting, and \(1.7\%/1.9\%\) mIoU in the MS setting.

DiscussionThe experimental results in this subsection demonstrate VMamba's adaptability to object detection, instance segmentation, and semantic segmentation. In Figure 4 (a), we compare VMamba's performance with Swin and ConvNeXt, highlighting its advantages in handling downstream tasks with comparable classification accuracy on ImageNet-1K. This result aligns with Figure 4 (b), where VMamba shows the most stable performance (_i.e._, modest performance drop) across different input image sizes, achieving a top-1 classification accuracy of \(74.7\%\) without fine-tuning (\(79.2\%\) with linear tuning) at an input resolution of \(768 768\). While exhibiting greater tolerance to changes

    \\  Backbone & AP\({}^{}\) & AP\({}^{}\) & Params & FLOPs \\  Swin-T & 42.7 & 39.3 & 48M & 267G \\ ConvNeXt-T & 44.2 & 40.1 & 48M & 262G \\  V/Mamba-T & 47.3 & 42.7 & 50M & 271G \\  Swin-S & 44.8 & 40.9 & 69M & 354G \\ ConvNeXt-S & 45.4 & 41.8 & 70M & 348G \\  V/Mamba-S & 48.7 & 43.7 & 70M & 349G \\  Swin-B & 46.9 & 42.3 & 107M & 496G \\ ConvNeXt-B & 47.0 & 42.7 & 108M & 486G \\ 
**Mask R-CNN 3\(\) MS schedule** & **4** & **4** & **4** \\  Swin-T & 46.0 & 41.6 & 48M & 267G \\ ConvNeXt-T & 46.2 & 41.7 & 48M & 262G \\ NAT-T & 47.7 & 42.6 & 48M & 258G \\  V/Mamba-T & 48.8 & 43.7 & 50M & 271G \\  Swin-S & 48.2 & 43.2 & 69M & 354G \\ ConvNeXt-S & 47.9 & 42.9 & 70M & 348G \\ NAT-S & 48.4 & 43.2 & 70M & 330G \\  V/Mamba-S & 49.9 & 44.2 & 70M & 349G \\   

Table 2: **Left: Results for object detection and instance segmentation on MSCOCO. \(AP^{b}\) and \(AP^{m}\) denote box AP and mask AP, respectively. FLOPs are calculated with an input size of \(1280 800\). The notation ‘\(1\)’ indicates models fine-tuned for \(12\) epochs, while ‘\(3\)MS’ denotes multi-scale training for \(36\) epochs. Right: Results for semantic segmentation on ADE20K. FLOPs are calculated with an input size of \(512 2048\). ‘SS’ and ‘MS’ denote single-scale and multi-scale testing, respectively.**in input resolution, VMamba also maintains linear growth in FLOPs and memory-consumption (see Figure 5 (a) and (c)) and maintains high throughput ( Figure 5 (b)), making it more effective and efficient compared to ViT-based methods when adapting to downstream tasks with inputs of larger spatial resolutions. This aligns with Mamba's advanced capability in efficient long sequence modeling .

### Analysis

Relationship between SS2D and Self-Attention.To formulate the response \(\) within the time interval \([a,b]\) of length \(T\), we denote the corresponding SSM-related variables \(_{i}_{i}^{1 D_{v}}\), \(_{i}^{1 D_{k}}\), and \(_{i}^{1 D_{k}}\) as \(^{T D_{v}}\), \(^{T D_{k}}\), and \(^{T D_{k}}\), respectively. Therefore, the \(j\)-th slice along dimension \(D_{v}\) of \(}\), denoted as \(}^{(j)}\) can be written as

\[}^{(j)}=(}}^{(j)}) }^{(j)}+}_{i=1}^{T}(}^{ (j)}}{}^{(j)}}})^{}( }^{(j)}).\] (4)

Figure 4: Illustration of VMamba’s adaptability to (a) downstream tasks and (b) input images with progressively increasing resolutions. Swin-T\({}^{*}\) denotes Swin-T tested with scaled window sizes.

Figure 5: Illustration of VMamba’s resource consumption with progressively increasing resolutions. Swin-T\({}^{*}\) denotes Swin-T tested with scaled window sizes.

Figure 6: Illustration of the activation map for query patches indicated by red stars. The visualization results in (b) and (c) are obtained by combining the activation maps from each scanning path in SS2D.

where \(}^{D_{k}}\) is the hidden state at step \(a\), \(\) denotes element-wise product. Particularly, \(}^{(j)}\) is only a scalar. The formulation of each element in \([_{1};;_{T}]^{T D _{k} D_{v}}\), _i.e._, \(_{i}^{D_{k} D_{v}}\), can be written as \(_{i}=_{j=1}^{i}e^{_{a-1+j}^{}}\), representing the cumulative attention weight at step \(i\) computed along the scanning path.

Consequently, the \(j\)-th dimension of \(\), _i.e._, \(^{(j)}^{T 1}\), can be expressed as

\[^{(j)}=[^{(j)}]}^{ (j)}+[(^{(j)})(}{^{(j)}})^{}]^{(j)},\] (5)

where \(\) denotes the temporal mask matrix of size \(T T\) with the lower triangular part set to 1 and elsewhere 0. Please refer to Appendix C for more detailed derivations.

In Eq. 5, the matrix multiplication process involving \(\), \(\), and \(\) closely resembles the self-attention mechanism, despite the inclusion of \(\).

Visualization of Activation Maps.To gain an intuitive and in-depth understanding of SS2D, we further visualize the attention values in \(^{}\) and \(()(/)^{}\) corresponding to a specific query patch within foreground objects (referred to as the 'activation map'). As shown in Figure 6 (b), the activation map of \(^{}\) demonstrates the effectiveness of SS2D in capturing and retaining traversed information, with all previously scanned tokens in the foreground region being activated. Furthermore, the inclusion of \(\) results in activation maps that are more focused on the neighborhood of query patches (Figure 6 (c)), which is consistent with the temporal weighting effect inherent in the formulation of \(\). Nevertheless, the selective scan mechanism allows VMamba to accumulate history along the scanning path, facilitating the establishment of long-term dependencies across image patches. This is evident in the sub-figure encircled by a red box (Figure 6 (d)), where patches of the sheep far to the left (scanned in earlier steps) remain activated. For more visualizations and further discussion, please refer to Appendix D.

Visualization of Effective Receptive Fields.The Effective Receptive Field (ERF)  refers to the region in the input space that contributes to the activation of a specific output unit. We conduct a comparative analysis of the central pixel's ERF across various visual backbones, both before and after training. The results presented in Figure 7 illustrate that among the models examined, only DeiT, HiViT, Vim and VMamba demonstrate global ERFs, while the others exhibit local ERFs despite their theoretical potential for global coverage. Moreover, VMamba's linear time complexity enhances its computational efficiency compared to DeiT and HiViT, which incur quadratic costs w.r.t. the number of input patches. While both VMamba and Vim are based on the Mamba architecture, VMamba's ERF is more uniform and 2D-aware than that of Vim, which may intuitively explain its superior performance.

Diagnostic Study on Selective Scan Patterns.We compare the proposed scanning pattern (_i.e._ Cross-Scan) to three benchmark patterns: unidirectional scanning (Unidi-Scan), bidirectional scanning (Bidi-Scan), and cascade scanning (Cascade-Scan, scanning the data row-wise and column-wise successively). Feature dimensions are adjusted to maintain similar architectural parameters and

Figure 7: Comparison of Effective Receptive Fields (ERF)  between VMamba and other benchmark models. Pixels with higher intensity indicate larger responses related to the central pixel.

FLOPs for a fair comparison. As illustrated in Figure 8, Cross-Scan outperforms the other scanning patterns in both computational efficiency and classification accuracy, highlighting its effectiveness in achieving 2D-Selective-Scan. Removing the DWConv layer, which has been shown to aid the model in learning 2D spatial information, further enhances this advantage. This underscores the inherent strength of Cross-Scan in capturing 2D contextual information through its adoption of four-way scanning.

## 6 Conclusion

This paper presents VMamba, an efficient vision backbone model built with State Space Models (SSMs). VMamba integrates the advantages of selective SSMs from NLP tasks into visual data processing, bridging the gap between ordered 1D scanning and non-sequential 2D traversal through the novel SS2D module. Furthermore, we have significantly improved the inference speed of VMamba through a series of architectural and implementation refinements. The effectiveness of the VMamba family has been demonstrated through extensive experiments, and its linear time complexity makes VMamba advantageous for downstream tasks with large-resolution inputs.

Limitations.While VMamba demonstrates promising experimental results, there is still room for improvement in this study. Previous research has validated the efficacy of unsupervised pre-training on large-scale datasets (_e.g._, ImageNet-21K). However, the compatibility of existing pre-training methods with SSM-based architectures like VMamba, as well as the identification of pre-training techniques specifically tailored for such models, remain unexplored. Investigating these aspects could serve as a promising avenue for future research in architectural design. Additionally, limited computational resources have prevented us from exploring VMamba's architecture at the Large scale and conducting a fine-grained hyperparameter search to further enhance experimental performance. Although SS2D, the core component of VMamba, does not make specific assumptions about the layout or modality of the input data, allowing it to generalize across various tasks, the potential of VMamba for integration into more generalized tasks remains unexplored. Bridging the gap between SS2D and these tasks, along with proposing a more generalized scanning pattern for vision tasks, represents a promising research direction.

## 7 Acknowledgments

This work was supported by National Natural Science Foundation of China (NSFC) under Grant No.62225208 and 62406304, CAS Project for Young Scientists in Basic Research under Grant No.YSBR-117, China Postdoctoral Science Foundation under Grant No.2023M743442, and Postdoctoral Fellowship Program of CPSF under Grant No.GZB20240730.

Figure 8: Performance comparison of different scanning patterns. The proposed Cross-Scan achieves superior performance in speed while maintaining the same number of parameters and FLOPs.