# Adversarially Robust Decision Transformer

Xiaohang Tang

University College London

xiaohang.tang.20@ucl.ac.uk

&Afonso Marques

University College London

afonso.marques.22@ucl.ac.uk

Equal Contribution.

Parameswaran Kamalaruban

Featurespace

kamal.parameswaran@featurespace.co.uk

&Iija Bogunovic

University College London

i.bogunovic@ucl.ac.uk

Equal Contribution.

###### Abstract

Decision Transformer (DT), as one of the representative Reinforcement Learning via Supervised Learning (RvS) methods, has achieved strong performance in offline learning tasks by leveraging the powerful Transformer architecture for sequential decision-making. However, in adversarial environments, these methods can be non-robust, since the return is dependent on the strategies of both the decision-maker and adversary. Training a probabilistic model conditioned on observed return to predict action can fail to generalize, as the trajectories that achieve a return in the dataset might have done so due to a suboptimal behavior adversary. To address this, we propose a worst-case-aware RvS algorithm, the Adversarially Robust Decision Transformer (ARDT), which learns and conditions the policy on in-sample minimax returns-to-go. ARDT aligns the target return with the worst-case return learned through minimax expectible regression, thereby enhancing robustness against powerful test-time adversaries. In experiments conducted on sequential games with full data coverage, ARDT can generate a maximin (Nash Equilibrium) strategy, the solution with the largest adversarial robustness. In large-scale sequential games and continuous adversarial RL environments with partial data coverage, ARDT demonstrates significantly superior robustness to powerful test-time adversaries and attains higher worst-case returns compared to contemporary DT methods.

## 1 Introduction

Reinforcement Learning via Supervised Learning (RvS) , has garnered attention within the domain of offline Reinforcement Learning (RL). Framing RL as a problem of outcome-conditioned sequence modeling, these methodologies have showcased strong performance in offline benchmark tasks . One of the representative RvS methods is Decision Transformer (DT) , which simply trains a policy conditioned on target return via behavior cloning loss. Given the apparent efficacy and simplicity of RvS, our attention is drawn to exploring its performance in adversarial settings. In this paper, we aim to use RvS to achieve _adversarial robustness_, a critical capability for RL agents to manage environmental variations and adversarial disturbances . Such challenges are prevalent in real-world scenarios, e.g., changes in road conditions and decision-making among multiple autonomous vehicles.

In offline RL with adversarial actions, the naive training of action-prediction models conditioned on the history and observed returns such as DT, or expected returns such as ESPER and DoC , which we refer to as Expected Return-Conditioned DT (ERC-DT), may result in a non-robust policy.

This issue arises from the potential distributional shifts in the policy of the adversary during offline learning. Specifically, if the behavior policies used for data collection are suboptimal, a well-trained conditional policy overfitted to the training dataset may fail to achieve the desired target return during testing, if the adversary's policy at test time has changed or become more effective.

We illustrate this using a sequential game with data collected by a uniform random policy with full coverage shown in Figure 1. The standard RvS methods (DT and ERC-DT) when conditioned on high target return, show poor worst-case performance. DT's failure stems from training data containing the sequences \((a_{0},_{2})\) and \((a_{1},_{3})\), inaccurately indicating both actions \(a_{0}\) and \(a_{1}\) as optimal. ERC-DT missteps because the adversary's uniform random policy skews the expected return in favor of \(a_{0}\) over \(a_{1}\), wrongly favoring \(a_{0}\). A robust strategy, however, exclusively selects \(a_{1}\) against an optimal adversary to guarantee a high reward. Moreover, in more complex multi-decision scenarios, the adversarial strategy should not only minimize but also counterbalance with maximized returns by the decision-maker at the subsequent state, aligning with a _minimax_ return approach for robust policy development.

**Main Contributions.** Consequently, there is a need for data curation to indicate the potential worst-case returns associated with each action in the Decision Transformer (DT), improving the robustness against adversarial actions. Building on this concept, we introduce the first robust training algorithm for Decision Transformer, the **Adversarially Robust Decision Transformer (ARDT)**. To the best of our knowledge, this paper represents the first exploration of the robustness of RvS methods from an _adversarial_ perspective. In ARDT, we leverage Expectile Regression  to transform the original returns-to-go to in-sample minimax returns-to-go, and train DT with the relabeled returns. In the experiments, we illustrate the robustness of ARDT in three settings (i) Games with full data coverage (ii) A long-horizon discrete game with partial data coverage (iii) Realistic continuous adversarial RL problems. We provide evidence showcasing the robustness of ARDT to more powerful adversary compared to the behavior policy and the distributional shifts of the adversary's policy in the test time. Furthermore, in the adversarial MuJoCo tasks, namely Noisy Action Robust MDP , ARDT exhibits better robustness to a population of adversarial perturbations compared to existing DT methods.

**Related work.** Reinforcement Learning via Supervised Learning is a promising direction for solving general offline RL problems by converting the RL problem into the outcome-conditioned prediction problem . Decision Transformer (DT) is one of the RvS instance , leveraging the strength of the sequence model Transformer . The variants of DT have been extensively investigated to address the trajectory stitching , stochasticity , goal-conditioned problems , and generalization in different environments . For different problems, it is critical to select an appropriate target as the condition of the policy. \(Q\)-Learning DT  relabels the trajectories with the estimated \(Q\)-values learned to achieve trajectory stitching . ESPER , aims at solving the environmental stochasticity via relabeling the trajectories with expected returns-to-go. Although our method also transform the returns-to-go to

Figure 1: **LHS** presents the game where decision-maker \(P\) is confronted by adversary \(A\). In the worst-case scenario, if \(P\) chooses action \(a_{0}\), \(A\) will respond with \(_{0}\), and if \(P\) chooses \(a_{1}\), \(A\) will counter with \(_{4}\). Consequently, the worst-case returns for actions \(a_{0}\) and \(a_{1}\) are \(0\) and \(1\), respectively. Therefore, the robust choice of action for the decision-maker is \(a_{1}\). **RHS** displays tables of action probabilities and the worst-case returns for the Decision Transformer (DT), Expected Return-Conditioned DT (ERC-DT) methods and our algorithm, when conditioned on the largest return-to-go \(6\). After training using uniformly collected data that covered all possible trajectories, the results reveal that DT fails to select the robust action \(a_{1}\), whereas our algorithm manages to do so.

estimated values, we instead associate worst-case returns-to-go with trajectories, aiming at improving its robustness against adversarial perturbations.

Adversarial RL can be framed as a sequential game; with online game-solving explored extensively in literature [19; 11; 40; 24; 20; 39; 38; 41]. In the offline setting, where there is no interaction with the environment, solving games is challenging due to distributional shifts in the adversary's policy, necessitating adequate data coverage. Tabular methods, such as Nash \(Q\)-Learning--a decades-old approach for decision-making in competitive games --and other similar methods [4; 56] address these challenges through pessimistic out-of-sample estimation. While value-based methods are viable for addressing adversarial problems, our focus is on enhancing the robustness of RvS methods in these settings. Note that while some offline multi-agent RL methods, including DT-based approaches, are designed for cooperative games [26; 48; 44; 47], our research is oriented towards competitive games.

## 2 Problem Setup

We consider the adversarial reinforcement learning problem involving a protagonist and an adversary, as described by Pinto et al. . This setting is rooted in the two-player Zero-Sum Markov Game framework [23; 33]. This game is formally defined by the tuple \((,,},T,R,p_{0})\). Here, \(\) represents the state space, where \(\) and \(}\) denote the action spaces for the protagonist and adversary, respectively. The reward function \(R:}\) and the transition kernel \(T:}( )\) depend on the state and the joint actions of both players. The initial state distribution is denoted by \(p_{0}\). At each time-step \(t H\), both players observe the state \(s_{t}\) and take actions \(a_{t}\) and \(_{t}}\). The adversary is considered adaptive if its action \(_{t}\) depends on \(s_{t}\) and \(a_{t}\). The protagonist receives a reward \(r_{t}=R(s_{t},a_{t},_{t})\), while the adversary receives a negative reward \(-r_{t}\).

Denote trajectory \(=(s_{t},a_{t},_{t},r_{t})_{t=0}^{H}\) and its sub-trajectory \(_{i:j}=(s_{t},a_{t},_{t},r_{t})_{t=i}^{j}\) for \(0 i j H\). The return-to-go refers to the sum of observed rewards from state \(s_{t}\) onwards: \((_{t:H})=_{t^{}=t}^{H}r_{t^{}}\). The protagonist policy \(\) and the adversary policy \(\) can be history-dependent, mapping history \((_{0:t-1},s_{t})\) to the distribution over protagonist and adversary's actions, respectively.

In offline RL, direct interaction with the environment is unavailable. Instead, we rely on an offline dataset \(\), which consists of trajectories generated by executing a pair of behavioral policies, \((_{},_{})\). Our objective in this adversarial offline RL setting is to utilize this dataset \(\) to learn a protagonist policy that seeks to maximize the return, while being counteracted by an adversary's policy \(\):

\[_{}_{}_{^{,}}[_{t}r_ {t}],\] (1)

where \(^{,}()=p_{0}(s_{0})_{t}(a_{t}_{0:t-1},s_{t})(_{t}_{0:t-1},s_{t},a_{t}) T(s_{t+1}  s_{t},a_{t},_{t})\). The maximin solution to this problem and its corresponding optimal adversary are denoted as \(^{*}\) and \(^{*}\), respectively.

### RvS in Adversarial Environments

We extend the RvS framework by incorporating adversarial settings within the Hindsight Information Matching (HIM) paradigm . In adversarial HIM, the protagonist receives a goal (denoted by \(z\)) and acts within an adversarial environment. The protagonist's performance is evaluated based on how well it achieves the presented goal. Mathematically, the protagonist aims to minimize a distance metric \(D(,)\) between the target goal \(z\) and the information function \(I()\) applied to a trajectory \(\). This sub-trajectory follows a distribution \(\) dependent on both the protagonist's policy \(_{z}=(a_{t}_{0:t-1},s_{t},z)\) and the test-time adversarial policy \(_{}\). Formally, the information distance is minimized as follows:

\[_{}_{^{_{z}},_{}}[D(I(),z)],\] (2)

where \(^{_{z},_{}}\) represents the trajectory distribution obtained by executing rollouts in a Markov Game with the protagonist's policy \(_{z}\) and the test-time adversarial policy \(_{}\). Under HIM framework, Decision Transformer  uses adopt return-to-go value as the information function, while ESPER  employs the expected return-to-go. The well-trained RvS policy, including DT and ESPER, is derived from behavior trajectories filtered based on the condition \(I()=z\):

\[_{z}=(a_{t}_{0:t-1},s_{t},z)=^{_{,_{ }}}(a_{t}_{0:t-1},s_{t},I()=z).\] (3)

If the conditional policy ensures that the information distance in Eq. (2) is minimized to \(0\), the generated policy \(_{z}\) is robust against \(_{}\) by simply conditioning on a large target return, or even the maximin optimal protagonist \(^{*}=_{z+}_{z}\) if the test-time adversary is optimal. Consequently, our objective shifts from learning a maximin policy in Eq. (1) to minimizing the distance in Eq. (2).

For a given target goal \(z\), the trajectories where the information function \(I()=z\) can be considered as optimal demonstrations of achieving that goal, i.e. information distance minimization, \(D(I(),z)=0\). However, in adversarial environments, this information distance is hard to be minimized to \(0\). In adversarial offline RL, a trajectory in the dataset might achieve a goal due to the presence of a weak behavior adversary, rather than through the protagonist's effective actions. Filtering datasets for trajectories with high return-to-go could select the ones from encounters with suboptimal adversaries. Consequently, an agent trained through behavioral cloning on these trajectories is not guaranteed to achieve a high return when facing a different and powerful adversary at test time. We show this formally through a theorem:

**Theorem 1**.: _Let \(_{}\) and \(_{}\) be the data collecting policies used to gather data for training an RvS protagonist policy \(\). Assume \(T(s_{t+1} s_{t},a_{t},_{t})=^{_{},_{ }}(s_{t+1}_{0:t-1},s_{t},a_{t},I()=z)\)2, for any goal \(z\) such that \(^{_{},_{}}(I()=z s_{0})>0\), the information distance can be minimized: \(_{^{_{z}},s_{}}[D(I(_{t}),z)]=0\) if \(_{}(_{t}_{0:t-1},s_{t},a_{t})=^{_{ },_{}}(_{t}_{0:t-1},s_{t},a_{t}, I()=z)\)._

Theorem 1 suggests that the protagonist policy \(_{z}\), defined in Eq. (3), can minimize the information distance when the test-time adversarial policy is \(_{}=^{_{},_{}}(_{t}_{0:t-1},s_{t},a_{t},I()=z)\). In other words, \(_{z}\), conditioned on a large target return, is guaranteed to be robust against \(^{_{},_{}}(_{t}_{0:t-1},s _{t},a_{t},I()=z)\). Consequently, the policies of DT and ESPER are only guaranteed to perform well against behavior adversarial policies and will be vulnerable to powerful test-time adversaries since their information function is dependent on behavior adversary. This is a concern because, in practice, offline RL datasets are often collected in settings with weak or suboptimal adversaries.

In this context, to enhance robustness, we consider using an information function \(I()\) to train the protagonist policy \(_{z}\) by simulating the worst-case scenarios we might encounter during test time. A natural choice for \(I()\) is the **minimax return-to-go**, which we formally introduce in Section 3 and use as a central component of our method. With this \(I()\) and sufficient data coverage, \(^{_{},_{}}(_{t}_{0:t-1},s _{t},a_{t},I()=z)\) can be approximated to the optimal adversarial policy, implying that \(_{z}\) is robust against the optimal adversary. Furthermore, even without sufficient data coverage, \(_{z}\) remains more robust compared to previous DT methods, which tend to overfit to the behavior adversarial policy.

## 3 Adversarially Robust Decision Transformer

To tackle the problem of suboptimal adversarial behavior in the dataset, we propose **Adversarially Robust Decision Transformer (ARDT)**, a worst-case-aware RvS algorithm. In ARDT, the information function is defined as the expected minimax return-to-go:

\[I()\ =\ _{_{t:H}}_{a_{t+1:H}}_{s_{t^{}+1}  T( s_{t^{}},a_{t^{}},_{t^{}})}[ _{t^{}=t}^{H}R(s_{t^{}},a_{t^{}},_{t^{}}) \ |\ _{0:t-1},s_{t},a_{t}].\] (4)

Intuitively, this information function represents the expected return when an adversary aims to minimize the value, while the protagonist subsequently seeks to maximize it. To train a protagonist policy as described in Eq. (3), ARDT first relabels the trajectory with the minimax returns-to-go in Eq. (4). Secondly, the protagonist policy is then trained similarly to a standard Decision Transformer (DT). This two-step overview of ARDT is provided in Figure 2, where the left block includes the minimax return estimation with expectile regression for trajectory relabeling, and the right block represents the DT training conditioned on the relabeled returns-to-go.

We relabel the trajectories in our dataset with the worst-case returns-to-go by considering the optimal in-sample adversarial actions. In practice, we approximate the proposed information function using an (in-sample) _minimax returns-to-go_ predictor, denoted by \(\). We simplify the notation of the appearance of actions in the dataset \(a_{t}:_{}(a_{t}|_{0:t-1},s_{t})>0\) and \(_{t}}:_{}(_{t}| _{0:t-1},s_{t},a_{t})>0\) to \(a_{t}\) and \(_{t}\), respectively. The optimal predictor \(^{*}\) is a history-dependent state-action value function satisfying the following condition that at any time \(t=1,,T\):

\[^{*}(_{0:t-1},s_{t},a_{t})\ =\ _{_{t}}r_{t}+ _{s_{t+1} T(|s_{t},a_{t},_{t})}[_{a_{t+1} }^{*}(_{0:t},s_{t+1},a_{t+1})],\] (5)

We adopt Expectile Regression (ER)  to approximate \(\). This choice is particularly advantageous for RL applications because it helps avoid out-of-sample estimation or the need to query actions to approximate the minimum or maximum return . Specifically, the loss function for ER is a weighted mean squared error (MSE):

\[L^{}_{}(u)=_{u}|-(u>0)| u ^{2}\,.\] (6)

Suppose a random variable \(y\) follows a conditional distribution \(y(|x)\), function \(g_{}(x):=_{g(x)}L^{}_{}(g(x)-h(x,y))\) can serve as an approximate minimum or maximum operator for the function \(h(x,y)\) over all possible value of \(y\), i.e.,

\[_{ 0}g_{}(x)=_{y:(y|x)>0}h(x,y),\ \ _{ 1}g_{}(x)=_{y:(y|x)>0}h(x,y).\] (7)

Our algorithm uses coupled losses for maximum and minimum operators to approximate the in-sample minimax returns-to-go. Formally, given an offline dataset \(\), we alternately update the minimax and maximin returns-to-go estimators: \(_{}(s,a)\) and \(Q_{}(s,a,)\). In each iteration, we fix \(\) (or \(\)), denoted as \(\) (or \(\)), and then update \(\) (or \(\)) based on the following loss functions:

\[^{}() =_{}L^{}_{} _{}(_{0:t-1},s_{t},a_{t})-Q_{}(_{0 :t-1},s_{t},a_{t},_{t})\,,\] (8) \[^{1-}() =_{}L^{1-}_{ }(Q_{}(_{0:t-1},s_{t},a_{t},_{t})-_{}( _{0:t},s_{t+1},a_{t+1})-r_{t})\,.\] (9)

The minimizers (\(^{*}\) and \(^{*}\)) of the two losses mentioned above (with \( 0\)), combined with Eq. (7), satisfy the following condition for all \(s_{t},a_{t},_{t}\):

\[_{^{*}}(_{0:t-1},s_{t},a_{t}) =_{}Q_{^{*}}(_{0:t-1},s_{t},a_ {t},),\] (10) \[Q_{^{*}}(_{0:t-1},s_{t},a_{t},_{t}) =_{s_{t+1}^{*}_{}(_{0:t-1},s _{t},a_{t},_{t})}[_{a^{}}_{ ^{*}}(_{0:t},s_{t+1},a^{})+r_{t}].\] (11)

By substituting Eq. (11) into Eq. (10), the minimax returns-to-go predictor \(_{^{*}}\) satisfies Eq. (5).

The formal algorithm of ARDT is presented in Algorithm 1, where from line 3-6 is the minimax return estimation diagrammed as the left block in Figure 2, and line 7-10 is the Transformer training on

Figure 2: Training of Adversarially Robust Decision Transformer. We adopt Expectile Regression for estimator \(\) to approximate the in-sample minimax. In the subsequent protagonist DT training, we replace the original returns-to-go with the learned values \(^{*}\) to train policy.

the right of Figure 2. Initially, we warm up two returns-to-go networks with the original returns-to-go values from the dataset. This step ensures the value function converges to the expected values, facilitating faster maximin value estimation and guaranteeing accurate value function approximation at terminal states. Subsequently, we estimate the minimax returns-to-go by alternately updating the parameters of the two networks based on the expectile regression losses in Eq. (9) and Eq. (8). Once the minimax expectile regression converges, we replace the original returns-to-go values in the trajectories with the values predicted by \(_{}\). Finally, we train the Decision Transformer (DT) conditioned on the states and relabeled returns-to-go to predict the protagonist's actions. Then the ARDT protagonist policy is ready to be deployed for evaluation in the adversarial environment directly.

## 4 Experiments

In this section, we conduct experiments to examine the robustness of our algorithm, Adversarially Robust Decision Transformer (ARDT), in three settings: (i) Short-horizon sequential games, where the offline dataset has full coverage and the test-time adversary is optimal (Section 4.1), (ii) A long-horizon sequential game, Connect Four, where the offline dataset has only partial coverage and the distributional-shifted test-time adversary (Section 4.2), and (iii) The standard continuous Mujoco tasks in the adversarial setting and a population of test-time adversaries (Section 4.3). We compare our algorithm with baselines including Decision Transformer and ESPER. The implementation details are in Appendix C. Notably, since the rewards and states tokens encompass sufficient information about the adversary's actions, all DT models are implemented not conditioned on the past adversarial tokens to reduce the computational cost.

### Full Data Coverage Setting

The efficacy of our solution is first evaluated on three short-horizon sequential games with adaptive adversary: (1) a single-stage game, (2) an adversarial, single-stage bandit environment depicting a Gambling round , and (3) a multi-stage game. These are depicted in Figure 1, Figure 7, and Figure 9, respectively. The collected data consists of \(10^{5}\) trajectories, encompassing all possible trajectories. The online policies for data collection employed by the protagonist and adversary were both uniformly random.

Figure 3 illustrates the efficacy of ARDT in comparison to vanilla DT and ESPER. Across all three environments, ARDT achieves the return of maximin (Nash Equilibrium) against the optimal adversary, when conditioned on a large target return. As illustrated in Figure 1, ARDT is aware of the worst-case return associated with each action it can take due to learning to condition on the relabeled

minimax returns-to-go, and successfully takes robust action \(a_{1}\). In single-stage game (Figure 3), ESPER is vulnerable to the adversarial perturbations, and DT fails to generate robust policy when conditioned on large target return. While, it is worth noting that DT has achieved worst-case return \(1\) when conditioning on target returns around \(1\). _This imposes the question: Can we learn the robust policy by simply conditioning on the largest worst-case return in testing with vanilla DT training?_

We show via the Gambling environment in Figure 3 that _vanilla DT can still fail even conditioning on the largest worst-case return_. In this environment, the worst-case returns of three protagonist actions are \(-15\), \(-6\) and \(1\). At the target return \(1\), DT is unable to consistently reach a mean return of \(1\). Similar to DT in the single-stage game, it tends to assign equal likelihood to \(a_{1}\) and \(a_{2}\) to reach return \(1\), leading to a drop of performance to less than \(1\) under adversarial perturbations. Therefore, simply conditioning on the robust return-to-go in testing is insufficient to generate robust policy with vanilla DT. Moreover, as the target return approaches \(5\), DT's performance drops more since DT tends to choose \(a_{0}\) more often to hopefully achieve a return of \(5\), but instead get negative return against the adversarial perturbations.

Conversely, ARDT's policy is robust since it is trained with worst-case returns-to-go. ESPER also performs well in Gambling due to that in this game the robust action is also the one with the largest expected return (Figure 9). ARDT also attains the highest return in a multi-stage game when conditioned on large target return, while other two algorithms fail. Consequently, in the context of full data coverage and an adaptive adversary, ARDT is capable of approximating the maximin (Nash Equilibrium) robust policy against the optimal adversarial policy, whereas DT is unable to do so.

### Discrete Game with Partial Data Coverage

_Connect Four_ is a discrete sequential two-player game with deterministic dynamics , which can also be viewed as a decision-making problem when facing an adaptive adversary. There is no

Figure 4: Average return of ARDT and vanilla DT on _Connect Four_ when trained on suboptimal datasets collected with different levels of optimality for both the online protagonist’s policy (\(30\%\), \(40\%\) and \(50\%\) optimal) and the adversary’s policy (\(10\%\), \(30\%\), \(50\%\) optimal), over \(10\) seeds. We test against a fixed adversary that acts optimally \(50\%\) of the time, and randomly otherwise.

Figure 3: Worst-case return versus target return plot comparing the proposed ARDT algorithm against vanilla DT, on our Single-stage Game (left), Gambling (centre) and our Multi-stage Game (right), over \(10\) seeds.

intermediate reward; rather, there is a terminal reward of either \(-1\), \(0\), or \(1\), meaning lose, tie or win for the first-moving player, respectively. Therefore we fix the target return in Connect Four as \(1\). We fix the protagonist to be the first-move player. This game has a maximum horizon of \(22\) steps. Since we have a solver of this game, we manage to collect the data with \(\)-greedy policy. We define the optimality of an \(\)-greedy policy of protagonist or adversary as \((1-) 100\%\). Each dataset includes a total of \(10^{6}\) number of steps for variable-length trajectories.

The results of training ARDT and DT on suboptimal datasets are presented in Figure 4. To demonstrate the stitching ability of the two methods, datasets collected by suboptimal protagonist and adversarial policies were chosen, namely at \(30\%\), \(40\%\) and \(50\%\) optimality for the protagonist and \(10\%\), \(30\%\), \(50\%\) optimality for the adversary. The figures show that ARDT outperforms DT learning from suboptimal datasets, and more clearly so when tested against more powerful adversaries than the behavior adversary.

We hypothesise that one important factor accounting for the negative returns of DT in the setting in 4 may be the lack of long, winning trajectories in the dataset. To exclude this factor, in Table 1 we mix near-random and near-optimal datasets for training, and test our algorithms against different levels of adversary. Compared to ESPER and DT, ARDT still significantly outperforms DT, which suggests that even in the presence of trajectories with high returns in the data, training DT with original returns-to-go in this setting will limit its ability to extract a powerful and robust policy. In addition, the results show that ESPER outperforms DT, indicating relabeling trajectories with expected return can help the extracted policy be robust to distributionaly-shifted and more powerful test-time adversarial policies. ESPER conditioned on the expected return generates policy overfitted to the behavioral adversarial policy, which can fail under distributional shift. However, ARDT as a worst-case-aware algorithm, can achieve greater robustness at test time.

Therefore, in discrete environment with partial data coverage and adaptive adversary, ARDT is more robust to distributional shift and a more powerful test-time adversary.

### Continuous Adversarial Environments

In this section, we test the algorithms on a specific instance of Adversarial Reinforcement Learning problems, namely Noisy Action Robust MDP (NR-MDP), which was introduced by Tessler et al. . This includes three continuous control problems of OpenAI Gym MuJoCo, Hopper, Walker2D and

   Test-time adversary & 30\% optimal & 50\% optimal & 70\% optimal & 100\% optimal \\  DT & 0.18 (0.09) & -0.28 (0.05) & -0.57 (0.06) & -1 (0.0) \\ ESPER & 0.44 (0.09) & 0 (0.1) & -0.42 (0.05) & **-0.98 (0.01)** \\
**ARDT (ours)** & **0.55 (0.11)** & **0.11 (0.22)** & **0.02 (0.09)** & -1 (0.0) \\   

Table 1: Average returns of ARDT, vanilla DT and ESPER on _Connect Four_ when trained on data mixed from both a near-random (30%, 50%) dataset and a near-optimal (90%, 90%) dataset. Evaluation is done against different optimality levels of adversary.

Figure 5: From left to right, (1) the worst-case return under adversarial perturbations with different weights in Halfcheetah, (2) in Hopper, and (3) average returns of algorithms in environments with different relative mass. The initial target returns in the environments Halfcheetah and Hopper are \(2000\) and \(500\), respectively.

Halfcheetah  involving the adversarial action noise. In NR-MDP, the protagonist and adversary act jointly. Specifically, they exact a total force over one or more parts of the body of the agent in a Mujoco environment, corresponding to the weighted sum of their individual forces (adversary's weight \(\)). Despite no longer being in the discrete setting, the return minimax expectile regression in ARDT is implemented with the raw continuous states and actions input, without any discretization.

To collect the data, we first trained Action Robust Reinforcement Learning (ARRL) online policies on NR-MDP tasks  to create the online protagonist policy and adversary policy for evaluation. Subsequently, the pre-collected data consist of the replayed trajectories removing the ones with low cumulative rewards. We collect the suboptimal trajectories with the \(\)-greedy of the saved ARRL agents, i.e. online protagonist and adversary policies. We then mix them with the pre-collected datasets to create the final training datasets. The proportion of random trajectories to pre-collected trajectories are \(0.01:1\), \(0.1:1\) and \(1:1\), respectively, for datasets categorised as being low, medium, and high randomness. At test time, we evaluate each of our trained ARDT protagonist policies against a population of \(8\) adversarial online policies, each across \(5\) seeds. We sweep over the same set of target returns for all algorithms, and pick the best results.

As shown in Table 4, ARDT has overall better worst-case performance than DT and ESPER. In Hopper, ARDT has significantly higher worst-case return than both ESPER and DT. In Halfcheetah, ARDT has significantly higher worst-case return than DT and competitive results with ESPER. This is despite the much lower data coverage than in the previous settings due to the continuous environment. Low data coverage is in theory a greater limitation for ARDT given that it attempts to learn a single best worst-case outcome, rather than an expectation over outcomes. Even so, our proposed solution outperforms the baselines in most settings across all three environments. In addition, ARDT also has better performance when varying the adversary's weight \(\) and relative mass (Figure 5), implying the robustness of ARDT to test-time environmental changes. Therefore, in the context of a continuous state and action space, and when facing a diverse population of adversaries or environmental changes, ARDT trained with minimax returns-to-go can still achieve superior robustness than our baselines.

### Ablation Study

Finally, we offer ablation study on the expectile level \(\) in discrete environments. Specifically, we varied \(\) for relabeling in Single-stage Game, Multi-stage Game and Connect Four. Then we tested against the optimal adversary in the first two environments and an \(\)-greedy adversary (\(=0.5\)) for the third environment.

According to the first two environments, we can confirm a pattern consistent with our theory that smaller alpha leads to more robustness. According to the Equation (6)-(9) in the paper, it should

    \\
**Datasets** & **DT** & **ESPER** & **ARDT** \\  Hopper-lr & 112.1 (63.4) & 66.8 (5.5) & **477.9 (219.9)** \\ Hopper-mr & 330.2 (286.8) & 103.2 (11.7) & **482.2 (83.7)** \\ Hopper-hr & **298.62 (319.72)** & 63.0 (11.5) & **331.69 (187.18)** \\  Walker2D-lr & **429.6 (31.9)** & **380.6 (156.9)** & **405.6 (39.8)** \\ Walker2D-mr & 391.0 (20.6) & 426.9 (29.7) & **508.4 (61.8)** \\ Walker2D-hr & 413.22 (54.69) & **502.6 (31.3)** & **492.63 (109.28)** \\  Halfcheetah-lr & 1416.0 (48.5) & **1691.8 (63.0)** & 1509.2 (33.8) \\ Halfcheetah-mr & 1229.3 (171.1) & **1725.6 (111.8)** & **1594.6 (137.5)** \\ Halfcheetah-hr & 1493.03 (32.87) & **1699.8 (32.8)** & **1765.71 (150.63)** \\   

Table 2: Results of DT, ESPER, and our algorithm ARDT with and without conditioned on past adversarial tokens: the **Worst-case Return** against \(8\) different online adversarial policies on MuJoCo _Noisy Action Robust MDP_ tasks across \(5\) seeds. We tested with the same set of target returns and select the best target for each method. To expand the data coverage, we inject random trajectories collected by \(0.1\)-greedy online policies into the pre-collected online datasets with high robust returns. We use low (suffix -lr), medium (suffix -mr) and high randomness (suffix -hr) to indicate the proportion of random trajectories.

be that when the \(\) is closer to 0, our estimate of minimax return is more accurate, and thus our algorithm is more robust. When the alpha is increased to 0.5, our method is converted to Expected-Return-Conditioned Decision Transformer (e.g. ESPER) since the expectile loss in this case reduces to Mean Square Error. This can be confirmed by comparing the results of Single-stage game and Multi-stage game in Figure 6 and 3.

In the third environment, Connect Four, the performance initially increases as decreases, but eventually drops. This is due to the weak stochastic adversary. When the value is too small, the algorithm becomes overly conservative. However, ARDT still outperforms DT significantly. Additionally, it also implies that we can tune the parameter alpha to adjust the level of conservativeness

## 5 Conclusions and Limitations

This paper introduces a worst-case-aware training algorithm designed to improve the adversarial robustness of the Decision Transformer, namely Adversarially Robust Decision Transformer (ARDT). By relabeling trajectories with the estimated in-sample minimax returns-to-go through expectile regression, our algorithm is demonstrated to be robust against adversaries more powerful than the behavior ones, which existing DT methods cannot achieve. In experiments, our method consistently exhibits superior worst-case performance against adversarial attacks in both gaming environments and continuous control settings.

Limitations.In our experiments, our practical algorithm estimates the minimax returns-to-go without accounting for stochasticity, as the transitions in all tested environments, as well as many real-world game settings, are deterministic when conditioned on the actions of both players. Future work should explore both stochastic and adversarial environments to further evaluate the performance of our proposed solution.

Broader Impact.Adversarial RvS holds potential for improving the robustness and security of autonomous agents in dynamic and potentially hostile environments. By training agents to withstand adversarial actions and unpredictable conditions, our work can improve the reliability and safety of technologies such as autonomous vehicles, cybersecurity defense systems, and robotic operations.

## 6 Acknowledgments

Ilija Bogunovic was supported by the EPSRC New Investigator Award EP/X03917X/1; the Engineering and Physical Sciences Research Council EP/S021566/1; and Google Research Scholar award. Xiaohang Tang was supported by the Engineering and Physical Sciences Research Council [grant number EP/T517793/1, EP/W524335/1]. The authors would like to thank the Department of Statistical Science, in particular Chakkapas Visavakul, for co-ordinating the computer resources, and the Department of Electronic and Electrical Engineering, and Department of Computer Science at University College London for providing the computer clusters.

Figure 6: **Ablation study** on expectile level \(\) over \(10\) random seeds. In connect four, we set the target return to be \(1\), and the adversary to be \(\)-greedy (\(=0.5\)), i.e. taking uniform random policy with probability \(0.5\). On Connect Four, ARDT with \(=0.01\) acts too conservative.