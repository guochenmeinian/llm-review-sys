# AverNet: All-in-one Video Restoration for

Time-varying Unknown Degradations

 Haiyu Zhao\({}^{1}\), Lei Tian\({}^{2}\), Xinyan Xiao\({}^{2}\), Peng Hu\({}^{1}\), Yuanbiao Gou\({}^{1}\), Xi Peng\({}^{1,3}\)

\({}^{1}\)College of Computer Science, Sichuan University, China.

\({}^{2}\)Baidu Inc., Beijing, China.

\({}^{3}\)State Key Laboratory of Hydraulics and

Mountain River Engineering, Sichuan University, China.

{haiyuzhao.gm, penghu.ml, gouyuanbiao, pengx.gm}@gmail.com

{tianlei09, xiaoxinyan}@baidu.com

Corresponding Authors

###### Abstract

Traditional video restoration approaches were designed to recover clean videos from a specific type of degradation, making them ineffective in handling multiple unknown types of degradation. To address this issue, several studies have been conducted and have shown promising results. However, these studies overlook that the degradations in video usually change over time, dubbed time-varying unknown degradations (TUD). To tackle such a less-touched challenge, we propose an innovative method, termed as All-in-one VidEo Restoration Network (AverNet), which comprises two core modules, _i.e._, Prompt-Guided Alignment (PGA) module and Prompt-Conditioned Enhancement (PCE) module. Specifically, PGA addresses the issue of pixel shifts caused by time-varying degradations by learning and utilizing prompts to align video frames at the pixel level. To handle multiple unknown degradations, PCE recasts it into a conditional restoration problem by implicitly establishing a conditional map between degradations and ground truths. Thanks to the collaboration between PGA and PCE modules, AverNet empirically demonstrates its effectiveness in recovering videos from TUD. Extensive experiments are carried out on two synthesized datasets featuring seven types of degradations with random corruption levels. The code is available at https://github.com/XLearning-SCU/2024-NeurIPS-AverNet.

## 1 Introduction

Video restoration aims to recover a high-quality video from a low-quality one that is corrupted by degradations such as noise, blur, and compression artifacts. Over the past few years, numerous studies  have been conducted, yielding promising performance in video restoration. However, these methods typically require prior knowledge of the specific type of degradation to design and train effective models, such as denoising, dehazing, and deblurring models . In practice, obtaining such prior knowledge in advance is challenging, especially when the data is affected by multiple unknown types of degradation.

In the field of image restoration, efforts  have been made to restore images affected by multiple unknown types of degradation using a unified model, which is known as all-in-one image restoration (AIR) . However, it is hard to achieve encouraging performance by simply applying existing AIR methods to videos due to the neglect of temporal information, which leads to inferior performance, as verified in our experiments. To favor all-in-one video restoration (AVR), a fewstudies [17; 18; 19] have been conducted in very recent, which typically assume that the frames of a given video contain unknown but the same type of degradation. In other words, these methods can only handle a single unknown type of degradation and would fail when faced with multiple unknown types of degradation for a given video. Clearly, the latter is more practical and challenging because degradations can vary over time in real-world scenarios (Fig. 1). For instance, motion blur can occur when a stationary object starts to move, and noise will appear when the scene changes to low-light conditions. Therefore, it is highly desirable to develop an AVR method capable of handling time-varying unknown degradations (TUD).

To overcome the aforementioned challenge of TUD, we propose an All-in-one VidEo Restoration Network (AverNet), which consists of two core modules, _i.e._, Prompt-Guided Alignment (PGA) module and Prompt-Conditioned Enhancement (PCE) module. To recover videos from time-varying degradations, PGA is designed based on the following observation. Specifically, a vital step in video restoration is spatial alignment across frames, which aims to eliminate frame difference at the pixel level. As shown in Fig. 2, compared to time-invariant degradations, time-varying degradations cause larger and more complex pixel shifts, making frame alignment significantly more challenging. Hence, to align frames at pixel level better, PGA optimizes a specific prompt conditioned on a video clip first and then uses this prompt to align the frames within the clip. To tackle unknown degradations in a given frame, PCE is designed to learn prompts corresponding to latent unknown degradations by building the conditional map between the corrupted frame and the ground truth. Through treating the prompts as the conditions, PCE transforms the task of video restoration from multiple unknown degradations into a known conditional restoration problem.

To summarize, the contributions of this work are as below:

* As far as we know, this work could be the first study on the TUD. Different from previous efforts on time-invariant degradation, our method is suitable for more practical scenarios wherein the types and levels of degradations are both unknown and changing over time.
* We show that the TUD challenge could be effectively addressed by our AverNet. In brief, AverNet employs the PGA module to address the issue of the large and complex pixel shifts and the PCE module to solve the problem of multiple unknown degradations.
* Although the TUD setting is practical, the data with ground-truth is scarce and even impossible to collect. To address the data scarcity issue, we develop a data synthesis approach to simulate the data of TUD in real world. Experiments on two datasets across seven types of degradations with random levels demonstrate the effectiveness of our AverNet.

## 2 Related Work

In this section, we introduce the related works in video restoration and all-in-one image restoration, and elaborate on the differences between our AverNet and the existing methods.

Figure 1: Illustration of classic and all-in-one video restoration. (a) aims to develop a specific model for each degradation to handle the corrupted video, assuming that the degradation types and levels are the same and known for all frames. In contrast, (b) intends to handle videos containing time-varying unknown degradations through a unified model, which is more practical and challenging.

### Video Restoration

Currently, deep video restoration methods could be roughly divided into two categories according to their architectures, _i.e._, sliding window-based methods [1; 5; 20; 21; 22; 23; 19] and recurrent methods [24; 25; 26; 2; 4]. The former kind of methods typically take a short sequence of frames as input and restore only the center frame. By leveraging the temporal information from adjacent frames, these methods have achieved promising performance. For example, EDVR  introduced the pyramid architecture and temporal-spatial attention modules to effectively aggregate information across frames. Shift-Net  proposed an efficient framework based on grouped spatial-temporal shift modules to implicitly aggregate inter-frame information. Although the methods have shown impressive performance, they encounter challenges in handling long-sequence videos and suffer from high memory consumption. To alleviate the problem, recurrent methods choose to propagate latent features from one frame to the next frame sequentially, accumulating information from previous frames for the restoration of subsequent frames. For instance, BasicVSR  proposed a concise and efficient network employing a bidirectional propagation scheme. BasicVSR++  improved the bidirectional scheme with second-order grid connections to implement an effective recurrent model.

Different from the above methods focusing on a specific degradation, ViWS-Net , CUDN  and Diff-TTA  explored a more generalized task of AVR, which aims to handle multiple unknown degradations through a unified model. Specifically, ViWS-Net introduces degradation messenger tokens to learn specific degradation information and employs them to guide the restoration. CUDN adaptively estimates the features of unknown degradations and employs the estimation to guide the model to remove diverse degradations. Diff-TTA introduces test-time adaptation techniques  to adapt the distribution of test data and recalibrates the parameters of pre-trained models to address unknown degradations. Although there is a similarity between AverNet and these methods in the task of AVR, they are remarkably different in both problem and solution. In problem, ViWS-Net, CUDN and Diff-TTA considered the unknown types of degradations, while AverNet considers not only the unknown types and levels of degradations, but also their variations over time, which is more practical and challenging. In solution, they follow the paradigm of sliding window-based methods and employ local information for restoration. In contrast, AverNet improves the propagation scheme of the recurrent paradigm to address TUD challenge, enabling it to effectively utilize global temporal information for AVR.

### All-in-one Image Restoration

Some recent works [13; 28; 29; 30; 31] have been devoted to all-in-one image restoration (AIR), which aims to recover images from multiple unknown types of degradations using a single model. The key to these methods is extracting discriminative information for different degradations, and employing this information to guide a single model for performing AIR. For example, AirNet  learns the degradation representations through contrastive learning, and uses the representations to guide the restoration network. PromptIR  proposes to use learnable prompts to encode

Figure 2: Illustration of the pixel shift issue. We compute the optical flow between two consecutive frames with time-invariant and time-varying degradations. Several directional vectors are visualized as red arrows to indicate the estimated pixel alignments between the two frames. One could observe that time-varying degradations lead to less accurate estimations compared to time-invariant degradations, causing a larger and more complex pixel shift after alignment.

degradation-specific information first and then guide the restoration of clean images. ProRes  introduces additional visual prompts to incorporate task-specific information and utilize the prompts to guide the network for AIR. MPerciver  proposes a multimodal prompt learning approach to exploit the generative priors of Stable Diffusion  to achieve high-fidelity all-in-one image restoration. AutoDIR  introduces a CLIP-based module to detect unknown degradations, and instructs a latent-diffusion-based module with a text prompt for restoration. In addition to AIR, a more recent work TAO  has explored a challenging task of open-set image restoration, which aims to handle unknown degradations that were unforeseen in the pretraining phase.

Unlike the above methods specialized for image restoration, our AverNet is devoted to all-in-one video restoration, which aims to recover videos affected by time-varying unknown degradations using a single model. In contrast to AIR focusing on unknown degradations, our AVR is more practical and challenging since the degradations are both unknown and time-varying.

## 3 Proposed Method

In this section, we elaborate on our AverNet and two modules, _i.e._, Prompt-Guided Alignment module (PGA) and Prompt-Conditioned Enhancement module (PCE).

The architecture of our AverNet is depicted in Fig. 3. Given an input video, residual blocks are first applied to extract shallow features from each frame. Then, the features are sequentially propagated four times across the video sequence to incorporate temporal information. Specifically, the first and third times of propagation are backward, in which the features are propagated from the last frame to the first one. The second and fourth times of propagation are forward, in which the features are propagated from the first frame to the last one. Such an alternating bidirectional propagation enables the features of each frame to involve abundant information from the whole video sequence for restoration. After propagation, the features are used to generate the final output for each frame through convolutions and pixel-shuffling  blocks. In the following, we will detail our core designs, _i.e._, PGA and PCE, for solving TUD challenge during the propagation process.

Figure 3: Architecture overview. (a) Overall architecture of our AverNet, which is mainly composed of propagation blocks. Each block consists of a (b) PGA module for spatially aligning features across frames with time-varying degradations, and a (c) PCE module for enhancing the features of current frame with unknown degradations. (d) PGI modules endow PGA and PCE with the capacity of conditioning on degradations by means of input-conditioned prompts. For simplicity, the superscripts \(j\) in (b) are omitted. In (c), the past feature is from the last time of propagation, and \(I_{k}\) refers to the indices of key frames.

### Prompt-Guided Alignment

To address the pixel shift issue and obtain aligned features from the video with time-varying degradations, PGA employs deformable convolution network  (DCN) to align the spatial contents between frames. The calculation of DCN depends on two parameters of offsets and masks, which indicate spatial coordinates shifts and weights, respectively. Typically, the offsets and masks are estimated from the frame features and optical flow  through convolutions. However, the time-varying degradations lead to inaccurate estimations and deteriorate the spatial alignments. Therefore, we further introduce the input-conditioned prompts that encode degradation information to guide the estimation. The graphical illustration is shown in Fig. 3(b). In the rest of this section, we will detail the alignment procedure in the forward propagation, which is similar in the backward propagation. For simplicity, we omit the superscript \(j\) which indicates the \(j\)-th time of propagation.

Given the shallow features \(l_{i}\) of the \(i\)-th frame, the previous features \(f_{i-1}\) from \((i-1)\)-th frame, and the optical flow \(v_{(i-1) i}\) from the \((i-1)\)-th to \(i\)-th frame, we first warp \(f_{i-1}\) through \(v_{(i-1) i}\):

\[_{i-1}=(f_{i-1},v_{(i-1) i}),\] (1)

where \(\) denotes the spatial warping function. Then, we concatenate the shallow features \(l_{i}\) and the warped features \(_{i-1}\) into a clip \(_{i}\) to estimate the offsets and masks of DCN. Instead of directly estimating them through a simple network, we introduce prompts to encode degradation-specific information conditioned on the clip, and integrate prompts into the clip to guide the alignment of the frames. Specifically, the integrated features \(g_{i}\) are generated by a convolution followed by a Prompt Generation & Integration (PGI) block (Fig. 3(d)):

\[g_{i}=PGI(_{3 3}(_{i})),\] (2)

where \(_{i}=Concat(l_{i},_{i-1})\) and \(Concat()\) refers to channel concatenation. Specifically, PGI obtains the integrated features by generating input-conditioned prompts and fusing them with the input features. Taking \(x\) as the input features, PGI first predicts attention-based weights \(W^{N}\) and then applies them to prompt embeddings \(E^{N}\) to obtain input-conditioned prompts \(P\). To be specific, PGI sequentially applies global average pooling (GAP), \(1 1\) convolution, and softmax operation on \(x\) to obtain the weights \(W\):

\[W=(_{1 1}((x))).\] (3)

After that, \(P\) is calculated as the weighted sum of \(E\) followed by \(3 3\) convolution for refinement:

\[P=_{3 3}(_{n=1}^{N}W_{n} E_{n}).\] (4)

Note that \(P\) has a fixed spatial size \(\), which may be different from the size of frame features. Therefore, we apply the bilinear upsampling operation to upscale \(P\) to the same size as the input features. Finally, the prompts are integrated into the input features \(x\) through:

\[g_{i}=(Concat(x,P)),\] (5)

where \(()\) denotes residual blocks. After obtaining the integrated features \(g_{i}\), we combine them with the input \(_{i}\) and refine them using a \(3 3\) convolution:

\[h_{i}=_{3 3}(Concat(_{i},g_{i})),\] (6)

where \(h_{i}\) is the intermediate features to compute DCN parameters. Finally, the DCN offsets \(o_{(i-1) i}\) and modulation masks \(m_{(i-1) i}\) are calculated as:

\[o_{(i-1) i} =v_{(i-1) i}+^{o}(h_{i}),\] (7) \[m_{(i-1) i} =(^{m}(h_{i})),\]

where \(R^{\{o,m\}}\) denotes a stack of convolutions, and \(\) denotes the sigmoid function. A DCN is then applied to the previous feature \(f_{i-1}\) for spatial alignment:

\[_{i}=(f_{i-1};o_{(i-1) i},m_{(i-1) i}),\] (8)

where \(()\) denotes a deformable convolution network.

### Prompt-Conditioned Enhancement

Here, we detail PCE module in the forward propagation, which is similar in the backward propagation. To handle multiple unknown degradations, PCE employs the prompts corresponding to the latent unknown degradations as the conditions to enhance the current frame features during propagation. However, the prompt extraction and conditional enhancement require additional computations which are non-trivial. To reduce the computational cost, PCE only conducts enhancement at selected key frames and collaterally enhances the features of other frames through propagation. Specifically, key frames are sparsely selected based on a fixed interval \(T\). The indices of key frames can be expressed as a list \(I_{k}\), which is an arithmetic sequence with a common difference of \(T\). During the propagation of key frames, PCE enhances the features based on the aligned features \(_{i}^{j}\) and the shallow features \(l_{i}\) of key frames.

To obtain the enhanced features \(f_{i}^{j}\), PCE first employs PGI to extract the key frame prompt \(k_{i}\) based on the shallow features \(l_{i}\) according to the procedures shown in Fig. 3(d). Then, the key frame prompt is used as the condition to enhance aligned features \(_{i}^{j}\) through a few residual blocks:

\[_{i}^{j}=(Concat(_{i}^{j},k_{i})), i I_ {k},\] (9)

where \(()\) denotes residual blocks, \(Concat()\) denotes concatenation along channel dimension, and \(I_{k}\) refers to the list of indices of key frames. After that, the enhanced features are fused with the propagation features of last time for the subsequent propagation:

\[f_{i}^{j}=_{i}^{j}+(Concat(f_{i}^{j-1},_{i }^{j})), i I_{k},\] (10)

where \(f_{i}^{j}\) and \(f_{i}^{j-1}\) are the propagation features of current and last time at the \(i\)-th frame, respectively.

In the cases without enhancement, _i.e._, the frame index \(i I_{k}\), the aligned features are directly concatenated with the propagation features of last time and then refined through residual blocks to obtain propagation features \(f_{i}^{j}\):

\[f_{i}^{j}=_{i}^{j}+(Concat(f_{i}^{j-1},_{i}^{j})),  i I_{k}.\] (11)

Note that \(f_{i}^{0}=l_{i}\) for the first time of propagation.

## 4 Experiment

In this section, we conduct experiments to evaluate our AverNet. In the following, we introduce the experimental settings first and then show quantitative and qualitative results. Finally, we conduct ablation analyses to demonstrate the effectiveness of our designs.

### Experiment Settings

**Video Synthesis Approach.** Although the TUD setting is practical, the data with ground-truth is scarce and even impossible to collect. To tackle this issue, we develop a data synthesis approach to simulate such pairs based on the degradation models in [39; 40]. To be specific, a clean video is first cut into multiple clips with a fixed interval \(t\). Then, for each clip, a series of degradations are sampled from seven candidate degradations with a probability of \(0.55\). Specifically, the candidate degradations include Poisson noise, Gaussian noise, speckle noise, resizing blur, Gaussian blur, JPEG compression, and video compression. The details for each degradation are provided in the appendix. After that, the degradations are shuffled and added to the clip to produce a corrupted one. Finally, the corrupted clips are assembled into a corrupted video that contains time-varying unknown degradations.

**Video Datasets.** We adopt two widely-used video datasets in experiments, _i.e._, DAVIS  and Set8 . Specifically, DAVIS contains 90 training sequences and 30 test sequences of resolution \(854 480\). Set8 has 8 test video sequences of resolution \(960 540\). We train all models on DAVIS training set, and test them on DAVIS-test and Set8. The training and test pairs are constructed through the above video synthesis approach. For the training set, the interval \(t\) of degradation variations is set to \(6\) and the degradations are the above seven degradations including three types of noise, two types of blur, and two types of compression. For the test set, we apply different settings to generate diverse sets for full evaluations, which are detailed in Sec. 4.2.

**Implementation Details.** We use the same settings for all experiments. To be specific, the number of channels is set to 96, and the embedding length, dimension, and size of prompts are set to 5, 96, and 96\(\)96, respectively. For optical flow estimation, we use the pre-trained SPyNet [43; 44] whose parameters and runtime are included in our AverNet.

**Training Details.** The experiments are conducted in PyTorch  framework with four NVIDIA GeForce RTX 3090 GPUs. For training, we use Charbonnier loss  and Adam  optimizer with \(_{1}=0.9\) and \(_{2}=0.999\). The initial learning rates of main and optical flow networks are set to \(1e^{-4}\) and \(2.5e^{-5}\), respectively, which are gradually decreased to \(1e^{-7}\) through cosine annealing strategy . The number and resolution of input frames are set to 12 and \(256 256\), respectively. We train the networks with the batch size of 1 for 600K iterations, in which the parameters of optical flow network will not be updated for the first 5K iterations.

### Comparison Experiments

In this section, we compare our AverNet with existing state-of-the-art methods on the video datasets with TUD. To be specific, existing methods include four representative all-in-one image restoration methods and four conventional video restoration methods. The all-in-one image restoration methods are WDiffusion , TransWeather , PromptIR , and AirNet . The video restoration methods are EDVR , BasicVSR++ , Shift-Net , and RVRT . Both the video and image restoration methods were trained on DAVIS training set from scratch according to the training settings in their papers. Note that the image restoration methods were trained and tested on each frame of the video sequences. We present the parameters and runtime of our AverNet and the compared methods in Tab. 1, which are computed according to their original test settings. Specifically, the runtime was computed on a video with 48 frames from DAVIS-test.

To comprehensively evaluate AverNet on dealing with TUD, we conduct experiments on time-varying unknown degradations with different variation intervals and different degradation combinations. To be specific, the variation interval refers to the interval of video degradation variations over time and degradation combinations refer to various types of degradations in the video.

**Evaluation on Different Variation Intervals.** To evaluate the effectiveness in handling TUD, we test all models on six test sets that are generated through the video synthesis approach, where the variation interval is \(t\). Specifically, six test sets are synthesized with the variation intervals \(t=6,12,24\) based on DAVIS-test and Set8, respectively.

As shown in Tab. 2, our method nearly outperforms other methods across all six test sets. For example, our method outperforms RVRT by 0.08dB-0.18dB in PSNR on DAVIS-test and outperforms ShiftNet by up to 0.99dB and 0.65dB on DAVIS-test and Set8, respectively. Additionally, as shown in Tab. 1, our method achieves the best performance while maintaining the highest efficiency. Specifically, compared with our method, the runtime of RVRT is more than double, and that of Shift-Net is over six times longer. In comparison with the all-in-one image restoration methods, our method yields significantly better results, showing the importance of leveraging temporal information in videos. For example, our AverNet outperforms AirNet by at least 1.53dB/0.0428 and 1.02dB/0.0345 in PSNR/SSIM on DAVIS-test and Set8, respectively, while requiring nearly half the runtime.

Fig. 4 and Fig. 5 present the qualitative results on DAVIS and Set8 datasets with interval \(t=12\), respectively. From the figures, one could observe that all-in-one image restoration methods like PromptIR and AirNet yield distorted and blurry results. Furthermore, BasicVSR++ and RVRT exhibit residual noise and artifacts. In contrast, our method excels in recovering structures and capturing finer details, resulting in clearer restorations.

    &  &  & AVR \\   & EDVR & BasicVSR++ & Shift-Net & RVRT & WDiffusion & TransWeather & AirNet & PromptIR & Ours \\  \#Param & 23.6M & 7.4M & 12.9M & 13.6M & 80.0M & 38.1M & 8.9M & 35.6M & 41.3M \\ Runtime & 6.78s & 5.09s & 45.54s & 15.07s & 3956.50s & 3.32s & 12.98s & 10.81s & 6.93s \\   

Table 1: Comparisons on parameters and runtime. From the table, one could observe that our method is more efficient than most AIR methods.

**Evaluation on Different Degradation Combinations.** To evaluate the robustness in the situation with different degradation combinations, we construct new test sets by removing one type of degradation from the seven candidate degradations in the video synthesis. For example, we construct a test set with a new combination of noise and blur degradations by removing JPEG compression and video compression in the seven degradations. As a result, we obtain three different combinations, _i.e._, noise&blur, noise&compression, and blur&compression.

From Tab. 1 and 3, one could observe that our method obtains comparable or even better performance while embracing much higher efficiency. For example, our method outperforms RVRT by 0.34dB and 0.24dB in PSNR on the DAVIS-test and Set8, respectively, with the noise&blur combination. Note that our method only requires less than half the time of RVRT to process the videos. Besides, our method outperforms Shift-Net by at most 1.01dB and 0.47dB in PSNR on DAVIS-test and Set8, respectively. Compared with all-in-one image restoration methods, our method has superior performance in every degradation combination on the test sets.

### Ablation Experiments

To investigate the effectiveness of our AverNet, we conduct ablation experiments on the core modules, namely, PGA and PCE. All experiments were conducted on the test sets with \(t=12\).

To verify the effectiveness of PGA and PCE, we replace each of them with conventional propagation and alignment modules, respectively, while excluding the guidance and the condition from prompts. The results are presented in Tab. 4. In the table, it is apparent that each component brings considerable improvement, with PSNR gains ranging from 0.16dB to 2.15dB on the two test sets. Specifically,

    &  &  \\   &  &  &  &  &  &  \\   & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\  WDiffusion & 31.74 & 0.8768 & 31.79 & 0.8784 & 31.92 & 0.8809 & 30.31 & 0.8784 & 30.02 & 0.8716 & 30.82 & 0.8746 \\ TransWeather & 31.11 & 0.8694 & 31.13 & 0.8699 & 31.26 & 0.8741 & 29.24 & 0.8662 & 28.95 & 0.8655 & 29.15 & 0.8632 \\ AirNet & 32.46 & 0.8873 & 32.46 & 0.8887 & 32.75 & 0.8928 & 30.71 & 0.8874 & 30.40 & 0.8806 & 31.16 & 0.8825 \\ PromptIR & 32.18 & 0.8843 & 32.19 & 0.8867 & 32.45 & 0.8900 & 30.79 & 0.8903 & 30.43 & 0.8821 & 31.19 & 0.8847 \\  EDVR & 28.70 & 0.7224 & 28.37 & 0.6991 & 29.07 & 0.7289 & 26.75 & 0.7259 & 26.94 & 0.7382 & 28.71 & 0.7675 \\ BasicVSR+ & 33.22 & 0.9204 & 33.07 & 0.9180 & 33.32 & 0.9210 & 30.90 & 0.9048 & 30.52 & 0.8965 & 31.35 & 0.9011 \\ Shift-Net & 33.09 & 0.9096 & 33.10 & 0.9113 & 33.34 & 0.9193 & 31.15 & 0.9027 & 30.82 & 0.8947 & 31.88 & 0.9000 \\ RVRT & 33.99 & 0.9314 & 33.98 & 0.9311 & 34.10 & 0.9315 & 31.73 & 0.9192 & 31.39 & 0.9113 & **32.47** & 0.9178 \\ AverNet (Ours) & **34.07** & **0.9333** & **34.09** & **0.9339** & **34.28** & **0.9356** & **31.73** & **0.9219** & **31.47** & **0.9145** & 32.45 & **0.9189** \\   

Table 2: Quantitative results compared to state-of-the-art methods on test sets with various variation intervals. \(t\) is the interval of degradation variations in the frame sequences. The best outcomes are highlighted in **bold**. From the table, one could observe that our method almost outperforms other methods on all test sets.

Figure 4: Qualitative results on the “tractor” video from DAVIS-test (\(t=12\)), from which one could observe that existing methods leave residual noise or artifacts in the results. In contrast, our method obtains clearer results that are closer to GT.

the model without PCE suffers from PSNR drops of 1.10dB and 1.67dB on DAVIS-test and Set8, respectively. Similarly, the model without PGA exhibits PSNR drops of 1.50dB and 1.33dB on the two test sets. Notably, as Set8 contains longer videos, the propagation errors caused by multiple unknown degradations are more serious. By enhancing the propagated features of key frames, PCE effectively mitigates these errors and makes significant contributions to the final performance. Additionally, the model without both PGA and PGE shows a significant PSNR drops of 1.66dB and 3.48dB, highlighting the effectiveness of prompt guidance and prompt conditioning.

To investigate the influences of intervals between key frames, we change \(T\) from 6, 12, to 24 and present the results in Tab. 5. One could observe that PCE with varied \(T\) obtains similar results on

    & PGA & PCE &  &  \\   & & & PSNR & SSIM & PSNR & SSIM \\  (A) & & & 32.43 & 0.8910 & 27.99 & 0.8404 \\ (B) & & ✓ & 32.59 & 0.9157 & 30.14 & 0.8958 \\ (C) & ✓ & & 32.99 & 0.9156 & 29.80 & 0.8755 \\ (D) & ✓ & ✓ & 34.09 & 0.9339 & 31.47 & 0.9145 \\   

Table 4: Ablation studies of the modules. Each module brings improvements in PSNR and SSIM, verifying their effectiveness.

Figure 5: Qualitative results on the “touchdown” video from Set8 (\(t=12\)), from which one could see that existing methods yield blurry or distorted results. In contrast, the results of our method have sharper outlines and less artifacts.

    &  &  \\  Method &  & Noise \& Comp. & Blur \& Comp. & Noise \& Blur & Noise \& Comp. & Blur \& Comp. \\   & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\  WDiffusion & 32.70 & 0.8990 & 33.52 & 0.9124 & 33.76 & 0.9142 & 31.64 & 0.8943 & 30.88 & 0.8968 & 31.22 & 0.8978 \\ TransWeather & 31.74 & 0.8863 & 32.53 & 0.9062 & 32.18 & 0.9017 & 29.74 & 0.8714 & 29.93 & 0.8886 & 29.61 & 0.8830 \\ AirNet & 33.41 & 0.9078 & 34.23 & 0.9184 & 34.59 & 0.9224 & 0.15 & 0.9065 & 31.27 & 0.9019 & 31.60 & 0.9027 \\ PromptHR & 33.69 & 0.9128 & 34.18 & 0.9213 & 33.87 & 0.9179 & 32.10 & 0.9033 & 31.54 & 0.9016 & 31.53 & 0.9047 \\  EDVR & 28.00 & 0.6809 & 29.58 & 0.7036 & 34.17 & 0.9082 & 27.82 & 0.7268 & 27.23 & 0.7245 & 32.15 & 0.8845 \\ BasicVSR++ & 33.89 & 0.9324 & 34.72 & 0.9391 & 34.82 & 0.9392 & 31.88 & 0.9189 & 31.42 & 0.9152 & 31.35 & 0.9146 \\ ShiftNet & 34.00 & 0.9277 & 34.91 & 0.9390 & 35.26 & 0.9376 & 32.66 & 0.9159 & 31.86 & 0.9184 & 32.08 & 0.9164 \\ RWRT & 34.67 & 0.9438 & 35.69 & 0.9504 & **35.94** & 0.9503 & 32.70 & 0.9291 & **32.40** & 0.9297 & 32.38 & 0.9291 \\ AverNet (Ours) & **35.01** & **0.9468** & **35.89** & **0.9531** & 35.87 & **0.9506** & **32.94** & **0.9326** & 32.33 & **0.930** & 32.37 & **0.9306** \\   

Table 3: Quantitative results compared to state-of-the-art methods in three degradation combinations, _i.e._, noise&blur, noise&compression, and blur&compression. The best outcomes are highlighted in **bold**. From the table, one could see that our method outperforms other methods in SSIM, and obtains comparable PSNR values to RVRT while requiring only half the runtime.

DAVIS-test while PCE with larger \(T\) suffers from 0.45dB drop in PSNR on Set8. This is because the Set8 contains much longer videos compared to DAVIS-test. Consequently, the accumulated propagation errors on Set8 are more serious. This result further shows the importance of our PCE module. According to the results, we find \(T=6\) is a suitable value and set the interval of key frames to \(6\) in all experiments.

## 5 Conclusions

In this paper, we study a practical and challenging problem in video restoration, _i.e._, time-varying unknown degradations. To solve the problem, we propose AverNet which could recover clean video from the corrupted ones with TUD. Different from existing video restoration methods, AverNet assumes the degradations are time-varying and could handle TUD without the prior of degradation. Extensive experimental results show the superiority of AverNet in both quantitative and qualitative comparisons.

## 6 Acknowledgment

This work was supported in part by the Fundamental Research Funds for the Central Universities under Grant CJ202303; in part by NSFC under Grant U21B2040; in part by Sichuan Science and Technology Planning Project under Grant 2024NSFTD0038.