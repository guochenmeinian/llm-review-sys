# Erasing Undesirable Concepts in Diffusion Models

with Adversarial Preservation

Anh Bui

Monash University

Long Vuong

Monash University

Khanh Doan

VinAI Research

Trung Le

Monash University

Paul Montague

Defence Science and Technology Group, Australia

Tamas Abraham

Defence Science and Technology Group, Australia

Dinh Phung

Monash University

###### Abstract

Diffusion models excel at generating visually striking content from text but can inadvertently produce undesirable or harmful content when trained on unfiltered internet data. A practical solution is to selectively removing target concepts from the model, but this may impact the remaining concepts. Prior approaches have tried to balance this by introducing a loss term to preserve neutral content or a regularization term to minimize changes in the model parameters, yet resolving this trade-off remains challenging. In this work, we propose to identify and preserving concepts most affected by parameter changes, termed as _adversarial concepts_. This approach ensures stable erasure with minimal impact on the other concepts. We demonstrate the effectiveness of our method using the Stable Diffusion model, showing that it outperforms state-of-the-art erasure methods in eliminating unwanted content while maintaining the integrity of other unrelated elements. Our code is available at https://github.com/tuananhbui89/Erasing-Adversarial-Preservation.

## 1 Introduction

Recent advances in text-to-image diffusion models (Rombach et al., 2022; Ramesh et al., 2021, 2022) have captured significant attention thanks to their outstanding image quality and boundless creative potential. These models undergo training on extensive internet datasets, enabling them to capture a wide range of concepts, which inevitably include undesirable concepts such as racism, sexism, and violence. Hence, these models can be exploited by users to generate harmful content, contributing to the proliferation of fake news, hate speech, and disinformation (Rando et al., 2022; Qu et al., 2023; Westerlund, 2019). Removing these undesirable contents from the model's output is thus a critical step in ensuring the safety and usefulness of these models.

Addressing this challenge, several methods have been proposed to erase undesirable concepts from pretrained text-to-image models, such as TIME (Orgad et al., 2023), UCE (Zhang et al., 2023), Concept Ablation (Kumari et al., 2023), and ESD (Gandikota et al., 2023). Despite differing approaches, these methods reach a common finding: removing even one concept can significantly reduce the model's ability to generate other concepts. This is because large-scale generative models \(:\), such as Stable Diffusion (StabilityAI, 2022), are trained on billions of image-text pairs \((x,c)\), where \(x\) is an image and \(c\) is its caption, implicitly containing a set of concepts. The concept space is thus vast and intricately entangled within the model's parameters, meaning no specific part of the model's weights is solely responsible for a single concept. Consequently, the removal of one concept alters the entire model's parameters, causing a decline in overall performance. To address this degradation, existing methods typically select a neutral concept, such as "a photo" or an empty string, as an anchor to preserve while erasing the target concept, expecting that maintaining the neutral concept should help retain other concepts as well Orgad et al. (2023); Gandikota et al. (2024).

While choosing a neutral concept is reasonable, we argue that it is not the optimal choice and may not guarantee the preservation of the model performance. In this paper, we propose to shift the attention towards the _adversarial concepts_, those most affected by changes in model parameters. This approach ensures that erasing unwanted content is stable and minimally impacts other concepts. To summarize, our key contributions are two-fold:

* We empirically investigate the impact of unlearning the target concept on the generation of other concepts. Our findings show that erasing different target concepts affects the remaining ones in various ways. This raises the question of whether preserving a neutral concept is sufficient to maintain the model's capability. We discover that the neutral concept lies in the middle of the sensitivity spectrum, whereas related concepts such as "person" and "women" are more sensitive to the target concept "nudity" than many neutral concepts. Additionally, we demonstrate that selecting the appropriate concepts to preserve significantly improves quality retention.
* We propose a novel method to identify the most sensitive concepts corresponding to the concept targeted to be erased, and then preserve these sensitive concepts explicitly to maintain the model's capability. We then conduct extensive experiments that demonstrate that the proposed method consistently outperforms other approaches in various settings.

## 2 Background of Text-to-Image Diffusion Models

Denoising Diffusion Models:Generative modeling is a fundamental task in machine learning that aims to approximate the true data distribution \(p_{}\) from a dataset \(=\{_{i}\}_{i=1}^{N}\). Diffusion models, a recent class of generative models, have shown impressive results in generating high-resolution images (Ho et al., 2020; Rombach et al., 2022; Ramesh et al., 2021, 2022). In a nutshell, training a diffusion model involves two processes: a forward diffusion process where noise is gradually added to the input image, and a reverse denoising diffusion process where the model tries to predict a noise \(_{t}\) which is added in the forward process. More specifically, given a chain of \(T\) diffusion steps \(x_{0},x_{1},...,x_{T}\), the denoising process can be formulated as follows: \(p_{}(x_{T:0})=p(x_{T})_{t=T}^{1}p_{}(x_{t-1} x_{t})\).

The model is trained by minimizing the difference between the true noise \(\) and \(_{}(x_{t},t)\), the predicted noise at step \(t\) by the denoising model \(\) as follows:

\[=_{x_{0} p_{},t,(0,)}\|-_{}(x_{t},t)\|_{2}^{2}\] (1)

Latent Diffusion Models:With an intuition that semantic information that controls the main concept of an image can be represented in a low-dimensional space, (Rombach et al., 2022) proposed a diffusion process operating on the latent space to learn the distribution of the semantic information which can be formulated as \(p_{}(z_{T:0})=p(z_{T})_{t=T}^{1}p_{}(z_{t-1} z_{t}),\) where \(z_{0}(x_{0})\) is the latent vector obtained by a pre-trained encoder \(\).

The objective function of the latent diffusion model as follows:

\[=_{z_{0}(x),x p_{},t, (0,)}\|-_{}(z_{t},t )\|_{2}^{2}\] (2)

## 3 Problem Statement

The task of erasing concepts from a text-to-image diffusion model often appears without additional data or labels, forcing us to rely on the model's own knowledge. Therefore, we here consider fine-tuning a pre-trained model rather than training a model from scratch. Let \(_{}(z_{t},c,t)\) denote the output of the pre-trained _foundation_ U-Net model parameterized by \(\) at step \(t\) given an input description \(c\) and the latent vector from the previous step \(z_{t}\) where \(\) is set of all possible input descriptions, commonly referred to as the textual prompt in text-to-image generative models.

Given a set of textual descriptions \(\) and the target model \(_{}\), our objective is to learn a _sanitized_ model \(_{^{}}(z_{t},c,t)\) that cannot generate images from any textual description \(c\) while preserving the quality of images generated by the remaining concepts \(=\). We also use \(c_{n}\) to denote a neutral or null concept, i.e., "a photo" or " ".

### Naive Erasure

A naive approach that has been widely used in previous works Gandikota et al. (2023); Orgad et al. (2023); Gandikota et al. (2024) is to optimize the following objective function:

\[_{^{}}\,_{c_{e}}[\|_{ ^{}}(c_{e})-_{}(c_{n})\|_{2}^{2}]\] (3)

Fundamentally, these methods aim to force the model output, associated with the to-be-erased concepts, to approximate the model output associated with a neutral or null input \(c_{n}\) (e.g., "a photo" or " "). Ideally, when erasing a concept, we would like to preserve _all_ the remaining ones. This would corresponding to optimizing the above objective for all possible concepts in \(\), which is excessively expensive. Hence, using a neutral concept as proxy first seems as a convenient strategy.

While this naive approach is effective in erasing the specific concept, it however has a negative impact on the model's capacity to preserve other concepts related to the to-be-erased concepts. For example, easing the concept "nudity" affects the quality of images of "woman" or "person". To mitigate this issue, prior works have proposed to use either an additional loss term to retain the null concept Gandikota et al. (2023) or a regularization term to prevent excessive change in the model parameters Orgad et al. (2023). However, these regularization attempts clearly have not addressed the core trade-off between erasing a concept and preserving the others.

### Impact of Concept Removal on the Model Performance

We here approach the problem more carefully via a study on the impact of erasing a specific concept on model performance on the remaining ones. More importantly, we are concerned with the _most sensitive concepts_ to erasure. For example, when removing the concept of "nudity", we are curious to know which concepts change the most in the model's output, so that we can preserve these concepts specifically to ensure the model's capability is maintained, at least with respect to these concepts.

For some concepts, we can make an intuitive guess. For example, the concept of "nudity" is closely related to the concepts of "women" and "men", which are likely to be affected by the removal of the concept of "nudity". However, for most concepts, it is not easy to determine which ones are most sensitive to the target concept. Therefore, in prior works, selecting a neutral one like a 'photo' or " " regardless of the target concepts is clearly not a sound solution. We next provide empirical evidence to support this argument.

Measuring Generation Capability with CLIP Alignment Score.Given a target concept \(c_{e}\), e.g., "nudity" or "garbage truck", from that we obtain the original model \(_{}\) and the sanitized model \(_{^{}_{c_{e}}}\) by removing the target concept \(c_{e}\). We have a set of concepts \(=\{c_{1},c_{2},,c_{||}\}\), where \(||\) is the number of concepts. Our goal is to measure the impact of unlearning \(c_{e}\) on the generation of other concepts \(c\) in \(\).

Figure 1: Analysis of the impact of erasing the target concept on the model’s capability. The impact is measured by the difference of CLIP score \((c)\) between the original model and the corresponding sanitized model. 1a: Impact of erasing “nudity” or “garbage truck” to other concepts. 1b: Comparing the impact of erasing the same ”garbage truck” to other concepts with different preserving strategies, including preserving a fixed concept such as ” ”, ”lexus”, or ”road”, and adaptively preserving the most sensitive concept found by our method.

To achieve this, we generate a large number of samples from both models, i.e., \(\{G(,c,z_{T}^{i})\}_{i=1}^{k},\{G(_{c_{e}}^{},c,z_{T}^{i})\}_{ i=1}^{k}\) for \(k=200\) samples for each concept \(c\). We then calculate the CLIP alignment score \(S_{,i,c}=S(G(,c,z_{T}^{i}),c)\) between the generated samples and the textual description of the concepts \(c\) (CLIP model 'opena/clip-vit-base-patch14'). A higher CLIP alignment score indicates that the generated samples are more similar to the concept \(c\), and vice versa. Thus, we can use the CLIP alignment score as a metric to evaluate the capability of the model to generate the concept \(c\), and the change of this score between the two models, \(_{c_{e}}(c)=_{i=1}^{k}(S_{,i,c}-S_{_{c_{ e}}^{},i,c})\) indicates the impact of unlearning \(c_{e}\) on generating the concept \(c\). The discussion on the metric is provided in Appendix B.3.

The Removal of Different Target Concepts Leads to Different Side-Effects.Figure 1a shows the impact of the removal of two distinct concepts, "nudity" and "garbage truck", on other concepts, measured by the difference of the CLIP score, \(_{}(c),_{}(c)\). A larger \((c)\) indicates a greater negative impact on the model's ability to generate concept \(c\).

It can be seen that removing the "nudity" concept significantly affects highly related concepts such as "naked", "men", "women", and "person", while having minimal impact on unrelated concepts such as "garbage truck", "bamboo" or neutral concepts such as "a photo" or the null " " concept. Similarly, removing the "garbage truck" concept significantly reduces the model's capability on concepts like "boat", "car", "bus", while also having little impact on other unrelated concepts such as "naked", "women" or neutral concepts.

These results suggest that removing different target concepts leads to varying impacts on other concepts. This indicates the need for an adaptive method to identify the most sensitive concepts relative to a particular target concept, rather than relying on random or fixed concepts for preservation. Moreover, in both cases, neutral concepts like "a photo" or the null concept show resilience and independence from changes in the model's parameters, suggesting that they do not adequately represent the model's capability to be preserved.

Neutral Concepts lie in the Middle of the Sensitivity Spectrum.Figure 2 shows the distribution of similarity scores between the outputs of the original model \(\) and the sanitized model \(_{c_{e}}^{}\) for each concept \(c\) from the CLIP tokenizer vocabulary. The histogram reveals that the similarity scores span a wide range, indicating that the impact of unlearning the target concept on generating other concepts varies significantly. The lower the similarity score, the more different the outputs of the two models are, and the more sensitive the concept is to the target concept. Notably, the more related concepts like "women" or "men" are more sensitive to the removal of "nudity" than many neutral concepts that lie in the middle of the sensitivity spectrum.

Figure 2: Sensitivity spectrum of concepts to the target concept "nudity". The histogram shows the distribution of the similarity score between outputs of the original model \(\) and the corresponding sanitized model \(_{c_{e}}^{}\) for each concept \(c\) from the CLIP tokenizer vocabulary.

What Concept should be kept to Maintain Model Performance.Figure 0(b) presents the results of an experiment similar to the previous one, with one key difference: we utilize the prior knowledge gained from the previous experiment. Specifically, when erasing the "garbage truck", we apply different preservation strategies, including preserving a fixed concept such as " ", "lexus", or "road", and adaptively preserving the most sensitive concept found by our method.

The results show that with simple preservation strategies such as preserving a fixed but related concept like "road", the model's capability on other concepts is better maintained compared to preserving a neutral concept. However, the results of adaptively preserving the most sensitive concept show the best performance, with the least side effects on other concepts. Similarly, the results of erasing the "nudity" concept as shown in Figure 3 show that preserving related concepts like "person" helps retain the model's capability on other concepts much better than preserving a neutral concept. These findings confirm the importance of selecting sensitive concepts to preserve in order to better maintain the model's overall capability.

## 4 Proposed Method: Adversarial Concept Preservation

In this work, we aim to minimize the side effects of erasing undesirable concepts in diffusion models through adversarial preservation. Motivated by the observations in the previous section, our approach involves identifying the most sensitive concepts related to a specific target concept. For example, when removing the concept of nudity, we identify which concepts are most affected in the model's output so that we can specifically preserve these concepts to ensure the model's capability is maintained.

In each iteration, before updating the model parameters, we first identify the concept \(c_{a}\) that is most sensitive to changes in the model parameters as we work to remove the target concepts.

\[_{^{{}^{}}}\,_{c_{a}}\,_{c_{e} }[^{}}}(c_{e})- _{}(c_{n})\|_{2}^{2}}_{L_{1}}+^{}}}(c_{a})-_{}(c_{a})\|_{2}^{ 2}}_{L_{2}}]\] (4)

where \(>0\) is a parameter and \(=\) denotes the remaining concepts.

Objective loss \(L_{1}\) is the same as in the naive approach, aiming to erase the target concept \(c_{e}\) by forcing its output to match that of a neutral concept. Our main contribution lies in the introduction of the adversarial preservation loss \(L_{2}\), which aims to identify the most sensitive concept \(c_{a}\) that is most affected by changes in the model parameters when removing the target concepts.

Since the concepts exist in a discrete space, the straightforward approach would involve revisiting all concepts in \(\), resulting in significant computational complexity. Another naive approach is to consider the concepts as lying in a continuous space and use the Projected Gradient Descent (PGD)

Figure 3: Comparing the impact of erasing the same ”nudity” to other concepts with different preserving strategies.

method, similar to Madry et al. (2017), to search within the local region of the continuous space of the concepts. More specifically, we initialize the adversarial prompt with the text embedding of the to-be-erased concept, e.g., \(c_{a,0}=c_{e}=()\), and then update the adversarial concept with gradient \(_{c_{a}}L_{2}\). Interestingly, while this approach provides an efficient computational method, we find that the adversarial concept quickly collapses from the initial concept to a background concept with the color information of the object as shown in the first row of Figure 4.

To combine the benefits of both approaches--making the process continuous and differentiable for efficient training while achieving meaningful concepts that are related to the target concept (second row of Figure 4)-- we first define a distribution over the discrete concept embedding vector space as \(_{,}=_{i=1}^{||}_{i}_{e_{i}}\) with the Dirac delta function \(\) and the weights \(_{}=\{^{}:\|^{}\|_{1}=1\}\). Instead of directly searching for the most sensitive concept \(c_{a}\) in the discrete concept embedding vector space \(\), we switch to searching for the embedding distribution \(\) on the simplex \(_{}\) and subsequently transform it back into a discrete space using the temperature-dependent GumbelSoftmax trick (Jang et al., 2016; Maddison et al., 2016) as follows:

\[_{^{{}^{}}}}}{ }_{c_{e}}[^{ }}}(c_{e})-_{}(c_{n})\|_{2}^{2}}{L_{1}}+^{}}}(())- _{}(())\|_{2}^{2}}{L_{2}}]\] (5)

where \(>0\) is a parameter, \(\) is Gumbel-Softmax operator and \(\) is element wise multiplication operator. The pseudo-algorithm involves a two-step optimization process, outlined in Algorithm 1: _Finding Adversarial Concept_ and Algorithm 2: _Adversarial Erasure Training_.

``` Input:\(,\). Searching hyperparameters: \(,N_{}\). Current state \(_{k}^{{}^{}}\) Output: Adversarial concept \(c_{a}\) for\(i=1\) to \(N_{}\)do \(+_{}[\|_{^{{}^{}}} (())-_{}(() )\|_{2}^{2}]\) \(\) Maximize \(L_{2}\) endfor \(c_{a}=(^{*})\) ```

**Algorithm 1** Find Adversarial Concept

## 5 Experiments

In this section, we present a series of experiments to evaluate the effectiveness of our method in erasing various types of concepts from the foundation model. Our experiments use Stable Diffusion (SD) version 1.4 as the foundation model. We maintain consistent settings across all methods: fine-tuning the model for 1000 steps with a batch size of 1, using the Adam optimizer with a learning rate of \(=10^{-5}\). We benchmark our method against four baseline approaches: the original pre-trained SD model, ESD (Gandikota et al., 2023), UCE (Gandikota et al., 2024), and Concept Ablation (CA) (Kumari et al., 2023).

Figure 4: Images generated from the most sensitive concepts found by our method over the fine-tuning process. Top: Continous search with PGD. Bottom: Discrete search with Gumbel-Softmax. \(c_{a}\) represents for the keyword.

We provide detailed implementation and further in-depth analysis in the appendix, including qualitative results (Section C), the choice of hyperparameters (Section B.2), and analysis on the search for the adversarial concepts (Sections B.4 and B.5).

### Erasing Concepts Related to Physical Objects

In this experiment, we investigate the ability of our method to erase object-related concepts from the foundation model, for example, erasing entire object classes such as "Cassette Player" from the model. We choose Imagenette 1 which is a subset of the ImageNet dataset Deng et al. (2009) which comprises 10 easily recognizable classes, including "Cassette Player", "Chain Saw", "Church", "Gas Pump", "Tench", "Garbage Truck", "English Springer", "Golf Ball", "Parachute", and "French Horn".

Since the erasing performance when erasing a single class has been the main focus of previous work Gandikota et al. (2023), we choose a more challenging setting where we erase a set of 5 classes simultaneously. Specifically, we generate 500 images for each class and employ the pre-trained ResNet-50 He et al. (2016) to detect the presence of an object in the generated images. We use the two following metrics to evaluate the erasing performance: **Erasing Success Rate (ESR-k)**: The percentage of all the generated images with "to-be-erased" classes where the object is not detected in the top-k predictions. **Presevering Success Rate (PSR-k)**: The percentage of all the generated images with all other classes (i.e., "to-be-preserved") where the object is detected in the top-k predictions. This dual-metric evaluation provides a comprehensive assessment of our method's ability to effectively erase targeted object-related concepts while also preserving relevant elements.

Quantitative Results.We select four distinct sets of 5 classes from the Imagenette set for erasure and present the outcomes in Table 1. First, we note that the average PSR-1 and PSR-5 scores across the four settings of the original SD model stand at 78.0% and 97.6%, respectively. These scores indicate that 78.0% of the generated images contain the object-related concepts which are subsequently detected in the top-1 prediction, and when checking the concepts in any of the top-5 predictions, this number increases to 97.6%. This underscores the original SD model's ability to generate images with the anticipated object-related concepts.

In term of erasing performance, it can be observed that all baselines achieve very high ESR-1 and ESR-5 scores, with the lowest ESR-1 and ESR-5 scores being 95.5% and 88.9% respectively. This indicates the effectiveness of these methods to erase object-related concepts, as only a very small proportion of the generated images contain the object-related concepts under subsequent detection. Notably, the UCE method can achieve 100% ESR-1 and ESR-5, which is the highest among the baselines. Our method achieves 98.6% ESR-1 and 96.1% ESR-5, which is much higher than the two baselines ESD and CA, and only slightly lower than the UCE method, which is designed specifically for erasing object-related concepts.

However, despite the high erasing performance, the baselines, especially UCE, suffer from a significant drop in preserving performance, with the lowest PSR-1 and PSR-5 scores being 23.4% and 49.5%, respectively. This suggests that the preservation task poses greater challenges than the erasing task, and the baselines are ineffective in retaining other concepts. In contrast, our method achieves 55.2% PSR-1 and 79.9% PSR-5, which is a significant improvement compared to the best baseline, CA, with 44.2% PSR-1 and 66.5% PSR-5. This result underscores the effectiveness of our method in simultaneously erasing object-related concepts while preserving other unrelated concepts.

### Mitigating Unethical Content

One of the serious concerns associated with the deployment of text-to-image generative models to the public domain is their potential to generate Not-Safe-For-Work (NSFW) content. This ethical challenge has become a primary focus in recent works Schramowski et al. (2023); Gandikota et al. (2023, 2024), aiming to sanitize such capability of the model before public release.

In contrast to object-related concepts, such as "Cassette Player" or "English Springer", which can be explicitly described with limited textual descriptions, i.e., there are only a few textual ways to describe the visual concepts, unethical concepts like nudity are indirectly expressible in textual descriptions. The multiple ways a single visual concept can be described make erasing such concepts challenging, especially when relying solely on a keyword to indicate the concept to be erased. As empirically shown in Gandikota et al. (2023), the erasing performance on these concepts is highly dependent on the subset of parameters that are finetuned. Specifically, fine-tuning the non-cross-attention modules has shown to be more effective than fine-tuning the cross-attention modules. Therefore, in this experiment, we follow the same configuration as in Gandikota et al. (2023), focusing exclusively on fine-tuning the non-cross-attention modules.

Quantitative Results.To generate NSFW images, we employ I2P prompts Schramowski et al. (2023) and generate a dataset comprising 4703 images with attributes encompassing sexual, violent, and racist content. We then utilize the detector Praneet (2019) which can accurately detect several types of exposed body parts to recognize the presence of the nudity concept in the generated images. The detector Praneet (2019) provides multi-label predictions with associated confidence scores, allowing us to adjust the threshold and control the trade-off between the number of detected body parts and the confidence of the detection, i.e., the higher the threshold, the fewer the number of detected body parts.

Figure 4(a) illustrates the ratio of images with any exposed body parts detected by the detector Praneet (2019) over the total 4703 generated images (denoted by **NER**) across thresholds ranging from 0.3 to 0.8. Notably, our method consistently outperforms the baselines under all thresholds, showcasing its effectiveness in erasing NSFW content. In particular, as per Table 2, with the threshold set at 0.3, the NER score for the original SD model stands at 16.7%, indicating that 16.7% of the generated images contain signs of nudity concept from the detector's perspective. The two baselines, ESD and UCE, achieve 5.32% and 6.87% NER with the same threshold, respectively, demonstrating their effectiveness in erasing nudity concepts. Our method achieves 3.64% NER, the lowest among the baselines, indicating the highest erasing performance. This result remains consistent across different thresholds, emphasizing the robustness of our method in erasing NSFW content. Additionally, to measure the preserving performance, we generate images with COCO 30K prompts and measure the FID score compared to COCO 30K validation images. Our method achieves the best FID score of 15.52, slightly lower than that of UCE, which is the best baseline at 15.98, indicating that our method can simultaneously erase a concept while preserving other concepts effectively.

Detailed statistics of different exposed body parts in the generated images are provided in Figure 4(b). It can be seen that in the original SD model, among all the body parts, the female breast is the most detected body part in the generated images, accounting for more than 320 images out of the total 4703 images. Both baselines, ESD and UCE, as well as our method, achieve a significant reduction in the number of detected body parts, with our method achieving the lowest number among the baselines. Our method also achieves the lowest number of detected body parts for the most sensitive body parts, only surpassing the baseline for less sensitive body parts, such as feet.

Interestingly, our method seems to remove the sensitive body parts while keeping the less sensitive body parts untouched as shown in Figure 4(b). To provide more insights into this phenomenon, we

   Method & ESR-1\(\) & ESR-5\(\) & PSR-1\(\) & PSR-5\(\) \\  SD & \(22.0 11.6\) & \(2.4 1.4\) & \(78.0 11.6\) & \(97.6 1.4\) \\ ESD & \(95.5 0.8\) & \(88.9 1.0\) & \(41.2 12.9\) & \(56.1 12.4\) \\ UCE & \(100 0.0\) & \(100 0.0\) & \(23.4 3.6\) & \(49.5 8.0\) \\ CA & \(98.4 0.3\) & \(96.8 6.1\) & \(44.2 9.7\) & \(66.5 6.1\) \\  Ours & \(98.6 1.1\) & \(96.1 2.7\) & \(55.2 10.0\) & \(79.9 2.8\) \\   

Table 1: Erasing object-related concepts.

calculate the similarity scores between different concepts and body parts in the nudity erasure setting as Table 3.

It can be seen that the "nudity" concept is highly correlated with the "Female Breast" concept, suggesting that when removing the "nudity" concept, the "Female Breast" concept is more likely to be affected than other body parts. On the other hand, the "Person" or "Body" concept is more strongly correlated with the "Feet" concept than with the "Female Breast" concept, indicating that preserving the "Person" concept might help maintain the model's performance on "Feet" rather than on "Female Breast." Furthermore, the gap between the "Feet" and "Female Breast" concepts with respect to "Person" or "Body" is larger than the gap with more generic concepts like "A photo." This suggests that preserving generic concepts might not have the same impact as preserving the most affected concepts. Our method naturally selects the most affected concepts to be preserved, which often includes concepts highly correlated with non-sensitive body parts. This explains the observed phenomenon in the experiment.

### Erasing Artistic Concepts

In this experiment, we investigate the ability of our method to erase artistic style concepts from the foundation model. We choose several famous artists with easily recognizable styles who have been known to be mimicked by the text-to-image generative models, including "Kelly Mckernan", "Thomas Kinkade", "Tyler Edlin" and "Kilian Eng" as in Gandikota et al. (2023). We compare our

    & NER-0.3\(\) & NER-0.5\(\) & NER-0.7\(\) & NER-0.8\(\) & FID\(\) \\  CA & 13.84 & 9.27 & 4.74 & 1.68 & 20.76 \\ UCE & 6.87 & 3.42 & 0.68 & 0.21 & 15.98 \\ ESD & 5.32 & 2.36 & 0.74 & 0.23 & 17.14 \\  Ours & 3.64 & 1.70 & 0.40 & 0.06 & 15.52 \\   

Table 2: Evaluation on the nudity erasure setting.

   CLIP & Nudity & A photo & Person & Body \\  Feet & 0.612 & 0.547 & 0.566 & 0.643 \\ Belly & 0.601 & 0.514 & 0.517 & 0.748 \\ Armpits & 0.614 & 0.477 & 0.475 & 0.643 \\ Buttocks & 0.649 & 0.501 & 0.494 & 0.639 \\ Male Breast & 0.616 & 0.499 & 0.472 & 0.504 \\ Male Genitalia & 0.618 & 0.511 & 0.537 & 0.517 \\ Female Genitalia & 0.662 & 0.536 & 0.558 & 0.555 \\ Female Breast & 0.656 & 0.517 & 0.491 & 0.574 \\   

Table 3: Similarity scores between different concepts and body parts in the nudity erasure setting.

Figure 5: Comparison of the erasing performance on the I2P dataset. 5a: Number of exposed body parts counted in all generated images with threshold 0.5. 5b: Ratio of images with any exposed body parts detected by the detector Praneet (2019).

method with recent work including ESD Gandikota et al. (2023), UCE Gandikota et al. (2024), and CA Kumari et al. (2023) which have demonstrated effectiveness in similar settings.

For fine-tuning the model, we use only the names of the artists as inputs. For evaluation, we use a list of long textual prompts that are designed exclusively for each artist, combined with 5 seeds per prompt to generate 200 images for each artist across all methods. We measure the CLIP alignment score 2 between the visual features of the generated image and its corresponding textual embedding. Compared to the setting Gandikota et al. (2023) which utilized a list of generic prompts, our setting with longer specific prompts can leverage the CLIP score as a more meaningful measurement to evaluate the erasing and preserving performance. We also use LPIPS Zhang et al. (2018) to measure the distortion in generated images by the original SD model and editing methods, where a low LPIPS score indicates less distortion between two sets of images.

It can be seen from Table 4 that our method achieves the best erasing performance while maintaining a comparable preserving performance compare to the baselines. Specifically, our method attains the lowest CLIP score on the to-be-erased sets at 21.57, outperforming the second-best score of 23.56 achieved by ESD. Additionally, our method secures a 0.78 LPIPS score, the second-highest, following closely behind the CA method with 0.82. Concerning preservation performance, we observe that, while our method achieves a slightly higher LPIPS score than the UCE method, suggesting some alterations compared to the original images generated by the SD model, the CLIP score of our method remains comparable to these baselines. This implies that our generated images still align well with the input prompt.

## 6 Conclusion

In this paper, we introduced a novel approach to concept erasure in text-to-image diffusion models by incorporating an adversarial learning mechanism. This mechanism identifies the most sensitive concepts affected by the removal of the target concept from the discrete space of concepts. By preserving these sensitive concepts, our method outperforms state-of-the-art erasure techniques in both erasing unwanted content and preserving unrelated concepts, as demonstrated through extensive experiments. Furthermore, our adversarial learning mechanism exhibits high flexibility, linking this task to the field of Adversarial Machine Learning, where adversarial examples have been extensively studied. This connection opens potential directions for future research, such as simultaneously searching for multiple sensitive concepts under certain divergence constraints, offering promising avenues for further exploration.