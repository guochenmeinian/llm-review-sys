# Adaptable Logical Control for Large Language Models

Honghua Zhang

UCLA

hzhang19@cs.ucla.edu

&Po-Nien Kung

UCLA

ponienkung@cs.ucla.edu

&Masahiro Yoshida

UCLA & Sony Group Corporation

masahiroyoshida@ucla.edu

&Guy Van den Broeck

UCLA

guyvdb@cs.ucla.edu

&Nanyun Peng

UCLA

violetpeng@cs.ucla.edu

Equal contributions.Work done at UCLA as a visiting scholar.

###### Abstract

Despite the success of Large Language Models (LLMs) on various tasks following human instructions, controlling model generation to follow strict constraints at inference time poses a persistent challenge. In this paper, we introduce Ctrl-G, a neuro-symbolic framework that enables tractable and adaptable control of LLM generation to follow logical constraints reliably. Ctrl-G combines any production-ready LLM with a Hidden Markov Model (HMM), guiding LLM outputs to adhere to logical constraints represented as deterministic finite automata. We show that Ctrl-G, when a TULU2-7B model is coupled with a 2B-parameter HMM, outperforms GPT4 in text editing: on the task of generating text insertions/continuations following logical constraints, our approach achieves over 30% higher satisfaction rate in human evaluation. When applied to medium-size language models (e.g., GPT2-large), Ctrl-G also beats its counterparts on standard benchmarks by large margins. Additionally, as a proof-of-concept study, we use Ctrl-G to assist LLM reasoning on the GSM benchmark, foreshadowing the application of Ctrl-G, as well as other constrained generation approaches, beyond traditional language generation tasks.

## 1 Introduction

Large language models (LLMs) have achieved remarkable performance on a wide range of challenging language generation tasks including translation , summarization , and open-domain creative generation . Nevertheless, many downstream applications benefit from fine-grained control of LLMs to follow logical constraints, e.g., avoid using bad words for detoxification  or inserting text that is coherent with contexts for document revision . Despite the recent advancement of LLM finetuning techniques such as instruction-tuning  and preference optimization , LLMs still fail to reliably follow logical constraints .

The major difficulty of achieving constrained generation from LLMs lies in the intractability of conditioning LLMs on logical constraints . One recently proposed framework called GeLaTo  uses tractable generative models, which _can_ be conditioned on logical constraints efficiently, to guide autoregressive generation from LLMs. Though GeLaTo guarantees that the logical constraints

Figure 1: Ctrl-G pipeline; both the LLM and the HMM are frozen once trained.

will be satisfied, it only works for the keyword-inclusion constraint. Significantly generalizing the GeLaTo framework, we propose Ctrl-G (shorthand for controllable generation while mimicking the keyboard shortcuts Ctrl-C and Ctrl-V) for **reliable**, **scalable** and **adaptable** control of LLMs to follow logical constraints. Ctrl-G consists of three major steps (see Fig. 1): (1) _distillation_: given a LLM, we distill a Hidden Markov Model as its white-box approximation; (2) _constraint specification_: we construct a deterministic finite automaton (DFA) to (compactly) represent the desired logical constraint; (3) _inference_: we condition the HMM on the DFA-specified constraint and compute this conditional probability to steer LLM generation towards satisfying the constraint.

Ctrl-G3 has three major advantages compared to its counterparts: (1) the desired logical constraints are guaranteed to be satisfied ; (2) once we have the distilled HMM, it can be applied to arbitrary constraints without retraining; (3) Ctrl-G works for any constraints specified as DFAs, which can be easily constructed for various applications by leveraging existing algorithms.

We evaluate Ctrl-G on the task of text editing: in the domain of story writing, we evaluate models' ability to generate suggestions for text insertions/continuations under combinations of logical constraints (e.g. keyphrase inclusion and length control; see Fig. 2). Human evaluation shows that Ctrl-G, where a TULU2-7B model  is combined with a 2B-parameter HMM, outperforms prominent LLMs including GPT3.5 and GPT4  by over 30% in overall satisfaction rate (i.e., percentage of the generated text that is not only fluent but also satisfies the constraints). We note that as the constraints become more complex, while the generation quality of GPT4 declines, Ctrl-G consistently produces high-quality text, highlighting its strong generalizability to complex constraints. Even when no constraint is present, Ctrl-G still matches with the generation quality of GPT4 in text insertion.

In addition, we demonstrate the extensive adaptability of Ctrl-G on two commonly used benchmarks: commonsense generation  and text infilling . When applied to variants of the GPT2 models, Ctrl-G outperforms prior constrained generation approaches by producing outputs of substantially higher quality while achieving 100% constraint satisfaction.

To further explore the potential of Ctrl-G, as a proof-of-concept, we conduct an empirical study on the Grade School Math (GSM) benchmark ; here, we use Ctrl-G to assist the LLM reasoning process by enforcing keyphrase-inclusion constraints. Performance improvement suggests the potential of Ctrl-G in applications of a scope broader than the traditional constrained generation tasks.

## 2 Preliminaries

In this section, we briefly summarize the background for (logically-)constrained generation and the basics for Hidden Markov Models. Notations introduced here will be used throughout the paper.

Constrained generationFor simplicity, we assume that the lengths of token sequences generated by LLMs are always bounded by some number \(n\) and denote the LLM distribution as \(p_{}(x_{1:n})\)4. Given

Figure 2: An example usage of Ctrl-G for text insertion with multiple constraints.

logical constraint \(\), our goal is to generate from \(p_{}(x_{1:n})\), which decomposes autoregressively:

\[p_{}(x_{1:n})=_{t}p_{}(x_{t} x_{< t},),\;\;\;\;p_{}(x_{t} x_{<t},) p_{}(x_{t}  x_{<t}) p_{}( x_{t},x_{<t});\]

that is, given that we have generated the first \(t-1\) tokens \(x_{<t}\), we want to generate the next token \(x_{t}\) from \(p_{}(x_{t} x_{<t}) p_{}( x_{t},x_{<t})\). The first term \(p_{}(x x_{<t})\) is just the next-token distribution of the LLM, but the marginal probability \(p_{}( x_{t},x_{<t})\), which characterizes how likely the constraint \(\) will be satisfied _in the future_, cannot be efficiently computed; specifically,

\[p_{}( x_{t},x_{<t})=_{x_{>t}\;\;x _{1:n}\;\;}p(x_{>t} x_{t},x_{<t});\]

that is, we need to marginalize over all possible future sequences \(x_{>t}\) such that, together with \(x_{ t}\), satisfy \(\). For example, say \(\) is the constraint that the phrase "in the park" must appear at the end of the generated text; to compute the desired marginal probability, we need to enumerate over all future token sequences with this phrase at the end, and there are exponentially many of them.

Prior workTo solve the problem of constrained generation, one line of work proposes search-based decoding algorithms like NeuroLogic Decoding [22; 21], which explicitly performs heuristic search to find high-probability token sequences that would (partially) satisfy the logical constraint; however such methods scale poorly because the search space grows exponentially with respect to the sequence length. The other line of works including GeDi , FUDGE  and NADO  train auxiliary neural classifiers to approximate the intractable term \(p_{}( x_{t},x_{<t})\); however, they do not guarantee that the constraints will be satisfied and the classifiers need to be retrained for different constraints. Some other methods use approximate inference techniques (e.g., sequential Monte Carlo sampling) to approximate the intractable conditional distributions [30; 11; 17], which provide no guarantee on the convergence rate and often suffer from the high-variance of sampling.

From GeLaTo to Ctrl-GA recent framework called GeLaTo  uses tractable generative models, in particular, Hidden Markov Models (HMMs), to guide LLM generation to satisfy the given logical constraints. Specifically, GeLaTo first (1) distills an HMM \(p_{}(x_{1:n})\) to approximate the LLM distribution \(p_{}(x_{1:n})\) and then (2) computes \(p_{}(|x_{t},x_{<t})\) as an approximation for \(p_{}(|x_{t},x_{<t})\). Compared to its counterparts, GeLaTo _guarantees_ that the constraints will be satisfied. Nevertheless, two major questions remain unanswered, limiting its downstream applications:

* GeLaTo only handles the keyword-inclusion constraint and it is unclear whether \(p_{}(\!\!x_{t+1},x_{1:t})\) can be tractably computed for other logical constraints;
* despite the success of GeLaTo on language models at the scale of \(0.1\) billion parameters, it is unclear whether the assumption \(p_{}( x_{ t})\!\!p_{}(\! x_{  t})\) would still hold for the more recent LLMs (e.g., Llama2), which have over 100 times more parameters.

We propose Ctrl-G as a generalization of GeLaTo and give positive answers to both questions.

Hidden Markov ModelsA Hidden Markov Model (HMM)  represents a joint probability distribution over \(n\) observed variables \(x_{1:n}\) and \(n\) hidden variables \(z_{1:n}\). Specifically, for language modeling, \(x_{t}\) represents the token at position \(t\) and \(z_{t}\) is the corresponding hidden state; \(z_{t}\) takes values in \(\{1,2,,h\}\), where \(h\) is the _number of hidden states_. An HMM models the joint distribution:

\[p(x_{1:n},z_{1:n})=p(x_{1} z_{1}) p(z_{1})_{2 t  n}p(x_{t} z_{t}) p(z_{t} z_{t-1});\]

in particular, the parameters of an HMM are given by the initial probability \(p(z_{1})\), the emission matrix \(p(x_{t}|z_{t})\) and the transition matrix \(p(z_{t+1}|z_{t})\); the number of parameters of HMMs grows quadratically with respect to \(h\). To perform inference on HMMs efficiently, we leverage the _Markov property_: \(p(x_{ t}\!\!z_{t},x_{<t})\!=\!p(x_{ t}\!\!z_{t})\). For example, we can efficiently compute \(p(x_{ t})=_{z_{t}}p(x_{ t},z_{t})\) by the following recurrence relation, referred to as the _forward algorithm_:

\[p(x_{ t},z_{t})\!=\!_{1 z_{t-1} h}\!p(x_{t} z_{t} ) p(z_{t}\!\!z_{t-1}) p(x_{ t-1},z_{t-1}).\]

## 3 Tractable probabilistic reasoning over logical constraints

The Ctrl-G pipeline consists of three steps (Fig. 1): (1) _distillation_: we train an HMM on samples drawn from the LLM to minimize their KL-divergence; (2) _constraint specification_: we construct a (compact) deterministic finite automaton (DFA) \(\) representing the desired logical constraint \(\) (i.e., \(\)_accepts_\(x_{1:n}\) if and only if \(x_{1:n}\) satisfies \(\)); (3) _inference_: for each step of the autoregressive generation from the LLM, we compute \(p_{}( x_{t},x_{<t})\) as an approximation for \(p_{}( x_{t},x_{<t})\) and then sample the next token from

\[p_{}(x_{t} x_{<t},) p_{}(x_{t} x_{< t}) p_{}( x_{t},x_{<t});\] (1)

here, given that \(\) is represented as \(\),

\[p_{}( x_{t},x_{<t})=_{x_{>t}\,x_{1:n}}p_{}(x_{>t} x_{t},x_{<t})\] (2)

For step (1) (distillation) we follow the procedure proposed by , and we describe step (2) and step (3) in Sec. 3.1 and Sec. 3.2, respectively. In the end of this section, we briefly discuss the distinction between pure logical reasoning and probabilistic reasoning over constraints.

### Logical constraints as DFAs

Deterministic finite automata (DFAs) [24; 31; 10] are computation models that _accept_ or _reject_ some given strings. Figure (a)a shows an example DFA encoding the constraint that the phrase "gets cold" must appear: it accepts all strings containing this phrase and rejects the others. The DFA consists of 3 different _states_ labeled \(A\), \(B\) and \(C\), where \(A\) is the _initial state_ and \(C\) an _accept state_. The states are connected by edges marked with sets of words (tokens, to be precise), which fully specify the _transition function_ of the DFA. A DFA decides whether a given string satisfies the constraint by consuming it left-to-right while transitioning from state to state accordingly; in the end, the DFA accepts the string if it is in an accept state. See Figure (b)b for an example.

**Definition 3.1**.: A _deterministic finite automaton_ (DFA) is a tuple \(\!=\!(Q,,,q_{0},F)\), where \(Q\) is a finite set of _states_, \(\) a finite set of _symbols_ (i.e., tokens of an LLM), \(:Q\!\!\!\!Q\) a _transition function_, \(q_{0}\) an _initial state_, and \(F\!\!Q\) a set of _accept states_. A string of tokens \(w_{1}w_{2} w_{n}\) is accepted by \(\) if there exists a sequence of states \(q_{0},q_{1} q_{n}\) s.t. \((q_{i},w_{i+1})\!=\!q_{i+1}\) for \(1\!\!i\!\!n,q_{n}\!\!F\).

One question naturally arises: how can we come up with DFA representations for logical constraints? We first note that in the real world, we can always assume that the lengths of the generated token sequences are _bounded by a constant_; hence DFAs can represent any logical constraints defined on this bounded set and the important question is whether we can do this _efficiently_. For many common logical constraints, we can efficiently construct their DFA representations via existing algorithms. For example, given a string consisting of \(n\) tokens, to encode the constraint that the string must appear, we can construct a DFA of size \(O(n)\) by adapting the well-known _Knuth-Morris-Pratt_ (KMP) algorithm  for string matching (e.g., Fig. (a)a). One can also easily specify _compositional_ logical constraints via DFAs by taking their intersection (logical and), union (logical or), complement (logical negation) or concatenation, which we illustrate throughout the rest of this paper.

### An efficient algorithm for marginalizing HMMs over DFAs

Now assume that we have a constraint \(\) encoded as a DFA \(\) with \(k\) states \(Q=\{1,2, k\}\) and \(m\) edges, and we are given a distilled HMM with \(h\) hidden states. To sample the next token from Eq. 1, we need to compute \(p_{}( x_{t},x_{<t})\), which is the marginal probability over all strings accepted by \(\) (see Eq. 2). In the following, we describe a tractable algorithm for computing this probability.

Figure 3: Example of a DFA representing the logical constraint that the phrase “gets cold” must appear in the generated text along with pseudo-code for representing this DFA in Ctrl-G.

In autoregressive generation, \(\) starts from the initial state and transitions according to the transition function as each new token is generated; we denote the state of \(\) after sampling the first \(t\) tokens \(x_{ t}\) as \(s_{t}\). In addition, we use the uppercase \(S_{t}\) to denote the _random variable_ representing the state of \(\) after sampling the first \(t\) tokens: e.g., \(S_{n} F\) denotes the event that the token sequence \(x_{1:n}\) is accepted by \(\). Dropping the subscript "hmm" from \(p_{}( x_{t},x_{<t})\), we compute

\[p( x_{t},x_{<t})=p(S_{n}\!\!F x_{t},x_{<t})=p(S_{n}\!\!F,x_{ t},x_{<t})/p(x_{t},x_{<t}).\]

The denominator \(p(x_{t},x_{<t})\) can be easily computed by the forward algorithm ; so we compute

\[ p(S_{n}\!\!F,x_{t},x_{<t})&= _{z_{t}}\!p(S_{n}\!\!F z_{t},x_{t},x_{<t}) p(z_{t},x_{t}, x_{<t})\\ &=_{z_{t}}\![\!\!\!F z_{t}, s_{t})}{p(S_{n}\!\!F z_{t},s_{t})}\!]\!\!p(z_{t},x_{t},x_{<t}) \] (3)

the first step follows from the law of total probability and the second step follows from the Markov properties of HMMs and DFAs, as well as the fact that \(s_{t}\) is fully determined by \(x_{ t}\). Again, the term \(p(z_{t},x_{t},x_{<t})\) can be computed by the forward algorithm and we reduce the problem to computing the boxed term. We compute \(p(S_{n}\!\!F z_{t},s_{t})\) for all \(1\!\!t\!\!n\), \(1\!\!z_{t}\!\!h\) and \(1\!\!s_{t}\!\!k\) via the following recurrence relation:

\[\!\!F z_{t},s_{t})}\!=_{z_{t+1}}p(z_{t+1} z _{t})_{s_{t+1}}\![\!\!\!F z_{t+1},s_{ t+1})}{p(S_{n}\!\!F z_{t+1},s_{t+1})}\!]\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!Even though both generations end with " in the park", it is clear that the output from guidance is not desirable as it forcefully appends the phrase to some irrelevant text. The reason is that guidance, by performing pure logical reasoning, only discard the next tokens \(x_{t}\) that would make \(\) unsatisfiable, while the probabilities of the other next tokens remain unchanged; in contrast, Ctrl-G performs _probabilistic reasoning_ by estimating \(p_{}( x_{t},x_{<t})\), i.e., we estimate how likely each next token \(x_{t}\) would eventually lead to \(\) being satisfied. Ctrl-G subsumes the other approaches in the sense that if we set \(p_{}( x_{t},x_{<t})=1\) for all non-zero values, that is, if we remove all probabilistic information, then it degenerates to pure logical reasoning.

## 4 Evaluating Ctrl-G on constrained generation benchmarks

### Commonsense Generation

Following prior work [21; 25], we first evaluate Ctrl-G on the Commonsene Generation (CommonGen) benchmark . Each test example of CommonGen provides 3 to 5 concepts (keywords) as input and the goal is to generate a natural sentence that incorporates all keywords, allowing for any of their inflections. For example, given _"car"_, _"snow"_ and _"drive"_ as concepts, both _"a man drives a car on a snow covered road"_ and _"the car drove through the snow"_ are considered acceptable.

DFA constructionFor CommonGen, given one keyword, say, "snow", we adapt the _Aho-Corsick algorithm_ to construct a DFA enforcing the constraint that _at least one of_ its inflections (e.g., "snow", "snowy") must appear. To encode the constraint that _multiple_ keywords must appear, we can simply take the intersection of the individual DFAs ; see appendix for an example.

Experiments & resultsWe use the GPT2-large checkpoint (only finetuned for domain adaptation) released by  as our base model and we follow the same pipeline to distill an HMM with 32768 hidden states: we sample 4M examples from the base model and train the HMM for 40 EM steps, each consisting of 100K examples. We compare Ctrl-G against FUDGE , NADO , NeuroLogic A*esque decoding  and GeLaTo ; GeLaTo uses the same base model as Ctrl-G. The results are summarized in Table 1, where the _Constraint_ column shows the percentage of the outputs containing all concepts. Compared to all baselines, Ctrl-G achieves not only 100% constraint satisfaction rate but also substantially higher generation quality measured by automatic evaluation metrics [29; 19; 40; 3].

Runtime comparisonFrom an algorithmic perspective, GeLaTo only handles keyword constraints hence it is a special case of Ctrl-G. Nevertheless, Ctrl-G also runs significantly faster than GeLaTo, as shown in Table 2. The GeLaTo implementation only tensorizes the HMM inference component, while the component that reasons about the constraints runs sequentially on CPU. In contrast, by representing DFAs as (weighted) adjacency matrices, Ctrl-G tensorizes the inference procedure for both HMMs and DFAs and runs on GPUs with full parallelization. Besides, both GeLaTo and Ctrl-G runs significantly faster than A*esque, which explicitly performs heuristic search.

Generalization to more keywordsTo evaluate the generalization performance of Ctrl-G, we construct test examples containing 6 to 9 concepts (CommonGen+): we randomly select 100 examples with 5 concepts from the dev split of CommonGen, and then augment them with additional keywords

    &  &  &  &  &  \\   & _dev_ & _test_ & _dev_ & _test_ & _dev_ & _test_ & _dev_ & _test_ & _dev_ & _test_ \\   \\  & 24.6 & - & 40.4 & - & - & - & - & - & - & 47.0\% \\ A*esque & - & 28.2 & - & 43.4 & - & 15.2 & - & 30.8 & - & 98.8\% \\ NADO & 30.8 & - & 44.4 & - & 16.1 & - & 32.0 & - & 88.8\% & - \\ GeLaTo & 34.0 & 34.1 & 46.2 & 45.9 & 17.2 & 17.5 & 32.2 & **33.5** & **100.0\%** & **100.0\%** \\ Ctrl-G & **35.1** & **34.4** & **46.7** & **46.4** & **17.4** & **17.6** & **32.7** & 33.3 & **100.0\%** & **100.0\%** \\  \\ A*esque & - & 28.6 & - & 44.3 & - & 15.6 & - & 29.6 & - & - \\ NADO & 26.2 & - & - & - & - & - & - & - & - \\ GeLaTo & 30.3 & 29.0 & 44.3 & 43.8 & 15.6 & 15.5 & 30.2 & 30.3 & **100.0\%** & **100.0\%** \\ Ctrl-G & **32.1** & **31.5** & **45.2** & **44.8** & **16.0** & **16.2** & **30.8** & **31.2** & **100.0\%** & **100.0\%** \\   

Table 1: CommonGen results. All methods are applied to the GPT2-large model.

sampled from their reference sentences. As shown in Fig. 4, Ctrl-G achieves 100% constraint satisfaction rate while preserving high generation quality across all settings.

### Text infilling

We also evaluate Ctrl-G on a text infilling benchmark  constructed from the ROC stories corpus . Each test example consists of a short story with some fragments masked out, each of a specified granularity; the goal is to fill in the masks. Here is an example: _"Jill wanted to knit her [WORD] a sweater. [SENTENCE] She finished [NGRAM] for her boyfriend's birthday. Jill was [WORD]."_

DFA constructionThe underlying logical constraint for the task of text infilling is similar to that of CommmonGen. We can view the non-masked parts, e.g., "Jill wanted to knit her" and "a sweater." from the example above, as keyphrases, and the task reduces to generating a piece of text such that all keyphrases appear in the given order. In this setting, given \(k\) text fragments, we first construct \(_{1},,_{k}\) using the KMP algorithm ; then, we _concatenate_ them to represent the constraint that they must appear in the given order. Though DFA concatenation is intractable in general , we observe that the KMP DFAs can actually be concatenated in linear time. See appendix for details.

Experiments & resultsWe use the GPT2-small checkpoint (only finetuned for domain adaptation with no supervision on the task of text infilling) released by  as the base model for Ctrl-G and compare against the ILM model, which is a GPT2-small model trained on this text infilling benchmark with full supervision. By applying the mask function from , we construct 4 test sets with different masking ratios (i.e., different percentage of masked characters) by changing the hyper-parameters. We measure the BLEU and ROUGE scores of the completed stories (i.e., including both the masked and unmasked parts) with respect to the original stories. The ILM model adopts sampling for decoding, so we run the ILM inference for 10 times to report the means and standard deviations. The results are summarized in Table 3. Based on , ILM is trained on a distribution with a masking ratio of approximately 15%, explaining why it achieves the best performance on the test set with 13% masking

    &  &  \\  \# of concepts & 3 & 4 & 5 & 3 & 4 & 5 \\  A*esque & 472.9 & 542.5 & 613.9 & 8.5 & 9.6 & 11.4 \\ GeLaTo  & \(69.8 32.3\) & \(97.9 39.5\) & \(143.0 44.4\) & \(49.8 20.8\) & \(88.7 30.5\) & \(127.6 30.4\) \\ Ctrl-G  & \(1.1 0.3\) & \(1.9 0.5\) & \(4.6 1.4\) & \(1.2 0.4\) & \(2.3 0.8\) & \(5.7 1.7\) \\ Ctrl-G  & \(4.1 0.9\) & \(9.0 2.0\) & \(22.3 5.4\) & \(4.7 1.6\) & \(11.0 3.8\) & \(27.6 8.3\) \\   

Table 2: Time (seconds) of generating one example on CommonGen (dev); # of HMM hidden states shown in brackets. Beam sizes used by A*esque, GeLaTo and Ctrl-G are 20, 128 and 128.

    &  &  \\  mask ratio & 13\% & 21\% & 32\% & 40\% & 13\% & 21\% & 32\% & 40\% \\  ILM & **85.2\(\)**0.1 & 76.3\(\)0.1 & 64.3\(\)0.1 & 53.8\(\)0.1 & **90.9\(\)**0.2 & **84.9\(\)**0.3 & 76.3\(\)0.4 & 68.4\(\)0.5 \\ Ctrl-G & **85.4** & **77.5** & **66.5** & **57.2** & 90.6 & **85.2** & **77.0** & **69.8** \\  diff. & \(+0.2\) & \(+1.2\) & \(+2.2\) & \(+3.4\) & \(-0.3\) & \(+0.3\) & \(+0.7\) & \(+1.4\) \\   

Table 3: Text infilling results (BLEU-4/ROUGE-L) across different masking ratios.

Figure 4: CommonGen+ results; Ctrl-G generalizes well on test examples with more than 5 concepts.

ratio. Note that the performance gap between Ctrl-G and ILM improves almost monotonically as the masking ratio increases, again illustrating the strong generalization performance of Ctrl-G.

## 5 Scaling up Ctrl-G for interactive text editing

Human-AI collaborative writing has been a long studied topic in the Human-Computer Interaction (HCI) community [12; 36]. One prior work  proposed CoAuthor, a graphical user interface for querying LLMs to generate continuation/insertion suggestions in arbitrary positions of a document. However, when using CoAuthor to ask for LLM suggestions, users are unable to specify their preferences. We propose to extend the CoAuthor system by allowing users to have fine-grained control over the suggestions generated by LLMs: for example, users can control the topic of the generated content by instructing LLMs to incorporate certain keyphrases, and they can also ask for more concise/detailed suggestions by controlling their lengths. For this application, we apply Ctrl-G to the TULU2-7B model and compare against prominent LLMs including GPT3.5 and GPT4.

### Experiment setup

Dataset constructionWe construct an evaluation dataset consisting of 800 test examples, each based on one story passage extracted from the CoAuthor dataset . These stories are jointly written by humans and the GPT3.5-turbo-instruct model, falling under ten different topics. For each story, we randomly split it into _prefix_, _infx_ and _suffix_; we mask out the _infx_ and view it as a gold reference. We consider two scenarios when evaluating the models: **continuation** and **insertion**. For continuation, we only provide _prefix_ to the model, and the model is supposed to generate one suggestion for continuation; for insertion, we provide both _prefix_ and _suffix_ to the model and the model is required to generate a piece of text that is coherent with both _prefix_ and _suffix_. Additionally, we consider imposing combinations of the following two constraints:

* **Keyphrase**: suggestions should include one to three given keyphrases.
* **Word Count**: suggestions should contain \(a\) to \(b\) words where \(1\!\!a\!\!b\!\!32\).

We consider all combinations of the following settings: insertion or continuation, w/ or w/o keyphrase constraint, w/ or w/o word-count constraint, resulting in 8 different settings. For each setting, we sample 100 stories from the CoAuthor dataset and create 100 test examples (e.g., Fig. 2).

Scaling up Ctrl-GWe adopt the TULU2-7B  model, which is an instruction-tuned variant of the Llama2  model with 7 billion parameters, as the base model for Ctrl-G. We further finetune the base model on 3000 examples extracted from the WritingPrompt dataset  for the task of text continuation, following the prompt "Continue the given text:" along with a story prefix. After finetuning, we use the same prompt to sample 5 million examples from the base model and train an HMM with 32768 hidden states (approx. 2 billion parameters). Note that for the task of text insertion, the base model _only sees the prefix_, while the suffix is incorporated as a part of the constraint \(\); i.e., the HMM is fully responsible for guiding the base model to generate a piece of text that will be coherent with the suffix. For generation, we sample 128 examples from \(p_{}\) with temperature \(0.7\) and pick the one with the highest likelihood given by the base model as the final output.

BaselinesWe compare Ctrl-G against prominent LLMs including the GPT3.5 model and the GPT4 model. To generate output from the GPT models, we adopt the prompt provided by the OpenAI documentation for text insertion/continuation, with constraints specified in the instructions. See appendix for the specific prompt templates. In addition to the GPT models, we also compare Ctrl-G against pure instruction-tuning: specifically, we construct 1000 training examples for the task of text insertion based on the WritingPrompt dataset and further finetune the TULU2-7B model for text insertion, following the prompt _"Generate the text at [INSERT_TEXT] tag:v[prefix]/[INSERT_TEXT]/[suffix]."_. For all baselines, for the purpose of fair comparison, we generate 128 samples for each test example and select the one with the highest probability as the final output.

Human evaluationTo evaluate the quality of the generated outputs, we conduct human evaluation through the Amazon Mechanical Turk (MTurk) platform. For each test example, we generate the outputs from TULU2 (prompt only), GPT3.5, GPT4 and Ctrl-G respectively, and ask annotators to rate their quality on a scale from 1 to 5. For each test example, we present the generated outputs from all models, along with their original context, to the annotators side-by-side and ask them to evaluate their quality; specifically, we ask the annotators to answer the following questions:* _Q1. is the paragraph coherent and grammatically correct?_
* _Q2. is the paragraph consistent and semantically reasonable?_
* _Q3. based on your answers to Q1&Q2, what is your rating for the overall quality?_

Note that we only ask human annotators to evaluate the coherency and fluency of the generated text and they are not aware of the required logical constraints. We ask three annotators to evaluate each output and compute their inter-annotator agreement score. See appendix for more details.

### Results

The evaluation results are summarized in Table 4, showing the quality score,5 constraint satisfaction rate, and overall satisfaction rate. In particular, the overall satisfaction rate denotes the percentage of test examples that (1) satisfy the constraint and (2) attain average quality scores\(>3\). For continuation, in terms of generation quality, GPT4 beats all other models; this is no surprise, as gigantic models like GPT3.5 (with 175B parameters) and GPT4 have significant advantage in generating high quality text continuations. However, despite the high generation quality, the success rates for GPT3.5 and GPT4 are relatively low (the highest 59%) while Ctrl-G always satisfy the specified constraints; hence in terms of the overall satisfaction rate, Ctrl-G beats all baselines by large margins when constraints are present. For the case of insertion, the "implicit" soft constraint here is that the generated parts need to be coherent with the given suffix, which is challenging for autoregressive models; in this case, in terms of pure generation quality, Ctrl-G beats/matches with the performance of GPT4 in all settings; for insertion, the success rate of all baselines becomes even lower compared to continuation, while Ctrl-G achieves 100% success rate in all settings. In terms of overall satisfaction rate, Ctrl-G again beats all baselines. The other observation is that the generation quality of GPT4 decreases as the logical constraints become more complex, while the generation quality of Ctrl-G stays relatively consistent across all settings, demonstrating strong generalization performance.

### Runtime analysis

We provide an empirical analysis on the runtime of Ctrl-G, with TULU2-7B as the base model. In addition to the computation cost of the base LLM, the major cost of Ctrl-G lies in the computation of \(p_{}(\,|\,x\!\!t)\), with a time complexity of \(O(nmh^{2})\) (Thm. 3.2); here \(n\) is the maximum sequence length, \(m\) is the size (i.e. # of edges) of the DFA, and \(h\) is the number of HMM hidden states. First, fixing the sequence length \(n\), we change the size of the DFA and verify that the time for generating each token scales roughly linearly with respect to the DFA size (Fig. 5 left). Then, fixing a DFA of

    &  &  \\   & _None_ & \(K\) & \(W\) & _K\&W_ & _Avg._ & _None_ & \(K\) & \(W\) & _K\&W_ & _Avg._ \\  _Quality_ & & & & & & & & & & & \\  TULU2 & 3.80 & 3.77 & 3.87 & 3.88 & 3.83 & 2.68 & 2.64 & 2.78 & 2.74 & 2.71 \\ GPT3.5 & 4.40 & 4.32 & **4.44** & **4.36** & 4.38 & 2.27 & 2.22 & 2.27 & 2.31 & 2.27 \\ GPT4 & **4.48** & **4.44** & **4.44** & 4.26 & **4.40** & **3.79** & 3.33 & 3.53 & 3.10 & 3.44 \\ Ctrl-G & 4.13 & 3.98 & 4.27 & 3.96 & 4.08 & **3.77** & **3.56** & **3.73** & **3.59** & **3.67** \\  _Success_ & & & & & & & & & & \\  TULU2 & - & 35\% & 33\% & 1\% & 23\% & - & 12\% & 20\% & 3\% & 12\% \\ GPT3.5 & - & 36\% & 62\% & 31\% & 43\% & - & 22\% & 54\% & 10\% & 29\% \\ GPT4 & - & 56\% & 55\% & 59\% & 57\% & - & 60\% & 20\% & 27\% & 36\% \\ Ctrl-G & - & **100\%** & **100\%** & **100\%** & **100\%** & - & **100\%** & **100\%** & **100\%** & **100\%** \\  _Overall_ & & & & & & & & & & \\  TULU2 & - & 30\% & 31\% & 1\% & 21\% & - & 7\% & 10\% & 1\% & 6\% \\ GPT3.5 & - & 36\% & 62\% & 31\% & 43\% & - & 0\% & 5\% & 2\% & 2\% \\ GPT4 & - & 56\% & 55\% & 57\% & 56\% & - & 41\% & 17\% & 14\% & 24\% \\ Ctrl-G & - & **89\%** & **97\%** & **90\%** & **92\%** & - & **76\%** & **78\%** & **82\%** & **79\%** \\   

Table 4: Evaluation results of interactive text editing. _K&W_ indicates that the model should adhere to both keyphrase (_K_) and word count (_W_) constraints simultaneously. We present the human evaluation score (_Quality_), constraint success rate (_Success_), and overall satisfaction rate (_Overall_), which represents the proportion of examples meeting logical constraints with a Quality score above 3.

size \( 900\), we change the sequence length \(n\) and measure the time for generating each token from Ctrl-G and the base LLM respectively. The gap between the two lines in Fig. 5 (right) shows the computation overhead introduced by Ctrl-G, which stays _constant_ with respect to the sequence length. On the other hand, however, due to the attention mechanism, the time for generating each token from the base LLM scales linearly with respect to \(n\). Hence, the computation cost will be dominated by the base model when generating long sequences. The runtime measurements are conducted on an NVIDIA-A100 GPU with 80GB memory.

## 6 Perspectives: improving LLM reasoning abilities via logical constraints

In this section, we explore the use of Ctrl-G on a non-traditional constrained generation application. As a case study, we apply Ctrl-G to assist the reasoning process of the TULU2-7B model on the grade school math (GSM) benchmark. As we naively apply chain-of-thought prompting, we observe that for 293 out of the 1319 test examples, the model fails to use all numbers provided in the problem statement; this leads to a much lower accuracy on the 293 examples compare to that on the complete test set. For such 293 test examples, we apply Ctrl-G to the TULU2-7B model to enforce the constraint that all numbers from the problem statement must be generated as part of the chain-of-thought reasoning process. We sample 16 outputs from the TULU2-7B model and do a majority vote; with Ctrl-G, the model achieves 28.3% accuracy, which is 3.4% higher than the marjoity-vote accuracy without Ctrl-G.

Our proof-of-concept study on the GSM benchmark illustrates one potential use case of Ctrl-G beyond traditional language generation tasks. Specifically, we demonstrate the possibility of "approximating" soft control (i.e., better reasoning ability in this setting) via logical constraints. For future work, we motivate the application of Ctrl-G, as well as other constrained generation approaches, on a broader scope of downstream tasks: e.g., helping LLM detoxification by conditioning on a set of bad words/phrases not appearing, improving the reasoning ability of LLMs by conditioning on generating longer reasoning sequences, and controlling the topic of the generated content by conditioning on the occurrence of certain keyphrases.

## 7 Conclusion

We propose Ctrl-G, a versatile framework that enables reliable and flexible inference-time control of LLMs; given any production-ready LLM, Ctrl-G distills an HMM as its approximation and uses it to guide the LLM to generate outputs that comply with any logical constraints specified as DFAs. We show that Ctrl-G, where a 7B-parameter TULU2 model is combined with a 2B-parameter HMM, beats significantly larger LLMs like GPT4 on the task of generating text insertions/continuations with logical constraints. On commonly used constrained generation benchmarks like CommonGen, Ctrl-G beats other constrained generation approaches, as well as supervised training, by large margins. In addition to the dominant paradigm of prompt engineering, our work opens up new avenues for achieving tractable, reliable and fine-grained inference-time control of LLMs.

Figure 5: Runtime analysis of Ctrl-G; Left: the generation time per token scales linearly w/ respect to DFA size. Right: the generation time per token stays constant w/ respect to sequence length.