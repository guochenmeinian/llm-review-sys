# Injecting Multimodal Information into

Rigid Protein Docking via Bi-level Optimization

 Ruijia Wang121 Yiwu Sun11 Yujie Luo11 Shaochuan Li11 Cheng Yang2 Xingyi Cheng1 Hui Li11 Chuan Shi21 Le Song11

BioMap Research

###### Abstract

The structure of protein-protein complexes is critical for understanding binding dynamics, biological mechanisms, and intervention strategies. Rigid protein docking, a fundamental problem in this field, aims to predict the 3D structure of complexes from their unbound states without conformational changes. In this scenario, we have access to two types of valuable information: sequence-modal information, such as coevolutionary data obtained from multiple sequence alignments, and structure-modal information, including the 3D conformations of rigid structures. However, existing docking methods typically utilize single-modal information, resulting in suboptimal predictions. In this paper, we propose xTrimoBiDock\({}^{}\) (or BiDock for short)4, a novel rigid docking model that effectively integrates sequence-and structure-modal information through bi-level optimization. Specifically, a cross-modal transformer combines multimodal information to predict an inter-protein distance map. To achieve rigid docking, the roto-translation transformation is optimized to align the docked pose with the predicted distance map. In order to tackle this bi-level optimization problem, we unroll the gradient descent of the inner loop and further derive a better initialization for roto-translation transformation based on spectral estimation. Compared to baselines, BiDock achieves a promising result of a maximum 234% relative improvement in challenging antibody-antigen docking problem.

## 1 Introduction

Protein-protein interactions (PPIs) are essential to the basic functioning of cells and larger biological systems. Due to their importance, elucidating such interactions up to atomic detail is necessary for understanding multicomponent complexes like ribosomes and discovering protein-based drugs, e.g., antibodies and peptides. However, the experimental golden standard for determining the structure of protein complexes, such as X-ray crystallography and cryo-EM, is extremely time-consuming.

Computational protein docking  provides an alternative route to predict the 3D structures of complexes from unbound states. Here, we focus on the fundamental problem of rigid protein docking  where no deformations occur within any protein during the docking process.

This assumption is reasonable in many biological environments and stabilizes the prediction of natural structures. Therefore, all we need is an appropriate SE(3) transformation shown in Figure 1, i.e., roto-translation transformation, that places the ligand protein at the correct orientation and location concerning the receptor protein. In this context, two types of information are available. The first is sequence-modal information, such as the coevolutionary signals captured in multiple sequence alignments (MSAs). The second is structure-modal information, like 3D coordinates of rigid bodies and bond angles. Both types of information are indispensable for performing rigid protein docking.

Classical docking software [13; 19; 46; 29; 58] generally follow a three-step framework for predicting complex structures. Firstly, a large number of candidate structures are randomly sampled to explore the conformational space of the complex. Secondly, a scoring function is employed to evaluate and rank the sampled structures based on their compatibility with the binding interface. Finally, the top-ranked structures are refined using an energy model to improve their accuracy and eliminate steric clashes. Due to the evaluation of lots of candidates, these methods tend to be computationally expensive, particularly for high-throughput workflows.

Recently, deep learning has shown significant computational speed-up in this field. EquiDock  has emerged as a pioneering method for applying deep learning to rigid protein docking. However, it solely relies on structure-modal information, neglecting the valuable sequence-modal information in databases. This drawback hampers its ability to capture evolutionary constraints and exploit the intricate sequence-structure relationships. Building upon the success of AlphaFold2 , AlphaFold-Multimer  has been hailed as a breakthrough in directly folding complex structures from amino acid sequences, which considers the sequence-modal information but fails to effectively utilize the given rigid structures, leading to unnatural complexes in some cases.

To overcome the aforementioned limitations, we propose xTrimoBiDock (or BiDock for short), a novel rigid docking model that seamlessly integrates sequence- and structure-modal information through bi-level optimization. (i) To effectively utilize multimodal information, we introduce a cross-modal transformer that injects multimodal information into an inter-protein distance map. (ii) To satisfy rigid docking, we optimize a roto-translation transformation to minimize the difference between the docked pose of unbound proteins and the predicted inter-protein distance map. This framework naturally lends itself to a nested bi-level optimization paradigm, where the outer loop is the learning of the cross-modal transformer and the inner loop is dedicated to solving the roto-translation transformation. Inspired by gradient-based bi-level optimization [24; 32; 11], we unroll the gradient descent of the inner loop for approximation. Due to the ruggedness of the optimization landscape, a substantial number of iterations are required for convergence. Thus, we further derive a better initialization for the roto-translation transformation using spectral estimation. Extensive experiments conducted on diverse datasets and evaluation protocols validate the effectiveness of BiDock.

In summary, our contributions are three-fold:

* We effectively leverage sequence- and structure-modal information for rigid protein docking. By naturally integrating the fusion of multimodalities and the docking of rigid bodies through bi-level optimization, we set up a new avenue for solving rigid protein docking.
* We solve the above bi-level optimization with unrolled gradient and spectral initialization. By unrolling the gradient descent of the inner loop and deriving a spectral estimation for initialization, BiDock enhances the convergence and controls the computational cost.
* Comprehensive experiments on three representative datasets demonstrate the effectiveness of the proposed model. Compared to state-of-the-art baselines, BiDock achieves the maximum 234% relative improvement in challenging but practical antibody-antigen docking.

Figure 1: Surface views of rigid protein docking. Keep receptor protein at a fixed location, and a roto-translation transformation \(T=(R,)\) is predicted to place ligand protein at the correct docked pose.

Related Work

Molecular DockingMolecular docking aims to characterize the binding poses between small molecule compounds and protein targets [42; 43; 63], playing an essential role in the discovery of effective and safe treatments for various diseases . Traditional computational methods have been greatly enhanced by deep learning techniques , which offer increased expressive power in identifying, processing, and extrapolating complex patterns in molecular data [39; 31; 21; 35; 4; 38; 16]. It is important to note that these methods have primarily focused on molecular ligands and often assume the availability of known binding pockets, limiting their direct applicability to protein-protein docking. Protein-protein docking is more challenging due to larger, flexible proteins and the need to predict unknown binding interfaces.

Protein DockingComputational docking software [46; 47; 44; 57; 58; 45; 15] predict complex structures based on a framework of candidate sampling, ranking [37; 6; 22], and refinement . These methods can be financially restrictive and time-consuming, as they often involve scoring and ranking thousands of candidate structures. Recently, deep learning has made significant contributions to structural biology [30; 17; 18; 36]. Notably, AlphaFold2  and RoseTTAFold  have been employed to improve protein structure prediction from various angles [26; 40], such as integrating physics-based docking methods  or extending multiple sequence alignments . Additionally, methods like AlphaFold-Multimer  and HSRN  have been developed to simultaneously fold and dock two proteins. Despite their remarkable achievements, these methods violate the rigidity of rigid docking and do not consider unbounded structures. EquiDock  is tailored for effective rigid docking but does not fully leverage the evolutionary information encoded in protein sequences, resulting in limited performance improvement compared to traditional docking software.

Bi-level OptimizationBi-level optimization has gained attention in the deep learning community for its ability to handle nested problem structures. It finds applications in various domains, including hyperparameter optimization  and metakenwedge extraction . Traditional bi-level optimization methods rely on game theory  or mathematical programming , which may not scale well to large datasets or have strict mathematical requirements. Alternatively, gradient descent methods offer a promising solution, and they can be divided into explicit gradient update , explicit proxy update [1; 8], implicit function , and closed-form methods . The first three are approximation methods suitable for general functions, while the last one is an accurate method specifically designed for certain functions. Recent surveys [33; 11] provide a more comprehensive review of these methods. In addition, the merging AI4Science directions, such as topology design  and protein representation learning , also present nested problem structures that can be compatible with bi-level optimization. By incorporating bi-level optimization into rigid protein docking, we anticipate that our work will have a significant impact at the intersection of these research areas.

## 3 BiDock Methodology

Multimodal InputThe available information on the sequence modality mainly includes information inside the primary sequence itself and co-evolutionary information from MSAs. Following the existing work [28; 23], we extract type features \(F^{typ}^{N_{res} 21}\) and primary pair features \(F^{pp}^{N_{res} N_{res} 73}\) from the primary sequence, where \(N_{res}\) is the number of residues. In terms of MSAs, we leverage cluster MSA features \(F^{msa}^{N_{cls} N_{res} 49}\), where \(N_{cls}\) is the number of cluster centers. For the structure modality, we extract angle features \(F^{ang}^{N_{res} 57}\) and pair features \(F^{p}^{N_{res} N_{res} 88}\) from the rigid protein structures. The angle features provide the orientation and position of each amino acid, while the pair features contain the distance information between amino acids. We present a brief introduction for features and give further details in Appendix B.

ArchitectureThe illustration of the framework is presented in Figure 2. Given sequence-modal features \(\{F^{typ},F^{msa},F^{pp}\}\) and structure-modal features \(\{F^{ang},F^{p}\}\), the cross-modal transformer with parameters \(\) transforms available features and integrates them to predict an inter-protein distance map \(\). To maintain the rigid-body assumption, we define the objective function of learning rotation-translation transformation \((R,)\) based on the distance map \(\) and coordinates of rigid proteins\(\{X^{3 m},Y^{3 n}\}\) as follows

\[(R,,)=_{R,}_{i=1}^{m} _{j=1}^{n}(\|X_{i}-RY_{j}-\|-_{ij}())^{2},\] (1)

where \(X_{i}\) is the \(i\)-th column of \(X\) and \(Y_{j}\) is the \(j\)-th column of \(Y\). Considering that optimizing roto-translation transformation \((R,)\) and parameters of cross-modal transformer \(\) constitutes a nested structure, we reformulate the rigid protein docking as a bi-level optimization problem

\[^{*}=*{argmin}_{}^{out}(R^{*}(), ^{*}(),)R^{*}(),^{*}()= *{argmin}_{R,}(R,,),\] (2)

where \(^{out}\) refers to the outer loss for the cross-modal transformer and its specific details will be introduced in the following subsections. To solve this bi-level optimization, we unroll the gradient descent of the inner loop to approximate \((R^{*}(),^{*}())\) by

\[ R_{t+1}&=(R_{t}, _{R}_{t}(R_{t},_{t},)),\\ _{t+1}&=(_{t}, _{}_{t}(R_{t},_{t},)), \] (3)

where \(\) represents the optimization algorithm, such as the stochastic gradient descent (SGD). However, due to the complexity of the problem and the rugged optimization landscape, the gradient descent in the inner loop often requires a large number of iterations to converge and faces challenges in finding the global minima. Therefore, we further derive spectral initialization to provide a more favorable starting point for better convergence.

### Cross-modal Transformer

Based on the multimodal features, the cross-modal transformer predicts an inter-protein distance map, as depicted in Figure 3. We first use multilayer perceptrons (MLPs) to capture the nonlinear relationship inside features and project them into the same space

\[ P^{pp}=(F^{pp}) P^{p}= (F^{p})\\ M^{typ}=(F^{typ}) M^{msa}=(F^{msa}) M^{ang}=(F^{ang}),\] (4)

where \(\{P^{pp},P^{p}^{N_{res} N_{res} c_{z}}\}\) are transformed pair features, \(\{M^{typ},M^{ang}^{N_{res} c_{m}}\}\) and \(M^{msa}^{N_{cls} N_{res} c_{m}}\) are transformed intra-sequence features. Then we add pair features as the input pair feature \(P\) of Evoformer 

\[P=P^{pp}+P^{p}.\] (5)

Similarly, we integrate intra-sequence features as another input

\[M=[(M^{typ}+M^{msa})\|M^{ang}],\] (6)

Figure 2: The overall framework of BiDock. Taking the information from sequence and structure modality as input, the cross-modal transformer fuses them and predicts an inter-protein distance map. To achieve rigid docking, the roto-translation transformation is learned by minimizing the difference between the docked pose of rigid structures and the predicted distance map. We unroll the gradient descent and further derive a spectral initialization to address the formed bi-level optimization.

where \(\|\) is the concatenation. Please note that the broadcast operation is used to make the dimensions consistent. Specifically, \(M^{typ}\) is broadcasted along the newly added first dimension during addition. Similarly, broadcasting is applied to the newly added first dimension of \(M^{ang}\) for concatenation. Through Evoformer, pair representation \(\) and evolution representation \(\) are obtained

\[,=(P,M).\] (7)

Finally, we utilize pair representation \(\) to predict the inter-protein distance map. Here we define two loss functions to supervise the learning of distance map and evolution representation. Concerning the distance map, we can directly calculate the ground truth from rigid proteins. However, this ground truth is naturally noisy because of the experimental resolution. In light of successful practice in protein structure prediction , we use a discretized distance map to replace the exact one. Specifically, distances are discretized into 64 bins ranging from 2 to 22 A. The prediction of discretized distances is converted into a classification problem by

\[_{ij}=(W(_{ij}+_{ji})),\] (8)

where \(W\) is the learnable parameter and \(\) is the activation function. Then the cross-entropy loss averaged over all residue pairs is

\[_{dist}=-_{i,j}_{k=1}^{64}_{ij}^{k} _{ij}^{k},\] (9)

where \(_{ij}^{k}\) represents the \(k\)th-element of one-hot encoding of discretized actual distance. It is worth noting that the predicted distance map \(\) can be obtained by using the mean of each bin.

Inspired by the masked language model , we leverage the evolution representation to reconstruct masked MSA values. We consider 23 classes, including 20 common amino acid types, an unknown type, a gap token, and a mask token, and introduce the mask policy in Appendix B. Thus, a masked MSA loss can be defined as follows

\[=(W),\] (10) \[_{msa}=-}_{i=1}^{N_{cls}}_{j  N_{mask}}_{k=1}^{23}A_{ij}^{k}(_{ij}^{k}),\] (11)

where \(N_{mask}\) denotes the number of masked tokens and \(A\) is the ground truth.

Similar to Equation (1), the SE(3) transformation optimized in the inner loop will exhibit differences from the ground truth complex, generating hypergradients through the cross-modal transformer

\[^{in}(R^{*}(),^{*}())=_{i=1}^{m} _{j=1}^{n}(\|X_{i}-R^{*}()Y_{j}-^{*}()\|-D_{ij} )^{2},\] (12)

Figure 3: Details on the architecture of cross-modal transformer that enables the interaction of different modalities and outputs updated representations. In particular, the pair representation is used to predict the inter-protein distance map.

where \(D\) is the ground truth distance between amino acids. Overall, the outer loss for the cross-model transformer is

\[^{out}=_{1}_{dist}+_{2}_{msa}+ _{3}^{in},\] (13)

where hyperparameters \(_{1}\), \(_{2}\) and \(_{3}\) balance the importance of different loss terms.

### Unrolled Algorithm for Hypergradient

To solve the bi-level optimization formulated as Equation (2), hyper gradient \(_{}^{out}\) is required in the outer level and can be unrolled via the chain rule

\[_{}^{out}=^{out}}{(R^{ *},^{*})}(),^{*}())}{ }+^{out}}{}.\] (14)

One method to calculate it is approximating \((R^{*}(),^{*}())\) via optimizer

\[(R_{t},_{t})=(R_{t-1},_{t-1}, ), t=1,,T,\] (15)

where \(T\) denotes the number of iterations. We explicitly calculate the gradients

\[_{ij} =X_{i}-R_{t}Y_{j}-_{t},\] (16) \[_{}_{t}(R_{t},_{t}, ) =_{i,j}_{ij}-\|_{ij}\|}{\| _{ij}\|}_{ij} Y_{j}}{ _{t}},\] \[_{}_{t}(R_{t},_{t}, ) =_{i,j}_{ij}-\|_{ij}\|}{\| _{ij}\|}_{ij},\]

where rotation matrix \(R\) and quaternion \(\) are converted to each other through the Bar-Itzhack algorithm , and \(\) means Hadamard product. After \(T\) rounds of iterations, \((R^{*}(),^{*}())\) can be approximated as \((R_{T},t_{T})\). We can compute the hypergradient by substituting

\[(),^{*}())}{}- (R_{T},_{T},)}{(R_{T}, _{T})^{}}.\] (17)

Due to the vast search space and the rugged landscape of the loss, the gradient descent algorithm with random initialization often requires a large number of iterations to converge and struggles to find the global minima. In the next subsection, we will derive a spectral initialization to enhance convergence.

### Spectral Initialization

Recall that we intend to derive a good initialization \((R_{0},_{0})\) for the gradient descent of Equation (1). To simplify it, we denote \(=RY-_{n}^{}\), where \(_{n}\) is the all-ones vector. Then substitute \(\) into

\[(\|X_{i}-RY_{j}-\|-_{ij})^{2}=X_{i}^{}X_{i}-2 X_{i}^{}_{j}+_{j}^{}_{j}+_{ij}^{2}-2_{ ij}\|X_{i}-_{j}\|.\] (18)

According to these variable combinations, we define four variables and two centering matrices

\[B=X_{1}^{}X_{1}&...&X_{1}^{}X_{1}\\ X_{2}^{}X_{2}&...&X_{2}^{}X_{2}\\...&...&...\\ X_{m}^{}X_{m}&...&X_{m}^{}X_{m}_{m n}C= _{1}^{}_{1}&...&_{m}^{}_{n}\\ _{1}^{}_{1}&...&_{n}^{}_{n}\\...&...&...\\ _{1}^{}_{1}&...&_{n}^{}_{n}_{m  n}\] (19) \[E=X^{}=X_{1}^{}_{1}&...&X_{1 }^{}_{n}\\ X_{2}^{}_{1}&...&X_{2}^{}_{n}\\...&...&...\\ X_{m}^{}_{1}&...&X_{m}^{}_{n}_{m n}F= _{11}^{}&...&_{1n}^{}\\ _{21}^{}&...&_{2n}^{}\\...&...&...\\ _{m1}^{}&...&_{mn}^{}_{m n}\]

\[H_{m}=I_{m}-J_{m} H_{n}=I_{n}-J_{n},\]

where \(I\) represents the identity matrix and \(J\) refers to the all-ones matrix. If the condition \( i,j,\|X_{i}-_{j}\|=_{ij}\) holds, the following equation exists

\[H_{m}FH_{n}=H_{m}(B-2E+C)H_{n}=-2H_{m}EH_{n}=-2H_{m}X^{}RYH_{n}.\] (20)By performing singular value decomposition (SVD) for \(H_{m}X^{}\) and \(YH_{n}\) respectively

\[H_{m}X^{}=U_{X}_{X}V_{X}^{}, YH_{n}=U_{Y}_{Y}V_{Y}^{ },\] (21)

the rotation matrix can be solved as

\[R=-V_{X}_{X}^{-1}U_{X}^{}H_{m}FH_{n}V_{Y}_{Y}^{-1}U_{ Y}^{}.\] (22)

Given the rotation matrix \(R\), the translation vector \(\) can be gotten using

\[H_{m}F =H_{m}(B-2E+C)=H_{m}B-2H_{m}X^{}RY-2H_{m}X^{} _{n}^{},\] (23) \[ =-V_{X}_{X}^{-1}U_{X}^{}H_{m}(F-B+2X^{} Y)_{n}.\] (24)

By replacing random initialization with the above spectral initialization, the gradient descent will approach the global minima faster and better. We will empirically verify this conclusion through subsequent experiments.

## 4 Experiments

DatasetsWe leverage Docking Benchmark 5.5 (DB5.5) , a gold standard dataset tailored for rigid docking. Following the experimental setting of EquiDock , the dataset is randomly partitioned into a test split of size 24. For a comprehensive comparison, we curate two datasets of antibodies (VH-VL) and antibody-antigen complexes (AB-AG) from Protein Data Bank (PDB)  and expect them to become new benchmarks. Specifically, the training set consists of 4,890 complexes containing at least one antibody chain, while the test set comprises 68 antibody-antigen complexes released after October 2022. The docking results of variable heavy-light chains and antigen-antibody can be separately evaluated. We direct the readers of interest to Appendix A for extraction criteria and detailed identifier lists. The statistics of the dataset are summarized in Table 1.

Evaluation ProtocolWe compare BiDock with two categories of representative methods, including three docking software ZDOCK , ClusPro , and HDOCK , and two deep learning models EquiDock , and AlphaFold-Multimer (Multimer for short ). To measure the quality of

  
**Datasets** & **Metrics** & **ZDOCK** & **ClusPro** & **HDOCK** & **EquiDock** & **Multimer** & **BiDock** \\   & _RMSD_\(\) & 12.491\(\)6.294 & 14.135\(\)8.153 & 11.328\(\)8.073 & 14.982\(\)3.304 & 7.797\(\)7.428 & **7.280\(\)**8.117 \\  & _TM-score_\(\) & 0.689\(\)0.114 & 0.702\(\)0.118 & 0.742\(\)0.167 & 0.714\(\)0.114 & 0.821\(\)0.162 & **0.847\(\)**0.158 \\  & _DockQ_\(\) & 0.084\(\)0.113 & 0.118\(\)0.192 & 0.314\(\)0.390 & 0.030\(\)0.029 & 0.469\(\)0.396 & **0.564\(\)**0.369 \\   & _RMSD_\(\) & 10.982\(\)3.864 & 5.899\(\)5.688 & 2.032\(\)2.388 & 18.293\(\)2.871 & 1.325\(\)0.530 & **1.242\(\)**0.602 \\  & _TM-score_\(\) & 0.596\(\)0.025 & 0.792\(\)0.156 & 0.926\(\)0.100 & 0.559\(\)0.017 & 0.962\(\)0.020 & **0.966\(\)**0.021 \\  & _DockQ_\(\) & 0.108\(\)0.134 & 0.404\(\)0.277 & 0.705\(\)0.201 & 0.032\(\)0.016 & 0.765\(\)0.094 & **0.773\(\)**0.187 \\   & _RMSD_\(\) & 18.892\(\)3.757 & 15.670\(\)6.996 & 15.779\(\)6.364 & 18.468\(\)2.706 & 13.650\(\)5.586 & **9.707\(\)**8.759 \\  & _TM-score_\(\) & 0.504\(\)0.059 & 0.596\(\)0.143 & 0.612\(\)0.137 & 0.502\(\)0.096 & 0.640\(\)0.124 & **0.773\(\)**0.187 \\   & _DockQ_\(\) & 0.035\(\)0.031 & 0.108\(\)0.181 & 0.090\(\)0.187 & 0.043\(\)0.017 & 0.108\(\)0.172 & **0.342\(\)**0.351 \\   & _maxDockQ_\(\) & 0.042\(\)0.043 & 0.136\(\)0.220 & 0.111\(\)0.237 & 0.043\(\)0.018 & 0.125\(\)0.214 & **0.414\(\)**0.386 \\   

Table 2: Quantitative results on protein docking. (bold: best; underline: runner-up)

  
**Datasets** & **\# Pairs of Proteins** & **\# Residues per Protein** & **\# Atoms per Protein** \\ 
**Training Set** & 4890 & 565.9 (\(\)264.9) & 4334.4 (\(\)2028.3) \\
**DB5.5 (Test)** & 24 & 428.4 (\(\)132.0) & 3308.0 (\(\)1000.5) \\
**VH-VL (Test)** & 68 & 230.1 (\(\)5.4) & 1749.9 (\(\)53.7) \\
**AB-AG (Test)** & 68 & 433.0 (\(\)72.0) & 3346.7 (\(\)568.8) \\   

Table 1: Statistics of datasets.

predictions, we report universally accepted metrics Root Mean Square Deviation (RMSD), TM-score (Template Modeling score), and DockQ . Please note that in the context of antibody-antigen docking, the original DockQ metric evaluates the docking performance by treating the entire antibody as a single entity to the antigen. Furthermore, we can assess the docking results separately for variable heavy/light chain (VH/VL) to the antigen and denote the maximum value as maxDockQ. Refer to Appendix B for details of experiments, including implementation and hyperparameters.

### Main Results and Analysis

Docking ResultsTable 2 demonstrates that BiDock generally produces acceptable predictions on all datasets. Notably, BiDock achieves a significant improvement in performance on the more challenging antibody-antigen docking, with a 234% relative gain over the runner-up on the DockQ metric. These results highlight the effectiveness of our bi-level optimization, which effectively leverages multimodal information. We also observe that some established docking software, such as HDOCK, still provides reliable predictions. In contrast, deep learning methods may have some leeway in performance. For instance, the mean and deviation of RMSD evaluated from EquiDock are relatively large, indicating that some inappropriate SE(3) transformations are learned.

To be more intuitive, we select representative baselines, HDOCK and Multimer, along with BiDock to analyze their performance differences on antibody-antigen docking. The distribution of DockQ for each method is displayed using a violin plot, and a direct comparison between Multimer and BiDock is also drawn in Figure 4. It reveals that the distribution of DockQ for BiDock is more concentrated around high values, indicating a higher percentage of successful docking predictions. Overall, these results suggest that our proposed BiDock is a promising approach for rigid protein docking.

VisualizationTo visually demonstrate the superiority of our proposed BiDock, we chose the spike glycoprotein \(7F6Z\) as an example antigen. The spike glycoprotein is crucial for the entry of coronaviruses into host cells, making it a target of great interest for therapeutic intervention and vaccine development. In Figure 5, we align the ground truth structure of this protein with predictions from competitive baselines. Upon inspection, it is evident that predictions from baselines exhibit noticeable deviations from the ground truth structures. Such inaccuracies can significantly impact our understanding of the binding mechanisms and hinder the design of effective interventions. In contrast,

Figure 4: An intuitive comparison of DockQ metric on antibody-antigen docking. The box inside (a) violin plot represents 25-75 percentiles, and the median is shown by a white dot. Scatters in (b) scatter plot appear under the dashed diagonal line, indicating that BiDock outperforms Multimer on these complexes.

Figure 5: Structure comparison between predictions and the ground truth of protein complex \(7F6Z\). The ground truth structures are represented in light gray, while predictions are colored cyan.

  
**Methods** & **Inference time** \\ 
**ZDOCK** & 72.61 \\
**ClusPro** & 87.27 \\
**HDOCK** & 20.40 \\
**EquiDock** & 0.60 \\
**Multimer** & 1.07 \\
**BiDock** & 1.47 \\   

Table 3: Total inference time of different methods on antibody-antigen docking. (unit: hour)BiDock successfully captures the correct docking interface and accurately predicts the binding pose, highlighting its potential for providing biological insights and aiding drug design.

Computational EfficiencyTable 3 shows the total inference time for antibody-antigen complexes. Traditional docking software, involving candidate sampling, ranking, and refinement steps, incurs substantial computational costs. Fortunately, deep learning methods provide a significant speed-up, which is particularly important in efficient screening. Although EquiDock is the fastest, its performance falls short of traditional software due to limitations in leveraging coevolution information and simple networks. On the other hand, Multimer and BiDock exhibit comparable inference times. Considering the performance improvement of our model, this trade-off is acceptable.

### Ablation Studies

Effects of Spectral InitializationRecalling our utilization of spectral estimation to derive a numerical solution for initializing the gradient descent in the inner loop, we conduct ablation experiments to demonstrate the effectiveness of this spectral initialization. Table 4 presents the results on antibody-antigen docking, where the variant without spectral initialization is denoted as "w/o SI" and the number of gradient descent steps is listed in parentheses. It is seen that the "w/o SI" variants exhibit slower convergence and lower accuracy, even when we further increase the number of gradient descent steps. These findings further support the benefits of our proposed spectral initialization in accelerating the optimization process and achieving superior results.

Effects of Bi-level OptimizationTo justify the effectiveness of bi-level optimization, we use a two-stage strategy: training outer and inner loops separately. Specifically, we use cross-entropy and masked MSA losses to train cross-modal transformer. Based on the resulting distance map, we compute roto-translation transformation with spectral initialization. The results on antibody-antigen docking are presented in Table 4, where "w/o Bi" denotes the variant without bi-level optimization (results reported from the original paper ). Results support our contributions of employing bi-level optimization for end-to-end optimization, which customizes the parameter learning of the cross-modal transformer for rigid docking.

Effects of Masked MSA LossIn addition to the essential loss for the distance map, we introduce a masked MSA loss in the outer loop to supervise the learning of evolution representations. We also conduct an ablation study to investigate its significance, as shown in Table 4. The "w/o MM" refers to models without masked MSA loss. According to the results, it can be concluded that the masked MSA loss enables the cross-modal transformer to more effectively leverage the rich evolutionary information and seamlessly integrate it with the structure-modal information.

Effects of OptimizerAs shown in Equation (15), we have the flexibility to choose different optimizers for the inner loop. To explore the impact of different optimizers on convergence speed and performance, we compare the effects of SGD and Adam optimizers. We vary the number of

    & **RMSD** & **DockQ** \\ 
**w/o SI(1)** & 14.140\(\)8.032 & 0.187\(\)0.256 \\
**w/o SI(2)** & 11.191\(\)8.754 & 0.290\(\)0.328 \\
**w/o SI(4)** & 9.806\(\)8.646 & 0.335\(\)0.347 \\
**w/o Bi(2)** & 10.090\(\)7.817 & 0.220\(\)0.232 \\
**w/o MM(2)** & 9.821\(\)8.688 & 0.330\(\)0.350 \\ 
**BiDock(2)** & **9.707\(\)**8.759 & **0.342\(\)**0.351 \\   

Table 4: Ablation studies on spectral initialization, bi-level optimization, and masked MSA loss. (number in parentheses: gradient descent steps; unit: thousand; bold: best)

    & **RMSD** & **DockQ** \\ 
**SGD(1)** & 9.939\(\)8.525 & 0.333\(\)0.341 \\
**SGD(2)** & 9.940\(\)8.557 & 0.335\(\)0.344 \\
**SGD(5)** & 9.914\(\)8.566 & 0.337\(\)0.345 \\
**SGD(10)** & 9.861\(\)8.548 & 0.337\(\)0.344 \\
**Adam(1)** & 9.780\(\)8.773 & 0.341\(\)0.350 \\ 
**Adam(2)** & **9.707\(\)**8.759 & **0.342\(\)**0.351 \\   

Table 5: Impacts of different optimizers on antibody-antigen docking. (number in parentheses: gradient descent steps; unit: thousand; bold: best)gradient descent steps and evaluate the final performance, as displayed in Table 5. With Adam, BiDock achieves faster convergence and attains higher accuracy, indicating that a better optimizer can navigate the landscape and find better minima during the optimization of the inner loop.

## 5 Conclusion

In this study, we introduce BiDock, a novel model for rigid protein docking that tackles the challenge of accurately predicting the 3D structure of protein complexes from unbound states. By formulating this problem as a bi-level optimization, BiDock combines the advantages of integrating multimodal information by a cross-modal transformer in the outer loop and maintaining the rigidity of learning roto-translation transformation in the inner loop. Additionally, we derive a spectral initialization to expedite convergence. The maximum 234% relative performance improvement validates the effectiveness of BiDock in rigid protein docking.

Limitations and Broader ImpactDespite the encouraging results, BiDock does not explicitly incorporate geometric constraints between residues when learning the distance map and does not account for potential atom clashes. Our future work will address these limitations and extend our framework to general proteins. Protein docking deepens our understanding of biological mechanisms and aids in the design of targeted interventions. This research may inspire the AI4Science community to pay more attention to the practical challenges in docking and promote further advancements in this field with significant real-world implications.