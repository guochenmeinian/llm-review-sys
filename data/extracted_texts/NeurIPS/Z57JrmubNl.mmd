# Tree-Rings Watermarks: Invisible Fingerprints for Diffusion Images

Yuxin Wen, John Kirchenbauer, Jonas Geiping, Tom Goldstein

University of Maryland

{ywen, jkirchen, jgeiping, tomg}@umd.edu

###### Abstract

Watermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called _Tree-Ring Watermarking_ that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, _Tree-Ring Watermarking_ subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID. Our watermark is semantically hidden in the image space and is far more robust than watermarking alternatives that are currently deployed. Code is available at https://github.com/YuxinWenRick/tree-ring-watermark.

## 1 Introduction

The development of diffusion models has led to a surge in image generation quality. Modern text-to-image diffusion models, like Stable Diffusion and Midjourney, are capable of generating a wide variety of novel images in an innumerable number of styles. These systems are general-purpose image generation tools, able to generate new art just as well as photo-realistic depictions of fake events for malicious purposes.

The potential abuse of text-to-image models motivates the development of _watermarks_ for their outputs. A watermarked image is a generated image containing a signal that is invisible to humans and yet marks the image as machine-generated. Watermarks document the use of image generation systems, enabling social media, news organizations, and the diffusion platforms themselves to mitigate harms or cooperate with law enforcement by identifying the origin of an image (Bender et al., 2021; Grinbaum and Adomatis, 2022).

Research and applications of watermarking for digital content have a long history, with many approaches being considered over the last decade (O'Ruanaidh and Pun, 1997; Langelar et al., 2000). However, so far research has always conceptualized the watermark as a minimal modification imprinted onto an existing image (Solachidis and Pitas, 2001; Chang et al., 2005; Liu et al., 2019; Fei et al., 2022). For example, the watermark currently deployed in Stable Diffusion (Cox et al., 2007), works by modifying a specific Fourier frequency in the generated image.

The watermarking approach we propose in this work is conceptually different: This is the first watermark that is truly invisible, as no post-hoc modifications are made to the image. Instead,the _distribution of generated images is imperceptibly modified and an image is drawn from this modified distribution._ This way, the actual sample carries no watermark in the classical additive sense, however an algorithmic analysis of the image can detect the watermark with high accuracy. From a more practical perspective, the watermark materializes in minor changes in the potential layouts of generated scenes, that cannot be distinguished from other random samples by human inspection.

This new approach to watermarking, which we call _Tree-Ring Watermarking_ based on the patterns imprinted into the Fourier space of the noise vector of the diffusion model, can be easily incorporated into existing diffusion model APIs and is invisible on a per-sample basis. Most importantly, _Tree-Ring Watermarking_ is _far more robust than existing methods_ against a large battery of common image transformations, such as crops, color jitter, dilation, flips, rotations, or noise. _Tree-Ring Watermarking_ requires no additional training or finetuning to implement, and the watermark can only be detected by parties in control of the image generation model. We validate the watermark in a number of tests, measuring negligible impact on image quality scores, high robustness to transformations, the low false-positive rate in detection, and usability for arbitrary diffusion models both with and without text conditioning.

## 2 Related Work

Diffusion ModelsDiffusion Models, arising out of the score-based generative models of the formalism of Song and Ermon (2019, 2020), are the currently strongest models for image generation (Ho et al., 2020; Dhariwal and Nichol, 2021). Diffusion models are capable of sampling new images at inference time by iteratively processing an initial noise map. The most prominent sampling algorithm in deployment is DDIM sampling (Nichol and Dhariwal, 2021) without additional noise, which can generate high-quality images in fewer steps than traditional DDPM sampling. Diffusion models are further accelerated for practical usage by optimizing images only in the latent space of a pre-trained VAE, such as in latent diffusion (Rombach et al., 2022).

Watermarking Digital ContentStrategies to imprint watermarks onto digital content, especially images, have a long tradition in computer vision. Approaches such as Boland (1996); Cox et al. (1996); O'Ruanaidh and Pun (1997) describe traditional watermark casting strategies based on imprinting a watermark in a suitable frequency decomposition of the image, constructed through DCT, DWT, Fourier-Mellin, or complex wavelet transformations. These frequency transformations all share the beneficial property that simple image manipulations, such as translations, rotations, and resizing are easily understandable and watermarks can be constructed with robustness to these transformations in mind. A fair evaluation of watermarking approaches appears in Pitas (1998); Kutter and Petitcolas (1999), which highlight the importance of measurement of false-positive rates

Figure 1: Pipeline for _Tree-Ring Watermarking_. A diffusion model generation is watermarked and later detected through ring-patterns in the Fourier space of the initial noise vector.

for each strategy and ROC-curves under attack through various image manipulations. Work continues on imprinting watermarks, with strategies based on SVD decompositions (Chang et al., 2005), Radon transformations (Seo et al., 2004) and based on multiple decompositions (Al-Haj, 2007).

Fingerprinting and Watermarking Generative ModelsThe development of modern deep neural networks opened up new possibilities for "deep" watermarking. Hayes and Danezis (2017) and Zhu et al. (2018) propose strategies to learn watermarking end-to-end, where both the watermark encoder and the watermark decoder are learned models, optimized via adversarial objectives to maximize transmission and robustness (Zhang et al., 2019). Notably these approaches still work like a traditional watermark in that the encoder imprints a post-hoc signal onto a given image - however the type of imprint is now learned. We refer to Wan et al. (2022) for an overview. A recent improvement is two-stage processes like Yu et al. (2022), where the trained encoder is used to imprint the watermark onto the training data for a generative model. This leads to a trained generative model where the watermark encoder is "baked in" to the model, making it easier to generate watermarked data. The Stable Signature of Fernandez et al. (2023), applies this idea to latent diffusion models by finetuning the latent decoder based on a pre-trained watermark encoder. Zhao et al. (2023) similarly train on watermarked data for unconditional diffusion models.

Existing image watermarking approaches first learn a watermark signal and then learn to either embed it into generated data or the generating model. This pipeline stands in contrast to watermarking approaches for language models such as Kirchenbauer et al. (2023). There, no training is necessary to generate watermark data and the output distribution of the generative model is altered to encode a watermark into generated data in a distributional sense. In the same vein, we propose an approach to alter the output distribution of diffusion models to effectively watermark their outputs. As discussed, this has a number of advantages, in comparison to related work we especially highlight that no training is necessary, that the watermark works with existing models, and that this is the first watermark that does not rely on minor modification of generated images. In this sense, this is the first watermark that is really "invisible", see Appendix Figure 6.

We note in passing that watermarking the output of generative models is not to be confused with the task of watermarking the weights of whole models, such as in Uchida et al. (2017), Zhang et al. (2018), Bansal et al. (2022), who are concerned with identifying and fingerprinting models for intellectual property reasons.

### Diffusion Models and Diffusion Inversion

We first introduce the basic notation for diffusion models and DDIM sampling (Ho et al., 2020; Song and Ermon, 2020; Dhariwal and Nichol, 2021). A forward diffusion process consists of \(T\) steps of the noise process a predefined amount of Gaussian noise vector to a real data point \(x_{0} q(x)\), where \(q(x)\) is the real data distribution, specifically:

\[q(x_{t}|x_{t-1})=(x_{t};}x_{t},_{t}),t\{0,1,...,T-1\},\]

where \(_{t}(0,1)\) is the scheduled variance at step \(t\). The closed-form for this sampling is

\[x_{t}=_{t}}x_{0}+_{t}},\] (1)

where, \(_{t}=_{i=0}^{t}(1-_{t})\).

For the reverse diffusion process, DDIM (Song and Ermon, 2020) is an efficient deterministic sampling strategy, mapping from a Gaussian vector \(x_{T}(0,\,1)\) to an image \(x_{0} q(x)\). For each denoising step, a learned noise-predictor \(_{}\) estimates the noise \(_{}(x_{t})\) added to \(x_{0}\). According to Equation (1), we can derive the estimation of \(x_{0}\) as:

\[_{0}^{t}=-_{t}}_{}(x_{t})} {_{t}}}.\]

Then, we add the estimated noise to \(_{0}\) to find \(x_{t-1}\):

\[x_{t-1}=_{t-1}}_{0}^{t}+_{t-1}} _{}(x_{t}).\]

We denote such a recursively denoising process from \(x_{T}\) to \(x_{0}\) as \(x_{0}=_{}(x_{T})\).

However, given the learned model \(_{}(x_{t})\), it is also possible to move in the opposite direction1. Starting from an image \(x_{0}\), Dhariwal and Nichol (2021) describes an inverse process that retrieves an initial noise vector \(x_{T}\) which maps to an image \(_{0}\) close to \(x_{0}\) through DDIM, where \(_{0}=_{}(x_{T},0) x_{0}\). This inverse process depends on the assumption that \(x_{t-1}-x_{t} x_{t+1}-x_{t}\). Therefore, from \(x_{t} x_{t+1}\), we follow:

\[x_{t+1}=}_{0}^{t}+_{t+1}} _{}(x_{t}).\]

We denote the whole inversion process from a starting real image \(x_{0}\) to \(x_{T}\) as \(x_{T}=_{}^{}(x_{0})\).

In this work, we re-purpose DDIM inversion \(_{}^{}\) for watermark detection. Given a generated image \(x_{0}\) with a starting noise \(x_{T}\), we apply DDIM inversion to find \(_{T}\). We empirically find DDIM's inversion performance to be quite strong, and \(_{T} x_{T}\). While it may not be surprising that inversion is accurate for unconditional diffusion models, inversion also succeeds well-enough for conditional diffusion models, even when the conditioning \(c\) is not provided. This property of inversion will be exploited heavily by our watermark below.

## 3 Method

In this section, we provide a detailed description of each layer of _Tree-Ring Watermarking_.

### Overview of _Tree-Ring Watermarking_

Diffusion models convert an array of Gaussian noise into a clean image. _Tree-Ring Watermarking_ chooses the initial noise array so that its Fourier transform contains a carefully constructed pattern near its center. This pattern is called the "key." This initial noise vector is then converted into an image using the standard diffusion pipeline with no modifications. To detect the watermark in an image, the diffusion model is inverted using the process described in Section 2.1 to retrieve the original noise array used for generation. This array is then checked to see whether the key is present.

Rather than imprint the key into the Gaussian array directly, which might cause noticeable patterns in the resulting image, we imprint the key into the Fourier transform of the starting noise vector. We choose a binary mask \(M\), and sample the _key_\(k^{*}^{|M|}\). As such, the initial noise vector \(x_{T}^{L}\) can be described in Fourier space as

\[(x_{T})_{i}k_{i}^{*}& i M\\ (0,1)&\] (2)

For reasons described below, we choose \(M\) as a circular mask with radius \(r\) centered on the low-frequency modes.

At detection time, given an image \(x_{0}^{}\), the model owner can obtain an approximated initial noise vector \(x_{T}^{}\) through the DDIM inversion process: \(x_{T}^{}=_{}^{}(x_{0}^{})\). The final metric is calculated as the L1 distance between the inverted noise vector and the key in the Fourier space of the watermarked area \(M\), i.e.

\[d_{}=_{i M}|k_{i}^{*}- (x_{T}^{})_{i}|,\] (3)

and the watermark is detected if this falls below a tuned threshold \(\). The process described above is straightforward. However, its success depends strongly on the construction of the "key" pattern, which we discuss below.

### Constructing a _Tree-Ring_ Key

We watermark images by placing a "key" pattern into the Fourier space of the original Gaussian noise array. Our patterns can exploit several classical properties of the Fourier transform for periodic signals that we informally state here.

* A rotation in pixel space corresponds to a rotation in Fourier space.
* A translation in pixel space multiplies all Fourier coefficients by a constant complex number.
* A dilation/compression in pixel space corresponds to a compression/dilation in Fourier space.
* Color jitter in pixel space (adding a constant to all pixels in a channel) corresponds to changing the magnitude of the zero-frequency Fourier mode.

A number of classical watermarking strategies rely on watermarking in Fourier space and exploit similar invariances (Pitas, 1998; Solachidis and Pitas, 2001). Our watermark departs from classical methods by applying a Fourier watermark to a random noise array _before_ diffusion takes place. Curiously, we will observe below that the invariant properties above are preserved in \(x_{T}\) even when image manipulations are done in pixel space of \(x_{0}\).

In addition to exploiting the invariances above, the chosen key should also be statistically similar to Gaussian noise. Note that the Fourier transform of a Gaussian noise array is also distributed as Gaussian noise. For this reason, choosing a highly non-Gaussian key may cause a distribution shift that impacts the diffusion model.

We consider three different types of keys, with the respective benefits of each pattern being demonstrated in subsequent experimental sections. We believe there are numerous other interesting and practical types that can be explored in future work.

_Tree-RingZeros:_ We choose the mask to be a circular region to preserve invariance to rotations in image space. The key is chosen to be an array of zeros, which creates invariance to shifts, crops, and dilations. This key is invariant to manipulations, but at the cost of departing severely from the Gaussian distribution. It also prevents multiple keys from being used to distinguish between models.

_Tree-RingRand:_ We draw the a fixed key \(k^{*}\) from a Gaussian distribution. The key has the same iid Gaussian nature as the original Fourier modes of the noise array, and so we anticipate this strategy will have the least impact on generation quality. This method also offers the flexibility for the model owner to possess multiple keys. However, it is not invariant to make image manipulations.

_Tree-Ringing:_ We introduce a pattern comprised of _multiple rings_, and constant value along each ring. This makes the watermark invariant to rotations. We choose the constant ring values from a Gaussian distribution. This provides some invariance to multiple types of image transforms, while also ensuring that the overall distribution is only minimally shifted from an isotropic Gaussian.

### Deriving P-values for Watermark Detection

A key desideratum for a reliable watermark detector is that it provide an interpretable P-value that communicates to the user how likely it is that the observed watermark could have occurred in a natural image by random chance. In addition to making detection results interpretable, P-values can be used to set the threshold of detection, i.e., the watermark is "detected" when \(p\) is below a chosen threshold \(\). By doing so, one can explicitly control the false positive rate \(,\) making false accusations statistically unlikely.

To this end, we construct a statistical test for the presence of the watermark that produces a rigorous P-value. The forward diffusion process is designed to map images onto Gaussian noise, and so we assume a null hypothesis in which the entries in the array \(x^{}_{T}\) obtained for a natural image are Gaussian. We find that this assumption holds quite well in practice, see Figure 7.

For any test image \(x^{}_{0}\), we compute the approximate initial vector \(x^{}_{T}\) and then set \(y=(x^{}_{T})\). We then define the following null hypothesis

\[_{0}:y}(,^{2}I_{}).\] (4)

Here, \(^{2}\) is an unknown variance, which we estimate for each image2 using the formula \(^{2}=_{i M}|y_{i}|^{2}\). To test this hypothesis, we define the score

\[=}_{i M}|k^{*}_{i}-y_{i}|^{2}.\] (5)

[MISSING_PAGE_FAIL:6]

also a watermarked image subjected to a transformation. For each image, we report a P-value. As expected, these values are large for non-watermarked images, and small (enabling rejection of the null hypothesis) when the watermark is present. Transformations reduce the watermark strength as reflected in the increased P-value.

## 4 Experiments

We perform experiments on two common diffusion models to measure the efficacy and reliability of the _Tree-Ring Watermarking_ technique across diverse attack scenarios. Furthermore, we carry out ablation studies to provide an in-depth exploration of this technique.

### Experimental Setting

We employ Stable Diffusion-v2 (Rombach et al., 2022), an open-source, state-of-the-art latent text-to-image diffusion model, along with a \(256 256\) ImageNet diffusion model3(Dhariwal and Nichol, 2021). In the main experiment, we use \(50\) inference steps for generation and detection for both models. For Stable Diffusion, we use the default guidance scale of \(7.5\), and we use an empty prompt for DDIM inversion, emulating that the image prompt would be unknown at detection time. The watermark radius \(r\) we use is \(10\). Later, we conduct more ablation studies on these important hyperparameters. All experiments are conducted on a single NVIDIA RTX A4000.

In our comparative analysis, we consider four baselines: two training-free methods, DwtDct and DwtDctSvd (Cox et al., 2007); a pre-trained GAN-based watermarking model, RivaGAN (Zhang et al., 2019; Goodfellow et al., 2014); and a pre-trained watermarking VAE decoder, used exclusively for Stable Diffusion model experiments, named Stable Signature (Fernandez et al., 2023). However, these baseline methods are designed for steganography, which conceals a target bit-string within an image. To ensure a fair comparison with our exclusively watermarking method, we employ the distance between the decoded bit-string and the target bit-string (Bit Accuracy) as the measurement metric. The approach of Cox et al. (2007) is currently deployed as a watermark mechanism in Stable Diffusion4.

### Benchmarking Watermark Accuracy and Image Quality

To benchmark the effectiveness of the watermark, we primarily report the area under the curve (**AUC**) of the receiver operating characteristic (ROC) curve, and the True Positive Rate when the False Positive Rate is at \(1\%\), denoted as **TPR@1%FPR**. To demonstrate the generation quality of the watermark images, we assess the Frechet Inception Distance (**FID**) (Heusel et al., 2017) for both models. Additionally, for the Stable Diffusion model, we also evaluate the **CLIP score**(Radford et al., 2021) between the generated image and the prompt, as measured by OpenCLIP-ViT/G (Cherti et al., 2022). For AUC and TPR\(@1\%\)FPR, we create \(1,000\) watermarked and \(1,000\) un watermarked images for each run. For FID, we generate \(5,000\) images for Stable Diffusion and \(10,000\) images for the ImageNet Model. The FID of Stable Diffusion is evaluated on the MS-COCO-2017 training dataset (Lin et al., 2014), and the FID of the ImageNet Model is gauged on the ImageNet-1k training dataset (Deng et al., 2009). All reported metrics are averaged across \(5\) runs using different random seeds following this protocol.

In Table 1, we present the main experimental results for Stable Diffusion and the ImageNet model. In the clean setting, all baselines except DwtDct and all _Tree-Ring Watermarking_ variants are strongly detectable. _Tree-Ring\({}_{Rand}\)_ and _Tree-Ring\({}_{Rings}\)_ show negligible impact on the FID and no impact on the CLIP score.

### Benchmarking Watermark Robustness

To benchmark the robustness of our watermark, we focus on documenting its performance under \(6\) prevalent data augmentations utilized as attacks. These include \(75^{}\) rotation, \(25\%\) JPEG compression, \(75\%\) random cropping and scaling, Gaussian blur with an \(8 8\) filter size, Gaussian noise with \(=0.1\), and color jitter with a brightness factor of \(6\). Additionally, we conduct ablation studies to investigate the impact of varying intensities of these attacks. We report both AUC and TPR\(@1\%\)FPR in the average case where we average the metrics over the clean setting and all attacks. In all ablation studies, we report the average case.

In Table 1, the baseline methods fail in the presence of adversaries. On the contrary, our methods demonstrate higher reliability in adversarial settings. Among them, _Tree-Ring\({}_{Ring}\)_ performs the best under adversarial conditions, a result of our careful watermark pattern design.

Further, we show the AUC for each attack setting in Table 2. Notably, _Tree-Ring\({}_{Zeros}\)_ demonstrates high robustness against most perturbations, except for Gaussian noise and color jitter. Similarly, _Tree-Ring\({}_{Rund}\)_ is robust in most scenarios but performs poorly when faced with rotation, as expected. Overall, _Tree-Ring\({}_{Ring}\)_ delivers the best average performance while offering the model owner the flexibility of multiple different random keys. It is worth noting that the baseline method RivaGan also demonstrates strong robustness in most scenarios, but it is important to highlight that our method is training-free and really "invisible".

### Ablation Experiments

In this section, we undertake exhaustive ablation studies with the Ring pattern on several key hyperparameters to demonstrate the efficacy of _Tree-Ring Watermarking_. Except for the ablation on attacks, the reported numbers represent averages over all attack scenarios and clean images.

**Number of Steps Used for Generation and Detection.** A key unknown variable for the model owner at the detection time is the actual number of inference steps used during the generation time.

    &  & AUC/T@1\%F & AUC/T@\(1\%\)F &  &  \\  & & (Clean) & (Adversarial) & & \\   & DwtDct & \(0.974\) / \(0.624\) & \(0.574\) / \(0.092\) & \(25.10_{0.09}\) & \(0.362_{0.000}\) \\  & DwtDctSvd & \(1.000\) / \(1.000\) & \(0.702\) / \(0.262\) & \(25.01_{0.09}\) & \(0.359_{0.000}\) \\  FID \(=25.29\) & RivaGAN & \(0.999\) / \(0.999\) & \(0.854\) / \(0.448\) & \(}\) & \(0.361_{0.000}\) \\  CLIP Score & _Tree-Ring\({}_{Zeros}\)_ & \(0.999\) / \(0.999\) & \(0.963\) / \(\) & \(26.56_{0.07}\) & \(0.356_{0.000}\) \\  \(=0.363\) & _Tree-Ring\({}_{Rand}\)_ & \(1.000\) / \(1.000\) & \(0.918\) / \(0.702\) & \(25.47_{0.05}\) & \(0.363_{0.001}\) \\  & _Tree-Ring\({}_{Ring}\)_ & \(1.000\) / \(1.000\) & \(\) / \(0.694\) & \(25.93_{1.3}\) & \(}\) \\   & DwtDct & \(0.899\) / \(0.244\) & \(0.536\) / \(0.037\) & \(17.77_{0.01}\) & - \\  & DwtDctSvd & \(1.000\) / \(1.000\) & \(0.713\) / \(0.187\) & \(18.55_{0.02}\) & - \\   & RivaGAN & \(1.000\) / \(1.000\) & \(0.882\) / \(0.509\) & \(18.70_{0.02}\) & - \\  FID \(=17.73\) & _Tree-Ring\({}_{Zeros}\)_ & \(0.999\) / \(1.000\) & \(0.921\) / \(0.476\) & \(18.78_{0.00}\) & - \\   & _Tree-Ring\({}_{Rand}\)_ & \(0.999\) / \(1.000\) & \(0.940\) / \(0.585\) & \(18.68_{0.09}\) & - \\   & _Tree-Ring\({}_{Ring}\)_ & \(0.999\) / \(0.999\) & \(\) / \(\) & \(}\) & - \\   

Table 1: Main Results. T\(@1\%\)F represents TPR\(@1\%\)FPR. We evaluate watermark accuracy in both benign and adversarial settings. Adversarial here refers to average performance over a battery of image manipulations. An extended version with additional details and standard error estimates can be found in Appendix Table 3.

   Method & Clean & Rotation & JPEG & Cr. \& Sc. & Blurring & Noise & Color Jitter & Avg \\  DwtDct & \(0.974\) & \(0.596\) & \(0.492\) & \(0.640\) & \(0.503\) & \(0.293\) & \(0.519\) & \(0.574\) \\ DwtDctSvd & \(\) & \(0.431\) & \(0.753\) & \(0.511\) & \(0.979\) & \(0.706\) & \(0.517\) & \(0.702\) \\ RivaGan & \(0.999\) & \(0.173\) & \(0.981\) & \(0.999\) & \(0.974\) & \(0.888\) & \(0.963\) & \(0.854\) \\ Stable Sig. & \(\) & \(0.658\) & \(0.989\) & \(\) & \(0.565\) & \(0.731\) & \(0.976\) & \(0.880\) \\  _{Zeros}\)_} & \(0.999\) & \(\) & \(0.984\) & \(0.999\) & \(0.977\) & \(0.877\) & \(0.907\) & \(0.963\) \\  & \(\) & \(0.486\) & \(\) & \(0.971\) & \(\) & \(\) & \( factor could potentially impact the precision of the DDIM inversion approximation of the initial noise vector. To scrutinize this, we systematically vary the number of steps for both the generation and detection time. Due to the computational demands of sampling with a high number of inference steps, we employ a total of \(400\) images for each run.

In Figure 3, we compare AUC across all step combinations. Surprisingly, even with a significant difference between the generation-time and detection-time \(\#\)steps, the decrease in AUC is minimal when the model owner uses a reasonable number of inference steps for detection without knowledge of the true generation-time steps. This indicates that the DDIM inversion maintains its robustness in approximating the initial noise vector, and is effective for watermark detection irrespective of the exact number of steps employed. Interestingly, we notice a trend where the detection power appears to be slightly stronger with fewer inference steps at detection time or a larger number of inference steps at generation time. This is an advantageous scenario as the model owner now does not actually need to carry out a large number of steps for DDIM inversion, while concurrently, the model owner (or the user) is free to choose the number of generation steps that achieve the best quality [Rombach et al., 2022].

**Watermark radii.** The radius of injected watermarking patterns is another critical hyperparameter affecting robustness and generation quality. The corresponding results are shown in Figure 4(a). As the watermarking radius increases, the watermark's robustness improves. Nevertheless, there is a trade-off with generation quality. We overall confirm a radius of \(16\) to provide reasonably low FID while maintaining strong detection power.

**Guidance scales.** Guidance scale is a hyperparameter that controls the significance of the text condition. Higher guidance scales mean the generation more strictly adheres to the text guidance, whereas lower guidance scales provide the model with greater creative freedom. Optimal guidance scales typically range between \(5\) and \(15\) for the Stable Diffusion model we employ. We explore this factor from \(2\) to \(18\) in Figure 4(b) and highlight that the strength of the guidance is always unknown during detection time. Although a higher guidance scale does increase the error for DDIM inversion due to the lack of this ground-truth guidance during detection, the watermark remains robust and reliable even at a guidance scale of \(18\). This is again beneficial for practical purposes, allowing the model owner to keep guidance scale a tunable setting for their users.

Figure 4: Ablation on Watermark Radii and Guidance Scales.

Figure 3: Ablation on Number of Generation Steps versus Detection Steps. Detection succeeds independent of the number of DDIM used to generate data.

**Attack strengths.** Further, we test out the robustness of _Tree-Ring Watermarking_ under each attack with various attack strengths. As shown in Figure 5, even with extreme perturbations like Gaussian blurring with kernel size \(40\), _Tree-Ring Watermarking_ can still be reliably detected.

## 5 Limitations and Future Work

_Tree-Ring Watermarking_ requires the model owner to use DDIM during inference. Today, DDIM is still likely the most popular sampling method due to its economical use of GPU resources and high quality. However, the proposed watermark will need to be adapted to other sampling schemes should DDIM fall out of favor. Further, the proposed watermark is by design only verifiable by the model owner because model parameters are needed to perform the inversion process. This has advantages against adversaries, who cannot perform a white-box attack on the watermark or even verify whether an ensemble of manipulations broke the watermark. However, it also restricts third parties from detecting the watermark without relying on an API. Finally, it is currently not yet clear how large the capacity for multiple keys \(k^{*}\) would be. Would it be possible to assign a unique key to every user of the API?

The effectiveness of the proposed watermark is directly related to the accuracy of the inverse DDIM process. Future work that improves the accuracy of this inversion (Zhang et al., 2023), or utilizes invertible diffusion models as described in Wallace et al. (2022), would also improve watermarking power further.

## 6 Conclusion

We propose a new approach to watermarking generative diffusion models using minimal shifts of their output distribution. This leads to watermarks that are truly invisible on a per-sample basis. We describe how to optimally shift, so that the watermark remains detectable even under strong image manipulations that might be encountered in daily usage and handling of generated images.

## 7 Acknowledgements

This work was made possible by the ONR MURI program, the Office of Naval Research (N000142112557), and the AFOSR MURI program. Commercial support was provided by Capital One Bank, the Amazon Research Award program, and Open Philanthropy. Further support was provided by the National Science Foundation (IIS-2212182), and by the NSF TRAILS Institute (2229885).

Figure 5: Ablation on Different Perturbation Strengths.