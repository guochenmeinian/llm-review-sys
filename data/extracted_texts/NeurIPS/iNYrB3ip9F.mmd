# Learning Superconductivity from Ordered and Disordered Material Structures

Pin Chen\({}^{1}\) Luoxuan Peng\({}^{1}\) Rui Jiao\({}^{2,3}\) Qing Mo\({}^{1}\) Zhen Wang\({}^{1}\) Wenbing Huang\({}^{4,5}\)

Yang Liu\({}^{2,3}\) Yutong Lu\({}^{1}\)

\({}^{1}\) National Supercomputer Center in Guangzhou,

School of Computer Science and Engineering, Sun Yat-sen University

\({}^{2}\)Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua University

\({}^{3}\)Institute for AIR, Tsinghua University

\({}^{4}\)Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{5}\) Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China

Yutong Lu are corresponding authors.

###### Abstract

Superconductivity is a fascinating phenomenon observed in certain materials under certain conditions. However, some critical aspects of it, such as the relationship between superconductivity and materials' chemical/structural features, still need to be understood. Recent successes of data-driven approaches in material science strongly inspire researchers to study this relationship with them, but a corresponding dataset is still lacking. Hence, we present a new dataset for data-driven approaches, namely SuperCon3D, containing both 3D crystal structures and experimental superconducting transition temperature (T\({}_{c}\)) for the first time. Based on SuperCon3D, we propose two deep learning methods for designing high T\({}_{c}\) superconductors. The first is SODNet, a novel equivariant graph attention model for screening known structures, which differs from existing models in incorporating both ordered and disordered geometric content. The second is a diffusion generative model DiffCSP-SC for creating new structures, which enables high T\({}_{c}\)-targeted generation. Extensive experiments demonstrate that both our proposed dataset and models are advantageous for designing new high T\({}_{c}\) superconducting candidates.

## 1 Introduction

The pursuit of high-temperature superconductors is driven by their promising applications in efficient energy transmission, advanced electromagnetics, and quantum computing [6; 36], yet their design is hindered by the enigmatic nature of high-T\(c\) unconventional superconductivity. Although BCS theory  aids in predicting T\(c\) for conventional superconductors through first-principles calculations, these methods are computationally demanding and limited to specific materials, necessitating extensive calculations for electron-phonon coupling. Moreover, the intrinsic disorder in many superconductors poses additional challenges for atomic-level design . Such complexities highlight the need for novel approaches in superconductor research and development.

Benefiting from massive public datasets in materials science, data-driven deep learning has been instrumental in predicting material properties , synthesizing structures , and more. These methods bypass complex physical theories and are crucial in superconductor research, aiding in T\({}_{c}\) prediction models for database analysis  and inverse design models for novel structures , underscoring deep learning's impact on accelerating superconducting material discovery and design. Specially, Graph Neural Networks (GNN) have been extensively applied to model ordered crystals[61; 16; 12; 64], fewer methods exist for representing disordered crystals , despite their prevalence in nature and databases like ICSD, where over 50% of structures are disordered. Therefore, developing methods to represent disordered structures in graphs is vital, especially for superconductivity research where T\({}_{c}\) enhancement often involves doping or applying pressure.

Recently, generative model is widely used in Natural Language Processing (NLP), Computer Vision (CV) and natural science. Inspired by non-equilibrium thermodynamics, Diffusion Models (DM) currently produce State-of-the-Art proteins , molecules  as well as crystals [62; 27]. However, in the field of crystal structure generation, existing models such as CDVAE utilizes the score matching method for atom coordinates, which does not ensure the translation invariance. DiffCSP focuses on crystal structure prediction tasks, which cannot be applied to design novel periodic materials from scratch.

Given the incomplete understanding of superconducting mechanisms, a data-driven approach shows great promise. Constructing a dataset that captures the structure-to-superconductivity relationship is essential for training AI models aimed at designing superconductors. Hence, we introduce SuperCon3D, a new dataset combining crystal structures and the critical temperature T\({}_{c}\) from SuperCon and ICSD. Utilizing SuperCon3D, we have developed two deep learning models for superconductor discovery and design. We propose a transformer-based GNN, SODNet, to analyze crystal geometries, including both ordered and disordered structures, potentially screening the entire ICSD. SODNet achieves SE(3)-equivariance through irreducible representation-based vector space features. Additionally, we introduce DiffCSP-SC, a transformer-based equivariant diffusion model for inverse design, capable of generating novel high T\({}_{c}\) superconductor candidates.

The main contributions of our work can be summarized as follows:

* A new dataset SuperCon3D containing both ordered-and-disordered crystal structures and experimental superconducting critical temperature is built for the first time.
* We propose two deep learning models to showcase the possible methods for exploring Supercon3D dataset. The experimental result indicate that our proposed models outperform the existing similar methods.
* Based on our proposed models, we present a list of candidate superconductors for future experimental validation. To the best of our knowledge, this is the first report of the candidate superconductors with disordered structures based on GNN methods.

## 2 Related Work

### Superconducting Dataset

The SuperCon database encompasses around 33,000 superconductors, providing only their chemical formulas. Jarvis conducted electron-phonon coupling calculations for 1,058 materials, creating a computational database with BCS superconducting properties . However, BCS theory applies mainly to conventional superconductors, and its predicted T\({}_{c}\) values require experimental validation. The recent S2S dataset includes 1,685 entries with crystal structures and binary superconducting labels for machine learning-based discovery , but it's geared towards classification tasks. Diverging from these approaches, we constructed a dataset comprising crystal structures and experimental T\({}_{c}\) values, suited for regression-based deep learning. Additionally, 3DSC  is a dataset that includes both T\({}_{c}\) and structural information, comprising over 9,150 data entries obtained through elemental matching and manual doping. In contrast, the data in SuperCon3D is entirely derived from experimental observations in databases.

### Crystal Modeling

Crystals are typically depicted as periodic graphs with a repeating minimum unit cell in a 3D lattice. While various equivariant GNN models have been developed for ordered crystal structures [61; 64; 11], research on representing disordered crystals is limited. Disorder, as defined by Muller et al. , involves varied orientations of atoms in unit cells, categorized into substitutional and positional disorder. MEGNet models disordered sites as elemental embeddings' linear combinations , only suitable for substitutional disorder. Our work aims to establish a comprehensive method for representing disordered graphs in crystals.

### Generative Models

Drawing on the concepts of non-equilibrium thermodynamics , diffusion models create links between data and prior distributions through forward and backward Markov chains . This method has made significant strides in image generation [46; 44]. Leveraging equivariant GNNs, diffusion models efficiently generate samples from invariant distributions, finding applications in conformation generation [51; 63], ab initio molecule design , protein generation , and more. The adaptation of diffusion models for crystal generation has also gained traction recently [62; 34; 27]. In our research, we enhance diffusion generative modeling by incorporating an attention-based approach, aimed at reverse-engineering novel superconducting structures with a focus on T\({}_{c}\) properties.

## 3 Problem Formulation

### From Ordered to Disordered Structures

We represent a 3D crystal as the infinite periodic arrangement of atoms in 3D space, and the smallest repeating unit is called a _unit cell_, as shown in Fig. 1. A unit cell can be defined as \(=(,)\), where \(=[_{1},_{2},_{3}]^{3 3}\) represents a minimum unit cell matrix containing three basic vectors to represent the periodicity of the crystal, and \(=\{S_{1},,S_{N}\}\) denotes a set of \(N\) sites located in the unit cell. Specifically, a _site_ describes a composition located at a specific position, which can be further defined as a triplet \(S_{i}=(_{i},_{i},_{i})\), where \(_{i}=[_{i,1},,_{i,m_{i}}]^{m_{i} h}\) lists the \(h\)-dimension features of the atom species composing the site, \(_{i}^{m_{i}}\) describes the occupancy of each specie, and \(_{i}^{3}\) denotes the Cartesian coordinate of the site. \(m_{i}\) denotes the number of atoms in one site. Generally a crystal structure is composed of ordered sites, where \(m_{i}=1\) and \(_{i}=\), _i.e._ each site is completely formed by a single atom specie. Under the influence of factors such as doping, superconductors may exhibit a disordered structure, containing two kinds of disordered sites:

Figure 1: Illustrations of periodic disorder patterns. The dotted red lines are minimum repeated cells. Grey lines are artificial boundaries to form one possible unit cell that repeats in infinite space for the given crystal. (a)\(\)(b): An illustration of periodic substitutional disorder patterns in 2D space. In this case, a new atomic specie replaces the origin one. (c)\(\)(d): An illustration of periodic positional disorder patterns in 2D space. Here, one site occurs position shift, and break the atomic symmetry in the crystal. The crystals are 3D structures in practice, and we use illustrations in 2D for simplicity.

**Substitutional Disorder (SD)**. As illustrated in Fig. 1(a)\(\)(b), SD involves a situation where the site is occupied by more than one atomic species. Specifically, for an SD site \(S_{i}\), we have

\[_{i}>1,\\ _{i,1}_{i,2}_{i,m_{i}},\\ _{i,1}+_{i,2}++_{i,m_{i}}=1\] (1)

**Positional Disorder (PD)**. In this case, one atom in the unit cell occurs position shift as shown in Fig. 1(c)\(\)(d). For a PD site \(S_{i}\), the atomic specie \(_{i,1}\) partially locates in \(_{i}\) with its occupancy.

\[_{i}=1,\\ _{i,1}<1.\] (2)

**SD+PD (SPD)**. Specially, when \(_{i,1}+_{i,2}++_{i,m_{i}}<1\) in equation 19, both SD and PD can occur simultaneously.

Typically, there is also the occurrence of interstitial disorder. However, it was not detected in our dataset. Further details are provided in the Appendix B.1.

### Superconducting Candidates Designing

In this study, we define the design of novel superconducting candidates in two ways: the first is "known materials repurposing", where the potential superconducting candidates are screened from known structures. And the second involves designing novel material structures that are potential superconducting candidates. The specific definition of the deep learning task is as follows:

**Superconductivity Prediction Task.** The task involves predicting the T\({}_{c}\) values given the crystal structure \(\). Then, we use the predicting models to screen the big structure database to find candidate superconductors with high T\({}_{c}\) value.

**Inverse Superconductor Generation Task.** This task predicts the chemical composition \(\), the Cartesian coordinates \(\), and the lattice matrix \(\) targeted on higher T\({}_{c}\) values. To reduce the exploration space, we set \(_{i}\) = 1 to generate ordered crystals. Such method can potentially design novel high T\({}_{c}\) superconductors.

## 4 The Proposed Method

### SODNet

Regarding the importance of symmetry in 3D physics space, it is essential to respect SE(3)-equivariance conditions in neural networks to reduce the model's dependence on data. To explore

Figure 2: Illustration of graph representation and equivariant graph attention layer in SODNet. (a). Illustration of node and edge embeddings. (b). The Type-0 and Type-L features operations in equivariant graph attention mechanism. \(\) denotes addition and \(\) within a circle stands for summation over all neighbors.

the geometric structures with ordered and disordered graphs in SuperCon3D dataset, we propose SODNet, an effective architecture with SE(3)-equivariant graph attention to exploit 3D geometric content. We establish SE(3)-equivariance by utilizing equivariant features derived from vector spaces containing irreducible representations and trainable equivariant operations with the help of e3nn . The core modules of the proposed SODNet is illustrated in Fig. 2. We elaborate the details as follows.

#### 4.1.1 Disordered Graph Representation

Considering the presence of disordered structures within SuperCon3D, we design two embedding blocks aimed at enhancing the model's ability to effectively capture these disordered inputs.

**Node embedding.** In the graph network approach, we apply the k-hot embedding  as the feature vector \(_{i,k}\), which encodes the atomic property corresponding to each atom specie. To extend such scheme to disordered structures, we further represent each site \(S_{i}\) as a linear combination of atomic occupancy and atomic encoding as:

\[_{i}=_{i,1},&$ is ordered},\\ _{k}_{i,k}_{i,k},&$ is SD or SPD},\\ _{i,1}_{i,1},&$ is PD}.\] (3)

**Edge embedding**. Then, we consider 3D geometric features by incorporating interatomic distance as well as vectors \(_{ij}\) equipped with spherical harmonics as follows:

\[ =_{i}_{j}(\|_{ij}\|),\] (4) \[_{ij} =(_{i})+(_{j}),\] (5) \[_{ij} =_{f}(_{ij}_{}^{TP}(_{ij}))\] (6)

where \((\|_{ij}\|)\) is the radial distribution function (RBF) expansion for interatomic bond distance. Specially, we set \(_{i}\) and \(_{j}\) to 1 when \(i\) and \(j\) sites are ordered. The initial edges are constructed by k-nearest neighbor (kNN) methods from Yan et al. . Here, we remove the close edges when bond distance meets \(\|_{ij}\| R_{i}+R_{j}\) to avoid strong interactions caused by disordered sites, where \(R_{i}\) and \(R_{j}\) are atomic radii. \(_{ij}\) combines the features of target node \(i\) and source node \(j\) with linear layers to obtain initial message. \(\) represents an MLP. \((_{ij})\) is spherical harmonics embeddings (SH) of relative position \(_{ij}\), \(c\) is weights parametrized by \(\). Finally, we obtain \(_{ij}\) to derive non-linear messages and attention weights.

#### 4.1.2 Equivariant Graph Attention

Given \(_{ij}\) containing multiple type-L vectors, which are SE(3)-equivariant irreps features. In the context of learning on 3D atomistic graphs, it is essential that features and learnable functions exhibit SE(3)-equivariance with respect to geometric transformations acting on the position \(_{ij}\). We split \(_{ij}\) into \(_{ij}^{L}\) and \(_{ij}^{0}\). The \(_{ij}^{0}\) is scalar and independent on inputs. However, the \(_{ij}^{L}\) consists of type-L vectors, which can break equivariance. Inspired by Liao and Smidt , we apply different operations to each group of \(_{ij}\).

**Type-0 features.** Given \(_{ij}^{0}\), we adopt the leaky ReLU activation and a softmax operation for \(_{ij}\):

\[_{ij} =^{}LeakReLU(_{ij}^{0}),\] (7) \[_{ij} =)}{_{k(i)}exp(_{ik})}\] (8)

Where \(\) is a learnable vector of the same dimension as \(_{ij}^{0}\) and \(_{ij}\) is a scalar.

**Type-L features.** We perform non-linear transformation on \(_{ij}^{L}\) to obtain non-linear message:

\[_{ij} =Gate(_{ij}^{L}),\] (9) \[_{ij} =_{f}(_{ij}_{}^{TP}(_{ij}))\] (10)We apply the equivariant gate activation as Weiler et al.  and present the details in Appendix B.2. Then, the similar method is eq. 6 is used to obtain \(v_{ij}\).

Finally, \(_{ij}\) and \(v_{ij}\) are further transformed features into scalars by multiplication operation. We perform mean aggregate over all nodes to predict the T\({}_{c}\) value by:

\[T_{c}(i) =(i)|}_{j(i)}_{ij}  v_{ij},\] (11) \[T_{c} =|}_{i}T_{c}(i)\] (12)

Where \((i)\) is the neighbors on node \(i\), and \(\) denotes the set of all nodes in the graph.

### DiffCSP-SC

Based on DiffCSP , we further equip our method with superconductivity guidance for crystal generation. The original DiffCSP proposes a periodic SE(3) equivariant model to jointly optimize lattice matrix \(\) and fractional coordinates \(=^{-1}\) in a diffusion-based framework, and additionally utilizes a time-dependent guidance model  for property optimization. Here, \(\) denotes the Cartesian coordinates. We extend DiffCSP with a more powerful architecture for SuperCon3D dataset.

#### 4.2.1 Transformer-based Architecture

The denoising and guidance model of the original DiffCSP share the same architecture, which is built upon EGNN , following the standard message passing neural networks (MPNN) framework . To capture the key features related to superconductivity, we employ a transformer-based model for DiffCSP-SC. Let \(^{(s)}=[_{1}^{(s)},,_{N}^{(s)}]\) denote the node representations in the \(s\)-th layer, where \(N\) is the number of nodes. The input feature is given by \(_{i}^{(0)}=(f_{}(_{i}),f_{}(t))\), where \(f_{}\) and \(f_{}\) are the atomic embedding and sinusoidal positional encoding [56; 24], respectively. \(\) is a multi-layer perception (MLP).

The output features \(_{i}^{(s)}\) are computed by

\[_{i}^{(s)}=_{i}^{(s-1)}+_{j=1}^{N}_{ij}^{(s)}_{ij} ^{(s)}\] (13)

where \(_{ij}\) is matrix capturing the similarity between queries and keys.

\[_{ij}^{(s)}=Softmax(_{i}^{(s)}_{ij}^{(s)}}{})\] (14)

Here \(d\) is the dimension of the hidden state. The queries, keys and values of \(_{i}^{(s)}\), \(_{ij}^{(s)}\) and \(_{ij}^{(s)}\) in attention mechanism are unfolded as follows:

\[_{i}^{(s)} =_{q}(_{i}^{(s-1)}),\] (15) \[_{ij}^{(s)} =_{k}(_{i}^{(s-1)},^{},_{}(_{j}-_{i})),\] (16) \[_{ij}^{(s)} =_{v}(_{i}^{(s-1)},^{},_{}(_{j}-_{i}))\] (17)

Where \(_{q}\), \(_{k}\) and \(_{v}\) are MLPs. \(\) is the unit lattice cell. Specially, \(^{}\) is used to ensure O(3)-equivariance in diffusion step. The transform \(_{}\) is able to extract various frequencies of all relative fractional distances that are helpful for crystal structure modeling, and more importantly, \(_{}\) is periodic translation invariant, namely, \(_{}(w(_{j}+)-w(_{i}+))=_{}( _{j}-_{i})\) for any translation \(\). The part corresponding to original DiffCSP is presented in Appendix B.3.

#### 4.2.2 Improved Predictor for Evaluation

After denoising process, we need to predict the T\({}_{c}\) values of the generated samples. We adopt SODNet as an effective substitute of DFT-based predictors. Similar to CDVAE , we calculate the success rate (SR) as the proportion of optimized structures reaching the required thresholds. Given the samples \(}\), SR is defined as

\[(})=}|}},(})>P_{100-} (_{})\|}{\|}\|},\] (18)

where \(\) is the SODNet predictor and \(P_{100-}(_{})\) is the \(100-\) percentile of the \(T_{c}\) values in the training set. Similarly, we define the novelty success rate (NSR) as a metric to assess the generation of novel structures, with detailed definitions and explanations provided in the Appendix D.

#### 4.2.3 Pre-training

Considering the SuperCon3D dataset's limited structures, which doesn't fully capture the diversity in atomic species, lattice parameters, and atomic spatial distributions, we pre-trained our model on approximately 1.14 million unique 3D crystals sourced from existing databases, including Materials Project, OQMD, ICSD and Matgen.

## 5 Experiments

### Setup

#### 5.1.1 SuperCon3D dataset.

We extracted approximately 33,000 superconductors with their chemical formulas and corresponding critical temperatures from SuperCon. After removing duplicates and non-superconductors, we identified 11,949 superconducting materials. Additionally, over 200,000 ordered and disordered crystal structures were gathered from the ICSD database . We then matched these 11,949 SuperCon entries with 208,425 ICSD entries based on chemical composition, space group and lattice parameter. Moreover, T\({}_{c}\) values and structural data for hydrogen-enriched superconductors were collated from various literature sources. This process resulted in 1,578 superconductor data entries, each featuring both T\({}_{c}\) and crystal structure. To ensure the dataset's integrity, all entries were vetted by domain experts and accompanied by referenced literature. Detailed data descriptions are provided in Appendix A.

#### 5.1.2 Evaluation Metrics.

We mainly compare our proposals with other crystal property predictors and inverse crystal structure generative models. For property predicting tasks, we mainly employ Mean Absolute Error (MAE) and R-Square (R\({}^{2}\)) for T\({}_{c}\) prediction. In addition, we also use visualization and interpretable analysis to verify our model. For inverse crystal structure generative task, we calculate the success rate (SR) as the percentage of the 100 optimized structures achieving 10, 30, 50 percentiles of the superconducting property distribution.

### Experimental Results and Discussion for Superconductivity Prediction

#### 5.2.1 Comparison on Dataset.

We present a summary of comparisons with previous crystal property predictors in Table 1. SODNet consistently outperforms the other competitors both on ordered and disordered structures. For example, SODNet achieves 17.6% reduction on MAE and about 4.4% improvements on R\({}^{2}\) than the second ranked Matformer. When considering the PD disordered structure between SODNet and MEGNet, SODNet gets about 41.7% reduction on MAE and almost 66.1% improvement on R\({}^{2}\) than MEGNet. It is worth noting that when we incorporate disordered structures into the training and validation sets, the metrics of R\({}^{2}\) and MAE both show improvements, indicating that accurately representing disordered structures is beneficial for the prediction of ordered structure properties. This also means that SODNet can be further improved with larger dataset in the future work.

#### 5.2.2 Ablation Study.

We conduct ablation studies to investigate crucial factors that influence the performance of the proposed SODNet. Table 2 shows the experimental results of SODNet with disordered graph representation and equivariant graph attention. When nodes and edges are embedded without atomic occupancy, both the MAE and R\({}^{2}\) metrics exhibit a decline in performance. Among them, node embedding is more sensitive to disordered graphs, leading to almost half of the performance loss.

Additionally, if we replace the type-L layer with MLPs, the proposed model achieves worse performance, indicating that the type-L features with equivariant activation function plays a crucial role in O(3) invariance for vectors.

#### 5.2.3 Real-world Superconductors Validation

    &  &  \\  Method & Train & Test & MAE (logK)\(\) & R\({}^{2}\) \\  RF-c & O & O & 0.738\(\)0.165 & 0.711\(\)0.050 \\ SVM-c & O & O & 0.632\(\)0.094 & 0.801\(\)0.041 \\ RF-geo & O & O & 0.741\(\)0.115 & 0.759\(\)0.051 \\ SVM-geo & O & O & 0.578\(\)0.114 & 0.827\(\)0.042 \\  SchNet & O & O & 0.891\(\)0.041 & 0.401\(\)0.032 \\ CGCNN & O & O & 0.879\(\)0.047 & 0.405\(\)0.022 \\ DimeNet++ & O & O & 0.811\(\)0.058 & 0.434\(\)0.092 \\ SphereNet & O & O & 0.762\(\)0.048 & 0.467\(\)0.096 \\ ALIGNN & O & O & 0.755\(\)0.049 & 0.479\(\)0.090 \\ Matformer & O & O & 0.748\(\)0.043 & 0.570\(\)0.135 \\  MEGNet & O & O & 0.794\(\)0.006 & 0.497\(\)0.009 \\  & O/SD & O/SD & 0.889\(\)0.049 & 0.431\(\)0.058 \\  SODNet & O & O & 0.622\(\)0.112 & 0.595\(\)0.101 \\  & O/SD/PD/SPD & O & **0.584\(\)0.119** & **0.634\(\)0.117** \\  & O/SD & O/SD & **0.518\(\)0.084** & **0.716\(\)0.064** \\  & O/SD/PD/SPD & O/SD/PD/SPD & **0.505\(\)0.055** & **0.748\(\)0.032** \\   

Table 1: Predicting models performance on SuperCon3D dataset. ‘O’ indicates that using ordered data. ML models with -c and -geo denote composition and structure features.

    &  \\  Method & MAE (logK)\(\) & R\({}^{2}\) \\   \\  \(w/o\) disorder node embedding & 0.990\(\)0.033 & 0.365\(\)0.044 \\ \(w/o\) disorder edge embedding & 0.592\(\)0.087 & 0.655\(\)0.046 \\    \\  \(w/o\) & equivariant operations & 0.611\(\)0.046 & 0.618\(\)0.027 \\  SODNet & **0.505\(\)0.055** & **0.748\(\)0.032** \\   

Table 2: Ablation studies of SODNet on SuperCon3D.

   Material & O/SD/PD \(T_{c}^{exp}\) (K) \(T_{c}^{pred}\) (K) & 
 Relative \\ Error(\%) \\  \\  CaH\({}_{6}\) & O & 215  & 242.25 & 12.67 \\ Ti & O & 26  & 8.50 & 67.31 \\ CsV\({}_{3}\)Sb\({}_{5}\) & O & 2.3  & 2.36 & 6 \\ Cs(V\({}_{0.93}\)Nb\({}_{0.07}\))\({}_{3}\)Sb\({}_{5}\) & SD & 4.45  & 4.71 & 5.84 \\ Zr\({}_{4}\)Rh\({}_{2}\)O & O & 3.73  & 4.12 & 10.45 \\ Zr4Pd2O & O & 2.73  & 2.82 & 3.3 \\ LaFeSiO\({}_{0.9}\) & PD & 10  & 7.93 & 20.7 \\   

Table 3: Recently discovered superconductors (not included in the training data).

To assess the model's real-world relevance, we gathered newly discovered superconductors from the last three years, not present in our training data. Table 8 reveals that except for titanium superconductors, other materials' critical temperatures (T\({}_{c}\)) are predicted with low relative error margins (below 21%). This underscores the model's ability to predict T\({}_{c}\) values beyond its training scope, highlighting its utility in new material discovery. The outlier predictions for titanium could stem from close atomic proximities under extreme pressures (248 GPa), a condition scarcely represented in our training set. More details are presented in E.1.

#### 5.2.4 Potential Superconducting Materials.

Using our model, we screened the ICSD database to identify potential high-T\({}_{c}\) superconductors. Appendix E.2 lists 27 candidates, including cuprate, H-rich, heavy-Fermion, iron-based, and other types. The top three candidates are Ba\({}_{1.1432}\)Co\({}_{0.1429}\)O\({}_{3.0009}\)Rh\({}_{0.8574}\), ErH\({}_{3}\), and Ba\({}_{0.515}\)Ca\({}_{0.485}\), previously unreported. This is the first identification of disordered superconducting candidates from ICSD using a GNN method. Given that most ICSD structures are experimentally synthesized, these candidates are valuable for further research. Our model effectively screens disordered high-T\({}_{c}\) structures, demonstrating its usefulness. Additionally, we highlight four prime high-T\({}_{c}\) candidates with analogous parent structures in Table 10 and 11 of the Appendix E.2. In the Appendix E.3, we provide an interpretation of our SODNet predictor by identifying the features that the model prioritizes when making predictions, using the case of order-and-disorder-MgB\({}_{2}\) as an example. This analysis demonstrates SODNet's ability to capture the correlations between superconducting properties and structural characteristics.

### Experimental Results and Discussion for Inverse Crystal Structure Generation

#### 5.3.1 Comparison on Dataset.

We summarize the comparisons to previous main generative models in Table 4 and present training details in Appendix C.2. Without pretraining, CDVAE, SyMat and DiffCSP generate poor crystal structures, exhibiting extremely low SR performance. The main reason for this phenomenon may be the vast compound space of superconducting materials, making it difficult to effectively sample the atomic species and atomic spatial coordinates. The DiffCSP-SC model we propose shows a slight performance improvement compared to the two models mentioned above under the same conditions. CDVAE lacks translation invariance for atomic coordinates, which affects the quality of generated structures. This low performance metric is also observed in the DiffCSP  and aligns with our findings. Moreover, in comparison to DiffCSP, DiffCSP-SC containing an attention mechanism exhibits higher SR performance, indicating that DiffCSP may capture structural features associated with high T\({}_{c}\). We will give more discussions for DiffCSP-SC in section _Ablation Study_. Notably, DiffCSP-SC consistently achieved the highest performance across the NSR metric, with detailed results presented in the Appendix D.

#### 5.3.2 Ablation Study.

In ablation studies detailed in Table 5, we examine key components of our DiffCSP-SC model. **1.** Assessing the transformer's impact, its removal and reverting to the original DiffCSP approach led to a notable performance drop, especially in SR10 and SR30 metrics, implicating a decrease in high T\({}_{c}\) superconductor generation. This suggests that attention mechanisms in transformers effectively capture the complex atomic compositions of high T\({}_{c}\) superconductors, which often involve multi-component, multi-element structures. **2.** The pre-training methodology's significance is highlighted by its ability to manage the vast feature space of atomic species, coordinates, and unit cells in ordered crystals. Without it, as seen when training solely on a limited subset from SuperCon3D, the model's efficacy in generating valid superconductors significantly diminishes.

   &  &  \\   & & SR10 & SR30 & SR50 \\  CDVAE & O & 0.03 & 0.03 & 0.03 \\ SyMat & O & 0.03 & 0.04 & 0.04 \\ DiffCSP & O & 0.04 & 0.05 & 0.05 \\ DiffCSP-SC & O & 0.05 & 0.05 & 0.10 \\  CDVAE & Pre-training + O & 0.25 & 0.25 & 0.30 \\ SyMat & Pre-training + O & 0.28 & 0.28 & 0.35 \\ DiffCSP & Pre-training + O & 0.30 & 0.30 & 0.45 \\ DiffCSP-SC & Pre-training + O & **0.37** & **0.37** & **0.50** \\  

Table 4: Results for inverse crystal structures generation. “O” and “Pre-training” indicate models trained on SuperCon3D’s ordered structures and a collection of 1.14 million stable structures, respectively.

#### 5.3.3 Candidate Superconductors.

Utilizing our model, we aimed to generate novel superconductor candidates with high T\({}_{c}\) values. Table 12 and 13 in Appendix E.4 displays 32 potential high T\({}_{c}\) superconducting materials categorized as cuprate, H-rich, heavy-Fermion, iron-based, and other types. We initially assessed the novelty of these structures through similarity calculations with our 1.14 million-structure database. Interestingly, our findings reveal three candidates, index 8, 9, and 12, previously reported for T\({}_{c}\) using computational methods. Additionally, another candidate, index 21 and 22, demonstrated superconductivity upon doping and pressing. Subsequently, density-functional theory (DFT) were performed on selected candidates to verify their superconducting properties. Notably, Van Hove singularities (VHS) were observed in the electronic structures of Ba\({}_{2}\)CuCl\({}_{2}\)O\({}_{2}\), Lu, and BaFe\({}_{2}\)Se\({}_{2}\), as further detailed in Appendix E.5. VHS is a significant aspect in superconductivity research, often explored for its potential influence .

## 6 Conclusion and Discussion

In conclusion, a novel dataset has been constructed as a benchmark for future deep learning-based superconductivity research. Utilizing the dataset, we put forth two deep learning approaches for the design of high T\({}_{c}\) superconductors: a property prediction model for screening the known structures, and a generative model for creating the novel structures. To further validate the efficacy of the model, we apply the predicting model to screen the entire ICSD and identify a list of ordered and disorder superconducting candidates. By employing pretraining on large-scale crystal structures, we have achieved the capability to perform reverse structure design on limited superconducting data points.

Our SuperCon3D dataset, featuring experimental structures and T\({}_{c}\) values, paves the way for real-world superconductor applications. Combined with SODNet, which addresses disordered graph issues previously overlooked by the AI community, and DiffCSP-SC for novel designs. However, the accuracy of data-driven models remains constrained by the collected superconducting dataset. As Fig. 4 in the Appendix shows, data unevenness and elemental skewness (especially in Cu and O) may bias the model. Additionally, as Table 8 indicates, atomic distributions under extreme pressures contribute to predictive errors. Addressing these, Fig. 8 presents our pipeline, combining DiffCSP-SC and SODNet, to design and validate novel superconductors through wet experiments, iteratively enriching the dataset for improved model training and accuracy.

## 7 Acknowledgement

This work was jointly supported by the following projects: National Science and Technology Major Project (2022ZD0117805), the National Natural Science Foundation of China (No. 61925601, No. 62376276), Beijing Nova Program (20230484278).