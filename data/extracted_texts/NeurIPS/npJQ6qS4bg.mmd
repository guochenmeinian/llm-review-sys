# Understanding and Minimising

Outlier Features in Transformer Training

Bobby He\({}^{1}\) Lorenzo Noci\({}^{1}\) Daniele Paliotta\({}^{2}\) Imanol Schlag\({}^{1}\) Thomas Hofmann\({}^{1}\)

\({}^{1}\)Department of Computer Science, ETH Zurich

\({}^{2}\)Machine Learning Group, University of Geneva

Correspondence to bobby.he@inf.ethz.chETH AI Center

###### Abstract

Outlier Features (OFs) are neurons whose activation magnitudes significantly exceed the average over a neural network's (NN) width. They are well known to emerge during standard transformer training and have the undesirable effect of hindering quantisation in afflicted models. Despite their practical importance, little is known behind _why OFs emerge during training_, nor _how one can minimise them_. Our work focuses on the above questions, first identifying several quantitative metrics, such as the kurtosis over neuron activation norms, to measure OFs. With these metrics, we study how architectural and optimisation choices influence OFs, and provide practical insights to minimise OFs during training. As highlights, we introduce a novel unnormalised transformer block, the _Outlier Protected_ block, and present a previously unknown benefit of non-diagonal preconditioning optimisers, finding both approaches to significantly reduce OFs and improve quantisation without compromising convergence speed, at scales of up to 7B parameters. Notably, our combination of OP block and non-diagonal preconditioner (SOAP) achieves 14.87 weight-and-activation int8 perplexity (from 14.71 in standard precision), compared to 63.4 int8 perplexity (from 16.00) with a default OF-prone combination of Pre-Norm model and Adam, when quantising OPT-125m models post-training.

## 1 Introduction

Despite their widespread use, our understanding of deep neural networks (NNs) and their training dynamics is very much incomplete. This, in part, reflects the complexity of traversing high-dimensional non-convex loss landscapes but is also symptomatic of the myriad design choices, such as NN architecture and optimiser hyperparameters, that a practitioner must take before training. While standard choices of architecture and optimiser exist, it is often unclear how these choices affect model performance or the emergence of various empirically observed phenomena during NN training.

Outlier Features (OF) are one such training phenomenon. OFs are neurons whose activation magnitudes are significantly larger than average in the same layer, i.e. across NN width . They have been widely observed in the popular transformer NN architecture , as we verify in Fig 1, and are of practical interest because their existence hinders quantisation . In particular, OFs cause large dynamic ranges in activations across NN width, leading to high quantisation errors in low precision matrix multiplications. As such, Outlier Feature Emergence (OFE) during training hinders low-precision training and inference, and minimising OFE could yield significant efficiency gains.

In this paper, we tackle OFE from two related angles: by (1) proposing interventions to minimise OFE without affecting model convergence or training stability, using insights motivated through (2) enhancing our understanding of why OFs appear during training. We argue that it is important to first understand why OFs appear during standard NN training dynamics in order to identify which designchoices influence OFE, and how. Though progress has been made [1; 13; 14; 10; 15], the mechanisms behind OFE remain largely unknown.

Alongside the practical motivation of model quantisation, we believe understanding OFs, and their causes during training, to be an interesting research question for several reasons. The emergence of Outlier Features in standard transformer training regimes raises the question if OFs are simply an artifact of certain design choices or a fundamental property of transformer training, essential for best performance. Understanding (the causes of) OFE better helps us to better understand NN training dynamics in general, and the roles played by different design choices. Moreover, we shed light on the differences between models at initialisation compared to during, or after, training. While NN initialisation is more commonly studied owing to analytic tractability [17; 18; 19; 20], understanding trained NNs is arguably more important as they exhibit rich feature learning behaviour [31; 32; 33; 34], like OFs, that arise during training. In the case of OFs, this has potentially wide-reaching implications, including for NN interpretability, which often focuses on the roles of individual neurons .

Our contributionsOverall, we show that OFE can be mitigated relative to standard practices, and highlight key design choices to do so. We start by introducing OFs, and in particular quantitative metrics to measure OFs in Sec 2. In Sec 3, we study the role of normalisation layers for OFE, and find that existing hypotheses do not fully capture the OF phenomenon. We proceed to show that removing normalisation through our _Outlier Protected_ transformer block minimises OFs, without loss of convergence speed or training stability compared to standard transformer blocks. In Sec 4, we consolidate our findings by identifying signal propagation as an important object that can predict OFs during training, and that choices that improve signal propagation during training also minimise OFE. In Sec 5, we consider optimisation hyperparameters, and highlight the importance of large diagonal adaptive learning rates for OFE. Finally, in Sec 6 we demonstrate the performance of our proposals to minimise OFs both at larger scales, up to 7B parameters, and in terms of improved quantisation performance. In the interests of space, in App A.3 we discuss additional related work.

## 2 Problem Setting

Consider an activation matrix \(^{n d}\) obtained from some neural network layer, where \(n\) is the number of batch inputs/sequence positions, and \(d\) is the number of neurons across NN width. In a typical NN layer, we matrix multiply \(\) by a weight matrix \(^{d d}\) to give \(^{n d}\), with \((,j)^{}\) element: \(_{k=1}^{d}_{,k}_{k,j}\). This fundamental operation is central to NN computation and can be seen as a sum over \(d\) terms, one for each neuron.

Several works have established that if the magnitudes of the summands \(\{_{,k}_{k,j}\}_{k=1}^{d}\) have large variations, then it becomes difficult to compute their sum in low precision, thereby precluding potential efficiency gains from "vector-wise" quantised training or inference (though significant progress has been made on the latter, [8; 36; 11]). These works have shown that (pre-)trained transformer  models possess such a deficiency, which is attributed to the existence of _Outlier Features_ (OFs) whose activations are much larger in magnitude compared to the other \(d-1\) neurons.

Figure 1: Outlier Features appear in open-source transformers  during training, as measured by our Kurtosis metric Eq (1). Our work investigates the design choices that influence their emergence.

Measuring OFsExisting works have measured OFs in architecture-specific ways  or using activation scales \(\|\|_{F}^{2}}}{{=}}_{ n,j  d}_{,j}^{2}\). We argue that measuring OFs should be independent of architecture/activation scale: barring exploding/vanishing activation scales, the relative difference in summands is what causes issues for vector-wise quantisation. We use two metrics to measure OFs:

1. **Kurtosis of neuron activation RMS**: Let \(^{d}\), such that \(_{j}=_{=1}^{n}_{,j}^{2}}\), be the vector of root mean-squared activations across inputs.3 Then, let \(()\) be the ratio of the fourth moment \(m_{4}\) to the squared second moment \(m_{2}\) over the empirical distribution of \(\): 2. **Max-Median Ratio** (across neurons): A metric for OFs more aligned with the original motivation of studying variation in summand magnitudes. Specifically, we compute: 
Variants of \(()\) have previously been proposed [14; 3], but our formulation in Eq (1) aggregates activations over inputs first, which allows us to link OFs and signal propagation in Sec 4. Though we focus our analysis on \(()\), Figs 11, 13 and 17 show that both our OF metrics are highly correlated. In this work, we measure OFs on the residual stream across different layers/blocks. For example, for Pre-Norm or Post-Norm models this is \(_{}\) in the notation of App A.1.4

Experimental SetupThroughout this work, we train transformers on the next-token language modelling task, and study OFs, on a range of datasets, including: 1) CodeParrot,5 2) Languini Books , 3) BookCorpus  and English Wikipedia,6 and 4) FineWeb-Edu . Unless stated otherwise our experimental results are conducted on CodeParrot, but importantly our conclusions regarding OFs are consistent throughout across language modelling datasets. In App E.1, we also explore OFs in image classification settings with other architectures like Vision Transformers  and MLPs.

In terms of architecture and optimiser, our default choices are the Pre-Norm transformer (App A.1) and AdamW  respectively, which are known to be prone to OFs, e.g. Fig 1. Our default architecture scale has width \(d=768\) and \(6\) layers, giving around 130M parameters, but we demonstrate our findings continue to hold at larger scales (up to 7B parameters) in Secs 3 and 6. In Secs 3 and 4 we examine alternatives to the Pre-Norm architecture and their effects on OFs, keeping AdamW as optimiser, while from Sec 5 onwards we additionally consider modifications to AdamW. Further experimental details and results beyond the main paper can be found in Apps D and E respectively.

## 3 Normalisation Layers and Outlier Features

Several works have highlighted the architectural choice of _Layer Normalisation_ (LN)  as a cause of OFE [1; 7; 15]. LN belongs to a family of normalisation (Norm) layers commonly used in sequence models, which normalise a representation vector \(^{d}\) across the width dimension independently for different sequence positions. In general, for a centring scalar \(c\{0,1\}\), a Norm layer maps \(\) to:

\[()=-c()}{()} {}+,()=_{i=1}^{d}_{i}, ()^{2}=_{i=1}^{d}(_{i}-c())^{2}\] (3)

LN is when \(c=1\), with a trainable scale \(\) and bias \(\) vectors initialised to all \(1\)s and \(0\)s respectively.

Previous works have attributed OFE to the \(,\) parameters of LN incurring outliers during training . It is therefore natural to ask if simpler Norms with different formulations of Eq (3) remove OFE. In particular, _Root Mean Square Normalisation_ (RMSNorm)  is a commonly used Norm known to be as performant as LN in Transformer training . Compared to LN, RMSNorm fixes the bias \(=0\) and removes the centring by setting \(c=0\), which highlights that centring is not a crucial operation in modern sequence modelling practices. One step further would be to remove trainable parameters entirely by fixing \(=1\), thus simply projecting \(\) to the hypersphere of norm \(\). This is dubbed _Simple RMSNorm_ (SRMSNorm) by Qin et al. , who find that SRMSNorm has minimal performance degradation but is more computationally efficient than LN and RMSNorm.

We compare these different Norms in Fig 2, where we see that independent of Norm choice, all Pre-Norm transformers incur OFE: the peak kurtosis during training across Norms is over 4 orders of magnitude larger than initialisation. We also show OFE not only in Pre-Norm  but also Post-Norm  blocks (more details on transformer blocks in App A.1), highlighting OFE occurs independent of where Norms are placed. In this experiment, the Pre-SRMSNorm model has highest Kurtosis, despite its lack of trainable Norm weights.

Having established that removing trainable weights in Norms still results in OFE, the next question we ask is: _how does removing standard Norms entirely influence Outlier Feature emergence_?

Recovering training benefits in unnormalised TransformersThis is a challenging question to answer, not least because comparing OFE in architectures that converge at different speeds may not be a fair comparison: Norms are well known to be an important component in most NN architectures, providing various benefits for initialisation, convergence speed, and training stability. Thus, to answer the above question, we must first review different hypotheses for the benefits of Norms in transformer training dynamics in order to motivate a novel transformer block that matches the Pre-Norm block in convergence speed, while eschewing standard Norm layers.

Several works  have observed that the initialisation benefits of Pre-Norm architectures can be recovered in unnormalised residual models using downweighted residual branches, through a theory known as Signal Propagation (Signal Prop) . Notably, Brock et al.  achieve state of the art performance on the ImageNet benchmark using unnormalised convolutional architectures. However, it has been observed that fixing Signal Prop at initialisation is not sufficient to fully capture the benefits of Norms for training dynamics in unnormalised transformers , which implies that Norms have training benefits specific to the self-attention based transformer model.

At the same time, Zhai et al.  show _Entropy Collapse_, where the Stochastic attention matrix has rows with low entropy and each sequence position attends to only one position instead of many, to be a key transformer training instability (see Eq (10)). Entropy collapse occurs because large attention logits saturate the softmax, and several _Entropy Regulation_ (EntReg) mechanisms have been proposed to control the attention logits and thus prevent entropy collapse. Existing entropy regulating methods include QK-Norm , _tanh_ thresholding (Grok-1), \(\)Reparam  and clamping the QK logits (DBRX). In standard Pre/Post-Norm attention blocks, a Norm layer appears before Query and Key weights and implicitly regulates attention entropy, to an extent.

Figure 3: The Outlier Protected Transformer Block. We remove Pre-Norms and replace them with an Entropy Regulation mechanism to prevent entropy collapse, as well as downscaling residuals with \(<1\).

Figure 2: Kurtosis becomes large (i.e. OFE) when training with different Norms at 130M scale. We plot the residual stream entering the 2nd of 6 blocks. Other layers in Fig 14.

Our key insight is to combine ideas from Signal Propagation and Entropy Collapse prevention to remove Normalisation layers while keeping their training benefits. This brings us to our _Outlier Protected_ (OP) Block, Fig 3, which replaces the Pre-Norm block by removing its normalisation layers in both Attention and MLP sub-blocks, and making three additional changes: 1) downweighting residual branches with some \(=O(1/})<1\) to recover Signal Prop benefits of Pre-Norms , 2) adding an Entropy Regulation mechanism to prevent Entropy Collapse; we mainly use QK-Norm as it is relatively simple and performed well in all of our settings, but present experiments with tanh in App E.2, and 3) (optionally) scaling the inputs before the MLP nonlinearity by a scalar \(\) to ensure the nonlinearity inputs are of order 1, as derived by Brock et al.  using straightforward Signal Prop arguments. App B presents a mathematical description of the OP block.

In Tab 1, we show that our Outlier Protected block matches the standard Pre-LN block in terms of convergence speed at scales up to 1.2B parameters when trained with next token prediction on the Languini books dataset  for nearly 4.5B tokens.7 In App E.2, we ablate our OP block and show that the lack of an entropy regulation mechanism without normalisation layers causes training instabilities. This demonstrates that preventing entropy collapse is necessary to match training stability and convergence speed in unnormalised Transformers.

We note that independent of OFs, the OP block is interesting in its own right because it shows that the initialisation-time Signal Prop and Entropy Collapse benefits of Norms in Transformers can be disentangled, and also reveals what was missing in previous methods that used Signal Prop arguments to correct initialisation defects in simplified unnormalised Transformers . However, we now focus on the benefits of the Outlier Protected block in reducing outlier features.

Removing Norms mitigates Outlier FeaturesIn Fig 2 we see that the Outlier Protected (OP) block greatly reduces OFE compared to standard blocks. Fig 4 presents the corresponding plots in our 1.2B parameter experiments using our kurtosis metric, for different layers. We draw several consistent conclusions: 1) peak kurtosis across the course of training is consistently higher in Pre-LN, sometimes by over 2 orders of magnitude, across different layers; 2) kurtosis across training is usually higher in Pre-LN (up to 4 orders of magnitude here), especially at early training times and in earlier layers; 3) OFE (measured via our metrics) does not need to be monotonic in training time. Together, these findings suggest that the OP block will lead to more quantisable models compared to standard Pre-Norm, as we will show in Sec 6. Tab 4 ablates the effect of Norm positioning on OFE.

Nevertheless, we observe in Fig 4 that kurtosis still slightly increases in our OP blocks (to relatively modest values, maximum around 20), usually monotonically throughout training. Moreover, the question of why normalisation layers cause outlier features is still unanswered despite the clear evidence that removing them mitigates OF prevalence. We investigate these questions next.

   Params & Block & Eval PPL \\ 
100M & Pre-LN & 19.1 \\  & OP & 18.9 \\ 
320M & Pre-LN & 16.2 \\  & OP & 16.2 \\ 
1.2B & Pre-LN & 13.9 \\  & OP & 13.9 \\   

Table 1: OP matches Pre-LN performance at scales up to 1.2B params, on Languini Books .7

Figure 4: Our OP block mitigates OFE. We plot activation kurtosis of the residual stream across layers. Experiments are at 1.2B scale on Languini Books using a max AdamW learning rate of \(0.001\) with linear warmup for the first 1.5% steps and linear decay thereafter. Notice the shared log-scaled y-axis: activation kurtosis is consistently (up to 4 orders of magnitude) lower in OP block, particularly in earlier layers. Also, peak kurtosis during training is always higher in Pre-LN. The OP model also removes the final LN before unembedding; the effect of the final LN on OFE is shown in Fig 10.

See 3 key relationships between models and OFE.

* OFE still occurs for weight-less or uncentred Norms, & both Pre/Post-Norm (Figs 2, 14 and 17).
* The OP Block (Fig 3) matches Pre-LN training speed/stability (Tabs 1 and 3), without standard Norms. It does so through an Entropy Regulation method to prevent attention entropy collapse.
* The OP Block greatly reduces OFE compared to standard blocks (Figs 2, 4 and 13).

## 4 Signal Propagation and Outlier Features

To better understand why OFs still appear (albeit greatly reduced) in the OP block, and why normalisation layers cause OFs, we examine _Signal Propagation_ behaviour during training and its effect on OFE. This will also clarify why modifications that improve Signal Propagation reduce OFE . Signal Propagation [17; 18; 56; 23; 25; 27; 28] studies the _input-wise_ Gram matrix \(_{}=^{}^{n n}\), and how \(_{}\) evolves in deep NNs for different layer features \(^{n d}\).

On the other hand, as we will see below, our kurtosis metric is related to the _feature-wise_ Gram matrix \(_{}}}{{=}}^{ }^{d d}\). Recall our kurtosis is a normalised \(4^{}\) moment of \(\), normalised by the square of the second moment \(m_{2}()=_{ n,j d}_{, j}^{2}=\|\|_{F}^{2}\). Because kurtosis is scale-invariant we can consider the setting where \(m_{2}()=1\) and the average squared activation is \(1\) without loss of generality.8 In this case, \((_{})=(_{})=nd\) by the cyclic trace property.

Then, our kurtosis, Eq (1), is \(()=_{j=1}^{d}(_{ =1}^{n}_{,j}^{2})^{2}=_{j=1}^{d}( _{})_{j,j}^{2}\), which is simply a second moment (or average squared value) of diagonal entries of the feature-wise Gram matrix \(_{}\). At the same time, again by the cyclic property of the trace, we have:

\[(_{}_{}) =(^{}^{}) =(^{}^{})= (_{I}_{I})\] (4) \[ n^{2}d()+_{i,j d;i j}( _{})_{i,j}^{2} =_{, n}(_{})_{, }^{2}\] (5)

In words, Eq (4) tells us that the sum of squared elements of \(_{}\) is equal to the sum of squared elements of \(_{}\). On the left of Eq (5) we decompose Eq (4) into our feature-wise kurtosis (Eq (1), of interest for OFE), plus the sum of squared off-diagonal elements of \(_{}\), equal to the sum of squared elements of \(_{}\) on the right. Hence, it is clear that Signal Propagation is relevant for OFE. Contrary to most existing works in Signal Propagaton, Eq (5) is true throughout training, not only at initialisation.

In particular, we see that the right-hand side of Eq (5) captures both the (normalised) activation norms across inputs \(_{ n}(_{})_{,}^{2}\) from the diagonal terms, and inner products between inputs \(_{, n;}(_{})_{ ,}^{2}\) in the off-diagonals. If \(\) is the output of a Norm layer, then \(_{}\) becomes a cosine similarity matrix with diagonals equal to 1. Deep NNs, and Transformers in particular, are well known to be susceptible to a particular Signal Prop defect called _rank collapse_[60; 25] where this cosine similarity matrix \(_{}\) degenerates to the all ones matrix and all inputs look identical to a deep layer. Noci et al.  and He et al.  demonstrate that, at least at initialisation, the off-diagonals of \(_{}\) are positive and increase monotonically with depth in deep Transformers towards rank collapse, even with Signal Prop inspired modifications that ensure a non-degenerate deep limit exists.

Bad Signal Prop encourages OFEFor OFE, the upshot of these observations is that poor Signal Propagation (in terms of large off-diagonal values of \(_{}\), close to rank collapse) will make the right-hand side of Eq (5) large (the rank collapsed limit has RHS \(n^{2}d^{2}\), compared to \(nd^{2}\) when the inputs are orthogonal and \(_{}\) is diagonal). In turn, this puts pressure on the LHS, which contains the feature kurtosis, to be large, hence OFE. This argument is not fully rigorous because the off-diagonals \(_{i,j d,i j}(_{})_{i,j}^{2}\), which captures correlations between different neuron features, could increase on the LHS to allow the kurtosis to remain low.9 Theoretically predicting this behaviour deep into modern NN training is outside the scope of this work; we note that while it is possible to write down training trajectories in feature learning regimes [33; 34], most works interpreting feature learning in NNs focus only on a single gradient step . Having said that, we formalise the intuition of bad Signal Prop leading to larger feature kurtosis in the context of Gaussian features in Prop G.1.

In any case, we can empirically study the link between bad signal propagation and OFEs, which we do in Figs 5 and 10 for Pre-LN & OP blocks trained with AdamW at 1.2B scale on Languini Books. We plot both the layerwise evolution of the kurtosis on the left and the average off-diagonal entry of \(_{}=^{}\) (i.e. the average input-wise correlation) on the right, normalised so that \(m_{2}()=1\).

As suggested by Eq (5), we see a strong association between kurtosis and Signal Propagation: the layers with larger kurtosis tend to be the ones with larger input correlations, and vice versa. In particular, in Fig 5, we see that the Pre-LN layer (2 in this case) with the most extreme OFE (kurtosis peaking over 1000) is precisely the one with the worst Signal Propagation closest to rank collapse (average input correlation peaking over 0.8) during training. Moreover, the trajectory of kurtosis closely tracks the trajectory of input correlations throughout training, with their peaks appearing at similar training steps, across layers. Fig 12 shows that the Adam-trained Pythia models  are very close to rank collapse, which partially explains their large OFs in Fig 1.

Given that Signal Propagation characteristics during training depict how a model creates structure (through increasing or decreasing the inner product for different inputs) in its layer representations to best learn the task at hand, our results suggest that OFs occur partly due to the inherent nature of the task that the model is trained on, particularly in architectures that are less prone to OFs, such as our OP block. In Transformers, this appears most apparent in the inputs to the final unembedding layer, which are linearly projected to the predictions: they tend to have similar kurtosis levels in both OP and Pre-Norm blocks, and the most extreme OFE rarely occurs in the final layers, (Figs 1, 4, 14 and 18). We hypothesise this is because extreme OFE in late layers would imply high kurtosis which could imply representations close to rank collapse by Eq (5), from which it may be hard to learn useful linear predictions with optimisers like Adam.

The correlation we identify between OFE and Signal Propagation also allows us to observe that interventions that worsen Signal Propagation _during training_ cause increased OFE. Likewise, methods improving Signal Propagation throughout training help to mitigate OFE. This can be seen in Fig 5 for downscaled residuals, \(h(x)=x+ f(x)\) with some \( 1\), which Wortsman et al.  show improve quantisation on vision-language models. We explore this link further in terms of normalisation layers and other architectural choices inspired by Signal Prop in App C.

## 5 Optimisation Choices and Outlier Features

So far, we have focused on the impact of architecture for OFE. As a result, up until now all models have been trained with AdamW  optimiser, which is an adaptive diagonal preconditioner, with default hyperparameters e.g. \(_{1}=0.9\), \(_{2}=0.999,=10^{-8}\). As OFE is a training phenomenon, it is important to also consider the role of optimsation choices, which we now explore.

Learning RatePerhaps unsurprisingly, we find that using smaller LRs leads to reduced OFE during training (Figs 6, 25 and 26), across different architectures. In these cases, slightly reducing the maximum LR in our scheduler (e.g. \(0.001 0.0003\) in Fig 6) did not lead to a loss in convergence speed (Fig 27), highlighting that one should use a smaller LR to avoid OFs if convergence is not affected.

Figure 5: Adam-trained Pre-LN layers at 1.2B scale with extreme OFE (left) are those with bad Signal Prop close to rank collapse during training (centre left). (**Right vs. left two plots**) Downweighting residual branches improves signal propagation during training and results in smaller OFs, particularly in early layers. Respective plots for OP (with & without final LN before unembedding) in Fig 10.

Figure 6: Smaller LRs lead to smaller OFs across different blocks.

AdaptivityHaving seen the importance of LR in OFE, we now assess the impact of adaptive LRs through the \(\) hyperparameter in Adam, where the Adam update is \(- m_{t}/(}+)\), \(\) is global LR, and \(m_{t}\) and \(v_{t}\) denote first and second-moments of each parameter's gradient, respectively. \(\) dampens adaptive preconditioning, with larger \(\) reducing adaptivity for parameters with smaller \(v_{t}\). In Figs 7, 29 and 9 and Tab 2 we show that increasing \(\) also reduces OFE. Thus, one should increase \(\) to reduce OFE, if convergence is not impacted (like Fig 28).

Non-Diagonal PreconditionersTo push the question of adaptivity further we consider the effect of diagonal adaptivity and OFE. First-order optimisers like AdamW  or AdaFactor  are the de-facto optimisers in deep learning, acting as _diagonal_ preconditioners where each parameter has its own adaptive learning rate to scale its gradient. On the other hand, popular second-order optimisers like K-FAC  or Shampoo  are _non-diagonal_ preconditioners acting on the full gradient. Second-order optimisers are known to converge faster _per-update_ than first-order methods, but first-order optimisers are much more widespread due to the additional overheads of non-diagonal preconditioning. We provide background on different NN optimisers in App A.2.

Recently, Vyas et al.  established a precise connection between first and second-order optimisers, namely: Shampoo  can be seen as running the diagonal AdaFactor  method after first rotating into the eigenbasis of Shampoo's preconditioner, before rotating back. This insight disentangles two effects of non-diagonal preconditioners: 1) transforming the parameter space in which one optimises via a rotation, and 2) using a diagonal optimisation method in the rotated space. Vyas et al.  use this insight to propose the SOAP optimiser, which applies AdamW in the rotated parameter space obtained from Shampoo's eigenbasis. SOAP is shown to converge slightly faster per step than Shampoo, which itself converges faster per step than AdamW/AdaFactor (verified in Fig 32).

In Fig 8, we compare OFE in popular first-order optimisers, 1) AdamW and 2) AdaFactor, to rotated non-diagonally preconditioned alternatives, 3) SOAP and 4) AdaFactor in Shampoo's eigenbasis (akin to Shampoo as shown by Vyas et al. ), trained using 130M Pre-Norm transformers on CodeParrot. We clearly see that rotating the parameter space in which one optimises, as done in SOAP/Shampoo, mitigates OFEs. This effect is independent of the diagonal preconditioner used and occurs even in spite of the Pre-Norm layers, which we know are susceptible to OFs, as in Sec 3.10 To further highlight the importance of diagonal adaptivity, in App E.1 we compare SGD to Adam on a setting where SGD can match Adam's convergence speed: image classification with an MLP. There, we again see that the non-adaptive SGD suffers less from OFs compared to the diagonal Adam.

Elhage et al.  show a similar result as Fig 8 using random rotations, while QuaRot  applies random Hadamard rotations to remove OFs for post-training quantisation (PTQ) using computational invariance. SpinQuant  extends QuaRot using learnt rotations but again only considers inference time, after OFs have emerged during training. Both QuaRot and SpinQuant incur additional overheads to apply rotations in the forward pass at inference time. In contrast, non-diagonal preconditioners optimise using "learnt" rotations that adapt during training, which leaves the forward pass intact post training and enables the improvement in convergence speed per step that Shampoo/SOAP enjoy relative to AdamW/AdaFactor (seen in Fig 32 and Tab 2). In sum, our results reveal an additional appeal of second-order optimisers: not only are they faster to converge per step, but they also lead to models that are not as prone to OFs and are thus easier to quantise, as we will see in Sec 6.

Breaking down kurtosis updatesThe findings in this section point to the importance of large diagonal adaptive LRs for OFE. This motivates us to break down the updates to kurtosis into terms of different powers in the learning rate \(\), in App F. We find that sub-leading order updates (in terms of LR) are the key driver in increasing kurtosis, providing a consistent mechanism for OFE that encapsulates our different observations concerning the roles of optimiser and architecture.

Figure 8: Diagonal optimisation on rotated parameters reduces OFs.

Figure 7: Larger Adam \(\) reduces OFs in 130M Pre-LN transformers.

See 5 Key takeways **Optimisation** Choles and **Open**.

* Large diagonal adaptive LRs can lead to Outlier Features during training (Figs 6, 25, 26 and 8).
* Reducing AdamW adaptivity via increased \(\) hyperparameter reduces OFs (Figs 7, 29 and 37).
* The non-diagonal preconditioning of second-order optimisers greatly minimises OFE (Fig 8 and Tab 2), even with OF-prone architectures like Pre-Norm.

## 6 Additional Experiments

We conclude our study with additional experiments regarding scaling and quantisation properties of our suggestions to minimise OFs. Further experimental details can be found in App D.

**Scale** Up until now, we have studied OFE and how we can minimise it at scales up to 1.2B parameters and 5B tokens. We now consider scaling both model size and training length in terms of loss performance and OFE. We compare scaling the OP and Pre-Norm blocks keeping AdamW as optimiser; it would be interesting to considering scaling experiments using second-order optimisers (in distributed settings e.g. ) in future work. The dataset is FineWeb-Edu . We warmup the LR for 5% of training steps to a maximum value (\(0.001\) and \(0.0003\) for 1.2B and 7B respectively), before cosine decay. Due to computational cost, very little hyperparameter tuning was performed with these experiments and all default hyperparameters were optimised for the Pre-Norm baseline.

So far we have studied OFE in training token counts that are relatively small compared to the "compute-optimal" Chinchilla recipe . In Fig 9 (first & third subplots), we scale the number of training tokens at 1.2B parameter scale to around 90B, which is beyond the Chinchilla prescription. We see that the OP block is indeed able to closely match Pre-Norm loss at longer token counts, but still benefits from significantly reduced OFs (at least an order of magnitude lower kurtosis throughout training) despite the aggressive AdamW LR and long training horizon.

In Fig 9 (second and fourth subplots), we additionally scale the model size to 7B parameters, a scale that has previously been hypothesised to be a sharp cutoff above which "systematic" OFs emerge . Due to cost, we only train for 6B tokens, which is more than enough for extreme OFs to emerge (over 600 kurtosis averaged across residual layers) with Pre-Norm layers. On the other hand, the OP block has peak kurtosis under 10 and yet still matches Pre-Norm loss performance at 7B scale. Although increasing AdamW \(\) from \(10^{-8}\) to \(10^{-5}\) also reduces peak kurtosis to under 10 with the OF-prone Pre-Norm model, it leads to a significant decrease in convergence speed in this setting.

**Quantisation** Returning to our original motivation, we investigate the effect of our suggested architectural and optimisation choices to minimise OFs in terms of quantisation. In Tab 2, we take the OPT-125m  setting of Bondarenko et al. , training models using AdamW in standard mixed FP16/FP32 precision on BookCorpus+Wikipedia for around 12B tokens. Post training, we quantise (PTQ) to int8 weight-and-activations, using the same quantisation recipe as . We report standard deviation over 3 seeds for PTQ, as random subsets of data are used to estimate the quantiser range.

Tab 2 compares both the standard precision perplexity (FP16/32) and also 8-bit quantised perplexity (W8A8) across architecture and optimiser choices. We additionally present our kurtosis metric, Eq (1), calculated after training and averaged across layers. We compare 3 different transformer blocks: a) standard Pre-LN, b) the Gated Attention block proposed by Bondarenko et al.  to reduce OFs, and c) our OP block, as well 5 different optimisation setups that are added one after another: 1) the default hyperparameters of , 2) removing dropout regularisation, 3) increasing the maximum LR from \(4 10^{-4} 10^{-3}\), 4) increasing AdamW \(\) from \(10^{-8} 10^{-5}\), and 5) changing AdamW to SOAP optimiser (keeping \(=10^{-8}\)). Optimiser choices 2) and 3) were designed to improve standard precision performance, albeit potentially at the detriment of quantisation performance due to increased OFs. Optimiser choices 4) and 5) were chosen to reduce OFs, from our findings in Sec 5.

Figure 9: Loss (left two plots) and kurtosis (right two plots) curves for different models trained with AdamW. Our conclusions on OFs continue to hold when scaling both token count and model scale.

As seen in Tab 2, our findings throughout the rest of our paper are validated. Firstly, our kurtosis metric to measure OFs is indeed highly correlated with _quantisation error_, which we define as the increase in perplexity from FP16/FP32 to W8A8. For example, the Pre-LN model without SOAP has consistently high kurtosis (over 25), and also consistently poor performance at W8A8 (over 45 quantisation error across all AdamW optimiser settings). Secondly, our OP block has consistently low kurtosis (below 12) compared to the other models, and this directly translates to low quantisation error (below 0.73 across all optimiser settings). This low kurtosis/quantisation error with OP block holds true even for aggressive optimiser choices, like large diagonal adaptive LRs, that improve standard precision perplexity but also increase kurtosis. Moreover, the baseline Gated Attention model of  struggles with OFs when dropout is removed and a large learning rate is used, leading to increased quantisation error (2-4 perplexity increase), but increasing AdamW \(\) as suggested in Sec 5 reduces kurtosis (29 to 16) and quantisation error to 0.76, whilst also improving FP16/32 perplexity.

Finally, changing AdamW to SOAP optimiser either dramatically reduces kurtosis (in the case of Pre-LN and OP) or matches the kurtosis reduction of increasing Adam \(\) (for Gated Attention), while also improving mixed precision performance for all models.11 This leads to the only non-catastrophic W8A8 perplexity (16.43) with Pre-LN, and the best overall W8A8 model when combining SOAP optimiser with our OP architecture (14.87 perplexity, with only 0.16 degradation from standard precision). This result highlights the combination of our architectural and optimiser suggestions for minimising OFs as a promising approach to training fast-converging and easily quantisable models.

## 7 Discussion

The goal of this work was to better understand the emergence of Outlier Features during standard NN training, and propose architectural and optimisation interventions that minimise their prevalence. On the architectural side, we have shown that Normalisation layers can have unwanted effects on OFs during training. Removing standard Norms through our Outlier Protected transformer block minimises OFs during training without loss of convergence speed or training stability. On the optimisation side, we highlight that large diagonal adaptive learning rates are crucial for OFs, and non-diagonal preconditioners offer an appealing combination of reduced OFs and improved convergence speed. We demonstrate our methods to minimise OFs are effective at scales of up to 7B parameters, and also directly translate to improved post-training quantisation performance. Overall, our results reveal the complex interactions between architecture and optimiser that lead to the OFs widely observed in LLMs. In future work, it would be interesting to consider applying our methods to minimise OFs for post-training quantisation performance at larger scales, and also low-precision training.

   Architecture & Optimiser Hyperparameters & Kurtosis & FP16/32 (\(\)) & W8A8 (\(\)) \\  Pre-LN & Default from  & 25.6 & 16.00 & 63.4\(\)50.1 \\  & \(-\)Dropout Regularisation & 46.7 & 15.53 & 105.9\(\)25.0 \\  & \(+\)Big LR (\(4 10^{-4}\)\(\)\(10^{-3}\)) & 61.4 & 15.40 & 59.0\(\)10.7 \\  & \(+\)Big Adam \(\) (\(10^{-8}\)\(\)\(10^{-5}\)) & 43.6 & 15.19 & 216.2\(\)87.5 \\  & \(+\)Non-diag Precond (SOAP) & 5.1 & 14.80 & 16.43\(\)0.12 \\  Gated Attention  & Default from  & 4.5 & 15.63 & 16.2\(\)0.09 \\  & \(-\)Dropout Regularisation & 28.8 & 15.08 & 18.7\(\)0.09 \\  & \(+\)Big LR (\(4 10^{-4}\)\(\)\(10^{-3}\)) & 28.8 & 14.99 & 17.04\(\)0.09 \\  & \(+\)Big Adam \(\) (\(10^{-8}\)\(\)\(10^{-5}\)) & 16.0 & 14.78 & 15.54\(\)0.01 \\  & \(+\)Non-diag Precond (SOAP) & 16.7 & 14.65 & 15.64\(\)0.01 \\  OP (ours) & Default from  & 3.6 & 15.64 & 16.01\(\)0.01 \\  & \(-\)Dropout Regularisation & 7.1 & 15.15 & 15.78\(\)0.03 \\  & \(+\)Big LR (\(4 10^{-4}\)\(\)\(10^{-3}\)) & 12.0 & 14.96 & 15.60\(\)0.01 \\  & \(+\)Big Adam \(\) (\(10^{-8}\)\(\)\(10^{-5}\)) & 6.0 & 14.89 & 15.62\(\)0.02 \\  & \(+\)Non-diag Precond (SOAP) & **1.2** & 14.71 & **14.87\(\)**0.01 \\   

Table 2: Average kurtosis across layers, plus standard precision (FP16/32) and quantised int8 (W8A8) perplexity of various 125m OPT models  trained on BookCorpus+Wikipedia . Kurtosis strongly correlates with int8 error across settings, and the best int8 setup combines our architectural (OP) and optimiser (SOAP) suggestions, with only 1.2 kurtosis and 0.16 perplexity increase.

## Reproducibility Statement

Our code for experiments on the CodeParrot dataset can be found at https://github.com/bobby-he/simplified_transformers.