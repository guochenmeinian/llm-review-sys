# Analyzing & Reducing the Need for

Learning Rate Warmup in GPT Training

 Atli Kosson & Bettina Messmer & Martin Jaggi

EPFL, Switzerland

firstname.lastname@epfl.ch

###### Abstract

Learning Rate Warmup is a popular heuristic for training neural networks, especially at larger batch sizes, despite limited understanding of its benefits. Warmup decreases the update size \(_{t}=_{t}_{t}\) early in training by using lower values for the learning rate \(_{t}\). In this work we argue that warmup benefits training by keeping the overall size of \(_{t}\) limited, counteracting large initial values of \(_{t}\). Focusing on small-scale GPT training with AdamW/Lion, we explore the following question: _Why and by which criteria are early updates \(_{t}\) too large?_ We analyze different metrics for the update size including the \(_{2}\)-norm, resulting directional change, and impact on the representations of the network, providing a new perspective on warmup. In particular, we find that warmup helps counteract large angular updates as well as a limited critical batch size early in training. Finally, we show that the need for warmup can be significantly reduced or eliminated by modifying the optimizer to explicitly normalize \(_{t}\) based on the aforementioned metrics.

## 1 Introduction

Neural networks are typically trained using variations of stochastic gradient descent. The weight updates \(\) have the form \(=\), where \(\) denotes the _learning rate_ and \(\) an unscaled update vector derived from the history of weight gradients. Throughout training, the learning rate is often adjusted over time \(t\) according to a _learning rate schedule_, \(=_{t}\). This schedule frequently includes an initial phase known as a learning rate warmup, where the learning rate starts low and is increased to a target value before being reduced according to a decay schedule. Both the choice of warmup and decay strategy can significantly affect the final model performance. In this work, we focus on the linear warmup introduced by Goyal et al.  for large batch size ResNet  training, which is also commonly used for transformers .

The length of the warmup is a hyperparameter that requires tuning, which is complicated by the fact that the reasons for its effectiveness are somewhat unclear. Warmup empirically helps stabilize training and allows the use of larger learning rates throughout the rest of training, which can speed up the process and provide beneficial regularization . Since the learning rate simply scales the size of the updates \(=\), warmup must achieve these effects by decreasing the size of early updates. However, it is not fully clear why this helps. _Are the initial updates too large for some reason?_ For example, we might need small \(_{t}\) values to counteract large \(\) values early in training. _How should we quantify what makes an update \(\) large? Why do large updates adversely affect training?_

In this work, we explore warmup from this perspective, focusing on GPT2  training with adaptive optimizers like AdamW  and Lion . We identify three key issues that necessitate warmup:

1. The way Adam handles momentum can lead to artificially large initial updates \(\).
2. Early updates \(\) are large compared to the initial weight magnitude of \(\) for matrices.
3. The gradients of early samples are highly correlated, limiting effective mini-batch sizes.

We demonstrate that simple modifications to the optimizer can mitigate the first two issues: eliminating the momentum bias correction in AdamW and scaling matrix updates to match their magnitude, akin to the Rotational Optimizers by Kosson et al. . We analyze the third issue in terms of the rate at which the internal neural representations of the network are changing (sometimes called feature learning). When the gradients of different samples are highly correlated these internal representations change too fast, which we conjecture can lead to issues with the non-linearities (e.g. activation functions) of the network. This can also be seen as the _critical batch size_ being too low early in training to enable the use of the peak learning rate. We derive a scaling factor based on the signal-to-noise ratio of the gradient that can help mitigate this, functioning like an automatic learning rate warmup. Alternatively, we show that using high momentum values in conjunction to the first two methods may suffice to enable efficient training without warmup.

## 2 Related Work

Learning rate warmups have been used since at least ResNet , where a lower constant learning rate was applied at the start of training. Earlier works may have employed similar concepts; for example, Sutskever et al.  utilized a momentum schedule that could induce a similar effect in the "effective learning rate" as defined by Fu et al. . The practice of linear warmup in its current form was popularized by Goyal et al.  and Vaswani et al. .

Warmup has been studied indirectly in various neural network optimizer works. A notable example is RAdam , a modification of Adam  aimed at reducing the need for warmup. However, Ma and Yarats  demonstrated that RAdam essentially incorporates a fixed warmup schedule within the optimizer. In appx. C.1 we show that this warmup effect is insufficient in our setup. Relative optimizers like LARS  and LAMB  are also believed to reduce the necessity for warmup . Bernstein et al.  propose a relative optimizer called Fromage and analyze how relative weight changes relate to representation changes, but differ from our approach in that they do not describe the effects of the gradient signal-to-noise ratio on this relationship. We build upon Kosson et al.  which showed that weight decay can make standard optimizers function as approximate relative optimizers and proposed variants that reduce the benefit of warmup without fully eliminating it.

The effect of warmup in transformers was empirically studied by Wortsman et al. . Xiong et al.  proposed the pre-LN normalization placement for transformers, showing it reduces the need for warmup. Huang et al.  studied initialization in transformers showing a link to warmup.

Finally, warmup has been studied directly on its own. Gotmare et al.  studied the effect of warmup, finding it helps avoid overly large updates to the weights of later layers which could be frozen to achieve a similar benefit. Gilmer et al.  study the need for warmup from a curvature perspective, showing it may help "push" the optimization trajectory towards flatter regions where higher learning rates are stable. Smith et al.  arrive at a similar conclusion, there is a stable learning rate that varies throughout training based on the curvature which limits the learning rate early on, necessitating warmup. These works focus on SGD with momentum, but it is less clear how curvature affects Adam-like or relative optimizers as we discuss later.

The relation between stochastic gradient noise and learning rate has been studied in several works [46; 28; 49; 34; 22; 27]. They find that the update size can be increased roughly linearly with the batch size up to a certain _critical batch size_ that depends on ratio of the mean and variance of the mini-batch gradient. We show how the signal-to-noise ratio (SNR) of the mini-batch gradient amplifies changes to the neural representations of a network given a normalized update in weight space. We observe that the SNR starts out high but decreases over time, which translates to large early changes in the internal representations without warmup.

## 3 Baseline Experimental Setup & Results

Our main experiments focus on the training of a 124M parameter GPT2  model on the OpenWebText corpus . The model has 12 transformer blocks with an embedding dimension of 768. Our base training is performed at batch size 480 with a sequence length of 1024. We train for 5000 iterations which translates into roughly 20 tokens per parameter, as suggested by Chinchila . The baselines use AdamW  (see also. 1) with weight decay \(=0.1\), momentum coefficient \(_{1}=0.9\), smoothing coefficient \(_{2}=0.95\), and \(=10^{-8}\). The learning rate schedule consists of a linear warmup followed by a constant phase and eventually linear cooldown spanning half of training (see examples in fig. 1). This schedule keeps the peak learning rate and decay phase identical for different warmup lengths. This differs from other schedules, e.g. cosine, where the warmup length typically affects the whole shape. The learning rate value and the warmup length are swept for various configurations. Our code is based on NanoGPT  with additional utilities by Kosson et al. . The hyperparameter values and base training configuration are adopted from NanoGPT. See appx. C.2 for experiments on additional architectures and datasets.

Figure 1 shows the baseline performance for our setup. We observe that even short warmup can significantly improve performance. Not using warmup results in faster initial progress for a given learning rate, but eventually falls behind leaving a permanent gap. Warmup not only stabilizes higher learning rates, but also prevents a lasting degradation of the model that can not be mitigated by simply training for slightly longer. We notice that although Adam normalizes the update size, its \(_{2}\)-magnitude varies significantly throughout training with a large spike at the start of training.

## 4 The Interaction of Momentum and the \(_{2}\)-Update Norm in AdamW

In this section we analyze the reason behind the large \(_{2}\)-norm of early updates in our AdamW baseline seen in panel 4 of fig. 1. We find that this primarily stems from the \(_{1}\) bias correction. We then explore to what extent these large initial updates contribute to the need for warmup by modifying the optimizer to directly control the \(_{2}\)-norm of the update. Although we find this is to be insufficient to replace warmup on its own, these changes are an important component of our later methods.

Adam-like optimizers such as AdamW (algo. 1) differ from simpler methods like SGD with momentum in that they normalize the update size with the gradient magnitude. This makes them invariant to a rescaling of the loss function and helps counteract potential differences in the gradient magnitude between layers. An important consequence of this is that the unscaled updates \(\) are not large simply due to large initial gradients, unlike in plain SGD and other optimizers that don't normalize their

```
0: Learning rate \(_{t}\), weight decay \(\), momentum \(_{1}\), magnitude smoothing \(_{2}\), \(\) for numerical stability
1:Initialize: Time step \(t 0\), parameter vector \(_{0}\), momentum vector \(_{0} 0\), magnitude vector \(_{0} 0\)
2:while stopping criteria not met :
3:\(t t+1\)
4:\(_{t}_{t-1}\)
5:\(_{t}_{1}_{t-1}+(1-_{1})_{t}\)
6:\(_{t}_{2}_{t-1}+(1-_{2})_{t}^{2}\)
7:\(}_{t}_{t}/(1-_{1}^{2})\)
8:\(_{t}_{t}/(1-_{2}^{2})\)
9:\(_{t}(1-_{t})_{t-1}-_{t}_ {t}/(_{t}}+)\) ```

**Algorithm 1** AdamW (PyTorch variant, differing from the original by Loshchilov and Hutter )

Figure 1: Warmup significantly benefits GPT2 training with AdamW. **Panel 1:** Trapezoidal learning rate schedules with different warmup lengths and 50% linear cooldown. **Panel 2:** Final validation loss for various learning rate and warmup configurations. Note the performance gap between no-warmup (black) and other configurations. **Panel 3:** Training curves comparing the best no-warmup run to a 5% warmup with the same learning rate. The warmup run quickly surpasses the no-warmup run. **Panel 4:** Comparison of \(_{2}\) update norms for these runs shows large initial updates without warmup.

updates. Such un-normalized optimizers might diverge to infinity if a high learning rate is combined with large initial gradients or large curvature, as the update size is unbounded. Preventing this could be an additional benefit of warmup for SGD on top of the effects discussed in this work.

Although AdamW normalizes the update size based on the gradient, its magnitude can still vary throughout training as seen in fig. 1. This can be caused by changes in the gradient magnitude over time, especially when using different values of \(_{1}\) and \(_{2}\). However, it can also be caused by momentum and especially the bias correction (algo. 1, line 7). The magnitude of \(_{t}\) depends on the alignment of subsequent gradients \(_{1},,_{t}\) whereas the normalization factor \(_{t}\) does not. For example, when each \(_{t}\) is an independent zero-mean random vector with a fixed second moment \([_{t}^{2}]=^{2}\), we have (see appx. B.1 for details):

\[[_{t}^{2}]=(1-_{1}^{2t})}{1+_{1}} ^{2},[_{t}]=(1-_{2}^{t})^{2}\] (1)

In this case the bias correction for \(_{1}\) is incorrect since it is derived for a constant gradient. With the bias correction the size becomes \([\|}\|^{2}]=^{t}}{1-_{1}^{t}} {1-_{1}}{1+_{1}}^{2}\), amplifying the norm of early updates by \(^{t})/(1-_{1}^{t})}\). This factor is larger if the gradients between successive steps are negatively correlated, which we empirically observe happening in our setup (see SS6.2).

The \(_{2}\)-norm of AdamW updates can therefore vary significantly due to the initial bias correction, changes in the alignment of the gradients throughout training, and potential variations in the gradient norm over time. Lion  is a closely related optimizer that uses an element-wise sign operation to normalize the update, giving \(+1\) for positive values, \(-1\) for negative values and \(0\) for zeros. Ignoring the possibility of zeros, this gives a constant update norm. Lion is closely related to Adam, and can be obtained by tracking the size of \(_{t}\) instead of \(_{t}\) in line 6 while setting \(_{2}=0\). It also uses a form of scaled Nesterov momentum instead of the traditional heavy-ball variant and a hyperparameter specification that differs significantly from AdamW. We propose a Lion variant, LionA (algo. 2), that keeps the hyperparameters more compatible with those of AdamW. The learning rate is kept comparable by scaling the \(_{2}\) update size to match that of AdamW in the random-gradient scenario, see appx. B.1 for the derivation of the scaling factors. Due to its ability to perfectly control the size of each update, we use Lion based methods for the remaining of this paper. This avoids confounding effects in Adam-like optimizers, such as \(\) being inaccurate from rapidly decreasing gradient magnitudes early in training, which can induce an additional warmup-like effect.

In fig. 2 we repeat the baseline sweep using LionA. **Despite perfect control of the \(_{2}\) update norm (as seen in panel 3), the benefit of warmup remains. This leads us to conclude that the \(_{2}\) update size is not sufficient to quantify the "effectively" large updates that we conjecture warmup mitigates.** The final panel shows that the angular update size (see definition in the following section), proposed to be a better measure of an effective step size by Wan et al. , still varies throughout training with a spike at the start of training. In the next section we explore the reasons for the large initial angular updates and how they significantly contribute to the need for warmup.

Figure 2: LionA (algo. 2) fails to significantly reduce the warmup advantage. **Panel 1:** Final validation loss across various learning rates and warmup percentages shows a reduced but still significant no-warmup penalty compared to AdamW (fig. 1). **Panel 2:** Training curves for 0% vs. 5% warmup at the highest stable learning rate for 0%, with warmup quickly overtaking no-warmup as before. **Panel 3:** LionA successfully controls the \(_{2}\)-update norm. **Panel 4:** Early angular updates (see §5) are large without warmup and do not follow the learning rate schedule throughout training.

```
0: Learning rate \(_{t}\), weight decay \(\), momentum \(\), Nesterov flag \(\)
1:Initialize: Time step \(t 0\), parameter vector \(_{0}\), momentum vector \(_{0} 0\)
2:while stopping criteria not met :
3:\(t t+1\)
4:\(_{t}_{t-1}\)
5:\(_{t}_{t-1}+(1-)_{t}\)
6:if Nesterov flag \(\) is set :
7:\(_{t}(1-_{t})_{t-1}-_{t} )^{2}+^{4}}( _{t}+(1-)_{t})\)
8:else:
9:\(_{t}(1-_{t})_{t-1}-_{t} }(_{t})\) ```

**Algorithm 2** LionA: A modified version of the Lion  optimizer for greater compatibility with AdamW (algo. 1). The sign operation replaces the magnitude smoothing, explicitly controlling the \(_{2}\)-norm of each update. Additional scaling keeps the hyperparameters \(,\) comparable to AdamW.

## 5 The Importance and Irregularity of the Angular Update Size

The effect of a weight vector \(_{t}^{C}\) used in a dot product with some vector \(\) (e.g., in a neuron):

\[_{t},=\|_{t}\|\|\|(( _{t},))\] (2)

can be understood in terms of its magnitude \(\|_{t}\|\) and direction \(_{t}/\|_{t}\|\). The magnitude acts like a gain, scaling the outputs, whereas the direction determines which input representations \(\) the system responds to. The angular update size  of an update \(_{t}_{t+1}\) is defined as:

\[(_{t+1},_{t})=(_{t-1}, _{t+1}}{\|_{t}\|\|_{t}\|})\] (3)

and measures how fast the direction of \(_{t}\) changes during training, and thus its "preference" for \(\).

With BatchNorm  and similar operations , a network can become invariant to the magnitude of weight vectors like \(_{t}\), such that only the direction matters and the vector is said to be _scale-invariant_. Weight Normalization  provides a good example of this, changing the system to:

\[_{t}/\|_{t}\|,=\|\|(( {w}_{t},))\] (4)

Note that although the system output is invariant to the magnitude \(\|_{t}\|\), traditional optimizers are not. Scaling the value of a scale-invariant weight vector by a factor of \(c>0\), results in a gradient that is scaled by \(c^{-1}\) and curvature that is scaled by \(c^{-2}\) (see appx. B.2). For SGD this scales the angular update by \(c^{-2}\) and for Adam-like optimizers it is scaled by \(c^{-1}\). With weight decay the magnitude of scale-invariant vectors trends towards a certain stable equilibrium value over time which also results in a specific expected angular update size as described by Wan et al. , Kosson et al. .

This has several important implications. Changing the initialization magnitude of scale-invariant weights will scale the angular updates over time for standard optimizers, resulting in effects similar to modifying the learning rate schedule. For initial weight magnitudes that are small compared to the equilibrium magnitude, the early angular updates will be large and these optimizers may benefit from learning rate warmup to counteract this. These effects also make the notion of "curvature" somewhat arbitrary as it can be scaled without changing the encoded function. Optimizers that specifically account for the weight magnitude would be invariant to these effects which may reduce the need for warmup from the traditional curvature perspective. Although standard transformers are not fully scale-invariant, many of the angular update insights still approximately hold for un-normalized weights .

In light of this, we modify LionA to better control the angular update size by making the updates to weight matrices proportional to their weight magnitude, resulting in algo. 3. We normalize the angular update size to match the equilibrium value, replacing weight decay with projections similar to Kosson et al. . However, unlike their RVs, we make the angular updates proportional to the learning rate schedule which we found was necessary for good performance in our case. We also do not rely on additional exponential moving averages to control the angular update size, instead utilizing the fixed update size from the LionA optimizer. This is similar to the Adam scheme used by Karras et al.  with good results for diffusion models. No additional normalization operations or scaling factors are introduced, which we still find to result in decent performance.

Figure 3 repeats the GPT2 training sweep with LionAR. Consistent with the findings of Kosson et al. **we find that controlling the angular updates stabilizes training and decreases the benefit from warmup, but does not entirely eliminate it in this setting**. Both the angular change and the \(_{2}\)-norm are simple measures of the update magnitude in parameter space that do not account for the direction or other aspects of the update. In the next section we show how a fixed update size in parameter space can result in large changes to the internal representations of the network (a.k.a. features, activations etc), as shown in the final panel of fig. 3.

## 6 Early Gradient Alignment Results in Large Representation Changes

Our two approaches to measuring and controlling the update size in weight space failed to fully explain the need for warmup. As an alternative to the parameters, we can analyze changes in

Figure 3: LionAR (algo. 3) reduces but does not fully eliminate the benefit of warmup. **Panel 1:** LionAR is more stable across learning rates and shows a reduced but still significant performance gap without warmup. **Panel 2:** Comparing the 0% and 5% warmup for learning rate \(\!10^{-2}\) shows the warmup run overtaking early in training. **Panel 3:** LionAR precisely controls the angular update size throughout training. **Panel 4:** Despite fixed angular (and thus relative) updates in weight space, the relative change of the internal representations (see §6) is large initially without warmup.

the internal representations or activations of the neural network (feature learning). Although this is harder to analyze and control, it may ultimately be a better measure of the true impact of an update. A parameter update can only affect the network output, and hence the loss, by changing the representation of the network inputs at some layer. Large, sudden, changes in the representations could significantly affect the non-linearities, potentially causing lasting issues such as dead ReLUs or vanishing gradients from saturated sigmoids. This could in turn explain the lasting performance degradation observed without warmup.

A given parameter update will affect the representations of each distinct input sample differently. The gradients computed on these samples also generally differ, but can align to some extent. For a higher gradient alignment, the impact of a parameter update of a given magnitude on the representations will be larger than otherwise. We will analyze this for a dot product making up a single neuron:

\[=^{}=[y_{1},,y_{B}]^{}=[,_{ 1},,,_{B}]^{}\] (5)

where \(=[_{1},,_{B}]^{C B}\) are the \(C\)-dimensional representations of a random mini-batch of \(B\) inputs that is fed into the neuron, \(^{C}\) is the weight vector, and \(^{B}\) is a batch of outputs. For a weight update \(+\), we aim to quantify the size of the output change \(=^{}\) computed on the same inputs. We focus on the _Relative Representation Change (RRC)_:

\[\|}{\|\|}=^{}\|}{\| {w}^{}\|}\] (6)

similar to the angular weight updates, as the sensitivity to the absolute change \(\|\|\) can be unclear due to normalization or other scaling operations. Note that this is a measure of a _local change_, not accounting for changes in the inputs \(\) from updates to preceding layers (_global change_).

### Normalized Gradient Descent

We will focus our analysis on the relatively tractable case of normalized gradient descent with updates:

\[=-}{[\|\|^{2}]}},=_{b=1}^{B}_{b}\] (7)

where \(_{b}\) is the gradient of some loss w.r.t. \(\) for the \(b\)-th element of the mini-batch. We will use the following definitions, properties, lemmas and assumptions for this system (see appx. B.4 for details):

* D1: We define \(_{b}=:}+}_{b}\) where \(}=[]\) and \(}_{b}\) is the difference with \([}_{b}]=\).
* D2: We define \(:=[\|}\|^{2}]/[\|}_{b}\|^ {2}]\) as the Signal-to-Noise Ratio (SNR) of the gradient.
* P1: For a neuron, \(_{b}_{b}\), and hence \(_{b}=(_{b},_{b})(\|_{ b}\|/\|_{b}\|)(}+}_{b})\).
* L1: Consider two independent random vectors \(^{C}\) and \(^{C}\), whose elements are independent and identically distributed (IID). If at least one of the vectors has a zero-mean distribution, then the expected value of the squared inner product of \(\) and \(\) is given by \([,^{2}]=[\|\|^{2}][\|\|^{2}]/C\).
* A1: We assume the following vector pairs satisfy L1: \((_{i},}_{b})\) when \(i b\), \((},}_{b})\) and \((,_{b})\).

This allows us to compute the expected square relative representation change (detailed in appx. B.4):

\[[( y_{b})^{2}]}{[y_{b}^{2}]}= C}{B^{2}\|\|^{2}}[\|\|^{2}]} [\|g_{b}\|^{2}]+[\|}_{b} \|^{2}]\] \[+}{[\|g_{b}\|^{2}]}(\|\|^{ 4}+\|^{2}[\|}_{b}\|^{2}]}{C})+2(B\!- \!1)\|\|^{2}\] (8) \[=C}{B^{2}\|\|^{2}}}(\!+\!1)++(}{ +1}(+)+2(B\!-\!1))\] (9)

The expected relative change in the output for a given sample can be broken down into three sources, the contribution of the sample itself (first term), random interference from the "noise" \(}_{i}\) of other samples (second term), and finally amplification of the common mean component \(}\) (third term).

The RRC expression provides many interesting insights. In the case of large input dimension \(C\) and small SNR \( 0\), keeping the RRC constant for different batch sizes involves scaling the learning rate \(\), as suggested by Malladi et al.  for Adam. When the SNR \(\) is some finite and value and \(C\) is still large, this scaling rule instead starts to break down around \(B=1/\), matching the predicted critical batch size of e.g. McCandlish et al. . The role of the dimension \(C\) in the expression is curious, suggesting that narrower layers experience larger changes due to random inference from other samples in a given batch. The \(C\) in the leading factor also suggests that the angular updates can be smaller for a larger input dimension, similar to what is proposed in \(\)-parameterization [44; 45]. Most importantly, **this expression shows that if the SNR changes throughout training the learning rate needs to be adjusted to keep the RRC constant. In particular, with large batch sizes, a high initial SNR results in large representation changes which warmup can help prevent.** The first panel of fig. 4 shows how eq. (9) predicts we should downscale the learning rate for different batch sizes and SNRs, assuming we originally scaled the learning rate \(\) and that \(C\) is large. The second panel confirms that the SNR indeed starts out large, suggesting lower initial learning rates are needed, i.e. warmup.

In the first panel of fig. 5, we show the results of adding a term that scales the update size as predicted by eq. (9). **This acts similar to an automatic warmup based on online measurements of the SNR which we obtain from the gradient accumulation of micro-batches.** Although this helps close the gap between warmup and no-warmup, the overall performance is slightly worse. One potential issue is that our batch size of 480 is quite large compared to the measured SNR, exceeding the critical batch size estimation throughout most of training. This results in a scaling of the step size throughout training, which distorts the decay phase. It also requires large learning rate values to counteract the scaling, which may destabilize the training of non-matrix weights like gains. We increase the weight decay by a factor of 32\(\) to try to increase the angular updates relative to gains in order to compensate, but this value was not tuned and is unlikely to be optimal. We believe approaches that aim to directly control the RRC are a promising direction but require further work to be practical.

### The Role of Momentum

Momentum is believed to be a key enabler of optimization with larger batch sizes [32; 39; 31]. However, it is unclear how it should change predictions for the critical batch size or relative representation change. Momentum spreads the application of a gradient out over multiple steps which tends to make each update smaller, especially for a random walk, which is reflected in our scaling coefficients in also. 2 and 3. The smaller updates are counteracted by an increased correlation in their direction, which can result in similar "long term" changes from each gradient sample, especially for simpler methods like SGD that don't normalize the step size. In the last two panels of fig. 4 we observe that in our setup the gradient and momentum are negatively correlated, counteracting each other. We find momentum crucial for performant training, panel 2 of fig. 5 shows significant degradation without it.

Figure 4: Equation (9) predicts that the learning rate needs to be downscaled for higher signal to noise ratios (\(\)) to keep the relative representation change constant. Larger batch sizes are affected more, with scaling becoming significant when \(>B^{-1}\). **Panel 2:** Measurements of the SNR for the two highlighted runs in fig. 3. Note the SNR starts very high but is also remains large in comparison to our \(B=480\) for almost all of training. **Panel 3:** The gradient is strongly oppositely aligned with the momentum vector for most of training (shown for an example layer). **Panel 4:** Projecting the momentum component of the updates onto the gradient component shows that this results in the momentum vector “cancelling” roughly half the gradient on average.

We believe the smaller update sizes for momentum combined with the potential for later gradients to counteract earlier gradients during their application over time, can help stabilize training. An otherwise large relative representation change is spread out over multiple steps and counteracted by later gradients. Higher values of momentum should amplify these effects. Looking at the total contribution of each gradient also implies that **with momentum early updates should be smaller when measured in parameter space, otherwise the relative representation change for those samples is too large.** This is equivalent to entirely removing the \(_{1}\) bias correction in AdamW (algo. 1, line 7), or introducing _an inverse bias correction_ in Lion like algorithms (see appx. B.1 for details). Higher \(\) values should help amplify the stabilization effects of momentum. **In fig. 5 we find that at higher momentum values LionAR no longer benefits from warmup unlike LionA which still needs it**. These experiments use Nesterov momentum and the additional inverse bias correction, though these adjustments offer only minor improvements compared to higher momentum.

## 7 The Detrimental Effects of Large Updates

In appx. A we empirically investigate potential causes for the lasting performance degradation from large initial updates for a small ResNet model. We find that the performance impact best correlates with the number of dead ReLUs and is improved by the use of leaky-ReLUs, which fits well with our perspective of large changes in the internal representations. We also investigated whether overfitting to initial training samples or the correlation between weight vectors of different neurons could explain the necessity for warmup, but these factors did not show a significant impact.

## 8 The Role of Larger Batch Sizes

Warmup is often used with larger batch sizes in particular, for example in the setting where Goyal et al.  first proposed using linear warmup. Although this was for SGD, we expect the need for warmup to be amplified at larger batch sizes for two of the reasons we identified. The first is that larger batch sizes are more likely to exceed the critical batch size early in training. The second is the size of early angular updates. As shown by Kosson et al. , the equilibrium weight magnitude depends on the learning rate and weight decay value. Common hyperparameter scaling rules for a modified batch size only change the learning rate but not the weight decay, which shifts the equilibrium magnitude. The smaller the initialization magnitude is compared to the equilibrium magnitude, the larger the early angular updates will be relative to their steady state value, potentially necessitating warmup.

## 9 Limitations

Our main experiments focus on a single network which may not be broad enough to generalize to a wide range of networks. In appx. C.2 we experiments with an additional dataset and architecture but the scale of the experiments is still limited and they cover a limited range of hyperparameters. We believe we identify real factors that contribute to the need for warmup, but these may not be the only

Figure 5: **Panel 1:** LionAR with a correction factor for the RRC based on eq. (9) does not benefit from a warmup. **Panel 2:** LionAR training without momentum results in drastically lower performance. **Panel 3:** In LionAR with increased momentum \(=0.98\), Nesterov momentum and an inverse bias correction for early momentum, no warmup performs best. **Panel 4:** The same does not apply to LionA, suggesting that these changes are not sufficient without controlling the angular updates.

ones across a broader range of settings. Similarly, the promising results for reducing or eliminating the warmup with higher momentum values or the relative representation correction would benefit from broader validation.

## 10 Conclusion

In this work we explored how the size of the updates \(=\) impacts the need for learning rate warmup. We showed that \(\) can be large initially when measured in terms of its \(_{2}\)-norm (SS4), the resulting directional change in \(\) (angular update, SS5), as well as the resulting change in the internal representations of the network (relative representation change, SS6). We argued that small initial values of the learning rate \(\) are beneficial to counteract large values of \(\), i.e. that a learning rate warmup simply keeps some notion of the overall "effective" update size reasonable. We showed this experimentally rather than theoretically by modifying the optimizers to normalize the size of \(\) based on each metric and measuring how these changes affected the benefit of using learning rate warmup.

The two weight-based measures of the update size, the \(_{2}\)-norm and angular update did not fully account for the need for warmup. However, quantifying the update size in terms of the relative change in neural representations shows potential. This measure is closely linked to the angular update size but accounts for changes in the signal characteristics of the gradient, which can vary significantly throughout training. Effectively controlling neural representation changes is a challenging task we leave for future work, but our initial attempts show encouraging results in reducing the need for a manually configured warmup. We also highlighted the importance of high momentum for warmup; when combined with angular update control and an inverse bias correction, it may enable efficient warmup-free training. Overall, our work provides new insights into the benefit of learning rate warmup with modern optimizers beyond SGD and suggests potential directions for eliminating it.

Although we present new methods we consider promising, we still recommend the use of a short warmup in practice. Fully eliminating it seems to require significant modifications that also need further validation across additional settings. However, we hope to have provided the reader with a new perspective and simple intuition for why warmup is beneficial for training. We also hope our work inspires further exploration of how learning should be controlled and scheduled in neural network training. In particular, it seems that the learning rate in current optimizers does not really control the "rate of learning", making learning rate schedules and the use of warmup highly arbitrary.