# Robust Reinforcement Learning with General Utility

Ziyi Chen, Yan Wen, Zhengmian Hu, Heng Huang

Department of Computer Science, Institute of Health Computing,

University of Maryland College Park

College Park, MA 20742, USA

{zc286,ywen1,zhu123,heng}@umd.edu

###### Abstract

Reinforcement Learning (RL) problem with general utility is a powerful decision making framework that covers standard RL with cumulative cost, exploration problems, and demonstration learning. Existing works on RL with general utility do not consider the robustness under environmental perturbation, which is important to adapt RL system in the real-world environment that differs from the training environment. To train a robust policy, we propose a robust RL framework with general utility, which subsumes many existing RL frameworks including RL, robust RL, RL with general utility, constrained RL, robust constrained RL, pure exploration, robust entropy regularized RL, etc. Then we focus on popular convex utility functions, with which our proposed learning framework is a challenging nonconvex-nonconcave minimax optimization problem, and design a two-phase stochastic policy gradient type algorithm and obtain its sample complexity result for gradient convergence. Furthermore, for convex utility on a widely used polyhedral ambiguity set, we design an algorithm and obtain its convergence rate to a global optimal solution.

## 1 Introduction

Reinforcement learning (RL) is an important decision-making framework  aiming to find the optimal policy that minimizes accumulative cost, which is also a _linear utility function_ of occupancy measure. Recent works have extended standard RL to more _general utility functions_ to account for a variety of practical needs, including risk-sensitive applications , exploration maximization , and safety constraints . There are provably convergent algorithms to solve RL with general utility . However, these works study RL with general utility in a fixed environment, which may fail in many applications where the policy is trained in a simulation environment but implemented in a different real-world environment .

To make the policy robust to such environmental change, robust RL has been proposed to find the optimal robust policy under the worst possible environment . However, all the existing robust RL works restrict to linear utility function to our knowledge. Therefore, we aim to answer the following research question:

_Q: Can we train a robust policy for RL with general utility and obtain convergence results?_

### Our Contributions

We affirmatively answer this question by proposing robust RL with general utility, the first learning framework that obtains a robust policy for general utility in the worst possible environment. It is formulated as a minimax optimization problem \(_{}_{}f(_{,})\) where \(f\) is the utility function and \(_{,}\) is the occupancy measure under the policy parameter \(\) and the environmentaltransition kernel parameter \(\). Our robust RL with general utility is a combination of its two important special cases, namely, RL with general utility  (formulated as \(_{}f(_{,})\) where the environmental parameter \(\) is fixed) and robust RL  where \(f\) is restricted to linear utility function. This new learning framework also covers many other existing RL frameworks including constrained RL  and robust constrained RL  with safety critical applications such as healthcare and unmanned drones, entropy regularized RL  and its robust extension  which help agents learn from human demonstration, pure exploration  with application to explore an environment with sparse reward signals and its robust extension, etc. These examples use convex utility functions \(f\), which is the focus of this paper. See Section 2.1 for details of these examples.

Then, we focus on designing provably convergent algorithms for our new proposed learning framework with the widely used convex utility function \(f\). In this case, our objective \(_{}_{}f(_{,})\) is still a highly challenging nonconvex-nonconcave minimax optimization problem. Hence, we have to utilize the structure and properties of \(_{,}\) to design algorithms and obtain convergence results. To elaborate, we design a projected stochastic gradient descent ascent algorithm with two phases. Interestingly, the first phase targeted at the objective function \(f\) obtains a stationary point of a different envelope function. Hence, we add a second phase targeted at a corrected objective \(\) to converge to a near-stationary solution of the original objective \(f\). The convergence analysis is non-trivial with two novel techniques. First, we have proved a projected gradient dominance property (Proposition 4) that is much stronger than the existing one on convex utility, with less assumptions, no bias term and applicability to more general parameterized policy. Second, in the convergence analysis of the second phase, we obtain convergence to a global Nash equilibrium (thus a stationary point) of \(\) by Proposition 4, which is close to a stationary point of \(f\) by proving that \(_{}(_{,})\!\!_{}f( _{,})\).

Furthermore, with convex utility function \(f\) and the widely used \(s\)-rectangular polyhedral ambiguity set \(\) (including the popular \(L^{1}\) and \(L^{}\) ambiguity sets), we design an alternative algorithm which **converges to a global optimal solution of this nonconvex-nonconcave optimization problem** at a sublinear rate \((1/K)\) (Theorem 3). This is much more challenging than global convergence for convex RL (that is, RL with convex utility function and fixed \(\)) [54; 51; 6] and for robust RL with linear utility satisfying Bellman equation [36; 20; 45; 15; 25], so we need novel algorithm design and novel techniques. First, we prove that \(_{}f(_{,})\) can be obtained in the finite set of vertices \(V()\) (Proposition 6). This is intuitive if \(f(_{,})\) is a convex function but in many applications, only \(f()\) is convex. To solve this challenge, we prove a novel local invertibility property of \(_{,}\) (Proposition 5) by checking Bellman equation of \(_{,}\) state by state in two cases. Then we prove Proposition 6 using a novel state-by-state extension from an optimal non-vertex \(\) to an optimal vertex \(\). Second, the major difficulty to design our algorithm is to find a descent direction of \(():=_{}f(_{,})\). We select the near-optimal vertices \(_{k} V()\) that may affect the optimization progress \((_{k+1})-(_{k})\), and find the descent direction with provably large descent for all the corresponding functions \(\{f(_{_{k},})\}_{_{k}}\) (Proposition 7) via convex optimization. Third, by Proposition 6, the global convergence measure \(_{k}:=(_{k})-_{}()\) at each iteration \(k\) either is \((1/k)\)-close to optimal (\((_{k})(1/k)\)) or enjoys large descent (Eq. (26)), so we prove the convergence in 3 cases: \((1/K)\)-optimal final \(_{K}\), iterate Eq. (26) from \(_{0}\) or from a \((1/k)\)-optimal intermediate \(_{k}\).

### Related Works

**RL with General Utility.** Standard RL aims to optimize over the accumulated reward/cost [21; 41]. Some early operation research works focus on other non-linear objectives such as variance-penalized MDPs , risk-sensitive objectives [22; 8; 52], entropy exploration , constrained RL [2; 1; 35] and learning from demonstration [39; 3].

 proposes RL with general utilities to cover the above applications and applies variational policy gradient method that provably converges to the global optimal solution for convex utility.  proposes a variance reduced policy gradient algorithm which requires \(}(^{-3})\) samples to achieve an \(\)-stationary policy for general utility and \(}(^{-2})\) samples to achieve an \(\)-global optimal policy for convex utility and overparameterized policy.  provides a meta-algorithm to solve the convex MDP problem as a min-max game between a policy player and a cost player who produces rewards that the policy player must maximize. They further show that any method-solving problems under the standard RL settings can be used to solve the more general convex MDP problem.  obtains policy gradient theorem for RL with general utilities.  proposes a simpler single-loop parameter-free normalized policy gradient algorithm with recursive momentum variance reduction. This algorithm requires \(}(^{-3})\) samples to achieve \(\)-stationary policy in general and \(}(^{-2})\) samples to achieve \(\)-global optimal policy for convex utility. For large finite state action spaces, it requires \(}(^{-4})\) samples to achieve \(\)-stationary policy via linear function approximation of the occupancy measure.  proposes decentralized multi-agent RL with general utilities.  shows that convex RL is a subclass of multi-agent mean-field games.

**Robust RL.** Robust RL is designed to learn a policy that is robust to perturbation of environmental factors. Usually robust RL is NP-hard , but becomes tractable for ambiguity sets that is \((s,a)\)-rectangular [36; 20; 45; 44; 29; 56] or \(s\)-rectangular [45; 42; 23; 26]. Methods to solve robust RL include value iteration [36; 20; 45; 15; 25], policy iteration [20; 4; 24] and policy gradient [29; 44; 56; 42; 26; 17; 28].

## 2 Robust Reinforcement Learning with General Utility

**Notations.** The space of probability distribution on a space \(\) is denoted as \(^{}\). If \(\) is finite, we denote its cardinality as \(||\). \(\|\|_{p}\) denotes \(p\)-norm of vectors and \(\|\|=\|\|_{2}\) by default.

**Reinforcement Learning with General Utility.** Reinforcement Learning (RL) with general utility is an emerging learning framework [54; 55; 6], specified by a tuple \(,,p_{},f,,\), with finite state space \(\), finite action space \(\), transition kernel \(p_{}(^{})^{}\) parameterized by \(\) (\(^{d_{}}\) is convex and compact), discount factor \((0,1)\), general utility function \(f:^{}\) and the distribution \(^{}\) of the initial state \(s_{0}\). At time \(t\), given the environmental state \(s_{t}\), the agent takes action \(a_{t}_{}(|s_{t})\) based on a policy \(_{}(^{})^{}\) parameterized by \(\) (\(^{d_{}}\) is convex). Then the environment transitions to state \(s_{t+1} p_{}(|s_{t},a_{t})\). The occupancy measure \(_{,}^{}\) at \((s,a)\!\!\) is defined below.

\[_{,}(s,a)\ }}{{=}}\ (1-)_{t=0}^{+}^{t}_{_{ },p_{}}(s_{t}=s,a_{t}=a|s_{0}),\] (1)

where \(_{_{},p_{}}\) denotes the probability measure of the Markov chain \(\{s_{t},a_{t}\}_{t 0}\) induced by policy \(_{}\), transition kernel \(p_{}\) and the initial distribution \(\). The aim of the agent is to find the optimal policy \(_{}\) that solves \(_{}f(_{,})\) given fixed transition kernel \(p_{}\). Here, \(f(_{,})\) can be seen as the overall cost of selecting policy \(_{}\) in the environment \(p_{}\), and there are many examples of the utility function \(f\) covering a variety of applications (See Section 2.1). However, existing works on RL with general utility assume a fixed environmental transition kernel \(p_{}\), which may fail in many applications where the policy is deployed in a real-world environment different from the simulation environment for training. To obtain a policy that is robust to such environmental change, we propose a new learning framework called _robust RL with General Utility_ as follows.

**Our Proposed Robust RL with General Utility.** The goal of our proposed _robust RL with general utility_ is to find an optimal robust policy under the worst possible environmental parameter \(\) from an ambiguity set \(\), as formulated by the following minimax optimization problem with general utility function \(f\).

\[_{}_{}f(_{,}),\] (2)

In practice, the distance between the real-world environment (for deployment) and simulation environment (for training) is assumed to be bounded. Therefore, \(\) is usually set as a neighborhood around the nominal kernel \(\) estimated from the simulation environment, i.e. \(=\{^{d_{}}:d(,) d_{0}\}\) with distance measure \(d\) and the distance upper bound \(d_{0} 0\).

### Examples of Our Robust RL with General Utility

**Example 1: RL with General Utility.**

When \(=\{\}\) for a fixed environmental parameter \(\), our proposed _robust RL with general utility_ (2) reduces to (non-robust) _RL with general utility_\(_{}f(_{,})\), as introduced above.

**Example 2: Robust Constrained RL and Its Special Cases.**

Robust constrained RL [38; 43; 40] is an emerging learning framework where an agent should obey safety conditions in all possible real-world environments, which is important in safety critical applications such as healthcare and unmanned aerial vehicle . For math formulation, denote \(c^{(0)},c^{(1)},,c^{(K)}\) as cost functions \(\). At time \(t\), the agent receives performance-related cost \(c^{(0)}(s_{t},a_{t})\) and safety-related costs \(\{c^{(k)}(s_{t},a_{t})\}_{k=1}^{K}\). Define value functions \(V^{(k)}_{,}\) and robust value functions \(V^{(k)}_{}\) as follows.

\[V^{(k)}_{,}\ }}{{=}}\  c^{(k)},_{,} =_{s,a}c^{(k)}(s,a)_{,}(s,a), V^{(k)}_{} \ }}{{=}}\ _{}V^{(k)}_{,},\ k=0,1, ,K.\] (3)

Then robust constrained RL is formulated as the following constrained policy optimization problem.

\[_{}V^{(0)}_{},\ \ V^{(k)}_{} _{k}\ \ k=1,,K,\] (4)

where \(_{k}\) is the safety threshold, and \(V^{(k)}_{}_{k}\) means that the safety constraints \(V^{(k)}_{,}_{k}\) holds for any environmental parameter \(\).

**Proposition 1**.: _The robust constrained RL problem (4) is a special case of our proposed robust RL with general utility (2) using the following convex utility function \(f\)._

\[f()= c^{(0)},,&\  c^{(k)},_{k}\ \ k=1,,K\\ +,&.\] (5)

After removing the safety constraints, _robust constrained RL_ reduces to an important special case called _robust RL_ (formulated as \(_{}_{} c^{(0)},_{,}\) with linear utility function \(f()= c^{(0)},\)) . Furthermore, when \(=\{\}\) for fixed \(\), _robust constrained RL_ and _robust RL_ reduce to _constrained RL_ and _RL_ respectively. All these examples are important special cases of our proposed _robust RL with general utility_ based on Proposition 1.

**Example 3: Robust Entropy Regularized RL and Its Special Cases.**

Robust entropy regularized RL is also an important RL framework with application to imitation learning and inverse reinforcement learning which help agents learn from human experts' demonstration [32; 33], and is formulated as the following minimax optimization problem.

\[_{}_{}_{s,a}_{,}( s,a)c(s,a)-_{s}_{,}(s)[_{ }(|s)],\] (6)

where \(c\) is a cost function, \(_{,}(s)=_{a}_{,}(s,a)\) is the state occupancy measure, and \([_{}(|s)]=-_{a}_{}(a|s)_{}(a |s)\) is the entropy regularizer (with coefficient \( 0\)) which encourages the agent to explore more states and actions and helps to prevent early convergence to sub-optimal policies.

**Proposition 2**.: _The robust entropy regularized RL problem (6) is a special case of our proposed robust RL with general utility (2) using the following convex utility function \(f\)._

\[f()=_{s,a}(s,a)c(s,a)+}(s,a^{})}.\] (7)

When \(=0\), _robust entropy regularized RL_ (6) reduces to _robust RL_. When \(c 0\) but \(>0\), _robust entropy regularized RL_ reduces to _robust pure exploration_. Furthermore, when \(=\{\}\), _robust entropy regularized RL_, _robust RL_ and _robust pure exploration_ reduce to _entropy regularized RL_, _RL_ and _pure exploration_ respectively. All these examples are important special cases of our proposed _robust RL with general utility_ based on Proposition 2.

### Gradients for Our Robust RL with General Utility

**Theorem 1**.: _The gradients of the objective function (2) for our proposed robust RL with general utility can be computed as follows._

\[_{}f(_{,})=_{_{},p_{}}[ _{t=0}^{+}^{t})}{ _{,}(s_{t},a_{t})}_{h=0}^{t}_{} _{}(a_{h}|s_{h})s_{0}],\] (8)\[_{}f(_{,})=_{_{},p_{}}[_{t= 0}^{+}^{t})}{_{ ,}(s_{t},a_{t})}]_{h=0}^{t}_{} p_{}(s_{h+1}| s_{h},a_{h})s_{0}].\] (9)

We make the following standard assumptions which are also used in RL with general utility .

**Assumption 1**.: _There exist constants \(l_{_{}},L_{_{}},l_{p_{}},L_{p_{}}>0\) such that for all \(s,s^{}\), \(a\), \(,^{}\) and \(,^{}\), we have_

\[\|_{}_{}(a|s)\|_{_{}}, \|_{}_{^{}}(a|s)-_{}_ {}(a|s)\| L_{_{}}\|^{}-\|,\] \[\|_{} p_{}(s^{}|s,a)\|_{p_{}}, \|_{} p_{^{}}(s^{}|s,a)-_{} p _{}(s^{}|s,a)\| L_{p_{}}\|^{}-\|.\]

**Assumption 2**.: _There exist constants \(l_{},L_{}>0\) such that for all \(,^{}^{}\), \(\|_{}f()\| l_{}\) and \(\|_{}f(^{})-_{}f()\| L_{ }\|^{}-\|\)._

**Proposition 3**.: _Under Assumptions 1 and 2, the gradients (8) and (9) satisfy the following bounds for any \(,^{}\) and \(,^{}\)_

\[\|_{}f(_{,})\|_{}:= }}{(1-)^{2}},\|_{}f(_{,})\|_{}:=}}{(1-)^{2}},\] (10) \[\|_{}f(_{^{},^{}})- _{}f(_{,})\| L_{,}\|^{}- \|+L_{,}\|^{}-\|,\] (11) \[\|_{}f(_{^{},^{}})-_{ }f(_{,})\| L_{,}\|^{}-\|+L_ {,}\|^{}-\|,\] (12)

_where \(L_{,}:=}^{2}|}(L_{ }+_{}|||})}{(1-)^{3}}+}_{}}{(1-)^{2}}\), \(L_{,}:=}_{_{}}|}}{(1-)^{3}}(L_{}+2_{} |||})\), \(L_{,}:=}_{_{}}|}(L _{}+_{}|||})}{(1-)^{3}}\), \(L_{,}:=}^{2}|}(L_{}+2 _{}|||})}{(1-)^{3}}+(L_{p_{}}+_{_{}}^{2}\,||)}{(1-)^{2}}\)._

In practice, the exact gradients (8) and (9) are unavailable and can only be estimated via stochastic samples. We refer the details to Appendix C as those largely follow .

Define the following projected gradients with stepsizes \(b,a>0\), which have been used to measure convergence of algorithms to stationary points of optimization  and RL problems .

\[G_{b}^{()}(,)\!:=\!\!-\!_{ }\!-\!b_{}f(_{,}), \;\;G_{a}^{()}(,)\!:=\!_{} \!+\!a_{}f(_{,})\!-\!\] (13)

## 3 Gradient Convergence for Convex Utility

**Assumption 3**.: _The utility function \(f()\) is convex._

Robust RL with convex utility functions \(f\) subsumes many important special cases, including robust constrained RL, robust entropy regularized RL, constrained RL, robust RL, RL, pure exploration, etc., as shown in Examples 2 and 3 in Section 2.1.

Partially inspired by the gradient descent ascent (GDA) algorithm  for nonconvex-concave minimax optimization, we design the projected stochastic GDA algorithm (Algorithm 1) with two phases to solve robust RL with convex utility. The first phase (called _original phase_) can be seen as projected stochastic GDA algorithm on the _original_ objective function \(f\). Specifically, in the \(k\)-th the outer loop with fixed \(_{k}\), the inner loop applies \(T\) projected stochastic gradient descent steps (14) to obtain \(_{k}\) which converges to the global solution of \((_{k}):=_{}f(_{,_{k}})\) as \(f\) is convex. Then, we update \(_{k}\) using the projected stochastic gradient ascent step (15). However, the output \(\) of the first phase only converges to a stationary point of the following the envelope function \(\)1.

\[():=_{^{}}(^{})-L_{ ,}\|^{}-\|^{2}.\] (18)

To converge to a stationary point of \(f\), we add the second phase (called _corrected phase_) which applies projected stochastic GDA to the following corrected objective.

\[_{}_{}(,):=f(_{ ,})-L_{,}\|-\|^{2}.\] (19)The convergence analysis of Algorithm 1 is challenging largely because \(f(_{,})\) is only a convex function of \(_{,}\) not of \(\). To tackle this challenge for non-robust convex RL with fixed \(\),  assumed that a global Lipschitz continuous inverse mapping from \(_{,}\) to \(\) exists.  relaxed this assumption to the following assumption of local inverse mapping, which covers the popular direct policy parameterization \(_{}(a|s)=_{s,a}\) and softmax policy parameterization \(_{}(a|s)=)}{_{a^{}}(_{s,a ^{}})}\) (see Proposition 8 for the proof).

**Assumption 4** (Local Invertibility of \(_{,}\)).: _There exists constants \(_{^{-1}}>0\) and \((0,1)\) such that for any fixed \(\) and \(\), the occupancy measure (1) satisfies: 1. There exists sets \(_{,}\) and \(_{_{,}}^{S}\) that contain \(\) and \(_{,}\) respectively, such that \(_{,}:_{,}_{_ {,}}\) is a bijection. Its inverse denoted as \(_{,}^{-1}\) is \(_{^{-1}}\)-Lipschitz. 2. There exists at least one optimal policy \(^{*}()_{^{}}f(_{^{ },})\) such that for any \([0,]\), \((1-)_{,}+_{^{*}(),} _{_{,}}\)._

**Proposition 4** (Projected Gradient Dominance for Convex Utility).: _Under Assumptions 1-4, the utility function \(f\) satisfies the following gradient dominance property for any \(\) and \(\)._

\[f(_{,})-_{^{}}f(_{^{ },}) _{^{-1}} L_{,}+1 +_{}\|G_{}^{()}(,)\|.\] (20)

**Remark:** Proposition 4 indicates that the function \(f(_{,})\) is projected gradient dominant for convex utility function \(f\), which is important in the convergence analysis of Algorithm 1. Our Proposition 4 is stronger than Lemma F.7 of , a similar gradient dominance property for convex RL which requires assumption of positive definite Fisher information matrix, involves bias in the error term and focuses on unconstrained optimization with softmax parameterized policy (a special of our general parameterized policy with constrained variable \(\)).

**Technical Novelty.** In our proof, to tackle the constraint \(\) which is more challenging than the unconstrained case \(=^{|||A|}\) in , we apply Assumption 4 to \(^{}:=- G_{}^{()}(,)\) not to the obvious choice \(\), which yields \(_{}\) for any \([0,]\) such that \(_{_{},}=(1-)_{^{},}+ _{^{*}(),}_{_{,}}\). Then

\[_{}f(_{^{},})^{}(_{}- ^{})[_{}f(_{^{},})-_{}f(_{,})+G_{}^{()}(, )]^{}(_{}-^{})- [\|G_{}^{()}(,)\|],\]

where (i) uses the projection property \((_{}-^{})^{}[G_{}^{()}(,)- _{}f(_{,})] 0\) and (ii) uses \(\|_{}-^{}\|()\). The above bound implies Eq. (20) since \(f\) is convex and \(_{}\)-Lipschitz.

**Assumption 5**.: \(\) _is convex and compact with diameter \(D_{}:=_{,^{}}\|^{}-\|>0\)._

Assumption 5 holds for the commonly used direct kernel parameterization \(p_{}(s^{}|s,a)=(s,a,s^{})\) (for all \(s,s^{}\) and \(a\)) and \(\) defined a compact neighborhood around a nominal transition kernel parameter \(\).

We show the gradient convergence result of Algorithm 1 by the following theorem and demonstrate the gradient convergence by the experiments in Appendix A.

**Theorem 2** (Gradient Convergence for Convex Utility).: _Suppose Assumptions 1-5 hold. For any precision \(0<}{L}_{^{-1}} L_{,}+4+_{}\), we can always find proper hyperparameter values of Algorithm 1 (see Eqs. (127)-(150) in Appendix N.6 for these hyperparameter values) such that the algorithm output \((_{},_{})\) is an \(\)-close to a stationary point, that is, \([\|G_{b}^{()}(_{}^{*},_{}) \|^{2}]^{2}\) and \([\|G_{a}^{()}(_{}^{*},_{})\|^ {2}]^{2}\) with projected gradients \(G_{b}^{()}\) and \(G_{a}^{()}\) defined in Eq. (13). The number of required stochastic samples is \(([(1-)^{-1}^{-1}])}{(1-)^{ 2}^{10}}\)._

Proof Sketch of Theorem 2 and Technical Novelty.Inspired by Appendix D of , \(\) from the first phase satisfies \(\|()\|^{2} 0\) (see Appendix N.2). Then, \(_{k}:=_{k,T^{}}\) from the inner update (16) of the second phase converges to the unique maximizer (denoted as \(_{k}^{*}\)) of the \(L_{,}\)-concave function \((_{k},)\) as \(T^{}+\) (see Appendix N.3). This means the update step (17) is approximately projected gradient descent for \(_{}()\), which yields the convergence rate of \(\|G_{b}^{()}(_{},_{} )\|^{2}\) (see Appendix N.4).

However, the biggest challenge is to obtain the convergence rate of \(\|G_{a}^{()}(_{},_{})\|^ {2}\) (see Appendix N.5), which corresponds to \(_{}f\) while the second _corrected phase_ aims at the corrected objective \(\). To show that \(_{}(_{k},_{k})_{}f(_{k}, _{k})\), note that \(_{}(_{k},_{k})-_{}f(_{k},_{k})= -2L_{,}(_{k}-)\) and that \(()=2L_{,}[^{*}()- ] 0\) (already proved) where \(^{*}()\) is the unique maximizer of \((^{})-L_{,}\|^{}-\|^{2}\), a strongly concave function of \(^{}\) in Eq. (18). Hence, it suffices to show \(_{k}^{*}()\). Note that \((_{k},_{k})\) is an **approximate Nash equilibrium** of \(\), i.e., \(_{k}_{k}^{*}:=_{}(_{k},)\) (proved above) and \(_{k}_{}(,_{k})\) (derived below).

\[[(_{k},_{k})-_{^{}} (^{},_{k})]=[f(_{_{k},_{k}} )-_{^{}}f(_{^{},_{k}})](\|G_{}^{()}(,)\|) (),\]

where (i) uses Proposition 4. Hence, based on the **property of Nash equilibrium**, we have \(_{k}_{}()=^{*}()\) where \(():=_{}(,)=()-L_{, }\|-\|^{2}\).

## 4 Global Convergence on Polyhedral Ambigity Set

This section aims to obtain a global optimal policy \(^{*}\) that minimizes the robust utility \(()}{=}_{}f(_{,})\). This maximization is challenging for convex utility \(f\). In contrast, global convergence results have been obtained without such challenge in some important special cases, including convex RL with fixed \(\)[54; 51; 6] and robust RL where linear utility \(f\) is amenable to both \(_{}\) and \(_{}\)[36; 45; 42; 23; 26]. Fortunately, we will show that by using the popular \(s\)-rectangular _polyhedral_ ambiguity set \(\), \(_{}\!f(_{,})\) always exists among the finitely many vertices of \(\).

### S-rectangular Polyhedral Ambiguity Set

In this subsection, we will introduce the popular \(s\)-rectangular polyhedral ambiguity set, and derive its important propositions for designing globally converged algorithm.

Fhe global convergence is generally NP-hard, even for the important special case called _robust RL with linear utility_, . A common practice to make the problem tractable is to use direct kernel parameterization \(p_{}(s^{}|s,a)=(s,a,s^{})\)[42; 28; 26; 17] and assume the ambiguity set \(\) to satisfy some certain rectangularity conditions, such as _\(s\)-rectangularity_ defined below [45; 42; 23; 26].

**Assumption 6**.: _We use direct kernel parameterization and assume that \(\) is \(s\)-rectangular, i.e., \(=_{s}_{s}:=\{(^{})^{}:(s,,)_{s}, s\}\), a Cartesian product of \(_{s}(^{})^{}\)._

**Proposition 5** (Local Invertibility of \(_{,}\).).: _Suppose Assumption 6 holds and \(\) is a convex set. For any \(\), \(_{0},_{1}\) and \(\), define the following kernel parameters \(_{}(^{})^{}\)._

\[_{}(s,a,s^{})=\,\,\,\,\,\,_{}(s,a,)^{},&\,\,_{,_{0}}(s)\!=\! _{,_{1}}(s)\!=\!0\\ }(s)_{1}(s,a,s^{})\!+\!(1\!-\! )_{,_{0}}(s)_{0}(s,a,s^{})}{_{ ,_{1}}(s)\!+\!(1\!-\!)_{,_{0}}(s)}, {otherwise},\] (21)

_where \(_{,}(s):=_{a}_{,}(s,a)\) for any \(s\), \(\) and \(\). Then \(_{}\) and its corresponding occupancy measure is \(_{,_{}}=_{,_{1}}+(1-) _{,_{0}}\)._

**Remark:** Proposition 5 indicates that the mapping from \(\) to \(_{,}\) is locally invertible for \(s\)-rectangular set \(\), which is important to solve the aforementioned challenge that convex utility is not amenable for \(_{}f(_{,})\). This role is similar to that played by the local invertibility assumption (Assumption 4) for policy \(\). To our knowledge, Proposition 5 has never been obtained in the existing literature.

**Assumption 7**.: _Under Assumption 6, for every \(s\), \(_{s}\) is a polyhedron spanned by a finite set of vertices \(V(_{s})\!:=\!\{_{m}^{(s)}\}_{m=1}^{M_{s}}\!\!_{s}\), i.e., \(_{s}=_{m=1}^{M_{s}}_{m}_{m}^{(s)}:_{m} 0,_{m=1}^{M_ {s}}_{m}=1}\)._

Polyhedral ambiguity set defined by Assumption 7 includes the widely used \(s\)-rectangular \(L_{1}\) and \(L_{}\) ambiguity sets, defined as \(=\{(^{})^{}:\|(s,: ;)-(s,:;)\|_{p}_{s}, s\}\) for \(p\{1,\}\) respectively [7; 19; 16], where \(\) is the nominal transition kernel usually obtained via empirical estimation. On polyhedral ambiguity set, the optimal kernels \(_{}\!f(_{,})\) can always be obtained at the vertices of \(\), as shown below.

**Proposition 6**.: _Under Assumptions 3, 6 and 7, for any \(\), we have \(_{}f(_{,})=_{ V()}f(_{ ,})\), where \(V()=_{s}V(_{s})\) is the vertex set._

**Technical Novelty.** Suppose a non-vertex kernel \(^{*}_{}\!f(_{,})/V()\) is optimal. Since \(f\) is convex, if \(_{,^{*}}\) is a convex combination of \(_{,^{*}_{1}}\) and \(_{,^{()}}\) for some \(_{1},_{0}\) (corresponding to \(^{*}_{1},^{()}\) respectively in the proof in Appendix I), then \(_{1},_{0}\) are also optimal. Ideally, if \(_{1} V()\) or \(_{0} V()\), the proof is done. However, this is not guaranteed since in Proposition 5 and Assumption 6, the convex combination coefficients differ among the states \(s\). To solve this challenge, it suffices to find such optimal \(_{1}\) that differs from \(^{*}\) at only one state \(s\) such that the non-vertex \(^{*}(s) V(_{s})\) is replaced with vertex \(_{1}(s) V(_{s})\). Then we can conduct such change from non-vertex to vertex for **only one state \(s\) at a time** until the kernel becomes vertex at every state, while keeping the optimality all the way. To find such \(_{1}(s)\), note that on polyhedral set \(_{s}\), there always exist \(_{1}(s) V(_{s})\) and \(_{0}(s)_{s}\), such that the non-vertex point \(^{*}(s)\) is a convex combination of \(_{1}(s)\) and \(_{0}(s)\), while \(^{*}(s^{})=_{1}(s^{})=_{0}(s^{})\) for any \(s^{} s\). Hence, there exists \(\) such that \(^{*}=_{}\) defined by Proposition 5, which implies that \(_{,^{*}}\) is a convex combination of \(_{,^{*}_{1}}\) and \(_{,^{()}}\).

### Globally Converged Algorithm

The original objective (2) is equivalent to the minimization problem \(_{}()\), where \(():=_{ V()}f(_{,})\) with finite vertex set \(V()\) based on Proposition 6. A natural choice to solve this minimization problem is the following policy update rule (for simplicity we consider the unconstrained policy space \(=^{d}\) as in [55; 6]).

\[_{k+1}=_{k}-_{k}d_{k},\] (22)

where \(_{k}>0\) is the stepsize and \(d_{k}\) is a unit descent direction of \((_{k})\). Subgradient descent method seems an obvious choice for \(d_{k}\) which aligns with the direction of a subgradient \(_{}f(_{_{k},_{k}})\) where \(_{k}_{ V()}f(_{_{k},})\). However, the convergence analysis of subgradient descent method  requires the convexity of \(f(_{,_{k}})\) which does not hold in our setting, and the function value is not monotonically decreasing. To solve these challenges, we design Algorithm 2 which selects near-optimal vertices \(_{k}:=\{ V():f(_{_{k},})_{^{} V ()}f(_{_{k},^{}})-_{k}\}\) with a certain threshold \(_{k}>0\) and obtains \(d_{k}\) by solving the convex optimization problem \(_{d B_{1}}A_{k}(d)\) up to precision \(_{k}>0\), where \(A_{k}(d)\) below denotes effective descent of \((_{k})\) along the direction \(d\).

\[A_{k}(d):=_{_{k}}_{}f(_{_{k},} )^{}d,d B_{1}:=\{d^{}^{d}:\|d^{ }\| 1\}.\] (23)

Here we only care about the near-optimal vertices in \(_{k} V()\) because for any worse vertices \( V()/_{k},f(_{k},)<_{^{} V()}f( _{_{k},^{}})-_{k}\) implies \(f(_{k+1},)<_{^{} V()}f(_{_{k+1},^ {}})\) for appropriate \(_{k}>0\). This means such worse \(\) can not affect the optimization progress \((_{k})-(_{k+1})\). Hence, by solving \(_{d B_{1}}A_{k}(d)\), we can obtain a direction \(d_{k}\) in which all the potentially effective function values \(\{f(_{_{k},})\}_{_{k}}\) have uniformly large amount of descent \(- f(_{_{k},})^{}d_{k}\).

To analyze the global convergence of Algorithm 2, we want to guarantee sufficient descent \((_{k})-(_{k+1})\) whenever \(_{k}\) is not close to optimal. It suffices to slightly alter Assumption 4 as follows.

**Assumption 8**.: _A variant of Assumption 4 holds which replaces the non-robust optimal policy \(^{*}()\) with a robust optimal policy \(^{*}_{}()\) and shrinks the range from \(\) to \( V()\)._

**Remark:** Assumption 8 is no stronger than Assumption 4 and also covers the popular direct policy parameterization. Also, Assumption 8 guarantees that from any policy \(\), there exists a partial curve \(\{_{}:[0,]\}\) towards a robust optimal policy \(^{*}\) such that \(_{_{},}=(1-)_{,}+_{ ^{*},}\), so we can utilize convexity of \(f\) and obtain the following important sufficient descent property.

**Proposition 7** (Sufficient Descent on Polyhedral Ambiguity Set).: _Under Assumptions 1-3 and 8, at any \(:=^{d}\), there exists a unit descent direction \(d\) (\(\|d\|=1\)) such that_

\[f(_{,})-f(_{^{*},}) -_{^{-1}}_{}f(_{, })^{}d_{+},\] (24)

_where \(^{*}_{^{}}()\) is given by Assumption 8 and \(x_{+}:=(x,0)\) for any \(x\)._

**Remark:**\(d\) in Proposition 7 is a good descent direction since whenever the function value gap \(f(_{,})-f(_{^{*},})>0\), it is dominated by the gradient descent amount \(-_{}f(_{,})^{}d>0\). Unlike existing gradient dominance properties for robust RL [42; 26; 17], \(f(_{,})-f(_{^{*},}) 0\) is possible so we use \([]_{+}\) to cover all cases. This brings challenge and thus novel techniques to obtain the first global convergence result of our robust RL with general convex utility as follows.

**Theorem 3** (Global Convergence for Convex Utility on Polyhedral Ambiguity Set).: _Implement Algorithm 2 with \(_{k}=_{}}{k+2}\), \(_{k}=_{}_{}}{k+2}\) and any \(_{k}>0\). Then under Assumptions 1-3, 6-8, thealgorithm output \(_{K}\) has the following global convergence rate._

\[(_{K})-_{^{}}(^{}) _{^{-1}}_{1 k K}_{k}+}}{K+1}(_{^{-1}}L_{,}+2_{}).\] (25)

**Remark:** The convergence rate \((1/K)\) matches the state-of-the-art of policy gradient type methods for robust RL , while the error term \(_{k}\) results from solving the convex optimization problem \(_{d B_{1}}A_{k}(d)\) in line 6 of Algorithm 2.

**Technical Novelty.** Applying Proposition 7 to Algorithm 2 with \(_{k}=2_{k}_{}\), we have

\[_{k}:=(_{k})-_{^{}}(^{ })_{^{-1}}[_{k}-A_{k}(d_{k}^{ })]_{+}+2_{k}_{}.\] (26)

To overcome the main difficulty caused by \([]_{+}\) above, we analyze each \(k\)-th iteration in **2 cases**\(A_{k}(d_{k}^{}) 0\) and \(A_{k}(d_{k}^{})<0\). If \(A_{k}(d_{k}^{}) 0\), then \(_{k}_{^{-1}}_{k}+2_{k}_{}\) and thus \(_{k+1}_{^{-1}}_{k}+3_{k}_{}\); If \(A_{k}(d_{k}^{})<0\), then in Eq. (26) we replace \(A_{k}(d_{k}^{})\) with \(A_{k}(d_{k}) A_{k}(d_{k}^{})<0\) and remove \([]_{+}\). This along with \((_{k+1})-(_{k})_{k}A_{k}(d_{k})+}{2}_{k}^{2}\) (by smoothness) implies

\[_{k+1}_{k}+} {k+2}+}.\] (27)

Then we obtain the rate (25) in **3 cases**: If \(A_{k}(d_{k}^{})<0\) for all \(k=0,1,,K-1\), iterate Eq. (27) from \(_{0}\); If \(A_{K-1}(d_{K-1}) 0\), \(_{K}_{^{-1}}_{K}+_{k}\); If \(A_{K^{}-1}(d_{K^{}-1}) 0\) while \(A_{k}(d_{k})<0\) for all \(k=K^{},,K-1\), iterate Eq. (27) from \(_{K^{}}_{^{-1}}_{K^{}-1}+3 _{K^{}-1}_{}\).

Algorithm 2 involves convex optimization problems \(_{d B_{1}}A_{k}(d)\), which can be solved via the following projected subgradient method for \(t=0,1,,T-1\).

\[d_{k,t+1}_{B_{1}}[d_{k,t}-_{}f( _{_{k},_{k,t}})],\ _{k,t}_{_{k}}_{}f(_{_{k},}) ^{}d_{k,t}.\] (28)

The best direction \(d_{k}^{}_{d\{d_{k,t}:0 t T\}}A_{k}(d)\) from the above subgradient method achieves \(_{k}\) accuracy within \(T=(_{k}^{-2})\) steps , which yields the following complexity result.

**Corollary 1**.: _Under the conditions of Theorem 3, for any \(>0\), implement Algorithm 2 with \(K=8_{^{-1}}^{-1}(_{^{-1}}L_{,}+2 _{})\) iterations and \(T=36_{^{-1}}^{2}_{}^{2}^{-2}\) subgradient descent updates (28) with stepsize \(=}_{}^{2}}\) to obtain \(d_{k}^{}\). Then the output \(_{K}\) achieves \((_{K})-_{^{}}(^{})\)._

Finally, we can prove that all these Assumptions 1-8 required by our convergence results (Theorems 2 and 3) can be satisfied by the following examples.

**Proposition 8**.: _Assumptions 1-8 are all satisfied if we use the following choices: \(\) Softmax policy parameterization \(_{}(a|s)=)}{_{a^{}}(_{,a^{ }})}\), where \(=[-R,R]^{||||}\) for some constant \(R>0\) to prevent \(_{}(a|s)\) from approaching 0. \(\) Direct kernel parameterization \(p_{}(s^{}|s,a)=_{s,a,s^{}}\) with \(s\)-rectangular \(L_{1}\) or \(L_{}\) ambiguity sets defined as \(=\{(^{S})^{S}:\|(s,:,:)-(s,:,:)\|_{p}_{s}, s\}\) for \(p\{1,\}\) respectively, where the fixed nominal kernel \(\) satisfies \((s,a,s^{})>_{s}, s,a,s^{}\) to prevent \(p_{}(s^{}|s,a)\) from approaching 0. \(\) The utility function \(f()\) defined in Eq. (7) for robust entropy regularized RL and its special cases, within the range \(=\{_{,}:,\}\) for the domains \(\) and \(\) selected above._

## 5 Conclusion

In this work, we propose robust RL with general utility, the first learning framework that obtains a robust policy for RL with general utility. We propose a stochastic policy gradient type algorithm for convex utilities and obtains its sample complexity result for gradient convergence. Furthermore, for convex utility on polyhedral ambiguity set, we propose an alternative policy gradient type algorithm and obtain its global convergence rate. Note that this globally converged algorithm requires enumeration among many vertices, and thus it is an important future direction to reduce enumeration by utilizing structural properties. In addition, to extend the results to large or continuous state-action space is also an interesting direction.