# Observational Scaling Laws and

the Predictability of Language Model Performance

 Yangjun Ruan\({}^{1,2,3}\)

yjruan@cs.toronto.edu

&Chris J. Maddison\({}^{2,3}\)

cmaddis@cs.toronto.edu

&Tatsunori Hashimoto\({}^{1}\)

thashim@stanford.edu

\({}^{1}\)Stanford University \({}^{2}\)University of Toronto \({}^{3}\)Vector Institute

###### Abstract

Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. We propose an alternative, _observational_ approach that bypasses model training and instead builds scaling laws from \(\)100 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, we show that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.

## 1 Introduction

Language model (LM) scaling plays a central role in discussions of model capabilities and affects everything from the tasks they can perform to the effectiveness of post-training techniques such as Chain-of-Thought . Due to this importance, understanding and predicting LM behaviors across scales, benchmarks, and algorithmic interventions is a major question for many researchers and engineers. Machine learning researchers may wish to understand whether their proposed algorithmic interventions remain effective in the face of future model scaling, while engineers and benchmark builders may wish to understand whether complex capabilities such as agentic abilities will scale predictably in the same way as existing LM benchmarks.

Scaling laws  have been powerful tools for understanding the scaling trend of LMs, which have shown that LMs follow a precise power-law relationship between compute measures (such as training FLOPs) and downstream capabilities ranging from perplexity  to benchmark performance . This power-law relationship has been used in a variety of ways - including hyperparameter and architecture selection  as well as model capability forecasting . Unfortunately, scaling analyses remain uncommon in many benchmarking and post-training studies, as most researchers do not have the compute resources to build scaling laws from scratch, and open models are trained at too few scales (3-5) for reliable scaling predictions.

We show that many scaling analyses, such as understanding complex LM capabilities (e.g., "emergent" behaviors) and post-training interventions, can be done with a lower-cost, higher-resolution, and broader-coverage alternative to the standard approach of training LMs across compute scales.

The starting point of our work is the observation that there now exist hundreds of open models spanning a large range of scales and capabilities. While we cannot directly use these models forcompute scaling laws (as the training compute efficiency varies widely across model families), we might hope that there exists a more general scaling law that holds across model families. In particular, we hypothesize that the downstream performance of an LM is a function of a low-dimensional space of capabilities (e.g., natural language understanding, reasoning, and code generation), and that model families vary only in the efficiency by which they convert training compute to these capabilities. If such a relationship held, it would imply that there is a log-linear relationship from low-dimensional capabilities to downstream capabilities _across_ model families (which would allow us to build scaling laws that leverage all existing models), as well as a log-linear relationship between training compute and capabilities _within_ each model family (as in standard compute scaling) (Fig. 1).

Through an analysis of standard LM benchmarks (e.g., Open LLM Leaderboard ), we find a few such capability measures that have scaling relationships with compute within model families (\(R^{2}>0.9\)) (Fig. 3), and with downstream metrics across families. We call such relationships _observational_ scaling laws as they predict complex downstream capabilities from simple observable quantities that we expect to scale with compute (like standardized benchmark performance)

The ability to build scaling laws across a large number of existing LMs from their standard benchmark metrics has significant advantages in cost, resolution, and coverage: Observational scaling incurs no training cost, while leveraging models spanning a much larger compute range than any single model family. It also significantly increases the resolution of scaling laws by virtue of using more models, which is useful for studying nearly discontinuous phenomena like "emergent" capabilities. Finally, observational scaling can combine model families from heterogeneous sources with very different scaling properties (e.g., LLaMA  vs StarCoder ) which allows us to study how different scaling strategies impact downstream performance and algorithmic interventions.

Finally, we show that using observational scaling laws is low-cost and straightforward, as there are a few model families that are sufficiently representative to replicate many of our core findings (Sec. 5). By using these representative families, we find that future works can easily make scaling predictions on benchmarks and post-training interventions by evaluating only 10-20 models.

We demonstrate the utility of observational scaling laws in three different settings that are challenging for compute scaling laws but are accurately predicted by ours: (i) **Emergent capabilities** (Sec. 4.1): We show that the high resolution of observational scaling laws reveals that the emergent behaviors of LMs  follow a smooth sigmoid, and can be predicted accurately using sub LLama- 2 7B models. (ii) **Agentic capabilities** (Sec. 4.2): We show that the more complex capabilities of LMs as agents, as measured by AgentBench  and AgentBoard , can be predicted with simple benchmark metrics. Our scaling law precisely predicts the GPT-4 performance using weaker models (sub GPT-3.5) and identifies programming capabilities as driving agent performance. (iii) **Post-training interventions** (Sec. 4.3): We show that our scaling laws can reliably predict the gains of post-training techniques, such as CoT  and Self-Consistency  at scale, even when they are fitted on weak models (sub

Figure 1: Observational scaling laws generalize existing compute scaling laws which directly relate training compute to downstream capabilities (dashed line) by hypothesizing the existence of a low-rank space of LM capabilities that have a log-linear relationship with compute (center), and can be extracted directly from standardized LM benchmarks (left). This enables us to get low-cost, high-resolution scaling predictions of LMs’ complex downstream capabilities from their observable standard benchmark metrics using nearly 100 publicly accessible LMs (left to right).

Llama-2 7B). Finally, we show how to select only 10-20 representative models to replicate our core findings, making our scaling analyses more accessible with a low cost (Sec. 5).

## 2 Related Work

In this section, we briefly review the most relevant related work on downstream scaling laws and benchmark correlations. We include an extended related work discussion in Appx. C.

Downstream scaling lawsScaling laws have been generalized beyond pretraining loss to analyze transfer learning  and downstream performance  across various domains. However, whether the LM downstream performance demonstrates a rapid "emergence" or is predictable with scaling laws remains debated . Finnweden  and Owen  have investigated the use of linear and sigmoidal scaling laws, derived from pretraining loss or computational measures, to extrapolate the benchmark performance. Arora and Goyal  derived a theory characterizing how LMs' complex skills can be derived as a composition of base skills. Our work differs in that we build practical higher-resolution scaling laws to predict LM downstream performance using multiple model families and their observable standard benchmark metrics.

Correlations between benchmarksNumerous works have studied the correlations between NLP benchmarks in various contexts . Most relevant to our work, Ilic  found that a single factor explains 85% of the variation on the Open LLM Leaderboard  and GLUE leaderboard , while Burnell et al.  extracted three factors for LM capabilities that account for 82% of the variation on HELM , aligning with our observations. Our work also observes such benchmark correlations and low-rank structures but is unique in utilizing these properties for the purpose of scaling predictions that can be used directly for benchmark and algorithm development.

## 3 Observational Scaling Laws

In this section, we introduce our observational scaling laws that generalize the standard compute scaling laws (Sec. 3.1). The key idea is to extract a low-dimensional capability measure for LMs from their observable benchmark performance (Sec. 3.2), which we find has a log-linear relationship with compute scale measures (Sec. 3.3) and can thus be used as surrogate "scale" for scaling analysis of complex LM capabilities (Sec. 3.4).

### Generalizing Compute Scaling Laws

Standard compute scalingIn _compute_ scaling laws, there is a hypothesized power-law relationship between models' compute measures \(C_{m}\) (e.g., training FLOPs) and their errors \(E_{m}\) (e.g., perplexity). Specifically, for a model \(m\) within a family \(f\) (e.g., Llama-2 7B, 13B, and 70B) we hypothesize

\[(E_{m})_{f}(C_{m})+_{f},\] (1)

and if this linear fit is sufficiently accurate, we draw inferences about the performance of a model at future compute scales \(C^{}>C\) by extrapolating this relationship. However, fitting such a scaling law can be tricky, as each model family \(f\) and downstream benchmark has its own scaling coefficients \(_{f}\) and \(_{f}\). This means that scaling experiments, especially for post-training analysis, are often fitted on very few (3-5) models sharing the same model family, and any predictions are valid only for a specific scaling strategy used within a model family.

Several studies [e.g., 25, 67] have generalized the functional form to analyze the scaling of LMs' downstream performance (where \(E_{m}\) is normalized to \(\)) with a sigmoidal link function \(\):

\[^{-1}(E_{m})_{f}(C_{m})+_{f},\] (2)

Observational scalingIn our work, we hypothesize the existence of a low-dimensional capability measure for LMs that relate compute to more complex LM capabilities and can be extracted from observable standard LM benchmarks, as illustrated in Fig. 1. Specifically, given \(T\) simple benchmarks and \(B_{i,m}\) the error of a model \(m\) on benchmark \(i[T]\), we hypothesize that there exists some _capability vector_\(S_{m}^{K}\) such that,

\[^{-1}(E_{m}) ^{}S_{m}+\] (3) \[S_{m} _{f}(C_{m})+_{f}\] (4) \[B_{i,m} _{i}^{}S_{m}.\] (5)

for \(_{f},_{f},^{K}\), \(\), and orthonormal vectors \(_{i}^{K}\).

We can view Eq. (3) and Eq. (4) as a generalization of Eq. (2), since combining them can recover the original scaling relationships for a single model family. However, when there are multiple model families, \(S_{m}\) serves as a shared, low-dimensional space of model capabilities from which all downstream metrics (\(E\) and \(B\)) are derived (as indicated by the absence of \(f\) in Eq. (3) and Eq. (5)), and model families only vary in their efficiency in converting compute into capabilities (Eq. (4)). One useful way of interpreting Eq. (4) is that \(_{f}\) represents the compute efficiency of a model family \(f\), and \(S_{m}\) is the capabilities of model \(m\) expressed in terms of log-FLOPs for this model family.

Finally, Eq. (5) ensures that these capabilities are not latent variables to be estimated for each model family, but are instead functions of fully observable properties (\(B\)). Since \(^{K T}\) is orthonormal, we can linearly estimate \(_{m}:= B_{m}\), which makes our scaling analysis significantly more robust. Importantly, this enables us to apply this to a large number of public models from heterogeneous sources, including those proprietary ones without any public information on \(C\) such as GPT-4.

### Identifying a Low-Dimensional Capability Space (Eq. (5))

We validate the existence of a low-dimensional capability measure \(S\) that linearly relates to standard LM benchmarks \(B\) by showing that only a few principal components of \(B\) capture most of its variation (Eq. (5)). We demonstrate that the benchmark-model matrix \(B\) for a reasonable, broad set of benchmarks and models is low-rank and that Eq. (5) is a reasonable assumption.

ModelsSince the benchmark-model matrix \(B\) can be directly measured for any LM, we include a large number of publicly accessible models for subsequent analysis. We collected a broad set of open LMs covering 21 model families (a collection of models across scales such as LLaMA-2 TB, 13B, 70B) and a total of 77 models. These encompass models trained from heterogeneous recipes, including standard training recipes like LLaMA , those trained on synthetic data like Phi , and models specifically trained on code data like StarCoder . For this analysis, we consider only pretrained base models to avoid the complexities introduced by instruction tuning. We also include an analysis for instruction-tuned models that include proprietary ones like GPT-4  in Appx. E.1, which demonstrates similar results. See Table D.1 for a detailed list of collected models.

BenchmarksWe collected a set of diverse benchmarks that assess various LMs' capabilities. These include popular aggregated benchmarks like MMLU  that assess the general knowledge of LMs. For more specialized evaluations, we included ARC-C , HellaSwag , Winogrande  for commonsense reasoning, GSM8K  for mathematical reasoning, HumanEval  for programming, TruthfulQA  for truthfulness, and XWinograd  for multilingual capabilities. We carefully collected these metrics from standardized evaluation protocols for comparability across LMs. In particular, we compiled them from standardized leaderboards, like the Open LLM Leaderboard  and EvalPlus , when available. Otherwise, we used standardized libraries such as the LM Eval Harness  to evaluate the LMs. See Appx. D.1 for full details of our data collection pipeline.

PCA analysisAfter obtaining the benchmark metrics for the LMs, we addressed potential missing values (less than \(1\%\) of all data), which may have occurred due to evaluation failures, by using PCA imputation. Subsequently, we applied PCA to extract the principal components of the evaluation metrics as the "principal capability" (PC) measures \(S\) (additional details in Appx. D.3).

Figure 2: Just a few capability dimensions explain most variability on a diverse range of standard LM benchmarks. We find that (a) the benchmark-model matrix is **low-dimensional** with the top 3 PCs explaining \( 97\%\) of the variance and (b) the PCs are **interpretable**: PC-1, PC-2, and PC-3 emphasize LMs’ general, reasoning, programming capabilities, respectively.

PC measures are low-dimensionalWe observe that the extracted PC measures are predominantly low-rank, with the top 3 PCs explaining \( 97\%\) of the variance, which supports a low-dimensional representation of benchmarks \(B\) (Fig. 2a). Surprisingly, we find that the first PC alone explains nearly 80% of the variation in LM capabilities. Taking a closer look at these PCs, we find that these capability measures represent interpretable directions in which LMs capabilities may naturally vary as a function of scale (Fig. 2b). Specifically, PC-1 represents the "general capability" as a weighted average of all metrics; PC-2 corresponds to the "reasoning capability", emphasizing mathematical and coding benchmarks; and PC-3 primarily reflects the "programming capability". These findings suggest that many simple LM capabilities (as covered in our benchmarks) can be expressed as a linear combination of just a few "principal capabilities" \(S\).

### Principal Capability Measures as Surrogate Scale Measures (Eq. (4))

We now show that the PC measures \(S\) scale log-linearly with training FLOPs within each model family, and can thus be interpreted as a cross-family generalization of compute \(C\). We discuss some additional applications of PC measures as a smooth cross-family evaluation metric in Appx. B.

SetupWe collected all available information about training FLOPs on each of our models, analyzing papers and other public information to identify model size \(N\) and pretraining data size \(D\). For the models where we were able to identify this information, we used the simple estimate of \(C 6ND\) to obtain model training FLOPs . See Table D.1 for our collected compute measures.

PC measures linearly correlate with log-compute measuresFig. 3 illustrates the correlation between the top PC-1 measure with the corresponding training FLOPs for models within each model family. We find that for each model family with controlled training recipes and comparable compute scale measures, the LMs' PC-1 measure _linearly_ correlates with their log-training FLOPs (with \(R^{2}>0.9\)). This linear correlation holds across a broad range of model families including those specifically trained on multilingual data like BLOOM  or those on code like StarCoder . It also generally holds for lower-ranked PCs such as PC-2 and PC-3, as shown in Fig. E.2. Together with Sec. 3.2, these results support the validity of Equations (4) and (5), in which we hypothesized that models share the same capability space and a log-linear relationship determines the efficiency by which each model family converts their compute into these principal capabilities.

### Fitting Observational Scaling Laws (Eq. (3))

Fitting regression with PC measuresGiven a certain downstream error metric \(E\) normalized to \(\) that measures certain LM capabilities, we slightly generalize Eq. (3) to

\[E_{m} h(^{}S_{m}+)\] (6)

where \(^{K}\) and \(\) are the regression weights and bias, \(h\) is the sigmoidal scale that accounts for the potential discrepancies in the floor performance. We fit the regression with ordinary least squares and restrict \(h[0.8,1.0]\), which results in \(h^{*}=1\) in most experiments.

Defining interpretable compute-like measuresRecall that the core component of our scaling law is the fitted linear transformation \(P_{m}}^{}S_{m}+^{*}\) that maps the extracted PCs into a scalar capability measure for a target downstream metric. While this is perfectly acceptable for prediction, our scaling analysis would be more interpretable if we expressed capabilities in units of FLOPs rather than an arbitrary scalar measure. We can achieve this by utilizing the fact that for a single family \(f\), our observational scaling law reduces to a compute scaling law (Eq. (3) & Eq. (4)). Specifically, we

Figure 3: The extracted PC measures _linearly correlate_ with log-compute within each model family. The linearity generally holds for various model families, and also for lower-ranked PCs (Fig. E.2).

note that when Eq. (4) holds exactly, we have that for a model \(m\) within a family \(f\),

\[P_{m}:={^{*}}^{}S_{m}+^{*}=w_{f}(C_{m})+b_{f}\] (7)

where \(w_{f}={^{*}}^{}_{f}\) and \(b_{f}={^{*}}^{}_{f}+^{*}\). This implies a linear correlation between the scalar capability \(P_{m}\) and the compute \((C)\) for models within a specific family on a downstream task (see empirical validation in Fig. E.3). Since \(_{f}\) and \(_{f}\) are unknown a priori, we can fit these coefficients \(w_{f},b_{f}\) via linear regression from \((C)\) to \(P\) using models from the specific family \(f\).

In the multi-model family case, we can map all models to a shared, FLOPs-based capability measure of a specific family \(f\). The core idea is to represent each model's capabilities by the following hypothetical: "how many FLOPs (\(_{m,f}\)) would it take for a model in a family \(f\) to match a model \(m\)". We call \(_{m,f}\) the \(f\)**-equivalent FLOPs** for model \(m\), as it represents the performance of model \(m\) relative to models in the reference model family \(f\). This measure can be computed fairly easily as

\[(_{m,f}):=^{*}}({^{*}}^{}S_{m}+^ {*}-b_{f}^{*}),\] (8)

obtained from solving for \((C_{m})\) in Eq. (7). Throughout the remainder of this work, we apply this scalar transformation where we pick Llama-2  as the reference family \(f\), and so the x-axis of all of our plots can be interpreted as "model capabilities, as measured in units of Llama-2 FLOPs".

## 4 Validating Observational Scaling Laws

We evaluate the usefulness of observational scaling laws by showing that they accurately predict the scaling behaviors of LMs over complex, hard-to-predict phenomena (like emergent phenomena and agentic abilities) and help estimate the value of techniques such as Chain-of-Thought.

To ensure that our scaling laws are actually predictive and that we are not simply overfitting through various choices in scaling law construction and hyperparameters, we design our experiments to have systematic holdout sets and robustness checks. We have also preregistered our predictions for _future_ models after the initial release of the paper as a test of whether our scaling laws overfit current models. We release our code including the implementation and collected data at https://github.com/ryoungj/ObsScaling.

Details in scaling law fitsFor extracting PC measures, we fixed the number of PCs \(K=3\) as it covered \( 97\%\) of the variation in benchmark performance and it consistently yielded the best performance across most of our experiments, see Appx. E.4 for robustness checks on PC selection. For the capability-equivalent scale transformation, we used the Llama-2  as the reference model family as it is currently the most representative and widely used open model in the community. For better interpretability and visualization, we used the accuracy metric, typically defined as \(Y=1-E\), for fitting the scaling laws and making the plots.

Holdout validationTo validate our observational scaling laws, our primary objective is to assess how accurately the scaling laws fit the available data and extrapolate from smaller-scale, less capable models to larger-scale, more powerful models. We validate this through systematic holdouts for the test set, where we split available models into weaker and stronger ones based on both scale or capability (e.g., FLOPs or accuracy). We used the weaker models to fit the scaling law and evaluated the extrapolated predictions on the stronger ones. To prevent any train-test leakage, all preprocessing steps (e.g., PCA imputation) were fitted on the train set only and then applied to the test set. Unless otherwise stated, we set the cutoff to include all models with training FLOPs less than or equal to that of Llama-2-7B (\(8.4 10^{22}\)) as training data, resulting in a training set of 47 models and a test set of 30 models. We included robustness checks for different holdout strategies in Appx. E.4.

As baselines, we compare our scaling predictions to using existing compute-based scale measures like training FLOPs and model size. We used the mean squared error (MSE) on the test set as our main evaluation measure, which is comparable as the target range is always normalized (0 to 1).

Preregisteration of predictionsIn the initial release of our paper (May 2024), we have preregistered our scaling predictions for future models (see preregistered functional forms in Appx. E.9) and committed to updating the manuscript on ArXiv with our prediction results after 4 months. We have assessed these predictions on new models released after the initial paper release, collected as of September 1st 2024, including most capable open models to date such as Llama 3.1-405B  and Qwen2-72B  (see the full collected model list in Appx. D.1.1), resulting in an additional test set of 20 models for robustness checks. The results are included in Fig. 4, and additional results on other tasks and new benchmarks are included in Appx. E.3.

### Predictability of "Emergent" Capabilities

Recent works have argued that many LM capabilities are "_emergent_" and cannot easily be predicted from small-scale models [27; 98]. There have been ongoing debates about whether these capabilities are truly discontinuous and whether the discontinuity is an artifact of the metric used [23; 39; 60; 78] or lack of high-resolution data points . The debate has been complicated by the fact that existing scaling analyses (including the original ones in Wei et al. ) have very few points . When there are only 5 models across many orders of magnitudes of scale, phenomena can appear to be discontinuous, even if the underlying phenomenon is a smooth but rapidly varying sigmoid.

We show that the higher resolution of observational scaling laws allows us to clearly see smooth sigmoidal curves in phenomena that were identified as emergent in Wei et al. , and even more surprisingly, we can often accurately forecast the transition points where models go from near-random to high performance using only models whose performance is only slightly above random. Our findings validate the observational approach to scaling laws and provide evidence that higher-resolution scaling laws could help us better understand scaling phenomena for LMs.

SetupWe tested on four BigBench  tasks that were labeled as "emergent" in Wei et al. , including two arithmetic tasks (3-digit subtraction and 2-digit multiplication) and two non-arithmetic tasks (word unscramble and Persian QA). Additional results on more tasks covering Wei et al.  are included in Appx. E.5. For the models, we included base pretrained models following the approach of Wei et al. . For non-arithmetic tasks, we used the default FLOPs cutoff. For arithmetic tasks, we found that this cutoff resulted in an excess of training data near perfect performance (see results in Fig. 13), making the prediction tasks trivial. Consequently, we reduced the cutoff to a quarter of the default value and also excluded GSM8K (which may be a superset of arithmetic tasks) from our base metrics \(B\) to make the tasks more challenging.

Prediction resultsFig. 4 shows our prediction results using our PC measures as well as the baseline of predicting performance based on training FLOPs. We find that these capabilities can be accurately predicted using our PC measures, even when only using models that perform poorly. In contrast, using training FLOPs results in significantly poorer extrapolation on the test set and fits on the train set, as indicated by the much higher MSE values. This discrepancy is likely due to the incomparability of training FLOPs across different model families. Additional results of the model size baseline are included in Appx. E.5.

Figure 4: “Emergent” capabilities of LMs can be accurately predicted from weaker models to stronger ones with observational scaling laws, and using PC measures as the predictor provides much more accurate predictions than using compute measures like training FLOPs and model size (see Fig. 12). Our preregistered predictions also accurately extrapolate to new models released after the initial paper release, including Llama-3.1-405B . Four tasks from BigBench , which are identified as “emergent” in , are used for illustration.

### Predictability of Agentic Capabilities

There is significant interest in building autonomous agents using LMs, with notable examples including AutoGPT , Devin , and SWE-agent . Although the performance of these agents still falls far below human-level on challenging real-world tasks [43; 62; 110], there is a belief that future models at larger scales will significantly enhance these agents' capabilities. However, there is a significant uncertainty about whether existing models that are trained for language and code capabilities will transfer well to agentic tasks that require taking actions over many rounds. In this section, we utilize our observational scaling laws to analyze the scaling properties of LMs' agentic capabilities w.r.t. their backbone model capabilities and show that agent performance is highly predictable from simple benchmark metrics.

SetupWe tested on two standardized agent evaluation benchmarks, AgentBench  and AgentBoard , each is a collection of diverse tasks for evaluating LMs' agentic capabilities. For both benchmarks, we utilized their provided aggregated metrics on all tasks for prediction. Specifically, we used the "Overall Score" on AgentBench, which is a weighted average of scores across all tasks (denoted as "OA" there), and the "Average Success Rate" on AgentBoard. We included models that have been evaluated on each benchmark, which encompasses both open instruction-tuned models like LLaMA-2-Chat , and proprietary models like GPT-4  and Claude-2 , see Table D.2 for a complete list of models. We followed the same procedure to collect standardized benchmark metrics \(B\) for instruction-tuned models, see Appx. D.1.2 for details. Notably, since compute scale measures are not available for proprietary models, only our observational scaling laws apply here and not compute scaling laws. The default FLOPs cutoff does not apply either, and thus we held out the top 10% performing models on each agent benchmark as the test set to simulate weak-to-strong predictions, which included GPT-4 and Claude-2 on AgentBench and GPT-4 on AgentBoard.

Prediction resultsFig. 5 illustrates the prediction results with our observational scaling laws using PC measures. We find that on both agent benchmarks, the performance of held-out models (GPT-4/Claude-2) can be accurately predicted from models with much weaker performance (> 10% gap). This indicates that the more complex agentic capabilities of LMs are well-correlated with and predictable from their base model capabilities, suggesting the promising scaling properties of LM-based agent capabilities as backbone LMs continue to scale up.

Interpreting the capability dimensionsIn Fig. 4(c), we visualize the weights assigned to the base evaluation metrics on both benchmarks, which are derived from the regression weights fitted on PC measures and applied with learned PCA transformation, i.e., \(^{}\). We observe that the fitted weights assign significant importance to programming capabilities (HumanEval) on both benchmarks, underscoring its significance in defining the agentic capabilities of LMs. The weights also emphasize general knowledge (MMLU) on AgentBench, and reasoning capabilities (GSM8K) on AgentBoard, suggesting that these capabilities may also be important for LMs' agentic capabilities.

### Predicting the Impact of Post-Training Techniques

When researchers propose a new prompting or post-training technique to improve a pretrained model, how can we know whether these gains will persist across models and scales? Systematic scaling analyses have been rare due to the small number of models within a single model family. Moreover, some recent works have argued that certain interventions, such as Chain-of-Thought , behave in an emergent way that is not predictable from smaller models . Using observational scaling laws, we show that it is possible to make relatively accurate predictions on the effectiveness of techniques

Figure 5: (a)-(b) The agentic capabilities of instruction-tuned LMs measured by agent benchmarks can be accurately predicted from weaker models (sub GPT-3.5) to stronger ones (e.g., GPT-4) by their PC measures. (c) The fitted weights (\(^{}\)) on both benchmarks demonstrate the importance of programming capabilities (HumanEval) for the agentic capabilities of LMs.

such as Chain-of-Thought (CoT)  and Self-Consistency (SC)  as model scale increases. We focus on these post-training interventions in particular, as they are sometimes discussed as examples of post-training interventions that require scale to be effective [98; 99].

Our approach to quantifying the scaling properties of post-training is straightforward: we fit one observational scaling law using base model performance on a target benchmark (e.g., GSM8K few-shot), and then fit another on the performance of models with the post-training intervention (e.g., GSM8K w/ CoT). Each of these fits produces a sigmoidal scaling curve as a function of \((_{f})\), and the relative gaps as a function of \((_{f})\) indicates the scaling efficiency of the intervention.

SetupWe tested on GSM8K with CoT and SC as post-training techniques and included additional results on BigBench-Hard  with CoT in Appx. E.6. As with our study on emergent phenomena on arithmetic tasks, we excluded GSM8K from the base metrics \(B\) to avoid making the prediction tasks trivial. We included all the pretrained base models listed in Table D.1 including those specifically trained for code data and applied the default FLOPs cutoff for holdout validation. For CoT, we followed Wei et al.  and compared CoT prompting using eight reasoning examples with naive prompting using only few-shot examples in the greedy decoding setting. For SC, we sampled five CoT reasoning paths at temperature 0.7 to aggregate the final answers following Wang et al.  and compared it with a single sampled CoT answer.

Prediction resultsFig. 5(a) shows the scaling predictions for CoT and SC using observational scaling laws. We find that the performance with (CoT, CoT + SC) and without (Naive) post-training techniques for stronger, larger scale models can be accurately predicted from weaker, smaller scale models. In contrast, predictions based on compute scale measures like model size and training FLOPs are less reliable as seen in Fig. E.15. Notably, the scaling trends between the two techniques differ; CoT shows a much more pronounced scaling trend compared to Self-Consistency w/ CoT.

Interpreting the capability dimensionsAnother advantage of observational scaling laws over scaling laws constructed on single families is that we can visualize the capabilities that are important to the post-training intervention. Fig. 5(b) visualizes the fitted regression weights \(\), mapped to the space of base capability benchmarks \(B\) via \(^{}\). We clearly see that when we go from Naive to CoT, there are significantly higher weights placed on MMLU and HumanEval - meaning that scaling models in a way that enhances general knowledge (MMLU) and code (HumanEval) leads to greater gaps between CoT and the baseline, while improving along commonsense, such as Winogrande does not necessarily lead to improvements at scale. These analyses can inform how different post-training interventions affect different scaling recipes - such as code models vs general-purpose LLMs.

## 5 Selecting Low-Cost Model Subsets for Practical Scaling Analyses

Although our observational scaling law incurs no training cost, it still requires evaluating our benchmarks and post-training methods on a larger number of models. To make observational scaling analyses more broadly accessible, we identify a small set of representative models that maintain high prediction accuracy while significantly reducing the evaluation cost.

MethodMore specifically, we consider the constrained optimization problem of identifying the optimal set of models to choose for a regression problem, subject to the constraint that we select a model subset \(\) of at most \(M_{}\) models from the set of all models \(_{a}\). To define optimality, we turn

Figure 6: (a) The LM performance with and without techniques like CoT and Self-Consistency can be accurately predicted with observational scaling laws. The fitted scaling curves indicate that CoT has a better scaling behavior than SC. See Fig. E.15 for detailed per-method scaling plots and comparison with compute baselines. (b) The fitted weights (\(^{}\)) demonstrate a very different pattern when CoT is applied, emphasizing general knowledge (MMLU) and programming capabilities (Humaneval).

to the theory of optimal experimental design, which states that for linear regression with a fixed design \(X\) and subset \(\), the expected prediction error from using the subset \(X_{}\) is \((X^{}X(X_{}^{}X_{})^{-1})\). This gives a straightforward objective achieving the _V-optimality_:

\[_{(_{a})||  M_{}}(S^{}S(S_{}^{}_{})^{-1})\] (9)

where \(S^{M K}\) is the model-capability matrix obtained from our PC analysis. We conduct a structured, exhaustive search over the 21 model families where we include or exclude entire model families under the budget constraint, as we believe these selected models are more interpretable.

ValidationWe followed the setup in Sec. 4.3 for validating our selection method, as this represents the most likely application scenario for our observational scaling laws by practitioners. Our objective is to replicate our scaling analysis (using a full set of 47 models) in Fig. 5(a) using a small subset of models selected by our method. In Fig. 5(a), we compute the geometric average of test MSEs on all prediction tasks (Naive, CoT, CoT + SC) as the evaluation metric for different selection methods. We find that our V-optimality selection method significantly outperforms random selection and quickly converges to the prediction performance of using the full set of models. In Fig. 6(b), we show that using only a small subset of 12 models selected by our method, the fitted scaling curves already effectively capture the scaling trends of different post-training methods, in contrast to randomly selected models (Fig. E.18). To facilitate future scaling analyses at alow cost, we provide a reference list of models selected with our method under different budget constraints in Table E.1.

## 6 Conclusion, Limitations, and Future Work

We have presented observational scaling laws that generalize existing compute scaling laws to handle multiple model families using a shared, low-dimensional capability space. Using this approach, we show that we can build low-cost, high-resolution, and broad-coverage scaling laws that allow us to make accurate predictions for many complex scaling phenomena, such as emergent behaviors, agentic capabilities, and the value of post-training interventions. We provide concrete practical prescriptions for practitioners to perform similar scaling analyses in the hopes of encouraging more quantitative, scaling-law-based approaches to designing benchmarks and post-training methods.

Limitations and future workFinally, we discuss some limitations of our approach and findings: Firstly, observational scaling laws are primarily applicable to post-training scaling analyses and do not directly translate to pretraining scenarios in the same way as standard compute-based scaling laws. Secondly, our study mostly focuses on the scaling behavior of model capabilities measured through few-shot prompting or basic prompting techniques (such as CoT, self-consistency, or simple agent scaffolding). Extending our approach to other post-training setups, including scenarios involving fine-tuning or more intensive inference-time computation [12; 81], would be valuable. Thirdly, while we have demonstrated that our observational scaling analyses can provide meaningful insights into improving particular models' complex capabilities, a promising direction for future work would be to apply the findings from our approach, such as by deriving surrogate measures for model complex capabilities that can be used to optimize models directly and efficiently. Lastly, our assumptions do not account for potential benchmark contamination (where particular benchmark data leaks into model training) or the heterogeneity within model families (where models within the same family may have varying compute efficiencies and scaling behaviors). Investigating the impact of these assumptions on our approach would be an interesting avenue for future research.

Figure 7: (a) Selecting the model subsets with our V-optimality criterion leads to significantly lower errors than random selection, and quickly converges to the errors of using the full set of models. (b) Using 12 (out of 47) models selected by our method maintains the overall prediction accuracy.