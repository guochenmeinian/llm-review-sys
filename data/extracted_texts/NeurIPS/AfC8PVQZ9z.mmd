# Multi-task Representation Learning for Pure Exploration in Bilinear Bandits

Subhojyoti Mukherjee

ECE Department

UW-Madison

Wisconsin, Madison

smukherjee27@wisc.edu

Qiaomin Xie

ISye Department

UW-Madison

Wisconsin, Madison

Robert Nowak

ECE Department

UW-Madison

Wisconsin, Madison

###### Abstract

We study multi-task representation learning for the problem of pure exploration in bilinear bandits. In bilinear bandits, an action takes the form of a pair of arms from two different entity types and the reward is a bilinear function of the known feature vectors of the arms. In the _multi-task bilinear bandit problem_, we aim to find optimal actions for multiple tasks that share a common low-dimensional linear representation. The objective is to leverage this characteristic to expedite the process of identifying the best pair of arms for all tasks. We propose the algorithm GOBLIN that uses an experimental design approach to optimize sample allocations for learning the global representation as well as minimize the number of samples needed to identify the optimal pair of arms in individual tasks. To the best of our knowledge, this is the first study to give sample complexity analysis for pure exploration in bilinear bandits with shared representation. Our results demonstrate that by learning the shared representation across tasks, we achieve significantly improved sample complexity compared to the traditional approach of solving tasks independently.

## 1 Introduction

Bilinear bandits (Jun et al., 2019; Lu et al., 2021; Kang et al., 2022) are an important class of sequential decision-making problems. In bilinear bandits (as opposed to the standard linear bandit setting) we are given a pair of arms \(_{t}^{d_{1}}\) and \(_{t}^{d_{2}}\) at every round \(t\) and the interaction of this pair of arms with a low-rank hidden parameter, \(_{*}^{d_{1} d_{2}}\) generates the noisy feedback (reward) \(r_{t}=_{t}^{}_{*}_{t}+_{t}\). The \(_{t}\) is random \(1\)-subGaussian noise.

A lot of real-world applications exhibit the above bilinear feedback structure, particularly applications that involve selecting pairs of items and evaluating their compatibility. For example, in a drug discovery application, scientists may want to determine whether a particular (drug, protein) pair interacts in the desired way (Luo et al., 2017). Likewise, an online dating service might match a pair of people and gather feedback about their compatibility (Shen et al., 2023). A clothing website's recommendation system may suggest a pair of items (top, bottom) for a customer based on their likelihood of matching (Reyes et al., 2021). In all of these scenarios, the two items are considered as a single unit, and the system must utilize available feature vectors \((_{t},_{t})\) to learn which features of the pairs are most indicative of positive feedback in order to make effective recommendations. All the previous works in this setting (Jun et al., 2019; Lu et al., 2021; Kang et al., 2022) exclusivelyfocused on maximizing the number of pairs with desired interactions discovered over time (regret minimization). However, in many real-world applications where obtaining a sample is expensive and time-consuming, e.g., clinical trials (Zhao et al., 2009; Zhang et al., 2012), it is often desirable to identify the optimal option using as few samples as possible, i.e., we face the pure exploration scenario (Fiez et al., 2019; Katz-Samuels et al., 2020) rather than regret minimization.

Moreover, in various decision-making scenarios, we may encounter multiple interrelated tasks such as treatment planning for different diseases (Bragman et al., 2018) and content optimization for multiple websites (Agarwal et al., 2009). Often, there exists a shared representation among these tasks, such as the features of drugs or the representations of website items. Therefore, we can leverage this shared representation to accelerate learning. This area of research is called multi-task representation learning and has recently generated a lot of attention in machine learning (Bengio et al., 2013; Li et al., 2014; Maurer et al., 2016; Du et al., 2020; Tripuraneni et al., 2021). There are many applications of this multi-task representation learning in real-world settings. For instance, in clinical treatment planning, we seek to determine the optimal treatments for multiple diseases, and there may exist a low-dimensional representation common to multiple diseases. To avoid the time-consuming process of conducting clinical trials for individual tasks and collecting samples, we utilize the shared representation and decrease the number of required samples.

The above multi-task representation learning naturally shows up in bilinear bandit setting as follows: Let there be \(M\) tasks indexed as \(m=1,2,,M\) with each task having its own hidden parameter \(_{m,*}^{d_{1} d_{2}}\). Let each \(_{m,*}\) has a decomposition of \(_{m,*}=_{1}_{m,*}_{2}^{-}\), where \(_{1}^{d_{1} k_{1}}\) and \(_{2}^{d_{2} k_{2}}\) are shared across tasks, but \(_{m,*}^{k_{1} k_{2}}\) is specific for task \(m\). We assume that \(k_{1},k_{2} d_{1},d_{2}\) and \(M d_{1},d_{2}\). Thus, \(_{1}\) and \(_{2}\) provide a means of dimensionality reduction. Furthermore, we assume that each \(_{m,*}\) has rank \(r\{k_{1},k_{2}\}\). In the terminology of multi-task representation learning \(_{1},_{2}\) are called _feature extractors_ and \(_{m,t},_{m,t}\) are called _rich observations_(Yang et al., 2020, 2022; Du et al., 2023). The reward for the task \(m\{1,2,,M\}\) at round \(t\) is

\[r_{m,t}=_{m,t}^{}_{m,*}_{m,t}+_{m, t}=_{m,t}^{}_{1}}_{_{m,t}^{}} _{m,*}_{1}^{}_{m,t}}_{_ {m,t}}+_{m,t}=_{m,t}^{}_{m,*}_{m,t}+ _{m,t}.\] (1)

Observe that similar to the learning procedure in Yang et al. (2020, 2022), at each round \(t=1,2,\), for each task \(m[M]\), the learner selects a left and right action \(_{m,t}\) and \(_{m,t}\). After the player commits the batch of actions for each task \(\{_{m,t},_{m,t}:m[M]\}\), it receives the batch of rewards \(\{r_{m,t}:m[M]\}\). Also note that in (1) we define the \(}_{m,t}^{k_{1}},}_{m,t} ^{k_{2}}\) as the latent features, and both \(}_{m,t},}_{m,t}\) are unknown to the learner and needs to be learned for each task \(m\) (hence the name multi-task representation learning).

In this paper, we focus on pure exploration for multi-task representation learning in bilinear bandits where the goal is to find the optimal left arm \(_{m,*}\) and right arm \(_{m,*}\) for each task \(m\) with a minimum number of samples (fixed confidence setting). First, consider a single-task setting and let \(_{*}\) have low rank \(r\). Let the SVD of the \(_{*}=^{}\). Prima-facie, if \(\) and \(\) are known then one might want to project all the left and right arms in the \(r r\) subspace of \(\) and \(\) and reduce the bilinear bandit problem into a \(r^{2}\) dimension linear bandit setting. Then one can apply one of the algorithms from Soare et al. (2014); Fiez et al. (2019); Katz-Samuels et al. (2020) to solve this \(r^{2}\) dimensional linear bandit pure exploration problem. Following the analysis of this line of work (in linear bandits) (Mason et al., 2021; Mukherjee et al., 2022, 2023) one might conjecture that a sample complexity bound of \((r^{2}/^{2})\) is possible where \(\) is the minimum reward gap and \(()\) hides log factors. Similarly, for the multi-task setting one might be tempted to use the linear bandit analysis of Du et al. (2023) to convert this problem into \(M\) concurrent \(r^{2}\) dimensional linear bandit problems with shared representation and achieve a sample complexity bound of \((Mr^{2}/^{2})\). However, these matrices (subspaces) are unknown and so there is a model mismatch as noted in the regret analysis of bilinear bandits (Jun et al., 2019; Lu et al., 2021; Kang et al., 2022). Thus it is difficult to apply the \(r^{2}\) dimensional linear bandit sample complexity analysis. Following the regret analysis of bilinear bandit setting by Jun et al. (2019); Lu et al. (2021); Kang et al. (2022) we know that the effective dimension is actually \((d_{1}+d_{2})r\). Similarly for the multi-task representation learning the effective dimension should scale with the learned latent features \((k_{1}+k_{2})r\). Hence the natural questions to ask are these:

**1) Can we design a single-task pure exploration bilinear bandit algorithm whose sample complexity scales as \(((d_{1}+d_{2})r/^{2})\)?**

**2) Can we design an algorithm for multi-task pure exploration bilinear bandit problem that can learn the latent features and has sample complexity that scales as \((M(k_{1}+k_{2})r/^{2})\)?**

In this paper, we answer both these questions affirmatively. In doing so, we make the following novel contributions to the growing literature of multi-task representation learning in online settings:

**1)** We formulate the multi-task bilinear representation learning problem. To our knowledge, this is the first work that explores pure exploration in a multi-task bilinear representation learning setting.

**2)** We proposed the algorithm GOBLIN for a single-task pure exploration bilinear bandit setting whose sample complexity scales as \(((d_{1}+d_{2})r/^{2})\). This improves over RAGE (Fiez et al., 2019) whose sample complexity scales as \(((d_{1}d_{2})/^{2})\).

**3)** Our algorithm GOBLIN for multi-task pure exploration bilinear bandit problem learns the latent features and has sample complexity that scales as \((M(k_{1}+k_{2})r/^{2})\). This improves over DouExpDes (Du et al., 2023) whose samples complexity scales as \((M(k_{1}k_{2})/^{2})\).

**Preliminaries:** We assume that \(\|\|_{2} 1\), \(\|\|_{2} 1\), \(\|_{*}\|_{F} S_{0}\) and the \(r\)-th largest singular value of \(_{*}^{d_{1} d_{2}}\) is \(S_{r}\). Let \(p d_{1}d_{2}\) denote the ambient dimension, and \(k=(d_{1}+d_{2})r\) denote the effective dimension. Let \([n]\{1,2,,n\}\). Let \(_{*},_{*}_{,}^{}_{*}\). For any \(,\) define the gap \((,)_{*}^{}_{* }_{*}-^{}_{*}\) and furthermore \(=_{_{*},_{*}}\), \((,)\). Similarly, for any arbitrary vector \(\) define the gap of \(^{p}\) as \(()(_{*}-)^{}_ {*}\), for some \(_{*}^{p}\) and furthermore, \(=_{_{*}}\), \(()\). If \(_{ 0}^{d d}\) is a positive semidefinite matrix, and \(^{p}\) is a vector, let \(\|\|_{}^{2}:=^{}\) denote the induced semi-norm. Given any vector \(^{||}\) we denote the \(\)-th component as \(_{}\). Let \(_{}:=\{^{||}:_{} 0,_{}_{}=1\}\) denote the set of probability distributions on \(\). We define \(()=\{-^{}:, ^{},^{}\}\) as the directions obtained from the differences between each pair of arms and \(^{*}()=\{_{*}-: _{*}\}\) as the directions obtained from the differences between the optimal arm and each suboptimal arm.

## 2 Pure Exploration in Single-Task Bilinear Bandits

In this section, we consider pure exploration in a single-task bilinear bandit setting as a warm-up to the main goal of learning representations for the multi-task bilinear bandit. To our knowledge, this is the first study of pure exploration in single-task bilinear bandits. We first recall the single-task bilinear bandit setting as follows: At every round \(t=1,2,\) the learner observes the reward \(r_{t}=_{t}^{}_{*}_{t}+_{t}\) where the low rank hidden parameter \(_{*}^{d_{1} d_{2}}\) is unknown to the learner, \(_{t}^{d_{1}},\,_{t}^{d_{2}}\) are visible to the learner, and \(_{t}\) is a \(1\)-sub-Gaussian noise. We assume that the matrix \(_{*}\) has a low rank \(r\) which is known to the learner and \(d_{1},d_{2} r\). Finally recall that the goal is to identify the optimal left and right arms \(_{*},_{*}\) with a minimum number of samples.

We propose a phase-based, two-stage arm elimination algorithm called **G-Optimal** Design for **Bilinear** Bandits (abbreviated as GOBLIN). GOBLIN proceeds in phases indexed by \(=1,2,\) As this is a pure-exploration problem, the total number of samples is controlled by the total phases which depends on the intrinsic problem complexity. Each phase \(\) of GOBLIN consists of two stages; the estimation of \(_{*}\) stage, which runs for \(_{}^{E}\) rounds, and pure exploration in rotated arms stage that runs for \(_{}^{G}\) rounds. We will define \(_{}^{E}\) in Section 2.1, while rotated arms and \(_{}^{G}\) are defined in Section 2.2. At the end of every phase, GOBLIN eliminates sub-optimal arms to build the active set for the next phase and stops when only the optimal left and right arms are remaining. Now we discuss the individual stages that occur at every phase \(\) of GOBLIN.

### Estimating Subspaces of \(_{*}\) (Stage 1 of the \(\)-th phase)

In the first stage of phase \(\), GOBLIN estimates the row and column sub-spaces \(_{*}\). Then GOBLIN uses these estimates to reduce the bilinear bandit problem in the original ambient dimension \(p d_{1}d_{2}\) to a lower effective dimension \(k(d_{1}+d_{2})r\). To do this, GOBLIN first vectorizes the \(^{d_{1}},^{d_{2}}\) into a new vector \(}^{p}\) and then solves the \(E\)-optimal design in Step \(3\) ofAlgorithm 1(Pukelsheim, 2006; Jun et al., 2019; Du et al., 2023). Let the solution to the \(E\)-optimal design problem at the stage \(1\) of \(\)-th phase be denoted by \(_{}^{E}\). Then GOBLIN samples each \(}\) for \([_{}^{E}_{,}}^{E}]\) times, where \(_{}^{E}=(d_{2}r}/S_{r})\) (step \(7\) of Algorithm 1). In this paper, we sample an arm \([_{}^{E}_{,}}^{E}]\) number of times. However, this may lead to over-sampling of an arm than what the design (\(G\) or \(E\)-optimal) is actually suggesting. However, we can match the number of allocations of an arm to the design using an _efficient Rounding Procedures_(see Pukelsheim (2006); Fiez et al. (2019)). Let \(}_{}\) be estimate of \(_{*}\) in stage \(1\) of phase \(\). GOBLIN estimates this by solving the following well-defined regularized minimization problem with nuclear norm penalty:

\[}_{}=*{arg\,min}_{^{d_{1} d_{2}}}L_{}()+ _{}\|\|_{}, L_{}()=,-^{E}}_{s=1}^{_{}^{E}}_{}(r_{s}  Q(_{s}_{s}^{})),\] (2)

where \(Q()\), \(_{}()\), are appropriate functions stated in Definition 1, 3 respectively in Appendix A.3. The \(Q()\) function takes as input the rank-one matrix \(_{s}_{s}^{}\) which is obtained after reshaping \(}_{s}\). Note that \(_{s}\), and \(_{s}\) are the observed vectors in \(d_{1}\) and \(d_{2}\) dimension and \(}_{}^{d_{1} d_{2}}\) Finally, set the regularization parameter \(_{} 4^{2})Cd_{4}d_{2}(2(d_{1}+d_{2})/ )}{_{}^{E}}}\). This is in step \(8\) of Algorithm 1.

### Optimal Design for Rotated Arms (Stage 2 of \(\)-th phase)

In stage \(2\) of phase \(\), GOBLIN leverages the information about the learned sub-space of \(_{*}\) to rotate the arm set and then run the optimal design on the rotated arm set. Once we recover \(}_{}\), one might be tempted to run a pure exploration algorithm (Soare et al., 2014; Fiez et al., 2019; Katz-Samuels et al., 2020; Zhu et al., 2021) to identify \(_{*}\) and \(_{*}\). However, then the sample complexity will scale with \(d_{1}d_{2}\). In contrast GOBLIN uses the information about the learned sub-space of \(_{*}\) to reduce the problem from ambient dimension \(d_{1}d_{2}\) to effective dimension \((d_{1}+d_{2})r\). This reduction is done as follows: Let \(}_{}=}_{}}_{}}_{}^{}\) be the SVD of \(}_{}\) in the \(\)-th phase. Let \(}_{}^{}\) and \(}_{}^{}\) be orthonormal bases of the complementary subspaces of \(}_{}\) and \(}_{}\) respectively. Let \(_{}\) and \(_{}\) be the active set of arms in the stage \(2\) of phase \(\). Then rotate the arm sets such that new rotated arm sets are as follows:

\[_{}=\{=[}_{}}_{}^{}]^{}_{ }\},}_{}=\{=[}_{ }}_{}^{}]^{} _{}\}.\] (3)

Let \(}_{}=[}_{}}_ {}^{}]^{}}_{}[}_{}}_{}^{}]\). Then define vectorized arm set so that the last \((d_{1}-r)(d_{2}-r)\) components are from the complementary subspaces as follows:

\[}_{} =\{[(_{1:r}_{1:r }^{});(_{r+1:d_{2}}_{1:r}^{ });(_{1:r}_{r+1:d_{2}}^{} );..\] \[...(_{r+1:d_{1}}_{r+1:d_{2}}^{})]^{d_{1}d_{2}}: _{},}_{}\}\] \[}_{,1:k} =[(}_{,1:r,1:r}); (}_{,r+1:d_{1},1:r});(}_{ ,1:r,r+1:d_{2}})],\] \[}_{,k+1:p} =(}_{,r+1:d_{1},r+1:d_{2}}).\] (4)

which implies \(\|}_{k+1:p}\|_{2}=O(d_{1}d_{2}r/_{}^{E})\) by Lemma 3 in Appendix A.1. So the last \(p-k\) components of \(}_{}\) are very small compared to the first \(k\) components. Hence, GOBLIN has now reduced the \(d_{1}d_{2}\) dimensional linear bandit to \((d_{1}+d_{2})r\) dimensional linear bandit using (3), (4). This is shown in step \(10\) of Algorithm 1.

Now in stage \(2\) of phase \(\), GOBLIN implements \(G\)-optimal design (Pukelsheim, 2006; Fiez et al., 2019) in the rotated arm set \(_{}\), \(}_{}\) defined in (3). To do this, first GOBLIN defines the rotated vector \(}=[_{1:d_{1}};_{1:d_{2}}]^ {p}\) that belong to the set \(}_{}\). Then GOBLIN solves the \(G\)-optimal design (Pukelsheim, 2006) as follows:

\[}_{}^{G}=*{arg\,min}_{_{ {}},}^{}_{}}\| }-^{}\|_{(_{} _{}}_{}},}^{}+_{}/n)^{-1}}^{2}.\] (5)

This is shown in step \(11\) of Algorithm 1 and \(_{}\) is defined in (6). It can be shown that sampling according to \(}_{}^{G}\) leads to the optimal sample complexity. This is discussed in Remark 1 in Appendix A.2. The key point to note from (5) is that due to the estimation in the rotated arm space \(}_{}\) we are guaranteed that the support of \((}_{}^{G})(k(k+1)/2)\)(Pukelsheim, 2006). On the other hand,if the G-optimal design of Fiez et al. (2019); Katz-Samuels et al. (2020) are run in \(d_{1}d_{2}\) dimension then the support of \(}_{}^{G}\) will scale with \(d_{1}d_{2}\) which will lead to higher sample complexity. Then \(\) samples each \(}}_{}\) for \(_{}^{G}_{,}}^{G}\) times, where \(_{}^{G}:=^{}^{G}(( _{}))(4^{2}||/)}{_{}^{2}}\). Note that the total length of phase \(\), combining stages \(1\) and \(2\) is \((_{}^{E}+_{}^{G})\) rounds. Observe that the stage \(1\) design is on the whole arm set \(}\) whereas stage \(2\) design is on the refined active set \(}_{}\).

Let the observed features in stage \(2\) of phase \(\) be denoted by \(}_{}^{_{}^{G} p}\), and \(_{}^{_{}^{G}}\) be the observed rewards. Define the diagonal matrix \(_{}\) as

\[_{}=[_{k},^{},,_{}^{} }_{p-k}]\] (6)

where, \(_{}^{}_{-1}^{G}/8k(1+_{-1}^{G}/ )\). Deviating from Soare et al. (2014); Fiez et al. (2019)\(\) constructs a regularized least square estimator at phase \(\) as follows

\[}_{}=*{arg\,min}_{^{p}}\|}_{} {}-_{}\|_{2}^{2}+\|\|_{ _{}}^{2}.\] (7)

This regularized least square estimator in (7) forces the last \(p-k\) components of \(}_{}\) to be very small compared to the first \(k\) components. Then \(\) builds the estimate \(}_{}\) from (7) only from the observations from this phase (step \(13\) in Algorithm 1) and eliminates sub-optimal actions in step \(14\) in Algorithm 1 using the estimator \(}_{}\). Finally \(\) eliminates sub-optimal arms to build the next phase active set \(}_{}\) and stops when \(|}_{}|=1\). \(\) outputs the arm in \(}_{}\) and reshapes it to get the \(}_{*}\) and \(}_{*}\). The full pseudocode is presented in Algorithm 1.

```
1:Input: arm set \(,\), confidence \(\), rank \(r\) of \(_{*}\), spectral bound \(S_{r}\) of \(_{*}\), \(S,S_{}^{}d_{2}r}{_{}^{E}S_{}^{2}} (+d_{2}}{_{}}),,_{}^{ }_{-1}^{G}/8(d_{1}+d_{2})r(1+^{G}} {})\). Let \(p d_{1}d_{2}\), \(k(d_{1}+d_{2})r\).
2:Let \(}_{1}},\! \!1\), \(_{0}^{G}(4^{2}||/)\). Define \(_{}\) as in (6), \(B_{}^{*}(8S+^{}S_{}^{ }})\).
3:Define a vectorized arm \(}[_{1:d_{1}},_{1:d_{2}}]\) and \(}}\). Let \(_{}^{E}d_{2}r(4^{2}||/ _{})}}{S_{r}}\). Let the \(E\)-optimal design be \(_{}^{E}*{arg\,min}_{ _{}}}\|_{} }}_{}}} }^{}^{-1}\|\).
4:while\(|}_{}|>1\)do
5:\(_{}=2^{-}\), \(_{}=/^{2}\).
6: (Stage 1:) Explore the Low-Rank Subspace
7: Pull arm \(}}\) exactly \(}_{,}}^{E}_{}^{E}\) times and observe rewards \(r_{t}\), for \(t=1,,_{}^{E}\).
8: Compute \(}_{}\) using (2).
9: (Stage 2:) Reduction to low dimensional linear bandits
10: Let the SVD of \(}_{}\!\!}_{} }_{}}_{}^{}\). Rotate arms in active set \(}_{-1}\) to build \(}_{}\) following (4).
11: Let \(}_{}^{G}*{arg\,min}_{_{ }}}_{},}^{ }}_{}}\|}-}^{}\|_{(_{}_{}}}\ }^{}+_{}/n)^{-1}}^{1}\).
12: Define \(^{G}((}_{}))_{_{ }}}_{},}^{ }}_{}}\|}-}^{}\|_{(_{}_{}}}}^{}+_{}/n)^{-1}}^{1}\).
13: Set \(_{}^{G}\!\!\!^{*}^{G}(( }_{}))(4^{2}||/_{})}{ _{}^{2}}\). Then pull arm \(}}\) exactly \(}_{,}}^{G}_{}^{G}\) times and construct the least squares estimator \(}_{}\) using only the observations of this phase where \(}_{}\) is defined in (7). Note that \(}_{}\) is also rotated following (4).
14: Eliminate arms such that \(}_{+1}}_{} \{}}_{}:_{ }^{}}_{}}}^{}-},}_{} >2_{}\}\)
15:\(+1\)
16:Output the arm in \(}_{}\) and reshape to get the \(}_{*}\) and \(}_{*}\) ```

**Algorithm 1** G-Optimal Design for Bilinear Bandits (GOBLIN) for single-task setting 

**Theorem 1**.: _(informal) With probability at least \(1-\), GOBLIN returns the best arms \(_{*}\), \(_{*}\), and the number of samples used is bounded by \((+d_{2})r}{^{2}}+d_{2}r}}{ S_{r}})\)._

**Discussion 1**.: In Theorem 1 the first quantity is the number of samples needed to identify the best arms \(_{*}\), \(_{*}\) while the second quantity is the number of samples to learn \(_{*}\) (which is required to find the best arms). Note that the magnitude of \(S_{r}\) would be free of \(d_{1},d_{2}\) since \(_{*}\) contains only \(r\) nonzero singular values and \(\|_{*}\| 1\), and hence we assume that \(S_{r}=(1/)\)(Kang et al., 2022). So the sample complexity of single-task GOBLIN scales as \((+d_{2})r}{^{2}})\). However, if one runs RAGE (Fiez et al., 2019) on the arms in \(,\) then the sample complexity will scale as \((d_{2}}{^{2}})\).

**Proof (Overview) of Theorem 1: Step 1 (Subspace estimation in high dimension):** We denote the vectorized arms in high dimension as \(}}\). We run the \(E\)-optimal design to sample the arms in \(}\). Note that this \(E\)-optimal design satisfies the distribution assumption of Kang et al. (2022) which enables us to apply the Lemma 3 in Appendix A.1. This leads to \(\|}_{}-_{*}\|_{F}^{2}d_{1}d_{2}r(2(d_{1}+d_{2})/)}{_{}^{E}}\) for some \(C_{1}>0\). Also, note that in the first stage of the \(\)-th phase by setting \(_{}^{E}=d_{2}r(4^{2}\|\|/_ {})}}{S_{r}}\) and sampling each arm \(}}\) exactly \(}_{,}}^{E}_{}^{E}\) times we are guaranteed that \(\|_{k+1:p}^{}\|_{2}=O(d_{1}d_{2}r/_{}^{E})\). Summing up over \(=1\) to \(_{2}(4^{-1})\) we get that the total sample complexity of the first stage is bounded by \((d_{2}r}/S_{r})\).

**Step 2 (Effective dimension for rotated arms):** We rotate the arms \(}}\) in high dimension to get the rotated arms \(}}_{}\) in step \(10\) of Algorithm 1. Then we show that the effective dimension of \(}\) scales \(8k(1+_{-1}^{G}/)\) when \(_{}^{}=^{G}}{8k(1+_{-1}^{ G}/)}\) in Lemma 7 of Appendix A.4. Note that this requires a different proof technique than Valko et al. (2014) where the budget \(n\) is given apriori and effective dimension scales with \((n)\). This step also diverges from the pure exploration proof technique of Fiez et al. (2019); Katz-Samuels et al. (2020) as there is no parameter \(_{}^{}\) to control during phase \(\), and the effective dimensions in those papers do not depend on phase length.

**Step 3 (Bounded Support):** For any phase \(\), we can show that \(1 p^{G}((}_{})) p/_{ }^{2}\) where, \(_{}=\{c>0:c(}-})\}\) is the gauge norm of \(\)(Rockafellar, 2015). Note that this is a worst-case dependence when \(^{G}((}_{}))\) scales with \(p\). Substituting this value of \(^{G}((}_{}))\) in the definition of \(_{}^{}\) we can show that \(_{}\) does not depend on \(}\) or \(=}-}^{}\). Then following Theorem 21.1 in Lattimore and Szepesvari (2020) we can show that the \(G\)-optimal design \(}_{}^{G}\) is equivalent to \(D\)-optimal design \(}_{}^{D}=_{}}_{}}\ }\ }^{ +}_{}}{|_{}|}\). Then using Frank-Wolfe algorithm (Jamieson and Jain, 2022) we can show the support \(}_{}^{G}\) or equivalently \(}}_{}^{D}\) is bounded by at most \(^{G}/)(8k(1+_{-1}^{G}/ )+1)}{2}\). This is shown in Lemma 9 (Appendix A.4).

**Step 4 (Phase length and Elimination):** Using the Lemma 9, concentration Lemma 5, and using the log determinant inequality in Lemma 7 and Proposition 1 (Appendix A.4) we show that the phase length in the second stage is given by \(_{}^{G}=^{B}((}_{}))(2||/)}{(^{}(_{ }-^{*}))^{2}}\). This is discussed in Discussion 3 (Appendix A.4). We show in Lemma 10 (Appendix A.4) that setting this phase length and sampling each active arm in \(}_{}\) exactly \(}_{,}}_{}^{G}\) times results in the elimination of sub-optimal actions with high probability.

**Step 5 (Total Samples):** We first show that the total samples in the second phase are bounded by \(O(}^{2}}((^{-1})| }|}{})_{2}(^{-1}))\) where the effective dimension \(k=(d_{1}+d_{2})r\). Finally, we combine the total samples of phase \(\) as \((_{}^{E}+_{}^{G})\). The final sample complexity is given by summing over all phases from \(=1\) to \(_{2}(4^{-1})\). The claim of the theorem follows by noting \((k/_{}^{2})(k/^{2})\).

## 3 Multi-task Representation Learning

In this section, we extend GOBLIN to multi-task representation learning for the bilinear bandit setting. In the multi-task setting, we now have \(M\) tasks, where each task \(m[M]\) has a reward model statedin (1). The learning proceeds as follows: At each round \(t=1,2,\), for each task \(m[M]\), the learner selects a left and right action \(_{m,t}\) and \(_{m,t}\). After the player commits the batch of actions for each task \(\{_{m,t},_{m,t}:m[M]\}\), it receives the batch of rewards \(\{r_{m,t}:m[M]\}\). Finally recall that the goal is to identify the optimal left and right arms \(_{m,*},_{m,*}\) for each task \(m\) with a minimum number of samples. We now state the following assumptions to enable representation learning across tasks.

**Assumption 1**.: _(Low-rank Tasks) We assume that the hidden parameter \(_{m,*}\) for all the \(m[M]\) have a decomposition \(_{m,*}=_{1}_{m,*}_{2}^{}\) and each \(_{m,*}\) has rank \(r\)._

This is similar to the assumptions in Yang et al. (2020, 2022); Du et al. (2023) ensuring the feature extractors are shared across tasks in the bilinear bandit setting.

**Assumption 2**.: _(Diverse Tasks) We assume that \(_{}(_{m=1}^{M}_{m,*})} {S_{r}}\), for some \(c_{0}>0\), \(S_{r}\) is the \(r\)-th largest singular value of \(_{m,*}\) and \(_{}()\) denotes the minimum eigenvalue of matrix \(\)._

This assumption is similar to the diverse tasks assumption of Yang et al. (2020, 2022); Tripuraneni et al. (2021); Du et al. (2023) and ensures the possibility of recovering the feature extractors \(_{1}\) and \(_{2}\) shared across tasks.

Our extension of GOBLIN to the multi-task setting is now a phase-based, _three-stage_ arm elimination algorithm. In GOBLIN each phase \(=1,2,\) consists of three stages; the stage for estimation of feature extractors \(_{1},_{2}\), which runs for \(^{E}_{}\) rounds, the stage for estimation of \(_{m,*}\) which runs for \(_{m}^{E}_{m,}\) rounds, and a stage of pure exploration with rotated arms that runs for \(_{m}^{G}_{m,}\) rounds. We will define \(^{E}_{m,}\) in Section 3.1, \(^{E}_{m,}\) in Section 3.2, while the rotated arms and \(^{G}_{m,}\) are defined in Section 3.3. At the end of every phase, GOBLIN eliminates sub-optimal arms to build the active set for the next phase and stops when only the optimal left and right arms are remaining. Now we discuss the individual stages that occur at every phase \(=1,2,\) for multi-task GOBLIN.

### Estimating Feature Extractors \(_{1}\) and \(_{2}\) (Stage 1 of Phase \(\))

In the first stage of phase \(\), GOBLIN leverages the batch of rewards \(\{r_{m,t}:m[M]\}\) at every round \(t\) from \(M\) tasks to learn the feature extractors \(_{1}\) and \(_{2}\). To do this, GOBLIN first vectorizes the \(,\) into a new vector \(}=[_{1:d_{1}};_{1:d_{2}}] {}_{m}\) and then solves the \(E\)-optimal design in step \(3\) of Algorithm 2. Similar to the single-task setting (Section 2) GOBLIN samples each \(}}_{m}\) for \(^{E}_{}^{E}_{,}}\) times for each task \(m\), where \(^{E}_{}=(d_{2}r}/S_{r})\) and \(^{E}_{,}}\) is the solution to \(E\)-optimal design on \(}\). Let the sampled arms for each task \(m\) at round \(s\) be denoted by \(_{m,s},_{m,s}\) which is obtained after reshaping \(}_{s}\). Then it builds the estimator \(}_{}\) as follows:

\[}_{} =*{arg\,min}_{^{d_{1} d _{2}}}L_{}()+_{}\|\|_{},\] \[L_{}() =,-_{}}_{m=1}^{M}_{s=1}^{^{E}_{}}_{ }(r_{m,s} Q(_{m,s}_{m,s}^{})),\] (8)

Then it performs SVD decomposition on \(}_{}\), and let \(}_{1}\), \(}_{2}\) be the top-\(k_{1}\) and top-\(k_{2}\) left and right singular vectors of \(}_{}\) respectively. These are the estimation of the feature extractors \(_{1}\) and \(_{2}\).

### Estimating Hidden Parameter \(_{m,*}\) per Task (Stage 2 of phase \(\))

In the second stage of phase \(\), the goal is to recover the hidden parameter \(_{m,*}\) for each task \(m\). GOBLIN proceeds as follows: First, let \(}_{m}=^{}}_{1,}\) and \(}_{m}=^{}}_{2,}\) be the latent left and right arm respectively for each \(m\). Then GOBLIN defines the vector \(}=[}_{m};}_{m}] }_{m}\) and then solves the \(E\)-optimal design in step \(11\) of Algorithm 2. It then samples for each task \(m\), the latent arm \(}}_{m}\) for \(^{E}_{m,}}^{E}_{m,,}}\) times, where \(^{E}_{m,}(k_{2}r}/S_{r})\) and \(}^{E}_{m,,}}\) is the solution to 

[MISSING_PAGE_EMPTY:8]

**Proof (Overview) of Theorem 2: Step 1 (Subspace estimation in high dimension):** The first steps diverge from the proof technique of Theorem 1. We now build the average estimator \(}_{}\) to estimate the quantity \(_{}=_{m=1}^{M}_{,m}\) using (8). This requires us to modify the Lemma 3 in Appendix A.1 and apply Stein's lemma (Lemma 1) to get a bound of \(\|}_{}-_{}\|_{F}^{2}d_{1 }d_{2}r(2(d_{1}+d_{2})/)}{_{}^{E}}\) for some \(C_{1}>0\). This is shown in Lemma 12 in Appendix A.6. Summing up over \(=1\) to \(_{2}(4^{-1})\) we get that the total samples complexity of the first stage is bounded by \((d_{2}r}/S_{r})\).

**Step 2 (Estimation of left and right feature extractors):** Now using the estimator in (8) we get a good estimation of the feature extractors \(_{1}\) and \(_{2}\). Let \(}_{1,}\), \(}_{2,}\) be the top-\(k_{1}\) left and top-\(k_{2}\) right singular vectors of \(}_{}\) respectively. Then using the Davis-Kahan \(\) Theorem (Bhatia, 2013) in Lemma 14, 15 (Appendix A.6) we have \(\|(}_{1,}^{})^{}_{1}\|,\|(}_{2,}^{})^{}_{2}\|(+d_{2})r/M_{}^{E}})\).

**Step 3 (Estimation of \(}_{m,}\) in low dimension):** Now we estimate the quantity \(}_{m,}^{k_{1} k_{2}}\) for each task \(m\). To do this we first build the latent arms \(}_{m}=^{}}_{}\) and \(}_{m}=^{}}_{}\) for all \(\)and \(\) for each \(m\), and sample them following the \(E\)-optimal design in step \(12\) of Algorithm 2. We also show in Lemma 16 (Appendix A.6) that \(_{}(_{}}} _{}}}^{})>0\) which enables us to sample following \(E\)-optimal design. Then use the estimator in (9). Then in Lemma 19 we show that \(\|}_{m,}-^{*}_{m,}\|_{F}^{2} C_{1 }k_{1}k_{2}r+k_{2})}{_{}}/_{m, }^{E}\) holds with probability greater than \((1-)\). Also, note that in the second phase by setting \(_{m,}^{E}=k_{2}r(4^{2}||/ _{})}/S_{r}\) and sampling each arm \(}}\) exactly \(}_{,}}^{E}_{ m,}^{E}\) times we are guaranteed that \(\|_{k+1:p}^{*}\|_{2}=O(k_{1}k_{2}r/_{m,}^{E})\) in the \(\)-th phase. Summing up over \(=1\) to \(_{2}4^{-1}\) across each task \(M\) we get that the total samples complexity of the second stage is bounded by \((Mk_{2}r}/S_{r})\).

**Step 4 (Convert to \(k_{1}k_{2}\) bilinear bandits):** Once GODLIN recovers \(}_{m,_{}^{E}}\) it rotates the arm set following (10) to build \(_{m}\) to get the \(k_{1}k_{2}\) bilinear bandits. The rest of the steps follow the same way as in steps \(2,3\) and \(4\) of proof of Theorem 1.

**Step 5 (Total Samples):** We show the total samples in the third phase are bounded by \(O(}{_{}^{2}}((^{-1})| |}{})_{2}(^{-1}))\) where the effective dimension \(k=(k_{1}+k_{2})r\). The total samples of phase \(\) is given by \(_{}^{E}+_{m}(_{m,}^{E}+_{m,}^{G})\). Finally, we get the total sample complexity by summing over all phases from \(=1\) to \(_{2}4^{-1}\). The claim of the theorem follows by noting \((k/_{}^{2})(k/^{2})\).

## 4 Experiments

In this section, we conduct proof-of-concept experiments on both single and multi-task bilinear bandits. In the single-task experiment, we compare against the state-of-the-art RAGE algorithm (Fiez et al., 2019). We show in Figure 1 (left) that GODLIN requires fewer samples than the RAGE with an increasing number of arms. In the multi-task experiment, we compare against the state-of-the-art DouExpDes algorithm (Du et al., 2023). We show in Figure 1 (right) that GODLIN requires fewer samples than DouExpDes with an increasing number of tasks. As experiments are not a central contribution, we defer a fuller description of the experimental set-up to Appendix A.8.

## 5 Conclusions and Future Directions

In this paper, we formulated the first pure exploration multi-task representation learning problem. We introduce an algorithm, GODLIN  that achieves a sample complexity bound of \(((d_{1}+d_{2})r/^{2})\) which improves upon the \(((d_{1}d_{2})/^{2})\) sample complexity of RAGE (Fiez et al., 2019) in a single-task setting. We then extend GODLIN for multi-task pure exploration bilinear bandit problems by learning latent features which enables sample complexity that scales as \((M(k_{1}+k_{2})r/^{2})\) which improves over the \((M(k_{1}k_{2})/^{2})\) sample complexity of DouExpDes (Du et al., 2023). Our analysis opens an exciting opportunity to analyze representation learning in the kernel and neural bandits (Zhu et al., 2021; Mason et al., 2021). We can leverage the fact that this type of optimal design does not require the arm set to be an ellipsoid (Du et al., 2023) which enables us to extend our analysis to non-linear representations.

Figure 1: (Left) Single-task experiment: results show the number of samples required to identify the optimal action pair for differing numbers of actions. (Right) Multi-task experiment: results show the number of samples required to identify the optimal action pair for varying numbers of tasks.