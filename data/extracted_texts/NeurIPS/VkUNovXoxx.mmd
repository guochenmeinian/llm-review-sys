# Nonparametric Teaching for Multiple Learners

Chen Zhang\({}^{1}\) Xiaofeng Cao\({}^{1}\)\({}^{2}\) Weiyang Liu\({}^{2,3}\) Ivor W. Tsang\({}^{4}\) James T. Kwok\({}^{5}\)

\({}^{1}\)School of Artificial Intelligence, Jilin University, China

\({}^{2}\)Max Planck Institute for Intelligent Systems, Germany, \({}^{3}\)University of Cambridge, UK

\({}^{4}\)CFAR and IHPC, Agency for Science, Technology and Research (A*STAR), Singapore

\({}^{5}\)Hong Kong University of Science and Technology, Hong Kong, China

u3567831@connect.hku.hk,xiaofengcao@jlu.edu.cn,vl396@cam.ac.uk

ivor_tsang@cfar.a-star.edu.sg,jamesk@cse.ust.hk

###### Abstract

We study the problem of teaching multiple learners simultaneously in the nonparametric iterative teaching setting, where the teacher iteratively provides examples to the learner for accelerating the acquisition of a target concept. This problem is motivated by the gap between current single-learner teaching setting and the real-world scenario of human instruction where a teacher typically imparts knowledge to multiple students. Under the new problem formulation, we introduce a novel framework - Multi-learner Nonparametric Teaching (MINT). In MINT, the teacher aims to instruct multiple learners, with each learner focusing on learning a scalar-valued target model. To achieve this, we frame the problem as teaching a vector-valued target model and extend the target model space from a scalar-valued reproducing kernel Hilbert space used in single-learner scenarios to a vector-valued space. Furthermore, we demonstrate that MINT offers significant teaching speed-up over repeated single-learner teaching, particularly when the multiple learners can communicate with each other. Lastly, we conduct extensive experiments to validate the practicality and efficiency of MINT.

## 1 Introduction

Machine teaching  considers the problem of how to design the most effective teaching set, typically with the smallest amount of (teaching) examples possible, to facilitate rapid learning of the target models by learners based on these examples. It can be thought of as an inverse problem of machine learning, in the sense that the student aims to learn a target model on a given dataset, while the teacher constructs such a (minimal) dataset. Machine teaching has many applications in computer vision , crowd sourcing  and cyber security .

Roughly speaking, machine teaching can be carried out in a batch  or iterative  fashion, depending on how teachers and learners interact with each other. Batch teaching focuses on single-round interaction, that is, the most representative and effective teaching dataset are designed to be fed to the learner in one shot. After that, the leaner solely and assiduously learns a target model from this dataset without further interaction. With practical considerations, iterative teaching extends such a single-round mode to a multi-round one. It studies the case where the teacher feeds examples based on learners' status (current learnt models) round by round, such that the learner can converge to a target model within fewer rounds. The minimal count of such rounds (or iterations) is referred to as _iterative teaching dimension_.

Considering that previous works on iterative machine teaching usually limit target models in a parameterized family, that is, assuming the target model can be represented by some parameters, nonparametric iterative machine teaching  extends such single family to a general nonparametric one. This allows multiple possibility of the target model family. Specifically, by formulating nonparametric teaching in a reproducing kernel Hilbert space (RKHS),  introduce various families of target models associated with kernels, _e.g._, Gaussian and Laplacian kernels in RKHS.

However, existing nonparametric teaching only focuses on the single-learner setting (_i.e._, teaching a scalar-valued target model or function to a single learner), and it is computationally inefficient to carry out this same single-learner teaching repeatedly for the multi-learner scenario, where the teacher needs to teach numerous scalar-valued target functions to multiple learners and a single learner can only learn one. For example, when taking a colored picture with three (RGB) channels as a multi-learner target function1 (_e.g._, [62; 20]), the iteration number of repeatedly performing single-learner teaching for each channel is intuitively triple that of carrying out them simultaneously. Another example is the scenario where the single-learner target model having a large input space. Such a model can be divided into multiple smaller ones with an input space of appropriate size, and these smaller models can be formulated together into a multi-learner target model [7; 64; 66; 61; 74]. Concretely, one can divide a single high-revolution picture into multiple sub-regions, and the original single-learner target function will become a multi-learner one. These examples motivate us to study a generalized framework, called multi-learner nonparametric teaching (MINT), where a vector-valued target model (instead of a scalar-valued model) is being taught. A comparison between single-learner teaching and MINT is illustrated in Figure 1.

It is therefore of great significance to generalize the recent single-learner nonparametric teaching  to MINT [11; 14; 77]. MINT is guided by the insight that repeatedly (or sequentially) teaching multiple scalar-valued target functions can be viewed as teaching a vector-valued target function. The theoretical motivation comes from the well-developed results of kernels for vector-valued functions [56; 44; 21], an important approach to deal with multiple data sources. This inspires us to formulate MINT as a teaching problem of a vector-valued target function, where sequentially teaching multiple scalar-valued functions (for single-learner teaching) becomes a special case of teaching a vector-valued function [43; 21; 14]. We emphasize that, compared to the case where a single learner is learning a vector-valued target function, the multi-learner setting offers a general framework that can be generalized to more complicated scenarios, _e.g._, learners operate within different feature spaces, and learners are able to communicate with each other. We summarize our major contributions below:

* By analyzing general vector-valued RKHS [10; 45; 4], we study the multi-learner nonparametric teaching (MINT), where the teacher selects examples based on a vector-valued target function (each component of the vector-valued function is a scalar-valued function for a single learner)2, such that multiple learners can learn their own target models simultaneously. * By enabling the communication among multiple learners, learners can update themselves with a linear combination of current learnt functions of all learners [23; 12].We study a communicated MINT where the teacher not only selects examples but also injects the guidance of communication.
* Under mild assumptions, we characterize the efficiency of our multi-learner generalization of nonparametric teaching. More importantly, we also empirically demonstrate its efficiency.

Figure 1: Comparison between the single-learner teaching and MINT. (a) In order to facilitate single-learner teaching, it is imperative to transform a colored image into a grayscale format. (b1) MINT allows for the simultaneous teaching of three scalar-valued target models, which are three (RGB) channels of a colored image. (b2) Partitioning a single image into multiple pieces and teaching them concurrently is also considered as a form of MINT.

Related Works

**Machine teaching**. Recently, there has been a surge of interest in the field of machine teaching, see  and references therein. Batch machine teaching has examined the behaviors of various types of learners, including linear learners , forgetful learners , version space learners , hypothesis learner  and reinforcement learners . Further, by extending the single-round teaching mode to a multi-round one, iterative teaching has attracted growing attention in recent studies . Specifically,  focuses on label synthesis teaching, while  proposes generative teaching. Additionally,  relaxes the parametric assumption on target models and generalizes the previous iterative teaching to a nonparametric iterative one . In contrast to previous works that mainly concentrate on the single-learner teaching, this work aims to address a more practical task - teaching multi-learner (vector-valued) target models. In this regard, the realistic practical scenario, classroom teaching , is highly relevant, where it examines multiple learners by partitioning them into groups in batch and iterative setting, respectively. However, their works are also limited to the parametric setting, and their methods therefore are not immediately generalizable to nonparametric situations. In contrast, our work investigates multi-learner teaching in the nonparametric setting.

**Multi-task functional optimization**. Functional optimization  is a fundamental and significant task in various fields, such as variational inference , barycenter problem , and Residual Networks . It involves mapping from input to output without having pre-defined parameters, optimized over a more general function space such as the reproducing kernel Hilbert space (RKHS), Sobolev space , and Frechet space . Notably, the functional gradient descent algorithm has been studied extensively for functional optimization in RKHS due to its regular properties . Meanwhile, modeling in RKHS of vector-valued functions  is an important approach to handle multi-task problem. Specifically,  focus on the analysis of the kernel and  study multi-task versions of online mirror descent, which displays the similarity to multi-learner teaching in the sense of simultaneous execution. These theoretical and empirical works motivate us to extend single-learner teaching to a multi-learner one by analyzing functional gradient in vector-valued RKHS.

## 3 Background

**Notation**. Let \(^{n}\) be a \(n\) dimensional input (_i.e._, feature) space and \(\) be a output (_i.e._, label) space. By \(^{d}=_{1}_{d} ^{n d}\) we denote a \(d\)-learner input space, and let \(^{d}=_{1}_{d} ^{d}\) be a \(d\)-learner output space. Let a \(d\)-dimensional column vector with \(a_{i}\), entries indexed by \(i_{d}\) ( \(_{k}\{1,,k\}\)), be \([a_{i}]^{d}=(a_{1},,a_{d})^{T}\) (we may denote it by \(\) for simplicity), and a 1-vector of size \(d\) be \(^{d}=(1,,1)^{T}^{d}\). By \(M_{(i,)}\) we denote \(i\)-th row vector of a matrix \(M\), and let \(M_{(,i)}\) be its \(i\)-th column vector. A \(d\)-learner teaching sequence in size \(d k\) is a collection of examples, notated as \(}=\{(x_{i,j},y_{i,j})\}^{3}\) with the learner index \(i_{d}\) and the example index \(j_{k}\). We notate the collection of such teaching sequence candidates by \(^{d}}\), which is referred to as the knowledge domain of the teacher .

Let \(K(x,x^{}):\) be a scalar-valued positive definite kernel function, which can be equivalently notated by \(K(x,x^{})=K_{x}(x^{})=K_{x^{}}(x)\), and one can abbreviate \(K_{x}()\) by \(K_{x}\). The scalar-valued reproducing kernel Hilbert space (RKHS) \(\) defined by \(K(x,x^{})\) is the closure of linear span \(\{f:f()=_{i=1}^{r}_{i}K(x_{i},),_{i},r ,x_{i}\}\) equipped with inner product \( f,g_{}=_{ij}_{i}_{j}K(x_{i},x_{j})\) when \(g=_{j}_{j}K_{x_{j}}\). We assume that given the scalar-valued target model \(f^{*}\), one can uniquely identify a teaching example by its \(x^{}\) for brevity, \((x^{},y^{})=(x^{},f^{*}(x^{}))\). Let \(^{d}=_{i}_{d}\) be a RKHS of vector-valued functions \(=[f_{i}]^{d}\) with \(f_{i}^{4}\), equipped with inner product \(,_{^{d}}=_{i=1}^{d} f_{i},g_{ i}_{}\). For simplicity, we use the vector-input \(K(,^{})=[K(x_{i},x_{i}^{})]^{d}\) to denote kernels in vector-valued RKHS \(^{d}\). For a functional \(F:^{d}\), its Frechet derivative  is defined as following:

**Definition 1**.: _(Frechet derivative in vector-valued RKHS) For a vector-valued functional \(F:^{d}\), its Frechet derivative \(_{}F[]\) at \(^{d}\) is defined implicitly as \(F[+]=F[]+ _{}F[],_{^ {d}}+(^{2})\) for any \(^{d}\) and \(\), which is a function in \(^{d}\)._

Using the Riesz-Frechet representation theorem , the evaluation functional of vector-valued functions is defined in the following:

**Definition 2**.: _For a vector-valued reproducing kernel Hilbert space \(^{d}\) with a positive definite kernel \(K_{}^{d}\), where \(=[x_{i,j_{i}}]^{d}^{d}\) and the example index \(j_{i}_{k}\), we define evaluation functional \(E_{}[]:^{d}\) as_

\[E_{}[]=,K_{}( )_{^{d}}=_{i=1}^{d} f_{i},K_{x_{i,j_{i}}} _{}=_{i=1}^{d}f_{i}(x_{i,j_{i}}),=(f_{1}, ,f_{d})^{T}^{d}.\] (1)

**Single-learner nonparametric teaching**.  formulates the single-learner nonparametric teaching as a functional minimization over single-learner \(\) in scalar-valued RKHS:

\[^{*}=*{arg\,min}_{} (^{*},f^{*})+() {s.t.}^{*}=(),\] (2)

where \(\) is a disagreement between \(^{*}\) and \(f^{*}\) (_e.g._, \(L_{2}\) distance defined in RKHS \((^{*},f^{*})=\|^{*}-f^{*}\|_{}\)), \(()\) is the length of the teaching sequence \(\) (_i.e._, the ITD defined in ) controlled by a regularized constant \(\), and \(\) denotes the learning algorithm of learners. Usually, \(()\) is taken as empirical risk minimization:

\[^{*}=*{arg\,min}_{f}_{(x,y) (x,y)}[(f(x),y)],\] (3)

with single-learner convex loss function \(\). As introduced in Section 1, iterative teaching  focuses on some specific optimization algorithm that the learner adopts . In the nonparametric setting, we consider the functional gradient descent:

\[f^{t+1} f^{t}-^{t}(;f^{t};^{t}),\] (4)

where \(t=0,1,,T\) serves as an iteration index, \(^{t}>0\) (_i.e._, a small constant) denotes the learning rate for the \(t\)-th iteration, and \(\) represents the gradient functional evaluated at \(^{t}\).

Specifically,  investigates the teaching algorithms within a practical teaching protocol and gray-box setting. This involves a teacher that has no knowledge about the learner, including the learning rate and specific loss function, but still is able to keep track of the learnt model during each iteration. Two functional teaching algorithms are proposed: Random Functional Teaching (RFT) and Greedy FT (GFT). The former essentially adopts random sampling, and it serves as a simple baseline, which can also be viewed as a functional analogue of stochastic gradient descent . In contrast, GFT picks examples by maximizing the corresponding disagreement between the target and current models , and has been shown to be more effective than RFT both theoretically and experimentally.

## 4 MINT: Multi-learner nonparametric teaching

In this section, we begin by defining multi-learner nonparametric teaching as a functional minimization in a vector-valued RKHS. Next, we analyze a vanilla MINT where multiple learners independently and simultaneously learns corresponding components of a vector-valued target function. Lastly, we investigate a communicated MINT where the teacher does not only provide examples but also guide multiple learners in the process of linearly combining present learnt functions.

### Teaching settings

To define MINT, we expand scalar-valued target models in single-learner teaching to vector-valued ones and modify other notations to suit the multi-learner setting. More specifically, we redefine functional minimization of Eq. 2 as follows:

\[}^{*}=*{arg\,min}_{ }^{d}}(}^{*},^ {*})+(}) }^{*}=(}),\] (5)where \(^{*}^{d}\) refers to a vector-valued target model, and other notations bear the same meaning as in Eq. 2. The learning algorithm \(\) arrives at the following solution:

\[}^{*}=*{arg\,min}_{^{d}} _{(,)}[((),)],\] (6)

where \((,)^{d}^{d}\) and \((,)[_{i}(x_{i},y_{i})]^{d}\). Evaluated at an example vector \((,)=[(x_{i,j_{i}},y_{i,j_{i}})]^{d}\) with the example index \(j_{i}_{k}\), the multi-learner convex loss \(\) therein is

\[((),)=_{i=1}^{d}_{i}(f_{i}(x_{i,j _{i}}),y_{i,j_{i}})=E_{}[[_{i}(f_{i},y_{i,j_{i}})]^{d} ],\] (7)

where \(_{i}\) is the convex loss for \(i\)-th learner. We can also express it as \(^{d},[_{i}(f_{i}(x_{i,j_{i}}),y_{i,j_{i}})]^{d}_ {^{d}}\), where the vector \(^{d}\) can be replaced by a weight vector \([w_{i}]^{d}^{d}\) to adjust the significance of each learner relative to others. Under iterative setting [36; 37] which explores teaching algorithms from the viewpoint of optimization and approximation, we present vector-valued functional gradient descent:

\[^{t+1}_{A^{t}} A^{t}^{t}-^{t}}(};A^{t}^{t};}^{t}),\] (8)

where \(\) denotes the element-wise multiplication, \(^{t}=[_{i}]^{d}=(_{1}^{t},,_{d}^{t})^{T}\) is a vector of learning rates that corresponds to \(d\) learners and the communication matrix \(A^{t}=*{arg\,min}_{A^{d d}}\|A^{t}- ^{*}\|_{^{d}}\) signifies a matrix with row sums that are equal to one in order to maintain the output's scale. Equivalently, by denoting that \(A^{t}_{(i,)}=*{arg\,min}_{M^{t}_{(i,)}^{1  d}}\|M^{t}_{(i,)}^{t}-f^{*}_{i}\|_{}\), it can also be expressed in a learner-specific (_i.e._, component-wise) fashion as \(f^{t+1}_{i} A^{t}_{(i,)} f^{t}_{i}-^{t}_{i}_{i}(;A^{t}_{(i,)}^{t};}^{t})\), where \(i_{d}\) is the learner index.

We investigate MINT in the gray-box setting, which is equivalent to the one considered in . To facilitate the theoretical analysis, we adopt some moderate assumptions regarding \(_{i}\) and kernels, which align with those made in .

**Assumption 3**.: _Each loss \(_{i}(f_{i}),i_{d}\) is \(L_{_{i}}\)-Lipschitz smooth, i.e., \( f_{i},f^{}_{i}\), \(x_{i}\) and \(i_{d}\)_

\[|E_{x_{i}}[_{f}_{i}(f_{i})]-E_{x_{i}}[ _{f}_{i}(f^{}_{i})]| L_{_{i}} |E_{x_{i}}[f_{i}]-E_{x_{i}}[f^{}_{i}]|,\]

_where \(L_{_{i}} 0\) is a constant. To simplify the notation, we assume that \(L_{_{i}}=L_{}\) for all \(i_{d}\)._

**Assumption 4**.: _Each kernel \(K(x,x^{})\) is bounded, i.e., \( x,x^{}\), \(K(x,x^{}) M_{K}\), where \(M_{K} 0\) is a constant._

With regards to diverse knowledge domains, we narrow the scope of investigation in this study to the synthesis-based teacher setting . Furthermore, it's worth noting that by limiting the knowledge domain to a specific pool, it can result in multiple learners converging to a suboptimal \(^{*}\), such findings of pool-based teachers are comparable and can be deduced accordingly, as discussed in Remark 7 of .

### Vanilla multi-learner teaching

In tackling MINT, we begin by examining a basic scenario in which multiple learners concurrently learns corresponding components of a vector-valued target function without communication between [28; 12], that is, \(A^{t}\) in Eq. 8 is assigned the identity matrix \(I_{d}\). This simplifies Eq. 8 to

\[^{t+1}^{t}-^{t}}(;^{t};}^{t}).\] (9)

In this vanilla setting, multiple learners do not linearly combine learned functions of all learners; rather, it updates its functions by Eq. 9 alone.

In light of the definition of Frechet derivative in vector-valued RKHS (as presented in Defi.1), we present Chain Rule for vector-valued functional gradients  as a Lemma.

**Lemma 5**.: _(Chain rule for vector-valued functional gradients) For differentiable functions \(G:\) that are functions of functionals \(F\), \(G(F[])\), the expression_

\[_{f}G(F[])=])}{ F[]} _{}F[]\] (10)

_is usually referred to as the chain rule._To obtain the derivative of the evaluation functional , we introduce Lemma 6, with the proof of this lemma deferred to Appendix B.

**Lemma 6**.: _For an evaluation functional in vector-valued RKHS \(E_{}[]=_{i=1}^{d}f_{i}(x_{i,j_{i}}):^{d}\) where \(=[x_{i,j_{i}}]^{d}^{d}\), its gradient is a \(d\)-dimensional vector \(_{}E_{}[]=K_{}=[K_{x_{i,j_{i}}}]^{d}^{d}\)._

Using Lemma 5 and Lemma 6, we offer an expansion viewpoint on the vector-valued functional gradients of \(\): Suppose we have a specific example vector \((,)=[(x_{i,j_{i}},y_{i,j_{i}})]^{d}^{d}^{d}\), the gradient \(}\) of the multi-learner loss function \(\) w.r.t. the vector-valued model \(\) can be expressed by

\[}(;;(,))=[. _{i}/ f_{i}|_{f_{i}(x_{i,j_{i}}),y_{i,j_{i}}}K_{x_{i,j_{i}}}]^{d}.\] (11)

We also broaden the applicability of RFT and GFT from their single-learner versions  to a multi-learner one. Under this context, RFT involves randomly picking examples for each learner, while GFT selects examples that satisfy

\[(^{t*}=*{arg\,max}_{[x_{i}]^{d}^{d}} \|[_{i}/ f_{i}|_{f^{t}_{i}(x_{i})} ]^{d}\|_{^{d}},\ \ ^{t*}=[{y^{t}}^{*}]^{d}=[f^{*}_{i}({x^{t*}_{i}} )]^{d})\] (12)

To avoid clutter in the notation, our examination is restricted to the selection of a single example for each learner during every iteration, and we provide the pseudo code in Appendix A.

In the upcoming discussion, we shall present our theoretical examination of the convergence performance of multi-learner RFT and GFT. Our approach differs from  as we focus on RFT's average performance by introducing the expectation operation over random sampling. This helps us gain valuable insights by averaging out the impact of randomness. Recall the teaching settings (Eq. 6, 9), we then proceed with our analysis of RFT's per-iteration reduction concerning \(\).

**Lemma 7**.: _(Sufficient Descent for multi-learner RFT) Suppose there are \(d\) learners, and the example mean for each learner is \(_{i}=_{x_{i}_{i}(x_{i})}(x_{i})<\), and the variance \(_{i}^{2}=_{x_{i}_{i}(x_{i})}(x_{i}-_{i})^{2} <,i_{d}\). Under the assumptions outlined in both 3 and 4, if \(_{i}^{t}} M_{K}}\) for all \(i_{d}\), then RFT teachers can, on average, reduce the multi-learner loss \(()\) by:_

\[_{[_{i}(x_{i})]^{d}}[(^{t+ 1})-(^{t})]-^{t}}{2}_{i=1}^{ d}(m_{i,t}(_{i})+^{}(_{i})}{2}_{i}^{2}),\] (13)

_where \(^{t}=_{i_{d}}_{i}^{t}\) and \(m_{i,t}() E_{}[(._{f}_{i}(f) |_{f=f^{t}_{i}})^{2}]\)._

Intuitively, \(m_{i,t}(_{i})\) serves as a measure of the gradient's magnitude (loss \(_{i}\) w.r.t. \(f^{t}_{i}\)) for the \(i\)-th learner at the example mean \(_{i}\) in the \(t\)-th iteration, and as \(f^{t}_{i}\) approaches \(f^{*}_{i}\), \(m_{i,t}(_{i})\) becomes increasingly small. According to Lemma 7, as proven in Appendix B, the average reduction of multi-learner loss \(\) per iteration is constrained by a negative upper bound. To be more precise, this upper bound is determined by a range of elements, such as the learning rate, the count of learners, the example variance \(_{i}^{2}\) and the gradient of \(_{i}\) at the example mean \(_{i}\) (\(i_{d}\)), and these elements are independent of each another. When the gradient at each \(_{i}\) is large, RFT on average can reduce \(\) by a significant amount. Meanwhile, the variance also has an impact on the reduction and this is governed by a diminutive constant of \(m_{i,t}^{}(_{i})/2\).

The least helpful teaching scenario in the \(t\)-th iteration across the \(d\) learners is represented by \(_{i_{d}}(m_{i,t}(_{i})+m_{i,t}^{}(_{i}) _{i}^{2}/2)\), which has the smallest gradient at the example mean. It is observed from Lemma 7 that the reduction of \(\) per iteration is, at a minimum, \(d\) times greater than that of the worst-case scenario \(-^{t}/2_{i_{d}}(m_{i,t}(_{i})+m_{i,t} ^{}(_{i})_{i}^{2}/2)\). In other words, when multi-learner RFT achieves a stationary point in the worst-case scenario, the multi-learner loss \(\) reaches convergence as well. This indicates that the convergence rate of multi-learner RFT is at least as fast as in the single-learner worst-case scenario (also faster than repeatedly teaching).

Introducing the expectation operation enables us to eliminate the randomness that arises from random sampling. In contrast to , which determines the decrease based on the discrepancy at specific but randomly chosen example \(x^{t}\) (involving randomness), we establish that the decrease on average is determined by the mean and variance. This valuable insight is important to understand the fundamentals of RFT, which is not considered in .

**Theorem 8**.: _(Convergence for multi-learner RFT) Suppose the vector-valued model for multiple learners is initialized with \(^{0}^{d}\) and returns \(^{t}^{d}\) after \(t\) iterations, we have the upper bound of \(_{i_{d}}m_{i,t}(_{i})+m_{i,t}^{}(_{i}) _{i}^{2}/2\) w.r.t. \(t\):_

\[_{i_{d}}(m_{i,t-1}(_{i})+m_{i,t-1}^{}( _{i})_{i}^{2}/2) 2_{[_{i}(x_{ i})]^{d}}[(^{0})]/(dt),\] (14)

_where \(0<=_{l\{0\}_{l-1}}^{l} 1 /(2L_{} M_{K})\), and given a small constant \(>0\) it would take approximately \((2_{[_{i}(x_{i})]^{d}} [(^{0})]/(d))\) iterations to reach a stationary point._

The proof for Theorem 8 can be found in Appendix B. Theorem 8 tells that the minimum of the non-negative term within the upper bound in Theorem 7, which is \(_{i_{d}}(m_{i,t}(_{i})+m_{i,t}^{}(_{i })_{i}^{2}/2)\), is also upper bounded, and the iterative teaching dimension is \(2_{[_{i}(x_{i})]^{d}}[(^{0})]/(d)\).

In comparison to RFT, GFT achieves a larger reduction in multi-learner loss \(\) per iteration, suggesting a faster convergence rate and a lesser number of iterations required to achieve convergence.

**Lemma 9**.: _(Sufficient Descent for multi-learner GFT) Under Assumption 3 and 4, if \(_{i}^{t}} M_{K}}\) for all \(i_{d}\), the GFT teachers can achieve a greater reduction in the multi-learner loss \(\):_

\[_{[_{i}(x_{i})]^{d}}[(^{t+1})-(^{t})]-^{t}}{2} _{i=1}^{d}m_{i,t}(x_{i}^{t*}),\] (15)

_where \(^{t}\) and \(m_{i,t}()\) retain their previous meaning._

The proof of the Lemma 9 is presented in Appendix B. GFT selects examples with the steepest gradient, which leads to \(m_{i,t}({x_{i}^{t}}^{*})(m_{i,t}(_{i})+m_{i,t}^{}( _{i})_{i}^{2}/2)\) for each learner. Consequently, it can be observed that per-iteration \(\) reduction of GFT has a tighter bound compared to RFT. This is due to the fact that GFT uses a greedy approach to select examples that maximizes the norm of difference between the current and target models. This allows the learners to take a larger step forward \(^{*}\) in per iteration. The tighter bound provides theoretical evidence supporting the effectiveness of GFT, which is consistent with the findings in the single-learner teaching .

**Theorem 10**.: _(Convergence for multi-learner GFT) Suppose the vector-valued model for multiple learners is initialized with \(^{0}^{d}\) and returns \(^{t}^{d}\) after \(t\) iterations, we have the upper bound of \(_{i_{d}}m_{i,t}({x_{i}^{t}}^{*})\) w.r.t. \(t\):_

\[_{i_{d}}m_{i,t-1}({x_{i}^{t-1}}^{*})t }_{[_{i}(x_{i})]^{d}}[(^{0})]+_{l=0}^{t-1}_{i=1}^{d}(\|{x_{i}^{l }}^{*}-_{i}\|_{2}),\] (16)

_where \(\) has the same definition as before._

It follows from Lemma 7 and 9 that when \({x_{i}^{t}}^{*}\) is close to \(_{i}\) for \(i_{d}\), then GFT and RFT perform similarly. In Theorem 10 (The proof is given in Appendix B), we theoretically show this relation by introducing the distance between \({x_{i}^{t}}^{*}\) and \(_{i}\), which provides a deep insight of the difference between RFT and GFT that is not considered in . Specifically, the per-iteration loss reduction under both RFT and GFT has negative upper bounds, and the difference between these two upper bounds can be seen by comparing Theorem 8 and Theorem 10. From a qualitative perspective, GFT can achieve better convergence speed-up because its negative upper bound can take smaller values than that of RFT. This gap is characterized by \(_{l=0}^{t-1}_{i=1}^{d}(\|{x_{i}^{t}}^{*}-_{i}\|_{2})\) which is the cumulative distance between select \({x_{i}^{t}}^{*}\) and mean \(_{i}\) for all learners and averaged over iterations. We emphasize that the purpose of our results is to show the difference between RFT and GFT, rather than proving that GFT always achieves better convergence than RFT (which is not always true). By comparing Theorem 8 and Theorem 10, we can learn that it is possible for GFT to have larger per-iteration loss reduction than RFT. However, we also recognize the intrinsic difficulty to show the exact conditions such that GFT can always be better than RFT. In contrast to our results, the parametric case (_e.g._, ) also has not obtained the necessary and sufficient conditions for greedy teaching to be better than random teaching. More generally,  also considers some alternative teaching strategies other than the greedy teaching, such as the parameterized teaching with a multi-iteration reward function. Despite not being able to fully characterize the difference of convergence rate between GFT and RFT, our existing theoretical analysis still poses an important open problem: _when and how can GFT provably achieve faster convergence than RFT?_

### Communicated multi-learner teaching

An infant would often compose previously learnt knowledge in order to grasp a new target concept, such as understanding what a zebra is by combining the learnt ideas of horses and black-and-white stripes. Such an efficient learning motivates us to explore the idea of communicated MINT, which enables the communication between learners. In other words, multiple learners can execute linear combination on the currently learnt functions of all learners , that is, \(A^{t}\) is no longer constrained to be an identity matrix.

In practice, to direct this communication, the teacher can utilize a two-layer perceptron (MLP) to derive the matrix \(A^{t}\) in Eq. 8 by searching a matrix \(A\) that minimizes \(\|A^{t}-^{*}\|_{^{d}}\) as much as possible, which is an addition step beyond example selection in each iteration.

**Proposition 11**.: _If the proximity between \(^{t}\) and \(^{*}\) is sufficiently close, meaning that \(\|^{t}-^{*}\|_{^{d}}\) where \(\) is a tiny positive constant, then \(A^{t}\) equals the identity matrix \(I_{d}\)._

The proof of Prop.11 is given in Appendix B. This suggests that there is no need for MLP to be used in solving matrix \(A^{t}\) in every iteration, but only at the beginning, because as the iterations progress, \(^{t}\) will approach near to \(^{*}\).

**Lemma 12**.: _Under Assumption 3, the communication across learners will result in a reduction of the multi-learner convex loss \(\) by \(0(^{t})-(A^{t}^{t}) 2L_{} \|^{t}-^{*}\|_{^{d}}\)._

Proof of Lemma 12 is given in Appendix B. The difference in \(\) between the case where the communication exists and that where it doesn't is lower bounded by zero and upper bounded by the distance between \(^{t}\) and \(^{*}\). This suggests that if \(^{t}\) is far from \(^{*}\), then matrix \(A^{t}\) can potentially decrease \(\) significantly at the best case while not causing any increase at the worst case.

**Theorem 13**.: _Suppose the communication in the \(t\)-th iteration of multiple learners is denoted by the matrix \(A^{t}\) and returns \(^{t+1}_{A^{t}}^{d}\), for both RFT and GFT we have:_

\[_{[_{i}(x_{i})]^{d}}[(^{t +1}_{A^{t}})-(^{t})]_{[ _{i}(x_{i})]^{d}}[(^{t+1}_{A^{t}})-(A^{t}^{t})] 0.\] (17)

Figure 3: Comparison of convergence performance between single-learner teaching and MINT. (a) is corresponding to (a)-(b) in Figure 2. (b) is for (c)-(d) in Figure 2. (c) pertains to teaching of a colored lion.

Figure 2: Comparison between single-learner teaching and MINT. (a) Repeatedly invoking single-learner GFT: teaching a white tiger at first and subsequently teaching a cheetah. (b) Simultaneous teaching of a white tiger and a cheetah by GFT. (c) Single-learner teaching of the lion. (d) Partitioning a single lion image into 16 pieces and teaching them concurrently.

Proof of Theorem 13 is in Appendix B. This shows that the addition of communication has led to an improvement in model updates, which is evident from the larger loss discrepancy between \(_{A^{t}}^{t+1}\) and \(^{t}\) compared to the difference observed between \(_{A^{t}}^{t+1}\) and \(A^{t}^{t}\).

## 5 Experiments and Results

Testing the teaching of a multi-learner (vector-valued) target model, MINT presents more satisfactory performance than repeatedly carrying out the single-learner teaching, which is consistent with our theoretical findings. Detailed configurations and supplementary experiments are given in the Appendix C.

**MINT in gray scale.** A grayscale figure can be viewed as a 3D surface where the \(z\) axis corresponds to the level of gray, while the \(x,y\) axes depict the placement of pixels . We consider two scenarios: one involves the simultaneous teaching of a tiger and a cheetah figure, while the other focuses on the teaching of a lion. After comparing (a) and (b) in Figure 2, we see that when teaching two target functions by GFT simultaneously, the vanilla MINT requires almost half the number of cost iterations compared to single-learner teaching, which is also evident from the loss plot shown in Figure 3 (a). By comparing (c) and (d) in Figure 2, we can observe that dividing a single-learner target figure into smaller pieces and recasting them into MINT can significantly improve the efficiency, which is also demonstrated by the loss plot in Figure 3 (b).

**MINT in three (RGB) channels.** To further demonstrate the benefits of communication, we examine with a lion image with three channels in RGB format. The loss plot in Figure 3 (c) reveals that the most efficient teaching is the communicated MINT for both RFT and GFT. The vanilla MINT and single-learner teaching follow in order of decreasing efficiency. Furthermore, as anticipated, the multi-learner GFT proves to be more efficient compared to RFT. One intriguing observation is that the communicated MINT leads to a significant reduction in multi-learner loss at the outset, which aligns with our theoretical findings in Lemma 12 and confirms the validity of Prop.11 that \(A^{t}\) could eventually become an identity matrix after numerous iterations. Figure 4 compares the specific learnt \(^{t}\) for three versions of GFT during each iteration, wherein we observe that MINT consistently outperforms the single-learner one, and the learnt image under the communicated MINT is more clear compared to that of the vanilla one. To be more persuasive, we also offer detailed and additional

Figure 4: Visualization of \(^{t}\) taught by GFT. Starting from a random initialization, the communicated multi-learner GFT help multiple learners learn a more clear image than the vanilla one followed by single-learner one.

experiments in Appendix, including channel-wise visualization of specific \(^{t}\) (Figure 7), RFT-taught \(^{t}\) (Figure 8-9) and teaching multiple learners with a particular initialization of \(^{0}\) (Figure 11-10), which includes an extreme case that only one-time communication is sufficient to help multiple learners learn \(^{*}\) (Figure 15).

## 6 Concluding Remarks and Future Work

In this paper, we seek to address a practical limitation of current nonparametric iterative machine teaching by enabling the teaching of multi-learner (vector-valued) target models. This generalization of teaching ability involves generalizing the model space from space of scalar-valued functions to that of vector-valued functions. In order to address multi-learner nonparametric teaching, we start by analyzing a vanilla MINT where the teacher picks examples based on a vector-valued target function such that multiple learners can learn its components simultaneously. Additionally, we consider the communicated MINT (_i.e._, multiple learners are allowed to carry out linear combination on current learnt functions) for further exploration. Through both theoretical analysis and empirical evidence, we demonstrate that the communicated MINT is more efficient than the vanilla MINT.

Moving forward, it could be interesting to explore other practical aspects related to nonparametric teaching. This will involve a deeper theoretical understanding and the development of more efficient teaching algorithms. Besides, it would be intriguing to establish connections between MINT and multi-output neural networks, which can further enhance its practical applications such as knowledge distillation. Moreover, generating teaching examples with a surrogate objective that does not need a target model (_e.g._, black-box teaching) is also an important direction (_e.g._, [17; 72]). More generally, (iterative) machine teaching is intrinsically connected to the recent popular data-centric AI. Understanding data-centric learning (_e.g._, text prompting, data augmentation, data distillation) may require a deeper understanding towards (iterative) machine teaching.