# Preference Learning of Latent Decision Utilities with a Human-like Model of Preferential Choice

Sebastiaan De Peuter

Aalto University

sebastiaan.depeuter@aalto.fi

&Shibei Zhu

Aalto University

shibei.zhu@aalto.fi

&Yujia Guo

Aalto University

yujia.guo@aalto.fi

&Andrew Howes

University of Exeter

andrew.howes@exeter.ac.uk

&Samuel Kaski

Aalto University

University of Manchester

samuel.kaski@aalto.fi

###### Abstract

Preference learning methods make use of models of human choice in order to infer the latent utilities that underlie human behavior. However, accurate modeling of human choice behavior is challenging due to a range of context effects that arise from how humans contrast and evaluate options. Cognitive science has proposed several models that capture these intricacies but, due to their intractable nature, work on preference learning has, in practice, had to rely on tractable but simplified variants of the well-known Bradley-Terry model. In this paper, we take one state-of-the-art intractable cognitive model and propose a tractable surrogate that is suitable for deployment in preference learning. We then introduce a mechanism for fitting the surrogate to human data and extend it to account for data that cannot be explained by the original cognitive model. We demonstrate on large-scale human data that this model produces significantly better inferences on static and actively elicited data than existing Bradley-Terry variants. We further show in simulation that when using this model for preference learning, we can significantly improve utility in a range of real-world tasks.

## 1 Introduction

AI systems need exact descriptions of tasks to be performed. However, humans find more complex tasks hard to describe. In response, preference learning has emerged as one way to learn from human feedback. It has been used to teach AI systems a variety of tasks from how to hand objects to humans to how to play Atari games . More recently, human feedback has been used to train large language models to summarize text , answer questions in natural language , and to train deep generative models to generate realistic medical images .

When learning from human feedback, it is generally assumed that some latent utility function \(f\) guides an individual's behavior, but that the individual cannot describe this function to the machine. Thus, _preference queries_ are used to elicit information about \(f\) from the user. A preference query gives a user a set of options \(x_{1},,x_{n}\) and asks the user to select their preferred option, i.e., the one with the highest utility. Given a model of how people make such choices, the machine can then infer the underlying function \(f\) from the user's chosen item \(y\). For example, Stiennon et al.  learned a utility function for text summaries by showing users a text with several summaries and asking them to choose the best summary.

There are several models of choice that have been used for learning preferences from human choices. Some recent work on Reinforcement Learning from Human Feedback (RLHF), for example, has useda simple binary choice model [5; 6]\(p(y=x_{1}|x_{1},x_{2})=(f(x_{1})-f(x_{2})),\) over choices \(x_{1}\) and \(x_{2}\), though generally, most preference learning approaches have used the Bradley-Terry model 

\[p(y=x_{i}|x_{1},,x_{n})=))}{_{j=1}^{n}(  f(x_{j}))}.\]

Although these models have proven to be practical, they are not realistic models of human choice behavior. Specifically, both models make choices between two options without taking into account the rank orderings of option attributes; a widely observed property of human decision-making [9; 10; 11]. As a result, these models fail to predict a number of apparent biases in human choice behavior. These include contextual choice effects [12; 13], which occur in situations where a decision maker's choice between two options is influenced by adding more options to the choice set [14; 12]. Say, for example, we have two options \(A\) and \(B\) and a user exhibits a probability of choosing between these. When a third _decoy_ option \(C\) is introduced which is strictly dominated by \(B\), there is a shift in the probability of choices from \(A\) to \(B\).

Though context effects are not certain to appear in preference queries posed to users, they are known to appear in a wide range of human tasks including risky choice tasks , multi-attribute choice tasks  and perceptual judgement tasks  and in many other species including jays and honeybees . These effects point to a potential gap in the accuracy of the models currently used, during preference learning, to interpret the choices made by users. Moreover, this gap has the potential to lead to incorrect inferences about the latent preference utilities of observed human decision-makers.

The contribution of this paper is threefold. First, we show that we can improve preference learning by leveraging computational rationality theory, a general cognitive-scientific theory which posits that human behavior is rational under cognitive bounds [17; 18]. We learn preferences from human choice behaviors using a state-of-the-art cognitive model that is based on a computational rational analysis of context-dependent choice under uncertainty and is backed by substantial empirical support in the psychology literature . Like all computationally rational models, behavior under this model emerges from the latent utility function and a latent set of cognitive bounds. This provides strong inductive biases when inferring these latent factors from human behavior which - as we will show experimentally - significantly improves learning from preferences. Our second contribution lies in making this cognitive model amenable to preference learning. To this end, we generalize it, and make inference practical by approximating intractable calculations with a surrogate we call the Computationally Rational Choice Surrogate (CRCS) model. Finally, we find experimentally that CRCS can sometimes perform worse than the Linear Context Logit (LCL) model . We hypothesize that human context effects are partially a consequence of cross-feature effects. These are not modeled in CRCS, but can be learnt by LCL. We therefore propose LC-CRCS, which takes advantage of these effects by combining CRCS with LCL.

We report three sets of experiments. In the first, we show that CRCS matches the original model's prediction of human choice behavior. In the second, we compare preference learning with CRCS to preference learning with recently proposed variants of the Bradley-Terry choice model. Using existing human data sets, we show that CRCS outperforms these in choice prediction and utility function inference, but performs worse than LCL on some tasks. We then show that LC-CRCS can additionally outperform LCL in these tasks. In the third set of experiments, we show the applicability of CRCS in three real-world use cases and verify its parameter recovery capability.

## 2 Background

### Learning from Preferences

Preference learning methods aim to infer latent utility functions from human choices. Depending on the type of queries presented, there are two main streams of research: (1) learning from pairwise comparisons or (2) learning from ranking, where humans rank a set of \(n\) options. Popular methods include Gaussian Process regression that captures the preference relationships of pairwise queries [21; 22]. Other work, such as as [23; 24; 25], uses Deep Neural Networks trained on ranked demonstrations to approximate the underlying reward functions. To reduce the computational burden created by the necessity for numerous queries, active learning techniques [26; 27; 28; 29] have been proposed for efficient query proposal with maximum information gain. However, these methods typically require consistent preference order within the ranking and do not consider any contextual effects within the query dataset. Reinforcement Learning approaches include Preference-Based Reinforcement Learning (PBRL) and Reinforcement Learning with Human Feedback (RLHF), where the reward function is inferred from the preference feedback. Work using ranking queries and human feedback can reach or even exceed human-level performance in several RL benchmarks [23; 24].

### Modeling contextual choice

To date, preference learning research has yet to make use of plausible models of human decision-making such as [30; 31]. These models are inspired by extensive studies of human behavior and give rise to _contextual choice effects_. Consider a hypothetical choice between two sightseeing trips, one to Paris and the other to London. Both trips come with free coffee. Let's say that \(70\%\) of people prefer Paris and \(30\%\) London. Now imagine that we add a third option which is identical to the London trip but without free coffee. If, for this three-trip choice problem, we observe that \(40\%\) prefer London with free coffee, then we will have observed a contextual choice effect known as a "preference reversal" . The choice frequency for London with coffee is increased by a context that includes a dominated choice. This effect has been observed both in sample averages and, more interestingly, within individuals. It has been taken as evidence that people are irrational  and have no stable preferences . Needless to say, both instability and irrationality pose severe challenges to the viability of preference learning.

More recent theories, however, demonstrate that contextual choice effects can be consequences of computationally rational processes that assume stable preferences. These theories explain contextual choice effects by modeling the fact that people compare attributes and/or utilities under uncertainty. These include Bayesian theories , rational analyses , and neurobiological relative encoding theories [13; 36]. These theories use comparisons between option attributes to compute expected utilities, such that these expectations are sensitive to the reliability of the comparison as an indicator of expected utility.

Other work has proposed variations on the Bradley-Terry model to include these contextual effects, with the same commitment to stable preferences. Bower and Balzano  posit that context effects are the result of humans comparing options only on the \(k\) most salient features within a context. They propose a Bradley-Terry model where utilities are calculated only on the \(k\) most salient features, where saliency is measured by the sample variance of each feature within the current set of options. Tomlinson and Benson  do not propose a specific theory of context effects, but rather propose to learn them from data. They introduce the Linear Context Logit (LCL) model, a Bradley-Terry model with a linear utility function, in which context effects are modeled as a context-dependent change in the (globally stable) weights of the utility function. This change is modeled as a linear function of the average attribute values of the set of options presented to a user (the context), and is inferred from human choice data. They further introduce the Decomposed LCL model, in which each feature induces its own context effect - whereas in LCL the features jointly induce a single context effect - and where the final context effect results from a mixture of these individual effects.

In the current paper, we commit to distinguishing behavioral choices, which are observable, from latent preferences, which are not. When we refer to "preferences" we are referring to the latent utility function \(f(x)\), and not to the observable choice behavior.

## 3 Modeling computationally rational choice

To learn latent preferences from human choice behavior, we build on a computationally rational model of choice behaviors by Howes et al.  which is sensitive to the aforementioned context effects. This model assumes that humans make utility-maximizing choices, but that the option utilities are estimated from noisy observations of the true utilities and noisy comparisons between the option attributes. Here we will first describe the original model in a general form. We then extend it to a general space of utility functions and introduce our Computationally Rational Choice Surrogate (CRCS) model, a model which replaces intractable computations in the original model with learned surrogates to allow tractable inference of the latent utility function. Finally, we introduce the LC-CRCS model, an extension of the CRCS model which is able to learn additional context effects not captured by the CRCS model.

### A computationally rational model of choice

Let \(x_{1},,x_{n}^{d}\) be a set of \(n\) options, each with \(d\) attributes. Let \(f:^{d}\) be a latent utility function that maps each option to its associated utility. As a shorthand, we will denote the utilities of a collection of options \(= x_{1},,x_{n}\) as \(= u_{1},,u_{n}\) where \(u_{i}=f(x_{i})\).

The cognitive model introduced by Howes et al.  assumes that when making choices, humans do not observe the options \(x_{1},,x_{n}\) nor their utilities directly. Instead, humans are assumed to make utility-maximizing choices based on two sets of noisy observations of the options. The first set are noisy observations \(}=_{1},,_{n}\) of the true utility of each option. These are modeled as samples from a Gaussian centered around the true utilities

\[ i 1,,n:_{i}(f(x_{i}), _{calc}^{2})\]

with noise \(_{calc}^{2}\) which we will call the _calculation noise_. The second set are noisy observations of the ordinal relation between the values of each attribute for each pair of options. Given an attribute \(k\) and a pair of options \((x_{i},x_{j})\), this ordinal relationship is defined by the following observation function:

\[o(x_{i,k},x_{j,k})=&x_{i,k}<x_{j,k}-_{k}\\ &x_{i,k}>x_{j,k}+_{k}\\ &\]

with \(_{k}\) an attribute-specific tolerance parameter. Intuitively, a larger \(_{k}\) creates a greater margin within which attribute values will be considered equal. For binary attributes, we set \(_{k}\) to zero. Each noisy ordinal observation \((x_{i,k},x_{j,k})\) is sampled as follows: with probability \(1-\) sample \((x_{i,k},x_{j,k})=o(x_{i,k},x_{j,k})\), otherwise sample uniformly at random from \(\{,,\}\). The _probability of ordinal error_\(\) is a parameter, and is the sole source of noise within the ordinal observations. We will denote the set of noisy ordinal observations as \(}=\{(x_{i,k},x_{j,k})\}_{k=1 d,i=1 n,j =i+1 n}\).

Given these observations \(}\) and \(}\) for options \(x_{1},,x_{n}\), and the choice model parameters \(=(_{calc}^{2},,_{1},,_{d})\), the above model implies a posterior distribution over the options' true utilities \(p(|},},)\) and associated expected values \([u_{i}|},},]\). As they do not observe true utilities of the options, humans are assumed to choose the option \(y\) with the highest expected utility:

\[y=*{argmax}_{x_{i}\{x_{1},,x_{n}\}}[u_{i}| },},].\]

Preference learning requires that we are able to reason about how various utilities lead to different choice behaviors. Therefore, to make the original cognitive model amenable to preference learning, we replace the fixed utility function \(f\) by a space of utility functions \(\{f_{w}\}_{w}\) parameterized by a utility parameter \(w\). We assume that the user being modeled makes choices based on some chosen parameter value \(w\), which is known only to them, and which we represent as an additional observed random variable in the model. Necessarily, any calculation of utility therefore depends on \(w\). Under these assumptions the user's posterior over utilities, and thus their choice \(y\), is:

\[y=*{argmax}_{x_{i}\{x_{1},,x_{n}\}}[u_{i}| },},w,].\] (1)

where this expectation is calculated under the posterior

\[p(u|},},w,)  p(}|,)_{}p(, ,}|w,)d\] \[=_{i=1}^{n}p(_{i}|u_{i},)\!\!_{} p(}|,)_{i=1}^{n}p(x_{i})p(u_{i}|x_{i},w)d.\] (2)

### Learning from choice behaviors

In our description of the model above, we have taken the point of view of the user making the choices. However, we now return to a preference learning perspective, i.e. that of an outside observer such as an AI system trying to infer the utility function that underlies these choices. We assume that the AI system observes the presented options \(x_{1},,x_{n}\), as well as the option \(y\) the user chooses. The goal is then to infer the unknown utility parameter \(w\) and choice model parameters \(\) from observedchoices \((,y)\). However, the noisy observations \(}\) and \(}\) on which the user bases their choice are part of their internal perception of the options, and are therefore not observable to an AI system. This means that in evaluating the likelihood of a choice \(y\) under the above choice model we must treat the observations as latent. This yields the following choice policy for the user:

\[p(y|,w,)=_{}}_{}}p(y| },},w,)p(}|, )p(}|,w,)d}d}.\] (3)

Here, \(p(y|},},w)\) is a point mass on \(y\) following equation (1). Given \(m\) pairs \((^{(l)},y^{(l)})\), a prior \(p(w)\) over the space of utility parameters and a prior \(p()\) over the space of choice model parameters, we can use the likelihood in equation (3) to infer a posterior over the parameters \(w\) and \(\):

\[p(w,|\{(^{(l)},y^{(l)})\}_{l=1}^{m}) p(w)p()_{ l=1}^{m}p(y^{(l)}|^{(l)},w,).\]

### Tractable inference through surrogates

The issue we face in calculating \(p(w,|\{(^{(l)},y^{(l)})\})\) is that the likelihood \(p(y|,w,)\) is intractable. First, the calculation of the expected values in equation (1) requires the evaluation of an intractable integral over \(\) in equation (2). The expected values can be approximated using a Monte Carlo estimate , but many samples are needed to achieve a good approximation. Second, the calculation of the likelihood itself requires the evaluation of an intractable integral over all possible observations in equation (3). As before, one could approximate this integral using a Monte Carlo estimate, but this would again require many samples.

Instead, we propose to train surrogate neural networks to approximate both these quantities. We introduce a first neural network \((},},w,)\) trained to predict a vector of the expected values \([|},},w,]\) from given observations \(},}\) and parameters \(w\) and \(\). Then \(()\) is trained by minimizing

\[_{}()=}_{p(w,,,},})}[\|(}, },w,)-\|_{2}].\] (4)

Samples \((w,,,},})\) are obtained by (1) sampling \(w p(w)\), \( p()\) and \( p()\) from their respective priors, (2) calculating \(u_{i}=f_{w}(x_{i})\) for each option, and (3) sampling the observations \(_{i} p(_{i}|u_{i})\) and \(} p(}|,)\). Note that the minimum of \(_{}()\) is exactly the function that assigns to each tuple \((},},w,)\) the vector of expectations \([|},},w,]\).

Next, we train a second neural network \((y|x,w,)\), which we will refer to as our _CRCS model_, to approximate the user's policy \(p(y|,w,)\) over choice behaviors. By using the fact that \((},},w,)[ |},},w,]\) we minimize the cross-entropy loss between \(\) and choices based on utilities predicted by \(\). The loss function is thus:

\[_{}()=}_{p(w,,, },})}[-(}_{x_{1},,x_{n}}(}, },w,)|,w,)].\]

Samples \((w,,,},})\) are obtained as above.

### Modeling cross-feature influence in CRCS

Although our proposed model can predict a range of context effects, it does not yet capture all. Although CRCS can model how each individual feature influences the expected utility of the options, it cannot model how features can impact each other. This is something that LCL does do: its utility weight updating mechanism changes the weight of each feature based on the mean value of all other features. Thus, features can influence how other features are valued. In the most general sense, LCL's fundamental mechanism corresponds to a function \(g(w,)\) which maps the utility weights \(w\) and the set of options \(\) (which make up the context) to a new set of weights \(w^{}\). We therefore propose to integrate this same mechanism into CRCS, resulting in a new model \((y|,g(w,),)\). As \(\) is differentiable, we can infer \(g\) from data using gradient descent. In the experiments that follow, we will use this approach in settings where \(f_{w}\) is a linear function. Thus, like LCL, we will define \(g\) as a linear function of \(x_{C}\): the mean attribute values of the options \(\). We will refer to the resulting model \((y|,w+(Ax_{C})^{T},)\) as LC-CRCS.

## 4 Experiments

We first validate the proposed CRCS model by comparing its results with the original computationally rational choice model by Howes et al. . Then we compare the proposed CRCS model and our LC-CRCS variant with three baselines on human choice data, and finally study the performance of the model on three case studies: car crash structure design, water drainage network design, and retrosynthesis planning.1

We evaluate our proposed CRCS model on four datasets of human choices. These datasets are large sets of choices \((^{(l)},y^{(l)})\) collected from human participants. The District-Smart dataset  contains pairwise preferences over voting districts, where participants were asked to choose the district they felt was most compact. The features extracted for each district are six geometric measures identified by the original authors as good measures of compactness. The Car-Alt dataset  contains choices between six hypothetical alternative fuel cars. Each car has 21 features, including size, range, operating cost, etc. We also use a dataset collected in , which we will call the Hotels dataset, where in a user study participants were asked which of three hotels they preferred. The hotels were collected from a booking site and had as features the price per night and average review rating. For each participant, a choice was collected on one of six sets of options constructed to target three known context effects: attraction, compromise and similarity. Lastly, we use the data collected by Dumbalska et al.  on a property task, which we will refer to as Dumbalska. Here, participants ranked three properties in order of best to worst value. For our purposes, we will treat the top-ranked item as the choice. Value was defined as the given rental cost minus the value participants thought the house was worth (which had been elicited in an earlier stage). For each participant, responses were collected on a large collection of choices, specifically engineered to span the entire range of potential context effects. Thus, unlike the other datasets, we have multiple recorded choices per participant. This allows us to make inferences per individual, rather than at the population level, and evaluate how well our choice models fit the preferences and context effects exhibited by individuals.

### Validation of the CRCS model: risky choice tasks with preference reversals

In this experiment, we validate our CRCS model against the original implementation of Howes et al.  on a risky choice task. In this task, a user is presented with a set of three options, each of which is a pair \((p_{i},v_{i})\) consisting of a probability \(p_{i}\) and a payoff \(v_{i}\). Upon selecting option \(i\), the user receives payoff \(v_{i}\) with probability \(p_{i}\), meaning that each option has expected payoff \(f(p_{i},v_{i})=p_{i} v_{i}\).

Comparing expected option values predicted by \(\) with the Monte Carlo estimates used in , we find that on sets of three options, both generally agreed on the relative magnitude of the utilities, and agreed on the ranking of the utilities in \(92.277\% 0.165\%\) (Agresti-Coull) of cases. Next, we verified \(\)'s ability to predict contextual preference reversals. This was tested on Range-Frequency decoy conditions  where two "Pareto-optimal" options with equal utility are presented along with a decoy option with slightly lower utility which is dominated by one of the other two options. Preference reversals - specifically, increased likelihood of choosing the Pareto-optimal option that dominates the decoy - have been observed in humans and are predicted by the original model. Figure 3 in the appendix shows that \(\) reproduces the range of reversal rates of the original model.

   Dataset & Bradley-Terry & Bower \& Balzano & LCL & CRCS (ours) & LC-CRCS (ours) \\  Hotels & 573 & 573 & 553 & **536** & **536** \\ District-Smart & 3432 & 3432 & 3305 & 3371 & **3276** \\ Car-Alt & 7414 & 7416 & 7290 & 7322 & 7345 \\ Dumbalska & 103669 & 103711 & 100683 & 100450 & **99147** \\   

Table 1: Choice model NLLs on human choice data sets. Bolded digits indicate a significant (p < 0.01) improvement over baselines (BT, BB, LCL).

### Evaluation on static human choice data

In this set of experiments, we evaluate each models' ability to generalize to unseen data. We compare our proposed CRCS model and the LC-CRCS variant against three baselines: vanilla Bradley-Terry, the variant proposed by (referred to as Bower & Balzano) and LCL . On four different datasets, we infer the parameters for each model on a training set of observed choices \(\{(^{(l)},y^{(l)})\}_{l=1}^{m}\) and calculate the negative log-likelihood (NLL) of a held-out test set under the inferred parameters. Inference was done using gradient descent on the NLL of the training set. We performed cross-validation, and report the sum of the test sets' NLLs across the folds. For Hotels, Car-Alt and District-Smart we split the choice data across 50, 20 and 10 folds respectively. By evaluating each choice model on each test fold, we obtained paired observations (one per condition) for each test fold, allowing us to perform a Wilcoxon rank test across the folds to test significance. On Dumbalska, we look at how well the choice models can fit to individuals, and thus perform cross-validation for each participant individually. We then treated the sum of the NLLs of the test sets per participants as individual measures, and tested significance using a Wilcoxon test across the participants. Following prior work, we used a linear utility function in all choice models on all datasets.

Table 1 shows the total NLL achieved by each model on each dataset. We observe that our proposed LC-CRCS model achieves the highest NLL on Hotels, District-Smart and Dumbalska. This difference is significant (p < 0.01) in all three cases. On Car-Alt, we see that LCL performs better than all other models, with the difference being significant (p < 0.01) for all except the CRCS model (p > 0.2). We theorize that the poor performance of the CRCS model on Car-Alt is due to insufficient option data to train \(\) on (see Appendix A.1), leading to poor estimates of expected utility and therefore poor choice predictions.

#### 4.2.1 Evaluating the inferred utility function

As part of the District-Smart human subject study, Kaufman et al.  collected rankings on six sets of districts from small groups of participants. Ranking such large sets is quite difficult, and we should expect these rankings to be quite noisy. However, like the binary choices that were collected, these rankings are indicative of people's true preferences, and thus should be consistent with any ranking of the same districts implied by the utility function we infer from the binary choices. To test this, we use our choice models to infer utility parameters on the entire set of binary choices. For each of the six sets, we then measure - using Kendall's \(\) - how consistent the ranking implied by the inferred utility parameters is with the ranking collected in the study. We report the average consistency across all six sets. Because the log-likelihood of CRCS and LC-CRCS is not convex, we repeat this procedure 25 times, starting from different points, to control for the effect local optima may have on the inferences. We test significance using a Wilcoxon test across the six sets of rankings.

Unlike the previous experiment, during inference we regularized the choice model parameters of LCL, CRCS and LC-CRCS. This was essential to infer utility parameters that were consistent with the collected rankings. For LCL we used the L1 matrix norm of the weight adaptation mechanism's parameter matrix as a regularization term. The L1 norm enforces sparsity and thus encourages LCL to fit only to the most significant context effects . For CRCS and LC-CRCS we used the probability of the choice model parameters under a chosen prior as the regularization term. Using our understanding of the model this allowed us to encode specific prior knowledge into the regularization. More details can be found in Appendix A.3. From the results in Table 2 we observe that both CRCS and LC-CRCS infer utility parameters that are significantly (p < 0.001) more consistent with the collected rankings than the baselines. LC-CRCS performs slightly worse than CRCS, though the difference was not yet significant.

   Dataset & Bradley-Terry & Bower \& Balzano & LCL & CRCS (ours) & LC-CRCS (ours) \\  District-Smart & 0.162 & 0.217 & 0.286 & **0.622** & **0.525** \\   

Table 2: Consistency of inferred utility function with separately collected rankings on District-Smart. Bolded digits indicate a significant improvement over baselines (BT, BB, LCL).

### Elicitation on human choice data

We now evaluate how well the choice models perform in a preference learning setting, where we actively select the queries we put to a user. Whereas in the previous set of experiments we evaluated how well the choice models perform on large amounts of data, here we are interested in how well they perform when minimal data is available. We use the wealth of data available per participant (between 530 and 1060 responses) in the Dumbalska data set to run a user experiment in silico. For each choice model, we use active learning to infer utility function and choice model parameters for each participant individually. At each time step of the experiment, we select from the set of queries recorded for the participant the most informative query using the expected information gain. The participant's response to this query is then revealed, and the posterior over the utility and choice model parameters is updated. We use a particle filter to maintain the posterior beliefs. This elicitation process is performed for 25 time steps on 75 participants. To evaluate the inferences made by the choice models, we calculate the expected likelihood of the remaining queries where the choice has not been revealed yet. As the training data is actively selected for each choice model independently, the data on which they are evaluated - the remaining queries - will differ, meaning that we cannot use a paired test as we have done so far. Instead, we test significance using an independent t-test.

Figure 0(a) shows the mean expected utility calculated over the participants as a function of time for each choice model. We observe that the two variants of our CRCS model make significantly (p < 0.01) better predictions of the participants' choices at all time steps (except 0). Where in the previous experiment (Table 1) we saw that LCL was close in predictive power to our proposed models, we see that in this low data setting the difference is much more pronounced. This is because a number of the context effects observed in the Dumbalska data set are built into the CRCS model. While LCL has shown it can learn some of these effects, it needs much more data to do so. Interestingly, we also observe that the LC-CRCS model, which can learn some new context effects on top of the ones built into \(\), shows significant (p < 0.05) improvement on the CRCS model itself, even when very little data is available. This shows that it provides us with the best of both worlds, showing quick adaptability in low data settings and good performance when more data is available.

### Simulated case studies

We now test the feasibility of using our model to learn a utility function from simulated choice behaviors in real-world tasks, and to use the inferred utility function to help a designer solve a task by recommending design solutions to them. We consider a preference learning setting where we learn utility and choice model parameters by iteratively eliciting a simulated designer's preferences over sets of candidate designs, chosen to maximize the expected information gain. Using \(\), we infer a posterior over the unknown parameters from the observed choices. To simulate a variety of users, we run this experiment in silico, using \(\) with utility parameters sampled from a non-informative prior and choice model parameters sampled from a prior designed to capture a wide range of behaviors exhibited by the CRCS model. At each time step we measure two things: our ability to recover the unknown parameters from the observed choices, and the utility of the design recommendations we make. The inference error is measured by the distance to the true parameters under our current posterior beliefs. The second is measured using the recommendation regret: the difference in utility between the designer's optimal design and the recommended design, which is chosen to be the design with the highest expected utility under the posterior.

#### 4.4.1 Case study 1: learning from preferences in structural design

The first use case involves the design of the frontal crash structure of a car to optimize three separate objectives \(g_{1}(),g_{2}(),g_{3}()\). The design is parameterized by five parameters \(= t_{1}, t_{5}\) which determine the thicknesses of various metal elements. We define the utility function as the Chebyshev scalarization of the original objectives: \(f_{w}()=_{i\{1,2,3\}}w_{i}|g_{i}()-z_{i}^{*}|\) where \(z_{i}^{*}\) denotes the ideal value of \(g_{i}()\) and the weights \(w\) sum to one. Different choices of the utility weights \(w\) correspond to different trade-offs between the objectives, and therefore to different solutions on the Pareto frontier. Figure 0(b) shows the average recommendation regret as a function of the number of queries across 300 runs of this experiment. We observe that the recommendation regret reduces quickly, yielding good recommendations after as few as 10 queries. We attribute this to the utility inference error, shown in Figure 5(a) in Appendix B.1, which reduces equally quickly.

#### 4.4.2 Case study 2: learning from preferences in water drainage network design

Our second use case is a water drainage network design problem . This use case is another multi-objective problem involving six objectives \(g_{1}(),,g_{6}()\). Here, we scalarize the problem using a weighted sum \(f_{w}()=_{i=1}^{6}w_{i}g_{i}()\) where the weights \(w\) sum to one. As before, different choices of \(w\) correspond to different solutions on the Pareto frontier. We ran 300 runs of this experiment. Here too we see that recommendation regret (Figure 0(c)) and utility inference error (Figure 0(a) in the appendix) drop quickly as we put more queries to the designer, with the largest reduction within the first 10 queries. We achieve high-quality recommendations after less than 10 queries.

#### 4.4.3 Case study 3: improving retrosynthesis planning with preference learning

Retrosynthesis planning, the problem of finding feasible reaction pathways to synthesize target molecules, is a central task of synthetic chemistry. Significant progress has been made in solving it through end-to-end automatic synthesis planning . Existing work has focused on expanding the search space of feasible reaction plans. Each route may satisfy an additional subset of properties, and different individuals or organizations may have different preferences over the properties. Chemists' preferences over these plans are often highly complex, representing trade-offs between multiple objectives informed by personal experience and company policy. However, learning their preferences in a way that can then inform AI-driven synthesis planning has not yet been done. We designed a chemist-in-the-loop retrosynthesis planning framework to generate routes with an inferred user model.

Figure 1: (a) Mean expected likelihood of unseen choice data as a function of the number of queries observed for various choice models on the Dumbalska elicitation task. (b-c) Mean recommendation regret as a function of the number of queries observed for the crash structure design and water drainage network design respectively. (d) Maximum utility within the top \(k\) of routes ranked by inferred utility as a function of \(k\). All plots show the mean \(\) twice the standard error around the mean.

To build a personalized retrosynthesis planner, we modified one of the state-of-the-art automatic retrosynthesis platforms, Aizynthfinder , built on Monte Carlo Tree Search (MCTS) with a fixed utility function. Details can be found in Appendix B.2.1. First, we proposed a new utility function as the weighted combination of five feature properties \(g_{1}(),,g_{5}()\), that correspond to _reactants cost, intermediates stability, reaction feasibility, total reaction success rate, poor reaction success rate_, and a route score computed by a data-driven scoring model \(g_{5}\). Given the input routes, this model predicts the distance between the current route and the (latent) optimal route. We trained this model on 47,055 synthetic routes extracted from the Journal of Medicinal Chemistry.

We report the inference error during preference learning in Appendix B.2.2. We integrated the inferred utility weights into our planning system and assessed the consistency of the generated route with the ground truth user utility preferences. We used inferred weights to synthesize 100 target molecules for each weight. In order to measure the recommendation quality, we evaluated the top-ranked routes from both Aizynthfinder and our model under the true utilities. Specifically, we measured the maximum true utility score among the list of top \(k\) recommendations. This is to show how far down from the recommended options list the user needs to go to find their optimal choice. Figure 6(c) shows that within the top \(k\) options, the reaction pathways recommended from our model reaches higher maximum utility score compared to the ones generated by Aizynthfinder. As a significance test, we use the Wilcoxon rank test across every molecule and every user utility with \(p<(1.61 10^{-53})\) for all \(k\).

## 5 Conclusion

In this paper we have proposed a tractable surrogate model of choice, called CRCS, inspired by theories of human decision-making. This model was shown to be a better basis for preference learning than some, but not all, existing models. In response, we modified the model so that it could make cross-feature observations of feature values extending the opportunity for contextual decision-making. We verified against human data from a range of tasks that the new model, called LC-CRCS, outperforms the tested models both in terms of its ability to predict choices and in its inferences of the utility function that underlies the observed choices. Moreover, we find that it corresponds well to previously reported experimental data demonstrating human susceptibility to contextual choice effects. Feasibility of using the new model for preference learning and its ability to recover parameters was also demonstrated in three case studies. Together, the results demonstrate the viability of CRCS and LC-CRCS in high performance preference learning systems.

Limitations and future workWe identify two primary limitations. First, training CRCS requires sufficiently many choice sets, or a sufficient well-specified task so that new sets can be generated. As we saw with Car-Alt, when insufficient choice sets are available for training, performance can suffer. Second, CRCS and CRCS-LC only work on choice sets of fixed size. Extending these surrogates to variable size choice sets is a promising direction for future work. Another promising direction for future work is the application of the current choice model to large language model (LLM) fine-tuning. Currently, given some featurization of LLM responses to a prompt, CRCS could be directly applied. However, this would ignore the reading and interpreting of these responses that human evaluators have to do. As such, we see an extension of the current choice model that integrates these cognitive processes, based on the same computational rationality theory, as potentially transformational future work.

Societal impactThis paper presents work motivated by the goal to advance the field of Machine Learning. The potential societal impact is in line with the broad body of prior work on learning from preferences and modeling humans, none of which we feel must be specifically highlighted here.

AcknowledgementsThis work was supported by the Research Council of Finland (flagship programme: Finnish Center for Artificial Intelligence, FCAI; grants 345604, 341763 and 359207), and the UKRI Turing AI World-Leading Researcher Fellowship, EP/W002973/1. Computational resources were provided by the Aalto Science-IT Project.