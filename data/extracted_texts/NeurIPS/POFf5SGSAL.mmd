# Wiki Entity Summarization Benchmark

Saeedeh Javadi

Polytechnic University of Turin

saeedeh.javadi@studenti.polito.it

&Atefeh Moradan

Aarhus University

atefeh.moradan@cs.au.dk

&Mohammad Sorkhpar

Indiana State University

msorkhpar@sycamores.indstate.edu

&Klim Zaporojets

Aarhus University

klim@cs.au.dk

&Davide Mottin

Aarhus University

davide@cs.au.dk

Ira Assent

Aarhus University

ira@cs.au.dk

Equal contribution

###### Abstract

Entity summarization aims to compute concise summaries for entities in knowledge graphs. Existing datasets and benchmarks are often limited to a few hundred entities and discard graph structure in source knowledge graphs. This limitation is particularly pronounced when it comes to ground-truth summaries, where there exist only a few labeled summaries for evaluation and training. We propose WikES (Wiki Entity Summarization Benchmark), a comprehensive _benchmark_ comprising of entities, their summaries, and their connections. Additionally, WikES features a dataset _generator_ to test entity summarization algorithms in different areas of the knowledge graph. Importantly, our approach combines graph algorithms and NLP models, as well as different data sources such that WikES does not require human annotation, rendering the approach cost-effective and generalizable to multiple domains. Finally, WikES is scalable and capable of capturing the complexities of knowledge graphs in terms of topology and semantics. WikES features existing _datasets_ for comparison. Empirical studies of entity summarization methods confirm the usefulness of our benchmark. Data, code, and models are available at: https://github.com/msorkhpar/wiki-entity-summarization.

## 1 Introduction

_Knowledge Graphs_ (KGs) are a valuable information representation: interconnected networks of entities and their relationships enable machine reasoning to empower question answering Hu et al. (2018); Lan et al. (2019), recommender systems Wang et al. (2018), information retrieval Raviv et al. (2016). KGs may comprise millions of entities representing real-world objects, concepts, or events.

Yet, the size and complexity of these KGs progressively expand, rendering it increasingly challenging to convey the essential information about an entity in a concise and meaningful way Suchanek et al. (2007); Vrandecic and Krotzsch (2014). This is where entity summarization becomes relevant. _Entity summarization_ (ES) Liu et al. (2021) is the process of generating a concise and informative summary that captures the most salient aspects of the entity description, based on the information available inthe KGs. In ES, the entity _description_ refers to all the triples involving such an entity. For instance, Figure 1 illustrates a set of relationships surrounding the entity Ellen Johnson Sirleaf in a KG, along with a possible summary for this entity. Extensive descriptions can overwhelm users and exceed the capacity of typical user interfaces, making it challenging to identify the most relevant triples. Entity summarization addresses this issue by computing an optimal compact summary for an entity, selecting a size-constrained subset of triples Liu et al. (2021).

Despite advances in entity summarization techniques Liu et al. (2021), the development and evaluation of these methods are hindered by a number of limitations in the benchmarks and datasets Liu et al. (2020); Cheng et al. (2023). The first limitation of the current benchmarks is the small dataset size, encompassing only a few hundred entities. Second, the generation of ground-truth summaries for testing mostly relies on expensive and lengthy manual annotation. Moreover, the dependence on a few human annotators often biases the data towards the annotators' preferences and knowledge. Third, existing benchmarks often disregard the wealth of information in the knowledge graph structure.

To address the above limitations, we propose:

* **Novel WikES benchmark for ES** based on summaries and graphs from Wikidata and Wikipedia.
* **Subgraph extraction method** preserving the complexity of real-world KGs; subsampling using random walks and proportionally preserving node degrees, WikES captures the structure of the entities up to the second-hop neighborhood, thereby ensuring that the connections in WikES accurately reflect those in the source KG.
* **Comprehensive summaries for _any_ entity in the KG**, ensuring that summaries are both relevant and contextually rich by deriving them directly from corresponding Wikipedia abstracts, minimizing human bias, as these abstracts are created and reviewed by several experts. In this manner, WikES is scalable, enabling it to generate large benchmark resources efficiently with high-quality annotation.
* **Automatic entity summarization dataset generator** allows for the creation of arbitrarily large datasets, encompassing various domains of knowledge.

Figure 1: KG subgraph of entity Ellen Johnson Sirleaf: arrows depict the subgraph of relationships to other entities, and labels indicate their roles. Selecting the bold edges as entity summaries of the most relevant triples may reduce information overload while concisely describing the entity.

Existing Datasets

Here, we review the existing datasets for entity summarization. Table 1 provides an overview and statistics of the current datasets in this field. FACES and INFO datasets have a higher density than the entities in the Entity Summarization Benchmark (ESBM). It is also clear that LMDB and FACES are not connected graphs, that challenge graph-based learning methods where the information cannot easily propagate in disconnected networks. Specifically, FACES consists of 12 connected components, which complicates the learning process for graph embedding methods by limiting the richness of information that can be leveraged from the graph.

We provide here a comprehensive description of each dataset or benchmark:

* **ESBM**Liu et al. (2020): The Entity Summarization Benchmark (ESBM) is the first benchmark to evaluate the performance of entity summarization methods. ESBM has three versions; v1.2 is the latest and most extensive version. This version comprises 175 entities, with 150 from DBpedia Lehmann et al. (2015) and 25 from LinkedMDB Hassanzadeh and Consens (2009). The summaries comprise triples selected by \(30\) "researchers and students" annotators. Each entity has exactly \(6\) summaries. Despite encompassing two datasets, ESBM has several limitations. First, the entity sampling method is not explained. In particular, some triples in the neighborhood of the entity are missing in the datasets. Second, there are no connections among the entities in the neighborhood, nor any two-hop neighborhood. Third, the expertise and background of the annotators are not assessed nor disclosed. Due to the expensive annotation process, the dataset size is small.
* **FACES**Gunaratna et al. (2015) is a dataset from DBpedia (version 3.9) Auer et al. (2007) and includes \(50\) randomly selected entities, each with at least \(17\) different types of relations. Similar to ESBM, the FACES ground-truth is also generated manually.
* **INFO**Cheng et al. (2023) contains \(100\) randomly selected entities from \(10\) classes in DBpedia. It comprises two sets of ground-truth summaries, REF-E and REF-W. REF-E summaries comprise a selection of triples from five experts adhering to a 140-character limit, similar to typical Google search result snippets. REF-W summaries are obtained by one expert who reads the abstract sections of the respective entities on Wikipedia and selects neighboring entities that closely match the Wikipedia abstracts. The number of ground-truth summaries per entity varies, as some experts evaluate multiple entities. This inconsistency complicates the evaluation process. The expertise of the annotators remains unspecified.

In contrast, our benchmark uses Wikidata to automatically map entities from Wikipedia to Wikidata. This automation allows us to efficiently generate summaries for any number of entities. Unlike previous work, we use the Wikipedia abstract as a summary instead of manual annotators. Each abstract is a collaboration of many users; as such, it should not introduce obvious biases. Additionally, with this process, we ensure high-quality and cost-effective summaries. Furthermore, we present the characteristics of our dataset in Table 3. The WikES benchmark includes a larger number of entities

   Metric & DBpedia (ESBM) & LMDB (ESBM) & FACES & INFO \\  Entities (\(||\)) & 2 721 & 1 853 & 1 379 & 1 410 \\ Relations (\(||\)) & 4 436 & 2 148 & 2 152 & 2 019 \\ Target Entities & 125 & 50 & 50 & 100 \\ Density & 0.0005 & 0.0006 & 0.0011 & 0.0010 \\ Sampling method & Not specified & Not specified & Not specified & Not specified \\ Connected-graph & Yes & No & No & Yes \\ Num-comp & 1 & 2 & 12 & 1 \\ Min Degree & 1 & 1 & 1 & 1 \\ Max Degree & 125 & 208 & 88 & 100 \\   

Table 1: Entity summarization datasets in terms of number of entities \(||\), triples \(||\), number of ground-truth summaries (target entities), density as \(||/{||\\ 2}\), graph connectivity, number of components, sampling method to select entities and subgraph, and minimum / maximum node degree.

and relations than existing datasets. It is a connected graph containing approximately 500 seed nodes. Further details regarding the specific characteristics of our dataset are provided in Section 3.4.

## 3 The WikES Benchmark

A _Knowledge Graph_\(=(,,)\) is a directed multigraph consisting of entities \(=\{v_{1},,v_{n}\}\), relationships \(\), and triples \(\). The set of edges \(=\{(i,j) v_{i},v_{j} r\) s.t. \((v_{i},r,v_{j})\}\) contains pairs of nodes connected by a relationship.

The _\(t\)-hop neighborhood_\(_{t}(v_{i})\) of node \(v_{i}\) is the set of nodes reachable from \(v_{i}\) within \(t\) edges when ignoring edge directions.

A _summary_ for an entity \(v_{i}\) is a subset \((v_{i})_{t}(v_{i})\) of triples from the \(t\)-description of \(v_{i}\), where the _\(t\)-description_ of an entity \(v_{i}\) in a knowledge graph \(\) is the set \(_{t}(v_{i})=\{(s,p,o) s_{t}(v_{i}) o _{t}(v_{i})\}\) of triples in which one of the entities is in the \(t\)-hop neighborhood of \(v_{i}\).

**Entity summarization** for an entity \(v_{i}\) in a knowledge graph \(\) aims to find a summary \((v_{i})\) that maximizes some score among all possible summaries for \(v_{i}\), i.e.,

\[*{arg\,max}_{(v_{i}) _{t}(v_{i})\\ |(v_{i})|=k}*{score}((v_{i})),\] (1)

### Extracting Summaries from Wikidata using Wikipedia Abstracts

We extract summaries for each Wikidata item using Wikipedia abstracts and infoboxes. Each abstract is a joint effort of many users and experts, which ensures quality and accuracy. Leveraging Wikipedia, we avoid time-consuming manual annotation and enable the automatic generation of large-scale datasets.

**Wikidata** is a free and collaborative knowledge base that collects structured data to support Wikipedia and other Wikimedia projects. It includes descriptions and labels for entities. The descriptions offer in-depth details, while the labels serve as concise identifiers, facilitating efficient data retrieval and integration in subsequent steps. We load all Wikidata items XML dump files published on 2023/05/012 as entities \(\) alongside their properties as relationships \(\) into a graph database3. The result is a graph that connects all Wikidata items and statements. We include items if they (1) are not marked as redirects, (2) belong to the main Wikidata namespace, and (3) have an English label or description. Additionally, we load metadata for each Wikidata item and property, including labels and descriptions, into a relational database4. **Wikipedia** pages contain infoboxes, abstracts, page content, categories, references, and more. Links to other Wikipedia pages are referred to as mentions. We detect these mentions in the abstracts and infoboxes of Wikipedia pages to use them later for labeling the summaries in Wikidata. We extract and load all the content from the XML dump files of Wikipedia pages, published on 2023/05/015, into a relational database under the same conditions as Wikidata: the pages must be in English and not redirected.

**Summary annotation.** We annotate the summaries in Wikidata using the corresponding Wikipedia pages. For each Wikipedia page corresponding to a Wikidata entity, we iterate through all connected Wikidata items using Wikidata properties. If a connected Wikidata item is mentioned in the Wikipedia abstract and infobox, we annotate the Wikidata item with the corresponding Wikidata property as part of the summary.

Wikidata is a directed multigraph, which means that each entity (Wikidata item) can be connected to another entity via multiple relations (Wikidata properties). Yet, links in Wikipedia are not labeled; as such, we need to select one of the relations for the summary. To annotate the correct Wikidataproperty as part of the summary, we employ the DistilBERT model Sanh et al. (2019). DistilBERT is a fast and lightweight model with a reduced number of parameters compared to the original BERT model. This way, we can efficiently process large amounts of data while maintaining high-quality embeddings for accurate relation selection.

Concretely, we first embed the abstract of the Wikidata item for which we are generating summaries using DistilBERT. We then calculate the cosine similarity between the embedding of the abstract and the embeddings of each candidate relation. Finally, we add the relation with the highest cosine similarity to the abstract embedding to the summary. This approach ensures that the most relevant Wikidata property is selected for the summary based on its semantic similarity to the Wikipedia abstract.

### Capturing the Graph Structure

Here we introduce the WikES generator algorithm. The main idea is to sample a connected graph that preserves the original graph structure. To this end, we employ random walks Pearson (1905).

A random walk is a stochastic process defined as a sequence of steps, where the direction and magnitude of each step are determined by the random variable \(_{t+1}=_{t}+_{t}\) where \(_{t}\) represents the position at time \(t\), and \(_{t}\) is the step taken from position \(_{t}\).

The process is a Markov process, characterized by its memoryless property:

\[P(X_{t+1}=x|X_{t}=x_{t},X_{t-1}=x_{t-1},,X_{0}=x_{0})=P(X_{t+1}=x|X_{t}=x _{t})\] (2)

In adapting this concept to our work, we redefine the number of random walks assigned to nodes based on their degrees, ensuring the distribution remains proportional to real data. This is achieved through logarithmic transformation and normalization. The logarithmic transformation is applied to reduce the impact of high-degree nodes and also low-degree nodes, making it more manageable for the random walk. Given a graph with node degrees \(\{d_{1},d_{2},,d_{i}\}\), the log-transformed degree for node \(i\) is \(L_{i}=(d_{i})\). These values are then normalized:

\[N_{i}=-(\{L\})}{(\{L\})-(\{L\})}\] (3)

where \(N_{i}\) is the normalized logarithmic degree of node \(i\). Finally, the number of random walks \(R_{i}\) assigned to each node is:

\[R_{i}=(+N_{i}(-))\] (4)

Here, minRW and maxRW are the user-defined minimum and maximum limits for random walks. This adaptation ensures that the random walks are proportional to the normalized logarithmic degree of each node, reflecting the true structure of the network. For a small dataset we set \(=100\) and \(=300\); for a medium dataset \(=150\) and \(=600\); for a large dataset, \(=300\) and \(=1800\). This ensures that the random walks are tailored to both the scale and the complexity of the dataset. Importantly, our approach can be used to extract further subgraphs at the scale needed for benchmarking in a given scenario.

Moreover, the random walk sampling process requires a set of seed nodes as a starting point. In our case, the seed nodes represent the target entities we are interested in. The seed nodes can be any Wikidata Item Identifier, Wikipedia title, or Wikipedia ID of the Wikipedia pages. We collect the seed nodes on the condition that they have at least \(k\) (default \(k=5\)) common entities with the abstract section and the infobox in the Wikipedia pages. Therefore, this model is flexible, allowing you to choose any seed nodes from any domain as an input. In the datasets that we generated, we collect seed nodes from Laouenan et al. (2022). This paper has published information about individuals from various domains. The authors collected data from multiple Wikipedia editions and Wikidata, using deduplication and cross-verification techniques to compile a database of 1.6 million individuals with English Wikipedia pages. The seed nodes that we use include actor, athletic, football, journalist, painter, player, politician, singer, sport, writer, lawyer, film, composer, novelist, poet, and screenwriter. Using combinations of these seed nodes, we generate four sets of datasets, with each set having small,medium, and large versions. In Table 4 in Section 6 in the supplementary material, we present the seed nodes and their proportions for each dataset and their corresponding train-test-val splits.

### WikES Generator

We discuss how WikES is created, and how further benchmarks can be generated without the need for manual annotators. Algorithm 1 details the generator, which consists of the following steps.

**Step1:** Retrieve summaries of each seed node (explained in Section 3.1)

**Step2:** Expand the graph using the random walk method in Section 3.2. Set the random walk's length \(n\) (default \(n=2\)), which means it explores up to the \(n\)-hop neighborhood of each seed node.

**Step3:** Check if the graph is connected. If it is, done. If not, identify all disconnected components and sort them by size, from largest to smallest. In each iteration, connect smaller components to the largest component using \(h\) connections. Utilize the shortest path method, selecting paths that are equal to or less than a minimum path length \(l\). Continue connecting nodes from the smaller component to the larger one until \(h\) nodes are connected. After each iteration, check graph connectivity again. If all components are connected to the largest component, the algorithm ends. Otherwise, re-sort components and increase \(l\) by 1. Repeat until the graph is a single connected component.

```
1:Input: Graph \(G\), seed nodes \(S\), random walk length \(n\), minimum path length \(l\)
2:Output: A connected graph
3:procedureGenerateGraph(\(G\), \(S\), \(n\), \(l\))
4:\(summaries(S)\)
5:\(G(G,S,n)\) mentioned in section 3.2
6:\(is\_connected(G)\)
7:while not \(is\_connected\)do
8:\(components(G)\)
9: Sort \(components\) by size in descending order
10:\(largest components\)
11:for\(comp\) in \(components[1:]\)do
12: Connect \(comp\) to \(largest\) using \(h\) connections via shortest paths \( l\)
13:\(G(G,comp,largest)\)
14:\(is\_connected(G)\)
15:if\(is\_connected\)then
16:break
17:endif
18:endfor
19:\(l l+1\)
20:endwhile
21:return\(G\)
22:endprocedure ```

**Algorithm 1** WikES Generator

### WikES Datasets

We generate three sizes for each of the four datasets, obtaining 12 datasets. We present their characteristics in Table 3 in section 6. The number of entities in the small datasets ranges from approximately 70\(k\) to 85\(k\), and the number of relations ranges from around 120\(k\) to 135\(k\). In the medium datasets, the number of entities ranges from 100\(k\) to 130\(k\), and the number of relations ranges from 195\(k\) to 220\(k\). The number of entities in the large datasets ranges from approximately 185\(k\) to 250\(k\), and the number of relations ranges from around 397\(k\) to 470\(k\). The average runtime for generating small graphs is approximately \(128\) seconds; for medium-sized graphs, it is approximately \(216\) seconds; and for large graphs, it is approximately \(512\) seconds. We construct the train-test-validation split for each dataset with \(70\%\) for training, \(15\%\) for testing, and \(15\%\) for validation. Detailed information about the run time, as well as the number of nodes and relations for these splits, is available on our GitHub repository. All graphs in each train-test-validation splits are connected.

## 4 Empirical Evaluation

We study the quality of WikES using the following metrics:

**F-Score.** Let \(_{m}\) the summary obtained by a summarization method and \(_{h}\) the ground-truth summary. We compare \(_{m}\) with \(_{h}\) using the F1-score based on precision \(P\) and recall \(R\):

\[=,=_{m}_{h}|}{|_{m}|}=_{m}_{h}|}{| _{h}|}\] (5)

The F1 score lies within . High F1 indicates that \(_{m}\) is closer to the ground-truth \(_{h}\).

**Mean Average Precision (MAP).** This metric is particularly suitable for evaluating ranking tasks because it takes into account the order of the predicted triples. MAP calculates precision at each position \(i\) in the predicted summary and averages these values over all relevant summary triples. It reflects both the relevance and the ranking quality of the predicted summaries. MAP, unlike F1-score, does not depend on a specific value of \(k\). This makes it a robust metric for assessing how well a summarization method ranks the relevant triples.

\[=_{n=1}^{N}^{|_{m}^{(n) }|}@{}i(_{h}^{(n)})&\ (n,i)\\ 0&}{|_{h}^{(n)}|}\] (6)

where \(N\) is the total number of entities, \(_{h}^{(n)}\) is the set of ground-truth summary triples for a particular entity \(v_{n}\), \(_{m}^{(n)}\) is the set of predicted summary triples for the entity \(v_{n}\), \(@{}i\) is the precision at the \(i\)-th position in the predicted summary, and \((n,i)\) indicates whether the \(i\)-th predicted triple for entity \(v_{n}\) is relevant (i.e., it belongs to \(_{h}^{(n)}\)). MAP scores are in the range , where a higher MAP indicates better performance in terms of correctly predicting relevant summary triples. To account for the varying lengths of the ground-truth summaries in real-world data, we also calculate MAP and F-score (which we refer to as dynamic MAP and dynamic F-score) by setting the length of the generated summary (\(|_{m}|\)) equal to the length of the corresponding ground-truth summary (\(|_{h}|\)).

We analyze our dataset and compare it with the ESBM benchmark using statistical measures such as frequency and inverse frequency of entities and relations. We calculate the F-score and MAP score for the top-5 and top-10 of both the ESBM dataset and our WikProFem. We choose top-5 and top-10 because we only have ground-truth summaries for top-5 and top-10 in the ESBM dataset. The F-score and MAP results for ESBM are presented in Figure 2. The statistics show that for DBpedia, the F-score using inverse relation frequency outperforms the random baseline by 0.15 for top-5 and by 0.34 for top-10. Furthermore, when using inverse entity frequency, DBpedia achieves an even higher F-score, surpassing the random baseline by 0.07 for top-5 and by 0.15 for top-10. For LMDB, we observe a similar trend when using inverse frequency. The F-score surpasses the random baseline by 0.10 for top-5 and by approximately 0.15 for top-10. Additionally, when employing entity frequency, LMDB achieves an F-score that is around 0.17 higher than the baseline for top-5 and 0.07 higher for top-10. The results demonstrate that ESBM exhibits a strong bias towards entity, reverse entity, and relation frequency. For Map score, we are exactly observing the same behavior for ESBM. We believe that the bias comes from the fact that the datasets are small, their second-hop neighborhood is not considered, and the relations between their first-hop neighbors are not considered. On the other hand, Figure 3 shows the F-score for top-\(5\), top-\(10\) and dynamic F-score on WikES. Since the length of summaries varies with the abstract, we calculate the F-score for each seed node based on its summary length. Results show that WikES F-score is close to random for different statistics, thus rejecting the hypothesis of obvious biases. We observe a minor bias towards node frequency in small datasets. Yet, as we increase the size of the dataset, this bias disappears. We observe a similar behavior with MAP in Figure 4 Furthermore, we use _the entire_ Wikidata to measure the F-score for our seed nodes. Thus, importantly, we observe that our dataset's F-score trend is comparable to that of the entire data, especially our large dataset. We also extracted the first-hop neighborhood of all our seed nodes and observed a small bias in the F-score top-5 and dynamic F-score. We conclude thatadding the two-hop neighborhood makes the sample follow the graph distribution. Thus, WikES is an unbiased benchmark that retains the source KG distribution.

We evaluate the performance of different entity summarization methods on our benchmark, and provide all implementations in the WikES GitHub repository.

* **PageRank** Ma et al. (2008) ranks nodes in a graph based on the structure of incoming links, with the idea that more important nodes are likely to receive more links from other nodes.
* **RELIN** Cheng et al. (2011) is a weighted PageRank algorithm that evaluates the relevance of triples within a graph structure. We have re-implemented this model according to the specifications in the referenced paper. On our smaller dataset version, RELIN takes approximately 6 hours to compute all summaries.
* **LinkSum**Thalhammer et al. (2016) is a two-step, relevance-centric method that combines PageRank with an adaptation of the Backlink algorithm to identify relevant connected entities. We have re-implemented it according to the paper. The LinkSum method initially takes 10 hours to compute the backlinks for each node in the small version of our dataset. By parallelizing the implementation, we reduced this to one hour. Additionally, the Backlink algorithm itself initially takes 100 minutes, but with parallelization, this was reduced to 10 minutes for the small version of our dataset.

Due to the inefficiency of the methods, we use a smaller version of WikES for evaluation. The results in Table 2 show that LinkSum outperforms both RELIN and PageRank. These findings suggest that

Figure 3: F1 for frequency statistics on WikiProFem.

Figure 2: F1 score and MAP for frequency statistics on ESBM datasets.

models capable of exploiting the graph structure while handling large-scale datasets and maintaining high accuracy in entity summarization are valuable for such real-world KGs, such as WikES.

## 5 Conclusion

We introduce WikES (Wiki Entity Summarization Benchmark), a benchmark for KG entity summarization which provides a scalable dataset generator that eschews the need for costly human annotation. WikES uses Wikipedia abstracts for automatic summary generation, ensuring contextually rich and unbiased summaries. It preserves the complexity and integrity of real-world KGs through a random walk sampling method that captures the structure of entities down to their second-hop neighborhoods. Empirical evaluations demonstrate that WikES provides high-quality large-scale datasets for entity summarization tasks, and that it captures the complexities of knowledge graphs in terms of topology, making it a valuable resource for evaluating and improving entity summarization algorithms.

    & &  &  &  \\  Model & Dataset & F-Score & MAP & F-Score & MAP & F-Score & MAP \\  PageRank & WikLiHArt & 0.024 & 0.01 & 0.081 & 0.02 & 0.175 & 0.046 \\  & WikCienema & 0.003 & 0.001 & 0.041 & 0.005 & 0.146 & 0.028 \\  & WikiPro & 0.060 & 0.02 & 0.169 & 0.049 & 0.288 & 0.109 \\  & WikiProFem & 0.032 & 0.01 & 0.093 & 0.024 & 0.145 & 0.036 \\ RELIN & WikLiHArt & 0.093 & 0.035 & 0.148 & 0.054 & 0.208 & 0.080 \\  & WikiCinema & 0.071 & 0.023 & 0.127 & 0.038 & 0.209 & 0.068 \\  & WikiPro & 0.125 & 0.053 & 0.200 & 0.086 & 0.273 & 0.127 \\  & WikiProFem & 0.111 & 0.050 & 0.179 & 0.081 & 0.219 & 0.095 \\ LinkSum & WikLiHArt & 0.184 & 0.080 & 0.239 & 0.109 & 0.225 & 0.127 \\  & WikiCinema & 0.119 & 0.048 & 0.152 & 0.060 & 0.135 & 0.068 \\  & WikiPro & 0.249 & 0.127 & 0.347 & 0.190 & 0.350 & 0.242 \\  & WikiProFem & 0.195 & 0.097 & 0.236 & 0.127 & 0.213 & 0.136 \\   

Table 2: Performance comparison of entity summarization models on the small version of WikES. The models are evaluated with different topK values (5 and 10) and a dynamic setting.

Figure 4: MAP for frequency statistics on WikiProFem.