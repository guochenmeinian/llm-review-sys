# Computing Approximate \(\ell_{p}\) Sensitivities

# Computing Approximate \(_{p}\) Sensitivities

Swati Padmanabhan

Massachusetts Institute of Technology

pswt@mit.edu

&David P. Woodruff

Carnegie Mellon University

dwoodruf@cs.cmu.edu

&Qiuyi (Richard) Zhang

Google Research

qiuyiz@google.com

###### Abstract

Recent works in dimensionality reduction for regression tasks have introduced the notion of sensitivity, an estimate of the importance of a specific datapoint in a dataset, offering provable guarantees on the quality of the approximation after removing low-sensitivity datapoints via subsampling. However, fast algorithms for approximating sensitivities, which we show is equivalent to approximate regression, are known for only the \(_{2}\) setting, in which they are popularly termed leverage scores. In this work, we provide the first efficient algorithms for approximating \(_{p}\) sensitivities and related summary statistics of a given matrix. In particular, for a given \(n d\) matrix, we compute \(\)-approximation to its \(_{1}\) sensitivities at the cost of \(O(n/)\) sensitivity computations. For estimating the total \(_{p}\) sensitivity (i.e. the sum of \(_{p}\) sensitivities), we provide an algorithm based on importance sampling of \(_{p}\) Lewis weights, which computes a constant factor approximation to the total sensitivity at the cost of roughly \(O()\) sensitivity computations. Furthermore, we estimate the maximum \(_{1}\) sensitivity, up to a \(\) factor, using \(O(d)\) sensitivity computations. We generalizeall these results to \(_{p}\) norms for \(p>1\). Lastly, we experimentally show that for a wide class of matrices in real-world datasets, the total sensitivity can be quickly approximated and is significantly smaller than the theoretical prediction, demonstrating that real-world datasets have low intrinsic effective dimensionality.

## 1 Introduction

Many modern large-scale machine learning datasets comprise tall matrices \(^{n d}\) with \(d\) features and \(n d\) training examples, often making it computationally expensive to run them through even basic algorithmic primitives. Therefore, many applications spanning dimension reduction, privacy, and fairness aim to estimate the importance of a given datapoint as a first step. Such importance estimates afford us a principled approach to focus our attention on a subset of only the most important examples.

In view of this benefit, several sampling approaches have been extensively developed in the machine learning literature. One of the simplest such methods prevalent in practice is uniform sampling. Prominent examples of this technique include variants of stochastic gradient descent methods  such as  developed for the "finite-sum minimization" problem

\[_{}_{i=1}^{n}f_{i}().\] (1.1)

where, in the context of empirical risk minimization, each \(f_{i}:_{ 0}\) in (1.1) measures the loss incurred by the \(i\)-th data point from the training set, and in generalized linear models, each \(f_{i}\) represents a link function applied to a linear predictor evaluated at the \(i\)-th data point. The aforementioned algorithms randomly sample a function \(f_{i}\) and make progress via gradient estimation techniques.

However, uniform sampling can lead to significant information loss when the number of important training examples is relatively small. Hence, _importance sampling_ methods that assign higher sampling probabilities to more important examples have emerged both in theory [15; 16] and practice [17; 18; 19; 20; 21]. One such technique is based on _sensitivity scores_, defined  for each coordinate \(i[n]\) as:

\[_{i}}}{{=}} _{}()}{_{j=1}^{n}f_{j}( )}.\] (1.2)

Sampling each \(f_{i}\) independently with probability \(p_{i}_{i}\) and scaling the sampled row with \(1/p_{i}\) preserves the objective in (1.1) in expectation for every \(\). Further, one gets a \((1)\)-approximation to this objective by sampling \((d^{-2})\) functions [22, Theorem \(1.1\)], where \(=_{i=1}^{n}_{i}\) is called the _total sensitivity_, and \(d\) is an associated VC dimension.

Numerous advantages of sensitivity sampling over \(_{p}\) Lewis weight sampling  were highlighted in the recent work , prominent among which is that a sampling matrix built using the \(_{p}\) sensitivities of \(\) is significantly smaller than one built using its \(_{p}\) Lewis weights in many important cases, e.g., when the total sensitivity \(\) is small for \(p>2\) (which is seen in many structured matrices, such as combinations of Vandermonde, sparse, and low-rank matrices as studied in  as well as those in many naturally occuring datasets (cf. Section4)). Additionally, sensitivity sampling has found use in constructing \(\)-approximators for shape-fitting functions, also called strong coresets [15; 26; 27; 28; 16], clustering [16; 30; 29], logistic regression [31; 32], and least squares regression [33; 34].

Despite their benefits, the computational burden of estimating these sensitivities has been known to be significant, requiring solving an expensive maximization problem for each datapoint, which is a limiting factor in their use . A key exception is for \(f_{i}()=(_{i}^{})^{2}\), when the sensitivity score of \(f_{i}\) is exactly the leverage score of \(_{i}\), which can be computed quickly in \(O((n))\) sensitivity calculations, as opposed to the naive \(O(n)\) sensitivity calculations (cf. Section2).

### Our contributions

In this work, we initiate a systematic study of algorithms for approximately computing the \(_{p}\) sensitivities of a matrix, generally up to a constant factor. We first define \(_{p}\) sensitivities:

**Definition 1.1**.: _[_15_]_ _Given a full-rank matrix \(^{n d}\) with \(n d\) and a scalar \(p(0,)\), let \(_{i}\) denote the \(i\)'th row of matrix \(\). Then the vector of \(_{p}\) sensitivities of \(\) is defined as1 (cf. Section2 for the notation)_

\[_{p}(_{i})}}{{=}}_{^{d},} _{i}^{}|^{p}}{\| \|^{p}_{p}},i[n],\] (1.3)

_and the total \(_{p}\) sensitivity is defined as \(_{p}()}}{{=}} _{i[n]}_{p}(_{i})\)._

These \(_{p}\) sensitivities yield sampling probabilities for \(_{p}\) regression, a canonical optimization problem that captures least squares regression, maximum flow, and linear programming, in addition to appearing in applications like low rank matrix approximation , sparse recovery , graph-based semi-supervised learning [38; 39; 40], and data clustering . However, algorithms for approximating \(_{p}\) sensitivities were known for only \(p=2\) (when they are called "leverage scores"). As is typical in large-scale machine learning, we assume \(n d\). We now state our contributions, which are three-fold.

**(1) Approximation algorithms.** We design algorithms for three prototypical computational tasks associated with \(_{p}\) sensitivities of a matrix \(^{n d}\). Specifically, for an approximation parameter \(>1\), we can simultaneously estimate all \(_{p}\) sensitivities up to an additive error of \(O(}{n}_{p}())\) via \(O(n/)\) individual sensitivity calculations, reducing our runtime by sacrificing some accuracy. We limit most of our results in the main text to \(_{1}\) sensitivities, but extend these to all \(p 1\) in AppendixC. To state our results for the \(_{p}\) setting, we use \(()\) to denote the number of non-zero entries of the matrix \(\), \(\) for the matrix multiplication constant, and we introduce the following piece of notation.

**Definition 1.2** (Notation for \(_{p}\) Results).: _We introduce the notation \((m,d,p)\) to denote the cost of approximating one \(_{p}\) sensitivity of an \(m d\) matrix up to an accuracy of a given constant factor._

**Theorem 1.3** (**Estimating all sensitivities**; informal version of Theorem 3.1 and Theorem C.2).: _Given a matrix \(^{n d}\) and \(1 n\), there exists an algorithm (Algorithm 1) which returns a vector \(}^{n d}_{ 0}\) such that, with high probability, for each \(i[n]\), we have_

\[_{1}(_{i})}_{i } O(_{1}(_{i})+_ {1}()).\]

_Our algorithm's runtime is \((()+ d^{})\). More generally, for \(p 1\), we can compute (via Algorithm 6) an estimate \(}^{n d}_{ 0}\) satisfying \(_{p}(_{i})}_{i } O(^{p-1}_{p}(_{i})+ }{n}_{p}())\) with high probability for each \(i[n]\), at a cost of \(O(()+(O(d^{(1,p/2)}),d,p))\)._

Two closely related properties arising widely in algorithms research are the _total sensitivity_\(_{1}()\) and the _maximum sensitivity_\(\|_{1}()\|_{}\). As stated earlier, one important feature of the total sensitivity is that it determines the total sample size needed for function approximation, thus making its efficient estimation crucial for fast \(_{1}\) regression. Additionally, it was shown in  that the sample complexity of sensitivity sampling is much lower than that of Lewis weight sampling in many practical applications such as when the total sensitivity is small; therefore, an approximation of the total sensitivity can be used as a fast test for whether or not to proceed with sensitivity sampling (involving the costly task of calculating all sensitivities).

While the total sensitivity helps us characterize the whole dataset (as explained above), the _maximum sensitivity_ captures the importance of the most important datapoint and is used in, e.g., experiment design and in reweighting matrices for low coherence . Additionally, it captures the maximum extent to which a datapoint can influence the objective function and is therefore used in differential privacy . While one could naively estimate the total and maximum sensitivities by computing all \(n\) sensitivities using Theorem 1.3, we give faster algorithms that, strikingly, have no polynomial dependence on \(n\). In the assumed regime of \(n d\), this is a significant runtime improvement.

**Theorem 1.4** (**Estimating the total sensitivity**; informal version of Theorem 3.5).: _Given a matrix \(^{n d}\), a scalar \(p 1\), and a small constant \(<1\), there exists an algorithm (Algorithm 2), which returns a positive scalar \(\) such that, with a probability of at least \(0.99\), we have_

\[_{p}()(1+O())_{p} ().\]

_Our algorithm's runtime is \((()+} d^{  p/2-1}(O(d^{(1,p/2)},d,p))\)._

In Algorithm 4 (presented in Appendix B.2), we give an alternate method to estimate \(_{1}()\) based on recursive leverage score sampling, without \(_{p}\) Lewis weight calculations, which we believe to be of independent interest. All of our algorithms utilize approximation properties of \(_{p}\) Lewis weights and subspace embeddings and recent developments in fast computation of these quantities.

**Theorem 1.5** (**Estimating the maximum sensitivity**; informal version of Theorem 3.7 and Theorem C.3).: _Given a matrix \(^{n d}\), there exists an algorithm (Algorithm 3) which returns a scalar \(}>0\) such that_

\[\|_{1}()\|_{}} O(\|_{1}()\|_{}).\]

_Our algorithm's runtime is \((()+d^{+1})\). Furthermore, via Algorithm 7, this result holds for all \(p\) and for \(p>2\), this guarantee costs \((()+d^{p/2}(O(d^{p/2} ),d,p))\)._

**(2) Hardness results.** In the other direction, while it is known that \(_{p}\) sensitivity calculation reduces to \(_{p}\) regression, we show the converse. Specifically, we establish hardness of computing \(_{p}\) sensitivities in terms of the cost of solving multiple corresponding \(_{p}\) regression problems, by showing that a \((1+)\)-approximation to \(_{p}\)-sensitivities solves \(_{p}\) multi-regression up to an accuracy factor of \(1+\).

**Theorem 1.6** (**Leave-One-Out \(_{p}\) Multiregression Reduces to \(_{p}\) Sensitivities**; informal2).: _Suppose that we are given an sensitivity approximation algorithm \(\), which for any matrix \(^{}^{n^{} d^{}}\) and accuracy parameter \(^{}(0,1)\), computes \((1^{})_{p}(^{})\) in time \((n^{},d^{},(^{}),^ {})\). Given a matrix \(^{n d}\) with \(n d\), let \(_{i}:=_{^{d-1}}\|_{-i} +_{:i}\|_{p}^{p}\) and \(_{i}^{*}:=_{^{d-1}}\|_{ -i}+_{:i}\|_{p}\) for all the \(i[d]\). Then, there exists an algorithm that takes \(^{n d}\) and computes \((1)_{i}\) for all \(i\) in time \((n+d,d,(),)\)._For \(p 2\), approximating all \(_{p}\) sensitivities to a high accuracy would therefore require improving \(_{p}\) multi-regression algorithms with \((d)\) instances, for which the current best high-accuracy algorithms take \((d)\,()\) time [44; 45]. More concretely, our hardness results imply that in general one cannot compute all sensitivities as quickly as leverage scores unless there is a major breakthrough in multi-response regression. In order to show this, we introduce a reduction algorithm that efficiently regresses columns of \(\) against linear combinations of other columns of \(\), by simply adding a row to \(\) capturing the desired linear combination and then computing sensitivities, which will reveal the cost of the corresponding regression problem. We can solve \((d)\) of these problems by augmenting the matrix to increase the rows and columns by at most a constant factor. We defer these details to Appendix D.

**(3) Numerical experiments.** We consolidate our theoretical contributions with a demonstration of the empirical advantage of our approximation algorithms over the naive approach of estimating these sensitivities using the UCI Machine Learning Dataset Repository  in Section4. We found that many datasets have extremely small total sensitivities; therefore, fast sensitivity approximation algorithms utilize this small intrinsic dimensionality of real-world data far better than Lewis weights sampling, with sampling bounds often a factor of \(2\)-\(5\)x better. We also found these sensitivities to be easy to approximate, with the accuracy-runtime tradeoff much better than our theory suggests, with up to \(40\)x faster in runtime. Lastly, we show that our algorithm for estimating the total sensitivity produces accurate estimates, up to small constant factors, with a runtime speedup of at least \(4\)x.

### Related work

Introduced in the pioneering work of , sensitivity sampling falls under the broad framework of "importance sampling". It has found use in constructing \(\)-approximators of numerical integrands of several function classes in numerical analysis, statistics (e.g., in the context of VC dimension), and computer science (e.g., in coresets for clustering). The results from  were refined and extended by , with more advances in the context of shape-fitting problems in [26; 27; 28], clustering [16; 29; 30], logistic regression [31; 32], least squares regression [33; 47; 34], principal component analysis , reinforcement learning [49; 50], and pruning of deep neural networks [51; 52; 53]. Sampling algorithms for \(_{p}\) subspace embeddings have also been extensively studied in functional analysis [54; 55; 56; 57; 58; 59] as well as in theoretical computer science [23; 60; 44; 45].

Another notable line of work in importance sampling uses Lewis weights to determine sampling probabilities. As we explain in Section2, Lewis weights are closely related to sensitivities. Many modern applications of Lewis weights in theoretical computer science we introduced in , which gave input sparsity time algorithms for approximating Lewis weights and used them to obtain fast algorithms for solving \(_{p}\) regression. They have subsequently been used in row sampling algorithms for data pre-processing [33; 61; 62; 23; 60], computing dimension-free strong coresets for \(k\)-median and subspace approximation , and fast tensor factorization in the streaming model . Lewis weights are also used for \(_{1}\) regression in:  for stochastic gradient descent pre-conditioning,  for quantile regression, and  to provide algorithms for linear algebraic problems in the sliding window model. Lewis weights have also become widely used in convex geometry , randomized numerical linear algebra [23; 69; 64; 70; 42], and machine learning [71; 72; 73].

## 2 Notation and preliminaries

We use boldface uppercase and lowercase letters for matrices and vectors respectively. We denote the \(i^{ th}\) row vector of a matrix \(\) by \(_{i}\). Given a matrix \(\), we use \(()\) for its trace, \(()\) for its rank, \(^{}\) for its Moore-Penrose pseudoinverse, and \(||\) for the number of its rows. When \(x\) is an integer, we use \([x]\) for the set of integers \(1,2,,x\). For two positive scalars \(x\) and \(y\) and a scalar \((0,1)\), we use \(x_{}y\) to denote \((1-)y x(1+)y\); we sometimes use \(x y\) to mean \((1-c)y x(1+c)y\) for some appropriate universal constant \(c\). We use \(\) to hide dependence on \(n^{o(1)}\). We acknowledge that there is some notation overloading between \(p\) (when used to denote the scalar in, for example, \(_{p}\) norms) and \(p_{i}\) (when used to denote some probabilities) but the distinction is clear from context. We defer the proofs of facts stated in this section to Appendix A.

Notation for sensitivities.We start with a slightly general definition of sensitivities: \(_{p}^{ B}(_{i})\) to denote the \(_{p}\) sensitivity of \(_{i}\) with respect to \(\); when computing the sensitivity with respect to the same matrix that the row is drawn from, we omit the matrix superscript:

\[_{p}^{}(_{i}):=_{}_{i }^{}|^{p}}{\|\|_{p}^{p}}_{p}(_{i}):=_{}_{i}^{ }|^{p}}{\|\|_{p}^{p}}.\] (2.1)

When referring to the vector of \(_{p}\) sensitivities of all the rows of a matrix, we omit the subscript in the argument: so, \(_{p}^{}()\) is the vector of \(_{p}\) sensitivities of rows of the matrix \(\), each computed with respect to the matrix \(\) (as in Equation (2.1)), and analogously, \(_{p}()\) is the vector of \(_{p}\) sensitivities of the rows of matrix \(\), each computed with respect to itself. Similarly, the total sensitivity is the sum of \(_{p}\) sensitivities of the rows of \(\) with respect to matrix \(\), for which we use the following notation:

\[_{p}^{}():=_{i[||]}_ {p}^{}(_{i})_{p}():=_{i[| |]}_{p}(_{i}),\] (2.2)

where, analogous to the rest of the notation regarding sensitivities, we omit the superscript when the sensitivities are being computed with respect to the input matrix itself.

While \(_{p}^{}(_{i})\) could be infinite, by appending \(_{i}\) to \(\), the generalized sensitivity becomes once again contained in \(\). Specifically, a simple rearrangement of the definition gives us the identity \(_{p}^{_{i}}(_{i})=1/(1+1/_{p}^{}(_{i}))\). We now define leverage scores, which are a special type of sensitivities and also amenable to efficient computation , which makes them an ideal "building block" in algorithms for approximating sensitivities.

Leverage scores are \(_{2}\) sensitivities.The leverage score of the \(i^{}\) row \(_{i}\) of \(\) is \(_{i}()}}{{=}}_{i} ^{}(^{})^{}_{i}\). The \(i^{}\) leverage score is also the \(i^{}\) diagonal entry of the orthogonal projection matrix \((^{})^{}^{}\). Since the eigenvalues of an orthogonal projection matrix are zero or one, it implies \(_{i}()=_{i}^{}(^{})^{}^{}_{i} 1\) for all \(i[n]\) and \(_{i=1}^{n}_{i}() d\) (see ).

It turns out that \(_{i}()=_{2}(_{i})\) (see ). This may be verified by applying a change of basis to the variable \(=\) in the definition of sensitivity and working out that the maximizer is the vector parallel to \(=^{}_{i}\). This also gives an explicit formula for the total \(_{2}\) sensitivity: \(_{2}()=()\).

The fastest algorithm for constant factor approximation to leverage scores is by , as stated next.

**Fact 2.1** ([60, Lemma 7]).: _Given a matrix \(^{n d}\), we can compute constant-factor approximations to all its leverage scores in time \(O(()(n)+d^{}(d))\)._

In this paper, the above is the result we use to compute leverage scores (to a constant accuracy). Indeed, since the error accumulates multiplicatively across different steps of our algorithms, a high-accuracy algorithm for leverage score computation does not serve any benefit over this constant-accuracy one. Leverage scores approximate \(_{1}\) sensitivities up to a distortion factor of \(\), as seen from the below known fact we prove, for completeness, in Appendix A.

**Fact 2.2** (Crude sensitivity approximation via leverage scores).: _Given a matrix \(^{n d}\), let \(_{1}(_{i})\) denote the \(i^{}\)\(_{1}\) sensitivity of \(\) with respect to \(\), and let \(_{i}()\) denote its \(i^{}\) leverage score with respect to \(\). Then we have \(()}{n}}_{1}(_{i}) ()}\)._

In general, \(_{p}\) sensitivities of rows of a matrix suffer a distortion factor of at most \(n^{|1/2-1/p|}\) from corresponding leverage scores, which follows from Holder's inequality. The \(_{p}\) generalizations of leverages scores are called \(_{p}\) Lewis weights , which satisfy the recurrence \(_{i}=_{i}(^{1/2-1/p})\). However, unlike when \(p=2\), \(_{p}\) Lewis weights are _not_ equal to sensitivities, and from an approximation perspective, they provide only a one-sided bound on the sensitivities \(_{p}^{}(_{i}) d^{(0,p/2-1)}\,_ {p}^{}(_{i})\). While we have algorithms for efficiently computing leverage scores  and Lewis weights , extensions for utilizing these ideas for fast and accurate sensitivity approximation algorithms for all \(p\) have been limited, which is the gap this paper aims to fill. A key regime of interest for many downstream algorithms is a small constant factor approximation to true sensitivities, such as for subsampling .

Subspace embeddings.We use sampling-based constant-approximation \(_{p}\) subspace embeddings that we denote by \(_{p}\). When the type of embedding is clear from context, we omit the subscript.

**Definition 2.3** (\(_{p}\) Subspace Embedding).: _Given \(^{n d}\) (typically \(n d\)), we call \(^{r n}\) (typically \(d r n\)) an \(\)-approximate \(_{p}\) subspace embedding of \(\) if \(\|\|_{p}_{}\|\|_{p}\  ^{d}\)._

When the diagonal matrix \(\) is defined as \(_{ii}=1/}\) with probability \(p_{i}=(_{i}())\) (and \(0\) otherwise), then with high probability \(\) is an \(_{2}\) subspace embedding for \(\)[33; 79]; further, \(\) has at most \((d^{-2})\) non-zero entries. For general \(p\), we use Lewis weights to construct \(_{p}\) subspace embeddings fast [23; Theorem 7.1]. This fast construction has been made possible by the reduction of Lewis weight computation to a few (polynomial in \(p\)) leverage score computations for \(p<4\) as well as \(p 4\). This efficient construction is the reason for our choice of this subspace embedding. We note that there has been an extensive amount of literature on designing \(_{1}\) subspace embeddings for example using Cauchy  and exponential  random variables.

For \(p 2\), it is known that the expected sample complexity of rows is \(_{i=1}^{n}p_{i}=O(d)\), and for \(p>2\), this is known to be at most \(O(d^{p/2})\). While these bounds are optimal in the worst case, one important application of faster sensitivity calculations is to compute the minimal subspace embedding dimension for average case applications, particularly those seen in practice. This gap is captured by the total sensitivity \(_{p}()\), which has been used to perform more sample efficient subspace embeddings [67; 27; 28], as sampling \((_{p}()d/^{2})\) rows suffice to provide an \(_{p}\) subspace embedding.

Accurate sensitivity calculations.While leverage scores give a crude approximation, constant-factor approximations of \(_{p}\) sensitivities seem daunting since we must compute worst-case ratios over the input space. However, we can compute the cost of one sensitivity specific row by reducing it to \(_{p}\) regression since by scale invariance, we have \(_{p}^{_{p}( _{i})}}}=_{_{i}^{T}=1}\|\|_{p}^{p}\).

This reduction allows us to use recent developments in fast approximate algorithms for \(_{p}\) regression [44; 25; 45], which utilize subspace embeddings to first reduce the number of rows of the matrix to \(O((d))\). For the specific case of \(p=1\), which is the focus of the main body of our paper, we may reduce to approximate \(_{1}\) regression on a \(O(d) d\) matrix, which is a form of linear programming.

**Fact 2.4**.: _Given an \(n d\) matrix, the cost of computing \(k\) of its \(_{1}\) sensitivities is \((()+k d^{})\)._

## 3 Approximating functions of sensitivities

We first present a constant-probability algorithm approximating the \(_{1}\) sensitivities in Algorithm1. Our algorithm is a natural one: Since computing the \(_{1}\) sensitivities of all the rows simultaneously is computationally expensive, we instead hash \(\)-sized blocks of random rows into smaller (constant-sized) buckets and compute the sensitivities of the smaller, \(O(n/)\)-sized matrix \(\) so generated, computing the sensitivities with respect to \(\), the \(_{1}\) subspace embedding of \(\). Running this algorithm multiple times gives our high-probability guarantee via the standard median trick.

**Theorem 3.1**.: _Given a full-rank matrix \(^{n d}\) and an approximation factor \(1< n\), let \(_{1}(_{i})\) be the \(_{1}\) sensitivity of the \(i^{}\) row of \(\). Then there exists an algorithm that, in time \((()+ d ^{})\), returns \(}_{>0}^{n}\) such that with high probability, for each \(i[n]\),_

\[_{1}(_{i})}_{i}  O(_{1}(_{i})+_{ 1}()).\] (3.1)

Proof Sketch of Theorem3.1; full proof in SectionB.1.: We achieve our guarantee via Algorithm1. To see this, first note that \(\|\|_{1}(\|\|_{1})\) (since as per Line1, \(\) is an \(_{1}\) subspace embedding of \(\)). Combining this with the fact that every row in \(\) is mapped, via the designed randomness, to some row in the matrix \(\) helps establish the desired approximation guarantee. The runtime follows from using Fact2.4 to compute \(_{1}\) sensitivities with respect to \(\) of \(||=O(n/)\) rows. 

As seen in SectionC.1, our techniques described above also generalize to the \(p 1\) case. Specifically, in SectionC.2, we show that reducing \(_{p}\) sensitivity calculations by an \(\) factor gives an approximation guarantee of the form \(O(^{p-1}_{p}(_{i}))+}{n} _{p}())\).

**Remark 3.2**.: _The sensitivities returned by our algorithm are approximate, with relative and additive error, but are useful in certain settings as they preserve \(_{p}\) regression approximation guarantees while increasing total sample complexity by only a small \((d)\) factor compared to true sensitivities while still keeping it much smaller than that obtained via Lewis weights. To see this with a simple toy

[MISSING_PAGE_FAIL:7]

**Fact 3.4** (Sampling via Lewis Weights ).: _Given \(^{n d}\) and \(p>0\). Consider a random diagonal \(_{p}\) sampling matrix \(^{n n}\) with sampling probabilities \(\{p_{i}\}\) proportional to the \(_{p}\) Lewis weight of \(\), i.e., for each \(i[n]\), the \(i^{}\) diagonal entry is independently set to be_

\[_{i,i}=\{1/p_{i}^{1/p}&p_{i}\\ 0&..\]

_Then, with high probability, \(\) with \(O(^{-2}d^{(1,p/2)}( d)^{2}(d/))\) rows is an \(_{p}\) subspace embedding for \(\) (cf. Definition 2.3)._

**Theorem 3.5**.: _Given a matrix \(^{n d}\) and an approximation factor \((0,1)\), there exists an algorithm, which returns a positive scalar \(\) such that, with a probability \(0.99\), we have_

\[_{p}()(1+O())_{p} ().\]

_Our algorithm's runtime is \((()+} d^{|p/2-1|}(O(d^{(1,p/2)},d,p))\)._

Proof.: Without loss of generality, we may assume \(\) to be full rank. Then, its Lewis weights satisfy \(_{i=1}^{n}_{p}(_{i})=d\). Per Line 2 of Algorithm 2, our sampling distribution is chosen to be \(v_{i}=_{p}(_{i})}{d}\) for all \(i[n]\). We sample from \(\) rows with replacement, with row \(i\) picked with a probability of \(v_{i}\). From the definition of \(v_{i}\) in Line 2 and \(r_{j}\) in Line 5 and the fact that \(_{p}\) is a constant factor \(_{p}\) subspace embedding of \(\), our algorithm's output satisfies the following unbiasedness condition:

\[(_{j[m]}r_{j})=( {m}_{j=1}^{m}_{i_{j}=1}^{n}_{p}^{_ {p}}(_{i_{j}})}{v_{i_{j}}} v_{i_{j}})=_{p}^{_{p}}() 2_{p}().\]

By independence, we also have the following bound on the variance of \(_{j[m]}r_{j}\):

\[(_{j[m]}r_{j})_{i=1 }^{n}_{p}^{_{p}}(_{i})^ {2}}{v_{i}}=_{i=1}^{n}_{p}^{ _{p}}(_{i})^{2}}{_{p}(_{i})},\]

with the final step holding by the choice of \(v_{i}\) in Line 2. In the case that \(p 2\), we have

\[(_{j[m]}r_{j}) _{i=1}^{n}_{p}^{_{p}}(_ {i})^{2}}{_{p}(_{i})}_{i[n]} _{p}^{_{p}}(_{i}) d^{p/2- 1}}{m}_{p}(),\]

where the first inequality uses Fact 3.3. Therefore, applying Chebyshev's inequality on \(_{j[m]}r_{j}\) (as defined in Line 5) with \(m=O(}{_{p}()^{2}})\) gives a a \(\)-multiplicative accuracy in approximating \(_{p}()\) (with the desired constant probability). For \(p 2\), we additionally have [24, Theorem \(1.7\)] the lower bound \(_{p}() d\), which when plugged into the value of \(m\) gives the claimed sample complexity. A similar argument may be applied for the case \(p<2\); specifically, we have that

\[(_{j[m]}r_{j}) _{i=1}^{n}_{p}^{_{p}}(_ {i})^{2}}{_{p}(_{i})}_{i[n]} _{p}^{_{p}}(_{i})_{p}(),\]

where the second step is by \(_{p}(_{i})_{p}(_{i})\) from Fact 3.3. For \(p<2\), we also have \(_{p}() d^{p/2}\) from [24, Theorem \(1.7\)]. Applying Chebyshev's inequality with this fact gives a sample complexity of \(m=O(d^{1-p/2}/^{2})\). This completes the correctness guarantee.

**Runtime.** We first compute all \(_{p}\) Lewis weights of \(^{n d}\) up to a constant multiplicative accuracy, the cost of which is \(O(((n)))\) leverage score computations for \(p<4\) and \(O(p^{3}(np))\) leverage score computations for \(p 4\). Next, we compute \(m=O(d^{|1-p/2|})\) sensitivities with respect to \(_{p}\). From , \(_{p}\) has, with high probability, \(O(d^{p/2} d)\) rows when \(p>2\) and \(O(d d)\) rows when \(p 2\). Summing the cost of computing these \(m\) sensitivities and that of computing the Lewis weights gives the claimed runtime.

**Remark 3.6**.: _We present in Appendix B.2 an alternate algorithm, Algorithm 4, for estimating the total \(_{p}\) sensitivity for \(p=1\). Algorithm 4 uses recursive computations of leverage scores, in contrast to Algorithm 2 which uses Lewis weights in a one-shot manner, and may be of independent interest._

### Approximating the maximum \(_{1}\) sensitivity

In this section, we present an algorithm that approximates \(\|_{1}()\|_{}=_{i\|\|}_{1} (_{i})\), the maximum of the \(_{1}\) sensitivities of the rows of a matrix \(\). As alluded to in Section1, a first approach to this problem would be to simply estimate all \(n\) sensitivities and compute their maximum. To do better than this, one idea, inspired by the random hashing approach of Algorithm1, is as follows.

If the matrix has a large number of high-sensitivity rows, then, intuitively, the appropriately scaled maximum sensitivity of a uniformly sampled subset of these rows should approximate \(\|_{1}()\|_{}\). Specifically, assume that the matrix has at least \(\) rows of sensitivity at least \(\|_{1}()\|_{}/\); uniformly sample \(n/\) rows and return \(}_{a}\), the (appropriately scaled) maximum of \(_{1}^{}}(_{i})\), for the sampled rows \(i\). Then, \(\|_{1}()\|_{}}_{a} O (\|_{1}()\|_{})\) with a constant probability. Here the upper bound guarantee is without any condition on the number of rows with large \(_{1}\) sensitivities.

In the other case, if the matrix does _not_ have too many high-sensitivity rows, we could estimate the maximum sensitivity by hashing rows into small buckets via Rademacher combinations of uniformly sampled blocks of \(\) rows each (just like in Algorithm1). Then, \(}_{b}\), the scaled maximum of the \(_{1}\) sensitivities of these rows satisfies \(\|_{1}()\|_{}}_{b} O (\|_{1}()\|_{})\). Here, it is the _lower_ bound that comes for free (i.e., without any condition on the number of rows with large \(_{1}\) sensitivities).

Despite the above strategies working for each case, there is no way to combine these approaches without knowledge of whether the input matrix has a enough "high-sensitivity" rows or not. We therefore avoid this approach and instead present Algorithm3, where we make use of \(_{}\), an \(_{}\) subspace embedding of \(\). Our algorithm hinges on the recent development [75, Theorem 1.3] on efficient construction of such an embedding with simply a subset of \((d)\) rows of \(\).

```
0: Matrix \(^{n d}\)
0: Scalar \(_{ 0}\) that satisfies \(\|_{1}()\|_{} C \|_{1}()\|_{}\)
1: Compute, for \(\), an \(_{}\) subspace embedding \(_{}^{O(d^{2}(d)) d}\) such that \(_{}\) is a subset of the rows of \(\)[75, Theorem 1.3]
2: Compute, for \(\), an \(_{1}\) subspace embedding \(_{1}^{O(d) d}\)
3: Return \(\|_{1}^{}}(_{} )\|_{}\) ```

**Algorithm 3** Approximating the Maximum of \(_{1}\)-Sensitivities

**Theorem 3.7** (**Approximating the Maximum of \(_{1}\) Sensitivities**).: _Given a matrix \(^{n d}\), there exists an algorithm, which in time \((()+d^{+1})\), outputs a positive scalar \(\) that satisfies_

\[(\|_{1}()\|_{}) O( \|_{1}()\|_{}).\]

Proof Sketch of Theorem3.7; full proof in AppendixB.3.: We achieve our guarantee via Algorithm3. Define \(^{}\) and \(_{i^{}}\) as: \(^{}\), \(i^{}=_{^{d},i[||]}_{i^{}}^{}\|_{1}}{\|\|_{1}}\). Since \(_{1}\) is an \(_{1}\) subspace embedding of \(\), we have, for any \(^{d}\), that \(\|_{1}\|_{1}=(\|\|_{1})\). If \(_{}\) contains the row \(_{i^{}}\), then \(\|_{1}\|_{1}{=}(\|\|_{1})\) implies \(\|_{1}^{}}(_{})\|_ {}=(1)\|_{1}()\|_{}\). In the other case, suppose \(_{i^{}}\) is not included in \(_{}\). Then we observe that

\[\|_{1}^{}}(_{})\|_ {}=_{^{d},_{j}_{ }}_{j}^{}}{\|_{1} \|_{1}}_{^{d}}_{}\|_{}}{\|_{1} \|_{1}}=(1)_{^{d}}_{}\|_{}}{\|\|_{1}},\] (3.2)

where the the second step is by choosing a specific vector in the numerator and the third step uses \(\|_{1}\|_{1}=(\|\|_{1})\). We further have,

\[_{^{d}}_{} \|_{}}{\|\|_{1}}_{} ^{}\|_{}}{\|^{}\|_{1}}^{}\|_{}}{\|^{} \|_{1}}_{i^{}}^{}^{}|}{\| ^{}\|_{1}}=}\|_{1}() \|_{},,\] (3.3)

where the first step is by choosing \(=^{}\), the second step is by the guarantee of \(_{}\) subspace embedding, and the final step is by definition of \(_{1}(_{i^{}})\). Combining Inequality3.2 and Inequality3.3 gives the claimed lower bound on \(\|_{1}^{}}(_{})\|_ {}\). The runtime follows from the cost of computing \(_{}\) from  and that of computing \((d)\) of \(_{1}\) sensitivities with respect to \(_{1}\).

## 4 Numerical experiments

We demonstrate our fast sensitivity approximations on multiple real-world datasets in the UCI Machine Learning Dataset Repository , such as the wine and fires datasets, for different \(p\) and varying approximation parameters \(\). We focus on the wine dataset here, with full details in Appendix E.

Experimental Setup.For each dataset and each \(p\), we first apply the Lewis weight subspace embedding to compute a smaller matrix \(\). Then, we run our sensitivity computation Algorithm 1 and measure the average and maximum absolute log ratio \(|(_{}/_{})|\) to capture the relative error of the sensitivity approximation. Note that this is stricter than our guarantees, which give only an \(O()\) additive error; therefore we have no real upper bound on the maximum absolute log ratio due to small sensitivities. We plot an upper bound of \((^{p})\), which is the worst case additive error approximation although it does provide relative error guarantees with respect to the average sensitivity. We compare our fast algorithm for computing the total sensitivity with the naive brute-force method by approximating all sensitivities and then averaging.

Analysis.In practice, we found most real-world datasets have easy-to-approximate sensitivities with total \(_{p}\) sensitivity about \(2\)-\(5\) times lower than the theoretical upper bound. Figure 1 shows that the average absolute log ratios for approximating the \(_{p}\) sensitivities are much smaller than those suggested by the theoretical upper bound, even for large \(\). Specifically, when \(=40\), we incur only a \(16\)x accuracy deterioration in exchange for a \(40\)x faster algorithm. This is significantly better than the worst-case accuracy guarantee which for \(p=3\) would be \(^{p}=40^{3}\).

More importantly, we find that empirical total sensitivities are much lower than the theoretical upper bounds suggest, often by a factor of at least \(5\), especially for \(p>2\). This implies that real-world structure can be utilized for improved data compression and justifies the importance of sensitivity estimation for real-world datasets. Furthermore, our novel algorithm approximates the total sensitivity up to an overestimate within a factor of \(1.3\), in less than a quarter of the time of the brute-force algorithm. Our observation generalizes to other real-world datasets (see Appendix E).