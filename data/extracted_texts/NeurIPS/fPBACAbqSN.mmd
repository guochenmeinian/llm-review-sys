# MIInference 1.0: Accelerating Pre-filling for

Long-Context LLMs via Dynamic Sparse Attention

 Huiqiang Jiang,\({}^{}\) Yucheng Li\({}^{}\),\({}^{}\) Chengruidong Zhang,\({}^{}\) Qianhui Wu, Xufang Luo,

**Surin Ahn**, **Zhenhua Han**, **Amir H. Abdi**, **Dongsheng Li**, **Chin-Yew Lin**, **Yuqing Yang**, **Lili Qiu**

Microsoft Corporation, \({}^{}\) University of Surrey

{hjiang,chengzhang,yuqyang}@microsoft.com,yucheng.li@surrey.ac.uk

Equal contribution. \({}^{}\)Work during internship at Microsoft.

###### Abstract

The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up pre-filling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce **MIInference** (_Million-tokens Inference_), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices--the _A-shape_, _Vertical-Slash_, and _Block-Sparse_--that can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM-4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MIInference effectively reduces inference latency by up to \(10\) for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MIInference.

Figure 1: Attention weights, especially in long-context LLMs, exhibit up to 96.8% sparsity in contexts of 128K. We propose **MIInference**, leveraging dynamic sparse attention to accelerate the pre-filling stage of long-context LLM inference. It achieves up to 10x speedup for 1M contexts on a single A100, as shown in (b), and matches or surpasses baselines, as demonstrated by Needle In A Haystack  in (a) on LLaMA-3-8B-1M .

Introduction

Large language models (LLMs) have entered the era of long-context processing, with some of them supporting context windows ranging from 128K to 10M tokens . These extended context windows enable LLMs to unlock a multitude of complex real-world applications, such as repository-level code understanding , long-document question-answering , self-play reasoning , extreme-label in-context learning , and long-horizon agent tasks .

However, due to the quadratic complexity of attention, it can take several minutes for the model to process the input prompt (i.e., the pre-filling stage) and then start to produce the first token, which leads to unacceptable Time To First Token experience, thus greatly hinders the wide application of long-context LLMs. As shown in Fig. 1(a), when serving LLaMA-3-8B on a single A100 machine, the model would keep users waiting for 3 minutes to finish the pre-filling stage given a prompt of 300K tokens, and this number increases to 30 minutes for a prompt of 1M tokens. The overhead of self-attention computation exceeds 90% of the total pre-filling latency, which makes it the major bottleneck in long-context processing of LLMs. Previous research has shown that the attention matrices are highly sparse , which has led to the development of fixed sparse attention methods such as Longformer  and BigBird . However, prior studies have also noted that attention distributions vary significantly across different inputs . This dynamic nature prevents prior sparse methods from being used directly on long-context LLMs without expensive training or fine-tuning. But if the dynamic sparse attention patterns could be efficiently predicted online, the pre-filling latency of long-context LLMs could be significantly reduced by calculating only the most important part of the attention weights.

Building upon this idea, we present **MInference**, a technique that reduces 95% of FLOPs in the attention computation to significantly accelerate the pre-filling stage of long-context LLM inference via dynamic sparse attention. Unlike existing dynamic sparse attention methods that introduce large computational overhead to estimate attention patterns with low-rank hidden dimensions , our method is designed specifically for long-context scenarios with minimal overhead in estimation. Specifically, we conduct extensive analysis and identify three general patterns of sparse attention in long-context LLMs: _A-shape_ pattern, _Vertical-Slash_ pattern, and _Block-Sparse_ pattern. Based on these findings, we introduce a kernel-aware search method to assign the optimal attention pattern for each head. Importantly, instead of fixed attention masks in prior studies, we perform an efficient online approximation to build a dynamic sparse mask for each head according to their assigned pattern and particular inputs. For example, to build a dynamic sparse mask for a specific prompt on one _Vertical-Slash_ head, we use a partial of attention weight consisting of the last last_q query and key vectors (i.e. \(_{[-,]}\) and \(\)) to estimate the most important indices of the vertical and slash lines globally on the attention matrix. For _Block-Sparse_ heads, we perform mean pooling on both query and key vectors in blocks of 64 and calculate the block-level attention weights to determine the most important blocks and thereby obtain a block-sparse dynamic mask. After obtaining the dynamic sparse mask, three optimized GPU kernels are used, which we developed for the above three sparse patterns. These kernels are based on the dynamic sparse compilers PIT , Triton  and FlashAttention , which enable extremely efficient computation of dynamic sparse attention.

Extensive experiments are conducted on various Long-context LLMs, including LLaMA-3-8B-1M , GLM-4-9B-1M , and Yi-9B-200K , across benchmarks with context lengths over 1M tokens, such as InfiniteBench , RULER , Needle In A Haystack , and PG-19 . Needle In A Haystack was also tested on Phi-3-Mini-128K  and Qwen-2-7B-128K . Results show that MInference speeds up the pre-filling stage by up to \(10\) for 1M contexts with LLaMA-3-8B on a single A100, reducing latency from 30 minutes to 3 minutes per prompt, while maintaining or improving accuracy.

## 2 Attention Heads: Dynamic, Sparse, and Characteristic

### Attention is Dynamically Sparse

The sparsity of attention weights in pre-trained LLMs, especially in long-context scenarios, has been well-documented . As shown in Fig. 1(b), for an attention matrix of size \(128k 128k\), retaining only the top 4k columns recalls 96.8% of the total attention. In other words, each token is attending to a limit number of tokens despite the long sequence it is processing.

On the other hand, although the sparse nature of attention matrices is shared across different inputs, the exact distributions of sparse pattern are highly dynamic. That is to say, a token at a given position only attends to a subset of the sequence in self-attention, and the exact tokens it attends to are highly context-dependent and vary significantly across different prompts. This dynamism has been mathematically demonstrated in prior studies . As depicted in Fig. 1(c), if we take the top 4k columns found in Fig. 1(b) and apply it on another prompt of 128k, the recall of attention would drop largely to 83.7%.

### Attention Sparsity Exhibits Patterns

Although the sparsity distribution of attention matrix is dynamic, previous works  have shown that they exhibit certain patterns in the two-dimensional space such as spatial clustering. Through our analysis of long-context prompts of various lengths and tasks, we have

   Patterns & A-shape & Vertical-Slash & Block-Sparse & Top-K \\  Spatial Distribution & Static structured & Dynamic structured & Dynamic structured & Dynamic fine-grained \\ Latency on GPU & Low & Medium & Low & High \\ Time to build the index & Zero & Small & Small & High \\   

Table 1: Comparison of different sparse patterns.

Figure 3: (a) Visualization of attention weights from different attention heads. For different prompts and tasks, the pattern of the same head is relatively consistent, but the sparse indices are dynamically changing.(b) Distance of the top-10 nearest non-zero element in the attention matrix. (c) Attention recall distribution using our identified patterns, where FLOPs in the kernel refer to the real FLOPs required for sparse attention computing using on GPUs. Here, a 1x64 block size is used for the _Vertical-Slash_ pattern, and a 64x64 block size is used for others on GPUs. All visualization are based on LLaMA-3-8B-Instruct-262K .

Figure 2: (a) Latency breakdown of the pre-filling stage. (b) How much attention scores can top-k (k=4096) columns cover in a 128k context. (c) Less attention scores are retrieved when reusing the top-k indices from another examples, indicating its dynamic nature. Visualizations are based on LLaMa-3-8B with a single A100.

categorized such attention sparse patterns into the _A-shape_, _Vertical-Slash_ (VS), and _Block-Sparse_ patterns, as shown in Fig. 3a and Fig. 4. Table 1 details the characteristics and differences between these three patterns.

_A-shape_ pattern The attention weights of these types of heads are concentrated on initial tokens and local windows [XTC\({}^{+}\)24, HWP\({}^{+}\)24], exhibiting relatively higher stability.

_Vertical-Slash_ (VS) pattern The attention weights are concentrated on specific tokens (vertical lines) [MJ23] and tokens at fixed intervals (slash lines). The positions of vertical and slash lines in this pattern dynamically change with the context content and exhibit a certain sparsity, making them difficult to be encompassed by local windows and _A-shape_ patterns.

_Block-Sparse_ pattern This sparsity pattern is the most dynamic, exhibiting a more dispersed distribution. Despite its dynamism, the attention weights maintain some characteristics of spatial clustering, which we identify as the block-sparse pattern. We analyzed the distances between non-zero attention weights and their top-k nearest non-zero neighbors within a 128k prompt as shown in Fig. 3b. The results indicate that across layers and heads, the distances between nearest non-zero values are generally concentrated around 5, suggesting a strong spatial clustering of the attention weights.

The point of these three patterns is that we can leverage them to perform highly efficient sparse computing for the attention matrix in long-context LLMs. In Fig. 3c, we test how efficient is our indentified patterns retrieving attention scores with limit computing cost on GPU (FLOPs). First, attention heads are labeled with one of the sparse pattern (detail see SS3.2). Then we demonstrate our patterns are significantly more efficient compared to other sparse methods [RCHG\({}^{+}\)24, XTC\({}^{+}\)24, PPJF24]. Specifically, with the same amount of FLOPs, our patterns achieve a notable higher recall on attention scores, which can potentially lead to better accuracy. For example, previous Top-K methods [RCHG\({}^{+}\)24, XTC\({}^{+}\)24, PPJF24] struggle with the _Block-Sparse_ pattern as they focus on specific tokens globally, while our pattern retrieves attention scores more efficiently and accurately. We example how we use these patterns on long-context LLMs and how we implement optimized GPU kernels for these patterns in SS3.

## 3 MInference 1.0

Following the analysis in SS2, we propose **MInference** to accelerate the pre-filling stage of long-context LLMs, consisting of three steps: 1) Offline attention pattern identification for each head; 2) Dynamic build of sparse indices w.r.t. the pattern; 3) Sparse attention calculation with optimized GPU kernels.

### Problem Formulation

When accelerating the pre-filling stage of long-context LLMs with sparse attention computing, the attention matrix can be formulated as follows:

\[=(}^{}-c(1-)),\] (1)

where \(M_{i,j}\{0,1\}\) represents the dynamic sparse mask for item \(i,j\) of the attention matrix. Here, \(c\) is a large constant, such as 1e5, ensuring that the less important attention weights for which \(M_{i,j}=0\) have values approaching zero after the softmax, i.e., \(A_{i,j} 0\).

Figure 4: The three sparse methods in MInference.

The goal of the dynamic sparse attention system is to achieve greater speedup with minimal overhead while retaining as much of the attention weights as possible. Formally, this can be expressed as:

\[&|()-_{}|,\\ &\ t_{}()+t_{}(), \] (2)

where \(t_{}\) and \(t_{}\) represent the time spent on dynamic sparse attention computation and estimation of the approximate dynamic sparse pattern, respectively.

### Speedup of Long-context LLM Inference via Dynamic Sparse Attention

Kernel-Aware Optimal Sparse Pattern SearchTo achieve the best accuracy with limited FLOPs budget, we propose an offline Kernel-Aware Optimal Sparse Pattern Search method. In this step, we determine which sparse pattern will be used for each attention head, and the optimal setting for the pattern in real calculation (e.g., the number of vertical/slash lines in _VS_ pattern; or the number of top-k blocks in _BS_ patterns). As shown in Algorithm 1, we first create the search space based on a target FLOPs for each pattern, ensuring all potential candidates (i.e., different patterns with different settings) have similar computational cost. _Kernel-aware_ here indicates the computational cost reflects the real FLOPs in GPU kernels, instead of conceptual estimations, which is crucial to achieve the optimal acceleration.

Next, we go through the search space with a reference example to decide the optimal pattern and setting. Specifically, we use recall of the attention output as the objective criterion when searching for the best pattern. This approach leverages FlashAttention  to reduce GPU memory overhead and incorporates the information from the \(\) matrix, enabling end-to-end selection of the best pattern, which further enhances performance.

``` Input:\(,,^{S d_{h}}\), \(k_{v},k_{s}\)  # Approximate vertical andslash pattern (last_q = 64) \(}(_{[, ]}^{}/+_{})\)  # Indices of top \(k_{v}\) vertical line, sum in vertical \(_{v}(_{v}(}),k_{ v})\)  # Indices of top \(k_{s}\) slash line, sum in slash \(_{s}(_{s}(}),k_{ s})\)  # Build sparse attention index \(_{vs}(_{v},_{s})\)  # Final dynamic sparse attention scores (only index block) \(((^{},_{vs })/)\)  # Sparse mixed scores and values \((,_{vs})\)  return \(\) ```

**Algorithm 2** Vertical-Slash Head

Sparsity Indices Approximation and Dynamic Sparse Attention CalculationDuring the inference stage, we will perform an online estimation on the attention matrix to dynamically determine the spatial distribution our sparse indices, based on the assigned patterns and the exact input. After that, we conduct the sparse attention computations with our optimized GPU kernels. The implementation details of our kernels can be found in Appendix C.4. Noted that the sparse mask is static for _A-shape_ heads, so there is no overhead in building the dynamic masks, and only sparse calculation is required.

_(i) Vertical-Slash head._ As shown in Algorithm 2, due to the continuity of vertical and slash lines, we mat (iv) PG-19 [RPJ\({}^{+}\)20]: Following StreamingLLM [XTC\({}^{+}\)24] and H2O [ZSZ\({}^{+}\)24], we use PG-19 for long-context language modeling tasks with prompts up to 100k tokens.

BaselinesWe include five training-free sparse attention approaches as our baselines: 1) StreamingLLM [XTC\({}^{+}\)24], which corresponds to the _A-shape_ pattern. We use 1k global tokens and 4k local windows in all our experiments; 2) StreamingLLM w/ dilated [BPC20], which sets dilated local windows with intervals in the local windows direction. We use 1k global tokens and 8k dilated attention windows with an interval of 1; 3) StreamingLLM w/ strided [CGRS19], which retains local windows while adding dilated attention. We use 1k global tokens, 2k local windows, and 4k dilated attention windows with an interval of 1; 4) InfLLM [XZH\({}^{+}\)24], which uses a memory unit to process streaming long sequences. Following the paper, we set 128 global tokens and 8k local windows in all experiments; 5) Ours w/ static, which utilizes static sparse indices in the _Vertical-Slash_ and _Block-Sparse_ heads. For all baselines, we perform sparse computation only during the pre-filling stage, while retaining dense computation during the decoding stage.

InfiniteBenchAs shown in Table 2, MInference achieves the best overall performance on InfiniteBench compared to baseline methods. Remarkably, MInference matches or even slightly surpasses the performance of the original full attention baseline on some tasks, despite the significant acceleration it provided. From the perspective of different tasks, our method not only performs well in natural language tasks such as summarization, QA, and code, but also maintains the original model's performance on retrieval-related tasks. Baseline methods such as StreamingLLM, on the

   Methods & En.Sum & En.QA & En.MC & En. Dia & Zh.QA & Code.Debug & Math.Find & Retr.PassKey & Retr.Num & Retr.KV & Avg. \\  _LLaMA-3-8B-262K_ & 20.2 & 12.4 & 67.3 & 6.0 & 12.9 & 22.1 & 26.6 & 100.0 & 100.0 & 14.4 & 38.2 \\ StreamingLLM & 21.0 & 8.2 & 40.2 & 10.0 & 10.4 & **25.9** & 30.0 & 86.8 & 5.1 & 0.8 & 23.8 \\ StreamingLLM w/ dilated & 20.1 & 9.4 & 44.5 & **15.5** & **11.2** & 20.5 & 27.5 & 5.0 & 87.5 & 0.5 & 24.2 \\ StreamingLLM w/ strided & 17.3 & 8.2 & 27.5 & 14.5 & 11.2 & 19.5 & 27.5 & 4.0 & 2.1 & 1.0 & 13.3 \\ InfLLM & **24.1** & 7.8 & 45.0 & 6.0 & 11.4 & 19.5 & 32.9 & **100.0** & **100.0** & 1.2 & 34.8 \\ Ours w/ static & 19.9 & **8.6** & **43.2** & 3.5 & 8.9 & 20.6 & 25.1 & 92.4 & 96.3 & 0.2 & 31.9 \\
**Ours** & 20.5 & **12.9** & **65.9** & 7.5 & **12.5** & 22.3 & **33.1** & **100.0** & **100.0** & **12.8** & **38.8** \\  _J-98-200K_ & 8.2 & 10.6 & 64.2 & 1.0 & 17.3 & 21.3 & 23.4 & 99.8 & 100.0 & 28.8 & 37.5 \\ StreamingLLM & 5.4 & **14.2** & 38.0 & **4.0** & 18.8 & 18.8 & 22.3 & 39.2 & 6.1 & 1.6 & 16.8 \\ StreamingLLM w/ strided & 5.7 & 4.2 & 15.0 & 0.0 & 18.2 & 0.0 & 2.9 & 0.0 & 0.0 & 0.0 & 4.2 \\ StreamingLLM w/ strided & 6.1 & 4.5 & 9.8 & 0.0 & 16.9 & 0.0 & 3.1 & 1.5 & 0.0 & 0.0 & 4.6 \\ InfLLM & 6.3 & 13.0 & 45.9 & 2.5 & **21.5** & 20.6 & 34.6 & 85.3 & 88.1 & 1.4 & 31.9 \\ Ours w/ static & 5.8 & 12.6 & 48.5 & 3.0 & 12.6 & 20.8 & **25.1** & 60.9 & 38.5 & 1.0 & 22.9 \\
**Ours** & **7.9** & 11.2 & **64.2** & 1.0 & 17.9 & **24.1** & 23.1 & **99.5** & **100.0** & **27.6** & **37.7** \\  _GLM-4-9B-1M_ & 28.3 & 9.7 & 68.6 & 39.5 & 12.1 & 29.4 & 38.9 & 100.0 & 100.0 & 41.0 & 46.7 \\ StreamingLLM & 27.7 & 6.4 & 40.2 & 12.5 & 10.8 & 27.7 & 21.1 & 97.1 & 25.6 & 0.6 & 27.0 \\ InfLLM & 28.0 & 7.3 & 45.0 & 14.0 & 10.7 & 27.9 & **39.4** & 98.0 & **100.0** & 2.6 & 37.3 \\
**Ours** & **28.8** & **9.6** & **68.6** & **38.5** & **12.0** & **30.7** & 39.1 & **100.0** & **100.0** & **43.0** & **47.0** \\   

Table 2: Performance of different methods with different base models on InfiniteBench [ZCH\({}^{+}\)24].

   Methods & Claimed & Effective & 4K & 8K & 16K & 32K & 64K & 128K & Avg. \\  _LLaMA-3-8B-262K_ & 262K & 16K & 97.2 & 91.8 & 87.3 & 80.8 & 77.4 & 72.2 & 84.4 \\ StreamingLLM & - & 4K & 97.2 & 38.1 & 37.5 & 17.2 & 14.2 & 9.4 & 35.0 \\ StreamingLLM w/ dilated & - & 4K & 23.4 & 0.7 & 1.4 & 18.8 & 16.5 & 15.6 & 12.7 \\ StreamingLLM w/ strided & - & 4K & 2.0 & 0.7 & 0.6 & 0.6 & 0.7 & 1.3 & 1.0 \\ InfLLM & - & 4K & 89.4 & 79.8 & 70.1 & 55.6 & 43.0 & 39.5 & 62.9 \\
**Ours** & - & **32K** & **97.7** & **91.2** & **88.5** & **85.0** & **82.3** & **77.6** & **87.0** \\  _Y-98-200K_ & 200K & 8K & 91.9 & 90.2 & 78.8 & 76.3 & 68.1 & 62.9 & 78.1 \\ StreamingLLM & - & 4K & 91.9 & 37.8 & 33.9 & 18.6 & 13.0 & 12.8 & 34.3 \\ StreamingLLM w/ dilated & - & 4K & 44.8 & 42.8 & 38.5 & 29.8 & 26.8 & 23.9 & 34.4 \\ StreamingLLM w/ strided & - &  & 2.6 & 0.7 & 0.6 & 0.6 & 1.2 & 0.5 & 1.1 \\ InfLLM & - & 4K & 80.3 & 83.9 & 60.7 & 45.2 & 38.6 & 30.2 & 56.5 \\
**Ours** & - & 8K & **92.3** & **89.7** & **79.0** & **73.8** & **64.7** & **56.9** & **74.7** \\  _GLM-4-9B-1M_ & 1M & 64K & 93.8 & 91.6 & 89.3 & 87.4 & 85.2 & 80.8 & 88.0 \\ StreamingLLM & - & 4K & 93.8 & 66.9 & 58.5 & 51.4 & 45.9 & 39.1 & 59.3 \\ InfLLM & - & 8K & **94.7** & 89.5 & 76.4 & 66.5 & 56.8 & 53.5 & 72.9 \\
**Ours** & - & 64K & 94.6 & **93.1** & **91.0** & **89.6** & **85.5** & **84.0** & **89.6** \\   

Table 3: Performance (%) of different models and different methods on RULER [HSK\({}^{+}\)24] evaluated at lengths from 4k to 128k.

contrary, struggle with these retrieval tasks. Additionally, on tasks such as dialogue QA, using local attention mechanisms can better handle these tasks, while our performance is closer to the original results, indicating that our method is not solely based on local windows. Extending the local windows' intervals in StreamingLLM, i.e., w/ dilated and w/ strided, has minimal impact on the model's performance.

RulerTo further reveal the true potential of our method in long-context LLMs, we evaluate MInference with the state-of-the-art long-context challenge, RULER. As shown in Table 3, MInference effectively maintains the long-context performance even in complex multi-hop or aggregation tasks in RULER. It even outperforms the original full attention for testing lengths beyond 32K, achieving effective context windows of 32K and 64K (context with performance over 85% is considered effective [HSK\({}^{+}\)24]) in LLaMA-3-8B-262K and GLM-4-9B-1M.

Language ModelingFollowing the approach of StreamingLLM [XTC\({}^{+}\)24] and H2O [ZSZ\({}^{+}\)24], we evaluate our methods against baselines on the language modeling task based on the PG-19 dataset [RPJ\({}^{+}\)20]. As shown in 5, our method yields best results compared to other sparse approaches, and exhibits minimal divergence compared to the full attention baseline. For prompts of 100K token, our perplexity is only 0.2 higher than the full attention, but lower than StreamingLLM for 0.25 and 0.75 on the Yi-9B-200K and LLaMA-3-262K models respectively.

Needle In A HaystackComparing Fig. 0(a) to Fig. 6, our method effectively retains the ability to process information at different positions across various context windows, ranging from 1k to 1M tokens. In contrast, methods like StreamingLLM and InfLLM (as shown in Appendix D.1), while effective at reducing latency, experience a sharp decline in performance once critical information extends beyond the range of global tokens and local windows.

Ablation StudyTo evaluate the contributions of different components in MInference, we introduce four variants for the ablation study: (1) Ours w/ static, which uses a static sparse mask in the _Vertical-Slash_ and _Block-Sparse_ patterns; (2) Ours w/ only A-shape, which is equivalent to StreamingLLM; (3) Ours w/ only block-sparse, which uses only the _Block-Sparse_ pattern in the dynamic sparse calculation. (4) Ours w/ only vertical-slash, which uses only the _Vertical-Slash_ pattern in the dynamic sparse calculation.

Tables 2, 3, and 4 present the ablation results. It first proves that using static indices significantly degrades LLM performance, especially in highly dynamic tasks like KV retrieval, where accuracy nearly drops to zero. This highlight the necessity of our dynamic strategy and the effectiveness of our dynamically built sparse indices. Additionally, remove any pattern from the three leads to varying degrees of performance degradation. Specifically, "only A-shape" can only capture information

Figure 5: Perplexity results on PG-19 [RPJ\({}^{+}\)20] using different models and methods.

Figure 6: Results on Needle In A Haystack of StreamingLLM [XTC\({}^{+}\)24] in LLaMA-3-8B-1M.

within local windows. The "only block-sparse" variant using only the _BS_ pattern, also results in significant performance declines. On the other hand, "only vertical-slash" manages to preserve most of the performance due to its balance between dynamicity and the StreamingLLM pattern, but still fall behind the full version of our method.

LatencyFig. 0(b) and 0(b) shows the latency and breakdown of MInference across different context windows on a single A100. At 100K, 300K, 500K, and 1M tokens, our method achieves speedups of 1.8\(\), 4.1\(\), 6.8\(\), and 10\(\), respectively. It reduces the pre-filling latency from 30 mins to 3 mins on a single A100 for a prompt of 1M token. By further utilizing tensor parallel [LMZ\({}^{+}\)24] and context parallel [LZA24, JTZ\({}^{+}\)24], this latency can be reduced to 22 seconds on 8x A100 GPUs. This significantly lowers the deployment cost of long-context LLMs and enhances user experience. And since our kernel is implemented based on Triton, it can be easily ported to other devices and achieve similar speedups, such as on the H100 or MI300X. Additionally, analyzing the latency breakdown, we found about 5%-20% of the overhead is spent on dynamic sparse index building, while the remaining time is spent on dynamic sparse calculation.

Integrate with KV cache compression methodsWe also combined MInference with a state-of-the-art KV cache compression method SnapKV [LHY\({}^{+}\)24], as shown in Table 5. This proves our method is compatible with KV cache compression techniques. For most tasks, performance remains nearly unchanged, with the average score even showing a slight increase, which further demonstrates the potential practical value of our method as an optimization for serving long-context LLMs. This phenomenon is also observed in other works, such as ShadowKV [SCB\({}^{+}\)24].

Scaling-up on Larger LLMsWe also evaluated MInference on larger LLMs, such as LLaMA-3-70B-1M3. As shown in Table 6, MInference maintains strong performance even in larger models. Notably, in dynamic tasks such as KV retrieval, MInference can match or even slightly improve performance compared to full attention. In contrast, baselines like InfLLM generally struggle with tasks such as KV retrieval.

## 5 Related Works

Sparse AttentionDue to the quadratic complexity of the attention mechanism, many previous works have focused on sparse attention to improve the efficiency of Transformers. These methods include static sparse patterns, cluster-based sparse approaches, and dynamic sparse attention.

   Methods & En.Sum & En.QA & En.MC & En.Dia & Zh.QA & Code.Debug & Math.Find & Retr.PassKey & Retr.Num & Retr.KV & Avg. \\  _LLaMA-3-70B-262K_ & 20.7 & 10.3 & 84.2 & 9.5 & 14.0 & 33.2 & 61.7 & 97.0 & 100.0 & 34.0 & 46.5 \\ StreamingLLM & 20.5 & 8.5 & 52.0 & **10.0** & 12.6 & 27.4 & 61.1 & 14.0 & 10.0 & 0.0 & 21.6 \\ Int.LLM & **24.1** & 8.1 & 57.0 & **10.0** & 12.9 & 27.4 & 52.3 & **100.0** & **100.0** & 0.0 & 39.2 \\
**Ours** & 20.6 & **10.1** & **83.4** & **10.0** & **14.1** & **34.1** & **61.9** & **100.0** & **100.0** & **39.0** & **47.3** \\   

Table 6: Performance of different methods using LLaMA-3-70B-Instruct-262K on InfiniteBench [ZCH\({}^{+}\)24].

   Methods & En.Sum & En.QA & En.MC & En.Dia & Zh.QA & Code.Debug & Math.Find & Retr.PassKey & Retr.Num & Retr.KV & Avg. \\  Ours & 20.5 & 12.9 & 65.9 & 7.5 & 12.5 & 22.3 & 33.1 & 100.0 & 100.0 & 12.8 & 38.8 \\ Ours w/ only block-sparse & 12.4 & 3.4 & 5.7 & 6.0 & 3.1 & 12.2 & 24.0 & 59.5 & 60.3 & 0.0 & 18.7 \\ Ours w/ only vertical-slash & 19.6 & 12.0 & 62.1 & 9.5 & 11.7 & 21.6 & 29.1 & 100.0 & 100.0 & 5.0 & 37.1 \\   

Table 4: Performance of different ablation methods using LLaMA-3-8B-Instruct-262K on InfiniteBench [ZCH\({}^{+}\)24].

   Methods & En.Sum & En.QA & En.MC & En.Dia & Zh.QA & Code.Debug & Math.Find & Retr.PassKey & Retr.Num & Retr.KV & Avg. \\   LLaMA-3 w/ SnapKV & 18.0 & **11.8** & 65.5 & 2.5 & 12.0 & 21.3 & 26.6 & **100.0** & **100.0** & 1.8 & 36.0 \\
**Ours** w/ SnapKV & **18.9** & 11.7 & **66.4** & **6.5** & **12.1** & **21.8** & **33.1** & **100.0** & **100.0** & **2.0** & **37.3** \\   

Table 5: Performance of different methods on InfiniteBench [ZCH\({}^{+}\)24] using SnapKV [LHY\({}^{+}\)24] in the decoding stage.

Static sparse patterns include techniques such as sliding windows [JSM\({}^{+}\)23, AJA\({}^{+}\)24], dilated attention [CGRS19, SGR\({}^{+}\)21, DMD\({}^{+}\)23], and mixed sparse patterns [BPC20, ZGD\({}^{+}\)20, LCSR21]. Cluster-based sparse methods include hash-based [KKL20] and kNN-based [RSVG21, NLC\({}^{+}\)24] methods. All of the above methods require pre-training the model from scratch, which makes them infeasible to be directly used as a plugin for reay-to-use LLMs. Recently, there has been work [DG24, ZAW24] to unify state space models [GGR22, GD24, DG24], and linear attention [KVPF20, SDH\({}^{+}\)23] into structured masked attention. Additionally, some works [WZH21, LQC\({}^{+}\)22, RCHG\({}^{+}\)24] leverage the dynamic nature of attention to predict sparse patterns dynamically. However, these approaches often focus on low-rank hidden states during the dynamic pattern approximation or use post-statistical methods to obtain the sparse mask, which introduce substantial overhead in the estimation step, making them less useful for long-context LLMs.

Scaling Context Windows of LLMsRecent research has focused on expanding the context window of pre-trained LLMs, that enables LLMs to handle more complex real-life applications [JYW\({}^{+}\)23, POC\({}^{+}\)23]. These methods can be categorized into: 1) Staged pre-training [NXH\({}^{+}\)23, FPN\({}^{+}\)24]; 2) Modifying or interpolating position embeddings [PSL22, CWCT23, PQFS24, DZZ\({}^{+}\)24]; 3) Utilizing external memory modules for context storage [BANG23, TSP\({}^{+}\)23, XZH\({}^{+}\)24]; 4) Expanding computations across multiple devices in a distributed manner [LZA24]. However, these methods do not alleviate the high inference costs in long-context processing.

Long-Context LLM InferenceRecent studies [Fu24] have tackled the high computational cost of attention and substantial KV cache storage in long-context scenarios from two angles: pre-filling and decoding. Pre-filling optimizations are primarily categorized as State Space Models [GGR22, GD24], linear attention methods [SDH\({}^{+}\)23, PAA\({}^{+}\)23], memory-based methods [MFG24, HBK\({}^{+}\)24], hybrid methods [LLB\({}^{+}\)24, RLL\({}^{+}\)24], and prompt compression methods [LDGL23, JWL\({}^{+}\)23, JW\({}^{+}\)24, PWJ\({}^{+}\)24]. However, these approaches require training from scratch or additional overhead and are difficult to implement directly in pretrained long-context LLMs. Recently, some studies [MEL24, XZH\({}^{+}\)24, LCL\({}^{+}\)24] have focused on using kNN or cluster-based sparse attention to accelerate LLM inference. However, these methods often lead to reduced accuracy, limited speedup, or are restricted to CPU scenarios.

In contrast, optimizations for the decoding stage are divided into [LJW\({}^{+}\)24]: 1) Reusing attention KV to reduce KV cache storage [Sha19, ALTdJ\({}^{+}\)23, SDZ\({}^{+}\)24, DA24, NLC\({}^{+}\)24]; 2) Static KV cache dropping [XTC\({}^{+}\)24, HWP\({}^{+}\)24]; 3) Dynamic KV cache dropping [ZSZ\({}^{+}\)24, LDL\({}^{+}\)24, GZL\({}^{+}\)24, OHY\({}^{+}\)24, LHY\({}^{+}\)24, APB\({}^{+}\)24]; 4) Dynamic KV cache offloading [RCHG\({}^{+}\)24, DHJ\({}^{+}\)24, TZZ\({}^{+}\)24, LCL\({}^{+}\)24, CSY\({}^{+}\)24, SCB\({}^{+}\)24]; 5) Methods for restoring performance loss due to KV cache compression [AAJ\({}^{+}\)24, DYZ\({}^{+}\)24]; 6) Hierarchical speculative decoding methods [SCY\({}^{+}\)24, CTS\({}^{+}\)24]; 7) KV cache quantitation [LYJ\({}^{+}\)24]. Nevertheless, these methods do not address the heavy computational burden of the attention in the pre-filling stage.

## 6 Conclusion

This paper addresses the expensive computational cost and the unacceptable latency of the attention calculations in the pre-filling stage of long-context LLMs. We propose MInference, a method that accelerates the pre-filling stage by leveraging dynamic sparse attention with spatial aggregation patterns. Specifically, we categorize attention heads into three types: _A-shape_, _Vertical-Slash_, and _Block-Sparse_. Using a kernel-aware optimal sparse pattern search method, we identify the optimal pattern for each head. Subsequently, we utilize a fast approximation approach to build dynamic sparse masks for different inputs, and then apply these mask to perform sparse attention calculations. Experimental results on benchmarks such as InfiniteBench, RULER, language modeling, and Needle In A Haystack demonstrate that our method effectively maintains the long-context capabilities of LLMs while achieving up to a \(10\) speedup, reducing the latency from 30 minutes to 3 minutes per prompt for 1 million token prompts on a single A100 GPU. Additionally, we have found that similar dynamic sparse attention patterns also exist in both multi-modal LLMs [WWL\({}^{+}\)24] and encoder-decoder LLMs [RSR\({}^{+}\)20]. Using MInference for pre-filling stage inference acceleration holds great promise.