# Deep Equilibrium Based Neural Operators for Steady-State PDEs

Tanya Marwah\({}^{1}\) Ashwini Pokle \({}^{1}\) J. Zico Kolter \({}^{1,2}\) Zachary C. Lipton \({}^{1}\) Jianfeng Lu \({}^{3}\) Andrej Risteski \({}^{1}\)

\({}^{1}\)Carnegie Mellon University \({}^{2}\) Bosch Center for AI \({}^{3}\) Duke University

{tmarwah,apokle,zicokolter,zlipton,aristesk}@andrew.cmu.edu

jianfeng@math.duke.edu

Equal contribution. Correspondence to tmarwah@andrew.cmu.edu and apokle@andrew.cmu.edu

###### Abstract

Data-driven machine learning approaches are being increasingly used to solve partial differential equations (PDEs). They have shown particularly striking successes when training an operator, which takes as input a PDE in some family, and outputs its solution. However, the architectural design space, especially given structural knowledge of the PDE family of interest, is still poorly understood. We seek to remedy this gap by studying the benefits of weight-ted neural network architectures for steady-state PDEs. To achieve this, we first demonstrate that the solution of most steady-state PDEs can be expressed as a fixed point of a non-linear operator. Motivated by this observation, we propose FNO-DEQ, a deep equilibrium variant of the FNO architecture that directly solves for the solution of a steady-state PDE as the infinite-depth fixed point of an implicit operator layer using a black-box root solver and differentiates analytically through this fixed point resulting in \((1)\) training memory. Our experiments indicate that FNO-DEQ-based architectures outperform FNO-based baselines with \(4\) the number of parameters in predicting the solution to steady-state PDEs such as Darcy Flow and steady-state incompressible Navier-Stokes. Finally, we show FNO-DEQ is more robust when trained with datasets with more noisy observations than the FNO-based baselines, demonstrating the benefits of using appropriate inductive biases in architectural design for different neural network based PDE solvers. Further, we show a universal approximation result that demonstrates that FNO-DEQ can approximate the solution to any steady-state PDE that can be written as a fixed point equation.

## 1 Introduction

Partial differential equations (PDEs) are used to model a wide range of processes in science and engineering. They define a relationship of (unknown) function and its partial derivatives. Most PDEs do not admit a closed form solution, and are solved using a variety of classical numerical methods such as finite element , finite volume , and spectral methods . These methods are often very computationally expensive, both as the ambient dimension grows, and as the desired accuracy increases.

This has motivated a rapidly growing area of research in data-driven approaches to PDE solving. One promising approach involves learning _neural solution operators_, which take in the coefficients of a PDE in some family and output its solution--and are trained by examples of coefficient-solution pairs.

While several architectures for this task have been proposed, the design space--in particular taking into account structural properties of the PDEs the operator is trained on--is still largely unexplored. Most present architectures are based on "neuralizing" a classical numerical method. For instance, Li et al. (2020) take inspiration from spectral methods, and introduce FNO: a trained composition of (parametrized) kernels in Fourier space. Brandstetter et al. (2022) instead consider finite-difference methods and generalize them into (learnable) graph neural networks using message-passing.

Our work focuses on families of PDEs that describe the steady-state of a system (that is, there is no time variable). Namely, we consider equations of the form:

\[L(a(x),u(x))=f(x), x,\] (1)

where \(u:^{d_{u}}\), \(a:^{d_{a}}\) and \(f:^{d_{f}}\) are functions defined over the domain \(\), and \(L\) is a (possibly non-linear) operator. This family includes many natural PDE families like Poisson equations, electrostatic equations, and steady-state Navier-Stokes.

We take inspiration from classical numerical approaches of fast-converging Newton-like iterative schemes (LeVeque, 2007; Farago and Karatson, 2002) to solve steady-state PDEs, as well as recent theoretical works for elliptic (linear and non-linear PDEs) (Marwah et al., 2021; Chen et al., 2021; Marwah et al., 2022) to hypothesize that very deep, but heavily weight-tied architectures would provide a useful architectural design choice for steady-state PDEs.

In this paper, we show that for steady state equations it is often more beneficial to weight-tie an existing neural operator, as opposed to making the model deeper--thus increasing its size. To this end, we introduce **FNO-DEQ**, a new architecture for solving steady-state PDEs. FNO-DEQ is a deep equilibrium model (DEQ) that utilizes weight-tied FNO layers along with implicit differentiation and root-solvers to approximate the solution of a steady-state PDE. DEQs are a perfect match to the desiderata laid out above: they can be viewed alternately as directly parameterizing the fixed points of some iterative process; or by explicitly expanding some iterative fixed point solver like Newton's or Broyden's method as an infinitely deep, weight-tied model.

Such an architecture has a distinct computational advantage: implicit layer models effectively backpropagate through the infinite-depth network while using only constant memory (equivalent to a single layer's activations). Empirically, we show that for steady-state PDEs, weight-tied and DEQ based models perform better than baselines with \(4\) the number of parameters, and are robust to training data noise. In summary, we make the following contributions:

* We show the benefits of weight-tying as an effective architectural choice for neural operators when applied to steady-state PDEs.
* We introduce FNO-DEQ, a FNO based deep equilibrium model (DEQ) that uses implicit layers and root solving to approximate the solution of a steady-state PDE. We further attest to the empirical performance of FNO-DEQ by showing that it performs as well as FNO and its variants with \(4\) number of parameters.
* We show that FNO-DEQ and weight tied architectures are more robust to both input and observation noise, thus showing that weight-tying is a useful inductive bias for architectural design for steady-state PDEs.
* By leveraging the universal approximation results of FNO (Kovachki et al., 2021) we show that FNO-DEQ based architectures can universally approximate the solution operator for a wide variety of steady-state PDE families.
* Finally, we create a dataset of pairs of steady-state incompressible Navier-Stokes equations with different forcing functions and viscosities, along with their solutions, which we will make public as a community benchmark for steady-state PDE solvers.

## 2 Related Work

Neural network based approaches for solving PDEs can broadly be divided into two categories. First are hybrid solvers (Bar-Sinai et al., 2019; Kochkov et al., 2021; Hsieh et al., 2019) which use neural networks in conjunction with existing numerical solvers. The main motivation is to not only improve upon the existing solvers, but to also replace the more computationally inefficient parts of the solver with a learned counter part. Second set of approaches are full machine learning based approaches that aim to leverage the approximation capabilities of neural networks (Hornik et al., 1989) to directly learn the dynamics of the physical system from observations.

Hybrid solvers like Hsieh et al. (2019) use a neural network to learn a correction term to correct over an existing hand designed solver for a Poisson equation, and also provide convergence guarantees of their method to the solution of the PDE. However, the experiments in their paper are limited to linear elliptic PDEs. Further, solvers like Bar-Sinai et al. (2019) use neural networks to derive the discretizations for a given PDE, thus enabling the use of a low-resolution grid in the numerical solver. Furthermore, Kochkov et al. (2021) use neural networks to interpolate differential operators between grid points of a low-resolution grid with high accuracy. This work specifically focuses on solving Navier-Stokes equations, their method is more accurate than numerical techniques like Direct Numerical Simulation (DNS) with a low-resolution grid, and is also \(80\) more faster. Brandstetter et al. (2022) introduced a message passing based hybrid scheme to train a hybrid solver and also propose a loss term which helps improve the stability of hybrid solvers for time dependent PDEs. However, most of these methods are equation specific, and are not easily transferable to other PDEs from the same family.

The neural network based approach that has recently garnered the most interest by the community is that of the operator learning framework (Chen and Chen, 1995; Kovachki et al., 2021; Lu et al., 2019; Li et al., 2020; Bhattacharya et al., 2021), which uses a neural network to approximate and infinite dimensional operator between two Banach spaces, thus learning an entire family of PDEs at once. Lu et al. (2019) introduces DeepONet, which uses two deep neural networks, referred to as the branch net and trunk net, which are trained concurrently to learn from data. Another line of operator learning framework is that of neural operators Kovachki et al. (2021). The most successful methodology for neural operators being the Fourier neural operators (FNO) (Li et al., 2020). FNO uses convolution based integral kernels which are evaluated in the Fourier space. Future works like Tran et al. (2021) introduce architectural improvements that enables one to train deeper FNO networks, thus increasing their size and improving their the performance on a variety of (time-dependent) PDEs. Moreover, the success of Transformers in domains like language and vision has also inspired transformer based neural operators in works like Li et al. (2022), Hao et al. (2023) and Liu et al. (2022). Theoretical results pertaining to the neural operators mostly include universal approximation results Kovachki et al. (2021), Lanthaler et al. (2022) which show that architectures like FNO and DeepONet can indeed approximate the infinite dimension operators.

In this work, we focus on steady-state equations and show the benefits of weight-tying in improving the performance of FNO for steady-state equations. We show that instead of making a network deeper and hence increasing the size of a network, weight-tied FNO architectures can outperform FNO and its variants \(4\) its size. We further introduce FNO-DEQ, a deep equilibrium based architecture to simulate an infinitely deep weight-tied network (by solving for a fixed point) with \((1)\) training memory. Our work takes inspiration from recent theoretical works like Marwah et al. (2021), Chen et al. (2021), Marwah et al. (2022) which derive parametric rates for some-steady state equations, and in fact prove that neural networks can approximate solutions to some families of PDEs with just \((d)\) parameters, thus evading the curse of dimensionality.

## 3 Preliminaries

We now introduce some key concepts and notation.

**Definition 1** (\(L^{2}(;^{d})\)).: _For a domain \(\) we denote by \(L^{2}(;^{d})\) the space of square integrable functions \(g:^{d}\) such that \(\|g\|_{L^{2}()}<\), where \(\|g\|_{L^{2}()}=(_{}\|g(x)\|_{_{2}}^{2}dx)^{1/2}\)._

### Neural Operators

Neural operators (Lu et al., 2019; Li et al., 2020; Bhattacharya et al., 2021; Patel et al., 2021; Kovachki et al., 2023) are a deep learning approach to learning solution operators which map a PDE to its solution. Fourier Neural Operator (FNO) (Li et al., 2020) is a particularly successful recent architecture parametrized as a sequence of kernel integral operator layers followed by non-linear activation functions. Each kernel integral operator layer is a convolution-based kernel function that is instantiated through a linear transformation in Fourier domain, making it less sensitive to the level of spatial discretization. Specifically, an \(L\)-layered FNO \(G_{}:^{d_{u}}^{d_{u}}\) with learnable parameters \(\), is defined as

\[G_{}:=_{L}_{L-1} _{1}\] (2)

where \(:L^{2}(;^{d_{u}}) L^{2}(^{d_{ }};^{d_{u}})\) and \(:L^{2}(^{d_{}};^{d_{}}) L ^{2}(^{d_{}};^{d_{u}})\) are projection operators, and \(_{l}:L^{2}(^{d_{}};^{d_{}}) L ^{2}(^{d_{}};^{d_{}})\) for \(l[L]\) is the \(l^{}\) FNO layer defined as,

\[_{l}(v_{l})=(W_{l}v_{l}+b_{l}+_{l} (v_{l})))\,.\] (3)

Here \(\) is a non-linear activation function, \(W_{l},b_{l}\) are the \(l^{th}\) layer weight matrix and bias terms. Finally \(_{l}\) is the \(l^{th}\) integral kernel operator which is calculated using the Fourier transform as introduced in Li et al. (2020) defined as follows,

\[_{l}(v_{l})=^{-1}(R_{l}(v_{l} ))(x) x,\] (4)

where \(\) and \(^{-1}\) are the Fourier transform and the inverse Fourier transform, with \(R_{l}\) representing the learnable weight-matrix in the Fourier domain. Therefore, ultimately, the trainable parameters \(\) is a collection of all the weight matrices and biases, i.e, \(:=\{W_{l},b_{l},R_{l},,W_{1},b_{1},R_{1}\}\).

### Equilibrium Models

Equilibrium models (Liao et al., 2018; Bai et al., 2019; Revay et al., 2020; Winston and Kolter, 2020) compute internal representations by solving for a fixed point in their forward pass. Specifically, consider a deep feedforward network with \(L\) layers :

\[z^{[i+1]}=f_{}^{[i]}(z^{[i]};x)i=0,...,L-1\] (5)

where \(x^{n_{z}}\) is the input injection, \(z^{[i]}^{n_{z}}\) is the hidden state of \(i^{th}\) layer with \(z^{}=\), and \(f_{}^{[i]}:^{n_{z} n_{z}}^{n_{z}}\) is the feature transformation of \(i^{th}\) layer, parametrized by \(\). Suppose the above model is weight-tied, _i.e.,_\(f_{}^{[i]}=f_{}, i\), and \(_{i}f_{}(z^{[i]};x)\) exists and its value is \(z^{}\). Further, assume that for this \(z^{}\), it holds that \(f_{}(z^{};x)=z^{}\). Then, equilibrium models can be interpreted as the infinite-depth limit of the above network such that \(f_{}^{}(z^{};x)=z^{}\).

Under certain conditions2, and for certain classes of \({f_{}}^{3}\), the output \(z^{}\) of the above weight-tied network is a fixed point. A simple way to solve for this fixed point is to use fixed point iterations, _i.e.,_ repeatedly apply the update \(z^{[t+1]}=f_{}(z^{[t]};x)\) some fixed number of times, and backpropagate through the network to compute gradients. However, this can be computationally expensive. Deep equilibrium (DEQ) models (Bai et al., 2019) explicitly solve for \(z^{}\) through iterative root finding methods like Broyden's method (Broyden, 1965), Newton's method, Anderson acceleration (Anderson, 1965). DEQs use implicit function theorem to directly differentiate through the fixed point \(z^{}\) at equilibrium, thus requiring constant memory to backpropagate through an infinite-depth network:

\[}{}=(I-(z^ {};x)}{ z^{}})^{-1}(z^{ };x)}{}\] (6)

Computing the inverse of Jacobian can quickly become intractable as we deal with high-dimensional feature maps. One can replace the inverse-Jacobian term with an identity matrix _i.e.,_ Jacobian-free (Fung et al., 2022) or an approximate inverse-Jacobian (Geng et al., 2021) without affecting the final performance. There are alternate formulations of DEQs (Winston and Kolter, 2020) that guarantee existence of a unique equilibrium point. However, designing \(f_{}\) for these formulations can be challenging, and in this work we use the formulation by Bai et al. (2019).

## 4 Problem setting

We first formally define the system of steady-state PDEs that we will solve for:

**Definition 2** (Steady-State PDE).: _Given a bounded open set \(^{d}\), a steady-state PDE can be written in the following general form:_

\[L(a(x),u(x))=f(x), x\] (7)

_Here \(L\) is a continuous operator, the function \(u L^{2}(;^{d_{u}})\) is the unknown function that we wish to solve for and \(a L^{2}(;^{d_{u}})\) collects all the coefficients describing the PDE, and \(f L^{2}(;^{d_{f}})\) is a function independent of \(u\). We will, for concreteness, assume periodic boundary conditions, i.e. \( z^{d}, x\) we have \(u(x+z)=u(x)\). (Equivalently, \(:=^{d}=[0,2]^{d}\) can be identified with the torus.) 4 Finally, we will denote \(u^{}:\) as the solution to the PDE._

Steady-state models a system at stationarity, _i.e.,_ when some quantity of interest like temperature or velocity no longer changes over time. Classical numerical solvers for these PDEs include iterative methods like Newton updates or conjugate gradient descent, typically with carefully chosen preconditioning to ensure benign conditioning and fast convergence. Furthermore, recent theoretical works (Marwah et al., 2021; Chen et al., 2021; Marwah et al., 2022) show that for many families of PDEs (e.g., steady-state elliptic PDEs that admit a variational formulation), iterative algorithms can be efficiently "neuralized", that is, the iterative algorithm can be represented by a compact neural network, so long as the coefficients of the PDE are also representable by a compact neural network. Moreover, the architectures constructed in these works are heavily weight-tied.

We will operationalize these developments through the additional observation that all these iterative schemes can be viewed as algorithms to find a fixed point of a suitably chosen operator. Namely, we can design an operator \(:L^{2}(;^{d_{u}}) L^{2}(;^{d_ {f}}) L^{2}(;^{d_{u}})\)5 such that \(u^{}=(u^{},f)\) and a lot of common (preconditioned) first and second-order methods are natural ways to recover the fixed points \(u^{}\).

Before describing our architectures, we introduce two components that we will repeatedly use.

**Definition 3** (Projection and embedding layers).: _A projection and embedding layer, respectively \(:L^{2}(;^{d_{u}}) L^{2}(;^{d _{f}}) L^{2}(^{d_{v}};^{d_{v}}) L^{2}( ^{d_{v}};^{d_{v}})\) and \(:L^{2}(^{d_{v}};^{d_{v}}) L^{2}(^{ d_{v}};^{d_{u}})\), are defined as_

\[(v,f) =((W_{P}^{(1)}v+b_{P}^{(1)}),(W_ {P}^{(2)}f+b_{P}^{(2)})),\] \[(v) =(W_{Q}v+b_{Q})\]

_where \(W_{P}^{(1)}^{d_{u} d_{v}},W_{P}^{(2)}^{d_{f}  d_{v}},W_{Q}^{d_{v} d_{u}}\) and \(b_{P}^{(1)},b_{P}^{(2)}^{d_{v}},b_{Q}^{d_{u}}\)._

**Definition 4** (Input-injected FNO layer).: _An input-injected FNO layer \(:L^{2}(^{d_{v}};^{d_{v}}) L^{2}(^{d_{v}};^{d_{v}}) L^{2}(^{d_{v}};^{d_{v}})\) is defined as_

\[(v,g):=g+(Wv+b+^{-1}(R^{(k)}( v)).\] (8)

_where \(W^{d_{v} d_{v}}\), \(b^{d_{v}}\) and \(R^{(k)}^{d_{v} d_{v}}\) for all \(k[K]\) are learnable parameters._

Note the difference between the FNO layer specified above, and the standard FNO layer Equation 3 is the extra input \(g\) to the layer, which in our architecture will correspond to a projection of (some or all) of the PDE coefficients. We also note that this change to the FNO layer also enables us to learn deeper FNO architectures, as shown in Section 5. With this in mind, we can discuss the architectures we propose.

Weight-tied architecture I: Weight-tied FNOThe first architecture we consider is a weighted version of FNO (introduced in Section 3), in which we repeatedly apply (\(M\) times) the same transformation in each layer. More precisely, we have:

**Definition 5** (FNO Weight-Tied).: _We define a \(M\) times weight-tied neural operator \(G_{}^{M}\) as,_

\[G_{}^{M}=^{L}^{L} ^{L}}_{}\]such that: \(,\) are projection and embedding layers as in Definition 3 an \(^{L}:L^{2}(^{d_{v}};^{d_{v}}) L^{2}(^{d_{v}};^{d_{v}}) L^{2}(^{d_{v}};^{d_{v}})\) is an \(L\)-layer FNO block, i.e, \(^{L}=_{L}_{L-1}_{L-2} _{1}\) where for all \(l[L]\), \(_{l}(,(f))\)6 is an input-injected FNO block as in Definition 4._

Weight-tied architecture II: Fno-deqIn cases where we believe a weight-tied \(G_{}^{M}\) converges to some fixed point as \(M\), unrolling \(G_{}^{M}\) for a large \(M\) requires a lot of hardware memory for training: training the model requires one to store intermediate hidden units for each weight-tied layer for backpropagation, resulting in a \((M)\) increase in the amount of memory required.

To this end, we use Deep Equilibrium models (DEQs) which enables us to implicitly train \(G_{}:=_{M}G_{}^{M}\) by directly solving for the fixed point by leveraging black-box root finding algorithms like quasi-Newton methods, . Therefore we can think of this approach as an infinite-depth (or infinitely unrolled) weight-tied network. We refer to this architecture as **FNO-DEQ**.

**Definition 6** (Fno-deq).: _Given \(,\) and \(^{L}\) in Definition 5, FNO-DEQ is trained to parametrize the fixed point equation \(^{L}\)\((v^{},(f))=v^{}\) and outputs \(u^{}=(v^{})\)._

Usually, it is non-trivial to differentiate through these black-box root solvers. DEQs enable us to implicitly differentiate through the equilibrium fixed point efficiently without any need to backpropagate through these root solvers, therefore resulting in \((1)\) training memory.

## 5 Experiments

**Network architectures.** We consider the following network architectures in our experiments.

**FNO**: We closely follow the architecture proposed by Li et al.  and construct this network by stacking four FNO layers and four convolutional layers, separated by GELU activation . Note that in our current set up, we recover the original FNO architecture if the input to the \(l^{}\) layer is the output of \((l-1)^{}\) layer _i.e.,_\(v_{l}=_{l-1}(v_{l-1})\).

**Improved FNO (FNO++)**: The original FNO architecture suffers from vanishing gradients, which prohibits it from being made deeper . We overcome this limitation by introducing residual connections within each block of FNO, with each FNO block \(_{l}\) comprising of three FNO layers \(\)_i.e.,_\(_{l}=_{L_{1}}^{l}_{L_{2}}^{l} _{L_{3}}^{l}\) and three convolutional layers, where \(\) is defined in Eq. (8).

**Weight-tied network (FNO-WT)**: This is the weight-tied architecture introduced in Definition 5, where we initialize \(v_{0}(x)=0\) for all \(x\).

**FNO-DEQ**: As introduced in Definition 6, FNO-DEQ is a weight-tied network where we explicitly solve for the fixed point in the forward pass with a root finding algorithm. We use Anderson acceleration  as the root solver. For the backward pass, we use approximate implicit gradients  which are light-weight and more stable in practice, compared to implicit gradients computed by inverting Jacobian.

Note that both weight-tied networks and FNO-DEQs leverage weight-tying but the two models differ in the ultimate goal of the forward pass: DEQs explicitly solve for the fixed point during the forward pass, while weight-tied networks trained with backpropagation may or may-not reach a fixed point . Furthermore, DEQs require \((1)\) memory, as they differentiate through the fixed point implicitly, whereas weight-tied networks need to explicitly create the entire computation graph for backpropagation, which can become very large as the network depth increases. Additionally, FNO++ serves as a non weight-tied counterpart to a weight-tied input-injected network. Like weighted networks, FNO++ does not aim to solve for a fixed point in the forward pass.

**Experimental setup.** We test the aforementioned network architectures on two families of steady-state PDEs: Darcy Flow equation and steady-state Navier-Stokes equation for incompressible fluids. For experiments with Darcy Flow, we use the dataset provided by Li et al. , and generate our own dataset for steady-state Navier-Stokes. For more details on the datasets and the data generation processes we refer to Sections B.1 and B.2 of the Appendix. For each family of PDE, we train networks under 3 different training setups: clean data, noisy inputs and noisy observations. For experiments with noisy data, both input and observations, we add noise sampled from a sequence of standard Gaussians with increasing values of variance \(\{(0,(_{k}^{2}))\}_{k=0}^{M-1}\), where \(M\) is the total number of Gaussians we sample from. We set \(_{0}^{2}=0\) and \(_{}^{2}=_{M-1}^{2} 1/r\), where \(r\) is the resolution of the grid. Thus, the training data includes equal number of PDEs with different levels of Gaussian noise added to their input or observations. We add noise to training data, and always test on clean data. We follow prior work  and report relative \(L_{2}\) norm between ground truth \(u^{}\) and prediction on test data. The total depth of all networks besides FNO is given by \(6B+4\), where \(B\) is the number of FNO blocks. Each FNO block has 3 FNO layers and convolutional layers. In addition, we include the depth due to \(\), \(\), and an additional final FNO layer and a convolutional layer. We further elaborate upon network architectures and loss functions in in Appendix A.

### Darcy Flow

For our first set of experiments we consider stationary Darcy Flow equations, a form of linear elliptic PDE with in two dimensions. The PDE is defined as follows,

\[-(a(x) u(x))&=f(x),  x(0,1)^{2}\\ u(x)&=0& x(0,1)^{2}. \] (9)

Note that the diffusion coefficient \(a L^{}()(;_{+})\), i.e., the coefficients are always positive, and \(f L^{2}(;^{d_{f}})\) is the forcing term. These PDEs are used to model the steady-state pressure of fluids flowing through a porous media. They can also be used to model the stationary state of the diffusive process with \(u(x)\) modeling the temperature distribution through the space with \(a\) defining the thermal conductivity of the medium. The task is to learn an operator \(G_{}:L^{2}(;^{d_{u}}) L^{2}(;^{d_ {a}}) L^{2}(;^{d_{u}})\) such that \(u^{}=G_{}(u^{},a)\).

We report the results of our experiments on Darcy Flow in Table 1. The original FNO architecture does not improve its performance with increased number of FNO blocks \(\). FNO++ with residual connections scales better but saturates at around 4 FNO blocks. In contrast, FNO-WT and FNO-DEQ with just a _single_ FNO block outperform deeper non-weight-tied architectures on clean data and on data with noisy inputs. When observations are noisy, FNO-WT and FNO-DEQ outperform FNO++ with a similar number of parameters, and perform comparably to FNO++ with \(4\) parameters.

We also report results on shallow FNO-DEQ, FNO-WT and FNO++ architectures. An FNO block in these shallow networks has a single FNO layer instead of three layers. In our experiments, shallow weight-tied networks outperform non-weight-tied architectures including FNO++ with \(7\) parameters on clean data and on data with noisy inputs, and perform comparably to deep FNO++ on noisy observations. In case of noisy observations, we encounter training instability issues in FNO-DEQ. We believe that this shallow network lacks sufficient representation power and cannot accurately solve for the fixed point during the forward pass. These errors in fixed point estimation accumulate over time, leading to incorrect values of implicit gradients, which in turn result in training instability issues.

### Steady-state Navier-Stokes Equations for Incompressible Flow

We consider the steady-state Navier-Stokes equation for an incompressible viscous fluid in the vorticity form defined on a torus, i.e., with periodic boundary condition,

\[ u&=+ f, x\\  u&=0& x\] (10)

where \(:=(0,2)^{2}\), and \(u:^{2}\) is the velocity and \(:\) where \(= u\), \(_{+}\) is the viscosity and \(f:\) is the external force term. We learn an operator \(G_{}:L^{2}(;^{d_{u}}) L^{2}(;^{d_ {f}}) L^{2}(;^{d_{u}})\), such that \(u^{}=G_{}(u^{},f)\). We train all the models on data with viscosity values \(=0.01\) and \(=0.001\), and create a dataset for steady-state incompressible Navier-Stokes, which we will make public as a community benchmark for steady-state PDE solvers.

Results for Navier-Stokes equation have been reported in Table 2 and Table 3. For both values of viscosity, FNO-DEQ outperforms other architectures for all three cases: clean data, noisy inputs and noisy observations. FNO-DEQ is more robust to noisy inputs compared to non-weight-tied architectures. For noisy inputs, FNO-DEQ matches the test-error of noiseless case in case of viscosity \(0.01\) and almost matches the test-error of noiseless case in case of viscosity \(0.001\). We provide additional results for noise level \(0.004\) in Appendix E. FNO-DEQ and FNO-WT consistently outperform non-weight-tied architectures for higher levels of noise as well.

In general, DEQ-based architectures are slower to train (upto \(\)2.5\(\) compared to feedforward networks of similar size) as we solve for the fixed point in the forward pass. However, their inductive bias provides performance gains that cannot be achieved by simply stacking non-weight-tied FNO layers. In general, we observe diminishing returns in FNO++ beyond 4 blocks. Additionally, training the original FNO network on more than 4 FNO blocks is challenging, with the network often diverging during training, and therefore we do not include these results in the paper.

## 6 Universal Approximation and Fast Convergence of FNO-DEQ

Though the primary contribution of our paper is empirical, we show (by fairly standard techniques) that FNO-DEQ is a universal approximator, under mild conditions on the steady-state PDEs. Moreover, we also show that in some cases, we can hope the fixed-point solver can converge rapidly.

As noted in Definition 2, we have \(:=^{d}\). We note that all continuous function \(f L^{2}(;)\) and \(_{}|f(x)|dx<\) can be written as, \(f(x)=_{^{d}}e^{ix^{T}}_{w}\). where \(\{_{}\}_{^{d}}\) are the Fourier coefficients of the function \(f\). We define as \(L^{2}_{N}()\) as the space of functions such that for all \(f_{N} L^{2}_{N}()\) with Fourier coefficients that vanish outside a bounded ball. Finally, we define an

    &  &  &  \\   & & & \(^{2}_{}=0\) & \((^{2}_{})^{i}=0.001\) & \((^{2}_{})^{t}=0.001\) \\  FNO & 2.37M & 1 & 0.0080 \(\) 5e-4 & 0.0079 \(\) 2e-4 & 0.0125 \(\) 4e-5 \\ FNO & 4.15M & 2 & 0.0105 \(\) 6e-4 & 0.0106 \(\) 4e-4 & 0.0136 \(\) 2e-5 \\ FNO & 7.71M & 4 & 0.2550 \(\) 2e-8 & 0.2557 \(\) 8e-9 & 0.2617 \(\) 2e-9 \\  FNO++ & 2.37M & 1 & 0.0075 \(\) 2e-4 & 0.0075 \(\) 2e-4 & 0.0145 \(\) 7e-4 \\ FNO++ & 4.15M & 2 & 0.0065 \(\) 2e-4 & 0.0065 \(\) 9e-5 & 0.0117 \(\) 5e-5 \\ FNO++ & 7.71M & 4 & 0.0064 \(\) 2e-4 & 0.0064 \(\) 2e-4 & **0.0109 \(\) 5e-4** \\ S-FNO++ & 1.78M & 0.66 & 0.0093 \(\) 5e-4 & 0.0094 \(\) 7e-4 & 0.0402 \(\) 6e-3 \\  FNO-WT & 2.37M & 1 & **0.0055 \(\) 1e-4** & **0.0056 \(\) 5e-5** & 0.0112 \(\) 4e-4 \\ FNO-DEQ & 2.37M & 1 & **0.0055 \(\) 1e-4** & **0.0056 \(\) 7e-5** & 0.0112 \(\) 4e-4 \\  S-FNO-WT & 1.19M & 0.33 & 0.0057 \(\) 3e-5 & 0.0057 \(\) 5e-5 & 0.0112 \(\) 1e-4 \\ S-FNO-DEQ & 1.19M & 0.33 & 0.0056 \(\) 4e-5 & 0.0056 \(\) 5e-5 & 0.0136 \(\) 0.011 \\   

Table 1: Results on Darcy flow: clean data (Col 4), noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being \((^{2}_{})^{i}\) and \((^{2}_{})^{t}\), respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2. Here, S-FNO++, S-FNO-WT and S-FNO-DEQ are shallow versions of FNO++, FNO-WT and FNO-DEQ respectively.

    &  &  &  \\   & & & \(^{2}_{}=0\) & \((^{2}_{})^{i}=0.001\) & \((^{2}_{})^{t}=0.001\) \\  FNO & 2.37M & 1 & 0.184 \(\) 0.002 & 0.218 \(\) 0.003 & 0.184 \(\) 0.001 \\ FNO & 4.15M & 2 & 0.162 \(\) 0.024 & 0.176 \(\) 0.004 & 0.152 \(\) 0.005 \\ FNO & 7.71M & 4 & 0.157 \(\) 0.012 & 0.187 \(\) 0.004 & 0.166 \(\) 0.013 \\  FNO++ & 2.37M & 1 & 0.199 \(\) 0.001 & 0.230 \(\) 0.001 & 0.197 \(\) 0.001 \\ FNO++ & 4.15M & 2 & 0.154 \(\) 0.005 & 0.173 \(\) 0.003 & 0.154 \(\) 0.006 \\ FNO++ & 7.71M & 4 & 0.151 \(\) 0.003 & 0.165 \(\) 0.004 & 0.149 \(\) 0.003 \\  FNO-WT & 2.37M & 1 & 0.151 \(\) 0.007 & 0.173 \(\) 0.017 & **0.126 \(\) 0.027** \\ FNO-DEQ & 2.37M & 1 & **0.128 \(\) 0.004** & **0.144 \(\) 0.007** & 0.136 \(\) 0.003 \\   

Table 2: Results on incompressible steady-state Navier-Stokes (viscosity=0.001): clean data (Col 4), noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being \((^{2}_{})^{i}\) and \((^{2}_{})^{t}\), respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2.

orthogonal projection operator \(_{N}:L^{2}() L^{2}_{N}()\), such that for all \(f L^{2}()\) we have,

\[f_{n}=_{N}(f)=_{N}(_{^{d}}f_{}e^{ix^{T} })=_{\|\|_{} N}_{}e^{ ix^{T}}.\] (11)

That is, the projection operator \(_{N}\) takes an infinite dimensional function and projects it to a finite dimensional space. We prove the following universal approximation result:

**Theorem 1**.: _Let \(u^{} L^{2}(;^{d_{u}})\) define the solution to a steady-state PDE in Definition 2, Then there exists an operator \(:L^{2}(;^{d_{u}}) L^{2}(;^{d _{f}}) L^{2}(;^{d_{u}})\) such that, \(u^{}=(u^{},f)\). Furthermore, for every \(>0\) there exists an \(N\) such that for compact sets \(K_{u} L^{2}(;^{d_{u}})\) and \(K_{f} L^{2}(;^{d_{f}})\) there exists a neural network \(G_{}:L^{2}_{N}(;^{d_{u}}) L^{2}_{N}(; ^{d_{f}}) L^{2}_{N}(;^{d_{u}})\) with parameters \(\), such that,_

\[_{u K_{u},f K_{f}}\|u^{}-G_{}(_{N}u^{},_{N}f) \|_{L^{2}()}.\]

The proof for the above theorem is relatively straightforward and provided in Appendix C. The proof uses the fact that \(u^{}\) is a fixed-point of the operator \(G(u,f)=u-(L(u)-f)\), allowing us to use the the results in Kovachki et al. (2021) that show a continuous operator can be approximated by a network as defined in Equation 2. Note that the choice of \(G\) is by no means unique: one can "universally approximate" any operator \(G(u,f)=u-A(L(u)-f)\), for a continuous operator \(A\). Such a \(G\) can be thought of as a form of "preconditioned" gradient descent, for a preconditioner \(A\). For example, a Newton update has the form \(G(u,f)=u-L^{}(u)^{-1}(L(u)-f)\), where \(L^{}:L^{2}(;^{d_{u}}) L^{2}(;^{d_{u}})\) is the Frechet derivative of the operator \(L\).

The reason this is relevant is that the DEQ can choose to universally approximate a fixed-point equation for which the fixed-point solver it is trained with also converges rapidly. As an example, the following classical result shows that under Lax-Milgram-like conditions (a kind of strong convexity condition), Newton's method converges doubly exponentially fast:

**Lemma 1**(Farago and Karatson (2002), Chapter 5).: _Consider the PDE defined Definition 2, such that \(d_{u}=d_{v}=d_{f}=1\). such that \(L^{}(u)\) defines the Frechet derivative of the operator \(L\). If for all \(u,v L^{2}(;)\) we have \(\|L^{}(u)v\|_{L^{2}()}\|v\|_{L^{2}()}\) and \(\|L^{}(u)-L^{}(v)\|_{L^{2}()}\|u-v\|_{L^{2}( )}\) for \(0<<\), then for the Newton update, \(u_{t+1} u_{t}-L^{}(u_{t})^{-1}(L(u_{t})-f),\) with \(u_{0} L^{2}(;)\), there exists an \(>0\), such that \(\|u_{T}-u^{}\|_{L^{2}()}\) if \(T(()/(}{\|L(u_{0})-f\|_{L^{2}()}})).\)_

For completeness, we include the proof of the above lemma in the Appendix (Section D). We note that the conditions of the above lemma are satisfied for elliptic PDEs like Darcy Flow, as well as many variational non-linear elliptic PDEs (e.g., those considered in Marwah et al. (2022)). Hence, we can expect FNO-DEQs to quickly converge to the fixed point, since they employ quasi-Newton methods like Broyden and Anderson methods (Broyden, 1965; Anderson, 1965).

    &  &  &  \\   & & & \(^{2}_{}=0\) & \((^{2}_{})^{i}=0.001\) & \((^{2}_{})^{t}=0.001\) \\  FNO & 2.37M & 1 & 0.181 \(\) 0.005 & 0.186 \(\) 0.003 & 0.178 \(\) 0.006 \\ FNO & 4.15M & 2 & 0.138 \(\) 0.007 & 0.150 \(\) 0.006 & 0.137 \(\) 0.012 \\ FNO & 7.71M & 4 & 0.152 \(\) 0.006 & 0.163 \(\) 0.002 & 0.151 \(\) 0.008 \\  FNO++ & 2.37M & 1 & 0.188 \(\) 0.002 & 0.207 \(\) 0.004 & 0.187 \(\) 0.003 \\ FNO++ & 4.15M & 2 & 0.139 \(\) 0.004 & 0.153 \(\) 0.002 & 0.140 \(\) 0.005 \\ FNO++ & 7.71M & 4 & 0.130 \(\) 0.005 & 0.151 \(\) 0.004 & 0.128 \(\) 0.009 \\  FNO-WT & 2.37M & 1 & 0.099 \(\) 0.007 & 0.101 \(\) 0.007 & 0.130 \(\) 0.044 \\ FNO-DEQ & 2.37M & 1 & **0.088 \(\) 0.006** & **0.099 \(\) 0.007** & **0.116 \(\) 0.011** \\   

Table 3: Results on incompressible steady-state Navier-Stokes (viscosity=0.01): clean data (Col 4), noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being \((^{2}_{})^{i}\) and \((^{2}_{})^{t}\), respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2.

Conclusion

In this work, we demonstrate that the inductive bias of deep equilibrium models--and weight-tied networks in general--makes them ideal architectures for approximating neural operators for steady-state PDEs. Our experiments on steady-state Navier-Stokes equation and Darcy flow equations show that weight-tied models and FNO-DEQ perform outperform FNO models with \( 4\) the number of parameters and depth. Our findings indicate that FNO-DEQ and weight-tied architectures are, in general, more robust to both input and observation noise compared to non-weight-tied architectures, including FNO. We believe that our results complement any future progress in the design and development of PDE solvers (Tran et al., 2021; Li et al., 2022b) for steady-state PDEs, and hope that our work motivates the study of relevant inductive biases that could be used to improve them.

## 8 Acknowledgements

TM is supported by CMU Software Engineering Institute via Department of Defense under contract FA8702-15-D-0002. AP is supported by a grant from the Bosch Center for Artificial Intelligence. ZL gratefully acknowledges the NSF (FAI 2040929 and IIS2211955), UPMC, Highmark Health, Abridge, Ford Research, Mozilla, the PwC Center, Amazon AI, JP Morgan Chase, the Block Center, the Center for Machine Learning and Health, and the CMU Software Engineering Institute (SEI) via Department of Defense contract FA8702-15-D-0002, for their generous support of ACMI Lab's research. JL is supported in part by NSF award DMS-2012286, and AR is supported in part by NSF awards IIS-2211907, CCF-2238523, Amazon Research Award, and the CMU/PwC DT&I Center.