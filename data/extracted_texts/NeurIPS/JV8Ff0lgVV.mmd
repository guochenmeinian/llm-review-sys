# DIFUSCO: Graph-based Diffusion Solvers

for Combinatorial Optimization

 Zhiqing Sun

Language Technologies Institute

Carnegie Mellon University

zhiqings@cs.cmu.edu

&Yiming Yang

Language Technologies Institute

Carnegie Mellon University

yiming@cs.cmu.edu

Our code is available at https://github.com/Edward-Sun/DIFUSCO.

###### Abstract

Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete \(\{0,1\}\)-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to **0.46%** on TSP-500, from 2.46% to **1.17%** on TSP-1000, and from 3.19% to **2.58%** on TSP-10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark.

## 1 Introduction

Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand significant expert efforts to approximate near-optimal solutions .

Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution . Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems . Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical . Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems , for example, when multiple optimal solutions exists for the same graph. Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt  and node swap . These methodshave also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework [113; 79].

Motivated by the recent remarkable success of diffusion models in probabilistic generation [102; 40; 94; 120; 96], we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFUision Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as to find a \(\{0,1\}\)-valued vector that indicates the optimal selection of nodes or edges in a candidate solution for the task. Then we use a message passing-based graph neural network [61; 36; 29; 107] to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\( N\)) denoising steps (Sec. 3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec. 3.2), which solves the training scalability issue of RL-based improvement heuristics methods.

We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular, Graikos et al.  proposed an image-based diffusion model to solve Euclidean Traveling Salesman problems by projecting each TSP instance onto a \(64 64\) greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image. The main difference between such _image-based_ diffusion solver and our _graph-based_ diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive  and improvement heuristics  solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge.

We investigate two types of probabilistic diffusion modeling within the DIFUSCO framework: continuous diffusion with Gaussian noise  and discrete diffusion with Bernoulli noise [5; 44]. These two types of diffusion models have been applied to image processing but not to NPC problems so far. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers.

Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network [9; 54], can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.

## 2 Related Work

### Autoregressive Construction Heuristics Solvers

Autoregressive models have achieved state-of-the-art results as constructive heuristic solvers for combinatorial optimization (CO) problems, following the recent success of language modeling in the text generation domain [106; 11]. The first approach proposed by Bello et al.  uses a neural network with reinforcement learning to append one new variable to the partial solution at each decoding step until a complete solution is generated. However, autoregressive models  face high time and space complexity challenges for large-scale NPC problems due to their sequential generation scheme and quadratic complexity in the self-attention mechanism .

### Non-autoregressive Construction Heuristics Solvers

Non-autoregressive (or heatmap) constructive heuristics solvers [53; 27; 28; 92] are recently proposed to address this scalability issue by assuming conditional independence among variables in NPC problems, but this assumption limits the ability to capture the multimodal nature [57; 33] of high quality solution distributions. Therefore, additional active search [6; 92] or Monte-Carlo Tree Search (MCTS) [27; 98] are needed to further improve the expressive power of the non-autoregressive scheme.

DIFUSCO can be regarded as a member in the non-autoregressive constructive heuristics category and thus can be benefited from heatmap search techniques such as MCTS. But DIFUSCO uses an iterative denoising scheme to generate the final heatmap, which significantly enhances its expressive power compared to previous non-autoregressive methods.

### Diffusion Models for Discrete Data

Typical diffusion models [100; 103; 40; 103; 85; 56] operate in the continuous domain, progressively adding Gaussian noise to the clean data in the forward process, and learning to remove noises in the reverse process in a discrete-time framework.

Discrete diffusion models have been proposed for the generation of discrete image bits or texts using binomial noises  and multinomial/categorical noises [5; 44]. Recent research has also shown the potential of discrete diffusion models in sound generation , protein structure generation , molecule generation , and better text generation [52; 38].

Another line of work studies diffusion models for discrete data by applying continuous diffusion models with Gaussian noise on the embedding space of discrete data [30; 69; 24], the \(\{-1.0,1.0\}\) real-number vector space , and the simplex space . The most relevant work might be Niu et al. , which proposed a continuous score-based generative framework for graphs, but they only evaluated simple non-NP-hard CO tasks such as Shortest Path and Maximum Spanning Tree.

## 3 DIFUSCO: Proposed Approach

### Problem Definition

Following a conventional notation , we define \(_{s}=\{0,1\}^{N}\) as the space of candidate solutions \(\{\}\) for a CO problem instance \(s\), and \(c_{s}:_{s}\) as the objective function for solution \(_{s}\):

\[c_{s}()=(,s)+(,s).\] (1)

Here \(()\) is the task-specific cost of a candidate solution (e.g., the tour length in TSP), which is often a simple linear function of \(\) in most NP-complete problems, and \(()\) is the validation term that returns \(0\) for a feasible solution and \(+\) for an invalid one. The optimization objective is to find the optimal solution \(^{**}\) for a given instance \(s\) as:

\[^{**}=*{argmin}_{_{s}}c_{s}( ).\] (2)

This framework is generically applicable to different NPC problems. For example, for the Traveling Salesman Problem (TSP), \(\{0,1\}^{N}\) is the indicator vector for selecting a subset from \(N\) edges; the cost of this subset is calculated as: \(_{}(,s)=_{i}x_{i} d_{i}^{(s)}\), where \(d_{i}^{(s)}\) denotes the weight of the \(i\)-th edge in problem instance \(s\), and the \(()\) part of Formula (1) ensures that \(\) is a tour that visits each node exactly once and returns to the starting node at the end. For the Maximal Independent Set (MIS) problem, \(\{0,1\}^{N}\) is the indicator vector for selecting a subset from \(N\) nodes; the cost of the subset is calculated as: \(_{}(,s)=_{i}(1-x_{i}),\), and the corresponding \(()\) validates \(\) is an independent set where each node in the set has no connection to any other node in the set.

Probabilistic neural NPC solvers  tackle instance problem \(s\) by defining a parameterized conditional distribution \(p_{}(|s)\), such that the expected cost \(_{_{s}}c_{s}() p(|s)\) is minimized. Such probabilistic generative models are usually optimized by reinforcement learning algorithms [111; 63]. In this paper, assuming the optimal (or high-quality) solution \(_{s}^{*}\) is available for each training instance \(s\), we optimize the model through supervised learning. Let \(=\{s_{i}\}_{1}^{N}\) be independently and identically distributed (IID) training samples for a type of NPC problem, we aim to maximize the likelihood of optimal (or high-quality) solutions, where the loss function \(L\) is defined as:

\[L()=_{s}[- p_{}(^{**}|s)]\] (3)

Next, we describe how to use diffusion models to parameterize the generative distribution \(p_{}\). For brevity, we omit the conditional notations of \(s\) and denote \(^{**}\) as \(_{0}\) as a convention in diffusion models for all formulas in the the rest of the paper.

### Diffusion Models in DIFUSCO

From the variational inference perspective , diffusion models [100; 40; 102] are latent variable models of the form \(p_{}(_{0}) p_{}(_{0:T})d _{1:T}\), where \(_{1},,_{T}\) are latents of the same dimensionality as the data \(_{0} q(_{0})\). The joint distribution \(p_{}(_{0:T})=p(_{T})_{t=1}^{T}p_{}(_{t-1}|_{t})\) is the learned reverse (denoising) process that gradually denoises the latent variables toward the data distribution, while the forward process \(q(_{1:T}|_{0})=_{t=1}^{T}q(_{t}|_ {t-1})\) gradually corrupts the data into noised latent variables. Training is performed by optimizing the usual variational bound on negative log-likelihood:

\[[- p_{}(_{0 })]&_{q}[-}( _{0:T})}{q_{}(_{1:T}|_{0})}] \\ &=_{q}[_{t>1}D_{KL}[q(_{t-1}|_{t},_{0})||p_{}(_{t-1}|_{t})]- p_{ }(_{0}|_{1})]+C\] (4)

where \(C\) is a constant.

Discrete DiffusionIn discrete diffusion models with multinomial noises [5; 44], the forward process is defined as: \(q(_{t}|_{t-1})=(_{t};= }_{t-1}_{t}),\) where \(_{t}=(1-_{t})&_{t}\\ _{t}&(1-_{t})\) is the transition probability matrix; \(}\{0,1\}^{N 2}\) is converted from the original vector \(\{0,1\}^{N}\) with a one-hot vector per row; and \(}\) computes a row-wise vector-matrix product.

Here, \(_{t}\) denotes the corruption ratio. Also, we want \(_{t=1}^{T}(1-_{t}) 0\) such that \(_{T}()\). The \(t\)-step marginal can thus be written as: \(q(_{t}|_{0})=(_{t};= }_{0}}_{t}),\) where \(}_{t}=_{1}_{2}_{t}\). And the posterior at time \(t-1\) can be obtained by Bayes' theorem:

\[q(_{t-1}|_{t},_{0})=_{t}| _{t-1},_{0})q(_{t-1}|_{0})}{q( _{t}|_{0})}=(_{t-1};= }_{t}_{t}^{}}_{0} }_{t-1}}{}_{0}}_{t} }_{t}^{}}),\] (5)

where \(\) denotes the element-wise multiplication.

According to Austin et al. , the denoising neural network is trained to predict the clean data \(p_{}(}_{0}|_{t})\), and the reverse process is obtained by substituting the predicted \(}_{0}\) as \(_{0}\) in Eq. 5:

\[p_{}(_{t-1}|_{t})=_{}}q( _{t-1}|_{t},}_{0})p_{}(}_{0}|_{t})\] (6)

Continuous Diffusion for Discrete DataThe continuous diffusion models [102; 40] can also be directly applied to discrete data by lifting the discrete input into a continuous space . Since the continuous diffusion models usually start from a standard Gaussian distribution \((,)\), Chen et al.  proposed to first rescale the \(\{0,1\}\)-valued variables \(_{0}\) to the \(\{-1,1\}\) domain as \(}_{0}\), and then treat them as real values. The forward process in continuous diffusion is defined as: \(q(}_{t}|}_{t-1})(}_{t};}}_{t-1},_{t})\).

Again, \(_{t}\) denotes the corruption ratio, and we want \(_{t=1}^{T}(1-_{t}) 0\) such that \(_{T}()\). The \(t\)-step marginal can thus be written as: \(q(}_{t}|}_{0})(}_{t};_{t}}}_{0},(1-_{t} ))\) where \(_{t}=1-_{t}\) and \(_{t}=_{=1}^{t}_{}\). Similar to Eq. 5, the posterior at time \(t-1\) can be obtained by Bayes' theorem:

\[q(}_{t-1}|}_{t},_{0})=}_{t}|}_{t-1},}_{0})q( }_{t-1}|}_{0})}{q(}_{t}|}_{0})},\] (7)

which is a closed-form Gaussian distribution . In continuous diffusion, the denoising neural network is trained to predict the unscaled Gaussian noise \(}_{t}=(}_{t}-_{t}}}_{0})/_{t}}=f_{}(}_{t},t)\). The reverse process  can use a point estimation of \(}_{0}\) in the posterior:

\[p_{}(}_{t-1}|}_{t})=q(}_{t-1}|}_{t},}_{t}-_{t}}f_{}(}_{t},t)}{_{t}}})\] (8)

For generating discrete data, after the continuous data \(}_{0}\) is generated, a thresholding/quantization operation is applied to convert them back to \(\{0,1\}\)-valued variables \(_{0}\) as the model's prediction.

### Denoising Schedule for Fast Inference

One way to speed up the inference of denoising diffusion models is to reduce the number of steps in the reverse diffusion process, which also reduces the number of neural network evaluations. The denoising diffusion implicit models (DDIMs)  are a class of models that apply this strategy in the continuous domain, and a similar approach can be used for discrete diffusion models .

Formally, when the forward process is defined not on all the latent variables \(_{1:T}\), but on a subset \(\{_{_{1}},,_{_{M}}\}\), where \(\) is an increasing sub-sequence of \([1,,T]\) with length \(M\), \(_{_{1}}=1\) and \(_{_{M}}=T\), the fast sampling algorithms directly models \(q(_{_{1-1}}|_{_{i}},_{0})\). Due to the space limit, the detailed algorithms are described in the appendix.

We consider two types of denoising scheduled for \(\) given the desired \(()<T\): linear and cosine. The former uses timesteps such that \(_{i}= ci\) for some \(c\), and the latter uses timesteps such that \(_{i}=() T\) for some \(c\). The intuition for the cosine schedule is that diffusion models can achieve better generation quality when iterating more steps in the low-noise regime [85; 121; 13].

### Graph-based Denoising Network

The denoising network takes as input a set of noisy variables \(_{t}\) and the problem instance \(s\) and predicts the clean data \(}_{0}\). To balance both scalability and performance considerations, we adopt an anisotropic graph neural network with edge gating mechanisms [9; 54] as the backbone network for both discrete and continuous diffusion models, and the variables in the network output can be the states of either nodes, as in the case of Maximum Independent Set (MIS) problems, or edges, as in the case of Traveling Salesman Problems (TSP). Our choice of network mainly follows previous work [54; 92], as AGNN can produce the embeddings for both nodes and edges, unlike typical GNNs such as GCN  or GAT , which are designed for node embedding only. This design choice is particularly beneficial for tasks that require the prediction of edge variables.

Anisotropic Graph Neural NetworksLet \(_{i}^{}\) and \(_{ij}^{}\) denote the node and edge features at layer \(\) associated with node \(i\) and edge \(ij\), respectively. \(\) is the sinusoidal features  of denoising timestep \(t\). The features at the next layer is propagated with an anisotropic message passing scheme:

\[}_{ij}^{+1}=^{}_{ij}^ {}+^{}_{i}^{}+^{} _{j}^{},\]

\[_{ij}^{+1}=_{ij}^{}+_{ }((}_{ij}^{+1}))+ _{}(),\]

where \(^{},^{},^{}, {Q}^{},^{}^{d d}\) are the learnable parameters of layer \(\), \(\) denotes the \(\) activation, \(\) denotes the Batch Normalization operator , \(\) denotes the aggregation function \(\) pooling , \(\) is the sigmoid function, \(\) is the Hadamard product, \(_{i}\) denotes the neighborhoods of node \(i\), and \(_{()}\) denotes a 2-layer multi-layer perceptron.

For TSP, \(_{ij}^{0}\) are initialized as the corresponding values in \(_{t}\), and \(_{i}^{0}\) are initialized as sinusoidal features of the nodes. For MIS, \(_{ij}^{0}\) are initialized as zeros, and \(_{i}^{0}\) are initialized as the corresponding values in \(_{t}\). A 2-neuron and 1-neuron classification/regression head is applied to the final embeddings of \(_{t}\) (\(\{_{ij}\}\) for edges and \(\{_{i}\}\) for nodes) for discrete and continuous diffusion models, respectively.

Hyper-parametersFor all TSP and MIS benchmarks, we use a 12-layer Anisotropic GNN with a width of 256 as described above.

### Decoding Strategies for Diffusion-based Solvers

After the training of the parameterized denoising network according to Eq. 4, the solutions are sampled from the diffusion models \(p_{}(_{0}|s)\) for final evaluation. However, probabilistic generative models such as DIFUSCO cannot guarantee that the sampled solutions are feasible according to the definition of CO problems. Therefore, specialized decoding strategies are designed for the two CO problems studied in this paper.

Heatmap GenerationThe diffusion models \(p_{}(|s)\) produce discrete variables \(\) as the final predictions by applying Bernoulli sampling (Eq. 6) for discrete diffusion or quantization for continuous diffusion. However, this process discards the comparative information that reflects the confidence of the predicted variables, which is crucial for resolving conflicts in the decoding process. To preserve this information, we adapt the diffusion models to generate heatmaps [53; 92] by making the following appropriate modifications: 1) For discrete diffusion, the final score of \(p_{}(}=1|s)\) is preserved as the heatmap scores; 2) For continuous diffusion, we remove the final quantization and use \(0.5(}_{0}+1)\) as the heatmap scores. Note that different from previous heatmap approaches [53; 92] that produce a single conditionally independent distribution for all variables, DIFUSCO can produce diverse multimodal output distribution by using different random seeds.

TSP DecodingLet \(\{A_{ij}\}\) be the heatmap scores generated by DIFUSCO denoting the confidence of each edge. We evaluate two approaches as the decoding method following previous work [92; 32]: 1) Greedy decoding , where all the edges are ranked by \((A_{ij}+A_{ji})/\|_{i}-_{j}\|\), and are inserted into the partial solution if there are no conflicts. 2-opt heuristics  are optionally applied. 2) Monte Carlo Tree Search (MCTS) , where \(k\)-opt transformation actions are sampled guided by the heatmap scores to improve the current solutions. Due to the space limit, a detailed description of two decoding strategies can be found in the appendix.

MIS DecodingLet \(\{a_{i}\}\) be the heatmap scores generated by DIFUSCO denoting the confidence of each node. A greedy decoding strategy is used for the MIS problem, where the nodes are ranked by \(a_{i}\) and inserted into the partial solution if there are no conflicts. Recent research  pointed out that the graph reduction and 2-opt search  can find near-optimal solutions even starting from a randomly generated solution, so we do not use any post-processing for the greedy-decoded solutions.

Solution SamplingA common practice for probabilistic CO solvers  is to sample multiple solutions and report the best one. For DIFUSCO, we follow this practice by sampling multiple heatmaps from \(p_{}(_{0}|s)\) with different random seeds and then applying the greedy decoding algorithm described above to each heatmap.

## 4 Experiments with TSP

We use 2-D Euclidean TSP instances to test our models. We generate these instances by randomly sampling nodes from a uniform distribution over the unit square. We use TSP-50 (with 50 nodes) as the main benchmark to compare different model configurations. We also evaluate our method on larger TSP instances with 100, 500, 1000, and 10000 nodes to demonstrate its scalability and performance against other state-of-the-art methods.

### Experimental Settings

DatasetsWe generate and label the training instances using the Concorde exact solver  for TSP-50/100 and the LKH-3 heuristic solver  for TSP-500/1000/10000. We use the same test instances as [54; 64] for TSP-50/100 and  for TSP-500/1000/10000.

Graph SparsificationWe use sparse graphs for large-scale TSP problems to reduce the computational complexity. We sparsify the graphs by limiting each node to have only \(k\) edges to its nearest neighbors based on the Euclidean distances. We set \(k\) to 50 for TSP-500 and 100 for TSP-1000/10000. This way, we avoid the quadratic growth of edges in dense graphs as the number of nodes increases.

Model Settings\(T=1000\) denoising steps are used for the training of DIFUSCO on all datasets. Following Ho et al. , Graikos et al. , we use a simple linear noise schedule for \(\{_{t}\}_{t=1}^{T}\), where \(_{1}=10^{-4}\) and \(_{T}=0.02\). We follow Graikos et al.  and use the Greedy decoding + 2-opt scheme (Sec. 3.5) as the default decoding scheme for experiments.

Evaluation MetricsIn order to compare the performance of different models, we present three metrics: average tour length (Length), average relative performance gap (Gap), and total run time (Time). The detailed description can be found in the appendix.

### Design Analysis

Discrete Diffusion v.s. Continuous DiffusionWe first investigate the suitability of two diffusion approaches for combinatorial optimization, namely continuous diffusion with Gaussian noise and discrete diffusion with Bernoulli noise (Sec. 3.2). Additionally, we explore the effectiveness of different denoising schedules, such as linear and cosine schedules (Sec. 3.3), on CO problems. To efficiently evaluate these model choices, we utilize the TSP-50 benchmark.

Note that although all the diffusion models are trained with a \(T=1000\) noise schedule, the inference schedule can be shorter than \(T\), as described in Sec. 3.3. Specifically, we are interested in diffusion models with 1, 2, 5, 10, 20, 50, 100, and 200 diffusion steps.

Fig. 1 demonstrates the performance of two types of diffusion models with two types of inference schedules and various diffusion steps. We can see that discrete diffusion consistently outperforms the continuous diffusion models by a large margin when there are more than 5 diffusion steps2. Besides, the cosine schedule is superior to linear on discrete diffusion and performs similarly on continuous diffusion. Therefore, we use cosine for the rest of the paper.

More Diffusion Iterations v.s. More SamplingBy utilizing effective denoising schedules, diffusion models are able to adaptively infer based on the available computation budget by predetermining the total number of diffusion steps. This is similar to changing the number of samples in previous probabilistic neural NPC solvers . Therefore, we investigate the trade-off between the number of diffusion iterations and the number of samples for diffusion-based NPC solvers.

    &  &  &  \\  & & Length\({}^{}\) & Gap(\%) & Length\({}^{}\) & Gap(\%) \\   ^{*}\) 2-OPT} & Exact & 5.69 & 0.00 & 7.76 & 0.00 \\  & heuristics & 5.86 & 2.95 & 8.03 & 3.54 \\   & Greedy & 5.80 & 1.76 & 8.12 & 4.53 \\  & Greedy & 5.87 & 3.10 & 8.41 & 8.38 \\  & Greedy & 5.71 & 0.31 & 7.88 & 1.42 \\  & Greedy & 5.73 & 0.64 & 7.84 & 1.07 \\  & Greedy & 5.73 & 0.4 & 7.84 & 0.94 \\  & Greedy & 5.70 & 0.14 & 7.89 & 1.62 \\  & Greedy\({}^{}\) & 5.76 & 1.23 & 7.92 & 2.11 \\
**Ours** & Greedy\({}^{}\) & **5.70** & **0.10** & **7.78** & **0.24** \\   & 1k\(\)Sampling & 5.73 & 0.52 & 7.94 & 2.26 \\  & 2k\(\)Sampling & 5.70 & 0.01 & 7.87 & 1.39 \\  & Transformer & 2k\(\)Sampling & 5.69 & 0.00 & 7.76 & 0.39 \\  & POMO & 8\(\)Augment & 5.69 & 0.03 & 7.72 & 0.14 \\  & Syn-NCO & 10\(\)Sampling & - & - & 7.79 & 0.39 \\  & MDAM & 50\(\)Sampling & 5.70 & 0.03 & 7.79 & 0.38 \\  & DPDP & 100\(\)InflowStreams & 5.70 & 0.00 & 7.77 & 0.00 \\
**Ours** & 16\(\)Sampling & **5.69** & **-0.01** & **7.76** & **-0.01** \\   

Table 1: Comparing results on TSP-50 and TSP-100. \(*\) denotes the baseline for computing the performance gap. \({}^{}\) indicates that the diffusion model samples a single solution as its greedy decoding scheme. Please refer to Sec. 4 for details.

Figure 2: The performance Gap (\(\%\)) are shown for continuous diffusion **(a)** and discrete diffusion **(b)** models on TSP-50 with different diffusion steps and number of samples. Their corresponding per-instance run-time (sec) are shown in **(c)**, where the decomposed analysis (neural network + greedy decoding + 2-opt) can be found in the appendix.

Fig. 2 shows the results of continuous diffusion and discrete diffusion with various diffusion steps and number of parallel sampling, as well as their corresponding total run-time. The cosine denoising schedule is used for fast inference. Again, we find that discrete diffusion outperforms continuous diffusion across various settings. Besides, we find performing more diffusion iterations is generally more effective than sampling more solutions, even when the former uses less computation. For example, \(20\) (diffusion steps) \( 4\) (samples) performs competitive to \(1\) (diffusion steps) \( 1024\) (samples), while the runtime of the former is \(18.5\) less than the latter.

In general, we find that \(50\) (diffusion steps) \( 1\) (samples) policy and \(10\) (diffusion steps) \( 16\) (samples) policy make a good balance between exploration and exploitation for discrete DI-FUSCO models and use them as the **Greedy** and **Sampling** strategies for the rest of the experiments.

### Main Results

Comparison to SOTA MethodsWe compare discrete DIFUSCO to other state-of-the-art neural NPC solvers on TSP problems across various scales. Due to the space limit, the description of other baseline models can be found in the appendix.

Tab. 1 compare discrete DIFUSCO with other models on TSP-50 and TSP-100, where DIFUSCO achieves the state-of-the-art performance in both greedy and sampling settings for probabilistic solvers.

Tab. 2 compare discrete DIFUSCO with other models on the larger-scale TSP-500, TSP-1000, and TSP-10000 problems. Most previous probabilistic solvers (except DIMES ) becomes untrainable on TSP problems of these scales, so the results of these models are reported with TSP-100 trained models. The results are reported with greedy, sampling, and MCTS decoding strategies, respectively. For fair comparisons [27; 92], MCTS decoding for TSP is always evaluated with only one sampled heatmap. From the table, we can see that DIFUSCO strongly outperforms the previous neural solvers on all three settings. In particular, with MCTS-based decoding, DIFUSCO significantly improving the performance gap between ground-truth and neural solvers from 1.76% to **0.46%** on TSP-500, from 2.46% to **1.17%** on TSP-1000, and from 3.19% to **2.58%** on TSP-10000.

    &  &  &  &  \\  & & Length \(\) & Gap \(\) & Time \(\) & Length \(\) & Gap \(\) & Time \(\) & Length \(\) & Gap \(\) & Time \(\) \\  Concorde & Exact & 16.55\({}^{*}\) & — & 37.66m & 23.12\({}^{*}\) & — & 6.65h & N/A & N/A & N/A \\ Gurobi & Exact & 16.55 & 0.00\% & 45.63h & N/A & N/A & N/A & N/A & N/A \\ LKH-3 (default) & Heuristics & 16.55 & 0.00\% & 46.28m & 23.12 & 0.00\% & 2.57h & 71.77\({}^{*}\) & — & 8.8h \\ LKH-3 (less trails) & Heuristics & 16.55 & 0.00\% & 3.03m & 23.12 & 0.00\% & 7.73m & 71.79 & — & 51.27m \\ Farthest Insertion & Heuristics & 18.30 & 10.57\% & 0s & 25.72 & 11.25\% & 0s & 80.59 & 12.29\% & 6s \\  AM & RL+G & 20.02 & 20.99\% & 1.51m & 31.15 & 34.75\% & 3.18m & 141.68 & 97.39\% & 5.99m \\ GCN & SL+G & 29.72 & 79.61\% & 6.67m & 48.62 & 110.29\% & 82.52m & N/A & N/A & N/A \\ POMO+EAS-Emb & RL+AS+G & 19.24 & 16.25\% & 12.80h & N/A & N/A & N/A & N/A & N/A \\ POMO+EAS-Tab & RL+AS+G & 24.54 & 82.22\% & 11.61\% & 49.56 & 141.36\% & 63.45h & N/A & N/A \\ DIMES & RL+G & 18.93 & 14.38\% & 0.97m & 26.58 & 14.97\% & 2.08m & 86.44 & 20.44\% & 4.65m \\ DIMES & RL+AS+G & 17.81 & 7.61\% & 2.10h & 24.91 & 7.74\% & 4.49h & 80.45 & 12.09\% & 3.07h \\ Ours (DIFUSCO) & SL+G & 18.35 & 18.05\% & 3.61m & 26.14 & 13.06\% & 11.68m & 98.15 & 36.75\% & 28.51m \\ Ours (DIFUSCO) & SL+G+1@P & **16.80** & **14.99** & **3.65m** & **23.56** & **1.99** & **12.06m** & **73.99** & **3.10\%** & **35.38m \\  EAN & RL+S+2-0pt & 23.75 & 43.57\% & 57.67m & 47.73 & 106.46\% & 5.93h & N/A & N/A & N/A \\ AM & RL+BS & 19.53 & 18.03\% & 21.99m & 29.90 & 29.23\% & 1.64h & 129.40 & 80.28\% & 1.81h \\ GCN & SL+BS & 30.37 & 8.53\% & 38.02m & 51.26\% & 21.73\% & 51.67m & N/A & N/A & N/A \\ DIMES & RL+S & 18.84 & 13.84\% & 10.6m & 26.36 & 14.01\% & 2.38m & 87.55 & 19.48\% & 4.80m \\ DIMES & RL+AS+S & 17.80 & 7.55\% & 2.11h & 24.89 & 7.70\% & 4.53h & 80.42 & 12.05\% & 3.12h \\ Ours (DIFUSCO) & SL+S & 17.23 & 4.08\% & 11.02m & 25.19 & 8.95\% & 46.08m & 95.52 & 33.09\% & 6.59h \\ Ours (DIFUSCO) & SL+S+2-0pt & **16.65** & **0.57\%** & **11.46m** & **23.45** & **1.43\%** & **48.09m** & **73.89** & **2.95\%** & **6.72h** \\  Att-gCN & SL+MCTS & 16.97 & 2.54\% & 22.20 & 23.86 & 3.22\% & 4.10m & 74.93 & 4.39\% & 21.49m \\ DIMES & RL+MCTS & 16.87 & 1.93\% & 2.92m & 23.73 & 2.64\% & 6.87m & 74.63 & 3.98\% & 29.83m \\ DIMES & RL+AS+MCTS & 16.84 & 1.76\% & 2.15h & 23.69 & 2.46\% & 4.62h & 74.06 & 3.19\% & 3.57h \\ Ours (DIFUSCO) & SL+MCTS & **16.63** & **0.46\%** & **10.13m** & **23.39** & **1.17\%** & **24.47**m & **73.62** & **2.58\%** & **47.36m** \\   

Table 2: Results on large-scale TSP problems. RL, SL, AS, G, S, BS, and MCTS denotes Reinforcement Learning, Supervised Learning, Active Search, Greedy decoding, Sampling decoding, Beam-search, and Monte Carlo Tree Search, respectively. \({}^{*}\) indicates the baseline for computing the performance gap. Results of baselines are taken from Fu et al.  and Qiu et al. , so the runtime may not be directly comparable. See Section 4 and appendix for detailed descriptions.

Generalization TestsFinally, we study the generalization ability of discrete DIFUSCO trained on a set of TSP problems of a specific problem scale and evaluated on other problem scales. From Fig. 3, we can see that DIFUSCO has a strong generalization ability. In particular, the model trained with TSP-50 perform well on even TSP-1000 and TSP0-10000. This pattern is different from the bad generalization ability of RL-trained or SL-trained non-autoregressive methods as reported in previous work .

## 5 Experiments with MIS

For Maximal Independent Set (MIS), we experiment on two types of graphs that recent work  shows struggles against, i.e., SATLIB  and Erdos-Renyi (ER) graphs . The former is a set of graphs reduced from SAT instances in CNF, while the latter are random graphs. We use ER-[700-800] for evaluation, where ER-[\(n\)-\(N\)] indicates the graph contains \(n\) to \(N\) nodes. Following Qiu et al. , the pairwise connection probability \(p\) is set to \(0.15\).

DatasetsThe training instances of labeled by the KaMIS3 heuristic solver. The split of test instances on SAT datasets and the random-generated ER test graphs are taken from Qiu et al. .

Model SettingsThe training schedule is the same as the TSP solver (Sec. 4.1). For SATLIB, we use discrete diffusion with \(50\) (diffusion steps)\( 1\) (samples) policy and \(50\) (diffusion steps)\( 4\) (samples) policy as the **Greedy** and **Sampling** strategies, respectively. For ER graphs, we use continuous diffusion with \(50\) (diffusion steps) \( 1\) (samples) policy and \(20\) (diffusion steps) \( 8\) (samples) policy as the **Greedy** and **Sampling** strategies, respectively.

Evaluation MetricsWe report the average size of the independent set (Size), average optimality gap (Gap), and latency time (Time). The detailed description can be found in the appendix. Notice that we disable graph reduction and 2-opt local search in all models for a fair comparison since it is pointed out by  that all models would perform similarly with local search post-processing.

Results and AnalysisTab. 3 compare discrete DIFUSCO with other baselines on SATLIB and ER-[700-800] benchmarks. We can see that DIFUSCO strongly outperforms previous state-of-the-art methods on SATLIB benchmark, reducing the gap between ground-truth and neural solvers from 0.63% to **0.21%**. However, we also found that DIFUSCO (especially with discrete diffusion in our preliminary experiments) does not perform well on the ER-[700-800] data. We hypothesize that this is because the previous methods usually use the node-based graph neural networks such as GCN  or GraphSage  as the backbone network, while we use an edge-based Anisotropic GNN (Sec. 3.4), whose inductive bias may be not suitable for ER graphs.

    &  &  &  } \\  & & Size \(\) & GAP \(\) & Time \(\) & Size \(\) & GAP \(\) & Time \(\) \\  KaMIS & Heuristics & 425.96\({}^{*}\) & — & 37.58m & 44.87\({}^{*}\) & — & 52.13m \\ Gurobi & Exact & 425.95 & 0.00\% & 26.00m & 41.38 & 7.78\% & 50.00m \\  Intel & SL+G & 420.6 & 1.48\% & 23.05m & 34.86 & 22.23\% & 6.06m \\ Intel & SL+TS & N/A & N/A & N/A & 38.80 & 13.43\% & 20.00d \\ DGL & SL+TS & N/A & N/A & N/A & 37.26 & 16.96\% & 22.71m \\ LwD & RL+S & 422.22 & 0.88\% & 18.83m & 41.17 & 28.55\% & 6.33m \\ DIMES & RL+G & 421.24 & 1.11\% & 24.17m & 32.84 & 14.78\% & 6.12m \\ DIMES & RL+S & 423.28 & 0.63\% & 20.26m & **42.06** & **6.26\%** & 12.01m \\  Ours & SL+G & **424.50** & **0.34\%** & 8.76m & 38.83 & 12.40\% & 8.80m \\ Ours & SL+S & **425.13** & **0.21\%** & 23.74m & 41.12 & 8.36\% & 26.67m \\   

Table 3: Results on MIS problems. \({}^{*}\) indicates the baseline for computing the optimality gap. RL, SL, G, S, and TS denote Reinforcement Learning, Supervised Learning, Greedy decoding, Sampling decoding, and Tree Search, respectively. Please refer to Sec. 5 and appendix for details.

## 6 Concluding Remarks

We proposed DIFUSCO, a novel graph-based diffusion model for solving NP-complete combinatorial optimization problems. We compared two variants of graph-based diffusion models: one with continuous Gaussian noise and one with discrete Bernoulli noise. We found that the discrete variant performs better than the continuous one. Moreover, we designed a cosine inference schedule that enhances the effectiveness of our model. DIFUSCO achieves state-of-the-art results on TSP and MIS problems, surpassing previous probabilistic NPC solvers in both accuracy and scalability.

For future work, we would like to explore the potential of DIFUSCO in solving a broader range of NPC problems, including Mixed Integer Programming (discussed in the appendix). We would also like to explore the use of equivariant graph neural networks [117; 45] for further improvement of the diffusion models on geometrical NP-complete combinatorial optimization problems such as Euclidean TSP. Finally, we are interested in utilizing (higher-order) accelerated inference techniques for diffusion model-based solvers, such as those inspired by the continuous time framework for discrete diffusion [12; 105].