# TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control

Weichao Zeng\({}^{1,3}\) & Yan Shu\({}^{1}\) & Zhenhang Li\({}^{1,3}\)

**Dongbao Yang\({}^{1,3}\) & Yu Zhou\({}^{2}\)**

\({}^{1}\) Institute of Information Engineering, Chinese Academy of Sciences

\({}^{2}\) VCIP & TMCC & DISSec, College of Computer Science, Nankai University

\({}^{3}\) School of Cyber Security, University of Chinese Academy of Sciences

{zengweichao, lizhenhang, yangdongbao}@iie.ac.cn

shuyan9812@gamil.com, yzhou@nankai.edu.cn

Corresponding Author

###### Abstract

Centred on content modification and style preservation, Scene Text Editing (STE) remains a challenging task despite considerable progress in text-to-image synthesis and text-driven image manipulation recently. GAN-based STE methods generally encounter a common issue of model generalization, while Diffusion-based STE methods suffer from undesired style deviations. To address these problems, we propose **TextCtrl**, a diffusion-based method that edits text with prior guidance control. Our method consists of two key components: (i) By constructing fine-grained text style disentanglement and robust text glyph structure representation, TextCtrl explicitly incorporates Style-Structure guidance into model design and network training, significantly improving text style consistency and rendering accuracy. (ii) To further leverage the style prior, a Glyph-adaptive Mutual Self-attention mechanism is proposed which reconstructs the implicit fine-grained features of the source image to enhance style consistency and vision quality during inference. Furthermore, to fill the vacancy of the real-world STE evaluation benchmark, we create the first real-world image-pair dataset termed **ScenePair** for fair comparisons. Experiments demonstrate the effectiveness of TextCtrl compared with previous methods concerning both style fidelity and text accuracy. Project page: https://github.com/weichaozeng/TextCtrl.

## 1 Introduction

Scene Text Editing (STE) refers to modifying the text with desired content on an input image while preserving the styles and textures of both the text and the background to maintain a realistic appearance . As a newly emerging task in the field of scene text processing , STE not only possesses distinctive application value [3; 4; 5] but also benefits the text-oriented downstream research in detection , recognition [7; 8], spotting  and reasoning [10; 11]. Recently, increasing attention has been paid to GAN-based and diffusion-based scene text editing methods.

Exploiting the Generative Adversarial Networks (GANs) , early works [1; 13] decompose STE into three subtasks: foreground text style transfer, background restoration and fusion. The divide-and-conquer manner significantly reduces the difficulty of pattern learning and enables the pre-training of sub-modules with additional supervision . However, the generalization capabilities of these methods are inevitably limited due to the constrained model capacity of GANs  and the challenges in accurately decomposing text styles . Besides, as observed in experiments, the divideand-conquer design brings about the "bucket effect", wherein the unstable background restoration quality leads to messy fusion artifacts.

Recently, large-scale text-to-image diffusion models [16; 17] have convincingly demonstrated strong capabilities in image synthesis and processing. Several methods attempt to realize STE in a conditional synthesis manner, including style image concatenation  and one-shot style adaptation . However, these methods are limited to **the coarse-grained learning of miscellaneous styles** from text images. Other methods [20; 21; 22] tend to resolve STE in a universal framework along with STG (Scene Text Generation) in an inpainting manner conditioned on full images, which enables the leverage of large-scale data for self-supervised learning . Nevertheless, their **style guidance predominantly originates from the image's unmasked regions**, which can be unreliable in complex scenarios and fail in style consistency. Besides, resulting from **the weak correlation between text prompt and glyph structure**[24; 25; 26], diffusion-based STE methods are prone to generating typos, which decreases the text rendering accuracy.

For the aforementioned problems, we identify insufficient prior guidance on both style and structure as the primary factor that impedes the previous methods from performing accurate and faithful scene text editing. As depicted in Fig. 1, we propose a conditional diffusion-based STE model, wherein our method decomposes the prerequisite of STE into two main aspects: text style disentanglement and text glyph representation. The fine-grained disentangled text style features ensure visual coherency, while the robust glyph structure representation improves text rendering accuracy. The dual Style-Structure guidance collectively contributes to significant enhancements in STE performance.

Furthermore, undesired color deviation and texture degradation compared with the source text image occasionally occur in the inference of diffusion-based STE methods, which is attributed to the error accumulation in the denoising process  as well as the domain gap between training and inference . To overcome this limitation, we introduce a glyph-adaptive mutual self-attention mechanism to improve the generator, which sets up a parallel reconstruction branch to introduce the source image style prior through cross-branch integration. The refined sampling process effectively eliminates visual inconsistency without requiring additional tuning.

Additionally, the deficiency of real-world evaluation benchmarks on STE has become a non-negligible problem as increasing methods are proposed. Early assessments , which rely on synthetic data, face significant limitations in practice due to the domain gap. Recent evaluations  emphasize text accuracy in edited real images but fail to benchmark visual quality adequately. Based on the observation that scene texts often occur in phrases with the same style and background in real-world scenery, we elaborately collect 1,280 text image pairs in terms of similar style and word length from scene text datasets to build the **ScenePair** dataset enabling comprehensive evaluation.

In summary, we improve STE with the full leverage of **Text** prior for comprehensive guidance **Control** throughout the model design, network training and inference control, termed as **TextCtrl**. Our main contributions are as follows:

Figure 1: Conceptual illustration of the decomposition of STE by TextCtrl. (a) Text style is disentangled into text background, text foreground, text font glyph and text color features. (b) Text glyph structure is represented by the cluster centroid of various font text features. (c) The explicit style features and structure features guide the generator to perform scene text editing.

* For the first time, we decompose the prerequisite of STE into fine-grained style disentanglement as well as glyph structure representation and incorporate the Style-Structure guidance with diffusion models to improve rendering accuracy and style fidelity.
* For further style coherency control during sampling, with the leverage of additional prior guidance through the reconstruction of the source image, we introduce a glyph-adaptive mutual self-attention mechanism that effectively eliminates visual inconsistency.
* We propose an evaluation benchmark **ScenePair** consisting of cropped text image pairs along with original full-size images. To the best of our knowledge, it is the first pairwise real-world dataset for STE which enables both visual quality assessment and rendering accuracy evaluation.

## 2 Related work

GAN-based Scene Text Editing.SRNet  first introduces the word-level editing method built in a divide-and-conquer manner. SwapText  further enhances SRNet with Thin Plate Spline Interpolation Network for curved text modification while STRIVE  extends the framework into the video domain of scene text replacement. Besides, TextStyleBrush  adopts a self-supervised strategy building on StyleGAN2  while MOSTEL  designs a semi-supervised training scheme.

Diffusion-based Scene Text Editing.Numerous studies have focused on adapting the diffusion model for scene text manipulation. DiffSTE  improves pre-trained diffusion models with a dual encoder design, wherein a character encoder for render accuracy and an instruction encoder for style control is used. DiffUTE  further utilizes an OCR-based image encoder as an alternative to CLIP Text encoder. Moreover, TextDiffuser  and UDiffText  leverage character segmentation masks for condition input and supervised labels respectively. To leverage text style, LEG  concats the source image as input while DBEST  relies on a fine-tuning process during inference. Recently, AnyText  adopted a universal framework to resolve STE and STG in multiple languages based on the prevalent ControlNet .

Image Editing with Diffusion Models.Image editing aims to manipulate a certain attribute (e.g. color, posture, position) of the target object while keeping the other context unchanged, which can be seen as the parent task of STE. Recent Diffusion-based methods have shown unprecedented potential with a wide variety of designs. Model-tuning methods [32; 33] fine-tune the entire model to enhance subject embedding in the output domain. Leveraging DDIM inversion , prompt-tuning methods  turn to improve identity preservation by optimizing null-text prompts through classifier-free guidance sampling . Recently, [36; 37] explored the self-attention layers in LDMs and demonstrated the rich semantic information preserved in queries, keys and values. Through the cross-frame substitute of keys and values of self-attention, they perform non-rigid editing without additional tuning.  further extends the cross-frame interaction to video domain for motion editing.

## 3 Method

Based on a conditional synthesis manner, in this work, we define the scene text editing process as \(I_{edit}=(C_{struct},C_{style})\) as shown in Fig. 2 (c). The text glyph structure feature is acquired from a character-based structure encoder as \(C_{struct}=(C_{text})\) in Fig. 2 (a) and the text style feature is derived from a style encoder \(C_{style}=(I_{source})\) in Fig. 2 (b). The module design and pre-training strategy to enable precise extraction for glyph structure and fine-grained disentanglement of text style are introduced in section 3.1 and section 3.2 respectively, with the whole model training process illustrated in section 3.3. Furthermore, details of the improved tuning-free inference control and the proposed glyph-adaptive mutual self-attention mechanism in Fig. 2 (d) are presented in section 3.4.

### Text Glyph Structure Representation Pre-training

Distinctive from natural objects, scene text possesses a complicated non-convex structure, wherein a minor stroke discrepancy can significantly alter visual perception and lead to misinterpretation , thus presenting unique challenges to editing accuracy. For scene text editing, an ideal text encoder is capable of encoding the target text concerning glyph structure rather than semantic information [20; 21] or certain image template [14; 23]. Specifically, for a certain text \(C_{text}=``Sifted"\), encoder \(\) is expected to be aware of the glyph structure of \(``S"\), \(``i"\), \(``f"\), \(``t"\), \(``e"\), \(``d"\) respectively.

To this end, we adopt a character-level text encoder to align the target text feature with its visual glyph structure. As depicted in Fig. 2 (a), the target text embedding in character level is processed with a transformer encoder \(\) to generate glyph structure features \(C_{struct}^{L d}\), which is further aligned to the visual feature of corresponding text image extracted by a frozen pre-trained scene text recognizer with CLIP loss [30; 40]\(_{clip}\). Differ from , we collect vast quantities of text fonts constructing a cluster \(\{font1,font2_{a}...fontn_{n}\}\) to render the corresponding text image with diverse fonts during training. The font-variance augmentation brings continuous glyph structure variation which implicitly enhances the projection from \(C_{struct}\) to the cluster centroids of visual features for robust text glyph structure representation.

### Text Style Disentanglement Pre-training

Text styles comprise a variety of aspects, including font, color, spatial transformation and stereoscopic effect, which visually mingle with each other and bring obstacles to disentangle the style features precisely in previous works. To realize the fine-grained disentanglement of the text style, we propose a multi-task pre-training paradigm as illustrated in Fig. 2 (b), involving text color transfer, text font transfer, text removal and text segmentation.

Concretely, given a text image \(I_{source}^{3 H W}\), we first extract the style feature \(C_{style}^{N d}\) with a ViT  backbone \(\), which is projected to texture feature \(c_{texture}^{N d}\) and spatial feature \(c_{spatial}^{N d}\) respectively. Subsequently, \(c_{texture}\) is employed in text color transfer and text font transfer while \(c_{spatial}\) is utilized for text removal and text segmentation.

Text Color Transfer.Since both intrinsic style and lighting conditions determine the text color, it is challenging to label or classify the holistic color. Instead, we refer to image style transfer and implicitly extract color through colorization training. A light-weight encoder-decoder \(^{c}\) is built to provide colorization on a black and white text image \(i^{c}_{in}^{1 h w}\) with an Adaptive Instance Normalization \(\) for source text color image \(i^{c}_{out}^{3 h w}\) written as:

\[i^{c}_{out}=^{c}_{dec}((^{c}_{ enc}(i^{c}_{in}),c_{texture})),\] (1)

Text Font Transfer.With a common intention with color transfer to capture stylized information but focusing on glyph boundary, font transfer is realized through the boundary reshaping process. Another

Figure 2: Decomposed framework of TextCtrl. (a) Text glyph structure encoder \(\) with corresponding glyph structure representation pre-training. (b) Text style encoder \(\) with corresponding style disentanglement pre-training. (c) Prior guided diffusion generator \(\). (d) The improved inference control with the Glyph-adaptive Mutual Self-attention mechanism.

light-weight encoder-decoder \(^{f}\) is employed to transfer a template font text glyph \(i^{f}_{in}^{3 h w}\) to the source font text glyph \(i^{f}_{out}^{3 h w}\) through Pyramid Pooling Module \(\) in latent space as:

\[i^{f}_{out}=^{f}_{dec}((^{f}_{enc}(i^{f}_{in}),c_{texture})),\] (2)

Text Removal and Text Segmentation.Text removal aims at erasing the text pixels and reasoning the background pixels covered by text while text segmentation decouples the spatial relationships between background and text. A residual convolution block with spatial attention mechanism  is adopted to construct a removal head \(^{r}\) and a segmentation head \(^{s}\) respectively to generate predicted background \(i^{r}_{out}^{3 h w}\) and predicted mask \(i^{s}_{out}^{1 h w}\) as:

\[i^{r}_{out}=^{r}(c_{spatial}), i^{s}_{out}=^{s}(c _{spatial}),\] (3)

Multi-task Loss.With the multi-task pre-training for fostering the text style extraction and disentanglement ability of \(SE\), the whole loss function for style pre-training can be expressed as:

\[_{disentangle}=_{color}(i^{c}_{out},i^{c}_{gt})+ _{font}(i^{f}_{out},i^{f}_{gt})+_{rem}(i^{r}_{out},i^{r} _{gt})+_{seg}(i^{s}_{out},i^{s}_{gt}),\] (4)

wherein we leverage MSE loss for \(_{color}\), MAE loss for \(_{rem}\) and Dice loss  for \(_{font}\) and \(_{seg}\). Synthetic groundtruth is leveraged for fine-grained supervision and the task-oriented pre-training achieves fine-grained textural and spatial disentanglement of stylized text images which fertilizes the style representation for downstream generator.

### Prior Guided Generation

With the robust glyph structure representation \(C_{struct}\) and fine-grained style disentanglement \(C_{style}\) mentioned above, a diffusion generator \(\) is employed to integrate the prior guidance and generate the edited result as shown in Fig. 2 (c). For \(C_{struct}\), since the U-Net in latent diffusion models contains both self-attention and cross-attention, wherein the cross-attention focuses on the relation between latent and external conditions [16; 17], we replace the key-value in cross-attention modules of \(\) with the linear projection of \(C_{struct}\) to provide glyph guidance for improving accurate text rendering. For \(C_{style}\), promising results have been shown by additional control injection  through the decoder of U-Net, based on which we apply the multi-scale style feature \(C_{style}\) to the skip-connections and middle block of the model \(\) to provide a style reference for high-fidelity rendering.

With the leverage of pre-trained model , the training is performed under a combined supervision on the synthetic text image data. Please refer to Appendix A for preliminaries of the diffusion model and Appendix B.2 for implementation details of training.

### Inference Control

During inference of the diffusion-based STE model, undesired color deviation and texture degradation occasionally occur. Such discrepancy can be partly attributed to the error accumulation during the iterative sampling process [27; 36]. Besides, the domain gap between training and inference impedes style consistency in complicated real-world scenery. To control the visual style consistency, we attempt to ameliorate the inference process by injecting style prior from the source image into editing. Specifically, we propose the Glyph-adaptive Mutual Self-Attention mechanism, which seamlessly incorporates the style of source images throughout the reconstruction process.

Reconstruction Branch.Rather than transforming random noise samples into an image, our objective is to execute an image-to-image translation, ensuring the preservation of style features. Initially, we perform DDIM inversion [27; 46] to generate an initial latent \(z^{T}_{source}\) from the source image \(I_{source}\). The reconstructed inversion process enables a reconstruction branch (\(z^{T}_{source},z^{T-1}_{source}...z^{0}_{source}\)) of the source image parallel to the editing branch (\(z^{T}_{edit},z^{T-1}_{edit}...z^{0}_{edit}\)), which benefits the proposed integration process, as shown by the arrow in Fig. 2 (d).

Glyph-adaptive Mutual Self-Attention Mechanism (GaMuSa).Diverge from the general image editing, the target text modification of STE can lead to significant changes in the condition representation, which impedes text style preservation through general prompt-tuning methods .

Recently, [36; 37] have demonstrated that self-attention layers in the diffusion model focus on latent internal relations. Based on the characteristic, we perform a mutual self-attention process between two branches as shown in Fig. 2(d), wherein at denoising step \(t\), the Key-Value features \(\{K_{s},V_{s}\}\) from reconstruction branch are introduced to the self-attention operation in editing branch. Rather than a direct replacement in the previous method , however, we prefer an integration of \(\{K_{s},V_{s}\}\) and \(\{K_{e},V_{e}\}\) to mitigate the domain gap between \(z_{source}^{t}\) and \(z_{edit}^{t}\).

It is worth noting that the original mutual self-attention process is hyper-parameter-sensitive to the starting time step. Premature initialization may introduce ambiguity and lead to spelling errors, whereas late intervention might fail to provide sufficient guidance. Inspired by the gradual deformation of text glyph during the iteration, we design a glyph-adaptive strategy to control the intensity of integration. Specifically, we employed a vision encoder \(\) of the pre-trained text recognizer  to construct a glyph-adaptive strategy for the harmonious integration of Key-Value between branches. Specifically, during the iterative process, the intermediate latent \(z_{edit}^{t}\) is decoded and processed with \(\) for the cosine similarity calculation with the target text embedding \(emb_{y}\) at intervals of \(\) steps to denote the glyph similarity of the intermediate edited image with target text. The similarity will serve as the intensity parameter \(\) and \(\) for controlling the integration between \(\{K_{s},V_{s}\}\) from reconstruction branch and \(\{K_{e},V_{e}\}\) from editing branch. The result of integration \(\{K_{es},V_{es}\}\) is subsequently leveraged in the self-attention modules of the editing branch for style coherency enhancement. The overall sampling pipeline is illustrated in Alg. 1.

\[=(( K_{s}+ K_{e})^{}}{})( V_{s}+ V_{e}),=1-.\] (5)

```
1:\(t=T,=0,=1,=5\);
2:for\(t=T,T-1...1\)do
3:\(z_{source}^{t-1},\{K_{s},V_{s}\}}(t,z_{source}^{t}, c_{source})\); \(\)Self-Attention.
4:\(\)
5:if at intervals of \(\)then
6:\(emb_{edit}=(_{dec}(z_{edit}^{t}))\);
7:\(=(emb_{edit} emb_{y})/(||emb_{edit}||*||emb_{y}||)\); \(\)Cosine Similarity.
8:endif
9:\(=1-\);
10:\(\{K_{es},V_{es}\}=\{K_{s},V_{s}\}+\{K_{e},V_{e}\}\); \(\)Integration.
11:\(z_{edit}^{t-1}}(t,z_{edit}^{t},c_{edit};\{K_{es},V_ {es}\})\); \(\)Glyph-adaptive Mutual Self-attention.
12:endforReturn\(z_{source}^{0}\), \(z_{edit}^{0}\) ```

**Algorithm 1** Glyph-adaptive Mutual Self-attention

## 4 Experiments

### Dataset and Metrics

Training Data.Based on [1; 3], we synthesize 200k paired text images for style disentanglement pre-training and supervised training of TextCtrl, wherein each paired images are rendered with the same styles (i.e. font, size, colour, spatial transformation and background) and different texts, along with the corresponding segmentation mask and background image. Furthermore, a total of 730 fonts are employed to synthesize the visual text images in text glyph structure pre-training.

ScenePair Benchmark.To provide assessments on both visual quality and rendering accuracy, we propose the first real-world image-pair dataset in STE. Specifically, we collect 1,280 image pairs with text labels from ICDAR 2013 , HierText  and MLT 2017 , wherein each pair consists of two cropped text images with similar text length, style and background, along with the original full-size images. Collecting methods and dataset details are introduced in Appendix C.

Evaluation Dataset.For a fair comparison, we conduct all the evaluations on real-world datasets. ScenePair consists of 1,280 cropped text image pairs along with original full-size images enabling both style fidelity assessment and text rendering accuracy evaluation. TamperScene  combines a total of 7,725 cropped text images with predefined target text to provide rendering accuracy evaluation. Nevertheless, it does not involve paired images for style assessment nor full-size images for evaluation on inpainting-based methods, demonstrating the necessity of the proposed ScenePair.

Evaluation Metrics.For visual quality assessment, we adopt the commonly used metrics including (i) _SSIM_, mean structural similarity; (ii) _PSNR_, the ratio of peak signal to noise; (iii) _MSE_, the mean squared error on pixel-level; (iv) _FID_, the statistical difference between feature vectors. For text rendering accuracy comparison, we measure with (i) _ACC_, word accuracy and (ii) _NED_, normalized edit distance, using an official text recognition algorithm  and corresponding checkpoint.

### Performance Comparison

Implementation.We conduct the comparison of the proposed TextCtrl with two GAN-based methods: SRNet  and MOSTEL  as well as three diffusion-based methods: DiffSTE , TextDiffuser  and AnyText  with their provided checkpoints. The quantitative results are illustrated in Tab. 1 and Tab. 2 while the qualitative results for comparison are shown in Fig. 3 and Fig. 4. Notably, DiffSTE , TextDiffuser  and AnyText  conduct STE with an inpainting manner on a full-size image, for which we employed the corresponding full-size image of each pair in ScenePair with the target text area masked as input and crop the generated target text area for style evaluation on text image level. SRNet , MOSTEL  and TextCtrl resolve STE in a synthesis manner on a text image, for which we perform a perspective process to paste the generated image back to the full-size image for style evaluation on full image level.

Text Style Fidelity.The text style fidelity assessment is performed on both the text image level and the full image level of ScenePair to enable a comprehensive comparison among methods. On

    &  &  \\   & SSIM \(\)(\( 10^{-2}\)) & PSNR \(\) & MSE \(\)(\( 10^{-2}\)) & FID \(\) & SSIM \(\)(\( 10^{-2}\)) & FID \(\) \\  SRNet  & 26.66 \(\) 0.00 & 14.08 \(\) 0.00 & 5.61 \(\) 0.00 & 49.22 \(\) 0.00 & 98.91 & 1.48 \\ MOSTEL  & 27.45 \(\) 0.00 & 14.46 \(\) 0.00 & 5.19 \(\) 0.00 & 49.19 \(\) 0.00 & 98.96 & 1.49 \\ DiTSTE  & 26.85 \(\) 0.08 & 13.44 \(\) 0.04 & 6.11 \(\) 0.04 & 120.34 \(\) 1.52 & 98.86 (76.91) & 2.37 (96.78) \\ TextDiffuser  & 27.02 \(\) 0.11 & 13.96 \(\) 0.03 & 5.75 \(\) 0.05 & 57.01 \(\) 0.44 & 98.97 (92.76) & 1.65 (12.23) \\ AnyText  & 30.73 \(\) 0.55 & 13.66 \(\) 0.07 & 6.19 \(\) 0.14 & 51.79 \(\) 0.35 & 98.99 (82.57) & 1.93 (16.92) \\  TextCtrl & **37.56** \(\) 0.32 & **14.99** \(\) 0.15 & **4.47** \(\) 0.15 & **43.78** \(\) 0.17 & **99.07** & **1.17** \\   

Table 1: Text style fidelity assessment within text image level and full-size image level, highlighted with **best** and _second_ best results. For full-size image evaluation, we replace the unedited region with the origin image while values in “()” denote the direct output of inpainting-based methods.

    &  &  &  \\   & ACC(\%) \(\) & NED \(\) & ACC(\%) \(\) & NED \(\) & ACC(\%) \(\) & NED \(\) \\  SRNet  & 17.84 \(\) 0.00 & 0.478 \(\) 0.000 & 9.61 \(\) 0.00 & 0.422 \(\) 0.000 & 39.96 \(\) 0.00 & 0.776 \(\) 0.000 \\ MOSTEL  & 37.69 \(\) 0.00 & 0.557 \(\) 0.000 & 22.50 \(\) 0.00 & 0.451 \(\) 0.000 & **76.79** \(\) 0.00 & 0.858 \(\) 0.000 \\ DiffSTE  & 31.35 \(\) 0.35 & 0.538 \(\) 0.002 & 21.56 \(\) 0.69 & 0.487 \(\) 0.002 & - & - \\ TextDiffuser  & 51.48 \(\) 0.19 & 0.719 \(\) 0.003 & 33.99 \(\) 0.34 & 0.635 \(\) 0.004 & - & - \\ AnyText  & 51.12 \(\) 0.21 & 0.734 \(\) 0.005 & 25.05 \(\) 0.05 & 0.593 \(\) 0.003 & - & - \\ TextCtrl & **84.67** \(\) 0.34 & **0.936** \(\) 0.003 & **66.95** \(\) 0.13 & **0.869** \(\) 0.007 & 74.17 \(\) 0.55 & **0.909** \(\) 0.011 \\   

Table 2: Text rendering accuracy evaluation with different methods, highlighted with **best** and _second_ best results. “Random” denotes that we replace the paired target text in ScenePair with randomly chosen text to verify the model robustness. Note that we are not able to evaluate inpainting-based STE methods  on TamperScene  since it does not contain full-size images.

text image level, TextCtrl outperforms other methods by at least 0.07, 0.53, 0.72 and 5.41 in SSIM, PSNR, MSE and FID respectively as represented in Tab. 1. GAN-based methods [1; 14] generally achieve a higher score on pixel-level assessment (i.e., PSNR, MSE). The reason lies in that they adopt a divide-and-conquer method, which contains a background restoration process to restrain the background region unaltered. Nevertheless, this may result in generating unsatisfied fuzzy images as shown in Fig. 3 column 3 and 4 due to the artifacts left by the unstable restoration process. Due to the loose style control result from the inpainting manner, undesired text style occurs occasionally in the outcome for diffusion-based methods [20; 21; 22]. On the contrary, TextCtrl benefits from the full leverage of disentangled style prior and the inference control for high-fidelity edited results. Further comparison on the full image level demonstrates the superiority of TextCtrl with precise manipulation and less style deviation against the inpainting-based methods with visualization in Fig. 4. Besides, it is not negligible that the inpainting strategy downgrades the image quality of unmasked regions.

Text Rendering Accuracy.Meanwhile, owing to the robust glyph structure representation, TextCtrl achieves superior spelling accuracy among all the methods, with more than 33% improvements in rendering accuracy of paired target text and randomly chosen text in ScenePair. TamperScene  contains a number of ambiguous low-resolution text images, which bring obstacles in style disentanglement and therefore impede the rendering accuracy of TextCtrl. Still, TextCtrl achieves a higher normalized edit distance that indicates the explicit mapping constructed between text and glyph. In comparison, GAN-based methods tend to yield ambiguous images, where source text left by imperfect removal blends with target text, resulting in fuzzy visual quality. Besides, due to the limited model capacity, GAN-based methods suffer from weak generalization and show incompetence with unseen style font as shown in Fig. 3 row 4 and 5. Diffusion-based methods achieve a disproportionate NED to their relatively low accuracy, which indicates their struggle with spelling mistakes. Notably, the inpainting manner serves as a primary factor that impedes the editing quality on small text for AnyText, whereas TextCtrl possesses the flexibility to perform editing on arbitrary scale text images.

### Ablation Study

TextCtrl significantly improves STE through the substantial leverage of prior information in proposed (i) glyph structure representation pre-training, (ii) style disentanglement pre-training and (iii) glyph-adaptive mutual self-attention. We delve into the efficacy of each module in the following section.

    &  &  \\   & ACC(\%) \(\) & NED \(\) & ACC(\%) \(\) & NED \(\) \\  CLIP  & 13.98 & 0.637 & 13.47 & 0.615 \\ \(\) w/o font-variance & 76.08 & 0.875 & 60.84 & 0.827 \\ \(\) w/o font-variance & **84.67** & **0.936** & **66.95** & **0.809** \\   

Table 3: Ablation experiment on glyph structure representation pre-training.

Figure 3: Qualitative comparison among different methods. Note that for the inpainting-based methods [20; 21; 22], we conduct the editing on the full-size images and perform the visualization of the edited text region.

[MISSING_PAGE_FAIL:9]

Inference control with Glyph-adaptive Mutual Self-attention (GaMuSa).In Tab. 5, we assess the style fidelity enhancement of GaMuSa in contrast with direct sampling and a prevalent enhancement method MasaCrtrl  on ScenePair. Quantitative results verify the effectiveness of GaMuSa in enhancing style control during inference on variant real-world text images. Further visualization in Fig. 6 demonstrates the ability to persist style fidelity of GaMuSa when confronted with situations including background color deviation, unseen font and glyph texture degradation.

## 5 Limitations and Conclusion

Challenging arbitrary shape text editing.Arbitrary shape text editing occurs occasionally when editing with text on a crescent signboard or a circular icon as shown in Appendix Fig. 11. These texts possess a complicated geometric attribution which is hard to disentangle through the style reference. Early works [13; 14] adopt the Thin-Plate-Spline (TPS) module to capture the accurate geometric distribution of text and perform a transformation on the template image as pre-processing. However, this strategy only takes effect in GAN-based methods which adopt an image-to-image paradigm. It remains a problem to effectively introduce accurate geometric prior guidance to diffusion models.

Sub-optimal visual quality assessments metric.Following previous STE methods, we adopt a variety of evaluation metrics for visual quality assessment. However, these metrics either focus on pixel-level discrepancy or concentrate on feature similarity in latent space, which is sub-optimal for assessing text style coherency. Besides, all these metrics rely on the paired data under which a ground-truth image is required. Though we collect a real-world image-pair dataset ScenePair in our work, a large amount of real-world text images remain unpaired and thus fail to provide visual quality assessment in editing. While human evaluation may be a possible solution, a more efficient and objective visual assessment metric is expected for scene text editing.

In this paper, we propose a diffusion-based STE method named TextCrrl with the leverage of disentangled text style features and robust glyph structure guidance for high-fidelity text editing. For further coherency control during inference, a glyph-adaptive mutual self-attention mechanism is introduced along with the parallel sampling process. Additionally, an image-pair dataset termed ScenePair is collected to enable the comprehensive assessment on real-world images. Extensive quantitative experiments and qualitative results validate the superiority of TextCrrl.