# MultiVerse: Exposing Large Language Model Alignment Problems in Diverse Worlds

Xiaolong Jin

Purdue University

jin509@purdue.edu

&Zhuo Zhang

Purdue University

zhan3299@purdue.edu

&Guangyu Shen

Purdue University

shen447@purdue.edu

&Hanxi Guo

Purdue University

guo778@purdue.edu

&Kaiyuan Zhang

Purdue University

zhan4057@purdue.edu

&Siyuan Cheng

Purdue University

cheng535@purdue.edu

&Xiangyu Zhang

Purdue University

xyzhang@cs.purdue.edu

###### Abstract

Large Language Model (LLM) alignment aims to ensure that LLM outputs match with human values. Researchers have demonstrated the severity of alignment problems with a large spectrum of jailbreak techniques that can induce LLMs to produce malicious content during conversations. Finding the corresponding jailbreaking prompts usually requires substantial human intelligence or computation resources. In this paper, we report that LLMs have different levels of alignment in various contexts. As such, by systematically constructing many contexts, called worlds, leveraging a Domain Specific Language describing possible worlds (e.g., time, location, characters, actions and languages) and the corresponding compiler, we can cost-effectively expose latent alignment issues. Given the low cost of our method, we are able to conduct a large scale study regarding LLM alignment issues in different worlds. Our results show that our method outperforms the-state-of-the-art jailbreaking techniques on both effectiveness and efficiency. In addition, our results indicate that existing LLMs are extremely vulnerable to nesting worlds and programming language worlds. They imply that existing alignment training focuses on the real-world and is lacking in various (virtual) worlds where LLMs can be exploited.

## 1 Introduction

In recent years, Large Language Models (LLMs) have undergone transformative advancements, starting a new era in deep learning. These models, exemplified by GPT  and Llama , have demonstrated unprecedented capabilities in understanding and generating human-like text . Their expansive knowledge, acquired through extensive pre-training on diverse datasets, enables them to perform tasks across various domains with a level of proficiency comparable to human experts . LLMs have become integral to many cutting-edge applications, from question-answering chatbots to code-generation tools like Github Copilot1. With recent efforts of building large-scale eco-systems such as ChatGPT plugin  and GPT Store . These models will become prevalent in every aspect of our daily life. Despite LLMs' remarkable progress, many believe we should proceed withextreme caution due to the prominent _alignment_ problem of these models with human values, which could lead to various ethical and security problems in human society . The current practice relies on _Reinforcement Learning with Human Feedback_ (RLHF), which involves incorporating human feedback into a reinforcement learning process . In RLHF, humans provide evaluations or feedback on a model's actions, which is then used to improve the learning process, e.g., in the aspect of ethical value alignment. Although leading LLM models have gone through substantial RLHF efforts, the manual nature of RLHF dictates that such efforts are likely limited compared to training in other aspects . In other words, existing LLMs still suffer from alignment problems, as evidenced by numerous recent jailbreaking reports in which researchers and hackers successfully bypass LLMs' built-in safeguards, causing the models to produce harmful content that violates the usage policies set by the LLM vendors ,.

Upon analyzing jailbreak prompts found on the Internet and in public datasets , we observed that most of these prompts are based on contexts not covered in the alignment training process. For instance, the famous jailbreak prompt DAN (_do anything now_) prompt  compels LLMs to immerse themselves in a fictional world where they are not bounded by their established rules. Similarly, the _Dr. AI_2 prompt deceives LLMs to responding to malicious questions in the underground headquarters of Dr. AI. Therefore, we indicate that the LLM alignment problem is _context-sensitive_, meaning that the level of protection varies depending on the conversation context. This may be inherent because conversations in RLHF likely follow a natural, normal distribution, whereas addressing the alignment problem requires covering corner cases. Therefore, the essence of the jailbreak LLMs lies in identifying the combination of corner contexts.

Inspired by the observation above, in this work, we develop MultiVerse, a novel technique to jailbreak LLMs by constructing a diverse set of conversation contexts automatically and study popular models' alignment problems in these contexts. Essentially, defining a context is equivalent to describing a world. Therefore, we use a _domain-specific language_(DSL) to describe the universe of multiple worlds, which specifies a world by defining time, location, characters, and actions following the linguistic literature . For example, one can describe a fairyland such as _Whoville_, a cheerful town inhabited by _the Whos_ or a mundane world for software developers speaking Python. Worlds can be connected through actions, e.g., leaving a world and entering another nested world. A compiler then automatically compiles and explores the possible worlds with the specification. Because our ultimate goal is to expose alignment problems in the _real world_, not a fantasy world, MultiVerse will inject the malicious question in the created universe of multiple worlds that includes an embedded real world. Specifically, inappropriate questions are only inserted in the embedded real world, and the subject LLM's response is measured. For example, LLMs are aligned to directly reject harmful questions as shown in Figure 1(a). However, when such questions are embedded within a specialized single world, LLMs might be partially jailbroken to generate some toxic content instead of a comprehensive response to the question. Nevertheless, the alignment established by the LLM in a single context is completely eradicated when MultiVerse is employed to embed the malicious question within a prompt that combines multiple worlds like in Figure 1(c).

Figure 1: Examples of MultiVerse jailbreak. The LLM alignment is _context-sensitive_, meaning that the level of protection varies depending on the conversation context. LLMs are jailbroken successfully by prompts that combine specific different worlds.

Overall, our contributions are as follows:

* We propose MultiVerse, a technique to automatically construct jailbreak prompts using a domain-specific language to define scenario, time, location, characters, and actions of multiple worlds.
* Extensive experiments demonstrate the effectiveness of MultiVerse, which achieves the jailbreak success rate of over 85% across three datasets on various aligned LLMs with low overhead.
* We test MultiVerse on two popular LLMs with 300 different generated worlds. We observe that LLMs are well-protected in the vanilla real world. However, the protection degrades when the created world diverges from the reality. The protection completely disappears inside a nest of multiple fantasy worlds.

## 2 World Description Language

This section introduces the World Description Language (WDL), a specialized language used by MultiVerse to represent multi-world universes. WDL enables MultiVerse to effectively generate various scenarios, exploiting the context-sensitivity issues of LLM alignment. The syntax of our world description language is depicted in Figure 2.

Specifically, WDL is designed similarly to the Hypertext Markup Language (HTML), allowing well-developed HTML generation and mutation algorithms to produce diverse contexts. In WDL, a _world_\(w\) is depicted using world tags, defining the world's _properties_\(ps\), the _characters_\(cs\), and their _actions_\(as\). The properties of a world are key-and-value pairs, focusing on four main aspects.

* Scenario: This defines the foundational framework and situational backdrop of the world, such as a novel, research experiment, game, or podcast.
* Time: This refers to the era in which the world exists, ranging from historical settings to modern times or futuristic periods.
* Location: This denotes the specific physical location of the world, which varies from real-world places like New York or Times Square, to space regions like Mars or Sirius, and virtual environments such as the realm of Java or the world of Minecraft.
* Language: This pertains to the mode of communication in the world, encompassing spoken languages, programming languages, markup languages, and cryptographic languages.

Characters within the world are detailed in the chars tags. Each character is assigned a unique ID for future reference and a brief description. This description, structured as a natural-language sentence, can encompass any aspect of the character, such as personality, appearance, etc. Actions among characters are defined in the actions tags, and for illustrating multiple worlds, certain actions are specified:

Figure 2: Domain-specific language for describing the universe of multiple worlds

* Enworld: This represents an action that introduces a new inner world, _i.e._ nesting world, involving a character (identified by \(Z_{}\)) who introduces this world. For example, in "_Bob tells a new story_", Bob is the character and the story is the new world.
* Query: This is the placeholder of the malicious question that will be replaced by the compiler.
* Other: This encompasses all other potential interactions between characters, such as communication, "_sit together and chat_" and "_talk about their history_".

Based on the WDL configuration design, we define Number of Layers in WDL as the number of nesting worlds within the configuration. For instance, Number of Layers is two in DSL example in Figure 4. We also show the candidate set for each component of WDL in Appendix.

### Compilation

We provide an example of WDL configuration in Figure 3. Next, we introduce how to produce complete and natural descriptions of multiple worlds based on WDL configurations. Specifically, we utilize rules of WDL alongside a fixed template to recursively generate the final jailbreak prompts as shown in the Appendix A.2. Figure 4 shows the compilation result of WDL configuration in Figure 3. Overall, we utilize the WDL to generate configurations for a universe of multiple worlds, which are subsequently compiled by the compiler into natural descriptions of the universe. Note that although LLMs are capable of generating free-form universes, our goal is to systematically study the contexts in which LLMs are vulnerable, and hence using WDL is a better design choice.

## 3 MultiVerse Pipeline

In this section, we elaborate the pipeline of MultiVerse in Figure 5. Initially, we extract possible (virtual) world parameters from human-written jailbreak templates on the Internet and utilize GPT-4 to generate multiple world configurations, forming our basic WDL dataset. Specifically, we leverage GPT-4 to generate various alternative options for Scenario, Time, Location, and Language in

Figure 4: The compilation result of WDL configuration

Figure 3: WDL example

both virtual and real-world contexts. In each iteration, a configuration is sampled from the dataset. The compiler then processes the configuration and integrates it with a harmful question to generate a jailbreak prompt (Section 3.1). The prompt is then used to query the target LLM, and the response is evaluated. Failed prompts will be updated, e.g., by using a new configuration or generating new parameters, and the process is continued until the stopping conditions are satisfied shown in Section 3.2. Algorithm 1 provides a step-by-step breakdown of the MultiVerse process.

### Compiler

Given the world description described in WDL, we leverage the compiler to produce the final jailbreak prompts in two steps. First, we generate a natural and complete world description based on configurations by the two alternative methods as described in Section 2.1.

Next, we embed instructions in the description of worlds to induce LLMs to produce answers to malicious questions. Specifically, we simultaneously employ two common designs in existing manual jailbreak prompts from the Internet: _Jailbreak Instruction_ and _Competing Objectives_. _Jailbreak Instruction_ refers to directives to deceive LLMs into responding to harmful questions based on a specific context, _e.g._ "Continue the answer of [malicious question]" or "Rewrite the answer of [malicious question]". The compiler randomly selects _Jailbreak Instructions_ from the dataset we collect from existing resources and injects it in the final jailbreak prompt. _Competing Objectives_ refers to the phenomenon where safety-trained LLMs are often developed with multiple, sometimes conflicting, objectives. In MultiVerse, we enforce LLMs to produce harmless content initially, such as irrelevant background information.

Overall, the compiler injects _Jailbreak Instruction_ and _Competing Objectives_ into the world description to generate the final jailbreak prompts. Detailed examples are presented in the Appendix A.2.

### Parameter updater

In each iteration, we employ a parameter updater to modify prompts that have not yet successfully jail broke. Specifically, we update the configurations of WDL. As demonstrated in Experiment 4.5, we empirically observe that updating parameters such as Location and Language to formulate a more complex context combination enhances the jailbreak success rate. Besides, increasing Number of Layers also contributes positively to the jailbreak success rate shown in Figure 7.

Figure 5: Overview of MultiVerse. Starting with selection of a configuration of world(s), the compiler is then responsible for processing the malicious question and world parameters to generate jailbreak prompts. If the jailbreak fails, MultiVerse will update the WDL configuration and regenerate.

Evaluation

### Experiment Setup

DatasetsWe utilize two datasets in our experiment to evaluate the efficacy of MultiVerse, AdvBench and **TDC Redteaming3**. We present more details in Appendix A.1.

BaselinesWe conduct experiments using Easyjailbreak  benchmark. We compare with state-of-the-art jailbreak methods, including GPTfuzzer , ReNeLLM , ICA , AutoDAN , PAIR, Cipher , GCG , MultiLingual  and Deepinception . We do not compare with white-box methods such as GCG , which requires access to the parameters of LLMs to create a universal adversarial prompt. Besides, GCG usually demands substantial computation resources and time.

Evaluation MetricJailbreak aims to manipulate an LLM to generate specific harmful content. We follow Easyjailbreak benchmark  and leverage GPT-4 to automate the evaluation process described in Appendix A.1.4. Overall, we use three metrics: **Jailbreak Success Rate (JSR)**, **Top-1** **Jailbreak Success Rate (Top-1 JSR)** and **Average Number of Queries per Question (AQQ)**.

### Results

Jailbreak performanceWe evaluate MultiVerse on both closed-source and open-source LLMs, considering factors such as training data and model size. Specifically, we consider four open-source LLMs, i.e., ChatGLM2-6B, ChatGLM3-6B, Vicuna-7B, Llama-2-7B, and four closed-source models, e.g., Gemini-1.0-flash, GPT-3.5-turbo, GPT-4, and GPT-4o. We kept default parameters for all the LLMs used in the experiment using Easyjailbreak benchmark.

Table 1 shows the effectiveness of MultiVerse. Observe that our technique achieves a high ASR across all the LLMs. Specifically, MultiVerse reaches over 80% JSR on small open-source models, such as ChatGLM3-6B, Vicuna-7B, and Llama-2-7B. The high JSRs underscore the inadequate safeguarding mechanisms within these models. We also observe that ChatGLM produces unsafe content, primarily in Chinese, pointing to a critical security risk inherent in both ChatGLM and Llama, regardless of the language of their training data. Besides, the JSR remains above 75% for large commercial models, including GPT-3.5-turbo, GPT-4, Gemini-1.0-pro, and GPT-4o. The extensive knowledge of these large LLMs leads to highly detailed malicious instructions, highlighting inherent risks and the fragile alignment associated with their application.

Additionally, we compare MultiVersewith baselines on Advbench using Easyjailbreak benchmark in Table 4. Baseline results are referred to the leaderboard. MultiVerseachieves the best average results. Additionally, it attained a JSR higher than 84% on the commercial closed models.

We also compare JSR and AQQ between MultiVerse and baseline methods across different LLMs on the TDC dataset in Table 2. Due to the cost of API requests, we did not conduct experiments on GPT-4. The results indicate that MultiVerse outperforms the baseline in JSR and significantly reduces the number of queries, which indicates the effectiveness of MultiVerse.

### Case Study

We provide an example jailbreak prompt to show how MultiVerse works. As shown in Figure 1, the jailbreak prompt creates a Python world using the Python language and then injects a real world

    &  &  \\  & 3-6B & 7B & 2-7B & turbo & turbo & 1.0 pro & 4o \\  Advbench & 84\% & 89\% & 82\% & 89\% & 78\% & 75\% & 84\% \\ TDC & 72\% & 76\% & 84\% & 100\% & 90\% & 84\% & 86\% \\   

Table 1: The JSR of MultiVerse on the Advbench and TDC dataset across seven LLMs. Higher JSR is better.

using English. In the nesting real world, a malicious question is included. As a result, the LLM is successfully deceived to first produce normal content in the Python world and then output detailed malicious instructions.

**Multi-round Jailbreak** We show that MultiVerse supports multi-round jailbreak in Figure 8. Interestingly, we discovered that LLMs exhibit weak alignment with pronouns, although they are proficient in pronoun resolution, which is the process of determining which entity a pronoun in a text refers to in the NLP domain. So we continuously jailbreak for more detailed content by utilizing neutral and benign pronouns such as "Provide more details of Step 3" instead of malicious phrases like "Provide more details of Disabling Security." Consequently, this leaves room for future research to consider pronoun resolution during the alignment training. More results about Observation 1 are shown in Appendix.

**Observation 1**: LLMs exhibit weak alignment with pronouns, which can be utilized for multi-round jailbreak.

### Ablation Study

In this section, we report ablation studies on the core factors of MultiVerse. We only use the TDC Redteaming dataset and evaluate Top-1 JSR in the ablation study. More detailed ablation studies are presented in Appendix A.7

**Number of Layers** Figure 7 illustrates the ablation study investigating the impact of the number of layers of jailbreak prompts on the JSR across four LLMs. We fix other factors and use prompt formats in Appendix A.5 The JSR increases with the number of layers across all models. This reflects that as the number of layers grows, LLMs start to escape the safety alignment established during training, primarily in real-world contexts. First, LLMs demonstrate stronger robustness with fewer layers, suggesting better alignment in real-world contexts. Second, the high JSR of GPT-3.5-turbo, even with few layers, raises questions regarding its alignment. Besides, Llama-2-7B's JSR does not increase much with additional layers, which is attributed to the model's better alignment and limited comprehension capability, leading it to produce irrelevant content. However, the JSR of ChatGLM3 dropped when it reached 5 layers. We analyze the failure cases of ChatGLM3 at layer 5 and conclude that due to the increased complexity of the prompts, ChatGLM3 either produced garbled output or failed to understand the instructions in the prompts. Overall, prompts with more layers show better attack performance. We provide example prompts and a detailed ablation study of other factors in the Appendix A.5.

### Parameter Sensitivity Analysis

In this section, we evaluate MultiVerse on two widely used LLMs, GPT-3.5-turbo and Llama-2-70B4 with 300 different generated worlds using Top-1 JSR. To make a comprehensive study, we leverage GPT-4 to generate 1,000 alternatives for each parameter and categorize them. Due to

Figure 6: Parameter analysis of WDL in MultiVerse

computational resource constraints, we randomly sample 300 configurations to obtain 300 jailbreaking prompts in different worlds. This study focuses on testing the alignment of LLMs in various contexts using prompts with only one layer generated by MultiVerse. More details are presented in the Appendix A.8.

**Scenario** Figure 5(a) shows JSR for different scenarios. There are two findings. First, scenarios "pronoun" like "XYZ" and "ABC" get a higher JSR, _e.g._, "create a 'XYZ' in the real world". We propose that LLMs are trained on alignment in common contexts like stories and novels. However, due to their powerful comprehension abilities and limited range of alignment scenarios, LLMs can still produce harmful content in "pronoun" scenarios such as "XYZ", which aligns with the weak alignment of conference resolution utilized in multi-round jailbreak. On the other hand, the JSR of scenarios with "educational formats" like "tutorials" is comparatively higher because these scenarios induce LLMs to produce more logical and useful harmful guidance.

**Observation 3**: LLMs are not well aligned in scenarios referred to as "pronoun" like 'XYZ'.

**Location** Figure 5(c) indicates that LLMs' alignment in virtual worlds is weaker compared to real-world locations. Interestingly, in our samples, the locations in "universe", "fairy tale" and "programming world" showed the highest JSR scores on two different LLMs. We attribute this to the alignment training process, where training data hardly included contexts from these three worlds. We also find LLMs can be misled even by specific real-world locations, such as New York, Beijing, and Times Square, indicating the fragile alignment.

**Time** Figure 5(b) demonstrates the alignment of LLMs is insensitive to parameter Time as the JSR across different Time is almost identical, suggesting that Time is an insignificant factor in the design of automated jailbreak systems.

**Language** We focus on a broader spectrum of languages, including natural language, programming languages and cryptographic languages in Figure 5(d). First, "markup language" exhibits a higher JSR because LLM's training data includes extensive data in these formats. Consequently, LLM could generate detailed harmful content using these languages once they are jailbroken. Second, "programming language" like "Python" is more vulnerable. This is due to the limited alignment training in the "Programming language" context, resulting in a deficient alignment for LLMs. Therefore, it underscores the importance and necessity of incorporating various languages comprehensively during the alignment training process.

**Observation 4**: LLMs show weak alignment in special language including 'Markup' language' and 'Programming language'.

## 5 Conclusion

In this study, we develop MultiVerse, a novel method to generate jailbreak prompts efficiently and automatically by leveraging the inherent context-sensitivity of LLM alignment training. Specifically, we use a domain-specific language to determine the scenario, time, location, characters, and action pa

Figure 8: Example of multi-round jailbreak using prompts generated by MultiVerse

Figure 7: Ablation study of number of layers.

rameters of jailbreak prompts. MultiVersE achieves the jailbreak success rate of over 75% across 3 datasets across various aligned LLMs. Besides, we offer detailed analysis of alignment vulnerabilities on different parameters with 300 different world configurations generated by MultiVersE. We propose the existing alignment training focuses on the real-world and is lacking in various (virtual) worlds where LLMs can be exploit.