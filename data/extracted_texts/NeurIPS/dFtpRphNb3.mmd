# Cookie Consent Has Disparate Impact on

Estimation Accuracy

 Erik Miehling Rahul Nair Elizabeth Daly

Karthikeyan Natesan Ramamurthy Robert Redmond

IBM Research

erik.miehling@ibm.com

{rahul.nair,elizabeth.daly}@ie.ibm.com

{knatesa,rredmond}@us.ibm.com

###### Abstract

Cookies are designed to enable more accurate identification and tracking of user behavior, in turn allowing for more personalized ads and better performing ad campaigns. Given the additional information that is recorded, questions related to privacy and fairness naturally arise. How does a user's consent decision influence how much the system can learn about their demographic and tastes? Is the impact of a user's consent decision on the recommender system's ability to learn about their latent attributes uniform across demographics? We investigate these questions in the context of an engagement-driven recommender system using simulation. We empirically demonstrate that when consent rates exhibit demographic-dependence, user consent has a disparate impact on the recommender agent's ability to estimate users' latent attributes. In particular, we find that when consent rates are demographic-dependent, a user disagreeing to share their cookie may counter-intuitively cause the recommender agent to know more about the user than if the user agreed to share their cookie. Furthermore, the gap in base consent rates across demographics serves as an amplifier: users from the lower consent rate demographic who provide consent generally experience higher estimation errors than the same users from the higher consent rate demographic, and conversely for users who choose to withhold consent, with these differences increasing in consent rate gap. We discuss the need for new notions of fairness that encourage consistency between a user's privacy decisions and the system's ability to estimate their latent attributes.

## 1 Introduction

Increased regulation surrounding cookies has emerged in recent years in response to growing concerns over online privacy and data protection. In the EU, cookie policy is driven primarily by the General Data Protection Regulation [(13)] and the ePrivacy Directive [(10)] which dictate that if a website wishes to use cookies to identify and track users, beyond "strictly necessary cookies," then consent must be explicitly obtained from the user. There is currently no federal law regulating the use of cookies in the US, however, several states have imposed regulation that requires websites to disclose their data collection practices to users [(5, 40)].

Marketers use cookies to narrow in on target audiences most likely to buy their products, primarily via improved user tracking and enhanced personalization. The actual mechanisms for obtaining user consent can introduce some additional fairness and privacy concerns. Many cookie consent interfaces are designed such that web content is at least partially obscured until the user makes aconsent decision. These interfaces often exhibit _deceptive design patterns_ - design practices that incentivize users to make choices that lower their privacy. While some websites do offer a clear "reject all" button, many present the user with a decision between a simple "I agree" or "accept all" and a much less immediate choice of "manage preferences." This leads to many users agreeing to cookic tracking simply out of convenience . Furthermore, and an observation that forms the basis of the current paper, evidence suggests that users exhibit inherent differences in their likelihood of agreeing to cookic tracking. A recent study by YouGov  revealed that a user's consent rate is influenced by their age and culture/geographical location. Of the markets surveyed in the study, users ranged from 64% agree (Poland) to 32% agree (US), with older individuals being less likely to provide consent than younger individuals, perhaps partially due to their overall lowered trust in tech companies when it comes to their personal data .

In this paper, we investigate how demographic-dependent consent rates impact the ability of the recommender system to learn users' latent attributes and offer targeted recommendations. The main finding of our paper is that when user consent rates differ by demographics, the recommender system possesses disparate estimation accuracies across users in different demographics. The finding is based on a simple model of recommendations where, prior to the interaction, users provide cookie consent according to demographic-dependent probabilities. Users' consent decisions, along with the cookie for those who have provided consent, allow the recommender agent to form refined beliefs on users' demographics and preferences. Through sequential interaction with the users, the agent learns to personalize content to maximize engagement. We empirically illustrate the following:

* _Disparate impact of consent:_ Under demographic-dependent consent rates, users' consent decisions can have a disparate impact on the recommender agent's estimation accuracy of users' latent attributes. In particular, we find that withholding consent can lead to _lower_ estimation errors for users in the lower consent rate population. In other words, a recommender system may know more about a person who opts not be tracked compared to those who willingly provide tracking information. Additionally, users from a population with a low consent rate who provide consent experience higher estimation errors than the same users from a high consent rate population.
* _Amplification effects:_ The difference in consent rates across cohorts serves as an amplifier for the above effects, with the disparities in estimation errors across cohorts increasing in the consent rate gap.

Note that our model is simple: ads are described by a single feature, the user pool is fixed, and user affinities/preferences do not change over time. This simplicity is intentional, with the goal being to isolate and understand the effects of specific model aspects (i.e., impact of different user consent rates) without the additional noise and confounding factors present in real-world settings. The observations on disparate impact that emerge from our setting should serve as an indicator that the same negative results may also exist in more complex recommender systems. Additionally, given the fundamental nature of the observed disparities, we believe that the search for mitigation strategies in this simplified setting can provide useful insights for the design of more complex systems.

## 2 Background

**Recommender systems.** The main challenge in recommender systems is making recommendations when only a sparse set of preference data is available. Broadly, recommender systems address this missingness via two approaches: content-based filtering, and collaborative filtering.1 Content-based filtering methods use known information about users and items to suggest content. For example, a content-based movie recommender system may compare known features of a movie (degree of humor, action, drama) with known user preferences to suggest movies to users that yield the greatest similarity. Collaborative filtering, on the other hand aims to \(learn\) these relevant features based on patterns in the observed preference/response data. Items in a collaborative filtering-based system are recommended to users based on what content other users with similar behavior have consumed (hence the term collaborative).

A popular collaborative-filtering algorithm is matrix factorization. Matrix factorization asserts that user data is not missing uniformly at random, but rather according to some low rank structure of which it aims to discover. The learned factors can then be used to infer unobserved ratings for other user-item pairs, in turn driving recommendations. Associating each user with factor \(u_{i}^{k}\) and each ad with factor \(v_{a}^{k}\), latent factor estimates \((},})=((u_{i})_{i},(v_{a})_{a})\) are obtained under standard matrix factorization via solution of a regularized loss-minimization on the known user responses

\[(},})=*{argmin}_{(,)} _{(i,a)}(u_{i}^{}v_{a}-r_{i,a})^{2}+_ {i}\|u_{i}\|_{2}+_{a}\|v_{a}\|_{2}\]

where \(\) is the set of user-item pairs \((i,a)\) where a response \(r_{i,a}\) has been recorded. A variety of modifications of standard matrix factorization exist (25), including incorporating ranking biases, user and item side information, varying confidence weights, and temporal effects. The above optimization is typically carried out on a mix of both historical data and new interaction data, with the latter data incorporated periodically via retraining.

**Cookies, consent, and behavioral advertising.** Cookies are small pieces of data that are stored on a user's device with the specific purpose ranging from storing login credentials (first-party cookies) to tracking user behavior for targeted advertising (third-party cookies). Given their capability of enabling inference of potentially sensitive user information (9, 39), regulation has emerged to increase transparency around their use. While specific laws vary, many countries now require web publishers to present some form of banner or pop-up either informing the user that information is being recorded or requesting the user to indicate their cookie consent decision prior to browsing the page.

Behavioral advertising describes the practice of using learned user behavior (e.g., user preferences) to make personalized recommendations. Cookies help to facilitate inference of this information via tracking user behavior across websites (e.g., search history, clicks, purchases). The end goal of behavioral advertising is to increase relevancy of recommendations, in turn increasing user engagement and the return of the ad campaign.

## 3 Related work

**Algorithmic feedback loops.** Much of the analysis of algorithmic feedback loops in the literature is centered on recommender systems and the trade-off between recommendation accuracy and topic diversity. It is generally accepted that as the recommender system learns to generate high engagement recommendations, topic diversity suffers which in turn may cause filter bubbles and echo chambers to emerge (51, 26, 33, 22, 6, 27, 24, 11, 18, 30, 50, 14). While a variety of mitigation techniques have been proposed - including techniques to identify and remove these effects (37), slow degeneracy (24), improve user heterogeneity (43), and disentangle user interest from user conformity (49) - the question of how to best balance accuracy with diversity, and addressing broader fairness concerns, is still very much an active area of research.

The above papers illustrate the many potential downsides of engagement-driven recommendation and collaborative filtering. To the best of the authors' knowledge, our paper is the first to analyze the impact of consent rates on the dynamics of the recommendation process.

**Privacy, fairness, and bias considerations in recommender systems.** Given that recommender systems are designed to personalize content to users, questions related to privacy, fairness, and bias naturally arise (23, 12, 7). Many notions of recommender system bias have been studied in the literature including _participation inequality_ where system usage varies across a user's attributes (e.g., gender, race, language, etc.), _selection bias_ where the users' choice behavior leads to non-representative item feedback, _conformity bias_ where users in similar groups act similarly, _exposure bias_ where suppliers/items exhibit fundamentally different levels of visibility, and _popularity bias_ where popular items are recommended more often than long tail items, among various others. Furthermore, it is widely recognized that the inherent feedback nature of recommender systems amplifies these biases (6, 30).

Research on the privacy and fairness of recommender systems aims to gain a deeper understanding of how the structure of the recommender system influences these biases and subsequently propose mitigation strategies via modified recommendation algorithms, with a large thread of research concerning the design of privacy-preserving recommender systems (1, 31, 47, 3). Most related to our paper are the structural questions of characterizing the ability of the recommender system to recover protected attributes from ratings of both a homogeneous user pool (42) and a mix of public/private users (44). Notably, (44) demonstrates that only a small number of public users (users willing to share preference information) with a large number of ratings in a pool of private users (users with hidden preferences) is sufficient to produce accurate system-wide recommendations. In contrast, our paper studies how inherent differences in user consent rates influences the system's ability to estimate users' latent attributes across groups.

**Simulators for recommender systems.** Given the complexity of the interactions between users and the learning behavior of the agent, simulators have become an increasingly popular tool for understanding the dynamics of recommender systems. Agent-based modeling allows for simulation of fine-grained user interactions, facilitating useful insights without running costly field tests. A variety of open-source recommender system simulators have emerged in recent years, namely RecoGym (35), RecSim (21), PyRecGym (36), Surprise (20), RecSim NG (32), and others (48), (8). The simulator developed for purposes of this study augments RecSim with the ability for the learning agent to maintain asymmetric (Bayesian) uncertainty over users.

## 4 A Simple Recommendation Model with Cookie Consent

Consider a recommendation environment consisting of a recommender, termed the _agent_, sequentially interacting with a fixed population of \(n\) individuals or _users_ (see Fig. 1). Before the interaction begins, users make consent decisions according to known cohort-dependent probabilities. The agent uses the consent decisions and revealed cookies to form refined (interim) beliefs on the users' cohorts, which in turn guide recommendations. The agent is periodically retrained using the updated history of recommendation-response pairs, with the overall goal of recommending content that maximizes engagement.

**Advertisement model.** Each ad is described by a single _topic_ feature \(=\{^{1},,^{m}\}\), where \(m\) is the number of possible ad topics. In each round \(t\), a subset of \(l<m\) ad topics are sampled to form the current set of recommendable ads \(_{t}\), termed the _ad pool_.

**User model.** Each user \(i[n]\) is described by three features: a _cookie_\(_{i}=\{^{1},,^{c}\}\), a demographic or _cohort_\(_{i}=\{^{1},,^{d}\}\), and a vector of _topic affinities_\(_{i}_{+}^{m}\). Each user's cookie-cohort pair is drawn from a known joint prior \(()\), i.e., knowledge of a user's cookie is at least partially informative for its cohort. Consent decisions are dictated by cohort-dependent probabilities \(q_{}\), \(\), with each user \(i\) in cohort \(\) revealing its cookie with the agent according to \(X_{i}(q_{})\).2 Topic affinities are generated according to log-normal distributions, with each user \(i\) from a given cohort \(\) possessing affinities \(_{i}(_{},_{}^{2})\), where \(_{}^{m}\) and \(_{}_{+}^{m}\) are the cohort-dependent means and standard deviations of \(_{i}\)'s natural logarithm.

Figure 1: The recommendation process. The advertisement sampler generates the ad pool (assumed to be refreshed in each round \(t\)). The user sampler initializes both latent attributes of users and consent decisions. In each round, the agent generates recommendations (ads) and records user responses (clicks).

In each round \(t\), each user is faced with a single ad and makes a binary decision to either click or not click. Specifically, when user \(i\) is recommended an ad \(a\), the user first scores the ad via the utility function \(u(a;_{i})=_{i}^{}\ _{_{a}}\) where \(_{_{a}}^{m}\) is the indicator vector on topics (a vector of zeros with a one in location of the topic of ad \(a\)). User \(i\)'s choice to click on \(a\), denoted by \(c_{i,a}\{0,1\}\), is stochastic and is dictated by \(c_{i,a}(p_{i,a})\), where \(p_{i,a}\) is the click probability given by the following logit model

\[p_{i,a}=))}{(u_{0})+(u(a;_{i}))}\] (1)

where \(u_{0}\) is the _no-click-mass_ used to model the possibility of the user not clicking on \(a\), assumed to be homogenous across users.

**Recommender agent.** The agent faces a fundamental trade-off between recommending high-engagement content and learning more about users' tastes. Our recommendation model combines two foundational models from the literature, namely incorporation of confidence weights [(19, 25)] into an online matrix factorization procedure [(41)]. Online matrix factorization addresses the conflicting objectives of the agent - exploration to improve estimates of latent factors, and exploitation of known high engagement content - by interleaving estimation of the latent factors with specification of recommendations. Including confidence weights allow us to model the agent's heterogeneous uncertainty on users due to their consent decisions and personalized recommendations. Note that while this is a stylized recommender model, it possesses the main features seen in more complex systems, specifically online learning and heterogeneous uncertainty across users.

Formally, the recommendation process evolves as follows. Given the agent's prior belief, \(()\), the users' consent decisions are used to form interim cohort beliefs \(_{i}()\) for each user \(i\). If user \(i\) gives consent (\(x_{i}=1\)) then the agent is informed of the user's cookie, \(_{i}\), and its updated belief of the user's cohort is formed as \(_{i}()=p( x_{i}=1,_{i})=(_{i},)}{_{^{}}q_{^{}}( _{i},^{})}\) for each \(\). If user \(i\) does not provide consent (\(x_{i}=0\)), no cookie information is revealed and the agent's belief of the user's cohort is given by \(_{i}()=p( x_{i}=0)=)_{ }(,)}{_{^{}}(1-q_{^{} })_{}(^{},^{})}\), \(\). Interim beliefs \(=(_{1},,_{n})\) are used to form beliefs \(_{0}=(_{0,1},,_{0,n})\) via an offline response set \(_{0}\) of recommendation-response pairs of the form \(\{(a_{i},c_{i,a})\}\) across users.

The recommender model at round \(t\) is represented by a pair of user-ad latent factor estimates \((}_{t},}_{t})\). Recommendations are generated via an \(\)-greedy bandit: with probability \(\), the ad recommended to user \(i\) in round \(t\), denoted by \(a_{i,t}\), is chosen uniformly at random, \(a_{i,t}(_{t})\), and with probability \(1-\), the recommendation is the ad in the current ad pool with the highest estimated value \(a_{i,t}=*{argmax}_{a_{t}}_{t,t}^{}_{a,t}\). Responses to the recommendations are appended to the response set, \(_{t+1}=_{t}\{(a_{i},c_{i,a}) i[n],a_{i}[m]\}\), and are used to maintain a cumulative count of clicks \(r_{i,a,t}\) across user-ad pairs.

Retraining consists of updating cohort beliefs and latent factor estimates using recommendation-response pairs, and is performed every \(T_{b}\) rounds. If \(t\) is a retraining round, cohort beliefs are first updated according to a Bayesian update \(_{i,t}=f(_{i,t-1},_{t}_{t-T_{b}})\) where \(_{t}_{t-T_{b}}\) is the set of responses since the previous retraining round. The heterogeneous beliefs on user cohorts gives rise to a weighted procedure. For a given set of cohort beliefs \(_{t}\), the agent computes confidence weights \(_{t}=(_{i,a,t})_{i,a}\) as the expected probability for seeing the current response counts. Specifically, each weight \(_{i,a,t}\) is the expected binomial probability, given \(_{t}\), for seeing \(r_{i,a,t}\) defined as

\[_{i,a,t}=_{_{i}_{i,t}}[p_{t}(I_{i,a,t},r_{i,a,t },_{i})]\]

where \(p_{t}(I,r,)\) is the binomial probability at round \(t\) given impressions \(I\), positive response counts \(r\), and cohort \(\). Updated latent factor estimates are computed via a confidence-weighted matrix factorization procedure as

\[(}_{t},}_{t})=*{argmin}_{(u,v) }_{(a_{i},c_{i,a})_{t}}\!\!\! _{i,a,t}(u_{i}^{}v_{a}-r_{i,a,t})^{2}+_{i[n]}\|u_{i }\|_{2}+_{a[m]}\|v_{a}\|_{2}\] (2)

where \(=\{(u_{1},,u_{n}) u_{i}^{k},i[n]\}\), \(=\{(v_{1},,v_{m}) v_{a}^{k},a[m]\}\) are the latent factor spaces, \(_{t}\) is the current history of responses, and \(>0\) is a regularization weight. If \(t\) is not a retraining round, then cohort beliefs and latent factor estimates are propagated as is from the previous round. Expressions for the Bayesian updates and expected binomial probabilities can be found in Appendix A, with pseudocode of the recommendation process in Appendix B.

Experiments

The following experiments study the dynamics of the recommendation process described in Section 4 particularly as it relates to users' consent rates. Empirical results were obtained via a simulator based on RecSim (21). Full details of the simulator and the experimental setup can be found in the supplementary material (Appendix C). Base model parameters assumed throughout this section are: number of users \(n=1000\), number of ads \(m=200\), ad pool size \(l=50\), and number of cohorts \(d=2\). Sensitivity analyses can be found in Appendix D.

The following notation/terminology will be used throughout this section to aid explanations. Let \(N_{1}^{}=\{i[n] x_{i}=1,_{i}=\}\) denote the set of users in cohort \(\) who have decided to share their cookie, with \(N_{0}^{}\) defined analogously. We refer to \(N_{1}^{}\) as the _consent group_ (in cohort \(\)) and \(N_{0}^{}\) as the _non-consent group_. Estimation errors on cohorts for each group are quantified by log loss, i.e., for a set of users \(N\) in group \((,x)\), the (average) log loss is denoted by \(L_{x}^{}()=-_{i N}_{} (_{i}=)(_{i}())\). Estimation errors on topic affinities are given by MSE.

### Impact of consent on estimation accuracy

The following set of experiments investigates the impact of cohort-dependent consent rates on the agent's estimates of the users' cohorts and topic affinities. Given users' consent decisions, and cookies for those who decided to provide consent, the agent forms interim beliefs on each user, \(_{i}\), \(i[n]\), as per the model description in Section 4. Fig. 2 illustrates the impact of these consent decisions on cohort and affinity estimation errors, for \(=\{,^{}\}\), in two cases: homogeneous consent rates (Fig. 2(a)) and heterogeneous consent rates (Fig. 2(b)).

Fig. 2(a) illustrates that when users' consent rates are identical across cohorts, estimation errors for the non-consent group \(N_{0}^{}\) are higher than the errors for the consent group \(N_{1}^{}\) regardless of their cohort \(\).

As seen in Fig. 2(b), this intuitive result breaks down when users' consent rates differ across cohorts. While withholding consent leads to a higher estimation error for the higher consent rate group, \(^{}\), withholding consent can lead to a _lower_ estimation error for the lower consent rate group, \(\). This inversion effect persists for the lower consent rate population for any sufficiently large separation of consent rates across cohorts, as illustrated by Fig. 3. These cohort-dependent effects of consent are summarized by Observation 1.

**Observation 1** (Comparison within cohorts).: _Let \(=\{,^{}\}\) and let \(\) be any non-fully informative prior. If \(q_{}=q_{^{}}\) (consent rates are independent of cohort) then non-consent leads to a higher estimation error than consent. If \(q_{}<q_{^{}}\) (consent rates are cohort-dependent with users in cohort \(\) providing consent as a lower rate than users in cohort \(^{}\)), then:_

1. _There exists a (prior-dependent) constant_ \(_{}\) _such that for_ \(q_{^{}}-q_{}>_{}\)_, the non-consent group in cohort_ \(\)_,_ \(N_{0}^{}\)_, experiences lower estimation errors than the consent group,_ \(N_{1}^{}\)_._
2. _The non-consent group in cohort_ \(^{}\)_,_ \(N_{0}^{^{}}\)_, experiences higher estimation errors than the consent group,_ \(N_{1}^{^{}}\)_._

Figure 2: Estimation errors over training rounds \(t\) (\(T_{b}=1\)) as a function of consent rates under binary cookie space \(=\{,^{}\}\) and symmetric prior \((,)=(^{},^{})=0.4\).

Observation 1 holds for any non-fully informative prior, meaning that there exists at least one cookie that does not reveal the user's cohort with certainty - a property that is almost certainly satisfied in practical (i.e., noisy) recommender systems.

For partially informative priors (each cookie only partially reveals the user's cohort), the statement of the above observation can be understood by looking at the agent's sources of information. Consider the following signals: i) the cookie value in the event that the user provided consent and, ii) under heterogeneous consent rates, the consent decision itself. The interplay between these two sources of information means that, under some priors, the act of a user withholding consent can itself be more informative than providing consent and revealing their cookie. Fig. 3 illustrates the estimation errors for different consent rate regimes under symmetric prior \((,)=(^{},^{})=0.45\).

The above figure, specifically Fig. 3(a), illustrates that under a very informative prior, a small gap in consent rates, \((q_{},q_{^{}})=(0.2,0.4)\), yields a consent signal that contains relatively low informativeness compared to the revealed cookie value, causing consent to yield lower estimation errors for both cohorts. However, as the absolute consent rates increase in both cohorts, this same consent rate gap (compare Figs. 3(a) and (c)) yields a consent signal that is sufficiently informative to yield lower estimation errors for withholding consent for the lower consent rate group.

Generally the larger the gap in consent rates, the more information the consent signal carries for the user's cohort. Apart from very informative priors, the consent signal carries sufficiently rich information to yield lower estimation errors for withholding consent for the lower consent rate population even for small gaps in the consent rates.

Observation 1 compares the impact of consent decisions within a given cohort under homogeneous and heterogeneous consent rates. A comparison of the impact of the same consent decision across users in different cohorts is summarized by the following observation.

**Observation 2** (Comparison across cohorts).: _Let \(=\{,^{}\}\) and let \(\) be any partially informative prior. If \(q_{}=q_{}^{}\), then the consent group experiences the same estimation errors regardless of cohort (similarly for non-consent). If \(q_{}<q_{^{}}\), then:_

1. _There exists a (prior-dependent) constant_ \(_{}\) _such that for_ \(q_{^{}}-q_{}>_{}\)_, the non-consent group in cohort_ \(^{}\)_,_ \(N_{0}^{^{}}\)_, experiences higher estimation errors than the non-consent group in cohort_ \(\)_,_ \(N_{0}^{}\)_._
2. _The consent group in cohort_ \(^{}\)_,_ \(N_{1}^{^{}}\)_, experiences lower estimation errors than the consent group in cohort_ \(\)_,_ \(N_{1}^{}\)_._

### Amplification effects

The following experiments investigate how the disparities observed in (O.1) and (O.2) are influenced by the relative consent rates across cohorts. For \(=\{,^{}\}\), define \(_{t}^{}=_{0}^{}(_{t})-_{1}^{}( _{t})\) as the difference in mean cohort errors between non-consent and consent for users in cohort \(\). Additionally, define \(_{t}^{x}=_{x}^{^{}}(_{t})-_{x}^{}( _{t})\) as the difference in mean cohort errors for consent decision \(x\).

Figure 3: Impact of consent versus non-consent on the agentâ€™s cohort estimation error in each cohort for various consent rate regimes under symmetric prior \((,)=(^{},^{})=0.45\).

In words, the quantities \(^{}_{t}\), \(\), represent how much worse the agent's estimation error is under non-consent versus consent for users in cohort \(\). The quantity \(^{0}_{t}\) (resp. \(^{1}_{t}\)) represents how much higher the agent's estimation error is for a user who withheld (resp. provided) consent if they belonged to cohort \(^{}\) versus if they belonged to cohort \(\). Figs. 4 and 5 illustrate how these quantities differ as a function of consent rates for a given number of training rounds (\(=10\)).

Fig. 4 illustrates that the relative loss in accuracy for non-consent (compared to consent) is greatest in cohort \(\) when \(q_{}\) is minimal and \(q_{^{}}\) is maximal, with the opposite effects observed in cohort \(^{}\). Fig. 5 illustrates that the users who withhold consent are on average impacted greater by a given gap in consent rates than users who consent, e.g., compare the magnitude of the values at \((q_{},q_{^{}})=(0.25,0.75)\) for non-consent versus consent.

In summary, the gap in consent rates serves as an amplifier for an individual user's consent decision. Users from the higher consent rate cohort who withhold consent experience higher estimation errors than the same users from the lower consent rate cohort, with the opposite holding for users in the consent group.

Figure 4: Relative errors for non-consent versus consent as a function of consent rates \((q_{},q_{^{}})\) for each cohort and fixed number of training rounds \(=10\).

Figure 5: Relative errors for a given consent decision across cohorts as a function of consent rates \((q_{},q_{^{}})\) for fixed number of training rounds \(=10\).

Discussion

Our results demonstrate that even in a simple recommendation model with reasonable modeling assumptions, the impact of a user's cookie consent decision is not necessarily straightforward. Particularly, when consent rates exhibit demographic-dependence, the user's consent decision itself acts as a signal, carrying information beyond the information contained in the actual cookie value. This leads to a demographic-dependent impact of consent, resulting in a user's privacy-protective decision (of disagreeing to share their cookie) potentially causing the system to know more about them than if the user were to intentionally reveal their cookie information.

To draw a comparison with the large body of existing literature in the fairness of recommender systems, many of the existing papers (see the papers referenced in Section 3) draw attention to the way in which the system learns similarities among users over the recommendation process as a primary source of unfairness. Indeed, this is fundamental to how matrix factorization algorithms (and many more complex recommender systems) operate: users who behave similarly, i.e., how they respond to the same recommendations, will be deemed as being similar (even falsely so). This will lead to these users seeing similar recommendations which can, in turn, lead to undesirable outcomes.

However, one of the goals of our paper is to draw attention to the informational effects of consent as a potential additional source of unfairness. When consent is a conscious decision of the user, and the rate of a given consent decision differs based on sensitive attributes of the user, e.g., age, then the system can use the consent decision to learn more about the user's sensitive attributes. When embedded within an algorithmic system, platforms may unintentionally profile users based on the absence of data.

These observations have possible implications in the design of recommender systems. Any information that the system can use to draw similarities will be used, most notably how users respond to recommendations, but also, and the focus of our paper, the inherent likelihood of a user to provide consent to cookie sharing. These similarities are used to infer to which group a user belongs and subsequently drive recommendations. The implications for algorithm design is that one must be mindful of the complexities of information revelation especially as it relates to how the algorithm will use the revealed information.

**New notions of fairness.** The design of recommender systems that are more aware of the informational effects of users' decisions may require new notions of fairness. It is well-known in the fairness community that static fairness metrics are insufficient for dynamic settings, and can actually actively harm the group they intend to protect [(28)].

In our setting, whether the system possessing heterogeneous accuracy about users results in a fair or unfair outcome depends on the product being recommended. Given our observations that the specific impact of a user's consent decision varies depending on the user's demographic, it is not hard to see that such accuracy disparity may lead to unfair situations. For example, consider an ad recommendation setting where some of the ads may be predatory to older individuals. According to our model, an older individual's desire to remain more private by deciding to disagree to cookie sharing may allow the system to know more about them, simply by virtue that more older individuals choose to not share their cookies, and subsequently make them a more likely target for such ads. Consequently, we emphasize the need for development of fairness metrics that ensure consistency of an individual user's privacy-protective decision and the amount (or accuracy) of information that the system knows about that user.

It is worth noting that this informational consistency property is distinct from the literature on privacy-preserving recommender systems. For instance, differentially-private recommender systems [(31, 29)] ensure individual-anonymity, via careful injection of noise, while still enabling extraction of aggregate-level information used to feed the recommendation algorithm. In contrast, a consistency-based approach would hypothetically still allow for some individuals to be (more accurately) identified if they choose to be (e.g., to receive more targeted recommendations) while honoring the privacy of individuals who chose to not share their information.

**Towards industry reform.** Our results contribute to the growing sentiment that the digital advertising space is in need of reform, particularly due to the unintended consequences of "the collection of personal data, tracking, and massive-scale profiling" [(2)]. As the move away from cookie-based tracking gains momentum, alternative mechanisms for user profiling are becoming more prevalent.

In the context of an advertising environment without cookies (often referred to as the _cookieless_ future), the shift to these alternative data collection methods may lead to informational disparities across companies. Given that the current advertising ecosystem contains players of vastly different sizes, ending the use of cookies may give asymmetric power to companies that have already collected significant amounts of personal data. Indeed, tools that facilitate access to user information that would otherwise be tracked by cookies are already in use, e.g., websites that allow users to create an account by using their Google/Microsoft credentials (a practice known as "auto-linking").

In general, these issues point to the fact that controls and regulations should be based on the downstream effects of user interactions with the advertising system. In particular, policy must carefully consider how much information has been extracted from a given user, whether it be from individual decisions/responses of the user or gleaned from inferred similarities with other users, and its privacy-compromising effects. To foster transparency, it would be beneficial for the industry to derive estimates of the monetary value of user information and share them with the public. In general, the advertising industry can be more transparent, not just about user data but also concerning the downstream effects on users, since real-world advertising engines are vastly more complex than our simplified setting and may not lend themselves to simple mathematical modeling.

**Limitations.** Our findings on the disparate impact of consent should be interpreted with the understanding that our model necessarily has some limitations. Broadly, our model uses a simplified definition of cookies in which cookies serves as a proxy for the users' cohorts. While the reason for this simplicity is to extract insights that depend directly on the user's cookie consent decisions, extending the definition of a cookie to more realistic settings by including user click/behavioral information would likely generate additional insights. Secondly, to capture the core aspects of recommender systems, our recommendation model is based on foundational recommendation algorithms (namely online + confidence-weighted matrix factorization). While these algorithms form the basis for modern recommender systems, it would be worthwhile to see how the insights extend to more modern algorithms. Lastly, we consider simplified ad and user pools; consideration of additional ad features and a more realistic dynamic user pool could influence the findings.

## 7 Concluding remarks and future directions

We've investigated the question of how heterogeneous cookie consent rates among users influence the recommender agent's ability to learn users' latent features (e.g., their demographics and tastes). We've empirically discovered, through construction of a simulator, that disparities in the agent's estimation accuracy across users emerge when consent rates exhibit demographic-dependence. Our observations show that seemingly simple informational decisions by users (i.e., whether to share their cookie) can have complex effects on the agent's information. Consideration of informational effects of users' decisions is crucial in the design of recommender systems. We encourage the development of new fairness metrics for dynamic settings that enforce consistency between a user's privacy decision and the amount of information that the system knows about them.

Future work focuses on studying the downstream effects of our observations via validation in real-world recommender systems. An additional direction is quantification of the aforementioned fairness consistency property such that it can be embedded as a constraint in the design of recommender system algorithms.

**Acknowledgments.** This research was funded in part by the Horizon Europe project AutoFair (grant agreement ID: #101070568).