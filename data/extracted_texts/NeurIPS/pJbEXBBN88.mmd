# Adversarial Examples Exist in Two-Layer ReLU Networks for Low Dimensional Linear Subspaces

Odelia Melamed

Weizmann Institute of Science, Israel, odelia.melamed@weizmann.ac.il

Gilad Yehudai

Weizmann Institute of Science, Israel, gilad.yehudai@weizmann.ac.il

Gal Vardi

TTI-Chicago and the Hebrew University of Jerusalem, galvardi@ttic.edu.

In this paper we focus on data which lies on a low dimensional "data-manifold". Specifically, we assume that the data lies on a linear subspace \(P^{d}\) of dimension \(d-\) for some \(>0\). We study adversarial perturbations in the direction of \(P^{}\), i.e. orthogonal to the data subspace. We show that the gradient projected on \(P^{}\) is large, and in addition there exist a _universal adversarial perturbation_ in a direction orthogonal to \(P\). Namely, the same small adversarial perturbation applies to many inputs. The norm of the gradient depends on the term \(\), while the perturbation size depends on the term \(\), i.e. a low dimensional subspace implies reduced adversarial robustness. Finally, we also study how changing the initialization scale or adding \(L_{2}\) regularization affects robustness. We show that in our setting, decreasing the initialization scale, or adding a sufficiently large regularization term, can make the network significantly more robust. We also demonstrate empirically the effects of the initialization scale and regularization on the decision boundary. Our experiments suggest that these effects might extend to deeper networks.

## 2 Related Works

Despite extensive research, the reasons for the abundance of adversarial examples in trained neural networks are still not well understood (Goodfellow et al., 2014; Fawzi et al., 2018; Shafahi et al., 2018; Schmidt et al., 2018; Khoury and Hadfield-Menell, 2018; Bubeck et al., 2019; Allen-Zhu and Li, 2020; Wang et al., 2020; Shah et al., 2020; Shamir et al., 2021; Ge et al., 2021; Wang et al., 2022; Dohmatob and Bietti, 2022). Below we discuss several prior works on this question.

In a string of works, it was shown that small adversarial perturbations can be found for any fixed input in certain ReLU networks with random weights (drawn from the Gaussian distribution). Building on Shamir et al. (2019), it was shown in Daniely and Shacham (2020) that small adversarial \(L_{2}\)-perturbations can be found in random ReLU networks where each layer has vanishing width relative to the previous layer. Bubeck et al. (2021) extended this result to two-layer neural networks without the vanishing width assumption, and Bartlett et al. (2021) extended it to a large family of ReLU networks of constant depth. Finally, Montanari and Wu (2022) provided a similar result, but with weaker assumptions on the network width and activation functions. These works aim to explain the abundance of adversarial examples in neural networks, since they imply that adversarial examples are common in random networks, and in particular in random initializations of gradient-based methods. However, trained networks are clearly not random, and properties that hold in random networks may not hold in trained networks. Our results also involve an analysis of the random initialization, but we consider the projection of the weights onto the linear subspace orthogonal to the data, and study its implications on the perturbation size required for flipping the output's sign in trained networks.

In Bubeck et al. (2021) and Bubeck and Sellke (2021), the authors proved under certain assumptions, that overparameterization is necessary if one wants to interpolate training data using a neural network with a small Lipschitz constant. Namely, neural networks with a small number of parameters are not expressive enough to interpolate the training data while having a small Lipschitz constant. These results suggest that overparameterization might be necessary for robustness.

Vardi et al. (2022) considered a setting where the training dataset \(\) consists of nearly-orthogonal points, and proved that every network to which gradient flow might converge is non-robust w.r.t. \(\). Namely, building on known properties of the implicit bias of gradient flow when training two-layer ReLU networks w.r.t. the logistic loss, they proved that for every two-layer ReLU network to which gradient flow might converge as the time \(t\) tends to infinity, and every point \(_{i}\) from \(\), it is possible to flip the output's sign with a small perturbation. We note that in Vardi et al. (2022) there is a strict limit on the number of training samples and their correlations, as well as the training duration. Here, we have no assumptions on the number of data points and their structure, besides lying on a low-dimensional subspace. Also, in Vardi et al. (2022) the adversarial perturbations are shown to exist only for samples in the training set, while here we show existence of adversarial perturbation for any sample which lies on the low-dimensional manifold.

It is widely common to assume that "real-life data" (such as images, videos, text, etc.) lie roughly within some underlying low-dimensional data manifold. This common belief started many successful research fields such as GAN (Goodfellow et al., 2014), VAE (Kingma and Welling, 2013), and diffusion (Sohl-Dickstein et al., 2015). In Fawzi et al. (2018) the authors consider a setting where the high dimensional input data is generated from a low-dimensional latent space. They theoretically analyze the existence of adversarial perturbations on the manifold generated from the latent space,although they do not bound the norm of these perturbations. Previous works analyzed adversarial perturbations orthogonal to the data manifold. For example, Khoury and Hadfield-Menell (2018) considering several geometrical properties of adversarial perturbation and adversarial training for low dimensional data manifolds. Tanay and Griffin (2016) analyzed theoretically such perturbations for linear networks, and Stutz et al. (2019) gave an empirical analysis for non-linear models. Moreover, several experimental defence methods against adversarial examples were obtained, using projection of it onto the data manifold to eliminate the component orthogonal to the data (see, e.g., Jalal et al. (2017); Meng and Chen (2017); Samangouei et al. (2018)).

Shamir et al. (2021) showed empirically on both synthetic and realistic datasets that the decision boundary of classifiers clings onto the data manifold, causing very close off-manifold adversarial examples. Our paper continues this direction, and provides theoretical guarantees for off-manifold perturbations on trained two-layer ReLU networks, in the special case where the manifold is a linear subspace.

## 3 Setting

Notations.We denote by \((A)\) the uniform distribution over a set \(A\). The multivariate normal distribution with mean \(\) and covariance \(\) is denoted by \((,)\), and the univariate normal distribution with mean \(a\) and variance \(^{2}\) is denoted by \((a,^{2})\). The set of integers \(\{1,..,m\}\) is denoted by \([m]\). For a vector \(v^{n}\), we define \(v_{i:i+j}\) to be the \(j+1\) coordinates of \(v\) starting from \(i\) and ending with \(i+j\). For a vector \(x\) and a linear subspace \(P\) we denote by \(P^{}\) the subspace orthogonal to \(P\), and by \(_{P^{}}(x)\) the projection of \(x\) on \(P^{}\). We denote by \(\) the zero vector. We use \(I_{d}\) for the identity matrix of size \(d\).

### Architecture and Training

In this paper we consider a two-layer fully-connected neural network \(N:^{d}\) with ReLU activation, input dimension \(d\) and hidden dimension \(m\):

\[N(x,_{1:m})=_{i=1}^{m}u_{i}(w_{i}^{}x)\;.\]

Here, \((z)=(z,0)\) is the ReLU function and \(_{1:m}=(w_{1},,w_{m})\). When \(_{1:m}\) is clear from the context, we will write for short \(N(x)\).

We initialize the first layer using standard Kaiming initialization (He et al., 2015), i.e. \(w_{i}(,I_{d})\), and the output layer as \(u_{i}(\{-},}\})\) for every \(i[m]\). Note that in standard initialization, each \(u_{i}\) would be initialized normally with a standard deviation of \(}\), for ease of analysis we fix the initial value to be equal to the standard deviation where only the sign is random.

We consider a dataset with binary labels. Given a training dataset \((x_{1},y_{1}),,(x_{r},y_{r})^{d}\{-1,1\}\) we train w.r.t. the logistic loss (a.k.a. binary cross entropy): \(L(q)=(1+e^{-q})\), and minimize the empirical error:

\[_{w_{1},,w_{m}}_{i=1}^{r}L(y_{i} N(x_{i},_{1: m}))\;.\]

We assume throughout the paper that the network is trained using either gradient descent (GD) or stochastic gradient descent (SGD). Our results hold for both training methods. We assume that only the weights of the first layer (i.e. the \(w_{i}\)'s) are trained, while the weights of the second layer (i.e. the \(u_{i}\)'s) are fixed.

### Assumptions on the Data

Our main assumption in this paper is that the input data lie on a low dimensional manifold, which is embedded in a high dimensional space. Specifically, we assume that this "data manifold" is a linear subspace, denoted by \(P\), which has dimension \(d-\). We denote by \(\) the dimension of the data "off-manifold", i.e. the linear subspace orthogonal to the data subspace, which is denoted by \(P^{}\). In this work we study adversarial perturbations in \(P^{}\). Note that adding a perturbation from \(P^{}\) of any size to an input data point which changes its label is an unwanted phenomenon, because this perturbation is orthogonal to any possible data point from both possible labels. We will later show that under certain assumptions there exists an adversarial perturbation in the direction of \(P^{}\) which also has a small norm. This reason for this assumption is so that the projection of the first layer weights on \(P^{}\) remain fixed during training. An interesting question is to consider general "data manifolds", which we elaborate on in Section 7.

To demonstrate that the low-dimensional data assumption arises in practical settings, in Figure 1 we plot the cumulative variance of the MNIST and CIFAR10 datasets, projected on a linear manifold. These are calculated by performing PCA on the entire datasets, and summing over the square of the singular values from largest to smallest. For CIFAR10, the accumulated variance reaches \(90\%\) at \(98\) components, and \(95\%\) at \(216\) components. For MNIST, the accumulated variance reaches \(90\%\) at \(86\) components, and \(95\%\) at \(153\) components. This indicates that both datasets can be projected to a much smaller linear subspace, without losing much of the information.

**Remark 3.1** (On the Margin of the Network).: _Given a neural network \(N:^{d}\) and a dataset \((x_{1},y_{1}),,(x_{r},y_{r})\) with binary labels which the network label correctly, we define the margin of the network as \(:=_{i[r]}y_{i}N(x_{i})\)._

_In our setting, it is possible to roughly estimate the margin without assuming much about the data, besides its boundedness. Note that the gradient of the loss decays exponentially with the output of the network, because \(||=|}{1+e^{-q}}|\). Hence, if we train for at most polynomially many iterations and label all the data points correctly (i.e. the margin is larger than \(0\)), then training effectively stops after the margin reaches \(O(^{2}(d))\). This is because if the margin is \(^{2}(d)\), then the gradient is of size:_

\[|L^{}(^{2}(d))|=|(d)e^{- ^{2}(d)}}{1+e^{-^{2}(d)}}|^{2}(d) d^{-(d)}\,\]

_which is smaller than any polynomials in \(d\). This means that all the data points on the margin (which consists of at least one point) will have an output of \(O((d))\)._

_The number of points which lie exactly on the margin is difficult to assess, since it may depend on both the dataset and the model. Some empirical results in this direction are given in Vardi et al. (2022), where it is observed (empirically) that for data sampled uniformly from a sphere and trained with a two-layer network, over \(90\%\) of the input samples lie on the margin. Also, in Haim et al. (2022) it is shown that for CIFAR10, a large portion of the dataset lies on the margin._

Figure 1: The cumulative variance for the (a) CIFAR10; and (b) MNIST datasets, calculated by performing PCA on the entire datasets, and summing over the square of the singular values from largest to smallest.

Large Gradient Orthogonal to the Data Subspace

One proxy for showing non-robustness of models, is to show that their gradient w.r.t. the input data is large (cf. Bubeck et al. (2021); Bubeck and Sellke (2021)). Although a large gradient does not guarantee that there is also an adversarial perturbation, it is an indication that a small change in the input might significantly change the output. Moreover, by assuming smoothness of the model, it is possible to show that having a large gradient may suffice for having an adversarial perturbation.

In this section we show that training a network on a dataset which lies entirely on a linear subspace yields a large gradient in a direction which is orthogonal to this subspace. Moreover, the size of the gradient depends on the dimension of this subspace. Specifically, the smaller the dimension of the data subspace, the larger the gradient is in the orthogonal direction. Our main result in this section is the following:

**Theorem 4.1**.: _Suppose that a network \(N(x)=_{i=1}^{m}u_{i}( w_{i},x)\) is trained on a dataset which lies on a linear subspace \(P^{d}\) of dimension \(d-\) for \( 1\), and let \(x_{0} P\). Let \(S=\{i[m]: w_{i},x_{0} 0\}\), and let \(k:=|S|\). Then, \(w_{p} 1-e^{-/16}\) (over the initialization) we have:_

\[\|_{P^{}}()}{ x}) \|}\;.\]

The full proof can be found in Appendix B. Here we provide a short proof intuition: First, we use a symmetry argument to show that it suffices to consider w.l.o.g. the subspace \(M:=\{e_{1},,e_{d-}\}\), where \(e_{i}\) are the standard unit vectors. Next, we note that since the dataset lies on \(M\), only the first \(d-\) coordinates of each weight vector \(w_{i}\) are trained, while the other \(\) coordinates are fixed at their initial value. Finally, using standard concentration result on Gaussian random variables we can lower bound the norm of the gradient. Note that our result shows that there might be a large gradient orthogonal to the data subspace. This correspond to "off-manifold" adversarial examples, while the full gradient (i.e. without projecting on \(P^{}\)) might be even larger.

The lower bound on the gradient depends on two terms: \(\) and \(\). The first term is the fraction of active neurons for the input \(x_{0}\), i.e. the neurons whose inputs are non-negative. Note that inactive neurons do not increase the gradient, since they do not affect the output. The second term corresponds to the fraction of directions orthogonal to the data. The larger the dimension of the orthogonal subspace, the more directions in which it is possible to perturb the input while still being orthogonal to the data. If both of these terms are constant, i.e. there is a constant fraction of active neurons, and "off-manifold" directions, we can give a more concrete bound on the gradient:

**Corollary 4.1**.: _For \(=(d)\), \(k=(m)\), in the setting of Theorem 4.1, with probability \( 1-e^{-(d)}\) we have:_

\[\|_{P^{}}()}{ x}) \|=(1)\;.\]

Consider the case where the norm of each data point is \(()=()\), i.e. every coordinate is of size \((1)\). By Remark 3.1, for a point \(x_{0}\) on the margin, its output is of size \((d)\). Therefore, for the point \(x_{0}\), gradient of size \((1)\) corresponds to an adversarial perturbation of size \((d)\), which is much smaller than \(\|x_{0}\|=()\). We note that this is a rough and informal estimation, since, as we already discussed, a large gradient at \(x_{0}\) does not necessarily imply that an adversarial perturbation exists. In the next section, we will prove the existence of adversarial perturbations.

## 5 Existence of an Adversarial Perturbation

In the previous section we have shown that at any point \(x_{0}\) which lies on the linear subspace of the data \(P\), there is a large gradient in the direction of \(P^{}\). In this section we show that not only the gradient is large, there also exists an adversarial perturbation in the direction of \(P^{}\) which changes the label of a data point from \(P\) (under certain assumptions). The main theorem of this section is the following:

**Theorem 5.1**.: _Suppose that a network \(N(x)=_{i=1}^{m}u_{i}( w_{i},x)\) is trained on a dataset which lies on a linear subspace \(P^{d}\) of dimension \(d-\), where \( 32(m-1)(m^{2}d)\). Let \(x_{0} P\)_and denote \(y_{0}:=(N(x_{0}))\). Let \(I_{-}:=\{i[m]:u_{i}<0\}\) and \(I_{+}:=\{i[m]:u_{i}>0\}\), and denote \(k_{-}:=|\{i I_{-}: w_{i},x_{0} 0\}|\), and \(k_{+}:=|\{i I_{+}: w_{i},x_{0} 0\}|\). Let \(k_{y_{0}}=k_{-}\) if \(y_{0}=1\) and \(k_{y_{0}}=k_{+}\) if \(y_{0}=-1\). For \(w^{d}\) denote \(:=_{P^{}}(w)\), and denote the perturbation \(z:=y_{0}(_{i I_{-}}_{i}-_{i  I_{+}}_{i})\) where \(=dN(x_{0})}{ k_{y_{0}}}\). Then, w.p. \( 1-5(me^{-/16}+d^{-1/2})\) we have that \(\|z\| 8N(x_{0})}}}\) and:_

\[(N(x_{0}+z))(N(x_{0}))\;.\]

The full proof can be found in Appendix C. Here we give a short proof intuition: As in the previous section, we show using a symmetry argument that w.l.o.g. we can assume that \(P=\{e_{1},,e_{d-}\}\).

Now, given the perturbation \(z\) from Theorem 5.1 we want to understand how adding it to the input changes the output. Suppose that \(y_{0}=1\). We can write

\[N(x_{0}+z)=_{i I_{-}}u_{i}( w_{i},x_{0}+ w_{ i},z)+_{i I_{+}}u_{i}( w_{i},x_{0}+ w _{i},z)\]

We can see that for all \(i\):

\[ w_{i},z = w_{i},_{j I_{-}}_{P^{}}(w_{j}) -_{j I_{+}}_{P^{}}(w_{j})\] \[=- w_{i},_{j=1}^{m}(u_{j})_{P ^{}}(w_{j}).\]

For \(i I_{-}\) we can write:

\[ w_{i},z=\,\|_{P^{}}(w_{i})\|^{2}- _{P^{}}(w_{i}),_{j i}(u_{j})_{P^{}}(w_{j}) \;,\]

and using a similar calculation, for \(i I_{+}\) we can write:

\[ w_{i},z=-\,\|_{P^{}}(w_{i})\|^{2}- _{P^{}}(w_{i}),_{j i}(u_{j})_{P^{}}(w_{j}) \;.\]

Using concentration inequalities of Gaussian random variables, and the fact that \(_{P^{}}(w_{i})\) did not change from their initial values, we can show that:

\[|_{P^{}}(w_{i}),_{j i}(u_{j})_{P^{ }}(w_{j})|}{d}\;,\]

while \(\|_{P^{}}(w_{i})\|^{2}\). Thus, for a large enough value of \(\) we have that \( w_{i},z 0\) for \(i I_{+}\) and \( w_{i},z}{d}(-)\) for \(i I_{-}\).

From the above calculations we can see that adding the perturbation \(z\) does not increase the output of the neurons with a positive second layer. On the other hand, adding \(z\) can only increase the input of the neurons with negative second layer, and for those neurons which are also active it increases their output as well if we assume that \(>m\). This means, that if there are enough active neurons with a negative second layer (denoted by \(k_{-}\) in the theorem), then the perturbation can significantly change the output. In the proof we rely only on the active negative neurons to change the label of the output (for the case of \(y_{0}=1\), if \(y_{0}=-1\) we rely on the active positive neurons). Note that the active positive neurons may become inactive, and the inactive negative neurons may become active. Without further assumptions it is not clear what is the size of the perturbation to make this change for every neuron. Thus, the only neurons that are guaranteed to help change the label are the active negative ones, which by our assumptions on \(\) are guaranteed to increase their output.

Note that our perturbation is _not_ in the direction of the gradient w.r.t. the input. The direction of the gradient would be the sum of all the active neurons, i.e. the sum (with appropriate signs) over all \(i[m]\) such that \( w_{i},x_{0} 0\). Our analysis would not have worked with such a perturbation, because we could not guarantee that inactive neurons would stay inactive.

The assumption that \(=(M)\) (up to log factors) is a technical limitation of our proof technique. We note that such an assumption is also used in other theoretical papers about adversarial perturbations (e.g. Daniely and Shacham (2020)).

Note that the direction of the perturbation \(z\) does not depend on the input data \(x_{0}\), only its size depends on \(x_{0}\). In fact, Theorem 5.1 shows that there is a single universal direction for an adversarial perturbation that can flip the label of any data point in \(P\). The size of the perturbation depends on the dimension of the linear subspace of the data, the number of active neurons for \(x_{0}\), the total number of neurons in the network and the size of the output. In the following corollary we give a specific bound on the size of the perturbations under assumptions on the different parameters of the problem:

**Corollary 5.1**.: _In the setting of Theorem 5.1, assume in addition that \(=(d)\) and \(k_{y_{0}}=(m)\). Then, there exists a perturbation \(z\) such that w.p. \( 1-5(me^{-/16}+d^{-1/2})\) we have \(\|z\|=O(N(x_{0}))\) and:_

\[(N(x_{0}+z))(N(x_{0}))\;.\]

The above corollary follows directly by noticing from Theorem 5.1 that:

\[\|z\| O(N(x_{0})}}} )=O(N(x_{0}))\;,\]

where we plugged in the additional assumptions. The assumptions in the corollary above are similar to the assumptions in Corollary 4.1. Namely, that the dimension of the data subspace is a constant fraction from the dimension of the entire space, and the number of active neurons is a constant fraction of the total number of neurons. Note that here we only consider active neurons with a specific sign in the second layer.

Note that the size of the perturbation in Corollary 5.1 is bounded by \(N(x_{0})\). By Remark 3.1, the output of the network for data points on the margin can be at most \(O(^{2}(d))\), since otherwise the network would have essentially stopped training. Therefore, if we consider an input \(x_{0}\) on the margin, and \(\|x_{0}\|=()=()\), then the size of the adversarial perturbation is much smaller than \(\|x_{0}\|\). For any other point, without assuming it is on the margin, and since we do not assume anything about the training data (except for being in \(P\)), we must assume that the size of the perturbation required to change the label will depend on the size of the output.

## 6 The Effects of the Initialization Scale and Regularization on Robustness

In Section 5, we presented the inherent vulnerability of trained models to small perturbations in a direction orthogonal to the data subspace. In this section, we return to a common proxy for robustness that we considered in Section 4 - the gradient at an input point \(x_{0}\). We suggest two ways that might improve the robustness of the model in the direction orthogonal to the data, by decreasing an upper bound of the gradient in this direction. We first upper bound the gradient of the model in the general case where we initialize \(w_{i}(,^{2}I_{d})\), and later discuss strategies to use this upper bound for improving robustness.

**Theorem 6.1**.: _Suppose that a network \(N(x)=_{i=1}^{m}u_{i}( w_{i},x)\) is trained on a dataset which lies on a linear subspace \(P^{d}\) of dimension \(d-\) for \( 1\), and assume that the weights \(w_{i}\) are initialized from \((,^{2}I_{d})\). Let \(x_{0} P\), let \(S=\{i[m]: w_{i},x_{0} 0\}\), and let \(k=|S|\). Then, w.p. \( 1-e^{-}\) we have:_

\[\|_{P^{}}()}{ x})\| }\;.\]

The full proof uses the same concentration bounds ideas as the lower bound proof and can be found in Appendix D. This bound is a result of the untrained weights: since the projection of the data points on \(P^{}\) is zero, the projection of the weights vectors on \(P^{}\) are not trained and are fixed at their initialization. We note that Theorem 4.1 readily extends to the case of initialization from \((,^{2}I_{d})\), in which case the lower bound it provides matches the upper bound from Theorem 6.1 up to a constant factor. In what follows, we suggest two ways to affect the post-training weights in the \(P^{}\) direction: (1) To initialize the weights vector using a smaller-variance initialization, and (2) Add an \(L_{2}\)-norm regularization on the weights. We next analyze their effect on the upper bound.

### Small Initialization Variance

From Theorem 6.1, one can conclude a strong result about the model's gradient without the dependency of its norm on \(\) and \(k\).

**Corollary 6.1**.: _For \(=}\), in the settings of Theorem 6.1, with probability \( 1-e^{-}\) we have_

\[\|_{P^{}}()}{ x})\| }\;.\]

The proof follows directly from Theorem 6.1, by noticing that \(k m\) and \( d\). Consider for example an input \(x_{0} P\) with \(\|x_{0}\|=()\), and suppose that \(N(x_{0})=(1)\). The above corollary implies that if the initialization has a variance of \(1/d^{2}\) (rather than the standard choice of \(1/d\)) then the gradient is of size \(O(1/)\). Thus, it corresponds to perturbations of size \(()\), which is the same order as \(\|x_{0}\|\).

### \(L_{2}\) Regularization

We consider another way to influence the projection onto \(P^{}\) of the trained weights vectors: adding \(L_{2}\) regularization while training. We will update the logistic loss function by adding an additive factor \(\|_{1:m}\|^{2}\). For a dataset \((x_{1},y_{1}),..,(x_{r},y_{r})\), we now train over the following objective:

\[_{j=1}^{r}L(y_{j} N(x_{j},_{1:m})))+ \|_{1:m}\|^{2}\;.\]

This regularization will cause the previously untrained weights to decrease in each training step which will decrease the upper bound on the projection of the gradient:

**Theorem 6.2**.: _Suppose that a network \(N(x)=_{i=1}^{m}u_{i}( w_{i},x)\) is trained for \(T\) training steps, using \(L_{2}\) regularization with parameter \( 0\) and step size \(>0\), on a dataset which lies on a linear subspace \(P^{d}\) of dimension \(d-\) for \( 1\), starting from standard initialization (i.e., \(w_{i}(0,I_{d})\)). Let \(x_{0} P\), let \(S=\{i[m]: w_{i},x_{0} 0\}\), and let \(k:=|S|\). Then, w.p. \( 1-e^{-}\) we have_

\[\|_{P^{}}()}{ x})\| (1-)^{T}}\;.\]

The full proof can be found in Appendix D.1. The main idea of the proof is to observe the projection of the weights on \(P^{}\) changing during training. As before, we assume w.l.o.g. that \(P=\{e_{1},,e_{d-}\}\) and denote by \(_{i}:=_{P^{}}(w_{i})\). During training, the weight vector's last \(\) coordinates are only affected by the regularization term of the loss. These weights decrease in a constant multiplaction of the previous weights. Thus, we can conclude that for every \(t 0\) we have: \(_{i}^{(t)}=(1-)^{t}_{i}^{(0)}\), where \(_{i}^{(t)}\) is the \(i\)-th weight vector at time \(t\). It implies that our setting is equivalent to initializing the weights with standard deviation \(}{}\) and training the model without regularization for \(T\) steps. As a result, we get the following corollary:

**Corollary 6.2**.: _For \((1-)^{T}}\), in the settings of 6.2, w.p. \( 1-e^{-}\) we get that:_

\[\|_{P^{}}()}{ x})\| }\;.\]

### Experiments

In this section, we present our robustness-improving experiments. 4 We explore our methods on two datasets: (1) A 7-point dataset on a one-dimensional linear subspace in a two-dimensional inputspace, and (2) A 25-point dataset on a two-dimensional linear subspace in a three-dimensional input space. In Figures 2 and 3 we present the boundary of a two-layer ReLU network trained over these two datasets. We train the networks until reaching a constant positive margin. We note that unlike our theoretical analysis, in the experiments in Figure 2 we trained all layers and initialize the weights using the default PyTorch initialization, to verify that the observed phenomena occur also in this setting. In the experiment in Figure 3 we use a different initialization scale for the improving effect to be smaller and visualized easily. In Figures 1(a) and 2(a) we trained with default settings. In Figures 1(b) and 2(b) we initialized the weights using an initialization with a smaller variance (i.e., initialization divided by a constant factor). Finally, in Figures 1(c) and 2(c) we train with \(L_{2}\) regularization.

Consider the adversarial perturbation in the direction \(P^{}\), orthogonal to the data subspace, in Figures 2 and 3. In figure (a) of each experiment, we can see that a rather small adversarial perturbation is needed to cross the boundary in the subspace orthogonal to the data. In the middle figure (b), we see that the boundary in the orthogonal subspace is much further. This is a direct result of the projection of the weights onto this subspace being much smaller. In the right experiment (c), we can see a similar effect created by regularization. In Appendix E we add the full default-scaled experiment in the two-dimensional setting to demonstrate the robustness effect. There, in both the small-initialization and regularization experiments, the boundary lines are almost orthogonal to the data subspace. In Appendix E we also conduct further experiments with deeper networks and standard PyTorch initialization, showing that our theoretical results are also observed empirically in settings going beyond our theory.

In Figure 4 we plot the distance from the decision boundary for different initialization scales of the first layer. We trained a \(3\)-layer network, initialized using standard initialization except for the first layer which is divided by the factor represented in the \(X\)-axis. After training, we randomly picked \(200\) points and used a standard projected gradient descent adversarial attack to change the label of

Figure 3: **Experiments on two-dimensional dataset demonstrating a smaller robustness effect.** We plot the dataset points and the decision boundary in 3 settings: (a) Vanilla trained network, (b) The network’s weights are initialized from a smaller variance distribution, and (c) Training with regularization. Colors are used to emphasise the values in the \(z\) axis.

Figure 2: **Experiments on a one-dimensional dataset.** We plot the dataset points and the decision boundary in 3 settings: (a) Vanilla trained network, (b) The network’s weight are initialized from a smaller variance distribution, and (c) Training with regularization.

each point, which is described in the \(Y\)-axis (perturbation norm, with error bars). The datasets are: (a) Random points from a sphere with \(28\) dimensions, which lies in a space with \(784\) dimensions; and (b) MNIST, where the data is projected on \(32\) dimensions using PCA. The different lines are adversarial attacks projected either on data subspace, on its orthogonal subspace, or without projection. It can be seen that small initialization increases robustness off the data subspace, and also on the non-projected attack, while having almost no effect for the attacks projected on the data subspace.

## 7 Conclusions and Future Work

In this paper we considered training a two-layer network on a dataset lying on \(P^{d}\) where \(P\) is a \(d-\) dimensional subspace. We have shown that the gradient of any point \(x_{0} P\) projected on \(P^{}\) is large, depending on the dimension of \(P\) and the fraction of active neurons on \(x_{0}\). We additionally showed that there exists an adversarial perturbation in the direction of \(P^{}\). The size of the perturbation depends in addition on the output of the network on \(x_{0}\), which by Remark 3.1 should be poly-logarithmic in \(d\), at least for points which lie on the margin of the network. Finally, we showed that by either decreasing the initialization scale or adding \(L_{2}\) regularization we can make the network robust to "off-manifold" perturbations, by decreasing the gradient in this direction.

One interesting question is whether our results can be generalized to other manifolds, beyond linear subspaces. We state this as an informal open problem:

**Open Problem 7.1**.: _Let \(M\) be a manifold, and \(\) a distribution over \(M\{ 1\}\). Suppose we train a network \(N:^{d}\) on a dataset sampled from \(\). Let \(x_{0} M\), then under what conditions on \(M,\ \) and \(N\), there exists a small adversarial perturbation in the direction of \((T_{x_{0}}M)^{}\), i.e. orthogonal to the tangent space \(T_{x_{0}}M\), of \(M\) at \(x_{0}\)._

Our result can be seen as a special case of this conjecture, where at all points \(x,x^{} M\), the tangent spaces are equal \(T_{x}M=T_{x^{}}M\). Another future direction would be to analyze deeper networks, or different architectures such as convolutions. Finally, it would also be interesting to analyze robustness of trained networks w.r.t. different norms such as \(L_{1}\) or \(L_{}\).