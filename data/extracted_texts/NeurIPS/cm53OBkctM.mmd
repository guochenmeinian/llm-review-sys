# Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space

Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space

Saghar Adler

University of Michigan

Vijay Subramanian

University of Michigan

###### Abstract

Models of many real-life applications, such as queueing models of communication networks or computing systems, have a countably infinite state-space. Algorithmic and learning procedures that have been developed to produce optimal policies mainly focus on finite state settings, and do not directly apply to these models. To overcome this lacuna, in this work we study the problem of optimal control of a family of discrete-time countable state-space Markov Decision Processes (MDPs) governed by an unknown parameter \(\), and defined on a countably-infinite state-space \(=_{+}^{d}\), with finite action space \(\), and an unbounded cost function. We take a Bayesian perspective with the random unknown parameter \(^{*}\) generated via a given fixed prior distribution on \(\). To optimally control the unknown MDP, we propose an algorithm based on Thompson sampling with dynamically-sized episodes: at the beginning of each episode, the posterior distribution formed via Bayes' rule is used to produce a parameter estimate, which then decides the policy applied during the episode. To ensure the stability of the Markov chain obtained by following the policy chosen for each parameter, we impose ergodicity assumptions. From this condition and using the solution of the average cost Bellman equation, we establish an \((dh^{d}|T})\) upper bound on the Bayesian regret of our algorithm, where \(T\) is the time-horizon. Finally, to elucidate the applicability of our algorithm, we consider two different queueing models with unknown dynamics, and show that our algorithm can be applied to develop approximately optimal control algorithms.

## 1 Introduction

Many real-life applications, such as communication networks, supply chains, and computing systems, are modeled using queueing models with countably infinite state-space. In the existing analysis of these systems, the models are assumed to be known, but despite this, developing optimal control schemes is hard, with only a few examples worked out . However, knowing the model, algorithmic procedures exist to produce approximately optimal policies  (such as value iteration and linear programming). Given the success of data-driven optimal control design, in particular Reinforcement Learning (RL), we explore the use of such methods for the countable state-space controlled Markov processes. However, current RL methods that focus on finite-state settings do not apply to the mentioned queueing models. With the model unknown, our goal is to develop a meta-learning scheme that is RL-based but obtains good performance by utilizing algorithms developed when models are known. Specifically, we study the problem of optimal control of a family of discrete-time countable state-space MDPs governed by an unknown parameter \(\) from a general space \(\) with each MDP evolving on the countable state-space \(=_{+}^{d}\) and finite action space \(\). The cost function is unbounded and polynomially dependent on the state, following the examples of minimizing waiting times in queueing systems. Taking a Bayesian view, we assume the model is governed by an unknown parameter \(^{*}\) generated from a fixed and known prior distribution. We aim to learn a policy \(\) that minimizes the optimal infinite-horizon average cost over a given class of policies \(\) with low Bayesian regret with respect to the (parameter-dependent) optimal policy in \(\).

To avoid many technical difficulties in countably infinite state-space settings, it is crucial to establish certain assumptions regarding the class of models from which the unknown system is drawn; some examples are: i) the number of deterministic stationary policies is not finite; and ii) in average cost optimal control problems, without stability/ergodicity assumptions, an optimal policy may not exist , and when it exists, it may not be stationary or deterministic . With these in mind, we assume that for any state-action pair, the transition kernels in the model class are categorical and skip-free to the right, i.e., with finite support with a bound depending on the state only in an additive manner; both are common features of queueing models where an increase in state is due to arrivals. A second set of assumptions ensure stability by assuming that the Markov chains obtained by using different policies in \(\) are geometrically ergodic with uniformity across \(\). From these assumptions, moments on hitting times are derived in terms of Lyapunov functions for polynomial ergodicity. These assumptions also yield a solution to the average cost optimality equation (ACOE) .

**Contributions:** To optimally control the unknown MDP, we propose an algorithm based on Thompson sampling with dynamically-sized episodes; posterior sampling is used based on its broad applicability and computational efficiency . At the beginning of each episode, a posterior distribution is formed using Bayes' rule, and an estimate is realized from this distribution which then decides the policy used throughout the episode. To evaluate the performance of our proposed algorithm, we use the metric of Bayesian regret, which compares the expected total cost achieved by a learning policy \(_{L}\) until time horizon \(T\) with the policy achieving the optimal infinite-horizon average cost in the policy class \(\). We consider regret guarantees in three different settings as follows:

1. In Theorem 1, for \(\) being the set of all policies and assuming that we have oracle access to the optimal policy for each parameter, we establish an \((dh^{d}|T})\) upper bound on the Bayesian regret of this algorithm compared to the optimal policy.

2. In Corollary 1, where class \(\) is a subset of all stationary policies and where we know the best policy within this subset for each parameter via an oracle, we prove an \((dh^{d}|T})\) upper bound on the Bayesian regret of our proposed algorithm, relative to the best-in-class policy.

3. In Theorem 2, we explore a scenario where we have access to an approximately optimal policy, rather than the optimal policy in set \(\) (which are all assumed to be stationary policies). When the approximately optimal policies satisfy Assumptions 3-4, we prove an \((dh^{d}|T})\) regret bound, relative to the optimal policy in set \(\).

Finally, to provide examples of our framework for developing approximately optimal control algorithms for stochastic systems with unknown dynamics, we study two different queueing models that meet our technical conditions. The first example is a continuous-time queueing system with two heterogeneous servers with unknown service rates and a common infinite buffer with the decision being the use of the slower server. Here, the optimal policy that minimizes the average waiting time is a threshold policy  which yields a queue-length after which the slower server is always used. The second model is a two-server queueing system, each with separate infinite buffers, to one of which a dispatcher routes an incoming arrival. Here, the optimal policy minimizing the waiting time is a switching-curve  with the specifics unknown for general parameter values, so we find the best policy within a commonly used set of switching-curve policies (Max-Weight policies ), and assign the arrival to the queue with minimum weighted queue-length. For both models, we verify our assumptions for the class of optimal/best-in-class policies corresponding to different service rates and conclude that our proposed algorithm can be used to learn the optimal/best-in-class policy.

**Related Work:** Thompson sampling , or posterior sampling, has been applied to RL in many contexts of unknown MDPs  and partially observed MDPs ; see tutorials  for a comprehensive survey. It has been used in the parametric learning context  to minimize either Bayesian  or frequentist  regret. The bulk of the literature, including , analyzes finite-state and finite-action models but with different parameterizations such that a general dependence of the models on the parameters is allowed. The work in  studies general state-space MDPs but with a scalar parameterization with a Lipschitz dependence of the underlying models. Our problem formulation specifically considers countable state-space models with the models related via ergodicity, which we believe is a natural choice. Our focus on parametric learning is also connected to older work in adaptive control  which studies asymptotically optimal learning for general parameter settings but with either a finite or countably infinite number of policies. Learning-based asymptotically optimal control in queues has a long history  but recently there is increased work that also characterizes finite-time regret performance with respect to a well-known good policy or the optimal policy; see  for a survey. A series of work has studied learning with Max-Weight policies to get stability and linear regret [44; 30] or just stability . A recent related work  considers learning optimal paramterized policies in queueing networks when the MDP is known. In a finite or countable state-space setting of specific queueing models where the parameters can be estimated, many works [2; 17; 53; 32; 31; 14; 21; 16] have used forced exploration type schemes to obtain either regret that is constant or scaling logarithmically in the time-horizon.

Another line of work studies the problem of learning the optimal policy in an undiscounted finite-horizon MDP with a bounded reward function. Reference  uses a Thompson sampling-based learning algorithm with linear value function approximation to study an MDP with a bounded reward function in a finite-horizon setting. Reference  considers an episodic finite-horizon MDP with known bounded rewards but unknown transition kernels modeled using linearly parameterized exponential families. A maximum likelihood (ML) based algorithm coupled with exploration done by constructing high probability confidence sets around the ML estimate is used to learn the unknown parameters. In another work,  extends the problem setting of  to an episodic finite-horizon MDP with unknown rewards and transitions modeled using parametric bilinear exponential families. To learn the unknown parameters, they use a ML based algorithm with exploration done with explicit perturbation. We note that all mentioned works consider a finite-horizon problem. In contrast, our work considers an average cost problem, an infinite-horizon setting, and provides finite-time performance guarantees. In addition, these works focus on an MDP with a bounded reward function. Our focus, however, is learning in MDPs with unbounded rewards with the goal of covering practical queueing examples. We note that the parameterization of transitions used in [48; 15] can be used within our framework. However, similar to our work, additional stability assumptions are necessary to guarantee asymptotic learning and sub-linear regret. Another issue with exponential transition families is that they do not allow for \(0\) entries, which limits their applicability in queueing models.

In another work,  studies discounted MDPs with unknown dynamics, and unbounded state-space, but with bounded rewards, and learns an online policy that satisfies a specific notion of stability. It is also assumed that a Lyapunov function ensuring stability for the optimal policy exists. We note that  ignores optimality and focuses on finding a stable policy, which contrasts with our work that evaluates performance relative to the optimal policy. Secondly,  considers a discounted reward problem, essentially a finite-time horizon problem. Average cost problems, such as ours, are infinite-time horizon problems, so connections to discounted problems can only be made in the limit of the discount parameter going to \(1\). Moreover,  considers a bounded reward function, simplifying their analysis but not practical for many queueing examples. Further, the assumption of a stable optimal policy with a Lyapunov function (as in ) is highly restrictive for bounded reward settings with discounting. Additionally, average cost problems with bounded costs need strong state-independent recurrence conditions for the existence of (stationary) optimal solutions, which many queueing examples don't satisfy; see . Further complications can also arise with bounded costs: e.g.,  shows that a stationary average cost optimal policy may not exist.

## 2 Problem formulation

We consider a family of discrete-time Markov Decision Processes (MDPs) governed by parameter \(\) with the MDP for parameter \(\) described by \((,,c,P_{})\). For exposition purposes, we assume that all the MDPs are on (a common) countably infinite state-space \(=_{+}^{d}\). We denote the finite action space by \(\), the transition kernel by \(P_{}:()\), and the cost function by \(c:_{+}\). As mentioned earlier, we will take a Bayesian view of the problem and assume that the model is generated using an unknown parameter \(^{*}\), which is generated from a given fixed prior distribution \(()\) on \(\). Our goal is to find a policy \(:\) that tries to achieve Bayesian optimal performance in policy class \(\), i.e., minimizes the expected regret with \(^{*}\) chosen from the prior distribution \(()\). For each value \(\), the minimum infinite-horizon average cost is defined as

\[J()=_{}_{T} {}_{t=1}^{T}c((t),A(t)), \]

where we optimize over a given class of policies \(\) and \((t)=(X_{1}(t),,X_{d}(t))\) and \(A(t)\) are the state and action at \(t\). Typically, we set this class to be all (causal) policies, but it is also possible to consider \(\) to be a proper subset of all policies as we will explore in our results. For a learning policy \(_{L}\) that aims to select the optimal control without model knowledge but with knowledge of \(\) and the prior \(\), the Bayesian regret until time horizon \(T 2\) is defined as

\[R(T,_{L})=_{t=1}^{T}c((t),A(t))-J(^{*}), \]

where the expectation is taken over \(^{*}\) and the dynamics induced by \(_{L}\). Owing to underlying challenges in countable state-space MDPs, we require the below assumptions on the cost function.

**Assumption 1**.: _The cost function \(c:_{+}\) is assumed to satisfy the following two conditions:_

1. _For every number_ \(z 0\) _and action_ \(a\)_,_ \(c(,a) z\) _outside a finite subset of_ \(\)_._
2. _The cost function is upper-bounded by a multivariate polynomial_ \(f_{c}:_{+}^{d}_{+}\) _which is increasing in every component on_ \(_{+}^{d}\) _and has maximum degree of_ \(r\)__\(( 1)\) _in any dimension. We can assume that_ \(f_{c}()=K_{i=1}^{d}(x_{i})^{r}\) _for some_ \(K>0\)_, where_ \(=(x_{1},,x_{d})\)_._

Thus, the cost function increases without bound (in the state) at a polynomial rate. This assumption is common in practice--holding costs in queueing models are polynomial in the state components. To avoid technical issues the infinite state-space setting also necessitates some assumptions on the class from which the unknown model is drawn. For instance, irreducibility of Markov chains on such state-spaces does not ensure positive recurrence (and ergodicity). Moreover, for average cost optimal control problems, without stability even the existence of an optimal policy is not guaranteed, and we need more conditions. The following assumption ensures a skip-free behaviour for transitions, which holds in many queueing models, where an increase in state corresponds to (new) arrivals.

**Assumption 2**.: _From any state-action pair \((,a)\), the transition is to a finite number of states. We also assume that all transition kernels are skip-free to the right: for some \(h 1\) which is independent of \(\) and \((,a)\), we have \(P_{}(^{};,a)=0\) for all \(^{}\{}_{+}^{d}:\|} \|_{1}>\|\|_{1}+h\}\)._

Learning necessitates some commonalities within the class of models so that using a policy well-suited to one model provides information on other models too. For us, these are in the form of constraints on the transition kernels of the models and stability assumptions. As simple union bound arguments don't work in the countably infinite state-space setting, we will use the stability assumptions instead. In our setting, we consider a class of models, each with a policy being well-suited to at least one model in the class, and use the set of policies to search within. Using a reduced set of policies is necessary as the number of deterministic stationary policies is infinite. To learn correctly while restricting attention to this subset policy class, requires some regularity assumptions when a policy well-suited to one model is tried on a different model. Our ergodicity assumptions are one convenient choice; see Appendix A.1 for details. These assumptions let us characterize the distributions of the first passage times of the Markov processes via stability conditions; see Lemmas 10 and 11.

**Assumption 3**.: _For any MDP \((,,c,P_{})\) with parameter \(\), there exists a unique optimal policy \(_{}^{*}\) that minimizes the infinite-horizon average cost within the class of policies \(\). Furthermore, for any \(_{1},_{2}\), the Markov process with transition kernel \(P_{_{1}}^{_{2}^{*}}\) obtained from the MDP \((,,c,P_{_{1}})\) by following policy \(_{_{2}}^{*}\) is irreducible, aperiodic, and geometrically ergodic with geometric ergodicity coefficient \(_{_{1},_{2}}^{g}(0,1)\) and stationary distribution \(_{_{1},_{2}}\). This is equivalent to the existence of finite set \(C_{_{1},_{2}}^{g}\) and Lyapunov function \(V_{_{1},_{2}}^{g}:[1,+)\) satisfying_

\[ V_{_{1},_{2}}^{g}()-1-_{_{1}, _{2}}^{g}V_{_{1},_{2}}^{g}(),\   C_{_{1},_{2}}^{g}P_{_{1}}^{ _{_{2}}^{*}}V_{_{1},_{2}}^{g}()<+,\  C_{_{1},_{2}}^{g},\]

_where \( V_{_{1},_{2}}^{g}():=P_{_{1}}^{_{_{2}}^ {*}}V_{_{1},_{2}}^{g}()-V_{_{1},_{2}}^{g}()\). Setting \(b_{_{1},_{2}}^{g}:=_{ C_{_{1},_{2}}^{g}}P_{ _{1}}^{_{_{2}}^{*}}V_{_{1},_{2}}^{g}()+V_{ _{1},_{2}}^{g}()\) yields_

\[ V_{_{1},_{2}}^{g}()-1-_{_{1}, _{2}}^{g}V_{_{1},_{2}}^{g}()+b_{_{1}, _{2}}^{g}_{C_{_{1},_{2}}^{g}}(),. \]

_Then, we have the following assumptions relating all the models in \(\):_

1. _The geometric ergodicity coefficient is uniformly bounded below_ \(1\)_:_ \(_{*}^{g}:=_{_{1},_{2}}_{_{1},_ {2}}^{g}<1\)_._
2. _We assume that_ \(\{0^{d}\}_{_{1},_{2}}C_{_{1},_{2}} ^{g}\) _and_ \(C_{*}^{g}=_{_{1},_{2}}C_{_{1},_{2}}^{g}\) _is a finite set. We further assume that_ \(b_{*}^{g}:=_{_{1},_{2}}b_{_{1},_{2}}^{g}<+\)

**Remark 1**.: _The uniqueness of the optimal policy is not essential for the validity of our results, provided that all optimal policies satisfy our assumptions. When this condition is not met, we need to select an optimal policy that is geometrically ergodic for all \(\). This issue can be avoided by using a smaller subset of policies for which ergodicity can be shown, such as Max-Weight policies._

Geometric ergodicity implies that all moments of the hitting time of state \(0^{d}\), say \(_{0^{d}}\), from any initial state \( 0^{d}\) are finite as \(_{}[^{_{0^{d}}}] c_{1}V^{g}()\) (for specific \(>1\) and \(c_{1}\)), and so, \(_{}[_{0^{d}}^{k}] c_{1}V^{g}()k^{1}/^{k}( )<+\) for all \(k\); see Appendix A.2. Function \(V^{g}\) is typically exponential in some norm of the state and yields an exponential bound for moments of hitting times, and a poor regret bound. To improve the regret bound, we need a different drift equation with function \(V^{p}\) with polynomial dependence on a norm of the state that bounds certain polynomial moments of \(_{0^{d}}\).

**Assumption 4**.: _Given \(_{1},_{2}\), Markov process obtained from MDP \((,,c,P_{_{1}})\) by following policy \(_{_{2}}^{*}\) is polynomially ergodic through the Foster-Lyapunov criteria: there exists a finite set \(C^{p}_{_{1},_{2}}\), constants \(^{p}_{_{1},_{2}}\), \(b^{p}_{_{1},_{2}}>0\), \(^{p}_{_{1},_{2}}[,1)\), and function \(V^{p}_{_{1},_{2}}:[1,+)\) satisfying_

\[ V^{p}_{_{1},_{2}}()-^{p}_{_{1},_ {2}}V^{p}_{_{1},_{2}}()^{^{p}_{_{1},_{2}}}+b^{p}_{_{1},_{2}}_{C^{p}_{_{1}, _{2}}}(),. \]

_Then, we have the following assumptions relating all the models in \(\):_

1. \(V^{p}_{_{1},_{2}}\) _is a polynomial with positive coefficients, maximum degree (in any dimension)_ \(r^{p}_{_{1},_{2}}\)_, and sum of coefficients_ \(s^{p}_{_{1},_{2}}\)_. We assume_ \(r^{p}_{*}=_{_{1},_{2}}r^{p}_{_{1},_{2}}<\) _and_ \(s^{p}_{*}=_{_{1},_{2}}s^{p}_{_{1},_{2}}<\)_._
2. _We assume that_ \(\{0^{d}\}_{_{1},_{2}}C^{p}_{_{1}, _{2}}\) _and_ \(C^{p}_{*}=_{_{1},_{2}}C^{p}_{_{1},_{2}}\) _is a finite set. We further assume that_ \(^{p}_{*}:=_{_{1},_{2}}^{p}_{_{1},_{2}}>0\) _and_ \(b^{p}_{*}:=_{_{1},_{2}}b^{p}_{_{1},_{2}}<\)_._
3. _Let_ \(K_{_{1},_{2}}():=_{n=0}^{}2^{-n-2}P^{_{ _{2}}^{*}}_{_{1}}^{n}(,0^{d})\)_, which is positive for any pair_ \(_{1},_{2}\) _by irreducibility. We assume that it is strictly positive in_ \(\)_:_ \(K_{*}:=_{_{1},_{2}}_{ C^{p}_{*}}K_{_{1}, _{2}}()>0\)_._

Assumptions 3-4 hold in many models of interest; see Appendix E. As average cost optimality is our design criterion, we need to ensure the existence of solutions to ACOE when \(\) is the set of all policies, or Poisson equation when \(\) is a subset of all policies. We discuss these two cases separately.

_Case 1: \(\) is the set of all policies._ For any parameter \(\), the MDP \((,,c,P_{})\) is said to satisfy the ACOE if there exists a constant \(J()\) and a unique function \(v(;):\) such that

\[J()+v(;)=_{a}c(,a)+_{ }P_{}(|,a)v(;)}\ v(0^{d};)=0.\]

From  if the following conditions hold, ACOE has a solution, \(J_{}\) is the optimal infinite-horizon average cost, and there is an optimal stationary policy with ACOE becoming (5): (i) for every \((,a)\) and \(z 0\), cost function \(c(,a) z\) outside a finite subset of \(\); (ii) there is a stationary policy with an irreducible and aperiodic Markov process with finite average cost; and (iii) from every \((,a)\) transition to a finite number of states is possible. From Assumptions 1-3, the above conditions hold.

_Case 2: \(\) is a proper subset of all policies._ Here, we posit that for every \(\) and its best in-class policy \(_{}^{*}\), there exists a constant \(J()\), the average cost of \(_{}^{*}\), and a function \(v(;):\) with

\[J()+v(;)=c(,_{}^{*}())+_{ }P_{}(|,_{}^{*}())v(; ). \]

This holds by the solution of the Poisson equation with the appropriate forcing function. For a Markov process \(\) on the space \(\) with transition kernel \(P\) and cost function \(()\), a solution to the Poisson equation  is a scalar \(J\) and function \(v():\) such that \(J+v=+Pv\), where \(v()=0\) for some \(\). In our setting using [41, Sections 9.6-9.8], for a model governed by \(\) following policy \(_{}^{*}\), we show a solution to the Poisson equation exists and is given by \(v^{_{}^{*}}(0^{d})=0\), and

\[J()=^{_{}^{*}}(0^{d})/^{_{}^{*}}_{ ^{*}}[_{0^{d}}]v^{_{}^{*}}()=^{_{}^{*}}()-J() ^{_{}^{*}}_{}[_{0^{d}}], , \]

where \(^{_{}^{*}}()=^{_{}^{*}}_{} _{i=0}^{_{0^{d}}-1}c((i),_{}^{*}((i))),\) and expectation is over trajectories of Markov chain \(\) with transition kernel \(P^{_{}^{*}}_{}\) starting in state \(\). In Appendix A.3, we present related definitions and show that from Assumptions 3-4, the requirements for the existence and finiteness of the solutions to Poisson equation are satisfied. Finally, we assume \(_{}J()\) is finite, which typically holds as a result of the boundedness assumptions stated in Assumptions 3 or 4, along with Assumption 1.

**Remark 2**.: _In Assumption 4 we can use any other policy \(_{_{2}}\) such that the Markov process obtained from MDP \((,,c,P_{_{1}})\) by following policy \(_{_{2}}\) is irreducible and polynomially ergodic via the Foster-Lyapunov criteria with the uniformity discussed. Irreducibility is important as the policy will be used at times when the state is not known in advance, specifically at Steps 14-17 in Algorithm 1._

**Assumption 5**.: _We assume that \(J^{*}:=_{}J()<+\)._

## 3 Thompson sampling based learning algorithm

We will use the learning algorithm Thompson sampling with dynamically-sized episodes from  to learn the unknown parameter \(^{*}\) and the corresponding policy, \(_{^{*}}^{*}\), but suitably modify it for our countable state-space setting. Consider the prior distribution \(_{0}=\) defined on \(\) from which \(^{*}\) is sampled. At each time \(t\), the posterior distribution \(_{t}\) is updated according to Bayes' rule as

\[_{t+1}(d)=_{}((t+1 )|(t),A(t))_{t}(d)} {_{^{}}_{^{}}( (t+1)|(t),A(t) )_{t}(d^{})}, \]

and the posterior estimate \(_{t+1}\), if generated, is from the posterior distribution \(_{t+1}\). The modified Thompson-sampling with dynamically-sized episodes algorithm (TSDE) is presented in Algorithm 1. The TSDE algorithm operates in episodes: at the beginning of episode \(k\), parameter \(_{k}\) is sampled from the posterior distribution \(_{t_{k}}\) and during episode \(k\), actions are generated from the stationary policy according to \(_{k}\), i.e., \(_{_{k}}^{*}\). Let \(t_{k}\) be the time the \(k\)-th episode begins. Define \(_{k+1}\) as the first time after \(t_{k}\) that the conditions of Line 6 of Algorithm 1 is triggered and \(t_{k+1}\) as the first time at or after \(_{k+1}\) where state \(0^{d}\) is visited; for the last episode started before or at \(T\), we ensure that \(t_{k}\) and \(_{k}\) are less than or equal \(T+1\). Explicitly, \(t_{1}=1\) and for \(k>1\), \(t_{k}=\{t_{k}:\,(t)=0^{d}t>T\}\). Let \(T_{k}=t_{k+1}-t_{k}\) be the length of the \(k\)-th episode and set \(_{k}=_{k+1}-t_{k}\) with the convention \(_{0}=1\). For any state-action pair \((,a)\), we define \(N_{1}(,a)=0\) and for \(t>1\),

\[N_{t}(,a)=\{t_{k} i<_{k+1} tk 1:((i),A(i))=(,a)\}.\]

Notice that for all state-action pairs \((,a)\) and \(_{k+1} t t_{k+1}\), we have \(N_{t}(,a)=N_{t_{k+1}}(,a)\). We denote \(K_{T}\) as the number of episodes started by or at time \(T\), or \(K_{T}=\{k:t_{k} T\}\). The length of episode \(k<K_{T}\) is not fixed and is determined according to two stopping criteria: (1) \(t>t_{k}+_{k-1}\), (2) \(N_{t}(,a)>2N_{t_{k}}(,a)\) for some state-action pair \((,a)\). After either criterion is met, the system will still follow policy \(_{_{k}}^{*}\) until the first time at which state \(0^{d}\) is visited; see Line 14 and Figure 1. We use this settling period to \(0^{d}\) because the system state can be arbitrary when thefirst stopping criterion is met. As the countable state-space setting precludes a simple union-bound argument to overcome this uncertainty (as in the literature for finite state settings), we let the system reach the special state \(0^{d}\). Another (essentially equivalent) option is to wait until the state hits the finite set \(C^{g}_{s}\) or \(C^{p}_{s}\) and then use a union bound argument for all states in either set. For analytical convenience, we only use the state samples observed before arrival \(_{k+1}\) to update the posterior distribution. The posterior update is halted during the settling period to \(0^{d}\) as we have no control on the states visited during it, despite it being finite in duration (by our assumptions).

## 4 Regret analysis of Algorithm 1

The performance of any learning policy \(_{L}\) is evaluated using the metric of expected regret compared to the optimal expected average cost of true parameter \(^{*}\), namely, \(J(^{*})\). In this section, we evaluate the performance of Algorithm 1 and derive an upper bound for \(R(T,_{TSDE})\), its expected regret up to time \(T\). In Section 2, we argued that at time \(t\) in episode \(k\) (\(t_{k} t<t_{k+1}\)), there exist a constant \(J(_{k})\) and a unique function \(v(;_{k}):\) such that \(v(0^{d};_{k})=0\) and

\[J(_{k})+v((t);_{k})=c((t),^{*}_{_{k}}(( t)))+_{}P_{_{k}}(|(t),^{*}_{_ {k}}((t)))v(;_{k}), \]

in which \(^{*}_{_{k}}\) is the optimal or best-in-class policy (depending on the context) according to parameter \(_{k}\) and \(J(_{k})\) is the average cost for the Markov process obtained from MDP \((,,c,P_{_{k}})\) by following \(^{*}_{_{k}}\). We derive a bound for the expected regret \(R(T,_{TSDE})\) following the proof steps of  while extending it to the countable state-space setting of our problem. Using (8), the regret is decomposed into three terms and each term is bounded separately:

\[R(T,_{TSDE})=[_{k=1}^{K_{T}}_{t=t_{k}} ^{t_{k+1}-1}c((t),^{*}_{_{k}}((t)))]-T\, [J(^{*})]=R_{0}+R_{1}+R_{2}, \] \[R_{0}= [_{k=1}^{K_{T}}T_{k}J(_{k})]-T\, [J(^{*})],\] (10) \[R_{1}= [_{k=1}^{K_{T}}_{t=t_{k}}^{t_{k+1}-1} v((t);_{k})-v((t+1);_{k})],\] (11) \[R_{2}= [_{k=1}^{K_{T}}_{t=t_{k}}^{t_{k+1}-1} v((t+1);_{k})-_{}P_{_{k}}( {y}|(t),^{*}_{_{k}}((t)))v(;_{k})]. \]

Before bounding the above regret terms, we address the complexities arising from the countable state-space setting. Firstly, we need to study the maximum state (with respect to the \(_{}\)-norm) visited up to time \(T\) in the MDP \((,,c,P_{^{*}})\) following Algorithm 1; we denote this maximum state by \(M^{T}_{^{*}}\). In Appendix C, we derive upper bounds on the moments of hitting times of state \(0^{d}\) and utilize this to bound the moments of random variable \(M^{T}_{^{*}}\), which then lets us study the number of episodes \(K_{T}\) by time \(T\). Another challenge in analyzing the regret is that the relative value function \(v(;)\) is unlikely to be bounded in the countable state-space setting. Hence, in (13) and (14), we find bounds for the relative value function in terms of hitting time \(_{0^{d}}\) from the initial state \(\). Based on these results, we provide an upper bound for the regret of Algorithm 1 in Theorem 1.

_Maximum state norm under polynomial and geometric ergodicity._ Here we state the results that characterize the maximum \(l_{}\)-norm of the state vector achieved up until and including time \(T\), and

Figure 1: MDP evolution in episode \(k<K_{T}\).

the resulting bounds on the number of episodes executed until time \(T\). Owing to space constraints the details (including formal statements) are presented in Appendix B. The results are listed as below:

(i) In Lemma 6, we bound the moments of the maximum length of recurrence times of state \(0^{d}\), using the ergodicity assumptions 3 and 4. This, along with the skip-free property, allows us to prove that the \(p\)-th moment of \(_{1 i T}_{0^{d}}^{(i)}\) and \(M_{^{*}}^{T}\) are both of order \(O(^{p}T)\).

(ii) In Lemma 7, we find an upper bound for the number of episodes in which the second stopping criterion is met or there exists a state-action pair for which \(N_{t}(,a)\) has increased more than twice.

(iii) In Lemma 8, we bound the total number of episodes \(K_{T}\) by time \(T\) by bounding the number of episodes triggered by the first stopping criterion, using the fact that in such episodes, \(_{k}=_{k-1}+1\). Moreover, to account for the settling time of each episode, we use geometric ergodicity and Lemma 6. It follows that the expected value of the number of episodes \(K_{T}\) is of the order \((||T})\).

_Regret analysis._ Next, we bound regret terms \(R_{0}\), \(R_{1}\) and \(R_{2}\) using the approach of  along with additional arguments to extend their result to a countably infinite state-space. We consider the relative value function \(v(;)\) of policy \(_{}^{*}\) introduced for the optimal policy in ACOE or for the best in-class policy in the Poisson equation. In either of these cases, policy \(_{}^{*}\) satisfies (5), which is the corresponding Poisson equation with forcing function \(c(,_{}^{*}())\) in a Markov chain with transition matrix \(P_{}^{_{}^{*}}\). In (6), we presented the solution \((J,v)\) to the Poisson equation, which yields the following upper bound for the relative value function, as argued in Appendix A.3:

\[v(;)^{_{}^{*}}()_{}^ {_{}^{*}}[Kd(\|\|_{}+h_{0^{d}})^{r} _{0^{d}}]. \]

We can similarly lower bound the relative value function using Assumption 5 as

\[v(;)-J()_{}^{_{}^{*}}[_{0^{ d}}]-J^{*}_{}^{_{}^{*}}[_{0^{d}}]. \]

From Assumption 3, all moments of \(_{0^{d}}\) and thus, the derived bounds are finite. Also, in Lemma 10 we bound the moments of \(_{0^{d}}\) of order \(i r+1\) using the polynomial Lyapunov function \(V_{_{1},_{2}}^{p}\), which is then used to bound the expected regret. We next bound the first regret term \(R_{0}\) from the first stopping criterion in terms of the number of episodes \(K_{T}\) and the settling time of each episode \(k\).

**Lemma 1**.: _The first regret term \(R_{0}\) satisfies \(R_{0} J^{*}\,[K_{T}(_{1 i T}_{0^{d}}^{(i)}+1)]\)._

Proof of Lemma 1 is given in Appendix B.4. From Lemma 6, all moments of \(_{1 i T}_{0^{d}}^{(i)}\) are bounded by a polylogarithmic function. Futhermore, as a result of Lemma 8, expected value of the number of episodes \(K_{T}\) is of the order \((||T})\), which leads to a \((||T})\) regret term \(R_{0}\). Next, an upper bound on \(R_{1}\) defined in (11) is derived. In the proof of Lemma 2 we argue that as the relative value function is equal to \(0\) at all time instances \(t_{k}\) for \(k K_{T}\), the only term that contributes to the regret is the value function at the end of time horizon \(T\). We use the lower bound derived in (14) to show that the second regret term \(R_{1}\) is \((1)\); the proof is given in Appendix B.5.

**Lemma 2**.: _The second regret term \(R_{1}\) satisfies \(R_{1} c_{2}\,[(M_{^{*}}^{T})^{r^{*}}]+c_{3}\), where \(c_{2}=J^{*}2^{r^{*}_{*}}s_{*}^{p}(_{*}^{p})^{-1}\) and \(c_{3}=J^{*}(_{*}^{p})^{-1}s_{*}^{p}(2h)^{r^{p}_{*}}+b_ {*}^{p}(K_{*})^{-1}\)._

From Lemma 6, \([(M_{^{*}}^{T})^{r^{*}}]\) is \(O(^{r^{p}_{*}}T)\); hence, \(R_{1}\) is upper bounded by a polylogarithmic function of the order \(r^{p}_{*}\). Finally, in Lemma 3, we derive an upper bound for the third regret term \(R_{2}\) defined in (12) using the bound derived for the relative value function in (13). To bound \(R_{2}\), we characterize it in terms of the difference between the empirical and true unknown transition kernel and following the concentration method used in , we argue that with high probability the total variation distance between the two distributions is small; for proof, see Appendix B.6.

**Lemma 3**.: _For problem-dependent constant \(c_{p_{3}}\) and polynomial \(Q(T)=c_{p_{3}}(Th)^{r+r^{p}_{*}}/48\), we have_

\[R_{2}((hT+h)+1)^{d}+c_{p_{3}}|T}_{2}(2| |T^{2}Q(T))[(M_{^{*}}^{T}+h)^{d+r +r^{p}_{*}}_{1 i T}_{0^{d}}^{(i)}].\]

The above Lemma results in a \((KrdJ^{*}h^{d+2r+r^{p}_{*}}|T})\) regret term as a result of Lemma 6, where \(h\) is the skip-free parameter defined in Assumption 2, \(d\) is the dimension of the state-space, \(K\) and \(r\) are the cost function parameters defined in Assumption 1, \(J^{*}\) is the supremum on the optimal cost, \(r^{p}_{*}\) is defined in Assumption 4, and where \(\) hides logarithmic factors in problem parameters one of which is \(^{d+r+r^{p}_{*}+2}(T)\). For simplicity, we have not included the Lyapunovfunctions related parameters in the regret. Finally, from Lemmas 1, 2, 3, along with the Cauchy-Schwarz inequality, we conclude that the regret of Algorithm 1\(R(T,_{TSDE})(=R_{0}+R_{1}+R_{2})\) is \((KrdJ^{*}h^{d+2r+r_{*}^{2}}|T})\); for brevity, we will state that regret is of the order \((dh^{d}|T})\).

**Theorem 1**.: _Under Assumptions 1-5, the regret of Algorithm 1, \(R(T,_{TSDE})\), is \((dh^{d}|T})\)._

Theorem 1 can be extended to the problem of finding the best policy within a sub-class of policies in set \(\), which may or may not contain the optimal policy. In Section 2, we stated that Assumptions 3 and 4 hold for policies in \(\) and we used this to argue that the Poisson equation has a solution given in (6). As a result, repeating the same arguments as in Theorem 1 with the modification that \(_{}^{*}\) is the best in-class policy of the MDP governed by parameter \(\), yields the following corollary.

**Corollary 1**.: _Under Assumptions 1 through 5, the regret of Algorithm 1 when using the best in-class policy is \((dh^{d}|T})\)._

**Requirement of an optimal policy oracle.** To implement our algorithm, we need to find the optimal policy for each model sampled by the algorithm--optimal policy for Theorem 1 and optimal policy within policy class \(\) for Corollary 1. In the finite state-space setting,  provides a schedule of \(\) values and selects \(\)-optimal policies to obtain \(()\) regret guarantees. The issue with extending the analysis of  to the countable state-space setting is that we need to ensure (uniform) ergodicity for the chosen \(\)-optimal policies. Another issue is that, to the best of our knowledge, there isn't a general structural characterization of all \(\)-optimal stationary policies for countable state-space MDPs or even a characterization of the policy within this set that is selected by any computational procedure in the literature; current results only discuss characterization of the stationary optimal policy. In the absence of such results, stability assumptions with the same uniformity across models as in our submission will be needed, which are likely too strong to be useful. However, if we could verify the stability requirements of Assumptions 3 and 4 for a subset of policies, the optimal oracle is not needed, and instead, by choosing approximately optimal policies within this subset, we can follow the same proof steps as  to guarantee regret performance similar to Corollary 1 (without knowledge of model parameters). Thus, in Theorem 2 we extend the previous regret guarantees to the algorithm employing \(\)-optimal policy; proof is given in Appendix B.8.

**Theorem 2**.: _Consider a non-negative sequence \(\{_{k}\}_{k=1}^{}\) such that for every \(k\), \(_{k}\) is bounded above by \(\) and an \(_{k}\)-optimal policy satisfying Assumptions 3 and 4 is given. The regret incurred by Algorithm 1 while using the \(_{k}\)-optimal policy during any episode \(k\) is \((dh^{d}|T})\)._

## 5 Evaluation and Conclusion: Application of Algorithm 1 to queueing models

Next, we present an evaluation of our algorithm. We study two different queueing models shown in Figure 2, each with Poisson arrivals at rate \(\), and two heterogeneous servers with exponentially distributed services times with unknown service rate vector \(^{*}=(_{1}^{*},_{2}^{*})\). Vector \(^{*}\) is sampled from the prior distribution \(\) defined on the space \(\) given as \(=\{(_{1},_{2})_{+}^{2}:+_{2}},1}{ _{2}} R\}\), for fixed \(R 1\) and \((0,0.5)\). The first condition ensures the stability of the queueing models, while the second guarantees the compactness of the parameter space of the parameterized policies. In both systems, the goal of the dispatcher is to minimize the expected sojourn time of jobs, which by Little's law  is equivalent to minimizing the average number of jobs in the system. After verifying Assumptions 1-5 in Appendix E for the cost function \(c()=\|\|_{1}\), Theorem 1 yields a Bayesian regret of order \((|T})\) for Algorithm 1.

**Model 1**.: _Two-server queueing system with a common buffer._ We consider the continuous-time queueing system of Figure 1(a), where the countable state space is \(=\{=(x_{0},x_{1},x_{2})_{+}\{0,1\} ^{2}\}\), where \(x_{0}\) is the queue length, and \(x_{i}\), \(i=1,2\) equal \(1\) if server \(i\) is busy. The action space is \(=\{h,b,1,2\}\), where \(h\) means no action, \(b\) sends a job to both servers, and \(i=1,2\) assigns a job to server \(i\). In , it is shown that by uniformization  and sampling the continuous-time Markov process at rate \(+_{1}^{*}+_{2}^{*}\), a discrete-time Markov chain is obtained, which converts the original continuous-time problem to an equivalent discrete-time problem where we need to minimize \(_{T}T^{-1}_{t=0}^{T-1}\|(t)\|_{1}\). Further,  shows that the optimal policy is a threshold policy \(_{t(^{*})}\) with optimal finite threshold \(t(^{*})\): always assign a job to the faster (first) server when free, and to the second server if it is free and \(\|\|_{1}>t(^{*})\), and take no action otherwise. InAppendix E.1, we argue that the discrete-time Markov process governed by \(\) and following threshold policy \(_{t}\) for any threshold \(t\) belonging to a compact set satisfies Assumptions 1-5.

_Model 2. Two heterogeneous parallel queues._ We consider the continuous-time queueing system of Figure 1(b) with countable state space \(=\{=(x_{1},x_{2})_{+}^{2}\}\), where \(x_{i}\) is the number of jobs in the server-queue pair \(i\). The action space is \(=\{1,2\}\), where action \(i\) sends the arrival to queue \(i\). We obtain the discrete-time MDP by sampling the queueing system at the arrivals, and then aim to find the average cost minimizing policy within the class \(=\{_{};[(c_{R}R)^{-1},c_{R}R]\}\), \(c_{R} 1\). Policy \(_{}:\) routes arrivals based on the weighted queue lengths: \(_{}()=(1+x_{1},(1+x_{2}))\) with ties broken for \(1\). Even with the transition kernel fully specified (by the values of arrival and service rates), the optimal policy in \(\) is not known except when \(_{1}=_{2}\) where the optimal value is \(=1\), and so, to learn it, we will use Proximal Policy Optimization for countable state-space MDPs . Note that  requires full model knowledge, which holds in our scheme as we use parameters sampled from the posterior for choosing the policy at the beginning of each episode. In Appendix E.2, we argue that the discrete-time Markov process governed by parameter \(\) and following policy \(_{}\) for \([(c_{R}R)^{-1},c_{R}R]\) satisfies Assumptions 1-5.

Next, we report the numerical results of Algorithm 1 in the two queueing models of Figure 2 and calculate regret using (2). The regret is averaged over 2000 simulation runs and plotted against the number of transitions in the sampled discrete-time Markov process. Figure 3 shows the behavior of the regret of the two queuing models for three different arrival rates and service rates distributed according to a Dirichlet prior over \([0.5,1.9]^{2}\). We observe that the regret is sub-linear in time and grows as the arrival rate increases. For the queueing model of Figure 1(a), the minimum average cost \(J()\) and optimal policy \(_{}^{*}\) are known explicitly  for every \(\), which are used in Algorithm 1 and for regret calculation. Conversely, for the second queueing model, \(J()\) and \(_{}^{*}\) are not known. The PPO algorithm  is used to empirically find both the optimal weight and the policy's average cost. Additional details of the simulations and more plots are presented in Appendix G.

**Conclusions and future work.** We studied the problem of learning optimal policies in countable state-space MDPs governed by unknown parameters. We proposed a learning policy based on Thompson sampling and established finite-time performance guarantees on the Bayesian regret. We highlighted the practicality of our proposed algorithm by considering two different queuing models and showing that our algorithm can be applied to develop optimal control policies. For future work we plan two directions to explore: to generalize our algorithm to consider polices that might not all be stabilizing, and also to simplify the algorithm using ideas from .

Figure 3: Regret performance for \(=0.3,0.5,0.7\). Shaded region shows the \(\) area of mean regret.

Figure 2: Two-server queueing systems with heterogeneous service rates.

## Disclosure of Funding

SA's research was supported by NSF via grants ECCS2038416, CCF2008130, and CNS1955777, and a grant from General Dynamics via MIDAS at the University of Michigan, Ann Arbor. VS's research is supported in part by NSF via grants ECCS2038416, CCF2008130, CNS1955777, and CMMI2240981.