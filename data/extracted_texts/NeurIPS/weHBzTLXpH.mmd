# T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation

Kaiyi Huang\({}^{1}\) Kaiyue Sun\({}^{1}\) Enze Xie\({}^{2}\) Zhenguo Li\({}^{2}\) Xihui Liu\({}^{1}\)

\({}^{1}\) The University of Hong Kong \({}^{2}\) Huawei Noah's Ark Lab

{huangky, kaiyue}@connect.hku.hk

{xie.enze, li.zhenguo}@huawei.com xihuliu@eee.hku.hk

Corresponding Author

###### Abstract

Despite the stunning ability to generate high-quality images by recent text-to-image models, current approaches often struggle to effectively compose objects with different attributes and relationships into a complex and coherent scene. We propose T2I-CompBench, a comprehensive benchmark for open-world compositional text-to-image generation, consisting of 6,000 compositional text prompts from 3 categories (attribute binding, object relationships, and complex compositions) and 6 sub-categories (color binding, shape binding, texture binding, spatial relationships, non-spatial relationships, and complex compositions). We further propose several evaluation metrics specifically designed to evaluate compositional text-to-image generation and explore the potential and limitations of multimodal LLMs for evaluation. We introduce a new approach, Generative mOdel finetuning with Reward-driven Sample selection (GORS), to boost the compositional text-to-image generation abilities of pretrained text-to-image models. Extensive experiments and evaluations are conducted to benchmark previous methods on T2I-CompBench, and to validate the effectiveness of our proposed evaluation metrics and GORS approach. Project page is available at https://karine-h.github.io/T2I-CompBench/.

Figure 1: Failure cases of Stable Diffusion v2 . Our compositional text-to-image generation benchmark consists of three categories: attribute binding (including color, shape, and texture), object relationship (including spatial relationship and non-spatial relationship), and complex compositions.

Introduction

Recent progress in text-to-image generation [2; 1; 3; 4; 5; 6] has showcased remarkable capabilities in creating diverse and high-fidelity images based on natural language prompts. However, we observe that even state-of-the-art text-to-image models often fail to compose multiple objects with different attributes and relationships into a complex and coherent scene, as shown in the failure cases of Stable Diffusion  in Figure 1. For example, given the text prompt "a blue bench on the left of a green car", the model might bind attributes to the wrong objects or generate the spatial layout incorrectly.

Previous works have explored compositional text-to-image generation from different perspectives, such as concept conjunction , attribute binding (focusing on color) [8; 9], and spatial relationship . Most of those works focus on a sub-problem and propose their own benchmarks for evaluating their methods. However, there is no consensus on the problem definition and standard benchmark of compositional text-to-image generation. To this end, we propose a comprehensive benchmark for open-world compositional text-to-image generation, T2I-CompBench. T2I-CompBench consists of three categories and six sub-categories of compositional text prompts: (1) **Attribute binding**. Each text prompt in this category contains at least two objects and two attributes, and the model should bind the attributes with the correct objects to generate the complex scene. This category is divided into three sub-categories (color, shape, and texture) based on the attribute type. (2) **Object relationships**. The text prompts in this category each contain at least two objects with specified relationships between the objects. Based on the type of the relationships, this category consists of two sub-categories, spatial relationship and non-spatial relationship. (3) **Complex compositions**, where the text prompts contain more than two objects or more than two sub-categories mentioned above. For example, a text prompt that describes three objects with their attributes and relationships.

Another challenge is the assessment of compositional text-to-image models. Most previous works evaluate the models by image-text similarity or text-text similarity (between the caption predicted from the generated images and the original text prompts) with CLIPScore [11; 12] or BLIP [13; 14]. However, both metrics do not perform well for compositionality evaluation due to the ambiguity of image captioning and the difficulty of compositional vision-language understanding. To address this challenge, we propose several evaluation metrics for different categories of compositional prompts. We propose _disentangled BLIP-VQA_ for attribute binding evaluation to overcome the ambiguous attribute correspondences, _UniDet-based metric_ for spatial relationship evaluation, and _3-in-1_ metric for complex prompts. We further investigate the potential and limitations of multimodal large language models such as _MiniGPT-4_ with Chain-of-Thought  for compositionality evaluation.

Finally, we propose a new approach, _Generative mOdel finetuning with Reward-driven Sample selection (GORS)_, for compositional text-to-image generation. We finetune the state-of-the-art Stable Diffusion v2  model with generated images that highly align with the compositional prompts, where the fine-tuning loss is weighted by the reward which is defined as the alignment score between compositional prompts and generated images. This approach is simple but effective in boosting the model's compositional abilities and can serve as a new baseline for future explorations.

In summary, our contributions are three-folded. (1) We propose a comprehensive benchmark for compositional text-to-image generation which consists of 6,000 prompts from 3 categories (attribute binding, object relationships, and complex compositions) and 6 sub-categories (color binding, shape binding, texture binding, spatial relationships, non-spatial relationships, and complex compositions). (2) We propose evaluation metrics that are specifically designed for compositional text-to-image evaluation. Experiments validate that the proposed evaluation metrics are highly correlated with human perception. (3) We benchmark several previous text-to-image models on our proposed benchmark and evaluation metrics, and propose a simple and effective approach, GORS, for boosting the compositionality of text-to-image models.

## 2 Related work

**Text-to-image generation.** Early works [18; 19; 20; 21; 22; 23] explore different network architectures and loss functions based on generative adversarial networks (GAN) . DALL-E  achieves impressive results with a transformer and discrete variational autoencoder (VAE) trained on web-scale data. Recently, diffusion models have achieved remarkable success for text-to-imagegeneration [26; 27; 1; 28; 29]. Current state-of-the-art models such as Stable Diffusion  still struggle to compose multiple objects with attributes and relationships in a complex scene. Some recent works attempt to align text-to-image models with human feedback [30; 31]. Concurrent work RAFT  proposes reward-ranked fine-tuning to align text-to-image models with certain metrics. Our proposed GORS approach is a simpler finetuning approach that does not require multiple iterations of sample generation and selection.

**Compositional text-to-image generation.** Researchers have delved into various aspects of compositionality in text-to-image generation to achieve visually coherent and semantically consistent results [7; 8; 33; 10]. Previous work focused on concept conjunction and negation , attribute binding with colors [8; 9; 34], and spatial relationships between objects [35; 10]. However, those work each target at a sub-problem, and evaluations are conducted in constrained scenarios. Our work is the first to introduce a comprehensive benchmark for compositional text-to-image generation.

**Benchmarks for text-to-image generation.** Early works evaluate text-to-image on CUB birds , Oxford flowers , and COCO  which are easy with limited diversity. As the text-to-image models become stronger, more challenging benchmarks have been introduced. DrawBench  consists of 200 prompts to evaluate counting, compositions, conflicting, and writing skills. DALL-EVAL  proposes Paints Skills to evaluate visual reasoning skills, image-text alignment, image quality, and social bias by 7,330 prompts. HE-T2I  proposes 900 prompts to evaluate counting, shapes, and faces for text-to-image. Several compositional text-to-image benchmarks have also been proposed. Park _et al_.  proposes a benchmark on CUB Birds  and Oxford Flowers  to evaluate the models' ability to generate images with object-color and object-shape compositions. ABC-6K and CC500  benchmarks are proposed to evaluate attribute binding for text-to-image models, but they only focused on color attributes. Concurrent work HRS-Bench  is a general-purpose benchmark that evaluates 13 skills with 45,000 prompts. Compositionality is only one of the 13 evaluated skills which is not extensively studied. We propose the first comprehensive benchmark for open-world compositional text-to-image generation, shown in Table 1.

**Evaluation metrics for text-to-image generation.** Existing metrics for text-to-image generation can be categorized into fidelity assessment, alignment assessment, and LLM-based metrics. Traditional metrics such as Inception Score (IS)  and Frechet Inception Distance (FID)  are commonly used to evaluate the fidelity of synthesized images. To assess the image-text alignment, text-image matching by CLIP  and BLIP2  and text-text similarity by BLIP  captioning and CLIP text similarity are commonly used. Some concurrent works leverage the strong reasoning abilities of large language models (LLMs) for evaluation [43; 44]. However, there was no comprehensive study on how well those evaluation metrics work for compositional text-to-image generation. We propose evaluation metrics specifically designed for our benchmark and validate that our proposed metrics align better with human perceptions.

## 3 T2I-CompBench

Compositionality of text-to-image models refers to the ability of models to compose different concepts into a complex and coherent scene according to text prompts. It includes composing attributes with objects, composing different objects with specified interactions and spatial relationships, and complex compositions. To provide a clear definition of the problem and to build our benchmark, we introduce three categories and six sub-categories of compositionality, attribute binding (including three sub-categories: color, shape, and texture), object relationships (including two sub-categories: spatial relationship and non-spatial relationship), and complex compositions. We generate 1,000 text prompts (700 for training and 300 for testing) for each sub-category, resulting in 6,000 compositional text

  
**Benchmark** & **Prompts number and coverage** & **Vocabulary diversity** \\  CC-500  & 500 attr bind (color) & 196 nouns, 12 colors \\ ABC-6K  & 6,000 attr bind (color) & 3,690 nouns, 33 colors \\ Attn-Exct  & 210 attr bind (color) & 24 nouns, 11 colors \\ HRS-comp  & 1,000 attr bind (color, size), 2,000 rel (spatial, action) & 620 nouns, 5 colors, 11 spatial, 569 actions \\  T2I-CompBench & 3,000 attr bind (color, shape, texture) & 2,316 nouns, 33 colors, 32 shapes, 23 textures \\  & 2,000 rel (spatial, non-spatial), 1,000 complex & 7 spatial rel, 875 non-spatial rel \\   

Table 1: Comparison of compositional text-to-image benchmarks.

prompts in total. We take the balance between seen _v.s._ unseen compositions in the test set, prompts with fixed sentence template _v.s._ natural prompts, and simple _v.s._ complex prompts into consideration when constructing the benchmark. The text prompts are generated with either predefined rules or ChatGPT , so it is easy to scale up. Comparisons between our benchmark and previous benchmarks are shown in Table. 1.

### Attribute Binding

A critical challenge for compositional text-to-image generation is attribute binding, where attributes must be associated with corresponding objects in the generated images. We find that models tend to confuse the association between attributes and objects when there are more than one attribute and more than one object in the text prompt. For example, with the text prompt "A room with blue curtains and a yellow chair", the text-to-image model might generate a room with yellow curtains and a blue chair. We introduce three sub-categories, color, shape, and texture, according to the attribute type, and construct 1000 text prompts for each sub-category. For each sub-category, there are 800 prompts with the fixed sentence template "a {adj} {noun} and a {adj} {noun}" (_e.g._, "a red flower and a yellow vase") and 200 natural prompts without predefined sentence template (_e.g._, "a room with blue curtains and a yellow chair"). The 300-prompt test set of each sub-category consists of 200 prompts with seen adj-noun compositions (adj-noun compositions appeared in the training set) and 100 prompts with unseen adj-noun compositions (adj-noun compositions not in the training set).

**Color.** Color is the most commonly-used attribute for describing objects in images, and current text-to-image models often confuse the colors of different objects. The 1,000 text prompts related to color binding are constructed with 480 prompts from CC500 , 200 prompts from COCO , and 320 prompts generated by ChatGPT.

**Shape.** We define a set of shapes that are commonly used for describing objects in images: long, tall, short, big, small, cubic, cylindrical, pyramidal, round, circular, oval, oblong, spherical, triangular, square, rectangular, conical, pentagonal, teardrop, crescent, and diamond. We provide those shape attributes to ChatGPT and ask ChatGPT to generate prompts by composing those attributes with arbitrary objects, for example, "a rectangular clock and a long bench".

**Texture.** Textures are also commonly used to describe the appearance of objects. They can capture the visual properties of objects, such as smoothness, roughness, and granularity. We often use materials to describe the texture, such as wooden, plastic, and rubber. We define several texture attributes and the objects that can be described by each attribute. We generate 800 text prompts by randomly selecting from the possible combinations of two objects each associated with a textural attribute, _e.g._, "A rubber ball and a plastic bottle". We also generate 200 natural text prompts by ChatGPT.

### Object Relationship

When composing objects in a complex scene, the relationship between objects is a critical factor. We introduce 1,000 text prompts for spatial relationships and non-spatial relationships, respectively.

**Spatial relationships.** We use "on the side of", "next to", "near", "on the left of", "on the right of", "on the bottom of", and "on the top of" to define spatial relationships. The two nouns are randomly selected from persons (_e.g._, man, woman, girl, boy, person, _etc._), animals (_e.g._, cat, dog, horse, rabbit, frog, turtle, giraffe, _etc._), and objects (_e.g._, table, chair, car, bowl, bag, cup, computer, _etc._). For spatial relationships including left, right, bottom, and top, we construct contrastive prompts by swapping the two nouns, for example, "a girl on the left of a horse" and "a horse on the left of a girl".

**Non-spatial relationship.** Non-spatial relationships usually describe the interactions between two objects. We prompt ChatGPT to generate text prompts with non-spatial relationships (_e.g._, "watch", "speak to", "wear", "hold", "have", "look at", "talk to", "play with", "walk with", "stand on", "sit on", _etc._) and arbitrary nouns.

### Complex Compositions

To test text-to-image generation approaches with more natural and challenging compositional prompts in the open world, we introduce 1,000 text prompts with complex compositions of concepts beyond the pre-defined patterns. Regarding the number of objects, we create text prompts with more than two objects, for example, "a room with a blue chair, a black table, and yellow curtains". In terms of the attributes associated with objects, we can use multiple attributes to describe an object (denoted as _multiple attributes_, _e.g._, "a big, green apple and a tall, wooden table"), or leverage different types of attributes in a text prompt (denoted as _mixed attributes_, _e.g._, the prompt "a tall tree and a red car" includes both shape and color attributes). We generate 250 text prompts with ChatGPT for each of the four scenarios: _two objects with multiple attributes, two objects with mixed attributes, more than two objects with multiple attributes, and more than two objects with mixed attributes_. Relationship words can be adopted in each scenario to describe the relationships among two or more objects. For each scenario, we split 175 prompts for the training set and 75 prompts for the test set.

## 4 Evaluation Metrics

Evaluating compositional text-to-image generation is challenging as it requires comprehensive and fine-grained cross-modal understanding. Existing evaluation metrics leverage vision-language models trained on large-scale data for evaluation. CLIPScore [11; 12] calculates the cosine similarity between text features and generated-image features extracted by CLIP. Text-text similarity by BLIP-CLIP  applies BLIP  to generate captions for the generated images, and then calculates the CLIP text-text cosine similarity between the generated captions and text prompts. Those evaluation metrics can measure the coarse text-image similarity, but fails to capture fine-grained text-image correspondences in attribute binding and spatial relationships. To address those limitations, we propose new evaluation metrics for compositional text-to-image generation, shown in Fig. 2. Concretely, we propose _disentangled BLIP-VQA_ for attribute binding evaluation, _UniDet-based metric_ for spatial relationship evaluation, and _3-in-1_ metric for complex prompts. We further investigate the potential and limitations of multimodal large language models such as _MiniGPT-4_ with Chain-of-Thought  for compositionality evaluation.

### Disentangled BLIP-VQA for Attribute Binding Evaluation

We observe that the major limitation of the BLIP-CLIP evaluation is that the BLIP captioning models do not always describe the detailed attributes of each object. For example, the BLIP captioning model might describe an image as "A room with a table, a chair, and curtains", while the text prompt for generating this image is "A room with yellow curtains and a blue chair". So explicitly comparing the text-text similarity might cause ambiguity and confusion.

Therefore, we leverage the visual question answering (VQA) ability of BLIP  for evaluating attribute binding. For instance, given the image generated with the text prompt "a green bench

Figure 2: Illustration of our proposed evaluation metrics: (a) Disentangled BLIP-VQA for attribute binding evaluation, (b) UniDet for spatial relationship evaluation, and (c) MiniGPT4-CoT as a potential unified metric.

and a red car", we ask two questions separately: "a green bench?", and "a red car?". By explicitly disentangling the complex text prompt into two independent questions where each question contains only one object-attribute pair, we avoid confusion of BLIP-VQA. The BLIP-VQA model takes the generated image and several questions as input and we take the probability of answering "yes" as the score for a question. We compute the overall score by multiplying the probability of answering "yes" for each question. The proposed disentangled BLIP-VQA is applied to evaluate the attribute binding for color, shape, and texture.

### UniDet-based Spatial Relationship Evaluation

Most vision-language models perform poorly in reasoning spatial relationships such as "left" and "right". Therefore, we introduce a detection-based spatial relationship evaluation metric. We first use UniDet  to detect objects in the generated image. Then we determine the spatial relationship between two objects by comparing the locations of the centers of the two bounding boxes. Denote the center of the two objects as \((x_{1},y_{1})\) and \((x_{2},y_{2})\), respectively. The first object is on the left of the second object if \(x_{1}<x_{2},|x_{1}-x_{2}|>|y_{1}-y_{2}|\), and the intersection-over-union (IoU) between the two bounding boxes is below the threshold of \(0.1\). Other spatial relationships "right", "top", and "bottom" are evaluated similarly. We evaluate "next to", "near", and "on the side of" by comparing the distances between the centers of two objects with a threshold.

### 3-in-1 Metric for Complex Compositions Evaluation

Since different evaluation metrics are designed for evaluating different types of compositionality, there is no single metric that works well for all categories. We empirically find that the Disentangled BLIP-VQA works best for attribute binding evaluation, UniDet-baased metric works best for spatial relationship evaluation, and CLIPScore works best for non-spatial relationship evaluation. Thus, we design a 3-in-1 evaluation metric which computes the average score of CLIPScore, Disentangled BLIP-VQA, and UniDet, as the evaluation metric for complex compositions.

### Evaluation with Multimodal Large Language Models

By aligning a pretrained visual encoder with a frozen large language model, multimodal large language models such as MiniGPT-4  have demonstrated great abilities in vision-language cross-modal understanding. We leverage MiniGPT-4 with Chain-of-Thought as an evaluation metric by feeding the generated images to the model and asking two questions: "describe the image" and "predict the image-text alignment score". More information on the prompt design is provided in the appendix. We believe more advanced multimodal LLMs have the potential to be a unified evaluation metric in the future, but the current models exhibit limitations such as inaccurate understanding of images and hallucination issues.

## 5 Method

We introduce a simple but effective approach, Generative mOdel finetuning with Reward-driven Sample selection (GORS), to improve the compositional ability of pretrained text-to-image models. Our approach finetunes a pretrained text-to-image model such as Stable Diffusion  with generated images that highly align with the compositional prompts, where the fine-tuning loss is weighted by the reward which is defined as the alignment score between compositional prompts and generated images.

Specifically, given the text-to-image model \(p_{}\) and a set of text prompts \(y_{1},y_{2},,y_{n}\), we first generate \(k\) images for each text prompt, resulting in \(kn\) generated images \(x_{1},x_{2},,x_{kn}\). Text-image alignment scores \(s_{1},s_{2},,s_{kn}\) are predicted as rewards. We select the generated images whose rewards are higher than a threshold to fine-tune the text-to-image model. The selected set of samples are denoted as \(_{s}\). During fine-tuning, we weight the loss with the reward of each sample. Generated images that align with the compositional prompt better are assigned higher loss weights, and vice versa. The loss function for fine-tuning is

\[()=_{(x,y,s)_{s}}s\| -_{}(z_{t},t,y)\|_{2}^{2},\] (1)where \((x,y,s)\) is the triplet of the image, text prompt, and reward, and \(z_{t}\) represents the latent features of \(x\) at timestep \(t\). We adopt LoRA  for efficient finetuning.

## 6 Experiments

### Experimental Setup

**Evaluated models.** We evaluate the performance of 6 text-to-image models on T2I-CompBench. _Stable Diffusion v1-4_ and _Stable Diffusion v2_ are text-to-image models trained on large amount of image-text pairs. _Composable Diffusion_ is designed for conjunction and negation of concepts for pretrained diffusion models. _Structured Diffusion_ and _Attend-and-Excite_ are designed for attribute binding for pretrained diffusion models. We re-implement those approaches on Stable Diffusion v2 to enable fair comparisons. _GORS_ is our proposed approach which finetunes Stable Diffusion v2 with selected samples and their rewards. Since calculating the rewards for GORS with the automatic evaluation metrics can lead to biased results, we also provide alternative reward models (Appendix D.3) which are different from the evaluation metrics, which is denoted as _GORS-unbiased_.

**Implemenrtation details.** Please find the implementation details in the appendix.

### Evaluation Metrics

We generate 10 images for each text prompt in T2I-CompBench for automatic evaluation.

**Previous metrics.**_CLIPScore_[11; 12] (denoted as _CLIP_) calculates the cosine similarity between text features and generated-image features extracted by CLIP. _BLIP-CLIP_ (denoted as _B-CLIP_) applies BLIP  to generate captions for the generated images, and then calculates the CLIP text-text cosine similarity between the generated captions and text prompts. _BLIP-VQA-naive_ (denoted as _B-VQA-n_) applies BLIP VQA to ask a single question (e.g., a green bench and a red car?) with the whole prompt.

**Our proposed metrics.**_Disentangled BLIP-VQA_ (denoted as _B-VQA_) is our proposed evaluation metric for attribute binding. _UniDet_ is our proposed UniDet-based spatial relationship evaluation metric. _3-in-1_ computes the average score of CLIPScore, Disentangled BLIP-VQA, and UniDet, as the evaluation metric for complex compositions. _MiniGPT4-Chain-of-Thought_ (denoted as _mGPT-CoT_) serves as a potential unified metric for all types of compositional prompts based on multimodal LLM.

**Human evaluation.** For human evaluation of each sub-category, we randomly select 25 prompts and generate 2 images per prompt, resulting in 300 images generated with 150 prompts per model in total. The testing set includes 300 prompts for each sub-category, resulting in 1800 prompts in total. The prompt sampling rate for human evaluation is \(8.33\%\). We utilize Amazon Mechanical Turk and ask three workers to score each generated-image-text pair independently based on the image-text alignment.The worker can choose a score from \(\{1,2,3,4,5\}\) and we normalize the scores by dividing them by 5. We then compute the average score across all images and all workers.

    &  &  \\   & CLIP & B-CLIP & B-VQA-n & B-VQA & mGPT-CoT & Human & CLIP & B-CLIP & B-VQA-n & B-VQA & mGPT-CoT & Human \\  Stable v1-4  & 0.3241 & 0.7454 & 0.8575 & 0.3795 & 0.7424 & 0.6533 & 0.3112 & 0.7077 & 0.6771 & 0.3576 & 0.7197 & 0.6160 \\ Stable v2  & 0.3355 & 0.7616 & 0.7249 & 0.5085 & 0.7764 & 0.7374 & **0.3283** & 0.7191 & 0.7517 & 0.4221 & 0.7279 & 0.6587 \\ Compoundable v2  & 0.3178 & 0.7352 & 0.5714 & 0.4063 & 0.7524 & 0.6817 & 0.3092 & 0.6985 & 0.6125 & 0.3299 & 0.7124 & 0.5133 \\ Structured v2  & 0.3319 & 0.7626 & 0.7184 & 0.4950 & 0.7822 & 0.7867 & 0.3178 & 0.7177 & 0.5040 & 0.4218 & 0.7228 & 0.6413 \\ Att-Extr  & 0.3374 & **0.7819** & 0.8362 & 0.6040 & **0.8194** & 0.8320 & 0.3139 & **0.7209** & 0.7723 & 0.4517 & 0.7299 & 0.6380 \\  GORS-unbiased (ours) & 0.3390 & 0.7667 & 0.8219 & 0.6414 & 0.7987 & 0.8253 & 0.3175 & 0.7149 & 0.7630 & 0.4546 & 0.7263 & 0.6573 \\ GORS (ours) & **0.3395** & 0.7681 & **0.8471** & **0.6643** & 0.8067 & **0.8320** & 0.2973 & 0.7201 & **0.7937** & **0.4785** & **0.7303** & **0.7040** \\   

Table 2: Benchmarking on attribute binding (color and shape), with scores unnormalized. Blue represents the proposed metric for the category, and green indicates the human evaluation, applicable to the following Table 3 and Table 4.

### Quantitative and Qualitative Evaluation

The quantitative evaluation results are reported on attribute binding (color), attribute binding (shape) (Table 2), attribute binding (texture), spatial relationship (Table 3), non-spatial relationship, and comprehensive compositions (Table 4), respectively. Qualitative results are shown Figure 3.

**Comparisons across evaluation metrics.** Previous evaluation metrics, CLIP and BLIP-CLIP, predict similar scores for different models and cannot reflect the differences between models. Our proposed metrics, BLIP-VQA for attribute binding, UniDet for spatial relationship, CLIP for non-spatial relationship, and 3-in-1 for complex compositions, highly align with the human evaluation scores.

**Comparisons across text-to-image models.** (1) Stable Diffusion v2 consistently outperforms Stable Diffusion v1-4 in all types of compositional prompts and evaluation metrics. (2) Although Structured Diffusion built upon Stable Diffusion v1-4 shows great performance improvement in attribute binding as reported in Feng _et al._, Structured Diffusion built upon Stable Diffusion v2 only brings slight performance gain upon Stable Diffusion v2. It indicates that boosting the performance upon a better baseline of Stable Diffusion v2 is more challenging. (3) Composable Diffusion built upon Stable Diffusion v2 does not work well. A similar phenomenon was also observed in previous work  that Composable Diffusion often generates images containing a mixture of the subjects. In addition, Composable Diffusion was designed for concept conjunctions and negations so it is reasonable that it does not perform well in other compositional scenarios. (4) Attend-and-Excite built upon Stable Diffusion v2 improves the performance in attribute binding. (5) Previous methods Composable Diffusion , Structure Diffusion  and Attend-and-Excite  are designed for concept conjunction or attribute binding, so they do not result in significant improvements in object relationships. (6) Our proposed approach, GORS, outperforms previous approaches across all types of compositional prompts, as demonstrated by the automatic evaluation, human evaluation, and qualitative results. The evaluation results of GORS-unbiased and GORS significantly exceed the baseline Stable v2. Besides, GORS-unbiased achieves on-par performance with GORS, indicating that our proposed approach is insensitive to the reward model used for selecting samples, and that the proposed approach works well as long as high-quality samples are selected.

**Comparisons across compositionality categories.** According to the human evaluation results, spatial relationship is the most challenging sub-category for text-to-image models, and attribute binding (shape) is also challenging. Non-spatial relationship is the easiest sub-category.

    &  &  \\   & CLIP & B-CLIP & mGPT-CoT & Human & CLIP & B-CLIP & -3-in-1 & mGPT-CoT & Human \\  Stable v1-4  & 0.3079 & 0.7565 & 0.8170 & 0.9653 & 0.2876 & 0.6816 & 0.3080 & 0.8075 & 0.8067 \\ Stable v2  & 0.3127 & 0.7609 & 0.8235 & 0.9827 & 0.3096 & 0.6893 & 0.3386 & 0.8094 & 0.8480 \\ Composable v2  & 0.2980 & 0.7038 & 0.7936 & 0.8120 & 0.3014 & 0.6638 & 0.2898 & 0.8083 & 0.7520 \\ Structured v2  & 0.3111 & 0.7614 & 0.8221 & 0.9773 & 0.3084 & **0.6902** & 0.3355 & 0.8076 & 0.8333 \\ Attn-Exct v2  & 0.3109 & 0.7607 & 0.8214 & 0.9533 & 0.2913 & 0.6875 & 0.3401 & 0.8078 & 0.8573 \\  GORS-unbiased (ours) & 0.3158 & **0.7641** & **0.8353** & 0.9534 & **0.3137** & 0.6888 & **0.3470** & **0.8122** & 0.8654 \\ GORS (ours) & **0.3193** & 0.7619 & 0.8172 & **0.9853** & 0.2973 & 0.6841 & 0.3328 & 0.8095 & **0.8680** \\   

Table 4: Benchmarking on the non-spatial relationship and complex compositions.

    &  &  \\   & CLIP & B-CLIP & B-VQA & mGPT-CoT & Human & CLIP & B-CLIP & UniDet & mGPT-CoT & Human \\  Stable v1-4  & 0.3081 & 0.7111 & 0.6173 & 0.4156 & 0.7836 & 0.7227 & 0.3142 & 0.7667 & 0.1246 & 0.8338 & 0.3813 \\ Stable v2  & 0.3158 & 0.7240 & 0.7054 & 0.4922 & 0.7851 & 0.7827 & 0.3206 & 0.7723 & 0.1342 & 0.8367 & 0.3467 \\ Compounds v2  & 0.3092 & 0.6995 & 0.5604 & 0.3645 & 0.7588 & 0.6333 & 0.3001 & 0.7409 & 0.0800 & 0.8222 & 0.3089 \\ Structured v2  & 0.3167 & 0.7234 & 0.7007 & 0.4900 & 0.7806 & 0.7760 & 0.3201 & 0.7726 & 0.1386 & 0.8361 & 0.3467 \\ Attn-Exct v2  & 0.3171 & 0.7206 & 0.7830 & 0.5963 & 0.8062 & 0.8400 & 0.3213 & 0.7742 & 0.1455 & **0.8407** & 0.4027 \\  GORS-unbiased (ours) & 0.3216 & 0.7291 & 0.7778 & 0.6025 & 0.7985 & 0.8413 & 0.3237 & **0.7882** & 0.7125 & 0.8241 & 0.4467 \\ GORS (ours) & **0.3233** & **0.7315** & **0.7991** & **0.6287** & **0.8106** & **0.8573** & **0.3242** & 0.7854 & **0.1815** & 0.8362 & **0.4560** \\   

Table 3: Benchmarking on attribute binding (texture) and spatial relationship.

### Human Correlation of the Evaluation Metrics

We calculate Kendall's tau (\(\)) and Spearman's rho (\(\)) to evaluate the ranking correlation between automatic evaluation and human evaluation. The scores predicted by each evaluation metric are normalized to 0-1 for better comparison.

The human correlation results are illustrated in Table 5. The results verify the effectiveness of our proposed evaluation metrics, BLIP-VQA for attribute binding, UniDet-based metric for spatial relationships, CLIPScore for non-spatial relationships, and 3-in-1 for complex compositions. MiniGPT4-CoT does not perform well in terms of correlation with human perception, but we believe that the multimodal LLM-based evaluation metrics have the potential to become a unified evaluation metric in the future. We leave explorations on boosting the performance of multimodal LLM for compositional prompts to future work.

### Ablation study

We conduct ablation studies with the attribute binding (color) sub-category on our proposed GORS approach and MiniGPT4-CoT evaluation metric.

Figure 3: Qualitative comparison between our approach and previous methods.

**Finetuning strategy.** Our approach finetunes both the CLIP text encoder and the U-Net of Stable Diffusion with LoRA . We investigate the effects of finetuning CLIP only and U-Net only with LoRA. As shown in Table 6, our model which finetunes both CLIP and U-Net performs better.

**Threshold of selecting samples for finetuning.** Our approach fine-tunes Stable Diffusion v2 with the selected samples that align well with the compositional prompts. We manually set a threshold for the alignment score to select samples with higher alignment scores than the threshold for fine-tuning. We experiment with setting the threshold to half of its original value, and setting the threshold to 0 (_i.e._, use all generated images for finetuning with rewards, without selection). Results in Table 6 demonstrate that half threshold and zero threshold will lead to worse performance.

**MiniGPT4 without Chain-of-Thought.** We compare the evaluation metric of MiniGPT-4 with and without Chain-of-Thought, and with MiniGPT-4 for captioning and CLIP for text-text similarity, denoted as _mGPT-CLIP_. As shown in Table 5, Chain-of-Thought improves the human correlation of MiniGPT4-based evaluation by a large margin.

## 7 Conclusion and Discussions

We propose T2I-CompBench, a comprehensive benchmark for open-world compositional text-to-image generation, consisting of 6,000 prompts from 3 categories and 6 sub-categories. We propose new evaluation metrics and an improved baseline for the benchmark, and validate the effectiveness of the metrics and method by extensive evaluation. One limitation is that we do not have a unified evaluation metric for all categories of the benchmark. Please refer to appendix for failure cases of our proposed metrics. Additionally, our dataset primarily focuses on 2D spatial relationships. We suggest that assessing 3D spatial relationships could integrate depth maps into our existing UniDet-based evaluation metric, which we leave for future work. When studying generative models, researchers need to be aware of the potential negative social impact, for example, it might be abused to generate fake news. We also need to be aware of the bias from the image generators and the evaluation metrics based on pretrained multimodal models. More discussions on the limitation and potential negative social impact are provided in the appendix.