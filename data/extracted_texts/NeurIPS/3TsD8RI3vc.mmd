# Preconditioned Crank-Nicolson Algorithms for Wide Bayesian Neural Networks

Lucia Pezzetti

ETH AI Center

lucia.pezzetti@ai.ethz.ch &Stefano Favaro

University of Torino

and Collegio Carlo Alberto

stefano.favaro@unito.it &Stefano Peluchetti

Cogent Labs

speluchetti@cogent.co.jp

Work done when Lucia Pezzetti was at the University of Torino

###### Abstract

Bayesian Neural Networks represent a fascinating confluence of deep learning techniques and probabilistic reasoning, offering a compelling framework for understanding uncertainty in complex predictive models. In this paper, we consider Bayesian Neural Networks with Gaussian initialization and we investigate the use of the preconditioned Crank-Nicolson algorithm to sample from the reparametrized posterior distribution of the weights as the width of the network grows. In addition to being robust in the infinite-dimensional setting, we prove that the acceptance probability of the preconditioned Crank-Nicolson sampler approaches 1 as the width of the network goes to infinity, independently of any stepsize tuning. We then compare how the efficiency of the Langevin Monte Carlo, the preconditioned Crank-Nicolson and the preconditioned Crank-Nicolson Langevin samplers are influenced by changes in the network width in some real-world cases. In particular, we demonstrate that in wide Bayesian Neural Networks configurations, the proposed method allows for more efficient sampling, as evidenced by a higher effective sample size and improved diagnostic results compared with the Langevin Monte Carlo algorithm.

## 1 Introduction

Bayesian Neural Networks (BNNs) have emerged as a powerful framework for combining deep learning with probabilistic reasoning, offering a principled approach to understanding uncertainty in complex predictive models [1; 2; 3; 4; 5; 6; 7]. Despite their potential advantages, BNNs face significant challenges, particularly in sampling from high-dimensional posterior distributions of network weights. As the width of neural networks increases, standard Markov Chain Monte Carlo (MCMC) methods often struggle with efficiency and scalability. In this paper, we give theoretical and empirical guarantees to address these challenges by leveraging function-space MCMC techniques, specifically the preconditioned Crank-Nicolson (pCN) algorithm , for sampling from the posterior distributions of wide BNNs. Our method exploits recent theoretical advances in the understanding of wide neural networks and offers a robust sampling procedure that remains effective as network width increases.

Background and Related Works

BNNs extend traditional neural networks by treating weights as random variables, allowing for the quantification of uncertainty in predictions. However, BNNs have reached far less popularity than their deterministic counterparts due to the high computational requirements and limited theoretical understanding. One of the major theoretical challenges concerns the comprehension of the parameter-space behavior of BNNs, in spite of the function one. Specifically, while it is established that under Gaussian initializations, the function distributions of wide BNNs converge to the Neural Network Gaussian Process (NNGP) limit [9; 10; 11; 12; 13; 14; 15], the dynamics of the posterior distribution remains less understood, with few exceptions [16; 17]. We contribute to this literature by exploring sampling from the posterior distribution of wide BNNs, focusing on understanding its behavior and properties from a parameter-space perspective.

## 3 Proposed Method: pCN for Wide BNNs

Our approach leverages the pCN algorithm to sample from the posterior distribution of weights in wide BNNs. The key insight is to exploit the reparametrization proposed in  of the BNN weights that brings the posterior distribution closer to a standard Gaussian as the network width increases. Specifically, if we consider a BNN with L hidden layers and let \(d^{l}\) be the width of the \(l-th\) layer, the reparametrization is defined as:

\[^{(l)}=^{-}^{(L+1)}-&l=L+1\\ ^{(l)}&else\] (1)

where \(=[^{(l)}]_{l=0,...,L}(0,I_{D})\) is the collection of flattened weights of the BNN and \(\) and \(\) are data-dependent terms defined as:

\[=(I_{d^{L}}+^{-2}^{T})^{-1}=^{-2} ^{T}y\] (2)

Here, \(\) represents the scaled output of the penultimate layer, and \(^{2}\) is the observation variance. The convergence in the KL-divergence of the reparametrized posterior distribution to a standard Gaussian \((0,I_{D})\) as the width goes to infinity suggests potential improvements in the mixing speed of MCMC procedures, compared to sampling from the notably arduous BNN posterior. Nevertheless, standard MCMC algorithms are notoriously ill-suited for the infinite-dimensional setting and must be carefully re-tuned as the dimension increases to avoid degeneracy in the acceptance probability [18; 19]. To address these challenges, we focus on the robust pCN method, specifically designed to perform reliably in infinite-dimensional spaces.

The pCN algorithm  for sampling from the reparametrized posterior uses the following proposal:

\[^{*}=}+ w, w(0,I_{D})\] (3)

where \([0,1)\) is a stepsize parameter. The acceptance probability for this proposal is given by:

\[a(^{*}|)=min\{1,exp(-()+(^{*}))\}\] (4)

where \(\) is the log-likelihood.

Our main theoretical contribution is the following theorem:

**Theorem 3.1**: _Consider the BNN model with the reparametrization 1. The acceptance probability of the pCN algorithm to sample from the reparametrized weight posterior, for any \([0,1)\), converges to 1 as the width of the network increases._

This result has profound implications for sampling from wide BNNs. It guarantees that as the network becomes wider, the pCN algorithm becomes increasingly efficient, with nearly all proposals being accepted. This contrasts sharply with traditional MCMC methods, which often require careful tuning of step sizes to maintain reasonable acceptance rates in high dimensions. The proof Theorem 3.1 relies on the convergence of the empirical Neural Network Gaussian Process (NNGP) kernel to a constant independent of the weights as the network width increases and is reported in Appendix A.

## 4 Empirical Results

To validate our theoretical findings and demonstrate the effectiveness of the pCN sampler for wide BNNs, we conducted a series of experiments on the CIFAR-10 dataset. We replicate the setting in (17) using a fully-connected neural network with one hidden layer and varying the width from 512 to 4096 neurons. The code is available at github.com/lucia-pezzetti/Function-Space-MCMC-for-Wide-BNNs.

### Acceptance Rate Convergence

Figure 1 shows the acceptance rates for pCN, underdamped Langevin Monte Carlo (LMC), and preconditioned Crank-Nicolson Langevin (pCNL) (8) samplers as a function of network width. The results clearly demonstrate that the pCN acceptance rate steadily increases as the network width grows, approaching 1 for very wide networks. This empirically confirms our theoretical prediction in Theorem 3.1. In contrast, the LMC sampler shows a decline in acceptance rate for wider networks, highlighting the advantage of pCN in high-dimensional settings. The empirical results also showcase that the pCNL, leveraging both the dimensional-robustness of the pCN and the gradient-informed proposal of the LMC, reaches the most desirable performances. This suggests the possibility to adapt Theorem 3.1 also to the Langevin version of the pCN sampler.

### Effective Sample Size Analysis

Figure 2 presents the per-step Effective Sample Size (ESS) (20) for different samplers across various network widths. Both the pCN and pCNL samplers show a consistent increase in ESS as the network width grows, indicating improved sampling efficiency. This is particularly evident for larger step sizes (\(=0.2\) and \(=0.1\)), where they outperform the LMC for wider networks. The increasing ESS demonstrates that the samplers not only maintains a high acceptance rate but also produces less correlated samples in high-dimensional spaces. The results for the smallest stepsize confirm the necessity of avoiding degeneracy in the stepsize, as this introduces autocorrelation among the collected samples and leads to a deterioration in their quality.

## 5 Discussion and Conclusion

In this paper, we investigated the effectiveness of the pCN sampler in sampling the posterior distribution of wide BNNs. Our method leverages recent theoretical insights into the behavior of wide neural networks and addresses the challenges of sampling in high-dimensional spaces. The key contributions

Figure 1: Comparison at different stepsizes (\(=0.2,0.1,0.01\)) of the acceptance probability obtained using: i. the underdamped LMC algorithm (or Metropolis Adjusted Langevin Algorithm: MALA); ii. the pCN algorithm; iii. the pCNL method. The neural network architecture used is a fully-connected with one hidden layer, and layer width that varies among the following values: \(512,1024,2048,4096\). The CIFAR-10 dataset is used, with the sample size fixed at \(n=256\). The acceptance rate of the pCN increases steadily as the width of the BNN grows at every stepsize, suggesting improved performance in wide BNNs and empirically confirming our theoretical analysis. The pCNL algorithm shows a similar trend in its acceptance rate, outperforming the other samples. In contrast, the LMC initially shows generally a deterioration in its acceptance rate as the width of the BNN increases, reflecting the sampler’s non-robustness in high-dimensional settings.

of our work include theoretical guarantees for the convergence of the pCN acceptance probability to 1 as network width increases, and their empirical validation through extensive experiments on real-world datasets.

Our findings have significant implications for Bayesian deep learning, offering a scalable and robust approach to uncertainty quantification in large neural networks and contributing to the broader goal of combining the expressiveness of deep learning with the rigorous uncertainty quantification of Bayesian methods. Future directions for this work include extending the approach to more complex network architectures, such as convolutional and recurrent neural networks, and investigating theoretical guarantees for the pCNL sampler.

Figure 2: ESS analysis of the LMC, pCN and pCNL algorithms as a function of the 1-layer FCN’s width for stepsizes \(=0.2\) (above), \(=0.1\) (middle) and \(=0.01\) (below). The solid lines represent the average per-step ESS, whereas the shaded areas indicate the variability of the per-step ESS delineated by its minimum and maximum values. The setting used in the experiments is the same as the setting of Figure 1: the layer width of the BNN varies among the following values: \(\{128,\ 512,\ 1024,\ 2096,\ 4192\}\). The CIFAR-10 dataset is used, with sample size fixed at \(n=256\). The poor LMC performance reflects the fact that standard MCMC procedures are ill-posed in high-dimensional settings. In contrast, the pCN and pCNL samplers demonstrate constant growth in ESS as the network width increases, indicating that enhancements in acceptance rate contribute positively to efficiency and performance. Finally, the smallest stepsize, \(=0.01\), heavily affects the behavior of both algorithms, introducing high autocorrelation among the samples and affecting their quality.