# Formal Theorem Proving by Rewarding LLMs to Decompose Proofs Hierarchically

Kefan Dong   Arvind Mahankali   Tengyu Ma

Stanford University

{kefandong, amahank, tengyuma}@stanford.edu

###### Abstract

Mathematical theorem proving is an important testbed for large language models' deep and abstract reasoning capability. This paper focuses on improving LLMs' ability to write proofs in formal languages that permit automated proof verification/evaluation. Most previous results provide human-written lemmas to the theorem prover, which is an arguably oversimplified setting that does not sufficiently test the provers' planning and decomposition capabilities. Instead, we work in a more natural setup where the lemmas that are directly relevant to the theorem are not given to the theorem prover at test time. We design an RL-based training algorithm that encourages the model to decompose a theorem into lemmas, prove the lemmas, and then prove the theorem by using the lemmas. Our reward mechanism is inspired by how mathematicians train themselves: even if a theorem is too challenging to be proved by the current model, a positive reward is still given to the model for any correct and novel lemmas that are proposed and proved in this process. During training, our model proposes and proves lemmas that are not in the training dataset. In fact, these newly-proposed correct lemmas consist of 37.7% of the training replay buffer when we train on the dataset extracted from Archive of Formal Proofs (AFP). The model trained by our RL algorithm outperforms that trained by supervised finetuning, improving the pass rate from 40.8% to 45.5% on AFP test set, and from 36.5% to 39.5% on an out-of-distribution test set.

## 1 Introduction

The reasoning abilities of large language models (LLMs) are a significant marker of artificial intelligence and critical for complex and safety-sensitive applications. Yet recent studies highlight the limited performance of LLMs on reasoning tasks (e.g., Mundler et al. (2023), Valmeekam et al. (2023) and references therein).

Automated theorem proving by LLMs is an excellent reasoning task that abstracts away the need for numerical manipulation or tool use (e.g., using a calculator) and allows for precise correctness evaluation with an automatic verifier (such as Isabelle (Nipkow et al., 2002) and Lean (De Moura et al., 2015)), even without ground truth. Thanks to tools such as Sledgehammer (Paulsson and Blanchette, 2012) that can automatically complete low-level details, the granularity of formal proofs is similar to natural language proofs (see Fig. 1 Left for an illustrative example). Note that verifying a proof is fundamentally much easier than generating the proof.1 Thus, learning to prove theorems from verifiers' supervision is reminiscent of weak-to-strong generalization (Burns et al., 2023).

Previous results in this area largely focus on setting where the theorem prover can use all the lemmas in the formal proof library, including those particularly written to decompose a specific theorem'sproof (Jiang et al., 2021; Polu and Sutskever, 2020) This setting arguably oversimplifies the problem and doesn't sufficiently test the models' planning and decomposition capabilities, and it is unclear whether the resulting models can be used to prove new theorems from scratch when such lemmas are not available at test time. Instead, we work in a more natural setup where the theorem prover needs to propose and prove lemmas to decompose the proof hierarchically itself (see Section 2 for more details). In Section C.2, we demonstrate that this task is indeed much more challenging.

In addition, most existing proof-generation algorithms leverage the formal verifier by (a) providing the verifier's current proof state to the LLMs step-by-step, and (b) using best-first search algorithms such as A\({}^{*}\) to build a multi-step proof from many LLM-generated steps (Jiang et al., 2022; Han et al., 2021). The major challenge of these method is the high computation cost incurred in both (a) and (b) because (a) requires re-running LLMs on a different context that contains a long verifier's proof state at every step, and (b) the search is expensive. Consequently, the best method along this line of research requires more than 1k GPU days with A100s to train a model with 600M parameters (Lample et al., 2022), whereas our method only takes less than 36 GPU days to train a 7B model.

To address these issues, we design a method, called Proof Decomposer (ProD), that uses LLMs to propose and prove new lemmas hierarchically and generate whole proofs directly without searching. We augment the vanilla formal proof syntax so that the model can propose new lemmas by stating their statements during the proof, and then prove the lemmas separately. Hence, a complete proof of a theorem will form a tree structure where the child nodes are the lemmas proposed in the proof of the parent node (Fig. 1 Right), and the theorem is proved only if all the proofs in the tree are correct.

Figure 1: **Left: in the dashed callout block, we show an example of an Isabelle proof and its explanation in natural language. Right: an example of a proof tree. The two child nodes correspond to the two new lemmas proposed in the proof of the root node.**

Figure 2: Illustration of our algorithm Proof Tree Generator (ProD)-RL. In step 2b, locally correct means that the statement is proved correctly using the proposed lemmas, and globally correct means that all the proposed lemmas are also proved correctly. As an important feature of our algorithm, even if Theorem 1 is not proved by the model because the proof to Lemmas 1 is incorrect, we still train on the correct lemma (Lemma 2) by setting its reward \(r=1\).

We train our LLMs with reinforcement learning (RL) in a way that somewhat imitates the mathematician's process: we reward correct partial proofs (i.e., proof sub-trees) even if the original theorem (i.e., the root node) is not proved entirely. Since our model can generate and prove novel lemmas during training, it could still make progress even if the model is given a very challenging theorem. This is similar to mathematicians publishing papers on interesting lemmas that do not solve the original goal. We illustrate our algorithm in Fig. 2, and defer the details to Section B.2.

We test our model ProD-RL by generating proof trees on holdout theorems that the model is never trained on, and we show that our model ProD-RL outperforms several other baselines. Compared with the supervised fine-tuned (SFT) model on the same training set, our model improves the pass rate from 40.8% to 47.5% on the holdout test set, whereas vanilla reinforcement learning without lemma proposals during training does not improve the corresponding SFT model (see Section 3.2). This is partly because our method encourages the model to propose and prove additional lemmas -- in fact, 37.7% of the proved lemmas during training are not in the dataset. As a result, the model still improves even if it's already fine-tuned on the same dataset with human-written ground-truth proofs.

## 2 Setup

**Conditional proofs.** We use the term _conditional proof_ to denote a proof that, in addition to the standard formal proof syntax, can propose new lemmas by enclosing their statements by <invoke> and </invoke> tokens (examples shown in the blue boxes of Fig. 1). A conditional proof has the following format:

\(t_{1}\) <invoke> \(l_{1}\) </invoke> \(t_{2}\) <invoke> \(l_{2}\) </invoke> \(\)\(t_{k}\) <invoke> \(l_{k}\) </invoke> \(t_{k+1}\)

where \(t_{1},,t_{k+1}\) denote proof segments in the original formal proof syntax (see e.g., Fig. 1, proof texts in black), and \(l_{1},,l_{t}\) denote proposed lemma statements (see e.g. Fig. 1, proof texts in red).2

**Proof tree nodes.** With the proposed lemmas, a complete proof forms a tree structure (as shown in Fig. 1). A node in a proof tree is a tuple of premises, context, a theorem statement, and a conditional proof. Premises represent the lemmas that are treated as common knowledge, which are typically not directly relevant to the proof. We allow the model to directly use them in the proof so that it does not have to repetitively prove all the fundamental facts, such as properties of continuous functions and natural numbers. Context represents the necessary contents to prepare the theorem statement, such as the definition of specific objects/functions. We use the context as part of the prompt for the LLMs to generate proofs, and to prepare the proof verifier to check the generated proofs.

**Correctness of conditional proofs and proof trees.** A proof tree node \(n\) with conditional proof \(t_{1}\) <invoke> \(l_{1}\) </invoke> \(t_{2}\) <invoke> \(l_{2}\) </invoke> \(\)\(t_{k}\) <invoke> \(l_{k}\) </invoke> \(t_{k+1}\) is _locally correct_ if, after adding \(l_{1},,l_{k}\) to the set of premises, \(t_{1} t_{k+1}\), is a proof to the statement of \(n\) acceptable by the formal verifier under the context of \(n\).

We consider a proof tree valid if, for every node, each of its child nodes corresponds to one proposed lemma and shares the same premises and context with its parent node. A tree node \(n\) is _globally correct_ with respect to a given set of tree nodes \(N\) if we can construct a valid proof tree with root \(n\) using the locally correct tree nodes in \(N\). Intuitively, global correctness corresponds to the standard notion of correctness (i.e., whether the theorem is proved), and local correctness is a weaker concept, referring to the correctness of conditional proofs assuming the proposed lemmas.

**Dataset construction.** We construct the datasets by parsing raw proof-library files into examples of the form (premises, context, statement, conditional proof). For any theorem \(s\), we construct an example where the premises are all the theorems from _predecessor_ files. To compute the context, we iteratively remove all the theorem statement and its proof from the proof-library files if the theorem is not referred to in the remaining file contents (in other words, in the first iteration we peel off the root nodes of the proof trees from the file, and then the nodes in the next level, etc.). Finally, to augment the original proof of theorem \(s\) to a conditional proof with lemma proposal, for every proof step \(t_{j}\) that calls lemmas \(l_{j,1},,l_{j,n_{j}}\) (and these lemmas were removed from the context), we insert the statements of these lemmas enclosed by the <invoke> and </invoke> tokens into the proof right before \(t_{j}\). We defer additional details of our dataset construction process to Appendix A.

We split the training and test set (AFP test) based on the dependency of the files in the proof library so that the examples in the training set never refer to any files in the test set. We also construct an additional test set AFP 2023 by parsing the AFP files submitted after the knowledge cutoff date of the Lemma model (April 2023) to eliminate potential data leakage issues.

Compared with prior works (Jiang et al., 2021; First et al., 2023), the two major differences in our setup are the availability of lemmas from the same file and the training/test split. In Section C.2, we discuss and test their effects in detail.

## 3 Experiments

### Reinforcement learning algorithm

Due to limited space, we present a sketch of our RL algorithm here and defer the details to Appendix B.

**Proof tree generation.** To generate proof trees using an autoregressive model \(_{}\), we need to first fine-tune the model to follow a specific format:

1. the input \(x\) to the model \(_{}\) is the concatenation of a context and a theorem statement, and
2. the expected output \(y\) of the model is a special token \(t_{0}\) followed by a conditional proof, where \(t_{0}\) is either <use_invoke> or <no_invoke>, denoting whether the following conditional proof should propose new lemmas.

We let the model generate the special token \(t_{0}\) before a conditional proof so that we can upscale the probability of the <use_invoke> token during RL to propose more lemmas for better exploration. Using the fine-tuned model, we can generate conditional proofs and complete the proof tree recursively, as shown in Alg. 1.

**Reinforcement learning.** Our method is illustrated in Fig. 2. We start with a supervised fine-tuned model so that it can generate conditional proofs in the desired format. At every round, we sample a batch of examples from the training dataset and generate proof trees using the current model. Then we call the proof verifier to test the generated proofs, and assign rewards based on the global correctness of the conditional proofs. Finally, we train the model using REINFORCE with a replay buffer.

We update the model using partial proofs (i.e., proof sub-trees) even if the original theorem from the dataset (i.e, the root of the proof tree) is not proved. Hence, our method can also be viewed as an instantiation of hindsight experience replay (Andrychowicz et al., 2017), where the hindsight trajectories are correct proof sub-trees.

### Main results

This section reports the models' pass@k performance on AFP test and AFP 2023 datasets. For a given dataset, pass@k measures the percentage of the theorems proved by at least one of \(k\) proofs generated by the model. Recall that the theorems in the test set are selected based on the dependencies of the AFP files and are not used in any proofs from the training set (see Section C.2 for more details). In other words, we do not train the model on test datasets using reinforcement learning. Instead, we test whether ProD-RL is a fundamentally better model when tested on new theorems.

As a baseline method, we train a model on a variant of the SFT dataset where all lemmas are kept in the context. It can be seen as a reproduction of First et al. (2023) with a slightly different way to obtain the context -- First et al. (2023) includes all the file content before the st

    & SFT w/o & RL w/o & & \\ Test set & lemma proposal & lemma proposal & ProD-SFT & ProD-RL \\  AFP test & 43.4 & 42.4 & 40.8 & **45.5** \\  AFP 2023 & **39.4** & 37.7 & 36.5 & **39.5** \\   

Table 1: Pass@16 of different models on AFP test sets. Our model with reinforcement learning (ProD-RL) improves upon the SFT model and outperforms baseline methods.

whereas we only keep the statement of previous lemmas. We also run reinforcement learning on the same RL dataset as our method (see Section E.2 for more details).

Table 1 shows the performance of our model on the AFP test sets. For a fair comparison, the baseline models are tested in our new setup without human-written lemmas. Note that the SFT model without lemma proposal outperforms the SFT model with lemma proposal. We hypothesize that it is because proposing correct lemmas itself is challenging, which distracts the model from learning to generate direct proofs. However, RL with lemma proposal improves the SFT model and outperforms others because the model proposes and proves additional lemmas that are not in the training dataset, whereas RL without lemma proposal yields no improvement.

In Fig. 3 (Left), we plot the pass rates with different numbers of samples per theorem on both AFP test and AFP 2023. Fig. 3 shows that on AFP test, the ProD-RL model significantly improves upon baseline methods as well as the ProD-SFT. However, on AFP 2023, the improvement is minor over SFT w/o lemma proposal, while ProD-RL still outperforms ProD-SFT. The results suggest that the baseline methods are more robust to heavier distribution shifts, while our method has a larger improvement when the test distribution is closer to the training distribution.

In Fig. 3 (Right), we decompose the proved theorems by the depth of their ground-truth proofs (shown on the \(x\)-axis) and the depth of generated proof trees (indicated by color). When there are multiple correct proof trees, we plot the one with the maximum depth. As a comparison, we also plot the success rates of the proofs generated by the RL model trained w/o lemma proposal. Fig. 3 (Right) shows that the improvement of ProD-RL mostly comes from proving theorems with low-to-medium difficulty where the depth of the ground-truth proof is at most 2. For more complex theorems, both models' pass rates are low and the improvement of our method is not significant, meaning that they are currently beyond the models' capability.

Due to limited space, we defer the case study for the generated lemmas to Appendix C.4.

## 4 Conclusion

In this paper, we design a reinforcement learning algorithm that encourages LLMs to write formal proofs by decomposing them hierarchically. We also design a more natural testing setup by removing the directly relevant lemmas from the context. We show that, by proposing and proving new lemmas that are not present in the training dataset, the resulting model ProD-RL outperforms or achieves comparable performance to baseline methods trained on the same dataset.

Figure 3: **Left**: The pass rate of different models on AFP test. Our RL model improves upon the SFT model whereas without proposing new lemmas (RL w/o lemma proposal), we do not observe any improvement. **Right**:The pass rate of theorems in AFP test grouped by the depth of their ground-truth proof. Grey bars represent the proof generated by the model SFT w/o lemma proposal, and the colored bars represent the proof trees generated by ProD-RL with various depths.