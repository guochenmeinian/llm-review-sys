# Multi-Swap \(k\)-Means++

Lorenzo Beretta

University of Copenhagen

lorenzo2beretta@gmail.com &Vincent Cohen-Addad

Google Research

cohenaddad@google.com &Silvio Lattanzi

Google Research

silviol@google.com &Nikos Parotsidis

Google Research

nikosp@google.com

Authors are ordered in alphabetical order.

###### Abstract

The \(k\)-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is often the practitioners' choice algorithm for optimizing the popular \(k\)-means clustering objective and is known to give an \(O( k)\)-approximation in expectation. To obtain higher quality solutions, Lattanzi and Sohler (ICML 2019) proposed augmenting \(k\)-means++ with \(O(k k)\) local search steps obtained through the \(k\)-means++ sampling distribution to yield a \(c\)-approximation to the \(k\)-means clustering problem, where \(c\) is a large absolute constant. Here we generalize and extend their local search algorithm by considering larger and more sophisticated local search neighborhoods hence allowing to swap multiple centers at the same time. Our algorithm achieves a \(9+\) approximation ratio, which is the best possible for local search. Importantly we show that our approach yields substantial practical improvements, we show significant quality improvements over the approach of Lattanzi and Sohler (ICML 2019) on several datasets.

## 1 Introduction

Clustering is a central problem in unsupervised learning. In clustering one is interested in grouping together "similar" object and separate "dissimilar" one. Thanks to its popularity many notions of clustering have been proposed overtime. In this paper, we focus on metric clustering and on one of the most studied problem in the area: the Euclidean \(k\)-means problem.

In the Euclidean \(k\)-means problem one is given in input a set of points \(P\) in \(^{d}\). The goal of the problem is to find a set of \(k\) centers so that the sum of the square distances to the centers is minimized. More formally, we are interested in finding a set \(C\) of \(k\) points in \(^{d}\) such that \(_{p P}_{c C}||p-c||^{2}\), where with \(||p-c||\) we denote the Euclidean distance between \(p\) and \(c\).

The \(k\)-means problem has a long history, in statistics and operations research. For Euclidean \(k\)-means with running time polynomial in both \(n,k\) and \(d\), a \(5.912\)-approximation was recently shown in Cohen-Addad et al. (2022), improving upon Kanungo et al. (2004), Ahmadian et al. (2019), Grandoni et al. (2022) by leveraging the properties of the Euclidean metric. In terms of lower bounds, the first to show that the high-dimensional \(k\)-means problems were APX-hard were Guruswami and Indyk (2003), and later Awasthi et al. (2015) showed that the APX-hardness holds even if the centers can be placed arbitrarily in \(^{d}\). The inapproximability bound was later slightly improved by Lee et al. (2017) until the recent best known bounds of Cohen-Addad and Karthik C. S. (2019), Cohen-Addad et al. (2022);  that showed that it is NP-hard to achieve a better than 1.06-approximation and hard to approximate it better than 1.36 assuming a stronger conjecture. From a more practical point of view, Arthur and Vassilvitskii (2009) showed that the widely-used popular heuristic of Lloyd Lloyd can lead to solutions with arbitrarily bad approximation guarantees, but can be improved by a simple seeding strategy, called \(k\)-means++, so as to guarantee that the output is within an \(O( k)\) factor of the optimum Arthur and Vassilvitskii .

Thanks to its simplicity \(k\)-means++ is widely adopted in practice. In an effort to improve its performances Lattanzi and Sohler , Choo et al.  combine \(k\)-means++ and local search to efficiently obtain a constant approximation algorithm with good practical performance. These two studies show that one can use the \(k\)-means++ distribution in combination with a local search algorithm to get the best of both worlds: a practical algorithm with constant approximation guarantees.

However, the constant obtained in Lattanzi and Sohler , Choo et al.  is very large (several thousands in theory) and the question as whether one could obtain a practical algorithm that would efficiently match the \(9+\)-approximation obtained by the \(n^{O(d/)}\) algorithm of Kanungo et al.  has remained open. Bridging the gap between the theoretical approach of Kanungo et al.  and \(k\)-means++ has thus been a long standing goal.

Our Contributions.We make significant progress on the above line of work.

* We adapt techniques from the analysis of Kanungo et al.  to obtain a tighter analysis of the algorithm in Lattanzi and Sohler . In particular in Corollary 4, we show that their algorithm achieves an approximation of ratio of \( 26.64\).
* We extend this approach to multi-swaps, where we allow swapping more than one center at each iteration of local search, improving significantly the approximation to \( 10.48\) in time \(O(nd poly(k))\).
* Leveraging ideas from Cohen-Addad et al. , we design a better local search swap that improves the approximation further to \(9+\) (see Theorem 12). This new algorithm matches the \(9+\)-approximation achieved by the local search algorithm in Kanungo et al. , but it is significantly more efficient. Notice that \(9\) is the best approximation achievable through local search algorithms, as proved in Kanungo et al. .
* We provide experiments where we compare against \(k\)-means++ and Lattanzi and Sohler . We study a variant of our algorithm that performs very competitively with our theoretically sound algorithm. The variant is very efficient and still outperforms previous work in terms of solution quality, even after the standard postprocessing using Lloyd.

Additional Related Work.We start by reviewing the approach of Kanungo et al.  and a possible adaptation to our setting. The bound of \(9+\) on the approximation guarantee shown by Kanungo et al.  is for the following algorithm: Given a set \(S\) of \(k\) centers, if there is a set \(S^{+}\) of at most \(2/\) points in \(^{d}\) together with a set \(S^{-}\) of \(|S^{+}|\) points in \(S\) such that \(S S^{-} S^{+}\) achieves a better \(k\)-means cost than \(S\), then set \(S:=S S^{-} S^{+}\) and repeat until convergence. The main drawback of the algorithm is that it asks whether there exists a set \(S^{+}\) of points in \(^{d}\) that could be swapped with elements of \(S\) to improve the cost. Identifying such a set, even of constant size, is already non-trivial. The best way of doing so is through the following path: First compute a coreset using the state-of-the-art coreset construction of Cohen-Addad et al. [2022b] and apply the dimensionality reduction of Becchetti et al. , Makarychev et al. , hence obtaining a set of \((k/^{4})\) points in dimension \(O( k/^{2})\). Then, compute grids using the discretization framework of Matousek  to identify a set of \(^{-O(d)} k^{O(^{-2}(1/))}\) grid points that contains nearly-optimum centers. Now, run the local search algorithm where the sets \(S^{+}\) are chosen from the grid points by brute-force enumeration over all possible subsets of grid points of size at most, say \(s\). The running time of the whole algorithm with swaps of magnitude \(s\), i.e.: \(|S^{+}| s\), hence becomes \(k^{O(s^{-2}(1/))}\) for an approximation of \((1+)(9+2/s)\), meaning a dependency in \(k\) of \(k^{O(^{-3}(1/))}\) to achieve a \(9+\)-approximation. Our results improves upon this approach in two ways: (1) it improves over the above theoretical bound and (2) does so through an efficient and implementable, i.e.: practical, algorithm.

Recently, Grunau et al.  looked at how much applying a greedy rule on top of the \(k\)-means++ heuristic improves its performance. The heuristic is that at each step, the algorithm samples \(\) centers and only keeps the one that gives the best improvement in cost. Interestingly the authors prove that from a theoretical standpoint this heuristic does not improve the quality of the output. Local search algorithms for \(k\)-median and \(k\)-means have also been studied by Gupta and Tangwongsan  who drastically simplified the analysis of Arya et al. . Cohen-Addad and Schwiegelshohn demonstrated the power of local search for stable instances. Friggstad et al. (2019); Cohen-Addad et al. (2019) showed that local search yields a PTAS for Euclidean inputs of bounded dimension (and doubling metrics) and minor-free metrics. Cohen-Addad (2018) showed how to speed up the local search algorithm using \(kd\)-trees (i.e.: for low dimensional inputs).

For fixed \(k\), there are several known approximation schemes, typically using small coresets Becchetti et al. (2019); Feldman and Langberg (2011); Kumar et al. (2010). The state-of-the-art approaches are due to Bhattacharya et al. (2020); Jaiswal et al. (2014). The best known coreset construction remains Cohen-Addad et al. (2022c,b).

If the constraint on the number of output centers is relaxed, then we talk about bicriteria approximations and \(k\)-means has been largely studied Bandyapadhyay and Varadarajan (2016); Charikar and Guha (2005); Cohen-Addad and Mathieu (2015); Korupolu et al. (2000); Makarychev et al. (2016).

## 2 Preliminaries

Notation.We denote with \(P^{d}\) the set of input points and let \(n=|P|\). Given a point set \(Q P\) we use \((Q)\) to denote the mean of points in \(Q\). Given a point \(p P\) and a set of centers \(A\) we denote with \(A[p]\) the closest center in \(A\) to \(p\) (ties are broken arbitrarily). We denote with \(\) the set of centers currently found by our algorithm and with \(^{*}\) an optimal set of centers. Therefore, given \(p P\), we denote with \([p]\) and \(^{*}[p]\) its closest ALG-center and OPT-center respectively. We denote by \((Q,A)\) the cost of points in \(Q P\) w.r.t. the centers in \(A\), namely

\[(Q,A)=_{q Q}_{e A}||q-c||^{2}\,.\]

We use ALG and OPT as a shorthand for \((P,)\) and \((P,^{*})\) respectively. When we sample points proportionally to their current cost (namely, sample \(q\) with probability \((q,)\,/(P,)\)) we call this the \(D^{2}\) distribution. When using \(O_{}()\) and \(_{}()\) we mean that \(\) is considered constant. We use \((f)\) to hide polylogarithmic factors in \(f\). The following lemma is folklore.

**Lemma 1**.: _Given a point set \(Q P\) and a point \(p P\) we have_

\[(Q,p)=(Q,(Q))+|Q|||p-(Q)||^{2}\,.\]

Let \(O_{i}^{*}\) be an optimal cluster, we define the _radius_ of \(O_{i}^{*}\) as \(_{i}\) such that \(_{i}^{2}|O_{i}^{*}|=(O_{i}^{*},o_{i})\), where \(o_{i}=(O_{i}^{*})\). We define the \(\)_-core_ of the optimal cluster \(O_{i}^{*}\) as the set of points \(p O_{i}^{*}\) that lie in a ball of radius \((1+)_{i}\) centered in \(o_{i}\). In symbols, \((O_{i}^{*})=P B(o_{i},(1+)_{i})\). Throughout the paper, \(\) is always a small constant fixed upfront, hence we omit it.

**Lemma 2**.: _Let \(O_{i}^{*}\) be an optimal cluster and sample \(q O_{i}^{*}\) according to the \(D^{2}\)-distribution restricted to \(O_{i}^{*}\). If \((O_{i}^{*},)>(2+3)(O_{i}^{*},o _{i})\) then \([q(O_{i}^{*})]=_{}(1)\)._

Proof.: Define \(:=(O_{i}^{*},)\,/(O_{i}^{*},o_{i})>2 +3\). Thanks to Lemma 1, for each \(c\) we have \(\|c-o_{i}\|^{2}(-1)_{i}^{2}\). Therefore, for each \(y(O_{i}^{*})\) and every \(c\) we have

\[(y,c)=||y-c||^{2}(-(1+))^{2} _{i}^{2}=_{}(_{i}^{2}).\]

Moreover, by a Markov's inequality argument we have \(|O_{i}^{*}(O_{i}^{*})||O_{i} ^{*}|\) and thus \(|(O_{i}^{*})|_{}(|O_{i}^{*}|)\). Combining everything we get

\[((O_{i}^{*})\,,)|(O_{i}^ {*})\,|_{c\\ y(O_{i}^{*})}(y,c)=_{} (|O_{i}^{*}|)_{}(_{i}^{2})\]

and \(|O_{i}^{*}|_{i}^{2}=(O_{i}^{*},)\), hence \(((O_{i}^{*})\,,)=_{}((O_{i}^{*},))\). 

## 3 Multi-Swap \(k\)-Means++

The single-swap local search (SSLS) \(k\)-means++ algorithm in Lattanzi and Sohler (2019) works as follows. First, \(k\) centers are sampled using \(k\)-means++ (namely, they are sampled one by one according to the \(D^{2}\) distribution, updated for every new center). Then, \(O(k k)\) steps of local search follow. In each local search step a point \(q P\) is \(D^{2}\)-sampled, then let \(c\) be the center among the current centers \(\) such that \((P,(\{c\})\{q\})\) is minimum. If \((P,(\{c\})\{q\})<(P,)\) then we swap \(c\) and \(q\), or more formally we set \((\{c\})\{q\}\).

We extend the SSLS so that we allow to swap multiple centers simultaneously and call this algorithm multi-swap local search (MSLS) \(k\)-means++. Swapping multiple centers at the same time achieves a lower approximation ratio, in exchange for a higher time complexity. In this section, we present and analyse the \(p\)-swap local search (LS) algorithm for a generic number of \(p\) centers swapped at each step. For any constant \(>0\), we obtain an approximation ratio \(/=^{2}+\) where

\[^{2}-(2+2/p)-(4+2/p)=0.\] (1)

The Algorithm.First, we initialize our set of centers using \(k\)-means++. Then, we run \(O(ndk^{p-1})\) local search steps, where a local search step works as follows. We \(D^{2}\)-sample a set \(In=\{q_{1} q_{p}\}\) of points from \(P\) (without updating costs). Then, we iterate over all possible sets \(Out=\{c_{1} c_{p}\}\) of \(p\) distinct elements in \( In\) and select the set \(Out\) such that performing the swap \((In,Out)\) maximally improves the cost2. If this choice of \(Out\) improves the cost, then we perform the swap \((In,Out)\), else we do not perform any swap for this step.

**Theorem 3**.: _For any \(>0\), the \(p\)-swap local search algorithm above runs in \((ndk^{2p})\) time and, with constant probability, finds an \((^{2}+)\)-approximation of \(k\)-means, where \(\) satisfies Equation (1)._

Notice that the SSLS algorithm of Lattanzi and Sohler (2019) is exactly the \(p\)-swap LS algorithm above for \(p=1\).

**Corollary 4**.: _The single-swap local search in Lattanzi and Sohler (2019), Choo et al. (2020) achieves an approximation ratio \(<26.64\)._

**Corollary 5**.: _For \(p=O(1)\) large enough, multi-swap local search achieves an approximation ratio \(<10.48\) in time \(O(nd poly(k))\)._

### Analysis of Multi-Swap \(k\)-means++

In this section we prove Theorem 3. Our main stepping stone is the following lemma.

**Lemma 6**.: _Let ALG denote the cost at some point in the execution of MSLS. As long as \(/>^{2}+\), a local search step improves the cost by a factor \(1-(1/k)\) with probability \((1/k^{p-1})\)._

Proof of Theorem 3.: First, we show that \(O(k^{p} k)\) local steps suffice to obtain the desired approximation ratio, with constant probability. Notice that a local search step can only improve the cost function, so it is sufficient to show that the approximation ratio is achieved at some point in time. We initialize our centers using \(k\)-means++, which gives a \(O( k)\)-approximation in expectation. Thus, using Markov's inequality the approximation guarantee \(O( k)\) holds with arbitrary high constant probability. We say that a local-search step is _successful_ if it improves the cost by a factor of at least \(1-(1/k)\). Thanks to Lemma 6, we know that unless the algorithm has already achieved the desired approximation ratio then a local-search step is successful with probability \((1/k^{p-1})\). To go from \(O( k)\) to \(^{2}+\) we need \(O(k k)\) successful local search steps. Standard concentration bounds on the value of a Negative Binomial random variable show that, with high probability, the number of trial to obtain \(O(k k)\) successful local-search steps is \(O(k^{p} k)\). Therefore, after \(O(k^{p} k)\) local-search steps we obtain an approximation ratio of \(^{2}+\).

To prove the running time bound it is sufficient to show that a local search step can be performed in time \((ndk^{p-1})\). This is possible if we maintain, for each point \(x P\), a dynamic sorted dictionary3 storing the pairs \(((x,c_{i})\,,c_{i})\) for each \(c_{i}\). Then we can combine the exhaustive search over all possible size-\(p\) subsets of \( In\) and the computation of the new cost function using time \(O(ndk^{p-1} k)\). To do so, we iterate over all possible size-\((p-1)\) subsets \(Z\) of \( In\) and update all costs as if these centers were removed, then for each point \(x P\) we compute how much its cost increases if we remove its closest center \(c_{x}\) in \(( In) Z\) and charge that amount to \(c_{x}\). In the end, we consider \(Out=Z\{c\}\) where \(c\) is the cheapest-to-remove center found in this way. 

The rest of this section is devoted to proving Lemma 6. For convenience, we prove that Lemma 6 holds whenever \(/>^{2}+O()\), which is wlog by rescaling \(\). Recall that we now focus on a given step of the algorithm, and when we say current cost, current centers and current clusters we refer to the state of these objects at the end of the last local-search step before the current one. Let \(O_{1}^{*} O_{k}^{*}\) be an optimal clustering of \(P\) and let \(^{*}=\{o_{i}=(O_{i}^{*})i=1 k\}\) be the set of optimal centers of these clusters. We denote with \(C_{1} C_{k}\) the current set of clusters at that stage of the local search and with \(=\{c_{1} c_{k}\}\) the set of their respective current centers.

We say that \(c_{i}\)_captures_\(o_{j}\) if \(c_{i}\) is the closest current center to \(o_{j}\), namely \(c_{i}=[o_{j}]\). We say that \(c_{i}\) is _busy_ if it captures more than \(p\) optimal centers, and we say it is _lonely_ if it captures no optimal center. Let \(}=\{o_{i}(O_{i}^{*},)> /k\}\) and \(}=\{[o_{i}] o_{i} ^{*}}\}\). For ease of notation, we simply assume that \(}=\{o_{1} o_{h}\}\) and \(}=\{c_{1} c_{h^{}}\}\). Notice that \(h^{}>h\).

Weighted ideal multi-swaps.Given \(In P\) and \(Out}\) of the same size we say that the swap \((In,Out)\) is an _ideal_ swap if \(In}\). We now build a set of _weighted_ ideal multi-swaps \(\). First, suppose wlog that \(\{c_{1} c_{t}\}\) is the set of current centers in \(}\) that are neither lonely nor busy. Let \(\) be the set of lonely centers in \(}\). For each \(i=1 t\), we do the following. Let \(In\) be the set of optimal centers in \(}\) captured by \(c_{i}\). Choose a set \(_{i}\) of \(|In|-1\) centers from \(\), set \(_{i}\) and define \(Out=_{i}\{c_{i}\}\). Assign weight \(1\) to \((In,Out)\) and add it to \(\). For each busy center \(c_{i}\{c_{t+1} c_{h^{}}\}\) let \(A\) be the set of optimal centers in \(}\) captured by \(c_{i}\), pick a set \(_{i}\) of \(|A|-1\) lonely current centers from \(\) (a counting argument shows that this is always possible). Set \(_{i}\). For each \(o_{j} A\) and \(c_{}_{i}\) assign weight \(1/(|A|-1)\) to \((o_{j},c_{})\) and add it to \(\). Suppose we are left with \(\) centers \(o_{1}^{} o_{}^{}}\) such that \([o_{i}^{}]}\). Apparently, we have not included any \(o_{i}^{}\) in any swap yet. However, since \(|}||}|\), we are left with at least \(^{}\) lonely centers \(c_{1}^{} c_{^{}}^{}}\). For each \(i=1\) we assign weight \(1\) to \((o_{i}^{},c_{i}^{})\) and add it to \(\).

**Observation 7**.: _The process above generates a set of weighted ideal multi-swaps such that: (i) Every swap has size at most \(p\); (ii) The combined weights of swaps involving an optimal center \(o_{i}}\) is \(1\); (iii) The combined weights of swaps involving a current center \(c_{i}\) is at most \(1+1/p\)._

Consider an ideal swap \((In,Out)\). Let \(O_{In}^{*}=_{o_{i} In}O_{i}^{*}\) and \(C_{Out}=_{c_{j} Out}C_{j}\). Define the reassignment cost \((In,Out)\) as the increase in cost of reassigning points in \(C_{Out} O_{In}^{*}\) to centers in \( Out\). Namely,

\[(In,Out)=(C_{Out} O_{In}^{*}, Out)-(C_{Out} O_{In}^{*},)\,.\]

We take the increase in cost of the following reassignment as an upper bound to the reassignment cost. For each \(p C_{Out} O_{In}^{*}\) we consider its closest optimal center \(^{*}[p]\) and reassign \(p\) to the current center that is closest to \(^{*}[p]\), namely \([^{*}[p]]\). In formulas, we have

\[(In,Out) _{p C_{Out} O_{In}^{*}}(p, [^{*}[p]])-(p,[p])\] \[_{p C_{Out}}(p,[^{* }[p]])-(p,[p])\,.\]

Indeed, by the way we defined our ideal swaps we have \([^{*}[p]] Out\) for each \(p O_{In}^{*}\) and this reassignment is valid. Notice that the right hand side in the equation above does not depend on \(In\).

**Lemma 8**.: \(_{p P}(p,[^{*}[p]]) 2OPT++2 }}\)_._

Proof.: Deferred to the supplementary material. 

**Lemma 9**.: _The combined weighted reassignment costs of all ideal multi-swaps in \(\) is at most \((2+2/p)(+}})\)._Proof.: Denote by \(w(In,Out)\) the weight associated with the swap \((In,Out)\).

\[_{(In,Out)}w(In,Out)(In,Out)\] \[_{(In,Out)}w(In,Out)_{p C_{Out}} (p,[^{*}[p]])-(p,[p])\] \[(1+1/p)_{c_{j}}_{p C_{j}}(p,[^{*}[p]])-(p,[p])\] \[(1+1/p)(_{p P}(p,[ ^{*}[p]])-).\]

The second inequality uses \((iii)\) from Observation 7. Applying Lemma 8 completes the proof. 

Recall the notions of radius and core of an optimal cluster introduced in Section 2. We say that a swap \((In,Out)\) is _strongly improving_ if \((P,( In) Out)(1-/k) (P,)\). Let \(In=\{o_{1} o_{s}\}}\) and \(Out=\{c_{1} c_{s}\}}\) we say that an ideal swap \((In,Out)\) is _good_ if for every \(q_{1}(o_{1}) q_{s}(o_{s})\) the swap \((,Out)\) is strongly improving, where \(=\{q_{1} q_{s}\}\). We call an ideal swap _bad_ otherwise. We say that an optimal center \(o_{i}}\) is good if that's the case for at least one of the ideal swaps it belongs to, otherwise we say that it is bad. Notice that each optimal center in \(}\) is assigned to a swap in \(\), so it is either good or bad. Denote with \(G\) the union of cores of good optimal centers in \(}\).

**Lemma 10**.: _If an ideal swap \((In,Out)\) is bad, then we have_

\[(O_{In}^{*},)(2+)(O_{In}^{*}, ^{*})+(In,Out)+/k.\] (2)

Proof.: Let \(In=\{o_{1} o_{s}\}\), \(=\{q_{1} q_{s}\}\) such that \(q_{1}(o_{1}) q_{s}(o_{s})\). Then, by Lemma 1\((O_{In}^{*},)(2+)(O_{In}^{*}, ^{*})\). Moreover, \((In,Out)=(P O_{In}^{*},  Out)-(P O_{In}^{*},)\) because points in \(P C_{Out}\) are not affected by the swap. Therefore, \((P,() Out)(2+) (O_{In}^{*},^{*})+(In,Out)+(P O_{In}^{*},)\). Suppose by contradiction that Equation (2) does not hold, then

\[(P,)-(P,()  Out)=\]

\[(P O_{In}^{*},)+(O_{In}^{*}, )-(P,() Out) /k.\]

Hence, \((,Out)\) is strongly improving and this holds for any choice of \(\), contradiction. 

**Lemma 11**.: _If \(/>^{2}+\) then \((G,)=_{}((P,))\). Thus, if we \(D^{2}\)-sample \(q\) we have \(P[q G]=_{}(1)\)._

Proof.: First, we observe that the combined current cost of all optimal clusters in \(^{*}}\) is at most \(k/k=\). Now, we prove that the combined current cost of all \(O_{i}^{*}\) such that \(o_{i}\) is bad is \((1-2)\). Suppose, by contradiction, that it is not the case, then we have:

\[(1-2)<_{\ o_{i}}}(O_{i}^{*},)_{\ (In,Out)}w(In,Out)(O_{In}^{*},)\] \[_{\ (In,Out)}w(In,Out)((2+)(O_{In}^{*}, ^{*})+(In,Out)+/k)\] \[(2+)+(2+2/p)+(2+2/p)}}+.\]

The second and last inequalities make use of Observation 7. The third inequality uses Lemma 10.

Setting \(^{2}=/\) we obtain the inequality \(^{2}-(2+2/p O())-(4+2/p O()) 0\). Hence, we obtain a contradiction in the previous argument as long as \(^{2}-(2+2/p O())-(4+2/p O())>0\). A contradiction there implies that at least an \(\)-fraction of the current cost is due to points in \(_{\ o_{i}}}O_{i}^{*}\). We combine this with Lemma 2 and conclude that the total current cost of \(G=_{\ o_{i}}}(O_{i}^ {*})\) is \(_{}((P,))\).

Finally, we prove Lemma 6. Whenever \(q_{1} G\) we have that \(q_{1}(o_{1})\) for some good \(o_{1}\). Then, for some \(s p\) we can complete \(o_{1}\) with \(o_{2} o_{s}\) such that \(In=\{o_{1} o_{s}\}\) belongs to a good swap. Concretely, there exists \(Out\) such that \((In,Out)\) is a good swap. Since \(In}\) we have \((O_{i}^{*},)>/k\) for all \(o_{i} In\), which combined with Lemma 2 gives that for \(i=2 s\)\(P[q_{i}(o_{i})]_{}(1/k)\). Hence, we have \(P[q_{i}(o_{i})i=1 s]_{,p}(1/k^{p-1})\). Whenever we sample \(q_{1} q_{s}\) from \((o_{1})(o_{s})\), we have that \((,Out)\) is strongly improving. Notice, however, that \((,Out)\) is a \(s\)-swap and we may have \(s<p\). Nevertheless, whenever we sample \(q_{1} q_{s}\) followed by any sequence \(q_{s+1} q_{p}\) it is enough to choose \(Out^{}=Out\{q_{s+1} q_{p}\}\) to obtain that \((\{q_{1} q_{p}\},Out^{})\) is an improving \(p\)-swap.

## 4 A Faster \((9+)\)-Approximation Local Search Algorithm

The MSLS algorithm from Section 3 achieves an approximation ratio of \(^{2}+\), where \(^{2}-(2+2/p)-(4+2/p)=0\) and \(>0\) is an arbitrary small constant. For large \(p\) we have \( 10.48\). On the other hand, employing \(p\) simultaneous swaps, Kanungo et al. (2004) achieve an approximation factor of \(^{2}+\) where \(^{2}-(2+2/p)-(3+2/p)=0\). If we set \(p 1/\) this yields a \((9+O())\)-approximation. In the same paper, they prove that \(9\)-approximation is indeed the best possible for \(p\)-swap local search, if \(p\) is constant (see Theorem \(3.1\) in Kanungo et al. (2004)). They showed that \(9\) is the right locality gap for local search, but they matched it with a very slow algorithm. To achieve a \((9+)\)-approximation, they discretize the space reducing to \(O(n^{-d})\) candidate centers and perform an exhaustive search over all size-\((1/)\) subsets of candidates at every step. As we saw in the related work section, it is possible to combine techniques from coreset and dimensionality reduction to reduce the number of points to \(n^{}=k poly(^{-1})\) and the number of dimensions to \(d^{}= k^{-2}\). This reduces the complexity of Kanungo et al. (2004) to \(k^{O(^{-3}^{-1})}\).

In this section, we leverage techniques from Cohen-Addad et al. (2021) to achieve a \((9+)\)-approximation faster 4. In particular, we obtain the following.

**Theorem 12**.: _Given a set of \(n\) points in \(^{d}\) with aspect ratio \(\), there exists an algorithm that computes a \(9+\)-approximation to \(k\)-means in time \(ndk^{O(^{-2})}^{O(^{-1})}() 2^{-poly( ^{-1})}\)._

Notice that, besides being asymptotically slower, the pipeline obtained combining known techniques is highly impractical and thus it did not make for an experimental test-bed. Moreover, it is not obvious how to simplify such an ensemble of complex techniques to obtain a practical algorithm.

Limitations of MSLS.The barrier we need to overcome in order to match the bound in Kanungo et al. (2004) is that, while we only consider points in \(P\) as candidate centers, the discretization they employ considers also points in \(^{d} P\). In the analysis of MSLS we show that we sample each point \(q_{i}\) from \((O_{i}^{*})\) or equivalently that \(q_{i} B(o_{i},(1+)_{i})\), where \(_{i}\) is such that \(O_{i}^{*}\) would have the same cost w.r.t. \(o_{i}\) if all its points were moved on a sphere of radius \(_{i}\) centered in \(o_{i}\). This allows us to use a Markov's inequality kind of argument and conclude that there must be \(_{}(|O_{i}^{*}|)\) points in \(O_{i}^{*} B(o_{i},(1+)_{i})\). However, we have no guarantee that there is any point at all in \(O_{i}^{*} B(o_{i},(1-)_{i})\). Indeed, all points in \(O_{i}^{*}\) might lie on \( B(o_{i},_{i})\). The fact that potentially all our candidate centers \(q\) are at distance at least \(_{i}\) from \(o_{i}\) yields (by Lemma 1) \((O_{i}^{*},q)(O_{i}^{*},o_{i})\), which causes the zero-degree term in \(^{2}-(2+2/p)-(3+2/p)=0\) from Kanungo et al. (2004) to become a \(4\) in our analysis.

Improving MSLS by taking averages.First, we notice that, in order to achieve \((9+)\)-approximation we need to set \(p=(1/)\). The main hurdle to achieve a \((9+)\)-approximation is that we need to replace the \(q_{i}\) in MSLS with a better approximation of \(o_{i}\). We design a subroutine that computes, with constant probability, an \(\)-approximation \(_{i}\) of \(o_{i}\) (namely, \((O_{i}^{*},_{i})(1+)(O_{i}^{*}, o_{i})\)). The key idea is that, if sample uniformly \(O(1/)\) points from \(O_{i}^{*}\) and define \(_{i}\) to be the average of our samples then \((O_{i}^{*},_{i})(1+)(O_{i}^{* },o_{i})\)

Though, we do not know \(O_{i}^{*}\), so sampling uniformly from it is non-trivial. To achieve that, for each \(q_{i}\) we identify a set \(N\) of _nice_ candidate points in \(P\) such that a \(poly()/k\) fraction of them are from \(O_{i}^{*}\). We sample \(O(1/)\) points uniformly from \(N\) and thus with probability \((/k)^{O(1/)}\) we sample only points from \(O_{i}^{*}\). Thus far, we sampled \(O(1/)\) points uniformly from \(N O_{i}^{*}\). What about the points in \(O_{i}^{*} N\)? We can define \(N\) so that all points in \(O_{i}^{*} N\) are either very close to some of the \((q_{j})_{j}\) or they are very far from \(q_{i}\). The points that are very close to points \((q_{j})_{j}\) are easy to treat. Indeed, we can approximately locate them and we just need to guess their mass, which is matters only when \( poly()\)ALG, and so we pay only a \(^{O(1/)}(1/)\) multiplicative overhead to guess the mass close to \(q_{j}\) for \(j=1 p=(1/)\). As for a point \(f\) that is very far from \(q_{i}\) (say, \(||f-q_{i}||_{i}\)) we notice that, although \(f\)'s contribution to \((O_{i}^{*},o_{i})\) may be large, we have \((f,o)(f,o_{i})\) for each \(o B(q_{i},_{i}) B(o_{i},(2+)_{i})\) assuming \(q_{i}(o_{i})\).

## 5 Experiments

In this section, we show that our new algorithm using multi-swap local search can be employed to design an efficient seeding algorithm for Lloyd's which outperforms both the classical \(k\)-means++ seeding and the single-swap local search from Lattanzi and Sohler (2019).

Algorithms.The multi-swap local search algorithm that we analysed above performs very well in terms of solution quality. This empirically verifies the improved approximation factor of our algorithm, compared to the single-swap local search of Lattanzi and Sohler (2019).

Motivated by practical considerations, we heuristically adapt our algorithm to make it very competitive with SSLS in terms of running time and still remain very close, in terms of solution quality, to the theoretically superior algorithm that we analyzed. The adaptation of our algorithm replaces the phase where it selects the \(p\) centers to swap-out by performing an exhaustive search over \(\) subsets of centers. Instead, we use an efficient heuristic procedure for selecting the \(p\) centers to swap-out, by greedily selecting one by one the centers to swap-out. Specifically, we select the first center to be the cheapest one to remove (namely, the one that increases the cost by the least amount once the points in its cluster are reassigned to the remaining centers). Then, we update all costs and select the next center iteratively. After \(p\) repetitions we are done. We perform an experimental evaluation of the "greedy" variant of our algorithm compared to the theoretically-sound algorithm from Section 3 and show that employing the greedy heuristic does not measurably impact performance.

The four algorithms that we evaluate are the following: 1) **KM++:** The \(k\)-means++ from Arthur and Vassilvitskii (2007), 2) **SSLS:** The Single-swap local search method from Lattanzi and Sohler (2019), 3) **MSLS:** The multi-swap local search from Section 3, and 4) **MSLS-G:** The greedy variant of multi-swap local search as described above.

We use MSLS-G-\(p=x\) and MSLS-\(p=x\), to denote MSLS-G and MSLS with \(p=x\), respectively. Notice that MSLS-G-\(p=1\) is exactly SSLS. Our experimental evaluation explores the effect of \(p\)-swap LS, for \(p>1\), in terms of solution cost and running time.

Datasets.We consider the three datasets used in Lattanzi and Sohler (2019) to evaluate the performance of SSLS: 1) KDD-PHY - \(100,000\) points with \(78\) features representing a quantum physic task kdd (2004), 2) RNA - \(488,565\) points with \(8\) features representing RNA input sequence pairs Uzilov et al. (2006), and 3) KDD-BIO - \(145,751\) points with \(74\) features measuring the match between a protein and a native sequence kdd (2004). We discuss the results for two or our datasets, namely KDD-BIO and RNA. We defer the results on KDD-PHY to the appendix and note that the results are very similar to the results on RNA.

We performed a preprocessing step to clean-up the datasets. We observed that the standard deviation of some features was disproportionately high. This causes all costs being concentrated in few dimensions making the problem, in some sense, lower-dimensional. Thus, we apply min-max scaling to all datasets and observed that this causes all our features' standard deviations to be comparable.

Experimental setting.All our code is written in Python. The code will be made available upon publication of this work. We did not make use of parallelization techniques. To run our experiments, we used a personal computer with \(8\) cores, a \(1.8\) Ghz processor, and \(15.9\) GiB of main memory We run all experiments 5 times and report the mean and standard deviation in our plots. All our plots report the progression of the cost either w.r.t local search steps, or Lloyd's iterations. We run experiments on all our datasets for \(k=10,25,50\). The main body of the paper reports the results for \(k=25\), while the rest can be found in the appendix. We note that the conclusions of the experiments for \(k=10,50\) are similar to those of \(k=25\).

Removing centers greedily.We first we compare MSLS-G with MSLS. To perform our experiment, we initialize \(k=25\) centers using \(k\)-means++ and then run \(50\) iterations of local search for both 

[MISSING_PAGE_FAIL:9]

quality. A very interesting open question is to improve our local search procedure by avoiding the exhaustive search over all possible size-\(p\) subsets of centers to swap out, concretely an algorithm with running time \((2^{poly(1/)}ndk)\).

Acknowledgements.This work was partially done when Lorenzo Beretta was a Research Student at Google Research. Moreover, Lorenzo Beretta receives funding from the European Union's Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement No. 801199.

Figure 3: Comparison of the cost produced by MSLS-G, for \(p\{1,4,7,10\}\) and \(k=25\) on the datasets KDD-BIO and KDD-PHU, divided by the mean cost of KM++ after running for fixed amount of time in terms of multiplicative factors to the average time for an iteration of Lloyd’s algorithm (i.e., for deadlines that are \(1,,20\) the average time of an iteration of Lloyd).

Figure 2: The first row compares the cost of MSLS-G, for \(p\{1,4,7,10\}\), divided by the mean cost of KM++ at each LS step, for \(k=25\). The legend reports also the running time of MSLS-G per LS step (in seconds). The second row compares the cost after each of the \(10\) iterations of Lloyd with seeding from MSLS-G, for \(p\{1,4,7,10\}\) and \(15\) local search steps and KM++, for \(k=25\).