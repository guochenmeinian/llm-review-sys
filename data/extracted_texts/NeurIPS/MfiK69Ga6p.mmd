# Protein Design with Guided Discrete Diffusion

Nate Gruver\({}^{*1}\) Samuel Stanton\({}^{*2}\) Nathan Frey\({}^{2}\) Tim G. J. Rudner\({}^{1}\) Isidro Hotzel\({}^{3}\)

Julien Lafrance-Vanasse\({}^{3}\) Arvind Rajpal\({}^{3}\) Kyunghyun Cho\({}^{1,2}\) Andrew Gordon Wilson\({}^{1}\)

###### Abstract

A popular approach to protein design is to combine a generative model with a discriminative model for conditional sampling. The generative model samples plausible sequences while the discriminative model guides a search for sequences with high fitness. Given its broad success in conditional sampling, classifier-guided diffusion modeling is a promising foundation for protein design, leading many to develop guided diffusion models for structure with inverse folding to recover sequences. In this work, we propose _diffusioN_**_O_**_ptimized **Sampling_** (NOS), a guidance method for discrete diffusion models that follows gradients in the hidden states of the denoising network. NOS makes it possible to perform design directly in sequence space, circumventing significant limitations of structure-based methods, including scarce data and challenging inverse design. Moreover, we use NOS to generalize LaMBO, a Bayesian optimization procedure for sequence design that facilitates multiple objectives and edit-based constraints. The resulting method, _LaMBO-2_, enables discrete diffusions and stronger performance with limited edits through a novel application of saliency maps. We apply LaMBO-2 to a real-world protein design task, optimizing antibodies for higher expression yield and binding affinity to several therapeutic targets under locality and developability constraints, attaining a 99% expression rate and 40% binding rate in exploratory _in vitro_ experiments.

## 1 Introduction

Optimizing protein sequences for improved function has the potential for widespread impact . Among many potential applications in engineering and medicine, engineered antibodies can be used to create cancer therapeutics that are much less harmful to the patient than radiotherapy or chemotherapy. Because the space of possible proteins is vast and discrete, and experimental validation is slow and expensive, all practical methods for protein design must restrict themselves to a small enriched library of candidates to find a viable option in as few measurements as possible . In practice these enriched libraries are usually obtained through massive high-throughput _in vitro_ screening , or in the case of antibodies by injecting a live animal with the target antigen and sequencing the animal's immune response . Generative protein models offer the tantalizing prospect of enriched libraries produced nearly instantly at a fraction of the cost. Success in real-world applications, however, has proven elusive, in part because naive generative models produce outputs that are similar to their training data and are therefore unlikely to improve target qualities .

There are many approaches to guided generation of proteins, but one broad and important distinction is between methods that search in sequence space and those that search in structure space. A basic tenet of molecular biology is "sequence determines structure, structure determines function" . Hence when optimizing a protein for a desired function, it may seem more direct to design the protein in structure space, where gradient-based sampling methods can be used in tandem with carefullyengineered potentials . One of the drawbacks of this approach is the optimized structure must still be converted back to an amino acid sequence in order to be synthesized, a task known as "inverse-folding" . There is no guarantee that the optimized structure can be realized by an actual sequence, and the inverse-folding procedure may not find the sequence if it exists. Structural models are also computationally intensive and limited by the scarcity of high-quality structural data. Searching directly in sequence space eliminates the need to recover sequence from structure. Protein sequence models are also comparatively fast, especially during inference, and can leverage sequence datasets that are often several orders of magnitude larger than their structural equivalents.

Although sequence models are arguably the most practical foundation for protein design, they have historically suffered from the challenges of optimizing discrete sequences, where gradient-based sampling is not directly applicable. As a result, many sequence search methods resort to gradient-free sampling methods like Metropolis-Hastings MCMC , which are flexible but computationally expensive, eroding a key advantage over structure search. Several methods have been proposed that maintain gradient-based search by performing guidance in a continuous latent space, with a learned decoder to sample discrete sequences . Notably, Stanton et al.  proposed LaMBO (**L**atent **M**ulti-Objective **B**ayesian **O**ptimization), an optimization method that uses masked language model (MLM) style denoising guided with Bayesian acquisition values to address the online, multi-objective nature of real-world protein design. While LaMBO can quickly sample sequences with improved acquisition value, it has two key limitations. First, LaMBO is built on top of MLMs which, while strong representation learners are not strong generative models. In particular, MLMs lag behind other methods in producing high likelihood samples or infills. Second, despite being designed to improve known sequences instead of designing them completely from scratch, LaMBO and related methods have no principled framework for simultaneously enforcing an edit budget and choosing optimal edit locations based on that budget.

To address the first issue we propose **NOS** (diffusio**N** **O**ptimized **S**ampling), a new method for controllable categorical diffusion (Figure 1). Diffusion models capture complex distributional dependencies by making iterative denoising steps, but there is relatively little previous work on how to control these processes. NOS generates sequences with both high likelihood and desirable qualities by taking many alternating steps between corruption, guidance, and denoising in the continuous latent space of the model. Our _in silico_ validation shows that NOS outperforms many state-of-the-art structure and sequence-based baselines on both unguided and guided infilling tasks.2 To address the second problem (choosing optimal edit locations) we propose using embedding-gradient feature attributions (i.e. saliency maps) to determine which positions on the sequence are most important to edit to improve the guidance objective value. We combine NOS with saliency-selected edits to create LaMBO-2, a more powerful variant of the original LaMBO algorithm. Exploratory _in vitro_ experimental validation of our designs provides evidence that LaMBO-2 can be used to create enriched antibody libraries without the aid of additional _in vitro_ high-throughput screening.

Figure 1: We propose diffusio**N** **O**ptimized **S**ampling (**NOS**), a method for gradient-guided sampling from discrete diffusion models. NOS uses \(T\) sampling steps of denoising diffusion, where each step consists of applying a corruption, gradient steps to optimize a value function, \(f\), and sampling of the next discrete sequence, or corresponding latent state. NOS generates samples that optimize an arbitrary objective while maintaining high likelihood with respect to a reference distribution of sequences. We combine NOS with LaMBO, a strong Bayesian optimization method for sequence design , to make LaMBO-2, our improved method for protein design.

Related Work

Discrete diffusionsAustin et al.  and Hoogeboom et al.  constructed diffusion models for discrete categorical data using a categorical noise process. Recently, categorical diffusion has shown promise as a competitor to autoregressive models in text generation for machine translation and summarization. The approaches can be roughly grouped into methods that apply categorical noise distributions directly to sequences (CMLM , SUNDAE ), and those that apply Gaussian corruptions to token-vector embeddings (SED , CDCD ). In this work we show that NOS can guide both types of categorical diffusions. Within the space of protein design, our method is also closely related to joint diffusion models over both sequence and structure [2; 51], which also circumvent inverse folding. Because these models still rely on structure information at training time, they can be limited by data availability in the same manner as pure structure models.

Discrete generative guidanceGradient guidance typically augments sampling from a generative model with gradient steps to increase the occurrence of a desired attribute . Gradient guidance is natural within the framework of continuous diffusion models , and Li et al.  use this connection to perform gradient-guided sampling from a diffusion language model. To obtain a continuous space, they perform Gaussian diffusion  on word embeddings, decoding out to tokens using a linear head. The original method required many careful engineering interventions, e.g. clamping latent representations to the nearest word embedding, that have been improved by recent methods, such as CDCD , but gradient guidance has not been discussed for these more recent formulations.

To achieve a similar form of gradient guidance without carefully engineering a latent space, Dathathri et al.  and Yang and Klein  propose gradient-guided autoregressive models by using the decoder's activations as a gradient-friendly latent space. These methods alternate between sampling from logits and ascending the likelihood of a separately trained classifier model. Surprisingly, despite work on gradient guidance for continuous noise diffusions and autoregressive language models, there has been little work on gradient guidance for general categorical diffusions that predict denoised categorical distributions (e.g. CMLM, SUNDAE, CDCD), which is a topic we explore in detail. One closely related method proposed in the context of generative models of small molecules is DiGress , which performs gradient guidance on one-hot token embeddings to bias the categorical sampling distribution of a denoising model. In our setting we show that categorical and Gaussian discrete diffusions guided with NOS outperform PPLM and DiGress (Subsec. 5.2).

Genetic algorithmsEvolutionary algorithms are a popular solution for black-box optimization in discrete spaces [3; 22]. These methods are often evaluated on their ability to optimize _in silico_ proxy estimates of actual _in vitro_ fitness, e.g. deep learning models trained on experimental datasets. In Subsec. 5.3 we baseline NOS against two genetic optimizers from protein design literature, AdaLead  and Proximal Exploration (PEX) . We show these baselines rapidly degrade sequence likelihood as the proxy fitness is improved, limiting the effective number of edits that can be made before checking the actual fitness of the samples, ultimately limiting their sample efficiency and rate of convergence to optimal solutions. By contrast, NOS consistently improves proxy fitness while maintaining sequence likelihood.

## 3 Background

We pose protein design as the problem of finding sequences, \(w^{L}\) with alphabet \(\) and fixed length \(L\),3 which maximize a single objective \(f(w)\) (e.g., binding affinity) or multiple objectives \(f_{1}(w),,f_{k}(w)\) (e.g., expression yield, binding affinity, and aggregation tendency). Designs can be generated from random noise (_ab initio_ design) or by making a fixed number of edits \(B\{1,,L-1\}\) to a seed sequence \(s^{L}\). A protein is only useful if it can be synthesized (i.e. expressed), and the objective value of non-expressing proteins is undefined since their properties cannot be measured. Therefore we must introduce the constraint \(w^{L}\), where \(\) is the set of all expressible proteins. Since naturally occurring sequences must express in order to be observed, \(p(w)\), the likelihood of a protein with respect to an empirical distribution of natural protein sequences, is often taken as a proxy for the tendency of a protein to express. In protein design, these proxies are typically called metrics of _naturalness_. Since we are looking for sequences that by definition have not yet been identified in nature, naturalness and our other objectives are often in tension.

We can trade off naturalness and objective value by drawing samples from the unnormalized density

\[(w)=(-(w))/Z,(w)=E(w)-v(w),,\] (1)

where \(E(w)=- p(w)\) is a scalar energy function, and the value function \(v:^{L}\) expresses the utility of a sequence with respect to our objectives. When designing proteins from primary sequence, sampling efficiently from the resulting energy function can be challenging. Simple approaches, such as the MCMC sampler used by Verkuil et al.  can require hundreds of thousands of steps to converge (Appendix C.2). Guided diffusion models are an appealing alternative because they construct a fixed-length Markov chain that quickly generates low-energy samples.

Diffusion modelsDenoising diffusion models construct samples by reversing a diffusion process that maps clean data points, \(x_{0}\), to samples from a prior \((x)\) (Figure 2). The forward process (\(x_{0} x_{T}\)) is composed of conditional distributions \(p(x_{t}|x_{t-1})\) (i.e., the noise process) that admit closed forms for the conditional distributions \(p(x_{t}|x_{0})\) and \(p(x_{t-1}|x_{t},x_{0})\) (e.g., independent Gaussian corruption). The reverse process (\(x_{T} x_{0}\)) converts samples from the prior into samples from the learned data distribution \(p_{}(x_{0})\) by repeatedly predicting the denoised variable \(_{0}\) from noisy values \(x_{t}\) and using the conditional distribution \(p(x_{t-1}|x_{t},_{0})\) to derive a transition distribution, \(p_{}(x_{t-1}|x_{t})\). The specific choice of noise process has been shown to significantly affect the likelihood and quality of image samples . For categorical data there are two common approaches to constructing a diffusion generative model, depending on the nature of the noise process. We include brief descriptions below and a more detailed account in Appendix A.

Continuous noiseTo learn a distribution \(p(w)\), one strategy is to first embed \(w\) to a continuous variable \(x_{0}\) with embedding matrix \(U_{}\) and apply Gaussian noise . The prior is taken to be \((x)=(0,I)\) while the forward process is \(p(x_{t}|x_{0})=(x_{t};_{t}}x_{0},(1- _{t})I)\) for \(_{t}\). The values of \(_{t}\) are determined by a user-specified corruption schedule. For the reverse process, we learn a function, \(p_{}(|x_{t},t)\), to predict the sequence from noised points \(x_{t}\) by minimizing the following loss with respect to \(\):

\[L()=_{w_{0},t}[- p_{}(w_{0}|x_{t})],\ x_{t}  p(x_{t}|x_{0}=U_{}w_{0}).\]

Using \(p_{}(|x_{t},t)\) we can construct a distribution for the reverse process

\[p_{}(x_{t-1}|x_{t})=_{}p(x_{t-1}|x_{t},_{0}=U_{ })p(|x_{t},t),\] (2)

where \(p(x_{t-1}|x_{t},x_{0})\) is also a Gaussian distribution. At inference time, we can use the learned reverse process to convert samples from \((x)\) into samples from the learned distribution \(p_{}(x_{0})\) by repeatedly sampling \(p_{}(x_{t-1}|x_{t})\), followed by sampling \(w p_{}(|x_{0},0)\).

Categorical noiseAlternatively, Austin et al.  proposed a forward process which operates directly on \(w\), by introducing an absorbing state for each token \(w^{(i)}=\) [MASK]. The forward process \((w_{0} w_{T})\) is defined by a discrete transition matrix, describing the probability of mutating a token into a [MASK], and the corresponding prior is simply a point mass on the sequence of all [MASK] tokens. To learn the parameters of the denoiser \(p_{}(_{0}|w_{t},t)\) we maximize the likelihood of the denoising process on ground truth sequences

\[L()=_{w_{0},t}[- p_{}(w_{0}|w_{t})],\ w_{t}  p(w_{t}|w_{0})\]

Figure 2: Two approaches to diffusion generative modeling for categorical variables. (**Left**) Categorical data is embedded into continuous variables with an accompanying continuous noise process. (**Right**) Categorical noise is applied directly to sequences, and corrupted sequences are denoised using standard language modeling methods.

Then, as above, we can use the denoiser to construct the reverse process

\[p_{}(w_{t-1}|w_{t})=_{_{0}}p(w_{t-1}|w_{t},_{0})p_{ }(_{0}|w_{t},t)\] (3)

where \(p(w_{t-1}|w_{t},w_{0})\) is also a categorical distribution derived using Bayes' rule. To sample, the transition distribution is applied for \(t=[T,...,0]\).

## 4 Methods

Now we present practical methods for efficiently sampling from \((w) p(w)(v(w))\) (Eq. 1) by modifying the learned transition distribution with a learned value function \(v_{}(w)\). We then show how this sampling method can be used to perform protein design through guided infilling in sequence space. As before, we provide the most salient information below and the full details in Appendix B.

### NOS: diffusioN Optimized Sampling

We introduce a general form of gradient guidance (NOS) for discrete diffusions with categorical denoising models, i.e. diffusion models that predict logits over the ground truth tokens (e.g. [21; 4]). The key challenge in applying gradient guidance to categorical data is simply the lack of a continuous representation. Fortunately, in any denoising network, e.g. \(p_{}(|x_{t},t)\), the discrete sequence \(w_{t}\) has many corresponding continuous representations in the form of hidden states of the model \(h_{t}=g_{d}(w_{t})\) for \(d\{0,,D\}\), where \(D\) is the depth of the encoder network and \(g_{0}(w_{t})=U_{}w_{t}\). Notably, for the Gaussian diffusion models in Sec. 3, we can equivalently have \(x_{t}=g_{0}(w_{t})\), as corruption and sampling are performed on the learned token embeddings. In the case of the categorical noise diffusion \(p_{}(_{0}|w_{t})=p_{}(_{0}|h_{t})\), and thus for the purpose of guidance, we can consider a general \(p_{}(|h_{t})\) for both forms of corruption.

To sample from \(_{}(w_{t}) p_{}(w_{t})(v_{}(w_{t}))\), we construct a modified denoising model,

\[_{}(|h_{t}) p_{}(|h_{t})(v_{ }(h_{t})).\]

This formulation requires that the denoising model and the value function share hidden states up to depth \(d\), and that the value function also be trained on corrupted inputs \(w_{t}\). In Appendix D.4 we propose a simple procedure for corrupted discriminative training inspired by label smoothing . Using this modified denoising model we can construct modified transition distributions using Eq. 2 or Eq. 3. There is one key difference between these transition distributions: in the continuous case (Eq. 2), smooth steps are taken in the token embedding space, while in the discrete case (Eq. 3) the transition leads to large jumps from one token embedding to another. In either case, it is possible to sample a discrete sequence \(w\) at any point in the chain using the logits of the denoiser \(p_{}(|h_{t})\). When using Eq. 2 to derive a continuous transition distribution, we call the method **NOS-C**, and when using Eq. 3 for discrete transitions, we call the method **NOS-D**.

To sample from the modified transition distribution at each diffusion step, we use Langevin dynamics with temperature \(>0\), with the update step,

\[h^{}_{t} h^{}_{t}-_{h^{}_{t}}[ (p_{}(|h^{}_{t})||p_{}(|h_{t}))-v_{ }(h^{}_{t})]+, 14.226378pt (0,I),\] (4)

where \(\) is the step size and \(\) is the regularization strength, followed by sampling \(p_{}(w_{t-1}|h^{}_{t})\) or \(p_{}(h_{t-1}|h^{}_{t})\). While the gradient \(_{h}v_{}\) guides towards high values of the objective, the KL term ensures the resulting transition distribution still maximizes the likelihood of the original prediction.

NOS is related to the popular method plug-and-play language model (PPLM), which can be used for gradient-guidance of autoregressive language models . PPLM guides sampling by taking gradient steps similar to Eq. 4 for each autoregressive decoding step (details in Appendix B). Unlike PPLM, NOS is a form of iterative refinement, meaning that tokens across the entire sequence can be modified at each optimization step. This distinction is particularly important for protein design, because function can be determined by complex interactions between distant parts of the sequence. As we see in Sec. 5, NOS leads to better trade-offs between likelihood and objective value.

### LaMBO-2: function-guided protein design

Many unique challenges arise when applying guided diffusion to real-world protein design tasks. Our approach builds on the LaMBO-1 algorithm proposed by Stanton et al. , which explicitly accounts for the online, multi-objective nature of protein design by optimizing a multi-objective Bayesian acquisition function. LaMBO-2 replaces the guided MLM sampler with NOS, selects edit positions based on acquisition value saliency, and replaces the discriminative deep kernel Gaussian process (GP) with ensemble-based uncertainty quantification.

Architecture and value functionIn order to apply the methods discussed in Subsec. 4.1 we require a generative diffusion model \(p_{}(w)\) and a discriminator \(_{}(w)\) which share hidden layers up to depth \(d\). The discriminator is trained to predict the objective function \(f\). Like LaMBO-1 our architecture consists of many task-specific feature extraction layers that share a bidirectional encoder. Bayesian optimization is an iterative cycle of inference and data acquisition. During the data acquisition phase of any iteration \(i\) we need to find sequences with maximal _acquisition value_\(v_{i}(w)=[u(w,f,_{i})]\), where \(_{i}\) is the data already available and the expectation is taken with respect to a posterior \(p_{}(f|_{i})\) and \(u\) is some utility function. For multi-objective tasks \(u\) is the hypervolume improvement utility function , however we note that single-objective tasks are easily accommodated by a different choice of utility function . To estimate the expectation we draw samples from \(p_{}(f|)\) with an approach we call _partial deep ensembles_, where the discriminative layers of the model above the shared encoder are replicated \(k\) times and randomly initialized . We provide further details about partial deep ensembles and our learned discriminators in Appendix D.2 and D.3.

Choosing edit positionsWhen \(B L\) encoder-only architectures allow very precise control of edit positions since we will only change positions we corrupt. However, this feature introduces the need for some procedure to choose those positions, ideally where edits will most improve our objective value. We automatically select edit positions by computing the gradient of the value function with respect to \(h_{0}\) to determine which positions affect the value estimate the most (see Figure 3 for an illustration). This method is related to the use of saliency maps to explain the decisions of classifiers . We use input saliency to induce a distribution over edit positions. Specifically, given an embedded sequence \(h_{0}\) we define \(s_{i}(h_{0})\), the saliency with respect to \(v\) of position \(i\{1,,L\}\) as

\[s_{i}(h_{0}):=_{j=1}(_{h}v(h_{0}))_{ij} ^{1/},\ },[\ w_{0}^{(i)}]=(h _{0})}{_{j}s_{j}(h_{0})},\] (5)

where \(>0\) is a temperature hyperparameter and \(0< 1\). As \(+\), \([\ w_{0}^{(i)}]=1/L\) for all \(i\). For each sequence we draw \(B\) edit positions without replacement according to Eq. 5. We conserve parts of the input we cannot change (e.g. the antigen sequence) by setting the the saliency to 0 before computing the edit position distribution. Importantly, the diffusion sampling process can also preserve the original values of selected positions when appropriate. If we select a highly conserved position, then the predicted logits will be low entropy and the guidance will incur a large KL penalty for changes (Eq. 4).

## 5 Experiments

We evaluate our methods on three increasingly complex antibody design tasks. First we compare our trained diffusion models on unguided infilling tasks, showing that sequence diffusion methods

Figure 3: An example of a binding affinity saliency map produced by LaMBO-2 with NOS-D. For simplicity, only the variable heavy (VH) region of the hu4D5 antibody is shown. Positions corresponding to complementarity defining regions (CDRs) are enclosed in green boxes. Converting this saliency map to an edit position distribution will concentrate computational resources on editing CDRH3, which is often manually selected by experts. Some resources are also allocated to the framework and other CDRs since these positions may also affect binding.

consistently outperform structure-based methods when only predicted structures are available4. We then evaluate NOS by optimizing two objectives that can be simulated effectively _in silico_. Lastly, we evaluate LaMBO-2 on antibody lead optimization, with both _in silico_ and _in vitro_ experiments.

### Unguided antibody CDR infilling

We focus on immunoglobulin G (IgG) format antibodies, which are comprised of a heavy (H) chain and a light (L) chain. Each chain has three complimentarity determining regions (CDRs), which tend to have strong effects on binding affinity to a target antigen but limited effects on other structural properties of the protein. Antibody design methods traditionally focus on proposing mutations to CDRs while leaving the rest of the protein fixed, which can be viewed as an infilling task. We select 10 seeds at random from paired OAS  and infill each CDR individually as well as in combination. To evaluate the performance of each model, we measure the sequence recovery rate, which is simply the accuracy of the infilling predictions given the ground truth sequence. As baselines, we include IgLM , a GPT2 language model trained on OAS, and two structure-based methods: DiffAb , a joint sequence-structure diffusion model trained on SAbDab, and RFDiffusion , a structural diffusion model trained on the PDB  that uses inverse folding to derive sequences. Although IgLM is trained with fill-in-the-middle augmentations , it does not natively support infilling multiple non-contiguous regions, and we do so by replacing regions that are not yet sampled with [UNK] tokens. For the structure-based methods, we provide starting structures generated with IgFold .

In Figure 4, we find that diffusion models often generate infills that are on-par or better than that those returned by IgLM by default, especially when multiple regions must be filled simultaneously. We also see that DiffAb, while being capable of sequence-structure co-design out of the box, often underperforms sequence-only diffusion, most likely because our sequence-based approaches have access to a larger training dataset, while paired datasets with sequences and structures are much more limited. Lastly RFDiffusion tends to generate relatively low likelihood CDR infills. The gap between DiffAb and RFDiffusion may be explained by the relative scarcity of antibody structures in the PDB compared to SAbDab, which has an antibody in every structure. The poor performance of structural methods on CDR infilling could also be a result of poor sequence recovery from structure during inverse folding, a problem that could be amplified for relatively unstructured loop regions like CDRs.

### Optimizing antibodies for _in silico_ objectives

To test guided sampling from our model, we run experiments on two simple single-objective tasks:

* The percentage of beta sheets, measured on primary sequence 
* The solvent accessible surface area (SASA) of the protein's predicted structure 

Figure 4: We infill antibody CDRs with discrete diffusion models (ours) and compare against structure-based diffusion models (DiffAb  and and RFDiffusion ) and an autoregressive antibody language model (IgLM ). We see diffusion on sequences alone–without structural priors–reliable leads to high sequence recovery. For structure based methods, we first fold seed sequences with IgFold  and then run joint sampling of sequence and structure for the CDR. We sample 10 infills for each of the 10 antibody seed sequences selected randomly from paired OAS .

Since we want plausibly natural antibodies with high objective value we examine the Pareto front for samples optimized for each objective, with log-likelihood assigned by ProtGPT  (trained on Uniref50 ) plotted against the value of the objective. As an autoregressive guided baseline, we run PPLM, using IgM as the base generative model (details in Appendix C.3). We use DiGress  as a guided diffusion baseline. DiGress uses gradients on one-hot representations to performing guidance in the embedding layer and is thus closely related to our approach. We discuss differences between the methods and the details of our DiGress experiments in Appendix C.5. For PPLM, DiGress, and NOS, we generate samples for many different setting of the control hyperparameters (Section 4.1), which yields samples across the spectrum from aggressively optimizing the objective to conservatively maintaining high likelihood. We also include DiffAb and RFDiffusion without guidance as baselines, as examples of popular "diversification" procedures, in which new samples are generated for later ranking and filtering. In Figure 5, we see that for both continuous and discrete corruptions NOS offers better trade-offs between optimizing the objective and maintaining high likelihood, while also generating high values of the objective at the extreme.

### Antibody lead optimization: _in silico_ evaluation

Having established the performance of NOS on simpler benchmarks, we now turn to real-world antibody design with LaMBO-2. From this point forward in all experiments we jointly condition on the heavy chain, light chain, and antigen sequence, and we jointly optimize the heavy and light chains only for improved expression yield and binding affinity to the antigen. As we discussed in Subsec. 4.2, one of the critical subproblems in Bayesian optimization is the identification of high value additions to the existing dataset. In this section we show that LaMBO-2 effectively applies NOS to this subproblem in the antibody design setting by finding high acquisition value sequences while preserving naturalness (which we quantify with the metric proposed by Shanehsazzadeh et al. ). We focus on optimizing hu4D5, a therapeutic antibody targeting the HER2 antigen5

Comparison with genetic algorithmsWe first compare LaMBO-2 to two discrete optimization baselines, AdaLead and PEX. We generated 32 designs from the hu4D5 seed with each method, optimizing the same acquisition function derived from the same model. To ensure a fair comparison we limited all methods to a total of 512 model evaluations and a maximum of 2 edits per sampling iteration. We evaluated both the sample acquisition value and naturalness after each iteration. We identified an empirical naturalness threshold below which expression became unreliable and treated this threshold as a simple inequality constraint. Note that this experiment evaluates how each method balances the tradeoff between acquisition value and naturalness as sampling progresses, and does not involve evaluations of the actual black-box objective.

Figure 5: Comparing samples from NOS (ours) with alternative guided generation methods and structure-based models. NOS exhibits higher likelihood for similar or dramatically improved values of the objective. (**left**) Sequence diversification (resampling and selecting improved points) with DiffAb  or RFDiffusion . DiffAb generates sequences and structures simultaneously, while sequences for RFDiffusion are obtained using ProteinMPNN . Compared with NOS, these methods do not effectively optimize the objective and yield low-likelihood sequences. (**right**) Guided generation using PPLM , a guidance method for autoregressive language models (in this case IgALM ) and DiGress, a competing guidance method for discrete diffusion models . NOS, PPLM, and DiGress are sampled for many settings of guidance strength (e.g. \(\) and \(\) (Eq. 4)) to demonstrate the full range of trade-offs between the objective and likelihood. We provide details about hyperparameter settings in Appendix C.5 and additional density plots in Appendix C.6.

LaMBO-2 strictly dominates PEX in terms of naturalness and acquisition value at every sampling iteration, with PEX producing infeasible samples beyond 4 iterations. AdaLead improved sample value the most rapidly of all methods in this experiment, but also degrades naturalness the fastest, violating the constraint after only 2 sampling iterations. In contrast LaMBO-2 samples satisfy the naturalness constraint out to 16 sampling steps, producing the highest value feasible solutions. This result highlights the importance of accounting for distributional constraints when optimizing empirical proxies of fitness, since the quality of the proxy signal degrades rapidly outside the support of the training data. Genetic algorithms easily hack empirical models by leaving the support of natural sequences, where training data is necessarily absent, leading to poor quality solutions that nevertheless attain high acquisition value. In Appendix D.5 we show that both sequence and structure-based unguided infilling (i.e. random hit diversification) has the opposite behavior, producing samples with reasonable naturalness but low acquisition value.

Effect of salient editsTo separate and independently study the effects of guidance (NOS) and salient position selection, we present an ablation in Figure 6 for optimization with a relatively small edit budget \(B\) (\(B<=10\%\) of mutable positions). To isolate the effects of salient edits we baseline against edit positions chosen uniformly at random, and to isolate the effects of guidance we set the step size \(\) (Eq. 4) to 0. Small edit distance constraints are common in antibody engineering because the goal is typically to increase binding affinity without altering the binding location on the antigen (i.e. the engineered antibody should bind to the same epitope) . One heuristic way to constrain the design to the same epitope is to set \(B 8\), (about 2.7% of the antibody sequence length) , precisely the range we consider in Figure 6.

In the few edit regime we find that while both interventions improve sample objective value, selecting positions using saliency has a much larger effect than guidance. Although gradient guidance is a reliable and generally applicable tool for improved sampling, the scale of the edit position search dominates the scale of the search over token replacements that guidance affects. With a vocabulary of 21 tokens the number of possible token combinations (\(21^{8}\)) is dwarfed by the combinations of possible edit positions (\(C_{8}^{300}\)). Salient selection of edit positions is, therefore, key to any practical application of NOS in budget-constrained design. Interestingly, this facet of protein design differs significantly from guided sampling of images, where generation is typically limited to fixed locations [50; 14], not a fixed edit budget spread over any location that will optimize the objective. These additional degrees of freedom pose an extra challenge.

### Antibody lead optimization: _in vitro_ evaluation

As our final evaluation in Figure 7 we present results using LaMBO-2 (specifically with NOS-D) to optimize 20 seed antibodies distributed across 4 different therapeutic target antigens, including

Figure 6: (**left**) Naturalness constraints present a challenge for genetic methods, which rapidly decline in naturalness even as their objective value improves. The grey dashed line is an empirical lower bound on naturalness above which _in vitro_ expression is reliable. Although AdaLead and PEX both improve the acquisition value, they quickly leave well-supported areas of the search space (drop below the dashed line), shown by the faded section of each curve. By contrast, the naturalness of LaMBO-2 samples degrades much more slowly while consistently improving the acquisition value. (**right**) Ablating the effects of guidance and edit position selection. We start with the hu4D5 HER2 antibody and vary the edit budget \(B\{8,32\}\), optimizing for expression yield and binding affinity. For all choices of edit budget, we find that the effect size of edit position selection is much larger than that of guidance, making salient unguided edits a surprisingly strong baseline.

hu4D5/HER26. We tested 374 designs in total over three rounds, retraining the model after each round and varying a range of design choices and hyperparameters. While expression and binding performance varied from round to round across seeds and targets, by the final round we were able to generate multiple subnircomolar binders for all 4 targets with a median of 5 edits to the seed. See Appendix D.7 for individual yield and affinity measurements and experimental details.

The improvements to yield and affinity over time can be attributed both to the data added to the training corpus and methodological insights gleaned after each round. For example, the sharp drop in expression in Round 2 can mainly be attributed to framework residue deletions that arose when (the KL penalty coefficient) was set too small. In the following round we tried a range of larger \(\) values and fixed the sequence lengths and expression immediately recovered.

Figure 7 also reports binding affinity results of a related experiment from Shanehsazzadeh et al.  for context, though we emphasize that there are substantial differences between our wetlab validation and that of Shanehsazzadeh et al.  which prevent a true apples-to-apples comparison. In the latter experiment 1M designs were generated for the HER2 target and screened with a high-throughput assay. After screening 421 designs were validated with a high-fidelity surface plasmon resonance (SPR) assay. In addition to wetlab screening, their experiment also restricted edits to specific antibody CDRs. We optimized antibodies for a range of targets including HER2 and relied exclusively on _in silico_ screening before validating with SPR, while placing no explicit restrictions on the edit locations. Despite these differences, our results provide initial evidence that it is possible to generate enriched libraries of antibody designs exclusively with _in silico_ methods operating only on primary sequence. While the experimental validation provided is preliminary, we are actively pursuing more rigorous experimental testing in the form of up-scaled and repeated expression and binding experiments and specificity assessment.

## 6 Discussion

There are many exciting directions for future work. The original LaMBO algorithm was used to optimize small molecules in addition to proteins, and applying LaMBO-2 to small molecule design is a fruitful direction, as LaMBO-2's improvements are not protein-specific. While sequence alignments are a convenient solution to the length change problem in protein design, padding methods  or diffusion with a variable-length corruption process (e.g. ) will be needed for applications like small molecules which do not admit alignments. We are also eager to consider optimizing much longer sequences, such as gene perturbations , which can exceed 20K tokens in length and may necessitate the use of recent advancements such as implicit convolutions [35; 57; 59] or clever modifications of self-attention [16; 13; 48]. More general notions of guidance such as classifier-free guidance  for text or class-conditional generation [62; 12] are another intriguing direction, since some goals are difficult to express as black-box functions or constraints [49; 58].

Figure 7: We use LaMBO-2 to optimize 20 seed antibodies for 4 different target antigens over three experimental rounds, retraining the model after each round. Some design choices and hyperparameters varied from round to round, with substantial impact on the results. In the last round we tested 56 antibodies and attained a 99% expression rate and 40% binding rate on average across targets. On average 43% of the expressing designs had higher yield and 21% of binding designs had higher binding affinity than the corresponding seed. These results are very encouraging when placed in context with a related experiment designing HER2 antibody libraries . Our results provide evidence that enriched antibody libraries can be created _in silico_ without the assistance of high-throughput _in vitro_ screening.