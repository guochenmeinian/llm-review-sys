# An Image is Worth 32 Tokens

**for Reconstruction and Generation**

**Qihang Yu1*, Mark Weber1,2*, Xueqing Deng1, Xiaohui Shen1, Daniel Cremers2, Liang-Chieh Chen1**

1 ByteDance 2 Technical University Munich * equal contribution

https://yucornetto.github.io/projects/titok.html

_32 tokens can work well for..._

Recent advancements in generative models have highlighted the crucial role of image tokenization in the efficient synthesis of high-resolution images. Tokenization, which transforms images into latent representations, reduces computational demands compared to directly processing pixels and enhances the effectiveness and efficiency of the generation process. Prior methods, such as VQGAN, typically utilize 2D latent grids with fixed downsampling factors. However, these 2D tokenizations face challenges in managing the inherent redundancies present in images, where adjacent regions frequently display similarities. To overcome this issue, we introduce **T**ransformer-based 1-Dimensional **T**okenizer (TiTok), an innovative approach that tokenizes images into 1D latent sequences. TiTok provides a more compact latent representation, yielding substantially more efficient and effective representations than conventional techniques. For example, a \(256 256 3\) image can be reduced to just **32** discrete tokens, a significant reduction from the 256 or 1024 tokens obtained by prior methods. Despite its compact nature, TiTok achieves competitive performance to state-of-the-art approaches. Specifically, using the same generator framework, TiTok attains **1.97** gFID, outperforming MaskGIT baseline significantly by 4.21 at ImageNet \(256 256\) benchmark. The advantages of TiTok become even more significant when it comes to higher resolution. At ImageNet \(512 512\) benchmark, TiTok not only outperforms state-of-the-art diffusion model DiT-XL/2 (gFID 2.74 _vs._ 3.04), but also reduces the image tokens by **64\(\)**, leading to **410\(\) faster** generation process. Our best-performing variant can significantly surpass DiT-XL/2 (gFID **2.13 _vs._ 3.04) while still generating high-quality samples **74\(\) faster**.

Figure 1: We propose **TiTok**, a compact **1D** tokenizer leveraging region redundancy to represent an image with only **32** tokens for image reconstruction and generation.

Introduction

In recent years, image generation has experienced remarkable progress, driven by the significant advancements in both transformers [19; 65; 70; 10; 71; 72] and diffusion models [16; 58; 29; 52; 21]. Mirroring the trends in generative language models [51; 62], the architecture of many contemporary image generation models incorporate a standard image tokenizer and de-tokenizer. This array of models utilizes tokenized image representations--ranging from continuous  to discrete vectors [57; 64; 19]--to perform a critical function: translating raw pixels into a latent space. The latent space (_e.g._, \(32 32\)) is significantly more compact than the original image space (\(256 256 3\)). It offers a compressed yet expressive representation, and thus not only facilitates efficient training and inference of generative models but also paves the way to scale up the model size.

Although image tokenizers achieve great success in image generation workflows, they encounter a fundamental limitation tied to their intrinsic design. These tokenizers are based on an assumption that the latent space should retain a 2D structure, to maintain a direct mapping for locations between the latent tokens and image patches. For example, the top-left latent token directly corresponds to the top-left image patch. This restricts the tokenizer's ability to effectively leverage the redundancy inherent in images to cultivate a more compressed latent space.

Taking one step back, we raise the question _"is 2D structure necessary for image tokenization?"_ To answer the question, we draw inspiration from several image understanding tasks where model predictions are based solely on high-level information extracted from input images --such as in image classification , object detection [8; 81], segmentation [67; 34; 74; 75], and multi-modal large language models [1; 41; 11]. These tasks do not need de-tokenizers, since the outputs typically manifest in specific structures other than images. In other words, they often format a higher-level 1D sequence as output that can still capture all task-relevant information. Prior arts, such as object queries [8; 67] or the perceiver resampler , encode images into a 1D sequence of a predetermined number of tokens (_e.g._, 64). These tokens facilitate the generation of outputs like bounding boxes or captions. The success of these methods motivates us to investigate a more compact 1D sequence as image latent representation in the context of image reconstruction and generation. It is noteworthy that the synthesis of both high-level and low-level information is crucial for the generation of high-quality images, providing a challenge for extremely compact latent representations.

In this work, we introduce a transformer-based framework [65; 17] designed to tokenize an image to a 1D discrete sequence, which can later be decoded back to the image space via a de-tokenizer. Specifically, we present **T**ransformer-based 1-**D**imensional **T**okenizer (TiTok), consisting of a Vision Transformer (ViT) encoder, a ViT decoder, and a vector quantizer following the typical Vector-Quantized (VQ) model designs . In the tokenization phase, the image is split and flattened into a series of patches, followed by concatenation with a 1D sequence of latent tokens. After the feature encoding process of ViT encoder, these latent tokens build the latent representation of the image. Subsequent to the vector quantization step [64; 19], the ViT decoder reconstructs the input images from the masked token sequence [15; 24].

Building upon TiTok, we conduct extensive experiments to probe the dynamics of 1D image tokenization. Our investigation studies the interplay between latent space size, model size, reconstruction fidelity, and generative quality. From this exploration, several compelling insights emerge:

1. Increasing the number of latent tokens representing an image consistently improves the reconstruction performance, yet the benefit becomes marginal after 128 tokens. Intriguingly, 32 tokens are sufficient for a reasonable image reconstruction.
2. Scaling up the tokenizer model size significantly improves performance of both reconstruction and generation, especially when number of tokens is limited (_e.g._, 32 or 64), showcasing a promising pathway towards a compact image representation at latent space.
3. 1D tokenization breaks the grid constraints in prior 2D image tokenizers, which not only enables each latent token to reconstruct regions beyond a fixed image grid and leads to a more flexible tokenizer design, but also learns more high-level and semantic-rich image information, especially at a compact latent space.
4. 1D tokenization exhibits superior performance in generative training, with not only a significant speed-up for both training and inference but also a competitive FID score compared to a typical 2D tokenizer, while using much fewer tokens.

In light of these findings, we introduce the TiTok family, encompassing models of varying model sizes and latent sizes, capable of achieving highly compact tokenization with as few as **32 tokens**. We further confirm the model's efficacy in image generation through the MaskGIT  framework. TiTok is demonstrated to facilitate state-of-the-art performance in image generation, while requiring latent spaces that are \(8\) to \(64\) smaller, resulting in significant accelerations during both the training and inference phases. It also generates images with similar or higher quality but up to \(410\) faster than state-of-the-art diffusion models such as DiT  (Fig. 2).

## 2 Related Work

**Image Tokenization.** Images have been compressed since the early days of deep learning with autoencoders [27; 66]. The general design of using an encoder that compresses high-dimensional images into a low-dimensional latent representation and then using a decoder to reverse the process, has proven to be successful over the years. Variational Autoencoders (VAEs)  extend the paradigm by learning to map the input to a distribution. Instead of modeling a continuous distribution, VQVAEs [50; 56] learn a discrete representation forming a categorical distribution. VQGAN  further improves the training process by using adversarial training . The transformer design of the autoencoder is further explored in ViT-VQGAN  and Efficient-VQGAN . Orthogonal to this, RQ-VAE  and MoVQ  study the effect of using multiple vector quantization steps per latent embedding, while MAGVIT-v2  and FSQ  propose a lookup-free quantization. However, _all_ aforementioned works share the same workflow of an image always being patchwise encoded into a _2D_ grid latent representation. In this work, we explore an innovative _1D_ sequence latent representation for image reconstruction and generation.

**Tokenization for Image Understanding.** For image understanding tasks (_e.g._, image classification , object detection [8; 81; 78], segmentation [67; 74; 76], and Multi-modal Large Language Models (MLLMs) [1; 41; 77]), it is common to use a general feature encoder instead of an autoencoder to tokenize the image. Specifically, many MLLMs [41; 43; 61; 32; 22; 11] uses a CLIP  encoder to tokenize the image into highly semantic tokens, which proves effective for image captioning  and VQA . Some MLLMs also explore discrete tokens [32; 22] or "de-tokenize" the CLIP embeddings back to images through diffusion models [61; 32; 22]. However, due to the nature of CLIP models that focus on high-level information, these methods can only reconstruct an image with high-level semantic similarities (_i.e._, the layouts and details are not well-reconstructed due to CLIP features). Therefore, our method is significantly different from theirs, since the proposed TiTok aims to reconstruct both the high-level and low-level details of an image, same as typical VQ-VAE tokenizers [35; 57; 19].

**Image Generation.** Image generation methods range from sampling the VAE , using GANs  to Diffusion Models [16; 58; 29; 52; 21; 44] and autoregressive models [63; 12; 50]. Prior studies that are most related to this work build on top of a learned VQ-VAE codebook to generate images. Autoregressive transformer [19; 69; 7; 37], similar to decoder-only language models, model each patch in a step-by-step fashion, thus requiring as many steps as token number, _e.g._, 256 or 1024. Non

Figure 2: **A speed and quality comparison of TiTok and prior arts on ImageNet \(256 256\) and \(512 512\) generation benchmarks. Speed-up is compared against DiT-XL/2 . The sampling speed (de-tokenization included) is measured with an A100 GPU.**

autoregressive (or bidirectional) transformers [80; 72; 68], such as MaskGIT , generally predict more than a single token per step and thus require significantly fewer steps to predict a complete image. Apart from that, further studies looked into improved sampling strategies [39; 40; 38]. As we focus on the tokenization stage, we apply the commonly used non-autoregressive sampling scheme of MaskGIT to generate a sequence of tokens that is later decoded into an image.

## 3 Method

### Preliminary Background on VQ-VAE

The image tokenizer plays a pivotal role in facilitating the generative models by providing a compact image representation at latent space. For the scope of our discussion, we primarily focus on the Vector-Quantized (VQ) tokenizer [64; 19], given its broad applicability across various domains, including but not limited to image and video generation [19; 9; 58; 71], large-scale pretraining [12; 5; 49; 3; 18] and multi-modal models [20; 73].

A typical VQ model contains three key components: an encoder \(Enc\), a vector quantizer \(Quant\), and a decoder \(Dec\). Given an input image \(^{H W 3}\), where \(H\) and \(W\) denote the image's height and width, the image is initially processed by the encoder \(Enc\) and converted to latent embeddings \(_{2D}=Enc()\), where \(_{2D}^{ D}\), which downsamples the spatial dimensions by a factor of \(f\). Subsequently, each embedding \(z^{D}\) is mapped (via the vector quantizer \(Quant\)) to the nearest code \(c_{i}^{D}\) in a learnable codebook \(^{N D}\), comprising \(N\) codes. Formally, we have:

\[Quant(z)=c_{i},i=*{argmin}_{j\{1,2,,N\}}\|z-c _{j}\|_{2}.\] (1)

During de-tokenization, the reconstructed image \(}\) is obtained via the decoder \(Dec\) as follows:

\[}=Dec(Quant(_{2D})).\] (2)

Despite the numerous improvements over VQ-VAE  (_e.g._, loss function , model architecture , and quantization/codebook strategies [80; 37; 72]), the fundamental workflow (_e.g._, the 2D grid-based latent representations) has largely remained unchanged.

### TiTok: From 2D to 1D Tokenization

While existing VQ models have demonstrated significant achievements, a notable limitation within the standard workflow exists: the latent representation \(_{2D}\) is often envisioned as a static 2D grid. Such a configuration inherently assumes a strict one-to-one mapping between the latent grids and the original image patches. This assumption limits the VQ model's ability to fully exploit the redundancies present in images, such as similarities among adjacent patches. Additionally, this approach constrains

Figure 3: **Illustration of image reconstruction (a) and generation (b) with the TiTok framework (c).** TiTok contains an encoder \(Enc\), a quantizer \(Quant\), and a decoder \(Dec\). Image patches, along with a few (_e.g._, 32) latent tokens, are passed through the Vision Transformer (ViT) encoder. The latent tokens are then vector-quantized. The quantized tokens, along with the mask tokens [15; 24], are fed to the ViT decoder to reconstruct the image.

the flexibility in selecting the latent size, with the most prevalent configurations being \(f=4\), \(f=8\), or \(f=16\), resulting in \(4096\), \(1024\), or \(256\) tokens for an image of dimensions \(256 256 3\). Inspired by the success of 1D sequence representations in addressing a broad spectrum of computer vision problems [8; 1; 41], we propose to use a 1D sequence, without the fixed correspondence between latent representation and image patches in 2D tokenization, as an efficient and effective latent representation for image reconstruction and generation.

**Image Reconstruction with TiTok.** To initiate our exploration, we establish a novel framework named **T**ransformer-based 1-**T**ol**emiser (TiTok), leveraging Vision Transformer (ViT) 1 to tokenize images into 1D latent tokens and subsequently reconstruct the original images from these 1D latents. As depicted in Fig. 3, TiTok employs a standard ViT for both the tokenization and de-tokenization processes (_i.e._, both the encoder \(Enc\) and decoder \(Dec\) are ViTs). During tokenization, we patchify the image into patches (with a patch embedding layer) \(^{ D}\) (with patch size equal to the downsampling factor \(f\) and embedding dimension \(D\)) and concatenate them with \(K\) latent tokens \(^{K D}\). They are then fed into the ViT encoder \(Enc\). In the encoder output, we only retain the latent tokens as the image's latent representation, thereby enabling a more compact latent representation of 1D sequence \(_{1D}\) (with length \(K\)). This adjustment decouples the latent size from image's resolution and allows more flexibility in design choices. That is, we have:

\[_{1D}=Enc(),\] (3)

where \(\) denotes concatenation, and we only retain the latent tokens from the encoder output.

In the de-tokenization phase, drawing inspiration from [15; 5; 24], we incorporate a sequence of mask tokens \(^{ D}\)--obtained by replicating a single mask token \(\) times--to the quantized latent tokens \(_{1D}\). The image is then reconstructed via the ViT decoder \(Dec\) as follows:

\[}=Dec(Quant(_{1D})),\] (4)

where the latent tokens \(_{1D}\) is first vector-quantized by \(Quant\) and then concatenated with the mask tokens \(\) before feeding to the decoder \(Dec\).

Despite its simplicity, we emphasize that the concept of compact 1D image tokenization remains underexplored in existing literature. The proposed TiTok thus serves as a foundational platform for exploring the potentials of 1D tokenization and de-tokenization for natural images. It is worth noting that although one may flatten 2D grid latents into a 1D sequence, it significantly differs from the proposed 1D tokenizer, due to the fact that the implicit 2D grid mapping constraints still persist.

**Image Generation with TiTok.** Besides the image reconstruction task which the tokenizer is trained for, we also evaluate its effectiveness for image generation, following the typical pipeline [19; 9]. Specifically, we adopt MaskGIT  as our generation framework due to its simplicity and effectiveness, allowing us to train a MaskGIT model by simply replacing its VQGAN tokenizer with our TiTok. We do not make any other specific modifications to MaskGIT, but for completeness, we briefly describe its whole generation process with TiTok.

The image is pre-tokenized into 1D discrete tokens. At each training step, a random ratio of the latent tokens are replaced with mask tokens. Then, a bidirectional transformer takes the masked token sequence as input, and predicts the corresponding discrete token ID of those masked tokens. The inference process consists of multiple sampling steps, where at each step the transformer's prediction for masked tokens will be sampled based on the prediction confidence, which are then used to update the masked images. In this way, the image is "progressively generated" from a sequence full of mask tokens to an image with generated tokens, which can later be de-tokenized back into pixel spaces. The MaskGIT framework shows a significant speed-up in the generation process compared to auto-regressive models. We refer readers to  for more details.

### Two-Stage Training of TiTok with Proxy Codes

**Existing Training Strategies for VQ Models.** Although most VQ models adhere to a straightforward formulation, their training process is notably sensitive, and the model's performance is heavily influenced by the adoption of more effective training paradigms. For instance, VQGAN  achievesa significant improvement in reconstruction FID (rFID) on the ImageNet  validation set, when compared to dVAE from DALL-E . This enhancement is attributed to advancements in perceptual loss [33; 79] and adversarial loss . Moreover, MaskGIT's modern implementation of VQGAN  utilizes refined training techniques without architectural improvements to boost the performance further. Notably, most of these improvements are exclusively applied during the training phase (_i.e_., through auxiliary losses) and significantly affect the models' efficacy. Given the complexity of the loss functions, extensive tuning of hyper-parameters involved, and, most critically, the missing of a publicly available code-base for reference or reproduction [9; 69; 72], establishing an optimal experimental setup for the proposed TiTok presents a substantial challenge, especially when the target is a compact 1D tokenization which was rarely studied in literature.

**Two-Stage Training Comes to the Rescue.** Although training TiTok with the typical Taming-VQGAN  setting is feasible, we introduce a two-stage training paradigm for an improved performance. The two-stage training strategy contains "warm-up" and "decoder fine-tuning" stages. Specifically, in the first "warm-up" stage, instead of directly regressing the RGB values and employing a variety of loss functions (as in existing methods), we propose to train 1D VQ models with the discrete codes generated by an off-the-shelf MaskGIT-VQGAN model, which we refer to as _proxy codes_. This approach allows us to bypass the intricate loss functions and GAN architectures, thereby concentrating our efforts on optimizing the 1D tokenization settings. Importantly, this modification does not harm the functionality of the tokenizer and quantizer within TiTok, which can still fully function for image tokenization and de-tokenization; the main adaptation simply involves the processing of TiTok's de-tokenizer output. Specifically, this output, comprising a set of proxy codes, is subsequently fed into the same off-the-shelf VQGAN decoder to generate the final RGB outputs. It is noteworthy that the introduction of _proxy codes_ differs from a simple distillation . As verified in our experiments, TiTok yields significantly better generation performance than MaskGIT-VQGAN.

After the first training stage with proxy codes, we optionally have the second "decoder fine-tuning" stage, inspired by [10; 53], to improve the reconstruction quality. Specifically, we keep the encoder and quantizer frozen, and only train the decoder towards pixel space with the typical VQGAN training recipe . We observe that such a two-stage training strategy significantly improves the training stability and reconstructed image quality, as shown in the experiments.

## 4 Experimental Results

### Preliminary Experiments of 1D Tokenization

Building upon TiTok, we explore a range of configurations, including the model size and the number of tokens, to identify the most efficient and effective setup for a 1D image tokenizer. These preliminary experiments serve to provide a thorough evaluation, seeking a practical configuration of TiTok.

**Preliminary Experimental Setup.** Unless specified otherwise, we train all models with images of resolution \(H=256\) and \(W=256\), using the open-source MaskGIT-VQGAN  to supply proxy codes for training. The patch size for both tokenizer and de-tokenizer is established with \(f=16\), and the codebook \(\) is configured to have \(N=1024\) entries with each entry a vector with \(16\) channels. For TiTok variants, we primarily investigate three model sizes--small, base, and large (_i.e_., TiTok-S, TiTok-B, TiTok-L)--comprising \(22M\), \(86M\), and \(307M\) parameters for encoder and decoder, respectively. We also assess the impact of varying the number of latent tokens \(K\) from \(16\) to \(256\). We perform ablation experiments with an efficient setting (_e.g_., shorter training).

**Evaluation Protocol.** Evaluation is conducted across multiple metrics to thoroughly assess the models, including both reconstruction and generation FID metrics (_i.e_., rFID and gFID)  on the ImageNet dataset. We examine training/inference throughput to offer a direct comparison of generative model's efficiency relative to different latent sizes. Furthermore, given that the 1D VQ model inherently serves as a form of compact image compression, we further investigate the semantic information retained by the model through linear probing following MAE setting . For the complete details of the training and testing protocols (_e.g_., hyper-parameters, training costs), we refer the reader to the supplementary material Sec. A.

After the setup, we now summarize the preliminary experimental findings below.

**An Image Can be Represented by 32 Tokens.** The redundancy inherent in image representation is well-acknowledged, as evidenced by the practice of masking significant portions of images (_e.g_.,75% in MAE ) to expedite the training process without negatively affecting performance. This strategy has been validated across a variety of computer vision tasks that rely on high-level image features . However, the efficacy of such approaches in the context of image reconstruction and generation--where both low-level and high-level details are crucial for creating realistic reconstructed and generated outputs--remains underexplored. Consequently, in this experiment, we aim to determine the minimum number of tokens required to reconstruct and generate high-quality images. As depicted in Fig. 4a, although model performance progressively improves with an increase in the number of latent tokens, significant enhancements are predominantly observed when \(K\) ranges from \(16\) to \(128\). Beyond this point, increasing the latent space size yields only marginal gains. _Intriguingly, we find that with merely \(32\) latent tokens, TiTok-L achieves performance better than a 2D VQGAN model  using 256 tokens_. This observation suggests that as few as \(32\) tokens may suffice as an effective image latent representation, optimizing the utilization of image redundancy.

**Scaling Up Tokenizer Enables More Compact Latent Size.** Another intriguing observation from Fig. 4a is that larger tokenizers facilitate more compact representations. Specifically, TiTok-B with \(64\) latent tokens achieves performance comparable to TiTok-S with \(128\) latent tokens, while TiTok-L with \(32\) latent tokens matches the performance of TiTok-B with \(64\) latent tokens. _This pattern indicates that with each incremental increase in TiTok size (e.g., from S to B, or from B to L), it is possible to reduce the size of the latent image representation without compromising performance._ This trend underscores the potential benefits of scaling up the tokenizer to achieve even more compact image representations.

**Semantics Emerges with Compact Latent Space.** To evaluate the learned image representation, we perform linear probing experiments on the image tokenizer, as shown in Fig. 4b. Specifically, we add a batch normalization layer  followed by a linear layer on top of the frozen features from TiTok encoder, with all hyper-parameters strictly following the MAE protocol . _We find that as the size of the latent representation decreases, the tokenizer increasingly learns semantically rich representations_, as indicated by the improved linear probing accuracy. This suggests that the model learns high-level information in scenarios of constrained representation space.

**Compact Latent Representation Improves Generative Training.** In addition to reconstruction capabilities, we assess TiTok's effectiveness and efficiency in generative downstream tasks, as illustrated in Fig. 4c and Fig. 4d. We note that variants of different tokenizer sizes yield comparable outcomes when the number of latent tokens is sufficiently large (_i.e._, \(K 128\)). However, within the domain of compact latent sizes (_i.e._, \(K 64\)), larger tokenizers notably enhance performance. Furthermore, the adaptability of 1D tokenization in TiTok facilitates more efficient and effective generative model training. For instance, model variants with \(K=32\), despite inferior reconstruction quality, demonstrate significantly better generative performance, _underscoring the advantages of employing a more condensed and semantically rich latent space for generative model training_. Additionally, the reduction in latent tokens markedly accelerates training and inference, with a \(12.8\) increase in training speed (2815.2 _vs._ 219.7 samples/s/gpu) and a \(4.5\) speed up sampling speed (123.1 _vs._ 27.5 samples/s/gpu), when utilizing \(K=32\) as opposed to \(K=256\).

Figure 4: **Preliminary experimental results with different TiTok variants.** We provide a comprehensive exploration in (a) ImageNet-1K reconstruction. (b) ImageNet-1K linear probing. (c) ImageNet-1K generation. (d) Training and inference throughput of MaskGIT-ViT as generator and TiTok as tokenizer (evaluated on A100 GPUs, inference includes de-tokenization step with TiTok-B). Detailed numbers can be found in supplementary material Sec. B.

### Main Experiments

Based on the observations above, the proposed TiTok family effectively trades off a larger model size to a more compact latent size. In this section, we majorly focus on ImageNet generation benchmarks against prior arts, and evaluate TiTok as a tokenizer in the generative MaskGIT framework .

**Implementation Details.** We primarily investigate the following TiTok variants: TiTok-S-128 (_i.e_., small model with 128 tokens), TiTok-B-64 (_i.e_., base model with 64 tokens), and TiTok-L-32 (_i.e_., large model with 32 tokens), where each variant designed to halve the latent space size while scaling up the model size. For resolution \(512\), we double the latent size to ensure more details are kept at higher resolution, leading to TiTok-L-64 and TiTok-B-128. In the final setting for TiTok training, the codebook is configured to \(N=4096\), and the training duration is extended to \(1M\) iterations (200 epochs). We also adopt the "decoder fine-tuning" stage to further enhance model performance, where the encoder and quantizer are kept frozen and the decoder is fine-tuned for \(500k\) iterations. For the training of generative models, we utilize the MaskGIT  framework without any specific modifications, except for the adoption of an arccos masking schedule . All other parameters are the same as previous setups, and all design improvements will be verified in the ablation studies.

**Main Results.** We summarize the results on ImageNet-1K generation benchmark of resolution \(256 256\) and \(512 512\) in Tab. 1 and Tab. 2, respectively.2

For ImageNet \(256 256\) results in Tab. 1, TiTok can achieve a similar level of reconstruction FID (rFID) with a much smaller number of latent tokens than other VQ models. Specifically, using merely \(32\) tokens, TiTok-L-32 achieves a rFID of \(2.21\), comparable to the well trained VQGAN from MaskGIT  (rFID 2.28), while using \(8\) smaller latent representation size. Furthermore, when using the same generator framework and same sampling steps, TiTok-L-32 improves over MaskGIT by a large margin (from \(6.18\) to \(2.77\) gFID), showcasing the benefits of a more effective generator training with compact 1D tokens. When compared to other diffusion-based generative models, TiTok can also achieve a competitive performance while enjoying an over \(\) speed-up during the sampling process. Specifically, TiTok-L-32 achieves a better gFID than LDM-4  (2.77 _vs_. 3.60), while generating images dramatically faster by **254** times (101.6 samples/s _vs_. 0.4 samples/s). Our best-performing variant TiTok-S-128 outperforms state-of-the-art diffusion method DiT-XL/2  (gFID \(1.97\)_vs_. \(2.27\)), with a \(13\) speed-up.

For ImageNet \(512 512\) results in Tab. 2, the significantly better accuracy-cost trade-off of TiTok persists. TiTok maintains a reasonably good rFID compared to other methods, especially considering that TiTok uses much fewer tokens (_i.e_., higher compression ratio). For generation, all TiTok variants

 tokenizer & \#tokens & codebook size \(\) & generator & gFID\(\) & P\(\) & S\(\) & T\(\) \\   \\  Taming-VQGAN\(\) & 1024 & 16384 & 1.14 & LDM-8  & 7.76 & 258M & 200 & - \\ VAE\(\) & 4096\(\)3 & - & 0.27 & LDM-4  & 3.60 & 400M & 250 & 0.4 \\  & & & & UViT-L/2  & 3.40 & 287M & 50 & 1.1 \\ VAE \(\) & 1024\(\)4 & - & 0.62 & UViT-H/2  & 2.29 & 501M & 50 & 0.6 \\  & & & & DiT-XL/2  & 2.27 & 675M & 250 & 0.6 \\   \\  Taming-VQGAN  & 256 & 1024 & 7.94 & Taming-Transformer  & 15.78 & 1.4B & 256 & 7.5 \\ RQ-VAE  & 256 & 16384 & 3.20 & RQ-Transformer  & 8.71 & 1.4B & 16.1 \\  & & & & 7.55 & 3.8B & 64 & 9.7 \\ MaskGIT-VQGAN  & 256 & 1024 & 2.28 & MaskGIT-ViT  & 6.18 & **177M** & **8** & 50.5 \\ ViT-VQGAN  & 1024 & 8192 & 1.28 & VIM-Large  & 4.17 & 1.7B & 1024 & 0.3 \\  TiTok-L-32 & 32 & 4096 & 2.21 & MaskGIT-ViT  & 2.77 & **177M** & **8** & **101.6** \\ TiTok-B-64 & 64 & 4096 & 1.70 & MaskGIT-ViT  & 2.48 & **177M** & **8** & 89.8 \\ TiTok-S-128 & 128 & 4096 & 1.71 & MaskGIT-UViT-L  & 2.50 & **8** & 53.3 \\  & & & & **1.97** & 64 & 7.8 \\ 

Table 1: **ImageNet-1K \(256 256\) generation results evaluated with ADM . \(\): Trained on OpenImages \(\): Trained on OpenImages, LAION-Aesthetics/-Humans . P: generator’s parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision.**significantly outperform our baseline MaskGIT  by a large margin. When compared with diffusion-based models, TiTok-L-64 shows a superior performance to DiT-XL/2  (\(2.74\)_vs_. \(3.04\)), while running **410\(\) faster**. The best-performing variant TiTok-B-128 can significantly outperform DiT-XL/2 by a large margin (\(2.13\)_vs_. \(3.04\)) but also generates high-quality samples **74\(\) faster**. We also provide visualization results and analysis in supplementary material Sec. D.

### Ablation Studies

We report the ablation studies regarding our final model designs in Tab. 3. Specifically, in Tab. 3(a), we ablate the tokenizer designs on image reconstruction. We begin with our baseline TiTok-L-32 which attains 6.59 rFID. Employing a larger codebook size improves the rFID by 0.74, while further increasing the training iterations (from 100 epochs to 200 epochs) yields another 0.37 improvement of rFID. On top of that, the "decoder fine-tuning" (our stage-2 training strategy) can substantially improve the overall reconstruction performance to 2.21 rFID.

In Tab. 3(b), we examine the effects of different masking schedules for MaskGIT with TiTok. Interestingly, unlike the original MaskGIT setting  which empirically found that the cosine masking schedule significantly outperforms the other schedules, we observe that MaskGIT equipped with TiTok changes the preference to the arccos or linear schedules. Additionally, unlike  which reported that the root schedule performs much worse than the others, we observe that TiTok is quite robust to different masking schedules. We attribute the observations to TiTok's ability to provide a more compact and more semantic meaningful tokens compared to 2D VQGAN, as compared to the cosine masking schedule, linear and arccos schedules have a lower masking ratio in the early steps. This coincides with the observation that masking ratio is usually higher for redundant signals (_e.g._, 75% masking ratio in images ) while relatively lower for semantic meaningful inputs (_e.g._, 15% masking ratio in languages ).

We ablate the effects of training paradigm in Tab. 3(c). We begin with the training setting of Taming-VQGAN , where TiTok-B-64 obtains 5.15 rFID, outperforming the original 2D Taming-VQGAN's 7.94 rFID under the same training setting. We also show the necessity of 1D tokenization by building a 2D variant of TiTok-B64, where the architecture remains the same except that image patches instead of latent tokens are used as image representation. As a result, we observe that the 2D variant suffers from a much worse performance (15.58_vs_. 5.15 rFID), since the fixed correspondences in 2D tokenization limited a reasonable reconstruction under compact latent space. This result

Table 2: **ImageNet-1K \(512 512\) generation results evaluated with ADM . \(\): Trained on OpenImages, LAION-Aesthetics and LAION-Humans . P: generator’s parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision.**

Table 3: **Ablation study improved final models for main experiments. We ablate the tokenizer designs, and generator designs on ImageNet-1k benchmark. The final settings are labeled in gray.**

Table 3: **Ablation study improved final models for main experiments. We ablate the tokenizer designs, and generator designs on ImageNet-1k benchmark. The final settings are labeled in gray.**

Table 2: **ImageNet-1K \(512 512\) generation results evaluated with ADM . \(\): Trained on OpenImages, LAION-Aesthetics and LAION-Humans . P: generator’s parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision.**demonstrates the effectiveness of the proposed 1D tokenization, especially at a much more compact latent size. **Although TiTok can achieve a reasonably well performance under straightforward single-stage training, there exists a performance gap compared to the MaskGIT-VQGAN  due to the missing of a strong training recipe, of which no public reference or access exists.** Therefore, we adopt the two-stage training with _proxy codes_, which proves to be effective and can outperform the MaskGIT-VQGAN (1.70 _vs._ 2.28 rFID). It is noteworthy that the two-stage training is not that crucial to obtain a reasonable 1D tokenizer, and we believe that TiTok, with the simple single-stage Taming-VQGAN's training setting, could also benefit from training on a larger-scale dataset  as demonstrated in  and we leave it for future work due to the limited compute.

**Details on the Two-Stage Training.** We provide more technical details on the two-stage training. Specifically:

* In the first stage (warm-up stage), we use an off-the-shelf ImageNet-pretrained MaskGIT-VQ tokenizer to tokenize the input image into 256 tokens, which we refer to as proxy codes.
* In the first stage training, instead of regressing the original RGB values, we use the proxy codes as reconstruction targets. Specifically, the workflow is: RGB images are patchified and flattened into a sequence and concatenated with 32 latent tokens, then they are fed into TiTok-Enc (Encoder of TiTok). Later, the latent tokens are kept as token representation and go through the quantizer. The quantized latent tokens are concatenated with 256 mask tokens and go through the TiTok-Dec (Decoder of TiTok). And the final output mask tokens are supervised by proxy codes using cross-entropy loss.
* Afterwards, we freeze both the TiTok-Enc and quantizer, and then only fine-tune the TiTok-Dec (responsible for reconstructing proxy codes) and MaskGIT-Dec (responsible for reconstructing RGB values from proxy codes) end-to-end towards pixel space, where the training losses include L2 loss, perceptual loss, and GAN loss following the common VQGAN paradigm.

Moreover, we also note that two-stage training is not necessary for TiTok training, and it works fine with the commonly used and publicly available Taming-VQGAN recipe as is shown in Tab. 3c. In this case, the whole workflow is pretty straightforward, where the TiTok-Dec will instead directly reconstruct the images at pixel space.

However, the Taming-VQGAN recipe (developed more than 3 years ago) leads to an inferior FID score when compared to state-of-the-art tokenizers, putting TiTok at disadvantage when compared against other methods. Therefore we propose the two-stage training to benefit TiTok from the state-of-art MaskGIT-VQGAN tokenizer, which shares a similar architecture to Taming-VQGAN but has a significantly better score (rFID 2.28 v.s. 7.94).

We also note that TiTok can work well with single-stage recipe, and it is promising to incorporate the recent modern VQGAN recipe from [47; 68] in our preliminary experiments.

## 5 Conclusion

In this paper, we have explored a compact 1D tokenization TiTok for reconstructing and generating natural images. Unlike the existing 2D VQ models that consider the image latent space as a 2D grid, we provide a more compact formulation to tokenize an image into a 1D latent sequence. The proposed TiTok can represent an image with \(8\) to \(64\) times fewer tokens than the commonly used 2D tokenizers. Moreover, the compact 1D tokens not only significantly improve the generation model's training and inference throughput, but also achieve a competitive FID on the ImageNet benchmarks. We hope our research can shed some light in the direction towards more efficient image representation and generation models with 1D image tokenization.