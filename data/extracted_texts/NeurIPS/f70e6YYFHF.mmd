# The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More

Oualil Kitouni\({}^{1}\)

IAIFI, Massachusetts Institute of Technology

Niklas Nolte \({}^{2}\)

IAIFI, Massachusetts Institute of Technology

Diane Bouchacourt \({}^{2}\)

IAIFI, Massachusetts Institute of Technology

Adina Williams \({}^{2}\)

IAIFI, Massachusetts Institute of Technology

Mike Rabbat \({}^{2}\)

IAIFI, Massachusetts Institute of Technology

Mark Ibrahim \({}^{2}\)

IAIFI, Massachusetts Institute of Technology

###### Abstract

Today's best language models still struggle with _hallucinations_: factually incorrect generations, which impede their ability to reliably retrieve information seen during training. The _reversal curse_, where models cannot recall information when probed in a different order than was encountered during training, exemplifies this in information retrieval. We reframe the reversal curse as a _factorization curse_ -- a failure of models to learn the same joint distribution under different factorizations. Through a series of controlled experiments with increasing levels of realism including _WikiReversal_, a setting we introduce to closely simulate a knowledge intensive finetuning task, we find that the factorization curse is an inherent failure of the next-token prediction objective used in popular large language models. Moreover, we demonstrate reliable information retrieval cannot be solved with scale, reversed tokens, or even naive bidirectional-attention training. Consequently, various approaches to finetuning on specialized data would necessarily provide mixed results on downstream tasks, unless the model has already seen the right sequence of tokens. Across five tasks of varying levels of complexity, our results uncover a promising path forward: factorization-agnostic objectives can significantly mitigate the reversal curse and hint at improved knowledge storage and planning capabilities.

## 1 Introduction

Although today's best language models produce impressively cogent, articulate text by learning the statistics of language, they still struggle to reliably retrieve information seen during training. Models are known to suffer from hallucinations, potentially responding with fabricated content that differs from the knowledge present in training data. Hallucinations pose a significant hurdle to the adoption of language models, especially in domains where reliable knowledge retrieval is paramount (Dahl et al., 2024). A well-studied failure mode underlying hallucinations is the _reversal curse_, which ascribes this deficiency to the precise order of words presented to the model at train-time (Berglund et al., 2023; Allen-Zhu and Li, 2023). For example, a model trained on sentences where _Paris_ always appears as the subject of the sentence, such as _"Paris is the capital of France"_, can be tuned to answer _"Paris is the capital of which country?"_ but not _"What is the capital of France?"_, even though these two formulations encode the same underlying information. Existing approaches aimed at mitigating the reversal curse have focused on data augmentations that involve training on both the forward and reversed tokens (Golovneva et al., 2024). In this work, we focus on learning objectives.

In Section 2, we propose the _factorization curse_, a framework that characterizes the reversal curse as a failure to model the same joint distribution under different factorizations. We show the prevailingleft-to-right next token prediction, autoregressive (AR) objective used in popular large models such as GPT (Radford et al., 2019) and Llama models (Touvron et al., 2023a, b), underlies the reversal curse. We illustrate in Figure 1 how the factorization in AR training only encodes information based on prior context, thereby limiting how well the model can retrieve information based on _later context_. Through this lens, we show the reversal curse is not merely a failure to learn logical implications, but a more general problem related to learning objectives. Given this framework, we hypothesize in Section 2.1 that factorization agnostic models, _i.e._, models trained in a manner that is less dependent on the specific token order while preserving the overall meaning, can store knowledge better and are less prone to the reversal curse. To validate our hypothesis and explore potential solutions, we conduct extensive experiments in controlled settings in Section 3.1, focusing on the effects of pretraining objectives on knowledge storage. Section 3.2 introduces _WikiReversal_, a realistic testbed based on Wikipedia knowledge graphs that closely replicates a knowledge-intensive finetuning application. In experiments with increasing levels of complexity and realism, we observe that scale, naive bidirectional objectives, and even left-to-right training do not resolve the reversal curse. These results suggest that finetuning strategies for downstream applications might not allow models to store knowledge adequately. Finally, in Section 4, we find that factorization-agnostic training is not only a promising initial solution for knowledge storage but also hints at improved planning capabilities in a minimal graph traversal task.

To summarize, our contributions are as follows:

1. We introduce the concept of the _factorization curse_, which posits that different objectives' decomposition of an input into context and prediction is a key factor underlying the reversal curse.
2. We conduct empirical studies with increasing levels of complexity and realism to validate our framework, comparing strategies such as standard autoregressive training (AR), AR with reversed sequences, and masked language modeling (MLM) as a prototypical bidirectional objective.
3. Building on our factorization curse framework, we identify factorization-agnostic objectives that allow for making predictions using every possible context decomposition, as a strong baseline solution. We explore its effectiveness across all investigated settings, including the WikiReversal setting.
4. We show that factorization-agnostic strategies are promising not only for knowledge storage/retrieval but also for planning, suggesting potentially broader implications for our findings.

## 2 The Factorization Curse

The reversal curse highlights how language models struggle to reliably retrieve information seen during training given some context. Our aim is to understand this failure by probing how common training objectives factorize an input into context and prediction. We show how common training objectives, including popular left-to-right AR and masked modeling objectives, struggle to learn factorizations that can help the model generalize better on a given task, a challenge we label the _factorization curse_.

Figure 1: **(Left) Reversal curse from training a model on sentences with _Paris_ before _France_. **(Right)** Left-to-right objective does not learn how to predict early tokens from later ones even if the information content is the same. The model overfits to a specific factorization of the joint distribution over tokens, and is unable to answer questions that require reasoning about a different factorization.

### Hypothesis: Reversal Curse as an Instance of Factorization Curse

Let us define the _factorization curse_ more formally. We first start with the usual left-to-right autoregressive model for a sequence \(\) with \(D\) tokens. This is the standard formulation in popular GPT-style (Radford et al., 2019; OpenAI, 2023) models and its loglikelihood is given by

\[ p()=_{t=1}^{D} p(x_{t}|_{<t}).\] (1)

Each token is represented as \(x_{t}\), where \(t\) is its index in the sequence. \(_{<t}\) represents all tokens that precede the \(t\)-th token in the sequence. The log probability of the entire sequence \(\) is computed as the sum of the log probabilities of each token \(x_{t}\), given all the preceding tokens \(_{<t}\). This is the _left-to-right factorization_ of the joint distribution over tokens. Note that there are many factorizations (\(D!\)) of the joint distribution, each given by some permutation \(\) of the tokens in the sequence, which we can write as \( p()=_{t=1}^{D} p(x_{(t)}|_{(<t)})\).

Example in Two TokensFor illustration purposes, let us walk through an example with \(D=2\). Suppose our goal is to model \(p()=p(x_{2},x_{1})=p(x_{2}|x_{1})p(x_{1})\). The left-to-right factorization loss optimizes

\[-_{AR}= p()= p(x_{2}|x_{1})+ p(x_{1}).\] (2)

Interestingly, we can readily see the reversal curse failure mode in \(_{AR}\). A model \(p_{}\) that attributes high likelihood to \(p_{}(x_{2}|x_{1})p_{}(x_{1})\) does not necessarily yield a high value for \(p_{}(x_{1}|x_{2})p_{}(x_{2})\) (a right-to-left factorization) even though the two expressions should be equal due to the chain rule of probability. Note that here we make no statement about the sequential order of the random variables or their relationship. The only statement we make is that, unsurprisingly, \(p_{}\) is not necessarily capable of modeling the joint distribution when presented with a different factorization. This is the _factorization curse_.

**Definition 1**: _(Factorization Curse). A model \(p_{}\) for the joint distribution of a sequence \(\) suffers from the factorization curse if, for a factorization order \(\) different from the "training" factorization order \(_{0}\) (which depends on the objective and model details), we have_

\[_{t}p_{}(x_{(t)}|x_{(<t)})_{t}p_{}(x_{ _{0}(t)}|x_{_{0}(<t)}).\] (3)

_In particular, the model may be optimal on \(_{0}\), but perform poorly on a different factorization._

ImplicationsThis has a number of implications. First, by definition, a highly factorization-dependent LLM will struggle to retrieve knowledge from earlier in the context given later information. Second, simply training on additional sequences with all tokens reversed would likely not alleviate the issue. Indeed, if the information we seek to retrieve is composed of multiple tokens, the factorization the LLM needs to handle is not right-to-left, but instead reversed in chunks. Thus, in order for any reverse training strategy to work, one must first parse the entities of interest then train with sequences reversed in entity-preserving chunks (see Section 3 and Figure 6).

Furthermore this explains why standard MLM approaches with fixed masking rates fail to address the issue, despite their bidirectionality, for two reasons: First, entities may span a larger number of tokens than the model masks, meaning there is never supervision signal to make the prediction from the right context (without leaking parts of the entity). Second, training with a fixed rate does not yield a good generative model. While the model is used to predicting, _e.g._, 15% of a full context-window during training, at inference, the model can fail to generalize (Tay et al., 2022) to the arbitrary-length sequences it encounters (see Figure 2). Zhang et al. (2024) suggest that encountering different length sequences during training encourages disentanglement and compositionality, which will be crucial for a good generative model.

Knowledge retrieval beyond reversal:A model that cannot learn how to retrieve information in reverse order will likely suffer from further downstream issues that are often ascribed to hallucination. For instance, let us take a model pretrained on entities in a database, say a list of soccer players with various attributes (statistics, game histories, _etc._) with the name appearing before the attributes as follows \(_{},_{}\). The model may memorize the database perfectly, but when queriedfor players that match specific criteria (_e.g._, played in a particular position, or have a specific nationality, _etc._), the model can produce hallucinated answers that do not match the training distribution due to lack of direct supervision of the form \(p(_{}|_{})\) during pretraining.

### Factorization-Agnostic Training Strategies

To store and retrieve knowledge in "all directions" for arbitrary-length entities and without external intervention (entity pre-parsing, retrieval augmented generation, _etc._), the model needs to be equally good at any factorization of the joint distribution. Below, we discuss two ways this can be achieved.

Permutation Language Modeling (PLM)A straightforward way to alleviate the factorization issue, is to write the autoregressive loss in a way that is independent of factorization by averaging over all permutations as follows

\[ p()=_{(S_{D})}[_{t=1} ^{D}p(x_{(t)}|_{(<t)})]_{ (S_{D})}[_{t=1}^{D} p(x_{(t)}|_{(<t)} )],\] (4)

where \(\) is a permutation sampled uniformly at random from \(S_{D}\), the permutation group on \(D\) tokens. The term \(_{(<t)}\) represents all tokens that precede the \(t\)-th token in the permuted sequence. The log probability of the entire sequence \(\) is then lower-bounded (using Jensen's inequality) by the expected sum of the log probabilities of each element \(x_{(t)}\), given all its preceding tokens in the permuted sequence. Note that all we did here is average over all factorizations. This formulation is used in XLNet (Yang et al., 2020). However, for practical reasons they end up training with a permutation on the last few tokens only. This partial prediction, as we argued above, can limit knowledge storage improvements because we do not know how to chunk tokens into entities a priori.

Uniform-Rate Masked Language Modeling (MLM-\(\))An alternative factorization-agnostic objective is to predict any context from any other context uniformly at random. This includes next-token, previous-token, predictions spanning multiple future or past tokens, and all other forms of contextual prediction. As it turns out, this generalization over objectives (amounting to something similar to masked language modeling with a randomly sampled masking rate \(r(0,1)\)) is a discrete diffusion model with an absorbing masking state (Austin et al., 2023; Kitouni et al., 2024). This diffusion formulation can be used to make a factorization-order-independent autoregressive model. See Figure 2 for an illustration of the differences between the MLM-\(\) objective and more standard MLM. Specifically, \(_{CT}\) from Kitouni et al. (2024)'s Proposition 1, which we will refer to as \(_{}}\) here, can be retrieved from Equation (4) as follows

\[_{}} =-_{(S_{D})}_{t=1}^{D} p( x_{(t)}|_{(<t)})\] \[=-_{(S_{D})}_{t (1,,D)}D p(x_{(t)}|_{(<t)})\] \[=-_{(S_{D})}_{t (1,,D)}_{( t)} p(x_{ }|_{(<t)})\] (5)

where the last equality is possible because all \(( t)\) tokens are equally likely to appear at position \(t\) as we average across all permutations, and so we can average over the predictions for each \(\) at this position. This approach can be implemented as a denoising process which recovers randomly masked tokens, like BERT (Devlin et al., 2019), but with uniformly sampled masking rates. This key difference allows training a generative model with masked modeling.2

## 3 Experiments

We now investigate information retrieval capabilities across learning objectives through the lens of different factorizations of the joint sequence probability. Specifically, we compare * **AR:** The standard autoregressive causal next-token prediction. Though all models generate tokens autoregressively, we use AR as a shorthand for left-to-right models, in line with Equation (1).
* **AR w/reverse:** AR prediction on sequences and their token-level reverse.3 * **MLM \(r\):** BERT-like masked language modeling with fixed random masking rate, \(r\).
* **MLM-\(\):** MLM with \(r(0,1)\). PLM results are similar, and are reported in the Appendix.

To ensure a fair comparison and allow each objective to perform optimally, we employ model architectures specifically designed for each objective. For autoregressive (AR) training, we use GPT-2 (Radford et al., 2019) and Mistral (Jiang et al., 2023). For masked language modeling (MLM), we use BERT (Devlin et al., 2019). Finally, for MLM-\(\), we employ an encoder-decoder model4 based on the GPT architecture (see Appendix G for details).

We study these models across increasing levels of complexity and realism, beginning with a controlled retrieval task using synthetic tokens to a retrieval task using natural text from Wikipedia articles. In our evaluation, we find that the degree to which the learning objective factorizes the input reliably explains performance across this wide range of information retrieval tasks. Factorization-agnostic methods show improved knowledge retrieval capabilities.

### Controlled Experiments in Factorization-Agnostic Training

Retrieval Task.We are particularly interested in models' ability to recall knowledge from data they were trained on. We will use a simple toy task, adapted from Golovneva et al. (2024), to evaluate this capability. First, we generate a collection of key-value pairs which are each composed of a sequence \(\{t_{i}\}^{i S}\) of tokens, _e.g._, consider the key-value pair

\[t_{0}t_{1}:t_{2}t_{3}.\]

Each key/value is unique and is composed of a unique set of tokens to control for any effects due to token statistics. Additionally, we generate two types of queries: (forward) "[value of] _key_ : _value_" and (backward) "[key of] _value_ : _key_". Models are trained on all key-value pairs and a subset of queries, and tested on unseen queries by completing tokens after the colon. Accuracy, measured using exact match, in Table 1 shows AR training does not retrieve keys from values and that reversing tokens does not improve backward retrieval. We observe entity-based reversing trivially solves this task. Additionally, while MLM does not suffer from a forward/backward disparity, its fixed masking rate causes poor overall results. Introducing a uniformly sampled rate via MLM-\(\) solves the task perfectly.

Non-reciprocal Relationships. Are models employing incorrect reversal heuristics?A weakness of the retrieval task is that it could be solved by assuming all relations between keys and values to be symmetric/reciprocal. In language, this is not always the case: even though they contain many of the same words, the sentence _Alice likes Bob_ does not necessarily imply that _Bob likes Alice_. To investigate whether models inappropriately rely on a reversal heuristic, we extend the retrieval task

Figure 2: MLM struggles when entities span more tokens than the masked span. MLM-\(\) encounters all possible masking fractions during training and does not suffer from this problem.

to a third entity for each sample, yielding statements of the form _"\(A B C\)"_. Question answering (QA) samples are of the form (forward) _"\(B\)?"_ and (backward) _"\(B\)?"_ where the right answers are \(C\) and \(A\), respectively. With a third entity in play, a model assuming symmetry would be unable to decide between \(A\) and \(C\) as the answer for either question.

The bottom of Table 1 shows that simply reversing entities (denoted with AR w/reverse (entity)*) leads to undesirable behaviour as can be seen from the large incorrect inference rate. However, adding simple delimiter tokens around entity reversed sequences (without asterisk) leads to more a robust model. Finally, MLM-\(\) learns the asymmetric relations correctly.

BioS.Next, we investigate performance of the different objectives for more complex but still controlled data. BioS (Zhu and Li, 2023) is a synthetic dataset consisting of biographies for 10k fictional individuals. For each individual, biographical details (birth date, birth city, _etc._) were randomly selected from a uniform distribution. The authors ensured each individual was assigned a unique full name. We reproduce some of their results on the birth_date-to-full_name task which aims to recover a person's full name from their birth date. Results are shown in Table 2. Again, the autoregressive, MLM and reversed-token training struggle to recover backward queries.

Training in a factorization-agnostic fashion leads to non-negligible backward performance. Interestingly, backward performance keeps improving over a long time (many times the number of epochs required for forward performance to reach 100%) (see Appendix F). If this delay is due to the low frequency of observing the right factorization in training, this could indicate that methods to automatically select data such as RHO-LOSS (Mindermann et al., 2022) could have a disproportionate impact in improving factorization-agnostic methods compared to standard AR training.

### Wikipedia Knowledge Graph Reversal

To bridge the gap between the controlled studies on synthetic datasets and more realistic evaluations, we introduce a new evaluation setup that combines the best of both approaches. Our setup involves finetuning a language model on real-world natural text from Wikipedia articles, along with a precise knowledge graph describing the relations and entities within them. This allows for principled experiments that mimic real-world use-cases where practitioners finetune pretrained models on domain-specific corpora. We compare finetuning a language model in this standard setup to training from scratch with MLM-\(\).

Experiment DesignWe introduce a new closed-book QA dataset to evaluate the ability of models to reason about entities and relations in both forward and backward directions. The dataset is derived

  
**Retrieval Task** & AR & AR w/reverse & MLM & MLM-\(\) \\   Forward \(\) & **100** & **100** & 21 & **100** \\ Backward \(\) & 0 & 0 & 22 & **100** \\ 
**Relationship Task** & AR w/reverse (entity)* & AR w/reverse (entity) & MLM & MLM-\(\) \\   Forward \(\) & 54 & **100** & 24 & **100** \\ Backward \(\) & 53 & **100** & 19 & **100** \\ Incorrect Inference \(\) & **44** & 0 & 0 & 0 \\   

Table 1: Exact match accuracy of different training paradigms on **(Top)** the retrieval task and **(Bottom)** relationship task. Due to the non-reciprocal nature of the relationship, a model that swaps the subject and object will make errors (e.g., inferring \(B\) is \(A\)’s child from \(A\) being \(B\)’s child). Shown in the bottom row. Entity reversal without a delimiter is marked with a*. Maximum values are bold.

    & AR & AR w/reverse & MLM & MLM-\(\) \\   Forward & **100** & **100** & 8 & **100** \\ Backward & 0 & 0 & 0 & **68** \\   

Table 2: BioS exact match accuracy for property retrieval in the backward direction (birth date to full name) and in the forward direction (full name to birthdate).

from the GenWiki corpus based on DBpedia (Jin et al., 2020), which contains 1.3 million text passages from Wikipedia, along with entity and relation annotations.

Extracting Relation Triples and Generating QuestionsFor each passage \(P\) with annotated entities \(E=e_{1},e_{2},...,e_{n}\), we consider only "forward" relation triples \((e_{i},r,e_{j})\), where \(e_{i}\) appears before \(e_{j}\) in the passage. For the example, in the passage _"Paris is the [...] capital of France [...]"_ (Figure 3), the triplet (_Paris_, _capitalOf_, _France_) is a "forward" triplet. Had the triplet (_France_, _hasCapital_, _Paris_) been present in the graph, we would consider it a "backward" triplet. We filter the data to contain only triplets (and corresponding passages) for which the relation \(r\) exists at least in \(500\) different instances. We generate forward questions \(F_{r}(e_{i})\) querying the tail of the relation (\(e_{j}\)) and backward questions \(B_{r}(e_{j})\) querying the head (\(e_{i}\)) using predefined templates. We filter out ambiguous samples to ensure each question has a single unique answer. Algorithm 1 in the Appendix summarizes the dataset creation process.

Table 3 reports the forward/backward performance disparity, particularly for autoregressive models. Mistral 7B, achieves a backward accuracy of around 5%, much lower than its forward accuracy. Interestingly, the model starts around the same backward accuracy in the beginnig of finetuning. This indicates there may still be backwards triplets present in a "forward fashion" within the model's training text. This could also explain the non-trivial backward performance of the AR model, despite its susceptibility to the reversal curse. MLM-\(\) attains the highest backward accuracy among the evaluated models, demonstrating its robustness to the reversal curse. This suggests training from scratch with MLM-\(\) can outperform a much larger (70 \(\)) pretrained language model finetuned on the same data. However, it still falls short of the AR model's forward performance, possibly due to the inherent difficulty of the task. Notably, significantly better results can be obtained by allowing models to leverage knowledge stored from the QAs themselves (see Appendix E.3 for details).

### Analyzing Representations Learned via Factorization-Agnostic Training

We further examine factorization-agnostic training by first comparing the role of random masking in MLM-\(\) versus standard masked language modeling. We also visualize the learned representations from MLM-\(\) showing they contain more distinct entity structure compared to standard AR training.

Understanding the role of random maskingTo understand the importance of varying the masking ratio as introduced in MLM-\(\) we compare MLM-\(\) to MLM with various masking ratios (15%, 40%, 85%) based on prior work (Wettig et al., 2023)). We find MLM exhibits much noisier performance that's consistently lower than MLM-\(\) with uniformly random masking ratio as shown in Figure 3(a). This suggests fixed masking ratios, whether with high or low values, are limited in what they can learn in contrast to MLM-\(\).

Figure 3: An example passage with a forward relation triple. The forward question queries the tail, backward queries the head. _WikiReversal_ is a collection of passages and forward/backward QAs.

    & Mistral 7B & MLM & MLM-\(\) & AR \\   Forward & \(\) & 3.4 & 11 & 14 \\ Backward & 5.2 & 2.7 & \(\) & 4.3 \\   

Table 3: Wikireversal task exact match QA accuracies. MLM-\(\), MLM and AR are are 100M parameter models trained from scratch.

Visualizing learned representations in the 3-entity relationship taskTo better probe the representations learned via MLM-\(\) we plot in Figures 3(b) and 3(c) the PCA projections after training on the relationship task from Section 3.1 for AR and MLM-\(\). Compared to AR, which learns disconnected components without apparent symmetry for entities never seen backwards during training, MLM-\(\) seems to have learned a form of translation symmetry across train and test samples. This suggests MLM-\(\) training leads to more structured entities in the model's representation space.

We also measure the computational efficiency relative to convergence of MLM-\(\) versus autoregressive (next token) training showing favorable efficiency for MLM-\(\) in Appendix Figure 9.

## 4 On the Importance of Future Predictions for Planning

Prior work argues next-token prediction auto-regressive loss is not conducive to planning (Dziri et al., 2023; LeCun, 2023; Gloeckle et al., 2024). Specifically, Bachmann and Nagarajan (2024) introduces a simple path finding task that requires basic planning: From a start node, multiple linear paths \(p_{1},p_{2},,p_{n}\) extend outward. They are given as symbolic sequences of this form: \(_{}/_{}=_{}\) A model is tasked to predict the sequence of nodes along path \(p_{i}\) that leads to a specified final node at the end of \(p_{i}\). They show that when trained with a standard autoregressive (AR) next-token prediction objective, the model is unable to effectively learn this task. This failure is attributed, at least in part, to the teacher-forcing supervision used during training. As illustrated in Figure 5, from the second node \(x_{2}=2\) onward along a path \(p_{i}=(x_{1},x_{2},,x_{m})\), the model can predict each "easy" token \(x_{t}\) for \(t>2\) by simply conditioning on the immediately previous teacher-forced token \(x_{t-1}\), without requiring retention of the earlier path history or look-ahead planning, a pitfall referred to as the "Clever Hans" cheat (see Section 4.5 (Bachmann and Nagarajan, 2024) and (Pfungst and Rosenthal, 1911)).

Figure 4: In panel (a) we compare MLM with varying masking ratios to MLM-\(\). In panels (b) and (c) we visualize the two main principal components of representations learned via AR versus MLM-\(\).

Figure 5: Star Graph Task: Illustration and Performance Comparison. The illustration shows the “Clever Hans” failure mode with teacher-forced AR ((Bachmann and Nagarajan, 2024) adapted).

Bachmann and Nagarajan (2024) found that predicting multiple future tokens in a teacher-less setting helped mitigate the issue of discovering the algorithm to correctly predict the initial "difficult" token \(x_{2}\). We identify this as an intermediate objective between standard next-token prediction and the factorization-agnostic objective studied in this work, which encourages planning capabilities via both far look-ahead and look-backward along the sequence. Figure 4(a) shows that the MLM-\(\) objective enables the model to reliably solve the path-planning task by better capturing the planning requirements.

## 5 Related Work

The reversal curse was first introduced in Berglund et al. (2023). Using text-overlap based heuristics for modeling inferences between sequence of text dates back nearly two decades in NIP (Gickman et al., 2005; Adams et al., 2007). As our modeling approaches have improved, increasing work has drawn attention to models overapplying text-overlap heuristics (Dasgupta et al., 2018; Naik et al., 2018; Sanchez et al., 2018; McCoy et al., 2019; Rajeee et al., 2022; Williams et al., 2022, i.a.). Perhaps most relevant is Sinha et al. (2019)'s evaluation, which used synthetic entity-based kinship data with multiple entities based on graph structures to expose model failures and is similar to our relationship task. Most recently, work aimed at mitigating the reversal curse by Allen-Zhu and Li (2023); Golovneva et al. (2024) suggest using data augmentations by reversing both token sequences, or if available, entity orders by training both on the forward and augmented text. Related projects have also trained and/or finetuned RoBERTa (Liu et al., 2019) or BERT (Devlin et al., 2019)-based models on input sequences with randomly shuffled word order (Gauthier and Levy, 2019; Chiang and Lee, 2020; Sinha et al., 2021). Lv et al. (2023) explore a fine-tuning objective with bidirectional attention and show that it can mitigate the reversal curse in the original synthetic setting from Berglund et al. (2023). However, they employ fixed masking rates. In addition to the standard objectives we explored, much recent work has gone into a variety of pre-training objectives including span-based and hybrid objectives (Joshi et al., 2020; Tay et al., 2022; Chowdhery et al., 2022). XLNet (Yang et al., 2020) utilizes a permutation language modeling objective, considering permutations of the input sequence during training. However, XLNet is not completely factorization-agnostic as it only predicts the last few tokens in each permutation.

Various benchmarks have been introduced to evaluate the reasoning capabilities of language models. Bachmann and Nagarajan (2024) present a study on the limitations of next-token prediction in capturing reasoning abilities, arguing that the standard autoregressive training objective hinders models' ability to plan. In a similar vein, Dziri et al. (2023) investigate the limits of transformer LLMs across three compositional tasks: multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. Their findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. They also provide theoretical arguments on abstract multi-step reasoning problems, highlighting how autoregressive generations' performance can rapidly decay with increased task complexity.

## 6 Discussion and Future Work

Limitations and Potential Extensions.MLM-\(\) has a much more challenging objective since we approximate all possible partitions of the input into context and predictions. Learning curves show delayed generalization, especially on backward samples. The main limitation of factorization-agnostic approaches is the optimization difficulty due to task complexity. Predicting one token ahead is far easier than predicting the last word of a novel with limited context, due to increasing entropy along longer horizons. This requires better schedules/curricula that smoothly interpolate the difficulty increase from next-token prediction to the highest-complexity factorization the model can handle.

This work highlights how alternative objectives can address some of the issues with current state-of-the-art language models, which rely on left-to-right autoregressive generative decoder pretraining. Despite impressive capabilities with increasing scales, there are concerns about reaching a plateau due to fundamental limitations, computational constraints, or data scarcity. We find that factorization-agnostic training can learn "more" from the same data in the context of reversal curse.

This presents a case for studying factorization-agnostic objectives and investing in approaches to scale them.