# Data Debugging is NP-hard for Classifiers Trained with SGD

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Data debugging is to find a subset of the training data such that the model obtained by retraining on the subset has a better accuracy. A bunch of heuristic approaches are proposed, however, none of them are guaranteed to solve this problem effectively. This leaves an open issue whether there exists an efficient algorithm to find the subset such that the model obtained by retraining on it has a better accuracy. To answer this open question and provide theoretical basis for further study on developing better algorithms for data debugging, we investigate the computational complexity of the problem named Debuggable. Given a machine learning model \(\) obtained by training on dataset \(D\) and a test instance \((_{},y_{})\) where \((_{}) y_{}\), Debuggable is to determine whether there exists a subset \(D^{}\) of \(D\) such that the model \(^{}\) obtained by retraining on \(D^{}\) satisfies \(^{}(_{})=y_{}\). To cover a wide range of commonly used models, we take SGD-trained linear classifier as the model and derive the following main results. (1) If the loss function and the dimension of the model are not fixed, Debuggable is NP-complete regardless of the training order in which all the training samples are processed during SGD. (2) For hinge-like loss functions, a comprehensive analysis on the computational complexity of Debuggable is provided; (3) If the loss function is a linear function, Debuggable can be solved in linear time, that is, data debugging can be solved easily in this case. These results not only highlight the limitations of current approaches but also offer new insights into data debugging.

## 1 Introduction

Given a machine learning model, data debugging is to find a subset of the training data such that the model will have a better accuracy if retrained on that subset . Data debugging serves as a popular method of both data cleaning and machine learning interpretation. In the context of data cleaning, data debugging (_a.k.a._ training data debugging  or data cleansing ) can be used to improve the quality of the training data by removing the flaws leading to mispredictions [3; 4; 5]. When it comes to ML interpretation, data debugging locates the part of the training data responsible for unexpected predictions of an ML model. Therefore it is also studied as a training data-based (_a.k.a._ instance-based ) interpretation, which is crucial for helping system developers and ML practitioners to debug ML system by reporting the harmful part of training data .

To solve the data debugging problem, existing researches adopt a two-phase score-based heuristic approach . In the first phase, a score representing the estimated impact on the model accuracy is assigned to each training sample in the training data. It is hoped that the harmful part of training data gets a lower score than the other part. In the second phase, training samples with lower scores are removed greedily and the model is retrained on the modified training data. The two phases are carried out iteratively until a well-trained model is obtained. Most of the related works focus ondeveloping algorithms to estimate the scores efficiently in the first phase [8; 9; 10; 11; 12; 13; 14; 15; 16], but rarely study the effectiveness of the entire two-phase approach.

Since it is computationally intractable to estimate the score for all possible subsets of the training data, it is often assumed that the score representing the impact of a subset is approximately equal to the sum of the scores of each individual training samples from the subset. However, Koh et. al.  showed this is not always the case. For a bunch of subsets sampled from the training data, they empirically studied the difference between the estimated impact and the actual impact of each subset by taking influence functions as the scoring method. The estimated impact is calculated by summing up the score by influence function of each training samples in the subset, and the actual impact is measured by the improvement of accuracy of the model retrained after removing the subset from training data. They found that the estimated impact tends to underestimate the actual impact. Removing a large number of training samples could result in a large deviation between estimated and actual impacts. Although an upper bound of the deviation under certain assumptions has been derived, it is still unknown whether the deviation can be reduced or eliminated efficiently.

The above deviation also poses challenges to the effectiveness of the entire approach. Suppose the influence function is adopted as the scoring method, the accuracy of the model is not guaranteed to improve due to the deviation reported in  if a large group of training samples are removed during each iteration. Moreover, there is no theoretical analysis for the effectiveness of the greedy approach in the second phase. Even if only one training sample is removed during each iteration of the two-phase approach, the accuracy of the model is still not guaranteed to be improved. The effectiveness of the entire two-phase approach is therefore not assured. This leaves the following open problem:

**Problem 1.1**.: Is there an efficient algorithm to find the subset of the training data, such that the model obtained by retraining on it has a better accuracy?

The computational complexity results presented in this paper demonstrate that it is unlikely to solve the data debugging problem efficiently in polynomial time. To figure out its hardness, we study the problem Debuggable which is the decision version of data debugging when the test set consists of only one instance. Formally, Debuggable is defined as follows:

**Problem 1.2** (Debuggable).: Given a classifier \(\), its training data \(T\), a test instance \((,y)\). Is there a \(T^{} T\), such that \(\) predicts \(y\) on \(\) if retrained on \(T^{}\)?

Basically, we prove that Debuggable is NP-complete, which means data debugging is unlikely to be solved in polynomial time. This result answers the open question mentioned above directly, this is, the large deviation of estimated impacts  cannot be reduced or eliminated efficiently. This is because if the impact of a subset of the training data could be accurately estimated as the sum of the impact of each training sample in the subset, data debugging can be solved in polynomial time, which is impossible unless P=NP.

Although Debuggable is generally intractable, we still hope to develop efficient algorithms tailored to specific cases. Thus it is necessary to figure out the root cause of the hardness for Debuggable. Previous research are always conducted based on the belief that the complexity of data debugging is due to the chosen model architecture is complicated. However, we show that at least for models trained by stochastic gradient descent (SGD), the hardness stems from the hyper-parameter configuration selected for the SGD training, which was not yet aware of by previous work. To cover a wide range of commonly used machine learning models, we take linear classifiers as the model and show that even for linear classifiers, Debuggable is NP-hard as long as they are trained by SGD. Moreover, we provided a comprehensive analysis on hyper-parameter configurations that affect the computational complexity of Debuggable, including the loss function, the model dimension and the training order. Training order, _a.k.a._ training data order  or order of training samples , refers to the order in which each training sample is considered during the SGD. Detailed complexity results are shown in Table 1.

Our contribution can be concluded as follows:

* We studied the computational complexity of data debugging and showed that data debugging is NP-hard for linear classifiers in the general setting for _all possible training orders_.
* We studied the complexity of Debuggable when the loss is fixed as the hinge-like function. For 2 or higher dimension, Debuggable is NP-complete when the training order is adversarially chosen; For one-dimensional cases, Debuggable can be NP-hard when the interception \(<0\), and is solvable in linear time when \( 0\).
* We proved that Debuggable is solvable in linear time when the loss function is linear.

Moreover, we have a discussion on the implications of these complexity results for machine learning interpretability and data quality, as well as limitations of score-based greedy methods. Our results suggest the further study as follows. (1) It is better to characterize the training sample and find the criterion which can be used to decide the existence of efficient algorithms; (2) Designing algorithms with CSP-solver is a potential way to solve data debugging more efficiently than the brute-force one; (3) Developing random algorithms is a potential way to solve data debugging successfully with high probability.

### Related Works

The solution of data debugging has applications in database query results reliability enhancement [2; 19], training data cleaning  and machine learning interpretation[9; 8; 10; 20; 21]. Existing works on data debugging mainly adopt a two-phase approach, which scores the training samples in the first phase and greedily deletes training samples with lower scores in the second phase. Most of the research focus on the first phase. There are mainly two ways of scoring adopted for data debugging in practice. Leave-one-out (LOO) retraining is a widely studied way, which evaluates the contribution of a training sample through the difference in the model's accuracy trained without that training sample. To avoid the cost of model retraining, Koh and Liang took influence functions as an approximation of LOO . After that, various extensions and improvements of the influence function based method are proposed, such as Fisher kernel , influence function for group impacts , second-order approximations  and scalable influence functions . Another way is Shapley-based scoring, where the impact of a training sample is measured by its average marginal contribution to all subsets of the training data . Since Shapley-base scoring suffers from expensive computational cost , recent works focus on techniques that efficiently estimate the Shapley value, including Monte-Carlo sampling , group testing [14; 15] and using proxy models such as \(k\)-NN [16; 3]. However, those methods do not admit any theoretical guarantee on the effectiveness. This paper discusses the limitations of the above methods and suggests some future directions on data debugging.

## 2 Preliminaries and Problem Definition

**Linear classifiers.** Formally, a (binary) linear classifier is a function \(_{}:^{d}\{-1,1\}\), where \(d\) is called its _dimension_ and \(^{d}\) its parameter. Without loss of generality, the bias term of a linear classifier is set as zero in this paper. All vectors in this paper are assumed to be _column_ vectors. For an input \(\), the value of \(_{}\) is defined as

\[_{}()=1&^{}  0\\ -1&\]

We denote the class of linear models as \(\).

**Training data.** A _training sample_ is a pair \((,y)\) in which \(^{d}\) is the input and \(y\{-1,1\}\) is the label of \(\). The _training data_ is a multiset of training samples. We employ \(^{}\) to denote that the parameter \(^{}\) is obtained by training the parameter \(\) on the training data \(T\), and employ \(,y)}^{}\) to denote that \(^{}\) is obtained by training \(\) on the training sample \((,y)\).

   Loss Function & Dimension & Training Order & Complexity \\  Not Fixed & Not Fixed & - & NP-hard \\ Hinge-like & \( 2\) & Adversarially Chosen & NP-hard \\ Hinge-like, \(<0\) & \(1\) & Adversarially Chosen & NP-hard \\ Hinge-like, \( 0\) & \(1\) & - & Linear Time \\ Linear & - & - & Linear Time \\   

Table 1: Computational complexity of the data debugging problem 

**Loss functions and learning rates.** Binary linear classifiers typically use unary functions on \(y^{}\) as their loss functions . Therefore we only consider loss functions of the form \(:y^{}\) for the rest of the paper.

The _linear_ loss is in the form of

\[_{}(y^{})=-(y^{ }+).\]

The _hinge-like_ loss function is defined as the following form

\[_{}(y^{})=- (y^{}+),&y^{}< \\ 0,&.\]

We call \(\) as the _interception_ of \(_{}\). We represent the learning rate of a model using a vector \(=(_{1},,_{d})\), where \(_{i} 0\) and each parameter \(w_{i}\) can be updated with the corresponding learning rate \(_{i}\).

**Stochastic gradient descent.** The stochastic gradient descent (SGD) method updates parameter \(\) from its initial value \(^{(0)}\) through several epochs. During each epoch, the SGD goes through the entire set of training samples in some training order through several iterations. The training order is defined as a sequence of training samples, in the form of \((_{1},y_{1})(_{n},y_{n})\). For \(1 i<j n\), \((_{i},y_{i})\) is considered before \((_{j},y_{j})\) during the SGD. We use \(w_{i}\) to denote the \(i\)-th coordinate of \(\). We also use \(^{(e,k)}\) to denote the value of \(\) at the end of \(k\)-th iteration of epoch \(e\) and use \(^{(e)}\) to denote the value of \(\) after the end of epoch \(e\). Assuming \((,y)\) to be the training sample considered at iteration \(k\), the stochastic gradient descent (SGD) method updates parameter \(w_{i}\) for each \(i\) by

\[w_{i}^{(e,k)} w_{i}^{(e,k-1)}-_{i}( y(^{(e,k-1)})^{})}{ w_{i}}\] (1)

In other words, we have

\[^{(e,k)}^{(e,k-1)}- (y(^{(e,k-1)})^{})\]

where \(=(_{1}} { w_{1}},,_{d}}{ w_{d}})\) is the Hadamard product. We say a training sample \(\) is _activated_ at iteration \(k\) during epoch \(e\) if \((y(^{(e,k-1)})^{}) 0\). The SGD terminates at the end of epoch \(e\) if \(\|^{(e-1)}-^{(e)}\|<\) for threshold \(\) or \(e\) reached some predetermined value. We denote \(^{*}=^{(e)}\). A linear classifier trained by SGD with the meta-parameters mentioned above is denoted as \(_{}(,,,T)=_{ ^{*}}\). With a slight abuse of notation, we define \(_{}(,,,T, )=_{^{*}}()\). We also use \(_{}(T,)\) to avoid cluttering when the context is clear.

**Problem definition.** With the above definitions, Debuggable for SGD-trained linear classifiers can be formalized as follows:

  Debuggable-Lin \\
**Input:** Training data \(T\), loss function \(\), initial parameter \(^{(0)}\), learning rate \(\), threshold \(\) and instance \((_{},y_{})\). \\
**Output:** "Yes": if \( T\) such that \(_{}(,,,T ,_{})=y_{}\); \\ "No": otherwise. \\  

We say \(_{}(,,,T)\) is _debuggable_ on \((_{},y_{})\) if \((,^{(0)},,,T,_{ },y_{})\) is a yes-instance of Debuggable-Lin, and not _debuggable_ on \((_{},y_{})\) otherwise.

## 3 Results for Unfixed Loss Functions

In this section, we prove the NP-hardness of Debuggable-Lin. Intuitively, Debuggable-Lin is to determine whether there exists a subset \(T^{} T\) where activated training samples within \(T^{}\) drive the parameter \(\) toward the region defined by \(y_{}^{}_{}>0\). The activation of training samples depends on the complex interaction between the training data and the model.

**Theorem 3.1**.: Debuggable-Lin is NP-hard for all training orders.

We only show the proof sketch and leave the details in the appendix.

Proof Sketch.: We build a reduction from an NP-hard problem Monotone 1-in-3 SAT :

Monotone 1-in-3 SAT

**Input:** A 3-CNF formula \(\) with no negation signs.

**Output:**"Yes": if \(\) has a 1-in-3 assignment, under which each clause contains exactly one true literal;

"No": otherwise.

For example, \(_{1}=(x_{1} x_{2} x_{3})(x_{2} x_{3} x_{4})\) is a yes-instance because \((x_{1},x_{2},x_{3},x_{4})=()\) is an 1-in-3 assignment; \(_{2}=(x_{1} x_{2} x_{3})(x_{2} x_{3} x_{4}) (x_{1} x_{2} x_{4})(x_{1} x_{3} x_{4})\) is a no-instance.

Given a 3-CNF formula \(\), our goal is to construct a configuration of the training process, such that the resulting model outputs the correct answer if and only if its training data \(T^{}\) encodes an 1-in-3 assignment \(\) of \(\). This can be done by carefully designing the encoding so that for each \(x_{i}\), \((x_{i})=\) if and only if \(_{x_{i}} T^{}\). Finally, we can construct some \(T\) with \(T T^{}\{_{x_{i}}|x_{i}\}\), such that some classifier trained on \(T\) is a yes-instance of Debuggable-Lin if and only if \(\) is a yes-instance of Monotone 1-in-3 SAT, thereby finishing our proof.

**The reduction.** Suppose \(\) has \(m\) clauses and \(n\) variables, let \(N=n+2m+1\). We set the dimension of the linear classifier to \(N\).

The input. Each coordinate of the input is named as

\[=(x_{c_{1}},,x_{c_{m}},x_{x_{1}},,x_{x_{n}},x_{b_{1}}, ,x_{b_{m}},x_{})^{}\]

We also use \(x_{i}\) to denote the \(i\)-th coordinate of \(\).

The parameters. Each coordinate of the parameter is named as

\[=(w_{c_{1}},,w_{c_{m}},w_{x_{1}},,w_{x_{n}},w_{b_{1}}, ,w_{b_{m}},w_{})^{}\]

We also use \(w_{i}\) to denote the \(i\)-th coordinate of \(\). Each \(w_{x_{j}}\) represents the truth value of variable \(x_{j}\), where 1 represents True and -1 represents False. Similarly, each \(w_{c_{j}}\) represents the truth value of clause \(c_{j}\) based on the value of its variables. \(w_{b_{j}}\) and \(w_{}\) are used for convenience of proof.

The initial value of the parameter is set to

\[^{(0)}=(,,}^{m},^{n},^{m},1)^{}\]

Loss function. We denote \(U(x_{0},):=\{x|x_{0}-<x<x_{0}+\}\) as the \(\)-neighborhood of \(x_{0}\) and define \(U( x_{0},)=U(x_{0},) U(-x_{0},)\). We define the _local ramp function_ as

\[r_{x_{0},}(x)=0&,x x_{0}-;\\ x-x_{0}+&,x U(x_{0},);\\ 2&,x x_{0}+.\]

The loss function is defined as

\[=-r_{-5,0.01}(y^{})-r_{- {1}{2},0.26}(y^{})-_{x_{0}\{ 1, 3\}}r_{x_{0},0.01}(y^{}).\]

\(\) is monotonically decreasing with derivatives

\[}{ w_{i}}=- yx _{i}&,y^{} U(-5,0.01);\\ -yx_{i}&,y^{} U(-,0.26);\\ -yx_{i}&,y^{}_{x_{0}\{ 1, 3\}}U(x_{0},0.01);\\ 0&,\] (2)Learning rate. The learning rate for SGD is set to be

\[=(^{m},,,}^{n},^{m},1)^{}.\]

Training data. We define two gadgets, \((i)\) and \((i,i_{1},i_{2},i_{3})\), as illustrated in Table 2 and 3. All the unspecified coordinates are set to zero. We use \(T_{0}\) to denote the training data. \((i)\) is contained in \(T_{0}\) if and only if \(x_{i}\), and \((i,i_{1},i_{2},i_{3})\) is contained in \(T_{0}\) if and only if \(c_{i}=(x_{i_{1}} x_{i_{2}} x_{i_{3}})\).

Threshold and instance. The threshold \(\) can be any fixed value in \(_{+}\). The instance is defined as \((_{},y_{})\), where \(y_{}=1\) and

\[_{}=(^{m},^ {n+m},)^{}.\]

The following reduction works for all possible training orders. Intuitively, during the training process, each \((i)\) in the training data will set \(w_{x_{i}}\) to around \(1\) (that is, mark \(x_{i}\) as True) in the first epoch, and each \((i,i_{1},i_{2},i_{3})\) will set \(w_{c_{i}}\) to near \(\) in the second epoch, if and only if exactly one of \(w_{x_{i_{1}}},w_{x_{i_{2}}},w_{x_{i_{3}}}\) is near \(1\) and the others near \(-1\) (that is, mark \(c_{i}\) as satisfied if exactly one of its literals is True and the others False). The training process terminates at the end of the second epoch. 

## 4 Results for Fixed Loss Functions

We have proved the NP-hardness for Debuggable-Lin when the loss function is not fixed. In this section, we study the complexity when the loss function is fixed as linear and hinge-like functions. Assuming that SGD terminates after only one epoch with a fixed order, we will show that Debuggable-Lin is solvable in linear time for linear loss. For hinge-like loss functions, Debuggable-Lin can be solved in linear time only when the dimension \(d=1\) and the interception \( 0\). For the rest cases, Debuggable-Lin becomes NP-hard.

### The Easy Case

We start with the linear loss function \(=-(y^{}+)\), with which all the training data are activated and \(^{*}=^{*}(T)=^{(0)}+_{(,y) T}  y\). Since \(y_{}\{-1,1\}\), Debuggable-Lin is equivalent to deciding whether

\[_{T^{} T}\{y_{}(^{*}(T^{}))^{ }_{}\}>0.\]

A training sample \((,y)\) is "good" if \(y_{}( y)^{}_{}>0\) and "bad" otherwise. The _good training-sample assessment_ (GTA) algorithm, as shown in Algorithm 1, deals with this situation by greedily picking all "good" training samples.

Denoting \(T^{*}\) as the set of all good data in \(T\), it follows that

\[y_{}(^{*}(T^{*}))^{}_{} =y_{}(^{(0)})^{}_{}+ _{(,y) T^{*}}y_{}( y )^{}_{}\] \[ y_{}(^{(0)})^{}_{}+_{(,y) T^{}}y_{}( y )^{}_{}\]

for all \(T^{} T\). Hence \(_{T^{} T}\{y_{}(^{*}(T^{}))^{ }_{}\}=y_{}(^{*}(T^{*}))^{} _{}\) and Debuggable-Lin can be solved by GTA in linear time. The following theorem is straightforward.

**Theorem 4.1**.: Debuggable-Lin is linear time solvable for linear loss functions.

```
0: Training data \(T\), loss function \(\), initial parameter \(^{(0)}\), learning rate \(\), threshold \(\) and test instance \((_{},y_{})\).
0: TRUE, iff \(_{}(,,,T)\) is debuggable on \((_{}y_{})\).
1\(^{(0)}\);
2for\((,y) T\)do
3if\(y_{}( y)^{}_ {}>0\)then
4\(+ y\);
5
6 end if
7if\(y_{}^{}_{} 0\)then
8 return TRUE;
9
10 end if
11return FALSE; ```

**Algorithm 1**Good Training-sample Assessment (GTA)

GTA is still effective for one-dimensional classifiers trained with hinge-like losses when \( 0\).

**Theorem 4.2**.: Debuggable-Lin is linear time solvable for hinge-like loss functions, when \(d=1\) and \( 0\).

Proof.: It suffices to prove that if \( T^{} T\) such that \(_{}(T^{},x_{})=y_{}\), \(_{}(T^{*},x_{})=y_{}\).

a) Suppose all the data in \(T^{*}\) are activated, we have

\[y_{}w^{*}(T^{*})x_{} =y_{}w^{(0)}x_{}+_{(x,y) T^{*}}y_{ } y xx_{}\] \[ y_{}w^{(0)}x_{}+_{(x,y) T^{ } T^{*}}y_{} y xx_{}+_{(x,y) T ^{} T^{*}}y_{} y xx_{}\] \[=y_{}w^{*}(T^{})x_{} 0\]

b) Suppose \((x,y) T^{*}\) is the first inactivated data during the training phase, and \(w\) is the current parameter, we have \(ywx>\). Since \((xy)(x_{}y_{}) 0\), we have \((x_{}y_{}) w 0\). Let \(T^{}\) be the set of training data appeared before \((x,y)\), we have \(y_{}w^{*}(T^{*})x_{} y_{}w^{*}(T^{ })x_{} 0\). 

### The Hard Case

The gradient of training data may not always be activated and could be affected by the training order. When the training order is adversarially chosen, the following theorem shows that Debuggable-Lin is NP-hard for all \(d 2\) and \(\).

**Theorem 4.3**.: If the training order is adversarially chosen and \(d 2\), Debuggable-Lin is NP-hard for _each_ hinge-like loss function at _every_ constant learning rate.

Proof sketch.: Since the result can be easily extended for all \(d>2\) by padding the other \(d-2\) dimensions with zeros, we only prove for the case of \(d=2\). We assume \(-1\) and leave the \(<-1\) case to the appendix. To avoid cluttering, we further assume \(=\) and \(=1\). The proof can be easily generalized by appropriately re-scaling the constructed vectors.

We build a reduction from the subset sum problem, which is well-known to be NP-hard:

```
0: Subset Sum Input: A set of positive integer \(S\), and a positive integer \(t\). Output: "Yes": if \( S^{} S\) such that \(_{a S^{}}a=t\); "No": otherwise. ```

**Algorithm 2**The Hard Case

[MISSING_PAGE_EMPTY:8]

which contradicts to the fact that \(y_{}^{}_{} 0\).

Let \(S^{}=\{a_{i}|(_{i},y_{i}) T^{*}\}\) and \(t^{}=_{a S^{}}a_{i}\), it suffices to prove \(t^{}=t\). Notice that

\[^{(0)}\{(_{i},y_{i})|1  j i\}}_{c} =((-18n^{2}m^{2}+|}{n+1}),3 _{a_{i} S^{}}a_{i})\] \[=((-18n^{2}m^{2}+|}{n+1}),3 t^{})\]

Hence \(y_{c}_{c}^{}_{c}=(-18n^{2}m^{2}+|}{n +1})(18n^{2}m^{2}-2)-9 tt^{}<-1\), thus

\[_{c}_{c},y_{c})}_{b}=_{ c}+_{c}y_{c}=((|}{n+1}-2),3(t^{ }-t))\]

(1) If \(t^{} t-1\), we have \(y_{b}_{b}^{}_{b}=(|}{n+1}-2+3( t-t^{}))>\), a contradiction. (2) If \(t^{} t+1\), we have \(y_{a}_{a}^{}_{a}=(|}{n+1}-2+3( t^{}-t))>\), another contradiction.

Therefore \(t^{}=t\), and this completes the proof. 

Moreover, Debuggable-Lin is NP-hard even when \(d=1\) and \(<0\).

**Theorem 4.4**.: If the training order is adversarially chosen and \(d=1\), Debuggable-Lin remains NP-hard for _each_ hinge-like loss function with \(<0\) at _every_ constant learning rate.

**Remarks.** The training order in this section can be arbitrary as long as the last three training samples are \((_{c},y_{c}),(_{b},y_{b}),(_{a},y_{a})\), respectively. All the training samples are "good" since for each \((,y) T\) we have \(^{}_{}y_{}>0\). This implies that Debuggable-Lin is NP-hard even if all the training data are "good" training samples, and exemplifies why the GTA algorithm fails for higher dimensions.

## 5 Discussion and Conclusion

In this paper, we provided a comprehensive analysis on the complexity of Debuggable. We focus on the linear classifier that is trained using SGD, as it is a key component in the majority of popular models.

Since Debuggable is a special case of data debugging, the above results proved the intractability of data debugging and therefore gives a negative answer to Problem 1.1 declared in the introduction. The complexity results also demonstrated that it is not accurate to estimate the impact of subset of training data by summing up the score of each training samples in the subset, _as long as the scores can be calculated in polynomial time_.

In Section 4, a training sample is said to be "good" if it can help the resulting model to predict correctly on the test instance. That is, it can increase \(y_{}(^{*})^{}_{}\). However, in our proof we showed that Debuggable remains NP-hard even if all training samples are "good". This suggests that the quality of a training sample does not depend only on some properties of itself but also on the interaction between the rest of the training data, which should be taken into consideration when developing data cleaning approaches.

Moreover, the NP-hardness of Debuggable implies that, it is in general intractable to figure out the causality between even the prediction of a linear classifier and its training data. This may be seem surprising since linear classifiers have long been considered "inherently interpretable". As warned in , _a method being "inherently interpretable" needs to be verified before it can be trusted_, the concept of interpretability must be _rigorously defined_, or at least its boundaries specified.

Our results suggests the following directions for future research. Firstly, characterizing the training sample may be helpful in designing efficient algorithms for data debugging; Secondly, designing algorithms using CSP-solver is a potential way to solve data debugging more efficiently than the brute-force algorithms; Finally, developing random algorithms is a potential way to solve data debugging successfully with high probability.