# Variational Classification

Shehzaad Dhuliawala

Department of Computer Science, ETH Zurich

{shehzaad.dhuliawala, mrinmaya.sachan}@inf.ethz.ch

&Mrinmaya Sachan

AI Centre, ETH Zurich

carl.allen@ai.ethz.ch

&Carl Allen

AI Centre, ETH Zurich

###### Abstract

We present _variational classification_ (VC), a latent variable generalisation of neural network softmax classification under cross-entropy loss. Our approach provides a novel probabilistic interpretation of the highly familiar softmax classification model, to which it relates comparably to variational vs deterministic autoencoders. We derive a training objective based on the evidence lower bound (ELBO) that is non-trivial to optimize, and an adversarial approach to maximise it. We reveal an inherent inconsistency within softmax classification that VC addresses, while also allowing flexible choices of distributions in the latent space in place of assumptions implicit in standard softmax classifiers. Empirical evaluation demonstrates that VC maintains accuracy while improving properties such as calibration and adversarial robustness, particularly under distribution shift and low data settings. By explicitly considering representations learned by supervised methods, we offer the prospect of the principled merging of supervised learning with other representation learning methods, e.g. contrastive learning, using a common encoder architecture.

## 1 Introduction

Classification is a core task in machine learning, from categorising objects (Klasson et al., 2019) and providing medical diagnoses (Adem et al., 2019; Mirbabaie et al., 2021), to identifying potentially life-supporting planets (Tiensuu et al., 2019). Classification tasks are commonly tackled by training domain-specific neural networks with a _sigmoid_ or _softmax_ output layer.1 Data samples \(x\) (in a domain \(\)) are mapped deterministically by a network \(f_{}\) (with weights \(\)) to a real vector \(z\!=\!f_{}(x)\), which is transformed in the softmax layer to a point on the simplex \(^{||}\), that parameterises \(p_{}(y|x)\), a discrete distribution over class labels \(y\!\!\):

\[p_{}(y|x)=w_{y}+b_{y}\}}{_{y^{}}\{z^{}w_{y^{}}+b_{y^{}}\}}\.\] (1)

Although softmax classifiers often perform well, they suffer well-known issues: (i) they are are poorly understood theoretically and in many respects a "black box" with predictions \(p_{}(y|x)\) hard to explain; (ii) predictions can vary significantly for imperceptible changes in the data (adversarial examples); (iii) predictions may identify a true label as the most probable class but poorly reflect uncertainty in the prediction (miscalibration); and (iv) they typically require a lot of data to train.

We introduce _Variational Classification_ (**VC**), which generalises softmax cross-entropy classification under a latent variable model (figure 2). The VC framework ascribes probabilistic roles to components of a softmax classifier: (i) the neural network (excluding the softmax layer) transforms a _mixture of unknown distributions_ in the data space to a _mixture of chosen distributions_ in the latent space; and (ii) the softmax layer converts the latter to class predictions by Bayes' rule. We show that, without tailoring the loss function to mitigate any particular issue with softmax classification, VCmaintains predictive accuracy while the additional latent structure improves calibration, robustness to adversarial perturbations and domain shift, and performance in low data regimes.

## 2 Background (Variational Auto-Encoder)

Estimatiing parameters of a latent variable model \(p_{}(x)=_{z}p_{}(x|)p_{}()\) is typically intractable, and instead one maximises the _evidence lower bound_ (ELBO):

\[_{x}p(x) p_{}(x)_{x}p(x)_{z}q_{}(z|x) p _{}(x|z)-(z|x)}{p_{}(z)}},\] (2)

The _variational auto-encoder_(VAE, Kingma & Welling, 2014; Rezende et al., 2014) is an implementation of the ELBO in which all distributions are assumed Gaussian, with \(p_{}(x|z)\), \(q_{}(z|x)\) parameterised by neural networks. The VAE probabilistically generalises a deterministic auto-encoder, allowing for uncertainty or stochasticity in the latent \(|x\), whose entropy is promoted and is constrained to a prior by the second ("regularisation") term.

## 3 Variational Classification

**A Latent Variable Model for Classification**: Data \(x\) and labels \(y\) are treated as samples of random variables \(\), \(\) jointly distributed by \(p(,)\). A softmax classifier is a deterministic function mapping \(x\), via a sequence of intermediate representations, to a point on the simplex \(^{||}\) that parameterises a categorical label distribution \(p_{}(|x)\).

Any intermediate representation \(z=g(x)\) can be considered the realisation of a _latent_ random variable sampled from a conditional (delta) distribution: \(z p(x|x)=_{z-g(x)}\). Under a (Markov) generative latent variable model (figure 2, _left_):

\[p(x)=_{y,z}p(x|z)p(z|y)p(y)\,\] (3)

class labels can be predicted:

\[p_{}(y|x)=_{z}p_{}(y|z)p_{}(z|x)\.\] (4)

**A softmax classifier is a special case of Eqn. 4** where: (i) \(f_{}\), the neural network up to the softmax layer, parameterises \(p_{}(z|x)=_{z-f_{}(x)}\), a delta distribution; (ii) the softmax layer input is considered a sample from \(p_{}(z|x)\); and (iii) \(p_{}(|z)\) is defined by the softmax layer (see RHS of Equation 1).

Figure 1: Empirical latent distributions: softmax inputs under (\(l\)) Softmax CE [“MLE”]; (\(c\)) MLE + Gaussian \(p_{}(|y)\) (contours); [“MAP”]; (\(r\)) VC objective [“Bayesian”]. Colour denotes MNIST class.

**Training a Classification LVM**: Similarly to the latent model for \(p_{}()\) (SS2), parameters of eqn 4 cannot generally be learned by directly maximising the likelihood, but rather a lower bound (_cf_ eqn 2):

\[_{x,y}\!\!\!p(x,y) p_{}(y|x) =_{x,y}\!\!\!p(x,y)\!_{z}q_{}(z|x) p_{ }(y|z,)-\!(z|x)}{p_{}(z|x)}+ (z|x)}{p_{}(z|x,y)}}\] \[_{x,y}\!\!\!p(x,y)\!_{z}q_{}(z|x)(z|y)p_{}(y)}{_{y^{}}p_{}(z|y^{})p_{ }(y^{})}\;\;_{}\] (5)

Here, \(p_{}(y|z,x)\!=\!p_{}(y|z)\) (figure 2), and the (freely chosen) variational posterior \(q_{}\) is assumed to depend only on \(x\) and to equal \(p_{}(z|x)\) (eliminating the second term).2**Maximising ELBO\({}_{}\) **implicitly encourages \(\) to learn a sufficient statistic for \(|\)**, i.e. \(q_{}(z|x) p(z|x,y)\). It can also be shown that **ELBO\({}_{}\) generalises Softmax Cross Entropy** (SCE), under above assumptions.

**Two versions of the same class-conditional latent distributions**:

* _Anticipated_ class-conditional latent distributions \(p_{}(z|y)\) are specified in ELBO\({}_{}\), encoded in the softmax layer, and need to be met for correct label predictions \(p(y|x)\) to be output;
* _Empirical_ class-conditional latent distributions are defined by \(q_{}(z|y)\!_{x}q_{}(z|x)p(x|y)\), i.e. by sampling \(q_{}(z|x)\) (parameterised by the neural network \(f_{}\)), given class samples \(x p(|y)\).

In several scenarios that arise in practice, e.g. for finite samples from a continuous data domain \(\) (e.g. images or sounds), or if classes are mutually exclusive, **ELBO\({}_{}\) is maximised if all latent representations of a class, hence the entire class-conditional distribution \(q_{}(|y)\), "collapse" to a point, irrespective of any variance in \(p_{}(z|y)\). Since SCE is a special case of ELBO\({}_{}\), this suggests that softmax classifiers may learn over-concentrated latent distributions and so give _over-confident_ and uncalibrated predictions (subject to the data distribution and model flexibility).

**Aligning anticipated and empirical latent distributions**: We align \(p_{}(z|y)\) and \(q_{}(z|y)\), or encourage \(p_{}(y|z)\) and \(q_{}(z|y)\) to be _consistent under Bayes' rule_ (_cf_\(p_{}(x|z)\) and \(q_{}(z|x)\) in the ELBO, SS2) by minimising \(D_{}\![\,q_{}(|y)\,]p_{}(|y) \!,\; y\!\!\), (weighted by \(\!>\!0\)) giving the full VC objective:

\[_{}=_{x,y}\!\!\!p(x,y)_{z}q_{}(z|x) (z|y)p_{}(y)}{_{y^{}}p_{}(z|y^{ })p_{}(y^{})}\;-\!_{z}q_{}(z|y)(z|y)}{p_{}(z|y)}\;+\; p_{}(y)}.\] (6)

Taken incrementally, \(q_{}\)-terms of \(_{}\) can be interpreted w.r.t. latent variable z as follows (see Fig. 1):

* maximising \(_{z}q_{}(z|x) p_{}(y|z)\) may overfit \(q_{}(z|y)\) to \(_{z-z_{y}}\) for finite samples; [MLE]
* adding _class priors_\(_{z}q_{}(z|y) p_{}(z|y)\) constrains the MLE point estimates \(z_{y}\) [MAP]
* adding _entropy_\(-\!_{z}q_{}(z|y) q_{}(z|y)\) encourages \(q_{}(|y)\) to "fill out" \(p_{}(|y)\). [Bayesian]

VC abstracts a typical neural network classifier, giving interpretability to its components:

* the neural network up to the last layer (\(f_{}\)) transforms a mixture of unknown class-conditional data distributions \(p(|y)\) to a mixture of analytically defined latent distributions \(p_{}(|y)\);
* assuming latent variables follow the anticipated class distributions \(p_{}(|y)\), the output layer applies Bayes' rule to give \(p_{}(|)\) (see figure 2) and thus the class prediction \(p(|x)\) (by eqn 4).

### Optimising the VC Objective

The second term of \(_{}\) is not readily computable since \(q_{}(|y)\) is implicit and cannot be evaluated only sampled from, as \(z\!\!q_{}(|x)\) (parameterised by \(f_{}\)) for class samples \(x\!\!p(|y)\). We therefore _approximate_ log ratios \((z|y)}{p_{}(z|y)}\) for each class \(y\) by training a binary classifier to distinguish \(z\!\!q_{}(|y)\) from \(z\!\!p_{}(|y)\) under an _auxiliary objective_\(_{}\) with parameters \(\):

\[_{}=_{y}p(y)\!_{z}\!q_{}(z|y) (T_{}^{y}(z))+\!_{z}\!p_{}(z|y)(1\!-\!(T_{}^ {y}(z))}\] (7)

where \((x)\!=\!(1+e^{-x})^{-1}\) is the logistic sigmoid, \(T_{}^{y}(z)\!=\!w_{y}^{}z+b_{y}\) and \(\!=\!\{w_{y},b_{y}\}_{y}\). This approach is _adversarial_: \(_{}\) is maximised when log ratios give a _minimal_ KL divergence (0), i.e. \(q_{}(z|y)\!=\!p_{}(z|y)\) and \(z\!\!q_{}(z|y)\) are indistinguishable from \(z\!\!p_{}(z|y)\); whereas \(_{}\) is maximised if the ratio is _maximal_ and the distributions are optimally discriminated. See Algorithm 1 for a summary.

[MISSING_PAGE_FAIL:4]