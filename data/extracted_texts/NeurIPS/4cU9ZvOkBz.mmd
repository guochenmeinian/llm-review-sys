# Crowdsourcing and Research with Human Subjects

What is my quantum computer good for? Quantum capability learning with physics-aware neural networks

Daniel Hothem

Quantum Performance Laboratory

Sandia National Laboratories

Livermore, CA 94550

dhothem@sandia.gov

&Ashe Miller

Quantum Performance Laboratory

Sandia National Laboratories

Albuquerque, NM 87185

anmille@sandia.gov

Timothy Proctor

Quantum Performance Laboratory

Sandia National Laboratories

Livermore, CA 94550

tjproct@sandia.gov

###### Abstract

Quantum computers have the potential to revolutionize diverse fields, including quantum chemistry, materials science, and machine learning. However, contemporary quantum computers experience errors that often cause quantum programs run on them to fail. Until quantum computers can reliably execute large quantum programs, stakeholders will need fast and reliable methods for assessing a quantum computer's capability--i.e., the programs it can run and how well it can run them. Previously, off-the-shelf neural network architectures have been used to model quantum computers' capabilities, but with limited success, because these networks fail to learn the complex quantum physics that determines real quantum computers' errors. We address this shortcoming with a new quantum-physics-aware neural network architecture for learning capability models. Our scalable architecture combines aspects of graph neural networks with efficient approximations to the physics of errors in quantum programs. This approach achieves up to \( 50\%\) reductions in mean absolute error on both experimental and simulated data, over state-of-the-art models based on convolutional neural networks, and scales to devices with 100+ qubits.

## 1 Introduction

Quantum computers have the potential to efficiently solve classically intractable problems in quantum chemistry , materials science , machine learning , and cryptography . While contemporary quantum computers are approaching the size and noise levels needed to solve interesting problems , they are far from being capable of reliably running most useful quantum programs . Until we build quantum computers capable of executing _any and all_ useful and interesting quantum programs, stakeholders will require fast, reliable, and scalable methods for predicting the programs that a given quantum computer can reliably execute.

The task of learning which quantum programs a particular quantum computer can reliably execute is known as _quantum capability learning_. Quantum capability learning is very difficult because the number of possible (Markovian) errors plaguing a quantum computer grows exponentially in its size [Blume-Kohout et al., 2022], i.e., in the number of qubits (\(n\)) it contains, and errors in a quantum program can combine in difficult-to-predict ways [Proctor et al., 2021a]. Most existing approaches to capability learning restrict themselves to learning how well a quantum computer executes a small set of quantum programs, by running all of these programs and estimating a success metric for each one [Lubinski et al., 2023, Proctor et al., 2024]. While these methods provide insight into a quantum computer's capability, they are not predictive.

Recently, several groups have proposed building predictive models of a quantum computer's capability using convolutional neural networks (CNNs) [Elsayed Amer et al., 2022, Hothem et al., 2024b, Vadali et al., 2024, Hothem et al., 2023b] and graph neural networks (GNNs) [Wang et al., 2022]. However, these neural-network-based capability models achieve only modest prediction accuracy when applied to real quantum computers, because they fail to learn the complex physics that determines real quantum computers' failures [Hothem et al., 2024b].

In this work, we introduce a novel quantum-physics-aware neural network (qpa-NN) architecture for quantum capability learning (Fig. 1). Our approach uses neural networks with GNN-inspired structures to predict the rates of the most physically relevant errors in quantum programs. These predicted error rates are then combined using an efficient approximation to the exact (but exponentially costly) quantum physics formula for how those errors combine to impact a program's success rate. Our approach leverages the graph structures that encode the physics of how errors' rates typically depend on both the quantum program being run and how a quantum computer's qubits are arranged, and it offloads the difficult-to-learn, yet classically tractable task of approximately combining these error rates to predict a circuit's performance to an already-known function. This enables our qpa-NNs to vastly outperform the state-of-the-art CNNs of Hothem et al. [2024b] on both experimental and simulated data without sacrificing the ability to model large devices of 100+ qubits.

Our qpa-NNs are enabled, in part, by focusing on learning a quantum computer's capability on high-fidelity quantum programs, which are those programs that a quantum computer correctly executes with high probability. High-fidelity programs are arguably the most interesting programs to study as we care far more about whether a quantum computer successfully executes a program 99% or 90% of the time rather than 1% or 10% of the time.

In a head-to-head comparison, our qpa-NNs achieve a \( 50\%\) reduction in mean absolute error (MAE) over the CNNs of Hothem et al. [2024b], on average and on the same experimental datasets. Our qpa-NNs achieve an average \( 36\%\) improvement over those CNNs even after fine-tuning those CNNs on the same subset of the training data (high-fidelity programs) used to train our qpa-NNs.

Our qpa-NNs' improved performance is likely largely due to their improved ability to model the impact of coherent errors on a program's success rate. Off-the-shelf networks struggle with coherent errors [Hothem et al., 2024b], but our qpa-NNs are designed to model how these errors add up and cancel out, making the qpa-NNs much better predictors in the presence of coherent errors. To verify this, we demonstrate that our qpa-NNs can accurately predict the performance of random circuits run on a hypothetical 4-qubit quantum computer experiencing only coherent errors. Our qpa-NN obtained a \( 50\%\) lower MAE than a CNN, averaged across five datasets, and the trained qpa-NN even exhibits moderate performance when making predictions for a different class of circuits (random mirror circuits [Proctor et al., 2021a]) simulated on the same hypothetical 4-qubit quantum computer, i.e., our qpa-NNs display moderate prediction accuracy on out-of-distribution data.

We make the following contributions in our work:

1. We introduce qpa-NNs, a bespoke neural network architecture for modeling the capability of a quantum computer, which outperform state-of-the-art CNN models by \( 50\%\) on experimental and simulated data.
2. We use our qpa-NNs to model the capability of a simulated 100-qubit device; the largest ever neural network capability learning demonstration by a factor of two.
3. We demonstrate, for the first time, how to train NNs to predict the process fidelity [Nielsen, 2002] of a circuit, which is the most widely used quantum channel error metric.
4. We provide evidence that the improved performance of our qpa-NNs is partly due to their ability to better model the effect of coherent errors, which are known to be challenging for other state-of-the-art methods.

Background

In this section, we review the background in quantum computing necessary to understand this paper. See Nielsen and Chuang (2010) for an in-depth introduction to quantum computing and Blume-Kohout et al. (2022) or Hashim et al. (2024) for a thorough description of the errors in quantum computers.

### Quantum computing

A quantum computer performs computations using qubits, which are two-level systems whose pure states are unit vectors in a complex two-dimensional Hilbert space, \(\). The pure states of \(n\) qubits are unit vectors in \(^{ n}\). The two orthonormal vectors \(|0\) and \(|1\) that are eigenvectors of the \(Z\) Pauli operator are identified as the _computational basis_ of \(\). Errors and noise in real quantum computers mean that they are typically in states \(\) that are probabilistic mixtures of pure states.

A quantum computation is performed by running a quantum program, typically known as a _quantum circuit_ (see illustration in Fig. 1a). An \(n\)-qubit quantum circuit (\(c\)) of depth \(d\) is defined by a sequence of \(d\) layers of logical instructions \(\{L_{i}\}\). Executing \(c\) consists of preparing each qubit in \(|0\), applying each \(L_{i}\), and then measuring each qubit to obtain an \(n\)-bit string \(b\). Each layer typically consists of parallel one- and two-qubit gates, and it is intended to implement a \(2^{n} 2^{n}\) unitary \(U(L_{i})\). Together, the layers are intended to implement \(U(c)=U(L_{d}) U(L_{1})\).

If quantum circuit \(c\) is implemented without error, its output bit string \(b\) is a sample from a distribution \((c)\) whose probabilities are given by \((b=x)=| x|U(c)|00 0|^{2}\) where \(|00 0=|0|0\), \(|x=|x_{1}|x_{n}\), and \(x_{i}\) is the \(i\)-th bit in \(x\). However, when a circuit is executed on a real quantum computer, errors can occur and this means that its output bit string \(b\) is a sample from some other distribution \((c)\). The process of errors corrupting a quantum computation can be modelled as follows. Each logic layer \(L_{i}\) implements the intended unitary superoperator \((L_{i}): U(L_{i}) U^{}(L_{i})\), where \(\) is a general \(n\)-qubit state, followed by an error channel \(_{i}\) that is a completely positive and trace preserving (CPTP) superoperator (Blume-Kohout et al., 2022). The imperfect implementation of a circuit \(c\) is then simply \(}(c)=_{i=1}^{d}_{i}(L_{i})\), and the output bit string \(b\) is \(x\) with probability \((b=x)=(|x x|}(c)[|00 0  00 0|])\).

### Quantum capability learning

Because quantum computers are error-prone, knowing which quantum circuits a particular quantum computer can execute with low error probability is important. Known as _quantum capability learning_(Proctor et al., 2021; Hothem et al., 2024), this task formally involves learning the mapping between a set of quantum circuits \(c\) and some success metric \(s(c)\) quantifying how well \(c\) runs on a quantum computer \(\). In this work, we consider a large class of circuits known as Clifford (or stabilizer) circuits (Aaronson and Gottesman, 2004), which are sufficient to enable quantum error correction (Campbell et al., 2017), and two widely-used success metrics: _probability of successful trial_ (PST) (a.k.a. success probability) and the _process fidelity_(a.k.a. entanglement fidelity) (Hothem et al., 2024; Nielsen, 2002).

PST is defined only for definite-outcome circuits, which are circuits whose output distribution has (if run without error) support on a single bit string, \(b(c)\). For any such circuit \(c\), PST is defined as

\[(c)=(\;b(c)\;\; \;c\;\;). \]

In practice, \((c)\) is estimated by running the circuit \(N_{} 1\) times on \(\) and calculating

\[(c)}=\;\;b(c)}{N _{}}. \]

Process fidelity is defined for all circuits, and it quantifies how close the actual quantum evolution of the qubits is to the ideal unitary evolution. It is given by

\[F(c)=}[}(c)^{-1}(c) ]. \]

Estimating \(F(c)\) is more complicated than estimating \((c)\), but efficient methods exist, such as mirror circuit fidelity estimation (Proctor et al., 2022). Hence, in theory, it is possible to efficiently gather training data using either \((c)\) or \(F(c)\) on arbitrarily large quantum computers.

### Modelling errors in quantum computers

Our qpa-NNs build in efficient approximations to the quantum physics of errors in quantum computers. They do so using the following parameterization of an error channel: \(=(_{j}_{j}G_{j})\). Here \(_{n}=\{G_{j}\}\) is the set of \(2^{2n+1}-2\) different Hamiltonian (H) and Stochastic (S) _elementary error generators_ introduced by Blume-Kohout et al. (2022), and \(_{j}\) is the _rate_ of error \(G_{j}\). Not every kind of error process can be represented in this form (e.g., amplitude damping, or non-Markovian errors), but this parameterization includes many of the most important kinds of errors in contemporary quantum computers. Each H and S error generator is indexed by a non-identity element of the \(n\)-qubit Pauli group (\(_{n}\)). The Pauli operator indexing an H or S error indicates the qubits it impacts and its direction, e.g., the H error generator indexed by \(X I^{(n-1)}\) is a coherent error on the 1st qubit and it rotates that qubit around its \(X\) axis.

Our qpa-NNs use approximate formulas for computing \((c)\) or \(F(c)\) from the rates of H and S errors, which we now review. Consider pushing each error channel \(_{i}\) to the end of the circuit and combining them together, i.e., we compute the error channel \((c)\) defined by \(}(c)=(c)(c)\). Then

\[(c) 1-_{P_{n}^{X,Y}}s_{P}+_{ p}^{2}, \]

where \(s_{P}\) and \(_{P}\) are the rates of the \(P\)-indexed \(S\) and \(H\) error generators, respectively, in \(c\)'s error channel \((c)\), and \(_{n}^{X,Y}\) is the set of \(n\)-qubit Pauli operators containing at least one \(X\) or \(Y\). Similarly,

\[F(c) 1-_{P_{n}}s_{P}+_{p}^{2}. \]

Equations (4) and (5) are good approximations for low-error circuits (Madzik et al., 2022). However, they both suffer from the same flaw: they require tracking \((4^{n})\) parameters. To address this problem, our qpa-NNs make an approximation: they only account for the contributions of a polynomially-sized set of errors that contains all those errors which are most likely to be experienced by a quantum computer. In this work, we chose to account for only local, low-weight errors, i.e., those with initial support on a small, connected subset of a device's connectivity graph.

Figure 1: **Quantum capability learning with quantum-physics-aware neural networks (qpa-NNs). Our qpa-NNs are a novel architecture for learning a quantum computer’s capability, i.e., the mapping from quantum circuits (or programs) to how well that imperfect quantum computer can run those circuits. These networks build in physical principles for how errors in quantum circuits occur—which can be expressed in terms of a quantum computer’s connectivity graph—and efficient approximations to the physics of how these errors combine to impact a circuit’s success rate.**Neural network architecture

Our neural network architecture (see Fig. 1) for quantum capability learning combines neural network layers that have GNN-like structures with efficient approximations to the physics of errors in quantum computers. The overall action of our neural networks is to map an encoding of a circuit \(c\) to a prediction for \((c)\) or \(F(c)\). The same network can predict either \((c)\) or \(F(c)\) by simply toggling between two different output layers that have no trainable parameters. Our architecture is divided into two sequential parts. The first part of our architecture is a neural network \(\) that has the task of learning about the kinds and rates of errors that occur in quantum circuits. We use GNN-like structures within \(\) to embed physics knowledge for how those errors depend on the quantum circuit being run. The second part of our architecture is a function \(f\) with no learnable parameters, that turns \(\)'s output into a prediction for \((c)\) or \(F(c)\).

### Physics-aware neural networks for predicting errors in quantum circuits

The neural network \(\)'s input is a quantum circuit \(c\) of depth \(d(c)\) represented by (i) a tensor \(I(c)\{0,1\}^{n d(c) n_{ch}}\) describing the gates in \(c\) (see Fig. 0(a)), and (ii) a matrix \(M(c)\{0,1\}^{2 n}\) describing the measurement of the qubits at the end of \(c\). \(\) maps \(I(c)\) to a matrix \(^{k d(c)}\) and \(M(c)\) to a vector \(^{k}\). \(_{ij}\) is a prediction for the rate with which error type \(j\) occurs during circuit layer \(i\), and \(m_{j}\) is a prediction for the rate with which error type \(j\) occurs when measuring the qubits at the end of a circuit. There are \(2(4^{n}-1)\) different possible error types that can occur in principle (see Section 2) so it is infeasible to predict all their rates beyond very small \(n\). However, the overwhelming majority of these errors are implausible, i.e., they are not expected to occur in real quantum computers [Blume-Kohout et al., 2022]. Our networks therefore predict the rates of every error from a relatively small set of error types \(=\{G_{1},,G_{k}\}\) containing the \(k\) most plausible kinds of error. \(\) is a hyperparameter of our networks. It can be chosen to reflect the known physics of a particular quantum computer and/or optimized using hyperparameter tuning. In our demonstrations, we choose \(\) to contain all one-body H and \(\) errors as well as all two-body H and \(\) errors that interact pairs of qubits within \(h\) steps on the modelled quantum computer's connectivity graph for some constant \(h\) (see Fig. 0(b)-c, where Fig. 0(c) shows an H or \(\) error in \(\) if \(h 2\)). This choice for \(\) encodes the physical principles that errors are primarily either localized to a qubit or are two-body interactions between nearby qubits [Blume-Kohout et al., 2022]. The size of \(\) grows with \(n\), and for planar connectivity graphs (as in, e.g., contemporary superconducting qubit systems [Arute et al., 2019]) it grows linearly in \(n\). This results in \(k=(n)\) errors whose rates \(\) must learn to predict.

The internal structures of \(\) are chosen to reflect general physical principles for how \(\) and \(\) depend on \(c\). \(_{ij}\) is a prediction for the rate that \(G_{j}\) occurs in circuit layer \(i\), and this error corresponds to a space/time location within \(c\)--because it occurs at layer index or time \(i\) and \(G_{j}\) acts on a subset of the qubits \(Q(G_{j})\) (see example in Fig. 0(c)). This error's rate will therefore primarily depend only on the gates in a time- and space-local region around its location in \(c\). Furthermore, this dependence will typically be invariant under time translations (this is true except for some exotic non-Markovian kinds of errors, which we discuss in Section 8.1). We can encode these structures into \(\) by predicting \(_{ij}\) from a space-time "window" of \(c\) around the associated error's location using a filter \(W_{j}\) that "slides" across the circuit to predict the rate of \(G_{j}\) versus time \(i\). Stacked more formally, we predict \(_{ij}\) using a multilayer perceptron \(_{j}\) whereby \(_{j}(W_{j}[I(c),i])=_{ij}\) and \(W_{j}[I(c),i]\) is a snippet of \(I(c)\) whose temporal origin is \(i\) (see Fig. 0(e)). The shape of each filter \(W_{j}\) is a hyperparameter of our networks and it can be designed to reflect general physical principles, the known physics of a particular quantum computing system, and/or optimized with hyperparameter tuning. The particular neural networks we present later herein use filters \(W_{j}(I(c),i)\) that snip out only layer \(i\) and discard the parts of the layer that act on qubits more than \(l\) steps away from \(Q(G_{j})\) in the quantum computer's connectivity graph (e.g., the filter shown in Fig. 0(e) corresponds to the error shown in Fig. 0(c) and \(l=1\)). This neural network structure has close connections to graph convolution layers [Kipf and Welling, 2016], as well as CNNs. We choose this structure as it can model spatially localized crosstalk errors, which are a ubiquitous but hard-to-model class of errors in quantum computers [Sarovar et al., 2020].

The network \(\) must also predict the rates of errors that occur during measurements (unless the qpa-NN will only ever predict \(F(c)\) not \((c)\)), but these are typically independent of the rates of gate errors (which are predicted by the \(_{j}\)). So we do not use the \(_{j}\) and their convolutional filters \(W_{j}\) to make predictions for \(\). Instead we use separate but structurally equivalent networks \(_{j}^{}\) with corresponding filters \(W^{}_{j}\) that take \(M(c)\) as input and implement only spatial filtering. That is, \(W^{}_{j}\) simply discards rows from \(M(c)\), as, unlike \(I(c)\), \(M(c)\) has no temporal dimension. The \(W^{}_{j}\) are hyperparameters of our networks allowing us to separately adjust the shape of each \(W^{}_{j}\) to reflect the known physics of errors induced by measuring qubits. In our demonstrations, our \(W^{}_{j}\) filters have the same structure as the \(W_{j}\) filters but with an independent \(l^{}\) steps parameter (large \(l^{}\) enables modelling many-qubit measurement crosstalk).

### Processing predicted error rates to predict capabilities

We process \(\)'s output to predict \((c)\) or \(F(c)\) using a function \(f\) with no learnable parameters. This turns \(\)'s output into the two quantities of interest, and it also makes training \(\) feasible. We cannot easily train \(\) in isolation because the error matrix \(\) predicted by \(\) is not a directly observable quantity. Generating the data needed to train \(\) directly would require extraordinarily expensive quantum process tomography (Nielsen et al., 2021), which is infeasible except for very small \(n\). In contrast, both \((c)\) and \(F(c)\) can be efficiently estimated (see Section 2) for a given circuit \(c\).

The function \(f\) computes an approximation to the value for \((c)\) or \(F(c)\) predicted by \(\) and \(\). The matrix \(\) encodes the prediction that \(c\)'s imperfect action is

\[}(c)=_{d}()(L_{d}) _{1}()(L_{1}), \]

where the \(L_{i}\) are the \(d\) layers of \(c\) (see Section 2) and \(_{i}()=(_{j=1}^{k}_{ij}G_{j})\), i.e., \(_{i}()\) is an error channel parameterized by the \(i^{}\) column of \(\). Equation (6) implies an exact prediction for \((c)\) or \(F(c)\) (e.g., Eq. (3)], but exactly computing that prediction involves explicitly creating and multiplying together each of the \(4^{n} 4^{n}\) matrices in Eq. (6). This is infeasible, except for very small \(n\). Instead our \(f\) computes an efficient approximation to this prediction.

Our function \(f\)'s action is most easily described by embedding \(\) into the space of all possible H and S errors \(_{n}\), resulting in a \(d(2^{2n+1}-2)\) matrix \(_{e}\) whose columns are \(k\)-sparse. However, we never construct these exponentially large matrices. Consider pulling each error channel to the end of the circuit, giving \(}(c)=^{}_{d}(^{}_{e}) ^{}_{1}(^{}_{e})(c)\) where \(^{}_{d}(^{}_{e})=(_{j=1}^{2^{n+1}-2}[ ^{}_{e}]_{ij}G_{j})\). Because \(c\) contains only Clifford gates and Clifford unitaries preserve the Pauli group (Aaronson and Gottesman, 2004), \(^{}_{e}\) has columns that are just \(c\)-dependent signed permutations of \(_{e}\)'s columns. The signed permutations required can be efficiently computed in advance (i.e., as an input encoding step) using an efficient representation of Clifford unitaries (Gidney, 2021). Furthermore, these permutations can be efficiently represented in two \(d k\) matrices: a _sign matrix_\(S(c)\) containing \( 1\) signs to be element-wise multiplied with \(\) and a _permutation indices matrix_\(P(c)\) containing integers between 1 and \(2^{2n+1}-2\), where \(P_{ij}\) specifies what error \(G_{j}\) becomes when pulled through the \(d-i\) circuit layers after layer \(i\).

We now have a representation of \(\)'s prediction for the circuit \(c\)'s error map \((c)\) as a sequence of error maps \(^{}_{d}(^{}_{e})^{}_{1}( ^{}_{e})\), and we need to predict \((c)\) or \(F(c)\). We can do so if we can compute \(\)'s prediction for the S and H error rates in \((c)\), as we can then apply Eq. (4) or Eq. (5). To achieve this, we combine the \(^{}_{i}()\) into a single error map using a first-order Baker-Campbell-Hausdorff (BCH) expansion. Using our embedded representation, this means simply approximating \((c)\) as \((c)(_{j}v_{j}G^{}_{j})\) where \(v_{j}=_{i=1}^{d}[^{}_{e}]_{ij}\), i.e., we sum over the rows of \(^{}_{e}\). To predict \(F(c)\) we then simply apply Eq. (4) (meaning summing up \(v_{j}\) with those elements that correspond to Hamiltonian errors squared). Because measurement errors impact \((c)\), to predict \((c)\) we again apply the BCH expansion to combine in the predicted measurement error map \((_{j=1}^{l}m_{j}G_{j})\) and then apply Eq. (5). The efficient representation of the overall action of \(f\) is illustrated in Fig. 1 (the addition of the measurement error map is not shown).

## 4 Datasets

### Experimental 5-qubit data

We used the 5-qubit datasets from Hothern et al. (2024) for our experimental demonstrations. Each of these datasets \(D=\{(c,(c)})\}\) was gathered by running random and periodic mirror circuits(two types of definite-outcome circuits) on 5-qubit IBM Q computers (ibmq_london, ibmq_essex, ibmq_burlington, ibmq_vigo, ibmq_ourense and ibmq_yorktown), and estimating the PST of each circuit using Eq. (2). Each circuit was run between \(1024\) and \(4096\) times, with the exact number depending upon how many times the circuit sampling process generated the circuit (some short, \(1\)-qubit circuits were generated multiple times). The random and periodic mirror circuits contained between \(1\) and \(5\) active qubits--called the circuit's _width_--and ranged in depth from \(3\) to \(515\) layers (alt. \(259\) layers for the ibmq_yorktown dataset).

As we focus on high-PST circuits, we removed all circuits with a PST less than \(85\%\) from each dataset, leaving between \(864\) (ibmq_burlington) and \(1369\) (ibmq_yorktown) circuits in each dataset. The remaining circuits were partitioned into training, validation, and test sets by their original assignment in Hothern et al. (2024b). This setup enables a direct comparison between our apa-NNs and the CNNs trained in Hothern et al. (2024b). Training set sizes ranged from \(682\) circuits on ibmq_burlington to \(1097\) circuits on ibmq_yorktown, with an approximate training, validation, testing split of \(80\%\), \(10\%\), and \(10\%\), respectively.

### Simulated 4-qubit data

For our 4-qubit simulations, we generated 5 datasets of \(5000\) high-fidelity (\(F(c)>85\%\)) random circuits, for a hypothetical 4-qubit processor with a "ring" geometry (i.e., like that in Fig. 0(b)). The circuits ranged in width (\(w\)) from \(1\) to \(4\) qubits, and in depth from \(1\) to \(180\) circuit layers. We designed each circuit for a randomly chosen subset of \(w\) qubits. Each circuit layer was created by _i.i.d._ sampling from all possible circuit layers on the \(w\) active qubits. We used a gate set containing two-qubit \(\) gates and 7 different single-qubit gates (specifically \(\{X(/2),Y(/2),X(3/2),Y(3/2),X(,Y(),Z()\}\) where \(P()\) denotes a rotation around the \(P\) axis of the Bloch sphere by \(\)). See Appendix C for additional details.

All circuits were simulated under the same error model, consisting of local coherent (i.e., H) errors, to exactly compute each \(c\)'s \(F(c)\) [Fig. 3 shows a histogram of \(F(c)\)]. After removing duplicate circuits, the resulting datasets \(D=\{(c,F(c))\}\) were partitioned into training, validation, and testing subsets, with a partition of \(56.25\%\), \(18.75\%\), and \(25\%\), respectively. The parameters of the error model were randomly selected: each gate was assigned a small error strength, which was then distributed randomly across all possible (local) one- or two-qubit coherent errors, for the one- and two-qubit gates, respectively. We chose a model with only coherent errors as these errors are ubiquitous, they are hard to model accurately and efficiently, and we conjecture that apa-NNs can model them.

We also generated \(5\) datasets of \(750\) random mirror circuits on the same hypothetical 4-qubit quantum computer. Again, the random mirror circuits varied in width from \(1\) to \(4\) qubits, and were designed to be run on a randomly selected subset of \(w\) qubits. However, instead of _i.i.d._ sampling of each circuit layer, each circuit was randomly sampled from the class of random mirror circuits on the \(w\) qubits. The depth of the mirror circuits ranged from \(8\) to \(174\) layers. Because we generated the mirror circuit datasets to evaluate how well apa-NNs and CNNs generalize to out-of-distribution circuits, they were used exclusively as testing sets. To ensure that no training was performed on mirror circuits, we removed any mirror circuits that appeared in the random circuit sets (in actuality, there were no duplicates).

### Simulated 100-qubit data

For our 100-qubit simulation, we generated a single dataset of \(5000\) high-fidelity (\(F(c)>91\%\)) random circuits, for a hypothetical 100-qubit quantum computer with a "ring" geometry. All of the circuits had a width of 100 qubits, and ranged in depth from 1 to 22 circuit layers. We sampled circuit layers using the same process and gate set as in the 4-qubit simulations.

We simulated every circuit using the same error model, consisting of local, weight-1 S and H errors. As before, the parameters of the error model were randomly selected and the data were partitioned into training, validation, and testing subsets according to a \(56.25\%\), \(18.75\%\), \(25\%\) split.

Unlike in our 4-qubit simulations, we did not compute \(F(c)\) exactly as doing so for a 100-qubit circuit is infeasible in the presence of coherent errors. Instead, we used a first-order simulation method to approximate \(F(c)\). In this method, \(F(c)\) is computed by assigning each gate its own error vector based on the error model, adding up the error vectors layer-wise to compute an error vector for each circuit layer, and then computing \(F(c)\) as in the second part of a qpa-NN [Figure 1(f)]. See Appendix C.4 for more details.

### Encoding schemes

We used two different encoding schemes for converting each circuit \(c\) into a tensor. For the CNNs on experimental data, we used the same encoding scheme as Hothem et al. (2024), as we used their data and networks. For all qpa-NNs, and the CNNs on simulated data, we used the following scheme. As outlined in Section 3, each width-\(w\) circuit \(c\) is represented by a three-dimensional tensor \(I(c)\{0,1\}^{n d(c) n_{ch}}\) describing the gates in \(c\) and a matrix \(M(C)\{0,1\}^{2 w}\) describing the measurement of the qubits. The \(ij\)-th entry of \(I(c)\),

\[I_{ij}(c)=(I_{ij1}(c),,I_{ijn_{ch}}(c)), \]

is a one-hot encoded vector of what happens to qubit \(i\) in layer \(j\). For the hypothetical 4-qubit ring processor, \(n_{ch}=11\): one channel for each single-qubit gate and four channels for the CNOT gates. There are four CNOT channels to specify if the qubit \(i\) is the target or control qubit and if the interacting qubit is to the left or right of qubit \(i\). We used an additional 4 or 8 CNOT channels for the experimental data, depending on the quantum computer's geometry. The first row in \(M(c)\) is the bitstring specifying which qubits are measured at the end of \(c\). When \(c\) is a definite-outcome circuit, the second row is its target bit string, i.e., the sole bit string in the support of \(c\)'s outcome distribution when it is executed without error [i.e., P\((c)\)]. Both \(I(c)\) and \(M(c)\) are zero-padded to ensure a consistent tensor shape across a dataset.

Additionally, each circuit \(c\) is accompanied by a permutation matrix \(P(c)^{n k}\) and sign matrix \(S(c)\{ 1\}^{n k}\). The \(ij\)-entry of \(P(c)\) specifies which error the \(j\)-th tracked error occurring after the \(i\)-th layer is transformed into at the end of the circuit. The \(ij\)-th entry of \(S(c)\) specifies the sign of that error.

## 5 5-qubit experiments

We now present the results from our head-to-head comparison between the qpa-NNs and the CNNs on the 5-qubit datasets used in Hothem et al. (2024). Figure 2 shows the mean absolute error (MAE) achieved by the CNNs (\(+\)) and the qpa-NNs (\(\)) on each of the datasets. For all datasets, MAE is lower for the qpa-NNs than the CNNs, with an average reduction of \(50.4\%\) (\(_{x}=16.7\%\), i.e., the standard deviation of percent-drop in MAE). The Bayes factor \(K\) is between \(K=10^{30}\) and \(K=10^{383}\) (here, \(K\) is the ratio of the likelihood of the qpa-NN to the likelihood of the CNN given the test data). This is overwhelming evidence that the qpa-NN is a better model (\(K 10^{2}\) is typically

Figure 2: **Prediction accuracy on real quantum computers.****(a)** The mean absolute error of our qpa-NNs (\(\)), the CNNs from Hothem et al. (2024) (o-CNN, \(+\)), and fine-tuned CNNs (ft-CNN, \(\)) on the test data. **(b)** The predictions of the three models for ibmq_vigo on the test data, and **(c)** the distribution of each model’s absolute error on the test data, including the 50\({}^{}\), 75\({}^{}\), 95\({}^{}\) and 100\({}^{}\) percentiles (lines) and the means (points).

considered decisive). These results strongly suggest that the extra infrastructure in the qpa-NNs is making a difference.

The improved performance of the qpa-NNs is not because of an increase in model size. For example, the ibmq_london CNN contains \(6,649,531\) trainable parameters compared to the \(1,218,348\) trainable parameters in the qpa-NN. Moreover, CNNs of similar or larger sizes than the qpa-NNs were included in the hyperparameter optimization space of the CNNs [Hothem et al., 2024b].

Nonetheless, comparing the qpa-NNs to the CNNs is somewhat unfair as the CNNs were trained on out-of-distribution circuits--they were trained on the entire training dataset from Hothem et al. [2024b] which also contains low-PST circuits. For a fairer comparison, we fine-tuned each CNN (Fig. 2, \(\)) on the same high-PST training set used to train the qpa-NNs. Fine-tuning typically increased the CNNs' performances (mean \(25.1\%\) improvement, \(_{x}=22.3\%\)). However, the qpa-NNs achieve a MAE that is lower than the fine-tuned CNNs by \(32.2\%\) on average (\(_{x}=17.3\%\)) and outperform the fine-tuned CNNs on all six datasets. \(K\) is between \(10^{28}\) and \(10^{238}\), which is overwhelming evidence that the qpa-NNs are better models than the fine-tuned CNNs.

## 6 4-qubit simulations

One reason why the extra infrastructure in our qpa-NNs may be necessary is that off-the-shelf networks struggle with modeling coherent errors [Hothem et al., 2024b]. To test our hypothesis, we trained a qpa-NN to predict the fidelity \(F(c)\) of random circuits executed on a hypothetical 4-qubit quantum computer experiencing purely coherent errors. We compared this qpa-NN to a hyperparameter-tuned CNN trained on the same data. Figure 3 shows the results from one representative dataset.

The qpa-NNs again significantly outperform the CNNs. Across the five datasets, the qpa-NNs' averaged a \(52.4\%\) reduction in MAE (\(_{x}=3.00\%\)) on the test data. We also see a significant improvement in the mean Pearson correlation coefficient, \(_{}=.968\) vs. \(_{}=.749\).

We also found that qpa-NNs trained on random circuits are modest predictors of the infidelity of random mirror circuits, which are a different family of circuits. This is an example of out-of-distribution generalization. Random mirror circuits differ in a variety of ways from the random circuits on which the qpa-NNs were trained, including both the presence of idle gates (which are noiseless in our simulations) and a motion-reversal structure in the circuits that causes the addition or cancellation of errors that are far apart in time. The qpa-NNs achieve an average MAE of \(.72\%\) on the random mirror circuits (\(_{x}=.046\%\)). Although this is a \(3.2\) increase in MAE over the in-distribution test data, the strong linear relation between the network's predictions and the ground truth (\(=.912\), \(_{x}=.009\)) strongly suggests that the qpa-NNs are learning information relevant to random mirror circuits.

Figure 3: **Demonstrating our qpa-NNs’ accuracy for hard-to-model coherent errors and at scale.****(a)** Scatter plot of the prediction errors on test data of a qpa-NN (\(\)) and CNN (\(\)) trained to predict the fidelity \(F(c)\) of random circuits run on a hypothetical 4-qubit quantum computer. The qpa-NN significantly outperforms the CNN. The top subplot contains a histogram (green bars) of the ground-truth fidelities. **(b)** Prediction errors on out-of-distribution test data, from random mirror circuits. The qpa-NN achieves modest prediction accuracy on this out-of-distribution task, suggesting that the qpa-NNs are accurately learning error rates. **(c)** Prediction errors on the 100-qubit test data, demonstrating that our qpa-NN approach can accurately predict \(F(c)\) for circuits run on large-scale quantum computers.

100-qubit simulation

Quantum-physics-aware neural networks scale just as well as CNNs, despite their extra infrastructure. To demonstrate their scalability, we trained a qpa-NN to predict the fidelity \(F(c)\) of random circuits executed on a hypothetical 100-qubit quantum computer experiencing a mix of stochastic and coherent errors. To our knowledge, this is the first creation of a capability model of any kind, for a 100+ qubit quantum computer. Figure 3(c) shows the results from our demonstration.

The qpa-NN achieved a MAE of \(0.097\%\). While the underlying noise model was quite simple, this result shows that it is technically feasible to construct qpa-NN capability models for today's moderate-scale quantum computers and for tomorrow's early fault-tolerant quantum computers.

## 8 Discussion

### Limitations

Our results are a significant improvement over the state of the art, but our approach does have several limitations:

1. As presently conceived, our approach assumes that the modelled quantum computer's error rates are invariant under time translations, which is a kind of Markovianity assumption (although it is weaker than the typical Markovian assumption used in conventional quantum computer models (Nielsen et al., 2021)). However, non-Markovian noise exists in quantum computers (White et al., 2020). In the future, we plan to address this issue by adding temporal information into our approach, perhaps with a temporal or positional encoding (Vaswani et al., 2017).
2. Our approach only considers two error classes (H and S errors). Other Markovian error classes, like amplitude damping, exist, but their error rates \(\) contribute to PST and fidelity at order \((^{3})\)(Madzik et al., 2022). Our approach can be easily extended to include those errors, if necessary, by learning their rates with \(\) and updating \(f\) to account for their presence.
3. Our current approach works for Clifford circuits, which includes arguably the most important kinds of circuits (e.g., quantum error correction circuits) but not all interesting circuits. This is because our method for efficiently propagating errors through circuits (implemented by \(f\) together with the \(S\) and \(P\) matrices) leverages the elegant mathematics of Clifford circuits. Our approach can be easily extended to generic few-qubit quantum circuits (\( 10\) qubits), but to obtain the efficiency needed for large \(n\) with general circuits we will need to develop approximate methods for propagating errors through those circuits.

### Conclusion

In this paper, we presented a new quantum-physics-aware neural network architecture for modelling a quantum computer's capability that significantly improves upon the state of the art. The new architecture concatenates two parts: (i) a neural network with structural similarities to GNNs that uses gate information and a quantum computer's connectivity graph to predict the rates of errors in each of a circuit's layers, and (ii) a non-trainable function that turns the predicted error rates into a capability prediction. By imbuing these networks with knowledge about how errors occur and combine within a circuit, we are able to outperform state-of-the-art CNN-based capability models by \( 50\%\) on both experimental data and simulated data. We also provided evidence that our quantum-physics-aware networks are learning the true physical error rates, as they exhibit modest prediction accuracy when predicting the fidelity of out-of-distribution quantum circuits, which would enable our networks to also be used to diagnose the error processes occurring in a particular quantum computer (an important task known as characterization or tomography (Nielsen et al., 2021)).

Understanding which quantum circuits a quantum computer can run, and how well it can run them, is an important yet challenging component of understanding a quantum computer's power. Given the complexity of the problem, neural networks are likely to play a large role in its solution. As our results demonstrate, our new physics-aware network architecture could play a critical role in building fast and reliable neural network-based capability models.