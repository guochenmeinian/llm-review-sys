# Expectation Alignment: Handling Reward Misspecification in the Presence of Expectation Mismatch

Malek Mechergui, Sarath Sreedharan

Colorado State University

Fort Collins, 80523

{Malek.Mechergui, Sarath.Sreedharan}@colostate.edu

###### Abstract

Detecting and handling misspecified objectives, such as reward functions, has been widely recognized as one of the central challenges within the domain of Artificial Intelligence (AI) safety research. However, even with the recognition of the importance of this problem, we are unaware of any works that attempt to provide a clear definition for what constitutes (a) misspecified objectives and (b) successfully resolving such misspecifications. In this work, we use the theory of mind, i.e., the human user's beliefs about the AI agent, as a basis to develop a formal explanatory framework, called Expectation Alignment (_EAL_), to understand the objective misspecification and its causes. Our _EAL_ framework not only acts as an explanatory framework for existing works but also provides us with concrete insights into the limitations of existing methods to handle reward misspecification and novel solution strategies. We use these insights to propose a new interactive algorithm that uses the specified reward to infer potential user expectations about the system behavior. We show how one can efficiently implement this algorithm by mapping the inference problem into linear programs. We evaluate our method on a set of standard Markov Decision Process (MDP) benchmarks.

## 1 Introduction

Given the accelerating pace of advancement within AI, creating agents that can detect and handle incorrectly specified objectives has become an evermore pressing problem . This has resulted in the development of several methods for addressing, among other forms of objective functions, misspecified rewards (cf. Hadfield-Menell et al. ). Unfortunately, these works operate under an implicit definition of what constitutes an incorrectly specified reward function. The few works that have tried to formalize how an agent could satisfy human objectives do so by avoiding the objective specification step altogether .

Our primary objective with this paper is to start with an explicit formalization of what constitutes a misspecified reward function and the potential reasons why the user may have provided one in the first place. However, our motivation for providing such a formalization goes beyond just the pedagogical. We see that such a formalization provides us with multiple insights of practical importance.

Our formulation will follow the intuition set by recent work (cf. ) that looks at reward functions as a means of specifying a task as opposed to being an end to itself. As such, the user starts with a target outcome or behavior or a set of outcomes or behaviors 1 that they want the agent to achieve. The reward function proposed by the user is one whose maximization, they believe,will result in a policy that will achieve the desired outcome. Figure 1 shows how users derive their reward specifications. As presented, their reward function will be informed by their beliefs about the agent and its capabilities (i.e., the user's theory of mind  about the agent) and their ability to reason about how the agent will act given a specific reward function.

A given reward function is said to be misspecified when the policy identified by the agent for the reward function doesn't satisfy the user's expectations. As discussed, the reasons for such misspecification could include users' incorrect beliefs about the agent and their limited inferential capabilities . To see how such misspecification might occur, consider a simple planning problem that can be captured by a Markov Decision Process (MDP) with only three states. Consider a problem where an agent can only perform two possible actions. These correspond to pressing two different switches. Each switch takes the agent to one of the two possible end states where the agent can no longer act. One corresponds to a safe end state, while the other corresponds to a safe end state, while the other corresponds to the safe end state, as such, identifies a reward function that they believe will result in such a state. If the supervisor is confused about which button leads to which state, they may end up coming up with a reward function that will result in the exact opposite outcome. Please note that the problem here isn't that the agent can't achieve the desired outcome but rather that optimizing for the specified reward will not result in the desired outcome. In fact, given the human's confusion about the buttons, no reward function exists that could lead the agent to the desired outcome, which the human will agree is a correct reward function for the task.

We will formalize this intuition about reward specification and, by extension, that of misspecification under the more general framework of _Expectation Alignment_ (_EAL_). _EAL_ framework will allow us to capture scenarios, like the one discussed above, where the human cannot come up with a reward that satisfies the expectation in both their and the agent's models. This invalidates all approaches that try to identify a 'true' human reward function, which is then passed onto the agent. Secondly, we will see how we can use _EAL_ to develop novel algorithms for handling reward misspecification that explicitly leverages potential causes of misalignment to come up with more effective queries to identify the human's original intent. To summarize, the contributions of the paper are:

* namely, _Expectation Alignment_.
* We develop a novel query-based algorithm to solve a specific instance of _EAL_ problems.
* We empirically demonstrate how the method compares against baseline methods for handling reward uncertainty in benchmark domains.

In the related work section (Section 5), we will also show how existing works on handling objective misspecification relate to our proposed framework. Since it will leverage the specifics of our proposed framework, we will look at the related work after defining our basic framework.

## 2 Background

We will primarily focus on problems that can be expressed as a Markov Decision Process or an MDP . An MDP is usually represented by a tuple of the form \(= S,A,T,R,,s_{0}\)2. Under this notation scheme, \(S\) captures the set of states, \(A\) the set of actions, \(T:S A S\) is the transition function such that \(T(s,a,s^{})\) captures the probability of an action \(a\) causing the state to transition from \(s\) to \(s^{}\), \(R:S A\) is the reward function, \([0,1)\) the discount factor and finally \(s_{0} S\) is the initial state (the agent is expected to start from \(s_{0}\)). Since we are focusing on

Figure 1: A diagrammatic overview of how specifying a reward function plays a role in whether or not their expectations are met.

problems related to reward specification, we will separate out all non-reward components of an MDP model, refer to it as the domain, and denote it as \(= S,A,T,,s_{0}\).

The objective of solving an MDP is to find a policy (a function of the form \(:S A\)) that maximizes the expected sum of discounted rewards (captured by a value function \(V:S\)). A policy is considered optimal (optimal policies will be denoted with the superscript '\(*\)', for example, \(^{*}\)') if it results in the highest value (referred to as the optimal value \(V^{*}\)). It is worth noting that an optimal policy is not unique, and for a given task, there may be multiple optimal policies with the same value. We will denote the set of optimal policies associated with a model \(\) as \(^{*}_{}\).

In this paper, we will be heavily utilizing the notion of discounted occupancy frequency or just occupancy frequency, \(x^{}:S A\)(Poupart, 2005), associated with a policy \(\). There are multiple ways to interpret occupancy frequency, but one of the most common ones is to think of them as the steady state probability associated with a policy \(\). In particular, the occupation frequency \(x^{}(s,a)\) captures the frequency with which an action \(a\) would be executed in a state \(s\) under the policy \(\). Sometimes, we will also need to identify the frequency with which a policy \(\) would visit a state \(s\). We can obtain this frequency by summing over the values of \(x^{}(s,a)\) for all actions, i.e., \(x^{}(s)=_{a}x^{}(s,a)\). It is possible to reformulate the value obtained under a policy in terms of its occupancy frequency and rewards. One can also identify the optimal value and, thereby, the optimal policy by solving the following linear program (LP) expressed using occupancy frequency variables:

\[_{x}_{s,a}x(s,a) r(s,a)\] (1) \[ s S,_{a}x(s,a)=(s,s_{0})+ _{s^{},a^{}}x(s^{},a^{}) T(s^{ },a^{},s)\]

Here, \(x\) is the set of variables that captures the occupancy frequency possible under a policy, and \((s,s_{0})\) is an indicator function that returns \(1\) when \(s=s_{0}\) and zero otherwise.

In this paper, we will be looking at settings where the human's (i.e. the user who is coming up with the reward function) understanding of the task may differ from that of the robot 3. As such, the information contained within the domain used by the robot might differ from the beliefs of the human. We will use the notation \(^{R}= S,A^{R},T^{R},^{R},s_{0}\) to denote the domain used by the robot, while \(^{H}= S,A^{H},T^{H},^{H},s_{0}\) is a representation of the human beliefs, i.e., the theory of mind they ascribe to the robot. Note that under this notation scheme, we assume that the two models share the same state space and starting state. This was purely done to simplify the discussion. Our basic formulation and the specific instantiation hold as long as a surjective mapping exists from robot to human states. This would usually be the case where the human model is, in fact, some form of abstraction of the true robot model.

## 3 Expectation Alignment Framework

In this section, we will first develop a framework to understand how humans go from expectations to reward specifications, which we will use to define reward misspecification. With the basic model in place, we can effectively invert it to develop methods to address such misspecification.

To build such a forward model, we need a formal method to represent behavioral expectations in the context of MDPs. While several promising mathematical formalisms could be adopted to represent a human's behavioral expectations, we will focus on using the notion of occupancy frequency. There are multiple reasons for this choice. For one, this is a natural generalization of the notions of reachability. Psychological evidence supports that people use the notion of goals in decision-making (Simon, 1977). While goals relate to the idea of reaching desirable end states, the notion of occupancy frequency allows us to extend it to intermediate ones. It even allows us to express probabilistic notions of reachability. Secondly, occupancy frequencies present a very general notion of problems that can be expressed as MDPs. As discussed in Section 2, the value of any given policy can be represented in terms of their occupancy frequency (Poupart, 2005). Finally, the popular use cases in the space of reward misspecification can be captured using occupancy frequency (See Section 5).

So, we start by representing the set of human expectations as \(^{H}\), which provides a set of states and any preferences placed on reachability for that state. More formally,

**Definition 1**.: _Given human's understanding of the task domain \(^{H}= S,A^{H},T^{H},s_{0},^{H}\), the **human expectation set** is denoted as \(^{H}\), where each element \(e\) in the set is given by the tuple of the form \(e= S_{e},,k\), where \(S_{e} S\), \(\{<,>,,,=\}\) is a relational operator and \(k\). \(\) and \(k\) places limit on the cumulative occupancy frequency over the set of states \(S_{e}\)._

For cases where the human wants the robot to completely avoid a particular state \(s\), there would be a corresponding expectation element \(\{s\},=,0\); on the other hand, if the human wants the robot to visit certain states, then there will be expectation elements that try to enforce high occupancy frequencies for those states. In the simplest settings, humans might convert such expectations to rewards by setting low or even negative rewards to the states that need to be avoided and high rewards to the states they want the robot to visit. One question that is worth considering is why humans don't just communicate this expectation set. Unfortunately, communicating the entire set may be relatively inefficient compared to coming up with a reward function (for example, one might be able to compactly represent the reward function as a piece of code, while this set may not). This could also be the case if the number of states is very high or even infinite, but even in these settings, the reward function could still be specified in terms of features. Secondly, even if they choose to specify desirable or undesirable states, the expectation set may contain states that the human incorrectly thinks are impossible (for example, the robot jumping onto the table) and may choose to ignore them. This is also related to the problem of unspecified side-effects (Zhang et al., 2018).

It is worth noting that Definition 1 merely provides a mathematical formulation that is general enough to capture human expectations. We don't expect people to maintain an explicit set of numeric constraints on occupancy frequencies in their minds. Regardless of what form their actual expectation takes, as long as it can be expressed as some ordering on how the states should be visited, it can be captured using sets of the form provided in the above definition. One could, in theory, also capture non-Markovian expectations using such formalisms. For example, there may be cases where the user might want the robot to visit a set of states in a specific sequence. We can capture such cases by creating augmented states that can track whether or not the robot visited the states in sequence. This method also allows Markovian reward functions to capture such considerations (cf. (Abel et al., 2021)), which is unsurprising since the expressivity of occupancy frequency parallels that of Markovian reward functions.

We will claim that a policy satisfies an expectation element for a domain if the corresponding occupancy frequency relation holds for the policy in that domain, or more formally:

**Definition 2**.: _A given policy \(\) is said to **satisfy an expectation element**\(e= S_{e},,k\) for a given domain \(\), or more concisely \(e_{}\), if the occupancy frequency for state \(s\) (\(x(s)\)) under policy \(\) as evaluated for \(\) satisfies the specified relation, i.e., \((_{s S_{e}}x^{}(s),k)\) is true._

To relate these expectations to the reward specification process, we first need a way to represent the human decision-making process. For this discussion, we will start with a very abstract model of human decision-making, which we will later ground in Section 4. In particular, we will represent human decision-making using a planning function \(^{H}\), that maps a given model to a (possibly empty) set of policies.

**Definition 3**.: _For a given model \(\), the set of policies that the human thinks will be selected is given by the **planning function**\(^{H}: 2^{}\), where \(\) is the space of possible models and \(\) the space of possible policies._

It is worth noticing that, as with \(^{H}\), \(^{H}\) is a reflection of the human's belief about the robot's model and planning capabilities. In many cases, humans might ascribe a lower level of cognitive capability to the robot. However, for this paper, we will ignore such considerations. With these definitions in place, we can describe how humans would choose an acceptable reward specification. Specifically, the human would think a reward function is sufficient if all policies they believe could be selected under this specification will satisfy their expectations, or more formally

**Definition 4**.: _For a given the human's belief about the task domain \(^{H}\) and an expectation set \(^{H}\), a reward function \(\) is considered **human-sufficient**, if for every policy in \(^{H}(^{H},)\), you have \(e_{^{H}}\), for all \(e^{H}\)._

In this definition, we can already see the outlines of the various sources of misspecification. For one, humans use their understanding of the task and the robot's capabilities (captured by \(^{H}\)) to choose the reward specification. The true robot capabilities or task-level constraints could drastically differ from human beliefs. Next, the policies they anticipated being selected (\(^{H}(^{H},)\)) could bevery different from what might be chosen by the robot. There is also the additional possibility that the human is not even able to correctly evaluate whether an expectation element holds for a given policy, especially given the fact that people are known to be notoriously bad at handling probabilities (Tversky and Kahneman, 1983; Kahneman and Tversky, 1981). In many cases, the expectations might be limited to the occupancy frequencies being forced to be zero (avoid certain states) or taking a non-zero value (should try to visit certain states). Here, the human could ignore the probabilities and reason using a determinized version of the model (Yoon et al., 2008). Even here, there is a possibility that humans could overlook some paths that might cause the expectation element to be unsatisfied.

In general, we will deem a reward function misspecified if at least one robot optimal policy has at least one unsatisfied expectation set element.

**Definition 5**.: _A human-sufficient reward function \(\) is **misspecified** with respect to the robot task domain \(^{R}\) and the human expectation set \(^{H}\), if there exists an \(e^{}^{H}\) and a policy \(\) that is optimal for \(^{R}=^{R},\), such that \(e^{}_{^{R}}\)._

In this setting, it no longer makes sense to talk about a true reward function anymore. As far as the human is concerned, \(\) is the true reward function because it exactly results in the behavior they want in _their_ domain model of the task (\(^{H}\)). One can even show that there may not exist a single reward function that allows the expectation set to be satisfied in both the human and robot models.

**Theorem 1**.: _There may be human and robot domains, \(^{H}\) and \(^{R}\), and an expectation set \(^{H}\), such that one can never come up with a human-sufficient reward function \(\) that is not misspecified with respect to \(^{R}\), even if one allows the human planning function \(^{H}\) to correspond to some subset of optimal policies in the human model._

Proof Sketch.: We can prove this by constructing an example where it holds. Consider the example discussed in the introduction. Here, there is a state that the human wants the robot to visit (i.e., the occupancy frequency \(>0\)), and there is a state that the human wants to avoid(i.e., the occupancy frequency \(=0\)). To achieve the expected behavior in the human model, they have to choose a reward function that will result in an optimal policy where the button they believe will lead to the goal state has to be pressed. However, if the functionality of the switches is reversed in the robot model, none of those rewards will result in a robot policy that will lead to the state that the human wanted to visit. This shows that, for this example, there exists no reward function that will result in a policy that satisfies the expectation set. Thus proving the theorem statement. 

This theorem shows that one can't talk about a single 'true' reward function that holds for both domains. In the example, as far as the human is concerned, the reward they came up with is the true reward since it results in the exact behavior they wanted. In fact, any reward function that results in the robot achieving its goal will be considered incorrect since it will never lead to the expected behavior in their model. This rules out the possibility of using methods like inverse reward design (Hadfield-Menell et al., 2017), which aims to generate a single reward function that applies to both the intended and actual environments. On the other hand, the underlying expectations are directly transferable across the domains. This brings us to what the robot's objective should be, namely, to come up with an expectation-aligned policy, i.e., one that _satisfies human expectations in the robot task domain_, when possible, or recognize when it cannot do so.

**Definition 6**.: _For a human specified reward function \(^{H}\), human expectation set \(^{H}\), and the robot task domain \(^{R}\), a policy \(^{E}\) is said to be **expectation-aligned**, if for all \(e^{H}\), you have \(e_{^{R}}^{E}\), where \(^{R}=^{R},^{H}\)_

The primary challenge to generating expectation-aligned policies is that the expectation set is not directly available. However, unlike previous works in this space, the fact that we have a model that maps the expectations to the reward function means that there is an opportunity to develop a more diverse set of possible solution approaches. For one, we could now look at the possibility of starting by learning the human model (using methods similar to the ones described by Gong and Zhang (2020) and Reddy et al. (2018)) and use that information to produce estimates of \(^{H}\) from the reward specification. Along the same lines, we could leverage psychologically feasible models as a stand-in for \(^{H}\) to further improve the accuracy of our estimates (possible candidates include noisy-rational model (Jeon et al., 2020)). In the next section, we show how we can develop effective algorithms for generating expectation-aligned policies by looking at an instance where the expectation set is limited to a certain form.

Identifying Expectation-Aligned Policies

We will ground the proposed framework in a use case with a relatively simple planning function \(\) that returns the set of optimal policies and where the expectation set takes a particular form. Specifically, we assume that there is an unknown set of states in the expectation set that cannot be visited, i.e., there exists some \(e^{H}\), takes the form \(\{s\},=,0\) and some states that they would like the robot to visit (i.e., \(e\) of the form \(\{s\}_{>},0\)). We will call the states that are part of the first set of expectations the forbidden state set \(^{}\) and the second one the goal set \(^{}\). The expectation set described here corresponds to the widely studied reward misspecification problem type, where the robot needs to avoid negative side effects while achieving the goal (cf. Saisubramanian et al. (2022)).

Let \(^{H}\) be the reward function specified by the user. Since there are already works on learning human beliefs about the domain, we will assume access to \(^{H}\). The first observation we can make from the given setting is the fact that any state that can be visited by a policy that is optimal in \(^{H}\) cannot be part of the forbidden state set (as evaluated in \(^{H}\)).

**Proposition 1**.: _There exists no state \(s^{}\) and policy \(^{*}_{^{H}}\), such that \(x^{}(s)>0\) is true._

The proof sketch for the proposition is provided in Appendix 8.1. Next, we can also see that the states part of \(^{}\) must be reachable under every optimal policy.

**Proposition 2**.: _For every state \(s^{}\) and policy \(^{*}_{^{H}}\), \(x^{}(s)>0\) must always be true._

The proof sketch for the above proposition is again provided in Appendix 8.1. One might be tempted to generate the sets \(^{}\) and \(^{}\) by looking at all states with low and high reward values, respectively. However, there are multiple problems with such a naive approach. Firstly, it is well-known that people encode solution strategies into the reward function in addition to eventual goals (Booth et al., 2023). This means there may be high-reward value states that are not part of \(^{}\) or relatively low-reward states that are not part of \(^{}\). These may have been added to get the robot to behave in certain ways. Secondly, humans may ignore states they think are guaranteed to be reached or will never be reached.

While we can't exactly calculate \(^{}\) and \(^{}\), we will proceed to show how we can generate supersets of these sets. In particular, we will calculate the set of all states not reachable under any optimal policies (denoted as \(}^{}^{}\)) and the set of all states that are reachable under every optimal policy (\(}^{}^{}\)). We can generate these sets by using modified forms of the LP formulation discussed in Equation 1. To test whether a given state \(s_{i}\) is part of \(}^{}\) or not, we will look at an LP that will try to identify an optimal policy that will visit \(s_{i}\). Specifically, the LP would look like:

\[_{x}_{s,a}x(s,a) r(s,a)+( _{a}x(s_{i},a))\\  s S,_{a}x(s,a)=(s,s_{0})+ _{s^{},a^{}}x(s^{},a^{}) T^{H}( s^{},a^{},s)\\ _{s,a}x(s,a) r(s,a)=V^{*}_{s_{0}}\] (2)

Where \(\) is some positive valued coefficient and \(V^{*}_{s_{0}}\) is the value of the optimal policy in the starting state \(s_{0}\). We can calculate \(V^{*}_{s_{0}}\) by solving Equation 1 on the human model with the specified reward. By adding the new term to the objective, we provide higher objective value to policies that visit the state \(s_{i}\). At the same time, the constraint \(_{a}x(s_{0},a)=V^{*}_{s_{0}}\) ensures we only consider optimal policies. We can now formally state the following property for the above LP formulation.

**Proposition 3**.: _For the LP described in Equation 2, if \(s_{i}^{}\) then for the optimal value \(x^{*}\) identified for the LP, the condition \(_{a}x^{*}(s_{i},a)=0\) must hold._

This follows directly from the definitions of the expectation set and the planning function (a longer sketch is provided in Appendix 8.1). We will again employ a variation of the LP formulation to identify \(}^{}\). Specifically, to test if a given state \(s_{i}\) is part of \(}^{}\) or not, we test the solvability of an LP that tries to create a policy optimal value that doesn't visit \(s_{i}\).

\[_{x}_{s,a}x(s,a) r(s,a)\\  s S,_{a}x(s,a)=(s,s_{0})+ _{s^{},a^{}}x(s^{},a^{}) T^{H}( s^{},a^{},s)\\ _{a}x(s_{0},a) r(s,a)=V^{*}_{s_{0}};_{a}x(s_{i},a

**Proposition 4**.: _For the LP described in Equation 3, if \(s_{i}^{}\), then there must exist no solution for the given LP._

Now with both sets \(}^{}\) and \(}^{}\) calculated, the objective of the robot is to generate a policy that covers all of the states in \(}^{}\), while avoiding the states in \(}^{}\). This may not always be possible, so we will need the system to query the user about whether a state \(s}^{}\) is actually part of \(^{}\) (i.e., "do I need to avoid \(s\)?"), and if a state \(s^{}}^{}\) is actually part of \(^{}\) ("do I need to visit \(s^{}\)?"). Based on their response, we can update the sets and try to see if we can now generate a policy that avoids remaining states in the updated set of forbidden states and visits all the states in the updated goal state set. We can identify whether such a policy exists and possible states to query by using the LP:

\[_{x,_{F},_{G}}-1_{d_{G} _{F}}d\] (4) s.t. \[ s S,_{a}x(s,a)=(s,s_{0})+_{s ^{},a^{}}x(s^{},a^{}) T^{R}(s^{},a^{ },s)\] \[ d_{G}_{F},d 0\] \[ s_{i}}^{}_{a}x(s_{ i},a)-d_{i}=0;\;\;\; s_{j}}^{}_{a}x(s_{ j},a)+d_{j} 0\]

As seen in the equation, this formulation requires the introduction of two new sets of bookkeeping variables \(_{F}\) and \(_{G}\), such that \(|_{F}|=|}^{}|\) and \(|_{G}|=|}^{}|\). To simplify notations, we will denote the bookkeeping variable corresponding to a state \(s_{i}}^{}}^{ }\) as \(d_{i}\). The next thing to note is that all occupancy frequency is calculated using the robot model. The general idea here is that we turn the requirement of meeting reachability constraints that are part of \(}^{}\) and \(}^{}\) into soft constraints that can be ignored at a cost. This is done through the manipulation of the bookkeeping variables. This brings us to the first theorem

**Theorem 2**.: _If there exists a policy that satisfies the requirement that no state in \(}^{}\) is visited and all states in \(}^{}\) is visited (therefore satisfied the underlying expectation), then the optimal solution for Equation 4, must assign 0 to all variables in \(_{G}\) and \(_{F}\)._

Proof Sketch.: The reasoning follows directly from how the last two constraints of Equation 4 and the fact setting more \(d\) variables to zero will result in a higher objective value. Thus, the LP would want to minimize assigning positive values to \(d\) variables. It will only give \(d\) a positive value if a forbidden state has a non-zero occupancy frequency or if some potential goal states are unreachable. 

When such solutions are not found, the \(d\) variables that take positive values tell us potential states we can query. After the query if the user says a state shouldn't be visited or should be visited thecorresponding states are removed from \(}^{}\) or \(}^{}\) and moved to the set \(^{}_{*}\) (set of known forbidden states) and \(^{}_{*}\) (set of known goal states). Once these sets become non-empty, we use an updated form of Equation 4, which now has hard constraints of the form.

\[ s_{i}^{}_{*},_{a}x(s_{i},a)=0;\  s_{i} ^{}_{*},_{a}x(s_{i},a) 0\] (5)

Theorem 2 also holds for this new LP. If the user answers that a state is neither a forbidden nor a goal state, then the state is simply removed from the corresponding superset. This process is repeated until a policy is found, if there are no more elements in \(}^{}}^{}\), or if the LP is unsolvable.

Algorithm 1 provides the pseudo-code for the query procedure. The procedure \(CreateLP\) creates an LP of the form described in Equation 4, for the current estimates for \(}^{}\) and \(}^{}\) (along with known ones). The estimates are refined through queries with the user (represented by the procedures \(QueryGoal\) and \(QueryForbidden\)). The algorithm stops if a solution is found that doesn't require any additional queries, if the LP is unsolvable, or if it runs out of states to query.

**Proposition 5**.: _Algorithm 1, is guaranteed to exit in finite steps for all finite state space MDPs._

The above proposition holds since, in the worst case, the algorithm would query about every state in the MDP. Appendix 8.2 provides an extension of the method for a planning function based on a noisy rational model.

## 5 Related Works

Given the ever-increasing capabilities of AI agents, there has been a lot of work that has looked at how to handle partially specified or misspecified objectives. Most work in this area tends to be motivated by the need to ensure safety. This is fueled by the recognition that people tend to be bad at correctly specifying their objectives (Booth et al., 2023; Milli et al., 2017), and optimizing for incorrectly specified objectives could have disastrous results (Hadfield-Menell et al., 2016). One could roughly categorize the work done to develop more general methods to address this problem into two broad categories.

In the first, the reward function is mostly taken to be true, but they assume there is a set of hard constraints, usually partially specified, on what state features could be changed (Weld and Etzioni, 1994), which are usually referred to as side-effects. As such, these works are usually framed in terms of avoiding negative side effects. Many works in this direction focus on the problem of how to identify potential side-effect features (cf. (Zhang et al., 2018; Saisubramanian et al., 2022; Mahmud et al., 2023b)), mostly by querying the users. Recent work has also looked at extending these methods to non-markovian side-effects (Srivastava et al., 2023) and even to possible epistemic side effects (Klassen et al., 2023). In addition to querying, there are also works that look at minimizing potential side effects when coming up with behavior (Klassen et al., 2022b). Previous work includes work in the context of factored planning problems (cf. (Klassen et al., 2022a; Mechergui and Sreedharan, 2023)) and reinforcement learning (Vamplew et al., 2021).

The second category treats the reward function provided by the user as merely constituting a partial specification or an observation of the true reward function held by the user. A canonical example is inverse reward design (Hadfield-Menell et al., 2017), which uses the specified reward function as a proxy to infer the true objective. For a good balance between conservatism and informativeness, Krasheninnikov et al. (2021) proposes an algorithm to infer the reward function from two conflicting reward function sources. Mahmud et al. (2023) use explanations within their framework to verify and improve reward alignment. Pan et al. (2022) proposes anomaly detection to tackle reward hacking due to misspecification. This is also related to the CIRL framework (Hadfield-Menell et al., 2016), which eschews an agent from having its own reward function, but tries to maximize the human reward function. However, the agent estimate of this reward function may also be incorrect or incomplete.

Our expectation-alignment framework allows us to capture the requirements of both of these sets of works. We can set negative side effects or safety constraints by setting the frequency of relevant states to zero. Our framework explicitly captures the fact that the specified reward should not be directly optimized in the true model. In scenarios where the reward function induces the same occupancy frequency in both human and robot models, recovering the human reward function suffices to generate expectation-aligned policies. But, as discussed this is not a general solution strategy.

Another stream of work that is relevant to the current problem is that of imitation learning (Torabi et al., 2019) or learning from demonstration (Argall et al., 2009). Under this regime, the teacher demonstrates a particular course of action to an agent, and it is expected to either imitate or generate behaviors that match the intent behind the demonstration. Apprenticeship learning (Abbeel and Ng, 2004), and other methods that use inverse reinforcement learning (cf. (Arora and Doshi, 2021)) treat the reward function as an unknown entity and use the demonstration and any knowledge about the demonstrator to identify the true reward function. The objective then becomes to identify a policy that maximizes this learned reward function. In some sense, our work inverts this paradigm and starts from a specified reward function and tries to recreate what expectations may have led to this reward function. In addition to all the work on partially specified rewards, our work is also connected to preference/model elicitation (cf. (Boutilier, 2002) and (Chen and Pu, 2004)). Our proposed method can be thought of as a form of preference elicitation, except we focus on learning some specific kinds of preferences. It is also worth noting that there have been some efforts within the inverse-reinforcement learning community to formalize misspecification (cf. (Skalse and Abate, 2023)). However, we see such work being complementary to our effort.

## 6 Evaluation

Our primary goal with the empirical evaluation was to compare our proposed method against two baseline methods selected from the two groups of works described in Section 5. Specifically, we wanted to test:

_"How the method described in Section 4 compared with existing methods in terms of (a) computational efficiency (measured in terms of time-taken), (b) overhead placed on the user (number of queries) and (c) ability to satisfy user-expectations."_

In particular, we selected Minimax-Regret Querying (MMRQ-k) (Zhang et al., 2018) (with query size \(k=2\) and all features set to unknown) and a modified form of the Inverse Reward Design method (Hadfield-Menell et al., 2017). To simplify the setting, we considered an MDP formulation where rewards were associated with just states as opposed to states and actions. For the modified IRD, we avoided the expensive step of calculating posterior distribution over the reward function set and instead directly used \(^{G}\) and \(^{F}\). For the expected reward function, a high positive reward was assigned for \(^{G}\) and a negative one for \(^{F}\). We then try to solve the MDP for this reward function using an LP planner. For the query-based methods, we simply check with the ground truth on whether a state belongs to the forbidden state set or to the goal state set.

**Test Domains.** We tested our method and baseline on five domains. Most of these are standard benchmark tasks taken from the SimpleRL library (Abel, 2019). Since all examples were variations of a basic grid-world domain, we considered four different sizes and five random instantiations of each grid size (obtained by randomizing the initial state, goal state, forbidden states, and location of objects). For each of the tasks, the expectation set consists of reaching the goal state and avoiding some random states in the environment. The human models were generated by modifying the original task slightly. The walkway involves a simple grid world where the robot can use a movable walkway to reach certain states easily, but the human is unaware of it. Obstacles involve the robot navigating to a goal while avoiding obstacles (the human model includes incorrect information about the obstacles). Four rooms (Sutton et al., 1999) involves the robot navigating through connected rooms, but in the human model, the use of certain doors may not be allowed. In Puddle, the robot needs to travel to the goal while avoiding certain terrain types, while the human model may be wrong about the location of various terrain elements. Finally, in maze, the robot needs to navigate a maze, while the human model may have incorrect information about what paths are available.

**Evaluation.** All the baselines were run with a time-bound of 30 minutes per problem. All experiments were run on AlmaLinux 8.9 with 32GB RAM and 16 Intel(R) Xeon(R) 2.60GHz CPUs. We used CPLEX (Bliek10, 2014) as our LP solver (no-cost edition)4. First, _we found that the MMRQ-k method could not solve any of the instances_. This was because the runtime was dominated by the time needed to calculate the exponential number of dominant policies. This shows how computationallyexpensive methods from the first group are. On the other hand, our method is much faster, and even in the largest grids, it took less than five minutes (Table 1) and only required very few queries. Note that the maximum number of queries that could be raised in each case corresponds to the total state space. In each case considered here the number of queries raised was substantially smaller than the state space size. Thus showing the effectiveness of our method compared to existing query methods.

In terms of comparing our method to IRD, the main point of comparison would be a number of violated expectations (after all IRD doesn't support querying). Table 1 shows how, in almost all the cases, IRD generated policies that resulted in expectation violations. On the other hand, our method guarantees policies that will never result in violation of user policies. While IRD is fast, please keep in mind that we avoided the expensive inference process of the original IRD with direct access to \(^{}\) and \(^{}\) (reported times don't include the calculation of these sets).

## 7 Conclusion

This paper introduces a novel paradigm for studying and developing approaches to handle reward misspecification problems. The _expectation-alignment_ framework takes the explicit stand that any reward function a user provides is their attempt to drive the agent to generate behaviors that meet some underlying expectations. We formalize this intuition to explicitly define reward misspecification, and we then use this framework to develop an algorithm to generate behavior in the presence of reward misspecification. We empirically demonstrate how our method provides a significant advantage over similar methods to handle this reward misspecification in standard MDP planning benchmarks.

**Limitations.** One aspect of reward misspecification we haven't discussed here is the one caused by human errors during transcription or communication of the reward function (say, bugs in the reward function code). As previous works have shown [Booth et al., 2023], reward misspecification is quite frequent in practice, even in the absence of such errors. This paper also focuses on settings where the models and planning functions are known upfront. We believe our method sets up a solid formal framework that can be used as a basis to develop future RL methods that could relax these assumptions. The paper also only instantiates a specific form of expectation set. More work needs to be done to identify when different forms of expectation may be appropriate. Not only could different forms of expectation set be more intuitive or natural for different settings, but it might also impact how effectively the user can be queried.

   &  &  \\  & Grid size & Query Count & Time (secs) & No of Violated Expectations & Time (msecs) \\  Walkway & (4,4) & 2.2 \(\) 0.83 & 7.7 \(\) 0.4 & 1.6 \(\) 0.5 & 32.3 \(\) 1.4 \\  & (5,5) & 3.2 \(\) 1.48 & 11.8 \(\) 0.45 & 2.26 \(\) 1.03 & 45 \(\) 1.12 \\  & (9,9) & 4.5 \(\) 4.8 & 60 \(\) 1 & 2.5 \(\) 3 & 215.7 \(\) 3 \\  & (11,11) & 14.25 \(\) 2.91 & 138.57 \(\) 6.32 & 3.6 \(\) 3.9 & 456.05 \(\) 19 \\  Obstacles & (4,4) & 2.4 \(\) 1.34 & 9.6 & 1.4 \(\) 0.58 & 32.4 \(\) 1.27 \\  & (5,5) & 3.5 \(\) 1 & 13 \(\) 0.3 & 2 \(\) 1.12 & 46.13 \(\) 2.42 \\  & (9,9) & 4.5 \(\) 3.1 & 62.8 \(\) 0.9 & 4 \(\) 2.66 & 216.66 \(\) 12.4 \\  & (11,11) & 11.75 \(\) 2.87 & 134.51 \(\) 6.88 & 6 \(\) 3.7 & 450 \(\) 26.6 \\  Four Rooms & (5,5) & 1.4 \(\) 0.55 & 3.1 \(\) 1 & 1.42 \(\) 0.75 & 49 \(\) 1.7 \\  & (7,7) & 1.8 \(\) 0.45 & 12.9 \(\) 0.7 & 1.07 \(\) 0.73 & 111.5 \(\) 3 \\  & (9,9) & 2.75 \(\) 0.5 & 71.2 \(\) 0.8 & 1.35 \(\) 0.74 & 251 \(\) 3.5 \\  & (12,12) & 4.44 \(\) 2.065 & 224.98 \(\) 6.84 & 1 \(\) 0.57 & 728 \(\) 5.22 \\  Puddle & (5,5) & 3 \(\) 2 & 13.2 & 1.13\(\) 0.63 & 49.5 \(\) 1.48 \\  & (7,7) & 5 \(\) 3.46 & 31.7 \(\) 7 & 1.2 \(\) 0.63 & 123.27 \(\) 15 \\  & (9,9) & 3.14 \(\) 2.6 & 421.1 \(\) 1.86 & 0.9 \(\) 0.66 & 275.2 \(\) 5.9 \\  & (11,11) & 2.44 \(\) 1.85 & 132.5 \(\) 7.132 & 1.3 \(\) 0.6 & 566.1 \(\) 7.8 \\  Maze & (3,3) & 1.66 \(\) 0.57 & 5.9 \(\) 0.1 & 1 \(\) 0.87 & 25.1 \(\) 1.51 \\  & (5,5) & 1.66\(\) 0.57 & 13 \(\) 1 & 1.36 \(\) 0.67 & 47.3 \(\) 1.3 \\  & (7,7) & 2.33 \(\) 0.57 & 30 \(\) 0.5 & 1.46 \(\) 0.66 & 106 \(\) 1.6 \\  & (9,9) & 7.5 \(\) 6.27 & 35.34 \(\) 14.18 & 1.42 \(\) 0.51 & 244.29 \(\) 14 \\  

Table 1: For our method, the table reports the number of queries raised and the time taken by our method. For IRD, it shows the number of expectations violated by the generated policy and the time taken. Note that our method is guaranteed not to choose a policy that results in violated expectations.