# Your representations are in the network: composable and parallel adaptation for large scale models

Yonatan Dukler  Alessandro Achille  Hao Yang  Benjamin Bowman  Varsha Vivek

Luca Zancato  Avinash Ravichandran  Charless Fowlkes  Ashwin Swaminathan

&Stefano Soatto

Work conducted while interning at AWS AI Labs.

###### Abstract

We present a framework for transfer learning that efficiently adapts a large base-model by learning lightweight cross-attention modules attached to its intermediate activations. We name our approach InCA (Introspective-Cross-Attention) and show that it can efficiently survey a network's representations and identify strong performing adapter models for a downstream task. During training, InCA enables training numerous adapters efficiently and in parallel, isolated from the frozen base model. On the ViT-L/16 architecture, our experiments show that a single adapter, 1.3% of the full model, is able to reach full fine-tuning accuracy on average across 11 challenging downstream classification tasks. Compared with other forms of parameter-efficient adaptation, the isolated nature of the InCA adaptation is computationally desirable for large-scale models. For instance, we adapt ViT-G/14 (1.8B+ parameters) quickly with 20+ adapters in parallel on a single V100 GPU (76% GPU memory reduction) and exhaustively identify its most useful representations. We further demonstrate how the adapters learned by InCA can be incrementally modified or combined for flexible learning scenarios and our approach achieves state of the art performance on the ImageNet-to-Sketch multi-task benchmark.

## 1 Introduction

Foundation models promise to achieve top performance with minimal adaptation on any downstream task. In the realm of language, the data and the hypothesis spaces are shared, and many tasks can be unified into a homogeneous representation. Visual inference domains, on the other hand, can be highly heterogeneous and possibly antagonistic. For instance, the hypothesis space for pose estimation is geometric, whereas for scene classification it is semantic and even domains that appear homogeneous, such as image classification into the 10 CIFAR classes, can trigger interference in the trained model in the presence of modest perturbations of the image statistics.

Antagonistic domains may interfere within the activations of the latter layers, which cannot be simultaneously minimal and sufficient for all domains. However, information about a dissimilar domain _may be present_ in earlier layers, and certainly in the input data which is trivially sufficientfor any task. Indeed, as opposed to just operating with the final model's representations, the typical approach of addressing domain variability in transfer learning is by applying full fine-tuning of the model on new data. By optimizing all of the model's parameters, each of the model representations can be potentially harnessed as a starting point for the adaptation.

While accurate and robust to a variety of domains, full fine-tuning of large-scale models entails sweeping computational and storage costs. Further, the resultant model can only function for its dedicated task, not allowing for shared computation and parallel execution of potential future tasks. To tackle the problem of efficient, versatile, and modular adaptation we introduce Introspective Cross-Attention (InCA). InCA operates on the base-model by attaching isolated shallow adapter modules utilizing any of the activation maps for downstream tasks.

Since modern architectures are deep and so is the variety of possible downstream tasks, iterating over all the possible candidate activations of a model is prohibitively expensive. Instead in InCA we search for useful model representations exhaustively and in parallel by training numerous isolated adapters attached to different activations. When using InCA, each adapter is light and the base-model is fixed and does not require any backpropagation. This makes InCA computationally efficient in terms of GPU memory which is crucial for scaling to large models. Further, the shallow InCA adapter networks simplify the training dynamics as compared with existing approaches which speeds up training considerably and makes the optimization straightforward and robust (see Appendix C).

In detail, during parallel training of InCA, a set of adapters sharing the same architecture are trained simultaneously and independently on a downstream task. Each adapter accepts an assigned activation and does not feed back to the backbone, leaving the backbone execution unaltered during both training and inference (Fig. 2). At inference, the learned adapters may be combined or a best performing adapter can be selected for downstream prediction. The InCA adapter architecture is simple, consisting of a single cross-attention module followed by a linear classifier for the downstream task. Despite the simplicity of the adapter, we observe that a single top-performing adapter trained for a downstream

Figure 1: **(Left) Top-1 Test Error for fine-grained classification transfer learning tasks evaluated with the ViT-L/16 architecture. InCA performs comparable to full fine-tuning on each challenging dataset. (Right) Max GPU Memory usage during training for different adaptation approaches and model sizes.**

Figure 2: **InCA Adaptation In (B), intermediate activation maps are extracted from a pretrained backbone during forward pass. Each activation map is passed to a lightweight InCA adapter (shown in (C)) or Open-InCA adapter (shown in (D)) depending on the task settings. In (A), we illustrate how multiple adapters are trained in parallel and independently, and during inference can be combined or used in parallel. In (C), (D) we present a schema of the InCA and Open-InCA adapters; see Sec. 3 for details.**

task is capable of achieving strong performance across a variety of different architectures and pre-trainings when tested on a diverse set of visual domains. Because our approach does not modify the execution of the model or any of the pre-trained layers as done in existing parameter-efficient approaches [28; 34; 27], our method can be automatically applied to _any_ architecture without the hassle of re-implementation and architecture specific modifications. In particular, we present results of InCA for ViT , SWIN , and CNN architectures [49; 74] on a suite of fine-grained recognition tasks.

Since the adapters learned in InCA are shallow and small (1.3% of the parameters on ViT-L/16), the strong performance of a single adapter implies that many pre-trained models _already contain_ strong representations for a diverse set of downstream tasks. Instead, previous approaches like linear probing fall short in using the representations, not because the task can not be solved using the existing model, but rather because of not having the right "extraction capacity" of cross-attention; we explore this systematically in Sec. 4 and present a theoretical proof for the advantage of the cross-attention layer in Appendix D. For challenging datasets, using intermediate representations as opposed to only adapting the last layer's representation is key to making InCA a versatile method that closes the gap with full fine-tuning on diverse tasks (See Fig. 1).

A byproduct of the exhaustive approach of InCA adaptation is a signature describing the performance of the internal representations of a network on different downstream tasks. This renders InCA as a powerful tool for understanding the underlying representations of pre-trained models and task space  (See Sec. 5, Appendix B). Curiously, we observe that in certain downstream datasets, different pre-trained models hold similar performance signatures, even when the pre-trained models use different architectures or pre-training augmentations.

The isolated training of the adapters means no backpropagation through the pre-trained model is taking place. This significantly reduces the correlation between adaptation cost and the pre-trained model size, which makes it possible to leverage very large architectures even with modest compute resources. Fig. 1 shows that one V100 GPU can train InCA with 40 adapters using an architecture as big as ViT-G/14. In contrast, existing parameter efficient approaches that backpropagate through the architecture exhaust GPU memory with any model larger than ViT-B/16.

Our contributions are summarized as

* We introduce InCA, a method that enables versatile downstream adaptation by surveying the internal network representations of any frozen model with lightweight cross-attention based modules as an alternative to full fine-tuning. On ViT-L/16, InCA matches full fine-tuning performance and reaches within \(0.7\%\) accuracy for a SWIN-L backbone on average over 11 diverse downstream domains.
* We demonstrate how the modular adapter architecture of InCA enables flexible learning and inference scenarios and present the Open-InCA adapter. With our approach we unlock powerful pre-trained models for reusable and parallel multi-task inference, and class-incremental learning.
* On the efficiency front, InCA scales to massive scale architectures under typical computation budgets while its implementation can be automatically applied to any new model. For example, training ViT-G/14 using InCA results in 76% GPU memory reduction as compared with full fine-tuning. Further, InCA is easy to optimize and highly parameter efficient (See Appendix C).

The rest of the paper is organized as follows: In Sec. 2 we review related work, and in Sec. 3 we present our approach. We empirically evaluate InCA on a wide set of visual recognition tasks in Sec. 4. Lastly we provide analysis of intermediate representation signatures (Sec. 5) followed by discussion (Sec. 6). Additional results and analysis including Open-InCA are presented in the Appendix.

## 2 Related works

**Transfer learning** Transfer learning in deep neural networks aims at endowing existing models with new "downstream" knowledge. The de-facto approach for transfer learning in deep learning modifies an existing pre-trained network by applying full, partial or linear fine-tuning on the model [37; 66; 45]. Depending on the the variability between the source and downstream task domains,different approaches aim at delineating different transfer domains , extending coverage of transfer via suitable pre-trainings such as meta-learning , or by selecting and combining from a set of expert models . More broadly the field of representation learning focuses on learning transferable representations  via different pre-training strategies that can be applied for downstream tasks .

Efficient adaptation methodsIn recent times, the top-performing pre-trained model architectures are becoming considerably larger , and we arrive at a crossroad as full fine-tuning of such large models is becoming out of reach in many practical settings. Recent works address the storage costs associated with large model transfer by proposing parameter efficient transfer approaches as opposed to storing all of the fine-tuned model parameters . These approaches achieve parameter efficiency by training a subset of existing parameters , inserting new weight parameters into existing modules  or via additional learnable activations or inputs, known as prompts . Compared with existing work , InCA is also parameter efficient, yet we place special emphasis on compute and optimization efficiency, especially in the large-scale model setting. Additional lines of work study learning via selective tuning, enabling multi-domain transfer .

Feature extraction with attentionSelf and cross-attention mechanisms aggregate information from a set of feature tokens  and create representations based on relative inter-feature importance as computed by the attention mechanism . Self-attention plays a key in the transformer architecture  enabling non-local information aggregation. In settings where the number of inputs and desired outputs differ, cross-attention enables flexible aggregation based on a pair of query and key feature sets. When using cross-attention, one can cross-attend between different sets of activations  or between a set of activations and learnable latent parameters . In our settings, the adapter architecture applies cross-attention on extracted representations from a pre-trained network, inspired by the cross-attention module of Perceiver . However, we train multiple adapters in parallel and avoid the iterative re-sampling architecture present in their work. More generally, cross-attention layers have been vital in many existing object detection and multi-modal systems that fuse activations , or apply cross-attention with learnable latents .

Learning with intermediate featuresThe re-use of intermediate representations in deep learning is vast and spans from works on interpretability , to state of the art approaches in object detection that harness intermediate layers for multi-resolution feature pyramids  and segmentation . For ConvNets, the work of  studies classification utilizing intermediate network representations and the authors observe a decrease in accuracy when probing earlier layers.

## 3 Method

We introduce InCA, a lightweight and modular transfer learning alternative to full fine-tuning, that avoids backpropagation through the base-model. Let \(f(x)=g_{n} g_{n-1} g_{1}(x)\) be a pre-trained feed-forward neural network of \(n\) layers, with \(g_{j}()\) corresponding to the \(j\)-th layer of the network. We denote the activation computed by \(g_{j}\) as \(f_{j}(x)=g_{j} g_{j-1} g_{1}(x)\). During network inference, a "forward" computation processes and computes each \(f_{j}(x)\) activation to arrive to the network's final prediction \(f(x)\). During standard training, all of the intermediate activations \(\{f_{1}(x), f_{n-1}(x),f_{n}(x)\}\) are held in GPU memory and are used to compute gradients to update the model. For large models, this incurs large computational and GPU memory costs  which limits using the best and largest available pre-trained models under typical computation budgets.

Instead, we attach a set of isolated "models" to the pre-trained model \(f\) at selected activations \(f_{j_{k}}\) and pass them as input to a set of lightweight and shallow networks \(h_{k}(a)\) with separate parameters and losses. With this, we can train a set of heterogeneous adapters \(h_{k}(a)\) in parallel, while computing inference of the pre-trained model \(f\) only once during each update (see Fig. 2). For a set of adapters \(h_{k}(a)\) that take as input intermediate activations from \(\{f_{j_{k}}\}\) training follows as:

1. Single inference of \(f\) through a data batch \(x\) which computes \(f(x)\) and selected activations \(\{f_{j_{k}}(x)\}\). 22. Calculate the batch predictions and losses for each adapter \(h_{k}\), \(_{k}=(h_{k}(f_{j_{k}}(x)),y)\).
3. Computing \(_{}=_{k}\) and applying automatic differentiation then efficiently resolves the gradient and updates of each \(h_{k}\) automatically as desired.

By avoiding backpropagation through the pre-trained \(f\) we decouple the majority of the training costs from depending on the size of the base model \(f\) and instead the costs correlate with the much smaller adapter set \(\{h_{k}\}\). Below we demonstrate that even a simple cross-attention module for \(h_{k}\) makes the overall adaptation sufficiently expressive yet highly efficient.

InCA adapterAfter extraction of the layer representation \(f_{k}(x)\), we have access to a high-dimensional activation map at our disposal. To predict a target label \(\) from the high-dimensional \(f_{k}(x)\), the typical approach is to apply dimension reduction such as averaging (avgpool) or computing maximum values over a subset of the dimensions and then applying a linear classification head:

\[=pool} f_{m}(x).\]

Nonetheless, this simple aggregation approach leads to loss of information which we observe empirically in Sec. 4 and theoretically analyze in Appendix D. Instead, we use a cross-attention module to intelligently aggregate information from the entire large-dimensional activation map \(f_{k}(x)\) into a fixed-dimensional representation based on a number of cross-attention queries. Specifically, for standard downstream adaptation, given an intermediate feature map \(=[z^{1},,z^{T}]=f_{k}(x)\) with \(T\) tokens or channels we use the following adapter architecture

\[v_{}()_{[1:m]} :=attn}_{}([z^{1},,z^{T}],[q_{1}, ,q_{m}])\] \[_{}() :=_{}(pool}(v_{}()_{[1:m]})).\]

Note that the query tokens \([q_{1}, q_{m}]\) are optimized along with \(\). The multi-head cross-attention layer outputs \(v_{}\) is produced by surveying the feature map \(f_{k}(x)\) with the query tokens \([q_{1}, q_{m}]\). Then, the classification output \(=_{}()\) is obtained through averaging the cross-attention outputs (if \(m>1\)) followed by a fully-connected classification head after normalizing with LayerNorm . Based on our experiments, using a single query token \(q\) (\(m=1\)) achieves strong performance and is computationally efficient and we report results with \(m=1\) unless otherwise stated.

For more flexible inference such as in the settings of continual and class-incremental learning tasks, we present a modular version of InCA that disentangles the representations learned between different classes, which we refer to as "Open-InCA". For a \(c\)-way classification task, define separate queries \([q_{1}, q_{c}]\) for each class to compute representations separately,

\[[v^{1}_{}(),,v^{c}_{}( )]:=attn}_{}([z^{1},,z^{T}],[q_{1}, ,q_{c}])\] \[InCA}_{}():=head}_{}([v^{1}_{}(),,v^ {c}_{}()])\]

Above, \(head}_{}\) is a linear operator layer that operates on a _matrix_ input \([a_{1},,a_{c}]\) "diagonally". Given a weight parameter \(W\), the operator is defined as the column-wise dot product,

\[head}_{}([a_{1}, a_{c}])=  W_{1},a_{1}, W_{c},a_{c}.\]

Open-InCA compositionIn the Open-InCA adapter architecture, unique queries \([q_{1},,q_{c}]\) are defined for each class along with \(head}_{}\) that independently processes each coordinate prediction. Both \(head}\), LayerNorm and the \(attn}\) module in \(InCA}\) operate on each input \(q_{i}\) independently which separates the representation learned for each class and enables isolating each adapter output coordinate as

\[InCA}()_{i}= W_{i},( attn}_{}([z^{1},,z^{T}],[q_{i}])).\]

Above, \(W_{i}\) corresponds to the \(i\)-th column of \(head}\) weight. As a result \(InCA}\) enables class-level modularity with the capabilities of new class insertion, deletion and isolated class updates without regression. For example, deleting class \(i\) from the \(InCA}\) architecture amounts to simply dropping the query and head parameters \(q_{i}\) and \(W_{i}\) for that coordinate. In the setting of class-incremental learning (CIL) different query-head pairs from \(InCA}\) can be combined together, as long as the parameters of the \(\) and \(attn}\) remain the same. In practice, this leads to the notion of training \(InCA}\) with fixed norm and cross-attention weight parameters, in what we refer to as "_query-only-training_". In query-only-training, the learning of a new class corresponds to learning just 2, \(d\) dimensional parameters per-class and adapter, where \(d\) is the token dimension. Nonetheless, when using pre-trained \(\)-\(\) layer parameters, "query-only-training" performs within the accuracy of InCA on many datasets. In Appendix A we compare results of \(\), \(\)-\(\) and query-only-training in class-incremental learning (CIL). In Tab. 6 of the Appendix we observe that even learning just the query and head parameters is capable of harnessing the large dimensional representation maps \(f_{k}(x)\).

Layer branching candidate selectionThe cross-attention adapters can be applied in parallel over any intermediate layer of the network and we observe that the performance of many tasks hinges on identifying the right intermediate layer to use for that task. When considering intermediate activations \(f_{k}(x)\), we observe that

* Using activations such that \(f_{j}(x)\) is directly computed from a residual connection yields better adapter accuracy. This reflects that network representations are refined through each residual block.
* The middle and later layers of the network provide stronger input representations for the adapter. This is likely since the representations of the early layers do not have discriminative enough features to be used directly for high-level tasks.

Two-Stage trainingIn settings where the base-model forward-propagation during InCA training is too constraining, one may conduct training in two stages. In the first stage, save the activations that serve the input for the adapter for the entire training set, by running the base-model inference for a single epoch. After saving, the second stage proceeds by training the adapters for \(T\) epochs with loaded activations. Suppose the per-epoch cost of the pre-trained model forward-propagation is \(C_{PT}\) and the per-epoch cost of adapter optimization is \(C_{A}\), then two-stage training reduces the time of training from \(O((C_{PT}+C_{A}) T)\) to \(O(C_{A} T+C_{PT})\), where \(C_{PT} C_{A}\). With two-stage training, we are able to reduce a 30-epoch adapter training job to 30 seconds for a cached Stanf. Cars dataset (\(\)8,000 training samples). We speculate that further optimization can reduce training costs to "real-time", enabling an array of user-interactive applications.

## 4 Experiments

DatasetsIn our experiments, we measure the capabilities of InCA on a diverse set of 11 fine-grained datasets consisting of: CUB-200 , Aircrafts, , Stanford Cars , Stanford Dogs , Oxford Flowers 102 , MIT-67 , Oxford Pets , Describable Textures (DTD) , European Flood , FGVC Herbarium , and EuroSAT Land Use dataset . In Table 4 we explore InCA in the settings of multi-task learning and evaluate it on the ImageNet-to-Sketch benchmark that is comprised of 5 datasets: WikiArt , Oxford Flowers , Sketch , Stanford Cars , and CUB-200 .

BaselinesFor downstream transfer experiments, we compare InCA adaptation to other adaptation protocols. 1) _paragon_: Full fine-tuning is considered as the paragon as it performs well on a diverse set of datasets but incurs steep computational and parameter costs. 2) We compare InCA to other parameter efficient approaches, including 2a) LoRA  2b) Visual Prompt Tuning (VPT)  where we apply the top-performing VPT approach, _VPT-Deep_. 2c) BitFit  and 2d) AdaLN  which is the LayerNorm analogous, AdaBN approach. Note, each approach we name in 2) requires backpropagating through the entire network's activations to update the learnable parameters and leads to a large computational overhead compared with InCA. 3) In addition, we also compare InCA with a suite of computationally efficient approaches that avoid backbone backpropagation like InCA. These include 3a) Linear Probing (LP), 3b) Intermediate Linear Probing (In. LP) which utilizes the same training procedure as InCA but with a LP classifier on the activations. 3c) MLP-3, which is a feed-forward network that consists of probing the base-model with a 3-layer feed-forward network, and 3d) Intermediate MLP-3 (In. MLP-3), the extension of MLP-3 to intermediate layers.

Training detailsIn all of our results (including multi-task settings) we use the same training configuration for InCA. We only change the adapter architecture input layer to automatically match the dimension of the base-model activation map. InCA is robust to hyper-parameters and our training schedule is consistent for all runs. This amounts to 30 training epochs, AdamW optimizer  with 2 learning rates, and cosine annealing learning schedule; we provide the full training detailsin Appendix F. While InCA is efficient enough to operate at larger resolutions, we use 224 image resolution unless stated otherwise. Nonetheless, InCA performance improves at 384 resolution while remaining computationally competitive (see Table 2).

Transfer learning on ViTIn Table 1, we demonstrate the transfer performance of InCA applied to ViT-L/16. For each dataset we train InCA and extract activations at residual layers of the ViT for the last 12 blocks and output layer. For all baselines and our method we use the ViT DeiT pre-training  and additionally report ViT-L/16 pre-training results in Appendix Table 8. In the table, we compare InCA to full fine-tuning as well as applying InCA on the last layer and observe that only InCA is capable of achieving good results on challenging datasets such as Aircraft, Stanf. Cars, etc. and closes the maximal gap to full fine-tuning to -2.3%. The second best adaptation approach is LoRA which achieves a maximum gap of -4.4% to full fine-tuning, yet at additional training costs.

For a single dataset we can train the InCA modules with 2 learning rates in parallel which corresponds to 26 InCA modules with identical architectures attached to 13 activation maps. In this case, the total training costs of InCA on a single dataset correspond to one base-model run. In Appendix C, we report the hyper-parameter settings and training cost of InCA and current state of the art adaptation method for transformers, VPT  which incurs up to \(8.7\) the training costs of InCA with a large hyper-parameter search (\(2\) vs. \(24\) settings).

Transfer learning on SWINIn Table 3 we present downstream adaptation results for the SWIN-L pre-trained model. InCA adaptation is applied to the 3rd and 4th stages of the network residual activations. Because of the heterogeneous activation dimensions of the hierarchical SWIN architecture,

    &  \\  Dataset & Full FT & InCA & InCA (last) & In. LP & LP & In. MLP-3 & MLP-3 & VPT  & LoRA  & AdaIN & BiFit  \\  CUB-200 & 9.1 & **8.7** & 9.4 & 16.2 & 16.2 & 13.9 & 13.9 & 10.4 & 12.7 & 15.6 & 15.4 \\ DTD & 18.2 & **17.2** & 18.4 & 18.9 & 20.6 & 17.4 & 20.1 & 21.4 & 19.4 & 22.2 & 21.9 \\ Float Depth & 18.9 & **17.1** & 19.6 & 17.8 & 22.8 & 17.6 & 20.1 & 19.0 & 19.6 & 18.7 & 18.7 \\ EuroSAT & 1.0 & 1.2 & 1.9 & 2.1 & 3.7 & 1.5 & 2.5 & 1.1 & **0.9** & 1.5 & 1.4 \\ Aircraft & 14.9 & **15.6** & 21.9 & 50.6 & 67.4 & 36.8 & 47.4 & 21.7 & 16.6 & 28.5 & 27.2 \\ Heftarian & 18.8 & **21.1** & **24.6** & 32.6 & 39.8 & 29.5 & 36.4 & 21.4 & 19.2 & 27.9 & 28.3 \\ MTF-67 & 10.4 & **9.0** & **9.0** & 9.7 & 10.5 & 10.1 & 11.2 & 14.8 & 14.8 & 15.1 & 15.1 \\ Oxford Powers & 0.6 & **0.3** & 0.4 & 0.6 & 1.1 & 0.5 & 0.7 & 2.2 & 4.0 & 7.0 & 7.2 \\ Oxford Pets & 4.2 & **4.0** & 4.2 & 6.1 & 6.4 & 5.3 & 5.5 & 6.9 & 4.3 & 5.5 & 5.3 \\ Staff Cars & 8.1 & **7.7** & 10.2 & 29.2 & 47.2 & 20.8 & 31.4 & 9.2 & 8.4 & 16.0 & 14.7 \\ Stand Dogs & 5.9 & 5.4 & 5.8 & 5.3 & 5.3 & 5.7 & 5.7 & 7.3 & 4.3 & 3.8 & **3.7** \\  Mean Top-1 Test Error & 10.0 (0.00) & **98.8 (-2.3)** & 11.5 (-7.0) & 17.2 (-38.7) & 21.9 (-5.25) & 14.5 (-21.9) & 17.7 (-32.5) & 12.3 (-6.8) & 11.3 (-4.4) & 14.7 (-13.6) & 14.4 (-12.3) \\    \\  No backbone backprop. & ✗ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ \\   

Table 1: **Fine-grained Classification Top-1 Test Error (ViT-L/16)** We compare InCA to full fine-tuning (Full FT) along with other adaptation approaches for downstream learning. For each method we summarize the maximum gap in performance compared with the full fine-tuning paragon. In addition, we report the parameter efficiency and whether the method requires backpropagation through the pre-trained model. The minimum error over the columns excluding Full FT is presented in bold.

    &  \\  Category & Architecture & Pretraining data & Full FT & InCA & InCA (last) & inter. LP & LP & Model size \\  Vanilla & ViT-B/16  & In21K & 13.0 (0) & 15.9 (-7.6) & 17.5 (-16.4) & 23.9 (-32.4) & 24.3 (-32.4) & 86.5M \\ Transformer & ViT-B/16  & AL.BEF (CC14M) & 13.8 (0) & 15.5 (-4.2) & 14.8 (-6.3) & 24.7 (-42.6) & 25.8 (-42.6) & 85.9M \\ ViT-L/16  & In21K (DeiT) & 10.0 (0.9) & 9.8 (-2.3) & 11.5 (-7.7) & 17.2 (-35.7) & 21.9 (-5.25) & 29.0 (-5.2) & 304.3M \\ ViT-L/16  & In21K (DeiT) & -1 & 9.2 (-6.6) & 11.7 (-9.1) & 17.3 (-38.1) & 22.0 (-5.4) & 304.7M \\ Clip-V/L/14  336  & 400M Im-Text & -1 & 9.2 & 10.6 & 19.6 & 21.8 & 304.2M \\ ViT-B/14  & 28 In-Text & -1 & 9.4 & 10.4 & 14.0 & 15.2 & 63.2M \\ ViT-G/14  & 28 In-Text & -1 & 9.6 & 10.4 & 15.3 & 16.8 & 1884.9M \\  Hier Transformer & SWIN-L  & In21K & 9.3 (0) & 10 (-3.6) & 12.4 (-9.5) & 15.8 (-31.3) & 18.3 (-40.5) & 196.5M \\  Convolutional & ConvNext-B  & In21K & 9.4 (0) & 10.7 (-7.4) & 12.5 (-12.6) & 19.1 (-44.2) & 19.4 (-44.2) & 88.5M \\  & ReNext-101  & IG-38D  & 11.4 (0) & 12 (-8.7) & 17.3 (-27.1) & 20.1 (-38.8) & 21.3 (-39.7) & 466.5M \\   

Table 2: **Mean Top-1 Test Error for transfer learning with a variety of ViT, SWIN, and convolutional networks, including different network scales and pre-training strategies. Averages are reported on the 11 datasets presented in Table 1.\({}^{}\) indicates Full FT was avoided due to prohibitive computational costs. For DeiT ViT-L/16 @384 the gap is computed with respect to the 224 pre-training.**the reported adaptation model sizes depend on the activation map used for the selected adapter. InCA achieves the smallest maximum gap to full fine-tuning on SWIN while being computationally efficient. As with ViT-L, in SWIN we observe that challenging datasets require using intermediate activation maps for InCA, closing the maximal gap from (-9.5%) to (-3.6%).

**Evaluating InCA on different pre-trained models** InCA can be applied to any feed-forward neural network _without_ any implementation changes. We simply specify the intermediate layer names and feature tensor-ordering for any new architecture and InCA can be used directly. We note this is in sharp contrast to methods that rely on specific layers such as convolution filters  or self-attention . We illustrate the architecture versatility of our method in Table 2. We report the mean and maximum test error gap from full fine-tuning on the 11 fine-grained dataset suite as studied in Table 1. We test different architecture families, which include vanilla vision transformers _i.e._ViTs, SWIN , and modern convolutional networks (ConvNext , ResNext ). In addition, we test models pre-trained via different strategies including supervised learning  and vision-language objectives . We also test InCA at different ViT scales from ViT/B-16 (86M) to ViT/G-14 (1.8B). For InCA adaptation, all model sizes were trained on a single V100 GPU with batch size 32, including for the larger input resolutions runs.

**Multi-task Experiments** InCA's isolated design is suitable for multi-task inference and a single pre-trained-model can efficiently evaluate a batch of samples on multiple tasks, allowing for "one-to-many" inference. We compare InCA on the ImageNet-to-Sketch multi-task benchmark in Table 4. All methods except BA\({}^{2}\) were trained with a ViT-L/16 model and evaluated with the ImageNet-to-Sketch version of each dataset . For BA\({}^{2}\), we report the adaptation on a ResNet-50  backbone, as the BA\({}^{2}\) approach requires convolutional filters. Overall, InCA is the top performing method reaching near the paragon on the evaluated datasets. Importantly for multi-task, only InCA and LP enable multi-task inference via "computation sharing" of the base model inference.

**Learning efficiency** Isolating the learning from the base-model means InCA learns shallow neural networks directly on a downstream task. By avoiding deeply backpropagated gradients through the base model, the adapters receive direct signal which improves the optimization dynamics and speed of training. We compare the number of training steps required to train InCA and VPT-Deep and observe that InCA can be optimized in 4.5\(\) fewer epochs than VPT. Here we don't take into account the additional GPU memory costs of optimizing VPT in each step, nor the required hyper-parameter sweeps used in VPT. More detailed efficiency comparison results are given in Appendix C.

    &  \\  Dataset & Full FT & InCA & InCA (last) & In. LP & LP & In. MLP-3 & MLP-3 & LoRA & AdaLN & BiFit \\  CUB-200 & 9.0 & 9.1 & 9.6 & 10.2 & 10.6 & 9.7 & 9.7 & 10.0 & 9.1 & **8.8** \\ DTD & 15.6 & 17.8 & 19.1 & 17.7 & 19.1 & 16.7 & 16.7 & **15.8** & 16.7 & 17.0 \\ Flood Depth & 17.6 & **16.3** & 18.3 & 18.5 & 18.5 & 16.7 & 18.5 & 17.1 & 16.9 & 17.8 \\ EuroSAT & 0.7 & 1.5 & 2.4 & 2.7 & 3.7 & 1.6 & 2.2 & **0.9** & 1.1 & 1.7 \\ Arcerns & 12.2 & **15.8** & 25.3 & 43.5 & 52.7 & 33.7 & 34.8 & 16.1 & 22.7 & 26.5 \\ Herbarium & 14.9 & **18.2** & 23.0 & 29.2 & 34.0 & 24.9 & 27.6 & 18.4 & 21.2 & 29.7 \\ MT-67 & 10.5 & 10.1 & 10.1 & 9.9 & 10.3 & 10.2 & 10.2 & 9.6 & 8.9 & **8.5** \\ Oxford Flowers & 0.5 & **0.3** & 0.4 & 0.5 & 0.5 & 0.5 & 0.5 & 0.4 & 0.4 & 0.4 \\ Oxford Pets & 4.6 & **4.7** & 5.5 & 5.0 & 5.5 & 5.5 & 5.5 & 5.2 & 4.8 & 4.9 \\ Stall. Cars & 7.3 & **8.4** & 15.0 & 29.2 & 39.0 & 22.4 & 26 & 9.6 & 14.2 & 18.4 \\ Stall. Dogs & 9.1 & 8.1 & 8.1 & **7.1** & **7.1** & 9.8 & 9.8 & 11.3 & 9.1 & 9.0 \\  Mean Top-1 Test Error & 9.3 (0) & **10.0 (-3.6)** & 12.4 (-9.5) & 15.8 (-31.3) & 18.3 (-40.5) & 13.8 (-21.5) & 14.7 (-22.6) & 10.4 (-3.9) & 11.4 (-10.5) & 13.0 (-14.8) \\  \(\%\) Trainable param.\({}^{5}\) & 100\% & 3.7\% & 3.7 \% & 0.1\% & 0.1\% & 2.8\% & 2.8\% & 0.8\% & 0.1\% & 0.1\% \\  No backbone backpop. & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ \\   

Table 3: **Fine-grained Classification Top-1 Test Error (SWIN-L)** We compare InCA to full fine-tuning (Full FT) along with other adaptation approaches for downstream learning. For each method we summarize the maximum gap in performance compared with the full fine-tuning paragon. In addition we report the parameter efficiency and whether the method requires backpropagation through the pre-trained model. § For SWIN-L different activations map sizes leads to different \(\%\) of trainable parameters and we report the maximum for each method. The minimum error over the columns excluding Full FT is in bold.

## 5 Analysis

We analyze the results of InCA adaptation, focusing on the performance signature of different intermediate representations used as input for the adapter and the relation between the top InCA layers with fine-tuning. Further in Appendix D, we provide a theoretical proof motivating the extraction capabilities of cross-attention as it is used in InCA.

**Intermediate representations** We consider the intermediate representation signature created by evaluating the accuracy of adapters that utilize different layers. In Figure 3, we review the adapter performance applied to different layer representations. Datasets like CUB-200 and Flood-Depth mostly prefer final representations, whereas for datasets like Aircrafts and Stanf. Cars, the best adaptations use earlier representations with decreasing performance towards the last activations. Curiously, we observe consistency in layer affinity for certain datasets while using different pre-trainings for the backbone and even when using different architectures (Appendix Fig. 5).

**InCA and partial-tuning** In Appendix B we compare InCA with gradually un-freezing the base-model and applying partial fine-tuning on a growing set of layers. We run a set of experiments where we fine-tune a pre-trained model starting at different freezing points, this means we optimize all layers of the network after the freezing point location. For each dataset we construct a "partial tuning curve" where we plot the final test accuracy vs. freezing point (Figure 4). Interestingly, we observe a direct correlation between the layer-location of the top InCA adapter and the point where the partial tuning curve saturates. In particular, the partial tuning test accuracy plateaus (to the tuning of more

    &  &  \\  Method & Avg. & Flowers & WikiArt & Sketch & Cars & CUB-200 &  \# of trainable \\ parameters \\  &  GPU Memory \\ (training) \\  & 
 Inference Time \\ (for all 5 tasks) \\  \\  Full fine-tuning & 10.5 & 0.6 & **14.7** & 14.4 & 10.8 & 12.2 & \(5\) & \(1\) & \(5\) \\ Linear probing & 29.8 & 10.9 & 37.2 & 29.3 & 44.5 & 27.9 & \(0.01\) & \(0.17\) & \(1.01\) \\ BA\({}^{2}\) & 15.9 & 4.3 & 27.7 & 20.7 & 7.9 & 18.8 & \(1.03\) & \(1\) & \(5\) \\ TAPS  & 10.4 & 0.6 & 15.8 & **14.0** & 11.1 & 10.4 & \(4.12\) & \(1.23\) & \(5\) \\ SpotTune  & 14.3 & 3.7 & 24.2 & 19.8 & **7.6** & 16.0 & \(5.27\) & \(2\) & \(7.3\) \\ 
**InCA** & **9.8** & **0.3** & 15.4 & 16.8 & 7.7 & **8.8** & \(0.06\) & \(0.51\) & \(1.13\) \\   

Table 4: **Multitask Efficiency and Top-1 Test Error** on “ImageNet-to-Sketch” benchmark. InCA is the top performing method on average and is parameter efficient. Further, only InCA and linear probing “share computation” of the pre-trained model and enable “one-to-many” inference execution measured in the “Inference Time” column. BA\({}^{2}\) is based on ResNet-50 and can not be applied to ViTs. The rest of the methods are based on ViT-L/16.

Figure 3: **InCA Layer Performance Signature Relative test error improvement of InCA adapters attached to different intermediate layers. We evaluate InCA with ViT-L/16 with adapters at each residual block starting from Block 11.**layers) at around the same layer location as the top performing InCA adapter layer location. Namely, the point of saturation of the partial tuning curve is where partial-tuning is capable of harnessing the representation found by InCA at that layer. This gives further evidence that _"your representations are in the network"_ and that fine-tuning surfaces existing representations that can be directly identified by InCA. However, InCA adaptation operates an order of magnitude more efficiently and scales better to large models.

## 6 Discussion

In this paper, we present an efficient and effective alternative to full fine-tuning for transfer learning, closing the gap to full fine-tuning on a diverse set of downstream datasets. InCA has many benefits: it inherently generalizes to different architectures, efficiently scales to massive models, optimizes effectively, and unlocks modular and flexible adaptation applications including multi-task and incremental learning. Further, through the parallel exhaustive search of InCA we are able to better understand the inner representation dynamics of neural networks and construct illuminating "representation signatures" of different models and datasets.