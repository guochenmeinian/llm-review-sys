# A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks

A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks

 Sara Babakniya

Computer Science

University of Southern California

Los Angeles, CA

babakniy@usc.edu

&Zalan Fabian

Electrical and Computer Engineering

University of Southern California

Los Angeles, CA

zfabian@usc.edu

Chaoyang He

FedML

Sunnyvale, CA

ch@fedml.ai

&Mahdi Soltanolkotabi

Electrical and Computer Engineering

University of Southern California

Los Angeles, CA

soltanol@usc.edu

&Salman Avestimehr

Electrical and Computer Engineering

University of Southern California

Los Angeles, CA

avestime@usc.edu

###### Abstract

Deep learning models often suffer from forgetting previously learned information when trained on new data. This problem is exacerbated in federated learning (FL), where the data is distributed and can change independently for each user. Many solutions are proposed to resolve this catastrophic forgetting in a centralized setting. However, they do not apply directly to FL because of its unique complexities, such as privacy concerns and resource limitations. To overcome these challenges, this paper presents a framework for **federated class incremental learning** that utilizes a generative model to synthesize samples from past distributions. This data can be later exploited alongside the training data to mitigate catastrophic forgetting. To preserve privacy, the generative model is trained on the server using data-free methods at the end of each task without requesting data from clients. Moreover, our solution does not demand the users to store old data or models, which gives them the freedom to join/leave the training at any time. Additionally, we introduce SuperImageNet, a new regrouping of the ImageNet dataset specifically tailored for federated continual learning. We demonstrate significant improvements compared to existing baselines through extensive experiments on multiple datasets.

## 1 Introduction

Federated learning (FL)  is a decentralized machine learning technique that enables privacy-preserving collaborative learning. In FL, multiple users (clients) train a common (global) model in coordination with a server without sharing personal data. In recent years, FL has attracted tremendous attention in both research and industry and has been successfully employed in various fields, such as autonomous driving , next-word prediction , health care , and many more.

Despite its popularity, deploying FL in practice requires addressing critical challenges, such as resource limitation and statistical and system heterogeneity . While tackling these challenges is an essential step towards practical and efficient FL, there are still common assumptions in most FL frameworks that are too restrictive in realistic scenarios.

In particular, one of the most common assumptions is that clients' local data distribution is fixed and does not change over time. However, in real-world applications , clients' data constantly evolve due to changes in the environment, trends, or new interests. For example,  presents the real-world data of an online shop, suggesting interest in items shifts through seasons. Another example arises in healthcare, where a model trained on old diseases should be able to generalize to new diseases . In such scenarios (Figure 1), the model must rapidly adapt to the incoming data while preserving performance on past data distributions to avoid catastrophic forgetting [28; 39].

In the centralized setting, such problems have been explored in continual learning [48; 34] (also called lifelong learning  or incremental learning [9; 7] based on the initial settings and assumptions). In recent years, various algorithms have been proposed in Continual Learning (CL) to tackle catastrophic forgetting from different angles and can achieve promising performance in different scenarios.

Despite all the significant progress, most CL methods are not directly applicable to the federated setting due to inherent differences (Table 1) between the two settings. For instance, experience replay  is a popular approach, where a portion of past data points is saved to maintain some representation of previous distributions throughout the training. However, deploying experience replay in FL has resource and privacy limitations. It requires clients to store and keep their data, which may increase the memory usage of already resource-limited clients. Furthermore, users may not be able to store data for more than a specific time due to privacy concerns. Finally, depending solely on the clients to preserve the past is not reliable, as clients leaving means losing their data.

To address the aforementioned problems, we propose MFCL, _Mimicking Federated Continual Learning_: a privacy-preserving federated continual learning approach without episodic memory. In particular, MFCL is based on training a generative model in the server and sharing it with clients to sample synthetic examples of past data instead of storing the actual data on the client side. The generative model training is data-free in the sense that no form of training data is required from the clients, and only the global model is used in this step. It is specifically crucial because this step does not require powerful clients and does not cause any extra data leakage. Finally, this algorithm has competitive performance; our numerical experiments demonstrate improvement by \(10\%-20\%\) in average accuracy while reducing the training overhead of the clients.

Moreover, benchmarking federated continual learning in practical scenarios requires a large dataset to split among tasks and clients. However, existing datasets are not sufficiently large, causing most of the existing works in federated continual learning evaluating on a few clients (\(5\) to \(20\)) [45; 24; 52]. To enable more practical evaluations, we release a new regrouping of the ImageNet dataset, _SuperImageNet_. SuperImageNet enables evaluation with many clients and ensures all clients are assigned sufficient training samples regardless of the total number of tasks and active clients.

 
**Challenge** & **Limitation** \\  Low memory & Clients cannot store many examples \\  Clients drop out & Causes loss of information stored in memory \\  New clients join & New clients only have access to new classes \\  Privacy & Limits data saving and sharing of the clients \\  

Table 1: Challenges that limit the direct use of continual learning methods in federated settings.

Figure 1: In the real world, users constantly change their interests, observe new data, or lose some of the old ones. As a result, the training dataset is divided into different tasks. For example, here, at \(Task=1\), the clientsâ€™ datasets dominantly include pictures of animals, and by the end of the training (\(Task=T\)), the trend shifts towards landscapes.

We summarize our contributions below:

* We propose a novel framework to tackle the federated class incremental learning problem more efficiently for many users. Our framework specifically targets applications where past data samples on clients are unavailable.
* We point out potential issues with relying on client-side memory for FCL. Furthermore, we propose using a generative model trained by the server in a _data-free manner_ to help overcome catastrophic forgetting while preserving privacy.
* We modify the client-side training of traditional FL techniques in order to mitigate catastrophic forgetting using a generative model.
* We propose a new regrouping of the ImageNet dataset, SuperImageNet, tailored to federated continual learning settings that can be scaled to a large number of clients and tasks.
* We demonstrate the efficacy of our method in more realistic scenarios with a larger number of clients and more challenging datasets such as CIFAR-100 and TinyImageNet.

## 2 Related Work

**Continual Learning.** Catastrophic forgetting  is a fundamental problem in machine learning: when we train a model on new examples, its performance degrades when evaluated on past data. This problem is investigated in continual learning (CL) , and the goal is for the model to learn new information while preserving its knowledge of old data. A large body of research has attempted to tackle this problem from different angles, such as adding regularization terms [31; 1; 41], experience replay by storing data in memory [2; 10; 4; 35], training a generative model [56; 53; 32], or architecture parameter isolation [16; 38; 19; 51].

In CL settings, the training data is presented to the learner as a sequence of datasets - commonly known as **tasks**. In each timestamp, only one dataset (task) is available, and the learner's goal is to perform well on all the current and previous tasks.

Recent work focuses on three main scenarios, namely task-, domain- and class-incremental learning (IL) . In _Task-IL_, tasks are disjoint, and the output spaces are separated by task IDs provided during training and test time. For _Domain-IL_, the output space does not change for different tasks, but the task IDs are no longer provided. Finally, in _Class-IL_, new tasks introduce new classes to the output space, and the number of classes increases incrementally. Here, we work on **Class-IL**, which is the more challenging and realistic, especially in FL. In most of the FL applications, there is no task ID available, and it is preferred to learn a _single_ model for all the observed data.

**Class Incremental Learning.** In standard centralized Class-IL, the model is trained on a sequence of non-overlapping \(T\) tasks \(\{^{(1)},^{(2)},...,^{(T)}\}\) where the data distribution of task \(t\), \(D^{t}\), is fixed but unknown in advance, while all the tasks share the same output space (\(\)). For task \(t\), \(D^{t}\) consists of \(N^{t}\) pairs of samples and their labels \(\{(x_{i}^{t},y_{i}^{t})\}_{i=1}^{N^{t}}\}\), where all the newly introduced classes (\(y_{i}^{t}\)) belong to \(^{t}\) (\(y_{i}^{t}\{^{t}\}\) and \(_{j=1}^{t-1}\{^{j}\}\{^{t}\}=\)). Moreover, a shared output space among all tasks means that at the end of task \(t\), the total number of available classes equals \(q=_{i=1}^{t}|^{i}|\).

**Federated Continual Learning.** In real-life scenarios, users' local data is not static and may evolve. For instance, users' interests may change over time due to seasonal variations, resulting in more examples for a given class. On the other hand, reliability issues or privacy concerns may lead to users losing part of their old data as well. In Federated Continual Learning (FCL), the main focus is to adapt the global model to new data while maintaining the knowledge of the past.

Even though FCL is an important problem, it has only gained attention very recently, and  is the first paper on this topic. It focuses on Task-IL, which requires a unique task ID per task during inference. Furthermore, it adapts separate masks per task to improve personalized performance without preserving a common global model. This setting is considerably different than ours as we target Class-IL with a single global model to classify all the classes seen so far.  employs server and client-side knowledge distillation using a surrogate dataset.  relaxes the problem as clients have access to large memory to save the old examples and share their data, which is different from the standard FL setting. Some works, such as [26; 44; 52], explore the FCL problem in domains other than image classification.  has proposed using variational embedding to send data to the server securely and then server-side training to rehearse the previous task for Domain-IL.

This work focuses on Class-IL for supervised image classification without memory replay, similar to [45; 24]. However,  allows overlapping classes between tasks and focuses on few-shot learning, which is different from the standard Class-IL. The most related work to ours is , where authors propose FedCIL. This work also benefits from generative replay to compensate for the absence of old data and overcome forgetting. In FedCIL, clients train the discriminator and generator locally. Then, the server takes a consolidation step after aggregating the updates. In this step, the server generates synthetic data using all the generative models trained by the clients to consolidate the global model and improve the performance. The main difference between this work and ours is that in our work, the generative model is trained by the server in a data-free manner, which can reduce clients' training time and computation and does not require their private data (detailed comparison in Appendix H).

**Data-Free Knowledge Distillation.** Knowledge distillation (KD)  is a popular method to transfer knowledge from a well-trained teacher model to a (usually) smaller student model. Common KD methods are data-driven, and at least a small portion of training data is required. However, in some cases, training data may not be available during knowledge distillation due to privacy concerns. To tackle this problem, a new line of work [12; 22] proposes _data-free knowledge distillation_. In such methods, a generative model is used as a training data substitute. This generative model is trained to generate synthetic images such that the teacher model predicts them as their assigned label (Figure 2). This method has recently become popular in CL [57; 50] as well, mainly due to the fact that it can eliminate the need for memory in preserving knowledge. Data-free KD has been previously used in FL  to reduce the effect of data heterogeneity. However, to the best of our knowledge, this is the first work that adapted such a technique in the context of federated continual learning.

## 3 Federated Class Incremental Learning with MFCL

In federated Class-IL, a shared model is trained on \(T\) different tasks. However, the distributed and private nature of FL makes it distinct from the centralized version. In FL, users may join, drop out, or change their data independently. Besides, required data or computation power for some centralized algorithms may not be available in FL due to privacy and resource constraints.

To address the aforementioned problems, we propose MFCL, which is less reliant on the client-side memory and computational power. This algorithm includes two essential parts: _first_, at the end of each task, the server trains a generative model with data-free knowledge distillation methods to learn the representation of the seen classes. _Second_, clients can reduce catastrophic forgetting by generating synthetic images from the trained generative model obtained from the server side. This way, clients are not required to use their memory for storing old data. Moreover, this technique can address the problem of newly connected clients without past data. Furthermore, since the server trains the generative model training without additional information, this step does not introduce **new** privacy issues. Finally, MFCL can help mitigate the data heterogeneity problem, as clients can synthesize samples from classes they do not own  in memory. Next, we explain the two key parts of MFCL: server-side generative model (Figure 3 Left) and client-side continual learning (Figure 3 Right).

### Server-Side: Generative Model

The motivation for deploying a generative model is to synthesize images that mimic the old tasks and to avoid storing past data. However, training these generative models on the client's side, where the training data exists, is _computationally expensive_, _requires a large amount of training data_ and can be potentially _privacy concerning_. On the other hand, the server has only access to the global model and aggregated weights and no data. We propose training a generative model on the server, but in a data-free manner, i.e., utilizing model-inversion image synthesis [57; 50]. In such approaches, the goal is to synthesize images optimized with respect to the discriminator (global model). Then, the

Figure 2: Data-Free Knowledge Distillation. The generator receives random noise as input labels and synthesizes images that are labeled correctly by the trained teacher model.

generative model is shared with the clients to generate images during local training. To this aim, we utilize a generative model with ConvNet architecture, \(\), that takes noise \(z(0,1)\) as input and produces a synthetic sample \(\), resembling the original training input with the same dimensions. In order to train this model, we must balance the various training objectives we detail next.

**Cross Entropy Loss.** First, the synthetic data should be labeled correctly by the current discriminator model (global model or \(\)). To this end, we employ cross entropy classification loss between its assigned label \(z\) and the prediction of \(\) on synthetic data \(\). Note that noise dimension can be arbitrary and greater than the current discovered classes of task \(t\); therefore, we only consider the first \(q\) dimension here, where \(q=_{i=1}^{t}|^{i}|\) (which is equal to the total number of classes seen in the previous tasks). Then, we can define the cross-entropy loss as

\[_{CE}=CE(argmax(z[:q]),()).\] (1)

**Diversity Loss.** Synthetic images can suffer from a lack of class diversity. To solve this problem, we utilize the information entropy (IE) loss . For a probability vector \(=(p_{1},p_{2},...,p_{q})\), information entropy is evaluated as \(_{info}()=-_{i}p_{i}(p_{i})\). Based on the definition, inputs with uniform data distributions have the maximum IE. Hence, to encourage \(\) to produce diverse samples, we deploy the diversity loss defined as

\[_{div}=-_{info}(_{i=1}^{bs}( _{i})).\] (2)

This loss measures the IE for samples of a batch (\(bs\): batch size). Maximizing this term encourages the output distribution of the generator to be more uniform and balanced for all the available classes.

**Batch Statistics Loss.** Prior works  in the centralized setting have recognized that the distribution of synthetic images generated by model inversion methods can drift from real data. Therefore, in order to avoid such problems, we add batch statistics loss \(_{BN}\) to our generator training objective. Specifically, the server has access to the statistics (mean and standard deviation) of the global model's BatchNorm layers obtained from training on real data. We want to enforce the same statistics in all BatchNorm layers on the generated synthetic images as well. To this end, we minimize the layer-wise distances between the two statistics written as

\[_{BN}=_{i=1}^{L}KL((_{i},_{i}^{ 2}),(_{i},_{i}^{2}))=}{}-(1-+(-)^{2}}{^{2}}).\] (3)

Here, \(L\) denotes the total number of BatchNorm layers, \(_{i}\) and \(_{i}\) are the mean and standard deviation stored in BatchNorm layer \(i\) of the global model, \(_{i},~{}_{i}\) are measured statistics of BatchNorm layer \(i\) for the synthetic images. Finally, \(KL\) stands for the Kullback-Leibler (KL) divergence.

We want to note that this loss does not rely on the BatchNorm layer itself but rather on their stored statistics (\(_{i},_{i}\) ). \(\) aims to generate synthetic images similar to the real ones such that the global model would not be able to classify them purely based on these statistics. One way to achieve this is to ensure that synthetic and real images have similar statistics in the intermediate layers, and this is

Figure 3: Overview of MFCL. **Left.** The server aggregates the updates every round and trains a generator using data-free methods at the end of each task. **Right.** Clients train their models locally using their local data and synthetic images of past tasks from the generator.

the role of \(_{BN}\). In our experiments, we employed the most common baseline model in CL, which already contains BatchNorm layers and measures those statistics. However, these layers are not a necessity and can be substituted by similar ones, such as GroupNorm. In general, if no normalization layer is used in the model, clients can still compute the running statistics of specific layers and share them with the server, and later, the server can use them in the training of the \(\).

**Image Prior Loss.** In natural images, adjacent pixels usually have values close to each other. Adding prior loss is a common technique to encourage a similar trend in the synthetic images . In particular, we can create the smoothed (blurred) version of an image by applying a Gaussian kernel and minimizing the distance of the original and \(Smooth()\) using the image prior loss

\[_{pr}=||-Smooth()||_{2}^{2}.\] (4)

In summary, we can write the training objective of \(\) as Equation 5 where \(w_{div}\), \(w_{BN}\) and \(w_{pr}\) control weight of each term.

\[_{}_{CE}+w_{div}_{div}+w_{BN} _{BN}+w_{pr}_{pr},\] (5)

### Client-side: Continual Learning

For client-side training, our solution is inspired by the algorithm proposed in . In particular, the authors distill the _stability-plasticity_ dilemma into three critical requirements of continual learning and aim to address them one by one.

**Current Task.** To have plasticity, the model needs to learn the new features in a way that is least biased towards the old tasks. Therefore, instead of including all the output space in the loss, the CE loss can be computed _for the new classes only_ by splitting the linear heads and excluding the old ones, which we can write as

\[_{CE}^{t}=CE(_{t}(x),y),&if\ y^{t}\\ 0,&O.W.\] (6)

**Previous Tasks.** To overcome forgetting, after the first task, we train the model using synthetic and real data simultaneously. However, the distribution of the synthetic data might differ from the real one, and it becomes important to prevent the model from distinguishing old and new data only based on the distribution difference. To address this problem, we only use the extracted features of the data. To this aim, clients freeze the feature extraction part and only update the classification head (represented by \(_{t}^{*}\)) for both real (\(x\)) and synthetic (\(\)) images. This fine-tuning loss is formulated as

\[_{FT}^{t}=CE(_{t}^{*}([x,]),y).\] (7)

Finally, to minimize feature drift and forgetting of the previous tasks, the common method is knowledge distillation over the prediction layer. However,  proposed _importance-weighted feature distillation_: instead of using the knowledge in the decision layer, they use the output of the feature extraction part of the model (penultimate layer). This way, only the more significant features of the old model are transferred, enabling the model to learn the new features from the new tasks. This loss can be written as

\[_{KD}^{t}=||(_{t}^{1:L-1}([x,]))- (_{t-1}^{1:L-1}([x,]))||_{2}^{2},\] (8)

where \(\) is the frozen linear head of the model trained on the last task (\(=_{t-1}^{L}\)).

In summary, the final objective on the client side as

\[_{_{t}}_{CE}^{t}+w_{FT}_{FT}^{t}+w_{KD} _{KD}^{t},\] (9)

where \(w_{FT}\) and \(w_{KD}\) are hyper-parameters determining the importance of each loss term.

### Summary of MFCL Algorithm

In summary, during the first task, clients train the model using only the \(_{CE}\) part of (9) and send their updates to the server where the global model gets updated (FedAvg) for \(R\) rounds. At the end of training task \(t=1\), the server trains the generative model by optimizing (5), using the latest global model. Finally, the server freezes and saves \(\) and the global model (\(_{t-1}\)). This procedure repeats for all future tasks, with the only difference being that for \(t>1\), the server needs to send the current global model (\(_{t}\)), precious task's final model (\(_{t-1}\)) and \(\) to clients. Since \(_{t-1}\) and \(\) are fixed during training \(_{t}\), the server can send them to each client once per task to reduce the communication cost. To further decrease this overhead, we can employ communication-efficient methods in federated learning, such as , that can highly compress the model with minor performance degradation, which we leave for future work. Algorithm 1 in the Appendix A shows different steps of MFCL.

## 4 SuperImageNet

In centralized Class-IL, the tasks are disjoint, and each task reveals a new set of classes; therefore, the total number of classes strongly limits the number of tasks. Moreover, we must ensure that each task has sufficient training data for learning. Thus, the number of examples per class is essential in creating CL datasets. However, the dataset needs to be split along the task dimension and clients in a Federated Class-IL setup. For instance, CIFAR-100, a popular dataset for benchmarking FL algorithms, consists of \(100\) classes, each with \(500\) examples, which must be partitioned into \(T\) tasks, and each task's data is split among \(N\) clients. In other words, for a single task, a client has access to only \(\) of that dataset; in a common scenario where \(N=100\) and \(T=10\), we can assign only \(50\) samples to each client (about \(5\) example per class in i.i.d data distribution), which is hardly enough.

To resolve this problem, prior works have used a small number of clients [45; 24; 52], combined multiple datasets , employed a surrogate dataset  or allowed data sharing among the clients . However, these solutions may not be possible, applicable, or may violate the FL's assumptions. This demonstrates the importance of introducing new benchmark datasets for federated continual settings.

We introduce **SuperImageNet**, a dataset created by superclassing the _ImageNet_ dataset, thus greatly increasing the number of available samples for each class. There are \(3\) versions of the dataset, each offering a different trade-off between the number of classes (for Class-IL) and the number of examples per class (for FL) as shown in Table 4. For example, _SuperImageNet-M_ has _10x_ more samples per class compared to CIFAR-100, which allows for an order of magnitude increase in the number of federated clients in while maintaining the same amount of training data per client. As shown in Figure 4, we have merged classes of similar concepts to increase the sample size per class.

## 5 Experiments

**Setting.** We demonstrate the efficacy of our method on three challenging datasets: CIFAR-100 , TinyImageNet  and SuperImageNet-L 1. For all datasets, we use the baseline ResNet18  as the global model and ConvNet architecture for \(\), which we explain in detail in the Appendix C.

**Dataset** & **\# examples/class** & **\# classes** \\   _SuperImageNet-S_ & \(2500\) & \(100\) \\ _SuperImageNet-M_ & \(5000\) & \(75\) \\ _SuperImageNet-L_ & \(7500\) & \(50\) \\ 

Table 2: Versions of SuperImageNet

Figure 4: Building SuperImageNet by regrouping ImageNet dataset. Labels in Blue are the original labels, and in Red are the labels in SuperImageNet.

Table 3 summarizes the setting for each dataset. For each dataset, there are 10 non-overlapping tasks (\(T=10\)), and we use Latent Dirichlet Allocation (LDA)  with \(=1\) to distribute the data of each task among the clients. Clients train the local model using an SGD optimizer, and all the results were reported after averaging over 3 different random initializations (seeds). We refer to Appendix F for other hyperparameters.

**Metric.** We use three metrics -Average Accuracy, Average Forgetting, and Wallclock time.

_Average Accuracy (\(}\)):_ Let us define Accuracy (\(^{t}\)) as the accuracy of the model at the end of task \(t\), over _all_ the classes observed so far. Then, \(}\) is average of all \(^{t}\) for all the \(T\) available tasks.

_Average Forgetting (\(\)):_ Forgetting (\(f^{t}\)) of task \(t\) is defined as the difference between the highest accuracy of the model on task \(t\) and its performance at the end of the training. Therefore, we can evaluate the average forgetting by averaging all the \(f^{t}\) for task \(1\) to \(T-1\) at the end of task \(T\).

_Wallclock time._ This is the time the server or clients take to perform one FL round in seconds. The time is measured rounds on our local GPU NVIDIA-A100 and averaged between different clients.

**Baseline.** We compare our method with **FedAvg**, **FedProx**, **FedProx\({}^{+}\)**, **FedCIL**, **FedLwF-2T** and **Oracle. FedAvg** and **FedProx** are the two most common aggregation methods; specifically, FedProx is designed for non-i.i.d data distributions and tries to minimize the distance of the client's update from the global model. Inspired by FedProx, we also explore adding a loss term to minimize the change of the current global model from one from the previous task, which we name \(^{+}\). **FedCIL** is a GAN-based method where clients train the discriminator and generator locally to generate synthetic samples from the old tasks. **FedLwF-2T** is another method designed for federated continual learning. In this method, clients have two additional knowledge distillation loss terms: their local model trained on the previous task and the current global model. Finally, **Oracle** is an upper bound on the performance: during the training of the \(i_{th}\) task, clients have access to all of their training data from \(t=1\) to \(t=i\).

### Results

Figure 5 shows the accuracy of the model on all the observed classes so far. In all three datasets, MFCL consistently outperforms the baselines by a large margin (up to \(25\%\) absolute improvement in test accuracy). In the CIFAR-100 dataset, the only baseline that can also correctly classify some examples from past data is **FedCIL**. Both MFCL and FedCIL benefit from a generative model (roughly the same size) to remember the past. Here, a similar generative model to the one in the  for the CIFAR-10 dataset is used. Since, in FedCIL, the clients train the generative and global models simultaneously, they require more training iteration. We repeat the same process and adapt similar architectures for the other two datasets. 2 But, given that GANs are not straightforward to fine-tune, this method does not perform well or converge. We explain more in the Appendix H.

We have further compared the performance and overhead of the methods in Table 4. The first two metrics, Average Accuracy and Average Forgetting reveal how much the model is learning new tasks while preserving its performance on the old task. As expected, FedAvg and FedProx have the highest forgetting values because they are not designed for such a scenario. Also, high forgetting for FedLwF-2T indicates that including teachers in the absence of old data cannot be effective. Notably, FedProx\({}^{+}\) has a lower forgetting value, mainly due to the fact that it also has lower performance for each task. Finally, FedCIL and MFCL have experienced the least forgetting with knowledge transferred from the old task to the new ones. Particularly, MFCL has the smallest forgetting, which means it is the most successful in preserving the learned knowledge.

We also compare the methods based on their computational costs. It is notable that some methods change after learning the first task; therefore, we distinguish between the cost of the first task and the other ones. As depicted, for \(T>1\), MFCL slightly increases the training time caused by employing the generative model. But, as a trade-off, it can significantly improve performance and forgetting.

   Dataset & \#Client &  \#Client \\ per round \\  & 
 \#classes \\ per task \\  \\   CIFAR-100 & 50 & 5 & 10 \\  TinyImageNet & 100 & 10 & 20 \\  SuperImageNet-L & 300 & 30 & 5 \\   

Table 3: Training parameters of each dataset.

[MISSING_PAGE_FAIL:9]

## 6 Discussion

**Privacy of MFCL.** Federated Learning, specifically FedAvg, is vulnerable to different attacks, such as data poisoning, model poisoning, backdoor attacks, and gradient inversion attacks [27; 36; 18; 20; 11; 33]. We believe, MFCL generally does not introduce any additional privacy issues and still it is prone to the same set of attacks as FedAvg. MFCL trains the generative model based on the weights of the _global model_, which is already available to all clients in the case of FedAvg. On the contrary, in some prior work in federated continual learning, the clients need to share a locally trained generative model or perturbed private data, potentially causing more privacy problems.

Furthermore, for FedAvg, various solutions and defenses, such as differential privacy or secure aggregation [55; 8], are proposed to mitigate the effect of such privacy attacks. One can employ these solutions in the case of MFCL as well. Notably, in MFCL, the server **does not** require access to the individual client's updates and uses the aggregated model for training. Therefore, training a generative model is still viable after incorporating these mechanisms.

In MFCL, the server trains the generator using only client updates. Figure 6 presents random samples of real and synthetic images from the CIFAR-100 dataset. Images of the same column correspond to real and synthetic samples from the same class. Synthetic samples do not resemble any specific training examples of the clients and thus preserve privacy. However, they consist of some common knowledge about the class and effectively represent the whole class. Therefore, they can significantly reduce catastrophic forgetting.

**Limitations.** In our method, clients need the generative model, the final global model of the last task, and the current global model, which adds overheads such as communication between the server and clients and storage. However, there are fundamental differences between storing the generative model and actual data. First, the memory cost is independent of the task size: as the number of tasks increases, clients either have to delete some of the existing examples of the memory to be able to add new ones or need to increase the memory size. In contrast, the generative model size is constant. Finally, clients can delete the generative model while not participating in the FL process and retrieve it later if they join. On the other hand, deleting data samples from memory results in a permanent loss of information. We have delved into this in Appendix D.

## 7 Conclusion

This work presents a federated Class-IL framework while addressing resource limitations and privacy challenges. We exploit generative models trained by the server in a data-free fashion, obviating the need for expensive on-device memory on clients. Our experiments demonstrate that our method can effectively alleviate catastrophic forgetting and outperform the existing state-of-the-art solutions.

## 8 Acknowledgment

This material is based upon work supported by ONR grant N00014-23-1-2191, ARO grant W911NF-22-1-0165, Defense Advanced Research Projects Agency (DARPA) under Contract No. FASTNICS HR001120C0088 and HR001120C0160, and gifts from Intel and Qualcomm. The views, opinions, and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.

Figure 6: Real vs synthetic data generated by the generative model for CIFAR-100 dataset.

[MISSING_PAGE_FAIL:11]

*  Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. _arXiv preprint arXiv:1701.08734_, 2017.
*  Jonas Geiping, Hartmut Bauermeister, Hannah Droge, and Michael Moeller. Inverting gradients-how easy is it to break privacy in federated learning? _Advances in Neural Information Processing Systems_, 33:16937-16947, 2020.
*  Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Francoise Beaufays, Sean Augenstein, Hubert Eichner, Chloe Kiddon, and Daniel Ramage. Federated learning for mobile keyboard prediction. _arXiv preprint arXiv:1811.03604_, 2018.
*  Matan Haroush, Itay Hubara, Elad Hoffer, and Daniel Soudry. The knowledge within: Methods for data-free model compression. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8494-8502, 2020.
*  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
*  Sean M Hendryx, Dharma Raj KC, Bradley Walls, and Clayton T Morrison. Federated reconnaissance: Efficient, distributed, class-incremental learning. _arXiv preprint arXiv:2109.00150_, 2021.
*  Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2(7), 2015.
*  Ziyue Jiang, Yi Ren, Ming Lei, and Zhou Zhao. Fedspeech: Federated text-to-speech with continual learning. _arXiv preprint arXiv:2110.07216_, 2021.
*  Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
*  James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _Proceedings of the national academy of sciences_, 114(13):3521-3526, 2017.
*  Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. _arXiv preprint arXiv:1610.05492_, 2016.
*  Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
*  Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. _Advances in neural information processing systems_, 30, 2017.
*  Timothee Lesort, Hugo Caselles-Dupre, Michael Garcia-Ortiz, Andrei Stoian, and David Filliat. Generative models from the perspective of continual learning. In _2019 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2019.
*  Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. _IEEE Signal Processing Magazine_, 37(3):50-60, 2020.
*  Zhizhong Li and Derek Hoiem. Learning without forgetting. _IEEE transactions on pattern analysis and machine intelligence_, 40(12):2935-2947, 2017.
*  David Lopez-Paz and Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. _Advances in neural information processing systems_, 30, 2017.
*  Lingjuan Lyu, Han Yu, and Qiang Yang. Threats to federated learning: A survey. _arXiv preprint arXiv:2003.02133_, 2020.
*  Yuhang Ma, Zhongle Xie, Jue Wang, Ke Chen, and Lidan Shou. Continual federated learning based on knowledge distillation.
*  Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In _Proceedings of the IEEE conference on Computer Vision and Pattern Recognition_, pages 7765-7773, 2018.

*  Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In _Psychology of learning and motivation_, volume 24, pages 109-165. Elsevier, 1989.
*  Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.
*  Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard Turner, and Mohammad Emtiyaz E Khan. Continual deep learning by functional regularisation of memorable past. _Advances in Neural Information Processing Systems_, 33:4453-4464, 2020.
*  Tae Jin Park, Kenichi Kumatani, and Dimitrios Dimitriadis. Tackling dynamics in federated incremental learning with variational embedding rehearsal. _arXiv preprint arXiv:2110.09695_, 2021.
*  Hadi Pouransari and Saman Ghili. Tiny imagenet visual recognition challenge. _CS231N course, Stanford Univ., Stanford, CA, USA_, 5, 2014.
*  Aman Priyanshu, Mudit Sinha, and Shreyans Mehta. Continual distributed learning for crisis management. _arXiv preprint arXiv:2104.12876_, 2021.
*  Daiqing Qi, Handong Zhao, and Sheng Li. Better generative replay for continual federated learning. _arXiv preprint arXiv:2302.13001_, 2023.
*  Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. _arXiv preprint arXiv:2003.00295_, 2020.
*  David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. _Advances in Neural Information Processing Systems_, 32, 2019.
*  Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. _Advances in neural information processing systems_, 30, 2017.
*  Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel, Daniel Benditiks, Liron Mor-Yosef, and Itai Zeitak. Overcoming forgetting in federated learning on non-iid data. _arXiv preprint arXiv:1910.07796_, 2019.
*  James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen, Hongxia Jin, and Zsolt Kira. Always be dreaming: A new approach for data-free class-incremental learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9374-9384, 2021.
*  James Smith, Cameron Taylor, Seth Baer, and Constantine Dovrolis. Unsupervised progressive learning and the stam architecture. _arXiv preprint arXiv:1904.02021_, 2019.
*  Anastasia Usmanova, Francois Portet, Philippe Lalanda, and German Vega. A distillation-based approach integrating continual learning and federated learning for pervasive services. _arXiv preprint arXiv:2109.04197_, 2021.
*  Gido M Van de Ven and Andreas S Tolias. Generative replay with feedback connections as a general strategy for continual learning. _arXiv preprint arXiv:1809.10635_, 2018.
*  Gido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. _arXiv preprint arXiv:1904.07734_, 2019.
*  Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H. Yang, Farhad Farokhi, Shi Jin, Tony Q. S. Quek, and H. Vincent Poor. Federated learning with differential privacy: Algorithms and performance analysis. _IEEE Transactions on Information Forensics and Security_, 15:3454-3469, 2020.
*  Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, Zhengyou Zhang, and Yun Fu. Incremental classifier learning with generative adversarial networks. _arXiv preprint arXiv:1802.00853_, 2018.
*  Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8715-8724, 2020.
*  Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung Ju Hwang. Federated continual learning with weighted inter-client transfer. In _International Conference on Machine Learning_, pages 12073-12086. PMLR, 2021.

*  Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In _International Conference on Machine Learning_, pages 3987-3995. PMLR, 2017.
*  Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous federated learning. In _International Conference on Machine Learning_, pages 12878-12889. PMLR, 2021.

MFCL Algorithm

Algorithm 1 summarizes our method. Here, for every task, clients train the local model using the shared generative model. At the end of each task, the server updates the generative model using data-free methods.

```
1:\(N\): #Clients, \([_{N}]\): Client Set, \(K\): #Clients per Round, \(u_{i}\): client i Update, \(E\): Local Epoch
2:\(R\): FL Rounds per Task, \(T\): #Tasks, \(t\): current task, \(||^{t}\): Task \(t\) Size, \(q\): #Discovered Classes
3:\(_{t}\): Global Model for task t, \(_{t}\): Generative Model, \(E_{}\): Generator Training Epoch
4:\(q 0\)
5:\(,_{1}()\)
6:for\(t=1\)to\(T\)do
7:\(q q+|^{t}|\)
8:\(_{t}(_{t},q)\) # Add new observed classes in the classification layer.
9:for\(r=1\)to\(R\)do
10:\(C_{K}([_{N}],K)\)
11:for\(c C_{K}\) in parallel do
12:\(u_{c}(_{t},,_{t- 1},E)\) # For \(t=1\) we do not need \(_{0}\) and \(\).
13:endfor
14:\(_{t}(_{t},[u_{c}])\)
15:endfor
16:\(_{t}(_{t})\) # Fix Global model.
17:\((_{t},E_{},q)\) # Train the generative model.
18:\(()\) # Fix generator weights.
19:endfor ```

**Algorithm 1** MFCL

## Appendix B Code for Reproduction

The codebase for this work and regrouping the ImageNet dataset is available at https://github.com/SaraBabakN/MFCL-NeurIPS23.

## Appendix C Details of the Generative Model

**Architectures.** In Table 6, we show the generative model architectures used for CIFAR-100, TinyImageNet, and SuperImageNet datasets. In all experiments, the global model has ResNet18 architecture. For the CIFAR-100 and TinyImageNet datasets, we change the first CONV layer kernel size to \(3 3\) from \(7 7\). In this table, CONV layers are reported as \(K K(C_{in},C_{out})\), where \(K\), \(C_{in}\) and \(C_{out}\) are the size of the kernel, input channel and output channel of the layer, respectively.

**Weight Initialization.** The generative model is randomly initialized for the first task and trained from scratch. For all the future tasks (t > 1), the server uses the previous generative model (t - 1) as the initialization.

**Synthetic Samples Generation.** To generate the synthetic data, clients sample i.i.d noise, which later would determine the classes via the argmax function applied to the first q elements (considering q is the total number of seen classes). Given the noise is sampled i.i.d, the probability of generating samples from class \(i\) equals \(\). Although this might not lead to the same number of synthetic samples from each class in every batch, the generated class distribution is uniform over all classes. Thus, in expectation, we have class balance in generated samples.

**Catastrophic Forgetting in the Generative Model.** The effectiveness of the \(\) is closely linked to the performance of the global model. If the global model forgets old classes after completing a task, the quality of corresponding synthetic data will decline. Hence, it is crucial to select a reliable generative model and a robust global model. A good generative model can assist the global model in preventing forgetting when learning new tasks. This model can then serve as a teacher for the next round of the \(\) model.

**Global Aggregation Method.** In this work, we have employed FedAvg to aggregate the client updates. Since the generator is always trained after the aggregation, its training is not impacted by changing the aggregation method. However, the generative model uses the aggregated model as its discriminator, and it is directly affected by the quality of the final global model. Therefore, any aggregation mechanism that improves the global model's performance would also help the generative model and vice versa.

## Appendix D Overheads of generative model

**Client-side.** Using \(\) on the client side would increase the computational costs compared to vanilla FedAvg. However, existing methods in CL often need to impose additional costs such as memory, computing, or both to mitigate catastrophic forgetting. Nevertheless, there are ways to reduce costs for MFCL. For example, clients can perform inference once, generate and store synthetic images only for training, and then delete them all. They can further reduce costs by requesting that the server generate synthetic images and send them the data instead of \(\). Here, we raise two crucial points about the synthesized data. Firstly, there is an intrinsic distinction between storing synthetic (or \(\)) and actual data; the former is solely required during training, and clients can delete them right after the training. Conversely, the data in episodic memory should always be saved on the client's side because once deleted, it becomes unavailable. Secondly, synthetic data is shared knowledge that can assist anyone with unbalanced data or no memory in enhancing their model's performance. In contrast, episodic memory can only be used by one client.

**Server-side.** The server needs to train the \(\)**once per task**. It is commonly assumed that the server has access to more powerful computing power and can compute more information in a faster time compared to clients. This training step does not have overhead on the client side and might slow down the whole process. However, tasks do not change rapidly in real life, giving the server ample time to train the generative model before any trends or client data shifts occur.

**Communication Cost.** Transmitting the generative model can be a potential overhead for MFCL, as it is a cost that clients must bear **once per task** to prevent or reduce catastrophic forgetting. However, several possible methods, such as compression, can significantly reduce this cost while maintaining excellent performance. This could be an interesting direction for future research.

## Appendix E More on the Privacy of MFCL

**MFCL with Differential Privacy.** We want to highlight that the generator can only be as good as the discriminator in data-free generative model training. If the global model can learn the decision boundaries and individual classes with a DP guarantee, the generator can learn this knowledge and present it through the synthetic example. Otherwise, if the global model fails to learn the current tasks, there is not much knowledge to preserve for the future. With the DP guarantee, the main challenge is training a reasonable global model; improving this performance can also help the generative model.

 
**CIFAR-100** & **TinyImageNet** & **SuperImageNet** \\   \((200,128 8 8)\) & \((400,128 8 8)\) & \((200,64 7 7)\) \\  \((-,128,8,8)\) & \((-,128,8,8)\) & \((-,64,7,7)\) \\  BatchNorm(128) & \((128)\) & \((64)\) \\  Interpolate(2) & \((2)\) & \((2)\) \\  \( 3(128,128)\) & \( 3(128,128)\) & \( 3(64,64)\) \\  BatchNorm(128) & \((128)\) & \((64)\) \\  LeakyReLU & LeakyReLU & LeakyReLU \\  Interpolate(2) & \((2)\) & \((2)\) \\  \( 3(128,64)\) & \( 3(128,128)\) & \( 3(64,64)\) \\  BatchNorm(64) & \((128)\) & \((64)\) \\  LeakyReLU & LeakyReLU & LeakyReLU \\  \( 3(64,3)\) & \((2)\) \\  Tanh & \( 3(64,64)\) \\  BatchNorm(3) & \((3)\) & \((64)\) \\     \( 3(64,3)\) & \((2)\) \\  Tanh & \( 3(64,64)\) \\  BatchNorm(3) & \((64)\) \\  Tanh & \( 3(64,64)\) \\  BatchNorm(3) & \((64)\) \\   
  \( 3(64,3)\) \\  Tanh & \((64)\) \\  BatchNorm(3) & \((3)\) \\  

Table 6: Generative model Architecture

**MFCL with Secure Aggregation.** If the clients do not trust the server with their updates, a potential solution is _Secure Aggregation_. In a nutshell, secure aggregation is a defense mechanism that ensures update privacy, especially when the server is potentially malicious. More importantly, since MFCL also does not require individual updates, it is compatible with secure aggregation and can be employed to align with Secure Aggregation.

**Privacy Concerns Associated with Data Storage.** Currently, some different regulations and rules limit the storage time of users' data. Usually, the service providers do not own the data forever and are obligated to erase it after a specific duration. Sometimes, the data is available only in the form of a stream, and it never gets stored. But most of the time, data is available for a short period of enough to perform a few rounds of training. In this way, if multiple service providers participate in federated learning, their data would dynamically change as they delete old data and acquire new ones.

**MFCL and Batch Statistics.** MFCL benefits from Batch Statistics Loss (\(_{BN}\)) in training the generative model. However, some defense mechanisms suggest not sharing local Batch Statistics with the server. While training the generative model without the \(_{BN}\) is still possible, it can reduce the accuracy. Addressing this is an interesting future direction.

## Appendix F Hyperparameters

Table 7 presents some of the more important parameters and settings for each experiment.

## Appendix G Hyperparameter tuning for MFCL

Hyperparameters can play an essential role in the final performance of algorithms. In our experiments, we have adapted the commonly used parameters, and here, we show how sensitive the final performance is regarding each hyperparameter. This is particularly important because hyperparameter tuning is very expensive in federated learning and can be unfeasible in continual learning. To this aim, we change one parameter at a time while fixing the rest. In Table 8, we report the final \(}\) of each hyperparameter on CIFAR-100 datasets with 10 tasks.

\(w_{div}\)**:** Weight of diversity loss (\(_{div}\)).

\(w_{BN}\)**:** Weight of Batch Statistics loss (\(_{BN}\)).

\(w_{pr}\)**:** Weight of Image Prior loss (\(_{FT}\)).

\(Z\_dim\)**:** Input noise dimension for training the \(\) model.

\(gen\_epoch\)**:** Number of iteration to train the \(\) model.

This is the setting that we used \(w_{div}=1\), \(w_{BN}=75,w_{pr}=0.001,Z\_dim=200\), \(gen\_epoch=5000\) and the average accuracy equals \(45.1\%\). (There may be a minor difference between this value and the result in the main manuscript. This discrepancy arises because we only ran the ablation for a single seed, whereas the results reported in the primary manuscript are the average of three different seeds.)

**Dataset** & **CIFAR-100** & **TinyImageNet** & **SuperImageNet-L** \\ 
**Data Size** & \(32 32\) & \(64 64\) & \(224 224\) \\  \(\#\)**Tasks** & \(10\) & \(10\) & \(10\) \\  \(\#\)**Classes per task** & \(10\) & \(20\) & \(5\) \\  \(\#\)**Samples per class** & \(500\) & \(500\) & \(7500\) \\ 
**LR** & All task start with 0.1 and exponentially decay to 0.01 \\ 
**Batch Size** & 32 & 32 & 32 \\ 
**Synthetic Batch Size** & 32 & 32 & 32 \\ 
**FL round per task** & 100 & 100 & 100 \\ 
**Local epoch** & 10 & 10 & 1 \\ 

Table 7: Parameter Settings in different datasets

 \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(\) & \(}\) & \(\) & \(}\) \\ 
0.1 & \(44.35\) & 0.1 & \(40.12\) & 0.0001 & \(43.10\) & \(110\) & \(42.39\) & 100 & \(40.77\) \\
0.5 & \(44.37\) & 1 & \(43.90\) & 0.001 & \(45.1\) & 200 & \(45.1\) & 5000 & \(45.1\) \\
1 & \(45.1\) & 10 & \(44.77\) & 0.01 & \(43.56\) & 1000 & \(45.01\) & 10000 & \(43.35\) \\
2 & \(44.08\) & 75 & \(45.1\) & 0.1 & 44.73 & & & & \\
5 & \(44.57\) & 100 & \(45.02\) & 1 & 44.37 & & & & \\ 

Table 8: Effect of different hyperparameters on the final \(}\) (in \(\%\)) for CIFAR-100 dataset.

This table shows how robust the final performance is with respect to each parameter, which is preferred both in federated and continual learning problems.

## Appendix H Comparison between MFCL and FedCIL

Here, we would like to highlight some distinctions between our algorithm and FedCIL, both of which aim to alleviate catastrophic forgetting using generative models.

* In FedCIL, **clients** train the local generative model **every round**, which adds great computational overhead. On the other hand, in our approach, the generative model is trained on the server and only **once per task**.
* Training models in GANs usually require a large amount of data that is not commonly available, especially on edge devices. Our data-free generative models address this issue.
* Training the generative model directly from the training dataset may pose a risk of exposing sensitive training data, which contradicts the goal of FL. On the other hand, MFCL uses only the information from the global model.
* FedCIL is limited to simpler datasets and FL settings, such as MNIST and CIFAR10, with fewer clients and less complex architectures. In contrast, our approach can handle more complex datasets, such as CIFAR100, TinyImageNet, and SuperImagenet, with a much larger number of clients.
* Training GAN models usually require more careful hyperparameter tuning. To train FedCIL for TinyImageNet and SuperImageNet, we tried SGD and Adam optimizers with learning rates \(\{0.1,0.05,0.01\}\) and local epoch \(\{1,2\}\). Furthermore, we adopt a generative model architecture with a similar input dimension and a total number of parameters in MFCL. However, the model did not converge to a good performance. While a more extensive hyperparameter search might improve the results, it can indicate the difficulty of the hyperparameter tuning of this algorithm. It is worth mentioning that in order to train the CIFAR-10 dataset, we used a local epoch \(8\) larger than the other baselines; otherwise, the performance on this dataset would also degrade.

In conclusion, FedCIL can be a good fit for a cross-silo federated learning setting with only a few clients, each possessing a large amount of data and computing resources. Meanwhile, while still applicable in the above setting, our method is also suitable for edge devices with limited data and power.