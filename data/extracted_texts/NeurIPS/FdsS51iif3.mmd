# PaintSeg: Training-free Segmentation via Painting

Xiang Li\({}^{1}\), Chung-Ching Lin\({}^{2}\), Yinpeng Chen\({}^{2}\), Zicheng Liu\({}^{2}\),

Jinglu Wang\({}^{2}\), Rita Singh\({}^{1}\), Bhiksha Raj\({}^{1,3}\)

\({}^{1}\)CMU, \({}^{2}\)Microsoft, \({}^{3}\)MBZUAI

x16@andrew.cmu.edu

###### Abstract

The paper introduces PaintSeg, a new unsupervised method for segmenting objects without any training. We propose an adversarial masked contrastive painting (AMCP) process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models. During the painting process, inpainting and outpainting are alternated, with the former masking the foreground and filling in the background, and the latter masking the background while recovering the missing part of the foreground object. Inpainting and outpainting, also referred to as I-step and O-step, allow our method to gradually advance the target segmentation mask toward the ground truth without supervision or training. PaintSeg can be configured to work with a variety of prompts, e.g. coarse masks, boxes, scribbles, and points. Our experimental results demonstrate that PaintSeg outperforms existing approaches in coarse mask-prompt, box-prompt, and point-prompt segmentation tasks, providing a training-free solution suitable for unsupervised segmentation. Code: https://github.com/lxa9867/PaintSeg.

## 1 Introduction

With deep learning advancements, significant progress has been made in the field of image generation and segmentation in recent years. A particular generative model, the denoising diffusion probabilistic model (DDPM), has demonstrated outstanding performance in a variety of generative tasks, such as image inpainting  and text-to-image synthesis . Similar developments have occurred in the field of object segmentation, such as the strong zero-shot capability and excellent segmentation quality demonstrated by SAM .

Image generation and segmentation can be mutually beneficial. Segmentation has been shown to be a critical technique in improving the realism and stability of generative models by providing pixel-level guidance during the synthesis process . Interesting to note is the fact that the relationship between segmentation and generative models does not appear to be solely one-sided. Generative models learning to "paint" objects actually know where the painted object is. The emergence of unsupervised image segmentation methods utilizing generative adversarial networks (GANs) has produced a line of methods that can segment objects in images  using generative models. These methods work on the assumption that object appearance and location can be perturbed without compromising scene realism. By using the GAN architecture to discriminate between perturbed and real images, these methods can achieve effective object segmentation. Moreover, a follow-up work  develops an approach to leverage pre-trained GAN by identifying "segmenting" direction in the latent space to discriminate object shapes.

In this paper, we present PaintSeg, an approach for unsupervised image segmentation that leverages off-the-shelf generative models. Unlike previous methods  that require training on top of these models, PaintSeg introduces a novel, training-free segmentation approach that relies on an adversarial masked contrastive painting (AMCP) process. The AMCP process creates a contrastbetween the original image and a painted image by alternating between inpainting and outpainting, with the former filling in the background and masking the foreground, and the latter retrieving the missing part of the object while masking the background and a portion of the foreground.

Both steps, as shown in Fig. 1, share the same operations while taking input from background and foreground masks, correspondingly. In the I-step, the object region is removed from the painted image, creating a significant contrast with the original image. Conversely, in the O-step, the background region exhibits a remarkable difference between the original and painted image. The foreground or background mask can be obtained by binarizing the contrastive difference in each step.

Although either I-step or O-step is capable of discriminating objects, the single-step method is less robust. The I-step involves segmenting objects based on background consistency without taking into account object information. As a result, the segmentation may be imperfect if the object part resembles the background. Similarly, in the O-step, only the object shape prior is utilized, resulting in a lack of background knowledge. This problem is addressed by introducing adversarial mask updating, in which I-steps and O-steps are alternated. During I-step, we only shrink the object mask to cut off background false positives, while during O-step, we expand it to link up foreground false negatives. Thereby, even if errors occur during the iteration of AMCP, they will be corrected in the next step without degradation. With the adversarial mask updating, the target mask can be gradually advanced to the ground truth.

With the robustness of AMCP, PaintSeg can deal with inaccurate initial masks and adapt to various visual prompts, such as coarse masks, bounding boxes, scribbles, and points. Compared to the recently published successes in image object segmentation study, our main contributions are as follows:

* We propose PaintSeg, a training-free approach to segmenting image objects based on heterogeneous visual cues. The method provides a direct bridge between generative models and segmentation.
* We introduce adversarial masked contrastive painting (AMCP), consisting of alternating I-step and O-step, to robustly segment objects.
* We conduct extensive experiments for analysis and comparisons on seven different image segmentation datasets, the results of which show the superiority and generalization ability of our methods.

## 2 Related Works

### Unsupervised Image Segmentation

Unsupervised methods for image segmentation are extensively investigated with the advancements in self-supervised. DINO  provides a self-supervised approach to explicitly bring out underlying semantic segmentation of images using a Vision Transformer (ViT) . Based on DINO, LOST , Deep Spectral Methods  and TokenCut  leverage self-supervised ViT features and propose to

Figure 1: **Illustration of adversarial masked contrastive painting (AMCP).** Given an input image and an initial mask, AMCP leverages alternating I-step and O-step to gradually refine the segmentation mask until it converges to the ground truth. Both steps share the same mask, paint, and contrast operations. The updated mask in each step is achieved by binarizing the contrastive difference between the original and painted images.

segment objects using NCut . Subsequently, [57; 55] introduce a second-stage training approach to further improve the segmentation quality. Found  incorporates background similarity as an additional refinement factor, while SelfMask  utilizes an ensemble of features [7; 8; 12] to enhance image representation. CutLER  enables multiple objects discovery capability by iteratively cutting objects with NCut and introduces a more powerful second-stage training. FreeSOLO  generates coarse masks with correlation maps that are then ranked and filtered by a "maskness" score. Another line of unsupervised methods learns to generate a realistic image by combining a foreground, a background and a mask [5; 71; 70; 59; 32; 18; 25] and then the object segmentor can be obtained as a byproduct.

### Prompt-guided Segmentation

Prompt-guided segmentation aims to segment objects assigned by prompts, e.g., mask, box, scribble and point. Semi-supervised video object segmentation (VOS) [1; 68; 38], aiming at segmenting object masks across frames given the first frame mask, is a typical mask-prompt task. The mainstream of VOS methods [72; 73] constructs pixel-level correspondence and propagates masks by exploring matches among adjacent frames. Interactive segmentation (IS) [78; 40; 58; 23] is another line of prompt-guided segmentation. IS permits users to leverage scribbles and points to assign target objects and segment them. In addition, an interactive correction is also featured by IS which introduces additional prompts to correct misclassified regions. MIS  is a recent work tackling unsupervised IS and proposes a multi-granularity region proposal generation to refine the mask. SAM  is a recently introduced zero-shot method for prompt-based segmentation which introduces a large-scale dataset and a strategy to mitigate the ambiguity of prompt. Beyond visual prompts, objects can also be referred by natural language or acoustic prompts. Referring image segmentation (RIS) [28; 74] and referring video object segmentation (R-VOS) [11; 76; 15; 35; 37] aims to segment objects in image/video referred by linguistic expressions. Audiovisual segmentation [77; 36] aims to segment sound sources in the given audiovisual clip.

### Conditional Image Generation

Conditional image generation refers to the process of generating images based on specific conditions or constraints. In most instances, the condition can be based on class labels, partial images, semantic masks, etc. Cascaded Diffusion Models  uses ImageNet class labels as a condition to generate high-resolution images with a two-stage pipeline of multiple diffusion models.  guides diffusion models to produce novel images from low-density regions of the data manifold. Apart from these, CLIP  has been widely used in guiding image generation in GANs with text prompts [21; 20; 79]. For diffusion models, Semantic Diffusion Guidance  investigates a unified framework for diffusion-based image generation with language, image, or multi-modal conditions. Dhariwal et al.  apply an ablated diffusion model to use the gradients of a classifier to guide the diffusion with a trade-off between diversity and fidelity. Additionally, Ho et al. introduce classifier-free guidance in conditional diffusion models by mixing the score estimates of a conditional diffusion model and a jointly trained unconditional diffusion model.

## 3 Problem Definition

We tackle the unsupervised prompt-guided image object segmentation task, which aims to predict the object mask \(M\{0,1\}^{1 H W}\) in an image \(I^{3 H W}\) given a visual prompt \(P\{0,1\}^{1 H W}\). The visual prompt can have a format of a point, a scribble, a bounding box or a coarse mask of the target object \(P\{P_{point},P_{scrib},P_{box},P_{mask}\}\). Following the convention, we assume the ground-truth object mask \(M\) must have an overlap with the visual prompt \(P M\).

## 4 Adversarial Masked Contrastive Painting

PaintSeg leverages adversarial masked contrastive painting (AMCP) to gradually refine the initial prompt \(P\) to the object mask \(M\). The AMCP approach is composed of alternating I-steps and O-steps, as illustrated in Figure 1. During each step, a region of the image is masked out based on the previous iteration's mask, and the masked region is then repainted and compared to the original image to refine the mask prediction. To improve the segmentation's robustness, PaintSeg introduces adversarialmask updating, which helps to ensure that the mask accurately reflects the object's boundaries. The I-step is used to shrink the object mask by leveraging background consistency, thereby eliminating false-positive regions. On the other hand, the O-step expands the object mask by utilizing object shape consistency to link up false-negative foreground regions.

### Contrastive Painting

We first discuss the rationality of segmenting objects by contrasting painted and original images. Given a visual prompt \(P\), the relation between the prompted area and object mask can be categorized into three types: background false positive (Fig. 2 (a)), foreground false negative (Fig. 2 (b)) and a hybrid of both. We tackle the prompt-guided segmentation by separately addressing the background false positive and foreground false negative with I-step and O-step respectively.

We discuss the painted content with different mask situations. To avoid ambiguity, we first denote the generative model taking background and foreground as conditions as inpainting model \(()\) outpainting model \(()\) respectively. We consider the prompted area as the initial mask \(M_{0}=P\). When the initial mask has false positives, i.e., \(M M_{0}\), as shown in Fig. 2 (a), the inpainted content tends to complete the background based on the background consistency. In this way, the inpainted pixels inside the object will have a significant difference compared to the original image. In contrast, when the initial mask has false negatives, i.e., \(M_{0} M\), as shown in Fig. 2 (b), the outpainted content tends to complete the partial object leading to a low difference with the original image inside object region. We notice that I-step can address background false-positive and O-step can address foreground false-negative. By alternating conducting I-step and O-step, we can leverage both foreground and background consistency and address more complicated cases.

### Contrastive Potential

Given the a image \(I\) and a mask \(M_{t}\), we define a contrastive potential \(\) to measure the region relations, which contains three terms

\[=_{paint}_{paint}+_{color}_{color}+_{prompt} _{prompt}.\] (1)

We introduce a box region \(B\) that encloses the foreground and define the \(_{paint}\) term as the distance between the painted and the original image. Specifically, \(_{paint}=B|(I)-(I_{paint})|_{2}\), where \(:^{3 H W}^{C H  W}\) is a function that projects the image to a high-dimensional space. \(\) denotes the Hadamard product.

The \(_{color}\) term measures the pixel-level color similarity inside and outside the mask \(M_{t}\). To compute it, we use the output of the conditional random field algorithm  and define \(_{color}=(M_{t} B,I B)\), where \(\) is a function that takes a mask and an image as inputs and outputs the probability of whether a pixel should belong to the masked region.

To further incorporate prompt information, we introduce prompt priors \(_{prompt}\) to the contrastive potential for box, scribble, and point prompts. Let us denote \([x_{l},y_{l}]\) as the coordinates of the \(l\)-th

Figure 2: **Illustration of I-step and O-step with initial prompts.** (a) We show an I-step with a box prompt as the initial mask, where the object region has a significant difference between the original and painted images. (b) We show an O-step with a scribble prompt as the initial mask, where the object region has a small difference between the original and painted images.

point in the prompt area. The prompt prior is defined as follows:

\[_{prompt}[i,j]=_{l}(x_{l},y_{l})[i,j],\] (2)

where \(\) is a two-dimensional Gaussian function, and \([i,j]=()^{2}}{_{x}^{2}}+)^{2} }{_{y}^{2}})\). Specifically, for the box prompt, we only take the center point of the box into account. The prompt priors are designed to leverage the positional information of the prompts to better locate the target object. By taking the maximum value of the Gaussian function over all points in the prompt area, \(_{prompt}\) captures the overall strength of the prompt signal.

### Adversarial I/O-Step

As shown in Fig. 1, I-step and O-step share the same mask, paint, and contrast processes while the input mask in I-step is the background mask and, in O-step, the foreground mask. Given the original image \(I\) and an input mask \(M_{t}\) from the \(t\)-th step of AMCP (assuming \(M_{t}\) is a background mask thus \(t+1\)-th step is an I-step), we first filter out the masked region by \(I M_{t}\) and then paint the image \(I_{paint}=(I M_{t})\). As discussed in Section 4.1, the foreground region will have a significant difference between painted and original images. We obtain the updated mask \(M_{t+1}\) by k-means clustering \(()\) over the contrastive potential \(\). Let us denote \(_{k}\) and \(S_{k}\{0,1\}^{H W}\) as the average value of all samples in the \(k\)-th cluster and its corresponding identity map (\(S_{k}[i,j]=1\) if pixel \([i,j]\) belongs to center \(k\) else 0). The updated mask can be found by

\[M_{t+1}=S_{k^{*}},\;k^{*}=_{k}_{k}.\] (3)

The updated mask \(M_{t+1}\) is a foreground mask thus the next step will be an O-step. Similarly, we paint the image by \(I_{paint}=(I M_{t+1})\). Here, the difference in background area will have a significant difference between painted and original images. Thereby, the updated mask \(M_{t+2}\) from O-step can be computed using the same rule as Eq. (3) which leads to a background mask. By updating the mask by Eq. (3), we notice that when the input mask \(M_{t}\) is a background mask, then the output mask will be a foreground mask and vice versa. Thereby, the alternating I-step and O-step can be automatically achieved.

As discussed in Section 4.1, I-step is advantageous for reducing false positives in the background, whereas O-step is beneficial for reducing false negatives in the foreground. Specifically, the updated mask is configured to only cut off pixels in the I-step, and to only link up pixels in the O-step. Let \(M_{t}^{+}\) and \(M_{t}^{-}\) as the dilated and eroded masks of \(M_{t}\). We constrain to only update the regions near the foreground-background boundary. In this way, the updating rule for AMCP can be rewritten as

\[M_{t+1}=S_{k^{*}}^{-}+_{t}(1-^{-}), \\ S_{k^{*}}^{+}+_{t}(1-^{+}),\\ ,\;k^{*}=_{k}_{k}\] (4)

where \(^{-}=_{t}-}^{-}\) and \(^{+}=M_{t}^{+}-M_{t}\) are the inner and outer neighbors of \(M_{t}\). Through the adversarial alternation of I-steps and O-steps, AMCP can handle more complex cases involving both false positives and false negatives. Due to the randomness inherent in generative painting, we paint the image \(N\) times in each step, and use the averaged mask as an output.

### Discussion

In this section, we introduce the mathematical formulation of AMCP. Mathematically, an image can be represented as a masked combination of a foreground image \(I_{F}\) and a background image \(I_{B}\)

\[I=I_{F} M+I_{B},\,M\{0,1\}^{H W 1}.\] (5)

\(M\) is a foreground mask. \(=1-M\). An inpainting model \([]\) is defined to generate pixels inside the mask given the pixels outside the mask as a condition. Similarly, an outpainting model \([]\) predicts pixels outside the mask given the pixels inside the mask as a condition. In our method, we aim to find a \(M\) that maximizes

\[*{arg\,max}_{M}-(I )^{-}\|_{d}}_{}+-[I M]^{+}\|_{d}}_{}\] (6)The first term aims to maximize the difference between the original image \(I\) and the inpainted image \((I)\) in the inner neighbor \(^{-}\) which corresponds to the I-step in AMCP. The second term aims to maximize the difference between the original image \(I\) and the outpainted image \([I M]\) in the outer neighbor \(^{+}\) corresponding to the O-step.

In each step, our mask, paint, and contrast operations can be considered as an expectation-maximization-like (EM-like) process with the latent variable of \(I_{paint}\) to maximize Eq. (6). On one hand, the \(I_{paint}\) is estimated by the mask and paint operations where the conditional probability \(p(I_{paint}|I,M)\) is characterized by the generative painting models (expectation step). On the other hand, the predicted mask \(M\) can be updated by maximizing the contrastive potential \(\) (maximization-tion step). Since the EM algorithm is sensitive to the initial value, solely updating with I-step or O-step cannot achieve robust performance. With the alternating I-step and O-step, we introduce an adversarial updating process which leads to a more robust mask estimation.

## 5 Experiment

### Datasets

For mask-prompt segmentation, we evaluate on DUTS-TE  and ECSSD . DUTS-TE contains 5,019 images selected from the SUN dataset  and ImageNet test set . ECSSD  contains 1,000 images that were selected to represent complex scenes. For box-prompt segmentation, we evaluate on PASCAL VOC  val set and COCO  MVAL datasets. COCO MVal contains 800 object instances from the validation set with 10 images from each of the 80 categories. For point-prompt segmentation, we use three datasets including GrabCut  which contains 50 images and corresponding segmentation masks that delineate a foreground object; Berkeley  which contains 96 images with 100 instances with more difficulty than GrabCut and DAVIS  which is a video dataset and 10% of the annotated frames are randomly selected, yielding 345 images that are used in the evaluation

### Experimental Setup

Evaluation metrics.In accordance with previous methods [30; 66], we evaluate segmentation quality using intersection over union (IoU).

Implementation details.We leverage the inpainting models trained with latent-diffusion pipeline  as our \(\) and \(\). We set the diffusion iterations to 50. We leverage DINO  pretrained VIT-S/8

   Method & Training & DUTS-TE  & ECSSD \\   \\ SelfMask \({}_{}\) & ✓ & 62.6 & 78.1 \\ SelfMask \({}_{}\) + BS  & ✓ & 66.0 & **81.8** \\ FOUND \({}_{}\) & ✓ & 63.7 & 79.3 \\ FOUND \({}_{}\) + BS  & ✓ & 66.3 & 80.5 \\
**PaintSeg** & & **67.0** & 80.6 \\  \\ Melas-Kyriazi et al. \({}_{}\) & 52.8 & 71.3 \\ LOST \({}_{}\) & 51.8 & 65.4 \\ LOST \({}_{}\) + BS  & 57.2 & 72.3 \\ DSS \({}_{}\) & 51.4 & 73.3 \\ TokenCut \({}_{}\) & 57.6 & 71.2 \\ TokenCut \({}_{}\) + BS  & 62.4 & 77.2 \\ SelfMask \({}_{}\) & 46.6 & 64.6 \\ FOUND’\({}_{}\) & - & 71.7 \\
**PaintSeg** & & **67.0** & **80.6** \\   

Table 1: **Qantitative results of coarse mask-prompted segmentation on DUTS-TE and ECSSD.** PaintSeg utilizes the coarse mask generated by unsupervised TokenCut  as prompt. BS denotes the application of the post-processing bilateral solver on the generated masks and the column ‘Learning’ specifies which methods have a training step. The best result per section is highlighted in **bold**. The second best result for each section is underlined. \(\) indicates the first-stage pseudo mask obtained without training.

 as our \(\). We use  as our \(()\) to calculate \(_{color}\). If no specification, for all experiments, the masked contrastive painting starts from the I-step and updates for 5 steps. We set the number of cluster centers to 3 in the first three steps for point, box and scribble prompts otherwise 2. We set \(_{paint}=0.8\), \(_{color}=0.2\) and \(_{prompt}=0.2\) if in I-step and \(_{prompt}=-0.2\) if in O-step. We average N=5 painted images to obtain the updated mask for each step. The \(_{x}\) and \(_{y}\) are set to \(\) of the width and height of the bounding box of the current stage mask respectively. \(^{+}\) and \(^{-}\) are the neighbors 32 pixels outside and inside the object boundary. We leverage dilation and erosion to filter out sparse points for each iteration. The kernel size is set to 5. For the mask and box prompts, we set the prompt as the initial mask. For the point and scribble prompts, we set the entire image as the initial masked region. The images are padded to \(512 512\) to fit the generative inpainting model.

### Main Results

Coarse mask prompt.Since the usage of the ground-truth coarse mask as a prompt is rare, we evaluate PaintSeg on two unsupervised salient object detection benchmarks and leverage the coarse mask generated from TokenCut  as our prompt. As shown in Table 1, PaintSeg achieves encouraging performance that is even comparable with training-based methods. Under the training-free setting, PaintSeg significantly outperforms previous methods by a margin of 4.6 IoU on DUTS-TE and 3.4 IoU on ECSSD. We attribute the performance improvement to the error correction capability of PaintSeg. With alternating between I-step and O-step, the proposed PaintSeg can handle noisy prompts effectively. The robustness of PaintSeg will be discussed in more detail in Section 5.4.

Point prompt.As shown in Table 2, we compare our method with state-of-the-art point prompt segmentation approaches. PaintSeg consistently outperforms the training-free methods. Even compared to training-based methods with ground truth supervision, PaintSeg still achieves the best performance on GrabCut and DAVIS datasets. MIS  is an unsupervised approach equipped with second-stage training. We notice that our method can significantly outperform it in terms of IoU, with improvements of 8.2, 6.8, and 16.1 on GrabCut, Berkeley, and DAVIS correspondingly.

Box prompt.Since there is no unsupervised box-prompted segmentation that can be directly compared, we compare the proposed method with several baselines including TokenCut , CutLER  and MaskRCNN . We first cut off the ground truth box region and then run the baselines. As shown in Table 3, when compared with training-based Mask-RCNN and CutLER, PaintSeg shows suboptimal performance, which can be explained by the lack of training to handle complex

   Method & Training Supervision & GrabCut & Berkeley & DAVIS \\  & _— Compared to methods with training_ & & & \\ DIOS\({}_{}\) & ✓ & ✓ & 64.0 & 66.0 & 57.8 \\ RITM \({}_{}\) & ✓ & ✓ & 81.0 & **77.7** & 66.0 \\ MIS \({}_{}\) & ✓ & & 76.2 & 63.2 & 53.3 \\
**PaintSeg** & & **84.4** & 70.0 & **69.4** \\  &  & \\ Random Walk \({}_{}\) & & 25.7 & 26.2 & 20 \\ GrowCut \({}_{}\)\({}_{}\) & & 26.7 & 26.2 & - \\ GraphCut \({}_{}\) & & 41.8 & 33.9 & 20 \\
**PaintSeg** & & **84.4** & **70.0** & **69.4** \\   

Table 2: **Qantitative comparison of point-prompted segmentation on GrabCut, Berkeley, and DAVIS. The point prompt is given as the centroid of each object.**

   Method & Training Supervision & PASCAL VOC & MVal \\  & _— Compared to methods with training_ —} & & \\ Mask-RCNN \({}_{}\) & ✓ & ✓ & **73.2** & **79.4** \\ CutLER \({}_{}\) & ✓ & & 63.5 & 74.8 \\
**PaintSeg** & & 59.7 & 69.6 \\  &  & \\ TokenCut \({}_{}\) & & 30.2 & 34.7 \\
**PaintSeg** & & **59.7** & **69.6** \\   

Table 3: **Qantitative comparison of box-prompted segmentation on PASCAL VOC and COCO MVal.**scenarios. However, as MaskRCNN is trained on 80 COCO object categories, the "unseen" gap remains substantial. PaintSeg provides an alternative solution that is not reliant on training, thus making it more general and capable of handling new categories of objects. When compared with unsupervised approaches, our method eclipses TokenCut by a large margin on both PASCAL VOC and COCO MVal datasets.

Qualitative results.We visualize the qualitative results with point and coarse mask prompt in Fig. 3. Our visualization depicts comparably reliable results. Comparatively, PaintSeg segments a relatively complete object, while baselines miss some parts of it.

### Analyses

Module effectiveness in AMCP.We step by step add proposed modules in AMCP to validate the effectiveness. As shown in Table 4, we report the results on ECSSD with coarse-mask prompts. We observe that the missing of either step impacts the performance, as evidenced by the significant drop in IoU (compared to alternating I-step and O-step). With the adversarial mask updating constraint, AMCP achieves the best performance of 80.6 IoU.

Robustness of AMCP with different prompts.In Table 5, we add noise to the initial prompt by randomly shifting the position to investigate the robustness of AMCP. The scale of random noise is determined, w.r.t., half the length of the diagonal of the ground-truth bounding box. We observe that AMCP remains robust and only shows a slight performance drop with a noise rate of less than 30%. The robust capability can be attributed to 1) the alternating I-step and O-step to leverage both background and object shape consistency, and 2) the adversarial mask updating to tackle the background false-positives and foreground false-negatives.

Design choices in AMCP.We conduct experiments to ablate the design choices in AMCP and their impacts on the segmentation performance. We first study the effect of cluster center numbers for quantizing contrastive potential. With a larger cluster center, AMCP will ignore more ambiguous regions. As shown in Table 5(a), we notice a cluster center of 2 achieves the best performance for mask prompt. After that, we ablate on the AMCP step number in Table 5(b). The segmentation performance keeps increasing until reaching a step number of 5. In this way, we choose 5 as our step number. As we leverage the diffusion-based generative model, we ablate the iterations for the diffusion process as

   I-step & O-step & AC & IoU \\  ✓ & & 78.4 \\  & ✓ & 77.9 \\ ✓ & ✓ & 79.5 \\ ✓ & ✓ & 80.6 \\   

Table 4: **Module effectiveness in AMCP**. AC: adversarial constraint for mask updating.

Figure 3: **Qualitative results of baselines and our PaintSeg with point and mask prompts. Green point denotes the point prompt. The mask prompt is generated by unsupervised TokenCut . BS represents the bilateral solver . We compare with RITM  and TokenCut .**

it can impact the image quality. As expected, Table 6c demonstrates that a larger iteration number can reach a better performance. To filter out irrelevant background regions, we crop a box region wrapping the given object mask to contrast images. We ablate the box size in Table 6d. We notice that a box slightly larger than the bounding box to the given mask can achieve the best performance. An explanation for this could be that a box tightly enclosing an object will result in a high proportion of object region, which may dominate the features and lead to ambiguity. Properly introducing background can make the extracted features more discriminative and easier for clustering.

Visualization of mask updating.To better illustrate the iterative process of AMCP, as shown in Fig. 4, we visualize the averaged mask output (among \(N\) painted images in each step) for each step with a box prompt. As the given mask only contains background false positives, I-step plays a major role to cut false-positive backgrounds in AMCP. The mask shrink can also be observed after the O-step which is due to the binarization of the averaged mask from the I-step instead of contrastive painting. We observe that the updated masks are gradually closer to the mask of the target object with AMCP.

## 6 Conclusion

To conclude, PaintSeg bridges the gap between generative models and segmentation. It is designed to provide a robust and training-free approach to unsupervised image object segmentation. With the proposed adversarial masked contrastive painting (AMCP) process, PaintSeg creates a contrast between the original image and the painted image by alternately applying I-steps (npainting) and O-steps (outpainting). The alternating I-step and O-step gradually improve the accuracy of the object mask by leveraging consistency in the background and the shape of the object. The competitiveness of our method on seven different image segmentation datasets suggests that PaintSeg can deal with inaccurate initial masks and adapt to various visual prompts, such as coarse masks, bounding boxes,

Table 6: **Design choices for AMCP. We report the performance with the coarse-mask prompt on ECSSD. (a) We ablate the cluster center when contrasting. (b) We ablate the step number for AMCP. (c) We ablate the diffusion iteration for generative painting. (d) We ablate on the cropped box size when contrasting. The rate denotes the proportion of cropped mask and the box of the current stage object mask.**

Figure 4: **Iterative process of AMCP with box prompt. We inverse the outputted background mask in I-step for better comparison. We only visualize the box prompted area.**

scribbles, and points. An extensive ablation analysis indicates a number of key factors and advantages of the proposed model, including its design choices and generalizability.

Limitation.In spite of PaintSeg's high performance for training-free image segmentation with heterogeneous visual prompts, it does not possess object discovery capabilities and therefore cannot automatically recognize instance-level masks in an image. Developing discovery capability can be achieved by conducting second-stage training on the segmentation results generated by PaintSeg, which is our future research focus.