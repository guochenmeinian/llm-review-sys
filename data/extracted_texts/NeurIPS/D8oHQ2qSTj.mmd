# Fairness-guided Few-shot Prompting for Large Language Models

Huan Ma1,2, Changqing Zhang1, Yatao Bian2, Lemao Liu2, Zhirui Zhang2,

**Peilin Zhao2, Shu Zhang2, Huazhu Fu3, Qinghua Hu1, Bingzhe Wu2\({}^{*}\)**

\({}^{1}\) College of Intelligence and Computing, Tianjin University, Tianjin, China

\({}^{2}\) AI Lab, Tencent, Shenzhen, China

\({}^{3}\) Institute of High Performance Computing (IHPC),

Agency for Science, Technology and Research (A*STAR), Singapore

\({}^{1}\) zhanchangqing@tju.edu.cn; \({}^{2}\) bingzhewu@tencent.com

Corresponding author

###### Abstract

Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context learning. We perform comprehensive experiments with state-of-the-art mainstream models such as GPT-3 on various downstream tasks. Our results indicate that our method can enhance the model's in-context learning performance in an effective and interpretable manner. Code is available at: https://github.com/MaHuanAAA.

## 1 Introduction

Large language models (LLMs), such as GPT-3  and BLOOM , have demonstrated remarkable ability in performing in-context learning (ICL) on downstream tasks. ICL refers to the process of conditioning an LLM to solve various downstream tasks using prompts constructed from a few demonstration input-output pairs  (i.e., few-shot prompting). Despite its impressive performance, prior research has shown that ICL suffers from high instability due to variations in the choice of in-context demonstrations, demonstration order, and prompt formats [4; 5]. Therefore, constructing an appropriate prompt has been identified as a critical factor for improving the performance of ICL .

Previous research studies this problem typically from two directions: (1) prompt tuning in the embedding space [7; 8; 9; 10; 11], (2) prompt searching in the text space [4; 12; 13; 14; 15; 16]. The key idea of prompt tuning is to inject task-specific embedding into hidden layers and then tune these embeddings using gradient-based optimization [8; 15]. However, these methods require to modify the original inference process of the model, which is impractical for the case of black-box LM servicessuch as GPT3 and ChatGPT . Furthermore, prompt tuning introduces additional computational and storage costs, which is typically expensive for LLM. A more feasible and efficient way is to optimize prompting via searching approximate demonstration samples and ordering in the original text space . Bunch of works are presented to constructs prompts from either "global" or "local" views. On the one hand, global-view based methods typically optimize the different elements of the prompt as a whole, with the aim of achieving superior performance. For example, one approach, as described in , constructs a search procedure that leverages the overall diversity of demonstrations. Another approach  attempts to optimize the ordering of the entire set of demonstrations to achieve better performance. In contrast to the global view, local-view based methods optimize each individual demonstration by designing different heuristic selection criteria such as prior work KATE . These methods have achieved impressive improvements on a wide range of tasks. However, most of them still suffer from the following limitations: (1) Most of current research mainly focuses on searching prompts along a single dimension, such as example selection or order. However, the overall influence of various dimensions on the performance remains unclear. (2) These methods are typically based on heuristic criteria, and there is a gap between them and actual performance. A unified view that explains how these methods work is needed. (3) More importantly, existing methods optimize prompts globally or locally, which may lead to suboptimal performance.

In this paper, we revisit this problem from the perspective of _predictive bias_. We find a key insight that the quality of a given prompt depends on its inherent bias. Based on this insight, we propose a surrogate metric based on predictive bias for evaluating the quality of prompts. This metric allows us to evaluate a prompt in a single forward process without an additional development set. Specifically, we apply a given prompt to a "content-free" input and expect the model output an uniform predictive distribution (a content-free input contains no useful information). Therefore, we employ the uniformity of the predictive distribution to characterize the bias of a give prompt. This shares a similar idea to the prior work which uses this metric to calibrate the model output . In contrast to this work which mainly focus on using this metric for calibration when the prompt is fixed, we further explore its usage in automatically searching an approximate prompt. Moreover, through extensive experiments, we empirically validate the correlation between the inherent bias of a given prompt and its quality measured by the average task performance on a given test set (see Fig. 2).

Moreover, this bias-based metric allows us to build prompting optimization techniques in a "local-to-global" manner. We present two novel strategies for efficiently searching high-quality prompts in a bias-guided way: (1) T-fair-Prompting (2) G-fair-Prompting. We focus on a general setting where a labeled set with size \(N\) is given. The goal of our strategies is to perform combinatorial optimization over this set to find near-optimal prompts (i.e., select demonstrations and their orders). Specifically, T-fair-Prompting uses an intuitive way that first computes the bias of each single demonstration (i.e., one-shot prompting) and then select the top-k fair demonstrations to form the final prompts. This strategy can be efficiently done with a complexity of \(O(N)\). Note that T-fair-Prompting is based on the assumption that the optimal prompt is usually constructed from demonstrations with the smallest individual bias. However, this may not hold true in real situations and often leads to sub-optimal solutions. Therefore, we further introduce G-fair-Prompting to improve the search quality. G-fair-Prompting follows the normal procedure of the greedy search which finds the optimal solution by making locally optimal choices at each step. At each step of the algorithm, the selected demonstration is the one which makes the updated prompts achieves the best fairness score.This strategy trades off the quality of the search with the worst-case time complexity. By accepting a higher worst-case time complexity of \(O(N^{2})\), the search quality is significantly improved. Note that G-fair-Prompting works from a local to global perspective, wherein bias of individual samples are considered in the early stages while the later stage focus on the reduction of global predictive bias.

To evaluate the effectiveness of our strategies, we conduct extensive experiments with current mainstream models, such as GPT-3 , on various downstream tasks. Our results indicate that our method can significantly enhance the model's in-context learning performance in an effective and interpretable manner. The overall contribution is summarized as follows:

* We introduce to use the predictive bias to assess the quality of a given prompt in an efficient and development set independent way and the empirical effectiveness of this metric is comprehensively validated.
* Based on the above idea, we propose two efficient and effective strategies, namely, T-fair-Prompting and G-fair-Prompting to optimize the prompts.

* The effectiveness of these two strategies are validated on various LLMs ranging from GPT-series models to LLaMA family  released by Meta recently. Consistent relative improvements of over \(10\%\) have been observed over different downstream tasks in contrast to SOTA methods.

**Relation to Calibration-before-use:** Our paper shares a similar metric with cal-before-use  to asses the predictive bias of a given prompt. However, the prior approach aims to use this metric to calibrate the output, which can be still easily affected by the quality of the used prompt (more results can be found in Table 2). In contrast, our research aims to find a near-optimal prompt on the original space to improve the model's performance, without requiring any post-adjustment to the output of the model. Moreover, we have firstly empirically validated the connection between predictive bias and the final task performance as shown in Fig. 2, which has not been studied in . Through experiments, we have discovered that, even without calibration, the prompt selected by our method can outperform a randomly selected prompt with calibration.

## 2 Related Work

In-context LearningPrevious research, as cited in [1; 20], has demonstrated that Large Language Models can complete tasks with zero- or few-shot learning using in-context learning. LLMs perform well with an appropriate prompt. However, recent works [4; 18] has shown that the performance of LLMs is affected by the prompt used. Therefore, determining the optimal prompt is a crucial and fundamental research area.

**Original space searching** A more intuitive approach for determining the best prompt is to search in the original space by selecting or reordering the prompt sentences entered by users. The searching can be concluded in two perspective. \(\)**Global view**: A naive strategy is to _enumerate_ all candidates to find the prompt that can achieve the best performance on validation set, but this strategy is computationally expensive since its complexity is \(_{k=1}^{n}C_{n}^{k}k!\) considering to demonstration selection and order permutation, where \(k\) represents the number of demonstrations selected, and \(C\) signifies the combinatorial function. Zhang et al.  find that errors frequently fall into the same cluster, where each cluster contains similar questions, so they proposed a _diversity-guided_ searching strategy to select diverse demonstrations. In addition to demonstrations selection, Lu et al.  have identified the impact of the prompt _order_ on the results. They found the best sequence which yields the most diverse prediction results on the probing set by generating a probing set through LLMs. However, this method is also computationally expensive, and it may be difficult to ensure that the generated probing set is sufficiently balanced. \(\)**Local view**: Previous studies  show that reducing the model's _uncertainty_ helps improve the model's performance, and  propose Active Prompting to select demonstrations according to the uncertainty of LLMs. KATE  selects the prompt based on the _distance_ amongst embeddings, with the goal of selecting the closest example. However, this method ignores the influence of the order of the examples and requires access to sentence embeddings.  demonstrate that LLMs can be easily distracted by irrelevant context, accordingly they identify several approaches for _filtering_ out irrelevant information in context.

In the realm of original space searching, most of the current methods tend to focus solely on the influence of a singular factor (highlighted above) on performance, utilizing heuristic metrics to select context demonstrations that perform well according to this criterion. While these investigations certainly bring benefits to the community, they lack a comprehensive consideration of both local and

Figure 1: ICL suffers from high instability due to high variations in demonstrations selection and order, even when post calibration is performed.

global perspectives. The method proposed offers a metric to select context demonstrations from the perspective of predictive bias, which naturally facilitates a transition from local view to global view.

## 3 Revisiting the Sensitivity across Demonstrations

In this section, we will clarify the notations and the templates used in this paper. Then, we will demonstrate some brief empirical results to show how different demonstration construction factors (e.g., example selection and order) affect performance. We further introduce the definition of predictive bias/fairness of a given prompt and show its connection to the predictive performance on different downstream tasks.

### Notations

We consider a training set consisting of \(N\) samples \(S=\{(x_{i},y_{i})\}_{i=1}^{N}\), where \(x_{i}\) is the sentence and \(y_{i}\) is the label of the \(i^{th}\) training sample, and \(\) is the space of all labels for the task. We use a template \(()\) to transform these sentences and labels into natural language space (i.e., prompt construction). Take an instance from the AGNews dataset  for example, we have \(x_{i}=,\ y_{i}=\), and \((x_{i},y_{i})\) is _"Article: Cubans Risking Life for Lure of America. Answer: World"_. We concatenate these demonstrations to form a prompt \(\), which by default is \(=(x_{1},y_{1})(x_{n},y_{n})\), where \(\) indicates sentences combination option. At test time, we append the prompt \(\) with \(=\). _Answer: "_ and feed it to a large language model \(\). The predicted class is given by:

\[=_{y}(y|),(y| )=(y|)}{_{y} (y|)},\] (1)

where \((y|)\) indicates the probability predicted by LLM, and the probability is normalized to fit the task. We denote the predictive distribution by \((x):=\{(y|)|y\}\). In this paper, we focus on evaluating the instability caused by demonstrations, and we fix the prompt template following prior work .

### Stability of Few-shot Prompting

As demonstrated by prior research, the few-shot prompting technique is highly susceptible to a variety of factors, including the selection and order of demonstrations [4; 18]. In this study, we delve deeper into the stability of few-shot prompting, specifically focusing on the recently released LLaMA family

Figure 2: Accuracy is highly consistency with fairness and greedy search can find a good prompt, where “Random” and “Oracle” indicate the average accuracy of all prompts and the upper-bound performance according to fairness.

by Meta . Additionally, we evaluate the stability of LLaMA models calibrated using the current state-of-the-art method [12; 15].

To elucidate the impact of demonstration selection, we select four demonstrations for each different seed and randomly sample an order for each combination. Subsequently, we present the performance on AGNews in the form of a boxplot, which displays the data distribution based on a five-number summary (minimum, first quartile [Q1], median, third quartile [Q3], and maximum). As shown in Fig.1(a)(b), the accuracy demonstrates significant variability across various demonstrations. The detailed settings please refer to Appendix A.7.

To investigate the influence of permutations, we examine all possible permutations of four fixed demonstrations, resulting in \(4!\) distinct candidates. Fig.1(c)(d) also reveals a high degree of variance. While post-calibration contributes to mitigating instability, it is essential to note that the model remains sensitive even after post-calibration. This finding underscores the importance of meticulous demonstration selection. In subsequent experiments, we discover that our approach can be employed to further enhance the performance of the calibrated model.

### Predictive Bias of ICL

As demonstrated in the preceding discussion, the performance of ICL is significantly impacted by various factors such as demonstration, permutation, and selection (refer to Appendix A.4 for additional information). Consequently, devising an efficient method for constructing an appropriate prompt with near-optimal performance is a crucial step in deploying LLMs for diverse downstream tasks. As outlined in the introduction, numerous studies aim to optimize prompts in ICL. This paper further investigates this issue through the lens of predictive bias, which refers to the discrepancy between targeted classes. 2

To achieve this, we initially introduce an efficient technique to assess the inherent predictive bias of a given prompt, drawing inspiration from previous work . We construct a training set-independent metric to measure predictive bias as follows: first, we merge the provided prompt with "semantic-free" test sample information (e.g., "[N/A]", denoted by \(\)) and obtain the LLM's predictive distribution for this sample. Ideally, the predictive distribution should closely resemble a uniform distribution, as the test sample lacks semantic information. In this paper, we employ entropy as a measure of predictive bias, defined as:

\[()=-_{y}p(y|) p(y| )\] (2)

Previous studies have utilized this metric to calibrate the model's output. In this paper, we conduct a comprehensive examination of the relationship between predictive bias and overall performance. Specifically, in a scenario with four training samples (due to the time-consuming nature of enumerating all prompt cases for a larger number), we enumerate all possible combinations and permutations of demonstrations for various datasets and LLMs. Subsequently, we arrange all candidates in descending order based on fairness, where an "index 0" denotes the prompt with the highest fairness. We perform experiments using five different seeds, resulting in training sets comprising distinct demonstrations while maintaining the test samples with seed 0. Fig. 2 displays the results for different models, revealing a strong correlation between the model's performance and fairness score (i.e., fairer prompts yield better performance). The red star, referred to as the "Oracle" represents the optimal average performance, which consistently correlates with higher fairness. This observation prompts us to enhance the ICL performance by identifying the fairest prompt.

Nevertheless, discovering the fairest demonstration combination proves to be a formidable challenge, given the existence of \(_{k=1}^{N}C_{N}^{k}k!\) distinct candidates. As the size of the training set increases, this task becomes intractable. In order to tackle this problem, we propose two efficient strategies for approximating the most suitable demonstrations in the subsequent section.

## 4 Fairest Prompt Search

Drawing upon the aforementioned observations, we propose two strategies aimed at identifying the most fair prompt, which have been empirically demonstrated to achieve superior performance. Let us consider a training set \(S\) comprising \(N\) samples; the goal of these search strategies is to select a subset of samples from the training set and construct the context in a specific order so as to optimize the fairness criterion in Eq. 2.

In an ideal scenario, we would consider the factors of demonstration selection and order permutation by examining \(_{k=1}^{N}C_{N}^{k}k!\) distinct candidates, which enumerates all possible situations. However, evaluating every candidate is infeasible, as demonstrated when \(N=8\), yielding over \(10^{6}\) candidates. In this paper, we introduce two search strategies to reduce computational cost: T-fair-Prompting and G-fair-Prompting. The T-fair-Prompting strategy decreases complexity from \((_{k=1}^{N}C_{N}^{k}k!)\) to \((N)\), but its performance hinges on the selection of \(k\) and may be unstable when an unsuitable value of \(k\) is chosen. As a result, we propose an additional greedy search strategy, termed G-fair-Prompting, which lowers complexity to \(O(N^{2})\) and offers a superior approximation of the oracle solution. Fig. 9 visualizes the computational costs over different training set size.

### T-fair-Prompting

The central idea of T-fair-Prompting (top-k) is founded on the heuristic understanding that the fairest prompt usually consists of demonstration samples with reduced individual biases. Consequently, T-fair-Prompting constructs the prompt through a two-stage process. Initially, the prediction bias is assessed when the prompt is formulated using individual demonstrations. Subsequently, the top-\(k\) fairest demonstrations are chosen and employed to prompt the LLM. It is important to note that fairer demonstrations are likely to be situated towards the end of the sequence, as the generation is more influenced by proximate demonstrations, in accordance with prior research . A comprehensive description of the process is presented in Algorithm 1, while a visual representation can be found in Fig. 3. Specifically, when \(k\) is equivalent to the size of the training set, the method degrade to a search for the optimal order of demonstrations. Nevertheless, T-fair-Prompting is heavily reliant on the chosen value of \(k\). More crucially, T-fair-Prompting addresses this issue through a purely local perspective, thereby neglecting considerations from a global standpoint, which typically results in sub-optimal outcomes. As a result, we subsequently introduce the G-fair-Prompting method, which operates in a local-to-global fashion, as described below.

Figure 3: Overview of Most-fair Prompting.

### G-fair-Prompting

The G-fair-Prompting (greedy search) algorithm adheres to the standard procedure of greedy search, which seeks the optimal solution by making locally optimal choices at each stage. In each step of the algorithm, the chosen demonstration is the one that allows the updated prompts to achieve the highest fairness score. This strategy balances the quality of the search with the worst-case time complexity. By accepting an increased worst-case time complexity of \(O(N^{2})\), the search quality is significantly enhanced. It is important to note that the G-fair-Prompting algorithm operates from a local to global perspective as shown by Algorithm 2. During the initial stages, the bias of individual samples is taken into account, while the later stages focus on reducing global predictive bias. Specifically, at each step, we insert a new demonstration \((x_{i},y_{i})\) from the remaining demonstration set \(^{}\) (ensuring demonstrations are not repeated) at the beginning of the current context \(\) and select the demonstration that maximizes the fairness improvement. Formally, at step 9 in Algorithm 2, the inserted demonstration should satisfy the following criterion:

\[_{x_{i}^{}}((x_{i},y_{i}) )((x_{i},y_{i}))>().\] (3)

```
1:Given: training set \(S=\{(x_{i},y_{i})\}_{i}^{N}\), pretrained LLM \(\), transformation template \(()\), and context-free input \(\)
2:Initial prompt \(\)
3:for\((x_{i},y_{i})\) in \(S\)do
4: Inference \(\{(y|(x_{i},y_{i}))|y\}\) via \(\)
5: Calculate the \(((x_{i},y_{i}))\) according to Eq. 2
6:endfor
7:Sort \(_{i=1,,N}((x_{i},y_{i}))\) in descending order
8:for\(d\) in \(1,,k\)do
9:Insert the most \(d\) fair demonstration at the head of \(\)
10:endfor
11:return\(\) ```

**Algorithm 1** T-fair-Prompting

## 5 Experiments

### Experimental Setup

**Models.** There are a large number of available LLMs (Appendix A.2) including open-source models and black-box cloud API. Recently, Meta has released their powerful pretrained LLMs, LLaMA. LLaMA models with 13B parameters can achieve comparable performance in contrast to BLOOM and GPT-3 with much larger model size. In this paper, we evaluate the effectiveness of our method on BLOOM (176B) and LLaMA models of different sizes. We have opted to employ LLaMA (65B) as a substitute for GPT-3 in our experiments, since openai strictly restricts the API access to certain areas.

**Datasets.** We conducted experiments on various text classification datasets , namely SST-2, AGNews, CoLA, TREC, and RTE. Furthermore, the maximum input length of LLaMA is 512, and the sentences in RTE are too long for LLaMA. The task descriptions and statistics are available in Table 6 in Appendix.

### Results

We conducted experiments on different settings and reported the results of five runs. We compared our method with the diversity-guided searching strategy proposed by Zhang et al. (Global view) and the similarity-guided searching strategy proposed by Liu et al. (Local view). Note that methodsbased on local view are time-consuming since they require searching different demonstrations for every test example. Table 1 shows the performance of the different strategies, where "Random" indicates the average accuracy for enumerating all situations, "Diversity" and "Similarity" indicate demonstrations are selected according to diversity and similarity (details please refer to Appendix A.6), respectively. For each dataset, we set the size of the training set to 4. "Diversity" and "Similarity" select 4 from 16 demonstrations, as they need more candidates. The baseline is expensive to compute since enumerating all candidates for 4 demonstrations in RTE on BLOOM will take more than 120 NVIDIA A100 GPU hours. We enumerate all candidates for the training set with 4 demonstrations on different models, as shown in Fig. 2. The results on models whose parameters less than 13B are shown in Table 4 (i.e., GPT2-XL (1.5B), LLaMA (7B), and LLaMA (13B)).

\(\)**G-fair-Prompting can reach a close approximation of enumeration.** To evaluate whether the G-fair-Prompting (Greedy) method can approximate the best performance of enumerating all candidates, we marked the performance of G-fair-Prompting with a green star (representing the closest value to averaged accuracy of G-fair-Prompting on the line). We found that G-fair-Prompting can achieve a very close approximation to enumeration. As shown in Fig. 2, most prompts searched by G-fair-Prompting achieved a top \(20\%\) ranking, and on BLOOM (176B), G-fair-Prompting almost found the most fair prompt.

\(\)**G-fair-Prompting outperforms T-fair-Prompting.** As shown in Table 1, although T-fair-Prompting achieves better performance compared with random selection, G-fair-Prompting consistently outperforms T-fair-Prompting. Furthermore, Top-2 significantly outperforms Top-4 in most cases (over \(5\%\)), indicating that the number of demonstrations selected is crucial. Overall, the results demonstrate that G-fair-Prompting achieves satisfactory performance with only a slight additional cost.

\(\)**Compared with SOTA methods.** We compared our methods with several State-of-the-Art (SOTA) methods, including diversity-guided and similarity-guided techniques. We observed that our **greedy** approach outperforms most of these SOTA methods in most situations, and the improvements of over \(10\%\) are observed on dataset TREC. The similarity-guided method, on the other hand, achieved the best performance on the topic classification task (AGNews). This is because it searches for a unique prompt for every different test example based on the dis

    &  &  &  &  & } &  **Ours** \\ **Top-4** \\  } & } \\   & SST2 & \(92.7_{2.3}\) & \(95.0_{0.9}\) & \(94.0_{0.9}\) & \(94.6_{0.5}\) & \(93.8_{2.1}\) & \(91.2_{4.0}\) \\   & AGNews & \(73.9_{5.9}\) & \(70.2_{10.1}\) & \(74.8_{3.8}\) & \(75.4_{2.2}\) & \(74.8_{2.3}\) & \(79.6_{1.4}\) \\   & TREC & \(47.9_{14.6}\) & \(46.0_{8.7}\) & \(31.4_{3.1}\) & \(55.4_{13.3}\) & \(39.2_{19.3}\) & \(66.8_{2.5}\) \\   & RTE & \(62.4_{4.2}\) & \(69.2_{1.9}\) & \(67.2_{3.5}\) & \(55.6_{1.0}\) & \(57.6_{1.9}\) & \(63.0_{2.1}\) \\   & CoLA & \(68.4_{4.8}\) & \(71.0_{3.7}\) & \(69.8_{2.5}\) & \(66.4_{8.6}\) & \(66.8_{3.7}\) & \(68.2_{6.2}\) \\   & SST2 & \(82.5_{11.8}\) & \(90.0_{2.7}\) & \(72.8_{4.4}\) & \(82.0_{11.1}\) & \(80.0_{12.2}\) & \(85.6_{8.2}\) \\   & AGNews & \(75.2_{5.0}\) & \(75.0_{5.1}\) & \(75.0_{2.4}\) & \(73.2_{3.9}\) & \(69.8_{4.4}\) & \(76.4_{4.6}\) \\   & TREC & \(68.1_{11.1}\) & \(68.2_{4.7}\) & \(60.6_{3.4}\) & \(71.4_{11.1}\) & \(57.8_{17.3}\) & \(80.2_{5.3}\) \\   & CoLA & \(66.9_{11.0}\) & \(68.8_{6.8}\) & \(72.8_{2.0}\) & \(63.8_{13.3}\) & \(69.8_{3.9}\) & \(70.6_{4.2}\) \\   & SST2 & \(90.0_{7.7}\) & \(90.8_{9.0}\) & \(87.4_{3.1}\) & \(88.2_{8.6}\) & \(95.8_{1.5}\) & \(87.8_{9.0}\) \\   & AGNews & \(76.8_{5.0}\) & \(78.2_{3.1}\) & \(78.2_{1.8}\) & \(77.0_{3.4}\) & \(76.2_{4.9}\) & \(76.0_{4.0}\) \\    & TREC & \(63.6_{14.2}\) & \(65.2_{10.9}\) & \(64.0_{5.5}\) & \(65.8_{13.0}\) & \(57.4_{19.9}\) & \(74.0_{12.2}\) \\    & CoLA & \(66.2_{9.8}\) & \(62.6_{8.6}\) & \(59.2_{14.0}\) & \(67.6_{11.7}\) & \(62.6_{6.5}\) & \(72.0_{4.5}\) \\   

Table 1: Accuracy for different prompting strategies (averaged on \(5_{(0,,4)}\) different seeds, where Top-\(k\) and Greedy indicate T-fair-Prompting with \(k\) demonstrations and G-fair-Prompting respectively).

Figure 4: BLOOM is not sensitive to CoLA.

tance between the embeddings of the training samples and the test example. This strategy selects demonstrations with labels that are the same as the test samples, and Language Models (LLMs) tend to predict biased predictions toward the labels that always appear in the context. However, the similarity-guided method may prove inadequate when applied to other tasks. Specifically, the similarity-guided strategy exhibits lower performance compared to random selection in QC and acceptability tasks. Furthermore, the G-fair-Prompting approach may occasionally falter when the model's sensitivity to the task is not immediately evident, as observed in the acceptability task on BLOOM (depicted in Fig. 4). Note that the training set size of compared methods is \(4\) larger than ours.

\(\)**Comparison with Calibration Method.** Post-calibration , can enhance the accuracy of a given prompt in most cases. However, when the selected prompt is of poor quality, the performance may remain inadequate even after calibration. We compared the performance of G-fair-Prompting with random selection with calibration (averaged on all candidates), and found that G-fair-Prompting can outperform random selection with calibration in most situations. For example, on the topic classification task, G-fair-Prompting achieves the best performance on most models. Moreover, we find that post calibration can harm the performance of the model and it occurs significantly times, so it is worthwhile to reconsider the influence of manipulating the model's probability directly.

Post calibration  can improve the accuracy of a certain prompt (in most cases), but when the selected prompt is very poor, the performance is still very poor even after calibration. We conducted experiments (Table 2) to compare the performance of G-fair-Prompting and random selection with calibration ("Average" and "Worst" indicate averaged accuracy and worst performance on all permutations of training examples), and observed that G-fair-Prompting outperforms random selection with calibration in most case. For instance, on the CoLA, G-fair-Prompting exhibited superior performance on most models. Additionally, we find that post-calibration could negatively affect the model's performance in many scenarios while it sometimes can improve the performance significantly even on selected prompts, for example, an improvement by \(10\%\) is observed on BLOOM-TREC. For more detailed discussions, please refer to Appendix A.5. Hence, it is crucial to reconsider the impact of directly manipulating the model's probability.

## 6 Conclusion

In this paper, we revisit the sensitivity of large language model across prompts, and analyse the issue from a predictive bias perspective. Accordingly, we employ a "content-free" strategy as a metric termed as fairness to evaluate the predictive bias of a fixed prompt and show that model's performance is highly consistency with fairness. Then, we propose two strategy to search the most fair prompt in the original space. We conduct extensive experiments on current famous LLMs, and validate the effectiveness of the proposed strategy. Moreover, in addition to fairness adopted in this paper, there would be more metrics for prompt searching in the future for different scenarios.

    &  & **BLOOM (176B)** & **LLaMA (33B)** & **LLaMA (65B)** \\  & & Average & Worst & Average & Worst & Average & Worst \\   & Random (cal) & \(66.8_{9.0}\) & \(57.2\) & \(69.2_{6.2}\) & \(59.4\) & **74.69.7** & **66.2** \\   & Ours & \(66.8_{2.5}\) & \(64.0\) & \(\) & \(\) & \(74.0_{12.2}\) & \(50.0\) \\  & Ours (cal) & \(\) & \(\) & \(76.6_{5.1}\) & \(70.0\) & \(72.8_{12.6}\) & \(48.0\) \\   & Random (cal) & \(73.0_{6.6}\) & \(61.8\) & \(71.9_{5.0}\) & \(64.0\) & **78.24.7** & **71.6** \\   & Ours & \(\) & \(\) & \(\) & \(\) & \(76.0_{4.0}\) & \(71.0\) \\   & Ours (cal) & \(77.4_{1.4}\) & \(76.0\) & \(76.0_{4.4}\) & \(68.0\) & \(76.4_{3.6}\) & \(70.0\) \\   & Random (cal) & \(\) & \(\) & \(67.8_{5.1}\) & \(63.6\) & \(54.0_{12.4}\) & \(42.4\) \\   & Ours & \(68.2_{6.2}\) & \(57.0\) & \(\) & \(64.0\) & **72.04.5** & **66.0** \\    & Ours (cal) & \(68.0_{5.2}\) & \(58.0\) & \(70.4_{3.8}\) & **65.0** & **72.04.5** & **66.0** \\   

Table 2: Accuracy comparison after post calibration.