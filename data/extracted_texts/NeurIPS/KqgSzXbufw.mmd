# Decentralized Noncooperative Games with Coupled Decision-Dependent Distributions

Wenjing Yan  Xuanyu Cao

Department of Electronic and Computer Engineering

The Hong Kong University of Science and Technology

wj.yan@connect.ust.hk, eexcao@ust.hk

Corresponding Author.

###### Abstract

Distribution variations in machine learning, driven by the dynamic nature of deployment environments, significantly impact the performance of learning models. This paper explores endogenous distribution shifts in learning systems, where deployed models influence environments, which in turn alters the data distributions that the learning models rely on. This phenomenon is formulated by a decision-dependent distribution mapping within the recently introduced framework of performative prediction (PP) (Perdomo et al., 2020). Our study investigates the performative effect in a decentralized noncooperative game, where players aim to minimize private cost functions while simultaneously managing coupled inequality constraints. In this context, we examine two equilibrium concepts for the studied game: performative stable equilibrium (PSE) and Nash equilibrium (NE), and establish sufficient conditions for their existence and uniqueness. Notably, we provide the first upper bound on the distance between the PSE and NE in the literature, which is challenging to evaluate due to the absence of strong convexity on the joint cost function. Furthermore, we develop a decentralized stochastic primal-dual algorithm for efficiently computing the PSE point. By rigorously bounding the performative effect, we prove that the proposed algorithm achieves sublinear convergence rates for both performative regret and constraint violations and maintains the same order of convergence rate as the case without performativity. Numerical experiments further confirm the effectiveness of our algorithm and theoretical results.

## 1 Introduction

Machine learning aims to generalize models trained on given datasets to make accurate predictions or decisions on new, unseen data (El Naqa and Murphy, 2015). The effectiveness of those models depends on the alignment between the training datasets and deployment environments (Quinonero-Candela et al., 2008). However, real-world environments are seldom static and often exhibit fluctuations that can severely degrade model performance (Zhou, 2022). In particular, shifts in data-generating distributions, driven by the dynamic nature of real-world conditions, present significant challenges for model deployment.

Distribution shifts in machine learning can occur exogenously or endogenously. Exogenous distribution shifts are driven by external factors beyond the control of the learning platforms, such as environmental changes (Chan et al., 2020) or policy amendments (Wu et al., 2021). In contrast, endogenous shifts arise from the system's inherent dynamics and interactions, where the deployed models affect environments, which in turn alters the data distributions that the learning models rely on (Dong et al., 2018). For instance, an increase in commodity prices may decrease user interest, thereby impacting sales. The key distinction lies in the controllability of endogenous shifts, providingan opportunity for designers to either exploit these shifts for improved performance or mitigate unintended consequences (Dean et al., 2023).

While substantial efforts have been made to address exogenous distribution changes, such as covariate shift (Chan et al., 2020), label shift (Wu et al., 2021), and concept drift (Lu et al., 2018), relatively little attention has been paid to the challenges posed by endogenous distribution shifts. Tackling these endogenous shifts is particularly challenging as data distributions are intrinsically linked to the decisions made by the learning model itself (Perdomo et al., 2020). As a result, addressing endogenous shifts may require the explicit modeling of feedback loops, consideration of causal relationships, and the adaptation of models to dynamic environments.

A notable advancement in this area is the recently proposed framework of "performative prediction (PP)" (Perdomo et al., 2020), also referred to as "decision-dependent learning" (Drusvyatskiy and Xiao, 2023). This framework elegantly captures the dynamic interplay between decisions and data distributions through a decision-dependent mapping, denoted by \(()\) where \(\) represents the decision variable. By linking \(\) to the data distribution, this formulation bridges the gap between model deployment and parameter optimization. Following the seminal work of (Perdomo et al., 2020), a growing body of research has emerged, focusing on stability and optimality analysis (Piliouras and Yu, 2023; Miller et al., 2021), as well as algorithmic design for various settings, including reinforcement learning (Mandal et al., 2023), online learning (Wood et al., 2021), bandit problems (Jagadeesan et al., 2022), and bilevel optimization (Lu, 2023).

This paper investigates endogenous distribution shifts in a decentralized noncooperative game, where players aim to minimize private cost functions while simultaneously managing coupled inequality constraints. To contextualize this setting, consider scenarios where strategic responses exhibit in learning environments and competitive interactions occur among players. For example, in autonomous vehicular networks, multiple vehicles compete to select their routes under constraints such as road capacities, traffic congestion, and travel costs. The route choices of each vehicle influence traffic patterns and consequently affect the travel times experienced by other vehicles (Mori et al., 2015). Similarly, in finance, traders compete to maximize profits under constraints like market capacities and inventory levels. The trading strategies of these participants impact market volatility and the distribution of asset prices, creating a dynamic pricing landscape (Fattouh and Mahadeva, 2014). These dynamics extend to other domains, such as electricity market competition (Moshari et al., 2010), ride-sharing platforms (Narang et al., 2023), natural resource extraction (Cust and Poelhekke, 2015), and online advertising auctions (Varian, 2009).

Despite its pervasiveness, this performative phenomenon has largely been overlooked in the studies of decentralized noncooperative games. This paper addresses the problem by formulating performativity using coupled decision-dependent distributions, following the PP framework of (Perdomo et al., 2020). However, the intricate interplay between decentralized players and endogenous distribution shifts presents challenging theoretical and algorithmic questions: _How do strategic responses in learning environments influence the game's equilibrium? How can players adapt their strategies effectively when confronted with coupled decision-dependent distributions? How can we design algorithms to exploit these dynamics for optimal decision-making?_ These questions form the core of our investigation, guiding us toward more resilient, adaptive, and efficient learning outcomes in decentralized games, especially in environments characterized by continuously evolving data and decision-making processes. Our main contributions are summarized below:

* We initially formulate the problem of decentralized noncooperative games with data performativity, where selfish players seek to minimize individual costs while managing coupled inequality constraints. Under this setting, we examine two equilibrium concepts: performative stable equilibrium (PSE) and Nash equilibrium (NE), and establish sufficient conditions for their existence and uniqueness. Compared to conventional games, this examination is more complicated due to the interplay between decision-making and distribution changes. Notably, we make a significant contribution by providing the first upper bound on the distance between the PSE and NE in the literature. Computing this distance in PP games is challenging due to the absence of strong convexity on the joint cost function, an essential property for determining the optimality gap of performative stable points in previous work. Instead, we characterize the distance by leveraging relations from strong duality and derive a result comparable to the findings of the prior work (Perdomo et al., 2020; Lu, 2023).

* To compute the PSE point of the PP-game, we propose a decentralized stochastic primal-dual algorithm based on repeated risk minimization (RRM). The development and convergence analysis of this algorithm face two primary challenges. First, there is a complex interaction between decentralized competition and endogenous distribution shifts. Second, players only have partial observation, as they communicate solely with neighbors, despite their private cost functions being influenced by the strategies of all players. We evaluate the performance of our algorithm by two commonly used metrics: performative regret, which measures the suboptimality of the strategy sequence generated by RRM relative to the PSE point, and constraint violation. By rigorously bounding the performative effect, we prove that the proposed algorithm achieves sublinear convergence rates for both metrics. Furthermore, our results show that while the performative effect slows down convergence, it does not degrade the order of performative regret compared to the case without performativity (Lu et al., 2020).

Finally, we conduct numerical experiments on a networked Cournot game and a ride-share market. The simulation results confirm the sublinear convergence of our algorithm. Furthermore, the results demonstrate that while greater performative strength leads to a wider gap between the PSE and NE, the discrepancy between these two equilibria remains marginal. This verifies both the effectiveness of the PSE solutions and the accuracy of our distance analysis between the PSE and NE.

**Related Work:** Among the numerous existing studies, two closely related works (Narang et al., 2023) and (Wang et al., 2023) have considered performative behaviors in games. A key distinction in our work is that our model requires all players' collective strategies to adhere to the constraints of the learning system, whereas both (Narang et al., 2023) and (Wang et al., 2023) address unconstrained settings. This difference results in fundamentally distinct algorithmic designs and convergence analyses. Our approach employs a primal-dual technique and requires consensus, whereas their methods only rely on local stochastic gradient descent. Additionally, we consider a mathematically richer model compared to (Wang et al., 2023), whose framework is structured in a specific form involving local costs dependent solely on individual strategies and a regularizer quantifying similarity among neighboring strategies. Furthermore, our algorithm design accounts for practical constraints where players can only communicate with their immediate neighbors, while (Narang et al., 2023) assumes full accessibility to all players' strategies across the entire network. Importantly, our work makes a significant contribution by providing the first upper bound on the distance between the performative stable equilibrium (PSE) and Nash equilibrium (NE)--a gap not previously addressed. Other related works such as (Li et al., 2022) and (Piliouras and Yu, 2023), have studied performative prediction in decentralized multi-agent optimization. The former focuses on consensus-seeking agents, while the latter is restricted to location-scale families. Finally, (Yan and Cao, 2024) considers the constrained performative prediction problem in a single-agent setting, whereas our paper addresses decentralized noncooperative games. A more comprehensive literature review is provided in Appendix A.

## 2 Problem Formulation

Consider a decentralized noncooperative game with \(n\) players. Each player \(i\) selects a strategy (or, interchangeably, decision, action), denoted as \(_{i}\), from its feasible set \(_{i}^{d}\). Let the collective decisions of all players be denoted as \(:=(_{1},,_{n})\), and the collective decisions of all players except player \(i\) be represented as \(_{-i}:=(_{1},,_{i-1}, _{i+1},,_{n})\), for any \(i[n]\), where \([n]\) denotes the set of integers \(\{1,2,,n\}\). Each player \(i\) has a private cost function \(J_{i}(_{i};_{i},_{-i})\), which depends on the random variable \(_{i}_{i}\), the player's private decision \(_{i}\), and the decisions of all other players \(_{-i}\). This paper considers a scenario where the underlying populations strategically respond to the players' decisions, causing shifts in data distributions. This interplay is modeled by a decision-dependent distribution mapping \(_{i}_{i}(_{i},_{-i})\) for all \(i[n]\). The objective of each player \(i\) is to selfishly minimize its _performative risk_\(_{_{i}_{i}(_{i},_{-i })}J_{i}(_{i};_{i},_{-i})\) (abbreviated as \(_{i}(_{i},_{-i})\)), subject to a coupled constraint \(_{i=1}^{n}_{i}(_{i})\), i.e.,

\[_{_{i}_{i}}& _{_{i}_{i}(_{i},_{-i})}J_{i}(_{i};_{i},_{-i} )\\ &_{i}(_{i})+ _{j i}_{j}(_{j}).\] (1)

Both \(J_{i}()\) and \(_{i}()\) are only locally accessible to player \(i\) for all \(i[n]\). In the game (1), each player solves its private optimization problem to determine the best strategy, given the current strategiesof all the other players. An equilibrium of the game (1) corresponds to a set of strategies where no player can improve its performance by deviating unilaterally from its strategy.

Denote by \(:=(_{1},,_{n})\) the concatenation of the variables \(_{i}\) and by \(J(;):=(J_{1} (_{1};),,J_{n}( _{n};))\) the concatenation of the cost functions \(J_{i}()\) for all \(i[n]\). A stochastic pseudogradient mapping of \(J(;)\) is defined as \( J(;):=( _{_{1}}J_{1}(_{1};),,_{_{n}}J_{n}( {}_{n};))\). We have the following assumption on \( J(;)\).

**Assumption 2.1**.: There exists a constant \(>0\) such that the stochastic gradient mapping \( J(;)\) is \(\)-strongly monotone, i.e., \( J(;)- J (;^{}),-^{}\|- ^{}\|_{2}^{2},,,^{}\), where \(:=_{1}_{n}\) and \(:=_{1}_{n}\).

Assumption 2.1 is commonly made in the literature of game theory. It suffices to guarantee the existence of Nash equilibrium for a stochastic game with fixed data distributions (Facchinei and Pang, 2003, Theorem 2.3.3(b)). However, in our paper, since the data distributions are decision-dependent, Assumption 2.1 does not imply the monotonicity of the gradient mapping of the joint performative risk, denoted by \(():=(_{1}(),, _{n}())\). Therefore, the existence and uniqueness (E\(\&\)U) conditions for the Nash equilibrium of the game (1) need further investigation.

We define a graph \(()\) to represent the impact of players' decisions on the data distributions of different players. In \(()\), the weight \(p_{ij}>0\) if player \(j\)'s decision affects player \(i\)'s data distribution, and \(p_{ij}=0\) otherwise. Particularly, \(p_{ii}\) represents the weight of self-influence. These weights are normalized as \(_{j=1}^{n}p_{ij}=1\), for all \(i[n]\). Clearly, the larger the weight \(p_{ij}\), the stronger the effect of player \(j\)'s decision on the data distribution of player \(i\).

Let \(_{1}(,^{})\) represent the _Wasserstein_-\(1\) distance between two probability measures \(\) and \(^{}\). Following (Wang et al., 2023), we impose the following assumption on the distributions \(\{_{i}\}_{i[n]}\).

**Assumption 2.2**.: For any \(i[n]\), there exists a constant \(_{i} 0\) such that, \(,^{}\), the distribution mapping \(_{i}\) is constrained by \(_{1}(_{i}(),_{i}(^{}))_{i} ^{n}p_{ij}\|_{j}-^{ }_{j}\|_{2}^{2}}\).

For any \(i[n]\), the parameter \(_{i}\) bounds the sensitivity of player \(i\)'s distribution with respect to (w.r.t.) the decision variations of all players. This \(\)-sensitivity property of distributions is conceptually akin to the Lipschitz continuity of functions that quantifies the variation of function values w.r.t argument changes. We also require the following assumptions.

**Assumption 2.3**.: For any \(i[n]\), the non-empty feasible set \(_{i}\) is closed, convex, and bounded, i.e., there exists a constant \(C 0\) such that, \(_{i}_{i}\), \(\|_{i}\|_{2} C\).

**Assumption 2.4**.: For any \(i[n]\) and \(_{i}_{i}\), the cost function \(J_{i}(_{i};_{i},_{-i})\) is convex w.r.t. \(_{i}\). Moreover, there exists a constant \(L_{i} 0\) such that \(J_{i}(_{i};)\) is \(L_{i}\)-smooth, i.e, \(\| J_{i}(_{i};)-  J_{i}(_{i}^{};^{} )\|_{2} L_{i}(\|_{i}-_{i}^{}\|_{2}+\|-^{ }\|_{2}),_{i}^{},_{i}^{}_{i},,^ {}\).

**Assumption 2.5**.: For any \(i[n]\) and \(_{i}_{i}\), the constraint function \(_{i}(_{i})\) is convex w.r.t. \(_{i}\). Moreover, there exist a constant \(G_{g} 0\) such that \(_{i}()\) is \(G_{g}\)-Lipschitz, i.e., \(\|_{i}(_{i})-_{i}( _{i}^{})\|_{2} G_{g}\|_{i} -_{i}^{}\|_{2},_{i}, _{i}^{}_{i}\).

Assumptions 2.3 and 2.5 are widely used in constrained optimization (Bertsekas, 2014; Yan and Cao, 2024a), and Assumption 2.4 is standard in the PP literature. From Yan and Cao (2024a, Proposition 1), under Assumptions 2.3 and 2.4, the cost function \(J_{i}(_{i};)\), \( i[n]\) is Lipschitz continuous, i.e., there exist a constant \(G_{i} 0\) such that \(|J_{i}(_{i};)-J_{i}(_{i}^{ };^{})| G_{i}(\|_{i}-_{i}^{}\|_{2}+\|- ^{}\|_{2}),_{i}, _{i}^{}_{i},, ^{}\). Moreover, Assumptions 2.3 and 2.5 imply the boundedness of \(\|_{i}(_{i})\|_{2}\), i.e., there exists a constant \(B 0\) such that \(\|_{i}(_{i})\|_{2} B,_{i}_{i},i[n]\).

## 3 Equilibrium of the PP-Game

This section examines two fundamental equilibrium concepts of the performative game (1): Nash equilibrium (NE) and performative stable equilibrium (PSE), as defined below.

**Definition 3.1** (Nash Equilibrium).: A vector \(^{}:=(_{1}^{ },,_{n}^{})\) achieves an NE of the game (1) if it holds for any \(i[n]\) that

\[_{i}^{}*{arg\,min}_{ _{i}_{i}} _{_{i}_{i}(_{i},_{-i}^{})}J_{i}( _{i};_{i},_{-i}^{ })\] subject to \[_{i}(_{i})+_{j i} _{j}(_{j}^{}).\]

**Definition 3.2** (Performance Stable Equilibrium).: A vector \(^{}}:=(_{1}^{}}, ,_{n}^{}})\) achieves a PSE of the game (1) if it holds for any \(i[n]\) that

\[_{i}^{}}*{arg\,min}_{ {}_{i}_{i}} *{E}_{_{i}_{i}(^{}})}J_{i}(_{i};_{i},_{-i}^{}})\] \[ _{i}(_{i})+_{j i}_{j}(_{j}^{}}).\]

NE is a fundamental concept in game theory. At NE, each player's strategy optimally aligns with its own interest, given the strategies of other players. Hence, no player has an incentive to deviate from its strategy unilaterally. In the case of performative games, the computation of NE needs to take into account the data distributions \(_{i}()\) for all \(i[n]\), as they are parameterized by the optimization variable \(\). However, this information is often unavailable in practice. Instead, at PSE, the data distribution of each player \(i[n]\) is fixed at \(_{i}(^{}})\) and the PSE point achieves an NE of the game (1) under the fixed data distribution of its own deployment. This formulation draws benign properties akin to problems with fixed data distributions, facilitating the adaptation of existing algorithms. Therefore, PSE is more frequently chosen as a performance metric in the literature of PP.

### Existence and Uniqueness of PSE

We first establish the condition for the E\(\&\)U of the PSE of the game (1). Our approach relies on repeated risk minimization (RRM) for closed-loop retraining. First, we define a mapping \(():=\{_{i}()\}_{i[n]}\) that, for any \(i[n]\),

\[_{i}^{}=_{i}():= *{arg\,min}_{_{i}_{i}} *{E}_{_{i}_{i}(_{i},_{-i})}J_{i}(_{i};_{i},_{ -i}^{})\] subject to \[_{i}(_{i})+_{j i}_{j}(_{j}^{}).\]

The mapping \(()\) outputs the NE of the game (1) under the fixed data distributions \(_{i}(_{i},_{-i})\) for all \(i[n]\). With Assumption 2.1, the E\(\&\)U of this NE is guaranteed, thereby ensuring the validity of the mapping \(()\). Based on \(()\), the RRM updates \(_{i}^{t}\) at each iteration \(t\) by

\[_{i}^{t+1}=_{i}(^{t}), i [n].\] (2)

Clearly, \(^{t+1}\) is an NE of the game (1) under the deployment of \(^{t}\). Additionally, we have that any fixed point of (2) achieves an PSE for the game (1), i.e., \(^{}}=(^{}})\). By investigating the convergence the iterative equation (2), we have the following sufficient condition for the E\(\&\)U of the PSE of the game (1).

**Theorem 3.3**.: _Suppose that Assumptions 2.1-2.5 hold. Then, for any \(,\), the mapping \(()\) satisfies_

\[\|()-()\|_{2} {1}{}^{n}L_{i}^{2}_{i}^{2}_{j[n]}p_{ij}} \|-\|_{2}.\]

_Thus, if it is satisfied that_

\[^{n}L_{i}^{2}_{i}^{2} _{j[n]}p_{ij}}<1,\] (3)

_the sequence generated by the RRM (2) converges to a unique PSE point \(^{}}\) at a linear rate that_

\[\|^{t+1}-^{}}\|_{2}( ^{n}L_{i}^{2}_{i}^{2}_{j[n]}p_{ ij}})^{t}\|^{1}-^{}}\|_{2}.\]

The proof of Theorem 3.3 is provided in Appendix B. According to Theorem 3.3, under Assumptions 2.1-2.5, when condition (3) holds, we have that: (i) the game (1) admits a unique PSE, and (ii) the RRM method (2) converges linearly to the PSE.

Since the influence weights \(\{p_{ij}\}_{j[n]}\) are normalized, with \(_{j=1}^{n}p_{ij}=1\) for all \(i[n]\), we generally have that \(p_{ij}=()\). Therefore, the contraction condition (3) exhibits good scalability w.r.t. the number of players. Moreover, according to the proof in Appendix B, if for any player \(i[n]\), its distribution \(_{i}()\) depends only on its own decision \(_{i}\), i.e., \(p_{ij}=0\) for all \(j i\), then we have

\[\|()-()\|_{2} {1}{}_{i[n]}L_{i}_{i}\|- \|_{2}.\]The contraction of the above iterative equation only requires that \(_{i[n]}L_{i}_{i}<1\). Furthermore, if all players exhibit equivalent model parameters that \(L_{1}==L_{n}=L\) and \(_{1}==_{n}=\) and \(p_{ij}=\) for all \(i,j[n]\), condition (3) reduces to \(<1\), recovering the contraction requirement of (Perdomo et al., 2020) for a single-agent PP case.

### Existence and Uniqueness of NE

First, we define a gradient mapping \(G_{}^{(i)}(_{i},_{-i}):=_{_{i}_{i}()}_{_{i}}J_{i}( _{i};_{i},_{-i})\) for any \(i[n]\), and \(G_{}():=(G_{}^{(1)}(),,G_{}^{(n)}())\). Moreover, for any \(i[n]\), define

\[H_{_{i},_{-i}}^{(i)}():=_{_{i}} _{_{i}_{i}(_{i},_{-i})} [J_{i}(_{i};)]_{_{i}= {}_{i}}\]

and \(H_{}():=(H_{_{1},_{-1}}^{(1)}(),,H_{_{n},_{-n}}^{(n)}( ))\). Then, for any \(i[n]\), the gradient of the performative risk \(_{i}(_{i},_{-i})\) w.r.t. \(_{i}\) is given by

\[_{_{i}}_{i}(_{i},_{-i})=G_{ _{i},_{-i}}^{(i)}(_{i},_{-i})+H_{ _{i},_{-i}}^{(i)}(_{i},_{-i}).\]

Define \(():=(_{_{i}} _{i}(),,_{_{i}}_{n}( ))\), we further have

\[()=G_{}()+H_{}( ).\]

From Facchinei and Pang (2003, Theorem 2.3.3(b)), to prove the E&U of the NE of the (1), we require the strongly monotonivity of the gradient mapping \(()\). Therefore, we have the following sufficient condition for the E&U of the NE of the game (1).

**Theorem 3.4**.: _Suppose that Assumptions 2.1-2.5 hold. If it is satisfied that_

\[-_{i=1}^{n}L_{i}_{i}_{j[n]}}-^{n}L_{i}^{2}_{i}^{2}p_{ii}}>0,\] (4)

_then, the PP-game (1) is strongly monotone and admits a unique NE._

The proof of Theorem 3.4 is presented in Appendix C. Since \(p_{ij}\) characterizes the influence of player \(j\)'s decision on the data distribution of player \(i\), we typically have \(p_{ij} p_{ii}\) for \(j i\) and thus \(_{j[n]}p_{ij}=p_{ii}\) for all \(i[n]\). Then, the condition (4) reduces to \(-_{i=1}^{n}L_{i}_{i}p_{ii}-^{n}L_{i}^{2} _{i}^{2}p_{ii}}>0\). Similarly, when \(L_{1}==L_{n}=L\), \(_{1}==_{n}=\), and \(p_{ij}=\) for all \(i,j[n]\), we require that \(-2L>0\), i.e., \(\), which recovers the condition to guarantee the convexity of the performative risk \(()\), and thereby the E&U of the performative optimal point of (Miller et al., 2021) for single-agent PP.

### Distance Between PSE and NE

**Theorem 3.5**.: _Define \(:=-_{i=1}^{n}L_{i}_{i}_{j[n]}}\) and \(:=_{i=1}^{n}G_{i}(1+_{i}_{j[n]}})\). Suppose that Assumptions 2.1-2.5 hold and \(>0\). Then, for every PSE point and NE point, we have the following relations:_

\[\|^{}-^{}\|_{2}}^{n}G_{i}^{2}_{i}^{2}p_{ii}} |(^{})-(^{ })|}^{n}G_{i}^{2} _{i}^{2}p_{ii}}.\]

The proof of Theorem 3.5 is presented in Appendix D. According to Theorem 3.5, the distance between the PSE and NE of the game (1) depends on the cost functions' parameters \(\), \(\{G_{i}\}\), \(\{L_{i}\}\), as well as the sensitivity of the data distributions \(\{_{i}\}\). Larger sensitivity parameters widen the gap between the PSE and NE, while a bigger monotonicity parameter \(\) reduces it. Notably, when the sensitivity parameter \(_{i}=0\) for all \(i[n]\), the game (1) reduces to a conventional stochastic game with fixed data distributions, and as a result, the PSE and NE converge to the same point.

To the best of our knowledge, this is the first result on the distance between PSE and NE of PP-games. Characterizing this distance is challenging in games due to the lack of strong convexity on the joint cost function \(J()\), which is an essential property for determining the optimality gap of performative stable points in previous work (Perdomo et al., 2020; Lu, 2023). In this paper, we characterize this gap by leveraging relations from strong duality (Boyd and Vandenberghe, 2004; Facchinei and Pang, 2010). Our result is comparable to the findings in (Perdomo et al., 2020) for single-agent PP problems wherein this optimality gap is bounded by \(\). In our case, when \(G_{1}==G_{n}=G\), \(_{1}==_{n}=\) and \(p_{ij}=\) for all \(i,j[n]\), we have \(\|^{}-^{}\|_{2}\).

## 4 Computation of the PSE

Although RRM theoretically has the capability to find a PSE point, how to perform risk minimization at its each update remains unknown. Moreover, RRM requires the computation of an NE for each deployment, which is computationally intensive. In this section, we present a decentralized stochastic primal-dual algorithm for efficiently computing the PSE of the game (1). Theoretical analysis is also provided on the convergence of the proposed algorithm.

### Algorithm Development

For each player \(i[n]\), define a regularized Lagrangian as

\[_{}^{(i)}(_{i},_{-i}, )=_{_{i}_{i}()}J_{i}( _{i};_{i},_{-i})+,_ {i}(_{i})+_{j i}_{j}(_{j}) ,\]

where \(_{+}^{m}\) is the dual variable. Denote by \(_{i}()\) the Jacobian matrix of \(_{i}()\). From the primal-dual theory (Boyd and Vandenbergheg, 2004; Facchinei and Pang, 2010), for any \(>0\), there exists a bounded Lagrangian multiplier \(^{}\) such that the following condition holds:

\[_{i}^{}= P_{_{i}}[_{i}^{}- (G_{^{}}^{(i)}(^{},^{})+_{i}(_{i}^{})^ {}^{})], i[n],\] \[^{}= [^{}+(_{i}( _{i}^{})+_{j i}_{j}(_{j}^{}) )]_{+},\]

where \(\) is a control parameter. Thus, given \(_{-i}^{}\) and under \(_{i}_{i}(^{})\), \((_{i}^{},^{})\) is a saddle point of the Lagrangian \(_{^{}}^{(i)}(_{i},_{-i}^{ },)\) for any \(i[n]\). The joint saddle point \((^{},^{})\) achieve the PSE of the game (1) under strong duality (Boyd and Vandenbergheg, 2004).

In the decentralized noncooperative game (1), each player can only communicate with its neighbors. We use \(()\) to denote the communication graph of the network, where \(=(a_{ij})_{n n}\) represents a weight matrix. In \(()\), \(a_{ij}=a_{ji}>0\) if there is a communication link between player \(i\) and play \(j\), and \(a_{ij}=a_{ji}=0\) otherwise. Let \(_{i}\) be the set containing player \(i\) and all its neighbors such that \(j_{i}\) if \(a_{ij}>0\). We assume that the communication graph \(()\) is connected and the weight matrix \(\) is doubly stochastic.

To find the saddle point \((^{},^{})\), we develop a decentralized stochastic primal-dual algorithm, as presented in Algorithm 1. The basic idea of Algorithm 1 is to perform gradient update on the primal variables \(_{i}\) for all \(i[n]\) and the dual variable \(\). In the decentralized noncooperative game, each player \(i[n]\) only observes information from its neighbors. However, its private cost funtion \(J_{i}(_{i};_{i},_{-i})\) involves all players' strategies. To solve this problem, we let each player \(i\) store an estimate for the strategies of all the other players, denoted by \(}_{ih}\), for all \(h i\). Define a vector \(}_{i}\) that concatenates all the estimates \(}_{ih}\). In each iteration \(t\), neighbors exchange strategy \(_{i}^{t}\), estimate \(}_{i}^{t}\), and dual variable \(_{i}^{t}\) with each other. Then, each player \(i\) updates the estimates \(}_{ih}\), for all \(h i\) by weighted average in Step 4. The primal variable \(_{i}^{t}\) is updated by gradient descent by Step 6, and the dual variable \(_{i}^{t}\) is updated by gradient ascent by Step 7. The coefficient \(_{t}\) is the stepsize at the \(t\)th iteration for all \(t[T]\).

### Performance Analysis

Before analyzing the performance of Algorithm 1, we define the performance metrics adopted in this paper. The first metric is performative regret. For any player \(i[n]\), its performative regret over \(T\) iterations is defined as

\[_{i}(T)\!:=\!_{t=1}^{T}(_{_{i}_{i}(^{}})}J_{i} (_{i};_{i}^{}}, _{-i}^{}})-_{i}( ^{}})).\]

The regret \(_{i}(T)\) measures the suboptimality of the sequence of decisions \(\{_{i}^{1},,_{i}^{T}\}\) taken by play \(i\) relative to \(_{i}^{}}\). Besides, since the decisions of all players are subject to constraints, another performance metric of constraint violation, denoted by \(_{g}(T)\), is required, defined as

\[_{g}(T)=\|[_{t=1}^{T}_{i=1}^{n }_{i}(_{i}^{t})]_{+}\| _{2}.\]

Any online or learning algorithm is regarded as "good" if both the time-average regret and the time-average constraint violation are sublinear, i.e., \(_{T}_{i}(T)/T o(1)\) for any \(i[n]\) and \(_{T}_{g}(T)/T o(1)\).

For analysis, we make the following assumption on the variance of the stochastic gradient \(_{_{i}}J_{i}(_{i};_{i},_{-i})\), \( i[n]\).

**Assumption 4.1**.: The stochastic gradient \(_{_{i}}J_{i}(_{i};_{i},_{-i})\) is unbiased that \(_{_{i}_{i}()} _{_{i}}J_{i}(_{i};_{i},_{-i})=G_{}^{(i)} (_{i},_{-i})\) and there exist constants \(_{0},_{1} 0\) such that \(_{i=1}^{n}_{_{i}_{i}()}\|_{_{i}}J_{i}(_{i };_{i},_{-i})-G_{}^{(i)}(_{i},_{-i}) \|_{2}^{2}_{0}^{2}+_{1}^{2}\|- ^{}}\|_{2}^{2},,\).

**Theorem 4.2**.: _Define \(:=-_{i=1}^{n}L_{i}_{i}_{j[n]}}\) and \(:=3(_{1}^{2}+3_{i=1}^{n}L_{i}^{2}(1+_{i}^{2} _{j[n]}p_{ij}))\). Suppose that Assumptions 2.1-2.5 and 4.1 hold and \(>0\). By Algorithm 1, if the stepsize satisfies \(_{t[T]}_{t}}{}\), then, the performative regret of the game (1) is bounded by_

\[_{i}(T)(}( }+_{t=1}^{T}_{t})}), i[n].\]

_Further, the constraint violation is bounded by_

\[_{g}(T)(}}+_{t=1}^{T}_{t})(1+_{t=1}^{T}_{t}^ {2})}).\]

For a sequence of diminishing stepsize \(_{t}=_{1}^{}(_{2}t+_{1})^{-}\), where \(_{1},_{2}>0\) and \(0<<1\), we have that: 1) \(_{t=1}^{T}_{t}(T^{1-})\); 2) \(_{t=1}^{T}^{2}(t)(T^{1-2})\). Plugging the above results into Theorem 4.2 yields

\[_{i}(T)(T^{}+T^{1-}),i[n]_{g}(T)(T ^{}+T^{}+T^{1-}).\]

Based on the above two inequalities, the best choice of \(\) is \(\) such that \(_{i}(T)(T^{}), i[n]\) and \(_{g}(T)(T^{})\). This convergence speed matches that of the decentralized noncooperative game without performativity (Lu et al., 2020).

The proof of Theorem 4.2 is provided in Appendix E. According to Theorem 4.2, the performative effect reduces the convergence rate by amplifying the coefficient \(}\) in the regret bounds. Specifically, as the sensitivity parameters \(_{i}\) increase, the coefficient \(\) decreases, leading to a slower convergence rate of \(_{i}(T)\) for all \(i[n]\). This occurs because a larger \(_{i}\) indicates a stronger performative influence, which more significantly impacts the algorithm's convergence. Nevertheless, the performative effect does not degrade the convergence order of Algorithm 1 compared to the case without performativity (Lu et al., 2020).

## 5 Numerical Experiments

In this section, we evaluate the effectiveness of our algorithm and theoretical results by conducting numerical experiments on a networked Cournot game (Abolhassani et al., 2014), which is a foundational model in economic theory (Allaz and Vila, 1993) for analyzing oligopolistic competitions. Weconsider a networked Cournot game with five firms selling a single commodity across three markets. Each firm aims to maximize its profit by determining the quantities it serves in all markets. The total accommodated quantity in each market is limited by its market capacity. The simulation details and additional numerical results are presented in Appendix F.1. We also provide an additional experiment on a ride-share market in Appendix F.2.

Fig. 1 illustrates the convergence of the time-average regrets of five firms, denoted by \(_{i}(t)/t\), \( i\), and the convergence of the time-average constraint violations of three markets, denoted by \(_{t^{}=1}^{t}_{i=1}^{n}g_{ij}(_{i} ^{t^{}})\), \( j\). The results demonstrate that both \(_{i}(t)/t\) and \(_{t^{}=1}^{t}_{i=1}^{n}g_{ij}(_{i} ^{t^{}})\) approach zero as the iterations increase. This verifies the sublinear convergence of the regrets and constraint violations in Theorem 4.2.

Fig. 2 (a) compares the normalized distance between \(^{t}\), generated by Algorithm 1, and the NE point \(^{}\), denoted as \(\|^{t}-^{}\|_{2}/\| {}^{t}\|_{2}\). The NE point is computed based on perfect knowledge of \(\{_{i}\}_{i[n]}\). We consider three different performative strengths: \(=0.2\), \(0.4\), and \(0.6\). It is observed that \(\|^{t}-^{}\|_{2}/\| {}^{t}\|_{2}\) stabilizes at values approximately equal to or smaller than \(10^{-1}\) with iterations, varifying the effectiveness of Algorithm 1. Additionally, a larger performative strength leads to a wider normalized distance between the convergent point of \(^{t}\) and \(^{}\). In Fig. 2 (b), we compare the total revenues, denoted by \(-_{i=1}^{5}_{i}(^{t})\) under the same three \(\) settings. We consider two scenarios: 1) "\(\)", where \(^{t}\) is generated by Algorithm 1; 2). "\(\)", where \(^{t}\) is generated by performing the same procedures as Algorithm 1 but with perfect information on the distributions \(\{_{i}()\}_{i[n]}\). The result demonstrates the close performance of the "\(\)" approach and the "\(\)" approach. More numerical results can be found in Appendix F.

**Conclusions:** We have studied the performative phenomenon in a decentralized noncooperative game where selfish players seek to maximize their individual profits while adhering to coupled inequality constraints. We have derived sufficient conditions for the E&U of both PSE and NE and provided the first upper bound on the distance between these two equilibria. Furthermore, we have developed a decentralized stochastic primal-dual algorithm for efficiently computing of the PSE point. Theoretical analysis has demonstrated the same order of convergence speed of our algorithm as the case without performativity. Finally, numerical simulations have been provided to verify the effectiveness of our algorithm and theoretical results.

Figure 1: Convergence of time-average regrets and time-average constraint violations.