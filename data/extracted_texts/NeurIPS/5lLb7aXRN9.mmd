# Unconditional stability of a recurrent neural circuit implementing divisive normalization

Shivang Rawat\({}^{1,2}\) David J. Heeger\({}^{3,4}\) Stefano Martiniani\({}^{1,2,5}\)

\({}^{1}\) Courant Institute of Mathematical Sciences, NYU

\({}^{2}\) Center for Soft Matter Research, Department of Physics, NYU

\({}^{3}\) Department of Psychology, NYU

\({}^{4}\) Center for Neural Science, NYU

\({}^{5}\) Simons Center for Computational Physical Chemistry, Department of Chemistry, NYU

{sr6364, david.heeger, sm7683}@nyu.edu

###### Abstract

Stability in recurrent neural models poses a significant challenge, particularly in developing biologically plausible neurodynamical models that can be seamlessly trained. Traditional cortical circuit models are notoriously difficult to train due to expansive nonlinearities in the dynamical system, leading to an optimization problem with nonlinear stability constraints that are difficult to impose. Conversely, recurrent neural networks (RNNs) excel in tasks involving sequential data but lack biological plausibility and interpretability. In this work, we address these challenges by linking dynamic divisive normalization (DN) to the stability of "oscillatory recurrent gated neural integrator circuits" (ORGaNICs), a biologically plausible recurrent cortical circuit model that dynamically achieves DN and that has been shown to simulate a wide range of neurophysiological phenomena. By using the indirect method of Lyapunov, we prove the remarkable property of unconditional local stability for an arbitrary-dimensional ORGaNICs circuit when the recurrent weight matrix is the identity. We thus connect ORGaNICs to a system of coupled damped harmonic oscillators, which enables us to derive the circuit's energy function, providing a normative principle of what the circuit, and individual neurons, aim to accomplish. Further, for a generic recurrent weight matrix, we prove the stability of the 2D model and demonstrate empirically that stability holds in higher dimensions. Finally, we show that ORGaNICs can be trained by backpropagation through time without gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which address the problems of exploding, vanishing, and oscillating gradients. By evaluating the model's performance on RNN benchmarks, we find that ORGaNICs outperform alternative neurodynamical models on static image classification tasks and perform comparably to LSTMs on sequential tasks.

## 1 Introduction

Deep neural networks (DNNs) have found widespread use in modeling tasks from experimental systems neuroscience. The allure of DNN-based models lies in their ease of training and the flexibility they offer in architecting systems with desired properties . In contrast, neurodynamical models like the Wilson-Cowan  or the Stabilized Supralinear Network (SSN)  are more biologically plausible than DNNs, but these models confront considerable training challenges due to the lack of stability guarantees for high-dimensional problems. Training recurrent neural networks (RNNs), by comparison, is more straightforward thanks to ad hoc regularization techniques like layer normalization, batch normalization, and gradient clipping/scaling, which help stabilize training without imposing strict stability constraints. Conversely, neurodynamical models require enforcing hard stability constraints while maintaining biological plausibility. In lower dimensions, it is relatively straightforward to derive constraints on model parameters that ensure a dynamically stable system . However, for high-dimensional systems, this becomes significantly more challenging, as integrating these hard constraints into the optimization problem is more complex . Stability is generally advantageous in DNNs, as it is linked to improved generalization, mitigation of exploding gradient problems, increased robustness to input noise, and simplified training techniques .

The divisive normalization (DN) model was developed to explain the responses of neurons in the primary visual cortex (V1) , and has since been applied to diverse cognitive processes and neural systems . Therefore, DN has been proposed as a canonical neural computation  that is linked to many well-documented physiological  and psychophysical  phenomena. DN models various neural processes: adaptation , attention , automatic gain control , decorrelation, and statistical whitening . The defining characteristic of DN is that each neuron's response is divided by a weighted sum of the activity of a pool of neurons (Eq. 2, below) like when normalizing the length of a vector. Due to its wide applicability and ability to explain a variety of neurophysiological phenomena, we argue that this characteristic should be central to any neurodynamical model. Both the Wilson-Cowan and SSN models have been shown to approximate DN responses , but only approximately in certain parameter regimes.

Normalization techniques have been extensively adopted for training DNNs, demonstrating their ability to stabilize, accelerate training, and enhance generalization . Divisive normalization can be viewed as a comprehensive normalization strategy, with batch and layer normalization being specific instances . Models implementing DN have shown superior performance compared to common normalization methods (Batch , Layer , Group ) in tasks such as image recognition with convolutional neural networks (CNNs)  and language modeling with RNNs . Despite the foundational role of these techniques in deep learning algorithms, their implementation is ad hoc, limiting their conceptual relevance. They serve as practical solutions addressing the limitations of current machine learning frameworks rather than offering principled insights derived from understanding cortical circuits.

It has been proposed that DN is achieved via a recurrent circuit . Oscillatory recurrent gated neural integrator circuits (ORGaNICs) are rate-based recurrent neural circuit models that implement DN dynamically via recurrent amplification . Since ORGaNICs' response follows the DN equation at steady-state, its steady-state response captures the full range of aforementioned neural phenomena explained by DN . ORGaNICs have further been shown to simulate key time-dependent neurophysiological and cognitive/perceptual phenomena under realistic biophysical constraints . Additional phenomena not explained by DN  can in principle be integrated into the model. In this paper, however, we focus on the effects of DN on the dynamical stability of ORGaNICs. Despite some empirical evidence that ORGaNICs are highly robust, the question of whether the model is stable for arbitrary parameter choices, and thus whether it can be robustly trained on ML tasks by backpropagation-through-time (BPTT), remains open.

Here, we establish the unconditional stability -- applicable across all parameters and inputs -- of a multidimensional two-neuron-types ORGaNICs model when the recurrent weight matrix is the identity. We prove this result, detailed in Section 4, by the indirect method of Lyapunov: we perform linear stability analysis around the model's analytically-known normalization fixed point and reduce the stability problem to that of a high-dimensional mechanical system, whose stability is defined in terms of a tractable quadratic eigenvalue problem. We then address the stability of the model with an arbitrary recurrent weight matrix in Section 5. While the indirect method of Lyapunov becomes intractable for such a system, we provide proof of unconditional stability for a two-dimensional circuit with an arbitrary recurrent weight and offer empirical evidence supporting the claim of stability for high-dimensional systems.

ORGaNICs can be viewed as biophysically plausible extensions of Long Short Term Memory units (LSTMs)  and Gated Recurrent Units (GRUs) , RNN architectures that have been widely used in ML applications . The main differences are that ORGaNICs operate in continuous time and have built-in dynamic normalization (via recurrent gain modulation) and built-in attention (via input gain modulation). Thus, we expect that ORGaNICs should be able to solve relatively sophisticated tasks . Here, we demonstrate (Section 6) that by virtue of their intrinsic stability, ORGaNICs can be trained on sequence modeling tasks by BPTT, in the same manner as traditional RNNs (unlike SSN that instead requires costly specialized training strategies ), despite implementing power-law activations . Moreover, we show that ORGaNICs trained by naive BPTT (i.e., without gradient clipping/scaling or other ad hoc strategies) achieve performance comparable to LSTMs on the tasks that we consider, despite no systematic hyperparameter tuning.

## 2 Related Work

**Trainable biologically plausible neurodynamical models**: There have been several attempts to develop neurodynamical models that mimic the function of biological circuits and that can be trained on cognitive tasks. Song et al.  incorporated Dale's law into the vanilla RNN architecture, which was successfully trained across a variety of cognitive tasks. Building on this, Soo et al.  developed a technique for such RNNs to learn long-term dependencies by using skip connections through time. ORGaNICs is a model that is already built on biological principles and can learn long-term dependencies intrinsically by tuning the (intrinsic or effective) time constants, therefore it does not require the method used in . Soo et al.  introduced a novel training methodology (dynamics-neural growth) for SSNs and demonstrated its utility for tasks involving static (time-independent) stimuli. However, this training approach is costly and difficult to scale (because SSNs, unlike ORGaNICs, are not unconditionally stable), and its applicability on tasks with dynamically changing inputs remains unclear.

**Dynamical systems view of RNNs:** The stability of continuous-time RNNs has been extensively studied and discussed in a comprehensive review by Zhang et al. . Recent advancements have focused on designing architectures that address the issues of vanishing and exploding gradients, thereby enhancing trainability and performance. A central idea in these designs is to achieve better trainability and generalization by ensuring the dynamical stability of the network. Moreover to avoid the problem of vanishing gradients the key idea is to constrain the real part of the eigenvalues of the linearized dynamical system to be close to zero, which facilitates the propagation and retention of information over long durations of time. Chang et al.  and Erichson et al.  achieve this by imposing an antisymmetric constraint on the recurrent weight matrix. Meanwhile, Rusch et al. [61; 62] propose an architecture based on coupled damped harmonic oscillators, resulting in a second-order system of ordinary differential equations that behaves similarly to how ORGaNICs behave in the vicinity of the normalization fixed point, as we show in Section 4. Despite their impressive performance on various sequential data benchmarks, these models lack biological plausibility due to their use of saturating nonlinearities (instead of normalization) and unrealistic weight parameterizations.

## 3 Model description

In its simplest form, the two-neuron-types ORGaNICs model [47; 48] with \(n\) neurons of each type can be written as,

\[_{y}} =-++(-^{+})(_{r}(^{+}}-^{-}}))\] (1) \[_{a}} =-+_{0}^{2}^{2}+\, ((^{+}+^{-})^{+2})\]

where \(^{n}\) and \(^{n}\) are the membrane potentials (relative to an arbitrary threshold potential that we take to be 0) of the excitatory (\(\)) and inhibitory (\(\)) neurons, evolving according to the dynamical equations defined above with \(\) and \(\) denoting the time derivatives. The notation \(\) denotes element-wise multiplication of vectors, and squaring, rectification, square-root, and division are also performed element-wise. \(\) is an n-dimensional vector with all entries equal to 1. \(^{n}\) is the input drive to the circuit and is a weighted sum of the input, \(^{m}\), i.e., \(=_{zx}\). The firing rates, \(^{}=||^{2}\) and \(^{+}=|}\) are rectified (\(|.,.|\)) power functions of the underlying membrane potentials. For the derivation of a general model with arbitrary power-law exponents, including the Eq. 1, see Appendix A. Note that the term \(^{+}}-^{-}}\) serves the purpose of defining a mechanism for reconstructing the membrane potential (which can be negative, depending on the sign of the input) from the firing rates \(^{}\) that are strictly nonnegative. \(^{+}\) and \(^{-}\) are the firing rates of neurons with complementary receptive fields such that they encode inputs with positive and negative signs, respectively. Note that only one of these neurons fires at a given time. In ORGaNICs, these neurons have a single dynamical equation for their membrane potentials, where the sign of indicates which neuron is active. Neurons with such complementary (anti-phase) receptive fields are found adjacent to each other in the visual cortex , and we hypothesize that such complementary neurons are ubiquitous throughout the neocortex. \(_{+}^{+n}\) and \(_{0}_{+}^{+n}\) are the input gains for the external inputs \(\) and \(\) fed to neurons \(\) and \(\), respectively. \(_{+}^{+}\) is the set of positive real numbers, \(\{x\,|\,x>0\}\). \(_{*}^{+n}\) determines the semistaturation of the responses of neurons \(\) by contributing to the depolarization of neurons \(\). \(_{y}_{*}^{+n}\) and \(_{a}_{*}^{+n}\) represent the time constants of \(\) and \(\) neurons.

In addition to receiving external inputs, both \(\) and \(\) neurons receive recurrent inputs, represented by the last term in both of the equations. \(_{r}^{n n}\) is the recurrent weight matrix that captures lateral connections between the \(\) neurons. This recurrent input is gated by the a neurons, via the term \((-^{+})\). Similarly, the _nonnegative_ normalization weight matrix, \(_{*}^{n n}\), encapsulates the recurrent inputs received by the \(\) neurons. The differential equations are designed in such a way that when \(_{r}=\) and \(=_{0}=b_{0}\) (i.e., with all elements equal to a constant \(b_{0}\)), the principal neurons follow the normalization equation exactly (and approximately when \(_{r}\)) at steady-state,

\[_{s}^{+}_{s}^{2}=^{2}}{^{2}+( ^{2}+-^{2})}.\] (2)

\(^{2}\) and \(-^{2}\) represent the contribution of neurons with complementary receptive fields to the normalization pool, and \(^{2}+-^{2}=^{2}\) is the contrast energy of the input. Note that the recurrent gain, \((-^{+})\), is a particular nonlinear function of the output responses/activation designed to achieve DN, while the input gain, \(^{+}\), is an input gate that can implement an attention mechanism.

## 4 Stability analysis of high-dimensional model with identity recurrent weights

We consider the stability of the general high-dimensional ORGaNICs (Eq. 1) when the recurrent weight matrix is identity, \(_{r}=\). We first simplify the dynamical system by noting that \(^{+}}-^{-}}=\) and \(^{+}+^{-}=^{2}\) yielding the following equations,

\[_{y}}& =-}+ \\ _{a}}&=-+_{0}^{2}^{2}+\,( ^{2})\] (3)

For identity recurrent weights, we have a unique fixed point, given by,

\[_{s}=}{_{0}^{2} ^{2}+(^{2}^{ 2})}};_{s}=_{0}^{2}^{2 }+(^{2}^{2})\] (4)

Since the normalization weights in the matrix \(\) are nonnegative, at steady-state we have \(_{s}>\), so that \(_{s}}=_{s}}\), and the corresponding firing rates at steady-state are,

\[_{s}^{}=^{2}}{ _{0}^{2}^{2}+(^{2 }^{2})};_{s}^{+}=_{0}^{2 }^{2}+(^{2}^ {2})}\] (5)

Note that we recover the normalization equation, Eq. 2, if \(=_{0}=b_{0}\). Since the fixed points of \(\) and \(\) neurons are known analytically, to prove that this fixed point is _locally asymptotically stable_ (i.e., the responses converge asymptotically to the fixed point), we apply the _indirect method of Lyapunov_ at this fixed point . This method allows us to analyze the stability of the nonlinear system in the vicinity of the fixed point by studying the corresponding linearized system. The Jacobian matrix \(^{2n 2n}\) about \((_{s},_{s})\), defining the linearized system, is given by,

\[=-(_{s}}}{ _{y}})&-(_{s}}{ _{s}_{y}}})\\ (}{_{a}})\, (_{s}_{s})&( }{_{a}})(-+\, (_{s}^{2}))\] (6)

where \(()\) is a diagonal matrix of appropriate size with the elements of the vector \(\) on the diagonal. A necessary and sufficient condition for local stability is that the real parts of all eigenvalues of this matrix are negative. We thus proceed by computing the characteristic polynomial for the Jacobian, \(p_{}()(-)\). The roots of this polynomial, found by setting \(p_{}()=0\), are the eigenvalues of the system. Consider the block matrix,

\[-=_{11}&_{12}\\ _{21}&_{22}=-( _{s}}}{_{y}})-&- (_{s}}{_{s} _{s}}})\\ (}{_{a}})\, (_{s}_{s})&( }{_{a}})(-+\, (_{s}^{2}))-\] (7)Notice that \(_{11}\) and \(_{12}\) are diagonal and therefore they commute, i.e., \(_{11}_{12}=_{12}_{11}\), so we have that \((-)=(_{22}_{11}- _{21}_{12})\) which is a property of the determinant of block matrices with commuting blocks . Therefore, the characteristic polynomial of the linearized system after expansion of the terms and simplification is given by,

\[(-)=^{2}+ [(}{_{a}})+ (_{s}}}{_{y}})- (}{_{a}}) (_{s}^{2})]+(_{s}}}{_{y}_{a}})\] (8)

Finding the roots of this polynomial is thus a quadratic eigenvalue problem of the form \(()(^{2}++ )=0\), which has been studied extensively . \(()\) can be interpreted as the characteristic polynomial associated with a system of linear second-order differential equations with constant coefficients of the form \(}}+}}+ =\). Therefore, proving the stability of our system (i.e., \(()<0\) for \(\{:()=0\}\)), is equivalent to proving the asymptotic stability of \(}}+}+ =\).

Tisseur et al.  and Kirillov et al.  list a set of constraints on the damping matrix, \(\), and stiffness matrix, \(\), that yield a stable system, but they are _not_ directly applicable to our system. In the context of a high-dimensional mechanical system, our system falls under the category of _gyroscopically stabilized systems with indefinite damping_. Few results are known about the conditions leading to the stability of such systems. By constructing a Lyapunov function, we prove (Appendix B) the following stability theorem that is directly applicable to our system, following an approach similar to Kliem et al. .

**Theorem 4.1**.: _For a system of linear differential equations with constant coefficients of the form,_

\[}}+}+ =\] (9)

_where \(^{n n}\) and \(^{n n}\) is a positive diagonal matrix (hence \( 0\)), the dynamical system is globally asymptotically stable if \(\) is Lyapunov diagonally stable._

Since the stiffness matrix,

\[=(_{s}}}{_{y} _{a}})=(_{0}^ {2}^{2}+(^{2} ^{2})}}{_{y}_{a}})\] (10)

is a positive diagonal matrix, a sufficient condition for stability of the system is that the damping matrix, \(\), given by,

\[= _{1}+_{2}-_{3}\] \[= (}{_{a}})+ (_{s}}}{_{y}})- (}{_{a}}) (_{s}^{2})\] (11) \[= (}{_{a}})+ (_{0}^{2}^{2}+ (^{2}^{2})}}{_ {y}})-(}{_{a}}) (^{2}^{2}}{ _{0}^{2}^{2}+(^{2} ^{2})})\]

is _Lyapunov diagonally stable_, i.e., there exists a positive definite diagonal matrix \(\), such that \(+^{}\) is positive definite.

Since all of the parameters are positive, and the weights in the matrix \(\) are nonnegative, we can conclude the following: \(_{1}\) and \(_{2}\) are positive diagonal matrices and \(_{3}\) is a matrix with all positive entries (that may or may not be symmetric). Therefore, \(\) is a _Z-matrix_, meaning that its off-diagonal entries are nonpositive. Further, a _Z-matrix_ is _Lyapunov diagonally stable_ if and only if it is a nonsingular _M-matrix_. Intuitively, _M-matrices_ are matrices with non-positive off-diagonal elements and "large enough" positive diagonal entries. Berman & Plemmons  list 50 equivalent definitions of nonsingular _M-matrices_. We use the one that is best suited for our problem.

**Theorem 4.2**.: _(Chapter 6, Theorem 2.3 from ) A Z-matrix matrix \(^{n n}\) is Lyapunov diagonally stable if and only if there exists a convergent regular splitting of the matrix, that is, it has a representation of the form \(=-\), where \(^{-1}\) and \(\) have all nonnegative entries, and \(^{-1}\) has a spectral radius smaller than 1._

We now show that, indeed, \(\) has a _convergent regular splitting_ for all combinations of the circuit parameters and for all inputs. We have already shown that \(\) is a _Z-matrix_, therefore, the first condition of the theorem is satisfied. Next, we consider the following splitting \(=-\) with \(=_{1}+_{2}\) and \(=_{3}\). Since \(_{1}\) and \(_{2}\) are positive diagonal matrices, \(^{-1}\) is nonnegative, while \(\) is also nonnegative because \(_{3}\) has all positive entries. Therefore, the only condition left to satisfy is that the spectral radius of \(^{-1}\) is smaller than 1, or that the matrix is convergent.

The matrix \(=^{-1}=(_{1}+_{2})^{-1} _{3}\) can be written as,

\[=(}{+(_{a} /_{y})_{0}^{2}^{ 2}+(^{2}^{2})}})(^{2}^{2}}{_{0}^{2} ^{2}+(^{2}^{2} )})\] (12)

We prove the following theorem (Appendix D) which directly applies to \(\),

**Theorem 4.3**.: _A matrix \(\) of the form \(=()\,\,(/ (+))\) is convergent (i.e., its spectral radius is less than 1), if \(^{n n}\) and \(,,^{n}\) satisfy \(0<t_{i}<1\), \(u_{i} 0\), \(v_{i}>0\) and \(w_{ij} 0\) for all \(i,j\)._

Defining \(/(+(_{a}/ _{y})_{0}^{2}^{ 2}+(^{2}^{2})})\), \(^{2}^{2}\) and \(_{0}^{2}^{2}\), it can be seen that they satisfy the constraints of the theorem, and thus \(\) is convergent. This implies that \(\) has a _convergent regular splitting_ and, as a result, the linearized dynamical system is unconditionally globally asymptotically stable for all the values of parameters and inputs. Further, the global asymptotic stability of linearization implies the local asymptotic stability of the normalization fixed point for ORGaNICs.

This result holds even when the neurons have different time constants, regardless of their type, as no assumptions were made about the time constants. This finding is significant for machine learning, particularly for designing architectures based on ORGaNICs. It allows neurons/units to integrate information at varying time scales while maintaining a stable circuit that performs normalization dynamically. Moreover, analytical expressions for eigenvalues can be obtained in the following case,

**Theorem 4.4**.: _Let \(_{r}=\), the normalization matrix be given by \(=\), where \(\) is the all-ones matrix, and the parameters are scalars, i.e., \(_{y}=_{y}\), \(_{a}=_{a}\), \(_{0}=b_{0}\), and \(=\). Under these conditions, the eigenvalues of the system admit closed form solutions (detailed in Appendix C)._

This result is particularly useful for neuroscience as it elucidates the connection between ORGaNICs parameters and the strength and frequency of oscillatory activity. Since we followed a direct Lyapunov approach to prove Theorem 4.1 as shown in Appendix B, we can derive an _energy_ (viz., Lyapunov function) for ORGaNICs as shown in Appendix H.

**Theorem 4.5**.: _When \(_{r}=\), the energy (Lyapunov function) minimized by ORGaNICs in the vicinity of the normalization fixed point, is given by,_

\[V(,)=_{i=1}^{n}t_{i}}}{{y_{s}}_{2}}^{2} [}}{_{a_{i}}}}}(y_{i}-{y_{s_{i}}} )^{2}+(}y_{i}-}}{y_{s_{i}}})^{2}].\] (13)

_Where \(t_{i}\) are the diagonal entries of \(\) and \({y_{s}}_{i}\) (\(a_{s_{i}}\)) are the steady-state values of neurons \(y_{i}\) (\(a_{i}\))._

Specifically, for a two-dimensional model (one \(y\) neuron and one \(a\) neuron) this expression simplifies to reveal that ORGaNICs behave like a damped harmonic oscillator with _energy_,

\[V(y,a)=}{_{a}}^{2}^{2}+wb^{2}z^{2}}\,(y -^{2}^{2}+wb^{2}z^{2}}})^{2}+(y-bz)^ {2}\] (14)

This result demonstrates that ORGaNICs minimize the residual of the instantaneously reconstructed gated input drive (\(y-bz\)), while also ensuring that the principal neuron's response, \(y\), achieves DN. The balance between these objectives is governed by the parameters and the external input strength. With fixed parameters, weaker inputs, \(z\), cause the model to prioritize input matching over normalization, whereas stronger inputs increasingly engage the normalization objective.

## 5 Stability analysis for arbitrary recurrent weights

Now, we relax the constraint that the recurrent weight matrix must be identity, allowing \(_{r}\), and see how the stability result changes. This leads to the following set of equations,

\[_{y}}& =-++(-})(_{r} )\\ _{a}}&=-+_{0}^{2}^{2}+\,( ^{2})\] (15)The linear stability analysis becomes intractable for a general \(_{r}\) because we no longer have a closed-form analytical expression for the steady states of \(\) and \(\). Additionally, the characteristic polynomial cannot be expressed in a way similar to Eq.8. Nevertheless, for a two-dimensional system,

\[_{y}&=-y+bz+(1-)w_{r}y\\ _{a}&=-a+b_{0}^{2}^{2}+wy^{2}  a\] (16)

we can prove the following, with a detailed analysis provided in Appendix E.

**Theorem 5.1**.: _Given that the recurrence is contracting, i.e., \(0<w_{r} 1\), when \(z>0\) (\(z<0\)) there exists a unique fixed point with \(y_{s}>0\) (\(y_{s}<0\)) and \(a_{s}>0\), and it is asymptotically stable._

**Theorem 5.2**.: _Given that the recurrence is expansive, i.e., \(w_{r}>1\), there are either 1 or 3 fixed points of which at least one is asymptotically stable. When \(z>0\) (\(z<0\)) there exists exactly 1 fixed point with \(y_{s}>0\) (\(y_{s}<0\)) and \(a_{s}>0\), and it is asymptotically stable. If \(b_{0}>1-1/w_{r}\), there are no additional fixed points. If \(b_{0}<1-1/w_{r}\), there exist either \(0\) or \(2\) additional fixed points with \(y_{s}<0\) (\(y_{s}>0\)) and \(a_{s}>0\) whose stability cannot be guaranteed._

We plot the phase portraits for these different cases in Fig. 1. The key takeaway is that there is always a fixed point \((y_{s},a_{s})\) with \(a_{s}>0\) and \(y_{s}\) having the same sign as \(z\). This fixed point is asymptotically stable regardless of the value of \(w_{r}\). Based on these results and the proven stability of arbitrary dimensional ORGaNICs when \(_{r}=\) (as shown in Section 4), we conjecture that

**Conjecture 5.3**.: _Consider high-dimensional ORGaNICs with an arbitrary recurrent weight matrix \(_{r}\) and no constraints on the remaining parameters. If the norm of the input drive satisfies \(|||| 1\), and the maximum singular value of \(_{r}\) is constrained to be 1, then the system possesses at least one asymptotically stable fixed point._

This conjecture is supported by empirical evidence showing consistent stability, as ORGaNICs initialized with random parameters and inputs under these constraints have exhibited stability in 100%

Figure 1: **Phase portraits for 2D ORGaNICs with positive input drive. We plot the phase portraits of 2D ORGaNICs in the vicinity of the stable fixed points for contractive (a, d) and expansive (b, c, e, f) recurrence scalar \(w_{r}\). A stable fixed point always exists, regardless of the parameter values. (a-c), The main model (Eq. 16). (d-f), The rectified model (Eq. 102). Red stars and black circles indicate stable and unstable fixed points, respectively. The parameters for all plots are: \(b=0.5\), \(_{a}=2\,\), \(_{y}=2\,\), \(w=1.0\), and \(z=1.0\). For (a) & (d), the parameters are \(w_{r}=0.5\), \(b_{0}=0.5\), \(=0.1\); for (b) & (e), \(w_{r}=2.0\), \(b_{0}=0.5\), \(=0.1\); and for (c) & (f), \(w_{r}=2.0\), \(b_{0}=1.0\), \(=1.0\).**

of trials, see Fig. 4. We further speculate that ORGaNICs may be _typically_ stable beyond this regime as we find that 100% of trials yield a stable circuit when the constraint on the maximum singular value of \(_{r}\) is increased to 2, but it becomes unstable when it is increased to 3.

## 6 Experiments

We provide further empirical evidence in support of Conjecture 5.3 that ORGaNICs is asymptotically stable by showing that stability is preserved when training ORGaNICs using naive BPTT on two different tasks: 1) static classification of MNIST, 2) sequential classification of pixel-by-pixel MNIST. Because these ML tasks have no relevance for neurobiological or cognitive processes, we relax one aspect of the biological plausibility of ORGaNICs, specifically, allowing arbitrary (learned) nonnegative values for the intrinsic time constants.1

### Static input classification task

We first show that we can train ORGaNICs on the MNIST handwritten digit dataset  presented to the circuit as a static input. This setting corresponds to evolving the responses of the neurons dynamically until they reach a fixed point solution and using the steady-state firing rates of the principal neurons to predict the labels, akin to deep equilibrium models . While the fixed point of the circuit is known when \(_{r}=\) (given by Eq. 89), we allow \(_{r}\) to be learnable and parameterized it to have a maximum singular value of 1. This constraint allows us to find the fixed point responses of all the neurons without simulation, using a fixed point iteration scheme (Algorithm 1) that converges with great accuracy in a few (less than 5) steps, see Fig. 4 & 5. We provide an intuition for why this algorithm works with empirical evidence of fast convergence in Appendix G.

We trained ORGaNICs on this task (details provided in Appendix I.1) and compared its performance to SSN  trained by dynamics-neutral growth . We found that ORGaNICs perform better than SSN with the same model size, and on par with an MLP (Table 1). We analyzed the eigenvalues of the Jacobian matrix of the trained model and consistently found the largest real part to be negative (Fig. 5), indicating stability. Moreover, we found that stability was maintained during training (Fig. 6).

### Time varying input

We trained unconstrained ORGaNICs by naive BPTT on a classification task of sequential MNIST (sMNIST), proposed by Le et al. . This is a challenging task because it involves long-term dependencies and requires the architecture to maintain and integrate information over long timescales. Briefly, the task involves the presentation of pixels of MNIST images sequentially (one pixel for each

   Model & Accuracy \\  SSN (50:50) & 94.9\% \\ SSN (80:20) & 95.2\% \\ MLP (50) & 98.2\% \\ 
**ORGaNICs** (50:50) & 98.1\% \\
**ORGaNICs** (80:80) & 98.2\% \\
**ORGaNICs** (two layers) & 98.1\% \\   

Table 1: Test accuracy on MNIST datasettimestep) in scanline order, and at the end of the input the model has to predict the digit that was presented. There is a more complicated version of this task, permuted sequential MNIST, in which the pixels of all images are permuted in some random order before being presented sequentially. We train ORGaNICs with different hidden layer sizes (number of \(\) neurons) on these two tasks by discretizing the rectified ORGaNICs with arbitrary recurrence, Eq. 87, which has all the properties that we have derived for the main model. Since an unstable fixed point is undesirable in such a task, as it may lead to diverging trajectories, we prefer the rectified model (Appendix F) over the main model. We proved that the 2D rectified ORGaNICs (Eq. 102) does not exhibit an unstable fixed point for positive inputs, as it can also be seen in Fig 1. The hidden states of the neurons are initialized with a uniform random distribution (for more details, see Appendix I.2). Additionally, we make the input gains \(\) and \(_{0}\) dynamical with their ODEs given by,

\[_{b}} =-+f(_{bx}+_{by}+_{ba})\] (17) \[_{b_{0}}}_{0} =-_{0}+f(_{box}+_{boy} +_{boa})\]

We achieved slightly better performance than LSTMs on sMNIST with a smaller model size and comparable performance on permuted sMNIST, without hyperparameter optimization and without gradient clipping/scaling (Table 2). We found that the trajectories of \(\) are bounded when it is trained on the sequential task (Fig. 7), indicating stability. We also show that the training of ORGaNICs is stable and does not require gradient clipping when the intrinsic time constants of the neurons are fixed (Table 2).

## 7 Discussion

**Summary:** While extensive research has been aimed at identifying highly expressive RNN architectures that can model complex data, there has been little advancement in developing robust, biologically plausible recurrent neural circuits that are easy to train and perform comparably to their artificial counterparts. Regularization techniques such as batch, group, and layer normalization have been developed and are implemented as ad hoc add-ons making them biologically implausible. In this work, we bridge these gaps by leveraging the recently proposed ORGaNICs model which implements divisive normalization (DN) dynamically in a recurrent circuit. We establish the unconditional stability of an arbitrary dimensional ORGaNICs circuit with an identity recurrent weight matrix (\(_{r}\)), with all of the other parameters and inputs unconstrained, and provide empirical evidence of stability for ORGaNICs with arbitrary \(_{r}\). Since ORGaNICs remain stable for all parameter values and inputs, we do not need to resort to techniques that are restrictive in parameter space, or that require designing unrealistic structures for weight matrices. ORGaNICs' intrinsic stability mitigates the issues of exploding and oscillating gradients, enabling the use of "vanilla" BPTT without the need for gradient clipping, which is instead required when training LSTMs. Moreover, ORGaNICs effectively address the vanishing gradient problem often encountered when training RNNs. This is achieved by processing information across various timescales, resulting in a blend of lossy and non-lossy neurons, while preserving stability. The model's effectiveness in overcoming vanishing gradients is further evidenced by its competitive performance against architectures specifically designed to address this issue, such as LSTMs.

**Dynamic normalization:** Normalization techniques, such as batch and layer normalization, are fundamental in modern ML architectures significantly enhancing the training and performance of

   Model & sMNIST & psMNIST & \# units & \# params \\  LSTMs  & 97.3\% & 92.6\% & 128 & 68k \\ AntisymmetricRNN  & 98.0\% & 95.8\% & 128 & 10k \\ coRNN  & 99.3\% & 96.6\% & 128 & 34k \\ Lipschitz RNN  & 99.4\% & 96.3\% & 128 & 34k \\ 
**ORGaNICs** (fixed time constants) & 90.3\% & 80.3\% & 64 & 26k \\
**ORGaNICs** (fixed time constants) & 94.8\% & 84.8\% & 128 & 100k \\ 
**ORGaNICs** & 97.7\% & 89.9\% & 64 & 26k \\
**ORGaNICs** & 97.8\% & 90.7\% & 128 & 100k \\   

Table 2: Test accuracy on sequential pixel-by-pixel MNIST and permuted MNISTCNNs. However, a principled approach to incorporating normalization into RNNs has remained elusive. While layer normalization is commonly applied to RNNs to stabilize training, it does not influence the underlying circuit dynamics since it is applied a-posteriori to the output activations, leaving the stability of RNNs unaffected. Furthermore, DN has been shown to generalize batch and layer normalization , leading to improved performance . ORGaNICs, unlike RNNs with layer normalization, implement DN dynamically within the circuit, marking the first instance of this concept being applied and analyzed in ML. Our work demonstrates that embedding DN within a circuit naturally leads to stability, which is greatly advantageous for trainability. This stability, a consequence of dynamic DN, sets ORGaNICs apart from other RNNs by providing both output normalization and model robustness. As a result, ORGaNICs can be trained using BPTT, achieving performance on par with LSTMs. The key insight is that the dynamic application of DN not only enhances training efficiency but also improves model robustness. This illustrates how the incorporation of neurobiological principles can drive advances in ML.

**Interpretability:** In the proof of stability, we establish a direct connection between ORGaNICs and systems of coupled damped harmonic oscillators, which have long been studied in mechanics and control theory. This analogy not only enables us to derive an interpretable energy function for ORGaNICs (Eq. 13), providing a normative principle of what the circuit aims to accomplish, but also sheds light on the link between normalization and dynamical stability of neural circuits. For a relevant ML task, having an analytical expression for the energy function allows us to quantify the relative contributions of the individual neurons in the trained model, offering more interpretability than other RNN architectures. For instance, Eq. 13 shows that the ratio of time constants (\(_{y}/_{a}\)) for E-I neuron pairs determines how much weight a neuron assigns to divisive normalization relative to aligning its responses with the input drive \(\). This insight provides a clear functional role for each neuron in the trained model. Moreover, since ORGaNICs are biologically plausible, we can understand how the various components of the dynamical system might be computed within a neural circuit , bridging the gap between theoretical models and biological implementation, and offering a means to generate and test hypotheses about neural computation in real biological systems (which we will be reporting elsewhere).

**Limitations:** Although the stability property pertains to a continuous-time system of nonlinear differential equations, typical implementations for tasks with sequential data involve an Euler discretization of these equations for training purposes. This might lead to a stiff dynamical system, potentially causing numerical instabilities and explosive dynamics, highlighting the importance of carefully parameterizing time constants and choosing a small enough time step to maintain stable dynamics. The proof of unconditional stability is only tractable for the two-dimensional circuit and the high-dimensional circuit with \(_{r}=\). Therefore, we can only conjecture the stability of ORGaNICs for arbitrary \(_{r}\), based on these two limiting cases and on empirical evidence. In the current form, the weight matrices of the input gain modulators, \(_{by}\), \(_{ba}\), \(_{boy}\), and \(_{boa}\), are each \(n n\). As a result, the number of parameters grows more rapidly with the hidden state size compared to other RNNs. To mitigate this, we plan to explore using compact and/or convolutional weights to prevent a significant increase in the number of parameters as the hidden state size expands.

**Attention mechanisms in ORGaNICs:** ORGaNICs have a built-in mechanism for attention: modulating the input gain \(\) (e.g., Eq. 17), coupled with DN. This attention mechanism aligns with experimental data on both increases in the gain of neural responses and improvements in behavioral performance . Moreover, this mechanism performs a computation that is analogous to that of an attention head in ML systems (including transformers ) as both operate by changing the gain over time. In ORGaNICs, DN replaces the softmax operation typically used in an attention head.

**Future work:** This study has explored only a single layer of ORGaNICs for the sequential tasks. Future work will examine how stacked layers with feedback connections, similar to those in the cortex, perform on benchmarks for sequential modeling and also on cognitive tasks with long-term dependencies. We have thus far shown that ORGaNICs can address the problem of long-term dependencies by learning intrinsic time constants. Future investigations will assess the performance of ORGaNICs for tasks with long-term dependencies by learning to modulate the responses of the \(\) and \(\) neurons to control the effective time constant of the recurrent circuit (without changing the intrinsic time constants) , i.e., implementing a working memory circuit capable of learning to maintain and manipulate information across various timescales.