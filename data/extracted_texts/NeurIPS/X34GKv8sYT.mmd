# Lorentz-Equivariant Geometric Algebra Transformers for High-Energy Physics

Jonas Spinner

Heidelberg University

j.spinner@thphys.uni-heidelberg.de

&Victor Breso

Heidelberg University

v.breso@thphys.uni-heidelberg.de

&Pim de Haan

Qualcomm AI Research

&Tilman Plehn

Heidelberg University

&Jesse Thaler

MIT / IAIFI

&Johann Brehmer

Qualcomm AI Research

Equal contributionQualcomm AI Research is an initiative of Qualcomm Technologies, Inc.

###### Abstract

Extracting scientific understanding from particle-physics experiments requires solving diverse learning problems with high precision and good data efficiency. We propose the Lorentz Geometric Algebra Transformer (L-GATr), a new multi-purpose architecture for high-energy physics. L-GATr represents high-energy data in a geometric algebra over four-dimensional space-time and is equivariant under Lorentz transformations, the symmetry group of relativistic kinematics. At the same time, the architecture is a Transformer, which makes it versatile and scalable to large systems. L-GATr is first demonstrated on regression and classification tasks from particle physics. We then construct the first Lorentz-equivariant generative model: a continuous normalizing flow based on an L-GATr network, trained with Riemannian flow matching. Across our experiments, L-GATr is on par with or outperforms strong domain-specific baselines.

## 1 Introduction

In the quest to understand nature on the most fundamental level, machine learning is omnipresent . Take the most complex machine ever built: at CERN's Large Hadron Collider (LHC), protons are accelerated to close to the speed of light and interact; their remnants are recorded by various detector components, totalling around \(10^{15}\) bytes of data per second . These data are filtered, processed, and compared to theory predictions, as we sketch in Fig. 1. Each step of this pipeline requires making decisions about high-dimensional data. More often than not, these decisions are rooted in machine learning, increasingly often deep neural networks . This approach powers most measurements in high-energy physics, culminating in the Higgs boson discovery in 2012 .

High-energy physics analyses put stringent requirements on network architectures. They need to be able to represent particle data and have to be expressive enough to learn complex relations in high-dimensional spaces precisely. Moreover, training data often come from precise theory computations and complex detector simulations, both of which require a considerable computational cost; architectures therefore need to be data efficient. Generative models of particle-physics data face additional challenges: because of detector boundaries and selection cuts, densities frequently feature sharp edges; at the same time, it is often important to model low-density tails of distributions precisely over multiple orders of magnitude of probability densities.

Off-the-shelf architectures originally developed for vision or language are popular starting points forhigh-energy physics applications [19; 37], but do not satisfy these goals reliably. We argue that this is because they do not make systematic use of the rich structure of the data. Particle interactions are governed by quantum field theories and respect their symmetries, notably the Lorentz symmetry of special relativity [39; 63]. First Lorentz-equivariant architectures have recently been proposed [10; 42; 68], but they are limited to specific applications and not designed with a focus on scalability.

In this work, we introduce the Lorentz Geometric Algebra Transformer (L-GATr), a new general-purpose network architecture for high-energy physics. It is based on three design choices. First, L-GATr is equivariant with respect to the Lorentz symmetry.3 It supports partial and approximate symmetries as found in some high-energy physics applications through symmetry-breaking inputs. Second, as representations, L-GATr uses the geometric (or Clifford) algebra over the four-vectors of special relativity. This algebra is based on the scalar and four-vector properties that LHC data are naturally parameterized in and extends them to higher orders, increasing the network capacity. Finally, L-GATr is a Transformer. It supports variable-length inputs, as found in many LHC problems, and even large models can be trained efficiently. Because it computes pairwise interactions through scaled dot-product attention, for which there are highly optimized backends like Flash Attention , the architecture scales particularly well to problems with many tokens or particles.

L-GATr is based on the Geometric Algebra Transformer architecture [14; 36], which was designed for non-relativistic problems governed by the Euclidean symmetry \((3)\) of translations, rotations, and reflections. Our L-GATr architecture generalizes that to relativistic scenarios and the Lorentz symmetry. To this end, we develop several new network layers, including a maximally expressive Lorentz-equivariant linear map, a Lorentz-equivariant attention mechanism, and Lorentz-equivariant layer normalization.

In addition to the general-purpose architecture, we develop the first (to the best of our knowledge) Lorentz-equivariant generative model. We construct a continuous normalizing flow with an L-GATr denoising network and propose training it with a Riemannian flow matching approach . This not only lets us train the model in a scalable way, but also allows us to encode more aspects of the problem geometry into the model: we can even hard-code phase-space boundaries, which are commonplace in high-energy physics.

We demonstrate L-GATr on three particle-physics applications. We first train neural surrogates for quantum field theoretic amplitudes, a regression problem with high demands on precision. Next, we train classification models and evaluate L-GATr on the popular benchmark problem of top tagging. Finally, we turn to the generative modelling of reconstructed particles, which can make the entire analysis pipeline substantially more efficient. The three applications differ in the role they play in the LHC analysis pipeline (see Fig. 1), data, and learning objective, highlighting the versatility of L-GATr. We find that L-GATr is on par with or outperforms strong domain-specific baselines across all problems, both in terms of performance and data efficiency.

Our implementation of L-GATr is available at https://github.com/heidelberg-hepml/lorentz-gatr.

## 2 Background and related work

High-energy physicsIn Fig. 1 we sketch the typical data-analysis pipeline in particle physics. Its central idea is to take the data collected in the detectors as well as the predictions from different theories of physics, process both in parallel, and ultimately compare their predictions. The pipeline includes various steps, including the computation of scattering probabilities or amplitudes in mathematical frameworks called quantum field theories, the Monte-Carlo sampling from the theory, the simulated interaction of particles with the detector, the dimensionality reduction of the raw detector output to a small number of observables, the data filtering to extract only collisions of interest, and the statistical analysis of whether two predictions are consistent.

What most steps in this pipeline have in common is the notion of _particles_, the main representation of data in high-energy physics. A particle is characterized by a discrete type label, an energy \(E\), and a spatial momentum \(^{3}\). Types include fundamental particles like electrons, photons, quarks, and gluons, composite particles like protons, as well as reconstructed objects like "jets"  or "particle-flow candidates" , which are the outputs of complex reconstruction algorithms. The energy and spatial momentum of a particle are conveniently combined into a _four-momentum_\(p=(E,)\).

The laws of fundamental physics  are invariant with respect to the choice of an inertial reference frame : they do not change under rotations and boosts from one un-accelerated reference frame into another.4 Together, these transformations form the _special orthochronous Lorentz group_\(^{+}(1,3)\).5 This group is the connected component of the orthogonal group on the four-vector space \(^{1,3}\) with Minkowski metric \((+1,-1,-1,-1)\). Lorentz transformations mix temporal and spatial components. Space and time should therefore not be considered as separate concepts, but rather as components of a four-dimensional space-time. Particle four-momenta are another instance of this: they transform in the vector representation of the Lorentz group as \(p^{} p^{}=_{}_{}^{}p^{}\) for \(^{+}(1,3)\), with the Lorentz transformation mixing energy and spatial momentum.

Geometric deep learningThe central tenet of geometric deep learning  is to embed the known structure of a problem into the architecture used to solve it, instead of having to learn it completely from data. The key idea is that of _equivariance_ to symmetry groups: when the inputs \(x\) to a network \(f\) are transformed with a symmetry transformation \(g\), the outputs should transform under the same element of the symmetry group, \(f(g x)=g f(x)\), where \(\) denotes the group action. What is known as "equivariance" in machine learning is often called "covariance" in physics .

GATrOur work is rooted in the Geometric Algebra Transformer (GATr) , a network architecture that is equivariant to \((3)\), the group of non-relativistic translations, rotations, and reflections. GATr represents inputs, hidden states, and outputs in the geometric (or Clifford) algebra \(_{3,0,1}\). A geometric algebra extends a base space like \(^{3}\) to higher orders and adds a bilinear map known as the _geometric product_. We provide a formal introduction in Appendix A. What matters in practice is that this vector space can represent various 3D geometric objects. Brehmer et al.  develop different layers for this representation and combine them in a Transformer architecture . For L-GATr, we build on the GATr blueprint, but re-design all components such that they can represent four-momenta and are equivariant with respect to Lorentz transformations.

Lorentz-equivariant architecturesRecently, some Lorentz-equivariant architectures have been proposed. Most closely related to this work is the Clifford Group Equivariant Neural Networks (CGENN) by Ruhe et al.  and their extensions to simplical complexes  and steerable convolutions . Like us, they use the geometric algebra over four-vectors. While they also use Lorentz-equivariant linear maps and geometric products, our architectures differ in a number of ways. In particular, they propose a message-passing graph neural network, while we build a Transformer archi

Figure 1: Schematic view of the data-analysis workflow in high-energy physics. Measurements (top) are processed in parallel with simulated data (bottom); their comparison is ultimately the basis for most scientific conclusions. In orange, we show how the three applications of L-GATr we experiment with in this paper fit into this workflow. The architecture is also applicable in several other stages, including reconstruction and inference.

tecture based on dot-product attention.

Other Lorentz-equivariant architectures include LorentzNet  and the Permutation Equivariant and Lorentz Invariant or Covariant Aggregator Network (PELICAN) . Both are message-passing graph neural network as well. Given a set of four-vectors, PELICAN computes all pairwise inner products, which are Lorentz invariants, and then processes them with a permutation-equivariant architecture. LorentzNet maintains scalar and four-vector representations and updates them with a graph attention mechanism similar to the one proposed by Villar et al. .

Flow matchingContinuous normalizing flows  are a class of generative models that push a sample from a base density through a transformation defined by an ordinary differential equation.

Specifically, the evolution of a point \(x^{d}\) is modelled as a time-dependent flow \(_{t}:^{d}^{d}\) with \(}{t}_{t}(x)=u_{t}(_{t}(x)),_{0}(x)=x\), where \(u_{t}\) is a time-dependent vector field.

Conditional flow matching [3; 53] is a simple and scalable training algorithm for continuous normalizing flows that does not require the simulation of trajectories during training. Instead, the objective is to match a vector field \(v_{t}(x)\), parametrized by a neural network, onto a conditional target vector field \(u_{t}(x|x_{1})\) along a conditional probability path \(p_{t}(x|x_{1})\), minimizing the loss \(_{}=_{t d,x_{1} q(x_{1}),x p _{t}(x|x_{1})}\|v_{t}(x)-u_{t}(x|x_{1})\|^{2}\), where \(x_{1} q(x_{1})\) are samples from the base distribution.

Choosing a well-suited probability path and corresponding target vector field can substantially improve the data efficiency and sampling quality. A principled approach to this choice is Riemannian flow matching (RFM) . Instead of connecting target and latent space points by straight lines in Euclidean space, RFM proposes to choose probability paths based on the metric of the manifold structure of the data space. If available in closed form, they propose to use geodesics as probability paths, which corresponds to optimal transport between base and data density.

## 3 The Lorentz Geometric Algebra Transformer (L-GATr)

### Lorentz-equivariant architecture

Geometric algebra representationsThe inputs, hidden states, and outputs of L-GATr are variable-size sets of tokens. Each token consists of \(n\) copies of the geometric algebra \(_{1,3}\) and \(m\) additional scalar channels.

The geometric algebra \(_{1,3}\) is defined formally in Appendix A. In practice, \(_{1,3}\) is a 16-dimensional vector space that consists of multiple subspaces (or grades). The \(0\)-th grade consists of scalars that do not transform under Lorentz transformations, for instance embeddings of particle types or regression amplitudes. The first grade contains space-time four-vectors such as the four-momenta \(p=(E,)\). The remaining grades extend these objects to higher orders (i. e. antisymmetric tensors), increasing expressivity. In addition, the geometric algebra defines a bilinear map, the geometric product \(_{1,3}_{1,3}_{1,3}\), which contains both the space-time inner product and a generalization of the Euclidean cross product.

This representation naturally fits most LHC problems, which are canonically represented as sets of particles, each parameterized with type information and four-momenta. We represent each particle as a token, store the particle type as a one-hot embedding in the scalar channels and the four-momentum in the first grade of the geometric algebra.

Lorentz-equivariant linear layersWe define several new layers that have both \(_{1,3}\) and additional scalar representations as inputs and outputs. For readability, we will suppress the scalar channels in the following. We require each layer \(f(x)\) to be equivariant with respect to Lorentz transformations \(^{+}(1,3)\): \(f( x)= f(x)\), where \(\) denotes the action of the Lorentz group on the geometric algebra (see Appendix A). Lorentz equivariance strongly constrains linear maps between geometric algebra representations:6

**Proposition 1**.: _Any linear map \(:_{1,3}_{1,3}\) that is equivariant to \(^{+}(1,3)\) is of the form_

\[(x)=_{k=0}^{4}v_{k} x_{k}+_{k=0}^{4}\,w_{ k}e_{0123} x_{k}\] (1)

_for parameters \(v,w^{5}\). Here \(e_{0123}\) is the pseudoscalar, the unique highest-grade basis element in \(_{1,3}\); \( x_{k}\) is the blade projection of a multivector, which sets all non-grade-\(k\) elements to zero._

We show this in Appendix A. In our architecture, linear layers map between multiple input and output channels. There are then ten learnable weights \(v_{k},w_{k}\) for each pair of input and output \(_{1,3}\) channels (plus the usual weights for linear maps between the additional scalar channels).

Lorentz-equivariant non-linear layersWe define four additional layers, all of which are manifestly Lorentz-equivariant. The first is the scaled dot-product attention

\[(q,k,v)_{i^{}c^{}}=_{i}_{i} \!(_{c=1}^{n_{c}}c},k_{ic}}{ {16n_{c}}})v_{ic^{}}\,,\] (2)

where the indices \(i,i^{}\) label tokens, \(c,c^{}\) label channels, \(n_{c}\) is the number of channels, and \(,\) is the \(_{1,3}\) inner product. This inner product can be rewritten as a pre-computed list of signs and a Euclidean inner product, which is why we can compute the attention mechanism with efficient backends developed for the original Transformer architecture, for instance Flash Attention . This is key to the good scalability of L-GATr, which we will demonstrate later.

When defining a normalization layer, we have to be careful: in the \(_{1,3}\) inner product, cancellations between positive-norm directions and negative-norm directions can lead to norm values much smaller than the scale of the individual components; dividing by the norm then risks blowing up the data. These cancellations are an unavoidable consequence of the geometry of space-time. We mitigate this issue by using the grade-wise absolute value of the inner product in the norm

\[(x)=x}_{c=1}^{n_{c}}_{k= 0}^{4} x_{c}_{k}, x_{c}_{k} }+\,,\] (3)

applying an absolute value around each grade of each multivector channel \( x_{c}_{k}\). Here \(>0\) is a constant that further numerically stabilizes the operation. This normalization was proposed by De Haan et al.  for \((3)\)-invariant architectures, we adapt it to the Lorentz-equivariant setting.

We also use the geometric product \((x,y)=xy\) defined by the geometric algebra \(_{1,3}\). Finally, we use the scalar-gated GELU  nonlinearities \((x)=( x_{0})x\), as proposed by Brehmer et al. .

Transformer architectureWe combine these layers into a Transformer architecture :

\[ =(x)\,,\] \[(x) =(( ),(),())+x\,,\] \[(x) = ((),())+x\,,\] \[(x) =(x)\,,\] \[(x) =(x)\,.\]

This L-GATr architecture is structurally similar to the original GATr architecture , but the representations, linear layers, attention mechanism, geometric product, and normalization layer are different to accommodate the different nature of the data and different symmetry group.

Lorentz symmetry breakingWhile fundamental physics is (to the best of our knowledge) symmetric under Lorentz transformations, the LHC measurement process is not. The direction of the proton beams presents the most obvious violation of this symmetry. Smaller violations are due to the detector resolution: particles hitting the central part of the detector (orthogonal to the beam in the detector rest frame) are typically reconstructed with a higher precision than those emerging at a narrow angleto the beam. Even smaller violations come, for instance, from individual defunct detector elements. Solving some tasks may therefore benefit from a network that can break Lorentz equivariance.

L-GATr supports such broken or approximate symmetries by including the symmetry-breaking effects as additional inputs into the network. Concretely, whenever we analyze reconstruction-level data, we include the beam directions; see Appendix C. This approach combines the strong inductive biases of a Lorentz-equivariant architecture with the ability to learn to break the symmetry when required.

### Lorentz-equivariant flow matching

In addition to regression and classification models, we construct a generative model for particle data. Besides the strict requirements on precision, flexibility, and data efficiency, generative models of LHC data need to be able to address sharp edges and long tails in high-dimensional distributions.

We develop a continuous normalizing flow based on an L-GATr vector field and train it with Riemannian flow matching (RFM) . This approach has several compelling properties: training is simulation-free and scalable and the generative model is Lorentz-equivariant.1 In addition, the RFM approach allows us to deal with sharp edges and long tails in a geometric way: we parameterize the reachable four-momentum space for each particle as a manifold and use geodesics on this manifold as probability paths from base samples to data points.

Probability paths perfect for particlesConcretely, reconstructed particles \(p=(E,)\) are often required to satisfy constraints of the form \(p_{1}^{2}+p_{2}^{2} p_{T,}^{2}\) and \(p^{2}>0\). Following Refs. [17; 18; 45; 48], we parameterize this manifold with physically motivated coordinates \(y=(y_{m},y_{p},,)\). These variables form an alternative basis for the particle four-momenta and are defined through the map

\[p=(E,p_{x},p_{y},p_{z})=f(y)=(+p_{T}^{2}^{2}},\ p_{T} ,\ p_{T},\ p_{T}),\] (4)

where \(m^{2}=(y_{m})\) and \(p_{T}=p_{T,}+(y_{p})\). This basis is better aligned with the physically relevant properties of particles in the context of a collider experiment: \(\) and \(\) represent the angle in which a particle is moving, \(y_{p}\) is a measure of the momentum with which it moves away from the collision, and \(y_{m}\) is related to its mass.

We define a constant diagonal metric in the coordinates \(y\) and use the corresponding geodesics as probability paths. This Riemannian manifold is geodesically convex, meaning any two points are connected by a unique geodesic, and geodesically complete, meaning that paths thus never enter four-momentum regions forbidden by the phase-space cuts. By also running the ordinary differential equation (ODE) solver in these coordinates, we guarantee that each sample satisfies the four-momentum constraints. As an added benefit, this choice of metric compresses the high-energy tails of typical particle distributions and thus simplifies learning them correctly.

In Fig. 2, we show target probability paths generated in this way. Our approach ensures that none of the trajectories pass through the phase-space region \(p_{T}<p_{T,}\), where the target density does not have support; instead, the geodesics lead around this problematic region.

Figure 2: Target vector field for Riemannian flow matching. Our choice of metric space guarantees that the generative model respects phase-space boundaries (red circle).

## 4 Experiments

We now demonstrate L-GATr in three applications. Each addresses a different problem in the data-analysis pipeline sketched in Fig. 1.

### Surrogates for QFT amplitudes

ProblemWe first demonstrate L-GATr as a neural surrogate for quantum field theoretical amplitudes [6; 7; 8; 59; 60], the core of the theory predictions that LHC measurements are compared to. These amplitudes describe the (un-normalized) probability of interactions of fundamental particles as a function of their four-momenta. As this is a fundamental interaction and does not include the measurement process, it is exactly Lorentz-invariant. Evaluating them is expensive, on the one hand because it requires solving complex integrals, on the other hand because the number of relevant terms combinatorially grows with the number of particles. Neural surrogates can greatly speed up this process and thus enable better theory predictions, but accurately modelling the amplitudes of high-multiplicity processes has been challenging.

As example processes, we study \(q Z+ng\), the production of a \(Z\) boson with \(n=1,,4\) additional gluons from a quark-antiquark pair. For each gluon multiplicity, we train a L-GATr model to predict the amplitude as a function of the four-momenta of the initial and final particles.8 The generation of the training data and the precise setup of the learning problem are described in Appendix C. We compare L-GATr to various baselines, including the Lorentz-equivariant message-passing architecture CGENN , a Transformer , and DSI, a baseline based on the Deep Sets framework  that we develop ourselves; we describe it in detail in Appendix B.

Surrogate qualityL-GATr consistently approximates the amplitudes with high precision, as we show in the left panel of Fig. 3. For a small number of particles, it is slightly worse than our own baseline DSI, but it scales much better to a large number of particles, where it outperforms all other methods. This is exactly the region in which neural surrogates could have the highest impact.

Data efficiencyIn the right panel of Fig. 3 we study the data efficiency of the different architectures. We find that L-GATr is competitive at any training data size, combining the small-data advantages of its strong inductive biases and the big-data advantages of its Transformer architecture.

Figure 3: Amplitude surrogates. **Left**: Surrogate error for processes of increasing particle multiplicity and complexity, training on the full dataset of \(4 10^{5}\) samples. L-GATr outperforms the baselines, especially at more complex processes. **Right**: Surrogate error as a function of the training dataset size.

### Top tagging

ProblemNext, we turn to the problem of classifying whether a spray of reconstructed hadrons originated from the decay of a top quark or any other process. This problem of top tagging is an important filtering step in any analysis that targets the physics of top quarks, the heaviest elementary particle in the Standard Model. Particle collisions involving these particles are of particular interest to physicists because the production and decay probabilities of top quarks are sensitive to several proposed theories of new physics, including for instance the existence of "supersymmetric" particles. We use the established top tagging dataset by Kasieczka et al. [49; 50] as a benchmark and compare to the published results for many algorithms and architectures.

ResultsAs shown in Tbl. 1, L-GATr is on par with or better than even the strongest baselines on this well-studied benchmark.

### Generative modelling

ProblemFinally, we study the generative modelling of reconstructed events as an end-to-end generation task [17; 18], bypassing the whole simulation chain visualized in Fig. 1. Such generative models can obliterate the computational cost of both the theory computations and the detector simulation at once. However, the high-dimensional distributions of reconstructed particles often have non-trivial kinematic features that are challenging for generative models to learn, for instance the properties of unstable resonances and angular correlations. We focus on the processes \(pp t+n\;\), the generation of top pairs with \(n=0 4\) additional jets, where the top quarks decay hadronically, \(t bq^{}^{}\).

We train continuous normalizing flows based on an L-GATr network with the Riemannian flow matching objective described in Sec. 3. As baselines, we consider similar flow matching models, but use MLP and Transformer networks as score models, as proposed by Refs. [18; 45]. We also construct a flow matching model using the E(3)-equivariant GATr from Ref. . Finally, we also train JetGPT  model, an autoregressive transformer architecture developed for particle physics that is not equivariant to the Lorentz symmetry.

Kinematic distributionsWe begin with a qualitative analysis of the samples from the generative models. In Fig. 4 we show example marginal distributions from the different models and compare them to the ground-truth distribution in the test set. We select three marginals that are notoriously difficult to model correctly for generative models. While the differences are subtle and only visible in tails and edges of the distributions, L-GATr matches the true distribution better than the baselines. However, none of the models are able to capture the kinematics of the top mass peak at percent-level precision yet.

   Model & Accuracy & AUC & \(1/_{B}\) (\(_{S}=0.5\)) & \(1/_{B}\) (\(_{S}=0.3\)) \\  TopoDNN  & 0.916 & 0.972 & – & 295 \(\) & 5 \\ LoLa  & 0.929 & 0.980 & – & 722 \(\) & 17 \\ P-CNN  & 0.930 & 0.9803 & 201 \(\) & 4 & 759 \(\) & 24 \\ \(N\)-subjettiness  & 0.929 & 0.981 & – & 867 \(\) & 15 \\ PFN  & 0.932 & 0.9819 & 247 \(\) & 3 & 888 \(\) & 17 \\ TreeNiN  & 0.933 & 0.982 & – & 1025 \(\) & 11 \\ ParticleNet  & 0.940 & 0.9858 & 397 \(\) & 7 & 1615 \(\) & 93 \\ ParT  & 0.940 & 0.9858 & 413 \(\) & 16 & 1602 \(\) & 81 \\ LorentzNet*  & 0.942 & 0.9868 & 498 \(\) & 18 & 2195 \(\) & 173 \\ CGENN*  & 0.942 & 0.9869 & 500 & 2172 & \\ PELICAN*  & **0.9426**\(\) 0.0002 & **0.9870**\(\) 0.0001 & – & **2250**\(\) & 75 \\ L-GATr (ours)* & 0.9423 \(\) 0.0002 & **0.9870**\(\) 0.0001 & **540**\(\) & 20 & 2240 \(\) & 70 \\   

Table 1: Top tagging. We compare accuracy, area under the ROC curve (AUC), and inverse background acceptance rate \(1/_{B}\) at two different signal acceptance rates (or recall) \(_{S}(0.3,0.5)\) for the top tagging dataset from Kasieczka et al. . Lorentz-equivariant methods are indicated with an asterisk*; the best results for each metric are in **bold**. For L-GATr, we show the mean and standard deviation of five random seeds. Baseline results are taken from the literature.

Log likelihoodNext, we evaluate the generative models quantitatively through the log likelihood of data samples under the trained models; see Appendix C for details. The left panel of Fig. 5 shows that the L-GATr models outperform all baselines across all different jet multiplicities. They maintain this performance advantage also for smaller training data size, as shown in the right panel. The flow models, including L-GATr, are more data-efficient than the autoregressive transformer JetGPT.

Classifier two-sample testHow close to the ground-truth distribution are these generative models really? Neither marginal distributions nor log likelihood scores fully answer this question, as the former neglect most of the high-dimensional information and the latter do not have a known ground-truth value to compare to. We therefore perform a classifier two-sample test . We find that L-GATr samples are difficult to distinguish from the ground-truth distribution: a classifier trained to discriminate them achieves only a ROC AUC of between 0.51 and 0.56, depending on the process. In contrast, Transformer and MLP distributions are more easily discriminated from the background, with ROC AUC results between 0.58 and 0.85. For details, see Appendix C.

Effect of Riemannian flow matchingHow important was our choice of probability paths through Riemannian flow matching for the performance of these models? In Tbl. 2 we compare the log likelihood of CFM L-GATr models that differ only in the probability paths. Clearly, the Riemannian flow matching approach that allows us to encode geometric constraints is crucial for a good performance. We find similarly large gains for all architectures.

### Computational cost and scalability

Finally, we briefly comment on L-GATr's computational cost. Compared to a vanilla Transformer, the architecture has some computational overhead because of the more complex linear maps. However, it scales exactly in the same way to large particle multiplicities, where both architectures are bottleneck by the same dot-product attention mechanism. At the same time, L-GATr is substantially more efficient than equivariant architectures based on message passing, both in terms of compute and memory. This is because high-energy physics problems do not lend themselves to sparse graphs, and for dense graphs, dot-product attention is much more efficient. See Appendix C for our measurements.

## 5 Discussion

Out of all areas of science, high-energy physics is a strong contender for the field in which symmetries play the most central role. Surprisingly, while particle physicists were quick to embrace machine learning, architectures tailored to the symmetries inherent in particle physics problems have received comparably little attention.

   Probability paths & NLL \\  Euclidean & -30.11 \(\) 0.98 \\ RFM & **-32.65 \(\) 0.01** \\   

Table 2: Benefit of Riemannian flow matching for generative models. We show the negative log likelihood on the \(t+0j\) test set (lower is better).

Figure 4: Generative modelling: Marginal distributions of reconstructed particles in the \(pp t+4\) jets process. We compare the ground-truth distribution (black) to three generative models: continuous normalizing flows based on a Transformer, MLP, or our L-GATr network. The three marginals shown represent kinematic features that are known to be challenging. The L-GATr flow describes them most accurately.

We introduced the Lorentz Geometric Algebra Transformer (L-GATr), a versatile architecture with strong inductive biases for high-energy physics: its representations are based on particle four-momenta, extended to higher orders in a geometric algebra, and its layers are equivariant with respect to the Lorentz symmetry of special relativity. At the same time, L-GATr is a Transformer, and scales favorably to large capacity and large numbers of input tokens.

We demonstrated L-GATr's versatility on diverse regression, classification, and generative modelling tasks from the LHC analysis workflow. For the latter, we constructed the first Lorentz-equivariant generative model based on Riemannian flow matching. Across all experiments, L-GATr performed as well as or better than strong baselines.

Still, L-GATr has its limitations. While the architecture scales better than comparable message-passing networks, it has some computational overhead compared to, for instance, efficient Transformer implementations. And while L-GATr should in principle be suitable for pretraining across multiple problems, we have not yet investigated its potential as a foundation model.

While the LHC is preparing for the high-luminosity runs and its legacy measurements, the high-energy physics community is optimizing all steps of the analysis pipeline. Deploying performant and data-efficient architectures such as L-GATr could improve this pipeline in many places. We hope that this will ultimately contribute to more precise measurements of nature at its most fundamental level.