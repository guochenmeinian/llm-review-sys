# MAGIS: LLM-Based Multi-Agent Framework

for GitHub Issue ReSolution

 Wei Tao

Fudan University

wtao18@fudan.edu.cn

&Yucheng Zhou

University of Macau

yucheng.zhou@connect.um.edu.mo

&Yanlin Wang

Sun Yat-sen University

wangylin36@mail.sysu.edu.cn

&Wenqiang Zhang

Fudan University

wqzhang@fudan.edu.cn

&Hongyu Zhang

Chongqing University

hyzhang@cqu.edu.cn

&Yu Cheng

The Chinese University of Hong Kong

chengyu@cse.cuhk.edu.hk

Corresponding author

###### Abstract

In software development, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing code. Large Language Models (LLMs) have shown promise in code generation but face difficulties in resolving Github issues, particularly at the repository level. To overcome this challenge, we empirically study the reason why LLMs fail to resolve GitHub issues and analyze the major factors. Motivated by the empirical findings, we propose a novel LLM-based **M**ulti-**A**gent framework for **G**itHub **I**ssue **re**S**olution, **MAGIS**, consisting of four agents customized for software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve **13.94%** GitHub issues, significantly outperforming the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the advanced LLM.

## 1 Introduction

In real-world software development, the code repository for a project is rarely set in stone. High-quality and popular software always evolves to address emergent bugs or new requirements. On platforms such as GitHub , issues typically signify the requirement for software evolution. However, addressing these issues poses significant challenges, as it requires implementing the code change across the entire repository and maintaining the existing functionality while integrating new capabilities. For example, django, a framework for over \(1.6\)M projects has \(34\)K issues . Consequently, resolving GitHub issues remains a significant challenge across academia and industry .

Large language models (LLMs) have demonstrated remarkable capabilities across a variety of tasks , including code generation and code understanding . Specifically, LLMs excel ingenerating function-level code, as evidenced by their performance on numerous benchmark datasets such as MBPP  and HumanEval . Despite their success, LLMs remain challenged in tasks that require advanced code generation capabilities, such as class-level code generation . Moreover, LLMs exhibit limitations in processing excessively long context inputs and are subject to constraints regarding their input context length . This limitation is particularly evident in repository-level coding tasks, such as solving GitHub issues, where the context comprises the entire repository, thus imposing constraints on directly using the full repository as input to LLMs.

To harness the full potential of LLMs, many LLM-based multi-agent systems are designed [23; 43; 52]. These methods have significantly improved LLMs' efficacy in code generation, enabling these systems to construct code repositories based on LLM. While these methods address the process of transitioning code repositories from inception to establishment, they rarely consider the handling of software evolution, e.g., resolving GitHub issues. For GitHub repositories, especially the popular ones, a large number of commits are pushed every day. These commits derive from a spectrum of evolutionary requirements that span bug fixes, feature additions, performance enhancements, etc . For open-source software, new requirements frequently emerge as issues in the project's repository.

Recently, Jimenez et al.  developed a benchmark, namely SWE-bench, to investigate the capability of popular LLMs in addressing GitHub issues. Their study reveals that LLMs fail to resolve over \(95\%\) of instances, even when file paths that require modifications are provided. This significantly low rate underscores the importance of understanding the reasons behind their suboptimal performance.

In this study, we analyze the factors impacting the effectiveness of LLMs in resolving GitHub issues. Furthermore, our empirical analysis has concluded a correlation between locating files/lines to be modified and the performance of resolving GitHub issues. Based on these insights, we propose a novel LLM-based multi-agent framework, termed MAGIS, comprising four types of agents: Manager, Repository Custodian, Developer, and Quality Assurance (QA) Engineer. Our approach facilitates the resolution of GitHub issues through collaboration among agents, each fulfilling a unique role: the Manager coordinates the entire process, the Repository Custodian enhances locating files, the Developer performs code changes after locating lines, and the QA Engineer reviews the code change.

In our experiment, we evaluate our framework on SWE-bench and compare its performance against existing popular LLMs, such as ChatGPT-3.5 , GPT-4 , and Claude-2 . The results demonstrate that our framework, utilizing GPT-4 as its base model, significantly outperforms baselines and achieves an eight-fold performance gain compared to the direct application of GPT-4. Further analysis reveals that additional factors, i.e., the planning of code change, locating lines within the code file, and code review process, can significantly influence the resolution rate.

Our main contributions are summarized as follows:

* We conduct an empirical analysis of LLMs in resolving GitHub issues and explore the correlation between locating code file/line, complexity of the code change, and the success rate in resolution.
* We propose a novel LLM-based multi-agent framework, MAGIS, to alleviate the limitations of existing LLMs on GitHub issue resolution. Both our designed four-type agents and their collaboration for planning and coding unlock LLMs' potential on the repository-level coding task.
* We compare our framework and other strong LLM competitors (i.e., GPT-3.5, GPT-4, and Claude-2) on the SWE-bench dataset. The results show MAGIS significantly outperforms these competitors. Further analysis confirms the effectiveness and necessity of our framework design.

## 2 Empirical Study

SWE-bench  reveals the challenges LLMs face in addressing GitHub issue resolution. For example, in their evaluation, GPT-4 can only resolve less than 2% issues of the test set. Conversely, in tasks like function-level code generation, LLMs exhibit superior performance (e.g., GPT-4 gets the score of \(67.0\) on HumanEval ). Given the complexity of GitHub issue resolution akin to repository-level coding, we aim to investigate **Why the Performance of Directly Using LLMs to Resolve GitHub Issue is Limited? (RQ 1)**. We answer this RQ from the following three aspects:

Locating the Files to be Modified.GitHub issue resolution is a repository-level coding task, distinguishing it from file-level coding tasks primarily in the challenge of locating the files requiringmodification. Jimenez et al.  employ the BM25 method  to retrieve relevant code files that are subsequently utilized as input to the LLM. After employing retrieval methods, it is necessary to select the top-\(K\) files or truncate the content based on the maximum context length of the LLM. Incorporating more files can enhance recall scores. However, it also imposes significant demands on the capabilities of LLMs. As demonstrated by the study , Claude-2 exhibits a decrease in the resolved ratio (from 1.96% to 1.22%) as recall scores increase (from 29.58 to 51.06). This decline may be attributed to the inclusion of irrelevant files or the limited capacity of LLMs to process longer contexts effectively. Consequently, optimizing the performance of LLMs can be better achieved by striving for higher recall scores with a minimized set of files, thus suggesting a strategic balance between recall optimization and the number of chosen files.

Locating the Lines to be Modified.Beyond the impact of file locating, we delve into the generation of failed instances when the correct modified files were provided. A typical code change consists of multiple hunks, each specifying the line numbers targeted for modification and detailing the changes made at these locations. To quantitatively analyze the accuracy of line localization, we use the line numbers' range of the modified content in the reference code change as the basis assuming that the correct modification location of the code change is uniquely determined in most cases. By calculating the coverage ratio of the line number ranges of the generated and reference, we can estimate the accuracy of line localization in the generation process, i.e.,

\[=^{n}_{j=0}^{m}|[s_{i},e_{i}] [s^{}_{j},e^{}_{j}]|}{_{i=0}^{n}(e_{i}-s_{i}+1)},\] (1)

where the numerator is the length of the intersection of modified lines between the reference divided into \(n\) hunks and the generation divided into \(m\) hunks, and the denominator is the number of modified lines in the reference. More details about Equation 1 can be found in Appendix A.1.

For \(574\) instances in the SWE-bench that experiments GPT-4 , the distribution of the coverage ratio between the results generated by three LLMs and the reference is shown in Fig. 1. From this, we observe that the performance of LLMs in generating the code change is probably related to their ability to locate code lines accurately (Detailed explanation can be found in Appendix A.2).

Furthermore, we assess the relationship between the coverage ratio and the issue resolution by calculating their correlation coefficient. Given that the distribution of these variables exhibits skewness, and the resolution result is binary (resolved or not), logistic regression is employed for the analysis across three LLMs. However, due to the limited number of successfully generated instances on GPT-4 and GPT-3.5, a statistically significant relationship is only detected in the result generated by Claude-2. The result, i.e., P-value < \(0.05\), shows statistical significance.

Specifically, with a coefficient, \(0.5997\), on Claude-2, there is a substantial and positive relation between improvements in the coverage ratio and the probability of successfully resolving issues, which demonstrates that locating lines is a key factor for GitHub issue resolution.

Complexity of the Code Changes.The complexity of the code change is reflected in various indices: the number of modified files, functions, hunks, and lines added or deleted. Firstly, we quantitatively assess the complexity by calculating the value of various indices corresponding to the reference code change. Secondly, the coefficient is calculated between the numbers in each index and the issue resolution. Tab. 1 shows the correlation scores under the logistic regression.

As shown in Tab. 1, all three LLMs demonstrate a statistically significant correlation with the issue resolution across several indices. The correlation scores for the number of files and functions modified

Figure 1: The comparison of line locating coverage ratio between three LLMs. The vertical axis representing the frequency of the range of line locating coverage ratio for each group, and the horizontal axis representing the coverage ratio.

are notably negative for all models, indicating that an increase in these indices is associated with a decreasing likelihood of issue resolution. This suggests that the more complex the code change, as indicated by a higher number of files and functions modified, may hinder the issue resolution. More analysis can be found in Appendix A.3. The analysis reveals a relationship between the complexity, as measured by several indices, and whether to successfully resolve the issues in software evolution. The negative correlations suggest that increased complexity, particularly in terms of the number of files and functions changed, tends to hinder issue resolution.

## 3 Methodology

Based on the empirical study identifying key factors affecting LLMs' issue resolution, we design the framework illustrated in Fig. 2. This framework aims to mitigate negative impacts by transforming the complex task of GitHub issue resolution into a collaborative effort. It incorporates four key roles for LLM-based agents working collaboratively in the workflow: 1_Manager_: this role tasks with team assembly, meeting organization, and plan formulation. 2_Repository Custodian_: it is responsible for locating the relevant files in the repository according to the GitHub issue and recording the change of the repository. 3_Developer_: this role participates in planning discussions and completes tasks from the Manager. 4_Quality Assurance (QA) Engineer_: it reviews the code change from Developers to ensure the quality of the whole repository.

The collaborative process involves planning and coding. In the planning, an issue is assigned to the Manager and the Repository Custodian. The custodian identifies candidate files relevant to the issue for modification. With the issue description and a list of candidate files, the Manager defines tasks and assembles a team, where each member is a Developer specifically designed for the defined task. The Manager holds a kick-off meeting with Developers and devises a plan. During coding, Developers undertake their assigned tasks from the Manager, and the QA Engineer reviews each code change. If a change fails to meet quality standards, the QA Engineer provides feedback, prompting further revisions until the QA Engineer approves or a set iteration limit is reached. More details can be found in our GitHub repository 2.

   LLM & \# Files & \# Functions & \# Hunks & \# Added LoC & \# Deleted LoC & \# Changed LoC \\  GPT-3.5 & \(-17.57^{*}\) & \(-17.57^{*}\) & \(-0.06^{*}\) & \(-0.02\) & \(-0.03\) & \(-0.53^{*}\) \\ GPT-4 & \(-25.15^{*}\) & \(-25.15^{*}\) & \(-0.06\) & \(-0.10\) & \(-0.04\) & \(-0.21\) \\ Claude-2 & \(-1.47^{*}\) & \(-1.47^{*}\) & \(-0.11^{*}\) & \(-0.09^{*}\) & \(-0.07^{*}\) & \(-0.44^{*}\) \\   

* The correlation between the index and the issue resolution is significant (P-value \(<0.05\)).

Table 1: Correlation between the complexity indices and the issue resolution.

Figure 2: Overview of our framework, MAGIS. The detailed version can be found in Fig. 14.

### Agent Role Design

Our workflow draws inspiration from the GitHub Flow, an effective human workflow paradigm adopted by many software teams. Both the human workflow and our LLM-based agent framework prioritize collaboration among individuals with diverse skills. While the underlying principles are similar, there are notable differences. Accordingly, we have tailored the roles as follows:

* **Manager**. The Manager's role is pivotal in planning. In conventional setups, managers decompose the issue into tasks according to the pre-formed team and allocate these tasks for members with different skills. In contrast, our Manager agent can first decompose the issue into tasks and then design Developer agents to form a team. This setup improves team flexibility and adaptability, enabling the formation of teams that can meet various issues efficiently.
* **Repository Custodian**. Considering extensive files in a repository, the custodian agent's task is to locate files relevant to the issue. Unlike humans, who can browse through the entire repository, the LLM-based agent faces challenges in browsing. Although LLMs have extended context limits, their application is constrained in two aspects. First, it is a high computational cost to query each file in an entire repository for each update, particularly when some repositories update frequently. Second, the performance of LLMs degrades when the context input is long .
* **Developer**. Compared to human developers, the Developer agent can work continuously and efficiently. Therefore, scheduling the agent to work in parallel is easier than scheduling humans who require considering factors beyond the task. Additionally, although numerous developer agents are capable of generating code , their ability to modify existing code is not equally proficient. To address this issue, our framework decomposes the code modification process into sub-operations including code generation. This approach enables Developers to leverage the benefits of automatic code generation thereby producing applicable code changes.
* **QA Engineer**. In software evolution, QA Engineers play a crucial role in maintaining software quality through code review . Despite their importance, code review practices are often undervalued or even overlooked . Such neglect can hinder software development, illustrated by instances where developers may experience delays of up to 96 hours awaiting code review feedback . To address this problem, our framework pairs each Developer agent with a QA Engineer agent, designed to offer task-specific, timely feedback. This personalized QA approach aims to boost the review process thereby better ensuring the software quality.

### Collaborative Process

#### 3.2.1 Planning

Three types of role agents engage in the planning: Repository Custodian, Manager, and Developer. This process comprises three phases: locating code files, team building, and kick-off meeting.

Locating Code Files.Firstly, the Repository Custodian employs the BM25 algorithm  to rank the files in the repository based on the GitHub issue description. Subsequently, the top \(k\) files are selected as potential candidates for further coding. However, as described in SS2, this simple retrieval method can introduce irrelevant files, increasing the cost and reducing the effectiveness of subsequent coding process. Therefore, we filter these files based on relevance to minimize their number. While it is feasible to directly assess the relevance between each file and the issue by LLMs, queries to the LLM may contain the same code snippets as previous ones, leading to unnecessary computational costs. Considering that applying the code change often modifies a specific part of the file rather than the entire file, we propose a memory mechanism to reuse the previously queried information.

```
1:Input: repository: \(_{i}\) including files \(\{f_{i}\}\), GitHub issue: \(q_{x}\), LLM: \(\)
2:Config: filter top width: \(k\), prompts: \(\), find the latest previous version of the file and its summary: \(find\)
3:Output: candidate files: \(^{k}_{i}\), repository evolution memory: \(\)
4:\(_{i}\) BM25(\(_{i}\), \(q_{x}\))
5:\(^{k}_{i}_{i}\):[\(:\)\(_{i}\)]
6:for\(f_{i}^{k}_{i}\)do
7:\(f_{h_{i}},s_{h}(f_{i},)\)
8:if\( f\) and len(\(s_{h}\)) \(<\) len(\(f_{i}\)) then
9:if\(h\) is i then
10:\(s_{i} s_{h}\)
11:else
12:\( d\) diff(\(f_{h}\), \(f_{i}\))
13:\(m( d,_{1})\)
14:\(s_{i} s_{h} m\)
15:endif
16:else
17:\(s_{i}(f_{i},_{2})\)
18:endif
19:\(\).update(\(\{f_{i}:s_{i}\}\))
20:if\(((s_{i},q_{x}),_{3})\) is false then
21:\(^{k}_{i}^{k}_{i}\) - \(f_{i}\)
22:endif
23:endfor ```

**Algorithm 1** Locating.

Algorithm 1 outlines the process of locating files with our designed memory \(\). If a file \(f_{i}\) is compared for the first time with an issue \(q_{x}\), the LLM \(\) with prompt \(_{2}\) compresses it into the summary \(s_{i}\), where \(i\) denotes the file's version. This summary is shorter than the code content in the file and it is stored in memory for future reuse. If the file \(f_{i}\) has been previously compared, the latest previous version (\(h\)) of the file \(f_{h}\) can be found by the script \(find\). Since \(f_{i}\) can be represented as the combination of \(f_{h}\) and the difference between them (\( d\) that be obtained via the "git diff" command), LLMs can understand \(f_{i}\) by using \(f_{h}\) and \( d\). If the difference is small and the file \(f_{i}\) is long, it is valuable to reuse the previous summary \(s_{h}\) stored in memory rather than the content of \(f_{i}\). Specifically, if the length of \(s_{h}\) is less than that of \(f_{i}\), \(\) with prompt \(_{1}\) can summarize the code changes \( d\) as a "commit message" \(m\). The combination of \(s_{h}\) and \(m\) forms the description of the newer version \(f_{i}\), enabling the LLM \(\) with prompt \(_{3}\) to determine whether it is relevant to the issue in fewer context length. Based on their relevance, the custodian agent filters irrelevant files, allowing the Manager agent to define tasks with remaining relevant files.

Team Building.In this process, the Manager agent has the flexibility to "recruit" team members as the issue needs. Firstly, upon receiving the located files, the Manager begins with analyzing the GitHub issue for the repository and breaks them into detailed file-level tasks. Specifically, for each code file \(f_{i}\) in the candidate set \(_{i}^{k}\), the Manager leverages the LLM \(\) with the prompt \(_{4}\) and the issue description \(q_{x}\) to define the corresponding file-level task \(t_{i}\). One issue can be converted to multiple tasks. These tasks, along with the associated code file, are stored in a task set \(_{i}^{k}\). Once a task is clarified, the Manager defines the personality role \(r_{i}\) of the Developer by invoking LLM \(\) with the prompt \(_{5}\) and the task \(t_{i}\).

By iterating through these candidate code files, the Manager agent ultimately designs a collection of Developer agent role descriptions \(_{i}^{k}\), thus forming the development team. The details of the team building are shown in Algorithm 2. This approach simplifies the task for LLMs because each team member only needs to handle a sub-task rather than resolving the entire complex issue.

Kick-off Meeting.After building the team, the Manager organizes a kick-off meeting. This meeting serves two purposes: 1 To confirm whether the tasks assigned by the Manager are reasonable and ensure that all Developers in the team can collaboratively resolve the issue \(q_{x}\), 2 To determine which Developers' tasks can be executed concurrently and which tasks have dependencies need to be sorted. The meeting takes the form of a circular speech: the Manager is responsible for opening the speech, guiding the discussion and summarizing the results, and the Developers provide their opinions based on previous discussions in turn. One example of the meeting can be found in Appendix B. After the meeting, Developers adjust their role descriptions \(_{i}^{k}\) based on the discussion \(recording\), and the Manager, leveraging the LLM \(\) and the prompt \(_{7}\), generates a main work plan \(c_{main}\). This plan is presented as code, and embedded into the program for execution. The meeting makes collaboration among Developers more efficient and avoids potential conflicts.

```
1:Input: candidate files: \(_{i}^{k}\), issue: \(q_{x}\), LLM: \(\)
2:Config: prompts: \(\)
3:Output: tasks: \(_{i}^{k}\), Developer agents' role description: \(_{i}^{k}\), plan: \(c_{main}\)
4:for\(f_{i}_{i}^{k}\)do
5:\(t_{i}((f_{i},q_{x}),_{4})\)
6:\(_{i}^{k}_{i}^{k}(f_{i},t_{i})\)
7:\(r_{i}(_{i},q_{x}),_{5})\)
8:\(_{i}^{k}_{i}^{k} r_{i}\)
9:endfor
10:recording = kick_off_meeting(\(_{i}^{k}\))
11:\(_{i}^{k}(_{i}^{k}\), recording), \(_{6})\)
12:\(c_{main}(,\,_{7})\) ```

**Algorithm 2** Making the plan.

#### 3.2.2 Coding

Based on the empirical study on line locating and the complexity (SS2), we transform the code change generation into the multi-step coding process that is designed to leverage the strengths of LLMs in code generation while mitigating their weaknesses in code change generation. Two types of agents participate in the coding process: Developers and QA Engineers. As outlined in Algorithm 3, for each task \(t_{i}\) and its associated code file \(f_{i}\) in \(_{i}^{k}\), the Developer agent generates the role description of the QA Engineer \(a_{i}\) by the LLM \(\) with the prompt \(_{8}\). Subsequently, Developers collaborate with their QA Engineers to execute the coding tasks. During each execution of the Developer, the range of lines of code that need to be modified is firstly determined as a set of intervals \(\{[s^{}_{i},e^{}_{i}]\}\) where \(s^{}_{i}\) represents the starting line number in the \(i\)-th hunk, and \(e^{}_{i}\) is the ending line number. The determination is generated by analyzing the task content \(t_{i}\) and file content \(f_{i}\) using \(\) with the prompt \(_{9}\). These intervals split the original code file \(f_{i}\) into parts to be modified (_old_part_) and parts to be retained. Developers then generate new code snippets, _new_part_, by \(\) with the prompt \(_{10}\). The code snippets replace _old_part_, resulting in a new version of the code file \(f^{}_{i}\). Utilizing Git tools, the code change \( d_{i}\) for this file \(f_{i}\) is generated. With the code change \( d_{i}\), QA Engineer produce _review_comment_ and _review_decision_, by the LLM \(\) with the prompt \(_{11}\). If the decision, _review_decision_, is negative (i.e., \(false\)), the feedback, _review_comment_, prompts Developers to revise the code in the next attempt. This iterative process continues until the code change meets the quality standards (i.e., _review_decision_ is \(true\)) or reaches a predefined maximum number of iterations. After the iteration, the final version of the code change, \( d\), is fixed, which is the ultimate modification result on each file. All generated final-version code changes during this process are merged into the repository-level code change \(\) as the issue solution.

## 4 Experiments and Analysis

### Setup

In the experiments, we employ the SWE-bench dataset as the evaluation benchmark because it is the latest dataset specifically designed for evaluating the performance of the GitHub issue resolution. SWE-bench comprises \(2,294\) issues extracted from \(12\) popular Python repositories, representing real software evolution requirements. Given the observation that experimental outcomes on the \(25\%\) subset of SWE-bench align with those obtained from the entire dataset , we opt for the same \(25\%\) subset previously utilized in experiments for GPT-4 according to their materials . Moreover, the experimental scores for the five LLMs, have been made available by them .

Our framework is flexible to integrate various LLMs. To compare with the scores reported by SWE-bench, GPT-4 is selected as the base LLM. Another reason for the selection is that GPT-4 shows remarkable performance on code generation and understanding as demonstrated on benchmarks such as MBPP  and HumanEval . Claude-2 is not chosen due to the unavailability of API access.

Following SWE-bench , the applied and resolved ratio is used to evaluate the performance under the setting with the files requiring modification provided. The applied ratio indicates the proportion of instances where the code change is successfully generated and can be applied to the code repository by Git. The resolved ratio refers to the proportion of instances where the code change is successfully applied and passes a series of tests. Additional elaboration is provided in Appendix C.

### How Effective is Our Framework? (RQ 2)

The comparative performance analysis between our framework and other LLMs on the same dataset is presented in Tab. 2. The results indicate that our framework significantly outperforms other LLMs. Notably, with a resolved ratio of \(13.94\%\), our framework's effectiveness is eight-fold that of the base LLM, GPT-4. This substantial increase underscores our framework's capability to harness the potential of LLMs more effectively. Furthermore, when contrasted with the pre

  
**Method** & **\% Applied** & **\% Resolved** \\  GPT-3.5 & \(11.67\) & \(0.84\) \\ Claude-2 & \(49.36\) & \(4.88\) \\ GPT-4 & \(13.24\) & \(1.74\) \\ SWE-Llama 7b & \(51.56\) & \(2.12\) \\ SWE-Llama 13b & \(49.13\) & \(4.36\) \\ 
**MAGIS** & **97.39** & **13.94** \\ MAGIS (w/o QA) & \(92.71\) & \(10.63\) \\ MAGIS (w/o hints) & \(94.25\) & \(10.28\) \\ MAGIS (w/o hints, w/o QA) & \(91.99\) & \(8.71\) \\   

Table 2: The comparison of overall performance between MAGIS and baselines on SWE-bench.

vious state-of-the-art LLM, Claude-2, our framework's resolved ratio exceeds that benchmark by more than two-fold. This superior performance unequivocally establishes the advance of our method.

The ablation study is designed to simulate two scenarios: 1 Without QA (w/o QA): Considering the QA Engineer agent as optional within our framework, we directly evaluate the code changes generated by the Developer agent, bypassing the QA process. This scenario aims to investigate the effectiveness and necessity of QA Engineer review. 2 Without hints (w/o hints): Hints refer to the textual content found in the comments section of pull requests, which are typically created before the first commit of the pull request. This setting means our framework operates without any clarifications except for the issue, despite such information being available on GitHub before the issue resolution process begins. This analysis aims to explore if the participation of humans could potentially improve the success rate of issue resolution.

Our framework shows a significant improvement in issue resolution, even without QA or hints. It achieves a resolved ratio of \(8.71\%\), which is five times higher than that of the base LLM. This increase underscores the contribution of other agents in MAGIS to its overall performance. Furthermore, integrating cooperation with QA or hints separately can further elevate the resolved ratio by \(1.92\%\) or \(1.57\%\), respectively. These findings underscore the value of QA Engineers and the participation of humans, as demonstrated by the resolved rates achieved through their integration.

For instance, to resolve the issue  from the repository Django, the developer modifies four hunks in two files , as shown in Fig. 15. Despite the availability of two provided files, our method opts for modifications in only one file, as illustrated in Figure 16. Remarkably, this simpler code change enables the repository to pass all requisite test cases.

Additional comparison can be found in Appendix D and E, and detailed case study is shown in Appendix H. Furthermore, the statistics on the generated code changes can be found in Appendix F.

### How Effective is Our Planning Process? (RQ 3)

To investigate the effectiveness of the planning process, we analyze the Repository Custodian and Manager agent. The performance of the Repository Custodian agent is observed in the recall score versus the file number curve, as shown in Fig. 4. This curve demonstrates that our method consistently outperforms the BM25 baseline across varying numbers of selected files, indicating that our approach can identify the maximum number of relevant code files with the minimum selection.

For the Manager agent, we examined the alignment of its generated task descriptions with the reference code change by LLM. Following the study , we select GPT-4 as an evaluator to score the correlation between the reference code change and the generated task description. The correlation scores are determined based on a set of criteria defined in Tab. 6. A higher correlation score indicates a better alignment and thus, a more accurate and effective planning direction. The distribution of these correlation scores is presented in Fig. 4. Notably, most of the scores are \(3\) or above, implying that the majority of task descriptions are in the right direction concerning planning. Furthermore, the higher scores correlate with a higher probability of issue resolution, indicated by a larger proportion of "resolved" outcomes in scores \(4\) and \(5\). This signifies that when the generated task description closely aligns with the reference, there is a higher possibility of resolving the issue. The analysis above demonstrates the effectiveness of both the Repository Custodian and the Manager agent in the planning process of our framework.

### How Effective is Our Coding Process? (RQ 4)

To evaluate the effectiveness of the coding process in our framework, we analyze the performance of Developers in locating code lines and resolving issues of different complexity.

Fig. 5 illustrates the distribution of the line locating coverage ratio of MAGIS and the baselines. This visualization reveals that our Developer agent frequently attains a line locating coverage ratio nearing \(1\). Compared with baselines, the Developer agent demonstrates a pronounced preference for higher distribution values close to \(1\), and conversely, a reduced preference for lower distribution values near \(0\). Such a distribution validates the superior performance of MAGIS in locating code lines.

Further analysis is provided in Fig. 6 illustrating the relationship between the line locating coverage ratio and the issue resolved ratio within those coverages. As shown in Fig. 6, the right four bars are higher than the five left, which indicates that the resolved ratio can increase with the line locating coverage. This observation also suggests that locating lines accurately is important for issue resolution. The cumulative frequency curve, shown in orange, provides an additional analysis, indicating the cumulative proportion of issues resolved ratio up to each point along the line locating coverage. A steady increase in cumulative frequency accompanies the increase in line locating coverage, reinforcing the idea that resolving issues is more successful in areas of high coverage. The slope of the curve's left half is lower than that of the right half, indicating that the benefits of increasing the coverage ratio are less pronounced at lower coverage ratios than at higher ones. Therefore, the Developer agent should prioritize improving its capability of locating code lines.

Moreover, as shown in Tab. 3, we present a logistic regression analysis that quantifies the correlation between several complexity indices and issue resolution. The results show that GPT-4 has significant negative correlations across the number of files and functions, suggesting that as these indices increase, the likelihood of issue resolution decreases. Conversely, the negative correlations are less pronounced with our model, MAGIS, particularly in the number of files and functions, suggesting mitigation of challenges corresponding to these complexity indices.

To evaluate the performance of the QA Engineer, the ablation experiment is conducted and the results are shown in Tab. 2. As the table shows, in settings with and without hints, the presence of the QA Engineer can increase the resolved ratio by \(1.57\%\) and \(3.31\%\), respectively. This overall enhancement

   Method & \# Files & \# Functions & \# Hunks & \# Added LoC & \# Deleted LoC & \# Changed LoC \\  GPT-4 & \(-25.15^{*}\) & \(-25.15^{*}\) & \(-0.06\) & \(-0.10\) & \(-0.04\) & \(-0.21\) \\ MAGIS & \(-1.55^{*}\) & \(-1.55^{*}\) & \(-0.12^{*}\) & \(-0.04^{*}\) & \(-0.06^{*}\) & \(-0.57^{*}\) \\   

* The correlation between the index and the issue resolution is significant (P-value \(<0.05\)).

Table 3: Correlation between the complexity indices and the issue resolution.

substantiates the QA Engineer's contribution to improving outcomes. Furthermore, a case detailed in Appendix I underscores the QA Engineer's effectiveness.

## 5 Related Work

Researchers have developed LLM-based multi-agent systems, enabling more complex task completion. For instance, MetaGPT [23; 24] simulates a programming team's Standardized Operating Procedures (SOPs) and achieves leading scores on benchmarks like HumanEval  and MBPP . Similarly, ChatDev  functions as a virtual development company, decomposing requirements into atomic tasks and utilizing mutual communication and self-reflection to mitigate LLM hallucinations. While these systems excel in transforming requirements into code, they often overlook the challenges of code change generation during software evolution . GitHub issues include different types of requirements and most of them belong to bug fixing. Previous researchers have proposed methods to localize the bugs [65; 42] and some researchers explored various methods to automatic program repair[57; 7; 55; 3; 59; 53]. The full version of related work can be found in Appendix J.

## 6 Conclusion

This paper illuminates the potential of LLMs in software development, particularly in resolving GitHub issues. Our empirical study identifies the challenges of direct LLM application. To address the challenges, we propose a novel LLM-based multi-agent framework, MAGIS, enhancing issue resolution through well-designed agents' collaboration. The superiority of MAGIS on the SWE-bench against popular LLMs highlights its effectiveness, pointing towards a promising direction for integrating LLMs into software evolution workflows.