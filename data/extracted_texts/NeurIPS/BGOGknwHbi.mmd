# Self-Guiding Exploration for

Combinatorial Problems

 Zangir Iklassov

MBZUAI

zangir.iklassov@mbzuai.ac.ae

&Yali Du

King's College London

yali.du@kcl.ac.uk

&Farkhad Akimov

MBZUAI

farkhad.akimov@mbzuai.ac.ae

&Martin Takac

MBZUAI

martin.takac@mbzuai.ac.ae

###### Abstract

Large Language Models (LLMs) have become pivotal in addressing reasoning tasks across diverse domains, including arithmetic, commonsense, and symbolic reasoning. They utilize prompting techniques such as Exploration-of-Thought, Decomposition, and Refinement to effectively navigate and solve intricate tasks. Despite these advancements, the application of LLMs to Combinatorial Problems (CPs), known for their NP-hardness and critical roles in logistics and resource management remains underexplored. To address this gap, we introduce a novel prompting strategy: Self-Guiding Exploration (SGE), designed to enhance the performance of solving CPs. SGE operates autonomously, generating multiple thought trajectories for each CP task. It then breaks these trajectories down into actionable subtasks, executes them sequentially, and refines the results to ensure optimal outcomes. We present our research as the first to apply LLMs to a broad range of CPs and demonstrate that SGE outperforms existing prompting strategies by over 27.84% in CP optimization performance. Additionally, SGE achieves a 2.46% higher accuracy over the best existing results in other reasoning tasks (arithmetic, commonsense, and symbolic).

## 1 Introduction

Large Language Models (LLMs) have emerged as powerful tools capable of executing reasoning tasks across various domains, including arithmetic, commonsense, and symbolic reasoning . These models may leverage prompting techniques such as Exploration-of-Thought , Decomposition , and Refinement  to break down and solve various tasks in a step-by-step manner. Recent research has been directed towards extending these techniques to tackle more sophisticated optimization challenges . Combinatorial problems (CPs) may represent a category of these complex optimization tasks, associated with intricate computational challenges.

Combinatorial Problems are characterized by their NP-hardness and inherent complexity, which result in an exponential growth in the number of potential solutions. This complexity presents substantial challenges in the research . CPs are especially crucial in sectors that require efficient logistics, planning, and scheduling. Currently, the dominant approach in these industries involves metaheuristic methods. These methods combine various simple but fast heuristics to effectively tackle CPs within specific constraints. Nonetheless, the effectiveness of these heuristics can vary significantly depending on the CP task and its associated constraints, necessitating a customized selection of heuristics to achieve optimal performance.

In the meantime, research on exploring LLMs to solve CPs reveals substantial gaps. While recent advancements indicate the effectiveness of LLMs in various reasoning tasks [38; 47; 32; 49], their application to CPs has been minimal. The literature indicates that existing generative models can address smaller instances of the Traveling Salesman Problem (TSP) [16; 23; 41]. However, as problem sizes increase, existing prompting strategies begin to yield inadequate responses, underscoring the need for more sophisticated prompting methods. Moreover, there is a notable scarcity of research addressing other complex CPs, particularly the Vehicle Routing and Job Scheduling Problems, which pose significant challenges in logistics, planning industries, and operations research.

In this work, we introduce a novel prompting strategy: self-guiding exploration (SGE), designed to enhance the problem-solving process for CPs. This algorithm works as a combination of exploration-of-thought, decomposition, and refinement prompting methods. The SGE approach autonomously generates multiple thought trajectories for a given CP, each trajectory representing a specific heuristic to tackle the given task. Each trajectory is then decomposed into subtasks, which are executed one by one, and their outputs are refined and combined into a final solution. Unlike the task-specific prompts utilized in other methods, SGE employs general-purpose prompts, allowing for the adaptive use of specific heuristic solutions tailored to various CPs, such as the Hungarian heuristic for the assignment problem and the Nearest Neighbor heuristic for the vehicle routing problem. Essentially, SGE acts as a versatile metaheuristic capable of identifying, combining, and refining task-specific heuristics for individual CP tasks.

Our work makes following contributions.Firstly, we present a novel investigation into the application of large language models for solving combinatorial problems. Secondly, we introduce a new prompting strategy, SGE, that autonomously generates thought trajectories, splits them into subtasks and refines the answers. Thirdly, we demonstrate that SGE outperforms existing prompting strategies such as Chain-of-Thought, Decomposition, and Self-Refinement, improving CP optimization performance by \(27.84\%\). Lastly, we validate the applicability of SGE across other reasoning tasks, including arithmetic, commonsense, and symbolic tasks, where our method achieves a \(2.46\%\) higher accuracy than the best existing results.

## 2 Related work

CP via classical approach.In classical research, combinatorial problems are predominantly tackled using heuristic and metaheuristic methods specifically crafted for particular tasks. Notable examples include the Ant-Colony Optimization and Tabu Search methods for addressing the Vehicle Routing Problem [28; 8; 15], the Shortest Processing Time and Most Work Remaining Heuristics for Job Scheduling Problems , and the Hungarian Algorithm for the Assignment Problem . These approaches are favored in industrial settings due to their simplicity and speed. However, they need to be individually tailored to each task and its constraints' setting. In contrast, exact solvers such as Google-OR-Tools  offer general and precise solutions, but their applicability is often limited to smaller-scale problems due to the inherent NP-hardness of combinatorial problems.

CP via learning-based approach.In AI literature, Reinforcement Learning (RL) has been a prominent approach for tackling combinatorial problems since the 1990s [21; 20; 45; 7; 2]. The integration of deep learning, particularly through innovations like Pointer Networks, has significantly enhanced RL's capability to handle more complex combinatorial tasks [35; 3]. Further advancements involve the use of Transformer networks [6; 34; 14], with notable applications in solving the Vehicle Routing Problem [25; 11]. Despite these advances, RL-based methods often still do not exceed the performance of traditional heuristics, especially when scalability and accurate state representation are required [36; 22; 44; 31].

CP with LLMs.Recent studies have leveraged large language models (LLMs), such as GPT-3.5 and GPT-4, to tackle combinatorial problems like the Traveling Salesman Problem using iterative prompting, where solutions are refined incrementally [23; 41]. Other works employ LLMs to autonomously generate executable code as novel heuristics for problems like the knapsack and traveling salesman [29; 17; 43; 18]. This promising approach enhances task-specific heuristics, potentially improving performance on specialized combinatorial tasks. In contrast, our focus is on leveraging LLMs to directly solve combinatorial problems in a generalizable manner, enabling a versatile approach applicable across a wide range of complex tasks.

Prompting strategies.The expressive capabilities of direct prompting in Large Language Models are theoretically limited to the complexity class \(^{}\). To effectively address combinatorial problems with LLMs, sophisticated prompting strategies are required. One basic approach is the Chain-of-Thought (CoT) prompting, introduced in , which encourages LLMs to articulate intermediate "thoughts" that inform the generation of the final output. This technique has given rise to advanced variations, including Self-consistency with CoT (CoT-SC), Tree-of-Thoughts (ToT), and Graph-of-Thought methods [37; 42; 46]. Additionally, decomposition prompting strategies can be employed [48; 12], since they simplify complex tasks into smaller, manageable subtasks via symbolic programs or structured algorithms, thus improving the performance of LLMs. In our experiments, we found these techniques to be insufficient, leading us to propose the Self-Guiding Exploration method as a more effective solution for tackling combinatorial problem tasks.

## 3 Preliminaries

We provide an overview of combinatorial problems, highlighting their inherent complexity with the classic example of the Traveling Salesman Problem (TSP) and an example of a combinatorial problem formulation in a prompt for use by a Large Language Model (LLM).

Combinatorial problems.Combinatorial problems involve decision-making processes where the goal is to assign binary decision variables \(x\) in order to optimize a cost function \(g(x_{1},...,x_{n})\), subject to task-specific constraints. A classic example of such a problem is the TSP. In the TSP, given a list of \(n\) cities and the distances \(d_{ij}\) between cities \(i,j\), the objective is to determine the shortest possible route that visits each city exactly once and returns to the starting city. \(x_{ij}\) is used as the action variable, indicating whether the route progresses from city \(i\) to city \(j\). The cost function to minimize in TSP is \(g(x)=_{i=1}^{n}_{j=1}^{n}d_{ij}x_{ij}\), under the condition that all cities visited exactly once \(_{i=1}^{n}x_{ij}=1\) and \(_{j=1}^{n}x_{ij}=1\) for all \(i,j\). Combinatorial problems are generally categorized as NP-hard due to their inherent computational complexity. For instance, a TSP with \(n\) cities presents \((n-1)!\) possible routes, rendering the evaluation of all potential solutions impractical and exceedingly time-consuming as \(n\) increases.

Prompting combinatorial problems in LLMs.To use LLM for solving CP tasks, we define \(f\) as the interface function of a generative LLM model, which accepts high-dimensional discrete input tokens and generates outputs within the same token space (\(f:W W\)). For each combinatorial problem task, the input \(Q\) to the LLM can be explicitly defined in a textual format. This description delineates the specific goal alongside a list of variables tailored to the task at hand. For instance, the objective of the Traveling Salesman Problem (TSP) can be textually articulated as "Find a route that minimizes the total travel distance, visits each city exactly once, and starts and ends in the same city." Subsequently, the variables, such as the distances between cities \(d_{ij}\), are provided in a format such as "The distance between city \(i\) and city \(j\) is [number]", laying out all necessary parameters for the model to process and generate solutions. The model will then process this structured input \(Q\) to produce the corresponding solution answer \(A\), where both \(Q\) and \(A\) are within token space \(W\); formally, \(A=f(Q)\). Given the inherent complexity of combinatorial problems, direct zero-shot prompting \(f(Q)\) is insufficient. Consequently, we propose a self-guiding exploration algorithm that employs metaheuristic-like strategies to effectively solve CP tasks.

## 4 Method

In this section, we introduce Self-Guiding Exploration (SGE) method and provide a detailed explanation of its algorithm designed to tackle combinatorial problems. The method (Fig 1), inspired by metaheuristic approaches, synthesizes multiple heuristic methods. It generates various thought trajectories, with each trajectory representing a specific heuristic approach. These trajectories are then integrated to form the final solution. To overcome the challenges of executing complex heuristics through LLMs in one step, our algorithm utilizes a decomposition strategy. This approach breaks down each trajectory into smaller, more manageable subtasks, enabling the solution to progress through sequential, simpler steps. This general-purpose algorithm is tailored to adapt to a wide range of combinatorial problems without the constraints of task-specific exemplars for few-shot solution generation.

### Algorithm

The proposed method's algorithm is segmented into five distinct phases, as outlined in Algorithm 1. These phases include exploring thought trajectories, decomposing each trajectory into subtasks, resolving each subtask to generate thoughts, obtaining feedback and refining the thoughts, and finally integrating all thoughts to formulate the answer. Each thought here represents one completed subtask.

Exploration.During the exploration stage, the model tackles the overarching problem \(Q\) by engaging with exploration meta-prompt. This prompt \(Z_{explore}\) is structured as: "List all possible methods to solve this problem. Return them separated by new lines." This prompt stimulates the model to enumerate potential methodologies pertinent to \(Q\). The sequence is then divided into task-specific trajectories of queries \(Q^{n}\) each incorporating a method to address \(Q\):

\[Q^{}=f(Q,Z_{explore}).\]

Decomposition.Following exploration, each trajectory query \(Q^{n}\) is processed through the model to break down the trajectory into actionable steps. The decomposition meta-prompt \(Z_{decomp}\) is formulated as: "List all steps to use the method. Return them separated by new lines." This leads to

Figure 1: **Self-Guiding Exploration**. The generative model autonomously addresses a combinatorial problem task \(Q\) through a five-phase process: (1) Exploration of \(N\) solution trajectories, where each trajectory offers potential solutions; (2) Decomposition of these trajectories into \(K\) subtasks, outlining specific steps for each method; (3) Resolution of each subtask, executing the outlined steps; (4) Feedback and Refinement, where feedback is gathered and used to refine each subtask; (5) Integration of all trajectories into a consolidated final solution \(A\). Distinct from traditional exploration/decomposition techniques, SGE(Q) functions entirely autonomously, eliminating the reliance on task-specific queries or manually created thought exemplars. This independence makes it universally applicable to all CP tasks without necessitating modifications.

the generation of subtask queries that operationalize the trajectory method:

\[Q^{n}_{}=f(Q,Q^{n},Z_{decomp}).\]

Subtask resolution.Post-decomposition, the subtask queries \(Q^{n}_{}\) are each split into \(K\) individual queries and processed by the model to generate thoughts. The model initially evaluates if the task is easily solvable using the meta-prompt \(Z_{check}\): "Is this problem easily solvable? Return yes or no": \(f(Q^{n}_{k},Z_{check})\). If the response is affirmative, the model executes the subtask query \(Q^{n}_{k}\) to generate a new thought:

\[T^{n}_{k}=f(Q,T^{n}_{k-1},Q^{n}_{k}).\]

Otherwise, the model engages a recursive instance of the self-guiding exploration algorithm on \(Q^{n}_{k}\) instead of the main task \(Q\) to navigate and decompose the complex subtask, producing:

\[T^{n}_{k}=f(Q,T^{n}_{k},Q^{n}_{k_{feedback}}).\]

Feedback and refinement.In this stage, the model utilizes an additional meta-prompt \(Z_{feedback}\) - "Give feedback to the proposed solution" - to generate feedback queries \(Q^{n}_{k_{feedback}}=f(Q,Q^{n}_{k},T^{n}_{k},Z_{feedback})\). This guides the model in refining the initial responses through reevaluation and enhancement of the thoughts:

\[T^{n}_{k}=f(Q,T^{n}_{k},Q^{n}_{k_{feedback}}).\]

Integration.Upon completion of all trajectories and their associated subtasks, the model employs a final meta-prompt \(Z_{integrate}\) - "Integrate all previous findings and provide the final answer" - to amalgamate the last thoughts into a definitive solution answer:

\[A=f(Q,T^{1}_{K},...T^{N}_{K},Z_{integrate}).\]

SGE draws inspiration from metaheuristic methods used to solve combinatorial problem tasks. Yet, due to its general-purpose nature and meta-prompts, it is also suitable for other tasks, beyond CPs. Essentially, it integrates elements of exploration-of-thought, decomposition, and refinement prompting strategies, but it does so without relying on task-specific prompts or solution exemplars. For additional information on these prompting strategies, see Section A.1

## 5 Experiments

This section details the experimental setup and presents the results of our proposed method applied to combinatorial problem tasks, as well as its performance on other reasoning tasks commonly explored in LLM research.

### Setup

CP tasks.The experiments were conducted on six combinatorial tasks: Assignment Problem, Knapsack Problem, Bin Packing Problem, Traveling Salesman Problem, Vehicle Routing Problem, and Job Scheduling Problem. The Assignment Problem, classified as P-complete, can be optimally solved using the Hungarian Algorithm. In contrast, the other tasks are NP-hard and ordered by increasing complexity. For a more detailed discussion of these CP tasks, refer to Section A.2. We included five distinct problem sizes, involving 5, 10, 15, 20, and 30 elements (nodes) such as cities in the TSP/VRP. To facilitate these experiments, a dataset was created, comprising 100 randomly generated instances for each problem size. These instances were characterized by uniformly distributed variables, such as the positioning of cities in TSP/VRP or bin volume in the Bin Packing Problem, over an interval from 0 to 100. The experiments utilized an NVIDIA A100 SXM 40GB GPU, paired with two AMD EPYC 7742 CPUs (8 cores each) and 256GB RAM. Our implementation is available online.1Baselines.We utilized four baseline prompting methods: Input-Output (IO) Direct Prompting, Chain-of-Thought Prompting, Self-Refine (Refine) Prompting, and Decomposition Prompting. The Input-Output (IO) approach involves a single prompt where the model is asked to provide a solution directly, without complex prompting. In this approach, we generate \(N\) sample candidates by repeatedly prompting the model with the same query \(Q\), \(N\) times. The responses are then aggregated through majority voting to identify the most common solution among the \(N\) outputs. We employ the Self-Refine (Refine) method , which includes a feedback-refinement procedure that aligns closely with phase four of SGE. Additionally, we use the zero-shot Chain-of-Thought method , which is the basic technique among Exploration-of-Thought methods. Lastly, we implemented the Decomposition method as described in . In our experiments, these baseline methods were tested across a range of five LLM models including GPT-4, GPT-3.5 by OpenAI, Gemini-1.5 by Google, and the Llama-2 series from Meta, which includes models with 70 billion and 7 billion parameters. We did not include prompting methods previously used in [16; 23; 41], as their prompting strategies showed inferior results compared to the zero-shot Chain-of-Thought approach when tested with our data.

Metrics.In our study, each method's performance is evaluated relative to IO (Input-Output) prompting. To quantify the improvement, we first measure the solution cost \(g_{io}\) for each combinatorial problem task using IO prompting (e.g., for the TSP, \(g_{io}=_{i=1}^{n}_{j=1}^{n}d_{ij}x_{ij}\)). We then calculate the cost \(g_{method}\) using alternative methods. The percentage improvement is computed as \(100 g_{io}-g_{method}\). For problems of smaller sizes, we are able to obtain optimal solutions using the Google-OR-Tools solver through a brute force approach. In such instances, we measure the cost of the optimal solution \(g_{opt}\) and determine the optimality gap as \(100-g_{opt}}{g_{opt}}\).

### Results on CP tasks

To evaluate the general performance of SGE on combinatorial problems, we conducted experiments comparing performance improvement to IO of SGE and CoT, Decomposition, and Refinement baselines using GPT-4 and Gemini-1.5 LLM models. Table 1 gives the results on six combinatorial problem tasks. The results analysis shows that the SGE method consistently outperforms CoT, Refine, and Decomposition methods across all tasks. Notably, the magnitude of improvement escalates with the increasing complexity of the problems, from polynomial to exponential. The margin with the second-best method, Decomposition, ranges from \(7.53\%\) for the Assignment Problem to \(37.13\%\) for the JSP. This trend suggests that the IO method may struggle with the computational demands of NP-hard problems, where more sophisticated strategies like SGE provide significant advantages. For a comprehensive view of all experimental results, see Section A.4.

To qualitatively evaluate the performance of the Exploration, Decomposition, and Refinement phases of SGE method, we assessed the LLM outputs for each phase across five random instances of each combinatorial problem task. Figure 2 illustrates an example of how our method addresses the TSP, showcasing outputs during each of the three phases of SGE. In the Exploration phase, the first box of Figure 2 displays how LLM \(f\) generates a list of potential algorithms suitable for solving

    &  &  \\  Task & CoT & Refine & Decomp & Ours & CoT & Refine & Decomp & Ours \\  Assignment & 11.46 & 14.47 & 33.80 & **41.33** & 11.66 & 13.98 & 31.94 & **40.46** \\ Knapsack & 15.37 & 17.16 & 51.95 & **70.39** & 13.85 & 16.85 & 48.62 & **65.87** \\ Bin Packing & 14.06 & 17.12 & 39.57 & **74.72** & 11.89 & 15.43 & 35.74 & **67.63** \\ Travelling Salesman & 13.64 & 15.75 & 38.49 & **72.10** & 14.34 & 15.90 & 36.36 & **68.09** \\ Vehicle Routing & 14.27 & 16.94 & 36.73 & **71.92** & 11.88 & 15.13 & 33.59 & **68.02** \\ Job Scheduling & 13.84 & 16.37 & 38.20 & **75.33** & 13.41 & 15.75 & 36.36 & **67.89** \\   

Table 1: **Percentage performance improvement compared to IO** on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).

the TSP, such as heuristic approaches like Nearest Neighbor, metaheuristic techniques like Ant Colony, and Mixed Integer Linear Programming (MILP) method. This phase adapts to different combinatorial problems by suggesting tailored algorithms, like the Hungarian algorithm for the Assignment problem, Greedy algorithms for the Knapsack problem, and Clustering methods for the Vehicle Routing Problem (VRP). Each list of algorithms forms the foundation for generating diverse candidate solutions tailored to each specific problem. The Decomposition phase, depicted in the second box of Figure 2, breaks down each identified algorithm into specific subtasks. This example shows the decomposition of the Nearest Neighbor algorithm for the TSP, where the initial subtasks are simple enough for direct processing by model \(f\). However, more complex tasks, such as loops, undergo further decomposition using SGE in a recursive manner, with computational or programming tasks being handled using Python within models like GPT-4 and Gemini-1.5 equipped with a Code Interpreter. Finally, the Refinement phase, illustrated in the third box of Figure 2, focuses on enhancing the candidate solutions developed in the previous stage. This example pertains to refining a solution derived from the Nearest Neighbor algorithm for the TSP by implementing the 2-opt algorithm. Renowned for its effectiveness in TSP and VRP contexts, the 2-opt algorithm optimizes the initial solution to find locally optimal solutions within a specific neighborhood, thus improving the overall quality of the candidate solutions. This example shows that SGE method adapts its approach to suit different combinatorial problems, finding a special set of heuristics for each task.

Effect of problem size on SGE performance.To evaluate the effect of problem size on SGE performance, we conducted experiments on all six tasks with input sizes of 5, 8, 12, 15, and 20 nodes using the GPT-4 model. Figure 3 gives the results of these experiments. The results analysis shows that generally, an increase in problem complexity, as determined by the size of the problem input, negatively influences performance improvement; larger problem sizes result in diminished performance improvement of the SGE method compared to IO. Specifically, tasks with 20 input nodes consistently exhibit lower performance improvements relative to the IO method than tasks with 5 input nodes. However, when comparing less disparate sizes, such as 8 and 12 nodes, the differential impact on performance is less pronounced and can occasionally be positive.

Figure 2: **Example of SGE inference across the Exploration, Decomposition, and Refinement phases for the Traveling Salesman Problem. The figure displays three boxes, each illustrating the prompt structure and corresponding example output for each phase.**

## 5 Conclusion

**Size** & **Method** & **Assignment** & **Knapsack** & **Bin Packing** & **TSP** & **VRP** & **JSP** \\   & IO & 45.45 & 90.10 & 108.2 & 100.3 & 102.0 & 105.3 \\  & CoT & 39.33 & 66.88 & 78.24 & 81.15 & 78.17 & 79.41 \\  & Refine & 36.42 & 61.98 & 77.40 & 71.62 & 72.49 & 71.72 \\  & Decomp & 14.66 & 21.56 & 40.00 & 43.62 & 40.65 & 44.15 \\  & Ours & **2.500** & **8.050** & **9.060** & **8.27** & **11.92** & **9.300** \\   & IO & 46.84 & 103.5 & 112.8 & 116.9 & 116.3 & 108.2 \\  & CoT & 39.70 & 73.84 & 85.08 & 89.01 & 89.48 & 85.21 \\  & Refine & 37.32 & 72.62 & 86.25 & 85.59 & 83.31 & 78.43 \\  & Decomp & 18.49 & 26.43 & 52.73 & 53.48 & 54.43 & 49.81 \\  & Ours & **8.290** & **14.88** & **20.95** & **15.19** & **19.65** & **21.26** \\   & IO & 49.11 & 101.5 & 120.7 & 121.6 & 118.5 & 117.6 \\  & CoT & 41.70 & 79.33 & 93.84 & 86.84 & 90.05 & 89.29 \\  & Refine & 40.35 & 77.09 & 82.23 & 88.57 & 88.40 & 87.02 \\   & Decomp & 21.12 & 35.82 & 55.40 & 57.51 & 59.19 & 56.01 \\   & Ours & **11.26** & **16.82** & **22.38** & **16.12** & **24.00** & **22.86** \\  

Table 2: **Optimality gap** of prompting methods using LLaMA-2-70B. The results are represented as performance percentage difference compared to optimal solutions (the smaller it is, the better).

Figure 4: **Effect of Model Choice on Performance Improvement** relative to the IO solution.

Figure 3: **Effect of Problem Size on Performance Improvement** relative to the IO solution using gpt-4 w/ code interpreter. The analysis spans problem instances of varying sizes, systematically presented from the smallest to the largest, specifically ranging from \(n=5\) to \(n=20\) nodes. Results are organized to highlight the impact of increasing problem complexity on the effectiveness of the solution.

[MISSING_PAGE_FAIL:9]

### Results on reasoning tasks

To evaluate the versatility of SGE in handling different types of tasks, we conducted experiments across eight datasets commonly referenced in LLM research, categorized into three distinct task types: arithmetic, commonsense reasoning, and symbolic reasoning. Table 3 gives the results of these experiments, with each dataset comprising train and test splits where SGE and baseline methods were applied to the test splits. The results analysis shows that the SGE method demonstrates incremental but consistently superior performance across all task categories. Notably, the method shows particular strength in arithmetic tasks, where it achieves an average improvement of 4.83%, compared to 1.24% in commonsense reasoning tasks and 1.32% in symbolic reasoning tasks. This demonstrates the method's applicability and effectiveness across a diverse range of tasks, extending beyond combinatorial problems.

## 6 Conclusion

This study has explored the application of Large Language Models to combinatorial problems, a category of tasks known for their NP-hardness. Our research introduced a 'Self-Guiding Exploration' prompting strategy that effectively utilizes the inherent strengths of LLMs. By generating multiple thought trajectories tailored to various CPs and autonomously decomposing them into manageable subtasks. Our findings confirm that SGE outperforms existing strategies, improving optimization performance by 27.84% and achieving a 2.46% higher accuracy in reasoning tasks. Notably, SGE shows a 34.85% smaller gap with the global optimum on complex tasks like the Job Sheduling Problem compared to baseline methods. These results underline the potential of advanced LLM strategies in complex problem-solving scenarios, suggesting that the right techniques can enhance the utility of LLMs in critical logistics and resource management applications.

Despite the performance improvements demonstrated by the SGE, several limitations have emerged that merit attention. Firstly, SGE performance depends on the choice of language model. Secondly, the operational costs associated with SGE are notably higher; it requires 87.89% more function calls than the Decomposition method. These issues present clear avenues for future research. Enhancing SGE's computational efficiency while maintaining its high performance could broaden its applicability and make it a more practical choice for a wider range of problems.