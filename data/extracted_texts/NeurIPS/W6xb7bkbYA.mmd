# UDC-SIT: A Real-World Dataset

for Under-Display Cameras

 Kyusu Ahn\({}^{1,3}\) Byeonghyun Ko\({}^{2}\) HyunGyu Lee\({}^{2}\) Chanwoo Park\({}^{2}\) Jaejin Lee\({}^{1,2}\)

\({}^{1}\)Dept. of Data Science, Seoul National University, Seoul, Republic of Korea

\({}^{2}\)Dept. of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea

\({}^{3}\)Research Center, Samsung Display Co., Ltd., Yongin, Republic of Korea

kyusu.ahn@snu.ac.kr, {byeonghyun, hyungyu}@aces.snu.ac.kr, {99chanwoo, jaejin}@snu.ac.kr

###### Abstract

Under Display Camera (UDC) is a novel imaging system that mounts a digital camera lens beneath a display panel with the panel covering the camera. However, the display panel causes severe degradation to captured images, such as low transmittance, blur, noise, and flare. The restoration of UDC-degraded images is challenging because of the unique luminance and diverse patterns of flares. Existing UDC dataset studies focus on unrealistic or synthetic UDC degradation rather than real-world UDC images. In this paper, we propose a real-world UDC dataset called UDC-SIT. To obtain the non-degraded and UDC-degraded images for the same scene, we propose an image-capturing system and an image alignment technique that exploits discrete Fourier transform (DFT) to align a pair of captured images. UDC-SIT also includes comprehensive annotations missing from other UDC datasets, such as light source, day/night, indoor/outdoor, and flare components (e.g., shimmers, streaks, and glares). We compare UDC-SIT with four existing representative UDC datasets and present the problems with existing UDC datasets. To show UDC-SIT's effectiveness, we compare UDC-SIT and a representative synthetic UDC dataset using five representative learnable image restoration models. The result indicates that the models trained with the synthetic UDC dataset are impractical because the synthetic UDC dataset does not reflect the actual characteristics of UDC-degraded images. UDC-SIT can enable further exploration in the UDC image restoration area and provide better insights into the problem. UDC-SIT is available at: https://github.com/mcrl/UDC-SIT.

## 1 Introduction

Under Display Camera (UDC) is a technology designed to place a camera module under the display to use the UDC area as a display space and take pictures when the camera operates. Since a larger screen-to-body ratio is a common consumer demand that leads to a trend toward bezel-less display products , such as smartphones, laptops, tablets, and TVs, the display products equipped with UDC will well meet such a market trend. Moreover, video conferencing uses UDC products that enable natural eye-tracking by arranging a camera in the center of the display. However, UDC has some drawbacks of image deterioration problems, such as low transmittance, blur, noise, and flare.

Since the pixels in the micrometer scale diffract the light traveling through the camera lens , degrading image quality, UDC displays have a lower pixel density above the camera lens to reduce transmittance loss and diffraction, as shown in Figure 1. However, a lower display resolution in the UDC area affects natural video viewing. Thus, improving the quality of UDC images is a critical problem. Especially UDC image reconstruction models to enable higher pixel densities around the camera are essential to overcome the problem for more comfortable viewing.

Several studies address the UDC image restoration problem. However, most of them  have limitations because their datasets do not completely reflect the properties of actual UDC images (i.e., are synthesized). A challenging problem in constructing the UDC dataset is finding a matching pair of the ground truth and distorted UDC images, which requires significant time and effort.

Feng _et al._ proposes a pseudo-real-world UDC dataset. However, their dataset contains images with occlusions. Ignatov _et al._ try to match two images for the same scene captured by different devices with different positions and angles. They use the SIFT key points  of paired images and the RANSAC algorithm  to align the two images in a pair. However, the geometric alignment algorithm fails to adequately perform when there is a large discrepancy in the scenes of the two images, such as UDC degradation.

This paper proposes a new UDC dataset called UDC-SIT (UDC's Still Images by the Thunder Research Group). As far as we know, it is the first real-world UDC dataset to overcome the problems of the existing UDC datasets.

We devise an image-capturing system that minimizes lens movement when capturing matching image pairs. We cut and take the UDC area of the flexible display panel of a smartphone with a UDC (e.g., Samsung Galaxy Z-Fold 3 ). We use the piece of the display panel as a lid on the camera lens of another smartphone with a non-UDC (e.g., Samsung Galaxy Note 10 ). The lid on the non-UDC can be opened and closed. As a result, our image-capturing system has minor geometric inconsistency compared to Feng _et al._. In addition, it helps to restrict rotations and tilts by the movement of the lid to a degree that does not notably influence alignment. Figure 2 illustrates our image-capturing system. We get a matching pair by capturing the ground-truth image with the lid off and the distorted image with the lid on.

We compensate for the pixel-position difference between the two images for the same scene by exploiting the discrete Fourier transform (DFT) . The discrepancy in pixel positions is inevitable because of the slight movement of the camera when opening and closing the lid.

The contributions of this paper are summarized as follows:

* We propose an image-capturing system for obtaining matched pairs of undistorted and UDC-distorted images for the same scene.
* We propose an image aligning technique for the paired images of the same scene by exploiting the discrete Fourier transform.

Figure 1: Comparison of conventional hole-display and under-display cameras. The UDC area has a lower pixel density since the pixel pattern functions as diffraction slits. (a) Under Display Camera (UDC). (b) Hole display camera. (c) Pixel structure of the UDC area. (d) Comparison between the UDC area and other display area.

Figure 2: The image capturing system to capture a pair of matching images (the ground-truth image and the degraded image by the UDC). (a) The lid opens to get a ground-truth image. (b) The lid closes to get the corresponding degraded image.

* We provide a real-world UDC dataset, UDC-SIT, which accurately reflects actual image degradations by the UDC. UDC-SIT offers a rich set of annotations that stimulate research in UDC image restoration. We make UDC-SIT publicly available (https://github.com/mcrl/UDC-SIT).
* To show UDC-SIT's effectiveness, we compare UDC-SIT and a representative synthetic UDC dataset using five representative learnable image restoration models.

## 2 Related work

Image restoration.Image restoration techniques restore a high-quality image from a degraded one. They include denoising, deblurring, and flare removal tasks. Camera-captured images often contain noisy and blurred pixels due to focus errors and incorrect light sensitivity. Restoring these images by removing noise and blur is a common task. Uformer  is one of the leading hierarchical transformer-based models for restoring such images. SIDD  comprises 30,000 noisy images captured by five smartphone cameras in 10 different scenes under various lighting conditions. DND  offers 50 pairs of noisy and noiseless images of very high resolution.

Light flares degrade image quality significantly due to strong light intensities. Wu _et al._ generate synthetic flare removal datasets and train a neural network by modeling the optical characteristics of flares. Also, Dai _et al._[11; 12] introduce a nighttime flare removal dataset to address the limitations of existing methods that only work well on daytime flares.

The UDC image restoration is complex, and its degradation patterns differ from other restoration tasks of images captured by standard cameras.

Existing UDC datasets.Zhou _et al._ tackle the UDC image restoration problem using paired images from a Monitor Camera Imaging System (MCIS) and synthesized PSFs using optical modeling. However, their dataset has limitations, including unrealistic flares captured from a monitor with a limited dynamic range and inaccurate PSFs. They provide only 300 pairs of images for T-OLED and P-OLED, respectively.

Feng _et al._ improve the UDC dataset. To measure the PSF, they place a white point light source one meter away from the OLED display of ZTE Axon 20  and following the methodology presented by Sun _et al._. By convolving the PSF with HDR images from the HDRI Haven dataset , a synthetic dataset is generated. However, it lacks real-world characteristics because the images and PSFs come from different devices. Also, the flare shapes are limited, not including the distorted flares addressed by Yoo _et al._.

Yoo _et al._ introduce a synthetic UDC dataset that includes spatially varying PSFs obtained by optical simulation using the Brown-Conrady Distortion model . However, their simulated flare distortions differ from real-world distortions, and their dataset is not publicly available.

Feng _et al._ create a pseudo-real dataset by capturing degraded images with ZTE Axon 20 UDC  and ground-truth images with iPhone 13 Pro camera . They face domain discrepancy and geometric misalignment challenges. Geometric misalignment is severe due to using different camera modules for the two paired images. AlignFormer  overcomes the geometric misalignment in UDC by aligning domain information with StyleConv  and AdaIN , and geometric information with an attention mechanism and a pre-trained optical flow estimator called RAFT .

DNN models for UDC image restoration.The UDC image restoration challenges [14; 53] use the datasets by Feng _et al._ and Zhou _et al._. In these challenges, most of the top-ranked teams [31; 44; 49; 54] use the U-Net model  and residual networks  as their backbone Deep Neural Network (DNN) model for restoration.

## 3 Obtaining aligned images

This section describes our alignment technique of the standard and UDC images for the same scene. Misalignment between the two images is not due to the UDC itself but rather a problem that arises when capturing them. Previous techniques, such as SIFT  and RANSAC , are inadequate for this purpose because of severe degradation by the UDC. Feng _et al._ use two cameras with different specifications, leading to variations in perspectives and contents between paired images. Some tools, such as AlignFormer , align these images but introduce occlusion regions.

In contrast, our approach minimizes misalignment without introducing occlusion regions. After capturing the two images with our image-capturing system, we exploit discrete Fourier transform (DFT) to align the two paired images. We especially exploit their spatial frequency domain after the DFT to achieve degradation-resilient alignment [9; 17; 27].

### Discrete Fourier transform

DFT converts a discrete signal represented by complex exponential waves into constituent frequencies. Equation 1 defines 2D DFT used to obtain the frequency representation of an image.

\[(u,v)=_{x=0}^{M-1}_{y=0}^{N-1}f(x,y) e^{-i2(}{M}+)},\] (1)

where \((u,v)\) denotes the frequency value, \((u,v)\) represents a point in the frequency domain, \(M N\) is the image size, and \(f(x,y)\) is the pixel value at a point \((x,y)\) in the image (i.e., in the spatial domain). Using Euler's formula decomposes the exponential function in Equation 1 into cosine and sine functions, and Equation 1 becomes

\[(u,v)=(u,v)+i(u,v),\] (2)

where \((u,v)\) and \((u,v)\) denote the real and the imaginary part of \((u,v)\), respectively. The _amplitude_\(|(u,v)|\) and _phase_\((u,v)\) of \((u,v)\) are defined as:

\[|(u,v)|=[^{2}(u,v)+^{2}(u,v)]^{ }(u,v)=^{-1}[(u,v )}{(u,v)}].\] (3)

### Alignment of paired images

Let \(M N\) be the image size. To measure the difference between the degraded image (D) and ground-truth image (G), we typically use Mean Squared Error (MSE),

\[MSE=_{x=0}^{M-1}_{y=0}^{N-1}(D(x,y)-G(x,y))^{2}.\] (4)

However, \(MSE\) primarily emphasizes local information at points \((x,y)\). In contrast, the spectrum's value for a specific point \((u,v)\) in the frequency domain relies on the collective contribution of all points \((x,y)\) in the spatial domain because \((u,v)\) is the sum that iterates through each pixel \((x,y)\) of the image in Equation 1. Thus, to align the two images, we assess the distance between paired images in both spatial and frequency domains as shown in Figure 3. To incorporate both local and global information, we employ a loss function that combines information in the spatial and frequency domains:

\[Loss=_{1}_{x=0}^{M-1}_{y=0}^{N-1}(D(x,y)-G(x,y))^{2}+_{ 2}_{u=0}^{M-1}_{v=0}^{N-1}_{amp}(u,v)+_{3} _{u=0}^{M-1}_{v=0}^{N-1}(u,v),\] (5)

where \(_{amp}(u,v)\) is the L1 distance for the amplitude defined as \(_{amp}(u,v)=|_{D}(u,v)-_{G}(u,v)|\), and \((u,v)\) is the L1 distance for the phase defined as \((u,v)=|_{D}(u,v)-_{G}(u,v)|\)).

\((u,v)\) at a point \((u,v)\) represents a distinct spatial frequency component, and applying inverse DFT to \((u,v)\) at a point \((u,v)\) generates a sinusoidal grating in the spatial domain. Figure 4(a) and Figure 4(c) illustrate the isolation of sinusoidal gratings associated with \(_{G}(u,v)\) and \(_{D}(u,v)\), respectively.

The significant advantage of using DFT in aligning the original and shifted images lies in its capability to decompose an image into its constituent spatial frequency components. Figure 4(a) and Figure 4(c) display the initial three low-frequency sinusoidal gratings from the two images, clearly revealing a distinct spatial shift. Notably, while the amplitude of sinusoidal gratings remains constant within the same column of Figure 4(a) and Figure 4(c), their corresponding phases exhibit variations. Figure 4(b)and Figure 4(d) visually depict the differences in amplitude (Figure 4(b)) and phase (Figure 4(d)) between the two images. Reducing \(\) is crucial for effectively aligning a misaligned pair of images for the same scene. Figure 4 is a conceptual illustration for paired images involving shifts without degradation. However, due to the degradation, UDC-degraded images also exhibit differences from the ground truth in amplitude. Figure 4 elucidates the importance of the phase component in the alignment of the images.

Our alignment algorithm minimizes the loss in Equation 5 between the ground-truth image and the degraded image. The misaligned degraded image is rotated, shifted and then cropped to achieve the same size as the ground truth until the loss reaches the minimum. Handling rotation and shifts are manageable, but addressing tilt presents a challenge. It is because tilt requires perspective transforms optimized for objects within a single image sharing the same plane. Thus, our current emphasis is on tackling shifts and rotations, with tilt considerations excluded. Nonetheless, our data collection is meticulous, limiting rotations and tilts to a degree that does not notably influence the alignment, as the PCK values in Table 2 affirm.

The original size of camera-captured images is (2016, 1512, 4). The ground truth image is center-cropped to (1792, 1280, 4). The degraded image is similarly cropped around the center. For the degraded image, iterative shifting of (x, y) coordinates and rotation are used to find the minimum loss point where the cropped degraded image aligns with the cropped ground truth image. The final cropped image size becomes (1792, 1280, 4) to ensure H and W are multiples of 256. The detailed algorithm is illustrated in Algorithm 1. Here, we establish \(s=20\), \(=0.3\), \(r=0.1\), and (\(_{1}\), \(_{2}\), \(_{3}\)) combinations in Table 3 as hyperparameters.

Figure 4: Frequency analysis of two paired images. One is the original image (G), and the other is the spatially shifted image (D) of G. Let \(_{X}(u,v)\) be the result of applying \(DFT\) to an image \(X\). (a) G in the spatial domain comprises multiple sinusoidal gratings. Each sinusoidal grating results from applying inverse \(DFT\) to \(_{G}(u,v)\). (b) The amplitude difference between G and D. There is no difference. (c) Similar to G, D in the spatial domain comprises multiple sinusoidal gratings. (d) The phase difference between G and D.

Figure 3: Overview of the proposed loss function to align two paired images. The loss function considers the difference between degraded (D) and ground-truth (G) images in both the spatial and frequency domains.

```
0: Images \(I_{G}\), \(I_{D}\) of size \((H,W)\), hyperparameters \(s\), \(_{r}\), \(r\), \(_{1}\), \(_{2}\), \(_{3}\) ```
0: Aligned images \(C_{G}\), \(C_{D}\) of size \((H^{*},W^{*})\)  Crop \(C_{G}\) from \(I_{G}\) using center crop Crop \(C_{D}\) from \(I_{D}\) to the size of \(C_{G}\)  Initialize best loss \(L_{}\) to a large value Initialize optimal shifts \(s_{}\), \(s_{}\), and rotation \(_{}\) to 0 for\(_{}\) from \(-_{r}\) to \(_{r}\) with step \(r\)do  Apply rotation of \(_{}\) to \(I_{D}\) to get \(I_{D_{}}\) for\(x_{}\) from \(-s\) to \(s\) with step 1 do for\(y_{}\) from \(-s\) to \(s\) with step 1 do  Calculate crop position \((p,q)\) relative to the center crop: \(p=x_{}+x_{}\) \(q=y_{}+y_{}\)  Crop image \(C_{D_{}}\) from \(I_{D_{}}\) at position \((p,q)\)  Calculate loss \(L\) using the loss function in Eq. 5 between \(C_{D_{}}\) and \(C_{G}\) if\(L<L_{}\) then  Update \(L_{}\) to \(L\)  Update \(s_{}\) to \(x_{}\)  Update \(s_{}\) to \(y_{}\)  Update \(_{}\) to \(_{}\) endif endfor endfor  Apply optimal rotation \(_{}\) to \(I_{D}\) to get \(I_{D_{}}\)  Calculate crop position \((p_{},q_{})\) relative to the center crop: \(p_{}=x_{}+s_{}\) \(q_{}=y_{}+s_{}\)  Crop \(I_{D_{}}\) to acquire an aligned image \(C_{D}\) at position \((p_{},q_{})\) ```

**Algorithm 1** Alignment of paired images \(I_{G}\) and \(I_{D}\)

We capture images in UDC-SIT without any Image Signal Processing (ISP) and in RAW format. While High Dynamic Range (HDR) captures more details in shadows and highlights, authentic UDC images are typically in Low Dynamic Range (LDR). Generating HDR images requires capturing multiple LDR images with different exposures, then combining them to create an HDR image . This process differs from general photography, so we gather our dataset in LDR.

## 4 Comparison with the existing UDC datasets

In this section, we compare UDC-SIT with the four existing representative UDC image datasets. We summarize the five datasets _Zhou-S_, _Feng-S_, _Yoo-S_, _Feng-R_, and _UDC-SIT_ in Table 1, where S and R stand for synthetic and real datasets, respectively. Feng _et al._ capture 330 images and then crop them into 6,747 small patches. We explain the detail in the appendix.

Noises and transmittance decrease.Under low-light conditions, the camera sensor amplifies both the desired signal and unintended random noise. Since the camera sensor in the UDC is positioned beneath the display pixels that decrease the transmittance, the camera operates in low-light conditions, resulting in noise amplification. The degraded images in the UDC dataset should effectively contain the unique UDC noise, which differs from standard cameras. For example, a

   Dataset & Scene & Dynamic range & Dataset size & Annotations & Publicly available \\  Zhou _et al._ & Synthetic & LDR & 300 & No & Yes \\ Feng _et al._ & Synthetic & HDR & 2,376 & No & Yes \\ Yoo _et al._ & Synthetic & LDR & - & No & No \\ Feng _et al._ & **Real** & HDR & 6,747 & No & Yes \\ UDC-SIT & **Real** & LDR & **2,340** & **Yes** & Yes \\   

Table 1: Comparison of the UDC datasets. Unlike others, UDC-SIT provides annotations, such as light source, day/night, indoor/outdoor, and flare types. Flare types are classified as shimmer, streak, and glare.

visual comparison is given in Figure 5. The image in Zhou-S in Figure 5(a) is captured from a monitor under controlled lighting conditions. Thus, there exists no noise. However, we observe the excessive transmittance decrease caused by P-OLED. In addition, the image in Feng-S in Figure 5(c) is generated by convolving the PSF on an HDR image from the HDRI Haven dataset . The HDR images in the dataset are meticulously created through exposure bracketing  and cleanup to ensure high quality without overexposure, chromatic aberration, and noise. Thus, the Feng-S image has reduced noise and minimal transmittance loss. On the other hand, the images in Feng-R in Figure 5(b) and in UDC-SIT in Figure 5(d) accurately depict the visible noise caused by the UDC. They also show the effect of transmittance decrease.

Figure 5: Comparison of the transmittance decrease and UDC noises. GT stands for ground truth. (a) Zhou-S . (b) Feng-R . (c) Feng-S . (d) UDC-SIT.

Figure 6: Comparison of flares. The light sources affect the flare shape. The images in (a) Zhou-S , (b) Feng-R , (c) Feng-S , and (d) UDC-SIT show the UDC flares by the round shape of light sources. The images in (e) Zhou-S , (f) Feng-R , (g) Feng-S , and (h) UDC-SIT show the UDC flares by fluorescent light sources.

**Flares.** Lens flares typically occur when intense light scatters or reflects within an optical system . In contrast, UDC flares result from light interacting with the display panel above the camera sensor, resulting in undesired reflections, diffraction, or scattering. Thus, it is essential for the images in the UDC dataset to exactly describe real UDC flares if they exist. For example, a visual comparison is given in Figure 6. The images in Zhou-S in Figure 6(a) and (e) do not exhibit any flares since they capture images displayed on a monitor. Similarly, the images in Feng-S in Figure 6(c) and (g) generate unrealistic flares by convolving PSFs to HDR images. Specifically, it lacks the generation of glare, streaks, and spatially variant flares. The shapes of real-world UDC flares vary depending on their positions, resulting in different diffraction patterns. However, the flares in the image in Feng-S in Figure 6(c) appear overly regular. (Figure 6(c) versus (b) and (d)). Moreover, the image in Feng-S in Figure 6(g) demonstrates different fluorescent light flares from the real-world UDC flares in Figure 6(f) and (h). Figure 6(b) versus (d) and (f) versus (h) depict various flare shapes from different UDC smartphones.

**Occlusion regions.** Although Feng-R consists of real images, it has some limitations. They avoid capturing close objects to avoid parallax and occlusion. Nonetheless, AlignFormer  used in Feng-R creates occlusion regions marked red in Figure 7. This area is typically excluded in training the DNN restoration models. This is why they call Feng-R as pseudo-real pairs.

**Alignment quality.** Finally, to measure the alignment quality of paired images, we compare the Percentage of Correct Keypoints (PCK) using LoFTR  as a keypoint matcher by following the methodology presented by Feng _et al._. A keypoint pair is considered correctly aligned when \(d< max(H,W)\), where \(d\) is the position difference between a pair of matched keypoints, \(\) is the

   Dataset & Need alignment & PCK (\(=0.01\)) & PCK (\(=0.03\)) & PCK (\(=0.10\)) \\  Zhou-S  & & 98.11 & 98.45 & 99.08 \\ Feng-S  & & 99.95 & 99.96 & 99.99 \\ Feng-R  & **58.75** & **95.08** & **99.93** \\ UDC-SIT & **97.26** & **98.56** & **99.35** \\   

Table 2: The PCK comparison between the datasets. UDC-SIT shows the best alignment and has PCK values close to 100% for all values of \(\).

   (\(_{1}\), \(_{2}\), \(_{3}\), \(_{}\)) & PCK (\(=0.01\)) & PCK (\(=0.03\)) & PCK (\(=0.10\)) \\  ( \(1\), \(0\), \(0\), \(0\) ) & 78.77 & 81.09 & 85.65 \\ ( \(0\), \(1\), \(0\), \(0\) ) & 34.05 & 50.70 & 64.66 \\ ( \(0\), \(0\), \(1\), \(0\) ) & 62.27 & 64.59 & 72.54 \\ ( \(1\), \(1\), \(0\), \(0\) ) & 35.55 & 49.41 & 63.25 \\ ( \(1\), \(0\), \(1\), \(0\) ) & **98.22** & **98.73** & **99.31** \\ ( \(1\), \(1\), \(1\), \(0\) ) & **86.13** & **95.02** & **99.23** \\ ( \(1\), \(0\), \(0\), \(0\), \(3\) ) & 72.40 & 77.40 & 83.30 \\ ( \(1\), \(0\), \(0\), \(0\), \(3\) ) & 43.83 & 47.78 & 59.60 \\   

Table 3: Analyzing UDC-SIT’s PCK relative to \(_{1}\), \(_{2}\), \(_{3}\), and \(_{}\). When no rotation is applied, \(_{}=0\); otherwise, \(_{}=_{r}\). Notably, \(_{2}=1\) significantly improves the alignment over using MSE alone (i.e., the case of (1, 0, 0, 0)).

Figure 7: Invalid mask visualization in Feng-R . (a) The degraded image captured by the ZTE Axon 20  UDC. (b) The ground-truth image captured by the iPhone 13 Pro rear camera . (c) Aligned ground-truth image by AlignFormer .

threshold, and \(H\) and \(W\) represent the height and width of the image. To apply a consistent alignment criterion across the datasets with different resolutions, we uniformly set \((H,W)=1024\), aligning with Feng _et al._. Then the PCK for an image pair is defined by the ratio of the number of correctly aligned keypoint pairs to the total number of keypoint pairs. Table 2 illustrates the comparison result. Since Zhou-S and Feng-S consist of synthetic images, they do not require an additional alignment process, resulting in PCK values close to 100%. On the other hand, Feng-R undergoes alignment using AlignFormer on the images in the paired dataset. It achieves 58.75% for \(=0.01\). Unlike Feng-R, UDC-SIT consistently exhibits PCK values close to 100% across all values of \(\).

Table 3 shows the analysis result of PCK values on average for various \(_{1}\), \(_{2}\), \(_{3}\), and \(_{}\) combinations. We found that the optimal combination of \(_{1}\), \(_{2}\), \(_{3}\), and \(_{}\) is different for each image pair. The images in UDC-SIT are selected by human inspection, and the PCK values of UDC-SIT in Table 2 originate from the human assessment. The human inspection result is comparable to results obtained by the best combination (1, 0, 1, 0) in Table 3.

Applying rotation (i.e., the case of (1, 0, 0, 0.3)) from Algorithm 1 significantly reduces PCK as shown in Table 3. While this rotation causes a slight MSE decrease, its impact on digitized images leads to notable PCK decline because of changes in ordinary translational correlation during the rotation .

To mitigate rotation and tilt effects, we use an image-capturing system in Figure 2, applying only shifts in (x, y) coordinates. Employing the DFT loss without MSE (the cases of (0, 1, 0, 0) and (0, 0, 1, 0)) yields lower PCK values of 34.05% and 62.27%, respectively (\(=0.01\)). Using only MSE and \(_{amp}(u,v)\) without \((u,v)\) (case (1, 1, 0, 0)) results in a reduced PCK of 35.55% (\(=0.01\)). The loss functions encompassing spatial and frequency domains, focusing on \((u,v)\) of \((u,v)\) (the cases of(1, 0, 1, 0) and (1, 1, 1, 0)), show high PCK values of 98.22% and 86.13%, respectively (\(=0.01\)). The case of (1, 0.1, 0.1, 0) lacks sufficient \(_{2}\) and \(_{3}\) to reflect the frequency domain loss effectively.

## 5 Effects on learnable restoration models

We demonstrate the effectiveness and benefits of UDC-SIT by comparing the UDC image restoration performance with Feng-S. We only compare UDC-SIT to Feng-S because Yoo-S is not publicly available, Feng-R is planned for a future release but is currently unavailable, and Zhou-S needs more flares that are essential for UDC image restoration. We use five learning-based image restoration models, including DISCNet , UDC-UNet , Uformer , ECFNet , SRGAN  using UDC-SIT. We modify the authors' code of the models to account for the difference in dynamic range and channel size between UDC-SIT and Feng-S . To ensure reproducibility, we explain the experimental setting in the appendix and our GitHub.

The restoration performance of the five models on Feng-S and UDC-SIT is given in Table 4. Models trained with Feng-S successfully restore the images with up to 2.0 times higher PSNR values than Input (except SRGAN). They also achieve a nearly perfect SSIM score. However, none of the models trained with UDC-SIT can restore the images to the same extent as Feng-S. Feng-S inadequately represents actual UDC degradation, such as intense flares. The high degree of glares, shimmers, and streaks results in substantial information loss. Consequently, model performance degradation occurs when those obscured objects become unrecognizable. Thus, restoration becomes more challenging with UDC-SIT than with conventional synthetic datasets.

Effects of annotations.It is noteworthy that annotations are specifically available in the UDC-SIT. Flare scenes heavily rely on the capturing environment. For example, artificial lights emit a different

    & & Input & DISCNet  & UDC-UNet  & Uformer-T  & ECFNet  & SRGAN  \\  Feng-S  & PSNR & 26.08 & 43.27 & 49.37 & 42.47 & 52.17 & 32.35 \\  & SSIM & 0.8561 & 0.9877 & 0.9933 & 0.9844 & 0.9958 & 0.9538 \\  UDC-SIT & PSNR & 21.03 & 26.32 & 27.44 & 27.28 & 28.22 & 24.70 \\  & SSIM & 0.7330 & 0.8457 & 0.8637 & 0.8594 & 0.9002 & 0.8195 \\   

Table 4: Restoration performance for synthetic and real UDC datasets. The term _Input_ refers to the PSNR and SSIM values between the paired degraded and ground-truth images.

spectrum than natural sunlight, resulting in a distinct diffraction pattern. Weak light sources can cause streaks under low-light conditions. Thus, we attach annotations, such as light source, day/night, indoor/outdoor, and flare components, to the images in UDC-SIT. UDC-SIT follows the classification labels of flare components by Dai et al.  including _shimmer_, _streak_, _glare_, and _light source_ as illustrated in Figure 8. Detailed explanations and instructions for the annotations, and experimental result of their effect on the restoration models can be found in the appendix.

## 6 Limitations

The degradation of images by the UDC depends on the display pixel patterns, which vary between products, as shown in Figure 6. When obtaining UDC-SIT, Galaxy Z Fold 3  display panel is used. Thus UDC-SIT is optimal for restoring Galaxy Z Fold 3  images. Models trained with UDC-SIT may not be suitable for restoring UDC images taken by other devices (e.g., ZTE Axon 20  and Galaxy Z Fold 4 ).

## 7 Conclusions

As far as we know, UDC-SIT is the first dataset to include real-world UDC degradation, such as low transmittance, blur, noise, and flare, along with detailed annotations for the light source, day/night, indoor/outdoor, and flare components. With UDC-SIT, one can train a UDC image restoration model to improve the quality of UDC images taken by the UDC. We propose an effective image-capturing system for paired UDC-distorted and ground-truth images. We also propose a technique for aligning the paired images by exploiting the discrete Fourier transform. The experimental result of comparing UDC-SIT and a representative synthetic UDC dataset with five representative learnable restoration models indicates that the models trained with the synthetic UDC dataset are impractical because the synthetic UDC dataset does not reflect the actual characteristics of UDC-degraded images. This implies UDC-SIT is an adequate dataset for UDC image restoration.