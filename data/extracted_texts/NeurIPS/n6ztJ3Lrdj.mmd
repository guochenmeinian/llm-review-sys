# Learning with Explanation Constraints

Rattana Pukdee

Carnegie Mellon University

rpukdee@cs.cmu.edu

&Dylan Sam

Carnegie Mellon University

dylansam@andrew.cmu.edu

&J. Zico Kolter

Carnegie Mellon University

Bosch Center for AI

zkolter@cs.cmu.edu

&Maria-Florina Balcan

Carnegie Mellon University

ninamf@cs.cmu.edu

&Pradeep Ravikumar

Carnegie Mellon University

pkr@cs.cmu.edu

Equal contribution

###### Abstract

As larger deep learning models are hard to interpret, there has been a recent focus on generating explanations of these black-box models. In contrast, we may have apriori explanations of how models should behave. In this paper, we formalize this notion as learning from _explanation constraints_ and provide a learning theoretic framework to analyze how such explanations can improve the learning of our models. One may naturally ask, "When would these explanations be helpful?" Our first key contribution addresses this question via a class of models that satisfies these explanation constraints in expectation over new data. We provide a characterization of the benefits of these models (in terms of the reduction of their Rademacher complexities) for a canonical class of explanations given by gradient information in the settings of both linear models and two layer neural networks. In addition, we provide an algorithmic solution for our framework, via a variational approximation that achieves better performance and satisfies these constraints more frequently, when compared to simpler augmented Lagrangian methods to incorporate these explanations. We demonstrate the benefits of our approach over a large array of synthetic and real-world experiments.

## 1 Introduction

There has been a considerable recent focus on generating explanations of complex black-box models so that humans may better understand their decisions. These can take the form of feature importance , counterfactuals , influential training samples , etc. But what if humans were able to provide explanations for how these models should behave? We are interested in the question of how to learn models given such apriori explanations. A recent line of work incorporates explanations as a regularizer, penalizing models that do not exhibit apriori given explanations . For example, Rieger et al.  penalize the feature importance of spurious patches on a skin-cancer classification task. These methods lead to models that inherently satisfy "desirable" properties and, thus, are more trustworthy. In addition, some of these empirical results suggest that constraining models via explanations also leads to higher accuracy and robustness to changing test environments. However, there is no theoretical analysis to explain this phenomenon.

We note that such explanations can arise from domain experts and domain knowledge, but also other large "teacher" models that might have been developed for related tasks. An attractive facet of the latter is that we can automatically generate model-based explanations given unlabeled data points. For instance, we can use segmentation models to select the background pixels of images solely onunlabeled data, which we can use in our model training. We thus view incorporating explanation constraints from such teacher models as a form of knowledge distillation into our student models .

In this paper, we provide an analytical framework for learning from explanations to reason when and how explanations can improve the model performance. We first provide a mathematical framework for model constraints given explanations. Casting explanations as functionals \(g\) that take in a model \(h\) and input \(x\) (as is standard in explainable AI), we can represent domain knowledge of how models should behave as constraints on the values of such explanations. We can leverage these to then solve a constrained ERM problem where we additionally constrain the model to satisfy these explanation constraints. Since the explanations and constraints are provided on randomly sampled inputs, these constraints are random. Nevertheless, via standard statistical learning theoretic arguments , any model that satisfies the set of explanation constraints on the finite sample can be shown to satisfy the constraints in expectation up to some slack with high probability. In our work, we term a model that satisfies explanations constraints in expectation, an _CE model_ (see Definition 1). Then, we can capture the benefit of learning with explanation constraints by analyzing the generalization capabilities of this class of CE models (Theorem 3.2). This analysis builds off of a learning theoretic framework for semi-supervised learning of Balcan and Blum . We remark that if the explanation constraints are arbitrary, it is not possible to reason if a model satisfies the constraints in expectation based on a finite sample. We provide a detailed discussion on when this argument is possible in Appendix B,D. In addition, we note that our work also has a connection with classical approaches in stochastic programming  and is worth investigating this relationship further.

Another key contribution of our work is concretely analyzing this framework for a canonical class of explanation constraints given by gradient information for linear models (Theorem 4.1) and two layer neural networks (Theorem 4.2). We focus on gradient constraints as we can represent many different notions of explanations, such as feature importance and ignoring spurious features. These corollaries clearly illustrate that restricting the hypothesis class via explanation constraints can lead to fewer required labeled data. Our results also provide a quantitative measure of the benefits of the explanation constraints in terms of the number of labeled data. We also discuss when learning these explanation constraints makes sense or is possible (i.e., with a finite generalization bound). We note that our framework allows for the explanations to be noisy, and not fully satisfied by even the Bayes optimal classifier. Why then would incorporating explanation constraints help? As our analysis shows, this is by reducing the estimation error (variance) by constraining the hypothesis class, at the expense of approximation error (bias). We defer the question of how to explicitly denoise noisy explanations to future work.

Now that we have provided a learning theoretic framework for these explanation constraints, we next consider the algorithmic question: how do we solve for these explanation-constrained models? In general, these constraints are not necessarily well-behaved and are difficult to optimize. One can use augmented Lagrangian approaches , or simply regularized versions of our constrained problems  (which however do not in general solve the constrained problems for non-convex parameterizations but is more computationally tractable). We draw from seminal work in posterior regularization , which has also been studied in the capacity of model distillation , to provide a variational objective. Our objective is composed of two terms; supervised empirical risk and the discrepancy between the current model and the class of CE models. The optimal solution of our objective is also the optimal solution of the constrained problem which is consistent with our theoretical analysis. Our objective naturally incorporates unlabeled data and provides a simple way to

Figure 1: A restricted hypothesis class \(_{,}\) (left). Our algorithmic solution to solve a proposed variational objective in Section 5 (right).

control the trade-off between explanation constraints and the supervised loss (Section 5). We propose a tractable algorithm that iteratively trains a model on the supervised data, and then approximately projects this learnt model onto the class of CE models. Finally, we provide an extensive array of experiments that capture the benefits of learning from explanation constraints. These experiments also demonstrate that the variational approach improves over simpler augmented Lagrangian approaches and can lead to models that indeed satisfy explanations more frequently.

## 2 Related Work

**Explainable AI.** Recent advances in deep learning have led to models that achieve high performance but which are also highly complex [20; 11]. Understanding these complex models is crucial for safe and reliable deployments of these systems in the real-world. One approach to improve our understanding of a model is through explanations. This can take many forms such as feature importance [31; 35; 23; 37], high level concepts [17; 44], counterfactual examples [39; 12; 25], robustness of gradients , or influential training samples [18; 43].

In contrast to generating post-hoc explanations of a given model, we aim to learn models given apriori explanations. There has been some recent work along such lines. Koh et al. , Zarlenga et al.  incorporates explanations within the model architecture by requiring a conceptual bottleneck layer. Ross et al. , Rieger et al. , Ismail et al. , Stacey et al.  use explanations to modify the learning procedure for any class of models: they incorporate explanations as a regularizer, penalizing models that do not exhibit apriori given explanations; Ross et al.  penalize input gradients, while Rieger et al.  penalize a Contextual Decomposition score . Some of these suggest that constraining models via explanations leads to higher accuracies and more robustness to spurious correlation, but do not provide analytical guarantees. On the theoretical front, Li et al.  show that models that are easier to explain locally also generalize well. However, Bilodeau et al.  show that common feature attribution methods without additional assumptions on the learning algorithm or data distribution do no better than random guessing at inferring counterfactual model behavior.

**Learning Theory.** Our contribution is to provide an analytical framework for learning from explanations that quantify the benefits of explanation constraints. Our analysis is closely related to the framework of learning with side information. Balcan and Blum  shows how unlabeled data can help in semi-supervised learning through a notion of compatibility between the data and the target model. This work studies classical notions of side information (e.g., margin, smoothness, and co-training). Subsequent papers have adapted this learning theoretic framework to study the benefits of representation learning  and transformation invariance . On the contrary, our paper focuses on the more recent notion of explanations. Rather than focus on the benefits of unlabeled data, we characterize the quality of different explanations. We highlight that constraints here are stochastic, as they depend on data points which differs from deterministic constraints that have been considered in existing literature, such as constraints on the norm of weights (i.e., L2 regularization).

**Self-Training.** Our work can also be connected to the self-training literature [5; 42; 40; 8], where we could view our variational objective as comprising a regularized (potentially simpler) teacher model that encodes these explanation constraints into a student model. Our variational objective (where we use simpler teacher models) is also related to distillation, which has also been studied in terms of gradients .

## 3 Learning from Explanation Constraints

Let \(\) be the instance space and \(\) be the label space. We focus on binary classification where \(=\{-1,1\}\), but which can be naturally generalized. Let \(\) be the joint data distribution over \((,)\) and \(_{}\) the marginal distribution over \(\). For any classifier \(h:\), we are interested in its classification error \((h):=_{(x,y) D}(h(x) y)\), though one could also use other losses to define classification error. Our goal is to learn a classifier with small error from a family of functions \(\). In this work, we use the words model and classifier interchangeably. Now, we formalize local explanations as functionals that take in a model and a test input, and output a vector:

**Definition 1** (Explanations).: _Given an instance space \(\), model hypothesis class \(\), and an explanation functional \(g:^{r}\), we say \(g(h,x)\) is an explanation of \(h\) on point \(x\) induced by \(g\)._For simplicity, we consider the setting when \(g\) takes a single data point and model as input, but this can be naturally extended to multiple data points and models. We can combine these explanations with prior knowledge on how explanations should look like at sample points in term of constraints.

**Definition 2** (Explanation Constraint Set).: _For any instance space \(\), hypothesis class \(\), an explanation functional \(g:^{r}\), and a family of constraint sets \(\{C(x)^{r} x\}\), we say that \(h\) satisfies the explanation constraints with respect to \(C\) iff:_

\[g(h,x) C(x),\  x.\]

In our definition, \(C(x)\) represents values that we believe our explanations should take at a point \(x\). For example, "an input gradient of a feature 1 must be larger than feature 2" can be represented by \(g(h,x)=_{x}h(x)\) and \((x)=\{(x_{1},,x_{d})^{d} x_{1}>x_{2}\}\). In practice, human annotators will be able to provide the constraint set \(C(x^{})\) for a random sample \(k\) data points \(S_{E}=\{x_{1}^{},,x_{k}^{}\}\) drawn i.i.d. from \(_{}\). We then say that any \(h\)\(S_{E}\)-satisfies the explanation constraints with respect to \(C\) iff \(g(h,x) C(x),\  x S_{E}\). We note that the constraints depends on random samples \(x_{i}^{}\) and therefore _are random_. To tackle this challenge, we can draw from the standard learning theoretic arguments to reason about probably approximately satisfying the constraints in expectation. Before doing so, we first consider the notion of explanation surrogate losses, which will allow us to generalize the setup above to a form that is amenable to practical estimators.

**Definition 3**.: _(Explanation surrogate loss) An explanation surrogate loss \(:\) quantifies how well a model \(h\) satisfies the explanation constraint \(g(h,x) C(x)\). For any \(h,x\):_

1. \((h,x) 0\)_._
2. _If_ \(g(h,x) C(x)\) _then_ \((h,x)=0\)_._

For example, we could define \((h,x)=1\{g(h,x) C(x)\}\). Given such a surrogate loss, we can substitute the explanation constraint that \(g(h,x) C(x)\) with the surrogate \((h,x) 0\). We now have the machinery to formalize how to reason about the random explanation constraints given a random set of inputs. First, denote the expected explanation loss as \((h,):=_{x}[(h,x)]\). We are interested in models that satisfy the explanation constraints up to some slack \(\) (i.e. approximately) in expectation. We define a learnability condition of this explanation surrogate loss as EPAC (Explanation Probably Approximately Correct ) learnability.

**Definition 4** (EPAC learnability).: _For any \((0,1),>0\), the sample complexity of \((,)\) - EPAC learning of \(\) with respect to a surrogate loss \(\), denoted \(m(,;,)\) is defined as the smallest \(m\) for which there exists a learning rule \(\) such that every data distribution \(_{}\) over \(\), with probability at least \(1-\) over \(S^{m}\),_

\[((S),)_{h}(h,)+.\]

_If no such \(m\) exists, define \(m(,;,)=\). We say that \(\) is EPAC learnable in the agnostic setting with respect to a surrogate loss \(\) if \(\ (0,1),>0\), \(m(,;,)\) is finite._

_Furthermore, for a constant \(\), we denote any model \(h\) with \(\)-Approximately Correct Explanation where \((h,)\), with a \(\) - CE models. We define the class of \(\) - CE models as_

\[_{,,}=\{h\ :\ (h,) \}.\] (1)

We simply use \(_{,}\) to denote this class of CE models. From natural statistical learning theoretic arguments, a model that satisfies the random constraints in \(S_{E}\) might also be a CE model.

**Proposition 3.1**.: _Suppose a model \(h\)\(S_{E}\)-satisfies the explanation constraints then_

\[(h,_{}) 2R_{k}()+},\]

_with probability at least \(1-\), when \(k=|S_{E}|\) and \(=\{(h,) h\}\)._

We use \(R_{k}()\) to denote Rademacher complexity; please see Appendix A where we review this and related concepts. Note that even when \(h\) satisfies the constraints exactly on \(S\), we can only guarantee a bound on the expected surrogate loss \((h,_{})\).We can achieve a bound similar to that in Proposition 3.1 via a single and simpler constraint on the empirical expectation \(|}_{x S_{E}}(h,x)\). We can then extend the above proposition to show that if \((h,S_{E})\), then \((h,D_{})+2R_{k}()+},\) with probability at least \(1-\). Another advantage of such a constraint is that the explanation constraints could be noisy, or it may be difficult to satisfy them exactly, so \(\) also serves as a slack. The class \(\) contains all surrogate losses of any \(h\). Depending on the explanation constraints, \(\) can be extremely large. We remark that the surrogate loss \(\) allows us to reason about satisfying an explanation constraint on a new data point and in expectation. However, for many constraints, \(\) does not have a closed-form or is unknown on an unseen data point. The question of which types of explanation constraints are generalizable may be of independent interest, and we further discuss this in Appendix B and provide further examples of learnable constraints in Appendix D.

**EPAC-ERM Objective.** Let us next discuss _combining_ the two sources of information: the explanation constraints that we set up in the previous section, together with the usual set of labeled training samples \(S=\{(x_{1},y_{1}),,(x_{n},y_{n})\}\) drawn i.i.d. from \(\) that informs the empirical risk. Combining these, we get what we call EPAC-ERM objective:

\[_{h}_{i=1}^{n}(h,x_{i},y_{i})\ \ \ \ _{i=1}^{k}(h,x_{i}^{}).\] (2)

We provide a learnability condition for a model that achieve both low average error and surrogate loss in Appendix F.

### Generalization Bound

We assume that we are in a doubly agnostic setting. Firstly, we are agnostic in the usual sense that there need be no classifier in the hypothesis class \(\) that perfectly labels \((x,y)\); instead, we hope to achieve the best error rate in the hypothesis class, \(h^{*}=_{h}_{}(h)\). Secondly, we are also agnostic with respect to the explanations, so that the optimal classifier \(h^{*}\) may not satisfy the explanation constraints exactly, so that it incurs nonzero surrogate explanation loss \((h^{*},D)>0\).

**Theorem 3.2** (Generalization Bound for Agnostic Setting).: _Consider a hypothesis class \(\), distribution \(\), and explanation loss \(\). Let \(S=\{(x_{1},y_{1}),,(x_{n},y_{n})\}\) be drawn i.i.d. from \(\) and \(S_{E}=\{x_{1}^{},,x_{k}^{}\}\) drawn i.i.d. from \(_{}\). With probability at least \(1-\), for \(h\) that minimizes empirical risk \(_{S}(h)\) and has \((h,S_{E})\), we have_

\[_{D}(h)_{D}(h^{*}_{-_{ k}})+2R_{n}(_{,+_{k}})+2},\]

\[_{k}=2R_{k}()+},\]

_when \(=\{(h,x) h,x\}\) and \(h^{*}_{}=_{h_{,}}_{ }(h)\)._

Proof.: The proof largely follows the arguments in Balcan and Blum , but we use Rademacher complexity-based deviation bounds instead of VC-entropy. We defer the full proof to Appendix E. 

Our bound suggests that these constraints help with our learning by shrinking the hypothesis class \(\) to \(_{,+_{k}}\), reducing the required sample complexity. However, there is also a trade-off between reduction and accuracy. In our bound, we compare against the best classifier \(h^{*}_{-_{k}}_{,-_{k}}\) instead of \(h^{*}\). Since we may have \((h^{*},)>0\), if \(\) is too small, we may reduce \(\) to a hypothesis class that does not contain any good classifiers. Recall that the generalization bound for standard supervised learning -- in the absence of explanation constraints -- is given by

\[_{D}(h)_{D}(h^{*})+2R_{n}( )+2}.\]

We can see the difference between this upper bound and the upper bound in Theorem 3.2 here as a possible notion of the goodness of an explanation constraint. We further discuss this in Appendix C.

Gradient Explanations for Particular Hypothesis Classes

In this section, we further quantify the usefulness of explanation constraints on different concrete examples and characterize the Rademacher complexity of the restricted hypothesis classes. In particular, we consider an explanation constraint of a constraint on the input gradient. For example, we may want our model's gradient to be close to that of some \(h^{}\). This translates to \(g(h,x)=_{x}h(x)\) and \(C(x)=\{x^{d}\:|\:\|x-_{x}h^{}(x)\|\}\) for some \(>0\).

### Gradient Explanations for Linear Models

We now consider the case of a uniform distribution on a sphere, and we use the symmetry of this distribution to derive an upper bound on the Rademacher complexity (full proof to Appendix H).

**Theorem 4.1** (Rademacher complexity of linear models with a gradient constraint, uniform distribution on a sphere).: _Let \(_{}\) be a uniform distribution on a unit sphere in \(^{d}\), let \(=\{h:x w_{h},x w_{h}^{d},||w_{ h}||_{2} B\}\) be a class of linear models with weights bounded by a constant \(B\). Let \((h,x)=(w_{h},w_{h^{}})\) be a surrogate loss where \((u,v)\) is an angle between \(u,v\). We have_

\[R_{n}(_{,})}(() p+ ),\]

_where \(p=(()}{})\) and \((x)=}_{0}^{x}e^{-t^{2}}dt\) is the standard error function._

The standard upper bound on the Rademacher complexity of linear models is \(}\). Our bound has a nice interpretation; we shrink our bound by a factor of \((+()p)\). We remark that \(d\) increases, we observe that \(p 1\), so the term \(()p\) dominates this factor. As a consequence, we get that our bound is now scaled by \(()\) and the the Rademacher complexity scales down by a factor of \(\). This implies that given \(n\) labeled data, to achieve a fast rate \(()\), we need \(\) to be as good as \(O(})\).

### Gradient Explanations for Two Layer Neural Networks

**Theorem 4.2** (Rademacher complexity of two layer neural networks (\(m\) hidden nodes) with a gradient constraint).: _Let \(\) be an instance space and \(_{}\) be a distribution over \(\) with a large enough support. Let \(=\{h:x_{j=1}^{m}w_{j}(u_{j}^{}x)|w_{j} ,u_{j}^{d},_{j=1}^{m}|w_{j}| B,\|u_{j}\|_{2}=1\}\) be a class of two layer neural networks with a ReLU activation function and bounded weight. Assume that there exists some constant \(C>0\) such that \(_{x_{}}[\|x\|_{2}^{2}] C^{2}\). Consider an explanation loss given by_

\[(h,x)= \|_{x}h(x)-_{x}h^{}(x)\|_{2}+ 1\{\| _{x}h(x)-_{x}h^{}(x)\|>\}\]

_for some \(>0\). Then, we have that \(R_{n}(_{,})}\)._

Proof.: (Sketch) The key ingredient is to identify the impact of the gradient constraint and the form of class \(_{,}\). We provide an idea when we have \(m=1\) node. We write \(h(x)=w(u^{}x)\) and \(h^{}(x)=w^{}(u^{}x)\). Note that \(_{x}h(x)-_{x}h^{}(x)=wu1\{u^{}x>0\}-w^{}u^{ }1\{(u^{})^{}x>0\}\) is

Figure 2: Visualization of the piecewise constant function of \(_{x}h(x)-_{x}h^{}(x)\) when \(h\) is a two layer NNs with 1 node. Background colors represent regions with non-zero value.

a piecewise constant function (Figure 2). Assume that the probability mass of each region is non-negative, our gradient constraint implies that the norm of each region cannot be larger than \(\).

1. If \(u,u^{}\) have different directions, we have 4 regions in \(_{x}h(x)-_{x}h^{}(x)\) and can conclude that \(|w|<,|w^{}|<\).
2. If \(u=u^{}\) have the same direction, we only have 2 regions in \(_{x}h(x)-_{x}h^{}(x)\) and can conclude that \(\|wu-w^{}u^{}\|=|w-w^{}|<\).

The gradient constraint enforces a model to have the same node boundary \((u=u^{})\) with a small weight difference \(|w-w^{}|<\) or that node would have a small weight \(|w|<\). This finding allows us to determine the restricted class \(_{,}\), and we can use this to bound the Rademacher complexity accordingly. For full details, see Appendix I. 

We compare this with the standard Rademacher complexity of a two layer neural network ,

\[R_{n}()}.\]

We can do better than this standard bound if \(<\). One interpretation for this is that we have a budget at most \(\) to change the weight of each node and for total \(m\) nodes, we can change the weight by at most \( m\). We compare this to \(B\) which is an upper bound on the total weight \(_{j=1}^{m}|w_{j}| B\). Therefore, we can do better than a standard bound when we can change the weight by at most two thirds of the average weight \(\) for each node. We would like to point out that our bound does not depend on the distribution \(\) because we choose a specific explanation loss that guarantees that the gradient constraint holds almost everywhere. Extending to a weaker loss such as \((h,x)=\|_{x}h(x)-_{x}h^{}(x)\|\) is a future research direction. In contrast, our result for linear models uses a weaker explanation loss and depends on \(\) (Theorem H.1). We also assume that there exists \(x\) with a positive probability density at any partition created by \(_{x}h(x)\). This is not a strong assumption, and it holds for any distribution where the support is the \(^{d}\), e.g., Gaussian distributions.

## 5 Algorithms for Learning from Explanation Constraints

Although we have analyzed learning with explanation constraints, algorithms to solve this constrained optimization problem are non-trivial. In this setting, we assume that we have access to \(n\) labeled data \(\{(x_{i},y_{i})\}_{i=1}^{n}\), \(m\) unlabeled data \(\{x_{n+i}\}_{i=1}^{m}\), and \(k\) data with explanations \(\{(x_{n+m+i},(,x_{n+m+i}))\}_{i=1}^{k}\). We argue that in many cases, \(n\) labeled data are the most expensive to annotate. The \(k\) data points with explanations also have non-trivial cost; they require an expert to provide the annotated explanation or provide a surrogate loss \(\). If the surrogate loss is specified then we can evaluate it on any unlabeled data, otherwise these data points with explanations could be expensive. On the other hand, the \(m\) data points can cheaply be obtained as they are completely unlabeled. We now consider existing approaches to incorporate this explanation information.

**EPAC-ERM:** Recall our EPAC-ERM objective from (2):

\[_{h}_{i=1}^{n}1\{h(x_{i}) y_{i}\}\ \ _{j=n+m+1}^{n+m+k}(h,x_{j})\]

for some constant \(\). This constraint in general requires more complex optimization techniques (e.g., running multiple iterations and comparing values of \(\)) to solve algorithmically. We could also consider the case where \(=0\), which would entail the hypotheses satisfy the explanation constraints exactly, which however is in general too strong a constraint with noisy explanations.

**Augmented Lagrangian objectives:**

\[_{h}_{i=1}^{n}1[h(x_{i}) y_{i}]+_{j=n+m+1}^{n+m+k}(h,x_{j})\]

As is done in prior work , we can consider an augmented Lagrangian objective. A crucial caveat with this approach is that the explanation surrogate loss is in general a much more complicated functional of the hypothesis than the empirical risk. For instance, it might involve the gradient of the hypothesis when we use gradient-based explanations. Computing the gradients of such a surrogate loss can be more expensive compared to the gradients of the empirical risk. For instance, in our experiments, computing the gradients of the surrogate loss that involves input gradients is 2.5 times slower than that of the empirical risk. With the above objective, however, we need to compute the same number of gradients of both the explanation surrogate loss and the empirical risk. These computational difficulties have arguably made incorporating explanation constraints not as popular as they could be.

### Variational Method

To alleviate these aforementioned computational difficulties, we propose a _new_ variational objective

\[_{h}(1-)*{}_{(x,y) }[(h(x),y)]+_{f_{,} }*{}_{x}[(h(x),f(x))],\]

where \(\) is some loss function and \(t 0\) is some threshold. The first term is the standard expected risk of \(h\) while the second term can be viewed as a projection distance between \(h\) and \(\)-CE models.

It can be seen that the optimal solution of **EPAC-ERM** would also be an optimal solution of our proposed variational objective. The advantage of this formulation however is that it decouples the standard expected risk component from the surrogate risk component. This allows us to solve this objective with the following iterative technique, drawing inspiration from prior work in posterior regularization [9; 14]. More specifically, let \(h_{t}\) be the learned model at time \(t\) and at each timestep \(t\),

1. We project \(h_{t}\) to the class of \(\)-CE models. \[f_{t+1,}= *{argmin}_{h}_{i=n+1}^{n+ m}(h(x_{i}),h_{t}(x_{i}))+(0,\,_{i=n+m+1}^{ n+m+k}(h,x_{i})-).\] The first term is the difference between \(h_{t}\) and \(f\) on unlabeled data. The second term is the surrogate loss, which we want to be smaller than \(t\). \(\) is a regularization hyperparameter.
2. We calculate \(h_{t+1}\) that minimizes the empirical risk of labeled data and matches pseudolabels from \(f_{t+1,}\) \[h_{t+1,}= *{argmin}_{h}_{i=1}^{n}(h(x _{i}),y_{i})+_{i=n+1}^{n+m}(h(x_{i}),f_{t+1,}(x_{i})).\] Here, the discrepancy between \(h\) and \(f_{t+1,}\) is evaluated on the unlabeled data \(\{x_{j}\}_{j=n+1}^{n+m}\).

The advantage of this decoupling is that we could use a differing number of gradient steps and learning rates for the projection step that involves the complicated surrogate loss when compared to the empirical risk minimization step. Secondly, we can simplify the projection iterate computation by replacing \(_{,}\) with a simpler class of teacher models \(_{,}\) for greater efficiency. Thus, the decoupled approach to solving the EPAC-ERM objective is in general more computationally convenient.

We initialize this procedure with some model \(h_{0}\). We remark that could see this as a constraint regularized self-training where \(h_{t}\) is a student model and \(f_{t}\) is a teacher model. At each timestep, we project a student model to the closest teacher model that satisfies the constraint. The next student model then learns from both labeled data and pseudo labels from the teacher model. In the standard self-training, we do not have any constraint and we have \(f_{t}=h_{t}\).

## 6 Experiments

We provide both synthetic and real-world experiments to support our theoretical results and clearly illustrate interesting tradeoffs of incorporating explanations. In our experiments, we compare our method against 3 baselines: (1) a standard supervised learning approach, (2) a simple Lagrangian-regularized method (that directly penalizes the surrogate loss \(\)), and (3) self-training, which propagates the predictions of (1) and matches them on unlabeled data. We remark that (2) captures the essence of the method in Ross et al. , except there is no \(_{2}\) regularization term.

Our experiments demonstrate that the proposed variational approach is preferable to simple Lagrangian methods and other supervised methods in many cases. In particular, the variational approach leads to a higher accuracy under limited labeled data settings. In addition, our method leadsto models that satisfy the explanation constraints much more frequently than other baselines. We also compare to a Lagrangian-regularized + self-training baseline (first, we use the model (2) to generate pseudolabels for unlabeled data and then train a new model on both labeled and unlabeled data) in Appendix L. We remark that this baseline isn't a standard method in practice and does not fit nicely into a theoretical framework, although it seems to be the most natural approach to using unlabeled data in this procedure. More extensive ablations are deferred to Appendix N, and code to replicate our experiments will be released with the full paper.

### Regression Task with Exact Gradient Information

In our synthetic experiments, we focus on a regression task where we try to learn some model contained in our hypothesis class. Our data is given by \(=^{d}\), and we try to learn a target function \(h^{*}:\). Our data distribution is given by \(X(0,^{2}I)\), where \(I\) is a \(d d\) identity matrix. We generate \(h^{*}\) by randomly initializing a model in the specific hypothesis class \(\). We assume that we have \(n\) labeled data, \(m\) unlabeled data, and \(k\) data with explanations.

We first present a synthetic experiment for learning with a perfect explanation, meaning that \((h^{*},S)=0\). We consider the case where we have the _exact_ gradient of \(h^{*}\). Here, let \(\) be a linear classifier and note that the exact gradient gives us the slope of the linear model, and we only need to learn the bias term. Incorporating these explanation indeed helps as both methods that include explanation constraints (Lagrangian and ours) perform much better (Figure 3).

We also demonstrate incorporating this information for two layer neural networks. We observe a clear difference between the simpler Lagrangian approach and our variational objective (Figure 4 - left). Our method is clearly the best in the setting with limited labeled data and matches the performance of the strong self-training baseline with sufficient labeled data. We note that this is somewhat expected, as these constraints primarily help in the setting with limited labeled data; with enough labeled data, standard PAC bounds suffice for strong performance.

We also analyze how strongly the approaches enforce these explanation constraints on new data points that are seen at test time (Figure 4 - right) for two layer NNs. We observe that our variational objective approaches have input gradients that more closely match the ground-truth target network's

Figure 4: Comparison of MSE on regressing a two layer neural network (left) and \(_{2}\) distance over input gradients as we vary the amount of labeled data \(n\) (right). Left is task performance and right is explanation constraint satisfaction. Results are averaged over 5 seeds. \(m=1000,k=20\).

Figure 3: Comparison of MSE on regressing a linear model. Results are averaged over 5 seeds. \(m=1000.\,k=20\).

input gradients. This demonstrates that, in the case of two layer NNs with gradient explanations, our approach best achieves both good performance and satisfying the constraints. Standard self-training achieves similar performance in terms of MSE but has no notion of satisfying the explanation constraints. The Lagrangian method does not achieve the same level of satisfying these explanations as it is unable to generalize and satisfy these constraints on new data.

### Tasks with Imperfect Explanations

Assuming access to perfect explanations may be unrealistic in practice, so we present experiments when our explanations are imperfect. We present classification tasks (Figure 5) from a weak supervision benchmark . In this setting, we obtain explanations through the approximate gradients of a single weak labeler, as is done in . More explicitly, weak labelers are heuristics designed by domain experts; one example is functions that check for the presence of particular words in a sentence (e.g., checking for the word "delicious" in a Yelp comment, which would indicate positive sentiment). We can then access gradient information from such weak labelers, which gives us a notion of feature importance about particular features in our data. We note that these examples of gradient information are rather _easy_ to obtain, as we only need domain experts to specify simple heuristic functions for a particular task. Once given these functions, we can apply them easily over unlabeled data without requiring any example-level annotations.

We observe that our variational objective achieves better performance than all other baseline approaches on the majority of settings defined by the number of labeled data. We remark that the explanation in this dataset is a noisy gradient explanation along two feature dimensions, yet this still improves upon methods that do not incorporate this explanation constraint. Indeed, our method outperforms the Lagrangian approach, showing the benefits of iterative rounds of self-training over the unlabeled data. In addition to our real-world experiments, we present synthetic experiments with noisy gradients in Appendix K.1.

## 7 Discussion

Our work proposes a new learning theoretic framework that provides insight into how apriori explanations of desired model behavior can benefit the standard machine learning pipeline. The statistical benefits of explanations arise from constraining the hypothesis class: explanation samples serve to better estimate the population explanation constraint, which constrains the hypothesis class. This is to be contrasted with the statistical benefit of labeled samples, which serve to get a better estimate of the population risk. We provide instantiations of our analysis for the canonical class of gradient explanations, which captures many explanations in terms of feature importance. It would be of interest to provide corollaries for other types of explanations in future work. As mentioned before, the generality of our framework has larger implications towards incorporating constraints that are not considered as "standard" explanations. For example, this work can be leveraged to incorporate more general notions of side information and inductive biases. We also discuss the societal impacts of our approach in Appendix O. As a whole, our paper supports using further information (e.g., explanation constraints) in the standard learning setting.

Figure 5: Comparison of accuracy on the YouTube (left) and the Yelp (right) datasets. Here, we let \(m=500,k=150,T=2,=0.0\). Results are averaged over 40 seeds.

Acknowledgements

This work was supported in part by DARPA under cooperative agreement HR00112020003, FA8750-23-2-1015, ONR grant N00014-23-1-2368, NSF grant IIS-1909816, a Bloomberg Data Science PhD fellowship and funding from Bosch Center for Artificial Intelligence and the ARCS Foundation.