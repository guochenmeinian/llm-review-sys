# Multi-turn Reinforcement Learning

from Preference Human Feedback

 Lior Shani

liorshani@google.com Aviv Rosenberg

&Asaf Cassel

avivros@google.com acassel@mail.tau.ac.il

Oran Lang

Daniele Calandriello

Avital Zipori

Hila Noga

Orgad Keller

Bilal Piot

Idan Szpektor

Avinatan Hassidim

Yossi Matias

Remi Munos

Equal contribution. \({}^{1}\) Google Research. \({}^{2}\) Google DeepMind. \({}^{3}\) Tel Aviv University.

###### Abstract

Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks. Existing methods work by emulating the preferences at the single decision (turn) level, limiting their capabilities in settings that require planning or multi-turn interactions to achieve a long-term goal. In this paper, we address this issue by developing novel methods for Reinforcement Learning (RL) from preference feedback between two full multi-turn conversations. In the tabular setting, we present a novel mirror-descent-based policy optimization algorithm for the general multi-turn preference-based RL problem, and prove its convergence to Nash equilibrium. To evaluate performance, we create a new environment, Education Dialogue, where a teacher agent guides a student in learning a random topic, and show that a deep RL variant of our algorithm outperforms RLHF baselines. Finally, we show that in an environment with explicit rewards, our algorithm recovers the same performance as a reward-based RL baseline, despite relying solely on a weaker preference signal.

## 1 Introduction

A pinnacle of human intelligence is the ability to communicate with an environment, forming complex interactions to accomplish challenging goals. Dialogues are one example of such dynamic communication, where one party reacts to signals from the other parties and dynamically plans ahead to steer communication towards their purpose. Recent years have seen scientific breakthroughs in developing Large Language Models (LLMs) that can communicate with humans in natural language (Ouyang et al., 2022; Anil et al., 2023; Touvron et al., 2023; OpenAI, 2024; Google, 2024). In order to align these models to human needs, many efforts have been made to train them with a given human feedback. In some concordance with human learning, this is usually achieved by reinforcing behaviors that align with the feedback, using a technique now called Reinforcement Learning from Human Feedback (RLHF; Christiano et al. (2017); Ziegler et al. (2019); Stiennon et al. (2020)).

RLHF methods build on the long studied field of Reinforcement Learning (RL), which focuses on learning optimal actions through reward feedback (a numerical signal) from the environment. However, defining a suitable reward function is challenging, leading to the common practice of collecting human preferences between choices. In the absence of rewards, a mapping from preference to reward is typically assumed in the form of the Bradely-Terry (BT; Bradley and Terry (1952)) model (Siennon et al., 2020; Rafailov et al., 2023), enabling the use of a wide-variety of well-researchedRL techniques. Alternatively, recent research (Munos et al., 2023; Azar et al., 2023; Tang et al., 2024) suggests a more direct use of preferences for learning, eliminating the need for this potentially limiting assumption.

Still, so far the main focus of both the RLHF and the direct preference learning literature was on single-turn scenarios, where given relevant context, the LLM generates one response and receives an immediate feedback that reflects its alignment quality. Importantly, while single-turn RLHF already provides significant gains for valuable AI systems, it lacks the adaptive and long-term capabilities that make human communication such a powerful tool, and usually characterize RL methods. This is especially apparent in temporally extended tasks, such as multi-turn dialogue (Irvine et al., 2023), complex tool use (Wang et al., 2022) and multi-step games (Hendrycks et al., 2022).

Contributions.In this work, we focus on improving the communication of AI agents with dynamic environments. To this end, we first extend the RLHF paradigm to the multi-turn setting, where the agent has a series of exchanges with an external (stochastic) environment (Section 3). Importantly, we consider (human) feedback that compares entire multi-turn conversations as opposed to single-turn scenarios, which compare individual actions on a per-turn basis. Conversation-level feedback allows to capture the long-term effect of individual actions, which may not be immediately apparent, and thus hard to define through turn-level feedback. For example, a seller agent asking too high a price may seem immediately bad, but becomes potentially good as part of a complete strategy to increase sale price. This difference is apparent in our preference model, making it better suited for multi-turn interactions.

Formalizing the multi-turn setting as a Contextual Markov Decision Process with end of interaction preference feedback, we devise several theoretically grounded algorithms (Section 4). Our main algorithm, Multi-turn Preference Optimization (MTPO), is a new policy optimization algorithm for the general multi-turn preference-based setting. MTPO is based on the Mirror Descent (MD) method (Nemirovskij and Yudin, 1983; Beck and Teboulle, 2003) together with self-play (Silver et al., 2017), and is proven to converge to a Nash equilibrium (Nash et al., 1950), i.e., a policy which is preferred over any other policy. We prove similar results for MTPO\(-\), a slight variant of MTPO that, similarly to Munos et al. (2023), uses a geometric mixture policy which interpolates the agent's policy with a fixed reference policy (with mixing rate \(\)). These algorithms utilize a new form of preference-based Q-function that accounts for the long-term consequences of individual actions. Finally, leveraging our theoretical framework, we modify this Q-function to create a multi-turn RLHF algorithm and prove its convergence to an optimal policy (w.r.t the learned reward function).

We complement our theoretical findings with a policy-gradient version of our multi-turn algorithms for deep learning architectures (Section 4.1). To validate our approach, we apply our algorithms to train a T5 encoder-decoder LLM (Raffel et al., 2020), aiming to enhance its multi-turn dialogue abilities (Sections 5 and 6). We test our approach in a scenario without explicit rewards, where conversation quality is evaluated solely through preferences. To that end, we create a new environment called Education Dialogue, where a teacher guides a student in learning a random topic, by prompting Gemini (Team et al., 2023). The conversation is judged based on preference feedback, using a constitution that defines effective learning (Bai et al., 2022; Lee et al., 2023) (see Section 5). In this environment, our multi-turn algorithms significantly outperform single-turn baselines, and our direct multi-turn preference approach outperforms multi-turn RLHF (Section 6). As an additional contribution, we publicly release the data of Education Dialogue.1 Finally, we demonstrate that even in a reward-based environment, our preference-based algorithm achieves comparable performance to learning directly from rewards, as in standard RL, despite using a weaker signal. For this experiment, we utilize the LMRL-Gym (Abdulhai et al., 2023) Car Dealer environment, simulating a conversation where the agent (car dealer) aims to maximize the sale price.

Related work.Most related to our work is the RLHF literature, which aims to improve an LLM policy using preference data collected from humans. Earlier methods model a proxy reward (Ouyang et al., 2022) or preference function (Zhao et al., 2022), and apply traditional RL techniques. More recent methods directly optimize the policy (Rafailov et al., 2023; Azar et al., 2023; Tang et al., 2024; Song et al., 2024; Ethayarajh et al., 2024). Another line of work, which forms the basis for MTPO, extends RLHF to games, aiming to compute a Nash equilibrium instead of an optimal policy w.r.t a fixed reward/preference. This includes Nash-MD (Munos et al., 2023), self-play and mixtures of the two like IPO-MD (Calandriello et al., 2024). Nonetheless, these methods only consider single-turn problems, whereas MTPO provides the first guarantees for multi-turn settings. Note the difference from concurrent attempts to extend direct preference optimization to the token level (Rafailov et al., 2024), while a true multi-turn approach must deal with the additional uncontrollable tokens generated by the human in-between agent turns.

More broadly, preference-based RL (see survey by Wirth et al. (2017)) studies feedback in terms of preferences over two alternatives rather than absolute rewards. Feedback can be provided in various ways, e.g., at the level of states (turn-level), or entire trajectories (Chen et al., 2022; Saha et al., 2023; Wu and Sun, 2023; Wang et al., 2023; Zhan et al., 2023; Wu and Sun, 2023). The focus of this work is last state feedback which is an instance of trajectory feedback. Another closely related model is RL with aggregate feedback where only the sum of rewards in a trajectory is revealed to the agent (Efroni et al., 2021; Cohen et al., 2021; Chatterji et al., 2021; Cassel et al., 2024).

Lastly, there is vast literature on using RL to improve natural language generation for dialogue systems. The pioneering work of Li et al. (2016) focuses on designing rewards to capture important dialogue attributes such as semantic coherence and ease of answering. Other works tackle task-oriented dialogue, using RL to enhance the agent's ability to solve the underlying dialogue task (Wei et al., 2018). Instead, this works takes a more general and fundamental approach, focusing on the algorithmic process of aligning an agent which repeatedly interacts with an environment. While dialogue systems are a promising application of our approach, as suggested by the experimental results of this paper (Sections 5 and 6), our algorithmic approach is much broader, including processes such as tool-use, reasoning, and many other applications that require aligning a complex multi-turn agent with human preferences.

## 2 Preliminaries

The interaction between an AI agent and its environment is captured in the fundamental contextual RL model, where the context is the initial prompt, the states are conversation summaries, and the actions are responses.

**Contextual Markov decision process.** A finite-horizon contextual Markov decision process (CMDP) \(\) is defined by a tuple \((,,,H,x_{1},_{c},p)\) where \(\) is the context space, \(\) is the state space, \(\) is the action space, \(H\) is the horizon, \(x_{1}\) is the initial state, \(_{c}_{}\) is the context distribution and \(p:_{}\) is the transition function such that \(p(x^{}\,|\,c,x,y)\) is the probability to transition to state \(x^{}\) after taking action \(y\) in state \(x\), given context \(c\).

An interaction between the agent and the CMDP environment proceeds in \(H\) steps. First, a context \(c\) is sampled from \(_{c}\), and then the agent begins in the initial state \(x_{1}\). In step \(h[\,H]\), the agent observes the current state \(x_{h}\), picks an action \(y_{h}\) and transitions to the next state \(x_{h+1}\) sampled from the transition function \(p( c,x_{h},y_{h})\). At the end of the interaction, the agent arrives in a final state \(x_{H+1}\). For simplicity, we assume that the state space can be decomposed into \(H+1\) disjoint subsets \(=_{h=1}^{H+1}X_{h}\) such that, in step \(h\) of the interaction, the agent is in some state \(x_{h}_{h}\). A policy \(:_{}\) is a mapping from a context and state to a distribution over actions. Together with transition \(p\), \(\) induces a distribution over trajectories denoted by \(_{,p}[]\) (and \(_{,p}[]\) for the expectation), in which the trajectory is generated by sampling the actions according to the policy and next states according to the environment.

### Single-turn Reinforcement Learning from Human Feedback

Unlike standard RL where the agent observes reward feedback for its actions, the influential work of Christiano et al. (2017) suggests to leverage preference data. In the single-turn setting, the agent generates a single sequence \(y\) given a context \(c\). This is modeled as a Contextual Multi-Armed Bandit (CMAB), which is a CMDP instance with horizon \(H=1\). The feedback is given in the form of preference between two generated sequences. Formally, there exists a preference model \(:\) such that \((y>y^{}\,|\,c)\) gives the probability that \(y\) is preferred over \(y^{}\) given context \(c\). Preferences naturally extend to policies via expectation \((>^{}\,|\,c)=_{y(\{\}c),y^{ }(\{\}c)}[(y y^{}\,|\,c)]\).

Reinforcement Learning from Human Feedback.In RLHF, it is assumed that there is a hidden reward function \(r:\) that defines the preferences through the Bradley-Terry (BT) model, i.e., \((y>y^{} c)=(r(c,y)-r(c,y^{}))\), where \(\) is the sigmoid. To reconstruct the reward, the RL algorithm is used to optimize the ELO score of the chosen action \(y\) using a cross-entropy loss. This technique was adapted to RL fine-tuning LLMs (Ziegler et al., 2019), and has become the standard approach for aligning LLMs to human feedback (Stiennon et al., 2020; Rafailov et al., 2023).

Learning from direct preferences.Recently Munos et al. (2023); Azar et al. (2023) suggested to drop the BT assumption, and learn a direct preference model instead of reward. Munos et al. (2023) propose the Nash-MD algorithm which converges to the Nash equilibrium of a (regularized) preference model, i.e., a policy which is preferred over any other policy. In iteration \(t+1\), Nash-MD updates its policy \(_{t+1}\) using a mirror descent (MD) step projected to a geometric mixture policy. The mixture policy \(_{t}^{}( c)_{t}( c)^{1-_{t}} ( c)^{_{t}}\) interpolates between the policy \(_{t}\) and a reference policy \(\), given a regularization coefficient \(>0\). Formally, for learning rate \(_{t}>0\),

\[_{t+1}( c)=_{( c)_{y}}_{t} (_{t}^{} c)-(\|_{t}^{} )[c] c,\]

where \((\|^{})[c](( c)\|^ {}( c))=_{y}(y c)(y c)}\).

## 3 Multi-turn Preference-Based RL

In the multi-turn setting, the agent repeatedly interacts with an external environment, an interaction we formulate using the CMDP model. Similarly to the single-turn case, we consider preference-based RL, where the feedback is given as preference instead of reward. However, in our case, we assume preferences are between final CMDP states with a shared initial context. Formally, there exists a preference model \(:_{H+1}_{H+1} \) such that \((x_{H+1} x_{H+1}^{} c)\) gives the probability that \(x_{H+1}\) is preferred over \(x_{H+1}^{}\) given context \(c\). That is, in order to receive feedback, a learning algorithm performs two interactions with the environment and observes a Bernoulli sample for which one is preferred. We follow the natural assumption of Munos et al. (2023) that the preference model is symmetric, i.e., \((x_{H+1}^{} x_{H+1} c)=1-(x_{H+1} x _{H+1}^{} c)\). We define the preference between a final state and a policy by \((x> c)=_{,p}[(x>x_{H+1} c)]\). Similarly, \((^{} c)=_{,p}[(x_{H+ 1}^{} c)]\). For brevity, since contexts are independent, we omit the context throughout the rest of the paper. Similarly to Munos et al. (2023), our objective is to find a policy \(^{*}\) which is preferred over any other alternative policy, i.e.,

\[^{*}_{}_{^{}}(^{}),\]

which is a Nash equilibrium in the above two-player game defined by the preference model, following the minimax theorem (Von Neumann, 1928) (see Lemma 3.2). Notably, due to the anti-symmetric nature of the preference objective, the Nash equilibrium will have both agents following the same policy, and thus can be expressed as a single policy.

Regularized preference model.In the rest of the paper, we will consider a regularized version of the preference model. This is motivated by practical RLHF algorithms (Stiennon et al., 2020), and generalizes the single-turn model in Munos et al. (2023). Let \(\) be a reference policy, and define the \(\)-regularized preference model as follows:

\[_{}(^{})=(^{}) -_{p}(\|)+_{p}(^{}\|),\]

where \(_{p}(\|)\) is the KL-divergence between the distributions that the policies induce over trajectories in the CMDP. We prove the following two results for the regularized preference model. First, its KL term has a value difference-like decomposition into the KL-divergences at individual states. Second, it has a unique Nash equilibrium (proofs in Appendix A).

**Lemma 3.1**.: _Let \(,^{}\) be two policies, then: \(_{p}(\|^{})=_{,p}_{h=1}^{H} (\|^{})[x_{h}]\)._

**Lemma 3.2**.: _There exists a unique Nash equilibrium of the regularized preference model \(_{}\)._

Trajectory-wise vs. turn-wise preference feedback.A naive adaptation of single-turn RLHF to the multi-turn scenario would treat each turn as a separate single-turn problem. This would require feedback for the preference between two actions in each turn. Instead, in the setting we consider, the preference feedback is only between two full trajectories. Note that a single feedback for the entire trajectory is much more natural when considering conversations, since only the full conversations tell whether the objective was reached. Moreover, collecting preference data for intermediate actions could lead to destructive biases because the quality of an action can change dramatically depending on the actions taken later in the trajectory. For example, a chatbot directly answering a user query is usually a required behavior. Yet, when the chatbot does not have sufficient information to respond well, asking the user for more details might be a better action. Consequently, it is very hard for a rarer to know which of these actions is better without observing how the conversation unrolls, i.e., without observing the user's reaction to the chatbot's question, and how the chatbot's response changes given this reaction. This difference demonstrates the challenge of multi-turn RL as it requires planning ahead instead of myopic reward maximization, which is the approach for single-turn RL.

The multi-turn setting in LLMs.While we consider a general preference-based RL setup (through the CMDP model), our focus is on applying this framework to multi-turn language-based interactions. The action space \(\) is a sequence of tokens of a vocabulary \(\), and the state space at step \(h\), \(_{h}\), is a sequence based on the past sequences. For example, in conversational dialogues, the state \(x_{h}\) holds the whole dialogue up to the \(h\)-th turn, the action \(y_{h}\) is the current sequence generated by the agent, and the next-state is simply the concatenation of the conversation \(x_{h}\) with the new \(y_{h}\) and a next sequence sampled by the environment (the user's response). Alternatively, in the complex tool-use case, where an agent repeatedly interacts with different APIs, the current state includes the original user query and a summary of results from APIs so far, the action is a new API call or user-facing response, and next state is a new sequence summarizing previous state with the new API response.

**Remark 3.3** (Token-level application to the single-turn auto-regressive case).: _Notably, this formulation also captures the single-turn auto-regressive case. Clearly, this holds when considering only one turn, \(H=1\), but it ignores the token-level optimization done at each turn. Instead, we frame the auto-regressive problem by limiting the actions at each step to single vocabulary tokens, \(=\), and assuming a null deterministic environment (\(x_{h+1}\) is the concatenation of \(x_{h}\) and \(y_{h}\)). Importantly, our results apply to the token-level, which is usually neglected when devising single-turn algorithms._

## 4 Algorithms for the multi-turn setting

Preference-based Q-function.Our algorithms rely on a fundamental concept in RL - the value and Q-functions. In reward-based RL, it is essential to define the value \(V^{}:\) as the expected reward when playing policy \(\) starting in some state \(x_{h}\), i.e., \(V^{}(x_{h})=_{,p}_{h^{}=h}^{H}r(x_{h^{ }},y_{h^{}}) x_{h}\). In the preference-based scenario, we argue that value functions remain a powerful tool, even though there is no reward to maximize. We define the following regularized preference-based value functions, which are key to our algorithm.

\[Q_{}^{,^{}}(x_{h},y_{h}) =_{,p}(x_{H+1}>^{})- _{h^{}=h}^{H}((\|) x_{h^{}}  x_{h},y_{h},\] \[V_{}^{,^{}}(x_{h}) =_{,p}(x_{H+1}>^{})- _{h^{}=h}^{H}((\|) x_{h^{}}  x_{h}.\]

There are a few interesting points in the definition above. First, note that these values are functions of two policies \(,^{}\). This is because the quality of a policy \(\) cannot be measured on its own, and must be compared to another policy \(^{}\). Second, while \(\) starts at the state \(x_{h}\), the comparison policy \(^{}\) starts its trajectory from the initial state. This is a significant difference from the usual paradigm of Q-functions in RL and might seem peculiar at first glance. However, this formulation captures the fact that the optimal policy in a state should be preferred not only over any other policy along the sub-tree starting from this state, but also over any other policy, even ones that do not pass through this state at all. Although different in concept, the following lemma shows that our preference-based Q-function satisfies a value difference lemma, allowing us to optimize the policy locally in order to maximize our global objective (proof in Appendix B).

**Lemma 4.1**.: _Let \(,^{},\) be policies, then the following value difference lemma holds:_

\[_{}(>)-_{}(^{}>)=_{^{},p}_{h=1}^{H}(-^{},Q_{ }^{,}) x_{h}+(^{ }\|) x_{h}-(\|) x _{h},\]

_where \(-^{},Q[x]( x)-^{} ( x),Q(x,)\) and \( x,y=_{i}x(i)y(i)\) is the inner product._MTPO.We present the MTPO (Multi-turn Preference Optimization) algorithm, which provably solves the multi-turn preference-based RL objective. Formally, we prove MTPO converges to the unique Nash equilibrium of the regularized preference model. MTPO is based on two key principles: First, the regularized preference model defines a two-player anti-symmetric constant-sum game which can be solved using a self-play mirror descent method (Munos et al., 2023; Calandriello et al., 2024). Second, our introduced Q-function allows to reduce the (global) optimization of the game into local mirror descent optimization problems in each state. Together they yield the MTPO update rule for iteration \(t+1\),

\[_{t+1}( x_{h})=_{}_{t},Q_{}^{_{ t},_{t}}}[x_{h}]-_{t}(\|)[x_{h}]-(1- _{t})(\|_{t})[x_{h}],\] (1)

where \(_{t}\) is a learning rate. The solution can be made explicit in the following form (Appendix E.2):

\[_{t+1}(y_{h} x_{h})y_{h} x_{h}^{ _{t}}_{t}(y_{h} x_{h})^{1-_{t}}e^{_{t}Q_{}^{_{t },_{t}}(x_{h},y_{h})}.\] (2)

The intuition behind the algorithm is observed nicely in this update rule - we improve the current policy in the direction of the regularized preference against itself (represented by the self-play Q-function), while not deviating too much and keeping close to the reference policy. The following is our main theoretical result: last-iterate convergence to Nash equilibrium (proof in Appendix B).

**Theorem 4.2**.: _Let \(_{}^{}\) be the Nash equilibrium of the regularized preference model, and \(\) be a bound on the magnitude of the Q-functions. Then, for \(_{t}=\), MTPO guarantees at every iteration \(t\),_

\[_{p}(_{}^{}\|_{t})^{2}}{ ^{2}(t+1)}.\]

_Let \(_{}\) be the minimal non-zero probability assigned by \(\), then \(\{4 H},1\}\)._

Proof sketch.: By Lemma 3.1, the global \(_{p}(_{}^{}\|_{t+1})\) can be decomposed to the local KL in each state \(x_{h}\), \((_{}^{}\|_{t+1})[x_{h}]\). Then, we use MD analysis in each state to bound the local KL as:

\[(_{}^{}\|_{t+1})[x_{h}](1-_{t })(_{}^{}\|_{t})[x_{h}]+2_{t}^{2} ^{2}\\ +_{t}(_{t}-_{}^{},Q_{}^{_{t },_{t}})[x_{h}]+(_{}^{}\|)[x_{h}]- (_{t}\|)[x_{h}],\]

giving a recursive guarantee dependent on the local one-step regularized advantage of the current policy against the Nash policy and an additional term bounded by \(\). We plug this local bound into the KL decomposition (Lemma 3.1), which gathers the local KL terms back to \(_{p}(_{}^{}\|_{t})\). Importantly, the value difference lemma (Lemma 4.1) aggregates the advantage terms to the global regularized preference \(_{}(_{t}>_{t})-_{}(_{}^{ }>_{t})\), which is non-positive by the optimality of \(_{}^{}\). This leaves us with the global recursive bound: \(_{p}(_{}^{}\|_{t+1})(1-_{t})_{p}(_{}^{}\|_{t})+2_{t}^{2}^{2}\). We conclude by unrolling the recursion with the chosen \(_{t}\). 

MTPO with mixture policy.Inspired by Nash-MD (Munos et al., 2023), we present a variant of MTPO which makes use of the mixture policy \(_{t}^{}( x)_{t}( x)^{1-_{t}}(  x)^{_{t}}\). This variant, which we call MTPO-\(\) (where \(\) will be the mixing coefficient in our experiments), gives similar theoretical guarantees (see Theorem B.2 in Appendix B) and performs better in practice (see Section 6). In fact, the following MTPO-\(\) update rule is almost equivalent to MTPO (Equation (1)) with the only difference being the policies that define the Q-function, \(Q_{}^{_{t},_{t}}\) vs. \(Q_{}^{_{}^{},_{t}^{}}\).

\[_{t+1}( x_{h})=_{}_{t},Q_{}^{_{ t}^{},_{t}^{}}}[x_{h}]-(\|_{t}^{})[x_{h}].\]

MTPO-\(\) naturally extends Nash-MD to the multi-turn setting, and reveals that Nash-MD is a self-play algorithm itself, but plays \(_{t}^{}\) instead of \(_{t}\). Practically, MTPO has the computational advantage over MTPO-\(\) (and Nash-MD) of not keeping the additional policy \(_{t}^{}\). Moreover, MTPO avoids the difficulty of computing the geometric mixture, which Munos et al. (2023) approximate heuristically via linear interpolation between the logits of the two policies.

Multi-turn RLHF.While we focused so far on our preference-based algorithms, our derivation holds for any online reward function since it is built on the mirror-descent method. Specifically, in the case of multi-turn RLHF, we consider the reward function \(r^{}\) learned from preference data using the Bradley-Terry model, and define the corresponding regularized Q-function \(Q_{}^{,}(x_{h},y_{h})=_{,p}\![r^{ }(x_{H+1})-_{h^{}=h}^{H}(\|)[x_{h^ {}}] x_{h},y_{h}]\). By replacing \(Q_{}^{_{},_{}}\) in Equation (1) with \(Q_{}^{_{},}\), we obtain the multi-turn RLHF algorithm that converges to the regular RLHF objective - the optimal regularized policy w.r.t. the reward \(r^{}\) (see Theorem B.6 in Appendix B). This complementary contribution emphasizes the similarity and difference between the RLHF and MTPO algorithms: The optimization process is identical for both methods, with the exception that the RLHF reward is fixed and computed w.r.t. the data policy, whereas preference-based MTPO uses an adaptive self-play mechanism to compute preferences w.r.t. the current policy.

### Deep RL implementation

Our deep RL implementation is a natural adaptation of the tabular algorithms presented in the previous section. At each iteration, training data is acquired by sampling a batch of contexts from the data, and using each context to sample two trajectories with the current policy \(_{_{t}}\). Then, the final states of both trajectories serve as inputs to a direct preference model that outputs the probability of one being preferred over the other. Similarly to the way the preference model is trained in Nash-MD (Munos et al., 2023), this preference model is trained in advance on the available offline preference data.

The update rule in Equation (1) relies on the \(Q\)-function of the current policy. We therefore use an actor-critic policy optimization based approach and train two models, a policy \(_{}\), and its value \(V_{,}^{_{_{t}},_{_{t}}}\), which is typically used to estimate the advantage, \(A_{}^{,^{}}(x,y) Q_{}^{,^{}}(x, y)-V_{}^{,^{}}(x)\)(Schulman et al., 2017). For simplicity and computational efficiency, we implement a policy-gradient (PG) based approach and ignore the MD stability term \((_{_{t}}\|_{_{t-1}})\), similarly to the implementation of the Nash-MD algorithm. We justify this simplification with the fact that the KL regularization w.r.t. the fixed reference policy \(\) already provides stability to our online algorithm, somewhat similarly to the way the Follow-The-Regularized-Leader (FTRL; Orabona (2019)) algorithm operates. Nevertheless, we believe that this additional MD penalty should contribute to the performance and stability of the algorithm, as shown in (Tomar et al., 2020), and we leave this for further research. This yields the following losses, when auction \(y\) is played at state \(x\),

\[_{}(;x,y) =-_{}^{_{_{t}},_{_{t}}}(x,y) _{}(y x)+(_{}\|)[x],\] \[_{}(;x) =_{}^{_{_{t}},_{_{t}}}(x)-V_ {,}^{_{_{t}},_{_{t}}}(x)^{2},\]

where \(_{}^{_{_{t}},_{_{t}}},_{}^{_{ _{t}},_{_{t}}}\), are estimations of the current value and advantage using Generalized Advantage Estimation (GAE, Schulman et al. (2017)). We also batch-normalize the value-loss and advantage as recommended in (Andrychowicz et al., 2020). Finally, when the policy is an auto-regressive language model, which generates actions token-by-token until an end-of-sequence signal is generated, we use a turn-level value (and not a token-level value as done in (Siennon et al., 2020)). That is, the value model gets as input a state represented by a sequence of tokens, and outputs a single scalar value instead of a scalar value for each token in the action sequence. This is justified by our analysis which treats whole turns as single actions. We leave the many ways to combine turn-level and token-level values for future research.

## 5 Experimental Setup

This section describes the domains and models used in our experiments. To create online environments suited for multi-turn RL, we mimic the RLHF process (Siennon et al., 2020), replacing the human parts with prompted state-of-the-art LLMs, similarly to Abdulhai et al. (2023) (see Figure 1):

1. **Dataset creation:** First, we devise a story-line for the user and the environment, describing their characters and goals. Then, we generate a dataset by prompting a state-of-the-art LLM such as Gemini (Team et al., 2023) or GPT (Brown et al., 2020) with the story-line. When generating data, a full conversation is sampled at once, meaning that both the agent and environment are generated together to make them more consistent. Furthermore, to create a diverse set of conversations, we devise a diverse list of attributes for both the agent and environment, sample attributes out of the list, and pass it to the generation prompt.
2. **Environment preparation:** Once the data is curated, we use it to fine-tune two smaller LLMs, one for the agent and one for the environment, using teacher forcing.
3. **Preference/reward learning:** We prepare preference data by sampling pairs of conversations from the agent and environment models. To label the data, we prompt a high-capacity LLM with either instructions on how to score a conversation, or criteria for preferring a conversation over another. The data is used to fine-tune two smaller LLMs: an RLHF reward model (with BT loss), and a preference model (with probability regression loss).

We experiment with two domains, preference-based _Education Dialogue_ and reward-based _Car Dealer_:

Education Dialogue.The core of our approach is learning when there is no clear reward, instead only (human) preferences can be acquired. To validate our approach in this scenario, we created a novel multi-turn task for evaluating algorithms based on preference data. In this scenario, which we term Education Dialogue, a teacher (agent) is faced with the task of teaching a student (environment) a given topic in the best means possible. We follow the dataset creation procedure and prompt Gemini Ultra (Google, 2024) to create such interactions between the teacher and student. The teacher is prompted with a learning topic in science, history, etc. The student is prompted with the characteristics of its learning habits, e.g., prefers interactive learning, lecture-based learning or hands-on activities. The preference model is prompted with instructions that define a good learning interaction. For reproducibility, and to further advance the research of the multi-turn setting, we openly release the data and prompts used to create this new benchmark. For more details, see Appendix C and the example in Figure 1.

Figure 1: Education Dialogue data generation process. _Top:_ prompt used to generate conversation with Gemini. _Middle:_ conversations sampled from the the interaction between the teacher and student models. _Bottom:_ prompt used for the preference oracle.

Car Dealer.In this LMRL-Gym (Abdulhai et al., 2023) domain, a car dealer is assigned with the task of selling a car at the highest price to a customer. We skip the data creation step, and directly use the Car Dealer published data to fine-tune the dealer (agent) and customer (environment) T5-large models. The reward is calculated by prompting a Flan-T5 model to extract the sale price from the conversation, whenever a sale has occurred. When using a preference-based algorithm, the preference of one trajectory over the other is computed using the BT model with the rewards of the two trajectories.

Single-turn baselines.The key hypothesis of this work is that conversation-level signals are preferred over single-turn signals for optimizing multi-turn trajectories. To verify this in the Education Dialogue domain, we devise two single-turn baselines by sampling data where each conversation turn has two different policy responses. The first baseline, called _single-turn-reward_, rates the two responses using a modified preference prompt (see Appendix C), in which the model is asked to evaluate the responses by their effect on the overall conversation. This technique is prevalent when human raters are asked to evaluate multi-turn data. The second baseline, called _single-turn-value_, assumes access to a Monte-Carlo estimate of the value: it uses our original preference prompt (see Figure 1) by continuing the trajectories of both possibilities and then calculating the preference in the end. For both baselines, we train an RLHF algorithm and a preference-based Nash-MD algorithm.

Models.The agent and environment are modeled with T5 encoder-decoder models. Specifically, we use the T5-large (770M) and T5-XL (3B) models. The same models are used for the RLHF BT-based reward and preference-based models. For prompted reward/preference models, we make use of the Flan-T5 XL (3B) (Chung et al., 2024). For training, we use a configuration of \(4 4\) Tensor Processing Units (TPUs; Jouppi et al. (2023)) which typically yields 0.1 training steps per second, where a step consists of learning a 10-turn episode. A detailed list of hyperparameters is found in Appendix D. We run each evaluation on 1600 random samples from an independent evaluation set.

## 6 Experiments

In this section we evaluate the algorithms proposed in Section 4. We start with the preference-based Education Dialogue environment (see Section 5), and compare our multi-turn algorithms to SFT (supervised fine-tuning) as well as single-turn baselines. We note that, unlike single-turn benchmarks which are based on data with real human preferences, our golden preference data itself is generated by an LLM (Gemini Ultra). Therefore, the true goal in our curated environment is to align the model with the preference of this highly capable LLM rather than a human rater. While human evaluation is always interesting, here it is actually only a proxy to alignment with the data distribution. To efficiently validate our models, we start with a thorough comparison between our baselines and candidates using a prompted Flan-T5 XL model as a judge, which was verified to correlate with the high-capacity Gemini Ultra (Table 1). We then compare our best candidates using the same Gemini Ultra which generated the preference alignment feedback (Table 2).

    & SL &  &  &  \\   & SFT & RLHF & Nash & RLHF & Nash & RLHF & MTPO & MTPO-\(\) \\  SFT & – & \(0.164\) & \(0.347\) & \(0.197\) & \(0.324\) & \(0.212\) & \(0.091\) & \(0.093\) \\ RLHF-reward & \(0.836\) & – & \(0.628\) & \(0.515\) & \(0.654\) & \(0.399\) & \(0.392\) & \(0.354\) \\ Nash-reward & \(0.653\) & \(0.372\) & – & \(0.411\) & \(0.51\) & \(0.328\) & \(0.281\) & \(0.242\) \\ RLHF-value & \(0.803\) & \(0.485\) & \(0.589\) & – & \(0.568\) & \(0.408\) & \(0.396\) & \(0.366\) \\ Nash-value & \(0.676\) & \(0.346\) & \(0.49\) & \(0.432\) & – & \(0.45\) & \(0.298\) & \(0.27\) \\ RLHF-multi & \(0.788\) & \(0.601\) & \(0.672\) & \(0.592\) & \(0.55\) & – & \(0.433\) & \(0.412\) \\ MTPO & \(0.909\) & \(0.608\) & \(0.719\) & \(0.604\) & \(0.702\) & \(0.567\) & – & \(0.439\) \\ MTPO-\(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & – \\   

Table 1: Side-by-side evaluation for Education Dialogue using Flan-T5 XL as the prompted preference model. Each entry is the average preference of 1,600 conversations generated with row method \(y\), over ones generated with column method \(y^{}\). We evaluate each method using 3 different seeds, compute 3 \(\) 3 comparisons matrix and report the mean (the standard deviation is reported in Appendix D).

Multi vs. single turn.Tables 1 and 2 show that all multi-turn algorithms (MTPO and multi-turn RLHF) with conversation-level feedback significantly outperform the single-turn baselines, validating our hypothesis. We conjecture that it is attributed to several factors: First, the effect of a single-decision on the whole conversation is hard to capture, causing highly inaccurate single-turn reward/preference models. Notably, this leads to inferior performance of Nash-MD compared to single-turn RLHF, since it optimizes to find Nash equilibrium of this inaccurate model while RLHF does not stray so far from the reference. Second, even if one could estimate the current policy's value, this estimate becomes biased when the policy changes during training. Finally, single-turn preferences consider only "local" decisions which share the same conversational path, and not how these decisions "globally" compare to other possible paths, as captured by the preference-based Q-function \(Q_{}^{_{t},_{t}}\) (see Section 4).

MTPO vs. multi-turn RLHF.Comparing our three multi-turn algorithms, we see two main results. First, the two variants of MTPO outperform multi-turn RLHF. This is expected since the environment is not reward-based, and hence it extends the results of  from the single-turn case, and supports the theoretical claim that MTPO converges to the Nash policy while multi-turn RLHF converges to the optimal policy w.r.t the learned reward (which is based only on the reference policy). Second, MTPO-\(\) outperforms MTPO. While both algorithms converge to the same Nash equilibrium, we conjecture that the superior performance of MTPO-\(\) stems from the stochasticity that the mixture policy \(_{t}^{}\) introduces. Namely, \(_{t}\) might tend towards deterministic behavior, causing less informative feedback from self-play, as the two sampled trajectories would be very similar. On the other hand, \(_{t}^{}\) is more stochastic, providing diversity in the sampled trajectories.

Reward-based environment.In an additional experiment, we test MTPO and multi-turn RLHF in the reward-based Car Dealer environment, where the goal is maximizing sale price (see Section 5). We compare a standard policy-gradient RL algorithm against our algorithms in two scenarios: an online scenario where the reward or preference feedback is given using an online oracle, and an RLHF-like setting, where we first create preference data using the oracle, and then use it to fine-tune a (BT) reward and preference models. Table 3 shows that even though MTPO receives preferences instead of the explicit optimization target (rewards), it still learns as good as RL. Interestingly, MTPO recovers a slightly higher reward than multi-turn RLHF despite the fact the true preferences are sampled from a BT model. This may imply that a preference model generalizes better than a BT-reward model, perhaps because it is independent of the sampling policy.

Limitations.This work presents a proof of concept for the potential of MTPO to improve existing single-turn techniques. Our experimental setup might be limited by the relatively small T5-based models and the use of prompt-based environments. We leave applications to state-of-the-art models and algorithms, and more realistic environments to future work.

    &  &  &  \\   &  & RLHF-reward & RLHF & MTPO-\(\) \\   & SFT & – & \(0.206\) & \(0.164\) & \(0.086\) \\  & RLHF-reward & \(0.794\) & – & \(0.452\) & \(0.277\) \\  & RLHF-multi & \(0.836\) & \(0.548\) & – & \(0.288\) \\  & MTPO-\(\) & \(\) & \(\) & \(\) & – \\   & SFT & – & \(0.295\) & \(0.101\) & \(0.041\) \\  & RLHF-reward & \(0.705\) & – & \(0.180\) & \(0.069\) \\   & RLHF-multi & \(0.899\) & \(0.82\) & – & \(0.139\) \\   & MTPO-\(\) & \(\) & \(\) & \(\) & – \\   

Table 2: Side-by-side evaluation for Education Dialogue using Gemini Ultra as the prompted preference model. Each entry is the average preference of 1,000 conversations generated with row method \(y\), over ones generated with column method \(y^{}\).

    &  &  \\   & Reward (RL) & MTPO & RLHF & MTPO \\  Reward (Price) & 58.4 (0.3) & 57.1 (0.2) & 53.2 (0.3) & 58.6 (0.3) \\   

Table 3: Car Dealer experiments averaged across 5 seeds and reported with 95% confidence interval.