# Private Stochastic Convex Optimization with Heavy Tails: Near-Optimality from Simple Reductions

Hilal Asi

Apple Inc.

hilal.asi94@gmail.com &Daogao Liu

University of Washington

liudaogao@gmail.com &Kevin Tian

University of Texas at Austin

kjtian@cs.utexas.edu

Part of this work was done while interning at Apple.

###### Abstract

We study the problem of differentially private stochastic convex optimization (DP-SCO) with heavy-tailed gradients, where we assume a \(k^{}\)-moment bound on the Lipschitz constants of sample functions, rather than a uniform bound. We propose a new reduction-based approach that enables us to obtain the first optimal rates (up to logarithmic factors) in the heavy-tailed setting, achieving error \(G_{2}}+G_{k}(}{ne})^{1-}\) under \((,)\)-approximate differential privacy, up to a mild polylog\(()\) factor, where \(G_{2}^{2}\) and \(G_{k}^{k}\) are the \(2^{}\) and \(k^{}\) moment bounds on sample Lipschitz constants, nearly-matching a lower bound of . We further give a suite of private algorithms in the heavy-tailed setting which improve upon our basic result under additional assumptions, including an optimal algorithm under a known-Lipschitz constant assumption, a near-linear time algorithm for smooth functions, and an optimal linear time algorithm for smooth generalized linear models.

## 1 Introduction

Differentially private stochastic convex optimization (DP-SCO), where an algorithm aims to minimize a population loss given samples from a distribution, is a fundamental problem in statistics and machine learning. In this problem, given \(n\) samples from a distribution \(\) over a sample space \(\), our goal is to privately find an approximate minimizer \(^{d}\) for the population loss

\[F_{}(x):=_{s}[f(x;s)],\]

where \(f(;s)\) is a convex function for all \(s\). The quality of an algorithm is measured by the excess population loss of its output \(\), that is \(F_{}()-_{x^{*}}F_{}(x^{*})\).

Extensive research efforts have been devoted to DP-SCO, resulting in important progress over the past few years . In an important milestone,  developed optimal algorithms (in terms of the excess population loss) for DP-SCO under a uniform Lipschitz assumption (i.e., where every \(f(;s)\) is assumed to have the same Lipschitz bound), and  followed this result with efficient and optimal algorithms that run in linear time for smooth functions. DP-SCO has also been explored in other notable settings, including developing faster algorithms for non-smooth settings , different geometries imposed on the solution space , and different notions of privacy .

Most existing results in DP-SCO are based on the assumption that the function \(f(;s)\) is uniformly \(G\)-Lipschitz for all \(s\). This assumption is convenient for private algorithm design, because it allows us to straightforwardly bound the _sensitivity_ of iterates of private algorithms, i.e., how far a pair of iterates defined via algorithms induced by neighboring datasets drift apart. Under the uniform Lipschitz assumption, the DP-SCO problem is relatively well-understood, as optimal and efficientalgorithms exist (sometimes requiring additional regularity assumptions) .2 State-of-the-art SCO algorithms satisfying \((,)\)-differential privacy (Definition 1) in the uniform Lipschitz setting result in excess population loss

\[GD(}+)}}{ n }),\] (1)

where \(D\) is the diameter of \(\). However, the assumption of uniformly \(G\)-Lipschitz gradients is strong, and may be violated in real-life applications where the distribution in question has heavy tails (see e.g. discussion in ). As a simple motivating example, consider mean estimation, where each \(f(;s)=\|-s\|^{2}\), so the minimizer of \(F_{}\) is the population mean. The uniform Lipschitz requirement amounts to \(\) having a bounded support over \(\), whereas an algorithm that can handle heavy tails only posits the weaker assumption that \(\) has bounded \(k\)-th moments. However, as pointed out by , many real-world datasets , especially those from biomedicine and finance, are usually unbounded or even heavy-tailed. As a result, existing algorithms for DP-SCO may have overly pessimistic performance bounds when \(G\) is large or even unbounded, necessitating the search for new private algorithms handling heavy-tailed gradients.

Motivated by this weakness of existing DP-SCO analyses, several papers studied the problem of DP-SCO with heavy-tailed gradients , formally defined in Definition 4. Rather than assuming uniformly Lipschitz gradients, this line of work builds on the more realistic assumption that the norm of the gradients has bounded \(k^{}\)-moments. In particular,  studied heavy-tailed private optimization for the related empirical loss, while  initiated an analogous study for the population loss. More recently,  also proposed algorithms to solve the heavy-tailed DP-SCO problem based on clipped stochastic gradient methods.

Despite the significant progress made in addressing heavy-tailed DP-SCO, it remains notably less understood compared to the uniformly Lipschitz setting. As a benchmark, under a notion called \(\)-concentrated differential privacy (CDP, see Definition 3), which translates to \((,)\)-DP for \(^{2}^{-1}()\),  established that the best excess population loss achievable scales as

\[G_{2}D}+G_{k}D(}{n} )^{1-},\] (2)

where \(G_{j}^{j}\) is the \(j^{}\) moment bound on the Lipschitz constant of sampled functions, see Definition 4. Note that as \(k\), the rate in (2) recovers the uniform Lipschitz rate in (1).

Unfortunately, existing works on heavy-tailed DP-SCO assume stringent conditions on problem parameters and are suboptimal in the general case. For example,  requires the loss functions to be uniformly smooth with various parameter bounds in order to guarantee optimal rates, while the recent work  obtains a suboptimal rate scaling as3\(G_{2}D}+G_{k}D(}{n})^{1- }\), which is worse than (2) by polynomial factors in the dimension for any constant \(k\).

### Our contributions

Motivated by the suboptimality of existing results for heavy-tailed DP-SCO, we develop the first algorithm for this problem, which achieves the optimal rate (2) up to logarithmic factors with no additional assumptions. Along the way, we give several simple reduction-based tools for overcoming technical barriers encountered by prior works. To state our results (deferring a formal problem statement to Definition 1), we assume that for some \(k 2\) and all \(j[k]\), we have

\[_{s}[_{x}\| f(x;s)\|^{j }] G_{j}^{j}.\]

Our results hold in several settings and are based on different reductions which allow us to apply strategies for DP-SCO from the uniform Lipschitz setting.

Near-optimal rates for heavy-tailed DP-SCO (Section 3).We design an algorithm for the \(k\)-heavy-tailed DP-SCO problem, which satisfies \(\)-CDP4 and attains near-optimal excess loss

\[G_{2}D)}{n}}+G_{k}D( ()}{n})^{1- }.\] (3)

This matches the lower bounds recently proved by  for \(\)-concentrated DP algorithms up to logarithmic factors, stated in (2). Standard conversions from CDP to \((,)\)-DP imply that our algorithm also obtains \( G_{2}D}+G_{k}D((1/ )}}{n})^{1-}\) under this parameterization. We note that our bound (3) holds with high probability \( 1-\), whereas the lower bound (2) is for an error which holds only in expectation (see Theorem 13, ). Our lossiness in (3) is due to a natural sample-splitting strategy used to boost our failure probability, and we conjecture that (3) may be optimal in the high-probability error bound regime.

As in , to establish our result we begin by deriving utility guarantees for a clipped stochastic gradient descent subroutine on an empirical loss, where clipping ensures privacy but induces bias, parameterized by a dataset-dependent quantity \(b_{}^{2}\) defined in (26). We give a standard analysis of this subroutine in Proposition 1, a variant of which (with slightly different parameterizations) also appeared as Lemma 27, . However, the key technical barrier encountered by the  analysis, when converting to population risk, was bounding \(b_{}^{2}\) over the sampled dataset, which naively depends on the \(2k^{}\) moment of gradients. This either incurs an overhead depending on \(G_{2k}\), or in the absence of such a bound (which is not given under the problem statement), leads to the aforementioned suboptimal rate in  losing a factor of \((}{n})^{}\) in the utility. We give a further discussion of natural strategies and barriers towards directly bounding \(b_{}^{2}\) in Appendix G.

Where we depart from the strategy of  is in the use of a new _population-level localization_ framework we design (see Algorithm 2), inspired by similar localization techniques in prior work  (discussed in more detail in Section 1.2). This strategy allows us to use constant-success probability bounds on the quantity \(b_{}\) (which also bound \(b_{}^{2}\)), which are easy to achieve depending only on \(G_{k}\) rather than \(G_{2k}\) via Markov's inequality. This bypasses the need in  for bounding \(b_{}^{2}\). The motivation for population-level localization is that we wish to aggregate empirical solutions to multiple datasets, some of which have small \(b_{}\), and others which do not. However, each dataset has a different empirical minimizer, so it is unclear how to argue about convergence if we apply the empirical localization. Instead, we aggregate solutions close to the population-level minimizer and share them across datasets via a simple geometric aggregation technique, showing that it suffices for a constant fraction of datasets to have this desirable property for us to carry out our population-level localization argument. We formally state our main result achieving the rate (3) as Theorem 1.

Interestingly, as a straightforward corollary of our new localization framework, we achieve a tight rate for high-probability stochastic convex optimization under a bounded-variance gradient estimator parameterization, perhaps the most well-studied formulation of SCO. To our knowledge, this result was only first achieved very recently by .5 However, we find it a promising proof-of-concept that our new framework directly yields the same result. For completeness, we include a derivation in Appendix E (see Theorem 5) as a demonstration of the utility of our framework.

Optimal rates with known Lipschitz constants (Appendix B).We next consider the _known Lipschitz_ setting, where each sample function \(f(;s)\) arrives with a value \(_{s}\) which is an overestimate of its Lipschitz constant, such that \(_{s}^{j}\) is bounded for all \(j[k]\) (see Assumption 2). As motivation, consider the problem of learning a generalized linear model (GLM), where \(f(;s)=(,s)\) for a known convex activation function \(\). Typically, the Lipschitz constant for \(f(;s)\) is simply the Lipschitz constant of \(\) times \(\|s\|\), which can be straightforwardly calculated. Thus, for GLMs, our known Lipschitz heavy-tailed assumption amounts to moment bounds on the distribution \(\).

Our second result, Theorem 2, shows a natural strategy obtains optimal rates in this known Lipschitz setting, eliminating logarithmic factors from Theorem 1. As mentioned previously, this result holds for the important family of GLMs. Our algorithm is based on a straightforward reduction to the uniformly Lipschitz setting: after simply iterating over the input samples, and replacing samples whose Lipschitz constant exceeds a given threshold with a new dummy sample, we show existing Lipschitz DP-SCO algorithms then obtain the optimal heavy-tailed excess population loss (2). Despite the simplicity of this result, to the best of our knowledge, it was not previously known.

Efficient algorithms for smooth functions (Appendices C and D).Finally, we propose algorithms with improved query efficiency for general smooth functions or smooth GLMs, with moderate smoothness bounds. Our strategy is to analyze the stability of clipped-DP-SGD in the smooth heavy-tailed setting, and use localization-based reductions to transform a stable algorithm into a private one . This results in linear-time algorithms for the smooth case with near-optimal rates. In order to prove the privacy of our smooth, heavy-tailed algorithm, we analyze a careful interplay of our clipped stochastic gradient method with the sparse vector technique (SVT) . At a high level, our use of SVT comes from the fact that under clipping, smooth gradient steps no longer enjoy the type of contraction guarantees applicable in the uniform Lipschitz setting (see Fact 3), so we must take care to not clip too often. The SVT is then used to ensure privacy of our count of how many clipping operations were used. In Appendix F, we provide a simple counterexample showing that the noncontractiveness of contractive steps after applying clipping is inherent. Our general smooth heavy-tailed DP-SCO result is stated as Theorem 3.

We believe the use of SVT within an optimization algorithm to ensure privacy may be of independent interest, as it is one of few such instances that have appeared in the private optimization literature to our knowledge; it is inspired by a simpler application of this technique carried out in .

On the other hand, we make the simple observation that for GLMs, clipping cannot make a contractive gradient step noncontractive, by taking advantage of the fact that the derivative of \(f(x;s)=( x,s)\) is a multiple of \(s\) for any \(x\) (see Lemma 14). We use this observation to give a straightforward adaptation of the smooth algorithm in  to the heavy-tailed setting, proving Theorem 4, which attains both a linear gradient query complexity and the optimal rate (2).

### Prior work

The best-known rates for heavy-tailed DP-SCO were recently achieved by . As discussed previously, their results do not provide the same optimality guarantees as our Theorem 1. The rate achieved by  is polynomially worse than the optimal loss (2) for any constant \(k\). On the other hand, the work of  uses a different assumption on the gradients than Assumption 1, which is arguably more nonstandard: in particular, they require that the \(k^{}\)-order central moments of each coordinate \(_{j}f(x;s)\) is bounded. Moreover, their algorithms require each sample function \(f(;s)\) to be \(\)-smooth, and the final rates have a strong dependence on the condition number \(=\) where \(\) is the strong convexity parameter (see Appendix C in  for additional discussion).

Our result in the heavy-tailed setting assuming \(\)-smoothness of sample functions, Theorem 3, is most directly related to Theorem 15 of . These two results respectively require

\[=O(}{D}^{1.5}}) =O(}{D}(}{ n} )^{}),\]

omitting logarithmic factors in our bound for simplicity, to obtain near-optimal rates. These regimes are different and not generally comparable. However, we find it potentially useful that our upper bound on \(\) grows as more samples are taken, whereas the  bound degrades with larger \(n\). It is worth mentioning that 's Theorem 15 shaves roughly one logarithmic factor in the error bound from our Theorem 3. On the other hand, Theorem 3 actually requires a looser condition than mentioned above (see (20)), which can improve its guarantees in a wider range of parameters.

Finally, we briefly contextualize our population-level localization framework in regard to previous localization schemes proposed by . The two localization schemes in  (see Sections 4.1 and 5.1 of that work) both follow the same strategy of gradually improving distance bounds to a minimizer in phases. However, their implementation is qualitatively different than our Algorithm 2, preventing their direct application in our algorithm. For instance, Section 4.1 of  does not use strong convexity and, therefore cannot take advantage of generalization bounds afforded to strongly convex losses (see discussion in ). On the other hand, the scheme in Section 5.1 of  serves a different purpose than Algorithm 2, aiming to solve strongly convex optimization by reducing it to non-strongly convex optimization; our Algorithm 2, on the other hand, directly targets non-strongly convex optimization as its goal. We view our approach as complementary to these prior frameworks and are optimistic it will find further utility in applications.

## 2 Preliminaries

General notation.We use \([d]\) to denote the set \(\{i i d\}\). We use \((x)\{ 1\}\) to denote the sign for \(x\), with \((0)=1\). We use \((,)\) to denote the multivariate normal distribution of specified mean and covariance. We denote the all-ones and all-zeroes vectors of dimension \(d\) by \(_{d}\) and \(_{d}\). We use \(\) to denote the Euclidean (\(_{2}\)) norm. We use \(_{d}\) to denote the identity matrix on \(^{d}\). We use \((C)\) to denote the \(_{2}\) ball of radius \(C\), and for \(x^{d}\), \((x,C)\) is used to denote \(\{x^{}^{d} x^{}-x C\}\). For a set \(^{d}\), we let \(}():=_{x,x^{}}  x-x^{}\), and we let \(_{}(x)\) denote the Euclidean projection of \(x\) to \(\), i.e. \(_{x^{}} x^{}-x\), which exists and is unique when \(\) is compact. We use \(f_{}\) to denote the restriction of a function \(f\) to \(\), i.e.

\[f_{}(x)=f(x)&x\\ &x.\] (4)

For \(x^{d}\), we use \(_{C}(x)\) as shorthand for \(_{(C)}(x)\), i.e. \(_{C}(X)\) denotes the clipped vector \(x(,1)\). We say two datasets \(,^{}\) are _neighboring_ if they differ in one entry, and \(||=|^{}|\). We say \(x\) is an \(\)-approximate minimizer to \(f:\) if \(f(x)-_{x^{}}f(x^{})\). For two densities \(,\) on the same probability space, and \(>1\), we define the \(\)-Renyi divergence

\[D_{}():=(()^{}()).\]

For an event \(\) on a probability space clear from context, we let \(_{}\) denote the \(0\)-\(1\) indicator of \(\). We say \(f:\) is \(L\)-Lipschitz if \(|f(x)-f(x^{})| L x-x^{}\) for all \(x,x^{}\); if \(f\) is differentiable and convex, an equivalent characterization is \( f(x) L\) for all \(x\). We say \(f:\) is \(\)-strongly convex if \(f( x^{}+(1-)x) f(x^{})+(1-)f(x)-  x-x^{}^{2}\) for all \(x,x^{}\). We say differentiable \(f:\) is \(\)-smooth if for all \(x,x^{}\), \( f(x)- f(x^{}) x-x^{}\).

Differential privacy.We begin with a definition of standard differential privacy.

**Definition 1** (Differential privacy).: _Let \( 0\), \(\). We say a mechanism (randomized algorithm) \(:^{n}\) satisfies \((,)\)-differential privacy (alternatively, \(\) is \((,)\)-DP) if for any neighboring \(,^{}^{n}\), and any \(S\), \([() S]()[ (^{}) S]+\)._

_More generally, for random variables \(X,Y\) satisfying \([X S]()[Y S]+\) for all \(S\), we say that \(X,Y\) are \((,)\)-indistinguishable._

Throughout the paper, other notions of differential privacy will frequently be useful for our accounting of privacy loss in our algorithms. For example, we define the following variants of DP.

**Definition 2** (Renyi DP).: _Let \(>1\), \( 0\). We say a mechanism \(:^{n}\) satisfies \((,)\)-Renyi differential privacy (RDP) if for any neighboring \(,^{}^{n}\), \(D_{}(()\|(^{}))\)._

**Definition 3** (Cdp).: _Let \( 0\). We say a mechanism \(:^{n}\) satisfies \(\)-concentrated differential privacy (alternatively, \(\) satisfies \(\)-CDP) if for any neighboring \(,^{}^{n}\), and any \( 1\), \(D_{}(()\|(^{}))\)._

For an extended discussion of RDP and CDP and their properties, we refer the reader to . We summarize the main facts about these notions we use here.

**Lemma 1** ().: _RDP has the following properties._

1. _(Composition): Let_ \(_{1}:^{n}\) _satisfy_ \((,_{1})\)_-RDP and_ \(_{2}:^{n}^{}\) _satisfy_ \((,_{2})\)_-RDP for any input in_ \(\)_. Then the composition of_ \(_{2}\) _and_ \(_{1}\)_, i.e. the randomized algorithm which takes_ \(\) _to_ \(_{2}(,_{1}())\)_, satisfies_ \((,_{1}+_{2})\)_-RDP._
2. _(RDP to DP): If_ \(\) _satisfies_ \((,)\)_-RDP, it satisfies_ \((+,)\)_-DP for all_ \((0,1)\)3. _(Gaussian mechanism): Let_ \(f:^{n}^{d}\) _be an_ \(L\)_-sensitive randomized function for_ \(L 0\)_, i.e. for any neighboring_ \(\)_,_ \(^{}\)_, we have_ \(\|f()-f(^{})\| L\)_. Then for any_ \(>0\)_, the mechanism which outputs_ \(f()+\) _for_ \((_{d},^{2}_{d})\) _satisfies_ \(}{2^{2}}\)_-CDP._

Private SCO.Throughout the paper, we study the problem of private stochastic convex optimization (SCO) with heavy-tailed gradients. We first define the assumptions used in our algorithms.

**Assumption 1** (\(k\)-heavy-tailed distributions).: _Let \(^{d}\) be a compact, convex set. Let \(\) be a distribution over a sample space \(\), such that each \(s\) induces a continuously-differentiable, convex, \(L_{s}\)-Lipschitz loss function \(f(;s):\),6 where \(L_{s}:=_{x}\| f(x;s)\|\) is unknown. For \(k\) satisfying \(k 2\), we say \(\) satisfies the \(k\)-heavy tailed assumption if, for a sequence of monotonically nondecreasing \(\{G_{j}\}_{j[k]}\),7 we have \(_{s}[L_{s}^{j}] G_{j}^{j}<\) for all \(j[k]\)._

In Appendix B, we consider a variant of Assumption 1 where we have explicit access to upper bounds on the Lipschitz constants \(L_{s}\), formalized in Assumption 2. Our goal is to approximately optimize a population loss over sample functions satisfying Assumptions 1 or 2, formalized in the following.

**Definition 4** (\(k\)-heavy-tailed private SCO).: _In the \(k\)-heavy-tailed private SCO problem, \(^{d}\) is a compact, convex set with \(()=D\). Further, \(\) is a distribution over a sample space \(\) satisfying Assumption 1. Our goal is to design an algorithm which provides an approximate minimizer in expectation to the population loss, \(F_{}(x):=_{s}[f(x;s)]\), subject to satisfying differential privacy. We say such an algorithm queries \(N\) sample gradients if it queries \( f(x;s)\) for \(N\) different pairs \((x,s)\). If \(\) further satisfies Assumption 2, we call the corresponding problem the known Lipschitz \(k\)-heavy-tailed private SCO problem._

We first observe the following consequence of Assumption 1, deferring a proof to Appendix A.

**Lemma 2**.: _Let \(\) be a distribution over \(\) satisfying Assumption 1. Then \(F_{}\) is \(G_{1}\)-Lipschitz._

We require the following claim which bounds the bias of clipped heavy-tailed distributions.

**Fact 1** (, Lemma 3).: _Let \(k>1\) and \(X^{d}\) be a random vector with \([\|X\|^{k}] G^{k}\). Then,_

\[\|_{C}(X)-X\|[\|X\|\| \|{}_{\|{}_{\|{}_{\|{}_ {}}.}}..]}{(k-1)C^{k-1}}.\]

We also use the following standard claim on geometric aggregation.

**Fact 2** (, Claim 1).: _Let \(S:=\{x_{i}\}_{i[k]}^{d}\) have the property that for (unknown) \(z^{d}\), \(|\{i[k]\|x_{i}-z\| R\}| 0.51k\) for some \(R 0\). There is an algorithm \(\) which runs in time \(O(dk^{2})\) and outputs \(x S\) such that \(\|x-z\| 3R\)._

Finally, given a dataset \(^{*}\) of arbitrary size, and \( 0\), we use the following shorthand to denote the regularized empirical risk minimization (ERM) objective corresponding to the dataset:

\[F_{,}(x):=|}_{s }f(x;s)+\|x\|^{2}.\] (5)

When \(=0\), we simply denote the function above by \(F_{}(x):=|}_{s}f(x ;s)\).

## 3 Heavy-Tailed Private SCO

In this section, we obtain near-optimal algorithms for the problem in Definition 4 using a new _population-level localization_ framework, combined with geometric aggregation for boosting weak subproblem solvers to succeed with high probability (Fact 2). Our algorithm's main ingredient, in Section 3.1, is a clipped DP-SGD subroutine for privately minimizing a regularized ERM subproblem, under a condition on a randomly sampled dataset holding with constant probability. Next, in Section 3.2 we show that our algorithm from Section 3.1 returns points near the minimizer of a regularized loss function over the population, using generalization arguments. Finally, we develop our population-level localization scheme in Section 3.3, and combine it with our subproblem solver to give our overall method for heavy-tailed private SCO. Several proofs and a generalization to strongly convex functions (Corollary 3) are deferred to Appendix A.

### Strongly convex DP-ERM solver

We give a parameterized subroutine for minimizing a DP-ERM objective \(F_{,}(x)\) associated with a dataset \(\) and a regularization parameter \( 0\) (recalling the definition (5)). In this section only, for notational convenience we identify elements of \(\) with \([n]\) where \(n:=||\), so we will also write

\[F_{,}(x):=_{i[n]}f_{i}(x)+\|x\|^{2},\]

i.e. we let \(f_{i}():=f(;s)\) where \(s\) is the element identified with \(i[n]\). Our subroutine is a clipped DP-SGD algorithm (Algorithm 1), which only clips the heavy-tailed portion of \( F_{,}\) (i.e. the sample gradients), and leaves both the regularization and additive noise unchanged. The utility of Algorithm 1 is parameterized by the following function of the dataset:

\[b_{}:=_{x}\|_{i[n]} f _{i}(x)-_{i[n]}_{C}( f_{i}(x))\|.\] (6)

In other words, \(b_{}\) denotes the maximum bias incurred by the clipped gradient of \(F_{}\) when compared to the true gradient, over points in \(\); note the maximum is achieved as \(\) is compact.

We are now ready to state our algorithm, \(DPSGD}\), as Algorithm 1.

```
1Input: Dataset \(^{n}\), clip threshold \(C_{ 0}\), regularization \(_{ 0}\), step sizes \(\{_{t}\}_{t[T]}_{ 0}\), noise \(^{2}_{ 0}\), iteration count \(T\), radius \(r_{ 0}\), domain \((r)\) with \( 0_{d}\)\(x_{0}_{d}\)for\(0 t<T\)do\(_{t}(0_{d},^{2}_{d})\)\(_{t}_{i[n]}_{C}( f_{i}(x_{t}))\)\(x_{t+1}*{argmin}_{x_{r}}\{_{t} _{t}+_{t},x+}{2} \|x\|^{2}+\|x-x_{t}\|^{2}\}\)
2 end for Return:\((t+4)x_{t}}{_{0 t<T}(t+ 4)}\) ```

**Algorithm 1**\(DPSGD}(,C,,\{_{t}\}_{t [T]},^{2},T,r,)\)

We provide the following guarantee on \(DPSGD}\), by modifying an analysis of .

**Proposition 1**.: _Let \( 0\), and \(\) be the output of \(DPSGD}\) with \(_{t}\) for all \(0 t<T\), \(^{2}T}{n^{2}}\), and \(T(n,}{d})\). \(DPSGD}\) satisfies \(\)-CDP, and_

\[[F_{,}()-F_{,}(x^{*})] d}{ n^{2}}+}^{2}}{}+ }{n},x^{*}:=*{argmin}_{x}F_{ ,}(x).\]

For ease of use of Proposition 1, we now provide a simple bound on \(b_{}\) which holds with constant probability from a dataset drawn from a distribution satisfying Assumption 1.

**Lemma 3**.: _Let \(^{n}\), where \(\) is a distribution over \(\) satisfying Assumption 1. With probability at least \(\), denoting \(b_{}\) as in (26), we have_

\[b_{}^{k}}{(k-1)C^{k-1}}.\]

We therefore have the following corollary of Proposition 1 and Lemma 3.

**Corollary 1**.: _Let \(^{n}\), where \(\) is a distribution over \(\) satisfying Assumption 1, and let \(x_{,}^{}:=*{argmin}_{x}F_{ ,}(x)\), following (5). If we run \(\)-\(\)-\(\) with parameters in Proposition 1 and \(C G_{k}(}{32d})^{}\), \(\)-\(\)-\(\) is \(\)-CDP, and there is a universal constant \(C_{}\) such that with probability \(\) over the randomness of \(\) and \(\)-\(\)-\(\), \(\), the output of \(\)-\(\)-\(\), satisfies_

\[\|-x_{,}^{}\| C_{} (}{}(}{n})^{1- }+}).\]

\(\)-\(\)-\(\) _queries at most \((n^{2},}{d})\) sample gradients (using samples in \(\))._

Proof.: Condition on the conclusion of Lemma 3, which holds with probability \(\). Therefore, Markov's inequality shows that with probability at least \(\), after a union bound with Proposition 1,

\[\|-x_{,}^{}\|^{2}  F_{,}()-F_{,}(x_{ ,}^{})\] \[d}{ n^{2}}+^{2k}}{  C^{2(k-1)}}+}{n}^{2}}{} (})^{1-}+}{n},\]

where we used strong convexity in the first inequality, and plugged in our choice of \(C\) in the last. The conclusion follows by rearranging the above display, and using \(+b^{2}} a+b\) for \(a,b_{ 0}\). 

### Localizing regularized population loss minimizers

Here, we use generalization arguments from the SCO literature to show how that our algorithm \(\)-\(\)-\(\) from Section 3.1 acts as an oracle which, with constant probability, returns a point near the minimizer of a regularized population loss. We begin with a standard helper statement.

**Lemma 4**.: _Let \( 0\), let \(\) be a distribution over \(\) satisfying Assumption 1, let \(\) where \(^{d}\) is compact and convex, and let_

\[x_{,}^{}:=*{argmin}_{x}\{F_ {}(x)+\|x-\|^{2}\},F_{}(x):=_{s}[f(x;s)].\] (7)

_Then \(\|-x_{,}^{}\|}{}\)._

Next, we apply a result on generalization due to  to bound the expected distance between a restricted empirical regularized minimizer and the minimizer of the population variant in (7).

**Lemma 5**.: _Let \( 0\), let \(^{n}\) where \(\) is a distribution over \(\) satisfying Assumption 1, and let \(\) where \(^{d}\) is compact and convex. Following notation (4), (5), let_

\[y:=*{argmin}_{x}\{[F_{}] _{(,r)}(x)+\|x-\|^{2} \},r:=}{}\]

_and let \(x_{,}^{}\) be defined as in (7). Then with probability \( 0.95\) over the randomness of \(^{n}\),_

\[\|y-x_{,}^{}\|_{2}}{ }.\]

**Corollary 2**.: _Let \(^{n}\), where \(\) is a distribution over \(\) satisfying Assumption 1, and let \(\) where \(^{d}\) is compact and convex. Let \( 0\) and define \(x_{,}^{}\) as in (7). There is a \(\)-CDP algorithm \(\) which queries \((n^{2},}{d})\) sample gradients (using samples in \(\)). With probability \(0.55\) over the randomness of \(\) and \(\), \(\) returns \(\) satisfying, for a universal constant \(C_{}\),_

\[\|-x_{,}^{}\| C_{} (}{}(}{n})^{1- {1}{k}}+}{}).\]Proof.: Condition on the conclusion of Lemma 5 holding for our dataset, which loses \(0.05\) in the failure probability. Next, consider the guarantee of Corollary 1, when applied to the truncated and shifted functions, \((x;s)(,r)}}(x-;s)\), where \(r\) is set as in Lemma 5. It shows that with probability \(\), \(\|+-y\|=O(}{}(}{n})^{ 1-}+r}{})\), for the point \(\) returned by the algorithm, and \(y\) the exact minimizer of the empirical loss restricted to \((,r)\). Therefore, the conclusion follows by overloading \(+\), applying the triangle inequality with the conclusions of Corollary 1 and 2, and taking a union bound over their failure probabilities. 

### Population-level localization

In this section, we provide a generic population-level localization scheme for stochastic convex optimization, which may be of broader interest. Our localization scheme is largely patterned off of the analogous localization methods developed by , but directly argues about contraction to population-level regularized minimizers (as opposed to empirical minimizers), which makes it compatible with our framework in Section 3.1 and 3.2, specifically the guarantees of Corollary 2.

```
1Input: Initial point \(x_{0}\), distribution \(\) over samples in \(\), for \(,\) inducing a \(k\)-heavy-tailed DP-SCO problem as in Definition 4, with a population loss \(F_{}:=_{s}[f(;s)]\), \( 0\), \(I\)
2for\(i[I]\)do
3\(_{i} 32^{i}\)
4\(x_{i}\) any point satisfying \[\|x_{i}-x_{i}^{}\|}{_{i}},x_{i}^{}:=*{argmin}_{x }\{F_{}(x)+}{2}\|x-x_{i-1} \|^{2}\}\] (8)
5 end for Return:\(x_{I}\) ```

**Algorithm 2**\((x_{0},,,I)\)

We briefly discuss the role of the hyperparameters \(,\) in Algorithm 2 for clarity. The parameter \(\) scales with the error guarantee of our regularized ERM solver; in particular, it will be determined by the bound in Corollary 2. The parameter \(\) specifies an initial regularization amount that will later be tuned to trade off the terms in the following Proposition 2.

**Proposition 2**.: _Following notation of Algorithm 2, let \(x^{}:=*{argmin}_{x}F_{}(x)\). Then,_

\[F_{}(x_{I})-F_{}(x^{}) }{ 8^{I}}+}{4}+}{2}.\]

_In particular, choosing \(\) to optimize this bound, we have_

\[F_{}(x_{I})-F_{}(x^{}) 2D }{8^{I}}}+D.\]

Proof.: We denote \(x_{0}^{}:=x^{}\) throughout the proof. First, we expand

\[F_{}(x_{I})-F_{}(x_{0}^{}) =F_{}(x_{I})-F_{}(x_{I}^{})+F_{ }(x_{I}^{})-F_{}(x_{0}^{})\] \[=F_{}(x_{I})-F_{}(x_{I}^{})+_{i [I]}F_{}(x_{i}^{})-F_{}(x_{i-1}^{}).\]

Moreover, for each \(i[I]\), since \(x_{i}^{}\) minimizes \(F_{}(x)+}{2}\|x-x_{i-1}\|^{2}\),

\[F_{}(x_{i}^{}) F_{}(x_{i}^{})+}{2}\|x_{i}^{}-x_{i-1}\|^{2} F_{}(x_ {i-1}^{})+}{2}\|x_{i-1}^{}-x_{i-1}\|^{2}.\]Combining the above two displays, and using that \(F_{}\) is \(G_{1}\)-Lipschitz (Lemma 2), we have

\[F_{}(x_{I})-F_{}(x^{})  G_{1}\|x_{I}-x_{I}^{}\|+_{i[I]}}{2}\|x_{i-1}^{}-x_{i-1}\|^{2}\] \[}{ 8^{I}}+_{i[I-1]}16^{i}}{2 _{i}}+}{2}}{ 8^{I}}+}{4}+}{2},\]

where we used the diameter bound assumption \(()=D\), as in Definition 4. 

In particular, note that Corollary 2 shows that by using \(n\) samples from \(\) and a CDP budget of \(\), with constant probability, we can satisfy the requirement (8) with \( A^{i}=O(G_{k}(}{n})^{1-}+ }{})\). By plugging this guarantee into the aggregation subroutine in Fact 2, we have our SCO algorithm.

```
1Input: Regularization center \(\), regularization \(_{ 0}\), sample split parameter \(J\), privacy parameter \(_{ 0}\), samples \(\{s_{}\}_{[nJ]}\), distance bound \(R_{ 0}\)
2for\(j[J]\)do
3\(^{j}\{s_{}\}_{(j-1)n< jn}\) for all \(j[J]\)
4\(x^{j}\) result of Corollary 2 using \(^{j}\), on loss defined by \(,\) with privacy parameter \(\), i.e., \(x^{j}\) is a point satisfying, with probability \(0.55\), for a universal constant \(C_{}\), \[\|x^{j}-x_{,}^{}\| C_{}( }{}(}{n})^{1- }+}{})\]
5
6 end for
7\(x(\{x^{j}\}_{j[J]},R)\) (see Fact 2)
8Return:\(x\) ```

**Algorithm 3**Aggregate-\((,,J,,\{s_{}\}_{[nJ]},R)\)

**Theorem 1**.: _Consider an instance of \(k\)-heavy-tailed private SCO, following notation in Definition 4, let \(x^{}:=*{argmin}_{x}F_{}(x)\), and let \( 0\), \((0,1)\). Algorithm 2 using Algorithm 3 in Line 5 is a \(\)-CDP algorithm which draws \(^{n}\), queries \(C_{}(n^{2},}{d})\) sample gradients (using samples in \(\)) for a universal constant \(C_{}\), and outputs \(x\) satisfying, with probability \( 1-\),_

\[F_{}(x)-F_{}(x^{}) C_{}(G_{k}D (()}{n} )^{1-}+G_{2}D )}{n}}).\]

## 4 Conclusion

In this work, we consider the DP-SCO with heavy-tailed gradients. When the \(k\)-th moments of gradients are bounded, we propose the population-level localization framework and attain near-optimal excess loss \(G_{2}D)}{n}}+G_{k}D( ()}{n})^{1-}\) with probability at least \(1-\) and satisfy \(\)-CDP. We can achieve a tight rate for high-probability SCO under a bounded-variance gradient estimator parameterization by applying the population-level localization framework. Moreover, we improve this basic result under additional assumptions, including an optimal algorithm under a known-Lipschitz constant assumption, a near-linear time algorithm for smooth functions, and an optimal linear time algorithm for smooth generalized linear models, with interesting techniques adapted to each setting.

It leaves many intriguing open problems in this direction. For example, can we design near-linear time algorithms for non-smooth functions? Can the population-level localization framework be applied to solve other problems? Can we establish a high-probability lower bound or eliminate the additional logarithmic term if we are only concerned with the excess bound in expectation? Can we evaluate the algorithm's performance through numerical simulations or real-world datasets? We leave these questions for future research.