# Reward Finetuning for Faster and More Accurate Unsupervised Object Discovery

Katie Z Luo\({}^{ 1,}\)\({}^{}\) Zhenzhen Liu\({}^{ 1}\) Xiangyu Chen\({}^{ 1}\)\({}^{1}\) Yurong You\({}^{1}\) Sagie Benaim\({}^{2}\) Cheng Perng Phoo\({}^{1}\)

**Mark Campbell\({}^{1}\) Wen Sun\({}^{1}\) Bharath Hariharan\({}^{1}\) Kilian Q. Weinberger\({}^{1}\)**

\({}^{1}\)Cornell University, Ithaca, NY \({}^{2}\)The Hebrew University of Jerusalem

Denotes equal contribution.Correspondences could be directed to kzl6@cornell.edu

###### Abstract

Recent advances in machine learning have shown that Reinforcement Learning from Human Feedback (RLHF) can improve machine learning models and align them with human preferences. Although very successful for Large Language Models (LLMs), these advancements have not had a comparable impact in research for autonomous vehicles--where alignment with human expectations can be imperative. In this paper, we propose to adapt similar RL-based methods to unsupervised object discovery, i.e. learning to detect objects from LiDAR points without any training labels. Instead of labels, we use simple heuristics to mimic human feedback. More explicitly, we combine multiple heuristics into a simple reward function that positively correlates its score with bounding box accuracy, i.e., boxes containing objects are scored higher than those without. We start from the detector's own predictions to explore the space and reinforce boxes with high rewards through gradient updates. Empirically, we demonstrate that our approach is not only more accurate, but also orders of magnitudes faster to train compared to prior works on object discovery. Code is available at https://github.com/katieluo88/DRIFT.

## 1 Introduction

Self-driving cars need to accurately detect the moving objects around them in order to move safely. Most modern 3D object detectors rely on supervised training from 3D bounding box labels. However, these 3D bounding box labels are hard to acquire from human annotation. Furthermore, this supervised approach relies on a pre-decided vocabulary of classes, which can cause problems when the car encounters novel objects that were never annotated.

Our prior work, MODEST , introduced the first method to train 3D detectors without labeled data. In that work, we point out that instead of specifying millions of labels, one can succinctly describe _heuristics_ for what a good detector output should look like. For example, one can specify that detector boxes should mostly enclose transient foreground points rather than background ones; they should roughly be of an appropriate size; their sides should be aligned with the LiDAR points; their bottom should touch the ground, etc. Although such heuristics are great for _scoring_ a set of boxes proposed by a detector, training a detector on them is hard for two reasons: First, these heuristics are often non-linear, non-differentiable functions of the detector parameters (for example, a slight shift of the box can cause all foreground points to fall off.) Second, existing object detection pipelines use carefully designed training objectives that heavily rely on labeled boxes, that are difficult to modify (for example, PointRCNN  infers point labels from box labels and uses these for training). For these reasons, MODEST had to utilize an admittedly slow self-training pipeline to incrementally incorporate common-sense heuristics.

In this paper, we propose a new reward ranking based framework that utilizes these common-sense heuristics directly as a reward signal. Our method relies on finetuning with reward ranking [31; 27; 13; 33], where given an initialized object detector, we finetune it for a predefined set of desirable detection properties. This bypasses the need to encode heuristics as differentiable loss functions and avoids the need to hand-engineer training paradigms for each kind of object detector. Recent success with reinforcement learning from human feedback (RLHF) has proven effective in improving machine learning models --in particular, large language models (LLMs)-- and aligning them with human preferences [31; 27]. However, these advancements have not been applicable to detection-based vision models that are trained with per-instance regression and are difficult to view under a probabilistic framework. To address this challenge, we utilize insights from reward ranked finetuning , a non-probabilistic paradigm designed for finetuning of LLMs, which inspired us to develop a similar framework for object discovery.

We refer to our method as _Discovery from Reward-Incentivized Finetuning (DRIFT)_. DRIFT does not require labels, and instead uses the Persistency Prior (PP) score [55; 3] as a heuristic to identify dynamic foreground points based on historical traversals. These foreground points give rise to rough (and noisy) label estimates , which we use to pre-train our detector. The resulting detector performs poorly but suffices to propose many boxes of roughly the right sizes that we can use for exploration. To facilitate reward ranked finetuning, we first propose a reward function to score boxes. Ideally, only boxes that tightly contain objects (e.g. a car) should yield high rewards. We achieve this by combining several simple heuristics (e.g. high ratio of foreground points) and assuming some rough knowledge about the object dimensions. During each iteration of training, DRIFT performs the following steps: 1. the object detector proposes many boxes in a given point cloud scene; 2. the boxes are "jittered" through random perturbations (as a means of exploration); 3. the boxes are scored according to the reward function; 4. the top-\(k\%\) non-overlapping boxes are kept as pseudo-labels for gradient updates.

We evaluate DRIFT on two large, real-world datasets [12; 24] and show that we significantly outperform prior self-training methods both in efficiency and generalizability. Experimental results demonstrate that using reward ranked finetuning for object discovery under our framework can quickly converge to a solution that is on par with out-of-domain supervised object detectors within a few training epochs, suggesting that DRIFT may point towards a more general unsupervised learning formulation for object detectors in an in-the-wild setting.

## 2 Related Works

**3D Object Detection.** 3D object detection models usually take in LiDAR point clouds or multi-view images as input and aim to produce tight bounding boxes that describe nearby objects [9; 52; 26; 39; 40; 53; 34]. Existing methods generally assume the supervised setting, in which the detector is trained with human-annotated bounding boxes. However, annotated data are often expensive to obtain

Figure 1: **Detection performance on Lyft test data as a function of training epochs. DRIFT demonstrates significantly stronger performance and faster learning. With only 9 hours of training, it outperforms both baselines that have been trained for days.**

and limited in quantity. Furthermore, in tasks such as self-driving, environments can have highly varied conditions, and detectors with supervised training often require adaptation with additional labels from the new environment .

**Unsupervised Object Discovery**. The unsupervised object discovery task aims to identify and localize salient objects without learning from labels. Most existing works perform discovery from 2D images [8; 14; 45; 36; 2; 43; 48] or depth camera frames [21; 23; 17; 25; 20; 1]. Discovery from 3D LiDAR point clouds is underexplored.  performs joint unsupervised 2D detection and 3D instance segmentation from sequential point clouds and images based on temporal, motion and correspondence cues. MODEST  pioneers in performing label-free 3D object detection. It exploits high-level common sense properties from unlabeled data and bootstraps a dynamic object detector via repeated self-training. Despite promising performance, it requires excessive training time, which makes it difficult for practical use and development.

**Reward Fine-Tuning for Model Alignment**. Recently, foundation models [6; 29; 11; 44; 35; 37] have been shown to achieve strong performance in diverse tasks [5; 50], but sometimes produce outputs that do not align with human values [18; 30; 10]. A line of research aims to improve model alignment under the paradigm of Reinforcement Learning with Human Feedback (RLHF). Some pioneering works [41; 31; 33] learn a reward model and train foundation models with Proximal Policy Optimization (PPO) , but PPO is often expensive and unstable to train, and more importantly, requires a probabilistic output on the action space. This makes it hard to use for the object detection setting, which primarily uses regression-based losses. Reward ranked finetuning [27; 13] is a simplified alternative paradigm. It samples from a foundation model itself, filters generations using the reward model, and conducts supervised finetuning with the filtered generations.

## 3 Discovery from Reward-Incentivized Finetuning

Our framework, DRIFT, is inspired by the recent success of reward ranked finetuning methods for improving model alignment in the NLP community [13; 27]. We show that a similar approach can be adapted for 3D object discovery.

**Problem Setup.** We wish to obtain a dynamic object detection model on LiDAR data, i.e., a model to detect mobile objects in the LiDAR point clouds, _without human annotations_. Let \(^{N 3}\) denote a \(N\)-point 3D point cloud captured by LiDAR from which we wish to discover objects. We assume inputs of _unlabeled_ point clouds collected by a car equipped with synchronized sensors including LiDAR (for point clouds) and GPS/INS (for accurate position and orientation). Since no annotation is involved, such a dataset is easy to acquire from daily driving routines; we additionally assume it to cover some locations with _multiple_ scans at different times for computation of PP-score.

**Dynamic Point Proposals.** DRIFT leverages prior works that use unsupervised point clouds to extract foreground-background segmentation proposals. While many works [22; 3] have promising dynamic foreground segmentation results, in this work we rely on point _Persistency Prior score_ (PP-score)  for its accuracy and leave the extension of other proposal methods to future work. For the purpose of this research, dynamic foreground points constitute LiDAR points reflecting off traffic participants (e.g. cars, bicyclists, pedestrians).

Using historical LiDAR sweeps collected at nearby locations of our point cloud \(\), the PP-score [3; 55]\(()[0,1]^{N}\) can provide an informative estimate on the per-point persistence, i.e., whether a point belongs to persistent background or not. The PP-score is defined as the normalized entropy over past point densities, based on the assumption that background space such as ground, trees, and buildings tend to exhibit consistent point densities across different LiDAR scans (high entropy), whereas points associated with mobile objects exhibit high density only if an object is present (low entropy).

### Rewarding "Good" Dynamic Boxes

We first establish a reward function that evaluates the quality of a set of bounding boxes for dynamic objects in a scene. We denote a set of \(M\) dynamic objects bounding boxes as \(=\{_{1},,_{M}\}\), where each bounding box \(_{i}\) is represented as an upright box with parameters \((x_{i},y_{i},z_{i},w_{i},l_{i},h_{i},_{i})\), defining the box's center, width, length, height, and z-rotation, respectively. The scoring function \(R\) scores the validity of the bounding boxes, given the observed point cloud \(\). In practice, a reward function that is positively correlated with IoU should suffice. We present our proposed reward function which aims to capture only dynamic points, filter nonsensical boxes, enforce correct size, and encourage proper box alignment to the captured dynamic points.

**Shape Prior Reward.** We enforce a box to not deviate significantly from a set of prototype sizes \(=\{(_{1},_{1},_{1}),,(_{C}, _{C},_{C})\}\) (Fig. 2 left). We assume the shape prior distribution is a mixture of \(C\) isotropic Gaussians with mixture weights \(_{i}\), diagonal variances \(_{i}\), and corresponding means as \((_{i},_{i},_{i})\). These low-level statistics may be acquired directly from the dataset, or from vehicle specs and sales data available online . In practice, we scale the mixtures such that the probabilities at the Gaussian means are equal for stability reasons. With this, the shape prior reward for box \(\) is computed as:

\[R_{}()=P_{}().\] (1)

**Alignment Reward.** Due to the nature of LiDAR sensing, the points will mostly fall on the lateral surfaces of an object. Therefore, a well-formed box should have dynamic points approximately close to the _boundary_ of a box (Fig. 2 middle). As  shows, PP-score allows for easy separation of dynamic and persistent background points. Let \(_{}\) denote the set of dynamic points, and let \(_{}\) be that of background points. In practice, since the PP-score is an approximation of ground-truth persistence, we define \(_{}=\{|()<0.6\}\) and \(_{}=\{|() 0.9\}\).

Given a box \(\), we denote all points within and close to the box as \(()\). Only points within \(()\) contribute to the reward of \(\). In practice, we let \(()\) consist of all points within a \( 2\) scaled up version of \(\) (with identical center and rotation). To score a box \(\), we design a reward function that identifies how "typical" the dynamic points within \(()\) are. For each dynamic point \(()_{}\), we compute the scaling factor \(s_{,}\) required so that the rescaled box touches \(\) with one of its sides; i.e., if a point is inside the box, the box would have to be scaled down (\(s_{,}<1\)) to touch the point, if the point is outside it must be scaled up (\(s_{,}>1\)). We assume that \(s_{,}\) roughly follows a Gaussian distribution centered near the box boundary, and visualize the actual distribution in Fig. 3.

We define our reward as the likelihood under this Gaussian distribution over scaling parameters. We approximate it as a Gaussian with hyper-parameters mean \(_{}\) and a variance \(_{}\). Our reward is the product of the probability of each point in \(()\):

\[R_{}()=_{ o()_{}} (s_{,}|_{},_{}).\] (2)

**Common Sense Heuristics and Filtering.** Lastly, a proper bounding box must capture the dynamic points, and avoid capturing the background points (Fig. 2, right). This heuristic can be encoded by a

Figure 3: **Distribution of dynamic points near ground truth bounding boxes.** We observe that dynamic points near bounding boxes fall in an approximate Gaussian distribution centered near box edges (\(s_{,} 0.8\)).

Figure 2: **Illustration of the reward components.** The reward encourages boxes that have proper shape and alignment, and capture more dynamic points and few background points.

simple weighted point count for each bounding box \(\):

\[R_{}()=_{}|_{} ()|-_{}|_{} ()|.\]

Intuitively, it assigns a reward in proportion to the number of dynamic points captured by the box, and a penalty in proportion to the number of background points captured.

Furthermore, boxes violating common sense should be assigned a low reward. We filter boxes that are too high up or too low from the ground, including those with too few dynamic points, or are too small or too large by directly assigning a reward of 0. In practice, we filter boxes that contain fewer than 4 dynamic points, or that have more than 80% persistent points, similar to .

In summary, the reward function is designed to be

\[R()=_{} R_{}()+ _{} R_{}()+R_{}()& \\ 0&\] (3)

### Exploration Strategy for Improved Discovery

We assume a simple exploration strategy for identifying good box proposals. Given a set of current object proposals given by the detector, we locally perturb the boxes in the output space:

\[_{}(_{0}, ),\] (4)

where \(_{}\) is the set of explored boxes, perturbed from the model proposals \(_{0}\). For each box \(_{0}\), a set of explored boxes are sampled according to a standard Gaussian noise along the position and size dimensions and uniform noise for orientation:

\[_{}^{}(_{0}^{},^{2}),\ _{}^{}(_{0}^{}, ^{2}),\ _{}^{}(_{0}^{}-, _{0}^{}+).\] (5)

Furthermore, to encourage proposals of boxes in foreground regions, we take inspiration from [56; 40] and re-use PP-score as _point-level_ semantic segmentation (foreground vs background) labels, with which the detector is encouraged to propose boxes at points that have low PP-scores (i.e.are likely to be foreground points). Following , for each point \(}\) with prediction \(}}\), we assign its target classification label \(}\) as:

\[}=&(})<_{L}}}=,\\ &\] (6)

In effect, this encourages all non-persistent points (i.e., low \((})\)) to propose boxes near dynamic regions for better exploration.

### Reward-Incentivized Finetuning

The reward function \(R\) allows us to quickly evaluate proposed bounding boxes \(\) and the task of 3D object discovery could be reduced to an optimization problem on the total reward in box set space:

\[^{*}=*{arg\,max}_{}_{}R(),\] (7)

where the sum is taken over the boxes in the set \(\). Although a direct optimization for \(^{*}\) is not plausible due to the non-polynomial search space and discontinuity in \(R\), \(R\) can serve as effective guidance to facilitate model finetuning. The underlying intuition is similar to curriculum learning [4; 28; 46]: the object detection model takes small steps to improve from its current predictions towards \(^{*}\) by following the direction provided by the maximum \(R\) direction in a local space.

As illustrated in Alg. 1, in each training iteration, we first let the object detector perform inference on a point cloud \(\) and propose a set of dynamic objects \(_{0}\) in the scene. To explore directionsof improvement with the non-differentiable reward function \(R\), we sample \(n\) boxes from \(_{0}\) (with replacement) and add an _i.i.d._ Gaussian noise on their location and size, and an uniform noise on orientation following Eq. 4. These sampled boxes are then ranked by the reward function \(R\), in which the top \(k\) non-overlapping boxes are selected by Non-Maximum Suppression (NMS) as training targets to finetune the object detector. Note that since DRIFT treats the model training/inference procedures as black boxes, it can be applied to any 3D object detection model.

In practice, it is observed that neural networks can acquire task knowledge from imperfect demonstrations [16; 51; 32]. MODEST  pre-trained the 3D object detector on noisy seed labels produced by DBSCAN  clustering on spatial and PP-score. We follow  and initialize our 3D object detector a model trained with discovered seed labels.

## 4 Experiments

**Datasets.** We experimented with two different datasets: Lyft Level 5 Perception dataset  and Ithaca-365 dataset . To the best of our knowledge, these are the two publicly available datasets that contain multiple traversals of multiple locations with accurate 6-DoF localization and 3D bounding box labels for traffic participants.

In the Lyft dataset, we experiment with the same split provided by , where the train set and test set are geographically separated. It consists of 11,873 train scenes and 4,901 test scenes. For the Ithaca365 dataset, we experimented with the full dataset which consists of 57,107 scenes for training and 1,644 for testing. For both datasets, we do not use any human-annotated labels in training. To show the generalizability of our method, we conduct the development on the Lyft dataset, i.e., all the hyperparameters of our approach are finalized through experiments on Lyft, and we use the exact same set of hyperparameters for all experiments in Ithaca365.

**Evaluation.** Following , we combine all traffic participants to a single mobile object class and evaluate the detector's performance on this class. Note that the labels are not used during training but solely for evaluation. For Lyft, we report the mean average precision (mAP) of the detector with the intersection over union (IoU) thresholds at 0.5 and 0.7 in bird-eye-view perspective. Note that mAP at 0.7 IoU threshold is a stricter and harder metric and was not evaluated in , and we include it to emphasize the effectiveness of our method. For Ithaca365, we adopt metrics similar to those in : we evaluate mean average precision (mAP) for dynamic objects under \(\{0.5,1,2,4\}\)m thresholds that determine the match between detection and ground truth; we also compute 3 types of true positive metrics (TP metrics), including ATE, ASE and AOE for measuring translation, scale and orientation errors. These TP metrics are computed under a match distance threshold of 2m; additionally, we also compute a distance-based breakdown (0-30m, 30-50m, 50-80m) for these metrics.

**Implementation.** We use PointRCNN  as our default architecture and we use the implementation provided by OpenPCDet . We train DRIFT with 120 epochs in Lyft and 30 epochs in Ithaca365 as the default setting, and observe that the performance generally improves with more training epochs (Fig. 1). We use \(_{shape}=1\), \(_{align}=1\), \(_{dyn}=0.001\) and \(_{bg}=0.001\). We use \(_{scale}=0.8\) and \(_{scale}=0.2\) for the alignment reward. We define the shape priors based on four typical types of traffic participants: Car, Pedestrian, Cyclist, and Truck. Specifically, we use the mean and standard deviation of box sizes of each class in the Lyft dataset, but we show that they generalize well to other domains like Ithaca365 and are not sensitive (Tab. 2) The exact prototype sizes \(\) and other implementation details can be found in the supplementary materials.

**Baselines.** To the best of our knowledge, MODEST  is the only prior work on this problem and we compare our method DRIFT against it with various variants of MODEST: (1) No Finetuning: the model trained with seed labels from PP-score without repeated self-training (MODEST (R0)) in ; (2) Self-Training (\(i\) ep): the model initialized with (1) and self-trained with \(i\) epochs without PP-score filtering; (3) MODEST (\(i\) ep): the model initialized with (1) and self-trained with \(i\) epochs with PP-score filtering (full MODEST model). For self-training in (2) and (3), we adopt 60 epochs for each self-training round in the Lyft dataset (same as that in ) and 30 epochs for the Ithaca365 dataset. To ensure a fair comparison, DRIFT is also initialized from (1) and use the same detector configurations as the baselines. Following , we also compare with the supervised counterparts trained with human-annotated labels from the same dataset (Lyft or Ithaca365) and from another out-of-domain dataset (KITTI).

**Dynamic Object Detection Results.** We report the performance of DRIFT and baseline detectors on Lyft in Tab. 1, and show the performance over the training epochs in Fig. 1. We report the performance on Ithaca365 in Tab. 2. Notably, DRIFT demonstrates significantly faster learning and strong performance. It provides more than 10\(\) speedup as compared to the baselines. On Lyft, DRIFT's performance at 60 epochs already surpasses the performance of both baselines at 600 epochs (10 self-training rounds) and approaches the performance of the out-of-domain supervised detector trained on KITTI . On Ithaca365, its performance at 30 epochs significantly surpasses both baselines trained at 300 epochs. It even outperforms the out-of-domain supervised detector trained on KITTI in mAP. Observe that the self-training performance starts collapsing with more rounds of self-training, and does not continue to improve.

Fig. 4 visualizes the detection on two scenes. Ground truth boxes are colored in green, predictions from the detector without fine-tuning are in yellow, and predictions from DRIFT are in red. We observe that the detector without fine-tuning occasionally produces false positive predictions, produces boxes with incorrect sizes, or misses moving objects, while DRIFT produces accurate detection.

**Rewards ablations.** We report the average reward per box for ground truth boxes, random boxes, and predicted boxes from different detectors in Tab. 3. The ground truth boxes have the highest rewards on average, while the random boxes have the lowest. This indicates that the reward reasonably reflects

    &  &  \\   & 0-30 & 30-50 & 50-80 & 0-80 & 0-30 & 30-50 & 50-80 & 0-80 \\  No Finetuning & 44.1 & 21.1 & 1.2 & 23.9 & 24.4 & 6.0 & 0.1 & 10.5 \\  Self-Train. (60 ep) & 50.0 & 29.0 & 3.4 & 28.6 & 32.5 & 10.0 & 0.3 & 14.0 \\ Self-Train. (600 ep) & 56.7 & 41.1 & 9.1 & 37.2 & 35.1 & 20.7 & 1.6 & 19.9 \\ MODEST (60 ep) & 49.6 & 29.7 & 3.4 & 28.8 & 31.3 & 10.2 & 0.3 & 14.4 \\ MODEST (600 ep) & 56.4 & **45.4** & 11.3 & 39.6 & 33.6 & 18.6 & 1.4 & 18.8 \\ DRIFT (30 ep) & 60.1 & 40.2 & 9.1 & 38.3 & 39.0 & 24.2 & 3.6 & 23.1 \\ DRIFT (60 ep) & 60.3 & 43.8 & 14.6 & 41.8 & 42.0 & 29.2 & 5.8 & 26.7 \\ DRIFT (120 ep) & **61.4** & 45.1 & **21.7** & **45.3** & **42.7** & **31.7** & **9.9** & **29.6** \\  Sup. on KITTI & 71.9 & 49.8 & 22.2 & 49.9 & 47.0 & 26.2 & 6.4 & 27.9 \\ Sup. on Lyft & 76.9 & 60.2 & 37.5 & 60.4 & 62.7 & 50.9 & 28.2 & 48.5 \\   

Table 1: **Detection performance on Lyft. DRIFT outperforms both baselines with 10% training time, and approaches the performance of the out-of-domain supervised detector trained on KITTI. Please refer to the setup of Sec. 4 for the metrics.**

    &  &  \\   & 0-30 & 30-50 & 50-80 & 0-80 & ATE & ASE & AOE \\  No Finetuning & 18.7 & 4.8 & 0.0 & 7.7 & 1.17 & 0.60 & 1.64 \\  Self-Train. (30 ep) & 25.9 & 9.2 & 1.2 & 12.4 & 1.08 & 0.62 & 1.57 \\ Self-Train. (300 ep) & 16.3 & 3.6 & 1.8 & 6.8 & 1.19 & 0.74 & 1.57 \\ MODEST (30 ep) & 14.6 & 0.7 & 0.0 & 3.7 & 0.83 & 0.52 & 1.53 \\ MODEST (300 ep) & 27.5 & 26.3 & 21.0 & 27.1 & 1.06 & 0.67 & **1.09** \\ DRIFT (15 ep) & 39.1 & 24.3 & 17.7 & 28.0 & 0.73 & **0.33** & 1.23 \\ DRIFT (30 ep) & **47.1** & **31.2** & **22.9** & **35.1** & **0.49** & 0.35 & 1.20 \\  Sup. on KITTI & 59.8 & 28.3 & 4.0 & 32.0 & 0.26 & 0.22 & 0.46 \\ Sup. on Ithaca365 & 75.7 & 48.3 & 22.6 & 51.5 & 0.18 & 0.13 & 0.33 \\   

Table 2: **Detection performance on Ithaca365. We observe DRIFT outperforms both baselines with significantly less training time. Please refer to the setup of Sec. 4 for the metrics.**the quality of the bounding box. And we observe the boxes predicted by DRIFT have higher rewards than those predicted by the baseline detectors.

Ablation study on the components of our reward is presented in Tab. 4, and visualization is shown in Fig. 5. Detection performance significantly drops when we remove one or more of the components. For example, when only common sense filtering is used, the detector just predicts boxes around foreground points. Without the shape prior reward, the detector predicts boxes with incorrect sizes.

Ablations Tab. 5 and Tab. 6 present the sensitivity analysis of the choices of \(_{}\) and \(_{}\). DRIFT achieves stable performance across reasonable choices of \(_{scale}\) and \(_{}\), showing the robustness of our method.

**Exploration.** We study the necessity of the exploration component and the effect of incorporating other sources for box sampling. In Tab. 7, we compare no exploration to: (1) sampling 200 boxes from box predictions, (2) sampling 100 from proposals near dynamic points and 100 from predictions, and (3) sampling 100 from seed labels and 100 from predictions. Observe that the exploration component is crucial for our method; by performing local exploration instead of simply updating from its own predictions, DRIFT avoids confirmation bias and ensures that labels improve over what it predicts. Furthermore, results show that sampling from the box predictions is sufficient for obtaining good performance; other sources do not provide obvious benefits.

We also explore the effects of modifying the exploration strategy. Tab. 8 compares the detector performance of using sample size of 50, 100 and 200, and noise scale \(\) (i.e.variation) of 0.3 vs. 0.6. Each detector is trained for 30 epochs. At noise scale 0.3, increasing the sample size from 50 to 200 significantly improves the detection performance. Using noise scale 0.6 significantly reduces the detection performance, indicating that smaller noise may be preferable.

**Filtering Budget of the Ranked Boxes.** We study the effect of the choice of top \(k\%\) for filtering boxes by reward ranking. Tab. 9 presents the detection performance with top 55%, 65%, 75% and 85%. DRIFT is robust to the choice of \(k\), with slightly decreased performance when \(k\) is too high.

   \)} &  \\   & IoU 0.5 & IoU 0.7 \\ 
0.8 & 38.3 & 23.1 \\
0.9 & 38.1 & 23.4 \\   

Table 6: Ablation on the alignment rewardâ€™s \(_{scale}\). We report the mAP (0-80m) on the Lyft dataset. We show the detection performance is not sensitive to the variance.

Figure 4: **Visualization of detections.** Qualitative results on two scenes from Lyft  and Ithaca365  datasets. Ground truth boxes are labeled with green in the LiDAR figures and predictions without fine-tuning and DRIFT are in yellow and red, respectively. We observe DRIFT learns to produce accurate detection with correct shape and localization.

[MISSING_PAGE_FAIL:9]

based framework is extremely flexible, and could easily be extended to other data modalities. For example, one could use image features to help identify objects inside of box proposals. Although in supervised settings image features have typically not added much in to the higher resolution LiDAR point clouds, in our unsupervised setting it is certainly possible that pixel information can help disambiguate objects from background. Further, we plan to explore the use of reward fine-tuning for other vision applications beyond object discovery.