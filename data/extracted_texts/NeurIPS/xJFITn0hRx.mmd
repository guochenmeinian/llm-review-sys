# Auto-PINN: Understanding and Optimizing Physics-Informed Neural Architecture

Yicheng Wang

Texas A&M University

wangyc@tamu.edu

&Xiaotian Han

Texas A&M University

han@tamu.edu

&Chia-Yuan Chang

Texas A&M University

cychang@tamu.edu

&Daochen Zha

Rice University

daochen.zha@rice.edu

&Ulasses Braga-Neto

Texas A&M University

ulisses@tamu.edu

&Xia Hu

Rice University

xia.hu@rice.edu

###### Abstract

Physics-Informed Neural Networks (PINNs) are revolutionizing science and engineering practices by harnessing the power of deep learning for scientific computation. The neural architecture's hyperparameters significantly impact the efficiency and accuracy of the PINN solver. However, optimizing these hyperparameters remains an open and challenging problem because of the large search space and the difficulty in identifying a suitable search objective for PDEs. In this paper, we propose Auto-PINN, the first systematic, automated hyperparameter optimization approach for PINNs, which employs Neural Architecture Search (NAS) techniques for PINN design. Auto-PINN avoids manually or exhaustively searching the hyperparameter space associated with PINNs. A comprehensive set of pre-experiments, using standard PDE benchmarks, enables us to probe the structure-performance relationship in PINNs. We discover that the different hyperparameters can be decoupled and that the training loss function of PINNs serves as an effective search objective. Comparison experiments with baseline methods demonstrate that Auto-PINN produces neural architectures with superior stability and accuracy over alternative baselines.

## 1 Introduction

Physics-informed neural networks (PINNs)  are promising partial differential equations (PDE) solvers that integrate machine learning with physical laws. Benefiting from the strong expressive power of deep neural networks, PINNs are widely adopted to solve various real-world problems, such as fluid mechanics[6; 9; 23], material science [8; 24; 25] and biomedical engineering [22; 10; 14]. PINNs do not require the time-consuming construction of elaborate grids, and can therefore be applied more easily to irregular and high-dimensional domains than traditional PDE solvers can.

The structures of PINNs are usually simple multilayer perceptrons (MLPs). Similar to other deep learning tasks, the neural architecture configurations of MLP networks, such as depths/widths, and activation functions, have a great effect on the performance of PINNs. However, there is little research on this problem. For instance, Raissi et al.  found that increasing the width and depth of PINNs can improve the predictive accuracy, but their experiments are limited to a single PDE problem within a very small search space. While the Tanh activation function is the default option for PINNs, some studies [2; 16] report that Sigmoid or Swish  functions are more effective in some cases. However, they did not reach a conclusion about which activation function is preferred for various PDE problems. Therefore, further investigation is required to understand the relationship between the PINN architectures and their performances. Moreover, there are a number of importanthyperparameters for training PINNs, such as the learning rate, the number of training epochs, and the choices of optimizers. Manually tuning the architecture and hyperparameters is tedious and laborious. Therefore, we are motivated to study the following research question: _Can we automate the process of architecture and hyperparameters selection to improve the performance of PINNs?_

Despite the recent progress of automated hyperparameter tuning [1; 3] and neural architecture search (NAS) [13; 19; 5], automating the neural architecture design of PINNs remains an open and challenging problem. First, the search space that includes both discrete and continuous hyperparameters is extremely large. Existing hyperparameter optimization methods usually search the whole hyperparameter space, which can be inefficient. Second, the search objective for PINNs is unclear. Unlike other tasks that can naturally use the performance metric (e.g., accuracy) as the search objective, many PDEs may have no exact solutions such that the error values are not available. Therefore, we have to identify an alternative search objective.

To this end, we first conduct a comprehensive set of benchmarking pre-experiments to understand the search space by studying the relationship between each hyperparameter and the performance. We make two key observations from the experiments. _First_, we find that some design choices play a dominant role in the performance. For example, there is often a dominant activation function working better for each PDE. This motivates us to reduce the search space by decoupling it in a certain order. For instance, we can determine the best activation function with a small number of search trials, and then fix the activation function and focus on the search of the other hyperparameters. We observe similar phenomenons for other hyperparameters such as the changing point, depth, and width, which enables us to decouple them step by step. _Second_, we discover that the loss values are highly correlated with the errors. This makes the loss value a desirable search objective since it can be naturally obtained during the search for all the PDEs.

Based on the above observations, we propose Auto-PINN, the first automated machine learning framework to optimize the neural architecture and the hyperparameters of PINNs. Auto-PINN adopts a step-by-step decoupling strategy for search. Specifically, we search one hyperparameter at each step with the others fixed to one or a few sets of options. This strategy decreases the scale of the search space drastically. We perform extensive experiments to evaluate Auto-PINN on seven PDE benchmarks with different training data sampling methods. The quantitative comparison results show that Auto-PINN outperforms the other optimization strategies on both accuracy and stability with fewer search trials.

We summarize our main contributions as follows:

* We conduct a comprehensive set of benchmarking pre-experiments on the hyperparameter-performance relationships of PINNs. Our observations suggest we can significantly reduce the search space by decoupling the search of different hyperparameters. We also identify the loss value of PINNs as a desirable search objective.
* We propose Auto-PINN, the first automated neural architecture and hyperparameter optimization approach for PINNs. The decoupling strategy can substantially decrease the search complexity.
* We evaluate Auto-PINN on a series of PDE benchmarks and compare it with other baseline methods. The results suggest that Auto-PINN can consistently search PINN architectures that display good accuracy in different PDE problems, which outperforms other search algorithms,

## 2 Preliminaries

### Benchmarking PDEs.

We select a set of standard PDE benchmarks for the experiments. We conducted pre-experiments on four representative PDEs. They are two diffusion (heat) equations, a wave equation and a Burgers' equation which are commonly used in PINN research . We will refer to them as **Heat_0**, **Heat_1**, **Burgers**, and **Wave** in the following sections for conciseness. These PDEs include different kinds of differential operators and boundary/initial conditions, which are capable of illustrating regular rules for PINNs in the pre-experiments. Moreover, we design different data sampling schemes for each PDE to improve the credibility of the results. These PDEs are also involved in the formal comparison experiments, along with another three PDEs, which are two advection equations (**Advection_0** and **Advection_1**) and a reaction equation (**Reaction**). The details of these PDEs and the data sampling methods are shown in the Appendix A.

### Search Space

In this section, we define the search space for the PINN architectures. Here we consider the following variables.

* _Width_ and _Depth_. For an MLP-structured PINN, the _depth_ is the number of hidden layers, and the _width_ means the number of neurons in each hidden layer. We set the _width_ ranging in \(\) with the step of \(8\) (only for **Heat_0**) or in \(\) with the step of \(4\) (for other PDEs). The _depth_ ranges in \(\) with the step of \(1\).
* _Activation Function_. The _activation functions_ in PINNs determine different non-linear elements in the networks. We provide four options: Tanh, Sigmoid, ReLU, and Swish . We leave the definitions of these activation functions in Appendix B.
* _Changing Point_. According to , PINNs can reach their best performance by training with an Adam optimizer in the first stage to get closer to the minimum and then switching to an L-BFGS second-order optimizer . We need to decide the timing of that change. Therefore, we introduce a hyperparameter named _Changing Point_ as a float number ranging from 0 to 1. This _changing point_ indicates the proportion of the epochs using Adam to the total training epochs. For example, if the training epoch number is set to 10000 with a 0.4 _Changing Point_, that means the PINN will be trained with the Adam optimizer for \(10000 0.4=4000\) epochs, followed by \(6000\)-epoch L-BFGS training. However, it makes little sense to search on a precise grid, so we only consider five discrete options \(\{0.1,0.2,0.3,0.4,0.5\}\).

Initially, we do not include _learning rates_ and the _training epochs_ in the search. However, we will show results on those two training hyperparameters in Section 5.3, which indicate that they have a small effect on the final architecture search results.

## 3 Pre-Experiments and Observations

As we mentioned previously, neural architecture optimization for PINNs is still an under-explored problem. Therefore, we should first explore general rules for the hyperparameters of PINNs. Different from other deep learning tasks, the training strategy and the physical constraints of PINNs are unique. Therefore, we do not simply apply a hyperparameter search algorithm, but first do pre-experiments to understand the behavior of PINNs. For all experiments in this section, the PINNs are trained with \(10000\) training epochs, and the learning rate is set to \(1e^{-5}\).

We first study the relationship between structure and performance of the PINNs. Throughout this paper, the main figure of merit to measure accuracy is the relative \(L_{2}\) error. The reported errors are averages over three separate random initializations of the neural network weights in each case.

A set of heatmaps is shown in Figure 1 to display the L2 error results. More heatmap results are shown in Appendix C. We obtained several observations from these heatmaps:

**Observation 1**: There is a dominant _activation function_ in PINNs working better for each PDE, which can be easily found by searching a small subset of the whole space. For example, it is easy to see that Tanh is the best choice for **Heat_0**. Median error value across the subsets is a good metric to determine the dominant _activation function_.

**Observation 2**: Under the dominant _activation function_, the larger _changing points_ perform better or comparably than smaller values, as can be seen in the average error values shown beside the y-axes.

**Observation 3**: The "wider and deeper PINNs are better" rule does not apply to all PDEs, e.g., the **Wave** PDE.

**Observation 4**: The error distances (the values in parentheses in the cells) are usually very small, i.e., when the loss functions reach the smallest values in the training processes, so do the L2 error values.

Observations 1 and 2 suggest that it is possible to decouple the _activation function_ and _changing point_ from the search space. Observation 3 indicates that further research on the MLP structure is needed to determine _widths_ and _depths_ for PINNs. Observation 4 means that overfitting is not a problem for PINNs. The error value at the minimum loss function value is close to the minimum error that a PINN can actually reach. However, it is just a local summary for each cell. We need to compare theloss-error relationship between cells to establish that the loss function is a good search objective over the entire space.

### Structure-Error Relationship

_Width_ and _depth_ are key structure hyperparameters of PINNs. However, as mentioned in Observation 3, the pre-experiments above cannot give a clear relationship between the structures and error values of PINNs because of the low sampling rate in the spaces of the two hyperparameters. For that reason, we continue by doing more specific pre-experiments on the structure-error relationship.

Figure 1: L2 Error heatmaps on different PDEs with different PINN configurations. Each row corresponds to a different PDE, and each column corresponds to a different activation function. The x-axis of each heatmap represents different _width_ and _depth_ settings of PINNs from small to larger scales. For instance, “\(32 4\)” means the _width_ is \(32\) and the _depth_ is \(4\). The y-axis labels are different _changing points_ from \(0.1\) to \(0.5\). For each cell in the heatmaps, the top number is the smallest L2 error value that the PINN actually reached in its training process. For a direct visual representation, the cells with deeper colors correspond to smaller error values, i.e., the PINNs have better performances. The number in the parentheses in each cell is the absolute distance from the smallest actual error value to the error value when the smallest loss function value is reached. On the top of each heatmap, we report the average, median, and minimum error values across the heatmap. The numbers in the parentheses around the x and y labels are mean error values across columns and rows, which show the average performance when the other hyperparameters are fixed.

The results are shown in Figure 2. Each data point (joined by lines) is the average L2 error over three random initializations. We fix the _activation function_ and the _changing point_ in each case and then sample the space of _width_ and _depth_. More results can be found in Appendix C. Clearly, there are many _width_ regions in which the best-performing network is not the deepest, e.g., _width_ within \(\) for the **Heat_0** and around \(100\) for the **Wave**. This confirms again Observation 3 that the "wider and deeper PINNs are better" rule does not always apply and that it is important to determine the optimal depth for a given width.

**Observation 5**: Good regions in the space of _width_ can be identified, and then one can search for the optimal _depth_.

### Loss-Error Relationship

We would like to investigate if the loss function is a representative search objective across the entire configuration space. Hence, we report the loss-error relationship for different PDEs in Figure 3. The \(x\)-axis is the lowest loss value that a PINN reached in the training process, and the \(y\)-axis is the corresponding L2 error value. Note that Observation 4 has informed us that the smallest training error values agree with the smallest L2 error values. Each point in the figure represents a PINN architecture configuration shown in Figure 1. The R-square value indicates a strong linear correlation between the logarithmic loss and error values across the whole search space. Therefore, it is appropriate to leverage these loss values to judge the real performance of the PINNs. We have more linear correlation evidence shown in Appendix C. Hence, we have another observation:

**Observation 6**: There is a strong linear correlation between the smallest log-loss values and the corresponding log-error values for the PINNs with different hyperparameter configurations in a PDE problem. We can take advantage of the training loss function values to assess the performance of the PINNs.

## 4 Auto-PINN: Automated Architecture Optimization for PINNs

The pre-experiments have provided us with a clear guideline on how to decouple the hyperparameter space, and we now can present our Auto-PINN approach. With the help of Observations 1-6, we are ready to decouple the hyperparameters in the large search space and find the best architectures with only a small number of search trials.

### Methodology

**Input:** A PDE problem with training and testing data points. The hyperparameter search space is mentioned in Section 2.2.

**Search Objective:** The smallest loss values reached by the PINNs in the training processes (Observations 3 and 6).

**Step 0.**: Set _changing points_ to 0.5 (Observation 2) for the following **Step 1** to **Step 2.2**.
**Step 1.**: Search the _activation function_ (Observation 1). Sample on the _width_ space exponentially and the _depth_ space uniformly. Use the median search objective to determine the dominant _activation function_. This _activation function_ will become the only choice in the following steps.

Figure 2: Structure-error relationship.

**Step 2.1**: Search the good-performance regions of _width_ (Observation 5). Split the _width_ space into several intervals. Sample several _width_ settings in each interval to represent the performance of their interval, then combine them with the uniformly sampled _depth_ settings. Use the median search objective to find several best _width_ intervals as the good-performance regions.
**Step 2.2**: Search the best _depth_ settings. Collect all _width_ settings inside the "good performance" regions with all _depth_ settings. Search and find the top \(K\) candidate structure configurations.
**Step 3**: Search the best _changing point_ for each candidate.
**Step 4**: Verify the performance of the searched PINN architectures. Retrain every selected PINN five times with different initializations and report its median performance.

**Output:**\(K\) different PINN architectures with small training loss values correlating with small testing errors (Observation 4).

Please refer to Appendix C to see the default settings of the Auto-PINN algorithm.

### Complexity Analysis

The scale of the search space is greatly decreased by utilizing the proposed Auto-PINN algorithm. The entire search space in Section 2.2 contains \(10,080\) possible configurations. Auto-PINN under the default settings only needs at most \(261\) trials to find the best architectures, which is only \(2.59\%\) to the whole search space. Detailed analysis is in Appendix C.

## 5 Experiments

### Experimental Settings

In this section, we present the results of experiments that validate the effectiveness of the proposed method.

**PDE Benchmarks.** We conducted experiments using the **Heat_0**, **Heat_1**, **Wave**, **Burgers**, **Advection_0**, **Advection_1** and **Reaction** PDEs, which are described in Appendix A.

**Baseline Methods.** Random Search and HyperOpt  are selected as the baseline methods for comparison with Auto-PINN. We set \(300\) sampling numbers for the two baseline models, which are higher than the Auto-PINN.

**Implementation Details.** We set the learning rates to \(1e-5\) and the number of training epochs to \(10000\). We implemented Auto-PINN and the baseline methods with the PINN package DeepXDE  and hyperparameter tuning package Tune . DeepXDE  is a user-friendly open-sourced library for physics-informed machine learning including common PINNs and different training strategies. We use DeepXDE with some modifications in the APIs. Tune  is another Python library for experiment execution and hyperparameter tuning. It supports all mainstream machine learning frameworks and a large number of hyperparameter optimization methods. We utilized the trial parallelism feature of Tune to make our Auto-PINN more efficient. Underlying MLP models are built and trained with the PyTorch framework. We ran the experiments on 4 Nvidia 3090 GPUs.

Figure 3: Loss-error relationship.

### Comparison Results

The results are shown in Figure 4. Compared with the two baseline methods, the results show that Auto-PINN is more stable, as can be seen from the concentrated distributions. However, the Random Search and HyperOpt suffer from very large performance variances frequently. Meanwhile, Auto-PINN is capable of finding architectures with good performance. In most cases, the architectures with the median error values found by Auto-PINN match or exceed the performances of the baseline methods with fewer search trials. In summary, Auto-PINN outperforms the baseline methods in both stability and accuracy with fewer search trials.

### Influence of Learning Rates and Training Epochs

As we mentioned, we do not consider the learning rates and training epochs in our search space. Further experimental results indicate that Auto-PINN is not sensitive to these two training hyperparameters. As shown in Figure 5, the searched architectures with different learning rates and epochs congregate at a specific region in the search space, which means those architectures searched by Auto-PINN are still available within a range of proper learning rates and epochs. Therefore, there is no need to search again with different learning rates and the numbers of epochs. On the other hand, we can see that the PDEs show different preferences in the structures. For example, the **Heat_0** PDE requires wider structures but is insensitive to the _depth_, whereas the **Wave** PDE is not sensitive to _width_ but prefers _shallower_ PINNs. Auto-PINN is able to identify consistent architectures for different PDEs, which is a very important point for future research.

Figure 4: Comparison Results. The violin plots illustrate the range of L2 errors of the searched architectures in the PDE problems with different data sampling settings.

Figure 5: Searched architecture distributions by Auto-PINN with different learning rates and epoch settings. The sizes of the markers display the relative performances of each PINN configuration. Larger markers mean better performance, that is, the corresponding PINNs have smaller testing error values.

### Influence of Data Sampling

Different PDEs have distinct sensitivity to data sampling, as illustrated in Figure 6, where we employed random and uniform sampling schemes, each using two different sampling densities, for the collocation, boundary,and initial points; see Appendix A for the details. The results confirm that the structure-sampling relationship is distinct for each PDE. Therefore, it is not appropriate to simply reuse the searched architectures when the sampling strategy changes. We remark that more sophisticated adaptive- and residual-based data sampling methods [7; 17; 18] for PINNs have been proposed. These sampling methods can be used as an internal setting for Auto-PINN to achieve better search accuracy in future work.

## 6 Conclusion and Future Work

In this paper, we proposed Auto-PINN, the first systematic neural architecture and hyperparameter optimization approach for PINNs, which can search for the best architectures and hyperparameters for different PDE problems within a large search space. We conducted a comprehensive set of pre-experiments to understand the search space of PINNs. Based on the observations, we proposed a step-by-step decoupling strategy to reduce the search space and use the loss value as the search objective. The comparison results demonstrate the stability and the effectiveness of Auto-PINN. In addition, we perform experiments to analyze the influences of the learning rate, the training epochs, and the data sampling strategies. We found that the performance is not sensitive to the learning rates and the training epochs, while the best configuration depends on the adopted sampling strategy. We hope the insights gleaned from our observations can motivate future exploration in PINNs and that Auto-PINN can serve as a strong baseline in future research. In future work, we plan to incorporate more sophisticated data sampling strategies into the search space of Auto-PINN to achieve better performances.