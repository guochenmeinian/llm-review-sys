# SMPLer-X: Scaling Up Expressive

Human Pose and Shape Estimation

 Zhongang Cai\({}^{*,1,2,3}\), Wanqi Yin\({}^{*,2,4}\), Ailing Zeng\({}^{5}\), Chen Wei\({}^{2}\), Qingping Sun\({}^{2}\),

**Yanjun Wang\({}^{2}\)**, Hui En Pang\({}^{1,2}\), Haiyi Mei\({}^{2}\), Mingyuan Zhang\({}^{1}\),

**Lei Zhang\({}^{5}\)**, **Chen Change Loy\({}^{1}\), Lei Yang\({}^{{},2,3}\), Ziwei Liu\({}^{{},1}\)**

\({}^{1}\) S-Lab, Nanyang Technological University, \({}^{2}\) SenseTime Research, \({}^{3}\) Shanghai AI Laboratory,

\({}^{4}\) The University of Tokyo, \({}^{5}\) International Digital Economy Academy (IDEA)

Equal contributions. \({}^{}\)Co-corresponding authors.

\({}^{1}\) S-Lab, Nanyang Technological University, \({}^{2}\) SenseTime Research, \({}^{3}\) Shanghai AI Laboratory,

\({}^{4}\) The University of Tokyo, \({}^{5}\) International Digital Economy Academy (IDEA)

###### Abstract

Expressive human pose and shape estimation (EHPS) unifies body, hands, and face motion capture with numerous applications. Despite encouraging progress, current state-of-the-art methods still depend largely on a confined set of training datasets. In this work, we investigate scaling up EHPS towards the first _generalist_ foundation model (dubbed **SMPLer-X**), with up to ViT-Huge as the backbone and training with up to 4.5M instances from diverse data sources. With big data and the large model, SMPLer-X exhibits strong performance across diverse test benchmarks and excellent transferability to even unseen environments. _1) For the data scaling_, we perform a systematic investigation on 32 EHPS datasets, including a wide range of scenarios that a model trained on any single dataset cannot handle. More importantly, capitalizing on insights obtained from the extensive benchmarking process, we optimize our training scheme and select datasets that lead to a significant leap in EHPS capabilities. _2) For the model scaling,_ we take advantage of vision transformers to study the scaling law of model sizes in EHPS. Moreover, our finetuning strategy turn SMPLer-X into _specialist_ models, allowing them to achieve further performance boosts. Notably, our foundation model SMPLer-X consistently delivers state-of-the-art results on seven benchmarks such as AGORA (107.2 _mm_ NMVE), UBody (57.4 _mm_ PVE), EgoBody (63.6 _mm_ PVE), and EHF (62.3 _mm_ PVE without finetuning). 2

## 1 Introduction

The recent progress in expressive human pose and shape estimation (EHPS) from monocular images or videos offers transformative applications for the animation, gaming, and fashion industries. This task typically employs parametric human models (_e.g._, SMPL-X ) to adeptly represent the highly complicated human body, face, and hands. In recent years, a large number of diverse datasets have entered the field , providing the community new opportunities to study various aspects such as capture environment, pose distribution, body visibility, and camera views. Yet, the state-of-the-art methods remain tethered to a limited selection of these datasets, creating a bottleneck in performance across varied scenarios and hindering the ability to generalize to unseen situations.

Our mission in this study is to explore existing data resources comprehensively, providing key insights crucial for establishing robust, universally applicable models for EHPS. Accordingly, we establish the first systematic benchmark for EHPS, utilizing 32 datasets and evaluating their performanceacross five major benchmarks. We find that there are significant inconsistencies among benchmarks, revealing the overall complicated landscape of EHPS, and calling for data scaling to combat the domain gaps between scenarios. This detailed examination emphasizes the need to reassess the utilization of available datasets for EHPS, advocating for a shift towards more competitive alternatives that offer superior generalization capabilities, and highlights the importance of harnessing a large number of datasets to capitalize on their complementary nature.

Moreover, we systematically investigate the contributing factors that determine the transferability of these datasets. Our investigation yields useful tips for future dataset collection: 1) the more is not necessarily, the merrier: datasets do not have to be very large to be useful as long as they exceed approximately 100K instances based on our observation. 2) Varying indoor scenes is a good alternative if an in-the-wild (including outdoor) collection is not viable. 3) synthetic datasets, despite having traceable domain gaps, are becoming increasingly potent to a surprising extent. 4) Pseudo-SMPL-X labels are useful when ground truth SMPL-X annotations are unavailable.

Equipped with the knowledge procured from the benchmark, we exhibit the strength of massive data with SMPLer-X, a _generalist_ foundation model that is trained using a diverse range of datasets and achieves exceptionally balanced results across various scenarios. To decouple from algorithmic research works, we design SMPLer-X with a minimalist mindset: SMPLer-X has a very simple architecture with only the most essential components for EHPS. We hope SMPLer-X could facilitate massive data and parameter scaling and serve as a baseline for future explorations in the field instead of a stringent investigation into the algorithmic aspect. Experiments with various data combinations and model sizes lead us to a well-rounded model that excels across all benchmarks that contests the community norm of limited-dataset training. Specifically, our foundation models demonstrate significant performance boost through both data scaling and model size scaling, reducing the mean primary errors on five major benchmarks (AGORA , UBody , EgoB-ody , 3DPW , and EHF ) from over 110 mm to below 70 mm (demonstrated in Fig. 1), and showcases impressive generalization capabilities by effectively transferring to new scenarios, such as DNA-Rendering  and ARCTIC .

Furthermore, we validate the efficacy of finetuning our _generalist_ foundation models to evolve into domain-specific _specialists_, delivering outstanding performance on all benchmarks. Specifically, we follow the same data selection strategy that empowers our specialist models to set new records on the AGORA leaderboard by being the first model to hit 107.2mm in NMVE (an 11.0% improvement) and achieving SOTA performance on EgoBody, UBody, and EHF.

Our contributions are three-fold. **1)** We build the first systematic and comprehensive benchmark on EHPS datasets, which provides critical guidance for scaling up the training data toward robust and transferable EHPS. **2)** We explore both data and model scaling in building the _generalist_ foundation model that delivers balanced results across various scenarios and extends successfully to unseen datasets. **3)** We extend the data selection strategy to finetune the foundation model into potent _specialists_, catering to various benchmark scenarios.

## 2 Related Work

**Expressive Human Pose and Shape Estimation (EHPS).** Due to the erupting 3D virtual human research applications [64; 65; 19; 18; 5] and the parametric models (e.g., SMPL  and SMPL-X ), capturing the human pose and shape (HPS) [26; 31; 28; 29; 36; 57; 58], and additionally hands and face (EHPS) [49; 59; 8; 51; 68; 13; 54; 63] from images and videos have attracted increasing

Figure 1: **Scaling up EHPS. Both data and model scaling are effective in reducing mean errors on primary metrics across key benchmarks: AGORA , UBody , EgoBody , 3DPW  and EHF . OSX  and H4W  are SOTA methods. Area of the circle indicates model size, with ViT variants as the reference (top right).**

attention. Optimization-based methods (e.g., SMPLify-X ) detect 2D features corresponding to the whole body and fit the SMPL-X model. However, they suffer from slow speed and are ultimately limited by the quality of the 2D keypoint detectors. Hence, learning-based models are proposed. One of the key challenges of EHPS is the low resolution of hands and face compared with the body-only estimation, making the articulated hand pose estimation and high-quality expression capture difficult. Accordingly, mainstream whole-body models first detect and crop the hands and face image patches, then resize them to higher resolutions and feed them into specific hand and face networks to estimate the corresponding parameters [8; 51; 68; 13; 54; 44; 63; 33]. Due to the highly complex multi-stage pipelines, they inevitably cause inconsistent and unnatural articulation of the mesh and implausible 3D wrist rotations, especially in occluded, truncated, and blurry scenes. Recently, OSX  proposes the first one-stage framework based on ViT-based backbone  to relieve the issues in previous multi-stage pipelines. This method provides a promising and concise way to scale up the model. However, they only use confined training datasets for a fair comparison and do not explore the combination of more data toward generalizable and precise EHPS.

**Multi-dataset Training for Human-centric Vision.** Recent efforts have been using multiple datasets in pretraining a general model for a wide range of downstream human-centric tasks. For example, HumanBench  leverages 37 datasets, whereas UniHCP  utilizes 33 datasets for tasks such as ReID, pedestrian detection, and 2D pose estimation. However, these works have only evaluated the efficacy of 2D tasks. Sarandi _et al._ take advantage of 28 datasets in training a strong model for 3D keypoint detection, which recovers only the skeleton of subjects without estimating body shapes and meshes. Pang _et al._ analyze 31 datasets for human pose and shape estimation (_i.e._, SMPL estimation). However, hands and face estimation is not included, and only fewer than ten datasets are used concurrently in the most diverse training. This paper targets to scale training data and model size for EHPS, that simultaneously recovers the expressive pose and shape of the human body, hands, and face.

## 3 Benchmarking EHPS Datasets

### Preliminaries

**SMPL-X.** We study expressive human pose and shape estimation via 3D parametric human model SMPL-X , which models the human body, hands, and face geometries with parameters. Specifically, our goal is to estimate pose parameters \(^{55 3}\) that include body, hands, eyes, and jaw poses; joint body, hands and face shape \(^{10}\), and facial expression \(^{10}\). The joint regressor \(\) is used to obtain 3D keypoints from parameters via \(R_{}(())\) where \(R_{}\) is a transformation function along the kinematic tree.

Figure 2: **Dataset attribute distributions.** a) and d) are image feature extracted by HumanBench  and OSX  pretrained ViT-L backbone. b) Global orientation (represented by rotation matrix) distribution. c) Body pose (represented by 3D skeleton joints) distribution. Both e) scenes and f) Real/Synthetic are drawn on the same distribution as d). All: all datasets. UMAP  dimension reduction is used with the x and y-axis as the dimensions of the embedded space (no unit).

**Evaluation Metrics.** We use standard metrics for EHPS. PVE (per-vertex error) and MPJPE (mean per-joint position error) measure the mean L2 error for vertices and regressed joints, respectively. The "PA" prefix indicates Procrutes Alignment is conducted before error computation. AGORA Leaderboard  introduces NMVE (normalized mean vertex error) and NMJE (normalized mean joint error) that take detection performance F1 score into consideration. Moreover, we propose MPE (mean primary error) that takes the mean of multiple primary metrics (MPJPE for 3DPW  test, and PVE for AGORA, UBody, EgoBody, and EHF) to gauge generalizability. All errors are reported in millimeters (mm).

### Overview of Data Sources

In this work, we study three major types of datasets. 1) motion capture datasets that leverage optical [21; 14; 12; 15; 16; 42] or vision-based [66; 17; 7; 5] multi-view motion capture systems, are typically collected in a studio environment. However, it is possible to include an outdoor setup, or utilize additional sensors such as IMUs . These datasets generally provide high-quality 3D annotations but are less flexible due to physical constraints, especially those built with immobile capture systems that require accurate sensor calibrations. 2) pseudo-annotated datasets [38; 1; 34; 37; 43; 27; 67; 2; 23; 46; 62; 53] that re-annotate existing image datasets with parametric human annotations [24; 45; 37]. These datasets take advantage of the diversity of 2D datasets, and the pseudo-3D annotations, albeit typically not as high-quality, have been proven effective [47; 31; 24]. 3) synthetic datasets [4; 6; 30; 48; 61] that are produced with renderings engines (_e.g._, Unreal Engine). These datasets produce the most accurate 3D annotations and can easily scale up with high diversity. However, the synthetic-real gap is not fully addressed. Key attributes of the datasets are included in Table 1.

To evaluate the EHPS capability across diverse scenarios, we select multiple key datasets to form a comprehensive benchmark. They should possess the desirable traits such as 1) having accurate SMPL or SMPL-X annotations, 2) being representative of certain aspects of real-life scenarios, 3) being widely used, but this requirement is relaxed for the new datasets which are released within two years, and 4) has a clearly defined test set. To this end, five datasets (AGORA , UBody , EgoBody , 3DPW , and EHF ) representing different aspects are selected as the evaluation datasets. We briefly introduce these five datasets and the rest in the Supplementary Material. **AGORA

is the most widely-used benchmark for SMPL-X evaluation. It is a synthetic dataset featuring diverse subject appearances, poses, and environments with high-quality annotation. We evaluate on both validation and test set (leaderboard) as the latter has a monthly limit of submissions. **UBody** is the latest large-scale dataset with pseudo-SMPL-X annotations that covers fifteen real-life scenarios, such as talk shows, video conferences, and vologs, which primarily consist of the upper body in images. We follow the intra-scene protocol in training and testing, where all scenarios are seen. **EgoBody** captures human motions in social interactions in 3D scenes with pseudo-SMPL-X annotations. It comprises a first-person egocentric set (EgoSet) and a third-person multi-camera set (MVSet). We test on the EgoSet with heavy truncation and invisibility. **3DPW** is the most popular in-the-wild dataset with SMPL annotations. Since SMPL-X annotation is not available, we map SMPL-X keypoints and test on 14 LSP  keypoints following the conventional protocol [26; 31]. **EHF** is a classic dataset with 100 curated frames of one subject in an indoor studio setup, with diverse body poses and especially hand poses annotated in SMPL-X vertices. It has a test set but no training or validation sets. Hence, it is only used to evaluate cross-dataset performance.

Besides being popular or the latest evaluation sets for EHPS, we further analyze if these five datasets collectively provide wide coverage of existing datasets. In Fig. 3, we randomly downsample all datasets to equal length (1K examples) and employ UMAP  to visualize several key aspects. We use pretrained ViT-L from HumanBench  and OSX  to process patch tokens flattened as feature vectors from images cropped by bounding boxes. HumanBench is trained for various human-centric tasks (_e.g._, Re-ID, part segmentation, and 2D pose estimation), whereas OSX is an expert model on EHPS. As for global orientation, it is closely associated with camera pose as we convert all data into the camera coordinate frame; we plot its distribution by using flattened rotation matrix representations. Moreover, we follow [50; 6; 47] to represent poses as 3D keypoints regressed from the parametric model. Specifically, we flatten 21 SMPL-X body keypoints, and 15 hand keypoints from each hand, regressed with zero parameters except for the body pose and hand poses. It is shown that 1) the five benchmark datasets have varied distribution, which is expected due to their different designated purposes, and 2) collectively, the five datasets provide a wide, near-complete coverage of the entire dataset pool.

### Benchmarking on Individual Datasets

In this section, we aim to benchmark datasets and find those that do well in various scenarios. To gauge the performance of each dataset, we train a SMPLer-X model with the training set of that dataset and evaluate the model on the _val/testing_ sets of five evaluation datasets: AGORA, UBody, EgoBody, 3DPW, and EHF. Here, the benchmarking model is standardized to use ViT-S as the backbone, trained on 4 V100 GPUs for 5 epochs with a total batch size of 128 and a learning rate of \(1 10^{-5}\). The dataset preprocessing details are included in the Supplementary Material.

In Table 1, we report the primary metrics (Sec. 3.1) and ranking of the 32 datasets. The complete results in the Supplementary Material. We also compute the mean primary error (MPE) to facilitate easy comparison between individual datasets. Note that for AGORA, UBody, EgoBody, and 3DPW, their performances on their own test set are excluded from computing MPE. This is because in-domain evaluation results are typically much better than cross-domain ones, leading to significant error drops. In addition, note that there are datasets designed for specific purposes (_e.g._, Talkshow  for gesture generation, DNA-Rendering  for human NeRF reconstruction), being ranked lower on our benchmark, which focuses on EHPS (a perception task) does not reduce their unique values and contributions to the computer vision community.

From the benchmark, we observe models trained on a single dataset tend to perform well on the same domain but often cannot do well on other domains. For example, the model trained on AGORA is ranked \(1^{st}\) on AGORA (val), but \(6^{th}\) on UBody, \(6^{th}\) on EgoBody, \(12^{th}\) on 3DPW, and \(24^{th}\) on EHF. This observation indicates that 1) the test scenarios are diverse, showcasing the challenging landscape of EHPS, and 2) data scaling is essential for training a robust and transferable model for EHPS due to significant gaps between different domains.

### Analyses on Dataset Attributes

In this section, we study attributes that contribute to generalizability. However, it is important to acknowledge that such analyses are not a straightforward task: the attributes often exhibit coupled effects. Consequently, counter-examples are inevitable (_e.g._, we observe that InstaVariety, an in-the-wild dataset, demonstrates strong performance, whereas LSPET, another in-the-wild dataset, does not perform as well). Despite the challenges in pinpointing the exact factors that determine the success of an individual dataset, we adopt a collective perspective and aim to identify general trends with several key factors  in Fig. 3, and discussed below.

**First**, Fig. 3a) shows that the performance of a dataset (in terms of ranking) is not strongly associated with the number of training instances once the instance number exceeds approximately 100K. Although a very small amount of training data is insufficient to train a strong model, having an exceedingly large amount of data does not guarantee good performance either. For example, MSCOCO only comprises 149.8K training instances but achieves a higher ranking compared to datasets with 10\(\) larger scales. This may be attributed to the diverse appearance and complex scenes present in the MSCOCO dataset. Hence, it would be more cost-effective to channel resources to improve diversity and quality, when the dataset has become adequately large.

**Second**, we categorize datasets into 1) in-the-wild, which contains data from diverse environments; 2) indoor with several scenes; 3) studio, which has a fixed multi-view setup. Particularly, Fig. 3b) shows that the top 10 are mostly in-the-wild datasets, indoor datasets concentrate in the top 20 and the studio dataset tends to be ranked lower in the benchmark. Moreover, Fig. 2e) illustrates that in-the-wild datasets exhibit the most diverse distribution, covering both indoor and studio datasets. Indoor datasets display a reasonable spread, and studio datasets have the least diversity. Our findings validate previous studies that suggest an indoor-outdoor domain gap . Differing from Pang _et al._, which does not differentiate between indoor and studio datasets, we argue that categorizing all datasets collected indoors into a single class oversimplifies the analysis. For example, consider EgoBody  and Human3.6M . Both datasets does not have outdoor data; however, EgoBody consists of a wide variety of indoor scenes, whereas Human3.6M consists of only one scene, which may contribute to the better ranking of EgoBody compared to Human3.6M. Hence, this suggests that in-the-wild data collection is the most ideal, but diversifying indoor scenes is the best alternative.

**Third**, most of the five contemporary synthetic datasets  demonstrate surprising strength and are ranked highly in Fig. 3c). It is worth noting that four (UBody, EgoBody, 3DPW, and EHF) of the five evaluation benchmarks used are real datasets, indicating that knowledge learned from synthetic data is transferable to real scenarios. To explain this observation, we take a close look at Fig. 2f): although real and synthetic datasets do not have extensive overlap, synthetic data possesses two ideal characteristics. First, there is a high overlap between real and synthetic data at the rightmost cluster. Referring to Fig. 2e), which is drawn from the same distribution, we find that this cluster primarily represents in-the-wild data. Therefore, synthetic data includes a substantial number of in-the-wild images that closely resemble real in-the-wild scenarios. Second, synthetic data also have scatters of image features on other clusters, indicating that synthetic data provides coverage to some extent for various real-world scenarios.

**Fourth**, Fig. 3d) reveals that a dataset can be valuable with accurate or pseudo-SMPL-X annotations, as they constitute the most of the top 10 datasets. A prominent example is InstaVariety , which has only pseudo-SMPL-X annotation produced by NeuralAnnot , yet, is ranked third in our benchmark. However, due to the differences in parameter spaces, SMPL annotations are less effective: it is observed that datasets with SMPL annotations tend to cluster in the lower bracket of the benchmark, especially those with pseudo-SMPL annotations. This observation suggests that SMPL-X

Figure 3: **Analysis on dataset attributes. We study the impact of a) the number of training instances, b) scenes, c) real or synthetic appearance, and d) annotation type, on dataset ranking in Table 1.**

annotations are critical to EHPS; fitting pseudo labels is a useful strategy even if they could be noisy. Moreover, using SMPL labels effectively for SMPL-X estimation remains a challenge.

## 4 Scaling up EHPS

### Model Architectures

Catering to our investigation, we design a minimalistic framework (dubbed SMPLer-X) that only retains the most essential parts for two reasons. First, it must be scalable and efficient as we train with a large amount of data. Second, we aim to create a framework that is decoupled from specific algorithm designs, providing a clean foundation for future research. To this end, SMPLer-X consists of three parts: a _backbone_ extracts image features, which we employ Vision Transformer  for its scalability; a _neck_ that predicts bounding boxes and crop regions of interest from the feature map for hands and face; regression _heads_ that estimate parameters for each part. Note that SMPLer-X does not require third-party detectors , cross-part feature interaction modules [8; 13], projection of coarse SMPL-X estimations , or a heavy decoder . As the design of SMPLer-X is not the focus of our investigation, more details are included in the Supplementary Material.

### Training the Generalist Foundation Models

The SOTA methods [37; 44] usually train with only a few (_e.g._, MSCOCO, MPII, and Human3.6M) datasets, whereas we investigate training with many more datasets. However, we highlight that the dataset benchmark in Table 1 cannot be used: selecting datasets based on their performance on the test sets of the evaluation benchmarks leaks information about the test sets. Hence, we construct another dataset benchmark in the Supplementary Material, that ranks individual datasets on the _training_ set of the major EHPS benchmarks. We use four data amounts: 5, 10, 20, and 32 datasets as the training set, with a total length of 0.75M, 1.5M, 3.0M, and 4.5M instances. We always prioritize higher-ranked

   \#Datasets & \#Inst. & Model & \#Param. & FPS & AGORA  & EgoBody  & UBody  & 3DPW  & EHF  & MPE \\ 
5 & 0.75M & SMPLer-X-S5 & 32M & 36.2 & 119.0 & 114.2 & 110.1 & 110.2 & 100.5 & 110.8 \\
10 & 1.58M & SMPLer-X-S10 & 32M & 36.2 & 116.0 & 88.6 & 107.7 & 97.4 & 89.9 & 99.9 \\
20 & 3.0M & SMPLer-X-S20 & 32M & 36.2 & 109.2 & 84.3 & 70.0 & 87.5 & 86.6 & 87.7 \\
32 & 4.5M & SMPLer-X-S32 & 32M & 36.2 & 105.2 & 82.5 & 68.1 & 83.2 & 74.1 & 82.6 \\ 
5 & 0.75M & SMPLer-X-S5 & 103M & 33.1 & 102.7 & 108.1 & 105.8 & 104.8 & 96.1 & 103.5 \\
10 & 1.58M & SMPLer-X-B10 & 103M & 33.1 & 97.8 & 76.4 & 107.3 & 89.9 & 74.7 & 89.2 \\
20 & 3.0M & SMPLer-X-B20 & 103M & 33.1 & 95.6 & 75.5 & 65.3 & 83.5 & 73.0 & 78.6 \\
32 & 4.54M & SMPLer-X-B12 & 103M & 33.1 & 88.0 & 72.7 & 63.3 & 80.3 & 67.3 & 74.3 \\ 
5 & 0.75M & SMPLer-X-S5 & 327M & 24.4 & 88.3 & 98.7 & 110.8 & 97.8 & 89.5 & 97.0 \\
10 & 1.58M & SMPLer-X-L10 & 327M & 24.4 & 82.6 & 69.7 & 104.0 & 82.5 & 64.0 & 80.6 \\
20 & 3.0M & SMPLer-X-L20 & 327M & 24.4 & 80.7 & 66.6 & 61.5 & 78.3 & 65.4 & 70.5 \\
32 & 4.54M & SMPLer-X-L32 & 327M & 24.4 & 74.2 & 62.2 & 57.3 & 75.2 & 62.4 & 66.2 \\ 
5 & 0.75M & SMPLer-X-H5 & 662M & 17.5 & 89.0 & 87.4 & 102.1 & 88.3 & 68.3 & 87.0 \\
10 & 1.5M & SMPLer-X-H10 & 662M & 17.5 & 81.4 & 65.7 & 100.7 & 78.7 & **56.6** & 76.6 \\
20 & 3.0M & SMPLer-X-H12 & 662M & 17.5 & 77.5 & 63.5 & 59.9 & **74.4** & 59.4 & 67.0 \\
32 & 4.54M & SMPLer-X-H132 & 662M & 17.5 & **69.5** & **59.5** & **54.5** & 75.0 & 56.8 & **63.1** \\   

Table 2: **Foundation Models.** We study the scaling law of the amount of data and the model sizes. The metrics are MPJPE for 3DPW, and PVE for other evaluation benchmarks. Foundation models are named “SMPLer-X-MN”, where M indicates the size of ViT backbone (S, B, L, H), N is the number of datasets used in the training. FPS: inference speed (frames per second) on a V100 GPU. MPE: mean primary error. AGORA uses the validation set, and EgoBody uses the EgoSet.

Figure 4: **Architecture of SMPLer-X, which upholds the idea that ”simplicity is beauty”. SMPLer-X contains a backbone that allows for easy investigation on model scaling, a neck for hand and face feature cropping, and heads for different body parts. Note that we wish to show in this work that model and data scaling are effective, even with a straightforward architecture.**

datasets. To prevent larger datasets from shadowing smaller datasets, we adopt a balanced sampling strategy. Specifically, all selected datasets are uniformly upsampled or downsampled to the same length and add up to the designated total length. To facilitate training, we follow OSX  to use AGORA, UBody, MPII, 3DPW, Human3.6M in COCO-format , and standardize all other datasets into the HumanData  format. We also study four ViT backbones of different sizes (ViT-Small, Base, Large and Huge), pretrained by ViTPose . The training is conducted on 16 V100 GPUs, with a total batch size of 512 (256 for ViT-Huge) for 10 epochs. More training details such as adapting SMPL or gendered SMPL-X in the training are included in the Supplementary Material.

In Table 2, we show experimental results with a various number of datasets and foundation model sizes. Foundation models are named "SMPLer-X-MN", where M can be S, B, L, H that indicates the size of the ViT backbone, and N indicates the number of datasets used in the training. For example, SMPLer-X-L10 means the foundation model takes ViT-L as the backbone, and is trained with Top 10 datasets (ranked according to the individual dataset performance on the training sets of the key evaluation benchmarks). It is observed that **1)** more training data (data scaling) leads to better performance in terms of MPE. The model performance improves gradually as the number of training datasets increases. However, besides the increment in training instances, more datasets provide a richer collection of diverse scenarios, which we argue is also a key contributor to the performance gain across evaluation benchmarks. **2)** A larger foundation model (model scaling) performs better at any given amount of data. However, the marginal benefits of scaling up decrease beyond model size L. Specifically, ViT-H has more than twice the parameters than ViT-L, but the performance gain is not prominent. **3)** The foundation model always performs better than in-domain training on a single training set. For example, SMPLer-X-B20, performs better on the validation set of AGORA, and test sets of UBody, EgoBody, and 3DPW, than models trained specifically on the corresponding training set in Table 1. This is useful for real-life applications: instead of training a model for each of the user cases, a generalist foundation model contains rich knowledge to be a one-size-fits-all alternative.

    & _{1}\) (_mm_)} & _{1}\) (_mm_)} \\   & All & Hands & Face & All & Hands & Face \\  Hand4Whole  & 73.2 & 9.7 & 4.7 & 183.9 & 72.8 & 81.6 \\ OSX  & 69.4 & 11.5 & 4.8 & 168.6 & 70.6 & 77.2 \\ OSX  & 45.0 & **8.5** & 3.9 & 79.6 & 48.2 & 37.9 \\ SMPLer-X-B1+ & 48.9 & 8.6 & 4.0 & 86.1 & 51.5 & 41.2 \\ SMPLer-X-L20 & 48.6 & 8.9 & 4.0 & 80.7 & 51.0 & 41.3 \\ SMPLer-X-L32 & 45.1 & 8.7 & **3.8** & 72.4 & 42.8 & 38.2 \\ SMPLer-X-L20J & **39.1** & 9.3 & **3.8** & **62.5** & **42.3** & **32.8** \\   

Table 4: **AGORA Val set. \(\) and \(*\) are finetuned on the AGORA training set, and trained on the AGORA training set only, respectively.**

    & _{1}\) (_mm_)} & _{1}\) (_mm_)} & _{2}\) (_mm_)} & _{1}\) (_mm_)} \\  Method & All & Body & All & Body & All & Body & Face & LHand & RHand & All & Body & Face & LHand & RHand \\  BELDAM  & 179.5 & 132.2 & 17.75 & 113.4 & 131.0 & 96.5 & 25.8 & 38.8 & 39.0 & 129.6 & 95.9 & 27.8 & 36.6 & 36.7 \\ Hand4Whole  & 144.1 & 96.0 & 141.1 & 92.7 & 135.5 & 90.2 & 41.6 & 46.3 & 48.1 & 132.6 & 87.1 & 46.1 & 44.3 & 46.2 \\ BELDAM  & 142.2 & 102.1 & 14.0 & 101.0 & 103.8 & 74.5 & **23.1** & **31.7** & **33.2** & 102.9 & 74.3 & **24.7** & **29.9** & **31.3** \\ PyMx-X  & 141.2 & 94.4 & 140.0 & 93.5 & 125.7 & 84.0 & 35.0 & 44.6 & 45.6 & 124.6 & 83.2 & 37.9 & 42.5 & 43.7 \\ OSX  & 130.6 & 85.3 & 127.6 & 83.3 & 122.8 & 80.2 & 36.2 & 45.4 & 46.1 & 119.9 & 78.3 & 37.9 & 43.0 & 43.9 \\ HybridX-X  & 120.5 & 73.7 & 115.7 & 72.3 & 112.1 & 68.5 & 37.0 & 46.7 & 47.0 & 107.6 & 67.2 & 38.5 & 41.2 & 41.4 \\ SMPLer-X-L20 & 133.1 & 88.1 & 128.9 & 84.6 & 123.8 & 81.9 & 37.4 & 43.6 & 44.8 & 119.9 & 78.7 & 39.5 & 41.4 & 44.8 \\ SMPLer-X-L32 & 122.8 & 80.3 & 191.9 & 77.6 & 114.2 & 74.7 & 35.1 & 41.3 & 42.2 & 110.8 & 72.2 & 36.7 & 39.1 & 40.1 \\ SMPLer-X-L20J & **107.2** & **65.8** & **104.1** & **66.3** & **99.7** & **63.5** & 29.9 & 39.1 & 39.5 & **96.8** & **61.7** & 31.4 & 36.7 & 37.2 \\   

Table 3: **AGORA test set. \(\) denotes the methods that are finetuned on the AGORA training set. \(*\)denotes the methods that are trained on AGORA training set only.**

   _{1}\) (_mm_)} & _{1}\) (_mm_)} \\  Method & All & Hands & Face & All & Hands & Face \\  Hand4Whole  & 50.3 & **10.8** & 5.8 & 76.8 & **39.8** & 26.1 \\ OSX  & 48.7 & 15.9 & 6.0 & 70.8 & 53.7 & 26.4 \\ SMPLer-X-L20 & 37.8 & 15.0 & 5.1 & 65.4 & 49.4 & 17.4 \\ SMPLer-X-L32 & **37.1** & 14.1 & **5.0** & **62.4** & 47.1 & **17.0** \\   

Table 10: **3DPW. \(\) denotes the methods that use a head for SMPL regression. \(\) and \(*\) are finetuned on the 3DPW training set and trained on 3DPW training set only, respectively. Unit: _mm_.**

   Method & MPJPE & PA-MPJPE \\   \\  OSX-SMPL \(\) & 74.7 & 45.1 \\ HybrIK  & 71.6 & **41.8** \\ CLIFF  & **68.0** & 43.0 \\   \\  Hand4Whole  & 86.6 & 54.4 \\ ExPose  & 93.4 & 60.7 \\ OSX  & 86.2 & 60.6 \\ SMPLer-X-B1\(\) & 95.6 & 67.6 \\ SMPLer-X-L20 & 78.3 & 52.1 \\ SMPLer-X-L32 & **75.2** & **50.5** \\ SMPLer-X-L20J & 76.8 & 51.5 \\   

Table 5: **EHF. As EHF does not have a training set to benchmark datasets, we do not perform finetuning. Moreover, EHF is not seen in our training and can be used to validate our foundation models’ transferability.**

[MISSING_PAGE_FAIL:9]

finetune experiments on ViT-L to match the backbone of current SOTA . The results are shown in the same tables as the foundation models (Table 3, 4, 5, 6, 7, and 10), where finetuning always lead to substantial performance enhancement on the foundation models.

## 5 Conclusion

In this work, we benchmark datasets for EHPS that provide us insights for training and finetuning a foundation model. Our work is useful in three ways. First, our pretrained model (especially the backbone) can be a plug-and-play component of a larger system for EHPS and beyond. Second, our benchmark serves to gauge the performances of future generalization studies. Third, our benchmarking-finetuning paradigm can be useful for the rapid adaptation of any foundation model to specific scenarios. Specifically, users may collect a training set, evaluate pretrained models of various other datasets on it, and select the most relevant datasets to finetune a foundation model.

**Limitations.** First, although we use five comprehensive benchmark datasets to gauge the generalization capability, they may still be insufficient to represent the real-world distribution. Second, our experiments do not fully investigate the impact of various model architectures due to the prohibitive cost of training the foundation model.

**Potential negative societal impact.** As we study training strong EHPS models and release the pretrained models, they may be used for unwarranted surveillance or privacy violation.

Figure 5: **Visualization.** We compare SMPLer-X-L32 with OSX  and Hand4Whole  (trained with the MSCOCO, MPII, and Human3.6M) in various scenarios such as those with heavy truncation, hard poses, and rare camera angles.