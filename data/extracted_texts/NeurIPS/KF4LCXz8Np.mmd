# On the Generalization Error of Stochastic Mirror Descent for Quadratically-Bounded Losses: an Improved Analysis

On the Generalization Error of Stochastic Mirror Descent for Quadratically-Bounded Losses: an Improved Analysis

 Ta Duy Nguyen

Department of Computer Science

Boston University

taduy@bu.edu

&Alina Ene

Department of Computer Science

Boston University

aene@bu.edu

&Huy Le Nguyen

Khoury College of Computer Sciences

Northeastern University

hu.nguyen@northeastern.edu

###### Abstract

In this work, we revisit the generalization error of stochastic mirror descent for quadratically bounded losses studied in Telgarsky (2022). Quadratically bounded losses is a broad class of loss functions, capturing both Lipschitz and smooth functions, for both regression and classification problems. We study the high probability generalization for this class of losses on linear predictors in both realizable and non-realizable cases when the data are sampled IID or from a Markov chain. The prior work relies on an intricate coupling argument between the iterates of the original problem and those projected onto a bounded domain. This approach enables blackbox application of concentration inequalities, but also leads to suboptimal guarantees due in part to the use of a union bound across all iterations. In this work, we depart significantly from the prior work of Telgarsky (2022), and introduce a novel approach for establishing high probability generalization guarantees. In contrast to the prior work, our work directly analyzes the moment generating function of a novel supermartingale sequence and leverages the structure of stochastic mirror descent. As a result, we obtain improved bounds in all aforementioned settings. Specifically, in the realizable case and non-realizable case with light-tailed sub-Gaussian data, we improve the bounds by a \( T\) factor, matching the correct rates of \(1/T\) and \(1/\), respectively. In the more challenging case of heavy-tailed polynomial data, we improve the existing bound by a \(\ T\) factor.

## 1 Introduction

Along with convergence analysis of optimization methods, understanding the generalization of models trained by these methods on unseen data is an important question in machine learning. However, despite the number of works attempting to answer it, the problem has not been fully understood, even in the simplest setting of linear predictors constructed with the standard stochastic gradient/mirror descent. A great part of prior works  focus only on the generalization on linearly separable data and/or of models trained with specific losses with exponentially decaying tails such as logistic loss. The question of what we can guarantee beyond these settings remains open.

Recently,  proposes a new approach to analyze the generalization error with _high probability_ of stochastic mirror descent for a broad class of quadratically bounded losses, beyond the realizable setting. This class of losses encapsulates both Lipschitz and smooth functions, for both regression and classification problems. The obtained bounds complement existing in-expectation bounds  and nearly match the counterpart of convergence rates in optimization. While this result pushes forward the state of the art, the obtained guarantees do not completely resolve the problem. The central piece of the proposed approach is a "coupling" technique between the iterates of the original problem and those projected onto a bounded domain. In this technique, one first constrains the problem in a bounded domain with a well chosen diameter. The bounded domain diameter allows to apply concentration inequalities as a blackbox and obtain bounds in high probability. Then using an inductive argument and a union bound across all iterations, one can show that the iterates in the original problem coincide with the ones in the constrained problem. Due to the union bound, the success probability decreases from \(1-\) to \(1-T\), where \(T\) is the number of iterations in the algorithm. This loss translates to a milder \( T\) factor loss in the guarantee in the case of realizable data, and a more significant \(\,T\) factor loss in the non-realizable setting when the data has polynomial tails. Thus a natural question arises of whether we can obtain a stronger analysis that closes these remaining gaps.

In this paper, we revisit these generalization bounds for quadratically bounded losses by . We introduce a novel approach to analyze the generalization errors of stochastic mirror descent in both realizable and non-realizable cases when the data are sampled IID or from a Markov chain. In all these cases, we remove the need to use the union bound argument, thus preventing the loss in the success probability. This translates to the following improvements:

\(-\) In the realizable, and the non-realizable cases with sub-gaussian tailed data and Markovian data, we improve the bounds by a \( T\) factor. This improvement comes from analyzing the moment generating function of a martingale difference sequence with well-chosen coefficients. In these cases, we also remove the necessity of using the coupling-based argument used in the same work by . Instead, by solely making use of the problem structure, we arrive at the same conclusion that with high probability, the iterates of stochastic mirror descent for quadratically bounded losses behave as if the problem domain is bounded.

\(-\) In the non-realizable case with polynomial tailed data, we improve the existing bound by a \(\,T\) factor. Due to the polynomial dependency on \(\), being able to maintain the same success probability through all iterations is crucial in this case. Unlike the previous work, we rely on a truncation technique. Using a more refined analysis of the truncated random variables, in combination with suitable concentration inequalities and the coupling technique, we improve the existing bounds significantly.

### Related Work

Broadly speaking, there is a rich body of works in optimization and generalization that provide convergence guarantees and generalization bounds for stochastic methods. Earlier works often focus on in-expectation bounds , and bounds in high probability  for problems with bounded domains or under various additional assumptions such as strong convexity, noise with light tails. Recent developments for optimization  are able to handle unconstrained problems and relax these assumptions, but also require changes to the algorithm such as gradient clipping.

In generalization error analysis, specifically, a number of prior works, including , focus only on linearly separable data. Among these,  only deal with exponentially tailed losses while  show generalization bounds for general smooth convex losses. Our work, similarly to , goes beyond the realizable setting and specific losses. We show high probability generalization bounds in both realizable and non-realizable settings for the broad class of quadratically bounded losses, for both regression and classification problems.

Other related works include the line of works that examine generalization errors via algorithmic stability. The works by  show the generalization error of an arbitrary algorithm via a quantity called uniform stability. By bounding this quantity for specific algorithms on a fixed training dataset, they derive generalization bounds. Our work focuses on a different setting where we assume the algorithm has access to a fresh data sample in each iteration. In this regard, the setting in our work,as well as the prior work by , stays closer to the world of optimization. However, in contrast to the optimization world in which we commonly impose assumptions on the stochastic gradients (such as having bounded variance or sub-gaussian noise), we make assumptions on the data (such as sub-gaussian or polynomial tailed data). This difference introduces several challenges which we overcome in our work.

The main point of reference for this paper is the work by . This work develops a "coupling" technique to bound the generalization error of stochastic mirror descent for quadratically bounded losses. This technique has been employed in prior works [10; 11; 9; 29; 27; 22] to obtain high probability convergence bounds of stochastic methods in optimization. Our work improves their results by using a different approach that takes a closer look at the mechanism of the concentration inequalities and leverages the problem structure. When the data are bounded or have sub-gaussian tails, analyzing the moment generating function of a novel martingale difference sequence allows us to maintain the same success probability, without using either the coupling technique or the union bound. This new analysis, however, does not change the observation by  that the iterates of the unconstrained and the constrained problems coincide with high probability. When the data have a polynomial tail, we rely on a truncation technique. In this case, the coupling technique is necessary but not the union bound, and we are still able to significantly improve the success rate.

In terms of techniques, the work by  for optimization is the closest to ours. In this work, the authors develop the whitebox approach to analyzing stochastic methods for optimization with light-tailed noise. In this work, we study generalization errors. Moreover, in all settings, our choice of martingale difference sequences and coefficients are a significant departure from the prior work. In particular, in  the choice of coefficients only depends on the problem parameters whereas in the realizable case, our coefficients depend also on the historical data. Our approach also allows for a flexible use of an induction argument without decreasing the success probability, while in  the bounds are simpler and can be easily achieved in a single step.

## 2 Preliminaries

In this section, we provide the general set up and necessary notations before analyzing stochastic mirror descent in the subsequent sections. Overall, we closely follow notations used in .

**Domain and norms**. In this work, we consider \(\)--the domain of the problem--to be a closed convex set or \(^{d}\). We will use \(\|\|\) to denote an arbitrary norm on \(\) and let \(\|\|_{*}\) be its dual norm. We define the Bregman divergence as \(_{}(w;v)=(w)-(v)-(v),w-v\) where \(:^{d}\) is a differentiable function that is \(1\)-strongly convex with respect to the norm \(\|\|\).

**Loss functions**. Each loss function \(:_{ 0}\) in our consideration can be written using a convex scalar function \(\) in one of the two following forms: 1) \((y,)=((y))\) where \((y)=1\) if \(y 0\) and \(=-1\) otherwise; and 2) \((y,)=(y-)\). The first form captures classification losses and the second regression losses. We will assume that subgradients \(\) of \(\) in the second argument always exist, and let \(^{}\) denote a subgradient in \(\). For a function \(f\), we also use \(\| f(w)\|\{\|g\|:g f (w)\}\). We further make the following assumptions, introduced in  as quadratic boundedness and self-boundedness.

**Assumption 1**. We assume that \(\) is \((C_{1},C_{2})\)-quadratically-bounded, for some constants \(C_{1},C_{2} 0\), i.e., for all \(y,\)

\[|^{}(y,)| C_{1}+C_{2}(|y|+ ||).\]

This condition captures both classes of Lipschitz and smooth functions. Indeed, Lemma 1.2 from  shows that \(\)-Lipschitz functions are \((,0)\)-quadratically-bounded while \(\)-smooth functions are \((|(0)|,)\)-quadratically-bounded.

**Assumption 2**. In the realizable setting, we assume that \(\) is \(\)-self-bounding, i.e., \(\) satisfies \(^{}(z)^{2} 2(z)\) for all \(z\).

The second assumption is a generalization of smoothness. This assumption is satisfied by smooth losses but also certain non-smooth losses such as the exponential loss. This condition is necessary in the current analysis to prove \(1/T\) rates in the realizable setting. The readers can refer to [34; 35] for more detailed discussion on this assumption.

Assumptions 1 and 2 are satisfied by commonly used loss functions in machine learning. These include the logistic loss \((y,)=(1+(-y))\) and the squared loss \((y,)=(y-)^{2}\) (see Lemma 1.4 in ).

For the loss function \(\) and the configuration \(w\), and sample \((x,y)\) where \(x\) denotes the attribute and \(y\) the label, we will write \(_{x,y}=(y,w^{T}x)\). We state the following crucial lemma which is the same as Lemma A.1 in , whose proof will be omitted.

**Lemma 1** (Lemma A.1 in ()).: _Suppose \(\) is \((C_{1},C_{2})\)-quadratically-bounded and \(B_{x} 0\) is given. Given \((x,y)\) such that \(\{\|x\|_{*},|y|\} B_{x}\) and any \(u,v\),_

\[\|_{x,y}(u)\|_{*}  B_{x}(C_{1}+C_{2}B_{x}(1+\|u|))\] \[|_{x,y}(u)-_{x,y}(v)|  B_{x}\|u-v\|(C_{1}+C_{2}B_{x}(1+\| u\|)).\]

Risk, IID and Markovian data.When sample \((x_{i},y_{i})\) arrives in iteration \(i\) of an algorithm, we will use the notation \(_{i}(w)=(y_{i},w^{T}x_{i})\). For an algorithm of \(T\) iterations, we use \(_{t}=((x_{1},y_{1}),,(x_{t},y_{t}))\) to denote the natural filtration up to and including time \(t\). When the data are IID and generated from a distribution \(\), we define the risk

\[(w)=_{(x,y)}[(y,w^{T}x)];\]

In contrast to IID data, Markovian data come from a stochastic process. This setting has also been considered in . We let \(P_{*}^{t}\) be the distribution of \((x_{t},y_{t})\) at iteration \(t\) conditioned on \(_{s}\). We make the following assumption regarding the uniform mixing time of the stochastic process. Note that similar assumptions have also appeared in .

**Assumption 3**.: We assume that for some \(, 0\) of our choice, there is a distribution \(\) such that

\[_{t_{ 0}}_{_{t}}(P_{t}^{t+ },).\]

We refer to the triple \((,,)\) as an approximate stationarity witness. We then define the risk according to the approximate stationary distribution \(\): \((w)=_{(x,y)}[(y,w^{T}x)].\)

Algorithm.Stochastic Mirror Descent is given in Algorithm 1. In this algorithm, for the simplicity of the analysis, we consider a fixed step size \(\). In each iteration, we pick a subgradient \(g_{t}_{t}(w_{t-1})\) and perform the update step.

We finally introduce a standard lemma used in the analysis of Stochastic Mirror Descent.

**Lemma 2**.: _For \(t 0\) and \(w_{}\), we have_

\[_{}(w_{};w_{t+1})-_{} (w_{};w_{t})(_{t+1}(w_{})-_{t+1}(w_{t}))+}{2}\|g_{t+1} \|_{*}^{2}.\]

Other notations.We will use \(w_{}\) to refer to a comparator of interest. For the simplicity of the exposition, we let \(D_{0}=_{}(w_{};w_{0})\), and \(^{*}=_{v}(v)\). For a loss function \(\) that is \((C_{1},C_{2})\)-quadratically-bounded, we let \(C_{4}=C_{1}+C_{2}(1+\|w_{}\|)\).

## 3 Generalization bounds of SMD for IID data

In this section, we distinguish between two cases: the realizable case and the non-realizable case. In the realizable case, there exists an optimal solution \(w^{*}\) such that \((w^{*})=0\). We will show that under mild assumptions, the risks of the solutions output by Algorithm 1 are bounded by \(O(1/T)\). In the non-realizable case, we will show, on the other hand, a weaker statement that the excess risks of the solutions are bounded by \(O(1/)\).

### Realizable case

In the realizable case, the comparator \(w_{}\) is not necessarily the global minimizer. To show the \(1/T\) rate, we will assume \(w_{}\) satisfies \((w_{})_{}(w_{};w_{0})/T\) and that the loss at \(w_{}\) is bounded. The guarantee for the iterates of Algorithm 1 is provided in Theorem 3.

**Theorem 3**.: _Suppose \(\) is convex, \((C_{1},C_{2})\)-quadratically-bounded, and \(\)-self-bounding. Given \(T\), \(((x_{t},y_{t}))_{t T}\) are IID samples with \(\{\|x_{t}\|_{*},|y_{t}|\} 1\) almost surely, \(w_{}\) satisfies \((w_{})_{}(w_{};w_{0})/T\), and \(_{t<T}_{t+1}(w_{}) C_{3}\) almost surely. Then for \(\), with probability at least \(1-2\), for every \(0 k T-1\)_

\[_{t=0}^{k}(w_{t})+_{}( w_{};w_{k+1})}{5(k+1)}+3(w_{ }).\]

_where \(C=}{5}D_{0}+4 C_{ 3}}+(D_{0}+ C_{3})\) with \(=\{1,\}\)._

The analysis of Theorem 3 relies on the use of concentration inequalities. In contrast to existing works that utilize concentration inequalities as a blackbox, we will make use of the mechanism for proving concentration inequalities in order to obtain stronger guarantees. The type of concentration inequalities we consider are shown by analyzing the moment generating function of suitably chosen martingale sequences. We will use Lemma 14 (Appendix) which gives a basic inequality that bounds the moment generating function of a bounded random variable. To start the analysis, we use Lemma 2 and Assumption 2 to obtain

**Lemma 4**.: _For all \(t 0\), we have_

\[_{}(w_{};w_{t+1})-_{} (w_{};w_{t})_{t+1}(w_{} )-_{t+1}(w_{t}),\]

\[_{}(w_{};w_{t}) _{}(w_{};w_{0})+_{i=1}^{t}_{ i}(w_{})=D_{0}+_{i=1}^{t}_{i}(w_{ }).\]

First, let us pay attention to the term \(_{i=1}^{t}_{i}(w_{})\). Recall that the terms \(_{i}(w_{})\) are non-negative and bounded by a constant \(C_{3}\) almost surely. We can analyze the term \(_{i=1}^{T}_{i}(w_{})\) which upper bounds all sums \(_{i=1}^{t}_{i}(w_{})\) by studying its moment generating function (or via a concentration inequality). We state this bound in the next lemma and defer the proof to the appendix.

**Lemma 5**.: _With probability at least \(1-,_{i=1}^{T}_{i}(w_{})T (w_{})+C_{3}\)._

Lemma 4 and lemma 5 and the assumption that \((w_{})=O(1/T)\) imply that with probability at least \(1-\), \(_{}(w_{};w_{t})\) is bounded. In other words, with probability at least \(1-\), the iterates \(w_{t}\) all lie in a bounded region. One could therefore proceed to assume that the problem domain is simply this bounded ball around \(w_{}\). This is the basic idea behind the "coupling" technique demonstrated in . However, the important question is how to obtain a bound for the risk of all iterates even when we are working with a problem with unbounded domain. Here, not paying close attention to the structure of the problem and the blackbox use of concentration inequalities lead to suboptimal bounds. On the other hand, as discussed above, a crucial novelty in our analysis is the choice of a supermartingale difference sequence, defined in the proof below. By working from first principles using moment generating function of this sequence, we derive two conclusions: 1) an improved risk bound can be obtained, and 2) the coupling technique is not necessary.

Proof Sketch.: Towards bounding the risk \(_{t=0}^{k}(w_{t})\), we define random variables

\[Z_{t} =z_{t}((w_{t})-(w_{ })-_{t+1}(w_{}))+z_{t}( _{}(w_{};w_{t+1})-_{} (w_{};w_{t}))\] \[-z_{t}((w_{})+ (w_{t})), 0 t T-1\] \[z_{t} =+2D_{0}+2_{i=1 }^{t}_{i}(w_{})}};=\{1, \}\]and we let \(S_{t}=_{i=0}^{t}Z_{i}\); \( 0 t T-1\). The reason to define these variables is because from Lemma 4, we can bound

\[[(Z_{t})_{t}] (z_{t}((w_{})+ (w_{t})))\] \[ [(z_{t}((w _{t})-(w_{})-_{t+1}(w_{}) )+z_{t}(_{t+1}(w_{})- _{t+1}(w_{t})))_{t}]\] \[= [(z_{t}((w _{t})-(w_{})+_{t+1}(w_{})-_{t+1}( w_{t})))_{t}]\]

where now inside the expectation, we have the term \((w_{t})-(w_{})+_{t+1}(w_{}) -_{t+1}(w_{t})\) which has expectation \(0\). Let \(C_{4}=C_{1}+C_{2}(1+\|w_{}\|)\), we can bound

\[|((w_{t})-(w_{})+ _{t+1}(w_{})-_{t+1}(w_{t}))| C_{4} \|w_{}-w_{t}\|\]

and \(z_{t}\|w_{}-w_{t}\|}\). Now we can apply Lemma 14 (Appendix) to bound

\[[(Z_{t})_{t}] (z_{t}((w_{})+ (w_{t})))\] \[ (z_{t}^{2}^{2}[ ((w_{t})-(w_{})+_{t+1}(w_{})-_{t+1}(w_{t}))^{2}_{t}])\] \[ (z_{t}^{2}^{2}[( _{t+1}(w_{})-_{t+1}(w_{t}))^{2}_{t} ])\] \[ (z_{t}^{2}^{2}C_{4}\|w_{ }-w_{t}\|[_{t+1}(w_{})+_{t+1}(w_{t}) _{t}])\] \[ (z_{t}((w_{} )+(w_{t})))\]

Therefore \([(Z_{t})_{t}] 1\) and hence \(((S_{t}))_{t 0}\) is a supermartingale. By Ville's inequality, we have with probability at least \(1-\), for all \(0 k T-1\)

\[_{t=0}^{k}Z_{t}\]

Expanding this inequality, in combination with Lemma 5, we obtain the conclusion. 

_Remark 6_.: The new analysis does not change the conclusion observed in --that is, with high probability, the iterate sequence \((w_{t})_{t 0}\) behaves as if the domain of the problem is bounded. We improve the probability that this event happens.

### Non-realizable case

In the non-realizable case, we do not aim for \(1/T\) but only \(1/\) rates. Hence we do not assume that the comparator \(w_{}\) satisfies \((w_{})_{}(w_{};w_{0})/T\) but rather the following assumption on the excess risk:

**Assumption 4**. Let \(^{*}=_{v}(v)\), assume that \((w_{})-^{*}_{}(w_{ };w_{0})}{}\).

We also relax the assumption on the data samples. In the previous case, the data are bounded, i.e \(\{\|x\|_{*},|y|\} 1\) a.s. We will consider in this section two settings, one when the data come from a sub-Gaussian distribution and one when the data distribution has a polynomial tail.

**Remark on Theorem 10 in **. There seems to be an issue with the proof of Theorem 10 in . The proof uses a variant of Azuma's inequality (, Problem 3.11) which allows the ranges of the random variables to not be specified up front. However, when bounding the range of relevant random variables, the proof uses \(Z_{t+1}\), which is correlated with the data \(\|x_{t+1}\|_{*}\) and \(y_{t+1}\). Thus the condition of Azuma's inequality is not satisfied. We do not see an immediate way to resolve this problem. In the following, we consider separately the two cases of sub-Gaussian and polynomial tailed data for which we use different proof techniques to show the error bounds. In the first case of IID data with sub-Gaussian tails, we proceed by bounding the moment generating function of a well-chosen martingale sequence. In the second case of IID data with polynomial tails, we introduce a truncation technique. In both cases, we are able to obtained better bounds compared with .

#### 3.2.1 IID data with sub-Gaussian tails

We will show the following guarantee:

**Theorem 7**.: _Suppose \(\) is convex, \((C_{1},C_{2})\)-quadratically-bounded. Given \(T\), \(((x_{t},y_{t}))_{t T}\) are IID samples with \(Q_{t}=\{1,\|x_{t}\|_{*}^{2},|y_{t}|^{2}\}\) and there exists \( 0\) such that for all \(\)_

\[\{[((Q_{t}^{2}-[Q _{t}^{2}]))],[((Q_ {t}-[Q_{t}]))]\}( ^{2}^{2})\]

_Let \(_{1}=[Q_{t}]\) and \(_{2}=[Q_{t}^{2}]\). Suppose that \(w_{}\) satisfies Assumption 4. Then for \(+2}}}\), with probability at least \(1-2\), for every \(0 k T-1\)_

\[_{t=0}^{k}((w_{t})-^{*})+ _{}(w_{};w_{k+1})}{(k+1 )}}{(k+1)}\]

_where \(R^{2}=16C_{4}^{2}(^{2}+4_{1}^{2}) ^{2}T+4D_{0}(1+)+4^{2}C_{4}^{2}(T_{2}+2})=O(1)\)._

_Remark 8_.: For zero-mean sub-Gaussian variable \(X\), the definition \([( X)](^{2} ^{2})\) for all \(\) is equivalent to \([(^{2}X^{2})]( ^{2}^{2})\) for all \(0\) (see ). The lemma below shows a property of sub-Gaussian variables under scaling and translating. First let us consider \(_{t=1}^{T}Q_{t}^{2}\). Similar to Lemma 5, by bounding the moment generating function of this term, we have the following (see also Section B4 in ).

**Lemma 9**.: _With probability at least \(1-\), \(_{t=1}^{T}Q_{t}^{2} T_{2}+2}\)._

Proof of Theorem 7.: The proof of this Theorem uses the technique developed in . We will also analyze the moment generating function of a suitable martingale sequence. However, the choice of the coefficients will differ significantly from the previous proof. In this case the structure of the problem is deeply integrated into the analysis of the martingale. We define

\[Z_{t} =z_{t}((w_{t})-(w_{}) )+z_{t}(_{}(w_{};w_{t+1})- _{}(w_{};w_{t}))\] \[-z_{t}^{2}\|g_{t+1}\|_{*}^{2}-4z_{ t}^{2}^{2}C_{4}^{2}(^{2}+4_{1}^{2})_{} (w_{};w_{t})  0 t T-1\]

where \(z_{t}=C_{4}^{2}(^{2}+4_{1}^{2})(T+ t+1)}\)\(-1 t T-1\)

and let \(S_{t}=_{i=0}^{t}Z_{i}; 0 t T-1\). By Lemma 2, we have

\[Z_{t}+4z_{t}^{2}^{2}C_{4}^{2}(^{2}+4_{1}^{2})_{}(w_{};w_{}) z_{t}( (w_{t})-(w_{})+_{t+1}(w_{})-_{t+1}(w_{t}))\]

where we have \([((w_{t})-(w_{})+_{t+ 1}(w_{})-_{t+1}(w_{t}))]=0\), and using the same notation \(C_{4}=C_{1}+C_{2}(1+\|w_{}\|)\), by Lemma 1,

\[|((w_{t})-(w_{})+_ {t+1}(w_{})-_{t+1}(w_{t}))|\] \[ |_{t+1}(w_{})-_{t+1}(w_{t })|+|(w_{t})-(w_{})|\] \[ |_{t+1}(w_{})-_{t+1}(w_{ t})|+[|_{x,y}(w_{t})-_{x,y}(w_{ })|]\] \[ (Q_{t}+_{1})\|w_{}-w_{t}\|C_ {4}=((Q_{t}-_{1})+2_{1})\|w_{}-w_{ t}\|C_{4}\]

Hence applying Lemma 15,we have

\[[(Z_{t})_{t}] (4z_{t}^{2}^{2}C_{4}^{2}(^{2}+4_{1}^{2}) _{}(w_{};w_{t}))\] \[= [(z_{t}_{t}((w_{t})- (w_{})+_{t+1}(w_{})-_{t+1} (w_{t})))_{t}]\]\[ (2z_{t}^{2}^{2}C_{4}^{2}\|w_{}-w_{t} \|^{2}(^{2}+4_{1}^{2}))\] \[ (4z_{t}^{2}^{2}C_{4}^{2}(^{2}+4_{1}^{2 })_{}(w_{};w_{t}))\]

Therefore \([(Z_{t})_{t}] 1\) and hence \(((S_{t}))_{t 0}\) is a supermartingale. By Ville's inequality, we have with probability at least \(1-\), for all \(0 k T-1\)

\[_{t=0}^{k}Z_{t}\]

Expanding this inequality we have

\[_{t=0}^{k}z_{t}(w_{t})+z_{k}_{} (w_{};w_{k+1})\] \[ +z_{-1}D_{0}+(w_{ })_{t=0}^{k}z_{t}+_{t=0}^{k}z_{t}^{2}\|g_{t+1} \|_{*}^{2}\] \[+_{t=0}^{k}+4z_{t}^{2}^{2}C_{4}^{ 2}(^{2}+4_{1}^{2})-z_{t-1})}_{ 0}_{} (w_{};w_{t})\] \[}{{}} +z_{-1}D_{0}+(w_{ })_{t=0}^{k}z_{t}+_{t=0}^{k}z_{t}^{2}\|g_{t+1} \|_{*}^{2}\]

where for \((a)\), by the choice of \(z_{t}=C_{4}^{2}(^{2}+4_{1}^{2})(T+1+t)}\) we have \(z_{t-1}-z_{t} 4z_{t}^{2}^{2}C_{4}^{2}(^{2}+4_{1}^{2} ).\) We highlight that this is where the structure of the problem comes into play. That is, by setting appropriate coefficients, we can leverage gain in the distance in the martingale difference sequence (\((z_{t}-z_{t-1})_{}(w_{};w_{t})\)) to cancel out the loss from bounding the moment generating function (\(4z_{t}^{2}^{2}C_{4}^{2}(^{2}+4_{1}^{2})_{ }(w_{};w_{t})\)). Another important property of the sequence \((z_{t})\) is that it is a decreasing sequence and \(}{z_{t}} 2\) for all \(t,k\). Hence we have

\[_{t=0}^{k}((w_{t})-^{*}) +_{}(w_{};w_{k+1})\] \[ 4C_{4}^{2}(^{2}+4_{1}^{2})^{2}(T+1+k)+2D_{0}+2((w_{})- ^{*})(k+1)+^{2}_{t=0}^{k}\|g_{t+1}\|_ {*}^{2}.\]

Combined with Lemma 9, with probability at least \(1-2\), for all \(0 k T-1\)

\[_{t=0}^{k}((w_{t})-^{*}) +_{}(w_{};w_{k+1})\] \[ 4C_{4}^{2}(^{2}+4_{1}^{2})^{2}(T+1+k)+2D_{0}+2((w_{}) -^{*})(k+1)+^{2}_{t=0}^{k}\|g_{t+1}\|_ {*}^{2};\]

and \(_{t=1}^{k+1}Q_{t}^{2} T_{2}+2}\)

Conditioned on this event, we will prove by induction that

\[_{}(w_{};w_{k})\] \[ R^{2} 16C_{4}^{2}(^{2}+4_{1}^{2}) ^{2}T+4D_{0}+4D_{0}+4^{2}C_{4}^{2}(T _{2}+2})\]

For the base case \(k=0\), it is trivial. Suppose for all \(t k\) we have \(_{}(w_{};w_{t}) R^{2}\), now we prove for \(t=k+1\). By Lemma 1,

\[^{2}_{t=0}^{k}\|g_{t+1}\|_{*}^{2}^{2}_{t=0}^{k}Q _{t+1}^{2}(C_{1}+C_{2}(1+\|w_{t}\|))^{2} ^{2}_{t=0}^{k}Q_{t+1}^{2}(C_{4}+C_{2}\|w_{t}-w_{}\|)^{2}\]\[ 2^{2}C_{4}^{2}_{t=1}^{k+1}Q_{t}^{2}+2^{2}C_{2}^{2} _{t=0}^{k}Q_{t+1}^{2}\|w_{t}-w_{}\|^{2}\] \[ ^{2}(2C_{4}^{2}+4C_{2}^{2}R^{2})(T_{2}+2 })\]

Therefore

\[_{}(w_{};w_{k+1})  8C_{4}^{2}(^{2}+4_{1}^{2})^{2}T+2D_{0}+2((w_{})-^{*} )(k+1)\] \[+^{2}(2C_{4}^{2}+4C_{2}^{2}R^{2})(T_ {2}+2})\] \[}{2}+4^{2}C_{2}^{2}(T_{2}+2 {T})R^{2} R^{2}.\]

Finally we obtain, \(_{t=0}^{k}((w_{t})-^{*})+_ {}(w_{};w_{k+1}) R^{2},\) as needed. 

#### 3.2.2 IID data with polynomial tails

**Theorem 10**.: _Suppose \(\) is convex, \((C_{1},C_{2})\)-quadratically bounded. Given \(T\), \(((x_{t},y_{t}))_{t T}\) are IID samples with \(Q_{t}=\{1,\|x_{t}\|_{*}^{2},|y_{t}|^{2}\}\) and for some \(p 2\) there exists \(M\) such that for all \(\)_

\[\{_{2 r 2p}\{[|Q_{t}- [Q_{t}]|^{r}]\},_{2 r p}\{ [|Q_{t}^{2}-[Q_{t}^{2}]|^{r} ]\}\} M\]

_Let \(_{1}=[Q_{t}]\) and \(_{2}=[Q_{t}^{2}]\). Suppose that \(w_{}\) satisfies Assumption 4. Then for \(+2M( )^{})}}\), with probability at least \(1-3\), for every \(0 k T-1\)_

\[_{t=0}^{k}((w_{t})-^{*})+ _{}(w_{};w_{k+1})}{(k+1 )}}{2(k+1)}\]

_where \(R=\{(1+)+^{2}C_{4}^{2} (T_{2}+2M()^{}) )},6((7()^{1/2p} +2_{1})+T_{2} C_{4}})\} =O(1)\), \(=\{1,\}\)._

_Remark 11_.: Since \(p 2\), the rate is \(O(}+}()^{})\). This rate improves over the \(O((}+}() ^{}))\) rate by  by a polynomial factor \(T^{}\) in the high probability regime where \(=(T)}\).

We will give a proof sketch for this theorem. The full proof is deferred to the appendix.

Proof Sketch.: The heavy tailed distribution of the data does not allow us to analyze the moment generating function. In this case, we rely on the coupling technique as in . Since it is not possible to apply Azuma's inequality due to the bounds on the variables being not measurable, and the variables are heavy tailed, we use truncation technique. We define,

\[v_{t}=_{\|w-w_{}\| R}\{ _{t}g_{t}(v_{t-1}),w+_{}(w;v_{t-1})\}\]

where we use \(g_{t}(v_{t-1})\) to denote the gradient at \(v_{t-1}\) using the same data point \((x_{t},y_{t})\) when computing \(w_{t}\) and we define

\[U_{t}=((v_{t})-(w_{})+_{t+1}(w _{})-_{t+1}(v_{t}))\]\[P_{t} =U_{t}&|U_{t}|(A+2_{1} )RC_{4}\\ (A+2_{1})RC_{4}(U_{t})& \] \[A =()^{1/2p}B_{t}=U_{t}-P_{t}.\]

We can write

\[_{t=0}^{k}U_{t}=_{t=0}^{k}(P_{t}-[P_{t}_{t}])+_{t=0}^{k}[P_{t}_{t} ]+_{t=0}^{k}B_{t}\]

We bound \(_{t=0}^{k}(P_{t}-[P_{t}_{t}])\) by applying Freedman's inequality. The terms \(_{t=0}^{k}[P_{t}_{t}]\) and \(_{t=0}^{k}B_{t}\) are both the bias terms can be bounded by analyzing the tail of the distribution and Markov's inequality. We also use Lemma 12 to bound \(_{t=0}^{k}\|y_{t+1}(v_{t})\|_{*}^{2}\). Finally, using the induction technique, we can prove that \(w_{t}=v_{t}\) with high probability and obtain the desired result. 

**Lemma 12** (Lemma A.5 from ).: _With probability \( 1-\), \(_{t=1}^{T}Q_{t}^{2} T_{2}+2M()^ {}.\)_

## 4 Generalization bounds of SMD for Markovian data

The final result we present in this paper is the following theorem for the case when the data are sampled from a Markov chain.

**Theorem 13**.: _Suppose \(\) is convex, \((C_{1},C_{2})\)-quadratically bounded. Given \(T\), \(((x_{t},y_{t}))_{t T}\) are sampled from a Markov chain with \(\{\|x_{t}\|_{*}^{2},|y_{t}|^{2}\} 1\) and \((,,=})\) is an approximate stationarity witness. Suppose that \(w_{}\) satisfies Assumption 4. Then for \(}\), with probability at least \(1-\), for every \(0 k T-1\)_

\[_{t=0}^{k}((w_{t})-^{*})+ _{}(w_{};w_{k+1})}{(k+1 )}}{2(k+1)}.\]

_where \(R=\{+2 D_{0}+16^{2}C_{4}^{2}T +2T^{2}C_{4}^{2}+4^{2} TC_{4}^{2})},6(2  C_{4}+2 C_{4} T+4 C_{4})\}=O(1)\) and \(C_{4}=C_{1}+C_{2}(1+\|w_{}\|)\)._

We will give a proof sketch for this theorem.

Proof Sketch.: The proof of this Theorem follow similarly to that of Theorem 7. The difference here is we need to bound \(\) different martingale difference sequences in the form of

\[[_{(i+1)+j}(w_{}) _{ i+j}]-[_{(i+1)+j}(w_{ i+j}) _{ i+j}]+_{(i+1)+j}(w_{}) -_{(i+1)+j}(w_{ i+j})\]

for \(0 j-1\), \(0 i\). We also need the assumption on the approximate stationarity witness to see that

\[|(w_{t})-(w_{})-[_{t+ }(w_{})_{t}]+[ _{t+}(w_{t})_{t}]| C_{4}R.\]

Now we only need the union bound over \(\) sequences, instead of all iterations. The success probability will decrease from \(1-\) to \(1-\). 

## 5 Conclusion

In this paper, we show a new approach to analyze the generalization error of SMD for quadratically bounded losses. Our approach improves a logarithmic factor for the realizable setting and non-realizable setting with light tailed data and a \(\)\(T\) factor for the non-realizable setting with polynomial tailed data from the prior work by . An inherent limitation of the current approach is the assumption that we can obtain a fresh sample in each iteration, whereas the setting with finite training data is still not well understood. In the realizable setting, we require that the data is bounded, as opposed to more relaxed assumptions in the non-realizable settings. We leave the question of resolving these issues for future works.