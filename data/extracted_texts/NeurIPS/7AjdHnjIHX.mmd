# COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs

Tiep Le

Intel Labs

tiep.le@intel.com

&Vasudev Lal

Intel Labs

vasudev.lal@intel.com

&Phillip Howard

Intel Labs

phillip.r.howard@intel.com

Equal contribution

###### Abstract

Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation. We make our code2 and the COCO-Counterfactuals dataset3 publicly available.

## 1 Introduction

While vision and language models have achieved remarkable performance improvements in recent years, out-of-domain (OOD) generalization remains a challenge for even the best models, which typically exhibit much lower performance in zero-shot evaluation settings than on withheld in-domain test sets. This has often been attributed to spurious correlations between non-causal features and labels in datasets which can be exploited during training as shortcuts to achieving artificially high in-domain performance (Geirhos et al., 2020). For example, image recognition models often learn to utilize spurious features in the backgrounds of images when trained for classification on datasets such as ImageNet (Singla and Feizi, 2021; Xiao et al., 2020).

Augmenting training datasets with counterfactuals, which study the impact on a response variable following a change to a causal feature, has been previously proposed as a strategy for countering this effect in NLP models (Levesque et al., 2012; Kaushik et al., 2019). Motivated by concepts in causal learning (Feder et al., 2022), these methods typically form counterfactual examples by making minimal edits to an input text such that a corresponding label or attribute of the text (e.g., sentiment)is changed. Training models with counterfactual examples therefore provides a strong inductive bias against learning spurious correlations in datasets, leading to greater robustness and improved generalization on OOD data (Eisenstein, 2022; Vig et al., 2020) as well as enabling better domain adaptation in low resource settings (Calderon et al., 2022).

Despite its success in the realm of NLP, the application of counterfactual data augmentation to multimodal vision-language models has largely remained unexplored, mainly due to low-resource settings involving multimodal data and challenges associated with creating paired counterfactual examples spanning multiple modalities. For example, consider the task of creating counterfactual examples for a multimodal dataset containing images with associated text captions. Creating a counterfactual to a given image-text example requires not only minimally editing a casual feature in the text caption, but also making a corresponding minimal edit to the image which ideally modifies only the changed causal feature while preserving other spurious features from the original image.

Collecting such counterfactual examples from existing image datasets is infeasible due to the massive variation in natural images that can accurately depict even identical text captions. While manual creation of counterfactual examples by humans is an option that has been employed previously for NLP (Kaushik et al., 2019; Gardner et al., 2020), this approach suffers from a lack of scalability due to the high cost of human labor, which would be compounded even further for multimodal counterfactuals due to the need for both text and image editing skills. Given these challenges, how can paired image-text counterfactual examples be created at the scale needed for effective model evaluation and data augmentation?

We address this problem by introducing a novel data generation pipeline for automatically creating multimodal counterfactual examples using text-to-image diffusion models. Our approach minimally edits captions from an existing image-text dataset and then leverages Stable Diffusion (Rombach et al., 2021) with cross-attention control (Hertz et al., 2022) to generate pairs of images with minimal differences (i.e., isolated to the counterfactual change). We employ our data generation pipeline to create at scale **COCO-Counterfactuals** (Figure 1), a counterfactual variant of the MS-COCO dataset (Lin et al., 2014).

We validate the quality of COCO-Counterfactuals using human evaluations and conduct zero-shot experiments showing that state-of-the-art multimodal models are challenged by our generated counterfactual examples. Our additional experiments show that training CLIP (Radford et al., 2021)

Figure 1: Examples of COCO-Counterfactuals, our minimal-edit counterfactuals dataset for images with paired text captions.

on COCO-Counterfactuals improves its performance on multiple OOD datasets, including zero-shot tasks not seen during training. We make COCO-Counterfactuals and the code for our counterfactual data generation pipeline publicly available under the CC BY 4.0 License.

## 2 Related Work

### Counterfactual Examples for NLP

Counterfactual data augmentation has been shown to improve the robustness of models across a wide range of problem domains in NLP. Kaushik et al. (2019) demonstrated that human-authored counterfactuals pose a significant challenge for existing models and that augmenting training datasets with counterfactual examples improves sentiment analysis and Natural Language Inference classifiers. Gardner et al. (2020) similarly used human experts to author minimally-edited contrast example sets for 10 NLP datasets and showed that model performance evaluated on them drops substantially.

A number of approaches have been proposed to move beyond reliance on human authors towards automated methods for generating counterfactual examples. Wang and Culotta (2021) and Yang et al. (2021) automatically construct counterfactual examples by identifying and removing or replacing potentially causal words. Howard et al. (2022) introduce a framework for generating looser counterfactuals which allow larger edits of original examples, resulting in more natural and linguistic counterfactual examples. Other semi-automated methods have been proposed to generate counterfactual examples while still relying on human input or labeling (Wu et al., 2021). To the best of our knowledge, none of these existing approaches for automatic counterfactual generation have been extended to multimodal image-text datasets.

### Image Benchmarks for Measuring Spurious Correlations

Several image datasets have been proposed as benchmarks for measuring the degree to which models have learned to rely on spurious correlations during training. _CelebA Hair Color_(Liu et al., 2015) is a binary image classification dataset that labels whether a person depicted has a blonde hair color, which is spuriously correlated with gender. Sagawa et al. (2019) constructed the _Waterbirds_ dataset by cropping images of landbirds or seabirds onto land and sea backgrounds, resulting in a binary classification task for bird type (i.e., landbird or seabird) in the presence of spurious correlations with the background. _Colored MNIST_(Arjovsky et al., 2019) artificially imposes colors on the MNIST handwritten digits dataset, where the color is spuriously correlated with the class label. Lynch et al. (2023) uses text-to-image models to generate _Spawrious_, an image classification dataset of four dog breeds spuriously correlated with six background locations. Unlike COCO-Counterfactuals, these datasets are limited only to image classification over a small number of labels and are primarily suited for evaluating model robustness as opposed to training data augmentation.

Thrush et al. (2022) introduced _Winoground_, an image-text dataset aimed at measuring visio-linguistic compositionality. Given two images and two captions which have the same words but in different order, the task is to correctly match each caption to its corresponding image. While their paired image-text examples can be viewed as counterfactuals, they focus only on edits to word order and rely on humans to create a dataset aimed specifically at evaluating compositionality. In contrast, our method automatically generates counterfactual examples with word content changes while also preserving non-causal spurious features across paired counterfactual images.

_FOIL-COCO_(Shekhar et al., 2017) contains 'foil' captions with a single change to the original MS-COCO caption to invalidate it for the accompanying image. They show that vision and language models struggle to correctly classify captions, detect the edited word, and correct the foiled caption. Our image-text counterfactuals similarly create 'foil' captions to MS-COCO captions, but goes further by also creating paired images which differ only according to how the caption was edited.

### Data Augmentation with Synthetic Images Generated from Text-to-image Models

Motivated by recent advances in text-to-image diffusion models (Nichol et al., 2021; Rombach et al., 2021; Saharia et al., 2022; Ramesh et al., 2022), data augmentation with synthetically-generated images has emerged as a growing topic of interest. He et al. (2022) showed that images generated by GLIDE (Nichol et al., 2021) for specific classes in image recognition datasets can be used for training to improve performance on the corresponding image classification tasks. Trabucco et al. (2023) perform image-to-image transformations for data augmentation using text-to-image diffusion models, observing improvements in few-shot image classification performance. Vendrow et al. (2023) represent class labels from image recognition datasets as custom tokens in the vocabulary of a text-to-image diffusion model, enabling them to generate images of objects from the original dataset under different domain shifts. While our data generation pipeline also leverages text-to-image diffusion models, our approach differs from prior work in our focus on producing minimal changes to paired image-text data in both the vision and language modalities.

## 3 COCO-Counterfactuals

We detail our data generation methodology for creating COCO-Counterfactuals (COCO-CFs), a synthetic multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset (Lin et al., 2014). While we showcase our methodology by generating and releasing the COCO-Counterfactuals dataset, our approach can be applied to automatically construct multimodal counterfactuals for any dataset containing image captions.4

### Creating Counterfactual Captions

Given an original image caption \(C_{o}\), our first task is to create a corresponding counterfactual caption \(C_{c}\) which alters a subject of \(C_{o}\) while preserving most of its original details. The altered subject represents the changed causal feature in our counterfactual example while the remaining preserved details from the original caption can be viewed as potentially spurious correlated features.

To alter a subject of \(C_{o}\), we first identify all nouns using NLTK (Bird et al., 2009) as candidate words for substitution5 For each of the \(i\{1,..,n\}\) identified nouns, we create 10 candidate counterfactual captions by replacing only the \(i\)-th noun in \(C_{o}\) with the [MASK] token and retrieving the top-10 most probable replacements via masked language modeling (MLM)6. This produces a total of \(n 10\) candidate counterfactual captions, which we then filter to retain only those in which the substituted word is also a noun.

Our aim is to substitute nouns with alternative words that represent different subjects, and yet still maintain ontological similarity to the original noun. Hence, we use a pre-trained sentence similarity model7 to measure the similarity between each candidate counterfactual caption and the original caption \(C_{o}\), keeping only those candidates which have a sentence similarity within the range \((0.8,0.91)\). Finally, we use GPT-2 to score the perplexity of all candidates which remain after filtering and choose the candidate having the lowest perplexity as our counterfactual caption \(C_{c}\).

### Generating Counterfactual Images

After creating a counterfactual caption \(C_{c}\), our next task is to generate synthetic images \(I^{s}_{o}\) and \(I^{s}_{c}\) from the original caption \(C_{o}\) and counterfactual caption \(C_{c}\) (respectively). Ideally we would like \(I^{s}_{o}\) and \(I^{s}_{c}\) to differ only in terms of the noun which was replaced in \(C_{o}\) to produce \(C_{c}\), thereby enabling the changed causal feature to be learned in the presence of other potentially spurious correlated features (i.e., the unchanged details between \(C_{o}\) and \(C_{c}\)). However, this is a challenge for existing text-to-image generation models as minor changes to a text prompt can produce significantly different images. For instance, prompting Stable Diffusion with the captions "A small child lounges with a _remote_ in his hand" and "A small child lounges with a _toy_ in his hand" may produce images that differ not only in the object that the child is holding, but also in other details such as his facial features, the manner in which he is laying, the color of his clothes, and the image background.

To address this issue, Hertz et al. (2022) proposed a methodology called Prompt-to-Prompt which injects cross-attention maps during the diffusion process to control the attention between certain pixels and tokens of the prompt during denoising steps. This enables separate generations from text-to-image diffusion models to maintain many of the same image details while isolating their differences according to how the text prompts differ. An example of counterfactual image-text pairs \((C_{o},I_{o}^{s})\) and \((C_{c},I_{c}^{s})\) generated with and without prompt-to-prompt is shown in Figure 2, illustrating how Prompt-to-Prompt enables the principle of minimal text edits for NLP counterfactuals to be extended to image generation.

Brooks et al. (2023) noted that making different changes to images may require varying the parameter \(p\) in Prompt-to-Prompt, which controls the number of denoising steps with shared attention weights. For example, changes that require more substantial structural modifications to the image may necessitate less overall similarity between the resulting images and thus fewer shared attention weights. We therefore adopt their proposed approach of over-generating 100 image pairs with Prompt-to-Prompt by randomly sampling values of the parameter \(p U(0.1,0.9)\)8. The resulting 100 image pairs are filtered using CLIP (Radford et al., 2021) to ensure a minimum cosine similarity of 0.2 between the encoding of each caption and its corresponding generated image, with the best image pair \((I_{o}^{s},I_{c}^{s})\) chosen from those which remain according to the directional similarity in CLIP space (Gal et al., 2022):

\[_{dir}=(C_{c})-E_{T}(C_{o}))(E_{I}(I_{c}^{s})-E_{ I}(I_{o}^{s}))}{||E_{T}(C_{c})-E_{T}(C_{o})||\;||E_{I}(I_{c}^{s})-E_{I}(I_{o}^{s})||}\] (1)

where \(E_{T}\) and \(E_{I}\) are CLIP's text and image encoders (respectively). The CLIP\({}_{dir}\) metric measures the consistency in changes between the two images \((I_{o}^{s},I_{c}^{s})\) and their corresponding captions \((C_{o},C_{c})\). Thus, selecting images with a higher CLIP\({}_{dir}\) improves the overall quality of our generated counterfactuals via greater consistency between the alterations made in both modalities.

### Generating COCO-Counterfactuals from MS-COCO

We apply our counterfactual caption and image generation pipeline described above to create the COCO-Counterfactuals dataset. Specifically, we generate candidate counterfactual captions for 25,014 original MS-COCO captions90, keeping only the best candidate counterfactual for each original caption that meets our filtering criteria. This produced a total of 24,508 original & counterfactual caption pairs \((C_{o},C_{c})\) after filtering and selection. Our image over-generation pipeline produced 2.45 million candidate image pairs \((I_{o}^{s},I_{c}^{s})\) for these 24.5k caption pairs, of which 17,410 had at least one generated image pair which met our filtering criteria. After selection according to the CLIP\({}_{dir}\) metric, a total of 34,820 image-caption pairs remain, comprising our COCO-Counterfactuals dataset11.

Figure 2: Examples of COCO-Counterfactuals generated with Prompt-to-Prompt (left) and without (right). Prompt-to-prompt enables us to extend the principle of minimal-edit text counterfactuals to the visual domain, isolating image differences to only the changed causal feature.

## 4 COCO-Counterfactuals Analysis

This section aims to show that, in addressing the challenges associated with low-resource settings involving multimodal data (see Section 1), our proposed novel data generation pipeline can serve as an efficient and scalable framework to automatically create high quality multimodal counterfactual examples in COCO-Counterfactuals (COCO-CFs). Toward this goal, we first employ human evaluation to analyze COCO-CFs. We then show that COCO-CFs can be used as a challenging dataset for model evaluation on zero-shot image-text retrieval and image-text matching tasks.

### Human Evaluation of COCO-Counterfactuals

We employ professional data annotators to conduct a human study on the quality of COCO-CFs. For each of the 34,820 images in COCO-CFs, we have at least one annotator choose whether the corresponding original or counterfactual caption best fits the image. Annotators can also choose "both" if both captions describe the image equally well, or "neither" if neither caption accurately describes the image. \(10\%\) of the images are labeled by 3 different individuals to estimate inter-annotator agreement, with the remaining images each labeled by a single annotator (see Appendix B.2 for additional details).

Table 1 provides the percentage of images from COCO-CFs which were matched to their correct caption by the human annotators. We also report the percentage of incorrect matches (i.e., the wrong caption was picked as best describing the image) as well as the percentage of "both" and "neither" labels. Overall, 73% of images were correctly matched to their corresponding caption (see Appendix A.3 for an analysis of incorrect matches). Images generated from the counterfactual caption had a 10% greater incidence of incorrect caption selections than those generated from the original caption. This could be due to the constraints imposed on the counterfactual image by Prompt-to-Prompt (i.e., shared attention weights with the original image), which increases the likelihood that the generated image lacks some of the details in its corresponding caption.

The Fleiss' kappa coefficient for the 10% of images labeled by three annotators was 0.74, indicating strong agreement among the annotators who participated in this study. Among those images which had label disagreement, 47.4% of the labels were correct, 27.3% were incorrect, and 18.6% selected "neither." This suggests that many of the disagreements are associated with images for which the correct caption choice is more ambiguous.

While we employed human annotators to validate the quality of COCO-Counterfactuals for this analysis, our automated counterfactual generation approach does not require the use of human annotators to produce a new dataset. Indeed, our experiments described subsequently in Section 5.1 show that COCO-Counterfactuals which were labeled as incorrect by humans have no negative impact on training data augmentation.

### COCO-Counterfactuals for Model Evaluation

Motivated by prior work which has proposed using counterfactuals as challenging test sets in NLP (Kaushik et al., 2019; Gardner et al., 2020), we further investigate whether our COCO-CFs can serve a similar purpose for state-of-the-art multimodal vision-language models such as CLIP, Flava (Singh et al., 2022), BridgeTower (Xu et al., 2022) and ViLT (Kim et al., 2021) for the zero-shot image-text retrieval and image-text matching tasks. We employed the HuggingFace implementations of these model in our experiments (see Appendix A.6 for more detail).

   Image set & Correct & Incorrect & Neither & Both \\  Generated from original caption & 79.10\% & 8.16\% & 10.18\% & 2.56\% \\ Generated from counterfactual caption & 67.27\% & 18.43\% & 10.74\% & 3.56\% \\ All images & 73.18\% & 13.30\% & 10.46\% & 3.06\% \\   

Table 1: Human evaluation results for COCO-Counterfactuals

#### 4.2.1 Zero-shot Image-text Retrieval

We evaluate the zero-shot image-text retrieval (ITR) performance of pre-trained Flava and BridgeTower models on COCO-CFs as well as _human-evaluated COCO-CFs_, which consists of only image-text pairs that were correctly matched in our human evaluation study (Section 4.1). Since pre-trained CLIP was employed in our counterfactual image generation process (see Section 3.2), it is not suitable for zero-shot ITR setting. Thus, we only report its evaluation in Appendix. A.6 for completeness. For baselines, we evaluate ITR performance of these models on the MS-COCO dataset.

Table 2 reports ITR performance (i.e., Recall at 1, 5, and 10) on COCO-CFs and human-evaluated-COCO-CFs for pre-trained BridgeTower and Flava models. The percentages enclosed within parentheses indicate the change in performance of a model on an evaluated dataset versus the performance of that model on MS-COCO (baseline). We observe that the performance of BridgeTower and Flava decreases significantly (up to \(51\%\) and \(57\%\), respectively) compared to the baseline's performance on both COCO-CFs and human-evaluated-COCO-CFs. These results demonstrate that COCO-Counterfactuals can serve as a challenging test set for SOTA multimodal vision-language models.

#### 4.2.2 Image-text Matching

Typically, during pre-training for image-text matching (ITM), multimodal models learn to differentiate actual image-text pairs from alternative images or captions which are randomly sampled from a dataset. By design, our COCO-CFs have the potential to make this task significantly more challenging by requiring models to also differentiate between minimally-edited image or text candidates. We measure the magnitude of this increased difficulty by comparing the difference in ITM scores between actual image-text pairs and their corresponding counterfactual or randomly sampled alternatives.

Let \((C_{o},I_{o})\) denote an original image-text pair from MS-COCO, \(I_{o}^{s}\) denote our synthetically-generated image corresponding to \(C_{o}\), and \((C_{c},I_{c}^{s})\) denote our corresponding synthetically-generated counterfactual image-text pair in COCO-Counterfactuals. We further denote \((C_{r},I_{r})\) as a different original image-text pair randomly sampled from MS-COCO such that \(I_{o} I_{r}\). For a given pre-trained

    &  &  &  \\   & & **Ro1** & **Ro5** & **Ro10** & **Ro1** & **Ro5** & **Ro10** \\    } & COCO-CFs & 21.72 (-**51.5**\%) & 46.94 (-.35) & 58.65 (-.29) & 179.34 (-.47) & 38.94 (-.35) & 49.95 (-.30) \\  & human-evaluated COCO-CFs & 26.36 (-**41.5**\%) & 54.1-(-2.5) & 66.31 (-.20) & 21.44 (-.37) & 45 (-.25) & 56.39 (-.21) \\    } & COCO-CFs & 21.28 (-**57.6**) & 46.64 (-.41) & 58.87 (-.34) & 37.76 (-.16) & 66.15 (-.12) & 75.83 (-.10) \\  & human-evaluated COCO-CFs & 26.1 (-**47.5**\%) & 54.23 (-.31) & 66.83 (-.25) & 43.4 (-.3) & 72.35 (-.3) & 81.44 (-.45) \\   

Table 2: Image-text retrieval performance on COCO-CFs and human-evaluated COCO-CFs for BridgeTower and Flava models. Largest drops of performance against the baseline are in boldface.

Figure 3: ITM score differences computed for existing multimodal models on COCO-Counterfactuals dataset.

multimodal model, we compute the following metrics using its ITM scoring function \(\):

\[_{r} =(C_{r},I_{r})-(C_{r},I_{o}) _{c} =(C_{c},I_{c}^{s})-(C_{c},I_{o}^{s})\] \[_{r} =(C_{r},I_{r})-(C_{o},I_{r}) _{c} =(C_{c},I_{c}^{s})-(C_{o},I_{c}^{s})\]

\(_{r}\) and \(_{r}\) scores can be viewed as measuring the confidence of a model's image or text retrieval (respectively) over two real image-text pairs from MS-COCO. Similarly, \(_{c}\) and \(_{c}\) scores measure image or text retrieval confidence, but using matched image-text pairs from COCO-Counterfactuals dataset. For all metrics, values greater than zero indicate that a model scores the correct image-text pair as more similar than its random or counterfactual alternative. Larger positive values can be viewed as indicating greater confidence in the model's correct discernment between the alternatives.

Figure 3 plots the distribution of these four metrics for three pre-trained multimodal models: CLIP, BridgeTower, and ViLT. All three models exhibit a significant negative distribution shift when presented with counterfactual alternatives rather than random alternatives, demonstrating the increased difficulty of COCO-Counterfactuals for existing models. A significant number of COCO-Counterfactuals are also incorrectly scored (i.e., have ITM score difference less than zero) by all three models. Even in cases where the counterfactual alternatives can be correctly discerned, we posit that the much smaller values of \(_{c}\) and \(_{c}\) may improve the efficiency of training through the increased difficulty of the ITM task.

#### 4.2.3 Discussion

When used as a test set, COCO-Counterfactuals by design evaluate the robustness of models to minimal changes in paired image-text data. Table 2 and Figure 3 show that existing models perform significantly worse when evaluated on COCO-Counterfactuals. Additionally, we find that training these same models on COCO-Counterfactuals produces an average relative improvement of 24.3% in image-text retrieval performance on withheld counterfactual examples (see Table 5 of Appendix A.1). These results point to the usefulness our dataset for evaluating and improving the robustness of multimodal models to counterfactual changes.

## 5 COCO-Counterfactuals for Training Data Augmentation

This section aims to evaluate whether COCO-Counterfactuals can serve as an alternative to real data for training data augmentation in low-resource scenarios. We train a fully unfrozen pre-trained CLIP model with its contrastive loss using various combinations of real data from MS-COCO and COCO-CFs datasets (see Appendix B.3 for additional training details). In order to investigate the robustness of models trained on COCO-CFs, we evaluate them on OOD datasets for image-text retrieval and image recognition. For baselines, we report the performance of pre-trained CLIP (i.e., without any additional training) as well as a CLIP model which has been additionally trained using only real data from MS-COCO. We repeat each of our training experiments with 25 different random seeds and report both the mean and standard deviation of performance measured across all random seeds. We also validate the statistical significance of performance improvements obtained by models trained on COCO-CFs using one-tailed t-tests.

### Image-text Retrieval

To evaluate OOD performance on the image-text retrieval task that CLIP was trained for, we use the 1K test set of Flickr30k dataset (Young et al., 2014). Table 3 reports the zero-shot performance of the baselines as well as CLIP trained with varying amounts of the original MS-COCO and COCO-CFs datasets. We observe that all CLIP models trained with COCO-Counterfactuals outperform pre-trained CLIP by an average of 5 points, based on the mean performance across text and image retrieval settings. Additionally, our best model trained with 20,894 COCO-Counterfactuals provides statistically significant improvements relative to training only on the real MS-COCO dataset across all settings. We also found that COCO-Counterfactuals improve in-domain performance on the MS-COCO test set, which we detail in Appendix A.5.

To investigate the potential impact of COCO-Counterfactuals which were labeled incorrectly by human annotators, we repeated our training data augmentation experiments using only image-text pairs which were correctly matched in our human evaluation study (Section 4.1). Overall, we found that excluding these incorrectly-labeled COCO-Counterfactuals from training data augmentation had a negligible impact on performance (see Appendix A.4). This suggests that training data augmentation is robust to noise introduced by synthetic data, and that the 26.82% of incorrectly-labeled COCO-Counterfactuals do not pose an issue for data augmentation applications. While certain use cases which require a high degree of confidence in the accuracy of generated counterfactuals may benefit from the use of human validation, we believe that these results demonstrate how our approach can be used for fully automated training data augmentation without human annotation.

### Image Recognition

Despite being trained for image-text retrieval, CLIP has exhibited impressive performance at zero-shot image recognition. Using the same approach as Radford et al. (2021) for the image recognition task (i.e., form a sentence "A photo of a \(\{c\}\)" for each class label \(c\) to obtain image-text matching scores), we evaluate whether CLIP models trained on COCO-Counterfactuals exhibit competitive OOD performance improvement to baselines' performance in this zero-shot classification setting.

Using the same CLIP models trained on varying amounts of MS-COCO and COCO-CFs, Table 4 reports their zero-shot classification accuracy on six image recognition datasets. We observe that training with an approximately 50-50 split of MS-COCO & COCO-CFs provides the best overall performance, offering improvements over pre-trained CLIP (without any additional training) on all datasets except Food101 and outperforming training with only MS-COCO on most datasets (see Appendix A.2 for additional analysis of performance differences).

### Discussion

Recent work investigating the suitability of synthetic training data for image recognition tasks has found that synthetic image data is much less efficient than real data, requiring 5x more synthetic training samples to achieve similar performance as models trained on real data (He et al., 2022). In contrast, our results show that training data augmentation with COCO-Counterfactuals is at least as efficient (Table 3) and sometimes more efficient (Table 4) than data augmentation with an identical amount of real data (\(|D_{}|=13,928)\). This finding suggests that our approach could be particularly valuable in low-resource settings where paired image-text data is scarce.

  
**Training dataset** & \(|D_{}|\) & \% CFs & **CIFAR10** & **CIFAR100** & **Food101** & **Caltech101** & **Caltech256** & **ImageNet** & **Mean** \\  None (pre-trained CLIP) & 0 & 0\% & 88.8 & 64.17 & **84.17** & 90.32 & 83.43 & 59.25 & 78.36 \\  MS-COCO & 13,928 & 0\% & 89.21\({}_{0.3}\) & 63.89\({}_{0.4}\) & 82.67\({}_{0.2}\) & 92.77\({}_{0.1}\) & 85.05\({}_{0.5}\) & 59.55\({}_{0.2}\) & 78.85\({}_{0.2}\) \\ MS-COCO + COCO-CFs & 13,928 & 0\% & 89.45\({}_{0.3}\) & 66.62\({}_{0.4}\) & 83.13\({}_{0.2}\) & 92.63\({}_{0.1}\) & **85.21\({}_{0.1}\)** & **59.66\({}_{0.2}\)** & **72.46\({}_{0.2}\)** \\  MS-COCO + COCO-CFs & 34,820 & 0\% & 89.16\({}_{0.3}\) & **66.88\({}_{0.4}\)** & 82.12\({}_{0.2}\) & **92.87\({}_{0.1}\)** & 84.95\({}_{0.2}\) & 59.22\({}_{0.3}\) & 79.20\({}_{0.2}\) \\ MS-COCO + COCO-CFs & 41,784 & 0\% & 88.51\({}_{0.5}\) & 65.97\({}_{0.5}\) & 82.06\({}_{0.2}\) & 92.77\({}_{0.1}\) & 84.59\({}_{0.2}\) & 58.8\({}_{0.2}\) & 78.80\({}_{0.2}\) \\   

Table 4: Zero-shot classification accuracy of pre-trained CLIP and CLIP models trained on varying amounts of data from MS-COCO and COCO-CFs datasets. All other settings are identical to Table 3.

    &  &  \\ 
**Training dataset** & \(|D_{}|\) & \% CFs & **R@1** & **R@5** & **R@10** & **R@5** & **R@10** & **Mean** \\  None (pre-trained CLIP) & 0 & 0\% & 67.1 & 89 & 93.8 & 69.4 & 90.6 & 94.9 & 84.13 \\  MS-COCO & 13,928 & 0\% & 77.90\({}_{0.4}\) & 93.79\({}_{0.2}\) & 97.11\({}_{0.1}\) & 75.14\({}_{0.4}\) & 93.72\({}_{0.2}\) & 96.69\({}_{0.2}\) & 89.06\({}_{0.1}\) \\ MS-COCO + COCO-CFs & 13,928 & 50\% & 76.66\({}_{0.5}\) & 94.53\({}_{0.3}\) & 96.84\({}_{0.2}\) & 75.75\({}_{0.4}\) & 93.60\({}_{0.2}\) & **96.96\({}_{0.2}\)** & 89.05\({}_{0.1}\) \\  MS-COCO + COCO-CFs & 34,820 & 0\% & 78.28\({}_{0.4}\) & **94.72\({}_{0.3}\)** & **97.27\({}_{0.2}\)** & 76.13\({}_{0.5}\) & 93.85\({}_{0.2}\) & 96.91\({}_{0.2}\) & **89.53\({}_{0.2}\)** \\ MS-COCO + COCO-CFs & 41,784 & 0\% & 77.75\({}_{0.5}\) & 94.51\({}_{0.3}\) & 97.03\({}_{0.2}\) & **76.38\({}_{0.3}\)** & **94.01\({}_{0.2}\)** & 96.79\({}_{0.2}\) & 89.41\({}_{0.1}\) \\   

Table 3: Image-text retrieval performance on the OOD Flickr30k 1K test set for pre-trained CLIP and CLIP models trained on varying amounts of data from MS-COCO and COCO-CFs datasets. \(|D_{}|\) indicates the total number of image-text pairs used for training, while % CFs indicates the percentage of those image-text pairs which were sampled from COCO-CFs. Results report mean over 25 different random seeds, with standard deviation as a subscript. Best results are in boldface. Results which use COCO-CFs are underlined when a one-tailed t-test indicates that their improvement over training only on MS-COCO is statistically significant (\(p 0.05\))Consistent with prior work on training data augmentation with NLP counterfactuals (Howard et al., 2022; Joshi and He, 2022), Tables 3 and 4 show that improvements in OOD performance with increasing amounts of counterfactual examples reaches a saturation point, beyond which additional data augmentation does not lead to further improvements. For image-text retrieval on Flickr30k (Table 3), this saturation point is reached with a 40 / 60% mixture of MS-COCO / COCO-Counterfactuals in the training dataset. In contrast, Table 4 shows that the saturation point for the OOD image recognition datasets is reached with a 50 / 50% split based on the mean of the six datasets. These results suggest that the optimal mixture of real examples and synthetically generated counterfactual examples may differ depending on the evaluation task and dataset.

While training data augmentation with COCO-Counterfactuals produces statistically significant performance improvements relative to training with only real data, the overall magnitude of these improvements is limited and varies by evaluation setting. COCO-Counterfactuals produce the largest improvements on zero-shot image recognition tasks, where its overall mean improvement over pre-trained CLIP is twice as large as that achieved by training on an equivalent amount of real data from MS-COCO. However, OOD generalization performance varies by dataset, which further analysis suggests, is related to domain gaps between altered subjects in COCO-Counterfactuals and the domain of the evaluation dataset (see Appendix A.2 for details).

## 6 Conclusion

We proposed an automated data generation methodology for creating counterfactual examples from image-text pairs to address the challenge of low-resource settings involving multimodal data. This approach was used to create COCO-Counterfactuals (COCO-CFs), a high-quality synthetic dataset of paired image-text counterfactuals derived from MS-COCO captions. COCO-CFs are challenging for existing pre-trained multimodal models and significantly increase the difficulty of the zero-shot image-text retrieval and image-text matching tasks. Our experiments demonstrate that augmenting training data with COCO-CFs improves OOD generalization on multiple downstream tasks.

In this work, we focused on the creation of task-agnostic counterfactual examples. A promising direction for future research is the adaptation of our approach to produce task-specific counterfactuals. For example, in the case of image recognition, the counterfactual changes could be limited to a targeted label distribution to produce counterfactual examples more tailored to the end task. Alternatively, task-specific model failures or spurious correlations could be diagnosed and used as a basis for determining which counterfactual changes to consider when creating the counterfactual captions. We believe that such approaches have the potential to produce counterfactuals which are more targeted for improving specific model deficiencies.

Another opportunity for future work is larger-scale automatic generation of counterfactual examples to enable full counterfactual pre-training of multimodal models. Additionally, we believe that extending our image-text counterfactuals to the video domain could be a promising path towards improving video transformers through counterfactual data augmentation.

Limitations & Ethical ConcernsMotivated by the desire to produce minimal-edit counterfactuals, we only considered changes to nouns. This is a common strategy for NLP counterfactuals (see Appendix B.1.1 for discussion), but alternative generation strategies such as controlled text decoding (Howard et al., 2022) could be used to enable a larger range of counterfactual changes, in addition to alterations of adjectives or verbs. We leave investigation of these directions to future studies.

Due to a limited compute budget, we only explored generating COCO-Counterfactuals using Stable Diffusion. Additionally, our training data augmentation experiments were limited to a single model (CLIP). It is possible that other text-to-image generation models may exhibit better performance for generating counterfactual image-text data. Additionally, the benefits of counterfactual data augmentation may vary for different multimodal vision-language models.

Despite the impressive recent improvements in text-to-image generation capabilities, models such as Stable Diffusion have well-known limitations that should be considered when utilizing datasets which are derived from them (see Appendix C.5 for a detailed discussion). We do not foresee significant risks of security threats or human rights violations in our work. However, the automated nature of our image generation process may introduce the possibility of our COCO-Counterfactuals dataset containing images that some individuals may consider inappropriate or offensive.