# How does fine-tuning affect your model? Mechanistic analysis on procedural tasks

Samyak Jain\({}^{1,}\)

Co-first authors. samyakjain.cse18@itbhu.ac.in, robert.kirk.3.14@gmail.com, eslubana@umich.edu.

Robert Kirk\({}^{2,}\)

University College London, UK

Ekdeep Singh Lubana\({}^{3,4,*}\)

Co-first authors. samyakjain.cse18@itbhu.ac.in, robert.kirk.3.14@gmail.com, eslubana@umich.edu.

Robert P. Dick\({}^{3}\)

Hidenori Tanaka\({}^{4,5}\)

Edward Grefenstette\({}^{2}\)

Tim Rocktaschel\({}^{2}\)

David Krueger\({}^{1}\)

\({}^{1}\)University of Cambridge, UK

\({}^{2}\)University College London, UK

\({}^{3}\)EECS Department, University of Michigan, Ann Arbor, MI, USA

\({}^{4}\)Center for Brain Science, Harvard University, Cambridge, MA, USA

\({}^{5}\)Physics & Informatics Laboratories, NTT Research, Inc., Sunnyvale, CA, USA

###### Abstract

Fine-tuning large pre-trained models has become the _de facto_ strategy for developing models that are safe to deploy. However, there has been little work that explains how fine-tuning alters the underlying capabilities learnt by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just modulate existing ones? We address this question empirically in _synthetic_ settings with mechanistic interpretability tools (e.g., network pruning and probing) to understand how the model's underlying capabilities are changing. Our extensive analysis of the effects of fine-tuning shows: (i) fine-tuning rarely alters the underlying model capabilities; (ii) a minimal transformation, which we call a 'wrapper', is typically learned on top of the underlying model capabilities; and (iii) further fine-tuning on a task where such wrapped capabilities are relevant leads to sample-efficient "revival" of the capability, i.e., the model begins reusing this capability in a few gradient steps. _This indicates practitioners can unintentionally remove a model's safety wrapper by merely fine-tuning it on a superficially unrelated task._

## 1 Introduction

Large language models (LLMs) pretrained on huge, web-crawled text datasets demonstrate extremely general capabilities . This has led to the current paradigm of machine learning, where practitioners often use model adaptation protocols such as fine-tuning to achieve unprecedented performance on a broad range of downstream tasks . Fine-tuning with different training objectives has also seen immense usage in mitigating "unsafe" capabilities (e.g., producing sensitive, biased, or toxic outputs), serving as an integral component of current state-of-the-art _alignment_ approaches like RLHF . Given the ubiquity of fine-tuning, a natural question emerges: precisely how does fine-tuning influence a pretrained model's capabilities to adapt it to the downstream dataset? The generality of an LLM's capabilities makes it feasible that fine-tuning protocols merely identify the most relevant capability and amplify its use for a given set of inputs, while inhibiting the use of other capabilities that are viable for processing the inputs. This can be concerning in scenarios where fine-tuning is used for removing undesirable behaviors from a model, as the protocol may just "hide" the undesirable capability.

Motivated by the above, we perform an extensive analysis of the effects of fine-tuning on a pretrained model's capabilities in _controlled settings_ where we can use mechanistic interpretability tools to understand precisely what is happening to the model's underlying capabilities. Specifically, we focuson compiled transformer models based on the _Tracr library_[15; 16]--which allows encoding specific computational programs into a transformer--and procedurally generated setups involving _probabilistic context-free grammars (PCFGs)_[17; 18]--a formal model designed to capture syntactic properties of natural and programmatic languages that has recently served as a testbed for mechanistically understanding language models [19; 20; 21]. While Tracr allows us to analyze models with perfectly encoded capabilities, models trained on PCFGs allow us to evaluate the effects of different pretraining design choices that may yield learning of "approximate" capabilities. Fine-tuning these models via the often used protocol of further training a pretrained model on a downstream dataset with a sufficiently small learning rate, we find when a "relevant" pretraining capability is present, the fine-tuned model learns a minimally transformed version of it. We call this transformation a wrapper and find that the wrapper can often be extremely localized, e.g., via mere pruning of a few weights or neurons, the model can start to reuse its pretraining capability and forget how to perform the downstream task. Morover, further fine-tuning the model on a subset of pretraining data leads to an extremely sample-efficient revival of the capability.

## 2 Defining our notion of capabilities

For precision, we first discuss the notion of capabilities that we aim to capture for analyzing how fine-tuning alters a model. Let \(_{}\) denote a dataset sampled from a distribution \(_{}\) over the domain \(\). We will assume the domain \(\) can itself be factorized into two domains \(_{I}\) and \(_{D}\). Correspondingly, a sample \(x\) can be divided into a tuple of variables \((x_{i}_{I},x_{d}_{D})\), where \(x_{i}\) identifies which capability a model should use to process the information encoded by the variable \(x_{d}\). This decomposition captures the idea that different prompts can force a pretrained LLM to elicit different capabilities, e.g., by adding the phrase "think step by step" . The identifier of capability \(c\) is denoted \(i_{c}\). Pretraining on \(_{}\) yields us a model \((.)\), where we distinguish between the domain and co-domain of the model for the purpose of our framework; in practice, one can assume \(=\) for language models trained on a sufficiently large dataset. We define a capability as follows.

Definition 1 (Capability.): Define a map \(:_{}_{}\), where \(_{}\). Let \(_{}\) be a sub-domain such that for all \(x X_{}\), the capability identifier variable is the same, i.e., \(x_{i}=_{}\). Then, we say the model \(\) "possesses a capability \(\)" if for all \(x_{}\), \((x)=(x_{d})\).

We use an idealized definition to relay our primary intuition and emphasize that we do not expect all capabilities a pretrained model seems to possess will act as perfectly as the definition necessitates. We next consider how the fine-tuning distribution \(_{X}^{}\) over the domain \(\) can interact with capabilities exhibited in the pretrained model. Our goal here is to capture the fact that a large-scale pretraining corpora is likely to have non-zero probability under the fine-tuning distribution. Correspondingly, it is unlikely that a pretrained model will lack _any_ capability relevant to the downstream task. This motivates a notion of "relevance of a capability". Specifically, let \(_{}_{X}^{,}\) denote the downstream dataset used for fine-tuning, where \(_{X}^{,}\) is the empirical distribution that captures a subset of the support with non-zero probability in the distribution \(_{X}^{}\).

Definition 2 (Relevance of a Capability.): Assume the capability \(\) possessed by the pretrained model \((.)\) can be transformed to a map \(\) via fine-tuning on \(_{}\), such that for all \(x_{X}^{,}\), the correct output is produced. Then, if for all \(x_{X}^{}\), \(\) yields the correct output, we claim capability \(\) is **strongly relevant** to the fine-tuning task; else, we call it **weakly relevant**.

For example (also see Fig. 1), a _weakly relevant_ capability can involve the ability to recognize a spurious feature that the model can learn to exploit to perform well on the fine-tuning dataset, without enabling generalization to the overall distribution that the fine-tuning dataset is sampled from. Meanwhile, a _strongly relevant_ capability is one that extract a causally relevant feature for that task. When a weakly relevant capability is used by the model, we find we can often localize neurons which implement the transform \(\) in Def. 2. We call such a \(\) a **wrapper** and \(\) a **wrapped capability**.

Figure 1: **Capability Relevance. Consider the task of completing a passage while maintaining its narrative. Herein, the ability to recognize the sentiment of a text will be deemed _strongly relevant_ and the ability to recognize negative words _weakly relevant_. Such words are often correlated with a negative sentiment.**

[MISSING_PAGE_FAIL:3]

capabilities setup of PCFG Counters in this section, relegating most results on compiled capabilities to App. I, learned capabilities to App. J, and also some preliminary analysis of the recently released TinyStories dataset  to App. H--results are consistent across all settings. In the PCFG Counters setup, the model is pretrained to count tokens \(}=\{a,b,c\}\) in a given string; during fine-tuning, the model is trained to count the token \(}=\), wherein the spurious correlation is defined by enforcing count of b to be 1 larger than that of a. The probability of embedding a spurious correlation in the train/test fine-tuning dataset is denoted \(R_{}/R_{}\), with \(R_{}\{0.0,0.5,0.8,1.0\}\) and \(R_{}\{0.0,1.0\}\). We use three sets of sampling probabilities of the task operands in the pretraining data: \(^{L}_{C}=(0.999,0.001,0.000)\), \(^{M}_{C}(0.9,0.1,0.0)\), or \(^{H}_{C}=(0.5,0.3,0.2)\). We also sweep the fine-tuning learning rate \(\), \(_{L}=10^{-3}\), \(_{M}=10^{-4}\), \(_{S}=10^{-5}\).

**Experiment 1:** We first evaluate the model's learning dynamics during fine-tuning (see Fig. 5). When the pretraining prior has low probability of sampling the target token, the fine-tuned model performs well only when the spurious correlation is present, i.e., \(R_{}=1\). As the prior is reduced, however, we observe this behavior significantly changes. Only having a sufficiently large sampling probability of the token during pretraining leads the model to avoid the spurious correlation. We argue this is likely to happen due to the model partially learning the strongly relevant capability, with fine-tuning enabling the completion of this learning. Based on this behavioral analysis, we hypothesize that the model learns a wrapper on the _weakly relevant capability_ of counting the token \(}\), adding the relevant difference to yield the correct result for \(}\). To evaluate this, we analyze the models fine-tuned with a low sampling prior via network pruning and linear probing (see Fig. 7). The details of pruning are discussed in the appendix. If the model learns a wrapper on this capability, deleting the wrapper neurons should recover the capability to count \(}\). As shown in Fig. 3, we find this is indeed the case. To confirm this hypothesis further, we perform probing analysis by training a linear layer on every block's residual output. As shown in Fig. 7 (a), in the presence of spurious correlations, the model with weakly relevant capability is able to perform well on counting the token \(}\) when fine-tuned using correlated data.

Takeaway: We find the model seems to exploit the spurious correlation when the sampling prior of target token is low during pretraining, but avoids it when the prior is sufficiently high to enable partial learning of the strongly relevant capability. Mechanistic analysis shows that the features learned by the weakly relevant capability can be localized in the model, indicating learning of a wrapper.

**Experiment 2:** The above analysis focuses on the learning of a capability relevant to the downstream task. We note analyze how the pre-training capability itself is affected during fine-tuning. Specifically, we use a model fine-tuned to count \(}\) and reverse fine-tune (reFT ) it to learn its original pre-training capability to count \(}\). As shown in Fig. 8 (a), even with a small learning rate (\(_{S}\)), the pretraining capability is revived via reFT ; this revival is made quicker with increase in \(_{C}(})\), as shown in Fig. 8 (appendix). This _indicates_ that upon fine-tuning, relevant "circuits" for the pre-training capability continue to persist in the model. To analyze this further, we train linear probes on intermediate representations from each block of the model to produce the output of the pre-training task of counting \(}\). If the pre-training capabilities are indeed present in the model after fine-tuning to count on token \(}\), the probes should yield us high performance. As shown in Fig. 7, the pre-training capabilities to count \(}\) is indeed present in the model, especially under strong-relevance. Under weak-relevance of the capability, a smaller fine-tuning learning rate (\(_{S}\)), as expected to be used in a practical scenario, leads to continued existence of the pre-training capability.

Takeaway: We find that the pre-training capability is not lost on fine-tuning.

Figure 3: **Pruning analysis: Fine-tuning task related features learned by the weakly relevant capability can be localized in the model:** Evaluation is done for the pre-training task of counting the token \(}\). The pre-training task capability can be revived on using a low learning rate \(_{S}\).

## Authors' Contributions

ESL conceived the project direction and developed a set of hypotheses on the limitations of fine-tuning, with inputs from RK. SJ and ESL co-designed a draft of the PCFG and Tracr setups, and came up with pruning and reverse fine-tuning analysis which led to validation and further refining of the hypotheses. SJ led the experimental execution and made the tasks considered in the paper precise in collaboration with ESL. RK proposed and ran the TinyStories experiments with inputs from ESL, SJ, EG and TR. Literature review and writing of the main paper was led by ESL. SJ led writing of the appendix. ESL, SJ, RK, and HT collaborated on design of all figures and plots. DSK acted as the primary senior advisor on the paper, with inputs from RPD, HT, EG, and TR as well.