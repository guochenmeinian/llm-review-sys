# Anytime Model Selection in Linear Bandits

Parnian Kassraie\({}^{1}\) Nicolas Emmenegger\({}^{1}\) Andreas Krause\({}^{1}\) Aldo Pacchiano\({}^{2,3}\)

\({}^{1}\)ETH Zurich \({}^{2}\)Broad Institute of MIT and Harvard \({}^{3}\)Boston University

{pkassraie, nicolaem, krausea}@ethz.ch pacchian@bu.edu

###### Abstract

Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly (\(M\)) with the number of models \(M\) in terms of their regret. Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALExp, which has an exponentially improved (\( M\)) dependence on \(M\) for its regret. ALExp has anytime guarantees on its regret, and neither requires knowledge of the horizon \(n\), nor relies on an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.

## 1 Introduction

When solving bandit problems or performing Bayesian optimization, we need to commit to a reward model _a priori_, based on which we estimate the reward function and build a policy for selecting the next action. In practice, there are many ways to model the reward by considering different feature maps or hypothesis spaces, e.g., for optimizing gene knockouts (Gonzalez et al., 2015; Pacchiano et al., 2022) or parameter estimation in biological dynamical systems (Ulmasov et al., 2016; Imani et al., 2019). It is not known a priori which model is going to yield the most sample efficient bandit algorithm, and we can only select the right model as we gather empirical evidence. This leads us to ask, can we perform adaptive model selection, while simultaneously optimizing for reward?

In an idealized setting with no sampling limits, given a model class of size \(M\), we could initialize \(M\) bandit algorithms (a.k.a agents) in parallel, each using one of the available reward models. Then, as the algorithms run, at every step we can select the most promising agent, according to the cumulative rewards that are obtained so far. Model selection can then be cast into an online optimization problem on a \(M\)-dimensional probability simplex, where the probability of selecting an agent is dependent on its cumulative reward, and the optimizer seeks to find the distribution with the best return in hindsight. This approach is impractical for large model classes, since at every step, it requires drawing \(M\) different samples in parallel from the environment so that the reward for each agent is realized.

In realistic applications of bandit optimization, we can only draw _one_ sample at a time, and so we need to design an algorithm which allocates more samples to the agents that are deemed more promising. Prior work (e.g., Maillard and Munos, 2011; Agarwal et al., 2017) propose to run a single meta algorithm which interacts with the environment by first selecting an agent, and then selecting an action according to the suggestion of that agent. The online model selection problem can still be emulated in this setup, however this time the optimizer receives partial feedback, coming from only one agent. Consequently, many agents need to be queried, and the overall regret scales with \(M\), again restricting this approach to small model classes. In fact, addressing the limited scope of such algorithms, Agarwal et al. (2017) raise an open problem on the feasibility of obtaining a \( M\) dependency for the regret.

We show that this rate is achievable, in the particular case of linearly parametrizable rewards. We develop a technique to "hallucinate" the reward for every agent that was not selected, and run the online optimizer with emulated full-information feedback. This allows the optimizer to assess the quality of the agents, without ever having queried them. As a result, our algorithm ALExp, satisfies a regret of rate \((\{M},n^{3/4}\})\), with high probability, simultaneously for all \(n 1\) (Theorem 1). Our key idea, leading to \( M\) dependency, is to employ the Lasso as a low-variance online regression oracle, and estimate the reward for the agents that were not chosen. This trick is made possible through our novel time-uniform analysis of online Lasso regression (Theorem 3). Consequently, ALExp is horizon-independent, and explores adaptively without requiring an initial exploration stage. Empirically we find that ALExp consistently outperforms prior work across a range of environments.

## 2 Related Work

Online Model selection (MS) for bandits considers combining a number of agents in a master algorithm, with the goal of performing as well as the best agent (Maillard and Munos, 2011; Agarwal et al., 2017; Pacchiano et al., 2020; Luo et al., 2022). This literature operates on black-box model classes of size \(M\) and uses variants of Online Mirror Descent (OMD) to sequentially select the agents. The optimizer operates on importance-weighted estimates of the agents' rewards, which has high variance (\(M\)) and is non-zero only for the selected agent. Effectively, the optimizer receives partial (a.k.a. bandit) feedback and agents are at risk of starvation, since at every step only the selected agent gets the new data point. These works assume knowledge of the horizon, however, we suspect that this may be lifted with a finer analysis of OMD.

Sparse linear bandits use sparsity-inducing methods, often Lasso (Tibshirani, 1996), for estimating the reward in presence of many features, as an alternative to model selection. Early results are in the data-rich regime where the stopping time \(n\) is known and larger than the number of models, and some suffer from \(M\) regret dependency (e.g., Abbasi-Yadkori et al., 2012). Recent efforts often consider the contextual case, where at every step only a finite stochastic subset of the action domain is presented to the agent, and that the distribution of these points is i.i.d. and sufficiently diverse (Li et al., 2022; Bastani and Bayati, 2020; Kim and Paik, 2019; Oh et al., 2021; Cella and Pontil, 2021). We do not rely on such assumptions. Most sparse bandit algorithms either start with a purely exploratory phase (Kim and Paik, 2019; Li et al., 2022; Hao et al., 2020; Jang et al., 2022), or rely on a priori scheduled exploration (Bastani and Bayati, 2020). The exploration budget is set according to the horizon \(n\). Therefore, such algorithms inherently require the knowledge of \(n\) and can be made anytime only via the doubling trick (Auer et al., 1995). Table 2 presents an in-depth overview.

ALExp inherits the best of both worlds (Table 1): its regret enjoys the \( M\) dependency of sparse linear bandits even on compact domains, and it has adaptive probabilistic exploration with anytime guarantees. In contrast to prior literature, we perform model selection with an online optimizer (EXP4), which hallucinates full-information feedback using a low-variance Lasso estimator instead of importance-weighted estimates. Moreover, our anytime approach lifts the horizon dependence and the exploration requirement of sparse linear bandits.

Our work is inspired by and contributes to the field of Learning with Expert Advice, which analyzes incorporating the advice of \(M\) oblivious (non-adaptive) experts, with bandit or full-information feedback (Haussler et al., 1998; Auer et al., 2002; McMahan and Streeter, 2009). The idea of employing an online optimizer for learning stems from this literature, and has been used in various applications of online learning (Foster et al., 2017; Singla et al., 2018; Muthukumar et al., 2019; Karimi et al., 2021; Liu et al., 2022). In particular, we are inspired by Foster and Rakhlin (2020) and Moradipari et al. (2022), who apply EXP4 to least squares estimates, for arm selection in \(K\)-armed

  & MS &  & MS & adaptive &  \\  & Technique & & Guarantee & exploration & \\  Sparse Linear &  & \( M\) & ✗ & ✗ & ✗ \\ Bandits & & & & & \\  MS for Black- &  & M\)} &  &  &  \\ Box Bandits & bandit feedback & & & & \\  MS for Linear &  &  &  &  &  \\ Bandits (Ours) & full-information & & & \\ 

Table 1: Overview of literature on online model selection for bandit optimization contextual bandits. However, their algorithms are not anytime, and due to the choice of estimator, the corresponding regret scales with \(((,))\).

## 3 Problem Setting

We consider a bandit problem where a learner interacts with the environment in rounds. At step \(t\) the learner selects an action \(_{t}\), where \(^{d_{0}}\) is a compact domain and observes a noisy reward \(y_{t}=r(_{t})+_{t}\) such that \(_{t}\) is an i.i.d. zero-mean sub-Gaussian variable with parameter \(^{2}\). We assume the reward function \(r:\) is linearly parametrizable by some unknown feature map, and that the model class \(\{_{j}:^{d_{0}}^{d},j=1,,M\}\) contains the set of plausible feature maps. We consider the setting where \(M\) can be very large, and while the set \(\{_{j}\}\) may include misspecified feature maps, it contains at least one feature map that represents the reward function, i.e., there exists \(j^{}[M]\) such that \(r()=_{j}^{}_{j^{}}()\). We assume the image of \(_{j}\) spans \(^{d}\), and no two feature maps are linearly dependent, i.e. for any \(j,j^{}[M]\), there exists no \(\) such that \(_{j}()=_{j^{}}()\). This assumption, which is satisfied by design in practice, ensures that the features are not ill-posed and we can explore in all relevant directions. We assume that the concatenated feature map \(()(_{1}(),,_{M}())\) is normalized \(\|()\| 1\) for all \(\) and that \(\|_{j^{}}\| B\), which implies \(|r()| B\) for all \(\).

We will model this problem in the language of model selection where a meta algorithm aims to optimize the unknown reward function by relying on a number of base learners. In order to interact with the environment the meta algorithm selects an agent that in turn selects an action. In our setting we thinking of each of these \(M\) feature maps as controlled by a base agent running its own algorithm. Base agent \(j\) uses the feature map \(_{j}\) for modeling the reward. At step \(t\) of the bandit problem, each agent \(j\) is given access to the full history \(H_{t-1}\{(_{1},y_{1}),,(_{t-1},y_{t-1})\}\), and uses it to locally estimate the reward as \(}_{t-1,j}^{}_{j}()\), where \(}_{t-1,j}^{d}\) is the estimated coefficients vector. The agent then uses this estimate to develop its action selection policy \(p_{t,j}()\). Here, \(\) denotes the space of probability measures defined on \(\). The condition on existence of \(j^{}\) will ensure that there is at least one agent which is using a correct model for the reward, and thereby can solve the bandit problem if executed in isolation. We refer to agent \(j^{}\) as the oracle agent.

Our goal is to find a sample-efficient strategy for iterating over the agents, such that their suggested actions maximize the cumulative reward, achieved over any horizon \(n 1\). This is equivalent to minimizing the cumulative regret \(R(n)=_{t=1}^{n}r(^{*})-r(_{t})\), where \(^{*}\) is a global maximizer of the reward function. We neither fix \(n\), nor assume knowledge of it.

## 4 Method

As warm-up, consider an example with deterministic agents, i.e., when \(p_{t,j}\) is a Dirac measure on a specific action \(_{t,j}\). Suppose it was practically feasible to draw the action suggested by every agent and observe the corresponding reward vector \(_{t}=(r_{t,j})_{j=1}^{M}\). In this case, model selection becomes a full-information online optimization problem, and we can design a minimax optimal algorithm as follows. We assign a probability distribution \(_{t}=(q_{t,j})_{j=1}^{M}\) to the models, and update it such that the overall average return \(_{t=1}^{n}_{t}^{}_{t}\) is competitive to the best agent's average return \(_{t=1}^{n}r_{t,j^{}}\). At every step, we update \(q_{t+1,j}(_{s=1}^{t}r_{s,j})\), since such exponential weighting is known to lead to an optimal solution for this classic online learning problem (Cesa-Bianchi and Lugosi, 2006). In our setting however, the agents are stochastic, and we do not have access to the full \(_{t}\) vector.

We propose the **A**nytime **Exp**onential weighting algorithm based on **L**asso reward estimates (ALExp), summarized in Algorithm 1. At step \(t\) we first sample an agent \(j_{t}\), and then sample an action \(_{t}\) according to the agent's policy \(p_{t,j_{t}}\). Let \(_{M}\) denote the \(M\)-dimensional probability simplex. We maintain a probability distribution \(_{t}_{M}\) over the agents, and update it sequentially as we accumulate evidence on the performance of each agent. Ideally, we would have adjusted \(q_{t,j}\) according to the average return of model \(j\), that is, \(_{ p_{t,j}}r()\). However, since \(r\) is unknown, we estimate the average reward with some \(_{t,j}\). We then update \(_{t}\) for the next step via,

\[q_{t+1,j}=_{s=1}^{t}_{s,j})}{_{i=1}^{M}( _{t}_{s=1}^{t}_{s,i})}\]for all \(j=1,,M\), where \(_{t}\) is the learning rate, and controls the sensitivity of the updates. This rule allows us to imitate the full-information example that we mentioned above. By utilizing \(_{t,j}\) and hallucinating feedback from all agents, we can reduce the probability of selecting a badly performing agent, without ever having sampled them (c.f. Fig. 4). It remains to design the estimator \(_{t,j}\). We concatenate all feature maps, and, knowing that many features are redundant, use a sparsity inducing estimator over the resulting coefficients vector. Mainly, let \(=(_{1},,_{M})^{Md}\) be the concatenated coefficients vector. We then solve

\[}_{t}=*{arg\,min}_{^{Md }}(;H_{t},_{t})=*{arg\,min}_{^{Md}}_{t}-_{t}}_{2}^ {2}+2_{t}_{j=1}^{M}_{j}}_{2}\] (1)

where \(_{t}=[^{}(_{s})]_{s t}^{t Md}\) is the feature matrix, \(_{t}^{t}\) is the concatenated reward vector, and \(_{t}\) is an adaptive regularization parameter. Problem (1) is the online variant of the group Lasso (Lounici et al., 2011). The second term is the loss is the mixed \((2,1)\)-norm of \(\), which can be seen as the \(_{1}\)-norm of the vector \((_{1}},,_{M}})^{M}\). This norm induces sparsity at the group level, and therefore, the sub-vectors \(}_{t,j}^{d}\) that correspond to redundant feature maps are expected to be \(\), i.e. the null vector. We then estimate the average return of each model by simply taking an expectation \(_{t,j}=_{ p_{t+1,j}}[}_{t}^{} ()]\). This quantity is the average return of the agent's policy \(p_{t+1,j}\), according to our Lasso estimator. In Section 5.2 we explain why the particular choice of Lasso is crucial for obtaining a \( M\) rate for the regret.

For action selection, with probability \(_{t}\), we sample agent \(j\) with probability \(q_{t,j}\) and draw \(_{t} p_{t,j}\) as per suggestion of the agent. With probability \(1-_{t}\), we choose the action according to some exploratory distribution \(()\) which aims to sample informative actions. This can be any design where \(()=\). We mix \(p_{t,j}\) with \(\), to collect sufficiently diverse data for model selection. We are not restricting the agents' policy, and therefore can not rely on them to explore adequately. In Theorem 1, we choose a decreasing sequence of \((_{t})_{t 1}\) the probabilities of exploration at step \(t 1\), since less exploration will be required as data accumulates. To conclude, the action selection policy of ALExp is formally described as the mixture

\[p_{t}()=_{t}()+(1-_{t})_{j=1}^{M}q_{t,j}p_{t,j}( ).\]

## 5 Main Results

For the regret guarantee, we consider specific choices of base agents and exploratory distribution. Our analysis may be extended to include other policies, since ALExp can be wrapped around any bandit agent that is described by some \(p_{t,j}\), and allows for random exploration with any distribution \(\).

**Base Agents.** We assume that the oracle agent has either a UCB (Abbasi-Yadkori et al., 2011) or a Greedy(Auer et al., 2002a) policy, and all other agents are free to choose _any arbitrary_ policy. Similar treatment can be applied to cases where the oracle uses other (sublinear) polices for solving linear or contextual bandits (e.g., Thompson, 1933; Kaufmann et al., 2012; Agarwal et al., 2014). In either case, agent \(j^{}\) calculates a ridge estimate of the coefficients vector based on the history \(H_{t}\)

\[}_{t,j^{}}*{arg\,min}_{ ^{d}}_{t}-_{t,j^{}}}_{2}^{2}+ }_{2}^{2}=(_{t,j^{}}^{}_{t,j^{}}+)^{-1}_{t,j^{}}^{}_{ t}.\]

Here \(_{t,j^{}}^{t d}\) is the feature matrix, where each row \(s\) is \(_{j^{}}^{}(_{s})\) and \(\) is the regularization constant. Then at step \(t\), a Greedy oracle suggests the action which maximizes the reward estimate \(}_{t-1,j}^{}_{j^{}}()\), and a UCB oracle queries \(*{arg\,max}u_{t-1,j^{}}()\) where \(u_{t-1,j^{}}()\) is the upper confidence bound that this agent calculates for \(r()\). Proposition 21 shows that the sequence \((u_{t,j^{}})_{t 1}\) is in fact an anytime valid upper bound for \(r\) over the entire domain.

**Exploratory policy.** Performance of ALExp depends on the quality of the samples that \(\) suggests. The eigenvalues of the covariance matrix \((,)_{}()^{}()\) reflect how diverse the data is, and thus are a good indicator for data quality. van de Geer and Buhlmann (2009) present a survey on the notions of diversity defined based on these eigenvalues. Let \(_{}(A)\) denote the minimium eigenvalue of a matrix \(A\). Similar to Hao et al. (2020), we assume that \(\) is the maximizer of the problem below and present our regret bounds in terms of

\[C_{}=C_{}(,)_{( )}_{}((,)),\] (2)which is greater than zero under the conditions specified in our problem setting. Prior works in the sparse bandit literature all require a similar or stronger assumption of this kind, and Table 2 gives an overview. Alternatively, one can work with an arbitrary \(\), e.g., \(()\), as long as \(_{}((,))\) is bounded away from zero. Appendix C.1 reviews some configurations of \((,,)\) that lead to a non-zero minimum eigenvalue, and Corollary 12 bounds the regret of ALExp with uniform exploration.

For this choice of agents and exploratory distribution, Theorem 1 presents an informal regret bound. Here, we have used the \(\) notation, and only included the fastest growing terms. The inequality is made exact in Theorem 14, up to constant multiplicative factors.

**Theorem 1** (Cumulative Regret of ALExp, Informal).: _Let \((0,1]\) and set \(\) to be the maximizer of (2). Choose learning rate \(_{t}=(C_{}t^{-1/2}/C(M,,d))\), exploration probability \(_{t}=(t^{-1/4})\) and Lasso regularization parameter \(_{t}=(C_{}t^{-1/2}C(M,,d))\), where_

\[C(M,,d)=(}+)}).\]

_Then ALExp satisfies the cumulative regret_

\[R(n)=n^{3/4}B+\,n^{3/4}C(M,,d)+ \ C_{}^{-1}C(M,,d) M\] \[+C_{}^{-1/2}n^{5/8} \]

_simultaneously for all \(n 1\), with probability greater than \(1-\)._

In this bound, the first term is the regret incurred at exploratory steps (when \(_{t}=1\)), the second term is due to the estimation error of Lasso (i.e., \(||-}_{t}||\)), and the third term is the regret of the exponential weights sub-algorithm. The fourth term, is the regret bound for the oracle agent \(j^{}\), when run within the ALExp framework. It does not depend on the agent's policy (greedy or optimistic), and is worse than the minimax optimal rate of \(\). This is because the oracle is suggesting actions based on the history \(H_{t}\), which includes uninformative action-reward pairs queried by other, potentially misspecified, agents. In Corollary 12, we provide a regret bound independent of \(C_{}\), for orthogonal feature maps, and show that the same \((\{M},n^{3/4}\})\) rate may be achieved even with the simple choice \(=()\).

### Proof Sketch

The regret is caused by two sources: selecting a sub-optimal agent, and an agent selecting a sub-optimal action. Accordingly, for any \(j\), we decompose the regret as

\[R(n)=_{t=1}^{n}r(^{})-r(_{t})=_{t=1}^{n}r(^ {})-r_{t,j}+_{t=1}^{n}r_{t,j}-r(_{t}).\] (3)

The first term shows the cumulative regret of agent \(j\), when run within ALExp. The second term evaluates the received reward against the cumulative average reward of model \(j\). We bound each term separately.

**Virtual Regret.** The first term \(_{j}(n)_{t=1}^{n}r(^{})-r_{t,j}\) compares the suggestion of agent \(j\), against the optimal action. We call it the virtual regret since the sequence \((_{t,j})_{t 1}\) of the actions suggested by model \(j\) are not necessarily selected by the meta algorithm, unless \(j_{t}=j\). This regret is merely a technical tool, and not actually realized when running ALExp. The virtual regret of the oracle agent may still be analyzed using standard techniques for linear bandits, e.g., Abbasi-Yadkori et al. (2011), however we need to adapt it to take into account a subtle difference: The confidence sequence of model \(j^{}\) is constructed according to the true sequence of actions \((_{t})_{t 1}\), while its virtual regret is calculated based on the virtual sequence \((_{t,j^{}})_{t 1}\), which the model suggests. The two sequences only match at the steps when model \(j^{}\) is selected. Adapting the analysis of Abbasi-Yadkori et al. (2011) to this subtlety, we obtain in Lemma 15 that with probability greater than \(1-\), simultaneously for all \(n 1\),

\[_{j^{}}(n)=(n^{5/8}C_{}^{-1/2}).\]

**Model Selection Regret.** The second term in (3) is the model selection regret, \(R(n,j)_{t=1}^{n}r_{t,j}-r(_{t})\), which evaluates the chosen action by ALExp against the suggestion of the \(j\)-th agent. Our analysis relies on a careful decomposition of \(R(n,j)\),

\[R(n,j)=_{t=1}^{n}-_{t,j}}_{}+ _{t,j}-_{i=1}^{M}q_{t,i}_{t,i}}_{}+ ^{M}q_{t,i}(_{t,i}-r_{t,i})}_{}+ ^{M}q_{t,i}r_{t,i}-r(_{t})}_{}.\]

We bound away each term in a modular manner, until we are left with the regret of the standard exponential weights algorithm. The terms (I) and (III) are controlled by the bias of the Lasso estimator, and are \((n^{3/4}C(M,,d))\) (Lemma 19). The last term (IV) is zero in expectation, and reflects the deviation of \(r(_{t})\) from its mean. We observe that the summands form a bounded Martingale difference sequence, and their sum grows as \(()\) (Lemma 18). Term (II) is the regret of our online optimizer, which depends on the variance of the Lasso estimator. We bound this term with \((C(M,,d) M)\), by first conducting a standard anytime analysis of exponential weights (Lemma 17), and then incorporating the anytime Lasso variance bound (Lemma 20). We highlight that neither of the above steps require assumptions about the base agents. Combining these steps, Lemma 16 establishes the formal bound on the model selection regret.

**Anytime Lasso.** We develop novel confidence intervals for Lasso with history-dependent data, which are uniformly valid over an unbounded time horizon. This result may be of independent interest in applications of Lasso for online learning or sequential decision making. Here, we use these confidence intervals to bound the bias and variance terms that appear in our treatment of the model selection regret. The width of the Lasso confidence intervals depends on the quality of feature matrix \(_{t}\), often quantified by the restricted eigenvalue property (Bickel et al., 2009; van de Geer and Buhlmann, 2009; Javanmard and Montanari, 2014):

**Definition 2**.: For the feature matrix \(_{t}^{t dM}\) we define \((_{t},s)\) for any \(1 s M\) as

\[(_{t},s) _{(J,)}}\|_{2}} {\|_{j}\|_{2}^{2}}}\] \[\ \ ^{d}\{0\},\ _{j J}\|_{j}\|_{2} 3 _{j J}\|_{j}\|_{2},\ J\{1,,M\},\ |J| s.\]Our analysis is in terms of this quantity, and Lemma 8 explains the connection between \((_{t},s)\) and \(C_{}\) as defined in (2), particularly that \((_{t},2)\) is also positive with a high probability.

**Theorem 3** (Anytime Lasso Confidence Sequences).: _Consider the data model \(y_{t}=^{}(_{t})+ _{t}\) for all \(t 1\), where \(_{t}\) is i.i.d. zero-mean sub-Gaussian noise, and \((_{t})_{t 1}\) is \((_{t})_{t 1}\)-predictable, where \(_{t}(_{1},,_{t}, _{1},,_{t-1})\). Then the solution of (1) guarantees_

\[( t 1:\ \|-}_{t}\|_{2}_{t}}{^{2}(_{t}, 2)}) 1-\]

_if the regularization parameter satisfies for all \(t 1\)_

\[_{t}}}( (2M/)+( d)_{+})+})}}.\]

Our confidence bound enjoys the same rate as Lasso with fixed (offline) data, up to \( d\) factors. We prove this theorem by constructing a self-normalized martingale sequence based on the \(_{2}\)-norm of the empirical process error (\(_{t}^{}_{t}\)). We then apply the "stitched" time-uniform boundary of Howard et al. (2021). Appendix B elaborates on this technique. Previous work on sparse linear bandits also include analysis of Lasso in an online setting, when \(_{t}\) is \(_{t}\) measurable. Cella and Pontil (2021) imitate offline analysis and then apply a union bound over the time steps, which multiplies the width by \( n\) and requires knowledge of the horizon. Bastani and Bayati (2020) also rely on knowledge of \(n\) and employ a scalar-valued Bernstein inequality on \(_{}\)-norm of the empirical process error, which inflates the width of the confidence sets by a factor of \(\). We work directly on the \(_{2}\)-norm, and use a curved boundary for sub-Gamma martingale sequences, which according to Howard et al. (2021) is uniformly tighter than a Bernstein bound, especially for small \(t\).

### Discussion

In light of Theorem 1 and Theorem 3, we discuss some properties of ALExp.

Sparse EXP4.Our approach presents a new connection between online learning and high-dimensional statistics. The rule for updating the probabilities in ALExp is inspired by the exponential weighting for Exploration and Exploitation with Experts (EXP4) algorithm, which was proposed by Auer et al. (2002) and has been extensively studied in the adversarial bandit and learning with expert advice literature (e.g., McMahan and Streeter, 2009; Beygelzimer et al., 2011). EXP4 classically uses importance-weighted (IW) or ordinary least squares (LS) estimators to estimate the return \(r_{t,j}\), both of which are unbiased but high-variance choices (Bubeck et al., 2012). In particular, in our linearly parametrized setting, the variance of IW and LS scales with \(Md\), which will lead to a \((M)\) regret. However, it is known that introducing bias can be useful if it reduces the variance (Zimmer and Lattimore, 2022). For instance, EXP3-IX (Kocak et al., 2014) and EXP4-IX (Neu, 2015) construct a biased IW estimator. Equivalently, others craft regularizers for the reward of the online optimizer, seeking to improve the bias-variance balance (e.g., Abernethy et al., 2008; Bartlett et al., 2008; Abernethy and Rakhlin, 2009; Bubeck et al., 2017; Lee et al., 2020; Zimmer and Lattimore, 2022). A key technical observation in this work is that our online Lasso estimator leads EXP4 to achieve sublinear regret which depends logarithmically on \(M\). This is due to the fact that while the estimator itself is \(Md\)-dimensional, its bias squared and variance scale with \(\). To the best of our knowledge, this work is first to instantiate the EXP4 algorithm with a sparse low-variance estimator.

Adaptive and Anytime.To estimate the reward, prior work on sparse bandits commonly emulate the Lasso analysis on offline data or on a martingale sequence with a known length (Hao et al., 2020; Bastani and Bayati, 2020). These works require a long enough sequence of exploratory samples, and knowledge of the horizon to plan this sequence. ALExp removes both of these constraints, and presents a fully adaptive algorithm. Crucially, we employ the elegant martingale bounds of Howard et al. (2021) to present the first time-uniform analysis of the Lasso with history-dependent data (Theorem 3). This way we can use all the data points and explore with a probability which vanishes at a \((t^{-1/4})\) rate. Our anytime confidence bound for Lasso, together with the horizon-independent analysis of the exponential weights algorithm, also allows ALExp to be stopped at any time with valid guarantees.

Rate Optimality.For \(M n\), we obtain a \((M})\) regret, which matches the rate conjectured by Agarwal et al. (2017). However, if \(M\) is comparable to \(n\) or smaller, our regret scales with \((n^{3/4})\), and while it is still sublinear and scales logarithmically with \(M\), the dependency on \(n\) is sub-optimal. This may be due to the conservative nature of our model selection analysis, during which we do not make assumptions about the dynamics of the base agents. Therefore, to ensure sufficiently diverse data for successful model selection, we need to occasionally choose exploratory actions with a vanishing probability of \(_{t}\). We conjecture that this is avoidable, if we make more assumptions about the agents, e.g., that a sufficient number of agents can achieve sublinear regret if executed in isolation. Banerjee et al. (2023) show that the data collected by sublinear algorithms organically satisfies a minimum eigenvalue lowerbound, which may also be sufficient for model selection. We leave this as an open question for future work.

## 6 Experiments

**Experiment Setup.** We create a synthetic dataset based on our data model (Section 3), and choose the domain to be \(1\)-dimensional \(=[-1,1]\). As a natural choice of features, we consider the set of degree-\(p\) Legendre polynomials, since they form an orthonormal basis for \(L^{2}()\) if \(p\) grows unboundedly. We construct each feature map, by choosing \(s\) different polynomials from this set, and therefore obtaining \(M=\) different models. More formally, we let \(_{j}(x)=(P_{j_{1}}(x),,P_{j_{s}}(x))^{s}\) where \(\{j_{1},,j_{s}\}\{0,,p\}\) and \(P_{j^{}}\) denotes a degree \(j^{}\) Legendre polynomial. To construct the reward function, we randomly sample \(j^{}\) from \([M]\), and draw \(_{j^{}}\) from an i.i.d. standard gaussian distribution. We then normalize \(||_{j^{}}||=1\). When sampling from the reward, we add Gaussian noise with standard deviation \(=0.01\). Figure 5 in the appendix shows how the random reward functions may look. For all experiments we set \(n=100\), and plot the cumulative regret \(R(n)\) averaged over \(20\) different random seeds, the shaded areas in all figures show the standard error across these runs.

**Algorithms.** We perform experiments on two UCB algorithms, one with oracle knowledge of \(j^{}\), and a naive one which takes into account all \(M\) feature maps. We run Explore-then-Commit (ETC) by Hao et al. (2020), which explores for a horizon of \(n_{0}\) steps, performs Lasso once, and then selects actions greedily for the remaining steps. As another baseline, we introduce Explore-then-Select (ETS) that explores for \(n_{0}\) steps, performs model selection using the sparsity pattern of the Lasso estimator. For the remaining steps, the policy switches to UCB, calculated based on the selected features. Performance of ETC and ETS depends highly on \(n_{0}\), so we tune this hyperparameter separately for each experiment. We also run Corral as proposed by Agarwal et al. (2017), with UCB agents similar to ALExp. We tune the hyper-parameters of Corral as well. To initialize ALExp we set the rates of \(_{t},_{t}\) and \(_{t}\) according to Theorem 1, and perform a light hyper-parameter tuning to choose the scaling constants. We have included the details and results of our hyper-parameter tuning in Appendix F.1. To solve (1), we use Celer, a fast solver for the group Lasso (Massias et al., 2018). Every time UCB policy is used, we set the exploration coefficient \(_{t}=2\), and every time exploration is required, we sample according to \(=()\). Appendix F includes the pseudo-code for all baseline algorithms.1

**Easy vs. Hard Cases.** We construct an easy problem instance, where \(s=2\), \(p=10\), and thus \(M=55\). Models are lightly correlated since each two model can have at most one Legendre polynomial in common. We also generate an instance with highly correlated feature maps where \(s=8\) and \(p=10\), which will be a harder problem, since out of the total \(M=55\) models, there are \(36\) models which have at least \(6\) Legendre polynomials in common with the oracle model \(j^{}\). Figure 1 shows that not only ALExp is not affected by the correlations between the models, but also it achievesa performance competitive to the oracle in both cases, implying that our exponential weights technique for model selection is robust to choice of features. ETC and ETS rely on Lasso for model selection, which performs poorly in the case of correlated features. Corral uses log-barrier-OMD with an importance-weighted estimator, which has a significantly high variance. The curve for Corral in Figures 1 and 2 is cropped since the regret values get very large. Figure 6 shows the complete results. We construct another hard instance (Fig. 2), where the model class is large (\(s=3,p=10,M=165\)). ALExp continues to outperform all baselines with a significant gap. It is clear in the regret curves how explore-then-commit style algorithms are inherently horizon-dependent, and may exhibit linear regret, if stopped at an arbitrary time. This is not an issue with the other algorithms.

**Scaling with M.** Figure 3 shows how well the algorithms scale as \(M\) grows. For this experiment we set \(s=2\) and change \(p\{9,,13\}\). While increasing \(M\) hardly affects ALExp and Oracle UCB, other baselines become less and less sample efficient.

**Learning Dynamics of ALExp.** Figure 4 gives some insight into the dynamics of ALExp when \(M=165\). In particular, it shows how ALExp can rule out sub-optimal agents without ever having queried them. Figure (a) shows the distribution \(_{t}\), at \(t=20\) which is roughly equal to the optimal \(n_{0}\) for ETC in this configuration. The oracle model \(j^{}\) is annotated with a star, and has the highest probability of selection. We observe that, already at this time step, more than \(80\%\) of the agents are practically ruled out, due to small probability of selection. However, according to Figure (b), which shows \(M_{t}\) the total number of visited models, less than \(10\%\) of the models are queried at \(t=20\). This is the key practical benefit of ALExp compared to black-box algorithms such as Corral. Lastly, Figure (c) shows how \(q_{t,j^{}}\) the probability of selecting the oracle agent changes with time. While this probability is higher than that of the other agents, Figure (c) shows that \(q_{t,j^{}}\) is not exceeding \(0.25\), therefore there is always a probability of greater than \(0.75\) that we sample another agent, making ALExp robust to hard problem instances where many agents perform efficiently. We conclude that ALExp seems to rapidly recognize the better performing agents, and select among them with high probability.

## 7 Conclusion

We proposed ALExp, an algorithm for simultaneous online model selection and bandit optimization. As a first, our approach leads to anytime valid guarantees for model selection and bandit regret, and does not rely on a priori determined exploration schedule. Further, we showed how the Lasso can be used together with the exponential weights algorithm to construct a low-variance online learner. This new connection between high-dimensional statistics and online learning opens up avenues for future research on high-dimensional online learning. We established empirically that ALExp has favorable exploration-exploitation dynamics, and outperforms existing baselines. We tackled the open problem of Agarwal et al. (2017), and showed that \( M\) dependency for regret is achievable for linearly parametrizable rewards. This problem remains open for more general, non-parametric reward classes.

Figure 4: ALExp can rule out models without ever having queried them (\(M=165\))

Figure 3: ALExp is hardly affected by increasing the number of models (y-axis have various scales)