# Unlocking the Potential of Global Human Expertise

Elliot Meyerson\({}^{1}\) Olivier Francon\({}^{1}\) Darren Sargent\({}^{1}\) Babak Hodjat\({}^{1}\) Risto Miikkulainen\({}^{1,2}\)

\({}^{1}\)Cognizant AI Labs \({}^{2}\)The University of Texas at Austin

{elliot.meyerson,olivier.francon,darren.sargent,babak,risto}@cognizant.com

###### Abstract

Solving societal problems on a global scale requires the collection and processing of ideas and methods from diverse sets of international experts. As the number and diversity of human experts increase, so does the likelihood that elements in this collective knowledge can be combined and refined to discover novel and better solutions. However, it is difficult to identify, combine, and refine complementary information in an increasingly large and diverse knowledge base. This paper argues that artificial intelligence (AI) can play a crucial role in this process. An evolutionary AI framework, termed RHEA, fills this role by distilling knowledge from diverse models created by human experts into equivalent neural networks, which are then recombined and refined in a population-based search. The framework was implemented in a formal synthetic domain, demonstrating that it is transparent and systematic. It was then applied to the results of the XPRIZE Pandemic Response Challenge, in which over 100 teams of experts across 23 countries submitted models based on diverse methodologies to predict COVID-19 cases and suggest non-pharmaceutical intervention policies for 235 nations, states, and regions across the globe. Building upon this expert knowledge, by recombining and refining the 169 resulting policy suggestion models, RHEA discovered a broader and more effective set of policies than either AI or human experts alone, as evaluated based on real-world data. The results thus suggest that AI can play a crucial role in realizing the potential of human expertise in global problem-solving.

## 1 Introduction

Integrating knowledge and perspectives from a diverse set of experts is essential for developing better solutions to societal challenges, such as policies to curb an ongoing pandemic, slow down and reverse climate change, and improve sustainability . Increased diversity in human teams can lead to improved decision-making , but as the scale of the problem and size of the team increases, it becomes difficult to discover the best combinations and refinements of available ideas . This paper argues that artificial intelligence (AI) can play a crucial role in this process, making it possible to realize the full potential of diverse human expertise. Though there are many AI systems that take advantage of human expertise to improve automated decision-making , an approach to the general problem must meet a set of unique requirements: It must be able to incorporate expertise from diverse sources with disparate forms; it must be multi-objective since conflicting policy goals will need to be balanced; and the origins of final solutions must be traceable so that credit can be distributed back to humans based on their contributions. An evolutionary AI framework termed RHEA (for Realizing Human Expertise through AI) is developed in this paper to satisfy these requirements. Evolutionary AI, or population-based search, is a biologically-inspired method that often leads to surprising discoveries and insights ; it is also a natural fit here since the development of ideas in human teams mirrors an evolutionary process . Implementing RHEA for a particular application requires the following steps (Fig. 1):1. **Define**. Define the problem in a formal manner so that solutions from diverse experts can be compared and combined.
2. **Gather**. Solicit and gather solutions from a diverse set of experts. Solicitation can take the form of an open call or a direct appeal to known experts.
3. **Distill**. Use machine learning to convert (distill) the internal structure of each gathered solution into a canonical form such as a neural network.
4. **Evolve**. Recombine and refine the distilled solutions using a population-based search to realize the complementary potential of the ideas in the expert-developed solutions.

RHEA is first illustrated through a formal synthetic example below, demonstrating how this process can result in improved decision-making. RHEA is then put to work in a large-scale international experiment on developing non-pharmaceutical interventions for the COVID-19 pandemic. The results show that broader and better policy strategies can be discovered in this manner, beyond those that would be available through AI or human experts alone. The results also highlight the value of soliciting diverse expertise, even if some of it does not have immediately obvious practical utility: AI may find ways to recombine it with other expertise to develop superior solutions.

Figure 1: _The RHEA (Realizing Human Expertise through AI) framework._ The framework consists of four components: Defining the prediction and prescription tasks, gathering the human solutions, distilling them into a canonical form, and evolving the population of solutions further. **a,** The _predictor_ maps context and actions to outcomes and thus constitutes a surrogate, or a “digital twin”, of the real world. For example, in the Pandemic Response Challenge experiment, the context consisted of data about the geographic region for which the predictions were made, e.g., historical data of COVID-19 cases and intervention policies; actions were future schedules of intervention policies for the region; and outcomes were predicted future cases of COVID-19 along with the stringency of the policy. **b,** Given a predictor, the _prescriptor_ generates actions that yield optimized outcomes across contexts. **c,** Humans are solicited to contribute expertise by submitting prescriptors using whatever methodology they prefer, such as decision rules, epidemiological models, classical statistical techniques, and gradient-based methods. **d,** Each submitted prescriptor is distilled into a canonical neural network that replicates its behavior. **e,** This population of neural networks is evolved further, i.e., the distilled models are recombined and refined in a parallelized, iterative search process. They build synergies and extend the ideas in the original solutions, resulting in policies that perform better than the original ones. For example, in the Pandemic Response Challenge, the policies recommend interventions that lead to minimal cases with minimal stringency.

To summarize, the main contributions of this paper are as follows: (1) Recognizing that bringing together diverse human expertise is a key challenge in solving many complex problems; (2) Identifying desiderata for an AI process that accomplishes this task; (3) Demonstrating that existing approaches do not satisfy these desiderata; (4) Formalizing a new framework, RHEA, to satisfy them; (5) Instantiating a first concrete implementation of RHEA using standard components; and (6) Evaluating this implementation in a global application: The XPRIZE Pandemic Response Challenge.

## 2 Illustrative Example

In this section, RHEA is applied to a formal synthetic setting where its principles and mechanics are transparent. It is thus possible to demonstrate how they can lead to improved results, providing a roadmap for when and how to apply it to real-world domains (see App. B for additional details).

Consider a policy-making scenario in which many new reasonable-sounding policy interventions are constantly being proposed, but there are high levels of nonlinear interaction between interventions and across contexts. Such interactions are a major reason why it is difficult to design effective policies and the main challenge that RHEA is designed to solve. They are unavoidable in complex real-world domains such as public health (e.g., between closing schools, requiring masks, or limiting international travel), traffic management (e.g., adding buses, free bus tokens, or bike lanes), and climate policy (e.g., competing legal definitions of "net-zero" or "green hydrogen", and environmental feedback loops) [19; 52; 60]. In such domains there exist diverse experts--e.g., policymakers, economists, scientists, local community leaders, and other stakeholders--whose input is worth soliciting before implementing interventions. In RHEA, this policy-making challenge can be formalized as follows:

**Define.** Suppose we are considering policy interventions \(a_{1},,a_{n}\). A policy action \(A\) consists of some subset of these. Suppose we must be prepared to address contexts \(c\{c_{1},,c_{m}\}\), and we have a black-box predictor \((c,A)\) to evaluate utility (Fig. 0(a)). In practice, \(\) will be a complex dynamical model such as an agent-based or neural-network-based predictor. In this example, to highlight the core behavior of RHEA, \(\) is a simple-to-define function containing the kinds of challenging nonlinearities we would like to address, such as context dependence, synergies, anti-synergies, threshold effects, and redundancy (the full utility function is detailed in Eq. 1). Similarly, \(\) is a simple cost function, defined as the total number of prescribed policy interventions. A prescriptor is a function \((c)=A\) (Fig. 0(b)). The goal is to find a Pareto front of prescriptors across the outcomes of utility \(\) and cost \(\). Note that the search space is vast: There are \(2^{mn}\) possible prescriptors.

**Gather.** Suppose prescriptors of unknown functional form have been gathered (Fig. 0(c)) from three experts: one "generalist", providing general knowledge that applies across contexts (see Fig. 1(c) for an example); and two "specialists", providing knowledge that is of higher quality (i.e. lower cost-per-utility) but applies only to a few specific contexts (Fig. 1(a)-0).

**Distill.** Datasets for distillation can be generated by running each expert prescriptor over all contexts. The complete behavior of a prescriptor can then be visualized as a binary grid, where a black cell indicates the inclusion of an intervention in the prescription for a given context (Fig. 1(a)-0). This data can be used to convert the expert prescriptors into rule sets or neural networks (Fig. 0(d), App. B.2).

**Evolve.** These distilled models can then be injected into an initial population and evolved using multi-objective optimization  (Fig. 0(e)). The full optimal Pareto front is obtained as a result.

With this formalization, it is possible to construct a synthetic example of RHEA in action, as shown in Fig. 2. It illustrates the optimal Pareto front. Importantly, this front is discoverable by RHEA, but not by previous machine learning techniques such as Mixture-of-Experts (MoE)  or Weighted Ensembles , or by the experts alone. RHEA is able to recombine the internal structure of experts across contexts (e.g., by adding \(a_{3},a_{4},a_{5}\) to \(a_{1},a_{2}\) in \(c_{1}\)). It can innovate beyond the experts by adding newly-applicable interventions (\(a_{6}\)). It can also refine the results by removing interventions that are now redundant or detrimental (\(a_{5}\) in \(c_{2}\)), and by mixing in generalist knowledge. In contrast, the discoveries of MoE are restricted to mixing expert behavior independently at each context, and Weighted Ensemble solutions can only choose a single combination of experts to apply everywhere.

The domain also illustrates why it is important to utilize expert knowledge in the first place. The high-dimensional solution space makes it very difficult for evolution alone (i.e. not starting from distilled expert prescriptors) to find high-quality solutions, akin to finding needles in a haystack. Experimental results confirm that RHEA discovers the entire optimal Pareto front reliably, even as the number of available interventions increases, while evolution alone does not (App. Fig. 6). Multi-objective reinforcement learning (MORL) methods also struggle in this domain (App. Fig. 7,8). Thus, RHEA harnesses the latent potential of expert solutions. It uses pieces of them as building blocks and combines them with novel elements to take full advantage of them. This ability can be instrumental in designing effective policies for complex real-world tasks. Next, RHEA is put to work on one particularly vexing task: optimizing pandemic intervention policies.

## 3 The XPRIZE Pandemic Response Challenge

The XPRIZE Pandemic Response Challenge [10; 11] presented an ideal opportunity for demonstrating the RHEA framework. XPRIZE is an organization that conducts global competitions, fueled by large cash prizes, to motivate the development of underfunded technologies. Current competitions target wildfires, desalination, carbon removal, meat alternatives, and healthy aging . In 2020 and 2021, the XPRIZE Pandemic Response Challenge was designed and conducted , challenging participants to develop models to suggest optimal policy solutions spanning the tradeoff between minimizing new COVID-19 cases and minimizing the cost of implemented policy interventions.

**Define.** The formal problem definition was derived from the Oxford COVID-19 government response tracker dataset [27; 54; 74], which was updated daily from March 2020 through December 2022. This dataset reports government intervention policies ("IPs") on a daily basis, following a standardized classification of policies and corresponding ordinal stringency levels in \(_{5}\) (used to define IP "cost") to enable comparison across geographical regions ("geos"), which include nations and subnational regions such as states and provinces. The XPRIZE Challenge focused on 235 geos (App. Fig 9) and those 12 IPs over which governments have immediate daily control : school closings, workplace closings, cancellation of public events, restrictions on gathering size, closing of public transport,

Figure 2: _An Illustration of RHEA in a Synthetic Domain._ The plots show the Pareto front of prescriptors discovered by RHEA vs. those of alternative prescriptor combination methods, highlighting the kinds of opportunities RHEA is able to exploit. The specialist expert prescriptors **a** and **b** and the generalist expert prescriptor **c** are useful but suboptimal on their own (purple \(\)’s). RHEA recombines and innovates upon their internal structure and is able to discover the full optimal Pareto front (blue \(\)’s). This front dominates that of Mixture-of-Experts (MoE; green \(\)’s), which can only mix expert behavior independently in each context. It also dominates that of Weighted Ensembling (yellow \(+\)’s), which can only choose a single combination of experts to apply everywhere. Evolution alone (without expert knowledge) also struggles in this domain due to the vast search space (App. Fig. 6), as do MORL methods (App. Fig. 7,8). Thus, RHEA unlocks the latent potential in expert solutions.

stay at home requirements, restrictions on internal movement, restrictions on international travel, public information campaigns, testing policy, contact tracing, and facial covering policy. Submissions for Phase 1 were required to include a runnable program ("predictor") that outputs predicted cases given a geo, time frame, and IPs through that time frame (Fig. 1a). Submissions for Phase 2 were required to include a set of runnable programs ("prescriptors"), which, given a geo, time frame, and relative IP costs, output a suggested schedule of IPs ("prescription") for that geo and time frame (Fig. 1b). By providing a set of prescriptors, submissions could cover the tradeoff space between minimizing the cost of implementing IPs and the expected number of new cases. Since decision makers for a particular geo could not simultaneously implement multiple prescriptions from multiple teams, prescriptions were evaluated not in the real world but with a predictor \(\) (from Phase 1), which forecasts how case numbers change as a result of a prescription. The formal problem definition, requirements, API, and code utilities are publicly available . Teams were encouraged to incorporate specialized knowledge in geos with which they were most familiar. The current study focuses on the prescriptors created in Phase 2. There are \(\!10^{620}\) possible schedules for a _single geo_ for 90 days, so brute-force search is not an option. To perform well, prescriptors must implement principled ideas to capture domain-specific knowledge about the structure of the pandemic.

**Gather.** Altogether, 102 teams of experts from 23 countries participated in the challenge. Some teams were actively working with local governments to inform policy ; other organizations served as challenge partners, including the United Nations ITU and the City of Los Angeles . The set of participants was diverse, including epidemiologists, public health experts, policy experts, machine learning experts, and data scientists. Consequently, submissions took advantage of diverse methodologies, including epidemiological models, decision rules, classical statistical methods, gradient-based optimization, various machine learning methods, and evolutionary algorithms, and exploited various auxiliary data sources to get enhanced views into the dynamics of particular geos  (Fig. 1c). The Phase 2 evaluations showed substantial specialization to different geos for different teams, a strong indication that there was diversity that could be harnessed. Many submissions also showed remarkable improvement over strong heuristic baselines, indicating that high-quality expertise had been gathered successfully. Detailed results of the competition are publicly available ; this study focuses on the ideas in them in the aggregate.

**Distill.** A total of 169 prescriptors were submitted to the XPRIZE Challenge. After the competition, for each of these gathered prescriptors \(_{i}\), an autoregressive neural network (NN) \(_{i}\) with learnable parameters \(_{i}\) was trained with gradient descent to mimic its behavior, i.e. to distill it  (Fig. 1d; App. C.1). Each NN was trained on a dataset of 212,400 input-output pairs, constructed by querying the corresponding prescriptor \(n_{q}\) times, i.e., through behavioral cloning:

\[_{i}^{*} =*{argmin}_{_{i}}_{}p(q)\| _{i}(q)-_{i}(q,_{i}(q),);_{i} \|_{1}dq\] (1) \[\,*{argmin}_{_{i}}} _{j=1}^{n_{q}}\|_{i}(q_{j})-_{i}(q,_{i}(q_{ j}),);_{i}\|_{1},\] (2)

where \(q\) is a query and \(\) is a function that maps queries (specified via the API in Define) to input data, i.e., _contexts_, with a canonical form. Each (date range, geo) pair defines a query \(q\), with \(_{i}(q)_{5}^{90 12}\) the policy generated by \(_{i}\) for this geo and date range, and \((q,_{i}(q))^{90}\) the predicted (normalized) daily new cases. Distilled models were implemented in Keras  and trained with Adam  using L1 loss (since policy actions were on an ordinal scale) (see App. C.1).

**Evolve.** These 169 distilled models were then placed in the initial population of an evolutionary AI process (Fig. 1e). This process was based on the same Evolutionary Surrogate-assisted Prescription (ESP) method  previously used to evolve COVID-19 IP prescriptors from scratch . In standard ESP, the initial population (i.e., before any evolution takes place) consists only of NNs with randomly generated weights. By replacing random neural networks with the distilled neural networks, ESP starts from diverse high-quality expert-based solutions, instead of low-quality random ones. ESP can then be run as usual from this starting point, recombining and refining solutions over a series of generations to find better tradeoffs between stringency and cases, using Pareto-based multi-objective optimization  (App. C.2). Providing a Pareto front of policy strategy options is critical, because most decision-makers will not simply choose the most extreme strategies (i.e. IPs with maximum stringency, or no IPs at all), but are likely to choose a tradeoff point appropriate for their particular political, social and economic scenario (Fig. 3d shows the real-world distribution of IP stringencies).

Evolution from the distilled models was run for 100 generations in 10 independent trials to produce the final RHEA models. As a baseline, evolution was run similarly from scratch. As a second baseline, RHEA was compared to the full set of distilled models. A third baseline was models with randomly initialized weights, which is often a meaningful starting point in NN-based policy search . All prescriptor evaluations, including those during evolution, were performed using the same reference predictor as in the XPRIZE Challenge itself; this predictor was evaluated in depth in prior work .

_Results._ The performance results are shown in Fig. 3. As is clear from the Pareto plots (Fig. 3a-c) and across a range of metrics (Fig. 3e-i), the distilled models outperform the random initial models, thus confirming the value of human insight and the efficacy of the distillation process. Evolution then improves performance substantially from both initializations, with distilled models leading to the best solutions. Thus, the conclusions of the illustrative example are substantiated in this real-world domain: RHEA is able to leverage knowledge contained in human-developed models to discover solutions beyond those from the AI alone or humans alone. The most critical performance metric is the empirical R1-metric (REM; ), which estimates the percentage of time a decision-maker with a fixed stringency budget would choose a prescriptor from a given approach among those from all approaches. For RHEA, REM is nearly 100%. In other words, not only does RHEA discover policies that perform better, but they are also policies that decision-makers would be likely to adopt.

## 4 Characterizing the Innovations

Two further sets of analyses characterize the RHEA solutions and the process of discovering them. First, IP schedules generated for each geo by different sets of policies were projected to 2D via

Figure 3: _Quantitative comparison of solutions._**a,** Objective values for all solutions in the final population of a single representative run of each method. **b,** Pareto curves for these runs. Distilled provides improved tradeoffs over Random and Evolved (from random), and RHEA pushes the front out beyond Distilled. **c,** Overall Pareto front of the union of the solutions from these runs. The vast majority of these solutions are from RHEA. **d,** The distribution of actual stringencies implemented in the real world across all geos at the prescription start date, indicating which Pareto solutions real-world decision makers would likely select, i.e., which tradeoffs they prefer. **e,** Given this distribution, the proportion of the time the solution selected by a user would be from a particular method (the REM metric); almost all of them would be from RHEA. **f,** The same metric, but based on a uniform distribution of tradeoff preference (RUN) **g,** Domination rate (DR) w.r.t. Distilled, i.e. how much of the Distilled Pareto front is strictly dominated by another method’s front. While Evolved (from scratch) sometimes discovers better solutions than those distilled from expert designs, RHEA improves \(\)75% of them. **h,** Max reduction of cases (MCR) compared to Distilled across all stringency levels. **i,** Dominated hypervolume improvement (HVI) compared to Distilled. For each metric, RHEA substantially outperforms the alternatives, demonstrating that it creates improved solutions over human and AI design, and that those solutions would likely be preferred by human decision-makers. (Bars show mean and st.dev. See App. C.3 for technical details of each metric.)

UMAP  to visualize the distribution of their behavior (Fig. 4a). Note that the schedules from the highest-performing (Pareto) submitted policies form a continuous 1D manifold across this space, indicating continuity of tradeoffs. This manifold serves as scaffolding upon which RHEA recombines, refines, and innovates; these processes are the same as in the illustrative example, only more complex. Evolution alone, on the other hand, produces a discordant scattering of schedules, reflecting its unconstrained exploratory nature, which is disadvantageous in this domain. What kind of structure does RHEA harness to move beyond the existing policies? Five high-level properties were identified that characterize how RHEA draws on submitted models in this domain: _swing_ measures the stringency difference between the strictest and least strict day of the schedule; _separability_ measures to what extent the schedule can be separated into two contiguous phases of different stringency levels; _focus_ is inversely proportional to the number of IPs used; _agility_ measures how often IPs change; and _periodicity_ measures how much of the agility can be explained by weekly periodicity

Figure 4: _Dynamics of IP schedules discovered by RHEA._ **a,** UMAP projection of geo IP schedules generated by the policies (App. C.4). The schedules from high-performing submitted expert models are concentrated around a 1-dimensional manifold organized by overall cost (seen as a yellow arc). This manifold provides a scaffolding upon which RHEA elaborates, interpolates, and expands. Evolved policies, on the other hand, are scattered more discordantly (seen as blue clusters), ungrounded by the experts. **b,** To characterize how RHEA expands upon this scaffolding, five high-level properties of IP schedules were identified and their distributions were plotted across the schedules. For each, RHEA finds a balance between the grounding of expert submissions (i.e., regularization) and their recombination and elaboration (i.e., innovation), though this balance manifests in distinct ways. For swing and separability, RHEA is similar to real schedules, but finds that the high separability proposed by some expert models can sometimes be useful. RHEA finds the high focus of the expert models even more attractive; in practice, they could provide policy-makers with simpler and clearer messages about how to control the pandemic. For focus, agility, and periodicity, RHEA pushes beyond areas explored by the submissions, finding solutions that humans may miss. The example schedules shown in **a(i-v)** illustrate these principles in practice (rows are IPs sorted from top to bottom as listed in Sec. 3; column are days in the 90-day period; darker color means more stringent). **(i)** Real-world examples demonstrate that although agility and periodicity require some effort to implement, they have occasionally been utilized (e.g. in Portugal and France); **(ii)** a simple example of how RHEA generates useful interpolations of submitted non-Pareto schedules, demonstrating how it realizes latent potential even in some low-performing solutions, far from schedules evolved from scratch; **(iii)** another useful interpolation, but achieved via higher agility than Pareto submissions; **(iv)** a high-stringency RHEA schedule that trades swing and separability for agility and periodicity compared to its submitted neighbor; and **(v)** a medium-stringency RHEA schedule with lower swing and separability and higher focus than its submitted neighbor. Overall, these analyses show how RHEA realizes the latent potential of the raw material provided by the human-created submissions.

(Fig. 4b; App. C.4). Some ideas from submitted policies, e.g., increased separability and focus, are readily incorporated into RHEA policies. Others, e.g. increased focus, agility, and periodicity, RHEA is able to utilize beyond the range of policies explored by the human designs. The examples in Fig. 4a illustrate these properties in practice. Example (i) shows a number of real policies, suggesting that goes are capable of implementing diverse and innovative schedules similar to those discovered by RHEA; e.g., weekly periodicity was actually implemented for a time in Portugal and France. Examples (ii-v) show RHEA schedules and their nearest submitted neighbors, demonstrating how innovations can manifest as interpolations or extrapolations of submitted policies. For instance, one opportunity is to focus on a smaller set of IPs; another is to utilize greater agility and periodicity. This analysis shows how RHEA can lead to insights on where improvements are possible.

Second, to understand how RHEA discovered these innovations, an evolutionary history can be reconstructed for each solution, tracing it back to its initial distilled ancestors (Fig. 5). Some final solutions stem from a single beneficial crossover of distilled parents, while others rely on more complex combinations of knowledge from many ancestors (Fig. 5a). While the solutions are more complex, the evolutionary process is similar to that of the illustrative example: It proceeds in a principled manner, with child models often falling between their parents along the case-stringency tradeoff (Fig. 5b-c). Based on these evolutionary histories, one can compute the relative contribution of each expert model to the final RHEA Pareto front (App C.5). These contributions are highly consistent across independent runs, indicating that the approach is reliable (Fig. 5d). Indeed in the XPRIZE competition, this contribution amount was used as one of the quantitative metrics of solution quality . Remarkably, although there is a correlation between the performance of expert models and their contribution to the final front, there are also models that do not perform particularly well, but end up making outsized contributions through the evolutionary process (Fig. 5e; see also Fig. 4a(ii)). This result highlights the value of soliciting a broad diversity of expertise, even if some of it does not have immediately obvious practical utility. AI can play a role in realizing this latent potential.

Figure 5: _Dynamics of evolutionary discovery process._**a,** Sample ancestries of prescriptors on the RHEA Pareto front. Leaf nodes are initial distilled models; the final solutions are the root. The history of recombinations leading to different solutions varies widely in terms of complexity, with apparent motifs and symmetries. The ancestries show that the search is behaving as expected, in that the cost of the child usually lands between the costs of its parents (indicated by color). This property is also visualized in **b** (and **c**), where child costs (and cases) are plotted over all recombinations from all trials (k-NN regression, k = 100). **d,** From ancestries, one can compute the relative contribution of each expert model to the final RHEA Pareto front (App C.5). This contribution is remarkably consistent across the independent runs, indicating that the approach is reliable (mean and st.dev. shown). **e,** Although there is a correlation between the performance of teams of expert models and their contribution to the final front, there are some teams with unimpressive quantitative performance in their submissions who end up making outsized contributions through the evolutionary process. This result highlights the value of soliciting a broad diversity of expertise, even if some of it does not have immediately obvious practical utility. AI can play a role in realizing this latent potential.

Discussion

Alternative Policy Discovery Methods.Our implementation of RHEA uses established methods in both the Distill and Evolve steps; the technical novelty comes from their unique combination in RHEA to unlock diverse human expertise. Popular prior methods for combining diverse models include ensembling  and Mixture-of-Experts , but, as highlighted in Fig. 2, although multi-objective variants have been explored in prior work , neither of these methods can innovate beyond the scaffolding provided by the initial experts. Evolution is naturally suited for this task: Crossover is a powerful way to recombine expert models, mutation allows innovating beyond them, and population-based search naturally supports multiobjective optimization. Other approaches for policy optimization include contextual bandits , planning-based methods , and reinforcement learning [29; 69], and an interesting question is how they might play a role in such a system. One approach could be to use evolutionary search for recombination and use another method for local improvement, akin to hybrid approaches used in other settings  (See App. A for a longer discussion).

Theory.It is intuitive why expert knowledge improves RHEA's search capability. However, any theoretical convergence analysis will depend on the particular implementation of RHEA. The present implementation uses NSGA-II, the convergence of which has recently been shown to depend critically on the size of jumps in the optimization landscape, i.e. roughly the maximum size of non-convex regions [20; 21]. On the OneJumpZeroJump benchmark, the tightest known upper-bound for convergence to the full ground truth Pareto front is \(O(N^{2}n^{k}/(k)^{k})\), where \(k\) is a measure of the jump size, \(n\) is the problem dimensionality, and \(N\) is the (sufficiently large) population size. In other words, a smaller jump size leads to a drastic convergence speed up. Distilling useful, diverse experts is conceptually analogous to decreasing the jump size. This effect is apparent in the illustrative domain, where the experts provide building blocks that can be immediately recombined to discover better solutions, but that are difficult to discover from scratch (Fig. 2). This interpretation is borne out in the experiments: RHEA continues to converge quickly as the action space (i.e. problem dimensionality) increases, whereas evolution regresses to only being able to discover the most convex (easily-discoverable) portions of the Pareto front (App. Fig. 6).

Generalizability.RHEA can be applied effectively to policy-discovery domains where (1) the problem can be formalized with contexts, actions, and outcomes, (2) there exist diverse experts from which solutions can be gathered, and (3) the problem is sufficiently challenging. In contrast, RHEA would not be effective, (1) if the problem is too easy, so that the input from human experts would not be necessary, (2) if the problem is hard, but no useful and diverse experts exist, and (3) if there is no clear way to define context and/or action variables upon which the experts agree.

The modularity of RHEA allows different implementations of components to be designed for different domains, such as those related to sustainability, engineering design, and public health. One particularly exciting opportunity for RHEA is climate policy, which often includes complex interactions between multiple factors . For example, given the context of the current state of the US energy grid and energy markets, the green hydrogen production subsidies introduced by the Inflation Reduction Act will in fact lead to _increases_ in carbon emissions, _unless_ the Treasury Department enacts three distinct regulations in the definition of "green hydrogen" . It is precisely this kind of policy combination that RHEA could help discover, and such a discovery process could be an essential part of a climate policy application. For example, the En-Roads climate simulator supports diverse actions across energy policy, technology, and investment, contexts based on social, economic, and environmental trajectories, and multiple competing outcomes, including global temperature, cost of energy, and sea-level rise . Users craft policies based on their unique priorities and expertise. RHEA could be used with a predictor like En-Roads to discover optimized combinations of expert climate policies that trade-off across temperature change and other the outcomes that users care about most.

Ethics and Broader Impact.As part of the UN AI for Good Initiative, we are currently building a platform for formalizing and soliciting expert solutions to SDG goals more broadly . Ethical considerations when deploying such systems are outlined below. See App. D for further discussion.

_Fairness._ In such problems with diverse stakeholders, breaking down costs and benefits by affected populations and allowing users to input explicit constraints to prescriptors can be crucial for generating feasible and equitable models. In this platform, RHEA could take advantage of knowledge that local experts provide and learn to generalize it; by treating each contributed model as a black box,it is agnostic to the type of models used, thus helping to make the platform future-proof. Fairness constraints can also be directly included in RHEA's multiple objectives.

_Governance and Democratic Accountability._ An important barrier in the adoption of AI by real-world decision-makers is trust . For example, such systems could be used to justify the decisions of bad actors. RHEA provides an advantage here: If the initial human-developed models it uses are explainable (e.g. are based on rules or decision trees), then a user can trust that suggestions generated by RHEA models are based on sensible principles, and can trace and interrogate their origins. Even when the original models are opaque, trust can be built by extracting interpretable rules that describe prescriptor behavior, which is feasible when the prescriptors are relatively compact and shallow , as in the experiments in this paper. That is, RHEA models can be effectively audited--a critical property for AI systems maintained by governments and other policy-building organizations.

_Data Privacy and Security._ Since experts submit complete prescriptors, no sensitive data they may have used to build their prescriptors needs to be shared. In the Gather step in Sec. 3, each expert team had an independent node to submit their prescriptors. The data for the team was generated by running their prescriptors on their node. The format of the data was then automatically verified, to ensure that it complied with the Defined API. Verified data from all teams was then aggregated for the Distill & Evolve steps. Since the aggregated data must fit an API that does not allow for extra data to be disclosed, the chance of disclosing sensitive data in the Gather phase is minimized.

_External Oversight._ Although the above mechanisms could all yield meaningful steps in addressing a broad range of ethical concerns, they cannot completely solve all issues of ethical deployment. So, it is critical that the system is not deployed in an isolated way, but integrated into existing democratic decision-making processes, with appropriate external oversight. Any plan for deployment should include a disclosure of these risks to weigh against the potential societal benefits.

_Sustainability and Accessibility._ Due to the relatively compact model size, RHEA uses orders of magnitude less compute and energy than many other current AI systems, which is critical for creating uptake by decision-makers around the world who do not have access to extensive computational resources or for whom energy usage is becoming an increasingly central operational consideration.

Limitations.Understanding the limitations of the presented RHEA implementation is critical for establishing directions for future work. The cost measure used in this paper was uniform over IPs, an unbiased way to demonstrate the technology, but, for a prescriptor to be used in a particular geo, costs of different IPs should be calibrated based on geo-specific cost-analysis. The geo may also have some temporal discounting in its cases and cost objectives. For consistency with the XPRIZE, they were not included in the experiments in this paper but can be naturally incorporated into RHEA in the future. When applying surrogate-developed policies to the real world, approximation errors can compound over time. Thus, user-facing applications of RHEA could benefit from the inclusion of uncertainty measures , inverse reinforcement learning , as well as humans-in-the-loop to prevent glaring errors. Distillation could also be limited in cases where expert models use external data sources with resulting effects not readily approximated by the inputs specified in the defined API. If this were an issue in future applications, it could be addressed by training models that generalize across domain spaces . RHEA prescriptors were evaluated in the same surrogate setting as prescriptors in the XPRIZE, but not yet in hands-on user studies. Hands-on user evaluation is a critical step but requires a completely different kind of research effort, i.e. one that is political and civil, rather than computational. Our hope is that the publication of the results of RHEA makes the real-world incorporation of these kinds of AI decision-assistants more likely.

Conclusion.This paper motivated, designed, and evaluated a framework called RHEA for bringing together diverse human expertise systematically to solve complex problems. The promise of RHEA was illustrated with an initial implementation and an example application; it can be extended to other domains in future work. The hope is that, as a general and accessible system that incorporates input from diverse human sources, RHEA will help bridge the gap between human-only decision-making and AI-from-data-only approaches. As a result, decision-makers can start adopting powerful AI decision-support systems, taking advantage of the latent real-world possibilities such technologies illuminate. More broadly, the untapped value of human expertise spread across the world is immense. Human experts should be actively encouraged to continually generate diverse creative ideas and contribute them to collective pools of knowledge. This study shows that AI has a role to play in realizing the full value of this knowledge, thus serving as a catalyst for global problem-solving.