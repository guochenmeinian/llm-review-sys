# Visual Anchors Are Strong Information Aggregators

For Multimodal Large Language Model

 Haogeng Liu\({}^{1,2}\), Quanzeng You\({}^{3}\), Xiaotian Han\({}^{3}\), Yongfei Liu\({}^{3}\),

**Huaibo Huang\({}^{1,2}\), Ran He\({}^{1,2}\), Hongxia Yang**

\({}^{1}\)MAIS & NLPR, Institute of Automation, Chinese Academy of Sciences

\({}^{2}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{3}\)ByteDance, Inc

liuhaogeng22@mails.ucas.ac.cn, huaibo.huang@cripac.ia.ac.cn

Corresponding author

###### Abstract

In the realm of Multimodal Large Language Models (MLLMs), vision-language connector plays a crucial role to link the pre-trained vision encoders with Large Language Models (LLMs). Despite its importance, the vision-language connector has been relatively less explored. In this study, we aim to propose a strong vision-language connector that enables MLLMs to achieve high accuracy while maintain low computation cost. We first reveal the existence of the visual anchors in Vision Transformer and propose a cost-effective search algorithm to extract them. Building on these findings, we introduce the Anchor Former (AcFormer), a novel vision-language connector designed to leverage the rich prior knowledge obtained from these visual anchors during pretraining, guiding the aggregation of information. Through extensive experimentation, we demonstrate that the proposed method significantly reduces computational costs by nearly two-thirds compared with baseline, while simultaneously outperforming baseline methods. This highlights the effectiveness and efficiency of AcFormer. Codes are available at https://github.com/liuhaogeng/Anchor-Former.

## 1 Introduction

Multimodal Large Language Models (MLLMs) have emerged as a focal point within contemporary research discourse. Prominently showcased by seminal works such as LLaVA [33; 34], BLIP-2 , Qwen-VL , and Flamingo , these models exhibit exceptional efficacy across a broad spectrum of tasks, spanning from nuanced image description [29; 48] to complex visual reasoning. Their versatility transcends conventional boundaries, finding practical application in quotiian scenarios such as smartphone interface design  and consequential real-world decision-making processes . This advancement is attributed to the availability of pre-trained Large Language Models (LLMs)  and vision encoders . By utilizing these pre-trained components and introducing a connecting layer between them, it is possible to construct robust MLLMs with only training the lightweight vision-language connector. This enables the development of MLLMs with the capacity to process visual inputs while retaining the linguistic prowess characteristic of LLMs. These enhanced MLLMs exhibit proficiency across various tasks, including narrative generation, code composition, and addressing complex queries .

In the construction of above MLLMs, the vision-language connector plays a pivotal role. A fundamental approach, as demonstrated in LLaVA , employs a linear projection layer as the connector. In the enhanced version, LLaVA-1.5 , the linear projection layer is expanded into a multilayerperceptron, enhancing the model's efficacy. Despite achieving notable performance, the large number of visual tokens, which extends the time required for computing attention and other processes , indicating potential for optimization to decrease the computation cost. Prior efforts have sought to address this concern. For instance, BLIP-2  introduces Q-Former and Flamingo  proposes Perceiver Resampler, both leveraging learnable queries as visual information aggregators. This mechanism utilizes the cross-attention between learnable queries and the outputs of visual encoders to effectively reduce the length of the visual sequence, thereby lowering the computation cost. Similarly, Qwen-VL  adopts a comparable structure but eliminates self-attention among the learnable queries. While these vision-language connectors substantially improve efficiency compared to naive linear projectors, they also exhibit a notable decrease in accuracy, as detailed in .

The primary computational bottleneck in MLLMs is the LLM component, where computational costs escalate significantly with the length of the input sequence, including both visual and textual tokens. To mitigate the computational cost, a straightforward approach is to reduce the number of input visual tokens with vision-language connector. A commonly adopted method is attention pooling, which offers greater flexibility than traditional pooling techniques. This method focuses on aggregating information within the visual tokens, necessitating aggregators with high information-gathering capabilities. Current attention pooling methods typically use randomly initialized learnable queries as information aggregators. We identify two main drawbacks with this approach: (1) The queries are randomly initialized without prior knowledge for aggregating visual information. They necessitates training on massive datasets (hundreds of millions) to be effective, as showed in Table 5 (2) The queries are fixed and invariant to different input images, potentially leading to significant information loss and low specificity for uncommon inputs, as illustrated in .

To address the aforementioned issue, we propose Anchor Former (AcFormer), a novel vision-language connector that enhances both the accuracy and efficiency of MLLMs compared with baselines. In order to build AcFormer, we identify more effective information aggregators by analyzing the visual features obtained from a pre-trained vision encoder from two perspectives: the feature map and the attention map. Our analysis reveals the presence of "visual anchors" within the visual tokens. Fundamentally, the transformers in the neural network aggregate information related to these visual anchors, central to the transformation process. Moreover, the positions of these anchors vary across different images. Despite this variability, we can effectively identify them using the attention matrix to carry out the cost-effective progressively search algorithm. With these observations, we propose a

Figure 1: Comparison of the **average normalized accuracy** (MMB, TextVQA, GQA). PR means Perceiver Resampler, which utilize the learnable query as information aggregator. Our method achieves **highest accuracy** comparing with the others while maintaining **high training speed**.

Anchor Selector, an important part of our AcFormer. The Anchor Selector utilize the progressive search algorithm with respect to the attention map. By this way, it effectively extracts visual anchors from the visual tokens generated by the Vision Transformer. And with the visual anchors, AcFormer utilize the naive cross attention to aggregate visual information for generating dense and complete visual representation.

In summary, our contributions can be summarized as follows:

* We reveal the existence of visual anchors within the visual tokens generated by pre-trained Vision Transformer, and subsequently propose cost-effective Anchor Selector to effectively extract these visual anchors.
* We propose the Anchor Former (AcFormer), a novel vision-language connector designed to improve the accuracy and efficiency of Multimodal Large Language Models (MLLMs) by leveraging the rich prior of information aggregation within visual anchors.
* We conduct comprehensive experiments across various vision-language tasks to empirically validate the efficacy of AcFormer.

## 2 Related Work

### Multimodal Large Language Models

The development of MLLMs has become financially viable due to the utilization of pre-trained vision encoders [41; 38; 12] and Large Language Models (LLMs) [44; 43; 52]. Spearheaded by initiatives like Flamingo  and BLIP-2 , the field has witnessed significant advancements [34; 42; 53; 3; 51; 45; 21]. Studies like LLaVA and MiniGPT-4 have introduced methodologies such as visual instruction tuning, enabling robust MLLMs capable of understanding human instructions. Additionally, efforts such as Emu and LaVIT  have proposed unified frameworks for generation and comprehension, integrating visual decoders and consolidating the training loss of visual and textual inputs. The progression of MLLMs has been supported by the availability of extensive visual-language training datasets [7; 33]. Innovations like Sphinx  and Monkey  have facilitated high-resolution image processing through techniques like sub-image cropping, thus advancing open-source MLLMs. Moreover, MobileVLM  has introduced a compact vision language model suitable for deployment on mobile devices.

### Vision-Language Connectors

Vision-language connectors typically employ either direct linear projection (LLaVA) or an information aggregation module, such as Flamingo, BLIP-2, and C-Abstractor, followed by linear projection. LLaVA, utilizing a simple multi-layer perceptron, effectively aligns visual features with the embedding space of Large Language Models (LLMs), but it suffers from high computational costs due to redundant input visual tokens, as highlighted by . BLIP-2 uses the Q-Former to aggregate visual information and establish robust baselines, while Flamingo employs the Perceiver Resampler. Both architectures leverage the cross-attention mechanism to aggregate visual information into learnable queries. However, these approaches require extensive data for training and may have limitations in tasks requiring fine-grained visual perception due to the constrained nature of the learned query's ability to capture all visual patterns. In contrast, Honeybee  proposes the C-Abstractor and D-Abstractor to address these challenges by introducing spatial priors into the feature representation. Our proposed method, AcFormer, consists of three parts: Anchor Selector, Information Aggregation Module and Linear Projection. While Flamingo and BLIP-2 use learnable queries for information aggregation, C-Abstractor applies a convolution network directly. Our method utilizes visual anchors as information aggregators, generating dense and complete visual representations for input images.

## 3 Methods

### Preliminaries

For MLLMs, their visual encoders are typically off-the-shelf pre-trained Vision Transformers (CLIP). Given input images \(^{B C H W}\) and a Vision Transformer denoted as \(F()\), the vision feature \(}\) is obtained as follows:

\[_{v}=F()^{B N D}.\] (1)

Here, \(C\), \(H\), and \(W\) represent the channel, height, and width of the input image, respectively, while \(N\) and \(D\) denote the number and dimension of the image tokens. There is one additional token, the [CLS] token added for the global representation of the image in contrastive learning. After obtaining the feature map, two common methods are used to combine it with LLM.

The first method utilizes gated cross-attention for modality fusion. Assuming the corresponding input instruction with the image is \(^{B N_{t} D_{t}}\), the computation can be expressed as:

\[_{h}=G(=,=_{v},=_{v}),\] (2)

where \(}\) represents the hidden states of the LLM and \(G()\) denotes the gated cross-attention layer.

The other approach involves converting the visual tokens into soft embeddings nd concatenating them with the text embeddings as the input of the LLM. Suppose the vision-language connector is V-L-Connector,

\[_{in}=(((_{v })),),\] (3)

Where Proj means the Linear Projection and \(\) means the text embedding. \(_{in}\) represents the input embeddings of the LLM.

### Visual Anchors

Within the ViT, the input pixel-level features undergo a series of transformations. Understanding how the visual semantic is learned will bring us better insight of building the vision-language connector. To analyse in a more intuitive way, we visualize the feature map and attention map.

Given a set of Vision Transformer's feature maps, denoted as \(^{N D}\), where \(N\) represents the number of tokens and \(D\) signifies the dimension of these tokens, we leverage dimension reduction for visual feature visualization. Initially, we extract all hidden states from the Vision Transformer. Subsequently, employing Principal Component Analysis (PCA) on each individual feature map, we derive low-dimensional features. We select the first three principal components to yield \(^{}^{N 3}\), followed by normalizing the pixel values to the range of \(\). To visually represent these features, we construct an image of the input size. Each patch within the image is then encoded using the obtained three-dimensional representation, effectively encapsulating the corresponding region's

Figure 2: Visualizations of the visual feature map and attention map pertaining to the [CLS] token. Here we select 10 layers in Vision Transformer to show their output. We present the attention maps corresponding to the [CLS] token in the final layer. Notably, special tokens within both the feature map and attention map are identified using red circles. These marked points are referred to as “visual anchors”. Details can be found in Section 3.2.

value within the image. By this way, we obtain the visualization of the features, as depicted in Figure 2 (Row 1 and Row 3). Regarding the attention map, we derive it by extracting the attention weights associated with the last layer's [CLS] token. These attention scores reflect the significance of respective tokens. We exclude the attention directed towards the [CLS] token itself, yielding the attention map \(^{H N}\), where \(H\) and \(N\) denote the number of attention heads and visual tokens. This process is depicted in Figure 2(Row 2 and Row 4).

Upon visual inspection, several noteworthy observations come to light. Specifically, the transformation of visual information exhibits a gradual obscuration of the feature map, accompanied by an increasing activation of specific tokens (depicted as pink and light yellow patches in the first and third lines, respectively). Initially, these activated tokens are just parts of the background, appearing indistinguishable from surrounding elements. However, over time, they progressively differentiate themselves from the background. Nevertheless, this evolutionary process lacks a discernible pattern. Examining the attention map, the [CLS] token, commonly used for aggregate global information of the input image, is anticipated to attend to the most salient regions. Paradoxically, the attention map of the [CLS] token predominantly focuses on a limited subset of tokens. We calculate the overlap among the activated tokens in feature map and the salient regions in attention map. The ratio reaches up to \(\%\) (500 images are sampled for calculation), conjecturing that this alignment is not coincidental. For further validation, we visualize the pre-trained MLLMs text generation attention matrix (text to visual tokens), result can be found in Figure 5.

Based on above observation, we name these tokens "visual anchors" and assume them serving as pivotal points for information aggregation during the transformation of visual features. While the [CLS] token indeed integrates visual information, its reliance alone proves insufficient, necessitating the involvement of other visual anchors in conveying information to the [CLS] token. Consequently, this process facilitates the extraction of meaningful representations from the image.

### Anchor Former

As illustrated above, the vision information is aggregated through visual anchors. Similarly, within Multimodal Large Language Models (MLLMs), structures such as Q-Former and Perceiver Resampler also leverage information aggregation modules. However, these models use learnable queries as Information Aggregator to extract information from the visual feature map produced by a pre-trained Vision Transformer. In this method, the same queries are used for all images, which can lead to two issues. First, their effectiveness requires extensive datasets for training as demonstrated in Table 5. Second, they may lead to significant information loss, as illustrated in [5; 39; 22].

Figure 3: Visualization of Anchor Former (AcFormer). We propose our token selection algorithm code in detail at Section D.

To address the aforementioned issues, we propose Anchor Former (AcFormer), which consists of Anchor Selector, Information Aggregation Module and Linear Projector. We show a rough view of Anchor Former in Figure 3. Our approach integrates insights from visual anchors with the established framework of the Perceiver Resampler. A key innovation in our method is the use of visual anchors associated with each image as Information Aggregator for aggregating visual information. The effectiveness of our approach relies on the selection of these salient points. This visual anchors allows for more precise and specific information extraction, improving both the accuracy and efficiency of the model.

Anchor Selector.To avoid additional computation, we leverage the attention map of the [CLS] token for visual anchor selection. We introduce a progressive search algorithm to construct the Anchor Selector. Assuming the attention map is \(^{H(N-1)}\), where \(H\) represents the number of attention heads and \(N-1\) denotes the number of visual tokens ([CLS] excluded). Let the token index list be \(TL\), initially containing only the index 0, corresponding to the [CLS] token, an essential anchor. Suppose we still need \(T_{N}\) tokens in addition to the [CLS] token. We select these tokens head by head, assuming each head will provide \(}{H}\) tokens. For each head, we first sort the indices of the visual tokens based on their attention scores. Then, we select the top \(}{H}\) tokens from this sorted list. If the chosen token is already in \(TL\), we choose the next token in the sorted order until we have the required number of unique tokens. We provide detailed algorithm in Figure 7.

Information Aggregation Module.In our approach, we employ selected visual anchors as Information Aggregator, combined with a cross-attention module to aggregate information. Let the Information Aggregator be denoted as \(^{(T_{N}+1) D}\) and the origin visual tokens as \(_{v}^{N D}\). Our proposed model, Information Aggregation Module, is a bidirectional transformer encoder. We denote the cross-attention module as Attn and the feedforward module as FF. Let LN represent layer normalization. For a single layer in the model, the operations are as follows:

\[_{out}=+((),(_{v}),(_{v})),\] (4)

\[=_{out}+((_{out})),\] (5)

Where \(\) means the hidden states. We use \(_{v}\) to represent the last hidden states. Let \(\) represents text embedding. We use the Proj to represent the Multilayer Perception. We obtain the final multimodal input embeddings for LLM as bellow,

\[_{in}=((_{v}),).\] (6)

## 4 Experiments

### Settings

Benchmarks.We employ nine distinct benchmarks to comprehensively assess the overall efficacy of our proposed method. The specifics of these benchmarks are delineated in Table 7. Notably, in our experimental setup, as we reduce the number of image tokens, our focus is primarily directed towards enhancing visual perception capabilities. As a result, we pay particular attention to benchmarks such as TextVQA, which challenged the model's fine-grained visual perception ability [21; 31].

Implementation details.In our experimental setup, we utilize 7B and 13B Vicuna-v1.5 as Large Language Models (LLMs) . The CLIP ViT-L/14 model, pre-trained with a resolution of 336, serves as our vision encoder. We select the last but one layer's output from the vision encoder as our vision feature. For Anchor Former, we configure it with 6 layers and a hidden dimension of 512, employing 8 attention heads, each with a dimension of 64. The feedforward module utilize 2048 as the hidden dimension. Regarding the training dataset, we leverage the dataset utilized in LLaVA-1.5 , with 558k samples for pre-training and 665k samples for instruction tuning. Our experimentation encompasses the evaluation of various vision-language connectors, including the Perceiver Resampler, Anchor Former, pooling, C-Abstractor and utilizing pooled tokens as queries for the Perceiver Resampler. To maintain consistency, we construct our model using the official code provided by LLaVA. Specifically, we adjust the pre-training initial learning rate from \(1e^{-3}\) to \(5e^{-4}\). Other configuration is the same with origin LLaVA-1.5.

### Main Results

We present the main results in Tables 1 and 2, organized by benchmark type. Upon observation of these tables, it becomes apparent that our model achieves robust performance despite being trained with limited data and visual tokens. Notably, even with only 145 or 257 tokens, our model achieves performance comparable to that of the original LLaVA-1.5 model, which utilizes 577 visual tokens as input. This performance holds across various benchmarks, including those that require high-level visual perception (e.g., VisWiz, TextVQA) and those that assess overall capability (e.g., MME, GQA).

However, it should be noted that although our model is overall effective, it performs slightly worse than LLaVA-1.5 on certain benchmarks such as GQA and VQAv2. Given that our method only applies significantly fewer visual tokens (less than half), the slightly performance drop meets expectations. Nevertheless, the performance gap is relatively small and can be considered marginal in light of the substantial increase in speed.

### Ablation Results

We mainly compare different visual Connectors. To facilitate understanding, we provide definitions for some terms. Pooling denotes direct pooling of visual token. Pooling-PR employs the pooled

  Model & LLM & Connector & V-T Num & Res & POPE & MME & MMB & MM-Vet & Speed (\(\)) \\   \\  MiniGPT-4 [(53)] & Vicuna-7B & Resampler & 32 & 224 & - & 49.2 & - & 34.5 & 60.5 & - \\  \\ DIEPCIS-9B [(25)] & LLMaM-7B & Cross Attn & 257 & 224 & - & - & 77.4 & - & - & - \\  \\ Quen-VL[(4)] & Open-7B & Resampler & 256 & 448 & - & 59.3 & **78.8** & 35.2 & 67.1 & - \\  \\ Daven-VL-Chat[(4)] & Quen-7B & Resampler & 256 & 448 & - & 57.5 & 78.2 & 38.9 & 68.2 & - \\  \\ LLaVA-1.5 [(33)] & Vicuna-7B & Linear & 577 & 336 & 58.2 & **62.0** & 78.5 & 50.0 & 66.8 & \(1.00\) \\  \\ Ours & Vicuna-7B & AcFormer & 257 & 336 & **58.2** & 61.2 & 78.4 & **52.8** & **69.4** & \(\) \\  \\ Approaches using 13B Large Language Models} \\   \\ InstrcRLIP[(11)] &  & Q-Former & 32 & 224 & - & 49.5 & - & 33.4 & 63.1 & - \\  \\ BLIP-2[(28)] & Vicuna-13B & Q-Former & 32 & 224 & - & 41.0 & 41.0 & 19.5 & 61.0 & - \\  \\ LLAVA-1.5 [(33)] & Vicuna-13B & Linear & 577 & 336 & 61.2 & **63.3** & **80.0** & 53.6 & 71.6 & \(1.00\) \\  \\ Ours & Vicuna-13B & AcFormer & 257 & 336 & **61.3** & 63.0 & 79.8 & **53.7** & **71.8** & \(\) \\  

Table 2: Results on General VQA tasks. V-T Num means the visual tokens number. V-T Num influences the computation cost that the bigger the V-T Num the heavier the computation cost is. Speed here means the relative pre-training speed with respect to LLaVA-1.5.

  Model & LLM & Connector & V-T Num & Res & POPE & MME & MMB & MM-Vet & Speed (\(\)) \\   \\  InstrcRLIP[(11)] & LLMaM2-7B & Q-Former & 32 & 224 & - & 49.2 & - & 34.5 & 60.5 & - \\  \\ Shikra [(6)] & Vicuna-7B & Linear & 257 & 224 & - & - & 77.4 & - & - & - \\ IDETCIS-9B [(25)] & LLMaM-7B & Cross Attn & 257 & 224 & - & 38.4 & 50.9 & 35.5 & - & - \\ Quen-VL[(4)] & Quen-7B & Resampler & 256 & 448 & - & 59.3 & **78.8** & 35.2 & 67.1 & - \\  \\ Open-VL-Chat[(4)] & Quen-7B & Resampler & 256 & 448 & - & 57.5 & 78.2 & 38.9 & 68.2 & - \\  \\ LLaVA-1.5 [(33)] & Vicuna-7B & Linear & 577 & 336 & 58.2 & **62.0** & 78.5 & 50.0 & 66.8 & \(1.00\) \\  \\ Ours & Vicuna-7B & AcFormer & 257 & 336 & **58.2** & 61.2 & 78.4 & **52.8** & **69.4** & \(\) \\  \\ Approaches using 13B Large Language Models} \\  InstrcRLIP[(11)] &  & Q-Former & 32 & 224 & - & 49.5 & - & 33.4 & 63.1 & - \\  \\ BLIP-2[(28)] & Vicuna-13B & Q-Former & 32 & 224 & - & 41.0 & 41.0 & 19.5 & 61.0 & - \\  \\ LLAVA-1.5 [(33)] & Vicuna-13B & Linear & 577 & 336 & 61.2 & **63.3** & **80.0** & 53.6 & 71.6 & \(1.00\) \\  \\ Ours & Vicuna-13B & AcFormer & 257 & 336 & **61.3** & 63.0 & 79.8 & **53.7** & **71.8** & \(\) \\  \\ 

Table 1: Results on benchmark designed for MLLMs. V-T Num means the visual tokens number. V-T Num influences the computation cost that the bigger the V-T Num the heavier the computation cost is. Speed here means the relative pre-training speed with respect to LLaVA-1.5.

tokens as queries for the Perceiver Resampler. Random-PR means the Perceiver Resampler using randomly selected tokens from the vision feature map as query. PR refers to the commonly used Perceiver Resampler. Our experimental findings validate the efficacy of our model from multiple perspectives. A comparison between PR and AcFormer indicates that the observed enhancement does not stem solely from an increase in **trainable parameters**. Additionally, comparison between AcFormer and Random-PR underscores the critical role of **Anchor Selector** in model performance. Furthermore, our evaluation of AcFormer against C-Abstractor reveals that the significance of inductive bias diminishes, as spatial coherence can be effectively maintained through cross-attention mechanisms within the Perceiver Resampler.

C-Abstractor.We trained the model with training data from LLaVA-1.5. Our findings, as presented in Table 3, indicate that the C-Abstractor method achieves comparable performance to AcFormer across most benchmarks. However, in tasks such as TextVQA, which necessitates high-level visual perception (often demanding fine-grained visual analysis), C-Abstractor exhibits inferior performance. This empirical evidence underscores the efficacy of our proposed AcFormer.

Pooling.One immediate consideration is to aggregate visual tokens based on their spatial positions by pooling and combine them with the [CLS] token to form the input visual features. Our empirical investigation reveals that direct pooling emerges as a potent technique for compressing visual information. However, it fails in TextVQA, leading to around 3 points drop.

Pooling-PR.Given our direct utilization of pooled tokens within Large Language Models, a concern arises regarding whether the observed degradation stems from the reduction in trainable parameters within the Anchor Former. To address this concern, we conducted additional experiments wherein the pooled tokens served as queries for the Perceiver Resampler, termed as Pooling-PR. Examination

   Model & LLM & Connector & V-T Num. & TextVQA & GQA & MMB & MME \\    } & Vicuna-7B & Top-P & 145 & 56.3 & 60.8 & 68.2 & 1798.8 \\  & Vicuna-7B & E-ViT & 146 & 57.1 & 61.0 & 68.3 & 1808.4 \\  & Vicuna-7B & AcFormer & 145 & **58.0** & **61.3** & **68.4** & **1846.1** \\   

Table 4: Ablation studies on whether to directly use the selected tokens as input.

   Model & LLM & Connector & V-T Num. & TextVQA & GQA & MMB & MME \\    } & Vicuna-7B & Pooling & 65 & 53.4 & 59.8 & 66.8 & 1734.0 \\  & Vicuna-7B & Pooling-PR & 65 & 53.9 & **60.0** & 66.8 & 1728.9 \\  & Vicuna-7B & Random-PR & 65 & 53.9 & 59.1 & 66.9 & 1728.7 \\  & Vicuna-7B & PR & 65 & 51.0 & 56.1 & 63.2 & 1702.8 \\  & Vicuna-7B & C-Abstractor & 65 & 52.8 & 59.0 & 67.0 & 1743.3 \\  & Vicuna-7B & AcFormer & 65 & **56.1** & 59.2 & **67.3** & **1744.2** \\    } & Vicuna-7B & Pooling & 145 & 55.1 & 60.9 & 68.0 & 1791.4 \\  & Vicuna-7B & Pooling-PR & 145 & 54.7 & 60.9 & 68.0 & 1759.1 \\  & Vicuna-7B & Random-PR & 145 & 54.6 & 59.7 & 67.0 & 1772.7 \\  & Vicuna-7B & PR & 145 & 52.1 & 56.4 & 65.4 & 1720.8 \\  & Vicuna-7B & C-Abstractor & 145 & 53.4 & 60.2 & 67.8 & 1775.4 \\  & Vicuna-7B & AcFormer & 145 & **58.0** & **61.3** & **68.4** & **1846.1** \\    } & Vicuna-7B & PR & 257 & 52.3 & 56.8 & 65.7 & 1735.9 \\  & Vicuna-7B & C-Abstractor & 257 & 53.7 & 60.8 & 68.3 & 1790.0 \\  & Vicuna-7B & AcFormer & 257 & **58.2** & **61.2** & **68.3** & **1848.8** \\    } & Vicuna-13B & PR & 145 & 53.4 & 56.9 & 64.7 & 1749.3 \\  & Vicuna-13B & C-Abstractor & 145 & 58.5 & 62.1 & 68.8 & 1823.6 \\  & Vicuna-13B & AcFormer & 145 & **60.7** & **62.8** & **69.2** & **1869.3** \\   

Table 3: Ablation studies. “Pooling” denotes direct pooling of visual token. “Pooling-PR” employs the pooled tokens as queries for the Perceiver Resampler. “Random-PR” means the Perceiver Resampler using randomly selected tokens from the vision feature map as query. “PR” refers to the Perceiver Resampler using learnable queries. “AcFormer” represents our proposed Anchor Former. The configuration of the C-Abstractor follows Honeybee . V-T Num means the visual tokens number.

of the results in the table reveals that this approach yielded even poorer performance compared to directly inputting the pooled tokens.

Random-Pr.While our method demonstrates superior performance compared with other approaches, a lingering question pertains to whether this improvement indeed stems from the Anchor Former. To address the concern, we randomly select tokens from the vision feature map as the Information Aggregator. The empirical results presented in Table 3 corroborate the efficacy of our proposed anchor selection method, substantiating its discernible impact on model performance.

Pr.While substantiated the effectiveness of AcFormer, an unresolved question pertains to its superiority over the commonly used Perceiver Resampler, which employs randomly initiallized learnable queries for visual information aggregation. Examination of the table reveals that the Perceiver Resampler exhibits the poorest performance among the aforementioned methods. This observation underscores the diminished performance of the structure under conditions of limited training data.

Anchor Direct-in.A related study  employ Top-K selection methodology for token compression. This approach shares similar token selection method with ours and involves the direct utilization of selected tokens as representations (without further cross attention) for visual information. However, it primarily caters to classification tasks, which prioritize global understanding, potentially rendering it less suitable for tasks requiring nuanced comprehension. Noteworthy is the proposition by the authors of Haurum  regarding EViT. They employ the selected tokens and incorporate pooling mechanisms for the remaining tokens as input. The findings, as detailed in Table 4, empirically underscore the significance of retaining unselected tokens, as they still encapsulate valuable information.

### Scaling Up the Training Dataset

We conducted a data scaling experiment to further evaluating our proposed method's performance on large-scale data scene. Considering the expensive training cost, we replaced the Large Language Model with OpenLLaMA-3B . Approximately 60 million image-text pairs were employed for pre-training, sampled from Laino115M, coyo-238M, and laion-coco100M datasets. For instruction fine-tuning, we utilized two datasets, covering LLaVA-665k and Cauldron . Notably, Cauldron (1.8M) is a larger instruction finetuning dataset than LLaVA-665k (0.66M) for MLLMs.

Our focus was on comparing Perceiver Resampler with our proposed AcFormer. The results, summarized in Table 5, consistently demonstrate the superiority of AcFormer over the Perceiver Resampler. This illustrate that our method not only works well for the scene of limited data but also for larger data (both pre-train dataset and instruction tuning dataset).

## 5 Conclusions

In this study, we reveal the existence of the visual anchors and present the hypothesis that visual anchors are strong visual information aggregators. With the observation, we propose the Anchor Former, an effective vision-language connector. Notably, Anchor Former distinguishes itself from the conventional information aggregation methods (e.g., Q-Former and Perceiver Resampler) by adopting

   Model & LLM & Connector & V-T Num. & TextVQA & GQA & OKVQA & VQAv\({}_{2}\) & VizWiz & MME \\   \\
**Instruction Finetuning Dataset:LLAVA-665k** & & & & & & & \\   LLaVA-1.5 \\  } & OpenLLaMA-3B & PR & 145 & 35.03 & 54.35 & 54.14 & 70.04 & 31.16 & 1592.7 \\  & OpenLLaMA 3B & AcFormer & 145 & **35.89** & **55.45** & **55.15** & **72.76** & **33.88** & **1622.3** \\   \\
**Instruction Finetuning Dataset:Cauldron (roughly 1.8M)** & & & & & & & \\    LLaVA-1.5 \\  } & OpenLLaMA-3B & PR & 145 & 38.76 & 44.98 & 49.66 & 70.83 & 32.87 & 1502.5 \\  & OpenLLaMA 3B & AcFormer & 145 & **40.49** & **45.67** & **50.57** & **73.01** & **32.97** & **1523.1** \\   

Table 5: Ablation studies on the visual connector when scaling up the training data.

visual anchors as Information Aggregator. To build Anchor Former, we present the progressive search algorithm to effectively extract the visual anchors. Extensive experiments on different benchmarks consistently underscores the efficacy of Anchor Former in improving the models' accuracy while simultaneously removing the redundant visual tokens in MLLMs.