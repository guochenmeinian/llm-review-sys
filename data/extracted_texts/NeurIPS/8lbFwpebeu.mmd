# Investigating how ReLU-networks encode symmetries

Georg Bokman  Fredrik Kahl

Chalmers University of Technology

{bokman, fredrik.kahl}@chalmers.se

###### Abstract

Many data symmetries can be described in terms of group equivariance and the most common way of encoding group equivariances in neural networks is by building linear layers that are group equivariant. In this work we investigate whether equivariance of a network implies that all layers are equivariant. On the theoretical side we find cases where equivariance implies layerwise equivariance, but also demonstrate that this is not the case generally. Nevertheless, we conjecture that CNNs that are trained to be equivariant will exhibit layerwise equivariance and explain how this conjecture is a weaker version of the recent permutation conjecture by Enetezari et al. (2022). We perform quantitative experiments with VGG-nets on CIFAR10 and qualitative experiments with ResNets on ImageNet to illustrate and support our theoretical findings. These experiments are not only of interest for understanding how group equivariance is encoded in ReLU-networks, but they also give a new perspective on Enetezari et al.'s permutation conjecture as we find that it is typically easier to merge a network with a group-transformed version of itself than merging two different networks.

## 1 Introduction

Understanding the inner workings of deep neural networks is a key problem in machine learning and it has been investigated from many different perspectives, ranging from studying the loss landscape and its connection to generalization properties to understanding the learned representations of the networks. Such an understanding may lead to improved optimization techniques, better inductive biases of the network architecture and to more explainable and predictable results. In this paper, we focus on ReLU-networks--networks with activation function \((x):=(0,x)\)--and how they encode and learn data symmetries via equivariance.

Equivariances can be built into a neural network by design. The most classical example is of course the CNN where translational symmetry is obtained via convolutional layers. Stacking such equivariant layers in combination with pointwise ReLU activations results in an equivariant network. We will pursue a different research path and instead start with a neural network which has been trained to be equivariant and ask how the equivariance is encoded in the network. This approach in itself is not new and has been, for instance, experimentally explored in  where computational methods for quantifying layerwise equivariance are developed. We will shed new light on the problem by deriving new theoretical results when network equivariance implies layerwise equivariance. We will also give counterexamples for when this is not the case. Another insight is obtained via the recent conjecture by Enetezari et al. (2013) which states that networks with the same architecture trained on the same data are often close to each other in weight space modulo permutation symmetries of the network weights. Our new conjecture 3.2 which we derive from the conjecture of Enetezari et al. states that most SGD CNN solutions will be close to group equivariant CNNs (GCNNs).

Our theoretical results and new conjecture are also validated with experiments. We focus exclusively on the horizontal flipping symmetry in image classification, for the following three reasons: 1. Horizontal flipping is ubiquitously assumed to not change the class of an image and horizontal flipping data augmentation is in practice always used. 2. The group of horizontal flips (\(S_{2}\)) is small and has simple representation theory. 3. As we study ReLU as activation function, if a network is layerwise equivariant, then the representations acting on the feature spaces must be permutation representations as we shall prove later (cf. Section 2.1).

The experiments show that networks trained on CIFAR10 and ImageNet with horizontal flipping data augmentation are close to GCNNs and in particular when we train networks with an invariance loss, they become very close to GCNNs. This is illustrated in Figure 1 and also discussed in Section 4. As further support of our main conjecture, we find that the interpolation barrier is lower for merging a ResNet50 with a flipped version of itself than the barrier for merging two separately trained ResNet50's. In summary, our main contribution is a new conjecture on how ReLU-networks encode symmetries which is supported by both theory and experiments.

### Related work

**Group equivariant neural networks.** The most common approach to encoding group symmetries into neural networks is by designing the network so that each layer is group equivariant [41; 14]. This has been done for many groups, e.g., the permutation group [43; 27] and the 2D or 3D rotation group [39; 34; 16; 4; 37]. Another possibility is to use symmetry regularization during training . Group

Figure 1: Illustration of how a VGG11 encodes the horizontal flipping symmetry. The 64 filters in the first convolutional layer of two VGG11-nets trained on CIFAR10 are shown, where each filter is next to a filter in the same net which after horizontally flipping the filter results in a similar convolution output. The order of the filters in the right columns is a permutation of the original order in the left columns. This permutation is obtained with the method in Figure 2, i.e., the columns here correspond to **1.** and **4.** there. **Net A** is trained with an invariance loss to output the same logits for horizontally flipped images. It has learnt very close to a GCNN structure where each filter in the first layer is either horizontally symmetric or has a mirrored twin. **Net B** is trained with horizontal flipping data augmentation. This net is quite close to a GCNN structure, but the mirrored filters are less close to each other than in Net A. More details are given in Section 4.

Figure 2: Permutation aligning horizontally flipped filters.

**1.** The three original filters in a convolutional layer.

**2.** The three filters flipped horizontally.

**3.** Permutation of the flipped filters to align them with the originals. This permutation is found using activation matching following .

**4.** Flipping the filters back to their original form for illustration.

equivariant nets have many possible applications, for instance estimating molecular properties  or image feature matching . For this paper, the most relevant group equivariant nets are so-called GCNNs acting on images, which are equivariant to Euclidean transformations [10; 42; 2; 40; 38]. In the experiments in Section 4 we will investigate how close to GCNNs ordinary CNNs are when they are trained to be equivariant--either by using an invariance loss or by using data augmentation.

**Measuring layerwise equivariance.** measure layerwise equivariance by fitting linear group representations in intermediate feature spaces of networks. Our approach is similar but we restrict ourselves to permutation representations and consider fitting group representations in all layers simultaneously rather than looking at a single layer at a time. We explain in Section 2.1 why it is enough to search for permutation representations, given that we have a network with ReLU activation functions.  measure layerwise equivariance by counting how many filters in each layer have group-transformed copies of themselves in the same layer. They find that often more layerwise equivariance means better performance. Our approach explicitly looks for permutations of the filters in each layer that align the filters with group-transformed versions of themselves. We are thus able to capture how close the whole net is to being a GCNN.  find evidence for layerwise equivariance using a qualitative approach of visualizing filters in each layer and finding which look like group-transformed versions of each other.  measure local layerwise equivariance, i.e., layerwise robustness to small group transformations, by computing derivatives of the output of a network w.r.t. group transformations of the input. They interestingly find that transformers can be more translation equivariant than CNNs, due to aliasing effects in CNN downsampling layers.

**Networks modulo permutation symmetries.** demonstrated that two CNNs with the same architecture trained on the same data often learn similar features. They did this by permuting the filters in one network to align with the filters in another network.  conjectured that it should be possible to permute the weights of one network to put it in the same loss-basin as the other network and that the networks after permutation should be linearly mode connected. I.e., it should be possible to average ("merge") the weights of the two networks to obtain a new network with close to the same performance. This conjecture has recently gained empirical support in particular through , where several good methods for finding permutations were proposed and  where the performance of a merged network was improved by resetting batch norm statistics and batch statistics of individual neurons to alleviate what the authors call variance collapse. In Section 3 we give a new version of 's conjecture by conjecturing that CNNs trained on group invariant data should be close to GCNNs.

### Limitations

While we are able to show several relevant theoretical results, we have not been able to give a conclusive answer in terms of necessary and sufficient conditions to when equivariance of a network implies layerwise equivariance or that the network can be rewritten to be layerwise equivariant. An answer to this question would be valuable from a theoretical point of view.

In the experiments, we limit ourselves to looking at a single symmetry of images - horizontal flipping. As in most prior work on finding weight space symmetries in trained nets, we only search for permutations and not scaled permutations which would also be compatible with the ReLU-nonlinearity. Our experimental results depend on a method of finding permutations between networks that is not perfect . Future improvements to permutation finding methods may increase the level of certainty which we can have about Conjectures 3.1 and 3.2.

## 2 Layerwise equivariance

We will assume that the reader has some familiarity with group theory and here only briefly review a couple of important concepts. Given a group \(G\), a representation of \(G\) is a group homomorphism \(:G(V)\) from \(G\) to the general linear group of some vector space \(V\). E.g., if \(V=^{m}\), then \((V)\) consists of all invertible \(m m\)-matrices and \(\) assigns a matrix to each group element, so that the group multiplication of \(G\) is encoded as matrix multiplication in \((G)(V)\).

A function \(f:V_{0} V_{1}\) is called _equivariant_ with respect to a group \(G\) with representations \(_{0}\) on \(V_{0}\) and \(_{1}\) on \(V_{1}\) if \(f(_{0}(g)x)=_{1}(g)f(x)\) for all \(g G\) and \(x V_{0}\). An important representation that exists for any group on any vector space is the trivial representation where \((g)=I\) for all \(g\). Ifa function \(f\) is equivariant w.r.t. \(_{0}\) and \(_{1}\) as above, and \(_{1}\) is the trivial representation, we call \(f\)_invariant_. A _permutation representation_ is a representation \(\) for which \((g)\) is a permutation matrix for all \(g G\). We will also have use for the concept of a group invariant data distribution. We say that a distribution \(\) on a vector space \(V\) is \(G\)-invariant w.r.t. a representation \(\) on \(V\), if whenever \(X\) is a random variable distributed according to \(\), then \((g)X\) is also distributed according to \(\).

Let's for now1 consider a neural network \(f:^{m_{0}}^{m_{L}}\) as a composition of linear layers \(W_{j}:^{m_{j-1}}^{m_{j}}\) and activation functions \(\):

\[f(x)=W_{L}(W_{L-1}( W_{2}(W_{1}x))).\] (1)

Assume that \(f\) is equivariant with respect to a group \(G\) with representations \(_{0}:G(^{m_{0}})\) on the input and \(_{L}:G(^{m_{L}})\) on the output, i.e.,

\[f(_{0}(g)x)=_{L}(g)f(x),^{m_{0}},g  G$}.\] (2)

A natural question to ask is what the equivariance of \(f\) means for the layers it is composed of. Do they all have to be equivariant? For a layer \(W_{j}\) to be \(G\)-equivariant we require that there is a representation \(_{j-1}\) on the input and a representation \(_{j}\) on the output of \(W_{j}\) such that \(_{j}(g)W_{j}=W_{j}_{j-1}(g)\) for all \(g G\). Note again that the only representations that are specified for \(f\) to be equivariant are \(_{0}\) and \(_{L}\), so that all the other \(_{j}:G(^{m_{j}})\) can be arbitrarily chosen. If there exists a choice of \(_{j}\)'s that makes each layer in \(f\) equivariant (including the nonlinearities \(\)), we call \(f\)_layerwise equivariant_. In general we could have that different representations act on the input and output of the nonlinearities \(\), but as we explain in Section 2.1, this cannot be the case for the ReLU-nonlinearity, which will be our main focus. Hence we assume that the the same group representation acts on the input and output of \(\) as above.

The following simple example shows that equivariance of \(f\) does not imply layerwise equivariance.

_Example 2.1_.: Let \(G=S_{2}=\{i,h\}\) be the permutation group on two indices, where \(i\) is the identity permutation and \(h\) the transposition of two indices. Consider a two-layer network \(f:^{2}\),

\[f(x)=W_{2}}(W_{1}x),\]

where \(W_{1}=(1 0)\) and \(W_{2}=0\). \(f\) is invariant to permutation of the two coordinates of \(x\) (indeed, \(f\) is constant \(0\)). Thus, if we select \(_{0}(h)=(0&1\\ 1&0)\) and \(_{2}(h)=1\), then \(f\) is equivariant (note that a representation of \(S_{2}\) is specified by giving an involutory \((h)\) as we always have \((i)=I\)). However, there is no choice of a representation \(_{1}\) that makes \(W_{1}\) equivariant since that would require \(_{1}(h)W_{1}=W_{1}_{0}(h)=(0 1)\), which is impossible (note that \(_{1}(h)\) is a scalar). The reader will however notice that we can define a network \(\), with \(_{1}=(0 0)\), \(_{2}=0\) for which we have \(f(x)=(x)\) and then \(\) is layerwise equivariant when choosing \(_{1}(h)=1\).

In the example just given it was easy to, given an equivariant \(f\), find an equivalent net \(\) which is layerwise equivariant. For very small 2-layer networks we can prove that this will always be the case, see Proposition D.1.

Example 2.1 might seem somewhat unnatural, but we note that the existence of "dead neurons" (also known as "dying ReLUs") with constant zero output is well known . Hence, such degeneracies could come into play and make the search for linear representations in trained nets more difficult. We will however ignore this complication in the experiments in the present work.

From an intuitive point of view, equivariance of a neural network should mean that some sort of group action is present on the intermediate feature spaces as the network should not be able to "forget" about the equivariance in the middle of the net only to recover it at the end. In order to make this intuition more precise we switch to a more abstract formulation in Appendix D.1. The main takeaway will be that it is indeed possible to define group actions on modified forms of the intermediate feature spaces whenever the network is equivariant, but this will not be very practically useful as it changes the feature spaces from vector spaces to arbitrary sets, making it impossible to define linear layers to/from these feature spaces.

An interesting question that was posed by Elesedy and Zaidi , is whether non-layerwise equivariant nets can ever perform better at equivariant tasks than layerwise equivariant nets. In Appendix C, we give a positive answer by demonstrating that when the size of the net is low, equivariance can hurt performance. In particular we give the example of _cooccurence of equivariant features_ in C.1. This is a scenario in image classification, where important features always occur in multiple orientations in every image. It is intuitive that in such a case, it suffices for the network to recognize a feature in a single orientation for it to be invariant _on the given data_, but a network recognizing features in only one orientation will not be layerwise equivariant.

### From general representations to permutation representations

We will now first sidestep the issue of an equivariant network perhaps not being layerwise equivariant, by simply assuming that the network is layerwise equivariant with a group representation acting on every feature space (this will to some degree be empirically justified in Section 4). Then we will present results on 2-layer networks, where we can in fact show that layerwise equivariance is implied by equivariance.

The choice of activation function determines which representations are at all possible. In this section we lay out the details for the ReLU-nonlinearity. We will have use for the following lemma, which is essentially the well known property of ReLU being "positive homogeneous".

**Lemma 2.2**.: _[_Godfrey et al._ (_18_, Lemma 3.1, Table 1)]_ _Let \(A\) and \(B\) be invertible matrices such that \((Ax)=B\,(x)\) for all \(x\). Then \(A=B=PD\) where \(P\) is a permutation matrix and \(D\) is a diagonal matrix with positive entries on the diagonal._

We provide an elementary proof in Appendix B.1. It immediately follows from Lemma 2.2 that if \(\) is \(G\)-equivariant with respect to representations \(_{0}\) and \(_{1}\) on the input and output respectively, then \(_{0}(g)=_{1}(g)=P(g)D(g)\) for all \(g G\). I.e., the representations acting on the intermediate feature spaces in a layerwise equivariant ReLU-network are scaled permutation representations. This holds also if we add bias terms to the layers in (1).

We note that Godfrey et al.  consider more nonlinearities than ReLU, and that their results on other nonlinearities could similarly be used to infer what input and output group representations are admissible for these other nonlinearities. Also, Wood and Shawe-Taylor  derive in large generality what nonlinearities commute with which finite group representations, which is very related to our discussion. However, to apply the results from  we would have to first prove that the representations acting on the inputs and outputs of the nonlinearity are the same.

For two-layer networks with invertible weight matrices we can show that equivariance implies layerwise equivariance with a scaled permutation representation acting on the feature space on which ReLU is applied. The reader should note that the invertibility assumption is strong and rules out cases such as Example 2.1.

**Proposition 2.3**.: _Consider the case of a two-layer network \(f:^{m}^{m}\),_

\[f(x)=W_{2}\,(W_{1}x),\]

_where \(\) is applied point-wise. Assume that the matrices \(W_{1}^{m m}\), \(W_{2}^{m m}\) are non-singular. Then \(f\) is \(G\)-equivariant with \(_{j}:G(^{m})\) for \(j=0,2\) on the input and the output respectively if and only if_

\[_{0}(g)=W_{1}^{-1}P(g)D(g)W_{1}_{2}(g)=W_{2}P(g)D (g)W_{2}^{-1},\]

_where \(P(g)\) is a permutation and \(D(g)\) a diagonal matrix with positive entries. Furthermore, the network \(f\) is layerwise equivariant with \(_{1}(g)=P(g)D(g)\)._

Proof.: Invertibility of \(W_{1}\) and \(W_{2}\) means that \(f\) being equivariant w.r.t. \(_{0}\), \(_{2}\) is equivalent to ReLU being equivariant w.r.t. \(_{1}(g)=W_{1}_{0}(g)W_{1}^{-1}\) and \(_{1}(g)=W_{2}^{-1}_{2}(g)W_{2}\). The discussion after Lemma 2.2 now shows that \(_{1}(g)=_{1}(g)=P(g)D(g)\) and the proposition follows. 

The set of group representations for which a two-layer ReLU-network can be \(G\)-equivariant is hence quite restricted. It is only representations that are similar (or conjugate) to scaled permutations that are feasible. In the appendix, we also discuss the case of two-layer networks that are \(G\)-invariant and show that they have to be layerwise equivariant with permutation representations in Proposition B.1.

### Permutation representations in CNNs--group convolutional neural networks

As explained in Section 2.1, for a ReLU-network to be layerwise equivariant, the representations must be scaled permutation representations. We will now review how such representations can be used to encode the horizontal flipping symmetry in CNNs. This is a special case of group equivariant convolutional networks--GCNNs--which were introduced by . An extensive reference is . The reader familiar with  can skip this section, here we will try to lay out in a condensed manner what the theory of horizontal flipping equivariant GCNNs looks like.

Note first that horizontally flipping an image corresponds to a certain permutation of the pixels. Let's denote the action of horizontally flipping by \(\). This is a permutation representation of the abstract group \(S_{2}\). If a convolutional layer \(\) only contains horizontally symmetric filters, then it is immediate that \(((x))=((x))\). It is however possible to construct more general equivariant layers.

Let's assume that the representations acting on the input and output of \(\) split into the spatial permutation \(\) and a permutation \(P\) of the channels. \(P_{0}\) on the input and \(P_{1}\) on the output. \(P_{0}\) and \(P_{1}\) need to have order maximum 2 to define representations of \(S_{2}\). One can show, using the so-called kernel constraint , what form the convolution kernel \(\) of \(\) needs to have to be equivariant. The kernel constraint says that the convolution kernel \(^{c_{1} c_{0} k k}\) needs to satisfy

\[()=P_{1} P_{0}^{T}(=P_{1} P_{0}),\] (3)

where \(\) acts on the spatial \(k k\) part of \(\), \(P_{1}\) on the \(c_{1}\) output channels and \(P_{0}\) on the \(c_{0}\) input channels. The last equality holds since \(P_{0}\) is of order 2. In short, permuting the channels of \(\) with \(P_{0}\) and \(P_{1}\) should be the same as horizontally flipping all filters. We see this in action in Figure 1, even for layers that are not explicitly constrained to satisfy (3). An intuitive explanation of why (3) should hold for equivariant layers is that it guarantees that the same information will be captured by \(\) on input \(x\) and flipped input \((x)\), since all horizontally flipped filters in \(()\) do exist in \(\).

Channels that are fixed under a channel permutation are called invariant and channels that are permuted are called regular, as they are part of the regular representation of \(S_{2}\). We point out three special cases--if \(P_{0}=P_{1}=I\), then \(\) has to be horizontally symmetric. If \(P_{0}=I\) and \(P_{1}\) has no diagonal entries, then we get a lifting convolution and if \(P_{0}\) and \(P_{1}\) both have no diagonal entries then we get a regular group convolution . In the following when referring to a _regular_ GCNN, we mean the "most generally equivariant" case where \(P\) has no diagonal entries.

## 3 The permutation conjecture by Entezari et al. and its connection to GCNNs

Neural networks contain permutation symmetries in their weights, meaning that given, e.g., a neural network of the form (1), with pointwise applied nonlinearity \(\), we can arbitrarily permute the inputs and outputs of each layer

\[W_{1} P_{1}W_{1}, W_{j} P_{j}W_{j}P_{j-1}^{T}, W_{L}  W_{L}P_{L-1}^{T},\]

and obtain a functionally equivalent net. The reader should note the similarity to the kernel constraint (3). It was conjectured by  that given two nets of the same type trained on the same data, it should be possible to permute the weights of one net to put it in the same loss-basin as the other net. Recently, this conjecture has gained quite strong empirical support .

When two nets are in the same loss-basin, they exhibit close to linear mode connectivity, meaning that the loss/accuracy barrier on the linear interpolation between the weights of the two nets will be close to zero. Let the weights of two nets be given by \(_{1}\) and \(_{2}\) respectively. In this paper we define the barrier of a performance metric \(\) on the linear interpolation between the two nets by

\[((_{1})+(_{2}))-( (_{1}+_{2}))}{((_{1})+ (_{2}))}.\] (4)

Here \(\) will most commonly be the test accuracy. Previous works  have defined the barrier in slightly different ways. In particular they have not included the denominator which makes it difficult to compare scores for models with varying performance. We will refer to (4) without the denominator as the absolute barrier. Furthermore we evaluate the barrier only at a single interpolation point--halfway between \(_{1}\) and \(_{2}\)--as compared to earlier work taking the maximum of barrier values when interpolating between \(_{1}\) and \(_{2}\). We justify this by the fact that the largest barrier valueis practically almost always halfway between \(_{1}\) and \(_{2}\) (in fact  also only evaluate the halfway point in their experiments). The permutation conjecture can now be informally stated as follows.

**Conjecture 3.1** (Entezari et al. [13, Sec. 3.2]).: _Most SGD solutions belong to a set \(\) whose elements can be permuted in such a way that there is no barrier on the linear interpolation between any two permuted elements in \(\)._

Importantly, we note that when applied to CNNs, the conjecture should be interpreted in the sense that we only permute the channel dimensions of the convolution weights, not the spatial dimensions. We will now explain why applying Conjecture 3.1 to CNNs that are trained on invariant data distributions leads to a conjecture stating that most SGD solutions are close to being GCNNs. For simplicity, we discuss the case where \(G=\) horizontal flips of the input image, but the argument works equally well for vertical flips or \(90\) degree rotations. The argument is summarised in Figure 3.

We will consider an image classification task and the common scenario where images do not change class when they are horizontally flipped, and where a flipped version of an image is equally likely as the original (in practice this is very frequently enforced using data augmentation). Let \(\) be the horizontal flipping function on images. We can also apply \(\) to feature maps in the CNN, whereby we mean flipping the spatial horizontal dimension of the feature maps.

Initialize one CNN \(\) and copy the initialization to a second CNN \(\), but flip all filters of \(\) horizontally. If we let \(x_{0},x_{1},x_{2},\) be the samples drawn during SGD training of \(\), then an equally likely drawing of samples for SGD training of \(\) is \((x_{0}),(x_{1}),(x_{2}),\). After training \(\) using SGD and \(\) using the horizontally flipped version of the same SGD path, \(\) will still be a copy of \(\) where all filters are flipped horizontally.2 This means according to Conjecture 3.1 that \(\) can likely be converted close to \(\) by only permuting the channels in the network.

Let's consider the \(j\)'th convolution kernel \(^{c_{j} c_{j-1} k k}\) of \(\). There should exist permutations \(P_{j}\) and \(P_{j-1}\) such that \(() P_{j} P_{j-1}^{T}\), where the permutations act on the channels of \(\) and \(\) on the spatial \(k k\) part. But if we assume that the \(P_{j}\)'s are of order 2 so that they define representations of the horizontal flipping group, this is precisely the kernel constraint (3), which makes the CNN close to a GCNN! It seems intuitive that the \(P_{j}\)'s are of order 2, i.e. that if a certain channel should be permuted to another to make the filters as close to each other as possible then it should hold the other way around as well. However, degeneracies such as multiple filters being close to equal can in practice hinder this intuition. Nevertheless, we conclude with the following conjecture which in some sense is a weaker version of Conjecture 3.1, as we deduced it from that one.

**Conjecture 3.2**.: _Most SGD CNN-solutions on image data with a distribution that is invariant to horizontal flips of the images will be close to GCNNs._

Figure 3: For every CNN that is trained with horizontal flipping data augmentation, there is a corresponding equally likely CNN that was initialized with filters horizontally flipped and trained on horizontally flipped images. This CNN is the same as the original but with flipped filters, also after training. According to the permutation conjecture , there should be a permutation of the channels of the flipped CNN that aligns it close to the original CNN. This implies that the CNN is close to a GCNN. Lighter blue means a spot in the parameter landscape with higher accuracy.

A measure for closeness to being a GCNN.To measure how close a CNN \(\) is to being a GCNN we can calculate the barrier, as defined in (4), of the linear interpolation between \(\) and a permutation of the flipped version of \(\). We call this barrier for the test accuracy the _GCNN barrier_.

## 4 Experiments

The aim of the experimental section is to investigate two related questions.

1. If a CNN is trained to be invariant to horizontal flips of input images, is it a GCNN? This question is related to the theoretical investigation of layerwise equivariance in Section 2.
2. If a CNN is trained on a horizontal flipping-invariant data distribution, will it be close to a GCNN? This is Conjecture 3.2.

To answer these two questions we evaluate the GCNN barrier for CNNs trained on CIFAR10 and ImageNet. We look at CNNs trained with horizontal flipping data augmentation to answer (Q2) and CNNs trained with an invariance loss on the logits to answer (Q1). In fact, for all training recipes horizontal flipping data augmentation is used. The invariance loss applied during training of a CNN \(\) is given by \(\|(x)-((x))\|\), where \(\) horizontally flips \(x\). It is added to the standard cross-entropy classification loss. To evaluate the invariance of a CNN \(\) we will calculate the relative invariance error \(\|(x)-((x))\|/(0.5\|(x)\|+0.5\|((x))\|),\) averaging over a subset of the training data. Another way to obtain invariance to horizontal flips is to use some sort of self-supervised learning approach. We will investigate self-supervised learning of ResNets in Section 4.2.

To align networks we will use activation matching [25; 1]. In activation matching, the similarity between channels in feature maps of the same layer in two different networks is measured over the training data and a permutation that aligns the channels as well as possible between the networks according to this similarity is found. Furthermore, we will use the REPAIR method by , which consists of--after averaging the weights of two networks--reweighting each channel in the obtained network to have the average batch statistics of the original networks. This reweighting can be merged into the weights of the network so that the original network structure is preserved. REPAIR is a method to compensate for the fact that when two filters are not perfect copies of each other, the variance of the output of their average will be lower than the variance of the output of the original filters. We will also report results without REPAIR.

Experimental details can be found in Appendix A. We provide code for merging networks with their flipped selfs at https://github.com/georg-bn/layerwise-equivariance.

### VGG11 on CIFAR10

We train VGG11 nets  on CIFAR10 . We will consider a couple of different versions of trained VGG11 nets, they are listed in Table 1.

We train 24 VGG11 nets for each model type and discard crashed runs and degenerate runs3 to obtain 18-24 good quality nets of each model type. In Figure 1 we visualize the filters of the first layer in two

 
**Name** & **Description** \\  CNN & Ordinary VGG11 trained using cross-entropy loss (C.-E.). \(9.23\)M parameters. \\ CNN w/o aug & Ordinary VGG11 trained without horizontal flipping augmentation. \\ CNN + inv-loss & Ordinary VGG11 trained using C.-E. and invariance loss (inv-loss). \\  CNN + late inv-loss & Ordinary VGG11 trained using C.-E. and inv-loss after \(20\%\) of the epochs. \\ GCNN & A regular horizontal flipping GCNN trained using C.-E. \(4.61\)M parameters. \\ PGCNN & A partial horizontal flipping GCNN trained using C.-E. The first two conv-layers \\  & are \(G\)-conv-layers and the rest are ordinary conv-layers. \(9.19\)M parameters. \\ PGCNN + late inv-loss & The PGCNN trained using C.-E. and inv-loss after \(20\%\) of the epochs. \\  

Table 1: Types of VGG11 nets considered. All except “w/o aug” are trained with horizontal flipping augmentation.

VGG11 nets. More such visualizations are presented in Appendix A. We summarise the statistics of the trained nets in Table 2. The most interesting findings are that the models with low invariance error also have low GCNN barrier, indicating that invariant models are layerwise equivariant. Note that the invariance error and GCNN barrier should both be zero for a GCNN. In order to have something to compare the numbers in Table 2 to, we provide barrier levels for merging two different nets in Table 4 in the appendix. Of note is that merging a model with a flipped version of itself is consistently easier than merging two separate models. In Figure 4 in the appendix, we show the distribution of permutation order for channels in different layers. The matching method sometimes fails to correctly match GCNN channels (for which we know ground truth matches), but overall it does a good job. In general, the unconstrained nets seem to learn a mixture between invariant and regular GCNN channels. Preliminary attempts with training GCNN-VGG11s with such a mixture of channels did however not outperform regular GCNNs.

### ResNet50 on ImageNet

Next we look at the GCNN barrier for ResNet50  trained on ImageNet . We consider a couple of different training recipes. First of all two supervised methods--the latest Torchvision recipe  and the old Torchvision recipe  which is computationally cheaper. Second, four self-supervised methods: BYOL , DINO , Moco-v3  and Simsiam . The results are summarised in Table 3. There are a couple of very interesting takeaways. First, the GCNN barrier for the supervised methods is unexpectedly low. When merging two separate ResNet50's trained on ImageNet,  report an absolute barrier of \(20\) percentage points, whereas we are see barriers of \(4.4\) percentage points for the new recipe and \(2.9\) percentage points for the old recipe. This indicates that it easier to merge a net with a flipped version of itself than with a different net. However, we also observe that for the self-supervised methods the barrier is quite high (although still lower than \(20\) percentage points). This is curious since they are in some sense trained to be invariant to aggressive data augmentation--including horizontal flips. Finally we note that when training with an invariance loss, the GCNN barrier vanishes, meaning that the obtained invariant net is close to being a GCNN.

 
**Name** & **Accuracy** & **Invariance Error** & **GCNN Barrier** \\  CNN & \(0.901 2.1 10^{-3}\) & \(0.282 1.8 10^{-2}\) & \(4.00 10^{-2} 4.9 10^{-3}\) \\ CNN w/o aug & \(0.879 1.8 10^{-3}\) & \(0.410 4.0 10^{-2}\) & \(4.98 10^{-2} 6.1 10^{-3}\) \\ CNN + inv-loss & \(0.892 2.3 10^{-3}\) & \(0.0628 6.7 10^{-3}\) & \(1.90 10^{-3} 8.9 10^{-4}\) \\ CNN + late inv-loss & \(0.902 2.8 10^{-3}\) & \(0.126 1.6 10^{-2}\) & \(1.33 10^{-2} 2.8 10^{-3}\) \\ GCNN & \(0.899 2.5 10^{-3}\) & \(1.34 10^{-6} 1.5 10^{-7}\) & \(8.92 10^{-4} 1.6 10^{-3}\) \\ PGCNN & \(0.902 3.7 10^{-3}\) & \(0.274 2.0 10^{-2}\) & \(3.09 10^{-2} 7.8 10^{-3}\) \\ PGCNN + late inv-loss & \(0.915 2.3 10^{-3}\) & \(0.124 1.5 10^{-2}\) & \(7.56 10^{-3} 1.8 10^{-3}\) \\  

Table 2: Statistics for VGG11 nets trained on CIFAR10.

   & **Invariance** & & & & **Halfway** & & **GCNN** & \\
**Training Method** & **Error** & **Accuracy** & **Accuracy** & **Accuracy**, & **Halfway** & **Barrier**, & **GCNN** \\  Torchvision new & \(0.284\) & \(0.803\) & \(0.803\) & \(0.731\) & \(0.759\) & \(0.0893\) & \(0.0545\) \\ Torchvision old & \(0.228\) & \(0.761\) & \(0.746\) & \(0.718\) & \(0.725\) & \(0.0467\) & \(0.0384\) \\ *Torchvision old & \(0.0695\) & \(0.754\) & \(0.745\) & \(0.745\) & \(0.745\) & \(0.00636\) & \(0.0054\) \\ BYOL & \(0.292\) & \(0.704\) & \(0.712\) & \(0.573\) & \(0.624\) & \(0.19\) & \(0.118\) \\ DINO & \(0.169\) & \(0.753\) & \(0.744\) & \(0.611\) & \(0.624\) & \(0.184\) & \(0.166\) \\ Moco-v3 & \(0.16\) & \(0.746\) & \(0.735\) & \(0.64\) & \(0.681\) & \(0.136\) & \(0.0805\) \\ Simsiam & \(0.174\) & \(0.683\) & \(0.667\) & \(0.505\) & \(0.593\) & \(0.252\) & \(0.121\) \\  

Table 3: Results for ResNet50’s trained using various methods on ImageNet. The model with an asterisk is trained by us. The last five are self-supervised methods. Flip-Accuracy is the accuracy of a net with all conv-filters horizontally flipped. Halfway Accuracy is the accuracy of a net that has merged the weights of the original net and the net with flipped filters—after permuting the second net to align it to the first. For the accuracy and barrier values w/o REPAIR we still reset batch norm statistics after merging by running over a subset of the training data.

The reader may have noticed that the "Flip-Accuracy", i.e., accuracy of nets with flipped filters, is markedly different than the accuracy for most nets in Table 3. This is a defect stemming from using image size 224. We explain this and present a few results with image size 225 in Appendix A.2.1.

## 5 Conclusions and future work

We studied layerwise equivariance of ReLU-networks theoretically and experimentally. Theoretically we found both positive and negative results in Section 2, showing that layerwise equivariance is in some cases guaranteed given equivariance but in general not. In Section 3, we explained how Entezari et al.'s permutation conjecture 3.1 can be applied to a single CNN and the same CNN with horizontally flipped filters. From this we extrapolated Conjecture 3.2 stating that SGD CNN solutions are likely to be close to being GCNNs, also leading us to propose a new measure for how close a CNN is to being a GCNN--the GCNN barrier. In Section 4 we saw quite strong empirical evidence for the fact that a ReLU-network that has been trained to be equivariant will be layerwise equivariant. We also found that it is easier to merge a ResNet with a flipped version of itself, compared to merging it with another net. Thus, Conjecture 3.2 might be a worthwhile stepping stone for the community investigating Conjecture 3.1, in addition to being interesting in and of itself. If a negative GCNN barrier is achievable, it would imply that we can do "weight space test time data augmentation" analogously to how merging two separate nets can enable weight space ensembling.