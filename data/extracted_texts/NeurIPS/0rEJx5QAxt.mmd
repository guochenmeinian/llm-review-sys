# Convex-Concave 0-Sum Markov Stackelberg Games

Denizalp Goktas

Brown University, Computer Science

denizalp_goktas@brown.edu

&Arjun Prakash

Brown University, Computer Science

arjun_prakash@brown.edu

&Amy Greenwald

Brown University, Computer Science

amy_greenwald@brown.edu

###### Abstract

Zero-sum Markov Stackelberg games can be used to model myriad problems, in domains ranging from economics to human robot interaction. We develop a policy gradient method which we prove solves these games in continuous state, continuous action settings, using noisy gradient estimates computed from observed trajectories of play. When the games are convex-concave, we prove that our algorithm converges to Stackelberg equilibrium in polynomial time. We also prove that reach-avoid problems are naturally modeled as convex-concave zero-sum Markov Stackelberg games, and show experimentally that Stackelberg equilibrium policies are more effective than their Nash counterparts in these problems.1

## 1 Introduction

Markov games  are a generalization of Markov decision processes (MDPs) comprising multiple players simultaneously making decisions over time, collecting rewards along the way depending on their collective actions. They have been used by practitioners to model many real-world multiagent planning and learning environments, such as autonomous driving , cloud computing , and telecommunications . Moreover, theoreticians are beginning to formally analyze policy gradient methods, proving polynomial-time convergence to optimal policies in MDPs , and to Nash equilibrium policies  in zero-sum Markov games , the canonical solution concept. While Markov games are a fruitful way to model some problems (e.g., robotic soccer ), others, such as reach-avoid , may be more productively modeled as sequential-move games, where some players commit to moves that are observed by others, before they make their own moves. To this end, we study two-player zero-sum Markov Stackelberg  (i.e., sequential-move) games. While polynomial-time value-iteration (i.e., planning) algorithms are known for these games assuming discrete states , we develop a policy gradient method that converges to Stackelberg equilibrium in polynomial time in continuous state, continuous action games, using noisy gradients based only on observed trajectories of play. Furthermore, we demonstrate experimentally that Stackelberg equilibrium policies are more effective than their Nash counterparts in reach-avoid problems.

A _(discounted discrete-time) zero-sum Markov Stackelberg game_ is played over an infinite horizon \(t=0,1,\) between two players, a leader and a follower. The game starts at time \(t=0\), at some initial state \(S^{(0)}()\) drawn randomly from a set of states \(\). At each time step \(t=1,2,\), the players encounter a state \(^{(t)}\), where the leader takes its action \(^{(t)}\) first, from its action space \((^{(t)})\), after which the follower, having observed the leader's action, makes it own move \(^{(t)}\), chosen from a feasible subset \((^{(t)},^{(t)})\) determined by the leader's action \(^{(t)}\)of its action space \((^{(t)})\).2 After both players have taken their actions, they receive respective rewards, \(-r(^{(t)},^{(t)},^{(t)})\) and \(r(^{(t)},^{(t)},^{(t)})\). The game then moves to time step \(t+1\) and transitions either to a new state \(S^{(t+1)} p(^{(t)},^{(t)},^{(t)})\) with probability \(\), called the _discount factor_, or the game ends with the remaining probability. Each player's goal is to play a (potentially history-dependent) _policy_ that maximizes its respective _expected (cumulative discounted) payoff_s, \(-}[_{t=0}^{}^{t}r(S^{(t)},A^{(t)},B^{(t)})]\) and \(}[_{t=0}^{}^{t}r(S^{(t)},A^{(t)},B^{(t)})]\).3

In zero-sum Markov Stackelberg games, when the reward function \((,) r(,,)\) is continuous and bounded, for all \(\), and the correspondence \(}(,)\) is continuous, as well as non-empty- and compact-valued, a _recursive_ (or _Markov perfect_) _Stackelberg equilibrium_ is guaranteed to exist , meaning a _stationary policy profile_ (i.e., a pair of mappings from states to the actions of the leader and the follower, respectively) specifying the actions taken at each state s.t. the leader's policy maximizes its expected payoff assuming the follower best responds, while the follower indeed best responds to the leader's policy. In other words, the aforementioned assumptions guarantee the existence of a _policy profile_\(^{*}(^{*}_{},^{*}_{})\), with \(^{*}_{}:\) and \(^{*}_{}:\), that solves the following _coupled_ min-max optimization problem:

\[_{_{}:}_{ _{}::\\ ,_{}()(, _{}())}}[ _{t=0}^{}^{t}r(S^{(t)},_{}(S^{(t)}),_{} (S^{(t)}))],\] (1)

where the expectation is with respect to \(S^{(0)}\) and \(S^{(t+1)} p(^{(t)},_{}(S^{(t)}),_{ {b}}(S^{(t)}))\). The problem is "coupled" because the players' actions sets constrain one another; in particular, the set of actions available to the follower at each state is determined by the leader's choice.

In spite of multiple compelling applications, including autonomous driving , reach-avoid problems in human-robot interaction , robust optimization in stochastic environments , and resource allocation over time , very little is known about computing recursive Stackelberg equilibria in zero-sum Markov Stackelberg games. A version of value iteration is known to converge in polynomial time when the state space is discrete , but this (planning) method becomes intractable in large or continuous state spaces. Furthermore, nothing is known, to our knowledge, about _learning_ Stackelberg equilibria from observed trajectories of play. We develop an efficient policy gradient method for convex-concave zero-sum Markov Stackelberg games, and we show that reach-avoid problems naturally lie in this class of games.

**Contributions.** Equation (1) reveals that the problem of computing Stackelberg equilibria in zero-sum Markov Stackelberg games is an instance of a coupled min-max optimization problem. Goktas and Greenwald  studied coupled min-max optimization problems assuming an _exact_ first-order oracle, meaning one that returns a function's exact value and gradient at any point in its domain. As access to an exact oracle is an unrealistic assumption in Markov games, we develop a first-order method for solving these problems, assuming access to a _stochastic_ first-order oracle, which returns noisy estimates of a function's value and gradient at any point in its domain. We show that our method converges in polynomial-time (Theorem 3.1) in a large class of coupled min-max optimization problems, namely those which are convex-concave.

We then proceed to study zero-sum Markov Stackelberg games, providing sufficient conditions on the action correspondence \(:\), the rewards \(r:\), and the transition probabilities \(p:_{+}\) to guarantee that the game is convex-concave. Furthermore, we develop a policy gradient algorithm that provably converges to Stackelberg equilibrium in polynomial time when such games are convex-concave (Theorem 4.1), the first reinforcement learning algorithm of this kind. Our method specializes to continuous state, continuous action zero-sum Markov games; as such, we provide a provably-convergent policy gradient method for these problems as well. Finally, we prove that our framework naturally models reach-avoid problems, and run experiments which show that the Stackelberg equilibrium policies learned by our method exhibit better safety and liveness properties than their Nash counterparts.

## 2 Preliminaries

**Notation.** All notation for variable types, e.g., vectors, should be clear from context; if any confusion arises, see Appendix A. Unless otherwise noted, we assume \(\) is the Euclidean norm, \(_{2}\). We let \(_{n}=\{_{+}^{n}_{i=1}^{n}x_{i}=1\}\) denote the unit simplex in \(^{n}\), and \((A)\), the set of probability distributions on the set \(A\). We also define the support of any distribution \(f()\) as \((f)\{:f()>0\}\). We denote the orthogonal projection operator onto a set \(C\) by \(_{C}\), i.e., \(_{C}()=*{arg\,min}_{ C}-^{2}\). We denote by \(_{}()\) the indicator function of a set \(\), with value 1 if \(\) and 0 otherwise. Given two vectors \(,^{n}\), we write \(\) or \(>\) to mean component-wise \(\) or \(>\), respectively. For any set \(\), we denote the diameter by \(()_{,^{}}-^{}\). Given a tuple consisting of a sequences of iterates and weights \((\{^{(t)}\}_{t},\{^{(t)}\}_{t})\), the weighted average of the iterates is given by \(}_{}}^{(t)}^{(t)}} {_{}^{(t)}}\).

**Mathematical Concepts.** Given \(^{n}\), the function \(f:\) is said to be _\(_{f}\)-Lipschitz-continuous_ w.r.t. norm \(\) iff \(_{1},_{2}\), \( f(_{1})-f(_{2})_{f} _{1}-_{2}\). If \(=\), then \(f\) is _convex_ (resp. _concave_) iff for all \((0,1)\) and \(,^{}\), \(f(+(1-)^{}))\, f()+(1-)f(^{})\). For any \(\), if the relation holds with equality, then \(f\) is called _affine_. A two-sided function \(h:\) is _biaffine_ if \( f(,)\) is affine for all \(\), and \( h(,)\) is affine for all \(\). \(f\) is _\(\)-strongly convex_ if \(f(_{1}) f(_{2})+_{}f(_{2}),_{1}-_{2}+}{{2}}_{1}-_{1}^{2}\). For convenience, we say that an \(l\)-dimensional vector-valued function \(:^{l}\) is log-convex, convex, log-concave, or concave, respectively, if \(g_{k}\) is log-convex, convex, pose-concave, or concave, for all \(k[l]\). A correspondence \(:\) is _concave_ if for all \((0,1)\) and \(,^{}\), \((+(1-)^{})()+(1-)()\), assuming Minkowski summation [22; 57].

We require notions of stochastic convexity related to stochastic dominance of probability distributions . Given non-empty and convex parameter and outcome spaces \(\) and \(\) respectively, a conditional probability distribution \( p()()\) is said to be _stochastically convex_ (resp. _stochastically concave_) in \(\) if for all continuous, bounded, and convex (resp. concave) functions \(v:\), \((0,1)\), and \(^{},^{}\) s.t. \(}=^{}+(1-)^{}\), it holds that \(_{ p(})}[v(O)] ()\,_{ p( ^{})}[v(O)]+(1-)\,_{ p (^{})}[v(O)]\).

## 3 Coupled Min-Max Optimization Problems

_A min-max Stackelberg game_, denoted \((,,f,)\), is a two-player, zero-sum game, where one player, called the _leader_, first commits to an action \(\) from its _action space_\(^{n}\), after which the second player, called the _follower_, takes an action \(()\) from a subset of of his _action space_\(^{m}\) determined by the _action correspondence_\(:^{n}\). As is standard in the optimization literature, we assume throughout that the follower's action correspondence can be equivalently represented via a _coupling constraint function_\(:^{n}^{m}^{d}\) s.t. \(()\{(,) \}\). An _action profile_\((,)\) comprises actions for both players. Once both players have taken their actions, the leader (resp. follower) receives a loss (resp. payoff) \(f(,)\), defined by an _objective function_\(f:^{n}^{m}\). We define the _marginal function_\(f^{*}()_{()}f(,)\), which, given an action for the leader, outputs its ensuing payoff, assuming the follower best responds. The constraints in a min-max Stackelberg game are said to be _uncoupled_ if \(()=\), for all \(\). A min-max Stackelberg game is said to be _continuous_ iff 1. the objective function \(f\) is continuous; 2. the action spaces \(\) and \(\) are non-empty and compact; and 3. the action correspondence \(\) is continuous, non-empty- and compact-valued.4

**Stackelberg Equilibrium.** The canonical solution concept for min-max Stackelberg games is the \((,)\)-_Stackelberg equilibrium (\((,)\)-SE, or SE if \(==0\)), an action profile \((^{*},^{*})\) s.t. \(_{^{n}_{}}[(^{*},^{*}) ]\) and \(_{}f^{*}()+ f(^{*},^{* })_{(^{*})}f(^{*},)-\), for \(, 0\).5 As a straightforward corollary of Theorem 3.2 of Goktas and Greenwald , a SE is guaranteed to exist in continuous Stackelberg games. Moreover, the set of SE can be characterized as solutions to the following _coupled min-max optimization problem_: \(_{}_{()}f(,)\).

An alternative but weaker solution concept previously considered for min-max Stackelberg games  is the \(\)_-generalized Nash equilibrium_ (\(\)_-GNE, or GNE if \(=0\)), i.e., \((^{*},^{*})(^{*})\) s.t. \(_{}f(,^{*})+ f (^{*},^{*})_{(^{*})} f(^{*},)-\), for some \( 0\).6 In general, the set of GNE and SE need not intersect; as such, GNE are not necessarily solutions of \(_{}_{()}f(, )\) (see, Appendix A of Goktas and Greenwald ). Furthermore, there is no GNE whose value is less than the SE value of a game. When a min-max Stackelberg game's constraints are uncoupled, a(n \(\))-GNE is called a(n \(\))_-saddle point_, or a(n \(\))_-Nash equilibrium_, and _is_ also an SE. Finally, a saddle point is guaranteed to exist  in continuous min-max Stackelberg games with uncoupled constraints, a convex-concave objective \(f\), and convex action spaces \(\) and \(\), in which case such games have traditionally been referred to as convex-concave min-max (simultaneous-move) games or saddle-point problems .

**Convex-Concave Games.** A min-max Stackelberg game is said to be _convex-concave_ if, in addition to being continuous, 1. \(f^{*}\) is convex; 2. \( f(,)\) is concave, for all \(\); 3. \(\) and \(\) are convex; and 4. \(\) is convex-valued. This definition generalizes that of convex-concave min-max (simultaneous-move) game, because in such games, the marginal function \(f^{*}\) is necessarily convex when \(f\) is convex, by Danskin's theorem . Assuming access to an exact first-order oracle, an \((,)\)-SE of a convex-concave min-max Stackelberg game can be computed in polynomial time when \(f\) and \(\) are Lipschitz-continuous , while the computation is NP-hard in continuous min-max Stackelberg games, even when \(\) and \(\) are convex, \(f\) is convex-concave, and \(\) is affine .

All the conditions that define a convex-concave Stackelberg game depend on the game primitives, namely \((,,f,)\), and are well-understood (see, for instance Section 5 of Rockafellar and Wets ), with the exception of the condition that the marginal function \(f^{*}\) be convex. While it is difficult to obtain necessary and sufficient conditions on the game primitives that ensure the convexity of \(f^{*}\), one possibility is to require \(f\) to be convex in \((,)\) and \(\) to be concave.7 The following sufficient conditions, which also guarantee concavity, were introduced by Goktas and Greenwald .

**Assumption 1** (Convex-Concave Assumptions).: _1. The objective function \(f(,)\) is convex in \((,)\), and concave in \(\), for all \(\); 2. the action correspondence \(\) is concave; 3. the action spaces \(\) and \(\) are convex._

As these assumptions are only sufficient, they are not necessarily satisfied in all applications of convex-concave min-max Stackelberg game. Hence, the convexity of the marginal function must sometimes be established by other means. We thus provide the following alternative set of sufficient conditions, which we use to show that the reach-avoid problem we study in Section 5 is convex-concave.

**Assumption 2** (Alternative Convex-Concave Assumptions).: _1. (Convex-concave objective) The objective \(f(,)\) is convex in \(\), for all \(\), and concave in \(\), for all \(\); 2. (log-convex-concave coupling) the coupling constraint \((,)\) is log-convex in \(\) for all \(\), and concave in \(\) for all \(\); and 3. the action spaces \(\) and \(\) are convex._

**Computation.** We now turn our attention to the computation of \((,)\)-SE in convex-concave min-max Stackelberg games, assuming access to an _unbiased first-order stochastic oracle_\((,,,)\) comprising random functions \(:^{n}^{m}\) and \(:^{n}^{m}^{d}\) and sampling distributions \(()\) and \(()\) s.t. \(_{}[(,; )]=f(,)\), \(_{}[(,; )]=(,)\), \(_{}[_{(,)}(,; )]= f(,)\), and \(_{}[_{(,)}(,;)]=(,)\). The following assumptions are required for the convergence of our methods.

**Assumption 3**.: _1. (Lipschitz game) \(f\) and \(\) are Lipschitz-continuous; 2. (concave representation) the coupling constraint function \((,)\) is concave for all \(\); 3. (Slater's condition) \(,}\) s.t. \((,})>0\); and 4. (stochastic oracle) there exists an unbiased first-order stochastic oracle \((,,,)\) with bounded variance s.t. \((,)\), \([\|(,;)\|^{2}]_{}\), \([\|_{(,)}(,;) \|^{2}]_{}\), and \([\|_{(,)}(,;)\|^{2}] _{}\), for \(_{},_{},_{} 0\)._

In the sequel, we rely on the following notation and definitions. For any action \(\) of the leader, we can re-express the marginal function in terms of the _Lagrangian_\((,;) f(,)+, (,)\) (see, for instance, Section 5 of Boyd et al. ) as follows: \(f^{*}()=_{}_{^{d}_{+ }}(,;)\). Further, we define the follower's best-response correspondence \(^{*}()*{arg\,max}_{}_{ ^{d}}(,;)\), and the KKT multiplier correspondence \(^{*}()*{arg\,min}_{^{d} _{+}}_{}(,;})\). With these definitions in hand, under Assumption 3, we can build an unbiased first-order stochastic oracle \(}(,;,,) (,;)+<,(, ;)>\) for the Lagrangian \(\) s.t. \(_{(,)}[}(, ;,,)]\), where the expectation is taken over \((,)\).

**Algorithms.** Assuming access to an exact first-order oracle \((f,)\), a natural approach to computing SE in convex-concave min-max Stackelberg games with uncoupled constraints games (i.e., saddle-point problems) is to simultaneously run projected gradient descent and projected gradient ascent on the objective function \(f\) w.r.t. \(\) and \(\), i.e., for \(t=0,1,2,\), \((^{(t+1)},^{(t+1)})_{}[ (^{(t)},^{(t)})+(-_{}f,_{}f)(^{(t)}, ^{(t)})]\), a method known under the names of _Arrow-Hurwicz-Uzawa, primal-dual_, and (simultaneous) _gradient descent ascent (GDA)_. Intuitively, any fixed point of GDA in such games, i.e., \((^{*},^{*})\) s.t. \(\|(^{*},^{*})-_{}[(^{*}, {y}^{*})+(-_{}f,_{}f)(^{*},^{*})]\|=0\), satisfies the necessary and sufficient optimality condition for an action profile to be a SE. More generally, in convex-concave min-max Stackelberg games (without coupled constraints), this approach fails, as the necessary and sufficient optimality condition for an action profile \((^{*},^{*})\) to be a SE is \(\|(^{*},^{*})-_{(^{*})}[( {x}^{*},^{*})+(-_{}f^{*}(^{*}),_{}f( ^{*},^{*}))]\|=0\), where, for any leader action \(},_{}f^{*}(}) (^{*}(}),^{*}(}); {})\), for some \((^{*},^{*})(})^{*}(})^{*}(})\), by the subdifferential envelope theorem . The observation that any subgradient of \(_{}f^{*}\) depends on the optimal KKT multipliers motivates a first-order method based on the gradient of the Lagrangian.

A min-max Stackelberg game can be seen as a three-player game \(_{}_{()}f(, )=_{}_{}_{ ^{d}_{+}}(,;)\), where the \(\)-player moves first, and the \(\)- and \(\)-players move second, simultaneously, because strong duality holds under Assumption 3 (Slater's condition ) for the inner min-max optimization problem, i.e., \(_{}_{^{d}_{+}}( ,;)=_{^{d}_{+}} _{}(,;)\). The problem of computing an SE can thus be reduced to the min-max optimization \(_{(,)^{d}_{+}}_{ }(,;)\), which we might hope to solve by running GDA on \((,;)\) w.r.t. \((,)\) and \(\) over \(^{d}_{+}\) and \(\), respectively. Although \((,;)\) is concave, \((,)(,;)\) is not convex, and its stationary points (i.e., points \((^{*},^{*};^{*})\) s.t. \(\|(^{*},^{*};^{*})-_{^{ d}_{+}}[(^{*},^{*};^{*})+(_{} ,-_{},-_{})(^{*},^{ *};^{*})]\|=0\)) do not necessarily coincide with SE even in simple convex-concave min-max Stackelberg games .

```
1:\(,,,,,,^{(0 )},T_{},\{_{}^{(t)}\}_{t},\) (+ for nested SGDA:) \(,^{(0)},^{(0)},T_{},\{_{ }^{(t)}\}_{t}\)
2:\((^{(t)},^{(t)},^{(t)})_{t=0}^{T_{}}\)
3:for\(t=0,,T_{}\)do
4:ifSaddle-Point-Oracle SGDthen
5: Find \((^{(t)},^{(t)})^{d}_{+}\) s.t. \(_{}(^{(t)},;^{(t)} )-_{^{d}_{+}}(, ^{(t)};^{(t)})\), the convexity of the marginal function \(f^{*}\), the algorithm runs a descent step on \(f^{*}\) w.r.t. \(\), in which, for any leader action \(\), a subgradient \(_{}f^{*}\) is approximated by \(}f^{*}}()=(},};)\). In this paper, we replace the exact first-order oracle used by nested GDA with a stochastic one, the gradient descent step with a step of stochastic gradient descent (SGD), and GDA with stochastic GDA (SGDA), using in both cases the stochastic 

[MISSING_PAGE_FAIL:6]

follower). Additionally, the _marginal action-value function_\(q^{*}(,)_{(,)}q^{ }(,,)\) is the payoff when play initiates at state \(\) with the leader taking action \(\), after which the follower best responds (at state \(\) only), with both players playing according to \(\) thereafter. Finally, for any leader policy \(_{}^{}\), we define the _marginal (state-value) function_\(u^{*}(_{})_{_{}(_{})}u(_{},_{})\).

**Recursive Stackelberg Equilibrium.** A policy profile \(^{*}(_{}^{*},_{}^{*}) ^{}^{}\) is called an \((,)\)_-recursive (or Markov perfect) Stackelberg equilibrium_ iff \(\), \(\|_{^{}}\|\,(,^{*}())\|\) and \(_{_{}(_{})}v^{(_{}^{*},_{})}()- v^{^{*}}() _{_{}^{}}_{_{} (_{})}v^{(_{},_{})}()+\). A recursive SE is guaranteed to exist in continuous state, continuous action zero-sum Markov Stackelberg games . A policy profile \(^{*}(_{}^{*},_{}^{*}) ^{}(_{}^{*})\) is called an \((,)\)_-Markov perfect GNE_ iff \(\), \(_{_{}(_{})}v^{(_{}^{*},_{}^{*})}()- v^{^{*}}() _{_{}^{}}v^{_{},_{}^{*}}()+\).

**Convex-Concave Markov Stackelberg Games.** As we have shown (Theorem 3.1), Stackelberg equilibria can be computed in polynomial time in convex-concave min-max Stackelberg games, assuming access to an unbiased first order-stochastic oracle. We now define an analogous class of Markov Stackelberg games, namely zero-sum Markov Stackelberg games in which the min-max Stackelberg game played at each state is convex-concave. A _convex-concave zero-sum Markov Stackelberg game_ is a continuous state, continuous action zero-sum Markov game where, for all policy profiles \(^{}^{}\), 1. the marginal action-value function \((,) q^{*}(,)\) is convex, 2. the action-value function \((,) q^{}(,,)\) is concave, for all \(\), 3. the state and action spaces \(,\) and \(\) are convex, and 4. the action correspondence \(\) is convex-valued. We note that any _continuous state, continuous action convex-concave zero-sum Markov game_, i.e., 1. \(=\), 2. \((,) r(,,)\) is convex, for all \(\), 3. \((,) r(,,)\) is concave, for all \(\), 4. \((,) p(,,)\) is stochastically convex, for all \(\); and 5. \((,) p(,,)\) is stochastically concave, for all \(\), is a convex-concave zero-sum Markov Stackelberg game for which the set of Markov perfect generalized Nash equilibria is a subset of the recursive SE.

As our plan is to use our nested SGDA algorithm to compute recursive Stackelberg equilibria, we begin by showing that zero-sum Markov Stackelberg games are an instance of min-max Stackelberg games. Assume parametric policy classes for the leader and follower, respectively, namely \(_{}\{_{}: \}^{}\) and \(_{}\{_{}: \}^{}\), for parameter spaces \(^{d}\) and \(^{d}\). Using these parameterizations, we redefine \(v^{(,)} v^{(_{},_{})}\), \(q^{(,)} q^{(_{},_{})}\), \(u(,) u(_{},_{})\), etc., and thus restate the problem of computing a recursive SE as finding \((,)\) that solves \(_{}_{()}v^{( ,)}()\), for all states \(\). As this optimization problem is infinite dimensional for continuous state games, we optimize the objective and satisfy the constraints, both in expectation over the initial state distribution \(\), thereby reducing the problem to the min-max Stackelberg game \(_{}_{()}u(,)\).

In Appendix D, assuming 1. biaffine parametric policy classes, i.e., \((,)_{}()\) and \((,)_{}()\) are biaffine, and 2. non-empty, compact, and convex parameter spaces \(\) and \(\), we show that the min-max Stackelberg game associated with any convex-concave zero-sum Markov Stackelberg game is also convex-concave (Lemma 4). We also provide sufficient conditions on the primitives \(\) of any zero-sum Markov Stackelberg game to ensure that it is convex-concave (Lemma 5 and 6). At a high level, our results allow us to conclude that a zero-sum Markov Stackelberg game is convex-concave if the 1. reward (resp. transition probability) function is concave (resp. stochastically concave) in the state and the follower's action; 2. the reward (resp. transition probability) function is convex (resp. stochastically convex) in the state and the leader's follower's actions; and 3. the follower's action correspondence is concave.

**Computation.** We now turn our attention to the computation of recursive SE in convex-concave zero-sum Markov Stackelberg games. Mirroring the steps by which policy gradient has been show to converge in other settings , we first define an unbiased first-order stochastic oracle for zero-sum Markov-Stackelberg games, given access to an unbiased first-order stochastic oracle for the reward and probability transition functions. We then establish convergence of nested SGDA in this setting by invoking Theorem 3.1 under the following assumptions.

**Assumption 4** (Convergence Assumptions).: _1. The parameter spaces \(\) and \(\) are non-empty, compact, and convex; 2. the policy parameterizations are biaffine, i.e., \((,)_{}()\) and \((,)_{}()\) are biaffine; 3. the set of non-trivially constrained sets is finite \(\), i.e. \(\|\|<\);_4. (Slater's condition) for all \(\) and \(\), there exists \(}\) s.t. \((,,})>0\); and 5. the reward \(r\), probability transition \(p\), and coupling constraint \(\) functions are Lipschitz-continuous._

Stochastic nested GDA relies on an unbiased first-order stochastic oracle \((,,,)\), which we can use to obtain unbiased first-order stochastic estimators of \(u\) and \(\). Since the constraints are deterministic, we simply set \((,;)((,_{}( ),_{}()))_{}\) and \(()()\), for any distribution \(()\) over the state space to obtain an unbiased first-order stochastic oracle for the constraints \(\). While for simplicity we define \(\) as such, \(\) is tractable to compute (i.e., finite-dimensional) only when \(\) is finite. When \(\) is infinite, our theoretical results generalize by setting \((,;)(_{}g_{c}(,_{}(),_{}()))_{c[d]}\); however, in practice, this estimator might be intractable, in which case one might choose to abandon our theoretical guarantees in favor of the biased estimator \((,;)(,_{}( ),_{}())\). In all cases, the definition of \(_{(,)}\) follows directly, since \(\) is deterministic. Now, for any history \(\) of length \(\), define the _cumulative payoff estimator_\((;)_{t=0}^{-1}(^{(0)})_{k= 0}^{t-1}^{k}p(^{(k+1)}^{(k)},(^{(k)}))r( ^{(k)},(^{(k)})))\). We then construct an estimator for \(u\) using _first-order gradient estimator_, i.e., we set \((,;)(_{}, _{};)\), and \(_{(,)}(,;)_{(,)}(_{},_{};)\). Regarding the variances of this oracle model, as \(\) and \(_{(,)}\) are deterministic, they have bounded variance. Moreover, if the policy and the reward and transition probability functions are Lipschitz-continuous, then \(\) and \(_{(,)}\) are also Lipschitz-continuous if their domains are compact (i.e., if \(\), \(\), and \(\) are compact). Hence \(\) and \( F\) likewise must be Lipschitz-continuous, which implies that their variances must be bounded, e.g., there exists \(_{ f}\) s.t. \(_{}[\|(,;)\|^{2}] \|(,;)\|_{}^{2}=_{ f}\) where the middle expression is well-defined since \(\) is Lipschitz-continuous over its compact domain.

With all of this machinery in place, we can now extend nested SGDA to compute recursive Stackelberg equilibria in zero-sum Markov Stackelberg games (Algorithm 2; Appendix C). In the usual case, when the policy parameterization does not represent the space of _all_ policies \(^{}^{}\), this result should be understood as convergence to the recursive Stackelberg equilibria of a game in which the players' action spaces are restricted to the parameterized policies.

**Theorem 4.1**.: _Let \(\) be a convex-concave zero-sum Markov Stackelberg game. Under Assumption 4, for any \(, 0\), if nested policy gradient descent ascent (Algorithm 2, Appendix C) is run with inputs that satisfy for all \(t_{+}\), \(_{}^{(t)},_{}^{(t)}(}{{}})\), and outputs \((^{*},^{*},^{*})\), then in expectation over all runs of the algorithm (i.e., sample paths of \(\) and \(\)), the policy profile \((_{^{*}},_{^{*}})\) is an \((+,)-\)recursive SE after \((}{{^{2}^{2}}})\) oracle calls._

## 5 Application: Reach-Avoid Problems

In this section, we endeavor to apply our algorithms to a real-world application, namely reach-avoid problems. In a reach-avoid problem (e.g., ), an agent seeks to reach one of a set of targets--achieve _liveness_--while avoiding obstacles along the way--ensuring _safety_. Reach-avoid problems have myriad applications, including network consensus problems , motion planning , pursuit-evasion games , autonomous driving , and path planning , to name a few.

The obstacles in a reach-avoid problem are not necessary stationary; they may move, either randomly or deliberately, around the environment. When the obstacles' movement is random, the problem can be modeled as an MDP. But when their movement is deliberate, so that they are more like a rational opponent than a stochastic process, the problem is naturally modeled as a zero-sum game, where the agent--the protagonist--aims to reach its target, while an antagonist--representing the obstacles--seeks to prevent the protagonist from doing so. Past work has modeled these games as simultaneous-move (e.g., ), imposing what should be a hard constraint--that the agent cannot collide with any of the obstacles--as a soft constraint in the form of large negative rewards.

Using the framework of zero-sum Markov Stackelberg games, we model this hard constraint properly, with the leader as the antagonist, whose movements impose constraints on the moves of the follower, the protagonist. We then use nested policy GDA to compute Stackelberg equilibria and simultaneous SGDA to compute GNE, and show experimentally that the protagonist learns stronger policies in the sequential (i.e., Stackelberg) game than in the simultaneous.

A (discrete-time discounted infinite-horizon continuous state and action) _reach-avoid game_\((l,,,,,,,r,)\) comprises two players, the _antagonist_ (or \(\)-player) and the _protagonist_ (or \(\)-player), each of whom occupies a state \(_{},_{}\) in a state space \(^{l}\), for some \(l\). The protagonist's goal is to find a path through the safe set \(\) that reaches a state in the target set \(\), while steering clear of the avoid set \(}=\). This safe and avoid set formulation is intended to represent capture constraints, which have been the focus of the reach-avoid literature .

Initially, the players occupy some state \(^{(0)}()\) drawn from an initial joint distribution \(\) over all states, excluding the target and avoid sets. At each subsequent time-step \(t_{+}\), the antagonist (resp. protagonist) chooses \(^{(t)}\) (resp. \(^{(t)}\)) from a set of possible directions \(^{l}\) (resp. \(^{l}\)) in which to move. After both the antagonist and the protagonist move, they receive respective rewards \(-r(^{(t)},^{(t)},^{(t)})\) and \(r(^{(t)},^{(t)},^{(t)})\). Then, either the game ends, with probability \(1-\), for some discount rate \((0,1)\), or the players move to a new state \(^{(t+1)}(^{(t)},,)=(_{}(^{(t)} _{},),_{}( ^{(t)}_{},))\), as determined by their respective displacement functions \(_{}:\) and \(_{}:\). We can express this deterministic transition as the following probability transition function \(p(^{},,) _{^{}}((, ,))\).

We define the feasible action correspondence \((,)\{(,,) \}\) via a vector-valued _safety constraint function_\(:^{2} ^{d}\), which outputs a subset of the protagonist's actions in the safe set, i.e., for all \((,)^{2}\), \((,)\{(,,)\}\). Note that we do not require this inclusion to hold with equality; in this way, the protagonist can choose to restrict itself to actions far from the boundaries of the avoid set, thereby increasing its safety, albeit perhaps at the cost of liveness. Overloading notation, we define the feasible policy correspondence \((_{})\{_{ }:_{}()(,_{ }()),\}\).

We consider two forms of reward functions. The first, called the _reach probability reward_, \(r(,,)=_{}( _{})\), is an indicator function that awards the protagonist with a payoff of \(1\) if it enters the target set, and \(0\) otherwise. Under this reward function, the cumulative payoff function (i.e., the expected value of these rewards in the long term) represents the probability that the protagonist reaches the target, hence its name. The second reward function is the _reach distance reward function_, \(r(,,)=-_{^{ }}\|_{}-^{ }\|^{2}\), which penalizes the protagonist based on how far away it is from the target set. With all these definitions in hand, we can now cast the reach-avoid game as a zero-sum Markov Stackelberg game \((2l,l,l,d,,,,,r,,p,)\).

The next assumption ensures that 1. under the reach probability reward function, a reach-avoid game is a convex-_non_-concave zero-sum Markov Stackelberg game (i.e., the marginal function \( u^{*}()\) is convex, and the cumulative payoff function \( u(,)\) is _non_-concave, for all \(\)); and 2. under the reach distance reward function, a reach-avoid game is a convex-concave zero-sum Markov Stackelberg game. Furthermore, a Markov perfect GNE is guaranteed to exist under this assumption, assuming the reach distance reward but not under the reach probability distance.10

To state this assumption, for convenience, we model the leader's policy \(_{}() _{}\) as parameterized by \(^{l l}\), and the follower's policy \(_{}() _{}\) as parameterized by \(^{l l}\). Note also that we assume decentralized, play, meaning the players learn only from their own state and rewards, and maintain their policies independently of one another.

**Assumption 5** (Convex-Concave Reach-Avoid Game).: _1. The state space \(\) and the target set \(\) are non-empty and convex; 2. the action spaces \(,\) are non-empty, compact and convex; 3. the displacement functions \(_{},_{}\) are affine; 4. \((,, )\) is log-convex for all \(\), and \((,, )\) for all \((,)\); 5. the players' parameter spaces \(\) and \(\) are non-empty, compact, and convex; and 6. the players policies are biaffine, i.e., \(_{}() _{}\) and \(_{}() _{}\)._

Part 1 is a standard assumption commonly imposed on reach-avoid games (see, for instance Fisac et al. ). Part 3 is satisfied by natural displacement functions of the type \((,,)=+ (,)\), for some \(\), which is a natural characterization of all displacement functions with constant velocity \(\), when \(=\{\| \|=1\}\). Part 4 is satisfied by various action correspondences, such as \((,,) \{_{^{}}}\|(_{ }(_{},),_{ })-^{}\|\}-1-\|_{}( _{},)-_{}\|\), which shrinksthe space of actions exponentially as the protagonist approaches the antagonist, and can thus be interpreted as describing a safety-conscious protagonist. The following theorem states the convex-concavity properties of reach-avoid games, and shows polynomial-time computability of recursive SE under Assumption 5. Note that for the reach probability reward function, it is not possible to obtain a polynomial-time convergence result, result since the rewards are not even continuous.

**Theorem 5.1**.: _Under the reach distance (resp. reach probability) reward function, any reach-avoid game for which Assumption 5 hold is convex-concave (resp. convex-non-concave). Moreover, if \(\) is Lipschitz-continuous, then nested SGDA is guaranteed to converge in such games to recursive SE policies in polynomial time._

**Experiments.** We ran a series of experiments on reach-avoid problems,11 which were designed to assess the efficacy of policies learned in a Stackelberg game formulation as compared to those learned in a simultaneous-move game formulation, assuming complex, i.e., neural, policy parameterizations.

We consider a variant of the two-player differential game introduced in Isaacs , played by two Dubins cars. A Dubins car is a simplified model of a vehicle with a constant forward speed \(\) and a constrained turning radius \(\). We model both the protagonist and antagonist as Dubins cars  moving around a 2-dimensional state space. The target set is a select subset of the state space, while the avoid set, which defines the safe set, is a ball around the antagonist.

We experiment with only the reach distance, not the reach probability, reward function. In all safe states, the reward is actually a penalty, measuring the protagonist's distance to the target set, while a bonus \(\) is awarded upon reaching a target, at which point the game ends. This reward function suffices for our Stackelberg game setup, which enforces the hard constraint that the protagonist cannot move into the avoid set. In our simultaneous-move game setup, we achieve a similar effect by enhancing the aforementioned reward function with a large penalty (\(-\)) whenever the protagonist touches the avoid set. As in the case of reaching the target, touching the avoid set ends the game.

We note that this reach-avoid game is not actually a continuous game, as there is a discontinuity in the reward function when the target is reached. Additionally, it is possible for the antagonist to be "cornered," meaning left with an empty set of feasible actions (in which case the game ends). For these reasons, recursive SE are not guaranteed to exist in our setup.

Our experiments were run on a 7x7 square grid, with the target set \(\) a closed ball of radius 1 centered along the lower edge, and the avoid set \(}\) a closed ball of radius \(0.3\) around the antagonist. We set the bonus (resp. penalty) for reaching the target (resp. avoid set) \(=200\), \(=30^{}\), and \(=0.25\).

Using this experimental setup, we train two agents by playing two games, the Stackelberg and simultaneous-move variants of the reach-avoid game, using nested policy GDA and SGDA, respectively. We evaluate the protagonists' policies to assess their safety and liveness characteristics.

To assess liveness, we ran our agents against an opponent that plays actions sampled uniformly at random. To assess safety, we ran our agents against an opponent that plays actions sampled uniformly at random. To assess safety, we ran our agents against an opponent who chases them, always taking actions that minimize their distance. Table 1 reports the number of wins (W), losses (L), and draws (D), and average game lengths, of 100 games against each opponent. An agent, playing the role of the protagonist, wins when it reaches the target set. A GNE agent loses if it enters \(}\), while a Stackelberg agent loses if it finds itself cornered. The game is a draw if neither player wins or loses within 50 time steps.

We find that the SE agent outperforms the GNE agent by a large margin. The SE agent wins almost all of its games against random, and roughly \(\) of its games against the chaser, while the GNE agent wins only half of its games against random, and none of its games against the chaser. Moreover, even when the SE agent loses or draws, it tends to stay alive longer than the GNE agent. Not only does our Stackelberg approach outperform GNE, it is tractable as well. Our methods thus seem to offer a promising path to further progress solving the myriad of robotic applications of reach-avoid.

 
**Match-up** & **Outcome** & **Mean win length** & **Loss/draw length** \\   GNE vs. random & 47 W, 18 L, 35 D & \(23.23 7.53\) & \(33.71 19.31\) \\  SE vs. random & 95 W, 2 L, 3 D & \(18.16 3.69\) & \(33.0 20.8\) \\  GNE vs. chaser & 0 W, 100 L, 0 D & N/A & \(8.53 1.90\) \\  SE vs. chaser & 63 W, 36 L, 1 D & \(21.63 5.04\) & \(11.06 7.71\) \\  

Table 1: Game results summary for GNE and SE agents.

## 6 Acknowledgments

Denizalp Goktas was supported by a JP Morgan AI fellowship. Arjun Prakash was partially supported by ONR N00014-22-1-2592 and the Quad Fellowship.