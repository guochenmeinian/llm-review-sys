# ANT: Adaptive Noise Schedule for Time Series Diffusion Models

Seunghan Lee, Kibok Lee1, Taeyoung Park1

Department of Statistics and Data Science, Yonsei University

{seunghan9613,kibok,tpark}@yonsei.ac.kr

Equal advising.

###### Abstract

Advances in diffusion models for generative artificial intelligence have recently propagated to the time series (TS) domain, demonstrating state-of-the-art performance on various tasks. However, prior works on TS diffusion models often borrow the framework of existing works proposed in other domains without considering the characteristics of TS data, leading to suboptimal performance. In this work, we propose Adaptive Noise schedule for Time series diffusion models (ANT), which _automatically_ predetermines proper noise schedules for given TS datasets based on their statistics representing non-stationarity. Our intuition is that an optimal noise schedule should satisfy the following desiderata: 1) It linearly reduces the non-stationarity of TS data so that all diffusion steps are equally meaningful, 2) the data is corrupted to the random noise at the final step, and 3) the number of steps is sufficiently large. The proposed method is practical for use in that it eliminates the necessity of finding the optimal noise schedule with a small additional cost to compute the statistics for given datasets, which can be done offline before training. We validate the effectiveness of our method across various tasks, including TS forecasting, refinement, and generation, on datasets from diverse domains. Code is available at this repository: https://github.com/seunghan96/ANT.

## 1 Introduction

Diffusion models have demonstrated outstanding performance in generative tasks across diverse domains, and various methods have been proposed in the time series (TS) domain to address a range of tasks, including forecasting, imputation, and generation . However, recent works primarily focus on determining _which architecture_ to use for a diffusion model, overlooking the useful information from the domain knowledge for other components, e.g., _noise schedule_.

Noise schedules control the noise added to the data across the diffusion process, with the choice of schedule being crucial for performance . Several works have explored the design of noise schedules ; however, they do not consider the characteristics of data, resulting in suboptimal performance, especially in the TS domain. Figure 1 shows the forecasting performance of various models including our method applied to TSDiff  on the M4 dataset . Among the various schedules, our method chooses a cosine schedule for the M4 dataset, yielding a 27.8% gain compared to a linear schedule of TSDiff, which is a common schedule across TS datasets.

This highlights the importance of selecting an appropriate schedule for each dataset.

In this work, we propose Adaptive Noise schedule for Time series diffusion models (ANT), a method for choosing an adaptive noise schedule based on the statistics representing non-stationarity of the TS dataset. These statistics measure the patterns appeared in the TS, with TS from the real world exhibiting high non-stationarity while white noise TS exhibiting low non-stationarity. We argue that a desirable schedule should gradually transform non-stationary TS into stationary ones, in line with

Figure 1: Performance gain by ANT.

prior research suggesting that the noise level at each step should remain consistent . Furthermore, we opt for a schedule that corrupts the TS into random noise at the final step to align the training and inference stages , and a schedule that has a sufficient number of diffusion steps to enhance sample quality . Specifically, we discover that a schedule which decreases the non-stationarity of TS on a _linear scale_ corrupts the TS into random noise gradually, making TS across steps more distinguishable, i.e., making each step equally meaningful. Figure 1(a) shows the forward process using two schedules: the base schedule of TSDiff (w/o ANT) and the schedule proposed by our method (w/ ANT) which aims to decrease the non-stationarity linearly.

Additionally, we investigate the usefulness of diffusion step embedding (DE) in TS diffusion models and argue that it is not necessary when using a linear schedule, as the step information is inherent in the data. We also discover that a non-linear schedule is more robust to total diffusion steps (\(T\)) compared to a linear schedule, yielding consistent performance across various resource constraints on \(T\). The main contributions of this paper are summarized as follows:

* We introduce ANT, an algorithm designed to select an appropriate noise schedule, which is 1) **adaptive** in that the schedule is selected based on the statistics of datasets, 2) **flexible** in that any noise schedule can be a candidate, 3) **model-agnostic** in that it only depends on the statistics of datasets, and 4) **efficient** in that no training is required, as the statistics can be precomputed offline.
* We study the components of diffusion models regarding noise schedules, arguing that diffusion step embedding is unnecessary when using a linear schedule. Additionally, we find that a non-linear schedule exhibits greater robustness to the number of diffusion steps compared to a linear schedule.
* We provide extensive experimental results across various datasets, demonstrating that our method outperforms the baseline in a range of tasks, including TS forecasting, refinement, and generation.

## 2 Background and Related Works

**Denoising diffusion probabilistic model (DDPM).** DDPM  is a well-known diffusion model where input \(^{0}\) is corrupted to a Gaussian noise during the forward process and \(^{0}\) is denoised from \(^{T}\) during the backward process with total \(T\) diffusion steps. For the forward process, \(^{t}\) is corrupted from \(^{t-1}\) iteratively with Gaussian noise of variance \(_{t}\) :

\[q(^{t}^{t-1})=(^ {t};}^{t-1},_{t}), t=1, ,T.\] (1)

Using a property of Gaussian transition kernel, the forward process of multiple steps can be written as \(q(^{t}^{0})=(^{ t};}^{0},(1-_{t})\,)\), where \(_{t}=_{s=1}^{t}_{s}\) and \(_{t}=1-_{t}\). For the backward process, \(^{t-1}\) is denoised from \(^{t}\) by sampling from the following distribution:

\[p_{}(^{t-1}^{t})=( ^{t-1};_{}(^{t},t),_{} (^{t},t)),\] (2)

where \(_{}(^{t},t)\) is defined by a neural network and \(_{}(^{t},t)\) is usually fixed as \(_{t}^{2}\). DDPM formulates this task as a noise estimation problem, where \(_{}\) predicts the noise added to \(^{t}\). With the predicted noise \(_{}(^{t},t)\), \(_{}(^{t},t)\) can be obtained by

\[_{}(^{t},t)=}}( ^{t}-}{_{t}}}_{ }(^{t},t)),\] (3)

and \(_{}\) is optimized using \(_{}=_{t,^{0},}[ \|-_{}(^{t},t)\|^{2}]\) as a training objective.

**Unconditional TS diffusion models.** Unlike most TS diffusion models which are conditional models [26; 38; 18; 30; 1; 6; 29; 40; 19], unconditional TS diffusion models do not use conditions (i.e. information of the observed values \(_{}^{0}\)) as explicit inputs during the training stage. Instead, they utilize them as guidance during the inference stage through a self-guidance mechanism. The backward process of unconditional models with the self-guidance term can be expressed as

\[p_{}(^{t-1}^{t},_{}^{0 })=(^{t-1};_{}(^{t},t )+s_{t}^{2}_{^{t}} p_{}(_ {}^{0}^{t}),_{t}^{2}),\] (4)

where \(s\) is the scale parameter controlling the self-guidance term. As the self-guidance mechanism helps avoid the need for architectural changes depending on the condition or task, we apply our method to TSDiff , which is the SOTA unconditional TS diffusion model. However, our method is not limited to the unconditional model; application to a conditional model, such as CSDI , is also discussed in Appendix M.

**Noise schedules for diffusion models.** The choice of the noise schedule is crucial for the performance of diffusion models , and various methods have been proposed in the computer vision domain [12; 32; 13], including a cosine schedule  to maintain the noise level at each step consistent, and rescaling method  to achieve zero signal-to-noise ratio (SNR) at the terminal step to address the misalignment between the training and inference stages. However, these methods are handcrafted for each dataset, without considering the characteristics of TS datasets. Moreover, recent TS diffusion models overlook the importance of noise schedules, treating them merely as hyperparameters, which motivates us to develop a noise schedule specifically tailored for TS datasets using their characteristics.

**Non-stationarity of TS.** The non-stationarity of a TS indicates how different it is from white noise, and various statistics using autocorrelation (AC) exist to assess this. Integrated autocorrelation time (IAT) , defined as \(_{}=1+2_{k=1}^{}_{k}\) where \(_{k}\) is AC at lag \(k\), quantifies AC over different lags. Additionally, lag-one autocorrelation (Lag1AC) quantifies AC between adjacent observations, and variance of autocorrelation (VarAC) measures the variance of AC over different lags . These statistics, which capture the noisiness of TS, inform the speed at which the TS collapses into noise during the diffusion process. In this paper, we propose a new statistic based on IAT that captures the non-stationarity of TS, accounting for both positive and negative AC.

## 3 Methodology

In this section, we introduce our main contribution, ANT, an adaptive noise schedule for TS diffusion models. ANT proposes a noise schedule resembling an ideal one that gradually diminishes the non-stationarity of TS as the diffusion step progresses. Subsequently, we investigate the properties of commonly used noise schedules and argue that: 1) The diffusion step embedding (DE) is unnecessary for TS diffusion models when a linear schedule is employed, and 2) non-linear schedules are more robust to the change of the number of diffusion steps than linear schedules in terms of the performance and the scale parameter of the self-guidance mechanism of unconditional diffusion models.

### ANT: Adaptive Noise Schedule for TS Diffusion Models

The overall framework of ANT is illustrated in Figure 2, where we aim to find a noise schedule that decreases the non-stationarity of TS on a linear scale. Figure 2a shows how the non-stationarity of a TS gradually decreases with ANT, while it decreases abruptly without ANT. This difference can be explained by computing the non-stationarity curve of a TS for a given noise schedule as shown in Figure 2b. ANT proposes a noise schedule that minimizes the discrepancy between the ideal linear line and the non-stationarity curve of the schedule, whose \(x\)-axis and \(y\)-axis represent the progress of the step (%) and the (normalized) statistics of non-stationarity, respectively. As illustrated in Figure 2c, reduction in discrepancy with ANT yields better performance compared to without ANT.

**Statistics of non-stationarity.** While various statistics can be used to quantify the non-stationarity of TS, we propose the _integrated absolute autocorrelation time (IAAT)_. IAT is a variation of IAT  that takes the absolute value of the autocorrelation to account for positive and negative correlations without canceling them out: \(_{}=1+2_{k=1}^{}|_{k}|\). Although IAAT shows the best performance, we note that ANT is robust across different statistics as shown in Table 8.

**Adaptive schedule.** Figure 3 illustrates the non-stationarity curves of various schedules for two datasets , with line width indicating total diffusion steps. The figure reveals that different

Figure 2: **Overall framework of ANT.** (a) shows that a base schedule (w/o ANT) abruptly corrupts TS at the earlier diffusion step, while a schedule proposed by ANT gradually corrupts it until the final step. (b) visualizes the non-stationarity curves of both schedules and their discrepancy from a linear line, with the schedule that gradually decreases the non-stationarity (w/ ANT) showing lower discrepancy. (c) shows that better performance is achieved as the curves get closer to a linear line.

schedules yield different shapes of curves, and that the same schedule may yield different curve shapes per dataset, highlighting the importance of selecting an adaptive schedule for each dataset.

**ANT score.** To select an adaptive noise schedule for each dataset, ANT takes a dataset \(\) and a schedule \(\) as inputs and returns the ANT score that measures how well \(\) works on \(\), with a lower score indicating a better schedule. The key component of the score is the discrepancy between a linear line \(l^{}\) and the non-stationarity curve \(l_{}\), which is normalized to the range  to account for potential differences in scales across statistics and datasets. On top of that, as shown in Figure 4, ANT considers two other desiderata of noise schedules: 1) _Noise collapse_: A noise schedule should corrupt data to random noise at the final step of the forward process to align the training and inference stages . 2) _Sufficient # of steps_: A noise schedule should have a sufficiently large \(T\), as increasing it generally improves sample quality . To meet the desiderata, ANT opts for a schedule with relatively low non-stationarity at the final step \(l_{}^{(T)}\) than the first step \(l_{}^{(1)}\), and a large \(T\). Then, given \(\) and \(\), the ANT score is defined as:

\[(,)=_{}_{ {noise}}_{}\] (5)

where \(_{}=(l^{},_{})\), \(_{}=1+l_{}^{(T)}/l_{}^{(1)}\), and \(_{}=1+1/T\) are the terms considering linear reduction of non-stationarity, noise collapse, and sufficient steps, respectively, and \(\) is a discrepancy metric between the two lines. Among various metrics, we use the difference in the area under the curve (AUC) using the trapezoidal rule, as it empirically performs the best in our experiments. However, we note that ANT is robust across different metrics \(\), as shown in Table 9. The pseudocode for calculating the ANT score is described in Appendix E.

### Diffusion Step Embedding for TS Diffusion Models

Diffusion models take the \(t\)-th step data (\(^{t}\)) and the step number \(t\) as inputs, where \(t\) is typically encoded in a DE. However, we argue that DE is _not necessary for TS diffusion models employing a linear schedule_, because information about the step is inherent in the data. To validate our claim, we conduct two experiments: 1) A proxy classification task to predict \(t\) with \(^{t}\), and 2) visualization of the embeddings of \(^{t}\) with various \(t\).

**Proxy classification.** We design a proxy classification task that identifies the step number in the forward diffusion pass given a noisy TS. For the task, we sample multiple subseries from M4 , each with varying steps of up to 100 steps. We build a 1D convolutional neural network (CNN) classifier with the architecture of [Conv1D - ReLU - Flatten - Linear], where Conv1D has the kernel size of 3 and 4 output channels. Figure 4(a) shows the results in confusion matrices, indicating that a TS corrupted using a linear schedule retains step information (high accuracy), whereas a TS corrupted using a non-linear schedule does not (low accuracy), making DE redundant for a model employing a linear schedule. We argue that this is due to the variance of noise for each step (\(_{t}\)), as a non-linear schedule adds most of the noise at the end of the steps, making a TS less distinguishable for most of the steps. In contrast, a linear schedule that gradually increases \(_{t}\) makes a TS more distinguishable across steps, allowing us to eliminate the DE in the model.

Figure 4: Desiderata of noise schedules.

Figure 5: Proxy task classification & t-SNE visualization.

**t-SNE visualization.** Figure 4(b) depicts the t-SNE visualizations of CNN features extracted from the proxy classification model, representing a noisy TS at various steps corrupted by a linear schedule. The results show directional point movement across all output channels as the step progresses, implying that diffusion step information is inherently present in the TS. As discussed in Appendix P, t-SNE visualizations employing a non-linear schedule also show directional point movement but with a common pattern across all output channels, indicating limited step information compared to a linear schedule. Based on these observations, we remove the DE from the diffusion models when employing a linear schedule. In experiments, we observe that DE is useful for models with non-linear schedules, but not for models with linear schedules, as shown in Table 6.

### Robustness of Non-linear Schedules

In diffusion models, the number of steps \(T\) determines the efficiency of overall process and sample quality. We argue that non-linear schedules are more robust to \(T\) than linear schedules in two aspects: 1) performance and 2) the optimal scale parameter \(s\) controlling the self-guidance in Eq. (4).

**Robustness of performance to \(T\).** Figure 5(a) depicts the coefficient of variation of continuous ranked probability score (CRPS)  of forecasting task with various \(T\), illustrating that the performance is robust to \(T\) when employing a non-linear schedule. This is supported by Figure 5(b), which shows the non-stationarity curves of various schedules, with each color representing a different dataset. The figure indicates that the shape of the curve, which determines the ANT score, remains consistent regardless of \(T\) when employing a non-linear schedule. It is important to note that the ANT score is highly correlated with the performance, as discussed in Section 4.4.

**Robustness of optimal \(s\) to \(T\).** Figure 6(a) shows that the optimal \(s\) is sensitive to \(T\) when employing linear schedules, whereas robust when using non-linear schedules. We discover that this robustness stems from the posterior variance (\(_{t}^{2}\)) of a schedule, which affects the self-guidance term in Eq. (4). Figure 6(b) illustrates the sum of the posterior variances (\(_{t=1}^{T}_{t}^{2}\)) of various schedules, indicating that this sum is more robust to the choice of \(T\) for non-linear schedules compared to linear schedules.

## 4 Experiments

**Experimental setup.** We demonstrate the effectiveness of our proposed method on three tasks: TS forecasting, refinement, and generation. For evaluation, we adopt ANT to TSDiff , where we use the official code to replicate the results. All experimental protocols adhere to that of TSDiff, where we utilize the CRPS  to assess the quality of probabilistic forecasts. We conduct experiments on eight TS datasets from different domains - Solar , Electricity , Traffic , Exchange , M4 , UberTLC , KDDCup , and Wikipedia . We present the mean and standard deviations calculated from three independent trials.

Figure 6: **Robustness of performance to \(T\). (a) shows that CRPS of non-linear schedules are robust to \(T\), supported by (b) which shows the robustness of non-stationarity curves to \(T\).**

Figure 7: **Robustness of optimal \(s\) to \(T\). (a) shows that optimal \(s\) of non-linear schedules are robust to \(T\), supported by (b) which shows the robustness of sum of the posterior variances to \(T\).**

[MISSING_PAGE_FAIL:6]

### Time Series Generation

To assess the generation quality of TS diffusion models with ANT, we evaluate the forecasting performance of downstream forecasters trained on the synthetic samples generated by various models. For the baseline methods, we employ TimeGAN , TimeVAE , and TSDiff . For the downstream models, we employ a simple linear (ridge) regression model, DeepAR , and Transformer . The results are shown in Table 4, indicating that ANT improves the performance of TSDiff, leading to SOTA performance in most cases. Further details regarding the generation task with Electricity are discussed in Section O.

### Analysis

**Proposed noise schedule.** Table 5 shows the schedules proposed by ANT and the schedules that yield the best performance (oracle) among all candidate schedules, along with the forecasting performance obtained when using these schedules. The result indicates that the schedules selected by ANT are mostly consistent with the oracle, and even for an exceptional case that ANT fails to select the best schedule, e.g., on Traffic, the performance gap from the oracle is not significant.

**Necessity of DE.** Table 6 displays the forecasting results of TSDiff applied with ANT, both with and without the use of DE across eight datasets. The result indicates that for all datasets except for Traffic, models trained with a linear schedule perform better when trained without DE, while models trained with a non-linear schedule perform better when trained with DE. Based on these findings, we do not use DE when employing a linear schedule.

**Ablation study.** To show the effectiveness of each component in ANT, we conduct an ablation study in Table 7. The result demonstrates that enforcing the schedule to gradually drop the non-stationarity is the most important component (\(_{}\)), and ensuring noise collapse (\(_{}\)) and sufficient steps (\(_{}\)) also help ANT achieve oracle for three datasets [23; 17; 3] when combined together. Figure 8 shows the correlation between the ANT score and CRPS on forcasting tasks via a scatter plot of candidate schedules and a bar plot of schedules selected in Table 7, implying that there is a strong correlation between the ANT score and CRPS, and the components in ANT strengthen the correlation further.

   &  &  &  \\    & & \(f\) & \(\) & \(T\) & w/o DE & \(\)/ DE &  &  \\  UberTLC &  & - & 20 & **0.163** & 0.160 & 0.172 \\ KDCDup & & - & - & **0.325** & 0.340 & 0.335 \\ Traffic & & - & 50 & **0.101** & **0.098** & 0.105 \\ Solar & - & - & 100 & **0.326** & 0.399 & 0.399 \\  Exchange & & 0.5 & 50 & 0.010 & **0.009** & 0.012 \\ Electricity & & 2.0 & 75 & 0.050 & **0.047** & 0.049 \\ Wikipedia & & 2.0 & 75 & **0.296** & **0.206** & 0.221 \\ M4 & 1.0 & 100 & 0.052 & **0.026** & 0.036 \\  

Table 6: TS forecasting w/ & w/o DE.

   &  &  &  \\    & & & & & & & & \\    & & \(f\) & \(\) & \(T\) & w/o DE & \(\)/ DE &  &  \\  UberTLC &  & - & 20 & **0.163** & 0.160 & 0.172 \\ KDCDup & & - & - & **0.325** & 0.340 & 0.335 \\ Traffic & & - & 50 & 0.101 & **0.098** & 0.105 \\ Solar & - & - & 100 & **0.326** & 0.399 & 0.399 \\  Exchange & & 0.5 & 50 & 0.010 & **0.009** & 0.012 \\ Electricity & & 2.0 & 75 & 0.050 & **0.047** & 0.049 \\ Wikipedia & & 2.0 & 75 & **0.296** & **0.206** & 0.221 \\ M4 & 1.0 & 100 & 0.052 & **0.026** & 0.036 \\  

Table 5: Schedules proposed by ANT and forecasting results with them.

**Robustness to statistics.** To see if ANT is sensitive to the choice of statistics of non-stationarity, we compare IAAT with other statistics including Lag1AC and VarAC. Table 8 shows the average CRPS across eight datasets in forecasting tasks, demonstrating that ANT with different statistics still outperforms the model without ANT, while IAAT performs best. We conjecture that this is because IAAT considers AC at all time lags, whereas Lag1AC only considers a single lag and VarAC focuses on the variance of AC rather than their exact values.

**Robustness to \(\).** Among the various metrics that can be employed for \(\) to measure the discrepancy between the two lines, we compare AUC with the mean-squared error (MSE), mean-absolute error (MAE), Pearson correlation (Corr.), and R-squared (\(R^{2}\)). Table 9 presents the average CRPS of eight datasets on the forecasting task, demonstrating the robustness of our method to the choice of metric.

**Generation process.** Figure 9 illustrates the generation process of a TSDiff model trained on M4 with and without ANT, where the color changes from black to red as the step progresses. The figure on the left shows the generated TS for each step, while the one on the right shows the non-stationarity of each step in terms of IAAT. As shown in Figure 9, ANT indeed makes the generation process linearly increase the non-stationarity.

**Effect of \(T\) on performance.** While it was believed that the larger the \(T\), the better the performance in diffusion models , we argue that this is not always true in the TS domain. Table 10 shows the oracle performance for each \(T\) among candidate schedules on forecasting tasks, indicating that the best performance is not always achieved with the largest \(T\), e.g., the best performance is achieved with \(T=20\) on UberTLC . Furthermore, as shown in Figure 10, ANT often outperforms the baseline method TSDiff with smaller \(T\), achieving the oracle performance in most cases.

**Efficiency of ANT.** ANT offers efficiency in two aspects: 1) Statistics of non-stationarity can be precomputed offline with a small additional cost, and 2) the proposed schedule with \(T\) smaller than that of the base schedule results in faster inference. First, Table (a)a displays the time spent calculating IAAT using the entire train dataset of Traffic  with schedules of five different values for \(T\). The results indicate that the computational cost is negligible compared to the training time shown in Table (b)b, making ANT practical for use. Second, Table (b)b shows the training and inference time on Traffic, where we report the training time with 1000 epochs and the inference time for a TS data averaged on the test dataset. The results demonstrate that we can save up to 50% of the inference time and reduce the training time by eliminating the DE.

    &  &  &  &  &  &  &  &  \\  \(_{}\) & & & & & & & & & & & \\  ✓ & & & Lin(75) & Cos(10.1,0) & Lin(20) & Cos(50.**0.5) & Cos(10.0,5) & Lin(20) & Lin(50) & Cos(75.2,0) \\ ✓ & ✓ & & Lin(75) & Cos(10.1,0) & Lin(20) & Cos(50.**0.5) & Cos(10.0,5) & Lin(20) & Lin(50) & Cos(75.2,0) \\ ✓ & ✓ & ✓ & Lin(75) & Cos(10.1,0) & Lin(20) & Cos(50.5) & Cos(10.0,5) & Lin(20) & Lin(50) & Cos(75.2,0) \\ ✓ & ✓ & ✓ & & **Lin(100)** & **Cost(75.2,0)** & Lin(50) & Cos(50.0,5) & Cos(100.1,0) & Lin(20) & Lin(50) & Cos(75.2,0) \\   & & & & & & & & & & \\   

Table 7: **Ablation study.** The table shows the schedules proposed by ANT when using some or all components of the ANT score.

    & Avg. \\  w/o ANT & 0.166 \\   & VarAC & 0.158 \\  & Lag1AC & 0.151 \\  & IAAT & **0.150** \\   & **0.150** \\   

Table 8: Robustness to statistics.

Figure 9: Generation process.

Furthermore, ANT offers a performance-computational cost trade-off in that it can be optimized with a constraint on \(T\) by selecting a schedule among candidates with \(T\) no larger than \(T^{*}\) (\(T T^{*}\)). Table (c)c shows the forecasting results with a constraint of \(T^{*}=50\), achieving better performance with no more than half of the steps of the base schedule. Further analyses regarding computational time and forecasting results under constraints on \(T\) are discussed in Appendix N and J, respectively.

**Flexibility of ANT.** As ANT does not require direct access to the noise schedule but the TS data corrupted by the schedule, any noise schedule can be a candidate. To confirm if ANT can judge the usefulness of non-trivial noise schedules other than the functions described in Table 1, we experiment an ensemble of cosine functions (\(^{*}\)), illustrated in Figure 11. Table 12 shows the results on two datasets  that achieve lower ANT scores with \(^{*}\), indicating the potential for a better schedule to be selected by relaxing structural constraints, or even without any functional form \(f\).

**Comparison with other schedules.** To demonstrate the effectiveness of ANT compared to noise schedules from other domains, we compare ANT with a cosine schedule (Cos)  and a schedule that enforces zero terminal SNR (Zero) , applied to TSDiff. Table 13 shows the average CRPS across eight datasets on forecasting tasks, where ANT outperforms the others. The results highlight the importance of a schedule being adaptive to the given TS dataset and considering the corruption speed.

## 5 Conclusion

In this work, we introduce ANT, a method designed to select an adaptive noise schedule for TS diffusion models using the statistics of non-stationarity. ANT is practical in that it predetermines a noise schedule before training, as the statistics can be precomputed offline with minimal additional cost. The proposed ANT is a simple yet effective method for TS diffusion models across various tasks, e.g., ANT improves the SOTA method TSDiff by 9.5% on average across eight forecasting tasks, highlighting the importance of adaptive schedules tailored to the characteristics of TS datasets.

**Limitations and future work.** We note that our contribution is not in finding the optimal schedule but in proposing a criterion for efficiently selecting a better noise schedule from a set of candidates based on the dataset characteristics. While our experiments search for an appropriate schedule from 35 candidates derived from linear, cosine, and sigmoid functions, any schedule can be considered a candidate for ANT, and incorporating additional schedules could lead to further performance gains at the expense of longer search times. We leave developing algorithms to determine the optimal schedule parameters based on our proposed criterion for future work. We hope that our research motivates further research across various domains to incorporate domain knowledge in the design of machine learning models, extending beyond noise schedules for diffusion models.

    &  &  \\  \(T\) & 10 & 20 & 50 & 75 & 100 \\  Time & 11.1 & 22.6 & 60.2 & 93.2 & 120.0 & \\    
    &  \\  w/o ANT & w/ ANT & w/ ANT & w/ ANT \\  w/ ANT & w/ conv. & \( 50\) & 0.157 \\ w/o conv. & \( 100\) & **0.150** \\   

Table 11: Efficiency of ANT.

    &  &  \\   & w/o Cos\({}^{*}\) & w/ Cos\({}^{*}\) & w/o Cos\({}^{*}\) & w/ Cos\({}^{*}\) \\  Schedule & Lin(20) & Cos\({}^{*}\)(20.0,2) & Lin(50) & Cos\({}^{*}\)(20,2.0) \\  ANT score & 0.054 & **0.053** & 0.088 & **0.050** \\ CRPS & 0.163 & **0.161** & 0.101 & **0.099** \\   

Table 12: Flexibility of ANT.

Figure 10: ANT vs. Oracle.

    &  &  \\   & w/o Cos\({}^{*}\) & w/ Cos\({}^{*}\) & w/o Cos\({}^{*}\) & w/ Cos\({}^{*}\) \\  Schedule & Lin(20) & Cos\({}^{*}\)(20.0,2) & Lin(50) & Cos\({}^{*}\)(20,2.0) \\  ANT score & 0.054 & **0.053** & 0.088 & **0.050** \\ CRPS & 0.163 & **0.161** & 0.101 & **0.099** & **0.099** \\   

Table 12: Flexibility of ANT.