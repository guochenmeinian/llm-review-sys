# Mitigating the Effect of Incidental Correlations on Part-based Learning

Gaurav Bhatt\({}^{13}\), Deepayan Das\({}^{2}\), Leonid Sigal\({}^{13}\), Vineeth N Balasubramanian\({}^{2}\)

\({}^{1}\)The University of British Columbia, \({}^{2}\)Indian Institute of Technology Hyderabad

\({}^{3}\) The Vector Institute, Canada

First author; **Email**: gauravbhatt.cs.iitr@gmail.com

###### Abstract

Intelligent systems possess a crucial characteristic of breaking complicated problems into smaller reusable components or parts and adjusting to new tasks using these part representations. However, current part-learners encounter difficulties in dealing with incidental correlations resulting from the limited observations of objects that may appear only in specific arrangements or with specific backgrounds. These incidental correlations may have a detrimental impact on the generalization and interpretability of learned part representations. This study asserts that part-based representations could be more interpretable and generalize better with limited data, employing two innovative regularization methods. The first regularization separates foreground and background information's generative process via a unique mixture-of-parts formulation. Structural constraints are imposed on the parts using a weakly-supervised loss, guaranteeing that the mixture-of-parts for foreground and background entails soft, object-agnostic masks. The second regularization assumes the form of a distillation loss, ensuring the invariance of the learned parts to the incidental background correlations. Furthermore, we incorporate sparse and orthogonal constraints to facilitate learning high-quality part representations. By reducing the impact of incidental background correlations on the learned parts, we exhibit state-of-the-art (SoTA) performance on few-shot learning tasks on benchmark datasets, including MiniImagenet, TieredImageNet, and FC100. We also demonstrate that the part-based representations acquired through our approach generalize better than existing techniques, even under domain shifts of the background and common data corruption on the ImageNet-9 dataset. The implementation is available on GitHub: https://github.com/GauravBh1010tt/DPViT.git

## 1 Introduction

Many datasets demonstrate a structural similarity by exhibiting "parts" or factors that reflect the underlying properties of the data . Humans are efficient learners who represent objects based on their various traits or parts, such as a bird's morphology, color, and habitat characteristics. Part-based methods learn these explicit features from the data in addition to convolution and attention-based approaches (which only learn the internal representations), making them more expressive . Most existing part-based methods focus on the unsupervised discovery of parts by modeling spatial configurations , while others use part localization supervision in terms of attribute vectors  or bounding boxes . Part-based methods come with a learnable part dictionary that provides a direct means of data abstraction and is effective in limited data scenarios, such as few-shot learning. Furthermore, the parts can be combined into hierarchical representations to form more significant components of the objectdescription [50; 55]. Part-based learning methods offer advantages in terms of interpretability and generalization, particularly in safety-critical domains like healthcare, aviation and aerospace industry, transportation systems (e.g., railways, highways), emergency response, and disaster management. These fields often face challenges in collecting large training samples, making part-based learning methods valuable. Furthermore, the ability to explain decisions becomes crucial in these contexts.

Various studies have indicated that correlations between image background and labels can introduce biases during machine learning model training [29; 32; 33; 34; 45; 46; 53; 59]. These correlations exist because specific background configurations create shortcuts for the model during training [32; 53]. While background information is crucial for decision-making, imbalanced background configurations can create unintended correlations, leading to undesirable outcomes. These correlations negatively impact the interpretability and generalization of part-based learners. For instance, let's consider a scenario where a laptop, a charger, and an iPod are on a table. In one case, let's examine the situation without any context or background information. Without background information, the model may struggle to understand the purpose and significance of components or parts such as the laptop, charger, and iPod on a table. It could fail to differentiate between these objects or grasp their functionalities, resulting in a lack of recognition and understanding. Conversely, suppose the model is predominantly trained with examples of these items on tables. In that case, it may overly focus on the background elements, such as the table itself, disregarding the individual entities. Thus it becomes essential to handle incidental correlations of image background to achieve a balanced understanding. Existing part-based approaches fail to handle incidental correlations that arise due to specific background signals dominating the training data (analogous to Figure 1(b)), thereby hampering their interpretability and generalization to limited data scenarios.

Having high-quality part representations is essential for achieving proficiency in part-based learning. In this context, quality pertains to the sparsity and diversity of the learned parts. Sparsity ensures that only a few parts are responsible for a given image, as images comprise a small subset of parts. Conversely, diversity in part representations prevents the parts from converging into a single representation and facilitates each part's learning of a unique data representation. Although incidental correlations can negatively affect learned parts' quality, the quality of part-based methods is a significant challenge that all part learners face. While previous studies have addressed the issue of part quality [50; 52], their solutions do not assure the sparsity and diversity of the learned parts, thereby failing to guarantee high-quality parts.

To solve the aforementioned challenges, we introduce the Disentangled Part-based Vision Transformer (DPViT), which is trained to be resilient to the incidental correlations of image backgrounds and ensures high-quality part representations. We propose a disentangled pre-training phase, which separates the generative process of the foreground and background information through a unique mixture-of-parts formulation. We impose structural constraints on the parts to ensure that the mixture-of-parts for foreground and background includes soft, object-agnostic masks without requiring direct supervision of part localization. The parts are learned to be invariant to incidental correlations using a self-supervised distillation fine-tune phase. To address the issue of the quality of learned parts, we impose sparse and spectral penalties on the part matrix to guarantee the high quality of learned part representations. We include an assessment of the sparse and spectral norms of the part matrix as a quantitative indicator of the learned parts' quality. Finally, we evaluate the effectiveness of our method on benchmark few-shot datasets, including MiniImagenet , TieredImageNet ,

Figure 1: **Impact of incidental correlations on the interpretability of part learners. We visualize the attention maps projected by the learned part dictionaries. Figure 1(b) illustrates the ViT-S backbone featuring a learnable part dictionary. However, it encounters difficulties in correctly identifying significant elements like the laptop, giving more attention to the background instead. In contrast, the proposed DPViT method successfully detects the most crucial parts of the image even in the presence of incidental correlations.**

and FC100 . To demonstrate the robustness of our proposed method to incidental correlations of backgrounds and common data corruptions, we use the benchmark ImageNet-9 dataset .

Our key contributions can be summarized as follows:

* We propose regularization techniques to disentangle the generative process of foreground and background information through a mixture-of-parts formulation. Additionally, we employ a self-supervised distillation regularization to ensure that the learned parts remain invariant to incidental correlations of the image background.
* We ensure the high quality of learned parts by employing both sparsity and spectral orthogonal constraints over the part matrix. These constraints prevent the parts from degenerating and encourage a diverse range of part representations.
* Apart from our evaluation of standard few-shot benchmark datasets, we also analyze the impact of incidental correlations of background and typical data distortions by utilizing the benchmark ImageNet-9 dataset .

## 2 Related Work

**Part-based learning.** The advantages of learning part-based representations have been extensively researched in image recognition tasks [15; 18; 38; 43; 49; 52; 55; 56]. Earlier methods attempted to learn parts by defining a stochastic generative process [19; 43]. Part-based methods have been broadly classified into unsupervised and supervised categories. Unsupervised methods concentrate on learning the spatial relationship between parts by using part dictionaries without the supervision of part localization [14; 15; 20; 21; 28; 49; 52; 54; 61]. In contrast, supervised part-based methods rely on the supervision of part localization through attribute vectors [16; 27; 41; 50] or part bounding box information . In the literature, parts are also referred to as concepts when supervision about part localization is involved [7; 41].

Discovering parts in an unsupervised way is a more challenging scenario that is applicable to most practical problems. Part dictionaries help data abstraction and are responsible for learning implicit and explicit data representations. For example,  clustered DCNN features using part-based dictionaries, while  introduced a generative dictionary-based model to learn parts from data. Similarly,  uses part-based dictionaries and an attention network to understand part representations. The ConstellationNet  and CORL  are some of the current methods from the constellation family [14; 61], and use dictionary-based part-prototypes for unsupervised discovery of parts from the data. Our approach also belongs to this category, as we only assume the part structure without requiring any supervision of part localization.

**Incidental correlations of image background**. Image backgrounds have been shown to affect a machine learning model's predictions, and at times the models learn by utilizing the incidental correlations between an image background and the class labels [4; 32; 42; 45; 46; 53; 58; 59]. To mitigate this issue, background researchers have used augmentation methods by altering background signals and using these samples during the training [32; 53; 58].  performed an empirical study on the effect of image background on in-domain classification. They introduce several variants of background augmentations to reduce a model's reliance on the background. Similarly,  uses saliency maps of the image foreground to generate augmented samples to reduce the effect of the image background. Recently,  showed the effectiveness of background augmentation techniques for minimizing the effect of incidental correlations on few-shot learning.

Unlike these methods, our approach does not depend on background augmentations but instead learns the process of generating foreground and background parts that are disentangled. Furthermore, our proposed approach is not sensitive to the quality of foreground extraction and can operate with limited supervision of weak foreground masks.

**Few-shot learning and Vision Transformers**. In recent years, few-shot learning (FSL) has become the standard approach to evaluate machine learning models' generalization ability on limited data [1; 2; 5; 13; 24; 36; 44; 47]. Vision transformers (ViT)  have demonstrated superior performance on FSL tasks [10; 22; 23; 30; 48; 51], and self-supervised distillation has emerged as a popular training strategy for these models [8; 17; 22; 30; 51]. A recent trend involves a two-stage procedure where models are pretrained via self-supervision before fine-tuning via supervised learning [8; 17; 22; 30; 60]. For example,  leverages self-supervised training with iBOT  as a pretext task, followed by inner loop token importance reweighting for supervised fine-tuning. HCTransformer  uses attribute surrogates learning and spectral tokens pooling for pre-training vision transformersand performs fine-tuning using cascaded student-teacher distillation to improve data efficiency hierarchically. SMKD  uses iBOT pre-training and masked image modeling during fine-tuning to distill knowledge from the masked image regions.

Our approach employs a two-stage self-supervised training strategy of vision transformers akin to [22; 30; 60]. However, unlike existing methods that focus on generalization in few-shot learning, our primary objective is to learn part representations that are invariant to incidental correlations of the image background. Our training procedure is designed to facilitate learning disentangled and invariant part representations, which is impossible through existing two-stage self-supervised pipelines alone.

## 3 Problem Formulation and Preliminaries

In few-shot classification, the aim is to take a model trained on a dataset of samples from seen classes \(^{seen}\) with abundant annotated data, and transfer/adopt this model to classify a set of samples from a disjoint set of unseen/novel classes \(^{novel}\) with limited labeled data. Formally, let \(^{seen}=\{(,y)\}\), where \(\) corresponds to an image and \(y^{seen}\) corresponds to the label among the set of seen classes. We also assume that during training, we have limited supervision of class-agnostic foreground-background mask (\(_{f}\),\(_{b}\)) for regularization during training, which can be easily obtained by any weak foreground extractor as a preprocessing step (following ). Please note that no mask information is required for \(^{novel}\) at inference.

We follow the work of [9; 60] on self-supervised training of ViTs to design our pretrain phase. During training, we apply random data augmentations to generate multiple views of the a given image \(x^{v}^{seen}\). These views are then fed into both the teacher and student networks. Our student network, with parameters \(_{s}\), includes a ViT backbone encoder and a projection head \(_{s}\) that outputs a probability distribution over K classes. The ViT backbone generates a \([cls]\) token, which is then passed through the projection head. The teacher network, with parameters \(_{t}\), is updated using Exponentially Moving Average (EMA) and serves to distill its knowledge to the student by minimizing the cross-entropy loss over the categorical distributions produced by their respective projection heads.

\[_{cls}=_{(,y)^{seen}}_{ce}(_{}^{t}(^{t})),_{}^{s}( _{}^{s}(^{2}))).\] (1)

For inference, we use the standard \(M\)-way, \(N\)-shot classification by forming _tasks_ (\(\)), each comprising of _support set_ (\(\)) and _query set_ (\(\)), constructed from \(^{novel}\). Specifically, a support set consists of \(M N\) images; \(N\) random images from each of \(M\) classes randomly chosen from \(^{novel}\). The query set consists of a disjoint set of images, to be classified, from the same \(M\) classes. Following the setup of , we form the class prototypes (\(_{m}\)) using samples from \(\). The class prototypes and learned feature extractor (\(_{}\)) are used to infer the class label \(\) for an unseen sample \(^{q}\) using a distance metric \(d\).

\[=*{arg\,max}_{m}\,d(_{}(^{q}), _{m});\,\,_{m}=_{(,y_{m}) }_{}().\] (2)

## 4 Proposed Methodology

Given an input sample \(^{H W C}\), and a patch size \(f\), we extract flattened 2D patches \(_{}^{N(F^{2} C)}\), where \(N\) is the number of patches generated and \((F,F)\) is the resolution of each image patch. Similar to a standard ViT architecture , we prepend a learnable \([class]\) token and positional embeddings to retain the positional information. The flattened patches are passed to multi-head self attention layers and MLP blocks to generate a feature vector \(z_{p}=MSA(x_{f})\).

Next, we define the parts as part-based dictionaries \(=\{_{k}^{F^{2} C}\}_{k=1}^{K}\), where \(_{k}\) denotes the part-vector for the part indexed as \(k\). The _part-matrix_ (\(\)) is initialized randomly and is considered a trainable parameter of the architecture. Note that the dimension of each part-vector is equal to the dimension of flattened 2D patches, which is \(F^{2} C\). For each part \(_{k}\), we compute a distance map \(^{k}^{N}\) where each element in the distance map is computed by taking dot-product between the part \(_{k}\) and all the \(N\) patches: \(^{k}=_{}_{k}\).

Using the distance maps \(^{N K}\), we introduce a multi-head cross-attention mechanism and compute the feature vector: \(z_{d}=MCA(F_{}())\), where \(F_{}\) is an MLP layer which upsamples \(:K F^{2} C\). The cross-attention layer shares a similar design to self-attention layers; the only difference is the dimensions of input distance maps. The cross-attention helps contextualize information across part-dictionary and the spatial image regions and provides complementary properties to \(MSA\) layers. (Please refer to Appendix for experiments on complementary properties of \(MSA\) and \(MCA\)).

Finally, we add the output feature vectors of \(MSA\) and \(MCA\) to form the feature extractor \(_{}\) defined in Eqn 1:

\[_{}=[z_{p} z_{d}]\] (3)

**Disentanglement of foreground-background space using mixture-of-parts**. We start by dividing the parts-matrix \(^{K F^{2} C}\) into two disjoint sets: foreground set \(_{}^{n_{f} C}\) and background set \(_{}^{n_{b} C}\), such that \(K=n_{f}+n_{b}\).

Next, we construct latent variables to aggregate the foreground and background information using a mixture-of-parts formulation over the computed distance maps \(\):

\[L_{F}=_{k n_{f}}_{k}^{k}+_{f};L_{B}=_{k n _{b}}_{k}^{k}+_{b}\] (4)

where, \(_{k}\) and \(_{k}\) are the learnable weights given to the \(k^{th}\) part-vector in the corresponding mixture, whereas \(_{f}\) and \(_{b}\) are Gaussian noises sampled from \((0,1)\). The Gaussian noise is added to the latent codes to ensure that mixture-of-parts are robust to common data distortions. Please note that the purpose of Gaussian noise is not to induce variability in the latent codes, as foreground information for a given image is deterministic.

Finally, our disentanglement regularization takes the form of an alignment loss between the latent codes and the class-agnostic foreground-background masks:

\[_{mix}=||(L_{F})-_{f}||_{2}+||(L_ {B})-_{b}||_{2}\] (5)

where, \((L)\) is the bilinear interpolation of a given latent code \(L\) to the same size as \(\).

During the architectural design phase, we employ distinct components (\(\)) for each encoder block. Consequently, \(z_{p}\) and \(z_{d}\) are calculated in an iterative manner and subsequently transmitted to the subsequent encoder block. Regarding the computation of \(L_{mix}\), we utilize the distance maps \(\) from the concluding encoder block. While it's feasible to calculate \(L_{mix}\) iteratively for each block and then aggregate them for a final \(L_{mix}\) computation, our observations indicate that this approach amplifies computational expenses and results in performance deterioration. As a result, we opt to compute \(L_{mix}\) using the ultimate encoder block.

**Learning high-quality part representations**. A problem with minimizing the mixture objective defined in Eqn 5 is that it may cause the degeneration of parts, thereby making the part representations less diverse. One solution is to enforce orthogonality on the matrix \(^{m n}\) by minimizing \(||^{T}-||\), similar to . However, the solution will result in a biased estimate as \(m<n\); that is, the number of

Figure 2: **Overview of proposed architecture - DPViT. We employ a learnable part dictionary to generate a formulation incorporating foreground and background information. The spatial distance maps, computed by the part dictionary, are utilized to determine the mixture of latent codes for foreground and background. Our transformer encoder comprises multi-head self-attention (MSA) and multi-head cross-attention (MCA) layers. The MSA layer takes embedded patches as input, while the MCA layers utilize the distance maps as input.**

parts (\(K\)) is always less than the dimensionality of parts (\(F^{2} C\)). In our experiments, we observed that increasing \(K\) beyond a certain threshold degrades the performance as computational complexity increases, and is consistent with the findings in . (Please refer to our Appendix section for experiments on the different values of \(K\)). To minimize the degeneration of parts, we design our quality assurance regularization by minimizing the spectral norm of \(^{T}-\), and by adding \(L_{1}\) sparse penalty on the part-matrix \(\). The spectral norm of \(^{T}-\) has been shown to work with over-complete (\(m<n\)) and under-complete matrices (\(m n\)) .

\[_{Q}(_{s},_{o})=_{s}||||_{1}+ _{o}}}^{T}-+}}^{T}- \] (6)

where \(\) is the identity matrix, \(_{s}\) and \(_{o}\) are the regularization coefficients for sparsity and orthogonality constraints. \(()\) is the spectral norm of the matrix \(\) which is computed using the scalable power iterative method described in .

**Disentangled Pretraining Objective**. We pretrain DPViT using the following loss function:

\[_{PT}=_{cls}_{cls}+_{mix}_{mix }+_{Q}(_{s},_{o})\] (7)

where \(_{cls},_{mix}\), \(_{s}\), and \(_{o}\) are the weights given to each loss term and are tuned on the validation set.

### Invariant fine-tuning

In the pretrain phase, our approach learns part representations that are disentangled and diverse, but it does not achieve invariance to the incidental correlations of image background. During the fine-tuning stage, we utilize the learned foreground latent code to extract the relevant foreground information from a given image \(x\): \(x_{f}=x(L_{F})\), where \(\) denotes the Hadamard product. The teacher network receives the original image, while the student network receives the foreground-only image. By distilling knowledge between the \([class]\) tokens and foreground latent codes \(L_{F}\) of the student and teacher networks, we achieve invariance to the incidental correlations of image background.

\[_{cls}^{inv}=_{ce}(_{}^{t}(_{ }^{t}(x)),_{}^{s}(_{}^{s}(x_{f}))); _{p}^{inv}=_{ce}(L_{F}^{t}(x),L_{F}^{s}(x_{f}))\] (8)

The two proposed invariant regularizations serve distinct purposes: \(_{cls}^{inv}\) encourages the model to classify images independently of the background, while \(_{p}^{inv}\) ensures that the latent foreground code captures relevant foreground information even when the background is absent, making the learned parts invariant to the incidental correlations.

**Invariant Fine-tuning Objective**. Finally, our fine-tuning objective is given as :

\[_{FT}=_{cls}_{cls}+_{cls}^{inv}_{cls}^{inv}+_{p}^{inv}_{p}^{inv}\] (9)

where \(_{cls},_{cls}^{inv}\), and \(_{p}^{inv}\) are the weights given to each loss term and are tuned on the validation set after pretraining.

## 5 Experiments

We evaluate the proposed approach on four datasets: MiniImageNet , TieredImageNet , FC100 , and ImageNet-9 . The MiniImageNet, TieredImageNet, and FC100 are generally used as benchmark datasets for few-shot learning. For MiniImageNet, we use the data split proposed in , where the data samples are split into 64, 16, and 20 for training, validation, and testing, respectively. The TieredImageNet  contains 608 classes divided into 351, 97, and 160 for meta-training, meta-validation, and meta-testing. On the other hand, FC100  is a smaller resolution dataset (32 \(\) 32) that contains 100 classes with class split as 60, 20, and 20.

To investigate the impact of background signals and data corruption on classifier performance, researchers introduced ImageNet-9 (IN-9L) . IN-9L is a subset of ImageNet comprising nine coarse-grained classes: dog, bird, vehicle, reptile, carnivore, insect, instrument, primate, and fish. Within these super-classes, there are 370 fine-grained classes, with a training set of 183,006 samples. The authors of  created different test splits by modifying background signals, resulting in 4050

Figure 3: Invariant fine-tuning of DPViT via distillation framework.

samples per split to evaluate various classifiers. We use three of these test splits for our evaluation: Original (with no changes to the background), M-SAME (altering the background from the same class), and M-RAND (altering the background from a different class). Additionally,  introduced a metric called BG-GAP to assess the tendency of classifiers to rely on background signal, which is measured as the difference in performance between M-SAME and M-RAND.

We use a ViT-S backbone for all our experiments and follow the same pipeline in iBOT  for pre-training, keeping most hyperparameters unchanged. We use a batch size of 480 with a learning rate of 0.0005 decayed alongside a cosine schedule and pre-train DPViT for 500 epochs for all four datasets. Fine-tuning is carried out on the same train data for 50 epochs utilizing the class labels (similar to ). The DPViT architecture has \(K\)=64 and \(n_{f}\)=40 for all of our experiments. The pre-training and fine-tuning are carried out on 4 A40 GPUs. More details related to the architectures and optimization are provided in Appendix.

We start our experimental study by examining how incidental correlation affects part-learners' interpretability and quality of learned parts. This study is conducted on the MiniImageNet dataset. We then present experimental results on few-shot learning on the MiniImageNet, TieredImageNet, and FC100 datasets. Lastly, we provide a quantitative analysis of the influence of incidental background correlations on the various test splits in the ImageNet-9 dataset, followed by ablation studies and a discussion.

### How do incidental correlations affect the interpretability of part learners?

To examine how incidental correlations impact various components, we solely employ the \(_{cls}\) loss to train DPViT on the MiniImageNet dataset. (Note that \(_{cls}\) is equivalent to ViT with parts). We then present a visualization of the attention layers (\(MSA\) and \(MCA\)) in Figure 4. The \(MSA\) layers effectively recognize relevant in-context information of some objects, but the \(CSA\) layers fail to pinpoint foreground details. This is because incidental correlations in the background dominate the \(CSA\) layers. However, incorporating the \(_{mix}\) regularization results in improved localization by the \(MCA\) layers, which are no longer influenced by incidental correlations. An important point to highlight is that the design of \(MSA\) layers causes them to focus on objects specific to a particular class. As a result, their effectiveness is reduced when multiple objects are present (as seen in Figure 4, where MSA misses objects on the left side). Conversely, \(CSA\) layers learn class-agnostic features

Figure 4: Visualizing _MSA_ and _MCA_ layers. The joint representation is obtained by averaging all attention heads (\(_{H}\)). We study the effect of \(_{mix}\) on the interpretability of part based learners.

Figure 5: **Visualizing the quality of learned parts for input image from Figure 4. In Figure 5(a) and 5(b), we show the foreground mixture \((L_{F})\) and four foreground parts selected randomly from \(n_{f}\). Meanwhile, Figure 5(c) and 5(d) show the sparse and orthogonal norms as metrics for evaluating the quality of the part matrix \(\).**

consistently throughout the training set, enabling them to perform well in the presence of multiple objects. For further investigation of the complementary properties of \(MSA\) and \(MCA\), please see the Appendix. Additionally, the Appendix includes visualizations of individual attention heads.

### Studying the quality of learned part representations

As previously stated, the interpretability of learned parts is directly influenced by the sparsity and diversity of the part matrix \(\). This is achieved by examining the impact of \(_{Q}\) during pretraining. We present visualizations of the learned foreground parts in Figure 5(a) and 5(b) for the input image \(x\) from Figure 4. While both setups successfully learn the foreground mixture, \((L_{F})\), without \(_{Q}\), the parts degenerate into a homogeneous solution (Figure 5(a)), lacking sparsity and diversity. With the inclusion of \(_{Q}\), however, the learned parts become sparse and diverse (Figure 5(b)).

We use the \(L\)-1 and orthogonal norms of the part matrix to assess the sparsity and diversity of the parts. As illustrated in Figure 5(c) and Figure 5(d), the addition of \(_{Q}\) maintains bounded norms, inducing sparsity and diversity among the parts. The results also highlight higher norm values by employing \(_{cls}+_{mix}\) losses, which show the degeneracy caused by the introduction of mixture loss. Moreover, the higher sparse and orthogonal norms for \(_{cls}\) demonstrate that part-based methods generally do not maintain the quality of parts.

### Few-shot Learning

We compare DPViT with recent part-based methods: ConstNet ,TPMN , and CORL ; ViT-based methods: SUN , FewTure , HCTransformer , and SMKD  (for SMKD we compare with their prototype-based few-shot evaluation on all the datasets); and recent ResNet based few-shot methods: _Match-feat_, _Label-halluc_, and _FeLMi_.

As shown in Table 1, the proposed method outperforms existing part-based methods by clear margins. Moreover, DPViT achieves competitive performance compared to current ViT-based methods, especially on the FC100 dataset with low-resolution images. The FSL results show that the self-supervised ViT methods give an edge over the existing ResNet-based backbones.

   &  &  &  &  \\  & & **-1shot** & **-5shot** & **1-shot** & **5-shot** & **1-shot** & **5-shot** \\ 
**ProtoNets** (2017)  & ResNet12 & \(60.93\)\(\)0.106 & \(78.53\)\(\)0.25 & \(65.65\)\(\)0.92 & \(38.40\)\(\)0.65 & \(37.50\)\(\)0.60 & \(52.50\)\(\)0.60 \\
**DeepEMD v2** (2020)  & ResNet12 & \(68.77\)\(\)0.29 & \(84.13\)\(\)0.53 & \(71.16\)\(\)0.87 & \(86.03\)\(\)0.58 & \(46.47\)\(\)0.26 & \(63.22\)\(\)0.71 \\
**COSOC** (2021)  & ResNet12 & \(69.28\)\(\)0.49 & \(85.16\)\(\)0.42 & \(73.57\)\(\)0.43 & \(87.57\)\(\)0.10 & - & - \\
**MiTSFEL** (2021)  & ResNet12 & \(63.98\)\(\)0.79 & \(82.04\)\(\)0.49 & \(70.97\)\(\)0.13 & \(86.16\)\(\)0.67 & - & - \\
**Match-feat** (2022)  & ResNet12 & \(63.62\)\(\)0.62 & \(87.21\)\(\)0.46 & \(71.22\)\(\)0.86 & \(85.43\)\(\)0.55 & - & - \\
**Label-Halluc** (2022)  & ResNet12 & \(67.04\)\(\)0.79 & \(85.87\)\(\)0.48 & \(71.97\)\(\)0.89 & \(86.00\)\(\)0.58 & \(47.37\)\(\)0.70 & \(67.92\)\(\)0.70 \\
**FeLMi** (2022)  & ResNet12 & \(67.47\)\(\)0.78 & \(68.04\)\(\)0.41 & \(76.33\)\(\)0.89 & \(87.07\)\(\)0.55 & \(49.02\)\(\)0.70 & \(68.68\)\(\)0.70 \\
**SIN** (2022)  & ViT & \(67.80\)\(\)0.45 & \(83.25\)\(\)0.30 & \(72.99\)\(\)0.50 & \(87.64\)\(\)0.33 & \(87.04\)\(\)0.33 & \(87.04\)\(\)0.75 \\
**FewTure** (2022)  & Swin-Tiny & \(72.40\)\(\)0.78 & \(86.38\)\(\)0.49 & \(76.32\)\(\)0.87 & \(89.96\)\(\)0.55 & \(47.68\)\(\)0.75 & \(63.81\)\(\)0.75 \\
**HCTransformer** (2022)  & 3\(\) ViT-S & \(\)\(\)0.17 & \(89.19\)\(\)0.13 & \(\)\(\)0.20 & \(91.72\)\(\)0.11 & \(48.27\)\(\)0.15 & \(66.42\)\(\)0.16 \\
**SMKD** (2023)  & ViT-S & \(74.28\)\(\)0.18 & \(88.82\)\(\)0.09 & \(78.83\)\(\)0.20 & \(91.02\)\(\)0.12 & \(50.38\)\(\)0.16 & \(68.37\)\(\)0.16 \\
**Construct** (2021)  & ResNet12 & \(64.89\)\(\)0.23 & \(79.95\)\(\)0.17 & \(70.15\)\(\)0.76 & \(86.10\)\(\)0.70 & \(43.80\)\(\)0.20 & \(59.70\)\(\)0.20 \\
**TPMN** (2021)  & ResNet12 & \(67.64\)\(\)0.633 & \(83.44\)\(\)0.43 & \(72.24\)\(\)0.76 & \(86.55\)\(\)0.63 & \(46.93\)\(\)0.71 & \(63.26\)\(\)0.74 \\
**CORL** (2023)  & ResNet12 & \(65.74\)\(\)0.53 & \(83.03\)\(\)0.33 & \(73.82\)\(\)0.58 & \(86.76\)\(\)0.52 & \(44.82\)\(\)0.73 & \(61.31\)\(\)0.54 \\ 
**VIT-with-parts (\(L_{cls}\))** & ViT-S & \(72.15\)\(\)0.20 & \(87.61\)\(\)0.15 & \(78.03\)\(\)0.19 & \(89.08\)\(\)0.19 & \(48.92\)\(\)0.13 & \(67.75\)\(\)0.15 \\
**Ours - DPViT** & ViT-S & \(73.81\)\(\)0.45 & \(\)\(\)0.35 & \(79.32\)\(\)0.48 & \(\)\(\)0.40 & \(\)\(\)0.23 & \(\)\(\)0.45 \\  

Table 1: Evaluating the performance of our proposed method on three benchmark datasets for few-shot learning - MiniImageNet, Tiered-ImageNet, and FC100. The top blocks show the non-part methods while the bottom block shows the part-based methods. The best results are bold, and \(\) is the 95% confidence interval in 600 episodes.

  Method & 1-shot \(\) & -shot \(\) & \(\|\|_{1}\) & \(\|^{T}-\|_{1}\) \\  SMKD  & 60.93 & 80.38 & - & - \\ \(_{cls}\) & 61.24 & 81.12 & 8.41 & 25.82 \\ \(

[MISSING_PAGE_FAIL:9]

6, assigning higher values to \(_{s}\) and \(_{o}\) places greater emphasis on \(_{Q}\), leading to lower norm values and consequently improving the quality of parts. However, this also results in a slight reduction in few-shot accuracy. After careful analysis, we determine that an optimal value of \(0.5\) for both \(_{s}\) and \(_{o}\) strikes a balance, maintaining the quality of parts while preserving few-shot generalization.

### Partial observability of foreground mask

Our training procedure employs foreground masks acquired through a foreground extractor, similar to the one described in . To study the dependence of DPViT on the availability of foreground masks, we examine the weak/limited supervision scenario for foreground masks, where only a small subset of samples possesses the corresponding masks. As depicted in Table 7, we observe that DPViT achieves comparable performance even when only 10% of the training samples have mask information. The performance difference is less than 1.5% for 5-shot and less than 0.8% for 1-shot performance. Furthermore, Figure 7 presents visualizations of image patches surrounding a random foreground and a background part. We also find that in the setup with a \(0\%\) foreground mask, equivalent to \(_{mix}=0\), no disentanglement is observed in the extracted patches. (More visualizations can be found in the Appendix section).

### Working with weak foreground extractor

The features of DPViT (\(_{}\)) depend entirely on the input sample \(\) in order to learn part representations, without explicitly incorporating the mask information. The mask serves as a weak signal to regularize our training objective and separate the foreground parts from the background. Additionally, introducing Gaussian noise in the latent codes enhances DPViT's ability to handle misalignment issues with the foreground masks. Consequently, the learned features remain unaffected by mistakes made by the existing foreground extractor. Moreover, we find that the mixture-of-parts representations can accurately determine the foreground location even when the mask information is missing or incorrect (as illustrated in Figure 6).

### Limitations of DPViT

A constraint within our framework involves relying on a pre-existing foreground extractor. In certain scenarios, such as the classification of tissue lesions for microbiology disease diagnosis, obtaining an existing foreground extractor might not be feasible. Similarly, DPViT focuses on learning components that are connected to the data, yet it doesn't encompass the connections between these components, like their arrangement and hierarchical combination. Introducing compositional relationships among these components could enhance comprehensibility and facilitate the creation of a part-based model capable of learning relationships among the parts.

## 7 Conclusion

In this work, we study the impact of incidental correlations of image backgrounds on the interpretability and generalization capabilities of part learners. We introduce DPViT, a method that effectively learns disentangled part representations through a mixture-of-parts approach. Furthermore, we enhance the quality of part representations by incorporating sparse and orthogonal regularization constraints. Through comprehensive experiments, we demonstrate that DPViT achieves competitive performance comparable to state-of-the-art methods, all while preserving both implicit and explicit interpretability.