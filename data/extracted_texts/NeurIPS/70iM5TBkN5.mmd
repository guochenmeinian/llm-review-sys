# A Large-Scale Human-Centric Benchmark for Referring Expression Comprehension in the LMM Era

Fangyun Wei

The University of Sydney

fwei8714@uni.sydney.edu.au

&Jinjing Zhao

The University of Sydney

jzha0100@uni.sydney.edu.au

Equal contribution.

Kun Yan

Peking University

kyan2018@pku.edu.cn

&Hongyang Zhang

University of Waterloo

hongyang.zhang@uwaterloo.ca

&Chang Xu

The University of Sydney

c.xu@sydney.edu.au

###### Abstract

Prior research in human-centric AI has primarily addressed single-modality tasks like pedestrian detection, action recognition, and pose estimation. However, the emergence of large multimodal models (LMMs) such as GPT-4V has redirected attention towards integrating language with visual content. Referring expression comprehension (REC) represents a prime example of this multimodal approach. Current human-centric REC benchmarks, typically sourced from general datasets, fall short in the LMM era due to their limitations, such as insufficient testing samples, overly concise referring expressions, and limited vocabulary, making them inadequate for evaluating the full capabilities of modern REC models. In response, we present HC-RefLoCo (Human-Centric Referring Expression Comprehension with Long Context), a benchmark that includes 13,452 images, 24,129 instances, and 44,738 detailed annotations, encompassing a vocabulary of 18,681 words. Each annotation, meticulously reviewed for accuracy, averages 93.2 words and includes topics such as appearance, human-object interaction, location, action, celebrity, and OCR. HC-RefLoCo provides a wider range of instance scales and diverse evaluation protocols, encompassing accuracy with various IoU criteria, scale-aware evaluation, and subject-specific assessments. Our experiments, which assess 24 models, highlight HC-RefLoCo's potential to advance human-centric AI by challenging contemporary REC models with comprehensive and varied data. Our benchmark, along with the evaluation code, are available at [https://github.com/ZhaoJingjing/13/HC-RefLoCo](https://github.com/ZhaoJingjing/13/HC-RefLoCo).

## 1 Introduction

Prior research in human-centric AI has predominantly concentrated on single-modality algorithms tasked with understanding, interacting with, or analyzing human behaviors and features. These tasks include face detection  and recognition , pedestrian detection  and re-identification , action recognition , pose estimation , among others. However, the recent emergence of large multimodal models (LMMs) such as GPT-4V  and Google Gemini  has shifted the research landscape towards integrating language semantics with visual content. This paradigm shift heralds a new era in human-centric AI, emphasizing multimodality. Referring expression comprehension (REC)  is a prime example of such amultimodal task. REC involves the localization of specific instances described by natural language inputs. Despite its relevance, there is a notable lack of benchmarks specifically designed to evaluate REC in human-centered contexts. This paper aims to address this gap by developing benchmarks for human-centric REC in the era of large multimodal models .

Existing human-centric REC benchmarks primarily derive from general REC datasets, such as RefCOCO , RefCOCO+ , and RefCOCOg , by filtering images and text annotations related to human categories. These resulting benchmarks, termed HC-RefCOCO, HC-RefCOCO+, and HC-RefCOCOg, where "HC" stands for human-centric, typically include a limited number of test samples, as illustrated in Table 1. For instance, HC-RefCOCO comprises only 1,519 images with 10,771 text annotations. Moreover, these benchmarks utilize brief text descriptions of the target instances, with the average word count of annotations being 3.4, 3.3, and 8.9 for HC-RefCOCO, HC-RefCOCO+, and HC-RefCOCOg, respectively. The advent of large language models, such as GPT-4  and LLAMA , has significantly enhanced the language understanding capabilities of AI models, making the processing of short texts less challenging in the REC task. Consequently, there is a pressing need for more comprehensive and challenging benchmarks that reflect the advanced capabilities of contemporary AI models in human-centric REC.

In this work, we introduce a new benchmark called HC-RefLoCo, which stands for Human-Centric Referring Expression Comprehension with Long Context. Comprehensive statistics can be found in Table 1. Our benchmark is characterized by the following five features:

**Large Scale.** Our benchmark includes 13,452 images accompanied by 24,129 instances with 44,738 annotations (referring expressions), providing a substantial dataset for HC-REC testing.

Figure 1: (a) An Example from our HC-RefLoCo benchmark. For each target object, we provide a comprehensive and detailed text description, with an average length of 93.2 words. Each sentence within this description is classified into one of the following categories: (b) appearance, (c) human-object interaction, (d) location, (e) action, (f) celebrity, (g) optical character recognition, or None.

**Long and Detailed Descriptions.** Utilizing the cutting-edge language model, GPT-4, we generate extensive descriptions (annotations) for each target instance. Each annotation is meticulously _reviewed_ and _manually_ revised to correct any hallucination issues. This rigorous process ensures the accuracy of the benchmark. The descriptions vary in length from 15 to 241 words, averaging 93.2 words. They encompass an extensive vocabulary of 18,681 words. An example can be found in Figure 1(a).

**Subject Labels.** Each text annotation is an extensive paragraph comprising multiple sentences. We manually categorize each sentence into one of the following subjects: appearance, human-object interaction (HOI), location, action, celebrity, optical character recognition (OCR), or None, as illustrated in Figure 1. This detailed labeling process enables a focused evaluation of modern REC models, assessing their proficiency in interpreting and processing varied linguistic inputs associated with specific subjects. By incorporating a wide range of subjects, we aim to provide a robust benchmark that challenges and advances the current capabilities of REC models, ensuring they can adeptly handle the complexity and diversity present in real-world applications.

**Broader Coverage of Instance Scales.** Compared to existing HC-REC benchmarks like HC-RefCOCO, HC-RefCOCO+, and HC-RefCOCOg, our dataset spans a wider range of instance scales. The square root of the instance size varies from 62.5 to 3720.7, with an average of 313.8, providing a more comprehensive representation of different object sizes.

**Various Evaluation Protocols.** Accuracy is a commonly used metric in evaluating existing REC models. An instance is considered successfully located if the Intersection over Union (IoU) between the predicted bounding box and the ground truth exceeds 0.5. This standard evaluation metric is referred to as Acc\({}_{0.5}\). To provide a comprehensive understanding of the models' strengths and weaknesses, we introduce various evaluation protocols, including:

* Accuracy using different IoU criteria such as Acc\({}_{0.5}\), Acc\({}_{0.75}\), Acc\({}_{0.9}\), and mean Accuracy (mAcc) across all IoU criteria, to thoroughly assess the models' localization capabilities.
* Accuracy on small, medium, and large instances to evaluate the models' efficiency across varying instance sizes.
* Subject-specific evaluation, which involves assessing models based on distinct subjects to validate the performance of REC models in managing diverse linguistic inputs and correlating detailed descriptions with precise visual elements.

In our experiments, we evaluate a total of 24 training-unconstrained models, which utilize any available data for training, including GPT-4V, bounding box-output models, and mask-output models, employing a range of evaluation protocols. With its extensive test samples, detailed annotations, the incorporation of subject labels, the broad coverage of instance scales, and the introduction of diverse evaluation protocols, we hope our benchmark will advance research in human-centric AI.

## 2 Related Work

**REC Benchmarks.** Referring expression comprehension (REC) refers to the process of localizing the specific instances described by natural language inputs. Current human-centric REC benchmarks primarily originate from general REC datasets like RefCOCO , RefCOCO+ , and RefCOCOg . RefCOCO, which stems from the COCO2014  dataset, contains 50,000 annotations across 19,994 images. The expressions in this benchmark are typically short and concise,

   Dataset & Images & Instances & Annotations & Avg. Words & Vocab. & Instance Size & Subjects \\  HC-RefCOCO  & 1,519 & 3,754 & 10,771 & 3.4 & 2,251 & 114.0 - 603.2 & - \\ HC-RefCOCO+  & 1,519 & 3,754 & 10,908 & 3.3 & 2,702 & 114.0 - 603.2 & - \\ HC-RefCOCOg  & 1,521 & 2,669 & 5,253 & 8.9 & 2,891 & 89.7 - 610.5 & - \\  HC-RefLoCo (Ours) & 13,452 & 24,129 & 44,738 & 93.2 & 18,681 & 62.5 - 3720.7 & 6 \\   

Table 1: Comparison between human-centric (HC) referring expression comprehension benchmarks and the proposed HC-RefLoCo benchmark. Statistics for HC-RefCOCO, HC-RefCOCO+, and HC-RefCOCOg are derived from the combination of their respective validation and test sets. Vocab.: vocabulary. Avg.: average.

including many locational descriptions such as "Right guy", "Far left man", and "Guy on left". In contrast, RefCOCO+ is of a similar scale, with 49,856 annotations across 19,992 images, but intentionally omits locational prepositions like "left" and "right", thereby increasing semantic complexity. Examples include "Man with light hat" and "Guy in white". RefCOCOg, on the other hand, offers more extensive annotations compared to its predecessors, with examples like "A person in a hat on a wooden bench" and "A man in white playing Frisbee". To improve performance on these benchmarks, datasets like GRIT , GranD  and RecapD  are commonly utilized as training sources. Although not exclusively designed for REC, datasets like Flickr30k Entities  and Visual Genome  are also frequently used for training. Compared to previous testing benchmarks, our HC-RefLoCo provides longer expressions, with an average length of 93.2 words, covering a vast vocabulary of 18,681 words.

**LMMs for Visual Grounding.** Recent advancements in large multimodal models (LMMs) such as Flamingo , BLIP-2 , MiniGPT-4 , InstructBLIP , mPLUG-Owl , and LLaVA , have significantly enhanced the integration of vision and language modalities by leveraging the progress in large language models (LLMs) . These models have shown remarkable improvements in tasks related to image understanding and visual question answering. However, instance localization remains a challenging aspect that requires LMMs to not only comprehend the relationship between visual elements and language but also to accurately generate bounding boxes for target instances. The REC task serves as a critical benchmark to evaluate the localization capabilities of these models. Pioneering models like KOSMOS-2 , Shikra , Grounding-GPT , Qwen-VL , and the SPHINX series  typically employ an auto-regressive causal Transformer with tokenized bounding box representations to tackle the REC task. To achieve more precise representations of target instances, recent approaches have suggested the use of masks instead of bounding boxes as outputs. Models such as LISA , PixelLLM , PSALM , and GlaMM  extend the segmentation paradigm initially developed by SAM . Another closely related area is open-vocabulary object detection and segmentation , which aims to locate any objects and identify their class labels using a word or phrase. However, this task still differs from the REC task, where the models have to identify the target based on an extended text description rather than a single word or phrase.

## 3 Benchmark Construction and Analysis

### Benchmark Construction

**Data Sources and Pre-Processing.** Our HC-RefLoCo benchmark is derived from several public object detection datasets, including the validation sets of COCO 2017  and Objects365 , as well as the validation and testing sets of OpenImage v7 . For COCO 2017 and Objects365, we retain all instances labeled as "person", whereas for OpenImage v7, we keep instances labeled as "human". We also exclude extremely small instances, specifically those occupying less than 1% of the total image area. We adopt the original bounding box annotations in these datasets. We also collect images of 367 celebrities from the LAION-5B  dataset. Each image in the dataset contains at least one of these celebrities and includes at least two people. The bounding boxes for the celebrities are manually annotated. Consequently, we compile a total of 3,520 images, each containing a single annotated instance.

In conclusion, our HC-RefLoCo benchmark comprises 200 images with 419 instances from COCO, 4,772 images with 10,070 instances from Objects365, 4,960 images with 10,120 instances from OpenImage v7, and 3,520 images with the same number of instances from LAION-5B.

**Referring Expression Generation.** Figure 2 illustrates the procedure for generating a referring expression (a.k.a. description) for each target instance. Given a target instance and its corresponding image, this involves a three-step process:

1. Employing GPT-4V to generate an instance-level description by inputting the cropped instance, following the prompt outlined in Section A.1.
2. Feeding the raw image into GPT-4V to expand the initial description generated in Step.1 by incorporating the context around the target instance, using the prompt described in Section A.2.

3. Subsequently, we manually review each referring expression to correct any errors, particularly those arising from hallucination issues in GPT-4V, ensuring that the descriptions accurately and uniquely identify the target instances.

**Annotation Expansion.** Up to now, our benchmark includes 13,452 images with 24,129 instances, each accompanied by a referring expression (annotation). Leveraging GPT-4's exceptional language capabilities, we prompt it to rewrite each referring expression, using the prompt detailed in Section A.3. This process effectively doubles the annotations. We then conduct a manual review of each rewritten referring expression, eliminating those that are improper or ambiguous to ensure that the revised annotations uniquely describe their respective target instances. Consequently, our final benchmark comprises 13,452 images, with 44,738 annotations describing 24,129 instances.

**Subject Labels.** We manually categorize each sentence within these expressions into one of the following subjects: appearance, human-object interaction (HOI), location, action, celebrity, optical character recognition (OCR), or None. The label criteria for each subject can be found in Section B.

**Data Format.** Each instance \(\) is associated with an image \(\), a bounding box \(=\{x,y,w,h\}\)--where \((x,y)\) represents the coordinates of the top left corner, and \(w\) and \(h\) denote the width and height--and a referring expression \(=\{_{1},...,_{n}\}\) containing \(N\) sentences. Each sentence \(_{i}\) within the expression \(\) has a subject label \(_{i}\).

### Analysis

**Annotation Length.** Figure 2(a) visualizes the distribution of annotation lengths for four different benchmarks: HC-RefCOCO, HC-RefCOCO+, HC-RefCOCOg, and our HC-RefLoCo. The distribution of our HC-RefLoCo is markedly different from the other three benchmarks--there is a distinct peak around 100 words, indicating that the referring expressions in HC-RefLoCo are significantly longer than those in the others, which have peaks within the 4-8 word range. Additionally, the distribution of HC-RefLoCo spans from around 50 to 150 words, showing a much broader range.

**Sentence Length.** The HC-RefLoCo benchmark features annotations composed of multiple sentences. Figure 2(b) illustrates the distribution of sentence lengths across four benchmarks, using all sentences from all annotations for statistical analysis. Our benchmark notably peaks at a sentence length of

Figure 2: The process of generating a referring expression for each target instance. Inspired by recent studies on GPT-4V , which demonstrate that GPT-4V can pay more attention to instances highlighted by a red circle within an image, we similarly encircle the target instance in red in Step-2.

approximately 18-20 words. In contrast, the other three benchmarks, HC-RefCOCO, HC-RefCOCO+, and HC-RefCOCOg, generally use single-sentence annotations, typically around 4-8 words.

Figure 4: Statistical analysis of (a) the image size, and (b) the instance size, across four benchmarks. The instance size is represented by its square root. Note that there is a high distribution overlap among HC-RefCOCO, HC-RefCOCO+, and HC-RefCOCOg since they derive from the same dataset.

Figure 5: (a) Per-subject analysis. (b) Distribution of instance center. We compare our HC-RefLoCo benchmark with the combination of HC-RefCOCO, HC-RefCOCO+ and HC-RefCOCOg.

Figure 3: Statistical analysis of annotation length and sentence length across four benchmarks.

**Image Size.** Figure 3(a) compares the image size distributions of our benchmarks against HC-RefCOCO, HC-RefCOCO+, and HC-RefCOCOg. Since all three compared benchmarks derive from the same COCO dataset, there is a high distribution overlap. In contrast, our HC-RefLoCo benchmark covers a wider range of image sizes.

**Instance Size.** In Figure 3(b), we visualize the instance size distribution across four benchmarks. Our benchmark spans a wider range of instance scales. The square root of the instance size varies from 62.5 to 3720.7, with an average of 313.8.

**Annotation and Image Number for Each Subject.** In our HC-RefLoCo benchmark, each annotation is a referring expression composed of multiple sentences for a given instance. Each sentence is assigned a specific subject label. As illustrated in Figure 4(a), we analyze the number of annotations containing at least one sentence with the corresponding subject label for each subject.

**Instance Center.** Figure 4(b) presents a scatter plot illustrating the distribution of instance centers across two datasets: our HC-RefLoCo benchmark and the combined datasets of HC-RefCOCO, HC-RefCOCO+, and HC-RefCOCOg. Our benchmark demonstrates a more uniform spatial distribution.

## 4 Evaluation

**Benchmark Usage.** Modern REC models are often trained on extensive and diverse datasets. For example, the SPHINX  model leverages a mix of 16 unimodal and multimodal datasets, encompassing millions of training samples. Our HC-RefLoCo benchmark is designed to assess the capabilities of these advanced models without imposing any limitations on the sources of training data. The benchmark is divided into two subsets: a validation set, comprising 30% of the data with 4,000 images, 7,190 instances, and 13,360 annotations; and a test set, comprising 70% of the data

    &  &  &  \\   & Acc\({}_{0.5}\) & Acc\({}_{0.75}\) & Acc\({}_{0.9}\) & mAcc & mAcc & mAcc \\  GPT-4V  & 17.4 & 2.6 & 0.3 & 5.5 & 5.5 & 5.6 \\ GroundingGPT  & 56.6 & 27.2 & 5.3 & 29.8 & 30.0 & 29.8 \\ Ferret 7B  & 44.9 & 32.6 & 11.7 & 30.0 & 30.6 & 29.7 \\ Ferret 13B  & 52.9 & 38.5 & 15.6 & 35.7 & 35.9 & 35.6 \\ MiniGPT4-v2  & 47.1 & 31.7 & 11.6 & 30.3 & 30.7 & 30.1 \\ KOSMOS-2  & 45.3 & 38.0 & 20.0 & 34.1 & 34.2 & 34.0 \\ Shikra  & 56.8 & 35.6 & 10.3 & 34.4 & 34.6 & 34.3 \\ OFA  & 48.4 & 37.0 & 21.7 & 35.3 & 35.2 & 35.3 \\ OFA-Large & 70.5 & 61.6 & 44.0 & 58.1 & 57.9 & 58.1 \\ Qwen-VL  & 67.9 & 56.8 & 34.8 & 52.8 & 53.1 & 52.6 \\ CogVLM  & 66.0 & 59.6 & 43.8 & 55.8 & 56.3 & 55.5 \\ Lenna  & 68.8 & 63.5 & 51.6 & 60.6 & 60.5 & 60.7 \\ ONE PEACE  & 79.3 & 69.0 & 43.8 & 63.1 & 63.4 & 62.9 \\ SPHINX-MoE  & 76.3 & 57.7 & 21.8 & 52.5 & 52.7 & 52.4 \\ SPHINX  & 77.5 & 61.0 & 27.0 & 55.4 & 55.8 & 55.2 \\ SPHINX-1k  & 80.7 & 68.6 & 41.1 & 63.0 & 63.0 & 62.9 \\ SPHINX-MoE-1k  & **85.8** & **77.3** & 53.7 & 71.4 & 71.5 & 71.4 \\ SPHINX-v2-1k  & 84.1 & 77.1 & **56.2** & **71.7** & **71.6** & **71.7** \\  PixelLM 7B\({}^{}\) & 38.5 & 24.7 & 11.8 & 24.5 & 24.6 & 24.4 \\ PixelLM 13B\({}^{}\) & 63.6 & 46.6 & 25.8 & 44.6 & 45.0 & 44.4 \\ LISA-explanatory\({}^{}\) & 47.6 & 37.6 & 27.0 & 36.7 & 36.7 & 36.7 \\ LISA\({}^{}\) & 52.4 & 42.1 & 31.3 & 41.1 & 41.1 & 41.1 \\ PSALM\({}^{}\) & 61.7 & 53.4 & 40.2 & 51.1 & 51.4 & 51.0 \\ GlaMM\({}^{}\) & **66.1** & **56.9** & **44.2** & **55.0** & **54.9** & **55.0** \\   

Table 2: Performance evaluation across 24 models on our HC-RefLoCo benchmark. Models indicated with a \(\) generate mask outputs, which we convert into tight bounding boxes to enable evaluation. Refer to Section D for the details of each model. NVIDIA A100 (80G) GPUs are used for evaluation.

with 9,452 images, 16,939 instances, and 31,378 annotations. While we provide these two splits, we encourage the combined use of both validation and test sets for model evaluation, particularly in the current era of large-scale multimodal models, where the use of unrestricted training data is common.

**Evaluation Protocols.** In the conventional evaluation protocol, an instance is deemed successfully located if the IoU between the predicted bounding box and the ground truth surpasses 0.5. Accuracy is then employed as the evaluation metric, known as \(_{0.5}\). To provide a more comprehensive assessment of model performance, we propose three evaluation protocols:

* In addition to \(_{0.5}\), we also measure \(_{0.75}\), \(_{0.9}\), and the mean accuracy (mAcc), which is the average of \(_{0.5}\) through \(_{0.95}\) at intervals of 0.05.
* Figure 4(a) presents the number of annotations for each subject. We further conduct a per-subject evaluation using mAcc as the evaluation metric.
* To assess robustness to variations in instance sizes, we report \(_{s}\), \(_{m}\), and \(_{l}\), representing the mAcc for small, medium, and large instances. The size of an instance is determined by taking the square root of its area. Instances are categorized as small if their size is less than 128, medium if their size ranges from 128 to 256, and large if their size exceeds 256.

## 5 Experiments

**Main Results.** We assess a total of 24 advanced models, which are divided into two categories based on their output types: 1) models producing bounding box outputs, including GPT-4V [54; 55; 56], GroundingGPT , Ferret , MiniGPT4-v2 [101; 4], KOSMOS-2 , Shikra , OFA , Qwen-VL , CogVLM , Lenna , ONE-PEACE , and SPHINX [15; 39], and 2) models generating mask outputs, including PixelLM , LISA , PSALM , and GlaMM . The specific prompt used for GPT-4V evaluation is described in Section A.4. For models that produce mask outputs, we convert these masks into tight bounding boxes to facilitate evaluation. The performance results are presented in Table 2.

   Model & Appearance & HOI & Celebrity & OCR & Action & Location \\  GPT-4V [54; 55; 56] & 5.0 & 5.1 & 12.0 & 5.1 & 3.6 & 4.6 \\ GroundingGPT  & 27.3 & 27.5 & 61.4 & 25.8 & 21.3 & 23.0 \\ Ferret 7B  & 27.9 & 27.9 & 57.0 & 27.0 & 24.2 & 25.1 \\ Ferret 13B  & 33.9 & 34.4 & 58.5 & 33.5 & 28.8 & 30.9 \\ MiniGPT4-v2  & 27.4 & 27.5 & 66.2 & 24.6 & 22.6 & 22.7 \\ KOSMOS-2  & 31.5 & 32.9 & 65.8 & 31.5 & 27.9 & 28.2 \\ Shikra  & 32.7 & 32.5 & 55.9 & 29.7 & 30.6 & 31.7 \\ OFA  & 35.2 & 35.3 & 36.8 & 35.2 & 32.3 & 32.2 \\ OFA Large & 58.4 & 58.3 & 56.0 & 56.9 & 55.1 & 55.2 \\ Qwen-VL  & 52.7 & 53.1 & 56.1 & 50.9 & 47.8 & 49.3 \\ CogVLM  & 54.8 & 53.6 & 66.9 & 50.3 & 55.9 & 55.2 \\ Lenna  & 61.8 & 62.3 & 50.6 & 61.6 & 56.5 & 57.2 \\ ONE PEACE  & 62.1 & 63.5 & **75.4** & 62.1 & 55.8 & 56.6 \\ SPHINX-MoE  & 51.6 & 52.9 & 64.4 & 52.1 & 45.5 & 47.9 \\ SPHINX  & 54.2 & 55.1 & 70.4 & 53.1 & 49.4 & 50.8 \\ SPHINX-1k  & 62.7 & 63.3 & 66.0 & 61.7 & 59.0 & 59.6 \\ SPHINX-MoE-1k  & 71.8 & 72.4 & 67.7 & 72.0 & 67.9 & 68.9 \\ SPHINX-v2-1k  & **72.4** & **73.0** & 64.1 & **72.3** & **68.7** & **69.6** \\  PixelLM 7B\({}^{}\) & 23.3 & 22.6 & 39.6 & 23.4 & 22.4 & 20.9 \\ PixelLM 13B\({}^{}\) & 43.8 & 44.9 & 54.8 & 44.0 & 38.9 & 40.3 \\ LISA-explanatory\({}^{}\) & 34.1 & 32.5 & 69.6 & 30.8 & 33.1 & 31.2 \\ LISA\({}^{}\) & 38.8 & 38.0 & **70.2** & 36.7 & 37.1 & 35.0 \\ PSALM\({}^{}\) & 51.7 & 51.6 & 47.3 & **52.2** & 48.3 & 49.5 \\ GlaMM\({}^{}\) & **54.0** & **53.4** & 68.7 & 51.7 & **51.3** & **51.3** \\   

Table 3: Per-subject evaluation across 24 models on our HC-RefLoCo. We report mAcc for each set.

**Per-Subject Evaluation.** As outlined in Section 3, our benchmark is divided into six subsets, each corresponding to one of the following subjects: appearance, human-object interaction (HOI), location, action, celebrity, and optical character recognition (OCR). This segmentation enables a focused evaluation of the model's performance on specific topics. Table 3 presents the mAcc for each subset. The SPHINX-v2-1k  model demonstrates the highest overall performance across most subsets, while ONE-PEACE  excels particularly in celebrity recognition.

**Scale-Aware Evaluation.** In Figure 6, we assess model performance across three groups categorized by instance size: large, medium, and small. The size of each instance is determined by the square root of its area. Specifically, small instances have a size less than 128, medium instances range from 128 to 256, and large instances exceed 256. Generally, most models exhibit a decline in performance as instance size decreases. CogVLM  shows the greatest robustness across different instance scales.

**Effects of Using Detailed and Contextual Annotations.** In our HC-RefLoCo benchmark, each annotation comprises multiple sentences, with each sentence labeled to a specific subject. We conduct per-subject evaluations under two scenarios: 1) using the original annotations; and 2) retaining only sentences that correspond to the specific subject while discarding the rest for each annotation. In Figure 7, we assess five models--KOSMOS-2 , Ferret 7B , MiniGPT4 v2 , SPHINX , and Shikra --each employing different language encoders: KOSMOS 1.3B , Vicuna 7B , LLaMa2 Chat 7B , LLaMa 2 13B , and LLaMa 7B , respectively. For most subjects, SPHINX and Shikra achieve higher performance when all sentences are used to describe the target instance, possibly due to their strong language encoders, LLaMa 2 13B  and LLaMa 7B . Conversely, we observe that MiniGPT4 v2  shows a significant decline in performance with annotations containing more contextual descriptions, highlighting its limitations in associating lengthy text descriptions with visual elements.

Figure 6: Scale-aware evaluation. Models are sorted in ascending order based on their performance on large instances. We use mAcc as the evaluation metric.

Figure 7: Per-subject evaluation under two scenarios: 1) using the original annotations (denoted as “All”); 2) retaining only sentences that correspond to the specific subject while discarding the rest for each annotation.

Additionally, we generate three extra sets by randomly selecting 1, 3, and 5 sentences from each annotation. In Figure 8 of Section C, we evaluate the same models on these three sets, alongside the original HC-RefLoCo benchmark. The results reveal that different models perform optimally on different sets, highlighting a trade-off--while longer descriptions offer more context, the models may fail to effectively associate the extended context with the target instance.

## 6 Conclusion

This paper introduces a novel benchmark called HC-RefLoCo, designed specifically for human-centric referring expression comprehension. HC-RefLoCo presents unique challenges and evaluation criteria for large multimodal models. Key features include: 1) a substantial benchmark with 13,452 images, 24,129 instances, and 44,738 annotations; 2) detailed annotations ranging from 15 to 241 words, with an average of 93.2 words, and an extensive vocabulary of 18,681 words; 3) sentence-level subject labels; 4) a wide range of instance scales; and 5) multiple evaluation protocols, including the utilization of various IoU criteria, subject-specific evaluations, and scale-aware evaluations. The benchmark and evaluation code will be publicly available to support the advancement of REC models, particularly in the LMM era.

**Acknowledgements.** This work was supported in part by the Australian Research Council under Projects DP240101848 and FT230100549.