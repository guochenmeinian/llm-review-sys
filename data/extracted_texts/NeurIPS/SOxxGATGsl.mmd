# Efficient Algorithms for Lipschitz Bandits

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Lipschitz bandits is a fundamental framework used to model sequential decision-making problems with large, structured action spaces. This framework has been applied in various areas. Previous algorithms, such as the Zooming algorithm, achieve near-optimal regret with \(O(T^{2})\) time complexity and \(O(T)\) arms stored in memory, where \(T\) denotes the size of the time horizons. However, in practical scenarios, learners may face limitations regarding the storage of a large number of arms in memory. In this paper, we explore the bounded memory stochastic Lipschitz bandits problem, where the algorithm is limited to storing only a limited number of arms at any given time horizon. We propose algorithms that achieve near-optimal regret with \(O(T)\) time complexity and \(O(1)\) arms stored, both of which are almost optimal and state-of-the-art. Moreover, our numerical results demonstrate the efficiency of these algorithms.

## 1 Introduction

Multi-armed Bandits (MAB) is a powerful framework used to balance the exploration-exploitation trade-off in online decision-making problems. Within this framework, a learner sequentially selects arms (actions, decisions, or items) and learns from the associated feedback, aiming to maximize the expected total reward within finite time horizons. Some well-known algorithms, such as UCB1 and Exp3, have achieved near-optimal regret by storing records of all arms in memory. In many bandit problems, algorithms can access information about the similarity between arms, suggesting that arms with similar characteristics often yield similar expected rewards. The Lipschitz bandits framework is a prominent variant that addresses decision-making in large, structured action spaces, where the expected reward of the arms follows a Lipschitz function. For instance, in recommendation systems, the arms correspond to items represented by feature vectors. Items with similar feature vectors are likely to result in similar outcomes or conversions.

Recently, a series of works in the field of online learning have been dedicated to managing scenarios with large action spaces while maintaining sub-linear memory usage. This direction is driven by the need to effectively tackle extensive real-world applications such as recommendation systems, search ranking, and crowdsourcing. In these applications, arms correspond to items, solutions, or models, which leads to significant memory demands. For instance, in recommendation systems, the learner faces the challenge of choosing from millions of items, like music and movies, to present to users, especially in scenarios characterized by limited space or an infinite number of arms. Therefore, the development of memory-efficient algorithms has become crucial for these applications. In recent years, substantial efforts have been made to address the challenge of bandits with limited memory (Assadi and Wang, 2020; Jin et al., 2021; Maiti et al., 2020; Agarwal et al., 2022; Assadi and Wang, 2022; Wang, 2023; Assadi and Wang, 2023a). However, previous research has mainly focused on unstructured action spaces, often overlooking the fact that in these applications, arms with similar characteristics tend to yield similar expected rewards.

One general approach to solving Lipschitz bandits is through discretizing the structured action space. Algorithms based on uniform discretization have been shown to achieve optimal worst-case regret up to a logarithmic factor (Kleinberg, 2004). Another strategy, adaptive discretization, progressively 'zoom's in' on more promising regions of the action space, yielding near-optimal problem-dependent regret (Kleinberg et al., 2019). However, existing algorithms like the Zooming algorithm necessitate \(O(T)\) stored arms in memory and \(O(T^{2})\) time complexity for stochastic Lipschitz bandits (Kleinberg et al., 2019; Feng et al., 2022), which may be impractical for many real-world applications. In this paper, we consider a typical scenario where the learner operates within the stochastic bandits framework over a Lipschitz action space while facing constraints on the number of arms that can be stored in memory.

The limited memory constraint and large structured action space present several challenges, necessitating a nuanced approach to effectively balance exploration and exploitation under uncertainty. One key challenge is the propensity to over-exploit suboptimal arms retained in memory, leading to high regret. Conversely, reading new arms into memory risks discarding potentially valuable arms. In scenarios with infinite actions, the vast search space requires numerous samples to ensure adequate exploration. The structured nature of the action space demands that algorithms focus on zooming in on more promising regions, but space constraints limit the learner's capacity to acquire comprehensive knowledge about the metric space. Traditional full-memory algorithms start by dividing the action space into many small subcubes, a process known as discretization. Each cube is treated as an arm, and in each round, the algorithm updates the average estimate of the selected cube's reward based on feedback. It then compares this estimate against all other cubes in the storage space through various computational methods.

### Our Contributions

Our primary insight revolves around two key aspects: metric embedding and pairwise comparisons. Metric embedding involves mapping elements from one metric space to another while preserving distance relationships as closely as possible. Our algorithm effectively maps the metric space to a tree, where each node represents a cube. Traversing this tree is analogous to navigating the entire metric space. Pairwise comparisons of arms reduce memory complexity. Instead of constantly covering the entire space, our approach considers all subcubes as a stream. From this stream, we continuously select cubes for pairwise comparisons, gradually converging to the optimal region.

Based on this insight, we introduce two algorithms: the Memory Bounded Uniform Discretization (MBUD) algorithm and the Memory Bounded Adaptive Discretization (MBAD) algorithm. The MBUD algorithm employs a uniform discretization strategy combined with an Explore-First approach. In this method, all cubes are of the same size. The algorithm prioritizes selecting a near-optimal arm following an exploration phase and allocates the remaining rounds to exploitation, achieving near-optimal worst-case regret. The exploration phase consists of "cross exploration phases" and the "summarize phase". During the cross exploration phases, exploration is confined to a subset of cubes to gather information about the optimal arm while minimizing regret. The summarize phase explores all cubes to pinpoint the optimal arm's location.

The MBAD algorithm utilizes an adaptive discretization strategy, incorporating a round-robin playing approach. This allows for subcubes within subcubes, organizing the entire action space into a tree structure. The algorithm selectively focuses on more promising regions of the action space, thereby attaining near-optimal instance-dependent regret. Each node in this structure represents a subcube, with parent and child nodes corresponding to subcubes and their subdivisions, respectively. Traversal involves transitioning from a node to its child and navigating through a parent node's children to the next subcube. Pruning prevents over-zooming through two conditions: discarding inferior cubes with high confidence and establishing a lower bound on cube edge length, which decreases as exploration progresses. These conditions ensure efficient exploration without over-zooming.

   Algorithm & Regret & Time complexity & Space complexity \\  Zooming(Kleinberg et al., 2019) & \((T^{})\) & \(O(T^{2})\) & \(O(T)\) \\ HOO(Bubeck et al., 2011a) & \((T^{})\) & \(O(T T)\) & \(O(T)\) \\ MBAD(Ours) & \((T^{})\) & \(O(T)\) & \(O(1)\) \\   

Table 1: Comparison with State-of-the-art Lipschitz Bandits AlgorithmsOverall, our contribution lies in pioneering memory-efficient algorithms for large structured action spaces, particularly within Lipschitz metric spaces. We introduce the MBUD and MBAD algorithms, which achieve near-optimal regret while requiring storage for only the best-estimate arm for exploitation and one additional arm for exploration. This means only two arms need to be stored in memory, regardless of the problem's scale. Furthermore, each algorithm exhibits \(O(T)\) time complexity, indicating that their execution time scales linearly with the number of rounds.

### Related Work

Lipschitz bandits.Multi-armed bandits is one of the most classical frameworks to model the trade-off between exploration and exploitation in online decision problems. The Lipschitz bandits framework considers the large, structured action space in which the algorithm has information on similarities between arms. The model was first introduced by Agrawal (1995) with interval \(\). The near-optimal upper and lower bounds for the worst case were provided in Kleinberg (2004) via the uniform discretization strategy. Subsequent work (Kleinberg et al., 2019) proposed the zooming algorithm, achieving near-optimal instance-dependent regret for the problem and studying the extension for the general metric action space. Several other works have established regret bounds for the stochastic reward feedback setting (Bubeck et al., 2011; Magureanu et al., 2014; Lazaric et al., 2014). Other works have also extended the results to the adversarial version (Podimata and Slivkins, 2021; Kang et al., 2023), contextual setting (Slivkins, 2014; Krishnamurthy et al., 2019; Lee et al., 2022), ranked setting (Slivkins et al., 2013), contract design (Ho et al., 2014), federated X-armed bandit (Li et al., 2024, 2024, 2020), and other settings (Bubeck et al., 2011; Lu et al., 2019; Wang et al., 2020; Grant and Leslie, 2020; Feng et al., 2022; Xue et al., 2024).

Memory-efficient learning.Another line relevant to this paper is online learning with memory constraints. Liau et al. (2018) considered stochastic bandits with constant arm memory and proposed an algorithm achieving an \(O( 1/)\) factor of optimal instance-dependent regret, where \(\) is the gap between the best arm and the second-best arm. Chaudhuri and Kalyanakrishnan (2020) studied stochastic bandits with \(M\) stored arms and showed there is an algorithm with regret \((KM+(K^{3/2})/M)\). Subsequent work (Agarwal et al., 2022) provided an algorithm achieving regret \(O()\). In addition to the bandits problem, there are also many works about other online learning problems. Srinivas et al. (2022); Peng and Zhang (2022) showed the trade-off between regret and memory for the expert problem. More pure exploration models with memory constraints were considered in Assadi and Wang (2020), including the coin tossing problem, noisy comparisons problem, and Top-\(K\) arms identification. Previous works on bandits with limited memory have not considered structured action spaces and could not deal with infinite actions. There are some other works on memory-efficient online learning (Peng and Rubinstein, 2023; Assadi and Wang, 2023). Beyond the online learning setting, the memory-efficient learning problem was solved in different situations, including statistical learning (Steinhardt et al., 2016; Garg et al., 2017; Raz, 2017; Garg et al., 2019; Sharan et al., 2019; Lyu et al., 2023), convex optimization (Marsden et al., 2022; Blanchard et al., 2023, 2020; Chen and Peng, 2023), estimation problems (Acharya et al., 2019; Diakonikolas et al., 2022; Berg et al., 2022), parity learning (Raz, 2019; Kol et al., 2017), and other learning problems (Hopkins et al., 2021; Brown et al., 2022; Chen et al., 2022).

## 2 Problem Setup and Preliminaries

Notations.In this paper, we use bold fonts to represent vectors and matrices. For a positive integer \(T\), we use \([T]\) to denote the set \(\{1,2,,T\}\). For a set \(\), we use \(||\) to denote its cardinality. For a random variable \(Z\), we use \([Z]\) to denote its expectation. For an event \(\), we use \([]\) to denote its probability.

### Problem Setup

We formally define the Lipschitz bandits problem below. Given \(T\) rounds, dimension \(d\), and arm space \(=^{d}\), each arm \(x\) is associated with an unknown reward distribution \(_{x}\). In each round \(t[T]\), the algorithm selects an arm \(x_{t}\) and obtains a scalar-valued reward feedback \(r_{t}\), which is a sample from the reward distribution \(_{x_{t}}\). The expected reward \(()\) of the reward distribution satisfy the Lipschitz condition: \(|(x)-(y)| L|x-y| x,y\). And we call \(L\) the Lipschitz constant. Then a problem instance is specified by the known number of time horizons \(T\), known Lipschitz constant \(L\), and unknown mean reward \(()\). For the purposes of simplification in our proofs, we assume \(L=1\). The algorithm aims to maximize the expected total reward \([_{t[T]}r_{t}]\)We use regret to measure the performance of the algorithm compared with the expected total reward of the best-fixed arm in action space \(\): \(_{}(T)=T_{x}(x)-[ _{t[T]}r_{t}]\).

Then we present the memory model employed in the paper. The algorithm operates by selecting arms from the memory and pulling them. When the memory reaches the capacity and the algorithm attempts to choose a new arm, it becomes necessary to discard at least one arm from the memory. Consequently, any statistical information associated with the discarded arm, including its index, mean reward, and number of pulls, is forgotten and will not be retained thereafter. We measure the space complexity of the algorithm by the hard constraint for the number of arms stored in the memory. This constraint aligns with the assumption of having oracle access to the input arm, as commonly defined in streaming problems.

### Covering Dimension and Zooming Dimension

Then we provide some technical tools that are used in this paper and introduce the covering dimension and zooming dimension for one action space \(\). We use the definitions in (Slivkins, 2019) and provide them below. Notice that the Lipschitz bandits problem is defined in an infinite-action space. We select a fixed, finite discretization actions space \(\). Let \(\{_{1},,_{N}\}[_{i}]\) be an cover of the action space \(\). Let \(\) denote the maximum diameter of \(_{i}\) for all \(i[N]\). Then the arm set \(=\{x_{i}|x_{i}_{i},i[N]\}\) is an \(\)-mesh. The covering dimension \(d\) of the action space \(\) is defined as \(d=_{a 0}\{||^{-},>0\}\). Let \(^{*}_{}:=_{x}(x)\) denote the expected per-round reward of the optimal arm in space \(\) and \((x):=^{*}_{}-(x)\) denote the gap between arm \(x\) and the optimal arm. Define \(_{j}=\{x:2^{-j}(x)<2^{1-j},j\}\), then set \(_{j}\) contains all arms whose gap is between \(2^{-j}\) and \(2^{1-j}\). Consider the \(\)-mesh \(_{j}\) for space \(_{j}\). Then the zooming dimension \(d_{z}\) for the action space \(\) is \(d_{z}=_{ 0}\{|_{j}|^{}, =2^{-j}, j\}\).

Covering dimension is a property of the action space while the zooming dimension is a property of the instance. Notice that we always have \(d_{z} d\). This is because the covering dimension considers the \(\)-mesh of the entire action space \(\), whereas the zooming dimension focuses only on the set \(_{j}\). The covering dimension is closely related to other notions of dimensionality in a metric space, such as the Hausdorff dimension, capacity dimension, and box-counting dimension, all of which characterize the covering properties in fractal geometry. Similarly, the zooming dimension is another measure used to evaluate the structure of a metric space. Both of these dimensions are widely utilized in the field of Lipschitz bandits. For further details and alternative formulations regarding the covering dimension and zooming dimension, refer to (Kleinberg et al., 2019).

## 3 Warm Up: Uniform Discretization Algorithm

This section provides the intuition, specification, and theoretical analysis of the Memory Bounded Uniform Discretization (MBUD) algorithm (shown in Algorithm 1) for the stochastic Lipschitz bandits problem.

Algorithm overview.To facilitate our discussion, we begin by outlining the core idea behind the algorithm. This algorithm employs a uniform discretization strategy and adopts an Explore-First methodology, which endeavors to identify a near-optimal arm following the exploration phase and dedicates the remaining rounds to exploitation. Throughout the exploration stage, the algorithm allocates two units of memory space: one for storing the best-estimated arm and another for temporarily holding a newly read arm. Note that the best-estimated arm serves a dual purpose: it is not only crucial for the exploitation phase but also enables the swift identification of sub-optimal arms.

The exploration phase in Algorithm 1 is divided into \( T\) phases, further structured into two main segments: the 'cross exploration phases' and the'summarize phase'. During the initial \( T-1\) phases, the algorithm iterates over the arms within the discretized action space to minimize regret. Exploration is limited to a subset of cubes at a time, allowing the algorithm to gather information about the optimal arm while minimizing regret. In the final phase, termed the'summarize phase', the algorithm revisits all arms within the uniform discretization space. Overall, each arm is read into memory twice to ensure thorough evaluation. Furthermore, we implement a budgeting strategy for each phase, wherein the total number of pulls across all arms is constrained by a predefined budget. The goal is to select the optimal arm with high probability after accumulating sufficient information during the previous phases. This structured approach balances exploration and exploitation under memory constraints, aiming to quickly identify the optimal arm while minimizing the sampling of suboptimal arms. The specifics of this approach will be detailed subsequently.

```
1:\(\), \(_{y} 0\), \(n_{y} 0\), \(B_{-1} 1\), \(=()^{1/(d+2)}\), \( T-1\).
2:for\(p=0,,-1\)do
3:\(B_{p}}\).
4:for\(q=1,,^{-d}\)do
5: Generate a new cube \(C(,,p,q)\), and select a arm \(\) from \(C\).
6:\((,_{y},n_{y})(c,,,_{y},n _{y}, B_{p})\).
7:endfor
8:endfor
9:for\(q=1,,^{-d}\)do
10: Generate a new cube \(C(,q)\), and select a arm \(\) from \(C\).
11:\((,_{y},n_{y})(c,,,_{y},n _{y}, B_{-1})\).
12:endfor
13: Play arm \(y\) until the end of the game. ```

**Algorithm 2** CrossCube

Exploration strategies.For the cross exploration phases, the gap between neighboring arms is \(\) (\(\) defined in Algorithm 1). There are \(O(^{-d})\) cubes (arms) in the discretization action set, which is an \(\)-mesh of \(\). Each cross exploration phase will only explore \(O()\) of them. We generate a new cube by using the function \(_{d}(a,b),a,b\) which converts the integer \(a\) to a \(d\)-dimension vector. And the \(i\)-th entry of the vector is the \(i\)-th right-most digit in base \(b\). To aid understanding, we offer several examples: \(_{3}(3,2)=(0,1,1)\), \(_{3}(1208,26)=(1,20,12)\), and \(_{2}(1208,26)=(20,12)\). The function could be done by a succession of Euclidean divisions by \(b\). For the summarize phase, the gap is \(\) and all cubes in the discretization set are explored.

The CrossCube function generates cubes for the cross exploration phases by calculating parameters based on the number of phases and the edge-length of the cubes. Specifically, CrossCube generates a new cube using a combination of two geometric sequences. It first calculates the parameters \(_{1}\) and \(_{2}\) as the maximum integers such that \(k^{d}\) and \(k^{d}^{-d}\), respectively. The function then determines the cube's position using these parameters and the edge-length \(\). The cube is defined by a node position generated by \(_{d}(p,_{1})\) and \(}_{d}(q,_{2})\), where \(_{d}\) is a geometric sequence generator that converts an integer to a \(d\)-dimensional vector. The GenerateCube function is similar to CrossCube but is used during the summarize phase to generate cubes without considering the phases. It calculates the parameter \(\) as the maximum integer such that \(k^{d}^{-d}\). The cube is then determined by the edge-length \(\) and a node position generated by \(_{d}(q,)\).

Compare strategy.Then we introduce the compare strategy, which is also useful for the MBAD algorithm described in the following section. The algorithm always selects the arm with the fewest pulls in the memory. After sufficient samples, it will eliminate one sub-optimal arm based on its upper confidence bound and then generate a new arm (i.e., read a new arm into the memory). Notice that the algorithm may prioritize two sub-optimal arms with a small gap. Therefore, there is a cap on the number of pulls each phase for any arm. It helps the algorithm in striking a balance between exploration (read a new arm) and exploitation (play arms in memory).

The algorithm maintains three statistics for one arm in memory: the index \(x\), the mean reward estimator \(_{x}\), and the number of pulls \(n_{x}\). The constant \(c\) is an exploration and exploitation balancing parameter. In the exploration part, there are \( T\) phases. Let \(B_{p}\) be the budget of samples for the \(p\)-th phase. We use \(y\) and \(x\) to denote the best-estimated arm and the new arm in the algorithm,respectively. If the upper confidence bound (UCB) of arm \(x\) is less than the lower confidence bound (LCB) of arm \(y\), then \(x\) is suboptimal with high probability. If the LCB of \(y\) is less than the LCB of arm \(x\), then \(x\) is not too bad with high probability. For the remaining cases, we could choose either \(x\) or \(y\), and we choose arm \(y\) at the end of the algorithm.

Flowchart.In Appendix A.1, we include a flowchart that illustrates the operation of the algorithm.

Theoretical result.The computational workload of the MBUD algorithm is characterized by a constant per-round operation, leading to a total time complexity of \(O(T)\), where \(T\) represents the number of rounds. Regarding space complexity, the MBUD algorithm necessitates the storage of merely two arms in memory at any given time. Additionally, the space requirements for the GenerateCube and CrossCube subroutines are minimal, each consuming \(O(1)\) units of space in terms of arm storage. Consequently, the overall space complexity of the algorithm is \(O(1)\).

We provide the theoretical result below and provide the details of the theoretical analysis in Appendix B. The result recovers the worst case regret in previous work and recovers the lower bound up to a logarithmic factor (Kleinberg, 2004).

**Theorem 1**.: _For the stochastic Lipschitz bandits problem with metric \((,)\) and time horizon \(T\), where \(=^{d}\) and \(\) is a known metric function. Algorithm 1 uses \(O(1)\) stored arms and achieves regret_

\[_{}(T)(T^{}),\]

_where \(d\) is the covering dimension of space \(\)._

The theoretical analysis is mainly based on the 'clean event', which holds that the observed mean average is a good estimator for the expectation with high probability. At a high level, the analysis shows that the deviation between the mean estimator of the best-estimated arm \(y\) and the optimal expected reward \(_{}^{*}\) is small enough when \(p 1\). Then the sub-optimal arms could be discarded quickly, which helps us to bound the incurred regret of sub-optimal arms and the exploitation phase. We bound the expected regret during all time horizons by considering the discretization error, the incurred regret of all sub-optimal arms during the exploration, and the sub-optimality of the selected arm before the exploitation together.

```
0: constant \(c\), arm \(x\) and \(y\), \(_{y}\), \(n_{y}\), \(b\).
1:\(_{x} 0\), \(n_{x} 0\).
2:while\(n_{x} b\) or \(n_{y} b\)do
3: Pull the least played arm between \(x\) and \(y\). If there is no single least played arm, select a random arm.
4: Update \(_{x}\), \(n_{x}\), \(_{y}\), \(n_{y}\).
5:if\(\{_{x}+},1\}<\{_{y}-},0\}\)then
6: Break and return \((y,_{y},n_{y})\).
7:elseif\(\{_{x}-},0\}>\{_{y}-},0\}\)then
8: Break and return \((x,_{x},n_{x})\).
9:endif
10:endwhile
11: Return \((y,_{y},n_{y})\). ```

**Algorithm 4** Compare

## 4 Adaptive Discretization Algorithm

This section provides the main idea, specification, and theoretical analysis of the Memory Bounded Adaptive Discretization (MBAD) algorithm (shown in Algorithm 5).

Algorithm overview.We begin with some intuitions. The MBUD algorithm achieves near-optimal regret in the worst case but fails to leverage the beneficial structure of 'nice' problem instances. To address this, we present the MBAD algorithm, which is based on adaptive discretization, and establish a near-optimal instance-dependent upper bound. The idea behind adaptive discretization is straightforward: the algorithm should focus more on promising regions. For instance, the zooming algorithm approximates the expected rewards over the action space and explores more in regions with a high probability of yielding high rewards. However, due to memory constraints, the algorithm cannot obtain a comprehensive picture of the action space over time. To overcome this obstacle, the MBAD algorithm employs a "round robin" strategy, storing the best-estimated arm as the next read arm in memory. Unlike the MBUD method, which chooses predetermined steps, the MBAD algorithm selects the next read arm based on the confidence radius of the arms in memory. Consequently, steps are smaller and probes (newly picked arms) are more numerous in promising regions.

```
1:\(y 0,_{y} 0,n_{y} 0\), \(B_{1}\).
2:for\(p=1,2,\)do
3:\(x 0\), \(_{x} 0\), \(n_{x} 0\), \(b_{p} B_{p}()^{1/(d+2)}\).
4:AdaptiveCube(4, 1).
5:\(B_{p+1} B_{p} T\).
6:endfor ```

**Algorithm 6** AdaptiveCube

Exploration strategies.The AdaptiveCube subroutine is the cornerstone of the MBAD algorithm, functioning as a recursive mechanism to navigate and leverage a cubic region within the decision space. This procedure dynamically adjusts the exploration granularity based on observed rewards and predetermined sampling constraints. Initially, the algorithm selects a cube \(C\) for exploration. If this cube is deemed sub-optimal compared to the optimal estimated arm stored in memory (denoted as arm \(\)), the algorithm discards this cube in favor of exploring a subsequent cube, following the generation rules outlined in the GenerateCube subroutine described in the MBUD algorithm (Section 3). Conversely, if the cube shows promise, the algorithm proceeds to explore within it, subdividing it into smaller subcubes for more detailed exploration. Each exploration phase is governed by a specific sample budget, which regulates the granularity of exploration to prevent excessive sampling of sub-optimal arms in the early stages. This adaptive exploration process continues until the entire action space has been thoroughly explored. The decision-making process is inherently dynamic, constantly evolving based on past actions to enhance the efficiency of future exploration and exploitation efforts.

To prevent the MBAD algorithm from "over-zooming", we implement two stop conditions. The first condition discards the current cube in favor of a new one once we are highly confident that the current cube is inferior to the best cube we've explored (see lines 4-5 of Algorithm 6). The second condition sets a lower bound on the edge length of the cube to be explored in each round, which gradually decreases as exploration progresses (see line 6 of Algorithm 6). These conditions together ensure the algorithm avoids over-zooming. In the initial learning phase, our knowledge of the optimal cube is limited, making it challenging to effectively distinguish suboptimal cubes using only the first stop condition. However, the second condition, with a larger initial lower bound on cube edge length, prevents over-zooming. As the learning process advances, the algorithm can more reliably eliminate suboptimal cubes, thus avoiding over-zooming on them.

Flowchart and algorithm description.Due to page limitations, Appendix A.2 contains a flowchart illustrating the operation of the algorithm along with its description.

Theoretical result.Analyzing the space complexity of the MBAD algorithm and its AdaptiveCube subroutine requires careful consideration due to the subroutine's recursive nature. Specifically, the conditional logic that triggers further recursion or partitioning into \(2^{d}\) subcubes adds layers of complexity. Within the AdaptiveCube subroutine, each recursive invocation contributes to the call stack, with space consumption directly proportional to the recursion depth. The space required to sustain the state of each cube, alongside the recursive call stack within AdaptiveCube, implies a complexity that scales linearly with recursion depth, complemented by constant overheads for variables preserved at each recursion level. Nonetheless, the algorithm's design allows for the direct computation of all parent and neighboring cube information from the current cube's coordinates and edge length, obviating the need for multiple cube storage in memory. Consequently, only a single cube needs to be maintained at any time during the AdaptiveCube process, affirming a space complexity of \(O(1)\) for the MBAD algorithm. This space complexity analysis directly informs the algorithm's time complexity. Similar to the MBUD algorithm, the overall time complexity of the MBAD algorithm remains linear with respect to the total number of rounds.

As a by-product of the MBAD algorithm, we introduce a simpler, more practical algorithm for scenarios where \(d_{z} 1\). Detailed descriptions and theoretical analyses of this algorithm can be found in Appendix D. We provide the theoretical result below and elaborate on the details of the theoretical analysis in Appendix C. The result establishes the optimal instance-dependent upper bound, up to a logarithmic factor, for the stochastic Lipschitz bandits problem. Previous works (Slivkins, 2014; Kleinberg et al., 2019) have already established related lower bounds, indicating that our work achieves near-optimal regret. While there are other forms of results, such as those presented in work (Magureanu et al., 2014), we believe that adopting one form is sufficient to demonstrate the near-optimal performance of our algorithm.

**Theorem 2**.: _For Lipschitz bandits with time horizon \(T\) and Lipschitz constant \(L\), Algorithm 5 with \(c 5\) achieves regret_

\[_{}(T)(T^{+1}{d_{z}+2}}),\]

_using \(O(1)\) stored arms, where \(d_{z}\) is the zooming dimension of space \(\)._

We also mainly consider the clean event. The algorithm plays in a 'round-robin' manner. There are at most \(O( T)\) phases because of the delicate design of the budget for each phase. For each phase, we show that the deviation between the mean reward of the best-estimated arm and optimal expected per-round reward \(^{*}_{}\) is small. Then the algorithm could approximately adjust the sub-optimality of arms and set more probes in more promising regions. Then we prove that the incurred regret could be bounded by \(O(T^{}( T)^{})\) by bounding the pulls of bad arms according to the definition of zooming dimension.

## 5 Numerical Evaluations

In this section, we show the efficiency of our algorithms through a series of numerical simulations. The baseline consists of three algorithms: the uniform discretization with UCB1 algorithm (UD) and the zooming algorithm. For the uniform discretization, we pick a fixed \(\)-mesh of the action space and run the UCB1 algorithm only considering the finite uniform discretization action space. The UCB1 algorithm is a popular algorithm for achieving near-optimal regret with finite action space. Kleinberg (2004) prove that the uniform achieves optimal worst-case regret up to logarithm factors. The zooming algorithm (Kleinberg et al., 2019) is an implementation of the adaptive discretization strategy, which deploys more probes in regions deemed more 'promising'. Theoretical analysis shows that the zooming algorithm both achieves optimal worst-case regret and instance-dependent regret up to logarithm factors.

We set \(=\) and choose the reward function \(f(x)=0.5-|x-0.5|\). In each round \(t\), the algorithm plays one arm \(x_{t}\) and receives a stochastic reward \(y\) satisfying

\[y=f(x)+,&0 f(x)+ 1\\ 1,&f(x)+>1\\ 0,&f(x)+<0.\]

Specifically, \((0,0.1^{2})\) is the Gaussian noise. The MBAD algorithm are only allowed to store two arms in memory, while there is no memory constraint for the UD+UCB1 algorithm and the zooming algorithm. All results are averages over 50 runs. Figure 1 displays the results obtained across varying time horizons, where the horizontal axis denotes the time horizon and the vertical axis measures regret. From the figure, we have that MBAD algorithm significantly outperforms the UD strategy. Additional numerical results are detailed in Appendix E.

## 6 Conclusion and Discussion

We consider the Lipschitz bandits with limited memory problem. We introduce two novel algorithms: the Memory Bounded Uniform Discretization (MBUD) algorithm and the Memory Bounded Adaptive Discretization (MBAD) algorithm, which are predicated on the principles of uniform and adaptive discretization, respectively. Theoretical analyses reveal that the MBAD algorithm achieves near-optimal performance with \(O(1)\) stored arms and \(O(T)\) time complexity, highlighting its efficiency and practical applicability. Moreover, numerical results show the efficiency of our algorithms.

The Lipschitz bandit problem in higher dimensions is often perceived as a 'needle in a haystack' problem. Intuitively, finding the optimal solution in such high-dimensional spaces seems extremely challenging, but this perception does not always hold in practice. Many scenarios reveal beneficial structures within Lipschitz bandits, which is why our research emphasizes not only worst-case regret but also instance-dependent regret. Our proposed algorithm achieves nearly optimal time and space complexity for both worst-case and instance-dependent regrets.

In practical applications, Lipschitz bandit problems are found in areas such as non-parametric estimation, model selection in machine learning tasks, and decision-making processes in robotics and games. Furthermore, research on Lipschitz bandits has inspired algorithmic advancements in other domains, such as decision trees and tree-based methods, where the principles from Lipschitz bandit algorithms guide the splitting and growth of trees. Despite these advancements, certain limitations remain. High-dimensional Lipschitz bandits can still pose significant computational challenges, especially in cases where the underlying structure is less apparent or more complex. Additionally, the requirement for sufficient exploration to accurately estimate the optimal arm can lead to increased computational overhead in large action spaces.

Our algorithm introduces a novel framework that efficiently addresses online decision-making and balances exploration and exploitation in Lipschitz action spaces. This framework leverages beneficial structures in the problem space to enhance performance while maintaining computational efficiency. We hope our approach could make a substantial contribution to the community, especially in areas that require efficient and effective decision-making under uncertainty.

Figure 1: The results obtained with different time horizons.