# Multi-LLM Debate: Framework, Principals, and Interventions

Andrew Estornell

ByteDance Research

andrew.estornell@bytedance.com &Yang Liu

University of California, Santa Cruz

yangliu@ucsc.edu

###### Abstract

The flexible and generalized nature of large language models has allowed for their application in a wide array of language-based domains. Much like their human contemporaries, these models are capable of engaging in discussions and debates as a means of improving answer quality. We first take a theoretical approach to analyzing debate and provide a framework through which debate can be mathematically examined. Building on this framework, we provide several theoretical results for multi-agent debate. In particular, we demonstrate that similar model capabilities, or similar model responses, can result in static debate dynamics where the debate procedure simply converges to the majority opinion. When this majority opinion is the result of a common misconception (possibly ingrained in the models through shared training data) debate is likely to converge to answers associated with that common misconception. Using insights from our theoretical results, we then propose three interventions that improve the efficacy of debate. For each intervention, we provide theoretical results demonstrating how debate is improved. We also demonstrate that these interventions result in better performance on four common benchmark tasks.

## 1 Introduction

Large language models (LLMs) have demonstrated a remarkable ability to perform unseen tasks with high efficacy. This behavior, often referred to as emergent, allows LLMs to serve as general-purpose tools for a wide array of language-based functions. One such behavior of particular interest is the ability of LLMs to intake and process opinions from other models (or humans). As shown in several previous works, this ability allows LLMs to collaboratively solve tasks by engaging in _debate_ Chan et al. (2023); Liang et al. (2023); Du et al. (2023). For a given task, multi-agent debate operates by eliciting responses from each model, distributing those responses among the models, and then eliciting updated responses from each model.

In this work, we aim to explore the debate procedure, by first providing a theoretical framework through which debate can be better understood. This framework draws inspiration from Bayesian inference and in-context learning, showing that debate can be partially viewed as a special type of in-context learning. Through this framework, we then provide several theoretical insights into the debate procedure. In particular, we demonstrate the susceptibility of multi-agent debate to echo-chamber effects. These echo-chamber effects are especially consequential when they stem from a shared misconception between a majority of models, which can arise from circumstances such as highly correlated training data between each model.

We then leverage results from our theoretical framework to improve the efficacy of the debate procedure. In particular, we propose three _interventions_ (modifications to the debate procedure). First, _diversity-pruning_ which aims to maximize the information entropy in model responses at each round of debate; this intervention has the added benefit of reducing the severity of the echo chambereffect. Second, _quality-pruning_ which aims to maximize the relevance of each model's response. We demonstrate that this intervention improves the likelihood that the debate procedure converges to correct answers. Lastly, _misconception-refitation_ which directly identifies, and attempts to refute misconceptions in model responses. This intervention takes inspiration from works such as Robinson et al. (2022) which demonstrate that LLMs are often more skilled at _evaluating_ answers, compared with directly providing answers. For each of our interventions, we provide theoretical results outlining the way in which each improves debate. We also conduct experiments on four common benchmarks demonstrating that these interventions improve debate efficacy in practice.

**Our contributions** 1) We propose a theoretical framework for multi-LLM debate that draws on connections from in-context learning and Bayesian inference. 2) We provide theoretical insights on several key principles of multi-LLM debate. 3) Using these insights, we design three debate interventions which result in consistent improvement to debate, across four language benchmarks (BoolQ, MMLU, MathQ, TruthfuQA) and three families of models (GPT, Lama, and Mistral).

## 2 Related Work

Our work is closely related to multi-agent debate, which focuses on iterative collaboration between agents to make a decision Chan et al. (2023); Liang et al. (2023); Du et al. (2023); Khan et al. (2024); Irving et al. (2018); Michael et al. (2023); Rasal (2024); Pham et al. (2023); Chang (2024). These works often focus on multi-agent debate in the context of question-answering tasks and aim to provide higher quality answers (compared to those of a single model) by engaging multiple models in discussion. The preliminary debate framework, proposed by Du et al. (2023), facilitates debate by first asking each model a question, and then iteratively re-asking agents that same question contextualized by the responses of all models in the previous round. Different variants of this procedure have been proposed: debate where models have different functionality Liang et al. (2023), round-robin style debate Chan et al. (2023), dynamically controlling the level of disagreement between agents in debate Chang (2024), or using judges to assess the correctness of debaters Khan et al. (2024). Other techniques for iteratively improving the quality of answers have also been proposed, e.g., chain-of-thought Wei et al. (2022); Kojima et al. (2022), self-consistency Wang et al. (2022); Singhal et al. (2023), and self-reflection Ren et al. (2023).

Similar to debate, there have been investigations into the use of different LLMs to engage with one-another Liu et al. (2023); Abdelnabi et al. (2023); Zhang et al. (2023); Li et al. (2023); Park et al. (2023), explain their rational to others Wang et al. (2023), or collaboratively engage in general tasks Li et al. (2023); Wang et al. (2023); Park et al. (2023); Wu et al. (2023); Hong et al. (2023); Li et al. (2023, 2023); Tsao and AILAB (2023). While debate has shown promise in a wide range of domains, several works have also demonstrated that the debate process can be unstable and can lead to worse performance than using just a single model Wang et al. (2024); Smit et al. (2023).

Our work is also related to in-context learning and Bayesian inference. The former, Brown et al. (2020); Min et al. (2022); Lampinen et al. (2022) demonstrates that LLMs can perform unseen tasks when provided only a few examples of that task. Other works Xie et al. (2021); Jiang (2023) have shown a connection between in-context learning and Bayesian inference; the additional examples provided to the model can be viewed as updates to the model's posterior distribution over tokens.

## 3 Preliminaries

DebateLet \(\) be a given question, with associated answer \(\), for example \(=\)_"What color is the sky?"_ and \(=\)_"Blue"_. Following the debate procedure proposed by Du et al. (2023), a collection of \(n\) LLMs (also referred to as agents) collaborate to infer the correct answer \(\) by iteratively engaging in discussion over \(T\) rounds, as described next:

* At round \(t=0\) each agent \(i\) observes task \(\), then provides response \(_{i}^{(0)}\).
* At rounds \(t>0\) each agent \(i\) observes task \(\) and the outputs of the \(n\) agents at the previous timestep \(Z^{(t-1)}=(_{1}^{(t-1)},,_{n}^{(t-1)})\), then provides response \(_{i}^{(t)}\).
* The debate process ends if \(t=T\) or if agents reach a consensus.

To measure if consensus is reached a function \(a\) extracts an _answer_1 from a given response \(\). Suppose \(=\)_"During the day, the sky is blue"_, then \(a()=\)_"Blue"_. At round \(t\), the probability that agent \(i\) provides response \(_{i}^{(t+1)}\), is given by

\[_{}_{i}^{(t+1)}}_{}}_{},=( _{1}^{(t)},,_{n}^{(t)})}_{},}_{}\] (1)

where \(_{i}\) captures model hyperparameters (such as training data, architecture, etc.). Both the input \((Z^{(t)},)\) as well as the hyperparameters \(_{i}\), ultimately influence the output \(_{i}^{(t+1)}\). Note that on each round, all agents observe the _same_ input, namely \((Z^{(t)},)\). Thus, differences in agent output \(_{i}^{(t+1)}\) are determined by both the stochastic nature of output generations, and the unique parameters \(_{i}\) of each model. For notational convenience, we drop the subscript in \(_{}\) when the parameters \(_{i}\) are given, and simply write \((|_{i})\).

The key distinction between our approach and "vanilla" debate, is that we will leverage latent concepts (discussed next) to modify the responses in \(Z^{(t)}\) in between each round of debate.

Latent ConceptsCentral to our investigation is the idea of _latent concepts_ in language generation. As outlined by Xie et al. (2021); Jiang (2023) latent concepts capture the idea that language is not generated at random (either by humans or by models). Rather, when generating language, we first have an idea or an intention form in our minds; we then select words that we believe will convey that underlying idea or intention. More formally, let \(\) be a _latent concept space_ and let \(\) be a _concept_. Following Xie et al. (2021), tasks \(\), and their associated answer \(\) are generated by first selecting a vector of latent concept \(\) and then sampling \((,) D()\), where \(D\) is some distribution mapping concepts to task-answer pairs. Similarly, when providing responses, models will observe \(\), and infer the latent concept \(\), or more generally a distribution over the latent concept space, and then generate a response according to those inferred concepts, i.e., the model's generation probability in Equation 1 can be expressed as

\[_{i}^{(t+1)}\;,Z^{(t)},_ {i}=_{}_{i}^{(t+1) }\;,,Z^{(t)},_{i} |\;,Z^{(t)},_{i}\] (2)

Note that the above holds by the law of total probability for any choice of latent concept space.

To provide a more concrete example of latent concepts, consider the question-answering task in the BoolQ dataset. One of the questions in this dataset is "Did Abraham Lincoln write the letter in the film Saving Private Ryan?" to which the correct answer is "Yes". The latent concept, in this case, corresponds to the actual scene in the movie where the Bixby letter (written by Lincoln) is read to a group of soldiers. Just as in our case, first a concept \(\) is drawn, e.g., the film is made; then from the film, a string \(\) is sampled, i.e., the previous question about the film.

For another example of latent concepts, we can think of arithmetic calculations such as multiplication. When we wish to express multiplication through language, we may write something like "\(4*4\)". The latent concepts behind this string are the mechanisms of multiplication (e.g., multiplication is just iterative addition, and addition itself is simply increasing the value of a number by one iteratively). These examples are intended to be easily digestible. However, latent concepts can also be significantly more abstract, such as a vector in some unknown embedding space.

## 4 A Theoretical Formulation of Multi-Agent Debate

We begin by providing a theoretical formulation of multi-agent debate. This formulation will provide key insights into the inner workings of the debate procedure, which we will use to improve debate.

The key behind our framework is to use the idea of latent concepts and expansion of each model's generation probability (Equation 2) in order to better understand debate. Prior to presenting our theoretical framework, we first state an important assumption.

**Assumption 4.1**.: For a given latent concept space \(\), the probability of generating response \(_{i}^{(t+1)}\) is conditionally independent of both the responses \(Z^{(t)}\) and the task \(\), given concept \(\) and model parameters \(_{i}\), i.e., \(_{i}^{(t+1)}\;,,Z^{(t)},_{i}=_{i}^{(t+1)}|\;, _{i}\).

Assumption 4.1 can be interpreted as saying that a model's generation \(_{i}\), is uniquely determined by the model's parameters \(_{i}\) and the concepts identified by a model, namely \(\). In the case of encoder-decoder-based models, one can conceptualize the joint between \(\) and \(\) as corresponding to the embedding produced by the encoder. With this embedding in hand, the original input (\(\), \(Z^{(t)}\)) no longer influences the model's output, rather the embedding and model parameters will uniquely determine the model's output.

Next, we derive the following lemma which will be useful in examining the way that model responses evolve debate rounds.

**Lemma 4.2**.: _The generation of model \(i\) at time \(t+1\) can be expressed as,_

\[_{i}^{(t+1)}|\;Z^{(t)},,_{i}_{} _{i}^{(t+1)}|,_{i} |,_{i}|_{i} }_{}\;^{n} _{j}^{(t)}|\;,_{i}}_{ }\]

The significance of this lemma is that we are able to express the probability of generating a given response \(_{i}^{(t+1)}\)_with_ the other model responses \(Z^{(t)}\) in terms of the probability of generating \(_{i}^{(t+1)}\)_without_ the other model responses and a skew term caused by those model responses. Note that,

\[_{i}^{(t+1)}|,_{i} _{}_{i}^{(t+1 )}|,_{i}|,_{i}|_{i}\]

Thus, we can think of \(_{i}^{(t+1)}|\;Z^{(t)},,_{i} \) as a weighted version of \(_{i}^{(t+1)}|\;,_{i}\), where the weights are given by the skew term \(_{j=1}^{n}_{j}^{(t)}|\;,_ {i}\).

Debate and In-Context LearningTo help conceptualize the role of latent concepts in debate, we discuss the work of Xie et al. (2021), which uses Bayesian inference over latent concepts to understand in-context learning. There are natural connections between in-context learning and multi-agent debate. In-context learning works as follows: given a task \(\) and a model \(f\), select several examples of task-answers pairs \((_{1},y_{1})(_{m},y_{m})\) which are _similar_ to \(\). Then prompt the model \(f\) for an answer to task \(\), given examples \((_{j},y_{j})\), i.e. \(=f\;(_{1},y_{1})( _{m},y_{m})\). A key result of Xie et al. (2021) is that latent concepts in the examples \((_{j},y_{j})\), particularly concepts shared between many examples, influence the model's answer \(\). For any concept where \(|\;(_{1},y_{1})(_{m},y _{m})\) is large relative to other concepts (i.e., there is a shared concept \(\) between the examples), the model is more likely to give response \(\) which also share that concept. Model responses at the previous round \(Z^{(t)}\) serve a similar function to the examples of in-context learning. The model's updated response at time \(t+1\), namely \(_{i}^{(t)}\), is influenced by concepts shared between the responses in \(Z^{(t)}\). The skew term in Lemma 4.2 provides a glimpse of how latent concepts conveyed by \(Z^{(t)}\) will influence the generation of \(_{i}^{(t)}\), namely that \(_{j=1}^{n}_{j}^{(t)}|\;,_ {i}\) reweighs the model's generation.

### Debate Objective

Through this perspective of debate we can more effectively design debate procedures by leveraging the concept space \(\). To do this, we will first formulate debate as an optimization problem where the _skew term_, described in Lemma 4.2, corresponds to the optimization variables. For a given task \(\) and answer \(y\), each round of debate can be formulated as the following optimization problem.

\[_{Z^{(t)}}_{i=0}^{n}a(_{i}^{(t+1)})=y| \;Z^{(t)},_{i}\]

At time \(t\) we aim to craft responses \(Z^{(t)}\) such that they maximize the probability of providing the correct answer at the next time step. Expanding this objective over the latent concept space \(\), yields

\[_{Z^{(t)}}\;_{i=1}^{n}_{} a(^{(t+1)})=y|\;,_{i} |\;_{i} |\;,_{i}_{j=1}^{n} _{j}^{(t)}|\;,_{i}\] (3)The key challenges with directly optimizing this objective are: 1) the true concept \(^{*}\) from which \(\) and \(y\) originate, as well as the relationship between \(_{j}^{(t)}\) and the underlying concepts, is unknown, 2) the responses in \(Z^{(t)}\)) are natural language. However, will allow us to design several approaches within the concept space to better optimize debate. To motivate these approaches, we first need to make several observations about the debate procedure as a whole.

## 5 Debate Principals

In this section, we discuss factors that affect the efficacy of LLMs debate. In particular, we look at the role of information diversity, both in terms of the diversity of responses in \(Z^{(t)}\) as well as diversity in model capabilities. We see that a lack of diversity in either aspect is detrimental to the debate process. Lastly, we study a particular type of homogeneity in debate, namely shared misconceptions in which a large portion of models all share a similar erroneous belief about the task \(x\).

### Information Diversity

We begin by examining how the debate procedure is affected by both the diversity of model abilities and the diversity of model responses. Homogeneity in either ability or responses will bias the debate procedure towards certain latent concepts.

Similar Model CapabilitiesSuppose the debate process is conducted with only one type of model (in effect \(n\) copies of the same model). That is, \(_{i}\) for all \(i[n]\). Then, as the number of agents increases, the debate procedure is more greatly impacted by the echo chamber effect, i.e., the probability that a round of debate results in a change to the most likely concept, perceived by agents, approaches \(0\). That is, a greater number of similar agents results in static debate dynamics, in essence defeating the purpose of debate.

**Theorem 5.1**.: _Suppose all \(n\) agents have identical configurations (\(_{i}\) for all \(i\)). For round \(t>0\) let \(^{(t)}=_{} \,|\,,Z^{(t)},\) and \(^{(t+1)}=_{} \,|\,,Z^{(t+1)},\), i.e., \(^{(t)}\) and \(^{(t+1)}\) are the concepts most likely to be inferred by a model with parameters \(\) when given task \(\) and responses \(Z^{(t)}\), \(Z^{(t+1)}\) respectively. Then \(^{(t)}=^{(t+1))} 1\) as \(n\)._

We defer a full proof to the Supplement, Section A. Theorem 5.1 implies that when debate is conducted with multiple copies of the same model (or very similar models), increasing the number of models results in debate centering on a single (unchanging) concept, rather than a balanced distribution over multiple concepts.

Similar Model OpinionsNext, we examine how similar responses impact the collaboration process. At time \(t\) suppose that there are \(n\) responses \(Z^{(t)}\) and at least \(m\) of those responses are _similar_, i.e., there exists some concept \(^{}\) such that \(^{}=_{}\,|\,_{j}^{(t)},_{i}\) for all \(j m\). That is, each of the \(m\) responses has a shared "most likely" concept when viewed by a model with parameters \(_{i}\).

**Theorem 5.2**.: _Suppose that \(Z^{(t)}\) contains at least \(m\) responses with the property that \(^{}=_{}_{i}^{(t)}\,|\,,_{i}\). Then, as \(m\) the model's generation at the next round (\(t+1\)) becomes uniquely determined by a single concept \(^{}\) i.e. \(_{i,1}^{(t+1)}\,|\,Z^{(t)},,_{i}}{_{i,2}^{(t+1)}\,|\,Z^{(t)}, ,_{i}}_ {i,1}^{(t+1)}\,|\,^{},_{i}}{ _{i,2}^{(t+1)}\,|\,^{},_{i}}\) for all response pairs \(_{(i,1)}^{(t+1)},_{(i,2)}^{(t+1)}\)._

We defer a full proof to the Supplement Section A. Theorem 5.2 indicates the susceptibility that LLM debate has towards tyranny of the majority. If a large number of models provide similar responses to a task \(\), then those repeated answers will drown out the single provided by the other models' responses, as well as the task \(\) itself. In Section 7 we demonstrate that this occurs in practice.

### Shared Misconceptions

Next, we study a particular type of homogeneity in model capabilities and responses, namely shared misconceptions. When a common misconception is shared among the models, debate is less effective and is likely to converge to erroneous concepts associated with the shared misconception. We now formalize the notion of misconceptions.

**Definition 5.3**.: **(Misconception)**: For a given concept \(^{*}\), a model with parameters \(i\) is said to have a _misconception_ regarding \(^{*}\) if there exists another concept \(^{}\) s.t.,

That is, for tasks generated from the concept \(^{*}\), the model believes that the erroneous concept \(^{}\) explains more than half of the tasks better than the true concept \(^{*}\).

There is said to be a _shared misconception_ if \(m\) of agents have a misconception and share the same erroneous concept \(^{}\). When the models share a common misconception the responses produced by those models are biased towards the erroneous concept \(^{}\).

**Theorem 5.4**.: _Let the true concept be \(^{*}\) and suppose that \(m\) of the \(n\) agents have a shared misconception for erroneous concept \(^{}\). Then, task and answer \((,y) D(^{*})\) expected average correctness of the debate procedure at the final round \(T\) is monotonically decreasing with \(m\), i.e., \(_{i}^{n}a(_{i}^{(T)})=y\) is decreasing with \(m\)._

We defer the full proof to the supplement Section A. It should be noted that the phenomenon of converging to erroneous concepts is unlikely to be mitigated by adding more models. When the misconceptions of one model are formed through training data, it is likely that other models will possess the same misconception unless specifically trained to avoid such errors due to the high correlation in training data between models.

## 6 Interventions

In this section, we discuss several modifications to the debate procedure, referred to as _interventions_. We break our interventions into two categories: **Pruning** which focuses on choosing which responses to keep in \(Z^{(t)}\), and **Modifying** which focuses on changing or editing the responses \(Z^{(t)}\).

### Pruning Interventions

At round \(t\) of debate, running interventions work by selecting only a subset of responses \(Z^{(t)}\) from \(Z^{(t)}\) before starting the next round \(t+1\). When using a pruning intervention, the models at round \(t+1\) will see only the pruned response set \(Z^{(t)}\), rather than the full response set \(Z^{(t)}\).

Diversity PruningLet KL represent Kullback-Leibler divergence. The diversity pruning intervention selects \(k\) of the \(n\) responses in \(Z^{(t)}\) which maximizes information entropy, i.e.,

\[Z^{(t)}=[_{i},_{j}  Z.}{}}}D(|\,_ {i}),\ D(|\,_{j}.))\] \[\ |Z|=k\]

Quality PruningQuality pruning aims to select the \(k\) responses in \(Z^{(t)}\) with the highest similarity to the task \(x\). Similar to diversity pruning, quality pruning selects \(k\) of the \(n\) responses at time \(t\). Rather than selecting for diversity, quality pruning aims to select the \(k\) highest question responses. This is done by selecting the \(k\) responses which maximize

\[Z^{(t)}=}{}}}_{_{i}, Z}}}}D(|\,.),\ D(|\,_{i})\] \[\ |Z|=k\]

In practice computing \(}}D(|.),D(|_{i}.)\) or \(}}D(|_{i}.),D( {}|_{j}.)\) is intractable. However, sentence embedding can be used as a proxy for these values. Section C discusses this in further detail.

Next, we show that when models have a shared misconception, diversity pruning decreases the likelihood that the debate procedure will converge to the erroneous concept corresponding to the shared misconception.

**Theorem 6.1**.: _Let the true concept be \(^{*}\) and suppose that at least \(n/2\) agents have a shared misconception for erroneous concept \(^{}\). Then diversity pruning decreases the probability that debate converges to an answer \(y^{}\) which is sourced from the erroneous concept \(^{}\), i.e. \(y^{} D(^{})\)._We defer the full proof to the Supplement, Section A.

**Theorem 6.2**.: _For a given task-answer pair \((,y)\) quality pruning increases the probability that debate converges to the correct answer, i.e. let \(Z^{(t)}\) be the set of all responses at time \(t\) and \(Z^{(t)}\) be the result of quality pruning, then \(_{i=1}^{n}(a(_{i}^{(t+1)})=y|,Z^{(t)}, _{i})>_{i=1}^{n}(a(_{i}^{(t+1)}=y| ,Z^{(t)},_{i})\)._

We defer the full proof to the Supplement, Section A.

**Remark 6.3**.: As shown by Theorems 6.1 and 6.2, diversity pruning decreases the probability that debate converges to incorrect answers sourced from a particular concept, while quality pruning increases the probability that debate converges to a correct answer sourced from the true concept. Both interventions can be used simultaneously to guide the debate procedure more effectively away from wrong answers and towards correct answers.

### Modification Interventions

Misconception RefutationIn addition to selecting which responses in \(Z^{(t)}\) will be used in the next round of debate, we can also modify the responses in \(Z^{(t)}\). Misconception refutation aims to do precisely this by updating response \(_{j}^{(t)}\) to be more relevant to the task \(\).

\[_{j}^{*}=_{}D( |),\ D(|\ )-D(| _{j}^{(t)}),\ D(|\ )\]

Similar to Diversity Pruning and Quality Pruning, the distributions in the above objective are intractable in practice. As such, we use a proxy to update each response \(_{j}^{(t)}\), specifically produce \(_{j}^{*}\) by having an LLM minimally modify the given response \(_{j}^{(t)}\). The model is first prompted for a list of misconceptions and errors identified in the response. Given the list of misconceptions, the model is asked for both a refutation of the misconception and a corrected version of the response. For more details, see Section C of the Supplement.

**Theorem 6.4**.: _For task-answer pair \((,y)\), misconception refutation increases the probability of debate converging to the correct answer, i.e. let \(Z^{(t)},Z^{*(t)}\) be the responses before and after misconception refutation, then \(_{i=1}^{n}(a(_{i}^{(t+1)})=y|,Z^{*(t)}, _{i})>_{i=1}^{n}(a(_{i}^{(t+1)}=y| ,Z^{(t)},_{i})\)._

## 7 Experiments

Experimental DesignWe conduct experiments on four common language model benchmarks. **BoolQ**Clark et al. (2019), which consists of \(3,270\) yes-no questions, **MMLU**Hendrycks et al. (2020) which consists of \(13,869\) multiple-choice questions (we use the \(3,406\) high-school-level questions), **TruthfulQA**Lin et al. (2021) which consists of \(817\) open-ended questions, and **MathQ** which consists of \(3,000\) arithmetic questions of the from \(a b c+d e f\). In the BoolQ, MMLU, MathQ, datasets model correctness is measured through regular expression matching. In the TruthfulQA dataset, model correctness is measured via an LLM judge (we use GPT-4 as the judge in all experiments)

We use four LLMs of increasing capability, **GPT-3.5** (GPT-3.5 Turbo) OpenAI (2022), **Llama-2** (Llama-2 7B Chat) Touvron et al. (2023), **Llama-3** (Llama-3 8B Instruct) Meta AI (2024), and **Mistral** (Mistral 7B Instruct v0.2) Jiang et al. (2023). For sentence embeddings (which serve as a proxy of the latent concepts \(\)), we use sentence embeddings from **ADA-2**OpenAI (2022). We compare a combination of our three interventions **Ours** (see Algorithm 1 full details) with the debate paradigm of Du et al. (2023) (Society of Minds) **SoM**.

We begin by making several empirical observations about the multi-agent debate process.

Tyranny of the MajorityFirst, we examine the susceptibility of models towards agreement with the majority opinion. That is, how likely are models to give a specific answer at round \(t+1\) when \(m\) of the models provided that specific answer at round the previous round (round \(t\))? For example, in BoolQ suppose the specific answer is "Yes", then we want to know: how likely is a model to give a "Yes"-answer at round \(t+1\) if that model observes \(m\) "Yes" answers at round \(t\).

To measure this, we first select a random target answer, e.g., "Yes", and then prompt \(m\) of the models (out of 11) to provide responses "Yes"-answers2 (while the other 11-\(m\) models are prompted to provide a different randomly selected answer). These 11 responses make up \(Z^{(t)}\), we then test each model's likelihood of providing the target at round \(t+1\) when observing \(Z^{(t)}\) before diversity pruning (solid) and after diversity running (hatched).

In Figure 1, we see models are susceptible to echo chamber effects (this phenomenon is predicted by Theorem 5.1). The likelihood of providing the majority answer increases when \(Z^{(t)}\) contains more instances of the majority answer (i.e., as \(m\) increases). Figure 1 also demonstrates that diversity pruning (with \(k=5\)) reduces this echo chamber effect. See the Supplement for details.

Diversity of OpinionsNext, we examine the effectiveness of SoM and our method as a function of opinion diversity. Figure 2 shows the average accuracy improvement of SoM (dashed) and our method (solid) over single model performance (i.e., average performance at round \(t=0\)), as a function of the similarity between all responses at round \(t=0\) of debate (measured via pairwise cosine similarity). We see that for BoolQ, MMLU, and TruthfulQA, SoM is less effective when the similarity between responses increases. This observation is predicted by Theorems 5.1 and 5.2, which show that debate, without interventions, is less effective when model responses are too similar.

We see that our method's improvement compared to SoM is greatest when model opinions are more similar (cosine similarity close to 1). Note that the MathQ benchmark, where responses consist primarily of arithmetic, serves as a counter-example to these observations. This is due to the fact that sentence embedding of any two arithmetic expansions will be similar, regardless of their _true_ similarity; as such, the cosine similarity between embedding is less meaningful on this benchmark.

Debate InterventionsNow, we examine the effectiveness of a combination of our three debate interventions (see Algorithm 1 for full details of how the interventions are combined). We begin with a per-round performance of our method and SoM, as shown in Figure 3. We see that typically, the advantage of our method over debate arises in the later rounds of debate. Next, in Table 1, we present a full set of results for single models, SoM, and a combination of our three interventions. In all cases, our method is either competitive with, or superior to, SoM.

Figure 1: Probability that each model echoes the majority answer at round \(t=11\), as the number of responses at time \(t=0\) gives that majority answer (debate between 12 models are used).

Figure 2: Average accuracy improvement as a function of response diversity at round \(0\) of debate.

In addition to providing results for the combination of our interventions, we also investigate the effectiveness of each intervention applied individually (see Table 3 of the Supplement). These results indicate that our method is most successful when applying all three interventions simultaneously. In fact, some interventions can be detrimental to the debate process when applied in isolation. This is expected as each intervention is inherently designed to be complementary.

## 8 Limitations

While we aim to address some of the fundamental issues of multi-LLM debate, such as tyranny of the majority, there are several factors that need to be considered when adopting our framework. Firstly, our theoretical results leverage a latent concept space, which may not be accessible in practice, necessitating the use of proxies such as sentence embeddings. Reliance on proxies is particularly consequential for quality and diversity pruning; these interventions are less effective in domains where sentence embeddings are less meaningful, e.g., arithmetic questions. Additionally, our interventions can increase the inference time of the debate procedure. Increased inference time stems primarily from misconception refutation, as this intervention requires re-prompting each debater multiple times.

## 9 Conclusion

Multi-agent debate is an effective tool for improving the efficacy of LLM responses. However, debate is naturally susceptible to issues such as tyranny of the majority and shared misconceptions between models. By making use of our theoretical framework for debate, we are able to establish interventions for the debate procedure which help to alleviate these issues and improve the general performance of multi-agent debate. We saw that diversity pruning reduces the influence of similar responses. This is especially helpful in settings where the majority of agents provide incorrect responses that share a common error. A combination of all three interventions consistently leads to better debate.

   & **Single** & **SoM** & **Ours** & **Single** & **SoM** & **Ours** \\    &  &  \\  \(6\) GPT-3.5 & \(.80_{ 0.14}\) & \(.84_{ 0.12}\) & \(.85_{ 0.12}\) & \(.73_{ 0.14}\) & \(.74_{ 0.16}\) & \(.79_{ 0.014}\) \\  \(6\) Llama-3 & \(.76_{ 0.14}\) & \(.78_{ 0.13}\) & \(.78_{ 0.13}\) & \(.67_{ 0.16}\) & \(.70_{ 0.015}\) & \(.75_{ 0.014}\) \\  \(6\) Llama-2 & \(.67_{ 0.17}\) & \(.68_{ 0.17}\) & \(.73_{ 0.16}\) & \(.41_{ 0.17}\) & \(.47_{ 0.18}\) & \(.52_{ 0.18}\) \\  \(6\) Mistral & \(.80_{ 0.14}\) & \(.82_{ 0.13}\) & \(.85_{ 0.12}\) & \(.66_{ 0.16}\) & \(.65_{ 0.16}\) & \(.66_{ 0.16}\) \\  \(3\) GPT-3.5 \(+3\) Llama-3 & - & \(.82_{ 0.14}\) & \(.84_{ 0.13}\) & - & \(.73_{ 0.16}\) & \(.78_{ 0.16}\) \\  \(3\) GPT-3.5 \(+3\) Mistral & - & \(.83_{ 0.13}\) & \(_{ 0.12}\) & - & \(.69_{ 0.17}\) & \(.72_{ 0.16}\) \\  \(3\) Llama-3 \(+3\) Mistral & - & \(.80_{ 0.14}\) & \(.80_{ 0.14}\) & - & \(.69_{ 0.17}\) & \(.74_{ 0.16}\) \\    &  &  \\  \(6\) GPT-3.5 & \(.61_{ 0.03}\) & \(.63_{ 0.02}\) & \(_{ 0.30}\) & \(.53_{ 0.35}\) & \(.88_{ 0.16}\) & \(.93_{ 0.01}\) \\  \(6\) Llama-2 & \(.47_{ 0.34}\) & \(.52_{ 0.35}\) & \(.55_{ 0.34}\) & \(.11_{ 0.013}\) & \(.13_{ 0.014}\) & \(.19_{ 0.015}\) \\  \(6\) Llama-3 & \(.53_{ 0.35}\) & \(.55_{ 0.032}\) & \(.55_{ 0.032}\) & \(.25_{ 0.16}\) & \(.33_{ 0.17}\) & \(.48_{ 0.018}\) \\  \(6\) Mistral & \(.48_{ 0.34}\) & \(.51_{ 0.035}\) & \(.53_{ 0.034}\) & \(.13_{ 0.13}\) & \(.19_{ 0.14}\) & \(.18_{ 0.014}\) \\  \(3\) GPT-3.5 \(+3\) Llama-3 & - & \(.56_{ 0.035}\) & \(.62_{ 0.031}\) & - & \(.76_{ 0.015}\) & \(.82_{ 0.014}\) \\  \(3\) GPT-3.5 \(+3\) Mistral & - & \(.52_{ 0.035}\) & \(.56_{ 0.035}\) & - & \(.56_{ 0.018}\) & \(.68_{ 0.17}\) \\  \(3\) Llama-3 \(+3\) Mistral & - & \(.49_{ 0.036}\) & \(.53_{ 0.035}\) & - & \(.22_{ 0.015}\) & \(.23_{ 0.015}\) \\  

Table 1: Accuracy of a solo model, debate, and our debate interventions: 10 rounds, 6 models.

Figure 3: Accuracy per round, our method and SoM when combing GPT-3.5 with Llama-3 or Mistral.