# Supervised Kernel Thinning

Albert Gong Kyuseong Choi Raaz Dwivedi

Cornell Tech, Cornell University

agong,kc728,dwivedi@cornell.edu

###### Abstract

The kernel thinning algorithm of  provides a better-than-i.i.d. compression of a generic set of points. By generating high-fidelity coresets of size significantly smaller than the input points, KT is known to speed up unsupervised tasks like Monte Carlo integration, uncertainty quantification, and non-parametric hypothesis testing, with minimal loss in statistical accuracy. In this work, we generalize the KT algorithm to speed up supervised learning problems involving kernel methods. Specifically, we combine two classical algorithms--Nadaraya-Watson (NW) regression or kernel smoothing, and kernel ridge regression (KRR)--with KT to provide a _quadratic_ speed-up in both training and inference times. We show how distribution compression with KT in each setting reduces to constructing an appropriate kernel, and introduce the Kernel-Thinned NW and Kernel-Thinned KRR estimators. We prove that KT-based regression estimators enjoy significantly superior computational efficiency over the full-data estimators and improved statistical efficiency over i.i.d. subsampling of the training data. En route, we also provide a novel multiplicative error guarantee for compressing with KT. We validate our design choices with both simulations and real data experiments.

## 1 Introduction

In supervised learning, the goal of coreset methods is to find a representative set of points on which to perform model training and inference. On the other hand, coreset methods in _unsupervised_ learning have the goal of finding a representative set of points, which can then be utilized for a broad class of downstream tasks--from integration  to non-parametric hypothesis testing . This work aims to bridge these two research threads.

Leveraging recent advancements from compression in the unsupervised setting, we tackle the problem of non-parametric regression (formally defined in Sec. 2). Given a dataset of \(n\) i.i.d. samples, \((x_{i},y_{i})_{i=1}^{n}\), we want to learn a function \(f\) such that \(f(x_{i}) y_{i}\). The set of allowable functions is determined by the kernel function, which is a powerful building block for capturing complex, non-linear relationships. Due to its powerful performance in practice and closed-form analysis, non-parametric regression methods based on kernels (a.k.a "kernel methods") have become a popular choice for a wide range of supervised learning tasks .

There are two popular approaches to non-parametric kernel regression. First, perhaps a more classical approach, is kernel smoothing, also referred to as Nadaraya-Watson (NW) regression. The NW estimator at a point \(x\) is effectively a smoothing of labels \(y_{i}\) such that \(x_{i}\) is close to \(x\). These weights are computed using the kernel function (see Sec. 2 for formal definitions). Importantly, the NW estimator takes \((n)\) pre-processing time (to simply store the data) and \((n)\) inference time for each test point \(x\) (\(n\) kernel evaluations and \(n\) simple operations).

Another popular approach is kernel ridge regression (KRR), which solves a non-parametric least squares subject to the regression function lying in the reproducing kernel Hilbert space (RKHS) of aspecified reproducing kernel function. Remarkably, KRR admits a closed-form solution via inverting the associated kernel matrix, and takes \((n^{3})\) training time and \((n)\) inference time for each test point \(x\).

Our goal is to overcome the computational bottlenecks of kernel methods, while retaining their favorable statistical properties. Previous attempts at using coreset methods include the work of Boutsidis et al. , Zheng and Phillips , Phillips , which depend on a projection type compression, having similar spirit to the celebrated Johnson-Lindenstrauss lemma, a metric preserving projection result. So accuracy and running depend unfavorably on the desired statistical error rate. Kpotufe  propose an algorithm to reduce the query time of the NW estimator to \(( n)\), but the algorithm requires super-linear preprocessing time.

Other lines of work exploit the structure of kernels more directly, especially in the KRR literature. A slew of techniques from numerical analysis have been developed, including work on Nystrom subsampling by El Alaoui and Mahoney , Avron et al. , Diaz et al. . Camoriano et al.  and Rudi et al.  combine early stopping with Nystrom subsampling. Though more distant from our approach, we also note the approach of Rahimi and Recht  using random features, Zhang et al.  using Divide-and-Conquer, and Tu et al.  using block coordinate descent.

Our contributions.In this work, we show how coreset methods can be used to speed up both training and inference in non-parametric regression for a large class of function classes/kernels. At the heart of these algorithms is a general procedure called kernel thinning [10; 9], which provides a worst-case bound on integration error (suited for problems in the original context of unsupervised learning and MCMC simulations). In Sec. 3, we introduce a meta-algorithm that recovers our two thinned non-parametric regression methods each based on NW and KRR. We introduce the _kernel-thinned Nadaraya-Watson estimator_ (KT-NW) and the _kernel-thinned kernel ridge regression_ estimator (KT-KRR).

We show that KT-NW requires \((n^{3}n)\) time during training and \(()\) time at inference, while achieving a mean square error (MSE) rate of \(n^{-}\) (Thm. 1)--a strict improvement over uniform subsampling of the original input points. We show that KT-KRR requires \((n^{3/2})\) time during training and and \(()\) time during inference, while achieving an near-minimax optimal rate of \(\) when the kernel has finite dimension (Thm. 2). We show how our KT-KRR guarantees can also be extended to the infinite-dimension setting (Thm. 3). In Sec. 5, we apply our proposed methods to both simulated and real-world data. In line with our theory, KT-NW and KT-KRR outperform standard thinning baselines in terms of accuracy while retaining favorable runtimes.

## 2 Problem setup

We now formally describe the non-parametric regression problem. Let \(x_{1},,x_{n}\) be i.i.d. samples from the data distribution \(\) (with density \(p\)) over the domain \(^{d}\) and \(w_{1},,w_{n}\) be i.i.d. samples from \((0,1)\). In the sequel, we use \(\|\|\) to denote the Euclidean norm unless otherwise stated. Then define the response variables \(y_{1},,y_{n}\) by the follow data generating process:

\[y_{i} f^{}(x_{i})+v_{i} i=1,2,,n, \]

where \(f^{}:\) is the _regression function_ and \(v_{i} w_{i}\) for some noise level \(>0\). Our task is to build an estimate for \(f^{}\) given the \(n\) observed points, denoted by

\[_{}((x_{1},y_{1}),,(x_{n},y_{n})).\]

Nadaraya-Watson (NW) estimator.A classical approach to estimate the function \(f^{}\) is kernel smoothing, where one estimates the function value at a point \(z\) using a weighted average of the observed outcomes. The weight for outcome \(y_{i}\) depends on how close \(x_{i}\) is to the point \(z\); let \(:^{d}\) denote this weighting function such that the weight for \(x_{i}\) is proportional to \((\|x_{i}-z\|/h)\) for some bandwidth parameter \(h>0\). Let \(:^{d}\) denote a shift-invariant kernel defined as

\[(x_{1},x_{2})=(\|x_{1}-x_{2}\|/h).\]

Then this smoothing estimator, also known as Nadaraya-Watson (NW) estimator, can be expressed as

\[()_{}} (,x)y}{_{x_{}}(,x)} \]whenever the denominator in the above display is non-zero. In the case the denominator in (2) is zero, we can make a default choice, which for simplicity here we choose as zero. We refer to the estimator (2) as Full-NW estimator hereafter. One can easily note that Full-NW requires \((n)\) storage for the input points and \((n)\) kernel queries for inference at each point.

Kernel ridge regression (KRR) estimator.Another popular approach to estimate \(f^{}\) is that of non-parametric (regularized) least squares. The solution in this approach, often called as the kernel ridge regression (KRR), is obtained by solving a least squares objective where the fitted function is posited to lie in the RKHS \(\) of a reproducing kernel \(\), and a regularization term is added to the objective to avoid overfitting.1 Overall, the KRR estimate is the solution to the following regularized least-squares objective, where \(>0\) denotes a regularization hyperparameter:

\[_{f}L_{_{}}+\|f\|_{} ^{2}, L_{_{}} _{(x,y)_{}}(f(x)-y)^{2}. \]

Like NW, an advantage of KRR is the existence of a closed-form solution

\[_{,}() _{i=1}^{n}_{i}(,x_{i})  \] \[ (+n_{n})^{-1} y_{1}\\ \\ y_{n}^{n}[ (x_{i},x_{j})]_{i,j=1}^{n}^{n n}. \]

Notably, the estimate \(_{,}\), which we refer to as the Full-KRR estimator, can also be seen as yet another instance of weighted average of the observed outcomes. Notably, NW estimator imposes that the weights across the points sum to \(1\) (and are also non-negative whenever \(\) is), KRR allows for generic weights that need not be positive (even when \(\) is) and need not sum to \(1\). We note that naively solving \(_{,}\) requires \((n^{2})\) kernel evaluations to compute the kernel matrix, \((n^{3})\) to compute a matrix inverse, and \((n)\) kernel queries for inference at each point. One of our primary goals in this work is to tackle this high computational cost of Full-KRR.

## 3 Speeding up non-parametric regression

We begin with a general approach to speed up regression by thinning the input datasets. While computationally superior, a generic approach suffers from a loss of statistical accuracy motivating the need for a strategic thinning approach. To that end, we briefly review kernel thinning and finally introduced our supervised kernel thinning approach.

### Thinned regression estimators: Computational and statistical tradeoffs

Our generic approach comprises two main steps. First, we compress the input data by choosing a coreset \(_{}_{}\) of size \(n_{}|_{}|\). Second, we apply our off-the-shelf non-parametric regression methods from Sec. 2 to the compressed data. By setting \(n_{} n\), we can obtain notable speed-ups over the Full versions of NW and KRR.

Before we introduce the thinned versions of NW and KRR, let us define the following notation. Given an input sequence \(_{}\) and output sequence \(_{}\), define the empirical probability measures

\[_{}_{(x,y)_{ }}_{(x,y)}_{} }}_{(x,y)_{} }_{(x,y)}. \]

Thinned NW estimator.The thinned NW estimator is the analog of Full-NW except that \(_{}\) is replaced by \(_{}\) in (2) so that the _thinned-NW estimator_ is given by

\[_{_{}}()_{}}(,x)y}{_{x_{ }}(,x)}=_{}(y)}{_{}} \]whenever the denominator in the display is not zero; and \(0\) otherwise. When compared to the Full-NW estimator, we can easily deduce the computational advantage of this estimator: more efficient \((n_{})\) storage as well as the faster \((n_{})\) computation for inference at each point.

Thinned KRR estimator.Similarly, we can define the _thinned KRR estimator_ as

\[_{_{},}() =_{i=1}^{n_{}}^{}_{i}(,x^{}_{i}), \] \[^{} (^{}+n_{}^{} _{n_{}})^{-1}y^{}_{1}\\ \\ y^{}_{n_{}}^{n_{}} ^{}[(x^{}_{i},x^{ }_{j})]_{i,j=1}^{n_{}}^{n_{} n _{}}\]

given some regularization parameter \(^{}>0\). When compared to Full-KRR, \(_{_{},^{}}\) has training time \((n_{}^{3})\) and prediction time \((n_{})\).

A baseline approach is standard thinning, whereby we let \(_{}\) be an i.i.d. sample of \(n_{}=\) points from \(_{}\). For NW, let us call the resulting \(_{_{}}\) (7) the standard-thinned Nadaraya-Watson (ST-NW) estimator. When \(n_{}=\), ST-NW achieves an excess risk rate of \((n^{-})\) compared to the Full-NW rate of \((n^{-})\). For KRR, let us call the resulting \(_{_{},^{}}\) (8) the standard-thinned KRR (ST-KRR) estimator. When \(n_{}=\), ST-KRR achieves an excess risk rate of \((}})\) compared to the Full-KRR rate of \(()\). Our goal is to provide good computational benefits without trading off statistical error. Moreover, we may be able to do better by leveraging the underlying geometry of the input points and summarize of the input distribution more succinctly than i.i.d. sampling.

### Background on kernel thinning

A subroutine central to our approach is kernel thinning (KT) from Dwivedi and Mackey (2010, Alg. 1). We use a variant called KT-Compress++ from Shetty et al. (2013, Ex. 6) (see full details in App. A), which provides similar approximation quality as the original KT algorithm of Dwivedi and Mackey (2010, Alg. 1), while reducing the runtime from \((n^{2})\) to \((n^{3}n)\).2 Given an input kernel \(_{}\) and input points \(_{}\), KT-Compress++ outputs a coreset \(_{}_{}\) with size \(n_{} n\). In this work, we leverage two guarantees of KT-Compress++. Informally, \(_{}\) satisfies (with high probability):

\[(L^{})\|(_{}- _{})_{}\|_{}  C_{1} n_{}}{n_{}} \] \[()_{\|h\|_{_{ {Alg}}} 1}|(_{}-_{})h|  C_{2}}_{ _{}}(_{2}(_{}),1/n_{ })}}{n_{}}, \]

where \(C_{1},C_{2}>0\) are constants that depend on the properties of the input kernel \(_{}\) and the chosen failure probability of KT-Compress++, \(_{}\) characterizes the radius of \(\{x_{i}\}_{i=1}^{n}\), and \(_{_{}}(_{2}(_{}),1/n_{})\) denotes the kernel covering number of \((_{})\) over the ball \(_{2}(_{})^{d}\) at a specified tolerance (see Sec. 4.2 for formal definitions).

At its highest level, KT provides good approximation of function averages. The bound (9) (formally stated in Lem. 1) controls the worst-case point-wise error, and is near-minimax optimal by Phillips and Tai (1996, Thm. 3.1). In the sequel, we leverage this type of result to derive generalization bounds for the kernel smoothing problem. The bound (10) (formally stated in Lem. 2) controls the integration error of functions in \((_{})\) and is near-minimax optimal by Tolstikhin et al. (2016, Thm. 1, 6). In the sequel, we leverage this type of result to derive generalization bounds for the KRR problem.

### Supervised kernel thinning

We show how the approximation results from kernel thinning can be extended to the regression setting. We construct two meta-kernels, the Nadaraya-Watson meta-kernel \(_{}\) and the ridge-regression meta-kernel \(_{}\), which take in a _base kernel_\(\) (defined over \(\) only) and return a new kernel (defined over \(\)). When running KT, we set this new kernel as \(_{}\).

#### 3.3.1 Kernel-thinned Nadaraya-Watson regression (KT-NW)

A tempting choice of kernel for KT-NW is the kernel \(\) itself. That is, we can thin the input points using the kernel

\[_{}((x_{1},y_{1}),(x_{2},y_{2})) (x_{1},x_{2}). \]

This choice is sub-optimal since it ignores any information in the response variable \(y\). For our supervised learning set-up, perhaps another intuitive choice would be to use KT with

\[_{}((x_{1},y_{1}),(x_{2},y_{2})) ((x_{1},y_{1}),(x_{2},y_{2})), \]

where \((x,y)\) denotes the concatenation of \(x\) and \(y\). While this helps improve performance, there remains a better option as we illustrate next.

In fact, a simple but critical observation immediately reveals a superior choice of the kernel to be used in KT for NW estimator. We can directly observe that the NW estimator is a ratio of the averages of two functions:

\[f_{}(x,y)() (x,) y,1_{}\] \[ f_{}(x,y)() (x,),\]

over the empirical distribution \(_{}\) (6). Recall that KT provides a good approximation of sample means of functions in an RKHS, so it suffices to specify a "correct" choice of the RKHS (or equivalently the "correct" choice of the reproducing kernel). We can verify that \(f_{}\) lies in the RKHS associated with kernel \((x_{1},x_{2})\) and \(f_{}\) lies in the RKHS associated with kernel \((x_{1},x_{2}) y_{1}y_{2}\). This motivates our definition for the Nadaraya-Watson kernel:

\[_{}((x_{1},y_{1}),(x_{2},y_{2})) (x_{1},x_{2})+(x_{1},x_{2}) y_{1}y_{2} \]

since then we do have \(f_{},f_{}(_{})\). Intuitively, thinning with \(_{}\) should simultaneously provide good approximation of averages of \(f_{}\) and \(f_{}\) over \(_{}\) (see the formal argument in Sec. 4.1). When \(_{}=(_{}, _{},)\), we call the resulting solution to (7) the kernel-thinned Nadaraya-Watson (KT-NW) estimator, denoted by \(_{}\).

As we show in Fig. 1(a), this theoretically principled choice does provide practical benefits in MSE performance across sample sizes.

Figure 1: **MSE vs choice of kernels. For exact settings and further discussion see Sec. 5.1.**

#### 3.3.2 Kernel-thinned kernel ridge regression (KT-KRR)

While with NW estimator, the closed-form expression was a ratio of averages, the KRR estimate (4) can not be expressed as an easy function of averages. However, notice that \(L_{_{}}\) in (3) is an average of the function \(_{f}:\) defined as

\[_{f}(x,y) f^{2}(x)-2f(x)y+y^{2} f().\]

Thus, there may be hope of deriving a KT-powered KRR estimator by thinning \(L_{_{}}\) with the appropriate kernel. Assuming \(f()\), we can verify that \(f^{2}\) lies in the RKHS associated with kernel \(^{2}(x_{1},x_{2})\) and that \(-2f(x)y\) lies in the RKHS associated with kernel \((x_{1},x_{2}) y_{1}y_{2}\). We now define the ridge regression kernel by

\[_{}((x_{1},y_{1}),(x_{2},y_{2}))^{2} (x_{1},x_{2})+(x_{1},x_{2}) y_{1}y_{2} \]

and we can verify that \(f^{2}(x)-2f(x)y\) lies in the RKHS \((_{})\).3 When \(_{}(_{ },_{},)\), we call the resulting solution to (8) the kernel-thinned KRR (KT-KRR) estimator with regularization parameter \(^{}>0\), denoted \(_{,^{}}\). We note that the kernel \(_{}\) also appears in (12, Lem. 4), except our subsequent analysis comes with generalization bounds for the KT-KRR estimator. Like for NW, in Fig. 1(b) we do a comparison for KRR-MSE across many kernel choices and conclude that the choice (14) is indeed a superior choice compared to the base kernel \(\) and the concatenated kernel (12).

## 4 Main results

We derive generalization bounds of our two proposed estimators. In particular, we bound the mean squared error (MSE) defined by \( f-f^{}_{2}^{2}=_{X}(f(X)-f^{ }(X))^{2}\). Our first assumption is that of a well-behaved density on the covariate space. This assumption mainly simplifies our analysis of Nadaraya-Watson and kernel ridge regression, but can in principle be relaxed.

**Assumption 1** (Compact support).: _Suppose that \(_{2}(_{}) ^{d}\) for some \(_{}>0\) and that the density \(p\) satisfies \(0<p_{} p(x) p_{}\) for all \(x\)._

### KT-Nw

For the analysis of the NW estimator, we define function complexity in terms of Holder smoothness following prior work .

**Definition 1**.: _For \(L>0\) and \((0,1]\), a function \(f:\) is \((,L)\)-Holder if for all \(x_{1},x_{2}\), \( f(x_{1})-f(x_{2}) L x_{1}-x_{2} ^{}\)._

Our next assumption is that on the kernel. Whereas typically the NW estimator does not require a reproducing kernel, our KT-NW estimator requires that \(\) be reproducing to allow for valid analysis.

**Assumption 2** (Shift-invariant kernel).: \(\) _is a reproducing kernel function (i.e., symmetric and positive semidefinite) defined by \((x_{1},x_{2})( x_{1}-x_{2}/h)\), where \(h>0\) and \(:\) is bounded by 1, \(L_{}\)-Lipschitz, square-integrable, and satisfies:_

\[(L_{}^{}(1/n))=( n),^{}(u)\{r:(r) u\}; \]

_Additionally, there must exist constants \(c_{1},c_{2}>0\) such that_

\[2^{j}_{x}_{ z [(2^{j-1}-)h,(2^{j}+)h]}(x,x-z)dz c_{1 }_{x}_{ z[0, h]}(x,x-z)dz \] \[(2^{j}+)^{d}2^{j}(2^{j-1}-1) c_{2} _{0}^{}(u)u^{d-1}du j=1,2,. \]

When \(\) is a compact kernel, such as Wendland, Sinc, and B-spline, Assum. 2 is easily satisfied. In App. G, we prove that Gaussian and Matern (with \(>d/2+1\)) kernels also satisfy Assum. 2. We now present our main result for the KT-NW estimator. See App. B for the proof.

**Theorem 1** (Kt-Nw).: _Suppose that Assum. 1 and 2 hold and that \(f^{}(,L_{f})\) with \((0,1]\). Then for any fixed \((0,1]\), the KT-NW estimator (7) with \(n_{}=\) and bandwidth \(h=n^{-}\) satisfies_

\[\|_{}-f^{}\|_{2}^{2} Cn^{-}^{2}n, \]

_with probability at least \(1-\), for some positive constant \(C\) that does not depend on \(n\)._

Tsybakov and Tsybakov , Belkin et al.  show that Full-NW achieves a rate of \((n^{-})\), which is minimax optimal for the \((,L)\)-Holder function class. Compared to the ST-NW rate of \(n^{-}\), KT-NW achieves strictly better rates for all \(>0\) and \(d>0\), while retaining ST-NW's fast query time of \(()\). Note that our method KT-NW has a training time of \((n^{3}n)\), which is not much more than simply storing the input points.

### kt-krr

We present our main result for KT-KRR using finite-rank kernels. This class of RKHS includes linear functions and polynomial function classes.

**Theorem 2** (Kt-krr for finite-dimensional RKHS).: _Assume \(f^{}()\), Assum. 1 is satisfied, and \(\) has rank \(m\). Let \(_{,^{}}\) denote the KT-KRR estimator with regularization parameter \(^{}=(}}{m/n_{}^ {2}})\). Then with probability at least \(1-2-2e^{-\|_{}^{2}}{e_{1}(\|f^{}\|_{ }^{2}+^{2})}}\), the following holds:_

\[\|_{,^{}}-f^{}\|_{2}^{2} }}{(n,n_{}^{2}}[\|f^{ }\|_{}+1]^{2} \]

_for some constant \(C\) that does not depend on \(n\) or \(n_{}\)._

See App. C for the proof. Under the same assumptions, Wainwright [29, Ex. 13.19] showed that the Full-KRR estimator \(_{,}\) achieves the minimax optimal rate of \(O(m/n)\) in \(O(n^{3})\) runtime. When \(n_{}=^{c}n\), the KT-KRR error rates from Thm. 2 match this minimax rate in \((n^{3/2})\) time, a (near) quadratic improvement over the Full-KRR. On the other hand, standard thinning-KRR with similar-sized output achieves a quadratically poor MSE of order \(}\).

Our method and theory also extend to the setting of infinte-dimensional kernels. To formalize this, we first introduce the notion of kernel covering number.

**Definition 2** (Covering number).: _For a kernel \(:\) with \(_{}\{f:\|f\|_{} 1\}\), a set \(\) and \(>0\), the covering number \(_{}(,)\) is the minimum cardinality of all sets \(_{}\) satisfying \(_{}_{h}\{g_{ }:_{x} h(x)-g(x)\}\)._

We consider two general classes of kernels.

**Assumption 3**.: _For some \(_{d}>0\), all \(r>0\) and \((0,1)\), and \(_{2}(r)=x^{d}:\|x\|_{2} r}\), a kernel \(\) is_

\[(,)_{}(_{2}(r),)_{d}(e/)^{ }(r+1)^{},>0\] \[(,) _{}(_{2}(r),)_{d}(1/ )^{}(r+1)^{}<2.\]

We highlight that the definitions above cover several popular kernels: LogGrowth kernels include finite-rank kernels and analytic kernels, like Gaussian, inverse multiquadratic (IMQ), and sinc [9, Prop. 2], while PolyGrowth kernels includes finitely-many continuously differentiable kernels, like Matern and B-spline [9, Prop. 3]. For clarity, here we present our guarantee for LogGrowth kernels and defer the other case to App. E.

**Theorem 3** (kt-krr guarantee for infinite-dimensional RKHS).: _Suppose Assum. 1 is satisfied and \(\) is LogGrowth\((,)\) (Assume. 3). Then \(_{,^{}}\) with \(^{}=(1/n_{})\) satisfies the following bound with probability at least \(1-2-2e^{-\|_{}^{2}^{n}n}{e_{1}(1/f^{} \|_{}^{2}+^{2})}}\):_

\[\|_{,^{}}-f^{}\|_{2}^{2}  Cn}{n}+n_{}}}{n_{ }}[\|f^{}\|_{}+1]^{2}. \]

_for some constant \(C\) that does not depend on \(n\) or \(n_{}\)._See App. E for the proof. When \(n_{}=\), ST-KRR achieves an excess risk rate of \(n^{-1/2}^{}n\) for \(\) satisfying \((,)\). While KT-KRR does not achieve a strictly better excess risk rate bound over ST-KRR, we see that in practice, KT-KRR still obtains an empirical advantage. Obtaining a sharper error rate for the infinite-dimensional kernel setting is an exciting venue for future work.

## 5 Experimental results

We now present experiments on simulated and real-world data. On real-world data, we compare our KT-KRR estimator with several state-of-the-art KRR methods, including Nystrom subsampling-based methods and KRR pre-conditioning methods. All our experiments were run on a machine with 8 CPU cores and 100 GB RAM. Our code can be found at [https://github.com/ag2435/npr](https://github.com/ag2435/npr).

### Simulation studies

We begin with some simulation experiments. For simplicity, let \(=\) and \(=[-,]\) so that \([X]=1\). We set

\[f^{}(x)=8(8 x)(x)=1 \]

and follow (1) to generate \((y_{i})_{i=1}^{n}\) (see Fig. 2). We let the input sample size \(n\) vary between \(2^{8},2^{10},2^{12},2^{14}\) and set the output coreset size to be \(n_{}=\) in all cases. For NW, we use the Wendland\((0)\) kernel defined by

\[(x_{1},x_{2})(1--x_{2}\|_{2}}{h})_{+}  h>0. \]

For KRR, we use the Gaussian kernel defined by

\[(x_{1},x_{2})(--x_{2}\|_{2}^{2}}{2h^{2}} ) h>0. \]

We select the bandwidth \(h\) and regularization parameter \(^{}\) (for KRR) using grid search. Specifically, we use a held-out validation set of size \(10^{4}\) and run each parameter configuration \(100\) times to estimate the validation MSE since KT-KRR and ST-KRR are random.

Ablation study.In Fig. 2, we compare thinning with our proposed meta-kernel \(_{}=_{}\) to thinning with the baseline meta-kernels (11) and (12). For our particular regression function (21), thinning with (11) outperforms thinning with (12). We hypothesize that the latter kernel is not robust to the scaling of the response variables. By inspecting (22), we see that \(\|(x_{1},y_{1})-(x_{2},y_{2})\|_{2}\) is heavily determined by the \(y_{i}\) values when they are large compared to the values of \(x_{i}\)--as is the case on the right side of Fig. 2 (when \(X>0\)). Since \(\) is a uniform distribution, thinning with (11) evenly subsamples points along the input domain \(\), even though accurately learning the left side of Fig. 2 (when \(X<0\)) is not needed for effective prediction since it is primarily noise. Validating our theory from Thm. 1, the best performance is obtained when thinning with \(_{}\) (13), which avoids evenly subsampling points along the input domain and correctly exploits the dependence between \(X\) and \(Y\).

In Fig. 2, we perform a similar ablation for KRR. Again we observe that thinning with \(_{}((x_{1},y_{1}),(x_{2},y_{2}))=(x_{1},x_{2})\) outperforms thinning with \(_{}((x_{1},y_{1}),(x_{2},y_{2}))=((x_{1},y_{1} ),(x_{2},y_{2}))\), while thinning with \(_{}=_{}\) achieves the best performance.

Comparison with Full, ST, RPCholesky.In Fig. 3(a), we compare the MSE of KT-NW to Full-NW, ST-NW (a.k.a "Subsample"), and RPCholesky-NW across four values of \(n\). This last method uses the pivot points from RPCholesky as the output coreset \(_{}\). At all \(n\) we evaluated, KT-NW achieves lower MSE than ST-NW and RPCholesky-NW. Full-NW achieves the lowest MSE across the board, but it suffers from significantly worse run times, especially at test time. Owing to its \((n^{3}n)\) runtime, KT-NW is significantly faster than RPCholesky-NW for training and nearly matches ST-NW in both training and testing time. We hypothesize that RPCholesky--while it provides a good low-rank approximation of the kernel matrix--is not designed to preserve averages.

In Fig. 3(b), we compare the MSE of KT-KRR to Full-KRR, ST-KRR (a.k.a "Subsample"), and the RPCholesky-KRR method from Chen et al. [6, Sec. 4.2.2], which uses RPCholesky to select

Figure 2: **Simulated data.**

landmark points for the restricted KRR problem. We observe that KT-KRR achieves lower MSE than ST-KRR, but higher MSE than RPCholesky-KRR and Full-KRR. In Fig. 3(b), we also observe that KT-KRR is orders of magnitude faster than Full-KRR across the board, with runtime comparable to ST-KRR and RPCholesky-KRR in both training and testing.

### Real data experiments

We now present experiments on real-world data using two popular datasets: the California Housing regression dataset from Pace and Barry  ([https://scikit-learn.org/1.5/datasets/real_world.html#california-housing-dataset](https://scikit-learn.org/1.5/datasets/real_world.html#california-housing-dataset); BSD-3-Clause license) and the SUSY binary classification dataset from Baldi et al.  ([https://archive.ics.uci.edu/dataset/279/susy](https://archive.ics.uci.edu/dataset/279/susy); CC-BY-4.0 license).

California Housing dataset (\(d=8,N=2 10^{4}\)).Tab. 1(a) compares the test MSE, train times, and test times. We normalize the input features by subtracting the mean and dividing by the standard deviation and use a 80-20 train-test split. For all methods, we use the Gaussian kernel (23) with bandwidth \(h=10\). We use \(=^{}=10^{-3}\) for Full-KRR, ST-KRR, and KT-KRR and \(=10^{-5}\) for RPCholesky-KRR. On this dataset, KT-KRR lies between ST-KRR and RPCholesky-KRR in terms of test MSE. When \(n_{}=\), RPCholesky pivot selection takes \((n^{2})\) time by Chen et al. [6, Alg. 2], compared to KT-COMPress++, which compresses the input points in only \((n^{3}n)\) time. This difference in big-O runtime is reflected in our empirical results, where we see KT-KRR take 0.0153s versus 0.3237s for RPCholesky-KRR.

SUSY dataset (\(d=18,N=5 10^{6}\)).Tab. 1(b) compares our proposed method KT-KRR (with \(h=10,^{}=10^{-1}\)) to several large-scale kernel methods, namely RPCholesky preconditioning , FALKON , and Conjugate Gradient (all with \(h=10,=10^{-3}\)) in terms of test classification error and training times. For the baseline methods, we use the Matlab implementation provided by Diaz et al.  ([https://github.com/eepperly/Robust-randomized-preconditioning-for-kernel-ridge-regression](https://github.com/eepperly/Robust-randomized-preconditioning-for-kernel-ridge-regression)). In

Figure 3: **MSE and runtime comparison on simulated data. Each point plots the mean and standard deviation across 100 trials (after parameter grid search).**

our experiment, we use \(4 10^{6}\) points for training and the remaining \(10^{6}\) points for testing. As is common practice for classification tasks, we use the Laplace kernel defined by \((x_{1},x_{2})(-\|x_{1}-x_{2}\|_{2}/h)\). All parameters are chosen with cross-validation.

We observe that KT-KRR achieves test MSE between ST-KRR and RPCholesky preconditioning with training time almost half that of RPCholesky preconditioning. Notably, our Cython implementation of KT-Compress++ thinned the four million training samples in only 1.7 seconds on a single CPU core--with further speed-ups to be gained from parallelizing on a GPU in the future.

## 6 Conclusions

In this work, we introduce a meta-algorithm for speeding up two estimators from non-parametric regression, namely the Nadaraya-Watson and Kernel Ridge Regression estimators. Our method inherits the favorable computational efficiency of the underlying Kernel Thinning algorithm and stands to benefit from further advancements in unsupervised learning compression methods.

The KT guarantees provided in this work apply only when \(f^{}()\) for some base kernel \(\). In practice, choosing a good kernel \(\) is indeed a challenge common to all prior work. Our framework is friendly to recent developments in kernel selection to handle this problem: Dwivedi and Mackey (2019, Cor. 1) provide integration-error guarantees for KT when \(f^{}()\). Moreover, there are recent results on finding the best kernel (e.g., for hypothesis testing (Bradhakrishnan et al., 2018, Sec. 4.2)). Radhakrishnan et al. (2018) introduce the Recursive Feature Machine, which uses a parameterized kernel \(_{M}(x_{1},x_{2})(-(x_{1}-x_{2})^{}M(x_{1}-x_{2})/ (2h^{2}))\), and propose an efficient method to learn the matrix parameter \(M\) via the average gradient outer product estimator. An exciting future direction would be to combine these parameterized (or "learned") kernels with our proposed KT methods for non-parametric regression.

## 7 Acknowledgements

AG is supported with funding from the NewYork-Presbyterian Hospital.