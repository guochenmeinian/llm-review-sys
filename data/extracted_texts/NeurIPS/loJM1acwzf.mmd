# MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations

Yubo Ma\({}^{1}\), Yuhang Zang\({}^{2}\), Liangyu Chen\({}^{1}\), Meiqi Chen\({}^{3}\), Yizhu Jiao\({}^{4}\)

**Xinze Li\({}^{1}\), Xinyuan Lu\({}^{5}\), Ziyu Liu\({}^{6}\), Yan Ma\({}^{7}\), Xiaoyi Dong\({}^{2}\), Pan Zhang\({}^{2}\) Liangming Pan\({}^{8}\), Yu-Gang Jiang\({}^{9}\), Jiaqi Wang\({}^{2}\), Yixin Cao\({}^{9*}\), Aixin Sun\({}^{1}\) \({}^{1}\)** S-Lab, Nanyang Technological University, \({}^{2}\) Shanghai AI Laboratory, \({}^{3}\) Peking University \({}^{4}\) University of Illinois Urbana-Champaign, \({}^{5}\) National University of Singapore, \({}^{6}\) Wuhan University \({}^{7}\) Singapore Management University, \({}^{8}\) University of Arizona, \({}^{9}\) Fudan University

Corresponding Authors. Project Page: https://mayubo2333.github.io/MMLongBench-Doc

###### Abstract

Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents **MMLongBench-Doc**, a long-context, multi-modal benchmark comprising 1,082 expert-annotated questions. Distinct from previous datasets, it is constructed upon 135 lengthy PDF-formatted documents with an average of 47.5 pages and 21,214 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (_i.e.,_ page number). Moreover, 33.7% of the questions are _cross-page questions_ requiring evidence across multiple pages. 20.6% of the questions are designed to be _unanswerable_ for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 44.9%, while the second-best, GPT-4V, scores 30.5%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs.

## 1 Introduction

Documents are one of the fundamental forms of information preservation and exchange. In each year, tens of millions of documents are created, read, saved, and dispatched . Beyond unstructured pure-text, documents feature both complicated layout structures and information across distinct modalities such as text, table, chart, image, _etc._ Accordingly, the automatic understanding of documents (Document Understanding; DU) stands as a long-standing task in urgent and practical needs.

Recently, a number of LVLMs, both closed-source ones (GPT-4o , Gemini-1.5 , Claude-3 , _etc._) and open-source ones (InternLM-XC2-4KHD , InternVL-Chat , Otter , LLaVA-NeXT , CogVLM , mPLUG-DocW1 1.5 , TextMonkey , _etc._) have been developed and presented the great potential to handle documents. Most of them have achieved promising performance on single-page DU datasets like DocVQA , ChartQA , InfoVQA , TAT-DQA , _etc._ However, considerable amounts of documents in the real world are long-contextdocuments with tens or even hundreds of pages. The understanding of these lengthy documents brings new challenges for LVLMs from at least two aspects: (1) **Localization**: identify and retrieve information from massive, heterogeneous information (similar to the _needle in a haystack_ task); (2) **Cross-page comprehension**: collect and reason over multi-source information across different pages. These two kinds of abilities are beyond the evaluation scopes of the aforementioned single-page DU datasets. Some recent DU datasets [16; 17; 18] feature multiple-page DU, but almost all their documents are either as short of only several pages or of low information density, making the localization-related questions over-simple. Additionally, few (if any) questions in these datasets necessitate cross-page comprehension. See more detailed related work in Section 2. In summary, there lacks a unified and high-quality benchmark on lengthy documents, leaving the evaluation of long-context DU largely unexplored.

In this paper, we present MMLongBench-Doc, a benchmark designed to evaluate the **M**ulti-**M**odality **Long**-context **D**oument understanding abilities of LVLMs. Towards a comprehensive benchmark, it incorporates lengthy documents from both four existing datasets [13; 17; 18; 19] and other various papers, brochures, _etc._ Consequently, our benchmark includes 135 PDF-formatted documents spanning across 7 diverse domains, with each document averaging 47.5 pages and 21,214.1 textual tokens. Regarding the questions, we employ ten expert-level annotators to (1) edit questions associated with documents from existing datasets to meet our benchmark's standard and (2) create new questions for all collected documents to expand the scale of the benchmark. Then a three-round, semi-automatic reviewing process ensures the benchmark's annotation quality. As a result, MMLongBench-Doc comprises 1,082 human-annotated questions, with 184 sourced from four existing datasets and 898 newly annotated. Being a multi-modal benchmark, the answer to each question requires evidence from one or more of these five in-document sources: _text_, _layout_, _chart_, _table_, and _image_. Questions are categorized into three types based on the number of evidence pages 1, with examples illustrated in Figure 1(a): (1) 494 _single-page_ questions (with one evidence page) mainly to evaluate localization abilities, (2) 365 _cross-page_ questions (with multiple evidence pages) to assess cross-page comprehension, and (3) 223 _wanswerable_ questions (no evidence for answering it, _i.e.,_ no evidence pages) to reduce shortcuts and measure LVLMs' potential hallucinations. Meta-information including evidence pages, sources, and answer formats, is preserved for fine-grained evaluation and analysis. Detailed descriptions of the annotation pipeline and statistics can be found in Section 3.

We conduct extensive experiments on MMLongBench-Doc to evaluate the long-context DU abilities of 14 LVLMs, including 4 proprietary and 10 open-source ones. Given a document, we screenshot each page and feed all of these PNG-formatted images to LVLMs in an end-to-end approach. For comparison, we also convert the documents to textual format by optical character recognition (OCR) and evaluate another 6 proprietary and 4 open-source 10 LLMs (6 proprietary and

Figure 1: MMLongBench-Doc evaluates understanding abilities of LVLMs on lengthy documents that span tens of pages and incorporate multi-modal elements. Experiments (bottom-right) indicate that most LVLMs struggle, even falling behind LLMs that are fed with only OCR-parsed documents.

4 open-source ones). The results in Figure 1(c) highlight the challenges that current LVLMs face with long-context DU. The best-performing LVLM, GPT-4o, achieves an overall F1 score of only 44.9%, while the second-best LVLM, GPT-4V, scores 30.5%. Moreover, all the remaining LVLMs tested with multi-modal documents performed worse than single-modal LLMs handling lossy, OCR-parsed texts. Specifically, the Gemini-1.5-Pro and Claude-3-Opus present 4.2% and 6.4% absolute decrease when the inputs change from document screenshots to OCR-parsed texts. Regarding open-source models, the best-performing LVLM lags behind the best-performing LLM by 11.7%. These results reveal that long-context DU is a far-from-resolved task for current LVLMs.

## 2 Related Work

**Benchmarks for Document Understanding.** A great amount of datasets have emerged to evaluate the DU capabilities of LVLMs. Many datasets focus exclusively on either a single component (_e.g.,_ table, chart) [13; 15; 21; 22] or a single page [12; 14] from the full documents. Some recent DU datasets [16; 17; 18; 23; 19] attempt to assess multi-page documents, but still exhibit shortcomings in terms of document length (page number), information density (token number) and the construction approaches. Specifically, MP-DocVQA  is an extension of DocVQA  and inherently absent of both cross-page and unanswerable questions. Annotating from scratch, DUDE  includes a small percentage of cross-page questions (2.1%) and unanswerable questions (12.7%). However, due to the relatively short context length (5.3 pages on average) and the use of crowd-sourced annotations, questions in DUDE tend to be less challenging and somewhat less rigorous. SlideVQA features 20-page documents and cross-page questions (12.9%). Nevertheless, the documents in SlideVQA are in slide-deck format and of relatively low information density. Moreover, these cross-page questions are HotpotQA-style  created by instantiating entity graphs and co-referencing in-graph entities across multiple pages. The entity graph from a closed document tends to be sparse and has significant shortcuts (see examples in Appendix A.4). These shortcuts sometimes lead to false cross-page questions that actually do not require answer evidence across different pages. The recent FinanceBench  features both extremely long-context documents and practical, scalable cross-page questions. However, its documents are exclusively financial reports. Additionally, the reference answers are in open-ended formats, making the expert-level manual evaluation indispensable. The above reasons limit the broader applicability of FinanceBench. To our best knowledge, MMLongBench-Doc is the first comprehensive, qualified, and easy-to-use benchmark on the long-context DU task. More detailed descriptions and comparisons are presented in Table 1.

**Models for Document Understanding.** There are two main branches of models for automatic DU tasks. The first approach employs two-stream, OCR-dependent architectures to separately encode textual information (parsed via OCR) and visual information (images and/or layout structures) [25; 26; 27]. In contrast, the second approach develops OCR-free models that understand documents

    &  &  &  \\  & **\#** & **Pages** & **\#** & **Tokens** & **Cross-page (\%)** & **Unans. (\%)** & **Doc. Rel.** & **Source** & **Avg. Position** \\  DocVQA  & 1.0 & 151.5 & ✗ & ✗ & ✗ & \# & TXT/L/C/TAB/I & - \\ ChartQA 2 & 1.0 & 236.9 & ✗ & ✗ & ✓ & C & - \\ InfovQA 2 & 1.2 & 288.0 & ✗ & ✗ & ✗ & L/C/TAB/I & - \\ TXT-DQA  & 1.1 & 577.0 & ✗ & ✗ & ✗ & TXT/TAB & - \\ VisualWebBench 2 & 1.0 & 452.4 & ✗ & ✗ & ✓ & LAY/I & - \\ PWC  & -12* & -7000.0 & ✗ & ✗ & ✗ & TAB & - \\ MP-DocVQA  & 8.3 & 2026.6 & ✗ & ✗ & ✗ & TXT/L/C/TAB/I & 6.0 \\ DUDE  & 5.7 & 1831.5 & ✓(2.1\%) & ✓(12.7\%) & ✗ & TXT/L/C/TAB/I & 2.5 \\ SlideVQA  & 20.0 & 2030.5 & ✓(13.9\%) & ✗ & ✗ & TXT/L/C/TAB/I & 9.1 \\  MMLongBench-Doc & 47.5 & 21214.1 & ✓(33.0\%) & ✓(22.5\%) & ✓ & TXT/L/C/TAB/I & 23.6 \\   

Table 1: Comparison between our benchmark and previous DU datasets. **Unans.**: unanswerable question. **TXT/L/C/TAB/I**: pure text/generalized layout/chart/table/image. **Doc. Rel.**: document relevance. Whether document information is indispensable for the answer. **Avg. Position**: the average page index on which the answer evidence is located. *:**-Statistics from .

in an end-to-end manner [28; 29]. With the rapid advancement of LVLMs, the latter approach has dominated the current DU solutions. As mentioned above, a range of LVLMs demonstrate promising performance on single-page DU datasets. However, as shown in Section 4, even the most advanced LVLMs fall significantly short of achieving satisfactory performance on our benchmark. It reveals that understanding lengthy documents still poses great challenges to current LVLMs.

Long-context LVLMs and LLMs.Lengthy documents necessitate the use of LVLMs or LLMs with extended context sizes. Several benchmarks [30; 31; 32; 33] and solutions [34; 35; 36; 37] have been proposed to evaluate and develop long-context LLMs. However, there exists limited related work for long-context LVLMs, leaving this area largely unexplored. Until very recently, contemporary studies [38; 39; 40] assess and/or improve LVLMs' multi-image understanding capabilities. Evaluations on both MMLongBench-Doc and these works indicate that current LVLMs are still not fully equipped to handle long-context DU and many other practical tasks that require extensive contextual comprehension.

## 3 MMLongBench-Doc

We design a three-stage annotation pipeline for the construction of our benchmark. The three stages will be introduced in Section 3.1, Section 3.2, and Section 3.3, respectively. We also provide key statistics of our benchmark in Section 3.4.

### Document Collection

As a long-context DU benchmark, the documents shall be of diverse topics and lengthy enough. To this end, we crawl a great amount of documents from various sources. Then we select the lengthy ones from these documents. Specifically, we encompass a diverse array of documents from two approaches. (1) **Existing documents** from four previous datasets: DUDE , SlideVQA , ChartQA , and FinanceBench . (2) **Newly-collected documents** from Arxiv 3, ManualsLib 4 and Google Search 5. Then we (1) filter out the documents with fewer than 15 pages or license restrictions and (2) down-sample documents from DUDE, SlideVQA, and FinanceBench for a more balanced distribution. Detailed descriptions of our selection and processing procedure can be found in Appendix A.1 and Appendix A.2.

In summary, we collect a total of 135 documents. Among them, 76 documents are from existing datasets and incorporate previously annotated questions (represented as triangles). The remaining 59 documents are newly collected and incorporate no existing questions. We manually categorize them into 7 types: _Research Report_, _Financial Report_, _Academic Paper_, _Brochure_, _Guideline_, _Administration & Industry File_, _Tutorial / Workshop_. We showcase some instances of these documents in Appendix A.3.

### Question and Answer Collection

To serve as a high-quality and comprehensive benchmark, the question annotation of our benchmark adheres to the following standards: (1) All questions shall be neither over-easy nor over-difficult. (2) Questions are not repetitively derived from the same page or the same pattern. (3) The distribution of evidence numbers, evidence sources, and evidence locations for the questions shall be balanced. (4) No questions shall be answered correctly without accessing the relevant documents.

Ten authors serve as expert-level annotators for the question-and-answer collection. All of them are doctors or Ph.D. students proficient in English reading and writing. Before formal annotation, they undergo a training session and pre-annotate three documents for practice. We iteratively review their annotation results and provide personalized feedback until their annotations meet the standards mentioned above. Regarding the formal annotation, we divide 135 documents into 54 batches (each having 2-4 documents) and dispatch these batches to annotators. We then ask the annotators to submit their results in units of batches and set reasonable time intervals for each batch's submission. Wetimely evaluate their annotations after each submission and remind the annotators if their questions in this turn diverge from the standards. It avoids the annotators rushing all assignments in a short time and benefits the annotation quality. We recommend the annotators take 60-90 minutes on each document. Specifically, the annotators shall rapidly read through the whole document in the first 15-30 minutes. For the remaining time, they shall dive deep into specific components to modify existing annotations and/or add new annotations as detailed below.

**Modify Existing Questions.** Documents collected from existing datasets had been annotated with some questions and answers from previous work. However, their crowd-sourcing annotations inevitably make some questions, answers, and other meta information unqualified. Therefore, we edit their annotations before including them as a component of our benchmark.

Specifically, we classify six potential problems in original annotations: _Wrong Answers or Evidence Pages_, _Repetitive Question_, _Ambiguous Question_, _Decontexualization-required Question_, _Low Document-relevant Question_ and _Potential Shortcut_. See detailed explanations and examples about these problems in Appendix A.4. Given an existing document, the annotators are tasked to evaluate each existing question's quality according to whether they have one or more above problems and assign a label from {Retain,Revise,Remove} for each question. Then the annotators would revise the Revise questions to meet our quality criteria and remove the Remove questions. Among all 425 original questions from 76 existing documents, 32.2% of them are revised and 46.1% are removed. We finally collect 211 questions in this procedure. The corresponding GUI is shown in Appendix A.7.

**Add New Questions.** We newly annotate questions on both existing and newly collected documents to expand the questions in our benchmark. Specifically, we ask annotators to add about 3 questions on existing documents, and 6 questions on newly-collected documents. Given most existing questions (even after editing) are single-page ones and sourced from texts, we put more focus on (1) cross-page and unanswerable questions and (2) questions sourced from tables, charts, and images for newly added questions to balance the distribution. We detail the quantitative requirements in Appendix A.5. Associated with questions, annotators also provide reference answers and meta-information (_i.e._, evidence sources, answer format, evidence locations) for all samples. We finalized a collection of 965 samples in this procedure. The corresponding GUI is shown in Appendix A.7.

### Quality Control

Combining the merits of humans and LVLMs, we adopt a three-round, semi-automatic quality control procedure to improve the annotation quality of our benchmark. We detail each round in the following components and leave the discussion of potential bias in Appendix A.6.

**Document-relevant Detection.** Our benchmark is designed to evaluate LVLMs' long-context document understanding abilities. All questions are expected to be unanswerable without access to corresponding documents. To remove low document-relevant questions (_i.e._, questions not relying on documents), we feed each annotated question **WITHM** documents to GPT-4o. A question will be identified as _low document-relevant_ question if GPT-4o correctly predicts under this case. Ultimately, 94 samples are identified as low document-relevant questions and removed in this round.

**Self-reflection.** We draw inspirations from MMBench  and leverage LVLMs to reduce the wrongly-annotated samples. Specifically, we feed the remaining questions from the last round **WITH** their documents to GPT-4o. Samples whose model predictions are inconsistent with the reference answers are sent back to corresponding annotators. The annotators are asked to check each question and identify whether the inconsistency is caused by _problematic annotation_ or not. As a result, 13.8% of the samples are identified as problematic annotations. The annotators revise them accordingly.

**Cross-checking.** In parallel, annotators cross-check the annotated samples from other annotators and determine the inconsistency reasons the same as described above. We calculate Cohen's kappa value of their identifications as 0.42 (17.5% inconsistent samples), showing a moderate agreement. Regarding the 17.5% inconsistent samples, two primary authors serve as meta-annotators and make final decisions on them (and if necessary, revise accordingly).

### Dataset Overview and Analysis

The main statistics of MMLongBench-Doc are presented in Table 2. Overall, our benchmark consists of 1,082 questions. These questions are constructed upon 135 lengthy documents across 7document types, with an average of 47.5 pages and 21,214.1 tokens. Please see detailed distributions of these documents in Figure 2. Regarding the questions, there are 494 single-page questions (1 evidence page), 365 cross-page questions (2+ evidence pages), and 223 unanswerable questions (no evidence page). These three types of questions evaluate the LVLMs's long-context DU capabilities from complementary aspects: the localization ability, the cross-page comprehension ability, and the hallucination severity, respectively. For single-page and cross-page questions, their answer evidence is scattered among different context sources (_i.e.,_ text, layout, table, chart, image) and evenly distributed across different locations of the documents (see Table 2, Figure 3 Left and Middle). Also notably, 28.6% of cross-page questions have more than two evidence pages, which further enhances the challenge of our benchmark.

## 4 Evaluation

### Evaluation Protocol

We follow MATHVISTA  to conduct a three-step evaluation protocol: _response generation_, _answer extraction_, and _score calculation_. We adopt such a protocol out of three considerations: (1) Current LVLMs are instructed to generate long responses, rather than short-form answers, in conventional settings. (2) The evaluation of long responses, however, remains an open and challenging problem. (3) We focus on the document understanding (not instruction following) abilities of LVLMs.

Specifically, we impose no limitations on _response generation_ stage to encourage LVLMs to answer the questions in a freestyle. Then we propose a unified LLM-based _answer extractor_ (GPT-4o under our setting) to convert their long responses to short-form answers. Finally, we use a rule-based _score calculator_ to evaluate the converted short answers. We report both generalized accuracy and generalized F1 score to balance the answerable (positive) and unanswerable (negative) questions. The used prompt, the high correlation between our automatic _answer extractor_ and human evaluation, and the detailed rules of our _score calculation_ are described in Appendix B.

### Experimental Setup

We evaluate 14 LVLMs on MMLongBench-Doc, including 4 proprietary LVLMs and 10 open-source LVLMs. To purely evaluate LVLMs' long-context DU abilities, we screenshot each page of the PDF-formatted document with 144 DPI and feed all these PNG-formatted images to LVLMs in an end-to-end approach. Notably, all evaluated open-source LVLMs do not support multi-image inputs or present significant performance drops when fed with excessive images (_e.g.,_ more than 10 or 20 images). Therefore, we employ a concatenation strategy that combines all screenshot pages into 1 or 5 images and feeds these concatenated images to open-source LVLMs. Regarding proprietary LVLMs, we adopt the same concatenation strategy and reduce the image number to 20 for Claude-3-Opus to fit its maximum image threshold. For GPT-4o, GPT-4V, and Gemini-1.5-Pro, we directly send all original screenshots to them (_i.e.,_ the image number equals the page number).

For comparison, we also use the Tesseract  OCR model to recognize and extract texts from the documents and feed the parsed documents to 10 LLMs, including 6 proprietary and 4 open-source

    &  &  &  &  \\  & &  &  &  &  &  &  &  &  &  &  &  \\   \\  & 6B & 128k & 23.4 & 12.7 & 9.7 & 10.2 & 12.2 & 18.8 & 11.5 & 18.1 & 16.3 & 14.9 \\ Mistral-Instructv-0.2  & 7B & 32k & 19.9 & 13.4 & 10.2 & 10.1 & 11.0 & 16.9 & 11.3 & 24.1 & 16.4 & 13.8 \\ Mistral-Instructv-0.1  & 8x7B & 32k & 24.2 & 14.8 & 12.5 & 15.0 & 13.7 & 21.3 & 14.1 & 13.1 & 17.0 & 16.9 \\ Mistral-Instructv-0.1  & 8x22B & 64k & 34.2 & 21.3 & 19.5 & 21.3 & 19.2 & 27.7 & 21.9 & 32.4 & 26.9 & 24.7 \\ _Propricency Models_ & & & & & & & & & & & & & \\ QWen-Plus  & - & 32k & 17.4 & 15.6 & 7.4 & 7.9 & 8.8 & 14.2 & 10.6 & 42.2 & 18.9 & 13.4 \\ DeepSeeX-V2  & - & 32k & 27.8 & 19.6 & 8.8 & 17.0 & 9.4 & 20.2 & 15.4 & 48.1 & 24.9 & 19.6 \\ Claude-3 Qpus  & - & 32k & 30.8 & 30.1 & 16.4 & 24.4 & 16.3 & 32.0 & 18.6 & 30.9 & 26.9 & 24.5 \\ Gemini-1.5-Pro  & - & 32k & 29.3 & 15.9 & 12.5 & 17.7 & 11.5 & 21.2 & 16.4 & **73.4** & 31.2 & 24.8 \\ GPT-4 turbo  & - & 128k & 36.5 & 21.0 & 20.7 & 24.3 & 17.3 & 28.7 & 23.8 & 31.2 & 27.6 & 25.9 \\ GPT-4o  & - & 128k & 41.1 & 23.4 & 28.5 & 38.1 & 22.4 & 35.4 & 29.3 & 18.6 & 30.1 & 30.5 \\   \\   \\ DeepSeeX-V1-Chatt  & 7.3B & 4k & 7.2 & 6.5 & 1.6 & 5.2 & 7.6 & 5.2 & 7.0 & 12.8 & 7.4 & 5.4 \\ IdeficSz  & 8B & 8k & 9.0 & 10.6 & 4.8 & 4.1 & 8.7 & 7.7 & 7.2 & 5.0 & 7.0 & 6.8 \\ Mintori-Ilama-V2.5  & 8B & 2k & 11.9 & 10.8 & 5.1 & 5.9 & 12.2 & 5.9 & 9.5 & 4.5 & 8.5 & 8.6 \\ InterMLM-X2-4KHD  & 8B & 16k & 9.9 & 14.3 & 7.7 & 6.3 & 13.0 & 12.6 & 7.6 & 9.6 & 10.3 & 9.8 \\ mPLUG-DecOwl 1.5  & 8.1B & 4k & 8.2 & 8.4 & 2.0 & 3.4 & 9.9 & 7.4 & 6.4 & 6.2 & 6.9 & 6.3 \\ Qwen-VL-Chatt  & 9.6B & 6k & 5.5 & 9.0 & 5.4 & 2.2 & 6.9 & 5.2 & 7.1 & 6.2 & 6.1 & 5.4 \\ Monkey-Chatt  & 9.8B & 2k & 6.8 & 7.2 & 3.6 & 6.7 & 9.4 & 6.6 & 6.2 & 6.2 & 6.2 & 5.6 \\ _Oepensource, 5-4B Models_ & & & & & & & & & & & & & \\ CoqVLM2-LMAMA-Chat  & 19B & 8k & 3.7 & 2.7 & 6.0 & 3.2 & 6.9 & 3.9 & 5.3 & 3.7 & 4.4 & 4.0 \\ InternVL-Chatt-V1.5  & 26B & 4k & 14.0 & 16.2 & 7.1 & 10.1 & 16.6 & 14.9 & 12.2 & 17.5 & 14.6 & 13.0 \\ EMU2-Chatt  & 37B & 2k & 6.1 & 9.7 & 2.6 & 3.8 & 7.7 & 5.7 & 6.1 & 16.5 & 8.3 & 5.5 \\ _Propricency Models_ & & & & & & & & & & & & & \\ Claude-3 Qpus  & - & 200k & 24.9 & 24.7 & 14.8 & 13.0 & 17.1 & 25.6 & 13.8 & 7.6 & 17.4 & 18.1 \\ Gemini-1.5-Pro  & - & 128k & 21.0 & 17.6 & 6.9 & 14.5 & 15.2 & 21.1 & 11.1 & 69.2 & 28.2 & 20.6 \\ GPT-4V(sion)  & - & 128k & 34.4 & 28.3 & 28.2 & 32.4 & 26.8 & 36.4 & 27.0 & 31.2 & 32.4 & 31.2 \\ GPT-4o  & - & 128k & **46.3** & **46.0** & **45.3** & **50.0** & **44.1** & **5.45** & **41.5** & 20.2 & **42.8** & **44.9** \\   \\  Human Experts & - & - & - & - & - & - & - & - & - & - & 65.8 & 66.0 \\   

Table 3: **Evaluation of various models on MMLongBench-Doc.** We report the generalized accuracy of five types of evidence sources including pure text (TXT), layout (LAY), chart (CHA), table (TAB), and image (IMG). We also present the generalized accuracy of questions categorized by the number of evidence pages: single-page (SIN), cross-page (MUL), and unanswerable (UNA) questions. The \(|\)**best** and \(|\)**second-best** performance in each section are highlighted.

ones. Texts exceeding their context lengths are truncated. Notably, as a key component of the classical solution for the DU task, the OCR model can handle most flattened texts and some structured tables in the document. However, it cannot perceive the information from the charts or images. Thus the TXT-formatted, OCR-parsed documents are lossy documents in which the information is not fully preserved. More detailed hyperparameters are introduced in Appendix B.5. Additionally, we also conduct manual evaluation on a subset of our datasets (238 questions from 29 documents) to indicate the difficulty of this task for humans.

### Main Results

We compare the performance of different LVLMs and LLMs in Table 3, reporting their generalized accuracy and F1 scores (shown in the last two columns). Regarding LVLMs, we draw several conclusions as below: (1) The performance demonstrates that long-context DU is still a challenging and unsolved task for current LVLMs. The best-performing LVLM, GPT-4o, merely achieves a 44.9% F1 score. The second best-performing LVLM, GPT-4V, lags behind by over 10% percent and presents a 31.4% F1 score. All other LVLMs only achieve about 20% or even lower F1 scores. (2) Though far from satisfactory, GPT-4o performs much better than all other models (including GPT-4V). Thus we speculate that the multi-modal pre-training paradigm significantly benefits LVLMs' cross-modality understanding capabilities. (3) Proprietary LVLMs perform better than open-source LVLMs by a large margin. We attribute it to the difference of acceptable image numbers: open-source LVLMs only support single-image or several-image inputs, while proprietary LVLMs can be fed with at least 20 images or even more. Given that lengthy documents have tens of even hundreds of pages, it is impractical for open-source LVLMs to accurately perceive the information in the documents from the excessively concatenated images. (4) The performances of different models are highly correlated with their acceptable image numbers and maximum image resolutions. Notably, open-source LVLMs that support high-resolution images (_i.e.,_ InternLM-XC2-4KHD and InternVL-Chat-v1.5) exhibit superior performance compared to those with lower resolution limits.

Surprisingly, LVLMs even demonstrate overall worse performance than LLMs, even LLMs are fed with lossy OCR-parsed documents. Specifically, Gemini-1.5-Pro and Claude-3 Opus have 4.2% and 6.4% absolute F1-score degradations on vision versions. And the best-performing LLM (Mixtral) also surpasses the best-performing LVLM (InternVL-v1.5) by 11.7%. The above results clearly reveal that most current LVLMs are still not proficient in cross-modality, long-context document understandings. It is promising that GPT-4o and GPT-4-turbo achieve better performance when seeing multi-modality PDF documents than parsed text by 14.4% and 5.3% F1-score, respectively. Their performances validate the feasibility, benefit, and necessity of understanding documents in an end-to-end, cross-modality approach. We speculate that the scarce related pre-training corpus (_i.e.,_ extremely multi-image or lengthy documents) hinders the long-context DU capabilities of other LVLMs. We will leave related explorations for future work.

Regarding the human evaluation, we observe 66.0% F1-score from our annotators and a significant performance gap (exceeding 20% in absolute) between the current LVLMs and humans. This gap highlights the challenges of document understanding for LVLMs and the necessity of our benchmark.

### Fine-grained Results.

**Document Type.** As illustrated in Figure 4, LVLMs and LLMs exhibit distinct performance patterns across various document types. Our findings include: (1) All evaluated models demonstrate decent performance on industrial documents, which tend to have more standardized formats and less non-textual information. (2) The GPT series and Mixtral (_i.e.,_ the SoTA open-source LLM) show relatively balanced performance across different document types. In contrast, other models perform significantly worse in specialized domains such as academic papers and financial reports. (3) When equipped with OCR, LLM-based models like GPT-4 and Mixtral achieve comparable or even superior performance on industrial documents, academic papers, and brochures. Conversely, end-to-end LVLMs outperform OCR+LLMs in areas such as tutorials, research reports, and guidelines. We speculate that comprehending these latter document types requires more extensive multi-modal information, from which LVLMs significantly benefit.

**Evidence Source.** We categorize questions based on their evidence sources and present fine-grained results in Figure 4 and Table 3. Our observations reveal that only GPT-4o exhibits relatively balanced performance across the different sources. Other LVLMs, however, show inferior performance on questions related to charts and/or images compared to those related to text and/or layout. Additionally, LLMs generally demonstrate better or comparable performance to LVLMs on text- and table-related questions but show worse performance on questions involving other elements. This highlights the limitations of OCR (and other PDF parsers) when dealing with charts and images, as well as the gap in OCR capabilities between LVLMs and pure-text LLMs.

**Evidence Position.** We also examine how the evidence locations (_i.e.,_ the page indexes where the answer evidence is found) affect model performance. The results shown in Figure 5 reinforce that MMLongBenchDoc poses significant challenges for current models, at least partially due to the extended length of the documents. Almost all models (except InternVL-v1.5) exhibit their best performance on questions derived from the initial pages, while their performance declines progressively as the page index increases. Interestingly, two proprietary models, Gemini-Pro-1.5 and Claude-3-Opus, experience particularly sharp declines in performance.

**Number of Evidence Page.** We observe a consistent trend that all models achieve higher scores on single-page questions than cross-page questions. It reveals that gathering and reasoning over all necessary information across different pages is not trivial for current LVLMs and LLMs. More interestingly, evaluated LVLMs behave differently on unanswerable questions. GPT-4o and Claude-3 Opus adopt more aggressive strategies and usually tend to provide some answers. It makes their answers more likely helpful, but also increases the risk of hallucination and unfaithfulness (see their scores on unanswerable questions are much lower than answerable questions). On the contrary, Gemini-1.5-Pro, DeepSeek-VL-Chat, and EMU2-Chat are much more cautious and tend to refuse to answer questions about which they are uncertain. It makes their answers safer but less helpful (with large amounts of responses like _I don't know_).

## 5 Analysis & Discussion

### Oracle Setting

We conduct additional experiments to explore to what extent the challenges of MMLongBenchDoc are caused by the long-context lengths of documents. Specifically, we feed 820 answerable questions along with their oracle evidence pages (instead of the whole documents) to three representative LVLMs and show results in Figure 6. On one hand, it indicates that long-context length is a

Figure 4: Fine-grained results on various document types and evidence sources.

Figure 5: Relationships between evidence positions and model performances.

significantly challenging factor for document understanding. Compared with the oracle-page setting, lengthy documents lead to more than 20% absolute performance degradation on Gemini-1.5-Pro and InternLM-XC2-4KHD. Regarding the single-page questions, the performance difference even achieves up to 30%. On the other hand, the overall performance achieves only about 40% and 30% for Gemini-1.5-Pro and InternLM-XC2-4KHD even under oracle-page setting. And the improvement for GPT-4o is much less (about 10%). It demonstrates that the development of long-context LVLMs can largely facilitate, though still can not fully solve, the long-context DU task.

### Error Analysis

We further conduct error analysis to understand the bottleneck of current LVLMs in a qualitative approach. Specifically, we randomly select 72 error predictions from GPT-4o's responses and manually check their error reasons. These errors are categorized into seven types: _Perceptual Error_, _Irrelevant Answer_, _Incomplete Evidence_, _Hallucinated Evidence_, _Extractor Error_, _Reasoning Error_ and _Knowledge Lacking_. The distribution of these errors is illustrated in Figure 7. It indicates that most errors come from the model's hallucination (_i.e.,_ wrong explanations and answers to unanswerable questions) and perceptual errors (mainly in visual contexts). Additionally, GPT-4o sometimes misunderstands the intent of questions and provides irrelevant responses. The errors caused by collecting incomplete evidence (for cross-page questions) are also unignorable. The descriptions and examples of these error types are detailed in Appendix C.1.

## 6 Conclusion

In this work, we present MMLongBench-Doc to evaluate the long-context DU capabilities of LVLMs. Extensive experiments on 14 LVLMs (and 10 LLMs for comparison) reveal that the understanding of lengthy documents poses great challenges to current LVLMs. Even though the performance of GPT-4o proves the benefit of end-to-end, multi-modality perception for DU tasks, most LVLMs struggle on long visual contexts (_i.e.,_ extremely multiple images) and show inferior performance compared to OCR+LLM pipelines. We hope that the construction of our benchmark could push forward the development of more powerful LVLMs on lengthy document understanding.