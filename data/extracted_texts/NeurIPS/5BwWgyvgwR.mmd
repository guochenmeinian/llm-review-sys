# Unsupervised Modality Adaptation with Text-to-Image Diffusion Models for Semantic Segmentation

Ruihao Xia\({}^{1,2}\), Yu Liang\({}^{2}\), Peng-Tao Jiang\({}^{2}\), Hao Zhang\({}^{2}\)

Bo Li\({}^{2}\), Yang Tang\({}^{1,3}\), Pan Zhou\({}^{4}\)

\({}^{1}\)East China University of Science and Technology, \({}^{2}\)vivo Mobile Communication Co., Ltd

\({}^{3}\)Peng Cheng Laboratory, \({}^{4}\)Singapore Management University

Work was done during interning at vivo.Corresponding author.

###### Abstract

Despite their success, unsupervised domain adaptation methods for semantic segmentation primarily focus on adaptation between image domains and do not utilize other abundant visual modalities like depth, infrared and event. This limitation hinders their performance and restricts their application in real-world multimodal scenarios. To address this issue, we propose Modality Adaptation with text-to-image Diffusion Models (MADM) for semantic segmentation task which utilizes text-to-image diffusion models pre-trained on extensive image-text pairs to enhance the model's cross-modality capabilities. Specifically, MADM comprises two key complementary components to tackle major challenges. First, due to the large modality gap, using one modal data to generate pseudo labels for another modality suffers from a significant drop in accuracy. To address this, MADM designs diffusion-based pseudo-label generation which adds latent noise to stabilize pseudo-labels and enhance label accuracy. Second, to overcome the limitations of latent low-resolution features in diffusion models, MADM introduces the label palette and latent regression which converts one-hot encoded labels into the RGB form by palette and regresses them in the latent space, thus ensuring the pre-trained decoder for up-sampling to obtain fine-grained features. Extensive experimental results demonstrate that MADM achieves state-of-the-art adaptation performance across various modality tasks, including images to _depth_, _infrared_, and _event_ modalities. We open-source our code and models at [https://github.com/XiaRho/MADM](https://github.com/XiaRho/MADM).

## 1 Introduction

Unsupervised Domain Adaptation for Semantic Segmentation (UDASS) involves a source domain with image-label pairs and a target domain with only unlabeled samples , and has achieved promising segmentation results in the image modality. Currently, most existing UDASS methods are restricted to transferring knowledge between similar image domains, such as from virtual scene  to real scene , or from daytime scene  to nighttime scene . However, these approaches do not account for the wide range of visual modalities present in real-world scenarios, such as depth, infrared, and event modalities, which are valuable in nighttime perception  but often lack sufficient and high-quality labels for supervising segmentation training. Hence, in this paper, we are particularly interested in extending UDASS to Unsupervised Modality Adaptation for Semantic Segmentation (UMASS) across different visual modalities, i.e., the adaptation of a model from a labeled source image modality to an unlabeled target modality.

Differences across images arise from the objects, lighting, camera parameters, etc, while there are fundamental disparities in imaging principles across modalities that lead to greater variability. Thisis illustrated by Figure 1 which shows that the similarity among various image domains tends to be higher than between different modalities. These significant modality discrepancies poses significant challenges to existing UDASS methods  on multimodal segmentation due to their limited pre-trained knowledge. Specifically, the backbone  used in current SoTA UDASS methods is pre-trained on the ImageNet-1K dataset  which contains one million images categorized into 1,000 distinct classes, providing the network with a foundational level of semantic understanding. While this backbone achieves promising results in UDASS, its insufficient pre-trained knowledge limits generalization to other visual modalities.

To address this issue, inspired by Text-to-Image Diffusion Models (TIDMs) , which are trained with internet-scale image-text pairs , we recognize that extensive pre-training data unifies samples with different distributions but similar semantic properties through texts, significantly enhancing the model's semantic understanding and generalization. Although TIDMs are not trained on other visual modalities, their large-scale samples and unification through texts enable them to adapt to a broader distribution of domains. Also, the extensive pretraining provides TIDMs with a robust understanding of high-level visual concepts, enabling their application to various domains, such as semantic matching , depth estimation , and 3D awareness . This strong prior motivates us to utilize TIDMs as a robust backbone for solving UMA. Therefore, we present MADM: Modality Adaptation with text-to-image Diffusion Models, which takes full advantage of the generalization of pre-trained diffusion models and facilitates robust adaptation for accurate semantic segmentation in other visual modalities. Specifically, TIDM is used to extract robust features for segmentation and is trained in a self-training manner  which takes unlabeled target samples as input and generates pseudo-labels for training TIDM itself. Building upon TIDMs  and self-training , our MADM incorporates two innovative components: Diffusion-based Pseudo-Label Generation (DPLG) and Label Palette and Latent Regression (LPLR), which address the challenges of unstable pseudo-labeling and lack of fine-grained features extraction, respectively.

First, significant modality discrepancies hinder robust and high-quality pseudo-label generation, which is crucial for further training and enhancing the model. Directly using these unstable labels to train the model can lead to serious biases in the target modality. Thus, we propose DPLG which adds latent noise to target samples before generating pseudo-labels where the noise level gradually

Figure 1: (1) On the left, we leverage the multi-modality model ImageBind  to quantify the similarity of images and modalities across datasets, i.e., GTA5-Synthetic , Dark Zurich-Nighttime , ACDC-Snow , DELIVER-Depth , FMB-Infrared , and DSEC-Event . Specifically, we randomly select 500 samples from each dataset, and compute the average cosine similarity of the output vectors within the dataset (right side of the text) and between the datasets (on the arrows). (2) On the right, we compare the quantitative results with the state-of-the-art (SoTA) method Rein  on three different modalities.

decreases as training stabilizes. These pseudo-labels then supervise the noise-free target modality predictions. The mechanism behind DPLG leverages the denoising property of diffusion models, making the target modality more consistent with pre-trained inputs, thus improving accuracy. Unlike previous supervised diffusion-based semantic segmentation methods  which adopt a single-step forward and remove the diffusion process, we find that a proper diffusion process can stabilize the generation of pseudo-labels and adapt more successfully to the target modality.

Second, TIDMs  encode images into the latent space using a pre-trained Variational AutoEncoder (VAE) for diffusion and denoising, and then up-sample the latent output to the original resolution using the VAE decoder. When adopting TIDMs as a backbone, the resolution of the features is too low, resulting in a loss of details. To address this, we propose LPLR to convert pixel-level classification into regression in RGB form, utilizing the up-sampling capability of the pre-trained VAE decoder in a recycling manner. Specifically, we use a palette to convert one-hot encoded labels into RGB form and encode these RGB labels into latent representations. Then, the model is trained with a regression loss to fit the latent labels, obtaining high-resolution fine-grained features via the VAE decoder. Different from previous diffusion-based semantic segmentation methods  that directly extract multi-scale features from the denoising UNet network, we take inspiration from depth estimation with TIDMs  and propose LPLR to extract high-resolution features. Our contributions are summarized as follows:

* We propose MADM, extending traditional UDASS to UMASS with a pre-trained TIDM backbone for generalizing across various visual modalities.
* We design Diffusion-based Pseudo-Label Generation (DPLG) to provide more robust pseudo-labels by adding annealed latent noise to target samples for stable modality adaptation.
* We introduce Label Palette and Latent Regression (LPLR) to convert semantic segmentation into regression for learning details, thereby repeatedly utilizing pre-trained VAE decoders for high-resolution feature extraction.
* We demonstrate the effectiveness of our MADM through extensive experiments on three different modalities: Image \(\) Depth , Infrared , and Event .

## 2 Related Works

### Unsupervised Domain Adaptation Semantic Segmentation

UDASS can be broadly categorized into two primary methods: adversarial and self-training methods. Adversarial approaches aim to align the distributions of the source and target domains at the level of images  or features  or outputs , thereby facilitating the transfer of knowledge. On the other hand, self-training methods  operate on the paradigm of pseudo-labeling, where the model's predictions on the target domain act as ground truth during training, iteratively refining the segmentation. Recently, the field has witnessed the development from CNN-based methods  to Transformer-based methods , leveraging the self-attention mechanism to capture long-range dependencies and enhance cross-domain feature representation.

However, most of these methods have predominantly focused on adaptation between different image domains, such as from synthetic  to real-world images or across varying environmental conditions . They will fail when adapting to other visual modalities, such as depth, infrared, or event, which have distinct data distribution. Thus, we propose MADM to address the limitation of UDASS for adapting to other unexplored visual modalities. Besides, different from multi/cross-modality domain adaptation  which has paired two modalities in both domains, our modalities are different in source and target.

### Text-to-Image Diffusion Models

Diffusion denoising probabilistic models  have set new benchmarks in the quality and controllability of generative tasks. These models are distinguished by their two-phase paradigm: the diffusion process that progressively adds noise to the sample, and the denoising process that learns to denoise the corrupted sample by a network. Utilizing the MSE loss between the residual noise and the prediction as a training objective, diffusion models have demonstrated greater training stability compared to generative adversarial networks  and VAEs . To achieve high-quality controllable image generation with reduced computational demands, Rombach _et al._ proposed the latent diffusion model that leverages cross-attention layers and confines the diffusion process to a low-resolution latent space, which has emerged as a widely recognized TIDM.

In recent advancements beyond the generative tasks, the exceptional semantic comprehension capabilities of TIDMs have been harnessed to enhance performance in many downstream applications. Xu _et al._ presented a novel framework that integrates a pretrained TIDM with a discriminative model to address the challenges of open-vocabulary segmentation. Similarly, Zhao _et al._ demonstrated the versatility of TIDMs by fine-tuning it to deal with various visual perception tasks, including semantic segmentation, referring image segmentation, and depth estimation. Gong _et al._ introduced innovative scene prompts and a prompt randomization strategy on TIDMs, achieving new milestones in domain generalization and test-time domain adaptation. Their works highlight the potential of TIDMs to generalize across diverse domains and adapt to new ones with minimal additional training. It's worth noting that the aforementioned methods necessitate only a single-step forward pass through the denoising UNet, significantly streamlining the inference process.

The successful and diverse applications of TIDMs inspire our exploration into the generalization of TIDMs to more challenging visual modalities. However, we observe that the latent diffusion property within TIDMs leads to the lower-resolution feature extraction. To address this limitation, we propose the LPLR that converts semantic segmentation into latent regression to obtain fine-grained features.

## 3 Method

### Overview

In UMASS, given the labeled source RGB modality \(\{(x_{s},y_{s})\}\) and the unlabeled target modality \(\{(x_{t})\}\), our objective is to train a network which accepts \(x_{t}\) as input and outputs the corresponding semantic segmentation results \(p_{t}\). As shown in Sec. 1, the primary challenge in this task stems from the significant disparities between the two modalities. To address this challenge, we propose to leverage the TIDM  as our backbone which is pre-trained on a vast array of image-text pairs  to enhance its generalization and can robustly extract features across modalities. Next, to overcome

Figure 2: Our framework is divided into three parts. (1) Self-Training: Supervised loss in the source modality \(_{s}\) and pseudo-labeled loss \(_{t}\) in the target modality are used to train the network. (2) Diffusion-based Pseudo-Label Generation (DPLG): In the early stage of training, we add noise on the latent representation \(z_{t}\) to stabilize the pseudo-label generation. (3) Label Palette and Latent Regression (LPLR): The one-hot encoded labels \(y_{s}/_{t}\) are converted to RGB form by palette and then encoded to the latent space to supervise the UNet output \(o_{s/t}\).

the inaccurate pseudo-labels due to large modality gaps, we propose Diffusion-based Pseudo-Label Generation (DPLG), which stabilizes pseudo-label generation by injecting noise to the target modality. Moreover, TIDMs can only extract low-resolution features within the latent space, leading to a loss of semantic detail. To address this, we propose the Label Palette and Latent Regression (LPLR), which transforms pixel-wise classification into the regression, thereby allowing us to harness the fine-grained features upsampled by the pre-trained VAE Decoder. Our framework is illustrated in Figure 2. Next, we will introduce the our proposed DPLG and LPLR in detail.

### Diffusion-based Pseudo-Label Generation

As shown in Figure 2, in MADM, we employ the TIDM to perform a single-step diffusion to extract multi-scale features from the intermediate output of the denoising UNet, following the approach in . First, samples from source and target modalities \(x_{s},x_{t}\) are encoded to the latent representation \(z_{s/t}=(x_{s/t})\) by the pretrained VAE encoder \(\). Without any additional noise, we feed them to the denoising UNet and obtain multi-scale features \(f_{s/t}=(z_{s/t},c)\), where \(c\) is a learnable conditioned embedding instead of a textual description. Then, these features are subsequently fed into a segmentation head to generate the semantic segmentation prediction \(p_{s/t}=(f_{s/t})\).

Our training method is anchored on the self-training DAFormer  prevalent in UDASS. The training objective is a composite of supervised loss from the source modality and pseudo-labeled loss from the target modality. For the labeled source modality, we use a cross-entropy loss between the prediction \(p_{s}\) and the ground truth labels \(y_{s}\):

\[_{s}=_{CE}(p_{s},y_{s}).\]

For the unlabeled target modality, we adopt a student-teacher paradigm in self-training . Here, the existing network acts as the student, and through the Exponential Moving Average (EMA), we derive a teacher network . The teacher network generates pseudo-labels \(_{t}\), which then supervise the student network's output on \(A(x_{t})\), where \(A()\) represents the strong data augmentation .

However, the data distribution varies greatly between modalities. The teacher network is unable to provide accurate pseudo-labels for self-training, resulting in an unstable modality adaptation to the target modality. We observe that more robust pseudo-labels can be generated by injecting appropriate noise in the latent space and therefore propose the Diffusion-based Pseudo-Label Generation (DPLG). The proposed DPLG exploits the denoising property (perception of noise) of diffusion models to improve the robustness of semantic understanding on target samples.

Specifically, given a target sample \(x_{t}\), we first encode it into the latent representation \(z_{t}=(x_{t})\) and then apply a diffusion process that adds noise \(\) to \(z_{t}\) to obtain a noisy latent representation:

\[z^{}_{t}=_{k}}z_{t}+(1-_{k}}) ,\ \ (0,I),\ \ k=(0,1-i/ ).\]

Here, \(_{k}\) is a predetermined schedule that controls the amount of noise added at each step . \(k\) is the diffusion step that controls the proportion of noise based on the initial diffusion step \(\) and noise addition period \(\), and \(i\) is the current iteration count.

This noisy representation \(z^{}_{t}\) is then used to generate pseudo-labels \(_{t}=((z^{}_{t},c))\). In the pre-training of TIDMs , the objective is to estimate noise from latent inputs containing various noise levels. By injecting noise into the latent code, we effectively simulate this noisy distribution. This simulation aligns the latent space more closely with the data distribution encountered during the pre-training phase. Such alignment fosters a more robust and accurate semantic interpretation, which, in turn, enhances the quality of the pseudo labels generated. This shares similar

Figure 3: We visualize the pseudo-labels for event modality at the iteration of 1250, 1750, and 2250. The introduction of DPLG effectively improves the quality of pseudo-labels.

spirits in other applications of diffusion models, such as the text-to-3D  where injecting extra noise into data can improve the denoising quality of image and yields better pseudo labels. As shown in Figure 3, our DPLG can generate more accurate pseudo-labels compared to the noise-free addition. By strategically incorporating noise in the pseudo-label generation, DPLG enhances the model's adaptability to the target domain and mitigates the bias of semantic understanding. Then, the pseudo-labeled loss is formulated as:

\[_{t}=_{CE}(p_{t},_{t}) q.\]

Here, \(q\) is the confidence value calculated by the softmax probability of \(p_{t}\)[17; 18; 43]. The consistency regularization of \(x_{t}\) between the teacher and student networks promotes adaptation to the target modality. It encourages the student network to match the teacher's predictions, even under the perturbations introduced by strong data augmentation.

### Label Palette and Latent Regression

TIDMs compress the sample into a latent space with an 8x down-sampling factor, which leads to a serious loss of semantic detail. Specifically, for the input sample with a resolution of \(512 512\), it is reduced to \(64 64\) after embedded via \(\). Then, within the denoising UNet decoder, multi-scale features are extracted \(f_{s/t}\) after the 5th, 8th, and 11th blocks: \(64 64\), \(32 32\), and \(16 16\).

For diffusion-based depth estimation , leveraging the VAE decoder \(\) to upsample the denoised latent representation back to the original resolution is a natural fit, which recovers the fine-grained scene details. However, the above method is not applicable to semantic segmentation due to the inherent difference between regression and classification. Therefore, to address the problem of semantic detail loss, we propose the Label Palette and Latent Regression (LPLR) module which converts semantic segmentation into regression and utilizes the VAE Decoder \(\) to obtain high-resolution semantic features.

Initially, the one-hot encoded labels \(y_{s}\) and \(_{t}\) are transformed into a perceptually meaningful RGB space with a pre-defined palette. These RGB representations are then encoded back into the latent space and supervise the UNet's output \(o_{s/t}=(z_{s/t},c)\) in a regression form:

\[_{s/t}^{reg}=|((y_{s}/_{t}))-o_{s /t}|.\]

With this supervision, we are able to utilize the VAE decoder \(\) to obtain a high-resolution semantic regression feature \((o_{s/t})\). This feature, combined with the multi-scale features, is then fed to the segmentation head \(p_{s/t}=(f_{s/t},(o_{s/t}))\).

By employing LPLR, we effectively convert the semantic segmentation into a regression problem that can be tackled by the VAE decoder's upsampling capabilities, which retain the fine-grained details necessary for accurate segmentation. Finally, the total training objective is a sum of these losses:

\[=_{s}+_{t}+_{reg}(_{s}^{reg }+_{t}^{reg}).\]

## 4 Experiments

### Implementation Details

In our work, we utilize the Stable Diffusion v1-4 model , which has been pre-trained on the LAION-5B  dataset, as our TIDM. For the segmentation head, we instantiate it with the decoder from DAFormer . We train our MADM for 10k iterations with a batch size of 2 and an image resolution of 512\(\)512. The optimization is instantiated with AdamW  with a learning rate of 5e-6. For hyperparameters \(\), \(\), and \(_{reg}\) in DPLG and LPLR, we set them to {5000,60,1.0}/{8000,50,1.0}/{8000,50,10.0} for depth/infrared/event modalities, respectively. In our experiments, we adopt the Cityscapes-Image  dataset as the source modality and the DELIVER-Depth , FMB-Infrared , and DSEC-Event  datasets as the target modalities. Since the semantic classes in these datasets are not identical, we merge some semantically similar classes during training. Experiments are conducted on a NVIDIA H800 GPU, occupying about 57G memory.

### Datasets Setting

**Cityscapes-Image.** Cityscapes  is the source dataset in our experiments, which constitutes a real-world collection of street-view images captured across 50 distinct urban environments. The dataset is split into 2,975 training images and 500 validation images with a resolution of 2048\(\)1024. It provides comprehensive semantic labeling at the pixel-level with 19 distinct semantic classes.

**DELIVER-Depth.** DELIVER  is a synthetic dataset containing five environmental conditions created by the CARLA simulator . The dataset contains 25 semantic classes and 3,983/2,005/1,897 samples for training/validation/testing with a resolution of 1024\(\)1024.

**FMB-Infrared.** FMB  is an urban street dataset with 1,500 RGB-Infrared pairs at a resolution of 800\(\)600 with 14 semantic classes. It contains a wide range of real driving scenes under different lighting and weather conditions.

**DSEC-Event.** DSEC  is a stereo event camera dataset for driving scenarios. Driving data are recorded for 3,193 seconds in diverse illumination conditions and urban/rural environments. Event data have a resolution of 640\(\)480 with 11 semantic classes and we aggregate them into the edge form in a recurrent manner .

### Comparison with State of the Art Methods

Table 1 presents the comparison with existing SoTA methods DAFormer , MIC , PiPa , and Rein  across three modalities: Depth, Infrared, and Event. The comparison is based on the Mean Intersection over Union (MIoU) over all classes, a standard measure of segmentation accuracy.

Our MADM demonstrates a strong performance, achieving the MIoU of 53.49%, 62.23%, and 56.31% on the depth, infrared, and event modalities, respectively. It showcases a significant improvement over the SoTA method Rein  by +3.26%, +1.58%, +4.45%, which underscores the robustness and effectiveness of MADM in handling the other visual modalities. Also, it is worth noting that the self-training loss in our MADM is built upon DAFormer , exceeding it average of +17.9%.

Figure 4 offers a intuitive comparison of the semantic segmentation results. MIC  leverages the SegFormer backbone  that is pre-trained on the ImageNet-1k dataset , enabling it to capture

Table 1: Semantic segmentation results evaluated with MIoU (%) on three modalities. **Bold** numbers are the best, _underscored_ second best.

more details within scenes, such as "pole". However, it exhibits weaker modality understanding, leading to frequent mis-segmentation, such as incorrectly classifying the "sky" as the "road". In contrast, Rein  is built upon the DINOv2 backbone that is pre-trained on extensive, curated datasets without explicit supervision . This results in an improved semantic understanding of modalities compared to MIC . Nonetheless, Rein still encounters issues with mis-segmentation and instances of under-segmentation.

Our MADM stands out for its exceptional ability to output precise segmentation results that closely mirror the ground truth. The incorporation of TIDM significantly enhances the generalization of our approach, providing an enhanced comprehension of diverse visual modalities and substantially mitigating the mis-segmentation.

### Ablation Studies

Table 2 presents the complete ablation studies that quantify the performance gains achieved by incorporating our proposed DPLG and LPLR into the baseline. The "Baseline" column indicates the performance of the MADM model without DPLG and LPLR. It serves as a refer

   Modality & Baseline & w/ DPLG & w/ LPLR & MADM \\  Depth & 50.61 & 51.65 & 52.91 & **53.17\(\)**0.26 \\ Infrared & 56.28 & 61.86 & 58.75 & **62.14\(\)**0.18 \\ Event & 52.27 & 52.84 & 52.05 & **56.12\(\)**0.20 \\  Average & 53.05 & +2.40 & +1.85 & +4.09 \\   

Table 2: Ablation of DPLG and LPLR in depth, infrared, and event modalities.

Figure 4: Qualitative semantic segmentation results generated by SoTA methods MIC , Rein , and our proposed MADM on three modalities.

ence point but achieves performance on par with the SoTA methods in Table 1, which demonstrates the strong generalization of TIDMs.

(1) When DGLP is employed, the MIoU is improved by an average of +2.40%, highlighting the effectiveness of generating robust pseudo-labels. Especially in the infrared modality, it achieves a +5.58% relative improvement over the baseline. The quantitative results of the pseudo-labels enhancement are shown in Figure 3. (2) The application of LPLR contributes to an average gain of +1.85%, emphasizing the importance of high-resolution features for segmentation tasks. (3) By employing both DGLP and LPLR, we observe a significant enhancement in +4.09% over the baseline, which underscores the synergistic benefits of combining robust pseudo-labels generation with fine-grained feature extraction.

### Diffusion-based Pseudo-Label Generation

We analyze the pivotal roles of \(\) and \(\) in our proposed DPLG, particularly within the depth modality. These two parameters control the diffusion step \(k\) on \(z_{t}\), which is central to the stability and quality of pseudo-labels.

Table 3 provides a detailed presentation of the impact of \(\) and \(\). For instance, when \(\) is set to 5,000, an increase in \(\) from 40 to 60 leads to a noticeable improvement in performance, with the model achieving its peak score of 53.49%. However, further increasing \(\) to 80 results in a decline in performance, indicating the existence of an optimal balance between these parameters.

In Figure 5, we offer an illustration of how the diffusion step \(k\) influences the generation of pseudo-labels. With noise-free addition (\(k=0\)), the model encounters difficulties in accurately segmenting the "car" and "person" classes. Upon introducing a moderate quantity of noise (\(k=10 50\)), the segmentation is noticeably enhanced, yielding more robust segmentation. Conversely, an excessive amount of noise (\(k=200\)) leads to a significant degradation in segmentation.

### Label Palette and Latent Regression

Table 4 analyzes the loss weight \(_{reg}\) within the proposed LPLR, which regulates the contribution of the regression losses. A minimal \(_{reg}\) of 1.0 and 3.0 yields MIoU of 53.31% and 54.40%, respectively, indicating the initial benefits of incorporating regression losses. Increasing \(_{reg}\) to 10.0 achieves the optimal MIoU of 56.31%, signifying the most effective balance between the segmentation and regression losses.

In Figure 6, we offer an illustration of the impact of LPLR. It can be seen that the utilization of LPLR results in a more fine-grained segmentation, e.g., "person" and "vegetation" in the left yellow box, "road" and "sidewalk" in the right yellow box, which greatly improves the performance of our MADM.

  \(_{reg}\) & 1.0 & 3.0 & 5.0 & 10.0 & 15.0 \\  MIoU & 53.31 & 54.40 & 55.84 & **56.31** & 55.31 \\  

Table 4: Ablation of \(_{reg}\) in LPLR on event modality.

Figure 5: At the 1,250th iteration, we present a visual analysis of diffusion step \(k\) in DPLG.

   \(\)\(\) & 2,000 & 5,000 & 8,000 & Average \\ 
40 & 51.62 & 52.46 & 52.52 & 52.20 \\
60 & 51.30 & **53.49** & 52.55 & 52.45 \\
80 & 52.01 & 52.85 & 48.63 & 51.16 \\   Average & 51.64 & 52.93 & 51.23 & - \\  

### Benefits of MADM in Nighttime Datasets

As mentioned in Section 1, we indicate that other visual modalities present in real-world scenarios are valuable in nighttime perception. In this section, experiments are conducted on the infrared modality to prove this. The FMB-Infrared dataset  includes both image and infrared modalities on daytime and nighttime scenes. We adapt from cityscapes  with daytime RGB images to the nighttime image modality and infrared modality by our proposed MADA, respectively. Figure 7 and Table 5 show that the infrared modality has a clear advantage in the "Person" class due to obvious thermal differences and a good suppression of light interference.

## 5 Conclusion

In this paper, we present MADM. With the powerful generalization of TIDMs, we extend domain adaptation to modality adaptation, aiming to segment other unexplored visual modalities in the real-world. Meanwhile, we propose DPLG and LPLR to solve the problems of pseudo-labeling instability and low-resolution features extraction within TIDMs. We hope our method can motivate further research on visual modalities other than RGB images. **Limitations:** However, despite using only a single-step forward for the diffusion model, the computation far exceeds existing UDASS networks. Future work could focus on distilling the knowledge of TIDMs into lightweight models when adapting. **Broader Impacts:** Our work pushes the boundary of semantic segmentation for other visual modalities, which will benefit several applications like multimodal fusion. To the best of our ability, MADM has little to no negative social impact.

   Method & Sky & Build. & Person & Pole & Road & S.walk & Veg. & Vehi. & Tr.S. & MIoU (avg) \\  RGB & **88.85** & 68.14 & 64.79 & **25.80** & **89.09** & **32.43** & 70.32 & **84.13** & 7.27 & 58.98 \\ Infrared & 87.94 & **82.40** & **82.69** & 21.50 & 76.21 & 26.50 & **76.61** & 83.80 & **16.69** & **61.59** \\   

Table 5: Semantic segmentation results of RGB and infrared modalities evaluated with MIoU (%) on FMB dataset .

Figure 7: Visualization of daytime RGB images in Cityscapes dataset \(\) nighttime RGB and Infrared modalities in FMB dataset