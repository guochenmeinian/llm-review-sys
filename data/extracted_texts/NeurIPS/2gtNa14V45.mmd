# OneActor: Consistent Subject Generation via

Cluster-Conditioned Guidance

Jiahao Wang\({}^{1,2}\), Caixia Yan\({}^{1,*}\), Haonan Lin\({}^{1}\), Weizhan Zhang\({}^{1,*}\), Mengmeng Wang\({}^{3,4}\),

Tieliang Gong\({}^{1}\), Guang Dai\({}^{4}\), Hao Sun\({}^{5}\)

\({}^{1}\)School of Computer Science and Technology, MOEKLINNS, Xi'an Jiaotong University

\({}^{2}\)State Key Laboratory of Communication Content Cognition

\({}^{3}\)College of Computer Science and Technology, Zhejiang University of Technology

\({}^{4}\)SGIT AI Lab, State Grid Corporation of China

\({}^{5}\)China Telecom Artificial Intelligence Technology Co.Ltd

uguisu@stu.xjtu.edu.cn  {yancaixia,zhangwzh}@xjtu.edu.cn

Corresponding authors.

This work was completed during the internship at SGIT AI Lab, State Grid Corporation of China.

###### Abstract

Text-to-image diffusion models benefit artists with high-quality image generation. Yet their stochastic nature hinders artists from creating consistent images of the same subject. Existing methods try to tackle this challenge and generate consistent content in various ways. However, they either depend on external restricted data or require expensive tuning of the diffusion model. For this issue, we propose a novel one-shot tuning paradigm, termed OneActor. It efficiently performs consistent subject generation solely driven by prompts via a learned semantic guidance to bypass the laborious backbone tuning. We lead the way to formalize the objective of consistent subject generation from a clustering perspective, and thus design a cluster-conditioned model. To mitigate the overfitting challenge shared by one-shot tuning pipelines, we augment the tuning with auxiliary samples and devise two inference strategies: semantic interpolation and cluster guidance. These techniques are later verified to significantly improve the generation quality. Comprehensive experiments show that our method outperforms a variety of baselines with satisfactory subject consistency, superior prompt conformity as well as high image quality. Our method is capable of multi-subject generation and compatible with popular diffusion extensions. Besides, we achieve a \(4\) faster tuning speed than tuning-based baselines and, if desired, avoid increasing the inference time. Furthermore, our method can be naturally utilized to pre-train a consistent subject generation network from scratch, which will implement this research task into more practical applications. Project page: https://johnneywang.github.io/OneActor-webpage/.

## 1 Introduction

Diffusion probabilistic models [13; 31; 34] have shown great success in image generation. As a prominent subset of them, text-to-image (T2I) diffusion models [14; 28] significantly improve artistic productivity. Designers can simply describe a subject (e.g. a character, an object, an art style) that they desire and then obtain high-quality images of the subject. However, relying on the random sampling, diffusion models fail to maintain a consistent appearance of the subject across the generated images. Taking Fig. 1(a) as an example, ordinary diffusion models denoise a random noise from the noise space to a clean latent code guided by the given prompt, where the clean code corresponds to asalient image in the image space. As shown, if given 4 different prompts of the same subject "hobbit" and random noises, ordinary models will generate 4 hobbits with different identities. In other words, the generation of the ordinary models lacks _subject consistency_.

The subject consistency is a necessity in practical scenarios such as designing an animation character, advertising a product and drawing a storybook protagonist. As diffusion models prevail, many works try to harness the diffusion models to generate consistent content through the following paths. _Personalization_ learns to represent a new subject from several given images and thus generates images of that. _Storybook visualization_ manages to generate consistent characters throughout a story. However, they require external data to function, either a given image set or a specific storybook dataset. Such a requirement not only complicates practical usage, but also limits the ability to illustrating imaginary, fictional or novel subjects. More recently, _consistent subject generation_ is first proposed to generate consistent images of the same subject only driven by prompts. Thus, artists can easily describe the subject and start their creation, which is a more intuitive manner. Despite the pioneering work, their repetitive tuning process of the backbone model leads to expensive computation cost and possible quality degradation. Later, a tuning-free approach  equips the backbone model with handicraft modules to eliminate the tuning process. Yet, the extra modules double the inference time and exhibit inefficiency when generating a large number of images.

In fact, diffusion models can generate two consistent images albeit with low probability, demonstrating their inherent potential for consistent subject generation. Motivated by this, we believe all the ordinary model needs is a learned semantic guidance to tame its internal potential. To this end, we propose a novel one-shot tuning paradigm, termed as OneActor. We start from the insight that in the latent space of a pre-trained diffusion model, samples of different subjects form different clusters , i.e. _base cluster_. In a specific base cluster, samples that share common appearance gather into the same sub-cluster, i.e. _identity sub-cluster_. In the latent space of Fig. 1, for example, a "hobbit" base cluster contains four different identity sub-clusters. Ordinary generations spread out into four different sub-clusters, causing subject inconsistency. While in our paradigm of Fig. 1(b), users first choose one satisfactory image from the generated proposals as the target. Then after a quick tuning, our OneActor quickly learns to find the denoising trajectories towards the target sub-cluster, generating images of the same subject. Throughout the process, we expect to learn in the semantic space and avoid harming the inner capacity of the latent space.

To achieve high-quality and efficient generation, we pioneer the cluster-guided formalization of the consistent subject generation task and derive a cluster-based score function. This score function underpins our novel cluster-conditioned paradigm which leverages posterior samples as cluster repre

Figure 1: For every subject in the latent space, there are identity sub-clusters within the subject base cluster. (a) Given different prompts and initial noises, ordinary diffusion models generate inconsistent images from different identity sub-clusters of the ”hobbit” base cluster. (b) While our OneActor, after a quick tuning, provides an extra cluster guidance and thus generates images from the same target sub-cluster that show a consistent identity. Different colors denote different identity sub-clusters.

sentations. To systematically address overfitting, we enhance the tuning with auxiliary samples to robustly align the representation space to the semantic space. We further develop the semantic interpolation and cluster guidance scale strategies for more nuanced and controlled inference. Extensive experiments demonstrate that with superior subject consistency and prompt conformity, our method forms a new Pareto front over the baselines. Our method requires only 3-6 minutes for tuning, which is at least \(4\) faster than tuning-based pipelines. This efficiency gain is achieved without necessarily increasing the inference time, making our method highly suitable for large-scale image generation tasks. Additionally, our method's inherent flexibility allows consistent multi-subject generation and seamless integration into various workflows like ControlNet .

Our main contributions are: (1) We revolutionize the consistent subject generation by pioneering the cluster-guided task formalization, which supersedes the laborious backbone tuning with a learned semantic guidance to perform efficient generation. (2) We introduce a novel cluster-conditioned generation paradigm, termed as OneActor, to pursue more generalized, nuanced and controlled consistent subject generation. It highlights an auxiliary augmentation during tuning and features the semantic interpolation and cluster guidance scale strategies. (3) Extensive experiments demonstrate our method's superior subject consistency, excellent prompt conformity and the \(4\) faster tuning speed without inference time increase. (4) We first establish the semantic-latent guidance equivalence of T2I models, offering a promising tool for precise generation control.

## 2 Preliminaries

Before introducing our method, we first give a brief review of ordinary text-to-image diffusion models. To start with, Gaussian diffusion models [34; 13] assume a forward Markov process that gradually adds noise to normal image \(_{0}\):

\[_{t}=_{t}}_{0}+_{t}},\] (1)

where \(t[0,T]\), \((,})\) and \(_{t}\) are a set of constants. Meanwhile, a denoising network \(_{}\), usually a U-Net , is trained to reverse the forward process by estimating the noise given a corrupted image:

\[()=_{t[1,T],_{0},_{t}}[ \|_{t}-_{}(_{t},t)\|^{2}].\] (2)

Once trained, we can sample images \(_{0}\) from a Gaussian noise \(_{t}\) by gradually removing the noise step by step with \(_{}\). To introduce conditional control, classifier-free guidance  trains \(_{}\) in both unconditional and conditional manners: \(_{}(_{t},t,_{})\) and \(_{}(_{t},t,)\), where \(\) is the given condition and \(_{}\) indicates no condition. Images are then sampled by the combination of two types of outputs:

\[_{}(_{t},t,_{})+s(_{ }(_{t},t,)-_{}(_{t},t,_ {})),\] (3)

where \(s\) is the guidance scale. For text control, conditions \(\) are generated by a text encoder \(E_{t}\), which projects text prompts \(\) into semantic embeddings: \(=E_{t}()\). To accelerate the pipeline, latent diffusion models  pre-train an autoencoder to compress images into latent codes: \(=E_{a}()\), \(=D_{a}()\). Thus, the whole diffusion process can be carried out in the latent space instead of the salient image space.

## 3 Method

In our task, given a user-defined description prompt \(^{tar}\) (e.g. _a hobbit with robes_), a user-preferred image \(^{tar}\) is generated by an ordinary diffusion model \(_{}\) and chosen as the target subject. Our goal is to equip the original \(_{}\) with a supportive network \(\), formulating \(_{,}\). After a quick one-shot tuning of \(\), our model can generate consistent images of the same subject with any other subject-centric prompt \(^{sub}=\{^{tar},\}\) (e.g. _a hobbit with robes + walking on the street_). To accomplish this task, We first give mathematical analysis in Sec. 3.1. Then we construct a cluster-conditioned model and tune it in a generalized manner in Sec. 3.2. During inference, we generate diverse consistent images with the semantic interpolation and cluster guidance in Sec. 3.3.

### Derivation of Cluster-Guided Score Function

To start with, let's revisit the ordinary generation process. Given \(N\) initial noises and the same subject prompt, generations of \(_{}\) fail to reach one specific sub-cluster, but spread to a base region \(^{base}\) of different sub-clusters. If we choose one sub-cluster as the target \(^{tar}\) and denote the rest as auxiliary sub-clusters \(^{max}_{i}\), then \(^{base}=^{tar}\{^{max}_{i}\}_{i=1}^{N-1}\). The key to consistent subject generation is to guide the denoising trajectories of \(_{}\) towards the expected target sub-cluster \(^{tar}\). From a result-oriented perspective, we expect to increase the probability of generating images of the target sub-cluster \(^{tar}\) and reduce that of the auxiliary sub-clusters \(^{max}_{i}\). Thus, if we consider the original diffusion process as a prior distribution \(p()\), our expected distribution can be denoted as:

\[p()^{tar})}{_{i=1}^{N-1}p( ^{max}_{i})}.\] (4)

We take the negative gradient of the log likelihood to derive:

\[-_{} p()-_{} p(^{tar} {x})+_{i=1}^{N-1}_{} p(^{max}_{i}).\] (5)

With the reparameterization trick of , we can further express the scores as the predictions of the denoising network \(\) from a latent diffusion model  in a classifier-free manner :

\[_{}(_{t},t)+_{1}}(_{t},t,^{tar})-_{}(_{t},t)] }_{}-_{2}_{i=1}^{N-1}}(_{t},t,^{max}_{i})-_{}(_{t},t)]}_{},\] (6)

where \(_{1},_{2}\) are guidance control factors. If we introduce the concept of score function , Eq. (6) can be regarded as a combination of _target attraction score_ and _auxiliary exclusion score_. This formula, termed as _cluster-guided score function_, is the core of our method. We will manage to realize it in our subsequent parts. The detailed derivation is shown in Appendix F.

### Generalized Tuning of Cluster-Conditioned Model

In accordance with theoretical analysis of Eq. (6), we propose a novel tuning pipeline as shown in Fig. 2. In general, we incorporate the cluster representations to construct a cluster-conditioned model \(_{}(_{t},t,)\) with a supportive network \(\). We then prepare the data and tune the model with the help of auxiliary samples.

**Cluster-Conditioned Model.** In this model, the supportive network \(\) is designed to process the posterior samples into cluster representations of the semantic space. To specify, for each sample

Figure 2: The overall architecture of our method. (a) We first generate base images and construct the target and auxiliary set. (b) We design a cluster-conditioned model and tune the projector with batched data. (c) The projector consists of a ResNet network, linear and AdaIN layers. Tuning and freezing weights are denoted by fire and snowflake marks. The items used to compute different objectives are outlined in different colors. The unimplemented theoretical models are semi-transparent.

\(\), \(\) transforms its latent code \(\) and prompt embedding \(\) into a subject-specific vector \(c_{}\), i.e. \(c_{}=(,)\). For the framework of \(\), a naive way is to use a feature extractor and a space projector. Since the original U-Net encoder \(E_{u}\) is already well-trained to extract features from the latent codes, we use it directly as the extractor. However, the U-Net extractor may cause extra computational burden. To bypass the extractor, we approximate the features of \(_{0}\) with that of \(_{1}\):

\[=E_{u}(_{1},) E_{u}(_{0}).\] (7)

Thus, we save the intermediates \(\) of the U-Net encoder during data generation and then directly feed them into the projector for the semantic output: \(c_{}=(,)=(,E_{t}())\). This approximation condenses \(\) to only a projector and lowers the computational cost by 30%. The output vector \(c_{}\) represents the semantic direction of the sample's sub-cluster. We then split \(\) into word-wise embeddings \(c_{i}\) to locate the base word embedding \(c_{b}\) and offset it by:

\[c_{b}^{}=c_{b}+c_{}.\] (8)

The modified embedding \(c_{b}^{}\) later replaces \(c_{b}\) to form \(^{}\) and guides the noise predictions. By now, all the factors (i.e. \(,\)) that could determine a sub-cluster are involved in the cluster-conditioned model, so we can express the cluster-related terms in Eq. (6) as:

\[_{}(_{t},t,)=_{,}(_{t},t,,c_{})=_{,}( _{t},t,E_{t}(),(,E_{t}())).\] (9)

**Generalized Tuning with Auxiliary Samples.** The major challenge of one-shot tuning is overfitting because insufficiency of data may lead to severe bias and limited diversity of generated images. To overcome this challenge, we tune the model with not only target samples, but also auxiliary samples. To elaborate, as shown in Fig. 2(a), given \(^{}\) (e.g. _a **hobbit with robes**_) which contains the base word \(p_{i}\) (e.g. _hobbit_), we first input it into the ordinary diffusion model for \(N\) base images and the corresponding intermediates to form a base set: \(^{}=\{_{i}^{},_{i}^{}\}_{i=1}^{N}\). We randomly choose one image as the target sample \(^{},^{}\) and gather the others to form an auxiliary set: \(^{}=\{_{i}^{},_{i}^{}\}_{i=1}^{N-1}\). We apply face crop and image flip to target image for an augmented set: \(^{}=\{_{i}^{},^{}\}_ {i=1}^{N-1}\). Then in every tuning step, we randomly select 1 target and \(K\) auxiliary samples to form a batch of data: \(=\{^{},^{}\}\{_{i}^{ },_{i}^{}\}_{i=1}^{K}\). Thus, more data contributes to the optimization and the batch normalization can be applied to the projector, resulting in a more generalized projection. To carry out the tuning, in Fig. 2(b), we first get the latent codes by: \(=E_{a}(),\) and add noise \(_{t}\) to them. Then we input the noisy latent \(_{t}\), prompt \(\) and feature \(\) to the cluster-conditioned model with the projector \(\). The prompt \(\) is a random template filled with the base word (e.g. _a portrait of a **hobbit**_). We apply the standard denoising loss for both target and auxiliary samples:

\[_{}() =_{t[1,T],_{0}^{},_{t}} [\|_{t}-_{,}(_{t}^{ },t,E_{t}(),(^{},))\|^{2}],\] (10) \[_{}() =_{t[1,T],_{0}^{},_{t}} [\|_{t}-_{,}(_{t}^{ },t,E_{t}(),(^{},))\|^{2}].\] (11)

**Simplifying into Average Condition.** For the auxiliary exclusion score in Eq. (6), it's laborious to calculate the noise predictions of \(N-1\) auxiliary conditions in every inference step. To simplify, we substitute with an average condition \(c_{}^{}\), which is derived by averaging the semantic vectors of the auxiliary instances:

\[c_{}^{}=_{i=1}^{K}c_{,i}^{} =_{i=1}^{K}(_{i}^{},).\] (12)

Intuitively, this average condition indicates the center of all the auxiliary sub-clusters and repels the denoising trajectories away. The denoising loss is also used in this condition:

\[_{}()=_{t[1,T],_{0}^{ },_{t}}[\|_{t}-_{,}( _{t}^{},t,E_{t}(),c_{}^{}\|^{2} ].\] (13)

Note that the data \(^{}\) is randomly chosen from target and auxiliary set. This average condition will act as the empty condition of classifier-free guidance  later in Sec. 3.3. During tuning, we only fire the projector from scratch and freeze all other components. As shown in Fig. 2(c), the light-weight projector comprises a ResNet  network, linear and AdaIN  layers. The complete tuning objective consists of the above 3 losses weighted by hyper-parameters \(_{1},_{2}\):

\[()=^{}()+_{1} ^{}()+_{2}^{}( ).\] (14)

### Inference Strategies for Versatile Generation

In the inference stage, traditional tuning-based pipelines exhibit the imbalance between consistency and diversity and fail to extend to multiple subjects generation. To address these challenges, we propose a semantic interpolation strategy and a latent guidance scale strategy to achieve more nuanced and controlled inference. We further develop two variants to perform multiple subjects generation.

**Proof and Implementation of Semantic Interpolation.** Classifier-free guidance  proves that the conditional interpolation and extrapolation in the latent space, controlled by a guidance scale, reflect the same linear effect in the image space. We argue that the semantic space of the prompt embeddings also shares this property. To prove it, we carry out a toy experiment on SDXL . Since the empty condition \(_{}\) is also a semantic embedding obtained by encoding an empty string, we perform interpolation in the semantic space: \(^{}=_{}+g(-_{})\). Then we replace \(\) with \(^{}\) in the inference process to generate the images in Fig. 3. As shown, for the latent interpolation (left), when we increase the guidance scale \(s\) in Eq. (3) within a proper range, the generated image will be more and more compliant with the prompt. While for the semantic interpolation (right), with \(g\) increasing, we can observe the same liner reflection in the image space. This proves that the semantic space also possesses the ability of guidance interpolation. We believe that this is because the semantic and latent space are entangled by the denoising network and thus share some properties in common. With this key insight, we can precisely control the offset guidance effect of \(c_{}\) by replacing Eq. (8) with:

\[c^{}_{b}=c_{b}+v c_{},\] (15)

where \(v\) is a controllable semantic scale. We further investigate its effect in Appendix A.4. In short, a moderate \(v\) leads to an optimal balance of consistency and diversity.

**Sample with Cluster-Guided Score.** Given a subject-centric prompt \(^{}\) in the inference stage, we first input its semantic embedding \(^{}\) to the tuned projector \(^{*}\) for two representations:

\[c^{}_{}=^{*}(^{},^{ {sub}}),\] (16)

\[c^{}_{}=_{i=1}^{N-1}^{*}(^{ }_{i},^{}).\] (17)

Then we approximate the N-1 auxiliary predictions in Eq. (6) with one prediction under the average condition:

\[_{i=1}^{N-1}[_{,^{*}}(_{t},t,^{ },c^{}_{,i})-_{}(_{t },t,_{})]_{,^{*}}(_{t },t,^{},c^{}_{})-_{}( _{t},t,_{}),\] (18)

where the cluster-conditioned terms are written in the form of Eq. (9). With this approximation, Eq. (6) can be further transformed into:

\[_{}(_{t},t,_{ })&+_{1}[_{,^{*}}( _{t},t,^{},c^{}_{})-_{}(_{t},t,_{})]\\ &-_{2}[_{,^{*}}(_{t},t,^{},c^{}_{})-_{}(_{t},t,_{})].\] (19)

Now we are ready to sample consistent images with the cluster-guided score of Eq. (19). Since \(_{1}\) and \(_{2}\) play the same role as \(s\) in Eq. (3), we can manipulate them for an optimal performance. For large-quantity image generation, we can set \(_{2}=0\) to avoid inference time increase at the expense of a slight quality decrease. Besides, since our method acts by guiding the latent code to a specific

Figure 3: The observation of semantic-latent guidance equivalence property. We vary the latent guidance scale on the left side and the semantic interpolation scale on the right, respectively. The semantic and latent manipulations show the same effect, which proves our argument.

cluster step by step, we can loosen the restrictions by applying cluster guidance to certain steps instead of all steps. The effects of these inference strategies are also discussed in Appendix A.4.

**Generating Multiple Subjects via Two Variants.** To extend the generation of single subject to multiple subjects, we develop two different variants. In the first variant, we treat the multi-subject generation as a joint distribution problem and assume that an image of two subjects belongs to the intersection sub-cluster \(^{tar}=^{tar}_{1}^{tar}_{2}\). Given multiple target prompts \(=\{^{tar}_{j}\}_{j=1}^{L}\) (e.g. [_a hobbit_ with _robes_, _a white dog_]), we combine the prompts together to one prompt \(^{tar}\) (e.g. _a hobbit_ with _robes_ and _a white dog_) containing multiple base words. We generate multi-subject images with \(^{tar}\) and modify the projector to output multiple semantic vectors \(=\{c_{,j}\}_{j=1}^{L}\). Thus, we can run the tuning and inference process above to generate consistent multi-subject images. However, with all subjects fixed from the beginning, this variant is unable to perform continual subject addition.

In the second variant, for each \(^{tar}_{j}\), we treat it as an individual and perform single-subject tuning for multiple projectors \(^{*}_{j}\). Then given multi-subject-centric prompt during inference, we offset the embeddings of multiple base words with the semantic outputs \(=\{c_{,j}\}_{j=1}^{L}\) of the projectors. To avoid intersecting leakage of semantic information, we constrain the effect of every \(c_{,j}\) to the subject's corresponding spatial mask in the denoising process. Since all subjects are independent in this variant, we can continually add new subjects without affecting existing ones.

## 4 Experiments

### Baselines

To evaluate the performance of OneActor, we compare it with a wide variety of baselines: (1) tuning-based personalization pipelines, Textual Inversion (TI)  and DreamBooth (DB)  in a LoRA  manner; (2) encoder-based personalization pipelines, IP-Adapter (IP) , BLIP-Diffusion (BL)  and ELITE (EL) ; (3) consistent subject generation pipelines, TheChosenOne (TCO)  and ConsiStory (CS) . We will denote them by the abbreviations for simplification. For personalization pipeline, we first generate one target image with the target prompt. Then we use the target image as the input to perform the personalization. Our method and the tuning-based pipeline are implemented on SDXL . Note that TCO and CS have not offered official open-source codes, so we compare to the results and illustrations from their written materials for fairness. We also construct 3 ablation models: original SDXL, our model excluding \(_{aver}\) and our model excluding \(_{aux}\), \(_{aver}\). More implementation details and ablation study are presented in Appendix A.3 and H.

### Qualitative Illustration

**Single Subject Generation.** We illustrate the visual results of personalization baselines and our OneActor in Fig. 4. As shown, TI generates high-quality and diverse images but fails to maintain the consistency with the target image. With global tuning, DB shows, albeit a certain degree of consistency, limited prompt conformity. IP maintains consistency in some cases, but is unable to adhere closely to the prompt. BL suffers from inferior quality during generation. In contrast, benefiting from the intricate cluster guidance and the undamaged denoising network, our method demonstrates a balance of superior subject consistency, content diversity as well as prompt conformity. We specifically compare our OneActor with other consistent subject generation pipelines. Fig. 5 shows that all pipelines manage to generate consistent and diverse images. Yet our method preserves more consistent details (e.g. the gray shirt of the man, the green coat of the boy). Additionally, we illustrate more examples in Fig. 6, which reaffirms the excellent performance of our method.

**Multiple Subjects Generation.** We show double subjects generation comparison between baselines and the two variants of our OneActor in Fig. 7. Most pipelines, represented by DB and TCO, fail to perform multiple subjects generation. CS extends generation of single subject to multiple subjects. However, CS shows mediocre layout diversity, probably due to their unstable SDSA operations . In comparison, both variants of our method excellently maintain the appearances of two subjects and display superior image diversity. In general, our method is able to generate different types of subjects including humans, animals and objects (or their combinations). Also, our method is versatile for arbitrary styles and expressions ranging from realistic to imaginary. More qualitative results including triple subjects generation and amusing applications are illustrated in Appendix B and C.

### Quantitative Evaluation

**Metrics.** To provide an objective assessment, we carry out comprehensive quantitative experiments. We incorporate multi-objective optimization evaluation and encompass two objectives, identity consistency and prompt similarity, which are popular in personalization tasks [8; 33]. We instruct ChatGPT  to generate subject target prompts and templates. For a fair and comprehensive comparison, we adopt two metric settings from the two consistent subject generation pipelines [4; 37]. For the first setting of , the identity consistency is CLIP-I, the normalized cosine similarity between the CLIP  image embeddings of generated images and that of target image, while the prompt similarity is CLIP-T, the similarity between the CLIP text embeddings of the inference prompts and the CLIP image embeddings of the corresponding generated images. For the second setting of , the identity consistency is the DreamSim score  which focuses on the foreground subject and the prompt similarity is the CLIP-score .

Figure 4: The qualitative comparison between personalization pipelines and our OneActor. TI lacks consistency, while DB and IP exhibit limited prompt conformity and diversity. BL suffers from poor quality in certain cases. In contrast, our method shows superior consistency, diversity as well as stability. Target prompts and base words are marked blue and red, respectively.

Figure 5: The qualitative comparison between consistent subject generation methods. Though all methods generate consistent images given different prompts, our OneActor refines more details such as the characters’ clothes.

**Results.** In Fig. 8(a), we can observe that the baselines originally form a Pareto front in the first setting. To specify, DB exhibits highest identity consistency at the cost of lowest prompt similarity and diversity. Quite the contrary, TI and BL sacrifice identity consistency for prompt similarity. TCO and IP form a moderate region with balanced identity consistency and prompt similarity. In contrast, since our method makes full use of the capacity of the original diffusion model instead of harming it, it achieves a significant boost of prompt similarity as well as satisfactory identity consistency. Note that our method Pareto-dominates IP, TI and BL. In Fig. 8(b), the original Pareto front is determined by IP and three variants of CS in the second setting. Our method pushes the front forward and shows dominance over EL, TI, DB and two variants of CS. In summary, whether in the first or second setting, our OneActor establishes a new Pareto front with a significant margin. These results correspond to the qualitative illustrations above. More experiments are shown in Appendix A.

## 5 Related Work

**Consistent Text-to-Image Generation.** A variety of works aim to overcome the randomness challenge of the diffusion models and generate consistent images of the same subject. To start with, _personalization_, given several user-provided images of one specific subject, aims to generate consistent images of that subject. Some works [8; 38] optimize a semantic token to represent the subject, while others [33; 3] tune the entire diffusion model to learn the distribution of the given subject. Later, [1; 10; 18; 36] find that tuning partial parameters of the diffusion model is effective enough. Apart from the tuning-based methods above, encoder-based methods [9; 5; 2; 42; 39] carefully encode the given subject into a representation and manage to avoid user-tuning. While _multi-concept composition_[19; 6] goes a step further to display multiple custom subjects on the same generated image via specially designed attention mechanisms. However, depending on external given images, they fail to generate imaginary or novel subjects. Recently, a new _consistent subject generation_ task is proposed, which aims to generate images of one subject given only its descriptive prompts.

Figure 6: More qualitative results of OneActor. Our method demonstrates excellent subject consistency across characters, animals and objects.

Figure 7: Double subjects generation comparison between baselines and the two variants of our OneActor. Only ConsiStory and our method are able to maintain consistency of multiple subjects.

Though, its tuning of the whole diffusion model is expensive and may degrade the generation quality. Later, a new approach  designs handicraft modules to avoid the tuning process and extends consistent generation from single subject to multiple subjects. Yet the extra modules significantly increase the inference time of every image. Besides, based on the localization of the subject, it is incapable of abstract subject generation like artistic style. For these issues, we design a new paradigm from a clustering perspective. Learning only in the semantic space, it requires shorter tuning without necessarily increasing inference time. Compared to the training-free pipelines, our pipeline can be naturally utilized to pre-train a consistent subject generation network from scratch.

**Semantic Control of Text-to-Image Generation Models.** The semantic control starts from classifier-free guidance , which first proposes a text-conditioned denoising network to perform text-to-image generation. It entangles the semantic and latent space so the prompt can guide the denoising trajectory towards the expected destination. Since then, many works manage to manipulate the semantic space to accomplish various tasks. For personalization, they either learn an extra semantic token to guide the latent to the subject cluster  or push the token embedding to its core distribution to alleviate the overfitting . For image editing, they calculate a residual semantic embedding to indicate the editing direction [27; 24]. Besides, previous works [40; 21] utilize the semantic interpolation of two conditional embeddings for a mixed visual effect. These works repeatedly confirm that solely manipulating the semantic space is an effective way to harness diffusion models for various goals. Hence, in our novel cluster-guided paradigm, we transform the cluster information into a semantic representation. This representation will later guide the denoising trajectories to the corresponding cluster.

## 6 Conclusion

This paper proposes a novel one-shot tuning paradigm, OneActor, for consistent subject generation. Leveraging the derived cluster-based score function, we design a cluster-conditioned pipeline that performs a lightweight semantic search without compromising the denoising backbone. During both tuning and inference, we devise several strategies for better performance and efficiency. Extensive and comprehensive experiments demonstrate that intricate semantic guidance is sufficient to maintain superior subject consistency, as achieved by our method. In addition to excellent image quality, prompt conformity and extensibility, our method significantly improves efficiency, with an average tuning time of just 5 minutes and avoidable inference time increase. Notably, the semantic-latent guidance equivalence property proven in this paper is a potential tool for fine generation control. Furthermore, our method is also feasible to pre-train a consistent subject generation network from scratch, which implements this research task into more practical applications.

Figure 8: The quantitative comparison between baselines and our OneActor in (a) TheChosenOne setting and (b) ConsiStory setting. Either way, our method establishes a new Pareto front with superior subject consistency and prompt conformity.