# MAN TruckScenes: A multimodal dataset for autonomous trucking in diverse conditions

Felix Fent\({}^{1}\)  Fabian Kuttenreich\({}^{2}\)1  Florian Ruch\({}^{2}\)  Farija Rizwin\({}^{2}\)

Stefan Juergens\({}^{2}\)  Lorenz Lechermann\({}^{2}\)  Christian Nissler\({}^{2}\)  Andrea Perl\({}^{2}\)

Ulrich Voll\({}^{2}\)  Min Yan\({}^{2}\)  Markus Lienkamp\({}^{1}\)

\({}^{1}\)Technical University of Munich

School of Engineering & Design

Institute of Automotive Technology

\({}^{2}\)MAN Truck & Bus SE

truckscenes@man.eu

###### Abstract

Autonomous trucking is a promising technology that can greatly impact modern logistics and the environment. Ensuring its safety on public roads is one of the main duties that requires an accurate perception of the environment. To achieve this, machine learning methods rely on large datasets, but to this day, no such datasets are available for autonomous trucks. In this work, we present MAN TruckScenes, the first multimodal dataset for autonomous trucking. MAN TruckScenes allows the research community to come into contact with truck-specific challenges, such as trailer occlusions, novel sensor perspectives, and terminal environments for the first time. It comprises more than 740 scenes of 20 s each within a multitude of different environmental conditions. The sensor set includes 4 cameras, 6 lidar, 6 radar sensors, 2 IMUs, and a high-precision GNSS. The dataset's 3D bounding boxes were manually annotated and carefully reviewed to achieve a high quality standard. Bounding boxes are available for 27 object classes, 15 attributes, and a range of more than 230 m. The scenes are tagged according to 34 distinct scene tags, and all objects are tracked throughout the scene to promote a wide range of applications. Additionally, MAN TruckScenes is the first dataset to provide 4D radar data with 360\({}^{}\) coverage and is thereby the largest radar dataset with annotated 3D bounding boxes. Finally, we provide extensive dataset analysis and baseline results. The dataset, development kit, and more are available online.

## 1 Introduction

Autonomous trucking has the potential to fundamentally change today's traffic by increasing safety on public roads, reducing logistics costs, and counteracting the shortage of drivers . However, the safe and reliable operation of autonomous trucks depends on an accurate perception of the surroundings. To achieve this, modern self-driving vehicles rely on machine learning algorithms to detect, track, and predict surrounding objects. However, the use of machine learning methods also drives the need for large-scale datasets.

While numerous datasets exist for autonomous passenger cars , datasets for autonomous trucks are missing. However, heavy-duty vehicles have their unique challenges. Large vehicles requiredifferent sensor mounting positions and rely on multiple sensors to cover the entire surrounding area. Moreover, trucks have to contend with occlusions from their own vehicle that change dynamically due to a movable truck-trailer combination and are affected by relative movements between their chassis and cabin. Furthermore, long-haul trucks operate in inherently different surroundings, such as logistics or container terminals and their functionality has to be ensured under all environmental conditions to maintain a functioning logistics system. Therefore, a dedicated truck dataset is needed to develop reliable perception solutions for self-driving trucks under all conditions.

To address this research gap and accelerate the development of self-driving trucks, we present MAN TruckScenes, the first large-scale dataset for autonomous trucking. Our multimodal dataset comprises data from a state-of-the-art sensor suite, including multiple high-resolution cameras, lidar, and radar sensors to provide full coverage of the surrounding area. Especially, the inclusion of six 4D radar sensors makes it the largest radar dataset available. In addition, the data of a high-precision GNSS and two IMU units are included to support a multitude of applications.

Moreover, most existing AV datasets are restricted to a particular operational design domain, whereas the MAN TruckScenes dataset covers a large geographical area, three seasons, nighttime driving, and numerous different weather conditions, including fog, rain, and snow. Furthermore, the dataset includes scenes in logistics terminals and recordings of vehicles with high relative velocities on the German Autobahn, making it the first dataset to promote the unique challenges of long-haul trucks.

In order to support the development of deep learning methods, MAN TruckScenes provides high-quality annotations for 747 scenes. These annotations consist of manually annotated 3D bounding boxes with unique instance identifiers for object tracking, standardized scene tags, and attribute labels to indicate the object's state. The dataset annotation was subject to a multi-stage labeling and quality assurance process of specially trained labelers to ensure accurate and consistent labeling.

The dataset will be published alongside a development kit, evaluation code, taxonomy, and detailed annotation instructions. This ensures transparency and reproducibility and simplifies the use of the dataset. Furthermore, we build on the established nuScenes  data format to ensure easy integration and code compatibility. Finally, MAN TruckScenes is published under the CC BY-NC-SA 4.0 license to accelerate research on autonomous trucks and shape the future of logistics.

Figure 1: Exemplary selection of a terminal, rain, and snow scene of the MAN TruckScenes dataset. The top row shows the fused lidar point cloud, the center row shows images of the front left camera, and the fused radar point cloud is shown at the bottom.

## 2 Related Work

The advances in autonomous driving have largely been driven by the release of public datasets. Over the course of the last decade, we have seen a trend towards high-quality, large-scale, and more diverse multimodal datasets that enabled innovation in the autonomous driving domain.

The KITTI  dataset, released in 2012, is one of the most influential autonomous driving datasets to date. It provides 22 scenes of annotated camera and lidar data in combination with high-precision GNSS and IMU data. However, the dataset's extent and diversity are limited, radar and map data are not provided, and driving data under severe weather conditions are not included.

The nuScenes  dataset had a great impact as one of the first large-scale, multimodal datasets for autonomous driving. Next to camera and lidar data, it also includes radar and map data for 1000 annotated scenes. It provides data from two different cities, annotates 23 different object classes, and introduces unlabeled intermediate sensor data sweeps. In addition to the small geographical coverage and limited coverage of severe weather conditions, the nuScenes dataset has been criticized for its sparse radar data [6; 19].

The Waymo Open  dataset is one of the largest annotated datasets for autonomous driving with a focus on scalability. It provides annotated camera and lidar data for 1150 scenes, covers a geographical area of 76 km\({}^{2}\), and provides a rich ecosystem of different related datasets. However, it does not include radar, GNSS, or map data and has a limited annotation range of 80 m with only four annotation classes.

The Argoverse 2  dataset takes a different approach with the provision of long-range annotations, comprehensive map data, and extensive object taxonomy. Nevertheless, it does not provide radar, GNSS, or IMU data and covers a limited geographical area.

The Zenseact Open Dataset (ZOD)  makes a different set of trade-offs. It provides 1473 annotated scenes from six different countries and facilitates long-range perception with an annotation range of 245 m. Despite its provision of two GNSS and IMU units, the ZOD's sensor setup, as well as its number of annotated samples, is limited.

The aiMotive  dataset focuses on the collection of diverse driving scenes under severe weather conditions. It provides long-range annotations and data from a multimodal sensor setup for three distinct areas. However, the dataset's extent is limited to a duration of 0.7 h and the two radar sensors do not cover a 360\({}^{}\) field of view (FoV).

    & KITTI & nuScenes & Waymo & Argo2 & ZOD & aiMotive & VoD & Ours \\  &  &  &  &  &  &  &  & \\  Scenes & 22 & 1000 & 1150 & 1000 & 1473 & 176 & 21 & 747 \\ Sample & 1.5 k & 40 k & 230 k & 150 k & 1.5 k & 27 k & 9 k & 30 k \\  Duration & 1.5 h & 5.5 h & 6.4 h & 4.2 h & 8.2 h & 0.7 h & 0.2 h & 4.2 h \\ Coverage & - & 4 km\({}^{2}\) & 76 km\({}^{2}\) & 17 km\({}^{2}\) & 26 km\({}^{2}\) & 180 km\({}^{2}\) & 2 km\({}^{2}\) & 100 km\({}^{2}\) \\  Camera & 4 & 6 & 5 & 9 & 1 & 4 & 1 & 4 \\ Lidar & 1 & 1 & 5 & 2 & 3 & 1 & 1 & 6 \\ Radar & 0 & 5 & 0 & 0 & 0 & 2 & 1 & 6 \\  GNSS & 1 & 1 & 0 & 0 & 2 & 1 & 1 & 1 \\ IMU & 1 & 1 & 0 & 0 & 2 & 1 & 0 & 2 \\ Map & no & yes & no & yes & no & no & no & no \\  Range & 91 m & 141 m & 80 m & 214 m & 245 m & 228 m & 26 m & 226 m \\ Classes & 3 & 23 & 4 & 30 & 15 & 14 & 13 & 27 \\  Vehicle & car & car & car & car & car & car & car & truck \\   

Table 1: Comparison of publicly available perception datasets for autonomous driving. The coverage represents the geographical coverage calculated according to  and range refers to the 99.9th percentile of all bounding box distances . Scenes are temporal consistent sequences and samples are annotated keyframes, following the nuScenes  notation. Vehicle refers to the recording vehicle.

The View-of-Delft  dataset is one of the first large-scale datasets with annotated 4D radar data and promotes the development of radar-based perception methods for vulnerable road users (VRUs) in city environments. Nevertheless, the geographical coverage of the dataset is limited, it does not include severe weather conditions, and only provides data from a single front-facing radar sensor.

While there are numerous other datasets focusing on various different aspects , all of the aforementioned datasets are limited to passenger cars, as shown in Table 1. To the best of our knowledge, there are only two datasets that include truck data. One is the TuSimple  dataset, which consists of 6408 images from a single front camera and provides annotations for lane markings only. The other is the SurMine  dataset, which is a proprietary dataset that only includes 2D bounding box annotations for 10665 camera images recorded in a mining facility. Hence, there are no large-scale or multimodal perception datasets for autonomous trucking.

## 3 Dataset

MAN TruckScenes aims to close this research gap by providing the first large-scale multimodal dataset for autonomous trucking. It consists of 747 scenes from typical long-haul truck environments and provides multimodal data with long-range annotations based on the nuScenes  format.

### Sensor Setup

The sensor suite consists of a multimodal sensor setup with four cameras, six lidar, and six radar sensors. Additionally, the dataset provides high-precision RTK-GNSS data and measurements of two IMU units. Detailed information on the sensor specifications can be found in Table 2 and the sensor positions are shown in Figure 2.

The main perception sensors are arranged in two sensor modules, one on either side of the vehicle, to maximize spatial coverage and minimize relative sensor movements. Each sensor module houses two

   Sensor & Details \\  Camera & 4\(\) Sekonix SF3324, RGB, 10 Hz, 1928 \(\) 1208, 120\({}^{}\)\(\) 73\({}^{}\) FoV \\ Lidar & 2\(\) Hesai Pandar64, 10 Hz, 64 layer, 360\({}^{}\)\(\) 40\({}^{}\) FoV, 200 m@10 \% \\  & 4\(\) Ouster OS0, 10 Hz, 64 layer, 360\({}^{}\)\(\) 90\({}^{}\) FoV, 35 m@10 \% \\ Radar & 6\(\) Continental ARS 548 RDI, 20 Hz, 76 GHz, 100\({}^{}\)\(\) 28\({}^{}\) \\ GNSS & 1\(\) GeneSys ADMA-G-PRO+, 100 Hz, 0.01 m pos., 0.015\({}^{}\) heading \\ IMU & 2\(\) Xsens MTi-680G-SK, 100 Hz, 9 DoF \\   

Table 2: Sensor specifications of the MAN TruckScenes setup.

Figure 2: Placement of the sensors and their corresponding coordinate systems from a top view perspective. The right sensor module is shown in the detailed drawing, the left module is identical but mirrored and the radar sensors are flipped around the x-axis. The top-mounted lidar sensors (at the sides and the front) are tilted downward. The GNSS uses a virtual coordinate system similar to the vehicle frame but is located next to the chassis IMU.

Sekonix cameras, one Hesai lidar, and three Continental radar sensors. Besides these two "corner modules", the vehicle is equipped with three Ouster lidar sensors mounted on the roof of the cabin, which are tilted downwards for blindspot coverage, and one additional Ouster lidar at the rear of the semi-trailer truck. The top mounted lidar sensors and the lidar sensors of the corner modules are positioned at a height of 3.2 m and 2.2 m, respectively. These elevated positions help to reduce occlusions, protect pedestrians, and prevent sensor damage. However, this sensor perspective marks a significant difference from conventional passenger car datasets.

Another unique feature of our dataset is the inclusion of six 4D radar sensors with a spatial coverage of nearly 360\({}^{}\) (except from the occlusion by the ego vehicle's trailer). In contrast to conventional 3D radar sensors that operate in the range-azimuth plane, 4D radar sensors can resolve objects in both azimuth and elevation angle. Furthermore, our radar sensors provide on average 2600 points per sample, while conventional radar sensors (e.g. in the nuScenes dataset) provide only 200 data points. Therefore, MAN TruckScenes is the first dataset to provide 4D radar data with 360\({}^{}\) coverage and is the largest radar dataset with annotated 3D bounding boxes.

To accurately capture the vehicle's state and position, the data of two IMUs and one RTK-GNSS are included. The GNSS unit uses a dual antenna setup to measure the heading angle of the vehicle and RTK correction data to achieve a position accuracy of up to 0.01 m. The two IMU units are mounted on the chassis and the cabin of the vehicle to get accurate measurements of the overall vehicle state on one side and measure relative movements between the chassis and the cabin on the other. This is important to compensate for sensor movements that are primarily mounted on the movable cabin and represent a special challenge of the truck case.

### Sensor Synchronization

Sensor synchronization is an important topic for multimodal datasets to ensure temporal sensor data alignment. This includes not only sensor time synchronization, which ensures that all sensors are based on the same reference clock, but also triggering the sensors to achieve cross-modality consistency.

The sensor time synchronization is based on the Precision Time Protocol (PTP) in accordance with the IEEE/IEC 1588-2008 . This procedure ensures the alignment of the individual sensor clocks with a high-precision Mobatime DTS 4160 PTP grandmaster clock. As a result, the individual sensors have a time deviation of less than 100 \(\)s and are referenced to the global UTC time. However, this only ensures that all sensors are based on the same reference clock, but not that all measurements are taken at the exact same point in time.

To temporally align the actual sensor measurements, the sensors have to be triggered. For this purpose, the lidar sensors are defined as reference sensors and all other perception sensors are triggered based on them. In the first step, all cabin-mounted lidar sensors are phase-synchronized such that their lidar points are temporally consistent in a clockwise manner. Secondly, the four cameras are triggered at the point where the lidar sweep and the rolling shutter of the cameras align in the center of the image. Lastly, the radar sensors are synchronized with the corresponding lidar sensors but triggered with a small delay to one another to minimize interference between them. Besides that, all radar sensors are assigned to slightly different frequency bands to further minimize interference.

### Sensor Calibration

While sensor synchronization ensures the temporal alignment of the sensor data, sensor calibration ensures the spatial alignment. The calibration aims to determine both the extrinsic and intrinsic sensor parameters to estimate both the sensor poses as well as the parameters of the sensor models.

The sensor calibration consists of four steps and includes an initial extrinsic calibration, a combined extrinsic and intrinsic calibration, an angular correction, and a final validation. The first step is a photogrammetric scan of the vehicle to determine the position and orientation of the sensors with respect to the vehicle frame. The vehicle frame is chosen to be the center of the rear axle projected onto the ground plan in accordance with ISO 8855:2011 . For the main calibration, the vehicle is placed in a dedicated calibration hall equipped with specialized calibration targets. These targets are represented by differently oriented planes in 3D space equipped with unique identifiers. The actual calibration uses a plane-matching algorithm to jointly estimate the extrinsic parameters of the camera and lidar sensors as well as the intrinsic camera parameters. Within this calibration procedure, the vehicle is moved back and forth to calibrate the sensors with respect to the vehicle frame. The third step uses a faraway calibration target aligned with the vehicle's longitudinal axis to correct remaining yaw angle errors and a dynamic landmark-based calibration to align the heading angle of the overall sensor setup. Finally, the calibration is validated on the data level by comparing point cloud alignments and projecting the point cloud data onto the image data. Moreover, an application-level validation is used to compare detections, odometry, and localization measurements to validate the calibration.

This procedure results in a precise calibration with pixel-level accuracy between camera and lidar sensors. However, the radar calibration solely relies on external measurements of the photogrammetric scan with a measurement tolerance of 0.05 mm. Therefore, angular errors of up to 0.03\({}^{}\) and sensor internal misalignments cannot be excluded. Besides that, the odometry-based validation can only ensure a heading error of less than 0.1\({}^{}\) and one has to be aware of increasing uncertainties within the camera intrinsic calibration towards the edges of the camera image.

### Sensor Data

The provided sensor data was going through different processing steps to ensure compatibility with the nuScenes  format and enhance usability. The properties of the five different sensor data types are explained in the following.

The camera data of the four camera sensors undergo an undistortion process using a pinhole camera model and a Lanczos interpolation scheme with a \(8 8\) kernel and constant padding values. The resulting image is cropped to a \(1980 943\) pixel size and stored as a compressed JPEG  image. The Camera data is sampled at a frequency of 10 Hz in correspondence with the lidar.

The lidar data is provided as point clouds of a variable number of points represented by their \(x,y,\) and \(z\)-coordinates in a cartesian sensor coordinate system, an intensity value, and an individual UNIX timestamp, allowing for point-wise motion compensation. The final point clouds are stored as binary (Marc Lehmann's LZF) compressed data in the point cloud data (pcd) format to save on disc space.

The radar data is also represented as point clouds where each point is defined by its \(x,y,\) and \(z\)-coordinates, its relative radial velocity in \(x,y,\) and \(z\)-direction as well as its radar cross section (rcs). In contrast to the lidar data, the radar data is stored as pcd files in a binary format due to its smaller file size.

The IMU data includes the current velocity and acceleration in \(x,y,\) and \(z\)-direction as well as the angle and angular velocity (rate) in roll, pitch, and yaw. The data is stored in a JSON file. The GNSS data provides the vehicle's position in UTM-WGS84 coordinates mapped to cell U32 and orientation given as quaternion (\(qw,qx,qy,qz\)). The GNSS data off all timestamps is stored in a JSON file. IMU and GNSS data are sampled with a frequency of 100 Hz to provide the possibility for highly accurate motion estimation.

Besides that, the rear lidar is limited to an FoV of 180\({}^{}\), which increases its detection range to 42 m at 10 % reflectivity, and the points of the top mounted lidar sensors are cut off for yaw angles beyond \(\)120\({}^{}\). It is also worth mentioning that the sensor data is not just provided at the sample annotation frequency of 2 Hz but at their individual sensor captioning frequencies listed in Table 2. These unannotated intermediate frames are denoted as sweeps. It is also important to note that the radar sensors cannot guarantee a fixed sampling rate of 20 Hz but rather drop frames, if their internal data processing takes too long, which results in an average data rate of 19.6 Hz.

### Scene Selection

The scene selection aims to select a diverse set of scenarios with challenging driving situations representing long-haul trucks' operational design domain. To meet this goal, 747 scenes with approximately 20 s each are manually selected from more than 25 h of measurement drives. The scenes include recordings from various different areas (e.g. highway, terminal, rural, city), three seasons of the year, challenging weather situations (e.g. rain, fog, snow), and difficult environmental conditions (e.g. nighttime, twilight). Furthermore, the dataset covers different driving maneuvers (e.g. overtaking, offloading), traffic scenarios, and observations of rare object classes (e.g. animals,emergency vehicles). As a result, the dataset includes rainy nighttime drives that cause mirroring effects in the lidar point cloud, recordings in a logistics terminal with a lot of metal shipping containers which lead to multipath reflections in the radar point cloud, and tunnel passages with strong illumination changes that effect the camera images. Therefore, the dataset provides challenging conditions for all sensor modalities to promote research in the area of robust perception. The distribution of the different scene tags is shown in Figure 3

### Data Annotation

The data of the selected scenes is manually annotated at 2 Hz based on a fused and ego motion-compensated lidar point cloud aggregation. To ensure a high-quality standard, independent annotation and quality assurance companies have been commissioned. Within the labeling process, each sample undergoes a maximum of three consecutive annotation and quality assurance cycles until the quality target is met. Furthermore, randomly selected samples pass through a dual-control cycle to validate the annotation quality.

Annotations are made in the form of 3D bounding boxes defined by their center point \((x,y,z)\), size \((w,l,h)\), and orientation \((qw,qx,qy,qz)\), given as quaternion. Each bounding box instance is classified according to 27 different object classes, using a hierarchical class structure, and in accordance with the nuScenes  data format. The distribution of the object categories is shown in Figure 4. In addition, every individual bounding box is labeled according to five attributes (with a total of 15 possible values) representing its visibility level or activity state. Furthermore, objects are tracked throughout the scene and assigned a unique and consistent tracking ID. Besides that, we label all scenes according to 34 distinct scene tags divided into seven different categories, including area, weather, and lighting conditions, as shown in Figure 3.

### Dataset Splits

The dataset is split into a train, validation, and test set to ensure an independent evaluation. While these splits should be different from one another to test the generalization capabilities of a method, the data splits should also have similar characteristics to ensure a fair evaluation.

To address this conflict of objectives, we developed a method to find a Pareto-optimal solution for this multi-objective optimization problem (MOOP). For this purpose, the optimization uses the NSGA-II  genetic algorithm with random sampling, polynomial mutation , and simulated binary crossover  without duplicates. The optimization problem

\[&_{s S}(f_{1}(s),f_{2}(s),...,f_{12}(s))\\ &|S_{}|=0.7|S|,\ \ |S_{}|=0.2|S|\] (1)

is given as a minimization problem with constraints, where \(S\) is the set of all scenes \(s\) defined by their scene properties (number of object classes, scene tags, sample timestamps, and ego positions) with cardinality \(|S|\). The objectives \(f_{1}(s)\) to \(f_{8}(s)\) are the minimization of the deviations of the discrete

Figure 3: Distribution of the scene tags for all 747 dataset scenes.

distributions of annotations across dataset splits, which are chosen to be the class distribution and the distributions of the seven individual scene tag categories. The objectives \(f_{9}(s)\) and \(f_{10}(s)\) aim to maximize the intra-split standard deviation of temporal (sample timestamps) and spatial (ego vehicle positions) components of the scenes. Finally, \(f_{11}(s)\) and \(f_{12}(s)\) aim to maximize the inter-split Kullback-Leibler (KL) divergence of the temporal and spatial components. The split sizes are set to 70 %, 10 %, and 20 % of all scenes for the train, validation, and test splits.

### Privacy

Compliance with data protection measures is a top priority, which is why the whole dataset is anonymized. The anonymization includes not only the blurring of faces and license plates in the image data but also the anonymization of timestamp information. Nevertheless, the temporal consistency is still guaranteed for all samples and sensor data. As shown by Alibeigi et al. , the chosen image anonymization should not affect the downstream perception tasks.

## 4 Tasks

MAN TruckScenes supports a multitude of different perception tasks through its multimodal nature, sequential data structure, and rich annotations. These tasks include object detection, tracking, prediction, and localization, even if we want to put special emphasis on 3D object detection.

The detection task requires detecting 3D bounding boxes of 12 different object classes which are a subset of the original 27 annotation classes. These classes are selected based on the nuScenes  detection task and the insides gained during our labeling campaign. As a result, the evaluation excludes object classes that are not present in all data splits and combines subclasses with high inter-class confusion (seen during labeling) in accordance with our hierarchical class structure.

The evaluation is based on the nuScenes Detection Score (NDS), which is a weighted sum of the mean Average Precision (mAP) and five True Positive (TP) metrics . In contrast to an Intersection over Union (IoU) based mAP, the NDS uses a distance-based mAP and averages the individual Average Precision (AP) values not only over the classes but also over four discrete distance thresholds. In addition to the mAP, the NDS takes five TP metrics into account, which are the Average Translation Error (ATE), Average Scale Error (ASE), Average Orientation Error (AOE), Average Velocity Error (AVE), and Average Attribute Error (AAE). Further details on the NDS and its calculation can be found in .

In contrast to the NDS, our detection range is not limited to 50 m during evaluation but considers objects of up to 150 m around the vehicle. This should emphasize the development of long-range detection methods, which are crucial for the safe operation of autonomous vehicles on highways. Furthermore, we introduce the animal and traffic sign classes as two additional detection classes, leading to 12 distinct detection classes.

Figure 4: Number of annotated 3D bounding boxes for all 27 object classes across all dataset splits.

## 5 Experiments

We provide baseline 3D object detection results for all three sensor modalities, which are shown in Table 3. The camera results are based on the PETR  model architecture with a pre-trained FCOS3D (V-99-eSE) backbone and an adjusted detection head to support 12-class classification. The detection distance is set to \( 150\,\) and the final image resolution is chosen to be \(320 800\) pixels. The radar baseline is set by RadarGNN , which was trained on fused, ego-motion compensated 4D radar point clouds with 6 aggregated sweeps. The node features consisted of the rcs values, timestamps, connectivity degrees, and the relative radial velocities of the radar points. Lastly, a CenterPoint  model was trained on fused ego-motion compensated lidar point clouds with 3 aggregated sweeps cropped to a \(300\, 300\,\) grid with a voxel resolution of \((0.1\,,0.1\,,0.2\,)\). The model used seven detection heads and was evaluated for 12 detection classes, according to our metric definition.

The results suggest that the detection quality of diverse object classes (like 'animal' or 'other vehicle') is insufficient and that the overall detection quality decreases significantly with increasing distance. Moreover, the results show a reduced detection quality in winter conditions and tunnels. Furthermore, it can be observed that the camera model provides insufficient results for minority classes and struggles with challenging weather and lighting conditions, whereas the radar-based detection model provides insufficient results for all non-metallic object classes. Moreover, it can be observed that the radar performance is negatively affected by reflections within tunnels or container terminals. The lidar performance, on the other hand, is reduced under rainy or foggy conditions but also struggles within tunnel scenarios and seems to be more sensitive to the number of available training samples. In general, the results show that a more robust long-range perception is required for safe autonomous trucking.

## 6 Conclusion

In this work, we presented MAN TruckScenes, the first dataset for autonomous trucking. It is a large-scale multimodal dataset for the development of autonomous truck perception applications. The dataset consists a diverse set of scenes recorded in a multitude of different environmental conditions and provides 360deg coverage of all sensor modalities, including 4D radar data. It contains carefully reviewed 3D bounding boxes, tracking information, and rich annotations for all objects within more than 230 m range. Furthermore, we introduced a generic method for compiling meaningful and balanced data splits. We provide baseline results for 3D object detection and promote research in all areas of perception, tracking, and prediction. For this purpose, we will maintain a public leaderboard and provide annotation instructions, taxonomy specifications, and a development kit. With this dataset, we aim to accelerate the development of autonomous trucking.

## 7 Limitations

The dataset is limited to measurements on public roads and logistic terminals in Germany recorded on a single autonomous test truck. The data distribution represents the operational design domain of a long-haul truck and is not representative for other distribution haulage. The dataset is manually annotated and, therefore, subject to human annotation errors even if our extensive quality assurance process seeks to minimize them. The extrinsic sensor calibration can be affected by cabin movements, the IMUs are exposed to vehicle vibrations, and the ego vehicle position is subject to GNSS inaccuracies even with RTK correction.

   Method & Dataset & Modality & mAP & NDS & ATE & ASE & AOE & AVE & AAE \\  PETR  & MAN & camera & 0.02 & 0.12 & 1.13 & 0.69 & 0.65 & 1.50 & 0.56 \\ RadarGNN  & MAN & radar & 0.07 & 0.11 & 0.89 & 0.81 & 1.13 & 8.00 & 0.57 \\ CenterPoint  & MAN & lidar & 0.27 & 0.41 & 0.41 & 0.35 & 0.28 & 2.73 & 0.20 \\   

Table 3: Baseline results on the test split of the MAN TruckScenes dataset v1.0.

## 8 Societal Impact

Despite the potential benefits of autonomous trucks the societal impact of the technology must be considered. Since autonomous trucks operate on public roads, their safety is a major concern that must be assured and cannot be compromised for economic reasons. While safety assurance is not just the key deployment factor, it is also the main driver for public acceptance and trust . Therefore, regulatory authorities have to establish standards that ensure the safety of autonomous trucking and allow cross-border operation. Besides that, the impact on labor must be taken into account. While some studies expect job losses through autonomous vehicles , others argue that trained personnel will still be needed in the overall logistics process to supervise the system or carry out manual tasks . Recent studies show that advanced automation can even help to improve working conditions and therefore, counteract the shortage of drivers and minimize turnover rates . Ultimately, regulatory measures must be put in place to ensure that the interests of the general public are protected.