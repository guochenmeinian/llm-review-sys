# Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks (Appendix)

Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks

 Eeshaan Jain\({}^{@sectionsign}\), Tushar Nandy\({}^{@sectionsign}\), Gaurav Aggarwal\({}^{}\),

Ashish Tendulkar\({}^{}\), Rishabh Iyer\({}^{*}\) and Abir De\({}^{@sectionsign}\)

\({}^{@sectionsign}\)IIT Bombay, \({}^{}\)Google, \({}^{}\)UT Dallas

{eeshaan,tnandy,abir}@cse.iitb.ac.in,

{gauravaggarwal,ashishvt}@google.com, rishabh.iyer@utdallas.edu

###### Abstract

Existing subset selection methods for efficient learning predominantly employ discrete combinatorial and model-specific approaches which lack generalizability. For an unseen architecture, one cannot use the subset chosen for a different model. To tackle this problem, we propose SubSelNet, a trainable subset selection framework, that generalizes across architectures. Here, we first introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This naturally provides us two variants of SubSelNet. The first variant is transductive (called as Transductive-SubSelNet) which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called as Inductive-SubSelNet) which computes the subset using a trained subset selector, without any optimization. Our experiments show that our model outperforms several methods across several real datasets.

## 1 Introduction

In the last decade, neural networks have drastically enhanced the performance of state-of-the-art ML models. However, they often demand massive data to train, which renders them heavily contingent on the availability of high-performance computing machineries such as GPUs and RAM. Such resources entail heavy energy consumption, excessive CO\({}_{2}\) emission, and maintenance cost.

Driven by this challenge, a recent body of work focuses on suitably selecting a subset of instances so that the model can be trained quickly using lightweight computing infrastructure [4; 23; 51; 32; 54; 37; 18; 20; 19; 21]. However, these methods are not generalizable across architectures-- the subset selected by such a method is tailored to train only one specific architecture and thus need not be optimal for training another architecture. Hence, to select data subsets for a new architecture, they need to be run from scratch. However, these methods rely heavily on discrete combinatorial algorithms, which impose significant barriers against scaling them for multiple unseen architectures. Appendix C contains further details about related work.

### Our contributions

Responding to the above limitations, we develop SubSelNet, a trainable subset selection framework. Specifically, we make the following contributions.

**Novel framework on subset selection that generalizes across models.** SubSelNet is a subset selector that generalizes across architectures. Given a dataset, once SubSelNet is trained on a set of model architectures, it can quickly select a small optimal training subset for any unseen (test) architecture, without any explicit training of this test model. SubSelNet is a non-adaptive method since it learns to select the subset before the training starts for a new architecture, instead of adaptively selecting the subset during the training process. Our framework has several applications in the context of AutoML [35; 68; 30; 43; 61; 9; 2; 24; 22; 3]. For example, Network Architecture Search (NAS) can have a signficant speed-up when the architectures during selection can be trained on the subsets provided by our method, as compared to the entire dataset. In hyperparameter selection, such as the number and the widths of layers, learning rates or scheduler-specific hyperparameters, we can train each architecture on the corresponding data subset obtained from our method to quickly obtain the trained model for cross-validation.

**Design of neural pipeline to eschew model training for new architecture.** We initiate our investigation by writing down a combinatorial optimization problem instance that outputs a subset specifically for one given model architecture. Then, we gradually develop SubSelNet, by building upon this setup. The key blocker in scaling up a model-specific combinatorial subset selector across different architectures is the involvement of the model parameters as optimization variables along with the candidate data subset. We design the neural pipeline of SubSelNet to circumvent this blocker specifically. This neural pipeline consists of the following three components: **(1)** GNN-guided architecture encoder: This converts the architecture into an embedded vector space. **(2)** Neural model approximator: It approximates the predictions of a trained model for any given architecture. Thus, it provides the accuracy of a new (test) model per instance without explicitly training it. **(3)** Subset sampler: It uses the predictions from the model approximator and an instance to provide a selection score of the instance. Due to the architecture encoder and the neural approximator, we do not need to explicitly train a test model for selecting the subset since the model approximator directly provides the predictions the model will make.

**Transductive and Inductive SubSelNet.** Depending on the functioning of the subset sampler in the final component of our neural pipeline, we design two variants of our model.

_Transductive-SubSelNet:_ The first variant is transductive in nature. For each new architecture, we utilize the the model approximator's predictions for replacing the model training step in the original combinatorial subset selection problem. However, the candidate subset still remains involved as an optimization variable. Thus, we still solve a fresh optimization problem with respect to the selection score provided by the subset sampler every time we encounter a new architecture. However, the direct predictions from the model approximator allow us to skip explicit model training, making this strategy extremely fast in terms of memory and time.

_Inductive-SubSelNet:_ In contrast to Transductive-SubSelNet, the second variant does not require to solve any optimization problem and instead models the selection scores using a neural network. Consequently, it is extremely fast.

We compare our method against six state-of-the-art methods on five real world datasets, which show that SubSelNet provides the best trade-off between accuracy and inference time as well as accuracy and memory usage, among all the methods.

## 2 Preliminaries

**Setting.** We are given a set of training instances \(\{(_{i},y_{i})\}_{i D}\) where we use \(D\) to index the data. Here, \(_{i}^{d_{x}}\) denotes the features, and \(y_{i}\) denotes the labels. In our experiments, we consider \(\) as a set of categorical labels. However, our framework can also be used for continuous labels. We use \(m\) to denote a neural architecture and represent its parameterization as \(m_{}\). We also use \(\) to denote the set of neural architectures. Given an architecture \(m\), \(G_{m}=(V_{m},E_{m})\) provides the graph representation of \(m\), where the nodes \(u V_{m}\) represent the _operations_ and the \(e=(u_{m},v_{m})\) indicates an edge, where the output given by the operation represented by the node \(u_{m}\) is fed to one of the operands of the operation given by the node \(v_{m}\). Finally, we use \(H()\) to denote the entropy of a probability distribution and \((m_{}(),y)\) as the cross entropy loss hereafter.

### Combinatorial subset selection for efficient learning

Given a dataset \(\{(_{i},y_{i})\}_{i D}\) and a model architecture \(m\) with its neural parameterization \(m_{}\), the goal of a subset selection algorithm is to select a small subset of instances \(S\) with \(|S|=b<<|D|\) such that training \(m_{}\) on the subset \(S\) gives nearly the same accuracy as training on the entire dataset \(D\). Existing works [20; 47; 19] adopt different strategies to achieve this goal, but all of them aim to simultaneously optimize for the model parameters \(\) as well as the candidate subset \(S\). At the outset,we may consider the following optimization problem.

\[*{minimize}_{,S D:|S|=b}_{i S}(m_{}( _{i}),y_{i})-}(\{ _{i}\,|\,i S\}),\] (1)

where \(b\) is the budget, \(}(\{_{i}\,|\,i S\})\) measures the representativeness of \(S\) with respect to the whole dataset \(D\) and \(\) is a regularizing coefficient. One can use submodular functions [11; 17] like Facility Location, Graph Cut, or Log-Determinant to model \(}(\{_{i}\,|\,i S\})\). Here, \(\) trades off between training loss and diversity.

**Bottlenecks of the combinatorial optimization** (1).: For every new architecture \(m\), one needs to solve a fresh version of the optimization (1) problem from scratch to find \(S\). Therefore, this is not generalizable across architectures. Moreover, the involvement of both combinatorial and continuous optimization variables, prevents the underlying solver from scaling across multiple architectures.

We address these challenges by designing a neural surrogate of the objective (1), which would lead to the generalization of subset selection across different architectures.

## 3 Overview of SubSelNet

Here, we give an outline of our proposed model SubSelNet that leads to substituting the optimization (1) with its neural surrogate, which would enable us to compute the optimal subset \(S\) for an unseen model, once trained on a set of model architectures.

### Components

At the outset, SubSelNet consists of three key components: (i) the architecture encoder, (ii) the neural approximator of the trained model, and (iii) the subset sampler. Figure 1 illustrates our model.

**GNN-guided encoder for neural architectures.** Generalizing any task across the different architectures requires the architectures to be embedded in vector space. Since a neural architecture is essentially a graph between multiple operations, we use a graph neural network (GNN)  to achieve this goal. Given a model architecture \(m\), we first feed the underlying DAG \(G_{m}\) into a GNN (\(_{}\)) with parameters \(\), which outputs the node representations for \(G_{m}\), _i.e._, \(_{m}=\{_{u}\}_{u V_{m}}\).

**Approximator of the trained model \(m_{^{*}}\).** To tackle lack of generalizability of the optimization (1), we design a neural model approximator \(g_{}\) which approximates the predictions of any trained model for any given architecture \(m\). To this end, \(g_{}\) takes input as \(_{m}\) and the instance \(_{i}\) and compute \(g_{}(_{m},_{i}) m_{^{*}}( _{i})\). Here, \(^{*}\) is the set of learned parameters of the model \(m_{}\) on dataset \(D\).

**Subset sampler.** We design a subset sampler using a probabilistic model \(_{}()\). Given a budget \(b\), it sequentially draws instances \(S=\{s_{1},...,s_{b}\}\) from a softmax distribution of the logit vector \(^{|D|}\) where \((_{i},y_{i})\) indicates a score for the element \((_{i},y_{i})\). We would like to highlight that we use \(S\) as an ordered set of elements, selected in a sequential manner. However, such an order does not affect the trained model, which is inherently invariant of permutations of the training data; it only affects the choice of \(S\). Now, depending on how we compute \(\) during _test_, we have two variants of SubSelNet: Transductive-SubSelNet and Inductive-SubSelNet.

Figure 1: Illustration of SubSelNet. (a) Overview: Given a model architecture \(m\), SubSelNet takes its graph \(G_{m}\) as input to the architecture encoder \(_{}\) to compute the architecture embedding. This, together with \(\) is fed into the model approximator \(g_{}\) which predicts the output of the trained model \(m_{^{*}}()\). Then this is fed as input to the subset sampler \(\) to obtain the training subset \(S\). (b) Neural architecture of different components: \(_{}\) consists of recursive message passing layer. The model approximator \(g_{}\) performs a BFS ordering on the emebddings \(_{m}=\{_{u}\}\) and feeds them into a transformer. Subset sampler optimizes for \(\) either via direct optimization for \(\) (Transductive) or via a neural network \(_{}\) (Inductive).

_Transductive-SubSelNet:_ During test, since we have already trained the architecture encoder \(_{}\) and the model approximator \(g_{}\), we do not have to perform any training when we select a subset for an unseen architecture \(m^{}\), since the trained model can then be replaced with \(g_{}(_{m^{}},_{i})\). Thus, the key bottleneck of solving the combinatorial optimization (1)-- training the model simultaneously with exploring for \(S\)-- is ameliorated. Now, we can perform optimization over \(\), each time for a new architecture. However, since no model training is involved, such explicit optimization is fast enough and memory efficient. Due to explicit optimization every time for an unseen architecture, this approach is transductive in nature.

_Inductive-SubSelNet:_ Here, we introduce a neural network to approximate \(\), which is trained together with \(_{}\) and \(g_{}\). This allows us to directly select the subset \(S\) without explicitly optimizing for \(\), unlike Transductive-SubSelNet.

### Training and inference

**Training objective.** Using the approximation \(g_{}(_{m},_{i}) m_{^{*}}(_{i})\), we replace the combinatorial optimization problem in Eq. (1) with a continuous optimization problem, across different model architectures \(m\). To that goal, we define

\[(S;m;,g_{},_{})=_{i S}(g_{}( _{m},_{i}),y_{i})- H(_{}())_{m}=_{}(G_{m})\] (2)

and seek to solve the following problem:

\[_{,,}-14.226378pt_{m}-14.226378pt _{}}{}(S;m;,g_{}, _{})+_{i S} KL(g_{}(_{m},_{i}), m_{^{*}}(_{i}))\] (3)

Here, we use entropy on the subset sampler \(H(_{}())\) to model the diversity of samples in the selected subset. We call our neural pipeline, which consists of architecture encoder \(_{}\), the model approximator \(g_{}\), and the subset selector \(\), as SubSelNet. In the above, \(\) penalizes the difference between the output of the model approximator and the prediction made by the trained model, which allows us to generalize the training of different models \(m\) through the model \(g_{}(_{m},_{i})\).

## 4 Design of SubSelNet

**Bottlenecks of end-to-end training and proposed multi-stage approach.** End-to-end optimization of the above problem is difficult for the following reasons. (i) Our architecture representation \(_{m}\) only represents the architectures and thus should be independent of the parameters of the architecture \(\) and the instances \(\). End-to-end training can make them sensitive to these quantities. (ii) To enable the model approximator \(g_{}\) accurately fit the output of the trained model \(m_{}\), we need explicit training for \(\) with the target \(m_{}\).

In our multi-stage training method, we first train the architecture encoder \(_{}\), then the model approximator \(g_{}\) and then train our subset sampler \(_{}\) (resp. \(_{_{}}\)) for the transductive (inductive) model. In the following, we describe the design and training of these components in details.

### Design of architecture encoder using graph neural network

Architectures can be represented as directed acyclic graphs with forward message passing. During forward computation, at any layer for node \(v\), the output \(a(v)\) can be represented as \(a(v)=(_{u(v)}_{v}(a(u)))\) with the root output as the input. Here, Act is the activation function and \(_{}\) are operation on a node of the network. Given a GNN has a similar computation process, the permutation-equivariant node representations generated are good representations of the operations within the architecture. This allows further coupling with transformer-based architectures since they are universal approximators of permutation equivariant functions .

**Neural parameterization.** Given a model \(m\), we compute the representations \(_{m}=\{_{u}|u V_{m}\}\) by using a graph neural network \(_{}\) parameterized with \(\), following the proposal of Yan et al. . We first compute the feature vector \(_{u}\) for each node \(u V_{m}\) using the one-hot encoding of the associated _operation_ (_e.g._, \(\), \(\)) and then feeding it into a neural network to compute an initial node representation \(_{u}=_{}(_{u})\). Then, we use a message-passing network, which collects signals from the neighborhood of different nodes and recursively computes the node representations [59; 58; 12]. Given a maximum number of recursive layers \(K\) and the node \(u\), we compute the node embeddings \(_{m}=\{_{u}|u V_{m}\}\) by gathering information from the \(k<K\) hops using \(K\) recursive layers as follows.

\[_{(u,v)}[k]=_{}(_{u}[k],_{v}[k]),\;_ {u}[k+1]=_{}(_{u}[k],_{v(u)} _{(u,v)}[k])\] (4)

Here, \((u)\) is the set of neighbors of \(u\). EdgeEmb is injective mappings, as used in . Note that trainable parameters from EdgeEmb and Update are decoupled. They are represented as the set of parameters \(\). Finally, we obtain our node representations as \(_{u}=[_{u},..,_{u}[K-1]]\).

**Parameter estimation.** We perform unsupervised training of \(_{}\) using a variational graph autoencoder (VGAE). This ensures that the architecture representations \(_{m}\) remain insensitive to the model parameters. We build the encoder and decoder of our GVAE by following existing works on graph VAEs [59; 46]. Given a graph \(G_{m}\), the encoder \(q(_{m}\,|\,G_{m})\), which takes the node embeddings \(\{_{u}\}_{u V_{m}}\) and maps it into the latent space \(_{m}=\{_{u}\}_{u V_{m}}\). Specifically, we model the encoder \(q(_{m}\,|\,G_{m})\) as: \(q(_{m}\,|\,G_{m})=((_{u}),(_{u}))\). Here, both \(\) and \(\) are neural networks. Given a latent representation \(_{m}=\{_{u}\}_{u V_{m}}\), the decoder models a generative distribution of the graph \(G_{m}\) where the presence of an edge is modeled as Bernoulli distribution \((_{u}^{}_{v})\). Thus, we model the decoder as \(p(G_{m}\,|\,)=_{(u,v) E_{m}}(_{u}^{} _{v})_{(u,v) E_{m}}[1-(_{u}^{}_{v})]\). Here, \(\) is a parameterized sigmoid function. Finally, we estimate \(,,\), and \(\) by maximizing the evidence lower bound (ELBO): \(_{,,,}_{ q(\,|\,G_{m} )}[p(G_{m}\,|\,)]-(q(\,|\,G_{m})\|p_{}( ))\).

### Design of model approximator

**Neural parameterization.** Having computed the architecture representation \(_{m}=\{_{u}\,|\,u V_{m}\}\), we next design the model approximator, which leverages these embeddings to predict the output of the trained model \(m_{^{*}}(_{i})\). To this aim, we developed a model approximator \(g_{}\) parameterized by \(\) that takes \(_{m}\) and \(_{i}\) as input and attempts to predict \(m_{^{*}}(_{i})\), _i.e._, \(g_{}(_{m},_{i}) m_{^{*}}(_{i})\). It consists of three steps. In the first step, we generate an order on the nodes. Next, we feed the representations \(\{_{u}\}\) in this order into a self-attention-based transformer layer. Finally, we combine the output of the transformer and \(_{i}\) using a feedforward network to approximate the model output.

_Node ordering using BFS order._ We first sort the nodes using breadth-first-search (BFS) order \(\). Similar to You et al. , this sorting method produces a sequence of nodes and captures subtleties like skip connections in the network structure \(G_{m}\).

_Attention layer._ Given the BFS order \(\), we pass the representations \(_{m}=\{_{u}\,|\,u V_{m}\}\) in the sequence \(\) through a self-attention-based transformer network. Here, the \(\), \(\), and \(\) functions are realized by linear networks on \(_{}\). We compute an attention-weighted vector \(_{u}\) as:

\[_{u}=_{c}^{}_{v}a_{u,v}(_{v}) { with, } a_{u,v}=_{v}((_{u})^{} (_{v})/)\] (5)

Here \(k\) is the dimension of the latent space, and the softmax operation is over the node \(v\). Subsequently, for each node \(u\), we use a feedforward network, preceded and succeeded by layer normalization operations to obtain an intermediate representation \(_{u}\) for each node \(u\). We present additional details in Appendix D. Finally, we feed \(_{u}\) for the last node \(u\) in the sequence \(\), _i.e._, \(u=(|V_{m}|)\), along with the feature \(_{i}\) into a feedforward network parameterized by \(_{F}\) to model the prediction \(m_{^{*}}(_{i})\). Thus, the final output of \(g_{}(_{m},_{i})\) is

\[_{m,_{i}}=_{_{F}}(_{(|V_{m}|)}, _{i})\] (6)

Here, \(_{}\), parameters of Query, Key and Value and layer normalizations form \(\).

**Parameter estimation.** We train our model approximator \(g_{}\) by minimizing the KL-Divergence between the approximated prediction \(g_{}(_{m},_{i})\) and the ground truth prediction \(m_{^{*}}(_{i})\), where both these quantities are probabilities across different classes. The training problem is as follows:

\[*{minimize}_{}\;_{i D,m}(m_{ ^{*}}(_{i})||g_{}(_{m},_{i}))\] (7)

**Generalization across architectures but not instances.** Note that the goal of the model approximator is to predict the output on \(\) in the training set \(D_{}\) for unseen architecture \(m^{}\) so that using these predictions, our method can select the subset \(S\) from \(D_{}\) in a way that \(m^{}\) trained on \(S\) shows high accuracy on \(D_{}\). Since the underlying subset \(S\) has to be chosen from the training set \(D_{}\) for an arbitrary architecture \(m^{}\), it is enough for the model approximator to mimic the model output only on the training set \(D_{}\)-- it need not have to perform well in the test set \(D_{}\).

### Subset sampler and design of transductive and inductive SubSELNet

**Subset sampler.** We draw \(S\), an ordered set of elements, using \(\) as follows. Having chosen the first \(t\) instances \(S_{t}=\{s_{1},..s_{t}\}\) from \(D\) with \(S_{0}=\), it draws the \((t+1)\)-th element \((,y)\) from the remaining instances in \(D\) with a probability proportional to \(((,y))\) and then repeat it for \(b\) times. Thus, the probability of selecting the ordered set of elements \(S=\{s_{1},...,s_{b}\}\) is given by

\[_{}(S)=_{t=0}^{b}_{s_{t+1}},y_{s_{t+ 1}}))}{_{s_{} D S_{}}((_{s_{}}, y_{s_{}}))}\] (8)

The optimization (3) suggests that once \(_{}\) and \(g_{}\) are trained, we can use them to approximate the output of the trained model \(m_{^{*}}\) for an unseen architecture \(m^{}\) and use it to compute \(\). Thus, this already removes a significant overhead of model training and facilitates fast computation of \(\), and further leads us to develop Transductive-SubSELNet and Inductive-SubSELNet based on how we can compute \(\), as described at the end of Section 3.1.

**Transductive-SubSELNet.** The first variant of the model is transductive in terms of the computation of \(\). Once we train the architecture encoder and the model approximator, we compute \(\) by solving the optimization problem explicitly with respect to \(\) every time when we wish to select a data subset for a new architecture. Given trained model \(_{}\), \(g_{}\) and a new architecture \(m^{}\), we solve the optimization problem to find the subset sampler \(_{}\) during inference time for a new architecture \(m^{}\).

\[_{}_{S_{}()}(S;m^{};,g_{ {}},_{})\] (9)

Such an optimization still consumes time during inference. However, it is still significantly faster than the combinatorial methods [20; 19; 37; 47] thanks to sidestepping the explicit model training using a model approximator.

**Inductive-SubSELNet.** In contrast to the transductive model, the inductive model does not require explicit optimization of \(\) in the face of a new architecture. To that aim, we approximate \(\) using a neural network \(_{}\) which takes two signals as inputs-- the dataset \(D\) and the outputs of the model approximator for different instances \(\{g_{}(_{m},_{i})\,|\,i D\}\) and finally outputs a score for each instance \(_{}(_{i},y_{i})\). Here, the training of \(_{}\) follows from the optimization (3):

\[_{}_{m}_{S_{_{}}}(S; m;_{},g_{},_{})\] (10)

Such an inductive model can select an optimal distribution of the subset that should be used to efficiently train any model \(m_{}\), without explicitly training \(\) or searching for the underlying subset.

**Architecture of \(_{}\) for Inductive-SubSELNet.** We approximate \(\) using \(_{}\) using a neural network which takes three inputs - \((_{j},y_{j})\), the corresponding output of the model approximator, _i.e._, \(_{m,_{j}}=g_{}(_{}(G_{m}),_{j})\) from Eq. (6) and the node representation matrix \(_{m}\) and provides us a positive selection score \(_{}(_{m},_{j},y_{j},_{m,_{j}})\). In practice, \(_{}\) is a three-layer feed-forward network containing Leaky-ReLU activation functions for the first two layers and sigmoid activation at the last layer.

### Training and inference routines

**Training.** The training phase for both transductive and inductive variants, first utilizes the TrainPieline(Algorithm 1) routine to train the GNN (TrainGNN), re-order the embeddings based on BFS ordering (BFS), train the model approximator (TrainApprox), to obtain \(\). TrainTransductive(Algorithm 2) routine doesn't require any further training, while the TrainInductive(Algorithm 3) routine uses the TrainPi to train \(\) for computing \(\).

**Inference.** Given a new architecture \(m^{}\), our goal is to select a subset \(S\), with \(|S|=b\) which would facilitate efficient training of \(m^{}\). Given trained SubSELNet, we compute \(_{m^{}}=_{}(G_{m^{}})\), compute the model approximator output \(g_{}(_{m^{}})\). Using them we compute \(\) forTransductive-SubSelNet by explicitly solving the optimization problem stated in Eq. 9 and draw \(S_{}()\). For the inductive variant, we draw \(S_{_{}}()\) where \(\) is the learned value of \(\).

Given an unseen architecture \(m^{}\) and trained parameters of SubSelNet, _i.e._, \(\), \(\) and \(\), the InferTransductive(Algorithm 2) routine solves the optimization problem on \(\) explicitly to compute \(\), where \(()\) is defined in Eq. (2).

InferInductive (Algorithm 3) utilizes \(\), _i.e._, trained parameters from the subset sampler to compute \(_{}\). Then the subset \(S^{*}\) is drawn from \(\) or \(_{}\) and is used to train \(m^{}\) using TrainNewModel.

## 5 Experiments

In this section, we provide comprehensive evaluation of SubSelNet against several strong baselines on five real world datasets. In Appendix E, we present additional results. Our code is in https://github.com/structlearning/subselnet.

### Experimental setup

**Datasets.** We use FMNIST , CIFAR10 , CIFAR100 , Tiny-Imagenet-200  and Caltech-256  (Cal-256). Cal-256 has imbalanced class distribution; the rest are balanced. We transform an input image \(_{i}\) to a vector \(_{i}\) of dimension \(2048\) by feeding it to a pre-trained ResNet50 v1.5 model  and use the output from the penultimate layer as the image representation.

**Model architectures and baselines.** We use model architectures from NAS-Bench-101  in our experiments. We compare Transductive-SubSelNet and Inductive-SubSelNet against three non-adaptive subset selection methods - (i) Facility location [11; 17] where we maximize \(FL(S)=_{j D}_{i S}_{i}^{}_{j}\) to find \(S\), (ii) Pruning , and (iii) Selection-via-Proxy  and four adaptive subset selection methods - (iii) Glister , (iv) Grad-Match , (v) EL2N  and (vi) GraNd . The non-adaptive subset selectors select the subset before the training begins and thus, never access the rest of the training set again during the training iterations. On the other hand, the adaptive subset selectors refine the choice of subset during training iterations and thus they need to access the full training set at each training iteration. Appendix D contains additional details about the baselines and Appendix E contains experiments with more baselines.

**Evaluation protocol.** We split the model architectures \(\) into 70% training (\(_{}\)), 10% validation (\(_{}\)) and 20% test (\(_{}\)) folds. However, training model approximator requires supervision from the pre-trained models \(m_{^{*}}\). Pre-training large number of models can be expensive. Therefore, we limit the number of pre-trained models to a diverse set of size 250, that ensures efficient representation over low-parameter and high-parameter regimes, and using more than this showed no visible advantage. We show the parameter statistics in Appendix D. However, for the architecture encoder, we use the entire set \(_{}\) for GNN training. We split the dataset \(D\) into \(D_{}\), \(D_{}\) and \(D_{}\) in the similar 70:10:20 folds. We present \(_{}\), \(_{}\), \(D_{}\) and \(D_{}\) to our method and estimate \(,\) and \(\) (for Inductive-SubSelNet model). None of the baseline methods supports any generalizable learning protocol for different architectures and thus cannot leverage the training architectures during test. Given an architecture \(m^{}_{}\), we select the subset \(S\) from \(D_{}\) using our subset sampler (\(_{}\) for Transductive-SubSelNet or \(_{}\) for Inductive-SubSelNet). Similarly, all the non-adaptive subset selectors select \(S D_{}\) using their own algorithms. Once \(S\) is selected, we train the test models \(m^{}_{}\) on \(S\). We perform our experiments with different \(|S|=b(0.005|D|,0.9|D|)\) and compare the performance between different methods using three quantities: (1) Relative Accuracy Reduction (RAR) computed as the drop in test accuracy on training with a chosen subset as compared to training with the entire dataset, i.e, \((S,D)=_{}|}_{m^{} _{}}(1-(m^{}\,|\,S)/(m^{ }\,|\,D))\) where \((m^{}\,|\,X)\) denotes the test accuracy when \(m^{}\) is trained on the set \(X\). Lower RAR indicates better performance. (2) Computational efficiency, _i.e._, the speedup achieved with respect to training with full dataset. It is measured with respect to \(T_{f}/T\). Here, \(T_{f}\) is the time taken for training with full dataset; and, \(T\) is the time taken for the entire inference task, which is the average time for selectingsubsets across the test models \(m^{}_{}\) plus the average training time of these test models on the respective selected subsets. (3) Resource efficiency in terms of the amount of memory consumed during the entire inference task, described in item (2), which is measured as \(_{0}^{T}(t)\,dt\) where \((t)\) is amount of memory consumed at timestamp \(t\) in the unit of GB-min.

### Results

**Comparison with baselines.** Here, we compare different methods in terms of the trade-off between Relative accuracy reduction RAR (lower is better) and computational efficiency as well as RAR and resource efficiency. In Figures 2 and 3, we probe the variation between these quantities by varying the size of the selected subset \(|S|=b(0.005|D|,0.9|D|)\) for non-adaptive and adaptive baselines, respectively. We make the following observations. **(1)** Our methods trade-off between accuracy vs. computational efficiency as well as accuracy vs. resource efficiency more effectively than all the methods, including the adaptive methods which refine their choice of subset as the model training progresses. **(2)** In FMNIST, our method achieves 10% RAR at \(\)4.4 times the speed-up and using 77% lesser memory than EL2N, the best baseline (Table 1, tables for other datasets are in Appendix E). **(3)** There is no consistent winner across baselines. However, Glister and Grad-Match mostly remain among top three baselines, across different methods. In particular, they outperform others in Tiny-Imagenet and Cal-256, in high accuracy (low RAR) regime.

Figure 3: Trade-off between RAR (lower is better) and speedup (top row) and RAR and memory consumption in GB-min (bottom row) for the adaptive methods – Glister , Grad-Match , EL2N ; GraNd  on all five datasets - FMNIST, CIFAR10 CIFAR100, Tiny-ImageNet and Caltech-256. In all cases, we vary \(|S|=b(0.005|D|,0.9|D|)\).

Figure 2: Trade-off between RAR (lower is better) and speedup (top row) and RAR and memory consumption in GB-min (bottom row) for the non-adaptive methods – Facility location [11; 17], Pruning , Selection-via-Proxy  on all five datasets - FMNIST, CIFAR10 CIFAR100, Tiny-ImageNet and Caltech-256. In all cases, we vary \(|S|=b(0.005|D|,0.9|D|)\).

Hybrid-SubSelNet.In FMINIST, CIFAR10 and CIFAR100, we observe that Transductive-SubSelNet offers better traded off than Inductive-SubSelNet. Here, we design a hybrid version of our model, called as Hybrid-SubSelNet and evaluate it on a regime where the gap between transductive and inductive SubSelNet is significant. One of such regimes is the part of the trade-off plot in CIFAR100, where the speed up \(T_{f}/T 28.09\) (Figures 2 and 3). Here, given the budget of the subset \(b\), we first choose \(B>b\) instances using Inductive-SubSelNet and the final \(b\) instances by running the explicit optimization routines in Transductive-SubSelNet. Figure 4 shows the results for \(B=\{25K,30K,35K,45K,50K\}\). We observe that Hybrid-SubSelNet allow us to smoothly trade off between Inductive-SubSelNet and Transductive-SubSelNet, by tuning \(B\). It allows us to effectively use resource-constrained setup with limited GPU memory, wherein the larger subset \(B\) can be selected using Inductive-SubSelNet on a CPU, and the smaller _refined_ subset \(b\) can then be selected by solving transductive variant on GPU.

Ablation study.Here, we experiment with three candidates of model approximator \(g_{}\) ( Feedforward, LSTM and our proposed attention based approximator) with three different subset samplers \(\) (uncertainty based, loss based and our proposed subset sampler). Thus, we have nine different combinations of model approximator and subset selection strategies. In the uncertainty and loss based subset samplers, we take top-\(b\) instances based on the uncertainty and loss. We measure uncertainty using the entropy of the predicted distribution of the target classes. We compare the performance in terms of the test RAR of the test architectures. Moreover, we also evaluate the model approximator \(g_{}\) alone -- without the presence of the subset sampler -- using KL divergence between the gold model outputs and predicted model outputs on the training instances \(}||_{}|}_{i D_{},m _{}}(m_{^{*}}(_{i})||g_{}( {H}_{m},_{i}))\). Table 3 summarizes the results for 3%, 5% and 10% subsets for CIFAR10. We make the following observations: (1) The complete design of our method, i.e., Our model approximator (Transformer) + Our subset sampler (SubSelNet) performs best in terms of RAR. (2) Our neural-network for model approximator mimics the trained model output better than LSTM and Feedforward architectures.

Can model approximator substitute our subset selector pipeline?The task of the model approximator \(g_{}\) is to predict accuracy for unseen architecture. Then, a natural question is that is it possible to use the model approximator to directly predict accuracy of the unseen architecture \(m^{}\), instead of using such long pipeline to select subset \(S\) followed with training on \(S\). However, as discussed in the end of Section 4.2, the model approximator \(g_{}\) is required to generalize across unseen architectures but not the unseen instances, as its task is to help select the _training_ subset. Table 3 already showed that \(g_{}\) closely mimics the output of the trained model for the unseen architecture \(m^{}_{}\) and on training instances \(\) (KL div column). Here, we investigate the performance of \(g_{}\) on the test instances and test architectures.

Table 2 shows that the performance of \(g_{}\) on the test instances is significantly poorer than our method. This is intuitive as generalizing both the model space and the instance space is extremely challenging, and we also do not need it in general.

Using SubSelNet in AutoML.AutoML-related tasks can be significantly sped-up when we replace the entire dataset with a representative subset. Here, we apply SubSelNet to two AutoML applications: Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO).

_Neural Architecture Search:_ We apply our method on DARTS architecture space to search for an architecture using subsets. During this search process, at each iteration, the underlying network is traditionally trained on the entire dataset. In contrast, we train this underlying network on the subset returned by our method for this architecture. Following Na et al. , we report test misclassification

   &  &  \\ 
**RAR** & 10\% & 20\% & 10\% & 20\% \\  GLISTER & 5.64 & 7.85 & 116.36 & 98.51 \\ GradMatch & 4.17 & 5.24 & 243.75 & 136.40 \\ EL2N & 6.50 & 16.42 & 139.89 & 77.63 \\  Inductive & 28.64 & 69.24 & 22.73 & 8.24 \\ Transductive & 28.63 & 68.36 & 21.25 & 8.24 \\  

Table 1: Speedup and memory (GB-min) in reaching 10% and 20% RAR on FMNIST

  \(b\) (in \% ) & 90\% & 70\% & 20\% \\  RAR(our \(S\)) - RAR(\(g_{}\)) & -0.487 & -0.447 & -0.327 \\  

Table 2: RAR using \(g_{}\) on CIFAR10

Figure 4: Hybrid-SubSelNet

error of the architecture which is selected by the corresponding subset selector guided NAS methods, _i.e._, our method (transductive), random subset selection (averaged over 5 runs) and proxy-data . Table 4 shows that our method performs better than the baselines.

_Hyperparameter Optimization:_ Finding the best set of hyperparameters from their search space for a model is computationally intensive. We look at speeding-up the tuning process by searching the hyperparameters while training the model on a small representative subset \(S\) instead of \(D\). Following Killamsetty et al. , we consider optimizer and scheduler specific hyperparameters and report average test misclassification error across the models trained on optimal hyperparameter choice returned by our method (transductive), random subset selection (averaged over 5 runs) and AUTOMATA . Table 5 shows that we are outperforming the baselines in terms of accuracy-speedup tradeoff. Appendix D contains more details about the implementation.

**Amortization Analysis.** Figures 2 and 3 show that our method is substantially faster than the baselines during inference, once we have our neural pipeline trained. Such inference time speedup is the focus of many other applications, E.g., complex models like LLMs are difficult and computationally intensive, but their inference is fast for several queries once trained. However, we recognize that there is a computational overhead in training our model, arising due to the pre-training of the models \(m_{^{*}}\) used for supervision. Since the prior training is only a one-time overhead, the overall cost is amortized by querying multiple architectures for their subsets. We measure amortized cost \(T_{}/M_{}\) (time in seconds), where \(T_{}\) is the total time used from beginning of the pipeline to end of reporting final accuracy on the test architectures and \(M_{}\) is the total number of training and test architectures. Table 6, shows the results for 10% subset on the top baselines for CIFAR10, which shows that the training overhead of our method (transductive) quickly diminishes with number of test architectures.

## 6 Conclusion

In this work, we develop SubSelNet, a subset selection framework, which can be trained on a set of model architectures, to be able to predict a suitable training subset before training a model, for an unseen architecture. To do so, we first design a neural architecture encoder and model approximator, which predicts the output of a new candidate architecture without explicitly training it. We use that output to design transductive and inductive variants of our model.

**Limitations.** The SubSelNet pipeline offers quick inference-time subset selection but a key limitation of our method is that it entails a pre-training overhead, although its overhead vanishes as we query more architectures. Such expensive training can be reduced by efficient training methods . In future, it would be interesting to incorporate signals from different epochs with a sequence encoder to train a subset selector. Apart from this, our work does not assume the distribution shift of architectures from training to test. If the architectures vary significantly from training to test, then there is significant room for performance improvement.

  } &  &  \\   &  &  &  &  \\  Full & 2.48 & 1 & 2.48 & 1 \\  Random & 5.4 & 16.66 & 3.72 & **11.29** \\ Proxy  & 2.92 & 2.87 & 2.88 \\ Our & **2.82** & **2.76** & **2.68** \\  

Table 4: Test Error (%) on architecture given by NAS on CIFAR10

  \) and \(\)**}} &  &  \\   & \(b=0.03D|D|\) & \(b=0.05D|\) & \(b=0.1|D|\) & (does not depend on \(b\)) \\  Feedforward \((g_{})\)+ Uncertainty (\(\)) & 0.657 & 0.655 & 0.547 & \\ Feedforward \((g_{})\)+ Loss (\(\)) & 0.692 & 0.577 & 0.523 & 0.171 \\ Feedforward + Inductive (our) (\(\)) & 0.451 & 0.434 & 0.397 & \\  LSTM \((g_{})\)+ Uncertainty (\(\)) & 0.566 & 0.465 & 0.438 & \\ LSTM \((g_{})\)+ Loss (\(\)) & 0.705 & 0.541 & 0.455 & \\ LSTM \((g_{})\)+ Inductive (our) (\(\)) & 0.452 & 0.412 & 0.386 & \\  Attn. (our) \((g_{})\)+ Uncertainty (\(\)) & 0.794 & 0.746 & 0.679 & \\ Attn. (our) \((g_{})\)+ Loss (\(\)) & 0.781 & 0.527 & 0.407 & **0.089** \\ Attn. (our) \((g_{})\)+ Inductive (our) (\(\)) & **0.429** & **0.310** & **0.260** & \\  

Table 3: RAR and KL-divergence for different \(g_{}+\) on CIFAR10 for 3%, 5% and 10% subset sizes

  } &  &  \\   &  &  &  &  \\  Full & 2.48 & 1 & 2.48 & 1 \\  Random & 5.4 & 16.66 & 3.72 & **11.29** \\  AUTOMATA & 5.26 & 0.51 & 3.39 & 0.20 \\ Our & **4.11** & 16.11 & **2.70** & 10.96 \\  

Table 5: Test Error (%) (TE) and Speed-up (S/U) for the hyperparameters selected by HPO on CIFAR10

**Acknowledgements.** Abir De acknowledges the Google Research gift funding.