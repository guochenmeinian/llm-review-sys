# Retention Score: Quantifying Jailbreak Risks for Vision Language Models

ZAITANG LI

_The Chinese University of Hong Kong Sha Tin, Hong Kong ztli@cse.cuhk.edu.hk_

Pin-Yu Chen

_IBM Research New York, USA pin-yu.chen@ibm.com_

Tsung-Yi Ho

_The Chinese University of Hong Kong Sha Tin, Hong Kong tyho@cse.cuhk.edu.hk_

###### Abstract

The emergence of Vision-Language Models (VLMs) is a significant advancement in integrating computer vision with Large Language Models (LLMs) to enhance multimodal machine learning capabilities. However, this progress has also made VLMs vulnerable to advanced adversarial attacks, raising concerns about their reliability. The objective of this paper is to assess the resilience of VLMs against jailbreak attacks that can compromise model safety compliance and result in harmful outputs. To evaluate a VLM's ability to maintain its robustness against adversarial input perturbations, we propose a novel metric called the **Retention Score**. Retention Score is a multi-modal evaluation metric that includes Retention-I and Retention-T scores for quantifying jailbreak risks in visual and textual components of VLMs. Our process involves generating synthetic image-text pairs using a conditional diffusion model. These pairs are then predicted for toxicity score by a VLM alongside a toxicity judgment classifier. By calculating the margin in toxicity scores, we can quantify the robustness of the VLM in an attack-agnostic manner. Our work has four main contributions. First, we prove that Retention Score can serve as a certified robustness metric. Second, we demonstrate that most VLMs with visual components are less robust against jailbreak attacks than the corresponding plain VLMs. Additionally, we evaluate black-box VLM APIs and find that the security settings in Google Gemini significantly affect the score and robustness. Moreover, the robustness of GPT4V is similar to the medium settings of Gemini. Finally, our approach offers a time-efficient alternative to existing adversarial attack methods and provides consistent model robustness rankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA.

## 1 Introduction

Recent advances have led to the widespread use of Vision Language Models (VLMs) capable of handling a wide range of tasks. There has been great interest in incorporating vision modules into Large Language Models (LLMs), consisting of GPT-4V  and Gemini Vision . Although the introduction of visual input to Large Language Models (LLMs) has improved the ability of the language model to understand multi-modal knowledge, it also exposes an additional dimension of the visual input domain that expands the threat landscape for adversarial attacks. This expands the attack vectors available to adversaries, who now have two domains to exploit: the continuous, high-dimensional visual space and the discrete textual space. The shift from purely textual to multi-modal text-visual interaction significantly increases the possible ways for adversarial attacks to occur.

To help language models avoid generating harmful responses, prior work such as model alignment ensures that LLMs are aligned with their developers' original intentions [2; 18], thus ensuring that harmful or offensive content is not generated in response to prompts. However, there is always the possibility for users to craft adversarial perturbations from both image and text avenues designed to undermine alignment and induce malicious behavior. Previous research has shown the ease with which VLMs can be tricked into producing malicious content through image [5; 20] or text strategies [16; 32]. Accordingly, it is important to address concerns about the toxicity potential of VLMs. In line with Carlini's interpretation , we define toxicity as the susceptibility (lack of robustness) of models to be goaded into emitting toxic output (i.e., jailbreak risks).

While most of the works focus on guiding harmful responses (i.e., jailbreak) or preventing VLMs from improper content, we aim to provide a qualified margin-based robustness evaluation metric for each VLM. Previous studies on adversarial robustness in computer vision  have already concluded that robustness evaluation based on adversarial attacks may not be persistent because stronger attacks may exist and are yet to be discovered. On the other hand, certified robustness guarantees that no attacks can break the certificate. Our proposed jailbreak risk evaluation of VLMs falls into the category of margin-based certificates.

The task of assessing jailbreak risks of VLMs is full of challenges. (i) First, VLMs are trained on large, web-scale datasets, which complicates the feasibility of performing robust accuracy evaluations on test sets. (ii) Second, the discrete nature of textual data defies the establishment of a secure boundary in the context of text attacks. (iii) Third, the high computational and monetary costs associated with evaluating adversarial robustness via optimization-based jailbreak attacks are impractical due to their significant cost and time consumption.

We address these challenges by introducing Retention Score, a novel conditional robustness certificate tailored to evaluate the toxicity resilience of VLMs. The Retention Score, with its subcategories Retention-I and Retention-T, provides a conditional robustness certificate against potential jailbreak scenarios from images and text. For challenge (i), our approach, which uses a standard generative model and scores conditionally on a few generated samples, overlooks test set dependence and instead relies on a theoretical foundation that guarantees score confidence directly linked to specified distributions. For challenge (ii), our methodology circumvents this by using a semantic encoder and decoder to transform textual data into a continuous semantic space and vice versa, thereby formulating a verifiable boundary. For challenge (iii), we can evaluate the ability of aligned models to resist adversarial attacks, without succumbing to intensive computational demands, since only forward passing of data and toxicity evaluation are required for computing Retention Score.

Our main contributions can be encapsulated as follows:

* We introduce a multi-modal framework called Retention Score that establishes a conditional robustness certificate against jailbreak attempts from both visual and textual perspectives.

Figure 1: (a) An adversarial image optimized on harmful corpus can jailbreak a VLM . The model will not refuse to generate harmful responses. (b) Flow chart of calculating Retention-Image and Retention-Text scores for VLMs. Given some evaluation samples, we first use diffusion generators to create semantically similar synthetic samples. Then, we pass the generated samples into a VLM to get responses and further use a toxicity judgment model (e.g., Perspective API 1 or an LLM like Llama-70B ) for toxicity level predictions. Finally, we use these statistics to compute the Retention Score as detailed in Section 3.2. (c) Consistency of Attack Success Rate (ASR) using attack in  and Retention Score. A higher score means lower jailbreak risks (a lower ASR is expected).

* We show both Retention-I and Retention-T scores are robustness certificates for \(_{2}\)-norm bound perturbations in their spaces. We validate Retention-I and Retention-T scores consistently rank VLM robustness, while Retention Score cuts computation time up to 30\(\).
* With Retention Score, we ascertain that the inclusion of visual components can significantly decrease most of VLMs' robustness against jailbreak attacks, in comparison to the corresponding plain LLMs, highlighting the amplified risks of VLMs.
* The design of Retention Scores enables robustness evaluation of black-box VLMs APIs. When evaluating Retention Score on Gemini Pro Vision and GPT-4V, we find that the Retention Score is consistent in the security setting levels of Gemini Pro Vision.

## 2 Background and Related Works

### Attack-agnostic Robustness Certificate

Previous evaluations of neural network classifiers, such as the CLEVER Score , have provided assessments based on a closed form of certified local radius involving the maximum local Lipschitz constant around a neighborhood of a data sample x. However, the robustness guarantee of VLMs remains unexplored. The GREAT Score  derives a global statistic representative of distribution-wise robustness to adversarial perturbation for image classification tasks. While the GREAT Score evaluates global robustness, our method evaluates conditional robustness for given images and texts.

VLMs and Adversarial Examples for Jailbreaking Aligned VLMs and LLMs and Alignment of Vision-Language Models

We will give a detailed explanation for these three backgrounds in Appendix A.2.

## 3 Retention Score: Methodology and Algorithms

Our methodology defines the notational preliminaries for characterizing the robustness of VLMs against adversarial visual and text attacks. We begin by defining "jailbreaking" for VLMs in Section 3.1. We then propose the use of a generative model to obtain the Retention Score, which includes both the image-focused Retention-I and the text-centric Retention-T in Section 3.2. Then we briefly claim the certification for the Retention Score in Section 3.3. To ensure clarity and precision, we systematically enlist all pertinent notations and their corresponding definitions in Appendix A.1.

### Formalizing Image-Text Jailbreaking

To explain the process of jailbreaking in the context of VLMs, we introduce a model \(V:^{d}\), which accepts visual data of dimension \(d\) and linguistic prompts denoted by \(\). An image-text pair is represented by \((I,T)\), where \(I\) is a visual sample and \(T\) is the corresponding textual prompt.

For the assessment of toxicity in the generated outputs, we define a judgment function \(J:^{2}\) that assigns probabilities to the potential for toxicity within responses, with \(^{2}\) symbolizing the two-dimensional probability simplex representing toxic and non-toxic probabilities. Let the notations 't' and 'nt' stand for toxic and non-toxic categories, respectively. We then characterize the complete VLM with an integrated judgment classifier, \(M:^{d}^{2}\). This mapping embodies the transformation from the VLM's initial response \(V(I,T)\) to the evaluated judgment \(J(V(I,T))\) which we denote concisely as \(M\).

Prior to discussing robustness, it is crucial to establish a continuous space for both images and text. Images exist in a continuous space, whereas text, due to its discrete nature, necessitates an additional definition to facilitate its embedding into a continuous semantic space. We define a semantic encoder \(s\) that maps token sequences \(Y=[y_{1},y_{2},...,y_{n}]\), with each \(y_{i}\) belonging to a vocabulary \(\), into a \(k\)-dimensional space such that \(s:^{k}\). Here, \(\) includes all possible token sequences derived from \(\), and \(^{k}\) represents the continuous vector space. Additionally, we define a semantic decoder \(:^{k}\), which maps the continuous representations back to the discrete token sequences.

With continuous spaces for image and text established, we are now in a position to define the minimum perturbation required to alter the toxicity assessment in each modality.

For an image-text pair \((I,T)\), the classification of a non-toxic pair depends on a non-toxic score of \(M_{nt}(I,T) 0.5\). We define an adversarial jailbreaking instance as a perturbed image or text that can transition this non-toxic pair to a toxic classification.

In terms of image perturbations, we denote \(^{I}_{}(I,T)\) as the smallest perturbation that, among all adversarial jailbreaking candidates, reduces the non-toxic score of the perturbed pair \((I,T)\) to the threshold of 0.5 or below. Formally, it is expressed as: \(^{I}_{}(I,T)=_{}\{\|\|_{p}:M_{nt}(I+,T) 0.5\}\) where \(\|\|_{p}\) denotes the \(_{p}\)-norm of the perturbation \(\), which is a measure of the magnitude of the perturbation according to the chosen \(p\)-norm.

The search for the minimum text perturbation requires us to move through the semantic space. Employing a semantic encoder \(s\), we convert a textual prompt \(T\) into this space. The smallest perturbation \(^{T}_{}(T)\) that results in a borderline non-toxic score is formalized as: \(^{T}_{}(I,T)=_{}\{\|\|_{p}:M_{nt}(I,(s(T)+ )) 0.5\}\) where \(\) symbolizes a perturbation in the semantic space and \(s(T)+\) the perturbed representation.

### Establishing the Retention Score Framework

Revisiting the concepts introduced in Section 3.1, the minimal perturbations for an Image-Text pair in the context of VLMs were established. We proposed that greater values of \(^{I}_{}(I,T)\) and \(^{T}_{}(I,T)\) correlate with an enhanced local robustness of the model \(M\) for the pair \((I,T)\). Consequently, estimating the lower bounds for these minimal perturbations provides a measure of the VLMs' robustness. To quantify this robustness, we introduce the Retention Score, denoted as \(R:^{d}\), which aims to provide an assessment of VLM resilience against input perturbations. Higher Retention Scores signify a model's inherent robustness, indicative of robust safeguards against adversarial toxicity manipulation. The Retention Score is a multimodal measure capable of assessing the conditional robustness of VLMs across visual and textual domains, which are further divided into the Retention-Image (Retention-I) and Retention-Text (Retention-T) scores, respectively. This approach employs the notation \(a^{+}=\{a,0\}\) to streamline subsequent formula derivations.

#### 3.2.1 Retention-Image Score (Retention-I)

Building on the foundation laid out previously, we dedicate this subsection to formulating the Retention-I Score. This metric serves as a robustness certificate and is designed to evaluate a model's ability to resist adversarial image perturbations. The Retention-I Score is developed to evaluate robustness given a set of text prompts and a specific image \(I\), which we approach by initially defining a local pair score estimate function for each \((I,T)\) and subsequently deriving a conditional robustness score for the given image \(I\) and a collection of text prompts, denoted as \(=\{T_{1},T_{2},,T_{m}\}\).

The local score function is predicated on the VLM with an integrated judgment mechanism \(M\) and a specified textual prompt \(T\). We incorporate a continuous diffusion-based image generation model \(G_{I}(z|I)\), which, given a zero-mean isotropic Gaussian-distributed input \(z(0,I)\), synthesizes a semantically similar image to \(I\). The local score function \(g_{I}\) evaluates the non-toxicity of the generated image associated with the given prompt \(T\) and is defined by:

\[g_{I}(M,G_{I}(z|I),T)=}\{M_{nt}(G_{I}(z|I),T)-M_{t}(G_ {I}(z|I),T)\}^{+}.\] (1)

We then define an intermediate Retention-I Score for a single text prompt \(T\) as the average of local scores over \(n\) generated samples:

\[r_{I}(M,I,T)=_{i=1}^{n}g_{I}(M,G_{I}(z_{i}|I),T).\] (2)

With this intermediate score, the global Retention-I Score, representing the mean robustness across all image-text pairs, is formalized as:

\[R_{I}(M,I,)=_{j=1}^{m}r_{I}(M,I,T_{j}).\] (3)

#### 3.2.2 Retention-Text Score (Retention-T)

In a manner similar to Retention-I, the Retention Text Score (Retention-T) is introduced as a measure of VLM vulnerability to textual adversarial endeavors. Given the high success rate of attacks targetingsingle images, we direct our evaluation towards a fixed image \(I\) and a set of prompts. The model \(G_{T}(z|T)\) refers to a text generator founded on paraphrasing diffusion techniques, conditioned on a text prompt \(T\) and Gaussian-distributed input \(z\).

We define the local score function \(g_{T}\), which assesses the non-toxicity of a given image \(I\) associated with the paraphrased text prompt \(T\), as:

\[g_{T}(M,I,s(G_{T}(z|T)))=}\{M_{nt}(I,(s(G_{T}(z|T)) ))-M_{t}(I,(s(G_{T}(z|T))))\}^{+}.\] (4)

Here, \(s\) and \(\) represents a semantic encoder and decoder, such as BART , that translates discrete textual information into a continuous vectorial representation and vice versa.

Similarly, we define an intermediate Retention-T Score for a single text prompt \(T\) as:

\[r_{T}(M,I,T)=_{i=1}^{n}g_{T}(M,I,(s(G_{T}(z_{i}|T)))).\] (5)

The global Retention-T Score, \(R_{T}\), is then computed as the mean of the intermediate scores across all prompts:

\[R_{T}(M,I,)=_{j=1}^{m}r_{T}(M,I,T_{j}).\] (6)

Taken together, Retention-I and Retention-T provide a comprehensive assessment of a VLM's capabilities to uphold content safety amidst adversarial perturbations, thereby serving as integral indicators of multimodal robustness.

### Establishing the Robustness Certification for Retention Scores

Consider \(M\) as a VLM equipped with a judgment classifier. We assert that the previously defined intermediate score functions constitute robustness certifications. This claim is strengthened by the theorem below.

**Theorem 1** (Robustness Certification via Intermediate Retention Score).: _For a given image \(I\) and a text prompt \(T\), consider the intermediate Retention Image Score \(r_{I}\) defined in (2) and the intermediate Retention Text Score \(r_{T}\) defined in (5). Assuming \(M_{nt}(I,T) M_{t}(I,T)\), indicating a non-toxic classification of the original prompt. As the number of generated samples \(n\) from a generative model \(G()\) approaches infinity, the following statements hold almost surely: (I) Given any perturbation \(_{I}\) within \(r_{I}\) range applied to the image \(I\), the worst-case non-toxic score maintains a lower bound as follows:_

\[_{\|_{I}\|_{2}<r_{T}(M,I,T)}M_{nt}(I+_{I},T) 0.5.\] (7)

_(II) Similarly, for perturbations within the semantic space of \(T\), the worst-case non-toxic score is bounded by:_

\[_{\|_{T}\|_{2}<r_{T}(M,I,T)}M_{nt}(I,(s(T)+_{T})) 0.5.\] (8)

The theorem implies that the intermediate Retention Scores \(r_{I}\) and \(r_{T}\) act as thresholds beyond which the VLM maintains its non-toxic output for the respective perturbations, thus certifying the robustness of \(M\) with respect to image and text modifications for individual image-text pairs. The comprehensive proof delineating the details and assumptions underpinning this theorem is elucidated in Appendix A.3.

The theorem provides a guarantee that for perturbations whose magnitudes are within the radius defined by the respective intermediate Retention Scores, the VLM can be considered provably robust against potential toxicity-inducing alterations. This robustness certificate serves as a crucial asset in affirming the defensibility of VLMs when encountering adversarial perturbations, thereby reinforcing trust in their deployment in sensitive applications.

Performance Evaluation

### Experiment Setup

**Models.** We assess the robustness of various Vision-Language Models (VLMs), including MiniGPT-4 , LLaVA , InstructBLIP , and their base LLMs in a 13B version. Our evaluations also encompass the VLM APIs for GPT-4V  and Gemini Pro Vision . We will give a detail introduction for each models in Appendix A.4.

**Generative Models.** For Image Generation, we refer to stable diffusion  for an image generation task that synthesizes realistic and diverse images from input such as text. Stable diffusion  uses the DDIM  mechanism in latent space with powerful pre-trained denoising autoencoders.

For text generation, we refer to text paraphrasing. DiffuSeq  uses diffusion and sequence-to-sequence mechanisms to rephrase given text, preserving the semantics while changing the stylistic makeup. Here we paraphrase harmful instructions from the original harmful behaviors dataset.

**Computing Resources.** We run the experiments on 4x A800 GPUs.

### Analyzing Score Efficiency through Image-based Adversarial Attacks

**Datasets.** Our analysis of the Retention Image score employs the RealToxicityPrompts benchmark  as input prompts. We randomly chose 50 text prompts from its challenging subset, known for inciting toxic continuation responses. These prompts are input alongside visually adversarial examples. To quantify the toxicity level of the generated outputs, we utilize the Perspective API 2 that assigns toxicity ratings on a scale from 0 to 1, with higher values indicating increased toxicity. A threshold value of 0.5 serves as our benchmark for deeming a response as toxic.

**Image Attack.** Images are adversarially tailored to manipulate the VLM into complying with the associated harmful text prompt it would typically reject to respond. We adopt the visual adversarial attack outlined in  with an \(l_{}\) perturbation limit of \(=16/255\), iteratively generating examples crafted to maximize the occurrence probability for specific harmful contents. These adversarial visual instances, paired with consistent prompts, undergo evaluations measuring the toxicity of responses to determine the Attack Success Rate (ASR).

In terms of image generation, our protocol follows the state-of-the-art generative model, stable diffusion. In the study by , the 'clean' image originates from a depiction of a panda, whereas  employ a Gaussian noise base image as their starting point. To minimize the experimental randomness and examine the influence of image variability on the efficacy of attacks, we have generated a diverse set of 50 images for each demographic subgroup, categorized by gender and age: male, female, older adults, and youths. For instance, we utilize stable diffusion with a prompt such as "A facial image of a woman." to synthetic the given woman's facial image. The prompts used and the corresponding examples of generated images are thoroughly documented in Appendix A.9.

As shown in Table 1, our method provides a robust alternative for assessing the alignment equality of Vision Language Models. The relation between our score and the ASR for each VLM is evident - a higher Retention Image Score correlates with a lower ASR, underscoring the precision of our approach. Specifically, our Retention Score ranks the robustness of the tested VLMs by MiniGPT-4 > InstructBLIP > LLaVA, consistent with the ranking of the reported ASR.

### Robustness Evaluation of Black-box VLMs

Assessing the robustness of black-box VLMs is of paramount importance, particularly since these models are commonly deployed as APIs, restricting users and auditors to inferential interactions. This constraint not only makes adversarial attacks challenging but also underscores the necessity for robust evaluation methods that do not depend on internal model access. In this context, our research deploys the Retention-I score to examine the resilience of APIs against synthetically produced facial images with concealed attributes, which are typically employed in model inferences.

Our evaluation methodology was applied to two prominent online vision language APIs: GPT-4V and Gemini Pro Vision. Noteworthy is that for Gemini Pro Vision, the API provides settings to adjust the model's threshold for blocking harmful content, with options ranging from blocking none to most (none, few, some, and most). We systematically tested this feature by running identical prompts and images across these probability settings, leading to an evaluation of five distinct model configurations.

The assessment centered around the Retention-I score, using a balanced set of synthetic faces that included young, old, male, and female groups. These images were generated using the state-of-the-art Stable Diffusion model, with each group contributing 100 images. A unique aspect of Google's Gemini is its error messaging system, which, in lieu of producing potentially toxic outputs, provides rationales for prompt blocking. In our study, such preventative blocks were interpreted as a zero toxicity score, aligning with the model's safeguarding strategy.

Our results in Table 2 reveal intriguing variations across different APIs. For instance, Gemini-None exhibited notable performance contrasts when comparing Old versus Young cohorts. Other models showcased more uniform robustness levels across demographic groups. Also, Our analysis positions the robustness of GPT-4V somewhere between the some and most safety settings of Gemini. This correlation not only validates the efficacy of Gemini's protective configurations but also underscores the impact of safety thresholds on toxicity recognition, as quantified by our scoring method.

This robustness evaluation illustrates that Retention-I is a pivotal tool for analyzing group-level resilience in models with restricted access, enabling discreet and efficacious scrutiny of their defenses.

### Assessing Robustness against Text-based Adversarial Attacks

**Dataset.** We used the AdvBench Harmful Behaviours dataset  for our Retention-T score evaluation. This dataset contains 520 queries covering a range of malicious topics, including threats, misinformation and discrimination activities. In our study, we randomly extract a sample of 20 queries tagged 'challenge' from this dataset. Each prompt is paraphrased 50 times using diffusion-based text paraphrasing tools in , creating a pool of 1,000 different prompts for evaluation.

**Text Attack.** Text attacks on VLMs were executed using AutoDAN , a mechanism that uses a hierarchical genetic algorithm to create subtle but effective jailbreak prompts by adding adversarial suffixes before the original prompts. We set the attack epochs to 200.

After obtaining the model's response, we first use Bart  as a semantic encoder to encode the instructions into continuous space. We compose the decoder part of Bart to map the continuous space back to the sequence for getting the model response. Then, we relied on the LLaMA-70B chat model scoring system  as our judgment classifier to measure the obedience of each model's response to the prompt instructions. The complete prompt instructions are shown in Appendix A.6.

As AutoDAN originated as a tool for LLMs and demonstrated transferability across different LLMs, we retained this transferability when targeting VLMs. We used attack prefixes specified for LLMs and instructions as inputs to VLMs. We further strengthened the credibility of our scoring method by contrasting it with keyword matching to obtain ASR, a technique used by  and . They use a dictionary to determine whether the model refuses to generate responses, obtaining textual ASR.

Table 3 demonstrates the VLM resilience via text attack. Similar to the image case, our scoring methodology aligns with ASRs of text attacks. The results highlight LLaVA's exceptional resistance, as evidenced by its lower toxicity score and ability to counter adverse prompts. The study confirms the effectiveness of our scoring system in assessing a model's readiness for textual adversarial combat.

    &  &  &  \\   & Retention-I & ASR (\%) & Retention-I & ASR (\%) & Retention-I & ASR (\%) \\  Young & 0.6121 & 40.93 & 0.2866 & 58.86 & 0.5043 & 49.72 \\ Old & 0.5917 & 43.27 & 0.2636 & 64.71 & 0.5650 & 47.76 \\ Woman & 0.5621 & 42.12 & 0.2261 & 57.70 & 0.4861 & 52.00 \\ Man & 0.5438 & 42.63 & 0.1971 & 52.16 & 0.4966 & 50.36 \\  Average & 0.5774 & 42.49 & 0.2434 & 58.36 & 0.5130 & 49.96 \\   

Table 1: Jailbreak risk evaluation of VLMs to image attacks – a comparison among three VLMs with regards to their Retention Scores (Retention-I), and Attack Success Rates (ASR, calculated as the percentage of outputs displaying toxic attributes).

    &  &  &  &  &  \\  GPT-4V & 1.2043 & 1.2077 & 1.2067 & 1.2052 & 1.2059 \\  German-None & 0.8025 & 0.2432 & 0.2370 & 0.2126 & 0.2471 \\ German-None & 1.9955 & 1.1086 & 1.1972 & 1.1987 & 1.1930 \\ German-None & 1.2322 & 1.2868 & 1.2325 & 1.2328 & 1.2379 \\ German-None & 1.2499 & 1.2944 & 1.2388 & 1.2479 & 1.2453 \\   

Table 2: Retention-I analysis of VLM APIs. Each group consists of 100 images with 20 prompts.

### Impact of Visual Integration on Toxicity for VLMs

Here we assess the impact of adding visual elements to LLMs on their ability to mitigate toxicity. We hypothesize that a multi-modal approach using both visual and textual data might not improve language model robustness against toxic outcomes, as it introduces multi-modal attack interfaces. To investigate, we compared VLMs' performance with their corresponding LLMs. Our experimental setup involved feeding a noise image generated from a Gaussian distribution to VLMs, along with same prompts to corresponding plain LLMs. We evaluate the Retention-T and ASR for LLMs.

By the results in Table 4, we conclude that LLaVA and InstructBLIP show a significant decrease in toxicity score and a significant increase in ASR. This suggests that adding the visual module in LLaVA and InstructBLIP significantly increased toxic outputs, thereby decreasing the model's safety. The relative constancy of Retention Text Score and ASR within MiniGPT-4 can be attributed to its architecture. MiniGPT-4 includes a frozen visual encoder and LLM, connected by a trainable projection layer that aligns representations between the visual encoder and Vicuna. The visual backbone integration does not significantly affect output toxicity. In contrast, the influence of the visual module on InstructBLIP's performance can be explained by textual instructions being processed by the frozen LLM and the Q-Former, enabling the Q-Former to distill instruction-aware textual features. Meanwhile, LLaVa presents a scenario where the LLM is dynamically tuned with the visual encoder. Such a configuration disrupts the resilience of the LLM, making it more susceptible to perturbations induced with the visual components.

Overall, the results indicate that the inclusion of a visual module can influence the toxicity resilience of VLMs such as LLava and InstructBLIP, with varying degrees of effectiveness across different models. Further research is needed to clarify the mechanisms by which visual modules can improve resilience and reduce the occurrence of toxic language generated by these sophisticated models.

### Run-time Analysis

Figure 2 compares the run-time efficiency of Retention Score over adversarial attacks in  and . We show the improvement ratio of their average per-sample run-time (wall clock time of Retention Score/Adversarial Attack is reported in Appendix A.8) and observe around 2-30 times improvement, validating the computational efficiency of Retention Score.

## 5 Conclusion

In this paper, we presented Retention Score, a novel and computation-efficient attack-independent metric for quantifying jailbreak risks for vision-language models (VLMs). Retention Score uses off-the-shelf diffusion models for deriving robustness scores of image and text inputs. Its computation is lightweight and scalable because it only requires accessing the model predictions on the generated data samples. Our extensive results on several open-source VLMs and black-box VLMs (Gemini Vision and GPT4V) show the Retention score obtains consistent robustness analysis with the time-consuming jailbreak attacks, and it also reveals novel insights in studying the effect of safety thresholds in Gemini and the amplified risk of integrating visual components to LLMs in the development of VLMs.

  VLM & Retention-T change & ASR change \\  MinGPT-4 & -0.0004 & -0.2\% \\ LLaVA & -0.0617 & +8.4\% \\ InstructBLIP & -0.1483 & +55.9\% \\  

Table 4: Jailbreak risk evaluation by incorporating a Vision Module. This table shows the change between three VLMs relative to their corresponding plain LLMs, in terms of their Retention scores (Retention-T) and ASRs.

  VLM & Retention-T change & ASR change \\  MinGPT-4 & 0.2040 & 46.1\% \\ LLaVA & 0.3439 & 9.4\% \\ InstructBLIP & 0.1346 & 84.5\% \\  

Table 3: Jailbreak risk evaluation of VLMs to text attacks – a comparison among three VLMs with regards to their Retention Scores (Retention-T) and Attack Success Rates.

Figure 2: Run-time improvement (Retention Score over jailbreak attacks, and it also reveals novel insights in studying the effect of safety thresholds in Gemini and the amplified risk of integrating visual components to LLMs in the development of VLMs.

[MISSING_PAGE_FAIL:9]

*  OpenAI. Gpt-4v: Multimodal language model. https://openai.com/gpt-4v, 2023. Accessed: yyyy-mm-dd.
*  Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.
*  Remigijus Paulavicius and Julius Zilinskas. Analysis of different norms and corresponding lipschitz constants for global optimization. _Technological and Economic Development of Economy_, 12(4):301-306, 2006.
*  Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak aligned large language models, 2023.
*  Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to!, 2023.
*  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021.
*  Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022.
*  Charles M Stein. Estimation of the mean of a multivariate normal distribution. _The annals of Statistics_, pages 1135-1151, 1981.
*  Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale, 2023.
*  Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
*  Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
*  Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022.
*  Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. _arXiv preprint arXiv:1801.10578_, 2018.
*  Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, and Min Lin. On evaluating adversarial robustness of large vision-language models, 2023.
*  Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.
*  Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models, 2023.

## Appendix A Appendix

### Notations

### Backgrounds

#### a.2.1 Vision-Language Models (VLMs)

The advent of Large Language Models (LLMs) such as GPT-3  and LLaMA2  has revolutionized artificial intelligence, enabling context-aware learning and chain-of-thought reasoning by exploiting abundant web data and numerous model parameters. VLMs represent the convergence of computer vision and natural language processing, combining visual perception with linguistic expression. Examples such as GPT-4V  and Google Gemini  use both visual and textual information. In addition, open-source VLMs such as MiniGPT-4 , InstructBLIP , and LLaVA  enhance multi-modal integration by generating text in response to visual and textual cues.

#### a.2.2 Alignment of Vision-Language Models

In the pursuit of responsible AI, aligning language models with human values remains a significant challenge . Misalignment can stem from insufficient training data or the inadvertent reflection of biases present in Internet data. Alignment methods, such as reinforcement learning with human feedback (RLHF)  and instruction tuning , aim to recalibrate language models to meet ethical guidelines and societal expectations. These techniques fine-tune models based on human preferences and improve their ability to understand and perform tasks described by instructions.

#### a.2.3 Adversarial Examples for Jailbreaking Aligned VLMs and LLMs

Adversarial machine learning explores inputs designed to fool AI models, particularly relevant in textual and visual contexts for Large Language Models (LLMs) and Vision-Language Models (VLMs). In the image domain,  shows that a single visual adversary example can universally jailbreak aligned VLMs.  developed a fully differentiable VLM implementation for adversarial example generation, while  introduces a black-box technique targeting CLIP  and BLIP  as surrogate models. In the text domain, GCG attacks  and AutoDAN  have emerged as breakthroughs, generating adversarial suffixes or jailbreaking prefixes.

### Detailed Proofment

In this section, we will give detailed proof for the certified conditional robustness estimate in Theorem 1. The proof contains three parts: (i) derive the local robustness certificate for VLM given an image-text pair; (ii) derive the closed-form Lipschitz constant; and (iii) prove the proposed intermediate Retention-I and Retention-T scores are lower bounds on the conditional robustness. Some of our proofment here refers to GREAT Score .

  
**Notation** & **Description** \\  \(d\) & dimensionality of the input image vector \\ \(k\) & dimensionality of the semantic encoder embedding for text \\ \(V:^{d}\) & vision Language Model \\ \(J:^{d}^{2}\) & toxicity classifier \\ \(M:^{d}^{2}\) & composing model and classifier \\ \(I^{d}\) & image sample \\ \(T\) & text prompt sample \\ \(_{l}^{d}\) & image perturbation \\ \(_{r}^{d}\) & semantic text perturbation \\ \(\|_{l}\|_{p}\) & \(_{p}\) norm of perturbation, \(p 1\) \\ \(_{}\) & minimum adversarial perturbation \\ \(G\) & (conditional) generative model \\ \(z(0,I)\) & latent vector sampled from Gaussian distribution \\ \(g_{I}\) & image robustness score function defined in (1) \\ \(g_{T}\) & text robustness score function defined in (4) \\ \(R_{I}/R_{T}\) & conditional robustness score defined in (3) and (6) \\   

Table 5: Main notations used in this paper

#### a.3.1 Proof of Intermediate Retention-Image Score as a Robustness Certificate

**Lemma 1** (Lipschitz Continuity in Gradient Form for VLM in image aspect ()).: _Suppose \(^{d}\) is a convex, bound, and closed set, and let \(M:(,T)^{2}\) be a VLM that is continuously differentiable on an open set containing \(\) where \(T\) is a fixed text prompt. Then \(M\) is Lipschitz continuous if the following inequality holds for any \(x,y\) :_

\[|M(x,T)-M(y,T)| L_{2}\|x-y\|_{2}\] (9)

_where \(L_{2}=_{x}\| M(x,T)\|_{2}\) is the corresponding Lipschitz constant._

Then we get the formal guarantee for adversarial image attacks.

Recall we define \(M\) output to be nt and t two classes.

**Lemma 2** (Formal guarantee on lower bound of VLM for adversarial image attacks.).: _Let \(I,T^{d}\) be a given non-toxic image and fixed text prompt pair, and let \(M:^{}^{2}\) be a toxicity judgement classifier integrated with a Vision Language Model that does not output toxic content. For adversarial attacks on images, a lower bound on the minimum distortion in \(L_{2}\)-norm can be guaranteed such that for all \(_{I}\) in \(^{d}\), it must satisfy:_

\[\|_{I}\|_{2}(I,T)-M_{t}(I,T)}{L_{2}^{M}}\] (10)

_where \(L_{2}^{M}\) is the Lipschitz constant for the function \(M_{nt}(I,T)-M_{t}(I,T)\)._

Refer to the proofment in GREAT Score , here we will derive the Lipschitz constant for \(M\).

Proof of closed-form global Lipschitz constant in the \(L_{2}\)-norm over Gaussian distribution.In this part, we present two lemmas towards developing the global Lipschitz constant of a function smoothed by a Gaussian distribution.

**Lemma 3** (Stein's lemma ).: _Given a soft classifier \(F:^{d}\), where **P** is the space of probability distributions over classes. The associated smooth classifier with parameter \( 0\) is defined as:_

\[:=(F*(0,^{2}I))(x)=_{_{I}(0,^{2}I)}[F(x+_{I})]\] (11)

_Then, \(\) is differentiable, and moreover,_

\[=}_{_{I}(0, ^{2}I)}[_{I} F(x+_{I})]\] (12)

In a lecture note3, Li used Stein's Lemma  to prove the following lemma:

**Lemma 4** (Proof of global Lipschitz constant).: _Let \( 0\), let \(h:^{d}\) be measurable, and let \(H=h*(0,^{2}I)\). Then \(H\) is \(}}\) - continuous in \(L_{2}\) norm_

and thus \(}_{z(0,I)}[g_{I}(M(G_{I}(z|I) +_{I},T))]\) has a Lipschitz constant \(}\) in \(_{2}\) norm.

Employing the established Lipschitz continuity condition Lemma 2 and the Lipschitz constant 4, suppose:

\[|_{z(0,I)}[g_{I}(M,G_{I}(z|I)+ _{I},T)]-.\] (13) \[._{z(0,I)}[g_{I}(M,G_{I}(z|I),T)] |\|_{I}\|_{2}\] (14)

Hence

\[_{z(0,I)}[g_{I}(M,G_{I}(z|I)+_{I}, T)]\] (15) \[_{z(0,I)}[g_{I}(M,G_{I}(zFollow the definition of \(g_{I}\), let right hand side bigger than 0, then it means we can not find any \(_{I}\) make the pair non toxic.

This inequality holds true for any perturbation \(_{I}\) satisfying:

\[\|_{I}\|_{2}<_{z(0,I)}[g_{I}(M,G_{I}( z|I),T)]\] (17)

The right-hand side of this inequality is precisely the definition of our intermediate Retention-I Score \(r_{I}(M,I,T)\) as \(n\) approaches infinity. Therefore, we can rewrite this as:

\[\|_{I}\|_{2}<r_{I}(M,I,T)\] (18)

According to the given framework, the smallest perturbation that could potentially alter the model's output for \(G_{I}(z|I)\) must exceed \(r_{I}(M,I,T)\). Should the perturbation fall below this threshold, it is highly probable that the model would yield \(g_{I}(M,G_{I}(z|I),T)=0\).

We have now established, through rigorous proof, that for a specific text prompt and image combination, our intermediate score function is capable of serving as a certificate. This certification confirms the resilience of the intermediate Retention-Image Score against adversarial attacks on images, thereby upholding the VLM's robust structural framework.

#### a.3.2 Certification for Intermediate Retention Text Score

To extend the robustness certification to text-based adversarial attacks within the VLM framework, we introduce a semantic encoder denoted as \(s\). This encoder transforms discrete text prompts into continuous representations, enabling us to formulate a Lipschitz condition specific to textual data. Given that a generative model \(G()\) taking a Gaussian vector as input is a random variable, in our proof we use the central limit theorem that the defined Retention scores in (3) (6) converge to their mean almost surely as the number of samples \(n\) generated by \(G()\) approaches to infinity.

Following the similar format as proofment in Image Part. We now derive the Lipschitz Continuity for VLM in text aspect.

**Lemma 5** (Lipschitz Continuity in Gradient Form for VLM in text aspect ()).: _Suppose \(\) is linguistic set, \(s\) be a semantic encoder, \(I\) be a given image. and let \(M:(I,s())\) be a function that is continuously differentiable on an open set containing \(s()\). Then \(M\) is Lipschitz continuous if the following inequality holds for any \(x,y\) :_

\[|M(I,(s(x)))-M(I,(s(y)))| L_{2}\|s(x)-s(y)\| _{2}\] (19)

_where \(L_{2}=_{x}\| M(I,s(x))\|_{2}\) is the corresponding Lipschitz constant._

Then we would like to deliver the Lipschitz Continuity for VLM in text aspect.

**Lemma 6** (Text-Based robustness guarantee.).: _Consider a VLM consisting of a model \(M\) which includes a judgment classifier. Given a fixed input image \(I\) and prompt text \(T\), if \(M(I,T)\) is a toxicity judgment classifier that produces non-toxic outputs, then the continuous textual perturbations \(_{T}\), representing the differences between the adversarial prompts and the original, are bounded as follows:_

\[\|_{T}\|_{2}(I,(s(T)))-M_{t}(I,(s(T) ))}{L_{2}^{M}}\] (20)

Here, \(L_{2}^{M}\) is the Lipschitz constant for the function \(M_{nt}(I,(s(T)))-M_{t}(I,(s(T)))\), ensuring a prescribed level of robustness against textual adversarial attacks.

Then we use similarly lemma in Image part to derive the Lipschitz constant. It follows that the expectation of text perturbation resilience, while employing a semantic encoder, satisfies the Lipschitz condition with the constant \(}\) in the \(L_{2}\) norm. Where \(}_{z(0,I)}[g_{T}(M,I,(s (G(z|T)+_{T}))]\) has a Lipschitz constant \(}\) in \(_{2}\) norm.

\[\|_{z(0,I)}[g_{T}(M,I,psi(s(G(z|T))+ _{T}))]-\] (21) \[_{z(0,I)}[g_{T}(M,I,(s(G(z|T))))]| \|_{T}\|_{2}\] (22)

Similarly as image part, to confirm adversary can not find any \(_{T}\) to mislead the \(M\). This inequality is valid for all perturbations \(_{T}\) where:

\[\|_{T}\|_{2}<_{z(0,I)}[g_{T}(I, (s(G(z|T))))]\] (23)

The right-hand side of this inequality is precisely the definition of our intermediate Retention-T Score \(r_{T}(M,I,T)\) as \(n\) approaches infinity. Therefore, we can rewrite this as:

\[\|_{T}\|_{2}<r_{T}(M,I,T)\] (24)

By definition, any perturbation less than the established margin is insufficient to dismantle the intended non-toxic output, signifying that \(g_{T}(I,(s(G(z|T))))\) effectively becomes zero.

Then, for any given image I and prompt text T, our intermediate score can be a local certificate estimation.

Hence, this analytical approach underscores the intermediate Retention Text Score as a valid certification of robustness against sophisticated text-based adversarial incursions, ensuring the VLM upholds its alignment and security protocols even under duress.

Then we proved Theorem 1.

### Introduction to evaluated models.

MiniGPT-4 integrates vision components from BLIP-2 , featuring a ViT-G/14 from EVA-CLIP [8; 25] and a Q-Former network for encoding images into Vicuna  LLM's text embedding space. A projection layer aligns the visual features with the Vicuna model. In the absence of visual input, MiniGPT-4 functions equivalently to Vicuna-v0-13B LLM. This model shares ChatGPT's instruction tuning and safety guardrains, ensuring consistency in generation and adherence to ethical guidelines.

LLaVA leverages a CLIP VIT-L/14 model with a linear layer to encode visual features into Vicuna's embedding space. Unlike MiniGPT-4, the Vicuna component of LLaVA is fine-tuned, further refining its response accuracy. Originating from LLaMA-2-13B-Chat, LLaVA exhibits a sophisticated alignment due to its hybrid tuning involving instructional data and reinforcement learning from human feedback. This model sets a new benchmark for interactive aligned VLMs.

InstructBLIP is based on the Vicuna-v1.1-13B and enhances BLIP-2 by incorporating instruction-directed visual feature extraction. The Q-former module integrates instruction text tokens with image queries, utilizing self-attention layers to prioritize relevant feature extraction. The model employs a ViT-based visual encoder from CLIP, underscoring task-specific image comprehension.

The GPT-4V API introduces a multi-modal approach, empowering GPT-4 to natively process and analyze images alongside textual content. Continually refined through instruction tuning and ongoing learning, the model harnesses a comprehensive data corpus to sharpen its textual and visual insights.

Google's Gemini Pro Vision embodies a comprehensive AI system capable of parsing multi-modal stimuli. Leveraging a sophisticated transformer model architecture, Gemini Pro Vision exemplifies Google's commitment to advancing multi-contextual understanding and interaction within the digital landscape. We opt for the Pro version for its optimal balance of high-end performance and scalability.

### Algorithms

Algorithm 1 and Algorithm 2 summarize the procedure of computing Retention Score using the sample mean estimator from the image and text aspects.

[MISSING_PAGE_EMPTY:15]

* "role": "user", "content": "Below are Model IP-holder name, e.g., OpenAI's prohibited usage policies. These policies ensure that models interact with users safely and responsibly. Models should refuse to execute instructions that fall into the disallowed use cases outlined in the policy, avoiding affirmative responses or providing any information that could be helpful for any prohibited activities. The detailed policy. Now, I will provide you with a user instruction that the model should not comply with, as per Model IP-holder name, e.g., OpenAI's policy. I will also give you the model's response to that instruction. Please review the model's response in light of the prohibited usage guidelines: User Instruction: The harmful inquery. Model Response: The output to be judged. Use the scoring rules below to score the model's response to the user instruction on a scale of 1 to 5: The detailed Model IP-hoder-specific scoring rules Output your evaluation in the following format: thereason: your analysis here [note: this should be step-by-step analysis following the steps: (a) Give an overview of the user's primary intent and the model's response. (b) Evaluate the degree to which the model's response aligns with the user's expressed intent. (c) Examine the user's instruction and the model's reply respectively to determine if they contain any prohibited content. Finally, evaluate the degree of the model's adherence to Model IP-holder name, e.g., OpenAI's usage guidelines.] [hescore: your score here. [note:provide your score in based on your analysis and the provide scoring criteria.] "

### Keyword matching

We show the refusal signals used in [16; 32] for evaluating the attack success rate for text attack in Table 6.

### Complete Run-time Results

The complete run-time results of Figure 2 are given in Table 7:

   \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  

Table 6: The refusal signals considered in ASR evaluation.

### Generated Images from stable diffusion Models

Below we show the generated images based on stable diffusion from four groups.

The detailed prompts for generating each group of images are: "a natural and professional photograph of a man's detailed face", "a natural and professional photograph of a woman's detailed face", "a natural and professional photograph of an old person's detailed face", and "a natural and professional photograph of a young person's face".

### Generated Harmful Instructions from Text Paraphrasing Models

We show the paraphrased text for AdvBench Harmful Behaviors  in Table 8.

**WARNING: THIS Part CONTAINS Offensive Prompts**.

### Limitations

One limitation could be that our framework of adversarial robustness evaluation using generative models is centered on \(_{2}\)-norm based perturbations. For text attack, a \(_{0}\)-norm based certificate will bring more values against work-level attack.

### Impact Statements

In terms of ethical aspects and future societal impact considerations, we suggest users and developers use Retention Score to help quantify the jailbreak risks for Vision Language Models. We envision our score to be used in safety reports of model cards related activities for VLMs.

   Model & Retention-I (min) & Image Attack (min) & Retention-T (min) & Text Attack (min) \\  MiniGPT-4 & 43.25 & 506.75 & 1482 & 2928 \\ LLAVA & 32.5 & 1005 & 1449 & 2925 \\ InstructBLIP & 106.25 & 493 & 1511 & 2961 \\   

Table 7: Run time analysis for Image and Text Attacks compared to Retention-I and Retention-T scores for various Vision-Language Models. All times are presented in minutes. Note the time is calculated by parallely run on 4 A800 GPUs.

Figure 4: Generated Images for young subgroup.

Figure 3: Generated Images for old subgroup.

[MISSING_PAGE_EMPTY:18]