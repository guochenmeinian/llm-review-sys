# Differential Privacy in Scalable General Kernel Learning via \(K\)-means Nystrom Random Features

Bonwoo Lee &Jeongyoun Ahn1

KAIST

Daejeon, 34141 South Korea

righthim@kaist.ac.kr, jyahn@kaist.ac.kr, parkcw2021@kaist.ac.kr

&Cheolwoo Park1

###### Abstract

As the volume of data invested in statistical learning increases and concerns regarding privacy grow, the privacy leakage issue has drawn significant attention. Differential privacy has emerged as a widely accepted concept capable of mitigating privacy concerns, and numerous differentially private (DP) versions of machine learning algorithms have been developed. However, existing works on DP kernel learning algorithms have exhibited practical limitations, including scalability, restricted choice of kernels, or dependence on test data availability. We propose DP scalable kernel empirical risk minimization (ERM) algorithms and a DP kernel mean embedding (KME) release algorithm suitable for general kernels. Our approaches address the shortcomings of previous algorithms by employing Nystrom methods, classical techniques in non-private scalable kernel learning. These methods provide data-dependent low-rank approximations of the kernel matrix for general kernels in a DP manner. We present excess empirical risk bounds and computational complexities for the scalable kernel DP ERM, KME algorithms, contrasting them with established methodologies. Furthermore, we develop a private data-generating algorithm capable of learning diverse kernel models. We conduct experiments to demonstrate the performance of our algorithms, comparing them with existing methods to highlight their superiority.

## 1 Introduction

As data collection and access continue to expand, protecting privacy has emerged as a crucial concern. _Differential privacy_ (DP), introduced by Dwork et al. (2006), stands as the current gold standard in data privacy. It provides a rigorous framework for privacy in statistical procedures, wherein noise is added proportional to the maximum deviation induced by the change of a single individual in the dataset. The field has seen significant advances in differentially private (DP) algorithms for machine learning. Notably, DP empirical risk minimization (ERM) has become a key area of research, attracting extensive attention (Chaudhuri et al., 2011; Kifer et al., 2012; Bassily et al., 2019; Wang et al., 2019; Feldman et al., 2020). These efforts focus on developing privacy-preserving statistical models with high generalization capabilities across various ERM problems, including regularized regression, logistic regression, support vector machines, and some other ERM problems equipped with non-convex or non-smooth loss functions. However, a vast majority of the current DP ERM algorithms primarily address linear ERM, suitable for datasets with linear structures.

Kernel methods hold significant importance in machine learning due to their ability to capture intricate, non-linear structures. Kernel-based learning is ubiquitous in both supervised and unsupervised settings as well as in hypothesis testing problems. For an extensive review on the subject, see Muandet et al. (2017). This work focuses on the supervised kernel learning problem via ERM as well as kernel mean embedding (KME) for probability distributions. Developing a DP framework for _general_ kernelsposes a significant hurdle due to their reliance on the local attributes of the data. For instance, not every kernel is amenable to a well-behaved DP solution. As a rather extreme case, the Kronecker delta kernel, \(k(x,y)=(x=y)\), yields a meaningful solution only when the test data exactly matches one of the training instances. Consequently, the solution becomes highly sensitive to the data changes, exhibiting substantial deviations with even a single data alteration.

As a result, there has been relatively little DP research regarding kernel ERM, especially compared to linear counterparts. Chaudhuri et al. (2011) suggested transforming kernel ERM into a linear form via random features, which facilitates scaling-down operations. However, their method is limited to translation-invariant kernels, excluding a wide variety of other kernels, such as polynomial kernels or the pyramid match kernel from the computer vision domain (Grauman and Darrell, 2007), the chi-squared kernel in hand gesture recognition (Abadi et al., 2015), Histogram intersection kernel in image classification (Maji et al., 2013), Bayesian kernel in protein prediction (Alashwal et al., 2009), and the diffusion kernel in manifold learning (Lafferty and Lebanon, 2005). Hall et al. (2013) proposed perturbing the output function of kernel ERM. Although their work can be applied to general kernels, it lacks scalability in practice (e.g., \(O(n^{3})\) in kernel ridge regression). Additionally, it requires access to the training data for every prediction, posing a potential threat of privacy leakage. The DP kernel ERM proposed by Jain and Thakurta (2013) can work with general kernels; however, its utility is guaranteed only when test data are accessible.

KME is a pivotal tool in diverse applications such as two-sample testing and synthetic data generation (Harder et al., 2021). It enables the comparison of probability distributions via metric in Reproducing Kernel Hilbert Space (RKHS), by embedding these distributions into elements of the RKHS. Additionally, Balog et al. (2018) found utility of KME for the private release of data. The embedding itself acts as a concise representation of the distributional information of the data. However, their methods appear to suffer in high-dimensional settings. Moreover, it is important to note that the DP functional release algorithm suggested by Hall et al. (2013) cannot be directly applied to DP KME. This limitation arises because the output is not a member of the RKHS, which is a crucial distinction since statistical methods related to KME heavily depend on operations within the RKHS.

Our study is motivated by the frequent oversight in prior research of the practical challenges in real-world implementations, such as scalability, private implementation without access to test data, accurate DP KME release, and the lack of information on future private data use. We propose algorithms designed for practical DP kernel ERM by integrating the Nystrom method. Although the Nystrom method has been a state-of-the-art technique with random Fourier features in scalable kernel learning, this work is the first to apply it to DP kernel learning. The proposed algorithms are designed to be scalable and are suitable even for unregularized learning or non-convex loss functions. Moreover, we demonstrate that our Nystrom-based techniques can be used to construct a DP estimate of KME, which can be utilized for private two-sample tests via maximal mean discrepancy, for example. We also address the cases where the model is unknown, such as in public releases of private data, by employing an algorithm that releases versatile private data suitable for kernel ERM with diverse objectives.

### Related Works

**DP ERM.** ERM serves as a fundamental tool in machine learning, with substantial literature on DP ERM dating back to Dwork et al. (2006b). Among the current works, most of which regard linear ERM, main approaches include output and objective perturbation methods (Chaudhuri et al., 2011; Kifer et al., 2012; Jain and Thakurta, 2014; McSherry and Talwar, 2007; Gopi et al., 2022, 2023; Mangoubi and Vishnoi, 2022), and gradient perturbation methods (Bassily et al., 2014; Wang et al., 2017; Song et al., 2013). However, Iyengar et al. (2019) pointed out that most output or objective perturbation-based algorithms suffer from scalability issues in their implementation. For example, they need to directly access optimal solutions in order to release private models, thereby reducing scalability, particularly in kernel learning. Additionally, they often require strong assumptions on learning problems to guarantee privacy, such as double differentiable loss or a strongly convex regularizer, which limits the range of applicable learning scenarios (Iyengar et al., 2019). In contrast, gradient or SGD-based output perturbation methods (Wu et al., 2017; Feldman et al., 2020) are relatively unconstrained by learning restrictions or scalability concerns. While a vast amount of literature exists on linear ERM, not all DP kernel ERM algorithms can capitalize on these findings.

**DP kernel ERM and KME.** To adapt techniques from DP linear ERM to kernel ERM, one needs random features of the kernel suitable for DP learning. Random Fourier features were suggested as a candidate for translation-invariant kernels (Chaudhuri et al., 2011), but none have been proposed for general kernels. Hall et al. (2013) and Jain and Thakurta (2013) addressed DP kernel ERM algorithms for general kernels without using random features. However, Jain and Thakurta (2013) assumes accessible test data, which may be inappropriate in certain cases. On the other hand, Hall et al. (2013) suggests a sophisticated method for releasing a private model but requires strong regularization. Moreover, both algorithms need the true solution before releasing the private model, resulting in scalability issues. The discussion on DP KME has been somewhat distant from DP kernel ERM, initially introduced by Balog et al. (2018) as a technique for private database release. Balog et al. (2018) developed private algorithms for releasing KME for both general and translation-invariant kernels. However, their methods suffer from the curse of dimensionality in both cases. Additionally, we note that the techniques in Hall et al. (2013) and Jain and Thakurta (2013) cannot be directly adapted to DP KME release.

### Main Contributions

Scalable DP kernel ERM for general kernels.We develop scalable algorithms for DP kernel ERM that accommodate general kernels, particularly non-translation-invariant ones. The Nystrom method serves as a principal tool that enables much-desired scalability in practical applications. Our work is the first to propose a DP \(K\)-means Nystrom method and incorporate it in DP kernel ERM. Also, to the best of our knowledge, this is the first attempt to compare the theoretical and experimental efficacy of scalable private kernel learning algorithms. The proposed algorithms reduce time complexity and memory costs, akin to the Nystrom method in non-private settings. Furthermore, experiments in Section 4 demonstrate the superior performance of our algorithms for different learning problems. We present a brief comparison of our approach and three existing methods in Table 1.

DP KME.We propose a data-dependent DP KME releasing algorithm based on the DP \(K\)-means Nystrom method. The error of the DP KME is shown to be primarily related to how accurately the kernel matrix is estimated by the Nystrom method. Our empirical study suggests its superiority compared to alternative DP KME releasing algorithms.

Private data release for versatile kernel learning.We consider the offline setting for database release, where the database owner releases data without knowledge of the models or statistics desired by users. We provide a DP algorithm that generates a dataset yielding excess empirical risk bounds of \(O(n^{-c})\) for the logistic and the Linex (Ma et al., 2019) losses if it is used for kernel ERM.

## 2 Preliminaries

### Differential Privacy

Among the numerous variants of DP, this work primarily employs \((,)\)-DP, as it is widely applicable to various statistical procedures. Let \(\) be the data space. We consider a randomized algorithm \(M:^{n}()\), where \(()\) is a family of distributions over an output space \(\). Let \(D D^{}\) indicate that datasets \(D\) and \(D^{}\) are neighbors, meaning they differ only by a single individual.

**Definition 2.1** (\((,)\)-DP, Dwork et al. (2006a)).: An algorithm \(M\) is \((,)\)-DP if the following holds:

\[_{D,D^{}^{n},D D^{}}_{A }(M(D) A)-e^{}(M(D^ {}) A).\]

Definition 2.1 states that an algorithm is DP if its randomness is proportional to the deviation in the output due to a single data change, as demonstrated in Proposition 1.

**Proposition 1** (Gaussian mechanism, Nikolov et al. (2013)).: _If a deterministic algorithm \(:^{n}^{d}\) satisfies \(_{D,D^{}^{n},D D^{}}\|(D)- (D^{})\|_{2}\), an algorithm defined by \(M(D):=(D)+})}{ }\) is \((,)\)-DP where \(\) follows the standard normal distribution on \(^{d}\)._

  Algorithms & General kernels & Scalable & Test data free & General objective \\  Chaudhuri et al. (2011) & ✗ & ✓ & ✓ & ✓ \\  Jain and Thakurta (2013) & ✓ & ✗ & ✗ & ✗ \\  Hall et al. (2013) & ✓ & ✗ & ✓ & ✗ \\  Proposed & ✓ & ✓ & ✓ & ✓ \\  

Table 1: Comparison of DP kernel ERM algorithms in terms of restrictions for privacy guarantee.

Statistical procedures, including the algorithms we propose, typically involve multiple steps, such as data-dependent evaluations and multiple references to the data. Proposition 2 provides a privacy guarantee for such procedures.

**Proposition 2** (Composition theorem, Dwork et al. (2006a)).: _For algorithms \(M_{1}:^{n}()\) and \(M_{2}:^{n}()\), the algorithm \(M:^{n}\) defined by \(M(D)=M_{2}(D,M_{1}(D))\) for \(D^{n}\) is \((_{1}+_{2},_{1}+_{2})\)-DP if \(M_{1}\) is \((_{1},_{1})\)-DP and \(M_{2}(,z)\) is \((_{2},_{2})\)-DP for every \(z\)._

### Kernel, RKHS, and Random Features

Kernel methods learn a non-linear structure by using a non-linear map, called a feature map \(\), to transform the data \(\{x_{i}\}_{i=1}^{n}\) and analyze the linear structure of \(\{(x_{i})\}_{i=1}^{n}\). An appropriate choice of \(\) captures the underlying complex structure of the data. The feature maps are inherently determined by the kernel function \(k\), a positive definite function with a corresponding RKHS \((_{k},\|\|_{_{k}})\), satisfying \((x),(y)_{_{k}}=k(x,y)\). Within the RKHS, the outputs of the feature map behave akin to vectors in Euclidean space, allowing one to learn the non-linear structure through a linear model.

A random feature is a map \(:^{m}\) that approximates the kernel function in a way that \(k(x,y)(x),(y)\), providing a linearized version of kernel learning, as will be discussed in Sections 3.2 and 3.3. Traditionally, \(k(x,y)[(x),(y)]\) is required for a random feature map, but we adopt a more broad terminology.

#### 2.2.1 Nystrom Methods

Nystrom methods are low-rank kernel matrix approximation methods that facilitate scalable kernel learning. For given dataset \(\{x_{i}\}_{i=1}^{n}\), and a kernel matrix \(\), the approximation \(}\) is calculated as \(}:=}^{T}}^{}}\) where \(}:=[k(z_{i},z_{j})|_{m m}\), and \(}:=[k(z_{i},x_{j})]_{m n}\). Here, \(Z=\{z_{1},,z_{m}\}\) are some pre-chosen landmark points, and \(\) represents the Moore-Penrose inverse. Then, \(}\) can be interpreted as projections of data points onto the plane in the RKHS:

\[}=[_{}(x_{i}),_{ }(x_{j})_{_{k}}]_{n n}\]

where \(=\{(z_{i})|1 i m\}_{k}\). Thus, the accuracy of \(}\), or the accuracy of scalable kernel learning via \(}\), depends on how closely the subspace \(\) resembles the data \(\{(x_{i})\}_{i=1}^{n}\), the corresponding elements in the RKHS of the original data. Consequently, Nystrom methods operate by selecting landmark points that effectively represent the original data, as demonstrated in the past studies using data-dependent landmark points such as \(K\)-means centroids or subsamples (Kumar et al., 2012; Zhang et al., 2008; He and Zhang, 2018).

### Problem Formulation

**DP kernel ERM**. For a given dataset \(D=\{(x_{i},y_{i})\}_{i=1}^{n}()^{n}\), a loss function \(l:^{2}_{ 0}\), and a regularization parameter \(>0\), we consider the following regularized kernel ERM:

\[*{arg\,min}_{f_{k}}^{}(f;D) _{i=1}^{n}l( f,(x_{i})_{_{k}},y_{ i})+\|f\|_{_{k}}^{2}.\]

The minimization finds a function \(f\) that best explains the data under the given loss, and the regularization parameter \(\) prevents over-fitting. We measure the quality of the model by the excess empirical risk of \(f\), defined by \(^{}(f;D)-_{g_{k}}^{}(g;D)\).

**DP KME**. For a given data \(\{x_{i}\}_{i=1}^{n}^{n}\), our objective is to design a DP KME release mechanism: an \((,)\)-DP mechanism \(M:^{n}(_{k})\) that is sufficiently close to the KME \(_{X}:=[(X)]\) in the RKHS norm with high probability. Since it is known that the empirical KME \(_{X}:=)++(x_{n})}{n}\) converges to \(_{X}\) with \(O_{p}(n^{-})\)(Muandet et al., 2017), our DP KME algorithm will focus on releasing \(_{X}\).

**Versatile DP kernel ERM**. We explore a DP public dataset release tailored for kernel ERM. Unlike mechanisms developed in **DP kernel ERM**, which train models for specific loss functions \(l\) and regularization parameters \(\), we develop a private dataset release \((,)\)-DP mechanism \(M:^{n}^{n}\). This mechanism enables learning a kernel ERM for post-given, possibly infinitely many \(l\) functions and \(\) values.

Proposed Methods

Proofs for the theorems presented in this section are deferred to Appendix 6.3.

### DP \(K\)-means Nystrom Approximation

This section presents a pivotal tool for achieving scalability for general DP kernel learning: DP \(K\)-means Nystrom method. It is instrumental for our DP algorithms to make predictions using released private functions without needing access to the original data, and to be easily incorporated into the current linear DP ERM framework, unlike Hall et al. (2013).

As demonstrated by Chaudhuri et al. (2011) and Balog et al. (2018), DP kernel learning can effectively utilize the established methods of DP linear learning through random features. However, the use of random Fourier features depends on specific kernel characteristics, particularly translation-invariance, which complicates the development of random features for more general kernels. To overcome this challenge, we adopt the Nystrom approximation scheme, which is suitable for constructing random features for a wider range of kernels. As discussed in Section 2.2.1, selecting landmark points that accurately capture the data structure is crucial for the Nystrom method. Following the landmark selection guidelines outlined in Zhang et al. (2008), namely the \(K\)-means approach, we provide a detailed rationale behind this criterion to demonstrate how effectively these points represent the underlying data structure.

Define \(_{Z}^{(Nys)}:^{m}\) as \(_{Z}^{(Nys)}:=(_{S}(x),b_{1}_{_ {k}},,_{S}(x),b_{m}_{_{k}})\) where \(\{b_{i}\}_{i=1}^{m}\) is an orthonormal basis of \(\). Here, \(^{(Nys)}\) is our random feature map. Our goal is to select suitable landmarks that minimize the approximation error of \(^{(Nys)}\) for the kernel \(k\) at \(\{(x_{i},x_{j})\}_{n n}\), which can be expressed as follows:

\[}_{i,j=1}^{n}(k(x_{i},x_{j})-_{Z}^{(Nys)} (x_{i}),_{Z}^{(Nys)}(x_{j}))^{2}=}\|-}\|_{F}^{2}.\] (1)

Note that the error described in Eq. (1) is also known as the Nystrom approximation error in the literature on low-rank kernel approximation. The accuracy of Nystrom-based kernel learning depends on the accuracy of the kernel matrix approximation: \(\|-}\|_{2}\), which is upper bounded by \(\|-}\|_{F}\). However, optimizing the landmark selection by minimizing LHS in Eq. (1) under DP is challenging because the problem is neither convex nor Lipschitz, making DP optimization difficult. Theorem 1 offers a comprehensive approach to identifying appropriate landmark points by solving the \(K\)-means problem, rather than directly addressing the complex challenge.

**Theorem 1**.: _If a kernel \(k\) is \(c^{}\)-Lipschitz2 for both of its arguments, the Nystrom approximation error is bounded by the quantization error of the landmark points for the original dataset:_

\[\|-}\|_{F} 2c^{}^{n} _{z_{j}}\|x_{i}-z_{j}\|_{2}^{2}}.\] (2)

Theorem 1 sheds light on the connection between low-rank kernel approximation and data clustering. Since the Nystrom method is essentially an orthogonal projection of data onto \(\) in RKHS and the LHS of Eq. (2) resembles the common clustering objective, the theorem essentially suggests that a proper clustering can identify a subspace \(\) that is closely aligned with the data. We emphasize that the bound stated in Theorem 1 is \(O()\), where \(e=_{i=1}^{n}_{z_{j}}\|x_{i}-z_{j}\|_{2}^{2}\). The bound is tighter than the \(O(ne\|}^{-1}\|_{F})\) bound provided in Zhang et al. (2008), if the sample size \(n\) and the number of landmark points \(m\) are sufficiently large such that \(}\) has small eigenvalues, even though the assumption on kernel is equivalent to the one made in Zhang et al. (2008).

In Algorithm 1, we present a DP algorithm for \(K\)-means-based Nystrom approximation. The obtained orthonormal basis \(\{b_{i}\}_{i=1}^{n}\) of \(\) and their corresponding random feature map \(^{(Nys)}\) can be used in the subsequent DP kernel learning. For the DP K-means step in line 2, the algorithm from the diffprivlib package implemented by IBM 3 can be used.

**Theorem 2**.: _Algorithm 1 is \(\)-DP, and \(R[^{(Nys)}(x)]_{i}=_{}(x),b_{i}_{ _{k}}\) for all \(i\) and \(x\)._

Although the privacy remains unaffected by the threshold parameter \(m_{0}\), it does influence the quality of the landmark points: the DP \(K\)-means centroids. Setting \(K=m\) and omitting the thresholding step will result in small cluster sizes for large \(m\), making the centroid estimates more susceptible to the noise added in the DP \(K\)-means step. This issue is exacerbated by a small privacy budget \(\), which further amplifies the noise. To mitigate this effect, thresholding is performed using \( m_{0}\). After applying cluster size thresholding, additional \(m-K\) landmark points are generated in line 3 from a distribution \(Q\). While \(Q\) can be chosen arbitrarily, we choose it as a mixture of \(K\) truncated normal distributions centered at \(z_{i}\)s, incorporating data information extracted from the DP \(K\)-means.

### Scalable DP Kernel ERM

Using the output of Algorithm 1, we can construct a DP kernel ERM for general kernels, as presented in Algorithm 2. Since the generation of random features is performed differentially privately, any existing DP linear ERM algorithms can be subsequently applied to achieve DP, in accordance with the composition theorem in Proposition 2. As an illustration, the method in Kifer et al. (2012), detailed in Algorithm 5 in Appendix 6.2, is adapted for general kernels in line 2 of Algorithm 2.

``` Input: given data \(\{x_{1},,x_{n}\}\), integer \(m\), and kernel \(k\). Output:\(_{k}\).  1. \(^{(Nys)},\{b_{1},,b_{m}\}/2\)-DP feature map and basis, output of Algorithm 1.  2. \(u\) by solving \((/2,)\)-DP linear ERM for data \(\{(^{(Nys)}(x_{i}),y_{i})\}_{i=1}^{n}\):  3. \(:=_{i=1}^{n}u_{i}b_{i}\).  4. return\(\) ```

**Algorithm 2** DP kernel ERM for general kernels

**Theorem 3**.: _Algorithm 2 is \((,)\)-DP._

Line 1 of Algorithm 2 transforms the data points to random features, and line 2 solves the DP linear ERM with respect to the newly produced data \(\{^{(Nys)}(x_{i})\}_{i=1}^{n}\). This step reduces the computational complexity, for example, from \(O(n^{3})\) to \(O(nm^{2}+nmd)\) in kernel ridge regression.

The linearization through random features introduces extra learning errors beyond the DP linear ERM error, specifically the approximation error for the given kernel, which is related to the \(\|-}\|_{2}\), as shown in Theorem 4. The experiment results in Appendix 6.1 suggest that our DP random feature algorithm approximates the Gaussian kernel better than random Fourier features for small \(m\).

Also, we note that although the privacy budgets were allocated equally for step 1 and 2, it may not be the optimal choice. The Fig. 3 demonstrates the utility can be improved in other privacy allocations. However, we fix the allocation as half since tuning the optimal allocation may involve privacy leakage. We included the detail in Appendix 6.1.

**Theorem 4**.: _If the loss function is \(c\)-Lipschitz with respect to its first argument, the excess empirical risk of Algorithm 2 equipped with \((,)\)-DP linear ERM algorithm is \(_{n,m}()+-\|_{2}}{2 n}\) with probability at least \(1-\) where \(_{n,m}()\) is an empirical risk bound of the DP linear ERM algorithm satisfied with probability at least \(1-\) for inputs in a unit ball._

### DP Kernel Mean Embedding

This section introduces a DP KME method using \(K\)-means Nystrom approximation in Algorithm 3. An important consideration in DP KME is that the released private embedding should be an element of the RKHS. Note that the noise level in the release of the coefficient vector \(w\) in line 3 of the algorithm is determined by Proposition 1, ensuring accuracy proportional to the sample size \(n\).

``` Input: given data \(\{x_{1},,x_{n}\}\), integer \(m\). Output: a DP kernel mean embedding \(f_{k}\)  1. \(^{(Nys)},\{b_{1},,b_{m}\}/2\)-DP feature map and basis, output of Algorithm 1.  2. \(w R(_{i=1}^{n}^{(Nys)}(x_{i})+})}{n})\) where \( N(0,I_{m m})\)  3. \(_{X}_{i=1}^{m}w_{i}b_{i}\)  4. return\(_{X}\) ```

**Algorithm 3** DP kernel mean embedding (DP KME)

**Theorem 5**.: _Algorithm 3 is \((,)\)-DP._

The following theorem regards the difference between our DP KME and the conventional empirical KME estimate \(_{X}\).

**Theorem 6**.: _The DP KME error in the RKHS norm is given as follows:_

\[\|_{X}-_{X}\|_{_{k}} -}\|_{2}}{}+(1+})}{n}(+})\]

_with probability at least \(1-\)._

According to Theorem 6, the accuracy of DP KME depends on the accuracy of the low-rank approximation of \(\). By adaptively selecting landmark points according to the data, we can effectively reduce the approximation error. The following theorem confirms the advantage of using the data distribution in DP KME.

**Theorem 7**.: _If the landmarks are selected independently of the data, the KME error of Algorithm 3 would become:_

\[\|_{X}^{()}-_{X}\|_{_ {k}} 2R(}{n}+}{n}}+ +2})}{n}( +}))+\|_{X}-_{S}_{X} \|_{_{k}}\]

_with probability at least \(1-\) where \(^{}=2\), and \(_{X}^{()}\) denotes the DP KME obtained by data-independently selected landmark points._

Theorem 7 addresses the scenario where landmark points are selected independently of the data, as suggested by Balog et al. (2018). The error depends on how closely the distribution of the landmark points \(\) resembles that of the data. Consequently, disregarding the data distribution \(_{X}\) can lead to suboptimal results in kernel learning.

### Data Release for Versatile DP Kernel ERM

Here, we consider scenarios where sensitive data must be released without knowing which ERM model(s) will be trained on it. We propose a framework for releasing privacy-preserving datasets suitable for kernel ERMs that ensures robust accuracy with diverse loss functions. The non-interactive local DP ERM framework utilizing a polynomial approximation of the gradient of the objective is employed for the purpose. The proposed Algorithm 4 is a modified version of Algorithm 5 from Zheng et al. (2017), tailored for kernel learning. Denote \(\|\|:=_{y}|y|\).

**Theorem 8**.: _Algorithm 4 is \((,)\)-DP._

In this versatile learning problem, we add a new constraint that the parameter space has a radius \(r>0\), which is a common restriction in private ERM contexts (Kifer et al., 2012). Then the ERM problem can be written as:

\[_{f,\|f\|_{_{k}} r}_{i}l(  f,(x_{i})_{_{k}},y)+ \|f\|_{_{k}}^{2}.\] (3)

We assume the following to ensure an efficient polynomial approximation of the loss by constraining its form. Note that the assumption accommodates a broader range of existing losses; for instance, logistic loss and smooth variants of hinge loss, compared to the assumption in Zheng et al. (2017).

**Assumption 1**.: The loss function satisfies \(l(,y)=l_{0}(y)\) for convex, \(c\)-Lipschitz \(l_{0}:_{ 0}\), which is \(b\)-smooth i.e., it is differentiable and satisfies \(l_{0}(t_{1})-l_{0}(t_{2}) l_{0}^{}(t_{2})(t_{1}-t_{2})+ (t_{1}-t_{2})^{2}\).

Theorem 9 states that the optimization in Eq. (3) can be solved using the output of Algorithm 4 with a theoretical utility guarantee.

**Theorem 9**.: _Under Assumptions 1, for a regularization parameter \(>0\) and a \(p\)th degree polynomial \(h\), there exists an algorithm \(_{l,}\) that takes the output of Algorithm 4 and returns a classifier with excess empirical risk:_

\[((^{2}+c^{2}+a_{}^{2}( )^{2p+1})\|\|^{2}^{2}}{n ^{2}}+ R\|\|r)\]

_with probability \(1-\), where \(:=\|l_{0}^{}(y)-h(y)\|_{}\) on \(|y| r\|\|R\), \(a_{}\) is the maximum absolute value of the coefficients of \(h\), and \(\) is a big \(O\) notation ignoring \( n\) factors._

_Remarks_.: The bound in Theorem 9 relies heavily on the quality of the polynomial approximation of the gradient. For example, the Huber loss4 used for support vector machines (SVM), gives a poor guarantee: \(O(( n)^{-1})\) with Chebyshev polynomial approximation \(h\) due to the lack of smoothness of the gradient. In contrast, smooth losses, such as the logistic loss \(l(,y)=(1+e^{-y})\), and the Linex loss \(l(,y)=e^{a(1-y)}-a(1-y)-1\), offer faster convergence of guaranteed excess empirical loss bounds: \(O(n^{-c})\) for some \(c>0\). The algorithm 9, detailed in Algorithm 7 in Appendix 6.2, utilizes an inexact oracle gradient method as outlined in Algorithm 3 of Dvurechensky and Gasnikov (2016).

## 4 Experiments

In this section, we demonstrate our proposed DP methods for DP kernel learning with simulated and real data. Specifics on the data generation can be found in Appendix 6.1.

The first example is designed to demonstrate the benefit of scalable DP kernel ERM for general kernels. The data comes from two classes with a polynomial boundary between them, making the non-translation-invariant polynomial kernel the most preferable choice for a kernel function. Weconsider three kernels: 3rd-order polynomial, Gaussian RBF, and linear kernels. Learning with the former two kernels is conducted as a DP linear ERM (SVM with Huber loss using the algorithm in Kifer et al. (2012)) with \(m=200\) random features obtained by Algorithm 1 and random Fourier features, respectively. The test data classification accuracy shown in Figure 1(a) clearly demonstrates the superior performance of the polynomial kernel across a wide range of the privacy budget.

We assess the estimation error of DP KME algorithms using the adult dataset (see Appendix 6.1 for details) with a Gaussian kernel for a fixed number of landmark points \(m\). Although Algorithm 3 uses the \(K\)-means-based Nystrom method, we also explore a subsampling-based version (outlined in Algorithm 6 in Appendix 6.2) with a privacy budget that is 100 times larger, as it demonstrates meaningful results in low privacy regions. We compare DP KME algorithms employing both methods, as well as Algorithm 1 in Balog et al. (2018). Figure 1(b) shows the superior performance of \(K\)-means-based method among the three methods. The higher errors for Balog et al. can be attributed to the misalignment of distributions between random samples and the data, causing a larger error than the error added by the noise for privacy guarantee. In contrast, Nystrom methods are data-dependent, incorporating data information into their kernel approximation. However, in highly private scenarios with small \(\) values, this capability is compromised, leading to accuracy loss. Furthermore, the \(K\)-means-based Nystrom method outperforms subsampling. This is partly due to the characteristic of the DP \(K\)-means algorithm, which adds noise inversely proportional to the cluster size, whereas the subsampling-based method adds noise indiscriminately. Consequently, DP \(K\)-means ensures that larger, more significant clusters are less perturbed, and selects important landmark points with greater accuracy, leading to improved performance in DP kernel learning. Also, it appears that the quality of the subsampling-based landmark points deteriorates easily with the addition of noise. Kernel approximation through DP subsamples is more susceptible to degradation under privacy constraints compared to DP \(K\)-means.

## 5 Discussion, Limitations, and Future Work

In this work, we propose a \(K\)-means Nystrom-based private scalable kernel learning framework that is applicable for general kernels. We have developed DP kernel ERM, KME, and public data release mechanisms for versatile kernel learning within this framework. Theoretical and empirical investigations verify that our new framework is superior to existing ones, offering better performance with fewer constraints.

The Nystrom method employed for the kernel matrix approximation has a few discussion points. First, we point out that utilizing DP \(K\)-means for scalability does not necessarily imply that the data must contain discernible clusters; instead, it helps ensure that the landmark points more effectively retain the prominent features of the data. Second, a _supervised_ Nystrom method, if feasible, could yield better results in certain cases. Specifically, in kernel ridge regression, focusing on a few eigenvectors aligned with the response, rather than the entire kernel matrix, might require less DP noise and enable more efficient learning.

Figure 1: (a) Comparison of classification accuracy of scalable DP kernel ERMs with different kernels over a range of the privacy budget (b) Comparison of embedding errors of the proposed DP KME with alternative approaches.

Even though our proposed framework is designed to work with general kernels, the current implementation is limited to handling Euclidean data due to its dependence on \(K\)-means. A potential remedy for this limitation would be to consider a \(K\)-medoids type clustering method, which ensures that the centroids are selected among actual observations. Further investigation into DP \(K\)-medoids Nystrom approximation and the development of subsequent private kernel learning methods are recommended as areas for future research.