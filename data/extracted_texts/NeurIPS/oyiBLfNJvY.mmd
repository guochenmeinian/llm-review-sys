# Exploration by Learning Diverse Skills through Successor State Representations

Paul-Antoine Le Tolguenec

ISAE-Supaero, Airbus

paul-antoine.le-tolguenec@airbus.com &Yann Besse

Airbus

yann.besse@airbus.com &Florent Teichteil-Konigsbuch

Airbus

florent.teichteil-konigsbuch@airbus.com &Dennis G. Wilson

ISAE-Supaero, Universite de Toulouse

dennis.wilson@isae-supaero.fr &Emmanuel Rachelson

ISAE-Supaero, Universite de Toulouse

emmanuel.rachelson@isae-supaero.fr

###### Abstract

The ability to perform different skills can encourage agents to explore. In this work, we aim to construct a set of diverse skills that uniformly cover the state space. We propose a formalization of this search for diverse skills, building on a previous definition based on the mutual information between states and skills. We consider the distribution of states reached by a policy conditioned on each skill and leverage the successor state representation to maximize the difference between these skill distributions. We call this approach LEADS: Learning Diverse Skills through Successor State Representations. We demonstrate our approach on a set of maze navigation and robotic control tasks which show that our method is capable of constructing a diverse set of skills which exhaustively cover the state space without relying on reward or exploration bonuses. Our findings demonstrate that this new formalization promotes more robust and efficient exploration by combining mutual information maximization and exploration bonuses.

## 1 Introduction

Humans demonstrate an outstanding ability to elaborate a repertoire of varied skills and behaviors, without extrinsic motivation and supervision. This ability is currently captured through the problem of unsupervised skill discovery in reinforcement learning , where one seeks to learn a finite set of policies in a given environment whose behaviors are notably different from each other. Implicitly, seeking behavioral diversity is also related to the question of efficient exploration, as a set of skills that better covers the state space is often preferable. Seeking diversity has recently been successfully studied through the prism of mutual information maximization . In this article, we argue maximizing mutual information might be ambiguous when seeking exploratory behaviors and propose an alternative, motivated variant. The key intuition underpinning this formulation is that a good set of exploratory skills should maximize state coverage, while preserving the ability to distinguish a skill from another. Then, we propose a new algorithm which implements this generalized objective, leveraging neural networks as estimators of the state occupancy measure. This algorithm demonstrates better exploration properties than state of the art methods, both those designed for reward-free exploration and those seeking skill diversity.

We motivate our developments with a first illustrative toy example. Let us consider a reward-free Markov decision process (38, MDP) \((,,P,,_{0})\) where \(\) is the state space, \(\) the action space, \(P\) the transition function, \(\) the discount factor and \(_{0}\) the initial state distribution. A behavior policy, parameterized by \(\), maps state to distribution over actions. We define a _skill encoding_\(z^{d}\) as an abstract set of skill descriptors that enhances the policy description and conditions the action taken \(a_{}(s,z)\), making the policy a function \(:^{d}()\). Unsupervised skill discovery generally considers a finite collection \(=\{z_{i}\}_{i[1,n]}\) of \(n\) skills, and a distribution \(p(z)\) on skills within \(\) (generally uniform). For a given \(\), each skill \(z\) induces a state distribution \(p(s|z)\). The state distribution under the full set of skills is hence \(p(s)=_{}p(s|z)p(z)\). When maximizing diversity, we aim to find \(^{*}\) such that, for any pair of skills \(z_{1},z_{2}\), the states visited by each skill are as separable as possible. Additionally, we would like the state coverage of the full set of skills to cover as much of the state space as possible.

Maximizing diversity has been formalized in the literature as the maximization of the mutual information \((S,Z)\) between the state random variable \(S\) and the skill descriptor \(Z\). This mutual information (39, MI) quantifies how predictable \(S\) is, when \(Z\) is known (and conversely):

\[(S,Z) D_{KL}((S,Z)||(S)(Z)),\] \[= (S)-(S|Z),\]

where \(D_{KL}\) is the Kullback-Leibler divergence and \(\) is the (conditional) entropy.

Figure 1 illustrates why \((S,Z)\) is an imperfect measure of diversity. In this figure, two sets \(_{1}\) and \(_{2}\) of four skills each are represented by their respective state coverage in a grid maze MDP (one symbol per skill). In each set of skills, each skill is picked with probability \(p(z)=1/4\). On the leftmost image (\(_{1}\)), each skill covers a single state, hence \(p(s|z)=1\) in this state and 0 otherwise. On the rightmost image (\(_{2}\)), \(p(s|z)=1/2\) on each covered state and 0 otherwise. Consequently, \(_{1}(S,Z)=_{1}(S)-_{1}(S|Z)= 4\) and \(_{2}(S,Z)=_{2}(S)-_{2}(S|Z)= 4\). The mutual information is hence ambiguous as it ranks the two sets of skills equally, while \(_{2}\) enables better exploration.

Our contribution is structured around maximizing a variant of \((S,Z)\), using the successor state representation  to enable its estimation. Section 2 puts this idea in perspective of the relevant literature, highlighting similarities and contrasts with previous works. Then Section 3 introduces and discusses our LEADS algorithm. Section 4 reports empirical evidence of the good exploratory properties of LEADS, demonstrating its ability to outperform state-of-the-art methods on a span of benchmarks.

## 2 Related work

This section endeavors to provide the reader with a comprehensive overview of approaches which aim at exploration in RL and skill diversity.

**Exploration bonuses.** Driving exploration by defining reward bonuses has been a major field of research for over two decades [25; 6; 1; 3; 36; 7; 2; 19]. These methods generally involve defining reward bonuses across the state space, which decrease over time in well-explored states. In large state spaces where count-based methods are challenging, these approaches require function approximation to generalize across the state space. However, as explained by Jarrett et al. , identifying these exploration bonus approximators is an adversarial optimization problem making these approaches unstable in practice .

**Quality Diversity.** Exploration can also be achieved as a by-product of behavior diversity search. Evolutionary algorithms have been used to search for new behaviors, such as in Novelty Search , and for a diversity of behaviors which encourage high performance through the growing domain

Figure 1: State distributions of two sets \(_{1}\) (left) and \(_{2}\) (right) of four skills each on a grid maze. Each skillâ€™s visited states are represented by a different symbol and distributed uniformly. The gray boxes are unreachable.

of Quality Diversity algorithms, exemplified by MAP-Elites . In these algorithms, skills are usually characterized through hand-crafted _behavior descriptors_, although there are methods which learn such descriptors through variational inference [10; 9]. Diversity is expressed as the coverage over such descriptors, and policies are modified through gradient-free  or gradient-based  updates in order to incrementally populate an archive of diverse skills. While these algorithms have demonstrated competitive results with exploration algorithms in RL , in general, they require expertise to define a behavior descriptor and are less sample-efficient than RL methods .

**Information theory approaches to diversity.** The field of information theory offers valuable insights into designing algorithms that generically promote diversity. Gregor et al. , Eysenbach et al.  and Sharma et al.  were pioneers in proposing methods to maximize either the reverse or forward form of \((S,Z)\) for a fixed set of skills \(\). To address the issue of unbalanced state distributions across skills throughout the entire environment, Campos et al.  suggest sequencing the problem into three stages: pure exploration in \(\) to promote sampled states diversity, state clustering via variational inference, and skill learning by maximisation of the forward form as proposed by Sharma et al. . In the exploration phase, Campos et al.  employs the State Marginal Matching algorithm proposed by Lee et al. , which resembles MI-based algorithms as it combines mutual information maximization with an exploration bonus to efficiently explore the state space.

Focusing even more on exploration, Liu and Abbeel  automate learning Novelty Search behavior descriptors by training an encoder with contrastive learning in order to promote exploration. Focusing also on exploration, Kamienny et al.  employ an incremental approach combined with the maximization of the reverse form of MI to eliminate skills that are not discernable enough, subsequently generating new skills from the most discernable ones.

Finally, recent studies including those by Park et al. , Park et al. , and Park et al. , integrate mutual information (MI) with trajectory-based metrics between states to enhance exploration. Specifically, these studies aim to maximize the Wasserstein distance, also known as the Earth mover's distance, between \((S,Z)\) and \((S)(Z)\) while the definition of MI uses the KL divergence. This distance requires specifying a metric for the state space. Park et al.  and Park et al.  uses the Euclidean distance, while Park et al.  employs the temporal distance.

It is important to note that, although these methods induce an interesting approach to exploration (as a by-product of diversity), they do not explicitly aim at encouraging coverage of the MDP's state space.

**Successor features for MI maximization.** Few attempts have been made to exploit universal successor features  in MI maximization methods. Machado et al.  decompose the learning process into two stages: first, they automatically acquire specific options, which are then used to develop a hierarchical policy that integrates these options. In contrast, Hansen et al.  first learn a general task-based policy conditioned on a task \(z\), and subsequently optimize the reverse form of MI by utilizing universal successor features to identify a range of diverse skills efficiently. More recently, Grillotti et al.  used successor features to optimize the quality of predefined behaviors based on the Quality Diversity formulation, which requires hand-crafted behavior descriptors.

The method we propose explicitly aims at promoting diversity with the goal of achieving good exploration. For this purpose, we turn the MI objective function into a new objective which promotes exploration, separating this work from previous contributions in MI maximization.

## 3 Promoting exploration with successor state representations

We now derive an algorithm that explicitly encourages exploration in diversity search. We start by casting mutual information maximization in terms of state occupancy measures and derive a lower bound which can be maximized with stochastic gradient ascent on the policy's parameters \(\) (Section 3.1). Then, as maximizing MI does not encourage exploration, we transform this lower bound so that its maximization fits this goal, leading to a new target quantity designed to promote both diversity and exploration (Section 3.2). This enables defining and implementing a general algorithm for _Learning Diverse Skills through successor state representations_ (LEADS, Section 3.3).

### MI lower bound with successor state representations

The mutual information \((S,Z)\) is naturally expressed through the (conditional) probability densities of \(S\) and \(Z\). Successor state representation estimators [4, SSR] provide a direct way of estimating these densities. More precisely, we identify the Successor State Representation as the density of the measure defined on the set of states. The SSR \(p(s|s_{1},,z)\) of a skill \((,z)\) is the state occupancy measure of policy \(_{}(s,z)\), when starting from \(s_{1}\), that is

\[p(s_{2}|s_{1},,z)=_{t=0}^{}^{t}p(s_{t}=s_{2}| s_{0}=s_{1},\\ a_{t}_{}(s_{t},z).).\] (1)

For the sake of readability, we make \(\) implicit in our notations going forward. This measure can be understood as the probability density of reaching the state \(s_{2}\) starting from another state \(s_{1}\) when the skill \(z\) is used . It encodes all the paths of the Markov chain that can be taken from \(s_{1}\) to \(s_{2}\) when the skill \(z\) is used . Here \((s_{1},s_{2})^{2}\) can be any two states of the Markov chain.

The SSR is the fixed point of a Bellman operator and one can employ temporal difference (TD) learning to guide a function approximator towards this fixed point. There are numerous methods for estimating the SSR density . In this paper, we use C-Learning  for estimating the SSR for all experiments. C-Learning proposes to cast the regression problem of learning \(p\) as a classification problem. Specifically, for a given pair \((s_{1},a,z)\), a classifier \((f_{}(s_{1},a,s_{2},z))\), where \(\) is the sigmoid function, is trained to predict whether \(s_{2}\) was sampled from an outcome trajectory from \(s_{1}\) (\(s_{2} p(|s_{1},a,z)\)) or from the marginal density of all possible skills (\(s_{2} p(s)\), the distribution over \(\)). The optimal classifier holds the following relation with the SSR's density: \(e^{f_{}(s_{1},a,s_{2},z)}=p(s_{2}|s_{1},a,z)/p(s_{2})\). We denote \(m_{}(s_{1},a,s_{2},z)=e^{f_{}(s_{1},a,s_{2},z)},\) as the SSR estimated via C-Learning, taking \(a_{}(s_{1},z)\). For the sake of brevity, we will use the notation \(m_{}(s_{1},s_{2},z)=}_{a_{}(s_{1}, z)}[m_{}(s_{1},a,s_{2},z)]\). We note that any method which reliably estimates \(m_{}(s_{1},s_{2},z)\) can be used instead of C-Learning in LEADS.

As described in Section 1, the mutual information between the state random variable \(S\) and the skill descriptor \(Z\) is \((S,Z)=D_{KL}((S,Z)||(S)(Z))\). This can be expressed as

\[(S,Z)=_{(s_{2},z) p(s,z)}[(p(s_{2}|z)/p( s_{2}))].\] (2)

State \(s_{2}\) can be any state \(s\) within the set \(\), yet we refer to it as \(s_{2}\) to ensure that when we incorporate the definition of the SSR, it aligns directly with the notation of Equation 1. By definition of the SSR under a given policy \((s,z)\), the state distribution \(p(s|z)\) obeys

\[p(s_{2}|z)=_{s_{1} p(s|z)}[p(s_{2}|s_{1},z)].\] (3)

By sampling \(s_{1} p(s|z)\), we can therefore estimate \(p(s_{2}|z)\), allowing for the injection of \(m(s_{1},s_{2},z)=p(s_{2}|s_{1},z)/p(s_{2})\) from Eysenbach et al.  into Equation 2:

\[(S,Z)=_{(s_{2},z) p(s,z)}[(_ {s_{1} p(s|z)}(m(s_{1},s_{2},z)))].\]

A Monte Carlo estimator of \((S,Z)\) can be obtained by sampling \((s,z)\) from a replay buffer. However, for every state \(s\) sampled this way, the estimate of \(p(s|z)\) provided by Equation 3 requires sampling a batch of \(s_{1} p(s|z)\). This is possible through keeping separate replay buffers for each skill, but would be computationally intractable for an accurate estimation. Hence, as in similar methods, we turn rather to a lower bound on \((S,Z)\), which admits a Monte Carlo estimator, and which will be maximized with respect to \(\) using stochastic gradient ascent.

Applying Jensen's inequality to the inner expectation in the previous expression yields

\[(S,Z)_{z p(z)\\ s_{1} p(s|z)\\ s_{1} p(s|z)}[(m(s_{1},s_{2},z))].\] (4)

Note that \(\) participates in this lower bound through \(m(s_{1},s_{2},z)=_{a_{}(|s_{1})}m(s_{1},a,s_{2},z)\). This quantity is essentially what an algorithm like DIAYN  maximizes: given separate experience buffers for each skill, one can compute a Monte Carlo estimate of this lower bound on the gradient of the mutual information \((S,Z)\) with respect to \(\).

### Promoting exploration with different skills

Looking closely at Equation 4, one can note that the interplay between each skill's state distribution is lost when deriving the lower bound. Hence, this bound can be maximized without actually pushing skills towards distinct states, hence without skill diversity. To regain the incentive to promote skill diversity, we propose to encourage exploration in each skill to put probability mass on states which receive little coverage by the full set of skills. In other words, for a given transition \((s,(s))\), we want to augment the probability of occurrence for one skill while decreasing it for all others.

Since \(m(s_{1},s,z) 0\), we can introduce the following bound:

\[(S,Z)z p(z)\\ s_{2} p(s|z)\\ s_{1} p(s|z)}{}[(,s_{2 },z)}{1+}{}m(s_{1},s_{2},z^{})} )].\] (5)

While Equation 5 is a looser lower bound than Equation 4, it goes towards the goal of diversity search originally captured by MI maximization: to have distinct state distributions for each skill.

However, as illustrated in the introductory example, maximizing MI does not promote large state coverage, nor exploratory behaviors. Equation 5 promotes skill diversity, but it does not explicitly encourage state coverage. We therefore argue that exploratory behaviors can be obtained by focusing the state distribution of each skill towards specific states within the support of \(p(s|z)\). Instead of sampling \(s\) according to \(p(s|z)\), we encourage exploration by attributing more probability mass to states that trigger exploration. We do so by creating a sampling distribution \((s|z)\) based on an uncertainty measure \(u_{t}(s,z)\), as explained next.

Biasing a skill towards promising states for exploration is often achieved through exploratory bonuses based on uncertainty measures , or repulsion mechanisms . These heuristic measures encourage exploration by pushing policies towards states of high uncertainty or away from previously covered transitions. The novelty of our approach is the use of an exploration bonus in the framing of mutual information maximization. However, by replacing \(s_{2} p(s|z)\) by another sampling distribution \((s|z)\), we lose the theoretic guarantee of maximizing the lower bound of Equation 5. Rather, we propose a new objective, expressing an incentive to explore within the search for skill diversity, but with a greater focus on state coverage through distinct skills than previous lower bounds on mutual information. The quantity we maximize is then:

\[()=z p(z)\\ s_{1} p(s|z)\\ a_{s}_{}(s_{1},z)\\ s_{2}(s|z)}{}[(, a_{z},s_{2},z)}{1+}{}m(s_{1},a_{z^{}},s_{2},z^{ })})].\] (6)

The uncertainty measure \(u_{t}\) which defines the distribution \((s|z)\) is designed to explore under-visited areas and to create repulsion between different skills. We define three desired properties for states to prioritize using this measure. To describe these properties, consider a sequence of policies \(_{t}\), each defining a set of skills \(\{_{t}(,z)\}_{z}\), and the corresponding sequence of SSRs \(m_{t}\); the three properties are then as follows. (1) A good target state \(s^{z}_{t}\) for skill \(z\) at time step \(t\) is one that has high probability of being visited by \(_{t}(,z)\), but was relatively infrequently visited by previous policies' state occupancy measures \(\{m_{k}\}_{k[1,t-1]}\) for any skill. (2) It is also a state which has both high probability of being reached by the current \(_{t}(,z)\) and low probability of being reached by any other current skill, starting from \(s_{0}\). (3) Given a previous target state \(s^{z}_{t-1}\), a good new target state is one that has high (resp. low) probability to be reached by \(_{t}(,z)\) (resp. any other skill), starting from \(s^{z}_{t-1}\). While the first property explicitly encourages visiting under-visited states, the two others do not encourage exploration per se, and rather strengthen skill diversity by pushing their state distributions apart. This leads to the idea of ranking target states according to:

\[u_{t}(s,z)=((s_{0},s,z_{i}) }{_{k=1}^{t-1}_{z^{}}m_{k}(s_{0},s,z^{})})}{}+z^{} z\\ }{ z}((s^{z} _{t-1},s,z)}{m_{t}(s^{z^{}}_{t-1},s,z^{})})}}+( (s_{0},s,z)}{m_{t}(s_{0},s,z^{})})\] (7)The \((s|z)\) distribution of Equation 6 therefore allocates more probability to states with high \(u_{t}(s,z)\). Empirically, we found that making \(\) deterministic as the Dirac distribution on a state that maximizes \(u_{t}(s,z)\) enabled efficient exploration. Appendix A provides a more formal perspective on the derivation of the \(u_{t}\) uncertainty measure.

### The LEADS algorithm

With the objective of maximizing \(()\) (Equation 6), we define the LEADS (Learning Diverse Skills through successor state representations) algorithm, presented in Algorithm 1.

``` Initialize \(_{0}\) for\(t[0,N]\)do # Collect samples \(_{z}=, z\) for\(e[1,n_{}]\)do  Sample skill \(z p(z)\) \(\{(s_{t},a_{t},r_{t},s_{t}^{})\}=\) episode with \(_{_{t}}(,z)\) from \(s_{0}\) \(_{z}=_{z}\{(s_{t},a_{t},r_{t},s_{t}^{})\}\) endfor # Learn the SSR  Learn \(m_{_{t}}\) for \(_{_{t}}\) using on-policy C-learning  Sample \(s(s|z)\) # Improve # for\(i[1,n_{}]\)do  Sample \(z p(z)\), \(s_{1} p(s|z)\) \(+_{}[()+_{h} ()]\)  Update \(_{t}\) using off-policy C-learning endfor endfor ```

**Algorithm 1** LEADS

First, the sampling phase populates separate replay buffers \(_{z}\) for each skill \(z p(z)\). This is achieved by rolling out \(n_{}\) episodes with a given policy \(_{}(,z)\). Then, C-learning is used to compute the parameters \(_{t}\) of a joint SSR \(m_{t}(s_{1},a,s,z)\) for \(_{t}\), and store it. We note that the on-policy version of C-learning can be used for this step as the data in the replay buffers has been collected with the current policy \(_{t}\). This SSR, and all previous ones, are then used to define \(u_{t}(s,z)\), which in turn defines the distribution \((s,z)\). As this can be approximated for a static policy, sampling can be performed before updating \(\). Sampling \(s(s|z)\) is performed by running all states in \(_{z}\) through \(u_{t}(s,z)\), although, to limit computational cost, we can restrict this evaluation and selection to only a uniformly drawn subset of each \(_{z}\).

We therefore have the necessary components to optimize \(\) according to \(()\): a means of sampling states and the SSR. At each gradient ascent step, we sample a mini-batch of states \(s_{1}\) from \(_{z}\) for a given skill \(z p(z)\) to estimate \(p(s|z)\). This permits calculating the objective \(()\) for the current \(\) using \(m_{_{t}}\) and performing a gradient step. In the gradient calculation, we also include an action entropy maximization term, as done in other works :

\[()=}_{s_{1} p(s|z) \\ z p(z)\\ a(s_{1},z)}[-(_{}(a|s_{1},z))].\] (8)

\(\) is therefore updated to maximize \(()+_{h}()\), although \(_{h}\) is intentionally kept small (0.05) to focus on the principal LEADS objective \(()\).

Finally, after each gradient step, the SSR is updated using the off-policy formulation of C-learning so that \(m_{_{t}}\) remains representative of the state distribution under \(_{_{t}}\). This is done without sampling new transitions and is justified by the fact that the target state \(s_{t}^{z}\) and the states along a trajectory to \(s_{t}^{z}\) are already within the replay buffer \(_{z}\). \(n_{SGD}\) steps of gradient ascent on \(\) are performed in this way, before a new iteration of LEADS is started.

Experiments & Results

We demonstrate LEADS on a set of maze navigation and robotic control tasks and compare its behavior to state-of-the-art exploration algorithms. We provide all code for LEADS and the baseline algorithms, as well as the scripts to reproduce the experiments (repository). All hyperparameters are summarized in Appendix C.

### Evaluation benchmarks

**Mazes.** Maze navigation has been frequently used in the exploration literature, as 2D environments allows for a clear visualization of the behaviors induced by an algorithm. The assessment of state coverage is also easier to understand than in environments with high-dimensional states. We design three different maze navigation tasks, named **Easy**, **U**, and **Hard** (depicted in Figure 2), of increasing difficulty in reaching all parts of the state space. In each of the mazes, the state space is defined by the agent's Euclidean coordinates \(=[-1,1]^{2}\) and the action space corresponds to the agent's velocity \(=[-1,1]^{2}\). Hitting a wall terminates an episode, making exploration difficult.

**Robotic control.** We further assess the capabilities of LEADS in complex robotic control tasks. These tasks allow evaluating the ability of LEADS to explore in diverse and high-dimensional state spaces. We evaluate LEADS on a variety of MuJoCo  environments from different benchmark suites. **Fetch-Reach** is a 7-DoF (degrees of freedom) robotic arm equipped with a two-fingered parallel gripper; its observation space is 10-dimensional. **Fetch-Slide** extends the former with a puck placed on a platform in front of the arm, increasing the observation space dimension to 25. **Hand** is a 24-DoF anthropomorphic robotic hand, with a 63-dimensional observation space. **Finger** a 3-DoF, 12-dimensional observation space, manipulation environment where a planar finger is required to rotate an object on an unactuated hinge. Appendix D discusses additional experiments and limitations on other MuJoCo tasks.

### Skill Visualization

The first goal of this work, as exposed in the introductory example, is to obtain a set of skills that are as distinct from each other as possible in their visited states, while simultaneously covering as much of the state space as possible. We first assess this property in LEADS, DIAYN  and LSD  through a visual analysis of their state space coverage in Figure 2. For all experiments, the number of skills is \(n_{}=6\). To ensure fairness, for each algorithm, we report the skill visualization from the experiment that achieves maximum coverage (as defined in the following section) out of five runs. For each algorithm, Figure 2 presents four figures: the three mazes and the Fetch-Reach environment. In all the mazes, we visualize the 2D coordinates of the states reached over training. Given that 2D visualization is not suitable for the Fetch-Reach environment due to its higher state space dimension,

Figure 2: Skill visualisation for each algorithm. Per algorithm, the tasks are the mazes **Easy** (top left), **U** (top right), **Hard** (bottom left), and the control task **Fetch-Reach** (bottom right).

we project the state onto the two first eigenvectors of a Principal Component Analysis (PCA) of the states encountered by all skills in this environment.

We note that LEADS clearly defines distinct skills in the state space. Furthermore, it leads to a more extensive exploration of the environment than LSD and DIAYN. The Hard maze (bottom left) is noteworthy, as some parts of the environment are difficult to access due to bottleneck passages in the maze. LEADS is the only algorithm that manages to reach all sections of the Hard maze.

Figure 3 further demonstrates the ability of LEADS to create distinct skills in the high-dimensional state space of the Hand environment. It reports the state distribution obtained by \(n_{}=12\) skills found by LEADS, projected onto the two first components of a PCA on all visited states. As in the maze navigation tasks, LEADS explores states in distinct clusters, organized by skill. These skills correspond to a visual variety of finger positions, illustrating the ability to discover varied behaviors.

Section 3.2 claimed that the uncertainty measure \(u_{t}(s,z)\) of Equation 7, and its associated distribution \(\) triggered relevant exploratory behaviors. Figures 4(a) and 4(b), display the final SSR and uncertainty measure, respectively, for the Hard maze task. The same visual representation of these densities for the other environments can be found in Appendix B. Figure 4(a) demonstrates that the SSR clearly separates states based on skill. The SSR is zero for all states outside of the distribution of a given skill, and all states in a skill have a non-zero probability of being reached by that skill. This confirms that this SSR can reliably be used within the objective function of Equation 6.

Similarly, Figure 4(b) illustrates how the uncertainty measure concentrates on states that promote both exploration and diversity. High values for this measure are found in very different states for each skill (hence promoting diversity), and within a skill, the highest value is found far from the starting state \(s_{0}\) (hence promoting exploration). The state maximizing \(u_{t}\) is represented by a colored dot for each skill. This confirms that the uncertainty measure achieves the joint goal of exploring under-visited areas and creating repulsion between skills. By defining \((s|z)\) as the state which maximizes \(u_{t}(s,z)\) for each skill \(z\), LEADS ensures a continuing exploration of the state space.

### Quantitative Evaluation

We now turn to a quantitative evaluation of the ability to explore using diversified skills. For the sake of completeness, we compare LEADS to seven seminal algorithms. Five of these are skill-based, namely DIAYN , SMM , LSD , CSD , and METRA . We use \(n_{}=6\) for each skill-based algorithm and for all environments. The two last algorithms are pure-explorationones, which do not rely on skill diversity and rather rely on exploration bonuses: Random Network Distillation (RND)  and Never Give Up (NGU) .

A metric of state space coverage should characterize how widespread the state distribution is, in particular across behavior descriptors that are meaningful for the task at hand. Following the practice of previous work, for high-dimensional environments, we use a featurization of the state, retaining a low-dimensional representation of meaningful variables: the \((x,y,z)\) coordinates of the gripper in Fetch-Reach, the \((x,y)\) coordinates of the puck in Fetch-Slide, and the angles of the finger's two joints and the hinge's angle in Finger. This projection is then discretized and the coverage of a trajectory is defined as the number of visited cells. Figure 5 depicts the evolution of coverage, normalized by the maximum coverage achieved by any run in the current environment. This figure illustrates how coverage progresses relative to the number of samples taken in the environment since the beginning of the algorithm. Shaded areas represent the standard error across five seeds for each algorithm.

One can note that LEADS outperforms other methods across almost all tasks. This is especially notable in the Fetch-Slide environment, where LEADS exceeds the coverage of all other methods by over 20%. Furthermore, besides LEADS, no algorithm is consistently good across all tasks. In the Fetch-Reach task, DIAYN, METRA, and CSD each achieved a coverage between 80% and 85%. LEADS excelled with 90% coverage, significantly outperforming NGU at 50% and SMM and RND, both at 20%. CSD surpasses LEADS and all other methods on the Finger environment, achieving

   Method & Easy (\%) & Umaze (\%) & Hard (\%) & Fetch Reach (\%) & Finger (\%) & Fetch Slide (\%) \\  RND & \(76.6 7.3\) & \(39.6 5.4\) & \(30.8 4.3\) & \(17.6 2.7\) & \(21.3 1.6\) & \(21.6 3.2\) \\ DIAYN & \(81.4 8.6\) & \(55.0 8.2\) & \(43.8 4.5\) & \(85.6 8.7\) & \(53.8 12.3\) & \(52.3 8.5\) \\ SMM & \(100.0 0.0\) & \(70.6 3.7\) & \(53.4 0.5\) & \(22.3 5.5\) & \(31.2 1.4\) & \(53.3 3.4\) \\ NGU & \(86.8 8.4\) & \(73.4 6.0\) & \(57.2 8.3\) & \(53.4 4.5\) & \(76.4 5.6\) & \(57.8 4.5\) \\ LSD & \(99.8 0.4\) & \(79.8 4.5\) & \(70.0 0.6\) & \(71.9 5.2\) & \(69.8 8.8\) & \(61.5 5.2\) \\ CSD & \(97.4 3.4\) & \(79.8 5.4\) & \(64.0 6.2\) & \(83.2 0.5\) & \(96.2 9.1\) & \(63.0 2.8\) \\ METRA & \(92.8 4.2\) & \(78.0 5.3\) & \(54.8 9.5\) & \(82.5 1.5\) & \(83.4 7.5\) & \(50.7 2.2\) \\ LEADS & \(100.0 0.0\) & \(\) & \(\) & \(89.7 8.8\) & \(87.4 4.6\) & \(\) \\   

Table 1: Final coverage percentages for each method on each environments. Bold indicates when a single method is statistically superior to all other methods (\(p<0.05\)). Full T-test results are presented in Appendix B.

Figure 5: Relative coverage evolution across six tasks. The x-axis represents the number of samples collected since the algorithm began.

a relative mean coverage of 96% where LEADS is still competitive with 87%. On this specific benchmark, NGU, METRA and LSD are competitive with LEADS, whereas DIAYN, SMM and RND exhibit low coverage.

## 5 Conclusion

In this work, we consider the problem of exploration in reinforcement learning, via the search for behavioral diversity in a finite set of skills. We take inspiration from the classical framework of maximization of the mutual information between visited states and skill descriptors, and illustrate why it might be insufficient to promote exploration. This motivates the introduction of a new objective function for skill diversity and exploration. This objective exploits an uncertainty measure to encourage extensive state space coverage. We leverage off-the-shelf estimators of the successor state representation to estimate this non-stationary uncertainty measure, and introduce the LEADS algorithm.

Specifically, LEADS estimates the successor state representation for each skill, then specializes each skill towards under-visited states while keeping skills in distinct state distributions. This results in an efficient method for state space coverage via skill diversity search. Our experiments highlight the efficacy of this approach, achieving superior coverage in nearly all tested environments compared to state-of-the-art baselines.

LEADS intrinsically relies on good SSR estimators. In this work, we used C-Learning  for all experiments, which proved efficient but might reach generalization limits in some environments. As a consequence, advances on the question of reliable SSR estimation will directly benefit LEADS, opening new perspectives for research.