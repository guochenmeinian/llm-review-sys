# Theoretical and Practical Perspectives on what Influence Functions Do

Andrea Schioppa\({}^{1}\) Katja Filippova\({}^{1}\) Ivan Titov\({}^{2,3}\) Polina Zablotskaia\({}^{1}\)

\({}^{1}\)Google DeepMind \({}^{2}\)University of Edinburgh \({}^{3}\)University of Amsterdam

{arischioppa, katjaf, polinaz}@google.com, ititov@inf.ed.ac.uk

###### Abstract

Influence functions (IF) have been seen as a technique for explaining model predictions through the lens of the training data. Their utility is assumed to be in identifying training examples "responsible" for a prediction so that, for example, correcting a prediction is possible by intervening on those examples (removing or editing them) and retraining the model. However, recent empirical studies have shown that the existing methods of estimating IF predict the leave-one-out-and-retrain effect poorly. In order to understand the mismatch between the theoretical promise and the practical results, we analyse five assumptions made by IF methods which are problematic for modern-scale deep neural networks and which concern convexity, numeric stability, training trajectory and parameter divergence. This allows us to clarify what can be expected theoretically from IF. We show that while most assumptions can be addressed successfully, the parameter divergence poses a clear limitation on the predictive power of IF: influence fades over training time even with deterministic training. We illustrate this theoretical result with BERT and ResNet models. Another conclusion from the theoretical analysis is that IF are still useful for model debugging and correcting even though some of the assumptions made in prior work do not hold: using natural language processing and computer vision tasks, we verify that mis-predictions can be successfully corrected by taking only a few fine-tuning steps on influential examples.

## 1 Introduction and related work

Influence Functions (IF)  have been regarded as a tool that can trace model behavior on any example to the training examples . Their theoretical justification lies in the ability to predict loss changes on a specific test point when training on a perturbed loss obtained by removing or down-sampling a given training point. In the case of an undesired model behavior on a test-point, the influential training examples for that test point have been assumed to be the ones "responsible" for the prediction so that intervening on those - e.g., by removing them and then _retraining_ the model, or by taking additional fine-tuning steps  - would result in a change in the loss or prediction. It has been confirmed that for linear models it is indeed the case .

Recently, in their extensive experiments,  and  could not find empirical support for the claim that IF approximate the Leave-Some-Out Retraining (LSOR) effect on the loss in deep neural networks. In particular, they show that the correlation between the ranking of training examples produced by LSOR and the IF-based ranking is low and considerably affected by choice of hyperparameters. How can this discrepancy be explained? And, given that the theoretical justification for IF lacks empirical support, does it mean that IF should be abandoned as an explainability and debugging tool altogether?

In this work we clarify what question Influence Functions (IF) actually answer. We first identify assumptions which are either implicit or not investigated in prior work: these concern convexity,numeric stability, training trajectory, and the the parameter divergence when retraining on a new loss. _In principle, any of these assumptions might be problematic and be the reason why the original theoretical justification for IF is not supported empirically_. However, we show how to address most of them successfully so that they cannot be the reason for the aforementioned discrepancy. Unfortunately, we also show that the parameter divergence is indeed problematic and requires to revise both theoretical and practical expectations about IF. This analysis paves the way to clarifying the question that IF can answer. As a first step in this direction, we need to distinguish two approaches to computing influence. The _Hessian-based Influence Functions (HIF)_ have a rigorous theoretical justification in statistics, but rely on strict convexity assumptions, which are not met in Deep Learning and which previous analyses of HIF in the Deep Learning literature still rely on . Our first contribution concerns HIF and is twofold: we prove (Theorem 1) that this unsatisfied convexity assumption is not as problematic for HIF as one may think-given a stationary point for the original loss, there is a nearby stationary point for the perturbed loss which can be approximated by using HIF. However, we point out a more serious problem with HIF: _there is no guarantee that by retraining on the perturbed loss one would get to that stationary point_. This observation provides an additional support to the second popular approach to IF, TracIn , which explicitly models the training dynamics and which additionally does not require to compute the expensive inverse Hessian vector product as it only uses gradient information.1 Despite TracIn being grounded in the training dynamics, we unveil a hidden additive modeling assumption in  that _prevents it from correctly modeling the (re)-training dynamics_. Our second contribution is thus to provide a theoretical analysis (Theorem 2) of how training trajectories change when perturbing the loss. This is a key result as it suggests that two training trajectories differing by a small loss perturbation _could diverge over time to the point of violating a first-order expansion assumption that IF make_, thus making IF's predictions unreliable. Our further theoretical and empirical investigation confirms this conjecture. Therefore, _what IF can do is to predict parameter changes when fine-tuning for a limited number of steps on the perturbed loss. This requires to adjust how IF are evaluated (Section 5) and applied (Section 6)_. In order to confirm the conjecture and re-adjust expectations regarding IF, we need a deeper analysis of the trajectory divergence.

Our third contribution is thus to validate the conjecture on trajectory divergence and its consequences for IF. We first prove (Theorem 3), using a discrete version of Gronwall's Lemma , an upper bound on the parameter divergence when (re)-training on a perturbed loss. We then empirically verify that the bound is sharp in Section 5 and that IF can approximate parameter changes along the perturbed trajectory _only for a limited amount of time_. We then empirically verify that this leads to a _fading (of accuracy)_ of IF predictions over time. Therefore, we theoretically demonstrate and empirically verify that IF can in general answer only what happens _when fine-tuning on a perturbed loss for a limited amount of time_, instead of a general retraining setting.

Therefore, our new theory indicates that IF have been used incorrectly as the emphasis has been on their LSOR potential. On the positive side, it suggests an alternative way of using IF that indeed yields substantial empirical improvement. To demonstrate that, as our final contribution, in Section 6 we propose and verify a very simple method for correcting mis-predictions by taking only a few gradient steps on influential examples. Our proposal is related to model editing , inter alia] in that the latter also aims at changing model predictions. However, there the model is given and the modifications are done on its parameters whereas IF aim at understanding how specific training examples are responsible for the current model behavior and editing predictions through the data. We leave for future work building connections between model editing and IF.

For the case of Hessian-based influence functions, the recent work of  has proposed to resolve the discrepancy between the theory and the empirical evidence by evaluating them using the proximal Bregman response function. However,  relies crucially on the convexity assumption: while under this assumption there is agreement between our work and their findings (e.g. our parameter divergence drives the linearization error [14, Sec. 4.4]), our theory applies also the the case where the Hessian is not positive definite. Moreover, our work covers also the scope of Gradient-based influence functions including TracIn .

Definition of Influence Functions and Notation

The different methods proposed to define Influence Functions share a common goal: _forecasting the change in the prediction on a test example when up (or down-) weighting a training example_. This is achieved by tracing the effect that re-weighting a training example has on the model parameters.

Removing or adding a training point can be modeled by a _perturbation_ of the loss function; such a perturbation can be made smooth by modeling the weighting of a training point by a continuous parameter \(\). More generally, let \(L()\) denote the loss function where \(^{N}\) are the model parameters. We model loss perturbations by introducing _a variation of the loss_, which is a smooth function \((,)\) depending on an additional vector parameter \(^{Q}\) and coinciding with the original loss for \(=0\). For example if \(l_{x}\) denotes the loss on a given training point \(x\), we set \((,)=L()+ l_{x}\). While the scalar case \(Q=1\) is the one commonly considered, e.g. , _we introduce the vector one_\(Q>1\) which arises naturally when considering the effect of re-weighting multiple points differently, e.g. when modifying the weights in a mixture of different data-sets.

The IF method of choice then predicts _what would happen if training on \((,)\) instead of \(L()\)_. The final parameters are modeled as a function \(_{}\) of the perturbation parameter; _assuming that such a function is well-defined and sufficiently smooth_, one then makes a first order expansion \(_{}_{0}+^{T}_{|0} _{}\), where \(_{|0}_{}\) is the \(Q N\)-dimensional Jacobian at \(=0\) (see also Section A).

Under this first order assumption it is then straightforward to measure the change in the loss \(l_{z}\) corresponding to a test point \(z\):

\[l_{z}(_{})-l_{z}(_{0})^{T}_{ |0}_{}_{|_{0}}l_{z}.\] (1)

We emphasize that (1) is _general to different IF methods_, which _differ in the specific derivation_ of \(_{|0}_{}\).

## 3 Problematic assumptions made by Influence Functions

Problematic Assumption #1: Convexity can be used to show that \(_{}\) is a function of \(\)

In order to show that for each value of \(\) there is a single value of \(_{}\) (so that one can model the final parameters as a function of the perturbation parameter), HIF relies on strict convexity of \(L\). While this assumption is realistic for the statistical models considered in , this is _not the case for neural networks_. Even when introducing a regularization term, the loss of a neural network is not even weakly convex, and optimization methods usually converge to saddle points . To the best of our knowledge, previous analyses of HIF in the Machine Learning literature, e.g. , _have relied on some form of strict convexity_. In Section 4.1 we will revisit HIF and prove (Theorem 1), roughly speaking, that near a given stationary point \(_{0}\) for the original loss \(L\), there is a stationary point \(_{}\) of the perturbed loss \((,)\) which can be modeled as a function of \(\) and such that \(_{|0}_{}\) is given by \(-H_{_{0}}^{-1}_{(,)|(0,_{0})}^{2}\) as in  (see Section A regarding the usage of \(^{2}\)).

### Problematic Assumption #2: The model Hessian is not degenerate

As HIF requires to apply the inverse model Hessian to \(_{(,)|(0,_{0})}^{2}\), one needs to _ensure that inverting the Hessian is numerically stable_. It has been empirically demonstrated  that most eigenvalues of the Hessian tend to cluster near \(0\). Numerically, this results in a considerable source of errors and instabilities when estimating HIF; while regularization can alleviate this problem, it introduces a hyper-parameter in the definition of HIF; the minimal value of such a hyper-parameter ensuring numerical stability depends on the _smallest negative eigenvalue of the Hessian_. Unfortunately, in realistic settings, this can be _larger in absolute value than reasonable values for the regularization parameter_. For example in our ResNet experiments regularization is of the order \(10^{-4}\), while the smallest negative eigenvalue is \(-5\). In Section 4.2 we discuss how the Arnoldi-based Influence Functions (abbr. ABIF) (which were introduced by  for computational efficiency) can be used to address such instability issues.

### Problematic Assumption #3: Training trajectory can be ignored in Hessian-based Influence

Even if we solve the Problematic Assumption #1 for HIF, there is no guarantee that when actually re-training from scratch on \((,)\) one would converge to the \(_{}\) given by Theorem 1 because the training trajectory is disregarded in HIF. As optimization is performed via some form of stochastic gradient descent,  propose _TracIn_ which averages gradient dot-products across checkpoints in order to take into account the path taken by the training process. Importantly, for a single checkpoint \(_{0}\) TracIn estimates \(_{}_{}\) as \(-^{2}_{(,)|(0,_{0})}\), so the inverse Hessian vector product does not need to be computed. While it seems that TracIn takes into account the training trajectory, in the next Assumption we identify an issue with the way it models the training trajectory.

### Problematic Assumption #4: The training trajectory can be modelled additively

The analysis of TracIn in  is based on a first-order expansion of the final change of the loss of a test point in terms of the gradient steps across the training trajectory. While this argument seems mathematically convincing, it overlooks that if one point is removed, or slightly up-sampled / down-sampled, _the subsequent training trajectory is modified_. Let \(_{,t}\) denote the value of the parameters after \(t\) steps when doing gradient descent on \((,)\). Denoting by \(T\) the end-time, TracIn derives

\[_{|0}_{,T}=-_{t=0}^{T-1} _{t}^{2}_{(,)|(0,_{0,t})},\] (2)

where \(_{t}\) is the learning rate at time step \(t\). Now, the right-hand side in formula (2) is purely additive in the time steps; and addition is commutative, so _the order of the time steps does not matter_. One way to see that this is problematic is by making \(\) time-dependent so that it differs from \(L\) only at a specific time step \(t\). In this case \(^{2}_{(,)}\) would be non-zero only at \(t\) and formula (2) would consist of a single term. However, we would expect the perturbation at \(t\)_to affect the following time steps_, so we should have at least \(T-t-1\) terms on the RHS for (2). In Section 4.3 we compute \(_{}_{,T}\) looking at the whole training trajectory and _discover an additional first-order term that is missing from TracIn_: _this term models the dependency of a time step on the earlier ones_. Concurrent work  also criticizes the additive assumption in TracIn on empirical grounds and proposes to build (re)-training simulators which are unfortunately computationally expensive as a new simulator must be fitted on the training set for each test point.

Problematic Assumption #5: \(_{}\) can be expanded to first order in \(\)

After we derive a formula for \(_{,T}\) and \(_{}_{,T}\) we realize that the latter can grow in norm in \(T\). However, if \(_{,T}\) can be Taylor-expanded in \(\), we _need \(_{,T}-_{0,T}\) to be \(O()\) and \(_{}_{,T}\) to be \(O(1)\)_. If this is not the case, the whole IF approach described in Section 2 breaks down because IF approximate the parameter change \(_{,T}-_{0,T}\) using the Taylor expansion \(^{T}_{}_{,T}\), but _the conditions to apply such a Taylor expansion are not satisfied_.

In Section 4 we show how assumptions #1-#4 can be successfully addressed which makes them not as problematic as they may first appear. However, for assumption #5 we will see that it puts a substantial limitation on the predictive power of IF. At the same time it allows for a new, locally bound perspective on IF - that influence holds for a limited number of fine-tuning steps (Section 5). Based on this finding, in Section 6 we propose a simple approach to use IF to correct mis-predictions which is theoretically grounded and is in addition much less compute intensive than those that involve re-training (e.g. ).

## 4 Addressing the problematic assumptions

_Proofs of all results are in the Appendix and we provide some motivation for the proof in the main text._

### HIF does not need Assumption #1

Previous work  on Hessian-based Influence Functions (HIF) has assumed that \(L\)_is strictly convex_ in order to claim that 1) _the minimum is unique_ so that \(_{}\) can be modeled as a function, and 2) to use the Implicit Function Theorem to differentiate through the optimality condition.

Here we _will just assume that the Hessian is not singular at \(_{0}\)_; by requiring that the final gradients do not change as we change \(\) we prove:

**Theorem 1**.: _Assume that \(\) is \(C^{k}\) (\(k 1\)) and let \(_{0}^{N}\); assume that the Hessian \(H_{_{0}}=^{2}_{|_{0}}L\) is non-singular; then there exist neighborhoods \(U\) of \(_{0}\) and \(V\) of \(0^{Q}\), and a \(C^{k}\)-function \(:V U\) such that \((0)=_{0}\) and \(() U\) is the unique solution in \(U\) of the equation_

\[_{}((),)=_{} (_{0},0).\] (3)

_Moreover, the gradient of \(\) at the origin is given by:_

\[_{|0}=-H_{_{0}}^{-1}^{2}_{(, )|(0,_{0})}.\] (4)

Idea of the proof.: Existence of \(()\) is formulated as a solution to the local problem eq. (3); building a local solution does not require convexity if one uses the Implicit Function Theorem. 

The requirement that \(_{}L((),)\) is constant in \(\) has allowed us to establish a link between the training under different losses \(\{(,)\}_{}\). If we assume that \(_{0}\) is a _stationary point_, i.e. \(_{}L(_{0})=0\), we can strengthen the conclusions:

**Corollary 1**.: _Under the assumptions of Theorem 1:_

1. _If_ \(_{0}\) _is a stationary point of_ \(L\)_, then each_ \(()\) _is a stationary point of the loss_ \((,)\)_._
2. _If_ \(_{0}\) _is a local (strict) minimum of_ \(L\)_, for_ \(\) _sufficiently small,_ \(()\) _is a local (strict) minimum of the loss_ \((,)\)_._
3. _Let_ \(Q=1\) _(hence epsilon is a scalar) with_ \((,)=L()+ l_{x}()\)_, where_ \(l_{x}\) _is the loss corresponding to a specific training point_ \(x\)_. We then obtain the classical result_ _[_13_]__:_ \[_{=0}=-(^{2}_{}L( _{0}))^{-1}_{}l_{x}(_{0}).\] (5)

### If Assumption #2 is not satisfied, use Arnoldi-based Influence Functions

Theorem 1 requires that \(H_{_{0}}\) is non-singular. If the Hessian is singular, _we just need to keep fixed those parameters that are responsible for the degeneracy_. More precisely, we diagonalize \(H_{_{0}}\); we let \(P_{1}\) be the subspace spanned by the eigenvectors corresponding to the non-zero eigenvalues and let \(P_{0}\) be its orthogonal complement, that is, the kernel of \(H_{_{0}}\). Up to an orthogonal transformation of the parameters, we can assume that \(P_{1}\) is spanned by the first \(N_{1}\)-coordinates and decompose \(=(,)^{N1}^{N2}\) so that \(H_{_{0}}=^{2}_{}L((_{0},_{0}))\) is non-singular. We then apply Theorem 1 to the restricted variation \((,) L((,_{0}),)\). In terms of the original parameters \(\), this means that the function \(()\) is constrained to lie in \(_{0}+P_{1}\), keeping the \(\)-component constantly equal to \(_{0}\). Concretely, we can approximate \(P_{1}\) using the Arnoldi iteration; therefore, _we can address the failure of Assumption #2 by using Arnoldi-based Influence Functions_ (ABIF) , which approximate \(P_{1}\) using the subspace spanned by the eigenvectors corresponding to the top-k (in absolute value) eigenvalues of the Hessian. The fact that ABIF stabilizes the estimation of influence scores has been observed empirically in previous work: in [17, Fig. 2] where using ABIF instead of the full Hessian improves the retrieval of mislabeled examples, and in [15, Fig. 3] where ABIF is the best solver on the QA task and is comparable to the other solvers on the text-completion task.

### The training trajectory can be traced to address Assumptions #3-#4

We need to improve our notation to correctly trace the training trajectory. The first issue is to keep track of the parameters across the time steps; the second issue are _sources of non-determinism_,e.g. batch selection or random state for dropout. When comparing training trajectories for different values of \(\) we want our notation to account for sources of non-determinism, as they might increase the difference between the training trajectories.

To address the first issue, we let \(_{,t}\) be the value of the parameters after training on \((,)\) for \(t\) steps. In particular, \(_{,0}\)_denotes_ the initial value condition that we assume held fixed at \(_{}\) for different values of \(\). As random state is a function of the training step (e.g. the batch to use at step \(t\)), to address the second issue, we just need to allow both the loss and the variation to depend on the train step, denoting them by \(L_{t}\) and \(_{t}\).

To simplify the exposition and for consistency with  we assume that models are trained with stochastic gradient descent. Letting \(_{t}\) be the learning rate at step \(t\) we prove:

**Theorem 2**.: _Assume that the model is trained for \(T\) time-steps with stochastic gradient descent. Denoting by \(H_{t}\) the Hessian \(^{2}_{|_{0,t}}L_{t}\), then the final parameters \(_{,T}\) satisfy:_

\[_{|0}_{,T}=-_{t=0}^{T-1}_{t}^ {2}_{(,)|(0,_{0,t})}_{t}-_{t=0}^{T-1} _{t}H_{t}_{|0}_{,t}.\] (6)

_Idea of the proof._ One can explicitly write a system of equations for \(_{,T}\) and apply the operator \(_{|0}\) to the solution of this system. 

Note that the second term on the RHS of (6), which is missing from (2), takes into account the _contribution of the earlier time steps_ that is missing from the analysis of . A practical consequence of this second term is that the norm of \(_{|0}_{,T}\) might grow (in \(T\)) more quickly than (2) would suggest: this is closely related to Assumption #5 which requires \(_{|0}_{,T}\) to be \(O(1)\). In particular, for a constant learning rate, while (2) suggests a linear growth in the time step \(T\), we will empirically verify in Section 5.1 that the growth is super-linear.

In Section K we illustrate the additional modeling error introduced by TracIn when computing the Jacobian \(_{|0}_{,T}\) in the case of retraining BERT with SGD.

### Assumption #5 becomes problematic over time

To address Assumption #5 we need to look into the _parameter divergence_\(\|_{,T}-_{0,T}\|\) between training on \(L\) and \(\). The training dynamics is a discrete version of an ODE for which uniqueness and differentiability of the solutions with respects to the initial conditions can be established by Gronwall's Lemma . The parameter \(\) can itself be considered an initial condition and we can prove a discrete version of Gronwall's Lemma to bound the parameter divergence. For simplicity of notation we prove the result for stochastic gradient descent, but we also sketch in the Appendix how to modify the argument to deal with optimizers.

**Theorem 3**.: _In the setting of Theorem 2 assume that, for \(t T\), \(_{,t}\) lies in a bounded region \(R\) such that_

\[_{t, R}\|_{t}(,)- (,0)\| C\] (7)

_and that for \( R\) each loss \(_{t}(,)\) and its gradient wrt. \(\) are \(A\)-Lipschitz in \(\). Then_

\[\|_{,T}-_{0,T}\| C_{s<T}_{s}(1+ (2A_{s<T}_{s})).\] (8)

_Idea of the proof._ If the time steps were infinitesimal, i.e. if time was made continuous, the evolution of \(_{,t}\) would be governed by an ODE; then the bound (8) would be straightforward by applying the classical Gronwall's Lemma . In our case we just need to modify the ODE arguments to work with discrete time. 

Note that the bound (8) is quite pessimistic as it involves an _exponential of the integrated learning rate_\(_{s<T}_{s}\). This means that as \(_{s<T}_{s}\) increases, the parameter divergence is no longer \(O()\) and the crucial Assumption #5 is no longer satisfied. This observation leads to a few crucial conclusions:

1. An IF method can predict \(_{,t}\) only for a limited amount of time-steps: it is therefore incorrect to evaluate IF methods on LSOR or retraining from scratch.
2. IF methods need to be evaluated on what they can potentially do; therefore the evaluation setup should consist of fine-tuning on the perturbed loss only a limited amount of steps with evaluation metrics being reported as a function of the step.
3. Applying IF for correcting mis-predictions should also involve a time-bound scenario: we propose such a method in Section 6.
4. Sources of non-determinism between two training runs will likely increase the parameter divergence. So one should try to reduce this with _deterministic training_. For example, in the case of re-weighting a point \(x\), i.e. setting \(=L+ l_{x}\), one should make sure to use the same batch \(B_{t}\) for the loss \(L\) at time step \(t\) across training runs for different values of \(\).

## 5 Illustrating the Theory

In this section we first demonstrate Theorem 3 empirically and then verify that the predictive power of influence scores degrades over time. Full details of our experimental setup are reported in the Appendix. We consider binary classification for nlp, where we fine-tune BERT on SST2; for computer vision we consider multi-class classification where we train from scratch ResNet on CIFAR10. _All our experiments use deterministic training_: the order of the training batches for the loss \(L\) is held fixed across different runs, as well are the random generators when dropout is used.

### Illustrating Parameter Divergence (Theorem 3)

Theorem 3 provides an upper bound when re-training on a perturbed loss. Such a bound is rather pessimistic as it involves the integrated learning rate. We therefore investigate empirically what happens with some typical Deep Learning setups. We take an intermediate checkpoint and keep training on a new loss obtained by up-sampling 16 training points with a weight \(\), that is: \(_{t}=L_{B_{t}}+ L_{B}\), where \(B_{t}\) is the training batch for step \(t\) and \(B\) is the batch of 16 points selected for up-sampling. For each time step \(t\) we then compute \(\|_{,T}-_{0,T}\|\) and then plot it against the integrated learning rate, see Figure 1.

Unfortunately, we observe that in these experiments _the upper bound in Theorem 3 is matched by a lower bound with the same exponential divergence_. We observe a first phase of quick divergence and then a second one in which the divergence is slower. For the second phase a linear fit of \(\|_{,T}-_{0,T}\|\) against the integrated learning rate appears to be strong: for example for BERT we obtain an \(R^{2}\) of at least \(0.9\) across the different values of \(\). The fitted slope, corresponding to \(A\) in Theorem 3 depends on \(\) and varies between \(30\) and \(130\).

Figure 1: Divergence of parameters (log-scale) as a function of the integrated learning rate. For each value of \(\) the divergence is exponential (corresponding to a line in log-scale) with two different divergence rates, one more steep at the beginning of (re)-training. (a) BERT, (b) ResNet

### Illustrating the fading of influence

We now verify that the predictive power of influence scores fades over time. We again fix a model checkpoint \(_{}\) and select 32 training points and 16 test points. For each training point \(x\) we retrain on \(_{t}^{x}=L_{B_{t}}-l_{x}\), i.e. \(x\) has been down-sampled; for each test point \(z\) and time step \(t\) we then compute the loss difference \((z,x,t)=l_{z,x,t}-l_{z,t}\) where \(l_{z,x,t}\) is obtained when (re)-training on \(_{t}^{x}\) and \(l_{z,t}\) is obtained when training on the vanilla loss \(L_{t}=L_{B_{t}}\). Again, we have kept the order of the batches \(B_{t}\) the same when re-training. Now, at the original checkpoint \(_{}\) we can compute the influence scores \(IF(z,x)\) for different methods, e.g. TracIn or HIF (using the the Conjugate Residual method2). For each time step we thus have \(32 16\) values of \((z,x,t)\) that can be linearly regressed against \(IF(z,x)\); _the corresponding Pearson correlation \(R(t)\) then measures the predictive power of influence scores on the loss shifts when re-training_. We repeat the experiments for ResNet \(9\) times and for BERT \(25\) times, with a different selection of train and test points, so that we obtain confidence intervals for the resulting time-series \(R(t)\). Ideally, the theory behind an influence method predicts \(R(t) 1\). However, as discussed above, Assumption #5 is indeed problematic and, because of the parameter divergence illustrated in 5.1 we expect \(R(t)\) to degrade over time.

As Figure 2 demonstrates, this is indeed the case. The predictive power is high for BERT after a few re-training steps and then degrades quickly oscillating around 0 (which is covered by the confidence intervals). For ResNet, the predictive power is never as high, _but it still degrades monotonically over time, as predicted by the theory_. As we see in Section 6, the proponents retrieved for ResNet are less effective at correcting mis-predictions than those retrieved for BERT: we conjecture that this is related to the worse predictive power of IF in the case of ResNet. As BERT was trained with the Adam Optimizer, we also considered a variant which takes into account the optimizer's pre-conditioner by multiplying the gradients by the square root of the pre-conditioning matrix. In this case the predictive power is worse than for the vanilla version of TracIn. For BERT, more plots and a further discussion about TracIn are included in the Appendix (Section G.1). In the Appendix (Section G.2), we also illustrate the fading of influence for another NLP pre-trained model, T5. While we have illustrated the fading of influence for both BERT and ResNet, in the latter case the peak of correlation is quite low; we repeated the same experiments on the ViT (Section G.3), where we find a high correlation peak as in the case of BERT.

## 6 Using Influential Examples for Error Correction

 propose to correct model mis-predictions by first using influence scores to retrieve the examples most responsible for a given prediction, and, after correcting them, _retraining the model_. Computational considerations aside, a key conclusion from the theory in Section 3 and the empirical verification in Section 5.2 is that IF only predict influence over a limited number of training steps. In this section we demonstrate that IF can still be used to correct model mis-predictions by _taking a few fine-tuning steps on influential examples_.

Figure 2: The predictive power of influence scores on the loss shifts degrades over time. (a) BERT, (b) ResNet. The line represents the average of the correlation \(R(t)\) across runs, with the shaded area the corresponding \(95\%\) confidence region.

Concretely, we propose to correct mis-predictions at a given test point \(z\) by first identifying a batch of influential examples \(B\) and then taking a few fine-tuning steps on the perturbed loss \(=L+ l_{B}\). We propose two methods: (1) _Proponents-correction_: we identify the set \(B\) of top-k proponents for the current \(x\) and _relabel_ them to what should be the correct prediction on \(x\); (2) _Opponents-tuning_: as opponents _oppose the current prediction_, we take \(B\) to be the set of top-k opponents of \(x\).

We investigate how well these error-correction techniques work on SST2 (BERT) and CIFAR10 (ResNet). As a baseline, we randomly sample a set \(B\) of training points with the same label as the prediction on \(x\) and then set their label equal to the correct one for \(x\). We take a maximum of \(50\) fine-tuning steps and take the top-50 proponents or opponents to build \(B\). The main metric we compute is the _success rate_, i.e. the ratio of mis-predictions successfully corrected within the limit of \(50\) steps. Additionally, we report _prediction retention_[DCAT21] on a fixed held-out set of \(50\) test examples, i.e. the ratio of examples predictions which have not changed after a correction - ideally, a correction does not cause too many changes in model predictions otherwise. We experiment with different values of \(\), starting from no up-sampling and gradually increasing it to when influential examples account for slightly more than half of the batch.

From Figure 3 we see that Proponents-correction and Opponents-tuning strongly outperform the baseline in binary classification (SST2). For multi-class classification (CIFAR10) Opponents-tuning is not effective, as we verified that only 54% of the retrieved opponents have the desired label; Proponents-correction still outperforms the baseline increasing on average the success rate by 2% and reducing the number of steps to take by 6%. We conjecture that for ResNet the improvement over the baseline is less than for BERT because of the worse predictive power of IF (Figure 2 (b)). In Appendix H we include additional plots showing the number of steps to correct mis-predictions as a function of the up-sampling parameter \(\).

The primary goal of the experiments in this section is to verify that tuning on influential examples results in a correction more reliably and faster than on other classes of examples. Since the SST2 and CIFAR10 datasets are largely clean, for the proponents method, the training example label is flipped to an incorrect one. In Appendix I we give examples from a noisy text classification dataset illustrating the scenario that the correction-with-proponents method is supposed to address.

## 7 Limitations

We derive the perturbed training trajectory (Theorem 2) and the divergence of trajectories (Theorem 3) for stochastic gradient descent and, while we sketch in the Appendix the modifications needed when using other optimizers, we do not pursue this topic in detail. In Section 6 we measure prediction retention after correcting mis-predictions. While this metric is intuitive and has been used previously (e.g., [DCAT21]), we do not distinguish between semantically similar and unrelated examples and thus do not check the consistency and generalization properties of the update [MBAB22], leaving a thorough study of the correction-retention tradeoff to future work.

Figure 3: Success rate and retention for error correction. (a) BERT, (b) ResNet

Conclusions

IF have been regarded as a tool that promises to trace model behavior to the training data. Unfortunately, recent studies have found no empirical support for such a claim as IF fail to predict the LSOR effect. This finding gives rise to the question of what IF methods really predict and whether they could be useful for model debugging. In this work we clarified which questions IF can be expected to answer. We first identified problematic assumptions made by IF methods - a priori any of these assumptions could be a reason for the observed empirical failure of IF. Thus, for each assumption we studied if it is indeed problematic. While most have turned out to be addressable in one way or another, we demonstrated that the one about parameter divergence puts a severe limitation on IF. With a deeper analysis of this assumption, we revised what can be theoretically expected from IF: IF methods are time-bound, that is, they can at most predict what happens when fine-tuning on a perturbed loss for a limited amount of time. With that, a practical usage of IF for model debugging is still possible - we proposed and empirically validated a theoretically-grounded procedure to apply IF to correct model mis-predictions.