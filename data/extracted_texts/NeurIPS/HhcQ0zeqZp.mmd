# Benchmarking Large Language Models on CMExam - A Comprehensive Chinese Medical Exam Dataset

Junling Liu\({}^{1}\)\({}^{*}\) Peilin Zhou\({}^{2}\)\({}^{}\) Yining Hua\({}^{3,4}\)\({}^{}\) Dading Chong\({}^{5}\)

Zhongyu Tian\({}^{6}\) Andrew Liu\({}^{5}\) Helin Wang\({}^{7}\) Chenyu You\({}^{8}\)

Zhenhua Guo\({}^{9}\) Lei Zhu\({}^{10}\) Michael Lingzhi Li\({}^{4,1}\)\({}^{1}\)

\({}^{1}\)Alibaba Group \({}^{2}\)Hong Kong University of Science and Technology (Guangzhou)

\({}^{3}\)Harvard University \({}^{4}\)Boston Children's Hospital \({}^{5}\)Peking University

\({}^{6}\)Second Affiliated Hospital of Zhejiang University School of Medicine \({}^{7}\)Johns Hopkins University

\({}^{8}\)Yale University \({}^{9}\)Tianyi Traffic Technology \({}^{10}\)Ant Group \({}^{11}\)Harvard Business School

{william.liuj, zhoupalin, andrew.promed, cszguo, zhulei0305}@gmail.com

1601213984@pku.edu.cn, zhongyutian@zju.edu.cn, hwang258@jhu.edu

yininghua@g.harvard.edu, chenyu.you@yale.edu, mili@hbs.edu

###### Abstract

Recent advancements in large language models (LLMs) have transformed the field of question answering (QA). However, evaluating LLMs in the medical field is challenging due to the lack of standardized and comprehensive datasets. To address this gap, we introduce **CMExam**, sourced from the **C**hinese **N**ational **M**edical **L**icensing **E**x**amination. **CMExam consists of 60K+ multiple-choice questions for standardized and objective evaluations, as well as solution explanations for model reasoning evaluation in an open-ended manner. For in-depth analyses of LLMs, we invited medical professionals to label five additional question-wise annotations, including _disease groups_, _clinical departments_, _medical disciplines_, _areas of competency_, and _question difficulty levels_. Alongside the dataset, we further conducted thorough experiments with representative LLMs and QA algorithms on CMExam. The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could generate relevant reasoning and demonstrate improved performance after finetuning, they fall short of a desired standard, indicating ample room for improvement. To the best of our knowledge, CMExam is the first Chinese medical exam dataset to provide comprehensive medical annotations. The experiments and findings of LLM evaluation also provide valuable insights into the challenges and potential solutions in developing Chinese medical QA systems and LLM evaluation pipelines.1

## 1 Introduction

Recent advancements brought by large language models (LLMs) such as T5 (Raffel et al., 2020) and GPT-4 (OpenAI, 2023) have revolutionized natural language processing (NLP). However, evaluating LLMs in the medical field poses significant challenges due to the paucity of standardized and comprehensive datasets compiled from reliable and unbiased sources (Li et al., 2023; Zhou et al., 2023; Hua et al., 2024; Ye et al., 2023; Liu et al., 2023). Most existing medical datasets (Hendrycks et al., 2020; Abacha et al., 2019; Li et al., 2023; Zhou et al., 2022) for language model evaluationhave limitations that hinder comprehensive assessment of LLM performance (Nori et al., 2023). Many datasets are insufficient in terms of size and diversity, preventing a thorough evaluation of LLM capabilities. Furthermore, most datasets primarily focus on text generation tasks rather than utilizing clear choice evaluations, impeding objective and quantitative measurement of LLM performance. Additionally, a majority of these datasets (Li et al., 2023; Pal et al., 2022; Zhu et al., 2020) are sourced from online forums and consumer feedback, which could suffer from significant bias and error. These challenges are particularly amplified in non-English languages, such as Chinese, due to the pervasive inequality in language resources that exists in the NLP field (Bird, 2020; Zeng et al., 2022; Fang et al., 2023). Overall, due to the lack of qualified evaluation datasets, the strengths and weaknesses of LLMs in the medical field have not been fully studied.

In response, we present a novel dataset called CMExam to overcome these challenges and benchmark LLM performance. CMExam is sourced from authentic medical licensing exams. It contains more than 60K questions and utilizes the multiple-choice question format to allow standardized and objective evaluations. Questions in CMExam have corresponding solution explanations that can be used to test LLM's reasoning ability in an open-ended manner. To offer diverse perspectives for measuring LLM performance in the medical field, we created five additional question-wise annotation dimensions based on authenticated resources and objective metrics. To reduce the substantial time and labor costs associated with annotating large-scale datasets, we propose an innovative strategy called GPT-Assisted Annotation. This approach harnessed the power of GPT-4 to automate the initial annotation process. Subsequently, the annotated data underwent a meticulous review and manual verification conducted by two medical professionals. Figure 1 shows an example question from CMExam and the annotation process.

Furthermore, we benchmark the performance of general domain LLMs and medical domain LLMs on answer prediction (multiple-choice) and answer reasoning (open-ended) tasks of CMExam. This comprehensive assessment aims to highlight the strengths and weaknesses of various approaches in Chinese medical QA, with a focus on LLMs. The main findings of this benchmark are as follows:

* GPT-4 (OpenAI, 2023) demonstrates impressive zero-shot performance on the answer prediction task compared to other models, though still significantly lagging behind human performance.
* GPT-3.5 (Brown et al., 2020) and GPT-4 generated reasonable answers on the answer reasoning task despite low BLEU and ROUGE scores. This is because they tended to generate short answers with reasonable quality.
* Existing medical domain LLMs, such as Huatuo (Li et al., 2023) and DoctorGLM (Xiong et al., 2023), exhibit poor zero-shot performance on both tasks, indicating their limited coverage of medical knowledge and substantial room for improvement.
* Lightweight LLMs (e.g., ChatGLM (Du et al., 2022)) fine-tuned on CMExam with supervision achieve performance close to GPT-3.5 on the answer prediction task. They also significantly outperform GPT-3.5 and GPT-4 on the reasoning task while having only 3% of the parameters of GPT-3.5.

In summary, this study provides valuable insights into the performance of LLMs in medical contexts from multiple perspectives, benefiting both the artificial intelligence research community and the medical research community. Our findings contribute to a deeper understanding of the capabilities and limitations of LLMs in the medical domain. Additionally, the CMExam dataset and benchmark introduced in this study serve as valuable resources to inspire researchers to explore more effective

Figure 1: An example question of CMExam. Abbreviations: Circulatory System Diseases (Circ), Internal Medicine (IM), Clinical Medicine (ClinMed), Medical Fundamentals (MedFund).

ways of integrating medical knowledge into LLMs, ultimately enhancing their performance in medical applications.

## 2 Related Work

Medical Question-Answering DatasetsTable 1 presents a summary of medical QA datasets published after 2017. In particular, we focus on categorizing the data source and question types of the different datasets. Most existing medical QA datasets adopt an open-ended format, primarily because they were constructed directly from consumer questions and answers from doctors. However, multiple-choice and fill-in-the-blank questions provide a more standardized and objective evaluation, and only a small portion of medical QA datasets have adopted these formats. Notable examples include CliCR (Suster and Daelemans, 2018), MEDQA (Jin et al., 2021), MMLU (Hendrycks et al., 2020), MLEC-QA (Zeng et al., 2023a), and MedMCQA (Pal et al., 2022). Note that the multiple-choice questions in MultiMedQA (Singhal et al., 2022) come from MEDQA, MedMCQA, and MMLU.

Data source types generally determine the reliability of a dataset. Consumer questions collected from web sources require human review to ensure the correctness of the answers. As datasets grow in size, quality control becomes increasingly challenging (Li et al., 2023). In contrast, datasets built from case reports (e.g., CliCR), research literature (e.g., BioAsq (Krithara et al., 2023)), medical books, exams, and related practices (e.g., MMLU and MedMCQA) are often more reliable.

From Table 1, we observe that there are few datasets based on multiple-choice questions from authoritative sources. This characteristic distinguishes CMExam from the MLEC-QA dataset, which is also derived from the Chinese National Medical Licensing Examination. In essence, CMExam has been meticulously crafted as a foundational benchmark dataset. It introduces question explanations for reasoning ability inspection, incorporates expansive annotation facets with authoritative references, and includes question-wise medical competencies and difficulty ratings calculated from human performance. These features make CMExam an indispensable resource for authoritative LLM performance assessment and meaningful human-machine comparisons. Table 2 presents a list of innovations and characteristics of CMExam, which are discussed in detail in the following sections.

Other Benchmark Datasets of Large Language ModelsThe assessment of LLMs has witnessed significant progress, with the introduction of diverse benchmarks that evaluate different dimensions across multiple languages, models and tasks (Liu et al., 2023b,c; Zhou et al., 2023a). Many datasets focus on assessing natural language understanding and reasoning capabilities of LLMs. RACE (Lai et al., 2017) includes English exams for Chinese middle and high school students. TriviaQA (Joshi et al., 2017) consists of question-answer pairs authored by trivia enthusiasts. DROP (Dua et al., 2019)

  
**Language** & **Data Source Type** &  \\   & & **Multiple Choice** & **Open-ended** \\   &  &  & LiveQA-Med (Abacha et al., 2017) \\  & & & CliCR\({}^{}\)(Suster and Daelemans, 2018) \\  & & & HealthQA (Zhu et al., 2019) \\  & & & MEDIQQA (Abacha et al., 2019b) \\  & & & emQA\({}^{}\)(Pamparai et al., 2018) \\  & & & MedQA\({}^{}\)(Pamparai et al., 2019) \\  & & & MedQA\({}^{}\)(Bena Abacha and Denner-Fushman, 2019) \\  & & & MedICA\({}^{}\)(Abacha et al., 2019a) \\  & & & MEDIQA\({}^{}\)(Abacha et al., 2019a) \\  & & & MASH-QA (Zhu et al., 2020) \\   &  & MEDQA\({}^{}\)(Jin et al., 2021) \\  & & MMLU\({}^{}\)(Hendrycks et al., 2020) & BioASQ (Krithara et al., 2023) \\  & & MedMCQA (Pal et al., 2022) & MultiMedQA\({}^{+}\)(Singhal et al., 2022) \\   &  &  & WebMedQA\({}^{+}\)(He et al., 2019) \\  & & & MedQA\({}^{}\)\(\)\(10^{}\)(Zhang et al., 2017) \\   & & & CModQA\({}^{}\)\(\)\(20^{}\)(Zhang et al., 2018) \\   & & & ChiMed (Tian et al., 2019) \\    & & & Huatoo-2671(Li et al., 2023) \\    & & & MLEC-QA\({}^{}\)(Zeng et al., 2023a) \\    & & **CMExam\({}^{+1}\)(ours)** & **CMExam\({}^{+1}\)(ours)** \\  

Table 1: A review of medical QA datasets. \({}^{*}\) indicates availability of additional annotations with authoritative references, \({}^{}\) indicates availability of benchmarks, and \({}^{}\) indicates datasets with more than 50K questionsevaluates reading comprehension with discrete reasoning and arithmetic components. GLUE (Wang et al., 2018) encompasses four existing NLU tasks, while SuperGLUE (Wang et al., 2019) extends it with a more challenging benchmark of eight language understanding tasks. Other datasets, such as HellaSwag (Zellers et al., 2019) and WinoGrande (Sakaguchi et al., 2021), focus on commonsense reasoning. TruthfulQA (Lin et al., 2021) includes health, law, finance, and politics, to assess LLMs' ability to mimic human falsehoods, while MMCU (Zeng, 2023) covers medical, legal, psychology, and education to evaluate multitask Chinese understanding. In addition to language understanding and reasoning, several datasets focus on specific subjects and topics, such as Python coding tasks (Chen et al., 2021), middle school mathematics questions (Cobbe et al., 2021) and defending against attacks (Yi et al., 2023; Xie et al., 2023; Pi et al., 2024). Notably, both C-Eval (Huang et al., 2023) and M3KE (Liu et al., 2023a) serve as multi-level multi-subject evaluation benchmarks, making them particularly suitable for assessing the capabilities of LLMs across multiple domains.

## 3 The CMExam Dataset

Data Collection and Pre-processingCMExam comprises authentic past licensed physician exams in the Chinese National Medical Licensing Examination (CNMLE) collected from the Internet. The CNMLE, also known as the Physician Qualification Examination, is a standardized exam that assesses applicants' medical knowledge and skills in China. It includes a written test with multiple-choice questions covering various medical subjects and a clinical skills assessment simulating patient diagnosis and treatment. We excluded questions that rely on non-textual information, including questions with external information such as images and tables, and questions with keywords "graph" and "table". Duplicate questions were removed from the dataset. In total, 96,161 questions, 68,119 of which were retained after pre-processing. The dataset was then randomly split into training/development/test sets with a ratio of 8:1:1. Each question in the dataset is associated with an ID, five candidate answers, and a correct answer. 85.24% of questions have brief solution explanations and questions in the test set contain additional annotations.

Data AnnotationCMExam provides a comprehensive analysis of LLM performance through five additional annotation dimensions. The first dimension involves disease groups based on the 11th revision of the International Classification of Diseases (ICD-11) (World Health Organization (WHO), 2021). ICD-11 is a globally recognized standard classification system for documenting and categorizing health conditions, consisting of 27 major disease groups. The second dimension comprises 36 clinical departments derived from the Directory of Medical Institution Diagnostic and Therapeutic Categories (DMIDTC) 2, published by the National Health Commission of China. DMIDTC is an authoritative guide used for categorizing and naming diagnostic and therapeutic subjects within healthcare institutes. In cases where the question cannot be successfully classified by ICD-11 or DMIDTC, the annotation is marked as "N/A". The third dimension refers to medical disciplines, which are categorized based on the List of Graduate Education Discisplinary Majors (2022) published by the Ministry of Education of the People's Republic of China3. This dimension encompasses seven categories representing study majors used in universities. The fourth dimension was created by two medical professionals within the team to assess the primary medical competency tested by each associated question. It consists of four categories. The fifth dimension represents five potential difficulty levels of each question, determined by analyzing the correctness rate observed in human performance data collected alongside the questions. For detailed information on these additional annotations including their potential values, please refer to Table 9, 12, 10, 11. And our proposed GPT-Assisted Annotation strategy is shown in supplementary materials.

  Annotation Content & References & Unique values \\  Disease Groups & The 11th revision of ICD-11 & 27 \\ Clinical Departments & The Directory of Medical Institution Diagnostic and Therapeutic Categories (DMIDTC) & 36 \\ Medical Disciplines & List of Graduate Education Discisplinary Majors (2022) & 7 \\ Medical Compencies & Medical Professionals & 4 \\ Difficulty Level & Human Performance & 5 \\  

Table 2: Additional annotations of CMExam.

Dataset CharacteristicsThe CMExam dataset has several advantages over previous medical QA datasets regarding: 1)_Reliability and Authenticity_: CMExam is sourced exclusively from the CNMLE that undergoes rigorous review and validation processes, ensuring its accuracy and adherence to established medical standards. 2) _Standardization and Comprehensiveness_: CMExam includes both multiple-choice questions that ensure fair and objective evaluations of models' performance and question-wise open-ended reasoning that allows in-depth analysis and assessment of model reasoning abilities and comprehension. Despite the inherent absence of explanations within the CNMLE, we cross-referenced exam questions with solutions offered by diverse online medical examination preparation platforms, effectively enhancing the dataset's informational depth. CMExam reflects the comprehensive coverage of medical knowledge and reasoning required in clinical practice, as it is sourced from carefully designed national medical exams. The inclusion of five additional annotation dimensions enhances the dataset's rigor and offers valuable insights for in-depth evaluation and analysis. 3) _Scale_: CMExam consists of over 60K high-quality questions, providing a large and reliable dataset.

Data StatisticsThe dataset has a total of 68,119 questions, with 65,950 answers being single-choice and 2,169 being multiple-choice, with a maximum of five answer choices. Among all questions, 85.24% have associated solution explanations 3. Figure 2 shows additional statistics visualization and more basic statistics of CMExam can be seen in supplementary materials. Within the test set, 4,493 questions (65.97%) have corresponding disease group annotations. The most prevalent disease group is Traditional Medicine Disease Patterns (TMDP), followed by Digestive System Diseases, Certain Infectious (Digest) and parasitic Diseases (InfDis), Endocrine, Nutritional, or Metabolic Diseases (Endo), and Circulatory System Diseases (Circ). For the associated clinical department annotations, 4,965 questions (72.90%) have been assigned values. The two most frequently represented clinical departments are Internal Medicine (IM) and Traditional Chinese Medicine (TCM), with Dentistry (Dent) and Surgery (Surg) following closely. Every question in the test set has been labeled with a discipline, where Clinical Medicine (ClinMed) comprises the largest proportion. Additionally, each question has been categorized into a competency area, with Medical Fundamentals (MedFund) being the predominant category. The difficulty levels of the questions align with common exam patterns, with a greater number of easy questions and a smaller number of hard questions.

Figure 2: Additional CMExam statistics. For the question length distribution subplot, only the portion within IQR is shown.

Benchmarks

### Baselines, Settings, and Metrics

Model SelectionThe LLMs we benchmarked on the CMExam can be divided into two groups based on domains: 1) _General Domain LLMs_: This group comprises GPT3.5/4 (Brown et al., 2020; OpenAI, 2023), ChatGLM (Du et al., 2022; Zeng et al., 2023b), LLaMA (Touvron et al., 2023), Alpaca (Taori et al., 2023), and Vicuna (Chiang et al., 2023). These models are general-purpose language models trained on a massive amount of general-purpose corpora; 2) _Medical Domain LLMs_: This group can be further divided into two subgroups. The first subgroup consists of representative LLMs specifically designed for the medical domain, including DoctorGLM (Xiong et al., 2023) and Huatuo (Wang et al., 2023). DoctorGLM is a healthcare-specific language model initialized with ChatGLM-6B parameters and further fine-tuned on Chinese medical dialogues extracted from ChatGPT. Huatuo, on the other hand, is a knowledge-enhanced model, which builds upon the LLaMA architecture and is additionally supervised-fine-tuned with knowledge-based instruction data harvested from the Chinese medical knowledge graph (CMeKG). The second subgroup comprises medical LLMs that were constructed through supervised fine-tuning of LLMs using the CMExam training set. This subgroup includes models fine-tuned on BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), PromptCLUE (Zhang and Xu, 2022) (T5-based), BART (Shao et al., 2021), Huatuo, ChatGLM, LLaMA, Alpaca, and Vicuna.

Human PerformanceTo effectively gauge the medical proficiency of LLMs, incorporating a measure of human performance into the benchmarking process is of paramount importance. Therefore, during data collection, we preserved the accuracy of human responses for each question. Human performance is estimated by computing a weighted average of response accuracy within each dimension, with weights determined by the number of respondents. This design ensures a robust comparison of LLMs' performance relative to human capabilities, particularly when larger respondent samples contribute to a question's accuracy.

Experimental SettingFor GPT models, we leveraged OPENAI's API to access the GPT-3.5-turbo and GPT-4-0314 models, given that their open-source variants are currently unavailable. The LLaMA, Alpaca, and Vicuna models were used in their respective 7B versions, while ChatGLM was evaluated using its publicly accessible 6B version. Additionally, we performed fine-tuning on open-source models using the CMExam dataset. We used P-tuning V2 (Liu et al., 2021) for ChatGLM-6B, with the length of prefix tokens set to 128, and the learning rate set to 2e-2, LoRA (Hu et al., 2021) for LLaMA, Alpaca, Vicuna, and Huatuo models, with the rank set to 8, alpha set to 16, and dropout at 0.05. For BERT models, we followed the fine-tuning methods outlined in (Devlin et al., 2019), with batch size set to 16, learning rate set to 2e-4, hidden dropout probability set to 0.4, and maximum input length set to 192. The fine-tuning processes for all models except BERT involved a batch size of 64, a maximum input length, and a target length of 256. All fine-tuning was performed using NVIDIA V100 GPUs for 10 epochs.

MetricsWe assess model performance on multiple choice questions using accuracy and weighted F1 score. These metrics are commonly employed in information retrieval and question-answering tasks to evaluate model performance. For the open-ended solution explanations of CMExam, BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003) were used to evaluate the discrepancy between model-generated explanations and ground truth.

### Results and Analysis

Overall ComparisonWe first assessed the performance of general domain LLMs and medical domain LLMs for answer prediction and reasoning tasks. The results are displayed in Table 3. For the answer prediction task, GPT-4 significantly outperforms other methods, demonstrating a zero-shot performance with an accuracy of 61.6% and an F1 score of 0.617. While a performance gap still exists when compared to human performance (which stands at 71.6% accuracy), it's noteworthy that this gap has been greatly reduced from what was observed with GPT-3.5. Among lightweight, general domain LLMs, ChatGLM outperforms LLaMA, Alpaca, and Vicuna, likely attributable to their limited coverage of the Chinese corpus. This restriction seemingly hampers their ability to provide accurate responses to CMExam queries. Furthermore, a noticeable deficiency in zero-shot performance is evident in lightweight medical domain LLMs such as Huatuo, owing to their restricted medical corpus diversity, which hampers the acquisition of broad medical knowledge and accurate interpretation of CMExam questions. Our findings suggest that finetuning models with CMExam enhance their performance. For instance, with an accuracy of 45.3%, ChatGLM-CMExam is comparable to GPT-3.5's performance, despite utilizing only about 3% of the parameters employed by GPT-3.5. It is noteworthy that encoder-only LLMs, such as BERT and RoBERTa, remain a robust baseline for answer prediction tasks. Their performance can par with, or even exceed, that of certain decoder-only LLMs, such as LLaMA-CMExam and Alpaca-CMExam, despite having fewer parameters.

For the solution explanation task, we observe that GPT models performed poorly on the BLEU metric, likely due to their tendency to generate short explanations. However, they exhibited an advantage on the ROUGE metric. As DoctorGLM is unable to return answer options according to the prompt, we only report its performance in the solution explanation task. Through finetuning, LLM was able to generate more reasonable explanations. For instance, ChatGLM-CMExam achieved scores of 31.10 and 18.94 on BLEU-1 and BLEU-4, respectively, and scores of 43.94, 31.48, and 29.39 on the ROUGE metrics.

Results by Disease GroupsDrawing upon ICD-11 annotations (26 categories), we conducted an analysis of the performance of several LLMs across various categories. To mitigate the potential impact of random variability resulting from the number of questions, we limited our analysis to categories containing more than 100 questions. According to Table 4, LLMs have uneven performance and significant gaps in knowledge. GPT-4's accuracy ranges from 74.4% for _Neo_ to 44.3% for _TCMDP_, GPT-3.5's accuracy ranges from 63.9% for _Neo_ to 31.0% for _TCMDP_ and ChatGLM-CMExam's accuracy ranges from 54.7% for _Psy_ to 42.9% for _RESP_.

Results by Clinical DepartmentsTo compare model performance regarding the clinical department dimension (36 categories), we only analyzed categories with more than 50 questions to ensure result representativeness. Results presented in Table 5 highlight that the models show relatively high accuracy on questions associated with commonly encountered departments, such as Emergency Medicine (_EM_), Internal Medicine (_IM_) and Surgery (_Surg_). Their accuracy on questions associated with rarer departments, such as Traditional Chinese Medicine (_TCM_). There is a marked discrepancy in the average accuracy among different departments, with the highest being 50.9% and the lowest being only 13.9%. This observation suggests there are notable variations in medical knowledge and reasoning approaches among different departments. Consequently, it may be necessary to examine specific optimization strategies for different departments.

Results by Medical DisciplinesThen, we evaluated LLM performance across seven medical disciplines. As depicted in Table 6, the performance of LLMs across disciplines such as Traditional Chinese Medicine (_TCM_), Traditional Chinese Pharmacy (_TCPharm_), and Pharmacy (_Pharm_) was notably subpar, with all accuracy rates falling below 42%. This pattern suggests a potential deficiency

    &  &  &  \\   & & & Acc (\%) & F1 (\%) & BLEU-1 & BLEU-4 & ROUGE-1 & ROUGE-2 & ROUGE-L \\   & GPT-3.5-Auto & 175B & 46.440.6 & 46.140.7 & 3.5640.67 & 1.4940.51 & 33.8040.19 & 16.3940.18 & 14.8340.13 \\  & GPT-4 & **51.641.41** & **61.740.1** & 0.1760.0 & 0.0640.00 & 29.7440.009 & 14.8440.04 & 11.5140.03 \\  & ChaGLM & 6B & 25.340.0 & 25.740.1 & 1.6514.08 & 0.505.040 & 26.5381.11 & 15.7540.05 & 17.9940.13 \\  & LLMaM & 7B & 0.440.0 & 0.340.0 & 11.9980.03 & 5.7040.00 & 27.3330.06 & 11.8880.03 & 10.7840.04 \\  & Vicuna & 7B & 5.080.0 & 4.840.1 & 20.1580.01 & 9.2640.00 & 84.3430.02 & 10.9640.01 & 6.3340.01 \\  & Alpaca & 7B & 5.850.0 & 8.440.0 & 25.400.00 & 25.0400.00 & 22.5200.00 & 9.5440.00 & 8.4400.00 \\   & Hauto & 7B & 12.940.0 & 7.040.0 & 0.2140.00 & 0.1240.00 & 25.1140.08 & 11.5640.00 & 9.7340.02 \\  & MedApaca & 7B & 20.040.0 & 10.740.0 & 0.0800.00 & 0.0040.000 & 1.9040.00 & 0.0460.00 & 0.5240.03 \\  & DoctorGLM & 6B & - & 94.490.0 & 2.6540.3 & 2.11140.08 & 6.6801.01 & 9.9990.06 \\   & PompeCLUE-base-CMExam & 0.1B & - & - & 18.7540.08 & 6.6540.00 & 40.8840.11 & 21.9940.11 & 18.3140.11 \\   & Bart-base-CMExam & 0.1B & - & - & 23.0640.00 & 10.3540.06 & 44.330.09 & 22.9420.09 & 20.800.09 \\   & Bart-large-chinese-CMExam & 0.1B & - & 23.0640.00 & 10.3540.06 & 44.330.09 & 22.9420.09 & 20.800.09 \\   & Barl-large-chinese-CMExam & 0.1B & 31.8\(\)0.2 & 31.260.2 & - & - & - & - & - \\   & BERT-CMExam & 0.1B & 31.8\(\)0.2 & 31.260.2 & - & - & - & - & - \\   & RoBERT-CMExam & 0.3B & 37.140.1 & 36.740.4 & - & - & - & - & - \\   & MedApaca-CMExam & 7B & 30.5\(\)0.1 & 34.604.0 & 16.5340.08 & 9.7840.7 & 4.3140.85 & 27.0560.00 & 24.5540.43 \\   & Huang-CMExam & 7B & 28.605.0 & 29.340.0 & 29.2490.1 & 16.7240.03 & 43.8520.42 & 25.860.00 & 22.1720.24 \\   & ChaGLM-CMExam & 6B & 45.3\(\)1.4 & 25.41 & **31.160.00** & **18.946.00** & 24.349.04 & **23.848.04** & **23.939.14** \\   & LLMaM-CMExam & 7B & 18.4\(\)0.5 & 20.605.0 & 29.250.23 & 16.250.10 & 1.658.040.12 & 25.536.00 & 22.6740.34 \\   & Alpaca-CMExam & 7B & 21.1\(\)0.6 & 24.940.4 & 29.5740.16 & 16.4040.12 & 45.4840.12 & 25.5360.18 & 2.27940.06 \\   & Vicuna-CMExam & 7B & 27.3\(\)0.5 & 28.240.3 & 29.82\(\)0.03 & 17.3040.01 & 44.9840.16 & 26.2580.13 & 2.244.09.09 \\   Random & Random & - & 3.140.2 & 5.140.3 & - & - & - & - & - \\  Human Performance & Human volunteers & - & 71.6 & - & - & - & - & - & - \\   

Table 3: Overall comparison on CMExam dataset. We **bold** the best result and underline the second best result.

in the exposure of these models to data within these categories. Conversely, disciplines such as _ClinMed_ and _Ph&PM_ demonstrated higher accuracy rates, likely due to the abundance of relevant data. The observed variability in performance across different disciplines underscores the distinctiveness of data characteristics and complexities inherent to each field, thereby advocating for discipline-specific model optimizations and enhancements.

Results by CompetenciesEvaluations based on medical competency areas aimed at a higher-level understanding of model capability in solving medical problems. As indicated in Table 7, the lowest average accuracy across LLMs was observed within the domain of mastering Medical Fundamentals (_MedFund_), with a meager average score of 42.1%. This result demonstrates that these models, predominantly trained on general textual data, have inadequate exposure to medical-specific data. While fine-tuning did provide some improvement, these models could benefit from additional medical scenario data to further augment their performance. It is worth highlighting that the average accuracy in the domain of Public Health Laws and Ethics (_PHL_) was reasonably high, notably achieving an average of 47.6%. In addition, the LLMs showcased their proficiency in accurate disease diagnosis.

Results by Question DifficultyTo evaluate model performance in tackling questions of varying levels of difficulty, we conducted experiments regarding the question difficulty dimension, which was calculated based on human exam-taker performance. As shown in Table 8, there's an evident trend where model accuracies decrease as question complexity rises. This pattern suggests that more sophisticated questions demand an extensive knowledge base and complex reasoning, which are challenging for the LLMs, thus reflecting patterns observed in human performance.

  
**Categories** & **GPT-4** & **GPT-3.5** & **ChatGLM** & **ChatGLM-CMExam** & **Average** \\  Neo & 74.4\(\)2.2 & 63.9\(\)1.4 & 32.4\(\)1.6 & 51.9\(\)0.2 & 55.6\(\)0.8 \\ Psy & 74.0\(\)0.7 & 62.0\(\)1.7 & 33.3\(\)1.3 & 54.7\(\)0.8 & 56.0\(\)0.9 \\ Factors & 70.0\(\)1.0 & 57.5\(\)1.4 & 28.0\(\)1.1 & 51.1\(\)1.4 & 51.6\(\)0.5 \\ MSK & 65.9\(\)0.8 & 53.8\(\)0.8 & 29.2\(\)0.4 & 53.5\(\)0.0 & 50.6\(\)0.4 \\ GU & 69.2\(\)0.4 & 52.1\(\)1.1 & 30.0\(\)0.2 & 49.5\(\)0.9 & 50.2\(\)0.3 \\ Inj & 65.9\(\)2.3 & 45.7\(\)1.3 & 37.2\(\)2.9 & 49.1\(\)1.8 & 49.5\(\)1.4 \\ Circ & 68.8\(\)0.3 & 49.3\(\)0.7 & 30.9\(\)0.7 & 47.0\(\)0.3 & 49.0\(\)0.2 \\ Endo & 70.6\(\)1.1 & 49.4\(\)1.1 & 25.5\(\)0.8 & 46.1\(\)0.4 & 47.9\(\)0.2 \\ Digest & 67.0\(\)1.0 & 48.8\(\)1.4 & 26.2\(\)0.7 & 49.4\(\)1.1 & 47.8\(\)0.4 \\ InfDis & 66.0\(\)0.5 & 49.2\(\)0.8 & 27.5\(\)0.6 & 48.2\(\)0.8 & 47.7\(\)0.4 \\ Neuro & 64.4\(\)1.2 & 48.7\(\)3.1 & 28.6\(\)0.4 & 45.3\(\)1.3 & 46.7\(\)1.1 \\ OBST & 63.5\(\)0.3 & 45.0\(\)2.4 & 25.7\(\)0.9 & 49.4\(\)0.3 & 45.9\(\)0.5 \\ BLOOD & 69.4\(\)0.3 & 45.3\(\)1.4 & 18.9\(\)1.6 & 43.3\(\)0.7 & 44.2\(\)0.4 \\ Resp & 62.7\(\)0.8 & 44.3\(\)1.4 & 24.5\(\)0.3 & 42.9\(\)0.0 & 43.6\(\)0.7 \\ N/A & 60.0\(\)0.1 & 46.8\(\)0.3 & 24.9\(\)0.2 & 42.5\(\)0.1 & 43.5\(\)0.1 \\ TCMDP & 44.3\(\)0.9 & 31.0\(\)0.6 & 24.2\(\)0.4 & 47.9\(\)0.0 & 36.9\(\)0.6 \\   

Table 4: Comparing disease classifications.

  
**Categories** & **GPT-4** & **GPT-3.5** & **ChatGLM** & **ChatGLM-CMExam** & **Average** \\  EM & 67.4\(\)0.2 & 49.8\(\)0.7 & 36.3\(\)0.4 & 50.2\(\)0.5 & 50.9\(\)0.1 \\ OBGYN & 66.4\(\)1.0 & 51.7\(\)1.5 & 28.6\(\)0.5 & 52.0\(\)0.0 & 49.7\(\)0.3 \\ IM & 70.2\(\)0.6 & 51.8\(\)0.8 & 26.0\(\)1.1 & 47.9\(\)0.9 & 49.0\(\)1.0 \\ ID & 67.4\(\)1.9 & 49.5\(\)3.3 & 26.1\(\)1.9 & 49.6\(\)3.8 & 48.2\(\)1.2 \\ Surg & 63.6\(\)0.8 & 49.5\(\)1.5 & 28.8\(\)0.5 & 47.7\(\)0.9 & 47.4\(\)1.5 \\ ClinNutr & 68.3\(\)2.4 & 48.3\(\)2.9 & 29.9\(\)1.1 & 47.8\(\)0.5 & 47.1\(\)0.7 \\ MedLabSci & 69.2\(\)0.6 & 48.3\(\)2.0 & 29.0\(\)1.5 & 40.8\(\)0.6 & 46.8\(\)0.2 \\ Ped & 64.5\(\)0.0 & 47.2\(\)1.4 & 26.7\(\)2.1 & 41.9\(\)5.5 & 45.1\(\)1.7 \\ N/A & 62.6\(\)0.2 & 48.6\(\)1.1 & 24.6\(\)0.4 & 44.3\(\)0.9 & 45.0\(\)1.0 \\ Ophth & 60.9\(\)0.5 & 39.1\(\)0.8 & 21.8\(\)0.8 & 54.0\(\)0.2 & 44.0\(\)0.8 \\ OccMed & 61.5\(\)4.3 & 38.5\(\)1.6 & 31.3\(\)4.3 & 41.5\(\)3.3 & 43.2\(\)2.5 \\ DENT & 54.9\(\)2.0 & 41.2\(\)1.6 & 27.9\(\)0.8 & 43.5\(\)0.9 & 41.9\(\)1.0 \\ TCM & 43.1\(\)1.3 & 31.4\(\)1.3 & 24.5\(\)1.9 & 45.8\(\)4.4 & 36.2\(\)0.6 \\ ENT & 41.3\(\)0.8 & 28.0\(\)0.6 & 29.3\(\)0.1 & 26.7\(\)0.1 & 31.3\(\)0.5 \\ ICM & 33.3\(\)0.0 & 11.1\(\)15.7 & 0.0\(\)0.0 & 11.1\(\)15.7 & 13.9\(\)4.8 \\   

Table 5: Comparing clinical department.

Results by Question LengthFinally, to investigate if model performance is associated with input lengths, we compared their performance regarding question lengths. Figure 3 illustrates that Large Language Models (LLMs) generally show higher accuracy with problem lengths between 60 and 90. However, their performance seems to falter with problems that are either too short or overly long. Additionally, we noticed that the effect of question length varies across different LLMs. For instance, GPT models tend to incrementally improve as the problem length expands, performing optimally within the 50 to 90 range. Conversely, ChatGLM-CMExam's performance fluctuates noticeably with varying lengths, and it tends to fall short compared to GPT models when addressing longer problems.

## 5 Conclusion and Discussions

In this work, we developed CMExam, a dataset sourced from the stringent Chinese National Medical Licensing Examination, featuring 60,000+ multiple-choice questions, with detailed explanations. CMExam ensures reliability, validity, and adherence to medical standards. It also demonstrates the practicality of employing GPT-4 to automate the annotation process, which strikes a harmonious balance between efficiency and cost-effectiveness while maintaining the desired level of accuracy and reliability of the annotation. Utilizing this large and reliable corpus, we tested several LLMs for answer selection and reasoning tasks. A performance gap was observed between LLMs and human experts, signaling the need for additional LLM research. CMExam's standardization and comprehensiveness also ensure objective evaluations of models while enabling in-depth analysis of their reasoning capabilities. The questions cover a wide spectrum of medical knowledge, augmented with five additional annotation dimensions for rigorous evaluation. This study aims to spur further exploration of LLMs in medicine by providing a comprehensive benchmark for their evaluation.

  
**Categories** & **GPT-4** & **GPT-3.5** & **ChatGLM** & **ChatGLM-CMExam** & **Average** \\  ClinMed & 67.9\(\)0.1 & 51.4\(\)0.4 & 27.3\(\)0.3 & 48.9\(\)0.4 & 48.8\(\)0.7 \\ PH\&PM & 68.2\(\)0.4 & 52.7\(\)1.7 & 26.2\(\)0.3 & 47.3\(\)1.0 & 48.6\(\)0.5 \\ ICWM & 56.1\(\)0.1 & 40.0\(\)2.3 & 29.4\(\)0.8 & 53.6\(\)0.7 & 44.8\(\)0.9 \\ Dent & 59.5\(\)0.7 & 43.9\(\)1.9 & 28.5\(\)1.1 & 45.3\(\)0.6 & 44.3\(\)0.3 \\ Pharm & 61.1\(\)0.4 & 46.3\(\)0.5 & 23.2\(\)0.2 & 37.0\(\)0.1 & 41.9\(\)0.3 \\ TCM & 53.5\(\)0.4 & 35.9\(\)0.2 & 24.1\(\)0.3 & 49.1\(\)0.0 & 40.6\(\)1.1 \\ TCPharm & 45.4\(\)1.2 & 35.6\(\)0.1 & 24.1\(\)1.0 & 43.1\(\)0.4 & 37.1\(\)0.5 \\   

Table 6: Comparing medical discipline.

  
**Categories** & **GPT-4** & **GPT-3.5** & **ChatGLM** & **ChatGLM-CMExam** & **Average** \\  Diag & 70.1\(\)5.5 & 50.9\(\)2.1 & 30.9\(\)2.8 & 51.6\(\)1.0 & 50.9\(\)1.4 \\ PHL & 64.2\(\)0.7 & 50.0\(\)0.5 & 26.8\(\)0.3 & 49.6\(\)0.1 & 47.6\(\)0.3 \\ Treat & 56.5\(\)0.5 & 43.0\(\)1.1 & 25.7\(\)0.2 & 47.4\(\)0.6 & 43.2\(\)0.8 \\ MeFund & 58.3\(\)0.3 & 44.6\(\)0.7 & 23.9\(\)0.5 & 41.6\(\)0.4 & 42.1\(\)0.9 \\ N/A & 54.8\(\)0.2 & 30.4\(\)0.4 & 23.7\(\)0.1 & 38.5\(\)0.2 & 36.9\(\)0.3 \\   

Table 7: Comparing LLMsâ€™ competencies.

Figure 3: Results stratified by question length.

  
**Categories** & **GPT-4** & **GPT-3.5** & **ChatGLM** & **ChatGLM-CMExam** & **Average** \\  Easy & 74.6\(\)0.1 & 58.5\(\)0.6 & 31.4\(\)0.2 & 61.5\(\)0.3 & 56.5\(\)0.4 \\ Manageable & 63.9\(\)0.2 & 47.4\(\)0.7 & 25.9\(\)0.5 & 46.1\(\)0.3 & 45.8\(\)0.6 \\ Moderate & 51.3\(\)0.6 & 36.8\(\)0.8 & 23.0\(\)0.4 & 34.5\(\)0.6 & 36.4\(\)0.7 \\ Difficult & 36.4\(\)0.9 & 26.2\(\)0.7 & 18.9\(\)0.5 & 24.3\(\)0.9 & 26.5\(\)0.6 \\ Extremely difficult & 27.2\(\)1.0 & 21.4\(\)2.2 & 15.8\(\)1.0 & 12.2\(\)1.1 & 19.1\(\)1.1 \\   

Table 8: Results by question difficulty.

We anticipate CMExam to contribute significantly to future advancements of LLMs, particularly in handling medical question-answering tasks.

LimitationsFirstly, while CMExam is derived from meticulously designed medical examinations, our process of excluding questions requiring non-textual information may inadvertently affect the balance of the remaining questions, potentially introducing unexpected biases. It is critical to acknowledge this aspect while interpreting any findings or analyses conducted using this dataset. Furthermore, the current BLEU and ROUGE metrics primarily evaluate the explanation task, but these measures are insufficient for assessing the reasonableness of the answer. In future work, we will incorporate human evaluation to provide a more comprehensive assessment of the models.

EthicsCMExam is a dataset derived from the Chinese National Medical Licensing Examination, which aligns with numerous datasets containing similar National Medical Licensing Examinations (Zeng et al., 2023; Hendrycks et al., 2020; Jin et al., 2021; Pal et al., 2022; Singhal et al., 2022). We have ensured adherence to applicable legal and ethical guidelines during data collection and use. The authenticity and accuracy of the exam questions have been thoroughly verified, providing a reliable basis for evaluating LLMs. Please note that the CMExam dataset is intended for academic and research purposes only. Any commercial use or other misuse that deviates from this purpose is expressly prohibited. We urge all users to respect this stipulation in the interest of maintaining the integrity and ethical use of this valuable resource.

Societal ImpactsWhile CMExam aims to enhance LLM evaluations in the medical field, it should not be misused for assessing individual medical competence or for patient diagnosis. Conclusions drawn from models trained on this dataset should acknowledge its limitations, especially given its single source and the specific context of the CNMLE. The use of this dataset should strictly be limited to research purposes to avoid potential misuse.