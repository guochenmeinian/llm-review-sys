# Transductive Learning Is Compact

Julian Asilis

USC

asilis@usc.edu

&Siddartha Devic

USC

devic@usc.edu

&Shaddin Dughmi

USC

shaddin@usc.edu

&Vatsal Sharan

USC

vsharan@usc.edu

&Shang-Hua Teng

USC

shanghua@usc.edu

###### Abstract

We demonstrate a compactness result holding broadly across supervised learning with a general class of loss functions: Any hypothesis class \(\) is learnable with transductive sample complexity \(m\) precisely when all of its finite projections are learnable with sample complexity \(m\). We prove that this exact form of compactness holds for realizable and agnostic learning with respect to any _proper_ metric loss function (e.g., any norm on \(^{d}\)) and any continuous loss on a compact space (e.g., cross-entropy, squared loss). For realizable learning with _improper_ metric losses, we show that exact compactness of sample complexity can fail, and provide matching upper and lower bounds of a factor of 2 on the extent to which such sample complexities can differ. We conjecture that larger gaps are possible for the agnostic case. Furthermore, invoking the equivalence between sample complexities in the PAC and transductive models (up to lower order factors, in the realizable case) permits us to directly port our results to the PAC model, revealing an almost-exact form of compactness holding broadly in PAC learning.

## 1 Introduction

Compactness results in mathematics describe the behavior by which, roughly speaking, an infinite system can be entirely understood by inspecting its finite subsystems: An infinite graph is \(k\)-colorable precisely when its finite subgraphs are all \(k\)-colorable , an infinite collection of compact sets in \(^{d}\) has non-empty intersection precisely when the same is true of its finite subcollections, etc. In each case, compactness reveals a profound and striking structure, by which local understanding of a problem immediately yields global understanding.

We demonstrate that supervised learning in the transductive model enjoys such structure. First, let us briefly review the transductive model, a close relative of the PAC model. In the realizable setting with a class of hypotheses \(^{}\), it is defined by the following sequence of steps:

1. An adversary selects unlabeled data \(S=(x_{1},,x_{n})^{n}\) and a hypothesis \(h\).
2. The unlabeled datapoints \(S\) are displayed to the learner.
3. One datapoint \(x_{i}\) is selected uniformly at random from \(S\). The remaining datapoints \[S_{-i}=(x_{1},,x_{i-1},x_{i+1},,x_{n})\] and their labels under \(h\) are displayed to the learner.
4. The learner is prompted to predict the label of \(x_{i}\), i.e., \(h(x_{i})\).

The expected error incurred by the learner over the uniformly random choice of \(x_{i}\) is its _transductive error_ on this learning instance, from which one can easily define the transductive sample complexity of a learner and of a hypothesis class.

Notably, transductive learning, originally introduced by Vapnik and Chervonenkis (1974) and Vapnik (1982), is a fundamental approach to learning with deep theoretical connections to the PAC model. We study the transductive model as employed by the pioneering work of Haussler et al. (1994), who introduced the celebrated _one-inclusion graph_ (OIG) to study transduction and used it to derive improved error bounds for VC classes. More recently, transductive learning and OIGs have been used to (among other work) establish the first characterizations of learnability for multiclass classification and realizable regression (Brukhim et al., 2022; Attias et al., 2023), to prove optimal PAC bounds across several learning settings (Aden-Ali et al., 2023), and to understand regularization in multiclass learning (Asilis et al., 2024). (See also Daniely and Shalev-Shwartz (2014); Alon et al. (2022); Montasser et al. (2022); Aden-Ali et al. (2023).) The transductive model also naturally generalizes to the agnostic setting, much like PAC learning, as articulated by Asilis et al. (2024).

### Contributions

Our results involve comparing a hypothesis class \(\) to its "finite projections." Formally, for a hypothesis class \(^{}\) and any finite collection of unlabeled data \(S\), we refer to the finite subsets of \(|_{S}\) as _finite projections_ of \(\). Note that \(\) is being "made finite" at two levels: first by restricting its functions to a finite region \(S\) of the domain, and second by passing to a finite subset of \(|_{S}\). Thus, any finite projection of \(\), e.g. \(|_{S}\), is necessarily a finite set of behaviors, \(||<\), regardless of whether \(|_{S}\) in its totality is infinite (as may easily occur if \(\) is infinite).

As our cornerstone result, we demonstrate in Theorem 3.6 that for the case of supervised learning with a large class of _proper_1 metric loss functions (including any norm on \(^{d}\) or its closed subsets; see Definition 3.2) a class \(\) can be learned with transductive sample complexity \(m\) precisely when the same is true of all its finite projections. In fact, in Theorem 3.7 we extend our results to arbitrary continuous losses on compact metric spaces, e.g., cross-entropy loss on finite-dimensional probability spaces and squared \(_{2}\) loss on compact subsets of \(^{d}\). For learning over arbitrary label spaces, we demonstrate in Theorems 3.8 and 3.9 that compactness fails: for realizable learning with metric losses, we provide matching upper and lower bounds of a factor of 2 on the extent to which such transductive sample complexities can differ. Our lower bound transfers directly to transductive learning in the agnostic case, for which we conjecture that larger gaps in sample complexity are possible.

We stress that our compactness results are _exact_ in the transductive model, avoiding dilution by asymptotics or even by constants. In addition, there is a growing body of work relating sample complexities in the transductive and PAC models, by which our results directly transfer in a black-box manner (Asilis et al., 2024; Aden-Ali et al., 2023; Dughmi et al., 2024). Notably, for realizable learning with any bounded loss, PAC sample complexities differ from their transductive counterparts by at most a logarithmic factor in \(\), the confidence parameter. Combined with our results, this reveals an almost-exact form of compactness for realizable PAC learning, as we describe in Section 3.4.2

Our results hold for improper learners, i.e., learners that are permitted to emit a predictor outside the underlying class \(\). Curiously, compactness of sample complexity can be seen to fail strongly when one requires that learners be proper, using the work of Ben-David et al. (2019). This demonstrates a structural difference between proper and improper learning; see Appendix B for further detail.

Our compactness results are underpinned by a generalization of the classic marriage theorems for bipartite graphs which may be of independent mathematical interest. The original marriage theorem, due to Philip Hall (Hall, 1935), articulates a necessary and sufficient condition for the existence of a perfect matching from one side of a _finite_ bipartite graph to the other. Subsequently, Marshall Hall (Hall Jr, 1948) extended the same characterization, referencing only finite subgraphs, to infinite graphs of arbitrary cardinality, provided the side to be matched has finite degrees -- the characterization being false otherwise, as can be seen by a simple countable example. This characterization therefore serves as a compactness result for matching on such infinite graphs. The proof of M. Hall features an involved analysis of the lattice of "blocking sets", and invokes the axiom of choice through Zorn's lemma. Simpler proofs have since been discovered: a topological proof by Halmos and Vaughan which invokes the axiom of choice through Tychonoff's theorem, and an algebraic proof by Rado  which also uses Zorn's lemma. At the heart of our paper is a compactness result (Theorem 3.3) for a variable-assignment problem which generalizes both supervised learning and bipartite matching: one side of a bipartite graph indexes infinitely many variables, the other indexes infinitely many functions that depend on finitely many variables each, and the goal is to assign all the variables in a manner that maintains all functions below a target value. Our proof draws inspiration from all three of the aforementioned proofs of M. Hall's theorem, and goes through Zorn's lemma.

### Related Work

The transductive approach to learning dates to the work of Vapnik and Chervonenkis  and Vapnik , and has inspired a breadth of recent advances across regression, classification, and various other learning regimes; see our introduction for a brief overview. Regarding transductive sample complexities, Hanneke et al.  recently demonstrated a trichotomy result for optimal transductive error rates in the _online_ setting of Ben-David et al. . In contrast, we focus on the classical (batch) setting, as described in Section 2.2.

Perhaps most related to the present work is Attias et al. , which introduces the \(\)-OIG dimension and demonstrates that it characterizes learnability for supervised learning problems with pseudometric losses. Notably, this is the first general dimension characterizing learnability across essentially the entirety of supervised learning. The \(\)-OIG dimension itself establishes a qualitative form of compactness -- as it is defined using only the finite projections of a class -- but we note that it has not been shown to tightly characterize the sample complexity of learning. Furthermore, it is analyzed only for realizable learning, which is in general not equivalent to agnostic learning (e.g., for regression). Our work, in contrast, establishes exact compactness for the sample complexity of transductive learning for both the realizable and agnostic settings, with respect to a general class of loss functions. Moreover, in Appendix B we extend our results to certain cases of distribution-family learning, including realizable learning of partial concept classes.

## 2 Preliminaries

### Notation

For a natural number \(n\), \([n]\) denotes the set \(\{1,,n\}\). For a predicate \(P\), \([P]\) denotes the Iverson bracket of \(P\), i.e., \([P]=1\) when \(P\) is true and 0 otherwise. When \(Z\) is a set, \(Z^{<}\) denotes the set of all finite sequences in \(Z\), i.e., \(Z^{<}=_{i=1}^{}Z^{i}\). For a tuple \(S=(z_{1},,z_{n})\), we use \(S_{-i}\) to denote \(S\) with its \(i\)th entry removed, i.e., \(S_{-i}=(z_{1},,z_{i-1},z_{i+1},,z_{n})\).

### Transductive Learning

Let us recall the standard toolkit of supervised learning. A learning problem is determined by a **domain**\(\), **label space**\(\), and **hypothesis class**\(^{}\). The elements of \(\) are functions \(\); such functions are referred to as **hypotheses** or **predictors**. Learning also requires a **loss function**\(\) (or \(d\)) from \(\) to \(_{ 0}\), which often endows \(\) with the structure of a metric space. Throughout the paper, we permit \(\) to be arbitrary. A **labeled datapoint** is a pair \((x,y)\) and an **unlabeled datapoint** is an element \(x\). A **training set**, or _training sample_, is a tuple of labeled datapoints \(S()^{<}\). A **learner** is a function from training sets to predictors, i.e., \(A:()^{<}^{}\).

**Definition 2.1**.: _Realizable **transductive learning** is defined as follows: An adversary selects \(S=(x_{i})_{i[n]}^{n}\) and a hypothesis \(h\). The unlabeled datapoints \(S\) are displayed to the learner. Then one datapoint \(x_{i}\) is selected uniformly at random from \(S\), and the remaining datapoints and their labels under \(h\) are displayed to the learner. Lastly, the learner is prompted to predict the label of \(x_{i}\), i.e., \(h(x_{i})\)._

We refer to the information of \((S,h)\) as in Definition 2.1 as an **instance** of transductive learning, and to \(x_{i}\) as the (randomly selected) **test datapoint** and \(S_{-i}\) the (randomly selected) **training datapoints**. The **transductive error** incurred by a learner \(A\) on an instance \((S,h)\) is its average error over the uniformly random choice of test datapoint, i.e.,

\[L_{S,h}^{}(A)=_{i[n]}A(S_{-i},h)(x_{ i}),h(x_{i}),\]where \(A(S_{-i},h)\) denotes the output of \(A\) on the sample \((x_{j},h(x_{j}))_{x_{j} S_{-i}}\).

Having defined transductive error, it is natural to define error rates and sample complexity.

**Definition 2.2**.: _The **transductive error rate** of a learner \(A\) for \(\) is the function \(_{A,}:\) defined by \(_{A,}(n)=_{S^{n},\;h}L^{}_{S,h}(A)\). The **transductive sample complexity** of a learner \(A\) for \(\) is the function \(m_{,A}()=\{m\;:\;_{A,}(m^ {}),\; m^{} m\}\)._

**Definition 2.3**.: _The **transductive error rate** of a class \(\) is the minimal error rate attained by any of its learners, i.e., \(_{}(n)=_{A}_{A,}(n)\). The **transductive sample complexity**\(m_{,}:_{>0}\) of \(\) is the function mapping \(\) to the minimal \(m\) for which \(_{}(m^{})\) for all \(m^{} m\). That is,_

\[m_{,}()=\{m\;:\;_{ }(m^{}),\; m^{} m\}.\]

_We say that \(\) is learnable in the realizable case with transductive sample function \(m\) when \(m_{,}() m()\) for all \(\)._

Informally, agnostic transductive learning is the analogue in which the adversary is permitted to label the data in \(S\) arbitrarily, and in which the learner need only compete with the best hypothesis in \(\). We defer the formal definition to Section 3.3.

## 3 Compactness of Learning

We present the central result of the paper in this section: the transductive sample complexity of learning is a compact property of a hypothesis class. In Section 3.1 we study compactness of realizable supervised learning over _proper_ loss functions, and demonstrate a strong compactness result: a class \(\) is learnable with transductive sample complexity \(m\) if and only if all its finite projections are learnable with the same complexity. In Section 3.2 we examine the case of realizable supervised learning over improper loss functions and prove a negative result: the previous compactness result no longer holds in this more general setting. Nevertheless, we demonstrate an approximate form of compactness, up to a factor of 2, for (improper) metric losses. Moreover, we show exact compactness for the special case of the (improper) 0-1 loss function, i.e., multiclass classification over arbitrary, possibly infinite label sets. Notably, this recovers M. Hall's classic matching theorem for infinite graphs (Hall Jr., 1948) as a corollary to our central result. In Section 3.3 we examine analogues of our results for agnostic learning, and in Section 3.4 we transfer our results to the PAC model via standard equivalences, obtaining approximate compactness of sample complexities. Due to space constraints, we defer an extension of our results to distribution-family PAC learning to Appendix B.

### Realizable Learning With Proper Loss Functions

We first consider the case of loss functions \(:_{ 0}\) defined on a proper metric space \(\).

**Definition 3.1**.: _A metric space is **proper** if its closed and bounded subsets are all compact._

A related notion is that of a proper map between metric spaces.

**Definition 3.2**.: _A function \(f:X Y\) between metric spaces is **proper** if it reflects compact sets, i.e., \(f^{-1}(U) X\) is compact when \(U Y\) is compact._

We remark that proper spaces are sometimes referred to as _Heine-Borel spaces_, and that their examples include \(^{d}\) endowed with any norm, all closed subsets of \(^{d}\) (under the same norms), and all finite sets endowed with arbitrary metrics. Further discussion of proper metric spaces is provided in Appendix A. The central technical result of this subsection is a compactness property concerning assignments of variables to metric spaces that maintain a family of functions below a target value \(\).

**Theorem 3.3**.: _Let \(L\) be a collection of variables, with each variable \( L\) taking values in a metric space \(M_{}\). Let \(R\) be a collection of proper functions, each of which depends upon finitely many variables in \(L\) and has codomain \(_{ 0}\). Then the following conditions are equivalent for any \(>0\)._

1. _There exists an assignment of all variables in_ \(L\) _which keeps the output of each function_ \(r R\) _no greater than_ \(\)_._
2. _For each finite subset_ \(R^{}\) _of_ \(R\)_, there exists an assignment of all variables in_ \(L\) _which keeps the output of each function_ \(r^{} R^{}\) _no greater than_ \(\)Proof.: \((1.)(2.)\) is immediate. Before arguing the reverse direction, some terminology: a _partial assignment_ of variables is an assignment of variables for a subset of \(L\). A partial assignment is said to be _completable_ with respect to \(^{} R\) if its unassigned variables can all be assigned so that all functions \(r^{} R^{}\) are kept below \(\). A partial assignment is _finitely completable_ if it is completable with respect to all finite subsets of \(R\). This is a pointwise condition: the completions are permitted to vary across \(R\)'s subsets.

**Lemma 3.4**.: _Given a finitely completable partial assignment with an unassigned variable, one such variable can be assigned while preserving finite completability._

Proof.: Fix any unassigned variable \( L\); we will assign it while preserving finite completability. For each set \(R^{} R\), let \(N_{R^{}}()\) consist of those assignments of \(\) that preserve completability with respect to \(R^{}\). By the assumption of finite completability, we have that \(N_{R^{}}()\) is non-empty for all finite \(R^{}\). We claim furthermore that \(N_{R^{}}()\) is compact for finite \(R^{}\).

To see why, let \(|R^{}|=k\) and let \(_{1},,_{m}\) be the variables in \(L\) upon which the functions in \(R^{}\) depend. Suppose without loss of generality that \(=_{1}\) and that nodes \(_{i+1},,_{m}\) have already been assigned. Consider the function \(f_{R^{}}:_{j=1}^{i}M_{_{j}}^{k}\) mapping assignments of the \(_{1},,_{i}\) to the outputs they induce on the functions in \(R^{}\). (Notably, this includes the assignments already made for \(_{i+1},,_{m}\).) As the functions in \(R^{}\) are proper, including when fixing some of their inputs, \(f_{R^{}}\) is as well.

Thus \(f_{R^{}}^{-1}([0,]^{k})\) is compact, as is its projection onto its first coordinate. That set is precisely \(N_{R^{}}()\), demonstrating our intermediate claim. We thus have a family of compact, non-empty sets \(=\{N_{R^{}}():R^{} R,|R^{}|<\}\). Note that finite intersections of elements of \(\) are non-empty, as \(_{i=1}^{j}N_{R_{i}}() N_{_{i=1}^{j}R_{i}}()\). In metric spaces, an infinite family of compact sets has non-empty intersection if and only if the same holds for its finite intersections. Thus, by compactness of each element of \(\), the intersection across all of \(\) is non-empty. That is, there exists an assignment for \(\) which is completable with respect to all finite subsets of \(R\). The claim follows. 

We now complete the argument using Zorn's lemma. Let \(\) be the poset whose elements are finitely completable assignments, where \(_{1}_{2}\) if \(_{2}\) agrees with all assignments made by \(_{1}\) and perhaps assigns additional variables. Note first that chains in \(\) have upper bounds. In particular, let \(\) be a chain and define \(_{}\) to be the "union" of assignments in \(\), i.e., \(_{}\) leaves \(\) unassigned if all \(\) leave \(\) unassigned, otherwise assigns \(\) to the unique element used by assignments in \(\).

Clearly \(_{}\) serves as an upper bound of \(\), provided that \(_{}\). To see that \(_{}\), fix a finite set \(R^{} R\). \(R^{}\) is incident to a finite collection of nodes in \(L\), say \(_{1},,_{m}\). Suppose \(_{1},,_{i}\) are those which are assigned by \(_{}\), and let \(_{1},,_{i}\) be assignments which assign (i.e., do _not_ leave free) the respective nodes \(_{1},,_{i}\). Then, as \(\) is totally ordered, it must be that one of \(_{1},,_{i}\) assigns all of the variables \(_{1},,_{i}\). That is, there exists \(_{j}\) which agrees with \(_{}\) in its action on \(_{1},,_{m}\). As \(_{j}\), it must be that \(_{j}\) is completable with respect to \(R^{}\). Then \(_{}\) is also completable with respect to \(R^{}\), as \(R^{}\) depends only upon \(_{1},,_{m}\).

Thus, invoking Zorn's lemma, \(\) has a maximal element \(_{}\). By Lemma 3.4, it must be that \(_{}\) does not leave a single variable unassigned, otherwise it could be augmented with an additional assignment. There thus exists a _total_ assignment that is finitely completable. As it has no free variables, it must indeed be maintaining all functions in \(R\) below \(\). The claim follows. 

**Remark 3.5**.: _A corollary to Theorem 3.3 is that the same claim holds when the target values \(\) vary over the functions \(r R\), as translations and scalings of proper functions \(Z\) are proper._

**Theorem 3.6**.: _Let \(\) be an arbitrary domain, \(\) a label set, and \(d\) a loss function such that \((,d)\) is a proper metric space. Then the following are equivalent for any \(^{}\) and \(m_{>0}\):_

1. \(\) _is learnable in the realizable case with transductive sample function_ \(m\)_._
2. _For any finite_ \(X\) _and finite_ \(^{}|_{X}\)_,_ \(^{}\) _is learnable in the realizable case with transductive sample function_ \(m\)_._

Proof.: \((1)(2.)\) is immediate. For the reverse direction, fix an \(>0\) and set \(n=m()\). Then fix a sequence of unlabeled datapoints \(S^{n}\). It suffices to demonstrate that a transductive learner for \(\) on instances of the form \(\{(S,h)\}_{h}\) can be designed which attains error \(\).

We will capture the task of transductively learning \(\) on such instances by way of a certain collection \(R\) of functions and \(L\) of variables. Each variable \( L\) will be permitted to take values in \(\), while each function \(r R\) depends upon exactly \(n\) variables in \(L\) and outputs values in \(_{ 0}\). More precisely, let \(R=|_{S}\) and \(L=_{S^{} S,|S^{}|=n-1}|_{S^{}}\). These serve merely as representations for the functions in \(R\) and variables in \(L\), not their true definitions (which will be established shortly). Note now that by suppressing the unlabeled datapoints of \(S\), we can equivalently represent elements of \(R\) as sequences in \(^{n}\) and elements of \(L\) as sequences in \((\{?\})^{n}\). In this view, each element of \(L\) is precisely an element of \(R\) which had exactly one entry replaced with a "?". See Figure 1.

Now, to model transductive learning, fix an element \(r R\) represented by \((y_{1},,y_{n})^{n}\). Then we will define \(r\) to be a function depending upon the variables \(_{1},,_{n} L\), where \(_{i}=(y_{1},,y_{i-1},?,y_{i+1},,y_{n})\). Given assignments for each of the variables \(_{1},,_{n}\) as values in \(\) -- semantically, completions of their "?" entries -- the node \(r\) then outputs the value \(_{i=1}^{n}d(y_{i},_{i})\). The two crucial observations are as follows: an assignment of each \( L\) corresponds precisely to the action of a learner responding to a query at test time, and the output of node \(r\) equals the error of a learner when \(r\) is the ground truth.

Thus, it remains to show that the variables in \(L\) can all be assigned so as to keep the outputs of the functions in \(R\) less than \(\). The condition (2.) grants us that this is true for each finite collection of functions \(R^{} R\). Now note that the functions \(r R\) are proper, as each such \(r\) is continuous and reflects bounded sets, and as \(\) itself is proper. Invoke Theorem 3.3 to complete the proof. 

Theorem 3.6 establishes an exact compactness in learning with respect to a flexible class of metric loss functions. One may note, however, that some non-metric losses are of central importance to

Figure 1: Depiction of variables \(L\) and functions \(R\) which model transductive learning, for a sequence of unlabeled datapoints \(|S|=3\) such that \(|_{S}\) contains the behaviors \((0,0,0)\), \((1,0,0)\), and \((0,1,0)\). Arrows denote functional dependence, i.e., each \(r R\) depends upon its incident variables.

machine learning, including the squared error on compact subsets of \(\) (which violates the triangle inequality) and the cross-entropy loss for finite-dimensional distributions (which is not symmetric). We now provide a modified form of Theorem 3.6 which captures these loss functions, in which the loss function \(_{}\) is permitted to differ from the underlying metric \(d\) on \(\). (E.g., such that \(d\) is the usual Euclidean norm on a compact subset \(\) of \(^{d}\), and \(_{}\) is any continuous loss function.)

**Theorem 3.7**.: _Let \(\) be an arbitrary domain, \((,d)\) a compact metric space, and \(^{}\) a hypothesis class. Let \(_{}:_{ 0}\) be a loss function employed for learning that is continuous with respect to the metric \(d\). Then the following are equivalent for any \(m_{>0}\):_

1. \(\) _is learnable in the realizable case with transductive sample function_ \(m\)_._
2. _For any finite_ \(X\) _and finite_ \(^{}|_{X}\)_,_ \(^{}\) _is learnable in the realizable case with transductive sample function_ \(m\)_._

Proof.: We adopt precisely the perspective of Theorem 3.6, seeing transductive learning modeled as a variable assignment problem with the same variables \(L\) and functions \(R\). To invoke Theorem 3.3, it remains only to show that the functions \(r R\) are proper. First note a continuous function from a compact space to \(\) is automatically proper, as closed subsets of compact sets are compact. Now recall that each \(r R\) is a sum of scaled copies of \(_{}\) with one input fixed. As each such function is continuous, \(r\) itself is continuous and thus proper. 

### Realizable Learning With Improper Loss Functions

It is natural to ask whether the requirement that \(\) be a proper metric space is essential to Theorem 3.6 or merely an artifact of the proof. We now demonstrate the former: for arbitrary metric losses, the error rate of learning \(\) can exceed that of all its finite projections by a factor of 2. Recall that \(_{}:_{ 0}\) denotes the transductive error rate of learning a class \(\), i.e., \(_{}(n)\) denotes the error incurred by an optimal learner for \(\) on (worst-case) samples of size \(n\).

**Theorem 3.8**.: _There exists a hypothesis class \(^{}\), metric loss function \(d\) on \(\), and \(n\) such that for any finite \(X\) and finite \(^{}|_{X}\), \(_{}(n) 2_{^{}}(n)\)._

Let us describe the main idea of Theorem 3.8, whose proof is deferred to Appendix C.1. The crucial step lies in the creation of the label space \(=R S\), where \(R\) is an infinite set whose points are all distance 2 apart, and \(S\) is an infinite set whose elements are indexed by the finite subsets of \(R\), e.g., as in \(s_{R^{}} S\) for finite \(R^{} R\). For all such \(R^{}\), define \(s_{R^{}}\) to be distance 1 from the elements of \(R^{}\), distance 2 from the other elements of \(R\), and distance 1 from all other points in \(S\). Then \(\) indeed forms a metric space, and it is straightforward to see that, for instance, the class of all functions from a one-element set to \(\) is more difficult to learn than its finite projections (equivalently, finite subsets).

We now prove a matching upper bound to Theorem 3.8, demonstrating that a factor of 2 is the greatest possible gap between the error rate of \(\) and its projections when the loss function is a metric.

**Theorem 3.9**.: _Let \(\) be a label set with a metric loss function and \(^{}\) a hypothesis class. Fix \(:_{ 0}\), and suppose that for any finite \(X\) and finite \(^{}|_{X}\), \(^{}\) has transductive error rate \(_{^{}}\). Then \(\) has transductive error rate \(_{} 2\)._

The proof of Theorem 3.9 is deferred to Appendix C.2, but let us briefly sketch the main idea. Fix \(n\), \(S^{n}\), and set \(=(n)\). Consider again the collection of functions \(R=|_{S}\) and variables \(L=_{S^{} S,|S^{}|=n-1}|_{S^{}}\), as described in the proof of Theorem 3.6. By the premise of the theorem, for any finite subset \(R^{} R\), there exists an assignment of variables \(L\) which maintains all functions in \(R^{}\) below \(\). Each such assignment induces an _apportionment_ of error to each function in \(R^{}\), i.e., a vector of length \(n\) with positive entries summing to \(\). For \(r R^{}\) depending upon variables \(_{1},,_{n}\), this apportionment tracks the contribution of each \(_{i}\) to the output of \(r\). The central technical step of the proof is to demonstrate that one can assign apportionments to each node \(r R\) such that any finite subset of the apportionments can be satisfied by an assignment of variables \(L\). Then let \(=(y_{1},,y_{i-1},?,y_{i+1},,y_{n})\) be a variable. We assign \(\) to the value \(\) such that the function \((y_{1},,y_{i},,y_{i+1},,y_{n})\) has minimal budget apportioned to \(\), among all such \(\). From an invocation of the triangle inequality, this learner at most doubles the output of any \(r R\).

Recall from Section 3.1 that proper metric spaces are sufficiently expressive to describe many of the most frequently studied label spaces, including \(^{d}\) (equipped with any norm) and its closed subsets.

What, then, is a typical example of a label space which fails to be proper? Perhaps the most natural example is multiclass classification over infinite label sets, i.e., \(\) equipped with the discrete metric \(_{0-1}(y,y^{})=[y y^{}]\). We will now demonstrate, however, that the particular structure of multiclass classification can be exploited to recover an exact compactness result in the style of Theorem 3.6. Notably, we do so by invoking M. Hall's classic matching theorem for infinite graphs, which for good measure we show to be a special case of our Theorem 3.3.

**Definition 3.10**.: _Let \(G=(L R,E)\) be a bipartite graph. An \(R\)**-matching** is a set \(E^{} E\) of disjoint edges which covers \(R\). A graph with an \(R\)-matching is said to be \(R\)**-matchable**._

**Definition 3.11**.: _A bipartite graph \(G=(L R,E)\) is **finitely \(R\)-matchable** if for each finite subset \(R^{}\) of \(R\), there exists a set \(E^{} E\) of disjoint edges which covers \(R^{}\)._

M. Hall's theorem states that an infinite bipartite graph \(G\) is \(R\)-matchable if and only if it is finitely \(R\)-matchable, provided that all nodes in \(R\) have finite degree. Before proving M. Hall's theorem by way of Theorem 3.3, we establish an intermediate lemma.

**Lemma 3.12**.: _Let \(G=(L R,E)\) be a bipartite graph such that all nodes \(r R\) have finite degree and \(G\) is finitely \(R\)-matchable. Then there exists a collection of edges \(E^{} E\) such that \(G^{}=(L R,E^{})\) is finitely \(R\)-matchable and all nodes in \(G^{}\) have finite degree._

The proof of Lemma 3.12 is deferred to Appendix C.3, but its intuition is fairly simple: by P. Hall's theorem, \(G\) is finitely \(R\)-matchable precisely when Hall's condition holds, i.e., \(|N(R^{})||R^{}|\) for all finite \(R^{} R\). Thus any \( L\) which is not incident to a Hall blocking set can be removed from \(G\) while preserving Hall's condition and finite \(R\)-matchability. Proceeding in this way, nodes can be removed until each remaining \( L\) is contained in a Hall blocking set \(R^{}_{}\). At this point, \(\)'s incident edges can be safely restricted to those which are incident with \(R^{}_{}\), a finite set.

We now prove M. Hall's theorem as a consequence of our Theorem 3.3.

**Theorem 3.13** (Hall Jr).: _Let \(G=(L R,E)\) be a bipartite graph in which all nodes \(r R\) have finite degree. Then \(G\) has an \(R\)-matching if and only if it is finitely \(R\)-matchable._

Proof.: The forward direction is clear. For the reverse, suppose \(G\) is finitely \(R\)-matchable. Then we may assume as a consequence of Lemma 3.12 that the nodes in \(L\) have finite degree as well. Let us think of each node \( L\) as a variable residing in the discrete metric space on its neighbors. We will also think of each node \(r R\) as a function of its neighbors, which outputs the number of neighbors that have not been assigned to \(r\) itself. Note that the discrete metric space on finitely many elements is proper, and furthermore that any function out of such a space is automatically proper. Then invoke Theorem 3.3 with \((r)=1-\) to complete the proof. (See Remark 3.5.) 

**Corollary 3.14**.: _Let \(^{}\) be a classification problem, i.e., employing the 0-1 loss function. Then the following are equivalent for any \(m_{>0}\):_

1. \(\) _is learnable in the realizable case with_ \(m\)_._
2. _For any finite_ \(X\) _and finite_ \(^{}|_{X}\)_,_ \(^{}\) _is learnable in the realizable case with transductive sample function_ \(m\)_._

Proof.: Certainly \((1.)(2.)\). Then suppose (2.) and fix \(S^{n}\). Now consider the bipartite graph \(G=(L R,E)\) with \(R=|_{S}\), \(L=_{S^{} S,|S^{}|=n-1}|_{S^{}}\), and where edges in \(E\) connect functions agreeing on common inputs. Then a learner for instances of the form \(\{(S,h)\}_{h}\) amounts precisely to a choice of incident node (equivalently, edge) for each \( L\). Furthermore, such a learner attains error \(\) precisely when its selected edges contribute indeget at least \(d=n(1-)\) to each node in \(R\). Using a splitting argument (i.e., creating \(d\) copies of each node in \(R\)), this is equivalent to asking for an \(R\)-perfect matching in a graph which, by (2.), is finitely \(R\)-matchable. Note that each node in \(R\) has degree \(n<\) and appeal to Theorem 3.13 to complete the proof.

### Agnostic Learning

Our discussion thus far has restricted attention to realizable learning: what can be said of the agnostic case? In short, all results from Sections 3.1 and 3.2 can be claimed for agnostic learning (with nearly identical proofs), with the exception of Theorem 3.9. To begin, let us briefly review transductive learning in the agnostic case. See Asilis et al. (2024) or Dughmi et al. (2024) for further detail.

**Definition 3.15**.: _The setting of **transductive learning in the agnostic case** is defined as follows:_

1. _An adversary selects a collection of_ \(n\) _labeled datapoints_ \(S()^{<}\)_._
2. _The unlabeled datapoints in_ \(S\) _are all revealed to the learner._
3. _One labeled datapoint_ \((x_{i},y_{i})\) _is selected uniformly at random from_ \(S\)_. The remaining labeled datapoints_ \(S_{-i}\) _are displayed to the learner._
4. _The learner is prompted to predict the label of_ \(x_{i}\)_._

Notably, transductive learning in the agnostic case differs from the realizable case in that the adversary is no longer restricted to label the datapoints in \(S\) using a hypothesis \(\). To compensate for the increased difficulty, and in accordance with the PAC definition of agnostic learning, a learner is only judged relative to best-in-class performance across \(\). Formally,

\[L_{S}^{}(A)=_{i[n]}(A(S_{-i})(x_{i}),y_{i} )-_{h}_{i[n]}(h(x_{i}),y_{i}).\]

Furthermore, one can use nearly identical reasoning as in the proofs of Theorems 3.6 and 3.7 to see that agnostic transductive learning is described by a system of variables \(L\) and functions \(R\). In particular, set \(R=^{n}\) and let \(L(\{?\})^{n}\) contain all sequences with exactly one "\(?\)". Then a function \(r=(y_{1},,y_{n})\) depends upon the variables \(\{_{i}\}_{i[n]}\), where \(_{i}=(y_{1},,y_{i-1},?,y_{i+1},,y_{n})\) and \(r(_{1},,_{n})=_{i[n]}d(y_{i},_{ i})-_{h}_{i[n]}(h(x_{i}),y_{i})\).

Now, as in the realizable case, a learner \(A\) corresponds precisely to an assignment of each variable \( L\) to a value in \(\), and \(A\) incurs agnostic transductive error at most \(\) if and only if the outputs of all nodes in \(R\) are maintained below \(\). Under the conditions of Theorems 3.6 or 3.7, exact compactness of sample complexity thus comes as an immediate consequence of Theorem 3.3 and our preceding discussion. Furthermore, when \(\) bears a discrete metric, learning reduces to an assignment problem in graphs, and exact compactness follows from a straightforward splitting argument applied to M. Hall's matching theorem (as in Corollary 3.14). We thus have the following theorem.

**Theorem 3.16**.: _Let \(\) and the loss function satisfy the conditions of Theorem 3.6, Theorem 3.7, or Corollary 3.14. Then the following conditions are equivalent for any \(^{}\) and \(m_{>0}\):_

1. \(\) _is learnable in the agnostic case with transductive sample function_ \(m\)_._
2. _For any finite_ \(X\) _and finite_ \(^{}|_{X}\)_,_ \(^{}\) _is learnable in the agnostic case with transductive sample function_ \(m\)_._

Regarding improper metric losses, note that our lower bound from Theorem 3.8 transfers directly to the agnostic case, as it established for a hypothesis class for which agnostic learning is precisely as difficult as realizable learning. We conjecture that larger differences in such error rates -- perhaps of arbitrarily large ratio -- are possible for the agnostic case.

### PAC Learning

Though our results have thus far been phrased in the language of transductive learning, we now demonstrate that they may be easily extended (in an approximate manner) to Valiant's celebrated PAC model (Valiant, 1984). The PAC model makes use of probability measures \(D\) over \(\), for which the _true error_ incurred by a predictor \(h\) is defined as \(L_{D}(h)=_{(x,y) D}\,(h(x),y)\).

**Definition 3.17**.: _Let \(\) be a collection of probability measures over \(\) and \(^{}\) a hypothesis class. A learner \(A\) is a **PAC learner for \(\) with respect to \(\)** if there exists a **sample function**\(m:(0,1)^{2}\) such that the following holds: for any \(D\) and \(,(0,1)^{2}\), a \(D\)-i.i.d. sample \(S\) with \(|S| m(,)\) is such that, with probability at least \(1-\) over the choice of \(S\),_

\[L_{D}(A(S))_{}L_{D}(h)+.\]_Agnostic PAC learning_ refers to the case in which \(\) consists of all measures over \(\), and **realizable PAC learning** to the case in which \(=\{D:_{}L_{D}(h)=0\}\).

**Definition 3.18**.: _The **sample complexity** of a learner \(A\) with respect to a hypothesis class \(\), \(m_{,A}:(0,1)^{2}\), is the minimal sample function it attains as a learner for \(\). The **sample complexity** of a class \(\) is the pointwise minimal sample complexity attained by any of its learners, i.e., \(m_{,}(,)=_{A}m_{,A}( ,)\)._

As previously mentioned, transductive learning bears a close connection to PAC learning: see Asilis et al. (2024) and Dughmi et al. (2024) for further detail on their approximate equivalence.

**Lemma 3.19** (Asilis et al. (2024, Proposition 3.6)).: _Let \(\) be a domain, \(\) a label set, and \(^{}\) a hypothesis class. Fix a loss function taking values in \(\). Then the following inequality holds for all \(,(0,1)\) and the constant \(e 2.718\):_

\[m_{,}(e(+)) m_{, }(,) Om_{,}( /2)(1/).\]

We now follow through on porting our results from the transductive model to the PAC model. The following is an immediate consequence of applying Lemma 3.19 to Theorems 3.9 and 3.7.

**Corollary 3.20**.: _Let \(\) be a domain, \(\) a label set, and \(^{}\) a hypothesis class. Suppose that the loss function \(\) is bounded and satisfies either of the following conditions:_

* \(\) _is a metric on_ \(\)_, or_
* \((,d)\) _is a compact metric space and_ \(\) _is continuous with respect to this topology._

_Then if all finite projections of \(\) are learnable with realizable PAC sample function \(m:(0,1)^{2}\), \(\) is learnable with sample complexity \(Om(,)(1/)\)._

Let us mention briefly that the connection between transductive learning and PAC learning may not be as tight in the agnostic case as in the realizable case. Through a straightforward use of Markov's inequality and a repetition argument, one can show that agnostic PAC sample complexities exceed transductive by at most a factor of \(1/\), but this is an unimpressive bound.

## 4 Conclusion

In this work, we studied the conditions under which the sample complexity of learning a class \(\) can be detected by examining its finite projections. Notably, we established exact compactness results for transductive learning with a broad class of proper or continuous loss functions, across both realizable and agnostic learning. Using bounds relating the transductive and PAC models, we were able to transfer many of our results (in an approximate form) to realizable PAC learning. We leave as an open problem whether compactness of agnostic transductive sample complexities can fail by more than a factor of 2 for arbitrary (improper) metric losses. Additional future work includes better understanding the relationship between the transductive and PAC models in the agnostic case, and examining compactness for loss functions which do not satisfy any of our properness, metric, or continuity conditions (though they may be of somewhat limited interest in learning theory). It would also be of interest to study the compactness of error rates in settings other than supervised learning, such as online or unsupervised learning.