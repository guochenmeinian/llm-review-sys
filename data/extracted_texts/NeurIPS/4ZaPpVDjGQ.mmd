# Breaking the Communication-Privacy-Accuracy Tradeoff with \(f\)-Differential Privacy

Richeng Jin1 Zhonggen Su1 Caijun Zhong1

Zhaoyang Zhang1 Tony Q.S. Quek2 Huaiyu Dai3

###### Abstract

We consider a federated data analytics problem in which a server coordinates the collaborative data analysis of multiple users with privacy concerns and limited communication capability. The commonly adopted compression schemes introduce information loss into local data while improving communication efficiency, and it remains an open problem whether such discrete-valued mechanisms provide any privacy protection. In this paper, we study the local differential privacy guarantees of discrete-valued mechanisms with finite output space through the lens of \(f\)-differential privacy (DP). More specifically, we advance the existing literature by deriving tight \(f\)-DP guarantees for a variety of discrete-valued mechanisms, including the binomial noise and the binomial mechanisms that are proposed for privacy preservation, and the sign-based methods that are proposed for data compression, in closed-form expressions. We further investigate the amplification in privacy by sparsification and propose a ternary stochastic compressor. By leveraging compression for privacy amplification, we improve the existing methods by removing the dependency of accuracy (in terms of mean square error) on communication cost in the popular use case of distributed mean estimation, therefore breaking the three-way tradeoff between privacy, communication, and accuracy.

## 1 Introduction

Nowadays, the massive data generated and collected for analysis, and consequently the prohibitive communication overhead for data transmission, are overwhelming the centralized data analytics paradigm. Federated data analytics is, therefore, proposed as a new distributed computing paradigm that enables data analysis while keeping the raw data locally on the user devices . Similarly to its most notable use case, i.e., federated learning (FL) [2; 3], federated data analytics faces two critical challenges: data privacy and communication efficiency. On one hand, the local data of users may contain sensitive information, and privacy-preserving mechanisms are needed. On the other hand, the user devices are usually equipped with limited communication capabilities, and compression mechanisms are often adopted to improve communication efficiency.

Differential privacy (DP) has become the gold standard for privacy measures due to its rigorous foundation and simple implementation. One classic technique to ensure DP is adding Gaussian or Laplacian noises to the data . However, they are prone to numerical errors on finite-precision computers  and may not be suitable for federated data analytics with communication constraints due to their continuous nature. With such consideration, various discrete noises with privacy guarantees have been proposed, e.g., the binomial noise , the discrete Gaussian mechanism , and the Skellam mechanism . Nonetheless, the additive noises in  and  assume infinite range, which renders them less communication-efficient without appropriate clipping. Unfortunately, clippingusually ruins the unbiasedness of the mechanism.  develops a Poisson binomial mechanism (PBM) that does not rely on additive noise. In PBM, each user adopts a binomial mechanism, which takes a continuous input and encodes it into the success probability of a binomial distribution. The output of the binomial mechanism is shared with a central server which releases the aggregated result that follows the Poisson binomial distribution. However,  focuses on distributed DP in which the server only observes the output of the aggregated results instead of the data shared by each individual user, and therefore, requires a secure computation function (e.g., secure aggregation ).

In addition to discrete DP mechanisms, existing works have investigated the fundamental tradeoff between communication, privacy, and accuracy under the classic \((,)\)-DP framework (e.g., [10; 11; 12; 13]). Notably, in the case of distributed mean estimation,  incorporates Kashin's representation and proposed Subsampled and Quantized Kashin's Response (SQKR), which achieves order-optimal mean square error (MSE) that has a linear dependency on the dimension of the private data \(d\). SQKR first computes Kashin's representation of the private data and quantizes each coordinate into a 1-bit message. Then, \(k\) coordinates are randomly sampled and privatized by the \(2^{k}\)-Random Response mechanism . SQKR achieves an order-optimal three-way tradeoff between privacy, accuracy, and communication. Nonetheless, it does not account for the privacy introduced during sparsification.

Intuitively, as compression becomes more aggressive, less information will be shared by the users, which naturally leads to better privacy protection. However, formally quantifying the privacy guarantees of compression mechanisms remains an open problem. In this work, we close the gap by investigating the local DP guarantees of discrete-valued mechanisms, based on which a ternary stochastic compressor is proposed to leverage the privacy amplification by compression and advance the literature by achieving a better communication-privacy-accuracy tradeoff. More specifically, we focus on the emerging concept of \(f\)-DP  that can be readily converted to \((,)\)-DP and Renyi differential privacy  in a lossless way while enjoying better composition property .

**Our contributions**. In this work, we derive the closed-form expressions of the tradeoff function between type I and type II error rates in the hypothesis testing problem for a generic discrete-valued mechanism with a finite output space, based on which \(f\)-DP guarantees of the binomial noise (c.f. Section 4.1) and the binomial mechanism (c.f. Section 4.2) that covers a variety of discrete differentially private mechanisms and compression mechanisms as special cases are obtained. Our analyses lead to tighter privacy guarantees for binomial noise than  and extend the results for the binomial mechanism in  to local DP. To the best of our knowledge, this is the first work that investigates the \(f\)-DP guarantees of discrete-valued mechanisms, and the results could possibly inspire the design of better differentially private compression mechanisms.

Inspired by the analytical results, we also leverage the privacy amplification of the sparsification scheme and propose a ternary stochastic compressor (c.f. Section 5). By accounting for the privacy amplification of compression, our analyses reveal that given a privacy budget \(\)-GDP (which is a special case of \(f\)-DP) with \(<\) (in which \(r\) is the ratio of non-zero coordinates in expectation for the sparsification scheme), the MSE of the ternary stochastic compressor only depends on \(\) in the use case of distributed mean estimation (which is the building block of FL). In this sense, we break the three-way tradeoff between communication overhead, privacy, and accuracy by removing the dependency of accuracy on the communication overhead. Different from existing works which suggest that, in the high privacy regime, the error introduced by compression is dominated by the error introduced for privacy, we show that the error caused by compression could be translated into enhancement in privacy. Compared to SQKR , the proposed scheme yields better privacy guarantees given the same MSE and communication cost. For the scenario where each user \(i\) observes \(x_{i}\{-c,c\}^{d}\) for some constant \(c>0\), the proposed scheme achieves the same privacy guarantee and MSE as those of the classic Gaussian mechanism in the large \(d\) regime, which essentially means that the improvement in communication efficiency is achieved for free. We remark that the regime of large \(d\) is often of interest in practical FL in which \(d\) is the number of training parameters.

## 2 Related Work

Recently, there is a surge of interest in developing differentially private data analysis techniques, which can be divided into three categories: central differential privacy (CDP) that assumes a trusted central server to perturb the collected data , distributed differential privacy that relies on secure aggregation during data collection , and local differential privacy (LDP) that avoids the need for the trusted server by perturbing the local data on the user side . To overcome the drawbacks of the Gaussian and Laplacian mechanisms, several discrete mechanisms have been proposed.  introduces the one-dimensional binomial noise, which is extended to the general \(d\)-dimensional case in  with more comprehensive analysis in terms of \((,)\)-DP.  analyzes the LDP guarantees of discrete Gaussian noise, while  further considers secure aggregation.  studies the Renyi DP guarantees of the Skellam mechanism. However, both the discrete Gaussian mechanism and the Skellam mechanism assume infinite ranges at the output, which makes them less communication efficient without appropriate clipping. Moreover, all the above three mechanisms achieve differential privacy at the cost of exploding variance for the additive noise in the high-privacy regimes.

Another line of studies jointly considers privacy preservation and compression. [10; 11] propose to achieve DP by quantizing, sampling, and perturbing each entry, while  proposes a vector quantization scheme with local differential privacy. However, the MSE of these schemes grows with \(d^{2}\).  investigates the three-way communication-privacy-accuracy tradeoff and incorporates Kashin's representation to achieve order-optimal estimation error in mean estimation.  proposes to first sample a portion of coordinates, followed by the randomized response mechanism .  and  further incorporate shuffling for privacy amplification.  proposes to compress the LDP schemes using a pseudorandom generator, while  utilizes minimal random coding.  proposes a privacy-aware compression mechanism that accommodates DP requirement and unbiasedness simultaneously. However, they consider pure \(\)-DP, which cannot be easily generalized to the relaxed variants.  proposes the Poisson binomial mechanism with Renyi DP guarantees. Nonetheless, Renyi DP lacks the favorable hypothesis testing interpretation and the conversion to \((,)\)-DP is lossy. Moreover, most of the existing works focus on privatizing the compressed data or vice versa, leaving the privacy guarantees of compression mechanisms largely unexplored.  proposes a numerical accountant based on fast Fourier transform  to evaluate \((,)\)-DP of general discrete-valued mechanisms. Recently, an independent work  studies privacy amplification by compression for central \((,)\)-DP and multi-message shuffling frameworks. In this work, we consider LDP through the lens of \(f\)-DP and eliminate the need for a trusted server or shuffler.

Among the relaxations of differential privacy notions [31; 16; 32], \(f\)-DP  is a variant of \(\)-DP with hypothesis testing interpretation, which enjoys the property of lossless conversion to \((,)\)-DP and tight composition . As a result, it leads to favorable performance in distributed/federated learning [34; 35]. However, to the best of our knowledge, none of the existing works study the \(f\)-DP of discrete-valued mechanisms. In this work, we bridge the gap by deriving tight \(f\)-DP guarantees of various compression mechanisms in closed form, based on which a ternary stochastic compressor is proposed to achieve a better communication-privacy-accuracy tradeoff than existing methods.

## 3 Problem Setup and Preliminaries

### Problem Setup

We consider a set of \(N\) users (denoted by \(\)) with local data \(x_{i}^{d}\). The users aim to share \(x_{i}\)'s with a central server in a privacy-preserving and communication-efficient manner. More specifically, the users adopt a privacy-preserving mechanism \(\) to obfuscate their data and share the perturbed results \((x_{i})\)'s with the central server. In the use case of distributed/federated learning, each user has a local dataset \(S\). During each training step, it computes the local stochastic gradients and shares the obfuscated gradients with the server. In this sense, the overall gradient computation and obfuscation mechanism \(\) takes the local dataset \(S\) as the input and outputs the obfuscated result \((S)\). Upon receiving the shared \((S)\)'s, the server estimates the mean of the local gradients.

### Differential Privacy

Formally, differential privacy is defined as follows.

**Definition 1** (\((,)\)-DP ).: _A randomized mechanism \(\) is \((,)\)-differentially private if for all neighboring datasets \(S\) and \(S^{}\) and all \(O\) in the range of \(\), we have_

\[P((S) O) e^{}P((S^{}) O)+,\] (1)

_in which \(S\) and \(S^{}\) are neighboring datasets that differ in only one record, and \(, 0\) are the parameters that characterize the level of differential privacy._

### \(f\)-Differential Privacy

Assuming that there exist two neighboring datasets \(S\) and \(S^{}\), from the hypothesis testing perspective, we have the following two hypotheses

\[H_{0}:S,\ \ H_{1}:S^{}.\] (2)

Let \(P\) and \(Q\) denote the probability distribution of \((S)\) and \((S^{})\), respectively.  formulates the problem of distinguishing the two hypotheses as the tradeoff between the achievable type I and type II error rates. More precisely, consider a rejection rule \(0 1\) (which rejects \(H_{0}\) with a probability of \(\)), the type I and type II error rates are defined as \(_{}=_{P}[]\) and \(_{}=1-_{Q}[]\), respectively. In this sense, \(f\)-DP characterizes the tradeoff between type I and type II error rates. The tradeoff function and \(f\)-DP are formally defined as follows.

**Definition 2** (tradeoff function ).: _For any two probability distributions \(P\) and \(Q\) on the same space, the tradeoff function \(T(P,Q):\) is defined as \(T(P,Q)()=\{_{}:_{}\}\), where the infimum is taken over all (measurable) rejection rule \(\)._

**Definition 3** (\(f\)-DP ).: _Let \(f\) be a tradeoff function. With a slight abuse of notation, a mechanism \(\) is \(f\)-differentially private if \(T((S),(S^{})) f\) for all neighboring datasets \(S\) and \(S^{}\), which suggests that the attacker cannot achieve a type II error rate smaller than \(f()\)._

\(f\)-DP can be converted to \((,)\)-DP as follows.

**Lemma 1**.: _[_15_]_ _A mechanism is \(f()\)-differentially private if and only if it is \((,)\)-differentially private with_

\[f()=\{0,1--e^{},e^{-}(1--)\}.\] (3)

Finally, we introduce a special case of \(f\)-DP with \(f()=(^{-1}(1-)-)\), which is denoted as \(\)-GDP. More specifically, \(\)-GDP corresponds to the tradeoff function of two normal distributions with mean 0 and \(\), respectively, and a variance of 1.

## 4 Tight \(f\)-DP Analysis for Existing Discrete-Valued Mechanisms

In this section, we derive the \(f\)-DP guarantees for a variety of existing differentially private discrete-valued mechanisms in the scalar case (i.e., \(d=1\)) to illustrate the main ideas. The vector case will be discussed in Section 6. More specifically, according to Definition 3, the \(f\)-DP of a mechanism \(\) is given by the infimum of the tradeoff function over all neighboring datasets \(S\) and \(S^{}\), i.e., \(f()=_{S,S^{}}_{}\{_{}():_{} \}\). Therefore, the analysis consists of two steps: 1) we obtain the closed-form expressions of the tradeoff functions, i.e., \(_{}\{_{}():_{}\}\), for a generic discrete-valued mechanism (see Section A in the supplementary material); and 2) given the tradeoff functions, we derive the \(f\)-DP by identifying the mechanism-specific infimums of the tradeoff functions over all possible neighboring datasets. We remark that the tradeoff functions for the discrete-valued mechanisms are essentially piece-wise functions with both the domain and range of each piece determined by both the mechanisms and the datasets, which renders the analysis for the second step highly non-trivial.

### Binomial Noise

In this subsection, we consider the binomial noise (i.e., Algorithm 1) proposed in , which serves as a communication-efficient alternative to the classic Gaussian noise. More specifically, the output of stochastic quantization in  is perturbed by a binomial random variable.

``` Input:\(x_{i}[0,1,,l]\), \(i\), number of trials \(M\), success probability \(p\).  Privatization:\(Z_{i} x_{i}+Binom(M,p)\). ```

**Algorithm 1** Binomial Noise 

**Theorem 1**.: _Let \(=Binom(M,p)\), the binomial noise mechanism in Algorithm 1 is \(f^{bn}()\)-differentially private with_

\[f^{bn}()=\{^{+}_{,}(),^{-}_{,}( )\},\] (4)in which_

\[^{+}_{,}()=P(+l)++l)P(<)}{P(=)}-= +l)}{P(Z=)},\\ [P(<),P()], [0,M-l],\\ 0,[P( M-l),1].\] (5)

\[^{-}_{,}()=P(-l)+=-l)P(>)}{P(=)}-=-l)}{P(Z=)},\\ [P(>),P()], [l,M],\\ 0,[P( l),1].\] (6)

_Given that \(P(=k)=p^{k}(1-p)^{M-k}\), it can be readily shown that when \(p=0.5\), both \(^{+}_{,}()\) and \(^{-}_{,}()\) are maximized, and \(f()=^{+}_{,}()=^{-}_{,}()\)._

Fig. 1 shows the impact of \(M\) when \(l=8\), which confirms the result in  that a larger \(M\) provides better privacy protection (recall that given the same \(\), a larger \(_{}\) indicates that the attacker makes mistakes in the hypothesis testing more likely and therefore corresponds to better privacy protection). Note that the output of Algorithm 1\(Z_{i}\{0,1,...,M+l\}\), which requires a communication overhead of \(_{2}(M+l+1)\) bits. We can readily convert \(f()\)-DP to \((,)\)-DP by utilizing Lemma 1.

**Remark 1**.: _The results derived in this work improve  in two aspects: (1) Theorem 1 in  requires \(Mp(1-p)(23(10d/),2l/s)>(23(10),2l/s)\), in which \(1/s\) is some scaling factor. When \(p=1/2\), it requires \(M 212\). More specifically, for \(M=500\),  requires \(>0.044\). Our results imply that there exists some \((,)\) such that Algorithm 1 is \((,)\)-DP as long as \(M>l\). For \(M=500\), \(\) can be as small as \(4.61 10^{-136}\). (2) Our results are tight, in the sense that no relaxation is applied in our derivation. As an example, when \(M=500\) and \(p=0.5\), Theorem 1 in  gives \((3.18,0.044)\)-DP while Theorem 1 in this paper yields \((1.67,0.039)\)-DP._

### Binomial Mechanism

``` Input:\(c>0\), \(x_{i}[-c,c]\), \(M\), \(p_{i}(x_{i})[p_{min},p_{max}]\)  Privatization:\(Z_{i} Binom(M,p_{i}(x_{i}))\). ```

**Algorithm 2** Binomial Mechanism 

In this subsection, we consider the binomial mechanism (i.e., Algorithm 2). Different from Algorithm 1 that perturbs the data with noise following the binomial distribution with the same success probability, the binomial mechanism encodes the input \(x_{i}\) into the success probability of the binomial distribution. We establish the privacy guarantee of Algorithm 2 as follows.

**Theorem 2**.: _The binomial mechanism in Algorithm 2 is \(f^{bm}()\)-differentially private with_

\[f^{bm}()=\{^{+}_{,}(),^{-}_{,}( )\},\] (7)

_in which_

\[^{+}_{,}()=1-[P(Y<k)+ P(Y=k)]=P(Y k)+-,\]

_for \([P(X<k),P(X k)]\) and \(k\{0,1,2,,M\}\), where \(X=Binom(M,p_{max})\) and \(Y=Binom(M,p_{min})\), and_

\[^{-}_{,}()=1-[P(Y>k)+ P(Y=k)]=P(Y k)+-,\]

_for \([P(X>k),P(X k)]\) and \(k\{0,1,2,,M\}\), where \(X=Binom(M,p_{min})\) and \(Y=Binom(M,p_{max})\). When \(p_{max}=1-p_{min}\), we have \(^{+}_{,}()=^{-}_{,}()\)._

**Remark 2** (**Comparison to )**.: _The binomial mechanism is part of the Poisson binomial mechanism proposed in . More specifically, in , each user \(i\) shares the output of the binomial mechanism

Figure 1: Impact of \(M\) on Algorithm 1 with \(l=8\).

\(Z_{i}\) with the server, in which \(p_{i}(x_{i})=+x_{i}\) and \(\) is some design parameter. It can be readily verified that \(p_{max}=1-p_{min}\) in this case. The server then aggregates the result through \(=(_{i}Z_{i}-)\).  requires secure aggregation and considers the privacy leakage of releasing \(\), while we complement it by showing the LDP, i.e., the privacy leakage of releasing \(Z_{i}\) for each user. In addition, we eliminate the constraint \([0,]\), and the results hold for any selection of \(p_{i}(x_{i})\). Moreover, the privacy guarantees in Theorem 2 are tight since no relaxation is involved. Fig. 2 shows the impact of \(M\) on the privacy guarantee. In contrast to binomial noise, the privacy of the binomial mechanisms improves as \(M\) (and equivalently communication overhead) decreases, which implies that it is more suitable for communication-constrained scenarios. We also derive the \(f\)-DP of the Poisson binomial mechanism, which are presented in Section C in the supplementary material._

In the following, we present two existing compressors that are special cases of the binomial mechanism.

**Example 1**.: _We first consider the following stochastic sign compressor proposed in ._

**Definition 4** (Two-Level Stochastic Compressor ).: _For any given \(x[-c,c]\), the compressor \(sto\)-\(sign\) outputs_

\[stosign(x,A)=1,,\\ -1,,\] (8)

_where \(A>c\) is the design parameter that controls the level of stochasticity._

_With a slight modification (i.e., mapping the output space from \(\{0,1\}\) to \(\{-1,1\}\)), \(sto\)-\(sign(x,A)\) can be understood as a special case of the binomial mechanism with \(M=1\) and \(p_{i}(x_{i})=}{2A}\). In this case, we have \(p_{max}=\) and \(p_{min}=\). Applying the results in Theorem 2 yields_

\[f^{stosign}()=^{+}_{,}()=^{-}_{ ,}()=1- ,[0,],\\ -,\ [,1]. \] (9)

_Combining (9) with (3) suggests that the \(sto\)-\(sign\) compressor ensures \(((),0)\)-DP._

**Example 2**.: _The second sign-based compressor that we examine is \(CLDP_{}()\)._

**Definition 5** (\(CLDP_{}()\)).: _For any given \(x[-c,c]\), the compressor \(CLDP_{}()\) outputs \(CLDP_{}()\), which is given by_

\[CLDP_{}()=+1,+ -1}{e^{}+1},\\ -1,--1}{e^{ }+1}.\] (10)

\(CLDP_{}()\) _can be understood as a special case of \(sto\)-\(sign(x,A)\) with \(A=+1)}{e^{}-1}\). In this case, according to (9), we have_

\[f^{CLDP_{}}()=1-e^{},[0,],\\ e^{-}(1-),\ [,1].\] (11)

_Combining the above result with (3) suggests that \(CLDP_{}()\) ensures \((,0)\)-DP, which recovers the result in . It is worth mentioning that \(CLDP_{}()\) can be understood as the composition of \(sto\)-\(sign\) with \(A=c\) followed by the randomized response mechanism , and is equivalent to the one-dimensional case of the compressor in . Moreover, the one-dimensional case of the schemes in  can also be understood as special cases of \(sto\)-\(sign\)._

## 5 The Proposed Ternary Compressor

The output of the binomial mechanism with \(M=1\) lies in the set \(\{0,1\}\), which coincides with the sign-based compressor. In this section, we extend the analysis to the ternary case, which can be understood as a combination of sign-based quantization and sparsification (when the output takes value 0, no transmission is needed since it does not contain any information) and leads to improved communication efficiency. More specifically, we propose the following ternary compressor.

Figure 2: Impact of \(M\) on Algorithm 2.

**Definition 6** (**Ternary Stochastic Compressor**).: _For any given \(x[-c,c]\), the compressor \(ternaricy\) outputs \(ternary(x,A,B)\), which is given by_

\[ternary(x,A,B)=1,,\\ 0,1-,\\ -1,,\] (12)

_where \(B>A>c\) are the design parameters that control the level of sparsity._

For the ternary stochastic compressor in Definition 6, we establish its privacy guarantee as follows.

**Remark 3** (Privacy amplification by sparsification).: _It can be observed from (9) and (13) that \(f^{ternary}()>f^{sto-sign}\) when \([,1-]\), and \(f^{ternary}()=f^{sto-sign}\), otherwise. Fig. 3 shows \(f^{ternary}()\) and \(f^{sto-sign}\) for \(c=0.1\), \(A=0.25,B=0.5\), and the shaded gray area corresponds to the improvement in privacy. It can be observed that communication efficiency and privacy are improved simultaneously. It is worth mentioning that, if we convert the privacy guarantees to \((,0)\)-DP, we have \(=()\) for both compressors. However, the ternary compressor ensures \(((2),0.05)\)-DP (i.e., \(f^{ternary}()\{0,0.95-2,0.5(0.95-)\}\)) while the \(sto-sign\) compressor does not. We note that for the same \(A\), as \(B\) increases (i.e., communication cost decreases), \(f^{ternary}()\) approaches \(f()=1-\) (which corresponds to perfect privacy)._

In the following, we present a special case of the proposed ternary stochastic compressor.

**Example 3**.: _The ternary-based compressor proposed in  is formally defined as follows._

**Definition 7** (\(ternarize()\)).: _For any given \(x[-c,c]\), the compressor \(ternarize()\) outputs \(ternarize(x,B)=sign(x)\) with probability \(|x|/B\) and \(ternarize(x,B)=0\) otherwise, in which \(B>c\) is the design parameter._

\(ternarize(x,B)\) _can be understood as a special case of \(ternary(x,A,B)\) with \(A=|x|\). According to Theorem 3, \(f^{ternary}()=1--\) for \([0,1-]\) and \(f^{ternary}()=0\) for \([1-,1]\). Combining the above result with (3), we have \(=\) and \(=0\), i.e., \(ternarize()\) provides perfect privacy protection (\(=0\)) with a violation probability of \(=\). Specifically, the attacker cannot distinguish \(x_{i}\) from \(x_{i}^{}\) if the output of \(ternarize()=0\) (perfect privacy protection), while no differential privacy is provided if the output of \(ternarize() 0\) (violation of the privacy guarantee)._

**Remark 4**.: _It is worth mentioning that, in , the users transmit a scaled version of \(ternarize()\) and the scaling factor reveals the magnitude information of \(x_{i}\). Therefore, the compressor in  is not differentially private._

## 6 Breaking the Communication-Privacy-Accuracy Tradeoff

In this section, we extend the results in Section 5 to the vector case in two different approaches, followed by discussions on the three-way tradeoff between communication, privacy, and accuracy. The results in Section 4 can be extended similarly. Specifically, in the first approach, we derive the \(\)-GDP in closed form, while introducing some loss in privacy guarantees. In the second approach, a tight approximation is presented. Given the results in Section 5, we can readily convert \(f\)-DP in the scalar case to Gaussian differential privacy in the vector case as follows.

**Theorem 4**.: _Given a vector \(x_{i}=[x_{i,1},x_{i,2},,x_{i,d}]\) with \(|x_{i,j}| c, j\). Applying the ternary compressor to the \(j\)-th coordinate of \(x_{i}\) independently yields \(\)-GDP with \(=-2^{-1}()^{d}})\)._

**Remark 5**.: _Note that \(||x_{i}||_{2} c\) is a sufficient condition for \(|x_{i,j}| c, j\). In the proof of Theorem 4, we first convert \(f^{ternary}()\)-DP to \((,0)\)-DP for the scalar case, and then obtain \((d,0)\)-DP

Figure 3: Sparsification improves privacy.

_for the \(d\)-dimensional case, followed by the conversion to GDP. One may notice that some loss in privacy guarantee is introduced since the extreme case \(|x_{i,j}|=c, j\) actually violates the condition \(||x_{i}||_{2} c\). To address this issue, following a similar method in , one may introduce Kashin's representation to transform the \(l_{2}\) geometry of the data into the \(l_{}\) geometry. More specifically,  shows that for \(D>d\), there exists a tight frame \(U\) such that for any \(x^{d}\), one can always represent each \(x_{i}\) with \(y_{i}[-_{0}/,-_{0}/]^{D}\) for some \(_{0}\) and \(x_{i}=Uy_{i}\)._

In Theorem 4, some loss in privacy guarantees is introduced when we convert \(f\)-DP to \(\)-GDP. In fact, since each coordinate of the vector is processed independently, the extension from the scalar case to the \(d\)-dimensional case may be understood as the \(d\)-fold composition of the mechanism in the scalar case. The composed result can be well approximated or numerically obtained via the central limit theorem for \(f\)-DP in  or the Edgeworth expansion in . In the following, we present the result for the ternary compressor by utilizing the central limit theorem for \(f\)-DP.

**Theorem 5**.: _For a vector \(x_{i}=[x_{i,1},x_{i,2},,x_{i,d}]\) with \(|x_{i,j}| c, j\), the ternary compressor with \(B A>c\) is \(f^{ternary}()\)-DP with_

\[G_{}(+)- f^{ternary}() G_{}(- )+,\] (14)

_in which_

\[=c}{}},\ \ =|1+|^{3}+|1- |^{3}+(1-)||^{3}]}}{( -}{B^{2}})^{3/2}d^{1/2}}.\] (15)

Given the above results, we investigate the communication-privacy-accuracy tradeoff and compare the proposed ternary stochastic compressor with the state-of-the-art method SQKR in  and the classic Gaussian mechanism. According to the discussion in Remark 5, given the \(l_{2}\) norm constraint, Kashin's representation can be applied to transform it into the \(l_{}\) geometry. Therefore, for ease of discussion, we consider the setting in which each user \(i\) stores a vector \(x_{i}=[x_{i,1},x_{i,2},,x_{i,d}]\) with \(|x_{i,j}| c=}, j\), and \(||x_{i}||_{2} C\).

**Ternary Stochastic Compressor**: Let \(Z_{i,j}=ternary(x_{i,j},A,B)\), then \([BZ_{i,j}]=x_{i,j}\) and \(Var(BZ_{i,j})=AB-x_{i,j}^{2}\). In this sense, applying the ternary stochastic compressor to each coordinate of \(x_{i}\) independently yields an unbiased estimator with a variance of \(ABd-||x_{i}||_{2}^{2}\). The privacy guarantee is given by Theorem 5, and the communication overhead is \((_{2}(d)+1)d\) bits in expectation.

**SQKR**: In SQKR, each user first quantizes each coordinate of \(x_{i}\) to \(\{-c,c\}\) with 1-bit stochastic quantization. Then, it samples \(k\) coordinates (with replacement) and privrizes the \(k\) bit message via the \(2^{k}\) Random response mechanism with \(\)-LDP . The SQKR mechanism yields an unbiased estimator with a variance of \((+2^{k}-1}{e^{}-1})^{2}C^{2}-||x_{i}||_{2} ^{2}\). The privacy guarantee is \(\)-LDP, and the corresponding communication overhead is \((_{2}(d)+1)k\) bits.

**Gaussian Mechanism**: We apply the Gaussian mechanism (i.e., adding independent zero-mean Gaussian noise \(n_{i,j}(0,^{2})\) to \(x_{i,j}\)), followed by a sparsification probability of \(1-A/B\) as in \(ternary(x_{i,j},A,B)\), which gives \(Z_{i,j}^{Gauss}=(x_{i,j}+n_{i,j})\) with probability \(A/B\) and \(Z_{i,j}^{Gauss}=0\), otherwise. It can be observed that \([Z_{i,j}^{Gauss}]=x_{i,j}\) and \(Var(Z_{i,j}^{Gauss})=^{2}+(-1)x_{i,j}^{2}\). Therefore, the Gaussian mechanism yields an unbiased estimator with a variance of \(^{2}d+(-1)||x_{i}||_{2}^{2}\). By utilizing the post-processing property, it can be shown that the above Gaussian mechanism is \(c}{}\)-GDP , and the communication overhead is \((_{2}(d)+32)d\) bits in expectation.

**Discussion**: It can be observed that for SQKR, with a given privacy guarantee \(\)-LDP, the variance (i.e., MSE) depends on \(k\) (i.e., the communication overhead). When \(e^{} 2^{k}\) (which corresponds to the high privacy regime), the variance grows rapidly as \(k\) increases. For the proposed ternary stochastic compressor, it can be observed that both the privacy guarantee (in terms of \(\)-GDP) and the variance depend on \(AB\). Particularly, with a given privacy guarantee \(<\) for \(r=A/B\), the variance is given by \((4d/^{2}+1)C^{2}-||x_{i}||_{2}^{2}\), which remains the same regardless of the communication overhead. **In this sense, we essentially remove the dependency of accuracy on the communication overhead and therefore break the three-way tradeoff between communication 

**overhead, privacy, and accuracy.1** This is mainly realized by accounting for privacy amplification by sparsification. At a high level, when fewer coordinates are shared (which corresponds to a larger privacy amplification and a larger MSE), the ternary stochastic compressor introduces less ambiguity to each coordinate (which corresponds to worse privacy protection and a smaller MSE) such that both the privacy guarantee and the MSE remain the same. Since we use different differential privacy measures from  (i.e., \(\)-GDP in this work and \(\)-DP in ), we focus on the comparison between the proposed ternary stochastic compressor and the Gaussian mechanism (which is order-optimal in most parameter regimes, see ) in the following discussion and present the detailed comparison with SQKR in the experiments in Section 7.

Let \(AB=c^{2}+^{2}\), it can be observed that the \(f\)-DP guarantee of the ternary compressor approaches that of the Gaussian mechanism as \(d\) increases, and the corresponding variance is given by \(Var(BZ_{i,j})=^{2}+c^{2}-x_{i,j}^{2}\). When \(A=B\), i.e., no sparsification is applied, we have \(Var(BZ_{i,j})-Var(Z_{i,j}^{Gauss})=c^{2}-x_{i,j}^{2}\). Specifically, when \(x_{i,j}\{-c,c\}, 1 j d\), the ternary compressor demonstrates the same \(f\)-DP privacy guarantee and variance as that for the Gaussian mechanism, i.e., **the improvement in communication efficiency is obtained for free (in the large \(d\) regime**)**. When \(B>A\), we have \(Var(BZ_{i,j})-Var(Z_{i,j}^{Gauss})=(1-)^{2}+c^{2}-x_{i,j}^{2}\), and there exists some \(B\) such that the ternary compressor outperforms the Gaussian mechanism in terms of both variance and communication efficiency. It is worth mentioning that the privacy guarantee of the Gaussian mechanism is derived by utilizing the post-processing property. We believe that sparsification brings improvement in privacy for the Gaussian mechanism as well, which is, however, beyond the scope of this paper.

**Optimality:** It has been shown that, for \(k\)-bit unbiased compression mechanisms, there is a lower bound of \((C^{2}d/k)\) in MSE . For the proposed ternary compressor, the MSE and the communication cost are given by \(O(ABd)\) and \(A((d)+1)d/B\) bits, respectively. Let \(k=A((d)+1)d/B\), it achieves an MSE of \(O(A^{2}d^{2}((d)+1)/k)\). Since \(A>c=C/\), the MSE of the ternary compressor is given by \(O(C^{2}d((d)+1)/k)\), which implies that it is order-optimal up to a factor of \((d)\). Note that the factor of \((d)\) is used to represent the indices of coordinates that are non-zero, which can be eliminated by allowing for shared randomness between the users and the server.

## 7 Experiments

In this section, we examine the performance of the proposed ternary compressor in the case of distributed mean estimation. We follow the set-up of  and generate \(N=1000\) user vectors with dimension \(d=250\), i.e., \(x_{1},...,x_{N}^{250}\). Each local vector has bounded \(l_{2}\) and \(l_{}\) norms, i.e., \(||x_{i}||_{2} C=1\) and \(||x_{i}||_{} c=}\).

Fig. 4 compares the proposed ternary stochastic compressor with SQKR and the Gaussian mechanism. More specifically, the left figure in Fig. 4 compares the privacy guarantees (in terms of the tradeoff between type I and type II error rates) of the ternary stochastic compressor and SQKR given the

Figure 4: For the left figure, we set \(k=10\) and derive the corresponding variance for SQKR, based on which \(A\) and \(B\) for the ternary stochastic compressor are computed such that they have the same communication overhead and MSE in expectation. The middle and right figures show the tradeoff between \(\)-GDP and MSE. For the middle figure, we set \(\{,,,1,2,4,6,8,10\}\) for the Gaussian mechanism, given which \(A\) and \(B\) are computed such that \(AB=c^{2}+^{2}\) and the sparsity ratio is \(A/B\). For the right figure, we set \(A\{5c,10c,20c,30c\}\) and \(A/B\{0.2,0.4,0.6,0.8,1.0\}\), given which the corresponding \(\)â€™s are computed such that \(AB=c^{2}+^{2}\).

same communication overhead and MSE. It can be observed that the proposed ternary stochastic compressor outperforms SQKR in terms of privacy preservation, i.e., given the same type I error rate \(\), the type II error rate \(\) of the ternary stochastic compressor is significantly larger than that of SQKR, which implies better privacy protection. For example, for SQKR with \(=2\), given type I error rate \(=0.5\), the type II error rate of the attacker is around \(f^{SQKR}()=0.068\), while the ternary compressor attains \(f^{ternary}()\) = 0.484. Given the same MSE and communication cost as that of SQKR with \(_{SQKR}=\{1,2,5\}\), if we translate the privacy guarantees of the ternary compressor from \(f\)-DP to \(\)-DP via Lemma 1 (we numerically test different \(\)'s such that \(f^{ternary}()\{0,1--e^{},e^{-}(1- -)\}\) holds for \(=0\)), we have \(_{ternary}=\{0.05,0.2,3.9\}\) for the ternary compressor, which demonstrates its effectiveness. The middle and right figures in Fig. 4 show the tradeoff between MSE and DP guarantees for the Gaussian mechanism and the proposed ternary compressor. Particularly, in the middle figure, the tradeoff curves for the ternary compressor with all the examined sparsity ratios overlap with that of the Gaussian mechanism with \(A/B=1\) since they essentially have the same privacy guarantees, and the difference in MSE is negligible. For the Gaussian mechanism with \(<1\), the MSE is larger due to sparsification, which validates our discussion in Section 6. In the right figure, we examine the MSEs of the proposed ternary compressor with various \(A\)'s and \(B\)'s. It can be observed that the corresponding tradeoff between MSE and privacy guarantee matches that of the Gaussian mechanism well, which validates that the improvement in communication efficiency for the proposed ternary compressor is obtained for free.

## 8 Limitation

The main results derived in this paper are for the scalar case, which are extended to the vector case by invoking the central limit theorem. In this case, the privacy guarantees derived in Theorem 5 are tight only in the large \(d\) regime. Fortunately, in applications like distributed learning, \(d\) corresponds to the model size (usually in the orders of millions for modern neural networks). Moreover, despite that the privacy-accuracy tradeoff of the proposed ternary compressor matches that of the Gaussian mechanism which is order-optimal in \((,)\)-DP, the optimality of the proposed ternary compressor in the \(f\)-DP regime needs to be further established.

## 9 Conclusion

In this paper, we derived the privacy guarantees of discrete-valued mechanisms with finite output space in the lens of \(f\)-differential privacy, which covered various differentially private mechanisms and compression mechanisms as special cases. Through leveraging the privacy amplification by sparsification, a ternary compressor that achieves better accuracy-privacy-communication tradeoff than existing methods is proposed. It is expected that the proposed methods can find broader applications in the design of communication efficient and differentially private federated data analysis techniques.