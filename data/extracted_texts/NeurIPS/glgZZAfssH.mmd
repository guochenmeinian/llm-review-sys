# Metric Space Magnitude for Evaluating the Diversity of Latent Representations

Katharina Limbeck

Rayna Andreeva

University of Edinburgh

Rik Sarkar

University of Edinburgh

Bastian Rieck

###### Abstract

The _magnitude_ of a metric space is a novel invariant that provides a measure of the 'effective size' of a space across multiple scales, while also capturing numerous geometrical properties, such as curvature, density, or entropy. We develop a family of magnitude-based measures of the intrinsic diversity of latent representations, formalising a novel notion of dissimilarity between magnitude functions of finite metric spaces. Our measures are provably stable under perturbations of the data, can be efficiently calculated, and enable a rigorous multi-scale characterisation and comparison of latent representations. We show their utility and superior performance across different domains and tasks, including (i) the automated estimation of diversity, (ii) the detection of mode collapse, and (iii) the evaluation of generative models for text, image, and graph data.

## 1 Introduction

Diversity is a key concept in representation learning, referring to the relative abundance and distinct-iveness of model outputs. Given the inherent complexity of deep learning models, the evaluation of diversity is thus crucial, enabling (i) the assessment of the _intrinsic richness_ of latent representations, and (ii) the evaluation of the extent to which models are capable of _preserving_ the properties of an input distribution. While the quantitative evaluation of generative models in particular relies on assessing trade-offs between fidelity and diversity with regards to a known reference distribution, reference-free diversity measures are becoming increasingly relevant when a ground-truth distribution is unknown or intractable. However, reference-based diversity metrics such as _recall_ are notoriously fallible, sensitive to parameter choices and therefore prone to incorrectly approximate the true data manifold, whereas reference-free diversity measures often rely on simple mean summaries that fail to pass basic consistency checks . Thus, existing methods lack expressivity to fully capture what it means for a space to be diverse, resulting in a critical need for novel measures that are (i) theoretically motivated, (ii) robust to noise, and (iii) capable of encoding the intrinsic diversity of data across varying levels of similarity rather than at a single fixed threshold.

**Our contributions.** Addressing this need, we propose a novel family of diversity measures based on _metric space magnitude_, a mathematical invariant that captures numerous important multi-scale geometric characteristics of metric spaces, including curvature, density, and entropy of an input space. Metric space magnitude merely requires a notion of dissimilarity between data points, permitting it to operate on both _local_ and _global_ scales. Hence, magnitude is poised to compare latent spaces, yielding a compact holistic summary of diversity that satisfies relevant theoretical requirements. Our work is the first to (i) introduce magnitude as a general tool for evaluating the diversity of latent representations, and (ii) formalise a notion of difference between the magnitude of two spaces acrossmultiple scales of similarity. We demonstrate that magnitude is stable and can detect curvature, highlighting its use as a multi-scale summary of the local and global geometry of data. Moreover, we empirically showcase the utility of our magnitude-based diversity measure across different modalities, namely text, image, and graph embeddings, for which we observe that our measure outperforms alternative embedding-based measures of intrinsic diversity. Finally, when a reference distribution is known, our magnitude-based notion of difference reliably detects _mode collapse_ and _mode dropping_, thus assisting practitioners in model evaluation and selection.

**In a nutshell:** We propose novel _multi-scale diversity measures_ based on the _magnitude_ of latent representations and show their theoretical and empirical advantages for _evaluating_ the diversity of text, image, and graph _embeddings arising from generative models_.

## 2 Related Work

Latent representations and embeddings have become indispensable tools for analysing data types such as images, text, and graphs. As evidenced by LLMs, understanding semantic relationships in data requires meaningful embeddings. Our work focuses on improving representation-based diversity evaluation and we thus consider the role diversity plays in this context.

Diversity measures.Assessing generative model diversity remains a challenge irrespective of the domain , as ground truth reference distributions or labelled data are often unavailable, and human evaluation remains costly. Thus, there exists a need for interpretable, automated and unsupervised measures of intrinsic diversity. _Reference-free evaluation_ is of particular importance for assessing generated text given the black-box-nature of LLMs , but also applicable across modalities. Motivated by this, a varied collection of diversity measures has been proposed, many of which are task-, domain- or model-specific ; only a fraction of them are applicable to analysing latent representations specifically. The most flexible methods summarise intrinsic diversity using average pairwise dissimilarities like \(L^{p}\) distances or BERT-scores . More recently, Friedman and Dieng  proposed the Vendi Score, inspired by principles from theoretical ecology. Other diversity measures are computed directly on embedding spaces, using e.g. the geometric mean of the standard deviation across each embedding dimension  or cluster-based measures . However, as we explore in Section3.1, none of these measures satisfy all theoretical guarantees required by an axiomatic approach to diversity, and they are limited in expressivity, providing only snapshots of diversity at a single fixed resolution. _Reference-based metrics_ define diversity as the extent to which generated samples cover the full variability of the real data . Examples include the Frechet Inception Distance (FID) or the Inception score (IS). However, they do not exclusively measure diversity but are also concerned with evaluating fidelity, i.e. the assessment of similarity between generated data and real data, making it unclear how single-number summaries such as FID and IS account for each aspect in the trade-off between diversity and quality. Thus, _precision_ and _recall_ have been suggested as more informative summary metrics  and seen various improvements [19; 28; 35]. Unfortunately, as Naeem et al.  show, even the improved versions of precision and recall fail to satisfy the useful conditions for strong evaluation metrics, such as (i) detecting identical reference and generated distributions, (ii) capturing mode dropping, and (iii) simplicity in selecting hyperparameters. To address these concerns, _density_ and _coverage_ have been proposed . Nevertheless, these metrics still rely on fixed-scale manifold approximations to assess diversity making them sensitive to parameter choices. By contrast, our magnitude-based measures have less stringent assumptions and can be defined in a parameter-free fashion.

Magnitude in machine learning.Since its introduction to measure biological diversity , magnitude was formalised by Leinster . Nevertheless, despite strong geometric properties , magnitude has only rarely been applied in a machine learning context. Recent publications started to bridge this gap, linking magnitude to boundary detection , edge detection in images , and the generalisation error of neural networks , as well as demonstrating its utility for multi-objective optimisation . However, the full potential of magnitude for measuring diversity remains largely unexplored since existing works ignore the nature of magnitude as an intrinsic multi-scale summary, which captures both local and global geometry and diversity of the data manifold. Our work is thus the first to leverage magnitude as a flexible, multi-scale measure of diversity in latent representations.

Methods

We first discuss the theoretical properties a suitable diversity measure should satisfy and then introduce metric space magnitude. Based on this, we outline our proposed method using magnitude for measuring the diversity of latent representation and its practical implementation.

### Desiderata for Diversity Measures

Given the difficulty in defining diversity, diversity metrics never measure diversity itself, but rather quantify related ideas. Entropy-based approaches, including magnitude, in particular share close links to diversity, often favoured in ecology for their computational benefits and agreement with fundamental axioms of diversity . Following this axiomatic approach, we highlight the following key requirements :

* _Effective size:_ A dataset with a fixed number of points is more diverse when points are separated e.g. distributed uniformly or maximally disordered and becomes less diverse as observations cluster together. Diversity is maximised when points are completely distinct and minimised when all observations are identical.
* _Monotonicity in observations_: Including a new observation does not decrease diversity.
* _Twin property_: Including a duplicate observation does not change diversity.
* _Multi-scale_: Diversity is summarised across multiple scales of (dis)similarity and thus captures both local and global trends in the data manifold.

This list is not conclusive; Appendix C.3 provides a more rigorous discussion of desirable properties and their definitions. We observe that many diversity measures for evaluating representations in ML do not satisfy these requirements as shown via counterexamples in Appendix C.4. For example, average similarity (AvgSim), the most frequently-used diversity measure in ML, cannot capture nuances in diversity and fails even in simple toy scenarios . Specifically, it does not give a measure of effective size and does not encode the entropy or disorder of a space, which is a key aspect of diversity. Consequently, AvgSim fails to distinguish that a more clustered representation is less diverse than a more uniformly sampled space as illustrated in Appendix C.5.1. Likewise, the geometric mean of the standard deviations across each embedding dimension [20, GMStds] does _not_ measure effective size, and even worse it equals zero whenever an embedding feature is constant. Even the Vendi Score [13, VS], a more purpose-built diversity measure, calculated as the exponential of the Shannon entropy of the eigenvalues of a normalised similarity matrix, shows undesirable behaviour under the inclusion of observations. Moreover, neither one of the aforementioned diversity measures fulfil the twin property nor monotonicity in observations , leading to counter-intuitive behaviour when capturing changes in diversity. For example, an exact repetition of the reference data could be wrongly judged to be more diverse than a model that generates more samples with small but relevant deviations from the reference. Further, we argue that diversity is a multi-scale trend that should describe the effective size of a space across multiple levels of (dis)similarity rather than rely on fixed-scale snapshots. Indeed, summarising both the coarse and more granular geometry of the data manifold is necessary to get a complete picture of both local and global differences in entropy, clusterability and diversity.

This discussion thus points out a glaring need for more principled diversity measures. Addressing this, _magnitude functions_ are particularly promising candidates for improved diversity measures that inherently satisfy all desiderata listed above, as shown in Appendix C.3. Many alternative summaries trivially fulfil a number of basic properties of diversity. However, it is non-trivial to satisfy _all_ the desired axioms, making magnitude functions unique in their formulation. This axiomatic justification as well as our multi-resolution approach to diversity are the distinguishing characteristics and main advantage of our proposed diversity evaluation methods.

### The Magnitude of a Metric Space

_Magnitude_ is an invariant that measures diversity by describing the 'effective number of points' of a metric space as a function of its scaled distances .

**Definition 3.1** (Magnitude of a metric space).: Let \(X=\{x_{1},,x_{n}\}^{D}\) be a finite metric space with an associated distance metric \(d\). For \(1 i,j n\), the _similarity matrix_ of \(X\) is calculated as \(_{X}(i,j)\ :=\ (-d(x_{i},x_{j}))\). If \(_{X}\) is invertible, the _magnitude_ of \(X\) is defined as

\[(X):=_{ij}(_{X}^{-1})_{ij}.\] (1)

The existence of magnitude is thus contingent on the existence of \(_{X}^{-1}\). For negative definite metrics \(d\) like the \(L^{1}\) and \(L^{2}\) distance, \(_{X}\) is invertible . Subsequently, we assume that \((X,d)\) permits the calculation of magnitude; in particular, \(X\) must _not_ have any duplicate points. While the magnitude of a metric space is expressive even at a _single_ scale [5; 21; 23], magnitude unleashes its full potential in a _multi-scale_ setting, assigning to a metric space not just a scalar but a function. To this end, we scale the distances in \(X\), effectively viewing the same space through different lenses, or at different 'zoom factors,' for example by converting distances from centimetres to metres. Computing the magnitude for all such scales then yields the _magnitude function_.

**Definition 3.2** (Magnitude function).: Let \((X,d)\) be a metric space and \(tX:=(X,d_{t})\) its scaled version with \(d_{t}(x,y):=t d(x,y)\) for a scaling factor \(t_{+}\). The _magnitude function_ of \((X,d)\) is the function \(_{X} t(tX)\).

For \(t(0,)\), the magnitude function is defined for all but finitely many values of \(t\). The magnitude function is also _continuous_[26; Corollary 5.5] for negative definite metrics.1 For finite metric spaces, we have \(_{t}(tX)=|X|=n\), i.e. the _cardinality_ of \(X\)[21; Proposition 2.2.6]. This limit behaviour exemplifies to what extent the magnitude function describes the diversity of a space as 'the effective number of points at scale \(t\).' Here, we extend magnitude functions to the domain \([0,)\) by defining \(_{X}(0):=1\).2 Intuitively, this extension means that any metric space, when viewed from far away, looks like a single point. Notice that neither Definition 3.1 nor Definition 3.2 explicitly require specific properties of a metric (like the triangle inequality) and we find magnitude computable for generalised distance functions, including cosine distances, provided the similarity matrix \(_{X}\) is invertible. Figure 1 illustrates how magnitude functions measure the effective number of distinct points for toy data, thus describing their diversity. Moreover, it provides an overview of our diversity evaluation framework, which we will now introduce.

### Magnitude for Evaluating Diversity

As a multi-scale geometric invariant, magnitude can be extended to evaluate the diversity of latent representations. Here, we are studying a set of latent representations \(=\{X_{1},X_{2},\}\), where each \(X_{i}\) is a finite subset of some latent space sharing the same notion of distance, e.g. \(X_{i}^{D}\). Given a latent representation \(X\), e.g. a text, image, or graph embedding, we can use the \(L^{1}\) or \(L^{2}\) distance as a metric or semi-metrics like the cosine distance. Based on the choice of metric, we can interpret \(_{X}(t)\) as the effective number of points at scale \(t\). In practice, this summarises how diverse points in the space are when observed at said scale factor. This multi-scale behaviour motivates us to propose a simple but expressive summary of a representation's magnitude function.

**Definition 3.3** (Area under the magnitude function, MagArea).: Let \(X\) be a metric space whose magnitude function \(_{X}(t)\) has been evaluated across the interval \(T=[t_{0},t_{}]\). We define the area under the magnitude function to be \(:=_{t_{0}}^{t_{}}_{X}(t )dt\).

Moreover, we extend this proposed summary to measure the difference in diversity between two representations generated by the _same_ (embedding) model. Notice that distances in these spaces are directly comparable and the respective magnitude functions can be compared across the same domain.

**Definition 3.4** (Magnitude function difference, MagDiff).: Let \(X\) and \(Y\) be two metric spaces that share the same notion of distance. Assume the associated magnitude functions \(_{X}(t)\) and \(_{Y}(t)\) have been evaluated across the same interval \(T=[t_{0},t_{}]\). We define the magnitude function difference to be \(:=_{t_{0}}^{t_{}}(_{X }(t)-_{Y}(t))dt\).

Definition 3.3 and Definition 3.4 constitute novel multi-scale approaches for summarising and comparing magnitude functions, leading to theoretically well-founded diversity measures. MagAreameasures the cumulative value of magnitude summarising a space's intrinsic diversity while MagDiff measures the accumulated difference in diversity between two spaces. As we will later demonstrate in our experiments, integrating the changes in magnitude across a _range_ of scale factors retains the desirable properties of single-scale magnitude, but yields more robust multi-scale summaries of diversity (see Appendix A.2 for an investigation of stability to perturbations). Furthermore, this comparison in terms of the effective number of points across scales remains directly interpretable.

### Practical Usage and Implementation

In order to use our magnitude metric for reference-free and reference-based diversity evaluation, we obviate the choice of evaluation interval using knowledge about the convergence behaviour of magnitude functions. As a consequence, our magnitude-based diversity measures do not require manual parameter selection. First, we define a magnitude function's convergence scale.

**Definition 3.5** (Convergence scale, \(t_{}\)).: Given a magnitude function \(_{X}(t)\), we define its approximate convergence scale as \(t_{}\), with \(_{X}(t_{})=|X|-\) for some small \(>0\). We set \( 0.05|X|\) in this work.

This convergence scale thus indicates the resolution at which at least \(95\%\) of observations are recognised by magnitude as being distinct. After reaching this convergence scale, we know that magnitude functions and hence diversity can increase by at most \(\) based on the convergence of magnitude towards the cardinality as illustrated in Figure 1. In practice, however, we find that _all_ relevant changes in diversity happen at smaller scales of distance when individual points are not yet clearly separated. We thus choose the convergence scale defined in Definition 3.5 to be the upper bound of the evaluation interval \(T\) to determine the most informative range of scales. We then find the convergence scale using numeric root-finding procedures as illustrated in Appendix B.2. When comparing the intrinsic diversity of multiple embeddings _without_ a reference dataset, we compute MagArea across \(T=[0,t_{}]\) and choose \(t_{}\) to equal the median of the convergence scales of the embeddings. Taking the median here provides a stable compromise between the convergence behaviour of all functions. For _reference-based comparisons_, we simply calculate MagDiff, the difference between the magnitude functions, across \(T=[0,t_{}]\) where \(t_{}\) is the convergence scale of the reference embedding. In practice, we _approximate_ the integrals in Definition 3.3 and

Figure 1: **Overview of our diversity evaluation pipeline.** (a) We start with an example of four latent spaces with \(200\) points, varying in diversity. (b) The magnitude function measures the effective number of points at \(t\), a scale of distance between observations. When the scale factor \(t\) almost equals zero, magnitude is close to 1, and a space effectively looks like one point. For large \(t\), the number of effective points is noticeably higher and magnitude converges towards the cardinality. We find the approximate convergence scale, \(t_{}\), at which magnitude almost equals the cardinality, and use it to define the evaluation interval \(T\) across which diversity changes most notably. (c) The more diverse the space, the higher the value of its magnitude function. By construction, \(X_{1}\) is more diverse than \(X_{2}\), \(X_{3}\), and \(X_{4}\), respectively, as we can see from the effective size of each space. We leverage this behaviour to define novel multi-scale indicators of diversity. (d) Our proposed measure of intrinsic diversity, MagArea, summarises the area under each magnitude function for _reference-free_ diversity evaluation. (e) In a _reference-based_ setting, we assess the difference in diversity using MagDiff, the area between two magnitude functions.

Definition 3.4 via numerical integration across evenly-spaced scales sampled from the evaluation interval \(T\). Choosing the number of scales is a trade-off between _accuracy_ and _computational performance_ as computational costs increase linearly with the number of times magnitude is evaluated. In terms of implementations, we also improve the efficiency of magnitude computations using a Cholesky decomposition (see A.5 for more details). Together with our automated scale-selection procedure, we thus overcome the main algorithmic hurdles that hitherto prevented the wider use of magnitude functions. Finally, we implement our methods in a Python package.3

### Limitations

MagDiff is a reference-free measure of intrinsic diversity, but does not measure _fidelity_. It should therefore not be interpreted in isolation, but jointly with coverage-based metrics, for instance. Moreover, while we improve the efficiency of magnitude computations (see A.5) compared to previous implementations , thus making magnitude calculations feasible for practical analyses, novel approximation methods would be required to enable scaling to hundreds of thousands of observations. Finally, we focus on evaluating representation-based diversity and show that, given a latent representation, magnitude yields a better notion of diversity than current embedding-based methods. We do not investigate whether embedding-based similarities are outperformed by alternative task- or domain-specific similarities. Instead, our evaluation relies on the utility of embedding models and assumes that latent spaces encode useful/realistic relationships between samples.

## 4 Experiments

Our experiments demonstrate how magnitude leads to a better understanding of representational diversity. We show the following results: (i) Magnitude functions capture the curvature of a space. (ii) Magnitude functions are interpretable measures of the intrinsic diversity of embeddings, yielding superior results than other diversity measures when predicting the diversity of sentence embeddings across different text-generation tasks. (iii) Magnitude functions characterise and distinguish latent representations of large language models. (iv) Magnitude functions successfully detect mode dropping in distributions of image, and graph embeddings, while also reliably detecting mode collapse in graph embeddings. We subsequently use MagArea in reference-free settings to characterise intrinsic diversity (i, ii), while using MagDiff for reference-based comparisons (iii, iv).

### Magnitude Functions Summarise Geometry

Magnitude functions encode the'shape,' i.e. the geometry that is characteristic of the intrinsic data manifold, by capturing curvature and diversity. Curvature estimation is an important task in numerous domains like computer vision, computational geometry, and computer-aided design. The notion of curvature is inherently linked to diversity: The more positively curved a space is, the lower its diversity as points on the more curved surface move closer and closer together, thus decreasing its diversity. For specific examples of manifolds, magnitude can be expressed in terms of volume and total scalar curvature , a theoretical connection that we are the first to investigate empirically for

   Method & MSE (\(\)) \\  SVR (selected PH features) & \(0.27 0.07\) \\ SVR (PH vectorisation) & \(0.17 0.05\) \\ SVR (all PH features) & \(0.16 0.03\) \\  SVR (distance matrices) & \(0.24 0.04\) \\ MLP (shallow) & \(1.15 0.52\) \\ MLP (deep) & \(1.56 0.68\) \\  MagArea (quantile) & \(\) \\ MagArea (piecewise linear) & \(\) \\   

Table 1: **Magnitude estimates curvature.** The MagArea outperforms more complex methods  using a _single feature_.

Figure 2: **Magnitude detects curvature.** Left: Magnitude functions for unit disks with varying curvature between \([-2,2]\). Right: MagArea exhibits a linear relationship with curvature, indicating that it serves as a expressive predictor.

a broader class of spaces. Previous works have shown that alternative multi-scale methods, such as _persistent homology_, are able to detect curvature [4; 41]. Here, we demonstrate that the magnitude function is capable of achieving comparable performance, using simpler methods and only a single feature, namely MagArea. To this end, we generate a balanced dataset of point clouds of different curvature (following Turkes et al.  and detailed in Appendix D.1). We first assess to what extent the magnitude function can detect whether a unit disk has positive or negative curvature. Our main observation from plotting the functions for both groups in Figure 2 is that there is a clear separation between spaces of negative and positive curvature. We further test if we can predict curvature as a regression task. To this end, we try both piecewise linear and quantile regression,4 using the area under the magnitude curve, MagArea, as a single feature. With \(5\)-fold cross validation, we achieve an MSE of \(0.05 0.03\) with the piecewise linear model and \(0.10 0.05\) using quantile regression. Both scores substantially improve on previous methods  that made use of highly-sophisticated topology-based features and more heavily-parametrised deep learning models (see Table 1). These results underscore the expressivity and power of magnitude-based metrics, which enable us to solve the _same_ task with a highly-simplified model. Moreover, this also demonstrates how magnitude describes the data manifold across multiple resolutions, motivating the use of magnitude functions as flexible, geometry-aware descriptors of diversity.

### Magnitude Measures the Intrinsic Diversity of Text Embeddings

Next, we demonstrate the utility of using magnitude for intrinsic diversity evaluation and study its correspondence to known ground-truth diversity of text data. We analyse data from Tevet and Berant , consisting of 1K sets of \(10\) sentences each, generated for unique input prompts for \(3\) different sentence generation tasks, namely story completion (story), dialogue response generation (resp), and 3-word prompt completion (prompt). Per task, \(10\) response sets have been generated using the same decoding parameter, the softmax-temperature \(\), which controls the diversity and randomness of the generated text. As \(\) decreases, models are skewed towards avoiding low-probability tokens. This leads to potentially higher quality and fidelity but lower diversity and creativity in generated text. We embed each set of responses using \(5\) pre-trained sentence transformer models , i.e. (1) bert-large-nli-stsb-mean-tokens, (2) roberta-base-nli-mean-tokens, (3) all-mpnet-base-v2, (4) all-distilroberta-v1, and (5) all-MiniLM-L12-v2. For each dataset and model, we compute the area under the magnitude function MagArea, evaluated until the

Figure 4: **MagArea correlates well with \(\) indicating the true diversity. Here, we use mpnet embeddings for the resp dataset. \(\) denotes the rank correlation between MagArea and \(\) (95% bootstrap interval, \(1000\) resamples).**

Figure 3: **MagArea outperforms alternative diversity measures at predicting the ground truth-diversity of generated sentences, controlled by the softmax-temperature across 3 tasks and 5 embedding models. Baseline measures, AvgSim and GMStds, perform worse in terms of the \(R^{2}\) scores. Points show the mean of the \(R^{2}\) scores, while lines represent the standard deviations across \(5\)-fold cross-validation (repeated \(10\) times).**

median convergence scale across all embeddings as detailed in Section3.4 using cosine distances. We compare this to the Vendi Score (VS), AvgSim, and GMStds, calculated using cosine similarities. Moreover, we analyse the performance of each diversity metric at predicting the ground-truth diversity scores, \(\), using \(5\)-fold cross-validation repeated \(20\) times, trained via isotonic regression models;5 and report their performance in terms of the coefficient of determination, \(R^{2}\). Figure4 depicts the positive rank correlation between magnitude and the softmax-temperature for one example setting, while Figure3 shows results concerning the predictive performance of different diversity measures.

We observe that MagArea consistently outperforms alternative diversity measures computed from the same representations. MagArea achieves a median rank of \(1\) across experiments in terms of \(R^{2}\) scores, followed by VS, AvgSim and GMStds. Indeed, MagArea is most frequently the best-performing diversity measure for 77% of resamples when predicting decoding parameters, ranking second in the remaining cases. Meanwhile, VS most often achieves second place. This demonstrates the strength of MagArea as a theoretically-motivated and entropy-based measure of intrinsic diversity. By contrast, the baseline measure GMStds fails for any embedding that has at least one constant dimension, even reaching negative \(R^{2}\) values for three of the five embedding models. This is followed by AvgSim, which, while being less fallible than GMStds, simply measures average similarity and even ranks last across \(27\%\) of resamples. A further comparison of performance scores shows that MagArea outperforms AvgSim by \(0.12\) higher mean \(R^{2}\) scores on story and \(0.07\) on resp or prompt across embedding models. We find no dataset for which either AvgSim or GMStds can be considered preferable predictors of the ground-truth diversity of text. Our results thus show the benefits of replacing simple summaries as the current standard for automated diversity evaluation with more sophisticated diversity measures like MagArea.

### Magnitude Distinguishes and Characterises Embedding Models

Motivated by the capability of magnitude functions to encode representations, we now check whether the embedding spaces of different large language models can be distinguished via their intrinsic structure. To this end, we analyse \(16384\) documents of four different HuggingFace datasets, as embedded by Wayland et al.  using six different models (see AppendixD.3 for more details). We then either use PCA and normalisation to reduce each embedding space to \(384\) dimensions (to obtain a comparable dimensionality) or use the original embeddings without preprocessing. Further we subsample \(300\) documents at random from each space, repeating this procedure \(200\) times. Finally we use a \(5\)-NN classifier to predict the embedding model based on the values of each diversity measure. This task is chosen to assess whether a simple classifier can distinguish embedding spaces solely based on their intrinsic diversity estimates. Table2 reports the results of \(5\)-fold cross-validation with \(20\) repetitions for both prepossessing choices. We either use Euclidean distances between single number summaries or, in the case of magnitude, use MagDiff directly as precomputed input distances for \(k\)-NN classification. We first observe that MagDiff best predicts the embedding model (with accuracies typically above \(90\%\)). Supplementary results in TableS.5 verify that these performance scores are almost identical for varying hyperparameter choices of \(k\) neighbours. Surprisingly, the results further remain consistent for both pre-processing choices. This indicates that there are inherent differences in the structure and diversity of embedding spaces, which are preserved throughout dimensionality reduction and captured by magnitude. By using the difference between magnitude functions as a holistic summary, we once again surpass other summary statistics (which we observe to fail in distinguishing the smaller embedding models). Our results thus demonstrate that using MagDiff for comparing latent spaces across multiple scales is considerably more expressive than using single-number summaries of diversity.

### Magnitude Evaluates Image Embeddings

_Mode dropping_ is a common issue in generative modelling, referring to the inability of a model to capture all parts of an input distribution (for instance, a model trained to generate images of animals suffers from mode dropping if it can only generate images of dogs). To simulate this, we randomly sample \(100\) images from each of the \(10\) classes in CIFAR10 and embed them using a pre-trained Inception V3 model . Subsequently, we re-sample increasingly more observations from _one_ preferred image class. We either drop modes sequentially, or we move the same number of observations simultaneously from all other classes. Thus, diversity decreases gradually with the same 'speed' across both procedures, but fidelity should not change. We treat each class as the preferred image class twice, leading to 20 re-samples per mode dropping scenario . Our analysis compares the changes in recall and coverage, setting the number of nearest neighbours to \(k=10\). Further, we calculate the relative change in \((0.5t_{})\), i.e. magnitude computed at half the convergence scale of the reference using Euclidean distances. Similarly, MagDiff is the difference between the magnitude functions relative to the area under the reference magnitude function.

Figure 5 shows the changes in diversity as modes are being dropped. Ideally, every diversity measure should show the _same_ decrease in diversity, irrespective of resampling strategy. However, we observe that both recall and coverage wrongly assess that diversity decreases faster during sequential resampling. Even worse, coverage only detects simultaneous mode dropping after around \(70\%\) of all points have shifted to one mode. This undesirable behaviour of both metrics is caused by their reliance on a fixed neighbourhood size for approximating the underlying manifold, thus overestimating the extent to which the perturbed samples reflect the diversity of the reference distribution. In comparison, MagDiff as well as magnitude evaluated at a single scale both successfully measure the gradual decrease in diversity across both mode dropping scenarios.

### Magnitude Evaluates Graph Generative Models

Diversity evaluation in graph learning is fraught with difficulties, in particular when aiming to detect common problems like _mode collapse_ or _mode dropping_[29; 40]. In the following, we will study graph generative models (GGMs), which take a set of input graphs and generate new samples that should follow the _same_ distribution. The question that we aim to answer here is whether our proposed magnitude-based metric is more expressive in capturing the diversity of the generated graphs than classical metrics like _maximum mean discrepancy_ (MMD) and measures inspired from evaluating image generative models (precision, recall, coverage, density). To this end, we analyse 3 synthetic (Lobster, Grid, and Community) and 2 real-world (Proteins and Ego) graph datasets, and compute commonly-used evaluation metrics [29; 40] as detailed in D.5. To test the diversity of generated samples, we replicate the experimental setup of Thompson et al.  and add our own measure, MagDiff computed using \(L^{2}\) distances from Graph Isomorphism Network [45; GIN] embeddings with varying hyperparameters. For the _mode collapse_ experiments, we substitute each embedded graph with its cluster centre. Thus, the degree of perturbation \(p\) equals the proportion of clusters collapsed in this manner. The larger the value of \(p\), the more clusters have been perturbed decreasing the diversity. For the _mode dropping_ experiments, we remove clusters, and keep the size of the generated dataset the same as the reference by randomly resampling from the remaining classes.

Figure 6 shows the results of both _mode collapse_ and _mode dropping_ for the Lobster dataset. We observe similar trends across all datasets, but have chosen this dataset as a running example. Ideal measures should exhibit high rank correlation to the degree of perturbation, indicating that they

    &  \\  DatasetMethod & MagDiff & AvgSim & VS & GMStds & MagDiff & AvgSim & VS & GMStds \\  cnn & \(\) & \(0.87 0.01\) & \(0.63 0.01\) & \(0.66 0.02\) & \(\) & \(0.88 0.02\) & \(0.67 0.03\) & \(0.66 0.03\) \\ patents & \(\) & \(0.92 0.01\) & \(0.63 0.02\) & \(0.66 0.02\) & \(\) & \(0.91 0.02\) & \(0.64 0.03\) & \(0.66 0.03\) \\ arXiv & \(\) & \(0.89 0.01\) & \(0.78 0.01\) & \(0.66 0.02\) & \(\) & \(0.88 0.02\) & \(0.78 0.02\) & \(0.66 0.03\) \\ bbc & \(\) & \(0.74 0.01\) & \(0.84 0.02\) & \(0.66 0.02\) & \(\) & \(0.73 0.03\) & \(0.84 0.02\) & \(0.66 0.03\) \\   

Table 2: **Magnitude characterises text embedding models.** We show the accuracy (\(\)) of different diversity scores for distinguishing between six embedding models, using a \(5\)-NN classifier.

Figure 5: **Magnitude correctly detects that diversity decreases in the same manner across simultaneous and sequential mode dropping** outperforming recall and coverage. Lines show the mean values of each metric across \(20\) resamples, shaded areas the standard deviations.

are capable of capturing the decrease in diversity properly, i.e. as a function of \(p\). We note that in contrast to our magnitude-based metric, _recall_ and _coverage_ exhibit worse results, as evidenced by their lower mean correlation coefficient. Despite being specifically designed to measure the diversity of a dataset , they only catch up to our magnitude metric when the degree of perturbation \(p\) is around \(0.9\) (see Figure 6, right-hand plots). Magnitude dominates in the majority of the values of \(p\) best showing the steady decrease in diversity, while recall and coverage become more sensitive for exceedingly large values of \(p\), i.e. in unrealistic situations where most of the modes have been dropped. Moreover, their performance is highly contingent on \(k\), the parameter used to construct a \(k\)-NN graph for computing these neighbourhood-based metrics. Magnitude functions meanwhile give more holistic summaries of both local and global patterns in diversity. Please refer to Figure S.16 for the aggregated results over all datasets, which exhibit a similar pattern (in that our metric outperforms both _recall_ and _coverage_).

## 5 Discussion

We have proposed novel diversity measures for evaluating latent representations. Our measures are based on _metric space magnitude_, a multi-scale invariant summarising geometrical characteristics of the input data. We have demonstrated axiomatically and empirically that our magnitude-based measures are superior to current baseline measures of intrinsic diversity. In a reference-free scenario, we observe that magnitude outperforms alternative measures when predicting the ground truth diversity for text embeddings. Given a reference dataset, we find that magnitude captures mode collapse and mode dropping better than existing metrics for evaluating generative models for both image and graph modalities. Furthermore, we have shown that magnitude can measure the intrinsic curvature of input data, outperforming previous methods. Magnitude thus gives a provably stable, unsupervised diversity metric that can be computed efficiently and allows users to flexibly choose a notion of dissimilarity. For future work, we believe that magnitude exhibits a strong potential for applications to unaligned spaces with varying notions of distances. Moreover, we believe that integrating magnitude into deep learning models would be beneficial for obtaining novel diversity- and geometry-based regularisation strategies.

Figure 6: **MagDiff outperforms existing graph diversity metrics at detecting mode collapse and mode dropping. We report the Spearman correlation between each metric and the degree of perturbation \(p\) for the Lobster dataset (the same pattern holds for Proteins, Community, Ego, Grid, see Appendix D.5). Violin and box plots show the distributions across different hyperparameter choices. Measures that capture the decrease in diversity accurately should increase as a function of \(p\). Rank correlation of \(1\) corresponds to an ideal metric. Our metric best captures the changes in diversity for both mode dropping and collapse.**

## Funding Disclosure

K.L. gratefully acknowledges support from the 2024 NeurIPS Financial Assistance Program. R.A. was supported by (i) the United Kingdom Research and Innovation (grant EP/S02431X/1), UKRI Centre for Doctoral Training in Biomedical AI at the University of Edinburgh, School of Informatics, (ii) the International Helmholtz-Edinburgh Research School for Epigenetics (EpiCrossBorders), and (iii) a Helmholtz Visiting Researcher Grant. B.R. was partially supported by the Bavarian state government with funds from the _Hightech Agenda Bavaria_. This work has received funding from the Swiss State Secretariat for Education, Research, and Innovation (SERI). The authors declare no competing interests. The funders had no role in the preparation of the manuscript or the decision to publish.

## Impact Statement

This paper presents work whose goal is to advance the evaluation diversity in representation learning, leading to increased fairness and trustworthiness in model evaluation. While representational diversity in terms of model outputs may have potential negative impacts, depending on the task at hand, we feel there are none that need to be specifically highlighted here. However, we acknowledge the potential for societal harm if our notion of representational diversity is confused with the meaning of diversity in the colloquial or societal context, which is admittedly even harder to measure and requires a larger discussion involving all affected communities.