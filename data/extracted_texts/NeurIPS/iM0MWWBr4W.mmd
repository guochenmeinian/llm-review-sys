# A Unified Model and Dimension

for Interactive Estimation

Nataly Brukhim

Princeton University

nbrukhim@princeton.edu

&Miro Dudik

Microsoft Research

mudik@microsoft.com

Aldo Pacchiano

Broad Institute of MIT and Harvard & Boston University

apacchia@broadinstitute.org

&Robert Schapire

Microsoft Research

schapire@microsoft.com

###### Abstract

We study an abstract framework for interactive learning called _interactive estimation_ in which the goal is to estimate a target from its "similarity" to points queried by the learner. We introduce a combinatorial measure called _dissimilarity dimension_ which is used to derive learnability bounds in our model. We present a simple, general, and broadly-applicable algorithm, for which we obtain both regret and PAC generalization bounds that are polynomial in the new dimension. We show that our framework subsumes and thereby unifies two classic learning models: statistical-query learning and structured bandits. We also delineate how the dissimilarity dimension is related to well-known parameters for both frameworks, in some cases yielding significantly improved analyses.

## 1 Introduction

We study a general interactive learning protocol called _interactive estimation_. In this model, the learner repeatedly queries the environment with an element from a set of _alternatives_, and observes a stochastic reward whose expectation is given by an arbitrary measure of the "similarity" between the queried alternative and the unknown ground truth. Thus, in rough terms, the goal is to estimate a target from its similarity to queried alternatives. By studying such a general abstraction of interactive learning, we are able to reason about the properties of a very broad family of learning settings, and to make connections across a variety of contexts.

Our results are based on a combinatorial complexity measure we introduce called the _dissimilarity dimension_, which is used to derive learnability bounds in our model. Intuitively, this measure corresponds to the length of the longest sequence of alternatives in which each one has a similar suboptimal value of similarity to all its predecessors. We then use the measure to analyze the performance of a simple, broadly-applicable class of algorithms which repeatedly make new queries that best fit the preceding observations. We prove both regret bounds and PAC generalization bounds that are all polynomial in the dissimilarity dimension.

We show that our learning framework subsumes two classic learning models that were seemingly unrelated prior to this work:

First, our model subsumes the statistical query (SQ) model, introduced by Kearns  for designing noise-tolerant learning algorithms. In the SQ model, the learner can sequentially ask certain queries of an oracle, who responds with answers that are only approximately correct, with the goal of correctly estimating a target. Despite its simplicity, it has been proven to be a powerful model. Indeed, a wide range of algorithmic techniques in machine learning are implementable using SQ learning. Thus, it has been proven useful, not only for designing noise-tolerant algorithms, but also for its connectionsto other noise models, and as an explanatory tool to prove hardness of many problems (see the survey of Reyzin ). We show that our framework subsumes the SQ model, and furthermore that the dissimilarity dimension generalizes well-known parameters that characterize SQ learnability.

Second, our model captures structured bandits, in which the learner repeatedly chooses actions which yield stochastic rewards, with the goal of minimizing regret relative to the best action in hindsight. Over more than a decade, the eluder dimension  has been a central technique for analyzing regret for contextual bandits and reinforcement learning (RL) with function approximation [30; 22; 29; 10]. We will see that the dissimilarity dimension is upper-bounded by the eluder dimension, and that there can in fact be a large gap between the two. This sometimes leads to an improved analysis when relying on the proposed dissimilarity measure rather than the eluder dimension.

Because SQ and bandits are both subsumed by our framework, all the results mentioned above directly apply to those settings as well, including the applicability of our general-purpose algorithms.

To summarize, our main contributions are as follows:

* **Unified framework.** We derive a general framework which captures various interactive learning settings, including specifically SQ and bandits.
* **Novel dimension, performance bounds.** We introduce the dissimilarity dimension that is used to derive learnability bounds in our model. We study a general, simple algorithm, and give a novel analysis that results in both regret and PAC generalization bounds that are polynomial in the new dimension. We also give lower bounds in the SQ and bandit settings.
* **Improved analysis.** We show instances in which the standard analysis of a certain class of algorithms using the eluder dimension yields bounds that are arbitrarily large, but in which an analysis using our dimension yields low regret bounds.

Related work.The interactive estimation model we consider in this work is defined with respect to an evaluation function that can be thought of as an arbitrary measure of the "similarity" between the queried alternative and the target. Previously, Balcan and Blum  developed a theory of similarity-based learning that generalizes kernel methods, providing sufficient conditions for a similarity function to be useful for learning. Chen et al.  review several approaches to classification based on similarity between examples, including, for instance, kernels and nearest neighbors. Ben-David et al.  studied a learning-by-distances model that resembles ours using a metric as a measure of similarity. In comparison to these works, our model admits an arbitrary similarity measure for which we derive a general dimension, algorithm, and bounds.

In the context of bandit and reinforcement learning, a parameter called the decision-estimation coefficient (DEC) has recently been proposed by Foster et al.  to characterize learnability in interactive decision making. Unlike DEC, our dimension is combinatorial in nature, and applies to settings like SQ, which are not captured by DEC.

As discussed above, our model subsumes SQ and bandits, both of which have been extensively studied (see the references above as well as various surveys [20; 23]).

## 2 Setting

In this paper, we study an interactive learning protocol called _interactive estimation_. In this protocol, the learner is provided with a set \(\) of _alternatives_, and an _evaluation function_\(:[-1,1]\). Intuitively, \(\) can be viewed as a measure of "similarity," though it need not be symmetric. There is also a distinguished alternative \(z^{*}\) called the _target_, fixed throughout the interaction, and unknown to the learner. In each of a sequence of steps \(t=1,,T\), the learner selects one alternative \(z_{t}\) and receives a stochastic _reward_\(r_{t}[-1,1]\) drawn independently, conditioned on \(z_{t}\), with expectation satisfying \([r_{t}\,|\,z_{t}]=(z_{t}\,|\,z^{*})\). Informally, by choosing alternatives and observing their similarity to \(z^{*}\), the learner aims to get close to the target. The special case when \(r_{t}=(z_{t}\,|\,z^{*})\), that is, when rewards are deterministic functions of the queried alternatives, is referred to as the _deterministic setting_.

We generally assume \((z^{*}\,|\,z^{*})(z\,|\,z^{*})\) for all \(z\) and denote this optimal value as \(^{*}(z^{*}\,|\,z^{*})\). We will assume that the value of \(^{*}\) is known to the learner or that we are provided with an alternate _optimality level_\(^{*}\) such that the task is to identify \(z\) with \((z\,|\,z^{*})\). At the end of Section 3 we discuss how this assumption can be relaxed.

We consider two alternative goals for a learner in this model: _sublinear regret_ and _PAC generalization_. A learner achieves _sublinear regret_ relative to an optimality level \(^{*}\) if \((T,)=o(T)\), where

\[(T,)=_{t=1}^{T}-(z_{t} z^{*}) .\]

We say that a learner achieves _PAC generalization_ if for any \(,>0\) and \(^{*}\), with probability at least \(1-\) (over the randomness of the query responses and the learner's own randomization), after \(m(,,)\) interactions in the protocol above, the learner outputs \(\) such that \(( z^{*})-\). The function \(m(,,)\) is referred to as sample complexity. We recover standard notions of regret and PAC generalization by setting \(=^{*}\).

**Example 1** (Point on a sphere).: _Let \(\) denote the standard Euclidean norm in \(^{n}\) and let \(=_{n-1}=\{^{n}:\; =1\}\) be the unit sphere in \(^{n}\). The goal is to estimate an unknown point \(^{*}_{n-1}\) based on rewards equal to the inner product between the queries and the target, that is, \(r_{t}=_{}(_{t}^{*}) _{t},^{*}\)._

We now introduce the two main examples corresponding to classic learning models that are subsumed by the interactive estimation model.

**Example 2** (Structured bandits).: _Let \(\) be an action set, \(\) a space of reward functions \(f:[-1,1]\), and \(f^{*}\) the target reward function. In step \(t\), the learner chooses an action \(a_{t}\) and receives reward \(r_{t}[-1,1]\) with \([r_{t} a_{t}]=f^{*}(a_{t})\). The goal is to maximize the sum of rewards. Let \(a^{*}=*{argmax}_{a}f^{*}(a)\) be an optimal action. To represent bandits in our formalism, we let \(=\), \(z^{*}=(f^{*},a^{*})\), and \(_{}(f,a)(f^{*},a^{*})=f^{*}(a)\)._

The structured bandit problem has been extensively studied, and Example 2 captures its expressiveness within our framework. For example, it recovers the possibly simplest case of \(K\)-armed bandits, by considering \(=\{1,...,K\}\) and \(=^{K}\). At each round, the learner chooses arm \(a_{t}\) and observes a reward \(r_{t}\) which is drawn from a distribution with mean \(f^{*}(a_{t})\). See Appendix D for a more concrete example of \(K\)-armed bandits instantiated within our framework. In Section 5 we also give concrete bounds for other example classes including linear bandits and GLM bandits.

We remark that although the protocol in which the learner submits pairs \((f,a)\) in each round rather than an action \(a\), may seem more complicated than the standard bandits protocol, it is in fact equivalent as the function \(f\) is ignored in the evaluation. Moreover, this formulation naturally captures any algorithms for realizable bandits. Such algorithms often keep track of a version space (set of functions \(f\) consistent with the data), and so at each point of interaction, there is an implicit \(f_{t}\) that is associated with \(a_{t}\) produced at that time. The above protocol simply makes the choice of \(f_{t}\) explicit.

**Example 3** (SQ learning).: _Given a domain \(\), the goal is to learn a binary classifier \(h^{*}:\{ 1\}\) from some hypothesis class \(\{ 1\}^{}\), based on training examples \((x,y)\) drawn from some distribution \(D\) such that \(y=h^{*}(x)\). In step \(t\), the learner produces a hypothesis \(h_{t}\) and observes the accuracy of \(h_{t}\) on a fresh finite sample. In this case, \(=\), the evaluation function is equal to the expected accuracy \(_{}(h h^{*})=_{x D}[h(x)h^{*}(x)]\), and the reward is the empirical accuracy on a fresh sample._

The SQ learning model considered in this work (Example 3) differs from the original model of Kearns  because it is restricted, as in previous works , to so-called _correlational_ queries (called CSQs) and assumes stochastic responses, as opposed to allowing arbitrary queries and adversarial responses. We discuss relationships between various SQ variants in Appendix B.

We finish this section by introducing a central concept of this paper, a new combinatorial complexity measure called the _dissimilarity dimension_ which, as we will see, allows us to derive learnability bounds for the interactive estimation protocol.

**Definition 1** (Dissimilarity dimension).: _For a set \(\), scalars \(\), \(>0\), and evaluation function \(:[-1,1]\), the dissimilarity dimension \(_{}(,,)\) is the largest integer \(d\) for which there exist \(z_{1},,z_{d}\) with \((z_{i} z_{i})\), and a scalar \(c-\), such that for all \(i<j\),_

\[(z_{i} z_{j})-c}.\]_Furthermore, denote the monotonic dissimilarity dimension as \(}_{}(,,)_{ ^{}}_{}(,,^{})\). 1_

Note that \(_{}(,,)=0\) if there is no \(z\) such that \((z\,|\,z)\), and otherwise \(_{}(,,) 1\). In particular, if \(^{*}\) then \(_{}(,,) 1\).

In rough terms, this dimension corresponds to the longest sequence of points with \(\)-large self-evaluation, such that the evaluation \((z_{i}\,|\,z_{j})\) of each point \(z_{i}\) relative to every successive point \(z_{j}\) is "small" (significantly less than \(\)), and also tightly clustered around some value \(c\). Thus, each point is similar to itself, but dissimilar from all successive points to about the same degree. The idea is illustrated in Figure 1. The monotonic dissimilarity dimension is the tightest upper bound on the dissimilarity dimension that is non-increasing in \(\).

Various concrete examples where the dissimilarity dimension can be bounded are provided in Section 5. For instance, using a general bound for linear bandits from Section 5, we can show that for the task of finding a point on a sphere based on inner products (Example 1), the dimension \(}_{_{}}}(,, ) 4n+3\), a bound that is independent of both \(\) and \(\).

## 3 Algorithms and upper bounds

In this section we analyze algorithms for the interactive estimation protocol, which we call _interactive estimation algorithms_. We show that when an interactive estimation algorithm satisfies two properties, _large self-evaluations_ and _decaying estimation error_, then its regret can be bounded using the dissimilarity dimension. We introduce a simple algorithm (Algorithm 1), which satisfies these properties for many standard classes of alternatives. The first property requires that the algorithm only select alternatives that would achieve the expected reward of at least \(\) if they were the target:

**Definition 2** (\(\)-large self-evaluations).: _An interactive estimation algorithm has \(\)-large self-evaluations if at every time step \(t=1,,T\), it selects a query \(z_{t}\) such that \(z_{t}_{}\), where_

\[_{}=\{z:(z\,|\,z)\}.\] (1)

Algorithm 1 satisfies this property, with the optimality level \(\) provided as input. At the end of the section, we discuss the case when \(\) is not provided and \(^{*}\) is unknown. We derive an optimistic version of Algorithm 1 that achieves \(^{*}\)-large self-evaluations with high probability.

The second property states that the queries produced by the algorithm provide increasingly good estimates of the expected rewards in the previous rounds (that is they are good estimators _with the benefit of hindsight_), as quantified by the square loss.

**Definition 3** (Decaying estimation error).: _An interactive estimation algorithm has decaying estimation error if there exists \(C_{T,} 0\) growing sublinearly in \(T\), that is, \(C_{T,}=o(T)\), such that with probability at least \(1-\) the sequence of queries \(z_{1},,z_{T}\) produced by the algorithm satisfies_

\[_{i=1}^{t-1}(z_{i} z_{t})-(z_{i} z^{*})^{2}  C_{T,}\] (2)

_for all \(t\{1,,T\}\) simultaneously._

Algorithm 1 optimizes an empirical version of Eq. (2), with the observed rewards \(r_{i}\) in place of the expectations \((z_{i} z^{*})\). Thus, in the deterministic setting, with \(r_{i}=(z_{i} z^{*})\), Algorithm 1 satisfies this property with \(C_{T,}=0\). It can also be shown that it satisfies this property when the set of alternatives is finite:

**Theorem 4**.: _Assume that \(||<\). Then Algorithm 1 satisfies the decaying estimation error property with \(C_{T,}=O(T||/)\)._

In the general case, when \(\) is infinite, we show that Algorithm 1 satisfies the decaying estimation error property with \(C_{T,}=O(TN/)\) where \(N\) is a suitable covering number of \(\) (see Corollary 18 in Appendix A.1). For example, for _linear bandits_, which is an instance of Example 2 in which the action set and function class correspond to a subset of the unit ball in \(^{n}\), we obtain \(C_{T,}=On(1/)+(T/)\).

In Appendix A.3, we discuss an approach in which we have access to an online regression oracle for the least squares problem in step 3. We show that a suitably modified version of Algorithm 1 has a decaying estimation error as long as the online regression oracle achieves a sublinear regret (but without further dependence on a covering number).

To develop some intuition how Algorithm 1 works, we can again consider the \(K\)-armed bandit problem (a special case of Example 2), and suppose that \(=0.75\). In each step, the algorithm picks a pair \((f_{t},a_{t})\), where \(f_{t}^{K}\) is the vector of mean reward estimates and \(a_{t}\) is the arm with the largest mean estimate. The estimates \(f_{t}(a)\), \(a=1,,K\), are formed by optimizing the least squares error of the observed rewards, under the constraint that at least one of the mean estimates must be above \(0.75\). As a result, the algorithm pulls the arm with the largest average reward as long as that average is above \(0.75\) (arms that have not been pulled are assumed to have averages above \(0.75\)). If all the averages are below \(0.75\) then the algorithm selects the arm \(a\) with the smallest value \(n_{a}(0.75-_{a})^{2}\), where \(n_{a}\) is how many times the arm has been pulled so far and \(_{a}\) is its average reward; it can be verified that this solves the least squares problem subject to the constraint that at least one of the mean estimates is above \(0.75\).

We next state our main results: a regret bound and a PAC generalization guarantee. They are both based on bounding how many "bad" queries any algorithm with large self-evaluations and decaying estimation error can make. Concretely, we say that a query \(z\) is \(\)_-bad_ if its suboptimalty gap is greater than \(\), that is, if

\[(z z^{*})<-.\]

The next lemma shows that the number of \(\)-bad queries is upper bounded polynomially in the dissimilarity dimension.

**Lemma 5** (Few bad queries).: _Let \(,>0\), and let \(d=}_{}(,,)<\) for some set \(\), evaluation function \(\) and \(^{*}\). Let \(\) be an interactive estimation algorithm with \(\)-large self-evaluations and a decaying estimation error with some \(C_{T,}\). Then, with probability at least \(1-\), the number of \(\)-bad queries that \(\) makes in \(T\) steps is at most \(2d^{1.5}(4/)+12d^{2.5}C_{T,}/^{2}\). Consequently, if \(C_{T,}(2T)\), then with probability at least \(1-\), the number of \(\)-bad queries is at most \(36d^{2.5}C_{T,}/^{2}\), and if \(C_{T,}=0\) then it is at most \(2d^{1.5}(4/)\)._The above result is the core component of our main theorems. The proof is given in Appendix A.4; here we sketch the main ideas. The goal is to show that the "bad" interval \([-1,-]\) cannot contain too many queries made by Alg. The proof starts by partitioning this interval into disjoint subintervals and then bounds the number of queries in each subinterval. It does so by constructing a graph with nodes corresponding to queries, which are connected by an edge if they satisfy the dimension conditions. The decaying errors that imply a certain minimum number of edges (as a function of number of queries). On the other hand, the dissimilarity dimension bounds the size of the largest clique, which implies an upper bound on the number of edges (using Turan's Theorem , a standard result from extremal graph theory). Combining the bounds yields an upper bound on the number of queries in the subinterval. Summing across subintervals proves the lemma.

The following theorems use Lemma 5 to bound both the regret and PAC sample complexity. The proofs are deferred to Appendices A.6 and A.7.

**Theorem 6** (Regret).: _Let \(,T>0\), and let \(d=}_{}(,,1/T)\) for some set \(\), evaluation function \(\) and \(^{*}\). Let \(\) be an interactive estimation algorithm with \(\)-large self-evaluations and a decaying estimation error with some \(C_{T,}\). If \(C_{T,}(2T)\) then with probability at least \(1-\), the regret of \(\) satisfies_

\[(T,) 1+12d^{1.25}T}.\]

_In the deterministic setting, \((T,) 1+12d^{1.5}\)._

For an algorithm with a decaying estimation error, the term \(C_{T,}\) is sublinear in \(T\), implying a sublinear regret in Theorem 6. For example, Algorithm 1 has a decaying estimation error with \(C_{T,}\) that scales logarithmically with \(T/\) for many standard function classes, and so the overall regret scales as \(O()\) (see Corollary 18 in Appendix A.1).

To derive PAC generalization guarantees, we apply a variant of online-to-batch reduction to any algorithm with large self-evaluations and a decaying estimation error. The resulting approach, shown in Algorithm 2, satisfies the following guarantee (proved in Appendix A.7):

**Theorem 7** (PAC generalization).: _Let \(,>0\), and let \(d=}_{}(,,)\) for some set \(\), evaluation function \(\) and \(^{*}\). Let \(\) be an interactive estimation algorithm with \(\)-large self-evaluations and a decaying estimation error with \(C_{T,}(2T)\), and suppose that we run Algorithm 2 with \(\) as the base algorithm, \(T 64d^{2.5}(C_{T,/2})/^{2}\), \(n_{1}=_{2}(4/)\), and \(n_{2}= 128(8n_{1}/)/^{2}\). Then, with probability at least \(1-\), the output \(\) satisfies_

\[( z^{*})-,\]

_and the overall number of issued queries is \(O(C_{T,/2})+^{2}(1/)}{^{2}}\)._

_In the deterministic setting, it suffices to run \(\) with \(T>2d^{1.5}(4/)\) and return \(=z_{}\) where \(=*{argmax}_{t\{1,,T\}}r_{t}\) is the index of the largest observed reward. Then, with probability 1, we obtain \(( z^{*})-\) and issue at most \(O(d^{1.5}(4/))\) queries._

Unknown \(^{*}\) and optimism.Algorithms 1 and 2 achieve performance guarantees with respect to a provided optimality level \(^{*}\). When it is not easy to provide a non-trivial \(\) (for example, when \(^{*}\) is unknown and cannot be non-trivially bounded), Algorithm 3 uses the optimistic least squares algorithmic template (see, e.g., ) to ensure \(^{*}\)-large self-evaluations with high probability and to achieve a sublinear \((T,^{*})\). Algorithm 3 takes as input a confidence radius parameter \(R\) of the same order as the decaying estimation error parameter \(C_{T,}\) for Algorithm 1. We can then show that \(z^{*}_{t}\) with high probability for all \(t\{1,,T\}\). Therefore, \(z_{t}\) must satisfy \((z_{t}|\,z_{t})(z^{*}|\,z^{*})=^{*}\). In Appendix A.1 we show this modified version of the algorithm satisfies the decaying estimation error property. This technique allows us to achieve a sublinear \((T,^{*})\) without knowing \(^{*}\) beforehand. Similar to the case of fixed \(\), it is possible to derive a version of Algorithm 3 that leverages an online regression oracle. (See Appendix A.3 for details.)

```
1:Input: set of alternatives \(\), evaluation function \(\), number of steps \(T\), confidence-set radius \(R\).
2:for\(t=1,,T\)do
3: Compute confidence set \[_{t} =*{argmin}_{z}_{i=1}^{t-1} (z_{i}\,|\,z)-r_{i}^{2},\] \[_{t} =z:\;_{i=1}^{t-1}(z_{i} \,|\,z)-(z_{i}\,|\,_{t})^{2} R}.\]
4: Submit the query \(z_{t}=*{argmax}_{z_{t}}(z\,|\,z)\).
5: Observe reward \(r_{t}\).
6:endfor ```

**Algorithm 3** Optimistic Interactive Estimation via Least Squares

## 4 Statistical queries

In this section we consider the statistical query (SQ) model, as defined in Example 3. In particular, we study the connection between our generalized framework and SQ learning, showing specifically that the dissimilarity dimension can be used to recover generalization bounds based on a known combinatorial parameter that characterizes SQ learning, called the _strong SQ dimension_. There are several notions of such a dimension [12; 25]. Here we focus on the one due to Szorenyi :

**Definition 4** (Strong SQ dimension, ).: _For a fixed distribution \(D\) over \(\), the strong SQ dimension of a hypothesis class \(\{ 1\}^{}\) with respect to some \(>0\), denoted \(_{}(,)\), is the largest number \(d\) for which there exist \(h_{1},,h_{d}\) such that:_

* \(| h_{i},h_{j}| 1-\) _for all_ \(1 i<j d\)_, and_
* \(| h_{i},h_{j}- h_{i^{}},h_{j^{}}| \) _for all_ \(1 i<j d\)_,_ \(1 i^{}<j^{} d\)_,_

_where \( h,h^{}:=_{x D}[h(x)h^{}(x)]\)._

The dissimilarity and strong SQ dimensions are closely related to one another in the sense of each providing a kind of polynomial bound on the other, as stated in the next proposition (see Appendix B.1 for the proof).

**Proposition 8**.: _Let \(D\) be a fixed distribution over \(\), and let \(\{ 1\}^{}\) be a hypotheses class. For \(>0\), let \(d_{}()=_{}(,)\), and let \(d_{}()=_{_{}}(,1,)\)._

_If \(d_{}() 2\) then_

\[d_{}(),\;4^{2}\,(d_{ {SQ}}())^{2}} d_{}() d_{}(/4),\,4^{2}\,(d_{}(/ 4)+1)^{2}}.\] (3)

_Similarly, if \(d_{}(4) 2\) then_

\[\{d_{}(4),\;(4) }}{8}\} d_{}() \{d_{}(),\;()+1}}{2} \}.\] (4)

We next give a lower bound based on the strong SQ dimension, which together with Proposition 8 will allow us to lower bound sample complexity of any interactive estimation algorithm in the SQ setting in terms of the dissimilarity dimension.

**Theorem 9** (SQ lower bound).: _Let \(>0\), and let \(\{ 1\}^{}\) be a hypothesis class with strong SQ dimension \(d_{}=_{}(,2) 11\). Let \(\) be any interactive estimation algorithm with the property that for any target \(h^{*}\), \(\) outputs an \(\)-approximation to \(h^{*}\) with probability at least \(2/3\) using at most \(m\) queries. Then \(m>}}/12\)._The proof relies on a reduction to a lower bound of Szorenyi . However, the lower bound of Szorenyi  holds within an SQ model that differs from ours, in that it allows adversarial query responses. Therefore, we first need to show how to obtain a learning algorithm \(^{}\) that can be used with an adversarial oracle from an interactive estimation algorithm \(\) that uses an unbiased stochastic query oracle (as we assume in this work). To do this, we apply the reduction technique developed by Feldman et al. . (See Appendix B.2 for the full proof and additional details.)

Combining Theorem 9 and Proposition 8 yields a lower bound on the sample complexity of interactive estimation in the SQ setting, for a sufficiently small \(\), in terms of the dissimilarity dimension:

**Corollary 10**.: _Let \(>0\), and let \(\{ 1\}^{}\) be a hypothesis class with strong SQ dimension \(_{}(,2) 11\). Let \(d_{}()=_{}(,1,)\). Assume \( 1/2()}\). Let \(\) be any interactive estimation algorithm with the property that for any target \(h^{*}\), \(\) outputs an \(\)-approximation to \(h^{*}\) with probability at least \(2/3\) using at most \(m\) queries. Then \(m>()}/12\)._

## 5 Bandits

In this section we focus on the bandits setting described in Example 2. We study the relationship between the dissimilarity dimension and the _eluder dimension_, a common combinatorial dimension for bounding regret of bandit algorithms. We show that eluder dimension can be used to upper bound the dissimilarity dimension, and we also highlight the cases when dissimilarity dimension leads to a tighter analysis.

Throughout this section we follow the setup introduced in Example 2. We consider an action set \(\), a class \(\) of reward functions \(f:[-1,1]\), and a target reward function \(f^{*}\). We map this to our setting by considering the set of alternatives \(=\), evaluation function \(_{}(f,a)(f^{},a^{})=f^{}(a)\) and the target \((f^{*},a^{*})\), where \(a^{*}=*{argmax}_{a}f^{*}(a)\).

### Comparison with eluder dimension

We start by describing the relationship between our dimension and the eluder dimension. Following Russo and Van Roy , we define \(\)-dependence and \(\)-eluder dimension as follows:

**Definition 5** (\(\)-dependence).: _An action \(a\) is \(\)-dependent on actions \(\{a_{1},,a_{n}\}\) with respect to \(\) if any pair of functions \(f,f^{}\) satisfying \(^{n}(f(a_{i})-f^{}(a_{i}))^{2}}\) also satisfies \(|f(a)-f^{}(a)|\). Furthermore, an action \(a\) is \(\)-independent of \(\{a_{1},,a_{n}\}\) with respect to \(\) if it is not \(\)-dependent on \(\{a_{1},,a_{n}\}\)._

**Definition 6** (\(\)-eluder dimension).: _The \(\)-eluder dimension \(_{}(,)\) is the length \(d\) of the longest sequence of elements in \(\) such that every element is \(\)-independent of its predecessors. Moreover, the monotone eluder dimension is defined as \(_{}}(,)_{ ^{}}_{}(,^{ })\)._

The next theorem shows that the dissimilarity dimension is upper bounded by the eluder dimension (see Appendix C.1 for a proof):

**Theorem 11**.: _Let \(=\), \(=_{}\), \(>0\), \(^{*}\). Then \(}_{}(,,3/2) 9\, }_{}(,)\)._

Nevertheless, as the next example shows, the eluder dimension can be arbitrarily large, while the dissimilarity dimension remains constant. In this example, the action set is a circle in \(^{2}\), that is, \(=\{^{2}:\,\|\|=1\}\). We fix two open semicircles \(U_{0},U_{1}\) with positive \(x\) and \(y\) coordinates, respectively, and for any \(N\) and \(>0\), construct a function class \(_{N,}\) with all the functions \(f:[-1,1]\) obtained by the following process. First, pick one of the semicircles \(U_{j}\) and any \(N\) points from \(U_{j}\). On each of these points, \(f\) can equal either \(+\) or \(-\). Everywhere else in \(U_{j}\), \(f\) equals zero, and everywhere outside \(U_{j}\), it equals the linear function \(,\) parameterized by some \( U_{j}\). Thus, the functions are constructed to be "simple" (namely, linear) near the optimal action \(\), but complex far from it. The eluder dimension is large to capture overall complexity, whereas the dissimilarity dimension is small to capture the simplicity near the optimum. (See Appendix C.5 for the formal construction of \(_{N,}\) and the proof of Proposition 12.)

**Proposition 12**.: _Let \((0,1/2)\), \(N\) and consider the action set \(=\). Then, there is a function class \(_{N,}[-1,1]^{}\), such that for \(_{N,}_{N,}\), \(=_{}\), it holds that \(_{}(_{N,},1,) 16\), but the eluder dimension is lower bounded as \(_{}(_{N,},) N\)._

Thus, our regret bound based on the dissimilarity dimension implies that (optimistic) least squares algorithms have a regret independent of \(N\). The same analysis with the eluder dimension  yieldsa regret bound scaling polynomially with \(N\). This shows that in the cases when the function classes are simple near the optimum, but complex far from it, the dissimilarity dimension can better capture the statistical complexity of bandit optimization than the eluder dimension.

### Dissimilarity dimension bounds

We next derive dissimilarity dimension bounds for several standard bandit classes. Existing bounds on eluder dimension can be used to immediately bound the dissimilarity dimension, but in several cases we are able to obtain tighter bounds.

We first consider _linear bandits_. Let \(_{n}=\{^{n}:\;\|\| 1\}\) be the unit ball in \(^{n}\). Actions are chosen from a set \(_{n}\); the reward function class is \(^{lb}=\{f_{}:\}\), where \(_{n}\) and \(f_{}()=,\). The corresponding set of alternatives is denoted \(^{lb}=^{lb}\). In this case we obtain the following bound (see Appendix C.2 for a proof):

**Theorem 13** (Linear bandits).: _Let \(^{lb}\) be as defined above, let \(=_{}\), and let \(>0\), \(^{}\). Then \(_{}(^{lb},,) 4n+3\). Moreover, when \(=1\), then \(_{}(^{lb},,) 2n+1\)._

The proof proceeds by deriving an upper bound as well as a lower bound on the rank of the matrix \(\) with entries \(M_{ij}=(z_{i} z_{j})-c\) obtained from elements \(z_{1},,z_{d}\) that satisfy the dimension condition for \(d=_{}(,,)\) with a scalar \(c\). The upper bound on the rank is \(n+1\), and the lower bound is \(d/4\) (which can be tightened to \(d/2\) when \(\) is symmetric). The upper bound is obtained by basic linear algebra and the lower bound from a standard result on ranks of perturbed identity matrices (2, Lemma 2.2). Combining these bounds then yields the claim of Theorem 13. Similar to existing bounds on eluder dimension (24, Proposition 6), our bound in Theorem 13 is linear in \(n\). However, the eluder dimension bound has an additional dependence on \(1/\), while our bound does not.

Next, we consider _generalized linear model_ (GLM) bandits. Similar to linear bandits, the action set is \(_{n}\), but the function class includes a nonlinearity. Specifically, we are provided with a function \(g:\) that is differentiable and strictly increasing, and consider the function class \(^{glm}=\{f_{}:\}\) where \(_{n}\) and \(f_{}()=g(,)\). Furthermore, we assume that there are \(,>0\) such that for all \(\), \(\), we have \( g^{}(,) {h}\). Define \(r=/\). We again denote \(^{glm}=^{glm}\). Using an existing bound on the eluder dimension for GLM bandits ((24), Proposition 7) and the fact that our dimension is bounded by the eluder dimension (Theorem 11), we obtain the following bound (see Appendix C.3 for a proof):

**Theorem 14** (GLM bandits).: _Let \(^{glm}\) be as defined above, let \(=_{}\), and let \(>0\), \(^{}\). Then \(_{}(^{glm},,) O(nr^{2}( {h}/))\)._

By considering a different proof technique, along the lines of Theorem 13, it might be possible to tighten this bound. We leave this extension for future work.

Next, we consider a bandit setting that is similar to GLMs, but in this case the non-linearity is provided by the non-differentiable rectified linear unit (ReLU) activation function \((x)=\{x,0\}\). We consider the action set \(=_{n}\), and the set of reward functions \(^{}\) consisting of all functions of the form \(f_{,b}()=(, -b)\) for some \(_{n}\) and \(b[0,1)\). The subset of \(^{}\) with a fixed value of \(b\) is denoted \(^{}_{b}\), and we consider the set of alternatives \(^{}_{b}=^{}_{b}_ {n}\).

Unlike the classes considered above, this setting can be shown to be challenging to learn in the general case. Indeed, it turns out that eluder dimension (as well as a related measure called star dimension) is growing at least exponentially with \(n\)(21; 10). The same lower bound can be shown for the dissimilarity dimension by a similar proof technique. The following theorem also provides an exponential upper bound, showing that in certain regimes the exponential dependence is tight (see Appendix C.4 for a proof):

**Theorem 15** (ReLU bandits).: _Let \(^{}\) be as defined above, let \(=_{}\), and let \(,b>0\) such that \(b 1-\). Then \(_{}(^{}_{b},1-b,)=O ^{-n/2}\), and \(_{}(^{}_{1-},,)= ^{-n/2}\)._

We note that previous work ((10), Theorem 5.1) has shown that for a function class of one-layer neural networks with ReLU activations, obtaining sublinear regret requires \(T=(^{-(n-2)})\).

## 6 Conclusion

In this paper, we have introduced a new model for interactive estimation and proposed a new combinatorial dimension, called dissimilarity dimension, to study the hardness of learning in this model. In (stochastic, correlational) statistical query learning, our dimension is polynomially related to the strong SQ dimension. In bandits, our dimension is upper bounded by the eluder dimension, and there are examples where the dissimilarity dimension leads to much tighter regret bounds.

While this work provides an initial investigation of the dissimilarity dimension, many open questions remain. For example, our regret bound for the general setting scales as \(d^{1.25}\). Is it possible to tighten this to linear dependence, as is the case, for example, for eluder dimension? On the algorithmic side, we currently require solving a least squares problem of size \(t\) in iteration \(t\). Although we also introduce an algorithm that leverages an online regression oracle (see Appendix A.3), the oracle-based approach still requires solving a least squares problem (on the data smoothed by the oracle). Is it possible to derive dissimilarity-dimension-based regret bounds directly for the predictions produced by the oracle? Ultimately, we hope investigations of relationships between dissimilarity dimension and related notions may help us understand the hardness of learning in interactive settings.