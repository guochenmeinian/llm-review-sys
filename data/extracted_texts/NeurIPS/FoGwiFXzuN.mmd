# How Far Can Transformers Reason?

The Globality Barrier and Inductive Scratchpad

 Emmanuel Abbe\({}^{1,2}\), Samy Bengio\({}^{1}\), Aryo Lotfi\({}^{2}\), Colin Sandon\({}^{2}\), Omid Saremi\({}^{1}\)

\({}^{1}\)Apple \({}^{2}\)EPFL

###### Abstract

Can Transformers predict new syllogisms by composing established ones? More generally, what type of targets can be learned by such models from scratch? Recent works show that Transformers can be Turing-complete in terms of expressivity, but this does not address the learnability objective. This paper puts forward the notion of _globality degree_ of a target distribution to capture when weak learning is efficiently achievable by regular Transformers. This measure shows a contrast with the expressivity results of Transformers captured by \(TC^{0}/TC^{1}\) classes (further studied here), since the globality relates to correlations with the more limited \(NC^{0}\) class. We show here experimentally and theoretically under additional assumptions that distributions with high globality cannot be learned efficiently. In particular, syllogisms cannot be composed on long chains. Further, we develop scratchpad techniques and show that: (i) agnostic scratchpads cannot break the globality barrier, (ii) educated scratchpads can break the globality with intermediate steps, although not all such scratchpads can generalize out-of-distribution (OOD), (iii) a notion of 'inductive scratchpad', that composes the prior information more efficiently, can both break the globality barrier and improve the OOD generalization. In particular, some of our inductive scratchpads can achieve length generalizations of up to \(6\) for some arithmetic tasks depending on the input formatting.

## 1 Introduction

Transformers  have proved to have strong learning capabilities, in particular in applications with large amounts of text, image, or audio data . Some reasoning capabilities are also notable in these settings, however, the picture deteriorates when the target complexity increases, such as in tasks involving more advanced forms of'reasoning' . While reasoning is present at all levels of learning, it is pushed to a higher level in tasks such as logic or mathematics, where 'learning by seeing enough representative examples' is precluded by the more combinatorial nature of the task. For such tasks, combining learned concepts in order to extrapolate seems necessary, as for the length generalization problem . Current Transformer-based models exhibit difficulties learning at scale on such tasks. Can we understand why and what is missing? We start with a specific motivational example before expanding the discussion to more general tasks.

### Syllogisms composition

Reasoning relates to the process of inferring new knowledge by composing efficiently some prior knowledge. A basic notion of reasoning is syllogism composition, e.g., inferring \(a c\) from \(a b\) and \(b c\). For instance, one may be given a set of implications:

 task 1 has priority over task 2 \\ task 1 has priority over task 3 \\ task 4 has priority over task 1 \\ task 1 has priority over task 5 \\  
 \(x>2\)\(\)\(x^{2}>3\) \\ \(x>2\)\(\)\((x-1)(x+1)>1\) \\ \(4^{x}>17\)\(\)\(x>2\) \\ \(x>2\)\(\)\(x-x^{4}>1-2x^{4}\) \\ 

and without additional background information, one would like to know using logic whether

\[| 56.905512pt4^{x}>17 .}{{}} x^{2}>3.\]

The goal here is to identify whether a syllogism can be composed1 by prior ones. Simplifying the input format, the above correspond to identifying paths in token sequences describing the directed edges of an underlying graph, i.e., whether there is a directed path \(3 5\) (case 1) or \(4 2\) (case 2) using the directed edges \(\{(1 2),(1 3),(4 1),(1 5)\}\).

This type of task is nontrivial for current LLMs, and we refer to Appendix I for experiments with GPT models.2 Note that here we are not interested specifically in solving a graph-based task, but rather in understanding when Transformers can compose and more generally how far they can do so. We would like to identify particular measures on the data distribution (e.g., syllogisms topologies in the above example) that capture when Transformers can efficiently learn.

### Hardness of long compositions

Consider the previous syllogism composition task where implications are drawn on a graph with 24 edges drawn randomly over 24 vertices. Picking vertices at distances 1 to 4 for the connected case and picking disconnected vertices uniformly at random lets a Transformer achieve a test accuracy of more than 80% after about 2000 iterations. However, does this mean that the model has learned to compose syllogisms, or has it found shortcuts, e.g., based on node degrees, to guess the implications often enough? In Appendix B.1, we provide empirical evidence supporting the latter. Motivated by this issue and to preclude spurious correlations, we consider the following distribution.

**Definition 1** (Cycle task).: _For \(n 1\), consider the binary classification task with equiprobable classes defined by_

1. _Class 1: a graph uniformly drawn on_ \(2n\) _vertices with two disjoint cycles of length_ \(n\) _and a pair of vertices in disjoint cycles queried for path;_
2. _Class 2: a graph uniformly drawn on_ \(2n\) _vertices with one cycle of length_ \(2n\) _and a pair of vertices at distance_ \(n\) _queried for path._

_The input of this task is the graph edge set with the queried vertices. The label is 0 if the two queried vertices are not connected (Class 1) and 1 if they are (Class 2). See Figure 0(a) for an illustration._

Figure 0(b) shows that the learning complexity increases 'exponentially' as \(n\) grows using GPT2-style Transformers of more than \(10M,25M,85M\) parameters; e.g., the \(10M\) model fails to learn for \(n 7\) in \(100k\) iterations. Why is that? Can a larger scale further help here?

_Can a large (poly-size) Transformer learn the cycle task when \(n\) gets large? If not, why so?_

A challenge for the cycle task is that there is no clear 'low-complexity pattern' in the input representation that indicates whether there are 1 or 2 cycles. No simple statistics based on degrees, edge counts,

Figure 1: Illustration of the cycle task for \(n=4\) (left) and the complexity to learn it (right).

or finite motif counts that can tell if the vertices are connected or not. One has to consider at least \(n\) edges in order to get any correlation with the presence of a path. In other words, the task requires a 'global reasoning' involving a 'large' number of input tokens and this seems hard for Transformers.

### Hardness of global reasoning

As discussed, the cycle task appears to be challenging for Transformers as it requires some global reasoning. Other tasks such as subset parities exhibit the same challenge. However the latter can be proved to be not efficiently learnable by various regular neural networks and noisy gradient descent, as one can get explicitly a class of functions (through orbit arguments [12; 13]) that has large statistical dimension  or low cross-predictability [12; 15] (see Appendix A.4). For the cycle task, we have a single distribution, and it is unclear how to use the invariances of Transformers to get arguments as in [12; 13], as the input distribution is not invariant under the symmetries of the model. We thus would like to develop a more general complexity measure that unifies why such tasks are hard for Transformer-like models and that formalizes the notion of 'global reasoning barrier' when models are trained from scratch. We also would like to understand how the scratchpad methodologies that have proved helpful in various settings (see Section 3) can help here. This raises the questions:

_(1) How can we formalize the 'global reasoning barrier' in general terms? (2) Can we break the 'global reasoning barrier' with scratchpad methodologies?_

### Our contributions

We provide the following contributions:

* We introduce the notion of _globality degree_ in Definition 2 to capture when weak learning is efficiently achievable by Transformers. The globality degree measures the least number of tokens required in addition to the token histogram to correlate nontrivially with the target; it is also related in Lemma 6 to correlations with \(NC^{0}\) circuits, showing the contrast between learnability and expressivity controlled by \(TC^{0}/TC^{1}\) with constant/logarithmic depth . It is an explicit measure that applies to a data distribution without requiring an orbit argument to infer a class of distributions [12; 13], giving a tight proxy for models like Transformers. We provide the following results based on the globality degree:
* A general conjecture (Conjecture 1), backed by experimental results, that claims efficient weak learning is achievable by a regular Transformer if and only if the globality degree is constant.
* Theorem 1 that proves the negative side of the above conjecture, the _globality barrier_, in an instance of the cycle task under certain technical assumptions. (The cycle task is also put forward in the paper as a simple benchmark to test the global reasoning capabilities of models.)
* We then switch to the use of'scratchpads' to help with the globality barrier:
* Agnostic scratchpad: we extend Theorem 1 to cases where a poly-size scratchpad is used by the Transformers, without any direct supervision of the scratchpad (i.e., the scratchpad mainly provides additional memory/compute). This shows that efficient weak learning is still not possible with such an agnostic scratchpad if the globality is non-constant. An educated guess about what to learn in the scratchpad based on some target knowledge is thus required.
* Educated scratchpad: we generalize the measure of globality to the 'autoregressive globality' to quantify when an educated scratchpad is able to break the globality of a task with subtasks of lower globality. We give experimental results showing that educated scratchpads with constant autoregressive globality allow Transformers to efficiently learn tasks that may originally have high globality. This gives a way to measure how useful a scratchpad can be to break a target into easier sub-targets.
* We introduce the notion of _inductive scratchpad_, a type of educated scratchpad that exploits 'induction' compared to a fully educated scratchpad and thus composes more efficiently the prior state information. We show that when the target admits an inductive decomposition, such as for the cycle, arithmetic, or parity tasks, the inductive scratchpad both breaks the globality and improves the OOD generalization in contrast to fully educated scratchpads. This gives significant length generalization on additions (from 10 to 18 or from 4 to 26 depending on the method) and parities (from 30 to 50-55). For instance, using different methods,  can length generalize from 10 to 12 digits for additions, and  can get roughly 10 extra bits for parities.
Results on the global reasoning barrier

**Prior literature.** Much work in the literature has been devoted to complexity measures for the sample/time complexity of learning. The largest portion is devoted to target classes in PAC settings, e.g., with the VC dimension measures , and some to statistical query (SQ) settings with the statistical dimension measures [14; 19]. Here, we are however interested in measures that are relevant to (1) regular Transformers trained by (S)GD, and (2) data distribution fixed by a task. Some recent works have studied complexity measures for (S)GD-trained neural networks. Various settings and measures have been used, such as the noise sensitivity [20; 6; 21], the cross-predictability [12; 15], the NTK alignment [22; 23], the INAL , the \(G\)-alignment , the information and generative exponents [25; 26; 27] and the leap ; we refer to Appendix A.4 for discussions on these.

However, despite this significant body of work, finding a simple measure giving a tight proxy for Transformer weak learning (i.e., the first non-trivial learning requirement) on a given data distribution, remains unsettled. We next propose such a measure.

### Defining globality and auto-regressive globality

We define now the notion of globality degree, which in turn will quantify the notion of globality (or global reasoning) barrier.

**Definition 2**.: **(Globality degree)** _For (a sequence of) distributions \(D\) on \(^{n}\), where \(\) is a finite alphabet set of \((n)\)-cardinality, define the globality degree of \(D\), \((D)\), as the smallest number of variables \(k[n]\) for which there exists \(S\), \(|S|=k\) such that_

\[I(X[S],_{X};Y)=n^{-O(1)}\]

_where \((X,Y) D\) and \(_{X}\) is the empirical measure of \(X\) (i.e., the histogram of tokens in \(X\))._

**Remark 1**.: _The globality degree, or simply globality, of a distribution measures the least number of input tokens to attend to in order to correlate non-trivially with the label when also given the histogram of tokens. The specific choice of the mutual information is not crucial, but one must use a proper measure of dependency (i.e., not just linear correlations), and the mutual information can have convenient chain rule properties. The definition can be related to correlations with \(NC^{0}\) circuits (besides for the histogram requirement, see Lemma 6) and also to low-degree polynomial testing, except that it is more general than the latter as it applies to arbitrary token space (without requiring polynomial definitions). Finally, we require the globality to achieve an inverse-polynomial mutual information, the weakest form relevant to weak learning with an inverse-polynomial edge, but one may naturally define the stronger notion with a mutual information of \((1)\)._

We now define the globality in the autoregressive setting.

**Definition 3**.: **(Globality degree in autoregressive setting)** _For \(D\) on \(^{n}^{m}\), define \((D)\) as the smallest integer \(k\) for which there exist sets of indices \(S_{1},,S_{m}\), \(|S_{t}| k\) for all \(t[m]\), such that_

\[I((X,Y_{<t})[S_{t}],_{X,Y_{<t}};Y_{t})=n^{-O(1)},\]

_where \((X,Y) D\) and \(_{X,Y_{<t}}\) is the empirical measure of \((X,Y_{<t})\)._

In the auto-regressive setting, the globality is mostly relevant when weak learning gives strong learning, in order to let the scratchpad learn each step.

As we will see in the next section, the globality degree is put forward as a tight proxy to understand efficient weak learning of regular Transformers for arbitrary data distributions. We first present the operational advantages of the definition, going back to the running example of the cycle task.

Attributes of \(\) and some examples.The globality has the attributes of being (i) a fairly explicit measure, (ii) applicable to any data distribution on tokens without having to infer a distribution class from the model invariances to estimate the distribution complexity, (iii) not limited to i.i.d. inputs but any input distribution, (iv) relevant to current models of interest such as Transformers.

In particular, back to the cycle task, we have that any set of \(n-1\) edges have the same distribution in Class 1 or 2, therefore the globality is at least \(n\):

**Lemma 1**.: _We have \(((n)) n\)._As discussed in the next section, this explains why the cycle task is hard to learn. In contrast, the example at the beginning of Section 1.2 has a much lower globality, as being connected correlates to query nodes having large enough degrees, and thus it can be expected for the model to learn with non-trivial accuracy (e.g., by using degree shortcuts).

### Transformers require low globality: formal results

**Definition 4**.: _A neural network of input dimension \(n\) (i.e., a directed acyclic graph with \(n\) inputs) and depth \(d\) (i.e., the longest path length from input to output) is_

1. _T-regular if it is a Transformer (e.g._,_ _[_1, 29_]__) of polynomial size with Normal Gaussian i.i.d. positional embeddings, Normal Gaussian i.i.d. weights, and bidirectional attention._
2. _T-regular with_ \(s\)_-scratchpad if we have a constant sized token alphabet_ \(\) _and a T-regular Transformer that takes an_ \(m\)_-sequence in_ \(\) _and outputs a probability mass function on_ \(\) _(with well-behaved_3 _softmax). To compute the value of this net and scratchpad on an input_ \(X^{n}\) _we set_ \(X_{n}=X\) _and then for each_ \(m\{n,,n+s-1\}\) _draw_ \(x_{m+1}\) _from the probability distribution represented by the net's value on_ \(X_{m}\) _and set_ \(X_{m+1}=X_{m} x_{m+1}\) _(concatenation). Then, we consider_ \(x_{n+s}\) _to be the overall output of the net._4__ 

**Remark 2**.: _In this paper, we focus on learnability via descent algorithms, but for the simpler question of expressivity, one wants to know whether there is any choice of parameters for which a Transformer can compute a target function (with limits to how precisely it can record values). (i) In , the expressivity of Transformers with constant alphabet size and values recorded to inverse-polynomial accuracy is investigated. It is shown that such a Transformer of constant depth was limited to computing functions in \(TC^{0}\). Conversely, it also showed that for any \(TC^{0}\) function, there is a constant-depth Transformer and instruction string such that when the Transformer is given the instruction string and \(x\) as its input it computes the function on \(x\). The same technique would extend to show that with logarithmic depth, one can reach \(TC^{1}\) (which includes connectivity tasks). (ii) In , well-behaved6 Transformers with a scratchpad are considered. It is shown that a Transformer with a scratchpad of logarithmic length is limited to computing functions in logspace. We tighten this to \(TC^{0}\) in Lemma 5 in Appendix H. On the other hand,  also shows that any function in \(P\) is computable by a Transformer with a scratchpad of polynomial length. (iii) If we allow Transformers of poly depth then we can convert any poly-sized circuit to a Transformer by replacing each gate in the circuit with an attention head that attends to the values of the appropriate input tokens or attention heads and performs the appropriate computation on them. That means any function in \(P\) (including the cycle task) is computable by a Transformer of poly depth and size._

We now state the general conjecture putting forward the globality barrier for learning.

**Conjecture 1**.: _A distribution \(P_{X,Y}\) with well-behaved7\(P_{X}\) is efficiently weakly8learnable by a T-regular Transformer if and only if9\(P_{X,Y}\) has constant globality._

**Remark 3**.: _(1) An essential property of the model for the above conjecture is that the probability distribution of the function computed by the model is invariant under permutations of the inputs, and if it is trained reasonably on samples drawn from a distribution drawn from a class that is symmetric under permutations of the inputs, its distribution will retain its symmetry under permutations of the inputs. For MLPs, we expect most of the results in this paper to apply, with the modification of the globality not having access to the empirical measure of \(X\), since one has additional symmetry obtained by exchanging tokens. (2) If we were to use causal masking or a specific choice of positional embeddings that would make it easier for the Transformer to focus on specific relevant subsets of the inputs, one could potentially learn functions with higher globality. For instance, we would expect to be unable to learn a function that computes the parity of some subset of \((n)\) input bits. However, if we had a positional embedding that gave one value for all of the active bits and mapped other bits to \(0\), then the Transformer would probably be able to learn the function in question. Likewise, causal masking makes it so that the first few elements of any list the Transformer computes depend only on the first few tokens, which makes it easier to learn functions that would rely on those symbols._

We prove the negative side of Conjecture 1 for a variant of the cycle task.

**Theorem 1**.: _Let \(G\) be a directed graph which consists of a cycle of length \(3n\) with probability \(2/3\) and 3 cycles of length \(n\) otherwise. Next, if there are \(3\) cycles pick one vertex from each and if there is one cycle pick 3 vertices that are each \(n\) edges apart. Then, label these vertices with \(a\_0\), \(b\_0\), \(c\_0\) uniformly at random. Next, number every other vertex by the distance from one of these three to it, and for each \(i\), label uniformly at random the vertices at distance \(i\) by \(a\_i\_0\_i\), and \(c\_i\). Finally, store the edges between \(a\_i\_1,b\_i-1,c\_i-1\) and \(a\_i\_0\_i,c\_i\) in \(X\) (as described in Figure 2), and let \(Y\) report whether \(a\_0,b\_0,c\_0\) are in the same cycle or not. Then if we train a T-regular neural network on \((X,Y)\) generated in this manner using population9 gradient descent with polynomial hyperparameters10 and a differentiable loss function then the network fails to weakly learn._

The proof of Theorem 1 is presented in Appendix F.

### Agnostic scratchpads cannot break the globality

Next, we put forward a conjecture that agnostic scratchpads (scratchpads without direct supervision on the scratchpad tokens) cannot break the globality barrier. See Appendix E for further discussion.

**Conjecture 2**.: _Consider training a \(T\)-regular net with an \(s\)-scratchpad to learn \(P_{X,Y}\) on a constant-sized alphabet by means of the following SGD algorithm. At each timestep, we draw a random sample \((X,Y)\) and compute a value for the net with scratchpad on \(X\). Let \(\) be the resulting scratchpad and for each \(i s\) and each \(_{i}\), define an alternative scratchpad by setting the first \(i-1\) entries of this scratchpad equal to those of \(\), setting its \(i\)-th entry to \(\), and using the net to compute the rest of its values. Then, regard the loss associated with setting the \(i\)th entry of the scratchpad to \(\) as the loss resulting from the associated scratchpad, and use the resulting gradient to carry out one step of SGD. Then if \(P_{X}\) is a well-behaved probability distribution, \(P_{X,Y}\) is efficiently weakly learnable by a T-regular neural network with a scratchpad if and only if \(P_{X,Y}\) has constant globality._

A natural counterpart of Theorem 1 holds for the previous conjecture (see Theorem 2). In order to define the Transformer's loss on any given input it takes the expectation over every possible value of the scratchpad it might generate, and its proof is essentially identical to that of Theorem 1.

## 3 Scratchpads to break the globality

**Prior literature.** It has been shown that training Transformers with the intermediate steps required to solve a problem can enhance learning. This idea is usually referred to as providing models with a scratchpad . The improved performance due to scratchpads has been reported on a variety of tasks including mathematical tasks and programming state evaluation . See Appendix A for further references.

### Educated scratchpad

We now provide a quantitative understanding of how the scratchpad can help with the notion of globality in the autoregressive setting (Definition 3). Assume that we want to learn target \(Y\) from input \(X^{n}\) such that \((X,Y) D\) and \((D)\) is high. If one can design intermediate targets \(Y_{1},,Y_{k}\) such that \(Y_{k}=Y\) and the sequence \((X,Y_{<i}) Y_{i}\) has low globality according to Definition 3, then one can expect to learn each step of the sequence efficiently and thus the target at the end. In this case, the intermediate targets give the 'educated scratchpad' (see Figure 3 for an illustration). We now show how designing low-globality scratchpads can help with learning by focusing on two examples: parity functions and the cycle task.

Results for learning parities.Consider learning parity function \(y=f(x_{1},,x_{n})=x_{1}x_{2} x_{k}\) where \(x_{1},,x_{n}\) are drawn i.i.d. in \(\{ 1\}\) with uniform distribution. For \(k\), one can easily check that the globality of this task is \(k\) as any \(k-1\) coordinates are independent of the output and the histogram of the tokens does not help. Parity functions are known to be hard to learn . More specifically, it has been previously shown that as \(k\) increases the parity task becomes harder to learn to the point that parity of degree \(\{k,n-k\}=(1)\) cannot be learned in \(poly(n)\) time with standard poly-size neural networks under standard training assumptions . Note that this is consistent with our results, as the globality is non-constant.

Now, consider learning this task with a scratchpad that breaks down the learning with intermediate targets \(y_{1},y_{2},,y_{k}\) such that

\[y_{1}=x_{1}, y_{2}=x_{1}x_{2}, y_{i}=y_{i-1}x_{i},  y_{k}=x_{1}x_{2} x_{k}=f(x),\]

i.e., \(y_{i}\) is the cumulative product of the first \(i\) bits. Note that each intermediate target \(y_{i}\) can be computed by using at most 2 of the previous tokens, implying the following lemma.

**Lemma 2**.: _The parity task with the cumulative product scratchpad has a globality of \(2\)._

Transformers with such a scratchpad can in fact easily learn parity targets, see Appendix B.3.

Results for the cycle task.Consider the cycle task and a scratchpad that learns the depth-first search (DFS) algorithm from the source query node.11 For example, consider the following input corresponding to two cycles \(a,x,q\) and \(n,y,t\): a>x; n>y; q>a; t>n; y>t; x>q; a?t;. In this case, doing a DFS from node a gives a>x>q>a where the fact that we have returned to the source node a and not seen the destination t indicates that the two nodes are not connected. Therefore, the full scratchpad with the final answer can be designed as a>x>q>a?0. Similarly, if the two nodes were connected the scratchpad would be a>...>t;1. One can easily check that the cycle task becomes low-globality with the DFS scratchpad.

Figure 3: An illustration showing how scratchpads can break the globality. The target may be efficiently learned if each scratchpad step is of low globality given the previous ones.

**Lemma 3**.: _The cycle task with the DFS scratchpad has a globality of \(3\)._

This follows from the fact that one only needs to find the next node in the DFS path (besides the label), which one can check with polynomial chance by checking the first edge.

In Figure 3(a) we show that a decoder-only Transformer with the DFS scratchpad in fact learns the cycle task when \(n\) scales.

**Remark 4**.: _If one has full knowledge of the target function, one could break the target into sub-targets using an educated scratchpad to keep the globality low and thus learn more efficiently (of course one does not have to learn the target under full target knowledge, but one may still want to let a model learn it to develop useful representations in a broader/meta-learning context). One could in theory push this to learning any target that is poly-time computable by emulating a Turing machine in the steps of the scratchpad to keep the overall globality low. Some works have derived results in that direction, such as  for some type of linear autoregressive models, or  for more abstract neural nets that emulate any Turing machine with SGD training. However, these are mostly theory-oriented works. In practice, one may be instead interested in devising a more 'generic' scratchpad. In particular, a relevant feature in many reasoning tasks is the power of induction. For instance, the parity and cycle tasks are two examples where learning an induction step function appears useful._

### Inductive Scratchpads

As discussed, scratchpads can break the global reasoning barrier with appropriate mid-steps. In this part, however, we show that fully educated scratchpads can be sensitive to the number of reasoning steps, translating into poor out-of-distribution (OOD) generalization. As a remedy, we put forward the concept of inductive scratchpad which applies to various reasoning tasks as in previous sections.

#### 3.2.1 Educated scratchpad can overfit in-distribution samples

Consider the cycle task with \(24\) nodes. For the test distribution, we use the normal version of the cycle task, i.e., either two cycles of size \(12\) and the nodes are not connected or a single cycle of size \(24\) where the distance between the query nodes is \(12\). For the train distribution, we keep the same number of nodes and edges (so the model does not need to rely on new positional embeddings for the input) but break the cycles to have uneven lengths: (1) a cycle of size \(6\) and a cycle of size \(18\) when the two nodes are not connected (the source query node is always in the cycle of size \(6\)) or (2) a cycle of size \(24\) where the nodes are at distance \(6\). Thus, in general, we always have \(24\) nodes/edges in the graphs. However, the length of the DFS path (i.e., number of reasoning steps) is \(6\) at training and \(12\) at test. We trained our model on this version of the task with the DFS scratchpad. The results are shown in Figure 3(b). We observe that the model quickly achieves perfect accuracy on the training distribution, yet, it fails to generalize OOD as the model overfits the scratchpad length and number of reasoning steps. In the next part, we introduce the notion of inductive scratchpad to fix this problem.

Figure 4: (Left) Learning the cycle task with a scratchpad. (Right) OOD generalization for the DFS and inductive scratchpads (see Section 3.2.1).

#### 3.2.2 Inductive scratchpad: definition and experimental results

In a large class of reasoning tasks, one can iteratively apply an operation to some state variable (e.g., a state array) to compute the output. This applies in particular to numerous graph algorithms (e.g., shortest path algorithms such as BFS or Dijkstra's algorithm), optimization algorithms (such as genetic algorithms or gradient descent), and arithmetic tasks.

**Definition 5** (Inductive tasks).: _Let \(Q\) be the question (input). We say that a task can be solved inductively when there is an induction function (or a state transition function) \(g\) such that_

\[s=g(Q,), s=g(Q,s),, s[k]=g(Q,s[k-1]),\]

_where \(s,,s[k]\) are the steps (or states) that are computed inductively. For example, the steps/states could be an array or the state of an automata that is being updated. Note that the termination is determined by the state. In the context of Transformers, one can use the generation of the end of sequence token <EOS> to terminate._

Inductive tasks with a fully educated scratchpad can overfit proofs.The fully educated scratchpad for the question \(Q\) as input would be s;s;...;s[k]<EOS>, where the token <EOS> ends the generation. However, this method may not fully utilize the fact that each state is only generated from the last state by applying the same (set of) operation(s). In particular, s[k] typically attends to all of the previous states. Further, the model may not be able to increase the number of induction steps beyond what it has seen during training, as shown in Figure 3(b) for the cycle task.

Now we show that by using attention masking and reindexing the positions of the tokens, one can promote the desired 'inductive' behavior. We call this the inductive scratchpad. As three showcases, we demonstrate that the inductive scratchpad can improve OOD generalization on the cycle task and length generalization on parity and addition tasks.

Inductive scratchpad implementation.The inductive scratchpad for an inductive task is similar in format to the fully educated scratchpad but it has the following modifications: (1) tokens: two new special tokens are used: the <START> token which separates the question from the intermediate states and the <STATE> token (denoted # hereafter) to separate the states. Using these tokens, for an input question \(Q\), the format of the inductive scratchpad reads <START>s#s#...#s[k]<EOS>. (2) generation: we want the model to promote induction and thus 'forget' all the previous states except the last one for the new state update. I.e., we want to generate tokens of s[i+1] as if the input was Q<START>s[i]#. To implement this, one can use attention masking and reindex positions (in order to have a proper induction) or simply remove the previous states at each time; (3) training: when training the scratchpad, we want the model to learn the induction function \(g\), i.e., learning how to output s[i+1]# from Q<START>s[i]#, which can be achieved with attention masking and reindexing the positions. As a result, the inductive scratchpad can be easily integrated with the common language models without changing their behavior on other tasks/data. We refer to Appendix C.2 for a detailed description of the inductive scratchpad implementation.

Inductive scratchpad for the cycle task.The DFS scratchpad of the cycle task can be made inductive by making each step of the DFS algorithm a state. E.g., for the input a>x;n>y;q>a;t>n;y>t; x>q;a7t;, the DFS scratchpad is a>x>q>a;0<EOS>, and the inductive scratchpad becomes <START>a7xq#a;0<EOS> where each state tracks the current node in the DFS. In Figure 3(b), we show that the inductive scratchpad for the cycle task can generalize to more reasoning steps than what is seen during training, and thus generalize OOD when the distance between the nodes is increased.

Length generalization for parity and addition tasks.We can use inductive scratchpads to achieve length generalization for the parity and addition tasks. For parities, we insert random spaces between the bits and design an inductive scratchpad based on the position of the bits and then compute the parity iteratively. The performance of this inductive scratchpad is depicted in Figure 4(a) where we can see a Transformer trained on inputs with up to \(30\) bits can generalize to samples with up to \(50/55\) bits depending on the seed. For the addition task, we propose two inductive scratchpads. (1) _Random space method_ that requires random spaces between the digits in the input and uses the position of the digits to compute the addition digit-by-digit (similar to the parity). With this scratchpad, we can generalize to numbers with 18 digits while training on numbers with up to 10 digits. (2) _Shift method_ that uses random tokens in the input and computes the addition digit-by-digit by shiftingthe operands. The latter enables us to generalize from 4 to 26 digits at the cost of having a less natural input formatting. The results for different seeds are provided in Figure 4(b). See details of these scratchpads in Appendices B.4, B.5.12 A rough comparison between the performance of different methods for addition is given in Table 1. Note that the settings used in these works are not exactly the same, e.g., our methods often work with smaller models and more natural input formatting. See Appendix A.2 for a detailed comparison for both the parity and addition tasks.

## 4 Conclusion

This paper shows that for the learning objective and in contrast to expressivity results, Transformers trained from scratch have a 'global reasoning barrier' quantified by the globality degree. The globality measure has a simpler form and broader applicability range than prior measures as discussed in Appendix A.4, it also has tighter applicability for Transformers. The measure is currently defined for weak learning (inverse-polynomial or constant edge), and a natural next step is to consider stronger learning requirements with notions of 'globality leap', e.g., expanding the current work in the direction of  but for more general distributions. Investigating the role of curriculum learning is another natural direction and we provide preliminary results here in Appendix B.2.

The globality is also defined in the autoregressive setting, to better quantify when scratchpads can break targets into easier sub-targets. Two negative results are shown for scratchpads: agnostic scratchpads still suffer from the globality barrier, and fully educated scratchpads can have poor OOD generalization. This motivates the introduction of the inductive scratchpad.

The inductive scratchpad can be used for a broad range of reasoning/algorithmic tasks and can easily be integrated into Transformers. The inductive scratchpad is independent of the number of reasoning steps since the model only learns the induction function. Consequently, the model better generalizes to inputs requiring different numbers of reasoning steps. This gives improvements of OOD/length generalization for the cycle task (Figure 3(b)), parity (Figure 4(a)), and addition (Figure 4(b)).

Another interesting aspect is whether the model can use an inductive behavior on new tasks if it was pre-trained on prior inductive tasks. Note that the inductive behavior of the inductive scratchpad is only determined by two special tokens. Thus, in principle, models can generate these special tokens and go into the inductive state for other tasks if pre-trained on inductive data. We leave the general investigations of pre-trained models and the automated learning of more general scratchpads, capitalizing on the measures defined here, to future works.

 
**Method** &  &  &  &  &  & **random space method** & **shift method** \\ 
**Performance** & \(8 9\) & \(10 12\) & \(40 50\) & \(5 15\) & \(40 65\) & \(10 18\) & \(4 26\) \\  

Table 1: Length generalization of different methods for the addition task where our methods are shown in bold. \(a b\) means generalizing to \(b\) digits when trained on \(a\) digits.

Figure 5: Length generalization for parity and addition tasks using different random seeds. The medians of the results are highlighted in bold.