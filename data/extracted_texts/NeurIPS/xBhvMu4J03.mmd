# Do Not Marginalize Mechanisms, Rather Consolidate!

Moritz Willig

Technical University of Darmstadt

moritz.willig@cs.tu-darmstadt.de &Matej Zecevic

Technical University of Darmstadt

matej.zecevic@tu-darmstadt.de &Devendra Singh Dhami

Eindhoven University of Technology

Hessian Center for AI (hessian.AI)

d.s.dhami@tue.nl &Kristian Kersting

Technical University Darmstadt

Hessian Center for AI (hessian.AI)

German Research Center for AI (DFKI)

kersting@cs.tu-darmstadt.de

DSD contributed while being with hessian.AI and TU Darmstadt before joining TUe.

###### Abstract

Structural causal models (SCMs) are a powerful tool for understanding the complex causal relationships that underlie many real-world systems. As these systems grow in size, the number of variables and complexity of interactions between them does, too. Thus, becoming convoluted and difficult to analyze. This is particularly true in the context of machine learning and artificial intelligence, where an ever increasing amount of data demands for new methods to simplify and compress large scale SCM. While methods for marginalizing and abstracting SCM already exist today, they may destroy the causality of the marginalized model. To alleviate this, we introduce the concept of _consolidating causal mechanisms_ to transform large-scale SCM while preserving consistent interventional behaviour. We show consolidation is a powerful method for simplifying SCM, discuss reduction of computational complexity and give a perspective on generalizing abilities of consolidated SCM.

## 1 Introduction

Even complex real world systems might be modeled using structural causal models (_SCM_) (Pearl, 2009) and several methods exist for doing so automatically from data (Spirtes et al., 2000, Pearl, 2009, Peters et al., 2017). While technically reflecting the causal structure of the systems under consideration, SCM might not entail intuitive interpretations to the user. Large scale SCM like, appearing for example in genomics, medical data (Squires et al., 2022, Ribeiro-Dantas et al., 2023) or machine learning (Scholkopf et al., 2021, Berrevoets et al., 2023), may become increasingly complex and thereby less interpretable. Contrary to this, computing average treatment effects might be too uninformative given the specific application, as the complete causal mechanism is compressed into a single number. Ideally a user could express the factors of interest and yield a reduced causal system that isolates the relevant mechanism from the rest of the model.

In contrast to other probabilistic models, SCM model the additional aspect of _interventions_. Consider for example a row of dominoes and its corresponding causal graph as shown in Figure 1. If the starting stone it tipped over, it will affect the following stones, causing the whole row to fall. Humans usually have a good intuition about predicting the unfolding of such physical systems (Gerstenberg, 2022, Beck and Riggs, 2014, Zhou et al., 2023). Second to that, it is easy to imagine what would happen, if we were to hold onto a domino stone, that is, intervening actively upon the domino sequence. Alternatively, we can programmatically simulate these systems to reason about their outcomes. A simulator tediously computes and updates positions, rotations and collision states of all objects inthe system. Depending on the abstraction level of our SCM, computations might be simplified to represent individual stones as binary variables, indicating a stone standing up or getting pushed over. Nonetheless, classical evaluation of simplified SCM is still performed step by step to be able to respect possible interventions on the individual stones. Given that we might only be interested in the outcome. That is, whether or not the last stone will tip over, computing all intermediate steps seems to be a waste of computation, as already noted by Peters and Halpern (2021). Under these premises, we are interested in preserving the ability to intervene while also being computationally efficient. Classical marginalization (Pearl, 2009; Rubenstein et al., 2017) is of no help to us as it destroys the causal aspect of interventions attached to the variables.

**Consolidation vs. Marginalization.** By marginalizing we do not only remove variables, but also all associated interventions, destroying the causal mechanisms of the marginalized variables. The insight of this paper, as alluded to in Figure 1 (center), is that there exists an intermediate tier of _consolidated_ models that fill the gap between unaltered SCM and ones with 'classical' marginalization applied. Consolidation simplifies the causal graph by compressing computations of consolidated variables into the equations of _compositional variables_ that are functionally equivalent to the initial model, while still respecting the causal effects of possible interventions. As such consolidation generalizes marginalization in the sense that marginalization can be modeled by consolidating without interventions (\(=\); see Def. 1 and Sec. 3). If questions involve causality, then consolidation necessarily needs to be considered since it can actually handle interventions (all cases where \(\)). If causal questions are not of concern, then marginalization can be considered as remaining the'standard' marginalization procedure. One perspective on our approach is to describe consolidation as 'intervention preserving marginalization'.

**Structure and Contributions of this paper.** In section two we discuss the foundation of SCM and related work. In section three we formally establish the composition of structural causal equations, partitioned SCM, and finally consolidated SCM. In section four we discuss the possible compression and computational simplifications resulting from consolidating models. We present an applied example for simplifying time series data and in a second example demonstrate how consolidation reveals the policy of a game agent. Finally, in section five, we discuss generalizing abilities of our method and provide a perspective on the broader impact and further research. The technical contributions of this paper are as follows:

* We define _causal compositional variables_ that yield functionally equivalent distributions to SCM under intervention.
* We formalize _(partially) consolidated SCM_ by partitioning SCM under a constraint that guarantees the consistent evaluation with respect to the initial SCM.
* We discuss conditions under which consolidation leads to compressed causal equations.
* We demonstrate consolidation on two examples. First, obtaining a causal model of reduced size and, secondly, revealing the underlying policy of a causal decision making process.

Figure 1: **Consolidation vs. Marginalization. Even simple real-world systems, like this row of dominoes, are composed of numerous intermediate steps. Classical structural causal models require the explicit evaluation of the individual structural equations to respect possible interventions along the computational chain and yield the final value of \(X_{5}\). The intermediate steps (\(X_{2},X_{3},X_{4}\)) might be _marginalized_ to obtain a simplified functional representation. Marginalization, however, loses some causal interpretation of the process, as interventions on the marginalized variables can no longer be performed. _Consolidation_ of causal models simplifies the graph structure (compare to Appendix D.1), while respecting interventions on the marginalized variables. Thus, preserving the ability to intervene on the underlying causal mechanisms. (Best viewed in color.)**

Preliminaries and Related Work

In general we write sets of variables in bold upper-case (\(\)) and their values in lower-case (\(\)). Single variables and their values are written in normal style (\(X\), \(x\)). Specific elements of a set are indicated by a subscript index (\(X_{i}\)). Probability distributions over a variable \(X\) or a set of variables \(\) are denoted by \(_{X}\) and \(_{}\) respectively. A detailed list of notation can be found in E.

Structural Causal Models provide a framework to formalize a notion of causality via graphical models (Pearl, 2009). From a computational perspective, structural equation models (SEM) can be considered instead of SCM (Halpern, 2000; Spirtes et al., 2000). While focusing on computational aspects of consolidating causal equations, we use Pearl's formalism of SCM. Modeling causal systems using SCM over SEM does not affect our freedom, as Rubenstein et al. (2017) show consistency between both frameworks. Similar to earlier works of Halpern (2000); Beckers and Halpern (2019) and Rubenstein et al. (2017), we do not assume independence of exogenous variables and model SCM with an explicit set of _allowed interventions_.

**Definition 1**: _A structural causal model is a tuple \(=(,,,,_{ })\) forming a directed acyclic graph \(\) over variables \(=\{X_{1},,X_{K}\}\) taking values in \(}=_{k\{1 K\}}_{k}\) subject to a strict partial order \(<_{}\), where_

* \(=\{X_{1},,X_{N}\},N K\) _is the set of endogenous variables._
* \(==\{X_{N+1},,X_{K}\}\) _is the set of exogenous variables._
* \(\) _is the set of deterministic structural equations,_ \(V_{i}:=f_{i}(^{})\)_, where the parents are_ \(^{}\{X_{j}\,|\,X_{j}<_{}V_{i}\}\)_._
* \(\{\{I_{i,v_{i}}\}_{i\{1 N\}}\}_{ }}\) _where_ \(v_{i}\) _is the_ \(i\)_-th element of_ \(\)_,_ \(_{i,v_{i}}\) _indicates an intervention_ \((X_{i}=v_{i})\) _and such that_ \(\)_._ \(\) _is the set of perfect interventions under consideration. A perfect intervention_ \((V_{i}=v_{i})\) _replaces the unintervened_ \(f_{i}\) _by the constant assignment_ \(V_{i}:=v_{i}\)_._
* \(_{}\) _is the probability distribution over_ \(\)_._

As we focus on computational aspects of SCM, we do not regard exogenous variables to be latent, but rather consider them to take values which are not under control of the causal system itself. As such, their values are not determined via any structural equation. By construction of \(\) at most one intervention on any specific variable can be included in any intervention set \(\). The additional constraint enforces that \(\) is closed under subsets, i.e. that any subset of any \(\) is also part of \(\). This condition is placed to yield valid intervention sets when partitioning the SCM. Every \(\) entails a DAG structure \(=(,)\) consisting of vertices \(\) and edges \(\), where a directed edge from \(X_{j}\) to \(X_{i}\) exists if \( x_{0},x_{1}_{j}.f_{i}(^{},x_{0}) f_{ i}(^{},x_{1})\). For every variable \(X_{i}\) we define \((X_{i}),(X_{i})\) and \((X_{i})\) as the set of direct children, direct parents and ancestors respectively, according to \(\).2 Additionally, every \(\) entails an observational distribution \(_{}\)3 by propagating \(_{}\) through the structural equations. Any perfect intervention \(I\) on a variable \(X_{i}\) replaces \(f_{i}\) with a new probability distribution \(_{I}\). As a consequence \(\) entails infinitely many intervened distributions \(_{}^{}\).

**Related Work.** Several works acknowledge the need for model simplification when working with causal models at different levels of modeling detail or finding consistent mappings between two already existing causal models (Rubenstein et al., 2017; Chalupka et al., 2016; Beckers et al., 2020; Zennaro et al., 2023; Brehmer et al., 2022). However, whenever providing explicit methods of mapping SCM, marginalization is considered as a tool of removing variables. Several other works have been dedicated to proving consistency and identifiability results for grouping or clustering variables in general (Anand et al., 2022; Squires et al., 2022). Works on \(\) abstractions by (Beckers and Halpern, 2019; Beckers et al., 2020) focus on simplifying models by mapping between SEM of different levels of abstractions. With regard to computational aspects, Rubenstein et al. (2017)demonstrate the causal consistency of SEM, providing simplifications results for marginalizing SEM. However, their theorems (cf. Sec.5) explicitly exclude interventions on the marginalized variables.

## 3 Consolidation of Causal Graphical Structures

In this section, we present an approach to consolidating structural equation systems under intervention. This is the key contribution of this work compared to previous works that only considered marginalization of unintervened subsystems (Pearl, 2009; Peters et al., 2017; Rubenstein et al., 2017). The focus is on computational aspects of marginalizing intermediate variables while preserving effects of interventions. A formalization of marginalizing intervenable structural equation systems is introduced in this section. Section 4 examines conditions under which consolidation leads to an actual reduction in complexity, followed by two practical examples.

We start with the definition of a _Causal Compositional Variable_ (CCV) that has similar semantics to cluster DAGs (Anand et al., 2022), in that both capture the causal semantics over a set of variables. In contrast to cluster DAGs, CCVs are defined over an SCM \(\) and moreover expose an interface for explicitly applying interventions to the individual variables inside the CCV. We define a CCV with a corresponding function \(\), that takes the exogenous variables \(\) as its input and outputs the values of a subset \(\). Thus we write \(_{}\) to denote the set of computed variables. To be able to condition on interventions, \(\) takes the set of interventions \(\) as it would be applied to the SCM as its second argument.

**Definition 2** (Causal Compositional Variable): _A variable \(X^{}_{}:=_{}(,) ^{|\,|\,4}\) is a causal compositional variable over some subset \(\) of an SCM \(\), if a consolidation function \(_{}:(^{|\,|},)^{| \,|\,\) exists for which \(_{X^{}_{}}=^{}_{}\) for all \(\), where \(^{}_{}\) is the distribution of target variables \(\) in under some intervention set \(\)._

Put in simple terms, \(_{}\) yields the same values for \(\) as would be determined by evaluation of the initial SCM \(\) given any \(_{}\). Naturally, there always exists such a function \(_{}\) for every \(\), which is computing \(\) via evaluation of \(\) itself. However, \(_{}\) is not required to adhere to the computation sequence imposed by the structural causal model \(\). In particular, \(_{}\) is not required to explicitly compute the intermediate values of any \(V_{i}\), which gives way to simplifying internal computations. As such a CCV serves as a possible stand-in for replacing whole SCM by a function of possibly simpler computational complexity:

**Definition 3** (Consolidated SCM): _Given a causal compositional variable \(X^{}_{}:=_{}(,)\) and some base SCM \(\), we call \(_{}=(,_{},_{},_{},_{_{}})\) a consolidated SCM._

The distributions of the consolidated SCM \(_{_{}}\) are not equal to that of the initial SCM \(_{}\), since \(_{}\) only computes a subset \(\) of all endogenous variables. However, for that subset \(\), the initial SCM and consolidated model yield the same \(^{}_{}\) for all \(\).

### Partition of Structural Causal Models

So far, we considered constructing compositional variables from SCM such that they exhibit functional equivalent behaviour and, by doing so, are able to replace base SCM by consolidated SCMs using CCVs. However, compositional variables trade off the'semantic' graph structure of a classical SCM against a computationally simpler (refer to Sec. 4), but 'black box' function. In practice we might, therefore, only want to replace certain parts of an SCM with consolidated functions. To achieve this goal, we formalize a partition of base SCM into multiple _sub SCM_. Multiple other works have considered the existence of joint variable clusters within SCM (Anand et al., 2022; Squires et al., 2022). However, allowing for arbitrary clusters may induce cycles to the model, which would be undesirable. In our work we constrain the clustering by requiring partitions that enforce acyclicity and, therefore, ensure a well defined evaluation order that is consistent with that of the initial SCM.

Endogenous nodes of a base SCM \(\) can be partitioned into \(L\) mutually exclusive exhaustive components \(=\{_{i}():i \{1,,L\}\}\) with \(\,_{i},_{j}:i j_{i}_{j}=\) and \(_{i\{1 L\}}_{i}=\). We also call \(\) the (exhaustive) partition. We can use any cluster \(\) to form a new sub SCM \(_{}\): \(_{}=(,_{},_{},_{},_{_{}})\) where \(_{}=()\), \(_{}\) are the structural equations of \(\), and \(_{_{}}\) is the distribution over \(_{}\) induced by the base SCM. As some intervention \(\) might intervene on variables which are no longer part of \(_{}\), we define a mapping \(_{}:_{}\) which removes those invalid interventions: \(_{}():=\{do(V_{i}=v_{i}):V_{i}_{}\}\). Consequently we define \(_{}:=\{_{}(): \}\). As expected, whenever a set of interventions \(\) does not intervene on any \(V\), \(_{}\) maps it to the empty set. For notational brevity, we assume the implicit application of \(\) on any \(\) whenever we apply interventions to a sub SCM. Figure 2 (left) presents an exemplary construction of sub SCM from a given partition of a base SCM.

Unconstrained partitions may divide SCM in an arbitrary way. To guarantee an evaluation order of the individual sub SCM that is consistent with that of the base SCM we need to ensure that any particular sub SCM can be evaluated in a continuous, self-enclosed manner. That is, no intermediate evaluation of external nodes, \(V\), is required. Figure 2 (center) illustrates a counter-example of a non-complying partition where an intermediate external evaluation to \(G^{}\) is required. To prevent such cases we require the partitions to yield a strict partial ordering under the following definition: the binary relation \(_{1}\,_{}\,_{2} A_{i} _{1},A_{j}_{2}:A_{i}<_{}A_{j}\) holds if at least one variable in \(_{1}\) needs to be evaluated before some other variable in \(_{2}\) according to \(<_{}\) of the base SCM. We call a partition \(\) "according to \(\)" iff \(_{}\) is a strict partial order5 over all \(\).

**Definition 4** (Partitioned SCM): _Given an exhaustive partition \(\), a partitioned SCM \(_{}\) for some base SCM \(\) is defined as \(_{}=(_{i},_{ _{i}},_{_{i}},_{_{i}}, _{_{_{i}}}),i\{1 L\}\) s.t. there exists a strict partial order \(R_{}\) over all \(_{i}\) according to \(\) and every \(_{_{i}}=(_{i},_{_{i}}, _{_{i}},_{_{i}},_{_{_{i}}})\) forms a valid sub SCM._

**Consistency of partitioned SCM evaluation.** To ensure for the consistent evaluation of all sub SCM \(_{}\) within a partitioned SCM \(_{}\) we need to ensure that the evaluation is carried out according to some \(_{}\) that is compliant according to the base SCM \(\).6 Doing so, guarantees that the value of every exogenous variable \(_{i}\) of a sub SCM \(_{_{i}}\) - that is not truly exogenous (\(_{i}_{}\)) - is computed as an endogenous variable \(V_{j}\) inside another \(_{_{i}}\), that is evaluated before \(_{_{i}}\) with \(_{i}:=_{j}\). For example \(G_{2}\) in Fig. 2 (left) computes the values of \(B\) and \(D\), required as exogenous variables by \(G_{3}\). Lastly, during evaluation, all \(_{}\) need to agree on the same set of applied interventions. This is done by fixing a particular \(^{}\) during evaluation and computing the intervention set \(^{}_{}:=_{}(^{})\) specific to every \(_{}\). An algorithm for evaluating partitioned SCM and its proof of consistency are presented in Appendix A.

Figure 2: **Consolidating SCMs.****(Left)** The base graph of an exemplary SCM \(\) gets deconstructed into three sub SCM using the partition set \(=\{\{A\},\{B,D\},\{C,E\}\}\). Exogenous variables are displayed with dashed circles. (**Center)** A subgraph \(^{}\) within a larger base SCM. There exists a directed path that exits and re-enters \(^{}\), thus preventing self-enclosed evaluation of \(^{}\). (**Right)** Consolidation of a sub SCM into a multivariate compositional variable. \(X_{2}\) is an aspect variable chosen by the user, (\(X_{2}\)). \(X_{4}\) and \(X_{5}\) are needed for further computation, thus \(^{}=\{X_{2},X_{4},X_{5}\}\). The value of \(X_{2}\) is computed via \(_{^{}}\) and interventions can be performed via its parameter \(\). The dotted line indicates that \(X_{2}\) is not an ‘independent’ variable. Specifically it is not allowed to intervene on \(X_{2}\) via ‘edge cutting’, making it independent of \(_{^{}}\) (and in consequence causing \(_{^{}}\) to compute inconsistent values for \(X_{6}\) and \(X_{7}\)).

Partial consolidation of SCM.Having defined partitioned SCM allows us to selectively swap out arbitrary sub SCM by their consolidated SCM. In Def. 3 we placed no constraints on \(\) to allow for arbitrary consolidation of variables. For sub SCM \(_{}\) that appear within a partitioned SCM \(_{}\) we need to constrain \(\) to additionally include all variables \(V_{}\) such that evaluation of \(_{}\) additionally computes all variables needed as exogenous by other sub SCMs. Fig. 2 (right) shows an exemplary sub SCM with \(X_{2}\) (green) chosen as a relevant aspect variable by the user, and \(X_{4}\), \(X_{5}\) being required by evaluations of subsequent SCM. Thus \(^{}=\{X_{2},X_{4},X_{5}\}\). Whether to consider \(\) or \(^{}\) depends on the standpoint of the user. From a computational perspective \(^{}\) is important as it holds all variables that need to be computed by \(\). On the other hand, the set \(\) captures aspects of the SCM important to the user i.e., variables of interest. We will therefore refer to sub SCM with \(_{^{}}\) (and in the same breath write \(_{^{}}\)) but use \(_{,}\) (see the following Def. 5) to retain the initial set of variables chosen by the user. Having defined consolidated SCM \(_{}\), partitioned SCM \(_{}\) and the required constraint on \(\) we are now equipped with the tools to define a partially consolidated SCM that yields a consistent \(_{}\) with the base SCM.

**Definition 5** (Partially Consolidated SCM): _A partially consolidated SCM \(_{,}\) is a partitioned SCM \(_{}\) such that a subset of sub SCM \(_{}\) are being replaced by consolidated SCM \(_{^{}}\) where \(^{}_{i}:=\{V_{i}_{}:(V_{i})\,\,(\,_{^{}}=(^{}, ^{},^{},^{},^{ }_{^{}}).V_{i}^{})\}\)._

Algorithm 1 summarizes all considerations of this chapter, starting out from a subset \(\) and partition \(\) up to a (partially) consolidated SCM \(_{,}\). An exemplary step-by-step application of the algorithm can be found in Appendix D.3. The purpose of the argmin operation in Line 8 is to minimize complexity of \(^{}_{^{}_{i}}\) by finding a minimal encoding. We discuss this step in more detail in the following section. After formally introducing consolidation, we are ready to illustrate its applicability.

```
1:procedureConsolidate(\(,,\))
2:for all\(_{i}\) in \(\)do
3:\(_{i}_{i}\)\(\) Filter aspect variables for the current \(_{i}\).
4:\(^{}_{i}_{i}(( _{i})_{i})\)\(\) Add variables that are required by other sub SCM.
5:\(_{_{i}}(_{i}) _{i}\)\(\) Define exogenous variables and interventions.
6:\(_{_{i}}\{_{_{i}}(): \}=\{\{\{(X_{i}=v)\,:X_{i} _{i}\}:\}\)
7:\(_{^{}_{i}}(_{_{i}},) \{_{j}:X_{j}_{i}\}\)\(\) Define a causal compositional variable via \(_{^{}_{i}}\).
8:\(^{}_{^{}_{i}}_{^{}_{ ^{}_{i}}}(^{}_{^{}_{i}})\)\(\) Minimize representation (see Sec. 4). \(^{}_{^{}_{i}}(_{_{i}} )=_{^{}_{i}}(_{_{i}})\)
9:\(_{_{i},}(^{}_{i}, _{_{i}},^{}_{^{}_{i}},_{_{i}},_{_{_{i}}})\)\(\) Define the sub SCM resulting from \(_{i}\) and \(\).
10:endfor
11:\(_{,}(^{}_{i}, _{_{i}},^{}_{^{}_{ i}},_{_{i}},_{_{ _{i}}}),i\{1|\,|\}\)\(\) Merge all \(_{_{i},}\).
12:return\(_{,}\)\(\) Return the consolidated SCM.
13:endprocedure ```

**Algorithm 1** Consolidation of Structural Causal Models

## 4 Compression of Causal Equations

Model consolidation can lead to compression by reducing the model's graph structure and leveraging redundant computations across equations. This may result in smaller, simpler models that are computationally more efficient and easier to analyze. Compressing structural equations to a minimal representation is highly dependent on the equations under consideration and probably incomputable for most problems. As there is ultimately no way of measuring compressibility of SCM by only considering their connecting graph structure, we provide a discussion with regard to some of the

Figure 3: **Consolidate Algorithm. The above pseudo-code summarizes the consolidation algorithm as described in this paper by utilizing causal compositional variables and partitioned SCM to obtain simplified SCM. Depending on the use-case Step 11 might be skipped and the partitioned SCM might be returned instead.**

information-theoretical implications. Specifically, we discuss compression properties for some of the basic structures appearing within SCM; namely chains, forks and colliders. In this section, we, first, analyze how consolidated models may leverage redundant computations for reducing complexity within chained equation in general. Second, we give a condition under which equations, and their interventions can be dropped from the consolidation model altogether. Thirdly, we analyse how interventions within the consolidated model affect our ability to compress equations. Lastly, we will walk through two examples of model compression.

**General compression of equation systems.** Using our formalization of (partially) consolidated SCM, we now have the chance to replace certain parts of an SCM with computationally simpler expressions. The notion of what a'simple' expression may be, varies depending on the application and is subjective to the user. To define a measurable metric, we reside to a simplified notion of complexity by measuring the representation length of our consolidated equations. We assume that all structural equations of an SCM can be expressed in terms of elementary operators, where each term contributes the same amount of complexity. As such, we can apply Kolmogorov complexity [Kolmogorov, 1963]. Then a desirable minimal representation of a structural equation \(f_{i}^{}\) is one that minimizes \((f_{i})\): \(f_{i}^{}:=*{argmin}_{f_{i}^{}}(f_{i}^{ })*{s.t.}f_{i}^{}(*{pa}(X_{i}))=f_{i}( *{pa}(X_{i}))\).

Classical marginalization reduces the number of variables in a graph. To keep the model consistent after marginalization, all children \(:=*{ch}(A)\) of a marginalized variable \(A\) additionally need to incorporate the values of \(*{pa}(A)\) to accommodate for the causal effects that where previously flowing through \(A\) into \(\). This modifies the structural equations of any \(B\), \(f_{B}^{}:=f_{B} f_{A}\), where \(f_{B}\) and \(f_{B}^{}\) are the structural equations of \(B\) before and after marginalization, respectively. Evaluation of the separate equations \(f_{A},f_{B}\) provides an upper bound on the complexity of the composed representation \((f_{B}^{})(f_{A}^{})+(f_ {B}^{})\)[Zvonkin and Levin, 1970]. Since the consolidated system is not required to compute \(A\) explicitly, the encoding length of \(f_{B}^{}\) might resort to directly computing \(B\) from the values of \(*{pa}(A)\). Also, the chain rule for Kolmogorov complexity only considers the case of reproducing \(f_{A}\) and \(f_{B}\) in their initial forms. In addition to that, we might also use semantic rules to reduce equation length, e.g. by collapsing consecutive additions \( a,b. c.a+b=c\) and so on. Whether consolidation actually leads to simplified equations depends strongly on the specific equations and their connecting graph structure. No simplification effects occur in cases of already minimal systems, while strong cancellation occurs in the case of \(f_{B}\), \(f_{A}\) being inverses to each other (see Appendix B.1). Lastly, we want to refer to Appendix B.2, where we showcase the insufficiency of matrix composition to obtain minimal function representations in the case of linear systems.

**Marginalizing child-less variables.** Regardless of the particular causal graph structure, all equations which do not affect \(P_{^{}}\) can be removed from the model to reduce its overall complexity. In particular we point out that \(P_{^{}}\) is invariant to all \(X*{an}(^{})\). By the following deduction we infer that we can always consolidate all child-less variables (if not part of \(^{}\) themselves) from \(\): \( X^{}[(*{ch}(X )=)]( X^{} X *{pa}(X^{}))(X^{} X *{an}(X^{})) X*{an}( ^{})]\). Since child-less variables do not affect \(P_{^{}}\), we can not only consolidate but _marginalize_ them. (Reducing to the same scenario as in Rubenstein et al. [2017, Thm. 9]). Therefore, we are allowed to drop interventions \((X_{i}=c)\) with \(X_{i}*{an}(^{})\) from the set of allowed interventions. This process can be applied repeatedly until we have pruned the SCM from all child-less variables irrelevant to \(^{}\).

### Simplifying Graphical Structures

In contrast to marginalization, consolidation preserves the effects of interventions for consolidated variables. This effectively adds conditional branching to every structural equation \(f_{i}\) if some \(\) with \((V_{i}=c)\) exists:

\[V_{i}:=c&(V_{i}=c)\\ f_{i}(*{pa}(V_{i}))&\] (1)

While conditional branching might prevent us from compressing equations, we consider that not all variables might be affected by interventions. As such, we might be able to utilize local structures within the graph to simplify equations. In the following we briefly discuss the possibilities of simplifying chains, forks and collider structures within the graphs of SCMs:

**Simplifying Chains.** Consolidating chains of consequent variables corresponds to'stacking' structural equations and computing the last non-consolidated variable directly. In the general case,conditional branching complicates the simplification of the stacked equations into a single closed-form representation. When considering the case of marginalization, that is without considering interventions, as done in Rubenstein et al. (2017), composition of equations turns into direct function composition \(X_{i}:=f_{i} f_{i-1} f_{i-2}\). To this end, a complexity bound on chained equations over finite discrete domains is discussed in Appendix B.3, as well as consolidation of the motivating dominoes example in Appendix D.1.

**Simplifying Forks.** Consolidating the parent node \(B\) of a fork structure, \(A B C\), might lead to a duplication of \(f_{B}\) into the equations of both child nodes, \(f^{}_{A}:=f_{A} f_{B}\), \(f^{}_{C}:=f_{C} f_{B}\). If \((B)\), then \(A\) and \(C\) will be confounded by exogenous variables. This is the reason why we did not require independence of exogenous variables in Def. 1. Still, consistency with the initial SCM is guaranteed, since we require all structural equations to be deterministic. As a consequence, every evaluation of the duplicated structural equations \(f_{B}\) inside \(f^{}_{A}\) and \(f^{}_{C}\) yields the same value when given the same inputs. While determinism of structural equations is formally required, we illustrate a consistent reparameterization of non-deterministic models in Appendix C.

**Simplifying Colliders.** Colliders are the most promising graphical structures for simplifying equations. When consolidating \(A\) and \(C\) of a collider \(A B C\), we might leverage mutual information between \(f_{A}\) and \(f_{C}\) to simplify \(f_{B}\). Especially in the case of \((A)=(C)\), consider \(A X C\) for example, we might be able to discard \(f_{A}\) and \(f_{C}\) altogether and compute \(B\) directly from \(X\).

### Time Series Example: Tool Wear

We will now demonstrate a simple application of consolidation for a possibly more applied scenario. Imagine that we want to create a causal model of an industrial unit under continuous use, e.g. a milling machine. At the end of every work day the length \(L\) and sharpness \(S\) of the milling cutter are measured. From these measurements other metrics such as the cutting accuracy \(A\) can be derived. Interventions on the process are performed by grinding the cutter,'resetting' it to a certain sharpness. While every intervention grinds away some material, the weight and size changes are negligible for the considered aspect of accuracy. Throughout our recordings we might encounter multiple such interventions. From the data we fit a 'classical' SCM that models the time series on a day-to-day basis, \(_{t-1}_{t}\). We observe the tool to loose some percentage of its sharpness per day depending on its utilization \(U_{t}\). The intervention \((S_{t}=1)\) resets the sharpness to a constant value, while \((S_{t}=1,L_{t}=1)\) models a tool replacement. As by Def. 1, \(\) needs to include \((L_{t}=1)\), which might be a recalibration of the machine. Figure 4 (left) shows the initial causal graph of the time series model as defined by the following SCM:

\[=&=\{_{t}=(0.5,0.05^{2 })\}\\ &=\{_{t},_{t},_{t}\}\\ &=(\{(_{t}=1),(_{t}=1.0),(_{t}=1,_{t}=1)\})\\ &=f_{l}(l,u)&:=(1.0-0.002u)l\\ f_{_{t}}(s_{t-1},u)&:=(1.0-0.3u)s_{t-1}\\ f_{a}(s)&:=0.8s^{2}\]

Figure 4: **Consolidating a real world mechanism. (Left)** The causal time-series model of a milling machine, representing tool length \(L\), utilization \(U\), sharpness \(S\) and accuracy \(A\). (Center-Left) Removing child-less nodes \(L_{t}\) and \(A_{t}\) and controlling for the parents \(U_{t}\) yields a simplified causal structure. (Center-Right) Plots for the consolidated structural equation of \(S\). Colored areas show the effects of varying \(\) by one and two sigma (\( 0.05, 0.1\)) respectively. Dashed grey lines indicate interventions, which are respected truthfully by the consolidated function. (Right) Marginalization, likewise, simplifies the model, but does not allow us to investigate the effects of interventions.

Now, we might be interested in extracting a formula for the total tool sharpness \(S_{t}\) at an arbitrary point in time \(t\). Thus, our consolidation set consists of all \(S_{t}\), \(=\{S_{t}\}\). Since \(U_{t}\) is exogenous, we make the additional assumption that the utilization follows a normal distribution and we simplify to the expected value \(=0.5\) (Figure 4, center-left). As laid out before, we can marginalize all child-less variables \(L_{t}\) and \(A_{t}\) not part of \(^{}\). As all \(L_{t}\) are no longer part of \(_{^{}}\), \(_{^{}}\) maps interventions \((S_{t}=s_{t},L_{t}=l_{t})(S_{t}=s_{t})\). Considering the unintervened case, structural equation are now simplified via function composition, \(f_{_{t}}:=f_{_{t}} f_{_{1}}\), which results in the following equation \(f_{_{t}}:=(1-0.3 0.5)^{t}=0.85^{t}\). According to Eq. (1), we now have to inspect interventions as potential candidates for conditional branching. All remaining interventions are of the form \((S_{t}=1.0)\). Applying an intervention at time \(t_{0}\) equals shifting the following equations by the time of that last intervention \(t^{}:=t-t_{0}\). Finally, we arrive at the following consolidated equation:

\[f_{S}(t) :=0.85^{t-t_{0}}\] \[ t_{0} =_{i}\{i(S_{i}) i  t\}\]

Fig. 4 (center-right) shows the resulting plot of the consolidated model under interventions \((S_{12}=1)\) and \((S_{24}=1)\). We successfully demonstrated the power of consolidation models for dynamical system while preserving the ability to intervene. In theory more complex dynamical systems could be consolidated. However, as these kind of self-referential models require a more involved discussion, we kindly refer the reader to Bongers et al. (2018, 2021); Peters et al. (2022) for further considerations.

### Revealing Agent Policy

In our second example we apply consolidation to a more complex causal graph relating the game state of a simple platformer environment to the actions of an agent. See Appendix D.4 for the full causal graph and structural equations. Throughout the level the agent ((1) in Fig. 5) can interact with a coin ((2) in Fig.), a power-up (3), an enemy (4) and the finish flag (5) to accumulate a certain reward (6) by doing so. The power-up is required to interact with the enemy. During play, the agent takes the state of the environment as its input and outputs the state of 'towards_coin', 'towards_powerup', etc. The order of the agent actions is then recorded via four 'planning_sequence_\(i\)' for \(i\{1 4\}\) variables. A causal graph, like the one presented in Appendix D.4, might be extracted automatically from observational data, or designed by an expert. Due to the sheer number of variables and edges, dependencies in the obtained SCM are hard to trace. To get a better understanding, we use consolidation to reveal the policy of our agent. We consolidate all endogenous variables except player

Figure 5: **Complex situations can be easy to understand using consolidation.** Encoding the behaviour of agents acting in game environments **(left)** often results in complex causal graphs **(right**; indeed unreadable due to complexity. A readable version is contained in the Appendix). Even very simple levels with a single agent, coin, power-up and enemy, entail causal graphs that are intuitively non-interpretable. Especially the intertwining of game mechanics and agent behaviour complicates the inference of the agents’ actual policy and makes it impossible to judge its performance. In our example a suboptimal greedy policy is embedded within the causal graph, which can be made visible using consolidation **(bottom**, Eq. (2))**. Please note that ps abbreviates ‘planning_sequence’. In contrast to marginalization (Eq. (3)), one can still intervene on the consolidated system.

entity 'distance' and the 'planning_sequence' variables. To be able to modify the agents behaviour, we allow interventions by forbidding the agent to target certain entities: \(=(\{(=0),( =0),(=0)\})\).

Like before, we consolidate equations considering the unintervened case, and then add back in conditional branching for interventions to yield equations (2) in Figure 5. Contrary to the very complex structure of the SCM, the consolidated equation reveals the actually very _simple_ policy of the agent. We find from the consolidated equation that the agent only collects the coin, if not intervened upon, and then heads directly towards the flag. This insight might not be obvious from the initial SCM and, at least, is difficult to spot a priori by looking at the unconsolidated equations. When inspecting the original SCM more closely we find, that a constant factor is added to the calculation of 'targeting_cost_powerup.' This factor might serve to accommodate for the time lost when speeding up or slowing down towards a target. Furthermore, we see that the agent pursues a greedy policy, thus, never considering the overall higher reward of the power-up and enemy together. Instead the policy ignores the power-up, due to its low reward and in consequence also never targets the enemy. This behaviour not only leads to strong simplification of the SCM, but also allows us to discard the imperfect policy, without the need to run possibly costly trials, just to come to the same conclusion.

To summarize, consolidation is a strictly more powerful operation than marginalization. More examples and domains can be found in Appendix D.

## 5 Conclusions

Consolidation is a powerful tool for transforming SCMs, while preserving the causal aspect of interventions. In addition, consolidating SCMs can lead to more general models. For example, recall the tool wear example of Sec. 4.2. While the initial causal graph operates on discrete time steps our consolidated function provides a continuous relaxation of the causal process. We can evaluate it at any point in time \(t\) and are no longer dependent on the day-to-day basis which was modeled by the initial graph. Additionally, the consolidated equation of our motivating example of rows of dominoes (compare Appendix D.1), yields a generalized formula that is independent of the actual number of dominoes that compose the row, by making use of first-order quantifiers. In our discussion of Sec. 4 we saw that we can further benefit from consolidation in all cases where the initial SCM does not already represent the smallest possible causal model. Lastly, these simplifications align with our goal of making SCMs more interpretable. Consider that the initial domino SCM only provides a 'local' view on the system, by only providing equations for every individual stone, "If stone A falls it pushes over stone B, except in the case of an intervention. If stone B falls,..." and so on. The equation of the consolidated SCM can be directly translated into a single natural language sentence, e.g. "The last domino will fall, if the first domino is pushed over, except in the case of holding onto or pushing over a stone along the way", capturing the causal mechanisms of the system much more intuitively. Our last example of Sec. 4.3 strikingly revealed the sub-optimal, greedy agent behaviour in a game setting. While we illustrated examples that are well suited for consolidation, we are positively inclined to expect consolidation to be helpful towards a broad range of applications.

**Limitations and Broader Impact.** Throughout the paper we considered exact consolidations, in that Def. 2 requires strict equality between \(_{}(,)\) and \(^{}_{}\). This assumption might be met in logic and idealized scenarios, but may hinder consolidation of SCM in other applications due to noise inherent to the system. The definition might be relaxed by allowing for small deviations of \(_{}\) from the distribution of the unconsolidated SCM. Thus, relaxing the strict equality \(_{X^{}_{}}=^{}_{}\) with \(|\,_{X^{}_{}}-^{}_{}|<\) for some small \(>0\), provides a relaxed consolidation constraint for noisy systems. SCM constitute a well suited framework for representing causal knowledge in the form of graphical models. The ability to trace effects through structural equations that yield explanations about the role of variables within causal models is required to make results accessible to non-experts. Actual causation, and only recently, causal abstractions and constrained causal models have come to attention in the field of causality (Halpern, 2016; Zennaro et al., 2023; Beckers and Halpern, 2019; Blom et al., 2020) and might be beneficial for future works on consolidation. Apart from computational advantages, consolidation of SCMs presents itself as a method that enables researchers to break down complex structures and present aspects of causal systems in a broadly accessible manner. Without such tools, SCMs run the danger of being only useful to specialized experts.