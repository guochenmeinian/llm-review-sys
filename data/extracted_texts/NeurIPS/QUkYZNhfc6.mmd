# FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models

FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models

 Hao Zhang

HKUST

hzhangcc@connect.ust.hk

Yanbo Xu

CMU, HKUST

yxubu@connect.ust.hk

Tianyuan Dai

Stanford University, HKUST

tdaiaa@connect.ust.hk

Yu-Wing Tai

Dartmouth College

yuwing@gmail.com

Chi-Keung Tang

HKUST

cktang@cs.ust.hk

These authors contributed equally to this work.

###### Abstract

The ability to create high-quality 3D faces from a single image has become increasingly important with wide applications in video conferencing, AR/VR, and advanced video editing in movie industries. In this paper, we propose Face Diffusion NeRF (FaceDNeRF), a new generative method to reconstruct high-quality Face NeRFs from single images, complete with semantic editing and relighting capabilities. FaceDNeRF utilizes high-resolution 3D GAN inversion and expertly trained 2D latent-diffusion model, allowing users to manipulate and construct Face NeRFs in zero-shot learning without the need for explicit 3D data. With carefully designed illumination and identity preserving loss, as well as multi-modal pre-training, FaceDNeRF offers users unparalleled control over the editing process enabling them to create and edit face NeRFs using just single-view images, text prompts, and explicit target lighting. The advanced features of FaceDNeRF have been designed to produce more impressive results than existing 2D editing approaches that rely on 2D segmentation maps for editable attributes. Experiments show that our FaceDNeRF achieves exceptionally realistic results and unprecedented flexibility in editing compared with state-of-the-art 3D face reconstruction and editing methods. Our code will be available at https://github.com/BillyXYB/FaceDNeRF.

## 1 Introduction

Rich and versatile 3D contents are in high demand in entertainment industries such as movie making, computer gaming and emerging applications such as Metaverse, which also have high potential in robotics learning as well, where high-quality synthetic data in large amounts are required to enhance generalizability. 3D generative methods such as EG3D  can generate high-fidelity NeRF from a single image, but the reconstructed NeRF cannot be easily controlled or edited. Some methods  use pixel-wise segmentation maps or user-supplied sketches to guide 3D editing. But these methods are hard to scale up due to the demanding editing requirement and limited editable attributes. Language is regarded as one of the most suitable candidates to provide control signals for 3D editing, especially given the current success of 2D semantics-driven editing , where image and language domains are bridged by CLIP, i.e., Contrastive Language-Image Pre-training . However, there is still substantial room for improvement in 3D generative and editable models, in terms of usability, edit-ability, and results quality.

Neural Radiance Field (NeRF), a new paradigm in 3D scene representation, embeds a 3D scene in a compact fully-connected neural network and achieves realistic rendering of novel views. NeRF is optimized to approximate a continuous and thus differentiable scene representation function, which has quickly become a dominant approach for many relevant downstream tasks including novel scene composition , surface reconstruction , and articulated 3D shape reconstruction  due to its differentiability, multi-view consistency, compactness, and fast inference. Since NeRF relies on training from multi-view images, its relighting is contingent upon the relighting of these 2D images. Nevertheless, current 2D relighting techniques  are unable to ensure view consistency among the relit multi-view images, resulting in a decline in the quality of the relit NeRF.

Significant attempts have been made in recent years to develop semantic-driven NeRF editing models. Pre-trained 2D language-image models are usually leveraged to enable multi-modality in 3D. CLIP-NeRF  introduces a disentangled conditional NeRF architecture, where the disentangled latent representations can be bridged by two mappers trained with a CLIP-based matching loss to the CLIP embedding, based on which the latent codes are updated for editing based on the input text prompt. Outperforming CLIP-based methods, DreamFusion  later adopts Imagen , a pre-trained text-to-image diffusion model, as a prior to guide the optimization of parameters of a randomly initialized NeRF through a novel SDS loss. Magic3D  further improves DreamFusion by introducing a coarse-to-fine optimization scheme with diffusion priors at different resolutions. However, these methods have two main limitations: lack of photorealism and long inference time (reportedly taking 1.5 hours for DreamFusion, and 40 minutes for Magic3D).

We believe the main reason underlying the less realistic results and slow inference of DreamFusion  and its variants lies in the random and thus unrestricted initialization of NeRF. Leveraging 3D generative models such as EG3D , whose generation is restricted by the discriminator during training for constrained NeRF generation, may offer a feasible solution. Although combining 2D generative models with CLIP  for realistic image generation and editing has been a widely adopted approach , no significant attempts have been made in 3D realistic context to our knowledge. Thus, in this paper, we present FaceDNeRF, which is to our knowledge the first work to enable semantic-driven NeRF editing in 3D with high photorealism and versatile relighting from a single image, given a text prompt and target light. FaceDNeRF freezes the weights of the EG3D network which can reconstruct photorealistic NeRF from a single image. To enable semantic-driven editing, we adopt stable diffusion  to guide the optimization through SDS loss introduced in DreamFusion . Based on SDS loss, identity loss, and feature loss, the latent vector in EG3D's latent space can be updated. Moreover, the proposed illumination loss allows explicit control over the lighting in a view-consistent manner.

Figure 1: **FaceDNeRF results. Given a face image, FaceDNeRF can reconstruct, edit, and relit a photo-realistic 3D face NeRF using a text prompt and a target light. Our method is not limited to human faces, and can be applied to other domains as well. Further results and videos are available in the supplemental materials.**

Related Work

**2D Generation and Editing**  Research on unconditional or text-driven image generation has made fruitful progress. Generative Adversarial Networks (GANs)  contribute to the first revolutionary 2D image generative methods, among which StyleGAN  and its variants [24; 27] stand out due to the expressive and well-disentangled latent spaces. StyleGAN-based image editing requires either an encoder trained to map a given image to the latent space [2; 39], or specifying latent update direction which requires explicit ground-truth annotations [1; 19; 59; 65]. Diffusion models [54; 56] represent another class of generative models that enables text-driven, photorealistic and highly diverse image generation. State-of-the-art text-to-image synthesis methods contribute effective mechanisms to guide samples toward semantics: classifier-free guidance [20; 37] generates images with or without class information during model training; CLIP guidance [13; 42; 69] where CLIP  trained on 400 million image-text pairs spearheaded cross-modal representation learning in modern vision-language tasks. Leveraging the rich joint embedding spaces of CLIP and expressiveness of diffusion models, stable diffusion  is arguably the best text-to-image model to date, synthesizing images of vast domains and styles based on a text prompt. Thus we adopt stable diffusion as the guidance model in our 3D approach.

**3D Generation and Editing**  NeRF , an implicit neural representation, has become the dominating modern approach for 3D generation due to its continuity, differentiability, compactness, and quality of novel-view synthesis over mesh and point cloud. GRAF  combines implicit neural rendering with GAN for generalizable NeRF. PiGAN  utilizes SiREN  to condition the implicit neural radiance field on the latent space. Although guaranteed with 3D consistency, volumetric rendering requires heavy computation. With limited computation, the image quality of these methods is still not comparable to those produced by current state-of-the-art 2D GANs. Thus, many recent approaches adopt hybrid structures. StyleNeRF  applies volume rendering in the early feature maps in low resolution, followed by upsampling blocks to generate high-resolution images. However, a regularizer based on NeRF is required to ensure 3D consistency during upsampling. Instead of using volume rendering in early layers, EG3D  performs the operation on a relatively high-resolution feature map using a hybrid representation for 3D features generated by StyleGAN2  backbone, named tri-plane, which is capable of incorporating more information than an explicit structure such as voxel. StyleSDF  shares a similar spirit but uses SiREN  for its mapping network, with the mapped result used as the input feature map followed by a style-based generator for upsampling.

Attempts have been made to generate 3D objects using diffusion models. Rodin , Realfusion , Set-the-Scene  and other diffusion-based 3D reconstruction methods have demonstrated the feasibility of constructing a face or other object from a single view image. However, the results are not adequately realistic with limited generalizability due to the scarcity of 3D data. It is also possible to approximate 3D assets using trained 2D diffusion models, as shown in [44; 32]. However, the quality of generated assets is visually worse than the outputs from the utilized 2D models, especially in areas such as the face, where high fidelity is an essential criterion.

**Illumination Control** 3D editable lighting on NeRF is a highly desirable feature. Image relighting methods can be roughly categorized into two groups: one involves estimating 3D face information such as 3DMM coefficients , albedo, and surface normals, and combining this information with a target lighting condition represented by spherical harmonics (SH) coefficients to generate relit images, such as [22; 34; 15]. Although guaranteed with 3D consistency, these methods have limited editability as they cannot easily accommodate changes in the original model, such as wearing glasses or different hairstyles. The other approach involves leveraging 2D/3D generative models to control illumination in the latent space, such as [4; 30]. While this approach is conducive to some editability, illumination can only be implicitly controlled by latent codes, that is, environmental lighting cannot be directly controlled by e.g., SH coefficients or cube mapping. Our method is amenable to both light editing means, bypassing any intrinsic separation which is not necessary in FaceDNeRF.

## 3 Method

### EG3D and \(^{+}\) Space

Our method utilizes trained EG3D generator  with its respective latent space. From initial latent code \(z^{512}\), a mapping network \(\) maps \(z\) to \(w\) in the space named \(\), where \(w^{1 512}\)During training and generation, the \(w\) code will be utilized to modulate all convolution layers as in StyleGAN2 . The generated features will be reshaped into three orthogonal feature plans \((F_{xy},F_{yz},F_{xz})\), where each of them has the resolution of \(N N C\). Given a camera pose, an augmented feature can be rendered using volume rendering as in , which will then be up-sampled by the super-resolution blocks. The rendering process acts as an inductive bias that enforces view-consistent results.

The inversion process inverses an input image to the latent space, such that the inverted code \(w^{}\) can faithfully reconstruct the given input. As shown in 2D GAN inversion , the quality of reconstruction is better when the inversion is conducted on \(^{+}\) space, where its latent codes \(w^{+}^{L 512}\) are used to separately modulate all \(L\) convolution blocks. In addition, the \(w^{+}\) space preserves the editability [58; 64]. Thus, the reconstruction and editing process of our method operates on the \(^{+}\) space.

### Formulation

Fig. 2 summarizes our method, which takes a single image \(x\), text prompt \(y\), and target lighting represented by SH coefficients \(l\) as inputs, and generates a reconstructed 3D face NeRF as output. Inspired by high-fidelity face NeRF reconstruction techniques, such as EG3D inversion . The Reconstruction and Identity Preservation (Sec. 3.3) is employed to guide the facial NeRF reconstruction process. To enable language-guided editing, we incorporate a Diffusion component (Sec. 3.4), which is based on the trained text-conditioned latent diffusion model. Notably, our method allows for explicit illumination control, thanks to the inclusion of the Illumination component (Sec. 3.5). To generate the target latent code, we randomly sample a value near the mean value of the latent space as a starting point. Then we solve an optimization problem to obtain the desired output.

\(_{R}\), \(_{ID}\), \(_{D}\) and \(_{IL}\) denote the Reconstruction loss, Identity Loss, Diffusion Loss and Illumination Loss, which will be elaborated in Sec. 3.3, Sec. 3.4 and Sec. 3.5. We solve the following optimization:

\[*{arg\,min}_{w+}_{ID}_{ID}(G(w,c ),x)+_{R}_{R}(G(w,c),x)+_{D}_{D}(G(w,c_{s }),y)+_{IL}_{IL}(G(w,c_{s}),l_{c_{s}}).\] (1)

Figure 2: **FaceDNeRF structure**. The Latent 512 scalars are initialized as the mean of sampled \(^{+}\) codes. The model computes the Reconstruction Loss and Identity Loss from the input image’s camera view. From other side views, given a text prompt, the model computes the Diffusion Loss by assessing the discrepancy between the predicted noise and random noise. The Illumination Loss is then computed by comparing the estimated SH coefficients of the rendered side-view image to the target SH coefficients. These Losses are then utilized to update the Latent 512 scalars iteratively through a carefully designed differentiable model via back-propagation.

where the \(G(,)\) is the EG3D generator, \(c\) is the camera pose of the input image (which can be estimated by ), \(c_{s}\) is the random side camera pose. \(l_{c_{s}}\) is the target illumination which varies with \(c_{s}\). \(_{ID}\), \(_{R}\), \(_{D}\) and \(_{IL}\) are weights of the corresponding loss functions. We optimize the \(w\) iteratively through gradient descent by back-propagating the gradient of the objective Eq. (1) through these four weighted differentiable loss functions.

### Reconstruction and Identity Preservation

The desired editing should preserve the background and identity of the input image, and hence we design the Reconstruction Loss and Identity Loss. Given input image \(x\) and rendered image \(G(w,c)\), we utilize VGG16 image encoder \(V()\) to extract the image features and construct the Reconstruction Loss as following:

\[_{R}(G(w,c),x)=||V(G(w,c))-V(x)||_{2}^{2}\] (2)

For human faces, we adopt the same Identity Loss as :

\[_{ID}(G(w,c),x)=1- R(x),R(G(w,c))\] (3)

where \(R\) is the pretrained ArcFace  network. VGG16 image encoder and ArcFace \(_{R}\) and \(_{ID}\) are only used to measure the difference between the input image and the rendered image from input camera's view to avoid the misalignment due to mismatched viewing directions. See our ablation study for an insightful analysis of the interaction between the Reconstruction and Identity losses.

### Prompt Editing with Diffusion Model

Although directly optimizing a NeRF by distilling a 2D diffusion model is possible [44; 32], the quality of the resulting model is often worse than the used 2D model, especially in the domain of human subject (refer to Appendix C.2). In addition, compared with 3D diffusion models, 3D GANs currently are capable of producing high-fidelity 3D NeRFs with a smooth latent space. Therefore, we perform Score Distillation in the latent space of trained 3D GAN model.

We modify the Score Distillation Sampling (SDS) from DreamFusion  as our diffusion loss. The loss connects the \(x^{}\) rendered from a sampled camera pose \(c_{s}\) with the denoising prediction conditioned on the editing prompt \(y\). The denoising process ca n be written as:

\[_{D}(x^{}=G(w,c_{s}),y)=_{(x^{}), y,t,}[}(};y,t)- _{2}^{2}]\] (4)

where \(()\) is the image encoder that encodes our rendered image \(x^{}=G(w,c_{s})\) to the latent space of the diffusion model, denoted as \(z\). Here \(z_{t}\) is the noisy version of \(z\) at time-step \(t\), \(}\) is the frozen denoising network of the trained diffusion model, where we sample \(t(0.02,0.98)\) to avoid very high and low noise levels; \((0,I)\), and its effect on input latent varies with time-step \(t\), which is same as done in Dreamfusion .

Notably, the camera pose \(c_{s}\) will be resampled randomly in each optimization iteration to ensure 3D view consistency. Unlike DreamFusion, our text prompt \(y\) is view independent, since we have the identity and reconstruction losses to constrain the optimization process, and that the EG3D generator has underlying 3D information in contrast to randomly initialized NeRF. Therefore, our diffusion loss back-propagates to the latent scalars, where the gradient of \(_{D}\) is given by

\[_{w}_{D}(x^{}=G(w,c_{s}),y) _{,t,}[w(t)(}(};y,t)-)}{ w}]\] (5)

As in Dreamfusion , \(w(t)\) absorbs the coefficient of the forward process, and we ignore the term \(}(};y,t)}{}}\) as the diffusion model is frozen.

### Explicit Illumination Control

As the trained 3D GAN mapped a continuous latent code to a 3D-consistent NeRF, direct manipulation of the latent ensures view-consistent illumination editing.We utilize  to construct our Illumination Loss, where the hourglass network can be denoted as:

\[_{s}^{*},_{t}^{*}=(_{t}, _{s})\] (6)where \(_{t}\) and \(_{s}^{*}\) are respectively the target SH lighting and the estimated SH lighting of the input image \(_{s}\), and \(_{t}^{*}\) is the relit image under the target SH lighting. Here, we only use \(_{s}^{*}\) to compute the \(L_{1}\) loss with \(_{t}\), and \(_{t}^{*}\) will be ignored, i.e., \(_{s}^{*}=^{}(_{s})\). Thus, our Illumination Loss is:

\[_{IL}(G(w,c_{s}),l_{c_{s}})=^{}( G(w,c_{s}))-l_{c_{s}}_{1}\] (7)

During the optimization process of \(w^{*}\), differentiability is crucial. Given \(G(w,c_{s})\) is differentiable with respect to \(w\), the differentiability of \(^{}(G(w,c_{s}))\) with respect to \(G(w,c_{s})\) guarantees the back-propagation onto the \(w\). Therefore, we replace the PyTorch in-place operations and other numpy operations in the \(^{}\) model with equivalent differentiable PyTorch tensor operations. As verified by experiments, our modified \(^{}\) achieves the same performance as the original model while being multi-view consistent as we render the output.

### Pivotal Tuning Inversion

The Pivotal Tuning Inversion (PTI) technique  is widely employed in many GAN inversion processes to overcome the trade-off between distortion and editability . After solving the Eq. (1), the obtained latent code \(w_{t}\) is utilized as a pivot to fine-tune the generator \(G\) using the similar loss function:

\[*{arg\,min}_{G}\ _{ID}_{ID}(G(w_{t},c),x)+ _{R}_{R}(G(w_{t},c),x)+_{D}_{D}(G(w_{t},c _{s}),y)+_{IL}_{IL}(G(w_{t},c_{s}),l_{c_{s}}).\] (8)

## 4 Experiments

This section presents our editing results and compare them with representative methods, emphasizing FaceDNeRF's disentanglement capability which is conducive to editing control, its flexibility in utilizing a text-guided diffusion model, as well as the multi-view consistency in illumination control. Furthermore, our method can be migrated to other data domains with trained generative models.

### 3D Editing from Single Image

Although diffusion models can generate diverse and realistic images in 2D, the lack of large and high-quality 3D datasets limits the performance of current diffusion models. On the other hand, the adversarial learning mechanism together with inductive bias of 3D rendering enables GAN to produce high-quality 3D results. From GAN's smooth latent space, our method finds the suitable latent code whose semantic information matches the editing demand. Fig. 1 and Fig. 3 demonstrate the detailed editing results of FaceDNeRF. Please refer to Sec. B of the appendix for more results.

Figure 3: **More FaceDNeRF results.** The middle column shows the input images, while the left and right halves of the images in the other two columns show the results of text prompts editing on the left and right, respectively. The first and last columns show the corresponding geometries.

### Comparison

**Comparison with representative editing methods** For effective and easy editing, the underlying latent space should be smooth and semantically meaningful. Compared with the latent space of 2D GANs, disentangling in 3D is much harder. InterfaceGAN  seeks a hyper-plane in the latent space with pre-trained classifiers, which makes editing feasible by interpolating latent codes in the direction orthogonal to the hyper-plane. However, the assumption that a good hyper-plane exists which is well-behaved for linear interpolation is only valid when the latent space is highly disentangled. As shown in Fig. 4, the underlying latent space in 3D is not well disentangled. Thus the interpolation (induced by editing) can interfere and affect other irrelevant attributes not to be edited. We also compare with the semantic-based method FENeRF , whose editing is performed by manually editing the semantic map. However, this representation limits editable attributes, especially on semantically complex attributes such as age and gender. In addition, we compare with language-guided StyleClip  using the same latent space. Our method achieves better editing quality conditioned on the same text prompt, indicating the superiority of utilizing the diffusion model as guidance. Please refer to

Figure 4: **Editing Comparison with representative methods**. We compare with classifier-based InterfaceGAN , semantic edited FENeRF  and language guided StyleClip  with EG3D. Images on the left side are editing results of gender, smile, and “An Eastern face” respectively. The right images are age editing comparison with , and our input text prompt are “A _XX_ is about _YY_ years old”, where _XX_ and _YY_ are depicted above. The comparison illustrates the flexibility and high-fidelity editing capability of FaceDNeRF.

Figure 5: **Illumination Comparison**. The images in the first row are relit images generated by DPR . The images in the second row are relit images generated by our method. Upon comparing the four red boxes, our method is observed to produce fewer artifacts and more realistic results. Furthermore, the images in the third row demonstrate that our method can not only edit face NeRF using text prompts but also allows explicit control of lighting.

Appendix Sec. C for more qualitative and quantitative comparisons, and insights into why SDS guidance outperforms CLIP guidance.

Illumination ComparisonThere have been attempts in 2D to control illumination, either implicitly or explicitly. To produce results compatible in 3D, a straightforward approach is rendering multi-view images from a given NeRF, followed by using the projected lighting direction for each view to render the edited NeRF. However, as shown in Fig. 5, without the constraint of 3D rendering, inconsistent illumination is easily observed, indicating imperfect alignment using 2D methods. With explicit 3D control, FaceDNeRF directly manipulates latent code in the disentangled space, capable of rendering consistent and realistic lighting results.

Migration to Other Data DomainFaceDNeRF is not limited to human faces: the semantically diverse and rich text-guidance diffusion model can be used to directly edit other data domains. As shown in Fig. 1, we perform editing on GANs trained on Cats and Cars, noting the reconstruction loss is also universal to all data domains.

### Text-conditioned 3D Generation

In addition to editing, FaceDNeRF can generate high-quality 3D models given a text prompt. Similar to Dreamfusion , we guide the generation process under the iterative supervision of a trained diffusion model. However, their unconstrained optimization with random NeRF initialization lacks the ability to generate realistic results. Fig. 6 shows some generated high-quality examples by our methods, including examples from other data domains.

Latent RegularizationAlthough the latent space is smooth, generation from rare-sampled latent codes may produce unrealistic results. Latent samples around mean latent code tends to give better outputs (truncation trick in StyelGAN2 ). In text-conditioned 3D generation, the optimized latent code is more likely to deviate from the latent mean since there is no reconstruction or identity restriction. Thus we add a latent regularization to encourage results around the mean, formulated as:

\[_{}(w,)=_{}\|w-\|_{2} ^{2}\] (9)

where \(\) is the sample mean of latent space. This regularization is equivalent to constraining the feasible space around the mean latent code (Lagrange Multiplier).

Figure 6: **Text-driven 3D Generation**. These images are generated and conditioned solely on text prompts. We utilize the trained EG3D generators  from different data domains.

### Generalizability Across Different Backbones & Data Domain

In addition to utilizing EG3D  as our backbone, the optimization process of FaceDNeRF can be effectively applied to alternative backbones.

**Face Domain Generalization** In this instance, we exhibit the implementation of PanoHead  as the chosen backbone. Given that PanoHead operates as a generative model for human facial representations, akin to the EG3D, it allows for the smooth integration of FaceDNeRF. The outcomes are illustrated in Fig. 7.

**Cross-Domain Generalization** Here we demonstrate the extension of our method to the human body domain, which is challenging due to the articulation of human bodies. We employ EVA3D , a cutting-edge human body NeRF generator for optimization. Notable artifacts are still present in state-of-the-art methods like EVA3D. As the latent variable deviates from the mean value, the generated result becomes blurry, and the face becomes unrecognizable. Fig. 14 in the appendix

Figure 8: Semantics-driven editing on human body by replacing EG3D with EVA3D, a state-of-the-art human body NeRF generator. Each row shows 2 examples with the middle image as input.

Figure 7: Results on replacing the EG3D backbone with PanoHead backbone. Top-left sub-image in the respective three sub-figures is the input image, with the input text prompt below each subfigure.

displays generation outcomes by random sampling in the latent space near its mean value, revealing a noticeable lack of 3D consistency and erroneous geometric estimation even near the mean value. Fig. 8 illustrates the editing results. Due to EVA3D's limited generation capacity, we conduct editing on images it generated from randomly sampled latent variables close to the mean value of the latent space. Specifically, we randomly sample two values in the latent space, averaging them to obtain a latent variable value. During each iteration, we calculate the feature loss using the input image and a rendering of the current human body NeRF from the same viewpoint. The diffusion loss and optimization pipeline remain identical to our EG3D version. As depicted in Fig. 8, despite EVA3D's suboptimal generation quality, our editing remains flexible and reasonable. High-quality NeRF generators, inversion methods, and advanced feature or identity preservation techniques are potential future works that could enhance the generalization of our pipeline to other domains.

### Ablation Study

To assess the impact of the weights of various losses on the generated NeRF, we perform ablation studies on the weights of Identity Loss \(_{ID}\), Reconstruction Loss \(_{R}\), and Diffusion Loss \(_{D}\). The frontal-view renderings of the NeRF results from these studies can be seen in Fig. 9. For a comprehensive analysis, please refer to Sec. D.1 in the appendix. Moreover, we investigated the impact of the editing prompt on editing outcomes. For results and a detailed discussion, please see Sec. D.2 in the appendix.

## 5 Conclusion and Discussion

We propose FaceDNeRF, a new generative method to reconstruct high-quality Face NeRFs from a single image, with semantic prompt editing and relighting capitalizing on recent stable diffusion contributions. We believe our optimization pipeline starts a new approach in semantics-driven editing and relighting given any NeRF GAN, and thus has a good impact and can spawn worthwhile future work in the years to come. Extensive experiments validate our significant improvement over state-of-the-art 3D face reconstruction and editing methods. The proposed FaceDNeRF is readily applicable to many real-world applications such as 3D face manipulation, which, however, might be used unethically. Also, the upper bound of our performance is limited by the chosen GANs or diffusion models. We leverage EG3D , which is trained on real-world datasets, thus our generation results are realistic but confined to real-world faces or objects, FaceDNeRF can go beyond faces and can be extended to a generic NeRF generation and editing template, where different 3D generators can replace EG3D  to produce NeRFs in various domains with ease.

Figure 9: **Ablation studies on \(_{ID}\), \(_{R}\), and \(_{D}\). Input text prompt is “A bald man with a thick mustache”. The central images in each row are identical with the setting (\(_{ID}\), \(_{R}\), \(_{D}\)) = (0.5, 0.4, 3\( 10^{-5}\)). Each row shows the impact of changing one of the weights associated with a particular loss term.**