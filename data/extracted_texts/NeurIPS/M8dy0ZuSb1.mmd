# Improving robustness to corruptions with multiplicative weight perturbations

Trung Trinh\({}^{1}\)   Markus Heinonen\({}^{1}\)   Luigi Acerbi\({}^{2}\)   Samuel Kaski\({}^{1,3}\)

\({}^{1}\)Department of Computer Science, Aalto University, Finland

\({}^{2}\)Department of Computer Science, University of Helsinki, Finland

\({}^{3}\)Department of Computer Science, University of Manchester, United Kingdom

{trung.trinh, markus.o.heinonen, samuel.kaski}@aalto.fi,

luigi.acerbi@helsinki.fi

###### Abstract

Deep neural networks (DNNs) excel on clean images but struggle with corrupted ones. Incorporating specific corruptions into the data augmentation pipeline can improve robustness to those corruptions but may harm performance on clean images and other types of distortion. In this paper, we introduce an alternative approach that improves the robustness of DNNs to a wide range of corruptions without compromising accuracy on clean images. We first demonstrate that input perturbations can be mimicked by multiplicative perturbations in the weight space. Leveraging this, we propose Data Augmentation via Multiplicative Perturbation (DAMP), a training method that optimizes DNNs under random multiplicative weight perturbations. We also examine the recently proposed Adaptive Sharpness-Aware Minimization (ASAM) and show that it optimizes DNNs under adversarial multiplicative weight perturbations. Experiments on image classification datasets (CIFAR-10/100, Tiny-ImageNet and ImageNet) and neural network architectures (ResNet50, ViT-S/16, ViT-B/16) show that DAMP enhances model generalization performance in the presence of corruptions across different settings. Notably, DAMP is able to train a ViT-S/16 on ImageNet from scratch, reaching the top-1 error of \(23.7\%\) which is comparable to ResNet50 without extensive data augmentations.1

## 1 Introduction

Deep neural networks (DNNs) demonstrate impressive accuracy in computer vision tasks when evaluated on carefully curated and clean datasets. However, their performance significantly declines when test images are affected by natural distortions such as camera noise, changes in lighting and weather conditions, or image compression algorithms (Hendrycks and Dietterich, 2019). This drop in performance is problematic in production settings, where models inevitably encounter such perturbed inputs. Therefore, it is crucial to develop methods that produce reliable DNNs robust to common image corruptions, particularly for deployment in safety-critical systems (Amodei et al., 2016).

To enhance robustness against a specific corruption, one could simply include it in the data augmentation pipeline during training. However, this approach can diminish performance on clean images and reduce robustness to other types of corruptions (Geirhos et al., 2018). More advanced data augmentation techniques (Cubuk et al., 2018; Hendrycks et al., 2019; Lopes et al., 2019) have been developed which effectively enhance corruption robustness without compromising accuracy on clean images. Nonetheless, a recent study by Mintun et al. (2021) has identified a new set of image corruptions to which models trained with these techniques remain vulnerable. Besides dataaugmentation, ensemble methods such as Deep ensembles and Bayesian neural networks have also been shown to improve generalization in the presence of corruptions (Lakshminarayanan et al., 2017; Ovadia et al., 2019; Dusenberry et al., 2020; Trinh et al., 2022). However, the training and inference costs of these methods increase linearly with the number of ensemble members, rendering them less suitable for very large DNNs.

ContributionsIn this work, we show that simply perturbing weights with multiplicative random variables during training can significantly improve robustness to a wide range of corruptions. Our contributions are as follows:

* We show in Section 2 and Fig. 1 that the effects of input corruptions can be simulated during training via multiplicative weight perturbations.
* From this insight, we propose a new training algorithm called Data Augmentation via Multiplicative Perturbations (DAMP) which perturbs weights using multiplicative Gaussian random variables during training while having the same training cost as standard SGD.
* In Section 3, we show a connection between adversarial multiplicative weight perturbations and Adaptive Sharpness-Aware Minimization (ASAM) (Kwon et al., 2021).
* Through a rigorous empirical study in Section 4, we demonstrate that DAMP consistently improves generalization ability of DNNs under corruptions across different image classification datasets and model architectures.
* Notably, we demonstrate that DAMP can train a Vision Transformer (ViT) (Dosovitskiy et al., 2021) from scratch on ImageNet, achieving similar accuracy to a ResNet50 (He et al., 2016) in 200 epochs with only basic Inception-style preprocessing (Szegedy et al., 2016). This is significant as ViT typically requires advanced training methods or sophisticated data augmentation to match ResNet50's performance when being trained on ImageNet from scratch (Chen et al., 2022; Beyer et al., 2022). We also show that DAMP can be combined with modern augmentation techniques such as MixUp (Zhang et al., 2018) and RandAugment (Cubuk et al., 2020) to further improve robustness of neural networks.

## 2 Data Augmentation via Multiplicative Perturbations

In this section, we demonstrate the equivalence between input corruptions and multiplicative weight perturbations (MWPs), as shown in Fig. 1, motivating the use of MWPs for data augmentation.

Figure 1: **Depictions of a pre-activation neuron \(z=^{}\) in the presence of (a) covariate shift \(\), (b) a multiplicative weight perturbation (MWP) equivalent to \(\), and (c) random MWPs \(\). \(\) denotes the Hadamard product. Figs. (a) and (b) show that for a covariate shift \(\), one can always find an equivalent MWP. From this intuition, we propose to inject random MWPs \(\) to the forward pass during training as shown in Fig. (c) to robustify a DNN to covariate shift.**

### Problem setting

Given a training data set \(=\{(_{k},y_{k})\}_{k=1}^{N}\) drawn i.i.d. from the data distribution \(\), we seek to learn a model that generalizes well on both clean and corrupted inputs. We denote \(\) as a set of functions whose each member \(:\) represents an input corruption. That is, for each \(\), \(()\) is a corrupted version of \(\).2 We define \(():=\{((_{k}),y_{k})\}_{k=1}^{N}\) as the training set corrupted by \(\). We consider a DNN \(:\) parameterized by \(\). Given a per-sample loss \(:_{+}\), the training loss is defined as the average loss over the samples \((;):=_{k=1}^{N}(,_{k},y_{k})\). Our goal is to find \(\) which minimizes:

\[(;()):=_{ }[(;())]\] (1)

without knowing exactly the types of corruption contained in \(\). This problem is crucial for the reliable deployment of DNNs, especially in safety-critical systems, since it is difficult to anticipate all potential types of corruption the model might encounter in production.

### Multiplicative weight perturbations simulate input corruptions

To address the problem above, we make two key assumptions about the corruptions in \(\):

**Assumption 1** (Bounded corruption).: _For each corruption function \(:\) in \(\), there exists a constant \(M>0\) such that \(\|()-\|_{2} M\) for all \(\)._

**Assumption 2** (Transferable robustness).: _A model's robustness to corruptions in \(\) can be indirectly enhanced by improving its resilience to a more easily simulated set of input perturbations._

Assumption 1 implies that the corrupted versions of an input \(\) must be constrained within a bounded neighborhood of \(\) in the input space. Assumption 2 is corroborated by Rusak et al. (2020), who demonstrated that distorting training images with Gaussian noise improves a DNN's performance against various types of corruption. We further validate this observation for corruptions beyond Gaussian noise in Section 4.1. However, Section 4.1 also reveals that using corruptions as data augmentation degrades model performance on clean images. Consequently, we need to identify a method that efficiently simulates diverse input corruptions during training, thereby robustifying a DNN against a wide range of corruptions without compromising its performance on clean inputs.

One such method involves injecting random multiplicative weight perturbations (MWPs) into the forward pass of DNNs during training. The intuition behind this approach is illustrated in Fig. 1. Essentially, for a pre-activated neuron \(z=^{}\) in a DNN, given a corruption causing a covariate shift \(\) in the input \(\), Figs. 0(a) and 0(b) show that one can always find an equivalent MWP \((,)\):

\[z=^{}(+)=((,))^{},(, )=1+/\] (2)

where \(\) denotes the Hadamard product. This observation suggests that input corruptions can be simulated during training by injecting random MWPs into the forward pass, as depicted in Fig. 0(c), resulting in a model more robust to corruption. We thus move the problem of simulating corruptions from the input space to the weight space.

Here we provide theoretical arguments supporting the usage of MWPs to robustify DNNs. To this end, we study how corruption affects training loss. We consider a feedforward neural network \((;)\)

Figure 2: **Depiction of how a corruption \(\) affects the output of a DNN.** Here \(}=()\). The corruption \(\) creates a shift \(}^{(0)}}=}-\) in the input \(\), which propagates into shifts \(}^{(h)}}\) in the output of each layer. This will eventually cause a shift in the loss \(}}\). This figure explains why the model performance tends to degrade under corruption.

of depth \(H\) parameterized by \(=\{^{(h)}\}_{h=1}^{H}\), which we define recursively as follows:

\[^{(0)}():=,^{(h)}():= ^{(h)}^{(h-1)}(),^{(h)}():=^{(h)}(^{(h)}()), h=1,,H\] (3)

where \((;):=^{(H)}()\) and \(^{(h)}\) is the non-linear activation of layer \(h\). For brevity, we use \(^{(h)}\) and \(^{(h)}_{}\) as shorthand notations for \(^{(h)}()\) and \(^{(h)}(())\) respectively. Given a corruption function \(\), Fig. 2 shows that \(\) creates a covariate shift \(_{}^{(0)}:=^{(0)}_{}- ^{(0)}\) in the input \(\) leading to shifts \(_{}^{(h)}:=^{(h)}_{}- ^{(h)}\) in the output of each layer. This will eventually cause a shift in the per-sample loss \(_{}(,,y):=(,_{},y)-(,,y)\). The following lemma characterizes the connection between \(_{}(,,y)\) and \(_{}^{(h)}\):

**Lemma 1**.: _For all \(h=1,,H\) and for all \(\), there exists a scalar \(C_{}^{(h)}()>0\) such that:_

\[_{}(,,y)_{ ^{(h+1)}}(,,y)_{} ^{(h)},^{(h+1)}_{F}+}^{(h) }()}{2}\|^{(h)}\|_{F}^{2}\] (4)

Here \(\) denotes the outer product of two vectors, \(,_{F}\) denotes the Frobenius inner product of two matrices of the same dimension, \(\|\|_{F}\) is the Frobenius norm, and \(_{^{(h)}}(,,y)\) is the Jacobian of the per-sample loss with respect to the pre-activation output \(^{(h)}()\) at layer \(h\). To prove Lemma 1, we use Assumption 1 and the following assumption about the loss function:

**Assumption 3** (Lipschitz-continuous objective input gradients).: _The input gradient of the per-sample loss \(_{}(,,y)\) is Lipschitz continuous._

Assumption 3 allows us to define a quadratic bound of the loss function using a second-order Taylor expansion. The proof of Lemma 1 is provided in Appendix A. Using Lemma 1, we prove Theorem 1, which bounds the training loss in the presence of corruptions using the training loss under multiplicative perturbations in the weight space:

**Theorem 1**.: _For a function \(:\) satisfying Assumption 1 and a loss function \(\) satisfying Assumption 3, there exists \(_{}\) and \(C_{}>0\) such that:_

\[(;())( _{};)+}}{2}\|\|_{F} ^{2}\] (5)

We provide the proof of Theorem 1 in Appendix B. This theorem establishes an upper bound for the target loss in Eq. (1):

\[(;())_{ }[(_{};) +}}{2}\|\|_{F}^{2}]\] (6)

This bound implies that training a DNN using the following loss function:

\[_{}(;):=_{}[(;)]+\|\|_{F}^{2}\] (7)

where the expected loss is taken with respect to a distribution \(\) of random MWPs \(\), will minimize the upper bound of the loss \((;}())\) of a hypothetical set of corruptions \(}\) simulated by \(\). This approach results in a model robust to these simulated corruptions, which, according to Assumption 2, could indirectly improve robustness to corruptions in \(\).

We note that the second term in Eq. (7) is the \(L_{2}\)-regularization commonly used in optimizing DNNs. Based on this proxy loss, we propose Algorithm 1 which minimizes the objective function in Eq. (7) when \(\) is an isotropic Gaussian distribution \((,^{2})\). We call this algorithm Data Augmentation via Multiplicative Perturbations (DAMP), as it uses random MWPs during training to simulate input corruptions, which can be viewed as data augmentations.

RemarkThe standard method to calculate the expected loss in Eq. (7), which lacks a closed-form solution, is the Monte Carlo (MC) approximation. However, the training cost of this approach scales linearly with the number of MC samples. To match the training cost of standard SGD, Algorithm 1 divides each data batch into \(M\) equal-sized sub-batches (Line 6) and calculates the loss on each sub-batch with different multiplicative noises from the noise distribution \(\) (Lines 7-9). The final gradient is obtained by averaging the sub-batch gradients (Line 11). Algorithm 1 is thus suitable for data parallelism in multi-GPU training, where the data batch is evenly distributed across \(M>1\) GPUs. Compared to SGD, Algorithm 1 requires only two additional operations: generating Gaussian samples and point-wise multiplication, both of which have negligible computational costs. In our experiments, we found that both SGD and DAMP had similar training times.

Adaptive Sharpness-Aware Minimization optimizes DNNs under adversarial multiplicative weight perturbations

In this section, we demonstrate that optimizing DNNs with adversarial MWPs follows a similar update rule to Adaptive Sharpness-Aware Minimization (ASAM) (Kwon et al., 2021). We first provide a brief description of ASAM and its predecessor Sharpness-Aware Minimization (SAM) (Foret et al., 2021):

SamMotivated by previous findings that wide optima tend to generalize better than sharp ones (Keskar et al., 2017; Jiang et al., 2020), SAM regularizes the sharpness of an optimum by solving the following minimax optimization:

\[_{}_{\|\|_{2}}(+;)+\|\|_{F}^{2}\] (8)

which can be interpreted as optimizing DNNs under adversarial additive weight perturbations. To efficiently solve this problem, Foret et al. (2021) devise a two-step procedure for each iteration \(t\):

\[^{(t)}=}(^{(t)}; )}{_{}(^{(t)}; )_{2}},^{(t+1)}=^{(t)}- _{t}(_{}(^{(t)}+^{(t) };)+^{(t)})\] (9)

where \(_{t}\) is the learning rate. Each iteration of SAM thus takes twice as long to run than SGD.

AsamKwon et al. (2021) note that SAM attempts to minimize the maximum loss over a rigid sphere of radius \(\) around an optimum, which is not suitable for ReLU networks since their parameters can be freely re-scaled without affecting the outputs. The authors thus propose ASAM as an alternative optimization problem to SAM which regularizes the _adaptive sharpness_ of an optimum:

\[_{}_{\|T_{}^{-1}\|_{2}}(+;)+\|\|_{F}^{2}\] (10)

where \(T_{}\) is an invertible linear operator used to reshape the perturbation region (so that it is not necessarily a sphere as in SAM). Kwon et al. (2021) found that \(T_{}=||\) produced the best results. Solving Eq. (10) in this case leads to the following two-step procedure for each iteration \(t\):

\[}^{(t)}=^{(t)}^{2} _{}(^{(t)};)}{ ^{(t)}_{}(^{(t)}; )_{2}},^{(t+1)}=^{(t)}- _{t}(_{}(^{(t)}+}^{(t)};)+^{(t)})\] (11)

Similar to SAM, each iteration of ASAM also takes twice as long to run than SGD.

ASAM and adversarial multiplicative perturbationsAlgorithm 1 minimizes the expected loss in Eq. (7). Instead, we could minimize the loss under the adversarial MWP:

\[_{}(;):=_{\|\|_{2}} (+;)+ \|\|_{F}^{2}\] (12)

Following Foret et al. (2021), we solve this optimization problem by using a first-order Taylor expansion of \((+;)\) to find an approximate solution of the inner maximization:

\[*{arg\,max}_{\|\|_{2}}(+ {};)*{arg\,max}_{\| \|_{2}}(;)+ {},_{}(;)\] (13)

The maximizer of the Taylor expansion is:

\[}()=_{ }(;)}{\|_{ }(;)\|_{2}}\] (14)

Substituting back into Eq. (12) and differentiating, we get:

\[_{}_{}(;) _{}(};)+=_{}}_{}}(};)+\] (15) \[=_{}}(} ;)+_{}(}() )_{}}(}; )+\] (16)

where \(}\) is the perturbed weight:

\[}=+}()=+^{2}_{} (;)}{\|_{} (;)\|_{2}}\] (17)

Similar to Foret et al. (2021), we omit the second summand in Eq. (16) for efficiency, as it requires calculating the Hessian of the loss. We then arrive at the gradient formula in the update rule of ASAM in Eq. (11). We have thus established a connection between ASAM and adversarial MWPs.

## 4 Empirical evaluation

In this section, we assess the corruption robustness of DAMP and ASAM in image classification tasks. We conduct experiments using the CIFAR-10/100 (Krizhevsky, 2009), TinyImageNet (Le and Yang, 2015), and ImageNet (Deng et al., 2009) datasets. For evaluation on corrupted images, we utilize the CIFAR-10/100-C, TinyImageNet-C, and ImageNet-C datasets provided by Hendrycks and Dietterich (2019), as well as ImageNet-\(}\)(Mitnun et al., 2021), ImageNet-D (Zhang et al., 2024), ImageNet-A (Hendrycks et al., 2021), ImageNet-Sketch (Wang et al., 2019), ImageNet-{Drawing, Cartoon} (Salvador and Oberman, 2022), and ImageNet-Hard (Taesiri et al., 2023) datasets, which encapsulate a wide range of corruptions. Detail descriptions of these datasets are provided in Appendix E. We further evaluate the models on adversarial examples generated by the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014). In terms of architectures, we use ResNet18 (He et al., 2016) for CIFAR-10/100, PreActResNet18 (He et al., 2016) for TinyImageNet, ResNet50 (He et al., 2016), ViT-S/16, and ViT-B/16 (Dosovitskiy et al., 2021) for ImageNet. We ran all experiments on a single machine with 8 Nvidia V100 GPUs. Appendix F includes detailed information for each experiment.

### Comparing DAMP to directly using corruptions as augmentations

In this section, we compare the corruption robustness of DNNs trained using DAMP with those trained directly on corrupted images. To train models on corrupted images, we utilize Algorithm 2 described in the Appendix. For a given target corruption \(\), Algorithm 2 randomly selects half the images in each training batch and applies \(\) to them. This random selection process enhances the final model's robustness to the target corruption while maintaining its accuracy on clean images. We use the imagecorruptions library (Michaelis et al., 2019) to apply the corruptions during training.

Evaluation metricWe use the corruption error \(_{c}^{f}\)(Hendrycks and Dietterich, 2019) which measures the predictive error of classifier \(f\) in the presence of corruption \(c\). Denote \(E_{s,c}^{f}\) as the error of classifier \(f\) under corruption \(c\) with corruption severity \(s\), the corruption error \(_{c}^{f}\) is defined as:

\[_{c}^{f}=(_{s=1}^{5}E_{s,c}^{f}) _{s=1}^{5}E_{s,c}^{f_{}}\] (18)

[MISSING_PAGE_FAIL:7]

most corruption scenarios, even though SAM takes twice as long to train and has higher accuracy on clean images. Additionally, DAMP improves accuracy on clean images over Dropout on CIFAR-100 and TinyImageNet. Finally, ASAM consistently surpasses other methods on both clean and corrupted images, as it employs adversarial MWPs (Section 3). However, like SAM, each ASAM experiment takes twice as long as DAMP given the same epoch counts.

ResNet50 / ImageNetTable 1 presents the predictive errors for the ResNet50 / ImageNet setting on a variety of corruption test sets. It shows that DAMP consistently outperforms the baselines in most corruption scenarios and on average, despite having half the training cost of SAM and ASAM.

ViT-S16 / ImageNet / Basic augmentationsTable 2 presents the predictive errors for the ViT-S16 / ImageNet setting, using the training setup from Beyer et al. (2022) but with only basic Inception-style preprocessing (Szegedy et al., 2016). Remarkably, DAMP can train ViT-S16 from scratch in 200 epochs to match ResNet50's accuracy without advanced data augmentation. This is significant as ViT typically requires either extensive pretraining (Dosovitskiy et al., 2021), comprehensive data augmentation (Beyer et al., 2022), sophisticated training techniques (Chen et al., 2022), or modifications to the original architecture (Yuan et al., 2021) to perform well on ImageNet. Additionally, DAMP consistently ranks in the top 2 for corruption robustness across various test settings and has the best corruption robustness on average (last column). Comparing Tables 1 and 2 reveals that ViT-S16 is more robust to corruptions than ResNet50 when both have similar performance on clean images.

ViT / ImageNet / Advanced augmentationsTable 3 presents the predictive errors of ViT-S16 and ViT-B16 on ImageNet with MixUp (Zhang et al., 2018) and RandAugment (Cubuk et al., 2020). These results indicate that DAMP can be combined with modern augmentation techniques to further improve robustness. Furthermore, using DAMP to train a larger model (ViT-B16) yields better results than using SAM/ASAM to train a smaller model (ViT-S16), given the same amount of training time.

    &  &  \\  & Error (\%) &  &  &  & }\)} &  &  &  &  &  &  \\  Dropout & \(23.6_{ 0.2}\) & \(90.7_{ 0.1}\) & \(95.7_{ 0.1}\) & \(61.7_{ 0.2}\) & \(61.6_{ 0.1}\) & \(49.6_{ 0.2}\) & \(88.9_{ 0.1}\) & \(77.4_{ 0.3}\) & \(78.3_{ 0.3}\) & \(85.8_{ 0.1}\) & \(76.6\) \\ DAMP & \(23.8_{ 0.1}\) & \(}\) & \(96.2_{ 0.1}\) & \(}\) & \(}\) & \(}\) & \(88.7_{ 0.1}\) & \(}\) & \(}\) & \(85.3_{ 0.2}\) & \(\) \\ SAM & \(23.2_{ 0.1}\) & \(90.4_{ 0.2}\) & \(96.6_{ 0.1}\) & \(60.2_{ 0.2}\) & \(60.7_{ 0.1}\) & \(47.6_{ 0.1}\) & \(}\) & \(74.8_{ 0.1}\) & \(77.5_{ 0.1}\) & \(85.8_{ 0.3}\) & \(75.8\) \\ ASAM & \(_{ 0.1}\) & \(89.7_{ 0.2}\) & \(96.8_{ 0.1}\) & \(58.9_{ 0.1}\) & \(59.2_{ 0.1}\) & \(45.5_{ 0.1}\) & \(88.7_{ 0.1}\) & \(72.3_{ 0.1}\) & \(76.4_{ 0.2}\) & \(}\) & \(74.7\) \\   

Table 1: **DAMP surpasses the baselines on corrupted images in most cases and on average. We report the predictive errors (lower is better) averaged over 3 seeds for the ResNet50 / ImageNet experiments. Subscript numbers represent standard deviations. We evaluate the models on IN-{C, \(}\), A, D, Sketch, Drawing, Cartoon, Hard}, and adversarial examples generated by FGSM. For FGSM, we use \(=2/224\). For IN-{C, \(}\)}, we report the results averaged over all corruption types and severity levels. We use 90 epochs and the basic Inception-style preprocessing for all experiments.**

Figure 4: **DAMP surpasses SAM on corrupted images in most cases, despite requiring only half the training cost. We report the predictive errors (lower is better) averaged over 5 seeds. A severity level of \(0\) indicates no corruption. We use the same number of epochs for all methods.**

## 5 Related works

DropoutPerhaps most relevant to our method is Dropout (Srivastava et al., 2014) and its many variants, such as DropConnect (Wan et al., 2013) and Variational Dropout (Kingma et al., 2015). These methods can be viewed as DAMP where the noise distribution \(\) is a structured multivariate Bernoulli distribution. For instance, Dropout multiplies all the weights connecting to a node with a binary random variable \(p()\). While the main motivation of these Dropout methods is to prevent co-adapitations of neurons to improve generalization on clean data, the motivation of DAMP is to improve robustness to input corruptions without harming accuracy on clean data. Nonetheless, our experiments show that DAMP can improve generalization on clean data in certain scenarios, such as PreActResNet18/TinyImageNet and ViT-S16/ImageNet.

Ensemble methodsEnsemble methods, such as Deep ensembles (Lakshminarayanan et al., 2017) and Bayesian neural networks (BNNs) (Graves, 2011; Blundell et al., 2015; Gal and Ghahramani, 2016; Louizos and Welling, 2017; Izmailov et al., 2021; Trinh et al., 2022), have been explored as effective defenses against corruptions. Ovadia et al. (2019) benchmarked some of these methods, demonstrating that they are more robust to corruptions compared to a single model. However, the training and inference costs of these methods increase linearly with the number of ensemble members, making them inefficient for use with very large DNNs.

Data augmentationData augmentations aim at enhancing robustness include AugMix (Hendrycks et al., 2019), which combines common image transformations; Patch Gaussian (Lopes et al., 2019), which applies Gaussian noise to square patches; ANT (Rusak et al., 2020), which uses adversarially learned noise distributions for augmentation; and AutoAugment (Cubuk et al., 2018), which learns

    &  &  &  &  \\  & & & Error (\%) \(\) & FGSM & A & C & \(\) & Cartoon & D & Drawing & Sketch & Hard & Avg \\   & 100 & 20.6h & 28.55 & 93.47 & 93.44 & 65.87 & 64.52 & 50.37 & 91.15 & 79.62 & 88.06 & 87.19 & 79.30 \\  & 200 & 41.1h & 28.74 & 90.95 & 93.33 & 66.90 & 64.83 & 51.23 & 92.56 & 81.24 & 87.99 & 87.60 & 79.63 \\   & 100 & 20.7h & 25.50 & 92.76 & 92.92 & 57.85 & 57.02 & 44.78 & 88.79 & 69.92 & 83.16 & 85.65 & 74.76 \\  & 200 & 41.1h & **23.75** & **84.33** & **90.56** & 55.58 & **55.58** & 41.06 & **87.87** & 68.36 & 81.82 & **84.18** & **72.15** \\  SAM & 100 & 41h & 23.91 & 87.61 & 93.96 & 55.56 & 55.93 & 42.53 & 88.23 & 69.53 & 81.86 & 85.54 & 73.42 \\ ASAM & 100 & 41.1h & 24.01 & 85.85 & 92.99 & **55.13** & 55.64 & **40.74** & 89.03 & **67.80** & **81.47** & 84.31 & 72.55 \\   

Table 2: **ViT-S16 / ImageNet (IN) with basic Inception-style data augmentations**. Due to the high training cost, we report the predictive error (lower is better) of a single run. We evaluate corruption robustness of the models using IN-{C, \(\), A, D, Sketch, Drawing, Cartoon, Hard}, and adversarial examples generated by FGSM. For IN-{C, \(\)}, we report the results averaged over all corruption types and severity levels. For FGSM, we use \(=2/224\). We also report the runtime of each experiment, showing that SAM and ASAM take twice as long to run than DAMP and AdamW given the same number of epochs. DAMP produces the most robust model on average.

    &  &  &  &  &  \\  & & & Error (\%) \(\) & FGSM & \(\) & A & C & Cartoon & D & Drawing & Sketch & Hard & Avg \\   & Dropout & 500 & 111h & 20.25 & 62.45 & 40.85 & 84.29 & 44.72 & 34.35 & 86.59 & 56.31 & 71.03 & 80.87 & 62.38 \\  & DAMP & 500 & 111h & **20.09** & 59.87 & **39.30** & **81.21** & **43.18** & 34.01 & **84.74** & **54.16** & **68.03** & **80.05** & **60.72** \\  & SAM & 300 & 123h & 20.17 & 59.92 & 40.05 & 83.91 & 44.04 & 34.34 & 85.99 & 55.63 & 70.85 & 80.18 & 61.66 \\  & ASAM & 300 & 123h & 20.38 & **59.38** & 39.44 & 83.64 & 43.41 & **33.82** & 85.41 & 54.43 & 69.13 & 80.50 & 61.02 \\   & Dropout & 275 & 123h & 20.41 & 56.43 & 39.14 & 82.85 & 43.82 & 33.13 & 87.72 & 56.15 & 71.36 & 79.13 & 61.08 \\  & DAMP & 275 & 124h & **19.36** & **55.20** & 37.87 & **80.49** & 41.67 & 31.63 & **87.06** & 52.32 & **67.91** & **78.69** & **91.99** \\  & SAM & 150 & 135h & 19.84 & 61.85 & 39.09 & 82.69 & 43.53 & 32.95 & 88.38 & 55.33 & 71.22 & 79.48 & 61.61 \\  & 150 & 136h & 19.40 & 58.87 & **37.41** & 82.21 & **41.18** & **30.76** & 88.03 & **51.84** & 69.54 & 78.83 & 59.85 \\   

Table 3: **ViT / ImageNet (IN) with MixUp and RandAugment**. We train ViT-S16 and ViT-B16 on ImageNet from scratch with advanced data augmentations (DAs). We evaluate the models on IN-{C, \(\), A, D, Sketch, Drawing, Cartoon, Hard}, and adversarial examples generated by FGSM. For FGSM, we use \(=2/224\). For IN-{C, \(\)}, we report the results averaged over all corruption types and severity levels. These results indicate that: (i) DAMP can be combined with modern DA techniques to further enhance robustness; (ii) DAMP is capable of training large models like ViT-B16; (iii) given the same amount of training time, it is better to train a large model (ViT-B16) using DAMP than to train a smaller model (ViT-S16) using SAM/ASAM.

augmentation policies directly from the training data. These methods have been demonstrated to improve robustness to the corruptions in ImageNet-C (Hendrycks and Dietterich, 2019). Mintun et al. (2021) attribute the success of these methods to the fact that they generate augmented images perceptually similar to the corruptions in ImageNet-C and propose ImageNet-\(}\), a test set of 10 new corruptions that are challenging to models trained by these augmentation methods.

Test-time adaptations via BatchNormOne effective approach to using unlabelled data to improve corruption robustness is to keep BatchNorm (Ioffe and Szegedy, 2015) on at test-time to adapt the batch statistics to the corrupted test data (Li et al., 2016; Nado et al., 2020; Schneider et al., 2020; Benz et al., 2021). A major drawback is that this approach cannot be used with BatchNorm-free architectures, such as Vision Transformer (Dosovitskiy et al., 2021).

## 6 Conclusion

In this work, we demonstrate that MWPs improve robustness of DNNs to a wide range of input corruptions. We introduce DAMP, a simple training algorithm that perturbs weights during training with random multiplicative noise while maintaining the same training cost as standard SGD. We further show that ASAM (Kwon et al., 2021) can be viewed as optimizing DNNs under adversarial MWPs. Our experiments show that both DAMP and ASAM indeed produce models that are robust to corruptions. DAMP is also shown to improve sample efficiency of Vision Transformer, allowing it to achieve comparable performance to ResNet50 on medium size datasets such as ImageNet without extensive data augmentations. Additionally, DAMP can be used in conjunction with modern augmentation techniques such as MixUp and RandAugment to further boost robustness. As DAMP is domain-agnostic, one future direction is to explore its effectiveness in domains other than computer vision, such as natural language processing and reinforcement learning. Another direction is to explore alternative noise distributions to the Gaussian distribution used in our work.

LimitationsHere we outline some limitations of this work. First, the proof of Theorem 1 assumes a simple feedforward neural network, thus it does not take into accounts modern DNN's components such as normalization layers and attentions. Second, we only explored random Gaussian multiplicative perturbations, and there are likely more sophisticated noise distributions that could further boost corruption robustness.

## Broader Impacts

Our paper introduces a new training method for neural networks that improves their robustness to input corruptions. Therefore, we believe that our work contributes towards making deep leading models safer and more reliable to use in real-world applications, especially those that are safety-critical. However, as with other methods that improve robustness, our method could also be improperly used in applications that negatively impact society, such as making mass surveillance systems more accurate and harder to fool. To this end, we hope that practitioners carefully consider issues regarding fairness, bias and other potentially harmful societal impacts when designing deep learning applications.