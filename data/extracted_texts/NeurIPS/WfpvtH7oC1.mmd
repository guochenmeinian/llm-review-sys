# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

like directional velocity of a robot learning to walk . In other cases, an environment model can aid search, as in the case of Chess or Go . In all cases access to a fast simulator is paramount. However, for many natural tasks--like teaching a robot to make an omelet--it is much simpler to tell when the task is completed than to supervise each individual step or model the environment dynamics. Learning in these sparse-reward settings, where an informative reward is only obtained extremely infrequently (e.g., at the end of successful episodes), is notoriously difficult. In order for a learning agent to improve its policy, the agent needs to find reward, which requires long periods of exploration, often in a coordinated fashion. One solution to the sparse-reward problem is to engineer a proxy dense-reward, but that requires significant expertise and can lead to undesired reward-hacking behavior .

Another class of solutions to the exploration problem aims to create temporally extended actions, or "skills", from interaction [69; 22; 51; 52] or demonstrations [38; 72; 57; 2; 6; 46]. Formally, given a dataset of demonstrations of related behavior to the desired task, we want to extract a new action space \(^{}_{i=1}^{}_{u=1}^{}_{u}\) consisting of sequences of the original action space, and then find a policy for the desired task using this action space, \(:^{}\). Current methods for skill-extraction rely on neural networks, which require large numbers of demonstrations and expensive training.

Like the long-range coordination required for exploration in sparse-reward RL, language models must model long-range dependencies between discrete tokens. Finer-grained character input leads to extremely long sequences and requires low-level modeling; coarser-grained word-level input results in the model poorly capturing rare and unseen words. The standard solution for language models is to create "subword" tokens somewhere in between individual characters and words, that can express any text [24; 67; 62; 39; 66; 31].

Lifting this idea from language modeling to RL, we propose a tokenization method for skill-learning from demonstrations: Subwords as Skills (SaS). Following prior work [18; 68], we discretize the action space where necessary and use a simple byte-pair encoding (BPE) scheme [24; 67] to obtain temporally extended actions. Then, we use this subword vocabulary as the action-space for online RL. As we demonstrate, such a method benefits from extremely fast skill-generation (seconds v.s. hours for neural network-based methods), 100\(\) faster rollouts due to the lack of an extra neural network during inference, and strong results in several sparse-reward domains. Additionally, we demonstrate transfer of skills collected in a different environment and we interpret the finite set of skills. Code is available for our experiments at https://github.com/dyunis/subwords_as_skills.

## 2 Related Work

**Exploration in RL:** Exploration is a fundamental problem in RL, particularly when reward is sparse. A common approach to encouraging exploratory behavior is to augment the (sparse) environment reward with a dense bonus term that biases toward exploration. This includes the use of (possibly approximate) state visitation counts [61; 45; 9; 13] and state entropy objectives [48; 30; 42; 60; 44; 79] that incentivize the agent to reach "novel" states. Related, "curiosity"-based bonuses encourage the agent to take actions in states where the effect is difficult to predict using a learned forward [65; 15; 74; 56; 1; 12] or inverse  dynamics model.

**Temporally Extended Actions and Hierarchical RL:** Another long line of work proposes action abstractions to enable more effective exploration  and simplify the credit assignment problem. Hierarchical reinforcement learning (HRL) [20; 34; 75; 11; 53; 54; 76; 21; 8; 40; 5; 77] considers the problem of learning policies with successively higher levels of abstraction, where the lowest level considers primitive actions in the environment and the higher levels reason over temporally extended transitions. A classic example of action abstractions is the options framework , which provides a standardization of HRL in which an option is a terminating sub-policy that maps states (or observations) to low-level actions. Options can be prescribed as predefined low-level controllers or learned via intermediate rewards [21; 20; 76]. Some simple instantiations of options include repeated actions  and self-avoiding random walks . Konidaris and Barto  learn a two-level hierarchy by incrementally chaining options ("skills") backwards from the goal state to the start state. Nachum et al.  propose a hierarchical algorithm that learns in a sample-efficient, off-policy fashion. Such gains require addressing normal off-policy instability and non-stationarity that comes with jointly learning low- and high-level policies. Levy et al.  use different forms of hindsight to address similar instability issues that arise when learning policies at multiple levels in parallel. One particularly related work applies grammar-learning to online RL , but such a method learns an ever-growing number of longer actions which is problematic in the sparse-reward setting.

**Skill Learning from Demonstrations:** In addition to the methods mentioned above in the context of HRL, there is an existing body of work that seeks to discover extended actions (skills) prior to, instead of during, online RL. Many methods have been developed for skill discovery from interaction [19; 25; 22; 78; 51; 52]. Most related to our setting is a line of work that explores skill discovery from demonstrations [38; 46; 2; 72; 57; 6]. As an example, Lynch et al.  learn a VAE [36; 64] on chunks of action sequences in order to generate a temporally extended action by sampling a single vector. Ajay et al.  follow a similar approach on top of entire trajectories and only rollout a partial trajectory at inference time. Some of these methods [2; 72; 57] condition on the observations when learning skills; however, such skills transfer poorly across domains unless they are trained on randomized environments [57; 6]. Others [46; 6] simply condition on actions, which means that the skills can be reused in any domain that shares the same action space. To extract more generalizable skills, we follow the latter approach. While a concurrent work  uses a method similar to ours in order to discover skills for supervised learning and transfer learning, we focus on the use of tokenization for online RL.

## 3 Method

Similar to prior work [46; 2; 72; 57; 6], we extract skills from demonstration data of action sequences. Formally, these sequences are a dataset of \(N\) trajectories with lengths \(\{n_{i}\}_{i=1}^{N}\) that involve the same action space as our downstream task:

\[=\{(a_{j})_{i}|i\{1,...,N\},\;j\{1,...,n_{i}\},\;a_{j} ^{d_{}}\},\]

where \(a_{j}\) denotes an individual action. After extracting skills from this dataset, we use these skills as the new action space for reinforcement learning on a downstream task. Crucially, our skills do not rely on observations in the demonstrations, which allows them to transfer to different environments even with very little data. In following subsections we detail our precise method.

### Byte-Pair Encoding

Byte-pair encoding (BPE) was first proposed as a simple method to compress files , but it has recently been used to construct vocabularies for NLP tasks with a resolution in between characters and whole-words [66; 67; 39; 62; 31].

Given a long sequence of tokens (e.g., characters) and an initial fixed vocabulary, BPE consists of two core operations: (i) compute the most frequent pair of neighboring tokens and add it to the vocabulary, and (ii) merge all instances of the pair in the sequence. These two steps of adding tokens and merging alternate until a desired vocabulary size is reached.

Figure 2: Abstract representation of our method. Given demonstrations in the same action space as our downstream task, we discretize the actions and apply a tokenization technique to recover “subwords” that form a vocabulary of skills. We then train a policy on top of these skills for a new task. We only require a common action space between demonstrations and the downstream task.

### Discretizing the Action Space

BPE requires an initial vocabulary \(\) and data formatted as a string of discrete tokens. Clustering is a simple way to form discrete tokens from a continuous action space. Prior work has leveraged these ideas in similar contexts [32; 68; 33] and we follow suit. For simplicity, we perform \(k\)-means clustering with the Euclidean metric on the actions of demonstrations in \(\) to form a vocabulary of \(k\) discrete tokens \(=\{v_{0},,v_{k}\}\). Our default choice for \(k\) will be two times the number of degrees-of-freedom (DoF) of the original action space, or \(2 d_{}\). We further study this choice in Appendix 4.5. Such clustering is the same as the action space of Shafiullah et al.  without the residual correction.

### Merging and Pruning the Subwords

After discretizing the action space (if continuous), we can relabel our demonstrations so that trajectories consist of "strings" of action tokens. Then, we can run BPE  with a large final vocabulary size on these strings to extract skills.

As it runs, BPE keeps all intermediate subwords that make up the longest units. In the context of language, this redundancy may not be particularly detrimental. However, in reinforcement learning redundancy in the action space of a policy will result in a large number of similar actions that compete for probability mass, making exploration and optimization difficult. Thus, we prune the BPE vocabulary to a much smaller size.

To prune our vocabulary to a size \(N_{}\), we choose a desired maximum length of skills, say \(L=10\) actions long. For our pruned vocabulary, we take the first \(N_{}\) subwords of length \(L\) that were found by BPE. If there are only \(m<N_{}\) subwords of length \(L\) discovered, we then take the first \(N_{}-m\) subwords of length \(L-1\), and so on until we reach the desired vocabulary size of \(N_{}\). We choose the first subwords of a certain length because by the design of BPE, those will be the most frequent units of that length. If our demonstrations contain common and useful behavior, these will be the most frequent chunks. We provide an algorithmic description of our entire skill-extraction method in Algorithm 1.

Implicit in our method is an assumption that portions of the demonstrations can be recomposed to solve a new task, i.e., that there exists a policy that solves the new task with the action space that we choose. One can imagine a counter-example where the subwords we obtain lack some critical action sequence without which the new task cannot be solved, either because it is lacking in the demonstrations or because extraction is imperfect. Still, we will show that this assumption is reasonable for several sparse-reward tasks.

## 4 Experiments

In the following sections, we demonstrate the empirical performance of our proposed method: first extracting skills from demonstrations and then using those skills as an action space for online sparse-reward RL. Unlike common methods for offline RL, we do not use any information about observations or reward in the demonstrations. We see that our extracted skills provide significant speed benefits and sensible exploration behavior. We also compare our observation-free unconditional skills to observation-conditioned skills and discuss performance. We then examine the transfer setting, where demonstrations come from a different domain. Finally, we present an ablation of hyperparameters.

### Reinforcement Learning with Unconditional Skills

**Tasks:** We consider online RL on AntMaze and Kitchen from D4RL , two very challenging sparse-reward state-based environments. AntMaze is a maze navigation environment with a quadrupedal robot where the reward is 0 except for at the goal, and Kitchen is a manipulation environment in a kitchen setting where reward is 0 except for on successful completion of a subtask. Demonstrations in AntMaze consist of trajectories between random start and end states in the same maze, while demonstrations in Kitchen consist of different sequences of subtasks than the eventual task. We also consider CoinRun , a discrete-action platforming game. Unlike AntMaze and Kitchen, CoinRun is a visual domain and the demonstrations are collected in levels distinct from those of the final task. All of these domains require many coordinated actions in sequence to achieve any reward, withhorizons between \(280\) and \(1000\) steps. See Appendix A for more information on the tasks and data. Due to the suboptimality of demonstrations on AntMaze, we filter demonstrations to remove portions that correspond to jittering in place.

**Baselines:** We consider SAC ; SAC-discrete  on top of our discretized \(k\)-means actions; Skill-Space Policy (SSP), a VAE  trained on sequences of \(10\) actions at a time ; and State-Free Priors (SFP) , a sequence model of actions that is used to inform action-selection during SAC inference. For SAC we use a standard implementation. For SAC-discrete we reimplement the method. For SSP we use the official implementation  and tune hyperparameters for new domains. For SFP we use official code , and are unable to tune hyperparameters due very large runtimes. Figure 3 provides the complete set of results. We report mean and standard deviation across five seeds. As defaults for our method, we use \(k=2 d_{}\) and \(N_{}=16\). We choose \(N_{}=10^{6}\) so that we always find sufficiently many skills with the desired length \(L=10\), which we choose to be comparable with SSP's length \(10\). We ablate these choices in Appendix 4.5. For more experimental details and hyperparameter settings, see Appendix A. Including our method, all methods only use

Figure 3: Main comparison (unnormalized scores). SSP corresponds to results from official code of Pertsch et al. , SSP-p corresponds to published results. AntMaze is scored \(0\)–\(1\), Kitchen is scored \(0\)–\(4\) in increments of \(1\), CoinRun is scored \(0\)–\(100\) in increments of \(10\). CoinRun is a discrete-action domain, so instead of SAC only SAC-discrete can be used. We see strong performance when compared to baselines across tasks.

the action sequences of demonstrations. For AntMaze, we take the best setting of SSP whether the demonstrations are filtered or not.

We see in Figure 3, that even in these challenging sparse-reward tasks, our method can perform well. We show strong performance over baselines, which mostly achieve 0 return, except for in CoinRun where we are competitive. The large standard deviations are due to the fact that we can only run a small number of seeds and some seeds fail to achieve any reward, but we will show that exploration behavior is still reasonable, which gives us more confidence in the conclusions.

Due to the simplicity of our method, it is significantly faster than baselines. In Table 1, we measure the wall-clock time required to generate skills, as well for a single rollout. We see that our method achieves extremely significant speedups compared to prior work. Our skill discovery method is fast as we simply need to run \(k\)-means and tokenization. SSP and SFP require training larger generative models. In the case of rollouts, our method predicts an entire sequence of actions using a simple policy every \(L\) steps, while SSP and SFP require larger models in order to predict the latent variable, and then generate the next action from that latent. The speedup of our method also translates to faster RL (around \(10\) hours for our method vs. \(24\) hours for SSP and \(1\) week for SFP).

### Exploration Behavior on AntMaze Medium

The stringent evaluation procedure for sparse-reward RL equally penalizes poor learning and exploration. In order to shed light on the poor performance of some methods in Figure 3, we examine exploration on AntMaze Medium. We choose this domain because it is straightforward to visualize good and bad exploration behavior by plotting maze coverage. In Figure 4 we plot state visitation for the first \(1\) million of \(10\) million steps of RL. We show the approximate start position in grey in the bottom left and the approximate goal location in green in the top right. Higher color saturation corresponds to a higher probability of that state. Color is scaled nonlinearly according to a power law between \(0\) and \(1\) for illustration purposes. Thin white areas between the density and the walls can be attributed to the fact that we plot the center body position, and the legs have a nontrivial size limiting the proximity to the wall.

Figure 4 visualizes the exploration behavior across methods, averaged over \(5\) seeds. We see that the \(0\) values for return in Figure 3 for SAC, SSP and SFP are likely due not to poor optimization, but rather to poor exploration early in training, unlike our method. Indeed, we show in Appendix C that

   Method & Skill Generation & Online Rollout \\  SSP & \(130000 1800\) & \(0.9 0.05\) \\ SFP & \(8000 500\) & \(4.1 0.1\) \\ SaS & 3\(\)1 & \(\) \\   

Table 1: Timing (mean \(\) one standard deviation) on AntMaze Medium in seconds. Methods measured on the same Nvidia RTX 3090 GPU with 8 Intel Core i7-9700 3 GHz CPUs @ 3.00 GHz. SSP takes \(\)\(36\) hours for skill generation and SFP takes \(\)\(2\) hours.

Figure 4: A visualization of state visitation in online RL on AntMaze Medium in the first \(1\) million timesteps for (a) SAC-discrete, (b) SFP, (c) SSP, and (d) our method averaged over \(5\) seeds. The grey circle in the bottom-left denotes the start position, while the green circle in the top-right indicates the goal. Notice that our method explores the maze much more extensively, with exploration behavior that is similar for all five seeds. SAC’s visitation is tightly concentrated on the start state, which is why there is so little red in (a) the visitation rendering for SAC-discrete (i.e., it is occluded by the gray circle).

on AntMaze Large, for which not all seeds succeeds (unlike AntMaze Medium, for which all seeds succeed), seeds that perform poorly still exhibit good exploration behavior. One reason for this could be due to the fact that our subwords are a discrete set, so the policy always has diverse options to pick, whereas continuous latent variables can model infinitely many skills with only minor differences. In addition, SAC has fundamental issues in sparse-reward environments as the signal to the Q-function is driven entirely by the entropy bonus, which will lead to uniform weighting on every action and as a result, Brownian motion in the action space. Such behavior is likely why the default setting for SAC  aggressively drives the policy to determinism, but in the sparse reward setting, this also results in a uniform policy. Without diverse and long sequences of coordinated actions, such uniform exploration is insufficient.

### Comparison to Observation-Conditioned Skills

Our skill extraction method does not rely on observations and so may lead to more generalizable skills. However, not conditioning on the observations comes with the drawback that a policy needs to learn the context to deploy skills from scratch. Alternatively, observation-conditioned skills bias policy exploration to match that of the demonstrations. This allows for more stable exploration , but worse generalization .

**Baselines:** Here we compare to the observation-conditioned extension of SSP, SPiRL  which biases a policy toward the use of skills in the same context as in the demonstrations. We also include OPAL , a concurrent work with SPiRL. We take numbers from the paper as OPAL is closed-source. Our tuning procedure for SPiRL is similar to SSP, where we consider the best setting over filtered and unfiltered demonstrations.

In Figure 5, we see that SPiRL shows very strong performance on Kitchen, where the overlap between the dataset and the downstream task is exact, but struggles with AntMaze, likely due to differences between the random trajectories in the dataset and the final task. We also note that our result for SPiRL in Kitchen is worse than the reported \(2\)-\(3\). Given that we use the official code, which already implements Kitchen, the difficulty of sparse-reward RL is likely to blame.

### Transferring Skills

One benefit of unconditioned skills is that they can be extracted from demonstrations that differ from the final task domain. In Figure 6, we highlight that such transfer is possible, and that with varying percentages of demonstrations (down to 10 trajectories) performance is fairly stable. It may seem odd that \(1\%\) performs better than \(10\%\) and \(25\%\), but this may be explained by the bias that random subsampling imposes on the demonstrations. By contrast, observation-conditioned methods require large amounts of trajectories in randomized environments to transfer effectively .

Figure 5: Comparison to methods with observation-conditioned skills. In general we see conditioning helps when the data closely overlaps with the downstream task (Kitchen), but not in AntMaze where the demonstrations are somewhat disjoint. OPAL is a closed-source method similar to SPiRL, so results are taken from Ajay et al. [2, Section 5.3].

Figure 6: Results on transferring skills extracted from AntMaze-M to downstream RL on AntMaze-U, with varying quantity of demonstrations. Even with 1% of the data, our method extracts useful skills

### Ablations

There are a few key hyperparameters of our method (\(k\), \(N_{}\) and \(L\)). In the following, we perform ablations over them in the AntMaze Medium and Kitchen environments. In general, behavior in the Kitchen environment is much noisier, which may indicate that RL training is still unstable.

Number of Discrete PrimitivesAll of our results in Figure 3 use the simple rule-of-thumb that \(k=2\) degrees-of-freedom. In Figure 7 we see that this choice seems to be acceptable, though it should be noted that significantly larger values of \(k\) lead to shorter skills as there are fewer and fewer common subwords with the desired length \(L\).

Subword LengthA crucial property of the vocabulary is the length of the subwords. Long subwords lead to more temporal abstraction and easier credit-assignment for the policy, but long subwords can also get stuck for many transitions, possibly leading to poor exploration. In Figure 8, we vary the value of subword length \(L\). Our default setting for each environment uses \(L=10\) to match the baselines, but we see that different values are also acceptable, though \(L=5\) makes RL more difficult in AntMaze.

Vocabulary SizeUltimately, the dimensionality of the action space will make exploration easier or harder. A large vocabulary results in too many paths for the policy to explore well, but a vocabulary that is too small may not include all the skills necessary to represent a good policy for the task. We see in Figure 9 that larger vocabulary sizes do in fact make RL more difficult in AntMaze.

Figure 8: Results for different choices of subword length \(L\), where the default setting is \(L=10\). Note the legend is left unsorted so that the default setting \(L=10\) is in a consistent color and position.

Figure 7: Results for different numbers of clusters. For AntMaze, DoF = \(d_{}=8\), Kitchen DoF = \(d_{}=9\), and the default setting is \(k=2 d_{}\). Note the legend is left unsorted so that the default setting \(k=2 d_{}\) is rendered in a consistent color and position across all plots.

Tokenizer AlgorithmAll of the results thus far have only considered the BPE tokenizer , but other tokenizers have seen benefits in language modeling, like WordPiece  or Unigram . We see in Figure 10 that BPE and WordPiece are somewhat interchangeable, but that performance suffers with Unigram. This is likely because, unlike BPE and WordPiece, Unigram does not discover a hierarchically structured vocabulary where shorter subwords are contained in longer subwords. Thus, naively picking the first \(N_{}\) subwords of length \(L=10\) may not extract the most common behavior. If we were to allow a length-independent vocabulary, Unigram might be a more natural choice, but we did not explore that here due to the necessity of comparing fairly with baselines.

## 5 Conclusion

Architectures from NLP have made their way into offline RL [14; 32; 68], but as we have demonstrated, there is a trove of further techniques to explore. Motivated by prior evidence that the full range of the action space is not required, we discretize and form skills through a simple tokenization method. Our method is much faster in skill generation and policy inference and leads to strong performance in several challenging sparse-reward tasks with a relatively small sample budget. In addition, the finite vocabulary size leads itself to interpretable skills: one can simply look at the execution to figure out what has been extracted (Appendix B). As proposed, however, there are a few key limitations. Discretization removes resolution from the action space, which may be detrimental in settings like fast locomotion (Appendix D), but this may be fixed by using more clusters \(k\) or a residual correction . In addition, like prior work execution of our subwords is open-loop, so exploration may be inefficient  and unsafe . Still, given the speed, performance and interpretability advantages, we believe that our tokenization method is the first step on a new road to efficient reinforcement learning.

Figure 10: Results for different choices of tokenizer algorithm, where BPE is the default.

Figure 9: Results for different choices of vocabulary size \(N_{}\), where the default setting is \(N_{}=16\). Note the legend is left unsorted so the default setting \(N_{}=16\) is a consistent color and position.