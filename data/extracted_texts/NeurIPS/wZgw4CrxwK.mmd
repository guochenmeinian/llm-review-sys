# Incentivizing Quality Text Generation

via Statistical Contracts

 Eden Saig\({}^{1}\),  Ohad Einav\({}^{1}\),  Inbal Talgam-Cohen\({}^{1,2}\)

\({}^{1}\) Technion - Israel Institute of Technology

\({}^{2}\) Tel Aviv University

{edens,ohadeinav,italgam}@cs.technion.ac.il

###### Abstract

While the success of large language models (LLMs) increases demand for machine-generated text, current pay-per-token pricing schemes create a misalignment of incentives known in economics as _moral hazard_: Text-generating agents have strong incentive to cut costs by preferring a cheaper model over the cutting-edge one, and this can be done "behind the scenes" since the agent performs inference internally. In this work, we approach this issue from an economic perspective, by proposing a pay-for-performance, contract-based framework for incentivizing quality. We study a principal-agent game where the agent generates text using costly inference, and the contract determines the principal's payment for the text according to an automated quality evaluation. Since standard contract theory is inapplicable when internal inference costs are unknown, we introduce _cost-robust_ contracts. As our main theoretical contribution, we characterize optimal cost-robust contracts through a direct correspondence to optimal composite hypothesis tests from statistics, generalizing a result of Saig et al. (NeurIPS'23). We evaluate our framework empirically by deriving contracts for a range of objectives and LLM evaluation benchmarks, and find that cost-robust contracts sacrifice only a marginal increase in objective value compared to their cost-aware counterparts.

## 1 Introduction

Modern-day LLMs are showing increasingly impressive capabilities, and simultaneously becoming increasingly costly. With rising success at handling complex tasks, conversational AI systems are seeing ubiquitous usage across critical domains such as healthcare , financial risk assessment , and law . To achieve such levels of performance, contemporary LLM architectures contain billions and even trillions of parameters, leading to a computational pipeline that requires dedicated facilities and substantial energy to operate .

Due to the high computational requirements of modern LLMs, language generation tasks are typically outsourced to commercial firms which generate text for a fee. These firms either maintain dedicated infrastructure optimized for inference workloads, or act as intermediaries that facilitate access to such resources. To address the tension between performance and computational costs, such firms typically have multiple service options, each offering a different trade-off between model quality and cost . Currently, the most common pricing scheme for such services is _pay-per-token_, in which users agree in advance to pay a fixed rate for each token of text generated by the system .

While simple and intuitive, the pay-per-token pricing scheme creates a misalignment of economic incentives between the firms and their consumers, known in the economic literature as _moral hazard_: As inference is performed internally and a fixed price is agreed upon in advance, firms can strategically increase their profit margin by generating text using a cheaper, lower-quality model. Due to the stochastic nature of language generation, consumers may not be able to reliably determine the quality of the model being used, exposing them to this kind of hazard.

Moral hazard is especially prevalent in cases where the text generation task is complex, and so evaluation is hard: Consider a scenario where a healthcare provider hires a firm to use conversational AI for summarizing medical notes. As medical diagnosis is an intricate and critical task, the healthcare provider wishes the medical summaries to be generated using the most advanced language model. Under the pay-per-token pricing scheme, the healthcare provider agrees in advance to pay the firm a fixed amount for each token generated. However, it is not hard to imagine that the firm may attempt to increase profit margins by routing some of the summarization requests to cheaper language models, instead of the most advanced one, without taking into account their purpose, and knowing that any lower-quality results would be attributed to the stochastic nature of LLM inference.

From pay-per-token to pay-for-performance.In the economic literature, the canonical solution to moral hazard problems is _pay-for-performance_, or _P4P_. Instead of paying a fixed price for any outcome, the parties agree in advance on a _contract_ that specifies a differential payment scheme - for example, agreeing in advance that the firm will receive higher pay when the generated text is considered to be of higher quality. When designed correctly, contracts incentivize rational agents to invest more effort, thus providing a way to align incentives. Interaction around contracts is modeled as a principal-agent game, where the principal commits to a payment scheme, and the agent responds by rationally selecting a utility-maximizing action. Within this framework, the principal seeks to design a contract which satisfies some notion of optimality, such as requiring the least amount of expected pay ("min-pay contract"), or the lowest budget ("min-budget contract").

In this work, we extend the theory of contract design, and use it to design optimal pay-for-performance pricing schemes for delegated text generation. Applying contract design to this setting requires us to overcome the challenges of _automated evaluation_ and _cost uncertainty_. The former stems from the need for a scalable measure of performance to support pay-for-performance pricing, while the latter arises from the principal's uncertainty about the agent's true internal cost structure, as commercial firms often regard operational costs and implementation details as proprietary information.

Our results.To tackle automated evaluation, we draw upon recent advances in the LLM evaluation literature , and propose a modular contract design framework which uses LLM evaluators as subroutines. More specifically, upon receiving generated text, our pricing scheme is implemented by evaluating the prompt-response pair using an automated evaluator and paying accordingly. The choice of evaluator can be tailored to the task: optimal pricing schemes in code generation tasks, for example, would rely on a pass/fail code evaluator , whereas evaluation of linguistic tasks can be achieved using an "LLM-as-a-judge" approach . In our theoretical analysis, we show that our framework is applicable even to intricate tasks where current evaluation methods are noisy and undecisive, as the principal can compensate for the noise by paying more (Proposition 1).

To address the challenge of cost uncertainty, we propose a new notion of _cost-robust contracts_, which are pay-for-performance schemes guaranteed to incentivize effort even when the internal cost structure is uncertain. Our main theoretical contribution is a statistical characterization of optimal cost-robust contracts (Theorem 1): We prove a direct correspondence between optimal cost-robust contracts and statistical hypothesis tests by showing that the min-budget and min-pay contract objectives correspond to minimax risk functions of composite hypothesis tests (Type-1+Type-2 errors and FP/TP, respectively). This significantly generalizes a recent result by Saig et al.  to arbitrary action spaces and multiple optimality objectives. The statistical connection provides intuition and interpretation for numerical results, and the applicability to multiple objectives allows system designers to accommodate different business requirements. Intriguingly, the relation between the optimal contract and the optimal statistical risk have the same functional form in both objectives (min-budget and min-pay). Moreover, multiplying optimal hypothesis tests by a constant whose value depends only on the statistical risk yields _approximately_-optimal contracts (Theorem 2).

Finally, we evaluate the empirical performance of cost-robust contracts by analyzing LLM evaluation benchmarks for two families of tasks. In the first experiment, we compare the performance of two-outcome contracts across code generation tasks with varying difficulty; results show that what determines the pricing scheme is the relative success rates of the models, not the task difficulty. In the second experiment, we compute multi-outcome contracts for an intricate conversational task evaluated via LLM-as-a-judge. Numerical results show that the optimal monotone cost-robust pricing scheme has an intuitive 3-level structure: pay nothing if the quality is poor, pay extra if it is exceptional, and pay a fixed baseline otherwise. We show our framework's flexibility by providing a comprehensive comparison across various contract objectives and simplicity constraints.

### Related work

Our main technical tool is algorithmic contract design (see [5; 20; 14] and subsequent works). Many works in this area address distributional robustness, e.g. ,  which also studies approximation guarantees of simple contracts, and the recent  which presents a distributionally-robust contract design approach for delegation of learning. However, to our knowledge, none address cost-robustness. Connections between contract design and statistics have long been known to exist at a high level (see, e.g., ), and were recently explored by  in the context of adverse selection, and  for two-action min-budget contract. From a technical standpoint, our work is closest to , which only proves the statistical connection for the special case of two-action min-budget contracts. Finally, we note that our cost-robustness framework is general, and our characterization results may be of independent interest. Additional related work appears in Appendix A.

## 2 Problem Setting: Contract Design for Text Generation

We study the delegation of a text generation task from a strategic _principal_ to _agent_, with a payment scheme designed to incentivize quality. Here we formulate the problem as a _contract design_ instance.

### Quality text generation (agent's perspective)

The core of our setting is a standard language generation task. Let \(V\) be a vocabulary of tokens, and denote the set of all token sequences by \(V^{*}\). A _text generator_\(g:V^{*} V^{*}\) is a mapping from a textual prompt \(_{0}\) to a response \(_{R}\). We assume that prompts are sampled from a distribution \(_{0} D(V^{*})\), and denote by \(D_{g}\) the distribution of prompt-response pairs, where the prompt is sampled from \(D\) and the response is generated by generator \(g\). Given a prompt and generated response, a _quality evaluator_ is a function \(q:V^{*} V^{*}[m]\) which scores the response on a scale of \(1,,m\). We use \(F_{g}\) to denote the distribution over scores \([m]\) induced by applying the quality evaluator to a random pair \((_{0},_{R}) D_{g}\), and \(F_{gj}\) to denote the probability of score \(j[m]\).

The agent has access to a collection of possible text generators \(=\{g_{1},,g_{n}\}\), which we also refer to for convenience by their indices \([n]\). Each model \(g_{i}\) is associated with a model-dependent cost \(_{i} 0\), which is the average cost (borne by the agent) of generating a single token from \(g_{i}\). For convenience we write \(D_{i}=D_{g_{i}}\) and \(F_{i}=F_{g_{i}}\). Denote by \(c_{i}=_{i}_{(_{0},_{R}) D_{i}}[\|_{R} \|]\) the expected cost of using the \(i\)th generator. We assume w.l.o.g. that the costs are non-decreasing, i.e., \(c_{1} c_{n}\), and that they reflect the inherent quality of the models. In contract design terminology, the generators are the agent's possible _actions_. The agent can choose a single (_pure_) action, or a distribution over text generators \(()\) known in game theory as a _mixed_ action.1 The cost \(c_{1}\) of the lowest-cost action is the agent's "opportunity cost", and unless stated otherwise \(c_{1}=0\).2

As an abstract contract design problem.The above setting is precisely a contract design setting with \(n\) actions and \(m\) outcomes . Such a setting is defined by the pair \((F,c)\), where \(F\) is an \(n m\) matrix with distribution \(F_{i}\) as its \(i\)th row for every \(i\) (known as the _distribution matrix_), and where \(c\) is a vector of costs. For every action \(i\), \(F_{i}\) and \(c_{i}\) are the outcome distribution and cost, respectively.

Figure 1: Interaction protocol. Principal commits to pay \(t(q)\) monetary units according to response quality, and sends prompt \(_{0}\); Agent selects text generator \(g(t)\), and generates response \(_{R}=g(_{0})\) at cost \(_{g}|_{R}|\); Principal evaluates response quality \(q(_{0},_{R})\), and pays accordingly.

Pay-for-performance and agent's utility.To incentivize high quality text generation, the principal commits in advance to a pay-for-performance contract, which specifies the amount of payment to the agent for generating a response with a certain quality. More formally, given a quality evaluator \(q\) with an output scale \(1,,m\), a _contract_\(t:[m]_{ 0}\) is a mapping from the estimated quality to the size of monetary transfer. Note that transfers are non-negative; this standard restriction is known as _limited liability_, and it mirrors the fact that when a principal hires an agent to perform a task, money flows in one way only (from principal to agent, and not vice versa). If transfers are increasing with score, we say \(t\) is a _monotone_ contract. Monotonicity is not without loss of generality, but is a desirable property as monotone contracts are generally simpler and easier to explain .

Given a contract \(t_{ 0}^{m}\) and an action \(()\), the agent's expected utility \(u_{A}(t;)\) (a.k.a. the agent's profit) is the difference between the expected reward and the expected cost of text generation:

\[u_{A}(t;)=_{g_{i};(_{0},_{R}) D_{i}} [t(q(_{0},_{R}))-_{i}|_{R}]]=_{g_{i} ;j F_{i}}[t(j)-c_{i}],\]

where \((_{0},_{R}) D_{i}\) are the prompt and generated response, \(t(q(_{0},_{R}))\) is the payment transferred to the agent based on the quality of response, and \(_{i}|_{R}|\) is the agent's cost of generating the response. We assume the agent is rational and therefore selects, when facing contract \(t\), an action \((t)\) which maximizes their expected profit (also known as the agent's _best response_):

\[(t)*{arg\,max}_{() }u_{A}(t;).\]

As is standard in contract theory, we assume the agent breaks ties consistently and in a way that agrees with the principal's preferences.3 The interaction model is summarized in Figure 1.

### Designing the contract (principal's perspective)

We assume that the principal seeks to obtain text generated by the model \(g_{n}\), the most advanced model with the (strictly) highest associated cost \(c_{n}>c_{n-1}\). We refer to \(g_{n}\) as the _target action_, i.e. the action which the principal wishes to incentivize. Taking the role of the principal, our goal is to design the "best" contract \(t^{*}\) that incentivizes the agent to generate responses using the target model \(g_{n}\). This is formalized by the following optimization problem:

\[t^{*}=*{arg\,min}_{t_{ 0}^{m}}\|t\| (t)=_{g_{n}},\] (1)

where \(\|t\|\) is a norm of \(t\) representing the principal's economic objective (see below), and \(_{g_{n}}\) is a point-mass distribution over text generators, supported by the target generator \(g_{n}\). We denote the set of contracts incentivizing action \(g_{n}\) by \((g_{n})=\{t_{ 0}^{m}(t)= _{g_{n}}\}\), and further note that the assumption of a single target action serves as a foundational step towards more complex contract design scenarios (see Appendix B.1).

Information structure (who knows what).The agent's available actions \(\) and the possible scores \([m]\) are known to both players. As the quality distributions \(F_{i}\) can be learned from past data, we assume they are known to both principal and agent. As the costs of inference \(\{_{i}\}\) depend on internal implementation details, we assume the costs are known to the agent but uncertain to the principal. We thus aim for a contract optimization framework which maximizes different types of objectives, and allows for optimization of \(t\) even when the costs incurred by the agent are uncertain to the principal.

Objectives: min-budget, min-pay and min-variance contracts.In eq. (1), different norms \(\|t\|\) correspond to different possible optimization goals of the principal: For example, a contract is _min-pay_ if it incentivizes the target action using minimum total expected payment \(_{j F_{n}}[t(j)]\) among all contracts in \((g_{n})\); In eq. (1), this corresponds to the \(_{1}\) norm weighted by the quality distribution of the target action. Similarly, a contract is _min-budget_ if it incentivizes the target action using minimum budget \(B_{t}=_{j}t(j)\); In eq. (1), this corresponds to the \(_{}\) norm. Additionally, we also consider a natural _min-variance_ objective, which was not previously studied to our knowledge. A min-variance contract minimizes the objective \((t)\), corresponding to a weighted \(_{2}\) norm. Optimal contracts for these objectives can be computed in polynomial time by solving a corresponding linear or convex-quadratic program (see Appendix D.1). We also consider approximately-optimal contracts:

**Definition 1** (\(\)-optimal contract).: _Let \( 1\). For contract setting \((F,c)\), let \(t^{*}(g_{n})\) be the optimal contract with respect to objective \(\|t\|\). A contract \(t(g_{n})\) is \(\)-optimal if \(\|t\|\,\|t^{*}\|\)._

## 3 Hypothesis Testing and Contracts

This section sets the stage for connecting cost-robust contracts to statistical tests in Section 4.

### Preliminaries

Simple hypothesis testsConsider two distributions \(F_{0},F_{1}([m])\). Given \(j[m]\) which is sampled from either \(F_{0}\) or \(F_{1}\), a _hypothesis test_ is a function \(:[m]\) which outputs \(1\) if \(j\) is likely to have been sampled from \(F_{1}\), and \(0\) otherwise4. In the hypothesis testing literature, \(F_{0}\) is a _simple null hypothesis_, and \(F_{1}\) is a _simple alternative hypothesis_. Performance measures of hypothesis tests are derived from the probabilities of making different types of errors: For a test \(\), the probability of false positives \(=_{j=1}^{m}F_{0,j}_{j}\) measures the rate of type-1 errors; This is when the test rejects the null hypothesis despite the sample being drawn from \(F_{0}\). Similarly, the probability of false negatives \(=_{j=1}^{m}F_{1,j}(1-_{j})\) measures the rate of type-2 errors, i.e. when the test does not reject the null hypothesis despite the sample being drawn from \(F_{1}\). We also denote the true positives by \(=_{j=1}^{m}F_{1,j}_{j}\); \(\) is also known as the test's power, and equal to \(1-\).

Composite hypothesis testsConsider now two _sets_ of distributions \(\{F_{k}\}_{k=1}^{n-1}\), \(\{F_{n}\}\), where \(F_{i}([m])\) for all \(i[n]\). In hypothesis testing terms, \(\{F_{k}\}_{k=1}^{n-1}\) is a _composite null hypothesis_. \(\{F_{n}\}\) is a simple alternative hypothesis as before, and a composite hypothesis test \(\) outputs \(1\) if a given \(j[m]\) is likely to have been sampled from \(F_{n}\). To define performance in the composite case, we denote by \(_{k}=_{j=1}^{m}F_{k,j}_{j}\) the standard type-1 error between distributions \(F_{k}\) and \(F_{n}\). As the alternative hypothesis is still simple, definitions of \(\) and \(\) remain as before, using \(F_{n}\) as the reference distribution. To measure the performance of hypothesis tests, it is common to take a _worst-case_ approach, and define the composite \(\) as the standard type-1 error \(_{k}\) against the worst-case distribution \(F_{k}\) in the null hypothesis set.

### Risk and minimax tests

To formalize the notion of worst-case error, let \(\) be a composite hypothesis test for \(\{F_{k}\}_{k=1}^{n-1}\), \(\{F_{n}\}\). For any \(k[n-1]\), define a _risk_ function \(r_{k}:^{m}_{ 0}\) to be a mapping from \(\) to a risk score, treating \(\) as a simple hypothesis test between distributions \(F_{k}\) and \(F_{n}\). A natural way of measuring risk is by combining the test's two error types. One measure is the _sum_ of errors, denoted by \(R_{k}():=_{k}+\). A classic result by Neyman and Pearson shows that \(R_{k}()\) is minimized by the _likelihood-ratio test_ for any fixed \(k\). Another measure is the _ratio_ of false positives to true positives, denoted by \(_{k}():=_{k}/\,.\). To generalize a risk measure \(r_{k}\) to a composite hypothesis test, we adopt the worst-case approach and define \(r():=_{k[n-1]}\{r_{k}()\}\). Thus, \(R()=_{k[n-1]}\{R_{k}\}=+\), and \(()=_{k[n-1]}\{_{k}\}=/\,.\).

**Definition 2** (Minimax hypothesis test).: _Let \(_{F}^{*}\) be a composite hypothesis test for \(\{F_{k}\}_{k=1}^{n-1}\), \(\{F_{n}\}\), and fix a risk function \(r_{k}\). The test \(_{F}^{*}\) is minimax optimal w.r.t. \(r\) if it minimizes the worst-case risk:_

\[_{F}^{*}=*{arg\,min}_{^{m}}_{k[n-1]}\{r_{ k}()\}=*{arg\,min}_{^{m}}\{r()\}.\]

_The minimax sum-optimal test and minimax ratio-optimal test are the minimax optimal tests with respect to the sum \(R\) and the ratio \(\), respectively._

Observe that the optimal risk of both types of tests is bounded by \(r() 1\), as the constant test \(_{j}=0.5\) satisfies \(R()=()=1\). We assume at least a small difference between the hypotheses, such that \(r()<1\). This allows us to define contracts based on these tests.

### From tests to contracts and back

We derive "statistical" contracts from hypothesis tests by multiplying them by a function of the risk, and derive "contractual" tests from contracts by normalizing them: Consider a contract setting \((F,c)\), with either known costs and \(b:=c_{n}-c_{1}\), or a cost upper bound \(b c_{n}-c_{1}\). Fix a risk function \(r\) and a corresponding budget function \(B_{r}(,b)\).

* **Test-to-contract**: Let \(\) be a test for sets \(\{F_{k}\}_{k=1}^{n-1}\), \(\{F_{n}\}\) with budget \(B_{r}(,b)[0,)\). The corresponding _statistical contract_\(t^{(r,)}\) is \(B_{r}(,b)\).
* **Contract-to-test**: Let \(t\) be a contract. The corresponding _contractual test_\(^{t}\) is \(t/\|t\|_{}\).

We are interested in the following statistical contracts corresponding to the tests from Definition 2:

**Definition 3**.: _Consider a contract setting \((F,c)\), with either known costs and \(b:=c_{n}-c_{1}\), or a cost upper bound \(b c_{n}-c_{1}\). The sum-optimal statistical contract \(B^{*}_{R}(b)^{*}_{R}\) is obtained from the minimax sum-optimal test \(^{*}_{R}\) multiplied by \(B^{*}_{R}(b):=_{R})}\). The ratio-optimal statistical contract \(B^{*}_{}(b)^{*}_{}\) is obtained from the minimax ratio-optimal test \(^{*}_{}\) multiplied by \(B^{*}_{}(b):=(^{*}_{})-(^{*}_{})}\)._

## 4 Cost-Robust Contracts

In this section we state and prove our main result - a direct connection between composite hypothesis testing and cost-robust contracts. Consider a contract design setting \((F,c)\) with increasing costs \(c_{1} c_{n-1}<c_{n}\), where \(n\) is the target action. In real-world settings, the principal may not have full knowledge of the agent's internal cost structure. We model this by assuming the principal is oblivious to the precise costs, but knows an upper bound \(b c_{n}-c_{1}\). We are interested in _robust_ contracts that incentivize the target action for _any_ cost vector compatible with the upper bound:

**Definition 4** (Cost-robust contracts).: _Consider a distribution matrix \(F\) and a bound \(b>0\) on the costs. Let \(_{b}\) be an ambiguity set of all increasing cost vectors \(c\) such that \(c_{n}-c_{1} b\). A contract is \(b\)-cost-robust if it implements action \(n\) for any cost vector \(c_{b}\)._

Informally, our main theoretical result shows that optimal cost-robust contracts are optimal hypothesis tests up to scaling, where the scaler depends on the risk measure which the test optimizes. Our approach can be applied to several notions of optimality, and each optimality criterion for contracts corresponds to a different optimality criterion for hypothesis tests. Specifically, we derive the correspondence for min-budget and min-pay optimality of contracts. Formally (recall Definition 3):

**Theorem 1** (Optimal cost-robust contracts).: _For every contract setting with distribution matrix \(F\) and an upper bound \(b\) on the (unknown) costs, let \(^{*}_{R}\) (resp., \(^{*}_{}\)) be the minimax sum-optimal (ratio-optimal) test with risk \(R^{*}\) (\(^{*}\)) among all composite hypothesis tests for \(\{F_{k}\}_{k=1}^{n-1}\), \(\{F_{n}\}\). Then:_

* _The_ sum_-optimal statistical contract_ \(B^{*}_{R}(b)^{*}_{R}\) _is_ \(b\)_-cost-robust with_ budget__\(b/(1-R^{*})\)_, and has the lowest budget among all_ \(b\)_-cost-robust contracts._
* _The_ ratio_-optimal statistical contract_ \(B^{*}_{}(b)^{*}_{}\) _is_ \(b\)_-cost-robust with_ expected total payment \(b/(1-^{*})\)_, and has the lowest expected total payment among all_ \(b\)_-cost-robust contracts._

Table 1 summarizes the contract vs. test equivalences arising from Theorem 1. In the special case of contract design settings combining (i) a binary action space (\(n=2\)), (ii) a zero-cost action (\(c_{1}=0\)), and (iii) a tight upper bound (\(b=c_{2}\)), the first half of Theorem 1 recovers a recently-discovered correspondence between hypothesis testing and (non-cost-robust) two-action min-budget contracts (31, Theorem 2). Theorem 1 is more general since it applies to any number of actions as well as to the standard min-pay objective. Thus, Theorem 1 can also be seen as extending the interpretable format of optimal contracts for binary-action settings beyond two actions.

In Appendix D.2, we derive our main lemmas, which are used in Appendix D.4 to prove Theorem 1. Our proofs rely on two main assumptions. The first assumption is that the cost uncertainty bounds are known; as these bounds become looser, the required budget and expected payout increase linearly. The second assumption is that the contract design problem is implementable - i.e., that there exists some contract incentivizing the target action. In particular, implementability holds when text generated by the target model has the highest quality in expectation (see Appendix C.1). Generally, a contract design problem is implementable when the observed quality distribution of the target model can't be emulated by a combination of alternative models at a lower cost (see, e.g., ).

   Economic objective & Objective function & Statistical objective & Risk function \\  Min-budget & \(_{j[m]}t_{j}\) & \(+\) & \(_{F_{k}}(=1)+_{F_{n}}(=0)\) \\ Min-pay & \(_{j F_{n}}[t_{j}]\) & \(/\) & \(_{F_{k}}(=1)}{_{F_{n}}(=1)}\) \\   

Table 1: Correspondence between cost-robust contracts and hypothesis tests, arising from Theorem 1.

### Additional properties of optimal cost-robust contracts

In this section, we focus for concreteness on min-_budget_ cost-robust contracts, and establish their approximation guarantees as well as their functional form under structural assumptions. First, in analogy to , it is natural to examine the approximation guarantees of cost-robust contracts. We show (recall Definition 1):

**Theorem 2** (Approximation guarantees).: _For every contract setting \((F,c)\), let \(0<a b\) be a lower and upper bound on the difference between the target cost and any other cost, i.e., \((c_{n}-c_{i})[a,b]\) for all \(i[n-1]\). Then the min-budget \(b\)-cost-robust contract for \((F,c)\) is \(\)-optimal with respect to the budget objective \(\|\|_{}\), and the approximation ratio \(\) is tight._

Proof in Appendix D.5. As a corollary, combining this result with Theorem 1 shows that statistical contracts are approximately optimal in the global sense: For any contract setting \((F,c)\) with corresponding minimax sum-optimal hypothesis test \(_{R}^{*}\), the contract \(t=-c_{1}}{1-R^{*}}_{R}^{*}\) is \(\)-optimal with respect to the budget metric and \(=-c_{1}}{c_{n}-c_{n-1}}\). We also note that similar results hold for min-_pay_ cost-robust contracts.

We next turn to consider the functional form of optimal cost-robust contracts (i.e., why their payments are as they are). One of the criticisms of optimal (non-robust) contracts is that the payments seem arbitrary and opaque. Compared to this, cost-robust contracts are more transparent and explainable. We show two additional results regarding their format, leveraging the connection to minimax hypothesis testing.

The first result explains the budget: By the minimax principle, there is a "least favorable distribution" (to use terminology from statistics) or, equivalently, a mixed strategy over the rows of \(F\) (to use terminology from game theory) such that no test can achieve for it better risk than the minimax risk \(R^{*}\). We show that the budget of the optimal cost-robust contract can be interpreted using this distribution. Formally, let \(\|F_{1}-F_{2}\|_{}\) be the total variation distance between distributions \(F_{1},F_{2}\), then the budget is as follows (see Appendix D.5 for a proof):

**Proposition 1** (Distribution distance determines budget).: _For every contract setting with distribution matrix \(F\) and spread of costs \(c_{n}-c_{1} b\), the minimum budget of a \(b\)-cost-robust contract is \(_{([n-1])}b/\|F_{n}-_{i<n}_{i}F_{i} \|_{}.\)_

The distribution \(_{i<n}_{i}F_{i}\) that maximizes the above expression is the least favorable one. Intuitively, the closer it is to the target distribution \(F_{n}\), the larger the budget needed for the agent to distinguish among them and prefer the target action. We also note that the required budget approaches infinity when the worst-case distribution distance approaches zero, coinciding with the implementability characterization known from the literature (see Appendix C.1). The second set of results adds standard structure to the distribution matrix \(F\) to obtain even simpler contract formats:

**Definition 5** (Monotone Likelihood Ratio (MLR)).: _A distribution matrix \(F\) satisfies MLR if \(F_{i,j}/F_{i^{},j}\) is monotonically increasing in \(j[m]\) for all \(i>i^{}\)._

Intuitively, if \(F\) satisfies MLR, then the higher the outcome \(j\), the more likely it is to origin from a more costly distribution \(F_{i}\) than from \(F_{i^{}}\) (recall that costs \(c_{i}\) are increasing in \(i\)). Consider minimax composite hypothesis tests for \(\{F_{k}\}_{k=1}^{n-1}\), \(\{F_{n}\}\); if \(F\) does not satisfy MLR, optimal such tests may require randomization (i.e., \(_{j}(0,1)\) for some outcome \(j\)) and/or non-monotonicity (i.e. \(_{j}>_{j^{}}\) for some pair of outcomes \(j<j^{}\)). However, if MLR holds for \(F\), then nice properties (determinism, monotonicity) hold for minimax tests (e.g., by the Karlin-Rubin theorem ), and consequently also for cost-robust contracts (see Appendix D.6 for a proof):

**Proposition 2** (MLR induces threshold simplicity).: _For every contract setting with distribution matrix \(F\) that satisfies MLR, and with spread of costs \(c_{n}-c_{1} b\), the min-budget \(b\)-cost-robust contract for \(F\) is a monotone threshold contract, which pays full budget to the agent for every outcome \(j\) above some threshold \(j^{*}\)._

For min-pay rather than min-budget, under MLR we get a monotone contract with a single positive payment, which is optimal (see [14, Lemma 7]). Finally, we note that finding cost-robust min-budget contracts with two levels of pay ("all-or-nothing" ) is computationally hard in the general case, as the reduction by [31, Theorem 3] also applies in the cost-robust case (see Appendix D.8).

## 5 Empirical Evaluation

We evaluate the empirical performance of our cost-robust contracts using LLM evaluation benchmarks. We compute binary and multi-outcome contracts for two distinct families of tasks based on evaluation scores from known benchmark datasets, optimizing the contract objectives set forth in Section 2.2. Our action space consists of the 7B, 13B, and 70B parameter model versions of the open-source Llama2 and CodeLlama LLMs [37; 30], which share the same architecture and hence similar inference costs. The benchmark data is used to create an empirical outcome distribution for each LLM in the action space. In both cases, contract optimization targets of the largest model variant (70B), which is the most performant and costly. Implementation details are provided in Appendix E.3, and code is available at: https://github.com/edensaig/llm-contracts.

Cost estimation.To estimate the inference costs of the language models, we leverage their open-source availability. We use energy consumption data from the popular Hugging Face LLM-Performance Leaderboard [22; 23], which we then convert to dollar values using conservative cost estimates (see Appendix E.1). As a first-order assumption of cost uncertainty, we assume that inference costs of alternative generators are bounded from below by the cost of the most energy-efficient alternative model (\(c_{1}\)), and bounded from above by the cost of the alternative model with the highest energy consumption (\(c_{n-1}\)).

### Binary-outcome contracts across tasks (code generation)

We begin by analyzing a simple contract design setting across benchmarks of varying difficulties. We use the LLM task of code-generation which has \(m=2\) outcomes: pass or fail. The analysis of a binary outcome space is motivated by the following theoretical property:

**Proposition 3**.: _For any contract design problem with \(m=2\) where the most-costly (target) action has the highest pass rate, the optimal contract is identical for all optimality objectives (min-pay, min-budget, and min-variance). Moreover, the optimal contract satisfies \(t_{}>0\), \(t_{}=0\)._

Proof in Appendix D.7. Proposition 3 allows us to compare performance across different evaluation tasks without being sensitive to the choice of contract objective and constraints such as monotonicity.

Datasets.We use evaluation data from two distinct benchmarks, which represent differing degrees of task difficulty. The Mostly Basic Programming Problems (MBPP) benchmark  contains 974 entry-level programming problems with 3 unit tests per problem. The HumanEval benchmark  consists of 164 hand-written functional programming problems. Included in each programming problem is a function signature, a doc-string prompt, and unit tests. There is an average of 7.7 unit tests per problem, and the overall score is based on a pass/fail evaluation of the responses. For each of these benchmarks, we create a binary-outcome (\(m=2\)) contract from the pass rates of the CodeLlama model family (CodeLlama-{7B,13B,70B}). We use the pass@1 values from the CodeLlama paper  (success rates for a single response), as they capture a setting where the agent gets paid for each response.

Figure 2: Empirical evaluation results. (**Left**) Outcome distributions and optimal contracts for Code Generation data, Section 5.1. (**Right**) Outcome distributions and optimal contracts for MT-Bench data, Section 5.2. For the contracts plot, solid lines represent cost-robust contracts, dashed lines represent cost-aware contracts, and dotted lines represent threshold contracts.

[MISSING_PAGE_FAIL:9]

Discussion

In this paper, we introduce cost-robust contracts as a means to address the emerging problem of moral hazards in LLM inference. Our aim is to offer flexible payment schemes that ensure integrity in current LLM markets, even when facing challenges of incomplete information. One of the key insights from our study is that cost-robust contracts can be relevant and effective in practical settings. Moreover, we generalize the work paved by Saig et al.  by uncovering stronger connections between the fields of contract design and statistical hypothesis testing. These connections underscore the statistical intuition that is prevalent in contract design.

Despite the promising results, our work still has several limitations that would do well to be addressed in future research. For one, the data we capture through the evaluation benchmarks does not accurately reflect real-world distributions, where the prompt space is much richer. A natural direction for future work is to explore approximation guarantees when learning contracts from data. Additionally, our analysis relies on a set of assumptions regarding the cost uncertainty and estimations, which should be carefully considered when designing contracts for Generative AI. Lastly, it would also be interesting to see our contract design framework applied to markets with a more elaborate action space.

Acknowledgements.The authors would like to thank Nir Rosenfeld, Ariel Procaccia, Stephen Bates, and Michael Toker for their insightful remarks and valuable suggestions. Eden Saig is supported by the Israel Council for Higher Education PBC scholarship for Ph.D. students in data science. This work received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant No.: 101077862, project: ALGORCONTRACT, PI: Inbal Talgam-Cohen), by the Israel Science Foundation (grant No.: 3331/24), by the NSF-BSF (grant No.: 2021680), and by a Google Research Scholar Award.