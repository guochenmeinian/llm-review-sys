# CLIP in Mirror: Disentangling text from visual images through reflection

Tiancheng Wang\({}^{1}\)   Yuguang Yang\({}^{2}\)   Linlin Yang\({}^{4}\)   Shaohui Lin\({}^{5}\)   Juan Zhang\({}^{1,3}\)

\({}^{1}\)Institute of Artificial Intelligence, Beihang University, Beijing, China

\({}^{2}\)School of Electronic Information Engineering, Beihang University, Beijing, China

\({}^{3}\)Zhongguancun Laboratory, Beijing, China

\({}^{4}\)State Key Laboratory of Media Convergence and Communication,

Communication University of China, Beijing, China

\({}^{5}\)School of Computer Science and Technology, East China Normal University, Shanghai, China

\({}^{6}\)Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China

Corresponding Author, lyang@cuc.edu.cn

###### Abstract

The CLIP network excels in various tasks, but struggles with text-visual images _i.e._, images that contain both text and visual objects; it risks confusing textual and visual representations. To address this issue, we propose MirrorCLIP, a zero-shot framework, which disentangles the image features of CLIP by exploiting the difference in the mirror effect between visual objects and text in the images. Specifically, MirrorCLIP takes both original and flipped images as inputs, comparing their features dimension-wise in the latent space to generate disentangling masks. With disentangling masks, we further design filters to separate textual and visual factors more precisely, and then get disentangled representations. Qualitative experiments using stable diffusion models and class activation mapping (CAM) validate the effectiveness of our disentanglement. Moreover, our proposed MirrorCLIP reduces confusion when encountering text-visual images and achieves a substantial improvement on typographic defense, further demonstrating its superior ability of disentanglement. Our code is available at https://github.com/tcwangbuaa/MirrorCLIP.

## 1 Introduction

The CLIP network  has demonstrated remarkable success, leading to its widespread application in real-world scenarios. However, it still struggles with text-visual images [7; 16; 11; 1], _i.e._, images that contain both text and visual objects, where CLIP can become confused when processing an image with misleading text in it. For instance, as shown in Figure 1, when asked to describe a visual object, the model might mistake an image of a dog for a cat due to the presence of the "cat" text. In contrast, when asked to recognize text in an image, the model might mistake the text of "eraser" for "egg" due to the visual object of eggs. Can we disentangle the textual and visual1 factors within CLIP? Achieving this disentanglement for CLIP can reduce confusion when encountering such images and enhance its robustness against typographic attacks .

Existing work emphasizes extracting visual features from image features and exploring additional structures  or training strategies [1; 11]. Specifically, Materzynska et al.  introduce a learnable projection, Defense Prefix  introduces a learnable prefix in the prompt, and PAINT  introduces fine-tuning with linearly interpolating neural network weights. However, these methods requiretraining with specified data and overlook textual features. Instead, we aim at a zero-shot architecture without retraining for CLIP and emphasize both textual and visual features via disentanglement.

In this paper, we propose MirrorCLIP, a simple yet efficient text-visual disentanglement framework to enhance CLIP's robustness in text-visual images. Our innovation is leveraging the differences of mirror effect between text and visual elements in the image - when images are horizontally flipped, visual objects maintain semantic consistency after flipping, while text typically becomes a nonsensical string. For instance, after being flipped, an image of a dog still remains its recognizability, while an image containing the word "cat" turns into a nonsensical string like "tac", resulting in the disappearance of its meaning. Based on this observation, we propose to decompose the image features of CLIP into textual and visual factors by contrasting them before and after flipping in the latent space. Specifically, MirrorCLIP employs a dual-stream zero-shot framework. The process begins by inputting both the original and horizontally flipped images into the image encoder to generate corresponding image features. By comparing these features, we generate a disentangling mask that identifies textual and visual regions of the latent variable. This mask is then used to separate textual and visual features. Specifically, the textual features are derived by excluding visual features from the original image features using the mask, while the visual features are obtained by combining image features of the original images with their flipped version.

Extensive experiments validate our proposed method. For text-visual disentanglement, the class activation maps (CAMs)  show that the disentangled textual and visual features correspond precisely to the regions of text and visual objects, respectively. Using the stable diffusion model [21; 20], visual features generate images similar to the original but without text, while textual features generate textual images (_i.e._, images only contain text), demonstrating the effectiveness of our method. To quantitatively evaluate the effectiveness of visual feature disentanglement, we compared the state-of-the-art typographic defense methods Defense Prefix  in 10 synthetic and 3 real-world typographic attack datasets using disentangled features. Typographic attacks add text on top of visuals, testing the model's robustness against textual perturbations. MirrorCLIP achieves substantial performance improvements, with a +4.17% increase in real-world datasets and a +5.89% increase in synthetic datasets. To further evaluate the disentangled textual features, we propose to recognize the typographic attack text. The results show that with disentangled textual features, the accuracy improves to 73.95%, compared to 39.32% without disentanglement. In summary, the contributions of our work are as follows:

* We observed that CLIP exhibits horizontal flip invariance for the visual factors of images but not for the textual factors, and propose a simple yet efficient solution to disentangle textual features from visual features in the latent space of CLIP accordingly.
* We propose MirrorCLIP, a zero-shot text-visual disentanglement framework, which can effectively achieve the disentanglement of visual and textual features without any additional training and significantly reduce confusion in text-visual images while improving the robustness of CLIP against typographic attacks.

Figure 1: The zero-shot prediction of CLIP before and after disentanglement, (a): prediction of text recognition, text of “eraser” is misclassified as “egg” before disentanglement, (b): prediction of image recognition, visual object of a dog is misclassified as a cat before disentanglement

* We qualitatively demonstrate the effectiveness of our disentangled representations through the salient regions of CAMs. Moreover, with stable diffusion models and our disentangled representations, we enable generation based on visual and textual factors.
* By evaluating on typographic images, we show that MirrorCLIP effectively achieves disentangled representations and greatly improves performance compared to CLIP without disentanglement, including a whopping \(16.82\%\) improvement on image recognition and \(34.63\%\) improvement on text recognition, surpassing state-of-the-art methods on defense against typographic attacks.

## 2 Related Work

**Vision-language models** have advanced significantly, learning generalized visual representations that align with textual descriptions . This capability enables VLMs to make few-shot or zero-shot decisions in open-world settings [8; 25; 26], making them highly effective for downstream tasks. However, this broad generalization also raises concerns about robustness, especially when dealing with images containing rich text elements, which can mislead the model's decision results. MirrorCLIP further explores this setting, aiming to disentangle textual and visual features from text-visual images to improve CLIP's robustness in these challenging scenarios.

**Typographic attacks** were first introduced by Goh _et al._, who revealed that the performance of vision-language models drops dramatically when input images contain misleading text. To mitigate this, Materzynska et al. applied a linear projection matrix to disentangle visual from textual features. Ilharco et al. interpolated between fine-tuned and original CLIP models, and Azuma et al.  introduced a learnable defense prefix. We utilize this task to evaluate the disentangled textual and visual features: visual features are used in typographic defense experiments, and textual features are used in typographic text recognition experiments.

**Disentangled representations of CLIP** have been studied to separate different types of information encoded in embeddings. Ramesh _et al._ used PCA to reconstruct CLIP embeddings and generated related images through diffusion models, revealing distinct semantic dimensions. Lemesle _et al._ found that textual and visual factors of an image do not share semantic representations in CLIP. Materzynska _et al._ trained projection matrices to disentangle visual and textual features. MirrorCLIP further explores the way to uncover the textual and visual components of the representations of text-visual images.

## 3 CLIP's Mirror Effect and Disentangling Masks

**Contrastive Language-Image Pretraining.** CLIP  aims to learn robust associations between text and images without requiring explicit labeling or supervision for specific tasks. It is pretrained on a dataset including 400 million image-text pairs without human annotation, which provides a broad spectrum of possible text-image associations. During training, through contrastive learning,

Figure 2: The cosine similarity of the image features encoded by the CLIP image encoder before and after horizontal flipping. Adding text to the image leads to a significant decrease in cosine similarity, indicating that CLIP does not exhibit horizontal flip invariance for textual factors.

CLIP optimizes to maximize the cosine similarity of embeddings between matching text-image pairs. This enables CLIP to learn the embeddings of images and text within a joint latent space, thereby allowing CLIP to extract the semantics of images. However, recent work has revealed that CLIP can become confused when faced with text-visual images [7; 16; 11; 1]. To address this issue, we leverage CLIP's mirror effect to achieve the disentanglement of textual and visual components within the image embeddings.

**CLIP's Mirror Effect.** When we observe objects in the mirror, we are able to identify their reflected presence. However, this may not be the case with text. Because the text that is mirrored appears as a string of non-sensical characters due to the letter distortion and the reversed writing order. CLIP is a joint image and text embedding model designed to recognize concepts in images. Does CLIP act like a human and exhibit a similar phenomenon?

To determine whether the distinct mirror effects between visual objects and text affect the representation of CLIP, we input both the original and flipped images into the encoder and calculate the cosine similarity between the resulting features. As shown in Figure 2, for clean input images, such as a dog, the cosine similarity remains approximately 1, indicating semantic invariance. However, when text is added, the similarity between the original and flipped images significantly decreases. Furthermore, our quantitative experiments on 10 public datasets reveal that similarity drops significantly from 0.9846 to 0.8225 once text is added to the images as shown in Figure 3 (a). This demonstrates that the image features of visual objects in CLIP have horizontal flip invariance, whereas that of text does not, which can be further exploited to disentangle these two factors from image representations.

**Disentangling Mask.** Given \(X\) and \(X^{f}\) represent the image features before and after image flipping, their cosine similarity can be written as:

\[cos(X,X^{f})=^{n}(X_{i} X_{i}^{f}) }{\|X\|\|X^{f}\|}=_{i=1}^{n}(}{\|X\|}^{f}}{\|X^{f}\|}),\] (1)

where \(\) denotes Hadamard product, \(n\) denotes the dimensionality of features. The cosine similarity is influenced by the product of elements at different positions after normalization. If the signs of elements at the corresponding positions change after flipping, the product becomes negative, leading to a decrease in cosine similarity. Previous research  has shown that different dimensions of CLIP's image embeddings encode distinct semantic information. Therefore, we use the change in the signs of elements at different positions before and after flipping to determine whether a position belongs to the textual or visual factors. Subsequently, we generate the disentangling mask for textual and visual factors with this characteristic. As shown in Figure 4 (a), the disentangling mask \(M\), and its corresponding textual mask \(M^{t}\) and visual mask \(M^{v}\), are calculated as

\[M=Sign(X X^{f}),\] (2)

\[M^{t}=-(M-1), M^{v}=(M+1),\] (3)

Figure 3: Results of mirror effect experiments, (a) Cosine similarity of image features before and after flipping on Original and Typographic datasets, (b) Proportion of textual mask on Original and Typographic datasets.

\(M^{t}\) and \(M^{v}\) are obtained by mapping \(\{-1,1\}\) in \(M\) to \(\{1,0\}\) and \(\{0,1\}\), respectively. Figure 4 (b) shows the disentangling masks for different images, where the black areas represent the textual mask and the white areas represent the visual mask. It can be observed that when text is added to the image, whether handwritten or printed, the area of the textual mask increases significantly. As shown in Figure 3 (b), we conduct experiments across 10 public datasets and reveal that the proportion of textual mask increases significantly from 0.0683 to 0.2431 after adding text to images, which confirms the validity of our mask generation method.

## 4 Zero-shot Disentanglement Framework

Utilizing the disentangling masks, we propose a zero-shot dual-stream disentanglement framework. The pipeline of the disentanglement framework is illustrated in Figure 5. The framework is straightforward and does not require any training. We generate the disentangling mask by comparing the image features before and after image flipping based on Eqs. 2 and 3. Due to the entangling between textual and visual features, any dimension of latent space may contain both textual and visual factors. Therefore, separating with a "boolean" disentangling mask is rough. For example, directly setting the

Figure 4: Generation of disentangling mask, (a) The disentangling mask is generated by contrasting the sign of corresponding positions in the image features before and after flipping. (b) Input images and generated disentangling masks (resized from \(1 512\) to \(16 32\)). After adding text to the image, the proportion of textual mask increases significantly.

Figure 5: Pipeline of zero-shot dual-stream disentanglement framework. The framework takes flipped and original images as input, generates disentangling masks by comparing their image features in the latent space, then utilizes the proposed textual filter and visual filter to generate textual and visual features, achieving disentanglement and completing downstream tasks.

visual mask area to 0 would lead to the loss of the textual semantic information within it. Instead, we propose a "soft" textual filter to remove visual features in the image features as below:

\[X^{t}=X M^{t}+X-X^{f} M^{v},\] (4)

where \(X^{t}\) denotes textual features. The textual filter in Equation 4 consists of two parts: the first part ensures the retention of textual features in the textual mask region, and the second part preserves textual features in the visual mask region while filtering out visual features.

Similarly, we propose a visual filter to get visual features \(X^{v}\) as below:

\[X^{v}=X^{f}+X M^{v}.\] (5)

In Equation 5, the first part uses flipped image features as visual features due to the disappearance of textual semantics after flipping, and the second part enhances robustness against images with flipped text by adding a visual mask region of original image features. After disentangling, either textual features or visual features can be used to replace original image features for inference depending on the specific task. As shown in Figure 1, our disentanglement framework can correct errors made by CLIP in image and text recognition.

## 5 Experiment

### Experimental Setup

**Overview.** To validate the effectiveness of the proposed MirrorCLIP, we conduct the following experiments. In Section 5.2, we first validate the difference in the mirror effect of text and visual elements in images with 10 clean public datasets and their corresponding synthetic typographic datasets. We then qualitatively visualize the quality of disentangled text and visual elements using CAMs and stable diffusion models. In Section 5.3, we primarily validate the disentanglement effectiveness for visual elements by performing a typographic defense experiment on 13 typographic datasets (10 synthetic datasets and 3 real-world datasets), following . In Section 5.4, we evaluate the disentanglement effectiveness for text elements by ensuring that the disentangled textual features can correctly recognize the text added to visual elements in the typographic datasets.

**Datasets. Clean public classification datasets** contain rich visual elements from the real world, which can be used to evaluate the robustness and performance of MirrorCLIP. These include ImageNet , Caltech101 , OxfordPets , StanfordCars , Flowers102 , Food101 , FGVC Aircraft , DTD , SUN397 , and EuroSAT . **Synthetic typographic Datasets** add text of incorrect categories to the images. We follow  to construct synthetic typographic datasets using the 10 clean public datasets mentioned above. **Real-world typographic datasets** include three publicly available real-world typographic attack datasets from Materzynska et al. , PAINT, and Defense Prefix (RTA-100) .

**Baselines.** To evaluate MirrorCLIP's disentanglement performance for visual elements, we benchmark against CLIP , Materzynska _et al._, PAINT  and Defense Prefix . To evaluate MirrorCLIP's disentanglement performance for text elements, we mainly compare it with the vanilla CLIP.

### Validation Experiments for MirrorCLIP

**Validation of different mirror effects for visual and text elements.** To validate our observation that CLIP exhibits horizontal flip invariance for visual features but not for textual features, we conducted experiments on the cosine similarity of image features before and after flipping across 10 clean public image classification datasets and their corresponding typographic datasets.

The average cosine similarity of image features before and after flipping for all samples in all datasets is shown in Table 1. According to the results, before adding text, the cosine similarity of image features before and after flipping is \(0.9846\) across \(10\) datasets, which is close to \(1\) and confirms CLIP's horizontal flip invariance for visual features. However, after adding text, the cosine similarity significantly decreases to \(0.8225\), which also validates CLIP's lack of horizontal flip invariance for textual features.

Visualization of attribution maps for textual and visual features.We employ CAM to showcase a more intuitive visualization towards the disentangled textual and visual features in Equations 4 and 5, respectively. CAMs illuminate the salient regions within an input image that contribute the most strongly to the target concept . Specifically, we employ DecomCAM , which is the state-of-the-art CAM method that strongly reduces the noise effect of typical CAM methods. Our interpretation target is constructed as \(X^{}X^{}\), \(X^{}X^{}\) for textual and visual features, respectively. As shown in Figure 6, our disentangled features can effectively separate regions of text and visual elements.

Similar images generation with textual and visual features.To visualize the effectiveness of disentanglement, we utilized Stable Diffusion models, Stable UnCLIP , with disentangled textual and visual features for image generation [20; 16]. We use image features, disentangled visual and textual features as image embedding conditions. The generated images are depicted in Figure 7.

When using images with irrelevant text for image variations, the generated images mix the semantics of text and visual elements. For example, an image of an apple with the text "earphones" might produce an image with the logo of Apple Inc. and non-sensical text. Similarly, a cat labeled as "dog" could result in an image that combines a cat and a dog's face with non-sensical text. Besides, after disentangling, visual features generate accurate visual content (_e.g._, an apple or a cat) without non-sensical strings, indicating successful filtering of textual features. Textual features, on the other hand, generate images filled with text without visual semantics, demonstrating effective separation.

In addition, we conducted controlled experiments. for images without text, visual features generate image-related patterns, while textual features produce nonsensical characters. For text-only images, visual features generate non-sensical patterns, whereas textual features generate coherent text and patterns, showcasing the effectiveness of our disentanglement framework.

### Evaluation on visual features disentanglement

To further evaluate the effectiveness of the disentangled visual features, we perform typographic defense experiments. Here, MirrorCLIP is required to exclude the interference of the text added and correctly recognize visual elements. The performance of visual features on clean public classification

   & ImageNet & Caltech & Food & Flowers & Pets & SAT & DTD & Cars & Aircraft & SUN & Avg. \\  Original & 0.9790 & 0.9810 & 0.9868 & 0.9907 & 0.9917 & 0.9897 & 0.9893 & 0.9810 & 0.9722 & 0.9849 & 0.9846 \\ Typographic & 0.8164 & 0.8628 & 0.8643 & 0.8047 & 0.7925 & 0.7998 & 0.8403 & 0.7563 & 0.8809 & 0.8071 & 0.8225 \\  

Table 1: Cosine similarity of image features before and after flipping on Clean and Typographic datasets.

Figure 6: Visualization for textual and non-textual features for typographic attacked data using class activation map.

datasets, synthetic typographic datasets, and real-world typographic datasets is shown in Tables 2, 3, and 4, respectively.

From Tables 3 and 4, it is evident that CLIP exhibits poor robustness when faced with typographic attacks, resulting in significant performance degradation. However, using the visual features obtained from our disentanglement framework to replace the original image features significantly improves performance on both synthetic and real-world typographic attack datasets (\(+15.49\%\) on synthetic typographic attack datasets and \(+21.27\%\) on real-world typographic attack datasets). Compared to Materzynska _et al._, PAINT , and Defense Prefix , which introduce additional parameters for training, our zero-shot method surpasses their performance without any additional training. This demonstrates the strong robustness of the visual features obtained from our simple but effective disentanglement framework across various types of images.

Additionally, we test the performance of disentangled visual features on clean datasets. According to Table 2, our zero-shot disentanglement framework slightly improves performance compared to the original CLIP model in clean images without text. This indicates that our method does not cause performance degradation when handling images without text elements.

### Evaluation on textual features disentanglement

To evaluate the performance of the textual features obtained from our framework, we employ these features to recognize the text elements in the typographic datasets. Our results are shown in Table 5. Based on the results, CLIP struggles to achieve high performance in text recognition within images due to the confusion between text and visual elements. However, after applying our disentanglement framework, substituting image features with textual features significantly improves CLIP's performance in text recognition (from \(39.32\%\) to \(73.95\%\)). This indicates that the disentangled textual features can precisely represent the text elements in the images, confirming the effectiveness of the proposed framework.

   & ImageNet & Caltech & Food & Flowers & Pets & SAT & DTD & Cars & Aircraft & SUN & Avg. \\  CLIP & 62.05 & 88.69 & 84.13 & 66.32 & 87.38 & 43.10 & 44.68 & 58.71 & 19.11 & 61.70 & 61.59 \\  Materzynska+  & 54.38 & 80.53 & 55.01 & 51.86 & 75.01 & 37.32 & 36.28 & 40.33 & 13.23 & 51.06 & 49.50 \\ PAINT  & 61.82 & 88.48 & 80.51 & 64.73 & 85.23 & 38.20 & 42.61 & 55.30 & 17.73 & 61.69 & 59.63 \\ Defense Prefix  & **62.48** & **89.28** & 83.65 & 63.82 & 87.22 & **43.85** & 40.64 & 57.47 & 19.26 & 61.41 & 60.91 \\  Ours & 62.34 & 89.17 & **84.52** & **66.34** & **87.71** & 43.77 & **45.00** & **59.07** & **19.41** & **62.12** & **61.95** \\  

Table 2: Results of image classification on original datasets.

Figure 7: Results of image variation using Stable unCLIP.

[MISSING_PAGE_EMPTY:9]

Table L). Therefore, we proposed the visual filter in Equation 5 to obtain visual features, aiming to ensure robustness against flipped images.

Moreover, as seen in Table 6, while textual features notably boost text recognition, they yield negligible accuracy in image recognition tasks. Conversely, visual features significantly enhance image recognition but have minimal impact on text recognition, which validates the effectiveness of our proposed filter in isolating visual and textual features.

## 6 Limitations and Conclusion

**Limitations.** Although our proposed framework achieves excellent disentanglement results with a simple approach, due to the deep entanglement between visual and textual features, our method cannot fully separate them. It does not affect performance in recognition tasks but may influence the results of image generation, as seen in Figure 7 with the examples of apple in the second row and textual features results in the first row. What's more, when facing extreme scenarios such as palindromes in the images, MirrorCLIP still work for normal palindromes, where the shape of the words changes before and after flipping (e.g., "did" to "bib"). However, for special palindromes, where the shape of the words remains basically unchanged (e.g., "mom" to "mom"), MirrorCLIP struggles to achieve disentanglement, although special palindromes are quite rare compared to other word, detailed experimental results are shown in Appendix F.

**Potential application.** We have initially explored object detection and text segmentation by combining MirrorCLIP with RegionCLIP  and SAM . The results show the potential of MirrorCLIP for different downstream tasks or applications. Relevant examples are shown in Figure 8. By using MirrorCLIP to get the disentangled visual region features of RegionCLIP, we can reduce the influence of textual factors and get more accurate detection results. By using the textual features obtained from MirrorCLIP to generate prompts for SAM, we can achieve text localization within images and perform preliminary text segmentation.

**Conclusion.** In this paper, we first discovered and verified that CLIP exhibits horizontal flip invariance for visual features while lacking this property for textual features. Leveraging this observation, we proposed a simple yet effective zero-shot dual-stream disentanglement framework MirrorCLIP by contrasting image features before and after flipping. We demonstrated the effectiveness of this framework through the visualization of attention maps with CAMs and similar image generation with stable diffusion models. Additionally, we conducted experiments on 13 synthetic and real-world typographic attack datasets to further validate the excellent disentanglement performance and robustness of our method across different samples. Furthermore, we surpass state-of-the-art methods in defense against typographic attacks without any additional training.

Figure 8: Potential Application Examples of MirrorCLIP. (a) Using MirrorCLIP to disentangle region features of RegionCLIP, before disentanglement, RegionCLIP mistakenly identified a price tag with text “papaya” as papaya and a laptop monitor as a television set because of the interference of text “television”. (b) Textual features disentangled by MirrorCLIP are used to provide prompts for SAM, achieving text region segmentation.