# JaxMARL: Multi-Agent RL Environments and Algorithms in JAX

Alexander Rutherford\({}^{1}\)\({}^{*}\) Benjamin Ellis\({}^{1}\)\({}^{*}\) Matteo Gallici\({}^{2}\)\({}^{*}\) Jonathan Cook\({}^{1}\)\({}^{}\)

Andrei Lupu\({}^{1}\) Garad Ingvarsson\({}^{3}\) Timon Willi\({}^{1}\) Ravi Hammond\({}^{1}\)

**Akbir Khan\({}^{3}\) Christian Schroeder de Witt\({}^{1}\) Alexandra Souly\({}^{3}\)**

**Saptarashmi Bandyopadhyay\({}^{4}\) Mikayel Samvelyan\({}^{3}\) Minqi Jiang\({}^{3}\) Robert Lange\({}^{5}\)**

**Shimon Whiteson\({}^{1}\) Bruno Lacerda\({}^{1}\) Nick Hawes\({}^{1}\) Tim Rocktaschel\({}^{3}\)**

**Chris Lu\({}^{1}\) Jakob Foerster\({}^{1}\)**

\({}^{1}\)University of Oxford, \({}^{2}\)Universitat Politecnica de Catalunya, \({}^{3}\)University College London,

\({}^{4}\)University of Maryland, \({}^{5}\)Technical University Berlin

Equal ContributionCore Contributor

###### Abstract

Benchmarks are crucial in the development of machine learning algorithms, with available environments significantly influencing reinforcement learning (RL) research. Traditionally, RL environments run on the CPU, which limits their scalability with typical academic compute. However, recent advancements in JAX have enabled the wider use of hardware acceleration, enabling massively parallel RL training pipelines and environments. While this has been successfully applied to single-agent RL, it has not yet been widely adopted for multi-agent scenarios. In this paper, we present JaxMARL, the first open-source, Python-based library that combines GPU-enabled efficiency with support for a large number of commonly used MARL environments and popular baseline algorithms. Our experiments show that, in terms of wall clock time, our JAX-based training pipeline is around 14 times faster than existing approaches, and up to 12500x when multiple training runs are vectorized. This enables efficient and thorough evaluations, potentially alleviating the evaluation crisis in the field. We also introduce and benchmark SMAX, a JAX-based approximate reimplementation of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. The code is available at https://github.com/flairox/jaxmarl.

## 1 Introduction

Benchmarks are crucial for developing new single and multi-agent reinforcement learning (MARL) algorithms. They define problems, enable comparisons, and focus research efforts. For example, the development of MuZero was driven by the challenges presented by Go and Chess . Similarly, decentralised StarCraft Micromanagement tasks  led to the creation of algorithms like QMIX , a popular MARL technique.

In RL research, the runtime of simulations and algorithms is a critical factor affecting the efficiency, thoroughness, and feasibility of experiments. RL training pipelines often require a large number of environment interactions and long, expensive, experimental runs significantly impede research progress. Hardware acceleration and parallelization is an approach to address this: by runningenvironments on hardware accelerators (e.g. GPUs), we can use many more environment instances in parallel than is feasible with a CPU, drastically improving runtime. However, such an approach typically requires significant engineering effort and often relies on non-Python codebases , which reduces its accessibility for many Machine Learning researchers where Python is the lingua franca. That said, recent releases, such as the JAX  library and PyTorch's functorch module , have improved accessibility by enabling Python code to be parallelized and just-in-time compiled on hardware accelerators. This laid the foundation for PureJaxRL , which leveraged JAX to implement a parallelized approach, demonstrating that running both the environment and the model training on the same GPU yields a 10x speedup over a traditional pipeline with a GPU-trained policy but a CPU-based environment, and 4000x when multiple training runs are vectorized. This speedup enables new research directions  and makes large-scale RL research more accessible .

We introduce JaxMARL, which brings these benefits to multi-agent learning. To the best of our knowledge, JaxMARL is the first open-source, Python-based library which leverages JAX for GPU acceleration and supports a wide range of popular MARL environments (as shown in Figure 1) as well as algorithms. We show that MARL greatly benefits from this approach as under the traditional approach, of using CPU-based environments, experiments tend to be particularly slow due to the increased computational burden of training multiple agents simultaneously and the higher sample complexity arising from challenges like non-stationarity and decentralized partial observability. Utilizing an end-to-end JAX-based pipeline for MARL significantly accelerates these experiments, opening up new possibilities for research in this field.

Alongside computational issues, MARL research also struggles with thorough evaluation standards . In particular, MARL papers typically only test on a few domains. Of the 75 recent MARL papers analysed by , 50% used only one evaluation environment and a further 30% used only two. While SMAC  and MPE , the two most used environments, have various tasks or maps, the lack of a standard set raises the risk of biased comparisons and incorrect conclusions. This leads to environment overfitting and unclear progress markers. By alleviating computational constraints, JaxMARL allows for rapid evaluations across a broad set of environments and hence is a powerful tool to address MARL's current evaluation crisis.

Figure 1: JaxMARL environments. We provide JAX-based implementations of a wide range of customizable MARL environments, covering continuous and discrete dynamics, variable number of agents, full and partial observability, and cooperative, competitive and mixed-incentive settings.

More specifically, in this paper, our contributions are as follows:

* **JAX Implementations of Popular MARL Environments:** We implement a wide range of popular MARL environments in JAX, enabling fast experimentation across diverse environments. Taking advantage of large scale parallelism, many of our environments run over three orders of magnitude faster on a GPU than their CPU-based counterparts. They are implemented in Python ensuring ease-of-use, following our philosophy set out in Figure 3.
* **New MARL Environment Suites:** We introduce two new MARL environment suites: SMAX and STORM. SMAX is an approximate reimplementation of the popular SMAC(v2)  benchmark entirely in JAX, rather than using the StarCraft II game engine like SMAC. It is therefore more customizable and significantly faster: SMAX training is 40,000x faster than the equivalent SMAC implementation on a single NVIDIA 2080 when multiple training runs are vectorized. STORM is a general-sum environment suite inspired by the Melting Pot  environment suite that features temporally extended actions within social dilemmas.
* **Implementation of Popular MARL Algorithms in JAX:** We implement many popular MARL algorithms in JAX, such as IPPO, MAPPO and QMIX. As outlined in Figure 3, our training pipeline is up to 14x faster than current popular approaches, and up to 12500x when multiple training runs are vectorized.
* **Comprehensive Benchmarking:** We thoroughly benchmark the speed and correctness of our environments and algorithms, comparing them to existing popular repositories. Generally, our end-to-end JAX implementations run several thousand times faster than their CPU-based counterparts while maintaining equivalent agent performance.
* **Environment Evaluation Recommendations and Best Practice:** Finally, we provide environment evaluation recommendations for different MARL research settings, such as centralized training with decentralized execution and zero-shot coordination. We also provide scripts for large scale evaluation and plotting based on best-practice in the field.

## 2 Background

Our work brings the benefits of hardware acceleration to multi-agent reinforcement learning.

Hardware Accelerated EnvironmentsJAX enables the use of Python code with any hardware accelerator, allowing researchers to write hardware-accelerated code easily. Within the RL community, writing environment code in JAX has gained recent popularity. This brings three chief advantages. Firstly, environments written in JAX can be very easily parallelised by using JAX's vmap operation, which vectorises a function across an input dimension. Secondly writing the environment in JAX allows the agent and environment to be co-located on the GPU, which eliminates the time taken to copy between CPU and GPU memory. Finally, the code written can be compiled just-in-time, thereby improving performance. Combined, these factors bring significant increases in training speed,with PureJaxRL  achieving a \(4000\)x speedup with vectorized training over traditional training in single-agent settings.

Multi-Agent Reinforcement Learning SettingsMulti-Agent Reinforcement Learning is a subfield of reinforcement learning that focuses on environments where multiple agents interact and learn simultaneously. MARL encompasses various settings that address interactions between multiple agents. The most widely-studied MARL setting is the fully-cooperative one, in which agents work together to achieve a common goal. One key framework is centralized training with decentralized execution (CTDE) , which allows agents to share information during the learning phase or use additional environment information. During execution, however, agents act based on their independent and partial observations of the environment. Another is zero-shot coordination, which focuses on training agents to coordinate successfully with unseen partners or in new environments without additional training . Beyond fully-cooperative settings, there are zero-sum and general-sum  benchmarks that study competitive and mixed incentive interactions.

## 3 JaxMARL

We present JaxMARL, a library containing simple and accessible JAX implementations of popular MARL environments and algorithms. JaxMARL enables significant acceleration and parallelisation over existing implementations. To the best of our knowledge, JaxMARL is the first open-source library that provides JAX-based implementations of a wide range of both MARL environments and baselines. JaxMARL's _interface_ is inspired by PettingZoo  and Gymax , while the _design philosophy_ is based on PureJaxRL  and CleanRL . We designed it to be a simple and easy-to-use interface for a wide range of MARL problems. A full specification is provided in the Appendix.

### Environments

JaxMARL contains a diverse range of JAX reimplementations of _existing_ environments. It also _introduces_ SMAX, a novel SMAC-like JAX environment, and STORM, an expansion of matrix games to grid-world scenarios. In this section, we introduce our environments while further details on their implementations can be found in the Appendix.

We measure the speed of our environments in steps per second when using random actions and compare our speed to that of the original environments in Table 3, see the Appendix for details.

#### 3.1.1 New Environments

SmaxThe StarCraft Multi-Agent Challenge (SMAC) is a popular benchmark in cooperative multi-agent reinforcement learning (MARL) but has several limitations. SMAC's environment lacks sufficient stochasticity for complex policies , and its reliance on the StarCraft II engine makes it slow and memory-intensive . Additionally, StarCraft II's constraints limit scenario variety and do not support competitive self-play without significant engineering. To address these issues, we introduce SMAX, a SMAC-like, hardware-accelerated, customizable environment. SMAX features more lightweight dynamics and a less exploitable AI. SMAX incorporates original SMAC scenarios and scenarios similar to those in SMACv2, but is also far more customizable. We provide more details on SMAX and how it improves on SMAC and SMACv2 in the Appendix.

Spatial-Temporal Representations of Matrix Games (STORM)Inspired by the "in the Matrix" games in Melting Pot 2.0 , the STORM  environment expands on matrix games by representing them as grid-world scenarios. Agents collect resources which define their strategy during interactions and are rewarded based on a pre-specified payoff matrix. STORM can represent cooperative, competitive or general-sum games, like the prisoner's dilemma . Thus, STORM can be used for studying paradigms such as _opponent shaping_, where agents act with the intent to change other agents' learning dynamics, which has been empirically shown to lead to more prosocial outcomes [17; 65; 41; 29; 69]. Compared to the Coin Game or simple matrix games, the grid-world setting presents a variety of new challenges such as partial observability, multi-step agent interactions, temporally-extended actions, and longer time horizons. Unlike the "in the Matrix" games from Melting Pot, STORM features stochasticity, increasing the difficulty .

#### 3.1.2 Existing Environments

We have also provided JAX-based implementations of several existing environments.

**Hanabi** is a fully-cooperative partially-observable multiplayer card game, where players can observe others' cards but not their own. It is a common benchmark for zero-shot coordination, theory of mind, and ad-hoc teamplay research [23; 24; 9; 40].

**Overcooked** is commonly used for assessing fully-cooperative and fully-observable Human-AI task performance. Our implementation mimics the original from Overcooked-AI . For a discussion on this environment's limitations see .

**MABrax** is a derivative of Multi-Agent MuJoCo , an extension of the MuJoCo Gym environment  that is commonly used for benchmarking continuous multi-agent robotic control.

**Multi-Agent Particle Environment (MPE)** tasks feature a 2D world with simple physics where particle agents can move, communicate, and interact with fixed landmarks .

**Coin Game** is a two-player grid-world environment which emulates social dilemmas such as the iterated prisoner's dilemma . While this is a common benchmark for the general-sum setting, previous work  has illustrated issues which STORM corrects.

**Switch Riddle** is a simple cooperative communication task included as a debugging tool.

### Algorithms

In this section, we present our re-implementation of five well-known MARL baseline algorithms using JAX. All of our training pipelines are fully compatible with JAX's jit and vmap functions, resulting in significant acceleration of the training processes, as outlined in Figure 3. It also enables parallelisation of training across many seeds and hyperparameters on a single GPU. We follow CleanRL's philosophy of providing clear, single-file implementations  and provide a brief overview of the implemented baselines in the Appendix.

**PPO** We implement both Independent PPO (IPPO) [55; 14] and Multi-Agent PPO (MAPPO) , with both implementations based on PureJaxRL . We utilise parameter sharing across homogeneous agents and provide both feed-forward and RNN policies.

**Q-learning** Our Q-Learning baselines, including Independent Q-Learning (IQL) , Value Decomposition Networks (VDN) , and QMIX , have been implemented in accordance with the PyMARL codebase  to ensure consistency with published results and enable direct comparisons with PyTorch.

## 4 Evaluation Recommendations

Previous work  has found significant differences in the evaluation protocols between MARL research works. We identify four main research areas that would benefit from our library: cooperative centralised training with decentralised execution (CTDE) , zero-shot coordination , general-sum games, and cooperative continuous action methods.

To aid comparisons between methods, we recommend standard _minimal_ sets of evaluation environments for each of these settings in Table 1. It's important to note that these are _minimal_ and we encourage as broad an evaluation as possible. For example, in the zero-shot coordination setting, all methods should be able to evaluate on Hanabi and Overcooked. However, it may also be possible to evaluate such methods on the SMACv2 settings of SMAX. Similarly, SMAX could be used to evaluate two-player zero-sum methods by training in self-play. For some settings, such as continuous action environments and general-sum games, there is only one difficult environment. We encourage further development of JAX-based environments in these settings to improve the quality of evaluation.

To compute aggregate performance statistics, we follow the recommendations of  and evaluate the inter-quartile mean across the different classes of environments. To do this, we recommend normalising performance of the algorithms on the relevant classes of environment (for example via looking at the maximum and minimum performance across algorithms as discussed in ), andcomputing a mean _per seed_. Then compute the inter-quartile mean across aggregated statistics in each environment. We provide code for performing this calculation. This allows environment classes to be compared fairly, without over-weighting those with more individual scenarios.

## 5 Results

To demonstrate the utility of our library, we evaluate PPO against Q-Learning algorithms on a range of cooperative environments. We discover that not only does it have improved performance, but also that it is more practical to use for end-to-end GPU training.

We then evaluate the speed of our library. We compare algorithm and environment runtimes with similar CPU-based environments. We find that when training PPO, JaxMARL is 31x quicker on SMAX when compared to training in SMAC and 14x quicker on MPE for a single run, and 12,500x for vectorized training runs. Finally, we verify the correctness of our implementations by performing thorough comparisons with prior work.

### Multi-Environment Comparison

We provide a preliminary comparison of our PPO and Q-Learning baselines in Figure 4. The IQM and mean were aggregated across 9 SMAX tasks, excluding the two maps with more than 10 units, all 5 Overcooked maps, and the 2 cooperative scenarios of MPE, running 10 seeds per task. We did not evaluate on Hanabi or all SMAX tasks because of the large memory overhead of storing the replay buffer for the Q-Learning methods on the GPU. We normalize the scores of each run on each task against the highest score obtained by any algorithm in that task, and then average the scores in each environment to avoid bias towards SMAX (which contributes more tasks).1

The aggregated IQM and Mean scores in Figure 4 show a clear advantage of PPO baselines over Q-Learning. Furthermore, in Table 2 we find that PPO is 6 times faster in SMAX and 10 times faster in Overcooked. We provide additional analysis of this in the Appendix.

### Speed Benchmarking

We compare the performance of our environments in steps per second when using random actions to the original environments in Table 3, with details of this test provided in the Appendix.

We next compare the speed of our training pipeline to that of PyMARL. As shown in Figure 4(a), a single Q-Learning training run for MPE's simple spread task takes 130 seconds with JaxMARL while PyMARL requires over an hour. Furthermore, using JAX we can parallelise over the entire

   Setting & Recommended Environments \\  CTDE & SMAX (all scenarios), Hanabi (2-5 players), Overcooked \\ Zero-shot Coordination & Hanabi (2 players), Overcooked (5 basic scenarios) \\ General-Sum & STORM (iterated prisoner’s dilemma), STORM (matching pennies) \\ Cooperative Continuous Actions & MABrax \\   

Table 1: Recommended minimal environment evaluation sets for different research settings

Figure 4: Normalised scores aggregated over SMAX, MPE and Overcooked. PPO shows a clear advantage.

training process within a single hardware accelerator. For QMIX on MPE, this allows us to complete 1024 individual training runs in 198.4 seconds, compared to 1 hour and 10 minutes for a single training run with PyMARL, a speed up of 21,500x per agent. This analysis is repeated for IPPO in the Appendix and we find a speedup of 12,500x. Figure Figure 4(c) demonstrates the speedup gained from using SMAX with JaxMARL's IPPO implementation compared to training on SMAC with PyMARL. Across a varying number of environment rollout threads, JaxMARL gives a speedup of up to 31x.

### Algorithm and Environment Correctness

In this section, we compare our environment and algorithm implementations with prior work and demonstrate equivalence where applicable.

**Overcooked** The transition dynamics of our Overcooked implementation match those of the Overcooked-AI implementation. We demonstrate this by training an IPPO policy on our implementation and evaluating the policy on both our implementation and the original at regular intervals. The performance is similar across the implementations. Results can be found in the Appendix.

**SMAX** SMAX and SMAC are different environments, as they have different opponent policies and dynamics. However, we demonstrate some similarity between them by comparing our IPPO and MAPPO implementations against MAPPO results on SMAC, using the implementation from . We show this figure, along with a more in-depth description of their differences, in the appendix.

   Environment & Original, 1 Env & Jax, 1 Env & Jax, 100 Envs & Jax, 10k Envs \\  MPE Simple Spread & \(8.3 10^{4}\) & \(5.5 10^{3}\) & \(5.2 10^{5}\) & \(4.0 10^{7}\) \\ Switch Riddle & \(2.7 10^{4}\) & \(6.2 10^{3}\) & \(7.9 10^{5}\) & \(6.7 10^{7}\) \\ Hanabi & \(2.1 10^{3}\) & \(1.4 10^{3}\) & \(1.1 10^{5}\) & \(5.0 10^{6}\) \\ Overcooked & \(1.9 10^{3}\) & \(3.6 10^{3}\) & \(3.0 10^{5}\) & \(1.7 10^{7}\) \\ MABrax Ant 4x2 & \(1.8 10^{3}\) & \(2.7 10^{2}\) & \(1.8 10^{4}\) & \(7.6 10^{5}\) \\ Starcraft 2s3z & \(8.3 10^{1}\) & \(5.4 10^{2}\) & \(4.5 10^{4}\) & \(2.7 10^{6}\) \\ Starcraft 27m vs 30m & \(2.7 10^{1}\) & \(1.5 10^{2}\) & \(1.1 10^{4}\) & \(1.9 10^{5}\) \\ STORM & – & \(2.5 10^{3}\) & \(1.8 10^{5}\) & \(1.5 10^{7}\) \\ Coin Game & \(2.0 10^{4}\) & \(4.7 10^{3}\) & \(4.1 10^{5}\) & \(4.0 10^{7}\) \\   

Table 3: Benchmark results for JAX-based MARL environments (steps-per-second) when taking random actions. All environments are significantly faster than existing CPU implementations.

Figure 5: JaxMARL speed benchmarking results. Figure 4(a) compares JaxMARL’s returns in MPE over wall clock time with PyMARL’s when using Q-Learning algorithms. Figure 4(b) demonstrates JaxMARL algorithms’ ability to train many seeds in parallel. The figure compares training time (on the x-axis) for a varying number of training runs (on the y-axis) training using QMIX on MPE. The red dotted represents the time taken to train a single agent with PyMARL. Figure 4(c) illustrates the speedup of a JaxMARL IPPO training run using SMAX compared to PyMARL using SMAC across a varying number of environment rollout threads.

We additionally present aggregate and detailed performance across SMAX in the Appendix. The PPO-based IPPO and MAPPO perform better than the Q-Learning methods, with the centralised information provided in the state helping MAPPO significantly outperform IPPO.

MpemOur MPE environment corresponds exactly to the PettingZoo implementation. We validate this for each environment using a uniform-random policy on \(1000\) rollouts, ensuring all observations and rewards are within a tolerance of \(1 10^{-4}\) at each transition. We additionally compare the results of our Q-Learning and PPO implementations with existing libraries, the results of which, along with the performance of IQL on the remaining MPE environments, can be found in the Appendix. We compare Q-Learning and PPO on Simple Spread in Figure 5(a).

MabraxAs Brax differs subtly from MuJoCo, MABrax does not correspond to MAMuJoCo but the learning dynamics are qualitatively similar. Results therefore are not directly comparable across the two environments. We report mean training return across 10 seeds for IPPO on halfcheetah_6x1 in Figure 5(b). We additionally report the training curves for IPPO on ant_4x2, hopper_3x1, walker2d_2x3 and humanoid_918 in the Appendix.

HanabiOur implementation matches the Hanabi Learning Environment. To verify the environment's accuracy, we obtained 10,000 action trajectories from the original C++ repository using their pretrained models. We confirmed that processing these action trajectories with JaxMARL produces the same states and returns as the C++ repository. In addition, we transferred the models trained with Pytorch/C++ to Jax and verified they obtain similar scores in JaxMARL. Finally, as shown in Figure 5(c), our IPPO and MAPPO models attain scores \(\) and \(\) respectively, which are better than (for IPPO) or similar to (for MAPPO) the scores attained in .

## 6 Related Work

MARL Libraries and AlgorithmsSeveral open-source libraries exist for both MARL algorithms and environments. The popular library PyMARL  provides PyTorch implementations of QMIX, VDN and IQL and integrates easily with SMAC. E-PyMARL  extends this by adding the actor-critic algorithms MADDPG , MAA2C , IA2C , and MAPPO, and supports SMAC, Gym , Robot Warehouse , Level-Based Foraging , and MPE environments. Recently released MARLLib  is instead based on the open-source RL library RLLib  and combines a wide range of competitive, cooperative and mixed environments with a broad set of baseline algorithms. Meanwhile, MALib  focuses on population-based MARL across a wide range of environments. However, none of these frameworks feature hardware-accelerated environments and thus lack the associated performance benefits.

Hardware-Accelerated and JAX-Based RLThere has also been a recent proliferation of hardware-accelerated and JAX-based RL environments. Isaac gym  provides a GPU-accelerated simulator for a range of robotics platforms and CuLE  is a CUDA reimplementation of the Atari Learning

Figure 6: Training Curves for a range of JaxMARL environments. Performance is aggregated across 10 seeds and error bars show standard error.

Environment . Both of these environments are GPU-specific and cannot be extended to other hardware accelerators. Madrona  is an extensible game-engine written in C++ that allows for GPU acceleration and parallelisation across environments. However, it requires environment code to be written in C++, limiting its accessibility. VMAS  provides a vectorized 2D physics engine written in PyTorch and a set of challenging multi-robot scenarios, including those from the MPE environment. For RL environments implemented in JAX, Jumanji  features mostly single-agent environments with a strong focus on combinatorial problems. The authors also provide an actor-critic baseline in addition to random actions. PGX  includes several board-game environments written in JAX. Gymnax  provides JAX implementations of the BSuite , classic continuous control, MinAtar  and other assorted environments. Gymnax's sister-library, gymnax-baselines, provides PPO and ES baselines. Further extensions to Gymnax  also include POPGym environments . Brax  reimplements the MuJoCo simulator in JAX and also provides a PPO implementation as a baseline. Jax-LOB  implements a vectorized limit order book as an RL environment that runs on the accelerator. Perhaps the most similar to our work is Mava , which provides a MAPPO baseline, as well as integration with the Robot Warehouse environment. None of these libraries combine a range of JAX-based MARL environments with both value-based and actor-critic baselines.

## 7 Conclusion

Hardware acceleration offers important opportunities for MARL research by lowering computational barriers, increasing the speed at which ideas can be iterated, and allowing for more thorough evaluation. We present JaxMARL, an open-source library of popular MARL environments and baseline algorithms implemented in JAX. We combine ease of use with hardware accelerator enabled efficiency to give significant speed-ups compared to traditional CPU-based implementations. Furthermore, by bringing together a wide range of MARL environments under one codebase, we have the potential to help alleviate issues with MARL's evaluation standards. We hope that JaxMARL will help advance MARL by enabling researchers to conduct research with thorough, fast, and effective evaluations.

Limitations and Future Work.While our work provides significant advancements, several limitations remain. First, we observe that the speedups are less pronounced for off-policy, value-based methods. Additionally, there are inherent challenges with end-to-end JAX implementations, such as the difficulty in efficiently handling environments with a variable number of agents or those with massive observation sizes. Furthermore, our MARL environments largely re-implement or draw inspiration from existing environment suites, meaning they do not yet push the boundaries of MARL capabilities. Developing novel MARL environments that push the boundaries of current capabilities could provide new and challenging benchmarks for the community.

Acknowledgements

This work received funding from the EPSRC Programme Grant "From Sensing to Collaboration" (EP/V000748/1). MG was partially founded by the FPI-UPC Santander Scholarship FPI-UPC_93. JF is partially funded by the UKI grant EP/Y028481/1 (originally selected for funding by the ERC). JF is also supported by the JPMC Research Award and the Amazon Research Award.