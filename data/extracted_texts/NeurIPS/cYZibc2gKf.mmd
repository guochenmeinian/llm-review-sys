# Abstract Reward Processes: Leveraging State Abstraction for Consistent Off-Policy Evaluation

Shreyas Chaudhari

University of Massachusetts

schaudhari@cs.umass.edu

&Ameet Deshpande

Princeton University

asd@cs.princeton.edu

Bruno Castro da Silva

University of Massachusetts

psilva@cs.umass.edu

&Philip S. Thomas

University of Massachusetts

pthomas@cs.umass.edu

###### Abstract

Evaluating policies using off-policy data is crucial for applying reinforcement learning to real-world problems such as healthcare and autonomous driving. Previous methods for _off-policy evaluation_ (OPE) generally suffer from high variance or irreducible bias, leading to unacceptably high prediction errors. In this work, we introduce STAR, a framework for OPE that encompasses a broad range of estimators--which include existing OPE methods as special cases--that achieve lower mean squared prediction errors. STAR leverages state abstraction to distill complex, potentially continuous problems into compact, discrete models which we call _abstract reward processes_ (ARPs). Predictions from ARPs estimated from off-policy data are provably consistent (asymptotically correct). Rather than proposing a specific estimator, we present a new framework for OPE and empirically demonstrate that estimators within STAR outperform existing methods. The best STAR estimator outperforms baselines in all twelve cases studied, and even the median STAR estimator surpasses the baselines in seven out of the twelve cases.

## 1 Introduction

Within _reinforcement learning_ (RL), _off-policy evaluation_ (OPE) is the foundational challenge of evaluating the performance, \(J()\), of policies \(\) that are different from the ones used to generate data. OPE methods are a general-purpose tool that can be used as part of a local policy search algorithm  to provide insight about policies similar to the current policy, or as a tool to evaluate policies without requiring their actual deployment for high-risk applications like those in healthcare , education , and recommendation systems . Despite many recent advances in OPE, existing methods struggle to give accurate predictions for many real-world applications , showing the need for new perspectives on OPE.

Existing methods can be broadly divided into two categories: _importance sampling_ (IS) based and model-based . IS-based methods are typically consistent (i.e., their predictions converge probabilistically to the correct value in the limit as the amount of data approaches infinity), but have variance that increases exponentially with the horizon . Model-based methods have lower variance but often introduce bias due to model class mismatch and are not generally guaranteed to be consistent . A third set of methods, which we call _mixture methods_, combine the predictions obtained from both of these categories . However, in some cases, combining the predictions also combines the drawbacks--high variance and bias. This leads us to ask: _Can we develop a framework for OPE that yields predictions that are both consistent and low variance?_In this paper, we introduce a new framework that attains this goal by combining the _machinery_ underlying IS-based and model-based approaches (not just their _predictions_). Our proposed framework is a fundamentally different approach to OPE that incorporates importance sampling _into_ model learning for OPE. Our approach is motivated by the intuition that humans build small mental models of their environment to plan and predict, selectively abstracting away information that is not relevant to the problem at hand . Similarly, complex sequential decision processes can be distilled into compact models that hold sufficient information for (off-)policy evaluation. Specifically, we propose creating small tabular models (even for continuous environments), which we call _abstract reward processes_ (ARPs), customized for the problem at hand and for the policy being evaluated. We call this framework for constructing a range of ARPs _state-abstract reward processes_ (STAR).

Idea Summary:A _Markov decision process_ (MDP) combined with a policy \(\) induces a Markov chain with rewards, called a _Markov reward process_ (MRP). A model of this process can be estimated to evaluate policy \(\). However, two main challenges arise: (1) like any model-based approach, estimating an MRP can introduce asymptotic bias if the chosen model class cannot represent the underlying MRP; and (2) the model of the MRP must be accurately estimated from _off-policy_ data, i.e., data generated by a behavior policy \(_{b}\) that differs from the evaluation policy \(_{e}\). The proposed framework addresses both challenges by modeling a special instantiation of an MRP, as detailed next.

First, to address potential model class mismatch, we propose mapping the large and possibly continuous set of states of the MDP to a finite set of _abstract states_ using a discrete state abstraction function. We then represent the resulting MRP defined over abstract states, referred to as the _abstract reward process_ (ARP), using tabular models. Since the ARP is finite, tabular models can represent it accurately, as depicted in Figure 1(a). However, using a discrete state abstraction may lead to loss of state information that can potentially introduce modeling errors. Surprisingly, we prove that despite state information being abstracted away, the maximum likelihood model of an ARP estimated from _on-policy_ data provides consistent estimates of the expected return of the behavior policy \(_{b}\).

Next, to estimate a model of the ARP corresponding to \(_{e}\) from data generated by \(_{b}\), we reweight occurrences of abstract states in the off-policy dataset using importance sampling, see Figure 1(b). In expectation, this has the effect of updating the abstract state visitation counts to reflect those resulting from the policy being evaluated. We prove that the weighted maximum likelihood estimate of a model of the ARP estimated from this off-policy dataset provides consistent estimates of the performance of the evaluation policy.The integration of importance sampling _into_ model estimation permits a favorable interpretation of weight clipping for mitigating variance (see Section 4.1).

The STAR framework offers two adjustable knobs--the state abstraction function, and the amount of weight clipping--that instantiate a range of OPE estimators. Varying the configurations of these knobs results in different bias-variance trade-offs for OPE, with existing OPE methods forming special cases of the range of estimators that lie within this framework.

Figure 1: **(a):** MDP \(M\) and policy \(_{b}\) are transformed into a discrete _abstract reward process_ (ARP) using a state abstraction function \(\). The ARP aggregates rewards (denoted by stars) and transition probabilities from all states that map to each abstract state. **(b):** A model of the ARP for the evaluation policy \(_{e}\) is constructed by: reweighting data generated by \(_{b}\) with importance weights \(\) (middle), applying the state abstraction function \(\), and performing weighted maximum likelihood estimation of the ARP (right). The expected return of a model of this ARP estimated from off-policy data is a _consistent_ estimator of the expected return of \(_{e}\).

Contributions:We empirically evaluate estimators instantiated in this framework on synthetic domains and a healthcare simulator built from real-world ICU data, where the best STAR estimator significantly outperforms baselines in all cases, and even the median STAR estimator surpasses baselines in seven out of twelve cases. It must be emphasized that this work does not propose a specific estimator for OPE; rather, it introduces a fundamentally different framework that offers fresh insights on approaches for off-policy evaluation. These insights open up exciting avenues for new research questions and future directions. In this paper, we introduce:

1. the first model-based approach for OPE that guarantees asympotic correctness of the estimates without model class assumptions, even for continuous state MDPs (Theorem 4.1).
2. the concept of _abstract reward processes_ for consistent OPE. ARPs abstract away the complexity of the underlying problem, and distill sufficient information for accurate policy evaluation (Theorem 3.1). Being finite, they can be consistently estimated.
3. a generalizing framework that provides a fresh perspective on OPE by merging the machinery of model-based and IS-based approaches. The framework offers two tunable knobs, various configurations of which instantiate a range of OPE estimators with varying bias-variance characteristics. Existing model-free and model-based methods are special cases in this framework.

## 2 Background and Notation

An MDP is a tuple \(M:=(,,p,r,,)\) where \(\) is the set of states, \(S_{t}\) is the state at time \(t\{0,1,\}\), \(\) is the set actions, \(A_{t}\) is the action at time \(t\), \(p:\) is the _transition function_ that characterizes state transition dynamics according to \(p(s,a,s^{}):=(S_{t+1}{=}s^{}|S_{t}{=}s,A_{t}{=}a)\), \(r:\) is the _reward function_ that characterizes rewards according to \(r(s,a):=[R_{t}|S_{t}{=}s,A_{t}{=}a]\), \(\) is the reward discount parameter, and \(:\) characterizes the initial state distribution according to \((s):=(S_{0}{=}s)\).1 A policy \(:\) characterizes how actions can be selected given the current state according to \((s,a):=(A_{t}{=}a|S_{t}{=}s)\). We consider finite horizon MDPs  where episodes terminate by some (unspecified) time \(T\)--which is common in practical applications of OPE. For simplicity, we set \(=1\), allowing us to omit \(\) terms.

For OPE, a dataset \(_{n}^{(_{b})}\) is collected by deploying a behavior policy \(_{b}\) on the MDP \(M\). The dataset of \(n\) logged trajectories is denoted by \(_{n}^{(_{b})}:=\{H^{i}\}_{i=1}^{n}\) where each \(H^{i}:=(S_{0}^{i},A_{0}^{i},R_{0}^{i},S_{1}^{i},)\) represents an independent trajectory generated by executing \(_{b}\). The performance of an evaluation policy \(_{e}\) is its expected return, denoted by2

\[J(_{e}):=[_{t=1}^{T}R_{t};_{e}].\] (1)

The problem of off-policy evaluation entails estimating \(J(_{e})\) with access only to data \(_{n}^{(_{b})}\), generated by a behavior policy \(_{b}\), without additional interaction with the MDP. To ensure that samples in \(_{n}^{(_{b})}\) are sufficiently informative, we make the common assumption that any outcome under \(_{e}\) has non-negligible probability of occurring under \(_{b}\).

**Assumption 2.1**.: There exists an (unknown) \(>0\) such that for all \(s\) and \(a\), \((_{b}(s,a)<)(_{e}(s,a)=0)\).

Background:For a detailed review of OPE methods, we refer the reader to surveys by Voloshin et al.  and Uehara et al. . Concepts fundamental to this approach are briefly introduced here.

1. **Importance Sampling:** Importance sampling  enables unbiased estimation of the expected value, \([f(X)]\), of a function \(f\) applied to a random variable \(X p\), given samples of a different random variable \(Y q\). The importance sampling estimator is \((p(Y)/q(Y))f(Y)\), where \(p(Y)/q(Y)\) is a term called an importance weight. This technique can provide unbiased estimates (i.e., \([(p(Y)/q(Y))f(Y)]=[f(X)]\)) and has proven effective for variance reduction in Monte Carlo sampling  and for model-free OPE in RL .
2. **State Abstraction:** State abstraction aims to reduce the size of the state space by grouping together similar states in a way that does not change the essence of the underlying problem [28; 43; 1]. A state abstraction function \(:\) lies in the set of functions \(\) that map each state \(s\) to an abstract state \(z\). We consider abstraction functions that partition \(\) into disjoint sets, where \(\) is a finite set.

Notation:Indicator functions are abbreviated for clarity. For example, \(_{t}^{i}\{z,z^{}\}:=\{(S_{t+1}^{(i)})=z^{},(S_{t}^{(i)})=z\}\) denotes the occurrence of abstract states \(z\) and \(z^{}\) at time steps \(t\) and \(t+1\) in the \(i^{}\) logged trajectory, with \(_{t}^{i}\{z\}\) defined correspondingly. The expected return of \(\) when obtained from \(O\)--where \(O\) may be the MDP, or an ARP--is denoted by \(J(;O)\). The sample estimate of a variable \(y\) estimated from \(n\) samples is denoted by \(_{n}\). Summation limits are often dropped for brevity, with \(_{t}\) denoting \(_{t=0}^{T}\) and \(_{i}\) denoting \(_{i=1}^{n}\).

### Markov Reward Processes

A _Markov reward process_ (MRP) extends the idea of a Markov chain by associating states with rewards. Formally, an MRP is a tuple \((,p,r,,)\) where \(\) is the set of states of the MRP, \(X_{t}\) is the state at time \(t\), \(p:\) is the transition function where \(p(x,x^{}):=(X_{t+1}{=}x^{}|X_{t}{=}x)\), \(r:\) is the reward function where \(r(x):=[R_{t}|X_{t}{=}x]\), \(\) is the discount factor, and \(:\) is the starting state distribution. We consider finite horizon MRPs where episodes terminate by some (unspecified) timestep and set \(=1\).

A specific MRP is induced by the use of a fixed policy \(\) on an MDP \(M\), where \(=\). The resulting transition and reward functions, denoted by \(p^{}\) and \(r^{}\) respectively, are:

\[p^{}(x,x^{})=(S_{t+1}=x^{},S_{t}=x;)}{ _{t}(S_{t}=x;)}, r^{}(x)=[R_{t}|S_ {t}=x;](S_{t}=x;)}{_{t}(S_{t}=x;)}.\] (2)

The Markov property  allows for further simplification of the above expressions (detailed in Appendix A.1), but this form is most conducive to our subsequent discussion. In this work, we focus on a specific instantiation of an MRP, described in the next section, where the set of states \(\) of the MRP are outputs of a state abstraction function \(\).

## 3 Abstract Reward Processes

An abstract reward process is a Markov reward process--derived from MDP \(M\) and policy \(\) and defined over _abstract_ states--that we use to evaluate \(\). The ARP provides two primary benefits for policy evaluation: (1) it preserves sufficient information to exactly evaluate the policy \(\), and (2) the ARP can be _consistently_ estimated from data. In this section, we formalize the concept of an ARP, and highlight the theoretical and practical benefits of using ARPs for policy evaluation.

Given a state abstraction function \(:\), the ARP \(^{}_{}\) is defined such that \(=\). Formally, \(^{}_{}\) is an MRP \((,^{}_{},^{}_{},_{})\), with \(=1\) (see Appendix A.1 for a discussion on termination in ARPs and MRPs). The components of the ARP are defined over _abstract_ states as:

\[^{}_{}(z,z^{}){:=}((S_{t+1}){=}z^{ },(S_{t}){=}z;)}{_{t}((S_{t}){=}z;)},^{ }_{}(z){:=}[R_{t}|(S_{t}){=}z;]((S_ {t}){=}z;)}{_{t}((S_{t}){=}z;)},\] (3)

and \(_{}(z):=((S_{0})=z)\). These expressions _cannot_ be simplified further, unlike the case of an MRP . Since \(\) is a finite set, i.e., the abstract states are discrete, the components of the ARP can be represented by matrices (we use uppercase letters to emphasize this). The expected return of \(^{}_{}\) can be computed efficiently using a linear solver to evaluate the expression \(J(;^{}_{}):=(^{}_{})^{-1}^{ }_{}_{}\), or via Monte Carlo rollouts of the reward process.

**ARPs are _Performance Preserving_:** The expected return of an ARP has a surprising property: even though some state information is abstracted away to create simple discrete abstract states, the finite ARP, derived from a possibly continuous and complex MDP, preserves sufficient information about the performance of the policy that defines the ARP _for all \(\)_.

**Theorem 3.1**.: \(\,\)_, the performance of a policy \(\) is equal to the expected return of the abstract reward process \(^{}_{}\) defined from MDP \(M\), i.e., \(J(;^{}_{})=J(;M)\)._

Proof.: See Appendix B.1. 

The result holds for the ground-truth ARP \(^{}_{}\). In practice, a model of the ARP must be estimated from data. Next, we describe how the choice of defining an ARP over discrete abstract states eliminates model class mismatch, enabling asymptotically correct estimation of the ARP from data.

**Eliminating Model Class Mismatch:** Methods that learn models from data make an assumption about the class of models used to represent the data. A significant challenge is that of _model class mismatch_, where this assumed model class is often unable to represent the true data distribution. As an example, a neural network parameterizing a univariate Gaussian distribution cannot accurately represent data generated from a bimodal distribution. In the context of this work, the transition function of an ARP may specify arbitrary probability distributions over discrete abstract states, necessitating a careful selection of the model class. _Tabular models_ are capable of representing _any_ distribution over discrete variables. Therefore, using tabular models when estimating an ARP from data ensures that there is no model class mismatch. This is why we employ state abstraction functions \(\) that partition the state space into a finite number of disjoint sets, or discrete abstract states. The abstraction functions may be viewed as: (a) a discrete clustering of the state space, or (b) a discretization of continuous states.

While this addresses model class mismatch, the use of a discrete state abstraction may itself be a source of modeling error. Mapping groups of (possibly continuous) states to discrete abstract states loses information about the state of the MDP. A process defined over the abstract states cannot in general capture the full complexity of the underlying MDP and policy. Nonetheless, Theorem 3.1 guarantees that the ARP is performance-preserving, ensuring that the use of discrete state abstractions is not a source of error for policy evaluation. Additionally, since we have eliminated model class mismatch, a perfect model of the ARP can be asymptotically estimated.

To estimate the ARP from \(^{(_{b})}_{n}\), apply the state abstraction function to states in \(^{()}_{n}\) to map them to the abstract state space. Denote the _maximum likelihood estimate_ of the model of the ARP obtained from the dataset (with abstract states) by \(}^{}_{n,}{:=}(,}^{ }_{n,},}^{}_{n,},_{n,})\). The components take the form:

\[}^{}_{n,}(z,z^{})=^ {i}_{t}\{z,z^{}\}}{_{i,t}^{i}_{t}\{z\}};}^{}_{n,}(z)=^{i}_{t}\{z\}R^{i}_{t }}{_{i,t}^{i}_{t}\{z\}};^{}_{n,}(z)= ^{n}^{i}\{z_{0}=z\}}{n}\] (4)

**Asymptotic Correctness:** With access to _on-policy data_\(^{()}_{n}\), the following result states that ARPs enable consistent model-based estimation of the policy's performance.

**Lemma 3.2**.: \(\)_, the expected return of the maximum likelihood estimate_\(}^{}_{n,}\)_converges almost surely to the expected return of the policy \(\), i.e., \(J(;}^{}_{n,})}{{ }}J(;M)\)._

Proof.: See Appendix B.2. 

As the amount of data (\(n\)) increases, the estimate of \(J(;M)\) becomes increasingly accurate, i.e., the return estimate is consistent. This result holds for all \(\). It implies that _even an arbitrarily small model derived from a large, complex sequential decision-making problem will not introduce asymptotic bias._ However, this theoretical guarantee requires on-policy data and so does not directly assist us in off-policy evaluation. To that end, we introduce a procedure for estimation of the ARP from _off-policy data_ that merges the machinery of IS-based and model-based methods.

### Estimation from Off-Policy Data: Weighted Maximum Likelihood Estimation

We present a method for consistent estimation of the ARP corresponding to the evaluation policy \(_{e}\) from off-policy data \(^{(_{b})}_{n}\). It relies on the following intuition:The expected value of the indicator function of an event represents the probability of that event.

Use importance sampling to approximate the probability of that event under a different distribution.

To estimate an ARP from off-policy data, assign importance weights \(_{0:t}\) to the abstract states \((Z_{t}:=(S_{t}))\) in the dataset \(_{n}^{(_{b})}\). Let \(H_{t}:=(S_{0},A_{0},R_{0},,S_{t-1},A_{t-1},R_{t-1},S_{t},A_{t})\) denote a sub-trajectory up to time \(t\). The importance weight \(_{0:t}\) is then the ratio of the probability of \(H_{t}\) under \(_{e}\) and \(_{b}\), i.e., \(_{0:t}:=;_{e})}{(H_{t};_{b})}=_{j=0}^{t}(S_{j},A_{j})}{_{e}(S_{j},A_{j})}\).3 The maximum likelihood estimate (MLE) of \(_{}^{_{e}}\) obtained from the weighted off-policy data is denoted by \(}_{n,}^{_{b}_{e}}:=(, }_{n,}^{_{b}_{e}},}_{n,}^{_{b}_{e}},}_{n,}^{_{b}_{e }})\), where \(}_{n,}^{_{b}_{e}}(z)=^{n} ^{i}\{z_{0}=z\}}{n}\) remains unchanged, and

\[}_{n,}^{_{b}_{e}}(z,z^{})=_{t}^{i}\{z,z^{}\}_{0:t}}{_{i,t}_{t}^{i}\{z \}_{0:t}},}_{n,}^{_{b}_{e}}(z)= _{t}^{i}\{z\}_{0:t}R_{t}^{i}}{_{i,t}_{t}^{i}\{z\}_{0:t}}.\] (5)

This estimation is a form of weighted maximum likelihood estimation . Including the importance ratios in the numerator and denominator of the estimated transition and reward functions of the ARP enables estimation from off-policy data generated by \(_{b}\). The estimated model of the ARP is consistent and, as shown next, allows for consistent off-policy evaluation.

**Lemma 3.3**.: _Under Assumption 2.1, the weighted maximum likelihood estimate \(}_{n,}^{_{b}_{e}}\) converges almost surely to the ground-truth ARP \(_{}^{_{e}}\), i.e., \(}_{n,}^{_{b}_{e}}}{{ }}_{}^{_{e}}\)._

Proof.: See Appendix B.3. 

## 4 Off-Policy Evaluation with ARPs

The expected return of \(}_{n,}^{_{b}_{e}}\) is a consistent estimate of the performance of policy \(_{e}\) since \(}_{n,}^{_{b}_{e}}\) is an asymptotically correct estimate of \(_{}^{_{e}}\), as per Lemma 3.3.

**Theorem 4.1**.: _The expected return of the ARP \(}_{n,}^{_{b}_{e}}\) (built from off-policy data) converges almost surely to the expected return of \(_{e}\), i.e., \(J(_{e};}_{n,}^{_{b}_{e}})}{{ }}J(_{e};M)\)._

Proof.: See Appendix B.4. 

To our knowledge, this is the _first_ instance of a model-based OPE method that comes with the theoretical guarantee of consistent performance estimates, even for continuous problems and without model class assumptions. So far, we have achieved one of the starting goals--that of _consistency_. However, the use of importance weights \(_{0:t}\) for weighted MLE is expected to introduce high variance. Next, we discuss methods to mitigate the variance of IS.

### Variance Reduction: Leveraging _Markovness_ of the State Abstraction

A common technique for mitigating the variance of IS-based methods for OPE is clipping the importance weights to the \(c\) most recent ratios [18; 3; 20], i.e., \(_{(t-c+1)^{+}:t}:=_{i=(t-c+1)^{+}}^{t}(S_{i},A_{i})}{_ {b}(S_{i},A_{i})}\), where \((t-c+1)^{+}{:=}(t-c+1,0)\). This is often a bad approximation for classical IS-based methods, as it implies that only the \(c\) most recent actions affect the reward distribution at any timestep, which rarely holds true in practice. In STAR, importance weights are incorporated into model estimation, resulting in a more reasonable implication of weight clipping.

By importance weighting the abstract-state occurrences as described in Equation (5), clipping importance weights, in this case, implies _that the \(c\) most recent abstract states are sufficient to determine the current abstract-state transition and reward distributions_. This allows actions from the distant past to influence the current reward, as _the effects of actions propagate through the abstract state transitions_, unlike in IS-based methods. This condition, that a recent history of abstract states is sufficient to predict the current abstract state transition distribution, often approximately holdsin practice as discussed in POMDP literature [30; 39]. While the approximation may introduce asymptotic bias in exchange for reduced variance, certain abstraction functions that satisfy specific conditions can incur no asymptotic bias.

Weight Clipping without Asymptotic Bias:Intuitively, the use of \(c\)-clipped importance weights, \(_{(t-c+1)^{+}:t}\), updates the estimated distribution of the previous \(c\) abstract states--as if under the evaluation policy--while leaving the ones before unchanged. \(c\)-clipping does not introduce asymptotic bias when the previous \(c\) abstract states form a sufficient statistic for predicting the current abstract state transition distribution. This notion of conditional independence from history given the recent past is referred to as the _Markovness_ of the abstraction function \(\). We posit that there exist abstraction functions that are \(c\)-th order Markov [51; 9].

**Definition 4.2** (\(c\)-th order Markov).: The abstraction function \(\) is \(c\)-th order Markov if \(((S_{t+1})|(S_{t}),,(S_{(t-c+1)^{+}});)\!=\!((S_ {t+1})|(S_{t}),,(S_{0});)\) for \(\{_{b},_{e}\}\).

Let \(}_{,c}^{_{b}_{e}}\) denote the ARP estimated using \(c\)-clipped importance weights, \(_{(t-c+1)^{+}:t}\), in place of \(_{0:t}\) in Equation (5).

**Theorem 4.3**.: _Given a \(c\)-th order Markov \(\), the expected return of the abstract reward process \(}_{,c}^{_{b}+_{e}}\) converges almost surely to the expected return of \(_{e}\), i.e., \(J(_{e};}_{,c}^{_{b}_{e}})}}{{}}J(_{e};M).\)_

Proof.: See Appendix B.5. 

Even when \(\) does not satisfy the above condition, weight clipping proves to be a practical approximation and results in low mean squared prediction error, as demonstrated empirically in Section 5. The steps for performing off-policy evaluation by estimating \(}_{,c}^{_{b}+_{e}}\) are highlighted in Algorithm 1.

### Fantastic \(\)'s and Where to Find Them

Discrete abstraction functions that are \(c\)-th order Markov with small values of \(c\) represent the most suitable abstractions for enabling asymptotically correct, low-variance off-policy evaluation using STAR. An automated approach to discovering such abstraction functions, however, remains elusive. In a manner reminiscent of the options framework , wherein one might consider the usefulness of options before having methods for constructing options automatically, this work emphasizes the remarkable effectiveness of state abstractions used in abstract reward processes for OPE. It motivates a research area akin to option discovery: _abstraction discovery for OPE_.

We expect the following factors to play an important role in the search for good abstraction functions: (a) state-visitation distributions of \(_{b}\) and \(_{e}\), determining the granularity of abstraction in different parts of the state set, and (b) the distribution shift in abstract state visitation induced by the two policies, determining the extent of weight clipping that can be applied. Both of these are affected by properties of the underlying MDP, in particular the transition function, and in our initial analyses, we observe varying effects of similar abstractions across different MDPs (Appendix C.2).

We observe that a simple approach of randomly initializing centroids and applying \(k\)-means clustering [34; 33], where each cluster denotes a discrete abstract state, results in abstractions that provide competitive OPE performance, often significantly outperforming existing methods. We call this naive clustering-based abstraction method CluSTAR, and use it for our experiments. In some cases, abstraction by aggregation of states can increase the difficulty of estimation of the transition function. For example, aggregation of two states with deterministic transitions--which can be estimated perfectly from a single observation of those transitions--creates stochastic transitions between abstract states. However, in general, state aggregation tends to simplify estimation by increasing the effective sample size .

Recovering Existing OPE Methods from STAR:Different configurations of \((,c)\) induce different ARPs, \(}_{,c}^{_{b}_{e}}\). For certain configurations of \((,c)\):

* \(||=1\) and no weight clipping: Mapping all states to a single abstract state yields the _weighted per-decision importance sampling_ (WPDIS) estimator .
* \(=\) and \(c=1\): Amounts to no state abstraction, and yields the maximum likelihood estimate of the MRP over states. The MRP is a combination of the approximate-model estimator  that directly estimates the model dynamics with the evaluation policy.

[MISSING_PAGE_FAIL:8]

STAR achieve prediction errors that are an order of magnitude lower than the best baseline. These results in Figure 3 emphasize that _highly compact ARPs that distill complex sequential processes are particularly effective for off-policy evaluation_. The best ARP in STAR for each domain abstracts: (a) the continuous state space of CartPole to \(||=32\) abstract states, (b) the 747 states of ICU-Sepsis to \(||=16\) abstract states, and (c) the 400 states of Asterix to \(||=8\) abstract states, to construct compact finite ARPs for OPE. Furthermore, the horizon length of CartPole is 50, and episodes in ICU-Sepsis and Asterix go up to 120 and 75 timesteps respectively. Such relatively long horizons have proven to be challenging for prior OPE methods. STAR leverages ARPs to enable OPE at scales commonly seen in practice.

## 6 Related Work

The problem of off-policy evaluation (OPE) has been extensively studied due to its relevance for practical applications of reinforcement learning [38; 35]. Extensive surveys on the topic, both theoretical  and empirical [58; 14] delineate the numerous approaches to the problem. Model-based approaches for OPE have proven effective [32; 62] but are restricted by the model class used. In this work, we use state abstraction to define compact models called abstract reward processes, and demonstrate their effectiveness as off-policy estimators. Historically, state abstraction research has focused on grouping similar states in a way that does not change the essence of the underlying problem [28; 43; 1], to reduce the complexity of the problem. However, the use of state abstractions for OPE remains under-explored. Pavse and Hanna  show that the use of state abstraction with marginalized importance sampling achieves variance reduction in high-dimensional state spaces, but their approach does not use abstraction to construct models. Jiang et al.  study abstraction selection for model-based RL, balancing model complexity and policy value suboptimality. Abstraction discovery or

Figure 3: Mean squared prediction errors of best and median ARPs from STAR compared against existing OPE methods. The empirically estimated bias-variance decomposition of the error is shown. The results are averaged over 200 trials, with error bars indicating standard error. Note: For ICU-Sepsis, regression-based methods (MRDR and Q-Reg) were computationally intractable due to the large state set, as the corresponding Weighted Least Squares methods for regression were too slow. In all domains and across all datasizes, the best ARP in STAR outperforms baselines in all cases, and the even the median estimator does so in 7 out of 12 cases.

learning has focussed on discretization of continuous state spaces to reduce problem complexity , and distilling the Markov features  or reward relevant features .

## 7 Discussion and Conclusion

In this paper, we have introduced a new framework for consistent model-based off-policy evaluation. This framework leverages state abstraction to prevent model class mismatch along with importance sampling to consistently learn models from off-policy data. Unlike traditional model-based methods, our approach eliminates the need for model class assumptions and provides theoretical guarantees for the obtained performance estimates. Moreover, using state abstraction increases the effective sample size , which is particularly beneficial in limited data regimes. Importantly, this work presents a framework with a new approach to OPE, rather than a specific new method. Estimators that lie within this framework significantly outperform existing OPE methods, with the best estimator consistently outperforming all baselines, as demonstrated in our empirical evaluation.

The framework has two main limitations: it requires knowledge of the probabilities of observed actions under the behavior policy, which may not always be available, and a principled method for selecting well-performing configurations of the abstraction function and the weight clipping factor remain elusive. Combining this work with regression IS  would be a practical extension that addresses the first limitation. Additionally, a data-driven approach to automated estimator selection based on characteristics of the domain, dataset sizes, and other factors, as suggested by Su et al. , would enhance its practical application.

Our findings indicate that even a simple class of abstraction functions can provide competitive OPE performance. We theoretically demonstrate the existence of certain abstraction functions that may offer better performance. Investigating the properties of abstraction functions and developing automated approaches to _abstraction discovery_ for ARPs are promising directions for future work on creating high-performing OPE methods.

AcknowledgementsWe thank Yash Chandak, Mohammad Ghavamzadeh and Dhawal Gupta for their helpful discussions and feedback on this work. We would also like to thank Josiah Hanna, Bo Liu, Cameron Allen, and the anonymous reviewers for their feedback.