# Unveiling the Hidden:

Online Vectorized HD Map Construction with

Clip-Level Token Interaction and Propagation

 Nayeon Kim1  Hongje Seong2  Daehyun Ji  Sujin Jang2

Samsung Advanced Institute of Technology (SAIT)

{nayeon.kim, hongje.seong, derek.ji, s.steve.jang}@samsung.com

Equal contribution.Corresponding author.

###### Abstract

Predicting and constructing road geometric information (_e.g._, lane lines, road markers) is a crucial task for safe autonomous driving, while such static map elements can be repeatedly occluded by various dynamic objects on the road. Recent studies have shown significantly improved vectorized high-definition (HD) map construction performance, but there has been insufficient investigation of temporal information across adjacent input frames (_i.e._, clips), which may lead to inconsistent and suboptimal prediction results. To tackle this, we introduce a novel paradigm of clip-level vectorized HD map construction, _MapUnveiler_, which explicitly unveils the occluded map elements within a clip input by relating dense image representations with efficient clip tokens. Additionally, MapUnveiler associates inter-clip information through clip token propagation, effectively utilizing long-term temporal map information. MapUnveiler runs efficiently with the proposed clip-level pipeline by avoiding redundant computation with temporal stride while building a global map relationship. Our extensive experiments demonstrate that MapUnveiler achieves state-of-the-art performance on both the nuScenes and Argoverse2 benchmark datasets. We also showcase that MapUnveiler significantly outperforms state-of-the-art approaches in a challenging setting, achieving +10.7% mAP improvement in heavily occluded driving road scenes. The project page can be found at https://mapunveiler.github.io.

## 1 Introduction

Vectorized HD map construction (_VHC_) is a task of predicting instance-wise vectorized representations of map elements (_e.g._, pedestrian crossings, lane dividers, road boundaries). Such static map elements are crucial information for self-driving vehicles, including applications like lane keeping [1; 6], path planning [29; 24; 15], and trajectory prediction [30; 40; 11]. Prior approaches to constructing dense and high-quality HD maps typically rely on SLAM-based offline methods (_e.g._, [48; 37; 38]). Such an offline method generally involves a series of steps including feature extraction and selection (_e.g._, edge, plane), odometry estimation via feature matching, and mapping. However, these processes involve complicated and computationally burdensome tasks, limiting their use to offline applications.

More recently, camera-based multi-view VHC has been actively investigated as a cost-efficient and real-time alternative to existing expensive offline approaches. Current works on camera-based VHC typically aim to extract unified 3D Bird's Eye View (_BEV_) features that cover the surrounding environment of the ego-vehicle [20; 22; 23], relying on various Perspective View (PV) to BEV transformation methods [50; 32; 21; 5]. Subsequently, a task-specific head follows to decode andpredict map elements from the extracted BEV features. Despite significant progress, prior works still suffer from frequently occluded map elements caused by dynamic foreground objects such as vehicles and pedestrians, as described in Fig. 1-(a). Moreover, the prediction performance degrades when applied to a larger perception range. To address such issues, prior works try to leverage temporal information extracted from a stream of preceding feature frames [46; 41]. While such approaches have achieved improved _online_ prediction performance, they still do not fully leverage the potential of temporal information across a longer range of frames. In particular, prior works do not consider the cumulative impacts of occluded map elements, leading to noisy BEV feature generation, as shown in Fig. 1-(b) and Tab. 3. Moreover, such cumulative flaws can ultimately lead to degraded performance in longer perception ranges (see Tab. 1), which is crucial for safety-critical autonomous driving. On the other hand, the key idea of conventional SLAM-based offline methods is to stack and associate static features collected over a longer window of frames (_i.e._, mapping features). This _offline_ mapping strategy effectively addresses occlusion issues by leveraging diverse views of the map elements across multiple frames, while it requires manual human annotation and complex pipelines.

Based on these insights, we introduce a novel clip-level construction framework, _MapUnveiler_, which incorporates the effective offline mapping strategy into state-of-the-art online VHC approaches. In contrast to the direct _global feature mapping_ used in offline methods, our MapUnveiler collects _clip-level_ temporal map information and learns differentiable associations for efficient online inference. We generate compact clip tokens consisting of temporal map information within a clip input and update BEV features with these tokens to unveil hidden map elements that are visible in certain frames, as shown in Fig. 1-(c). Subsequently, we transfer the inter-clip tokens to the next clip's BEV features to facilitate establishment the long-term intra-clip associations among map elements.

As a result, MapUnveiler outperforms state-of-the-art methods on two widely recognized benchmarks: +1.3% mAP on nuScenes  and +0.9% mAP on Argoverse2 . We also observe a marginal increase in computational burden, as we exploit the benefits of the clip-level inference strategy, which allows an efficient inference of multi-frame inputs. In summary, our contributions include:

* We propose MapUnveiler, an online VHC model that incorporates the offline mapping strategy and unveils the hidden maps by interacting multiple dense BEV features with compact tokens.
* We introduce the _clip-level_ pipeline to infer MapUnveiler online by mapping within a clip set of BEV features and propagating the map information to subsequent timestamps, thereby building a global map efficiently.
* Our method significantly improves a frame-level model on longer perception range settings (58.6% \(\) 68.7%) and heavy occlusion splits (53.1% \(\) 63.8%), and achieves state-of-the-art performance on two standard VHC benchmarks with marginal extra computations (15.6 FPS \(\) 12.7 FPS).

Figure 1: (a) Existing approaches relying on single-frame inference cannot capture the entire map information in the BEV features [26; 22; 23]. (b) Recent alternatives explore temporal information via streaming, but they cannot address the inherent nature of maps and propagate noise from previous timestamps. (c) We directly unveil hidden maps in BEV features by interacting with clip tokens that contain high-level map information. We visualize BEV features by 1D PCA projection. The BEV features are extracted from (a) MapTRv2 , (b) StreamMapNet , and (c) our MapUnveiler.

## 2 Related Work

Multi-View HD Map Construction.SLAM (Simultaneous Localization and Mapping)  has been a central technique for constructing accurate HD maps [48; 37; 38]. However, these methods require memory-intensive, complex pipelines for global mapping of geometric features, and are therefore typically executed offline. Recently, deep learning approaches have emerged as an appealing alternative to those expensive offline methods, enabling online HD map construction using cost-efficient multi-view camera sensors. The perspective-view (PV) to bird's-eye-view (BEV) transformation methods [50; 32; 21; 5] enable the generation of 3D features from the surrounding environment of the ego-vehicles using camera sensors, even in the absence of precise spatial cues. BEVFormer  utilizes the deformable attention mechanism  to extract BEV features and predict rasterized semantic maps. However, it cannot generate instance-wise representation of map elements. To address this, HDMapNet  introduces a heuristic method to group pixel-level semantic maps into a vectorized representation. Similarly, VectorMapNet  proposes an end-to-end learning approach to predicting vectorized map representations. Although such methods have demonstrated notable prediction performance in single-frame inference, they do not consider the temporal information from multi-frame inputs. More recently, StreamMapNet  and SQD-MapNet  have proposed a streaming feature paradigm , which aims to leverage temporal information for improved temporal consistency across predictions. However, these methods propagate dense BEV features directly, incorporating map information from previous frames that may have been occluded and undetected, resulting in the accumulation of noise, as shown in Fig. 1-(b). To address this issue, we propose an end-to-end clip token learning approach that combines offline mapping techniques with online strategies, aiming for high performance and computational efficiency.

Temporal Token Learning.With the rapid development of transformers , there has been significant interest in efficient token learning alongside dense features, _e.g._, CNN representations. In particular, temporal token learning has emerged as an attractive alternative to memory-intensive spatio-temporal dense CNN representations [45; 31]. VisTR  extends DETR  into the 3D domain to extract spatio-temporal instance tokens that can be directly used for instance segmentation. IFC  proposes an efficient spatio-temporal token communication method, which replaces the heavy interactions within dense CNN features. VITA  learns efficient video tokens from frame-level instance tokens without dense CNN features. Cutie  updates CNN representations with tokens to avoid spatio-temporal dense matching. TTM  introduces an efficient long-term memory mechanism by summarizing tokens into memory rather than stacking [2; 18; 34] or recurrence [14; 8]. While all the aforementioned approaches were designed to handle foreground instances, we discover the potential of token learning to construct background maps. By learning compact tokens and interacting with dense BEV features, we impose traditional mapping into online VHC model and enable online running.

Figure 2: Our framework takes clip-level multi-view images and outputs clip-level vectorized HD maps. All components in the frame-level MapNet (_i.e._, Backbone, PV to BEV, Map Decoder) are adopted from MapTRv2 . The frame-level MapNet extracts map queries and BEV features independently at each frame. MapUnveiler generates compact clip tokens that contain clip-level temporal map information and directly interact with dense BEV features. With the updated BEV features, we construct high-quality clip-level vectorized maps. The generated map tokens and clip tokens are then written to memory.

## 3 Method

### Overview

We present the overall architecture of MapUnveiler in Fig. 2. Given a set of synchronized multi-view images (_i.e_., clip inputs), our model sequentially construct clip-level vectorized HD maps. We first extract frame-level BEV features and map queries, which are then used as inputs to MapUnveiler module. We employ memory tokens, which are written from the previous clip and facilitate the establishment of long-term temporal relationships. From the memory tokens and map queries, we generate clip tokens that embed temporal map element cues in a compact feature space. This is the first step to understand clip-level map information. We then update BEV features with clip tokens, which is the core step of unveiling hidden maps. Using the updated (unveiled) BEV features, we extract map tokens and construct clip-level vectorized HD maps. After a clip-level inference, we generate new memory tokens using clip tokens, map tokens, and the current memory tokens. The new memory tokens are used for providing temporal cues for the subsequent clip-level inference. Since we opt for a clip-level pipeline, MapUnveiler efficiently infers with a temporal stride \(S\), performing clip-level inference only \(N_{T}/S\) times for a sequence of \(N_{T}\) frames. In the following subsections, we detail each module in the proposed framework.

### Frame-level MapNet

We adopt MapTRv2  as our frame-level MapNet architecture to extract a clip set of BEV features and map queries from synchronized multi-view images. We extract perspective view (PV) image features using a backbone network, then these PV image features are transformed into BEV features through a PV-to-BEV module. Following the setup of MapTRv2, we adopt ResNet50  and Lift, splat, shoot (LSS) -based BEV feature pooling  for our backbone and PV-to-BEV module, respectively. These BEV features are utilized for querying maps in the map decoder. With their BEV features, the map decoder outputs frame-level map queries which can be directly used for constructing vectorized HD maps. Finally, the frame-level MapNet outputs BEV features and map queries, which are the results from the PV-to-BEV module and the map decoder, respectively. BEV features represent rasterized map features, whereas map queries embed vectorized map information; thus, we can directly construct vectorized HD maps using the map queries.

### MapUnveiler

MapUnveiler is a novel framework designed to unveil invisible map information that cannot be captured by frame-level BEV features alone. To avoid heavy computations, we adopted a clip-level inference scheme with temporal window (clip length) \(T\) and stride \(S\). A detailed explanation of the inference scheme with the temporal window \(T\) and stride \(S\) is provided in Appendix (see Sec. A.1). Our MapUnveiler consists of two main components: (1) Intra-clip Unveiler and (2) Inter-clip Unveiler. For each clip-level pipeline, our Intra-clip Unveiler generates vectorized maps for \(T\) frames. The Inter-clip Unveiler then writes memory tokens with the tokens generated in Intra-clip Unveiler to build global relationships.

Figure 3: A detailed implementation of Intra-clip Unveiler. We use blue, red, and green arrows to indicate the flows of map tokens, BEV features, and clip tokens, respectively. In each attention and feed forward layer, standard layer normalization, dropout, and residual connections are followed.

#### 3.3.1 Intra-clip Unveiler

Intra-clip Unveiler is composed of a sequence of \(L\) layers. It initially takes a clip set of frame-level map queries \(Q^{map}\), BEV features \(F^{BEV}\), and memory read \(U^{Read}\) (read at Inter-clip Unveiler, detailed in Sec. 3.3.2). In the first step, compact clip tokens are created by the clip token generator. The BEV updater then unveils hidden maps in the BEV features with the clip tokens. Finally, map generator outputs clip-level map tokens from the updated BEV features. The map tokens are directly used for constructing vectorized HD maps with perception heads. We illustrate the Intra-clip Unveiler in Fig. 3. In the followings, we describe the detailed implementation of each module.

Clip Token Generator.Clip token generator yields clip tokens \(U^{clip}_{l}~{}^{N_{e} C}\) from frame-level map queries \(Q^{map}~{}~{}^{T N_{i} N_{p} C}\), where \(N_{c}\), \(N_{i}\), and \(N_{p}\) denote the clip token size, number of predicted map element, and number of points per map element, respectively. To globally gather intra-clip map features, we opt for a naive cross-attention . Through this step, we obtain compact clip-level map representations, enabling efficient intra-clip communication with small-scale features.

BEV Updater.The second step is the BEV Updater, which updates bev features \(F^{BEV}~{}~{}^{T H W C}\) with the clip tokens \(U^{clip}_{l}\) to unveil the hidden map element information. In cross-attention, query is derived from the bev features \(F^{BEV}\), and the key and value are derived from the clip token \(U^{clip}_{l}\). The output of this step is robustly updated bev features \(U^{BEV}_{l}~{}^{T H W C}\) enhanced via clip tokens for hidden areas relative to the original bev features. The main idea of BEV Updater is to avoid heavy computation in spatio-temporal cross attention. To achieve this, we do not directly communicate intra-clip BEV features, but instead decouple the spatial BEV features and the temporal clip tokens. We then update the spatial BEV features with compact temporal clip tokens, effectively communicating spatio-temporal information with reasonable computational costs. The updated bev features \(U^{BEV}_{l}\) are used as value features in the next step.

Map Generator.The last step is the Map Generator, which generates map tokens \(U^{map}_{l}^{T N_{i} N_{p} C}\) using the updated BEV features \(U^{BEV}_{l}\) created in the previous step. The objective of this step is to generate a refined version of frame-level map queries. As illustrated in Fig. 3, the map generator uses deformable attention  and decoupled self-attention  mechanisms, following . In deformable attention, query is derived from the map queries \(Q^{map}\), and the value is derived from the updated BEV features \(U^{BEV}_{l}\). Since the updated BEV features are spatio-temporally communicated, we directly extract map tokens. Each map token represents a vectorized map element through a 2-layer Multi-Layer Perceptron (MLP). The map tokens \(U^{map}_{l}\) are written to the memory of the Inter-clip Unveiler, and when the map tokens \(U^{map}_{l}\) of the \(L\)-th layer pass through the prediction head, vectorized maps are generated.

#### 3.3.2 Inter-clip Unveiler

Inter-clip Unveiler propagates the tokens from previous clip input to the next one, thereby preserving the dense temporal information from the prior frames. As shown in Fig. 2, Inter-clip Unveiler writes map tokens \(U^{map}_{l}\) and clip tokens \(U^{clip}_{l}\) from the Intra-clip Unveiler to the memory. Here, we adopt token turning machine (TTM)  to efficiently manage the long-term map information. In the followings, we describe the detailed implementation of read and write.

Read.We generate compact tokens that contain a global map information by reading from memory tokens and map queries. Following TTM , we read with the token summarizer  which efficiently selects informative tokens from inputs as follows:

\[U^{read}=Read(U^{memory}_{t-2S:t-S},Q^{map})=S_{N_{c}}([U^{memory}_{t-2S:t-S }||Q^{map}]),\] (1)

where \([U^{memory}_{t-2S:t-S}||Q^{map}]\) denotes the concatenation of two elements, and \(U^{memory}_{t-2S:t-S}\) denotes memory tokens for a clip. We employ the location-based memory addressing used in  utilizing the positional embedding (detailed in Section 3.3.3). Note that the memory is not available in the first clip-level pipeline. Therefore, we initially write the memory token from learnable clip embeddings.

Write.We employ the write operation with the same token summarizer  that is used in . The new memory \(U^{memory}_{t-S:t}~{}~{}^{M C}\) is generated by summarizing the clip tokens \(U^{clip}_{L}\), maptokens \(U_{L}^{map}\), and old memory \(U_{t-2:st-S}^{memory}\) as follows:

\[U_{t-S:t}^{memory}=Write(U_{L}^{clip},U_{L}^{map},U_{t-2:St-S}^{memory})=S_{M}([U_ {L}^{clip}||U_{L}^{map}||U_{t-2:St-S}^{memory}]),\] (2)

where \(M\) denotes the memory token size. The newly generated memory through the write operation is used in the read operation of the first layer of the Intra-clip Unveiler in the subsequential per-clip process. If the tokens within the memory are not re-selected in the subsequent steps, it will be removed from the memory, and the selection mechanism will be determined through the learning. We employ the same memory addressing method used in the read operation. The write operation is applied in the last layer of the Intra-clip Unveiler, generating new memory that preserves the information of the clip tokens and map tokens.

#### 3.3.3 Positional Embedding

While the standard transformer structure is permutation-invariant, we require position information added with temporal information to predict map elements at the clip-level. For the BEV features (\(P_{B}\)), we use a fixed 3D sinusoidal positional embedding, following . For the map tokens (\(P_{M}\)), we use learnable positional embeddings used in frame-level MapNet  with newly defined learnable temporal positional embeddings. For the clip tokens (\(P_{C}\)), we define new learnable positional embeddings. Similarly, learnable positional embeddings are defined for the memory tokens that are used in read and write of the Inter-clip Unveiler.

#### 3.3.4 Loss

Since our model is built on top of the frame-level MapNet (MapTRv2 ), we basically follow the loss functions used in MapTR  and MapTRv2 . Specifically, we employ the overall loss functions as

\[_{MapUnveiler}=_{c}^{M}_{cls}^{M}+_{p}^{M} _{p2p}^{M}+_{d}^{M}_{dir}^{M}+_{s}^{M} _{PVSeg}^{M},\] (3)

\[_{Frame\_MapNet}=_{one2one}+_{one2many}+ _{dense},\] (4)

\[_{one2one}=_{c}^{F}_{cls}^{F}+_{p}^{F} _{p2p}^{F}+_{d}^{F}_{dir}^{F},\] (5)

\[_{dense}=_{t}^{F}_{depth}+_{b}^{F} _{BEVSeg}+_{s}^{F}_{PVSeg},\] (6)

where \(_{MapUnveiler}\) and \(_{Frame\_MapNet}\) indicate the loss functions for training MapUnveiler and frame-level mapnet, respectively. \(_{cls}\), \(_{p2p}\), \(_{dir}\), and \(_{PVSeg}\) denote classification loss , point-to-point loss , edge direction loss , and PV segmentation loss , respectively. \(_{one2one}\), \(_{one2many}\), and \(_{dense}\) are used for training frame-level MapNet and denote one-to-one loss , one-to-many loss , and auxiliary dense prediction loss , respectively. \(L_{depth}\) and \(_{BEVSeg}\) denote depth prediction loss  and BEV segmentation loss , respectively. We set hyperparameters to \(_{c}^{M}=2\), \(_{p}^{M}=5\), \(_{d}^{M}=0.005\), \(_{s}^{M}=2\), \(_{c}^{F}=2\), \(_{p}^{F}=5\), \(_{d}^{F}=0.005\), \(_{t}^{F}=3\), \(_{b}^{F}=1\), and \(_{s}^{F}=2\). Note that we did not use one2many loss  to save GPU memory during the main training.

## 4 Experiments

### Dataset and Metric

We construct experiments on two standard VHC benchmarks: nuScenes  and Argoverse2  datasets. nuScenes offers synchronized six-view images with high-quality HD maps. It provides 1,000 scenes and each scene consists of about 20 seconds. In this dataset, we follow the official scene split of 700, 150, and 150 for training, validation, and testing, respectively. Similarly Argoverse2 provides synchronized seven-view images with high-quality HD maps. It contains 1,000 scenarios and each scenario consists of about 15 seconds. Argoverse2 dataset provides the official scene split of 700, 150, and 150 for training, validation, and testing, respectively, which we follow.

For each dataset, we construct maps in two perception ranges: a standard range of 60\(\)30\(m\) and a longer range of 100\(\)50\(m\). Additionally, we create challenging validation splits to demonstrate the efficacy of our model under heavy occlusions. Specifically, we collect the occluded frames if any dynamic objects exists within 2.5\(m\) around the ego vehicle. Thankfully, nuScenes provides 3D cuboid annotations for vehicles and pedestrians, allowing us to automatically create the new challenging split, and the result with this split is given in Sec. 4.4.

For fair comparisons with state-of-the-art VHC methods, we follow the standard metric [20; 26] of average precision (AP) under several Chamfer thresholds \(\{0.5m,1.0m,1.5m\}\). We report the average AP in three Chamfer thresholds for each semantic map categories of pedestrian crossing, divider, and boundary. We average AP at three Chamfer thresholds and report the results for each semantic map category: pedestrian crossing, divider, and boundary. To validate the scalability in map categories, we further examine our model by learning semantic centerline, and the results are given in Appendix (see Sec. A.2).

### Implementation Details

Our model is trained with 8 NVIDIA A100 GPUs using batch size of 16. We train MapUnveiler using AdamW  with a learning rate of \(6 10^{-4}\) and a weight decay of 0.01. We follow the standard setup [22; 23] that training models for the total epochs of 24 with an image resolution of \(800 450\) and the total epochs of 6 with an image resolution of \(614 614\) on nuScenes and Argoverse2 datasets, respectively. The number of the embedding dimension \(C\), memory tokens \(M\), and clip tokens \(N_{c}\), map elements \(N_{i}\), points per map element \(N_{p}\) are set to 256, 96, 50, 50, and 20 respectively. We use temporal window size \(T\) and stride \(S\) of 3 and 2, respectively. The spatial size of the BEV feature is \(100 200\). We pre-train the frame-level MapNet on the standard setting and then main train our model to facilitate the exploitation of the frame-level map information. For fair comparisons, MapUnveiler is pre-trained for 12 and 3 epochs, and then main trained for an additional 12 and 3 epochs, resulting in a total of 24 and 6 epochs on nuScenes and Argoverse2 datasets, respectively. During main training, we perform clip-level inference three times and compute losses after each clip-level inference to effectively handle the GPU memory overhead.

### Comparisons

We compare our MapUnveiler against state-of-the-art VHC methods trained with standard settings, which use ResNet50  as a backbone, multi-camera modality, and train for 24 epochs and 6 epochs on nuScenes  and Argoverse2  respectively. As shown in Tab. 1, MapUnveiler achieves state-of-the-art performance on all validation sets. Specifically, we surpass the state-of-the-art temporal model (SQD-MapNet ) by mAP of 4.1% and 4.7% on two range settings of nuScenes validation set and 7.2% and 6.1% on Argoverse2. We also outperform a heavy VHC model (HiMap ) by 1.3% and 0.9% on two benchmark sets. Notably, we boost frame-level model (MapTRv2 ) by 6.5% and 10.1% on nuScenes and 3.1% and 4.8% on Argoverse2. The superiority of our approach is particularly evident in the long-range setting: mAP on 60\(\)30\(m\)\(\)100\(\)50\(m\) settings are 61.5%\(\)58.6% (-2.9%)

    &  &  &  \\  & & Epoch & AP\({}_{P}\) & AP\({}_{d}\) & AP\({}_{b}\) & mAP & FPS & Epoch & AP\({}_{p}\) & AP\({}_{d}\) & AP\({}_{b}\) & mAP \\   \(\) \\ \(\) \\ \(\) \\  } & MapTR[(10; 23)] & 24 & 46.3 & 51.5 & 53.1 & 50.3 & 16.7* & 6 & 54.7 & 58.1 & 56.7 & 56.5 \\  & MapTR[(10; 23)] & 24 & 47.7 & 54.4 & 51.4 & 51.2 & 16.7* & - & 54.6 & 60.0 & 58.0 & 57.5 \\  & GeMap[(10; 24)] & 24 & 49.2 & 53.6 & 54.8 & 52.6 & 13.2* & - & - & - & - & - \\  & PiotNet[(10; 23)] & 9 & 24 & 56.2 & 56.5 & 60.1 & 57.6 & 11.1* & - & - & - & - & - \\  & BeMapNet((10; 23)] & 30 & 57.7 & 62.3 & 59.4 & 59.8 & 9.7* & - & - & - & - & - \\  & MapTRv2[(10; 24)] & 24 & 59.8 & 62.4 & 62.4 & 61.5 & 15.6 & 6 & 62.9 & 72.1 & 67.1 & 67.4 \\  & StreamMapNet[(10; 24)] & 24 & - & - & - & 62.9 & 12.5* & 6 & 62.0 & 59.5 & 63.0 & 61.5 \\  & SQD-MapNet[(10; 24)] & 24 & 63.0 & 62.5 & 63.3 & 63.9 & - & 6 & 64.9 & 60.2 & 64.9 & 63.3 \\  & MGMap((10; 25)] & 24 & 61.8 & 65.0 & 67.5 & 64.8 & 12.3* & - & - & - & - & - \\  & MapR[(10; 24)] & 27 & 24 & 63.4 & 68.0 & 67.7 & 66.4 & 14.2* & 6 & 64.3 & 72.3 & 68.1 & 68.2 \\  & HIMap(10; 25)[(11)] & 30 & 62.6 & **68.4** & **69.1** & 66.7 & 9.7* & 6 & **69.0** & 69.5 & **70.3** & 69.6 \\  & MapUnveiler (ours) & 24 & **67.6** & 67.6 & 68.8 & **68.0** & 12.7 & 6 & 68.9 & **73.7** & 68.9 & **70.5** \\   \(\) \\ \(\) \\ \(\) \\  } & MapTR[(10; 23)] & 24 & 45.5 & 47.1 & 43.9 & 45.5 & 16.7* & 6 & - & - & - & 47.5 \\  & MapTRv2[(10; 24)] & 24 & 58.1 & 61.0 & 56.6 & 58.6 & 15.6 & 6 & 66.2 & 61.4 & 54.1 & 60.6 \\   & StreamMapNet[(10; 24)] & 24 & 62.9 & 63.1 & 55.8 & 60.6 & 12.5* & 6 & - & - & - & 57.7 \\   & SQD-MapNet[(10; 24)] & 24 & 67.0 & 65.5 & 59.5 & 64.0 & - & 6 & 66.9 & 54.9 & 56.1 & 59.3 \\   & MapUnveiler (ours) & 24 & **68.0** & **70.0** & **68.2** & **68.7** & 12.7 & 6 & **69.7** & **67.1** & **59.3** & **65.4** \\   

Table 1: Comparisons on nuScenes and Argoverse2 val sets. AP\({}_{p}\), AP\({}_{d}\), AP\({}_{b}\) indicate the average precision for pedestrian crossing, divider, and boundary, respectively. FPS is measured using a single NVIDIA A100 GPU. * are taken from the corresponding papers and are scaled based on the FPS of MapTRv2  for a fair comparison.

and 68.0%\(\)68.7% (+0.7%) for MapTRv2  and ours. Although MapUnveiler incorporates temporal modules, we achieve a reasonable inference speed (12.7 FPS) compared to frame-level MapNet (MapTRv2 , 15.6 FPS), surpassing both performance and speed compared to the state-of-the-art (HiMap , 9.7 FPS).

Qualitative Comparison.Fig. 4 shows qualitative comparisons on two perception range settings on nuScenes benchmark. We compare our MapUnveiler with the state-of-the-art frame-level model (MapTRv2 ) and temporal model (StreamMapNet ). As shown in the figure, MapTRv2 often mis-detects complex pedestrian crossings and cannot precisely predict dividers occluded by vehicles on the road. StreamMapNet struggles to accurately predict the boundaries of intersections, despite leveraging temporal information. In contrast, MapUnveiler consistently delivers accurate results for all map categories in most cases. We provide more qualitative results on various scenes in Appendix and full-frame results through the supplementary video.

### Analysis Experiments

In this study, we present extensive experimental results on nuScenes benchmark and provide analysis.

Long Training Schedules.Tab. 2 demonstrates the performance improvement achieved through longer training schedules. We obtained (24, 48, 110) and (6, 12, 30) epoch models by pre-training for (12, 24, 24) and (3, 6, 6) epochs, and then main training for (12, 24, 86) and (3, 6, 24) epochs on nuScenes and Argoverse2 datasets, respectively. As given in the table, the performance consistently improves with longer training, although the gains are relatively marginal. We conjecture that our MapUnveiler converges rapidly because we train the temporal modules from a pre-trained frame-level MapNet. Since our model shows no significant gain from the 110 epoch model on nuScenes, we opt for 48 epochs for the remaining analysis experiments.

Robustness to Occlusion.Tab. 3 presents results on our challenging validation splits collected based on the nearest dynamic objects (detailed in Sec. 4.1). For comparisons, we evaluate MapTRv2 and StreamMapNet using the code and pre-trained weight provided in each official repository. As shown in the table, MapUnveiler surpasses MapTRv2 and StreamMapNet in all evaluated metrics. In particular, existing approaches degrade performance significantly compared to the results on the standard

    &  &  \\  & Epoch & AP\({}_{p}\) & AP\({}_{d}\) & AP\({}_{b}\) & mAP & Epoch & AP\({}_{p}\) & AP\({}_{d}\) & AP\({}_{b}\) & mAP \\   & 24 & 67.6 & 67.6 & 68.8 & 68.0 & 6 & 68.9 & 73.7 & 68.9 & 70.5 \\  & 48 & 69.5 & 69.4 & 70.5 & 69.8 & 12 & 69.0 & 74.9 & 69.1 & 71.0 \\  & 110 & 71.0 & 69.1 & 71.8 & 70.6 & 30 & 72.5 & 74.2 & 71.9 & 72.9 \\   & 24 & 68.0 & 70.0 & 68.2 & 68.7 & 6 & 69.7 & 67.1 & 59.3 & 65.4 \\  & 48 & 68.4 & 71.2 & 68.3 & 69.3 & 12 & 70.4 & 66.8 & 59.3 & 65.5 \\  & 110 & 71.2 & 71.7 & 72.2 & 71.7 & 30 & 71.7 & 67.9 & 62.6 & 67.4 \\   

Table 2: Experimental results of the long training schedules.

Figure 4: Qualitative comparisons on two range variants of nuScenes val set: 60\(\)30\(m\) and 100\(\)50\(m\). We compare our MapUnveiler with MapTRv2  and StreamMapNet . We marked significant improvements from MapTRv2 and StreamMapNet using green boxes.

[MISSING_PAGE_EMPTY:9]

We also ablate the input in write, and the result is given in Tab. 8. We significantly improve the performance by writing clip tokens to the memory (67.1%\(\)69.1%). This suggests that the propagation of memory tokens facilitates constructing the global map. By additionally writing map tokens, we further boost the performance by explicitly providing the current map information (69.1%\(\)69.8%). Unfortunately, however, we cannot obtain performance gain by writing dense BEV feature additionally. We conjecture that memorizing different types of features, _i.e._, vectorized representation of tokens and rasterized representation of dense features, may disturb training and tend to converge to a sub-optimal state.

Temporal Window Size \(T\) and Stride \(S\).We train MapUnveiler with various temporal window size \(T\) and stride \(S\). As shown in Tab. 9, increasing the temporal window leads to performance improvement by interacting with more frames in a clip. However, training with a \(T=5\) setting requires \(>\) 40GB of GPU memory, limiting the GPU models. If we train MapUnveiler with \(T=3\), it consumes \(<\) 32GB of GPU memory, making it possible to train with various GPU models, and it achieves comparable performance. Additionally, selecting either too short (\(S=1\)) or too long (\(S=3\)) a temporal stride yields sub-optimal results. Therefore, we opt for \(T=3\) and \(S=2\) as the default setting.

Memory Token Size \(M\).Tab. 10 presents the experimental results with various memory token sizes. Selecting a memory token size of 96 or above does not significantly change the performance, whereas choosing a size smaller than 96 results in considerable performance degradation. This indicates that MapUnveiler does not require a large memory token size to store the 50 clip tokens and 50\(\)20 map tokens, demonstrating that it performs memory-efficiently.

Clip Token Size \(N_{c}\).Tab. 11 gives the results with various clip token sizes. MapUnveiler achieves state-of-the-art performance of 69.2% using only 25 clip tokens. We further boost the performance by using a larger clip token size of 50. However, employing clip token sizes beyond 50 disrupts learning and leads to a performance degradation.

More Analysis in Appendix.We provide additional experimental results including centerline, 3D map construction, geo-disjoint split, various backbones, and limitation analysis in Appendix.

### Limitation

MapUnveiler takes temporally consecutive frames as input. Therefore, if an intermediate frame is not correctly inputted due to communication errors in real-world scenarios, the unveiling within the clip may not be performed properly. We expect that the performance will recover quickly from the subsequent unveiling pipeline. Additionally, if the model cannot see a clear region across all frames as shown in Fig. 5, MapUnveiler may fail to recognize the occluded map information.

## 5 Conclusion

In this paper, we present a new VHC paradigm that constructs clip-level maps to efficiently incorporate conventional offline mapping strategies. In contrast to the recent temporal VHC models that do not consider the cumulative impacts of occluded map elements and directly propagate the noisy BEV features, we unveil the hidden map and noise in BEV features by interacting with compact clip tokens. To establish global mapping efficiently, we propagate the clip tokens instead of dense BEV features. With these two advanced modules, we propose MapUnveiler, which significantly outperforms previous works in challenging scenarios such as long-range perception and heavy occlusions. Since we introduce a novel insight into online VHC approaches to incorporate mapping strategy efficiently, we hope that our research motivates follow-up studies to delve deeper into establishing global mapping online and leads to practical VHC solutions.

Figure 5: A limitation under heavy occlusions. MapUnveiler initially roughly predicts the boundary where we highlighted in green. However, the invisible regions are continuous, and MapUnveiler eventually predicts the region as having no boundary.