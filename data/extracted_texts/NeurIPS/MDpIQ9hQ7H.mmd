# total loss loss = loss_MC + loss_MT + loss_CA

From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning

Pusen Dong1, Tianchen Zhu1, Yue Qiu1, Haoyi Zhou1,2, Jianxin Li1,2

1Beihang University, Beijing, China

2Zhongguancun Laboratory, Beijing, China

{dongps, zhutc,qiuyue, zhouhy, lijx}@act.buaa.edu.cn

###### Abstract

Safe reinforcement learning (RL) requires the agent to finish a given task while obeying specific constraints. Giving constraints in natural language form has great potential for practical scenarios due to its flexible transfer capability and accessibility. Previous safe RL methods with natural language constraints typically need to design cost functions manually for each constraint, which requires domain expertise and lacks flexibility. In this paper, we harness the dual role of text in this task, using it not only to provide constraint but also as a training signal. We introduce the Trajectory-level Textual Constraints Translator (TTCT) to replace the manually designed cost function. Our empirical results demonstrate that TTCT effectively comprehends textual constraint and trajectory, and the policies trained by TTCT can achieve a lower violation rate than the standard cost function. Extra studies are conducted to demonstrate that the TTCT has zero-shot transfer capability to adapt to constraint-shift environments.

## 1 Introduction

In recent years, reinforcement learning (RL) has achieved remarkable success in multiple domains, such as go game  and robotic control . However, deploying RL in real-world scenarios still remains challenging. Many real-world decision-making applications, such as autonomous driving  require agents to obey certain constraints while achieving the desired goals. To learn a safe constrained policy, some safe RL works  have proposed methods to maximize the reward while minimizing the constraint violations after training or during training.

However, several limitations prevent the existing safe RL methods' widespread use in real-world applications. Firstly, these methods often require mathematical or logical definitions of cost functions, which require domain expertise (**Limitation 1**). Secondly, their cost function definitions are frequently specific to a particular context and cannot be easily generalized to new tasks with similar constraints (**Limitation 2**). Lastly, most current safe RL methods focus on constraints that are logically simple, typically involving only one single entity or one single state , which can't represent the real-world safety requirements and lack universality (**Limitation 3**).

Using natural language to provide constraints  is a promising approach to overcome **Limitation 1** and \(}\) because natural language allows for flexible, high-level expression of constraints that can easily adapt to different scenarios. Regarding **Limitation 3**, previous approaches primarily employ what we call the _single state/entity textual constraint_. The single-state/entity textual constraint focuses solely on constraints related to one specific state or entity, limiting the ability to model complex safety requirements in real-world scenarios. Many safety requirements involve interactions and dependencies among multiple states or entities over time. By only addressing a single state or entity, these constraints fail to capture the dynamic relationships and temporal aspects that are crucial for ensuring safety in complex environments. So we suggest using a more generalizable constraint type _trajectory-level textual constraint_. The trajectory-level textual constraint is a more universal constraint with complex logic semantics involving multiple states/entities. It is a nonlinear combination of multiple entities or multiple environment states and can model any constraint requirements in real-world scenarios. The trajectory-level constraints in natural language form are the highly abstract expression of the agent's behavior guidelines, serving as a more natural and straightforward way to introduce constraints. Notably, the set of trajectory-level constraints encompasses single-state constraints, as any single-state constraint can be reformulated as an equivalent trajectory-level constraint. Examples of trajectory-level textual constraint and single state/entity textual constraint are presented in Table 1.

Employing trajectory-level textual constraints across the entire trajectory poses two significant challenges. Firstly, determining whether an RL agent violates textual constraint over a trajectory is non-trivial, it needs to have the perception of the historical states and actions observed along the trajectory (**Challenge 1**). Secondly, the trajectory-level safety problem is susceptible to sparse cost , where cost is only imposed when the agent violates textual constraints at the final time step, making it challenging for the agent to learn which actions contribute to a gradual escalation of risk (**Challenge 2**). For instance, the agent needs to learn constraints like, "Don't touch lava after you touch grass." Without intermediate feedback, however, it struggles to understand how early actions, such as stepping on grass, contribute to eventual violations.

To address **Challenge 1**, we propose a general approach to align the trajectory's factual logic with the text's semantic logic, eliminating the need for manual encoding or separate models for each type of constraint. Our method employs a sequence model  for modeling agent historical interactions with the environment, and a pre-trained language model (LM)  to comprehend natural language constraints. We then maximize the embedding similarities between matching pairs (trajectory, text) and minimize the embedding similarities between non-matching pairs using a contrastive learning approach  similar to CLIP . By calculating the similarity between the textual embeddings of the constraint and the trajectory, we can predict whether a constraint is violated in this trajectory. Our method uniquely leverages text as both a source of constraints and a unified supervisory signal for trajectory encoding. In this dual role, text not only provides constraints but also guides the training process, enabling the model to naturally handle diverse semantic constraints without requiring specific model adjustments for each type. This design allows for a more flexible and generalizable system, significantly simplifying the handling of complex, multi-dimensional constraints.

In addition, to address the issue of cost sparsity (**Challenge 2**), we introduce a method for temporal credit assignment . The proposed approach involves decomposing the one episodic cost of the textual constraint into multiple parts and allocating them to each state-action pair within the trajectory. This method offers denser cost signals regarding the relationship between the textual constraint and the agent's every action. It informs the agent which behaviors are risky and which are safer, thereby enhancing safety and aiding model performance.

Our experiments demonstrate that the proposed method can effectively address **Challenge 1** and **Challenge 2**. In both 3D navigation  and 2D grid exploration  tasks, agents trained using our method achieve significantly lower violation rates (up to 4.0x) compared to agents trained with ground-truth cost functions while maintaining comparable rewards and more importantly, our method obtains the Pareto frontier . In addition to this, our method has zero-shot adaptation capability to adapt to constraint-shift environments without fine-tuning.

|p{113.8pt}} 
**Single state/entity constraint (previous)** & **Trajectory-level constraint (ours)** \\  Don’t drive car. **(single entity)** & Don’t drive car _after you drink wine_. **(multi entities)** \\  Don’t touch lava. **(single state)** & Don’t touch lava _more that three times_. **(multi states)** \\  Don’t step in the gassed area. **(single state)** & Don’t stay in the gas area for _more than 5 minutes_, your gas mask will fail. **(multi states)** \\  

Table 1: **Comparison of trajectory-level constraints and previous single state/entity constraints.** Universal Trajectory-level constraints can model any constraint requirements in real-world scenarios. But single state/entity constraint can only model the constraint requirements on individual state/entity.

Related Work

**Safe RL.** Safe RL aims to train policies that maximize reward while minimizing constraint violations [11; 13]. In prior work, there are usually two ways to learn safe policies: (1) consider cost as one of the optimization objectives to achieve safety [48; 6; 49; 39; 44; 33], and (2) achieve safety by leveraging external knowledge (e.g. expert demonstration) [35; 32; 1; 47]. These typical safe RL algorithms require either human-defined cost functions or human-specified cost constraints which are unavailable in the tasks that constraints are given by natural language.

**RL with Natural Language.** Prior works have integrated natural language into RL to improve generalization or learning efficiency in various ways. For example, Hermann et al.  studied how to train an agent that can follow natural language instructions to reach a specific goal. Additionally, natural language has been used to constrain agents to behave safely. For instance, Prakash et al.  trained a constraint checker to predict whether natural language constraints are violated. Yang et al.  trained a constraint interpreter to predict which entities in the environment may be relevant to the constraint and used the interpreter to predict costs. Lou et al.  used pre-trained language models to predict the cost of specific states, avoiding the need for artificially designed cost functions. However, previous methods cannot uniformly handle textual constraints with one framework, which limits their applicability.

**Credit Assignment in RL.** Credit assignment studies the problem of inferring the true reward from the designed reward. Prior works have studied improving sample efficiency of RL algorithms through credit assignment, for example by using information gain , as an intrinsic bonus reward to aid exploration. Goyal et al.  proposed the use of natural language instructions to perform reward shaping to improve the sample efficiency of RL algorithms. Liu et al.  learned to decompose the episodic return as the reward for policy optimization. However, to the best of our knowledge, our work is the first to apply credit assignment to safe RL.

## 3 Preliminaries

**Problem formulation.** Trajectory-level constraint problem can be formed as the Constrained Non-Markov Decision Process (CNMDP) [3; 43], and it can be defined by the tuple <\(S,A,T,R,,C,Y,^{*}\)>. Here \(S\) represents the set of states, \(A\) represents the set of actions, \(T\) represents the state transition function, \(R\) represents the reward function, and \((0,1)\) represents the discount factor. In addition, \(Y\) represents the set of trajectory-level textual constraints (e.g., "You have 10 HP, you will lose 3 HP every time you touch the lava, don't die."), which describes the constraint that the agent needs to obey across the entire trajectory. \(C\) represents the cost function determined by \(y Y\). \(^{*}\) represents the set of historical trajectories.

**RL with constraints.** The objective for the agent is to maximize reward while obeying the specified textual constraint as much as possible . Thus, in our task setting, the agent needs to learn a policy \(\): \(S Y^{*} P(A)\) which maps from the state space \(S\), textual constraints \(Y\) and historical trajectories \(^{*}\) to the distributions over actions \(A\). Given a \(y\), we learn a policy \(\) that maximizes the cumulative discounted reward \(J_{R}\) while keeping the cumulative discounted cost (average violation rate) \(J_{C}\) below a constraint violation budget \(B_{C}(y)\):

\[_{}J_{R}()=_{}[_{t=0}^{} R(s_{t})]  J_{C}()=_{}[_{t=0}^{} C(s_{t },a_{t},y,_{t})] B_{C}(y).\] (1)

Here \(B_{C}(y)\) and \(C(s_{t},a_{t},y,_{t})\) are two functions both depending on textual constraint \(y\). \(_{t}\) represents the historical trajectory at time step \(t\).

**Episodic RL.** Similar to the task with episodic rewards , in our task setting, a cost is only given at the end of each trajectory \(\) when the agent violates the textual constraint \(y\). In other words, before violating \(y\), the cost \(C(s_{t},a_{t},y,_{t})=0\) for all \(t<T\). For simplicity, we omit the discount factor and assume that the trajectory length is at most \(T\) so that we can denote \(a_{T}\) as the final action that causes the agent to violate \(y\) without further confusion. Therefore, the constraint qualification of the objective in RL with constraints becomes \(J_{C}()=_{}[C(s_{T},a_{T},y,_{T})] B_{C}(y)\). Due to the sparsity of cost, a large amount of rollout trajectories are needed to help the agent distinguish the subtle effects of actions on textual constraint . This situation will become more serious when trajectory-level constraints are complex and difficult to understand.

## 4 TTCT: Trajectory-level Textual Constraints Translator

In this section, we introduce our proposed framework **TTCT** ( **T**rajectory-level **T**extual **C**onstraints **T**ranslator) as shown in Figure 1. TTCT has two key components: _the text-trajectory alignment component_ and _the cost assignment component_. The text-trajectory alignment component is used to address the violations prediction problem. The cost assignment component is used to address the sparse cost problem.

### Text-Trajectory Alignment Component

We propose a component to learn from offline data to predict whether a given trajectory violates textual constraints. The core idea of this component is to learn trajectory representations under textual supervision and connect trajectory representation to text representation. If the distance between the two representations in the embedding space is sufficiently close, we can consider that the given trajectory violates the constraint. Our approach does not require modeling entities of the environment like previous work, such as , which involves labeling hazardous items artificially in every observation. Instead, we model this task as a trajectory-text multimodal learning problem. Hence, our method can learn trajectory representations and text representations from the pairs _(trajectory, trajectory-level textual constraint)_. We believe that learning from the supervision of natural language could not only enhance the representation power but also enable flexible zero-shot transfer .

Formally, given a batch of \(N\) (trajectory \(\), trajectory-level textual constraint \(y\)) pairs. For each pair, the trajectory corresponds to the text, indicating that the given trajectory violates the given textual constraint. The trajectory can be defined as \(=(s_{1},a_{1},s_{2},a_{2},...,s_{T-1},a_{T-1},s_{T},a_{T})\), where \(T\) is the step at which the textual constraint \(y\) is first violated by the trajectory. Here \(s_{t}\) is a \(d_{s}\)-dimensional observation vector. Each state in the trajectory is processed by a state encoder to obtain a representation \(v_{t}^{s}\), also action is processed by an action encoder to obtain a representation \(v_{t}^{a}\). Then, we concatenate \(v_{t}^{s}\) and \(v_{t}^{a}\) to obtain a vector representation \(v_{t}\) for each state-action pair. After that, we learn separate unimodal encoders \(g_{T}\) and \(g_{C}\) for the trajectory and textual constraint, respectively. The trajectory encoder \(g_{T}\) utilizes a causal transformer to extract the trajectory representation from the input state-action representation sequence \(\{v_{t}\}_{t=1}^{T}\):

\[H_{1},H_{2},H_{3},...,H_{T-1},H_{T}=g_{T}(\{v_{t}\}_{t=1}^{T}),\] (2)

where \(H_{t}\) is a \(d_{H}\)-dimensional vector. The final embedding \(H_{T}\) is used as the representation for the entire trajectory. Specifically, the causal Transformer processes the trajectory sequence by maintaining a left-to-right context while generating embeddings. This allows the model to capture temporal dependencies within the trajectory and obtain the embeddings for time steps before \(T\). The textual constraint encoder \(g_{C}\) is used to extract features that are related to the constraints and it could be one of a wide variety of language models:

\[L=g_{C}(y),\] (3)

Figure 1: **TTCT overview. TTCT consists of two training components: (1) _the text-trajectory alignment component_ connects trajectory to text with multimodal architecture, and (2) _the cost assignment component_ assigns a cost value to each state-action based on its impact on satisfying the constraint. When training RL policy, the text-trajectory alignment component is used to predict whether a trajectory violates a given constraint and the cost assignment component is used to assign non-violation cost.**where \(L\) is a \(d_{L}\)-dimensional vector representation. Then we define symmetric similarities between the two modalities with cosine distance:

\[_{T}(,y)=()* L^{}}{\|H_{T} \|\|L\|},\ _{T}(y,)=()* H_{T}}{\|L \|\|H_{T}\|},\] (4)

where \(\) is a learnable parameter and \(T\) means that use last embedding of trajectory to calculate similarity. And we use softmax to calculate \(trajectory text\) and \(text trajectory\) similarities scores:

\[p_{i}^{ y}()=_{T}(,y_{i}))}{_{j=1}^ {N}(_{T}(,y_{j}))},\ p_{i}^{y}(y)=_{T}(y,_{i}))}{_{j=1}^{N}(_{T}(y,_{j} ))}.\] (5)

Let \(q^{ y}(),q^{y}(y)\) indicate the ground-truth similarity scores, where the **negative pair** (trajectory doesn't violate textual constraint) has a probability of 0 and the **positive pair** (trajectory violate textual constraint) has a probability of 1. In our task setting, a trajectory can correspond to multiple textual constraints, and vice versa. For example, two textual constraints such as _"Do not touch lava"_ and _"After stepping on water, do not touch lava"_ might both be violated by a single given trajectory. This many-to-many relationship between trajectories and textual constraints implies that the same textual constraints (or different textual constraints with the same semantics) can apply to multiple trajectories, while a single trajectory may comply with several textual constraints. So there may be more than one positive pair in \(q_{i}^{ y}()\) and \(q_{i}^{y}(y)\). Therefore, we use Kullback-Leibler (KL) divergence  as the **multimodal contrastive (MC)** loss to optimize our encoder similar to :

\[_{MC}=_{(,y)}[(p ^{ y}(),(q^{ y}()))+(p^{y }(y),(q^{y}(y)))],\] (6)

where \(\) is the training set.

In addition to the multimodal contrastive (MC) loss, we also introduce a **within-trajectory (WT)** loss. Specifically, suppose we have a trajectory's representation sequence (\(H_{1},H_{2},H_{3},...,H_{(T-1)},H_{T}\)) and its corresponding textual constraint embedding \(L\), we can calculate cosine similarity \(_{t}(,y)\) between embedding \(H_{t}\) and \(L\) using Equation 4. Then we can calculate similarity scores within the trajectory:

\[p_{t}^{}(y)=_{t}(,y))}{_{k=1}^{T}( _{k}(,y))}.\] (7)

Different from \(p_{i}^{y}(y)\) in Equation 5, which measures similarity scores across \(N\) trajectories, Equation 7 is used to measure the similarity scores of different time steps within a trajectory. The reason for doing this is that the textual constraint is violated at time step \(T\), while in the previous time steps the constraint is not violated. Therefore, the instinct is to maximize the similarity score between the final trajectory embedding \(H_{T}\) and the textual constraint embedding \(L\), while minimizing the similarity score between all previous time step embeddings (\(H_{1},H_{2},H_{3},...,H_{(T-1)}\)) and the textual constraint embedding \(L\). Based on this instinct, we introduce **within-trajectory (WT)** loss:

\[_{WT}=_{(,y)}[-(_{t=1} ^{T-1}(1-p_{t}^{}(y))+(p_{T}^{}(y)))],\] (8)

where the first term is responsible for minimizing the similarity score for the embedding of time steps before \(T\) in the trajectory sequence, while the second term is responsible for maximizing the similarity score for the embedding of time step \(T\).

By combining these two losses \(_{WT},_{MC}\), we can train a text encoder to minimize the distance between embeddings of semantically similar texts, while simultaneously training a trajectory encoder to minimize the distance between embeddings of semantically similar trajectories. Crucially, this approach enables us to align the text and trajectory embeddings that correspond to the same semantic constraint, fostering a cohesive representation of their shared meaning, and further determining whether the trajectory violates the constraint by calculating embedding similarity.

### Cost Assignment Component

After solving the problem of predicting whether a given trajectory violates a textual constraint, there is still the issue of cost sparsity. Motivated by the works of temporal credit assignment , we propose a component to capture the relationship between the state-action pair and the textual constraint and assign a cost value to each state-action based on its impact on satisfying the constraint.

Specifically, suppose we have a (trajectory \(\), textual constraint \(y\)) pair and its representation (\(\{H_{t}\}_{t=1}^{T}\), \(L\)) obtained from text-trajectory alignment component. The textual constraint representation \(L\) is processed by an episodic-cost prediction layer \(F^{e}\) to obtain a predicted episodic cost \((y)=(F^{e}(L))\) for the entire trajectory. We expect the episodic cost can be considered as the sum of cost on all non-violation state-action pairs: \((y)=_{t=1}^{T-1}(s_{t},a_{t},y,_{t})\). To evaluate the significance of each timestep's action relative to textual constraints, we employ an attention mechanism:

\[e_{t}=(_{t}(,y)).\] (9)

Here we regard the text representation as the \(query\), each time step's representation in the trajectory as the \(key\), and compute the attention score \(e_{t}\) based on the cosine similarity metric. After that, we use the sigmoid function to make sure the score falls within the range of 0 to 1. Each attention score \(e_{t}\) quantifies the degree of influence of the state-action pair (\(s_{t},a_{t}\)) on violating the textual constraint. Then we obtain an influence-based representation \(H_{t}^{*}=e_{t}H_{t}\). To predict the cost \((s_{t},a_{t},y,_{t})\), we incorporate a feed-forward layer called the cost assignment layer \(F^{c}\) and output the predicted non-violation single step cost as:

\[(s_{t},a_{t},y,_{t})=(F^{c}((H_{t}^{ *},L))).\] (10)

The loss function for measuring inconsistency of the episodic cost \((y)\) and the sum of non-violation single step costs \((s_{t},a_{t},y,h_{t})\) can be formed as:

\[_{CA}=_{(,y)}[(_{t=1}^{T-1}(s_{t},a_{t},y,_{t})-(y))^{2}].\] (11)

This mutual prediction loss function \(_{CA}\) is only used to update the episodic-cost prediction layer and cost assignment layer, and not to update the parameters of the trajectory encoder or text encoder during backpropagation. This helps ensure the validity of the predictions by preventing overfitting or interference from other parts of the model.

The effectiveness of this component comes from two main sources. First, the text-trajectory alignment component projects semantically similar text representations to nearby points in the embedding space, allowing the episodic-cost prediction layer to assign similar values to embeddings with close distances. This aligns with the intuition that textual constraints with comparable violation difficulty should yield similar episodic costs. Second, the cost assignment layer leverages the representational power of the text-trajectory alignment component to capture complex relationships between state-action pairs and constraints, enabling accurate single-step cost predictions.

In our experiment, we simultaneously train the text-trajectory alignment component and cost assignment component end-to-end with a uniform loss function \(_{TTCT}\):

\[_{TTCT}=_{MC}+_{WT}+_{CA}.\] (12)

By doing this, we can avoid the need for separate pre-training or fine-tuning steps, which can be time-consuming and require additional hyperparameter tuning. Also, this can enable the cost assignment component to gradually learn from the text-trajectory alignment component and make more accurate predictions over time.

In the test phase, at time step \(t\) we encode trajectory \(_{t}\) and textual constraint \(y\) with Equation 3 and Equation 2 to obtain the entire trajectory embedding \(H_{t}\) and text embedding \(L\). Then we calculate distance score \((_{t},y)\) using Equation 4. The predicted cost function \(\) is given by:

\[=1,&(_{t},y),\\ (F^{c}((H_{t}^{*},L))),&,\] (13)

where \(\) is a hyperparameter that defines the threshold of the cost prediction. \(=1\) indicates that the TTCT model predicts that the textual constraint is violated. Otherwise, if the textual constraint is not violated by the given trajectory, we assign a predicted cost using Equation 10.

Policy Training

Our Trajectory-level textual constraints Translator framework is a general method for integrating free-form natural language into safe RL algorithms. In this section, we introduce how to integrate our TTCT into safe RL algorithms so that the agents can maximize rewards while avoiding early termination of the environment due to violation of textual constraints. To enable perception of historical trajectory, the trajectory encoder and text encoder are not only used as **frozen plugins**\(g_{T}\) and \(g_{C}\) for cost prediction but also as **trainable sequence models**\(g_{T}^{*}\) and \(g_{C}^{*}\) for modeling historical trajectory. This allows the agent to take into account historical context when making decisions. To further improve the ability to capture relevant information from the environment, we use LoRA  to fine-tune both the \(g_{T}^{*}\) and \(g_{C}^{*}\) during policy training. The usage of \(g_{T}\), \(g_{C}\) and \(g_{T}^{*}\), \(g_{C}^{*}\) is illustrated in Appendix A.4 Figure 8.

Formally, let's assume we have a policy \(\) with parameter \(\) to gather transitions from environments. We maintain a vector to record the history state-action pairs sequence, and at time step \(t\) we use \(g_{T}^{*}\) and \(g_{C}^{*}\) to encode \(_{t-1}\) and textual constraint \(y\) so that we can get historical context representation \(H_{t-1}\) and textual constraint representation \(L\). The policy selects an action \(a_{t}=_{}(o_{t},H_{t-1},L)\) to interact with environment to get a new observation \(o_{t+1}\). And we update \(_{t}\) with the new state-action pair \((o_{t},a_{t})\) to get \(_{t}\). With \(_{t}\) and \(L\), \(_{t}\) can be predicted according to Equation 13. Then we store the transition into the buffer and keep interacting until the buffer is full. In the policy updating phase, after calculating the specific loss function for different safe RL algorithms, we update the policy \(\) with gradient descent and update \(g_{T}^{*}\), \(g_{C}^{*}\) with LoRA. It is worth noting that \(g_{T}\) and \(g_{C}\) are not updated during the whole policy training phase, as they are only used for cost prediction. The pseudo-code and more details of the policy training can be found in Appendix A.4.

## 6 Experiments

Our experiments aim to answer the following questions: **(1)** Can our TTCT accurately recognize whether an agent violates the trajectory-level textual constraints? **(2)** Does the policy network, trained with predicted cost from TTCT, achieve fewer constraint violations than trained with the ground-truth cost function? **(3)** How much performance improvement can the cost assignment (CA) component achieve? **(4)** Does our TTCT have zero-shot capability to be directly applicable to constraint-shift environments without any fine-tuning? We adopt the following experiment setting to address these questions.

### Setup

**Task.** We evaluate TTCT on two tasks (Figure 2 (a,b)): 2D grid exploration task _Hazard-World-Grid_ (Grid)  and 3D robot navigation task _SafetyGoal_ (Goal) . And we designed over 200 trajectory-level textual constraints which can be grouped into 4 categories, to constrain the agents. A detailed description of the categories of constraints will be given in Appendix A.1. Different from the default setting, in our task setting, when a trajectory-level textual constraint is violated, the

Figure 2: (a) One layout in **Hazard-World-Grid**, where orange tiles are lava, blue tiles are water and green tiles are grass. Agents need to collect reward objects in the grid while avoiding violating our designed textual constraint for the entire episode. (b) Robot navigation task **SafetyGoal** that is built in Safety-Gymnasium , where there are multiple types of objects in the environment. Agents need to reach the goal while avoiding violating our designed textual constraint for the entire episode. (c) **LavaWall**, a task has the same goal but different hazard objects compared to Hazard-World-Grid.

environment is immediately terminated. This is a more difficult setup than the default. In this setup, the agents must collect as many rewards as possible while staying alive.

**Baselines.** We consider the following baselines: PPO, PPO_Lagrangian(PPO_Lag), CPPO_PID, FOCOPS. PPO does not consider constraints and simply aims to maximize the average reward. We use PPO to compare the ability of our methods to obtain rewards. As for the last three algorithms, we design two training modes for them. One is trained with standard ground-truth cost, where the cost is given by the human-designed violation checking functions, and we call it ground-truth (GC) mode. The other is trained with the predicted cost by our proposed TTCT, which we refer to as cost prediction (CP) mode. More information about the baselines and training modes can be found in Appendix A.1.2.

**Metrics.** We take average episodic reward (Avg. R) and average episodic cost (Avg. C) as the main comparison metrics. Average episodic cost can also be considered as the average probability of violating the constraints. The higher Avg. R, the better performance, and the lower Avg. C, the better performance.

### Main Results and Analysis

The evaluation results are shown in Figure 3 and the learning curves are shown in Figure 4. We can observe that in the _Hazard-World-Grid_ task, compared with PPO, the policies trained with GC can reduce the probability of violating textual constraints to some extent, but not significantly. This is because the sparsity of the cost makes it difficult for an agent to learn the relevance of the behavior to the textual constraints, further making it difficult to find risk-avoiding paths of action. In the more difficult _SafetyGoal_ task, it is even more challenging for GC-trained agents to learn how to avoid violations. In the CPPO_PID and FOCOPS algorithms trained with GC mode, the probability of violations even rises gradually as the training progresses. In contrast, the agents trained with predicted cost can achieve lower violation probabilities than GC-trained agents across all algorithms and tasks and get rewards close to GC-trained agents.

These results show that TTCT can give an accurate predicted episodic cost at the time step when the constraint is violated, it can also give timely cost feedback to non-violation actions through the cost assignment component so that the agents can find more risk-averse action paths. And these results answer questions **(1)** and **(2)**. The discussion about the violation prediction capability of the text-trajectory component can be found in Appendix B.1. The interpretability and case study of the cost assignment component can be found in Appendix B.2.

### Ablation Study

To study the influence of the cost assignment component. We conduct an ablation study by removing the cost assignment component from the full TTCT. The results of the ablation study experiment are shown in Figure 4. We can observe that even TTCT without cost assignment can achieve similar performance as GC mode. And in most of the results if we remove the cost assignment component, the performance drops. This shows that **our text trajectory alignment component can accurately predict the ground truth cost, and the use of the cost assignment component can further help us learn a safer agent**. These results answer questions **(3)**.

Figure 3: **Evaluation results of our proposed method TTCT. The blue bars are our proposed cost prediction (CP) mode performance and the orange bars are the ground-truth cost (GC) mode performance. The black dashed lines are PPO performance. (a) Results on Hazard-World-Grid task. (b) Results on SafetyGoal task.**

### Further Results

**Pareto frontier.** Multi-objective optimization typically involves finding the best trade-offs between multiple objectives. In this context, it is important to evaluate the performance of different methods based on their Pareto frontier , which represents the set of optimal trade-offs between the reward and cost objectives. We plot the Pareto frontier of policies trained with GC and policies trained with CP on a two-dimensional graph, with the vertical axis representing the reward objective and the horizontal axis representing the cost objective as presented in Figure 6. The solution that has the Pareto frontier closer to the origin is generally considered more effective than those that have the Pareto frontier farther from the origin. We can observe from the figure that the policies trained with predicted cost by our TTCT have a Pareto frontier closer to the origin. This proves the effectiveness of our method and further answers Questions **(1)** and **(2)**.

Figure 4: **Learning curve of our proposed method TTCT. Each column is an algorithm. The six figures on the left show the results of experiments on the Hazard-World-Grid task and the six figures on the right show the results of experiments on the SafetyGoal task. The solid line is the mean value, and the light shade represents the area within one standard deviation.**

Figure 5: **Ablation study of removing the cost assignment (CA) component. The blue bars are cost prediction (CP) mode performance with full TTCT and the orange bars is the cost prediction (CP) mode performance without CA component. The black dashed lines are PPO performance. (a) Ablation results on Hazard-World-Grid task. (b) Ablation results in SafetyGoal task.**

Figure 6: **Results of Pareto frontiers. We compare the performance of 200 policies trained using cost prediction (CP) and 200 policies trained with ground-truth cost (GC). The \(\) symbol represents the policy on the Pareto frontier. And we connect the Pareto–optimal policies with a curve.**

**Zero-shot transfer capability.** To explore whether our method has zero-shot transfer capability, we use the TTCT trained under the _Hazard-World-Grid_ environment to apply directly to a new environment called _LavaWall_ (Figure 2 (c)) , without fine-tuning. The results are shown in Figure 7. We can observe that the policy trained with cost prediction (CP) from TTCT trained under the Hazard-World-Grid environment can still achieve a low violation rate comparable to the GC-trained policy. This answers Question (**4**).

## 7 Conclusion and Future Work

In this paper, we study the problem of safe RL with trajectory-level natural language constraints and propose a method of trajectory-level textual constraints translator (TTCT) to translate constraints into a cost function. By combining the text-trajectory alignment (CA) component and the cost assignment (CA) component, our method can elegantly solve the problems of predicting constraint violations and cost sparsity. We demonstrated that our TTCT method achieves a lower violation probability compared to the standard cost function. Thanks to its powerful multimodal representation capabilities, our method also has zero-shot transfer capability to help the agent safely explore the constraint-shift environment. This work opens up new possibilities for training agents in safe RL tasks with total free-form and complex textual constraints.

Our work still has room for improvement. The violation rate of our method is not absolute zero. In future work, we plan to investigate the application of TTCT in more complex environments and explore the integration of other techniques such as meta-learning  to further improve the performance and generalization capabilities of our method.