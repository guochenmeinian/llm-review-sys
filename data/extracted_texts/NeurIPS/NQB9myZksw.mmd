# Robustly overfitting latents for flexible neural image compression

Yura Perugachi-Diaz

Vrije Universiteit Amsterdam

y.m.perugachidiaz@vu.nl

&Arwin Ganskeoele

Centrum Wiskunde

& Informatica

awg@cwi.nl

&Sandjai Bhulai

Vrije Universiteit Amsterdam

s.bhulai@vu.nl

###### Abstract

Neural image compression has made a great deal of progress. State-of-the-art models are based on variational autoencoders and are outperforming classical models. Neural compression models learn to encode an image into a quantized latent representation that can be efficiently sent to the decoder, which decodes the quantized latent into a reconstructed image. While these models have proven successful in practice, they lead to sub-optimal results due to imperfect optimization and limitations in the encoder and decoder capacity. Recent work shows how to use stochastic Gumbel annealing (SGA) to refine the latents of pre-trained neural image compression models. We extend this idea by introducing SGA+, which contains three different methods that build upon SGA. We show how our method improves the overall compression performance in terms of the R-D trade-off, compared to its predecessors. Additionally, we show how refinement of the latents with our best-performing method improves the compression performance on both the Tecnick and CLIC dataset. Our method is deployed for a pre-trained hyperprior and for a more flexible model. Further, we give a detailed analysis of our proposed methods and show that they are less sensitive to hyperparameter choices. Finally, we show how each method can be extended to three- instead of two-class rounding.

## 1 Introduction

Image compression allows efficient sending of an image between systems by reducing their size. There are two types of compression: lossless and lossy. Lossless image compression sends images perfectly without losing any quality and can thus be restored in their original format, such as the PNG format. Lossy compression, such as BPG Bellard (2014), JPEG Wallace (1992) or JPEG2000 Skodras et al. (2001), loses some quality of the compressed image. Lossy compression aims to preserve as much of the quality of the reconstructed image as possible, compared to its original format, while allowing a significantly larger reduction in required storage.

Traditional methods Wallace (1992); Skodras et al. (2001), especially lossless methods, can lead to limited compression ratios or degradation in quality. With the rise of deep learning, neural image compression is becoming a popular method Theis et al. (2017); Toderici et al. (2017). In contrast with traditional methods, neural image compression methods have been shown to achieve higher compression ratios and less degradation in image quality Balle et al. (2018); Minnen et al. (2018); Lee et al. (2019). Additionally, neural compression techniques have shown improvements compared to traditional codecs for other data domains, such as video. Agustsson et al. (2020); Habibian et al. (2019); Lu et al. (2019).

In practice, neural lossy compression methods have proven to be successful and achieve state-of-the-art performance Balle et al. (2018); Cheng et al. (2020); Minnen et al. (2018); Lee et al. (2019). These models are frequently based on variational autoencoders (VAEs) with an encoder-decoder structureKingma and Welling (2013). The models are trained to minimize the expected rate-distortion (R-D) cost: \(R+ D\). Intuitively, one learns a mapping that encodes an image into a compressible latent representation. The latent representation is sent to a decoder and is decoded into a reconstructed image. The aim is to train the compression model in such way that it finds a latent representation that represents the best trade-off between the length of the bitstream for an image and the quality of the reconstructed image. Even though these models have proven to be successful in practice, they do have limited capacity when it comes to optimization and generalization. For example, the encoder's capacity is limited which makes the latent representation sub-optimal Cremer et al. (2018). Recent work Campos et al. (2019); Guo et al. (2020); Yang et al. (2020) proposes procedures to refine the encoder or latents, which lead to better compression performance. Furthermore, in neural video compression, other work focuses on adapting the encoder Aytekin et al. (2018); Lu et al. (2020) or finetuning a full compression model after training to improve the video compression performance van Rozendaal et al. (2021).

The advantage of refining latents Campos et al. (2019); Yang et al. (2020) is that improved compression results per image are achieved while the model does not need to be modified. Instead, the latent representations for each individual image undergo a refining procedure. This results in a latent representation that obtains an improved bitstream and image quality over its original state from the pre-trained model. As mentioned in Yang et al. (2020), the refining procedure for stochastic rounding with Stochastic Gradient Gumbel Annealing (SGA) considerably improves performance.

In this paper, we introduce SGA+, an extension of SGA that further improves compression performance and is less sensitive to hyperparameter choices. The main contributions are: _(i)_ showing how changing the probability space with more natural methods instead of SGA boosts the compression performance, _(ii)_ proposing the sigmoid scaled logit (SSL), which can smoothly interpolate between the approximate \(\), linear, cosine and round, _(iii)_ demonstrating a generalization to rounding to three classes, that contains the two classes as a special case, and _(iv)_ showing that SGA+ not only outperforms SGA on a similar pre-trained mean-scale hyperprior model as in Yang et al. (2020), but also achieves an even better performance for the pre-trained models of Cheng et al. (2020).

Further, we show how SSL outperforms baselines in an R-D plot on the Kodak dataset, in terms of peak signal-to-noise ratio (PSNR) versus the bits per pixel (BPP) and in terms of true loss curves. Additionally, we show how our method generalizes to the Tecnick and CLIC dataset, followed by qualitative results. We analyze the stability of all functions and show the effect of interpolation between different methods with SSL. Lastly, we analyze a refining procedure at compression time that allows moving along the R-D curve when refining the latents with another \(\) than a pre-trained model is trained on Gao et al. (2022); Xu et al. (2023). The code can be retrieved from: https://github.com/yperugachidiaz/flexible_neural_image_compression.

## 2 Preliminaries and related work

In lossy compression, the aim is to find a mapping of image \(x\) where the distortion of the reconstructed image \(\) is as little as possible compared to the original one while using as little storage as possible. Therefore, training a lossy neural image compression model presents a trade-off between minimizing the length of the bitstream for an image and minimizing the distortion of the reconstructed image Balle et al. (2017); Lee et al. (2019); Minnen et al. (2018); Theis et al. (2017).

Neural image compression models from Balle et al. (2017); Cheng et al. (2020); Minnen et al. (2018); Theis et al. (2017), also known as hyperpriors, accomplish this kind of mapping with latent variables. An image \(x\) is encoded onto a latent representation \(y=g_{a}(x)\), where \(g_{a}()\) is the encoder. Next, \(y\) is quantized \(Q(y)=\) into a discrete variable that is sent losslessly to the decoder. The reconstructed image is given by: \(=g_{s}()\), where \(g_{s}()\) represents the decoder. The rate-distortion objective that needs to be minimized for this specific problem is given by:

\[ =R+ D\] \[=_{x p_{x}}[-_{2}p_{}( )]}_{}+_{x p_{x}} [d(x,)]}_{},\] (1)

where \(\) is a Lagrange multiplier determining the rate-distortion trade-off, \(R\) is the expected bitstream length to encode \(\) and \(D\) is the metric to measure the distortion of the reconstructed image compared to the original one \(x\). Specifically for the rate, \(p_{x}\) is the (unknown) image distribution and \(p_{}\) represents the entropy model that is learned over the data distribution \(p_{x}\). A frequently used distortion measure for \(d(x,)\), is the mean squared error (MSE) or PSNR.

In practice, the latent variable \(y\) often consists of multiple levels in neural compression. Namely, a smaller one named \(z\), which is modeled with a relatively simple distribution \(p(z)\), and a larger variable, which is modeled by a distribution for which the parameters are predicted with a neural network using \(z\), the distribution \(p(y|z)\). We typically combine these two variables into a single symbol \(y\) for brevity. Furthermore, a frequent method of quantizing \(Q()\) used to train hyperpriors consists of adding uniform noise to the latent variable.

### Latent optimization

Neural image compression models have been trained over a huge set of images to find an optimal encoding. Yet, due to difficulties in optimization or due to constraints on the model capacity, model performance is sub-optimal. To overcome these issues, another type of optimizing compression performance is proposed in Campos et al. (2019); Yang et al. (2020) where they show how to find better compression results by utilizing pre-trained networks and keeping the encoder and decoder fixed but only adapting the latents. In these methods, a latent variable \(y\) is iteratively adapted using differentiable operations at test time. The aim is to find a more optimal discrete latent representation \(\). Therefore, the following minimization problem needs to be solved for an image \(x\):

\[_{}[-_{2}p_{}()+ d(x,)].\] (2)

This is a powerful method that can fit to a test image \(x\) directly without the need to further train an entire compression model.

### Stochastic Gumbel Annealing

Campos et al. (2019) proposes to optimize the latents by iteratively adding uniform noise and updating its latents. While this method proves to be effective, there is still a difference between the true rate-distortion loss (\(}\)) for the method and its discrete representation \(\). This difference is also known as the discretization gap. Therefore, Yang et al. (2020) propose the SGA method to optimize latents and show how it obtains a smaller discretization gap. SGA is a soft-to-hard quantization method that quantizes a continuous variable \(v\) into the discrete representation for which gradients can be computed. A variable \(v\) is quantized as follows. First, a vector \(_{r}=( v, v)\) is created that stacks the floor and ceil of the variable, also indicating the rounding direction. Next, the variable \(v\) is centered between \((0,1)\) where for the flooring: \(v_{L}=v- v\) and ceiling: \(v_{R}= v-v\). With a temperature rate \((0,1)\), that is decreasing over time, this variable determines the soft-to-hardness where \(1\) indicates training with a fully continuous variable \(v\) and \(0\) indicates training while fully rounding variable \(v\). To obtain unnormalized log probabilities (logits), the inverse hyperbolic tangent (\(\)) function is used as follows:

\[logits=(-\,(v_{L})/,-(v_{R})/).\] (3)

To obtain probabilities a softmax is used over the \(logits\), which gives the probability \(p(y)\) which is the chance of \(v\) being floored: \(p(y= v)\), or ceiled: \(p(y= v)\). This is approximated by the Gumbel-softmax distribution. Then, samples are drawn: \((logits,)\)Jang et al. (2016) and are multiplied and summed with the vector \(_{r}\) to obtain the quantized representation: \(=_{i}(v_{r,i}*y_{i})\). As SGA aids the discretization gap, this method may not have optimal performance and may not be as robust to changes in its temperature rate \(\).

Besides SGA, Yang et al. (2020) propose deterministic annealing Agustsson et al. (2017), which follows almost the same procedure as SGA, but instead of sampling stochastically from the Gumbel Softmax, this method uses a deterministic approach by computing probabilities with the Softmax from the \(logits\). In practice, this method has been shown to suffer from unstable optimization behavior.

### Other methods

While methods such as SGA aim to optimize the latent variables for neural image compression at inference time, other approaches have been explored in recent research. Guo et al. (2021) proposed a soft-then-hard strategy alongside a learned scaling factor for the uniform noise to achieve better compression and a smoother latent. These methods are used to fine-tune network parameters but not the latents directly. Zhu et al. (2022) proposed using Swin-transformer-based coding instead of ConvNet-based coding. They showed that these transforms can achieve better compression with fewer parameters and shorter decoding times. van Rozendaal et al. (2021) proposed to also fine-tune the decoder alongside the latent for video compression. While accommodating the additional cost of saving the model update, they demonstrated a gain of \( 1dB\). Zhang et al. (2021) and Dupont et al. (2021) proposed using implicit neural representations for video and image compression, respectively. He et al. (2022) proposed an improved context model (SCCTX) and a change to the main transform (ELIC) that achieve strong compression results together. El-Nouby et al. (2023) revisited vector quantization for neural image compression and demonstrated it performs on par with hyperprior-based methods. Li et al. (2020) proposed a method to incorporate trellis-coded quantization in neural codecs. While these approaches change the training process, our work differs in that we only consider the inference process. Balcilar et al. (2023) proposes latent shift, a method that can further optimize latents using the correlation between the gradient of the reconstruction error and the gradient of the entropy.

## 3 Methods

As literature has shown, refining the latents of pre-trained compression models with SGA leads to improved compression performance Yang et al. (2020). In this section, we extend SGA by introducing SGA+ containing three other methods for the computation of the unnormalized log probabilities (\(logits\)) to overcome issues from its predecessor. We show how these methods behave in probability space. Furthermore, we show how the methods can be extended to three-class rounding.

### Two-class rounding

Recall from SGA that a variable \(v\) is quantized to indicate the rounding direction to two classes and is centered between (0,1). Computation of the unnormalized log probabilities is obtained with \(\) from Equation (3). Recall, that in general the probabilities are given by a softmax over the _logits_ with a function of choice. As an example, for SGA the logits are computed with \(\). The corresponding probabilities for rounding down is then equal to: \((v_{L})}}{e^{(v_{L})}+e^{(v _{R})}}\). Then looking at the probability space from this function, see Figure 0(a), the \(\) function can lead to sub-optimal performance when used to determine rounding probabilities. The problem is that gradients tend to infinity when the function approaches the limits of \(0\) and \(1\), see Appendix A for the proof that gradients at 0 tend to \(\). This is not ideal, as these limits are usually achieved when the discretization gap is minimal. In addition, the gradients may become larger towards the end of optimization. Further analyzing the probability space, we find that there are a lot of possibilities in choosing probabilities for rounding to two classes. However, there are some constraints: the probabilities need to be monotonic functions, and the probabilities for rounding down (flooring) and up (ceiling) need to sum up to one. Therefore, we introduce SGA+ and propose three methods that satisfy the above constraints and can be used to

Figure 1: Probability space for (a) Two-class rounding (b) Three-class rounding

overcome the sub-optimality that the \(\) function suffers from. We opted for these three as they each have their own interesting characteristics. However, there are many other functions that are also valid and would behave similarly to these three.

We will denote the probability that \(v\) is rounded down by:

\[p(y= v),\] (4)

where \(y\) represents the random variable whose outcome can be either rounded down or up. The probability that \(v\) is rounded up is conversely: \(p(y= v)=1-p(y= v)\).

Linear probabilitiesTo prevent gradient saturation or vanishing gradients completely, the most natural case would be to model a probability that linearly increases or decreases and has a gradient of one everywhere. Therefore, we define the linear:

\[p(y= v)=1-(v- v).\] (5)

It is easy to see that: \(p(y= v)=v- v\). In Figure 0(a), the linear probability is shown.

Cosine probabilitiesAs can be seen in Figure 0(a), the \(\) tends to have gradients that go to infinity for \(v\) close to the corners. Subsequently, a method that has low gradients in that area is by modeling the cosine probability as follows:

\[p(y= v)=^{2}().\] (6)

This method aids the compression performance compared to the \(\) since there is less probability of overshooting the rounding value.

Sigmoid scaled logitThere are a lot of possibilities in choosing probabilities for two-class rounding. We introduced two probabilities that overcome sub-optimality issues from \(\): the linear probability from Equation (5), which has equal gradients everywhere, and cosine from Equation (6), that has little gradients at the corners. Besides these two functions, the optimal probability might follow a different function from the ones already mentioned. Therefore, we introduce the sigmoid scaled logit (SSL), which can interpolate between different probabilities with its hyperparameter \(a\) and is defined as follows:

\[p(y= v)=(-a^{-1}(v- v)),\] (7)

where \(a\) is the factor determining the shape of the function. SSL is exactly the linear for \(a=1\). For \(a=1.6\) and \(a=0.65\) SSL roughly resembles the cosine and \(\). For \(a\) the function tends to shape to (reversed) rounding.

Note that the main reason behind the linear version is the fact that it is the only function with constant gradients which is also the most robust choice, the cosine version is approximately mirrored across the diagonal of the \(-(x)\) which shows that it is more stable compared to the \(-(x)\), and the reason behind the SSL is that it is a function that can interpolate between all possible functions and can be tuned to find the best possible performance when necessary.

### Three-class rounding

As described in the previous section, the values for \(v\) can either be floored or coiled. However, there are cases where it may help to round to an integer further away. Therefore, we introduce three-class rounding and show three extensions that build on top of the linear probability Equation (5), cosine probability Equation (6), and SSL from Equation (7).

The probability that \(v\) is rounded is denoted by: \(p(y= v) f_{3c}(w|r,n)\), where \(w=v- v\) is centered around zero. Further, we define the probability that \(v\) is rounded \(+1\) and rounded \(-1\) is respectively given by: \(p(y= v-1) f_{3c}(w-1|r,n)\) and \(p(y= v+1) f_{3c}(w+1|r,n)\). Recall, that \(v_{L},v_{R}\), whereas \(w[-0.5,0.5]\). Defining \(w\) like this is more helpful for the 3-class since it has a center class. The general notation for the three-class functions is given by:

\[f_{3c}(w|r,n)=f((w r))^{n},\] (8)

where \(()\) clips the value at \(0\) and \(1\), \(r\) is the factor determining the height and steepness of the function and power \(n\) controls the peakedness of the function. Note that \(n\) can be fused with temperature \(\) together, to scale the function. This only accounts for the computation of the logits and not to modify the Gumbel temperature, therefore, \(\) still needs a separate definition.

Extended linearRecall that the linear probability can now be extended to three-class rounding as follows:

\[f_{linear}(w)=|w|.\] (9)

A special case is \(f_{3c,linear}(w|r=1,n=1)\), where the function is equivalent to the linear of the two-class rounding from Equation (5). For \(r<1\) this function rounds to three classes, and for \(n 1\) this function is not linear anymore.

In Figure 0(b), three-class rounding for the extension of Equation (5) can be found. As can be seen, solid lines denote the special case of two-class rounding with \(r=1\) and \(n=1\), dashed lines denote three-class rounding with \(r=0.9\) and \(n=1\) and dotted lines denote the two-class rounding with \(r=1\) and \(n=3\), which shows a less peaked function. For an example of two- versus three-class rounding, consider the case where we have variable \(v=-0.95\). For two-class rounding there is only the chance of rounding to \(-1\) with \(p(y= v)\) (red solid line), a chance to round to \(0\) with \(p(y= v+1)\) (green solid line) and zero chance to round to \(-2\) with \(p(y= v-1)\) (yellow solid line). For three-class rounding, with \(r=0.9\) and \(n=1\), when \(v=-0.95\) we find a high chance to round to \(-1\) with \(p(y= v)\) (red dashed line) and a small chance to round to \(0\) with \(p(y= v+1)\) (green dashed line) and a tiny chance to round to \(-2\) with \(p(y= v-1)\) (yellow dashed line).

Extended cosineSimilarly, we can transform the cosine probability from Equation (6) to three-class rounding:

\[f_{cosine}(w)=().\] (10)

When \(f_{3c,cosine}(w|r=1,n=2)\), this function exactly resembles the cosine for two-class rounding, and for \(r<1\) this function rounds to three classes.

Extended SSLAdditionally, SSL from Equation (7) can be transformed to three-class rounding as follows:

\[f_{SSL}(w)=(-a^{-1}(|w|)),\] (11)

where \(a\) is the factor determining the shape of the function. When \(f_{3c,SSL}(w|r=1,n=1)\), this function exactly resembles the two-class rounding case, and for \(r<1\), the function rounds to three classes. Recall that this function is capable of exactly resembling the linear function and approximates the cosine from two-class rounding for \(a=1\) and \(a=1.6\), respectively.

## 4 Experiments

In this section, we show the performance of our best-performing method in an R-D plot and compare it to the baselines. Further, we evaluate and compare the methods with the true R-D loss performance (\(}\)), the difference between the method loss and true loss (\(-}\)), and corresponding PSNR and BPP plot that respectively expresses the image quality, and cost over \(t\) training steps. Finally, we show how our best-performing method performs on the Tecnick and CLIC dataset and show qualitative results.

Following Yang et al. (2020), we run all experiments with temperature schedule \((t)=(\{-ct\},_{max})\), where \(c\) is the temperature rate determining how fast temperature \(\) is decreasing over time, \(t\) is the number of train steps for the refinement of the latents and \(_{max}(0,1)\) determines how soft the latents start the refining procedure. Additionally, we refine the latents for \(t=2000\) train iterations, unless specified otherwise. See Section 4.2 for the hyperparameter settings.

Figure 2: Performance plots of (a) True R-D Loss (b) Difference in loss (c) PSNR (d) BPP.

[MISSING_PAGE_FAIL:7]

the image compressed by SSL, we observe that there are less artifacts visible in the overall image. For instance, looking at the window we see more texture compared to the base model and \(\) method.

Hyperparameter settingsRefinement of the latents with pre-trained models, similar to the one trained in Yang et al. (2020), use the same optimal learning rate of \(0.005\) for each method. Refinement of the latents with the models of Cheng et al. (2020) use a \(10\) times lower learning rate of \(0.0005\). Following Yang et al. (2020), we use the settings for \(\) with temperature rate \(_{max}=0.5\), and for STE we use the smaller learning rate of \(0.0001\), yet STE still has trouble converging. Note that, we tuned STE just as the other baselines. However, the STE method is the only method that has a lot of trouble converging. Even with smaller learning rates, the method performed poorly. The instability of training is not only observed by us, but is also observed in Yang et al. (2020); Yin et al. (2019). For SGA+, we use optimal convergence settings, which are a fixed learning rate of \(0.0005\), and \(_{max}=1\). Experimentally, we find approximately best performance for SLL with \(a=2.3\).

## 5 Analysis and Societal Impact

In this section we perform additional experiments to get a better understanding of SGA+. An in-depth analysis shows the stability of each proposed method, followed by an experiment that expresses changes in the true R-D loss performance when one interpolates between functions. Further, we evaluate three-class rounding for each of our methods. Finally, we show how SGA+ for semi-multi-rate behavior improves the performance of its predecessor and we discuss the societal impact. Note, the results of the additional experiments are obtained from the model trained with \(=0.0075\).

    &  &  \\  &  &  &  &  \\  & Kodak & Tecnick & CLIC & Kodak & Tecnick & CLIC & Kodak & Tecnick & CLIC & Kodak & Tecnick & CLIC \\  Base vs SSL & 0.50 & 0.57 & 0.56 & 0.82 & 0.95 & 0.89 & -10.30 & -11.60 & -13.18 & -16.23 & -18.77 & -20.11 \\ Base vs Atanh & 0.26 & 0.28 & 0.31 & 0.69 & 0.79 & 0.78 & -5.52 & -5.91 & -7.37 & -13.82 & -15.93 & -17.70 \\ Atanh vs SSL & 0.24 & 0.28 & 0.26 & 0.14 & 0.16 & 0.11 & -5.04 & -5.97 & -6.20 & -2.86 & -3.34 & -2.76 \\   

Table 1: Pairwise Comparison between \(\) and SSL of BD-PSNR and BD-Rate.

Figure 4: Qualitative comparison of a Kodak image from pre-trained model trained with \(=0.0016\). Best viewed electronically.

Figure 3: R-D performance for SSL on (a) Kodak with the baselines, (b) Tecnick with the base model and \(\) and (c) Kodak for semi-multi-rate behavior with \(\). Best viewed electronically.

Temperature sensitivityTable 2 represents the stability of \(\) and the SGA+ methods, expressed in true R-D loss, for different \(_{max}\) settings for the temperature schedule. As can be seen, the most optimal setting is around \(_{max}=1\) for each of the methods. In the table we find that the linear function is least sensitive to changes in \(_{max}\). To further examine the stability of the linear function compared to \(\), we subtract the best \(_{max}\), column-wise, from the linear and \(\) of that column. Also taking into account the sensitivity results of the one of Yang et al. (2020) in Appendix C.2 we find in general, that overall performance varies little compared to the best \(_{max}\) settings of the other methods and has reasonable performance. While SSL has the largest drop in per performance when reducing \(_{max}\), it achieves the highest performance overall for higher values of \(_{max}\). If there is no budget to tune the hyperparameter of SGA+, the linear version is the most robust choice. Further, we evaluated the necessity of tuning both the latents and hyper-latents. When only optimizing the latents with the linear approach for \(_{max}=1\), we found a loss of \(0.6234\). This is a difference of \(0.0012\), which implies that optimizing the hyper-latent aids the final loss.

InterpolationTable 4 represents the interpolation between different functions, expressed in true R-D loss. In Appendix B.2 the corresponding loss plots can be found. Values for \(a<1\) indicate methods that tend to have larger gradients for \(v\) close to the corners, while high values of \(a\) represent a method that tends to a (reversed) step function. The smallest \(a=0.01\) diverges and results in a large loss value compared to the rest. For \(a=2.3\) we find a lowest loss of \(0.6175\) and for \(a=5\) we find fastest convergence compared to the rest. Comparing these model results with the model results with the one of Yang et al. (2020), see Appendix C.2, we find that the Cheng et al. (2020) model obtains more stable curves.

Three-class roundingIn Table 3, the true R-D loss for two versus three-class rounding can be found at iteration \(t=2000\) and in brackets \(t=500\) iterations. For each method, we performed a grid search over the hyperparameters \(r\) and \(n\). As can be seen in the table, most impact is made with the extended version of the linear of SGA+, in terms of the difference between the two versus three-class rounding at iteration \(t=2000\) with loss difference \(0.0045\) and \(t=500\) with \(0.0159\) difference. In Appendix B.3 a loss plot of the two- versus three- class rounding for the extended linear method can be found. Concluding, the three-class converges faster. For the extended cosine version, there is a smaller difference and for SSL we find that the three-class extension only boosts performance when run for \(t=500\). In Appendix C.2, the three-class experiments for the pre-trained model similar to Yang et al. (2020) can be found. We find similar behavior as for the model of Cheng et al. (2020), whereas with the linear version most impact is made, followed by the cosine and lastly SSL. The phenomenon that three-class rounding only improves performance for \(t=500\) iterations for SSL may be due to the fact that SSL is already close to optimal. Additionally, we ran an extra experiment to asses what percentage of the latents are assigned to the 3-classes. This is run with the best settings for the linear version \(f_{3c,}(w|r=0.98,n=2.5)\). At the first iteration, the probability is distributed as follows: \(p(y= v)=0.9329\), for \(p(y= v-1)=0.0312\), and \(p(y= v+1)=0.0359\). This indicates that the class probabilities are approximately \(3.12\) for class \(-1\) and \(3.6\) for class \(+1\). This is a lot when taking into account that many samples are taken for a large dimensional latent. In

   a & R-D Loss \\ 
0.01 & \(0.7323\) \\
0.3 & \(0.6352\) \\ \(0.65\) (approx tanh) & \(0.6260\) \\
0.8 & \(0.6241\) \\
1 (linear) & \(0.6220\) \\
1.33 & \(0.6199\) \\ \(1.6\) (approx cosine) & \(0.6186\) \\
2.3 & \(0.6175\)\(\) \\
5 & \(0.6209\) \\   

Table 4: True R-D loss results for the interpolation between different functions by changing \(a\) of the SSL.

  
**Function \(}\)** & **0.2** & **0.4** & **0.6** & **0.8** & **1.0** \\  \((v)\) & \(0.6301\) & \(0.6273\) & \(0.6267\) & \(0.6260\) & \(0.6259\) \\ \(1-v\) (linear) & \(0.6291\)\(\) & \(0.6229\)\(\) & \(0.6225\) & \(0.6222\) & \(0.6220\) \\ \(^{(2)}\) & \(0.6307\) & \(0.6233\) & \(0.6194\) & \(0.6186\) & \(0.6187\) \\ \(^{(-a^{-1})}(v)\) & \(0.6341\) & \(0.6233\) & \(0.6196\) & \(0.6181\) & \(\) \\  \((v)\) & \(0.0010\) & \(0.0044\) & \(0.0073\) & \(0.0079\) & \(0.0084\) \\ \(1-v\) (linear) & \(0\) & \(0\) & \(0.0031\) & \(0.0041\) & \(0.0045\) \\   

Table 2: True R-D loss for different \(_{max}\) settings of: \((v)\), linear, cosine and SSL with \(a=2.3\). The lowest R-D loss per column is marked with: \(\). Note that the function containing \(\) is unnormalized.

  
**Function Rounding** & Two & Three \\  \(f_{3c,}(w|r=0.98,n=2.5)\) & \(0.6220\)\( 0.6520\) & \(0.6175\)\( 0.6425\) \\ \(f_{3c,}(w|r=0.98,n=3)\) & \(0.6187\)\( 0.6516\) & \(0.6175\)\( 0.6449\) \\ \(f_{3c,}(w|r=0.93,n=2.5)\) & \(0.6175\)\( 0.6489\) & \(0.6203\)\( 0.6540\) \\   

Table 3: True R-D loss of two- versus three-class rounding for SGA+ with the extended version of the linear, cosine, and SSL method at iteration 2000 and in brackets after 500 iterations.

conclusion, three-class rounding may be attractive under a constraint optimization budget, possibly because it is easier to jump between classes. Additionally, for the extended linear three-class rounding also results in faster convergence.

Semi-multi-rate behaviorAn interesting observation is that one does not need to use the same \(\) during refinement of the latents, as used during training. This is also mentioned in Gao et al. (2022) for image and Xu et al. (2023) for video compression. As a consequence of this approach, we can optimize to a neighborhood of the R-D curve without the need to train a new model from scratch. We experimented and analyze the results with methods: \(\) and SSL. Figure 2(c) shows the performance after \(t=500\) iterations for the model of Cheng et al. (2020). We find that SSL is moving further along the R-D curve compared to \(\). Note the refinement does not span the entire curve and that the performance comes closer together for running the methods longer, see Appendix B.4. For future work it would be interesting how SGA+ compares to the methods mentioned in Gao et al. (2022); Xu et al. (2023), since SSL outperforms \(\).

Societal impactThe improvement of neural compression techniques is important in our data-driven society, as it allows quicker development of better codecs. Better codecs reduce storage and computing needs, thus lowering costs. However, training these codes requires significant computational resources, which harms the environment through power consumption and the need for raw materials.

## 6 Conclusion and Limitations

In this paper we proposed SGA+, a more effective extension for refinement of the latents, which aids the compression performance for pre-trained neural image compression models. We showed how SGA+ has improved properties over SGA and we introduced SSL that can approximately interpolate between all of the proposed methods. Further, we showed how our best-performing method SSL outperforms the baselines in terms of the R-D trade-off and how it also outperforms the baselines on the Tecnick and CLIC dataset. Exploration of SGA+ showed how it is more stable under varying conditions. Additionally, we gave a general notation and demonstrated how the extension to three-class rounding improves the convergence of the SGA+ methods. Lastly, we showed how SGA+ improves the semi-multi-rate behavior over SGA. In conclusion, especially when a limited computational budget is available, SGA+ offers the option to improve the compression performance without the need to re-train an entire network and can be used as a drop-in replacement for SGA.

Besides being effective, SGA+ also comes with some limitations. Firstly, we run each method for 2000 iterations per image. In practice this is extremely long and time consuming. We find that running the methods for 500 iterations already has more impact on the performance and we would recommend doing this, especially when a limited computational budget is available. Future work may focus on reducing the number of iterations and maintaining improved performance. Note, higher values for \(a\) flatten out quickly, but they achieve much better gains with low-step budgets. Further, the best results are obtained while tuning the hyperparameter of SSL and for each of our tested models this lead to different settings. Note, that the experiments showed that the linear version of SGA+ is least sensitive to hyperparameter changes and we would recommend using this version when there is no room for tuning. Additionally, although three-class rounding improves the compression performance in general, it comes with the cost of fine-tuning extra hyperparameters. Finally, it applies for each method that as the temperature rate has reached a stable setting, the performance will be less pronounced, the longer you train, but in return requires extra computation time at inference.