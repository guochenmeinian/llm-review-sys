# UMFC: Unsupervised Multi-Domain Feature Calibration for Vision-Language Models

Jiachen Liang\({}^{1,2}\), Ruibing Hou\({}^{1}\)1, Minyang Hu\({}^{1,2}\), Hong Chang\({}^{1,2}\), Shiguang Shan\({}^{1,2}\), Xilin Chen\({}^{1,2}\)

\({}^{1}\) Institute of Computing Technology, Chinese Academy of Sciences

\({}^{2}\)University of Chinese Academy of Sciences

{jiachen.liang, minyang.hu}@vipl.ict.ac.cn, {houruibing, changhong, sgsshan, xlchen}@ict.ac.cn

###### Abstract

Pre-trained vision-language models (_e.g._, CLIP) have shown powerful zero-shot transfer capabilities. But they still struggle with domain shifts and typically require labeled data to adapt to downstream tasks, which could be costly. In this work, we aim to leverage unlabeled data that naturally spans multiple domains to enhance the transferability of vision-language models. Under this unsupervised multi-domain setting, we have identified inherent model bias within CLIP, notably in its visual and text encoders. Specifically, we observe that CLIP's visual encoder tends to prioritize encoding domain over discriminative category information, meanwhile its text encoder exhibits a preference for domain-relevant classes. To mitigate this model bias, we propose a _training-free_ and _label-free_ feature calibration method, Unsupervised Multi-domain Feature Calibration (UMFC). UMFC estimates image-level biases from domain-specific features and text-level biases from the direction of domain transition. These biases are subsequently subtracted from original image and text features separately, to render them domain-invariant. We evaluate our method on multiple settings including transductive learning and test-time adaptation. Extensive experiments show that our method outperforms CLIP and performs on par with the state-of-the-arts that need additional annotations or optimization. Our code is available at [https://github.com/GIT-LJc/UMFC](https://github.com/GIT-LJc/UMFC).

## 1 Introduction

Recently, Vision-Language Foundation Models (VLFMs) such as CLIP , BLIP , Flamingo  and ALIGN  have demonstrated remarkable performance across various downstream tasks. These VLFMs formulate the training objective as contrastive learning, leveraging millions of image-text pairs to establish a shared embedding space. Equipped with a wide range of visual and text representations, VLFMs exhibit the capability to tackle downstream tasks in a zero-shot manner.

Despite VLFMs being exposed to abundant examples, they may still encounter examples with new variations in downstream tasks. To address the problem of distribution shift between the pre-training and downstream domains, a natural approach involves fine-tuning VLFMs on various target tasks, such as prompt engineering  and adapter learning . However, these methods generally require labeled samples for fine-tuning, which is prohibitively expensive to be satisfied in reality. Conversely, abundant unlabeled data are often available for downstream tasks. Notably, in practical scenarios, the unlabeled data typically contains multiple domains, which exacerbates the adaptation difficulty of VLFMs. Therefore, in this paper, we aim to improve the adaptation performance of VLFMs directly on unlabeled data that naturally spans multiple domains.

In this unsupervised multi-domain setting, we observe that CLIP cannot perform well when unlabeled data are drawn from mixed distributions. As shown in Figure 1(a), even within the same class space, the accuracy of CLIP varies significantly across different domains. While CLIP performs exceptionally well for images from common distributions encountered during pre-training, _e.g._, achieving \(83.0\%\) accuracy on _real_ domain, it struggles with rare distributions encountered during pre-training, _e.g._, only achieving \(14.2\%\) accuracy on _quickdraw_ domain. Above observations highlight that CLIP exhibits _model biases_ that lead to incorrect predictions in specific scenarios. This raises a fundamental question: _where do these biases originate_?

We point out that these model biases stem from deficiencies in the visual encoder and textual encoder. _On the side of visual encoder, we observe that CLIP' visual encoder prioritizes encoding domain information over discriminative category information._ As shown in Figure 1(b), features from the same domain clearly cluster together, whereas a notable gap separates features from different domains. This phenomenon indicates that CLIP's visual encoder exhibits a higher sensitivity to domain information over category information. When the domain of downstream tasks shifts from pre-trained tasks, the mismatch in domain-specific information encoded in image features could adversely affect classification accuracy. _On the side of textual encoder, we observe that CLIP demonstrates varying category preferences across different domains._ Specifically, CLIP tends to classify images into categories whose name are closely related to corresponding domain. As shown in Figure 1(c), within "quickdraw" domain, a large portion of samples (\( 30\%\)) are classified as "squiggle" or "line" categories. Conversely, within "painting" domain, CLIP favors categories like "paintcan" and "paintbrush". This observation shows that the class embeddings encoded by CLIP's textual encoder inherently carry domain-specific information, misleading the model to prioritize categories highly associated with respective domains. Due to the combined effects of visual and textual encoder biases, CLIP's performances vary significantly across different domains.

In this paper, we aim to calibrate CLIP to mitigate the model biases. Initially, we analyze the model biases from a probabilistic standpoint. The factors influencing \(p(y|x)\), affected by domain variable \(z\), can be decoupled into two parts: the sample distribution conditioned on classes \(p(x|y,z)\) and the class distribution \(p(y|z)\). If the two probability distributions are independent of \(z\), domain shifts will not affect the predictions. To this end, we propose **Unsupervised Multi-domain Feature Calibration** (UMFC), a simple yet efficient framework for calibrating CLIP to generalize to various downstream tasks using multi-domain unlabeled data. UMFC jointly calibrates CLIP through two _training-free_ modules: Image Feature Calibration module (IFC) and Text Feature Calibration module (TFC). **Firstly**, IFC focuses on calibrating CLIP's image encoder to prioritize category-level over domain-level information, thereby reducing prediction error caused by domain shifts. Specifically, we calculate the average image features for each domain \(i\), denoted as \(_{i}\), and posit that the prediction of \(_{i}\) reflects the inherent bias of CLIP's image encoder within that domain. By subtracting this domain-specific bias from original predictions, we can derive domain-agnostic predictions. **Secondly**, IFC focuses on calibrating CLIP's text encoder to remove its preference towards domain-related class names. As observed by [7; 28; 8], a "global direction" exists in CLIP, representing the shift from training distribution to unseen distribution, shared across image and text embedding spaces. Motivated by this observation, we utilize the shift direction between different-domain images to estimate the text shift direction. Then, we subtract this shift vector to counteract the text encoder's bias towards domain-related categories. By combing IFC and TFC, we can calibrate the features on both CLIP's image and text encoders, thereby alleviating the model bias in downstream tasks.

Figure 1: On DomainNet dataset, we visualize (a) The accuracy of CLIP on the six domains. (b) The image features extracted by CLIP’s image encoder across different domains. The visualization show that CLIP exhibits inherent model bias. (c) The number of predictions for different classes on _quickdraw_ and _painting_ domains.

We validate the efficacy of UMFC on three downstream tasks: unsupervised calibration, transductive learning, and test-time adaptation, demonstrating consistent gains over CLIP. UMFC presents a low-cost solution for classification, unlocking the potential of CLIP-like models for practical scenarios characterized by abundant images across multiple domains but scarce labels.

## 2 Related Work

**Few-Shot Learning.** Few-shot learning (FSL) [17; 16; 18; 10; 34] aims to learn a good model on a novel task with few labeled samples. Traditional FSL methods [17; 16; 18; 10; 34] often rely on abundant related training tasks for pre-training and design specialized algorithms to transfer cross-task knowledge, facilitating task adaptation. Additionally, some semi-supervised learning (SSL) methods [26; 14; 15] tackle the scarcity of labeled samples by assuming access to extensive unlabeled data. While both FSL and SSL methods have achieved encouraging results, their generalization abilities are limited. Recently, the CLIP model is proposed, which is a vision-language foundation model that learns a shared vision-language embedding space. As pre-trained on large-scale data, CLIP exhibits impressive zero-shot ability across various downstream tasks. Based on CLIP model, many works focus on improving its performance further on downstream tasks with few labeled data. For example, CoOp , CoCoOp , MaPLe  and PromptSRC  leverage few labeled samples to learn the prompt in the continual input embedding space, offering a parameter-efficient way of fine-tuning foundation models. Similarly, CLIP-Adapter  and Tip-Adapter-F  introduce a lightweight adapter module to produce adaptive multi-modal features. However, these methods require extra labeled data and the process of tuning pre-trained parameters for CLIP, which is cost-expensive. Differently, in this work, we focus on utilizing unlabeled data to enhance CLIP's performance in a training-free manner.

**Domain Adaptation / Domain Generalization.** Recently, several methods [23; 3; 8; 31; 2; 33; 5; 19; 7] exploit a large-scale pre-trained model (_e.g._, CLIP) to address the domain adaptation and generalization problems. For example, RISE  leverages CLIP as a teacher to regularize the student's learned representation through images. The work  utilizes domain-invariant and domain-specific prompts for multi-source unsupervised domain adaptation. Other works [8; 7; 5] focus on utilizing the transferability between visual and textual modalities to guide domain information transfer. PromptStyler  attempts to simulate various distribution shifts to explore diverse styles in a joint vision-language space. PODA  and LADS  generate samples in the style of the target domain, and adapt to the target domain based on these samples. However, these language-guided methods require the prior of target-domain names, which may not be satisfied in reality. In contrast, our UMFC method does not require any extra information about the target domain.

**Test-Time Adaptation.** Test-time adaptation aims to adapt a pre-trained model to test tasks, where distribution of test data differs from that of pre-training data. TPT  proposes a test-time prompt tuning strategy, which extends traditional TTA methods to vision-language models. Building upon TPT, DiffTPT  utilizes pre-trained diffusion models to augment the diversity of test data samples used in prompt tuning. SwapPrompt  employs a dual prompts paradigm to enhance the swapped prediction mechanism. However, these prompt learning methods are computationally expensive and time-consuming. Different from above methods, our UMFC only needs to calibrate features in a training-free way, making it more efficient for test-time adaptation.

## 3 CLIP and Model Biases

In this section, we first describe the backgrounds of CLIP and then analyze its bias issue in downstream tasks. We attribute the cause of bias to the visual encoder bias and text encoder bias.

### Contrastive Language-Image Pre-training (CLIP)

CLIP  consists of two parallel encoders: a visual encoder that maps image inputs into image features, and a text encoder that maps text inputs into text features. The model is trained with a contrastive loss that maximizes similarity between positive image-text pairs while minimizing similarity between negative pairs. This process ensures alignment between the features of images and their corresponding textual descriptions within the feature space. Trained on a vast collection 

[MISSING_PAGE_FAIL:4]

\(p(y_{i}|x)\) is equal to maximize the summation \(_{z}p(x|y_{i},z)p(y_{i}|z)\), as

\[ p(y_{i}|x) =p(x|y_{i},z)p(y_{i}|z)p (z)}{p(x)}\] \[=_{z}p(x|y_{i},z)p(y_{i}|z). \]

Equation 3 shows that the domains affect posterior probability distribution \(p(y|x)\) by disturbing two terms: the sample distribution conditioned on classes \(p(x|y,z)\) and the class distribution \(p(y|z)\).

### UMFC: Unsupervised Multi-domain Feature Calibration

As analyzed in Section 3.2, CLIP exhibits both visual and text encoder bias, impacting its generalization ability to downstream tasks. As shown in Equation 3, these biases stem from the probability distributions \(p(x|y,z)\) and \(p(y|z)\), which are disturbed by domain information. To mitigate the model biases, an intuitive idea is to make the two probability distributions \(p(x|y)\) and \(p(y)\) independent of domain variable \(z\) by calibrating image and text features. To this end, we propose _training-free_ UMFC approach, consisting of an Image Feature Calibration (IFC) module to alleviate visual encoder bias and Text Feature Calibration (TFC) module to alleviate text encoder bias, facilitating the transfer of CLIP to downstream tasks. Algorithms are provided in the Appendix C.

Image Feature Calibration Module.On the side of visual encoder, we focus on making the conditional probability \(p(x|y)\) independent of domain variable \(z\), which is equivalent to achieving \(p(x|y,z)=p(x|y)\). A straightforward method is to align image feature distributions given a class \(y\) across different domains. Unfortunately, as only mixed unlabeled data is provided, we cannot access to class labels and domain labels of images. However, we empirically observe that CLIP's visual encoder prioritizes encoding domain information over discriminative category information. Thus, we can distinguish image features from different domains through a simple clustering algorithm. Formally, we assume that there are \(M\) clusters \(\{c_{1},...,c_{M}\}\) after applying a clustering algorithm. Each cluster is assumed to follow a Gaussian distribution \(c_{i}(_{i},_{i})\), where mean vector \(_{i}=(\|}_{f c_{i}}f)\) represents the average of image features from cluster \(c_{i}\). Due to the model biases, the pseudo labels produced by zero-shot CLIP are not reliable. Therefore, we directly align the margin image feature distribution of each cluster by subtracting the corresponding mean vector2. Specifically, for each visual feature \(f\) belonging to the cluster \(c_{i}\), we calibrate it as follows:

\[f^{}=}{\|f-_{i}\|_{2}}. \]

Text Feature Calibration Module.On the side of text encoder, we focus on making the class probability \(p(y)\) independent of domain variable \(z\), which is equivalent to achieving \(p(y|z)=p(y)\). Previous works  have found that CLIP's performance is sensitive to class name \(y\). For example, replacing category names with synonyms or near-synonymous terms can significantly impact CLIP's prediction results. Due to the influence of domain information in image features, text encoder bias can cause CLIP to categorize domain-specific images into categories whose names are semantically similar to that domain. We further observe that such sensitivity to class names varies across different domains, as shown in Figure 1(c). This observation inspires us to calibrate text features by removing domain-specific information. However, a challenge arises as domain labels are unavailable. To address this issue, we attempt to extract domain information from unlabeled images, and then transfer this domain information to the text embedding space to estimate the text bias.

**Observation 4.1**: _Cross-Modality Transition Direction. The underlying assumption behind using images to simulate the corresponding shifts in texts is that the transition direction from domain \(i\) to domain \(j\) is consistent across both the image embedding and text embedding spaces , which can be formulated as:_

\[)-F(x^{j})}{\|F(x^{i})-F (x^{j})\|_{2}}^{i};y^{i}) -T(^{j};y^{j})}{\|T(^{i};y^{i}) -T(^{j};y^{j})\|_{2}}, \]_where \((x^{i},y^{i})\) and \((x^{j},y^{j})\) represent training samples from domain \(i\) and domain \(j\) respectively, \(p^{i}\) and \(p^{j}\) denote the domain-specific text prompts for domain \(i\) and \(j\) respectively. For example, the text prompt of "quickdraw" domain can be "a quickdraw image of a [class]"._

Inspired by Observation 4.1, we estimate the text-level domain transition direction using different-domain images. By clustering image features, we calculate the average feature \(_{i}\) of unlabeled images from each domain \(i\), representing the domain-specific feature for that domain. Also, we calculate the average feature of all unlabeled images \(_{}\), representing the domain-invariant feature since it encompasses various domain distributions. Following Equation 5, the transition information \(}\) of domain \(i\) can be computed as \(}=_{i}-_{}\). By subtracting this domain transition vector, we suppress the preference for specific class names in the original text features. To ensure that the calibrated text features effectively eliminate biases towards a wide range of domains, we integrate the text features calibrated on each domain. Specifically, for the text feature \(t_{j}\) of class \(j\), we calibrate it as follows:

\[t^{}_{j}=_{i=1}^{M}-}}{\|t _{j}-}\|_{2}}, \]

where \(M\) is the number of clusters.

**Inference.** After calibration with IFC and TFC modules, we obtain the final prediction results. Specifically, this calibration modifies the prediction probability on test image \(x\) as follows:

\[p(y_{i}|x)=,t^{}_{i} )/)}{_{j=1}^{K}((f^{},t^{}_ {j})/)}. \]

## 5 Experiments

### Experimental Setting

**Datasets.** Our UMFC is _training free_, which only calibrates the image and text features using Equation 4 and 6. To analyze model's generalization capability, we use two large-scale datasets for evaluation: 1) _DomainNet_ consists of 569,010 images with 345 categories from six domains: Clipart (C), Infograph (I), Painting (P), Quickdraw (Q), Real (R), Sketch (S). 2) _ImageNet Variants_ composed of several datasets shifted from ImageNet, including ImageNet-A (IN-A) , ImageNet-R (IN-R) , and ImageNet-Sketch (IN-S) . We form the class space for _ImageNet Variants_ by taking the union of the class sets in IN-A and IN-R. To ensure the reliability of the evaluation results, we randomly sample the test data to construct a balanced test set where both domain and category distributions are uniform.

**Evaluation Paradigms.** Our approach is a universal feature calibration technique that can be applied across multiple scenarios. In this work, we explore three settings: 1) _Unsupervised Calibration_ (UC) where the unlabeled training set is provided for computing the calibration vectors; 2) _Transductive Learning_ (TL) where the entire unlabeled test set is provided at once, without providing any training data; 3) _Test-Time Adaptation_ (TTA) where the model can be adapted to test samples shifted from training distribution. Different from TL, the test data usually arrives in batches in TTA setting. More details on the experimental setup, please refer to Appendix D.

### Main Results on Unsupervised Calibration.

**Implementation Details.** We select CLIP  as our pre-trained vision-language model. We use CLIP with ViT-B/16  as image encoder, and keep the original transformer as the text encoder. By default, a fixed prompt, "a photo of a [class]", is employed for all datasets. The images are resized to \(224 224\). The hyper-parameter \(M\) (cluster number) is set to \(6\) for DomainNet. Remarkably, our method is training-free where both image encoder and text encoder remain frozen throughout the entire pipeline. All experiments are performed on a GeForce RTX 3090 Ti GPU.

**Baselines.** We compare our method with four groups of methods: (1) CLIP and its variants to show zero-shot predictions: CLIP  that uses a fixed prompt "a photo of a [class]"; CLIP-E  that uses an ensemble of prompt templates. (2) CLIP-D  that utilizes the domain information of test samples and designs domain-specific prompts. (3) Few-shot learning methods: CoOp  that performs prompt tuning using labeled data of downstream tasks; CLIP-Adapter  that trains an adapter using task-specific labeled data. (4) Unsupervised Fine-tuning method: MUST  that fine-tunes the model using unlabeled multi-domain data.

**Analysis.** As shown in Table 1, UMFC achieves superior performance over CLIP and competitive performance with few-shot methods, CoOp  and CLIP-Adapter . We can observe that: **(1)** CLIP-D incorporates domain information into text prompts, such as "a clipart image of a [class]' for test samples from "clipart" domain. However, creating domain-specific templates is challenging due to unknown and potentially mixed domain sources of test samples. So, we only use CLIP-D as an oracle result. As shown in Table 1, our method can achieve competitive performance with CLIP-D without prior domain labels for test samples. **(2)** On domains where CLIP performs poorly, _e.g._, "quickdraw", our UMFC significantly outperforms others. Compared to CLIP, UMFC achieves about \(5\%\) performance gain on quickdraw domain. In addition, combined with more diverse prompts of CLIP-E, UMFC further improves performance. **(3)** MUST  uses abundant unlabeled data from 6 domains for fine-tuning. Our UMFC achieves better performance than MUST even without any additional training. **(4)** For CoOp  and CLIP-Adapter , we fine-tune them using labeled samples from multiple domains. Specifically, we provide \(k\) labeled samples per class for each domain, resulting in a total of \((6 k 345)\) labeled samples available for training. When the number of labeled samples is \(6 345\), our method outperforms few-shot fine-tuning methods. While the performance is higher for few-shot methods with a larger number of labeled samples (_i.e._, \(24 345\)), it is essential to highlight the challenges in obtaining some labeled data for each class and each domain in real-world scenarios. In contrast, our method does not require selecting class-balanced and domain-balanced labeled samples and incurs no additional training overhead.

Table 2 provides \(8 345\) labeled samples from a single domain for CoOp fine-tuning and an equal number of unlabeled samples for UMFC calibration. While CoOp trained on a single domain can improve performance within that domain, its performance declines on other domains. This decline becomes particularly notable when a significant distribution gap between the training and test domains exists, _e.g._, CoOp (Q), leading to a large decrease in average performance across multiple domains. In contrast, our method achieves consistent performance improvements on both training and unseen domains with the same amount of training data. Additionally, unlike CoOp, UMFC does not require labeled data or any parameter fine-tuning.

### Main Results on Transductive Learning

In this part, we compare our UMFC with three groups of methods: (1) Zero-Shot CLIP models. (2) Domain Generalization (DG) method: MIRO  that trains CLIP on available data to learn great generalization capability. (3) Data-Free DG method: PromptStyler  that learns to simulate diverse distributions. The hyper-parameter \(M\) (cluster number) is set to 3 for ImageNet-Variants. The compared results are shown in Table 3 and Table 4. As seen, our UMFC can achieve the best average performance on DomainNet and ImageNet-Variants benchmarks, which validates the effectiveness of our approach in transductive learning.

    & Method & C & I & P & Q & R & S & Avg \\   & CLIP  & 71.21 & 49.47 & 64.61 & 14.23 & 82.98 & 64.81 & 57.88 \\  & CLIP-E  & 73.16 & 54.17 & 67.02 & 15.86 & 84.30 & 67.49 & 60.33 \\   & CLIP-D  & 73.90 & 55.84 & 67.75 & 17.84 & 83.26 & 67.56 & 61.03 \\   & MUST  & 74.83 & 56.48 & 61.80 & 19.06 & 82.88 & 70.31 & 60.89 \\   & **UMFC (ours)** & 73.02 & 55.30 & 66.36 & 19.67 & 83.54 & 66.87 & 60.79 \\   & **UMFC + CLIP-E** (ours) & 73.84 & 56.59 & 67.39 & 20.03 & 84.33 & 67.90 & **61.68** \\   & CoOp (\(6 1 345\))  & 72.73 & 53.95 & 66.80 & 19.58 & 82.53 & 67.27 & 60.48 \\  & CoOp (\(6 4 345\))  & 74.7 & 54.96 & 68.29 & 22.14 & 82.94 & 69.48 & 62.09 \\   & CLIP-Adapter (\(6 1 345\))  & 72.67 & 51.69 & 67.84 & 17.82 & 84.26 & 65.75 & 60.00 \\   & CLIP-Adapter (\(6 4 345\))  & 74.35 & 53.79 & 69.94 & 19.71 & 85.26 & 66.90 & 61.66 \\   

Table 1: Results on DomainNet under multi-domain Unsupervised Calibration. CLIP denotes zero-shot CLIP with a fixed text prompt template “a photo of a [class]”, CLIP-E uses the ensemble prompt templates designed for Imagenet , CLIP-D uses the domain-specific templates, _i.e._, “a [domain] image of [class]”. CoOp and CLIP-Adapter are trained on multi-domain labeled data, _e.g._, \(6 1 345\) denotes the number of labeled data.

### Main Results on Test-Time Adaptation

Implementation Details.In TTA setting, where we cannot access all data simultaneously, we adopt an incremental clustering approach. By default, the batch size is set to 100. Initially, we apply K-Means clustering algorithm to the first batch of data. For subsequent batches, we use the prototype classification, with cluster centers serving as prototypes, to assign samples in that batch to different clusters. Then, the cluster centers and calibration statics \(\{_{i}\}_{i=1}^{M}\) are updated accordingly. We have developed two strategies for updating the calibration statics: **UMFC-Memory** that stores the feature information of each batch to calculate the statistical information; **UMFC-EMA** that uses Exponential Moving Average (EMA) to update statistical information.

Analysis.We mainly compare our UMFC with TPT  in test-time adaptation, where TPT fine-tunes the prompts by minimizing the entropy of the predictions. The comparison results are shown in Table 6 and Table 5. We can observe that: **(1)** As shown in Table 6, UMFC achieves performance gains across all domains, with particularly noticeable gains in those where CLIP performs poorly. For example, UMFC substantially improves upon CLIP on "quickdraw" and "infograph" domains, with

   Method & C & I & P & Q & R & S & Avg \\  CLIP  & 71.21 & 49.47 & 64.61 & 14.23 & 82.98 & 64.81 & 57.88 \\ TPT  & 73.23 & 52.63 & 68.00 & 12.79 & 84.39 & 66.68 & 59.62 \\ 
**UMFC-Memory** & 72.82 & 55.12 & 66.82 & 19.92 & 83.62 & 66.82 & **60.85** \\
**UMFC-EMA** & 72.99 & 54.94 & 66.64 & 18.58 & 83.54 & 66.75 & 60.57 \\   

Table 6: Comparison Results on DomainNet under Test-Time Adaptation. UMFC-Memory and UMFC-EMA represent different ways to update the statics vectors for calibration.

   Method & C & I & P & Q & R & S & Avg \\  CLIP  & 71.21 & 49.47 & 64.61 & 14.23 & 82.98 & 64.81 & 57.88 \\ CLIP-E  & 73.16 & 54.17 & 67.02 & 15.86 & 84.30 & 67.49 & 60.33 \\ CLIP-D  & 73.90 & 55.84 & 67.75 & 17.84 & 83.26 & 67.56 & 61.03 \\ MIRO  & - & - & - & - & - & - & 54.00 \\ PrompStyle & 73.10 & 50.90 & 69.20 & 13.30 & 85.40 & 65.30 & 59.40 \\ 
**UMFC** & 73.01 & 55.44 & 66.89 & 20.14 & 83.66 & 67.51 & **61.11** \\   

Table 3: Comparison Results on DomainNet under Transductive Learning.

   Method & C & I & P & Q & R & S & Avg \\  CLIP  & 71.21 & 49.47 & 64.61 & 14.23 & 82.98 & 64.81 & 57.88 \\ TPT  & 73.23 & 52.63 & 68.00 & 12.79 & 84.39 & 66.68 & 59.62 \\ 
**UMFC-Memory** & 72.82 & 55.12 & 66.82 & 19.92 & 83.62 & 66.82 & **60.85** \\
**UMFC-EMA** & 72.99 & 54.94 & 66.64 & 18.58 & 83.54 & 66.75 & 60.57 \\   

Table 2: Results on DomainNet under single-domain Unsupervised Calibration. \(8 345\) samples (each class has \(8\) samples) from a single domain are provided. CoOp (C/Q/I) and UMFC (C/Q/I) denote training samples for CoOp and UMFC from the “Clipart/”/Quickdraw/”Infograph” domains, respectively.

   Method & C & I & P & Q & R & S & Avg \\  CLIP  & 71.21 & 49.47 & 64.61 & 14.23 & 82.98 & 64.81 & 57.88 \\ CLIP-E  & 73.16 & 54.17 & 67.02 & 15.86 & 84.30 & 67.49 & 60.33 \\ CLIP-D  & 73.90 & 55.84 & 67.75 & 17.84 & 83.26 & 67.56 & 61.03 \\ MIRO  & - & - & - & - & - & - & 54.00 \\ PrompStyle & 73.10 & 50.90 & 69.20 & 13.30 & 85.40 & 65.30 & 59.40 \\ 
**UMFC** & 73.01 & 55.44 & 66.89 & 20.14 & 83.66 & 67.51 & **61.11** \\   

Table 4: Comparison Results on ImageNet-Variants under Transductive Learning.

   Method & IN-A & IN-R & IN-S & Avg \\  CLIP  & 42.13 & 66.95 & 74.58 & 61.22 \\ TPT  & 47.16 & 59.95 & 67.49 & 58.20 \\ 
**UMFC-Memory** & 42.76 & 67.03 & 75.15 & 61.65 \\
**UMFC-EMA** & 42.21 & 67.82 & 76.26 & **62.10** \\   

Table 5: Comparison Results on ImageNet-Variants under Test-Time Adaptation.

an accuracy gain of \(5.6\%\). Table 5 shows that the improvement on ImageNet-A is less significant than on ImageNet-R and ImageNet-S. The reason may be attributed to the absence of distinct domain styles in ImageNet-A, limiting the effectiveness of our calibration method that relies on domain information. **(2)** UMFC-Memory and UMFC-EMA with different statistical update strategies exhibit similar performance. However, UMFC-EMA, which updates features based on the most recent data, notably reduces storage requirements. Moreover, UMFC exhibits significantly higher computational efficiency without training, whereas TPT requires executing optimization steps on \(64\) different augmented views of each test image. Thus, our method is more suitable for rapid deployment.

### Ablation Study

#### 5.5.1 The effectiveness of IFC.

We firstly evaluate the effectiveness of IFC. As shown in Table 7, IFC individually contributes to performance gains of CLIP, about \(3\%\) average gains. Furthermore, Figure 1(b) and Figure 2(a) visualize the image features extracted by CLIP with/without IFC respectively. As shown in Figure 1(b), the vanilla CLIP maps different-domain images to different clusters in the feature space. Conversely, Figure 2(a) shows that IFC leads to the merge of image features from different domains, validating its effectiveness of reducing domain-specific information.

#### 5.5.2 The effectiveness of TFC.

As shown in Table 7, TFC also individually contributes to performance gains of CLIP. To assess the effectiveness of calibrated text features in eliminating domain bias, we construct a domain classifier. Specifically, we utilize the text features generate by CLIP, with domain prompts "[domain]", as the domain classifer. Then, we use this domain classifier to perform domain classification for the text features before and after calibration (_i.e._, computing the cosine similarity between domain classifier and text features). As shown in Figure 2(b), the original text features exhibit a long-tail phenomenon, with a higher probability of being classified into the "real" domain. This suggests that the original text features are biased towards the "real" domain, overlooking their generalization ability to other domains. However, after calibration with TFC, the text features exhibit a near-uniform distribution across domains, indicating that the calibrated text features have largely eliminated domain bias. Consequently, our method enhances performance in other domains without compromising the model's performance in its strongest domain. As shown in Table 7, the combination of IFC and TFC (_i.e._, UMFC) can further bring performance gains, which validates the complementary of image-level calibration and text-level calibration for CLIP.

#### 5.5.3 The impact of cluster number \(M\).

Our method involves clustering the unlabeled data to determine their respective clusters. We evaluate our method with respect to the number of clusters \(M\). As shown

   Method & C & I & P & Q & R & S & Avg \\  CLIP  & 71.21 & 49.47 & 64.61 & 14.23 & 82.98 & 64.81 & 57.88 \\  IFC & 72.98 & 55.07 & 66.65 & 19.87 & 83.54 & 66.97 & 60.85 \\ TFC & 71.44 & 49.99 & 65.59 & 13.9 & 83.25 & 64.68 & 58.14 \\ 
**UMFC** & 73.01 & 55.44 & 66.89 & 20.14 & 83.66 & 67.51 & **61.11** \\   

Table 7: Ablation study on the effects of TFC and IFC under Transductive Learning.

Figure 2: On DomainNet dataset, we visualize (a) The image features extracted by UMFC image encoder across different domains. (b) The classification probabilities of CLIP’s text features on different domains.

in Table 8, our UMFC consistently outperforms vanilla CLIP, even when the number of clusters \(M\) does not match the number of domains (6 for DomainNet). More importantly, our method is not sensitive to changes in \(M\). Refer to the Appendix E for more analysis.

The impact of batch size in TTA setting.We present results across various batch sizes during test-time adaptation to confirm that UMFC is robust to batch size variations. When the sample count is initially lower than the number of clusters \(M\), K-Means clustering cannot be directly applied. To address this, we used the first \(M\) samples as the initial cluster centers and then proceeded with the same test-time adaptation. As shown in Table 9, even in the extreme case of a batch size of 1, our method still demonstrates consistent improvement.

## 6 Conclusion

In this work, we point out that the model biases hinder the transfer ability of pre-trained vision-language models. We develop UMFC, a simple unsupervised calibration method that mitigates model biases in both visual encoder and text encoder through a training-free manner. We demonstrate the effectiveness of our method across multiple settings, including unsupervised calibration, transductive learning, and test-time adaptation. Without the need for any annotations and training, UMFC improves the zero-shot generalization ability of CLIP.

Limitations and Broader Impacts.Our method requires unlabeled samples from the target domain to establish domain-level calibration vectors. Although our method is much cheaper and more accessible than fine-tuning or few-shot methods , the requirement of unlabeled data can still be a limitation for some specific scenarios.