# Segment Anything without Supervision

XuDong Wang Jingfeng Yang Trevor Darrell

UC Berkeley

code: https://github.com/frank-xwang/UnSAM

###### Abstract

The Segmentation Anything Model (SAM) requires labor-intensive data labeling. We present Unsupervised SAM (UnSAM) for promptable and automatic whole-image segmentation that does not require human annotations. UnSAM utilizes a divide-and-conquer strategy to "discover" the hierarchical structure of visual scenes. We first leverage top-down clustering methods to partition an unlabeled image into instance/semantic level segments. For all pixels within a segment, a bottom-up clustering method is employed to iteratively merge them into larger groups, thereby forming a hierarchical structure. These unsupervised multi-granular masks are then utilized to supervise model training. Evaluated across seven popular datasets, UnSAM achieves competitive results with the supervised counterpart SAM, and surpasses the previous state-of-the-art in unsupervised segmentation by 11% in terms of AR. Moreover, we show that supervised SAM can also benefit from our self-supervised labels. By integrating our unsupervised pseudo masks into SA-1B's ground-truth masks and training UnSAM with only 1% of SA-1B, a lightly semi-supervised UnSAM can often segment entities overlooked by supervised SAM, exceeding SAM's AR by over 6.7% and AP by 3.9% on SA-1B.

## 1 Introduction

Trained on massive unlabeled data using self-supervised learning methods, Large Language Models (LLMs)  in natural language processing have revolutionized our world and redefined human-computer interactions. In the domain of computer vision, the recent introduction of the Segment Anything Model (SAM)  has dramatically transformed the field with its exceptional ability to handle diverse image segmentation tasks. However, the need for comprehensive manual labeling of training data--over 20 minutes per image --limits SAM from following the scaling laws that benefit LLMs . As a result, despite SA-1B  being the most extensive segmentation dataset available, it contains only about 11 million images. Moreover, human-annotated data often introduces significant biases based on the annotators' perceptions of "what constitutes an instance", which frequently leads to the oversight of small entities within the images.

This challenge raises a crucial question addressed in this paper: _Can we "segment anything" without supervision?_ In response, we present **UnSAM**, an innovative unsupervised learning method capable of performing both interactive and whole-image segmentation without the need for supervision.

How can we achieve fine-grained and multi-granular segmentation masks comparable to those in SA-1B  without supervision? Insights from neuroscience suggest that the human visual system exploits the structure of visual scenes by decomposing dynamic scenes into simpler parts or motions. This perception of hierarchically organized structures implies a powerful "divide-and-conquer" strategy for parsing complex scenes . Drawing inspiration from this, we introduce a divide-and-conquer approach designed to generate hierarchical image segmentation results directly from raw, unlabeled images. The divide-and-conquer approach is a crucial element of UnSAM, enabling it to effectively parse and segment images at multiple levels of granularity.

Our pseudo-mask generation pipeline initiates with a top-down clustering approach (_i.e._, the divide stage), to extract initial semantic and instance-level masks using a Normalized Cuts-based method CutLER [39; 31]. Subsequently, UnSAM refines these masks using a bottom-up clustering method (_i.e._, the conquer stage): within each mask, we iteratively merge semantically similar pixels into larger segments based on various similarity thresholds. The resulting masks at different thresholds in the conquer stage, along with the masks produced in the divide stage, create a hierarchical structure. Technically, we can generate a vast range of granularities with minimal extra cost! Furthermore, UnSAM captures more subtle details that pose challenges for human annotators, significantly enriching the granularity and utility of unsupervised segmentation models.

Equipped with these sophisticated multi-granular pseudo masks as "ground-truth" labels, UnSAM is adeptly trained to perform both interactive and automatic whole-image segmentation, demonstrating remarkable versatility across various segmentation scenarios. We have observed that our UnSAM model frequently identifies objects that SAM  overlooks, particularly types of objects or parts typically missed by ground-truth annotations of SA-1B , such as human ears, animal tails, _etc_.

The capabilities of UnSAM are rigorously tested across seven major whole-entity and part segmentation datasets, _e.g._, MSCOCO , LVIS , SA-1B , ADE , Entity , PartImageNet  and PACO . As illustrated in Fig. 1, we demonstrate some noteworthy behaviors:

* The performance gap between unsupervised segmentation models and SAM can be significantly reduced: By training on just 1% of SA-1B's unlabeled images with a ResNet50 backbone, UnSAM not only advances the state-of-the-art in unsupervised segmentation by 10% but also achieves comparable performance with the labor-intensive, fully-supervised SAM.
* The supervised SAM can also benefit from our self-supervised labels: integrating our unsupervised pseudo masks with SA-1B's ground-truth data and retraining UnSAM on this combined data enables UnSAM+ to outperform SAM's AR by over 6.7% and AP by 3.9%. We observed that UnSAM and UnSAM+ can often discover entities missed by SAM.

Figure 1: UnSAM significantly surpasses the performance of the previous SOTA methods in unsupervised segmentation, and delivers impressive whole image and promptable segmentation results, rivaling the performance of the supervised SAM . This comparative analysis features our unsupervised UnSAM, the supervised SAM, and an enhanced version, UnSAM+, across a variety of datasets. The top section displays raw images (row 1) alongside whole image segmentation outputs from UnSAM (row 3), and SAM (row 2). The bottom section highlights our promptable segmentation results using a point prompt (_i.e._, the star mark). The right panel quantitatively compares the performance across models, including metrics like Mask AR (%) and Point IoU.

Related Works

### Self-supervised Image Segmentation

Recent advances in unsupervised image segmentation [39; 28; 44; 6; 8; 41; 38; 42; 35; 12; 7; 37] have leveraged the emergent segmentation capabilities of self-supervised Vision Transformers (ViT) [8; 14; 17] to "discover" objects within images. Initial efforts, such as TokenCut  and LOST , have produced semantically meaningful pixel groupings for salient objects by utilizing the class-attention mechanism of self-supervised ViTs. As a representative work in the unsupervised segmentation domain, CutLER  introduced a cut-and-learn pipeline for unsupervised object detection and image segmentation. CutLER initially generates high-quality pseudo masks for multiple objects using MaskCut , followed by learning a detector on these masks using a loss dropping strategy. Extending this approach, VideoCutLER  employs a cut-synthesis-and-learn strategy for segmenting and tracking multiple instances across video frames without supervision. Additionally, SOHES  introduced the global-local self-exploration method to cluster image features from high to low cosine similarity, obtaining pseudo masks that cover multiple hierarchical levels.

In contrast, UnSAM introduces a divide-and-conquer pipeline that generates more pseudo masks per image at the same processing speed, but with enhanced quality and broader coverage across hierarchical levels. Furthermore, UnSAM captures more subtle details that pose challenges for human annotators, significantly enriching the granularity and utility of unsupervised segmentation models.

### Promptable Image Segmentation

Tradition segmentation models have focused on predicting masks for all instances or semantic parts within a single image simultaneously. Recently, however, models have begun to interact with users, generating segmentation masks based on user inputs such as points [21; 23; 47; 45; 11], text descriptions , or bounding boxes . Moreover, some approaches now frame segmentation tasks within an in-context learning framework [43; 3], utilizing in-context examples to define distinct segmentation tasks. For example, the Segment Anything model  can produce masks in a zero-shot manner based on different types of prompts. One limitation of SAM is that it only produces three class-agnostic masks. An extension, Semantic-SAM , aims to segment and recognize objects at multiple granularities through a multi-choice learning scheme, allowing each click point to produce masks at multiple levels along with their semantic labels. Nevertheless, both models are supervised and rely on large-scale, human-annotated data, which introduces issues of annotator bias and scalability limitations.

In contrast, our unsupervised UnSAM and lightly semi-supervised UnSAM+ model demonstrate superior performance in the promptable segmentation task, offering a robust alternative to these fully-supervised approaches.

## 3 Preliminaries

### Cut and Learn (CutLER) and MaskCut

CutLER  introduces a cut-and-learn pipeline to precisely segment instances without supervision. The initial phase, known as the cut stage, uses a normalized cut-based method, MaskCut , to generate high-quality instance masks given the patch-wise cosine similarity matrix \(W_{ij}=K_{j}}{|K_{i}|_{2}|K_{j}|_{2}}\), where \(K_{i}\) is "key" features of patch \(i\) in the last attention layer of unsupervised ViT. To extract multiple instance masks from a single image, MaskCut repeats this operation but adjusts by masking out patches from previously segmented instances in the affinity matrix: \(W^{t}_{ij}=_{i=1}^{s}M^{s}_{ij})(K_{j}_{i =1}^{s}M^{s}_{ij})}{\|K_{i}\|_{2}\|K_{j}\|_{2}}\) Subsequently, CutLER's learning stage trains a segmentation/detection model on these pseudo-masks with drop-loss. Please check Appendix A.2 for more details on CutLER.

### Segment Anything Model (SAM) and SA-1B

Segment Anything  tackles the promptable segmentation task. At its core lies the Segment Anything Model (SAM), which is capable of producing segmentation masks given user-provided points, boxes, and masks in a zero-shot manner. One significant contribution of SAM is the release of the SA-1B dataset , which comprises 11M high-resolution images and 1.1 billion segmentation masks, providing a substantial resource for training and evaluating segmentation models. While SAM significantly accelerates the labeling of segmentation masks, annotating an image still requires approximately 14 seconds per mask. Given that each image contains over 100 masks, this equates to more than 30 minutes per image, posing a substantial cost and making it challenging to scale up the training data effectively. For more details on SAM and SA-1B, please check Appendix A.3.

## 4 UnSAM: Segment Anything without Supervision

### Divide-and-Conquer for Hierarchical Image Segmentation

Our segment anything without supervision model starts by generating pseudo masks that respect the hierarchical structure of visual scenes without supervision. This approach is motivated by the observation that the "divide and conquer" strategy is a fundamental organizational principle employed by the human visual system to efficiently process and analyze the vast complexity of visual information present in natural scenes [4; 27]. Our pseudo-mask generation pipeline **divide-and-conquer**, which is summarized in Alg. 1 and illustrated in Fig. 2, consists of two stages:

**Divide stage**: we leverage a Normalized Cuts (NCuts)-based method, CutLER [39; 31], to obtain semantic and instance-level masks from unlabeled raw images. CutLER's cut-and-learn pipeline and its MaskCut method are discussed in Sec. 3.1. However, the coarser-granularity masks predicted by CutLER can be noisy. To mitigate this, we filter out masks with a confidence score below a threshold \(\). Empirically, salient semantic and instance-level entities typically encompass richer part-level entities (for example, a person has identifiable parts such as legs, arms, and head, whereas a background sky contains few or no sub-level entities). To extract these part-level entities with a hierarchical structure, we employ a conquer phase.

**Conquer stage**: for each instance-/semantic-level mask discovered in the previous stage, we employ iterative merging [1; 6] to decompose the coarse-grained mask into simpler parts, forming a hierarchical structure.

More specifically, we first crop local patches using the masks we obtained in the divide phase, and bi-linearly interpolate local patches to the resolution of \(256 256\). We then feed them into DINO pre-trained ViT-B/8  encoder \(f()\), and extract 'key' features \(k_{i}=f(p_{i})\) from the last attention layer as patch-wise features for local patches \(p_{i}\). Subsequently, the conquer phase employs iterative merging [1; 6] to group patches into larger clusters, with pre-defined cosine similarity thresholds at \(\{_{1},...,_{l}\}\), where \(l\) is the predefined granularity levels.

Figure 2: Our divide-and-conquer pipeline for generating the “ground-truth” pseudo masks used for training UnSAM without human supervision begins with a top-down clustering approach (_i.e._, the divide stage), to extract initial semantic/instance-level masks using a Normalized Cuts -based CutLER . Subsequently, we refine these masks using a bottom-up clustering method (_i.e._, the conquer stage): within each mask, we iteratively merge semantically similar pixels into larger segments using various similarity thresholds. The resulting masks at different thresholds create a hierarchy. We zoom-in selected regions to visualize details.

In iteration \(t\), our method finds two adjacent patches \((p_{i},p_{j})\) from two separate clusters \((C^{t}_{m},C^{t}_{n})\) with the highest cosine similarity \(_{i}k^{t}_{j}}{||k^{t}_{i}||_{2}||k^{t}_{j}||_{2}}\), merges them into one cluster, and updates \(k^{t}_{i}\) and \(k^{t}_{j}\) to \(k^{t}_{i}+a_{n}k^{t}_{j}}{a_{m}+a_{n}}\), where \(a_{m}\) is the number of patches in cluster \(C^{t}_{m}(p_{i} C^{t}_{m})\). The conquer stage repeats this step until the maximum cosine similarity is less than \(_{t}\), collects all merged clusters as new part-level pseudo masks, and uses smaller threshold \(_{t+1}\) to iterate again. Each coarse-grained mask discovered in the divide stage can form a hierarchical structure \(H\) after the conquer stage:

\[H=\{S_{0},S_{1},...,S_{t},...,S_{l}\},S_{t}=\{C^{t}_{1},...,C^{t}_{n_{ t}}\}n_{i} n_{j}i<j\] (1)

\(n_{t}\) is the number of clusters/masks belonging to granularity level \(t\) and \(n_{0}=1\).

**Mask merging:** The new part-level pseudo masks discovered in the conquer stage are added back to the semantic and instance-level masks identified in the divide stage. We then use Non-Maximum Suppression (NMS) to eliminate duplicates. Following previous works in unsupervised image segmentation , we also employ off-the-shelf mask refinement methods, such as Conditional Random Fields (CRF)  and CascadePSP , to further refine the edges of the pseudo masks. Finally, we filter out the post-processed masks that exhibit significant differences in Intersection-over-Union (IoU) before and after refinement.

**Preliminary results:** The divide-and-conquer pipeline achieves a pseudo mask pool with more entities, a broader range of granularity levels, and superior quality compared to previous work, _e.g._, CutLER , U2Seg  and SOHES . As shown in Table 3, its pseudo masks reach 23.9% AR on 1000 randomly selected validation images from the SA-1B dataset , representing a 45.7% improvement over the state-of-the-art.

**Key distinctions over prior works on pseudo-mask generation:** The divide-and-conquer strategy employed by UnSAM sets it apart from previous works:

 rely solely on top-down clustering methods, providing only instance and semantic-level masks, and thereby missing the hierarchical structure present in complex images. In contrast, our pipeline captures this hierarchical structure by identifying more fine-grained pixel clusters.

While  does incorporate some hierarchical structure through bottom-up clustering with iterative merging, it still misses many fine-grained instances and some large-scale instance masks. Additionally, the iterative merging in  focuses on small regions below a certain mask size threshold, primarily to refine noisy small masks, limiting its ability to detect a full range of entity sizes. Our experimental results demonstrate qualitatively and quantitatively superior performance compared to prior works, particularly in producing high-quality, detailed pseudo-masks that better capture the hierarchical complexity of visual scenes.

### Model Learning and Self-Training

Although the pseudo masks generated by our pipeline are qualitatively and quantitatively superior to those from prior works, they can still be somewhat noisy. Our self-supervised pipeline has limitations in identifying certain types of instances. For example, iterative merging sometimes fails to correctly associate disconnected parts of the same entity. To address this, we utilize a self-training strategy to further enhance UnSAM's model performance. UnSAM learns an image segmentation model using the masks discovered by the divide-and-conquer strategy. It has been observed that self-training enables the model to "clean" the pseudo masks and predict masks of higher quality . Once we have prepared the pseudo-masks, UnSAM can be integrated with any arbitrary whole-image or promptable image segmentation models during the model learning or self-training stage.

**Whole-image segmentation**. We choose the vanilla Masked Attention Mask Transformer (MaskFormer)  for simplicity. The key innovation of Mask2Former is the introduction of a masked attention mechanism in the transformer's cross-attention block, defined as \((M+QK^{T})V\), where the attention mask \(M\) at feature location \((x,y)\) is given by: \(M(x,y)=0&M(x,y)=1\\ -&\). This mechanism constrains attention within the region of the predicted mask. UnSAM is then trained using the following mask prediction loss:

\[=_{}_{}+_{} _{}\] (2)

where \(_{}\) and \(_{}\) is the cross-entropy and Dice loss, with \(_{}\) and \(_{}\) as their respective weights.

After one round of self-training UnSAM on the pseudo-masks, we perform a second round of self-training by merging high-confidence mask predictions (with a confidence score greater than \(_{}\)) as the new 'ground-truth' annotations. To avoid duplication, we filter out ground truth masks that have an IoU greater than 0.5 with the predicted masks.

**Promptable Image Segmentation**. Similar to SAM , our unsupervised SAM can also produce high-quality object masks from input prompts such as points. We utilize Semantic-SAM  as the base model for predicting multiple granularity levels of masks from a single click. During the learning process, we randomly sample points within an inner circle (\( 0.1(_{},_{})\)) of the mask to simulate user clicks.

### UnSAM+: Improving Supervised SAM with Unsupervised Segmentation

The supervised SAM model's  reliance on human-annotated data introduces a significant bias based on the annotator's perception of _'what constitutes an instance'_, frequently missing some entities within the image. In contrast, since our mask generation pipeline does not rely on human supervision, it can often identify valid objects or parts that are overlooked by SA-1B's  ground-truth annotations.

Motivated by this observation, we leverage UnSAM to improve the performance of the supervised SAM  by implementing a straightforward yet effective strategy: merging SA-1B's ground-truth masks \(D_{}\) with our unsupervised segmentation masks \(D_{}\) based on the IoU, formulated as:

\[D^{i}_{}=D^{i}_{}\{ C_{m} D^{i}_{ }^{}(C_{m}, C_{n} D^{i}_{}) _{}\}\] (3)

\(_{}\) is the IoU threshold, \(^{}\) is the maximum IoU between \(C_{m}\) and any mask \(C_{n}\) in \(D^{i}_{}\), and \(D^{i}_{}\) and \(D^{i}_{}\) is the set of SA-1B and unsupervised masks within image \(i\), respectively. We then train UnSAM+ on \(D_{}\) for promptable image segmentation and whole-image segmentation. The fusion approach leverages the strengths of both supervised and unsupervised annotations, addressing the limitations inherent in human-annotated datasets while significantly enriching the diversity and comprehensiveness of the training data. This results in a more robust and generalizable segmentation model UnSAM+, surpassing the performance of SAM.

## 5 Experiments

### Model Training Settings

We provide a brief overview of the model training settings and include more details in Appendix A.1.

**Pseudo mask generation.** In the divide stage, we set the confidence threshold \(\)=0.3; in the conquer stage, we choose threshold \(_{merge}=[0.6,0.5,0.4,0.3,0.2,0.1]\). When merging the pseudo masks with the ground truths for training UnSAM+, we select \(_{}=0.02\). **Whole-image segmentation.** UnSAM picks DINO  pre-trained ResNet-50  as the backbone and Mask2former  as the mask decoder. The default learning rate is \(5 10^{-5}\) with a batch size of 16 and a weight decay of \(5 10^{-2}\). We train the model for 8 epochs. **Promptable segmentation.** UnSAM uses the self-supervised pre-trained Swin-Transformer  Tiny model as the backbone, and leverages Semantic-SAM  as the base model. We set the number of hierarchy levels to 6, which is also the number of predicted masks UnSAM generates per prompt during inference. One can easily train with a different number of granularity levels as needed. For all experiments, we train UnSAM with 1\(\)4% unlabeled images from SA-1B dataset .

### Evaluation Datasets and Metrics

**Whole-image segmentation.** To evaluate our model's performance, we test our models on various datasets in a zero-shot manner to evaluate the performance of segmenting entities from all granularity levels. We choose COCO , LVIS , ADE20K , EntitySeg , and SA-1B  that mainly encompass semantic-/instance-level entities; PartImageNet  and PACO  that cover part-level entities. The SA-1B test set consists of randomly selected 1000 images not included in our training set. Notably, each dataset only covers entities from certain hierarchical levels and certain pre-defined classes, while our model generates masks from all levels and all classes. Hence, the COCO Average Precision (AP) metric could not reflect our model's authentic performance in segmenting all entities in the open-world. Following prior work [39; 6], we mainly consider Average Recall (AR) to compare with different models.

Figure 3: Unsupervised pseudo-masks generated by our divide-and-conquer pipeline not only contain precise masks for coarse-grained instances (column 5), _e.g._, cameras and persons, but also capture fine-grained parts (column 3), _e.g._, digits and icons on a tiny camera monitor that are missed by SA-1B’s  ground-truth labels.

[MISSING_PAGE_FAIL:8]

When compared to the supervised SAM , UnSAM's AR across all datasets is already very close, with only a 1% difference. On PartImageNet  and PACO , UnSAM surpasses SAM by 24.4% and 11.6%. This further demonstrates the excellent capability of our divide-and-conquer pipeline in discovering details that human annotators tend to miss.

Furthermore, our UnSAM+, trained with integrated unsupervised pseudo masks and SA-1B  ground truth, outperforms SAM's  AR by over 6.7% and AP by 3.9% as shown by Table 2 and 4. UnSAM+ demonstrates superior average recall compared to SAM across all evaluation datasets except for ADE20K , which is dominated by semantic-level annotations. UnSAM+'s significantly 16.2 % higher AR on small entities further confirms that our pseudo masks can effectively complement the SA-1B datasets with more details it ignores and the UnSAM+ can often discover entities missed by SAM as demonstrated in Fig. 4 and Fig. 7.

**Point-based promptable segmentation.** As shown in Table 5, UnSAM trained with our pseudo masks achieve 40.3% MaxIoU and 59.5% OracleIoU on COCO. Notably, we train the model with only 1% of the data that SAM  uses and a backbone with 4\(\) fewer parameters. Moreover, the UnSAM+ trained with integrated pseudo masks and SA-1B ground truths outperforms SAM on both MaxIoU and OracleIoU with 0.3% and 1.3% respectively. Qualitative results are shown in Fig. 6.

Figure 5: UnSAM not only discovers more fine-grained masks than the previous state-of-the-art unsupervised segmentation method , but also provides segmentation masks with a wide range of granularity. We show qualitative comparisons between UnSAM (with 3 levels of granularity) and baseline models on SA-1B .

Figure 6: Qualitative comparisons of promptable image segmentation between the fully-supervised SAM , our unsupervised UnSAM, and the lightly semi-supervised UnSAM+. Both UnSAM and UnSAM+ consistently deliver high-quality, multi-granular segmentation masks in response to the point prompts (_i.e._, the star mark).

  & Backbone & Sup. & Unsup. &  & Point (Max) & Point (Oracle) \\  & (\# params) & Labels & Labels & & 1-IoU & 1-IoU \\  SAM (B) & ViT-B/8 (85M) & ✓ & ✗ & 100\% & 52.1 & 68.2 \\ UnSAM & Swin-Tiny (25M) & ✗ & ✓ & 1\% & 40.3 & 59.5 \\ UnSAM+ & Swin-Tiny (25M) & ✓ & ✓ & 1\% & **52.4** & **69.5** \\ 

Table 5: Despite using a backbone that is 3\(\) smaller and being trained on only 1% of SA-1B, our lightly semi-supervised UnSAM+ surpasses the fully-supervised SAM in promptable segmentation task on COCO.

## 6 Summary

Image segmentation is a fundamental task in computer vision, traditionally relying on intensive human annotations to achieve a detailed understanding of visual scenes. We propose UnSAM, an unsupervised segmentation model that significantly surpasses the performance of previous state-of-the-art methods in unsupervised image segmentation. Additionally, our unsupervised UnSAM model delivers impressive results, rivaling the performance of the cutting-edge supervised SAM, and exceeding it in certain semi-supervised settings.

Figure 7: More visualizations on SA-1B . From top to bottom are raw images, segmentation by SAM, segmentation by UnSAM, and segmentation by UnSAM+.

**Acknowledgement.** We thank helpful discussions with Jitendra Malik, Cordelia Schmid, Ishan Misra, Xinlei Chen, Xingyi Zhou, Alireza Fathi, Renhao Wang, Stephanie Fu, Qianqian Wang, Baifeng Shi, Max Letian Fu, Tony Long Lian, Songwei Ge, Bowen Cheng and Rohit Girdhar. We thank Shengcao Cao and Hao Zhang for their help in reproducing baseline results. XuDong Wang and Trevor Darrell were funded by DoD including DARPA LvLL and the Berkeley AI Research (BAIR) Commons.