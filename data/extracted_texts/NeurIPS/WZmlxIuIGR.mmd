# Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark

Jiaming Ji\({}^{1,*}\), Borong Zhang\({}^{1,*}\), Jiayi Zhou\({}^{1,*}\), Xuehai Pan\({}^{1}\), Weidong Huang\({}^{1}\)

Ruiyang Sun\({}^{1}\), Yiran Geng\({}^{1}\), Yifan Zhong\({}^{1,2}\), Juntao Dai\({}^{1}\), Yaodong Yang \({}^{1,}\)

\({}^{1}\) Institute for AI, Peking University

\({}^{2}\) Beijing Institute for General Artificial Intelligence (BIGAI)

{jiamg.ji, borongzh}@gmail.com, gaiejj@outlook.com

yaodong.yang@pku.edu.cn

Equal Contribution. Corresponding author.

Work done when Jiayi Zhou visited Peking University.

###### Abstract

Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for safer, more reliable, and responsible real-world applications. The website of this project can be accessed at https://sites.google.com/view/safety-gymnasium.

## 1 Introduction

AI systems possess enormous potential to spur societal progress. However, their deployment is frequently hindered by substantial safety considerations [1; 2; 3; 4]. Distinct from pure reinforcement learning (RL), Safe reinforcement learning (SafeRL) seeks to optimize policies while concurrently adhering to multiple constraints, addressing the challenge of employing RL in scenarios with critical safety implications [5; 6; 7; 8; 9]. This strategy proves particularly pertinent in real-world applications such as autonomous vehicles  and healthcare , where system failures or unsafe actions can result in grave consequences, such as accidents or harm to individuals. In large language models (LLMs), some studies have also shown that the toxicity of the models can be reduced through SafeRL [12; 13]. Incorporating safety constraints ensures adherence to predefined boundaries and regulatory standards, fostering trust and enabling exploration in environments with high-risk potential. Overall, SafeRL is instrumental in guaranteeing the dependable operation of intelligent systems in intricate and high-stake domains.

Simulation environments have become instrumental in fostering the advancement of RL. Eminent examples such as Gym , Atari , and dm-control  underline their importance. These versatile platforms permit researchers to swiftly design and execute varied tasks, thus enabling efficient evaluation of algorithmic effectiveness and intrinsic limitations. However, within the sphere of SafeRL, there is a notable dearth of dedicated simulation environments, which impedes comprehensive exploration of SafeRL. In recent years, there have been strides to address this gap. DeepMind presented AI-Safety-Gridworlds, a suite of RL environments showcasing various safety properties of intelligent agents . Afterward, OpenAI introduced the Safety Gym benchmark suite, a collection of high-dimensional continuous control environments incorporating safety-robot tasks . Over the past two years, several additional environments have been developed by researchers, including safe-control-gym , MetaDrive , etc.

**Compared to Safety Gym1 Safety-Gymnasium inherits and expands the settings of some tasks of Safety Gym, aiming to bolster the community's growth further. Compared with Safety Gym, we have made the following major improvements:**

* **Refactoring of the physics engine.** Safety Gym utilizes _mujoco-py_ to enable Python-based customization of MuJoCo components. However, _mujoco-py_ stopped updates and support after 2021. In contrast, Safety-Gymnasium supports MuJoCo directly, eliminating the reliance on _mujoco-py_. This facilitates access to the latest MuJoCo features (_e.g._, rendering speed and accuracy improved, etc.) and lowers the entry barrier, particularly due to _mujoco-py's_ dependency on specific GCC versions and more.
* **Extension of Agent and Task Components.** Safety Gym initially supports only three agents and tasks. On this basis, Safety-Gymnasium has been further expanded, introducing more diverse agents and task components and expanding safety tasks to cover multi-agent domains. Finally, Safety-Gymnasium launched a high-dimensional test component based on Issac-Gym , further enriching the benchmark.
* **Enhanced Visual Task Support.** The visual components of Safety Gym are simplistic (consisting of basic geometric shapes), and _mujoco-py_ relies on OpenGL for visual rendering, which results in significant virtualization performance loss on headless servers. In contrast, Safety-Gymnasium, built on MuJoCo, achieves rendering speeds on CPU that are twice as fast as the former. Additionally, it offers more comprehensive visual component support.
* **Easy Installation and High Customization.** Safety Gym is cumbersome to install and relies heavily on the underlying software. One of the design motivations of Safety-Gymnasium is the ease of use so that everyone can focus on algorithm design. Safety-Gymnasium can be easily installed with one simple command pip install safety-gymnasium. While benefiting from the highly integrated framework, Safety-Gymnasium only needs 100 lines of code to customize the required environment.

In this work, we introduce Safety-Gymnasium, a collection of environments specifically for SafeRL, built upon the Gymnasium [14; 22] and MuJoCo . Enhancing the extant Safety Gym framework , we address various concerns and expand the task scope to include vision-only and multi-agent scenarios. Additionally, we released SafePO, a single-file style algorithm library containing over 16 state-of-the-art algorithms. Collectively, our contributions are enumerated as follows:

* **Environmental Components.** We provide various safety-oriented tasks under the umbrella of Safety-Gymnasium. These tasks encompass single-agent, multi-agent, and vision-based challenges, each with varying constraints. Our environments are categorized into two primary types: Gymnasium-based, featuring agents of escalating complexity for algorithm verification and comparison, and Issac-Gym-based, incorporating sophisticated agents that harness the parallel processing power of Issac-gym's GPU. This empowers researchers to explore SafeRL algorithms in complex scenarios. Further details can be found in Section 4.
* **Algorithm Components.** We offer the SafePO algorithm library, which comprises a single-file style housing 16 diverse algorithms. These algorithms encompass both single-agent and multi-agent approaches, along with first-order and second-order variants, as well as Lagrangian-basedand Projection-based methods. Through meticulous decoupling, each algorithm's code resides in an individual file. A more in-depth exploration of SafePO is presented in Section 5.
* **Insights and Analysis.** Combining Safety-Gymnasium and SafePO, we conduct a detailed analysis of existing algorithms. Our analysis encompasses 16 algorithms across 54 distinct environments, covering various scenarios such as single-agent and multi-agent setups with varying constraint complexities. This analysis delves into each algorithm's strengths, constraints, and avenues for enhancement. We provide access to all metadata, fostering community verification and encouraging further research. Further details can be found in Section 6.

## 2 Related Work

Safety EnvironmentsIn RL, agents need to explore environments to learn optimal policies by trial and error. It is currently typical to train RL agents mostly or entirely in simulation, where safety concerns are minimal. However, we anticipate that challenges in simulating the complexities of the real world (_e.g._, human-AI collaborative control [1; 2]) will cause a shift towards training RL agents directly in the real world, where safety concerns are paramount [20; 24; 25]. OpenAI includes safety requirements in the Safety Gym , which is a suite of high-dimensional continuous control environments for measuring research progress on SafeRL. Safe-control-gym  allows for constraint specification and disturbance injection onto a robot's inputs, states, and inertial properties through a portable configuration system. DeepMind also presents a suite of RL environments, AI-Safety-Gridworlds , illustrating various safety properties of intelligent agents.

SafeRL AlgorithmsCMDPs have been extensively studied for different constraint criteria [26; 27; 28; 29]. With the rise of deep learning, CMDPs are also moving to more high-dimensional continuous control problems. CPO  proposes the first general-purpose policy search algorithm for SafeRL with guarantees for near-constraint satisfaction at each iteration. However, CPO's policy updates hinge on Taylor approximations and the inversion of high-dimensional Fisher information matrices. These approximations can occasionally lead to inappropriate policy updates. FOCOPS  applies a primal-dual approach to solve the constrained trust region problem directly and subsequently projects the solution back into the parametric policy space. Similarly, CUP  offers non-convex implementations through a first-order optimizer, thereby not requiring a strong approximation of the convexity of the objective.

## 3 Preliminaries

### Constrained Markov decision process

SafeRL [6; 33] is often formulated as a Constrained Markov decision process (CMDP) , which is a tuple \(=(,,,R,,,)\). Here \(\) and \(\) are the state space and action space correspondingly. \((s^{}|s,a)\) is the probability of state transition from \(s\) to \(s^{}\) after taking action \(a\). \(R(s^{}|s,a)\) denotes the reward obtained by the agent performing action \(a\) in state \(s\) and transitioning to state \(s^{}\). The set \(=\{(c_{i},b_{i})\}_{i=1}^{m}\), where \(c_{i}\) are cost functions: \(c_{i}:\) and the cost thresholds are \(b_{i},i=1,,m\). \(():\) is the initial state distribution and the discount factor \([0,1)\).

A stationary parameterized policy \(_{}\) is a probability distribution defined on \(\), \(_{}(a|s)\) denotes the probability of taking action \(a\) in state \(s\). We use \(_{}=\{_{}:^{p}\}\) to denote the set of all stationary policies and \(\) is the network parameter needed to be learned. Let \(_{_{}}^{|S||S|}\) denotes a state transition probability matrix and the components are: \(_{_{}}[s,s^{}]=_{_{}}(s^{}|s)= _{a}_{}(a|s)(s^{}|s,a)\), which denotes one-step state transition probability from \(s\) to \(s^{}\) by executing \(_{}\). Finally, we let \(d^{_{}}_{_{}}(s)=(1-)_{t=0}^{}^{t} _{_{}}(s_{t}=s|s_{0})\) to be the stationary state distribution of the Markov chain starting at \(s_{0}\) induced by policy \(_{}\) and \(d^{}_{_{}}(s)=_{s_{0}()}[d^{}_{_{ }}(s)]\) to be the discounted state visitation distribution on initial distribution \(\).

The objective function is defined via the infinite horizon discounted reward function where for a given \(_{}\), we have \(J^{R}(_{})=[_{t=0}^{}^{t}R(s_{t+1}|s_{t},a _{t})|s_{0},a_{t}_{}]\). The cost function is similarly specified via the following infinite horizon discount cost function: \(J^{C}_{i}(_{})=[_{t=0}^{}^{t}C_{i}(s_{t+1}| s_{t},a_{t})|s_{0},a_{t}_{}]\).

Then, we define the feasible policy set \(_{}\) as : \(_{}=_{i=1}^{m}\{_{}_{}\;\;\;\;J_{ i}^{C}(_{}) b_{i}\}\). The goal of CMDP is to search the optimal policy \(_{}\): \(_{}=_{_{}_{}}J^{R}(_{})\).

### Constrained Markov Game

Safe multi-agent reinforcement learning is often formulated as a Constrained Markov Game \((,,,,,,R,,)\). Here, \(=\{1,,n\}\) is the set of agents, \(\) and \(=_{i=1}^{n}^{i}\) are the state space and the joint action space (_i.e._, the product of the agents' action spaces), \(:\) is the probabilistic transition function, \(\) is the initial state distribution, \([0,1)\) is the discount factor, \(R:\) is the joint reward function, \(=C_{j}^{i}}_{1 j m^{i}}^{i}\) is the set of sets of cost functions (every agent \(i\) has \(m^{i}\) cost functions) of the form \(C_{j}^{i}:^{i}\), and finally the set of corresponding cost threshold is given by \(=b_{j}^{i}}_{1 j m^{i}}^{i}\). At time step \(t\), the agents are in a state \(_{t}\), and every agent \(i\) takes an action \(_{t}^{i}\) according to its policy \(^{i}(^{i}_{t})\). Together with other agents' actions, it gives a joint action \(_{t}=(_{t}^{1},,_{t}^{n})\) and the joint policy \(()=_{i=1}^{n}^{i}(^{i} )\). The agents receive the reward \(R(\,_{t},_{t})\), meanwhile each agent \(i\) pays the costs \(C_{j}^{i}(\,_{t},_{t}^{i})\), \( j=1,,m^{i}\). The environment then transits to a new state \(_{t+1}(\,_{t},_{t})\).

The objective of reward function are \(J()_{_{0}^{0},_{0, },_{1,}}[_{t=0}^{} ^{t}R(_{t},_{t})]\), and costs function are \(J_{j}^{i}()_{_{0}^{0},_{0,},_{1,}}[_{t=0}^ {}^{t}C_{j}^{i}(\,_{t},_{t}^{i}) ] c_{j}^{i}\), \( j=1,,m^{i}\).

We are examining a fully cooperative setting where all agents share a common reward function. Consequently, the goal of safe multi-agent RL is to identify the optimal policy that maximizes the expected total reward while simultaneously ensuring that the safety constraints of each agent are satisfied. Then we define the feasible joint policy set \(_{}=_{i=1}^{n}\{_{}_{}\;\;\;\;J_ {j}^{i}() c_{j}^{i}, j=1,,m^{i}\}\). The goal of CMG is to search the optimal policy \(_{}=_{_{}_{}}J(_{})\).

## 4 Safety Environments: Safety-Gymnasium

Safety-Gymnasium provides a seamless installation process and minimalistic code snippets to basic examples, as shown in Figure 1. Due to the limited space of the paper, we provide a more detailed description (_e.g._, detailed instructions, the composition of the robot's observation space and action space, dynamic structure, physical parameters, etc.) in Appendix B and Online Documentation2.

### Gymnasium-based Learning Environments

In this section, we introduce Gymnasium-based environment components from three aspects: (1) the robots (both single-agent and multi-agent); (2) the tasks that are supported within the environment; (3) the safety constraints that are upheld.

Figure 1: Using Safety-Gymnasium to create, step, render a specific safety-task environment.

Supported RobotsAs shown in Figure 2, Safety-Gymnasium inherits three pre-existing agents from Safety Gym , namely Point, Car, and Doggo. By meticulously adjusting the model parameters, we have successfully mitigated the issue of excessive oscillations during the runtime of Point and Car agents. Building upon this foundation, we have introduced two additional robots: racecar [34; 35], and ant , to enrich the single-agent scenarios. As for multi-agent robots, we have leveraged certain configurations from multi-agent MuJoCo , deconstructing the original single-agent structure and enabling multiple agents to control distinct body segments. This design choice has been widely adopted in various research works [37; 38; 39].

Supported TasksAs shown in Figure 3, the Gymnasium-based learning environments support the following tasks. For a more detailed task specification, please refer to our online documentation3.

* _Velocity._ The robot aims to facilitate coordinated leg movement of the robot in the forward (right) direction by exerting torques on the hinges.
* _Run._ The robot starts with a random initial direction and a specific initial speed as it embarks on a journey to reach the opposite side of the map.
* _Circle._ The reward is maximized by moving along the green circle and not allowed to enter the outside of the red region, so its optimal path follows the line segments \(AD\) and \(BC\).
* _Goal._ The robot navigates to multiple goal positions. After successfully reaching a goal, its location is randomly reset while maintaining the overall layout.
* _Push._ The objective is to move a box to a series of goal positions. Like the goal task, a new random goal location is generated after each achievement.
* _Button._ The objective is to activate a series of goal buttons distributed throughout the environment. The agent's goal is to navigate towards and contact the currently highlighted button, known as the goal button.

Supported ConstraintsAs shown in Figure 3, the Gymnasium-based environments support the following constraints. For a more detailed task specification, please refer to our online documentation.

* _Velocity-Constraint_ involves safety tasks using MuJoCo agents . In these tasks, agents aim for higher reward by moving faster, but they must also adhere to velocity constraints for safety. Specifically, in a two-dimensional plane, the cost is computed as the Euclidean norm of the agent's velocities (\(v_{x}\) and \(v_{y}\)).

Figure 2: **Upper: The Single-Agent Robots of Gymnasium-based Environments. Lower: The Multi-Agent Robots of Gymnasium-based Environments.**

* _Pillars_ are employed to represent large cylindrical obstacles within the environment. In the general setting, contact with a pillar incurs costs.
* _Hazards_ are utilized to model areas within the environment that pose a risk, resulting in costs when an agent enters such areas.
* _Sigwalls_ are designed specifically for Circle tasks. Crossing the wall from inside the safe area to the outside incurs costs.
* _Vases_ represent static and fragile objects within the environment. Touching or displacing these objects incurs costs for the agent.
* _Gremlins_ represent moving objects within the environment that can interact with the agent.

#### 4.1.1 Vision-only tasks

Vision-only SafeRL has gained significant attention as a focal point of research, primarily due to its applicability in real-world contexts [40; 41]. While the initial iteration of Safety Gym offered rudimentary visual input support, there is room for enhancing the realism of its environment. To effectively evaluate vision-based SafeRL algorithms, we have devised a more realistic visual environment utilizing MuJoCo. This enhanced environment facilitates the incorporation of both RGB and RGB-D inputs (as shown in Figure 5). An exemplar of this environment is depicted in Figure 4, while comprehensive descriptions are available in Appendix B.5.

### Issac-Gym-based Learning Environments

In this section, we introduce Safety-DexterousHands, a collection of environments built upon DexterousHands  and the Isaac Gym engine . Leveraging GPU capabilities, Safety-DexterousHands enables large-scale parallel sample collection, significantly accelerating the training process. The environments support both single-agent and multi-agent settings. These environments involve two robotic hands (refer to Figure 6 (a) and (b)). In each episode, a ball randomly descends near the right hand. The right hand needs to grasp and launch the ball toward the left hand, which subsequently catches and deposits it at the target location.

Figure 4: Vision-only Tasks of Gymnasium-based Environments.

Figure 3: **Upper: Tasks of Gymnasium-based Environments; Lower: Constraints of Gymnasium-based Environments.**

For timestep \(t\), let \(x_{b,t}\), \(x_{g,t}\) to be the position of the ball and the goal, \(d_{p,t}\) to denote the positional distance between the ball and the goal \(d_{p,t}=\|x_{b,t}-x_{g,t}\|_{2}\). Let \(d_{a,t}\) denote the angular distance between the object and the goal, and the rotational difference is \(d_{r,t}=2\{|d_{a,t}|,1.0\}\). The reward is defined as follows, \(r_{t}=\{-0.2( d_{p,t}+d_{r,t})\}\), where \(\) is a constant balance of positional and rotational reward.

**Safety Joint** constrains the freedom of joint 4 of the forefinger (refer to Figure 6 (c) and (d)). Without the constraint, joint 4 has freedom of \([-20^{},20^{}]\). The safety tasks restrict joint 4 within \([-10^{},10^{}]\). Let \(\) be the angle of joint 4, and the cost is defined as: \(c_{t}=([-10^{},10^{}])\).

**Safety Finger** constrains the freedom of joints 2, 3 and 4 of forefinger (refer to Figure 6 (c) and (e)). Without the constraint, joints 2 and 4 have freedom of \([0^{},90^{}]\) and joint 4 of \([-20^{},20^{}]\). The safety tasks restrict joints 2, 3, and 4 within \([22.5^{},67.5^{}]\), \([22.5^{},67.5^{}]\), and \([-10^{},10^{}]\) respectively. Let \(\), \(\), \(\) be the angles of joints 2, 3, and the cost is defined as:

\[c_{t}=([22.5^{},67.5^{}],[22.5^{},67.5^{}],[-10^{},10^{}]).\] (1)

## 5 Safe Policy Optimization Algorithms: SafePO

This section provides a detailed discussion of the design of SafePO. Features such as strong performance, extensibility, customization, visualization, and documentation are all presented to demonstrate the advantages and contributions of SafePO.

CorrectnessFor a benchmark, it is critical to ensure its correctness and reliability. Firstly, each algorithm is implemented strictly according to the original paper (_e.g._, ensuring consistency with the gradient flow of the original paper, etc.). Secondly, we compare our implementation with those line by line for algorithms with a commonly acknowledged open-source code base to double-check the correctness. Finally, we compare SafePO with existing benchmarks (_e.g._, Safety-Starter-Agents5 and RL-Safety-Algorithms6) and SafePO outperforms or achieves comparable performance with other existing implementations, as shown in Table 1.

Figure 5: The RGB and RGB-D input of Gymnasium-based Environments.

Figure 6: Tasks of Safety-DexterousHands.

[MISSING_PAGE_FAIL:8]

## 6 Experiments and Analysis

**Reward and Cost.** Episodic reward and cost exhibit a trade-off relationship. Unconstrained algorithms aim to maximize reward through risky behaviors. HAPPO  achieves higher rewards compared to MAPPO  across 8 velocity-based tasks, accompanied by a simultaneous increase in average costs. SafeRL algorithms tend to maximize reward while adhering to constraints. As depicted in Table 2, in the velocity task, compared to PPO , PPO-Lag  achieves a reduction of \(98\%\) in cost while only experiencing a decrease of \(45\%\) in reward.

**Randomness and Oscillation.** The randomness of tasks is correlated with the oscillation of algorithms' performance. All SafeRL algorithms achieve average episodic costs within the cost_limit for velocity tasks. The divergence in episodic rewards between algorithms is negligible, and the distribution of optimal policies is tightly clustered. However, pronounced oscillations are present in navigation tasks characterized by high stochasticity. Out of the 20 navigation tasks examined,

   &  &  &  &  &  &  &  &  &  \\ 
**Safety Navigation** & \(J^{R}\) & \(J^{R}\) & \(J^{C}\) & \(J^{R}\) & \(J^{C}\) & \(J^{R}\) & \(J^{R}\) & \(J^{R}\) & \(J^{C}\) & \(J^{R}\) & \(J^{R}\) & \(J^{C}\) & \(J^{R}\) & \(J^{R}\) & \(J^{C}\) & \(J^{R}\) & \(J^{R}\) & \(J^{C}\) & \(J^{R}\) & \(J^{C}\) & \(J^{R}\) & \(J^{C}\) \\    & 1.00 & 4.42 & **0.09** & **0.86** & 0.23 & 1.95 & 0.10 & **0.70** & 0.16 & 2.07 & 0.12 & 4.01 & 0.03 & 1.01 & **0.03** & **0.17** & 0.01 & 0.46 \\  & 1.00 & 1.81 & 0.79 & 0.56 & 0.65 & 0.15 & 0.90 & 0.63 & 0.14 & 0.17 & 0.28 & 1.87 & 0.60 & 0.82 & 0.02 & 1.22 \\  & 1.00 & 1.81 & **0.26** & **0.94** & **0.25** & **0.74** & 0.47 & 1.49 & **0.78** & **0.19** & **0.05** & **0.09** & **0.24** & 0.34 & 1.33 & **0.09** & **0.67** \\  & 1.00 & 1.00 & **0.13** & **0.00** & **0.30** & **0.00** & **0.03** & **0.03** & **0.01** & **0.00** & **0.07** & **0.00** & **0.00** & **0.20** & **0.00** & **-0.30** & **0.03** \\  & 1.00 & 1.60 & 0.01 & 0.04 & 0.04 & 1.08 & **0.00** & **0.10** & **0.13** & 0.17 & 0.03 & 1.76 & 0.05 & 1.00 & 5.04 & 0.04 & 2.11 \\  & 1.00 & 8.42 & 0.81 & 0.82 & 1.60 & 2.77 & 1.61 & **1.90** & 1.70 & 3.11 & 1.67 & 1.33 & 1.41 & 1.99 & 0.76 & 0.71 & 0.84 & 1.12 \\  & 1.00 & 2.28 & **0.43** & **0.93** & 0.82 & 1.09 & 0.02 & 4.07 & **0.35** & **0.86** & 0.86 & 0.63 & 0.61 & 1.42 & **0.19** & **0.63** & **0.52** & **0.93** \\  & 1.00 & 7.16 & **0.46** & **0.78** & **1.38** & **0.70** & **0.40** & **0.47** & 1.11 & 1.42 & 0.33 & 1.14 & 0.64 & 2.36 & 0.32 & **0.95** & **0.29** & **0.36** \\  & 1.00 & 7.57 & **0.01** & **0.03** & 0.00 & 1.27 & **0.01** & 0.07 & **0.01** & **0.09** & **0.00** & **0.03** & **0.05** & **0.02** & **0.02** & **0.05** & **0.06** & 3.68 \\  & 1.00 & 3.14 & 0.77 & 0.46 & 0.67 & 1.37 & 0.52 & 0.55 & 1.82 & 0.66 & 1.22 & **0.31** & **0.55** & 0.80 & 2.04 & 0.03 & 0.73 & 4.49 \\  & 1.00 & 2.28 & **0.05** & **0.18** & **0.69** & **0.00** & **0.00** & 1.08 & 0.30 & **0.00** & **0.00** & **0.00** & **0.00** & **0.00** & **0.04** & 1.27 \\  & 1.00 & 1.31 & **0.09** & **0.00** & **0.53** & **0.78** & **0.32** & **0.04** & 0.54 & 1.55 & **0.46** & **0.00** & **0.36** & **0.00** & **0.00** & **0.08** & **0.06** & **0.40** \\  & 1.00 & 6.02 & 0.22 & 1.27 & 1.29 & 0.00 & 0.84 & 0.12 & 1.13 & 0.12 & 0.01 & 2.00 & 2.18 & 1.28 & 0.25 & 1.53 & 1.53 \\  & 1.00 & 8.10 & 0.86 & 0.93 & 1.67 & 1.35 & 1.72 & 1.26 & 1.66 & 1.42 & 1.69 & 1.74 & 1.33 & 2.26 & **0.82** & **0.63** & **0.84** & **0.89** \\  & 1.00 & 1.90 & 4.17 & 0.47 & 0.78 & 0.91 & 0.91 & 0.15 & 0.55 & **0.99** & **0.93** & **0.79** & **0.17** & **0.78** & **0.46** & **0.73** & **0.95** & 1.36 \\  & 1.00 & 2.31 & 0.98 & 1.33 & **0.88** & **0.05** & **0.35** & **0.94** & **0.32** & **0.22** & **0.80** & 0.77** & 1.25 & **2.32** & **0.80** & **1.13** & **2.51** \\  & 1.00 & 1.37 & -0.01 & 9.40 & -0.02 & 1.79 & 0.36 & -0.03 & -0.03 & -0.01 & -0.19 & 0.20 & -0.42 & -0.02 & -0.02 & -0.02 & -0.02 & -0.02 & -0.02 & -0.02 & -0.02 & -0.02 & -0.02 & -0.02 & -0.02 \\  & 1.00 & 1.557 & 0.83 & 1.90 & 1.80 & 2.37 & 0.58 & 1.03 &optimal policies are spread out more, leading to observable differences in algorithm performance across various tasks.

**Lagrangian vs. Projection.** In contrast to projection-based methods, the Lagrangian-based methods tend to display more oscillation. A notable disparity becomes apparent upon examining the oscillatory patterns in the episodic cost around the designated safety constraints during training, as presented in Figure 8(b). Both CPO  and PPO-Lag  demonstrate oscillations; however, those exhibited by PPO-Lag are more conspicuous. This discrepancy is manifested in a higher proportion of instances classified as _Strongly Unsafe_ and _Strongly Safe_ for PPO-Lag, while CPO maintains a more centered distribution. Nevertheless, an excessively cautious policy has the potential to undermine performance. In contrast, the projection-based method PCPO  exhibits lower average costs and rewards in navigation and velocity tasks than CPO. This distinction is further accentuated when examining the contrast between MACPO and MAPPO-Lag.

**Lagrangian vs. PID-Lagrangian.** Incorporating a PID controller within the Lagrangian-based framework proves to be effective in mitigating inherent oscillations. As shown in Figure 8, CPPO-PID  displays episodic rewards during training that closely resemble those of PPO-Lag. However, CPPO-PID demonstrates a reduced frequency of instances entering the _Strongly Unsafe_ region, resulting in a more significant proportion of _Safe_ states and improved safety performance.

## 7 Limitations and Future Works

Ensuring safety remains a paramount concern. Across various tasks, safety concerns can be transformed into corresponding constraints. However, a limitation of this study is its inability to encompass all forms of constraints. For instance, safety constraints related to human-centric considerations are paramount in human-AI collaboration, yet these considerations have not been fully integrated within the scope of this study. This work focuses on safety tasks within a simulated environment and introduces an extensive testing component. However, the transferability of the results to complex real-world safety-critical applications may be limited. A promising work for the future involves transferring policy refined within the Safety-Gymnasium to physical robotic platforms, which holds profound implications.

    & **MAPPO** & **HAPPO** & **MAPPO-Lag** & **MACPO** \\   & \(^{}\) & \(^{}\) & \(^{}\) & \(^{}\) & \(^{}\) \\  2544AVEL & 1.00 & 35.76 & 1.05 & 39.12 & 0.57 & 0.00 & 0.51 & 0.14 \\   \\ 2544AVEL & 1.00 & 35.76 & 1.05 & 39.12 & 0.57 & 0.00 & 0.51 & 0.14 \\   \\ 2544AVEL & 1.00 & 39.00 & 1.01 & 1.07 & 39.12 & 0.58 & 0.00 & 0.00 & 0.25 \\   \\ 2544AVEL & 1.00 & 39.00 & 1.01 & 1.07 & 39.04 & **0.28** & 0.00 & 0.37 \\   \\ 2544AVEL & 1.00 & 25.58 & 1.00 & 27.00 & **0.07** & 0.00 & 0.25 & 1.03 \\   \\ 2544AVEL & 1.00 & 65.44 & 2.79 & 17.18 & 0.54 & 0.04 & 0.35 & 1.30 \\   \\ 2544AVEL & 1.00 & 22.59 & 1.35 & 33.67 & 0.00 & 0.01 & 0.27 & 1.21 \\   

Table 3: The normalized performance of SafePO’s multi-agent algorithms on Safety-Gymnasium.