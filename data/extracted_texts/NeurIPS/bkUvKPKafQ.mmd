# ChatQA: Surpassing GPT-4 on Conversational QA and RAG

Zihan Liu \({}^{1}\)

&Wei Ping \({}^{1}\)&Rajarshi Roy \({}^{1}\)&Peng Xu \({}^{1}\)

Chankyu Lee \({}^{1}\)&Mohammad Shoeybi \({}^{1}\)&Bryan Catanzaro \({}^{1}\)

\({}^{1}\) NVIDIA

Correspondence to: Zihan Liu <zihanl@nvidia.com>, Wei Ping <wping@nvidia.com>

###### Abstract

In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on retrieval-augmented generation (RAG) and conversational question answering (QA). To enhance generation, we propose a two-stage instruction tuning method that significantly boosts the performance of RAG. For effective retrieval, we introduce a dense retriever optimized for conversational QA, which yields results comparable to the alternative state-of-the-art query rewriting models, while substantially reducing deployment costs. We also present the ChatRAG Bench, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score: 53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without relying on any synthetic data from OpenAI GPT models. Notably, Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09 by a margin. These results demonstrate the exceptional quality of the proposed ChatQA recipe. To advance research in this field, we open-sourced the model weights, instruction tuning data, ChatRAG Bench, and retriever for the community: https://chatqa-project.github.io/.

## 1 Introduction

Most recently, ChatGPT (OpenAI, 2022) and its follow ups (OpenAI, 2023; Anthropic, 2023; Google, 2023) have led to the paradigm shift of building question answering (QA) and retrieval-augmented generation (RAG) system in production and research community. In particular, the following aspects of the models are preferred: _i)_ The users can interact with the QA models in a conversational way, thus one can easily raise follow-up questions. _ii)_ The models are capable of integrating retrieved chunks of evidence in both open-domain or long document settings, where the provided context is much longer than the context window of LLM (e.g., Anthropic, 2023; Xu et al., 2023). _iii)_ The generalist models can answer any questions with respect to table, arithmetic calculation in zero-shot manner without dataset-specific fine-tuning, while matching the accuracies of fine-tuned models. To this end, we focus on building the state-of-the-art model with all these key capabilities that are essentially important for many real-world applications.

However, building such a model that can match the accuracy of the state-of-the-art proprietary models, e.g., GPT-4 (OpenAI, 2023), is still a grand challenge for the research community. In this work, we introduce ChatQA, a family of open-sourced models that can outperform GPT-4 while utilizing relatively weak open-weights foundation model. We also open-source our training data, detailingtechniques for synthetic data generation, along with alternative human annotation processes aimed at eliminating reliance on OpenAI GPT models for open research purpose.

Specifically, we make the following contributions:

1. We propose a two-stage instruction tuning method and design a dataset curation recipe that can largely enhance LLM's capability of integrating user provided or retrieved context for conversational QA and RAG tasks. We demonstrate that the proposed instruction tuning method significantly outperforms strong alignment baselines or RLHF-based recipes (e.g., Llama2-Chat, Llama3-Instruct) on RAG and various conversational QA tasks.
2. For retrieval, we show that fine-tuning the single-turn QA retriever on human-annotated data or synthetic multi-turn QA dataset works as well as utilizing the state-of-the-art LLM-based query rewriting model, i.e., GPT-3.5-Turbo (OpenAI, 2022). Our result also highlights the promising direction of utilizing synthetic data generation for training customized retriever.
3. We introduce ChatRAG Bench, a comprehensive benchmark with ten conversational QA datasets, including five datasets with long documents that need retrieval and three datasets with tabular data and arithmetic calculation. We apply ChatQA training recipe on different text foundation models, and show the superb generalization capability of the proposed methods. In terms of average score on ChatRAG Bench, our ChatQA-1.0-70B (54.14) based on Llama2 can outperform GPT-40613 (53.90) and GPT-4-Turbo-2024-04-09 (54.03) without utilizing any synthetic data from ChatGPT models. Notably, much smaller Llama3-ChatQA-1.5-8B can perform comparable with GPT-4 models, while Llama3-ChatQA-1.5-70B outperforms GPT-4-Turbo-2024-04-09 by a margin.
4. We study the "unanswerable" scenario, where the LLM needs to generate "cannot answer" to avoid hallucination. We show that incorporating a small amount of "unanswerable" samples significantly enhances model's capability to handle it. Our ChatQA-1.0-70B outperforms GPT-3.5-Turbo in this regard, while has a slight gap compared to GPT-4-0613 (around 3.5%).

We discuss related work in SS 2. We introduce the two-stage instruction tuning method and data curation for ChatQA in SS 3, and study retrieval in conversational QA in SS 4. We present the experimental setup in SS 5, results in SS 6, and conclude the paper in SS 7.

## 2 Related Work

### Conversational QA and RAG

Question answering in a conversational way naturally improves user experiences by addressing follow-up questions. The model can also raise clarification questions for users if necessary, which can reduce hallucination. Thus, it becomes the default format of deploying QA models in production (e.g. OpenAI, 2022; Google, 2023; Anthropic, 2023b). In contrast to the latest LLM-based generalist solution (e.g., OpenAI, 2022), most of the previous studies focus on fine-tuned expert models on specific domains or datasets (Feng et al., 2020; Izacard and Grave, 2021; Chen et al., 2022; Gao et al., 2022; Nakamura et al., 2022; Adlakha et al., 2022; Wu et al., 2023).

In recent years, many conversational QA datasets have been introduced. The models are asked to answer questions based on provided context or documents, which involves retrieval-augmented generation (RAG) if the provided documents are longer than the context window of LLM. The provided context or documents can be: _i_) text-only documents from various domains (Feng et al., 2020; Anantha et al., 2021; Saeidi et al., 2018; Adlakha et al., 2022; Aliannejadi et al., 2021; Reddy et al., 2019; Qu et al., 2020; Wu et al., 2023; Deng et al., 2022; Guo et al., 2021; Choi et al., 2018; Campos et al., 2020), or _ii_) documents comprising plain text along with tables (Pasupat and Liang, 2015; Nakamura et al., 2022; Chen et al., 2022).

### Retrieval for Multi-Turn QA

RAG is critically important for conversational QA in open-domain setting, e.g., utilizing update-to-date information from search engine, or when the proprietary documents are longer than the context window of LLM. The dense retrievers are usually trained to retrieve the top-_k_ relevant chunks given a single question (e.g., Lin et al., 2023; Wang et al., 2022; Izacard et al., 2022). In conversational QA, the follow-up questions (e.g., with pronouns referring to entities mentioned in the previous conversation) may have insufficient information for retrieval, while feeding them along with all of the dialogue history can be redundant, thus leading to sub-optimal results.

Conversational Query RewritingMost of the previous solutions are query rewriting methods. The latest turn of question is rewritten to be a standalone query without additional information from previous dialogue history (Vakulencko et al., 2021; Ye et al., 2023; Mo et al., 2023), so it can be directly used by retrieval model to retrieve relevant context (Vakulenko et al., 2021; Mele et al., 2021; Raposo et al., 2022; Mo et al., 2023). Many datasets have been collected to facilitate this line of research (Elgohary et al., 2019; Chu et al., 2020; Qu et al., 2020; Anantha et al., 2021; Brabant et al., 2022), alongside multiple proposed query rewriting methods (Ishii et al., 2022; Yu et al., 2020; Wu et al., 2022; Del Tredici et al., 2021; Chen et al., 2022; Galimzhanova et al., 2023). For example, Wu et al. (2022) and Chen et al. (2022) proposed to use reinforcement learning methods for the query rewriting. Yu et al. (2020) investigated few-shot generative models like GPT-2 for query rewriting. Galimzhanova et al. (2023) studied instruction tuned GPT-3.5-Turbo and showed that it achieved state-of-the-art results for conversational query rewriting.

Fine-tuning Retriever for multi-turn QASome previous work fine-tune a single-turn query retriever on in-domain conversational query and context pairs (Feng et al., 2020; Gao et al., 2022; Adlakha et al., 2022; Wu et al., 2023), so it can directly take a concatenation of dialog history and current query as input. In this work, we focus on the zero-shot evaluation. We fine-tune a single-turn query retriever on a high-quality multi-turn dataset. Then, we evaluate zero-shot capability of the fine-tuned retriever on five benchmark datasets. Surprisingly, we find this simple approach can obtain comparable zero-shot results as the state-of-the-art query rewriting model, i.e., GPT-3.5-Turbo.

### Instruction Tuning

The goal of instruction tuning is to equip LLMs with the capability to follow natural language instructions (Wei et al., 2022; Sanh et al., 2022; Mishra et al., 2022; Iyer et al., 2022; Du et al., 2022; Ouyang et al., 2022; Wang et al., 2023; Zhang et al., 2023; Gao et al., 2023; Chung et al., 2022; Muennighoff et al., 2022; Xu et al., 2023; Wang et al., 2022; Zhou et al., 2023; Albalak et al., 2024). There has been a surge in the development of high-quality instruction tuning datasets, including FLAN (Chung et al., 2022), Self-Instruct (Wang et al., 2022), unnatural Instructions (Honovich et al., 2022), Dolly (Conover et al., 2023), and OpenAssistant (Kopf et al., 2023).

Although numerous research on instruction tuning has been conducted, a few works focused on improving RAG or context awareness generation for QA. Lin et al. (2023) introduced a retrieval-augmented instruction tuning method, which appends top-\(k\) retrieved chunks for LLM fine-tuning. Wang et al. (2024) applied instruction tuning after retrieval-augmented pretraining. In contrast, we propose a two-stage instruction tuning method to improve generation with retrieval or provided context. We find that appending top-\(k\) retrieved chunks for LLM fine-tuning does not help for a wide range of conversation QA tasks (see Appendix SSA.1 for details). Similar to the latest work (Zhang et al., 2023), we show that adding a small amount of "unanswerable" samples in instruction tuning prompts the model to respond "cannot answer" when needed, significantly reducing hallucination.

## 3 ChatQA

In this section, we propose a two-stage instruction tuning method for ChatQA. See Figure 1 for an illustration. Our method starts with a pretrained LLM base model. At stage-1, we apply supervised fine-tuning (SFT) as in Ouyang et al. (2022) on a blend of instruction-following and dialog datasets. After that, our model exhibits good capability to follow instructions. However, its capability for contextualized or RAG-based QA remains limited. Hence, we introduce a subsequent stage, called context-enhanced instruction tuning, which is designed specifically for enhancing our model's capability for context-aware or retrieval-augmented generation in conversational QA.

Figure 1: Two-stage instruction tuning framework for ChatQA.

### Stage-1: Supervised Fine-tuning

To construct a large and comprehensive supervised fine-tuning (SFT) dataset, we follow Xu et al. (2023), Wang et al. (2024) and gather a combined set of 128K SFT samples from high-quality instruction tuning datasets. It consists of 1) a social dialogue dataset Soda (Kim et al., 2022), 2) a long-form QA dataset EL15 containing elaborate answers (Fan et al., 2019), 3) FLAN and chain-of-thought datasets (Wei et al., 2022; Chung et al., 2022; Longpre et al., 2023), 4) LLM synthetic instruction tuning datasets, including Self-Instruct (Wang et al., 2022) and Unnatural Instructions (Honovich et al., 2022), and 5) a private crowd-sourced conversational dataset, as well as two public human-written conversation datasets: OpenAssistant (Kopf et al., 2023), and Dolly (Conover et al., 2023).

We unify the structure of all the SFT data in a conversational format. We first add a "System" role at the beginning to set up a general instruction guiding LLM to provide polite and helpful answers. We also add "User" and "Assistant" roles to incorporate instruction and response pairs from the SFT dataset. We apply fine-tuning using this unified format on an LLM foundation model.

### Stage-2: Context-Enhanced Instruction Tuning

To further enhance the model's conversational QA capability over a given context, we conduct a second stage instruction tuning, which integrates contextualized QA datasets into the instruction tuning blend. One of the key elements for stage-2 is to obtain a high-quality document-grounded conversational QA dataset. We leverage two strategies of collecting such dataset. One is to leverage GPT-3.5-Turbo to generate a synthetic dataset (referred to as SyntheticConvQA). Another is to collect human-annotated dataset (referred to as HumanAnnotatedConvQA), which enables us to avoid the reliance on OpenAI models. For both datasets, we also collect unanswerable data samples the answers cannot be found within the given context. More details can be found in the Appendix B.

To boost the QA capability in handling tabular documents and arithmetic calculation, we add the TAT-QA dataset (Zhu et al., 2021) which contains both elements. In addition, we integrate contextualized single-turn QA datasets to further strengthen the QA capability of our model. We also retain the stage-1 SFT dataset in the training blend to maintain the model's instruction-following capability.

Finally, the training blend for stage-2 consists of: 1) A conversational QA dataset: HumanAnnotatedConvQA or SyntheticConvQA, 2) single-turn QA datasets: DROP (Dua et al., 2019), NarrativeQA (Kocisky et al., 2018), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD1.1 (Rajpurkar et al., 2016), SQuAD2.0 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), TAT-QA (Zhu et al., 2021), and 3) all of SFT datasets from stage-1. As for the training of Llama3-ChatQA-1.5, we additionally incorporate HybridDial (Nakamura et al., 2022) and our collected around 2K QA pairs within the financial domain to further improve our model's capability in tabular understanding and arithmetic calculations. We follow the similar template as in stage-1 to unify all single-turn QA and conversational QA datasets. The differences are in two parts: 1) Following the system role, we append relevant context for the single-turn question or the multi-turn conversation, and 2) Just before the single-turn question or multi-turn conversation, we integrate further instruction based on the answer types of different QA datasets (e.g., short answer, long answer, arithmetic calculation). We use the format for SFT dataset from stage-1. 3

Figure 2: Illustration of fine-tuning retriever for multi-turn QA.

## 4 Retrieval for Multi-Turn QA

In conversational QA tasks, when a document becomes too lengthy to feed directly into LLMs, a retriever that can handle conversational queries becomes essential. This conversational retriever encodes the concatenation of the dialogue history and the current query, and then retrieve relevant context from documents. After that, only the relevant context will be used as inputs for LLMs. The state-of-the-art retrievers, e.g., Dragon (Lin et al., 2023), are optimized for single-turn queries, resulting in a limited generalization capability for multi-turn conversational queries. In Figure 2, we depict our retriever fine-tuning method to alleviate this issue. We propose to use conversational query and context pairs for further fine-tuning a single-turn query retriever to cope with multi-turn queries.

An alternative solution is conversational query rewriting method which uses a query rewriter to rewrite the current question based on the conversational history. The rewritten query is then directly used as the input to a single-turn query retriever for retrieving relevant context. In addition to the embedding and search cost, the query rewriting model introduces a large amount of extra computational expense to generate the rewritten query.

### Fine-tuning Retriever for Multi-turn QA

To build a high-quality fine-tuning dataset, we leverage the conversational QA dataset from either HumanAnnotatedConvQA or SyntheticConvQA to construct conversational query and context pairs. For the HumanAnnotatedConvQA, we directly take the annotations of the conversational query and context pairs, and use them to further fine-tune a single-turn query retriever. For the SyntheticConvQA, we first cut each document in the conversational QA dataset into different chunks. Then, we calculate the 4-gram recall score between agent's answer and each chunk. After that, we consider the chunk that has the highest recall score as the gold chunk for the current user's question. Finally, the constructed conversational query and context pairs are used to fine-tune a single-turn query retriever.

### Conversational Query Rewriting

To build powerful conversational query rewriting model, we take GPT-3.5-Turbo as the rewriter given that Galimzhanova et al. (2023) demonstrated the state-of-the-art query rewriting results using GPT-3.5-Turbo. Similar to Galimzhanova et al. (2023), we not only provide GPT-3.5-Turbo with the rewriting task instruction, but also give it few-shot rewriting examples to enhance the quality of rewriting results. More details can be found in Appendix E.1.

### Comparisons

In Table 1, we compare the query rewriting and fine-tuning methods across five datasets in the zero-shot setting. More details about these datasets can be found in SS5.2. We conduct experiments on a state-of-the-art retriever, Dragon (Lin et al., 2023), and a strong unsupervised retriever, E5-unsupervised (Wang et al., 2022), which is not finetuned on MS MARCO (Nguyen et al., 2016). In terms of the experiments on Dragon, we find that fine-tuning performs marginally worse than query rewriting in average top-1 recall by 1.74%, while it achieves better results on average top-5 recall by 0.54%. It demonstrates the effectiveness of the fine-tuning approach for the conversational retrieval.

    &  &  &  &  &  &  \\   & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 & top-5* & top-20* & top-5* & top-20* \\  Adlakhta et al. (2022) & - & - & - & - & - & - & - & - & 70.40* & - & - \\ Wu et al. (2023) & - & - & - & - & - & - & - & - & - & - & 71.10* \\  E5-unsupervised\({}^{}\)(Wang et al., 2022) & 31.56 & 59.22 & 23.02 & 55.33 & 43.49 & 77.68 & 44.71 & 84.99 & 26.25 & 37.67 & 20.32 & 40.44 \\ E5-unsupervised + Rewrite\({}^{}\) & 33.23 & 61.02 & 25.56 & 58.00 & 46.00 & 80.01 & 45.50 & 58.89 & 27.58 & 39.15 & 21.53 & 42.04 \\ E5-unsupervised + Fine-tune\({}^{}\) & **77.95** & **75.00** & 45.28 & 80.96 & 46.52 & 80.74 & 53.37 & 89.91 & 41.01 & 51.07 & 52.79 & 72.31 \\  Dragon\({}^{}\)(Lin et al., 2023) & 46.29 & 73.09 & 43.33 & 75.61 & 56.80 & 82.86 & 46.17 & 81.96 & 57.68 & 78.80 & 27.49 & 46.22 \\ Dragon + Rewrite\({}^{}\) & **54.46** & 80.13 & 47.60 & 80.60 & 47.10 & 77.15 & 51.73 & 85.78 & 73.07 & 88.19 & 52.79 & 68.92 \\ Dragon + Fine-tune\({}^{}\) & 52.72 & **80.67** & 48.94 & 83.01 & 52.64 & 81.95 & 50.73 & 87.17 & 67.86 & 86.28 & 43.43 & 64.94 \\ - SyntheticConvQA\({}^{}\) & 52.98 & **81.15** & 48.64 & 83.47 & 54.75 & 83.23 & 49.63 & 86.70 & 64.48 & 85.24 & 47.41 & 67.13 \\   

Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. \({}^{}\)The inputs for these two models are a concatenation of the dialogue history and the current query. \({}^{}\)The input for this model is the rewritten query. \({}^{}\)denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. \({}^{}\)The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).

In addition, we observe that the results are comparable between using HumanAnnotatedConvQA and SyntheticConvQA for fine-tuning. This highlights that our human-annotated dataset is in high-quality, and we _do not rely on_ ChatGPT models for building the state-of-the-art multi-turn query retriever.

Surprisingly, fine-tuning performs significantly better than rewriting on E5-unsupervised. We conjecture that since E5-unsupervised does not use human-annotated query and context pairs in the pre-training stage, it leads to weak generalization for the high-quality rewritten query. In contrast, using a high-quality dataset to fine-tune E5-unsupervised brings a giant boost, with more than a 15% improvement on both average top-1 and top-5 recall scores.

Therefore, fine-tuning a good single-turn retriever on high-quality conversational query context pairs performs on par with leveraging the state-of-the-art rewriter. However, rewriting method requires extra computational time for autoregressive generation process and probably also API cost for using powerful models like GPT-3.5-Turbo. In contrast, our proposed multi-turn fine-tuning bypasses these issues. For the QA evaluations across these five datasets, we consistently use the retrieved top-5 results from the fine-tuning approach for all the QA models. We put more results on comparisons between rewriting and fine-tuning methods in the Appendix E.2.

## 5 Experimental Setup

### Baselines

We develop ChatQA models based on our in-house GPT-{8B, 22B} base models (pretrained with 3.5 trillion tokens), Llama2-{7B, 13B, 70B} base models (Touvron et al., 2023), and Llama3-{8B, 70B} base models (Meta, 2024). We compare ChatQA models against Llama2-Chat-{7B, 13B, 70B} and Llama3-Instruct-{8B, 70B}, which are shown to possess strong instruction following and conversational QA capabilities (Touvron et al., 2023). In addition, we compare against a powerful RAG model, Command R+, which has 104 billion parameters, as well as three very strong OpenAI models: GPT-3.5-Turbo-0613, GPT-4-0613, and GPT-4-Turbo-2024-04-09. For fair comparison, when retrieval is needed, we use the same top-\(k\) retrieved chunks from our best retriever as the context for all baselines and our ChatQA models. Note that we have carefully tuned the instructions for all the baselines to ensure they achieve as good as possible results.4

### ChatRAG Bench: Evaluation Benchmarks

To evaluate the model's capability on conversational QA and RAG, we construct ChatRAG Bench, a collection of 10 datasets covering a wide range of documents and question types, which require models to generate responses from (retrieved) context, comprehend and reason over tables, conduct arithmetic calculations, and indicate when questions cannot be found within the context.

Long Document DatasetsWe collect five conversational QA datasets with long documents: Doc2Dial (D2D) (Feng et al., 2020); QuAC (Choi et al., 2018), QReCC (Anantha et al., 2021), TopiOCQA (TCQA) (Adlakha et al., 2022), and INSCIT (Wu et al., 2023). One can find details of these datasets in Appendix F. Since the documents of these datasets cannot be directly fitted into LLMs with a sequence length of 4K or 8K tokens. Hence, we run our best multi-turn query retriever to get top-k relevant chunks as the inputs. For Doc2Dial, QuAC, and QReCC, we segment documents into around 300-word chunks, and we retrieve top-5 relevant chunks as context for each user question. For TopiOCQA and INSCIT, we follow their original segmentation, resulting in smaller chunks. Hence, we retrieved top-20 chunks to obtain similar context length to the first three datasets (experiments can be found in SS4.3).

Short Document DatasetsTo increase the diversity of document lengths, we collect five conversational QA datasets with short documents (less than 1.5K words): CoQA (Reddy et al., 2019), a dataset covering a wide range of domains; DoQA (Campos et al., 2020), a dataset covering cooking, travel and movie domains with unanswerable cases; ConvFinQA (Chen et al., 2022), a dataset based on Financial domain with tabular document and requiring arithmetic calculation and complex numerical reasoning; SQA (Pasupat & Liang, 2015); and HybriDial (HDial) (Nakamura et al., 2022). Both SQA and HybriDial have tabular documents from Wikipedia and require reasoning for complex questions.5

[MISSING_PAGE_FAIL:7]

tuning makes the average scores greatly drop by 10.92 (from 54.14 to 43.22). This is because the stage-2 tuning (i.e., context-enhanced instruction fine-tuning) enables the model to learn how to effectively leverage information from retrieved or relevant context.

Effectiveness of Single-Turn DataTo investigate how single-turn QA datasets affect model's multi-turn QA capability, we conduct an ablation study by removing them from the ChatQA-1.0-70B training blends in stage-2. As shown in Table 3, incorporating single-turn QA datasets in the stage-2 training blends generally make the scores increase across all benchmark datasets, leading to an average improvement of 1.83 score. Interestingly, we observe improvement in ConvFinQA, SQA, and HybriDial (table-based datasets), despite the added single-turn QA datasets not having tabular data in the documents. These results align with our intuitions. Adding single-turn QA datasets improves the model's capability to effectively leverage relevant context for the answers, resulting in better scores on ChatRAG Bench.

Effectiveness of Conversational QA DataWe further explore the how conversational QA data affect the model's multi-turn QA capability by removing HumanAnnotatedConvQA data from the ChatQA stage-2 training blends. As illustrated in Table 3, "w/o ConvQAData" makes the results significantly worse than ChatQA-1.0-70B (average scores degrading from 54.08 to 48.97). We observe large degradation in datasets with text-based documents, such as QuAC, QReCC, and DoQA. However, the degradation in datasets with table-based documents (e.g., ConvFinQA, SQA) are small, thanks to having TAT-QA (Zhu et al., 2021) in the training blends.

Human Annotated Data vs. GPT-3.5-Turbo Synthetic DataIn Table 3, we also compare our ChatQA models using the 7k GPT-3.5-Turbo synthetic dataset (SyntheticConvQA) and our collected 7k human-annotated dataset (HumanAnnotatedConvQA). First, we find that both achieve comparable results in terms of average scores, which suggests that we do not need to rely on synthetic data from OpenAI models to build the state-of-the-art conversational QA models. Second, we find that using human-annotated data achieved significant improvements on QuAC and DoQA datasets. This can be attributed to the fact that the human-annotated data have higher quality on unanswerable cases which exists in QuAC and DoQA datasets. Eventually, it leads to the overall improvements on these two datasets. Detail results and analyses on unanswerable cases can be found in SS6.2.

Human EvaluationDespite F1 scores being the most commonly used metrics for evaluating the quality of QA models, there are often multiple ways to answer questions, which makes the automatic metrics less than perfect. Therefore, we use human evaluations to further compare our ChatQA-1.0-70B with GPT-4-0613. In this human evaluation, we ask annotators to verify the facts in ChatQA-1.0-70B and GPT-4's outputs and determine which model provides a more accurate response to the question. The human evaluation results averaged over the 10 datasets in ChatRAG Bench are shown in Figure 3. We find that our ChatQA-1.0-70B and GPT-4 are tie most of the time (69.1%), and GPT-4 achieves slightly higher win rate (3.3%) than ours. This further confirms our model has powerful capability to produce correct answers. 7

Additional Ablation Studies & Case StudiesWe conduct ablation studies on ChatQA training in terms of using top-\(k\) chunks as context for stage-2 tuning (See Appendix A.1). Furthermore, we also study how different factors affect the model's generation quality on the inference stage, in terms of: 1) the number of retrieved context/chunks; 2) context ordering; and 3) different retrievers (See Appendix A.3). In addition to quantitative results, we perform detailed qualitative case studies, and compare the output of ChatQA-1.0-70B, GPT-3.5-Turbo-0613, and GPT-4-0613 in Appendix I.

### Evaluation of Unanswerable Case

Evaluation SetupIn this section, we study another aspect of the model's capability, which is to discern if a question can be answered within the provided context. Generating an answer in unanswerable case will lead to hallucination. To allow this evaluation, we require the model to indicate it when no answer can be found in the given context.

Figure 3: Human evaluation (A/B testing) comparing ChatQA-1.0-70B to GPT-4-0613 on ChatRAG Bench.

We use QuAC and DoQA datasets which have such unanswerable cases to evaluate such capability. Specifically, for unanswerable case, we consider the model indicating that the question cannot be answered as correct8, and as for answerable cases, we consider the model not indicating the question is unanswerable as correct (i.e., the model giving an answer). Note that for answerable cases, we only select the samples where correct context is retrieved. In the end, we calculate the average accuracy score of unanswerable and answerable cases as the final metric. We consider this average accuracy as the evaluation metric since it is in the same spirit of F1 metric which measures the harmonic mean of precision and recall scores.

ResultsIn Table 4, we compare our models with OpenAI models across QuAC and DoQA datasets. We observe that OpenAI models show powerful capability in this task, especially for GPT-4-0613. Compared to them, our best model (ChatQA-1.0-70B) achieved a significantly better average accuracy than GPT-3.5-Turbo, while we still has a slight gap compared to GPT-4 and GPT-4-Turbo (around 3.5% and 3.2%, respectively). Furthermore, we conduct ablation studies in terms of the number of unanswerable samples for training. We find that using a small amount of unanswerable samples (e.g., 1.5k) is able to achieve remarkable results on the unanswerable evaluation and incorporating more unanswerable samples does not necessarily lead to higher accuracy scores in the unanswerable evaluation (Detail results are in the Appendix H.2).

### Evaluation on Single-Turn QA and RAG Benchmark

In addition to ChatRAG Bench, we further evaluate Llama3-ChatQA-1.5 models on knowledge-intensive single-turn QA datasets: Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and HotpotQA (Yang et al., 2018), and compare them against frontier RAG models. We use the Dragon retriever (Lin et al., 2023a) to extract the top-k most relevant contexts, which are then used as the inputs for Llama3-Instruct and Llama3-ChatQA-1.5. We report the best results obtained from using the top-15, top-20, and top-25 contexts. In Table 5, we show that, despite its significantly smaller model size, Llama3-ChatQA-1.5-8B performs better than the state-of-the-art RA-DIT (65B) model. Llama3-ChatQA-1.5-70B remarkably outperforms existing frontier RAG models.

## 7 Conclusion

In this paper, we build a family of models that surpass GPT-4 on conversational QA and RAG. We introduce ChatRAG Bench, a collection of 10 conversational QA datasets covering comprehensive

   Models & Avg & Avg & QuAC & QuAC & Avg & DoQA & DoQA &  \\  & Both & QAC & (no*) & (yes*) & DoQA & (no*) & (yes*) & \\  ChatQA-1.0-70B & **77.25** & 80.76 & 77.66 & 83.85 & 73.74 & 68.81 & 78.67 & **54.14** \\ Command R+ & 68.11 & 69.61 & 41.79 & 97.42 & 66.62 & 46.37 & 86.87 & 50.93 \\ GPT-3.5-Turbo-0613 & 73.27 & 78.34 & 61.91 & 94.76 & 68.21 & 51.99 & 84.43 & 50.37 \\ GPT-4.0613 & **80.73** & 87.42 & 83.45 & 91.38 & 74.05 & 74.28 & 73.82 & **53.90** \\ GPT-4.7-Turbo-2024-04-09 & 80.47 & 88.73 & 80.42 & 97.03 & 72.21 & 72.28 & 72.13 & **54.03** \\   

Table 4: Accuracies on answerable and unanswerable samples QuAC and DoQA datasets. Avg-Both is the averaged score between QuAC and DoQA. ChatRAG is the average score on the ChatRAG Bench. * “no” and “yes” denote unanswerable and answerable samples, respectively.

   Models & Average & NQ & TriviaQA & HotpotQA \\  Atlas (11B) (Izacard et al., 2023) & 39.4 & 26.7 & 56.9 & 34.7 \\ Raven (11B) (Huang et al., 2023) & - & 29.6 & 65.7 & - \\ RECDMP (20B) (Xu et al., 2024) & 42.1 & 37.0 & 59.0 & 30.4 \\ InstructRetro (438) (Wang et al., 2024) & - & 38.9 & 65.6 & - \\ RePlug (65B) (Shi et al., 2023) & 44.5 & 28.8 & 72.6 & 32.0 \\ RA-DIT (65B) (Lin et al., 2024) & 50.1 & 35.2 & 75.4 & 39.7 \\ Llama3-Instruct-58B (Meta, 2024) & 42.5 & 30.9 & 70.7 & 26.0 \\ Llama3-Instruct-70B (Meta, 2024) & 53.6 & 42.7 & 82.4 & 35.6 \\  Llama3-ChatQA-1.5-8B & 52.3 & 42.4 & 81.0 & 33.5 \\ Llama3-ChatQA-1.5-70B & **58.7** & **47.0** & **85.6** & **42.2** \\   

Table 5: Zero-shot exact match scores on Natural Questions (NQ), TriviaQA, and HotpotQA, which were evaluated using the data split from the KILT Benchmark (Petroni et al., 2021).

evaluations on RAG, table-based reasoning, arithmetic calculations, and unanswerable scenarios. Our ChatQA-1.0-70B model built on Llama2 can slightly outperform GPT-4-0613 and GPT-4-Turbo without using any synthetic data from OpenAI GPT models. Remarkably, Llama3-ChatQA-1.5-70B even surpasses GPT-4-Turbo in all categories of ChatRAG Bench. In addition, we demonstrate that fine-tuning a single-turn query retrieve using our curated conversational QA data performs comparably to the state-of-the-art LLM-based query rewriting model, without incurring extra computational time and potential API costs associated with rewriting. Furthermore, we show that incorporating a small amount of "unanswerable" samples can significantly enhance our model's capability to handle scenarios where answers are unavailable.

## Impact Statement

In this section, we discuss potential positive and negative social impacts that could arise from our ChatQA models.

Positive ImpactsFirst, ChatQA enables users to interact with their data, including documents and tables, and provides accurate responses to their questions. This enhances productivity and efficient learning. Second, ChatQA can improve customer support services by quickly retrieving information about products, services, and common issues, allowing customer queries to be addressed more efficiently. Third, ChatQA can assist professionals, such as those in the medical field, by gathering relevant information and improving decision-making.

Potential Negative ImpactsFirst, although our ChatQA models demonstrate impressive accuracy in generating answers, they still have the potential to provide factually inaccurate responses. Second, LLMs can inadvertently learn and perpetuate biases present in their pre-training data, which may lead to biased responses in our QA models as they are further fine-tuned based on these LLMs. Third, there is a risk of malicious use of our ChatQA models, such as gathering personal information by inputting documents containing relevant personal content.

## Limitation

ChatQA models are optimized for RAG and conversational QA across various types of questions and documents, making them very useful in many real-world applications. However, they may not perform well on code-related tasks or math reasoning tasks compared to GPT-4 and other versatile frontier models, because ChatQA's instruction-tuning blend does not include any code generation data. In the future, we plan to include more code-related and math reasoning data to support potential use cases.