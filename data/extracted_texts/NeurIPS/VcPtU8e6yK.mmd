# Bifrost: 3D-Aware Image compositing with Language Instructions

Lingxiao Li\({}^{1}\) Kaixiong Gong\({}^{1}\) Weihong Li\({}^{1}\) Xili Dai\({}^{2}\)

**Tao Chen\({}^{3}\) Xiaojun Yuan\({}^{4}\) Xiangyu Yue\({}^{1}\)**

\({}^{1}\)MMLab, The Chinese University of Hong Kong

\({}^{2}\)The Hong Kong University of Science and Technology (Guangzhou)

\({}^{3}\)Fudan University \({}^{4}\)University of Electronic Science and Technology of China

https://github.com/lingxiao-li/Bifrost

 Corresponding author

###### Abstract

This paper introduces _Bifrost_, a novel 3D-aware framework that is built upon diffusion models to perform instruction-based image composition. Previous methods concentrate on image compositing at the 2D level, which fall short in handling complex spatial relationships (_e.g._, occlusion). _Bifrost_ addresses these issues by training MLLM as a 2.5D location predictor and integrating depth maps as an extra condition during the generation process to bridge the gap between 2D and 3D, which enhances spatial comprehension and supports sophisticated spatial inter

Figure 1: _Bifrost_ **results on various personalized image compositing tasks.****Top:**_Bifrost_ is adept at precise, arbitrary object placement and replacement in a background image with a reference object and a language instruction, and achieves 3D-aware high-fidelity harmonized compositing results; **Bottom Left:** Given a coarse mask, _Bifrost_ can change the pose of the object to follow the shape of the mask; **Bottom Right:** Our model adapts the identity of the reference image to the target image without changing the pose.

actions. Our method begins by fine-tuning MLLM with a custom counterfactual dataset to predict 2.5D object locations in complex backgrounds from language instructions. Then, the image-compositing model is uniquely designed to process multiple types of input features, enabling it to perform high-fidelity image compositions that consider occlusion, depth blur, and image harmonization. Extensive qualitative and quantitative evaluations demonstrate that _Bifrost_ significantly outperforms existing methods, providing a robust solution for generating realistically composited images in scenarios demanding intricate spatial understanding. This work not only pushes the boundaries of generative image compositing but also reduces reliance on expensive annotated datasets by effectively utilizing existing resources in innovative ways.

+
Footnote †: In Norse mythology, a burning rainbow bridge that transports anything between Earth and Asgard.

## 1 Introduction

Image generation has flourished alongside the advancement of diffusion models (Song et al., 2021; Ho et al., 2020; Rombach et al., 2022; Ramesh et al., 2022). Recent works (Saharia et al., 2022; Liu et al., 2023; Brooks et al., 2023; Zhang et al., 2023; Huang et al., 2023; Li et al., 2023; Chen et al., 2024; He et al., 2024) add conditional controls, _e.g._, text prompts, scribbles, skeleton maps to the diffusion models, offering significant potentials for controllable image editing. Among these methods for image editing, generative object-level image compositing (Yang et al., 2023; Song et al., 2023; Chen et al., 2024; Song et al., 2024) is a novel yet challenging task that aims to seamlessly inject an outside reference object into a given background image with a specific location, creating a cohesive and realistic image. This ability is significantly required in practical applications including E-commerce, effect-image rendering, poster-making, professional editing, _etc._

Achieving arbitrary personalized object-level image compositing necessitates a deep understanding of the visual concept inherent to both the identity of the reference object and spatial relations of the background image. To date, this task has not been well addressed. Paint-by-Example (Yang et al., 2023) and Objectstitch (Song et al., 2023) use a target image as the template to edit a specific region of the background image, but they could not generate ID-consistent contents, especially for untrained categories. On the other hand, (Chen et al., 2024; Song et al., 2024) generate objects with ID (identity) preserved in the target scene, but they fall short in processing complicated 3D geometry relations (_e.g._, the occlusion) as they only consider 2D-level composition. To sum up, previous methods mainly either **1)** fail to achieve both ID preservation and background harmony, or **2)** do not explicitly take into account the geometry behind the background and fail to accurately composite objects and backgrounds in complex spatial relations.

We conclude that _the root cause of aforementioned issues is that image composition is conducted at a 2D level_. Ideally, the composition operation should be done in a 3D space for precise 3D geometry relationships. However, accurately modeling a 3D scene with any given image, especially with only one view, is non-trivial and time-consuming (Liu et al., 2023). To address these challenges, we introduce _Bifrost_, which offers a 3D-aware framework for image composition without explicit 3D modeling. We achieve this by leveraging depth to indicate the 3D geometry relationship between the object and the background. In detail, our approach leverages a multi-modal large language model (MLLM) as a 2.5D location predictor (_i.e.,_ bounding box and depth for the object in the given background image). With the predicted bounding box and depth, our method yields a depth map for the composited image, which is fed into a diffusion model as guidance. This enables our method to achieve good ID preservation and background harmony simultaneously, as it is now aware of the spatial relations between them, and the conflict at the dimension of depth is eliminated. In addition, MLLM enables our method to composite images with text instructions, which enlarges the application scenario of _Bifrost_. _Bifrost_ achieves significantly better visual results than the previous method, which in turn validates our conclusion.

We divide the training procedure into two stages. In the first stage, we finetune an MLLM (_e.g._, LLaVA (Liu et al., 2023, 2024)) for 2.5D predictions of objects in complex scenes with language instructions. In the second stage, we train the image composition model. To composite images with complicated spatial relationships, we introduce depth maps as conditions for image generation. In addition, we leverage an ID extractor to generate discriminative ID tokens and a frequency-aware detail extractor to extract the high-frequency 2D shape information (detail maps) for ID preservation. We unify the depth maps, ID tokens, and detail maps to guide a pre-trained text-to-image diffusion model to generate desired image composition results. This two-stage training paradigm allowsus to utilize a number of existing 2D datasets for common visual tasks _e.g._, visual segmentation, and detection, avoiding collecting large-volume text-image data specially designed for arbitrary object-level image compositing.

Our main contributions can be summarized as follows: **1)** We are the first to embed _depth_ into the image composition pipeline, which improves the ID preservation and background harmony simultaneously. **2)** We delicately build a counterfactual dataset and fine-tuned MLLM as a powerful tool to predict 2.5D location of the object in a given background image. Further, the fine-tuned MLLM enables our approach to understand language instructions for image composition. **3)** Our approach has demonstrated exceptional performance through comprehensive qualitative assessments and quantitative analyses on image compositing and outperforms other methods, _Bifrost_ allows us to generate images with better control of occlusion, depth blur, and image harmonization.

## 2 Related Work

### Image compositing with Diffusion Models

Image compositing, an essential technique in image editing, seamlessly integrates a reference object into a background image, aiming for realism and high fidelity. Traditional methods, such as image harmonization (Jiang _et al._, 2021; Xue _et al._, 2022; Guerreiro _et al._, 2023; Ke _et al._, 2022) and blending (Perez _et al._, 2003; Zhang _et al._, 2020, 2021; Wu _et al._, 2019) primarily ensure color and lighting consistency but inadequately address geometric discrepancies. The introduction of diffusion models (Ho _et al._, 2020; Sohl-Dickstein _et al._, 2015; Song _et al._, 2021; Rombach _et al._, 2022) has shifted focus towards comprehensive frameworks that address all facets of image compositing. Methods like (Yang _et al._, 2020; Song _et al._, 2023) often use CLIP-based adapters to utilize pretrained models, yet they compromise the object's identity preservation, focusing mainly on high-level semantic representations.More recent studies prioritize maintaining the appearance in generative object compositing. Notable developments in this field include AnyDoor (Chen _et al._, 2024) and ControlCom (Zhang _et al._, 2023a). AnyDoor integrates DINOv2 (Oquab _et al._, 2023) with a high-frequency filter, while ControlCom introduces a local enhancement module, both improving appearance retention. However, these approaches still face challenges in spatial correction capabilities. Most recent work IMPRINT (Song _et al._, 2024) trains an ID-preserving encoder that enhances the visual consistency of the object while maintaining geometry and color harmonization. However, none of the work can deal with composition with occlusion and more complex spatial relations. In contrast, our models novelly propose a 3D-aware generative model that allows more accuracy and complex image compositing while maintaining geometry and color harmonization.

### LLM with Diffusion Models

The open-sourced LlaMA (Touvron _et al._, 2023; Chiang _et al._, 2023) substantially enhances vision tasks by leveraging Large Language Models (LLMs). Innovations such as LLaVA and MiniGPT-4 (Liu _et al._, 2023a; Zhu _et al._, 2024) have advanced image-text alignment through instruction-tuning. While many MLLM-based studies have demonstrated effectiveness in text-generation tasks like human-robot interaction, complex reasoning, and science question answering, GILL (Koh _et al._, 2023) acts as a conduit between MLMs and diffusion models by enabling LLMs to process and generate coherent images from textual inputs. SEED (Ge _et al._, 2023b) introduces a novel image tokenizer that allows LLMs to handle and concurrently generate images and text, with SEED-2 (Ge _et al._, 2023a) enhancing this process by better aligning generation embeddings with image embeddings from unCLIP-SD, thus improving the preservation of visual semantics and realistic image reconstruction. Emu (Sun _et al._, 2024), a multimodal generalist, is trained on a next-token-prediction model. CM3Leon (Yu _et al._, 2023a), utilizing the CM3 multimodal architecture and adapted training methods from text-only models, excels in both text-to-image and image-to-text generations. Finally, SmartEdit (Huang _et al._, 2024) proposes a Bidirectional Interaction Module that enables comprehensive bidirectional information interactions between images and the MLLM output, allowing it for complex instruction-based image editing. Nevertheless, these methods requires text-image pairs data to train and do not support accurate spatial location prediction and subject-driven image compositing.

## 3 Method

The overall pipeline of our _Bifrost_ is elaborated in Fig. 2. Our method consists of two stages: 1) in stage 1, given the input image of object and background, and text instruction that indicates the location for object compositing to the background, the MLLMs are finetuned on our customized dataset for predicting a 2.5D location, which provides the bounding box and a depth value of the object in the background; 2) in stage 2, our _Bifrost_ performs 3D-aware image compositing according to the generated 2.5D location, images of object and background and their depth maps estimated by a depth predictor. As we divide the pipeline into two stages, we can adopt the existing benchmarks that have been collected for common vision problems and avoid the demand of collected new and task-specific paired data.

We detail our pipeline in the following section. In Sec. 3.1 we discuss building our customized counterfactual dataset and fine-tuning multi-modal large language models to predict 2.5D locations given a background image. Following this, in Sec. 3.2, we introduce the 3D-aware image compositing pipeline that uses the spatial location predicted by MLLM to seamlessly integrate the reference object into the background image. Finally, we discuss combining two stages in Sec. 3.3 and show more application scenarios of our proposed method.

### Finetuning MLLM to Predict 2.5D Location

Given a background image \(_{bg}\) and text instruction \(_{T}\), which is tokenized as \(H_{T}\), our goal is to obtain the 2.5D coordinate of the reference object we want to place in. We indicate the 2.5D coordinate as \(l\) which consists of a 2D bounding box \(b=[x_{1},y_{1},x_{2},y_{2}]\) and an estimated depth value \(d\). During the training stage, the majority of parameters \(\) in the LLM are kept frozen, and we utilize LoRA (Hu _et al._, 2022) to carry out efficient fine-tuning. Subsequently, for a sequence of length \(L\), we minimize the negative log-likelihood of generated text tokens \(X_{A}\), which can be formulated as:

\[L_{}(_{bg},_{T})=-_{i=1}^{L} p_{\{\} }(x_{i}_{bg},_{T,<i},X_{A,<i})\] (1)

**Dataset Generation.** However, there is a lack of dataset containing image-text data pairs to teach the MLLM to predict the reasonable location to place in the background image \(_{bg}\) following the instruction \(_{T}\) and it is crucial to create such dataset. Since the model needs to predict both a 2D bounding box and depth value in \(Z\) axis, this requires the model to be capable of understanding both the spatial relationship between objects and the size relationship between objects (_e.g._, the bounding box of a dog should be smaller than the car if the user wants to place a dog near the car) and the physical laws in the image (_e.g._, a bottle should be placed on the table rather than floating in the air).

Figure 2: **Overview of the inference pipeline of _Bifrost_. Given background image \(_{bg}\), and text instruction \(_{T}\) that indicates the location for object compositing to the background, the MLLM first predicts the 2.5D location consists of a bounding box and the depth of the object. Then a pre-trained depth predictor is applied to estimate the given images’ depth. After that. The depth of the reference object is scaled to the depth value predicted by MLLM and fused in the predicted location of the background depth. Finally, the masked background image, fused depth, and reference object image are used as the input of the compositing model and generate an output image \(_{out}\) that satisfies spatial relations in the text instruction \(_{T}\) and appears visually coherent and natural (_e.g._, with light and shadow that are consistent with the background image).**Furthermore, the image should not contain the object we want to place (_i.e._, the MLLM should learn to predict the location). To this end, we build a novel counterfactual dataset based on MS COCO dataset (Lin et al., 2014). For a background image \(x\) and annotation \(a\), we use a pre-trained depth predictor DPT (Ranft et al., 2021) to estimate the depth map \(d\). We randomly select \(k\) objects and choose one as the target (\(o_{s}\)), defining spatial relations with the other \(k-1\) objects (_e.g._, left of, above, in front of). GPT-3 (Brown et al., 2020) generates text descriptions \(D\) mentioning all objects and their relative positions, as shown in Fig. 3. The ground truth includes the bounding box and depth value of \(o_{s}\). Using diffusion-based inpainting (Yu et al., 2023b), we remove \(o_{s}\) from \(_{bg}\). We collect 30,080 image-instruction-answer pairs for training and 855 for testing, with examples in Fig. 4. Further details are in Appendix B.1.

### 3D-Aware Image compositing

The training pipeline of _Bifrost_'s 3D-aware image compositing module is shown in Fig. 5. Given the reference object, background, and estimated 2.5D location, _Bifrost_ extracts ID Token, detail map, and depth maps to generate high-fidelity, diverse object-scene compositions that respect spatial relations. Unlike previous works using only 2D backgrounds and objects, our key contribution is incorporating depth maps to account for spatial relationships. Additionally, we use large-scale data, including videos and images, to train the model to learn the appearance changes, ensuring high fidelity in generated images. Details of different components are as follows.

**ID Extractor.** We leverage pre-trained DINO-V2 (Oquab et al., 2023) to encode images into visual tokens to preserve more information. We use a single linear layer to bridge the embedding space of DINOV2 and the pre-trained text-to-image UNet. The final projected ID tokens can be gotten by:

\[_{i}=_{i}(_{obj}),\] (2)

where \(_{i}\) indicates the ID extractor.

**Detail Extractor.** The ID tokens inevitably lose the fine details of the reference object due to the information bottleneck. Thus, we need extra guidance for the complementary detail generation. We apply a **high-frequency map** to represent fine-grained details of the object. We further applied **mask shape augmentation** to provide more practical guidance of the pose and view of the generated object, which mimics the casual user brush used in practical editing. The results are shown at the bottom of Fig. 1. After obtaining the high-frequency map, we stitch it onto the scene image at the specified locations and pass the collage to the detail extractor:

\[_{}=_{h}(_{h}),\] (3)

Figure 4: **Examples of 2.5D counterfactual dataset for fine-tuning MLLM.**

Figure 3: **Overview of the 2.5D counterfactual dataset generation for fine-tuning MLLM.** Given a scene image \(I\), one object \(o\) was randomly selected as the object we want to predict (_e.g._, the laptop in this figure). The depth of the object is predicted by a pre-trained depth predictor. The selected object is then removed from the given image using the SAM (_i.e._ mask the object) followed by an SD-based inpainting model (_i.e._, inpaint the masked hole). The final data pair consists of a **text instruction**, a **counterfactual image**, and a 2.5D **location** of the selected object \(o\).

where \(_{h}\) is the detail extractor and \(_{}\) is the extracted high-frequency condition. More details can be found in Appendix B.2

**Depth Extractor.** As we mentioned in Sec. 1, all existing image compositing works can only insert objects as foreground. As Fig. 7 shows, the generated images look wired when the inserted object has some overlap with objects in the background. We tackle this problem by proposing a simple yet effective method by adding depth control to the model. By utilizing pre-trained depth predictor DPT (Ranftl _et al._, 2021), we could generate depth-image pairs without demanding extra annotation. Formally, given a target image \(_{tar}\), we have

\[_{}=_{d}((_{tar})),\] (4)

where \(_{d}\) is the depth extractor and \(_{}\) is the extracted depth condition. In our experiment, the detail and depth extractors utilize ControlNet-style UNet encoders, generating hierarchical detail maps at multiple resolutions.

**Feature Injection.** Given an image \(z_{0}\), image diffusion algorithms progressively add noise to the image and produce a noisy image \(z_{t}\), where t represents the number of times noise is added. Given a set of conditions including time step \(t\), object ID condition \(_{i}\), high-frequency detail condition \(_{h}\), as well as a depth condition \(_{d}\), image diffusion algorithms learn a UNet \(_{}\) to predict the noise added to the noisy image \(z_{t}\) with

\[_{composite}=_{_{0},,_{i},_{f} (0,1)}[|-_{}(_{t}, ,_{i},_{}).]| 2],\] (5)

where \(_{f}=_{h}+_{d}\) and \(\) is a hyper parameter weight between two controls. Specifically, the ID tokens are injected into each UNet layer via cross-attention. The high-frequency detail and depth maps are first added together and then concatenated with the UNet decoder features at each resolution. During training, the pre-trained UNet encoder parameters are frozen to retain priors, while the decoder is fine-tuned to adapt to the new task.

**Classifier-free Guidance.** To achieve the trade-off between identity preservation and image harmonization, we find that classifier-free sampling strategy (Ho and Salimans, 2022) is a powerful tool.

    & YouTubeVOS & MOSE & VIPSeg & VitonHD & MSRA-10K & DUT & HFIickr & LVIS & SAM (subset) \\ 
**Type** & Video & Video & Video & Image & Image & Image & Image & Image & Image \\
**\# Samples** & 4453 & 1507 & 3110 & 11647 & 10000 & 15572 & 4833 & 118287 & 178976 \\
**Variation** & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ & ✗ \\   

Table 1: **Statistics of the datasets used in the image compositing stage.**

Figure 5: **Overview of training pipeline of _Bifrost_ on image compositing stage.** A segmentation module is first adopted to get the masked image and object without background, followed by an ID extractor to obtain its identity information. The high-frequency filter is then applied to extract the detail of the object, stitch the result with the scene at the predicted location, and employ a detail extractor to complement the ID extractor with texture details. We then use a depth predictor to estimate the depth of the image and apply a depth extractor to capture the spatial information of the scene. Finally, the ID tokens, detail maps, and depth maps are integrated into a pre-trained diffusion model, enabling the target object to seamlessly blend with its surroundings while preserving complex spatial relationships.

Previous work (Tang _et al._, 2022) found that the classifier-free guidance is actually the combination of both prior and posterior constraints.

\[ p(_{t})+(s-1) p( _{t})\, p(_{t})+s ( p(_{t})- p(_ {t})),\] (6)

where \(s\) denotes the classifier-free guidance scale. In our experiments, we follow the settings in (Zhang _et al._, 2023b).

\[_{}=_{}+s(_{}- _{}),\] (7)

where \(_{}\), \(_{}\), \(_{}\), \(s\) are the model's final output, unconditional output, conditional output, and a user-specified weight respectively. In the training process, we randomly replace \(50\%\) object ID condition \(_{i}\) with empty strings. This approach increases _Bifrost_'s ability to directly recognize semantics in the input conditions as a replacement for the ID tokens. We further replace \(30\%\) of the depth map or detail map as blank to increase the robustness of our model. (_i.e._, our model could generate high-quality images with only one condition, either from detail or depth control).

**Dataset Generation.** The ideal training data consists of image pairs capturing the same object in different scenes and poses, which existing datasets lack. Previous works (Yang _et al._, 2023; Song _et al._, 2023) use single images with augmentations like rotation and flip, which are insufficient for realistic pose and view variants. To address this, we use video datasets to capture frames of the same object as complementary following (Chen _et al._, 2024; Song _et al._, 2024). As illustrated in Fig. 6, our data preparation pipeline selects two frames from a video and extracts foreground masks. One frame is masked and cropped around the object to serve as the reference, while the other frame--with an augmented mask shape--acts as the background. The unmasked version of this second frame serves as the training ground truth. The dataset quality is another key to better identity preservation and pose variation. The full data used is listed in Tab. 1, which covers a large variety of domains such as nature scenes (SAM (Kirillov _et al._, 2023), LVIS (Gupta _et al._, 2019), HFlickr (Cong _et al._, 2020), DUT (Wang _et al._, 2017), and MSRA-10K (Borji _et al._, 2015)), panoptic video segmentation datasets (YoutubeVOS (Xu _et al._, 2018), VIPSeg (Miao _et al._, 2022), and MOSE (Ding _et al._, 2023)), and virtual try-on dataset (VitongID (Choi _et al._, 2021)).

### Inference

The overall inference pipeline is shown in Fig. 2. Given background image \(_{bg}\), and text instruction \(_{T}\), our fine-tuned MLLM predicts the 2.5D location of object formulated as bounding box \(b=[x_{1},y_{1},x_{2},y_{2}]\) and an estimated depth value \(d\). Then, we first scale the depth map of the reference image to the predicted depth and fuse it into the bounding box location of the background depth map. The background image is masked following the bounding box. Finally, one can generate the composited image following the pipeline in Fig. 2. We also support various formats of input (_e.g._, user-specified mask and depth), which allows more application scenarios as Fig. 1 shows. More details can be found in the Appendix B.

## 4 Experiment

### Implementation Details

**Hyperparameters.** We choose LLaVA (Liu _et al._, 2023a, 2024) as our method to fine-tune multi-modal large language models and Vicuna (Chiang _et al._, 2023) as the LLM. The learning rate is

    & MiniGPTv2 & LLaVA (baseline) & Ours \\ 
**BBox(MSE)** (\(\)) & 0.2694 & 0.0653 & **0.0496** \\
**BBox(IoU)** (\(\)) & 0.0175 & 0.7567 & **0.8515** \\
**Depth (MSE)** (\(\)) & \(\) & \(\) & **0.0658** \\   

Table 2: **Quantitative evaluation results on the accuracy of the MLLM’s prediction of _Bifrost_. Note: MiniGPTv2 and LLaVA (baseline) do not support depth prediction.**

Figure 6: **Data preparation pipeline of leveraging videos. Given a clip, we first sample two frames, selecting an instance from one frame as the reference object and using the corresponding instance from the other frame as the training supervision.**

set as \(2e^{-5}\) and train 15 epochs. We choose Stable Diffusion V2.1 (Rombach _et al._, 2022) as the base generator for the image compositing model. During training, we set the image resolution to \(512 512\). We choose Adam (Kingma and Ba, 2014) optimizer with an initial learning rate of \(1e^{-5}\). More details can be found in the Appendix A.

**Zoom-in Strategy.** During inference, given a scene image and a location box, we expand the box into a square using an amplification ratio of 2.0. The square is then cropped and resized to \(512 512\) for input into the diffusion model. This approach enables handling scenes with arbitrary aspect ratios and location boxes covering extremely small or large areas.

**Benchmarks.** To evaluate the performance of our Fine-tuned MLLM, we collect 855 image-instruction-answer pairs for testing as we mentioned in Sec. 3.1. For quantitative results of spatial aware image compositing, we follow the settings in (Chen _et al._, 2024) that contain 30 new concepts from DreamBooth (Ruiz _et al._, 2023) for the reference images. We manually pick 30 images with boxes in COCO-Val (Lin _et al._, 2014) for the scene image. Thus, we generate 900 images for the object-scene combinations.

**Evaluation metrics.** We test the IoU and MSE loss to evaluate the accuracy of the predicted bounding box and depth of our MLLM. For the image compositing model, we evaluate performance on our constructed DreamBooth dataset by following DreamBooth (Ruiz _et al._, 2023) to calculate the CLIP-Score and DINO-Score, which measure the similarity between the generated region and the target object. We also compute the FID (Heusel _et al._, 2017) to assess realism and compositing quality. Additionally, we conduct user studies with 30 annotators to rate the results based on fidelity, quality, diversity, and 3D awareness.

   & Paint-by-Example & Object-Stitch & TF-ICON & AnyDoor & Ours (w/o depth) & Ours (w/ depth) \\ 
**Quality** (\(\)) & 2.24 & 2.66 & 2.75 & 3.57 & 3.64 & **3.96** \\
**Fidelity** (\(\)) & 2.05 & 2.56 & 2.63 & 3.58 & 3.89 & **4.03** \\
**Diversity** (\(\)) & **3.87** & 2.42 & 2.36 & 3.57 & 3.61 & 3.07 \\
**3D Awareness** (\(\)) & 3.56 & 3.37 & 3.42 & 2.51 & 2.56 & **4.21** \\  

Table 4: **User study** on the comparison between our _Bifrost_ and existing alternatives. “Quality”, “Fidelity”, “Diversity”, and “3D Awareness” measure synthesis quality, object identity preservation, object local variation (_i.e._, across four proposals), and spatial relation awareness (_i.e._, occlusion) respectively. Each metric is rated from 1 (worst) to 5 (best).

Figure 7: **Qualitative comparison with reference-based image generation methods, including Paint-by-Example (Yang _et al._, 2023), ObjectStitch (Song _et al._, 2023), and AnyDoor (Chen _et al._, 2024), where our _Bifrost_ better preserves the geometry consistency. Note that all approaches do not fine-tune the model on the test samples.**

    & Paint-by-Example & Object-Stitch & TF-ICON & AnyDoor & Ours \\ 
**DINO-score** (\(\)) & 72.225 & 73.108 & 74.743 & 76.807 & **77.746** \\
**CLIP-score** (\(\)) & 84.584 & 84.836 & 85.627 & 88.735 & **89.722** \\
**FID** (\(\)) & 22.286 & 19.489 & 18.945 & 15.858 & **15.025** \\   

Table 3: **Quantitive evaluation results on the performance of image compositing. _Bifrost_ outperforms all other methods across all metrics.**

### Quantitative Evaluation

**MLLM Evaluation.** To evaluate the effectiveness of our fine-tuned MLLM, we compare our model with two vison-language LLMs: LLaVA (Liu _et al._, 2023) and miniGPTv2 (Zhu _et al._, 2024). However, to our knowledge, none of the MLLM support accurate depth prediction. The results are shown in Tab. 2, which indicates that our fine-tuned MLLM outperforms other models significantly on the spatial bounding box prediction task.

**Spatial-Aware Image compositing Evaluation.** To demonstrate the effectiveness of our model, we test our model and four existing methods (Paint-by-Example (Yang _et al._, 2023), ObjectStitch (Song _et al._, 2023), TF-ICON (Lu _et al._, 2023)), and AnyDoor (Chen _et al._, 2024) on the dreambooth (Ruiz _et al._, 2023) test sets. The same inputs (a mask and a reference object) are used in all models. As shown in Tab. 3, _Bifrost_ consistently outperforms baselines across all metrics, indicating that our model generates images with both realism and fidelity.

### Qualitative Evaluation

To better evaluate the performance of our spatial-aware image compositing model, we qualitatively compare our method against prior methods as shown in Fig. 7. PbE and ObjectStitch show natural compositing effects, but they fail to preserve the object's ID when the object has a complex texture or structure. Although AnyDoor maintains a fine-grained texture of the object, it can not handle occlusion with other objects in the scene. In contrast, our model achieves better ID preservation and demonstrates flexibility in adapting to the background even in complex scenes with occlusion(_i.e._, the poop emoji in the third row is seamlessly injected between the bowl and flowerpot with no artifact). We also provide more results of other application scenarios in Fig. 8.

**User Study.** We organize user study to compare Paint-by-Example, ObjectStitch, TF-ICON, AnyDoor, and our model. The user-study results are listed in Tab. 4. It shows that our model owns evident superiorities for fidelity, quantity, and 3D awareness, especially for 3D awareness. Without depth control, our model can generate more diverse objects adjusted for the background. Nevertheless, the quality fidelity, and 3D awareness degrade significantly if the depth control is removed. However, as (Yang _et al._, 2023) only keeps the semantic consistency but our methods preserve the instance identity, they naturally have larger space for the diversity. In this case, _Bifrost_ still gets higher rates than (Song _et al._, 2023; Lu _et al._, 2023) and competitive results with (Chen _et al._, 2024), which verifies the effectiveness of our method. This results indicate that introducing the depth information is the key for _Bifrost_ to achieve both high fidelity and 3D-aware image compositing. More details are in the Appendix F.

Figure 8: **Results of other application scenarios of _Bifrost_.**

Figure 9: **Qualitative ablation study on the core components of _Bifrost_, where the last column is the result of our full model, “HF-Filter” stands for the high-frequency filter in the detail extractor.**

### Ablation Study

Tab. 5 shows the effect of components in _Bifrost_. Performance drops significantly without video data during training, as it cannot adjust poses and views to adapt to unseen scenes with only image data. Fig. 9 visualizes results of removing different components. CFG achieves a trade-off between identity preservation and image harmonization, significantly boosting performance. Setting the collage region from the high-frequency map to an all-zero map evaluates the HF Filter's contribution, showing it effectively guides the generation of fine structural details. Adding depth control further improves the fidelity and quality of generated images, as visualized in Fig. 10, where the red vase moves from behind the table to the front of the white vase as depth increases.

### Limitations and Future Work

Our _Bifrost_ achieves high-quality, instruction-based image compositing but has limitations. **1)** While our fine-tuned MLLM performs well on in-domain test datasets, it struggles with OOD datasets containing untrained objects and more complex scenes. A failure case is illustrated in Fig. 11, where the model intends to position the backpack behind the bed. While MLLM predicts the correct location, it overlaps with the chair in the background. Nonetheless, our approach still outperforms prior work (Chen _et al._, 2024), which fails to handle occlusions between the bed and chair due to insufficient understanding of spatial and depth relationships. This issue can be addressed by increasing the size of the training dataset (_e.g._, utilizing large-scale, OpenImages (Kuznetsova _et al._, 2018). **2)** The use of depth maps significantly enhances our model's generation capabilities, enabling precise control over object placement and complex spatial relationships. However, this reliance on depth maps restricts the diversity of generated objects, particularly in terms of novel poses and views. Although we employ classifier-free guidance during inference to provide some flexibility, it represents a trade-off between spatial control and object diversity. This can potentially be solved by robust depth control during the training stage. We leave this for future work.

## 5 Conclusion

In conclusion, _Bifrost_ represents a significant advancement in object-level image compositing. Leveraging the capabilities of powerful MLLM, our framework successfully addresses the limitations of existing SD-based image compositing methods, facilitating their transition from research to practical applications. Our experimental results demonstrate that _Bifrost_ not only achieves state-of-the-art quality and fidelity in instruction-based, object-level image compositing but also provides a controllable generation process that accommodates complex spatial relationships. Future research endeavors encompass the refinement of the MLLM through the utilization of more extensive datasets and extending our framework to support 3D and video-based personalized object compositing tasks, further broadening its applicability and enhancing its performance.

Figure 11: **Failure case** from the out-of-distribution dataset and comparison with AnyDoor.

Figure 10: **Ablation study** of different depth control from deep to shallow.

    & Baseline & +Video Data & +CFG & +HF Filter & +Depth \\ 
**DINO-score** (\(\)) & 68.693 & 71.573 & 75.578 & 76.372 & **77.746** \\
**CLIP-score** (\(\)) & 80.458 & 83.536 & 86.394 & 88.634 & **89.722** \\
**FID** (\(\)) & 20.465 & 17.168 & 15.837 & 15.342 & **15.025** \\   

Table 5: **Quantitative ablation studies** on the core components of the image compositing model of _Bifrost_. Note that: \(+\) indicates adding one component based on the previous model.