# Distributionally Robust Bayesian Optimization

with \(\)-divergences

Hisham Husain

Amazon

hushisha@amazon.com

&Vu Nguyen

Amazon

vutngn@amazon.com

&Anton van den Hengel

Amazon

hengelah@amazon.com

###### Abstract

The study of robustness has received much attention due to its inevitability in data-driven settings where many systems face uncertainty. One such example of concern is Bayesian Optimization (BO), where uncertainty is multi-faceted, yet there only exists a limited number of works dedicated to this direction. In particular, there is the work of Kirschner et al. , which bridges the existing literature of Distributionally Robust Optimization (DRO) by casting the BO problem from the lens of DRO. While this work is pioneering, it admittedly suffers from various practical shortcomings such as finite contexts assumptions, leaving behind the main question _Can one devise a computationally tractable algorithm for solving this DRO-BO problem_? In this work, we tackle this question to a large degree of generality by considering robustness against data-shift in \(\)-divergences, which subsumes many popular choices, such as the \(^{2}\)-divergence, Total Variation, and the extant Kullback-Leibler (KL) divergence. We show that the DRO-BO problem in this setting is equivalent to a finite-dimensional optimization problem which, even in the continuous context setting, can be easily implemented with provable sublinear regret bounds. We then show experimentally that our method surpasses existing methods, attesting to the theoretical results.

## 1 Introduction

Bayesian Optimization (BO)  allows us to model a black-box function that is expensive to evaluate, in the case where noisy observations are available. Many important applications of BO correspond to situations where the objective function depends on an additional context parameter , for example in health-care, recommender systems can be used to model information about a certain type of medical domain. BO has naturally found success in a number of scientific domains  and also a staple in machine learning for the crucial problem of hyperparameter tuning .

As with all data-driven approaches, BO is prone to cases where the given data _shifts_ from the data of interest. While BO models this in the form of Gaussian noise for the inputs to the objective function, the context distribution is assumed to be consistent. This can be problematic, for example in healthcare where patient information shifts over time. This problem exists in the larger domain of operations research under the banner of _distributionally robust optimization_ (DRO) , where one is interested in being _robust_ against shifts in the distribution observed. In particular, for a given _distance_ between distributions \(\), DRO studies robustness against adversaries who are allowed to modify the observed distribution \(p\) to another distribution in the set:

\[\{q:(p,q)\},\]

for some \(>0\). One can interpret this as a ball of radius \(\) for the given choice of \(\) and the adversary perturbs the observed distribution \(p\) to \(q\) where \(\) is a form of "budget".

Distributional shift is a topical problem in machine learning and the results of DRO have been specialized in the context of supervised learning [12; 13; 11; 10; 7; 16; 22], reinforcement learning  and Bayesian learning , as examples. One of the main challenges however is that the DRO is typically intractable since in the general setting of continuous contexts, involves an infinite dimensional constrained optimization problem. The choice of D is crucial here as various choices such as the Wasserstein distance [6; 7; 9; 48], Maximum Mean Discrepancy (MMD)  and \(\)-divergences 1[12; 13] allow for computationally tractable regimes. In particular, these specific choices of D have shown intimate links between regularization  which is a conceptually central topic of machine learning.

More recently however, DRO has been studied for the BO setting in Kirschner et al. , which as one would expect, leads to a complicated minimax problem, which causes a computational burden practically speaking. Kirschner et al.  makes the first step and casts the formal problem however develops an algorithm only in the case where D has been selected as the MMD. While, this work makes the first step and conceptualizes the problem of distributional shifts in context for BO, there are two main practical short-comings. Firstly, the algorithm is developed specifically to the MMD, which is easily computed, however cannot be replaced by another choice of D whose closed form is not readily accessible with samples such as the \(\)-divergence. Secondly, the algorithm is only tractable when the contexts are finite since at every iteration of BO, it requires solving an \(M\)-dimensional problem where \(M\) is the number of contexts.

The main question that remains is, _can we devise an algorithm that is computationally tractable for tackling the DRO-BO setting_? We answer this question to a large degree of generality by considering distributional shifts against \(\)-divergences - a large family of divergences consisting of the extant Kullback-Leibler (KL) divergence, Total Variation (TV) and \(^{2}\)-divergence, among others. In particular, we exploit existing advances made in the large literature of DRO to show that the BO objective in this setting for any choice of \(\)-divergence yields a computationally tractable algorithm, even for the case of continuous contexts. We also present a robust regret analysis that illustrates a sublinear regret. Finally, we show, along with computational tractability, that our method is empirically superior on standard datasets against several baselines including that of Kirschner et al. . In summary, our main contributions are

1. A theoretical result showing that the minimax distributionally robust BO objective with respect to \(\) divergences is equivalent to a single minimization problem.
2. An efficient algorithm, that works in the continuous context regime, for the specific cases of the \(^{2}\)-divergence and TV distance, which admits a conceptually interesting relationship to regularization of BO.
3. A regret analysis that specifically informs how we can choose the DRO \(\)-budget to attain sublinear regret.

## 2 Related Work

Due to the multifaceted nature of our contribution, we discuss two streams of related literature, one relating to studies of robustness in Bayesian Optimization (BO) and one relating to advances in Distributionally Robust Optimization (DRO).

In terms of BO, the work most closest to ours is Kirschner et al.  which casts the distributionally robust optimization problem over contexts. In particular, the work shows how the DRO objective for any choice of divergence D can be cast, which is exactly what we build off. The main drawback of this method however is the limited practical setting due to the expensive inner optimization, which heavily relies on the MMD, and therefore cannot generalize easily to other divergences that are not available in closed forms. Our work in comparison, holds for a much more general class of divergences, and admits a practical algorithm that involves a finite dimensional optimization problem. In particular, we derive the result when D is chosen to be the \(^{2}\)-divergence which we show performs the best empirically. This choice of divergence has been studied in the related problem of Bayesian quadrature , and similarly illustrated strong performance, complimenting our results. There also exists work of BO that aim to be robust by modelling adversaries through noise, point estimates or non-cooperative games [37; 32; 3; 39; 47]. The main difference between our work and theirs is that the notion of robustness we tackle is at the _distributional_ level. Another similar work to ours is that of Tay et al.  which considers approximating DRO-BO using Taylor expansions based on the sensitivity of the function. In some cases, the results coincide with ours however their result must account for an approximation error in general. Furthermore, an open problem as stated in their work is to solve the DRO-BO problem for continuous context domains, which is precisely one of the advantages of our work.

From the perspective of DRO, our work essentially is an extension of Duchi et al. [12; 13] which develops results that connect \(\)-divergence DRO to variance regularization. In particular, they assume \(\) admits a continuous second derivative, which allows them to connect the \(\)-divergence to the \(^{2}\)-divergence and consequently forms a general connection to constrained variance. While the work is pioneering, this assumption leaves out important \(\)-divergences such as the Total Variation (TV) - a choice of divergence which we illustrate performs well in comparison to standard baselines in BO. At the technical level, our derivations are similar to Ahmadi-Javid  however our result, to the best of our knowledge, is the first such work that develops it in the context of BO. In particular, our results for the Total Variation and \(^{2}\)-divergence show that variance is a key penalty in ensuring robustness which is a well-known phenomena existing in the realm of machine learning [12; 11; 10; 22; 1].

## 3 Preliminaries

Bayesian OptimizationWe consider optimizing a _black-box_ function, \(f:\) with respect to the _input_ space \(^{d}\). As a black-box function, we do not have access to \(f\) directly however receive input in a sequential manner: at time step \(t\), the learner chooses some input \(_{t}\) and observes the _reward_\(y_{t}=f(_{t})+_{t}\) where the noise \(_{t}(0,_{f}^{2})\) and \(_{f}^{2}\) is the output noise variance. Therefore, the goal is to optimize

\[_{}f().\]

Additional to the input space \(\), we introduce the _context_ spaces \(\), which we assume to be compact. These spaces are assumed to be separable completely metrizable topological spaces.2 We have a reward function, \(f:\) which we are interested in optimizing with respect to \(\). Similar to sequential optimization, at time step \(t\) the learner chooses some input \(_{t}\) and receives a context \(c_{t}\) and \(f(_{t},c_{t})+_{t}\). Here, the learner can not choose a context \(c_{t}\), but receive it from the environment. Given the context information, the objective function is written as

\[_{}_{c p}[f(,c)],\]

where \(p\) is a probability distribution over contexts.

Gaussian ProcessesWe follow a popular choice in BO  to use GP as a surrogate model for optimizing \(f\). A GP  defines a probability distribution over functions \(f\) under the assumption that any subset of points \(\{_{i},f(_{i})\}\) is normally distributed. Formally, this is denoted as:

\[f()(m(),k(,^{ })),\]

where \(m()\) and \(k(,^{})\) are the mean and covariance functions, given by \(m()=[f()]\) and \(k(,^{})=[(f()-m() )(f(^{})-m(^{}))^{T}]\). For predicting \(f_{*}=f(_{*})\) at a new data point \(_{*}\), the conditional probability follows a univariate Gaussian distribution as \(pf_{*}_{*},[_{1}..._{N}],[y_{1},....y_{N}]((_{*}),^{2} (_{*}))\). Its mean and variance are given by:

\[(_{*})= _{*,N}_{N,N}^{-1},\] (1) \[^{2}(_{*})= k_{**}-_{*,N}_{N,N}^{-1}_{*,N}^{T}\] (2)

where \(k_{**}=k(_{*},_{*})\), \(_{*,N}=[k(_{*},_{i})]_{ i N}\) and \(_{N,N}=[k(_{i},_{j})]_{  i,j N}\). As GPs give full uncertainty information with any prediction, they provide a flexible nonparametric prior for Bayesian optimization. We refer to Rasmussen and Williams  for further details on GPs.

Distributional RobustnessLet \(()\) denote the set of probability distributions over \(\). A _divergence_ between distributions \(:()()\) is a dissimilarity measure that satisfies\((p,q) 0\) with equality if and only if \(p=q\) for \(p,q()\). For a function, \(h:\), base probability measure \(p()\), the central concern of Distributionally Robust Optimization (DRO) [4; 42; 5] is to compute

\[_{q B_{,0}(p)}_{q(c)}[h(c)],\] (3)

where \(B_{,}(p)=\{q():(p,q)\}\), is ball of distributions \(q\) that are \(\) away from \(p\) with respect to the divergence \(\). The objective in Eq. (3) is intractable, especially in setting where \(\) is continuous as it amounts to a constrained infinite dimensional optimization problem. It is also clear that the choice of \(\) is crucial for both computational and conceptual purposes. The vast majority of choices typically include the Wasserstein due to the transportation-theoretic interpretation and with a large portion of existing literature finding connections to Lipschitz regularization [6; 7; 9; 48]. Other choices where they have been studied in the supervised learning setting include the Maximum Mean Discrepancy (MMD)  and \(\)-divergences [12; 13].

Distributionally Robust Bayesian OptimizationRecently, the notion of DRO has been applied to BO [26; 54], who consider robustness with respect to shifts in the context space and therefore are interested in solving

\[_{}_{q B_{,}(p)} _{c q}[f(,c)],\]

where \(p\) is the reference distribution. This objective becomes significantly more difficult to deal with since not only does it involve a constrained and possibly infinite dimensional optimization problem however also involves a minimax which can cause instability issues if solved iteratively.

Kirschner et al.  tackle these problems by letting \(\) be the kernel Maximum Mean Discrepancy (MMD), which is a popular choice of discrepancy motivated by kernel mean embeddings . In particular, the MMD can be efficiently estimated in \(O(n^{2})\) where \(n\) is the number of samples. Naturally, this has two main drawbacks: The first is that it is still computationally expensive since one is required to solve two optimization problems, which can lead to instability and secondly, the resulting algorithm is limited to the scheme where the number of contexts is finite. In our work, we consider \(\) to be a \(\)-divergence, which includes the Total Variance, \(^{2}\) and Kullback-Leibler (KL) divergence and furthermore show that minmax objective can be reduced to a single maximum optimization problem which resolves both the instability and finiteness assumption. In particular, we also present a similar analysis, showing that the robust regret decays sublinearly for the right choices of radii.

## 4 \(\)-Robust Bayesian Optimization

In this section, we present the main result on distributionally robustness when applied to BO using \(\)-divergence. Therefore, we begin by defining this key quantity.

**Definition 1** (\(\)-divergence): _Let \(:(-,]\) be a convex, lower semi-continuous function such that \((1)=0\). The \(\)-divergence between \(p,q()\) is defined as_

\[_{}(p,q)=_{q(c)}[(( c))],\]

_where \(dp/dq\) is the Radon-Nikodym derivative if \(p q\) and \(_{}(p,q)=+\) otherwise._

Popular choices of the convex function \(\) include \((u)=(u-1)^{2}\) which yields the \(^{2}\) and, \((u)=|u-1|\), \((u)=u u\) which correspond to the \(^{2}\) and KL divergences respectively. At any time step \(t 1\), we consider distributional shifts with respect to an \(\)-divergence for any choice of \(\) and therefore relevantly define the DRO ball as

\[B_{}^{t}(p_{t}):=\{q():_{}(q, p_{t})_{t}\},\]

where \(p_{t}=_{s=1}^{t}_{c_{s}}\) is the reference distribution and \(_{t}\) is the distributionally robust radius chosen at time \(t\). We remark that for our results, the choice of \(p_{t}\) is flexible and can be chosen based on the specific domain application. The \(\) divergence, as noted from the definition above, is only defined finitely when the measures \(p,q\) are absolutely continuous to each other and there is regarded as a _strong_ divergence in comparison to the Maximum Mean Discrepancy (MMD), which is utilized in Kirschner et al. . The main consequence of this property is that the geometry of the ball \(B_{}^{t}\) would differ based on the choice of \(\)-divergence. The \(\)-divergence is a very popular choice for defining this ball in previous studies of DRO in the context of supervised learning due to the connections and links it has found to variance regularization .

We will exploit various properties of the \(\)-divergence to derive a result that reaps the benefits of this choice such as a reduced optimization problem - a development that does not currently exist for the MMD . We first define the convex conjugate of \(\) as \(^{}(u)=_{u^{}_{}}(u u^{ }-(u^{}))\), which we note is a standard function that is readily available in closed form for many choices of \(\).

**Theorem 1**: _Let \(:(-,]\) be a convex lower semicontinuous mapping such that \((1)=0\). Let \(f\) be measurable and bounded. For any \(>0\), it holds that_

\[_{}_{q B_{}^{t}(p)}_{c  q}[f(,c)]=_{, 0,b }(b-_{t}-_{p_{t}(c)}[ ^{}(,c)}{})]).\]

**Proof (Sketch)** The proof begins by rewriting the constraint over the \(\)-divergence constrained ball with the use of Lagrangian multipliers. Using existing identities for \(f\)-divergences, a minimax swap yields a two-dimensional optimization problem, over \( 0\) and \(b\).

We remark that similar results exist for other areas such as supervised learning , robust optimization  and certifying robust radii . However this is, to the best of our knowledge, the first development when applied to optimizing expensive black-box functions, the case of BO. The above Theorem is practically compelling for three main reasons. First, one can note that compared to the left-hand side, the result converts this into a single optimization (max) over three variables, where two of the variables are \(1\)-dimensional, reducing the computational burden significantly. Secondly, the notoriously difficult max-min problem becomes only a max, leaving behind instabilities one would encounter with the former objective. Finally, the result makes very mild assumptions on the context parameter space \(\), allowing infinite spaces to be chosen, which is one of the challenges for existing BO advancements. We show that for specific choices of \(\), the optimization over \(b\) and even \(\) can be expressed in closed form and thus simplified. All proofs for the following examples can be found in the Appendix Section 8.

**Example 2** (\(^{2}\)-divergence): _Let \((u)=(u-1)^{2}\), then for any measurable and bounded \(f\) we have for any choice of \(_{t}\)_

\[_{}_{q B_{}^{t}(p_{t})}_{c  q}[f(,c)]=_{}(_{p_{ t}(c)}[f(,c)]-_{p_{t}(c)}[f( ,c)]}).\]

The above example can be easily implemented as it involves the same optimization problem however now appended with a variance term. Furthermore, this objective admits a compelling conceptual insight which is that, by enforcing a penalty in the form of variance, one attains robustness. The idea that regularization provides guidance to robustness or generalization is well-founded in machine learning more generally for example in supervised learning . We remark that this penalty and its relationship to \(^{2}\)-divergence has been developed in the similar yet related problem of Bayesian quadrature . Moreover, it can be shown that if \(\) is twice differentiable then \(_{}\) can be approximated by the \(^{2}\)-divergence via Taylor series, which makes \(^{2}\)-divergence a centrally appealing choice for studying robustness. We now derive the result for a popular choice of \(\) that is not differentiable.

**Example 3** (Total Variation): _Let \((u)=|u-1|\), then for any measurable and bounded \(f\) we have for any choice of \(_{t}\)_

\[_{}_{q B_{}^{t}(p_{t})}_ {c q}[f(,c)]=_{}(_{p _{t}(c)}[f(,c)]-}{2}(_{c}f(,c)-_{c}f(,c))).\]

Similar to the \(^{2}\)-case, the result here admits a variance-like term in the form of the difference between the maximal and minimal elements. We remark that such a result is conceptually interesting since both losses admit an objective that resembles a mean-variance which is a natural concept in ML, but advocates for it from the perspective of distributional robustness. This result exists for the supervised learning in Duchi and Namkoong  however is completely novel for BO and also holds for a choice of non-differentiable \(\), hinting at the deeper connection between \(\)-divergence DRO and variance regularization.

### Optimization with the GP Surrogate

To handle the distributional robustness, we have rewritten the objective function using \(\) divergences in Theorem 1. In DRBO setting, we sequentially select a next point \(_{t}\) for querying a black-box function. Given the observed context \(c_{t} q\) coming from the environment, we evaluate the black-box function and observe the output as \(y_{t}=f(_{t},c_{t})+_{t}\) where the noise \(_{t}(0,_{f}^{2})\) and \(_{f}^{2}\) is the noise variance.

As a common practice in BO, at the iteration \(t\), we model the GP surrogate model using the observed data \(\{_{i},y_{i}\}_{i=1}^{t-1}\) and make a decision by maximizing the acquisition function which is build on top of the GP surrogate:

\[_{t}=_{}().\]

While our method is not restricted to the form of the acquisition function, for convenience in the theoretical analysis, we follow the GP-UCB . Given the GP predictive mean and variance from Eqs. (7,8), we have the acquisition function for the \(^{2}\) in Example 2 as follows:

\[^{^{2}}():= _{c}_{t}(,c)+ }_{t}(,c)-}{|C|}_{c} _{t}(,c)-_{t}^{2}}\] (4)

where \(_{t}\) is a explore-exploit hyperparameter defined in Srinivas et al. , \(}=_{c}_{t}(,c)\) and \(c q\) can be generated in a one dimensional space to approximate the expectation and the variance. In the experiment, we select \(q\) as the uniform distribution, but it is not restricted to. Similarly, an acquisition function for Total Variation in Example 3 is written as

\[^{TV}():= _{c}_{t}(,c)+ }_{t}(,c)-}{2}_{t}( ,c)-_{t}(,c).\] (5)

We summarize all computational steps in Algorithm 1.

Computational Efficiency against MMD.We make an important remark that since we do not require our context space to be finite, our implementation scales only linearly with the number of context samples \(|C|\) drawing from \(q\). This allows us to discretize our space and draw as many context samples as required while only paying a linear price. On the other hand, the MMD  at every iteration of \(t\) requires solving an \(|C|\)-dimensional constraint optimization problem that has no closed form solution. We refer to Section 5.2 for the empirical comparison.

```
1:Input: Max iteration \(T\), initial data \(D_{0}\), \(\)
2:for\(t=1,,T\)do
3: Fit and estimate GP hyperparameter given \(D_{t-1}\)
4: Select a next input \(_{t}=()\)
5:\(^{2}\)-divergence: \(():=^{^{2}}()\) from Eq. (4)
6: Total Variation: \(():=^{TV}()\) from Eq. (5)
7: Observe a context \(c_{t} q\)
8: Evaluate the black-box \(y_{t}=f(_{t},c_{t})+_{t}\)
9: Augment \(D_{t}=D_{t-1}(_{t},c_{t},y_{t})\)
10:endfor ```

**Algorithm 1** DRBO with \(\)-divergence

### Convergence Analysis

One of the main advantages of Kirschner et al.  is the choice of MMD makes the regret analysis simpler due to the nice structure and properties of MMD. In particular, the MMD is well-celebrated for a \(O(t^{-1/2})\) convergence where no such results exist for \(\)-divergences. However, using Theorem 1, we can show a regret bound for the Total Variation with a simple boundedness assumption and show how one can extend this result to other \(\)-divergences. We begin by defining the _robust regret_, \(R_{T}\), with \(\)-divergence balls:

\[R_{T}()=_{t=1}^{T}_{q B_{}^{t}}_{q(c)}[f( _{t}^{*},c)]-_{q B_{}^{t}}_{q(c)}[f(_{t},c)],\] (6)where \(_{t}^{}=*{arg\,max}_{}_{q  B_{,}^{t}}_{q(c)}[f(,c)]\). We use \(_{t}\) to denote the generated kernel matrix from dataset \(D_{t}=\{(_{i},c_{i})\}_{i=1}^{t}\). we now introduce a standard quantity in regret analysis in BO is the _maximum information gain_: \(_{t}=_{D:|D|=t} _{t}+_{f}^{-2}_{t}\) where \(_{t}=[k([_{i},c_{i}],[_{j},c_{j}]) ]_{ i,j t}\) is the covariance matrix and \(_{f}^{2}\) is the output noise variance.

**Theorem 4** (\(\)-divergence Regret): _Suppose the target function is bounded, meaning that \(M=_{(,c)}|f(,c)|<\) and suppose \(f\) has bounded RKHS norm with respect to \(k\). For any lower semicontinuous convex \(:(-,]\) with \((1)=0\), if there exists a monotonic invertible function \(_{}:[0,)\) such that \((p,q)_{}(_{}(p,q))\), the following holds_

\[R_{T}()_{T}}}{(1+_{f}^{-2})} +(2M+})_{t=1}^{T}_{}(_{ t}),\]

_with probability \(1-\), where \(_{t}=2||f||_{k}^{2}+300_{t}^{3}(t/)\), \(_{t}\) is the maximum information gain as defined above, and \(_{f}\) is the standard deviation of the output noise._

The full proof can be found in the Appendix Section 8. We first remark that with regularity assumptions on \(f\), sublinear analytical bounds for \(_{T}\) are known for a range of kernels, e.g., given \(^{d+1}\) we have for the RBF kernel, \(_{T}((T)^{d+2})\) or for the Matern kernel with \(>1\), \(_{T}(T^{ }( T))\). The second term in the bound is directly a consequence of DRO and by selecting \(_{t}=0\), it will vanish since any such \(_{}\) will satisfy \(_{}(0)=0\). To ensure sublinear regret, we can select \(_{t}=_{}^{-1}(+})\), noting that the second term will reduce to \(_{t=1}^{T}_{t}\). Finally, we remark that the existence of \(_{}\) is not so stringent since for a wide choices of \(\), one can find inequalities between the Total Variation and \(D_{}\), to which we refer the reader to Sason and Verdu . For the examples discussed above, we can select \(_{}(t)=t\) for the TV. For the \(^{2}\) and \(\) cases, one can choose \(_{^{2}}(b)=2}\) and \(_{}(b)=1-(-b)\).

Figure 1: Two settings in DRO when the stochastic solution and robust solution are different (_top_) and identical (_bottom_). _Left_: original function \(f(,c)\). _Middle_: selection of input \(_{t}\) over iterations. _Right_: performance with different \(\).

## 5 Experiments

**Experimental setting.** The experiments are repeated using \(30\) independent runs. We set \(|C|=30\) which should be sufficient to draw \(c}}{{}}q\) in one-dimensional space to compute Eqs. (4,5). We optimize the GP hyperparameter (e.g., learning rate) by maximizing the GP log marginal likelihood . We will release the Python implementation code in the final version.

**Baselines.** We consider the following baselines for comparisons. _Rand_: we randomly select \(_{t}\) irrespective of \(c_{t}\). _BO_: we follow the GP-UCB  to perform standard Bayesian optimization (ignoring the context \(c_{t}\)). The selection at each iteration is \(_{t}=*{argmax}_{}()+_{t} ()\). _Stable-Opt_: we consider the worst-case robust optimization presented in Bogunovic et al. . The selection at each iteration \(_{t}=*{argmax}_{}*{argmin}_{c }(,c)+_{t}(,c)\). _DRBO MMD_: Since there is no official implementation available, we have tried our best to re-implement the algorithm.

We consider the popular benchmark functions3 with different dimensions \(d\). To create a context variable \(c\), we pick the last dimension of these functions to be the context input while the remaining \(d-1\) dimension becomes the input \(\).

### Ablation Studies

To gain understanding into how our framework works, we consider two popular settings below.

**DRBO solution is different from stochastic solution.** In Fig. 0(a), the vanilla BO tends to converge greedily toward the stochastic solution (non-distributionally robust) \(*{argmax}_{}f(,)\). Thus, BO keeps exploiting in the locality of \(*{argmax}_{}f(,)\) from iteration \(15\). On the other hand, all other DRBO methods will keep exploring to seek for the distributionally robust solutions. Using the high value of \(_{t}\{0.5,1\}\) will result in the best performance.

**DRBO solution is identical to stochastic solution.** When the stochastic and robust solutions coincide at the same input \(^{*}\), the solution of BO will be equivalent to the solution of DRBO methods. This is demonstrated by Fig. 0(b). Both stochastic and robust approaches will quickly identify the optimal solution (see the \(_{t}\) selection). We learn empirically that setting \(_{t} 0\) will lead to the best performance. This is because the DRBO setting will become the standard BO.

The best choice of \(\) depends on the property of the underlying function, e.g., the gap between the stochastic and DRBO solutions. In practice, we may not be able to identify these scenarios in advance. Therefore, we can use the adaptive value of \(_{t}\) presented in Section 4.2. Using this adaptive setting, the performance is stable, as illustrated in the figures.

### Computational efficiency

The key benefit of our framework is simplifying the existing intractable computation by providing the closed-form solution. Additional to improving the quality, we demonstrate this advantage in terms of computational complexity. Our main baseline for comparison is the MMD . As shown in Fig. 2, our DRBO is consistently faster than the constraints linear programming approximation used for

Figure 3: Cumulative robust regret across algorithms. The results show that the proposed \(^{2}\) and TV achieve the best performance across benchmark functions. Random and vanilla BO approaches perform poorly which do not take into account the robustness criteria. Best viewed in color.

MMD. This gap is substantial in higher dimensions. In particular, as compared to Kirschner et al. , our DRBO is \(5\)-times faster in _5d_ and \(10\)-times faster in _6d_.

### Optimization performance comparison

We compare the algorithms in Fig. 3 using the robust (cumulative) regret defined in Eq. (6) which is commonly used in DRO literature [33; 26]. The random approach does not make any intelligent information in making decision, thus performs the worst. While BO performs better than random, it is still inferior comparing to other distributionally robust optimization approaches. The reason is that BO does not take into account the context information in making the decision. The StableOpt  performs relatively well that considers the worst scenarios in the subset of predefined context. This predefined subset can not cover all possible cases as opposed to the distributional robustness setting.

The MMD approach  needs to solve the inner adversary problem using linear programming with convex constraints, additional to the main optimization step. As a result, the performance of MMD is not as strong as our TV and \(^{2}\). Our proposed approach does not suffer this pathology and thus scale well in continuous and high dimensional settings of context input \(c\).

Real-world functions.We consider the deterministic version of the robot pushing objective from Wang and Jegelka . The goal is to find a good pre-image for pushing an object to a target location. The 3-dimensional function takes as input the robot location \((r_{x},r_{y})[-5,5]^{2}\) and pushing duration \(r_{t}\). We follow Bogunovic et al.  to twist this problem in which there is uncertainty regarding the precise target location, so one seeks a set of input parameters that is robust against a number of different potential pushing duration which is a context.

We perform an experiment on Wind Power dataset  and vary the context dimensions \(|C|\{30,100,500\}\) in Fig. 4. When \(|C|\) enlarges, our DRBO \(^{2}\), TV and KL improves. However, the performances do not improve further when increasing \(|C|\) from \(100\) to \(500\). Similarly, MMD improves with \(|C|\), but it comes with the quadratic cost w.r.t. \(|C|\). Overall, our proposed DRBO still performs favourably in terms of optimization quality and computational cost than the MMD.

## 6 Conclusions, Limitations and Future works

In this work, we showed how one can study the DRBO formulation with respect to \(\)-divergences and derived a new algorithm that removes much of the computational burden, along with a sublinear regret bound. We compared the performance of our method against others, and showed that our results unveil a deeper connection between regularization and robustness, which serves useful conceptually.

Limitations and Future WorksOne of the limitations of our framework is in the choice of \(\), for which we provide no guidance. For different applications, different choices of \(\) would prove to be more useful, the study of which we leave for future work.

Figure 4: All divergences improve with larger \(|C|\). However, MMD comes with the quadratic cost.

Figure 2: We compare the computational cost across methods. Our proposed DRBO using \(^{2}\) and TV take similar cost per iteration which is significantly lower than the DRBO MMD .