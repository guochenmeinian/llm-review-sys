# LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching

LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching

 Duy M. H. Nguyen\({}^{*}\)

Corresponding authors, \({}^{1}\)Co-Senior authors. University of Stuttgart, \({}^{2}\)IMPRS for Intelligent Systems

Hoang Nguyen\({}^{3}\)

Nghiem T. Diep\({}^{3}\)

Tan N. Pham\({}^{3,4}\)

Tri Cao\({}^{3}\)

Binh T. Nguyen\({}^{4}\)

Paul Svoboda\({}^{5}\)

Nhat Ho\({}^{6}\)

Shadi Albarqouni\({}^{7,8}\)

Pengtao Xie\({}^{9,10}\)

Daniel Sonntag\({}^{1}\)

Mathias Niepert\({}^{*}\)

Corresponding authors, \({}^{1}\)Co-Senior authors. University of Stuttgart, \({}^{2}\)IMPRS for Intelligent Systems

\({}^{1}\)University of Stuttgart, \({}^{2}\)IMPRS for Intelligent Systems

\({}^{3}\)German Research Center for Artificial Intelligence, \({}^{4}\)University of Science - VNUHCM,

\({}^{5}\)Max Planck Institute for Informatics, \({}^{6}\)University of Texas at Austin \({}^{7}\)Helmholtz Munich,

\({}^{8}\)University Hospital Bonn, \({}^{9}\)UC San Diego, \({}^{10}\)MBZUAI, \({}^{11}\)Oldenburg University.

###### Abstract

Obtaining large pre-trained models that can be fine-tuned to new tasks with limited annotated samples has remained an open challenge for medical imaging data. While pre-trained deep networks on ImageNet and vision-language foundation models trained on web-scale data are prevailing approaches, their effectiveness on medical tasks is limited due to the significant domain shift between natural and medical images. To bridge this gap, we introduce LVM-Med, the first family of deep networks trained on large-scale medical datasets. We have collected approximately \(1.3\) million in medical images from 55 publicly available datasets, covering a large number of organs and modalities such as CT, MRI, X-ray, and Ultrasound. We benchmark several state-of-the-art self-supervised algorithms on this dataset and propose a _novel self-supervised contrastive learning algorithm using a graph matching formulation_. The proposed approach makes three contributions: (i) it integrates prior pair-wise image similarity metrics based on local and global information; (ii) it captures the structural constraints of feature embeddings through a loss function constructed via a combinatorial graph-matching objective; and (iii) it can be trained efficiently end-to-end using modern gradient-estimation techniques for black-box solvers. We thoroughly evaluate the proposed LVM-Med on \(15\) downstream medical tasks ranging from segmentation and classification to object detection, and both for the in and out-of-distribution settings. LVM-Med empirically outperforms a number of state-of-the-art supervised, self-supervised, and foundation models. For challenging tasks such as Brain Tumor Classification or Diabetic Retinopathy Grading, LVM-Med improves previous vision-language models trained on 1 billion masks by 6-7\(\%\) while using only a ResNet-50. We release pre-trained models at this link https://github.com/duyhominhngguyen/LVM-Med.

## 1 Introduction

Constructing large-scale annotated medical image datasets for training deep networks is challenging due to data acquisition complexities, high annotation costs, and privacy concerns . Vision-language pretraining has emerged as a promising approach for developing foundational models that support various AI tasks. Methods such as CLIP , Align , and Flava  propose a unified model trained on large-scale image-text data, showing exceptional capabilities and performance across various tasks. However, their effectiveness in the medical domain still remains unclear. A recent work SAM  trains large vision models on over one billion annotated masks from 11M natural images, enabling interactive segmentation. Nevertheless, SAM's zero-shot learning performance is moderate on other datasets [7; 8], highlighting the need for fine-tuning to achieve satisfactory results .

To facilitate the development of foundation models in the medical domain, we make two major contributions. First, we have curated a vast collection of 55 publicly available datasets, resulting in approximately 1.3 million medical images covering various body organs and modalities such as CT, MRI, X-ray, ultrasound, and dermoscopy, to name a few. Second, we propose LVM-Med, a novel class of contrastive learning methods, utilizes pre-trained ResNet-50 and a ViT network SAM. We evaluate various instances of LVM-Med relative to popular supervised architectures and vision-language models across \(15\) medical tasks. To our best knowledge, this is the first time such a large-scale medical dataset has been constructed and used to investigate the capabilities of SSL algorithms.

LVM-Med incorporates a second-order graph-matching formulation, which subsumes and extends a large class of contrastive SSL methods. Given a batch of images, two random transformations are applied to each image, and the resulting transformed images are then fed to an image encoder. The embedding vectors obtained from images in a batch are used to construct two graphs where vertices represent pairs of transformed images generated from the same original one. Through solving a graph-matching problem [11; 12], we learn feature representation such that their encoding serves as suitable priors for a global solution of the graph-matching objective. This approach is distinct from prior contrastive learning methods that focus on merely optimizing pair-wise distances [13; 14] between transformed images or learning contrastive distances with positive and negative samples [15; 16; 17; 18; 19]. It is worthwhile noting that previous contrastive learning methods are special instances of our general framework (Figure (1), right).

LVM-Med has several advantages over existing approaches. First, it integrates advanced pair-wise image similarity taken from prior SSL methods into vertex affinities, resulting in both global and local information that can be efficiently fused. Second, it uncovers underlying structures of feature embeddings by utilizing edge constraints, enhancing robustness in the presence of similar entities in medical datasets. Third, though combinatorial problems are typically non-differentiable, LVM-Med can efficiently calculate gradients through the discrete combinatorial loss function using modern implicit maximum likelihood estimation techniques. Consequently, LVM-Med can scale successfully on large-scale data. In a wide range of \(15\) medical experiments, LVM-Med sets a new state-of-the-art in fully fine-tuning or prompt-based segmentation, linear and fully fine-tuning image classification, and domain generalization, outperforming several vision-language models trained on a hundred million image-text instances.

We summarize major contributions in this work, including:

1. We present a collection of large-scale medical datasets, serving as a resource for exploring and evaluating self-supervised algorithms.

Figure 1: (left) Overview of the body organs and modalities in our collected dataset; (right) LVM-Med unifies and extends contrastive and instance-based self-supervised learning approaches by specifying graphâ€™s properties.

2. We propose LVM-Med, a novel SSL approach based on second-order graph matching. The proposed method is flexible in terms of integrating advanced pair-wise image distance and being able to capture structural feature embedding through the effective utilization of second-order constraints within a global optimization framework.
3. On both ResNet-50 and ViT architectures, LVM-Med consistently outperforms multiple existing self-supervised learning techniques and foundation models across a wide range of downstream tasks.

## 2 Related Work

### Self-supervised learning in medical image analysis

The latest approaches of _global feature_ SSL rely on shared embedding architecture representations that remain invariant to different viewpoints. The variation lies in how these methods prevent collapsing solutions. _Clustering methods_[20; 21; 22] constrain a balanced partition of the samples within a set of cluster assignments. _Contrastive methods_[15; 16; 17; 18] uses negative samples to push far away dissimilar samples from each other through contrastive loss, which can be constructed through memory bank , momentum encoder , or graph neural network . Unlike contrastive learning, _instant-based learning_ depends on maintaining the informational context of the feature representations by either explicit regularization [13; 25] or architectural design [26; 27]. Our work relates to contrastive and instance-based learning, where a simplified graph-matching version of 1-N or 1-1 reverts to these approaches.

In contrast to global methods, _local methods_ specifically concentrate on acquiring a collection of local features that depict small portions of an image. A contrastive loss function can be used on those feature patches at different criteria such as image region levels , or feature maps [29; 14]. These strategies are also widely applied in the medical context, thereby pre-text tasks based on 3D volume's properties, such as reconstructing the spatial context , random permutation prediction  and self-restoration [32; 33], are proposed. Our LVM-Med model on this aspect can flexible unifying both global and local information by adding them to the affinities matrixes representing the proximity of two graphs, enhancing expressive feature representations.

### Vision-language foundation models

In order to comprehend the multi-modal world using machines, it is necessary to create foundational models that can operate across diverse modalities and domains . CLIP  and ALIGN  are recognized as groundbreaking explorations in foundation model development. These models demonstrate exceptional proficiency in tasks such as cross-modal alignment and zero-shot classification by learning contrastive pretraining on extensive image-text pairs from the web, despite the presence of noise. To further support multi-modal generation tasks such as visual question answering or video captioning, recent works such as FLAVA  and OmniVL  are designed to learn cross-modal alignment as well as image-video language models. Conversely, the SAM model  utilized a supervised learning strategy with over \(1\) billion masks on \(11\) million user-prompt interactions and achieved impressive zero-shot segmentation performance on unseen images. While many efforts have been proposed for natural image domains, limited research has been conducted on large-scale vision models for medical imaging. This motivated us to develop the LVM-Med model.

### Graph matching in visual computing

Graph matching is a fundamental problem in computer vision, which aims to find correspondences between elements of two discrete sets, such as key points in images or vertices of 3D meshes, and used in numerous vision tasks, including 3D vision [36; 37], tracking [38; 39], shape model learning [40; 41], and many others [42; 43; 44; 45]. In this framework, the vertices of the matched graphs correspond to the elements of the discrete sets to be matched. Graph edges define the cost structure of the problem, namely, second order, where pairs of matched vertices are penalized in addition to the vertex-to-vertex matchings. This allows us to integrate the underlying geometrical relationship between vertices into account but also makes the optimization problem NP-hard. Therefore, many approximate approaches have been proposed to seek acceptable suboptimal solutions by relaxing discrete constraints [46; 47]. In other directions, gradient estimation techniques for black-box solvers are employed to make the hybrid discrete-continuous matching framework be differentially end-to-end [48; 49; 50]. Our LVM-Med follows the latter direction and, for the first time, presents the formulation of contrastive learning as a second-order graph-matching problem.

## 3 Methodology

### Dataset construction

We provide detailed information about the collected datasets in the Appendix. The data was collected from publicly available resources, which include a diverse set of modalities and body organs as illustrated in Figure 1 (left). The data format is a combination of 2D images and 3D volumes as well as X-ray, MRI, CT, Ultrasound, etc. For datasets whose data dimensions are 3D volumes, we slice them into 2D images. To avoid potential test data leaking for downstream tasks, we use the default training partition in each dataset; otherwise, we randomly sample with \(20\%\) total images. In total, we obtain approximately \(1.3\) million images. More statistics on the dataset are presented in the Appendix.

### Contrastive learning as graph matching

Figure 2 provides an illustration of our LVM-Med method, which learns the feature representation \(f_{}\) by matching two distorted views derived from the same input image through a graph-matching formulation. Below we describe in detail each component.

#### 3.2.1 Graph construction on feature embedding

Given a batch of \(N\) images \(=\{_{1},\,_{2},\,..,_{N}\}\) sampled from a dataset, we generate for each image \(_{i}\) two transformed images \(_{i}^{s}\) and \(_{i}^{t}\) by using two transformations \(s,t T\) sampled from \(T\), a set of pre-defined image transformations. After the transformations, each image is of shape \((C H W)\), where \(C\) is the number of channels and \((H,W)\) the original spatial dimensions. These distorted images are fed into an encoder \(f_{}:^{C H W}^{D R  S}\) to produce two representations \(_{i}^{s}=f_{}(_{i}^{s})\) and \(_{i}^{t}=f_{}(_{i}^{t})\) where \(D\) is the number of feature channels and \((R,S)\) are the spatial dimensions of the feature map. On each such representation, we perform an average pooling

Figure 2: LVM-Med Overview. Avg is the average pooling layer, MPN denotes for message passing network, OH indicates the combinatorial solver, and \((c^{v},c^{e})\) represents vertex and edge affinity matrices. For each image \(_{i}\) in batch size, we generated two distorted versions and fed them into the feature representation \(f_{}\) and another projector \(h_{}\). The obtained embeddings \(_{i}^{t},\ (s,t)\) are used to build two graphs \(G^{s},G^{t}\). We further design a message passing network \(g_{e}\) that aggregate feature per node by their neighbor information. Then we compute vertex and edge affinities \(^{v},^{e}\) and use them to solve the graph matching. The output afterward is compared with pairs of ground truth \((_{i}^{s},_{i}^{t}),i(1,..,N)\) representing distorted images generated from the same sample. In the backward pass, we use modern gradient-estimation techniques to approximate \(}\) and \(}\).

operation \(:^{D R S}^{D}\) followed by another projection \(h_{}:^{D}^{F}\) to form two feature embeddings \(_{i}^{s}=h_{}((_{i}^{s}))\), and \(_{i}^{t}=h_{}((_{i}^{t}))^{F}\) with \(F<D\).

Given a set of embeddings for a batch \(\), we construct two graphs \(G^{s}\) and \(G^{t}\) where, for each pair \((_{i}^{s},_{i}^{t})\) of corresponding distorted images, we add a node representing \(_{i}^{s}\) to \(G^{s}\) and a node representing \(_{i}^{t}\) to \(G^{t}\). Hence, for each \(\{s,t\}\), we construct a graph \(G^{}=(V^{},E^{})\) with \(V^{}=\{_{1}^{},...,_{N}^{}\}\) the set of vertices and \(E^{}\) the set of edges \(e_{ij}^{}=(_{i}^{},_{j}^{})\). The node-level feature matrix is given by \(^{}=_{1}^{},...;_{N}^{} ^{N F}\) which associates each vertex \(_{i}^{}\) with its feature embedding \(_{i}^{}\). We create edges for each graph \(G^{}\) through a \(k\)-nearest neighbors algorithm using the feature matrix \(^{}\). The adjacency matrix \(^{}^{N N}\) is defined as \(A_{ij}^{}=1\) if \(e_{ij}^{} E^{}\) and \(A_{ij}=0\) otherwise. With the two graph structures given, we obtain a node-attributed graph \(G^{}=(V^{},^{},^{})\) on which a graph neural network \(g_{}\) is used to aggregate the nodes' features. In particular, \(g_{}\) computes an embedding \(}^{}=g_{}(^{},^{})\) by performing message passing operations. We set \(g_{}\) to be a graph convolutional network  consisting of \(l+1\) layers \(g_{}=\{g_{l},g_{l-1},..,g_{0}\}\) where the output of layer \(l\) is computed as

\[H_{l}^{}=(^{-}(^{}+_{N}) ^{-}H_{l-1}^{}g_{l-1}),\] (1)

where \(_{N}\) is the identity matrix modeling self-connections; \(\) is a diagonal matrix with \(_{ii}=_{j}_{ij}^{};g^{l-1}\) are the trainable parameters for each layer; \(()\) is an activation function; and \(H_{0}^{}=^{}\).

We use the outputs of the last layer as embeddings for the nodes, that is, \(}^{}=H_{l}^{}^{N F}\) given the shared graph network \(g_{}\).

We now have two graphs \(G^{s},G^{t}\) with node attribute matrices \(}^{s},\ }^{t}\), the outputs of the graph neural networks. Next, a graph-matching problem is constructed and solved where the gold matching is given by the pairs \((_{i}^{s},_{i}^{t})\  i\{1,..,N\}\).

#### 3.2.2 Learning affinities with global and local context

To represent potential connections for a pair of node \((_{i}^{s},_{a}^{t})\) where \(_{i}^{s} G^{s},\ _{a}^{t} G^{t}\), we design a vertex affinity matrix \(^{v}^{|V^{s}||V^{t}|}\) where \(c_{ia}^{v}\) is the prior (feature-based) similarity between \(_{i}^{s}\) and \(_{a}^{t}\). An advantage of our formulation is its ability to integrate advanced pair-wise distance can be smoothly integrated to \(c_{ia}^{v}\), resulting in more expressive proximity representation. In particular, we leverage both global and local consistency derived from feature embeddings of distorted images. The _global distance_ used in several prior works can be computed as \(c_{ia}^{glo}(_{i}^{s},_{a}^{t})=(}_{i}^{s},}_{a}^{t})\) where \(()\) denotes cosine similarity; \(}_{m}^{}\) is the embedding of \(_{m}^{}\) (\(\{s,t\},\ m\{i,a\}\)) obtained after message passing in Eq. (1).

Compared to global methods that implicitly learn features for the entire image, local methods concentrate on explicitly learning a specific group of features that characterize small regions of the image. As a result, they are more effective for dense prediction tasks such as segmentation . While recent works applied these tactics as a part of pair-wise minimization conditions  Instead, we integrate them as a part of vertex costs \(c_{ia}^{v}\) and use it to solve the graph matching problem. Indeed, we adapt both location- and feature-based local affinity computed as:

\[c_{ia}^{lo}(_{i}^{s},_{a}^{t})=_{p}(_{p }^{s},_{(p)}^{t})+_{p}(_{p}^{s}, _{^{}(p)}^{t})\] (2)

where \(=\{(r,s)|\ (r,s)[1,...,R][1,..,S]\}\) be the set of coordinates in the feature map \(_{i}^{s}^{D R S}\) of \(_{i}^{s}\); \(_{p}^{}\) (\(\{s,t\}\)) be the feature vector at position \(p\); \((p)\) denote the spatial closest coordinate to \(p\) in coordinates of feature map \(_{a}^{t}\) estimated through transformations on original image \(_{i}\); finally \(\ ^{}(p)\) represents the closest feature vector to \(p\) in \(_{a}^{t}\) using \(l^{2}\) distance. Intuitively, the local cost in Eq. (2) enforces invariance on both spatial location and between embedding space at a local scale. Our final affinity cost is computed as:

\[c_{ia}^{v}(_{i}^{s},_{a}^{t})=(c_{ia}^{glo}(_{i}^{s}, _{a}^{t}))+(1-)(c_{ia}^{lo}(_{i}^{s},_{a}^{t} )+c_{ia}^{lo}(_{a}^{t},_{i}^{s}))\] (3)

#### 3.2.3 Self-supervision through second-order graph matching

While the standard graph matching problem for vertex-to-vertex correspondences can be used in our setting (LAP), it fails to capture the similarity between edges. If there are duplicated entitiesrepresented by distinct nodes in the same graph, the LAP will consider them identical and skip their neighboring relations. For instance, during the image sampling, two consecutive image slides were sampled from a 3D volume, resulting in their appearances have s a small difference. In such cases, it is complicated to correctly identify those augmented images generated from the same one without using information from the relations among connected nodes in the constructed graph. To address this problem, we introduce additional edge costs \(^{e}^{|E^{s}||E^{t}|}\) where \(c^{e}_{ia,jb}\) represents the similarity between an edge \(v^{s}_{ij}=(^{s}_{i},^{s}_{j}) E^{s}\) and \(v^{t}_{ab}=(^{t}_{a},^{t}_{b}) E^{t}\). These edge costs (second-order) are computed as \(c^{e}_{ia,jb}=((}^{s}_{i}-}^{s}_{j}),(}^{t }_{a}-}^{t}_{b}))\).

We now establish the second-order graph-matching problem. Denoting \(=\{0,1\}^{|V^{s}||V^{t}|}\) be indicator vector of matched vertices, i.e., \(v_{ia}=1\) if the vertex \(^{s}_{i} V^{s}\) is matched with \(^{t}_{a} V^{t}\) and \(v_{ia}=0\) otherwise. The node correspondence between two graphs \(G^{s}\) and \(G^{t}\) that minimizes the global condition stated as:

\[&(^{v},^{e})=*{ arg\,min}_{ U(,)}-_{i,a}c^{v}_{ia}v_{ia}-_{i,j,a,b}c^{e} _{ia,jb}v_{ia}v_{jb}\\ & U(,)=\{\{0,1\}^{N  N}|_{N}=,^{T}_{N}=\}\] (4)

and \(_{N}\) be a \(n\)-dimension one-value vector. The constraint \(U(,)\) restricts \(\) satisfying the one-to-one matching. Essentially, the Eq. (4) solves the vertex-to-vertex correspondence problem using both node and edges affinities, which can be seen as a form of structural matching (Figure (1),right) and generally can be integrated with higher-order graph constraints as triangle connections or circles. In the experiment, we found out that Eq. (4) significantly improved downstream task performance compared to the pure linear matching approach (Table (6)). Since the Eq. 4 in general is an NP-Hard problem  due to its combinatorial nature, we thus use efficient heuristic solvers based on Lagrange decomposition techniques .

#### 3.2.4 Backpropagating through a graph matching formulation

With \(}=(^{v},^{e})\) a solution obtained from the solver, we use the Hamming distance and an optimal solution \(^{*}\) to define the following loss function

\[L(},^{*})=}.(1-^{*})+^{*}.(1-}).\] (5)

The proposed approach aims to learn the feature representation function \(f_{}\) such that its output minimizes Eq. (5). However, this is a difficult problem because the partial derivatives of the loss function w.r.t vector costs \(^{v},^{e}\), i.e., \( L/^{v}\) and \( L/^{e}\), are zero almost everywhere  due to the objective function in Eq. (4) being piece-wise constant, preventing direct gradient-based optimization.

To approximate the gradients required for backpropagation, we adopt IMLE . Let \(=(^{v},^{e})\) be the input to the combinatorial graph matching problem in Eq. (4). The core idea of IMLE is to define a probability distribution \((;)\) over solutions of the combinatorial optimization problem, where the probability of a solution is proportional to its negative cost, and to estimate \( L/\) through the gradients of the expectation \(_{}_{}(;)} [L(},^{*})]\). Since exact sampling from \((;)\) is typically intractable, IMLE instead chooses a noise distribution \(()\) and approximates the gradient of the expectation over \((;)\) with the gradient of the expectation over \(()\)

\[_{}_{}(;)} [L(},^{*})]_{}_ {()}[L((+ ),^{*})].\]

The above approximation invokes the reparameterization trick for a complex discrete distribution. A typical choice for \(()\) is the Gumbel distribution, that is, \(()(0,1)\). Now, by using a finite-difference approximation of the derivative in the direction of the gradient of the loss \(_{}}L(},^{*})\), we obtain the following estimation rule:

\[_{}_{}(;)} [L(},^{*})]_{ ()}}-(+-_{}}L(}, ^{*}))},\] (6)where \(}=(+)\), \(\) is a step size of finite difference approximation. Using a Monte Carlo approximation of the above expectation, the gradient for \(\) is computed as a difference of two or more pairs of perturbed graph-matching outputs. We summarize in Algorithm 1 the forward and backward steps for \(^{v},^{}\).

``` functionForwardPass(\(^{v},^{c}\)) //Gumbel noise distribution sampling \(,^{}(0,1)\) //Grab-matching with perturbed \((c^{v},c^{c})\) \(}=(^{v}+,^{}+ ^{})\) //Single sample gradient estimate //Save values for the backward pass \((}{^{v}},}{ ^{}})=}-(^{v}_{ },^{}_{})\) save \((^{v},^{c})\), \((,^{})\) and \(}\) return\(}\) ```

**Algorithm 1** Forward and Backward Pass for \(^{v}\), \(^{c}\)

## 4 Experiments

### Implementation details

Pre-trainingWe utilize Resnet50  and Vision Transformer (ViT-B/16)  to train our LVM-Med. For Resnet50, we load pre-trained from ImageNet-1K , and SAM Encoder backbone weight  for ViT. The raw image is augmented to two different views by using multi-crop techniques as  and followed by flip (probability 50 \(\%\)), color jitter, random Gaussian blur, and normalization. We trained the LVM-Med with 100 epochs on the collected dataset. The batch size of \(3200\) is used for ResNet50 and we reduced it to \(2800\) for ViT due to memory limitation. The model is optimized with Adam  with an initial learning rate \(2 10^{-3}\) and reduced halved four times. We use \(16\) A100-GPUs per with \(80\)GB and complete the training process for LVM-Med with ResNet-50 in five days and LVM-Med with ViT encoder in seven days. Other competitor SSL methods as VicRegl, Twin-Barlon, Dino, etc, are initialized from ResNet-50 pre-trained ImageNet-1K and trained with \(100\) epochs with default settings as LVM-Med.

To balance samples among different modalities, we combine over-sampling and data augmentation to increase the total samples. Specifically, new samples from minority classes are generated by duplicating images and applying random crop operations covering \(85-95\%\) of image regions and then rescaling them to the original resolutions. Note that these augmentations are not used in the self-supervised algorithm (operations \(s,t T\)) to avoid generating identical distorted versions in this sampling procedure.

Downstream TasksTable 1 lists the datasets and downstream tasks used in our experiments. We cover segmentation, object detection, and image classification problems. It is important to note that in most settings, we utilize simple configurations for all datasets, skipping extra pre-processing for data augmentation. For instance, overlapping image patches with stride operations in the original samples  to increase training data in the Drive dataset or combining different 3D MRI modalities to fuse information  in the BRATS-2018 are excluded in our downstream setups.

To validate LVM-Med algorithms, we compare with 2D-SSL methods trained in our dataset and foundation models like Clip , Align , Flava , and SAM  with pre-trained ViT (Bert for

 
**Evaluation** & **Downstream Task Data** & **Modifiedity** & **Nums** & **Task** \\  Fine-Tuning & BRTS2018  & 3D MRI & 285 & Tumor Segmentation \\ Fine-Tuning & MMWHS-CT  & 3D CT & 20 & Heart Structures Segmentation \\ Fine-Tuning & MMWHS-MRI  & 3D MRI & 30 & Heart Structure Segmentation \\ Fine-Tuning & NICE-2018  & 2D Densoscopy & 2596 & Sain Letic Segmentation \\ Fine-Tuning & JSRT  & 2D X-ray & 247 & Multi-Organ Segmentation \\ Fine-Tuning & KSVR  & 2D Endoscopy & 1000 & Detection \\ Fine-Tuning & DIVE  & Fanoss & 40 & Vesent Segmentation \\ Fine-Tuning & BUD  & 2D Ultrasound & 647 & Breast Cancer Segmentation \\ Linear Evaluation \& & FGARD  & Funds & 1841 & DR Grading \\ Fine-Tuning & Brain Tumor Classification & 2D MRI & 3264 & Brain Tumor Classification \\ Fine-Tuning & Multi-site Prostate & 3D MRI & 116 & Prostate Segmentation \\ Fine-Tuning & VidNet  & 2D X-ray & 18000 & Lung Diseases Detection \\  

Table 1: Summary of datasets and downstream tasks

Figure 3: FGADR performance with top architectures.

Align) taken from each method, respectively. During the downstream task, trained SSL weights are then extracted and attached in U-Net for ResNet50 backbone, TransUnet  for ViT, and then fine-tuned with training splits of each dataset. Depending on the downstream task's properties, we apply different image resolutions and other parameters like the number of epochs and learning rate for different data domains. Details for these configurations are presented in Appendix.

### 2D- and 3D-based segmentation

We evaluate LVM-Med on _eight_ medical segmentation tasks, including five 2D-based and three 3D-based segmentation. In 2D settings, we also compare with 2D supervised architectures, such as U-Net, U-Net++, Attention U-Net, etc. These networks are initialized with ResNet-50 pre-trained ImageNet. Additionally, we investigate the prompt-based segmentation settings inspired by the current success of SAM's zero-shot learning. We utilized the ground truths and added random noise to simulate box-based user prompts as . We next compare three variations of SAM: (i) freezing image and prompt encoders, only fine-tuning mask decoder; (ii) without any training and inference using box prompts; (iii) similar to (i) but replacing the original image encoder by LVM-Med's ViT architecture taken from SAM trained in our dataset.

In 3D settings, we segment 2D slices and merge results for a 3D volume. We also benchmarked with 3D self-supervised methods from . Tables (2) and (3) show that our two versions with ResNet-50 and Sam's ViT hold the best records in each category. For instance, we outperform 2D SSL methods trained on the same dataset, surpassing foundation models such as SAM, Flava, and Clip. In the prompt-based settings, LVM-Med also delivers better performance compared with SAM. Second, LVM-Med achieves the best overall results on _seven of eight segmentation tasks_, mostly held by

LVM-Med with ResNet-50. The improvement gaps vary on each dataset, for e.g., from \(3-5\%\) on Kvasir and BUID compared with 2D supervised methods.

### Linear and finetuning image classification

We analyze LVM-Med on image classification tasks using linear probing (frozen encoders) and fully fine-tuning settings, two popular evaluations used in self-supervised learning. The experiments are conducted on the FGADR Grading and Brain tumor classification tasks. Table (5) presents the average accuracy metric on three training times. LVM-Med (ResNet-50) consistently outperforms other approaches on two datasets. For example, it is better than Clip by \(10.46\%\) and \(8.46\%\) on FGADR and Brain Tumor datasets with linear evaluation. In the foundation model setting, LVM-Med (ViT) also improves SAM's results by \(7.32\%\) and \(4.69\%\) on FGADR with linear and fully-finetuning. Another point we observe is that the overall 2D-SSL methods based on ResNet-50 and trained on the collected medical dataset achieve higher accuracy than foundation models using ViT. We also compare LVM-Med with the top methods on the FGADR dataset, including AFN-Net , JCS , CoLL , and DRG-Net . We choose the DRG-Net as the backbone and replace the employed encoder with our weights (R50). Figure (3) shows that LVM-Med hold the first rank overall.

### Object detection & In-out-distribution evaluation

Figure 4 indicates our performance on the object detection task using VinDr and Kvasir datasets. We use Faster R-CNN and load ResNet-50 from 2D SSL pretrained weights. Results are presented by Average Precision with IoU=0.5 over three training times. Compared to pre-trained Imagenet, LVM-Med still outperforms by 1-2\(\%\) though overall, our improvements are smaller than image classification and segmentation tasks.

We also validate LVM-Med performance on the in-out-distribution setting in Table (4) using the segmentation task on the Multi-Prostate dataset. We train LVM-Med and other competitors in BMC data and use the trained models to predict the remaining datasets. Both two versions of LVM-Med with ResNet-50 and ViT, on average, surpass all baselines, which validates the potential abilities of LVM-Med for the in-out-distribution problem.

### Ablation study

We do the following settings to evaluate the performance of components used in LVM-Med: (i) LVM-Med without using second-order graph matching conditions, i.e., only solving vertex-to-vertex correspondence problem; (ii) LVM-Med without using message passing network \(g_{e}\) in Eq. (1) to aggregate information from local connections; (iii) LVM-Med w/o using approximate gradients from Gumbel noise in Eq. (6). For this, we add a constant value to \(^{v},^{e}\) as prior works [57; 49], and finally (iv) LMV-Med without using local similarity \(c_{ia}^{lo}\) in Eq. (2). Other ablation studies are presented in Appendix. Table (6) indicates that all factors contribute to the final performance, wherein the second-order and Gumbel noise are the two most two important parts.

   Method & Cls(Acc) & Seg. (Disc) \\  LVM-Med (Full) & **67.47** & **83.05** \\ LVM-Med w/o second-order & 62.17 & 80.21 \\ LVM-Med w/o message passing & 65.08 & 81.19 \\ LVM-Med w/o Gaussemble noise & 64.32 & 81.37 \\ LVM-Med w/o local similarity & 65.67 & 81.54 \\   

Table 6: LVM-Med ablation study. Results are reported on an average of five 2D segmentation and two linear classification tasks. The two most important factors are highlighted.

Figure 4: LVM-Med on object detection.

  
**Method** & **Linear Evaluation (Procer)** &  \\   & **FGADR** & **Brain Tumor Cls.** & **FGADR** & **Brain Tumor Cls.** \\   & 66.6\(\) 0.41 & 63.03 \(\) 0.32 & 66.37 \(\) 0.77 & 74.20 \(\) 1.35 \\  & 65.39 \(\) 1.91 & 62.27 \(\) 0.32 & 67.35 \(\) 1.36 & 71.19 \(\) 1.55 \\  & 65.30 \(\) 1.70 & 62.52 \(\) 1.67 & 67.55 \(\) 0.28 & 72.52 \(\) 3.56 \\  & 65.98 \(\) 1.01 & 62.32 \(\) 1.92 & 67.55 \(\) 1.79 & 74.51 \(\) 0.43 \\  & 65.35 \(\) 1.93 & 64.47 \(\) 0.55 & 67.49 \(\) 1.78 & 71.30 \(\) 0.55 \\  & 64.71 \(\) 0.60 & 59.64 \(\) 1.36 & 65.69 \(\) 1.36 & 71.88 \(\) 2.03 \\  & **1.73M-Med (R50)** & **68.33 \(\) 0.48** & **66.33 \(\) 0.31** & **68.32 \(\) 0.48** & **76.26 \(\) 2.23** \\  Clip  & 57.87 \(\) 0.50 & 57.87 \(\) 0.71 & 57.48 \(\) 0.86 & 34.86 \(\) 2.27 \\ Flox  & 31.87 \(\) 0.50 & 59.15 \(\) 0.43 & 57.18 \(\) 0.66 & 34.01 \(\) 5.97 \\  & 36.95 \(\) 1.04 & 30.71 \(\) 2.35 & 57.28 \(\) 0.97 & 36.96 \(\) 0.04 \\  & 53.13 \(\) 0.41 & 31.81 \(\) 4.26 & 38.75 \(\) 1.12 & 60.66 \(\) 1.36 \\  & **1.73M-Med (SAMPs ViT)** & **62.46 \(\) 0.86** & **99.31 \(\) 0.48** & **63.44 \(\) 0.73** & **63.74 \(\) 2.08** \\   

Table 5: Performance comparison on linear evaluation and fine-tuning classification. The results are reported with average Accuracy on three training times.

Conclusion

We have demonstrated that a self-supervised learning technique based on second-order graph-matching, trained on a large-scale medical imaging dataset, significantly enhances performance in various downstream medical imaging tasks compared to other supervised learning methods and foundation models trained on hundreds of millions of image-text instances. Our findings are supported by the benefits shown in two different architectures: ResNet-50 and ViT backbones, which can be used for either end-to-end or prompt-based segmentation.

**Limitations and Future Work.** We propose to investigate the following points to improve LVM-Med performance. Firstly, extending LVM-Med to a hybrid 2D-3D architecture to allow direct application for 3D medical tasks instead of 2D slices. Secondly, although LVM-Med with ViT backbone utilizes more total parameters, in some cases, it is less effective than LVM-Med ResNet-50. This raises the question of whether a novel approach could improve the performance of ViT architectures. Finally, integrating multi-modal information such as knowledge graphs, bio-text, or electronic health records for LVM-Med is also important to make the model more useful in real-world applications.