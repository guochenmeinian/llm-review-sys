# Do SSL Models Have Deja Vu? A Case of Unintended Memorization in Self-supervised Learning

Casey Meehan\({}^{*,1}\), Florian Bordes\({}^{*,2,3}\), Pascal Vincent\({}^{2,3}\)

Kamalika Chaudhuri\({}^{{},1,3}\), Chuan Guo\({}^{{},3}\)

\({}^{1}\)UCSD, \({}^{2}\)Mila, Universite de Montreal, \({}^{3}\)FAIR, Meta

\({}^{*}\)Equal contribution, \({}^{}\)Equal direction contribution

cmeehan@eng.ucsd.edu, florian.bordes@umontreal.ca,

{pascal,kamalika,chuanguo}@meta.com

###### Abstract

Self-supervised learning (SSL) algorithms can produce useful image representations by learning to associate different parts of natural images with one another. However, when taken to the extreme, SSL models can unintendedly memorize _specific_ parts in individual training samples rather than learning semantically meaningful associations. In this work, we perform a systematic study of the unintended memorization of image-specific information in SSL models--which we refer to as _deja vu memorization_. Concretely, we show that given the trained model and a crop of a training image containing only the background (_e.g._, water, sky, grass), it is possible to infer the foreground object with high accuracy or even visually reconstruct it. Furthermore, we show that _deja vu_ memorization is common to different SSL algorithms, is exacerbated by certain hyperparameter choices, and cannot be detected by conventional techniques for evaluating representation quality. Our study of _deja vu_ memorization reveals previously unknown privacy risks in SSL models, as well as suggests potential practical mitigation strategies.

## 1 Introduction

Self-supervised learning (SSL) [11; 12; 33; 2; 9; 20] aims to learn general representations of content-rich data without explicit labels by solving a _pretext task_. In many recent works, such pretext tasks rely on joint-embedding architectures whereby randomized image augmentations are applied to create multiple views of a training sample, and the model is trained to produce similar representations for those views. When using cropping as random image augmentation, the model learns to associate objects or parts (including the background scenery) that co-occur in an image. However, doing so also arguably exposes the training data to higher privacy risk as objects in training images can be explicitly memorized by the SSL model. For example, if the training data contains the photos of individuals, the SSL model may learn to associate the face of a person with their activity or physical location in the photo. This may allow an adversary to extract such information from the trained model for targeted individuals.

In this work, we aim to evaluate to what extent SSL models memorize the association of specific objects in training images or the association of objects and their specific backgrounds, and whether this memorization signal can be used to reconstruct the model's training samples. Our results demonstrate that SSL models memorize such associations beyond simple correlation. For instance, in Figure 1 **(left)**, we use the SSL representation of a _training image crop containing only water_ and this enables us to reconstruct the object in the foreground with remarkable specificity--in this case a black swan. By contrast, in Figure 1 **(right)**, when using the _crop from the background of a test set image_ that the SSL model _has not seen before_, its representation only contains enough information toinfer, through correlation, that the foreground object was likely some kind of waterbird -- but not the specific one in the image.

Figure 1 shows that SSL models suffer from the unintended memorization of images in their training data--a phenomenon we refer to as _deja vu memorization_1 Beyond visualizing _deja vu_ memorization through data reconstruction, we also design a series of experiments to quantify the degree of memorization for different SSL algorithms, model architectures, training set size, _etc_. We observe that _deja vu_ memorization is exacerbated by the atypically large number of training epochs often recommended in SSL training, as well as certain hyperparameters in the SSL training objective. Perhaps surprisingly, we show that _deja vu_ memorization occurs even when the training set is large--as large as half of ImageNet --and can continually worsen even when standard techniques for evaluating learned representation quality (such as linear probing) do not suggest increased overfitting. Our work serves as the first systematic study of unintended memorization in SSL models and motivates future work on understanding and preventing this behavior. Specifically, we:

* Elucidate how SSL representations memorize aspects of individual training images, what we call _deja vu_ memorization;
* Design a novel training data reconstruction pipeline for non-generative vision models. This is in contrast to many prominent reconstruction algorithms like [7; 8], which rely on the model itself to generate its own memorized samples and is not possible for SSL models or classifiers;
* Propose metrics to quantify the degree of _deja vu_ memorization committed by an SSL model. This allows us to observe how _deja vu_ changes with training epochs, dataset size, training criteria, model architecture and more.

## 2 Preliminaries and Related Work

**Self-supervised learning** (SSL) is a machine learning paradigm that leverages unlabeled data to learn representations. Many SSL algorithms rely on _joint-embedding_ architectures (_e.g._, SimCLR , Barlow Twins , VICReg  and Dino ), which are trained to associate different augmented views of a given image. For example, in SimCLR, given a set of images \(=\{A_{1},,A_{n}\}\) and a randomized augmentation function \(\), the model is trained to maximize the cosine similarity of draws of \(((A_{i}))\) with each other and minimize their similarity with \(((A_{j}))\) for \(i j\). The augmentation function \(\) typically consists of operations such as cropping, horizontal flipping, and color transformations to create different views that preserve an image's semantic properties.

SSL representations.Once an SSL model is trained, its learned representation can be transferred to different downstream tasks. This is often done by extracting the representation of an image from

Figure 1: **Left:** Reconstruction of an SSL training image from a crop containing only the background. The SSL model memorizes the association of this _specific_ patch of water (pink square) to this _specific_ foreground object (a black swan) in its embedding, which we decode to visualize the full training image. **Right:** The reconstruction technique fails on a public test image that the SSL model has not seen before.

the _backbone model2_ and either training a linear probe on top of this representation or finetuning the backbone model with a task-specific head . It has been shown that SSL representations encode richer visual details about input images than supervised models do . However, from a privacy perspective, this may be a cause for concern as the model also has more potential to overfit and memorize precise details about the training data compared to supervised learning. We show concretely that this privacy risk can indeed be realized by defining and measuring _deja vu_ memorization.

Privacy risks in ML.When a model is overfit on privacy-sensitive data, it memorizes specific information about its training examples, allowing an adversary with access to the model to learn private information [30; 16]. Privacy attacks in ML range from the simplest and best-studied _membership inference attacks_[26; 25; 24] to _attribute inference_[17; 22; 21] and _data reconstruction_[7; 1; 19] attacks. In the former, the adversary only infers whether an individual participated in the training set. Our study of _deja vu_ memorization is most similar to the latter: we leverage SSL representations of the training image background to infer and reconstruct the foreground object. In another line of work in the NLP domain [6; 7]: when prompted with a context string present in the training data, a large language model is shown to generate the remainder of string at test time, revealing sensitive text like home addresses. This method was recently extended to extract memorized images from Stable Diffusion . We exploit memorization in a similar manner: given partial information about a training sample, the model is prompted to reveal the rest of the sample.3 In our case, however, since the SSL model is not generative, extraction is significantly harder and requires careful design.

## 3 Defining _Deja Vu_ Memorization

What is _deja vu_ memorization?At a high level, the objective of SSL is to learn general representations of objects that occur in nature. This is often accomplished by associating different parts of an image with one another in the learned embedding. Returning to our example in Figure 1, given an image whose background contains a patch of water, the model may learn that the foreground object is a water animal such as duck, pelican, otter, _etc._, by observing different images that contain water from the training set. We refer to this type of learning as _correlation_: the association of objects that tend to co-occur in images from the training data distribution.

A natural question to ask is _"Can the reconstruction of the black swan in Figure 1 be reasoned as correlation?"_ The intuitive answer may be no, since the reconstructed image is qualitatively very similar to the original image. However, this reasoning implicitly assumes that for a random image from the training data distribution containing a patch of water, the foreground object is unlikely to be a black swan. Mathematically, if we denote by \(\) the training data distribution and \(A\) the image, then

\[p_{}:=_{A}((A)= (A)=)\]

is the probability of inferring that the foreground object is a black swan through _correlation_. This probability may be naturally high due to biases in the distribution \(\), _e.g._, if \(\) contains no other water animal except for black swans. In fact, such correlations are often exploited to learn a model for image inpainting with great success [32; 27].

Despite this, we argue that reconstruction of the black swan in Figure 1 is _not_ due to correlation, but rather due to _unintended memorization_: the association of objects unique to a single training image. As we will show in the following sections, the example in Figure 1 is not a rare success case and can be replicated across many training samples. More importantly, failure to reconstruct the foreground object in Figure 1 (**right**) on test images hints at inferring through correlation is unlikely to succeed--a fact that we verify quantitatively in Section 4.1. Motivated by this discussion, we give a verbal definition of _deja vu_ memorization below, and design a testing methodology to quantify _deja vu_ memorization in Section 3.1.

**Definition:** A model exhibits _deja vu memorization_ when it retains information so specific to an individual training image, that it enables recovery of aspects particular to that image given a part that does not contain them. The recovered aspect must be beyond what can be inferred using only correlations in the data distribution.

We intentionally kept the above definition broad enough to encompass different types of information that can be inferred about the training image, including but not restricted to object category, shape, color and position. For example, if one can infer that the foreground object is red given the background patch with accuracy significantly beyond correlation, we consider this an instance of _deja vu_ memorization as well. We mainly focus on object category to quantify _deja vu_ memorization in Section 4 since the ground truth label can be easily obtained. We consider other types of information more qualitatively in the visual reconstruction experiments in Section 5.

**Distinguishing memorization from correlation.** When measuring _deja vu_ memorization, it is crucial to differentiate what the model associates through _memorization_ and what it associates through _correlation_. Our testing methodology is based on the following intuitive definition.

**Definition:** If an SSL model associates two parts in a training image, we say that it is due to _correlation_ if other SSL models trained on a similar dataset from \(\) without this image would likely make the same association. Otherwise, we say that it is due to _memorization_.

Notably, such intuition forms the basis for differential privacy (DP; Dwork et al. ; Dwork & Roth )--the most widely accepted notion of privacy in ML.

### Testing Methodology for Measuring _Deja Vu_ Memorization

In this section, we use the above intuition to measure the extent of _deja vu_ memorization in SSL. Figure 2 gives an overview of our testing methodology.

Dataset splitting.We focus on testing _deja vu_ memorization for SSL models trained on the ImageNet dataset 4. Our test first splits the ImageNet training set into three independent and disjoint subsets \(\), \(\) and \(\). The dataset \(\) is called the _target set_ and \(\) is called the _reference set_. The two datasets are used to train two separate SSL models, \(_{A}\) and \(_{B}\), called the _target model_ and the _reference model_. Finally, the dataset set \(\) is used as an auxiliary public dataset to extract information from \(_{A}\) and \(_{B}\). Our dataset splitting serves the purpose of

Figure 2: Overview of testing methodology. **Left:** Data is split into _target set_\(\), _reference set_\(\) and _public set_\(\) that are pairwise disjoint. \(\) and \(\) are used to train two SSL models \(_{A}\) and \(_{B}\) in the same manner. \(\) is used for KNN decoding at test time. **Right:** Given a training image \(A_{i}\), we use \(_{A}\) to embed \((A_{i})\) containing only the background, as well as the entire set \(\) and find the \(k\)-nearest neighbors of \((A_{i})\) in \(\) in the embedding space. These KNN samples can be used directly to infer the foreground object (_i.e._, class label) in \(A_{i}\) using a KNN classifier, to be visualized directly or their embeddings can be averaged as input to the trained RCDM to visually reconstruct the image \(A_{i}\). For instance, the KNN visualization results in Figure 1 (left) when given \(_{A}((A_{i}))\) and results in Figure 1 (right) when given \(_{A}((B_{i}))\) for an image \(B_{i}\).

distinguishing memorization from correlation in the following manner. Given a sample \(A_{i}\), if our test returns the same result on \(_{A}\) and \(_{B}\) then it is likely due to correlation because \(A_{i}\) is not a training sample for \(_{B}\). Otherwise, because \(\) and \(\) are drawn from the same underlying distribution, our test must have inferred some information unique to \(A_{i}\) due to memorization. Thus, by comparing the difference in the test results for \(_{A}\) and \(_{B}\), we can measure the degree of _deja vu_ memorization5.

Extracting foreground and background crops.Our testing methodology aims at measuring what can be inferred about the foreground object in an ImageNet sample given a background crop. This is made possible because ImageNet provides bounding box annotations for a subset of its training images--around 150K out of 1.3M samples. We split these annotated images equally between \(\) and \(\). Given an annotated image \(A_{i}\), we treat everything inside the bounding box as the foreground object associated with the image label, denoted \((A_{i})\). We take the largest possible crop that does not intersect with any bounding box as the background crop (or _periphery crop_), denoted \((A_{i})\)6

KNN-based test design.Joint-embedding SSL approaches encourage the embeddings of random crops of a training image \(A_{i}\) to be similar. Intuitively, if the model exhibits _deja vu_ memorization, it is reasonable to expect that the embedding of \((A_{i})\) is similar to that of \((A_{i})\) since both crops are from the same training image. In other words, \(_{A}((A_{i}))\) encodes information about \((A_{i})\) that cannot be inferred through correlation. However, decoding such information is challenging as these approaches do not learn a decoder associated with the encoder \(_{A}\).

Here, we leverage the public set \(\) to decode the information contained in \((A_{i})\) about \((A_{i})\). More specifically, we map images in \(\) to their embeddings using \(_{A}\) and extract the \(k\)-nearest-neighbor (KNN) subset of \(_{A}((A_{i}))\) in \(\). We can then decode the information contained in \((A_{i})\) in one of two ways:

* _Label inference:_ Since \(\) is a subset of ImageNet, each embedding in the KNN subset is associated with a class label. If \((A_{i})\) encodes information about the foreground object, its embedding will be close to samples in \(\) that have the same class label (_i.e._, foreground object category). We can then use a KNN classifier to infer the foreground object in \(A_{i}\) given \((A_{i})\).
* _Visualization:_ Since we have access to a KNN subset associated to a given \((A_{i})\), we can visualize directly the images associated to this subset. Then, we can infer through visualizing what is common within this subset, what information can be retrieved for this single crop. In addition, to simplify the visualization pipeline and to map directly a given crop representation to an image, we train an RCDM --a conditional generative model--on \(\) to decode \(_{A}\) embeddings into images. The RCDM reconstruction can recover qualitative aspects of an image remarkably well, such as recovering object color or spatial orientation using its SSL embedding. Given the KNN subset, we average their SSL embeddings and use the RCDM model to visually reconstruct \(A_{i}\).

In Section 4, we focus on quantitatively measuring _deja vu_ memorization with label inference, and then use the KNN to visualize _deja vu_ memorization in Section 5.

## 4 Quantifying _Deja Vu_ Memorization

We apply our testing methodology to quantify a specific form of _deja vu_ memorization: inferring the foreground object (class label) given a crop of the background.

Extracting model embeddings.We test _deja vu_ memorization on a variety of popular SSL algorithms, with a focus on VICReg . These algorithms produce two embeddings given an input image: a _backbone_ embedding and a _projector_ embedding that is derived by applying a small fully-connected network on top of the backbone embedding. Unless otherwise noted, all SSL embeddings refer to the projector embedding. To understand whether _deja vu_ memorization is particular to SSL, we also evaluate embeddings produced by a supervised model \(_{A}\) trained on \(\). We apply the same set of image augmentations as those used in SSL and train \(_{A}\) using the cross-entropy loss to predict ground truth labels.

Identifying the most memorized samples.Prior works have shown that certain training samples can be identified as more prone to memorization than others [16; 28; 29]. Similarly, we provide a heuristic to identify the most memorized samples in our label inference test using confidence of the KNN prediction. Given a periphery crop, \((A_{i})\), let \(_{A}(A_{i})\) denote its \(k\)-nearest neighbors in the embedding space of \(_{A}\). From this KNN subset we can obtain: **(1)**\(_{A}^{}(A_{i})\), the vector of class probabilities (normalized counts) induced by the KNN subset, and **(2)**\(_{A}^{}(A_{i})\), the negative entropy of the probability vector \(_{A}^{}(A_{i})\), as confidence of the KNN prediction. When entropy is low, the neighbors agree on the class of \(A_{i}\) and hence confidence is high. We can sort the confidence score \(_{A}^{}(A_{i})\) across samples \(A_{i}\) in decreasing order to identify the most confidently predicted samples, which likely correspond to the most memorized samples when \(A_{i}\).

### Population-level Memorization

Our first measure of _deja vu_ memorization is population-level label inference accuracy: _What is the average label inference accuracy over a subset of SSL training images given their periphery crops?_ To understand how much of this accuracy is due to \(_{A}\)'s _deja vu_ memorization, we compare with a correlation baseline using the reference model: \(_{B}\)'s label inference accuracy on images \(A_{i}\). In principle, this inference accuracy should be significantly above chance level (\(1/1000\) for ImageNet) because the periphery crop may be highly indicative of the foreground object through correlation, _e.g._, if the periphery crop is a basketball player then the foreground object is likely a basketball. Figure 3 compares the accuracy of \(_{A}\) to that of \(_{B}\) when inferring the labels of images in \(A_{i}\)7 using \((A_{i})\). Results are shown for VIRReg and the supervised model; trends for other models are shown in Appendix A.5. For both VIRReg and supervised models, inferring the class of \((A_{i})\) using \(_{B}\) (dashed line) through correlation achieves a reasonable accuracy that is significantly above chance level. However, for VIRReg, the inference accuracy using \(_{A}\) (solid red line) is significantly higher, and the accuracy gap between \(_{A}\) and \(_{B}\) indicates the degree of _deja vu_ memorization. We highlight two observations:

* The accuracy gap of VIRReg is significantly larger than that of the supervised model. This is especially notable when accounting for the fact that the supervised model is trained to associate randomly augmented crops of images with their ground truth labels. In contrast, VIRReg has no label access during training but the embedding of a periphery crop can still encode the image label.
* For VIRReg, inference accuracy on the \(1\%\) most confident examples is nearly \(95\%\), which shows that our simple confidence heuristic can effectively identify the most memorized samples. This result suggests that an adversary can use this heuristic to identify vulnerable training samples to launch a more focused privacy attack.

The _deja vu_ score.The curves of Figure 3 show memorization across confidence values for a single training scenario. To study how memorization changes with different hyperparamters, we extract a single value from these curves: the _deja vu score_ at confidence level \(p\). In Figure 3, this the gap between the solid red (or gray) and dashed red (or gray) where confidence (\(x\)-axis) equal \(p\%\). In other words, given the periphery crops of set \(\), \(_{A}\) and \(_{B}\) separately select and label their top \(p\%\) most confident examples, and we report the difference in their accuracy. The _deja vu_ score captures both the degree of memorization by the accuracy gap and the _ability to identify memorized examples_ by the confidence level. If the score is 10% for \(p=33\%\), \(_{A}\) has 10%

Figure 3: Accuracy of label inference using the target model (trained on \(\)) vs. the reference model (trained on \(\)) on the top \(\%\) most confident examples \(A_{i}\) using only \((A_{i})\). For VIRReg, there is a large accuracy gap between the two models, indicating a significant degree of _deja vu_ memorization.

higher accuracy on its most confident third of \(\) than KNN\({}_{B}\) does on its most confident third. In the following, we set \(p=20\%\), approximately the largest gap for VICReg (red lines) in Figure 3.

Comparison with the linear probe train-test gap.A standard method for measuring SSL performance is to train a linear classifier--what we call a 'linear probe'--on its embeddings and compute its performance on a held out test set. From a learning theory standpoint, one might expect the linear probe's train-test accuracy gap to be indicative of memorization: the more a model overfits, the larger is the difference between train set and test set accuracy. However, as seen in Figure 4, the linear probe gap (dark blue) fails to reveal memorization captured by the _deja vu_ score (red) 8.

### Sample-level Memorization

The _deja vu_ score shows, _on average_, how much better an adversary can select and classify images when using the target model trained on them. This average score does not tell us how many individual images have their label successfully recovered by KNN\({}_{A}\) but not by KNN\({}_{B}\). In other words, how many images are exposed by virtue of _being in training set_\(\): a risk notion foundational to differential privacy. To better quantify what fraction of the dataset is at risk, we perform a sample-level analysis by fixing a sample \(A_{i}\) and observing the label inference result of KNN\({}_{A}\) vs. KNN\({}_{B}\). To this end, we partition samples \(A_{i}\) based on the result of label inference into four distinct categories: Unassociated - label inferred with neither KNN; Memorized - label inferred only with KNN\({}_{A}\); Misrepresented - label inferred only with KNN\({}_{B}\); Correlated - label inferred with both KNNs.

Intuitively, unassociated samples are ones where the embedding of \((A_{i})\) does not encode information about the label. Correlated samples are ones where the label can be inferred from \((A_{i})\) using correlation, _e.g._, inferring the foreground object is basketball given a crop showing a basketball player. Ideally, the misrepresented set should be empty but contains a small portion of examples due to chance. _Deja vu_ memorization occurs for memorized samples where the embedding of SSL\({}_{B}\) does not encode the label but the embedding of SSL\({}_{A}\) does. To measure the pervasiveness of _deja vu_ memorization, we compare the size of the memorized and misrepresented sets. Figure 5 shows how the four categories of examples change with number of training epochs and training set size. The unassociated set is not shown since the total share adds up to one. The misrepresented set remains under \(5\%\) and roughly unchanged across all settings, consistent with our explanation that it is due to chance. In comparison, VICReg's memorized set surpasses \(15\%\) at 1000 epochs. Considering that up to 5% of these memorized examples could also be due to chance, we conclude that **at least 10% of VICReg's training set is _deja vu_ memorized.**

Figure 4: Effect of training epochs and train set size on _deja vu_ score (red) in comparison with linear probe accuracy train-test gap (dark blue) for VICReg. **Left:**_deja vu_ score increases with training epochs, indicating growing memorization while the linear probe baseline decreases significantly. **Right:**_deja vu_ score stays roughly constant with training set size while linear probe gap shrinks significantly, suggesting that memorization may be problematic even for large datasets.

Figure 5: Partition of samples \(A_{i}\) into the four categories: unassociated (not shown), memorized, misrepresented and correlated for VICReg. The memorized samples—those whose labels are predicted by KNN\({}_{A}\) but not by KNN\({}_{B}\)—occupy a significantly larger share of the training set than the misrepresented samples—those predicted by KNN\({}_{B}\) but not KNN\({}_{A}\) by chance.

## 5 Visualizing _Deja Vu_ Memorization

Beyond enabling label inference using a periphery crop, we show that _deja Vu_ memorization allows the SSL model to encode other forms of information about a training image. Namely, we leverage an external public dataset \(\) and use it to find the nearest neighborhoods in this public dataset given a training periphery crop. We aim to answer the following two questions: **(1)** Can we visualize the distinction between correlation and _deja Vu_ memorization? **(2)** What foreground object details can be extracted from the SSL model beyond class label?

Public image retrieval pipelineFollowing the pipeline in Figure 2, we use the projector embedding to find the KNN subset for the periphery crop, \((A_{i})\), and visualize the images belonging to this KNN subset.

RCDM pipeline.RCDM is a conditional generative model that is trained on the _backbone embedding_ of images \(X_{i}\) to generate an image that resembles \(X_{i}\). At test time, following the pipeline in Figure 2, we first use the projector embedding to find the KNN subset for the periphery crop, \((A_{i})\), and then average their backbone embeddings as input to the RCDM model. Then, RCDM decodes this representation to visualize its content.

Visualizing Correlation vs. Memorization.Figure 6 shows examples of dams from the correlated set (left) and the memorized set (right) as defined in Section 4.2, along with the associated KNN set. In Figure 5(a), the periphery crop is represented by the pink square, which contains concrete structure attached to the dam's main structure. As a result, both SSL\({}_{A}\) and SSL\({}_{B}\) produce embeddings of

Figure 6: Correlated and Memorized examples from the _dam_ class. Both SSL\({}_{A}\) and SSL\({}_{B}\) are SimCLR models. **Left:** The periphery crop (pink square) contains a concrete structure that is often present in images of dams. Consequently, the KNN can infer the foreground object using representations from both SSL\({}_{A}\) and SSL\({}_{B}\) through this correlation. **Right:** The periphery crop only contains a patch of water. The embedding produced by SSL\({}_{B}\) only contains enough information to infer that the foreground object is related to water, as reflected by its KNN set. In contrast, the embedding produced by SSL\({}_{A}\) memorizes the association of this patch of water with dam and the KNN select images of dams.

Figure 7: Visualization of _deja Vu_ memorization beyond class label. Both SSL\({}_{A}\) and SSL\({}_{B}\) are VICReg models. The four images shown belong to the memorized set of SSL\({}_{A}\) from the _badger_ class. KNN images using embeddings from SSL\({}_{A}\) can reveal not only the correct class label, but also the specific badger species: _European_ (left) and _American_ (right). Such information does not appear to be memorized by the reference model SSL\({}_{B}\).

\((A_{i})\) whose KNN set in \(\) consist of dams, _i.e._, there is a correlation between the concrete structure in \((A_{i})\) and the foreground dam. In Figure 6b, the periphery crop only contains a patch of water, which does not strongly correlate with dams in the ImageNet distribution. Evidently, the reference model SSL\({}_{B}\) embeds \((A_{i})\) close to that of other objects commonly found in water, such as sea turtle and submarine. In contrast, the KNN set according to SSL\({}_{A}\) all contain dams despite the vast number of alternative possibilities within the ImageNet classes which highlight memorization in SSL\({}_{A}\) between this specific patch of water and the dam.

Visualizing Memorization Beyond Class Label.Figure 7 shows four examples of badgers from the memorized set. In all four images, the periphery crop (pink square) does not contain any indication that the foreground object is a badger. Despite this, the KNN set using SSL\({}_{A}\) consistently produce images of badgers, while the same does not hold for SSL\({}_{B}\). More interestingly, the KNN using SSL\({}_{A}\) in Figure 7a all contain _European_ badgers, while reconstructions in Figure 7b all contain _American_ badgers, accurately reflecting the species of badger present in the respective training images. Since ImageNet-1K does _not_ differentiate between these two species of badgers, our reconstructions show that SSL models can memorize information that is highly specific to a training sample beyond its class label9.

## 6 Mitigation of _deja vu_ memorization

We cannot yet make claims on why _deja vu_ occurs so strongly for some SSL training settings and not for others. To gain some intuition for future work, we present additional observations that shed light on which parameters have the most salient impact on _deja vu_ memorization.

Deja vu memorization worsens by increasing number of training epochs.Figure 4a shows how _deja vu_ memorization changes with number of training epochs for VICReg. The training set size is fixed to 300K samples. From 250 to 1000 epochs, the _deja vu_ score (red curve) grows _threefold_: from under 10% to over 30%. Remarkably, this trend in memorization is _not_ reflected by the linear probe gap (dark blue), which only changes by a few percent beyond 250 epochs.

Training set size has minimal effect on _deja vu_ memorization.Figure 4b shows how _deja vu_ memorization responds to the model's training set size. The number of training epochs is fixed to 1000. Interestingly, training set size appears to have almost _no_ influence on the _deja vu_ score (red line), indicating that memorization is equally prevalent with a 100K dataset and a 500K dataset. This result suggests that _deja vu_ memorization may be detectable even for large datasets. Meanwhile, the

Figure 8: Effect of two kinds of hyper-parameters on VICReg memorization. **Left:**_deja vu_ score (red) versus the _invariance_ loss parameter, \(\), used in the VICReg criterion (100k dataset). Larger \(\) significantly reduces _deja vu_ memorization with minimal effect on linear probe validation performance (green). \(=25\) (near maximum _deja vu_) is recommended in the original paper. **Right:**_deja vu_ score versus projector layer-guillotine regularization —from projector to backbone. Removing the projector can significantly reduce _deja vu_. In Appendix A.7 we show that the backbone still can memorize.

Figure 9: Effect of model architecture and criterion on _deja vu_ memorization. **Left:**_deja vu_ score with VICReg for resnet (purple) and vision transformer (green) architectures versus number of model parameters. As expected, memorization grows with larger model capacity. This trend is more pronounced for convolutional (resnet) than transformer (ViT) architectures. **Right:** Comparison of _deja vu_ score 20% conf. and ImageNet linear probe validation accuracy (P: using projector embeddings, B: using backbone embeddings) for various SSL criteria.

[MISSING_PAGE_FAIL:10]