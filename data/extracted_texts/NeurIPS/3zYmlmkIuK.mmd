# Asynchronous Multi-Agent Reinforcement Learning

with General Function Approximation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We study multi-agent reinforcement learning (RL) where agents cooperate through asynchronous communications with a central server to learn a shared environment. Our first focus is on the case of multi-agent contextual bandits with general function approximation, for which we introduce the Async-NLin-UCB algorithm. This algorithm is proven to achieve a regret of \((() N()})\) and a communication complexity of \((M^{2}_{E}())\), where \(M\) is the total number of agents and \(T\) is the number of rounds, while \(_{E}()\) and \(N()\) are the Eluder dimension and the covering number of function space \(\) respectively. We then progress to the more intricate setting of multi-agent RL with general function approximation, and present the Async-NLSVI-UCB algorithm. This algorithm enjoys a regret of \((H^{2}() N()})\) and a communication complexity of \((HM^{2}_{E}())\), where \(H\) is the horizon length and \(K\) the number of episodes. Our findings showcase the provable efficiency of both algorithms for collaborative learning within nonlinear environments and minimal communication overhead.

## 1 Introduction

Multi-agent reinforcement learning (RL) is an important paradigm in RL, and has been successfully applied to real-world tasks such as robotics (Williams et al., 2016; Liu et al., 2019; Ding et al., 2020; Liu et al., 2020; Na et al., 2022), games (Vinyals et al., 2017; Berner et al., 2019; Jaderberg et al., 2019; Ye et al., 2020), and control systems (Bazzan, 2009; Yu et al., 2014, 2020; Min et al., 2022; Xu et al., 2023). By learning cooperatively, agents benefit from sharing learning experiences, enabling them to collectively enhance their decision-making capabilities. This collaborative process is usually accomplished through the utilization of a central server, whose task is to aggregate local data and deliver feedback for the agents.

There has been an excellent line of work establishing provably efficient algorithms for multi-agent bandits and RL. However, most existing works are restricted to the synchronous setting, where communications between all agents and the server must happen simultaneously. This is impractical since in many scenarios the availability of agents may vary and be unpredictable. Ideally, communication should be allowed to happen asynchronously to offer the agents more flexibility. He et al. (2022) and Min et al. (2023) studied this setting respectively for _linear contextual bandits_ and _linear Markov Decision Processes_ (MDPs), both of which assumes linearity in the environment, and introduced algorithms with low regret and communication cost. Yet the linear function class is quite limited, and does not encompass practical reinforcement learning scenarios where nonlinearity is prevalent.

To address the aforementioned drawback, in this work, we tackle environments with general function approximation, broadening the applicability of the algorithm to more realistic and complex scenarios. We first delve into multi-agent contextual bandits with general function approximation, where multiple agents interact with homogeneous environments in parallel to solve a common objective. Notably, the communication protocol is designed to be flexible and asynchronous, allowing agents to initiate communication with the server and acquire new policy functions whenever the need arises. The primary objective is to minimize total regret while reducing communication cost as much as possible.

We propose an algorithm Async-NLin-UCB, which adapts a fully asynchronous communication protocol, and leverages various methods for tackling nonlinear function approximation. Despite the flexibility of communication, our algorithm performs almost as well as a single agent, in terms of a regret that is mostly independent of the number of agents and a low communication cost.

We then progress to multi-agent RL with general function approximation under similar requirements and objectives. We propose an algorithm named Async-NLSVI-UCB based on Least-Squares Value Iteration (LSVI) to learn the underlying Markov decision processes (MDPs), which demonstrates similar advantages with provably low regret and communication cost.

Our main contributions are summarized in the following:

* For asynchronous multi-agent nonlinear contextual bandits, we propose the algorithm Async-NLin-UCB, which enjoys an \((() N()}+_{E}( ))\) regret and an \((M^{2}_{E}())\) communication complexity, where \(_{E}()\) and \(N()\) are respectively the Eluder dimension and the covering number of function space \(\).
* For asynchronous multi-agent nonlinear MDPs, we propose the algorithm Async-NLSVI-UCB, which enjoys an \((H^{2}() N()}+H^{2} _{E}())\) regret and a communication complexity of \((HM^{2}_{E}())\).
* At the core of our algorithm, we design a _communication criterion_ in order to tackles the challenges posed by both asynchronous communication and the nonlinearity of function approximation. To guarantee a low communication cost, we propose a low switching communication criterion that allows the agent to trigger communication rounds.
* We carefully design our _download content_ from server to local agents, which consist only of decision and bonus functions, with no mention of any specific historical data. This effectively protects user data against exposure by disallowing local users from obtaining the data of others.

**Notation.** We use lower case letters to denote scalars. We denote by \([n]\) the set \(\{1,,n\}\). For two positive sequences \(\{a_{n}\}\) and \(\{b_{n}\}\) with \(n=1,2,\), we write \(a_{n}=O(b_{n})\) if there exists an absolute constant \(C>0\) such that \(a_{n} Cb_{n}\) holds for all \(n 1\). We use \(()\) to further hide the polylogarithmic factors. For two non-negative integers \(a,b\) satisfying \(a<b\) and a sequence \(\{s_{i}\}\) indexed by integers \(i\), we use \(s_{[a b]}\) to denote the subsequence \(\{s_{a},s_{a+1},,s_{b}\}\).

## 2 Related Work

### Multi-Agent Bandits

First, there is a multitude of previous work on distributed or federated multi-armed bandits and stochastic linear bandits (Liu and Zhao, 2010; Szorenyi et al., 2013; Landgren et al., 2016; Chakraborty et al., 2017; Landgren et al., 2018; Martinez-Rubio et al., 2019; Sankararaman et al., 2019; Wang et al., 2020; Ma et al., 2021; Huang et al., 2021). For the more realistic setting of contextual bandits, most previous work are within the scope of linear contextual bandits with synchronized communication. Korda et al. (2016) introduced two novel distributed confidence ball (DCB) algorithms for linear bandit problems in peer-to-peer networks. Wang et al. (2020) considered both P2P and star-shaped communication, achieving near-optimal regret and low communication cost that is largely independent of the time horizon in their algorithm DisLinUCB. Dubey and Pentland (2020) proposed FedUCB, an algorithm focusing on differential-privacy.

Li and Wang (2022) first considered an asynchronous communication protocol and proposed the algorithm Async-LinUCB with near-optimal regret, yet the algorithm contains a download step for all agents triggered by the central server. Their results are flexible and contains a parameter to control the trade-off between regret and communication cost. He et al. (2022) improved the setting to a fully asynchronous communication, proposing the algorithm FedLinUCB with near-optimal regret of \((d)\) and low communication cost of \((dm^{2})\), comparable to the benchmark in single-agent contextual linear bandits (Abbasi-Yadkori et al., 2011). We consider the same communication protocol in our results. A summary of these results along with ours can be found in the first four rows of Table 1.

### Multi-Agent RL

Multi-agent reinforcement learning is decidedly more challenging than contextual bandits. There is also a vast literature on this setting, with many works discussing different aspects of multi-agent RLthan ours. For example, there are works focusing on convergence guarantees (Zhang et al., 2018, 2018, 2018), non-stationary or heterogeneous environments (Lowe et al., 2017; Yu et al., 2021; Dubey and Pentland, 2021; Kuba et al., 2022; Liu et al., 2022; Jin et al., 2022), and deep federated RL (Clemente et al., 2017; Espeholt et al., 2018; Horgan et al., 2018; Nair et al., 2015; Zhuo et al., 2019), to name a few. We refer to a recent survey on federated reinforcement learning Qi et al. (2021) for a more comprehensive summary.

Narrowing it down to multi-agent RL with function approximation, the benchmark is the LSVI-UCB algorithm in the single-agent setting (Jin et al., 2020), with an \((d^{3/2}H^{2})\) regret. Dubey and Pentland (2021) proposed CoopLSVI for multi-agent linear MDPs, which requires a synchronized communication through central server, and proves a regret of \((d^{3/2}H^{2})\). They also extended their result to the heterogeneous setting. Min et al. (2023) considered the fully asynchronous setting and introduced the Async-Coop-LSVI-UCB algorithm, with a \((d^{3/2}H^{2})\) regret not dependent on the number of agents \(M\), as well as a low communication cost. A summary of these results along with ours can be found in the last three rows of Table 1.

### General function approximation

Reinforcement learning with general function approximation extends the well-studied case of linear MDPs to more general classes of MDPs, and has gained a lot of traction in recent years (Wang et al., 2020; Jin et al., 2021; Foster et al., 2023; Du et al., 2021; Agarwal and Zhang, 2022; Agarwal et al., 2023). Previous works focus on different measures of complexity for the function classes, for example the Bellman rank proposed by Jiang et al. (2017), the Bellman Eluder dimension introduced in Jin et al. (2021), the Decision-Estimation Coefficient in Foster et al. (2023), and generalized Eluder dimension in Agarwal et al. (2023). Our work considers the Eluder dimension with the introduction of uncertainty estimators \(D^{2}\), which has been widely utilized to establish results in RL with general function approximation (Agarwal et al., 2023; Zhao et al., 2023; Ye et al., 2023; Di et al., 2023).

## 3 Preliminaries

In this section, we introduce the formal definition of both multi-agent nonlinear contextual bandits and MDPs and some related concepts, and discuss the asynchronous communication protocol.

### Multi-Agent Contextual Bandits with General Function Approximation

We assume a global action set \(\) that is known to all agents. At each round \(t[T]\), a single arbitrary agent \(m_{t}[M]\) is chosen to participate. The agent receives a contextual decision set \(_{t}\) and chooses from the set an action \(a_{t}_{t}\) to perform, and subsequently receives a random reward \(r_{t}\)

   Algorithm & Regret & Communication & 
 Fully \\ asynchronous \\  \\  DisLinUCB & \(d^{2}T\) & \(d^{3}M^{3/2}\) & ✗ \\  Async-LinUCB & \(dM^{(1-)/2} T\) & \(dM^{1+} T\) & ✗ \\  FedLinUCB & & \(d T\) & \(dM^{2} T\) & ✓ \\  Async-NLin-UCB & \( NT} T\) & \(_{E}M^{2}^{2}T\) & ✓ \\ (ours) & \( NT} T\) & \(_{E}M^{2}^{2}T\) & ✓ \\  Coop-LSVI & & & \\ [Dubey and Pentland, 2021] & \(d^{3/2}H^{2} K\) & \(dHM^{3}\) & ✗ \\  Async-Coop-LSVI-UCB & \(d^{3/2}H^{2}\) & \(dHM^{2} K\) & ✓ \\  Async-NLSVI-UCB & & & \\ (ours) & \( N}H^{2} K\) & \(_{E}HM^{2}^{2}K\) & ✓ \\   

Table 1: Comparison of our result against baseline methods for multi-agent contextual bandits and MDPs. Note that the first four rows are for contextual bandits, and the last three are for reinforcement learning. Only our algorithms are in the general function approximation setting. We abbreviate \(_{E}=_{E}()\) and \(N=N()\), and hide logarithmic factors. For algorithms with synchronized communication, each communication round actually corresponds to \(M\) rounds in asynchronous settings, which explains the extra \(M\) terms.

The assumption of general function approximation is that the reward is generated according to

\[r_{t}=f^{*}(a_{t})+_{t},\] (1)

where \(f^{*}\) is the ground truth objective function, and \(_{t}\) is a random noise variable. We assume the the objective function lies within a known function class \(\). In addition, we also make the following assumptions regarding the function class and noise variables, which are standard assumptions for contextual bandits (Abbasi-Yadkori et al., 2011; He et al., 2022):

**Assumption 3.1**.: Suppose the following conditions hold for the contextual bandits environment:

* For any \(f\) and \(a\), \(|f(a)| 1\);
* \(_{t}\) is \(R\)-sub-Gaussian conditioned on data history: \(e^{_{t}}a_{1:t},m_{1:t},r_{1:t-1} (R^{2}^{2}/2),\).

**Learning Objective.** The primary goal of contextual bandits is to minimize the cumulative regret

\[(T)=_{t=1}^{T}[f^{*}(a_{t})-_{a_{t}}f^{*}(a)].\]

Notice that this summation is across all time steps does not depend on agent participation order, as should be the case for the resulting regret bound. To achieve this goal, agents are allowed to communicate with the server to upload their interaction history and update their policy. The secondary learning objective is to reduce communication overhead. We will explain the communication protocol further in Section 3.4.

### Multi-Agent Episodic MDPs with General Function Approximation

We consider episodic MDPs, which are a classic family of models in reinforcement learning (Sutton and Barto, 2018). It is characterized by the following elements, which we assume to be homogeneous across all agents: a state space \(\), an action space \(\), the horizon length \(H\), transition probability functions \(=\{_{h}(|,)\}_{h=1}^{H}\) and reward functions \(\{r_{h}(,)\}_{h=1}^{H})\). Similar to the bandit case, for each episode \(k=1,,K\), a single agent \(m=m_{k}\) is chosen to participate. An episode \(k\) begins with an initial state \(s_{1}^{*}\), which is drawn from an unknown fixed distribution. Then for steps \(h=1,,H\), the participating agent \(m\) selects an action \(a_{h}^{k}\) based on the observed state \(s_{h}^{k}\). After each action, the agent receives a reward \(r_{h}^{k}=r_{h}(s_{h}^{k},a_{h}^{k})\), where \(r_{h}:\) is the reward function at step \(h\). Here for the sake of convenience, we assume the reward function to be deterministic, but it is not difficult to generalize our result to stochastic rewards. We also assume \(r_{h}(s,a)\) for all \((s,a)\) without loss of generality. The environment then transitions to the next state according to \(s_{h+1}^{k}_{h}(|s_{h}^{k},a_{h}^{k})\), where \(_{h}\) is the transition probability at step \(h\). The episode terminates when \(r_{H}\) is observed.

The strategy an agent employs to interact with the environment is called the agent's _policy_, which can be described by a set of decision functions \(=\{_{h}\}_{h=1}^{H}\), where \(_{h}:\) is the decision function at level \(h\), mapping the current state to an action to select.

**Value Functions.** For any policy \(=\{_{h}\}\), we define \(Q\)-value functions and \(V\)-value functions:

\[Q_{h}^{}(s_{h},a_{h}):=_{h^{}=h}^{H}r_{h^{ }}(s_{h^{}},a_{h^{}})s_{h},a_{h}, V_{h }^{}(s_{h}):=_{h^{}=h}^{H}r_{h^{}}(s_{h^{ }},a_{h^{}})s_{h},\] (2)

where the expectation is taken over the trajectory \((s_{1},a_{1},,s_{h},a_{h})\), determined by the transition probability functions \(\) and policy \(\). The optimal strategy \(^{*}\) is the maximizer of the value functions:

\[^{*}:=*{argmax}_{}V_{h}^{}(s_{1}), s_{1}.\]

We also have optimal value functions \(Q_{h}^{*}:=Q_{h}^{^{*}}\) and \(V_{h}^{*}:=V_{h}^{^{*}}\), which satisfy Bellman equations

\[Q_{h}^{*}(s_{h},a_{h})=r_{h}(s_{h},a_{h})+V_{h+1}^{*}(s_{h+1} )s_{h},a_{h}, V_{h}^{*}(s_{h})=_{a}Q_{h}^ {*}(s_{h},a).\] (3)

**Function Approximation.** We approximate \(Q\)-value functions with function classes \(\{_{h}\}_{h=1}^{H}\), which contain real value functions with domain \(\). One basic assumption is that \(Q_{h}^{*}_{h}\) for all steps \(h[H]\). Now with the convention that functions at level \(H+1\) are uniformly zero, i.e., \(f_{H+1}=0\), we define the Bellman operator \(_{h}\):

\[(_{h}f_{h+1})(s_{h},a_{h}):=r_{h}(s_{h},a_{h})+f_{h +1}(s_{h+1})s_{h},a_{h},\]

and we expect \(_{h}\) to map any function in \(_{h+1}\) to a function in \(_{h}\), i.e., \(_{h}_{h+1}_{h}\). This is called the completeness assumption, which is a fundamental assumption in RL with general function approximation (Wang et al., 2020; Jin et al., 2021).

**Learning Objective.** The primary goal in multi-agent MDPs is to minimize the cumulative regret over \(K\) episodes

\[(K)=_{k=1}^{K}V_{1}^{*}(s_{1}^{k})-V_{1}^{_{m,k}}(s_{1}^{ k}),\]

where \(_{m,k}\) is the policy of agent \(m=m_{k}\) at round \(k\), while the secondary objective is to minimize the communication cost.

### Eluder Dimension and Covering Number

To measure the complexity of the learning objective, Russo and Van Roy (2013) first proposed the concept of Eluder dimension, which we define below.

**Definition 3.2** (\(\)-dependence).: For a function class \(\) on domain \(\), a point \(z\) is _\(\)-dependent_ on \(\) if, for any \(f_{1},f_{2}\) satisfying \(}f_{1}(z^{})-f_{2}(z^{ })^{2}}\), it must hold that \(|f_{1}(z)-f_{2}(z)|\). Accordingly, \(z\) is _\(\)-independent_ of \(\) if it is not \(\)-dependent on \(\).

**Definition 3.3** (Eluder dimension).: The _\(\)-Eluder dimension_\(_{E}(,)\) is the length of the longest sequence of elements in \(\) satisfying that, for some \(_{0}>\), each element is \(_{0}\)-independent of the set consisting of its predecessors.

It has been demonstrated that the Eluder dimension roughly corresponds to regular dimension concepts in linear and quadratic cases (Russo and Van Roy, 2013), and that the Eluder family is strictly larger than the generalized linear class (Li et al., 2022). Note that our Eluder definition can be applied to either the contextual bandit case with \(=\) or the MDPs case with \(=\).

We also introduce covering number for function classes (Wainwright, 2019) in the following:

**Definition 3.4** (Covering number).: An _\(\)-cover_ of \(\) is any subset \(_{}\) such that for any \(f\), there exists \(f^{}_{}\) that \(\|f-f^{}\|_{}\). The _covering number_ of \(\), denoted by \(N(,)\), is the minimal cardinality of its \(\)-cover.

### Communication Protocol

We consider a star-shaped communication model (He et al., 2022; Min et al., 2023), where the agents communicate through a central server to collaborate. To ensure asynchronous communication, we mandate that all communications must be initiated by a participating agent. Specifically, at the end of a time step / episode, the agent will decide whether or not to trigger a communication round. If so, the agent uploads its local data history and receives some global data for future decision making. The _communication cost_ is the total number of communication rounds initiated by the agents.

One variability is the form of global data that the communicating agent downloads from server. It may be tempting to have the server send all its stored trajectories to the agent for future decision making, but this will unnecessarily expose other agents' data to the current participating agent. We will come back to this issue and our solution in Section 4.2.

## 4 Multi-Agent Contextual Bandits

In this section, we introduce the Asynchronous Nonlinear UCB (Async-NLin-UCB) algorithm designed for multi-agent contextual bandits with general function approximation, and provide a theoretical result for its regret and communication cost.

### Algorithm: Async-NLin-UCB

Algorithm 1 takes as input the total number of time steps \(T\), regularization parameter \(\), communication parameter \(\) and exploration radii \(\{_{t}\}_{t=1}^{T}\).

In the algorithm, there are some variables that go through different versions as \(t\) progresses through \(1,,T\). For clarity, here we give them an extra subscript \(t\) to denote the version of that variable before (not included) the least squares calculation on Line 12 at round \(t\).

Throughout the learning process, the server maintains a global history set \(Z_{t}^{}\) that stores action-reward pairs \((a,r)\), initialized on Line 2 and updated only during communication rounds. Each local agent \(m\) maintains a decision function \(f_{m,t}\) for taking action, a bonus function \(b_{m,t}\) for checking communication criterion, and a local data history set \(Z_{m,t}^{}\), all initialized on Line 3. Each step of Algorithm 1 contains two parts: local exploration and server updates.

**Part I: Local Exploration.** At step \(t\) a single agent \(m=m_{t}\) is active (Line 5). It receives a decision set, finds the greedy action according to its decision function \(f_{m,t}\), receives a reward, and updates its local dataset \(Z_{m,t}^{}\) (Lines 5 - 7).

After exploration, the agent checks if the switch condition is true using its bonus function:

\[_{(a,r) Z^{}_{m,t}}b_{m,t}^{2}(a)/_{^{}}^{ 2}+,\] (4)

where \(t^{}\) is the last time step when agent \(m\) communicated with the server. If so, the agent initiates a communication round and uploads its local data (Line 9), prompting the server to begin global policy updates. We will discuss the reasons behind this switch condition in Section 4.2.

**Part II: Server Updates.** After receiving a new local data history from an agent, the server merges the data into its global dataset \(Z^{}_{t}\) (Line 11), and calculate a function \(_{t+1}\) which minimizes the sum of squares error according to the current dataset \(Z^{}_{t}\) (Line 12):

\[_{t+1}=*{argmin}_{f}_{(a,r) Z^{ }_{t}}f(a)-r^{2}.\] (5)

The next step is to obtain a bonus function \(b_{t+1}\) from the oracle \(_{}\) from Definition 4.1 (Line 12). We discuss the specifics of this construction in detail up next in Section 4.2. Finally, the server sends the optimistic value function \(_{t+1}+b_{t+1}\) and the bonus function \(b_{t+1}\) back to agent \(m\) for future exploration and updates; agent \(m\) also resets its local data history to an empty set (Lines 13 and 15).

### Uncertainty Estimators and Bonus Functions

In this section, we introduce uncertainty estimators and bonus functions, and give a detailed explanation for our communication criterion (4). Most of these apply to the MDPs setting as well.

**Uncertainty Estimators.** First we define the uncertainty estimator of new data \(a\) against data history \(Z\), which is considered in many works on bandits and RL with general function approximation (Gentile et al., 2022; Agarwal et al., 2023):

\[D_{,}(a;Z)=_{f_{1},f_{2}}|f_{1}(a)-f_{2} (a)|,r) Z}|f_{1}(a^{})-f_{2}( a^{})|^{2}},\] (6)

here \(\) is the regularization parameter, \(\) is a function class. Intuitively, the uncertainty estimator measures the difference between functions on new data \(a\) against the difference on historical data \(Z\).

**Switch Condition Based On Uncertainty Estimators.** The determinant-based criterion is a common technique used in contextual bandits and RL with linear function approximation to reduce policy switching or communication cost (Abassi-Yadkori et al., 2011). For nonlinear function approximation, one can use uncertainty estimators to formulate a new form of switch condition:

\[_{(a,r) Z^{}_{t}}D_{,}^{2}(a;Z^{}_{t}).\] (7)

where we use \(Z^{}_{t}\) and \(Z^{}_{t}\) to denote newly accumulated data and old historical data. This criterion has a similar function as the determinant-based criterion in linear settings. Parameter \(\) controls communication frequency: smaller \(\) indicates more frequent communication, more accurate decision functions and smaller regret, thus implying a trade-off between regret and communication cost.

**Bonus Function Oracle.** Next, we introduce bonus functions obtained through oracles that approximate the uncertainty estimators.

**Definition 4.1** (Bonus Function Oracle \(_{}\)).: Given domain \(\), the oracle \(_{}(Z,;,)\) takes the following as inputs: a dataset \(Z\) consisting of a series of data points \((z,e)\), where \(z\) and \(e\) is some additional data content; function class \(\) with functions \(f:_{ 0}\); regularization parameter \(\) and exploration radius \(\). It returns a function \(b_{}:_{ 0}\) satisfying for any \(z\) that

* \(b(z)f_{1}(z)-f_{2}(z):f_{1},f_{2}, _{(z,e) Z}f_{1}(z)-f_{2}(z)^{2}^{2}}\);
* \(D_{,}(z;Z) b(z)/+} C_{}D_{,}(z;Z)\),

where \(C_{}\) is an absolute constant.

_Remark 4.2_.: Similar bonus function oracles have been proposed in previous works (Definition 3 in Agarwal et al. (2023)). The accessibility of these oracles is also supported by previous works that proposed methods to compute bonus functions (Kong et al., 2023; Wang et al., 2020b). In this definition, we leave the domain and data format to be variable so the oracle can be applied to both contextual bandits and MDPs. For bandits, the domain is \(\), and the data format has \(z=a\) and \(e=r\). The first property of the bonus function guarantees the optimism of decision functions \(_{t+1}+b_{t+1}\) (see Lemma 6.1 for MDPs or Lemma A.2 for bandits), while the second property links bonuses to uncertainty estimators.

**Switch Condition Based On Bonus Functions.** If we try to adapt the switch condition (7) in our setting, a local agent will require access to historical data \(Z_{t}^{}\) to calculate uncertainty estimators \(D_{,}^{2}(a;Z_{t}^{})\). For multi-agent learning, this dataset consists of the collective data from all agents, and giving local agent access is a clear violation of data privacy. Our solution is to let local agents download bonus functions and set communication criterion to (4), using bonus functions instead of uncertainty estimators.

**Decision Functions Based On Bonus Functions.** Another benefit of introducing the bonus function is evident from our exploration method in line 6. A common practice for nonlinear RL algorithms is to construct _confidence sets_ of functions during policy update, and find the optimal function within the confidence sets during exploration (Agarwal et al., 2023; Ye et al., 2023). However, in a multi-agent setting, this would involve the download of confidence sets, which is impractical due to the complex nature of function classes. With the bonus function, local agents need only download the _decision function_ from the server for future exploration, which for contextual bandits is simply \(_{t+1}+b_{t+1}\).

### Theoretical Results

Our main results for Algorithm 1 are summarized in the following theorem, which provides a regret upper bound and communication complexity order.

**Theorem 4.3**.: _By taking \(=O(1/T)\), \(_{t}=C_{,1}+RC(M,)(3MN(, )/)\) and \(C(M,)=+M\), the regret of Algorithm 1 within \(T\) rounds is_

\[O_{1}}(T/\{1,\})+(1+M)_{E}^{2}(T/\{1,\}),\]

_where we abbreviate \(_{E}:=_{E}(,/T)\); the total communication complexity is_

\[O(1+M)^{2}/_{E}^{2}(T/\{1,\})\]

_Remark 4.4_.: When reduced to linear contextual bandits, where \(_{E}(,/T)=(d)\) and \( N(,)=(d)\), our result on regret correspond exactly to Theorem 5.1 of He et al. (2022), except for an extra \(1+M\) term in the communication cost, an unimportant term when taking \(=1/M^{2}\) that comes from the complication of communication cost analysis in nonlinear settings.

## 5 Multi-Agent Reinforcement Learning

In this section, we introduce the Asynchronous Nonlinear Least Squares Value Iteration UCB (Async-NLin-UCB) algorithm for multi-agent MDPs with general function approximation, and a corresponding theoretical result.

### Algorithm: Async-NLSVI-UCB

To better represent the elements in the datasets, we sometimes use \(o_{h}\) to represent the tuple \((s_{h},a_{h},r_{h},s_{h+1})\) and \(z_{h}\) to represent \((s_{h},a_{h})\) when there is no confusion. Similar to the band-dit case, we give some variables an extra subscript \(k\) here for clarity, which denotes the version of the variable before (not included) Line 14 at episode \(k\).

```
1:Input: total number of rounds \(K\), parameters \(\), \(\), \(_{k,h}\) for \(k=[K]\) and \(h[H]\)
2:Server init: Set \(Z_{h}^{}=\) for all \(h[H]\).
3:Local init:\( m[M]\) and \(h[H]\), set \(Q_{m,h}=1\), \(b_{m,h}=(,_{h};,_{0,h})\), \(Z_{m,h}^{}=\).
4:for\(k=1,,K\)do
5: Agent \(m=m_{k}[M]\) is active and receives initial state \(s_{1}^{k}\).
6:for\(h=1,,H\)do
7: Take action \(a_{h}^{k}=*{argmax}_{a}Q_{m,h}(s_{h}^{k},a)\), receive reward \(r_{h}^{k}\) and next state \(s_{h+1}^{k}\).
8: Update \(Z_{m,h}^{}=Z_{m,h}^{}\{(s_{h}^{k},a_{h}^{k},r_{h}^{k}, s_{h+1}^{k})\}\).
9:endfor
10:if switch condition (8) is met then
11: Send new data \(\{Z_{m,h}^{}\}_{h[H]}\) to server.
12:on server:
13: Update \(Z_{h}^{}=Z_{h}^{} Z_{m,h}^{}\).
14: Initialize \(Q_{H+1}=V_{H+1}=0\).
15:for\(h=H,H-1,,1\)do
16: Calculate \(_{h}\) according to (9) and bonus function \(b_{h}=_{}(Z_{h}^{},_{h};,_{k,h})\).
17: Calculate \(Q_{h}\) and \(V_{h}\) according to (11).
18:endfor
19: Send \(\{Q_{h}\}_{h=1}^{H}\) and \(\{b_{h}\}_{h=1}^{H}\) to agent \(m\).
20:endof server
21: Agent \(m\) receives \(Q_{m,h}=Q_{h}\), \(b_{m,h}=b_{h}\) and resets \(Z_{m,h}^{}=\) for all \(h[H]\).
22:endif
23:endfor ```

**Algorithm 2** Federated Nonlinear MDPs

The server maintains global historical datasets \(Z_{k,h}^{}\) containing sequences of tuples \((s_{h},a_{h},r_{h},s_{h+1})\), initialized in Line 2. Each local agent \(m\) maintains optimistic value functions \(\{Q_{m,k,h}\}_{h=1}^{H}\), bonus functions \(\{b_{m,k,h}\}_{h=1}^{H}\), and local datasets \(\{Z_{m,k,h}^{}\}_{h=1}^{H}\), all initialized in Line 3.

Each episode \(k\) of Algorithm 2 also consists of the two parts local exploration and server updates.

**Part I: Local Exploration.** At step \(k\) an agent \(m=m_{k}\) is active (Line 5). It interacts with the environment by executing the greedy policy according to \(\{Q_{m,k,h}\}_{h=1}^{H}\), obtaining a trajectory \(\{(s_{h}^{k},a_{h}^{k},r_{h}^{k},s_{h+1}^{k})\}_{h=1}^{H}\), which is then stored into the local historical datasets \(Z_{m,k,h}^{}\) (lines 6 - 9).

After exploration, the agent checks for the following switch condition: there exists \(h[H]\) so that

\[_{o_{h} Z_{m,k,h}^{}}b_{m,k,h}^{2}(s_{h},a_{h})/_{ k^{},h}^{2}+,\] (8)

where \(k^{}\) is the last communication round for \(m\). If so, the agent triggers communication (Line 11).

**Part II: Server Updates.** After receiving new data, the server merges it with its global datasets \(Z_{k,h}^{}\) (Line 13) and calculates value function estimates \(\{Q_{k+1,h}\}_{h=1}^{H}\) and \(\{V_{k+1,h}\}_{h=1}^{H}\) using LSVI.

Suppose we already have \(Q\)- and \(V\)-value function estimates \(Q_{k+1,h+1}\) and \(V_{k+1,h+1}\) at level \(h+1\). We solve the least squares problem for \(_{h}\) to minimize the Bellman error (Line 16):

\[_{k+1,h}=*{argmin}_{f_{h}_{h}}_{o_{h}  Z_{k,h}^{}}f_{h}(z_{h})-r_{h}-V_{k+1,h+1}(s_{h+1}) ^{2}.\] (9)

We now also define the uncertainty estimator of a new pair of data \(z=(s,a)\) against data history \(Z\) with normalization parameter \(\) and function class \(\) as

\[D_{,}(z;Z)=_{f_{1},f_{2}}f_{1}(z)-f _{2}(z) Z}|f_{1}(z^{})-f _{2}(z^{})|^{2}}.\] (10)

Similar to the bandits setting, the uncertainty can be approximated with the bonus function acquired from an oracle \(_{}\) in Definition 4.1. In this case, the domain \(=\), and the data format corresponds to \(z=(s,a)\) and \(e=(r,s^{})\). Despite these definitions not depending on the step \(h\), we expect the parameters \(z,Z,\) to always come from same step \(h\). Finally, we allow the bonus function classes \(_{h}=_{h,}\) to vary between different levels.

After calling oracle for \(b_{k+1,h}\) (Line 16), we can obtain value function estimates (Line 17):

\[Q_{k+1,h}(s,a)=_{k+1,h}(s,a)+b_{k+1,h}(s,a), V_{k+1,h}(s)=_ {a}Q_{k+1,h}(s,a).\] (11)

Iterating through \(h=H,,1\), the server calculates a set of updated \(Q\)-value functions \(\{Q_{k+1,h}\}_{h=1}^{H}\) and bonus functions \(\{b_{k+1,h}\}_{h=1}^{H}\), and send them back to agent \(m\) for future exploration and updates (lines 19 and 21).

### Theoretical Results

We summarize the regret and communication cost of Algorithm 2 in the following theorem:

**Theorem 5.1**.: _Taking \(=O(1/HK)\), \(_{h,k}=C_{,2}+HC(M,)\) and \(N():=_{h}N(_{h},)N(_{h+1},)N( _{h+1},)\), the regret within \(K\) rounds is bounded by \(OH_{2}K}(K/\{1, \})+H^{2}(1+M)_{E}^{2}(K/\{1,\})\)._

_where we abbreviate \(_{E}:=_{E}(,/K)\); the total communication complexity is_

\[OH(1+M)^{2}_{E}(,/K)^{2}(K/\{ 1,\}).\]

_Remark 5.2_.: This result when reduced to linear MDPs correspond well to Theorem 5.1 in Min et al. (2023). Taking \(=1/M^{2}\), we get a regret of \(H^{2} N}+H^{2}_{E}\) and a communication cost of \(HM^{2}_{E}\), where \(N=_{h}\{N(_{h},),N(_{h},)\}\).

## 6 Proof Sketch

In this section, we provide an outline for the proof of Theorem 5.1, while a more detailed proof can be found in Appendix B, and the full versions of the following lemmas are in Appendix B.1.

### Regret Upper Bound

For the regret upper bound, the first lemma establishes optimism of value function estimates.

**Lemma 6.1**.: _Taking \(_{k,h}\) as in Theorem 5.1, with probability at least \(1-\), for all \(k\), \(z\) and \(h[H]\), \(|_{h}Q_{k+1,h+1}(z)-_{k+1,h}(z)| b_{k+1,h}(z)\)._

This allows us to decompose regret into a sum of bonuses:

\[(K)=_{k=1}^{K}V_{1}^{*}(s_{1}^{k})-V_{1}^{_{m,h}}(s_{ 1}^{k})\]

\[_{k=1}^{K}_{h=1}^{H}_{_{m,k}}Q_{m,k,h}- _{h}Q_{m,k,h+1}(s_{h}^{k},a_{h}^{k})_{k=1}^{K}\! _{h=1}^{H}\!2b_{m,k,h}(s_{h}^{k},a_{h}^{k}).\] (12)

The sum of bonuses is equal to the sum of uncertainty up to a constant, which we bound in the following lemma corresponding to the elliptical potential lemma (Abbasi-Yadkori et al., 2011).

**Lemma 6.2**.: _Define universal datasets as \(Z_{k,h}^{}=\{o_{h}^{k^{}}\}_{k^{}[k]}\). Then we have for any \(h[H]\):_

\[_{k=1}^{K}\!D_{,}^{2}(z_{h}^{k};Z_{k-1,h}^{}) =O_{E}(,/K)^{2}(K/\{1,\}).\]

Careful examination exposes a problem: the uncertainty \(D_{,}(z;Z_{k,h}^{})\) corresponding to bonuses are based on server data \(Z_{k,h}^{}\) instead of universal data \(Z_{k,h}^{}\). The next lemma bridges this gap:

**Lemma 6.3**.: _For any \(z\), \(k[K]\), \(h[H]\), \(D_{,}^{2}(z;Z_{k,h}^{})(1+M)D_{, }^{2}(z;Z_{k,h}^{})\)._

With these, we can deduce the regret bound from (12).

### Communication Cost

For communication cost, we employ an _epoch segmentation_ scheme, which defines \(N\) epochs segmented by episodes \(\{k_{i}\}_{i=1}^{N}\), with \(k_{i}\) being the smallest episode satisfying

\[_{o_{h} Z_{k_{i},h}^{} Z_{k_{i-1},h}^{}} _{h=1}^{H}D_{,_{h}}^{2}(z_{h};Z_{k_{i-1},h}^{})  1.\] (13)

This is a generalization of epoch segmentation based on doubling determinants in linear settings, yet the lack of determinant in the nonlinear case dramatically increases its complexity. Intuitively, switch condition (8) suggests an agent must gather a substantial amount of data to trigger communication, yet a careful analysis according to (13) yields a maximum of \(M+C/\) communication rounds within one epoch. With this we only need an upper bound for the number of epochs \(N\). This is derived by summing (13) over all epochs, then using Lemma 6.1 and Lemma 6.3 to bound the left hand side.

## 7 Conclusions

We propose the algorithms Async-NLin-UCB and Async-NLSVI-UCB to tackle multi-agent nonlinear contextual bandits and MDPs with asynchronous communication. We prove that our algorithms enjoy low regret and communication cost, which are comparable to previous results.

Our algorithms employ a communication criterion that allows the agents to trigger communication rounds, effectively controlling communication cost while promoting the asynchronous protocol. Moreover, we carefully design the contents of server download to guard against data exposure.