# Structured Voronoi Sampling

Afra Amini\({}^{1}\) Li Du\({}^{2}\) Ryan Cotterell\({}^{1}\)

\({}^{1}\)ETH Zurich \({}^{2}\)Johns Hopkins University

{afra.amini, ryan.cotterell}@inf.ethz.ch

leodu@cs.jhu.edu

###### Abstract

Gradient-based sampling algorithms have demonstrated their effectiveness in text generation, especially in the context of controlled text generation. However, there exists a lack of theoretically grounded and principled approaches for this task. In this paper, we take an important step toward building a principled approach for sampling from language models with gradient-based methods. We use discrete distributions given by language models to define densities and develop an algorithm based on Hamiltonian Monte Carlo to sample from them. We name our gradient-based technique Structured Voronoi Sampling (svs). In an experimental setup where the reference distribution is known, we show that the empirical distribution of svs samples is closer to the reference distribution compared to alternative sampling schemes. Furthermore, in a controlled generation task, svs is able to generate fluent and diverse samples while following the control targets significantly better than other methods.

https://github.com/AfraAmini/svs

## 1 Introduction

Gradient-based sampling algorithms such as Hamiltonian Monte Carlo [hmc; 32] and Langevin dynamics  are widely used in Bayesian inference due to their efficiency in drawing samples from high-dimensional space . Such algorithms construct a Markov Chain that has the desired distribution as its stationary distribution and use the gradient information of this distribution to efficiently navigate the state space. Additionally, gradient-based sampling schemes have recently been deployed in computer vision to generate high-quality images from state-of-the-art models [13; 43], and are a popular choice for tasks such as image synthesis  and image-to-image translation .

In natural language processing, there have been several attempts to apply gradient-based sampling techniques to sampling text from neural language models [21; 23; 38]. The motivation behind this approach to text generation is to sample from energy-based probabilistic models, where the normalization factor is not tractable. One such case is in controlled text generation, where energy functions are usually defined as a linear combination of LM probabilities and the probability of satisfying a set of predefined constraints [21; 23; 38]. In contrast to computer vision, however, applying gradient-based sampling schemes to text generation is nuanced as text, in contrast to images, is discrete.

Upon closer inspection, none of the proposed algorithms actually defines a valid Markov Chain Monte Carlo [MCMC; 12; 29] scheme that will draw samples from the model in the limit. For instance, Qin et al.  relax the language model from a distribution over strings to a distribution over logits. While the relaxation does transform the language model into a continuous distribution, it introduces bias. Kumar et al. [MuCoLA; 21] take a different approach. They derive a constrained gradient-basedsampler where the constraint is enforced through a projection. However, the projection invalidates the MCMC procedures, leading to an algorithm without guarantees.1

We derive a principled Hamiltonian Monte Carlo scheme for generating text from language models, which we call a structured Voronoi sampler. Our scheme consists of two steps. First, we give a recipe for encoding discrete distributions as densities over \(^{d}\); we term the resulting encoding Voronoi measures.2 Second, we derive a refractive Hamiltonian Monte Carlo algorithm  for sampling from an arbitrary Voronoi measure. In our theoretical analysis, we show that, despite the presence of discontinuities, we are able to give proof that our sampler satisfies the detailed balance condition and, thus, is a correct MCMC scheme.

To empirically evaluate the performance of structured Voronoi sampling, we begin by applying it to a toy example where the exact reference probability distribution is known. We compare the empirical distribution of drawn samples with the reference distribution and show Voronoi sampler's distribution is closer to the reference distribution than MuCoLa or unconstrained hmc. Furthermore, we use our sampling scheme for controlled generation, where the goal is to use GPT-2 to generate restaurant reviews for a target type of food, e.g., Italian, Fast food, or Japanese, and separately to generate text with a positive sentiment. We find that structured Voronoi sampling outperforms fudge, MuCoLa, and Langevin Dynamics algorithms in terms of adhering to the control target. Additionally, the samples generated by structured Voronoi sampling are comparably fluent and diverse to those produced by the other methods.

## 2 Language Models

Let \(\) be an alphabet, a finite, non-empty set. By \(^{*}}}{{=}}_{n=0}^{ }^{n}\), we denote the Kleene closure of \(\).3 A probability distribution over \(^{*}\) is called a **language model** (**LM**). The elements of \(\) may be characters, subword pieces, or words; the choice lies with the modeler. Language models can be factored autoregressively by means of the chain rule of probability, i.e., for any string \(=w_{1} w_{N}^{*}\), we can write

\[p()=p()_{n=1}^{N}p(w_{n}_{<n}),\] (1)

where \(\) is a distinguished end-of-sequence token and \(_{<n}\) is the prefix of length \((n-1)\) of the string \(\). We define \(}}{{=}} \{\}\), and require the conditional distributions \(p(_{<n})\) to be defined over \(\). While all language models can be factored autoregressively, not all conditionals \(p(_{<n})\) can be assembled into a language model. In some cases, probability mass must be placed on infinite sequences . In this work, we assume working with **tight** language models, i.e., that they indeed define valid probability distributions over \(^{*}\).

### Language Modeling with Embeddings

Most neural language models make use of embeddings. Moreover, in most language models, the weights are shared between the language model head and the embedding layer. Such an embedding-based language model is defined as follows

\[p(w_{n}_{<n})}}{{=}}_{w_{n}}(_{<n})}{_{w}_{w}(_{<n})},\] (2)

where \(_{w}^{d}\) is the embedding of \(w\) and \(:^{*}^{d}\) is a real-valued encoding of the context. Notably, the context embedding \((_{<n})^{d}\) is obtained by inputting the context \(_{<n}\) into the language model, converting it to embeddings \(_{_{<n}}\), passing it through neural network layers, and extracting the encoding from the model at position \(n-1\).

Interestingly, one can lift an embedding-based language model to be a distribution over the set of **base embeddings**: \(=\{_{w}:w\}\). Specifically, we can associate a sequence of embeddings \(=[_{1},,_{N}]\) to any string \(=w_{1} w_{N}\). Substituting each \(w\) with the corresponding \(_{w}\), we can rewrite Eq. (1) as

\[p()=p(_{})_{n=1}^{N}p( _{n}_{<n}).\] (3)

This transformation encodes a language model as a distribution over real-valued vectors. However, \(p()\) only places a positive probability on a countable set, and is zero everywhere else.

### Controlled Language Modeling with Embeddings

In a controlled generation task, we are interested in a subset of strings \(\) that have a target property \(t\). Therefore, we want to model and sample from a conditional distribution \(p( t)\), e.g., sample a sentence given a topic. Following Bayes' rule, one can write \(p( t) p(t)\,p()\). Previous papers model \(p(t)\) with an embedding-augmented classifier. Such a classifier receives embeddings \(\) associated with \(\) and predicts the probability of the target \(t\). Notably, if the classifier and the LM share the same base embeddings \(\), the controlled LM can also be lifted as a distribution over the base embeddings

\[p( t)=}\,p(t)\,p(),\] (4)

where \(Z_{t}=_{}p(t)\,p()\) is an intractable normalization factor, that sums over the embeddings of all possible strings.4 Identically to \(p()\), \(p( t)\) only places a positive probability on a countable set.

## 3 Voronoi Measures

In this section, we demonstrate how to encode an embedding-based language model as a density that places positive probability on a set with a measure greater than zero. Such encoding allows us to derive a principled gradient-based sampling approach to generate samples in SS6. We start with some definitions.

**Definition 1**.: _An **embedding-augmented** probability distribution over the first \(M\) positive integers \([M]\) is an array \(=[p_{1},,p_{M}]\) such that \(p_{m} 0\) and \(_{m=1}^{M}p_{m}=1\) where we assume that there is a real-valued embedding \(\{_{m}\}_{m=1}^{M}^{d}\) associated with each \(m[M]\)._

Embedding-augmented distributions can be viewed as densities over \(^{d}\) using the following simple encoding

\[p()=p_{m},&=_{m}\\ 0,&.\] (5)

Eq. (5), however, yields a density that is \(0\) almost everywhere (with respect to the standard Lebesgue measure) and its gradient with respect to \(p_{m}\) is also zero almost everywhere. Thus, Eq. (5) is not amenable to gradient-based sampling, and to derive a meaningful gradient-based sampling we require a more nuanced encoding.

To provide such an encoding, we introduce the **Voronoi measure**. Given an embedding-augmented distribution \(=[p_{1},,p_{M}]\) with embeddings \(\{_{m}\}_{m=1}^{M}\), and a compact set \(^{d}\) that covers the embeddings, i.e., \(\{_{m}\}_{m=1}^{M}\), we define the **Voronoi cell** for the \(m^{}\) item with respect to the compact set \(\) as follows

\[C_{m}=:,||-_{m} ||_{2}^{2}||-_{m^{}}||_{2}^{2}, m^{ } m}.\] (6)

Now, using the definition of a Voronoi cell \(C_{m}\) given in Eq. (6), we can define a density that is _not_ zero almost everywhere as follows. The strategy is to spread out the probability mass \(p_{m}\) over the entirety of the set \(C_{m}\). To do so, we assume access to a set of **base measures**\(\{_{m}\}_{m=1}^{M}\) that give us a reference for how to judge the probability mass in each \(C_{m}\). We make this encoding formal in the following definition.

**Definition 2**.: _Let \(=[p_{1},,p_{M}]\) be an embedding-augmented distribution with embeddings \(\{_{m}\}_{m=1}^{M}^{d}\), and let \(\) be a compact set such that \(\{_{m}\}_{m=1}^{M}\). Furthermore, let \(\{_{m}\}_{m=1}^{M}\) be a set of base measures over \(^{d}\) that are absolutely continuous with respect to the standard Lebesgue measure \(\) over \(^{d}\), i.e., \(_{m}\). Define the \((,)\)-**Voronoi measure** as follows_

\[p_{}()}}{{=}}\{ ()}}{_{m}(C_{m^{*}()})} _{m}}{}(),\ \ \ .\] (7)

_where we define projection_

\[m^{*}()}}{{=}}*{ argmin}_{m[M]}||-_{m}||_{2}^{2}.\] (8)

In the following proposition, we make precise the sense in which a Voronoi measure encodes the original embedding-augmented distribution.

**Proposition 1**.: _Let \(=[p_{1},,p_{M}]\) be an embedding-augmented distribution with embeddings \(\{_{m}\}_{m=1}^{M}^{d}\), and let \(p_{}\) be the corresponding Voronoi measure Eq. (7). Then, \(p_{}(C_{m})=p_{m}\) where \(C_{m}\) is defined as in Eq. (6). See App. C.1 for proof._

**Example 1**.: _Suppose \(=[p_{1},,p_{4}]\) is a categorical distribution, and there are 4 embeddings in \(^{2}\) associated with each \(p_{i}\), namely: \(_{1}=,_{2}=[-1,1],_{3}=[-1,-1],_ {4}=[1,-1]\). Given the \(=[-2,2][-2,2]\) and the embedding-augmented probability distribution \(\), Eq. (7) defines a Voronoi measure over this space, where the Voronoi cells are visualized in Fig. 1. We will discuss in SS6 how Voronoi sampling navigates this space._

### Structured Voronoi Measures

To encode language models as densities more naturally, we introduce a generalization of the Voronoi measure, which we term a structured Voronoi measure. Now, rather than a distribution over \(M\) elements, we assume to have a sequence of length \(N\). Each token in the sequence takes value in \([M]\). Let \(=[m_{1},,m_{N}][M]^{N}\). We define a **structured Voronoi cell** as \(C_{}=_{n=1}^{N}C_{m_{n}}\), where \(\) denotes the Cartesian product and we define the individual Voronoi cell as

\[C_{m_{n}}=:,||- _{m_{n}}||_{2}^{2}||-_{m^{}}||_{2}^{2}, m ^{} m_{n}}.\] (9)

**Proposition 2**.: _Let \(\) be a measure on \(^{d}\). Then, we have the product measure space as \((C_{})=_{n=1}^{N}(C_{m_{n}})\). See App. C.2 for proof._

**Definition 3**.: _Let \(\) be an embedding-augmented distribution over \([M]^{N}\). For \([M]^{N}\), we denote \(\)'s probability as \(p_{}\), and \(\)'s embedding as \(_{}^{N d}\). Let \(\) be a compact set that covers the embeddings \(_{}\) and let \(\) be a base measure absolutely continuous with respect to the Lebesgue measure \(\). We define the \((,)\)-**Voronoi measure** as follows_

\[p_{}()}}{{=}}\{ ^{*}()}}{(C_{^{*}()})}}{}(),\ \ \ \\ 0,\  to Text Generation

Def. 3 gives us the flexibility to use any probability distribution over sequences of embeddings to define a structured Voronoi measure. For example, one can substitute \(p_{^{*}()}\) with an embedding-augmented LM, i.e., Eq. (3), to encode a language model as a structured Voronoi measure. Another example is to encode a controlled LM as a structured Voronoi measure by substituting \(p_{^{*}()}\) with Eq. (4).

Base Measure.We have explained how to encode our desired distribution as a structured Voronoi measure. However, in order to actually implement a gradient-based sampler, we need to specify the base probability measures \(\{_{m}\}_{m=1}^{M}\). Given an embedding-augmented probability \(p()\) it is natural to follow the gradient of \( p()\) with respect to the word embedding, i.e., \(_{}=_{} p()\). Thus, if we want to follow a direction similar to \(_{}\), one natural choice for \(\) is a Gaussian measure centered at the gradient \(_{}\)_restricted_ to Voronoi cell \(C_{}\),6 which we define:

\[(A)=})}_{A C_{}}(- \|_{}-\|_{2}^{2})().\] (12)

The normalizer ensures the measure is a probability measure. Furthermore, we have that \(\)'s Radon-Nikodym derivative with respect to the Lebesgue measure \(\) is given by

\[}{}()}}{{=}}})} (-||_{}-||_{2}^{2}),&  C_{}\\ 0,&\] (13)

Eq. (13) should be recognizable as the standard Gaussian density, albeit one that is truncated to the Voronoi cell \(C_{}\).

**Proposition 3**.: _Eq. (13) is absolutely continuous with respect to the Lebesgue measure \(\). See App. C.3 for proof._

**Proposition 4**.: _The gradient of the log of the Voronoi measure \(p_{}\) is given by_

\[_{} p_{}()=_{ }-,&(C_{})\\ ,& C_{}\\ ,&\] (14)

_where the first two blocks in the case statement apply if there exists some \(\) such that \((C_{})\) or \( C_{}\). See App. C.4 for proof._

## 5 Gradient-Based Sampling

In this section, we first review gradient-based sampling methods, namely hmc and Langevin dynamics. Then we discuss how they have been used in previous papers on text generation. This will set the stage for our algorithm in SS6, which is based on hmc.

### Hamiltonian Monte Carlo

The goal of hmc, originally proposed by Duane et al. , is to design a better proposal distribution in the standard Metropolis-Hastings MCMC by taking advantage of the gradient information in a principled way. Concretely, to sample from a given distribution \(p()\) where \(^{d}\), hmc treats \(\) as the coordinates of the particles in some fictitious physical system. It then introduces an auxiliary momentum variable \(^{d}\) associated with each coordinate and defines a Hamiltonian function \(H(,)\). Here, the Hamiltonian \(H\) has the intuitive physical interpretation of the total energy of some conservative system, and, in classical mechanics, decomposes into the potential energy \(U()\) and the kinetic energy \(K()\), i.e., \(H(,)=U()+K()\). This formulation is convenient in part because if we define the joint distribution \(p(,) e^{-H(,)}\) as in energy-based models, then

\[p(,) e^{-U()} e^{-K()},\] (15)which means we can treat \(\) and \(\) as independent variables. Naturally, we can let \(U()=- p()\) so that \(\) has the marginal of the target distribution. It is also common practice to set \(K()=^{}M^{-1}/2\) so that the momentum variable has a Gaussian distribution. Here, \(M^{d d}\) is called the **mass matrix** and commonly set to identity.7

The Hamiltonian \(H\) determines the equations of motion in a physical system, given by the Hamiltonian equations, which is also known as Hamiltonian dynamics,

\[}{t}=},}{t}=-}.\] (16)

We are now ready to give a high-level description of how hmc generates a single sample (see Algorithm 1): First sample a momentum \(\) from a Gaussian (line 1), then evolve the system using the Hamiltonian equations Eq. (16) for some predetermined amount of time (lines 4 to 7), and finally accept the new state with the Metropolis-Hastings acceptance criterion (lines 8 to 13). Note that the Hamiltonian equations often admit no closed-form solution in practice, and hence one needs to use numerical integrators for approximation. In particular, the leapfrog integrator, corresponding to lines 5 to 7, is almost always used in hmc.

Ingeniously designed by Duane et al. , the efficiency of this elegant procedure is a result of several favorable properties of Hamiltonian mechanics--namely volume preservation, reversibility, and conservation. Upon exact simulation of the Hamiltonian dynamics, these properties will lead to the acceptance probability of one, i.e., every proposed move will be accepted. In App. D, we will give intuition about these properties. Since the method developed later in SS6 will subsume hmc as a special case, we delay the proof of correctness until then.

Langevin Dynamics.Langevin dynamics is a simplification of hmc, where only one leapfrog step is performed. Furthermore, what is usually referred to as Langevin dynamics is in fact the _uncorrected_ Langevin dynamics, where the acceptance criterion is ignored, i.e., every proposed sample is accepted; see Algorithm 2. While uncorrected Langevin dynamics is guaranteed to converge when the energy function is smooth [31, SS5.3], there is no such guarantee with the presence of a discontinuity in the energy function. Nevertheless, due to its simplicity, Langevin dynamics is the only gradient-based sampling method that has been applied for text generation applications.

```
0:\(^{t}\): current sample, \(U\): potential energy function, \(\): step size, \(L\): number of leapfrog steps
0: next sample: \(^{t+1}\)
1:\((,)\)
2:\(H^{t} U(^{t})+K()\)
3:\(^{t+1}^{t}\)
4:for\(l=1,,L\):
5:\(- U(^{t+1})\)
6:\(^{t+1}^{t+1}+\)
7:\(- U(^{t+1})\)
8:\(H^{t+1} U(^{t+1})+K()\)
9:\( H H^{t+1}-H^{t}\)
10:if\((0,1)<e^{- H}:\)
11:return\(^{t+1}\)
12:else
13:return\(^{t+1}^{t}\) ```

**Algorithm 1** HMC

### Applications to Controlled Generation

One of the primary advantages of using gradient-based techniques for controlled generation is that it provides a means to sample from the conditional distribution Eq. (4) without having to calculate the normalization factor \(Z_{t}\). For example in hmc algorithm (Algorithm 1), all we need to calculate regarding the potential energy \(U()=- p( t)\) is two terms: (1) the gradient of \(U\) with respect to \(\): \( U\), and (2) the difference between the potential energy of two points \( U\) for the Metropolis criterion. Fortunately, both terms are independent of \(Z_{t}\), and we can sample from the conditional distribution without the need to compute \(Z_{t}\).

MuCoLa.As defined in Eq. (4), \(p( t)\) is \(^{d}\)-valued, so it is tempting to apply a gradient-based sampling technique to sample from it. And, indeed, Kumar et al.  have proposed such a scheme based on Langevin dynamics. They define the potential energy of the embeddings as \(U()}}{{=}}- p( t)\) and apply Langevin dynamics out of the box; see Algorithm 3. However, since \(U()\) is zero for any vector other than vectors in \(\) (defined in SS2.1), they modify the sampling process and project the proposed sample to the base embeddings set at each step of the sampling process (line 3 of the algorithm). The added projection step, however, is neither volume-preserving nor time-reversible. Hence, this sampling procedure does not sample from the intended distribution and is not a valid MCMC algorithm.

cold and other schemes.Alternative approaches have been proposed in the literature to reformulate a language model as potential energy over the logit [cold; 38] or simplex space [14; 20]. However, these formulations are not suitable for principled gradient-based sampling. cold only employs a heuristic energy function to select among the candidate generations obtained via top-\(k\) sampling, and the simplex-based approach requires an extra constraint to ensure the sample stays on the simplex.

## 6 Structured Voronoi Sampling

Given our structured Voronoi measure \(p_{}\), one can apply hmc to sample from it. In this section, we take one step further and propose a variation of hmc that is more suitable to sample from \(p_{}\). Importantly, \(p_{}\) contains discontinuities whereas the generic leapfrog integrator does not account for such sudden jumps in the potential function. In other words, even if the leapfrog integrator itself is volume preserving and time-reversible, the sudden jumps in potential can lead to large deviations in the Hamiltonian value, causing a low acceptance rate. We therefore would like to find an alternative to leapfrog in such situations.

A Physical Analogy.In classical mechanics, discontinuity with smooth boundary in the potential function occurs naturally, e.g., in collision dynamics or a slab magnetic field, and is referred to as a potential barrier (or interface). Upon encountering a potential barrier, a particle will either be reflected from or transmitted through the barrier surface, depending on whether it has enough kinetic energy to overcome the potential jump (e.g., [8; SS4.6.2]). Such behavior is similar to reflection-refraction phenomenon in optics. The key insight here is that, in both cases, the Hamiltonian is conserved.

Reflection and Refraction.To give a precise mechanical description of this behavior, suppose a particle encounters a potential barrier at position \(\) with momentum \(\). We can decompose momentum as \(=_{}+_{}\) where \(_{}\) is normal to the barrier and \(_{}\) parallel to it. Let \( U\) be the signed potential energy difference between the two sides of the barrier. If \(\|_{}\|_{2}^{2}>2 U\), then the particle has enough kinetic energy to overcome the barrier, and its momentum's normal component will instantaneously become \(_{}^{}=_{}\|_{2}^{2}-2 U} _{}}{\|_{}\|_{2}^{2}}\) after being transmitted through the barrier (refraction). Otherwise, if \(\|_{}\|_{2}^{2} 2 U\), the particle will be reflected from the barrier and the normal component will instantaneously become \(_{}^{}=-_{}\). We show in App. E.1 that Hamiltonian is conserved in either case. The reflect-refract process is summarized in Algorithm 5.

### A Sampling Algorithm

Noting that the discontinuity surfaces of \(p_{}\) are all piecewise smooth, we can build on the above and develop a sampling algorithm for \(p_{}\) to handle discontinuity in a principled and effective way. In fact, we only need to make one change to the generic hmc, which is updates \((,)\) according to the mechanical description given above. Concretely, we need to replace step 2 in the hmc outline in SS5.1: When a single step of leapfrog encounters no discontinuity, we may advance to the next point as in hmc; however, when there is discontinuity, if a full leapfrog step is taken, we need to proceed by repeatedly computing where the discontinuity is encountered, taking a smaller step up to the discontinuity and refracting-reflecting based on the potential energy difference. This process is continued until we have exhausted the step size. Since refracting-reflecting conserves Hamiltonian (App. E.1), this process yields a better acceptance rate in the presence of a discontinuity. See Algorithm 4 for the details of this sampling procedure, and App. F.1 for how to efficiently find discontinuities. We will supply proof of correctness in App. E.2.

A note on calculating the base measure.To adjust the momentum, one needs to compute \( U\), which implies computing the difference between two base measures, as defined in Eq. (12). However, computing such an integral in a high-dimensional space is not practical. Therefore, make an assumption that the base measures of Voronoi cells are equal, and thus do not have an effect on \( U\). However, such an assumption might not hold. See App. A for limitations.

## 7 Experiments

We empirically assess the performance of Voronoi sampling in a series of experiments. In each experiment, we perform a grid search to find the best set of hyperparameters, these experimental details can be found in App. H. Our open-source implementation will be available upon publication.

### Toy Example

We first apply our Voronoi sampling8 method on the toy example discussed earlier in Example 1, where the reference probability distribution is tractable and known \(p()\). The potential energy is then set to \(U()=- p_{}()\). Importantly, the toy experiment is intentionally designed such that the base measure of all of the Voronoi cells is equal, therefore, we can safely ignore calculating the base measure and arrive at exact sampling methods.

We compare Voronoi sampling to MuCoLa and hmc. To make a fair comparison, we add the Metropolis criterion9 to the MuCoLa algorithm and only do one leapfrog step in all algorithms. Furthermore, to see the effect of the reference distribution on the performance of the sampling algorithm, we anneal this distribution with 6 temperatures, where the lower temperatures lead to peaky distributions and the higher temperatures to uniform-like distributions.

We take \(200\) samples after \(500\) burn-in iterations, and compare the empirical distributions of samples to the reference by measuring the Jensen-Shannon (JS) divergence between the two. As results in Fig. 2 show, the JS divergence between the reference distribution and the empirical distribution of Voronoi samples is the smallest. The difference between the methods is more pronounced at lower temperatures, as the change in potential energy is greater, resulting in more errors in the leapfrog integrator. In App. I we provide more empirical support that Voronoi sampling converges faster to the reference distribution, especially when we increase the dimensionality of the sampling space.

### Sampling from Language Models

Next, we apply our method to sample from a language model. The underlying LM is a finetuned GPT-20 on E2E dataset ; see App. G for dataset statistics. As opposed to the previous experiment, the reference distribution of the LM is not tractable. Therefore, we use the empirical distribution of ancestral samples as an unbiased estimate of the reference distribution. Note that ancestral sampling incrementally draws samples from a language model, where at each step of the generation a token \(w_{n}\) is sampled with the probability given by the LM: \(p(w_{n}_{<n})\). Therefore, the process can give unbiased estimates of the reference distribution.

We follow SS4 to define a structured Voronoi measure \(p_{}\) using the LM probability. We then implement svs, where the potential energy is set to \(- p_{}\). To empirically measure the benefit of reflection-refraction step in svs, we compare it to applying Langevin dynamics directly to \(p_{}\). We implement MuCoLa as a baseline, which writes the potential energy using an embedding-augmented LM, i.e., Eq. (3).

We show the distribution of samples' perplexity in Fig. 3. The green trace is the empirical distribution of \(1000\) ancestral samples. While all the sampling methods result in distributions comparably close to ancestral samples' distribution, we observe that svs manages to model the tail of the distribution better. On the other hand, MuCoLa and langevin tend to take samples from the mode of the distribution more often.

### Controlled Generation

Finally, we apply our structured Voronoi sampling to \(2\) controlled generation task. The goal of the first task is to generate restaurant reviews for a target food type \(t\), e.g., Italian, Fast food, Japanese, etc. The goal of the second task is to control the sentiment of the generations to enforce a positive sentiment. We train classifiers to predict the target \(t\) (food type or positive sentiment) from the input sequence \(p(t)\).11 We implement two baselines:

fudge.Yang and Klein  offer a heuristic approach to sample from the conditional distribution. They incrementally sample tokens under the language model. At each sampling step, they adjust the probabilities given by the LM, by feeding each candidate prefix to a classifier and obtain the probability of that prefix following the control target.

MuCoLa.Kumar et al.  treat \(p( t)\), Eq. (4), as a distribution in \(^{d}\) and apply Langevin dynamics directly to sample a sequence of embeddings \(\). The potential energy is defined as \(- p( t)\). When rewriting this potential energy with Bayes' rule, it has been shown empirically, that adding a hyperparameter \(\) is helpful to keep the balance between the classifier and LM. Therefore, the final potential energy is defined as:

\[U()}}{{=}}- p()-  p(t).\] (17)

As mentioned earlier, \(p( t)\) only places a positive probability on a countable set. We, therefore, use Def. 3 to define structured Voronoi measures and set the potential energy to

\[U()}}{{=}}- p_{}( )- p_{}(t).\] (18)

We then apply Langevin dynamics and svs to sample according to this potential energy.

Evaluation.We sample \(120\) sentences of length \(20\)12 and evaluate the generations on three metrics:

* **Success:** is defined as the percentage of generations that adhere to the control target. To determine whether a generation conforms to the specified target we use an _evaluator classifier_.
* **Fluency:** is measured by the mean and standard deviation of perplexity under the language model.
* **Diversity:** is measured by the mean number of distinct \(n\)-grams (\(n=1,2,3\)) in a set of samples, normalized by the length of the sequence.

As results in Table 1 show,13 fudge tends to achieve the highest diversity, however, it fails to follow the control target. MuCoLa either generates fluent results without paying enough attention to the control, or sacrifices fluency in favor of following the control target; thus, the high variance in success rates. Both langevin and svs result in a high success rate and maintain fluency and diversity, and svs is effective in maintaining a balance between various metrics and producing fluent sentences that adhere to control targets.

## 8 Related Work

Controlled Generation.Numerous approaches have been proposed to enforce controls during the text generation process [19; 24; 41], _inter alia_]. For example, weighted decoding [10; 17] scores each candidate token with a weighted sum of its score under the language model and its adherence to control targets, subsequently selecting candidates with the highest scores. fudge method adopts a similar scoring function, resembling a Bayesian formulation for \(p( t)\). After making simplifying assumptions and factorizing \(p( t)\), fudge samples tokens autoregressively based on their scores. More recently, a line of research attempts to directly sample from \(p( t)\) by reformulating it as an energy-based model and sampling from it using efficient gradient-based sampling algorithms. As discussed in SS5.2 cold reformulates \(p( t)\) as an energy-based model on the logit space and uses that to select samples with high energy from a number of candidate generations. MuCoLa offers a sampling algorithm motivated by Langevin Dynamics that operates in the embedding space.

Gradient-based Sampling.Our work is closely related to the line of research that makes use of gradient information to sample from complex distributions [7; 31; 46]. Gradient-based samplers [15; 32] are shown to be highly effective when sampling from continuous distributions [3; 4; 37]. However, it is a difficult problem to adapt gradient-based samplers to discrete settings [36; 50]. More recently, several papers proposed promising gradient-based MCMC for discrete distribution that are reversible chains [11; 39; 49]. Our work instead formulates an irreversible Markov chain based on HMC. We leave it to future work to explore the utility of these recent papers on text generation.

## 9 Conclusion

In this work, we propose structured Voronoi sampling, a principled gradient-based sampling method for text generation. To formulate the energy function used in svs, we define structured Voronoi measures on the embedding space and show how such measures can encode language models. In a controlled generation task, svs outperformed other sampling methods in following the control target while producing comparably fluent and diverse samples.

## Broader Impacts

It has been repeatedly shown that LMs can generate harmful, toxic, or non-factual content [9; 35; 42]. In fact, an application of the controlled generation scheme discussed in this paper could be to mitigate such issues. However, the same method could be used to generate misinformation, or toxic content intentionally.