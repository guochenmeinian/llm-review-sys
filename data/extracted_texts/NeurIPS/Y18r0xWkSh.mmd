# Posterior Contraction Rates for Matern Gaussian Processes on Riemannian Manifolds

Paul Rosa

University of Oxford

&Viacheslav Borovitskiy

ETH Zurich

Alexander Terenin

University of Cambridge

and Cornell University

&Judith Rousseau

University of Oxford

###### Abstract

Gaussian processes are used in many machine learning applications that rely on uncertainty quantification. Recently, computational tools for working with these models in geometric settings, such as when inputs lie on a Riemannian manifold, have been developed. This raises the question: can these intrinsic models be shown theoretically to lead to better performance, compared to simply embedding all relevant quantities into \(^{d}\) and using the restriction of an ordinary Euclidean Gaussian process? To study this, we prove optimal contraction rates for intrinsic Matern Gaussian processes defined on compact Riemannian manifolds. We also prove analogous rates for extrinsic processes using trace and extension theorems between manifold and ambient Sobolev spaces: somewhat surprisingly, the rates obtained turn out to coincide with those of the intrinsic processes, provided that their smoothness parameters are matched appropriately. We illustrate these rates empirically on a number of examples, which, mirroring prior work, show that intrinsic processes can achieve better performance in practice. Therefore, our work shows that finer-grained analyses are needed to distinguish between different levels of data-efficiency of geometric Gaussian processes, particularly in settings which involve small data set sizes and non-asymptotic behavior.

## 1 Introduction

Gaussian processes provide a powerful way to quantify uncertainty about unknown regression functions via the formulation of Bayesian learning. Motivated by applications in the physical and engineering sciences, a number of recent papers  have studied how to extend this model class to spaces with geometric structure, in particular Riemannian manifolds including important special cases such as spheres and Grassmannians , hyperbolic spaces and spaces of positive definite matrices , as well as general manifolds approximated numerically by a mesh .

These Riemannian Gaussian process models are starting to be applied for statistical modeling, and decision-making settings such as Bayesian optimization. For example, in a robotics setting, Jaquier et al.  has shown that using Gaussian processes with the correct geometric structure allows one to learn quantities such as the orientation of a robotic arm with less data compared to baselines. The same model class has also been used by Coveney et al.  to perform Gaussian process regression on a manifold which models the geometry of a human heart for downstream applications in medicine.

Given these promising empirical results, it is important to understand whether these learning algorithms have good theoretical properties, as well as their limitations. Within the Bayesian framework, a natural way to quantify data-efficiency and generalization error is to posit a data-generating mechanism model and study if--and how fast--the posterior distribution concentrates around the true regression function as the number of observations goes to infinity.

Within the Riemannian setting, it is natural to compare _intrinsic_ methods, which are formulated directly on the manifold of interest, with _extrinsic_ ones, which require one to embed the manifold within a higher-dimensional Euclidean space. For example, the two-dimensional sphere can be embedded into the Euclidean space \(^{3}\): intrinsic Gaussian processes model functions on the sphere while extrinsic ones model functions on \(^{3}\), which are then restricted to the sphere. Are the former more efficient than the latter? Since embeddings--even isometric ones--at best only preserve distances locally, they can induce spurious dependencies, as points can be close in the ambient space but far away with respect to the intrinsic geodesic distance: this is illustrated in Figure 1. In cases where embeddings significantly alter distances, one can expect intrinsic models to perform better, and it is therefore interesting to quantify such differences.

In other settings, the manifold on which the data lies can be unknown, which makes using intrinsic methods directly no longer possible. There, one would like to understand how well extrinsic methods can be expected to perform. According to the _manifold hypothesis_, it is common for perceptual data such as text and images to concentrate on a lower-dimensional submanifold within, for instance, pixel space or sequence space. It is therefore also interesting to investigate how Gaussian process models--which, being kernel-based, are simpler than for instance deep neural networks--perform in such scenarios, at least in the asymptotic regime.

In this work, we develop geometric analogs of the Gaussian process posterior contraction theorems of van der Vaart and van Zanten . More specifically, we derive posterior contraction rates for three main geometric model classes: (1) the intrinsic Riemannian Matern Gaussian processes, (2) truncated versions of the intrinsic Riemannian Matern Gaussian processes, which are used in practice to avoid infinite sums, and (3) the extrinsic Euclidean Matern Gaussian processes under the assumption that the data lies on a compact Riemannian manifold. In all cases, we focus on IID randomly-sampled input points--commonly referred to as _random design_ in the literature--and contraction in the sense of the \(L^{2}(p_{0})\) distance, defined in Section 2. We focus on _compact_ Riemannian manifolds: this allows one to define Matern Gaussian processes through their Karhunen-Loeve expansions, which requires a discrete spectrum for the Laplace-Beltrami operator--see for instance Borovitskiy et al.  and Chavel , Chapter 1--and is a common setting in statistics .

Contributions.We show that all three classes of Gaussian processes lead to optimal procedures, in the minimax sense, as long as the smoothness parameter of the kernel is aligned with the regularity of the unknown function. While this result is natural--though non trivial--in the case of intrinsic Matern processes, it is rather remarkable that it also holds for extrinsic ones. This means that in order to understand their differences better, finite-sample considerations are necessary. We therefore present experiments that compute the worst case errors numerically. These experiments highlight that intrinsic models are capable of achieving better performance in the small-data regime. We conclude with a discussion of why these results--which might at first seem counterintuitive--are very natural

Figure 1: Samples from different Matern Gaussian processes on different manifolds, namely a one-dimensional dumbbell-shaped manifold and a two-dimensional sphere. Notice that the values across the dumbbellâ€™s bottleneck can be very different for the intrinsic process in (a), despite being very close in the ambient Euclidean distance and in contrast to the situation for the extrinsic model in (b). On the other hand, there is little qualitative difference between (c) and (d), since the embedding produces a reasonably-good global approximation to geodesic distances on the sphere.

when viewed from an appropriate mathematical perspective: they suggest that optimality is perhaps best seen as a basic property or an important guarantee that any sensible model should satisfy.

## 2 Background

_Gaussian process regression_ is a Bayesian approach to regression where the modeling assumptions are \(y_{i}=f(x_{i})+_{i}\), with \(_{i}(0,_{}^{2})\), \(x_{i} X\), and \(f\) is assigned a Gaussian process prior. A _Gaussian process_ is a random function \(f:X\) for which all finite-dimensional marginal distributions are multivariate Gaussian. The distribution of such a process is uniquely determined by its _mean function_\(m()=(f())\) and _covariance kernel_\(k(,^{})=(f(),f(^{}))\), hence we write \(f(m,k)\).

For Gaussian process regression, the posterior distribution given the data is also a Gaussian process with probability kernel \((,)=(m_{(,)},k_{ (,)})\), see Rasmussen and Williams ,

\[m_{(,)}()=_{():}(_{}+_{}^{2})^{-1},\] (1) \[k_{(,)}(,^{})=_ {(,^{})}-_{():}(_{ }+_{}^{2})^{-1}_{(^{})}.\] (2)

These quantities describe how incorporating data updates the information contained within the Gaussian process. We will be interested studying the case where \(X\) is a Riemannian manifold, but first review the existing theory on the asymptotic behaviour of the posterior when \(X=^{d}\).

### Posterior Contraction Rates

Posterior contraction results describe how the posterior distribution concentrates around the true data generating process, as the number of observations increases, so that it eventually uncovers the true data-generating mechanism. The area of _posterior asymptotics_ is concerned with understanding conditions under which this does or does not occur, with questions of _posterior contraction rates_--how fast such convergence occurs--being of key interest. At present, there is a well-developed literature on posterior contraction rates, see Ghosal and van der Vaart  for a review.

In the context of Gaussian process regression with _random design_, which is the focus of this paper, the true data generating process is assumed to be of the form

\[y_{i} x_{i}(f_{0}(x_{i}),_{}^{2}) x_{i} p_{0}\] (3)

where \(f_{0}^{X}\), a class of real-valued functions, and \((,^{2})\) denotes the Gaussian with moments \(,^{2}\). Note that, in this particular variant, these equations exactly mirror those of the Gaussian process model's likelihood, including the use of the same noise variance \(_{}^{2}\) in both cases: in this paper, we focus on the particular case where \(_{}\) is known in advance. This setting is restrictive, one can extend to an unknown \(_{}>0\) using techniques that are not specific to our geometric setting: for instance, the approach of  allows to handle an unknown \(_{}\) if one assumes an upper and lower bound on it and keep the same contraction rates. In practice, more general priors, including ones that do not assume an upper or lower bound on \(_{}\), can be used, such as a conjugate one like in Banerjee --these can also be analyzed to obtain contraction rates, albeit with additional considerations. The generalization error for prediction in such models is strongly related to the _weighted \(L^{2}\) loss_ given by

\[ f-f_{0}_{L^{2}(p_{0})}=(_{X} f(x)-f_{0} (x)^{2}\,p_{0}(x))^{1/2}\] (4)

which is arguably the natural way of measuring discrepancy between \(f\) and \(f_{0}\), given the fact that the covariates \(x_{i}\) are sampled from \(p_{0}\). The posterior contraction rate is then defined as

\[_{,}\,_{f(,)}  f-f_{0}_{L^{2}(p_{0})}^{2}\] (5)

where \(_{f(,)}()\) denotes expectation under the posterior distribution while \(_{,}()\) denotes expectation under the true data generating process.1 In the case of covariates distributed on \(^{d}\), posterior contraction rates have been derived under Matern Gaussian process priors  in van der Vaart and van Zanten , who showed the following result.

**Result 1** (Theorem 2 of van der Vaart and van Zanten ).: _In the Bayesian regression model, let \(f\) be a mean-zero Matern Gaussian process prior on \(^{d}\) with amplitude \(_{f}^{2}\), length scale \(\), and smoothness \(>d/2\). Assume that the true data generating process is given by (3), where \(p_{0}\) has a Lebesgue density on \(X=^{d}\) which is bounded from below and above by \(0<c_{p_{0}}<C_{p_{0}}<\), respectively. Let \(f_{0} H^{}^{}\) with \(>d/2\), where \(H^{}\) and \(^{}\) the Sobolev and Holder spaces, respectively. Then there exists a constant \(C>0\), which does not depend on \(n\) but does depend on \(d\), \(_{f}^{2}\), \(\), \(\), \(\), \(p_{0}\), \(_{e}^{2}\), \(\|f_{0}\|_{H^{}()}\), and \(\|f_{0}\|_{^{}()}\), such that_

\[_{,}\,_{f(|,)}\|f-f_{0 }\|_{L^{2}(p_{0})}^{2} Cn^{-}\] (6)

_and, moreover, the posterior mean satisfies_

\[_{,}\,\|m_{(|,)}-f_{0}\|_{L^{2}(p_{ 0})}^{2} Cn^{-}.\] (7)

Note that \(m_{(|,)}\) is the Bayes estimator  of \(f\) associated to the weighted \(L^{2}\) loss and that the second inequality above is a direct consequence of the first. Therefore the posterior contraction rate implies the same convergence rate for \(m_{(|,)}\). The best rate is attained when \(=\): that is, when true smoothness and prior smoothness match--which is known to be minimax optimal in the problem of estimating \(f_{0}\): see Tsybakov . In this paper, we extend this result to the manifold setting.

### Related Work and Current State of Affairs

The formalization of posterior contraction rates of Bayesian procedures dates back to the work of Schwartz  and Le Cam , but has been extensively developed since the seminal paper of Ghosal et al.  for various sampling and prior models, see for instance [20; 42] for reviews. This includes, in particular, work on Gaussian process priors [54; 56; 57; 43; 49]. Most of the results in the literature, however, assume Euclidean data: as a consequence, contraction properties of Bayesian models under manifold assumptions are still poorly understood, with exception of some recent developments in both density estimation [7; 8; 60] and regression [63; 60].

The results closest to ours are those of Yang and Dunson  and Castillo et al. . In the former, the authors use an extrinsic length-scale-mixture of squared exponential Gaussian processes to achieve optimal contraction rates with respect to the weighted \(L^{2}\) norm, using a completely different proof technique compared to us, and their results are restricted to \(f_{0}\) having Holder smoothness of order less than or equal to two. On the other hand Castillo et al.  consider, as an intrinsic process on the manifold, a hierarchical Gaussian process based on its heat kernel and provide posterior contraction rates. For the Matern class, Li et al.  presents results which characterize the asymptotic behavior of kernel hyperparameters: our work complements these results by studying contraction of the Gaussian process itself toward the unknown ground-truth function. One can also study analogous discrete problems: Dunson et al.  and Sanz-Alonso and Yang  present posterior contraction rates for a specific graph Gaussian process model in a semi-supervised setting. In the next section, we present our results on Matern processes, defined either by restriction of an ambient process or by an intrinsic construction, and discuss their implications.

## 3 Posterior Contraction Rates on Compact Riemannian Manifolds

We now study posterior contraction rates for Matern Gaussian processes on manifolds, which are arguably the most-widely-used Gaussian process priors in both the Euclidean and Riemannian settings. We begin by more precisely describing our geometric setting before stating our key results and discussing their implications. From now on, we write \(X=\), to emphasize that the covariate space is a manifold.

**Assumption 2**.: _Assume that \(^{D}\) is a smooth, compact submanifold (without boundary) of dimension \(d<D\) equipped with the standard Riemannian volume measure \(\)._

We denote \(||=_{}d(x)\) for volume of \(\). With this geometric setting defined, we will need to describe regularity assumptions in terms of functional spaces on the manifold \(\). We work with Holder spaces \(^{}()\), defined using charts via the usual Euclidean Holder spaces, the Sobolev spaces \(H^{s}()\), and Besov spaces \(B^{s}_{,}()\) which are one of the ways of generalizingthe Euclidean Holder spaces of smooth functions to manifolds. We follow Coulhon et al.  and Castillo et al. , and define these spaces using the Laplace-Beltrami operator on \(\) in Appendix A.

Recall that the data-generating process is given by (3), with \(f_{0}\) as the true regression function and \(p_{0}\) as the distribution of the covariates.

**Assumption 3**.: _Assume that \(p_{0}\) is absolutely continuous with respect to \(\), and that its density, denoted by \(p_{0}\), satisfies \(c p_{0} C\) for \(0<c,C<\). Assume the regression function \(f_{0}\) satisfies \(f_{0} H^{}() B^{}_{,}()\) for some \(>d/2\), and that \(_{}^{2}>0\) is fixed and known._

This setting can be extended to handle unknown variance \(_{}\) by putting a prior on \(_{}\), following the strategy of Salomond  and Naulet and Barat . Since we are focused primarily on the impact of the manifold, we do not pursue this here. With the setting fully defined, we proceed to develop posterior contraction results for different types of Matern Gaussian process priors: intrinsic, intrinsic truncated and extrinsic.

### Intrinsic Matern Gaussian Processes

We now introduce the first geometric Gaussian process prior under study--the Riemannian Matern kernel of Whittle , Lindgren et al. , and Borovitskiy et al. . This process was originally defined using stochastic partial differential equations: here, we present it by its Karhunen-Loeve expansion, to facilitate comparisons with its truncated analogs presented in Section 3.2.

**Definition 4** (Intrinsic Matern prior).: _Let \(>0\), and let \((_{j},f_{j})_{j 0}\) be the eigenvalues and orthonormal eigenfunctions of the Laplace-Beltrami operator on \(\), in increasing order. Define the intrinsic Riemannian Matern Gaussian process through its Karhunen-Loeve expansion to be_

\[f()=^{2}}{C_{,}}_{j=1}^{} }+_{j}^{-}_{j}f_{j}() _{j}(0,1)\] (8)

_where \(,,_{f}^{2}\) are positive parameters and \(C_{,}\) is the normalization constant, chosen such that \(|}_{M}(f(x))\,(x)=_{f} ^{2}\), where \(\) denotes the variance._

The covariance kernels of these processes are visualized in Figure 2. With this prior, and the setting defined in Section 3, we are ready to present our first result: this model attains the desired optimal posterior contraction rate as soon as the regularity of the ground-truth function matches the regularity of the Gaussian process, as described by the parameter \(\).

**Theorem 5**.: _Let \(f\) be a Riemannian Matern Gaussian process prior of Definition 4 with smoothness parameter \(>d/2\) and let \(f_{0}\) satisfy Assumption 3. Then there is a \(C>0\) such that_

\[_{,}\,_{f(,)}\|f- f_{0}\|_{L^{2}(p_{0})}^{2} Cn^{-}.\] (9)

All proofs are given in Appendix B. Our proof follows the general approach of van der Vaart and van Zanten , by first proving a contraction rate with respect to the distance \(n^{-1/2}\|f()-f_{0}()\|_{^{n}}\) at input locations \(\), and then extending the result to the true \(L^{2}\)-distance by applying a suitable concentration inequality. The first part is obtained by studying the _concentration function_, which is known to be the key quantity to control in order to derive contraction rates of Gaussian process priors--see Ghosal and van der Vaart  and van der Vaart and van Zanten  for an overview.

Figure 2: Different Matern kernels \(k(,x)\) on different manifolds.

Given our regularity assumptions on \(f_{0}\), the most difficult part lies in controlling the small-ball probabilities \([\|f\|_{()}<]\): we handle this by using results relating this quantity with the entropy of an RKHS unit ball with respect to the uniform norm. Since our process' RKHS is related to the Sobolev space \(H^{+d/2}()\) which admits a description in terms of charts, we apply results on the entropy of Sobolev balls in the Euclidean space to conclude the first part. Finally, to extend the rate to the true \(L^{2}(p_{0})\) norm, following van der Vaart and van Zanten , we prove a Holder-type property for manifold Matern processes, and apply Bernstein's inequality. Together, this gives the claim.

This result is good news for the intrinsic Matern model: it tells us that asymptotically it incorporates the data as efficiently as possible at least in terms of posterior contraction rates, given that its regularity matches the regularity of \(f_{0}\). An inspection of the proof shows that the constant \(C>0\) can be seen to depend on \(d\),\(_{f}^{2}\), \(\), \(\), \(\), \(p_{0}_{}^{2}\), \(\|f_{0}\|_{H^{}()}\), \(\|f_{0}\|_{B^{}_{}()}\), and \(\|f_{0}\|_{^{}()}\). Theorem 5 extends to the case where the norm is raised to any power \(q>1\) rather than the second power, with the right-hand side raised to the same power: see Appendix B for details. We now consider variants of this prior that can be implemented in practice.

### Truncated Matern Gaussian Processes

The Riemannian Matern prior's covariance kernel cannot in general be computed exactly, since Definition 4 involves an infinite sum. Arguably the simplest way to implement these processes numerically is to truncate the respective infinite series in the Karhunen-Loeve expansion by taking the first \(J\) terms, which is also optimal in an \(L^{2}()\)-sense.

Note that the truncated prior is a randomly-weighted finite sum of Laplace-Beltrami eigenfunctions, which have different smoothness properties compared to the original prior: the truncated prior takes its values in \(^{}()\) since the eigenfunctions of \(\) are smooth--see for instance De Vito et al. . Nevertheless, if the truncation level is allowed to grow as the sample size increases, then the regularity of the process degenerates and one gets a function with essentially-finite regularity in the limit.

Truncated random basis expansions have been studied extensively in the Bayesian literature in the Euclidean setting--see for instance Arbel et al.  and Yoo et al.  or Ghosal and van der Vaart , Chapter 11 for examples with priors based on wavelet expansions. It is known that truncating the expansion at a high enough level usually allows one to retain optimality. Instead of truncating deterministically, it is also possible to put a prior on the truncation level and resort to MCMC computations which would then select the optimal number of basis functions adaptively, at the expense of a more computationally intensive method--this is done, for instance, in van der Meulen et al.  in the context of drift estimation for diffusion processes. Random truncation has been proven to lead in many contexts to adaptive posterior contraction rates, meaning that although the prior does not depend on the smoothness \(\) of \(f_{0}\), the posterior contraction rate--up to possible \( n\) terms--is of order \(n^{-/(2+d)}\): see for instance Arbel et al.  and Rousseau and Szabo .

By analogy of the Euclidean case with its random Fourier feature approximations , we can call the truncated version of Definition 4 the _manifold Fourier feature_ model, for which we now present our result.

**Theorem 6**.: _Let \(f\) be a Riemannian Matern Gaussian process prior on \(\) with smoothness parameter \(>d/2\), modified to truncate the infinite sum to at least \(J_{n} cn^{}\) terms, and let \(f_{0}\) satisfy Assumption 3. Then there is a \(C>0\) such that_

\[_{,}\,_{f(| ,)}\|f-f_{0}\|_{L^{2}(p_{0})}^{2} Cn^{-}.\] (10)

The proof is essentially-the-same as the non-truncated Matern, but involves tracking dependence of the inequalities on the truncation level \(J_{n}\), which implicitly defines a sequence of priors rather than a single fixed prior.

This result is excellent news for the intrinsic models: it means that they inherit the optimality properties of the limiting one, even in the absence of the infinite sum--in spite of the fact that the corresponding finite-truncation prior places its probability on \(^{}()\). Again, the constant \(C>0\) can be seen to depend on \(d\),\(_{f}^{2}\), \(\),\(\),\(\),\(p_{0}\),\(_{}^{2}\), \(\|f_{0}\|_{H^{}()}\), \(\|f_{0}\|_{B^{}_{}()}\), and \(\|f_{0}\|_{^{}()}\). This concludes our results for the intrinsic Riemannian Matern priors. We now study what happens if, instead of working with a geometrically-formulated model, we simply embed everything into \(^{d}\) and formulate our models there.

### Extrinsic Matern Gaussian Processes

The results of the preceding sections provide good reason to be excited about the intrinsic Riemannian Matern prior: the rates it obtains match the usual minimax rates seen for the Euclidean Matern prior and Euclidean data, provided that we match the smoothness \(\) with the regularity of \(f_{0}\). Another possibility is to consider an extrinsic Gaussian process, that is, a Gaussian process defined over an ambient space. This has been considered by Yang and Dunson  for instance for the square-exponential process, in an adaptive setting where one does not assume that the regularity \(\) of \(f_{0}\) is explicitly known, but where \( 2\). In this section we prove a non-adaptive analog of this result for the Matern process.

**Definition 7** (Extrinsic Matern prior).: _Assume that the manifold \(\) is isometrically embedded in the Euclidean space \(^{D}\), such that we can regard \(\) as a subset of \(^{D}\). Consider the Gaussian process with zero mean and kernel given by restricting onto \(\) the standard Euclidean Matern kernel_

\[k_{,,_{f}^{2}}(x,x^{})=_{f}^{2}}{ ()}\|_{^ {D}}}{}^{}K_{} \|_{^{D}}}{}\] (11)

_where \(_{f},,>0\) and \(K_{}\) is the modified Bessel function of the second kind ._

Since the extrinsic Matern process is defined in a completely agnostic way with respect to the manifold geometry, we would expect it to be less performant when \(\) is known. However, it turns out that the extrinsic Matern process converges at the same rate as the intrinsic one, as given in the following claim.

**Theorem 8**.: _Let \(f\) be a mean-zero extrinsic Matern Gaussian process prior with smoothness parameter \(>d/2\) on \(\), and let \(f_{0}\) satisfy Assumption 3. Then for some \(C>0\) we have_

\[_{,}\,_{f(| ,)}\|f-f_{0}\|_{L^{2}(p_{0})}^{2} Cn^{-}.\] (12)

Theorem 8 is a surprising result because the optimal rates in this setting only require the knowledge of the regularity \(\), but not the knowledge of the manifold or the intrinsic dimension. More precisely, the prior is not designed to be an adaptive prior, since it is a fixed Gaussian process, but it surprisingly adapts to the dimension of the manifold, and thus to the manifold.

The proof is also based on control of concentration functions. The main difference is that, although the ambient process has a well known RKHS--the Sobolev space \(H^{s+D/2}(^{D})\)--the restricted process has a non-explicit RKHS, which necessitates further analysis. We tackle this issue by using results from Grosse and Schneider  relating manifold and ambient Sobolev spaces by linear bounded trace and extension operators, and from Yang and Dunson  describing a general link between the RKHS of an ambient process and its restriction. This allows us to show that the restricted process has an RKHS that is actually norm-equivalent to the Sobolev space \(H^{+d/2}()\), which allows us to conclude the result in the same manner as in the intrinsic case.

As consequence, our argument applies _mutatis mutandis_ in any setting where suitable trace and extension theorems apply, with the Riemannian Matern case corresponding to the usual Sobolev results. In particular, our arguments therefore apply directly to other processes possessing similar RKHSs, such as for instance various kernels defined on the sphere--see e.g. Wendland , Chapter 17 and Hubbert et al. . The constant \(C>0\) can be seen to depend on \(d\),\(D\),\(_{f}^{2}\), \(\),\(\),\(\),\(p_{0}\),\(_{}^{2}\), \(\|f_{0}\|_{H^{}()}\),\(\|f_{0}\|_{B^{}_{}()}\),\(\|f_{0}\|_{^{}()}\)--notice that here \(C\) depends implicitly on \(D\) because of the presence of trace and extension operator continuity constants. We now proceed to understand the significance of the overall results.

### Summary of Results

As a consequence of our previous results, fixing a single common data generating distribution determined by \(p_{0},f_{0}\), under suitable conditions the intrinsic Matern process, its truncated version, and the extrinsic Matern process all possess the _same_ posterior contraction rate with respect to the \(L^{2}(p_{0})\)-norm, which depends on \(d\), \(\), and \(\), and is optimal if the regularities of \(f_{0}\) and the prior match. These results imply the following immediate corollary, which follows by convexity of \(\|\|_{L^{2}(p_{0})}^{2}\) using Jensen's inequality.

**Corollary 9**.: _Under the assumptions of Theorems 5, 6 and 8, it follows that, for some \(C>0\)_

\[_{,}\|m_{(|,)}-f_{0}\|_{L^ {2}(p_{0})}^{2} Cn^{-}\] (13)

_where \(m_{(|,)}\) is the posterior mean given a particular value of \((x_{i},y_{i})_{i=1}^{n}\)._

When \(=\), the optimality of the rates we present in the manifold setting can be easily inferred by lower bounding the \(L^{2}\)-risk of the posterior mean by the \(L^{2}\)-risk over a small subset of \(\) and using charts, which translates the problem into the Euclidean framework for which the rate is known to be optimal--see for instance Tsybakov .

To contextualize this, observe that even in cases where the geometry of the manifold is non-flat, the asymptotic rates are unaffected by the choice of the prior's length scale \(\)--in either the intrinsic, or the extrinsic case--but only by the smoothness parameter \(\). Indeed, the RKHS of the process is only determined--up to norm equivalence--by \(\), which plays an important role in the proofs. This, and the fact that extrinsic processes attain the same rates, implies that the study of asymptotic posterior contraction rates _cannot detect geometry_ in our setting, as was already hinted by Yang and Dunson . Hence, in the geometric setting, optimal posterior contraction rates should be thought of more as a basic property that any reasonable model should satisfy. Differences in performance will be down to constant factors alone--but as we will see, these can be significant. To understand these differences, we turn to empirical analysis.

## 4 Experiments

From Theorems 5, 6 and 8, we know that intrinsic and extrinsic Gaussian processes exhibit the same posterior contraction rates in the asymptotic regime. Here, we study how these rates manifest themselves in practice, by examining how worst-case errors akin to those of Corollary 9 behave numerically. Specifically, we consider the pointwise worst-case error

\[v^{()}(t)=_{\|f_{0}\|_{^{+d/2}} 1}_{_{ i}(0,_{}^{2})}m_{(|,)} ^{()}(t)-f_{0}(t)^{2}\] (14)

where \(m_{(|,)}^{()}\) is the posterior mean corresponding to the zero-mean Matern Gaussian process prior with smoothness \(\), length scale \(\), amplitude \(_{f}^{2}\), which is intrinsic if \(=\) or extrinsic if \(=\). We use a Gaussian likelihood with noise variance \(_{}^{2}\) and observations \(y_{i}=f_{0}(x_{i})+_{i}\), and examine this quantity as a function of the evaluation location \(t\). By allowing us to assess how error varies in different regions of the manifold, this provides us with a fine-grained picture of how posterior contraction behaves.

One can show that \(v^{()}\) may be computed without numerically solving an infinite-dimensional optimization problem. Specifically, (14) can be calculated, in the respective intrinsic and extrinsic cases, using

\[v^{()}(t) =k^{()}(t,t)-^{()}_{ }^{()}_{}+_{ }^{2}^{-1}^{()}_{ }\] (15) \[v^{()}(t) (^{()}_{^{ }}-_{t}^{()}_{^{}})( ^{()}_{^{}^{}})^{-1}( ^{()}_{^{}}-^{( )}_{^{}}_{t}^{})+_{ }^{2}_{t}_{t}^{}\] (16)

where, for the extrinsic case, \(_{t}=^{()}_{}(^ {()}_{}+_{}^{2})^{-1}\), and \(^{}\) is a set of points sampled uniformly from the manifold \(\), the size of which determines approximation quality. The intrinsic expression is simply the posterior variance \(k^{()}_{(|,)}(t,t)\), and its connection with worst-case error is a well-known folklore result mentioned somewhat implicitly in, for instance, Mutny and Krause . The extrinsic expression is very-closely-related, and arises by numerically approximating a certain RKHS norm. A derivation of both is given in Appendix F. To assess the approximation error of this formula, we also consider an analog of (16) but instead defined for the intrinsic model, and compare it to (15): in all cases, the difference between the exact and approximate expression was found to be smaller than differences between models. By computing these expressions, we therefore obtain, up to numerics, the pointwise worst-case expected error in our regression model.

For \(\) we consider three settings: a dumbbell-shaped manifold, a sphere, and the dragon manifold from the Stanford 3D scanning repository. In all cases, we perform computations by approximatingthe manifold using a mesh, and implementing the truncated Karhunen-Loeve expansion with \(J=500\) eigenpairs obtained from the mesh. We fix smoothness \(=\), amplitude \(_{f}^{2}=1\), and noise variance \(_{}^{2}=0.0005\), for both the intrinsic and extrinsic Matern Gaussian processes. Since the interpretation of the length scale parameter is manifold-specific, for the intrinsic Gaussian processes we set \(=200\) for the dumbbell, \(=0.25\) for the sphere, and \(=0.05\) for the dragon manifold. In all cases, this yielded functions that were neither close to being globally-constant, nor resembled noise. Each experiment was repeated \(10\) times to assess variability. Complete experimental details are given in Appendix G.2

The length scales \(\) are defined differently for intrinsic and extrinsic Matern kernels: in particular, using the same length scale in both models can result in kernels behaving very differently. To alleviate this, for the extrinsic process, we set the length scale by maximizing the extrinsic process' marginal likelihood using the full dataset generated by the intrinsic process, except in the dumbbell's case where the full dataset is relatively small, and therefore a larger set of 500 points was used instead. This allows us to numerically match intrinsic and extrinsic length scales to ensure a reasonably-fair comparison.

Figure 3 shows the mean, and spatial standard deviation of \(v_{}(t)\), where by _spatial standard deviation_ we mean the sample standard deviation computed with respect to locations in space, rather than with respect to different randomly sampled datasets. From this, we see that on the dumbbell and dragon manifold--whose geometry differs significantly from the respective ambient Euclidean spaces--intrinsic models obtain better mean performance. The standard deviation plot reveals that intrinsic models have errors that are less-variable across space. This means that extrinsic models exhibit higher errors in some regions rather than others--such as, for instance, regions where embedded Euclidean and Riemannian distances differ--whereas in intrinsic models the error decays in a more spatially-uniform manner.

In contrast, on the sphere, both models perform similarly. Moreover, both the mean and spatial standard deviation decrease at approximately the same rates, indicating that the extrinsic model's predictions are correct about-as-often as the intrinsic model's, as a function of space. This confirms the view that, since the sphere does not possess any bottleneck-like areas where embedded Euclidean distances are extremely different from their Riemannian analogs, it is significantly less affected by differences coming from embeddings.

Figure 3: Worst-case error estimates for the intrinsic and extrinsic processes, on the _dumbbell_, _sphere_, and _dragon_ manifolds (lower is better, \(y\) axis is in the logarithmic scale). We see that, on the dumbbell and dragon manifold, intrinsic models achieve lower expected errors than extrinsic models for the ranges considered (top), and that their expected error consistently varies less as a function of space (bottom). In contrast, on the sphere, both models achieve similar performance, with differences between models falling within the range of variability caused by different random number seeds. We also see that the difference between computing the pointwise worst-case error exactly and approximately, in the intrinsic case where computing this difference is possible, is small in all cases.

In total, our experiments confirm that there are manifolds on which geometric models can perform significantly better than non-geometric models. This phenomenon was also noticed in Dunson et al. , where a prior based on the eigendecomposition of a random geometric graph, which can be thought as an approximation of our intrinsic Matern processes, is compared to a standard extrinsic Gaussian process. In our experiments, we see this through expected errors, mirroring prior results on Bayesian optimization performance. From our theoretical results, such differences cannot be captured through posterior contraction rates, and therefore would require sharper technical tools, such as non-asymptotic analysis, to quantify theoretically.

## 5 Conclusion

In this work, we studied the asymptotic behavior of Gaussian process regression with different classes of Matern processes on Riemannian manifolds. By using various results on Sobolev spaces on manifolds we derived posterior contraction rates for intrinsic Matern process defined via their Karhunen-Loeve decomposition in the Laplace-Beltrami eigenbasis, including processes arising from truncation of the respective sum which can be implemented in practice. Next, using trace and extension theorems which relate manifold and Euclidean Sobolev spaces, we derived similar contraction rates for the restriction of an ambient Matern process in the case where the manifold is embedded in Euclidean space. These theoretical asymptotic results were supplemented by experiments on several examples, showing significant differences in performance between intrinsic and extrinsic methods in the small sample size regime when the manifold's geometric structure differs from the ambient Euclidean space. Our work therefore shows that capturing such differences cannot be done through asymptotic contraction rates, motivating and paving the way for further work on non-asymptotic error analysis to capture empirically-observed differences between extrinsic and intrinsic models.