# Human-Aware Vision-and-Language

Navigation: Bridging Simulation to Reality with

Dynamic Human Interactions

Heng Li\({}^{1*}\), Minghan Li\({}^{1*}\), Zhi-Qi Cheng\({}^{1*}\), Yifei Dong\({}^{2}\),

**Yuxuan Zhou\({}^{3}\), Jun-Yan He\({}^{4}\), Qi Dai\({}^{5}\), Teruko Mitamura\({}^{1}\), Alexander G. Hauptmann\({}^{1}\)**

\({}^{1}\)Carnegie Mellon University \({}^{2}\)Columbia University

\({}^{3}\)University of Mannheim \({}^{4}\)Alibaba Group \({}^{5}\)Microsoft Research

Project Page: https://lpercc.github.io/HA3D_simulator/

Equal contribution, authors listed in random order. \({}^{}\)Corresponding author. See Author Contributions section for detailed roles (Sec. 5).

###### Abstract

_Vision-and-Language Navigation_ (VLN) aims to develop embodied agents that navigate based on human instructions. However, current VLN frameworks often rely on static environments and optimal expert supervision, limiting their real-world applicability. To address this, we introduce _Human-Aware Vision-and-Language Navigation_ (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. We propose the _Human-Aware 3D_ (HA3D) simulator, which combines dynamic human activities with the Matterport3D dataset, and the _Human-Aware Room-to-Room_ (HA-R2R) dataset, extending R2R with human activity descriptions. To tackle HA-VLN challenges, we present the _Expert-Supervised Cross-Modal_ (VLN-CM) and _Non-Expert-Supervised Decision Transformer_ (VLN-DT) agents, utilizing cross-modal fusion and diverse training strategies for effective navigation in dynamic human environments. A comprehensive evaluation, including metrics considering human activities, and systematic analysis of HA-VLN's unique challenges, underscores the need for further research to enhance HA-VLN agents' real-world robustness and adaptability. Ultimately, this work provides benchmarks and insights for future research on embodied AI and _Sim2Real transfer_, paving the way for more realistic and applicable VLN systems in human-populated environments.

## 1 Introduction

The dream of autonomous robots carrying out assistive tasks, long portrayed in _"The Simpsons,"_ is becoming a reality through embodied AI, which enables agents to learn by interacting with their environment . However, effective _Sim2Real_ transfer remains a critical challenge . Vision-and-Language Navigation (VLN)  has emerged as a key benchmark for evaluating Sim2Real transfer , showing impressive performance in simulation . Nevertheless, many VLN frameworks  rely on simplifying assumptions, such as _static environments_, _panoramic action spaces_, and _optimal expert supervision_, limiting their real-world applicability and often leading to an overestimation of Sim2Real capabilities .

To bridge this gap, we propose _Human-Aware Vision-and-Language Navigation_ (HA-VLN), extending traditional VLN by incorporating _dynamic human activities_ and _relaxing key assumptions_. HA-VLN advances previous frameworks by (1) adopting a limited 60\({}^{}\) field-of-view egocentric action space, (2) integrating dynamic environments with 3D human motion models encoded using the SMPL model , and (3) learning to navigate considering dynamic environments from suboptimal expertdemonstrations through an adaptive policy (Fig. 6). This setup creates a more realistic and challenging scenario, enabling agents to navigate in human-populated environments while maintaining safe distances, narrowing the gap between simulation and real-world scenes.

To support HA-VLN research, we introduce the Human-Aware 3D (HA3D) simulator, a realistic environment combining dynamic human activities with the Matterport3D dataset . HA3D utilizes the self-collected Human Activity and Pose Simulation (HAPS) dataset, which includes 145 human activity descriptions converted into 435 detailed 3D human motion models using the SMPL model  (Sec. 2.1). The simulator provides an interactive annotation tool for placing human models in 29 different indoor areas across 90 building scenes (Fig. 12). Moreover, we introduce the Human-Aware Room-to-Room (HA-R2R) dataset, an extension of the Room-to-Room (R2R) dataset  incorporating human activity descriptions. HA-R2R includes 21,567 instructions with an expanded vocabulary and activity coverage compared to R2R (Fig. 3 and Sec. 2.2).

Building upon the HA-VLN task and the HA3D simulator, we propose two multimodal agents to address the challenges posed by dynamic human environments: the Expert-Supervised Cross-Modal (VLN-CM) agent and the Non-Expert-Supervised Decision Transformer (VLN-DT) agent. The innovation of these agents lies in their cross-modal fusion module, which dynamically weights language and visual information, enhancing their understanding and utilization of different modalities. VLN-CM learns by imitating expert demonstrations (Sec. 2.2), while VLN-DT demonstrates the potential to learn solely from random trajectories without expert supervision (Fig. 4, right). We also design a rich reward function to incentivize agents to navigate effectively (Fig. 5).

To comprehensively evaluate the performance of the HA-VLN task, we design new metrics considering human activities, and highlight the unique challenges faced by HA-VLN (Sec. 3.2). Evaluating state-of-the-art VLN agents on the HA-VLN task reveals a significant performance gap compared to the Oracle, even after retraining, thereby underscoring the complexity of navigating in dynamic human environments (Sec. 3.3). Moreover, experiments show that VLN-DT, trained solely on random data, achieves performance comparable to VLN-CM under expert supervision, thus demonstrating its superior generalization ability (Sec. 3.4). Finally, we validate the agents in the real world using a quadruped robot, exhibiting perception and avoidance capabilities, while also emphasizing the necessity of further improving real-world robustness and adaptability (Sec. 3.5).

Our main contributions are as follows: (1) Introducing HA-VLN, a new task that extends VLN by incorporating dynamic human activities and relaxing assumptions; (2) Offering HA3D, a realistic simulator, and HA-R2R, an extension of the R2R dataset, to support HA-VLN research and enable the development of robust navigation agents; (3) Proposing VLN-CM and VLN-DT agents that utilize expert and non-expert supervised learning to address the challenges of HA-VLN, showcasing the effectiveness of cross-modal fusion and diverse training strategies; and (4) Designing comprehensive evaluations for HA-VLN, providing benchmarks and insights for future research.

Figure 1: **HA-VLN Scenario: The agent navigates through environments populated with dynamic human activities. The task involves optimizing routes while maintaining safe distances from humans to address the _Sim2Real_ gap. In this scenario, the agent encounters various human activities, such as someone talking on the phone while pacing in the hallway, someone taking off their shoes in the entryway/foyer, and someone carrying groceries upstairs. The HA-VLN agent must adapt its path by waiting for humans to move, adjusting its path, or proceeding through when clear, thereby enhancing real-world applicability.**

Human-Aware Vision-and-Language Navigation

We introduce _Human-Aware Vision-and-Language Navigation_ (_HA-VLN_), an extension of traditional Vision-and-Language Navigation (_VLN_) that bridges the _Sim2Real gap_ between simulated and real-world navigation scenarios. As shown in Fig. 1, _HA-VLN_ involves an embodied agent navigating from an initial position to a target location within a dynamic environment, guided by natural language instructions \(=<w_{1},w_{2},,w_{L}>\), where \(L\) denotes the total number of words and \(w_{i}\) represents an individual word. At the beginning of each episode, the agent assesses its initial state \(_{0}=<_{0},_{0},_{0},_{0}^{60}>\) within a \( t=2\) seconds observation window, where \(_{0}=(x_{0},y_{0},z_{0})\) represents the initial 3D position, \(_{0}\) the heading, \(_{0}\) the elevation, and \(_{0}^{60}\) the egocentric view within a 60-degree field of view. The agent executes a sequence of actions \(_{T}=<a_{0},a_{1},,a_{T}>\), resulting in states and observations \(_{T}=<_{0},_{1},,_{T}>\), where each action \(a_{t}=<a_{},a_{},a_{},a _{},a_{},a_{}>\) leads to a new state \(_{t+1}=<_{t+1},_{t+1},_{t+1},_{t+1}^ {60}>\). The episode concludes with the stop action \(a_{}\).

In contrast to traditional _VLN_ tasks, _HA-VLN_ addresses the _Sim2Real gap_ by relaxing three key assumptions, as depicted in Fig. 1:

1. **Egocentric Action Space:**_HA-VLN_ employs an egocentric action space \(\) with a limited 60\({}^{}\) field of view \(_{t}^{60}\), requiring the agent to make decisions based on human-like visual perception. The state \(_{t}=<_{t},_{t},_{t},_{t}^{60}>\) captures the agent's egocentric perspective at time \(t\), enabling effective navigation in real-world scenarios.
2. **Dynamic Environments:**_HA-VLN_ introduces dynamic environments based on 3D human motion models \(=<h_{1},h_{2},,h_{N}>\), where each frame \(h_{i}^{6890 3}\) encodes human positions and shapes using the Skinned Multi-Person Linear (SMPL) model . The agent must perceive and respond to these activities in real-time while maintaining a safe distance \(d_{}\), reflecting real-world navigation challenges.
3. **Sub-optimal Expert Supervision:** In _HA-VLN_, agents learn from sub-optimal expert demonstrations that provide navigation guidance accounting for the dynamic environment. The agent's policy \(_{}(a_{t}|_{t},,)\) aims to maximize the expected reward \([r(_{t+1})]\), considering human interactions and safe navigation. The reward function \(r:\) assesses the quality of navigation at each state, allowing better handling of imperfect instructions in real-world tasks.

Building upon these relaxed assumptions, a key feature of _HA-VLN_ is the inclusion of human activities captured at 16 FPS. When human activities fall within the agent's field of view \(_{t}^{60}\), the agent is considered to be interacting with humans. _HA-VLN_ introduces the Adaptive Response Strategy, where the agent detects and responds to human movements, anticipating trajectories and making real-time path adjustments. Formally, this strategy is defined as:

\[_{}(a_{t}|_{t},,)=_{ a_{t}}P(a_{t}|_{t},)[r( _{t+1})],\] (1)

where \([r(_{t+1})]\) represents the expected reward considering human interactions and safe navigation. To support the agent in learning, the _HA3D_ simulator (Sec. 2.1) provides interfaces to access human posture, position, and trajectories, while _HA-VLN_ employs sub-optimal expert supervision (Sec. 2.2) to provide weak signals, reflecting real-world scenarios with imperfect demonstration.

### HA3D Simulator: Integrating Dynamic Human Activities

The _Human-Aware 3D (HA3D) Simulator_ generates dynamic environments by integrating natural human activities from the custom-collected _Human Activity and Pose Simulation (HAPS) dataset_ with the photorealistic environments of the Matterport3D dataset  (see Fig. 2 and Fig. 12).

**HAPS Dataset.** HAPS addresses the limitations of existing human motion datasets by identifying 29 distinct indoor regions across 90 architectural scenes and generating 145 human activity descriptions. These descriptions, validated through human surveys and quality control using GPT-4 , encompass realistic actions such as walking, sitting, and using a laptop. The Motion Diffusion Model (MDM)  converts these descriptions into 435 detailed 3D human motion models \(\) using the SMPL model, with each description transformed into three distinct 120-frame motion sequences1. The dataset also includes annotations of human-object interactions and the relationship between human activities and architectural layouts. After manual selection, approximately 422 models were retained. Further details on the dataset are provided in App. B.1.

**Human Activity Annotation.** An interactive annotation tool accurately locates humans in different building regions (see Fig. 12). Users explore buildings, select viewpoints \(_{i}=(x_{i},y_{i},z_{i})\), set initial human positions, and choose 3D human motion models \(_{i}\) based on the environment of \(_{i}\). To follow real-world scenarios, multiple initial human viewpoints \(_{}=\{_{1},_{2},,_ {k}\}\) are randomly selected from a subset of all viewpoints in the building. The number of people in each building is estimated by dividing the building area by the average area per capita in the U.S. (2021, 67\(m^{2}\))  and rounding up. In the Matterport3D dataset, these viewpoints are manually annotated to facilitate the transfer from other VLN tasks to HA-VLN. This setup ensures agents can navigate environments with dynamic human activities updated at 16 FPS, allowing real-time perception and response. Detailed statistics of activity annotation are in App. B.2.

**Realistic Human Rendering.** HA3D employs _Pyrender_ to render dynamic human bodies with high visual realism. The rendering process aligns camera settings with the agent's perspective and integrates dynamic human motion using a 120-frame SMPL mesh sequence \(= h_{1},h_{2},,h_{120}\). Each frame \(h_{t}=(_{t},_{t},_{t})\) consists of shape parameters \(_{t}^{10}\), pose parameters \(_{t}^{72}\), and mesh vertices \(_{t}^{6890 3}\) calculated based on \(_{t}\) and \(_{t}\) through the SMPL model. At each time step, the 3D mesh \(h_{t}\) is dynamically generated, with vertices \(_{t}\) algorithmically determined to form the human model accurately. These vertices are then used to generate depth maps \(_{t}\), distinguishing human models from other scene elements. HA3D allows real-time adjustments of human body parameters, enabling the representation of diverse appearances and enhancing interactivity. More details on the rendering pipeline and examples of rendered human models are in App. B.3.

**Agent-Environment Interaction.** Compatible with the Matterport3D simulator's configurations , HA3D provides agents with environmental feedback signals at each time step \(t\), including first-person RGB-D video observation \(_{t}^{60}\), navigable viewpoints, and a human "collision" feedback signal \(c_{t}\). The agent receives its state \(_{t}=_{t},_{t},_{t},_{t}^{60}\), where \(_{t}=(x_{t},y_{t},z_{t})\), \(_{t}\), and \(_{t}\) denote position, heading, and elevation, respectively. The agent's policy \(_{}(a_{t}|_{t},,)\) maximizes expected reward \([r(_{t+1})]\) by considering human interactions for safe navigation. The collision feedback signal \(c_{t}\) is triggered when the agent-human distance \(d_{a,h}(t)\) falls below a threshold \(d_{}\). Customizable collision detection and feedback parameters enhance agent-environment interaction. Details on visual feedback, optimization, and extended interaction capabilities are in App. B.4.

Figure 2: **Human-Aware 3D (HA3D) Simulator Annotation Process:** HA3D integrates dynamic human activities from the Human Activity and Pose Simulation (HAPS) dataset into the photorealistic environments of Matterport3D. The annotation process involves: (1) integrating the HAPS dataset, which includes 145 human activity descriptions converted into 435 detailed 3D human motion models in 52,200 frames; (2) annotating human activities within various indoor regions across 90 building scenes using an interactive annotation tool; (3) rendering realistic human models; and (4) enabling interactive agent-environment interactions.

**Implementation and Performance.** Developed using C++/Python, OpenGL, and Pyrender, HA3D integrates with deep learning frameworks like PyTorch and TensorFlow. It offers flexible configuration options, achieving up to 300 fps on an NVIDIA RTX 3050 GPU with 640x480 resolution. Running on Linux, the simulator has a low memory usage of 40MB and supports multi-processing for parallel execution of simulation tasks. Its modular architecture enables easy extension and customization. The simulator supports various rendering techniques, enhancing visual realism. It provides high-level APIs for real-time data streaming and interaction with external controllers. PyQt5-based annotation tools with an intuitive interface will be made available to researchers. Additional details on the simulator's implementation, performance, and extensibility are provided in App. B.5.

### Human-Aware Navigation Agents

We introduce the _Human-Aware Room-to-Room (HA-R2R) dataset_, extending the Room-to-Room (R2R) dataset  by incorporating human activity descriptions to create a more realistic and dynamic navigation environment. To address HA-VLN challenges, we propose two agents: the _expertsupervised Cross Modal (VLN-CM) agent_ and _the non-expert-supervised Decision Transformer (VLN-DT) agent_. An _Oracle agent_ provides ground truth supervision for training and benchmarking.

**HA-R2R Dataset.** HA-R2R extends R2R dataset by incorporating human activity annotations while preserving its fine-grained navigation properties.The dataset was constructed in two steps: 1) mapping R2R paths to the HA3D simulator, manually annotating human activities at key locations; and 2) using GPT-4  to generate new instructions by combining original instructions, human activity descriptions, and relative position information, followed by human validation. The resulting dataset contains 21,567 human-like instructions with 145 activity types, categorized as _start_ (1,047), _obstacle_ (3,666), _surrounding_ (14,469), and _end_ (1,041) based on their positions relative to the agent's starting point (see App. C.1 for details). Compared to R2R, HA-R2R's average instruction length increased from 29 to 69 words, with the vocabulary expanding from 990 to 4,500. Fig. 3A shows the instruction length distribution by activity count, while Fig. 3B compares HA-R2R and R2R distributions. Fig. 3C summarizes viewpoints affected by human activities, and Fig. 14 illustrates the instruction quality by analyzing common word frequencies. More details are provided in App. C.1.

**Oracle Agent: Ground Truth Supervision.** The Oracle agent serves as the ground truth supervision source to guide and benchmark the training of expert-supervised and non-expert-supervised agents in the HA-VLN system. Designed as a _teacher_, the Oracle provides realistic supervision derived from the HA-R2R dataset, strictly following language instructions while dynamically avoiding human activities along navigation paths to ensure maximal expected rewards. Let \(G=(N,E)\) be the global navigation graph, with nodes \(N\) (locations) and edges \(E\) (paths). When human activities affect nodes \(n N\) within radius \(r\), those nodes form subset \(N_{h}\). The Oracle's policy \(^{*}_{}\) re-routes on the modified graph \(G^{}=(N N_{h},E^{})\), where \(E^{}\) only includes edges avoiding \(N_{h}\), ensuring the Oracle avoids human-induced disturbances while following navigation instructions optimally. Algorithm 1 details the Oracle's path planning and collision avoidance strategies. During training, at step \(t\), a cross-entropy loss maximizes the likelihood of true target action \(a^{*}_{t}\) given the previous state-action sequence \( s_{0},a_{0},s_{1},a_{1},,s_{t}\). The target output \(a^{*}_{t}\) is defined as the Oracle's next action from the current location to the goal. Please refer to App. C.2 for more details.

Figure 3: **Dataset Analysis of HA-R2R:****(A)** Impact of human activities on instruction length, tokenized using NLTK WordNet, showing the variation in instruction length caused by different types of human activities. **(B)** Comparison of instruction length distributions between HA-R2R and the original R2R dataset. HA-R2R demonstrates a more uniform distribution, facilitating balanced training. **(C)** Analysis of viewpoints affected by human activities: “Visible” denotes activities within the agent’s sight, “Isolated” refers to key navigation nodes impacted by human activities, and “Occupied” indicates the presence of humans at specific viewpoints.

**VLN-CM: Multimodal Integration for Supervised Learning.** We propose the Vision-Language Navigation Cross-Modal (VLN-CM) agent, an LSTM-based sequence-to-sequence model  augmented with a cross modality fusion module for effective multimodal integration (Fig. 4, left). The language instruction \(= w_{1},w_{2},,w_{L}\), where \(w_{i}\) denotes the \(i\)-th word, is encoded into BERT embeddings \(\{e_{1},e_{2},,e_{L}\}\), which are processed by an LSTM to yield context-aware representations \(\{u_{1},u_{2},,u_{L}\}\). Simultaneously, visual observations \(_{t}\) at each timestep \(t\) are encoded using ResNet-152 , producing an image feature map \(\{c_{1},c_{2},,c_{N}\}\), where \(N\) is the number of visual features. The fusion module integrates the context encoder outputs and image features via cross-attention, generating a unified representation \(m_{t}\) at each timestep \(t\). An LSTM-based action decoder predicts the next action \(a_{t+1}\) from the action space \(=\{a_{},a_{},a_{},a_{ },a_{},a_{}\}\) conditioned on \(m_{t}\) and the previous action \(a_{t}\). The agent is trained via supervised learning from an expert Oracle agent using cross-entropy loss:

\[_{}=_{a}y_{t}(a) p(a_{t}|,_{t}),\] (2)

where \(_{}\) is the cross-entropy loss, \(y_{t}(a)\) is the ground truth action distribution from the expert trajectory at timestep \(t\), and \(p(a_{t}|,_{t})\) is the predicted action distribution given instruction \(\) and observation \(_{t}\) at timestep \(t\).

**VLN-DT: Reinforcement Learning with Decision Transformers.** We present the Vision-Language Navigation Decision Transformer (VLN-DT), an autoregressive transformer  with a cross-modality fusion module for navigation without expert supervision2 (Fig. 4, right). VLN-DT learns from sequence representations \(=(_{1},_{1},_{1},,_{t},_{ t})\) to predict the next action \(_{t}\), where \(_{t}\) is the state at timestep \(t\), and \(_{t}=_{t^{}=t}^{T}r_{t^{}}\) is the Return to Go. The cross-modality fusion module computes \(_{t}\) by processing the average pooling vector of the BERT embedding  for a language instruction \(\) (excluding the [CLS] token) and the image feature map of the current observation \(_{t}^{60}\), extracted using a pre-trained ResNet-152 . The fusion module dynamically weights the language and visual modalities using an attention mechanism, enhancing \(_{t}\). The fused representations are then fed into the causal transformer, which models \(\) autoregressively to determine \(_{t}\). We train VLN-DT using \(10^{4}\) random walk trajectories, each with a maximum length of 30 steps, a context window size of 15 steps, and an initial Return To Go of 5 to guide the agent's exploration-exploitation balance . Three reward types are designed to incentivize effective navigation: target reward (_based on distance to the target_), distance reward (_based on movement towards the target_), and human reward (_based on collisions with humans_) . Fig. 5 shows the impact of different reward strategies on navigation performance. The loss function \(_{}\) for training VLN-DT is a supervised learning objective with cross-entropy loss:

\[_{}=_{a}y_{t}^{*}(a) p(a_{t}|s_{t}),\] (3)

Figure 4: **Model Architectures of Navigation Agents: The architectures of the Vision-Language Navigation Cross-Modal (VLN-CM) agent (left) and the Vision-Language Navigation Decision Transformer (VLN-DT) agent (right). Both agents employ a cross-modality fusion module to effectively integrate visual and linguistic information for predicting navigation actions. VLN-CM utilizes an LSTM-based sequence-to-sequence model for expert-supervised learning, while VLN-DT leverages an autoregressive transformer model to learn from random trajectories without expert supervision.**where \(y_{t}^{*}(a)\) is the ground truth action distribution from the random trajectory at timestep \(t\), and \(p(a_{t}|s_{t})\) is the predicted action distribution given instruction \(\) and observation \(_{t}\) at timestep \(t\). The implementation of VLN-DT is summarized in App. C.3.

## 3 Experiments

We evaluated our Human-Aware Vision-and-Language Navigation (HA-VLN) task, focusing on human perception and navigation. Experiments included assessing different assumptions (Sec. 3.2), comparing with state-of-the-art (SOTA) VLN agents (Sec. 3.3)3, analyzing our agents' performance (Sec. 3.4), and validating with real-world quadruped robot tests (Sec. 3.5).

### Evaluation Protocol for HA-VLN Task

We propose a two-fold evaluation protocol for the HA-VLN task, focusing on both _human perception_ and _navigation_ aspects. The _human perception_ metrics evaluate the agent's ability to perceive and respond to human activities, while the _navigation-related_ metrics assess navigation performance. As human activities near critical nodes4 greatly influence navigation, we introduce a strategy to handle dynamic human activities for more accurate evaluation5. Let \(A^{c}i\) be the set of human activities at critical nodes in navigation instance \(i\). The updated _human perception_ metrics are:

\[=^{L}(c_{i}-|A^{c}i|)}{L},=^{L}-|A^{c}i|,1)}}{ L},\] (4)

where _TCR_ reflects the overall frequency of the agent colliding with human-occupied areas within a 1-meter radius, _CR_ is the ratio of navigation instances with at least one collision, and \(\) denotes the ratio of instructions affected by human activities. The updated _navigation_ metrics are:

\[=^{L}di}{L},=^{L} (c_{i}-|A^{c}_{i}|=0)}{L},\] (5)

where _NE_ is the distance between the agent's final position and the target location, and _SR_ is the proportion of navigation instructions successfully completed without collisions and within a predefined navigation range. Please refer to App. D.1 for more details.

### Evaluating HA-VLN Assumptions

We assessed the impact of relaxing traditional assumptions on navigation performance by comparing HA-VLN and VLN task, relaxing each assumption individually.

**Panoramic vs. Egocentric Action Space** (Tab. 1): Shifting from a panoramic to an egocentric action space significantly degrades overall performance, with Success Rate (SR) dropping by 70.0% in seen environments and by 43.8% in unseen environments. Additionally, there is a marked increase in

    &  &  \\   & **NE \(\)** & **TCR \(\)** & **CR \(\)** & **SR \(\)** & **NE \(\)** & **TCR \(\)** & **CR \(\)** & **SR \(\)** \\ 
**Egocentric** & 7.21 & 0.69 & 1.00 & 0.20 & 8.09 & 0.54 & 0.58 & 0.16 \\
**Panoramic** & 5.58 & 0.24 & 0.80 & 0.34 & 7.16 & 0.25 & 0.57 & 0.23 \\ 
**Difference** & **4.163** & **-0.45** & **-0.20** & **-0.14** & **-0.93** & **-0.29** & **-0.01** & **+0.07** \\
**Percentage** & **-22.69** & **-65.25** & **-20.05** & **+70.05** & **-11.5** & **-53.75** & **-17.75** & **+43.85** \\   

Table 1: Egocentric vs. Panoramic Action Space Comparison

    &  &  \\   & **NE \(\)** & **TCR \(\)** & **CR \(\)** & **SR \(\)** & **NE \(\)** & **TCR \(\)** & **CR \(\)** & **SR \(\)** \\ 
**Optimal** & 3.61 & 0.15 & 0.52 & 0.53 & 5.43 & 0.26 & 0.69 & 0.41 \\
**Sub-optimal** & 3.98 & 0.18 & 0.63 & 0.50 & 5.24 & 0.24 & 0.67 & 0.40 \\ 
**Difference** & **+0.37** & **+0.03** & **+0.11** & **-0.03** & **-0.19** & **-0.02** & **-0.02** & **-0.01** \\
**Percentage** & **+10.25** & **+20.05** & **+21.25** & **-5.75** & **-3.55** & **-7.75** & **-2.95** & **-2.45** \\   

Table 2: Optimal vs. Sub-Optimal Expert Comparisonboth Navigation Error (NE) and Target Collision Rate (TCR), underscoring the critical importance of panoramic action spaces for effective and reliable navigation in complex, dynamically human-populated environments.

**Static vs. Dynamic Environment** (Tab. 3): Introducing dynamic human motion into the environment reduces SR by 46.7% in seen environments and by 19.4% in unseen settings, presenting a substantial obstacle to reliable and effective navigation while highlighting the challenges inherent in human-aware task performance.

**Optimal vs. Sub-optimal Expert** (Tab. 2):

Training with a sub-optimal expert marginally increases NE by 10.2% and reduces SR by 5.7% in seen environments. Although slightly lower in accuracy, sub-optimal expert guidance introduces greater realism to the agent's training, offering navigation experiences more aligned with real-world variability and thus contributing to improved robustness in human-aware metrics.

### Evaluation of SOTA VLN Agents on the HA-VNL Task

We evaluated state-of-the-art (SOTA) Vision-and-Language Navigation (VLN) agents on the Human-Aware Vision-and-Language Navigation (HA-VLN) task. Each agent was adapted for HA-VLN by incorporating panoramic action spaces and sub-optimal expert guidance to navigate dynamic, human-occupied environments. Our evaluations included both retrained and zero-shot performance assessments, revealing substantial performance degradations in HA-VLN scenarios compared to traditional VLN tasks and significant gaps from the oracle, underscoring the increased complexity introduced by human-aware navigation.

**Retrained Performance.** In retrained HA-VLN settings, even the best-performing agent achieved a maximum success rate (SR) of only 40% in unseen environments, which is 49% lower than the oracle's SR (Tab. 4, Tab. 6). The impact of human occupancy is marked, with SR reductions of up to 65% in unseen settings. Despite retraining, agents remain limited in their human-aware capabilities, exhibiting high Target Collision Rates (TCR) and Collision Rates (CR). For instance, the Speaker-Follower model records TCR and CR values of 0.24 and 0.87 in seen environments, which contrast

    &  &  \\   & **w/s human** & **w/ human** & **Difference** & **w/s human** & **w/ human** & **Difference** \\   & **NE \(\)** & **SR \(\)** & **NE \(\)** & **SR \(\)** & **NE \(\)** & **NE \(\)** & **SR \(\)** & **NE \(\)** & **SR \(\)** \\ 
**Speaker-Follower** & 6.62 & 0.35 & 5.85 & 0.34 & **11.87\%** & **32.95\%** & 3.36 & 0.66 & 7.16 & 0.23 & **41.81\%** & **-0.82\%** \\
**Brex (PGSCAR)** & 3.93 & 0.63 & 4.95 & 0.41 & **12.85\%** & **34.95\%** & 3.90 & 0.72 & 3.86 & 0.36 & **41.82\%** & **-0.86\%** \\
**Brex (PGSCAR)** & 4.29 & 0.59 & 4.67 & 0.42 & **8.85\%** & **33.51** & 3.01 & 7.17 & 3.86 & 0.38 & **98.44\%** & **-0.65\%** \\
**AightNet** & 4.01 & 0.62 & 3.98 & 0.50 & **-0.47\%** & **19.45\%** & 2.68 & 0.75 & 5.24 & 0.40 & **99.85\%** & **-0.67\%** \\   

Table 4: Performance of SOTA VLN Agents on HA-VLN (Retrained)

    &  &  \\   & **NE \(\)** & **SR \(\)** & **NE \(\)** & **SR \(\)** & **NE \(\)** & **SR \(\)** \\ 
**Static** & 2.68 & 0.75 & 4.01 & 0.62 \\
**Dynamic** & 5.24 & 0.40 & 3.98 & 0.50 \\ 
**Difference** & **-2.56** & **-0.35** & **-0.03** & **-0.12** \\
**Percentage** & **-49.55\%** & **-46.7\%** & **-0.75** & **-19.45** \\   

Table 3: Static vs. Dynamic Environment Comparison

    &  &  \\   & **w/s human** & **w/ human** & **Difference** & **w/s human** & **w/ human** & **Difference** \\ 
**Speaker-Follower** & 6.62 & 0.35 & 5.72 & 0.24 & **47.66\%** & **-31.45** & 3.36 & 0.66 & 0.49 & **47.63\%** & **-39.45** \\
**Rec (PEXALEM)** & 3.93 & 0.63 & 6.93 & 0.26 & **47.63\%** & **-38.75** & 2.90 & 0.72 & 7.92 & **-0.21** & **461.75\%** & **-70.85** \\
**Rev (OSCAR)** & 4.29 & 0.59 & 7.45 & 0.23 & **47.34\%** & **-61.05** & 3.11 & 0.71 & 8.37 & 0.20 & **+169.19\%** & **-71.85** \\
**AightNet** & 4.01 & 0.63 & 6.27 & 0.30 & **56.45\%** & **-51.65** & 2.68 & 0.75 & 7.16 & 0.25 & **sharply with the oracle's significantly lower TCR of 0.04 and CR of 0.175 (Tab. 5, Tab. 6). These disparities highlight the challenges agents face in adapting to human-centered dynamics.

**Zero-shot Performance.** The zero-shot performance of SOTA VLN agents in HA-VLN environments reveals even more pronounced challenges. While leading agents achieve up to 72% SR in traditional VLN tasks for unseen environments, this drops significantly under HA-VLN constraints (Tab. 7). Even Airbert, designed to manage complex environmental contexts, struggles in human-occupied settings, with navigation errors rising by over 167% and SR falling by nearly 67%. These results highlight the considerable difficulty agents encounter in dynamic, human-centric settings, emphasizing the necessity for further advancements in training strategies and navigation models to improve robustness and adaptability in real-world, human-aware navigation tasks.

### Evaluation of Agents on HA-VLN Task

In this work, we introduce two agent models: the Vision-Language Navigation Decision Transformer (VLN-DT), trained on a dataset generated via random walk, and the Vision-Language Navigation Cross-Modal (VLN-CM), trained under expert supervision. This section compares their performance and examines the impact of various reward strategies on task execution.

**Performance Comparison.** Table 8 presents a comparative analysis of our agents on HA-VLN tasks. VLN-DT, trained with 100% random walk data, demonstrates comparable performance to the expert-supervised VLN-CM, exhibiting strong generalization capabilities. Notably, VLN-CM's performance degrades significantly as the proportion of random walk data increases; with 100% random data, Success Rate (SR) declines by 83.6% in seen and 81.5% in unseen environments. This outcome underscores VLN-DT's robustness and reduced dependency on expert guidance, making it well-suited for diverse and unpredictable scenarios.

**Reward Strategy Analysis.** Figure 5 illustrates the effect of different reward strategies on VLN-DT's performance. A straightforward reward for decreasing target distance resulted in inefficient trajectories with an elevated collision rate. Introducing a penalty-based distance reward achieved modest improvements in Success Rate (SR) and Collision Rate (CR). However, applying additional penalties for human collisions did not significantly enhance performance, underscoring the need for more advanced, human-aware reward strategies to effectively navigate agents through dynamic, human-populated environments.

This analysis highlights the advantages of VLN-DT's design in balancing adaptability and efficiency across various conditions while identifying key areas for future development in reward strategies tailored for human-aware navigation. Detailed performance metrics can be found in Appendix D.4.

### Evaluation on Real-World Robots

To assess real-world applicability, we deployed our trained agent on a Unitree quadruped robot equipped with a stereo fisheye camera, ultrasonic distance sensors, and an inertial measurement unit

    &  &  &  \\   & & **NE\(\)** & **TCR\(\)** & **CR\(\)** & **SR\(\)** & **NE\(\)** & **TCR\(\)** & **CR\(\)** & **SR\(\)** \\ 
**VLN-DT (Ours)** & 100\% & 8.51 & **0.30** & 0.77 & **0.21** & **8.22** & **0.37** & **0.58** & 0.11 \\   & 0\% & **7.31** & 0.38 & **0.73** & 0.19 & 8.22 & 0.42 & 0.62 & **0.12** \\  & 3\% & 7.23 & 0.75 & 0.87 & 0.20 & 8.23 & 0.82 & 0.61 & 0.13 \\
**VLN-CM (Ours)** & 25\% & 7.85 & 0.85 & 0.61 & 0.16 & 8.42 & 0.99 & 0.52 & 0.12 \\  & 50\% & 8.67 & 0.98 & 0.52 & 0.11 & 8.74 & 1.15 & 0.45 & 0.09 \\  & 100\% & 10.61 & 1.01 & 0.62 & 0.03 & 10.39 & 1.14 & 0.48 & 0.02 \\   

Table 8: Performance Comparison of Our Proposed Agents on HA-VLN Tasks.

Figure 5: Effects of Reward Strategies on VLN-DT.

(IMU) (Fig. 15). The agent operates on an NVIDIA Jetson TX2, processing RGB images to make action inferences, which are subsequently executed via a Raspberry Pi 4B. Continuous IMU feedback enables the robot to monitor and adjust its movement for precision.

Experiments were conducted in office environments to evaluate the agent's navigation performance both in the absence and presence of humans. In human-free scenarios (Fig. 16), the agent successfully demonstrated accurate navigation by reliably following prescribed instructions. In human-populated settings, the agent exhibited human-aware navigation, detecting and actively avoiding individuals in its path (Fig. 17). However, we also observed cases where the robot's performance degraded, resulting in collisions due to sudden, unpredictable changes in human behavior (Fig. 18), which highlights the inherent challenges of navigating dynamic, human-centric environments.

These experiments underscore the effectiveness of transferring learned policies from simulated settings to physical robots, while also revealing areas for improvement. Specifically, the findings highlight the necessity for enhanced robustness and adaptability to better manage real-world complexity. Additional experimental details and results are provided in App. D.4.

## 4 Discussion

**Applications & Extensions.** The HA3D simulator advances the field of human-centered simulation by accommodating widely-adopted 3D formats, including.obj and.glb, thus streamlining integration and promoting broader research utility. This adaptability enables researchers to expand character diversity and customize agents within simulated scenes, fostering the creation of complex, multi-agent interactive environments. Moreover, the framework's architecture readily supports the incorporation of additional dynamic entities, such as animals and autonomous robots, thereby further enhancing the simulation's capacity to represent realistic, richly populated scenarios. For implementation details, please refer to our GitHub repository.

**Limitations.** While the Human-Aware Vision and Language Navigation (HA-VLN) framework constitutes a significant step forward in embodied AI navigation, certain limitations persist. The framework's current scope captures human presence and basic movement but does not yet model the breadth of human behavioral patterns and social nuances, which may affect the robustness of trained agents in real-world applications where human interactions are more complex and varied. Additionally, the HA3D and HA-R2R datasets are confined to indoor environments, which may limit the generalizability of trained agents across diverse real-world settings, particularly in outdoor contexts where navigation dynamics differ substantially.

**Future Work.** To further enhance the HA-VLN framework, future research should prioritize refining human behavior modeling to encompass more sophisticated social interactions, nuanced group dynamics, and contextualized interpersonal behaviors. The inclusion of avatars with heightened behavioral fidelity would enrich the simulation's realism, enabling more effective modeling of human-agent interactions. Extending the simulator to support outdoor environments is also paramount, as this expansion would allow for the development of agents capable of navigating across a wider range of real-world scenarios. These improvements, coupled with advanced domain adaptation techniques and robust strategies for managing environmental uncertainty, are essential to foster the development of highly adaptable and resilient VLN systems capable of seamless operation within diverse, human-populated environments.

## 5 Conclusion

This work presents the Human-Aware Vision and Language Navigation (HA-VLN) framework, which integrates dynamic human activities while relaxing restrictive assumptions inherent to conventional VLN systems. Through the development of the Human-Aware 3D (HA3D) simulator and the Human-Aware Room-to-Room (HA-R2R) dataset, we provide a comprehensive environment for the training and evaluation of HA-VLN agents. We introduce two agent architectures--the Expert-Supervised Cross-Modal (VLN-CM) and the Non-Expert-Supervised Decision Transformer (VLN-DT)--each leveraging cross-modal fusion and diverse training paradigms to support effective navigation in dynamically populated settings. Extensive evaluation highlights the contributions of this framework while underscoring the need for continued research to strengthen HA-VLN agents' robustness and adaptability for deployment in complex, real-world environments.

## Author Contributions

Heng Li was responsible for agent development and evaluations, drafted the initial agent and evaluation sections, and revised the final manuscript based on review feedback. Minghan Li was responsible for simulator development and evaluations, prepared the initial draft of the simulator and evaluation sections, conducted real-world testing, and created the project website. Zhi-Qi Cheng supervised the design and development of both the agent and simulator, managed the entire project execution, designed the evaluation plan, drafted the initial manuscript, revised the final version, and provided guidance to the entire team. Yifei Dong designed the initial simulation prototyping, drafted the related work section, and provided revision suggestions. Yuxuan Zhou offered collaborative feedback and contributed revision suggestions. Jun-Yan He participated in project discussions. Qi Dai provided invaluable strategic guidance and contributed to manuscript revisions. Teruko Mitamura offered constructive feedback, and Alexander G. Hauptmann provided critical insights and contributed to manuscript refinement. We also thank the anonymous reviewers for their valuable suggestions.