# Recovering Unbalanced Communities in the

Stochastic Block Model with Application to

Clustering with a Faulty Oracle+
Footnote â€ : Authors are in alphabetical order.

Chandra Sekhar Mukherjee \({}^{}\)

chandrasekhar.mukherjee@usc.edu

Pan Peng

ppeng@ustc.edu.cn

Jiapeng Zhang

jiapengz@usc.edu

###### Abstract

The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. However, our understanding of SBM with unbalanced communities (argaubly, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes. We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons. Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant.

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than \(()\), even in the presence of \((n)\) small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than \((n^{2/5})\) small clusters.

## 1 Introduction

Graph clustering (or community detection) is a fundamental problem in computer science and has wide applications in many domains, including biology, social science, and physics. Among others, the stochastic block model (SBM) is one of the most basic models for studying graph clustering, offering both a theoretical arena for rigorously analyzing the performance of different types of clustering algorithms, and synthetic benchmarks for evaluating these algorithms in practice. Since the 1980s (e.g., ), there has been much progress towards the understanding of the statistical and computational tradeoffs for community detection in SBM with various parameter regimes. We refer to the recent survey  for a list of such results.

In this paper, we focus on a very basic version of the stochastic block model.

**Definition 1.1** (The \((n,k,p,q)\) model).: _In this model, given an \(n\)-vertex set \(V\) with a hidden partition \(V=_{i=1}^{k}V_{i}\) such that \(V_{i} V_{j}=\) for all \(i j\), we say a graph \(G=(V,E)\) is sampled from \((n,k,p,q)\), if for all pairs of vertices \(v_{i},v_{j} V\), (1) an edge \((v_{i},v_{j})\) is added independently with probability \(p\), if \(v_{i},v_{j} V_{}\) for some \(\); (2) an edge \((v_{i},v_{j})\) is added independently with probability \(q\), otherwise._

We are interested in the problem of _fully recovering_ all or some of the clusters, given a graph \(G\) that is sampled from \((n,k,p,q)\). A cluster \(V_{i}\) is said to be fully recovered if the algorithm outputs a set \(S\) that is exactly \(V_{i}\). Most of the previous algorithms on the full recovery of SBM either just work for the _nearly balanced_ case (i.e., each cluster has size \(()\)) when \(k\) is small, say \(k=o( n)\) (see e.g. ), or only work under the following assumption:

* _All_ of the latent clusters are sufficiently large4, i.e., for each \(j\), \(|V_{j}|=()\) (see e.g., ).

From a practical perspective, many real-world graphs may have many communities of different sizes, that is, large and small clusters co-exist in these graphs. This motivates us to investigate how to recover the communities in SBM if the latent communities have very different sizes. In particular, we are interested in _efficiently_ recovering all the large clusters in the presence of small clusters. However, such a task can be quite difficult, as those small clusters may be confused with noisy edges. Indeed, most previous algorithms try to find all the \(k\)-clusters in one shot, which always computes some structures/information of the graph that are sensitive to noise (and small clusters). For example, the classical SVD-based algorithms (e.g., ) first compute the first \(k\) singular vectors of some matrix associated with the graph and then use these \(k\) vectors to find clusters. Such singular vectors are sensitive to edge insertions or deletions (e.g. ). In general, this difficulty was termed by Ailon et al.  as "_small cluster barrier_" for graph clustering.

To overcome such a barrier, Ailon et al.  proposed an algorithm that recovers all large latent clusters in the presence of small clusters under the following assumptions (see ),

* none of the cluster sizes falls in the interval \((/c,)\) for a number \((}{p-q})\) and \(c>1\) is some universal constant;
* there exists a large cluster, say of size at least \(:=(\{}{p-q},}\})\).

The algorithm in  then has to exhaustively search for such a gap, and then apply a convex program-based algorithm to find a large cluster of size at least \(\). As we discuss in the Appendix D. the assumption of the recoverable cluster being larger than \((/(p-q))\) is (relatively) natural as any polynomial time algorithm can only recover clusters of size \(()\), under the planted clique conjecture. Still, two natural questions that remain are

1. _Can we break the small cluster barrier without making the first assumption on the existence of a gap between the sizes of some clusters?_
2. _Can we remove the dependency of the size of the recoverable cluster on the number \(k\) of clusters? In particular, when \(k\), can we still recover a cluster of size \(()\)?_

The above questions are inherently related to the clustering problem under the faulty oracle model which was recently proposed by Mazumdar and Saha , as an instance from the faulty oracle model is exactly the graph that is sampled from SBM with corresponding parameters. Thus, it is natural to ask _if one can advance the state-of-the-art algorithm for recovering large clusters for the graph instance from the faulty oracle model using an improved algorithm for the SBM?_

### Our contributions

We affirmatively answer all three questions mentioned above. Specifically, we demonstrate that clusters of size \(()\) can be successfully recovered in both the standard SBM and the faulty oracle model, _regardless of_ the number of clusters present in the graph. This guarantee surpasses any previous achievements in related studies. The practical implications of this finding are significant since real-world networks often exhibit a substantial number of clusters (see e.g. ), varying in size from large to small.

#### 1.1.1 Recovering large clusters in the SBM

We first provide a singular value decomposition (SVD) based algorithm, _without_ assuming there is a gap between the sizes of some clusters, for recovering large latent clusters. Furthermore, the recoverability of the largest cluster is unaffected by the number of underlying clusters.

**Theorem 1.2** (Recovering one large cluster).: _Let \(G\) be a graph that is generated from the SBM(\(n,k,p,q\)) with \(=(,)\). If both of the following conditions are satisfied: (1) the size of the largest cluster, denoted by \(s_{}\), is at least \(s^{*}:=}{(p-q)}\); (2) \(^{2}=( n/n)\). There exists a polynomial time algorithm that exactly recovers a cluster of size at least \(}{7}\) with probability \(1-}\)._

We have the following remarks about Theorem 1.2. (1) By the assumption that \(^{2}=( n/n)\), we obtain that \(p=()\), which further implies that the expected degrees are at least logarithmic in \(n\). This is necessary as exact recovery in SBM requires the node degrees to be at least logarithmic even in the balanced case (i.e. when all the clusters have the same size; see e.g. ). (2) In contrast to the work , our algorithm breaks the small cluster barrier and improves upon the result of  in the following sense: we do not need to assume there is a large interval such that the sizes of clusters do not fall in, nor do our bounds get affected with increasing number of small clusters. (3) As a byproduct of Theorem 1.2, we give an algorithm that improves a result of  on partially recovering clusters in the SBM in the balanced case. We refer to Appendix C for details.

In addition, the tradeoff of the parameters in our algorithm in Theorem 1.2 is nearly optimal up to polylogarithmic factors for constant \(p\) and \(q\) under the _planted clique conjecture_ (see Appendix D).

**Recovering more clusters**. We can apply the above algorithm to recover even more clusters, using a "peeling strategy" (see ). That is, we first recover the largest cluster (under the preconditions of Theorem 1.2), say \(V_{1}\). Then we can remove \(V_{1}\) and all the edges incident to them and obtain the induced subgraph of \(G\) on the vertices \(V^{}:=V\{V_{1}\}\), denoting it as \(G^{}\). Note that \(G^{}\) is a graph generated from SBM(\(n^{},k-1,p,q\)) where \(n^{}=n-|V_{1}|\). Then we can invoke the previous algorithm on \(G^{}\) to find the largest cluster again. We can repeat the process until the we reach a point where the recovery conditions no longer hold on the residual graph. Formally, we introduce the following definition of _prominent_ clusters.

**Definition 1.3** (Prominent clusters).: _Let \(V_{1},,V_{k}\) be the \(k\) latent clusters and \(s_{1},,s_{k}\) be the size of the clusters. WLOG we assume \(s_{1} s_{k}\). Let \(k^{} 0\) be the smallest integer such that one of the following is true. (1) \(s_{k^{}+1}<+1}^{k} s_{i}}}{(p-q)}\), (2) \(^{2}<(_{i=k^{}+1}^{k}s_{i})/(_{i=k^{}+1}^{k}s_{i})\). We call \(V_{1},,V_{k^{}}\)_ prominent _clusters of \(V\)._

By the above definition, Theorem 1.2, and the aforementioned algorithm, which we call RecursiveCluster, we can efficiently recover all these prominent clusters.

**Corollary 1.4** (Recovering all the prominent communities).: _Let \(G\) be a graph that is generated from the SBM(\(n,k,p,q\)) model. Then there exists a polynomial time algorithm RecursiveCluster that correctly recovers all the prominent clusters of \(G\), with probability \(1-o_{n}(1)\)._

**Experimental Comparisons.** We evaluate the performance of our algorithm in the simulation settings outlined in  and confirm its effectiveness. Moreover, the experiments conducted in  established that their gap constraint is an observable phenomenon. We demonstrate that our algorithm can accurately recover clusters even without this gap constraint. Specifically, we succeed in identifying large clusters in scenarios where there were \((n)\) single-vertex clusters, a situation where the guarantees provided by  are inadequate. We observed that simpler spectral algorithms, such as , also failed to perform well in this scenario. Furthermore, we observe that the run-time of our algorithm is significantly faster than the SDP based approach of . Finally, we present empirical evidence of the efficacy of our techniques beyond their theoretical underpinnings.

#### 1.1.2 An algorithm for clustering with a faulty oracle

We apply the above algorithm to give an improved algorithm for a clustering problem in a faulty oracle model, which was proposed by . The model is defined as follows:

**Definition 1.5**.: _Given a set \(V=[n]:=\{1,,n\}\) of \(n\) items which contains \(k\) latent clusters \(V_{1},,V_{k}\) such that \(_{i}V_{i}=V\) and for any \(1 i<j k\), \(V_{i} V_{j}=\). The clusters \(V_{1},,V_{k}\) are unknown. We wish to recover them by making pairwise queries to an oracle \(\), which answers if the queried two vertices belong to the same cluster or not. This oracle gives correct answer with probability \(+\), where \((0,1)\) is a bias parameter. It is assumed that repeating the same question to the oracle \(\), it always returns the same answer5._

Our goal is to recover the latent clusters _efficiently_ (i.e., within polynomial time) with high probability by making as few queries to the oracle \(\) as possible. One crucial limitation of all the previous polynomial-time algorithms ([23; 21; 27; 20; 14]) that make sublinear6 number of queries is that they _cannot_ recover large clusters, if there are at least \((n^{2/5})\) small clusters. Now we present our result for the problem of clustering with a faulty oracle.

**Theorem 1.6**.: _In the faulty oracle model with parameters \(n,k,\), there exists a polynomial time algorithm \((s)\), such that for any \(n s^{2}n}{}\), it recovers all clusters of size larger than \(s\) by making \((^{2}n}{^{4} s^{4}}+^{2}n} {s^{2}})\) queries in the faulty oracle model._

We remark that our algorithm works without the knowledge of \(k\), i.e., the number of clusters. Note that Theorem 1.6 says even if there are \((n)\) small clusters, our efficient algorithm can still find all clusters of size larger than \(( n}{})\) with sublinear number of queries. We note that the size of clusters that our algorithm can recover is nearly optimal under the planted clique conjecture. Due to space constraints, all the missing algorithms, analyses, and proofs are deferred to Appendix E and F.

### Our techniques

Now we describe our main idea for recovering the largest cluster in a graph \(G=(V,E)\) that is generated from SBM(\(n,k,p,q\)).

Previous SBM algorithmsThe starting point of our algorithm is a Singular Value Decomposition (SVD) based algorithm by , which in turn is built upon the seminal work of . The main idea underlying this algorithm is as follows: Given the adjacency matrix \(A\) of \(G\), project the columns of \(A\) to the space \(A_{k}\), which is the subspace spanned by the first \(k\) left singular vectors of \(A_{k}\). Then it is shown that for appropriately chosen parameters, the corresponding geometric representation of the vertices satisfies a _separability_ condition. That is, there exists a number \(r>0\) such that 1) vertices in the same cluster have a distance at most \(r\) from each other; 2) vertices from different clusters have a distance at least \(4r\) from each other. This is proven by showing that each projected point \(P_{}\) is close to its center, which is point \(\) corresponding to a column in the expected adjacency matrix \([A]\). There are exactly \(k\) centers corresponding to the \(k\) clusters. Then one can easily find the clusters according to the distances between the projected points.

The above SVD-based algorithm aims to find all the \(k\) clusters at once. Since the distance between two projected points depends on the sizes of the clusters they belong to, the parameter \(r\) is inherently related to the size \(s\) of the smallest cluster. Slightly more formally, in order to achieve the above separability condition, the work  requires that the minimum distance (which is roughly \((p-q)\)) between any two centers is at least \(()\), which essentially leads to the requirement that the minimum cluster size is large, say \(()\), in order to recover all the \(k\) clusters.

High-level idea of our algorithmIn comparison to the work , we do not attempt to find all the \(k\) clusters at once. Instead, we focus on finding large clusters, one at a time. As in , we first project the vertices to points using the SVD. Then instead of directly finding the "perfect" clusters from the projected points, we first aim to find a set \(S\) that is somewhat close to a latent cluster that is large enough. Formally, we introduce the following definition of \(V_{i}\)_-plural_ set.

**Definition 1.7** (Plural set).: _We call a set \(S V\) as a \(V_{i}\)-plural set if (1) \(|S V_{i}| 2^{13} n\); (2) For any \(V_{j} V_{i}\) we have \(|S V_{j}| 0.1|S V_{i}|\)._That is, a plural set contains sufficiently many vertices from one cluster and much fewer vertices from any other cluster.

Recall that \(s^{*}:=}{(p-q)}\) for \(C=2^{13}\), and \(s_{} s^{*}\). We will find a \(V_{i}\)-plural set for any cluster \(V_{i}\) that is large enough, i.e., \(|V_{i}|}{7}\). To recover large clusters, our crucial observation is that it suffices to separate vertices of one large cluster from other _large_ clusters, rather than trying to separate it from all the other clusters. This is done by setting an appropriate distance threshold \(L\) to separate points from any two different and _large_ clusters. Then by refining Vu's analysis, we can show that for any \(u V_{i}\) with \(|V_{i}|}{7}\), the set \(S\) that consists of all vertices whose projected points belong to the ball surrounding \(u\) with radius \(L\) is a \(V_{i}\)-plural set, for some appropriately chosen \(L\). It is highly non-trivial to find such a radius \(L\). To do so, we carefully analyze the geometric properties of the projected points. In particular, we show that the distances between a point and its projection can be bounded in terms of the \(k^{}\)-th largest eigenvalue of the expected adjacency matrix of the graph (see Lemma 2.2), for a carefully chosen parameter \(k^{}\). To bound this eigenvalue, we make use of the fact that \(A\) is a sum of many rank \(1\) matrices and Weyl's inequality (see Lemma 2.3). We refer to Section 2 for more details.

Now suppose that the \(V_{i}\)-plural set \(S\) is independent of the edges in \(V V\) (which is _not_ true and we will show how to remedy this later). Then given \(S\), we can run a statistical test to identify all the vertices in \(V_{i}\). To do so, for any vertex \(v V\), observe that the subgraph induced by \(S\{v\}\) is also sampled from a stochastic block model. For each vertex \(v V_{i}\), the expected number of its neighbors in \(S\) is

\[p|S V_{i}|+q|S V_{i}|=q|S|+(p-q)|S V_{i}|.\]

On the other hand, for each vertex \(u V_{j}\) for some different cluster \(V_{j} V_{i}\), the expected number of its neighbors in \(S\) is

\[p|S V_{j}|+q|S V_{j}|=q|S|+(p-q)|S V_{j}| q |S|+(p-q) 0.1|S V_{i}|,\]

since \(|S V_{j}| 0.1|S V_{i}|\) for any \(V_{j} V_{i}\). Hence there exists a \(((p-q)|S V_{i}|)\) gap between them. Thus, as long as \(|S V_{i}|\) is sufficiently large, with high probability, we can identify if a vertex belong to \(V_{i}\) or not by counting the number of its neighbors in \(S\).

To address the issue that the set \(S\) does depend on the edge set on \(V\), we use a two-phase approach: that is, we first randomly partition \(V\) into two parts \(U,W\) (of roughly equal size), and then find a \(V_{i}\)-plural set \(S\) from \(U\), then use the above statistical test to find all the vertices of \(V_{i}\) in \(W\) (i.e., \(V U\)), as described in IdentifyCluster(\(S,W,\)) (i.e. Algorithm 4).

Note that the output, say \(T_{1}\), of this test is also \(V_{i}\)-plural set. Then we can find all vertices of \(V_{i}\) in \(U\) by running the statistical test again using \(T_{1}\) and \(U\), i.e., invoking IdentifyCluster(\(T_{1},U,\)). Then the union of the outputs of these two tests gives us \(V_{i}\). We note that there is correlation between \(T_{1}\) and \(U\), which makes our analysis a bit more involved. We solve it by taking a union bound over a set of carefully defined bad events; see the proof of Lemma 2.7.

### Other related work

In  (which improves upon ), the author also gave a clustering algorithm for SBM that recovers a cluster at a time, while the algorithm only works under the assumption that all latent clusters are of size \(()\), thus they do not break the "small cluster barrier".

The model for clustering with a faulty oracle captures some applications in _entity resolution_ (also known as the _record linkage_) problem [16; 24], the signed edges prediction problem in a social network [22; 26] and the correlation clustering problem . A sequence of papers has studied the problem of query-efficient (and computationally efficient) algorithms for this model [23; 21; 27; 20; 14]. We refer to references [23; 21; 27] for more discussions of the motivations for this model.

## 2 The algorithm in the SBM

We start by giving a high-level view of our algorithm (i.e., Algorithm 1). Let \(G=(V,E)\) be a graph generated from \((n,k,p,q)\). For a vertex \(v\) and a set \(T V\), we let \(N_{T}(v)\) denote the number of neighbors of \(v\) in \(T\).

We first preprocess (in Line 1) the graph \(G\) by invoking Algorithm 2 Preprocessing, which randomly partitions \(V\) into four subsets \(Y_{1},Y_{2},Z,W\) such that each vertex is added to \(Y_{1},Y_{2},Z,W\)with probability \(1/8,1/8,1/4,1/2\), respectively. Let \(Y=Y_{1} Y_{2},U=Y Z\). See Figure 1 for a visual presentation of the partition. Let \(\) (resp. \(\)) be the bi-adjacency matrix between \(Y_{1}\) (resp. \(Y_{2}\)) and \(Z\). This part is to reduce the correlation between some random variables in the analysis, similar to in  and . Then we invoke (in Line 2) Algorithm 3 EstimatingSize to estimate the size of the largest cluster. It first samples \( n\) vertices from \(Y_{2}\) and then counts their number of neighbors in \(W\). These counters allow us to obtain a good approximation \(\) of \(s_{}\).

We then repeat the following process to find a large cluster (or stop when the number of iterations is large enough). In Line 4-7, we sample a vertex \(u Y_{2}\) and consider the column vector \(}\) corresponding to \(u\) in the bi-adjacency matrix \(\) between \(Y_{2}\) and \(Z\). Then we consider the projection \(P_{_{}}}\) of \(}\) onto the subspace of the first \(k^{}\) singular vectors of \(\) for some appropriately chosen \(k^{}\), and the set \(S\) of all vertices \(v\) in \(Y_{2}\) whose projections are within distance \(L/20\) from \(}\), for some parameter \(L\). In Lines 9-15, we give a process that completely recovers a large cluster when \(S\) is a plural set. More precisely, we first test if \(|S|/21\) and if so, we invoke Algorithm 4 to obtain \(T_{1}=(S,W,)\), which simply defines \(T_{1}\) to be the set of all vertices \(v W\) with \(N_{S}(v) q|S|+(p-q)}{56}\). Then we check (Line 10) if the set \(T_{1}\) satisfies a few conditions to test if \(u\) is indeed a good center (so that \(S\) is a plural set) and test if \(T_{1}=V_{1} W\). If so, we then invoke \((T_{1},U,)\) to find \(V_{1} U\). Note that we use a two-step process to find \(V_{1}\), as \(N_{S}(u)\) is not a sum of independent events for \(u U\).

```
1:\(,,Y_{2},Y_{1},Z,W(G,p,q)\)
2:\((G,p,q,W,Y_{2})\)
3:for\(i=1,,h= n\)do
4: sample a vertex \(u\) from \(Y_{2}\)
5:\(\)the column vector consisting of the edges between \(u\) and \(Z\)
6:\(}\)\(P_{_{}}}\), the projection of \(}\) onto the subspace of the first \(k^{}\) singular vectors of \(\), where \(k^{}=(p-q)/\)
7:\(S\{v Y_{2}\): \(|}-}|\}\), where \(} P_{_{_{}}}}\) and \(L=(p-q)}\)
8:if\(|S|}{21}\)then
9: Invoke \((S,W,)\) to get set \(T_{1}\)
10:if\(|T_{1}|}{6}\) or \( v T_{1}\) s.t. \(N_{T_{1}}(v)(0.9p+0.1q)|T_{1}|\) or \( v W T_{1}\) s.t. \(N_{T_{1}}(v)(0.9p+0.1q)|T_{1}|\)then
11: continue
12:else
13: Invoke \((T_{1},U,|T_{1}|)\) to obtain a set \(T_{2}\)
14: Merge the two sets to form \(T=T_{1} T_{2}\)
15: Return \(T\).
16: Return \(\) ```

**Algorithm 1**Cluster(\(G=(V,E),p,q\)): Recovering one large cluster

### The analysis

We first show that EstimatingSize outputs an estimator \(\) approximating the size of the largest cluster within a factor of \(2\) with high probability.

**Lemma 2.1**.: _Let \(\) be as defined in Line 6 of Algorithm 3. Then with probability \(1-n^{-8}\) we have \(0.48 s_{} 0.52 s_{}\)._

Recall that \(\) (resp. \(\)) is the bi-adjacency matrix between \(Y_{1}\) (resp. \(Y_{2}\)) and \(Z\). Let \(A\) and \(B\) be the corresponding matrices of expectations. That is, \(=A+E\), where \(E\) is a random matrix consisting of independent random variables with \(0\) means and standard deviations either \(\) or \(\).

For a vertex \(u Y_{1}\), let \(}\) and \(\) represent the column vectors corresponding to \(u\) in the matrices \(\) and \(A\) respectively (We define analogous notations for \(\) and \(B\) when \(u Y_{2}\).). We let \(e_{u}:=}-\), i.e., \(e_{u}\) is the random vector with zero mean in each of its entries. Recall that \(}=P_{_{_{}}}}\).

Now we bound the distance between \(P_{_{k^{}}}}\) and the expectation vector \(\). We set \(=0.002\) in the following.

**Lemma 2.2**.: _Follows the setting of Algorithm 2, we fix \(Y_{1},Y_{2},Z,W\). For any vector \(u Y_{2}\) and \(k^{} 1\) we have \(\|P_{_{k^{}}}(})-\|}}\|(P_{_{k^{}}}-I)A\|+\|P_{_{k^{ }}}(e_{u}).\|\)_

_Furthermore, for some constant \(C_{2}\), and \(\) as described above we have_

1. \(\|(P_{_{k^{}}}-I)A\|=\|(P_{_{k^{}}}-I)-(P_{_{k^{}}}-I)E\| 2C_{2}+_{k^{ }+1}(A)\) _with probability_ \(1-(n^{-3})\) _for a random_ \(\)_, where_ \(_{t}(A)\) _is the_ \(t\)_-th largest sigular value of_ \(A\)_._
2. _For any set_ \(V^{} Y_{2}\) _s.t._ \(|V^{}|}\)_, with probability_ \(1-n^{-8}\)_, we have_ \(\|P_{_{k^{}}}(e_{u})\|}\) _for at least_ \((1-2)\) _fraction of the points_ \(u V^{}\)_._

We have the following result regarding the \(t\)-th largest singular value \(_{t}(A)\) of \(A\).

**Lemma 2.3**.: _For any \(t>1\), \(_{t}(A)(p-q)n/t\)._

Now we introduce the a definition of good center, the ball of which induces a plural set.

**Definition 2.4** (Good center).: _We call a vector \(}\) a good center if it belongs to a cluster \(V_{i}\) such that \(|V_{i}|}{4}\) and \(\|P_{_{k^{}}}(e_{u})\| }\)._

That is, a good center is a vertex that belongs to a large cluster and has a low \(_{2}\) norm after the projection. Then by Lemma 2.2, we have the following corollary on the number of good centers.

**Corollary 2.5**.: _If \(s_{} 16 n\), then with probability \(1-n^{-8}\) there are \((1-2) s_{}\) many good centers in \(V\)._

This implies that if we sample \(}\) many vertices independently at random, we shall sample a good center with probability \(1-n^{-8}\).

Good center leads to plural setWe show that if at line 4 a good center from a cluster \(V_{i}\) is chosen, then the set \(S\) formed in line 7 is a \(V_{i}\)-plural set. Recall that \(L=(p-q)}\). Let \(L_{}:=L\).

**Lemma 2.6**.: _Let \(u\) be a good center belonging to \(V_{i} Y_{2}\) and \(S=\{v Y_{2}:\|}-}\| L_{}/20\}\). Then it holds with probability \(1-(n^{-3})\) that \(|V_{i} S|/21\) and for any other cluster \(V_{}\) with \( i\), \(|S V_{}| 1.05\). Thus \(S\) is a \(V_{i}\) plural set as \(1/21 1/10 1.05\)._

**Plural set leads to cluster recovery** We now prove that given a plural set for a large cluster \(V_{i}\), we can recover the whole cluster. This is done by two invocations of Algorithm 4.

**Lemma 2.7**.: _Let \(U,W\) be the random partition as specified in Algorithm 1. Let \(S Y_{2}\) be the \(V_{i}\)-plural set where \(|V_{i}| s_{}/4\). Let \(T_{1}:=(S,W,)\) and \(T:=T_{1}(T_{1},U,)\). Then with probability \(1-(n^{-3})\), it holds that \(T_{1}=V_{i} W\), \(T_{1}}{6}\) and \(T=V_{i}\)._

**Testing if \(T_{1}\) is a sub-cluster** Since \(S\) may not be a plural set, we show that we can test if \(T_{1}=W V_{i}\) for some large cluster \(V_{i}\) using the conditions of Line 10 of Algorithm 1.

**Lemma 2.8**.: _Let \(v\) be a good center from \(V_{i} Y_{2}\) such that \(|V_{i}|}{4}\) and let \(S=\{u Y_{2}:\|}-}\|}{30}\}\). Let \(T_{1}\) be the set returned by IdentifyCluster(\(S,W,\)). Then with probability at least \(1-n^{-8}\), \(|T_{1}|}{6}\) and \(N_{T_{1}}(u)(0.9p+0.1q)|T_{1}|\) for any \(u T_{1}\) and \(N_{T_{1}}(u)(0.9p+0.1q)|T_{1}|\) for any \(u W T_{1}\)._

Finally, we show that if the set \(T_{1} V_{i} W\) for some large cluster \(V_{i}\), then it satisfies one of the conditions at line 10 of Algorithm 1. Together with the previous results this guarantees correct recovery of a large set at every round.

**Corollary 2.9**.: _Let \(T_{1}=(S,W,)\) be a set such that \(T_{1} V_{i} W\) for any underlying community \(V_{i}\) of size \(|V_{i}| s_{}/7\). Then with probability \(1-n^{-8}\) either \(|T_{1}|}{6}\) or there is a vertex \(u T_{1}\) such that \(N_{T_{1}}(u)(0.9p+0.1q)|T_{1}|\)._

**Remark 2.10**.: _Note that in Lemma 2.8 and Corollary 2.9, the quantity \(N_{T_{1}}(u)\) for any \(u T_{1}\) is a sum of independent events. This is because the event that a vertex in \(v W\) is chosen in \(T_{1}\) is solely based on \(N_{u}(S)\), where \(S T=\). Thus, for any \(u_{1},u_{2} T\), there is an edge between them (as per underlying cluster identities) independent of other edges in the graph._

The proofs of the above results are deferred to Appendix 2.1.

Now we are ready to prove Theorem 1.2.

**Proof of Theorem 1.2** By the precondition, we have that \(s_{} s^{*}\). First, in Line 2, Lemma 2.1 guarantees that \(0.48s_{} 0.52s_{}\). By Corollary 2.5 and the fact that we iteratively sampled vertices \(( n)\) times, with probability \(1-n^{-8}\), one such vertex \(u\) is a good center. Given such a good center, by Lemma 2.6, we know with probability \(1-(n^{-3})\), a \(V_{i}\)-plural set is recovered on Line 7. Then by Lemma 2.7, given such a \(V_{i}\)-plural set, the two invocations of Identifycluster recovers the cluster \(V_{i}\) with probability \(1-(n^{-3})\). Furthermore, Lemma 2.8 shows that if the sampled vertex \(\) is a good center, then with probability \(1-n^{-8}\) none of the conditions of line 10 are satisfied, and we are able to recover a cluster. On the other hand, Corollary 2.9 shows that if \(T_{1} V_{i} W\) for any large cluster \(V_{i}\), (\(V_{i}:|V_{i}| s_{}/7\)) then one of the conditions of line 10 is satisfied with probability \(1-n^{-8}\) and the algorithm goes to the next iteration to sample a new vertex in line 4. Taking a union bound on all the events for at most \(( n)\) iterations guarantees that algorithm 1 finds a cluster of size \(s_{}/7\) with probability \(1-(n^{-2})\). This completes the correctness of Algorithm 1.

## 3 The algorithm in the faulty oracle model

We describe the main ideas of our algorithm NoisyClustering for clustering with a faulty oracle. Let \(V\) be the set of items that contains \(k\) latent clusters \(V_{1},,V_{k}\) and \(\) be the faulty oracle. Following the idea of , we first sample a subset \(T V\) of appropriate size and query \((u,v)\) for all pairs \(u,v T\). Then apply our SBM clustering algorithm (i.e. Algorithm 1 Cluster) on the graph (with all the edges for the pairs that are reported to belong to the same cluster) induced by \(T\) to obtain clusters \(X_{1},,X_{t}\) for some \(t k\). We can show that each of these sets is a subcluster of some large cluster \(V_{i}\). Then we can use majority voting to find all other vertices that belong to \(X_{i}\), for each \(i t\). That is, for each \(X_{i}\) and \(v V\), we check if the number of neighbors of \(v\) in \(X_{i}\) is at least \(|}{2}\). In this way, we can identify all the large clusters \(V_{i}\) corresponding to \(X_{i}\), \(1 i t\)Furthermore, we can just choose a small subset of \(X_{i}\) of size \(O(})\) for majority voting to reduce query complexity. Then we can remove all the vertices in \(V_{i}\)'s and remove all the edges incident to them from both \(V\) and \(T\) and then we can use the remaining subsets \(T\) and \(V\) and corresponding subgraphs to find the next sets of large clusters. The algorithm NoisyClustering then recursively finds all the large clusters until we reach a point where the recovery condition on the current graph no longer holds. The pseudocode and the analysis of NoisyClustering are deferred to Appendix F.

## 4 Experiments

Now we exhibit various properties of our algorithms by running it on several unbalanced SBM instantiations and also compare our improvement w.r.t the state-of-the-art. We start by running our algorithm RecursiveCluster on the instances used by the authors of . WLOG, we assume that \(|V_{1}||V_{2}||V_{k}|\). We denote the algorithm in  by ACX.

Comparison with ACXIn Exp-\(1\) (abbreviated for Experiment #1) and Exp-\(2\), our algorithm recovers the largest cluster while ACX recovers all the clusters. This is because we have a large, _constant_ lower bound on the size of the clusters we can recover. If we scale up the size of the clusters by a factor of \(20\) in those instances, then we are also able to recover all clusters.

**Overcoming the gap constraint in practice** Exp-\(3\) is the "mid-size-cluster" experiment in . In this case, ACX recovers the largest cluster completely, but only some fraction of the second-largest cluster, which is an incorrect outcome. In , the authors used this experiment to emphasize that their "gap-constraint" is not only a theoretical artifact but also observable in practice. In comparison, we recover the largest cluster while do not make any partial recovery of the rest of the clusters. In Exp-\(4\), we modify the instance in Exp-\(3\) by changing the size of the second cluster to \(200\). Note that this further reduces the gap, and ACX fails in this case as before. In comparison, we are able to recover both the largest and the second largest cluster. This exhibits that we are indeed able to overcome the experimental impact of the gap constraint observed in  in the settings of Table 1.

We then run some more experiments in the settings of Table 2 to describe other properties of our algorithms as well as demonstrate the practical usefulness of our "plural-set" technique.

**Many clusters** Exp-\(5\) covers a situation where \(k=(n)\) (specifically \(n/3\)), which can not be handled by ACX, as the size of the recoverable cluster in  is lower bounded by \(k n/(p-q)^{2}>n\). In comparison, our algorithm can recover the two main clusters. We also remark, in this setting, the spectral algorithm in  with \(k=1000\) can not geometrically separate the large clusters.

**Recovery of small clusters** Exp-\(6\) describes a situation where the peeling strategy successfully recovers clusters that were smaller than \(\) in the original graph. Once the largest cluster is removed, the smaller cluster then becomes recoverable in the residual graph. Finally, we discuss the usefulness of the plural set.

**Run-time comparison** Here, note that our method is a combination of a \((p-q)/\) dimensional SVD projection, followed by some majority voting steps. Furthermore, we have

   Exp. \# & \(n\) & \(p,q\) & \(k\) & Cluster sizes & Recovery by us & Recovery by \\  & & & & & & & \\
2 & \(3200\) & \(0.8,0.2\) & \(5\) & \(\{800,200,200,50,50\}\) & Largest cluster & All clusters \\
3 & \(750\) & \(0.8,0.2\) & \(4\) & \(\{500,150,70,30\}\) & Largest cluster & _Incorrect_ \\
4 & \(800\) & \(0.8,0.2\) & \(4\) & \(\{500,200,70,30\}\) & Two largest clusters & Recovery \\   

Table 1: Comparing RecursiveCluster with ACX 

   Exp. \# & \(n\) & \(p,q\) & \(k\) & Cluster sizes & Recovery by us \\ 
5 & \(2900\) & \(0.7,0.3\) & \(1000\) & \(\{1000,903\}\{1\}_{i=1}^{997}\) & Large clusters \\
6 & \(12300\) & \(0.85,0.15\) & \(4\) & \(\{12000,100,100,100\}\) & All clusters \\   

Table 2: Further Evaluation of RecursiveCluster\((p-q)/ 2\). This implies that the time complexity of our algorithm is \((2 n^{2.5})\). In comparison, the central tool used in the algorithms by  is an SDP relaxation, which scales as \((n^{3})\). This implies that the asymptotic time complexity of our method is also an improvement on the state-of-the-art. We also confirm that the difference in the run-time becomes observable even for small values of \(n\). For example, our algorithm recovers the largest cluster in Experiment 1 (with \(n=1100\)) of table 1 in 1.4 seconds. In comparison,  recovers all \(4\) clusters, but takes 44 seconds.

**On the importance of plural sets** Recall that in Algorithm 1 (which is the core part of RecursiveCluster), we first obtain a plural-set \(S\) in the partition \(Y_{2}\) of \(V\) (see Figure 1 to recall the partition). \(S\) is not required to be \(V_{i} Y_{2}\) for any cluster \(V_{i}\), but the majority of the vertices in \(S\) must belong to a large cluster \(V_{i}\) (which is the one we try to recover). We have the following observations:

1. In Exp-3 of Table 1, in the first round we recover a cluster \(V_{1}\). Here in our first step, we recover a plural set \(S\), where \(S V_{1} Y_{2}\). That is, we _do not recover_ all the vertices of \(V_{1}\) in \(Y_{2}\) when forming the plural-set.
2. In Exp-4 of Table 1, in the second iteration we recover a cluster \(V_{2}\). However, **the plural set \(S V_{2}\), and in fact contains a few vertices from \(V_{4}\)?** This is in fact the exact situation that motivates the plural-set method.

In both cases, the plural-set is then used to recover \(S_{1}:=V_{1} W\) and \(V_{2} W\) respectively, and then \(S_{1}\) is used to recover the vertices of the corresponding cluster in \(U\). Thus, our technique enables us to _completely_ recover the largest cluster even though in the first round we may have some misclassifications. A more thorough empirical understanding of the Plural sets in different applications is an interesting future work.

We conclude our paper with some more discussion and future directions.

## 5 Conclusion

In this work, we design a spectral algorithm that recovers large clusters in the SBM model in the presence of arbitrary numbers of small clusters and compared to previous work, we do not require gap constraint in the size of consecutive clusters. Some interesting directions that remain open are as follows.

1. We note that both our algorithm and  require knowledge of the probability parameters \(p\) and \(q\) ( also need the knowledge of \(k\), the number of clusters). Thus, whether parameter-free community recovery algorithms can be designed with similar recovery guarantees is a very interesting question.
2. Both our result (Algorithm 1) as well as  have a multiplicative \( n\) term in our recovery guarantees. In comparison, the algorithm by Vu , which is the state-of-the-art algorithm in the dense case when "all" clusters are large, only has an additive logarithmic term. This raises the question of whether this multiplicative logarithmic factor can be further optimized when recovering large clusters in the presence of small clusters.

Additionally, we note that the constant in our recovery bound is quite large (\(2^{13}\)), and we have not made efforts to optimize this constant. We believe this constant can be optimized significantly, such as through a more careful calculation of the Chernoff bound in Theorem A.2.