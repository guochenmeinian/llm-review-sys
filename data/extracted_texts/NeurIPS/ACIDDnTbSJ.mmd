# Feint Behaviors and Strategies:

Formalization, Implementation and Evaluation

 Junyu Liu

Brown University

liu_junyu@brown.edu &Xiangjun Peng

The Chinese University of Hong Kong

xjpeng@cse.cuhk.edu.hk

###### Abstract

Feint behaviors refer to a set of deceptive behaviors in a nuanced manner, which enable players to obtain temporal and spatial advantages over opponents in competitive games. Such behaviors are crucial tactics in most competitive multi-player games (e.g., boxing, fencing, basketball, motor racing, etc.). However, existing literature does not provide a comprehensive (and/or concrete) formalization for Feint behaviors, and their implications on game strategies. In this work, we introduce the first comprehensive formalization of Feint behaviors at both action-level and strategy-level, and provide concrete implementation and quantitative evaluation of them in multi-player games. The key idea of our work is to (1) allow automatic generation of Feint behaviors via Palindrome-directed templates, combine them into meaningful behavior sequences via a Dual-Behavior Model; (2) concertize the implications from our formalization of Feint on game strategies, in terms of temporal, spatial and their collective impacts respectively; and (3) provide a unified implementation scheme of Feint behaviors in existing MARL frameworks. The experimental results show that our design of Feint behaviors can (1) greatly improve the game reward gains; (2) significantly improve the diversity of Multi-Player Games; and (3) only incur negligible overheads in terms of time consumption.

## 1 Introduction

In most real-world Multi-Player Games (e.g., boxing, basketball, motor racing, and etc.), players exhibit complex behaviors, which imply hard-to-be-quantified interactions. Simulating these games requires to model the players' behaviors into action spaces as the elements (i.e., which we denote such a decision as "action-level"), and explore strategies based on these elements . As one of the most common behaviors in real-world games, Feint behaviors represent a class of tactic behaviors, which are used to mislead opponents to obtain temporally-strategic advantages1. Such behaviors generally exhibit nuanced differences with normal behaviors, in terms of action movements2; but these behaviors can help the agent to obtain strategic advantages to a considerable extent3. However, no existing literature provides a comprehensive formalization (and/or concrete modeling of) Feint behaviors, at either action level or strategy level, and we justify this claim below.

* To the best of our knowledge,  is the first work to mention Feint behaviors, as a proof-of-concept to construct animations for nuanced behaviors in two-player boxing games, based on the unpredictability introduced in stochastic and simultaneous decision making.
* A more recent work  learn control strategies to animate nuanced agent behaviors like Feint in gaming environments, from motion clips using Deep Reinforcement Learning.

**Summary.** Prior attempts do not provide detailed formalization to address the action-level characteristic of Feint and do not give Feint behavior generation guidelines. As for the strategy-level formalization, existing learning-based works either neglect Feint behaviors or assume that Feint are just glitches from other behaviors, by inducing similar impacts implicitly.4

**Overview.** Our work provides the first comprehensive and concrete formalization of Feint behaviors in action-level and strategy-level. We first present an automatic approach to generate Feint behaviors using **Palindrome-directed Templates** based on our observation on Feint characteristics, and provide **Dual-Behavior Model** to examine the design for the combination of Feint behaviors and normal actions. Based on the action-level formalization, we then model the Feint behavior impacts on strategy-level in terms of the temporal, spatial, and their collective impacts under a learnable scheme. Then, we provide a concrete and unified implementation to incorporate the action-level and strategy-level formalization in common Multi-Player Reinforcement Learning (MARL) frameworks; so that we can showcase the effectiveness of our formalization5.

**Results.** To properly examine the effectiveness of our formalization, we extensively construct a complex and physics-based boxing game as abstraction of some animation-related works [34; 35]. We use a two-player and a six-player scenario with 4 commonly used MARL models (MADDPG , MASAC [11; 13], MATD3 , and MAD3PG [4; 6]) to extensively evaluate our formalization. We also evaluate our formalization of Feint in a strategic real-game, Alpha Star, to examine the diversity gain introduced by our formalization. The results show that our formalization of Feint can significantly increase the gaming rewards in all scenarios with all 4 MARL models. We also extensively examine our approaches: for the Diversity Gain, our method can increase the exploitation of the search space by 1.98X, measured by the Exploitability metrics; and our implementation scheme only incur less than 5% overheads in terms of per game episode time consumption. We conclude that our formalization of Feint behaviors is effective and practical, significantly increasing players' game rewards and making Multi-Player Games more interesting.

## 2 Background

### Feint Behaviors in the Real-World and Simulated Games

Feint behaviors are common for human players, as a set of active actions to obtain strategic advantages in real-world games. Examples can include sports games such as boxing, basketball, and motor racing [10; 9; 12], and electronic games such as King of Fighters and Starcraft [33; 5]. Feint behaviors are not simple deceptive behaviors as their goal is to not to gain rewards for themselves but to create temporal and spatial advantages for some short-term follow-up actions. In addition, Feint behaviors have nuanced action formalizations. Though Feint is undoubtedly important in many real-world games, there still lacks a comprehensive formalization of Feint in Multi-Player Game simulations using Non-Player Characters (NPCs). There are only a limited amount of works to tackle this issue.  is an early example of incorporating Feint as a proof-of-concept, which focuses on constructing animations for nuanced game strategies for more unpredictability from NPCs. More recently,  learn multi-level control strategies of agents via deep reinforcement learning from a set of motion clips including nuanced behaviors like Feint (i.e. in animating combat scenes). However, these prior works (1) lack concrete formalizations of Feint behavior characteristics, which cannot fully unveil the variety of Feint behaviors in the action level; and (2) lack comprehensive explorations of Feint behaviors implications on game strategies, which neglects the potential impacts of fusing effective Feint behaviors into strategies; and (3) solely focus on Two-Player Games, which can not be effectively generalized to multi-player scenarios.

### MARL Models at Strategy-Level in Multi-Player Game Simulations

Multi-Agent Reinforcement Learning (MARL) aims to learn optimal policies for agents in a multi-agent environment, which consists of various agent-agent and agent-environment interactions6. Many single-agent Reinforcement Learning methods (e.g. DDPG , SAC , PPO  and TD3 , D4PG ) can not be directly used in multi-agent scenarios, since the rapidly-changing multi-agent environment can cause highly unstable learning results (evidenced by ). Thus, recent effortson MARL model designs aim to address such an issue.  proposes Counterfactual Multi-Agent (COMA) policy gradients, which uses centralised critic to estimate the Q-function and decentralised actors to optimize agents' policies.  proposes Multi-Agent Deep Deterministic Policy Gradient (MADDPG), which decreases the variance in policy gradient and instability of Q-function of DDPG in multi-agent scenarios.  proposes Multi-Agent Actor-attention critic (MAAC), which applies attention entropy mechanism to enable effective and scalable policy learning. These models can have varied impacts within a diverse set of scenarios.  introduces Multi-agent Distributed Deep Deterministic Policy Gradient (MAD3PG), which extends the D4PG to multi-agent scenarios with distributed critics to enable distributed tracking.  proposes Multi-Agent Twin Delayed Deep Deterministic Policy Gradient (MATD3), which integrates twin delayed Q-learning and addressing the overestimation bias in Q-values in a multi-agent setting. Though different MARL models have different design details, they all share the same high-level learning structure. Thus, our goal is to provide a unified scheme to fuse our formalization of Feint behaviors into game simulations that can be learned using common MARL models, enabling effective Feint behaviors impacts regardless of specific design choices of MARL models.

## 3 Formalizing Feint Behavior

We introduce our formalization of Feint behaviors in action level regarding (1) how to automatically generate Feint behavior with templates from common attack behaviors; and (2) how can the generated Feint behaviors be synergistically combined with follow-up high-reward actions. We first introduce our methodology to automatically generate Feint behaviors, by exploiting our newly-revealed insight called **Palindrome-directed Generation of Feint Templates**. Next, we illustrate key design choices on how to combine the generated Feint behaviors with follow-up actions in a **Double-Behavior Model**, which forms the foundation for the designs of Feint-accounted strategy designs in Section 47.

### Feint Behavior Characteristics and Templates

Since Feint behaviors aim to provide deceptive attacks, they are naturally expected to be derived from a subset of existing attack behaviors. Based on our exploration, we derive two key findings from an extensive amount of attack behaviors. First, most attack behaviors can be decomposed into three action sequences, which are Stretch-out Sequence (Sequence 1), Reward Sequence (Sequence 2), and Retract Sequence (Sequence 3) (an example shown in the first row in Figure 1). We elaborate on each action sequence in detail.

Sequence 1 leads the agent's movements to the Reward Sequence (in boxing, approaching the opponents before actually hitting them); Sequence 2 contains actions that gain game rewards (in boxing, physical contact with the opponents); and Sequence 3 retracts an agent's movements to a relative rest position (in boxing, retracting back to a preparation position for next behaviors). Second, body movements in Sequence 1 and Sequence 3 usually have semi-symmetric yet reverse-order action patterns in the timeline. A behavior usually starts and ends in a similar physical state due to physical restrictions (e.g., bones and muscles stretching restrictions for a humanoid).

The above three-stage decomposition of attack behaviors has motivated a series of constraints, to deliver proper design of Feint generators. To satisfy the above two requirements, we propose a Feint behavior template generator called **Palindrome-directed Generation of Feint Templates**, by extracting subsets of semi-symmetrical actions from an attack behavior and synthesizing them as a Feint behavior. The general method to generate these templates are (1) by extracting subsets of unit actions from an attack behavior, a Feint behavior can be considered as a semi-finished real attack behavior. This ensures the high similarity of a generated Feint behavior with an attack behavior, thus opponents can be deceived; and (2) by synthesizing semi-symmetric action sections, the overall movements can be connected smoothly and the naturalness of humanoid actions can be guaranteed. Within our proposed template generator **Palindrome-directed Generation of Feint Templates**, there are two key adjustable parameters in practice: (1) sequence composition positions for Feint templates; and (2) sequence length for Feint templates. We describe our rationale in appendix B. Figure 1 demonstrates 3 templates for generating Feint behavior templates in boxing games.

### Feint Behavior in Consecutive Game Steps

Standalone Feint behaviors are meaningless in competitive games since the Feint behaviors themselves do not gain rewards. Only by effectively combining Feint behaviors with intended follow-up actionscan showcase their effectiveness. Thus, we define an effective Feint cycle as a **Dual-Behavior Model,** which jointly considers a Feint behavior and its intended follow-up behavior (can be a single action or an action sequence). Our formalization for standalone Feint behaviors (Section 3.1) already provides a large number of possible Feint behaviors. However, not all these morphologically reasonable Feint actions can be directly combined with all high-reward follow-up actions in combating scenarios. Therefore, certain constraints are demanded to construct effective combinations of Feint behaviors and follow-up actions. Hereby, we introduce two major considerations and then propose relevant restrictions, to enable naturalistic and suitable combinations of Feint behaviors and follow-up actions.

(1) **Physical Constraints:** Physical constraints need to be accounted for when synthesizing Feint behaviors and follow-up actions. The ending physical state for a Feint behavior must be a state that is physically possible for an agent to perform the follow-up high-reward actions. For example, if an agent in boxing games finishes Feint behavior with the left foot forward, but the following attack behavior starts with the right foot forwarded, the synthesis of these two behavior is inappropriate since this combination is physically unrealistic.

Figure 1: An example of **Palindrome-directed Generation Templates of Feint behaviors**. The first row shows an action sequence of a cross-punch behavior. Three examples of templates are shown as,, and to demonstrate physically realistic generation of Feint behaviors.

Figure 2: Dual-action Model - high-level abstraction and demonstration of internal stage transitions

To ensure that the combinations of Feint behaviors and follow-up actions obey the physical constraints, we use a Reverse Search Principle which decides the intended follow-up actions (behavior) first and then use the starting physical state of this behavior to search and compose proper Feint behaviors (a more detailed description combined with strategy is described in Section 5). By first selecting an intended follow-up high-reward behavior, the end physical state of the Feint behavior is constrained to be close to the starting physical state of the follow-up behavior. Thus the composition of possible Feint behaviors using the Palindrome-directed templates should aim to start and end at a physical state that is close to the follow-up behavior.

(2) **Effectiveness:** The effectiveness of the incorporation of Feint behaviors is evaluated by whether the following attack actions can successfully gain rewards from the opponent. A successful Feint behavior would usually enable an agent to gain temporal and spatial advantages when performing the follow-up behaviors. Thus, the two design parameters introduced in Section 3.2 play crucial roles in combining Feint with follow-up behaviors. The abstraction of an ideal Dual-Behavior model that can enable an agent with temporal and spatial advantage is illustrated in Figure 2 and a corresponding example is provided in Figure 6. An effective Feint behavior creates temporal advantages that make the opponents to defend in a wrong direction and enable temporal advantages to allow the follow-up high-reward behavior to successfully gain rewards on the opponents.

To ensure the consistency and correctness of the understanding, we provide a detailed demonstration for successful and unsuccessful Feint cases in Appendix C.

## 4 Formalizing Feint Behaviors in Strategies

To effectively fuse the Feint behaviors with Dual-Behavior Model into game interactions, we provide the strategy-level formalization of Feint behaviors. We use Multi-Agent Reinforcement Learning (MARL) schemes to discuss our formalization of Feint behaviors in the strategy level, as MARL provides flexibility in exposing multiple adjustable parameters in learnable policy models. As discussed in generating Feint behaviors (Section 3.1) and composing them in the Dual-Behavior Models (Section 3.2), the key considerations for effective Feint cycle is to enable temporal and spatial advantages for an agent. Thus, our strategy-level formalization centers on how to address the temporal, spatial, and their collective impacts of Feint behaviors with Dual-Behavior Models. A more concrete introduction for fusing of Feint into the MARL frameworks is presented in Section 5.

### The Basic Formalization: Derivation and Limitations

Under commonly used MARL schemes [25; 19], we define a \(K\)-agent _Non-transitive Active Markov Game Model_ as a tuple \( K,,,P,R,,\) is the state space; \(=\{_{i}\}_{i=1}^{K}\) is the set of action space for each agent, where there are no dominant actions; \(P\) performs state transitions of current state by agents' actions: \(P:_{1}_{2}... _{K}()\), where \(()\) denotes the set of probability distribution over state space \(S\); \(R=\{R_{i}\}_{i=1}^{K}\) is the set of reward functions for each agent; \(=\{_{i}\}_{i=1}^{K}\) is the set of policy models for each agent; and \(=\{_{i}\}_{i=1}^{K}\) is the set of policy parameters for each agent's policy model. For simplicity, we use \(i\) to denote an agent from our description perspective and use \(-i\) to refer to all other agents.

We first summarize two major limitations of existing works to justify that they cannot deliver a sufficient formalization of Feint behaviors in Multi-Player Games. Since there is no prior formalization, we discuss relevant works and derive the key features to discuss them in detail.

The basic formalization on temporal impacts is insufficient for Multi-Player Games. Multi-Player Games require agents to account for complex future planning for decision-making, which is critical for deceptive behaviors like Feint [22; 24; 26]. Several works simplify the temporal impacts of deceptive game strategies in different gaming scenarios.  uses a discount factor \(\) to calculate the reward for an agent \(i\) following actions as \(_{t=0}^{}^{t}R^{i}(s_{t},a_{t}^{i},a_{t}^{-i})\) for agent \(i\). However, such a method suffers from the "short-sight" issue , since the weights for future actions' rewards shrink exponentially with time, which are not suitable for all gaming situations (discussed in ). More recently,  applies a long-term average reward for an agent \(i\), to equalize the rewards of all future actions as \(_{t=0}^{T}R^{i}(s_{t},a_{t}^{i},a_{t}^{-i})\). However, such a method is restricted by the "far-sight" issue, since there are no differentiation between near-future and far-future planning. The mismatch between abstraction granularity heavily saddles with the design of Feint, because they use relatively static representations (e.g. static \(\) and \(T\)). Therefore, they cannot be aware of any potential changes of strategies in different phases of a game. Hence, the temporal dimension is simplified hereby.

The basic formalization of spatial impacts are generally in simplified 2-player scenarios only, which cannot be effectively generalized to Multi-Player Game scenarios. Prior works, which attempt to fuse Feint into complete game scenarios, only consider two-player scenarios . However, in Multi-Player (more then two player) Games, gaming strategies (especially deceptive strategies) yield spatial impacts on other agents. Such impacts have been overlooked by all prior works. This is because an agent, who launches the Feint behaviors, can impact not only the target agent but also other agents in the scenario. Therefore, the influences of such an action needs to account for spatial impacts . Moreover, with a new dimension accounted, the interactions between them also raise a potential issue for their mutually collective impacts.

### Our Formalization in a Generalized Game Model

Therefore, to deliver an effective formalization of Feint in Multi-Player Games, it is essential to consider the temporal, spatial and their collective impacts comprehensively. We first discuss the Temporal Dimension, then we elaborate our considerations on Spatial Dimension, and finally we summarize the design for the collective impacts from both temporal and spatial dimensions.

#### 4.2.1 Temporal Dimension: Influence Time

To formalize the temporal impacts of Feint behaviors based on our Palindrome-directed Templates and the Dual-Behavior Model, we use a _Dynamic Short-Long-Term_ manner to emulate them, which differ from the prior works' formalization (Section 4.1). The short-term period refers to a complete Dual-Behavior Model (Section 3.2), including a Feint behavior followed by an intended high-reward behavior led by the Feint. The long-term period is the time steps after this Feint cycle. The rationale behind such a design choice is that: the purpose of Feint is to obtain strategic advantages against the opponent in the temporal dimension, aiming to benefit the follow-up high-reward behavior. Hence, the _Dynamic Short-Long-Term_ temporal impacts of Feint shall be (1) the actions that follow Feint behaviors (e.g. actual attacks) in a short-term period of time should have a strong correlation to Feint ; (2) the actions in the long-term periods explicitly or implicitly depend on the effect of the Feint and its following actions; and (3) for different Dual-Behavior models in different gaming scenarios, the threshold that divides short-term and long-term should be dynamically adjusted to enable sufficient flexibility in strategy making.

For _Dynamic Short-Long-Term_, we use the time-step length of a Dual-Behavior Model \(t_{s}\) as the short-term planning threshold. The short-term (the Dual-Behavior) starts at time step \(t_{0}\) with actions of a Feint behavior \(\{a_{t_{0}}^{i},...,a_{t_{0}+t_{f}}^{i}\}\) and actions of a high-reward behavior \(\{a_{t_{0}+t_{f}+1}^{i},...,a_{t_{0}+t_{s}}^{i}\}\) sampled from Feint policy \(^{{}^{}}\) (\(t_{f}\) denotes the Feint behavior length). We use two different set of scheduler weights on Feint behaviors \(_{Feint}=\{_{t_{0}},...,_{t_{0}+t_{f}}\}\) and the following attack behavior \(_{attack}=\{_{t_{f}},...,_{t_{0}+t_{s}}\}\). \(_{Feint}\) are small weights as regularizers since the Feint behaviors themselves do not intend to gain rewards, while \(_{attack}\) are large weights to emphasize the importance of the intended behaviors that Feint lead to. Thus, we formalize the short-time reward as

\[ Rew_{short}(_{i}^{{}^{}},t_{0},t_{f},t_{s },)=_{t=t_{0}}^{t=t_{0}+t_{f}}_{Feint_{t}}R^{i}(s_{t},a_ {t}^{i},a_{t}^{-i})+_{t=t_{s}}^{t=t_{0}+t_{s}}_{attack_{t}}R^{i}(s_{ t},a_{t}^{i},a_{t}^{-i})\] (1)

, since the purpose of Feint policy \(_{i}^{{}^{}}\) is to actively find effective combinations of Feint behaviors and high-reward behaviors in Dual-Behavior Models that can benefit in a short-term period. We then consider long-term planning after the short-term planning threshold \(t_{s}\): we use a set of discount factor \(=\{_{t_{0}+t_{s}+1},...,_{T}\}\) on the long-term average reward calculation (proposed by ), to distinguish these reward from short-term rewards:

\[ Rew_{long}(_{i}^{{}^{}},t_{0},t_{s},T, )=_{t=t_{0}+t_{s}+1}^{T}_{t}R^{i}(s_{t},a_{t}^{ i},a_{t}^{-i})\] (2)

where \(T\) denotes the end time of the long-term planning threshold.

Finally, we put them together to formalize the _Short-Long-Term_ reward calculation mechanism, when an agent \(i\) plans to perform a Feint action at time \(t_{0}\) with a short-term planning threshold \(t_{s}\) and the end time \(T\) as:

\[ Rew_{temporal}(_{i}^{{}^{}},t_{0},t_{f},t_ {s},T,,)=_{short}& Rew_{short}( _{i}^{{}^{}},t_{0},t_{f},t_{s},)\\ +_{long}& Rew_{long}(_{i}^{{}^{}},t_ {0},t_{s},T,)\] (3)where \(_{short}\) and \(_{long}\) are weights for dynamically balancing the weight of short-term and long-term rewards for different gaming scenarios. \(_{short}\) and \(_{long}\) are initially set as \(0.67\) and \(0.33\) and are adjusted to achieve better performance with the iterations of training.

#### 4.2.2 Spatial Dimension: Influence Range

The spatial advantage of Feint behaviors refers to deceiving the opponents (i.e., deviating the opponents' actions from their policies while exploring new possible advantage game states). In a Multi-Player Game (i.e. usually more than two players), the strict one-to-one relationship between two agents is not realistic, since an agent can impact both the target agent and other agents. Therefore, the influences on all other agents shall maintain different levels . Therefore, our work includes the spatial dimension of Feint impacts by fusing spatial distributions (i.e., joint state-action space distribution). The key idea of this design is to model the influence range of Feint behaviors as the divergence of occupancy measures during policy learning. More specifically, we incorporate Behavioral Diversity from , to mathematically calculate and maximize the diversity gain of Feint behaviors on the influence range.

We follow the occupancy measure introduced in , the distribution of state-action pairs \(_{}(s,)=_{}(s)( s)\), to measure a joint policy \(=(_{i},_{-i})\). \(_{}(s)\) can be calculated by the normalized-weighted sum of game state visit probabilities for the joint policy \(=(_{i},_{-i})\) of all agents. Game state \(s\) can be parameterized by a set of physical properties. We demonstrate a set of commonly used parameters in boxing games : the relative positions \(p(i,-i)\), relative moving orientations \(o(i,-i)\), the linear velocities \((i,-i)\), and angular velocities \((i,-i)\). \(s\) can thus be composed in a vector \(s=(p(i,-1),o(i,-i),(i,-i),(i,-i))\). The spatial domain influence of Feint policy can be naturally represented by the occupancy measure. When players apply Feint behaviors to deceive opponents, the resulting spatial impact is to exploit new possibilities to achieve more advantageous state transitions from the current state. When a Feint policy \(^{{}^{}}_{i}\) is added, we aim to maximize the effective influence range under the influence distribution of Feint. Specifically, we maximize the divergence of the new distribution of joint policy \(^{}=(^{{}^{}}_{i},_{-i})\) introduced by Feint from the distribution of the old one \(=(_{i},_{-i})\) without Feint. By using Behavior Diversity , such a maximization problem at state \(s\) can be formalized as:

\[max_{^{{}^{}}_{i}}Rew_{spatial}(^{{}^{}}_{i},_{-i},s)=D _{f}(_{^{{}^{}}_{i},_{-i}}(s)_{_{i},_{-i}}(s))\] (4)

where we use the divergence as the reward \(Rew_{spatial}\) and the general \(f\)-divergence is used to measure the discrepancy of two distributions.  introduces multiple ways to approximately calculate such divergence for policy models represented by neural networks.

### Collective Impacts: Influence Degree

Solely relying on the Temporal Dimension and Spatial Dimension overlooks the interactions between them, and these two dimensions are expected to have mutual influences for realistic modeling . Therefore, we consider the influence degree for the collective impacts.

We formulate it for a Feint policy \(^{{}^{}}_{i}\) in Multi-Player Games which performs a full Feint cycle (i.e., a complete Dual-Behavior Model and long-term actions) that starts at \(t_{0}\) and ends at \(T\) as:

\[Rew_{collective}(^{{}^{}}_{i},_{-i}) =_{1}_{\{^{{}^{}}_{i},_{-i}\}}Rew_{temporal }(,t_{0},t_{f},t_{s},T,,)\] \[+_{2}_{s=s_{0}}^{s_{T}}Rew_{spatial}(^{{}^{ }}_{i},_{-i},s)\] (5)

where temporal impacts \(Rew_{temporal}\) (Section 4.2.1) for agent \(i\) are are aggregated on spatial domain considering all agents and spatial impacts \(Rew_{spatial}\) (Section 4.2.2) are aggregated on the temporal domain for all the states in the time period. \(_{1}\) and \(_{2}\) denote the weights of aggregated temporal impacts and spatial impacts respectively, enabling flexible adaption to different gaming scenarios. They are initially set as \(0.5\).

In addition to the collective impacts of Feint itself in terms of temporal domain and spatial domain, our formalized impacts of Feint can also result in response diversity of opponents, since different related opponents (spatial domain) at different time steps (temporal domain) can have diverse response. Such Response Diversity can be used as a reward factor that makes the final reward calculation more comprehensive . Thus, to incorporate the Response Diversity together with our final reward calculation model, we refer to  to characterize the diversity gain incurred by our collective impacts formalization. For an agent who maintains a pool of policy \(_{i}=\{^{1}_{i},,^{M}_{i}\}\) while opponents maintain a pool of policy \(_{-i}=\{^{1}_{-i},,^{N}_{-i}\}\), an empirical payoff matrix \(A_{_{i}_{-i}}\), can be calculated element-wise using the reward function \(Rew_{collective}(^{k}_{i},^{j}_{-i})\) for \((k,j)\) entry. Thus the Response Diversity gain for a Feint policy \(^{M+1}\) can be measured by follows:

\[Rew_{collective-diversity}(^{M+1}_{i})=D(_{M+1} A_{_{i} _{-i}})\] (6)

\[^{T}_{M+1}:=(Rew_{collective}(^{M+1}_{i},^{j}_{-i}))^{N}_{j=1}.\] (7)

where \(D(a_{M+1} A_{_{i}_{-i}})\) represents the diversity gain of the Feint action on current policy space. We follow the method in  for the quantification of diversity gain, which uses a practical and differential lower bound for feasible computation.

## 5 Proof-of-concept Implementation

To provide a unified implementation scheme of Feint into most MARL frameworks, we choose to implement on the training iteration level and avoid changing the MARL models themselves. We create an additional policy model (e.g., MADDPG , MASAC , MAD3PG , MATD3 , etc.) for each agent as the Feint policy, which works together with the regular policy models for agents but is trained and inferenced differently.

Figure 3 illustrates the full process of our implementation of Feint in game iterations. We implement the Feint behavior generation in an imaginary play module in training iterations (i.e., game steps). The imaginary play module decides whether an agent should initiate a Feint behavior, composes a Dual-Behavior Model using Palindrom-directed templates, and utilizes the Feint reward calculation to evaluate the quality of the generated action sequence in the Dual-Behavior model.

In our experimental settings, Feint behavior templates can be pre-computed only once before training, providing a fast lookup for composing Dual-Behavior models in gaming iterations. Algorithm 1 in Appendix E illustrates the pseudo-code for pre-computing available Feint behavior templates given a set of available attack behaviors \(B\). For each pair of attack behaviors \((Behavior_{i},Behavior_{j})\) in \(B\), we check whether there are physically satisfiable states illustrated in Figure 1. If any of the three conditions are satisfied, we cut out the available actions (\(Avail_{i}\) and \(Avail_{j}\)) and store their compositions as an available template candidate. Note that the stored templates do not enforce the Feint behaviors to contain the exact same action sequences. Instead, tuples \((a_{k},Avail_{i},Avail_{j})\) are served as keys for quick lookup and action restrictions in composing Dual-Behavior models during gaming iterations.

During gaming iterations, the imaginary play will only be activated when no Dual-Behavior Model is in progress and the current physical state \(s_{c}\) of an agent is close to a physical state \(s_{r}\) where it is physically realistic for the agent to perform a high-reward action \(a_{target}\), while the possibility of performing \(a_{target}\) is relatively low according to its regular policy model (i.e., action \(a_{target}\) are highly likely to be diminished by other agents current actions). Thus, the purpose of Feint behavior

Figure 3: Illustration of Feint behavior implementation in game iterations

is to lead the agent to a state \(s_{r}\) where the agent can maximize the game environment reward by performing the intended high-reward action \(a_{target}\) (i.e., other agents are deceived by Feint to perform other actions which cannot effectively diminish the high-reward actions performed by the agent).

When the imaginary play is activated, available Dual-Behavior models can be generated using the last known action \(a_{t}\) and the intended high-reward action \(a_{target}\). Algorithm 2 in Appendix E shows the pseudo-code for composing available Dual-Behavior models with backward searches. The intended high-reward action \(a_{target}\) provides constraints on the ending actions of the Feint behaviors, while the last known action \(a_{t}\) provides constraints on the starting action of Feint behaviors. Thus, the available Dual-Behavior models can be quickly computed by doing a one-pass search from the pre-computed Feint behavior templates. The composed Dual-Behavior models can thus constrain the available action choices for the Feint policy model (set other actions' possibilities to 0) in corresponding templates and use a reflection frame to compose a (semi-)palindrome leading to the agent's physical state \(s_{r}\). After having available Dual-Behavior models which are composed of Feint behavior and followed by high-reward actions, the short-term reward can be calculated by Equation 1. After this Dual-Behavior action sequence, the imaginary play would play a few steps to incorporate the long-term reward (using Equation 1). The collective reward (Section 4.2.2) can thus be calculated. This reward is then compared to an accumulated reward from an imaginary play using only the agent's regular policy model in the same number of time steps. If the Feint collective reward is higher, the action sequence of the dual action model will be applied in the following real-game steps. When a Dual-Behavior Model is in progress, the actions will not be sampled from the regular policy models.

In the real game steps, where all the agents' actions interact with the environment and the real game rewards are calculated, our formalization of Feint only changes the way to update the Feint policy models for agents. The Feint policy models are updated only when corresponding Dual-Behavior Models finish and are updated using the accumulated real game rewards for that period. The regular policy models are updated as usual settings (e.g., after some fixed steps - an episode).

## 6 Experimental Studies

### Methodology

**Testbed Implementations.** Due to the lack of a general benchmark, we selectively implement two scenarios under a customized manner. They consist of a "1 vs 1" and a "3 vs 3" multi-player free fight games, based on widely-provided testbeds. We provide additional details on how our testbeds are designed and implemented in appendix D.

**Evaluation Metrics.** Our main evaluation objective is the gaming rewards. We first examine the gaming outcomes when using the MADDPG, MASAC, MATD3, and MAD3PG MARL models, by comparing the per episode gaming rewards of agents across all scenarios8. We also evaluate other metrics, and report our results in appendix F.

### Major Results

Figure 4 shows the game reward comparisons of using Feint behaviors or not in the Two-Player scenario (Section 6.1) for 4 MARL models. The first row shows the baseline results where all agents are trained normally, while the second row shows the results where the player labeled with "Good" incorporates Feint behaviors. In most of the baseline results (e.g., using MADDPG, MAD3PG, and MATD3), the two players' rewards tend to progress to a similar level when after enough training iterations. For MASAC, the "Good" player seems to gain higher rewards than its opponents when the training iterations are large, but the advantage is not stable and such a phenomenon can likely be the instability of the MASAC algorithm itself. For all the results where Feint behaviors are incorporated, we can see a significant advantage gain for the "Good" player. Thus, our formalization of incorporating Feint behaviors can effectively improve the actual game rewards in two-player combating scenarios.

To further evaluate the effectiveness of our formalization of Feint behaviors in multi-player scenarios, Figure 5 shows the game reward comparisons in Six-Player scenario (Section 6.1) for 4 MARL models. The first row shows the baseline results while the second row shows the results where the player labeled with "Good 3" incorporates Feint behaviors. In all baseline results, all 6 players seem to achieve similar levels of rewards after enough training iterations. In comparison, in all results where the "Good 3" player incorporates Feint, it gains significantly more rewards than the opponents as well as its teammates. This result shows that our formalization of Feint can not only gain higher rewards towards the direct opponents, but also gain advantages among teammates who do not incorporate Feint. Another interesting observation is that there are no more symmetric patterns in the players' rewards, showing that the gaming interactions in multi-player scenarios have enough complexity (Note that the scenario is not designed to be a zero-sum game).

## 7 Conclusions and Main Implications

This work introduces the first comprehensive formalization, implementation and quantitative evaluations of Feint in Multi-Player Games. We provide automatic generation of Feint behaviors using Palindrome-directed Templates and synergistically combine Feint with follow-up actions in Dual-Behavior Model. The decision choices on the action-level are fused into strategy-level formalizations in game interactions. We provide a concrete implementation scheme to incorporate Feint into common MARL frameworks. The results show that our design of Feint can (1) greatly improve the reward gains from the game; (2) significantly improve the diversity of Multi-Player Games; and (3) only incur negligible overheads in terms of the time consumption. We conclude that our formalization of Feint is effective and practical, to make Multi-Player Games more interesting.

The successful formation of Feint behaviors and strategies imply the potential unsafety of existing machine learning models (and, clearly, future ones also). Therefore, the wide adoption of machine learning models certainly demand the consideration of this work (and its variants), for building responsible Artificial Intelligence; and/or leveraging them for the future society in a responsible way.

Figure 4: Comparison of Game Reward when using Feint and not using Feint in a 1 VS 1 scenario.

Figure 5: Comparison of Game Reward when using Feint and not using Feint in a 3 VS 3 scenario.