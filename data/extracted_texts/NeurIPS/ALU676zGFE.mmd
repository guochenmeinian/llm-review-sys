# MTGS: A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction

Anshul Gupta Samy Tafasca Arya Farkhondeh Pierre Vuillecard Jean-Marc Odobez

Idiap Research Institute, Martigny, Switzerland

Ecole Polytechnique Federale de Lausanne, Switzerland

{agupta, stafasca, afarkhondeh, pvuillecard, odobez}@idiap.ch

###### Abstract

Gaze following and social gaze prediction are fundamental tasks providing insights into human communication behaviors, intent, and social interactions. Most previous approaches addressed these tasks separately, either by designing highly specialized social gaze models that do not generalize to other social gaze tasks or by considering social gaze inference as an ad-hoc post-processing of the gaze following task. Furthermore, the vast majority of gaze following approaches have proposed models that can handle only one person at a time and are static, therefore failing to take advantage of social interactions and temporal dynamics. In this paper, we address these limitations and introduce a novel framework to jointly predict the gaze target and social gaze label for all people in the scene. It comprises (i) a temporal, transformer-based architecture that, in addition to frame tokens, handles person-specific tokens capturing the gaze information related to each individual; (ii) a new dataset, VSGaze, built from multiple gaze following and social gaze datasets by extending and validating head detections and tracks, and unifying annotation types. We demonstrate that our model can address and benefit from training on all tasks jointly, achieving state-of-the-art results for multi-person gaze following and social gaze prediction. Our annotations and code will be made publicly available.

## 1 Introduction

Social interaction plays a pivotal role in our daily lives and is influenced by an array of behavioral elements, encompassing not only verbal communication but also non-verbal cues like gestures [(53)] or body language. In particular, the ability to decode people's gaze, including communicative behaviors like eye contact and shared attention on a particular object, is highly related to our capacity to connect with or learn from others [(44)]. In contrast, the absence or impairment of these skills is often indicative of developmental disorders such as autism [(42)]. Thus, designing social gaze prediction algorithms

Figure 1: Results of our multi-person and temporal transformer architecture for joint gaze following and social gaze prediction, namely Looking at Humans (LAH), Looking at Each Other (LAEO), and Shared Attention (SA). For each person, the social gaze predictions are listed with the associated person ID (_e.g._ in frame 1, person 2 is in SA with person 4). More qualitative results can be found in the supplementary G.

and systems has attracted considerable attention from different communities, ranging from medical diagnosis to human-robot interactions [42; 43; 36].

In this work, we investigate whether we can build a unified framework to infer from video data the _gaze target_ and _social gaze label_ in _one stage_ for _all people_ in the scene. This requires: (i) A new architecture capable of jointly modeling these tasks, (ii) A large-scale dataset with annotations for all the tasks. Specifically, we focus on the LAH, LAEO and SA social gaze tasks (illustrated in Fig. 1).

Methods for social gaze prediction in the literature adopt one of two approaches. The first one focuses on the design of dedicated networks to process pairs of head crops and potentially other scene information [31; 30; 11; 5; 12; 46]. While their specialization makes them effective, they offer little room for generalization to other gaze-related tasks. The second one first address the gaze following tasks , defined as predicting the 2D location people's gaze targets, and then uses the predicted gaze points to infer social gaze through ad-hoc post-processing schemes. For instance, combining gaze following heatmaps from multiple people to predict shared attention .

However, gaze following itself is a challenging task. Besides geometric aspects, the task requires understanding and establishing a correspondence between top-down information related to the person's state, activity, cognitive intent, and bottom-up saliency related to the scene context (salient items like objects or talking people). Furthermore, gaze following performance, as measured by distance, does not always translate to similar social semantic performance (_e.g_. when evaluating if the predicted gaze point falls on a person's head or not ).

**Motivation.** Existing methods for gaze following suffer from several drawbacks. First, most of the methods perform prediction for a single person [40; 41; 9; 14; 17; 48], requiring multiple inference passes on the same image to process multiple people in the scene. In contrast, a multi-person gaze following architecture processes the image only once and has to capture salient items for all people in the scene, while maintaining the ability to infer the gaze target of each individual. This is inherently more complex and challenging. Another drawback of the single-person formulation is that it does not explicitly model people's interactions, thereby preventing the possibility of jointly inferring people's gaze target and social gaze attributes. Secondly, the majority of proposed models for gaze following are static, using only a single image at a time. This is partly due to the absence of large and diverse video datasets, and the difficulty of leveraging large-scale static ones like GazeFollow . This is a limitation, as temporal information can capture head and gaze coordination patterns  which can help gaze direction inference, especially when the eyes are not completely visible .

Finally, none of these methods have investigated learning the gaze following and social gaze prediction tasks jointly. Thus, it remains a research question whether such formulation can improve performance by having social cues inform the gaze following task and vice-versa, or if performance would degrade as we try to accommodate multiple tasks, datasets, and people within the same framework.

**Contributions.** Given these motivations, we propose a new, unified framework for gaze following and social gaze prediction with the following contributions:

* A novel temporal and multi-person architecture for gaze following and social gaze prediction (Sec. 3). Our approach posits people as specific tokens that can interact with each other and the scene content (_i.e_. frame tokens). This token-based multi-person representation allows for the modeling of (i) temporal information at multiple levels (from 2D gaze direction to 2D gaze target level), (ii) the joint prediction of the gaze target and social gaze label.
* A new dataset, VSGaze, that unifies annotation types across multiple gaze following and social gaze datasets (Sec. 4.1.1).
* New social gaze protocols and metrics for better evaluating semantic gaze following performance (Sec. 4.3).

**Results.** In our experiments (Sec. 5), we show that our architecture achieves state-of-the-art results for multi-person gaze following, while also performing competitively against single-person models. It is also able to leverage our proposed VSGaze dataset to jointly tackle gaze following and social gaze prediction, achieving competitive performance compared to methods trained on individual tasks. In particular, our experiments further demonstrate that the performance _benefits from this joint prediction,_ i.e_. _adding the social loss, improves gaze following performance, and vice-versa_. Finally, the new social gaze metrics provide complementary information to the standard distance-based metrics, helping assessing model performance from the social interaction perspective.

It is worth noting that our architecture is easily extendable and allows for the integration of auxiliary person-specific information that can influence the final predictions. In the supplementary F, we explore this aspect by integrating people's speaking status in the person tokens to improve the results.

Related work

**Gaze Following.** Typical methods for this task exploit a two-branch architecture: one for processing the scene and the other for processing the person of interest (40; 9; 14; 17; 22; 23; 26). They have distinguished themselves by the addition of other relevant modalities like depth [(14; 17; 23)], pose (17), and objects (20), or by potentially leveraging scene geometry [(19; 23; 14)]. However, only a few efforts have addressed the multi-person case. (22) first proposed a simple architecture relying on a scene backbone to get a person-agnostic image representation that is subsequently fused with the head crop features of each individual obtained using another backbone. While this reduces computation, the model does not account for person interactions, as each head is processed separately. In another direction, [(52; 51)] rely on a transformer-based architecture to perform multi-person gaze following. Their methods borrow from DETR (6), taking the image as input and simultaneously predicting the head bounding box and gaze target for every person in the scene. While these methods can implicitly model person interactions, an important limitation is that they compute performance on detected heads which are matched to the ground truth. Given that both the head detection and matching steps are error-prone, it precludes comparing their results to others. We provide examples in the supplementary (Fig. 8) where (51) makes incorrect head detections.

**Temporal gaze estimation.** Temporal information has proven effective for 3D gaze estimation. Previous research developed models to learn from various inputs, including face, eyes, and facial landmarks using a multi-stream recurrent CNN [(37)]; eyes and visual stimuli or raw RGB frames from in-the-wild settings using convolutional RNNs or LSTMs [(38; 25)]; and the temporal coordination of gaze, head, and body orientations using LSTMs [(34)]. However, the use of such methods for the gaze following task in arbitrary scenes has been underexplored. The only exceptions are (9) who introduced a convolutional LSTM block at the bottleneck of the heatmap prediction architecture, and (32) who leveraged temporal attention over aggregated frame-level features. However, both approaches only showed a slight improvement compared to their static versions, highlighting the challenge of exploiting temporal information for this task. Conceptually, the methods did not model 2D gaze direction dynamics, and can not be extended for multi-person gaze inference.

**Social gaze prediction.** Several research papers in the literature are dedicated to the study of looking at each other (LAEO) and shared attention (SA) tasks. For LAEO, most methods rely on processing the head crops to obtain some gaze directional information, and then combining it with 2D or inferred 3D geometric information to predict the LAEO label [(31; 11; 30; 5)]. Drawbacks include processing pairs of persons independently and, as they only process heads and do not address gaze following, lacking global image context and not extending easily to other social tasks like SA. Similarly to [(52; 51)], a recent paper [(15)] proposed an encoder-decoder transformer architecture to predict heads and the LAEO labels, and while achieving good results, suffers from the same drawbacks.

Regarding shared attention, the first method to address it in the wild was [(12)], which framed the problem as 2 tasks: the binary classification of whether SA occurs in a frame, and the inference of the location of the SA target object. Their method combined predicted 2D gaze cones of people in the scene with a heatmap of object region proposals, while others [(46)] directly inferred SA from the raw image. Since then, several methods leveraged combining gaze following heatmap predictions of all people [(9; 52)] and improved performance. Nevertheless, the above task formulation [(12)] used by all papers suffers from two main issues: (i) it cannot distinguish between multiple SA instances if they occur in the same frame; (ii) it does not determine which specific people are sharing attention. Our work solves both problems by framing the task as a binary classification between pairs of people. This formulation is more natural and has the benefit of extending to other social gaze tasks.

Finally, none of the previous works performed both social gaze prediction tasks. There are three notable and interesting exceptions. First, [(13)] who addressed the inference of gaze communication activities (atomic and events, including LAEO and SA) using a graph-based approach with 12 dimensional tokens. However, as it predicts a single gaze'state' for each person, it is problematic as it does not allow for simultaneous LAEO and SA. It also does not identify the other person involved in the social gaze interaction. The second is [(7)] who addressed dyadic communication by proposing a gaze following style 2-branch architecture processing order-dependent pairs of people. However, inference using [(7)] is inefficient because the model needs \(!}{(N_{p}-2)!}\) forward passes to consider all pairwise relationships for a given scene with \(N_{p}\) people. Also both methods do not address the gaze following task. More recently, [(16)] did address all tasks using a graph approach, but there was no temporal modeling nor joint training on all tasks.

## 3 Architecture

As motivated in Section 1, designing a temporal architecture for gaze following is challenging given the lack of large scale video datasets. Since the broader scene tends to remain relatively static for a short temporal window, we focused on modeling person-level temporal information to simplify the learning problem. In particular, our architecture processes person and scene tokens through separate transformers, with a temporal transformer for processing the person tokens. At the same time, it facilitates interactions between the scene and person tokens via cross-attention.

**Approach overview.** Our approach is illustrated in Fig. 2. It takes as input a sequence of \(t=1 T\) frames, as well as the head bounding box tracks \(_{i,1:T}^{}\) and corresponding head crops \(_{i,1:T}^{}\) which are assumed to have been extracted for each of the \(i=1 N_{p}\) persons. The outputs are the sequence of gaze heatmaps \(_{i,1:T}\) and in-out gaze labels \(_{i,1:T}\) for each person \(i\), as well as the sequence of per-frame pair-wise social gaze labels for each task and pair \(i,j\{1 N_{p}\}\): \(_{i j,1:T}\) for LAH, \(_{i j,1:T}\) for LAEO, and \(_{i,j,1:T}\) for SA.

The model proceeds as follows. First, each frame \(t\) is processed by a standard ViT tokenizer to produce the set of patch-wise frame tokens \(_{t}\), resulting in a sequence of frame tokens \(_{1:T}\). In parallel, the Person Module processes the sequence of head crops from each person \(i\) using the Temporal Gaze Processor, and the resulting sequence outputs are then tokenized at each frame along with the bounding box locations to produce the sequence of person token \(_{i,1:T}\). Secondly, the Interaction Module jointly processes the frame and person tokens, iteratively updating them at each time step through person-scene cross-attention interaction components and scene ViT self-attention, and in time through person spatio-temporal social interaction components. Finally, the Prediction Module processes at each time step the resulting frame and person tokens (from multiple blocks) to infer the sequence of gaze heatmaps and in-out gaze labels for each person, as well as pair-wise social gaze labels. We detail the three modules in the next sections.

### Person Module

This module aims to model person-specific information relating to gaze and head location.

**Temporal Gaze Processor.** It aims to capture all gaze-related information (direction, dynamics). First, individual head crops \(_{i,t}^{}\) are processed by a Gaze Backbone \(_{}\) to produce gaze embeddings according to \(_{i,t}^{}=_{}(_{i,t}^{ })\). Then, to model the gaze dynamics of a person, we rely on a Temporal Gaze Encoder \(_{}\) to process the sequence \(_{i,1:T}^{}\) of gaze embeddings plus learnable temporal position embeddings \(_{1:T}\) and obtain their temporal counterparts: \(_{i,1:T}^{}=_{}(_{i,1:T }^{}+_{1:T})\). \(_{}\) is implemented as a single Transformer layer with self-attention. Finally, to supervise the learning of relevant gaze embeddings, we attach a Gaze Vector Decoder that predicts a person's 2D gaze vector at each time step, \(_{i,t}^{}=_{}(_{i,t}^{})\), where \(_{}\) is implemented as a 2-layer MLP.

**Person tokenization.** The person tokens are obtained by projecting the temporal gaze embeddings and normalized 4d head box locations using learnable linear layers (\(_{}\) and \(_{}\) respectively) to tokens of same dimension than frame token, and adding them together:

Figure 2: Proposed architecture for multi-person temporal gaze following and social gaze prediction. See approach overview in Section 3.

\[_{i,t}=_{}(_{i,t}^{})+_{}(_{i,t}^{}).\] (1)

### Interaction Module

The Interaction module aims at modeling the exchange of information between persons and the scene at each time step, as well as the spatio-temporal social interactions between people. One important goal of this process is to align the person and frame token representations so that (i) _person-specific_ gaze heatmaps can be predicted from the set of output frame tokens and each person output token; (ii) in-out gaze and social gaze prediction can be made from the person tokens.

To do so, we designed the module to consist of \(B\) blocks, each comprising Person-Scene Interaction and Spatio-Temporal Social Interaction components. The input to the first block is the set of person tokens \(_{1:N_{p},1:T}\) from the Person Module, and the frame tokens \(_{1:T}\). Each block then processes the set of output1 person tokens \(_{1:N_{p},1:T}^{}\) and output frame tokens \(_{1:T}^{}\) from the previous block, and returns updated tokens after a series of self/cross-attention layers through the components.

**Person-Scene Interaction.** This component models the interactions between people and the scene and can capture inferring gaze to scene objects or body parts like hands or exploit some global context. It is inspired by ViT-Adaptor (8) which has shown good performance for dense prediction tasks when relying on pretrained models and small amounts of data for the target task. It proceeds in 3 steps:

(i) People-to-Scene Encoder \(_{}^{}\): it updates the frame tokens with person information relevant to gaze by processing the frame tokens \(_{t}^{}\) and frame-level person tokens \(_{1:N_{p},t}^{}\) according to \(_{t}^{}=_{}^{}(_{t }^{},_{1:N_{p},t}^{})\). It is implemented as a single Transformer layer with cross-attention, where \(_{t}^{}\) generate the queries and \(_{1:N_{p},t}^{}\) generate the keys and values.

(ii) The updated frame tokens \(_{t}^{}\) pass through the standard set of of ViT layers \(_{b}\) to process the scene information, resulting in the output frame tokens for the block \(b\): \(_{t}^{}=_{b}(_{t}^{})\).

(iii) Scene-to-People Encoder \(_{}^{}\): it updates the person tokens so that they capture location information related to the salient items they are probably looking at. It works by processing the frame-level person tokens \(_{1:N_{p},t}^{}\) and obtained frame tokens \(_{t}^{}\) according to: \(_{1:N_{p},t}^{}=_{}^{}( _{1:N_{p},t}^{},_{t}^{})\). It is also implemented as a single Transformer layer with cross-attention, where the set \(_{1:N_{p},t}^{}\) generates the queries and \(_{t}^{}\) generates the keys and values.

**Spatio-temporal Social Interaction.** This component allows the sharing of information between people and of the alignment of their representations for social gaze prediction. This also include modeling the temporal evolution of individual tokens. To achieve this, a Social Encoder \(_{}^{}\) first processes and updates the frame-level person tokens \(_{1:N_{p},t}^{}\) to capture interactions between people at each frame, according to: \(_{1:N_{p},t}^{}=_{}^{}( _{1:N_{p},t}^{})\). It is followed by a Temporal Person Encoder \(_{}^{}\) that processes the updated person token sequences \(_{i,1:T}^{}\) of each person \(i\) and updates them to capture temporal patterns of attention, resulting in the output person tokens for the block: \(_{i,1:T}^{}=_{}^{}(_{ i,1:T}^{})\). Both \(_{}^{}\) and \(_{}^{}\) are implemented as a single Transformer layer with self-attention.

### Prediction Module

The Prediction Module processes the set of output person \(\{_{1:N_{p},1:T}^{}\}\) and frame \(\{_{1:T}^{}\}\) tokens from all Interaction Module blocks to predict the person-specific gaze heatmaps and in-out labels, as well as the pair-wise social gaze labels.

**Gaze Heatmap Prediction.** Here, we follow the model introduced in (49) which takes inspiration from the DPT decoder (39) for dense prediction tasks, and adapts it to handle multiple heatmap predictions from the same ViT outputs. This is performed _by conditioning the decoding on each person's token_. The standard DPT decodes the image features from multiple layers of a ViT in a Feature Pyramid Network (27) style. It works by fusing at block level \(b\) the feature maps from level \(b+1\) after an upsampling stage, and the feature maps computed by a reassemble stage from the ViT output of block \(b\). We aim to apply this approach to the frame tokens \(\{_{1:T}^{},b=1:B\}\),but conditioned on a specific person. In our model, this is achieved through a modification in the reassemble stage, in which the image feature maps produced by the standard reassembble stage are multiplied at every location (using a Hadamard product) with the projected person token \(_{i,t}^{}\) of that same block level. The gaze heatmap \(_{i,t}\) for each person at each frame is thus obtained as:

\[_{i,t}=(\{(_{t}^{},_{i,t}^ {}),b=1:B\})\] (2)

where \(\) denotes this conditional DPT. See (49) and supplementary H for details.

**Social Gaze Prediction.** This decoder processes the person tokens from all \(B\) Interaction Module blocks to predict the social gaze label for every pair of people in every frame. In practice, the \(B\) tokens \(\{_{i,t}^{}_{i,t}^{}\}\) corresponding to a single person in a frame are linearly projected and concatenated to produce a multi-scale person token \(_{i,t}^{}\). Then, to predict a social gaze label, pairs of these tokens are concatenated and processed by the decoders \(E\) for LAH and \(\) for SA (illustrated through the Pairwise Instance Generator in Fig. 2). Their outputs are the predicted LAH score \(_{i j,t}\) for person \(i\) looking at \(j\), and the predicted SA score \(_{i,j,t}\) for \(i,j\).

\[_{i j,t}=E(_{i,t}^{},_{j,t}^{ })_{i,j,t}=C(_{i,t}^{},_{j,t}^{ }).\] (3)

\(E\) and \(C\) are implemented as 3-layer MLPs with residual connections. For LAEO, both people \(i,j\) need to be looking at each other for a positive label, and either one can be looking away for a negative label. Hence, we simply compute the LAEO score \(_{i j,t}\) as \((_{i j,t},_{j i,t})\).

**In-Out Prediction.** This decoder \(\) processes the multi-scale person tokens \(_{i,t}^{}\) to predict at every frame whether people are looking inside the frame or outside the frame, as \(_{i,t}=(_{i,t}^{})\), where \(\) is implemented as a 5-layer MLP with residual connections.

### Losses

The total loss \(\) is a linear combination of the gaze heatmap loss \(_{}\), gaze vector loss \(_{}\), social gaze losses \(_{}\), \(_{}\) and the in-out loss \(_{}\):

\[=_{}_{}+_{} _{}+_{}_{}+ _{}_{}+_{}_{}\] (4)

\(\) is applied at each time step per person for \(_{}\), \(_{}\), \(_{}\), and per pair for \(_{}\), \(_{}\). All losses are standard: \(_{}\) is defined as the pixel-wise MSE loss between the GT and predicted heatmaps, \(_{}\) as the cosine loss, and the social gaze and in-out losses as binary cross-entropy losses. Since LAEO is inferred from LAH predictions (Sec 3.3), we do not have any LAEO loss.

## 4 Experiments

### Datasets

We perform experiments on multiple gaze following and social gaze datasets.

**GazeFollow (41)** is a large-scale static dataset for gaze following, featuring 122K images. Most images are annotated for a single person with their head bounding box and gaze target point. The test set contains gaze point annotations by multiple annotators. Despite lower quality images and annotations, given its rich diversity, it remains a good dataset to use for pre-training.

**VideoAttentionTarget (VAT) (9)** is a video dataset, annotated with head bounding boxes, gaze points, and inside vs outside frame gaze for a subset of the people in the scene. It contains 1331 clips collected from 50 shows on YouTube.

**ChildPlay (48)** is a recent video dataset for gaze following, annotated with head bounding boxes, gaze points, and a label indicating 7 non-overlapping gaze classes including inside frame, outside frame and gaze shifts. It contains 401 clips from 95 YouTube videos, and features children playing and interacting with other children and adults.

**VideoCoAtt (12)** is a video dataset for shared attention estimation, containing 380 videos or 492k frames from TV shows. When a shared attention behavior occurs (i.e. about 140k frames), the relevant frames are annotated with the bounding box of the target object, as well as the head bounding boxes of the people involved.

**UCO-LAEO (31)** is a video dataset for LAEO estimation, annotated with head bounding boxes, and a label indicating whether two heads are LAEO. It contains 22,398 frames from 4 TV shows.

Two other interesting datasets with annotations for multiple social gaze behaviours are VACATION (13) and GP-static (7). However, VACATION does not provide annotations for all social gaze behaviours when they occur simultaneously (examples in supplementary Fig. 7). This annotation scheme is problematic and is linked to their method design as described in Sec. 2. On the other hand, GP-static only considers dyadic interactions and is not publicly available.

#### 4.1.1 VSGaze dataset

A limitation of the above datasets is that they only contain annotations for gaze following or specific social gaze tasks. Hence, we propose the **V**ideo dataset with **S**ocial gaze and **Gaze** following annotations or **VSGaze** dataset. VSGaze extends head track annotations and unifies annotation types across VAT, ChildPLay, VideoCoAtt and UCO-LAEO. This allows for joint training of gaze following and social gaze, and provides new tasks and metrics for evaluating performance on the component datasets. The construction of VSGaze is described below.

**Extending Head Track Annotations.** As each dataset contains annotations for only a subset of people in the scene, we detect all missing heads using the pre-trained Yolov5 head detection model (24) used by Tafasca _et al_. (48), and track them using the ByteTrack algorithm (54). We further manually verified the accuracy of the obtained tracks. This step is vital to obtain positive and negative social gaze pairs. Consider a scene with \(3\) people, \(i,j,k\), where \(i\) is looking at \(j\). If only \(i\) is annotated, the positive LAH pair \(i j\) would be missed. The negative LAH pair \(i k\) would also be missed.

**Unifying Gaze Following and Social Gaze Annotations.** Given the extended set of head bounding box annotations, as well as existing gaze following and social gaze annotations, we then process these annotations to obtain gaze following and social gaze labels across all the datasets. The gaze target for UCO-LAEO and VideoCoAtt is set as the center of the LAEO and SA target bounding box respectively. LAH pairs are obtained by checking if the gaze target falls inside another person's head box. For LAEO we check the reverse as well. SA pairs are obtained by checking if the gaze targets for both people fall in the same head box. We provide the detailed protocol in the supplementary B.

**Annotation statistics** are summarized in Table 1. Overall, VideoCoatt is the largest source of annotations except for LAEO. We also see that the pair-wise annotations are skewed towards negative cases. The statistics further provide insight into the content of the datasets. As VAT, VideoCoAtt and UCO-LAEO contain clips from TV shows, there are many more instances of looking at other people and at each other. On the other hand for ChildPlay, LAH mainly occurs when the supervising adult looks at a child and there is limited LAEO.

### Training and Validation

We follow standard practice (9; 14; 17) by first training the static version of our model (i.e. with no temporal attention \(_{}\), \(_{}^{}\)) on GazeFollow. It is trained for 20 epochs with a learning of rate of \(1 10^{-4}\). The resulting weights then serve as initialization for our proposed temporal model. We freeze the ViT \(\) and train the temporal model on VSGaze for another 20 epochs with a learning rate of \(3 10^{-6}\). For validation, we use the provided splits for UCO-LAEO, VideoCoAtt and ChildPlay, and the splits proposed by Tafasca _et al_. (48) for GazeFollow and VAT. For all our experiments, we use \(T=5\) frames with a temporal stride of \(3\). To allow for batch training, we randomly sample up to \(N_{p}=4\) people in a scene (padding in case there are less). During testing, we evaluate per sample and consider all people. The Interaction Module consists of \(B=4\) blocks, interacting with \(\) at layers \(\{2,5,8,11\}\). Additional implementation details are provided in the supplementary E.

  
**Dataset** & **Gaze Points** & **LAH** & **LAEO** & **SA** \\  GazeFollow (41) & 118k & 27k/493k & 0 & 0 \\  VAT (9) & 109k & 74k/729k & 13k/461k & 16k/94k \\ ChildPlay (48) & 217k & 59k/682k & 7k/351k & 4k/55k \\ VideoCoAtt (12) & 367k & 290k/155k & 0 & 400k/918k \\ UCO-LAEO (31) & 21k & 12k/36k & 10k/54k & 0 \\
**VSGaze** & 714k & 444k/2998k & 30k/866k & 420k/1067k \\   

Table 1: Person-wise gaze point and pair-wise social gaze annotation (positive/negative) statistics for our datasets. VSGaze unifies annotation types across VAT, ChildPlay, VideoCoatt and UCO-LAEO.

### Evaluation

**Gaze Following.** We use the standard metrics:

* _AUC_: for GazeFollow, the predicted heatmap is compared against a binary GT map with value 1 at annotated gaze point positions, to compute the area under the ROC curve.
* _Distance (Dist.)_: the arg max of the heatmap provides the gaze point. We can then compute the L2 distance between the predicted and GT gaze point on a \(1 1\) square. For GazeFollow, we compute Minimum (Min.) and Average (Avg.) distance against all annotations.
* _In-Out AP (AP\({}_{10}\))_: it is the Average Precision (AP) of the In-Out gaze prediction scores.

**Social Gaze.** We propose an evaluation protocol for LAH, as well as a new pair-wise evaluation protocol for SA as motivated in Sec. 2.

_Social gaze decoders:_ For the LAH task we compute an F1 score. A sample is an individual person \(i\), and at inference, it is assigned the person \(\) for which the \(_{i j,t}\) is the largest. Hence, for a GT positive case, the prediction will be considered as a true positive if \(\) matches the GT target AND \(_{i,t}\) is above \(0.5\). Otherwise, the prediction is a false negative. Similarly, a GT negative is a true negative if \(_{i,t}\) is below \(0.5\), otherwise it is a false positive. For LAEO, as with LAH, we compute an F1 score. A sample is a pair of people \(i,j\), and for person \(i\), we consider \(_{i,t}\) with the highest score. We then set \(_{i j,t}\) to 0 \( j\) before computing the performance. We also do the reverse for \(j,i\). For SA, we compute a standard AP score by considering a sample as a pair of people, and thresholding predicted scores at different values to compute the area under the Precision-Recall curve.

_Gaze following methods:_ Social gaze predictions are obtained by post-processing the predicted gaze points. For LAH, we check whether the predicted gaze point for a person falls inside the target person's head box. For LAEO we check the reverse as well. We compute F1 scores for both. For SA, we check if the distance between two people's predicted gaze points is within a set of thresholds and compute an AP score.

## 5 Results

### Comparison against the State-of-The Art

We compare against recent SoTA methods addressing either social gaze tasks or gaze following. In addition, for fairness and to evaluate the impact of the VSGaze dataset, we also re-trained on this dataset the static image based models of Chong (9) (Chongs\({}_{S}\)*) and Gupta (17) (Gupta*), as well as the temporal model of (9) (Chong\({}_{T}\)*), the only temporal gaze following model with available code.

**VSGaze.** The results on VSGaze are given in Table 2. Note that regarding our approach, for social gaze, we compute the scores by leveraging either the predictions from the respective task decoders (Ours), or by post-processing the gaze following outputs of our model (Ours-PP).

Compared to the baselines, we observe that our model achieves the best performance for all tasks except for in-out gaze prediction. In particular, we achieve significant gains in the distance and AP\({}_{}\) metrics when leveraging the predictions from the SA decoder. The latter highlights the importance of modeling SA as a classification task compared to post-processing gaze following outputs, which struggles to capture whether the gaze points for a pair of people falls on the same semantic item.

In addition, we note that better gaze following performance does not always translate to better social gaze performance. For instance, although Chong\({}_{S}\)* has a better distance score compared to Chong\({}_{T}\)*, it performs worse for shared attention. This effect is even more pronounced on ChildPlay (Supp C), and suggests the benefit of considering social gaze metrics for better characterizing the performance

  
**Method** & **PP** & **Dist. \(\)** & \(_{10}\) & \(_{}\) & \(_{}\) & \(_{}\) \\  Chong\({}_{S}\)* (9) & ✓ & 0.121 & 0.918 & 0.778 & 0.562 & 0.288 \\ Chong\({}_{S}\)* (9) & ✓ & 0.130 & **0.956** & 0.764 & 0.529 & 0.331 \\ Gupta\({}^{+}\)(17) & ✓ & 0.119 & 0.929 & 0.784 & 0.590 & 0.335 \\ Ours-noGoer & ✗ & - & - & 0.738 & 0.579 & 0.515 \\ Ours-noGoer & ✓ & 0.111 & 0.945 & 0.802 & 0.598 & 0.339 \\ Ours & ✗ & **0.107** & 0.940 & 0.795 & 0.590 & **0.576** \\ Ours-PP & ✓ & **0.107** & 0.940 & **0.812** & **0.603** & 0.352 \\   

Table 2: Comparison against gaze following methods on VSGaze. All models were trained on VSGaze. PP indicates social gaze predictions from post-processing gaze following outputs (✓) vs predictions from decoders (✗). Best results are in bold, second best results are underlined.

of gaze following models, especially its semantic performance. In the supplementary C, we provide a breakdown of performance on each of the component datasets of VSGaze.

**State-of-the-art comparison: fine tuning on individual datasets.** Table 3 compares our model against task specific methods. For GazeFollow, we use our static model (Ours-static) that was trained on GazeFollow and used to initialize our model trained on VSGaze. For the video datasets, as SoTA methods were trained (or finetuned) on individual datasets, for fairness we also fine-tune our model on these datasets, investigating two initialization alternatives: either from the model trained on GazeFollow (Ours), or from the model trained on VSGaze (Ours\(\)). Note that we are unable to compare against previous results for VideoCoatt due to our new pair-wise evaluation protocol that better captures SA performance (Sec 4.3).

On GazeFollow and VAT, our model outperforms the only other comparable multi-person gaze following model of Jin (22). It also achieves competitive or better results to single-person methods, even those leveraging auxiliary modalities such as depth (14; 50; 4; 23; 21; 48). Importantly, on the social LAEO task, we set the new state of the art on UCO-LAEO, far outperforming methods designed specifically for LAEO (31; 30; 11).

We also note that fine-tuning using the VSGaze model initialization can improve results compared to the standard protocol of fine-tuning after training on GazeFollow (ex. distance on ChildPlay and AP\({}_{}\) on UCO-LAEO). This suggests that training on VSGaze can leverage the complementary knowledge provided by the different tasks and datasets, which follows observations made in other works addressing multi-task training (10).

### Analysis

**Impact of Architecture.** Comparing the performance of our model with no social gaze losses (Ours-noSoc) against the baselines (Table 2), we see that it already performs on par or better than them while being much more efficient as it processes the image only once for all people in the scene. It also serves as a strong gaze following baseline to compare performance against.

**Impact of Social Gaze Loss.** Our architecture can further benefit from the social gaze losses, showing improved gaze following performance and social gaze prediction (Ours and Ours-PP, Table 2). In particular, we observe significant gains for the SA compared to Ours-noSoc. Interestingly, the addition of the social gaze losses also better aligns the gaze following outputs for social gaze prediction. Comparing Ours-PP and Ours-noSocial, we see that performance for all social gaze tasks is improved.

**Impact of Gaze Following Loss.** We additionally train our model without the standard gaze following losses: heatmap, gaze vector and in-out (Ours-noGF, Table 2). Across VSGaze, we see that

Table 3: Comparison against task specific methods fine-tuned on individual datasets. Best multi-person results are in bold, overall best results are underlined. Multi indicates multi-person () vs single-person () gaze following methods. Ours is initialized from training on GazeFollow, while Ours\(\) is initialized from training on VSGaze.

performance for all social gaze tasks drops, which indicates that the gaze following and social gaze losses provide complementary information, and using both can give improved performance.

**Impact of VSGaze.** When comparing the performance of the models trained on VSGaze (Supp Table 4) against their versions fine-tuned on individual datasets (Table 3), we see that the fine-tuned models always perform better. This is because fine-tuning allows the models to learn dataset specific priors (ex. more LAH cases in VAT, Table 1). For instance, on VAT, Gupta* has a distance score of \(0.138\) when trained on VSGaze, compared to a score of \(0.134\) when directly fine-tuned on VAT. Also, our model has a distance score of \(0.112\) when trained on VSGaze, and a score of \(0.105\) when fine-tuned on VAT. This highlights the challenge in leveraging multiple datasets: while we may expect better performance by having more data, the different priors and statistics bring additional difficulties. Nevertheless, our model trained on VSGaze is able to achieve strong performance across all datasets.

**Impact of temporal information.** Comparing the static and temporal versions of our model trained on VSGaze, we observe improvements in performance for shared attention (\(0.555\) vs \(0.576\), Tab. 5 in supplementary), and similar or slightly improved performance for other metrics. This is in contrast with Chong\({}_{T}\), which often has lower performance than Chong\({}_{S}\) (for distance, LAH and LAEO metrics in Table 2). These results follow observations from prior work (Section 2) and highlight the challenge in leveraging temporal information for gaze following. We provide a detailed analysis in supplementary D.

**The supplementary** (overview in A) provides additional ablations and discussions.

## 6 Conclusion

We propose a new framework for multi-person, temporal gaze following and social gaze prediction comprising of a novel architecture and dataset. Through a series of experiments, we show that our model can effectively learn from a mix of video-based datasets with different statistics, to perform gaze following and social gaze prediction without sacrificing performance on any of them. The trained model can then be further fine-tuned on individual datasets to improve performance towards a specific scenario or task.

We hope that our proposed framework opens new directions for modeling people's gaze behavior, and are keen to see applications of the proposed dataset and social gaze losses in other methods. In the future, we intend to further investigate the benefits of temporal information and other auxiliary signals, including new ways of incorporating them into the architecture. We also plan to expand the VSGaze dataset to include more samples and annotations.

**Acknowledgement.** This research has been supported by the AI4Autism project (Digital Phenotyping of Autism Spectrum Disorders in Children, grant agreement number CRSII5 202235/1) of the the Sinergia interdisciplinary program of the SNSF.