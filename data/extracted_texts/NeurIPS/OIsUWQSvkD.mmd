# Identifying Causal Effects Under Functional Dependencies

Yizuo Chen

Department of Computer Science

University of California, Los Angeles

Los Angeles, CA 90024

yizuo.chen@ucla.edu

&Adnan Darwiche

Department of Computer Science

University of California, Los Angeles

Los Angeles, CA 90024

darwiche@cs.ucla.edu

###### Abstract

We study the identification of causal effects, motivated by two improvements to identifiability which can be attained if one knows that some variables in a causal graph are functionally determined by their parents (without needing to know the specific functions). First, an unidentifiable causal effect may become identifiable when certain variables are functional. Second, certain functional variables can be excluded from being observed without affecting the identifiability of a causal effect, which may significantly reduce the number of needed variables in observational data. Our results are largely based on an elimination procedure which removes functional variables from a causal graph while preserving key properties in the resulting causal graph, including the identifiability of causal effects.

## 1 Introduction

A causal effect measures the impact of an intervention on some events of interest, and is exemplified by the question "what is the probability that a patient would recover had they taken a drug?" This type of question, also known as an interventional query, belongs to the second rung of Pearl's causal hierarchy  so it ultimately requires experimental studies if it is to be estimated from data. However, it is well known that such interventional queries can sometimes be answered based on observational queries (first rung of the causal hierarchy) which can be estimated from observational data. This becomes very significant when experimental studies are either not available, expensive to conduct, or would entail ethical concerns. Hence, a key question in causal inference asks when and how a causal effect can be estimated from available observational data assuming a causal graph is provided .

More precisely, given a set of _treatment_ variables \(\) and a set of _outcome_ variables \(\), the causal effect of \(\) on \(\), denoted \((|do())\) or \(_{}()\), is the marginal probability on \(\) when an intervention sets the states of variables \(\) to \(\). The problem of identifying a causal effect studies whether \(_{}()\) can be uniquely determined from a causal graph and a distribution \(()\) over some variables \(\) in the causal graph , where \(()\) is typically estimated from observational data. The causal effect is guaranteed to be identifiable if \(\) correspond to all variables in the causal graph (with some positivity assumptions); that is, if all such variables are observed. When some variables are hidden (unobserved), it is possible that different parameterizations of the causal graph will induce the same distribution \(()\) but different values for the causal effect \(_{}()\) which leads to unidentifiability. In the past few decades, a significant amount of effort has been devoted to studying the identifiability of causal effects; see, e.g., [3; 2; 4; 5; 6; 7]. Some early works include the _back-door criterion_[8; 2] and the _front-door criterion_[3; 2]. These criteria are sound but incomplete as they may fail to identify certain causal effects that are indeed identifiable. Complete identification methods include the do-calculus , the identification algorithm in , and the ID algorithm proposed in . These methods require some positivity assumptions (constraints) on the observational distribution \(()\) and can derivean identifying formula that computes the causal effect based on \(()\) when the causal effect is identifiable. Some recent works take a different approach by first estimating the parameters of a causal graph to obtain a fully-specified causal model which is then used to estimate causal effects through inference [11; 12; 13; 14]. Further works focus on the efficiency of estimating causal effects from finite data [15; 16; 17; 18], the general identifiability of causal effects from both observational and experimental data , and the causal effect identification with data collected from sub-populations .

A recent line of work studies the impact of additional information on identifiability, beyond causal graphs and observational data. For example,  showed that certain unidentifiable causal effects can become identifiable given information about context-specific independence. Our work in this paper follows the same direction as we consider the problem of causal effect identification in the presence of a particular type of qualitative knowledge called _functional dependencies_. We say there is a functional dependency between a variable \(X\) and its parents \(\) in the causal graph if the distribution \((X|)\) is deterministic but we do not know the distribution itself (i.e., the specific values of \((x|)\)). We also say in this case that variable \(X\) is _functional_. Previous works have shown that functional dependencies can be exploited to improve the efficiency of Bayesian network inference [23; 24; 25; 13; 26]. We complement these works by showing that functional dependencies can also be exploited for identifiability. In particular, we show that some unidentifiable causal effects may become identifiable given such dependencies, propose techniques for testing identifiability in this context, and highlight other implications of such dependencies on the practice of identifiability.

Consider the following motivational example where we are interested in how the enforcement of speed limits may affect car accidents. The Driving Age (\(A\)) is functionally determined by Country (\(C\)); Driving Age and Country are causes of Speed (\(X\)); and Speed and Driving Age are causes of Accidents (\(Y\)). The DAG on the right captures the causal relations among these variables, where variable \(A\) is circled to indicate it is functional. Suppose further that variables \(C,X,Y\) are observed. According to classical causal-effect identification methods (e.g., do-calculus, ID algorithm), the causal effect of \(X\) on \(Y\) is unidentifiable in this case. However, if we take into account that variable \(A\) is a function of \(C\), which restricts the class of distributions under consideration, then the causal effect of \(X\) on \(Y\) becomes identifiable. This exemplifies the improvements to identifiability pursued in this paper.

Consider a causal graph \(G\) and a distribution \(()\) over the observed variables \(\) in \(G\). To check the identifiability of a causal effect, it is standard to first apply the _projection_ operation in [27; 28] which constructs another causal graph \(G^{}\) with \(\) as its non-root variables, and follow by applying an identification algorithm to \(G^{}\) like the ID algorithm . This two-stage procedure, which we will call _project-ID_, is applicable only under some positivity constraints (assumptions) which preclude some events from having a zero probability. Since such positivity constraints may contradict functional dependencies, we formulate the notion of _constrained identifiability_ which takes positivity constraints as an input (in addition to the causal graph \(G\) and distribution \(()\)). We also formulate the notion of _functional identifiability_ which further takes functional dependencies as an input. This allows us to explicitly treat the interactions between positivity constraints and functional dependencies, which is needed for combining classical methods like project-ID with the results we present in this paper.

We start with some technical preliminaries in Section 2. We formally define positivity constraints and functional dependencies in Section 3 where we also introduce the problems of constrained and functional identifiability. Section 4 introduces two primitive operations, _functional elimination_ and _functional projection,_ which are needed for later treatments. Sections 5 presents our core results on functional identifiability and how they can be combined with existing identifiability algorithms. We finally close with concluding remarks in Section 6. Proofs of all results are included in the Appendix.

## 2 Technical Preliminaries

We consider discrete variables in this work. Single variables are denoted by uppercase letters (e.g., \(X\)) and their states are denoted by lowercase letters (e.g., \(x\)). Sets of variables are denoted by bold uppercase letters (e.g., \(\)) and their instantiations are denoted by bold lowercase letters (e.g., \(\)).

### Causal Bayesian Networks and Interventions

A Causal Bayesian network (CBN) is a pair \( G,\) where \(G\) is a _causal graph_ in the form of a directed acyclic graph (DAG), and \(\) is a set of conditional probability tables (CPTs). We have one CPT for each variable \(X\) with parents \(\) in \(G\), which specifies the conditional probability distributions \((X|)\). This CPT will often be denoted by \(f_{X}(X,)\) so \(f_{X}(x,)\) for all instantiations \(x,\) and \(_{x}f_{X}(x,)=1\) for every instantiation \(\).

A CBN induces a joint distribution over its variables \(\) which is exactly the product of its CPTs, i.e., \(()=_{V}f_{V}\). Applying a treatment \(do()\) to the joint distribution yields a new distribution called the _interventional distribution_, denoted \(_{}()\). One way to compute the interventional distribution is to consider the _untilated CBN_\( G^{},^{}\) that is constructed from the original CBN \( G,\) as follows: remove from \(G\) all edges that point to variables in \(\), then replace the CPT in \(\) for each \(X\) with a CPT \(f_{X}(X)\) where \(f_{X}(x)=1\) if \(x\) is consistent with \(\) and \(f_{X}(x)=0\) otherwise. Figure 0(a) depicts a causal graph \(G\) and Figure 0(b) depicts the mutilated causal graph \(G^{}\) under a treatment \(do(x_{1},x_{2})\). The interventional distribution \(_{}\) is the distribution induced by the mutilated CBN \( G^{},^{}\), where \(_{}()\) corresponds to the causal effect \((|do())\) also notated by \((_{})\).

### Identifying Causal Effects

A key question in causal inference is to check whether a causal effect can be (uniquely) computed given the causal graph \(G\) and a distribution \(()\) over a subset \(\) of its variables. If the answer is yes, we say that the causal effect is _identifiable_ given \(G\) and \(()\). Otherwise, the causal effect is _unidentifiable_. Variables \(\) are said to be _observed_ and the remaining variables are said to be _hidden_, where \(()\) is usually estimated from observational data. We start with the general definition of identifiability (not necessarily for causal effects) from [2, Ch. 3.2.4] with a slight rephrasing.

**Definition 1** (Identifiability ).: _Let \(Q(M)\) be any computable quantity of a model \(M\). We say that \(Q\) is identifiable in a class of models if, for any pairs of models \(M_{1}\) and \(M_{2}\) from this class, \(Q(M_{1})=)}\) whenever \(_{M_{1}}()=_{M_{2}}()\) where \(\) are the observed variables._

In the context of causal effects, the problem of identifiability is to check whether every pair of fully-specified CBNs (\(M_{1}\) and \(M_{2}\) in Definition 1) that induce the same distribution \(()\) will also produce the same value for the causal effect. Note that Definition 1 does not restrict the considered models \(M_{1}\) and \(M_{2}\) based on properties of the distributions \(_{M_{1}}()\) and \(_{M_{2}}()\). However, in the literature on identifying causal effects, it is quite common to only consider CBNs (models) that induce distributions which satisfy some positivity constraints, such as \(()>0\). We will examine such constraints more carefully in Section 3 as they may contradict functional dependencies.

It is well known that under some positivity constraints (e.g., \(()>0\)), the identifiability of causal effects can be efficiently tested using what we shall call the _project-ID_ algorithm. Given a causal graph \(G\), project-ID first applies the projection operation in [27; 28; 29] to yield a new causal graph \(G^{}\) whose hidden variables are all roots and each has exactly two children. These properties are needed by the ID algorithm , which is then applied to \(G^{}\) to yield an identifying formula if the causal effect is identifiable or FAIL otherwise. Consider the causal effect \(_{x_{1}x_{2}}(y)\) in Figure 0(a) where the only hidden variable is the non-root variable \(B\). We first project the causal graph \(G\) in Figure 0(a) onto its observed variables to yield the causal graph \(G^{}\) in Figure 0(c) (all hidden variables in \(G^{}\) are auxiliary and roots). We then run the ID algorithm on \(G^{}\) which returns the following (simplified) identifying formula: \(_{x_{1}x_{2}}(y)=_{acd}(c|x_{1})(d|x_{1},x_{2})\,_{x_{1}^{ }x_{2}^{}}(y|x_{1}^{},x_{2}^{},a,c,d)\)\((x_{2}^{}|x_{1}^{},a,c)\). Hence, the causal effect \(_{x_{1}x_{2}}(y)\) is identifiable and can be computed using the above formula. Moreover, all quantities in the formula can be obtained from the distribution \((A,C,D,X_{1},X_{2},Y)\) over observed variables, which can be estimated from observational data. More details on the projection operation and the ID algorithm can be found in Appendix A.

## 3 Constrained and Functional Identifiability

As mentioned earlier, Definition 1 of identifiability [2, Ch. 3.2.4] does not restrict the pair of considered models \(M_{1}\) and \(M_{2}\). However, it is common in the literature on causal-effect identifiability to only consider CBNs with distributions \(()\) that satisfy some positivity constraints. Strict positivity,\(()>0\), is perhaps the mostly widely used constraint [9; 29; 2]. That is, in Definition 1, we only consider CBNs \(M_{1}\) and \(M_{2}\) which induce distributions \(_{M_{1}}\) and \(_{M_{2}}\) that satisfy \(_{M_{1}}()>0\) and \(_{M_{2}}()>0\). Weaker, and somewhat intricate, positivity constraints were employed by the ID algorithm in  as discussed in Appendix A, but we will apply this algorithm only under strict positivity to keep things simple. See also  for a recent discussion of positivity constraints.

Positivity constraints are motivated by two considerations: technical convenience, and the fact that most causal effects would be unidentifiable without some positivity constraints (more on this later). Given the multiplicity of positivity constraints considered in the literature, and given the subtle interaction between positivity constraints and functional dependencies (which are the main focus of this work), we next provide a systematic treatment of identifiability under positivity constraints.

### Positivity Constraints

We will first formalize the notion of a _positivity constraint_ and then define the notion of _constrained identifiability_ which takes a set of positivity constraints as input (in addition to the causal graph \(G\) and distribution \(()\)).1

**Definition 2**.: _A positivity constraint on \(()\) is an inequality of the form \((|)>0\) where \(\), \(\) and \(=\). That is, for all instantiations \(,\), if \(()>0\) then \((,)>0\)._

When \(=\), the positivity constraint is defined on a marginal distribution, \(()>0\). We may impose multiple positivity constraints on a set of variables \(\). We will use \(_{}\) to denote the set of positivity constraints imposed on \(()\) and use \((_{})\) to denote all the variables mentioned by \(_{}\). The weakest set of positivity constraints is \(_{}=\{\}\) (no positivity constraints as in Definition 1), and the strongest positivity constraint is \(_{}=\{()>0\}\).

We next provide a definition of identifiability for the causal effect of treatments \(\) on outcomes \(\) in which positivity constraints are an input to the identifiability problem. We call it _constrained identifiability_ in contrast to the (unconstrained) identifiability of Definition 1.

**Definition 3**.: _We call \( G,,_{}\) an identifiability tuple where \(G\) is a causal graph (DAG), \(\) is its set of observed variables, and \(_{}\) is a set of positivity constraints._

**Definition 4** (Constrained Identifiability).: _Let \( G,,_{}\) be an identifiability tuple. The causal effect of \(\) on \(\) is said to be identifiable with respect to \( G,,_{}\) if \(_{}^{1}()=_{}^{2}()\) for any pair of distributions \(^{1}\) and \(^{2}\) which are induced by \(G\) and that satisfy \(^{1}()=^{2}()\) and that also satisfy the positivity constraints \(_{}\)._

For simplicity, we say "identifiability" to mean "constrained identifiability" in the rest of paper.

We next show that without some positivity constraints, most causal effects would not be identifiable. We say that a treatment \(X\) is a _first ancestor_ of some outcome \(Y\) if \(X\) is an ancestor of \(Y\) in causal graph \(G\), and there exists a directed path from \(X\) to \(Y\) that is not intercepted by \(\{X\}\). A first ancestor must exist if some treatment variable is an ancestor of some outcome variable.

**Proposition 5**.: _The casual effect of \(\) on \(\) is not identifiable wrt an identifiability tuple \( G,,_{}\) if some \(X\) is a first ancestor of some \(Y\), and \(_{}\) does not imply \((X)>0\)._

Figure 1: Example adapted from . Hidden variables are circled. A bidirected edge \(X\)\(\)\(\)\(\)\(\)\(Y\) is compact notation for \(X H Y\) where \(H\) is an auxiliary hidden variable.

Hence, identifiability is not possible without some positivity constraints if at least one treatment variable is an ancestor of some outcome variable (which is common). Consider the causal graph on the right where \(U\) is the only hidden variable. By Proposition 5, the causal effect of \(\{X_{1},X_{2}\}\) on \(\{Y_{1},Y_{2}\}\) is not identifiable if the considered distributions do not satisfy \((X_{2})>0\) as \(X_{2}\) is a first ancestor of \(Y_{2}\).

As positivity constraints become stronger, more causal effects become identifiable since the set of considered models becomes smaller. Consider the causal graph on the right in which all variables are observed, \(=\{X,Y,Z\}\). Without positivity constraints, \(_{}=\), the causal effect of \(X\) on \(Y\) is not identifiable. However, it becomes identifiable given strict positivity, \(_{}=\{(X,Y,Z)>0\},\) leading to the identifying formula \(_{x}(y)=_{z}(y|x,z)(z).\) This causal effect is also identifiable under the weaker positivity constraint \(_{}=\{(X|Z)>0\}\),2 which implies \((X)>0,\) so strict positivity is not necessary for identifiability even though it is typically assumed for this folklore result. This is an example where strict positivity may be assumed for technical convenience only as it may facilitate the application of some identifiability techniques like the do-calculus .

### Functional Dependencies

A variable \(X\) in a causal graph is said to _functionally depend_ on its parents \(\) if its distribution is deterministic: \((x|)\{0,1\}\) for every instantiation \(x,\). Variable \(X\) is also said to be _functional_ in this case. In this work, we assume _qualitative_ functional dependencies: _we do not know the distribution \((X|),\) we only know that it is deterministic.3_

The table on the right shows two variables \(B\) and \(C\) that both have \(A\) as their parent. Variable \(C\) is functional but variable \(B\) is not. The CPT for variable \(C\) will be called a _functional CPT_ in this case. Functional CPTs are also known as (causal) mechanisms and are expressed using structural equations in structural causal models (SCMs) . By definition, in an SCM, every non-root variable is assumed to be functional (when noise variables are represented explicitly in the causal graph).

Qualitative functional dependencies are a longstanding concept. For example, they are common in relational databases, see, e.g., , and their relevance to probabilistic reasoning had been brought up early in [22, Ch. 3]. One example of a (qualitative) functional dependency is that different countries have different driving ages, so we know that "Driving Age" functionally depends on "Country" even though we may not know the specific driving age for each country. Another example is that a letter grade for a class is functionally dependent on the student's weighted average even though we may not know the scheme for converting a weighted average to a letter grade.

In this work, we assume that we are given a causal graph \(G\) in which some variables \(\) have been designated as functional. The presence of functional variables further restricts the set of distributions \(\) that we consider when checking identifiability. This leads to a more refined problem that we call _functional identifiability (F-identifiability)_, which depends on four elements.

**Definition 6**.: _We call \( G,,_{},\) an F-identifiability tuple when \(G\) is a DAG, \(\) is its set of observed variables, \(_{}\) is a set of positivity constraints, and \(\) is a set of functional variables in \(G\)._

**Definition 7** (F-Identifiability).: _Let \( G,,_{},\) be an F-identifiability tuple. The causal effect of \(\) on \(\) is F-identifiable wrt \( G,,_{},\) if \(_{}^{1}()=_{}^{2}()\) for any pair of distributions \(^{1}\) and \(^{2}\) which are induced by \(G\), and that satisfy \(^{1}()=^{2}()\) and the positivity constraints \(_{}\), and in which variables \(\) functionally depend on their parents._

Both \(_{}\) and \(\) represent constraints on the models (CBNs) we consider when checking identifiability, and these two types of constraints may contradict each other. We next define two notions that characterize some important interactions between positivity constraints and functional variables.

**Definition 8**.: _Let \( G,,_{},\) be an F-identifiability tuple. Then \(_{}\) and \(\) are consistent if there exists a parameterization for \(G\) which induces a distribution satisfying \(_{}\) and in which \(\) functionally depend on their parents. Moreover, \(_{}\) and \(\) are separable if \((_{})=\)._

If \(_{}\) is inconsistent with \(\) then the set of distributions \(\) considered in Definition 7 is empty and, hence, the causal effect is not well defined (and trivially identifiable according to Definition 7). As such, one would usually want to ensure such consistency. Here are some examples of positivity constraints that are always consistent with a set of functional variables \(\): positivity on each treatment variable, i.e., \(\{(X)>0,X\},\) positivity on the set of non-functional treatments, i.e., \(\{()>0\},\) positivity on all non-functional variables, i.e., \(\{()>0\}.\) All these examples are special cases of the following condition. For a functional variable \(W,\) let \(_{W}\) be variables that intercept all directed paths from non-functional variables to \(W\) (such \(_{W}\) may not be unique). If none of the positivity constraints in \(_{}\) mentions both \(W\) and \(_{W},\) then \(_{}\) and \(\) are guaranteed to be consistent (see Proposition 25 in Appendix C).

Separability is a stronger condition and it intuitively implies that the positivity constraints will not rule out any possible functions for the variables in \(.\) We need such a condition for one of the results we present later. Some examples of positivity constraints that are separable from \(\) are \(\{()>0\}\) and \(\{()>0\}.\) Studying the interactions between positivity constraints and functional variables, as we did in this section, will prove helpful later when utilizing existing identifiability algorithms (which require positivity constraints) for testing functional identifiability.

## 4 Functional Elimination and Projection

Our approach for testing identifiability under functional dependencies will be based on eliminating functional variables from the causal graph, which may be followed by invoking the project-ID algorithm on the resulting graph. This can be subtle though since the described process will not work for every functional variable as we discuss in the next section. Moreover, one needs to handle the interaction between positivity constraints and functional variables carefully. The first step though is to formalize the process of eliminating a functional variable and to study the associated guarantees.

Eliminating variables from a probabilistic model is a well studied operation, also known as marginalization; see, e.g., [38; 39; 40]. When eliminating variable \(X\) from a model that represents distribution \(()\), the goal is to obtain a model that represents the marginal distribution \(()=_{x}(x,)\) where \(=\{X\}.\) Elimination can also be applied to a DAG \(G\) that represents conditional independencies \(,\) leading to a new DAG \(G^{}\) that represents independencies \(^{}\) that are implied by \(.\) In fact, the projection operation of [27; 28] we discussed earlier can be understood in these terms. We next propose an operation that eliminates functional variables from a DAG and that comes with stronger guarantees compared to earlier elimination operations as far as preserving independencies.

**Definition 9**.: _The functional elimination of a variable \(X\) from a DAG \(G\) yields a new DAG attained by adding an edge from each parent of \(X\) to each child of \(X\) and then removing \(X\) from \(G\).4_

For convenience, we sometimes say "elimination" to mean "functional elimination" when the context is clear. From the viewpoint of independence relations, functional elimination is not sound if the eliminated variable is not functional. In particular, the DAG \(G^{}\) that results from this elimination process may satisfy independencies (identified by d-separation) that do not hold in the original DAG \(G\). As we show later, however, every independence implied by \(G^{}\) must be implied by \(G\) if the eliminated variable is functional. In the context of SCMs, functional elimination may be interpreted as replacing the eliminated variable \(X\) by its function in all structural equations that contain \(X\). Functional elimination applies in broader contexts than SCMs though. Eliminating multiple functional variables in any order yields the same DAG (see Proposition 22 in Appendix B). For example, eliminating variables \(\{C,D\}\) from the DAG in Figure 1(a) yields the DAG in Figure 1(c) whether we use the order \(_{1}=C,D\) or the order \(_{2}=D,C\).

Functional elimination preserves independencies that hold in the original DAG and which are not preserved by other elimination methods including projection as defined in [27; 28]. These independencies are captured using the notion of D-separation [41; 42] which is more refined than the classical notion of d-separation [43; 44] (uppercase D- versus lowercase d-). The original definitionof D-separation can be found in . We provide a simpler definition next, stated as Proposition 10, as the equivalence between the two definitions is not immediate.

**Proposition 10**.: _Let \(,,\) be disjoint variable sets and \(\) be a set of functional variables in DAG \(G\). Then \(\) and \(\) are D-separated by \(\) in \( G,\) iff \(\) and \(\) are d-separated by \(^{}\) in \(G\) where \(^{}\) is obtained as follows. Initially, \(^{}=\). Repeat the next step until \(^{}\) stops changing: add to \(^{}\) every variable in \(\) whose parents are in \(^{}\)._

To illustrate the difference between d-separation and D-separation, consider again the DAG in Figure 1(a) and assume that variables \(C,D\) are functional. Variable \(G\) and \(I\) are not d-separated by \(A\) but they are D-separated by \(A\). That is, there are distributions which are induced by the DAG in Figure 1(a) and in which \(G\) and \(I\) are not independent given \(A\). However, \(G\) and \(I\) are independent given \(A\) in every induced distribution in which the variables \(C,D\) are functionally determined by their parents. Functional elimination preserves D-separation in the following sense.

**Theorem 11**.: _Consider a DAG \(G\) with functional variables \(\). Let \(G^{}\) be the result of functionally eliminating variables \(^{}\) from \(G\). For any disjoint sets \(\), \(\), \(\) in \(G^{}\), \(\) and \(\) are D-separated by \(\) in \( G,\) iff \(\) and \(\) are D-separated by \(\) in \( G^{},^{}\)._

We now define the operation of functional projection which augments the original projection operation in  in the presence of functional dependencies.

**Definition 12**.: _Let \(G\) be a DAG, \(\) be its observed variables, and \(\) be its hidden functional variables (\(=\)). The functional projection of \(G\) on \(\) is a DAG obtained by functionally eliminating variables \(\) from \(G\) then projecting the resulting DAG on variables \(\)._

We will now contrast functional projection and classical projection using the causal graph in Figure 1(a), assuming that the observed variables are \(=\{A,B,G,H,I\}\) and the functional variables are \(=\{C,D\}\). Applying classical projection to this causal graph yields the causal graph in Figure 1(b). To apply functional projection, we first functionally eliminate \(C,D\) from Figure 1(a), which yields Figure 1(c), then project Figure 1(c) or variables \(\) which yields the causal graph in Figure 1(d). So we now need to contrast Figure 1(b) (classical projection) with Figure 1(d) (functional projection). The latter is a strict subset of the former as it is missing two bidirected edges. One implication of this is that variables \(G\) and \(I\) are not d-separated by \(A\) in Figure 1(b) because they are not d-separated in Figure 1(a). However, they are D-separated in Figure 1(a) and hence they are d-separated in Figure 1(d). So functional projection yielded a DAG that exhibits more independencies. Again, this is because \(G\) and \(I\) are D-separated by \(A\) in the original DAG, a fact that is not visible to projection but is visible to (and exploitable by) functional projection.

An important corollary of functional projection is the following. Suppose all functional variables are hidden, then two observed variables are _D-separated_ in the causal graph \(G\) iff they are _d-separated_ in the functional-projected graph \(G^{}\). This shows that such D-separations in \(G\) appear as classical d-separations in \(G^{}\) which allows us to feed \(G^{}\) into existing identifiability algorithms as we show later. This is a key enabler of some results we shall present next on testing functional identifiability.

## 5 Causal Identification with Functional Dependencies

Consider the causal graph \(G\) in Figure 2(a) and let \(=\{A,X,Y\}\) be its observed variables. According to Definition 4 of identifiability, the causal effect of \(X\) on \(Y\) is not identifiable with respect to \( G,,_{}\) where \(_{}=\{(A,X,Y)>0\}\). We can show this by projecting the

Figure 3: \(B\) is functional.

Figure 2: Contrasting projection with functional projection. \(C,D\) are functional. Hidden variables are circled.

causal graph \(G\) on the observed variables \(\), which yields the causal graph \(G^{}\) in Figure 2(b), then applying the ID algorithm to \(G^{}\) which returns FAIL. Suppose now that the hidden variable \(B\) is known to be functional. According to Definition 7 of F-identifiability, this additional knowledge reduces the number of considered models so it actually renders the causal effect identifiable -- the identifying formula is \(_{x}(y)=_{a}(a)(y|a,x)\) as we show later. Hence, an unidentifiable causal effect became identifiable in light of knowledge that some variable is functional even without knowing the structural equations for this variable.

The question now is: How do we algorithmically test F-identifiablity? We will propose two techniques for this purpose, the first of which is geared towards exploiting existing algorithms for classical identifiability. This technique is based on eliminating functional variables from the causal graph while preserving F-identifiability, with the goal of getting to a point where F-identifiability becomes equivalent to classical identifiability. If we reach this point, we can use existing algorithms for classical identifiability, like the ID algorithm, to test F-identifiability. This can be subtle though since hidden functional variables behave differently from observed ones. We start with the following result.

**Theorem 13**.: _Let \( G,,_{},\) be an F-identifiability tuple. If \(G^{}\) is the result of functionally eliminating the hidden functional variables \(()\) from \(G\), then the causal effect of \(\) on \(\) is F-identifiable wrt \( G,,_{},\) iff it is F-identifiable wrt \( G^{},,_{},\)._

An immediate corollary of this theorem is that if all functional variables are hidden, then we can reduce the question of F-identifiability to a question of identifiability since \(=\) so F-identifiability wrt \( G^{},,_{},=\) collapses into identifiability wrt \( G^{},,_{}\).

**Corollary 14**.: _Let \( G,,_{},\) be an F-identifiability tuple where \(_{}=\{()>0\}\) and \(\) are all hidden. If \(G^{}\) is the result of functionally projecting \(G\) on variables \(\), then the causal effect of \(\) on \(\) is F-identifiable wrt \( G,,_{},\) iff it is identifiable wrt \( G,,_{}\).5_

This corollary suggests a method for using the ID algorithm, which is popular for testing identifiability, to establish F-identifiability by coupling ID with functional projection instead of classical projection. Consider the causal graph \(G\) in Figure 3(a) with observed variables \(=\{A,B,C,F,X,Y\}\). The causal effect of \(X\) on \(Y\) is not identifiable under \(()>0\): projecting \(G\) on observed variables \(\) yields the causal graph \(G^{}\) in Figure 3(b) and the ID algorithm produces FAIL on \(G^{}\). Suppose now that the hidden variables \(\{D,E\}\) are functional. To test whether the causal effect is F-identifiable using Corollary 14, we functionally project \(G\) on the observed variables \(\) which yields the causal graph \(G^{}\) in Figure 3(c). Applying the ID algorithm to \(G^{}\) produces the following identifying formula: \(_{x}(y)=_{bf}(f|b,x)_{acx^{}}(y|a,b,c,f,x^{}) (a,b,c,x^{})\) so \(_{x}(y)\) is F-identifiable.

We stress again that Corollary 14 and the corresponding F-identifiability algorithm apply only when all functional variables are hidden. We now treat the case when some of the functional variables are observed. The subtlety here is that, unlike hidden functional variables, eliminating an observed functional variable does not always preserve F-identifiability. However, the following result identifies conditions that guarantees the preservation of F-identifiability in this case. If all observed functional variables satisfy these conditions, then we can again reduce F-identifiability into identifiability so we can exploit existing methods for identifiability like the ID algorithm and do-calculus.

**Theorem 15**.: _Let \( G,,_{},\) be an F-identifiability tuple. Let \(\) be a set of observed functional variables that are neither treatments nor outcomes, are separable from \(_{},\) and that have observed

Figure 4: Variables \(A,B,C,F,X,Y\) are observed. Variables \(D,E\) are functional (and hidden).

parents. If \(G^{}\) is the result of functionally eliminating variables \(\) from \(G\), then the causal effect of \(\) on \(\) is F-identifiable wrt \( G,,_{},\) iff it is F-identifiable wrt \( G^{},,_{}, \)._

We now have the following important corollary of Theorems 13 & 15 which subsumes Corollary 14.

**Corollary 16**.: _Let \( G,,_{},\) be an F-identifiability tuple where \(_{}=\{()>0\}\) and every variable in \(\) satisfies the conditions of Theorem 15. If \(G^{}\) is the result of functionally projecting \(G\) on \(\), then the causal effect of \(\) on \(\) is F-identifiable wrt \( G,,_{},\) iff it is identifiable wrt \( G^{},,_{}\)._

Consider again the causal effect of \(X\) on \(Y\) in graph \(G\) of Figure 3(a) with observed variables \(=\{A,B,C,F,X,Y\}\). Suppose now that the observed variable \(F\) is also functional (in addition to the hidden functional variables \(D,E\)) and assume \((A,B,C,X,Y)>0\). Using Corollary 16, we can functionally project \(G\) on \(A,B,C,X,Y\) to yield the causal graph \(G^{}\) in Figure 3(d), which reduces F-identifiability on \(G\) to classical identifiability on \(G^{}\). Since strict positivity holds in \(G^{}\), we can apply any existing identifiability algorithm and conclude that the causal effect is not identifiable. For another scenario, suppose that the observed variable \(B\) (instead of \(F\)) is functional and we have \((A,C,F,X,Y)>0\). Again, using Corollary 16, we functionally project \(G\) onto \(A,C,F,X,Y\) to yield the causal graph \(G^{}\) in Figure 3(e), which reduces F-identifiability on \(G\) to classical identifiability on \(G^{}\). If we apply the ID algorithm to \(G^{}\) we get the identifying formula (which we denote as Eq. 1): \(_{x}(y)=_{af}(f|a,x)_{cx^{}}(y|a,c,f,x^{})(a,c,x^{})\). In both scenarios above, we were able to test F-identifiability using an existing algorithm for identifiability.

Corollary 16 (and Theorem 15) has yet another key application: it can help us pinpoint observations that are not essential for identifiability. To illustrate, consider the second scenario above where the observed variable \(B\) is functional in the causal graph \(G\) of Figure 3(a). The fact that Corollary 16 allowed us to eliminate variable \(B\) from \(G\) implies that observing this variable is not needed for rendering the causal effect F-identifiable and, hence, is not needed for computing the causal effect. This can be seen by examining the identifying formula (Eq. 1) which does not contain variable \(B\). This application of Corollary 16 can be quite significant in practice, especially when some variables are expensive to measure (observe), or when they may raise privacy concerns; see, e.g., .

Theorems 13 & 15 are more far-reaching than what the above discussion may suggest. In particular, even if we cannot eliminate every (observed) functional variable using these theorems, we may still be able to reduce F-identifiability to identifiability due to the following result.

**Theorem 17**.: _Let \( G,,_{},\) be an F-identifiability tuple. If every functional variable has at least one hidden parent, then a causal effect of \(\) on \(\) is F-identifiable wrt \( G,,_{},\) iff it is identifiable wrt \( G,,_{}\)._

That is, if we still have functional variables in the causal graph after applying Theorems 13 & 15, and if each such variable has at least one hidden parent, then F-identifiability is equivalent to identifiability.

The method we presented thus far for testing F-identifiability is based on eliminating functional variables from the causal graph, followed by applying existing tools for causal effect identification such as the project-ID algorithm and the do-calculus. This F-identifiability method is complete if every observed functional variable either satisfies the conditions of Theorem 15 or has at least one hidden parent that is not functional. This elimination-based method not only tests identifiability but also provides an identifying formula if the causal effect turns out to be identifiable.

We next present another technique for reducing F-identifiability to identifiability. This method is more general and much more direct than the previous one, but it does not allow us to fully exploit some existing tools like the ID algorithm due to the positivity assumptions they make. The new method is based on pretending that some of the hidden functional variables are actually observed and is inspired by Proposition 10 which reduces D-separation to d-separation using a similar technique.

**Theorem 18**.: _Let \( G,,_{},\) be an F-identifiability tuple where \(_{}=\{(X)>0,X\}\). A causal effect of \(\) on \(\) is F-identifiable wrt \( G,,_{},\) iff it is identifiable wrt \( G,_{},^{}\) where \(^{}\) is obtained as follows. Initially, \(^{}=\). Repeat until \(^{}\) stops changing: add to \(^{}\) a functional variable from \(\) if its parents are in \(^{}\)._

Consider the causal effect of \(X\) on \(Y\) in graph \(G\) of Figure 3(a) and suppose the observed variables are \(=\{A,B,C,X,Y\}\), the functional variables are \(\{D,E,F\}\) and we have \((X)>0\). By Theorem 18, the causal effect of \(X\) on \(Y\) is F-identifiable iff it is identifiable in \(G\) while pretendingthat variables \(^{}=\{A,B,C,D,E,F,X,Y\}\) are all observed. In this case, the casual effect is not identifiable but we cannot obtain this answer by applying an identifiability algorithm that requires positivity constraints which are stronger than \((X)>0\). If we have stronger positivity constraints that imply \((X)>0,X\), then only the if part of Theorem 18 will hold, assuming \(_{}\) and \(\) are consistent. That is, confirming identifiability wrt \( G,_{},^{}\) will confirm F-identifiability wrt \( G,,_{},\) but if identifiability is not confirmed then F-identifiability may still hold. This suggests that, to fully exploit the power of Theorem 18, one would need a new class of identifiability algorithms that can operate under the weakest possible positivity constraints.

## 6 Conclusion

We studied the identification of causal effects in the presence of a particular type of knowledge called functional dependencies. This augments earlier works that considered other types of knowledge such as context-specific independence. Our contributions include formalizing the notion of functional identifiability; the introduction of an operation for eliminating functional variables from a causal graph that comes with stronger guarantees compared to earlier elimination methods; and the employment (under some conditions) of existing algorithms, such as the ID algorithm, for testing functional identifiability and for obtaining identifying formulas. We also provided a complete reduction of functional identifiability to classical identifiability under very weak positivity constraints, and showed how our results can be used to reduce the number of variables needed in observational data.