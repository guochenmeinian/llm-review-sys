# Implicit Differentiable Outlier Detection Enables

Robust Deep Multimodal Analysis

 Zhu Wang &Sourav Medya &Sathya N. Ravi

Department of Computer Science, University of Illinois at Chicago

{zwang260,medya,sathya}@uic.edu

###### Abstract

Deep network models are often purely inductive during both training and inference on unseen data. When these models are used for prediction, but they may fail to capture important semantic information and implicit dependencies within datasets. Recent advancements have shown that combining multiple modalities in large-scale vision and language settings can improve understanding and generalization performance. However, as the model size increases, fine-tuning and deployment become computationally expensive, even for a small number of downstream tasks. Moreover, it is still unclear how domain or prior modal knowledge can be specified in a backpropagation friendly manner, especially in large-scale and noisy settings. To address these challenges, we propose a simplified alternative of combining features from pretrained deep networks and freely available semantic explicit knowledge. In order to remove irrelevant explicit knowledge that does not correspond well to the images, we introduce an _implicit Differentiable_ Out-of-Distribution (OOD) detection layer. This layer addresses outlier detection by solving for fixed points of a differentiable function and using the last iterate of fixed point solver to backpropagate. In practice, we apply our model on several vision and language downstream tasks including visual question answering, visual reasoning, and image-text retrieval on different datasets. Our experiments show that it is possible to design models that perform similarly to state-of-the-art results but with significantly fewer samples and less training time. Our models and code are available here: [https://github.com/ellenzhuwang/implicit_vkood](https://github.com/ellenzhuwang/implicit_vkood)

## 1 Introduction

Numerous neural network models are constructed via the stacking of explicit layers that transform inputs into outputs through a sequence of operations associated with the designated layers. Although these explicit layers are expressive, they may be unnecessary in many large-scale applications where only the function value and its gradients are required. For instance, it is possible to employ implicit layers for diverse tasks, including hyperparameter optimization , meta learning , and solving inverse problems in image processing . Indeed, if a desired input-output correspondence _within_ a network can be expressed as an optimization problem, gradients can be computed efficiently .

Recently, JFB  proposed an efficient method to backpropagate through implicit layers by implementing them as a Network Operator \(()\). Here, \(\) is defined as a sequential application of a map \(F()\), which is guaranteed to converge in \(T<\) iterations. Formally, \((x)\) can be represented as,

\[(x;)=_{T}(x;), \]

where \(\) corresponds to parameters of a potential multimodal model. The convenience of \(\) in Equation (1) lies in its ability to backpropagate through \(\) without requiring its full sequence (ortrajectory) or solving an inverse problem with its Jacobian matrix for gradients. In Vision tasks, implicit maps like clustering with fixed parameters - for instance, \(\) could be \(k-\)means with fixed means - have been shown to enhance performance . To enable small-scale custom applications or to fine-tune large-scale models, fixed parameters are insufficient, so we consider the problem of evaluating gradients with respect to the means efficiently.

**Introducing "Benign" noise in Multimodal Pipelines.** Recent advancements  have demonstrated that the combinations of multiple modalities improve understanding and boost generalization performance. However, these models, trained by a simple relation (_matched or unmatched_) between the image and text pair, often falter in capturing important semantic information and implicit dependencies within datasets. For example, as depicted in Figure 1, these models may recognize the green light but fail to infer the legality of a person crossing at the green light. Thus, the incorporation of "noisy" external knowledge presents a potential solution. In this work, we explore the design of an implicit Network Operator \(\) to manage feature-level noise during training. Specifically, \(\) is employed to integrate external knowledge in multimodal learning, which is a deep network with parameters \(\) developed to align multiple modalities .

**Using External Knowledge for Reasoning Purposes.** A knowledge graph collects information, organizes it in a specific structure, i.e., ontology, and is utilized to deduce novel insights and knowledge from the integrated information . Often, external knowledge may be beneficial for reasoning tasks . Most recent knowledge-based multimodal models  focus on entity retrieval methods from external knowledge resources such as structural knowledge graphs and large language models. Unfortunately, these methods become impractical when combined with features derived from external knowledge resources that are _prone_ to incompleteness and noise. Furthermore, vision-language models like those in  often struggle to filter noise pairs during training, resulting in slow convergence. For instance, in Figure 1, given an object "street" and relation "locatedAt", noisy objects like "fire hydrant" are returned with high confidence by ConceptNet . However, the "fire hydrant" is absent in the input image, which is undesirable for training objectives, such as image-text matching.

We propose an _implicit out-of-distribution (OOD) detection layer_ to handle outliers from external knowledge as discussed above. We demonstrate how such a layer can be expressed as a network operator \(\) in Equation (1) for memory saving and efficient training. Specifically, in the ongoing example in Figure 1, our proposed approach approximates the density of in-distribution features,

Figure 1: Our Proposed Architecture. The inputs are images and captions/questions. Image patches are processing by a visual encoder, while the caption, integrated with a knowledge graph, is passed to a language encoder. The extracted features are then fed into the OOD detection layer. This implicit layer identifies noise concepts before forwarding the features to the multimodal encoder. The ITM and MLM are training objectives, and VQA, NLVR, and image-text retrieval are downstream tasks.

which are extracted from the triplets in the caption (question) corpus, by a fixed point iterative layer. Thus, \(F\) is trained with implicit differentiation at the fixed point according to OOD scores  for the retrieved knowledge triplets. Overall, we provide an end-to-end training framework with knowledge graphs and multimodal models in an implicitly differentiable manner, as illustrated in Figure 1 showcase the effectiveness of the proposed implicit layer based on \(\) in large-scale multimodal pipelines. We conducted several experiments on multiple vision and language tasks, such as visual question answering and image text retrieval. We have included extensive evaluations and ablation studies to demonstrate the improvements in memory usage and training time compared to vanilla backpropagation.

**We summarize our three primary contributions as follows: (1)** Our proposed implicit layer can perform outlier detection tasks efficiently with less memory costs during backpropagation in practical settings. This layer can scale to different datasets and multimodal backbones. **(2)** We demonstrated that integrating explicit knowledge enhances multimodal fusion models. Our model learned from visual and textual modalities while using external knowledge seamlessly that may available in certain situations. **(3)** Resulting model after training and/or fine-tuning, outperforms baseline models on various downstream tasks. Moreover, we provided various ablation studies and an interactive simulation based user study, which shows that users can sustain their desired in-distribution inputs or features while being able to update model parameters simultaneously.

## 2 Related Work

**Deep implicit layers.** Implicit differentiation has been an emerging and competitive alternative to explicit unrolling for backpropagation. From the practical point of view, it is often memory-efficient and numerically stable compared to vanilla unrolling [7; 25]. Recent works apply implicit functions and layers in various problems with deep neural network, such as optimization layers [2; 6], convolutional sparse modeling  and object representations . Moreover, implicit layers can be used to produce representations that are robust to perturbations, as well as better empirical performance in downstream predictions. For example, [3; 5] used path independence as a technique to improve the OOD generalization. The main argument is that path independence can be guaranteed while training using implicit layers instead of unrolling which is crucial for our applications.

**Out-of-distribution (OOD) detection.** Out-of-distribution detection is usually studied from the statistical point of view, mainly under Robust Statistics . We will summarize recent developments focusing on our use cases. For OOD detection, Maximum Mahalanobis Distance  and energy score  are statistical methods for detecting features that may not be obtained using training data. Recently, GEM  has introduced a provable and competitive OOD detection method to estimate outliers for deep networks. GEM score can be used to recognize irrelevant distributions of concepts during training. The primary focus is on the detection performance with simulated outliers, whereas our focus in this paper is on utilizing them in real-world noisy data for _training_ purposes efficiently.

**Vision-and-language transformers and knowledge-based VQA.** Most recent vision-and-language models have shown improved performance by optimizing self-supervision based losses. For example, [10; 27; 32; 31; 54; 1] introduce multimodal architectures wherein cross-modal features are used to learn combined representations of visual and textual contents that can be used to improve predictions. In Vision and Language Processing, knowledge representations in the form of extracted consensus or semantic concepts are aligned with visual concepts to construct concepts vocabulary [24; 19; 60]. In knowledge-based VQA, different explicit knowledge bases can be integrated for answering purposes [40; 63; 23; 22; 53; 36; 11]. In this work, we explore the effectiveness of utilizing external knowledge in deep multimodal models during training.

## 3 Implicit OOD Detection Layer for Multimodal Analysis

**Basic Notations.** We denote input features as \(x_{i}^{d},i=1,,N\) including features of extracted knowledge triplets \(l_{j}^{d},j=1,,M\). The goal of OOD layer is to compute a score \(s(l_{j})_{+}\) for \(l_{j}\) to estimate density of in-distribution (ID) data. In this section, we explain how to estimate the distribution of the ID features as a (normalized) linear combination of simpler distributions using an implicit layer to detect outliers and backpropagate for gradients, efficiently.

### Finding Fixed Points for Forward Pass

Finite mixtures with \(k\) components are conceptually simple, and computationally attractive. Mathematically, they can produce accurate approximations to most density functions . So, we approximated the density of ID features using a Gaussian Mixture Model (GMM) and used \(_{k}^{*}^{d},_{k}^{*}^{d d},k=1,,K\) to denote the optimal means and covariance matrices of \(K\) components.

To obtain an optimal GMM, we solved \(^{*}\) and \(^{*}\) by using Expectation Maximization (EM) algorithm . Our main observation is that the update rule used in an EM-based algorithm of \(_{k}\) on the current iterate \(t T\) can be written as a fixed point iteration as follows:

\[_{k}^{t+1}^{N}(-w(_{k}^{t}))x_{i}}{_{i =1}^{N}(-w(_{k}^{t}))} \]

where the weight of current iterate \(_{k}^{t}\) on \(x_{i}\) denoted by \(w(_{k}^{t}):=_{k=1}^{K}\|_{k}^{-0.5}(x_{i}-_{k}^{t})\|_{2}^{2}\). We updated \(_{k}\) similarly using \((x_{i}-_{k}^{t})(x_{i}-_{k}^{t})^{T}\) in the numerator of Equation (2). Therefore, we obtained an approximation of in-distribution density that can be used for further outlier detection.

### GEM score for Outlier Detection

After computing the optimal means \(^{*}\) and covariances \(^{*}\) using the above described fixed point function, the final ingredient we need is a samplewise and memory efficient forward pass OOD detection method. After obtaining the ID parameters, we can compute a score for each \(l_{j}\), the features derived from external knowledge triplet. In deep network context, the recently introduced GEM score  has already been tested on a few classification applications. Specifically, we used GEM score to filter anomalous triplets. GEM score is also beneficial to obtain memory efficient gradients, details will be explained in Section 3.4. Given a derived feature from a knowledge triple \(j M\), its GEM score \(s(l_{j})\) is defined using an energy function as,

\[s(l_{j})=_{k=1}^{K}(-(l_{j}-_{k}^{*})^{T} _{k}^{-1}(l_{j}-_{k}^{*})) \]

where \(l_{j}\) is the textual feature of retrieved external triplets, \(_{k}^{*}\) is the optimal means and \(_{k}^{*}\) is the optimal covariance matrix of ID features.

**Do we require fixed point iterations in Equation (2) to be exact?** In our training pipeline, we only require that the gradients provided by the proposed OOD detection layer to be a descent direction with respect to the loss as a function of network parameters. Thus, the only use of \(^{*}\) and \(^{*}\) is to calculate OOD score \(s\) in Equation (3), so the approximate \(^{*},^{*}\) may be sufficient enough for the training purpose. To see this, we considered a simplified setup in which language features \(l\) in Equation (3) are first processed by ReLU based upstream layer parameterized by \(W_{}\), followed by energy calculation using the output means \(_{k}^{*}\) of EM algorithm.

Formally, we considered the loss function given by \(E(W_{};^{*}):=-((0.5\|(W_{} l)-^{*}\|_{2}^{2}))\) where we assumed number of components to be one i.e., \(K=1\) for simplicity. We now look at the gradients computed by using the approximate output of EM algorithm. Now, the gradients of the energy score (Equation (3)) itself is given by Chain rule as follows:

\[_{W_{}}E(W_{};^{*}) =(W_{} l)-^{*}\|_{2}^{2})))}{ W_{ }}\] \[=-((t_{0})-^{*})((t_{0 })) l^{T} \]

where \(t_{0}:=W_{} l\), and \(\) denotes the Hadamard or Elementwise product. Importantly, \(_{W_{}}E(W_{};^{*})\) is linear in \(^{*}\). By definition of a _descent_ direction, it is possible to reduce the loss using an approximate \(\) computed with finite iterations as long as \((_{W_{}}E(W_{};^{*})^{}_{W_ {}}E(W_{};))<0\). Our calculation above considered only one upstream layer which is rarely the case in practice. We leave extensions to more upstream layers as future work.

```
Input \(x_{i}\,i[N]\), \(l_{j}\,j[M]\), \(_{k}^{0}\,k[K]\), \(_{k}^{0}\,k[K]\), \(T\) Output \(s(l_{j}),_{l_{i}}s(l_{j})(l_{j},_{i}^{*},_{i}^{*})\)\(\) Feature-wise OOD scores and Gradients - Begin Forward Pass - while\(t T\)do Update \(_{k}^{t+1}\) using Equation (2) endwhile  Set \(_{k}^{*}\) to be the last iterate of \(_{k}^{T}\) OOD Detection. Compute \(s(l_{j})\) in Equation (3) - Begin Backward Pass - Jacobian Free Backpropagation. Output gradient \(_{l_{j}}s(l_{j})(l_{j},_{i}^{*},_{i}^{*})\) by computing derivative of composition of log-sum-exp and ReLU functions in Equations (3) using Chain rule.
```

**Algorithm 1** Fixed Point Network Operator based OOD Detection Layer for Language Features \(l_{j}\)

### Implementing Differentiable OOD layer in Multimodal Pipeline

To implement our implicit OOD detection layer discussed in previous sections in multimodal pipelines, we propose a novel architecture called **VK-OOD** by fusing **V**ision and external **K**nowledge features. In essence, we used an **OOD** detector as the network operator \(\) and detected the retrieved knowledge triplets which can potentially lead to slow convergence of upstream and/or downstream layers during training.

**Architecture.** Given an image with a caption (question), our pipeline consists of the following steps: **(1)** Transform the image to visual features using the vision encoder, such as ViT-based , **(2)** Retrieve knowledge triplets using external knowledge bases, and transform to textual features with the language encoder, such as BERT-based , more details of retrieval module are introduced in Appendix A, **(3)** Approximate density of in-distribution features, and filter outliers in the image-triplet pairs with the proposed implicit OOD detection layer, **(4)** Finally, we learn vision and textual representations by a multimodal encoder with multiple training objectives.

**Image-text Matching Loss with GEM scores.** We aimed to incorporate the calculated GEM scores to the widely used image-text matching (ITM) loss. Specifically, we consider the model is encouraged to not only match images and texts correctly, but also to map OOD pairs farther away from ID pairs in the feature space. Given image-text pairs \(D=\{(v_{j},l_{j})\}_{j=1}^{M}\), the similarity of image-text pair based on \(s()\) is derived as:

\[p(v_{q},l_{j})=,l_{j})/+(m(v_{q},l_{ j})) s(l_{j}))}{_{u=1}^{M}(m(v_{q},l_{u})+(m(v_ {q},l_{j})) s(l_{u}))} \]

where \(m()\) is the cosine similarity of image-text pair, \(\) is a learnable weight parameter, \(()\) is a sign function. Then, the modified ITM loss is written as:

\[_{}=_{(v,l) D}(y_{itm},p(v,l)) \]

where \(\) denotes the cross-entropy, \(y_{itm}\) corresponds to the image-text matching label. Note that, \(s()\) of the same triplets may vary on random masks and different tasks. Hence, we also modified other training objectives losses based on \(s()\), which are similar to \(_{}\). See more details in B.

### Efficient Backpropagation for OOD Detection Layer

Having found \(_{k}^{*},_{k}^{*}\) in the forward pass, we can view the initial part of our OOD network operator \(\) as the EM algorithm. It outputs the optimal parameters of ID feature density given by \(,\). However, EM viewed as an operator from \(^{d}^{d}\) (mapping \(l_{j}^{d}\) to \(_{k}^{*}^{d}\)) makes backpropagation tricky since the Jacobian of such a map will be a \(^{d d}\) matrix, practically infeasible for training purposes even when \(d 100\) is not very large. Hence, in this section, we describe how we adopted Jacobian-Free backpropagation for the implicit OOD detection layer.

**Applying Chain Rule with Limited Unrolling for Gradients.** Since each iteration in GMM as in Equation (2) is differentiable, we can easily backpropagate through few iterations of EM algorithm to compute gradients. First we used Chain rule to illustrate the necessary components to obtain gradients. Note that from Equation (3), we know that \(_{s}\) is a scalar, and moreover for a fixed \(,s\) itself is a smooth scalar function \(s_{}:^{d}\). Thus, we consider the following composition of maps to compute \(_{x}^{d}\) gradient with respect to \(x_{i}\) (denoted generically as \(x\)),

\[x^{*} s\  \]

where \(^{*}\) is the output of our OOD layer from forward pass. The key difficulty is in approximating the Jacobian \(J_{x,^{*}}^{d}\) and/or its inverse for backpropagation since it maybe dense. We now describe three standard ways that can be used to backpropagate through implicit layers. **First**, in the vanilla backpropagation often called as "unrolling" is used where each iterate is stored, and a "path" gradient approximation is calculated. That is, since the parmeters of each unrolled layer are fixed, we can simply use them along with the closed form gradients available for Equation (2) for backpropagation. **Second,** Jacobian based methods form an approximation to the Jacobian inverse or in other words, solve a linear system formed using the fixed point condition in Equation (2) itself. Often, this approach yields better gradients. **Third,** Jacobian-Free Backpropagation (JFB) combines both these approaches, that is, in which we simply use or unroll the last few iterates of the EM algorithm for gradients. As we can see, JFB enables to train with fixed memory costs (\((1)\)) and efficient backpropagation without computing gradients at each iterations. The time complexity is \((n)\) for forward and \((1)\) for backprop. We provide empirical results of the above three methods in Section 4.1.

Benefits of EM algorithm.EM updates are provably convergent in various settings, and it takes few iterations when it is guaranteed to converge . This also implies that we can simply initialize \(_{k}\) randomly and perform few more iterations to find a fixed point. However, since the denominator in Equation (2) contains terms that are also unknown parameters, such update schemes may be numerically unstable. In such cases, we can simply use recently proposed gradient based EM algorithms as the network operator, for fine-tuning deep networks in large scale settings, see  for convergence analysis.

Interaction via Outlier Detection.The applications which require a high number of image patches (or concepts), the likelihood that one of the patch features or text features to be an outlier also increases dramatically. In high dimensional settings, this increases the training time taken by first order methods significantly, especially when mini-batches are used to compute gradients . Alternatively, when features \(l_{j}\) are computationally easy to extract, say using CLIP , it is reasonable to expect that a certain fraction of the \(l_{j}\) are outliers, and should not be used for backpropagation purposes. In a more optimistic scenario, we consider to customize our predictions, and handle "on-the-fly" integration of explicit knowledge. In our framework, this corresponds to treating \(_{k}\) in Equation (3) as trainable parameters. We can update the initialization \(_{k}\) without storing the trajectory, or forming the full Jacobian which can be expensive, as in our Algorithm 1.

## 4 Experiments

In this section, we present empirical results addressing the following questions: (1) How effective and robust is the OOD detection layer? (2) What improvement does the Jacobian-Free backpropagation (JFB) provide in computational cost within our VK-OOD architecture? (3) How do external knowledge resources influence the performance of multimodal pipelines? (4) Can VK-OOD work with different backbones in large-scale settings for various downstream tasks? To answer (1) and (2), we performed experiments on VQA task with various setups, comparing memory and training costs with unrolled EM (vanilla), Jacobian-based-EM, and JFB-EM. To answer (3), we evaluated the influences of knowledge resources by explicitly (or intentionally) introducing outliers in the pipeline. Finally, for (4) we utilized ViLT , CLIP , and BLIP  as the backbone model and illustrate performance on downstream tasks, including natural language for visual reasoning and image-text retrieval. More implementation and training details are described in the Appendix C.1. For ablation studies, we fixed the backbone model to be ViLT with pretrained parameters.

### Results and analysis of OOD detection layer

**Backprop methods.** We used different backpropagation methods in OOD detection layer with ViLT as the backbone. As shown in Table 1, compared to vanilla and Jacobian-based methods, JFB-EMoutperformed them in term of accuracy on VQAv2 task. It is trained with significantly less memory and time cost per epoch, which is beneficial for large-scale settings. Additionally, we conducted experiments on \(T\) - number of iterations, which is a critical parameter to compute fixed points. The results are shown in Figure 2(a). We found the improvements to be marginal for \(T 5\), and JFB achieves faster performance on all settings. Based on the results in Figure 2(a), we fixed \(T=10\) for the subsequent experimental settings. Additionally, we have provided the fixed point error plot over iterations in Figure 2(b). We observed that the squared euclidean distance between successive iterates indeed went to zero showing convergence of forward pass.

**Number of Clusters.** We investigated the effects of the number of clusters on optimizing the GMM process, and the results are presented in Figure 2(c) (see green line). The general trend indicates that performance improves with an increase in the number of clusters. Qualitatively, Figure 2 shows that learned output features using U-MAP embeddings . Feature embedding spaces of multiple modalities on COCO val dataset are shown in different colors representing different clusters. We provide example images of the clusters in Appendix C.3. The results show that our VK-OOD model can identify clusters over the extracted multimodal features - our layer can accurately detect outliers.

**Robustness of OOD layer.** Usually OOD features are not present within the training datasets themselves. However, we may encounter outliers when integrating external knowledge triplets into the training pipeline. If we denote \(M\) as the number of external knowledge triplets, then \(M=0\) corresponds to the ID setup - no outliers. To delve into further quantitative analysis, we considered two setups: one with \(M=0\), i.e., ID setup, and OOD setup with \(M=5\) (so possibly \(5\) outliers per language caption) that corresponds to augmenting features from external knowledge. We can see from the results in Table 1(b) that there is not a significant difference from the the rate of convergence perspective -- as indicated by squared norm of successive iterates \(\|_{t}-_{t+1}\|_{2}^{2}\) -- in both setups. However, from the accuracy (Acc) column in Table 1(b), we can conclude that the performance in VQA tasks has significant improvements over iterations when considering external knowledge.

   Method & \#Param(M) & \#FLOPs(G) &  &  &  \\  & & & Forward & & Backward & \\  Vanilla-EM & 152.6 & 185.2 & 39.6 & 26.8 & 18673 & 76.6 \\ JB-EM & 125.2 & 115.7 & 39.6 & 12.7 & 14512 & 76.6 \\ JFB-EM & 124.8 & 108.6 & 39.6 & 6.3 & 13674 & 76.8 \\   

Table 1: Experimental results of different backpropagation method in the dense OOD detection layer. JFB-EM is much more efficient in backward pass and use less memory. It also outperforms on the VQAv2 task in terms of accuracy.

Figure 3: Ablation studies results on VQA task. (a) Results on accuracy with different \(T\) iterations in EM on VQAv2 dataset. (b) \(\|_{t}-_{t+1}\|_{2}^{2}\) over EM iterations in forward pass.(c) Results on accuracy with different numbers of external triplets and the number of clusters in the GMM process (see Equation (2)) on OKVQA dataset.

Figure 2: Visualization of the multimodal feature space. Here, \(k\) denotes the number of clusters.

**Incomplete Knowledge Triplets.** To evaluate the sensitivity of models to OOD detection performance, we conducted experiments of incomplete knowledge triplets with missing values. Note that, we only dropped language features inputs due to computational reasons - augmenting image patches requires more resources such as GPUs. However, since \(x\) is used for both visual and language features, the implementation remains the same as that of dropping patches in language features. So, it is equivalent to drop either the visual or the language feature since the computational effort involved in running EM algorithm depends only on the total number of features. We now present more results of \(\|_{t}-_{t+1}\|_{2}^{2}\) and OKVQA performance in term of accuracy over optimization iterations in Table 1(a). With higher level of incompleteness, the rate of convergence is slower as expected. Once again, as in previous robustness experiments, we found accuracy gain over iterations here also.

### Ablation study on VK-OOD Components and External Knowledge

**Effectiveness of Each Component.** To compare the impact of the proposed OOD layer in VK-OOD, we considered different combinations of inclusion and exclusion of knowledge graph representations (KG) and OOD detection layer. The results are shown in Table 3. The results show that our model achieves the best performance when both the components are included in the model. Moreover, comparing the results on VQAV2 and OKVQA datasets, the results imply that external knowledge triplets (KG) can be beneficial to improve the performance especially on VQA task. Furthermore, using OOD layer even when there are no external knowledge triplets has good performance. This shows that including OOD layer in our model is helpful and able to capture the noise of multiple modalities, such as missing or mismatching modalities.

**Numbers of external knowledge triplets.** We conducted experiments to explore the impact of the amount of the knowledge triplets. We evaluate this on VQA tasks using OKVQA dataset. Figure 2(c) shows the experimental results. As expected, we observed that increasing the number of retrieved knowledge triplets improve the accuracy of predicted answers. We achieved the best accuracy of \(52.4\%\) when the number of triplets is \( 5\).

**Knowledge resources.** We evaluated different knowledge resources, i.e., different embeddings of implicit and/or explicit knowledge. Table 4 shows benefits provided by different external resources in our pipeline. We queried knowledge from Wikidata  and used BERT to get embeddings \(l_{j}^{d}\). Our model produced 1.8% and 18.7% more accurate results than the best and worst performing baselines respectively. Moreover, using ConceptNet embeddings solely, our multimodal training pipeline also learn implicit knowledge in the multimodal fusion encoder. We compared our model performance with the models using external knowledge resources. As results shown in Table 4, all the different external knowledge resources brought improvements in the OKVQA tasks. Our proposed model takes advantages of implicit knowledge from vision-language models and integrates explicit knowledge prior information, thus outperforms other models using external knowledge resources without OOD detection.

### Scalability of VK-OOD

In this section, we incorporated the proposed OOD detection layer to various backbone models with different architectures and model sizes. We initialized the parameters (\(\) and \(\)) of our proposed

Table 2: Fixed point error is measured using consecutive iterate distance \(\|_{t}-_{t+1}\|_{2}^{2}\). (a) \(\) error and OKVQA accuracy over the EM optimization iterations in different level of incompleteness. (b) \(\) error and VQA accuracy over the EM optimization iterations in ID and OOD setups.

   &  \\  KG & OOD & VQAV2 & OKVQA \\   & & 73.9 & 45.5 \\ ✓ & & 74.6 & 48.3 \\  & ✓ & 74.1 & 46.2 \\ ✓ & ✓ & **76.8** & **52.4** \\  

Table 3: Ablation studies of different components of VK-OOD. “KG” and “OOD” denote knowledge graph and OOD detection layer respectively.

OOD detection layer in two different ways, scalar \(\) or the dense one which \(\) is a \(d d\) matrix, where d is the dimension of input embeddings. As shown in Table 5, the number of parameters in scalar VK-OOD are approximately similar to the baselines. Specifically, comparing to other baseline models, while our _scalar_ VK-OOD increases the #-parameters slightly \(- 0.4\) million more parameters (since \(d 700\)) - it **significantly** improved the performance in downstream tasks.

We then conducted experiments on several downstream tasks. In all these experiments, our model consistently achieved the best and second-best performance compared to _five state-of-the-art (SOTA)_ vision-language models across _three downstream tasks_ on various datasets(see Table 5). We trained on open-source data and compare the results with the models having similar number of parameters. In particular, for VQA tasks, we evaluated on the VQAv2 test set. As shown in Table 5, our model outperformed all the baselines on this dataset, yielding an accuracy of 77.9%. Furthermore, in the Natural Language for Visual Reasoning (NLVR) task, VK-OOD achieved the best and second-best result in terms of accuracy with different backbone models, respectively. We also evaluated our model, along with the baseline models, on the test set of COCO and F30K dataset for image-text retrieval task. Our model produced the best performance and outperformed the best and worst-performing baselines, with the exception of BLIP, by up to 0.5% and 11.6% respectively on the COCO dataset (Table 5). In other settings and on the F30K dataset, the results are similar. Further details are provided in Appendix C.2.

The performance of our model demonstrates its ability for visual reasoning while integrating both implicit and explicit knowledge. Moreover, we are able to show that the proposed OOD detection layer can scale with different models and contribute improvements to various tasks. For inference, since \(\) and \(\) are fixed, the inference time is similar to the backbone models considered - the average inference time for one VQA instance is around 57 ms on a single 2080Ti GPU. Therefore, we believe that our implicit layer can function as a plug-and-play module and can be easily integrated with other vision-language models.

### Qualitative Analysis

**Visualizing Attention with Multimodal Information.** Figure 4 is an example of multimodal alignment results from our VK-OOD representations. We use Grad-cam  to visualize the multimodal maps of the models on the image corresponding to knowledge triplets.

  Method & Knolwedge resources & OKVQA \\  ConceptBERT & CN & 33.7 \\ KRISP & Wiki + CN & 38.4 \\ MAVEx & Wiki + CN + GI & 39.4 \\ KAT-B & Wiki + GPT3 & 50.6 \\ UnifER & CN + ViLT & 42.1 \\   & CN - w/o BERT & 51.1 \\  & Wiki & 51.9 \\   & Wiki + CN & 52.2 \\   & CN & **52.4** \\  

Table 4: Results on the different knowledge resources on OKVQA dataset. “CN” and “GI” denote ConceptNet and Google Images respectively.

   &  &  &  &  &  \\  & & & & TR R@5 & IR R@5 & TR R@5 & IR R@5 \\  ViLT & 87 & 70.3 & 74.6 & 86.2 & 72 & 95.6 & 86.8 \\ UNITER & 155 & 72.7 & 75.8 & 87.4 & 78.5 & 97.1 & 92.4 \\ ALBEF & 314 & 74.5 & 80.5 & 91.4 & 81.5 & 99.4 & 96.7 \\ VinVL & 157 & 75.9 & 83.1 & 92.6 & 83.2 & - & - \\ BLIP* & 346 & 77.5 & 82.8 & 95.2 & **85.4** & **99.8** & **97.5** \\  VK-OOD-s(ViLT) & 87.4 & 76.7 & 84.3 & 90.9 & 81.6 & 97 & 94.3 \\ VK-OOD-s(CLIP) & 113.4 & 76.2 & 83.8 & 92.8 & 83.4 & 99.6 & 96.7 \\ VK-OOD-s(BLIP) & 346.4 & 77.8 & 84.1 & **95.4** & 85.2 & **99.8** & 97.2 \\ VK-OOD-1(ViLT) & 125 & 76.8 & **84.6** & 91.7 & 81.3 & 97.2 & 94.5 \\ VK-OOD-l(CLIP) & 151 & 76.1 & 83.9 & 93.1 & 83.6 & 99.6 & 96.8 \\ VK-OOD-l(BLIP) & 412 & **77.9** & 84.5 & 95.1 & 84.8 & 99.6 & 97.1 \\  

Table 5: Overall performance on multiple downstream tasks. We demonstrate VK-OOD scale with different model backbones and achieve the best and second-best results. VK-OOD-s is the scalar case, and VK-OOD-l is the dense case. *our implementation.

Figure 4, our model has the capability to attend to the extracted knowledge concepts, such as buildings and traffic lights. Thus, our model can detect more objects to provide the ability for answering open questions. We discuss more user studies on interactive OOD detections by feeding in domain knowledge with different distributions in the Appendix C.3.

Furthermore, we present prediction results in Figure 5 with VK-OOD model on OKVQA dataset along with the extracted knowledge triplets based on the captions. In the example, the caption \(\)apple, used for, making apple pie\(\) is useful to obtain correct answers. This observation validated that explicit knowledge provides more reasoning capability than implicit knowledge. Moreover, our model detected OOD triplets by combining modalities, i.e, the apple fruit is in image, thus is not used for computing. The last one is a failure case, because the ground truth caption is not sufficiently specific. Therefore, it might be beneficial to consider more inference and reasoning abilities in multimodal analysis, such as chain of thoughts .

## 5 Discussion

**Limitations.** A potential limitation of our proposed implicit layer arises when the covariance matrix \(\) is dense. In such cases, a fast linear system solver would be required to evaluate the likelihood. We will consider to explore sparse approaches to further saccelerater OOD detection layer in the future. Additionally, given that we have demonstrated that explicit knowledge can serve as supervision in vision-language training, we believe that various knowledge bases, such as medical knowledge graphs, can provide user-desired domain distributions.

**Conclusions.** In this work, we presented a training framework designed to facilitate multimodal analysis under distribution shifts and/or the presence of outlier distributions within the input feature space. Several other models have been proposed to exploit special structures in the available modalities for faster training purposes, as mentioned in , and for tasks involving egocentric vision . While the approaches have been shown to perform well in large-scale settings, they may not be sufficient on their own. For instance, most frames in a video have low semantic information content and may require complex processing pipelines . We argued that handling outliers within the context of multimodal analysis is an crucial topic as more models are integrated or fused. Our implicit OOD detection layer can be directly instantiated within such complex pipelines, possibly allowing us to intervene and accelerate the training process while reducing computation costs. We demonstrated through extensive empirical analysis across various setups that incorporating OOD detection in the training pipeline can significantly enhance the performance on downstream tasks.

Figure 4: Visualization of the attention maps of image and triplets alignment. The original sample caption is “A man riding a bicycle down a city street”. We highlight areas in the image corresponding to different knowledge triplets. Our model learns different objects and localize those objects correctly.

Figure 5: Example case studies with OKVQA dataset. We show the examples of retrieved knowledge triplets. Our model is able to detect outliers of retrieved triplets shown in orange. The predicted answers are finetuning on OKVQA dataset. Comparing with the baseline results, our model provides more correct answers. Note that, the baseline model here is our implementation without KG and OOD components.