# GAIA: Rethinking Action Quality Assessment for AI-Generated Videos

Zijian Chen\({}^{1}\), Wei Sun\({}^{1*}\), Yuan Tian\({}^{1}\), Jun Jia\({}^{1}\), Zicheng Zhang\({}^{1}\),

**Jiarui Wang\({}^{1}\), Ru Huang\({}^{2}\), Xiongkuo Min\({}^{1*}\), Guangtao Zhai\({}^{1*}\), Wenjun Zhang\({}^{1}\) \({}^{1}\)**Shanghai Jiao Tong University \({}^{2}\)East China University of Science and Technology \({}^{*}\)Corresponding authors [https://github.com/zijianchen98/GAIA](https://github.com/zijianchen98/GAIA)

###### Abstract

Assessing action quality is both imperative and challenging due to its significant impact on the quality of AI-generated videos, further complicated by the inherently ambiguous nature of actions within AI-generated video (AIGV). Current action quality assessment (AQA) algorithms predominantly focus on actions from real specific scenarios and are pre-trained with normative action features, thus rendering them inapplicable in AIGVs. To address these problems, we construct **GAIA**, a **G**eneric **AI**-generated **A**ction dataset, by conducting a large-scale subjective evaluation from a novel causal reasoning-based perspective, resulting in 971,244 ratings among 9,180 video-action pairs. Based on GAIA, we evaluate a suite of popular text-to-video (T2V) models on their ability to generate visually rational actions, revealing their pros and cons on different categories of actions. We also extend GAIA as a testbed to benchmark the AQA capacity of existing automatic evaluation methods. Results show that traditional AQA methods, action-related metrics in recent T2V benchmarks, and mainstream video quality methods perform poorly with an average SRCC of 0.454, 0.191, and 0.519, respectively, indicating a sizable gap between current models and human action perception patterns in AIGVs. Our findings underscore the significance of action quality as a unique perspective for studying AIGVs and can catalyze progress towards methods with enhanced capacities for AQA in AIGVs.

## 1 Introduction

Action quality assessment (AQA), which aims to quantify _how well_ actions are performed, is a growing area of research across various domains (_e.g._, ). It is becoming especially challenging since generative models like Sora  have revolutionized the creation of visually realistic videos. Assessing how well an action is presented can be difficult because of the inherent difference between real videos and generated videos . At minimum, a well-performed action should correctly contain all relevant objects as well as the action subject with recognizable motion presentation while conforming to the physical world dynamics . Moreover, the exponential growth of text-to-video (T2V) models has also given rise to formidable challenges in the evaluation of video action quality, underscoring the increasing need for reliable solutions.

However, there is a significant gap in the existing AQA research. First, prior work has contributed multiple AQA datasets, which predominantly focus on _domain-specific_ actions from real videos and collect coarse-grained _expert-only_ human ratings  on limited dimensions. Meanwhile, the content discrepancies in those AQA videos are often subtle, as the action subjects typically perform similar actions within a consistent environment. Examples include swimming and diving in a natatorium or gymnastics in a gym, which lacks consideration for scene diversity. Second, theexisting AQA approaches mainly follow a pose-based or vision-based feature extraction, aggregation, and score regression ternary form, which usually adopt powerful 3D backbone networks that are pre-trained on large action recognition datasets  for better feature migration. Nevertheless, a distinguishing characteristic of generated videos is that they may contain atypical actions with various body or object artifacts over time , such as aberrant limb count, irrational object shape, and physically implausible motion, due to the stochasticity and unstable nature of the diffusion process. In such cases, the model learned from real action videos may fail in AIGVs with worse prediction performance. At present, it remains unclear to what degree any T2V model can achieve visually rational action generation that varies in action categories, much less the cognitive mechanism of action quality that affects human perception.

To address these issues, we present **GAIA**, a **G**eneric **AI**-generated **A**ction dataset encompassing 9,180 AI-generated videos from 18 T2V models, spanning both lab studies and commercial platforms, which covers a variety of whole-body, hand, and facial actions. Specifically, we recruit 54 participants and conduct a large-scale human evaluation to evaluate the action quality _first-of-this-kind_ from three causal reasoning-based perspectives: subject quality, action completeness, and action-scene interaction. Among them, as the major premise of an action, the quality of the action subject directly affects the whole action process. Assessing action completeness ensures that the generated action is not only temporally coherent but also logically and narratively complete. Action-scene interaction considers the spatial relationships, environmental factors, and interactions with other elements within the scene that can influence the perception of the action's quality and realism. Crucially, it provides quantifiable action state estimations based on the behavior of human reasoning in perceiving an action. In theory, this makes complicated coupled action quality approachable and tractable. In practice, the full potential of multi-dimension methods remains largely untapped due to a scarcity of existing datasets, exacerbated by the difficulty of obtaining reliable group subjective opinions. This complements earlier research, which primarily concentrated on AQA under a single real scenario and lacked rating granularity.

We prove the value and type of insights GAIA enables by using it to evaluate the action generation ability and weaknesses across different categories of 18 representative T2V models (several times more than existing benchmarks ). Moreover, we contribute a holistic benchmark based on GAIA, which reveals that the existing AQA methods and action-related automatic metrics even video quality assessment (VQA) approaches, correlate poorly with human evaluation. Overall, our study could serve as a pilot for future endeavors aimed at developing accurate AQA methods in generative scenarios while providing substantial insights for better defining the quality of AIGVs.

Figure 1: **Data construction pipeline and content overview of GAIA. (a)** Curation process of the GAIA dataset, resulting in 9,180 videos with 971,244 human ratings. (b) The distribution of unique actions per class. (c) 3D scatter plot of the mean opinion score (MOS) in three dimensions and video examples with diverged scores.

[MISSING_PAGE_FAIL:3]

advanced visual AI agents to achieve high-quality, long-form video generation. In addition to the above laboratory studies, several derived commercial video generation products, _e.g._, Gen-2 , Genmo , Pika , Neverends , MoonValley , Morph , Stable Video , and Sora , have harvested widespread attention from both academia and industry, exhibiting great possibilities for future AI-assisted video creation.

**Evaluations on Video Generative Models.** Early video generation models shared the same frame-wise evaluation metrics as T2I models, such as Inception Score (IS) , Frechet Inception Distance (FID) , and CLIPScore , as well as their variants for video . These metrics are all group-targeted and not suitable for assessing a single video. For text-to-video (T2V) models, several benchmarks  have been proposed to assess various perspectives like video fidelity , temporal quality , text-video alignment . Despite covering various dimensions, these works lack specificity and breadth with limited model exploration and human group annotation. Our work differs from current research in three key aspects: 1) We created 510 distinct action prompts covering both coarse-grained and fine-grained actions, each applied with 18 T2V models for extensive assessment. 2) Our casual reasoning-based and multi-dimensional action quality evaluation offers valuable and comprehensive insights into video generation. 3) We have quantitatively validated a large amount of existing metrics that none of them performs well on the AI-generated action quality assessment task.

## 3 Dataset Acquisition

### Data Collection

**Prompt Sources.** The marvelous interrelation and working mechanism of body, hand, and face have a high degree of inner unity, which together constitute the key elements of actions . Hence, we sampled action keywords for GAIA from a variety of sources, including the Kinetics-400  for whole-body actions, the EgoGesture dataset  and the _valence-arousal_ model of affect  for fine-grained local hand and facial actions, respectively (Fig. 1(b)). Besides, to avoid linguistic bias and ensure each action keyword appears explicitly in the prompt, we leverage the GPT-4  to design an assembled prompt strategy (Fig. 1(a)). It consists of a _common head_, an _action-oriented description_, and an _output control_, where we intentionally leave out specialized suffixes such as _8k_, _HDR_, _photographic_, and _high fidelity_ for fairness. In the meantime, an expert review of the generated prompts is organized to examine the hallucination problem of large language model (LLM) while avoiding NSFW issues for ethical concerns. At last, we obtain 510 prompts for all action categories with an average length of 8.25 words.

  
**Model** & **Year** & **Mode** & **Resolution** & **FPS** & **Length** & **Speed** & **Feature** & **Open Source** \\  CogVideo  & 22.05 & T2V & 480\(\)480 & 8 & 4s & 12s & \(-\) & \(\) \\ Text2Video-Zero  & 23.03 & T2V & 512\(\)512 & 4 & 2s & 21s & \(\) \\ ModelScope  & 23.03 & T2V & 256\(\)256 & 8 & 2s & 6s & \(-\) & \(\) \\ ZeroScope\({}_{ 5\%}\) & 23.06 & T2V & 576\(\)320 & 8 & 3s & 20s & \(\) \\ LaVie  & 23.09 & T2V & 512\(\)320 & 8 & 2s & 14s & Interpol./Super Res. & \(\) \\  & 23.10 & T2V, 2V & 512\(\)320 & 8 & 2s & 41s & \(-\) & \(\) \\ VideoCrafterf  & 23.10 & T2V, 12V & 1024\(\)576 & 8 & 2s & _OOM_ & \(-\) & \(\) \\ Show-1  & 23.10 & T2V & 576\(\)320 & 8 & 4s & 231s & \(-\) & \(\) \\  & 23.10 & T2V & 672\(\)384 & 8 & 1s & 14s & Personalized & \(\) \\  & 23.12 & T2V, 2V & 384\(\)256 & 8 & 2s & 10s & Cam. Ctrl & \(\) \\ VideoCrafterf  & 24.01 & T2V, 12V & 512\(\)320 & 8 & 2s & 45s & \(-\) & \(\) \\  & 24.03 & T2V, 12V, 2V & 1024\(\)576 & 25 & \(-\)12s & _OOM_ & Multi-Agent & \(\) \\ Gen-1  & 23.02 & V2V & 768\(\)448 & 24 & 4s & 52s\({}^{}\) & Style & \(-\) \\ Genmo  & 23.10 & T2V, 2V & 2048\(\)1536 & 15 & 4s & 60s\({}^{}\) & Style, Cam. Ctrl & \(-\) \\ Gen-2  & 23.12 & T2V, 12V & 1408\(\)768 & 24 & 4s & 140s\({}^{}\) & Mot./Cam. Ctrl & \(-\) \\ Pika  & 23.12 & T2V, 12V, 2V & 1088\(\)640 & 24 & 3s & 45s\({}^{}\) & Mot./Cam. Ctrl, Sound & \(-\) \\  & 23.12 & T2V, 12V & 1024\(\)576 & 10 & 3s & 260s\({}^{}\) & \(-\) & \(-\) & \(-\) \\  & 24.01 & T2V, 12V & 1184\(\)672 & 50 & 4s & 386s\({}^{}\) & Style, Cam. Ctrl & \(-\) \\  & 24.01 & T2V, 1290\(\)1080 & 24 & 3s & 196s\({}^{}\) & Mot./Cam./fps Ctrl & \(-\) \\  & 24.03 & T2V, 12V & 1024\(\)576 & 24 & 4s & 125s\({}^{}\) & Style, Mot./Cam. Ctrl & \(\) \\   

Table 2: Summary of popular video generation models: from _open-source_ lab studies to large-scale _commercial_ creation platforms. We tested the average generation speed (seconds/item) on an NVIDIA RTX4090 locally, except for those closed-source models. OOM is the abbreviation of _out-of-memory_. \({}^{}\)We report the online generation speed under free plan.

**Text-to-Video Models.** To evaluate the action quality of AI-generated videos thoroughly, we select **18** representative T2V models for generation including: 1) **11** open-sourced lab studies: Text2Video-Zero , ModelScope , ZeroScope , LaVie , Show-1 , Hotshot-XL , AnimateDiff , VideoCrafter (resolution at 512\(\)320 and 1024\(\)576) , VideoCrafter , and Mora ; 2) 7 popular commercial creation applications: Gen-2 , Genmo , Pika , NeverEnds , MoonValley , Morph Studio , and Stable Video , shown in Tab. 2. Note that LogVideo  and Gen-1  are excluded due to the language and mode restrictions. Since we focus on human-centric actions in this paper, other settings such as camera motions or styles are set by template. At last, **9,180** videos were collected. _We defer more details to the Appendix (Sec. B.1)_.

### Task Definition: the Action Syllogism

Considering the peculiar characteristics of AI-generated videos, to collect a more explainable and nuanced understanding of public perception on action assessment, instead of collecting professional skill scores as in existing AQA studies [76; 115], we opt to collect annotations from a novel perspective, namely the causal reasoning _syllogism_[93; 54]. Specifically, we decompose an action process into three parts: 1) action subject as _major premise_, 2) action completeness as _minor premise_, and 3) interaction between action and scenes as _conclusion_, according to the syllogism theory. The rationale for this strategy is as follows: **(a)** The visibility of the action in videos is greatly affected by the rendering quality of the action subject, which is a crucial element of visual saliency information, while humans excel at perceiving such generated artifacts [21; 51; 70]. **(b)** Moreover, unlike _parallel-form_ feedbacks, the order of these three parts in action syllogism inherently aligns with the human reasoning process. For instance, while human annotators are shown with an action scene about "_A musician is playing the piano_", they can intuitively reason like: \(\) a musician as the major premise, which is the subject to execute the action of _playing the piano_; \(\) appearance or completeness of action as the minor premise, containing the spatial and temporal boundaries in the given scenario; \(\) phenomenon of the keys being pressed as the conclusion describing a reasonable result considering both constraints. This reasoning-form evaluation has many merits. First, by breaking down an action into its constituent parts, researchers can more clearly identify and analyze the specific elements that contribute to the perceived quality of the action. Second, such causal reasoning-based strategy is inherently aligned with human perception and can help in understanding how different parts of action are perceived by the public, which can lead to insights into what makes AI-generated action convincing or unconvincing. Third, this scheme allows for a comparative analysis of AI-generated action against natural human action, revealing where AI excels and where it may need improvement.

### Subjective Action Quality Assessment

**Participants and Apparatus.** To ensure the comprehensiveness, fairness, and reliability of the evaluation, we recruit a total of **54** participants to participate in our human evaluation, as shown in Tab. 3. All with normal (corrected) eyesight. Considering the viewing effect, a 27-inch Lenovo monitor with a resolution of 2560x1440 is used for video display. The viewing distance and optimal horizontal viewing angle are set as 1.9 times the height of the display (\( 70\)cm) and [\(31^{},58^{}\)], respectively. Before the annotation, we instructed all participants to have a clear and consistent understanding of all evaluated aspects and tested their eligibility via a 30-video pre-labeling . In the tutorial for each dimension, participants are guided to rate 10 generated-real video pairs with the same caption. Their answer is compared with ground-truth ratings that were developed by multiple experts. Raters needed to achieve at least 75% ratings that satisfied \(|ground\_truth-rating|<1.5_{expert}\) to move on to the formal study.

**Main Process.** We adopted a single-stimulus methodology in this evaluation and asked participants to focus on the given action keyword as well as the corresponding prompt and evaluate three action-related dimensions of AI-generated videos, _i.e._, _subject quality_, _action completeness_, and _action-scene interaction_, by dragging the slide button at a \(\) continuous rating scale. We randomly divided the 9,180 videos in GAIA into 31 sessions, with each session, except the last, comprising 300 videos. Ten _golden videos_ with expert opinions from the real-world action database  were added to each session as an inspection to control the scoring deviations. Only participants who had a high agreement

  
**Category** &  &  &  \\   & Male & Female & \(w\)/AGC & \(w\)/AGC & **Age** \\ 
**Number** & 39 & 15 & 25 & 29 & 23.4\(\)2.6 \\   

Table 3: **Statistics of participants. \(w\)/AIGC and \(w\)/\(o\)AIGC denote participants who have or do not have used AI generation tools, respectively.**(Pearson linear correlation coefficient, \(PLCC>0.7\)) with the mean opinion score (MOS) from experts were eligible to continue to the next session, leaving 48 remaining. To reduce visual fatigue, there is a rest segment with at least 15 minutes per 150 videos [90; 21; 19]. In summary, it took participants approximately 2.6 hours to finish one session, and all experiments took over a month to complete. Each participant was compensated $12 for each session according to the current ethical standard .

**Quality Control.** In addition to the above pre-labeling and in-process check trial, we noticed 5 line clickers (all male) with over 40% of the same ratings. We removed all their ratings from GAIA dataset. Besides, we follow Otani _et al._'s  recommendation that uses the inter-annotator agreement (IAA) metric (Krippendorff's \(\)) to assess the quality of ratings, where Krippendorff's \(\) for _subject quality_, _action completeness_, and _action-scene interaction_ perspectives are 0.6771, 0.6243, and 0.6311, respectively, indicating appropriate variations among annotators. We further calculated the SRCC score using bootstrapping as in KonIQ-10k . Fig. 2 shows the mean agreement (Spearman rank-order correlation coefficient, SRCC) between the MOS values as the number of observers grows. When considering the correlation between nearly 70% of the participants in our study, the mean SRCC reaches remarkably high values of 0.9556, 0.9531, and 0.9627 in terms of _subject quality_, _action completeness_, and _action-scene interaction_, respectively, which provides a reasonable reference population size for subsequent subjective AQA studies. At last, we obtained a total of **971,244** reliable ratings with an average of **105.8** ratings per video (**35.27** per dimension). We then perform Z-score normalization to the raw MOS of each subject to avoid inter-annotator scoring biases. Here, we abbreviate the MOS of three perspectives as \(_{s}\), \(_{c}\), and \(_{i}\) according to their initials for simplicity. A higher value indicates superior performance or quality in that particular aspect.

### Dataset Statistics and Analysis

**Overall Observations.** Each data sample in GAIA consists of four elements: the action keyword \(k\), the corresponding prompt \(t\), the generated video \(v\), and the action quality-related human annotations {\(_{a}\}_{a}\). \(\) is the collection of three perspectives. Fig. 4 illustrates two examples of generated videos with small (_shaking hands_) and large (_riding bike_) movements. In Fig. 1(c), we visualize the 3D scatter map of human-annotated _subject quality_, _action completeness_, and _action-scene interaction_ scores in the GAIA dataset and examine three extreme cases, where two dimensions are most differently or consistently (noted in _purple circles_). In general, the generated videos receive lower-than-average human ratings (\(_{s}=35.48\), \(_{c}=33.81\), \(_{i}=30.25\)) on three perspectives, suggesting the inferior performance of existing models to produce artifact-free videos with coherent actions. From Tab. 4, we notice a significantly higher correlation between \(_{c}\) and \(_{i}\) (_0.931_ Spearman's \(\), _0.791_ Kendall's \(\)) than other pairs, indicating that

   Movie & \(_{c}\) & \(_{c}\) & \(_{c}\) & \(_{c}\) & \(_{c}\) \\  Spearman’s \(\) & 0.863 & 0.886 & 0.931 & \\ Kendall’s \(\) & 0.704 & 0.703 & 0.791 & \\   

Table 4: **Effects of perspectives.** The correlation between different perspectives for all 9,180 videos in **GAIA**.

Figure 3: **MOS distributions across different models in terms of subject quality, action completeness, and action-scene interaction.** 11 Lab studies: (a)-(k); 7 Commercial applications: (l)-(r).

action completeness is a great premise of its rich interaction with the scene context, which further demonstrates the _syllogism_-based action evaluation strategy. More results are in Sec. B.2.1.

**Model-wise Comparison.** As illustrated in Fig. 3 and Fig. 6, the commercial T2V models generally perform better than models from lab studies in three evaluated dimensions. Most models exhibit left-skewed MOS distribution in all three dimensions. Among them, VideoCrafter2  and Morph Studio  are basically the best models in their respective fields (see Fig. 13 in the Appendix for detailed ranking in all dimensions). Additionally, we can observe a trend of increasing performance year by year, from the Text2Video-zero  and ModelScope  released in March 2023 to the VideoCrafter2  in early 2024. Nevertheless, most models prove decent proficiency on one single dimension, _i.e._, better subject quality than action completeness and action-scene interaction, which exposes the defects of existing models in producing temporal coherent and complete actions. Surprisingly, the newly proposed Mora  significantly underperforms other models in all three perspectives, we speculate that it is limited by the core dependency model in its demo code, stable video diffusion (SVD), an earlier image-to-video model. Furthermore, comparing the two resolution versions of VideoCrafter1 (\(512 320\) and \(1024 576\)) , as well as the commercial models and the lab studies (with an average resolution of \(596 378\) and \(1385 835\)), we can conclude that higher resolution plays an important role in improving action recognizability, resulting in advancements in

Figure 4: **Visualization of generated videos: Sort by subject quality from highest to lowest. The action keyword (relatively small (_left_) and large (_right_) movement) is highlighted in pink.**

the subject quality and action completeness. A similar conclusion applies to the performance gains as the frame rate increases.

**Class-wise Comparison.** We investigate the MOS distribution across action categories via box plots, as presented in Fig. 5. It can be observed that the \(_{}\), \(_{}\), and \(_{}\) of complex actions such as _jumping/throwing_ and _racquet-bat_ are lower than actions with small movements (_e.g._, _communication_, _touching person_, and _using tools_) (\(p<0.01\), Two-side T-test), indicating that existing T2V models struggle to render actions with drastic motion changes, where atypical body postures are more easily involved. Additionally, when it comes to the local hand action categories, the actions contain subtle movements, _e.g._, _rotate or move fingers/palm_, or _numeral representation_ receive significantly lower MOSs than others, showing the inferior capacity of generating fine-grained actions. Specifically, the frequency of outliers in Fig. 5 reflects the response variance of evaluated models under specific action word conditions, which further supports the above viewpoints. _Beyond the above observations, we further analyze the diversity of contents in GAIA (see Sec. B.2)._

## 4 Diagnosis of Automatic Evaluation Metrics

### Experimental Setup

To evaluate the performance of conventional AQA methods, we choose four approaches, _i.e._, USDL , ACTION-NET , CoRe , and TSA  for comparison. We also select six action-related metrics from recent T2V benchmarks (VBench  and EvalCrafter ) as comparisons. Additionally, we include seven representative VQA methods (TLVQM , VIDEVAL , VSFA , BVQA , SimpleVQA , FAST-VQA , and DOVER ) to reveal the potential relation between action quality and video quality. We further investigate the performance of video-text alignment metrics, since a high-quality action should be consistent with its target prompt. Seven metrics including four variants of CLIPScore  and three vision-language model (VLM)-based metrics, which replace CLIP with more advanced VLMs (BLIP , LLaVA-v1.5-7B , and InternLM-XComposer2-VL ) are evaluated. SRCC and PLCC are adopted as criteria to evaluate the performance of these models. _More implementation details can be found in Sec. C_.

### Main Results and Analysis

**Do conventional AQA methods still work?** As shown in Tab. 5, all AQA methods perform poorly with an average SRCC of \(0.4367\), \(0.4722\), and \(0.4664\) in terms of subject quality, action completeness, and action-scene interaction, respectively. Specifically, USDL takes a manually defined Gaussian

Figure 5: **Box plots of \(_{}\), \(_{}\), and \(_{}\) across action categories.** (a), (b), and (c) show whole-body actions. (d) and (f) show hand and facial actions. For each box, median is the central box, and the edges of the box represent the 25th and 75th percentiles, while red circles denote outliers.

Figure 6: Comparison of T2V models regarding the averaged MOS in three dimensions. We sorted them bottom-up by their release dates.

distribution as the learning objective to address the uncertainty during the human assessment process, which, on the contrary, exacerbates the prediction inaccuracy. Benefiting from the dynamic-static hybrid stream, ACTION-NET can capture the body postures at specific moments during an action process and thus performs marginally better than the rest models in terms of subject quality. For CoRe and TSA, the input requirement is a pairwise query and exemplary video, which is not exactly applicable to AIGVs, since the same action from different models can vary significantly from generation quality to content scenarios, failing the contrastive regression strategy . Most importantly, plagued by the generation quality of AIGV itself, it is difficult for those commonly used inflated 3D ConvNets (I3D) backbone to learn normative action features as in Kinetics . In general, existing AQA methods focus mainly on assessing actions in a similar environment, where the differences between videos are subtle, which is in accord with its goal (_most for specific tasks rather than a generic AQA_).

**Which action-related metric performs better?** As reported in the second part of Tab. 5, all action-related metrics selected from existing benchmarks achieve extremely low correlation in the GAIA dataset with the best scores of \(0.2453\), \(0.2895\), and \(0.2861\) in subject quality, action completeness, and action-scene interaction. Among them, the "Human Action" from VBench  and the "ActionScore" from EvalCrafter  adopt a similar approach that utilizes the action classification accuracy to quantify the action quality. Their incapability can be attributed to 1) the used recognition model, VideoMAE V2  and UMT , are pre-trained only on Kinetics 400 action classes  while our GAIA encompasses much broader action types; 2) based on the premise that action subject is clearly visible and temporally consistent, a condition that is challenging to fulfill in the majority of existing AIGVs. Using optical flow-based metrics, "Dynamic Degree" and "Flow-Score", to measure the movement of actions fails since the motion amplitude of different actions varies. "Motion Smoothness" is proposed to evaluate whether the motion in AIGVs follows the physical law of the real world based on the frame interpolation theory . However, it is not conducive to videos with a low frame rate and cannot justify the rationality of the generated action result such as _badminton ball flying against gravity_. As for the "Subjective Consistency" metric, there is a potential for misapplication in assessing the quality of the subject, since variability in subject posture throughout the action can easily lead to inter-frame subject inconsistencies. Consolidating the above experimental results, we can conclude that current action evaluation metrics fall short of providing reliable action assessments, necessitating a concerted effort to address these issues for the emerging AIGVs.

**Comparison to the VQA methods.** Considering the intrinsic correlation of action quality on the content quality of videos, we select seven representative VQA methods to validate whether VQA

  
**Dimensions** &  **Pre-training/** \\ **Methods/ Metrics** \\  &  **Webset** \\ **Initialization** \\  &  **Subject** \\ **Selected** \\  &  **Completeness** \\ **Selected** \\  &  **Interaction** \\  & 
 _All-Combined_ \\  \\ 
**@USDL (CVPR 20)** & & & 0.4197 & 0.4203 & 0.4365 & 0.4517 & 0.4289 & 0.4434 & 0.4223 & 0.4321 \\
**@ACTION-NET** (ACM M20)  & Kinetics & **0.4533** & **0.4612** & 0.4722 & 0.4765 & 0.4703 & 0.4829 & 0.4857 & 0.4592 \\
**@CRE (ICCV 2011)** & & & 0.4304 & 0.4343 & 0.4538 & 0.4577 & 0.4521 & 0.4514 & 0.4437 & 0.4415 \\
**@TSA (CVPR 22)** & & & 0.4435 & 0.4477 & **0.4963** & **0.4981** & **0.4941** & **0.4953** & **0.4861** & **0.4823** \\
**@Subject Consistency** & D&D\(\) & & 0.2447 & 0.2562 & 0.2116 & 0.2506 & 0.2054 & 0.2012 & 0.2298 & 0.2275 \\
**@Motion Smoothness** & AMT  & & 0.2402 & 0.1913 & 0.1474 & 0.1625 & 0.1741 & 0.1693 & 0.1957 & 0.1813 \\
**@Dynamic Degree** & RAFT  & & 0.1285 & 0.0831 & 0.0903 & 0.0682 & 0.1141 & 0.0758 & 0.1162 & 0.0787 \\
**@Human Action** & UMT  & **0.2453** & **0.2369** & **0.2898** & **0.2812** & **0.2861** & **0.2743** & **0.2831** & **0.2741** \\ Action-Score  & VideMax V2  & & 0.2032 & 0.1823 & 0.2867 & 0.2623 & 0.2869 & 0.2432 & 0.2600 & 0.2377 \\
**@Flow-Score** & RAFT  & & 0.1471 & 0.1541 & 0.0816 & 0.1273 & 0.1041 & 0.1309 & 0.1106 & 0.1430 \\
**@TD/QM (TD/TD)** & & & 0.5037 & 0.5175 & 0.427 & 0.4135 & 0.2409 & 0.4093 & 0.4095 & 0.4085 & 0.4758 \\
**@VDEWL (CIF 2012)** & NA (_shandraft_) & & 0.5237 & 0.5446 & 0.2483 & 0.4375 & 0.4121 & 0.234 & 0.4684 & 0.4801 \\
**@VSEA (ACM M19)** & & & 0.5594 & 0.5762 & 0.4940 & 0.5017 & 0.4709 & 0.4811 & 0.5098 & 0.5215 \\
**@AVQA (ICSVT22)** & _fused_  & & 0.5902 & 0.5888 & 0.4876 & 0.4946 & 0.4761 & 0.4825 & 0.5201 & 0.5289 \\
**@SimpleVQA (ACM M22)** & Kinetics  & & 0.5920 & 0.5974 & 0.4981 & 0.5078 & 0.4843 & 0.4971 & 0.5219 & 0.5322 \\
**@FAST-VQA (ECCV2)** & Kinetics  & & 0.6015 & 0.6092 & 0.5157 & 0.5215 & 0.5154 & 0.5216 & 0.5276 & 0.5475 \\
**@DOVER (ICCV2)** & LLSV  & & **0.6173** & **0.6301** & **0.5198** & **0.5238** & **0.5164** & **0.5278** & **0.5389** & **0.5802** \\
**@CLIPS-Score (ViT-B16)** & OpenAI-400M  & & 0.3560 & 0.5314 & 0.5384 & 0.5775 & 0.3535 & 0.3652 & 0.5777 & 0.5711 \\
**@CLIPScore (ViT-B49)** & OpenAI-400M  & & 0.3396 & 0.3330 & 0.3944 & 0.3871 & 0.3875 & 0.3821 & 0.3815 & 0.3826 \\
**@**\_same as the above._** & LAUQN-2B  & & 0.3179 & 0.3101 & 0.3551 & 0.3504 & 0.3380 & 0.3531 & 0.3458 \\
**@CLIPScore** & OpenAI-400M  & & 0.3211 & 0.3156 & 0.3657 & 0.3574 & 0.3585 & 0.3426 & 0.3601 & 0.3515 \\
**@RLIPScore** & COCO  & & 0.3453 & 0.3386 & 0.4174 & 0.4028 & 0.4044 & 0.3994 & 0.4118 & 0.4054 \\
**@UASAScore** & LLAVA-PT  & & 0.3484 & 0.3436 & 0.4189 & 0.4133 & 0.4077 & 0.4025 & 0.4124 & 0.4086 \\
**@**InternalMScore** & _fused_  & **0.3678** & **0.3642** & **0.4324** & **0.4257** & **0.4301** & **0.4227** & **0.4314** & **0.4246** \\   

Table 5: **Performance benchmark on GAIA.**_All-Combined_ indicates that we sum the MOS of three dimensions and rescale it to  as the overall action quality score. \(\), \(\), \(\), and \(\) denote the evaluated conventional AQA method, action-related metrics, VQA methods, and video-text alignment metrics, respectively. All experiments for AQA and VQA methods are retrained on each dimension under 10 random train-test splits at a ratio of 8:2.

approaches are applicable for AQA tasks in AI-generated scenarios, as shown in the third part of Tab. 5. We can observe that VQA methods surpass all AQA methods and action-related metrics by a large margin (on average \(25.04\%\) and \(131.1\%\) better than their respective best methods in terms of SRCC) in the subject quality dimension, while deep learning-based VQA methods perform better than traditional VQA methods (TLVQM and VIDEVAL) that rely on handcraft features. Notably, all VQA methods exhibit a relatively superior capacity to evaluate the subject quality than assessing the action completeness and action-scene interaction, indicating a potential emphasis on low-level technical distortions such as noises, sharpness, blur, and artifacts within the current VQA frameworks, which may not fully encapsulate the temporal-level normativity and interactive facets of action content. Such a conclusion is also supported by evidence from being equipped with different quality-aware initializations, as BVQA and DOVER are pre-trained with spatial distortion-dominated datasets [25; 36; 50; 33; 116]. Moreover, BVQA and SimpleVQA leverage the SlowFast model  as their motion feature extractor. This model has demonstrated effectiveness in various action recognition tasks due to its dual pathway design, which captures both spatial semantics and motion information parallelly. However, it encounters problems when applied to AIGVs, primarily because of the limited frames. Another plausible explanation for these subpar performances is the pure regression-based prediction strategy that lacks consideration of textual information, as the same MOS for different actions could lead to a large visual discrepancy.

**Evaluation on video-text alignment metrics.** We further evaluate the performance of video-text alignment metrics in measuring action quality considering their capacity in cross-modality feature mapping. Specifically, we compute the cosine similarity between the image embedding and the action prompt embedding to record a deviation degree between the sketch of the content and target action semantics. As listed in Tab. 5, the widely used CLIPScore achieves a weak correlation with human opinion, especially in the subject quality dimension, while performing relatively better with respect to action completeness and action-scene interaction dimensions. We conjecture that this is because such alignment-based metrics are intrinsically sensitive to high-level vision information (_action semantics_) rather than low-level generative flaws (_e.g., blur, noise, textures_). Meanwhile, we see a decent performance gain on evaluated dimensions (\(+8.2\%\), \(+9.6\%\), \(+10.9\%\), \(+13.1\%\) in terms of SRCC) when replacing CLIP with a more powerful VLM, such as InternLM-XComposer2-VL, showing an underlying possibility of building more accurate AQA metrics as VLMs evolve. We also conduct a T-test with a 95% confidence level to assess the statistical significance of the performance difference between any two methods (Tab. 9 and Fig. 15). More results are discussed in Sec. D.

## 5 Conclusion

Assessing action quality in AI-generated videos is a critical topic since it is an intuitive manifestation of the model generation ability and an imperative factor influencing the viewing experience of a video that requires data beyond the currently available prompt and video pairs datasets. We present GAIA, a well-curated generic AI-generated action dataset comprising 9,180 videos generated from 18 popular T2V models with 971,244 human annotations collected. We use it to evaluate the action generation ability of existing T2V models and benchmark the performance of current AQA and VQA methods. Our analysis characterizes the distinctness, variation, and capacity evolution of existing T2V models while revealing the inferiority of traditional AQA and VQA algorithms in providing subjectively consistent action quality assessments for AI-generated videos. We hope that GAIA will facilitate the development of accurate AQA algorithms for AIGVs while elucidating the factors to which humans are sensitive during action perception.

**Limitations and Societal Impact.** First, the videos in our dataset are limited in scope concerning subject types and styles, which constrains its applicability. The current synthetic actions are relatively simple as opposed to the complicated motions in real life. Second, videos in our dataset are generated with limited resolutions, frame rates, or lengths due to the imbalance between industry and academia, which could be further refined as the T2V model evolves. Third, different from prior work [78; 119; 66; 76; 115; 122], the action annotations in our dataset were collected based on a causal reasoning syllogism, which stands in stark contrast to the conventional practice of collecting a single quality score. Investigations of such a strategy on AQA would be a fruitful avenue for follow-up work. We anticipate that this work will lead to improved action quality in AI-generated videos, promoting the development of objective AQA metrics in generation domains and the understanding of human action perception mechanisms. Besides, this can make models pre-trained on this dataset less biased in assessing incomplete actions and irrational actions that easily appear in AI-generated scenarios.

Acknowledgements

This work was supported in part by the China Postdoctoral Science Foundation under Grant Number 2023TQ0212 and 2023M742298, in part by the Postdoctoral Fellowship Program of CPSF under Grant Number GZC20231618, in part by the Shanghai Pujiang Program under Grant 22PJ1407400, and in part by the National Natural Science Foundation of China under Grant 62271312, 62301316, 62101325 and 62101326.