# Happy: A Debiased Learning Framework for

Continual Generalized Category Discovery

 Shijie Ma\({}^{1,2}\), Fei Zhu\({}^{3}\), Zhun Zhong\({}^{4,5}\), Wenzhuo Liu\({}^{1,2}\), Xu-Yao Zhang\({}^{1,2}\), Cheng-Lin Liu\({}^{1,2}\)

\({}^{1}\)MAIS, Institute of Automation, Chinese Academy of Sciences, China

\({}^{2}\)School of Artificial Intelligence, University of Chinese Academy of Sciences, China

\({}^{3}\)Centre for Artificial Intelligence and Robotics, HKISI-CAS, China

\({}^{4}\)School of Computer Science and Information Engineering, Hefei University of Technology, China

\({}^{5}\)School of Computer Science, University of Nottingham, NG8 1BB Nottingham, UK

mashijie2021@ia.ac.cn   xyz@nlpr.ia.ac.cn

Corresponding author.

###### Abstract

Constantly discovering novel concepts is crucial in evolving environments. This paper explores the underexplored task of Continual Generalized Category Discovery (C-GCD), which aims to incrementally discover new classes from _unlabeled_ data while maintaining the ability to recognize previously learned classes. Although several settings are proposed to study the C-GCD task, they have limitations that do not reflect real-world scenarios. We thus study a more practical C-GCD setting, which includes more new classes to be discovered over a longer period, without storing samples of past classes. In C-GCD, the model is initially trained on labeled data of known classes, followed by multiple incremental stages where the model is fed with unlabeled data containing both old and new classes. The core challenge involves two conflicting objectives: discover new classes and prevent forgetting old ones. We delve into the conflicts and identify that models are susceptible to _prediction bias_ and _hardness bias_. To address these issues, we introduce a debiased learning framework, namely Happy, characterized by Hardness-aware prototype sampling and soft entropy regularization. For the _prediction bias_, we first introduce clustering-guided initialization to provide robust features. In addition, we propose soft entropy regularization to assign appropriate probabilities to new classes, which can significantly enhance the clustering performance of new classes. For the _harness bias_, we present the hardness-aware prototype sampling, which can effectively reduce the forgetting issue for previously seen classes, especially for difficult classes. Experimental results demonstrate our method proficiently manages the conflicts of C-GCD and achieves remarkable performance across various datasets, _e.g_., 7.5% overall gains on ImageNet-100. Our code is publicly available at https://github.com/mashijie1028/Happy-CGCD.

## 1 Introduction

In the open world [1; 2; 3], visual concepts are infinite and evolving and humans can cluster them with previous knowledge. It is also important to endow AI with such abilities. In this regard, Novel Category Discovery (NCD) [4; 5; 6] and Generalized Category Discovery (GCD) [7; 8; 1; 9; 10] endeavor to transfer [4; 11] the knowledge from labeled classes to facilitate clustering new classes. However, they are constrained to _static_ settings where models only learn _once_, which contradicts the ever-changing world. Thus, extending them to the temporal dimension is important. In the literature, Continual Novel Category Discovery (C-NCD) [12; 13; 14] and Continual Generalized CategoryDiscovery (C-GCD) [15; 16; 17; 18] aim to discover novel classes continually. C-NCD assumes all data come from new classes, while C-GCD further considers the coexistence of old and new ones. However, C-GCD still has some limitations, _e.g._, some works [15; 17] store labeled data of old classes, causing storage and privacy  issues. Others [16; 18] consider limited incremental stages and novel categories or assume a prior ratio of known samples , failing to reflect practical cases.

In this paper, we tackle the task of C-GCD, but with more realistic considerations: (1) More learning stages with more new classes. (2) At each stage, data from previous stages are inaccessible  for storage and privacy concerns. (3) Unlabeled data contain samples from old classes but are fewer than new ones in each class, and the ratio of them is unknown. The C-GCD setting is illustrated in Figure 1 with two phases: (1) Initial supervised learning (Stage-0). The model is trained on labeled classes to acquire general knowledge. (2) Continual unsupervised discovery (Stage-1 \( T\)). At each stage, the model learns from unlabeled data containing both new and old classes. Note that, old classes include initially labeled classes as well as those discovered in previous stages. The core challenge is managing the conflicts between discovering new classes and preventing forgetting old ones.

To explore the nature of the conflicts, we conducted preliminary experiments (Section 3.2) which reveal two issues: Models (1) tend to misclassify new classes as old, leading to collapsed accuracy of new classes, and (2) exhibit catastrophic forgetting of old classes. We summarize them as underlying issues to be addressed: (1) Models display overconfidence in old classes and severe _prediction bias_. (2) The features of old classes are disrupted when learning novel classes. Meanwhile, the similarity between clusters varies, leading to biased hardness across classes for classification.

To address these issues, we propose a debiased framework, namely Happy, which is characterized by **H**ardness-**a**ware **p**rototype sampling and soft entropy regularization. Specifically, on the one hand, to better discover new classes during incremental stages, we utilize clustering-guided initialization for new classes, ensuring a reliable feature distribution. More importantly, to mitigate the _prediction bias_ between new and old classes, we introduce soft entropy regularization to allocate necessary probabilities to the classification head of new classes, which is essential for new class discovery. On the other hand, to prevent catastrophic forgetting in rehearsal-free C-GCD, we model the class-wise distribution in the feature space for old classes, and sample them when learning novel classes, which significantly mitigates catastrophic forgetting. Furthermore, we devise a metric to quantify the hardness of each learned class, and prioritize sampling features from categories with greater difficulty. This helps the model to consolidate difficult knowledge accordingly and thus improves the overall performance. Consequently, these designs enable our model to specifically address the challenges in C-GCD, _i.e._, effectively discover new classes while preventing catastrophic forgetting of old classes.

In summary, our contributions are: (1) We extend Continual Generalized Category Discovery (C-GCD) to realistic scenarios. In addition, we propose a debiased learning framework called Happy, which excels in effectively discovering new classes while preventing catastrophic forgetting with reduced bias in the introduced C-GCD settings. (2) We propose cluster-guided initialization and soft entropy regularization for collectively ensuring stable clustering of new classes. On the other hand, we present hardness-aware prototype sampling to mitigate forgetting. (3) Comprehensive experiments show that our method remarkably discovers new classes with minimal forgetting of old classes, and outperforms state-of-the-art methods by a large margin across datasets.

Figure 1: The diagram of Continual Generalized Category Discovery (C-GCD). In this paper, we focus on a more pragmatic setting with (1) more continual stages and more novel categories, (2) rehearsal-free learning, and (3) no prior knowledge of the ratio of new class samples.

Related Works

Category Discovery.Novel Category Discovery (NCD) [4; 5; 21] is firstly formalized as deep transfer clustering , _i.e_., transferring the knowledge from labeled classes to help cluster new ones. Early works employ robust rank statistics [5; 22] for knowledge transfer. UNO  proposes a unified objective with Shinkhorn-Knopp algorithm . Later works [24; 25; 26] exploit relationships between samples and classes. NCD assumes unlabeled data only contain new classes. Instead, Generalized Class Discovery (GCD) [7; 27] further permits the existence of old classes. Thus models need to classify old classes and cluster new ones in the unlabeled data. Recent works handle GCD with non-parametric contrastive learning [8; 28; 10] or parametric classifiers with self-training [9; 29; 30]. More recent works explore GCD in other settings, _e.g_., active learning  and federated learning . In summary, both NCD and GCD are limited to _static_ settings where models only learn _once_.

Continual Category Discovery.Pioneer works [12; 13; 14] study the incremental version of NCD, assuming unlabeled data only contain new classes, and we call them C-NCD. Recent works [15; 16; 17; 18] explore the incremental version of GCD, we collectively refer to them as Continual Generalized Category Discovery (C-GCD). GM  proposes a framework of growing and merging. In the growing phase, the model performs novelty detection and implements clustering on the novelties. Then GM integrates the newly acquired knowledge with the previous model in the merging stage. Kim _et al_.  utilize noisy label learning and the proxy and anchor scheme to split the data in C-GCD. Zhao _et al_.  propose a non-parametric soft nearest-neighbor classifier and a density-based sample selection method. Orthogonally, Wu _et al_.  argue that the initial labeled data are not fully exploited and present a meta-learning  framework to learn a better initialization for continual discovery. Despite effectiveness, C-GCD settings studied by the above methods still have some limitations, _e.g_., the number of stages is very few with limited new classes, and the assumption of prior ratio of old classes or storing previously labeled samples is unrealistic [19; 34]. In this paper, we extend C-GCD to more pragmatic scenarios, as shown in Figure 1.

## 3 Preliminaries

We first formalize Continual Generalized Category Discovery (C-GCD) (Section 3.1). To delve deeper into the issues, we conduct preliminary experiments (Section 3.2). Results reveal that models are susceptible to two types of _bias_, which significantly degrade the performance and motivate us to propose the debiased learning framework in Section 4.

### Problem Formulation and Notations

Task Definition.As shown in Figure 1, C-GCD has two phases: (1) Initial supervised learning (Stage-0). The model is trained on labeled data \(^{0}_{}=\{(_{i}^{l},y_{i})\}_{i=1}^{N^{0}}\) of initially labeled classes \(^{0}_{}=^{0}_{}\), to learn general knowledge and representations. We denote \(^{0}_{}=\). (2) Continual unsupervised discovery (Stage-1 \(\)\(T\)). At Stage-\(t\) (\(1 t T\)), the model is fed with an unlabeled dataset \(^{t}_{}=\{(_{i}^{u})\}_{i=1}^{N^{t}}\), which contains both old and new classes. We denote the categories in \(^{t}_{}\) as \(^{t}=^{t}_{}^{t}_{}\). \(K^{t}_{}=|^{t}_{}|\), \(K^{t}_{}=|^{t}_{}|\) and \(K^{t}=K^{t}_{}+K^{t}_{}\) denote the number of "old", "new" and "all" classes respectively. Note that, after the first stage, _i.e_., when \(t 2\), "old" classes include initially labeled classes \(^{t}_{}\) and all new classes discovered in previous stages, _i.e_., \(^{t}_{}=^{0}_{}\{(^{ i}_{})\}_{i=1}^{t-1}\), and "new" classes refer to the classes unseen before. At the next stage, new classes from the current stage become the subset of old classes, _i.e_., \(^{t}_{}=^{t-1}_{}^{t- 1}_{}\). The number of novel classes \(K^{t}_{}\) at stage \(t\) is known _a-prior_ or estimated using off-the-shelf methods [5; 7; 35] in advance. After training of each stage, the model will be evaluated on the disjoint test set \(^{t}_{}=\{(_{i},y_{i})\}_{i=1}^{N^{t}_{}}\) containing all seen classes \(^{t}_{}^{t}_{}\).

Realistic Considerations.Our C-GCD is more realistic than prior arts [15; 16; 18] in that: (1) More stages with more new classes to be discovered. (2) Rehearsal-free. Previous samples are inaccessible for storage and privacy issues. (3) At each continual stage, old classes have fewer samples per class than new classes in the unlabeled data, and the proportion of old samples is unknown.

Notations.At Stage-\(t\), we decompose the model into encoder \(^{t}_{}()\) and parametric classifier \(^{t}_{}=[\{^{}_{i}\}_{i=1}^{K^{t}_{}};\{^ {}_{j}\}_{j=1}^{K^{t}_{}}]\) with head of old and new classes. The classifier is \(_{2}\)-normalized without bias term, _i.e_., \(\|^{t}_{i}\|=1\). The encoder maps the input \(_{i}\) to a feature vector \(_{i}=^{t}_{}(_{i})^{d}\). Here,we use \(_{2}\)-normalized hyperspectral feature space, _i.e._, \(_{i}=_{i}/\|_{i}\|\). The classifier finally produces a probability distribution \(_{i}=(_{}^{t}(_{i})/_{p})^{K^{t}}\) using softmax function \(()\).

### Preliminary Experiments: Two Bias Problems

We conduct preliminary experiments on CIFAR100  using the model described in Section 3.1, which is initially trained on \(^{0}_{}\) and continually discovers new classes on \(^{t}_{}\) using unsupervised self-training scheme . Results reveal that models are prone to the following two types of _bias_.

_Prediction bias_ **in probability space.** As illustrated in Figure 2 (a), the model's accuracy for new classes has collapsed. The reason is that old classes \(^{0}_{}\) are trained under full supervision while new classes are under unsupervised self-training [9; 37], which brings about overconfidence [38; 39; 40] in old classes, as in (b). In this case, _prediction bias_ could occur, where some new classes are incorrectly predicted as old ones, which motivates us to constrain the model to give necessary attention and predictive probabilities to new classes to compensate for this intrinsic gap, as discussed in Section 4.2.

_Hardness bias_ **in feature space.** After adding constraints to ensure learning new classes (Section 4.2), their accuracies significantly fluctuate across incremental stages, leading to unstable performance, as shown in Figure 2 (c). The underlying cause is that some clusters are more similar to others in the feature space, resulting in lower accuracy of these difficult classes. As in (d), _hardness bias_ (defined in Section 4.3) is obvious across classes. This paper focuses on the hardness of previously learned categories \(^{t}_{}\), and addresses how to avoid these biases in preventing forgetting in Section 4.3.

## 4 The Proposed Framework: **H**appy

Overview of the Method.As shown in Figure 1, C-GCD has two phases: (1) Initial supervised learning (Stage-0). The model is trained on labeled samples of \(^{0}_{}\) (Section 4.1). Our contribution mainly lies in (2) Continual unsupervised discovery (Stage-1\(\) T). Motivated by the conflicts between new class discovery and the forgetting of old classes, as well as the two types of _bias_ discussed in Section 3.2, we propose the debiased learning framework **H**appy as illustrated in Figure 3. Specifically, for category discovery, we propose initialization of new heads and soft entropy regularization to resist _prediction bias_ (Section 4.2). To mitigate forgetting, we consider _hardness bias_ and present hardness-aware prototype sampling (Section 4.3). The overall objective is derived in Section 4.4.

### Supervised Training at the Initial Stage

At Stage-0, the model is trained on labeled data \(^{0}_{}\) from a large number of classes \(^{0}_{}\) to learn general representations, which serves as the foundation for subsequent continual category discovery. We use standard supervised cross-entropy loss on the batch \(B\): \(_{}=_{i B}-y_{i}_{i}\), where \(_{i}=(_{}^{0}(_{}^{0}(_{i}))/)\) denotes the prediction. To reduce overfitting, we further employ supervised  and self-supervised contrastive learning  in the \(_{2}\)-normalized projection space:

\[^{t}_{}=-_{i B} (i)|}_{q(i)}_{i}^{}_{q}^{ }/_{c})}{_{n i}(_{i}^{}_{n}^{}/ _{c})},\ ^{u}_{}=-_{i B}_{ i}^{}_{i}^{}/_{c})}{_{n i}(_{i}^{}_{n} ^{}/_{c})},\] (1)

w

Figure 2: Preliminary results. We identify two issues and underlying causes, including **(a) Issue 1:** performance gap between old and new classes, caused by **(b) Reason 1:** model’s overconfidence in old classes, _i.e._, _prediction bias_. **(c) Issue 2:** accuracy fluctuations in new class across various stages, caused by **(d) Reason 2:** different categories have varying levels of difficulty, _i.e._, _hardness bias_.

where \((i)\) is the positive set with the same label and \(_{c}\) is temperature. The overall loss function is:

\[_{}=_{}+_{0}_{ }^{l}+(1-_{0})_{}^{u}.\] (2)

### Classifier Initialization and Soft Entropy Regularization

Continuously discovering unlabeled new classes is challenging, as _prediction bias_ towards old classes could collapse new class accuracy (Section 3.2). Therefore, we need to constrain the model to pay more attention to new classes to ensure effective category discovery.

Clustering-guided Initialization.Randomly initialized classifiers bring about unstable training. We argue that clustering could provide a good initialization for new classes. Specifically, at Stage-\(t\), we employ KMeans  on \(_{}^{t}\) and obtain \(K^{t}=K_{}^{t}+K_{}^{t}\)\(_{2}\)-normalized cluster centroids \(\{_{i}\}_{i=1}^{K^{t}}\). Among them, the \(K_{}^{t}\) centroids least similar to old heads, as measured by maximum cosine similarity with them, serve as the potential initialization for new class heads:

\[\{t_{j}\}_{j=1}^{K_{}^{t}}=_{t_{j}}(-_{i}_{t_{j }}^{}_{i}^{}),\ \ i=1,,K_{}^{t}._{j}^{}= _{t_{j}},\ \ j=1,,K_{}^{t}.\] (3)

Group-wise Soft Entropy Regularization.Entropy regularization [9; 30; 29] is common to avoid trivial solutions of clustering in _static_ settings. However, at each stage of C-GCD, there are generally more old classes. Directly employing it equally across all classes will allocate most of the probability to old classes, leading to _prediction bias_ and collapsed performance (as in Figure 2 (a, b)). To address this, we need to constrain the model explicitly. Considering that at each stage, there are fewer new classes but more samples per new class, and old classes have been well-learned previously, we propose to treat all old classes as a whole and the new classes as another, and derive C-GCD as binary classification. Specifically, we first compute the marginal probability in the batch \(}^{K^{t}}=_{i B}_{i}\). Thus, \(_{}=_{c_{}^{t} }}^{(c)}\) and \(_{}=_{c_{}^{t} }}^{(c)}\) are scalars indicating the marginal distribution on old and new classes respectively, where the superscript \((c)\) denotes class indices and \(_{}+_{}=1\). Then we propose soft entropy regularization on the marginal distribution of the old and the new:

\[_{}^{}=_{} _{}+_{}_{}.\] (4)

In this way, the model could focus more on each new class, ensuring reliable learning in new classes. We also employ entropy regularization within the new and old classes to avoid trivial solutions:

\[_{}^{}=_{c_{ }^{t}}}^{(c)}}^{(c)},_{ }^{}=_{c_{}^{t}} }^{(c)}}^{(c)}.\] (5)

Figure 3: Illustration of the proposed happy framework. **Top:** Overall learning pipeline for continual stages. **Bottom Left:** Clustering-guided Initialization, together with Soft Entropy Regularization (Section 4.2) ensures effective novel class category. **Bottom Right:** Hardness-aware Prototype Sampling (Section 4.3) remarkably mitigates catastrophic forgetting of old classes.

To sum up, the soft entropy regularization is employed in a group-wise manner on three groups, _i.e._, "inter old-new" (Eq. (4)), "intra old" and "intra new" (Eq. (5)), and we add them together:

\[_{}=_{}^{ }+_{}^{}+_{}^{ }.\] (6)

The soft regularization ensures effective learning of new classes. See Section 5.4 for more discussions.

Overall Loss for New Class Discovery.To achieve self-training on unlabeled data, we perform self-distillation [9; 37]. Specifically, we use another augmented view \(_{i}^{}\) to produce sharpened \(_{i}^{}\) with smaller temperature \(_{t}<_{p}\) and employ cross-entropy loss to supervise the prediction \(_{i}\): \(_{}=_{i B}(_{i}^{ },_{i})+(_{i},_{i}^{})\). The overall objective for new category discovery is:

\[_{}=_{}+_{1}_{},\] (7)

where \(_{1}\) controls the importance of the proposed regularization loss.

### Hardness-aware Prototype Sampling

Modeling Learned Classes.Catastrophic forgetting [44; 45; 46] is a notorious problem in continual learning, especially when previous samples are inaccessible. Instead of storing seen samples, we can model the feature distribution for learned classes. Since the data in each incremental stage are unlabeled, at the end of each incremental stage, we perform class-wise Gaussian distribution in the feature space using models' predictions on \(_{}^{t}\):

\[_{c}=}_{_{i}=c}_{}^{t}(_{i}), _{c}=}_{_{i}=c}(_{}^{t}( _{i})-_{c})(_{}^{t}(_{i})-_{c})^{}, c=1,,K^{t},\] (8)

where \(_{i}=*{arg\,max}_{c}_{i}^{(c)}\) denotes the prediction, \(_{c}\) and \(_{c}\) are mean and covariance. Note that, for Stage-0, we directly use the ground-truth labels instead of predictions in Eq. (8). We call \(_{c}\) as prototypes. When learning new knowledge, one can sample features from old classes \((_{c},_{c})\), and classify them correctly to mitigate forgetting. We find a shared diagonal matrix  empirically works fine, _i.e._, \(_{c}=r\), where \(r\) is computed at Stage-0 as \(r^{2}=}_{c_{}^{0}}*{ Tr}(_{c})/d\).

Incorporating Hardness to Learned Classes.As in Figure 2 (c), accuracy fluctuations across classes are significant, and treating all classes equally leads to _hardness bias_ and suboptimal results. Intuitively, difficult classes should receive more attention during sampling. Here, we propose an unsupervised metric, considering the samples with higher similarity to others are more prone to be confused and therefore more difficult. We define hardness \(h_{i}\) and obtain hardness distribution as:

\[h_{i}=}^{t}-1}_{j=1,j i}^{K_{}^{t}} (_{i},_{j})_{}^{(i)}= (h_{i}/_{h})=/_{h})}{_{j=1}^{K_{}^{ t}}(h_{j}/_{h})},\] (9)

where \(i=1,,K_{}^{t}\) and \(_{}\) is the categorical distribution to sample classes. Those with higher hardness are more likely to be sampled, which better suppresses the forgetting of hard classes.

Sequential Sampling.We first sample categories from categorical distribution \(c_{}\) and then sample class-wise features from Gaussian distribution of the sampled classes \(_{c}(_{c},r)\) for classification. The loss for hardness-aware prototype sampling is :

\[_{}=_{c p_{}}_{ _{c}(_{c},r)}-y_{c}(_{} ^{t}(_{c})/_{p}).\] (10)

Overall Loss for Mitigating Forgetting.As training proceeds, the feature space becomes outdated for previous prototypes, we thus apply knowledge distillation  using the last stage model and current training dataset, _i.e._, \(_{}=_{i B}1-(_{}^{t}( _{i}),_{}^{t-1}(_{i}))\). The overall loss is:

\[_{}=_{}+_{2}_{ {kd}},\] (11)

where \(_{2}\) controls the weight of the knowledge distillation.

### Overall Learning Objective

To continually discover new classes without forgetting old ones, we combine the losses for new (Eq. (7)) and old classes (Eq. (11)), and contrastive learning (Eq. (1)) to formulate the final objective:

\[_{}=_{}+_{}+ _{3}_{}^{u}.\] (12)

## 5 Experiments

### Experimental Setup

**Datasets.** We construct C-GCD on four datasets: CIFAR100  (C100), ImageNet-100  (IN100), Tiny-ImageNet  (Tiny) and CUB , each is split into two subsets: (1) Stage-0, where 50% of classes serving as \(_{}^{0}\) constitute initial **labeled** data. (2) Stage-1 \( T\) (\(T=5\) by default). At each stage, the remaining classes are evenly sampled as new classes, along with all previously learned classes to constitute continual **unlabeled** data. Detailed dataset statistics are shown in Table 2.

**Evaluation Protocol.** At each stage, after training on \(_{}^{t}\), the model is evaluated on disjoint test \(_{}^{t}\), _i.e._, _inductive_ setting, which contains both new \(_{}^{t}\) and old classes \(_{}^{t}\). The accuracy is calculated using ground truth \(y_{i}\) and models' predictions \(_{i}\) as: \(ACC=_{p(_{t})}_{i=1}^{M} (y_{i}=p(_{i}))\), where \(M=|_{}^{t}|\) and \((^{t})\) is the set of all permutations across all classes \(_{}^{t}_{}^{t}\). The optimal permutation could be computed _once_ using Hungarian algorithm  on all classes, and we report "All", "Old" and "New" accuracies as evaluate metrics.

**Implementation Details.** Following the convention [7; 10; 18; 31], we use ViT-B/16  pretrained by DINO  as the backbone, and fine-tune only the last transformer block for all experiments. The output [CLS] token is chosen as feature representation. At Stage-0, models are trained

    &  &  &  &  &  &  &  \\   & & All & All & Old & New & All & Old & New & All & Old & New & All & Old & New \\   & KMeans  & 66.16 & 40.27 & 41.76 & 32.80 & 37.14 & 38.33 & 30.00 & 36.20 & 37.63 & 26.20 & 36.66 & 38.30 & 23.50 & 35.69 & 36.79 & 25.80 \\  & VanillaGCD  & 90.82 & 72.32 & 78.50 & 41.40 & 67.04 & 72.50 & 34.30 & 57.99 & 62.26 & 28.10 & 56.60 & 59.55 & 33.00 & 51.36 & 35.70 & 30.30 \\  & SimGCD  & 90.36 & 73.37 & 68.44 & 8.00 & 62.56 & 72.43 & 3.30 & 54.17 & 61.61 & 2.10 & 47.62 & 53.37 & 1.60 & 43.53 & 47.86 & 4.60 \\  & SimGCD+  & 90.36 & 75.93 & **87.04** & 20.40 & 67.07 & 75.33 & 17.50 & 58.45 & 64.33 & 17.30 & 54.31 & 58.71 & 19.10 & 50.49 & 53.90 & 19.80 \\  & FRoST  & 90.36 & 78.67 & **79.58** & **63.30** & 65.31 & 68.88 & 43.90 & 58.01 & 61.09 & 36.50 & 49.27 & 50.90 & 36.20 & 48.03 & 48.17 & 46.80 \\  & GM  & 90.36 & 76.58 & 79.80 & 60.50 & 71.10 & 74.52 & **60.60** & 53.61 & 68.16 & 31.00 & 59.47 & 62.51 & 37.60 & 54.11 & 54.74 & 48.40 \\  & MetaGCD  & 90.82 & 76.12 & 83.60 & 38.70 & 69.40 & 72.82 & 48.90 & 61.95 & 65.76 & 35.30 & 58.22 & 61.21 & 34.30 & 55.78 & 58.47 & 31.60 \\  & Ihappy (Ours) & 90.36 & **80.40** & 85.26 & 56.10 & **74.13** & **78.27** & 49.30 & **68.23** & **70.86** & **49.80** & **62.26** & **63.75** & **50.30** & **59.99** & **60.96** & **51.30** \\   & KMeans  & 85.56 & 54.90 & 57.04 & 44.20 & 54.73 & 56.37 & 44.90 & 54.67 & 56.66 & 40.80 & 54.63 & 56.25 & 41.70 & 53.92 & 56.18 & 33.60 \\  & VanillaGCD  & 95.96 & 70.13 & 72.92 & 59.62 & 60.93 & 73.77 & 44.80 & 68.50 & 70.63 & 53.60 & 65.56 & 67.85 & 47.20 & 54.64 & 57.44 & 38.40 \\  & SimGCD  & 96.20 & 79.67 & 61.88 & 19.06 & 70.23 & 78.83 & 18.60 & 61.90 & 67.43 & 23.20 & 56.67 & 60.92 & 22.60 & 52.90 & 56.40 & 21.40 \\  & SimGCD+  & 96.20 & 83.07 & 95.16 & 22.60 & 74.57 & 83.47 & 42.12 & 67.60 & 73.57 & 25.80 & 62.09 & 66.83 & 24.20 & 57.62 & 61.47 & 23.00 \\  & FRoST  & 96.20 & 87.50 & 92.96 & 60.20 & 79.63 & 83.37 & 57.20 & 76.78 & 77.00 & 75.20 & 66.18 & 68.65 & 46.60 & 63.82 & 66.04 & 40.60 \\  & GM  & 96.20 & 89.53 & 95.04 & 62.00 & 82.34 & 86.93 & 54.08 & 7.97 & 79.17 & 69.60 & 72.80 & 74.65 & 58.00 & 71.08 & 71.76 & 65.00 \\  & MetaGCD  & 95.96 & 75.27 & 78.20 & 60.20 & 73.79 & 75.93 & 54.90 & 69.35 & 72.20 & 49.04 & 67.22 & 70.10 & 44.20 & 66.68 & 69.31 & 43.00 \\  & Ihappy (Ours) & 96.20 & **91.20** & **95.36** & **70.40** & **87.83** & **90.83** & **69.80** & **85.22** & **86.40** & **77.00** & **81.93** & **83.00** & **73.40** & **78.58** & **79.11** & **73.80** \\   & KMeans  & 61.70 & 35.42 & 35.46 & 35.20 & 34.99 & 35.75 & 30.40 & 34.80 & 36.07 & 25.90 & 34.77 & 35.90 & 24.90 & 34.62 & 35.63 & 25.50 \\  & VanillaGCD  & 84.20 & 55.93 & 85.92 & 41.00 & 54.96 & 58.58 & 33.20 & 52.82 & 55.74 & 32.40 & 42.81 & 51.46 & 27.60 & 45.94 & 48.06 & 26.90 \\  & SimGCD  & 85.86 & 66.95 &with 100 epochs. Subsequently, we train models 30 epochs at each continual stage with a batch size of 128 and a learning rate of 0.01. We set \(\{_{1},_{2},_{3}\}\) as 1 and temperature \(\{_{p},_{h}\}\) as 0.1 while \(_{t}\) as 0.05. All experiments are run on NVIDIA GeForce RTX 4090 GPUs.

### Comparison with State-of-the-Arts

We compare our methods with (1) Kmeans  on pre-trained features, (2) GCD methods: VanillaGCD , SimGCD , SimGCD+LwF , and (3) recent continual category discovery works: FRoST , GM  and MetaGCD . Since GM  requires storing exemplar samples, we adjust it to sampling features. For a fair comparison, all methods use the same objective (Eq. (2)) to pre-train the model at Stage-0. Results are reported in Table 1, Table 3, and Table 4.

Happy **outperforms prior methods by a large margin.** For example in Table 1, on IN100, compared to MetaGCD  and GM , our approach achieves an improvement of 11.90% and 7.50% for 'All' accuracy, respectively. On C100, Happy improves the previous state-of-the-art by 3.45% and 13.60% for old and new classes across 5 stages. Besides, our method produces more balanced accuracy between 'Old' and 'New'. These improvements benefit from our consideration of underlying bias in the task of C-GCD and the tailor-made debiased components in Happy.

Happy **effectively balances discovering new classes with mitigating forgetting old classes.** To decouple and analyze the two conflicting objectives, we use \(_{f}\) and \(_{d}\) in  to evaluate the overall forgetting of labeled classes and the discovery of new classes respectively. Table 3 shows that VanillaGCD  and MetaGCD  struggle with category discovery due to the weak supervision of contrastive learning. In addition, FRoST  focuses solely on new classes, at the expense of old class performance. In contrast, our method effectively balances both, achieving improvements of 6\(\)12% in two metrics.

**C-GCD with more continual stages.** To explore more realistic and challenging scenarios, we conduct C-GCD with 10 continual stages. Results in Table 4 demonstrate that Happy consistently outperforms other counterparts, showcasing Happy is a competent long-term novel category discoverer.

### Ablation Study

Here, we conduct extensive ablations on each main component (Table 5) and analyze how our method handles the conflicting goals between discovering new classes and mitigating forgetting old ones. Finally, we delve into the mechanism of hardness in our framework.

**How does** Happy **achieve remarkable category discovery?** In Table 5 (a), we observe that models trained with only \(_{}\) are collapsed in 'New' ACC. (b) and (c) incorporate soft entropy regularization and the designed initialization, respectively. In addition, (d) combines both of them and brings significant improvements for new classes, _e.g._, 28.21% on CUB. From (a) to (d), the initialization

    &  & Tiny & Data &  &  & \)} & \)} & \)} & \)} \\   & \(_{f}\) & \(_{d}\) & \(_{f}\) & \(_{d}\) & \(_{f}\) & \(_{f}\) & C100 & 9428 & 7824 & 7824 & 7687 & 70.55 & 8633 & 5714 & 5623 & 55.15 \\  VanillaGCD & 17.10 & 33.42 & 20.20 & 32.22 & & & & & & & & & \\ FRoST & 22.82 & 45.34 & 21.62 & 34.96 & & & & & & & & & \\ MetaGCD & 16.56 & 37.76 & 19.30 & 33.68 & & & & & & & & & \\  Happy & **11.22** & **51.36** & **9.75** & **43.38** & & & & & & & & & \\   

Table 4: ‘All’ ACC of C-GCD across 10 continual stages.

    &  &  &  &  \\   & \(_{}\) & init & \(_{}\) & \(_{}\) & All & Old & New & All & Old & New \\  (a) & ✗ & ✗ & ✗ & ✗ & 50.95 & 58.66 & 1.96 & 53.28 & 60.75 & 4.70 \\ (b) & ✓ & ✗ & ✗ & ✗ & 57.67 & 65.58 & 7.44 & 59.26 & 65.99 & 14.91 \\ (c) & ✗ & ✓ & ✗ & ✗ & 58.26 & 65.33 & 12.84 & 63.11 & 68.62 & 27.33 \\ (d) & ✓ & ✓ & ✗ & ✗ & 60.51 & 67.39 & 16.36 & 64.53 & 69.34 & 32.91 \\ (e) & ✗ & ✗ & ✓ & ✓ & 57.75 & 66.32 & 1.96 & 57.67 & 66.05 & 3.69 \\ (f) & ✓ & ✓ & ✓ & ✗ & 66.89 & 69.98 & 47.94 & 66.36 & 70.94 & 37.15 \\  (g) & ✓ & ✓ & ✓ & ✓ & **69.00** & **71.82** & **51.36** & **68.88** & **71.29** & **53.13** \\   

Table 5: Ablations on the main components. Average accuracies of 5 stages are reported.

produces robust and desirable feature location, and \(_{}\) mitigates _prediction bias_ and ensures necessary learning of new classes. Additionally, mitigating the forgetting of old classes also helps ((d)\(\)(g)), as it ensures the preservation of general representations for most classes, which in turn benefits the clustering of new classes.

**How does \(\) mitigate catastrophic forgetting?** (f) includes hardness-aware sampling based on (d), which improves 'Old' ACC by 2.59% on CIFAR100. However, without \(_{}\), the feature space could drift significantly when learning new classes and become misaligned with the learned classifier, which degrades the performance. As a whole, (g) incorporates \(_{}\) to remarkably improve 'Old' by 4.43% and 1.95% on CIFAR100 and CUB. Similarly, better clustering of new classes also benefits old ones because incorrectly classifying new as old can hinder the learning of old classes. In this sense, the learning of new and old classes is mutually reinforcing.

**How does hardness-awareness help C-GCD?** To delve into the effectiveness of hardness-aware modeling, we conduct ablations with and without it. Results (average 'All' accuracy across 5 stages) in Figure 4 show that hardness-awareness consistently improves performance across various datasets. We also present sensitivity analysis on temperature \(_{h}\) in Eq. (9). As Table 6 shows, \(_{h}=0.1\) is a proper choice. When \(_{h}\) is too large, \(_{}\) convergences to the uniform distribution, which is similar to the one without hardness modeling. A small \(_{h}\) also brings suboptimal results. In such cases, \(_{}\) becomes overly sharp, resulting in the sampling of only a very limited number of hard classes, which exacerbates the forgetting of remaining categories.

### Further Analysis

**Does incorporating class prior into regularization necessarily improve results?** Without any prior knowledge about the proportion of new and old class samples, we employ soft entropy regularization in Eq. (4) to prevent bias. A natural question arises: _Can the introduction of information about the ratio of new to old class samples at each stage further enhance performance?_ To explore this issue, we directly use the ground truth ratio of old and new samples \(^{}_{},^{}_{}\) as a prior and modify Eq. (4) as \(^{}_{}=-^{}_{}_{}-^{}_{} _{}\). That is, using cross-entropy to supervise the model's predictive probabilities \(_{},_{}\), which surprisingly degrades performance as shown in Figure 5. The reason lies in the gap between the model's predicted ratio of new class samples (pred new ratio) and the prior ratio of new classes (\(^{}_{}\) (gt ratio), as revealed in Figure 6, which is caused by the confidence gap between old and new classes (Figure 2). This gap ultimately causes the predicted ratio of new samples to exceed \(^{}_{}\), bringing about degraded performance than using Eq. (4) without any prior.

**Unknown class number scenarios.** Previous experiments assume the number of new classes \(K^{t}_{}\) is known, which often does not hold in reality. At the start of each stage, we need to first estimate the number of new classes before instantiating the classifier. Prior arts [5; 7] query some labeled data when estimating the class number, which is not applicable in the purely unsupervised setting of C-GCD. Instead, we employ off-the-shelf _silhouette score_ to estimate \(K^{t}_{}\) in an unsupervised manner. Specifically, we compute _silhouette score_ using mean intra-cluster distance and mean nearest-cluster distance

   \(_{h}\) & C100 & Tiny \\ 
0.01 & 66.40 & 62.36 \\
0.05 & 68.06 & 64.95 \\
0.1 & **69.00** & **65.56** \\
1 & 68.01 & 64.76 \\
10 & 67.59 & 63.93 \\   

Table 6: Sensitivity analysis of \(_{h}\).

and select the number of classes corresponding to the highest score value as the estimation. Then we utilize the estimated number for training and evaluation. Average accuracies across 5 stages on CIFAR100 are reported in Table 7. Our method outperforms others when \(K^{t}_{}\) is not known _a-prior_.

Happy **could effectively mitigate two types of bias in C-GCD.** As elaborated in Section 3.2, models in C-GCD are susceptible to _prediction bias_ and _hardness bias_. To validate the effectiveness of the proposed method in bias mitigation, we design metrics to quantitatively measure these biases. Specifically, for _prediction bias_, we provide two metrics: (1) \( p()=_{}-_{}\) denotes the difference in marginal probabilities between old and new classes (see Section 4.2). (2) \( r()\) denotes the proportion of new classes' samples misclassified as old classes. Both \( p\) and \( r\) are calculated on the test data after Stage-1. The results in Table (a)a from two datasets demonstrate that \(_{}\) effectively reduces prediction bias, with a significantly lower marginal probability gap and fewer new class samples misclassified as the old. For _hardness bias_, we also present two metrics: (1) \(Var_{0}()\) denotes the variance in accuracy of the initial labeled classes \(^{0}_{}\). (2) \(Acc_{h}()\) denotes the accuracy of the hardest class in \(^{0}_{}\). Both metrics are calculated after 5 stages. Results in Table (b)b demonstrate that hardness-aware sampling effectively reduces _hardness bias_, with lower accuracy variance and higher hardest accuracy. In this regard, the proposed modules competently alleviate both types of bias, which is consistent with our motivation.

## 6 Conclusive Remarks

We tackle the pragmatic but underexplored task of Continual Generalized Category Discovery (C-GCD), which involves conflicting goals of continually discovering unlabeled new classes while preventing forgetting old ones. We further identify _prediction bias_ and _hardness bias_ hinder the effective learning of both old and new classes. To overcome these issues, we propose a debiased framework namely Happy. The clustering-guided initialization and soft entropy regularization collectively alleviate _prediction bias_ and ensure the clustering of new classes. On the other hand, by modeling the hardness of learned classes, we propose hardness-aware prototype sampling to dynamically place more attention on difficult classes, which significantly prevents the forgetting of old classes. Overall, our method achieves better discovery of new classes with minimal forgetting of old classes, which is validated by extensive experiments across various scenarios.

Limitations and Future Works.Due to the imbalanced labeling conditions between the initial and continual stages in C-GCD, the model's confidence is not calibrated and there is an obvious confidence gap between old and new classes, in these cases, incorporating prior information even degrades performance (Section 5.4). Future work should incorporate confidence calibration  into C-GCD to further mitigate potential biases. Another promising direction is to devise competent class number estimation methods for C-GCD, because in the unsupervised setting, class number estimation becomes significantly challenging. Additionally, this paper primarily discusses classification tasks, while future works could extend the C-GCD learning paradigm to object detection , segmentation  and multi-modal learning [56; 57; 58].