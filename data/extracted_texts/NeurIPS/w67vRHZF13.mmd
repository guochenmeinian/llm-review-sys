# Unified Generative and Discriminative Training for Multi-modal Large Language Models

Wei Chow\({}^{1}\) Juncheng Li\({}^{1,}\) Qifan Yu\({}^{1}\) Kaihang Pan\({}^{1}\) Hao Fei\({}^{2}\) Zhiqi Ge\({}^{1}\) Shuai Yang\({}^{1}\) Siliang Tang\({}^{1,}\) Hanwang Zhang\({}^{3}\) Qianru Sun\({}^{4}\)

\({}^{1}\)Zhejiang University \({}^{2}\)National University of Singapore

\({}^{3}\)Nanyang Technological University \({}^{4}\)Singapore Management University

{xieqiao, junchengli, yuqifan, kaihangpan}@2ju.edu.cn

{zhiqige, syang, siliang}@2ju.edu.cn

haofei37@nus.edu.sg, hanwangzhang@ntu.edu.sg, qianrusun@smu.edu.sg

 Corresponding Author.

###### Abstract

In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms. Generative training has enabled Multimodal Large Language Models (MLLMs) to tackle various complex tasks, yet issues such as hallucinations and weak object discrimination persist. Discriminative training, exemplified by models like CLIP, excels in zero-shot image-text classification and retrieval, yet struggles with complex scenarios requiring fine-grained semantic differentiation. This paper addresses these challenges by proposing a unified approach that integrates the strengths of both paradigms. Considering interleaved image-text sequences as the general format of input samples, we introduce a structure-induced training strategy that imposes semantic relationships between input samples and the MLLM's hidden state. This approach enhances the MLLM's ability to capture global semantics and distinguish fine-grained semantics. By leveraging dynamic sequence alignment within the Dynamic Time Warping framework and integrating a novel kernel for fine-grained semantic differentiation, our method effectively balances generative and discriminative tasks. Extensive experiments demonstrate the effectiveness of our approach, achieving state-of-the-art results in multiple generative tasks, especially those requiring cognitive and discrimination abilities. Additionally, our method surpasses discriminative benchmarks in interleaved and fine-grained retrieval tasks. By employing a retrieval-augmented generation strategy, our approach further enhances performance in some generative tasks within one model, offering a promising direction for future research in vision-language modeling. The project repository is here.

## 1 Introduction

In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms: generative training and discriminative training. **Generative Training** has achieved remarkable success in enabling Multimodal Large Language Models (MLLMs) [1; 55; 86] to develop a wide range of powerful capabilities that can handle various complex tasks (_e.g.,_ open-world visual question-answering, image caption generation, etc.) within a single model. However, challenges such as hallucinations and weak image object discrimination abilities [7; 89] persist. **Discriminative Training**, exemplified by CLIP , exhibits remarkable representation capabilities for zero-shot image-text classification and retrieval. Nonetheless, it encounters difficulties in processing complex scenarios (_i.e.,_ retrieving multi-modal documents with interleaved images and texts) [53; 54] and exhibits a limited ability to discern detailed semantic differences [79; 85].

The disparity between these two paradigms has sparked recent studies aimed at imparting discriminative ability to generative pre-trained MLLMs. However, certain aspects of performance still pose limitations (_e.g.,_ singular discriminative tasks , weak discriminative task performance , weak generalization , etc.), while others entail compromising the model's original generative capabilities .

Overall, the reason generative paradigms struggle with performing discriminative tasks like retrieval is due to overlooking two crucial abilities:

_(i)_ **Comprehensively capturing the global semantics**. Recent studies have revealed that causal LLMs tend to exhibit a bias towards capturing global information from the input samples, often resulting in a tendency to overlook information located in the middle, especially for long sequences [15; 57]. As illustrated in Figure 1(a), we chose 500 samples from WebQA , where the task is to find and reason about the right image-text pair among five distractors to produce a yes or no answer. We conducted experiments using VILA , a MLLM with state-of-the-art interleaved image-text comprehension ability, alongside our model. When placing the relevant pair in different positions, the performance of MLLMs followed a 'U' shape, indicating a bias in capturing global semantic information. Consequently, MLLMs encounter difficulties in forming comprehensive representations that encompass global semantics for retrieval tasks.

_(ii)_ **Keenly differentiating the detailed semantics**. Some research [47; 82] has found that the existing generative training framework cannot fully distinguish input semantics in certain contexts, causing MLLMs to struggle with tasks requiring fine-grained semantics [46; 98]. As depicted in Figure 1(b), we noticed that MLLMs face challenges in choosing the right description for two similar images in the MMVP-VLM benchmark . This indicates that MLLMs struggle to effectively differentiate the detailed semantics of input samples, naturally leading to difficulties in forming effective queries for retrieval.

In this paper, we argue that the current separated paradigms possess the potential for achieving synergistic gains. We propose **Sugar**: **S**tructure-induced approach to **u**nify **g**enerative **a**nd discriminative paradigms (shown in Figure 2), leveraging discriminative training to acquire the two abilities above while harnessing the potential of generative training in complex discriminative tasks like image-text interleaved retrieval and fine-grained retrieval. Specifically, we explicitly impose the semantic relationships between different input samples as an induced structural constraint on the hidden state of MLLMs. We consider the interleaved image-text sequence as the general format of input samples, and then formulate the relationship between any two samples as a dynamic sequence alignment problem within the Dynamic Time Warping framework [67; 33]. In this way, we can explicitly modulate the hidden states of the MLLM by leveraging the semantic relationships between interleaved input sequences, thereby encouraging the MLLM to fully **capture the global semantics** of the inputs.

To further enhance the ability to **differentiate fine-grained semantics**, we integrate a novel kernel into the Dynamic Time Warping framework. Leveraging the strengths of various discriminative pre-trained models, it performs dynamic sequence alignment for diverse embeddings tailored to specific contexts, thus addressing the inherent limitations in fully utilizing input semantics. Through this explicit structure-induced constraint, our framework enables MLLMs to capture the global semantics and fine-grained details of the input multimodal sequence more effectively, thus bridging the gap between generative and discriminative training paradigms.

Figure 1: (a) In WebQA , the accuracy roughly forms a “U” shape curve when the relevant image-text pair for a question appears at different positions. While our model also shows similar trends, it tends to be more stable overall. (b) The accuracy of various types of questions in MMVP-VLM , it can be observed that our model’s performance improves on such tasks after introducing the discriminative training. Details can be seen in Appendix E.3

Our method effectively balances both discriminative and generative tasks, demonstrating synergistic benefits. _(i)_ Large-scale generative pre-trained models possess semantic-rich hidden states [41; 91; 23], which facilitate **discriminative tasks** like retrieval. Moreover, harnessing the capabilities of MLLM is crucial for complex discriminative tasks, such as interleaved image-text retrieval and fine-grained retrieval. _(ii)_ By integrating discriminative tasks, the model's effectiveness in **generative tasks**, particularly within tasks requiring cognitive and discrimination abilities, is enhanced, thereby mitigating certain occurrences of hallucinations. _(iii)_ We can employ Sugar to realize **retrieval-augmented generation**, eliminating the need for an off-the-shelf retrieval module , thereby amplifying the performance of various generative tasks. The usage of off-the-shelf retrieval presents a challenge wherein the retriever's performance affects the generator's final output . This necessitates independent optimization of both components, posing a dilemma in selecting optimal configurations. However, our approach circumvents such optimization challenges.

Through extensive experimentation, we have demonstrated the effectiveness of our approach. For generative tasks, Sugar establishes new state-of-the-art results on the tasks for complicated multi-modal comprehension tasks (_i.e.,_ DEMON ), fine-grained semantic distinctions (_i.e.,_ VizWiz , MME ), object hallucinations detection (_i.e.,_ POPE ) (Section 4.2 and Section 4.3). For discriminative tasks, we achieved competitive results in image-text retrieval compared, and significantly surpassed CLIP in interleaved retrieval and fine-grained retrieval (Section 4.4). Furthermore, employing the retrieval-augmented generation (RAG) strategy led to further improvements in a series of generative tasks (Section 4.5).

## 2 Related Work

**Multi-modal Large Language Models**. Flamingo  and BLIP-2  integrate LLMs with visual encoders, showcasing impressive zero-shot capabilities by aligning visual features with language representations. Building upon the advancements of LLaVA-1.5 , subsequent studies [103; 19; 94; 6; 42; 72; 95; 98; 45] propose fine-tuning MLLMs with multimodal instruction tuning data . Recently, there has been a surge in research [52; 80; 22; 21; 48] dedicated to enhancing the capacity of MLLMs to process interleaved image-text inputs effectively. However, these models primarily focus on generative tasks, overlooking the importance of introducing discriminative constraints. In this paper, we propose a structure-induced joint training strategy for unifying generative and discriminative tasks, further enhancing the capabilities of MLLMs, especially those requiring cognitive and discriminative abilities.

**Vision-Language Pre-training**. Vision-Language Pre-training primarily come in two forms: single-stream and dual-stream. In single-stream models, the embeddings for the image and text modalities are concatenated and jointly encoded [39; 50], while in dual-stream models, they are encoded by separate modality-specific encoders with optional cross-modality fusion [73; 31; 5]. These models have shown effectiveness in tasks such as classification and retrieval. However, they face challenges including difficulty in processing complex composed sequences [53; 54] and limited ability to discern detailed semantic differences [81; 79]. Recent attempts to utilize generative MLLMs for discriminative tasks have faced limitations, such as singular discriminative tasks , weak discriminative task performance , poor generalization , and compromised generative capabilities .

**LLMs for Retrieval**. Early models for retrieval primarily focused on word representations [16; 64; 74], with minimal generative capabilities. Some recent works have endeavored to fine-tune generative pre-trained LLMs to generate discriminative embeddings, albeit at the expense of compromising the model's original generative capabilities [44; 70; 65; 63; 24; 71]. GRIT  integrates generative and discriminative tasks in NLP and demonstrates mutual benefits between them. However, its training cost is prohibitively high compared to individual tasks. Moreover, due to its specialized attention mechanism, the model can only be trained from scratch.

**Retrieval-Augmented Generation**. Retrieval-Augmented Generation (RAG)[25; 69], which harnesses the advanced inference capabilities of LLMs along with external knowledge, has the potential to significantly mitigate issues related to long-tail entities and reduce the occurrence of hallucina

Figure 2: Our structure-induced generative and discriminative training joint training strategy.

tory responses [29; 36; 101; 77; 90; 92; 97]. Recently, there have also been related studies in the multimodal domain attempting to utilize retrieval augmentation [93; 96]. These methods typically require an additional retrieval module (_e.g.,_ CLIP), leading to component optimization challenges where the overall model performance is affected by the performance of the retrieval model, as well as concerns regarding the compatibility between the retrieval model and the MLLMs. Furthermore, retrieval modules like CLIP struggle to handle compositional or fine-grained scenarios, posing certain challenges for retrieval.

## 3 Method

As illustrated in Figure 3, we initially introduce the problem formulation and offer an overview of our structure-induced joint training strategy in Section 3.1. Subsequently, we delve into the specifics of dynamic sequence alignment algorithm in Section 3.2. Finally, we further introduce the Triple Kernel to aid in discriminating detailed semantics in Section 3.3.

### Problem Formulation and Architecture Overview

We view the interleaved image-text sequence as the general format for input samples, where images and textual data are alternately arranged. Typically, Multimodal Large Language Models (MLLMs) [55; 52; 11; 6] are tailored to generate text based on such input sequences, and it is conventionally optimized using self-regressive loss \(_{g}\). A special scenario arises when the input comprises only one image and a question, prompting the MLLMs to generate an answer accordingly.

While intuitive, this optimization objective solely supervises text generation and lacks constraints on the hidden states of the entire interleaved sequence input. Additionally, the existing generative training framework struggles to fully distinguish input semantics in certain contexts, such as discerning fine-grained object details. Consequently, it fails to adequately capture the global information or distinguish detailed semantics of the input samples.

Hence, we introduce a structure-induced constraint \(_{d}\) (see in Figure 2), which explicitly imposes the semantic relationships between different input samples as an induced structural constraint on the hidden states of MLLMs, facilitating the model in **capturing global semantics**. We conceptualize the derivation of semantic relationships between input samples as a Dynamic Sequence Alignment problem . Additionally, we straightforwardly select a token in the hidden state of the MLLM to encompass all preceding input information, eliminating the need for training any specialized tokens.

To further effectively **distinguish detailed semantics**, we integrate a novel kernel into the Dynamic Time Warping framework. Leveraging the strengths of various discriminative pre-trained models. Combined with this newly proposed loss with a hyperparameter \(\), the training objective can be formulated as:

\[=_{g}+_{d}\] (1)

### Dynamic Sequence Alignment

We formulate the computation of relationships within input interleaved sequences as a dynamic sequence alignment problem, and solve it by global alignment kernel. For two interleaved image-text sequence, each consisting of \(n\) and \(m\) images/sentences in total respectively (which we'll refer to as slices later on). We encode and normalize each slice, resulting in two sequences \(=(x_{1},,x_{n})\) and

Figure 3: (a) **Dynamic Sequence Alignment**. Semantically matched slices are connected with a blue dashed line. The arrows indicate the direction of the ordered temporal alignment path. With these alignments, we can obtain the similarity between two interleaved inputs for training. (b) **Sugar Framework**. Sugar supports both multi-modal generation and retrieval simultaneously.

\(=(y_{1},,y_{m})\) all of which take values in a state space \(\), that is two elements of \(^{}}}{{=}}_{i=1}^{ }^{i}\). In our setting, \(\) is simply \(^{d}\), \(d\) refers to the feature dimension. We define the global alignment kernel as follows, and it has been proved to be positive-definite under mild conditions and may prove more robust to quantify the similarity of two sequences [73; 68]:

\[K(,)=_{(,)}_ {i=1}^{||}e^{-_{}}(0,1]\] (2)

Following the suggestion by , we let \(_{}=}(x_{_{1}(i)},y_{_{2}( i)})+(2-e^{-(i)},y_{_{2}(i)})}{2 ^{2}}})\), \(\) is standard deviation, and it can be calculated by \(=}\) for \(x_{i},y_{i}\) in \(,\). \(\) is a fixed pre-defined hyperparameter and \((x_{_{1}(i)},y_{_{2}(i)})\) is the distance between slice \(x_{_{1}(i)}\) and \(y_{_{2}(i)}\) for an alignment (details for the definition of alignment can be seen in Appendix D.2).

Due to the causal attention mechanism, the token in hidden state of MLLM can encapsulate information from preceding tokens in the sequence. Therefore, we directly utilize the last token \(d_{i}\) of a sequence from the MLLM's hidden state and map it to the \(r_{i}\) using an MLP to represent the entire in-context sequence. During training, we obtain a set of \((r_{1},r_{2},,r_{n})\) and their corresponding input sequence embedding set \((_{1},_{2},,_{n})\). It is noteworthy that \(r_{i}\) and \(r_{j}\) (\(_{i}\) and \(_{j}\)) may originate from the same sequence but occupy different positions, thus enabling our method to utilize samples more efficiently.

Leveraging the GAK, we can derive the similarity matrix of \((r_{1},r_{2},,r_{n})\) and \((_{1},_{2},,_{n})\) distinctively, denoted as \(^{r}\), \(^{l}^{n n}\). For imposing the semantic relationships between different input samples as an induced structural constraint on the hidden state of MLLMs, we employ Mean Squared Error (MSE) loss aligned \(^{r}\) with the label matrix \(^{l}\). This approach eliminates the need for pre-defined label (_i.e.,_ positive and negative candidates) during training, allowing seamless integration into the aforementioned training framework (for specific training templates, please refer to Appendix E.1). Thus, we have the discriminative loss \(_{d}\):

\[_{d}=_{i=1}^{n}_{j=1}^{n}(m_{ij}^{r}-m_{ij} ^{l})^{2}\] (3)

Additionally, when both \(\) and \(\) contain only one slice, the computed result of the formula is monotonically increasing with the directly calculated cosine similarity (proof can be seen in Appendix 3). Therefore, in such cases, we simplify the computation by directly using cosine similarity. If \(r_{i}\) and \(r_{j}\) comes from the same input interleaved sample, we manually set \(m_{ij}^{l}=1\).

### Detailed Semantics Modeling

To further effectively distinguish detailed semantics, we further propose the Triple Kernel (TK), a positive definite kernel compatible with the previous framework. The TK leverages representations from diverse pre-trained discriminative models across uni-modal and cross-modal settings, harnessing their respective strengths. The definition is as below:

For two slice \(a,b^{d}\), meets (i) \(|a|=|b|=2\), \(d=d_{1}+d_{2}\), \(a=(a_{1},a_{2}),b=(b_{1},b_{2})\), \(a_{1},b_{1}^{d_{1}},a_{2},b_{2}^{d_{2}}\) and \(|a_{1}|=|a_{2}|=|b_{1}|=|b_{2}|=1\); or (ii) \(|a|=|b|=1\). We define triper kernel as follows:

\[(a,b)=\{\|a_{1}-b_{1}\|^{2}&|a|=|b|=2a,b\\ \|a_{2}-b_{2}\|^{2}&|a|=|b|=2a,b\\ \|a-b\|^{2}&.\] (4)

We prove triple kernel \(\) is a conditionally positive-definite kernel defined on \(\) (Appendix 2), aligning with the kernel definition in , thereby possessing its properties.

In practice, we let the feature dimension \(d=d_{1}+d_{2}\). For images, we employ DINOv2-base  and CLIP ViT-L/14  for encoding, then concatenate the embeddings after normalization. For sentences, we utilize BGE-base  and CLIP ViT-L/14, keeping the dimension unchanged. By utilizing the Triple Kernel, we can fully leverage the strengths of these three models, effectively distinguishing detailed semantics.

Figure 4: Selected examples for various image-text tasks. The pink background indicates retrieval results, while the blue background indicates generated results. More examples are provided in the Appendix F.2.

## 4 Experiments

To assess Sugar's **generative** ability, we conduct a comprehensive comparison with state-of-the-art models on 11 commonly used visual-language benchmarks in Section 4.2. Furthermore, we evaluate more complicated multimodal comprehension tasks on DEMON with 29 datasets in Section 4.3. For **discriminative** tasks, we compare performance across three different retrieval tasks: image-text retrieval, interleaved retrieval, and fine-grained retrieval in Section 4.4. Subsequently, we leverage Sugar's discriminative ability for **retrieval-augmented generation** compared with common used retrieve module in Section 4.5. Finally, we conduct ablation experiments to analyze the effectiveness of our method in Section 4.6.

### Setup

We apply our method to VILA , a recent state-of-the-art MLLM supporting interleaved input. We further fine-tune VILA using LoRA . Details about the experiments setting, datasets and the instruction examples, please check in Appendix E.

### Multimodal Comprehension on 11 Benchmarks

We conduct a comprehensive comparison with state-of-the-art models on 11 commonly used benchmarks, as shown in Table 1. Compared to existing models, Sugar achieves remarkable improvements over the second-best performing model on tasks requiring fine-grained semantics (_i.e.,_ LLaVAWd, VizWiz, SQA improve by 8%, 4.5%, 1.8% respectively) and benchmarks for detecting hallucinations (_i.e.,_ POPE ), while maintaining competitive results in other tasks. Notably, Sugar excels in discriminative tasks and still achieves 5 state-of-the-art results and 3 second-best results on 11 benchmarks for generative tasks, even outperforming some models larger than 7B. Our results demonstrate the benefits of incorporating the discriminative loss, aiding in fine-grained semantic tasks and reducing hallucinations.

### Complicated Multimodal Comprehension on DEMON

Table 2 demonstrates the superior performance of Sugar on the DEMON benchmark, which comprises 7 categories and a total of 29 sub-tasks. These tasks are considerably more complex than the previously used 11 common benchmarks. DEMON is tailored to evaluate the capacity of models and systems to understand demonstrative instructions that include multiple, interleaved, and multimodal contexts, presenting the essential information needed to complete a task. Sugar surpasses the previous state-of-the-art model on the DEMON benchmark, VPG-C , across 6 of 7 categories. For example, we achieve performance improvements of 36.1% in Text-Rich Images QA (TRQA) tasks and 17.2% in Visual Relation Inference (VRI) tasks, both of which require detailed semantics, compared to the second-best performing model. This underscores our advanced ability to associate interleaved

   Method & LLM & Res. & VQA\({}^{}\) & GOA & VizWiz & SQA\({}^{}\) & POPE & MME\({}^{}\) & MME\({}^{}\) & MMB & LLaVAWd & MM-Vet \\  BLIP-2  & Vicuna-13B & 224 & 41.0 & 41 & 19.6 & 61 & 42.5 & 85.3 & 1293.8 & 290.0 & – & 29.1 & 22.4 \\ InstrcRLBIP  & Vicuna-13B & 224 & – & 49.2 & 34.5 & 60.5 & 50.1 & – & – & – & 36 & – & 26.2 \\ InstrcRLBIP  & Vicuna-13B & 224 & – & 49.5 & 33.4 & 63.1 & 50.7 & 78.9 & 12.2 & 28.9 & – & – & 25.6 \\ Shikra  & Vicuna-13B & 224 & 77.4 & – & – & – & – & – & – & – & 58.8 & – & – \\ IDFECS-8B  & LLaMA-7B & 224 & 50.9 & 38.4 & 35.5 & – & 25.9 & – & – & – & 48.2 & – \\ IDFECS-8B  & LLaMA-6B & 224 & 60.0 & 45.2 & 36.0 & – & 30.9 & – & – & – & 54.5 & – & – \\ Qven-VL  & Owen-7B & 448 & 78.7 & 59.3 & 35.2 & 67.1 & 63.8 & – & – & 38.2 & – & – \\ Qven-VL  & Owen-7B & 448 & 78.2 & 57.7 & 38.9 & 68.2 & 61.5 & – & 1487.5 & **360.7** & 60.6 & – \\ LLaVA-1.5  & Vicuna-7B & 336 & 78.5 & 62.0 & 50.0 & 66.8 & 58.2 & 85.9 & 1510.7 & – & 64.3 & 49.0 & 30.5 \\ VILA-7B  & Lima-27 & 336 & 79.9 & **62.3** & 57.8 & 68.2 & 64.4 & 85.5 & 1533.0 & 296.1 & **68.9** & 70.0 & **34.9** \\ 
**Sugar** & Vicuna-7B & 336 & 76.0 & 58.7 & **60.4** & **69.4** & 57.5 & **86.6** & **1550.8** & 3000.0 & 64.9 & **75.6** & 31.3 \\   

Table 1: Comparison with state-of-the-art methods on 11 visual-language benchmarks. We mark the best performance **bold** and the second-best underlined. Benchmark names are abbreviated due to space limits. VQA-v2 ; GOA ; VizWiz ; SQA\({}^{}\): ScienceQA-IMG ; VQA\({}^{}\): TextVQA ; POPE ; MME\({}^{}\), MME\({}^{}\): MME Perception, MME Cognition ; MMB: MMBBench ; LLaVAWd: LLaVA-Bench(In-the-Wild)-Detail ; MM-Vet . \({}^{*}\) indicates the training images of the datasets are observed during training.

text-image inputs for stronger in-context understanding, and Sugar's strong capability to capture global semantics in interleaved sequences, facilitated by joint training with discriminative loss.

### Zero-shot Cross-modal Information Retrieval

**Image-text Retrieval.** We evaluated the performance of Sugar on the widely adopted MSCOCO  dataset in the context of a standard image-text retrieval task. Sugar demonstrated comparable performance to FROMAGe  in R@\(1\) and surpassed it in R@\(5\) and R@\(10\), highlighting Sugar's superiority in normal retrieval tasks.

**Interleaved Retrieval.** To assess the proficiency of Sugar in processing multimodal contextual information, we evaluated its performance in retrieving relevant images conditioned on sequences of interleaved image-text inputs from the Visual Storytelling (VIST) dataset . We conducted evaluations across several experimental configurations, following the same setup as FROMAGe  (see Appendix F.1). Our results show that Sugar outperforms FROMAGe in most settings, particularly achieving a 20.3% improvement in the 5c+4i configuration, significantly surpassing both CLIP and BLIP-2. This demonstrates that our method effectively leverages MLLMs' ability to handle complex interleaved sequence inputs, thereby achieving superior retrieval performance.

**Fine-grained Retrieval.** We tested fine-grained retrieval using the Winoground dataset , which evaluates the ability to perform vision-linguistic compositional reasoning. Surprisingly, Sugar outperformed all discriminative pre-trained models (both single-stream and dual-stream encoder architectures), achieving improvements of 5.3%, 77.1%, and 86.2% over the second-best model in the Text, Image, and Group dimensions, respectively. This demonstrates Sugar's strong capability to distinguish detailed semantics and performing compositional reasoning.

### Retrieval-Augmented Generation

Due to Sugar's dual capabilities in both discrimination and generation, we can achieve retrieval augmentation without the need for an additional retrieval module. For performing retrieval-augmented generation (RAG), we selected two tasks, namely VizWiz and SQA1, as they offer held-in data that were not seen during model training. We utilized a mixed set comprising the widely-used LLaVA-1.5 SFT subset and the held-in datasets of the two tasks as the knowledge base and employed different

  & LLM & MDM & SPM & VST & MIR & KGOA & RMO & MIR\({}^{*}\) \\  OpenPraming  & MTP-TB & 16.9 & 24.2 & 13.9 & 21.7 & 32.0 & 30.6 & 41.6 \\ BLIP-2  & Vicuna-13B & 26.1 & 21.3 & 10.7 & 17.9 & 39.2 & 33.5 & 39.7 \\ InstruckID  & Vicuna-TAB & 33.6 & 24.4 & 11.5 & 21.2 & 47.4 & 44.4 & 48.6 \\ MiniGP+TA  & Vicuna-TAB & 13.7 & 17.1 & 8.0 & 16.6 & 30.3 & 26.4 & 43.5 \\ LLaVA  & Vicuna-TAB & 7.8 & 10.7 & 8.3 & 15.9 & 36.2 & 28.3 & 41.5 \\ mPlug-Owl  & LLaVA-TAB & 12.7 & 19.3 & 5.4 & 16.3 & 33.3 & 32.5 & 42.5 \\ VPO-C  & Vicuna-TAB & 37.5 & 25.2 & 25.9 & **22.2** & 48.6 & 44.9 & 50.3 \\ VILA-TAB  & Vicuna-TAB & 47.8 & 25.8 & 13.2 & 17.2 & 60.1 & 42.1 & 50.5 \\ 
**Sugar** & Vicuna-TAB & **51.8** & **34.3** & **32.3** & 16.8 & **64.4** & **65.9** & **51.7** \\  &  &  &  &  &  &  & \\ 

Table 2: Comparision with state-of-the-art method on DEMON  benchmark.

[MISSING_PAGE_FAIL:9]