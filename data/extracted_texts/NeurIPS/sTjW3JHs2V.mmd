# Let the Flows Tell: Solving Graph Combinatorial Optimization Problems with GFlowNets

Dinghuai Zhang

Mila

Correspondence to dinghuai.zhang@mila.quebec.

&Hanjun Dai

Google DeepMind

Nikolay Malkin, Aaron Courville, Yoshua Bengio, Ling Pan

Mila

###### Abstract

Combinatorial optimization (CO) problems are often NP-hard and thus out of reach for exact algorithms, making them a tempting domain to apply machine learning methods. The highly structured constraints in these problems can hinder either optimization or sampling directly in the solution space. On the other hand, GFlowNets have recently emerged as a powerful machinery to efficiently sample from composite unnormalized densities sequentially and have the potential to amortize such solution-searching processes in CO, as well as generate diverse solution candidates. In this paper, we design Markov decision processes (MDPs) for different combinatorial problems and propose to train conditional GFlowNets to sample from the solution space. Efficient training techniques are also developed to benefit long-range credit assignment. Through extensive experiments on a variety of different CO tasks with synthetic and realistic data, we demonstrate that GFlowNet policies can efficiently find high-quality solutions. Our implementation is open-sourced at https://github.com/zdhNarsil/GFlowNet-CombOpt.

## 1 Introduction

Combinatorial optimization (CO) is a branch of optimization that studies problems of minimizing or maximizing some cost over a finite feasible set. CO problems usually involve discrete structures, such as graphs, networks, and permutations, and require optimizing an objective function subject to discrete constraints in the solution space, which is often NP-hard. CO problems have broad applications, including in medicine, engineering, operations research, and management (Paschos, 2010), and have spurred the development of discrete mathematics and theoretical computer science for a century (Kuhn, 1955; Kruskal, 1956; Ford & Fulkerson, 1956).

During the past few decades, researchers have developed numerical solvers such as Gurobi(Gurobi Optimization, 2023) to give approximate solutions via integer programming. In recent years, interest in learning-based methods for solving CO problems has grown significantly. These approaches leverage the power of deep networks to learn the inherent structure of CO problems and provide efficient

Figure 1: Illustration of GFlowNet for a toy MIS problem where all the states form a DAG. Every trajectory starts from the same initial state (whose vertices are all gray). Each transition denotes adding one vertex to the solution set, _i.e._, changing one vertex to black. See Section 3.2 for details.

and effective solutions. One family of machine learning methods utilizes solver-found solutions to provide a supervised learning (SL) signal for training neural networks (Selsam et al., 2018; Gasse et al., 2019; Nair et al., 2020). This line of algorithms requires expensive precomputation by the numerical solver to produce supervision labels. On the other hand, unsupervised learning (UL) methods search for the solution without the help of a solution oracle. One branch of UL methods, called probabilistic methods, decodes the heatmap generated from one pass of a neural network to get solutions (Toenshoff et al., 2019; Karalias and Loukas, 2020). These methods can achieve fast inference at the cost of a large optimality gap. Another branch of unsupervised methods is reinforcement learning (RL), which iteratively refines or constructs the problem solution with practitioner-specified MDPs (Bello et al., 2016; Deudon et al., 2018; Wu et al., 2019).

Despite many recent efforts to apply deep RL to CO problems, such approaches have fundamental limitations. For example, due to the symmetry in problem configurations, there could be multiple optimal solutions to the same CO problem (Li et al., 2018). Standard RL algorithms such as Fujimoto et al. (2018) are grounded in the nature of cumulative reward maximization and fail to promote diversity in the solutions. Although entropy-regularized RL (Haarnoja et al., 2017, 2018; Zhang et al., 2023) converges to a stochastic policy instead of a deterministic one, these methods target _trajectory-level entropy_ rather than _solution-level entropy_. Consequently, the agent may get trapped in solutions that can be reached by many trajectories and lack the ability to generate diverse candidate solutions. In addition, the performance of RL methods largely depends on the designed reward function and relies on a dense per-step reward for learning the value functions and policies. As a result, it is challenging to apply RL in our problems if only a terminal reward is provided.

Although there are attempts to fix these issues (Kwon et al., 2020; Ahn et al., 2020), they are mostly problem-specific and only achieve marginal improvement. In this work, we turn to a more principled framework, namely generative flow networks (Bengio et al., 2021, GFlowNets), to search for high-quality diverse candidates in CO problems. GFlowNet is a novel decision-making framework for learning stochastic policies to sample composite objects with probability proportional to a given terminal reward, which is suitable for problems where the solution is only related to the terminal state of generative trajectories. We design MDPs for a variety of NP-hard CO problems, where the intermediate states form a flow network in the latent space, and GFlowNet learns an agent to sequentially make decisions in this environment (Figure 1). Another challenge of applying GFlowNets is learning from long trajectories. In large-scale graph CO problems, the GFlowNet agent will encounter a very long trajectory before termination, rendering the task of credit assignment challenging. To this end, we develop efficient learning algorithms to train GFlowNets from transitions rather than complete trajectories, which greatly helps the learning process, especially in large-scale setups. Through extensive experiments on different CO tasks, we demonstrate the advantage of our proposed GFlowNet approach. In summary, our contributions are as follows:

* We design a problem-specific MDP for GFlowNet training on four different CO tasks.
* We propose an efficient GFlowNet learning algorithm to enable fast credit assignment for the GFlowNet agents with long trajectories that emerge in our graph CO problems.
* The empirical advantage of GFlowNets is validated through experiments on different CO problems.

## 2 Preliminaries

### GFlowNets

Generative flow networks, or GFlowNets, are variational inference algorithms that treat sampling from a target probability distribution as a sequential decision-making process (Bengio et al., 2021, 2023). We briefly summarize the formulation and the main training algorithms for GFlowNets.

We assume that a fully observed, deterministic MDP with set of states \(\) and set of actions \(\) is given. The MDP has a designated _initial state_, denoted \(_{0}\). Certain states are designated as _terminal_ and have no outgoing actions; the set of terminal states is denoted \(\). All states in \(\) are assumed to be reachable from \(_{0}\) by a (not necessarily unique) sequence of actions (see Figure 2). A _complete trajectory_ is a sequence of states \(=(_{0}_{1} _{n})\), where \(_{n}\) and each pair of consecutive states is related by an action, _i.e._, \( i\ (_{i},_{i+1})\).

A _policy_ on the MDP is a choice of distribution \(P_{F}(^{}|)\) for each \(\) over the states \(^{}\) reachable from \(\) in a single action.2 A policy induces a distribution over complete trajectories via

\[P_{F}(_{0}_{1} _{n})=_{i=0}^{n-1}P_{F}(_{i+1}_{i}).\]

The marginal distribution over the final states of complete trajectories is denoted \(P_{F}^{}\), a distribution on \(\) that may in general be intractable to compute exactly, as \(P_{F}^{}()=_{}P_{F}()\), with the sum taken over all complete trajectories that end in \(\).

A _reward function_ is a mapping \(_{>0}\), which is understood as an unnormalized probability mass on the set of terminal states (we will typically make the identification \(R()=(-()/T)\), where \(:\) is an energy function and \(T>0\) is a temperature parameter). The learning problem approximately solved by a GFIowNet is to fit a policy \(P_{F}(^{}|)\) such that the induced distribution \(P_{F}^{}()\) is proportional to the reward function, _i.e._,

\[P_{F}^{}() R()=(-()/T).\] (1)

The policy \(P_{F}(^{}|)\) is parametrized as a neural network with parameters \(\) taking \(\) as input and producing the logits of transitioning to each possible subsequent states \(^{}\). This problem is made difficult both by the intractability of computing \(P_{F}^{}\) given \(P_{F}\) and by the unknown normalization constant (partition function) on the right side of (1). Learning algorithms overcome these difficulties by introducing auxiliary objects into the optimization. Next, we review two relevant objectives.

Detailed balance (DB)The DB objective (Bengio et al., 2023), requires learning two objects in addition to a parametric forward policy \(P_{F}(^{}|;)\) (we omit \(\) when it causes no ambiguity):

* A _backward policy_, which is a distribution \(P_{B}(|^{};)\) over the parents (predecessors) of any noninitial state in the MDP;
* A _state flow_ function \(F(;):_{>0}\).

The detailed balance loss for a single transition \(^{}\) is defined as

\[_{}(,^{};)= (;)P_{F}(^{}| ;)}{F(^{};)P_ {B}(|^{};)})^{2}.\] (2)

The DB training theorem states that if \(_{}(,^{};)=0\) for all transitions \(^{}\), then the policy \(P_{F}\) satisfies (1), _i.e._, samples proportionally to the reward. The manner of selecting transitions \(^{}\) on which to minimize (2) is discussed below.

The loss that performs best for the problems in this paper (see (6) in SS3.3) is equivalent to DB with a particular parametrization of \( F(s)\) that bootstraps learning by expressing the log-state flow as an additive correction to a _partially accumulated_ negative energy.

Trajectory balance (TB)The TB objective (Malkin et al., 2022) features a simpler parametrization: in addition to the action policy \(P_{F}\), one learns a backward policy \(P_{B}\) and only a single scalar \(Z_{}\), an estimator of the partition function corresponding to the initial state flow \(F(_{0})\) in the DB parametrization. The TB loss for a complete trajectory \(=(_{0}_{1} _{n}=)\) is

\[_{}(;)=(}_{i=0}^{n-1}P_{F}(_{i+1}|_{i};)}{R()_{i=0}^{n-1}P_{B}(_{i}|_{i+1}; )})^{2}.\] (3)

The TB training theorem states that if \(_{}(;)=0\) for all complete trajectories \(\), then the policy \(P_{F}\) satisfies (1). Furthermore, \(Z\) then equals the normalization constant of the reward, \(=_{}R()\).

In practice, policies and flows are typically output in the log domain, _i.e._, a neural network predicts logits of the distributions \(P_{F}(|)\), \(P_{B}(|^{})\) and the log-flows \( F()\) and \( Z\).

Training policy and explorationThe DB and TB losses depend on individual transitions or trajectories, but leave open the question of how to choose the transitions or trajectories on which they are minimized. A common choice is to train in an on-policy manner, _i.e._, rollout trajectories \( P_{F}(;)\) and perform gradient descent steps on \(_{}(;)\) or on \(_{}(_{i},_{i+1};)\) for transitions \(_{i}_{i+1}\) in \(\). In this case, DB and TB have close connections to variational (ELBO maximization) objectives (Malkin et al., 2022).

However, an exploratory behaviour policy can also be used, for example, by sampling \(\) from a version of \(P_{F}\) that is tempered or mixed with a uniform distribution (resembles \(\)-greedy exploration in RL). Note that, unlike policy gradient methods in RL, GFlowNet objectives require no differentiation through the sampling procedure that yields \(\). The ability to stably learn from off-policy trajectories is a key advantage of GFlowNets over hierarchical variational models (Zimmermann et al., 2022; Malkin et al., 2023). See related study in Section C.

Conditional GFlowNetsThe MDP and the reward function in a GFlowNet can depend on some conditioning information. For example, in the tasks we study, a GFlowNet policy sequentially constructs the solution to a CO problem on a graph \(\), and the set of permitted actions depends on \(\). The conditional GFlowNets we train achieve amortization by sharing the policy model between different \(\), enabling generalization to \(\) not seen in training.

### Graph combinatorial optimization problems

We focus on the following four NP-hard CO problems on graphs: maximum independent set (MIS), maximum clique (MC), minimum dominating set (MDS), and maximum cut (MCut). A CO problem can be described with an undirected graph \(=(V,E)\), where \(V\) is the set of vertices and \(E\) is the set of edges. Such problems typically require one to optimize over variables in a finite composite space to maximize or minimize some particular graph properties, as follows. Without loss of generality, we assume that all the graph weights equal one for simplicity, as our method can be easily extended to weighted graphs.

Maximum independent setIn graph theory, an independent set is a set of vertices \(S\) in a given graph structure \(\) where any pair of vertices \(\{i,j\} S\) are not neighbors _i.e._, \( i,j S,(i,j) E\). The MIS problem is to find such an independent set that has the largest possible size \(|S|\).

Maximum cliqueA clique is a subset of the vertices \(S V\) where all pairs of vertices are adjacent, _i.e._, \( i,j S,(i,j) E\). The MC problem is to find a clique that has the largest size. We also remark that MC can be considered as a complementary problem of MIS, in the sense that the MC of any graph is actually the MIS of its complementary graph.

Minimum dominating setA dominating set \(S V\) for a graph \(\) is a subset of vertices such that, for any vertex in this graph, it is either in \(S\), or it has a neighbor in \(S\). The MDS problem is to find the smallest dominating set for a given graph structure.

Maximum cutGiven a subset of graph vertices \(S V\), the cut is defined as the number of edges between \(S\) and \(V S\). The MCut problem is to find a set of vertices \(S\) that maximizes the cut.

## 3 Methodology

### Optimization as probabilistic inference

A CO problem can be seen as a constrained energy minimization problem in a discrete composite space \(_{}()\), where \(\) is the target energy function and \(\) is the solution space or feasible set. This optimization problem can be considered roughly as sampling from an energy-based model \(p^{*}_{T}()\{-()/T\}\), where \(T\) is the temperature parameter that controls the smoothness of the density landscape. Sampling from such highly structured space is non-trivial, therefore we propose to use GFlowNets to amortize this inference process. According to the GFlowNet theory, a perfectly trained GFlowNet with the reward function \(R()=\{-()/T\}\) will be able to accurately sample from \(p^{*}_{T}()\). In this way, GFlowNets trained with a reasonably small temperature \(T\) can be used to search for solutions to given CO problems, as stated below.

**Proposition 1**.: _Assume that the GFlowNet is perfectly trained for a given temperature \(T\), i.e., the training loss over a policy with full support equals zero. Then, if \(T\), the distribution sampledby GFlowNet will converge to a uniform distribution on \(\); if \(T 0\), the distribution sampled by GFlowNet will converge to the uniform distribution on optimal solutions to the optimization problem._

Notice that each graph \(\) corresponds to one unique sampling problem; thus, here we learn a graph conditional GFlowNet to amortize this condition distribution \(p(|)\), _i.e._, every learnable GFlowNet component (like the forward / backward policy or the flow function) is conditioned on the graph \(\).

### Designing Markov decision processes for GFlowNets

This section illustrates the design of appropriate Markov decision process (MDP) formulations for GFlowNet learning; see Figure 2 for a summary.

StateFor all the CO problems studied in this work, the solution \(\) is a subset of vertices for a given graph, represented as a binary vector \(=(^{1},,^{|V|})\{0,1\}^{|V|}\), where \(^{i}=1\) indicates that the \(i\)-th vertex belongs to the set and \(^{i}=0\) indicates that it does not. Note that the feasible solution space \(\) is a subset of the full space \(\{0,1\}^{|V|}\), as some binary vectors encode a vertex set outside the feasible set (_e.g._, a non-independent set in the MIS problem). The solution space is also the GFlowNet's terminal state space. The design of the GFlowNet state space is inspired by that in Zhang et al. (2022). We begin by defining

\[}\{(^{1},,^{|V|}): ^{d}\{0,1,\},d=1,,|V|\},\] (4)

where \(\) represents an unspecified "void" or "yet unspecified" situation for a particular vertex. Notice that \(}\) is a superset of the terminal state space \(\). The initial state is the all-void vector \(_{0}=(,,,)\). In our MDP design, we only allow transforming vertex values from void (\(\)) to non-void (\(0\) or \(1\)) according to problem-specific rules. The trajectory terminates when all the entries are non-void, _i.e._, every entry is either \(0\) or \(1\). Notice that we do not need to have an explicit stop action in the agent's action space.

Notice that not all vectors in \(}\) that have no void entries lie in \(\). Therefore, we must restrict \(\) so that the set of terminal states is exactly the set of vectors encoding feasible solutions \(\). To be precise, we define the state space \(\) to be the set of states \(}\) such that there exists at least one \(\) that can be obtained from \(\) by valid transitions.

Care must be taken to modify the set of permitted actions from each state accordingly, so that actions always produce states in \(\). This turns out to be doable in the problems we study (as, for example, any subset of an independent set is independent). As an example, we next describe our action / transition / reward design for the MIS problem. Details for other tasks are deferred to the Appendix B.

ActionThe initial state is all-void, meaning that the partially constructed independent set is initialized with the empty set. The action of the MIS MDP is simply to choose one void vertex and add it to the current solution set by turning the entry value of the chosen vertex from \(\) to \(1\).

Figure 2: Our proposed MDP designs for different CO tasks. All the tasks aim at a set of vertices as the problem solution, thus we use \(0/1/\) to represent “not in the set” / “in the set” / ”unspecified” for each vertex, which corresponds to white / black / gray in the figures. For each figure, the first arrow denotes conducting one action (marked with hammer), and the second arrow denotes the designed transition to guarantee any intermediate state represents a valid independent set / clique / dominating set / cut. The rightmost graph in each figure shows a feasible solution for the CO problem, where all vertices are specified (_i.e._, belongs to the GFlowNet terminal state space \(\)).

TransitionTo ensure that the state can always be completed to an independent set, it is essential to carefully handle the actions taken. When a void vertex is chosen and its entry value is modified to \(1\), we also update the entry values of all its neighboring vertices to \(0\), which excludes the possibility of getting two adjacent vertices in the following steps. This proactive approach ensures that the independent set constraint is not violated in subsequent steps.

We remark that the feasible set \(\) in this problem consists not of all independent sets, but of _order-maximal_ independent sets, _i.e._, those to which no vertex can be added while keeping them independent. Non-order-maximal independent sets cannot be constructed with such transitions.

RewardWe set the log reward to be the resulting independent set size, _i.e._, \(()=-||_{1}\), where \(||_{1}\) denotes the \(_{1}\) norm, _i.e._, the number of \(1\)'s in the binary vector \(\).

### Factors affecting training efficiency

Transition-based GFlowNet trainingMost successful GFlowNet implementations (_e.g._, those in https://github.com/GFNOrg/) require a complete trajectory to compute the training loss and its gradient. Existing implementations with DB also specify the training loss at the trajectory level:

\[(;)=_{t=0}^{n-1}_{}( _{t},_{t+1};),=(_{0},_{1},,_{n}),\] (5)

and calculate the parameter gradient update with \(_{()}[_{}(;)]\) with some potentially off-policy distribution \(()\). This is also the case for other GFlowNet algorithms, such as flow matching (Bengio et al., 2021) and subtrajectory balance (Madan et al., 2023).

These implementations work well for moderate-scale trajectories, as previous GFlowNet works have shown (Malkin et al., 2022; Madan et al., 2023). However, for very long trajectories, such a design hinders efficient training: for a single complete trajectory which contains \(n\) transitions, one needs \(n\) calls of the neural network forward passes and storing all the intermediate feature maps and parameters in the GPU memory. Each forward pass contains multiple message passing operations on given graph structures, which is computationally expensive (linearly increasing with \(n\)) in terms of speed and memory storage. In our experiments, we encounter large-scale problems where the trajectory is as long as \( 400\) in length; nonetheless, with such graphs, our adopted graph neural networks can only support forward and backward passes with batch size approximately \(250\) (which is much smaller than \(400\)) on a \(40\) GB GPU memory device. This prohibits the usage of these trajectory-based GFlowNet training objectives on large-scale graph applications. In addition, the correlation between consecutive samples may incur stability issues (Mnih et al., 2015).

Figure 4: Comparison between different GFlowNet variants.

Figure 3: Illustration of transition-based GFlowNet training. We break complete trajectories into transitions, and then randomly choose some of the them to form a buffer to train the GFlowNet agent. We use \(\) to denote the terminal state \(_{n}\). \(}()\) denotes designed intermediate learning signals.

To this end, we turn to use a transition-based GFlowNet training approach whitout the knowledge of complete trajectories, which is first proposed in Deleu et al. (2022) and has shown effectiveness in Nishikawa-Toomey et al. (2022); Deleu et al. (2023). We randomly sample \(B\) transitions \(=\{(^{b},^{rb})\}_{b=1}^{B}\) from a complete trajectory and construct detailed balance-based loss: \(()=_{b=1}^{B}(^{b},^{rb};)\). This enables GFlowNets to be efficiently trained with long trajectories and limited GPU memory, which also converges faster. A corresponding schematic illustration of the algorithm can be found in Figure 3. In Figure 4, we show a comparison of learning efficiency between transition-based and trajectory-based GFlowNet implementations, where "-traj" denotes the latter variant. Different methods here share similar speed for one epoch training, thus from the figure we can see that transition-based approaches learn more efficiently. Besides the improved computation efficiency, our transition-based approach is also more memory efficient - as the memory occupation is proportional to the batch size instead of trajectory lengths (as for trajectory-based implementation), demonstrating its applicability to long-horizon problems.

Improving credit assignment with intermediate learning signalsFor normal GFlowNet training methods, the only learning signal in a trajectory comes from the terminal states and their associated reward values. This results in a relatively slow credit assignment process, and is inefficient to propagate information from near-terminal states to the early states due to the difficulty of attributing credits of each action in a long trajectory. The ability to learn from incomplete trajectories is especially important in our proposed transition-based training, since we are not using all the transitions from the complete trajectories. Therefore, we incorporate intermediate learning signals via the forward-looking (Pan et al., 2023, FL) technique:

\[_{}(,^{};)=(-}()+(;)+ P_{F}( ^{}|;)+}(^{ })-(^{};)- P_{B}( |^{};))^{2},\] (6)

where \(}():\) is a continuation of the reward energy \(():\) which is only defined in the terminal state space \(\). For simplicity, here we ignore the conditioning on graph structure \(\). This FL method enables dense supervision signals to GFlowNet training, resulting in faster credit assignment as can be seen in Figure 3. Notice here that we need to design a handcrafted reward \(}()\) for all possible latent states to reflect our estimation on intermediate signals. For MIS problems, we naturally define \(-}()\) to be the number of vertices in the current set, _i.e._, the number of \(1\) entries in state \(\). This semantically coincides with the definition of an MIS terminal reward. We defer the intermediate reward design for other tasks to the Appendix. The effectiveness of FL against the DB or TB algorithms can be seen in Figure 4. We summarize the resulting algorithm in Algorithm 1.

## 4 Related work

GFlowNetsGFlowNets were intended as diversity-seeking samplers for biological sequence and molecule design, an application area that continues to motivate research (Bengio et al., 2021; Jain et al., 2022, 2023; Shen et al., 2023; Jain et al., 2023). However, much recent work has used GFlowNets as samplers for Bayesian posterior distributions, _e.g._, causal discovery (Deleu et al., 2022; Nishikawa-Toomey et al., 2022; Atanackovic et al., 2023), amortized variational EM with discrete latents (Hu et al., 2023), neurosymbol inference (van Krieken et al., 2022), and feature attribution in classifiers (Li et al., 2023). The theory and optimization techniques for GFlowNets have also evolved, with improved training objectives and exploration techniques (Malkin et al., 2022; Madan et al., 2023; Pan et al., 2022, 2023; Shen et al., 2023), better understanding of their connections to variational methods (Zhang et al., 2022; Malkin et al., 2023; Zimmermann et al., 2022), and extensions to stochastic (Zhang et al., 2023; Pan et al., 2023) and continuous (Lahlou et al., 2023) MDPs.

ML for combinatorial optimizationThe surge of machine learning for CO problems alleviates the reliance on hand-crafted heuristics while enabling generalization to new instances (Bengio et al., 2018; Cappart et al., 2021). Some methods (Li et al., 2018; Gasse et al., 2019; Gupta et al., 2020; Sun and Yang, 2023) rely on supervised information from expert solvers, which can be hard to obtain. Alternative approaches that leverage reinforcement learning (Dai et al., 2017; Kool et al., 2019; Chen and Tian, 2019; Yolcu and Poczos, 2019; Ahn et al., 2020; Delarue et al., 2020; Drori et al., 2020) or other unsupervised learning objectives (Karalias and Loukas, 2020; Sun et al., 2022; Wang et al., 2022) broaden the applicability of learning for CO problems. However, the mode-collapse issue might hinder the diversity and thus the solution coverage. In this regard, GFlowNets are easy to train while also designed for discovering multiple modes. As the first example to show the advantage of GFlowNets in CO problems, Zhang et al. (2023a) tackles the robust job scheduling problem that is central to compiler optimization. In our paper, we formalize CO under the GFlowNet framework with principled MDP design and demonstrate its effectiveness in a wide range of graph CO tasks.

## 5 Experiments

We conduct extensive experiments on various graph CO tasks to demonstrate the effectiveness of the proposed GFlowNet approach. For MIS problems, we follow the setup in the MIS benchmark from Bother et al. (2022), while for other tasks, we follow the experimental setup in Sun et al. (2022).

DatasetsAs Dai et al. (2021) have pointed out that problems in existing synthetic graph data are relatively easy for MIS and MC, we take the more complicated RB graphs (Xu and Li, 2000) following Karalias and Loukas (2020). For realistic data, we take the SATLIB dataset (Hoos et al., 2000), which is reduced from SAT instances in conjunctive normal form. For the other two tasks, namely MDS and MCut, we adopt BA graphs (Barabasi and Albert, 1999) following Sun et al. (2022). For all types of synthetic graph data, we generate two scales of datasets (denoted Small and Large), which respectively contain around \(200\) to \(300\) vertices and \(800\) to \(1200\) vertices.

BaselinesFor MIS problems, we compare with the baselines in the MIS benchmark. For classical operation research (OR) methods, we include a general-purpose mixed-integer program solver (Gurobi) and a MIS-specific solver (Lamm et al., 2017, KaMIS). For learning-based methods, we compare with a reinforcement learning-based PPO method (Ahn et al., 2020), and supervised learning with tree search refinement, in two different implementations (Li et al. (2018, Intel) and Bother et al. (2022, DGL)). For the non-MIS tasks, we compare with the Gurobi solver, two heuristic methods which are greedy and mean-field annealing (Bilbro et al., 1988, MFA), and two state-of-the-art probabilistic methods (Karalias and Loukas, 2020) and the annealed version (Sun et al., 2022). We use Erdos and Anneal to denote these two learning-based methods. For max-cut problems, we also adopt a semi-definite programming baseline (coined SDP in the result tables) which aims at a relaxation of the MCut task. We set its maximal running time to \(10\) hours. For the methods already contained in the MIS benchmark, we train them from scratch and report performance with the same protocol; for other algorithms, we write our own implementations and test them in the same way.

Results & analysisWe evaluate both the performance and the inference time and report the mean value of the objective (_e.g._, set size in MIS) and the approximation ratio relative to the best-performing non-ML solver, treated as an oracle.3 The time denotes the total latency of evaluating on the test set. The best results among non-OR methods are marked in bold. MIS results are demonstrated in Table 1, where the problem-specific solver KaMIS is served as the gold standard for calculating the drop. For MIS methods, the larger the size of the independent set found, the better the algorithm. The "UL" (unsupervised learning) refers to the algorithms that do not need labels (_i.e._, solutions found by solvers) in the training set. For SATLIB, none of the supervised learning baselines can

    &  &  &  &  \\   & & Size \(\) & Drop \(\) & Time \(\) & Size \(\) & Drop \(\) & Time \(\) & Size \(\) & Drop \(\) & Time \(\) \\   Gurobi & OR & \(19.98\) & \(0.01\%\) & 47:34 & 40.90 & \(5.21\%\) & 2:10:26 & \(425.95\) & \(0.00\%\) & 3:43:19 \\ KaMIS & OR & \(20.10\) & \(0.00\%\) & 1:24:12 & \(43.15\) & \(0.00\%\) & 2:03:36 & \(425.96\) & \(0.00\%\) & 4:15:41 \\  PPO & UL & \(19.01\) & \(5.42\%\) & 1:17 & \(32.32\) & \(25.10\%\) & 7:33 & \(421.49\) & \(1.05\%\) & 13:12 \\ Intel & SL & \(18.47\) & \(8.11\%\) & 13:04 & \(34.47\) & \(20.12\%\) & 20:17 & — & — & — \\ DGL & SL & \(17.36\) & \(13.61\%\) & 12:47 & \(34.50\) & \(20.05\%\) & 23:54 & — & — & — \\ Ours & UL & \(\) & \(4.57\%\) & 0:32 & \(\) & \(13.14\%\) & 4:22 & \(\) & \(0.57\%\) & 23:13 \\   

Table 1: Max independent set experimental results. We report the absolute performance, approximation ratio (relative to KaMIS), and inference time. All algorithms fall into three categories: OR (operations research), SL (supervised learning), and UL (unsupervised learning). “\(\)” denotes no reasonable result is achieved by the corresponding algorithm in 10 hours. Time shown as H:M:S.

achieve meaningful results (average MIS size \(<400\)) within \(10\) hours, thus we use "--" to mark performance. One can see that GFlowNets surpass all the learning-based baselines in the sense of achieving the largest independent set solution.

For the MC & MDS & MCut problems, we exhibit the experimental results on small graphs in Table 2 and results on large graphs in Table 3, where Gurobi serves as the gold standard to calculate the performance drop ratio. Notice that for MDS problems, the smaller the vertex size result, the better the performance, which is opposite to other tasks. We observe that GFlowNet outperforms other baselines across different tasks and problem scales, with the only exception being the large-scale max-cut problem. This reflects the fairly universal effectiveness of the proposed method. We also notice that GFlowNets require a longer inference time compared to other methods. This is because the GFlowNet only adds one vertex into the set at each step, thus it needs multiple steps to output a vertex set solution. In contrast, probabilistic methods such as "Erdos goes neural" (Karalias and Loukas, 2020) only require one neural network forward pass during inference time to give a coarse estimate of the solution. We can see how this compares with a sequential generation approach as in GFlowNets, where each decision is taken in the context of the previously already taken decisions, ensuring a better coordinated set of decisions.

Ablation studyWe now conduct in-depth ablation studies to evaluate the key design choices in our method. We first study the difference between a series of GFlowNet variants in Figure 4, based on the MIS task with small scale graph data. We compare transition-based FL, trajectory-based FL, transition-based DB, trajectory-based DB, and TB (which only has trajectory-based implementation). Our result indicates that GFlowNet's transition-based FL implementation learns the fastest for CO tasks, which supports our modeling choice in Section 3.3. Figure 5 in Section C summarizes additional ablation studies including the temperature annealing, off-policy exploration strategy, and network architectures. We vary the GFlowNet's temperature coefficient which scales the temperature hyperparameter; we ablate the off-policy exploration during the rollout stage of GFlowNet training; we also ablate the GFlowNet architecture. Our results indicate that the GFlowNet is robust to a

    &  &  &  &  \\   & & Size \(\) & Drop \(\) & Time \(\) & Size \(\) & Gap \(\) & Time \(\) & Size \(\) & Drop \(\) & Time \(\) \\   Gurobi & OR & \(19.05\) & \(0.00\%\) & 1:55 & \(27.89\) & \(0.00\%\) & 1:47 & \(732.47\) & \(0.00\%\) & 13:04 \\ SDP & OR & — & — & — & — & — & — & — & \(700.36\) & 4.38\% & 35:47 \\  Greedy & H & \(13.53\) & \(28.98\%\) & 0:25 & \(37.39\) & \(25.41\%\) & 2:13 & \(688.31\) & \(6.03\%\) & 0:13 \\ MFA & H & \(14.82\) & \(22.15\%\) & 0:27 & \(36.36\) & \(23.29\%\) & 2:56 & \(\) & \(3.88\%\) & 1:36 \\ Erdos & UL & \(12.02\) & \(36.90\%\) & 0:41 & \(30.68\) & \(9.09\%\) & 1:00 & \(693.45\) & 5.33\% & 0:46 \\ Anneal & UL & \(14.10\) & \(25.98\%\) & 0:41 & \(29.24\) & \(4.62\%\) & 1:01 & \(696.73\) & 4.88\% & 0:45 \\ Ours & UL & \(\) & \(14.75\%\) & 0:42 & \(\) & \(2.52\%\) & 2:20 & \(\) & \(3.85\%\) & 2:57 \\   

Table 2: Max clique, min dominating set, and max cut results on small graphs (\(|V|\) between \(200\) and \(300\)). We report absolute performance, approximation ratio, and inference time. All algorithms fall into three categories: OR (operations research), H (heuristic), and UL (unsupervised learning). The time latency is shown in the form of hour:minute:second.

    &  &  &  &  \\   & & Size \(\) & Drop \(\) & Time \(\) & Size \(\) & Gap \(\) & Time \(\) & Size \(\) & Drop \(\) & Time \(\) \\   Gurobi & OR & \(33.89\) & \(0.00\%\) & 16:40 & \(103.80\) & \(0.00\%\) & 13:48 & \(2915.29\) & \(0.00\%\) & 1:05:29 \\ SDP & OR & — & — & — & — & — & — & — & \(2786.00\) & \(4.43\%\) & 10:00:00 \\  Greedy & H & \(26.71\) & \(21.17\%\) & 0:25 & \(140.52\) & \(26.13\%\) & 35:01 & \(2761.06\) & \(5.29\%\) & 3:07 \\ MFA & H & \(27.94\) & \(17.56\%\) & 2:19 & \(126.56\) & \(17.98\%\) & 36:31 & \(2833.86\) & 2.79\% & 7:16 \\ Erdos & UL & \(25.43\) & \(24.96\%\) & 2:16 & \(116.76\) & \(11.10\%\) & 3:56 & \(\) & \(1.54\%\) & 2:49 \\ Anneal & UL & \(27.46\) & \(18.97\%\) & 2:16 & \(111.50\) & \(6.91\%\) & 3:55 & \(2863.23\) & \(1.79\%\) & 2:48 \\ Ours & UL & \(\) & \(7.29\%\) & 4:50 & \(\) & \(5.88\%\) & 32:12 & \(2864.61\) & \(1.74\%\) & 21:20 \\   

Table 3: Max clique, min dominating set, and max cut results on large graphs (whose \(|V|\) is between \(800\) and \(1200\)). We use the same format as Table 2.

wide range of hyperparameters, making it appealing to be applied to different CO problems, which validates the effectiveness of our proposed methodology from another perspective.

## 6 Conclusion

Our work focuses on the challenges of solving CO problems using unsupervised learning approaches. This work contributes to this growing field by proposing to apply the principled GFlowNet decision-making framework to CO tasks. By combining the power of probabilistic inference and sequential decision-making, GFlowNets offer a promising direction for finding a diverse set of high-quality candidate solutions in CO problems. Technically, we have developed problem-specific MDPs and efficient learning algorithms to address the challenges associated with learning from long trajectories, making our approach practical and scalable in large-scale setups. Our extensive numerical results showcase the effectiveness and efficiency of GFlowNets in solving NP-hard level CO problems, highlighting their ability to generate a set of diverse high-quality solutions. We believe that our work opens up new possibilities for addressing the limitations of existing approaches and paves the way for future research at the intersection of machine learning and combinatorial optimization.