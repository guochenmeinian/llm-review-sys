# On Certified Generalization in Structured Prediction

Bastian Boll

Image & Pattern Analysis Group

Heidelberg University

bastian.boll@iwr.uni-heidelberg.de &Christoph Schnorr

Image & Pattern Analysis Group

Heidelberg University

schnoerr@math.uni-heidelberg.de

###### Abstract

In structured prediction, target objects have rich internal structure which does not factorize into independent components and violates common i.i.d. assumptions. This challenge becomes apparent through the exponentially large output space in applications such as image segmentation or scene graph generation. We present a novel PAC-Bayesian risk bound for structured prediction wherein the rate of generalization scales not only with the number of structured examples but also with their size. The underlying assumption, conforming to ongoing research on generative models, is that data are generated by the Knothe-Rosenblatt rearrangement of a factorizing reference measure. This allows to explicitly distill the structure between random output variables into a Wasserstein dependency matrix. Our work makes a preliminary step towards leveraging powerful generative models to establish generalization bounds for discriminative downstream tasks in the challenging setting of structured prediction.

## 1 Introduction

### Overview

_Structured prediction_ is the task of predicting an output which itself contains internal structure. As an example, consider the problem of image segmentation. The output to be predicted is an assignment of semantic classes to each image pixel. However, the segmentation problem is not merely a pixelwise classification, because each pixel is not independently assigned a semantic class. If two pixels are adjacent in the image plane, it is more likely that they belong to the same class than to different ones. A way to remedy this problem is to enumerate all possible segmentations of an image and treat the problem as classification. However, the output space of this classification is now exponentially large. This exponential (in the number of pixels) size of the output space indicates a much more difficult problem than the unstructured case of classification. In fact, it also presents an immediate challenge for any statistical learning theory which requires assumptions on the number of output classes .

From a practical perspective, structured prediction problems present a challenge of label aquisition. For instance, dense manual segmentation of an image is significantly more labour intensive than manual classification. In turn, one would expect that pixelwise segmentation of an image contains much richer information to be exploited in supervised learning. However, this is not represented in typical statistical learning theory which assumes all data to be independently drawn from the true underlying distribution. In the extreme case of only a single structured example being available for training, these statistical learning theories can not make any meaningful statement on generalization.

Addressing this point in particular,  presents an analysis of the dependency structure in the output and proves a risk certificate which decays in both the number of structured examples \(m\)_and their size \(d\)_. Refering back to the example of image segmentation, this amounts to a high-probability bound on the fraction of misslabeled pixels which _decays with the number of labeled pixels_ observed during training as opposed to merely the number of segmented images.

At the core of statistical learning theory lies the concentration of measure phenomenon which posits that a stable function of a large number of weakly dependent random variables will take values close to its mean [37; 9]. This is relevant because model risk, the expected loss on unseen data, is the mean of empirical risk under the draw of the sample. Learning theories can thus be built on concentration of measure results. In the present work, we focus on PAC-Bayesian learning theory [51; 43; 42]. For an overview of PAC-Bayesian theory we refer to [13; 27; 1].

In addition to a concentration result such as a moment-generating function (MGF) bound, PAC-Bayesian arguments employ a change of measure, typically via Donsker and Varadhan's variational formula. In particular, stochastic predictors, i.e. distributions over a hypothesis class of models are considered. A core objective is to construct generalization bounds which hold uniformly over all stochastic predictors called _PAC-Bayes posteriors_. Model complexity is then measured as relative entropy to a reference stochastic predictor called _PAC-Bayes prior_. As this terminology alludes to, the PAC-Bayes posterior is informed by more data than the PAC-Bayes prior. Note however, that PAC-Bayesian theory generalizes Bayesian theory because prior and posterior are not typically connected via likelihood.

PAC-Bayesian theory has gained considerable attention in recent years due to the demonstration of non-vacuous _risk certificates in deep learning_. Since then, a line of research has succeeded in tightening the risk bounds of deep classifiers [48; 47; 16]. In addition, multiple authors have studied ways to weaken underlying assumptions of bounded loss [29; 28] and i.i.d. data .

The majority of recent works in this field focuses on classification or regression. _Structured_ prediction has received comparatively little recent attention, an exception being the work  which provides a PAC-Bayesian perspective on the implicit loss embedding framework for structured prediction .

### Related Work

Here, we continue a line of research started by [39; 40; 38] which aims to construct PAC-Bayesian risk bounds for structured prediction that account for generalization from a single (but large) structured datum. Instrumental to their analysis is the stability of inference and quantified dependence in the data distribution. The latter is expressed in terms of \(\)-mixing coefficients, the total variation distance between data distributions conditioned on fixed values of a subset of variables. For structured prediction with Hamming loss, a coupling of such conditional measures can be constructed  such that \(\)-mixing coefficients yield an upper-bound that allows to invoke concentration of measure arguments . The result is an MGF bound which the authors employ in a subsequent PAC-Bayesian construction - achieving generalization from a single datum.

The underlying assumption of these previous works is that data are generated by a Markov random field (MRF). This model assumption is somewhat limiting because Markov properties, certain conditional independences, likely do not hold for many real-world data distributions. In addition, MRFs are difficult to work with computationally. Exact inference in MRFs is NP-hard  and thus learning, which often contains inference as a subproblem, presents significant computational roadblocks. Even once an MRF has been found which represents data reasonably well, numerical evaluation of the PAC-Bayesian risk certificate proposed in  will again be computationally difficult.

Nevertheless, we share the sentiment of previous authors that some assumption on the data-generating process is required in structured prediction. This is because conditional data distributions, the distribution of data conditioned on a fixed set of values for a subset of variables, are central to establishing concentration of measure via the martingale method . Consider again the example of image segmentation. Once we have fixed a sufficiently large number of pixels to arbitrary values (and class labels), even a large dataset will not contain an abundance of data which match these values and thus provide statistical power to learn the conditional distribution. This problem is well-known in conditional density estimation .

### Contribution, Organization

In this work, we propose to instead assume a triangular and monotone transport, a _Knothe-Rosenblatt (KR) rearrangement_[32; 49; 12; 8; 41] of a reference measure as data model. This choice is attractive for multiple reasons. First, any data distribution which does not contain atoms can be representeduniquely in this way  which should suffice to represent many distributions of practical interest. With regard to conditional distributions, the KR-rearrangement has the convenient property that conditioning on a fixed value for a subset of variables can again be represented by KR-rearrangement. We will use this property in our construction of coupling measures between conditional distributions.

Specifically,

* We present a novel PAC-Bayesian risk bound for structured prediction wherein the rate of generalization scales not only with the number of structured examples but also with their size.
* Based on data generated by KR-rearrangement of a tractable reference measure, we distill relevant structure of the data distribution into a Wasserstein dependency matrix. Our analysis hinges on state-of-the-art results in concentration theory  which serve to bound moment-generating functions by properties of the Wasserstein dependency matrix. We subsequently invoke a PAC-Bayesian argument to derive the desired risk certificate.
* We also propose to leverage a construction of bad input data as a computational tool to find entries of the Wasserstein dependency matrix.

We stress the fact that many established approaches to generative modelling can be seen as instances of measure transport. For instance, it includes normalizing flows , diffusion models , generative adversarial networks and variational autoencoders . While most measure transport models which currently enjoy empirical success are not KR-rearrangements, we hope that the methods presented here can lay the foundation of leveraging powerful generative models to build risk certificates for discriminative downstream tasks.

OrganizationThe central concepts of concentration theory and measure transport are described in the preliminary Sections 2 and 3. The main results are presented in Section 4. In Section 5 we present a first discussion of computational aspects related to the presented framework. The paper closes on a discussion of limitations in Section 6 and a conclusion in Section 7.

Basic NotationFor any \(d\), denote \([d]=\{1,,d\}\). If \(z^{d}\) is a vector, we refer to the subvector of entries with index in a set \(I[d]\) as \(z^{I}\). In particular, index sets of interest will be half-open and closed intervals \((i,d][d]\) and \([i,d][d]\). Analogously, we will index the output of vector-valued functions \(f^{I}\) and marginal measures \(^{I}\). For a set \(^{d}\), we denote its complement in \(^{d}\) by \(^{c}=^{d}\) and for a measure \(\) on \(^{d}\), we denote the conditional measure given \(^{c}\) as \(|^{c}\). If \(Z\) is a random variable with distribution \(\) on \(^{d}\) and \(I,J[d]\) are disjoint index sets with \(I J=[d]\), we denote the conditional law of \(Z^{I}\) given \(Z^{J}=z^{J}\) as \((dz^{I}|z^{J})\).

## 2 Concentration of Measure and Generalization

Let \(\) denote an input space and \(\) denote an output space. Let \(\) be a distribution on \(^{d}=()^{d}\). There are two restrictions inherent to this setup. First, an input is always paired with an output and thus the number of inputs needs to match the number of outputs. Second, all structured data will be drawn from \(\) and thus the size of each structured datum will be the same. Otherwise, \(\) and \(\) can in principle be arbitrary sets which admit metrics. For concreteness, think of \(=\) as being a set of gray values and \(=\) containing signed distances from a semantic boundary  in an image with \(d\) pixels. In this case, \(^{d}\) contains all binary segmentations of grayvalue images.

The goal of learning is to find parameters \(\) which define a predictor \(_{}^{d}^{d}\) such that the _risk_

\[()=_{(X,Y)}[L(_{}(X),Y)]\] (1)

with respect to some loss function \(L^{d}^{d}\) is minimized. However, the true risk (1) is typically intractable because \(\) is unknown. A related tractable quantity is the _empirical risk_

\[_{m}(,_{m})=_{k[m]}L(_{ }(X^{(k)}),Y^{(k)})\] (2)of a sample \(_{m}=(X^{(k)},Y^{(k)})_{k[m]}\) drawn from \(^{m}\). We further assume that the loss of structured outputs is the mean of bounded _pointwise_ loss \(\)

\[L(^{(k)},y^{(k)})=_{i[d]}(^{ (k)}_{i},y^{(k)}_{i})\;.\] (3)

In PAC-Bayesian constructions, we consider stochastic predictors \(\), i.e. measures on a hypothesis space \(\) of predictors \(_{}^{d}^{d}\) as identified with measures on the underlying parameter space from which \(\) is selected. We then take the above notions of risk and empirical risk in expectation over parameter draws

\[()=_{}[()], _{m}(,_{m})=_{}[_{m}(,_{m})]\;.\] (4)

The expected value of empirical risk with respect to the sample is the true risk. For this reason, a central tool for the study of generalization is the _concentration of measure_ phenomenon. Informally, it states that a stable function of many weakly dependent random variables concentrates on its mean. We will invoke a line of reasoning put forward in  and propose a novel approach to structured prediction based on the measure-transport framework outlined in Section 1. To this end, we first define the following formal notions of _stability_ and _dependence_.

Let \(\) be a metric such that \(\) has finite diameter

\[\|\|=_{,^{}}(,^{})<\] (5)

and let \(^{d}(z,z^{})=_{i[d]}(z_{i},z^{}_{i})\) denote the corresponding product metric on \(^{d}\).

**Definition 1** (Local oscillation).: _Let \(f^{d}\) be Lipschitz with respect to \(^{d}\). Then the quantities_

\[_{i}(f)=_{z,z^{}^{d},z^{}_{[d] \{i\}}=z_{[d]\{i\}}})|}{(z_{i},z^{ }_{i})}, i[d]\] (6)

_are called the local oscillations of \(f\)._

The vector of local oscillations gives a granular account of stability. In order to discuss interdependence of data in a probability space \((^{d},,)\), define the Markov kernels

\[K^{(i)}(z,dw)=_{z^{(i-1]}}(dw^{[i-1]})^{[i,d]}(dw^{[i,d]}|z^{ [i-1]}), i[d]\] (7)

as well as \(K^{(d+1)}(z,dw)=_{z}(dw)\) and their action on functions

\[K^{(i)}f(z)= f(y)K^{(i)}(z,dw)= f(z^{[i-1]}w^{[i,d]})^{[i,d]}(dw^{[ i,d]}|z^{[i-1]})\] (8)

where in the edge case \(i=1\), the condition on \(z^{[i-1]}=z^{\{\}}\) is removed. Here \(K^{(i)}(z,dw)\) is a Borel measure for every \(z\) and (8) computes the expected value of \(f\) at \(z\), conditioned on the fixed realization of the subvector \(z^{[i-1]}\). It turns out that the effect of the kernel (7) on local oscillations serves to quantify dependence of data with joint distribution \(\).

**Definition 2** (Wasserstein matrix).: _For \(i[d+1]\), let \(K^{(i)}\) denote the Markov kernel (8). A matrix \(V^{(i)}_{ 0}^{d d}\) is called a Wasserstein matrix  for \(K^{(i)}\), if_

\[_{k}(K^{(i)}f)_{j[d]}V^{(i)}_{kj}_{j}(f), \,k[d]\] (9)

_for any function \(f^{d}\) which is Lipschitz with respect to \(^{d}\)._

The two concepts defined above will be used in Section 4 to construct a moment-generating function bound via the martingale method.

Triangular Measure Transport

Suppose a structured output is composed of \(d>0\) unstructured data in a space \(\). Then the target measure \(\) of interest is a measure on \(^{d}\) which does not factorize into simpler distributions. A popular method of representing complex joint distributions of interdependent random variables is to define a map \(T^{d}^{d}\) which transports a tractable _factorizing_ reference measure \(^{d}\) to the target measure \(\), i.e. \(T_{}^{d}=\). This abstract framework encompasses many generative models such as normalizing flows [54; 53; 33; 46; 50], diffusion models [52; 30], generative adversarial networks and variational autoencoders [10; 26]. Here, we focus on transport maps \(T\) which are monotone and triangular in the sense that \(T(z)_{i}\) only depends on the inputs \(z^{[i]}\) and each \(T(z^{[i-1]},)_{i}\) is an increasing function. Such a map is called a _Knothe-Rosenblatt (KR) rearrangement_[32; 49; 12; 8; 41]. If both \(^{d}\) and \(\) have no atoms then the KR rearrangement exists and is unique . In particular, normal distribution \(^{d}\) and any absolutely continuous (with respect to the Lebesgue measure) distribution \(\) meet these criteria. The KR rearrangement has the useful property that certain conditional distributions have a simple representation.

**Lemma 3** (Lemma 1 of ).: _Let \(T^{d}^{d}\) be the KR-rearrangement which satisfies \(T_{}^{d}=\). For arbitrary \(i[d]\), let \(z^{[i]}^{i}\) be fixed. Then_

\[(dw^{(i,d]}|z^{[i]})=T^{(i,d]}(^{[i]},)_{}^{d-i}\] (10)

_where \(^{[i]}\) is the unique element of \(^{i}\) such that \(T^{[i]}(^{[i]})=z^{[i]}\)._

Numerical realization of KR rearrangements has recently received attention  and more broadly, a variety of triangular transport architectures exists [20; 21]. However, we do not focus on numerical considerations in the present theoretical work.

One may wonder if data generated by KR-rearrangement implicitly restricts the choice of possible input and output spaces since the monotonicity requirement on \(T\) can only be satisfied if the underlying set \(\) is ordered. However, many feature spaces of practical interest are still permissible for what follows. In particular, both \(\) and \(\) may be Euclidean spaces or hypercubes. Inkeeping with the introductory image segmentation example, suppose \(=^{3}\) contains RGB color values and \(=[-1,1]\) contains signed distance from a semantic boundary in the image plane. Then we can interpret \(^{d}=(^{3}[-1,1])^{d}\) as a product of compact intervals in \(^{4d}\) which is clearly permissible as underlying space for KR-rearrangement. All presented results hold irrespective of whether \(\) contains such internal dimensions as long as a natural ordering of the set \(\) is induced.

## 4 PAC-Bayesian Risk Certificate

In this section we present a novel PAC-Bayesian risk bound for structured prediction which combines three main ingredients.

1. A concentration of measure theorem for dependent data (Theorem 4) which builds on the notion of a Wasserstein dependency matrix;
2. a simple construction of coupling measures between conditional distributions (Lemma 5) which serves to represent the Wasserstein dependency matrix;
3. a PAC-Bayesian argument (Theorem 7) employing Donsker-Varadhan's variational formula in concert with concentration of measure results.

The first theorem summarizes key results from  on the concentration of measure phenomenon for dependent random variables. We have slightly generalized by augmenting the underlying Doob martingale construction with the inclusion of a set \(\) of _bad inputs_. For inputs in this set, data stability requirements do not necessarily hold. We call the complement \(^{c}=^{d}\) the set of _good inputs_. The concept of good and bad inputs as well as related proof techniques were originally proposed by . Here, we incorporate them into the more general concentration of measure formalism of . A full proof of the following theorem is deferred to Appendix A.

**Theorem 4** (**Moment-generating function (MGF) bound for good inputs)**.: _Let \(^{d}\) be a measurable set of bad inputs. Suppose for each \(i[d+1]\), \(V^{(i)}\) is a Wasserstein matrix for the _Markov kernel \(K^{(i)}\) defined in (7) on the set of good inputs, that is_

\[_{k}(K^{(i)})_{j[d]}V^{(i)}_{kj}_{j}( ),\,k[d]\] (11)

_for all Lipschitz (with respect to \(^{d}\)) functions \(^{c}\). Define the Wasserstein dependency matrix_

\[^{d d},_{ij}=\|\|V^{(i+1)}_{ij}\] (12)

_Then for all Lipschitz functions \(f^{d}\), the following MGF bound holds_

\[_{z|^{c}}[((f(z)- _{|^{c}}f))](}{8}\|(f)\|_{2}^{2})\,.\] (13)

An upper bound on the moment generating function will be used in the PAC-Bayesian argument concluding this section. The function \(f\) in question will be the loss of a structured datum \(z\). Regarding (13), our goal is to bound the norm \(\|(f)\|_{2}^{2}\) through properties of the data distribution. We will use the fact that data is represented by measure transport to establish such a bound after the following preparatory lemma.

**Lemma 5** (**Coupling from transport**).: _Let \(^{d}\) be a reference measure on \(^{d}\) and \(F,G^{d}^{d}\) be measurable maps. Define the map \((F,G)\) by_

\[(F,G)^{d}^{d}^{d}, z (F(z),G(z))\] (14)

_Then \((F,G)_{}^{d}\) is a coupling of \(F_{}^{d}\) and \(G_{}^{d}\)._

Proof.: Let \(A^{d}\) be measurable, then

\[(F,G)_{}^{d}(A,^{d})=^{d}((F,G)^{-1}(A,^{d} ))=^{d}(F^{-1}(A))=F_{}^{d}(A)\] (15)

which shows that \(F_{}^{d}\) is the first marginal of \((F,G)_{}^{d}\). An analogous argument for the second marginal shows the assertion. 

By assuming \(\) to be represented via KR-rearrangement of a factorizing reference measure, Lemma 3 gives an explicit representation of KR-rearrangement for conditional distributions. From there, we invoke Lemma 5 to construct a coupling between conditional distributions and subsequently follow a line of reasoning put forward in  to explicitly construct Wasserstein matrices for the kernels (7) which yield a bound on (13) by Theorem 4. This leads to the following proposition. A full proof is deferred to Appendix A.

**Proposition 6** (**Wasserstein dependency matrix from KR-rearrangement**).: _Let \((^{d},,)\) be a probability space with \(=T_{}^{d}\) for the KR-rearrangement \(T^{d}^{d}\) and a reference measure \(^{d}\) on \(^{d}\). Let each \(\) be equipped with a metric \(\) and have finite diameter \(\|\|<\). Let \(f^{d}\) be a Lipschitz function with respect to the product metric \(^{d}\). Let \(^{d}\) denote a set of bad inputs and define the corresponding set \(=T^{-1}()^{d}\). Let \(\) be the unique KR-rearrangement that satisfies \(_{}^{d}=^{d}|^{c}\) and denote \(=T\). Suppose there exist constants \(L_{ij}\) such that for all \(v,z^{c}\) with \(v^{|d\{i\}}=z^{[d]\{i\}}\) it holds_

\[_{^{(i,d]}}[(^{(i,d]}(^{ [i]},)_{j},^{(i,d]}(^{[i]},)_{j})] L _{ij}(v_{i},z_{i})\] (16)

_where \(^{[i]}\) and \(^{[i]}\) are uniquely defined through \(^{[i]}(^{[i]})=v^{[i]}\) and \(^{[i]}(^{[i]})=z^{[i]}\). Then \(=D\) is a Wasserstein dependency matrix for \(|^{c}\) with_

\[D_{ij}=0&i>j\;,\\ 1&i=j\;,\\ L_{ij}&i<j\;.\] (17)

We remark that \(\) indeed distills the dependency structure of \(\). To illustrate this, let \(^{\{i\}}\) be independent from \(^{\{j\}}\) for some \(i[d]\), \(j(i,d]\). Then conditioning on a different value of \(^{\{i\}}\) does not change the distribution \(^{\{j\}}\). Thus,

\[^{(i,d]}(^{[i]},)_{j}=^{(i,d]}( ^{[i]},)_{j},\,^{(i,d]}, ^{[d]\{i\}}=^{[d]\{i\}}, j( i,d]\] (18)

and the choice \(L_{ij}=0\) satisfies (16). It directly follows that \(_{ij}=0\).

The following theorem states the main result of the present work.

**Theorem 7** (PAC-Bayesian risk certificate for structured prediction).: _Fix \((0,(-e^{-1}))\), let \(\) be a data distribution on \(^{d}\) with \(T_{}^{d}=\) the Knothe-Rosenblatt rearrangement for a reference measure \(^{d}\) on \(^{d}\) and fix a measurable set \(^{d}\) of bad inputs with \(()\). Fix a PAC-Bayes prior \(\) on a hypothesis class \(\) of functions \(^{d}^{d}\) and a loss function \(\) which assumes values in \(\). Define the oscillation vector \(\) by_

\[_{i}=_{h}_{i}L(h,) _{^{c}}, i[d]\] (19)

_where \(L(h,)_{^{c}}\) denotes the restriction of \(L(h,)\) to \(^{d}\). Suppose all oscillations \(_{i}\) are finite, suppose the condition (16) is satisfied and denote by \(D\) the matrix with entries (17). Then, with probability at least \(1-\) over realizations of a training set \(_{m}=(Z^{(k)})_{k=1}^{m}\) drawn from \((|^{c})^{m}\) it holds for all PAC-Bayes posteriors \(\) on \(\) that_

\[()_{m}(,_{m})+2 {d}\|D\|_{2}+[ :]}{2m}}+\.\] (20)

Note that the generalization gap on the right hand side of (20) decays with \(d\). This accounts for generalization from a _single_ example. In fact, if only \(m=1\) structured example is available, but \(d 1\), Theorem 7 still certifies risk. This effect can however be negated by the norm \(\|D\|\). If structured data contain strong global dependence, then \(\|D\|\) will not be bounded independently of \(d\) and thus, in the worst case of \(\|D\|(d)\) the assertion is no stronger than PAC-Bayesian bounds for unstructured data. The same point was observed in .

The measure of the bad set under the data distribution \(\) is assumed to be bounded by \(\). This is to account for a small number of data which contain strong dependence. In order to prevent these bad data from dominating \(D\), thereby negating the decay of the bound in \(d\) as described above, it is preferable to exclude them from the sample, reduce the sample size \(m\) and pay the penalty \(\) in (20).

To prove Theorem 7, we broadly follow the argument put forward in . This augments typical PAC-Bayesian constructions in the literature by the inclusion of a set of bad inputs. We first reconcile the data being conditioned on \(^{c}\) with risk certification for unconditioned data, leading to the addition of \(\) on the right hand side of (20). The model complexity term \([:]\) is due to Donsker and Varadhand's variational formula. Subsequently, the moment generating function bound of Theorem 4 is instantiated through the Wasserstein dependency matrix constructed in Proposition 6. Markov's inequality then gives a pointwise risk bound for fixed value of a free parameter. In order to optimize this parameter, the bound is made uniform on a discrete set of values through a union bound. A full proof is presented in Appendix A.

In Theorem 7, we combine the PAC-Bayesian construction of  with the more general concentration of measure theory of . Crucially, concentration of measure results used in  are predicated on the assumption of data generated by a Markov random field . Our work is more flexible in two major ways.

1. Our assumption on the data-generating distribution is likely more representative of real-world data as measure transport models have repeatedly been shown to yield convincing data generators.
2. Markov random fields are difficult to handle computationally because inference in general Markov random fields is NP-hard  such that one is forced to learn based on approximate inference procedures .

Our work also allows for more general metrics \(\) as opposed to the singular choice of Hamming norm required in . Additionally, the key results of  are constructed to ensure all data drawn from the unconditioned distribution \(\) are in the good set. This reduces the probability of correctness \(1-\) by \(m\). Instead, we assume data drawn from \(|^{c}\), effectively reducing the number of available samples by a factor of \(1-\), but keeping the probability of correctness high. This allows the set of bad inputs to be used more effectively as a computational tool in Section 5.

Comparing the dependency of (20) on \(d\) with the respective result in , it first appears as though our bound decays with a faster rate (\(d\) instead of \(\)). However, this will not typically be the case in practice because \(\|D\|_{2}\) grows with rate \(\) in most situations. To see this, consider the case of localdependency in the sense that

\[L_{ij}=1,j_{i}\;,\\ 0,\] (21)

for local neighborhoods \(_{i}[d]\) which contain a fixed number of \(c\) elements and let \(_{i}=\) for all \(i[d]\) and some constant value \(>0\). Then

\[\|D\|_{2}=|_{i} |^{2}}=c\;.\] (22)

Clearly, if dependence is localized and the oscillations \(\) do not decay in \(d\), then \(\|D\|_{2}\) contains a factor that grows with rate \(\), leading to the same asymptotic rate of (20) already observed in .

Note that  additionally allows for a set of bad hypotheses \(_{}\) which do not conform to stability assumptions. In our construction, this means restricting the bound (19) to oscillations on the set of good hypotheses. We omit this extension for clarity of exposition, but do not expect it to necessitate major changes to the presented proofs. The same applies to the derandomization strategy proposed by  which is based on hypothesis stability.

Further,  considers a large number of applicable orderings for random variables by introducing a filtration of their index set. This notion is not easily compatible with our assumption of KR-rearrangement, because triangularity of transport depends on the order of variables.

## 5 Bounding the Bad Set

With regard to numerical risk certificates, a key technical aspect of Section 4 concerns the quantities \(L_{ij}\) in (16). Here, we propose a way to use the set of bad inputs as a computational tool to this end. Suppose we assign arbitrary fixed values to \(L_{ij}\) and subsequently _define_\(^{d}\) as the set of inputs on which the condition (16) fails. Then we have fulfilled the prerequisites of Proposition 6 by construction and are left with bounding \(()\). Note that

\[()=_{Z}(Z)=_{Z} [\{Z\}]\] (23)

and the indicator function \(\) assumes values in the bounded set \(\{0,1\}\). Therefore, Hoeffding's inequality gives the following.

**Proposition 8** (**Upper bound on the bad set)**.: _Let \(\) be a data distribution and let \(}_{n}^{n}\) be a sample of size \(n\). Fix an error probability \((0,1)\). Then_

\[()_{Z}_{n}}\{Z\}+}\] (24)

_with probability at least \(1-\) over the sample._

Checking the condition \(Z\) requires evaluating (16) which comes down to finding a Lipschitz constant for a one-dimensional function. If this is computationally feasible for the given data model, then the concentration argument (24) can be used to bound \(()\) with high probability. Because (24) decays only in the number of structured examples \(()\), it can not be used to show generalization from a single structured example. However, PAC-Bayesian risk certificates are typically dominated by the KL complexity term in (20) which decays with the size of structured examples as well. Thus, Proposition 8 should still be useful in practice.

Note that Proposition 8 makes a pointwise statement about a fixed value of \(L_{ij}\) which has limited utility for learning \(L_{ij}\) from data. To remedy this problem, we can first define a discrete set \(=(L^{(k)})_{k[l]}\) of candidate matrices and select error probabilities \(_{k}\) for each of the events

\[((L^{(k)}))_{Z}_{n} }\{Z(L^{(k)})\}+}}\] (25)

such that \(_{k[l]}_{k}=\). Then, by a union bound with probability at least \(1-\) over the sample none of the events (25) occurs. We have thus constructed a uniform bound over the set of candidate matrices which allows us to select the one which minimizes the generalization gap in (20).

In order to make this strategy most effective, domain knowledge on the application at hand should be applied when constructing candidate matrices and assigning error probabilities. For instance, the limited empirical findings of  on ImageNet  indicate that the majority of natural images contain mostly local signal. In image segmentation, this is conducive to concentration, because it can lead to many small values in an optimal Wasserstein dependency matrix. In particular, if dependency decays with distance in the image domain, one should select configurations \(L^{(k)}\) in which \(L^{(k)}_{ij}\) is small if \(i\) is distant from \(j\) in the image domain and allowed to assume larger values if \(i\) is close to \(j\) in the image domain.

We give an _intuitive interpretation_ of the relationship between Proposition 8 and Theorem 7 as follows. Suppose the majority of samples from a structured data distribution contain mostly local signal. The locality of signal in samples indicates weak global dependence of random variables which in turn manifests in small entries of a Wasserstein dependency matrix. However, a small number of bad data may contain only weak local signal. For instance, an image in which every pixel has the same value does not give more information to a learner if it is doubled in size. Even worse, a small (but not null) set of bad data will dominate the Wasserstein dependency matrix and prevent generalization that scales with \(d\). Proposition 8 thus estimates an upper bound on the likelihood of bad data under \(\) which are then excluded from the concentration argument underlying the bound (20). This relationship is further illustrated in a numerical toy example in Appendix C.

## 6 Limitations

We assume that structured data are drawn from a distribution \(\) which arises from a reference measure through KR-rearrangement. This is motivated by the fact that a large class of data distributions can be represented in this way and that some assumption on the data-generating distribution is required in order to make statements on conditional distributions required to quantify dependence. However, it is an open question how closely a measure transport distribution learned from data approximates the unknown distribution from which the data are drawn. A first step towards this goal is made in the recent work , but a non-asymptotic theory of generalization for generative models is left for future work. Accordingly, we do not present any empirical results on real-world data.

We describe how dependency in the data distribution which is relevant for generalization of discriminative models can be written as properties of the KR-rearrangement through a Wasserstein dependency matrix. Section 5 further outlines how the construction of bad data can be used as a computational tool to this end. We do not cover how to compute the involved Lipschitz constants \(L_{ij}\) in (16). In practice, this will require nontrivial numerical machinery because a deterministic bound on the expected value under the reference distribution appears needed. One possible approach is to employ quasi-Monte-Carlo methods [19; 18] which come with deterministic error bounds and have recently been applied to PAC-Bayesian certification of ordinary (non-structured) classification risk . This entails a fine-grained stability analysis of the involved integrands and is likely to be computationally expensive because deterministic error bounds tend to be pessimistic compared to their stochastic counterparts.

## 7 Conclusion

We have presented a PAC-Bayesian risk certificate for structured prediction. Compared to earlier work, we make the assumption of data generated by KR-rearrangement of a reference measure. This approach is able to represent all atom-free data distributions and yields an explicit quantification of dependence via Wasserstein dependency matrices. It also indicates a way of leveraging powerful generative models to compute risk certificates for downstream discriminative tasks.