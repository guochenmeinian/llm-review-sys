# LLM-Pruner: On the Structural Pruning

of Large Language Models

Xinyin Ma  Gongfan Fang  Xinchao Wang

National University of Singapore

maxinyin@u.nus.edu, gongfan@u.nus.edu, xinchao@nus.edu.sg

Corresponding author

###### Abstract

Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely _3 hours_, requiring only _50K_ data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner

## 1 Introduction

Recently, Large Language Models (LLMs)  have demonstrated remarkable proficiency in language understanding and generation. With the increase in model size, they are better equipped to handle complex tasks  and even exhibit emergent abilities . However, notwithstanding their impressive performance, LLMs pose challenges in deployment and inference. Their extensive scale engenders substantial computational demands, and the multitude of parameters involved can induce long latencies and other related issues. Several techniques are proposed to solve these problems, like model pruning , knowledge distillation ,quantization  within the context of pre-trained language model (PLM).

While previous methods have effectively maintained model performance amidst parameter reduction, they primarily target compression within specialized domains or for designated tasks in the context of task-specific compression. For instance, a PLM is fine-tuned on a particular dataset, such as one of the classification tasks in the GLUE benchmark , after which these models are distilled into a smaller classification model . Although this paradigm could potentially be employed for LLM compression, it compromises the LLM's capacity as a versatile task solver, rendering it suited to a single task exclusively.

Thus, we strive to compress the LLM in a new setting: to reduce the LLM's size while preserving its diverse capabilities as general-purpose task solvers, as depicted in Figure 1. This introduces the task-agnostic compression of LLMs, which presents two key challenges:

* **The size of the training corpus of the LLM is enormous.** Previous compression methods heavily depend on the training corpus. The LLM has escalated the corpus scale to 1 trillion tokens or more . The extensive storage needs and protracted transmission times make the dataset difficult to acquire. Furthermore, if the dataset is proprietary, acquisition of the training corpus merges on impossibility, a situation encountered in .
* **The unacceptably long duration for the post-training of the pruned LLM.** Existing methods require a substantial amount of time for post-training the smaller model . For instance, the general distillation in TinyBERT takes around 14 GPU days . Even post-training a task-specific compressed model of BERT demands around 33 hours . As the size of both the model and corpus for LLMs increases rapidly, this step will invariably consume an even more extensive time.

To tackle the aforementioned challenges associated with the task-agnostic compression of LLMs, we introduce a novel approach called LLM-Pruner. Since our goal is to compress LLMs with reduced data dependency and expedited post-training, how to prune model with the minimal disruption to the origin is crucial. To accomplish this, we propose a dependency detection algorithm that identifies all the dependent structures within the model. Once the coupled structure is identified, we employ an efficient importance estimation strategy to select the optimal group for pruning under the task-agnostic setting, where the first-order information and an approximated hessian information is taken into account. Finally, a rapid recovery stage is executed to post-train the pruned model with limited data.

Contribution.In this paper, we propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. To the best of our knowledge, LLM-Pruner is the first framework designed for structured pruning of LLMs. We conclude the advantages of the LLM-Pruner as (i) Task-agnostic compression, where the compressed language model retains its ability to serve as a multi-task solver. (ii) Reduced demand for the original training corpus, where only 50k publicly available samples are needed for compression, significantly reducing the budget for acquiring the training data (iii) Quick compression, where the compression process ends up in three hours. (iv) An automatic structural pruning framework, where all the dependent structures are grouped without the need for any manual design. To evaluate the effectiveness of LLM-Pruner, we conduct extensive experiments on three large language models: LLaMA-7B, Vicuna-7B, and ChatGLM-6B. The compressed models are evaluated using nine datasets to assess both the generation quality and the zero-shot classification performance of the pruned models. The experimental results demonstrate that even with the removal of 20% of the parameters, the pruned model maintains 94.97% of the performance of the original model.

Figure 1: Illustration of LLM-Pruner. (i) Task-specific compression: the model is fine-tuned then compressed on a specific task. (ii) TinyBERT: First distill the model on unlabeled corpus and then fine-tune it on the specific task. (iii) LLM-Pruner: Task-agnostic compression within 3 hours.

Related Work

Compression of Language Model.Language models [9; 31; 25] have gained much attention and increase the need to reduce the size of parameters and reduce the latency [23; 48]. To compress the language model, previous works can be divided into several categories: network pruning [21; 63; 32; 15], knowledge distillation [46; 47; 40], quantization [68; 1; 71] and other techniques, like early exit  or dynamic token reduction . We focus on the pruning of the language models, especially structural pruning . Structural pruning removes the entire filter from the neural network, which is more hardware friendly. There are several ways to remove the structure, such as l1-dependent pruning [16; 72], first-order importance estimation , hessian-based estimation [21; 54] or the optimal brain surgeon [24; 21]. As for the pruning unit in structural pruning, some works adopt the entire layer  as the minimal unit, and others take the multi-head attention  or the feed-forward layers [18; 36] as the basic structure to prune. CoFi  studies the pruning unit in different granularity.

Efficient and Low Resource Compression.With the growing size of neural network models, there is an increasing demand for efficient and low-resource compression [67; 66; 30; 29; 65]. As for the efficient compression,  accelerate the post-training by defining the reconstruction error as a linear least squares problem. [13; 12] propose the layer-wise optimal brain surgeon. As for the constraint of availability of the training corpus, data-free pruning [45; 70] come up with several strategies to prune the model by measuring neurons' similarity. Besides, [34; 33; 42] proposes methods that distill the model without reliance on the training corpus of the model. However, those methods are too time-consuming, involving synthesizing samples by backpropagating the pre-trained models.

## 3 Methods

In this section, we provide a detailed explanation of LLM-Pruner. Following the conventional model compression pipeline, LLM-Pruner consists of three steps: **(1) Discovery Stage** (Section 3.1). This step focuses on identifying groups of interdependent structures within LLMs. **(2) Estimation Stage** (Section 3.2). Once the coupled structures are grouped, the second step entails estimating the contribution of each group to the overall performance of the model and deciding which group to be pruned. **(3) Recover Stage** (Section 3.3). This step involves fast post-training that alleviates potential performance degradation caused by the removal of structures.

### Discover All Coupled Structure in LLMs

In light of the limited availability of data for post-training, it becomes imperative to prioritize the removal of structures with minimal damage when compressing the model. This underscores the dependency-based structural pruning, which ensures coupled structures are pruned in unison. We provide an experiment in Section 4.3 to show the importance of dependency-based structural pruning when compressing the large language model.

Structure Dependency in LLMs.Similar to , the pruning begins by building the dependency for LLMs. Assume \(N_{i}\) and \(N_{j}\) are two neurons in the model, \((N_{i})\) and \((N_{i})\) represents all the neurons that point towards or point from \(N_{i}\). The dependency between structures can be defined as:

\[N_{j}(N_{i})^{-}(N_{j})=1 N_{j} N_{i}\] (1)

where \(^{-}(N_{j})\) represents the in-degree of neuron \(N_{j}\). Noting that this dependency is directional, we can therefore correspondingly obtain another dependency:

\[N_{i}(N_{j})^{+}(N_{i})=1 N_{i} N_{j}\] (2)

where \(^{+}(N_{i})\) represents the out-degree of neuron \(N_{i}\). The principle of dependency here is, if a current neuron (e.g., \(N_{i}\)) depends solely on another neuron (e.g., \(N_{j}\)), and the neuron \(N_{j}\) is subjected to pruning, it follows that the neuron \(N_{i}\) must also undergo pruning. We provide a detailed case in Appendix A.

Trigger the Dependency Graph.By having the definition of dependency, the coupled structures in the LLM can be analyzed automatically. Considering any neuron within the LLM as the initial trigger, it possesses the capability to activate neurons that depend on it. Subsequently, these newly triggered neurons can serve as the subsequent triggers to identify the dependency and activate their respective dependent neurons. This iterative process continues until no new neurons are detected. Those neurons then form a group for further pruning. Taking LLaMA as an example, by searching over all the neurons as the initial trigger, we can locate all the coupled structures, as shown in Figure2.

Given the diversity in the structure of different LLMs, manual analysis and removal of coupled structures in each LLM could be extremely time-consuming. However, by employing LLM-Pruner, all coupled structures can be automatically identified and extracted.

### Grouped Importance Estimation of Coupled Structure

Till now, all coupled structures within the model are grouped. Weights within the same group should be pruned simultaneously, as partial pruning not only increases parameter size but also introduces misaligned intermediate representations. Therefore, we estimate the importance of the group as a whole, as opposed to evaluating the importance of modules. Given the limited access to the training dataset, we explore the use of public datasets or manually created samples as alternative resources. Although the domains of these datasets may not perfectly align with the training set, they still provide valuable information for assessing the importance.

Vector-wise Importance.Suppose that given a dataset \(=\{x_{i},y_{i}\}_{i=1}^{N}\), where N is the number of samples. In our experiments, we set N equal to 10 and we use some public datasets as the source of \(\). A group (as previously defined as a set of coupled structures) can be defined as \(=\{W_{i}\}_{i=1}^{M}\), where M is the number of coupled structures in one group and \(W_{i}\) is the weight for each structure. While pruning, our goal is to remove the group that has the least impact on the model's prediction, which can be indicated by the deviation in the loss. Specially, to estimate the importance of \(W_{i}\), the change in loss can be formulated as :

\[I_{W_{i}}=|()|=|_{W_{i}}()- _{W_{i}=0}()|=|^{ }()}{ W_{i}}W_{i}}_{ 0}-W_{i}{}^{}HW_{i}+ (\|W_{i}\|^{3})|\] (3)

where \(H\) is the hessian matrix. Here, \(\) represents the next-token prediction loss. The first term is typically neglected in prior work [24; 54; 12], as the model has already converged on the training dataset, where \(^{}/ W_{i} 0\). However, since \(\) here is not extracted from the original training data, which means that \(^{}/ W_{i} 0\). This presents a desirable property for determining the importance of \(W_{i}\) by the gradient term under LLMs, since computation of the second term, the Hessian matrix, on the LLM is impractical with \((N^{2})\) complexity.

Figure 2: Illustration of the coupled structures in LLaMA. We simplify the neurons in each layer to make the dependent group clear. The trigger neuron, marked as a circle with a bell, cause weights with dependency pruned (dashed lines), which may propagate (red dashed lines) to coupled neurons (dashed circles). A group can be triggered by a variety of trigger neurons. Taking Group Type B as an example, the trigger for this group involves (i) the attention head, (ii) the output neuron in Query, Key or Value, and (iii) the input neuron in the final output projection.

Element-wise Importance.The above can be considered as an estimate for the weight \(W_{i}\). We can derive another measure of importance at a finer granularity, where each parameter within \(W_{i}\) is assessed for its significance:

\[I_{W_{i}^{k}}=|()|=|_{W_{i}^{k}}( )-_{W_{i}^{k}=0}()|=|()}{ W_{i}^{k}}W_{i}^{k}-W_{i}^{k}H_{kk}W_{i}^{k }+(\|W_{i}^{k}\|^{3})|\] (4)

Here, \(k\) represents the k-th parameter in \(W_{i}\). The diagonal of the hessian \(H_{kk}\) can be approximated by the Fisher information matrix, and the importance can be defined as:

\[I_{W_{i}^{k}}=|_{W_{i}^{k}}()-_{W_{i}^{k}=0}( )||()}{ W_{i}^{ k}}W_{i}^{k}-_{j=1}^{N}(( _{j})}{ W_{i}^{k}}W_{i}^{k})^{2}+(\| W_{i}^{k}\|^{3})|\] (5)

Group Importance.By utilizing either \(I_{W_{i}^{k}}\) or \(I_{W_{i}}\), we estimate the importance at the granularity of either a parameter or a vector of weight. Remembering that our goal is to estimate the importance of \(\), we aggregate the importance scores in four ways: (i) Summation: \(I_{}=_{i=1}^{M}I_{W_{i}}\) or \(I_{}=_{i=1}^{M}_{k}I_{W_{i}^{k}}\), (ii) Production: \(I_{}=_{i=1}^{M}I_{W_{i}}\) or \(I_{}=_{i=1}^{M}_{k}I_{W_{i}^{k}}\), (iii) Max: \(I_{}=_{i=1}^{M}I_{W_{i}}\) or \(I_{}=_{i=1}^{M}_{k}I_{W_{i}^{k}}\); (iv) Last-Only: Since deleting the last executing structure in a dependency group is equivalent to erasing all the computed results within that group, we assign the importance of the last executing structure as the importance of the group: \(I_{}=I_{W_{i}}\) or \(I_{}=_{k}I_{W_{i}^{k}}\), where \(l\) is the last structure. After assessing the importance of each group, we rank the importance of each group and prune the groups with lower importance based on a predefined pruning ratio.

### Fast Recovery with Low-rank Approximation

In order to expedite the model recovery process and improve its efficiency under limited data, it is crucial to minimize the number of parameters that need optimization during the recovery phase. To facilitate this, we employ the low-rank approximation, LoRA, to post-train the pruned model. Each learnable weight matrix in the model, denoted as \(W\), encompassing both pruned and unpruned linear projection in the LLM, can be represented as \(W\). The update value \( W\) for \(W\) can be decomposed as \( W=PQ^{d^{-} d^{+}}\), where \(P^{d^{-} d}\) and \(Q^{d d^{+}}\). The forward computation can now be expressed as:

\[f(x)=(W+ W)X+b=(WX+b)+(PQ)X\] (6)

where \(b\) is the bias in the dense layer. Only training \(P\) and \(Q\) reduces the overall training complexity, reducing the need for large-scale training data. Besides, the extra parameters \(P\) and \(Q\) can be reparameterized into \( W\), which would not cause extra parameters in the final compressed model.

## 4 Experiments

### Experimental Settings

Foundation Large Language Model.To showcase the effectiveness and versatility of LLM-Pruner, we test it over three open-source large language models with two kinds of structure: LLaMA-7B , Vicuna-7B 2 and ChatGLM-6B .

Evaluation and Datasets.To assess the performance of the model in the task-agnostic setting, we follow LLaMA's evaluation to perform zero-shot task classification on common sense reasoning datasets: BoolQ , PIQA , HellaSwag , WinoGrande , ARC-easy , ARC-challenge  and OpenbookQA . Follow , the model ranks the choices in the multiple choice tasks or generates the answer in the open-ended generation 3. Additionally, we complement our evaluation with a zero-shot perplexity (PPL) analysis on WikiText2  and PTB .

Implementation Details.In the model pruning process, we use 10 randomly selected samples from Bookcorpus , each truncated to a sequence length of 128, as the calibration samples for establishing dependency and calculating the gradient for both LLaMA and Vicuna. For ChatGLM, we select 10 random samples from DailyDialog . During the recovery phase, we utilize the cleaned version of Alpaca , which comprises approximately 50k samples. Remarkably, tuning these samples requires merely 3 hours on a single GPU with only 2 epochs. More hyper-parameters of pruning and training can be found in Appendix B.

Statistics of the Compressed Model.Table 3 presents the statistic of the 7B models that are used in our experiments: the parameter count, MACs, memory requirements and latency for running each model. The statistical evaluation is conducted using the inference mode, where the model is fed a sentence consisting of 64 tokens. The latency is tested under the test set of WikiText2 on a single A5000. Here, the 'Block' strategy implies that the pruned unit in the model consists of Group Type A and Group Type B as illustrated in Figure 2, whereas 'Channel' indicates that the unit to be pruned is Group Type C. We delve into an analysis of these two choices in Section 4.2(Channel Strategy vs. Block Strategy). The pruning ratio stated here denotes the approximate ratio of parameters to be pruned since the number of parameters within each pruned structure does not perfectly match the total number of pruned parameters.

### Zero-shot Performance

Table 1,2,4 and 5 shows the zero-shot performance of the pruned model. Based on the evaluation conducted on LLaMA, employing a 20% parameter reduction without post-training, the pruned

   Pruning Ratio & Method & WikiText2\(\) & PTB\(\) & BoolQ & PIQA & HellLSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\   & LLaMA-7B & - & - & 76.5 & 79.8 & 76.1 & 70.1 & 72.8 & 47.6 & 57.2 & 68.59 \\  & LLaMA-7B* & 12.62 & 22.14 & 73.18 & 78.35 & 72.99 & 67.01 & 67.45 & 41.38 & 42.40 & 63.25 \\   & L2 & 582.41 & 1022.17 & 59.66 & 58.00 & 37.04 & 52.41 & 33.12 & 28.58 & 29.80 & 42.65 \\  & Random & 27.51 & 43.19 & 61.83 & 71.33 & 56.26 & 54.46 & 57.07 & 32.85 & 35.00 & 52.69 \\   & Channel & 74.63 & 153.75 & 62.75 & 62.73 & 41.40 & 51.07 & 41.38 & 27.90 & 30.40 & 45.38 \\   & Vector & 22.28 & 41.78 & 61.44 & 71.71 & 57.27 & 54.22 & 55.77 & 33.96 & 38.40 & 53.25 \\  & Element2 & 19.77 & 36.66 & 59.39 & 75.57 & 65.34 & 61.33 & 59.18 & 37.12 & 39.30 & 56.82 \\  & Element2 & 19.09 & 34.21 & 57.06 & 25.68 & 68.50 & 59.83 & 60.94 & 36.52 & **30.00** & 56.69 \\   & Channel & 22.02 & 38.67 & 59.08 & 73.39 & 64.02 & 60.54 & 57.95 & 35.38 & 38.40 & 55.57 \\  & Vector & 18.84 & 33.05 & 65.75 & 74.70 & 64.52 & 59.35 & 60.65 & 36.26 & 39.40 & 57.23 \\   & Element2 & **17.37** & 30.39 & **69.54** & 76.44 & 68.11 & **65.11** & 63.43 & **37.88** & **40.00** & **60.07** \\   & Element2 & 17.58 & **30.11** & 64.62 & **77.20** & **68.80** & 63.14 & **64.31** & 36.77 & 39.80 & 59.23 \\   

Table 1: Zero-shot performance of the compressed LLaMA-7B. The average is calculated among seven classification datasets. ‘Underline’ indicates the best pruning-only performance, while ‘bold’ represents the overall best performance with the same pruning ratio, considering both pruning and post-training. The ‘Channel’ strategy only prunes the dependent group of Type C, while all other methods employ the ‘Block’ strategy to prune dependent groups in both Type A and Type B. Since  did not provide its prompt, the evaluation of the result with \({}^{*}\) is performed under different prompts, which is lower than the official results.

   Pruning Ratio & Method & WikiText2\(\) & PTB\(\) & BoolQ & PIQA & HellLSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\  Ratio = 0\% & LLaMA-13B* & 11.58 & 20.24 & 68.47 & 78.89 & 76.24 & 70.09 & 74.58 & 44.54 & 42.00 & 64.97 \\   & L2 & 61.15 & 91.43 & 61.50 & 67.57 & 52.90 & 57.54 & 50.13 & 31.14 & 36.80 & 51.08 \\  & Random & 19.24 & 31.84 & 63.33 & 73.18 & 63.54 & 60.85 & 64.44 & 36.26 & 38.00 & 57.99 \\  & Channel & 49.03 & 100.48 & 62.39 & 66.87 & 49.17 & 58.96 & 49.62 & 31.83 & 33.20 & 50.29 \\  & Block & 16.01 & 29.28 & 67.68 & 77.15 & 73.41 & 65.11 & 68.35 & 38.40 & 42.40 & 61.79 \\   & L2 & 20.97 & 38.05 & **73.24** & 76.77 & 71.86 & 64.64 & 67.59 & 39.93 & 40.80 & 62.12 \\  & Random & 16.84 & 31.98 & 64.19 & 76.06 & 68.89 & 63.30 & 66.88 & 38.31 & 40.80 & 59.78 \\   & Channel & 17.58 & 29.76 & 69.20 & 76.55 & 68.89 & 66.38 & 62.08 & 38.99 & 39.60 & 60.24 \\   & Block & **15.18** & **28.08** & 70.31 & **77.91** & **75.16** & **67.88** & **71.09** & **42.41** & **43.40** & **64.02** \\   

Table 2: Zero-shot performance of the compressed LLaMA-13B. Here we adopt Element1 as the importance estimation for ‘Channel’ and ‘Block’.

   Model & Strategy & Ratio & \#Params & \#MACs & Memory & Latency \\   & - & - & 6.74B & 424.02G & 12884.5MB & 69.32s \\  & Channel & 20\% & 5.39B & 339.36G & 10363.6MB & 61.50s \\   & Block & 20\% & 5.42B & 339.60G & 10375.3MB & 58.55s \\   & Channel & 50\% & 3.37B & 212.58G & 6556.3MB & 40.11s \\   & Block & 50\% & 3.35B & 206.59G & 6533.9MiB & 37.54s \\   

Table 3: Statistics of the base model and the compressed model.

model manages to retain 89.8% of the performance exhibited by the unpruned model. Furthermore, through the efficient post-training, the classification accuracy further improves to 60.07%, achieving 94.97% of the accuracy attained by the original model. This demonstration proves the feasibility of using LLM-Pruner to effectively compress the model, even without relying on training data, and within a remarkably short period of time. Surprisingly, we discover that on most datasets, the pruned model with 5.4B LLaMA even outperformed chatGLM-6B. This highlights the superiority of the LLM-Pruner: if a smaller model with a customized size is required, LLM-Pruner is more cost-effective compared to retraining another model with a satisfying performance. However, with 50% parameters pruned, a large accuracy degradation is observed (see Appendix C.4). Compressing LLMs under high compression rates still remains a large challenge.

The compression results of Vicuna-7B align with those of LLaMA, as pruning 20% of parameters on Vicuna-7B maintains performance at 92.03% of the original model. We test a smaller pruning rate of 10% on chatGLM-7B, where the pruned model only experiences a marginal performance decrease of 0.89%, which can be recovered through post-training. Despite the pruned model outperforming the uncompressed model, we don't assert it is better than the original model. This is largely because chatGLM-6B, a bilingual model, has limited English pre-training exposure. Post-training, however, introduces it to more English corpus, albeit limited, improving its English comprehension.

Ablation: Impact of Importance Estimation.We conduct tests on all proposed importance estimation techniques mentioned in Section 3.2. The results can be found in Table 1 and 4. Here, _Element_a represents the importance evaluation utilizing the n-th order term in Eq.5. _Vector_ represents the result corresponding to Eq.3. Based on the results obtained from LLaMA-7B and Vicuna-7B, pruning algorithms achieved the best average performance mostly by leveraging the second-order derivatives for each parameter. Nonetheless, given that first-order derivatives are considerably more efficient than second-order derivatives, though yielding slightly inferior results, we still vote for the first-order term as a competitive method. Besides, the results on chatGLM-7B differed significantly from these findings. The importance estimation on each parameter fails, performing even worse than l2, while the importance estimation on the weight matrix reaches the best performance.

Ablation: Channel Strategy vs. Block Strategy.From the results presented in Table 2, it is evident that pruning 'Channel' significantly deteriorates performance compared to pruning 'Block'. This discrepancy arises because the layers within the stacked transformer do not evenly distribute their importance. As shown in Figure 3, the first and last layers have a profound impact on the model's performance, and pruning them results in more substantial performance degradation compared to other layers. However, due to the uniform treatment of the 'Channel' group across all layers, it becomes inevitable to prune the first and last layers, leading to a significant decline in performance.

    & Method & WikiText2 \(\) & PTB\(\) & BooIQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-e & OBQA & Average \\  Ratio = 0\% & Vicuna-7B & 16.11 & 61.37 & 76.57 & 77.75 & 70.64 & 67.40 & 65.11 & 41.21 & 40.80 & 62.78 \\   & 12 & 3539.98 & 5882.21 & 55.90 & 56.15 & 32.37 & 51.85 & 30.01 & 28.41 & 28.20 & 40.41 \\  & random & 346.3 & 112.44 & 61.47 & 70.89 & 54.67 & 56.27 & 55.60 & 31.74 & 34.60 & 52.18 \\    & Channel & 71.75 & 198.88 & 51.77 & 63.93 & 42.58 & 55.17 & 43.94 & 29.27 & 33.40 & 45.72 \\    & Vector & 27.03 & 92.51 & 62.17 & 71.44 & 55.80 & 53.43 & 55.77 & 33.28 & 37.80 & 52.81 \\   & Element2 & 24.70 & 94.34 & 62.87 & 75.41 & 64.00 & 58.41 & 60.98 & 37.12 & 39.00 & 56.83 \\    & Element2 & 25.74 & 92.88 & 61.70 & 75.30 & 63.75 & 56.20 & 53.22 & 36.60 & 37.00 & 56.25 \\   & Vector & 19.94 & **74.66** & 63.15 & 74.59 & 61.95 & 60.30 & 60.48 & 36.00 & **39.40** & 56.64 \\   & Element2 & **18.97** & 76.78 & 60.40 & 75.63 & **65.45** & **63.22** & **63.05** & **37.71** & 39.00 & **57.78** \\    & Element2 & 19.69 & 78.25 & **63.33** & **76.17** & 65.13 & 60.22 & 62.84 & 37.12 & 39.20 & 57.71 \\   

Table 4: Zero-shot performance of the compressed Vicuna-7B

Figure 3: Layer sensitivity for Pruning: Removing Groups in only one layer.

### More Analysis

Impact of Different Pruning Rates.We investigate the impact of pruning the LLM at various pruning ratios in Figure 4. We compare our pruning results with the L2 strategy because L2 is also a data-free pruning algorithm. It is observed in the experiment of LLaMA that when the pruning ratio reaches approximately 20%, the magnitude-dependent algorithm experiences a rapid collapse, leading to the loss of information. Conversely, by employing LLM-Pruner, we are able to increase the pruning ratio to around 60% while achieving an equivalent perplexity level. Furthermore, in the case of Vicuna-7B, removing 10% parameters results in a performance decline equivalent to that of LLM-Pruner with 60%. The utilization of LLM-Pruner enables a significant increase in the number of model parameters that can be pruned, thereby substantially reducing computational overhead.

Tuning on the External Dataset.To tune the pruned model, we utilize the external dataset Alpaca . The evaluation curves of the pruned model on two zero-shot datasets during the post-training process are depicted in Figure 5. The results demonstrate a rapid decrease in the perplexity of the pruned model within 300 steps, followed by a gradual increase. We provide a more comprehensive evaluation in Appendix C.3. It is important to note that if the model is trained for an excessive number of steps, it runs the risk of overfitting the external dataset, potentially compromising its performance in other general-purpose tasks.

Impact of Dependency-based Structured Pruning.To study the importance of dependency-based structural pruning, we conduct an experiment to disrupt dependencies within groups, where each weight matrix \(W_{i}\) is pruned solely based on the importance score estimated on itself. Table 6 presents the results demonstrating the impact of dependencies in structural pruning. In the absence of dependencies, the model nearly fails in the zero-shot generation and classification tasks. Even with tuning, the model fails to recover, showing a substantial difference compared to the results in dependency-based pruning.

Impact of Different Aggregation Strategies.We conduct tests on the aggregation algorithms proposed in Section 3.2. Our experimental results unveil notable discrepancies in model performance across different aggregation strategies, with particular emphasis on the 'Last-only' strategy. Among the evaluated approaches, the 'Max' strategy attains the most favorable outcomes in terms of perplexity, signifying enhanced coherence and fluency in sentence generation. However, it is important to note that the 'Max' strategy exhibits the poorest zero-shot classification results compared to all four strategies. Conversely, the 'Last-only' strategy showcases superior classification performance but

Figure 4: The pruning results on LLaMA-7B (left) and Vicuna- Figure 5: Perplexity on zero-shot 7B (right) with different pruning rates.

    &  &  &  &  &  &  &  &  \\  Ratio = 0\% & ChatGLM-6B & 67.95 & 46.37 & 52.33 & 48.36 & 29.95 & 37.40 & 47.05 \\   & L2 & 61.97 & 37.22 & 49.72 & 42.05 & 28.24 & 35.40 & 42.43 \\  & Random & 65.29 & 43.18 & 51.30 & 47.52 & 29.52 & 34.60 & 45.24 \\   & Vector & 66.32 & 43.51 & 53.04 & 47.56 & **30.72** & **35.80** & 46.16 \\   & Element\({}^{1}\) & 54.35 & 28.07 & 50.59 & 27.82 & 24.66 & 33.20 & 36.45 \\  w/ tune & Vector & **67.74** & **46.35** & **53.99** & **51.01** & 29.95 & 35.00 & **47.34** \\   

Table 5: Zero-shot Performance of the compressed ChatGLM-6Bsuffers from the poorest generation quality. In our experiments, we make a trade-off by selecting the 'Summation' strategy since it shows both good generalization quality and classification performance.

Comparison with DistilBERTWe show the comparison results of DistilBERT and LLM-Pruner on LLaMA-7B in Table 8. LLM-Pruner outperforms DistilBERT by 4.24% on average with even a smaller size. The reason lies in that LLM-Pruner minimizes model disruption during pruning, whereas DistilBERT merely selects one layer out of two. As a result, the model pruned by LLM-Pruner demands less data to recover its performance compared with DistilBERT, consequently achieving superior performance.

Scratch Training vs. Pruning.We compare LLM-Pruner with StableLM-3B4 with a similar parameter size. To ensure fairness, both models are fine-tuned on the Alpaca dataset. The experimental results of these two models are shown in the Table 9. LLM-Pruner crafts lightweight LLMs with low resources, and even can sometimes achieve better performance than LLMs from scratch training. However, we also acknowledge that the LLaMA-3B obtained by LLM-Pruner will not always outperform other 3B models from scratch training, due to the huge gap in the size of training corpus.

More Data for RecoveryDespite our primary experiments being conducted using 50k samples, we remain convinced that the inclusion of additional data could substantially enhance the recovery process, albeit at a considerably higher computational cost. Consequently, we conduct an experiment aimed at model recovery with more data, employing a dataset comprising 2.59 million samples . The results are detailed in Table 10. From the results, it is evident that the performance of the compressed model closely approximates that of the base model, exhibiting only a marginal performance decrease of 0.89%.

Case Study.We provide some examples of sentences generated by the model compressed using LLM-Pruner in Table 11. We made efforts to ensure a minimal overlap between these generated sentences and the information contained in the tuning corpus, which demonstrates that the information originates from the original model rather than the tuning corpus. We provide additional examples in the Appendix, including the generated sentences of the model without post-training. From the

    & Method & WikText2 & PTB1 & Average \\  w/o Tuning & w/o dependency & 68378.42 & 79942.47 & 38.32 \\ w/ dependency & 19.09 & 34.21 & 56.69 \\  w/ Tuning & w/o dependency & 13307.46 & 13548.08 & 38.10 \\  & w/ dependency & 17.58 & 30.11 & 59.23 \\   

Table 6: Effect of the dependency-based structural pruning. Average represents the average performance on 7 classification datasets.

    & Method & WikText2 & PTB1 & ARC+ & PIQA & ORQA† \\   Summation & 66.13 & 164.25 & 40.70 & 63.49 & 34.80 \\ Max & 62.59 & 144.38 & 39.60 & 63.71 & 34.60 \\ Production & 77.63 & 192.88 & 37.84 & 62.08 & 35.00 \\ Last-only & 130.00 & 170.88 & 41.92 & 64.75 & 35.20 \\   

Table 7: Impact of different aggregation strategies on group importance estimation. Experiments are performed on LLaMA-7B.

    & Method & WikText2 & PTB1 & ARC+ & PIQA† & ORQA† \\    & w/o Denting & 68378.42 & 79942.47 & 38.32 \\  & w/ dependency & 19.09 & 34.21 & 56.69 \\    & w/o dependency & 13307.46 & 13548.08 & 38.10 \\  & w/ dependency & 17.58 & 30.11 & 59.23 \\    & & & & & & & & \\ 

Table 8: DistilBert vs. LLM-Pruner. The average here means the average score on the above seven datasets.

    & Method & WikText2 & PTB1 & ARC+ & PIQA† & ORQA† \\    & w/o Denting & 68378.42 & 79942.47 & 38.32 \\  & w/ dependency & 19.09 & 34.21 & 56.69 \\    & w/o dependency & 13307.46 & 13548.08 & 38.10 \\  & w/ dependency & 17.58 & 30.11 & 59.23 \\    & & & & & & & \\ 

Table 8: DistilBert vs. LLM-Pruner. The average here means the average score on the above seven datasets.

    & \#Param & Latency & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\  StableLM-3B & 3.6B & 31.69s & 48.78 & 69.48 & 44.52 & 54.62 & 50.93 & 25.17 & 27.40 & 45.84 \\ LLaMA-3B & 3.6B & 37.96s & 61.41 & 70.08 & 51.01 & 55.01 & 46.80 & 30.38 & 37.40 & 50.30 \\    & & & & & & & & & & \\ 

Table 9: Scratch Training (StableLM-3B) vs. Pruning (LLaMA-3B by LLM-Pruner)cases in Table 11, it is evident that the sentences generated by the compressed model are comparable to those produced by the original model. They exhibit fluency, relevance, and informativeness regarding the given topic. Nevertheless, during our experiments, we observed that the pruned model's performance deviates from that of the original model, particularly when generating lengthy sentences. Occasionally, it may generate sentences that are meaningless or contain repetitive tokens.

## 5 Conclusion

In this paper, we propose LLM-Pruner, a structured pruning approach for large language models. LLM-Pruner aims to compress sizable language models in a task-agnostic manner while minimizing the dependency on the original training corpus and preserving the linguistic capabilities of LLMs. LLM-Pruner accomplishes this by iteratively examining each neuron within the model as a trigger for identifying dependency groups, thereby constructing the LLM's dependency graph. Subsequently, LLM-Pruner assesses the importance of these groups using both parameter-wise and weight-wise estimation. Finally, we utilize LoRA for fast recovery and adjustment of the pruned model. We evaluate the efficacy of LLM-Pruner on three distinct models--LLaMA, Vicuna, and ChatGLM--utilizing various zero-shot datasets. Our experimental results indicate that LLM-Pruner successfully prunes the model, reducing computational burden while retaining its zero-shot capabilities. Nevertheless, considerable performance degradation occurs when employing high pruning rates, such as the removal of 50% of LLaMA's parameters, resulting in a substantial decline in model performance. Additionally, we observe instances in which the model generates incoherent sentences. Addressing the challenges associated with compressing LLMs at higher pruning rates remains a challenging task.