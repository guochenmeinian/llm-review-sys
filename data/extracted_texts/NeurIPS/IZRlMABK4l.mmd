# Supplementary Materials for

Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction

 Zeshuai Deng\({}^{1}\)

Zhuokun Chen\({}^{1}\)

Shuaicheng Niu\({}^{1}\)

Thomas H. Li\({}^{5}\)

Bohan Zhuang\({}^{3}\)

Mingkui Tan\({}^{1}\)

Corresponding author. Email: mingkuitan@scut.edu.cn, bohan.zhuang@gmail.com\({}^{1}\)South China University of Technology, \({}^{2}\)Pazhou Lab, \({}^{3}\)ZIP Lab, Monash University,

\({}^{4}\)Key Laboratory of Big Data and Intelligent Robot, Ministry of Education,

\({}^{5}\)Peking University Shenzhen Graduate School

###### Abstract

Image super-resolution (SR) aims to learn a mapping from low-resolution (LR) to high-resolution (HR) using paired HR-LR training images. Conventional SR methods typically gather the paired training data by synthesizing LR images from HR images using a predetermined degradation model, _e.g._, Bicubic down-sampling. However, the realistic degradation type of test images may mismatch with the training-time degradation type due to the dynamic changes of the real-world scenarios, resulting in inferior-quality SR images. To address this, existing methods attempt to estimate the degradation model and train an image-specific model, which, however, is quite time-consuming and impracticable to handle rapidly changing domain shifts. Moreover, these methods largely concentrate on the estimation of one degradation type (_e.g._, blur degradation), overlooking other degradation types like noise and JPEG in real-world test-time scenarios, thus limiting their practicality. To tackle these problems, we present an efficient test-time adaptation framework for SR, named SRTTA, which is able to quickly adapt SR models to test domains with different/unknown degradation types. Specifically, we design a second-order degradation scheme to construct paired data based on the degradation type of the test image, which is predicted by a pre-trained degradation classifier. Then, we adapt the SR model by implementing feature-level reconstruction learning from the initial test image to its second-order degraded counterparts, which helps the SR model generate plausible HR images. Extensive experiments are conducted on newly synthesized corrupted DIV2K datasets with 8 different degradations and several real-world datasets, demonstrating that our SRTTA framework achieves an impressive improvement over existing methods with satisfying speed. The source code is available at https://github.com/DengZeshuai/SRTTA.

## 1 Introduction

Image super-resolution (SR) aims to reconstruct plausible high-resolution (HR) images from the given low-resolution (LR) images, which is widely applied in microscopy , remote sensing  and surveillance . Most previous SR methods  hypothesize that the LR images are downsampled from HR images using a predefined degradation model, _e.g._, Bicubic down-sampling. However, due to the diverse imaging sensors and multiple propagations on the Internet, real-world images may contain different degradation types (_e.g._, Gaussian blur, Poisson noise, and JPEG artifact) . Besides, the realistic degradations of real-world imagesmay dynamically change, which are often different from the training one, limiting the performance of pre-trained SR models in dynamically changing test-time environments.

Recently, zero-shot SR methods  are proposed to train an image-specific SR model for each test image to alleviate the degradation shift issue. For example, ZSSR  uses a predefined/estimated degradation model to generate an image with a lower resolution for each test image. With this downsampled image and the test image, they can train an image-specific SR model to super-resolve the test image. Moreover, DualSR  estimates the degradation model and trains the SR model simultaneously to achieve better performance. However, these methods usually require thousands of iterations to estimate the degradation model or train the SR model, which is very time-consuming. Thus, these methods cannot handle real-world test images with rapidly changing domain shifts.

To reduce the inference time of zero-shot methods, some recent works  introduce meta-learning  to accelerate the adaptation of the SR model, which still requires a predefined/estimated degradation model to construct paired data to update the model. However, most degradation estimation methods  focus on the estimation of one degradation type, which limits the adaptation of SR models to test images with other degradations. Recently, test-time adaptation (TTA) methods  are proposed to quickly adapt the pre-trained model to the test data in target domain without accessing any source training data. These methods often use simple augmentation operations (_e.g._, rotation or horizontal flip) on the test image, and construct the pseudo label as the average of the predicted results . For image SR, the pseudo-HR image constructed using this scheme  may still contain the degradation close to the test image (_e.g._, Gaussian blur). With such a pseudo-HR image, the adapted SR model may not be able to learn how to remove the degradation from the test image (see results in Table 1). Therefore, how to quickly and effectively construct the paired data to encourage the SR model to remove the degradation is still an open question.

In this paper, we propose a super-resolution test-time adaptation framework (SRTTA) to adapt a trained super-resolution model to target domains with unknown degradations, as shown in Figure 1. When the degradation shift issue occurs, the key challenge is how to quickly and effectively construct (pseudo) paired data to adapt SR models to the target domain without accessing any clean HR images. To this end, we propose a second-order degradation scheme to construct (pseudo) paired data. Specifically, with a pre-trained degradation classifier, we quickly identify the degradation type from the test images and randomly generate a set of degradations to obtain the second-order degraded images. The paired data, which consists of the second-order degraded images and the test image, enables a rapid adaptation of SR models to the target domain with different degradations. To facilitate the learning of reconstruction, we design a second-order reconstruction loss to adapt the pre-trained model using the paired data in a self-supervised manner. After fast adaptation using our method, the SR model is able to learn how to remove this kind of degradations and generate plausible HR images. Moreover, we also design an adaptive parameter preservation strategy to preserve the knowledge of the pre-trained model to avoid the catastrophic forgetting issue in long-term adaptation. Last but not least, we use eight different degradations to construct two new benchmarks, named DIV2K-C and DIV2K-MC, to comprehensively evaluate the practicality of our method. Experimental results on both our synthesized datasets and several real-world datasets demonstrate that our SRTTA is able to quickly adapt the SR model to the test-time images and achieve an impressive improvement.

Our main contributions are summarized as follows:

* **A novel test-time adaptation framework for image super-resolution**: We propose a super-resolution test-time adaptation (SRTTA) framework to adapt any pre-trained SR models to different target domains during the test time. Without accessing any ground-truth HR images, our SRTTA is applicable to practical scenarios with unknown degradation in a self-supervised manner.
* **A fast data construction scheme with second-order degradation**: We use a pre-trained classifier to identify the degradation type for a test image and construct the paired data using our second-order degradation scheme. Since we do not estimate the parameters of the degradation model, our scheme enables a rapid model adaptation to a wide range of degradation shifts.
* **New test datasets with eight different domains**: We construct new test datasets named DIV2K-C and DIV2K-MC, which contain eight common degradations, to evaluate the practicality of different SR methods. Experimental results on both synthesized datasets and real-world datasets demonstrate the superiority of our SRTTA, _e.g._, 0.84 dB PSNR improvement on DIV2K-C over ZSSR .

## 2 Related Work

**Real-world super-resolution.** To alleviate the domain shift issues, GAN-based methods [54; 5; 24; 36] tend to learn the degradation model of the real-world images in the training stage. These methods often train a generator that explicitly learns the degradation model of real-world images. Besides, some methods [55; 60; 49] try to enumerate most of the degradation models that can be encountered in real-world applications. Based on the estimated/predefined degradation models, these methods can generate LR images whose distribution is similar to real-world images. However, due to the complex and unknown processing of real-world images, it is hard to mimic all types of degradation during the training phase. Instead, some existing methods [15; 2; 22; 21; 34] try to estimate the image-specific degradation model during the test time, which helps to reconstruct more plausible HR images. For instance, optimization-based methods [15; 22; 21] estimate the blur kernel and SR image together in an iterative manner. However, these methods cannot generate satisfactory results when the test images contain different types of degradation (_e.g._, Poisson noise and JPEG artifact) . Thus, these methods still suffer from the domain shift on test images with unknown degradation.

**Zero-shot super-resolution.** Zero-shot methods [43; 9; 45; 41; 12] aim to train an image-specific SR model for each test image to alleviate the domain shift issue. These methods [43; 12] use a predefined/estimated degradation model to generate an image with a lower resolution from each test image. To estimate the image-specific degradation in a zero-shot manner, KernelGAN  utilizes the internal statistics of each test image to learn the degradation model specifically and then uses ZSSR  to train an SR model with the estimated degradation. However, these methods usually require a lot of time to estimate the degradation model or train the SR model. MZSR  and MLSR  are proposed to reduce the number of iteration steps for each test image during test time. Recently, DDNM  was proposed to use a pre-trained diffusion model to ensure the generated images obey the distribution of natural images. However, these methods still require a predefined (Bicubic downsampling) or an estimated degradation model. The predefined Bicubic degradation suffers from the domain shift due to its difference from the underlying degradation of real-world images. The estimation methods [2; 30] may focus on the estimation of a single degradation type (_e.g._, blur) while ignoring other degradation. Thus, these methods often result in unsatisfactory HR images for the test images with different degradation types . In this paper, we use a degradation classifier to quickly recognize the degradation type and randomly generate the degradations with this type. Therefore, we do not need to estimate the degradation model, which is time-saving.

**Test-time adaptation.** Recently, test-time adaptation (TTA) methods [46; 47; 57; 38; 48; 6; 39; 59] have been proposed to alleviate the domain shift issue by online updating the pre-trained model on the test data. TTT  uses an auxiliary head to learn the test image information from the self-supervised task. Tent  proposed to adapt the pre-trained model with entropy-based loss in an unsupervised manner. CoTTA  uses a weight-averaged pseudo-label over training steps to guide the pre-trained model adaptation. However, these methods are mainly developed for image classification and may ignore the characteristics of image super-resolution. Thus, these methods may not be effective in adapting the SR model to remove the degradation from the test image. In this paper, we focus on the image SR task and address the degradation shift issue with our SRTTA framework.

## 3 Preliminary and Problem Definition

**Notation.** Without loss of generality, let \(\) be a clean high-resolution (HR) image and \(_{c}\) be the clean low-resolution (LR) image downsampled from \(\) using Bicubic interpolation, _i.e._, \(_{c}=_{s}\), where \(s\) is the scale factor of Bicubic downsampling. Let \(\) denote a real-world test image degraded from \(\), _i.e._, \(=()\), where \(()\) is the degradation process. We use \(_{sd}\) to denote the LR image that is further degraded from the real-world image \(\). In this paper, we call the test image \(\) as a **first-order** degraded image and the image \(_{sd}\) degraded from \(\) as a **second-order** degraded image. \(f_{}()\) is a super-resolution (SR) model with parameters \(\).

**Image degradation.** The degradation process of real-world test images can be modeled by a classical degradation model \(()\)[33; 49]. Formally, let \(\) be a blur kernel, \(\) be an additive noise map and \(q\) be the quality factor of \(JPEG\) compression, the degraded image \(\) is defined by

\[=()=[()_{s }+]_{JPEG_{q}},\] (1)

where \(\) denotes the convolution operation, \(_{s}\) denotes the downsampling with a scale factor of \(s\), and \(JPEG_{q}\) denotes the JPEG compression with the quality factor \(q\). Similarly, the second-orderdegraded image \(_{sd}\) can be formulated as

\[_{sd}=(())=()=[( )_{s}+]_{JPEG_{q}}.\] (2)

**Degradation shift between training and testing.** Existing SR methods [31; 60; 16] often construct paired HR-LR training images by either collecting from the real world or synthesizing LR images from HR images via a pre-defined degradation model, _i.e._, Bicubic down-sampling. However, due to diverse camera sensors and the unknown processing on the Internet, the degradation process of real-world test images may differ from that of training images, _called domain shifts_[27; 48; 32]. In these cases, the SR model often fails to generate satisfactory HR images.

**Motivation and challenges.** Though recently some blind SR methods [12; 43] have been proposed to address the degradation shift issue, they still suffer from two key limitations: low efficiency and narrow focus on a single degradation type, _e.g._, blur degradation. In this work, we seek to resolve these issues by directly learning from the shifted testing LR image at test time, which poses two major challenges: 1) How to quickly and effectively construct the (pseudo) paired data to adapt SR models to test domains with unknown degradations? and 2) How to design a generalized test-time learning framework that facilitates the removal of various types of degradation, considering that we have only a low-resolution test image at our disposal?

## 4 Efficient Test-Time Adaptation for Image Super-Resolution

In this section, we illustrate our proposed super-resolution test-time adaptation (SRTTA) framework that is able to quickly adapt the pre-trained SR model to real-world images with different degradations. The overall framework and pipeline are shown in Figure 1 and Algorithm 1.

Given a test image \(\), we first construct the paired data using our second-order degradation scheme. Specifically, we use a pre-trained degradation classifier to recognize the degradation type for the test image. Based on the predicted degradation type, we randomly generate a set of degradation (_e.g._, a set of blur kernels \(\)) and use them to construct a set of paired data \(\{_{sd}^{i},\}_{i=1}^{N}\). With the paired data, we adapt the pre-trained SR model to remove the degradation from the test image. Notably, before performing the test-time adaptation, we freeze the important parameters to preserve the knowledge of the pre-trained model to alleviate the forgetting problem in long-term adaptation. After adaptation, we use the adapted model to generate the corresponding HR image for the test image.

Figure 1: An overall illustration of the proposed super-resolution test-time adaptation (SRTTA) framework. Given a test image \(\), we use a pre-trained degradation classifier to predict the degradation type \(C()\), _e.g._, blur, noise, and JPEG degradation. Based on the predicted degradation type \(C()\), we construct a set of paired data \(\{_{sd}^{i},\}_{i=1}^{N}\) and adapt the SR model with our adaptation loss \(_{a}\) and \(_{s}\). When test samples are clean images, we directly use the frozen pre-trained SR model to super-resolve these clean images without adaptation.

### Adaptive Data Construction with Second-Order Degradation

In this part, we propose a novel second-order degradation scheme to effectively construct paired data, enabling the fast adaptation of SR models to the target domain with different degradations.

Unlike existing methods [43; 2; 30], we consider more degradation types and avoid estimating the degradation model. Existing methods [43; 2; 30] mainly focus on precisely estimating the blur kernels when constructing the lower-resolution images (second-order degraded images), which is time-consuming. Instead, we use a pre-trained degradation classifier to quickly identify the degradation types (blur, noise, and JPEG) of test images, and then we construct the second-order degraded images based on the predicted degradation types. Without the time-consuming degradation estimation process, our scheme enables a fast model adaptation to a wide range of degradation shifts.

**Adaptive data construction.** In this part, we design an adaptive data construction method to obtain the second-order degraded images \(_{sd}^{i}\). Specifically, based on the classical degradation model in Eqn. (1), we train a multi-label degradation classifier \(C()\) to predict the degradation types for each test image, including blur, noise and JPEG degradation types, denoted by \(c_{b}\), \(c_{n}\) and \(c_{j}\{0,1\}\), respectively. With the predicted degradation types, we randomly generate \(N\) degradations and construct a set of second-order degraded images \(\{_{sd}^{i}\}_{i=1}^{N}\), which can be formulated as

\[&_{sd}=D(,C())=D_{j}(D_ {b}(,c_{b})+D_{n}(c_{n}),c_{j}),\\ & D_{b}(,c_{b})=c_{b}()+(1- c_{b}),\ D_{n}(c_{n})=c_{n},\\ & D_{j}(,c_{j})=c_{j}JPEG_{q}()+(1-c_{j}) ,\] (3)

where the blur kernel \(\), noise map \(\) and quality factor \(q\) are randomly generated using a similar recipe of Real-ESRGAN . Unlike previous methods [43; 2], we do not further downsample the test image \(\) when constructing \(_{sd}\), since the pretrained SR model has learned the upsampling function (the inverse function of downsampling) during the training phase. Due to the page limit, we put more details in the supplementary materials.

Since the pre-trained SR model has been well-trained on the clean domain (Bicubic downsampling), we simply ignore adapting the clean images in test-time. For these images, we use the pre-trained SR model to super-resolve them, _i.e._, \(}=_{^{0}}()\) when \(c_{b}=c_{n}=c_{j}=0\).

### Adaptation with Second-Order Reconstruction

In our SRTTA framework, we design a self-supervised adaptation loss and an adaptation consistency loss to update the pre-trained SR models to test images with degradation.

**Self-supervised adaptation.** To adapt the pre-trained model to remove the degradation, we design a self-supervised adaptation loss based on the Charbonnier penalty function [4; 28]. Specifically, we encourage the SR model to reconstruct the test images \(\) from the second-order degraded images \(_{sd}\) at the feature level, which can be formulated as

\[_{s}(,_{sd})=^{l}( )-f_{}^{l}(_{sd}))^{2}+},\] (4)where \(f^{l}_{}()\) denotes the output features of the \(l\)-th layer. We simply set \(f^{l}_{}()\) to be the output features of the second-to-last convolution layer. \(\) is a small positive value that is set to \(10^{-3}\) empirically.

**Consistency maximization.** To keep the model consistent across adaptation, we design an adaptation consistency loss to encourage the output of the adapted model to be close to that of the pre-trained model, which is formulated as

\[_{a}(,_{sd})=_{ ^{0}}()-f^{l}_{}(_{sd}))^{2}+},\] (5)

where \(f^{l}_{^{0}}()\) denotes the output features of the \(l\)-th layer of the pre-trained SR model.

**Second-order reconstruction loss.** Our final second-order reconstruction loss consists of a self-supervised adaptation loss and an adaptation consistency loss, which is formulated as

\[=_{s}(,_{sd})+ _{a}(,_{sd}),\] (6)

where \(\) is a balance hyperparameter (the ablation study can be found in Table 5).

### Adaptive Parameter Preservation for Anti-Forgetting

To avoid catastrophic forgetting in long-term adaptation, we propose an adaptive parameter preservation (APP) strategy to freeze the important parameters during adaptation. To select the important parameters, we evaluate the importance of each parameter using the Fisher information matrix [26; 38]. Given a set of collected clean images \(_{c}\), we design an augmentation consistency loss \(_{c}\) to compute the gradient of each parameter. Based on the gradient, we compute the diagonal Fisher information matrix to evaluate the importance of each parameter \(^{0}_{i}\), which is formulated as

\[(^{0}_{i})=_{c}|}_{_{c}_{c}}(_{c}(_{c})}{ ^{0}_{i}})^{2},\] (7) \[_{c}(_{c})=}-f_{ ^{0}}(_{c}))^{2}+},\;\;s.t.\;\;}=_{i=1}^{8}_{i}(f_{^{0}}(_{i}(_{c} ))),\] (8)

where \(_{i}\{_{j}\}_{j=1}^{8}\) is an augmentation operation, which is the random combination of a 90-degree rotation, a horizontal and a vertical flip on the input image \(_{c}\). \(_{i}\) is the inverse operation of \(_{i}\) that rolls back the image \(_{i}(_{c})\) to its original version \(_{c}\). With \((^{0}_{i})\), we select the most important parameters using a ratio of \(\) and freeze these parameters during the adaptation. The set of selected parameters \(\) can be formulated as

\[=\{^{0}_{i}|(^{0}_{i})>_{}, ^{0}_{i}^{0}\},\] (9)

where \(_{}\) denotes the first \(\)-ratio largest value obtained by ranking the value \((^{0}_{i})\), \(\) is a hyperparameter to control the ratio of parameters to be frozen. Note that we only need to select the set of significant parameters \(\) once before performing test-time adaptation.

## 5 Experiments

### Experimental Details

**Testing data.** Following ImageNet-C , we degraded 100 validation images from the DIV2K  dataset into eight domains. We select the eight degradation types that do not extremely change the image content, including Gaussian Blur, Defocus Blur, Glass Blur, Gaussian Noise, Poisson Noise (Shot Noise), Impulse Noise, Speckle Noise, and JPEG compression. In total, we create a new benchmark dataset **DIV2K-C**, which contains 800 images with different single degradation, to evaluate the performance of different SR methods. Besides, we further construct a dataset named **DIV2K-MC**, which consists of four domains with mixed multiple degradations, including BlurNoise, BlurJPEG, NoiseJPEG, and BlurNoiseJPEG. Specifically, test images from the BlurNoiseJPEG domain contain the combined degradation of Gaussian Blur, Gaussian Noise and JPEG simultaneously. Moreover, we also evaluate our SRTTA on real-world images from DPED , ADE20K  and OST300 , whose corresponding ground-truth HR images can not be found. To evaluate the anti-forgetting performance, we use a benchmark dataset Set5 .

**Implementation details and evaluation metric.** We evaluate our approach using the baseline model of EDSR  with only 1.37 M parameters for 2 \(\) SR. To demonstrate the effectiveness of our SRTTA, we conduct experiments in two settings, including a **parameter-reset** setting and a **lifelong** setting. In the **parameter-reset** setting, the model parameters will be reset after the adaptation of each domain, which is the **default setting** of our SRTTA. In the **lifelong** setting, the model parameters will never be reset in the long-term adaptation, in this case, we call our methods as SRTTA-lifelong. For the balance weight in Eqn. (6), we set \(\) to 1. For the ratio of parameters to be frozen, we set the \(\) to 0.50. Please refer to more details in the supplementary materials.

To compare the inference times of different SR methods, we measure all methods on a TITAN XP with 12G graphics memory for a fair comparison. Due to the memory overload of HAT  and DDNM , we chop the whole image into smaller patches and process them individually for these two methods. To evaluate the performance of different methods, we use the common metrics PSNR  and SSIM  and report the results of all methods on the DIV2K-C dataset. Due to the page limit, we mainly report PSNR results and put more results in the supplementary materials.

**Compared methods.** We compare our SRTTA with several state-of-the-art methods including supervised pre-trained SR methods, blind SR methods, zero-shot SR methods, and a TTA baseline method. 1) The supervised pre-trained SR models learn super-resolution knowledge with a predefined degradation process, _i.e._, the Bicubic downsampling. These methods include SwinIR , IPT  and HAT , and the EDSR baseline . 2) Blind SR models predict the blur kernel of test images and generate HR images simultaneously, _e.g._, DAN  and DCLS-SR . 3) Zero-shot SR models often use a predefined/estimated degradation kernel to construct the LR-HR paired images based on the assumption of the cross-scale patch recurrence [14; 62; 37], and they use the LR-HR paired images to train/update the SR models for each test image. These methods include ZSSR , KernelGAN +ZSSR , MZSR , DualSR , DDNM . 4) Moreover, we implement a baseline TTA method (dubbed TTA-C) that utilizes the augmentation consistency loss \(_{c}\) in Eqn. (8) to adapt the pre-trained model, similar to MEMO  and CoTTA .

### Comparison with State-of-the-art Methods

We evaluate the effectiveness of our methods in terms of quantitative results and visual results. We report the PSNR results of our SRTTA and existing methods on the DIV2K-C dataset for \(2\) SR in Table 1 (more results are put in the supplementary materials). Since DAN  and DCLS-SR  are trained on paired images with Gaussian Blur degradation, they achieve the best results in Gaussian Blur and Glass Blur degradation. However, they merely achieve a limited performance on average due to the ignoring of the noise and JPEG degradations in their degradation model. Moreover, ZSSR  achieves state-of-the-art performance on average due to the thousands of iterations of training steps for each image. Though KernelGAN  estimates a more accurate blur kernel and helps to generate more plausible HR images for Gaussian Blur and Glass Blur degradation images, it is harmful to the performance of ZSSR  on the noise images, since the degradation model of KernalGAN  does not cover the noise degradation. Moreover, the baseline TTA-C may be harmful to adapting the pre-trained model to blur degradation due to the simple augmentation, resulting in a limited

   Method & Gaussian & Defocus & Glass & Gaussian & Poisson & Impulse & Speckle & JPEG & Mean & 
 GPU Time \\ (seconds/image) \\  \\  SwinIR  & 30.40 & 25.52 & 27.82 & 25.35 & 22.36 & 15.34 & 30.45 & 30.74 & 26.00 & 13.08 \\ IPT  & 28.93 & 24.08 & 26.39 & 22.96 & 20.08 & 13.06 & 28.27 & 28.36 & 24.02 & 55.36 \\ HAT  & 29.00 & 24.08 & 26.40 & 22.31 & 19.33 & 11.91 & 28.02 & 28.25 & 23.66 & 25.01 \\ DAN  & **34.32** & 25.58 & 31.72 & 26.36 & 23.28 & 11.46 & 30.64 & 31.08 & 26.81 & 3.10 \\ DCLS-SR  & 33.93 & 25.55 & **31.98** & 25.45 & 21.59 & 8.12 & 30.66 & 30.86 & 26.02 & 1.45 \\ ZSSR  & 29.91 & 25.54 & 27.79 & 26.79 & 24.24 & **19.14** & 30.95 & 31.01 & 26.92 & 117.65 \\ KernelGAN +ZSSR & 30.18 & **25.87** & 29.01 & 21.45 & 19.32 & 17.93 & 25.07 & 26.11 & 24.37 & 231.41 \\ MZSR  & 30.14 & 25.54 & 28.03 & 25.94 & 23.48 & 17.05 & 30.00 & 30.49 & 26.33 & 3.34 \\ DuaIR  & 29.00 & 24.40 & 28.18 & 22.30 & 20.11 & 17.22 & 24.99 & 27.44 & 23.87 & 210.85 \\ DDNM  & 28.46 & 24.09 & 26.39 & 24.37 & 21.92 & 13.98 & 28.00 & 28.26 & 24.51 & 2,288.55 \\ EDSR  & 30.28 & 25.52 & 27.82 & 25.87 & 22.96 & 15.87 & 30.52 & 30.83 & 26.21 & - \\ TTA-C & 30.21 & 25.50 & 27.79 & 26.37 & 23.57 & 16.41 & 30.25 & 30.91 & 26.38 & 13.59 \\  SRTTA (ours) & 31.07 & 25.86 & 29.01 & **29.65** & 26.69 & 16.15 & **32.33** & **31.30** & **27.76** & 5.38 \\ SRTTA-lifelong (ours) & 31.07 & 25.83 & 29.18 & 29.48 & **27.10** & 16.27 & 31.71 & 31.22 & 27.73 & 5.38 \\   

Table 1: Comparison with existing state-of-the-art SR methods on DIV2K-C for 2\(\) SR regarding PSNR (\(\)) and inference time (second/image), which is measured on a single TITAN XP GPU. The bold number indicates the best result and the underlined number indicates the second best result.

adaptation performance. Instead, our methods achieve the best performance in terms of PSNR on average. For quality comparison, we provide the visual results of our SRTTA and the compared methods in Figure 2. As shown in Figure 2, our SRTTA is able to remove the degradation and reconstruct clearer SR images with less noise or fewer JPEG artifacts.

**Comparison of inference time.** Moreover, we also compare the inference time of different methods in Table 1. Due to the additional time for test-time adaptation, our SRTTA cannot achieve the best inference speed. However, those methods with less inference time are mostly trained on domains with Gaussian blur degradation only, such as DAN , DCLS-SR , and MZSR , which still suffer from the domain shift issue under noise or JPEG degradation. Instead, with comparable efficiency, our SRTTA achieves an impressive improvement on average for all domains (see results in Table 1). In conclusion, our SRTTA achieves a better tradeoff between performance and efficiency.

**More results on test domains with mixed multiple degradations.** In this part, we evaluate our SRTTA on DIV2K-MC to further investigate the effectiveness of SRTTA under mixed multiple degradations. In Table 2, our SRTTA achieves the best performance on four domains with different mixed degradations, e.g., 26.47 (ZSSR) \(\) 28.48 (our SRTTA-lifelong) regarding the average PSNR metric. These results further validate the effectiveness of our proposed methods.

**Evaluation in terms of human eye-related metrics.** In this part, we further evaluate different methods in terms of the Frechet Inception Distance (FID)  and the Learned Perceptual Image Patch Similarity (LPIPS) distance , which correlate well with perceived image quality and are commonly used to evaluate the quality of generated images [10; 55]. We evaluate different methods on two synthesized datasets, including DIV2K-C and DIV2K-MC. As shown in Table 3, our SRTTA achieves the lowest values of both FID and LPIPS scores, demonstrating our SRTTA is able to generate images with higher visual quality.

### Further Experiments

In this part, we further conduct several ablation studies to demonstrate the effectiveness of each component of our SRTTA. Last, we evaluate our SRTTA on several datasets with real-world test images and provide the visual comparison results of different methods.

Figure 2: Visualization comparison on DIV2K-C test images with degradation for 2\(\) SR.

**Effect of the degradation classifier \(C()\).** To investigate the effect of the degradation classifier, we compare our SRTTA with a baseline that does not use the degradation classifier. Specifically, this baseline generates the second-order degraded images with the random degradation type. In this case, a test image with blur degradation can be randomly degraded with blur, noise, or JPEG degradation. We report the PSRN results of our SRTTA and this baseline in Table 4. Experimental results demonstrate the necessity of the degradation classifier in our second-order degradation scheme.

**Effect of \(_{s}\) and \(_{a}\) in Eqn. (6).** To investigate the effect of the self-supervised adaptation loss \(_{s}\) and adaptation consistency loss \(_{a}\) in Eqn. (6), we report the mean PSNR results of our SRTTA with \(_{s}\)-only and \(_{a}\)-only. As shown in Table 4, without the adaptation consistency loss \(_{a}\), the SR models with only the \(_{s}\) will inevitably result in a model collapse. This is because the SR model is prone to output the same output for any input images without meaning. When we remove the \(_{s}\) loss, SRTTA can only achieve a limited performance, which demonstrates that \(_{s}\) truly helps to encourage the SR models to learn how to remove the degradation during the adaptation process. These experimental results demonstrate the effectiveness of the \(_{s}\) and \(_{a}\) in our framework.

**Effect of the hyper-parameters \(\) in Eqn. (6).** To investigate the effect of the weight of adaptation consistency loss \(\) in Eqn. (6), we report the mean PSNR results of our SRTTA with different \(\) in Table 5. When the \(\) is too small (\(<1\)), the self-supervised degradation loss \(_{s}\) dominates the adaptation process and often results in the collapse of SR models. When the \(\) is too large (\(>1\)), the adaptation consistency loss \(_{a}\) may have a great constraint on the adapted SR model to be the same as the pre-trained model, which may be harmful to the adaptation of the SR performance. With \(=1\), our SRTTA achieves the best results on the DIV2K-C dataset on average. Therefore, we simply set the \(\) to be \(1\) for all other experiments.

**Effect of adaptive parameter preservation in Eqn. (9).** In this part, we investigate the effect of our adaptive parameter preservation (APP) strategy on test-time adaptation. We compare our APP with the existing anti-forgetting method Stochastic Restoration (STO) , which randomly restores a different set of parameters (\(1\%\) parameters) after each adaptation step. Moreover, we also compare our APP with the random selection (RS) baseline, which randomly selects a fixed set of parameters to freeze before adaptation. As shown in Table 6, though STO achieves the best anti-forgetting performance on the clean Set5 dataset, the STO severely hinders the TTA performance. The random selection baseline only achieves a limited performance of both TTA and anti-forgetting. Instead, our APP consistently outperforms the random selection baseline with the same ratio of frozen parameters. Moreover, our APP with \(=0.5\) achieves the best adaptation performance on DIV2K-C (see more

   Method & Avg. PSNR \\  SRTTA (ours) & **27.76** \\ - w/o classifier \(C()\) & 26.06 \\ - w/o \(_{s}\) & 27.15 \\ - w/o \(_{a}\) & 10.24 \\   

Table 4: Effectiveness of components in SRTTA on DIV2K-C.

    &  &  \\  & FID / LPIPS & FID / LPIPS \\  SwinIR  & 27.90 / 0.2441 & 60.62 / 0.2781 \\ IPT  & 68.22 / 0.2345 & 58.24 / 0.2453 \\ HAT  & 64.92 / 0.2352 & 60.73 / 0.2640 \\ DAN  & 73.59 / 0.2260 & 56.96 / 0.2263 \\ DCLS-SR  & 33.44 / 0.2472 & 57.93 / 0.2299 \\ ZSBR  & 56.66 / 0.1931 & 52.78 / 0.2152 \\ KernelGAN +ZSBR & 88.28 / 0.2160 & 80.19 / 0.2371 \\ MZSR  & 68.27 / 0.2085 & 16.72 / 0.24463 \\ DDNM  & 70.80 / 0.2101 & 59.64 / 0.2083 \\ EDSR  & 69.70 / 0.2242 & 57.95 / 0.2338 \\ TTA-C & 66.95 / 0.2188 & 56.32 / 0.2293 \\  SRTTA (ours) & 54.37 / 0.1877 & 36.88 / 0.1915 \\ SRTTA-lifelong (ours) & **53.30 / 0.1828** & **35.72 / 0.1832** \\   

Table 3: Comparison with different methods in terms of FID(\(\)) and LPIPS(\(\)) on both DIV2K-C and DIV2K-MC datasets.

    &  &  \\  & FID / LPIPS & FID / LPIPS \\  SwinIR  & 27.90 / 0.2441 & 60.62 / 0.2781 \\ IPT  & 68.22 / 0.2345 & 58.24 / 0.2453 \\ HAT  & 64.92 / 0.2352 & 60.73 / 0.2640 \\ DAN  & 73.59 / 0.2260 & 56.96 / 0.2263 \\ DCLS-SR  & 33.44 / 0.2472 & 57.93 / 0.2299 \\ ZSBR  & 56.66 / 0.1931 & 52.78 / 0.2152 \\ KernelGAN +ZSBR & 88.28 / 0.2160 & 80.19 / 0.2371 \\ MZSR  & 68.27 / 0.2085 & 16.72 / 0.4463 \\ DDNM  & 70.80 / 0.2101 & 59.64 / 0.2083 \\ EDSR  & 69.70 / 0.2242 & 57.95 / 0.2338 \\ TTA-C & 66.95 / 0.2188 & 56.32 / 0.2293 \\  SRTTA (ours) & 54.37 / 0.1877 & 36.88 / 0.1915 \\ SRTTA-lifelong (ours) & **53.30 / 0.1828** & **35.72 / 0.1832** \\   

Table 3: Comparison with different methods in terms of FID(\(\)) and LPIPS(\(\)) on both DIV2K-C and DIV2K-MC datasets.

   \(\) & 0 & 0.1 & 0.5 & 1 & 2 & 5 \\  DIV2K-C & 10.24 & 13.96 & 22.63 & **27.76** & 27.52 & 27.31 \\ Set5 & 11.42 & 37.66 & 37.75 & 34.59 & 35.41 & 35.89 \\   

Table 5: Effects of different \(\) (in Eqn. (6)) under parameter-reset setting. We report average PSNR (\(\)) on DIV2K-C (with degradation shift) and Set5 (w/o degradation shift).

results in the supplementary materials), thus we set the ratio of preservation parameters to be 0.5 in our SRTTA by default. These results demonstrate the effectiveness of our APP strategy.

**Visualization results on the real-world images.** We also provide the visual results of different methods on the real-world images from DPED  and ADE20K . We use our SRTTA-lifelong model that has been adapted on DIV2K-C to perform test-time adaptation on the real-world images from DPED  and ADE20K , respectively. As shown in Figure 3, SRTTA-lifelong is able to generate HR images with fewer artifacts. These results demonstrate that our method is able to be applied to real-world applications. Please refer to more results in the supplementary materials.

## 6 Conclusion

In this paper, we propose a super-resolution test-time adaptation (SRTTA) framework to quickly alleviate the degradation shift issue for image super-resolution (SR). Specifically, we propose a second-order degradation scheme to construct paired data for each test image. Then, our second-order reconstruction loss is able to quickly adapt the pre-trained SR model to the test domains with unknown degradation. To evaluate the effectiveness of our SRTTA, we use eight different types of degradations to synthesize two new datasets named DIV2K-C and DIV2K-MC. Experiments on the synthesized datasets and several real-world datasets demonstrate that our SRTTA is able to quickly adapt the SR model for each image and generate plausible high-resolution images.

    &  &  &  \\   & & 0.3 & 0.5 & 0.7 & 0.3 & 0.5 & 0.7 \\  DIV2K-C (with degradation shift) & 27.17 & 27.52 & 27.62 & 27.68 & 27.72 & **27.73** & 27.73 \\  Set5 (w/o degradation shift) & 35.57 & 33.95 & 34.02 & 34.24 & 34.11 & 34.23 & 34.38 \\   

Table 6: Ablation studies of adaptive parameter preservation (APP) strategy on DIV2K-C and Set5 under the lifelong setting. We report PSNR (\(\)) and results on DIV2K-C are averaged over 8 different degradation types. The compared stochastic restoration (STO)  select 1% parameters for each adaptation iteration and Random Selection (RS) is evaluated by selecting different \(\) parameters.

Figure 3: Visualization comparison on real-world test images from DPED  and ADE20K .