# How many classifiers do we need?

Hyunsuk Kim

Department of Statistics

University of California, Berkeley

hyskim7@berkeley.edu

&Liam Hodgkinson

School of Mathematics and Statistics

University of Melbourne, Australia

lhodgkinson@unimelb.edu.au

&Ryan Theisen

Harmonic Discovery

ryan@harmonicdiscovery.com

&Michael W. Mahoney

ICSI, LBNL, and Dept. of Statistics

University of California, Berkeley

mmahoney@stat.berkeley.edu

###### Abstract

As performance gains through scaling data and/or model size experience diminishing returns, it is becoming increasingly popular to turn to ensembling, where the predictions of multiple models are combined to improve accuracy. In this paper, we provide a detailed analysis of how the disagreement and the polarization (a notion we introduce and define in this paper) among classifiers relate to the performance gain achieved by aggregating individual classifiers, for majority vote strategies in classification tasks. We address these questions in the following ways. (1) An upper bound for polarization is derived, and we propose what we call a neural polarization law: most interpolating neural network models are 4/3-polarized. Our empirical results not only support this conjecture but also show that polarization is nearly constant for a dataset, regardless of hyperparameters or architectures of classifiers. (2) The error rate of the majority vote classifier is considered under restricted entropy conditions, and we present a tight upper bound that indicates that the disagreement is linearly correlated with the error rate, and that the slope is linear in the polarization. (3) We prove results for the asymptotic behavior of the disagreement in terms of the number of classifiers, which we show can help in predicting the performance for a larger number of classifiers from that of a smaller number. Our theoretical findings are supported by empirical results on several image classification tasks with various types of neural networks.

## 1 Introduction

As performance gains through scaling data and/or model size experience diminishing returns, it is becoming increasingly popular to turn to _ensembling_, where the predictions of multiple models are combined, both to improve accuracy and to form more robust conclusions than any individual model alone can provide. In some cases, ensembling can produce substantial benefits, particularly when increasing model size becomes prohibitive. In particular, for large neural network models, _deep ensembles_ are especially popular. These ensembles consist of independently trained models on the same dataset, often using the same hyperparameters, but starting from different initializations.

The cost of producing new classifiers can be steep, and it is often unclear whether the additional performance gains are worth the cost. Assuming that constructing two or three classifiers is relatively cheap, procedures capable of deciding whether to continue producing more classifiers are needed. To do so requires a precise understanding of how to predict ensemble performance. Of particular interest are majority vote strategies in classification tasks, noting that regression tasks can also be formulated in this way by clustering outputs. In this case, one of the most effective avenues forpredicting performance is the _disagreement_: measuring the degree to which classifiers provide different conclusions over a given dataset. Disagreement is concrete, easy to compute, and strongly linearly correlated with majority vote prediction accuracy, leading to its use in many applications. However, _a priori_, the precise linear relationship between disagreement and accuracy is unclear, preventing the use of disagreement for predicting ensemble performance.

Our goal in this paper is to go beyond disagreement-based analysis to provide a more quantitative understanding of the number of classifiers one should use to achieve a desired level of performance in modern practical applications, in particular for neural network models. In more detail, our contributions are as follows.

1. We introduce and define the concept of **polarization**, a notion that measures the higher-order dispersity of the error rates at each data point, and which indicates how polarized the ensemble is relative to the ground truth. We state and prove an upper bound for polarization (Theorem 1). Inspired by the theorem, we propose what we call a **neural polarization law** (Conjecture 1): most interpolating (Definition 2) neural network models are 4/3-polarized. We provide empirical results supporting the conjecture (Figures 1 and 2).
2. Using the notion of polarization, we develop a refined set of **bounds on the majority vote test error rate**. For one, we provide a sharpened bound for any ensembles with a **finite number** of classifiers (Corollary 1). For the other, we offer a new, tighter bound under an additional condition on the **entropy** of the ensemble (Theorem 4). We provide empirical results that demonstrate our new bounds perform significantly better than the existing bounds on the majority vote test error (Figure 3).
3. The **asymptotic behavior of the majority vote error rate** is determined as the number of classifiers increases (Theorem 5). Consequently, we show that we can predict the performance for a larger number of classifiers from that of a smaller number. We provide empirical results that show such predictions are accurate across various pairs of model architecture and dataset (Figure 4).

In Section 2, we define the notations that will be used throughout the paper, and we introduce upper bounds for the error rate of the majority vote from previous work. The next three sections are the main part of the paper. In Section 3, we introduce the notion of polarization, \(_{}\), which plays a fundamental role in relating the majority vote error rate to average error rate and disagreement. We explore the properties of the polarization and present empirical results that corroborate our claims. In Section 4, we present tight upper bounds for the error rate of the majority vote for ensembles that satisfy certain conditions; and in Section 5, we prove how disagreement behaves in terms of the number of classifiers. All of these ingredients are put together to estimate the error rate of the majority vote for a large number of classifiers using information from only three sampled classifiers. In Section 6, we provide a brief discussion and conclusion. Additional material is presented in the appendices.

## 2 Preliminaries

In this section, we introduce notation that we use throughout the paper, and we summarise previous work on the performance of the majority vote error rate.

### Notations

We focus on \(K\)-class classification problems, with features \(X\), labels \(Y[K]=\{1,2,...,K\}\) and feature-label pairs \((X,Y)\). A classifier \(h:[K]\) is a function that maps a feature to a label. We define the error rate of a single classifier \(h\), and the disagreement and the tandem loss  between two classifiers, \(h\) and \(h^{}\), as the following:

\[: \;L(h)=_{}[(h(X) Y)]\] \[: \;D(h,h^{})=_{}[(h(X) h ^{}(X))]\] \[: \;L(h,h^{})=_{}[(h(X) Y )(h^{}(X) Y)],\]

where the expectation \(_{}\) is used to denote \(_{(X,Y)}\). Next, we consider a distribution of classifiers, \(\), which may be viewed as an _ensemble_ of classifiers. This distribution can represent a variety of different cases. Examples include: (1) a discrete distribution over finite number of \(h_{i}\), e.g., a weighted sum of \(h_{i}\); and (2) a distribution over a parametric family \(h_{}\), e.g., a distribution of classifiers resulting from one or multiple trained neural networks. Given the ensemble \(\), the _(weighted) majority vote_\(h_{}^{}:[K]\) is defined as

\[h_{}^{}(x)=*{arg\,max}_{y[K]}_{}[ (h(x)=y)].\]

Again, \(_{}\) denotes \(_{h}\), and we use \(_{},_{^{2}},_{}\) for \(_{h},_{(h,h^{})^{2}},_{h }\), respectively, throughout the paper. In this sense, \(_{}[L(h)]\) represents _the average error rate_ under a distribution of classifiers \(\) and \(_{^{2}}[D(h,h^{})]\) represents _the average disagreement_ between classifiers under \(\). Hereafter, we refer to \(_{}[L(h)]\), \(_{^{2}}[D(h,h^{})]\), and \(L(h_{}^{})\) as the **average error rate**, the **disagreement**, and the **majority vote error rate**, respectively, with

\[L(h_{}^{})=_{}[(h_{}^{ }(X) Y)].\]

Lastly, we define the point-wise error rate, \(W_{}(X,Y)\), which will serve a very important role in this paper (for clarity, we will denote \(W_{}(X,Y)\) by \(W_{}\) unless otherwise necessary):

\[W_{}(X,Y)=_{}[(h(X) Y)].\] (1)

### Bounds on the majority vote error rate

The simplest relationship between the majority vote error \(L(h_{}^{})\) and the average error rate \(_{}[L(h)]\) was introduced in . It states that the error in the majority vote classifier cannot exceed twice the average error rate:

\[L(h_{}^{}) 2_{}[L(h)]\] (2)

A simple proof for this relationship can be found in  using Markov's inequality. Although (2) does not provide useful information in practice, it is worth noting that this bound is, in fact, tight. There exist pathological examples where \(h_{}^{}\) exhibits twice the average error rate (see Appendix C in ). This suggests that we can hardly obtain a useful or tighter bound by relying on only the "first-order" term, \(_{}[L(h)]\).

Accordingly, more recent work constructed bounds in terms of "second-order" quantities, \(_{^{2}}[L(h,h^{})]\) and \(_{^{2}}[D(h,h^{})]\). In particular,  and  designed a so-called _C-bound_ using the Chebyshev-Cantelli inequality, establishing that, if \(_{}[L(h)]<1/2\), then

\[L(h_{}^{})_{^{2}}[L(h,h^{})]- _{}[L(h)]^{2}}{_{^{2}}[L(h,h^{})]-_{}[L(h)]+}.\] (3)

As an alternative approach,  incorporated the disagreement \(_{^{2}}[D(h,h^{})]\) into the bound as well, albeit restricted to the binary classification problem, to obtain:

\[L(h_{}^{}) 4_{}[L(h)]-2_{^{2}}[D (h,h^{})].\] (4)

While (3) and (4) may be tighter in some cases, once again, there do exist pathological examples where this bound is as uninformative as the first-order bound (2). Motivated by these weak results,  take a new approach by restricting \(\) to be a "good ensemble," and introducing the _competence_ condition (see Definition 3 in our Appendix A). Informally, competent ensembles are those where it is more likely--in average across the data--that more classifiers are correct than not. Based on this notion,  prove that competent ensembles are guaranteed to have weighted majority vote error _smaller_ than the weighted average error of individual classifiers:

\[L(h_{}^{})_{}[L(h)].\] (5)

That is, the majority vote classifier is always beneficial. Moreover,  proves that any competent ensemble \(\) of \(K\)-class classifiers satisfy the following inequality.

\[L(h_{}^{})(_{}[L(h)]- _{^{2}}[D(h,h^{})]).\] (6)

We defer further discussion of competence to Appendix A, where we introduce simple cases for which competence does _not_ hold. In these cases, we show how one can overcome this issue so that the bounds (5) and (6) still hold. In particular, in Appendix A.3, we provide an example to show the bound (6) is tight.

The Polarization of an Ensemble

In this section, we introduce a new quantity, \(_{}\), which we refer to as the _polarization_ of an ensemble \(\). First, we provide examples as to what this quantity represents and draw a connection to previous studies. Then, we present theoretical and empirical results that show this quantity plays a fundamental role in relating the majority vote error rate to average error rate and disagreement. In Theorem 1, we prove an upper bound for the polarization \(_{}\), which highlights a fundamental relationship between the polarization and the constant \(\). Inspired from the theorem, we propose Conjecture 1 which we call a _neural polarization law_. Figures 1 and 2 present empirical results on an image recognition task that corroborates the conjecture.

We start by defining the polarization of an ensemble. In essence, the polarization is an improved (smaller) coefficient on the Markov's inequality on \(_{}(W_{}>0.5)\), where \(W_{}\) is the point-wise error rate defined as equation (1). It measures how much the ensemble is "polarized" from the truth, with consideration of the distribution of \(W_{}\).

**Definition 1** (Polarization).: _An ensemble \(\) is \(\)-polarized if_

\[\,_{}[W_{}^{2}]_{}(W_{ }>1/2).\] (7)

_The **polarization** of an ensemble \(\) is_

\[_{}:=_{}(W_{}>1/2)}{_{ }[W_{}^{2}]},\] (8)

_which is the smallest value of \(\) satisfies inequality (7)._

Note that the polarization always takes a value in \(\), due to the positivity constraint and Markov's inequality. Also note that ensemble \(\) with polarization \(_{}\) is \(\)-polarized for any \(_{}\).

To understand better what this quantity represents, consider the following examples. The first example demonstrates that polarization increases as the majority vote becomes more polarized from the truth, while the second example demonstrates how polarization increases when the constituent classifiers are more evenly split.

Example 1Consider an ensemble \(\) where 75% of classifiers output Label 1 with probability one, and the other 25% classifiers output Label 2 with probability one.

* **Case 1.**_The true label is Label 1 for the whole data._ In this case, the majority vote in \(\) results in zero error rate. The point-wise error rate \(W_{}\) is \(0.25\) on the entire dataset, and thus \(_{}(W_{}>0.5)=0\). The polarization \(_{}\) is \(0\).
* **Case 2.**_The true label is Label 1 for half of the data and is Label 2 for the other half._ In this case, the majority vote is only correct for half of the data. The point-wise error rate \(W_{}\) is \(0.25\) for this half, and is \(0.75\) for the other half. The polarization \(_{}\) is \(0.5/0.3125=1.6\).
* **Case 3.**_The true label is Label 2 for the whole data._ In this case, the majority vote in \(\) is wrong on every data point. The point-wise error rate \(W_{}\) is \(0.75\) on the entire dataset and thus \(_{}(W_{}>0.5)=1\). The polarization \(_{}\) is \(1/0.3125=3.2\).

Example 2Now consider an ensemble \(\) of which 51% of classifiers always output Label 1, and the other 49% classifiers always output Label 2.

* **Case 1.** The polarization \(_{}\) is now \(0\), the same as in Example 1.
* **Case 2.** The polarization \(_{}\) is \(0.5/0.2501 2\), which is larger than \(1.6\) in Example 1.
* **Case 3.** The polarization \(_{}\) is now \(1/0.2501 4\), which is larger than \(3.2\) in Example 1.

In addition, the following proposition draws a connection between polarization and the competence condition mentioned in Section 2.2. It states that the polarization of competent ensembles cannot be very large. The proof is deferred to Appendix A.2.

**Proposition 1**.: _Competent ensembles are \(2\)-polarized._

Now we delve more into this new quantity. We introduce Theorem 1, which establishes (by means of concentration inequalities) an upper bound on the polarization \(_{}\). The proof of Theorem 1 is deferred to Appendix B.1.

**Theorem 1**.: _Let \(\{(X_{i},Y_{i})\}_{i=1}^{m}\) be independent and identically distributed samples from \(\) that are independent of an ensemble \(\). Then the polarization of the ensemble, \(_{}\), satisfies_

\[_{}\{,(}++4SP}}{2S})^{2}\},\] (9)

_with probability at least \(1-\), where \(S=_{i=1}^{m}W_{}^{2}(X_{i},Y_{i})\) and \(P=_{i=1}^{m}(W_{}(X_{i},Y_{i})>1/2).f_

Surprisingly, in practice, \(_{}=\) appears to be a good choice for a wide variety of cases. See Figure 1 and Figure 2, which show the polarization \(_{}\) obtained from VGG11 , DenseNet40 , ResNet18, ResNet50 and ResNet101  trained on CIFAR-10  with various hyperparameters choices. The trend does not deviate even when evaluated on an out-of-distribution dataset, CIFAR-10.1 . For more details on these empirical results, see Appendix C.

**Remark.** We emphasize that values for \(_{}\) that are larger than \(\) does _not_ contradict Theorem 1. This happens when the non-constant second term in (9) is larger than \(\), which is often the case for classifiers which are not interpolating (or, indeed, that underfit or perform poorly).

**Definition 2** (Interpolating, ).: _A classifier is **interpolating** if it achieves an accuracy of 100% on the training data._

Putting Theorem 1 and the consistent empirical trend shown in Figure 2 together, we propose the following conjecture.

**Conjecture 1** (Neural Polarization Law).: _The polarization of ensembles comprised of independently trained interpolating neural networks is smaller than \(\)._

## 4 Entropy-Restricted Ensembles

In this section, we first present an upper bound on the majority vote error rate, \(L(h_{}^{})\), in Theorem 2, using our notion of polarization \(_{}\) which we introduced and defined in the previous section. Then, we present Theorems 3 and 4 which are the main elements in obtaining tighter upper bounds on \(L(h_{}^{})\). Figure 3 shows our proposed bound offers a significant improvement over state-of-the-art results. The new upper bounds are inspired from the fact that classifier prediction probabilities tend to concentrate on a small number of labels, rather than be uniformly spread over all the possible labels.

Figure 1: Polarizations \(_{}\) obtained from ResNet18 trained on CIFAR-10 with various sets of hyper-parameters tested on **(a)** an out-of-sample CIFAR-10 and **(b)** an out-of-distribution dataset, CIFAR-10.1. Red dashed line indicates \(y=4/3\), a suggested value of polarization appears in Theorem 1 and Conjecture 1.

This is analogous to the phenomenon of _neural collapse_. As an example, in the context of a computer vision model, when presented with a photo of a dog, one might expect that a large portion of reasonable models might classify the photo as an animal other than a dog, but not as a car or an airplane.

We start by stating an upper bound on the majority vote error, \(L(h_{}^{})\) as a function of polarization \(_{}\). This upper bound is tighter (smaller) than the previous bound in inequality (6) when the polarization is lower than \(2\), which is the case for competent ensembles. The proof is deferred to Appendix B.2.

**Theorem 2**.: _For an ensemble \(\) of \(K\)-class classifiers,_

\[L(h_{}^{})(K-1)}{K}( _{}[L(h)]-_{^{2}}[D(h,h^{})] ),\]

_where \(_{}\) is the polarization of the ensemble \(\)._

Based on the upper bound stated in Theorem 2, we add a restriction on the entropy of constituent classifiers to obtain Theorem 3. The theorem provides a tighter scalable bound that does _not_ have explicit dependency on the total number of labels, with a small cost in terms of the entropy of constituent classifiers. The proof of Theorem 3 is deferred to Appendix B.3.

**Theorem 3**.: _Let \(\) be any \(\)-polarized ensemble of \(K\)-class classifiers that satisfies \(_{}(h(x) A(x))\), where \(y A(x)[K]\) and \(|A(x)| M\), for all data points \((x,y)\). Then, we have_

\[L(h_{}^{})\ [ (1+)_{}[L(h)]- _{^{2}}[D(h,h^{})]].\]

While Theorem 3 might provide a tighter bound than prior work, coming up with pairs \((M,)\) that satisfy the constraint is not an easy task. This is not an issue for a discrete ensemble, however. If \(\) is a discrete distribution of \(N\) classifiers, then we observe that the assumption of Theorem 3 must always hold with \((M,)=(N\!+\!1,0)\). We state this as the following corollary.

**Corollary 1** (Finite Ensemble).: _For an ensemble \(\) that is a weighted sum of \(N\) classifiers, we have_

\[L(h_{}^{})\ N}{N\!+\!1}( _{}[L(h)]-_{^{2}}[D(h,h^{})] ),\] (10)

_where \(_{}\) is the polarization of the ensemble \(\)._

Figure 2: Polarization \(_{}\) obtained **(a)** from various architectures trained on CIFAR-10 and **(b)** only from interpolating classifiers trained on various datasets. Red dashed line indicates \(y=4/3\). In subplot **(b)**, we observe that the polarization of all interpolating models expect one are smaller than \(4/3\), which aligns with Conjecture 1.

See Figure 3, which provides empirical results that compare the bound in Corollary 1 with the C-bound in inequality (3), and with inequality (6) proposed in . We can observe that the new bound in Corollary 1 is strictly tighter than the others. For more details on these empirical results, see Appendix C.

Although the bound in Corollary 1 is tighter than the bounds from previous studies, it's still not tight enough to use it as an estimator for \(L(h_{}^{})\). In the following theorem, we use a stronger condition on the entropy of an ensemble to obtain a tighter bound. The proof is deferred to Appendix B.4.

**Theorem 4**.: _For any \(\)-polarized ensemble \(\) that satisfies_

\[_{}[_{^{2}}(h(X) Y,h^{}(X) Y,h(X) h^{}(X))]\, _{}[_{}(h(X) Y)],\] (11)

_we have_

\[L(h_{}^{})\,\,[(1+)\,_{ }[L(h)]-_{^{2}}[D(h,h^{})]].\]

The condition (11) can be rephrased as follows: compared to the error \(_{}(h(x) y)\), the entropy of the distribution of wrong predictions is small, and it is concentrated on a small number of labels. A potential problem is that one must know or estimate the smallest possible value of \(\) in advance. At least, we can prove that \(=\) always satisfies the condition (11) for an ensemble of \(K\)-class classifiers. The proof is deferred to Appendix B.4.

**Corollary 2**.: _For any \(\)-polarized ensemble \(\) of K-class classifiers, we have_

\[L(h_{}^{})\,\,[(1+ )_{}[L(h)]-_{^{2}}[D(h,h^{} )]].\]

Naturally, this \(\) is not good enough for our goal. We discuss more on how to estimate the smallest possible value of \(\) in the following section.

## 5 A Universal Law for Ensembling

In this section, our goal is to predict the majority vote error rate of an ensemble with large number of classifiers by just using information we can obtain from an ensemble with a small number, e.g., three,

Figure 3: Comparing our new bound from Corollary 1 (colored black), which is the right hand side of inequality (10), with bounds from previous studies. Green corresponds to the C-bound in inequality (3), and blue corresponds to the right hand side of inequality (6). ResNet18, ResNet50, ResNet101 models with various sets of hyperparameters are trained on CIFAR-10 then tested on **(a)** the out-of-sample CIFAR-10, **(b)** an out-of-distribution dataset, CIFAR-10.1.

of classifiers. Among the elements in the bound in Theorem 4,

\[\,[(1+)\,_{}[L(h)]-_{^ {2}}[D(h,h^{})]],\]

we plug in \(=\) as a result of Theorem 1; and since \(_{}[L(h)]\) is invariant to the number of classifiers, it remains to predict the behavior of \(_{^{2}}[D(h,h^{})]\) and the smallest possible value of \(\), \(_{}=_{}[_{^{2}} (h(X) Y,h^{}(X) Y,h(X) h^{}(X))]}{2 _{}[_{}(h(X) Y)]}\). Since the denominator \(_{}[_{}(h(X) Y)]= _{}[L(h)]\) is invariant to the number of classifiers, and the numerator resembles the disagreement between classifiers, \(_{}\) is expected to follow a similar pattern as \(_{^{2}}[D(h,h^{})]\). Note that the numerator of \(_{}\) has the same form as the disagreement, differing by only one less label. Both are \(V\)-statistics that can be expressed as a multiple of a \(U\)-statistic, as shown in equation (12). In the next theorem, we show that the disagreement for a finite number of classifiers can be expressed as the sum of a hyperbolic curve and an unbiased random walk. Here, \([x]\) denotes the greatest integer less than or equal to \(x\) and \(\) is the Skorokhod space on \(\) (see Appendix B.5).

**Theorem 5**.: _Let \(_{N}\) denote an empirical distribution of \(N\) independent classifiers \(\{h_{i}\}_{i=1}^{N}\) sampled from a distribution \(\) and \(_{1}^{2}=_{h}(_{h^{}} _{}(h(X) h^{}(X)))\). Then, there exists \(D_{}>0\) such that_

\[_{(h,h^{})_{N}^{2}}[D(h,h^{})]=(1- {N})(D_{}+}Z_{N}),\]

_where \(Z_{N}=0\), \(Z_{N}_{1}^{2}\) and \(\{}{_{1}}Z_{[Nt]}\}_{t}\) converges weakly to a standard Wiener process in \(\) as \(N\)._

Proof.: Let \((h_{i},h_{j})=_{}(h_{i}(X) h_{j}(X))\). We observe that

\[}{N(N\!-\!1)}_{(h,h^{})_{N}^{2 }}[D(h,h^{})]=_{i,j=1}^{N}_{}(h_{i}(X) h_{j}(X))\\ =_{i,j=1}^{N}(h_{i},h_{j})}{=}_{1 i<j N}(h_{i},h_{j }) U_{N},\] (12)

which is a \(U\)-statistic with the kernel function \(\). Let \(_{0}=_{(h,h^{})^{2}}(h,h^{})\).

The invariance principle of \(U\)-statistics (Theorem 7 in Appendix B.5) states that the process \(_{N}=(_{N}(t),t)\), defined by \(_{N}()=^{2}}}(U_{k}-_{0})\) and \(_{N}(t)=_{N}()\), converges weakly to a standard Wiener process in \(\) as \(N\!\!\), since \(_{1}^{2}=_{h}_{h^{}}(h,h^ {})\). Therefore, \(U_{N}\) converges in probability as \(N\!\!\) to \(D_{}_{0}\).

Letting \(Z_{N}\!=\!_{1}_{N}(1)\!=\!}{2}(U_{N}\!-\!D_{})\), we can express \(U_{N}\) as \(U_{N}\!=\!D_{}\!+\!}Z_{N}\), with \(Z_{N}\!=\!0\) and \(Z_{N}\!\!_{1}^{2}\). Since \(}{_{1}}Z_{[Nt]}\!=\!}\,_{N}()\!=\!}\,_{N}(t)\), it follows by Slutsky's Theorem that \(\{}{_{1}}Z_{[Nt]}\}_{t}\) converges weakly to a standard Wiener process in \(\) as \(N\!\!\). 

Theorem 5 suggests that the disagreement within \(N\) classifiers, \(_{_{N}^{2}}[D(h,h^{})]\), can be approximated as \(D_{}\). From the disagreement within \(M(\!N)\) classifiers, \(D_{}\) can be approximated as \(_{_{M}^{2}}[D(h,h^{})]\), and therefore we get

\[_{_{N}^{2}}[D(h,h^{})]_{_{M}^{2}}[D(h,h^{})].\] (13)

Assume that we have three classifiers sampled from \(\). We denote the average error rate, the disagreement, and the \(_{}\) from these three classifiers by \(_{3}[L(h)]\), \(_{3}[D(h,h^{})]\), and \(_{3}\), respectively. Then, from Theorem 4 and approximation (13) (which applies to both disagreement and \(_{}\)), we estimate the majority vote error rate of \(N\) classifiers from \(\) as the following:

\[L(h_{}^{}) [(1+ _{3})\,_{3}[L(h)]- _{3}[D(h,h^{})]]\] \[=[_{3}[L(h)]+( _{3}_{3}[L(h)]-_{3}[D(h,h^{}) ])].\] (14)

Alternatively, we can use the polarization measured from three classifiers, \(_{3}\), instead of \(=\), to obtain:

\[L(h_{}^{})=_{3}[_{3}[L(h)]+ (_{3}_{3}[L(h)]-_{3}[D(h,h^{ })])].\] (15)

Figure 4 presents empirical results that compare the estimated (extrapolated) majority vote error rates in equations (14) and (15) with the true majority vote error for each number of classifiers. ResNet18 models are tested on four different dataset: CIFAR-10, CIFAR-10.1, Fashion-MNIST  and Kuzushiji-MNIST  where the models are trained on the corresponding train data. MobileNet  is trained and tested on the MNIST  dataset. Not only do the estimators show significant improvement compared to the bounds introduced in Section 2.2, we observe that the estimators are very close to the actual majority vote error rate; and thus the estimators have practical usages, unlike the bounds from previous studies. In Figure 4(a2), existing bounds (3) and (6) are much larger compared to the average error rate. This is also the case for (architecture, dataset) pairs of other subplots.

Figure 4: Comparing the estimated (extrapolated) majority vote error rates in equation (14) (blue-dashed lines) and (15) (orange-dashed lines) with the true majority vote error (green solid line) for each number of classifiers. The solid sky-blue line corresponds to the average error rate of constituent classifiers. Subplots **(a1), (b), (c), (d), (e)** show the results from different pairs of (classification model, dataset). Subplot **(a2)** overlays the right hand side of inequality (3) (C-bound, colored red) and inequality (6) ( bound, colored purple) on the subplot **(a1)**. These two quantities from previous studies are much larger compared to the average error rate. We see the same pattern for other (architecture, dataset) pairs, which we therefore omit from the plot. For more details on these empirical results, see Appendix C.

Discussion and Conclusion

This work addresses the question: how does the majority vote error rate change according to the number of classifiers? While this is an age-old question, it is one that has received renewed interest in recent years. On the journey to answering the question, we introduce several new ideas of independent interest. (1) We introduced the polarization \(_{}\), of an ensemble of classifiers. This notion plays an important role throughout this paper and appears in every upper bound presented. Although Theorem 1 gives some insight into polarization, our conjectured neural polarization law (Conjecture 1) is yet to be proved or disproved, and it provides an exciting avenue for future work. (2) We proposed two classes of ensembles whose entropy is restricted in different ways. Without these constraints, there will always be examples that saturate even the least useful majority vote error bounds. We believe that accurately describing how models behave in terms of the entropy of their output is key to precisely characterizing the behavior of majority vote, and likely other ensembling methods.

Throughout this paper, we have theoretically and empirically demonstrated that polarization is fairly invariant to the hyperparameters and architecture of classifiers. We also proved a tight bound for majority vote error, under an assumption with another quantity \(\), and we presented how the components of this tight bound behave according to the number of classifiers. Altogether, we have sharpened bounds on the majority vote error to the extent that we are able to identify the trend of majority vote error rate in terms of number of classifiers.

We close with one final remark regarding the metrics used to evaluate an ensemble. Majority vote error rate is the most common and popular metric used to measure the performance of an ensemble. However, it seems unlikely that a practitioner would consider an ensemble to have performed adequately if the majority vote conclusion was correct, but was only reached by a relatively small fraction of the classifiers. With the advent of large language models, it is worth considering whether the majority vote error rate is still as valuable. The natural alternative in this regard is the probability \(_{}(W_{}>1/2)\), that is, the probability that at least half of the classifiers agree on the correct answer. This quantity is especially well-behaved, and it frequently appears in our proofs. (Indeed, every bound presented in this work serves as an upper bound for \(_{}(W_{}>1/2)\).) We conjecture that this quantity is useful much more generally.

Acknowledgements.We would like to thank the DOE, IARPA, NSF, and ONR for providing partial support of this work.