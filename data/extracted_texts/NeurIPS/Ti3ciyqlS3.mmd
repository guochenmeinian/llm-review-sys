# Improving Temporal Link Prediction via

Temporal Walk Matrix Projection

 Xiaodong Lu

CCSE Lab, Beihang University

Beijing, China

xiaodonglu@buaa.edu.cn

&Leilei Sun

CCSE Lab, Beihang University

Beijing, China

leileisun@buaa.edu.cn

Corresponding Author.

Tongyu Zhu

CCSE Lab, Beihang University

Beijing, China

zhutongyu@buaa.edu.cn

&Weifeng Lv

CCSE Lab, Beihang University

Beijing, China

lwf@buaa.edu.cn

###### Abstract

Temporal link prediction, aiming at predicting future interactions among entities based on historical interactions, is crucial for a series of real-world applications. Although previous methods have demonstrated the importance of relative encodings for effective temporal link prediction, computational efficiency remains a major concern in constructing these encodings. Moreover, existing relative encodings are usually constructed based on structural connectivity, where temporal information is seldom considered. To address the aforementioned issues, we first analyze existing relative encodings and unify them as a function of temporal walk matrices. This unification establishes a connection between relative encodings and temporal walk matrices, providing a more principled way for analyzing and designing relative encodings. Based on this analysis, we propose a new temporal graph neural network called TPNet, which introduces a temporal walk matrix that incorporates the time decay effect to simultaneously consider both temporal and structural information. Moreover, TPNet designs a random feature propagation mechanism with theoretical guarantees to implicitly maintain the temporal walk matrices, which improves the computation and storage efficiency. Experimental results on 13 benchmark datasets verify the effectiveness and efficiency of TPNet, where TPNet outperforms other baselines on most datasets and achieves a maximum speedup of \(33.3\) compared to the SOTA baseline. Our code can be found at https://github.com/lxd99/TPNet.

## 1 Introduction

Many real-world dynamic systems can be abstracted as a temporal graph , where entities and interactions among them are denoted as nodes and edges with timestamps respectively. Temporal link prediction, aiming at predicting future interactions based on historical interactions, is a fundamental task for temporal graph learning, which can not only help us understand the evolution pattern of the temporal graph but also is crucial for a series of real-world tasks such as recommendations for online platforms  and information diffusion prediction .

Relative encodings have become an indispensable module for effective temporal link prediction  where, without them, node representations computed independently by neighbor aggregation will failto capture the pairwise information. As the toy example shown in Figure 1, A and F will have the same node representation due to sharing the same local structure. Thus it can not be determined whether D will interact with A or F at \(t_{3}\) according to their representations. However, by assigning nodes with relative encodings (i.e., additional node features) specific to the target link before computing the node representation, we can highlight the importance of each node and guide the representation learning process to extract pairwise information. For example, in Figure 1, we can infer from the relative encoding of E (in red circle) that D is more likely to interact with F than with A since D and F share a common neighbor, E (detailed discussed in Section 2.2). Although achieving remarkable success, injecting pairwise information based on relative encodings is still far from satisfactory.

Figure 1: Without relative encodings, the learned node representations fail to capture the correlation between nodes. (For each link \((u,v,t)\), the relative encoding here for a node \(w\) is [\(g(u,w),g(v,w)\)], where \(g(u,w)=1\) if there is an interaction between u and w before t, otherwise \(g(u,w)=0\).)

### Definitors

**Definition 1** (Temporal Graph).: A temporal graph can be considered as a sequence of non-decreasing chronological interactions \(=[(\{u_{1},v_{1}\},t_{1})\,(\{u_{2},v_{2}\},t_{2})\,]\) with \(0 t_{1} t_{2}\), where \((\{u_{i},v_{i}\},t_{i})\) can be considered as a undirected link between \(u_{i}\) and \(v_{i}\) with timestamp \(t_{i}\). Each node \(u\) can be associated with node feature \(_{u}^{d_{N}}\), and each interaction \((\{u,v\},t)\) has link feature \(_{u,v}^{t}^{d_{E}}\), where \(d_{N}\) and \(d_{E}\) denote the dimensions of the node feature and link feature.

**Definition 2** (Temporal Link Prediction).: The interaction sequence reflects the graph dynamics, and thus the ability of a model to capture the evolution pattern of a dynamic graph can be evaluated by how accurately it predicts the future interactions based on historical interactions. Formally, given the interactions before \(t\) (i.e., \(\{(\{u^{},v^{}\},t^{})|t^{}<t\}\)) and two nodes \(u\), \(v\), the temporal link prediction task aims to predict whether there will be an interaction between \(u\) and \(v\) at \(t\).

**Definition 3** (K-hop Subgraph).: We use the notation \((t)=((t),(t))\) to denote the graph snapshot at \(t\), where \((t)\) includes all the interactions that happen before \(t\) and \((t)\) includes all the nodes appear in \((t)\). Besides, we defined the k-hop subgraph of node \(u\) as \(_{u}^{k}(t)=(_{u}^{k}(t),_{u}^{k}(t)\), where \(_{u}^{k}(t)(t)\) is the set of nodes whose shortest path distance to \(u\) is less than \(k\) on \((t)\) and \(_{u}^{k}(t)(t)\) is the set of interactions between \(_{u}^{k}(t)\).

**Definition 4** (Temporal Walk).: A k-step temporal walk \(W\) on \((t)\) is a sequence of node-time pair with decreased temporal order , which can be denoted as \(W=[(w_{0},t_{0}),,(w_{k},t_{k})]\) with \(t=t_{0}>t_{1}>>t_{k}\) and \((\{w_{i},w_{i+1}\},t_{i+1})\) is an edge on \((t)\) for \(0 i k-1\). Figure 2 shows a visual illustration of a temporal walk. Here, we stipulate the first timestamp \(t_{0}\) as the current time \(t\) to avoid undefinedness of \(t_{0}\). We use the notation \(_{u,v}^{k}(t)\) to denote the set of all k-step temporal walks from \(u\) to \(v\) on \((t)\). Specially, \(_{u,w}^{0}(t)=\{[(u,t)]\}\) if \(u=w\) and \(_{u,w}^{0}(t)=\) otherwise. When there is no ambiguity, we replace \(_{u,v}^{k}(t)\) with \(_{u,v}^{k}\).

### Relative Encoding for Dynamic Link Prediction

#### 2.2.1 Unified Framework

Given a future link \((u,v,t)\) to be predicted, existing temporal link prediction methods (referred as node-wise methods) usually first learn the node representations \(_{u}(t)\) and \(_{v}(t)\) independently, which are obtained by encoding their k-hop subgraphs,

\[_{u}(t)=f_{}(_{u}^{k}(t),_{u,k}^{N},_{u,k}^{E}),\ \ \ \ _{v}(t)=f_{}(_{v}^{k}(t),_{v,k}^{N},_{v,k}^{E}),\] (1)

where \(_{u,k}^{N}\) and \(_{u,k}^{E}\) are the features of nodes and edges in \(_{u}^{k}(t)\) respectively 2. The \(f_{}()\) here can be any encoding function that maps a graph into a representation such as a k-layer GNN with a pooling layer Then the link likelihood \(p_{u,v}^{t}\) is given \(p_{u,v}^{t}=f_{}(_{u}(t),_{v}(t))\). The \(f_{}()\) is a decoding function that maps the node representations into link likelihood such as an MLP with a Sigmoid output layer. Detailed discussion about existing methods can be found in Appendix F.2.

As mentioned in the Section 1, learning representations independently might fail to capture the pairwise information of the given link. Thus recent methods (referred as link-wise methods) inject the pairwise information by constructing a relative encoding \(^{w|(u,v)}\) for each node \(w_{u}^{k}(t)_{v}^{k}(t)\) as an additional node feature (Detailed construction way for different methods will be introduced in Section 2.2.2). Then Equation (1) will be changed into

\[_{u}(t)=f_{}(_{u}^{k}(t),_{u,k}^{N}_{u,k}^{R},_{u,k}^{E}),\ \ \ \ _{v}(t)=f_{}(_{v}^{k}(t),_{v,k}^{N}_{v,k}^{R},_{v,k}^{E}),\] (2)

where \(_{u,k}^{R}\) is the relative encodings for nodes in \(_{u}^{k}(t)\) and \(_{u,k}^{N}_{u,k}^{R}\) indicate the new node features obtained by concatenating \(_{u,k}^{N}\) and \(_{u,k}^{R}\). Intuitively, the relative encoding \(^{w|(u,v)}\) for each node \(w\) reflects its importance to predicted link \((u,v,t)\), which can guide the representation learning process to extract the pairwise information specific to the predicted link from the subgraph. For the detailed construction way, the relative encoding \(^{w|(u,v)}\) is the concatenation of two similarity features \(^{w|u}\) and \(^{w|v}\), where \(^{w|u}/^{w|v}\) encodes some form of similarity between \(u/v\) and \(w\) (e.g., the number of k-step temporal walks from \(u\) to \(w\)). Although the designs of similarity featurefor different methods are induced from different heuristics, we find that they can be unified in a function about the temporal walk matrix, which is

\[^{w|u}=g([A_{u,w}^{(0)}(t),A_{u,w}^{(1)}(t),,A_{u,w}^{(k)}(t)]), A _{u,w}^{(i)}(t)=_{W_{u,w}^{i}}s(W)\;\;\;\;0 i  k.\] (3)

The \(s()\) here is a score function that maps a temporal walk to a scalar and \(^{(i)}(t)\) denotes an i-hop temporal walk matrix whose each entry \(A_{u,w}^{(i)}\) is the sum of the score of all i-step temporal walks from \(u\) to \(w\). \(g()\) is a function applied on the vector of \([A_{u,w}^{(0)}(t)\), \(A_{u,w}^{(1)}(t),,A_{u,w}^{(k)}(t)]^{k+1}\) to extract similarity feature. The above equation shows that each relative encoding implies a construction of the temporal walk matrix based on weighting each temporal walk (i.e., \(s()\)). Next, we will analyze existing relative encodings and show how they can be represented in the form of Equation (3).

#### 2.2.2 Analysis of Existing Methods

Our analysis focuses on the four representative link-wise methods DyGFormer, PINT, NAT, and CAW, which cover all the link-wise methods in the benchmark of dynamic link prediction .

**DyGFormer**. The \(^{w|u}\) of DyGFormer is a one-dimensional vector representing the number of links between \(w\) and \(u\). For DyGFormer, we can first set the \(s()\) to be a one-const function (i.e., \(s(W)=1\) in for any \(W\)), which will make \(A_{u,w}^{(k)}\) be the number of the k-step temporal walks from \(u\) to \(w\). Then, setting \(g()\) to be a function that selects the second number of a vector (i.e., \(g([x_{0},x_{1},..,x_{k}])=x_{1}\)) will make Equation (3) generate the similarity feature of DyGFormer.

**PINT**. The \(^{w|u}\) of PINT is a (\(k+1\))-dimensional vector, where \(r_{i}^{w|u}\) is the number of \((i-1)\)-step temporal walks from \(u\) to \(w\) for \(1 i k+1\). Setting \(s()\) and \(g()\) can be set to a one-const function and an identity function respectively will make Equation (3) outputs the similarity feature of PINT.

**NAT**. NAT maintains a series of fix-sized hash maps \(s_{0}^{(u)},...,s_{u}^{(k)}\) and generates the \(^{w|u}\) based on the hash maps. As proved in Appendix A.1, if the size of the hash maps becomes infinite, the \(^{w|u}\) is equivalent to a \((k+1)\)-dimensional vector, where, for \(1 i k+1\), \(r_{i}^{w|u}=1\) if the shortest temporal walk from \(u\) to \(w\) is less than \(i\); otherwise, \(r_{i}^{w|u}=0\). Let \(h()\) be a binary function where \(h(x)=1\) if \(x>0\) and \(h(x)=0\) otherwise. Setting the \(s()\) to be a one-const function and \(g()\) to a function of \(g([x_{0},x_{1},...,x_{k}])=[h(_{i=0}^{0}x_{i}),h(_{i=0}^{1}x_{i}),...,h (_{i=0}^{k}x_{i})]\) can make the Equation (3) generate the similarity feature of NAT.

**CAWN**. The similarity feature \(^{w|u}\) of CAWN reflects the probability of a node \(w\) being visited when performing a temporal walk from \(u\). Specifically, CAWN first samples a set of temporal walks beginning from \(u\) based on a causal sampling strategy. Then, for each node \(w\), the similarity feature \(^{w|u}\) is extracted based on its occurrence in the sampled walks. As proved in Appendix A.2, the similarity feature \(^{w|u}\) is the estimation of a (\(k+1\))-dimensional vector, where, for \(1 i k+1\), \(r_{i}^{w|u}\) is the probability of visiting \(w\) through a (\(i-1\))-step temporal walk matrix based on the sampling strategy of CAWN, which can be represented as \(r_{i}^{w|u}=_{W_{u,w}^{i-1}}s^{}(W)\). The value \(s^{}(W)\) for a given temporal walk \(W=[(w_{0},t_{0}),(w_{1},t_{1}),...,(w_{k},t_{k})]\) is defined as \(_{i=0}^{k-1}-t_{i+1}))}{_{(i=,),^{ } w_{i},}(-(t_{i}-^{}))}\), where \(\) is hyperparameter to control the sampling process, \(_{w_{i},t_{i}}=\{(\{w^{},w\},^{})|^{}<t_ {i}\}\) is the set of interactions attached to \(w_{i}\) before \(t_{i}\), and \(-t_{i+1}))}{_{(i=^{},w),^{} w _{i},t_{i}}}\) can be considered as the probability of moving from \((w_{i},t_{i})\) to \((w_{i+1},t_{i+1})\) in the sampling process. Setting \(s()\) to \(s^{}()\) and \(g()\) to be an identity function can make Equation (3) generate the similarity feature of CAWN.

**Conclusion**. According to the above analysis, we can conclude that (i) Equation (3) provides a unified view to consider the injection of pairwise information from the construction of temporal walk matrix, where different relative encodings (implicitly or explicitly) correspond to a kind of temporal walk matrix. (ii) Examining existing relative encodings from the unified view reveals their limitations. First, the relative encodings of existing methods (except CAWN) ignore the temporal information carried by each temporal walk, where their score function \(s()\) always yield \(1\) and the entries of the temporal walk matrix is just the count of the temporal walks. Next, although CAWN considers temporal information, they estimate the temporal walk matrix from the sampled temporal walks, which needs time-consuming graph sampling and may introduce large estimation errors. In the next section, we will present a new temporal walk matrix to simultaneously consider the temporal and structural information and show how to efficiently maintain the proposed temporal walk matrix.

## 3 Methodology

TPNet mainly consists of two modules: Node Representation Maintaining (NRM) and Link Likelihood Computing (LLC). The NRM is responsible for encoding the pairwise information, which maintains a series of node representations. Such representations will be updated when new interaction occurs and can be used to decode the temporal walk information between two nodes. The LLC module is a prediction module, which utilizes the maintained node representations and auxiliary information (e.g., like features) to compute the likelihood of the link to be predicted.

### Node Representation Maintaining

**Temporal Matrix Construction**. Based on the Equation (3), designing a temporal walk matrix relies on the definition of the score function \(s()\), where the element of a temporal walk matrix is \(A_{u,v}^{(k)}(t)=_{W M_{u,v}^{k}}s(W)\). Unlike most previous methods that only count the number of temporal walks, we consider the temporal information carried by a temporal walk in \(s()\) to simultaneously consider the temporal and structural information. Formally, let \(t\) be the current time, given a temporal walk \(W=[(w_{0},t_{0}),(w_{1},t_{1}),..,(w_{k},t_{k})]\), the value of the score function is \(s(W)=_{i=1}^{k}e^{-(t-t_{i})}\), where \(>0\) is a hyperparameter to control the time decay weight. As the current time \(t\) goes on, for each interaction \((\{w_{i},w_{i+1}\},t_{i+1})\) in the temporal walk \(W\), its weight \(e^{-(t-t_{i})}\) will decay exponentially. The design of the score function is motivated by the widely observed time decay effect [1; 10] on the temporal graph, where the importance of interactions will decay as time goes on, benefiting better modeling the graph evolution patterns. In the following part of Section 3, the notation of \(s()\) and \(^{(k)}(t)\) will specifically refer to the score function and temporal walk matrix proposed in this part. Besides, for \(^{(0)}(t)\), we stipulate it as an identity matrix constantly.

**Node Representation Maintaining**. Directly computing the temporal walk matrices is impractical since we need to enumerate the temporal walks and each matrix needs expensive \(O(n n)\) space complexity to store. Thus, we implicitly maintain the temporal walk matrices by maintaining a series of node representations \(^{(0)}(t),^{(1)}(t),...,^{(k)}(t)^{n d_{ R}}\), where \(d_{R} n\) is the dimension of node representations and \(_{u}^{(l)}(t)^{d_{R}}\) encodes the information about the \(l\)-step temporal walks beginning from \(u\) for \(0 l k\). The node representations will be updated when a new interaction occurs. Specifically, we first construct a random feature matrix \(^{n d_{R}}\), where each entry of \(\) is independently drawn from Gaussian distribution with mean 0 and variance \(}\). Then we initialize \(^{(0)}(t)\) as \(\) and \(^{(1)}(t),^{(2)}(t),...,^{(k)}(t)\) as zero matrix. When a new interaction \((u,v,t)\) occurs, for \(1 l k\), we update the representations of \(u\) and \(v\) by

\[_{u}^{(l)}(t^{+})=_{u}^{(l)}(t)+e^{ t}*_{v}^{(l-1)}( t), 14.226378pt_{v}^{(l)}(t^{+})=_{v}^{(l)}(t)+e^{  t}*_{u}^{(l-1)}(t),\] (4)

where the \(t^{+}\) denotes the time right after \(t\). A pseudocode for maintaining the node representations is shown in Algorithm 1. The maintaining mechanism here can be considered as a random feature propagation mechanism on the temporal graph, where we initialize the zero layer's representation

Figure 2: A illustration of the temporal walk.

of each node as a random feature and repeatedly propagate these features among nodes from the low layer to the high layer as interactions continuously appear. The following theorem shows the relationship between the node representations and the temporal walk matrices.

**Theorem 1**.: _If any two interactions on temporal graph \(\) have different timestamps, the obtained representations \(^{(0)}(t),^{(1)}(t),...,^{(k)}(t)\) satisfy \(e^{- lt}*^{(l)}(t)=^{(l)}(t)\) for \(0 l k\)._

For simplicity, we assume the timestamps of the interaction are different, and we show how to update the representations when multiple interactions have the same timestamps in Appendix C. We leave the proof in the Appendix B.1. The above theorem shows that the obtained node representation is the projection (i.e. linear transformation) of the temporal walk matrices, where the transform matrix is the initial random feature matrix \(\). The next theorem shows that the node representations preserve the inner product of the temporal walk matrices.

**Theorem 2**.: _Given any \((0,1)\), let \(\|\|_{2}\) denote the L2 norm, \(c_{u,v}^{l_{1},l_{2}}\) denote \((\|_{u}^{(l_{1})}(t)\|_{2}^{2}+\|_{v}^{(l_{2})}(t)\|_{ 2}^{2})\), and \(}^{(l)}(t)\) denote \(e^{- lt}*^{(l)}(t)\), if dimension \(d_{R}\) of node representations satisfy \(d_{R}}(4^{1/3}(k+1)n)\), then for any \(1 u,v n\), and \(0 l_{1},l_{2} k\), we have_

\[(|}_{u}^{(l_{1})}(t),}_{v}^{ (l_{2})}(t)-_{u}^{(l_{1})}(t),_{v}^{(l_{2})}(t) | c_{u,v}^{l_{1},l_{2}}) 1-,\] (5)

_where \(,\) denotes the inner product and \(||\) denotes taking absolute value._

We leave the proof in Appendix B.2. The above theorem shows that the inner product of the node representations is approximately equal to the inner product of the temporal walk matrices (i.e., \(}_{u}^{(l_{1})}(t),}_{v}^{(l_{2})}(t) _{u}^{(l_{1})}(t),_{v}^{(l_{2})}(t)\)). Specifically, given any error rate \(\), if the dimension of the node representations satisfies a certain condition, the difference between \(}_{u}^{(l_{1})}(t),}_{v}^{(l_{2})}(t)\) and \(_{u}^{(l_{1})}(t),_{v}^{(l_{2})}(t)\) for any \(u,v,l_{1},l_{2}\) will be less than \( c_{u,v}^{l_{1},l_{2}}\) with high probability (\( 1-\)). The error rate can approach 0 infinitely and thus the inner product of the node representations can approach that of temporal walk matrices infinitely, at the cost of increasing the dimension \(d_{R}\). As we will see in Section 4.3, a small dimension (\( n\)) in practice is enabled to make the inner product of node representations a good estimation of that of temporal walk matrices. Additionally, due to the i-th row of \(^{(0)}(t)\) being the one-hot encoding of i (since it is an identity matrix), we have \(_{u}^{(l)}(t),_{w}^{(0)}(t)=A_{u,w}^{(l)}(t)\). Thus, we can obtain \([A_{u,w}^{(0)}(t),,A_{u,w}^{(k)}(t)]\) in Equation (3) by calculating the inner product between all layers of \(u\)'s representation and the zero layer of \(w\)'s representation, expressed as \([}_{u}^{(0)}(t),}_{w}^{(0)}(t),, }_{u}^{(k)}(t),}_{w}^{(0)}(t)]\).

**Remark.** Compared to directly computing the temporal walk matrices \(^{(0)}(t),..,^{(k)}(t)\), which needs to enumerate the temporal walks and \(O((k+1)n^{2})\) space complexity to store, maintaining the node representations largely improve the computation and storage efficiency, which only needs \(O((k+1)nd_{R})\) space complexity to store and \(O(kd_{R})\) time complexity to update when a new interaction occurs. Actually, Theorem 2 is the direct result of Theorem 1 based on Johnson-Lindenstrauss Lemma , where the random projection can preserve the inner product and norm . Notably, the method for implicitly maintaining temporal walk matrices via random feature propagation can be extended to other types of temporal walk matrices, provided they meet a specific condition (i.e., the updating function of the temporal walk matrix can be written as the linear combination of its rows). This condition is not restrictive, and all the temporal walk matrices discussed in Section 2 fulfill it. We show their propagation mechanism and related discussion in Appendix D. In conclusion, the unified function in Equation (3), combined with methods of implicitly maintaining the temporal walk matrices, provides a new way to design a more effective and efficient way to inject pairwise information.

### Link Likelihood Computing

Given an interaction \((u,v,t)\) to be predicted, we compute its happening likelihood based on the obtained node representations and auxiliary features. Specifically, we first decode a pairwise feature \(_{u,v}(t)\) from the node representations obtained in Section 3.1. Then we compute the node embeddings \(_{u}(t)\) and \(_{v}(t)\) for node \(u\) and \(v\) respectively based on their historical interactions. Finally, we give the link likelihood based on \(_{u}(t),_{v}(t),_{u,v}(t)\). For notation simplicity, we omit the suffix of \(_{u}(t),_{v}(t),_{u,v}(t)\) and denote them as \(_{u},_{v},_{u,v}\) in the following part.

**Pairwise Feature Decoding**. Although we can obtain the \((k+1)\)-dimensional feature in Equation (3) by calculating the inner product between the zero-layer representation of one node and all layers of the other node's representation, this method does not consider the correlation between all layers of both nodes. Therefore, we use representations from all layers to decode the pairwise information. Specifically, we first take the node representation of u and v from different layers, which can be denoted as \(_{*}=[e^{- 0t}_{*}^{(0)},...,e^{- kt}_{*}^{(k)}] ^{(k+1) d_{R}}\) with * could be u or v. Then we concatenate them together to get \(_{u,v}=[_{u},_{v}]^{2(k+1) d_{R}}\) and obtain the raw pairwise feature \(}_{u,v}\) by computing the inner product among different rows of \(_{u,v}\), which is \(}_{u,v}=(_{u,v}_{u,v}^{T})\) with flat\(()\) means flatten a matrix of \(^{2(k+1) 2(k+1)}\) into a vector of \(^{4(k+1)^{2}}\). Finally, we feed the raw pairwise feature \(}_{u,v}\) into an MLP to get the pairwise feature \(_{u,v}\), which is \(_{u,v}=(((}_{u,v})+1))\). The ReLU\(()\) here is used to reduce estimation error, where the inner product of temporal walk matrices should be larger than zero and we thus set it to be zero if the inner product of the node representations is negative. The \(()\) is used to scale the raw pairwise feature, where the range of the inner product between different layers varies greatly and the \(+1\) is the shift term to avoid the undefined value of \((0)\). We will see in Section 4.3 that these two operations can improve the training stability.

**Auxiliary Feature Learning**. The auxiliary features such as link features also provide rich information for modeling the evolution patterns of the temporal graph. In this part, we follow the previous methods [13; 9] and consider the auxiliary feature learning as a sequential learning problem. Specifically, for node \(u\), we take its recent \(m\) interactions \(S_{u}=[(\{u,w_{1}\},t_{1}),...,(\{u,w_{m}\},t_{m})]\) before \(t\) and learn node embedding \(_{u}\) from this sequence. We first fetch the node features \(_{u,N}=[_{w_{1}},...,_{w_{m}}]^{m d_{N}}\) and edge features \(_{u,E}=[_{u,w_{1}}^{t_{1}},...,_{u,w_{1}}^{t_{m}}]^{m d_{E}}\). For timestamps, we map the timestamps into temporal features \(_{u,T}=[(t-t_{1}),..,(t-t_{n})]^{m d_{T}}\) like , where \(( t)=[(w_{1} t),..,(w_{d_{T}} t)]\) is a time encoding function to learn the periodic temporal pattern. Besides, we construct a relative encoding sequence \(_{u,F}=[_{u,w_{1}}_{v,w_{1}},...,_{u,w_{m}} _{v,w_{m}}]^{m 8(k+1)^{2}}\) to inject the pairwise features, where \(_{u,w_{m}}\) denote the pairwise feature of \(u\) and \(w_{m}\) and \(\) is the concatenation operation. After obtaining the above feature sequence, we concatenate them together and feed it into an MLP to get the final feature sequence \(_{u}^{(0)}=([_{u,N},_{u,E},_{u,T},_{u, F}])^{m d}\). Subsequently, we stack \(l\) layers of MLP-Mixer  to capture the temporal and structural dependencies within the feature sequence, which is

\[}_{u}^{(l)} =_{u}^{(l-1)}+_{1}^{(l)}(_{2}^{(l)} (_{u}^{(l-1)}))\] (6) \[_{u}^{(l)} =}_{u}^{(l)}+_{3}^{(l)}(_{4}^ {(l)}(}_{u}^{(l)})).\]

Finally, we get the node embedding by mean pooling \(_{u}=(_{u}^{(l)})\). The procedure to get node embedding \(_{v}\) is similar and for the node that does not have \(m\) interactions, we pad the feature sequence with zero. Then, the likelihood of the link \((u,v,t)\) is given by \(p_{u,v}^{}=([_{u},_{v},_{u,v}])\), where \(()\) is a 2-layer MLP model with Sigmoid activation function in its output layer.

## 4 Experiments

### Experimental Settings

**Datasets and Baselines**. We conduct experiments on 13 benchmark datasets for temporal link prediction, which are Wikipedia, Reddit, MOOC, LastFM, Enron, Social Evo., UCI, Flights, Can. Parl., US Legis., UN Trade, UN Vote and Contact. Eleven popular temporal graph learning methods are selected as baselines including JODIE, DyRep, TGAT, TGN, CAWN, EdgeBank, TCL, GraphMixer, NAT, PINT, and DyGFormer. Details about datasets and baselines can be found in Appendix F.

**Task Settings**. The task settings strictly follow . Specifically, we conduct experiments under two settings: 1) the transductive setting that predicts links between nodes that have been seen during training and 2) the inductive setting that predicts links between nodes that are not seen during training. Three different negative sampling strategies introduced by  are used to sample the negative links and the Average Precision (AP) and Area Under the Receiver Operating Characteristic Curve (AUC-ROC) are adopted as the evaluation metrics. For dataset splitting, we chronologically split

[MISSING_PAGE_FAIL:8]

speedups compared to the SOTA baseline DyGFormer and CAWN respectively on LastFM. The CAWN and DyGFormer models utilize time-consuming graph queries (e.g., temporal walk sampling) to construct relative encodings, which constitute the main computational bottleneck and consume over \(70\%\) of the running time. In contrast, TPNet caches historical interactions into node representations and constructs pairwise features based on these representations, thereby enhancing efficiency.

**Scalability Analysis**. To further verify the scalability of TPNet, we generated a series of random graphs with an average degree fixed at 100 and the number of edges varying from \(15\) to \(18\). Figure 4 shows the change of running time and GPU memory of TPNet, where the growth of the running time and GPU memory is close to and less than the linear growth curve respectively, showing the good scalability of TPNet. In contrast, PINT explicitly stores the temporal walk matrices and encounters out-of-memory error when the edge number reaches \(17\) (shown inAppendix G.3), verifying the impracticability of explicitly storing temporal walk matrices.

### Ablation Study

**Proposed Components**. To verify the effectiveness of the proposed components in TPNet, we compare TPNet with the following variants: 1) w/o NR that remove the node representations and the corresponding features that decoded from them. 2) w/o Time that only considers the structural information by setting the time decay weight \(\) to be zero. 3) w/o Scale that remove the \(()\) and \(()\) in the pairwise feature decoding. As shown in Table 2, there is a dramatic performance drop of w/o NR, which shows that the pairwise information carried by the node representations plays a vital role in the performance of TPNet. There is also an obvious performance drop of w/o Time, which confirms the necessity of incorporating temporal information in temporal walk matrix construction. Besides, the unreasonable poor performance of the w/o Scale is due to the various distribution of node representations across different layers, where, without scaling the raw pairwise features, the training will be unstable, and numerical overflow errors may even occur on some datasets. Further details on the distribution of node representations from different layers are provided in Appendix G.5.

**Dimension Change**. To verify the influence of the node representation dimension. We vary the dimensions from 1 to 128 and report the performance of TPNet (denoted as TPNet-d). As shown in Figure 3, the required dimension of node representations is small, where only 1-dimensional and 16-dimensional node representations can achieve satisfactory performance on MOOC and LastFM respectively. For different datasets, we observe that the average degree may be a main influence of the required dimension, where sparse graphs (like MOOC and Wikipedia) only need a small dimension, and dense graphs (like LastFM and Enron) may require a larger dimension. Empirically, setting the dimension to be \(10*(2)\) is enough to achieve satisfactory performance on all datasets, where \(E\) is the number of edges. Results on more datasets can be found in Appendix G.4.

## 5 Related Work

Temporal link prediction  aims at predicting future interactions based on historical topology, which is crucial for a series of real-world applications [2; 3; 18]. Earlier methods considered the temporal graph as a series of graph snapshots that are sampled at regularly-spaced timestamps [19; 20], which will lose the fine-grained temporal information due to ignoring the temporal orders of

   Datasets & TPNet & w/o NR & w/o Time & w/o Scale \\  MOOC & \(96.39_{+0.0}\) & \(83.21_{+0.19}\) & \(94.62_{+0.04}\) & \(63.04_{+0.00}\) \\ LastFM & \(94.50_{+0.00}\) & \(76.25_{+0.00}\) & \(94.30_{+0.00}\) & N/A \\ Enron & \(92.90_{+0.00}\) & \(83.23_{+0.00}\) & \(92.85_{+0.00}\) & N/A \\ UCI & \(97.35_{+0.00}\) & \(88.70_{+1.15}\) & \(97.19_{+0.00}\) & \(73.13_{+1.25}\) \\ US Legis. & \(0.858_{+1.15}\) & \(69.47_{+1.15}\) & \(71.83_{+1.00}\) & \(70.44_{+1.97}\) \\ UN Trade & \(87.24_{+0.00}\) & \(62.56_{+0.00}\) & \(65.98_{+0.00}\) & \(56.58_{+1.00}\) \\ UN Note & \(75.12_{+0.00}\) & \(52.58_{+0.00}\) & \(54.80_{+0.00}\) & \(53.20_{+0.00}\) \\   

Table 2: Ablation study results, where N/A indicates the numerical overflow error.

Figure 3: Influence of node representation dimension.

interactions in a graph snapshot. Recently, some continuous-time temporal graph learning methods have been proposed [21; 3; 14; 13], which consider the temporal graph as a sequence of interactions with irregular time intervals to fully capture the graph dynamics. For example, TGN  maintained a dynamic memory vector for each node and generated node representations by aggregating memory vectors via temporal graph attention to capture the evolution pattern of the temporal graph. However, capturing pairwise information by merely aggregating representations of neighboring nodes  is challenging. To address this issue, the link-wise method was proposed, which constructs relative encodings as additional node features to inject the pairwise information into the representation learning process [6; 7; 9; 8]. For example, Souza et al.  proposed a relative encoding based on temporal walk counting and theoretically showed that constructing the relative encodings can improve the expressive power of models in distinguishing different links. Despite these advances, existing ways to construct the relative encodings are still far from satisfactory, where computation efficiency is a main concern and temporal information is seldom considered. In this paper, we unify existing relative encodings into a function of temporal walk matrices and explore encoding the pairwise information effectively and efficiently by temporal walk matrix projection.

## 6 Limitation

One limitation of our method is that the matrix construction approach requires manual predefined settings. Different networks may necessitate distinct construction methods, potentially leading to additional human effort in experimenting with various approaches. For instance, the proposed temporal walk matrix that incorporates the time decay effect may not be optimal for networks characterized by long-term dependencies. Developing an adaptive matrix construction technique will be an interesting direction for future research.

## 7 Conclusion

In this paper, we study the problem of pairwise information injection for temporal link prediction. We unify existing construction ways of relative encodings into a unified function, which reveals a connection between the relative encoding and temporal walk matrix. Then we propose a new temporal link prediction model, TPNet, to address the computational inefficiencies and the ignorance of temporal information in previous methods. TPNet introduces a new temporal walk matrix to simultaneously consider the temporal and structural information and a random feature propagation mechanism to maintain the temporal walk matrices efficiently. Theoretically, TPNet preserves the inner product of the maintained temporal walk matrices and empirically outperforms other link-wise methods in both effectiveness and efficiency. An interesting future direction may be designing an adaptive feature propagation mechanism.