# Tart: A plug-and-play Transformer module for task-agnostic reasoning

Kush Bhatia\({}^{*}\)  Avanika Narayan\({}^{*}\)  Christopher De Sa Christopher Re

{kushb, avanika, chrismre}@cs.stanford.edu, cdesa@cs.cornell.edu

Equal Contribution

###### Abstract

Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training. In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for _each_ specific task. In-context learning, however, consistently underperforms task-specific tuning approaches _even_ when presented with the same examples. While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our experiments actually reveal that LLM representations contain sufficient information to make good predictions. As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks. This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and, as a proof of concept, propose Tart which generically improves an LLM's reasoning abilities using a synthetically trained reasoning module. Tart trains this Transformer-based reasoning module in a task-agnostic manner using _only synthetic_ logistic regression tasks and composes it with an arbitrary real-world pre-trained model without any additional training. With a single inference module, Tart improves performance across different model families (GPT-Neo, Pythia, Bloom), model sizes (100M - 6B), tasks (14 NLP classification tasks), and even across different modalities (audio and vision). On the RAFT Benchmark, Tart improves GPT-Neo (125M)'s performance such that it outperforms Bloom (176B), and is within \(4\%\) of GPT-3 (175B). 1

## 1 Introduction

Large language models (LLMs) show in-context learning capabilities which enable them to perform a task given only a few examples, without updating the model parameters [9; 7]. This task-agnostic capability allows for a single model to be applied to a wide range of tasks [1; 41; 28]. In contrast, traditional task adaptation approaches, such as fine-tuning, update the model parameters for each specific task.

Despite being task-agnostic, in-context learning is seldom the practitioner's method of choice since it consistently underperforms task-specific adaptation approaches [20; 9]. Most existing works attribute this performance gap to the limited context window of LLMs which can only accommodate a few task examples [15; 14; 23]. However, we show that this gap between in-context learning and fine-tuning approaches exists _even_ when presented with the same task examples; in-context learning can underperform fine-tuning by up to 30 points across binary classification benchmarks (see Figure 4(a)).

We first investigate why this quality gap exists. We decompose an LLM's in-context learning capability into two abilities: learning good _representations_ for the task and performing probabilistic inference,or _reasoning_, over these representations3. Is the gap because the data representations learned by the models lack task-specific information or is there some deficiency in the the model's ability to use this information. We take a first step towards understanding this hypotheses experimentally in Section 2 by measuring both the reasoning and the representation gaps across a variety of LLM families (GPT-Neo , Pythia , Bloom ) over a suite of binary classification tasks. Our results suggest that LLMs indeed have the necessary information in the representations, and that the majority of the quality gap (up to \(79\%\)) can be attributed to their inability to perform simple forms of reasoning, e.g. linear classification. We further find that adaptation methods (such as fine-tuning) improve the model along both representation and reasoning, but primarily improve the task-specific classification (or reasoning) ability which accounts for \(72\%\) of the gained performance on the task.

Rather surprisingly, most existing techniques for improving the performance gap, such as prompt engineering or active example selection, focus entirely on the LLM's learned representations. With these methods, the users are effectively searching the language space for a natural language template that best represents the task. This search enables the users to optimize the representations for their specific task which makes the LLM more likely to solve it. In contrast, our work explores the orthogonal direction of improving the LLM's reasoning abilities.

As a first step towards improving this reasoning ability, we fine-tune LLMs using synthetically generated binary classification tasks (see Section A.1 for examples). While this approach provides an improvement over the model's base in-context learning performance (up to 17%, see Figure 7), this approach requires fine-tuning each LLM individually and might affect their generative abilities . This brings about a question: can we improve the reasoning capabilities of LLMs without interfering with it pre-existing capabilities, that is, can we make the process both task and model agnostic?

We show that it is indeed possible to improve the reasoning capabilities in a completely agnostic manner. Rather than directly fine-tuning, our proposed algorithm Tart, Task-Agnostic Reasoning Transformer (Figure 2), improves an LLM's reasoning abilities using an independent synthetically trained module. Tart trains this additional Transformer module using only synthetically generated binary classification tasks from the logistic model independent of the downstream task or the base LLM. This inference module can be composed, _without any additional training_, with the embeddings of an arbitrary pre-trained LLM to improve upon its reasoning abilities.

Tart is _task, model, and domain_ agnostic. Using a single inference module trained on synthetic data, we exhibit that Tart not only generalizes across three model families (GPT-Neo, Pythia, Bloom) over 14 NLP classification tasks, but even across different domains (vision and speech; see Figure 6). In terms of quality, we show that Tart's performance is 18.4% better than in-context learning, 3.4% better than task-specific adapters, and is within 3.1% of full task-specific fine-tuning across a suite of NLP tasks. On the RAFT Benchmark , Tart improves GPT-Neo (125M)'s performance such that it outperforms Bloom (176B), and is within \(4\%\) of GPT-3 (175B). Tart is

Figure 1: **Taxonomy of task adaptation strategies.** (Left) Comparison of different adaptation strategies across three desiderata: task-agnostic, quality, scalability. (Right) Demonstration of parameter updates across adaptations strategies, colored regions represent parameter changes as a result of the adaptation strategy.

data-scalable and overcomes the limited context length bottleneck of in-context learning. While each example spans multiple tokens in an LLM, often spanning hundreds of tokens, Tart's reasoning module encodes each example using only two tokens - one for the context and the other for the label. This data-scalability can lead to improvements of up to \(6.8\%\) (see Figure (c)c).

From a theoretical standpoint, we show that the generalization abilities of Tart depends mainly on the distribution shift between the natural text embedding distribution produced by the LLM and the synthetic data distribution, measured in terms of the Wasserstein-1 metric (Theorem 1 in Section 3.4).

To summarize, our main contributions are as follows:

* Study why in-context learning does not perform as well as task-specific fine-tuning despite having access to the same information, via a representation-reasoning decomposition.
* Propose a new task-agnostic method, Tart, which bridges the performance gap to task-specific methods and is trained using only synthetic classification data.
* Demonstrate that Tart works across different NLP tasks for a range of model families. The same inference module generalizes to vision and speech domains, suggesting that this capability is somehow uniformly lacking across a range of foundation models.

Related work.Prompt engineering focuses on improving the in-context task adaptation abilities of LLMs by modifying prompts. A line of work improves performance by carefully designing the natural language task specifications [4; 42] while others improve performance by optimizing the examples chosen for the prompt [10; 24], encouraging the models to sequentially reason [16; 42; 45] and aggregating prompts [39; 38]. Unfortunately, prompt-based task adaptation is noisy . Alternatively, prompt tuning improves the in-context abilities of models by training a small amounts of learnable vectors [21; 20; 25] for specific tasks. While these methods have been shown to improve in-context learning performance, they require task-specific fine-tuning and are not task-agnostic.

Recent works seek to understand the in-context learning property of LLMs by presenting mechanistic interpretations of in-context learning  and performing exploratory analysis of in-context learning behaviors . Existing literature demonstrates that LLMs can learn simple function classes in-context  and propose that LLMs are performing gradient descent when learning tasks in-context . Complementary to these, our work provides insights on the mechanisms of in-context learning and its deficiencies. Furthermore, task transfer strategies adapt LLMs to a pre-specified target task. Strategies range from parameter efficient finetuning (PEFT) [12; 47] to Low-Rank adaptation (LoRA)  which introduces trainable rank decomposition matrices into each layer.

## 2 Task adaptation strategies: Taxonomy and evaluation

We begin by describing the problem of adapting pre-trained language models for a collection of downstream tasks while being task-agnostic, competent in performance, and data-scalable. Given

Figure 2: **Tart. (Left) Inference module training procedure: The inference module is trained on sequences of synthetically generated logistic regression tasks. (Right) End-to-end framework: Tart composes a pre-trained LLM with the inference module. Tart uses the LLM to embed the input text. These embeddings, along with the train labels, are passed as a sequence to the inference module which generates a final prediction.**

these criteria, we evaluate existing task adaptation approaches and propose a representation-reasoning decomposition to understand their relative performances.

### Problem statement and evaluation criteria

Our focus is on methods for adapting pre-trained large language models (LLMs) for downstream tasks. Specifically, given an LLM and limited labeled data for a task, how does one adapt the model to the task? In this work, our focus is on classification tasks. For ease of presentation, we consider binary classification tasks in the main paper and defer the extension to multi-class setup to Appendix D. When evaluating a task adaptation strategy, we care about the following properties:

Task-agnostic.Given the general capabilities of pre-trained LLMs, we strive to utilize the same model across different tasks without requiring any task-specific training. With the increase in model sizes, the cost of deploying task-specific models increase both during training (expensive hyper-parameter search) as well as during inference (deploying several models). In general, task-agnostic methods will scale better with increasing model sizes by side-stepping both these costs.

Performance quality.We would like the adaptation approaches to be competitive in performance when compared with task-specific approaches across a wide range of tasks. For the binary classification tasks, the method should have accuracy comparable with task-specific approaches.

Data-scalable.The task adaptation method should be scalable with the number of labeled task examples. In particular, the method should be capable of learning from large datasets, and continually improve its performance quality.

### Taxonomy of task adaptation strategies

We can broadly taxonomize the existing task adaptation strategies for LLMs as in-context learning, fine-tuning the model, and training task-specific adapters (see Figure 1).

In-context learning.In-context learning allows for adapting the model without updating any model parameters, by simply providing a few demonstrations of the task in the LLM prompt. In-context learning is completely task-agnostic since the same model can be used across tasks since no weights are updated at inference time. However, its performance is usually not on par with task-specific methods and it does not scale well with data since the number of examples that can be utilized is bottlenecked by the context length of the model.

Fine-tuning.This traditional class of methods update the model weights to adapt it specifically for the task, typically by performing gradient descent over the labeled dataset. Fine-tuning methods are not task-agnostic since they change the underlying model significantly but usually achieve state-of-the-art performance for any given task and are data scalable.

Adapters.Adapters adapt the underlying LLM to a specific task by composing the LLM base model with an additional set of parameters which are optimized for the task. In contrast to fine-tuning which performs updates to the base model, adapters keep the base model frozen and only update the additional parameters. Performance of adapters is usually competitive with full fine-tuning.

### Understanding performance via Representation-Reasoning decomposition

From the taxonomy of task adaptation approaches, only in-context learning satisfies the task-agnostic property but it consistently underperforms the task-specific tuning approaches. This section investigates why this performance gap exists. We hypothesize that it is either because (a) the representations learned by the LLM are insufficient to learn a good predictor for the specific task, or (b) the LLM lacks the capability to reason over these representations to make good predictions for the task.

To understand whether the representations have sufficient information, we train a task-specific linear classifier using these representations, also known as linear probing, and evaluate its accuracy (\(_{}\)). Using this as an intermediate, we decompose the performance gap

\[_{}}:=_{}-_{} =_{}-_{}}_{_{}}+_{}-_{}}_{ _{}}\] (1)

where \(_{}\) represents the gap in performance which can be attributed to insufficient representation capacity and \(_{}\) is the performance gap due to insufficient reasoning abilities. Mathematically, we define the quality of a models representations to be the gap between the fine-tuned model's accuracy and the accuracy of linear probing - the smaller the gap the better the representations. Similarly, the models reasoning ability, or the ability to perform linear classification, is measured by the gap between the in-context learning and linear probing accuracies - if this gap is small, that means the model does not improve by learning any task-specific decision boundary and therefore already possesses the ability to reason.

Using the above decomposition, we consider the following hypotheses:

* LLM representations have enough information to perform the task in-context, but they lack the reasoning abilities to perform the task well.
* Fine-tuning affects both the representations and reasoning but the improvement in reasoning abilities primarily leads to better performance.
* Fine-tuning and adapters are not task-agnostic because the task-specific training hurts their ability to transfer reasoning.

We now analyze each of the task adaptation approaches through the lens of the above hypotheses. We perform all experiments with three different classes of language models (GPT-Neo, Pythia, Bloom) across a collection of 6 binary classification tasks. See Appendix B for further details.

In-context learning: LLMs lack reasoning abilities.We begin by studying the representation and reasoning gaps, as defined in eq.1, for in-context learning. In Figure3(a), we plot the average accuracy across datasets for in-context learning, task-specific fine-tuning, and linear probing. We see that across models and different numbers of in-context examples, the reasoning gap \(_{}\) accounts for up to \(79.11\)% of the performance gap between in-context learning and fine-tuning. This indicates that the LLM representations have sufficient information but they lack the ability to reason over them.

Fine-tuning: Improves task-specific reasoning.We next investigate how fine-tuning for a specific task affects the performance of the base model. In Figure3(b), we show a scatter plot of the gains that can be attributed to improved representations against the reasoning gains. We see that, across models, reasoning improvements accounts for \(73.06\)% of the improvements. This indicates that while fine-tuning improves both reasoning and representations of the LLM, the gains are predominantly due to improvements in task-specific reasoning. Furthermore, this task-specific fine-tuning of the LLM hurts its performance on other tasks. In Figure3(c), we show that the accuracy of a model fine-tuned on the AGNews dataset , leads to an average decrease of \(25.77\)% on other tasks. Furthermore, this drop in accuracy can be attributed to the drop in task-specific reasoning capabilities--these account for \(72.58\)% of the drop (see Appendix B for more details).

Adapters: Impairs task-agnosticity via reasoning.Task-specific adapters do not change the underlying representation ability of the model. To study their ability to generalize across tasks, we train an adapter for the AGNews dataset and evaluate it on other tasks. In Appendix B, we show that the performance drops across tasks by an average of \(19.8\)%, indicating that adapters only learn task-specific reasoning abilities.

Figure 3: All results for GPT-Neo (125M). (a) Accuracy of in-context learning vs. linear probing on model embeddings: representations have sufficient information. (b) Fine-tuning majorly improves task-specific reasoning across datasets. (c) Accuracy (averaged across \(6\) datasets) of task-specific fine-tuned model vs. accuracy of model fine-tuned on AGNews and evaluated on task. Fine-tuning hurts task-agnosticity.

Tart: Task-Agnostic Reasoning Transformers

The above analysis showed how it is the effective reasoning capabilities of the LLMs which limits its performance when compared with task-specific adaptation approaches. Building on this insight, we propose Tart, which learns a general-purpose reasoning module completely agnostic to the underlying base LLM and when composed with any LLM via its embeddings, generically improves upon its reasoning abilities. Tart is a completely task-agnostic method which works across a suite of tasks without any task-specific training.

### Overview of algorithm

Tart comprises two components: a generic task-agnostic reasoning module, and embeddings from the base LLM. The reasoning module is trained using only synthetic data (Gaussian logistic regression problems), agnostic of the auto-regressively trained language model, with the objective of learning to perform probabilistic inference (Section 3.2). This learned transformer module is then composed with the base LLM, without any training, by simply aggregating the output embedding and using those as an input along with the class label (Section 3.3). Together, these components make Tart task-agnostic, boost performance quality by improving reasoning, and make the approach data-scalable by aggregating input embeddings into a single vector.

### Reasoning module: Can transformers learn to do probabilistic inference?

Tart's reasoning module is a Transformer-based model which is trained to perform probabilistic inference in-context using only synthetically generated data.

#### 3.2.1 Training the reasoning module

The reasoning module is a Transformer model which is auto-regressively trained on a _family_ of logistic regression tasks, with each input sequence corresponding to a different logistic regression problem. We next describe the model architecture and the training procedure.

Model architecture.The reasoning module is based on the standard decoder-only Transformer architecture from the GPT-2 family. The GPT-2 architecture comprises a causal (unidirectional) transformer with 12 decoder layers and 8 attention heads per layer for a total of 22 M parameters. The model has absolute positional encodings for each position. Please see Appendix C.1 for details. The architecture takes as input a sequence of vectors and is trained to predict the next vector in the sequence. The input sequence consists of \(k\) pairs of labeled examples \((x_{1},y_{1}),(x_{2},y_{2}),,(x_{k},y_{k})\), with each example \(z_{i}=(x_{i},y_{i})\) using only two input positions of the transformer - one for the covariates \(x\) and the other for the label \(y\). This is in contrast to standard LLMs where each example is spread over multiple tokens which limits how many examples can be put in the context. For example, with a context window of 2048, our module can support 1024 examples while the base model can support only 10 examples, assuming each demonstration comprises 200 natural language tokens.

Training procedure.This module is trained using gradient descent to minimize the population loss

\[(T_{}):\;=_{x,y}[_{i=1}^{k}_{ }(T_{}(z_{1:i-1},x_{i}),y_{i})]\;,\] (2)

where \(z_{1:i-1}\) corresponds to the first \(i-1\) examples and \(_{}\) is the cross-entropy loss evaluated on the transformer prediction and the true \(y_{i}\). Each training sequence \(s_{t}\) used to update the parameters \(\) comprises a different \(d\)-dimensional logistic regression problem, sampled as

\[s_{t}:w_{t}(0,I_{d}), x_{i,t} (0,I_{d}), y_{i,t}( x_{i,t},w_{t} )i[k]\;,\] (3)

where \(\) represents the sigmoid function and the multiplier \(\) determines the noise level of the problem. The above sampling model comprises three components, the regressor \(w_{t}\), the covariates \(x_{i,t}\) and the labels \(y_{i,t}\) sampled as

* the regressor \(w_{t}\) is sampled from a standard Normal distribution
* the covariates \(x_{i,t}\) are sampled from a standard Normal distribution
* the corresponding \(y_{i,t}\) are sampled based on the sigmoid of the inner product \( x_{i,t},w_{t}\)

See Figure 2 (Left) for a demonstration of a 2d dataset sampled from this process. The process is sampling noisy-linearly separable data in \(d\) dimensions. We train our model with \(d=16\) and \(k=256\). We describe the model hyper-parameters and the training procedure in more detail in Appendix C.1. Observe that the embedding dimension of the base LLM and the input dimension of the reasoning module might not always match. In order to compose these models together, we perform a dimensionality reduction via PCA to reduce the embedding dimension.

#### 3.2.2 Properties of reasoning module

The task-agnostic reasoning module described above is trained to perform well on a family of logistic regression tasks. We study some properties of the reasoning module, in particular how well it learns to perform the task at an instance level and how robust is it to variations in the noise level \(\).

Accuracy of probabilistic inference.For understanding the instance level performance of our reasoning module, we evaluate it on a sample of \(64\) different logistic regression problems, sampled according to eq.3. For each problem, we train task-specific linear classifiers using logistic regression and compare them with our task-agnostic reasoning module. In Figure4 we plot the deviation of the predicted probabilities (averaged over the \(64\) problems) from the true probabilities for our reasoning module and the task-specific logistic solvers as a function of the number of examples used for predictions. We observe that the error for our reasoning module decreases as a function of the number of in-context examples and is within \(2\%\) of the task-specific logistic function.

Robustness to noise level.We study the robustness of the learned module to the noise levels, \(\), of the logistic regression problem. Recall that we trained our inference module by fixing the noise level \(=10\). At inference time, we vary the noise level to \([0.5,1,10,20]\), where lower values corresponds to noisier problem. The reasoning module generalizes to easier problem without any drop in accuracy but as we make the problem harder \((=[0.5,1])\), the error increases progressively (see Figure4).

### Role of representations: How to embed training examples?

The reasoning module composes with a base LLM through its final layer embeddings. A natural way to produce these embeddings is to place all the train examples in-context and then average the embedding vectors corresponding to the particular example (see Figure2). At inference time, we append the test example to the training set, and average the embeddings corresponding to this example. We call these vanilla embeddings. Our experiments reveal that these embeddings seem to saturate (or even hurt performance) beyond a certain number of in-context examples (see Figure4). One reason can be that the causal nature of the model causes these embeddings to have asymmetric information--the embeddings of each example is influenced by its preceding examples.

To counter this asymmetry, we propose _leave-one-out_ (LOO) embeddings where the embeddings for each training point is formed by placing all the other train examples before it in the prompt such that all the embedding are formed with the same information content. In Figure4, changing the embedding style from vanilla to LOO consistently improves performance across models and tasks. The LOO-embeddings help Tart be data-scalable by enabling it to embed a much larger number of points than the context window can support. To do so, we use only a subset of the train examples as the in-context prompt. The reasoning module, by its architecture design, can already accommodate many more examples than supported by the context window of the base LLM.

Figure 4: **Properties of Tartâ€™s inference module**. (a) Comparison with learned logistic function: inference module recovers underlying probabilities. (b) Variation in error with different noise levels for model trained on \(=10\). (c) Comparison of Tart performance when using LOO embeddings and vanilla embeddings.

### Theoretical analysis: Generalization of Tart to natural language tasks

We study the generalization properties of the proposed task-agnostic method Tart. Note that that the inference module is trained completely on synthetic data while at evaluation time, our input is the embeddings from a natural language task. In Theorem 1 we show that its performance on the natural language task depends on the distribution shift from the synthetic to the true distribution (see Appendix C.3 for a formal statement and proof).

**Theorem 1** (Informal).: _Let \(\) represent the class of transformer models and \(T_{S}\) denote the trained reasoning module on set \(S\) of synthetic regression with \(n_{}\) sequences sampled from distribution \(P_{}\) in eq.3. The error of the transformer \(T_{S}\) when evaluated on a distribution \(P_{}\) over natural language sequences is_

\[_{P_{}} W_{1}(P_{},P_{} )+()}{n_{}}}+_{P_{}}(T_{S})\,\] (4)

_where \(W_{1}\) denotes the Wasserstein-1 metric, \(()\) represents the complexity of class \(\), and \(\) represents the error on the empirical distribution._

A few comments are in order: The first term represents the distribution shift error between the true natural language task and the synthetic task. The second term corresponds to the generalization error on the logistic regression task, which can be made arbitrarily small because scales with \(n_{}\), the number of synthetic datapoints which can be generated without any cost. The third term above is the optimization error indicating how well has the reasoning module \(T_{S}\) fit to the synthetic training set.

Observe that for our algorithm Tart, the distribution \(P_{}\) corresponds to the distribution induced on the embeddings of the natural language data (post the PCA and renormalization) while the distribution \(P_{}\) is the one corresponding to the sampling process in equation3.

## 4 Experimental evaluation

We evaluate Tart on a wide range of binary classification tasks across three domains: language, vision and audio. We demonstrate that Tart improves base in-context performance and closes the gap with standard task-specific strategies. We also conduct ablations to demonstrate that Tart scales with model size and can support 10x more samples than in-context learning.

### Experimental setup

Datasets.We briefly describe the datasets used, with details available in Appendix E.1. We consider \(14\) different binary classification tasks ranging from sentiment classification, news article categorization to spam detection. The evaluation datasets include: SST , Rotten Tomatoes , SMS Spam , IMDB , Civil Comments , AGNews , DBPedia , and the Youtube dataset . Since AGNews and DBPedia14 are multi-class datasets, we construct 4 binary classification tasks from each dataset respectively. For each dataset, we truncate the input text to be at most 100 characters to enable us to fit sufficient number of samples in-context.

Model families.We evaluate our method across three different families of models: GPT-Neo , Pythia , and Bloom . For our evaluations across 14 datasets, we use GPT-Neo (125M), Pythia (160M) and Bloom (560M). For ablations on larger models, we evaluate models with 1B parameters across each of the model families (i.e., GPT-Neo (1.3B), Pythia (1.4B) and Bloom (1.7B)) and models with 3B parameters (i.e., GPT-Neo (2.7B), Pythia (2.8B) and Bloom (3B)). We additionally evaluate on GPT-J (6B) .

Baselines.We evaluate our models against all types of task-adaptation strategies described in Section2.2: 1) in-context learning, 2) full fine-tuning, 3) last layer fine-tuning, 4) LM head fine-tuning, and 5) adapters. For each baseline, we perform an extensive hyper-parameter search over number of epochs and learning rate for each dataset in order to optimize performance (see AppendixE.1 for hyperparameter details). For Tart, we chose a base default set of parameters and use the _same_ inference module with the exact same weights for all the experiments in this section.

### Natural language benchmark evaluations

Performance with respect to baselines.As shown in Appendix E.2, averaged across all tasks and model families, Tart improves upon the base in-context learning performance by an average of \(18.4\) points, improves upon adapter heads by \(3.4\) points, and is within \(3.1\) points of full fine-tuning. We also observe that Tart consistently outperforms the task specific strategies of LM head fine-tuning and last layer fine-tuning.

Performance on RAFT BenchmarkWe evaluate Tart on all binary classification tasks in the RAFT Benchmark, following the protocol used in HELM . When applied with GPT-Neo (125M), Tart [\(0.634\)] outperforms Bloom (176B) [\(0.595\)], and is within \(4\%\) points of GPT-3 (175B) [\(0.673\)], both of which are \(1000\)x larger in size.

Performance with number of in-context examples.Our results demonstrate that performance of Tart scales with number of in-context examples (see Figure 4(a)). Across \(14\) tasks and \(3\) model families, when scaling from \(18\) to \(64\) examples, Tart improves performance by an average of \(4.8\)%. Correspondingly, full fine-tuning improves performance by \(9.0\)%.

Scaling with base model size.We analyze how different task-adaptation strategies scale with respect to model size using the GPT-Neo family: GPT-Neo (125M), GPT-Neo (1.3B) and GPT-J (6B). Figure 4(b) shows that when scaling from 100M to 6B parameters, performance of task-specific methods and Tart increases as a function scale. For Tart, the performance increases by \(9.8\%\) while using the same inference module across model sizes. Furthermore, the difference in performance between Tart and fine-tuning baseline reduces from \(7.5\%\) to \(2.2\%\) from the 100M scale to 6B scale.

Beyond context length.We evaluate the data-scaling properties for both in-context learning and Tart (Figure 4(c)). To demonstrate the scaling property, we do not truncate the input text to \(100\) characters and utilize the entire text sequences. For Tart, we observe that accuracy continues to improve when scaling from \(18\) to \(256\) in-context examples with \(6.8\)% lift in performance. In comparison, ICL, which is bottlenecked by context length, supports \(10\)x less samples, with the context window saturating at \(24\) examples only and lags Tart by an average of \(19.1\)%.

Extension to other architectures.We compare the performance of Tart by changing the underlying architecture used to train the reasoning module. Specifically, we train a reasoning module that has the Hyena architecture  using the same synthetic logistic regression tasks. In appendix E.3, we show that the accuracies obtained by swapping the Transformer with the Hyena module maintains the same accuracy as before. Thus, Tart's performance is robust to the underlying architecture.

Performance in small sample regime.While our focus has been on scaling up number of in-context examples (ranging from 18-64), in Appendix E.4 we compare the performance of Tart with fine-tuning and in-context learning in the small sample regime. Specifically, we look at few-shot learning with number of examples varying from 4-10. Across 6 datasets, Tart outperforms fine-tuning by 4.1% (with max. gains up to 20.6%) and in-context learning by 11.6% (with max. gains up to 35.7%). See Figure 26 for detailed results.

Figure 5: **Effects of scale**. (a) Effect of number of in-context examples on performance for different task adaptation strategies. (b) Effect of model size on the performance of different task adaptation strategies. (c) Beyond context length limitations, performance comparison with respect to number of in-context examples.

### Extensions to other modalities

We demonstrate that Tart is not only agnostic to models and tasks, but also modalities. We extend Tart to classification tasks on modalities beyond language: vision and audio. For vision tasks, we use representations from Google's 307M parameter pretrained Vision Transformer (ViT) model : ViT-large-patch16-224. For audio tasks, we use representations from OpenAI's 1.5B parameter pretrained Whisper model : Whisper-large. In applying Tart to the representations from these models, we provide a way for performing in-context learning in modalities beyond text. We refer the reader to Appendix E.5 for further details on the experiment setup.

Vision application.We evaluate the performance of Tart on binary classification versions of MNIST  (classes 0 and 8) and CIFAR-10  (classes plane and bird). As shown in Figure 5(a) and 5(b), performance of Tart is competitive with task-specific adaptation approaches.

Audio application.We evaluate Tart on a binary classification version of the Speech Commands dataset , where the task is to classify "stop" and "go" utterances. As shown in Figure 5(c), performance of Tart is competitive with task-adaptation approaches.

### Extension to multi-class classification problems

While we focused on binary classification tasks in this paper, in Appendix D we demonstrate how to extend Tart to multi-class problems. Instead of training a module on synthetic binary classification problem, we modify the synthetics to sample from a multinomial extension of the logistic model. We evaluated this multi-class model on 4-class classification tasks (AGNews, Ecommerce) as well as a 14-class classification problem (DBPedia-14). Our evaluations show that Tart can outperform in-context learning by up to 45.9% on AGNews, 52.6% on Ecommerce and 59.3% on DBPedia-14.

## 5 Discussion

We look at the problem of task-agnostic learning with LLMs. We show that LLMs lack the ability to perform simple reasoning over their learned representations and introduce Tart, a task, model and domain agnostic method for improving their reasoning abilities. In this work, we focus on classification tasks, showing that synthetic, logistic regression task data can be used to train a generic reasoning module capable of completing this class of tasks. In future work, we seek to understand whether synthetic tasks exist for training other generic reasoning modules, capable of improving base LLM performance on tasks such as generation or summarization.

#### Acknowledgements

We are grateful to Simran Arora, Rishi Bommasani, Niladri Chatterji, Arjun Desai, Sabri Eyuboglu, Neha Gupta, Karan Goel, Erik Jones, Ananya Kumar, Cassidy Laidlaw, Megan Leszczynski, Piero Molino, Laurel Orr, Michael Poli, Dimitris Tsipras, Michael Wornow, Ce Zhang, and Michael Zhang for their helpful comments and feedback, and discussions which helped shape this project. We thank Neel Guha for helpful discussions in shaping the narrative of this paper.

We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML);

Figure 6: Tart can generalize across domains using the same inference module that was used for language benchmarks: Performance across vision tasks (MNIST, CIFAR-10) and an audio task (Speech Commands).

US DEVCOM ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare. CDS was supported by a NSF CAREER (award 2046760).

The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.