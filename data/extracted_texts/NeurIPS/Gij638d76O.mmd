# Neural Latent Geometry Search:

Product Manifold Inference via

Gromov-Hausdorff-Informed Bayesian Optimization

Haitz Saez de Ocariz Borde

Oxford Robotics Institute

University of Oxford

&Alvaro Arroyo

Oxford-Man Institute

University of Oxford

Ismael Morales Lopez

Mathematical Institute

University of Oxford

&Ingmar Posner

Oxford Robotics Institute

University of Oxford

&Xiaowen Dong

Machine Learning Research Group

University of Oxford

###### Abstract

Recent research indicates that the performance of machine learning models can be improved by aligning the geometry of the latent space with the underlying data structure. Rather than relying solely on Euclidean space, researchers have proposed using hyperbolic and spherical spaces with constant curvature, or combinations thereof, to better model the latent space and enhance model performance. However, little attention has been given to the problem of automatically identifying the optimal latent geometry for the downstream task. We mathematically define this novel formulation and coin it as neural latent geometry search (NLGS). More specifically, we introduce an initial attempt to search for a latent geometry composed of a product of constant curvature model spaces with a small number of query evaluations, under some simplifying assumptions. To accomplish this, we propose a novel notion of distance between candidate latent geometries based on the Gromov-Hausdorff distance from metric geometry. In order to compute the Gromov-Hausdorff distance, we introduce a mapping function that enables the comparison of different manifolds by embedding them in a common high-dimensional ambient space. We then design a graph search space based on the notion of _smoothness_ between latent geometries, and employ the calculated distances as an additional inductive bias. Finally, we use Bayesian optimization to search for the optimal latent geometry in a query-efficient manner. This is a general method which can be applied to search for the optimal latent geometry for a variety of models and downstream tasks. We perform experiments on synthetic and real-world datasets to identify the optimal latent geometry for multiple machine learning problems.

+
Footnote â€ : *Equal contribution

## 1 Introduction

There has been a recent surge of research employing ideas from differential geometry and topology to improve the performance of learning algorithms (Bortoli et al., 2022; Hensel et al., 2021; Chamberlain et al., 2021; Huang et al., 2022; Barbero et al., 2022, 2022). Traditionally, Euclidean spaces have been the preferred choice to model the geometry of latent spaces in the ML community (Weber, 2019; Bronstein et al., 2021). However, recent work has found that representing the latent space with a geometry that better matches the structure of the data can provide significant performance enhancements inboth reconstruction and other downstream tasks (Shukla et al., 2018). In particular, most works have employed _constant curvature model spaces_ such as the Poincare ball model (Mathieu et al., 2019), the hyperboloid (Chami et al., 2019), or the hypersphere (Zhao et al., 2019), to encode latent representations of data in a relatively simple and computationally tractable way.

While individual model spaces have sometimes shown superior performance when compared to their Euclidean counterparts, more recent works (Gu et al., 2018; Skopek et al., 2019; Saez de Ocariz Borde et al., 2023b, a; Zhang et al., 2020; Fumero et al., 2021; Pfau et al., 2020) have leveraged the notion of _product spaces_ (also known as _product manifolds_) to model the latent space. This idea allows to generate more complex representations of the latent space for improved performance by taking Cartesian products of model spaces, while retaining the computational tractability of mathematical objects such as _exponential maps_ or _geodesic distances_, see Saez de Ocariz Borde et al. (2023b). Despite the success of this methodology, there exists no principled way of obtaining the _product manifold signature_ (_i.e._, the choice and number of manifold components used to generate the product manifold and their respective dimensionalities) to optimally represent the data for downstream task performance. This procedure is typically performed heuristically, often involving a random search over the discrete combinatorial space of all possible combinations of product manifold signatures, which is an exceedingly large search space that hampers computational efficiency and practical applicability. Some other work related to latent space geometry modelling can be found in Lubold et al. (2023); Hauberg et al. (2012); Arvanitidis et al. (2017).

**Contributions. 1)** In this paper, we consider a novel problem setting where we aim to search for an optimal latent geometry that best suits the model and downstream task. As a particular instance of this setting, we consider searching for the optimal product manifold signature. Due to the conceptual similarity with neural architecture search (NAS) strategies (Elsken et al., 2018; Zoph and Le, 2016; Pham et al., 2018), we coin this problem as _neural latent geometry search (NLGS)_, which we hope will encourage additional work in the direction of optimal latent geometry inference. We test our framework on a variety of use cases, such as autoencoder reconstruction (Mathieu et al., 2019) and latent graph inference (Saez de Ocariz Borde et al., 2023b), for which we create a set of custom datasets.

**2)** To search for the product manifold signature, we must be able to compare product manifolds. This is traditionally done by computing the Hausdorff distance between manifolds (Taha and Hanbury, 2015), which however requires manifolds to reside in the same metric space. To address this limitation, in this work we develop to our knowledge the first computational method to mathematically compare product manifolds. Our approach generalizes classical algorithms and allows the comparison of manifolds existing in different spaces. This is achieved by defining an isometric embedding that maps the manifolds to a common high-dimensional ambient space, which enables the computation of their Gromov-Hausdorff distances.

**3)** Leveraging the Gromov-Hausdorff distances between candidate latent space manifolds, we design a principled and query-efficient framework to search for an optimal latent geometry, in the sense that it yields the best performance with respect to a given machine learning model and downstream task.

Our approach consists of constructing a geometry-informed graph search space where each node in the graph represents a unique candidate product manifold, associated with the model performance using this manifold as its embedding space. The strength of edges in the graph are based on the inverse of the Gromov-Hausdorff distance, thereby encoding a notion of "closeness" between manifolds in the search space. We then perform efficient search over this space using Bayesian optimization (BO). We compare our proposed method with other search algorithms that lack the topological prior inherent in our model. Empirical results demonstrate that our method outperforms the baselines by a significant margin in finding the optimal product manifold.

**Outline.** In Section 2 we discuss manifold learning, and product manifolds of constant curvature model spaces such as the Euclidean plane, the hyperboloid, and the hypersphere. We also review relevant mathematical concepts, particularly the Hausdorff and Gromov-Hausdorff distances from metric geometry (Gopal et al., 2020). Section 3 presents the problem formulation, the proposed methodology to compare product manifolds, as well as how the search space over which to perform geometry-informed Bayesian optimization is constructed. Finally, Section 4 explains how our custom synthetic and real-world datasets were obtained, and the empirical results. Lastly, in Section 5 we conclude and discuss avenues for future work.2

## 2 Background

**Manifold Learning** Manifold learning is a sub-field of machine learning that uses tools from differential geometry to model high-dimensional datasets by mapping them to a low-dimensional latent space. This allows researchers to analyze the underlying structure of data and improve machine learning models by capturing the geometry of the data more accurately. Manifold learning is based on the assumption that most observed data can be encoded within a low-dimensional manifold (see Figure 1) embedded in a high-dimensional space (Fefferman et al., 2013). This has seen applications in dimensionality reduction (Roweis and Saul, 2000; Tenenbaum et al., 2000), generative models (Goodfellow et al., 2014; Du et al., 2021; Bortoli et al., 2022), and graph structure learning for graph neural networks (GNNs) (Topping et al., 2021). In all these application, the key is to find a topological representation as an abstract encoding that describes the data optimally for the downstream task.

**Product Manifolds.** In this work, we model the geometry using model space Riemannian manifolds (Appendix A.1) and Cartesian products of such manifolds. The three so-called model spaces with constant curvature are the Euclidean plane, \(^{n}=^{d_{}}_{K_{}}=^{d_{ }}\), where the curvature \(K_{}=0\); the hyperboloid, \(^{n}=^{d_{}}_{K_{}}=\{_{p} ^{d_{}+1}:_{p},_{p}_{ }=1/K_{}\}\), where \(K_{}<0\) and \(,_{}\) is the Lorentz inner product; and the hypersphere, \(^{n}=^{d_{}}_{K_{}}=\{_{p} ^{d_{}+1}:_{p},_{p}_{ 2}=1/K_{}\}\), where \(K_{}>0\) and \(,_{2}\) is the standard Euclidean inner product. These have associated exponential maps and distance functions with closed form solutions, which can be found in Appendix A.2. A product manifold can be constructed using the Cartesian product \(=_{i=1}^{n_{}}_{K_{i}}^{d_{i}}\) of \(n_{}\) manifolds with curvature \(K_{i}\) and dimensionality \(d_{i}\). Note that both \(n_{}\) and \(d_{i}\) are hyperparameters that define the product manifold \(\) and that must be set a priori. On the other hand, the curvature of each model space \(K_{i}\) can be learned via gradient descent. One must note that the product manifold construction makes it possible to generate more complex embedding spaces than the original constant curvature model spaces, but it does not allow to generate any arbitrary manifold nor to control local curvature.

Hausdorff and Gromov-Hausdorff Distances for Comparing Manifolds.The Hausdorff distance between two subsets of a metric space refers to the greatest distance between any point on the first set and its closest point on the second set (Jungeblut et al., 2021). Given a metric space \(X\) with metric \(d_{X}\), and two subsets \(A\) and \(B\), we can define the _Hausdorff distance_ between \(A\) and \(B\) in \(X\) by

\[^{X}_{}(A,B)=(_{a A}d_{X}(a,B),\,_{b  B}d_{X}(b,A)).\] (1)

A priori this quantity may be infinite. Hence we will restrict to compact subsets \(A\) and \(B\). In this case, we can equivalently define \(_{}(A,B)\) as the smallest real number \(c 0\) such that for every \(a A\) and every \(b B\) there exist \(a^{} A\) and \(b^{} B\) such that both \(d_{X}(a,b^{})\) and \(d_{X}(a^{},b)\) are at most \(c\).

We note that the previous definition does not require any differentiable structures on \(X\), \(A\) and \(B\). They can be merely metric spaces. This generality allows us to distinguish the metric properties of Euclidean, hyperbolic and spherical geometries beyond analytic notions such as curvature. However, the definition in Equation 1 only allows to compare spaces \(A\) and \(B\) that are embedded in a certain metric space \(X\). The notion of distance that we shall consider is the Gromov-Hausdorff distance, which we define below in Equation 3.

Given a metric space \(X\) and two isometric embeddings \(f:A X\) and \(g:B X\), we define

\[^{X,f,g}_{}(A,B)=^{X}_{}(f(A),g(B)).\] (2)

Figure 1: Schematic of a manifold \(\) and open subsets \(U_{i}\) and \(U_{j}\). An open chart is a homeomorphism of an open subset of the manifold onto an open subset of the Euclidean hyperplane. Here, \(_{ij}\) is a transition function.

Now, given two metric spaces \(A\) and \(B\), we denote by \((A,B)\) (standing for "embedding spaces of \(A\) and \(B\)") as the triple \((X,f,g)\) where \(X\) is a metric space and \(f:A X\) and \(g:B X\) are isometric embeddings. We define the _Gromov-Hausdorff distance between \(A\) and \(B\)_ as:

\[_{}(A,B)=_{(X,f,g)(A,B)} _{}^{X,f,g}(f(A),g(B)).\] (3)

We should note that, since we assume that both \(A\) and \(B\) are compact, there is a trivial upper bound for their Gromov-Hausdorff distance in terms of their diameters. The _diameter_ of a metric space \(Y\) is defined to be \((Y)=_{y,y^{} Y}d_{Y}(y,y^{})\). Given \(a_{0} A\) and \(b_{0} B\), we can define the isometric embeddings \(f:A A B\) and \(g:B A B\) given by \(f(a)=(a,b_{0})\) and \(g(b)=(a_{0},b)\). It is easy to see that \(_{}^{A B,f,g}(A)((A), (B))\). Since the triple \((A B,f,g)\) belongs to \((A,B)\), we can estimate \(_{}(A,B)((A),(B)).\) To compare \(^{n}\), \(^{n}\) and \(^{n}\), we propose taking closed balls of radius one in each space. Since balls of radius one in any of these spaces are homogeneous Riemannian manifolds, they are isometric to each other. By estimating or providing an upper bound for their Gromov-Hausdorff distance, we can compare the spaces. This notion of distance between candidate latent geometries will later be used to generate a search space for our framework. With exactly an analogous argument as before, we can notice that given two compact balls of radius one \(B\) and \(B^{}\), of centres \(x_{0}\) and \(x_{0}^{}\), we can embed \(B\) into \(B B^{}\) by the mapping \(f:b(b,x_{0}^{})\). From here, it is obvious to see that \(_{}^{B B^{},f,}(B,B B^{ })=1\). In particular, this gives us the bound \(_{}(B,B^{}) 1\). This is the estimation we will take as the Gromov-Hausdorff distances of product manifolds that simply differ in one coordinate (such as, say, \(^{2}\) and \(^{2}^{2}\)).

## 3 Neural Latent Geometry Search: Latent Product Manifold Inference

In this section, we leverage ideas discussed in Section 2 to introduce a principled way to find the optimal latent product manifold. First, we introduce the problem formulation of NLGS. Next, we outline the strategy used to compute the Gromov-Hausdorff distance between product manifolds, and we discuss how this notion of similarity can be used in practice to construct a graph search space of latent geometries. Lastly, we explain how the Gromov-Hausdorff-informed graph search space can be used to perform NLGS via BO.

### Problem Formulation

The problem of NLGS can be formulated as follows. Given a search space \(\) denoting the set of all possible latent geometries, and the objective function \(L_{T,A}(g)\) which evaluates the performance of a given geometry \(g\) on a downstream task \(T\) for a machine learning model architecture \(A\), the objective is to find an optimal latent geometry \(g^{}\):

\[g^{}=*{arg\,min}_{g}L_{T,A}(g).\] (4)

In our case we model the latent space geometry using product manifolds. Hence we effectively restrict Equation 4 to finding the optimal product manifold signature:

\[n_{}^{},\{d_{i}\}_{i n_{}^{}}^{},\{K_{i }\}_{i n_{}^{}}^{}=*{arg\,min}_{n_{},d_{i},K_{i}}L_{T,A}(n_{},\{d_{i}\}_{i n_{}},\{K_{i}\}_{i n_{}}),\] (5)

where \(n_{}\) is the number of model spaces composing the product manifold \(\), \(\{d_{i}\}_{i n_{}}\) are the dimensions of each of the model spaces of constant curvature, and \(\{K_{i}\}_{i n_{}}\) their respective curvatures. We further simplify the problem by setting \(d_{i}=2,\  i\), and by restricting ourselves to \(K_{i}\{-1,0,1\}\), in order to limit the size of the hypothesis space.

### Quantifying the Difference Between Product Manifolds

Motivation.From a computational perspective, we can think of the Hausdorff distance as a measure of dissimilarity between two point sets, each representing a discretized version of the two underlying continuous manifolds we wish to compare. Taha and Hanbury (2015) proposed an efficient algorithm to compute the exact Hausdorff distance between two point sets with nearly-linear complexity leveraging early breaking and random sampling in place of scanning. However, the original algorithm assumes that both point sets live in the same space and have the same dimensionality, which is a limiting requirement. Gromov-Hausdorff distances, as opposed to usual Hausdorff distances, allow us to measure the distance between two metric spaces that a priori are not embedded in a common bigger ambient space. However, this has the caveat that they are not computable. For instance, for our application we must calculate the distance between each pair of the following three spaces: \(^{n}\), \(^{n}\) and \(^{n}\). However, \(^{n}\) does not isometrically embed in \(^{n}\) and hence in order to compare \(^{n}\) and \(^{n}\), we must work in a higher dimensional ambient space such as \(^{n+1}\). In the case of \(^{n}\), finding an embedding to a Euclidean space is more complicated and is described in Appendix B.2 (\(^{n}\) will be embedded isometrically into \(^{6n-6}\)). However, in this process there will be choices made about which embeddings to consider (in particular, we cannot exactly compute the infimum that appears in the definition of Gromov-Hausdorff distance in Equation 3). Likewise, using the original algorithm by Taha and Hanbury (2015) it is not possible to compute the Hausdorff distance between product manifolds based on an unequal number of model spaces, for instance, there is no way of computing the distance between \(^{n}\) and \(^{n}^{n}\). In this section we give an upper bound for \(}(^{n},^{n})\), and then describe an algorithm to give an upper bound for \(}(^{n},^{n})\) and \(}(^{n},^{n})\).

Strategy.The spaces \(^{n}\) and \(^{n}\) isometrically embed into \(^{6n-6}\) in many ways. This may seem redundant because both spaces already embed in \(^{n+1}\). However, the interest of considering this higher dimensional Euclidean space is that \(^{n}\) will also isometrically embed into it. Crucially, this will provide a common underlying space in which to compute Hausdorff distances between our geometries \(^{n}\), \(^{n}\) and \(^{n}\), which will lead to an estimation of their mutual Gromov-Hausdorff distance. The embedding of \(^{n}\) into \(^{6n-6}\) that we shall describe appears in Henke and Nettekoven (1987) and is made explicit in (Blanusa, 1955). We also refer the reader to the exposition (Brander, 2003, Chapter 5), which puts this results in a broader context while also summarising related advances on the topic of isometrically embedding homogeneous spaces into higher dimensional ones. For \(n=2\), we name this embedding \(F:^{2}^{6}\) (Appendix B.2). For simplicity, in our experiments in Section 4, we will work with product manifolds generated based on constant curvature model spaces of dimension \(n=2\).

Now we can summarise our strategy to estimate \(}(B_{^{2}},B_{^{2}})\) as follows (for \(}(B_{^{2}},B_{^{2}})\) it will be entirely analogous). The first step consists of approximating our infinite smooth spaces by finite discrete ones. For this, we consider several collections of points \(\{P_{i}\}_{i I}\) in \(^{2}\) that are sufficiently well distributed. The exponential map can be applied to the collection of points \(:T_{0}^{2}^{2} B_{^{2}}\) to get several collections of points \(Q\) in \(B_{^{2}}\) (again, well distributed by construction). In addition, we will consider several isometric embeddings \(f_{k}:B_{^{2}}^{6}\). Hence, we take

\[}(B_{^{2}},B_{^{2}})_{i,j,k }^{^{6},f_{k},F}}(P_{i},Q_{j})=_{i,j,k} ^{^{6}}}(f_{k}(P_{i}),F(Q_{j})).\] (6)

In Appendix B, we gradually unravel the previous formula and give explicit examples of the involved elements. In particular, in Appendix B.1 we explain how to generate points in the balls of radius one, and in Appendix B.2 how to describe the isometric embedding \(F:B_{^{2}}^{6}\). The results obtained for the Gromov-Hausdorff distances between product manifolds are used to generate the graph search space introduced in the next section.

### The Gromov-Hausdorff-Informed Graph Search Space

Gromov-Hausdorff Edge Weights.In Section 2, we used Cartesian products of constant curvature model spaces to generate candidate latent geometries. We now turn our attention to constructing a search space to find the optimal latent geometry. To do so, we first consider all possible combinations of product manifolds based on a given number of model spaces, represented by \(n_{s}\). Furthermore, we denote the total number of products (model spaces) used to form the product manifold \(\) with \(n_{p}\). Conceptually, \(n_{s}\) is the number of model space _types_ used, while \(n_{p}\) refers to the overall number, or _quantity_, of model spaces that form the resulting product space. For instance, if only the Euclidean plane, the hyperboloid, and the hypersphere

   Comparison Pair & \(d_{}()\) & \(w_{()}\) \\  \((^{2},^{2})\) & 0.23 & 4.35 \\ \((^{2},^{2})\) & 0.77 & 1.30 \\ \((^{2},^{2})\) & 0.84 & 1.20 \\   

Table 1: Estimated Gromov-Hausdorff distances (up to two decimal places) between model spaces and corresponding edge weights in the graph search space.

are taken into account, then \(n_{s}=3\). If all product manifold combinations are considered, the number of elements in the search space increases to \(_{i=1}^{n_{p}}n_{s}^{i}\). However, we assume _commutativity_ for latent product manifolds, implying that the output of a trained neural network with a latent geometry \(_{i}_{j}\) should be the same as that with \(_{j}_{i}\). We refer to this concept as the _symmetry of parameterization_, as neural networks can rearrange the weight matrices of their neurons to achieve optimal performance for two equivalent latent manifolds. This assumption reduces the search space from growing exponentially to \((n_{p}^{2})\), assuming three constant curvature model spaces are considered (see Appendix B.5 for a more complete explanation).

We model the search space as a graph over which we perform BO. More formally, we consider a graph \(\) given by \((,)\), where \(=\{v_{i}\}_{i=1}^{N}\) are the nodes of the graph and \(=\{e_{j}\}_{j=1}^{M}\) is the set of edges, where edge \(e_{j}=(v_{i},v_{j})\) connects nodes \(v_{i}\) and \(v_{j}\). The topology of the graph is encoded in its _adjacency matrix_, which we denote with \(^{N N}\). In our case, we focus on a _weighted_ and _undirected_ graph, meaning that \(_{i,j}=_{j,i}\), and \(_{i,j}\) can take any positive real values. We consider a setting in which the function \(()\) to minimize is defined over the nodes of the graph, and the objective of using BO is to find the node associated to the minimum value \(v^{}=_{v}(v)\).

In our setting, each node in the graph represents a different latent space product manifold, and the value at that node is the validation set performance of a neural network architecture using this latent geometry. We use the inverse of the Gromov-Hausdorff distance between product manifolds to obtain edge weights.

In order to impose additional structure on the search space, we only allow connections between nodes corresponding to product manifolds which differ by a single model space. For example, within the same dimension, \(^{2}^{2}\) and \(^{2}^{2}\) would be connected with edge weighting \(w_{^{2},^{2}}\) while \(^{2}^{2}\) and \(^{2}^{2}\) would have no connection in the graph. Furthermore, product manifolds in different dimensions follow the same rule. For instance, we would have a connection of strength one between \(^{2}^{2}\) and \(^{2}^{2}^{2}\), but no connection between, for instance, \(^{2}\) and \(^{2}^{2}^{2}\) or \(^{2}\) and \(^{2}^{2}\). This construction induces a sense of directionality into the graph and generates clusters of product manifolds of the same dimension. Finally, it should be mentioned that in practice there are only four edge weights. For example, the connectivity strength between \(^{2}^{2}\) and \(^{2}^{2}\) is \(w_{^{2},^{2}}\) since \(_{}(^{2}^{2},^{2} ^{2})=_{}(^{2},^{2})\) given that \(_{}(^{2},^{2})=0\). Visual representations of the graph search space can be found Figures 2 and 3. Note that both figures use the same colour scheme, and in Figure 3 there is an increase in the dimen

Figure 3: Example graph search space for product manifolds composed of up to seven model spaces. Manifolds of different dimensionality are connected with edges coloured grey. Node labels have been omitted for visual clarity.

Figure 2: Slice of the graph search space for latent geometries of dimension 4: product manifolds obtained using 2 models spaces of dimension 2. The graph edges are shown in different colours to depict a different degree of connectivity (black: \(w_{^{2},^{2}}\), red: \(w_{^{2},^{2}}\), blue: \(w_{^{2},^{2}}\)), this is determined by the inverse of the Gromov-Hausdorff distance between the different product manifolds.

sionality of the product manifolds from left to right (e.g. on the top left corner, one can see a triangle corresponding to the three model spaces). We refer the reader to Appendix B.6 for additional visualizations.

Bayesian Optimization over the Graph Search Space.We aim to find the minimum point within the graph search space through the use of BO (see Appendix C). While performing BO on graphs allows us to search for the minimum over a categorical search space, some of the key notions used in BO over Eucledian spaces do not translate directly when operating over a graph. For example, notions of similarity or "closeness" which are trivially found in Eucledian space through the \(_{1}\) or \(_{2}\) norms require more careful consideration when using graphs.

In our setting, we employ a _diffusion kernel_(Smola and Kondor, 2003; Kondor and Vert, 2004) to compute the similarity between the nodes in the graph. The diffusion kernel is based on the eigendecomposition of the graph Laplacian \(^{N N}\), defined as \(=-\) where \(\) is the degree matrix of the graph. In particular, the eigendecomposition of the Laplacian is given by \(=^{}\), where \(=[_{1},,_{N}]\) is a matrix containing eigenvectors as columns and \(=(_{1},,_{N})\) is a diagonal matrix containing increasingly ordered eigenvalues. The _covariance matrix_ used to define the GP over the graph is given by

\[(,)=e^{-} ^{},\] (7)

where \(\) is the _lengthscale_ parameter. Our approach is therefore conceptually similar to that of (Oh et al., 2019), which employ a diffusion kernel to carry out a NAS procedure on a graph Cartesian product. We highlight, however, that the main contribution of this work is the construction of the graph search space, and we employ this search procedure to showcase the suitability of our method in finding the optimal latent product manifold. For reference, we note that other works employing Bayesian optimization in graph-related settings include Cui and Yang (2018); Como et al. (2020); Ma et al. (2019); Ru et al. (2020); Cui et al. (2020); Wan et al. (2021).

## 4 Experimental Setup and Results

In this section, a detailed outline of the experimental results is provided. It comprises of experiments performed on synthetic datasets, as well as experimental validation on custom-designed real-world datasets obtained by morphing the latent space of mixed-curvature autoencoders and latent graph inference. The results demonstrate that the Gromov-Hausdorff-informed graph search space can be leveraged to perform NLGS across a variety of tasks and datasets.

### Synthetic Experiments on Product Manifold Inference

In order to evaluate the effectiveness of the proposed graph search space, we conduct a series of tests using synthetic data. We wish to present a setting for which the latent optimal product manifold is known by construction. To do so, we start by generating a random vector \(\) and mapping it to a "ground truth" product manifold \(_{T}\), which we choose arbitrarily, using the corresponding exponential map. The resulting projected vector is then decoded using a neural network with frozen weights, \(f_{}\), to obtain a reference signal \(y^{_{T}}\), which we wish to recover using NLGS. To generate the rest of the dataset, we then consider the same random vector but map it to a number of other product manifolds \(\{_{i}\}_{i n_{P}}\) with the same number of model spaces as \(_{T}\) but different signatures. Decoding the projected vector through the same neural network yields a set of signal, \(\{y^{_{i}}\}_{i n_{P}}\). We use the aforementioned signals, to set the value associated with each node of the graph search space to be \(MSE(y^{_{T}},y^{_{i}})\). In this way, our search algorithm should aim to find the node in the graph that minimizes the error and hence find the latent geometry that recovers the original reference signal.

Our method consists in using BO over our Gromov-Hausdorff-informed search space. We compare it against random search, and what we call "Naive BO", which performs BO over a fully-connected graph which disregards the Gromov-Hausdorff distance between candidate latent geometries. The figures presented, namely Figures 4 and 5, display the results. Each plot illustrates the performance of the algorithms and baselines as they select different optimal latent product manifolds denoted as \(_{T}\). Notably, the figures demonstrate that the algorithm utilizing the Gromov-Hausdorff-informed search space surpasses all other baselines under consideration and consistently achieves the true function minimum. Figure 4 generally requires fewer iterations compared to Figure 5 to attain the global minimum, primarily due to the smaller size of the search space. It is important to note that our benchmark solely involves the same algorithm but with a change in the search space to a topologically uninformative one. This modification allows us to evaluate the impact of incorporating this information into the optimization process. We have not considered other benchmarks such as evolutionary algorithms since they always rely on a notion of distance between sampled points. Furthermore, as there are no existing algorithms to compute the distance between arbitrary product manifolds, we have not included these methods in our benchmark as they would simply converge to random search. All results are shown on a log scale, and we apply a small offset of \(=10^{-3}\) to the plots to avoid computing \((0)\) when the reference signal is found.

Figure 4: Results (mean and standard deviation over 10 runs) for candidate latent geometries involving product manifolds composed of 13 model spaces. For each plot a different ground truth product manifold \(_{T}\) is used to generate the reference signal.

Figure 5: Results (mean and standard deviation over 10 runs) for candidate latent geometries involving product manifolds composed of 20 model spaces. Same setup as above.

### Experiments on Real-World Datasets

To further validate our method, we conduct additional tests using image and graph datasets. Specifically, we focus on image reconstruction and node classification tasks.

#### Image Reconstruction with Autoencoder

We consider four well-known image datasets: MNIST (Deng, 2012), CIFAR-10 (Krizhevsky et al., 2014), Fashion MNIST (Xiao et al., 2017) and eMNIST (Cohen et al., 2017). Some of these datasets have been shown to benefit from additional topological priors, see Khrulkov et al. (2020) and Moor et al. (2020). We use an autoencoder, detailed more thoroughly in Appendix D, to encode the image in a low dimensional latent space and then reconstruct it based on its latent representation. To test the performance of different latent geometries, we project the latent vector onto the product manifold being evaluated and use the reconstruction loss at the end of training as the reference signal to use in the graph search space. We consider a search space size of \(n_{p}=7\) for MNIST and \(n_{p}=8\) for CIFAR-10, Fashion MNIST and eMNIST. The results are displayed in Figure 6. In line with previous experiments, the Gromov-Hausdorff-informed search graph enables us to find better solutions in a smaller amount of evaluations.

#### Latent Graph Inference

Finally, we consider searching for the optimal product manifold for a graph neural network node classification task using latent graph inference. In particular, we build upon previous work by Saez de Ocariz Borde et al. (2023) which used product manifolds to produce richer embeddings spaces for latent graph inference. We consider the Cora (Sen et al., 2008) and Citeseer (Giles et al., 1998) citation network datasets, and a search space consisting of product manifolds of up to seven model spaces \(n_{p}=7\). The aim is to find the latent space which gives the best results for the node classification problem using minimal query evaluations. Performing BO alongside the Gromov-Hausdorff informed search space gives a clear competitive advantage over Naive BO and random search in the case of Cora. For Citeseer, the experiments do not give such a clear-cut improvement, which can be attributed to the lack of smoothness of the signal on the graph search space and its incompatibility with the intrinsic limitations of the diffusion kernel.

Figure 6: Results (mean and standard deviation over 10 runs) for image reconstruction tasks.

Figure 7: Results (mean and standard deviation over 10 runs) for latent graph inference datasets.

Conclusion

In this work, we have introduced _neural latent geometry search (NLGS)_, a novel problem formulation that consists in finding the optimal latent space geometry of machine learning algorithms using minimal query evaluations. In particular, we have modeled the latent space using product manifolds based on Cartesian products of constant curvature model spaces. To find the optimal product manifold, we propose using Bayesian Optimization over a graph search space. The graph is constructed based on a principled measure of similarity, utilizing the Gromov-Hausdorff distance from metric geometry. The effectiveness of the proposed method is demonstrated through our experiments conducted on a variety of tasks, based on custom-designed synthetic and real-world datasets.

**Limitations and Future Work.** While the NLGS framework is general, we have restricted ourselves to using product manifolds of constant curvature model spaces to model the geometry of the latent space. Furthermore, we have only considered curvatures \(\{-1,0,1\}\), and model spaces of dimension two in order for the optimization problem to be tractable. In future research, there is potential to explore alternative approaches for modeling the latent space manifold. Additionally, the field of Bayesian Optimization over graphs is still in its early stages. Incorporating recent advancements in the area of kernels on graphs (Borovitskiy et al., 2021; Zhi et al., 2023) could lead to improved performance. In our current research, our emphasis is on introducing NLGS and providing an initial solution under a set of simplifying assumptions related to the potential latent manifolds available and the optimization algorithm. The investigation of the impact of using different similarity measures to compare latent structures also remains a subject for future research.

## Societal Impact Statement

This work is unlikely to result in any harmful societal repercussions. Its primary potential lies in its ability to enhance and advance existing data modelling and machine learning methods.