# An _Inverse_ Scaling Law for CLIP Training

Xianhang Li\({}^{*}\)  Zeyu Wang\({}^{*}\)  Cihang Xie

\({}^{*}\)equal contribution

UC Santa Cruz

###### Abstract

CLIP, one of the pioneering foundation models that connect images and text, has enabled many recent breakthroughs in computer vision. However, its associated training cost is prohibitively high, imposing a significant barrier to its widespread exploration. In this paper, we present a surprising finding that there exists an _inverse_ scaling law for CLIP training, whereby the larger the image/text encoders used, the shorter the sequence length of image/text tokens that can be applied in training. Moreover, we showcase that the strategy for reducing image/text token length plays a crucial role in determining the quality of this scaling law.

As a result of this finding, we are able to successfully train CLIP even with limited computational resources. For example, using **8** A100 GPUs, our CLIP models achieve zero-shot top-1 ImageNet-1k accuracies of **63.2%** in \(\)**2 days. 67.8%** in \(\)**3 days**, and 69.3%** in \(\)**4 days**. Our method also works well when scaling up -- with G/14, we register a new record of **83.0%** ImageNet-1k zero-shot accuracy, and meanwhile accelerate the training by \(\)**33\(\)** compared to its OpenCLIP counterpart. By reducing the computation barrier associated with CLIP, we hope to inspire more research in this field, particularly from academics. Our code is available at https://github.com/UCSC-VLAA/CLIPA.

## 1 Introduction

Foundation models  have emerged as a key driving force behind recent breakthroughs in multiple fields, including natural language processing , computer vision , and robotics , and have enabled groundbreaking real-world applications such as ChatGPT  and Stable Diffusion . However, the development, training, and deployment of these models present significant challenges due to their high computational resource requirements and the need for specialized technical expertise, consequently restricting accessibility to a small group of researchers and technology companies.

We hereby focus on studying CLIP , one of the pioneering foundation models  that bridge the gap between text and images and propels computer vision research into the "post-ImageNet" era. The impact of CLIP has been profound, not only in significantly advancing models' zero/few-shot capabilities and out-of-distribution generalization , but also in driving the development of the next generation of image-text foundation models, such as DALL-E  and Flamingo . Although CLIP training is conceptually simple, reproducing CLIP has been challenging for researchers for years.

To increase the accessibility of CLIP, two significant milestones have been achieved: the OpenCLIP  project, which open-sourced the implementation of CLIP, and the release of LAION-400M and LAION-5B datasets , providing a wealth of high-quality image-text pairs for training. Yet, despite these strides, the cost of training associated with CLIP remains prohibitively high. For instance, replicating OpenCLIP-B/32's 62.9% zero-shot top-1 ImageNet-1k accuracy necessitates 36 hours of training with 128 A100 GPUs . This cost is projected to rise considerably with the scaling law[42; 11], which suggests that model performance typically scales proportionally with model size and the number of training tokens, thereby limiting the ability to explore CLIP more broadly and making it challenging for researchers to replicate and build upon these groundbreaking results.

In this paper, we report a surprising finding related to CLIP training that reveals an _inverse_ scaling law. Specifically, we demonstrate that larger image/text encoders allow for the use of shorter image/text token sequences during CLIP training, with only a minor impact on performance. As illustrated in Fig. 1, while a small model S/16 requires a minimum image/text token length of 101/16 to avoid a noticeable performance drop (_e.g._, within 1% in zero-shot ImageNet-1k  accuracy) compared to the vanilla training with the full token resolution, scaling up to L/16 can significantly reduce this requirement to a minimum image/text token length of 50/6. Additionally, it is worth noting that the strategy for reducing image/text tokens is critical, and those that maximize the retention of original (semantic) information tend to yield better scaling effects.

As a byproduct of this observation, we introduce CLIPA, a framework that can train CLIP efficiently and effectively at scale. For example, by training our CLIPA-L/16 for \(\)3 days on a server with eight A100 GPUs, it achieves a highly competitive **67.8%** zero-shot top-1 accuracy on ImageNet-1k. This performance stands in stark contrast to OpenCLIP-B/16, which attains a 67.1% zero-shot top-1 accuracy on ImageNet-1k but requires \(\)\(61\) hours of training on 176 A100 GPUs , thereby costing over **16\(\)** more GPU hours than our CLIPA-L/16. Our CLIPA can accelerate training more with bigger models -- with G/14, CLIPA not only runs \(\)**33\(\)** faster than OpenCLIP in training, but also impressively registers a record-high ImageNet-1k zero-shot top-1 accuracy of **83.0%.

We hope this research will encourage a more diverse group of researchers, particularly those with limited computation resources, to delve into the exploration of CLIP training, or the training of foundation models in general.

## 2 Related Works

**Contrastive Language-Image Pre-training.** Over the past few years, the advent of CLIP  and ALIGN  has transformed the field of visual feature learning through language supervision. By exploiting vast quantities of web-scale data, these pioneering foundation models have shown exceptional zero-shot and out-of-distribution capabilities [42; 65; 70]. The streamlined design of CLIP has facilitated scaling to an unprecedented degree, resulting in substantial performance improvements. As a result, CLIP has been instrumental in empowering a wide range of applications, spanning from segmentation , video understanding , and image generation , to 3D understanding and manipulation [71; 57]. Furthermore, CLIP has played a vital role in catalyzing the development of next-generation image-text foundation models [46; 2; 49].

**Efficient CLIP Training.** The unparalleled success of CLIP hinges on the scale of both the data [42; 52; 25; 7; 66; 73] and the model [65; 37; 54]. While CLIP adheres impeccably to the scaling law [42; 11], it has also inadvertently sparked a race in large-scale training, one that is seemingly beyond the reach of many researchers in the field. This motivates the development of numerous efficient CLIP training methods. From the data perspective, de-replicating [41; 59; 1], re-sampling [61; 19; 31], and automated data curation  have been crucial in creating smaller but high-quality training datasets for accelerating training. On the other hand, FLIP  borrows the idea of masking

Figure 1: **The _inverse_ scaling law for CLIP training.** It indicates that larger image/text encoders enable training with fewer image/text tokens while maintaining competitive performance.

from Masked Image Modeling , and removes a large portion of the input image patches (50-75%) for fast CLIP training. The concurrent work RECLIP  shows resizing input images into a smaller size is a more effective strategy in speeding up training. Our work is based on FLIP, but it goes a step further by 1) exploring more effective semantic-preserving strategies for token length reduction in CLIP training; 2) pushing the idea of token length reduction to an extreme (_i.e_., with only 17 images tokens and 8 text tokens), yielding a significant increase in training acceleration (up to \(25\)).

Scaling law for Language Models.The scaling law has emerged as a powerful tool, linking language model performance with model size, training data, and computational resources with a power-law relation . This conclusion is empirically supported by the GPT model series [6; 37], T-5 [45; 13] and PaLM [12; 3] model families. In this paper, we focus on the scaling behavior of CLIP, but with two critical differences: 1) while the sample efficiency in the language model's scaling law is realized by using few training samples, we probe it by using fewer tokens in each image-text pair in CLIP training; 2) rather than comparing models of different sizes, our observation focuses on performance drop of the same model trained with input of various token lengths.

## 3 Reducing Image/Text Tokens

We study a total of eight token reduction strategies for CLIP training, four for image-based and four for text-based. Although many of these strategies have been extensively studied in the context of masked image/language modeling, such as random masking, which is generally the most effective, we observe that their effects on CLIP training are distinct.

### Training Setup

Our training setup largely follows FLIP . We use the vanilla ViT  as our visual encoder and the non-autoregressive Transformer  architecture as our text encoder. We train our models on the LAION-400M  dataset for 6.4 epochs, equivalent to \(\)2,000 ImageNet-1k epochs; this is then followed by a 0.36-epoch fine-tuning stage on full-resolution images (\(224 224\)) with a maximum text length of 32. To ensure effective contrast between training samples, we set the batch size to \(32k\). We apply a base learning rate of 8e-6 in the main training stage and 4e-7 in the fine-tuning stage. Gradient Checkpointing  is used to conserve GPU/TPU memory. Our data augmentation includes a simple random resizing crop with a minimum cropping ratio of 40%. Detailed hyperparameter settings and model configurations can be found in the appendix. We train L/16 CLIP models using various token reduction strategies and report the corresponding zero-shot top-1 accuracy on ImageNet-1k .

### Image

We start our exploration with FLIP , which employs the random masking strategy from MAE  to reduce image token length during CLIP training. By setting the masking ratio to 75%, our re-implementation effectively reports a zero-shot top-1 ImageNet-1k accuracy of 67.6%.

In addition to random masking, we investigate two other strategies studied in MAE: _grid masking_, which preserves one patch in each \(2 2\) grid window, and _block masking_, which removes large blocks from the input. Fig. 2 provides visual examples of these three strategies at a 75% masking ratio. Intriguingly, while MAE deems random masking as the best strategy for masked image modeling, we notice that CLIP training has a differing preference. For example, grid masking attains a competitive zero-shot top-1 ImageNet-1k accuracy of 67.3%, while block masking is the most effective, achieving a zero-shot top-1 ImageNet-1k accuracy of 68.5%.

Figure 2: Visual comparison of different strategies for reducing image token length.

**Analysis.** We attribute this preference discrepancy to the two tasks' distinct learning natures. In masked image modeling, the objective is to generate absent information from a masked input. Therefore, strategies like random masking that effectively minimize retained information are preferred. In contrast, CLIP training aims to maximize information extraction from the input to achieve better discrimination between different samples. Strategies like block masking, which tend to preserve more structured patterns, can help models yield stronger performance.

**Resizing.** Building upon this analysis, we propose to apply image resizing as a more direct solution to retaining full image information. We use anti-aliasing bilinear interpolation as the resizing method to best preserve image quality. By training with the image resized to \(112 112\) (which is computationally equivalent to 75% masking), the L/16 model achieves a zero-shot top-1 ImageNet-1k accuracy of 68.9%. Notably, this simple resizing strategy surpasses all different mask strategies, highlighting the importance of retaining full input information in CLIP training.

### Text

We next investigate how different strategies for reducing text tokens impact CLIP training. To speed up training, we default to resizing images to \(112 112\) as the image input. We begin with two techniques previously explored in FLIP: _truncation_ and _random masking_. Truncation selects the first \(N\) text tokens and discards the rest, while random masking randomly drops a portion of the text tokens. An illustrative example of these two strategies with a token length of 4 is shown in Fig. 3. By setting a maximum text token length of \(8\), truncation performs slightly better than random masking, resulting in a performance of 68.2% vs. 67.8%.

**Block masking.** We conjecture that the performance gain of truncation over random masking may be partially attributed to the use of consecutive text inputs. This leads us to investigate the efficacy of _block masking_, which randomly preserves consecutive text sequences during training. We limit the number of consecutive text tokens after masking to one for simplicity. With a maximum text token length of 8, this strategy achieves a competitive performance of 68.2%, outperforming random masking by 0.4%.

**Syntax masking.** Another potential approach to improving random masking is to assign different masking priorities to parts of speech. Specifically, we prioritize retaining nouns, followed by adjectives, and then other words. We refer to this strategy as syntax masking. With a maximum text token length of \(8\), syntax masking achieves the best performance among all strategies, recording a zero-shot top-1 ImageNet-1k accuracy of 69.0%.

In the next section, we systematically analyze how these four image-based strategies, namely, _random masking_, _grid masking_, _block masking_, and _image resizing_, and four text-based strategies, namely, _truncation_, _random masking_, _block masking_, and _syntax masking_, scale with varying token lengths across different model sizes.

## 4 An _Inverse_ Scaling Law

**Training setup.** Models of three different scales are used: S/16, B/16, and L/16. Each model includes a visual encoder, namely ViT-S/16 (22M parameters), ViT-B/16 (87M parameters), and ViT-L/16 (304M parameters) . In addition, we use text encoders with 33M, 53M, and 109M parameters, respectively. All these models are trained using the same setup outlined in Sec. 3.1, with one exception that a larger learning rate of 8e-7 is utilized during fine-tuning for S/16 and B/16.

### Image

We first ablate how varying image token lengths affect CLIP training. Specifically, for random masking, block masking, and image resizing, we range the image token length from the full resolution

Figure 3: Visual comparison of different strategies for reducing text token length.

(196 tokens) to an order of magnitude smaller one (16 tokens); for grid masking, the smallest length is set to 49 (i.e., selecting one in each \(2 2\) window), as it is non-trivial to further reduce it. Note we do not touch the setup for text encoders here, keeping the maximum length for text tokens as 32.

**Main Observation.** We analyze the zero-shot top-1 accuracy on ImageNet-1k  and plot the performance drop compared to the full resolution baseline in Fig. 4. Firstly, we note that performance generally decreases monotonically as token length reduces, which is expected given that models learn less information per sample. The only exceptional case occurs for block masking -- when nearly halving the token length from 197 to 99, the performance for L/16 even slightly increases by 0.3%.

Furthermore, we observe that the performance drop for all four token reduction strategies becomes smaller as the model size increases. For instance, when reducing the token length from 197 to 17 using the resizing strategy, S/16 experiences a 6.2% performance drop, whereas scaling up the model size to B/16 reduces this drop to 4.3%; further using the considerably larger L/16 results in only a 3.0% performance drop. In other words, it suggests that larger models have the ability to achieve the same performance drop compared to the full-resolution baseline by utilizing fewer image tokens, as compared to their smaller counterparts. We term this phenomenon as the _inverse scaling law for CLIP training_, implying that by using larger models, we can train with fewer image tokens per sample while still delivering competitive performance.

Lastly, we find that the quality of this inverse scaling law strongly depends on how tokens are removed. More precisely, the more information that is retained, the smaller the length of tokens that can be applied during training. For instance, with a performance drop threshold of 2%, random masking requires 99 tokens for B/16 training. However, switching to image resizing, which retains substantially more image information, allows for a significant reduction in the minimum token length, down to 37.

For interested readers, we additionally offer two alternative views to understanding this scaling behavior in Fig. 9 (_i.e._, model size _vs_. performance drop) and Fig. 10 (_i.e._, token number _vs_. accuracy) in the Appendix.

**Zero-shot retrieval.** We further evaluate the image/text retrieval performance of CLIP with varying image token lengths on the challenging COCO  dataset. Fig. 5 shows the performance drop across different models for four image token reduction strategies. We note that, in most cases, the inverse scaling law proves consistent, as the degree of performance drop gradually decreases with increasing model size. For instance, using the random masking strategy that reduces the token length from 197 to 17, S/16 experiences a performance drop of 6.6% and 7.1% for image and text retrieval tasks, respectively. In comparison, the performance drops for B/16 are 5.8% and 5.9%, respectively; this performance drop is further reduced to 4.6% and 4.1% for L/16.

**Zero-shot robustness evaluation.** Fig. 6 reports robustness of the aforementioned models, tested on the ImageNet-V2 , ImageNet-R , ImageNet-A , and ImageNet-Sketch  datasets. We observe that, in most cases, larger models have a lesser performance drop than small models, which again confirms the validity of this inverse scaling law.

Figure 4: **The inverse scaling law on image tokens.** Compared to small models, larger models can utilize fewer image tokens to achieve the same performance drop to the full-resolution baseline.

### Text

We next study the impact of altering the maximum text token length on CLIP training. We use all four text reduction strategies introduced in Sec. 3.3, and for each strategy, we range the maximum text token length from 32 to 4. Additionally, to speed up training, we apply a resized \(112 112\) image as input, which runs \(\)\(4\) faster than the \(224 224\) input, while only slightly affecting performance, _i.e._, 0.3% drop on zero-shot top-1 ImageNet-1k accuracy for L/16.

Main observation.The data presented in Fig. 7 reflects a pattern similar to the one observed with image token, _i.e_., the inverse scaling law is also evident when learning with text tokens. For example, when the maximum text length is set to 4 and the model size is scaled from S/16 to L/16, we observe a decrease in the performance drop from 5.7% to 5.2% for truncation, 3.4% to 2.0% for syntax masking, 4.3% to 2.9% for block masking, and 5.9% to 5.1% for random masking. Moreover, our analysis suggests that syntax masking is the most effective strategy for reducing text tokens, especially when setting the maximum text token lengths to be extremely short. For instance, with B/16 and a maximum text token length of 4, all other strategies incur a performance drop of more than 4.0%, whereas syntax masking results in a performance drop of merely 3.0%. Furthermore, we observe that for all strategies, the sensitivity of CLIP training to the reduction of text tokens remains relatively low until a threshold of 8 tokens is reached (_e.g._, the performance drop is less than \(\)1.0%). However, beyond this point, the use of fewer text tokens leads to an abrupt performance drop.

Figure 5: Zero-shot image/text retrieval performance on COCO . Recall@1 is reported.

Figure 6: Zero-shot robustness performance.

Lastly, we notice another intriguing inverse scaling law uniquely related to syntax masking: reducing the text token length from 32 to 16 or 8 consistently enhances the performance of B/16 and L/16 models. This observation suggests that the language signal in our training data may be noisy, and filtering out certain information could potentially facilitate more effective representation learning.

**Zero-shot robustness & zero-shot retrieval evaluations.** We observe a similar trend for zero-shot robustness evaluation, where larger models typically yield smaller relative performance drops. In terms of zero-shot retrieval performance, for all four text token reduction strategies, we make two interesting observations: 1) there is almost no performance drop for all models when the text token length is reduced to 16; 2) further reducing the text token length to 8 or less, scaling up model size does not evidently help to reduce the performance drop. This second observation is expected, as reducing text length directly affects the capability to align image and text features in a fine-grained manner. Due to space limitations, we include the detailed results of the zero-shot image/text retrieval performance and the zero-shot robustness in Appendix.

### ConvNeXt

In addition to ViT, we validate whether this inverse scaling law is also apparent within the context of CNN architectures. For this analysis, we select ConvNeXt , given its outstanding performance on various visual benchmarks. Although different masking strategies are applicable for ConvNeXt, they can only offer a modest training speedup due to the lack of computationally efficient support for sparse convolution . However, image resizing emerges as a viable strategy for expediting CLIP training with ConvNeXt, as it avoids the need of using sparse convolution [55; 20].

We focus on studying ConvNeXt-T and ConvNeXt-B, which are of a similar scale to ViT-S/16 and ViT-B/16, respectively. We utilize the same training setup as for ViT, and incorporate additional augmentations [9; 10]. The full results are listed in Appendix.

**Main Observation.** We observe that ConvNeXt-B consistently shows a smaller performance drop than ConvNeXt-T when a smaller input size is applied. By setting a performance drop of 1.5% as the threshold, we find that while ConvNeXt-T necessitates an input image size of \(112 112\), scaling to ConvNeXt-B enables further reduction of the input size to \(96 96\). These observations confirm the existence of the inverse scaling law for ConvNeXt in CLIP training.

## 5 Training CLIP with Limited Resources

Our discussions in Sec. 4 reveal that larger models have the ability to train with fewer tokens while still preserving competitive performance. This ability brings substantial practical benefits, including improved memory footprint and faster training speed. In this section, we showcase how this inverse scaling law can be leveraged to train CLIP models efficiently and effectively, particularly when computational resources are limited.

Figure 7: **The inverse scaling law on text tokens.** Similar to the observation with image tokens, larger models enable training with fewer text tokens while maintaining competitive performance.

We start by recasting the image resizing results of Fig. 4 in the context of _computation vs. performance_ shown in Fig. 8. In addition to the clear performance advantage of larger models over smaller ones, an interesting observation is that this inverse scaling law offers the potential for faster and more powerful CLIP training. For instance, our L/16 model, using a total image token length of 17, outperforms the standard B/16 model setup (_i.e_., with a total image token length of 197) by 2.5%, while achieving a \(1.7\) speedup. This process can be further accelerated by training with fewer text tokens, especially when employing a large text encoder (_e.g_., as in H/14).

Motivated by the above observations, we introduce an effective and efficient CLIP training strategy: training with a larger model but with reduced input token lengths. This approach, dubbed as _CLIPA_, enables CLIP training even with academic resources. The training setup of CLIPA follows the protocol outlined in Section 3.1, with the addition of color jitter and grayscale image augmentation , and the usage of global average pooling in ViT . To reduce the token length in CLIP training, image resizing and text truncation are used by default. More training details can be found in Appendix. All these models are trained using the OpenCLIP codebase  in PyTorch  on a machine equipped with 8 NVIDIA A100 GPUs.

As demonstrated in Tab. 1, our CLIPA provides both faster training times and improved performance in comparison to OpenCLIP. For example, our CLIPA-B/16 surpasses the vanilla OpenCLIP-B/32 baseline by 0.3% on zero-shot ImageNet-1k classification, more importantly, requiring \(\)**10\(\)** fewer GPU hours. Similarly, our CLIPA-L/16 outperforms the vanilla OpenCLIP-B/16 baseline by 0.7%, yet consumes **17\(\)** fewer GPU hours. Notably, our CLIPA-B/16 can be trained on an 8-A100 GPU machine in \(\)\(2\) days, and CLIPA-L/16 in \(\)\(3\) days, highlighting the efficiency and effectiveness of CLIPA in facilitating CLIP training while preserving competitive performance.

  & & & & & & & & & & & & & & & \\  & & & & & & & & & & & & & & & \\ model & samples/image resolution & GPU hours & & & & & & & & & & & & \\  & & & & & & & & & & & & & & \\  OpenAL-B/32, Our Eval & 12.88\(\)22\({}^{d}\) & 4600 & 63.3 & 55.9 & 31.6 & 69.3 & 44.2 & 42.3 & 30.4 & 50.2 & 58.9 & 77.6 \\ OpenAI-B/16, Our Eval & 12.88\(\)22\({}^{d}\) & 10700 & 68.3 & 61.9 & 49.9 & 77.7 & 55.3 & 48.2 & 33.1 & 52.4 & 62.1 & 81.9 \\ OpenAI-L/14, Our Eval & 12.88\(\)22\({}^{d}\) & 50800 & 75.5 & 69.8 & 70.8 & 87.8 & 68.9 & 59.6 & 36.5 & 56.4 & 65.3 & 85.1 \\  OpenCLIP-B/32, Our Eval & 12.88\(\)22\({}^{d}\) & 4600 & 62.9 & 55.1 & 21.7 & 73.4 & 48.9 & 49.4 & 35.3 & 52.6 & 61.7 & 79.0 \\ OpenCLIP-B/16, Our Eval & 12.88\(\)22\({}^{d}\) & 10700 & 67.1 & 59.6 & 33.2 & 77.9 & 51.5 & 52.4 & 38.3 & 55.4 & 65.5 & 83.3 \\ OpenCLIP-L/14, Our Eval & 12.88\(\)22\({}^{d}\) & 50800 & 72.8 & 65.4 & 46.5 & 84.9 & 59.9 & 59.6 & 43.0 & 59.7 & 70.3 & 87.6 \\   CLIPA-B/16 (507.16) & 2.56\(\)112\({}^{d}\)+128M\(\)22\({}^{d}\) & 444 & 63.2 & 55.6 & 28.3 & 72.2 & 44.3 & 48.7 & 32.2 & 53.1 & 58.3 & 75.3 \\ CLIPA-L/16 (171.16) & 2.56\(\)61\({}^{d}\)+128M\(\)22\({}^{d}\) & 62.8 & 67.8 & 60.4 & 38.3 & 81.2 & 52.8 & 56.4 & 40.1 & 58.4 & 64.0 & 81.5 \\ CLIPA-L/16 (137.78) & 2.56\(\)89\({}^{d}\)+128M\(\)22\({}^{d}\) & 826 & 69.3 & 61.7 & 43.6 & 84.0 & 55.4 & 58.7 & 39.8 & 56.8 & 67.5 & 81.9 \\  

Table 1: **Training CLIPA with limited resources.** CLIPA models are first pre-trained with smaller token lengths with 2.56B training samples and subsequently fine-tuned with full token lengths with 128M epochs on LAION-400M. These models are trained on an 8-A100 GPU machine. ‘(IX,TY)’ indicates the model is pre-trained with an image token length of \(X\), and a maximum text token length of \(Y\). Image resizing and text truncation are used for token length reduction.

Figure 8: **Accuracy _vs._ compute trade-off.** The x-axis shows overall training cost, and the y-axis shows corresponding ImageNet-1k zero-shot accuracy. The models are trained with different token lengths, resulting in varying compute costs. \({}^{*}\)_ indicates the application of additional color jitter and grayscale augmentation, as well as the use of global average pooling instead of the classification token. These modifications are found to be beneficial for stabilizing training with reduced token lengths in large models.

**H/14 model.** We hereby include a bigger model, CLIPA-H/14, for experiments. Note that, here we cut the input text token length from 32 to 8, yielding an additional \(\)1.3\(\) training speedup. These results are added to Fig. 8. With an input image size of 84 and a text token length of 8, our CLIPA-H/14 achieves a compelling zero-shot top-1 ImageNet-1k accuracy of 72.8%. This performance is on par with that of OpenCLIP-L/14, while the total computational requirement is reduced by \(\)**25\(\)**.

## 6 CLIPA at Scale

In this section, we delve deeper into the scaling behavior of CLIPA with larger models (_e.g_., G/14) and larger datasets (_i.e_., LAION-2B  and DataComp-1B ). We default to the setup of 12.8B pre-training samples. We find that extending the fine-tuning schedule at a resolution of \(224 224\) from 128M to 512M training samples, followed by another 128M samples's training at \(336 336\) resolution, demonstrates a clear improvement with the H/14 model (79.1% _vs_. 77.7%). Moreover, our updated fine-tuning schedule incorporates a random masking strategy at both resolutions (30% for \(224 224\) and 40% for \(336 336\)), which reduces the training overheads by a large margin with little-to-no performance drop. More details can be found in the Appendix.

**Main results.** As shown in Tab. 2, when trained on the same dataset LAION-2B and with the \(224 224\) resolution, our CLIPA-H/14 attains comparable performance with OpenCLIP-H/14 but merely with \(\)**1/15** training computations. This signifies a remarkable decrease in cost - _e.g_., given that the training cost for the reported OpenCLIP result amounts to \(\)5,600 GPU-days, CLIPA could save \(\)5,230 GPU-days. Additionally, compared with FLIP-H/14, our CLIPA-H/14 achieves a better 79.1% ImageNet-1k performance but can be 6\(\) faster.

When continuing scaling our model size to G/14, with the same number of seen samples from the DataComp-1B  dataset, we successfully establish a new state-of-the-art open-sourced ImageNet-1k zero-shot accuracy of **83.0%.** Notably, this is achieved with \(\)**33**\(\) less computation compared with previous OpenCLIP-G/14 model. These findings could potentially pave the way for the training of even larger models on even larger datasets, particularly for those with substantial access to GPU/TPU resources.

To further evaluate the performance of our approach, we also evaluate our CLIPA-H/14 model on the VTAB benchmark . The results are included in Tab. 3. On this highly diverse and challenging set of vision tasks, CLIPA still achieves comparable or even superior performances but with significantly less training cost, demonstrating its good generalizability.

## 7 Limitation

The recent work  shows that CLIP models generally are limited at capturing relationships, attributes, and order information. To give a more comprehensive evaluation, we compare our CLIPA model with OpenCLIP on the ARO benchmark , a dataset created to evaluate the ability to understand different types of relationships, attributes, and order information. The results are shown in the Appendix (Tab. 14). We can observe that, while OpenCLIP-B/16 slightly outperforms CLIPA-B/16, the absolute performance of both models remains somewhat limited.

   &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & & \\  FLIP-H/14. Our Eval & LAION-2B & 25.6\(\)22\(\) 1286\(\)22\(\)12 & 24 & 74.1 & 76.3 & 90.8 & 64.7 & 67.5 & 49.9 & 67.3 & 59.4 \\ Overall-PLPL-H/14 & LAION-3B & 31.2\(\)22\(\) & 5.7 & 78.0 & 78.8 & 89.2 & 89.3 & 65.7 & 66.6 & 46.3 & 66.0 & 79.3 & 79.8 \\ OurCLIPA-C/14 & LAION-3B & 31.2\(\)22\(\) & 47.0\(\)22\(\)12 & 29.8 & 80.1 & 71.6 & 70.6 & 89.2 & 73.0 & 69.1 & 64.3 & 67.8 & **79.5** \\  CLIPA-H/00/15 (20.75) & LAION-2B & 12.8\(\)66\(\)22\(\)202\(\)212 & 00 & 47 & 79.5 & 71.4 & 66.2 & 91.3 & 71.4 & 66.0 & 84.3 & 69.9 & 79.2 & 91.8 \\
12.8\(\)66\(\)202 & -5.1\(\)22\(\)202\(\)212 & 00 & 47 & 79.1 & 71.3 & 92.2 & 69.9 & 70.0 & 50.2 & 61.8 & 67.8 & 52.8 & 52.8 \\ CLIPA-H/00/14 (20.75) & DACom-1B & 12.8\(\)66\(\)2\(\)202\(\)212 & 00 & 44 & 81.5 & 75.0 & 78.9 & 54.3 & 74.1 & 72.7 & 69.1 & 67.5 & 57.8 & 69.8 \\
12.8\(\)66\(\)202 & -5.1\(\)22\(\)212 & 00 & 48 & 81.5 & 68.7 & **94.4** & 77.4 & 72.8 & 41.2 & 67.2 & 76.3 & 40.8 \\ CLIPA-H/00/15 (20.75) & DACom-2B & 12.8\(\)66\(\)2\(\)202\(\)212 & 00 & 48 & 82.7 & 79.8 & 87.1 & 89.1 & 77.4 & 71.3 & 50.0 & **69.7** & 79.7 & 71.8 \\
12.8\(\)66\(\)202 & -5.1\(\)202\(\)212 & 00 & 48 & **80.6** & **79.3** & **88.3** & **89.4** & **89.2** & **79.8** & 50.4 & 87.8 & 78.2 & 50.1 \\  

Table 2: **Training CLIPA at scale.** CLIPA models are first pre-trained with smaller token lengths with 12.8B pre-training samples and subsequently fine-tuned with full token lengths. The Compute cost is measured in the GFLOPs of the model times the number of samples seen during training. *(IX,TY)’ indicates the model is pre-trained with an image token length of \(X\), and a maximum text token length of \(Y\).

To mitigate this relational understanding issue, a composition-aware hard negative mining strategy (NegCLIP) is introduced in . Note that this strategy is extremely lightweight, and can be seamlessly integrated as an additional fine-tuning stage in enhancing CLIP's text understanding ability. Our results in Tab. 14 also corroborate the efficacy of NegCLIP, e.g., both OpenCLIP and CLIPA nearly double their performance on benchmarks like COCO-Order and Flickr30k-Order. Concerning the initial underperformance on ARO benchmarks, we leave it as a future work.

## 8 Conclusion

In this paper, we delve deep into CLIP training. Our investigation unveils an intriguing inverse scaling law, suggesting that larger models require fewer input tokens during training. Moreover, among the eight token reduction strategies we studied, we identify that resizing for image input and syntax masking for text input provides the best overall scaling quality. This finding underscores the crucial role of semantic information preservation in efficient CLIP training. Our findings can enable significantly faster CLIP training with better results, especially given limited resources. We hope that our work could encourage a wider range of researchers, particularly those lacking access to substantial computational resources, to engage more in exploring CLIP training.

## 9 Broader Impact

Large foundation models trained by language supervision have emerged as a pivotal force driving recent advancements in the language and vision domain. Our discovery of the inverse scaling law has democratized access to this technology, enabling the training of proficient CLIP models on a modest budget. This breakthrough has substantial environmental implications, as it significantly curtails tens of thousands of GPU/TPU hours, thereby reducing energy consumption and associated carbon emissions. It is also worth mentioning that our models are trained on publicly available web-scale datasets . Therefore, the derived weights may inadvertently mirror any bias or harmful contents inherent in the training sets. As such, care should be taken when interpreting the outputs of such models and deploy them in the real-world applications.