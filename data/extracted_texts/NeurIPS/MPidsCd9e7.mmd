# Adversarially Robust Dense-Sparse Tradeoffs via Heavy-Hitters

David P. Woodruff

Department of Computer Science

Carnegie Mellon University

dwoodruf@andrew.cmu.edu

&Samson Zhou

Department of Computer Science

Texas A&M University

samsonzhou@gmail.com

###### Abstract

In the adversarial streaming model, the input is a sequence of adaptive updates that defines an underlying dataset and the goal is to approximate, collect, or compute some statistic while using space sublinear in the size of the dataset. In 2022, Ben-Eliezer, Eden, and Onak showed a dense-sparse trade-off technique that elegantly combined sparse recovery with known techniques using differential privacy and sketch switching to achieve adversarially robust algorithms for \(L_{p}\) estimation and other algorithms on turnstile streams. However, there has been no progress since, either in terms of achievability or impossibility. In this work, we first give improved algorithms for adversarially robust \(L_{p}\)-heavy hitters, utilizing deterministic turnstile heavy-hitter algorithms with better tradeoffs. We then utilize our heavy-hitter algorithm to reduce the problem to estimating the frequency moment of the tail vector. We give a new algorithm for this problem in the classical streaming setting, which achieves additive error and uses space independent in the size of the tail. We then leverage these ingredients to give an improved algorithm for adversarially robust \(L_{p}\) estimation on turnstile streams. We believe that our results serve as an important conceptual message, demonstrating that there is no inherent barrier at the previous state-of-the-art.

## 1 Introduction

Adversarial robustness for big data models is increasingly important not only for ensuring the reliability and security of algorithmic design against malicious inputs and manipulations, but also to retain guarantees for honest inputs that are nonetheless co-dependent with previous outputs of the algorithm. One such big data model is the streaming model of computation, which has emerged as a central paradigm for studying statistics of datasets that are too large to store. Common examples of datasets that are well-represented by data streams include database logs generated from e-commerce transactions, Internet of Things sensors, scientific observations, social network traffic, or stock markets. To capture these applications, the one-pass streaming model defines an underlying dataset that evolves over time through a number of sequential updates that are discarded irrevocably after processing, and the goal is to compute or approximate some fixed function of the dataset while using space sublinear in both the length \(m\) of the data stream and the dimension \(n\) of the dataset.

The streaming model of computation.In the classical _oblivious_ streaming model, the stream \(S\) of updates \(u_{1},,u_{m}\) defines a dataset that is fixed in advance, though the ordering of the sequence of updates may be adversarial. In other words, the dataset is oblivious to any algorithmic design choices, such as instantiations of internal random variables. This is vital for many streaming algorithms, which crucially leverage randomness to achieve meaningful guarantees in sublinear space. For example, the celebrated AMS sketch  initializes a random sign vector \(s\) and outputs \(| s,f|\) as the estimate for the \(L_{2}\) norm of the underlying frequency vector \(f\) defined by the stream. To showcorrectness of the sketch, we require \(s\) to be chosen uniformly at random, independent of the value of \(f\). Similar assumptions are standard across many fundamental sublinear algorithms for machine learning, such as linear regression, low-rank approximation, or column subset selection.

Unfortunately, such an assumption is unreasonable , as an honest user may need to repeatedly interact with an algorithm, choosing their future actions based on responses to previous questions. For example, in recommendation systems, it is advisable to produce suggestions so that when a user later decides to dismiss some of the items previously recommended by the algorithm, a new high-quality list of suggestions can be quickly computed without solving the entire problem from scratch . Another example is in stochastic gradient descent or linear programming, where each time step can update the eventual output by an amount based on a previous query. For tasks such as linear regression, actions as simple as sorting a dataset have been shown to cause popular machine learning libraries to fail .

**Adversarially robust streaming model.** In the adversarial streaming model , a sequence of adaptively chosen updates \(u_{1},,u_{m}\) is given as an input data stream to an algorithm. The adversary may choose to generate future updates based on previous outputs of the algorithm, while the goal of the algorithm is to correctly approximate or compute a fixed function at all times in the stream. Formally, the _black-box_ adversarial streaming model can be modeled as a two-player game between a streaming algorithm \(\) and a source \(\) that creates a stream of adaptive and possibly adversarial inputs to \(\). Prior to the game, a fixed statistic \(\) is determined, so that the goal of the algorithm is to approximate \(\) on the sequence of inputs seen at each time. The game then proceeds across \(m\) rounds. In the \(t\)-th round:

1. \(\) computes an update \(u_{t}\) for the stream, which possibly depends on all previous outputs from \(\).
2. \(\) uses \(u_{t}\) to update its data structures \(_{t}\), acquires a fresh batch \(R_{t}\) of random bits, and outputs a response \(Z_{t}\) to the query \(\).
3. \(\) observes and records the response \(Z_{t}\).

The goal of \(\) is to induce from \(\) an incorrect response \(Z_{t}\) to the query \(\) at some time \(t[m]\) throughout the stream using its control over the sequence \(u_{1},,u_{m}\). By the nature of the game, only a single pass over the stream is permitted. In the context of our paper, each update \(u_{t}\) has the form \((i_{t},_{t})\), where \(i_{t}[n]\) and \(_{t}\{ 1\}\). The updates implicitly define a frequency vector \(f^{n}\), so that \(u_{t}\) changes the value of the \((i_{t})\)-th coordinate of \(f\) by \(_{t}\).

**Turnstile streams and flip number.** In the turnstile model of streaming, updates are allowed to either increase or decrease the weight of elements in the underlying dataset, as compared to insertion-only streams, where updates are only allowed to increase the weight. Whereas various techniques are known for the adversarial robustness on insertion-only streams, significantly less is known for turnstile streams. While near-optimal adversarially robust streaming algorithms for fundamental problem such as \(L_{p}\) estimation have been achieved in polylogarithmic space for \(p 2\) by  in the insertion-only model, it is a well-known open question whether there exists a constant \(C=(1)\) such that the same problems require space \((n^{C})\), where \(n\) is the dimension of the underlying frequency vector. Indeed,  showed that the existence of a constant \(C=(1)\) such that no linear sketch with sketching dimension \(o(n^{C})\) can approximate the \(L_{2}\) norm of an underlying frequency vector within even a polynomial multiplicative factor, when the adversarial input stream is turnstile and real-valued.

Given an accuracy parameter \((1+)\), the _flip number_\(\) is the number of times the target function \(\) changes by a factor of \((1+())\). It is known that for polynomially-bounded monotone functions \(\) on insertion-only streams, we generally have \(=( m)\), but for turnstile streams that toggle the underlying frequency vector between the all-zeros vector and a nonzero vector with each update, we may have \(=(m)\). There are various techniques that then implement \(\) or even roughly \(\) independent instances of an oblivious algorithm, processing all stream updates to all instances. Therefore, the space complexity of these approaches are at least roughly \(\) times the space required by the oblivious algorithm, which may not be desirable in large setting of \(=(m)\) for turnstile streams. By considering _dense-sparse tradeoffs_,  gave a general framework that improved upon the \(}()=}()\) space bounds due to the flip number. In particular, their results show that \(}(m^{p/(2p+1)})\) space suffices for the goal of \(L_{p}\) norm estimation, where the objective is to estimate \((f_{1}^{p}++f_{n}^{p})^{1/p}\) for an input vector \(f^{n}\), which is an important problem that has a number of applications, such as network traffic monitoring , clustering and other high-dimensional geometry problems , low-rank approximation and linear regression , cascaded norm estimation , and entropy estimation . Unfortunately, there has been no progress for \(L_{p}\) estimation on turnstile streams since the work of , either in terms of achievability or impossibility. Thus we ask:

Is there a fundamental barrier for adversarially robust \(L_{p}\) estimation on turnstile streams beyond the dense-sparse tradeoffs?

### Our Contributions

In this paper, we answer the above question in the negative. We show that the techniques of  do not realize a fundamental limit for adversarially robust \(L_{p}\) estimation on turnstile streams. In particular, we give an algorithm that uses space \(}(m^{c})\), for some constant \(c<\) for \(p(1,2)\). We first require an adversarially robust algorithm for heavy-hitters.

**Heavy hitters.** Recall that the \(\)-\(L_{p}\)-heavy hitter problem is defined as follows.

**Definition 1.1** (\(\)-\(L_{p}\)-heavy hitters).: _Given a vector \(f^{n}\) and a threshold parameter \((0,1)\), output a list \(\) that includes all \(i\) such that \(f_{i}\|f\|_{p}\) and includes no \(j\) such that \(f_{j}<\|f\|_{p}\)._

Generally, heavy-hitter algorithms actually solve the harder problem of outputting a estimated frequency \(_{i}\) such that \(|_{i}-f_{i}| C\|f\|_{p}\), for each \(i[n]\), where \(C\) is some constant such as \(\). Observe that such a guarantee solves the \(\)-\(L_{p}\)-heavy hitters problem because each \(i\) such that \(f_{i}\|f\|_{p}\) must have \(_{i}>\|f\|_{p}\) and similarly each \(j\) such that \(_{j}\) must have \(f_{j}\|f\|_{p}\). We give an adversarially robust streaming algorithm for the \(L_{p}\)-heavy hitters problem on turnstile streams.

**Theorem 1.2**.: _Let \(p\). There exists an algorithm that uses \(}(}m^{(2p-2)/(4p-3)})\) bits of space and solves the \(\)-\(L_{p}\)-heavy hitters problem at all times in an adversarial stream of length \(m\)._

Though not necessarily obvious, our result in Theorem1.2 improves on the dense-sparse framework of  across all \(p[1,2)\). Specifically, the result of  uses space \(}(m^{})\) for \(=\), while our result uses space \(}(m^{})\) for \(=\). It can be shown that \(-=\), which is at positive for all \(p[1,2)\). Thus our result is an important conceptual contribution showing that the true nature of the heavy-hitter problem lies beyond the techniques of .

A particular regime of interest is \(p=1\), where the previous dense-sparse framework of  achieves \(}(m^{1/3})\) bits of space, but our result in Theorem1.2 only requires polylogarithmic space.

**Moment estimation.** Along the way to our main result, we also give a new algorithm for estimating the residual of a frequency vector up to some tail error. More precisely, given a frequency vector \(f\) that is defined implicitly through a data stream and a parameter \(k>0\), let \(g\) be a tail vector of \(f\), which omits the \(k\) entries of \(f\) largest in magnitude, breaking ties arbitrarily. Similarly, let \(h\) be a tail vector of \(f\) that omits the \((1-)k\) entries of \(f\) largest in magnitude, where \((0,1)\) serves as an error parameter. Then we give a one-pass streaming algorithm that outputs an estimate for \(\|g\|_{p}^{p}\) up to additive \(\|h\|_{p}^{p}\), using space \((, n)\). In particular, our space is independent of the tail parameter \(k\). We defer the full guarantees of our algorithm as well as a more formal discussion to Section3. We then give our main result:

**Theorem 1.3**.: _Let \(p\) and \(c=-23p+4}{(4p-3)(12p+3)}\). There exists a streaming algorithm that uses \((m^{c})(,(nm))\) bits of space and outputs a \((1+)\)-approximation to the \(L_{p}\) norm of the underlying vector at all times of an adversarial stream of length \(m\)._It can again be shown that our result in Theorem 1.3 again improves on the dense-sparse framework of  across all \(p(1,2)\). For example, for \(p=1.5\), the previous result uses space \(}(m^{3/8})=}(m^{0.375})\), while our algorithm uses space \(}(m^{47/126})}(m^{ 0.373})\). Although our quantitative improvement is mild, we believe it nevertheless illustrates an important message which shows that the dense-sparse technique does not serve as an impossibility barrier.

### Technical Overview

Recall that the _flip number_\(\) to be the number of times the \(F_{p}\) moment changes by a factor of \((1+())\), given a target accuracy \((1+)\). Given a stream with flip number \(\), the standard _sketch-switching_ technique  for adversarial robustness is to implement \(\) independent instances of an oblivious streaming algorithm for \(F_{p}\) estimation, iteratively using the output of each algorithm only when it differs from the output of the previous algorithm by a \((1+)\)-multiplicative factor. Subsequently,  showed that by using differential privacy, it suffices to use roughly \(\) independent instances of an oblivious streaming algorithm for \(F_{p}\) estimation to achieve correctness at all times for an adaptive input stream. Unfortunately, the flip number for a stream of length \(m\) can be as large as \((m)\), such as in the case where the underlying frequency vector alternates between the all zeros vector and a nonzero vector.

The dense-sparse framework of  observes that the only case where the flip number can be large is when there are a large number of times in the stream where the corresponding frequency vector is somewhat sparse. For example, in the above scenario where the underlying frequency vector alternates between the all zeros vector and a nonzero vector, all input vectors are \(1\)-sparse. In fact, they notice that for \(F_{p}\) estimation, that once the frequency vector has at least \(m^{C}\) nonzero entries for any fixed constant \(C(0,1)\), then since all entries must be integral and all updates only change each entry by \(1\), at least \(_{}(m^{C/p})\) updates are necessary before the \(p\)-th moment of the resulting frequency vector can differ by at least \((1+)\)-multiplicative factor. Hence in the stream updates where the frequency vector has at least \(m^{C}\) nonzero entries, the flip number can be at most \((m^{1-C/p})\), for \(=(1)\). Thus it suffices to run \(}(m^{1/2-C/2p})\) independent instances of the oblivious algorithm, using the differential privacy technique of . Moreover, in the case where the vector is \(m^{C}\)-sparse, there are sparse recovery techniques that can exactly recover all the nonzero coordinates using \(}(m^{C})\) space, even if the input is adaptive. Hence by balancing \(}(m^{C})=}(m^{(1/2-C /2p)})\) at \(C=\),  achieves \(}(m^{p/(2p+1)})\) overall space for \(F_{p}\) estimation for adaptive turnstile streams.

Our key observation is that for \(p(1,2)\), if the frequency vector has at least \(m^{C}\) nonzero entries, a sequence of \(_{}(m^{C/p})\) updates may not always change the \(p\)-th moment of the underlying vector. For example, if the updates are all to separate coordinates, then the \(p\)-th moment may actually change very little. In fact, a sequence of \(_{}(m^{C/p})\) updates may _only_ change the \(p\)-th moment of the underlying vector by \((1+)\) if most of the updates are to a small number of coordinates. As a result, most of the updates are to some coordinate that was either initially a heavy-hitter or subsequently a heavy-hitter. Then by tracking the heavy-hitters of the underlying frequency vector, we can handle the hard input for , thus demanding a larger number of stream updates before the \(p\)-th moment of the vector can change by \((1+)\). Consequently, the number of independent instances decreases, which facilitates a better balancing and allows us to achieve better space bounds. Unfortunately, there are multiple challenges to realizing this intuition.

**Heavy-hitters.** First, we need a streaming algorithm for accurately reporting the frequencies of the \(L_{p}\)-heavy hitters at all times in the adaptive stream. However, such a subroutine is not known and naively, one might expect an estimate of the \(L_{p}\) norm might be necessary to identify the \(L_{p}\) heavy-hitters. Moreover, algorithms for finding \(L_{p}\) heavy-hitters are often used to estimate the \(L_{p}\) norm of the underlying frequency, e.g., . Instead, we use a turnstile streaming algorithm DetHH for \(L_{p}\) heavy-hitters  that uses sub-optimal space \(}(} n^{2-2/p})\) bits of space for \(p(1,2]\), rather than the optimal CountSketch, which uses \((}^{2}n)\) bits of space. However, the advantage of DetHH is that the algorithm is deterministic, so we can utilize the previous intuition from the dense-sparse framework of . In particular, if the universe size is small, then we can run DetHH, and if the universe size is large, then we collectively handle these cases using an ensemble of CountSketch algorithms via differential privacy. We provide the full details of the robust \(L_{p}\)-heavy hitter algorithm in Section2, ultimately achieving Theorem1.2.

**Residual estimation.** It remains to estimate the contribution of the elements that are not \(L_{p}\) heavy-hitters, i.e., the residual vector, toward the overall \(p\)-th moment. More generally, given a tail parameter \(k>0\) and an error parameter \((0,1)\), let \(g\) be a tail vector of \(f\) that omits the \(k\) entries of \(f\) largest in magnitude, breaking ties arbitrarily and let \(h\) be a tail vector of \(f\) that omits the \((1-)k\) entries of \(f\) largest in magnitude. We define the level sets of the \(p\)-th moment so that level set \(_{}\) roughly consists of the coordinates of \(g\) with magnitude \([(1+)^{},(1+)^{+1})\). We then estimate the contribution of each level set to the \(p\)-th moment of the residual vector using the subsampling framework introduced by .

Namely, we note that any "significant" level set has either a small number of items with large magnitude, or a large number of items that collectively have significant contribution to the \(p\)-th moment. In the former case, we can use CountSketch to identify the items with large magnitude, while in the latter case, it can be shown that after subsampling the universe, there will be a large number of items in the level set that remain. Moreover, these items will now be heavy with respect to the \(p\)-th moment of the resulting frequency vector after subsampling with high probability. Thus, these items can be identified by CountSketch on the subsampled universe. Furthermore, after rescaling inversely by the sampling probability, the total number of such items in the level set can be estimated accurately by rescaling the number of the heavy-hitters in the subsampled universe. Hence in both cases, we can estimate the number of items in the significant level sets and subtract off the largest \(k\) such items. We provide the full details of the residual estimation algorithm in Section3, culminating in Theorem3.4.

## 2 Adversarially Robust \(L_{p}\)-Heavy Hitters

In this section, we give an adversarially robust algorithm for \(L_{p}\)-heavy hitters on turnstile streams. We first recall the following deterministic algorithm for \(L_{p}\)-heavy hitters on turnstile streams.

**Theorem 2.1**.: _[_10_]_ _For \(p[1,2)\), there exists a deterministic algorithm 1 that solves the \(\)-\(L_{p}\) heavy-hitters on a universe of size \(t\) and a stream of length \(m\) and uses \(}t^{2-2/p}\) bits of space._

We also recall the following variant of CountSketch for answering a number of rounds of adaptive queries, as well as a more general framework for answering adaptive queries.

**Theorem 2.2**.: _[_12_]_ _For \(p[1,2)\), there exists a randomized algorithm 1 that uses \(}(}{^{2}} n )\) bits of space, and for \(\) different times \(t\) on an adaptive stream of length \(m\) on a universe of size \(n\), reports for all \(i[n]\) an estimate \(^{(t)}}\) such that \(^{(t)}}-f_{i}^{(t)}|\|f^{(t)} \|_{2}\), where \(f^{(t)}\) is the induced frequency vector at time \(t\)._

**Theorem 2.3**.: _[_12_, 12_]_ _Given a streaming algorithm \(\) that uses \(S\) space and answers a query with constant failure probability \(_{0}<\), there exists a data structure that answers \(Q\) adaptive queries, with probability \(1-\) using space \((S^{2})\)._

While 1 has better space guarantees than 1, determinism nevertheless serves an important purpose for us. Namely, adversarial input can induce failures on randomized algorithms but cannot induce failures on deterministic algorithms. On the other hand, the space usage of 1 grows with the size of the universe. Thus, we now use insight from the dense-sparse framework of . If the universe size is small, then we shall use 1. On the other hand, if the universe size is large, then shall use the following robust version of CountSketch, requiring roughly \(\) number of independent instances, where \(\) is the flip number. The key observation is that because the universe size is large, then the flip number will be much smaller than in the worst possible case. Moreover, we can determine which case we are in, i.e., the large universe case or the small universe case, by using the following \(L_{0}\) estimation algorithm:

**Theorem 2.4**.: _[_12_]_ _There exists an insertion-deletion streaming algorithm 1 that uses \((} n( + m))\) bits of space, and with probability at least \(1-\), outputs a \((1+)\)-approximation to \(L_{0}\)._We give our algorithm in full in Algorithm 1. Because DetHH is a deterministic algorithm, it will always be correct in the case where the universe size is small. Thus, we first prove that in the case where the universe size is large, then RobustCS ensures correctness within each sequence of \(\) updates.

**Lemma 2.5**.: _Suppose the number of distinct elements at the beginning of a block is at least \(50t\). Let \(S\) be the output of RobustCS at the beginning of a block. Then conditioned on the correctness of RobustCS, \(S\) solves the \(L_{p}\)-heavy hitter problem on the entire block._

Next, we show that RobustCS ensures correctness in between blocks as well. We also analyze the space complexity of our algorithm.

**Lemma 2.6**.: _With high probability, RobustCS is correct at the beginning of each block of length \(\)._

**Lemma 2.7**.: _The total space by the algorithm is \(}(}m^{(2p-2)/(4p-3)})\) bits of space._

Given our proof of correctness in Lemma 2.5 and Lemma 2.6, as well as the space analysis in Lemma 2.7, then we obtain Theorem 1.2.

```
0: Turnstile stream of length \(m\) for a frequency vector of length \(n\)
0: Adversarially robust heavy-hitters
1:\(t(m^{p/(4p-3)})\), \( t^{1/p}\), \(b\),\(\)
2: Initialize DetHH with threshold \(\)
3: Initialize RobustCS robust to \(b\) queries, with threshold \(\) for \(r=(})\) rounds
4: Initialize \(}()\) copies LZZeroEst with accuracy \(2\) robust to \(b\) queries
5:for each block of \(\) updates do
6: Update DetHH, RobustCS, and all copies of LZZeroEst
7:if\(=\) at the beginning of the block then
8: Return the output of DetHH
9:else
10: Return the output of RobustCS at the beginning of the block
11: Let \(Z\) be the output of robust LZZeroEst\(\)Theorem 2.3 and Theorem 2.4
12:if\(Z>100t\)then
13:\(\)
14:else
15:\(\) ```

**Algorithm 1** RobustHH: Adversarially robust \(L_{p}\)-heavy hitters

## 3 Oblivious Residual Estimation Algorithm

In this section, we consider norm and moment estimation of a residual vector, permitting bicriteria error by allowing some slack in the size of the tail. Specifically, suppose the input vector \(f\) arrives in the streaming model. Given a tail parameter \(k>0\) and an error parameter \((0,1)\), let \(g\) be a tail vector of \(f\) that omits the \(k\) entries of \(f\) largest in magnitude, breaking ties arbitrarily and let \(h\) be a tail vector of \(f\) that omits the \((1-)k\) entries of \(f\) largest in magnitude. We give an algorithm that estimates \(\|g\|_{p}^{p}\) up to additive \(\|h\|_{p}^{p}\), using space \((, n)\), which is independent of the tail parameter \(k\). It should be noted that our algorithm is imprecise on \(\|g\|_{p}^{p}\) in two ways. Firstly, it incurs additive error proportional to \(\). Secondly, the additive error has error with respect to \(h\), which is missing the top \((1-)k\) entries of \(f\) in magnitude, rather than the top \(k\). Nevertheless, the space bounds that are independent of \(k\) are sufficiently useful for our subsequent application of \(L_{p}\) estimation. We first define the level sets of the \(p\)-th moment and the contribution of each level set.

**Definition 3.1** (Level sets and contribution).: _Let \(>0\) be a parameter and let \(m\) be the length of the stream. Let \(M\) be the power of two such that \(m^{p} M<(1+)m^{p}\) and let \(\). Then for each integer \( 1\), we define the level set \(_{}:=\{i[n]\;\;f_{i}[},})\}\). We also define the contribution \(C_{}\) of level set \(_{}\) to be \(C_{}:=_{i_{}}(f_{i})^{p}\).__For a residual vector \(g\) of \(f\) with the top \(k\) coordinates set to be zero, we similarly define the level sets \(_{}\) and \(D_{}\) of \(g\) in the natural way, i.e., \(D_{}:=_{i_{}}(g_{i})^{p}\) for \(_{}:=\{i[n]\ |\ g_{i}[}, })\}\)._

```
0: Stream \(s_{1},,s_{m}\) of items from \([n]\), accuracy parameter \((0,1)\), \(p\)
0:\((1+)\)-approximation to \(F_{p}\)
1:\(\), \(L}()\), \(P=}((nm))\), \(R}()\), \( 2^{20}\)
2:for\(t=1\) to \(t=m\)do
3:for\((i,r)[P][R]\)do
4: Let \(U_{i}^{(r)}\) be a (nested) subset of \([n]\) subsampled at rate \(p_{i}:=(1,2^{1-i})\)
5:if\(s_{t} U_{i}^{(r)}\)then
6: Send \(s_{t}\) to \(_{i}^{(r)}\) with accuracy \(^{3}\)
7: Let \(M=2^{i}\) for some integer \(i 0\), such that \(m^{p} M<2m^{p}\)
8:\(c k\)
9:for\(=1\) to \(=L\)do
10:\(i(1,(1+)^{}- (nm)}{^{3}})\)
11: Let \(H_{i}^{(r)}\) be the outputs of \(\) at level \(i\)
12: Let \(S_{i}^{(r)}\) be the set of ordered pairs \((j,_{j})\) of \(H_{i}^{(r)}\) with \((_{j})^{p}[}, }]\)
13:\(|}}_{r [R]}|S_{i}^{(r)}|\), \(T_{}(0,}-c)\)
14:\(c(c-},0)\)
15:\(|} T_{}(1+)^{}\)
16: Return \((k)}}=_{[L]}|}|(1+ )^{}\) ----------------------------------------------------------------

**Algorithm 2** ResidualEst: residual \(F_{p}\) approximation algorithm, \(p\)

Our algorithm attempts to estimate the contribution of each level set. Some of these level sets contribute a "significant" amount to the \(p\)-th moment of \(f\), whereas other level sets do not. It can be seen that the number of items in each level set that is contributing can be estimated up to a \((1+())\)-approximation. In particular, either a contributing level set has a small number of items with large mass, or a large number of items that collectively have significant mass. We use the heavy-hitter algorithm \(\) to detect the level sets with a small number of items with large mass, and count the number of items in these level sets. For the large number of items that collectively have significant mass, it can be shown that after subsampling the universe, there will be a large number of these items remaining, and those items will be identified by \(\) on the subsampled universe. Moreover, the total number of such items in the level set can be estimated accurately by rescaling the number of the heavy-hitters in the subsampled universe inversely by the sampling probability. We can thus carefully count the number of items in the contributing level sets and subtract off the largest \(k\) such items. Because we only have \((1+)\)-approximations to the number of such items, it may be possible that we subtract off too many, hence the bicriteria approximation.

Finally, we note that for the insignificant level sets, we can no longer estimate the number of items in these level set up to \((1+)\)-factor. However, we note that the number of such items is only an \(\) fraction of the number of items in the lower level sets that are contributing. Therefore, we can show that it suffices to set the contribution of these level sets to zero. Our algorithm appears in full in Algorithm 2.

We now show that the number of items (as well as their contribution) in each "contributing" level set with a small number of items with large mass will be estimated within a \((1+)\)-approximation.

**Lemma 3.2**.: _Let \(r[R]\) be fixed. Then with probability at least \(\), we have that simultaneously for all \(j U_{i}^{(r)}\) for which \((f_{j})^{p}.F_{p}(U_{i}^{(r)})}{2^{2}^{2}(nm)}\). \(H_{}^{(r)}\) outputs \(_{j}\) with \((1-)(f_{j})^{p}(_{j})^ {p}(1+)(f_{j})^{p}\)._We now show that the number of items in each "contributing" level set is estimated within a \((1+)\)-approximation, including the level sets that contain a large number of small items.

**Lemma 3.3**.: _Given a fixed \((0,1)\), let \(_{}\) be a fixed level set and let \(r[R]\) be fixed. Let \(i=(1,(1+)^{}-(nm)}{^ {3}})\). Define the events \(_{1}\) to be the event that \(|U_{i}^{(r)}|}\) and \(_{2}\) to be the event that \(F_{p}(U_{i}^{(r)})}{2^{i}}\). Then conditioned on \(_{1}\) and \(_{2}\), for each \(j_{} U_{i}^{(r)}\), there exists \((j,_{j})\) in \(S_{i}^{(r)}\) such that with probability at least \(\), \((1-)(f_{j})^{p}(_{j})^ {p}(1+)(f_{j})^{p}\)._

Putting things together, we have the following full guarantees for our algorithm.

**Theorem 3.4**.: _There exists a one-pass streaming algorithm ResidualEst that takes an input parameter \(k 0\) (possibly upon post-processing the stream) and uses \(}(}^{3}(nm))\) bits of space to output an estimate \((k)}}\) with \([|(k)}}-F_{p,(k)} | F_{p,((1-)k)}] \)._

## 4 Adversarially Robust \(L_{p}\) Estimation

In this section, we give an adversarially robust algorithm for \(F_{p}\) moment estimation on turnstile streams. Due to the relationship between the \(F_{p}\) moment and the \(L_{p}\) norm, our result similarly translates to a robust algorithm for \(L_{p}\) norm estimation. We first require an algorithm to recover all the coordinates of the underlying frequency vector if it is sparse.

**Theorem 4.1**.: _[_1_]_ _There exists a deterministic algorithm SparseRecover that recovers a \(k\)-sparse frequency vector defined by an insertion-deletion stream of length \(n\). The algorithm uses \(k(n)\) bits of space._

```
0: Turnstile stream of length \(m\) for a frequency vector of dimension \(n\)
0: Adversarially robust heavy-hitters
1:\(c-23p+4}{(4p-3)(12p+3)}\), \(-\), \(}{100m^{}}\), \(k(})\), \(( m^{c/p}k^{1-1/p})\), \(\)
2: Initialize SparseRecover with sparsity \((m^{c})\)
3: Initialize RobustWith with threshold \(\)
4: Initialize LZeroEst with accuracy \(2\) robust to \(b:=\) queries
5: Initialize ResidualEst with parameter \(k\) and accuracy \(()\) robust to \(b\) queries
6:for each block of \(\) updates do
7: Update RobustHH, LZeroEst, and ResidualEst
8:if\(=\) at the beginning of the block then
9: Let \(g\) be the vector output by SparseRecover
10:\(\|g\|_{p}^{p}\)
11: Return \(\)
12:else
13: Let \(g\) be the vector output by RobustHH at the beginning of the block
14: Let \(\) be the output of ResidualEst
15:\(\|g\|_{p}^{p}\)
16: Return \(+\)
17: Let \(Z\) be the output of robust LZeroEst
18:if\(Z>100t\)then
19:\(\)
20:else
21:\(\) ```

**Algorithm 3** Adversarially robust \(L_{p}\)-estimation

We remark that SparseRecover is deterministic and guarantees correctness on a turnstile stream, even if the frequency vector is not sparse at some intermediate step of the stream. On the other hand,if the frequency vector is not sparse, then a query to SparseRecover could be erroneous. Hence, our algorithm thus utilizes robust LZeroEst to detect whether the underlying frequency vector is dense or sparse. Similar to , the intuition is that due to the sparse case always succeeding, the adversary can only induce failure if the vector is dense, which in turn decreases the flip number. However, because we also accurately track the heavy-hitters, then the adversary must spread the updates across a multiple number of coordinates, resulting in a larger number of updates necessary to double the residual vector. Since the number of updates is larger, then the flip number is smaller, and so our algorithm can use less space. Unfortunately, even though the residual vector may not double in its \(p\)-th moment, the \(p\)-th moment of entire frequency vector \(f\) may change drastically. This is a nuance for the analysis because our error guarantee can no longer be relative to the \(\|f\|_{p}^{p}\). Indeed, \(\|f\|_{p}^{p}\) additive error may induce \((1+)\)-multiplicative error at one point, but at some later point we could have \(\|f^{}\|_{p}^{p}\|f\|_{p}^{p}\), so that the same additive error could even be polynomial multiplicative error. Hence, we require the ResidualEst subroutine from Section3, whose guarantees are in terms of the residual vector. We give our algorithm in full in Algorithm3.

We upper bound the amount that the \(p\)-th moment of the residual vector can change, given a bounded number of updates.

**Lemma 4.2**.: _Let \(f\) be a frequency vector and \(g\) be the residual vector omitting the \(k\) coordinates of \(f\) largest in magnitude. Let \(v\) be any arbitrary vector such that \(\|v\|_{1}\|g\|_{p} k^{1-1/p}\) and \(\|v\|_{1}\|g\|_{1}\). Let \(u\) be the residual vector omitting the \(k\) coordinates of \(f+v\) largest in magnitude. Then we have \(\|g\|_{p}^{p}-\|u\|_{p}^{p}|\|g\|_{p}^{p}\)._

We now show correctness and space complexity of Algorithm3, after which Theorem1.3 follows.

**Lemma 4.3**.: _For \( n=( m)\), Algorithm3 uses \(}(} m^{c})\) bits of space in total. Moreover, for any fixed time during a stream, let \(f\) be the induced frequency vector and let \(\) be the output of Algorithm3. Then we have that with high probability, \((1-)\|f\|_{p}^{p}(1+)\|f\|_{p}^{p}\)._

## 5 Empirical Evaluations

In this section, we describe our empirical evaluations for comparing the flip number of the entire vector and the flip number of the residual vector on real-world datasets. Note that these quantities parameterize the space used by the algorithm of  and by our algorithm, respectively.

**CAIDA traffic monitoring dataset.** We used the CAIDA dataset  of anonymized passive traffic traces from the 'equinix-nyc' data center's high-speed monitor. The dataset is commonly used for empirical evaluations on frequency moments and heavy-hitters. We extracted the sender IP addresses from 12 minutes of the internet flow data, which contained 2,9922,873 total events.

**Experimental setup.** Our empirical evaluations were performed Python 3.10 on a 64-bit operating system on an AMD Ryzen 7 5700U CPU, with 8GB RAM and 8 cores with base clock 1.80 GHz. We compare the flip number of the entire data stream versus the flip number of the residual vector across various values of the algorithm error \(\{10^{-1},10^{-2},,10^{-5}\}\), values of the heavy-hitter threshold \(\{4^{-1},4^{-2},,4^{-10}\}\), and the frequency moment parameter \(p\{1.1,1.2,,1.9\}\). We describe the results in Figure1.

Figure 1: Empirical evaluations on the CAIDA dataset, comparing flip number of the \(p\)-th frequency moment and the residual, for \(==0.001\) and \(p=1.5\) when not variable. Smaller flip numbers indicate less space needed by the algorithm.

**Results and discussion.** Our empirical evaluations serve as a simple proof-of-concept demonstrating that adversarially robust algorithm can use significantly less space than existing algorithms. In particular, existing algorithms use space that is an increasing function of the flip number of the \(p\)-th frequency moment, while our algorithms use space that is an increasing function of the flip number of the residual, which is significantly less across all settings in Figure 1. While the ratio does increase as the exponent \(p\) increases in Figure 1, there is not a substantial increase, i.e., \(1.24\) to \(1.31\) from \(p=1.1\) to \(p=1.9\). On the other hand, as \(\) decreases in Figure 1, the ratio increases from \(1.002\) for \(=4^{-1}\) to \(1.6\) for \(=4^{-10}\). Similarly, in Figure 1, the ratio of these quantities begins at \(1.17\) for \(=10^{-1}\) and increases to as large as \(1.75\) for \(=10^{-5}\). Therefore, even in the case where the input is not adaptive, our empirical evaluations demonstrate that these flip number quantities can be quite different, and consequently, our algorithm can use significantly less space than previous existing algorithms.