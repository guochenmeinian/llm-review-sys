# Causal Imitation for Markov Decision Processes:

a Partial Identification Approach

 Kangrui Ruan

Columbia University

kr2910@columbia.edu

&Junzhe Zhang

Syracuse University

jzhan403@syr.edu

&Xuan Di

Columbia University

sharon.di@columbia.edu

Equal contribution; Kangrui: work performed while at Columbia University.

&Elias Bareinboim

Columbia University

eb@cs.columbia.edu

###### Abstract

Imitation learning enables an agent to learn from expert demonstrations when the performance measure is unknown and the reward signal is not specified. Standard imitation methods do not generally apply when the learner and the expert's sensory capabilities mismatch and demonstrations are contaminated with unobserved confounding bias. To address these challenges, recent advancements in causal imitation learning have been pursued. However, these methods often require access to underlying causal structures that might not always be available, posing practical challenges. In this paper, we investigate robust imitation learning within the framework of canonical Markov Decision Processes (MDPs) using partial identification, allowing the agent to achieve expert performance even when the system dynamics are not uniquely determined from the confounded expert demonstrations. Specifically, first, we theoretically demonstrate that when unobserved confounders (UCs) exist in an MDP, the learner is generally unable to imitate expert performance. We then explore imitation learning in partially identifiable settings -- either transition distribution or reward function is non-identifiable from the available data and knowledge. Augmenting the celebrated GAIL method (Ho & Ermon, 2016), our analysis leads to two novel causal imitation algorithms that can obtain effective policies guaranteed to achieve expert performance.

## 1 Introduction

Children often learn how to behave in an unfamiliar environment by imitating adults. Imitation learning (IL) enables a learning agent to behave in an unknown environment by observing expert demonstrations. It provides a viable approach for policy learning from demonstrations when the reward function is not fully known and reward signals are not specified . Imitation learning has been widely applied across disciplines, such as autonomous driving , robotics , natural language processing , and chronic disease management .

It has been acknowledged in the literature that imitation learning could face significant challenges when _unobserved confounding bias_ in expert demonstrations cannot be ruled out _a priori_. For illustration with simplicity, consider a Multi-Armed Bandit (MAB) model  described in Fig. 1; \(X\{0,1\}\) is a binary action, and

Figure 1: A multi-armed bandit model.

\(Y\) is the reward; \(U\) is a latent covariate (to the imitator) uniformly drawn over a binary domain \(\{0,1\}\). Values of the reward are decided by a reward function \(Y X U\) where \(\) is a "\(xor\)" operator. An expert demonstrator, having access to covariate \(U\), selecting action based on an expert policy \(X U\). Evaluating the expert's performance gives \([Y]=[ U U]=1\). On the other hand, an imitator, mimicking the expert's behavior, will follow a policy \((X)=P(X)=0.5\), selecting action uniformly at random. Evaluating the imitator's performance gives \(_{}[Y]=_{x}(x)[x U]=0.5\), far from the expert's performance \([Y]=1\).

Causal Inference (CI) addresses the challenges of unobserved confounding bias within the observational data . It leverages causal knowledge integral to the data generation process, typically represented as a causal diagram or potential outcomes . More recently, incorporating causal inference methods into the imitation learning paradigm, _causal imitation learning_ has evolved into a critical area of research . To compensate for the presence of unobserved confounding bias, these methods rely on additional structural or parametric knowledge about causal relationships among variables in the environment. By utilizing such domain knowledge, the imitator is able to recover the underlying system dynamics (i.e., causal effect) from confounded expert demonstrations and, in turn, obtain an imitating policy that can achieve the expert's performance.

By and large, the combination of causal knowledge and observational data does not always allow one to point-identify the causal effect, called the _non-identifiable_. That is, more than one parametrization of the target effect is compatible with the same observational data and model assumptions [32, Def. 3.2.2]. For instance, in the MAB environment described previously, the imitator's performance \(_{}[Y]\) is not identifiable from the confounded observational distribution \(P(X,Y)\)[32, Thm. 3.4.1]. _Partial identification_ methods concerned with inferring about target causal effects in non-identifiable settings, and has been a target of growing interest in the domains of causal inference , econometrics , and more recently, in machine learning . Among these works, two approaches are often employed: (1) bounds are derived for the target effect under minimal assumptions, or (2) additional untestable assumptions are invoked under which the causal effect is identifiable, and then sensitivity analysis is conducted to assess how the target causal effect varies as the untestable assumptions are changed. Despite their effectiveness in addressing data bias, partial identification has still been rarely explored in the context of imitation learning.

This paper studies the partial identification for imitation learning in a generalized sequential decision-making environment of Markov Decision Processes (MDPs, ). The imitator must determine a sequence of actions, while unobserved confounders cannot be ruled out a priori in expert demonstrations. We discuss the solutions case-by-case, depending on the identifiability of the underlying system dynamics from the confounded data, including the reward function \(\) and the transition distribution \(\). Specifically, our contributions can be summarized as follows. (1) We theoretically prove that when unobserved confounders generally exist, it is infeasible to learn a robust policy that is guaranteed to achieve expert performance from the demonstration data. (2) When only the transition distribution \(\) is identifiable, we propose a novel imitation algorithm that leverages the bounds over the non-identifiable reward \(\); by matching the weighted occupancy measure, the imitator is able to obtain a policy that can outperform the expert. (3) We propose an alternative algorithm when the reward \(\) is identifiable, but there is unobserved confounding affecting the transition \(\). Our proposed algorithms could be implemented by augmenting the celebrated generative adversarial imitation learning framework (GAIL, ). Table 1 briefly summarizes this paper's main contributions. Due to space constraints, all proofs are provided in Appendices A and B.

### Preliminaries

This section introduces the basic notations and definitions used throughout the paper. We use capital letters to denote random variables (\(X\)), small letters for their values (\(x\)), and \(_{X}\) for the domain

    & **Identifiable Reward** & **Non-Identifiable Reward** \\ 
**Identifiable Transition** & Standard IRL (e.g., GAIL) & CAIL-\(\) (Alg. 1 in Sec. 3.1) \\ 
**Non-Identifiable Transition** & CAIL-\(\) (Alg. 2 in Sec. 3.2) & Inimitable (Thm. 1 in Sec. 2.1) \\   

Table 1: Summary of main contributions in this paper, including the analysis and proposed algorithms.

[MISSING_PAGE_FAIL:3]

For analytical clarity, we define reward function \((s,x)\) as the expected value \(_{y}y(s,x,y)\). Fix the discounted factor \(\). A common objective for an agent to optimize is the cumulative return \(R_{t}=Y_{t}+ Y_{t+1}+^{2}Y_{t+2}+=_{k=0}^{}^{ k}Y_{t+k}\).

Imitation Learning.When a detailed parametrization of the transition distribution \(\) and the reward function \(\) is available, the agent can obtain an optimal policy using standard planning algorithms . However, in many practical applications, complete knowledge of these parametrizations is often unavailable, necessitating a learning process. In this paper, we consider the _imitation learning_ setting, where the agent has access to observed trajectories generated by the expert. More specifically, at each time step \(t\), the expert selects an action \(X_{t} f_{X}(s_{t},_{t})\) based on the current state \(S_{t}=s_{t}\) and latent noise \(_{t}=_{t}\). Fig. 1(b) shows the graphical representation of the data-generating process of the expert; the highlighted bi-directed arrows, e.g., \(X_{t} Y_{t}\), indicate the presence of an unobserved confounder \(U_{t}\) affecting both the action \(X_{t}\) and outcome \(Y_{t}\). We summarize the expert trajectories using the observational distribution \(P(,,)\) over sequences of variables \(=\{X_{1},X_{2},\}\), \(=\{S_{1},S_{2},\}\), and \(=\{Y_{1},Y_{2},\}\). It is verifiable from Fig. 1(b) that Markov property holds with regard to distribution \(P(,,)\). For any horizon \(T\),

\[P(}_{1:T},}_{1:T},}_{1:T})=P(s_{1} )_{t=1}^{T}P(x_{t} s_{t})}(s_{t},x_{t},s_{t+1} )}(s_{t},x_{t},y_{t})\] (4)

where \(}\) and \(}\) are the expert's nominal transition distribution and reward function computed from the _observational_ distribution as follows:

\[}(s,x,s^{})=P(S_{t+1}=s^{}  S_{t}=s,X_{t}=x),}(s,x)= [Y_{t} S_{t}=s,X_{t}=x]\] (5)

By convention in imitation learning, we assume the rewards \(Y_{t}\) are generally unobserved to the learner; instead, it has access to a parametric family \(\) containing the expert's nominal reward function \([Y_{t} s_{t},x_{t}]\). Given the expert demonstrations \(\) sampled from \(P(X_{1},X_{2},,S_{1},S_{2},)\) and the parametric reward family \(\), the imitator attempts to learn policy \(\) that can achieve expert performance, i.e., \(_{}[_{t=1}^{}^{t-1}Y_{t}][_{t=1}^{}^{t-1}Y_{t}]\). Standard imitation methods focus on the identifiable setting where the imitator's transition distribution \(\) and reward function \(\) is consistent with the expert's nominal transition \(}\) and reward \(}\). Formally,

**Definition 2** (Causal Consistency).: For an interventional distribution \(P_{}\) and an observational distribution \(P\) satisfying the Markov property (Def. 1), Causal Consistency is said to hold with respect to \(P_{}\) and \(P\) if the following statement is true, for every time step \(t=1,2,\),

\[P_{x_{t}}(s_{t+1} s_{t})=P(s_{t+1} s_{t},x_{t}),  P_{x_{t}}(y_{t} s_{t})=P(y_{t} s _{t},x_{t})\] (6)

When the invariances of Def. 2 hold, the learner could recover the parametrization of the transition distribution \(\) from observational data \(P(,)\) and infer about the reward function \(\) from the parametric family \(\). An imitating policy \(\) is obtainable by solving the following minimax program,

\[^{*}=_{}_{}_{s,x}(s,x)(P(x s)(s)-(x s)_{}(s))\] (7)

where the imitator's \(_{}\) and the expert's \(\) occupancy measures are defined as, respectively, \(_{}(s)=_{t=0}^{}^{t}P_{}(S_{t}=s)\) and \((s)=_{t=0}^{}^{t}P(S_{t}=s)\). The solution \(\) is guaranteed to achieve expert

Figure 2: Causal diagrams where \(S_{t}\) represents the state, \(X_{t}\) represents the action (shaded blue) and \(Y_{t}\) represents the latent reward (shaded red). (a) MDP\({}_{}\) describes the imitator’s interaction with the environment; (b) MDP\({}_{}\) shows the data-generating process for the expert demonstrations.

performance when the gap \(^{*} 0\). This means that the imitator following policy \(\) performs as well as the expert, even in the worst-case environment instance compatible with the demonstration data and model assumption. Several imitation learning algorithms have been proposed to solve the optimization problem in Eq. (7), including [1; 50; 19].

Graphical criteria exist [33; 46; 34] to examine whether causal consistency (Def. 2) holds from causal knowledge of the environment, including the celebrated _backdoor_ condition [32; Def. 3.3.1],. In MDPs, this means that the causal links between the latent noise \(_{t}\) and action \(X_{t}\) are not effective - the graphical representation of the imitator's (Fig. 2a) and the expert's (Fig. 2b) data-generating process coincide. However, in practice, causal consistency could be fragile and does not necessarily hold due to the presence of unobserved confounders in the demonstration data [57; 39]. The remainder of this paper studies imitation learning when violations occur in the invariance relationships of Eq. (6).

### Imitation with Non-Identifiable Transition and Reward

We first consider the imitation setting described in Fig. 2b where unobserved confounders generally exist in the expert demonstrations; both the transition distribution \(\) and reward function \(\) are not identifiable from Eq. (6). Here, we will show that expert performance is not imitable by constructing worst-case MDP instances where the expert always outperforms the imitator.

The state value function \(V_{}(s)\) is defined as the expected return given the imitator's starting state \(S_{t}=s\) following a policy \(\), i.e., \(V_{}(s)=_{}[R_{t} S_{t}=s]\). For any policy \(\), the imitator's performance can be written as \(_{}[R_{1}]=_{s_{1}}P(s_{1})V_{}(s_{1})\). The value function of any state \(s\) can thus be recursively defined using the celebrated _Bellman Equation_:

\[V_{}(s)=_{x}(x s)((s,x)+_{s^{} }(s,x,s^{})V_{}(s^{}))\] (8)

where \(\) denotes the discount factor. While the transition distribution \(\) and the reward function \(\) are not uniquely discernible from the observational distribution due to the unobserved confounding, it is still possible to learn about them from demonstrations using partial identification. Without loss of generality, the reward \(Y_{t}\) is normalized in a real interval \(\). Through rigorous adaptation of the bounding strategies established in [29; 55], we successfully derive the bounds for the transition distribution \(\) and reward function \(\), for every realization \((s,x,s^{})\),

\[(s,x,s^{})[}(s,x,s^{})P(x s),\;}(s,x,s^{ })P(x s)+P( x s)]\] (9) \[(s,x)[}( s,x)P(x s),\;}(s,x)P(x s)+P( x  s)]\] (10)

Among the above quantities, \(}\) and \(}\) are the expert's nominal transition distribution and reward function in Eq. (5); \(P(x s)\) stands for the propensity score \(P(X_{t}=x S_{t}=s)\) and \(P( x s)=1-P(x s)\). We can then construct a worst-case MDP for any policy \(\) at state \(s\) by solving the following optimization program: minimize the Bellman's equation in Eq. (8) as the objective function, subject to the observational constraints in Eqs. (9) and (10). Solving this program enables a valid MDP construction since the transition distribution \(\) and the reward function \(\) are independent components induced by the underlying model and can be optimized separately.

**Theorem 1**.: _Given any positive observational distribution \(P(,,)>0\), there exists an MDP model \(\) compatible with the causal graph of Fig. 2b such that \(P(,,;)=P(,,)\) and for any policy \(\), any time step \(t=1,2,\), any state \(s\),_

\[V_{}(s;)<[R_{t} S_{t}=s;].\] (11)

In other words, there always exists a candidate MDP instance \(\) compatible with the demonstration data such that an imitator is always unable to achieve expert performance (r.h.s. in Eq. (11)), regardless of the deployed policy \(\). It follows from Thm. 1 that there is no policy \(\) learnable from confounded demonstrations that is guaranteed to perform at least as the expert in all possible scenarios. This means that expert performance is not imitable when unobserved confounding generally exists. The following example demonstrates the challenges of unobserved confounding in a single-stage MDP.

**Example 1** (Single-Stage MDP).: Consider a 1-stage MDP model with horizon \(T=1\). For any policy \((X_{1} S_{1})\), the imitator's expected return is \(_{}[Y_{1}]=_{s_{1},x_{1}}(s_{1},x_{1}) (x_{1} s_{1})P(s_{1})\). It followsfrom the tight lower bound in Eq. (10) that there exists an worst-case MDP model \(\) compatible with the observational distribution \(P(X_{1},S_{1},Y_{1})\) such that \((s_{1},x_{1})=[Y_{1} s_{1},x_{1}]P(x_{1}  s_{1})\). In this MDP instance \(\), the imitator's expected return can be further written as

\[_{}[Y_{1}]=_{s_{1},x_{1}}[Y_{1} s_{1 },x_{1}]P(x_{1}|s_{1})(x_{1}|s_{1})P(s_{1})<_{s_{1},x_{1}}[Y_ {1} s_{1},x_{1}]P(x_{1}|s_{1})P(s_{1})\] (12)

The last step holds since probabilities of the policy \((x_{1} s_{1})\) and \(_{x_{1}}(x_{1} s_{1})=1\). Marginalizing the above equation gives \(_{}[Y_{1}]<[Y_{1}]\) - the imitator is unable to achieve expert performance regardless of the deployed policy \(\). This analysis applies analogously to the MAB model in Fig. 1, which can be thought of as a 1-stage MDP with no initial state \(S_{1}=\). We refer the readers to Appendix F for more examples about the 2-stage MDP.

## 3 Partial Identification for Robust Imitation

The impossibility results in Thm. 1 imply that robust imitation cannot be guaranteed when unobserved confounders generally exist in the demonstration data. This means we must explore alternative assumptions to learn an imitating policy guaranteed to achieve expert performance. Meanwhile, standard imitation methods apply when causal consistency of Def. 2 holds, and no unobserved confounder affects the transition or reward function. A natural question at this point arises: whether robust imitation is feasible for settings between the unconfounded (Fig. 0(b)) and fully confounded cases (Fig. 0(a)), where unobserved confounding bias affects only either the transition distribution or reward function? This section aims to answer this question.

### Imitation with Identifiable Transition and Non-Identifiable Reward

We first examine the setting graphically described in Fig. 2(a) where the reward function is confounded, while the transition distribution is identifiable from the demonstration data. In this case, the first equation of Def. 2 holds while the second one fails. To initiate the discussion, we write the expected return of a candidate policy \(\) in an MDP environment as follows ,

\[_{}[R_{1}]=_{s,x}(s,x)(x  s)_{}(s)\] (13)

Among quantities in the above equation, the state occupancy measure \(_{}(s)=_{t=0}^{}^{t}P_{}\) (\(S_{t}=s\)) is a function of the initial state distribution \(P(s)=P(S_{1}=s)\) and the transition distribution \(\). Specifically, \(_{}(s)\) can be recursively written as \(_{}(s)=P(s)+_{s^{},x}(s^{ },x,s)(x s^{})_{}(s^{})\). When the transition distribution is unconfounded (Fig. 2(a)), one could recover its parametrization \((s,x,s^{})\) following the first formula of Def. 1. Therefore, what remains undetermined in Eq. (13) is the non-identifiable reward function \(\). It follows from Eq. (10) that parametrization of \((s,x)\) can be bounded from the observational distribution. The imitator's expected return could thus be lower bounded as \(_{}[R_{1}]_{s,x}}( s,x)P(x s)(x s)_{}(s)\), where \(}\) is the nominal reward function defined in Eq. (5). Similarly, the expert's expected return could be decomposed as \([R_{1}]=_{x,s}}(s,x)P(x s )(s)\), where \((s)=_{t=0}^{}^{t}P(S_{t}=s)\) is the expert's occupancy measure. Optimizing the worst-case gap between the imitator \(_{}[R_{1}]\) and expert \([R_{1}]\) leads to a minimax optimization problem, the solution of which leads to a possible imitating policy.

Figure 3: (a) MDP\({}_{}\) shows a data-generating process for expert demonstrations where only the reward \(Y_{t}\) is confounded with the action \(X_{t}\); (b) MDP\({}_{}\) shows a data-generating process for expert demonstrations where only the next state \(S_{t+1}\) is confounded with the action \(X_{t}\).

**Theorem 2**.: _Given an MDP \(M\) compatible with the causal graph of Fig. 2(a), let \(\) be a parametric family containing the conditional reward \([Y_{t} s_{t},x_{t}]\). Consider the following optimization program,_

\[^{*}=_{}_{}_{s,x}}(s,x)P(x s)((s)-(x s)_{}(s))\] (14)

_When the gap \(^{*} 0\), the solution \(^{*}\) is an imitating policy satisfying \(_{^{*}}[R_{1}][R_{1}]\)._

In other words, Thm. 2 computes an imitating policy within the environment depicted in Fig. 2(a) by finding a policy maximizing the worst-case reward function compatible with the demonstration data and the expert's nominal reward. Later in Sec. 4, we will demonstrate that such a solution exists and robust imitation learning is feasible in Fig. 2(a).

The optimization program in Thm. 2 could be solved by augmenting some standard imitation learning such as GAIL . To make the argument more precise, let the parametric family \(\) be a set of reward function \((s,x)\) taking values in the real space \(\). We penalize the complexity of a reward function \(\) by subtracting a convex regularization function \(()\) from Eq. (14); the detailed definition of \(()\) is given by [19, Eq. 13]. Solving the optimization program of Eq. (14) is equivalent to matching weighted occupancy measures between the imitator and the expert, shown in Appendix B,

\[^{*} =_{}^{*}(P(x s)(s)-P(x s)(x s )_{}(s))\] (17) \[=_{}_{D(0,1)^{g X}}[(D(S,X))]+ _{}[P(x s)(1-D(S,X))],\] (18)

where \(^{*}=_{}a^{}-()\) is a conjugate function of \(\); function \(D(0,1)\) is a discriminator classifier (e.g, a neural network). The above optimization problem is in the form of two neural networks competing against each other in a zero-sum game. The detailed implementation of our proposed algorithm, called CAIL-\(\), is provided in Alg. 1. Compared to the standard GAIL algorithm, Alg. 1 adds weight to the signal generated by the discriminator for the imitator and then attempts to match the distribution between the weighted samples and expert demonstrations.

### Imitation with Non-Identifiable Transition and Identifiable Reward

In this section, we examine the MDP\({}_{}\) environment as graphically depicted in Fig. 2(b), where the reward function is unconfounded, but UCs affect the action \(X_{t}\) and the next state \(S_{t+1}\) simultaneously. In this setting, the second equation of Causal Consistency (Def. 2) is satisfied, aligning the reward function \(\) with the expert's nominal reward function. However, the first equation of Def. 2 does not generally hold due to confounding bias, making the transition distribution \(\) not identifiable from demonstrations. Despite these challenges, we utilize partial identification techniques to bound the transition function \(\), and subsequently estimate the imitator's performance.

More precisely, consider again the expected return decomposition in Eq. (13). The identifiable reward function \(\) must be contained in the parametric space of the expert's nominal reward \(\). The transition distribution \(\) can be bounded from the demonstration data using Eq. (9). One could thus obtain a lower bound over the imitator's performance by reasoning about the worst-case occupancy measure compatible with demonstrations. Formally, with the fixed reward function \(\) and the fixed policy \(\), the imitator's return \(_{}[R_{1}]\) is bounded by:

\[_{}[R_{1}]_{,_{}} _{s,x}(s,x)(x s)_{}(s)\] (19)

\[\ \ _{}(s) 0,_{s}_{}(s)=,\ _{}(s)=P(s)+_{s^{},x} (s^{},x,s)(x s^{})_{}(s^{})\]

\[:_{s} (s^{},x,s)=1,\ (s,x,s^{})}(s,x,s^ {})P(x s)\\ (s,x,s^{})}(s,x,s^ {})P(x s)+P( x s)\] (20)

The above optimization problem is similar to the classical linear program for planning in MDPs . The main difference is that the transition distribution \(\) is no longer fixed but bounded in a convex space \(\) specified from the observational data. Therefore, we develop an imitating policy by minimizing the performance gap between the imitator and the expert in the worst-case environment compatible with the observational data and prior knowledge.

**Theorem 3**.: _Given an MDP \(M\) compatible with the causal graph of Fig. 2(b), let \(\) be a parametric family containing the conditional reward \([Y_{t} s_{t},x_{t}]\), and \(\) be a parametric family over conditional probabilities \(P(s_{t+1} s_{t},x_{t})\) defined in Eq. (20). Consider the following program,_

\[^{*}=_{}\ _{}\ _{ }\ _{s,x}(s,x)(P(x s)(s)-(x s)_{}(s ;))\] (21)

_When the gap \(^{*} 0\), the solution \(^{*}\) is an imitating policy satisfying \(_{^{*}}[R_{1}][R_{1}]\)._

We solve the optimization program in Thm. 3 by augmenting GAIL, a standard imitation method . By penalizing the complexity of a reward function \(\) using a convex regularization function \(()\) from Eq. (14), Eq. (21) is reducible to the following distribution matching problem,

\[^{*}=_{}\ _{D(0,1)^{}}\ _{ }\ [(D(S,X))]+_{}[(1-D(S,X)); ],\] (22)

We present the step-by-step implementation of our imitation method, CAIL-\(\), in Alg. 2. It is similar to the standard GAIL ; however, a significant distinction arises at step 4, where the imitator collects trajectories from the worst-case occupancy measure as presented in Eq. (19) and Eq. (20), which is obtainable by iteratively solving a series of linear programs. We refer readers to Appendix C for a more detailed discussion, where we propose an iterative algorithm designed to find the worst-case occupancy measure efficiently.

## 4 Experiments

In this section, we validate the theoretical findings presented in Thm. 1 and illustrate the applications of the proposed CAIL algorithms (Alg. 1 and Alg. 2) on various causal imitation learning tasks. Such tasks range from synthetic causal models to real-world scenarios. To summarize, when both the transition and the reward are confounded, there always exists a worst-case MDP instance \(\) compatible with the expert demonstrations, but the imitator consistently fails to match expertperformance, aligning with the proof provided in Sec. 2.1. When either the transition or the reward is confounded, we systematically evaluate our algorithms against the standard BC and GAIL methods, highlighting the importance of optimizing within the worst-case SCM. Standard BC mimics the expert's nominal behavior policy \(P(X|S)\) via supervised learning; standard GAIL learns a policy by solving a min-max game . We provide in Appendix D more details on the experiment setup.

MDPobs - Random Instances.This experiment aims to empirically validate the theoretical findings discussed in Thm. 1. Consider SCM instances compatible with Fig. 1(b) including binary observed variables \(S_{t},X_{t},Y_{t}\{0,1\}\). \(1000\) random discrete MDPs are sampled, in other words, the reward functions and the transition probabilities are generally different among these models. The expert is able to observe the state \(S_{t}\), the unobserved variable \(U_{t}\). However, the imitator, lacking access to both \(U_{t}\) or the reward \(_{}[Y_{t}]\), makes decisions solely on \(S_{t}\). As shown in Fig. 3(a), imitators consistently failed to match expert performance. Specifically, prevalent negative performance gaps indicate that most of imitators were consistently worse than experts; only in rare cases did the performance gaps near \(-0.5\), supporting our theoretical insights in Thm. 1. In summary, imitators fail to achieve the expert's performance when both the reward and the transition are confounded.

MDPobs.Y - Driving.To demonstrate the proposed framework, as outlined in Alg. 1, we consider a scenario when an autonomous vehicle ('ego vehicle') aims to learn optimal driving strategies from expert demonstrations. The state \(S_{t}\) contains some critical driving information, e.g., the velocities of the ego vehicle and the leading vehicle and the spatial distance between them. The action \(X_{t}\) represents acceleration or deceleration decisions the ego vehicle makes. The unobserved variable \(U_{t}\) represents some information accessible to the expert but inaccessible to the imitator, e.g. slippery road conditions . The reward \(Y_{t}\) is designed to reflect multiple realistic driving objectives, e.g., safety, comfort, efficiency, and so on. \(U_{t}\) has an effect on the reward \(Y_{t}\). Unlike the scenarios described in , due to UCs between \(X_{t}\) and \(Y_{t}\) at each step \(t\), it is impossible to find a \(\)-backdoor admissible set. BC, GAIL, and CAIL utilize the same policy space \((x s)\). The major difference between CAIL and GAIL lies in that CAIL optimizes the imitator by the weighted reward generated from the discriminator - \(P(x s)(1-D(s,x))\). As illustrated in Fig. 3(b), where means and standard deviations are computed over \(100\) trajectories, CAIL consistently outperforms BC and GAIL.

MDPobs.S - Medical Treatment.Consider the challenge of providing medical treatment to acutely ill patients, where the primary goal is to learn a policy so that the morality rate can be decreased. We utilize the real-world medical treatment dataset, i.e., Medical Information Mart for Intensive Care III (MIMIC-III) dataset . MIMIC-III consists trajectories of clinical information (e.g., heart rate, oxygen saturation, and so on) recorded at various time intervals. However, due to privacy concerns, certain essential variables are masked or not properly recorded , e.g., socioeconomic status or the experience levels of caregivers . Specifically, the state \(S_{t}\) encapsulates the critical health information for the patients, e.g., prolonged elevated heart rate (peHR). The action \(X_{t}\) represents whether to treat the medicine or not. The reward \(Y_{t}\) is designed to represent the intent of the doctor as much as possible, e.g., avoiding the patient's mortality. The unobserved confounded

Figure 4: Simulation results for our experiments. Fig. 3(a) illustrates the performance gap histogram for the experiment MDPobs, where negative values indicate performance worse than expert performance. Fig. 3(b) shows the convergence plot for CAIL, GAIL, and BC performance. Fig. 3(c) shows the final performance, where y-axis represents the expected return.

\(U_{t}\) simultaneously affects the action \(X_{t}\) and the next state \(S_{t+1}\). Simulation results are illustrated in Fig. 4c, which shows that the proposed framework performs the best among all strategies. BC and IRL fail to obtain an imitating policy that could match expert performance.

## 5 Conclusion

This paper investigates imitation learning in Markov Decision Processes where the unobserved confounding bias cannot be ruled out _a priori_. We establish theoretically that when such unobserved confounders generally exist, it is infeasible to obtain a robust imitating policy that can perform at least as well as the expert across all possible environments compatible with the demonstration data and prior knowledge. Departing from this critical realization, our research diverges into two distinct problem settings - one where only the transition distribution is unconfounded, but the reward function is non-identifiable due to unobserved confounding; and the other where the reward function is unconfounded and the transition distribution is non-identifiable. We then propose novel imitation learning algorithms using partial identification techniques, which allow the imitator to obtain effective policies that can achieve expert performance for both problem settings. Through extensive experiments, we empirically validate the theoretical findings and systematically evaluate our algorithms on different scenarios, ranging from simulated causal models to real-world datasets.