# Diffusion-based Reinforcement Learning via

Q-weighted Variational Policy Optimization

Shutong Ding\({}^{1,3}\) Ke Hu\({}^{1}\) Zhenhao Zhang\({}^{1}\) Kan Ren\({}^{1,3}\) Weinan Zhang\({}^{2}\) Jingyi Yu\({}^{1,3}\) Jingya Wang\({}^{1,3}\) Ye Shi\({}^{1,3}\)

\({}^{1}\)ShanghaiTech University \({}^{2}\)Shanghai Jiao Tong University

\({}^{3}\)MoE Key Laboratory of Intelligent Perception and Human Machine Collaboration

{dingsht, v-huke, v-zhangzh1, renkan}@shanghaitech.edu.cn

{yujingyi, wangjingya, shive}@shanghaitech.edu.cn

wnzhang@sjtu.edu.cn

###### Abstract

Diffusion models have garnered widespread attention in Reinforcement Learning (RL) for their powerful expressiveness and multimodality. It has been verified that utilizing diffusion policies can significantly improve the performance of RL algorithms in continuous control tasks by overcoming the limitations of unimodal policies, such as Gaussian policies. Furthermore, the multimodality of diffusion policies also shows the potential of providing the agent with enhanced exploration capabilities. However, existing works mainly focus on applying diffusion policies in offline RL, while their incorporation into online RL has been less investigated. The diffusion model's training objective, known as the variational lower bound, cannot be applied directly in online RL due to the unavailability of 'good' samples (actions). To harmonize the diffusion model with online RL, we propose a novel model-free diffusion-based online RL algorithm named Q-weighted Variational Policy Optimization (QVPO). Specifically, we introduce the Q-weighted variational loss and its approximate implementation in practice. Notably, this loss is shown to be a tight lower bound of the policy objective. To further enhance the exploration capability of the diffusion policy, we design a special entropy regularization term. Unlike Gaussian policies, the log-likelihood in diffusion policies is inaccessible; thus this entropy term is nontrivial. Moreover, to reduce the large variance of diffusion policies, we also develop an efficient behavior policy through action selection. This can further improve its sample efficiency during online interaction. Consequently, the QVPO algorithm leverages the exploration capabilities and multimodality of diffusion policies, preventing the RL agent from converging to a sub-optimal policy. To verify the effectiveness of QVPO, we conduct comprehensive experiments on MuJoCo continuous control benchmarks. The final results demonstrate that QVPO achieves state-of-the-art performance in terms of both cumulative reward and sample efficiency. Our official implementation is released in https://github.com/wadx2019/qvpo/.

## 1 Introduction

Recent years have witnessed significant success in the application of diffusion policy in imitation learning [6; 22; 27; 32], and offline reinforcement learning [40; 1; 16; 4; 5; 11; 44; 26]. In imitationlearning, diffusion models [13; 35; 7] are often employed to capture the intricate distributions found within expert datasets. Furthermore, in offline RL, certain works replace unimodal policies with diffusion models to enrich the diversity in sampled actions or decision sequences. This success can be largely attributed to the robust expressiveness and multimodality inherent in diffusion models [13; 35]. Such methods have demonstrated their effectiveness in practical scenarios like robotic navigation [36; 43], robot arm manipulation[6; 17; 33; 23], dexterous hand and legged robot locomotion control .

While diffusion policies have been extensively explored, their application in online RL has received relatively less attention. In online RL, value estimation varies with policy changes , posing a significant challenge. While diffusion models can effectively capture data distribution, they cannot directly improve the policy due to their training objectives . Consequently, integrating diffusion models into traditional online RL framework [34; 10; 9; 38] is challenging. Some works, such as DIPO , propose leveraging Q-function gradients to update actions for higher rewards, followed by employing diffusion models to fit the updated action distribution. However, relying on gradient updates constrains the algorithm's exploration capability. QSM  directly aligns the score and the gradient of the Q-function under the perspective of score-matching. However, since the gradient of the Q-network is inaccurate, QSM has a doubled approximation error from the alignment process, which prevents the policy from converging to optimality.

To address these issues, we propose a novel model-free online RL algorithm called Q-weighted Variational Policy Optimization (QVPO). The core idea behind QVPO is straightforward yet effective. By revisiting the VLO objective of diffusion models and the policy objective of online RL, we discovered that the Variation LOwer Bound (VLO) objective, with appropriate weights for state-action pairs, can form a tight lower bound of the policy objective under certain conditions. This new objective, termed Q-weighted VLO loss, is complemented by Q-weight transformation functions, allowing it to be applied to general RL tasks. Additionally, we add an entropy term to the diffusion policy loss to enhance exploration. Calculating the exact entropy is intractable due to the inaccessibility of the probability density function (PDF) of diffusion policies. Therefore, we design an entropy regularization term from another perspective. Moreover, while diffusion policies offer advantages like multimodality, they also introduce large policy variance, leading to inefficient interaction with environments. To address this, we develop an efficient behavior policy through action selection to improve sample efficiency. With QVPO, diffusion policies can fully leverage their exploration capability and multimodality in the online RL paradigm. QVPO achieves state-of-the-art performance in terms of both cumulative reward and sample efficiency compared to traditional online RL methods and existing diffusion-based RL methods in MuJoCo locomotion tasks .

Our contributions are summarized as follows:

1) **Q-weighted VLO Loss.** By revisiting the VLO loss of diffusion models and policy loss in online RL, we propose the Q-weighted VLO loss, which is the core component of the proposed QVPO method. We further extend its application to general RL tasks via equivalent Q-weight transformation functions. Additionally, we theoretically prove that the Q-weighted VLO loss is a tight lower bound of the policy objective in online RL.

2) **Diffusion Entropy Regularization & Efficient Behavior Policy.** We find that the exploration ability of diffusion policies declines with limited diffusion steps. To address this, we apply a special entropy regularization term to the policy loss, which is nontrivial for diffusion policies. Additionally, to cope with the large policy variance of the diffusion model, we develop an efficient behavior policy through action selection to improve sample efficiency.

3) **State-of-the-art Performance.** We verified the effectiveness of QVPO on MuJoCo locomotion benchmarks. Experimental results indicate that QVPO achieves state-of-the-art performance in terms of sample efficiency and episodic reward compared to both previous traditional and diffusion-based online RL algorithms.

## 2 Related Works

In this section, we will review the existing works that utilize diffusion models in decision-making tasks and generally divide them into four categories according to different applications.

**Diffusion Policies for Imitation Learning.** Imitation learning is a framework for learning a behavior policy from expert datasets without explicit reward. Diffusion models can effectively fit the distribution of given datasets due to their powerful expressiveness. Some research works such as DiffusionPolicy , Crossway Diffusion , AVDC  and  exemplify this by generating robot action sequences via diffusion based policy conditioned on visuomotor observation. Considering the reward is sparse or inaccessible in the expert dataset, BESO  leverages the diffusion model in the domain of goal-conditioned imitation learning to learn a goal-specified policy.

**Diffusion Planners.** Planning in RL involves using the dynamic function to inform decision-making over a long horizon. Diffusion planners reduce compounding errors by directly constructing the complete trajectory rather than relying on one-step transitions. Diffuser  arranges state-action sequences into a structured two-dimensional array and samples the trajectories based on reward guidance. In subsequent work, Decision Diffuser  generates state sequences with classifier-free guidance and then uses inverse dynamic function to derive actions to alleviate the distribution shift problem. Latent Diffuser  generates sequences in a learned latent space and then reconstructs the original trajectories. A bunch of works such as [21; 3; 8] leverage hierarchical structures to enhance learning efficiency and decision-making capabilities by decomposing complex tasks into a hierarchy of sub-tasks.

**Diffusion Policies for Offline RL.** Unlike most imitation learning problems without reward and assuming an optimal expert to provide the data, offline RL faces the challenge of learning optimal policy from suboptimal offline datasets . Diffusion-QL  integrates the diffusion model with the conventional Q-learning framework. However, the training of Diffusion-QL is unstable in the out-of-distribution (OOD) state region, SRDP  reconstructs the state to alleviate the distribution shift caused by OOD. EDP  reduces the computation cost during diffusion inference by applying Tweedie's formula to predict actions. CEP  draws samples by diffusion sampling with exact guidance defined by the energy function based on contrastive learning. CPQL  uses a consistency model to represent the policy. Some works [12; 4; 11] utilize diffusion models to construct the optimal policy by weighted regression [29; 28]

**Diffusion Policies for Online RL.** Online RL faces a more challenging problem in that it requires algorithms to learn the policy and make decisions in real-time interaction with the environment. DIPO  is the first to employ diffusion policies in online RL and proposes a novel policy improvement method. During diffusion policy updates, each action in the reply buffer is updated via action gradient to receive higher rewards. However, the action of gradient updating may deviate from the behavior policy, and the paradigm of gradient updating limits the algorithm's ability of global exploration and raises computational costs. QSM  introduces a new update role for diffusion policy by aligning the score of the diffusion model with the gradient of the Q-function to increase the Q-value of state-action pairs. However, it overlooks the impact of inaccuracies in the value function gradient and biases introduced during the alignment process when updating the policy. This can lead to the policy being influenced by suboptimal data within the buffer. Compared with these existing diffusion-based online RL algorithms, QVPO performs policy optimization with the Q-weighted VLO loss which can be theoretically proven to be the tight lower bound of the policy objective of online RL. This means QVPO does not incur additional errors in policy optimization. Besides, diffusion policy entropy regularization and efficient behavior policy via action selection techniques are provided to further enhance the performance of the diffusion policy in online RL.

## 3 Preliminaries

### Reinforcement Learning

For reinforcement learning (RL), an MDP is defined as \((,,p,r,_{0},)\), where \(\) is the state space, \(\) is the action space, \(p:[0,)\) is the transition probability function of the next state \(s_{t+1}\) given the current state \(s_{t}\) and the action \(a_{t}\), \(r:[r_{},r_{}]\) is the bounded reward function, \(_{0}:[0,)\) is the distribution of the initial state \(s_{0}\) and \(\) is the discount factor for value estimation. In an MDP, the process starts from an initial state \(s_{0}\) sampled from \(_{0}\) and then samples actions \(a_{t}\) from the policy \((a|s):[0,)\) given the state \(s_{t}\). Here \(=(s_{0},a_{0},s_{1},a_{1})\) denotes this kind of trajectory and \(\) denotes the distribution of trajectory \(\) given the policy \((a|s)\). The state-value function \(V_{}(s)=_{}[_{t=0}^{t}r_{t}|s_{0}=s]\) represents the expected reward that the agent can obtain under the policy \(\) in the state \(s\). Compared to the state-value function, the action-state function is \(Q_{}(s,a)=_{}[_{t=0}^{t}r_{t}|a_{0}=a,s_{0}=s]\) where action \(a_{0}\) is provided as input. Further, the advantage function refers to the extra benefit of taking a specific action relative to taking the average action at a given state which is defined as the following:\(A_{}(s,a)=Q_{}(s,a)-V_{}(s)\). The goal of RL is to learn the policy that maximizes the discounted expectedcumulative reward, defined as \(J()=_{}[_{t=0}^{t}r_{t}]\), which can be optimized via performing multiple steps of policy improvement \(_{k+1}=_{}_{_{k}}[Q_{_{k}}(s,a)((a  s))]\).

### Denoising Diffusion Probabilistic Models

Denoising diffusion probabilistic models (DDPM)  are powerful latent variable generative models that transform any data distribution into a simple Gaussian distribution by adding noise (forward process) and then denoise it using neural networks (reverse process). Given a dataset \(\{_{0}^{i}\}_{i=1}^{N}\) for \(_{0}^{i} q(_{0})\), the forward process of DDPM transforms the distribution \(q(x_{0})\) into a tractable prior Gaussian distribution by incorporating Gaussian noise in \(T\) steps with the transitions:

\[q(_{t}_{t-1}):=(_{t};}_{t-1},_{t}),\] (1)

where \(_{t}\) is the variance schedule. Using the Markov chain property, we can obtain the distribution of \(_{t}\) conditioned on \(_{0}\):

\[q(_{t}_{0})=(_{t};_{t}}_{0},(1-_{t})),\] (2)

where \(_{t}=1-_{t}\) and \(_{t}=_{s=0}^{t}_{s}\). Eventually, when \(T\), \(_{T}\) converges to an isotropic Gaussian distribution. In the reverse process, DDPM first generates noise from the prior distribution \(p(_{T})=(_{T};0,)\) and then gradually denoises it by learning parameterized transitions \(p_{}(_{t-1}_{t})=(_{t-1}; _{}(_{t},t),_{}(_{t},t))\) to fit \(q(_{t}_{t-1},_{0})\), where \(\) denotes the learnable parameters. The training objective of DDPM is to maximize the Variational LOwer Bound (VLO) defined as \(L_{}=_{q(_{0:T})}[( _{0:T})}{q(_{1:T}_{0})}]\). Finally, with \((0,)\), the loss in DDPM takes the form of:

\[_{t[1,T],_{0},_{t}}[||_{t}- _{}(_{t}}_{0}+_ {t}}_{t},t)||^{2}].\] (3)

## 4 Q-weighted Variational Policy Optimization

In this section, we will first revisit the VLO loss  of the diffusion model and the policy objective of online RL . Then, we derive the Q-weighted VLO loss, which is the principal component of the proposed Q-weighted Variational Policy Optimization (QVPO). However, the Q-weighted VLO loss cannot be directly applied in general RL tasks. To address this issue, we also provide equivalent Q-weight transformation functions. Since the derived Q-weighted VLO loss is the tight lower bound of the policy loss in online RL, QVPO can effectively avoid the drawbacks of existing online RL methods for diffusion policies [42; 31] mentioned in Section 2.

QVPO also involves practical techniques for the underlying issues of optimizing diffusion policies. Firstly, to further enhance the exploration ability of diffusion policy, we develop a special entropy regularization. This is not trivial for diffusion policy since the log probability of the state-action pair is not available. Besides, we devise an efficient behavior policy via action selection for the diffusion model to avoid the large policy variance of diffusion, which results in sample inefficiency. With these practical techniques, diffusion policy can achieve better performance with fewer online interactions with the environments. Finally, the training procedure of QVPO is shown in Figure 1.

### Q-weighted Variational Objective for Diffusion Policy

As mentioned in DIPO , optimizing diffusion policies in the online RL paradigm is nontrivial, which principally results from two reasons. In one aspect, if we directly apply the deterministic policy gradient to diffusion policies like Diffusion-QL , the backpropagation chain through the denoising procedure of the diffusion model becomes quite long. This leads to high computational costs and instability during training, which severely restricts the performance of diffusion policies in online RL. In another aspect, samples from the optimal policy are required when directly using the variational bound objective to train diffusion policies. However, these optimal samples are typically unavailable in online RL.

In this context, we revisited the VLO objective of diffusion models and the RL policy objective. To our surprise, we discovered that by adding the appropriate weights to the VLO objective, it becomes a tight lower bound of the RL policy objective under certain conditions.

**Theorem 1**.: _(Lower Bound of RL Policy Objective) If \(Q(s,a) 0\) for any state-action pair \((s,a)\), the \(Q\)-weighted variational bound objective of diffusion policy_

\[_{s,a p(s^{}|s,a),_{k}(a|s)}[Q(s,a_{0})_{a_{1:T} q(a_{1:T} s,a_{0})}[(a_{0:T}  s)}{q(a_{1:T} s,a_{0})}]]\]_is the tight lower bound of the objective of RL policy_

\[_{s,a p(s^{}|s,a),_{k}(a|s)}[Q(s,a)(_{}(a |s))],\]

_and the equality holds when the policy converges._

The proof can be referred to in the supplementary materials. According to this theorem, we can derive the Q-weighted variational bound loss as (4), which can be applied to diffusion policy optimization.

\[()_{s,a_{k}(a|s),,t }[^{2}}{2_{t}^{2}_{t}(1-_{t} )}Q(s,a)\|-_{}(_{t}}a+_{t}},s,t)\|^{ 2}].\] (4)

According to , removing the coefficient \(^{2}}{2_{t}^{2}_{t}(1-_{t} )}\) does not affect the training of diffusion. Hence, the final Q-weighted VLO loss is defined as

\[()_{s,a_{k}(a|s),,t }[Q(s,a)\|-_{}(_{t}}a+_{t}},s,t)\|^ {2}].\] (5)

While we can now use (5) to optimize the diffusion policy, two issues remain to be resolved.

1) **Negative \(Q\) value.** In real-world decision-making tasks, it is difficult to ensure that the returned reward is always non-negative, which means the \(Q\) value may be negative for some state-action pairs. Therefore, to apply QVPO in practical tasks, we must address situations where the \(Q(s,a)\) value is negative for certain states and actions.

2) **High-quality Training Samples.** According to (5), achieving significant policy improvement requires obtaining certain rare state-action samples with high Q values. However, this is a significant challenge with limited interaction with the environment. This issue is also common in other online RL methods based on weighted policy training, such as RWR , and it hinders their application to real-world tasks.

### Equivalent Transformation for Q-weighted VLO Loss

To resolve these two problems in training diffusion policies with the Q-weighted VLO loss, we propose equivalent Q-weight transformation functions to convert the Q value into an equivalent positive Q weight and leverage the powerful data-synthesizing ability of the diffusion model to generate high-quality samples for training. The new equivalent Q-weighted VLO loss is defined in (6). We will introduce the detailed implementation of these solutions in the following contexts.

\[()_{s,a_{k}(a|s),,t }[_{eq}(s,a)\|-_{}( {_{t}}a+_{t}},s,t)\|^ {2}].\] (6)

Figure 1: The training pipeline of QVPO. In each training epoch, QVPO first utilizes the diffusion policy to generate multiple action samples for every state. Then, these action samples will be selected and endowed with different weights according to the Q value given by the value network. Besides, action samples from uniform distribution are also created for the diffusion entropy regularization term. With these action samples and weights, we can finally optimize the diffusion policy via the combined objective of Q-weighted VLO loss and diffusion entropy regularization term.

**Theorem 2**.: _(Optimal Solution in One Policy Improvement Step) Considering the optimization problem in policy improvement step \(_{k+1}=_{}_{_{k}}[Q_{_{k}}(s,a)((a  s))]\), the optimal policy in a given state \(s\) is_

\[p(a s)=\{_{Q(s,a)>0}(a)(a s)Q(s,a)da},&Q(s,a)>0\\ 0,&.\] (7)

_when there exists \(a\) such that \(Q(s,a)>0\), and_

\[p(a s)=_{i=1}^{N}(x-a_{i}),a_{i}\{a Q(s,a )=_{a}Q(s,a),(a s)>0\},\] (8)

_when \(Q(s,a) 0\) for any \(a\), \((x-a)\) is the Dirac function, \(N\) is the cardinality of set \(\{a Q(s,a)=_{a}Q(s,a),(a s)>0\}\), \((a s)\) is the behavior policy, \(p(a s)\) is the policy to be optimized and \(_{Q(s,a)>0}\) is the indicator function that judges whether \(Q(s,a)>0\)._

The proof can be referred to in the supplementary materials. Hence, with Theorem 2, we can obtain the probability density function (PDF) of the optimal policy in one policy improvement step and further use \(\) as the weight to achieve an equivalent optimized diffusion policy according to (6). However, the difficulty of judging whether any \(Q(s,a) 0\) in a certain state \(s\) is still a problem. Therefore, we first come up with an approximate solution "_qcut_" weight transformation function, which returns \(Q(s,a)\) when \(Q(s,a) 0\) and a small value \(>0\) when \(Q(s,a)<0\). Furthermore, the "_qcut_" weight transformation function just endows the best action with the weight mentioned above and discards the left action samples via setting them as zero.

In short, _qcut_ weight transformation function takes the formulation of the above two situations into account and "cuts" them via a small value \(>0\). Notably, we remove the denominator \(_{Q(s,a)>0}(a s)Q(s,a)da\) in _qcut_ weight transformation function. This is because the denominator \(_{Q(s,a)>0}(a)(a s)Q(s,a)da\) is somewhat like the \(V(s)\), which denotes the "importance" of the state \(s\). Therefore, removing this item can allow important states to obtain larger weights during training, which is confirmed in practice.

However, we find that the _qcut_ weight transformation function does not work very well when Q values in most state-action pairs are less than zero. To address this issue, we propose another "_qadv_" weight transformation function, which is much simpler but performs better in our experiments.

\[_{eq}(s,a)_{quadv}(s,a)=\{A(s,a ),&A(s,a) 0\\ 0,&A(s,a)<0.\] (9)

where \(A(s,a)=Q(s,a)-V(s)\) is the advantage function. _qadv_ weight transformation function replaces the \(Q(s,a)\) with the advantage \(A(s,a)\) to avoid the second situation. Moreover, applying Q value and advantage in policy optimization has been proved equivalent and can also reduce the training variance in previous works like .

It is worth noting that, for _qadv_ weight transformation function, we do not require an extra network to approximate \(V(s)\) to calculate \(A(s,a)\). We can always sample a certain number of actions for a state \(s\) with diffusion policy and estimate \(V(s)\) using the average of their \(Q(s,a)\). Besides, for _qadv_ function, we also recommend just selecting the best action sample with the largest A value for each state in policy optimization, which can further improve the quality of training samples and reduce the training cost.

**Remark.** Although the weighted VLO loss of QVPO looks similar to AWR  and EDP , they are different intrinsically. With the Q-weighted VLO loss, QVPO trains the diffusion policy with the weighted sample from the current policy, while AWR or EDP trains the diffusion policy with the weighted sample from the policy in the offline dataset (replay buffer). These "_on-policy_" samples truly benefit the online training. Moreover, AWR and EDP utilize the \(\) weight function; but this function is too conservative for online RL.

### Enhancing Policy Exploration via Diffusion Entropy Regularization

While diffusion policies can achieve impressive performance with the Q-weighted VLO loss in online RL, their powerful exploration capabilities have yet to be fully harnessed. As noted in Diffusion-QL, the policy expressiveness of the diffusion model decreases with fewer diffusion steps. In our experiments, we find that not only does policy expressiveness decline, but the exploration capability of the diffusion model also diminishes when the number of diffusion steps is reduced. However, it is essential to limit the number of diffusion steps to avoid excessive training and evaluation costs in real-world applications. Therefore, studying how to enhance the exploration ability of the diffusion model with a limited number of diffusion steps is necessary.

Adding an extra entropy regularization term to the policy loss appears to be a good solution, as it has been validated for categorical policies in discrete action spaces and Gaussian policies in continuous action spaces. However, estimating the entropy for a diffusion policy is nontrivial due to the inaccessibility of the log-likelihood of action samples. Moreover, maximizing the entropy of a policy can be viewed as narrowing the distance between the policy and a maximum entropy distribution (i.e., uniform distribution) in some sense. Thus, by recalling the VLO loss of the diffusion model, the entropy of a diffusion model can be increased with training samples from the uniform distribution. Based on this idea, we propose an entropy regularization term for diffusion policies as follows,

\[_{ent}()_{s,a,,t}[_{ent}(s)\|-_{} (_{t}}a+_{t}},s,t )\|^{2}],\] (10)

where \(_{ent}(s)=_{ent}_{i=1}^{N}(s,a_{i})}{N}\) is the coefficient related to state \(s\) for balancing the trade-off between exploitation and exploration. Here, \(N\) represents the number of selected training samples from the diffusion policy for state \(s\). When \(_{ent}\) is large, the exploration ability of the diffusion policy is improved, whereas the exploitation ability is reduced. Figure 2 is the experiment results on a continuous bandit toy example that clearly illustrates the effect of this entropy regularization term on the diffusion policy, and the concrete reward function of this bandit problem is \(R(x)=_{i=1}^{3}w_{i}_{i}}(-}(x-_{i})^{T}(x-_{i}))\), where \(w_{i}=1.5\), \(_{i}=0.1\), and \(_{i}=[-1.35,0.65]^{T}\), \([-0.65,1.35]^{T}\) and \([-1.61,1.61]^{T}\) respectively.

### Reducing Diffusion Policy Variance via Action Selection

Despite the diffusion model that allows the online RL agent to seek better policies, it also introduces a large policy variance. This results in inefficiency for online interactions of the behavior policy with the environment. To address this issue, we propose an efficient behavior policy via action selection for diffusion model \(_{}^{K}(a s)\), which improves the sample efficiency. Specifically, it is motivated by the idea that the efficiency of action samples from behavior policy depends on their Q values. In other words, action samples from behavior policy with high Q values indicate that the following trajectory also has a large underlying reward and is of great significance for training. Specifically, the efficient behavior policy \(_{}^{K}(a s)\) is defined as:

\[_{}^{K}(a s)*{argmax}_{a\{a_{1},, a_{K}_{}(a s)\}}Q(s,a).\]

Furthermore, while using the efficient behavior policy \(_{}^{K}(a s)\) as a behavior policy can improve sample efficiency, it is not recommended to apply the same action selection number \(K\) to the target

Figure 2: A toy example on continuous bandit to show the effect of diffusion entropy regularization term via the changes of the explorable area for diffusion policy with the training procedure. The contour lines indicate the reward function of continuous bandit, which is an arbitrarily selected function with 3 peaks.

policy for calculating the TD target. This is due to the large policy variance of the diffusion policy, which can result in a serious overestimation of the target Q value. Hence, if we choose the action selection number \(K_{b}\) for the behavior policy, a smaller action selection number \(K_{t}<K_{b}\) for the target policy is often a good choice in practice.

## 5 Experiments

In this section, we first describe a practical implementation of QVPO, as shown in Algorithm 1, which is implemented in an off-policy fashion. Then, we evaluate the proposed algorithm on different decision-making tasks and with various hyper-parameters. With these experimental results, we aim to answer three questions:

* How does QVPO compare to previous popular online RL algorithms and existing diffusion-based online RL algorithms?
* How does the entropy regularization term affect the performance of QVPO?
* Can the \(K\)-efficient behavior policy significantly improve the sample efficiency of QVPO?

**Input:** Diffusion policy \(_{}(a s)\), value network \(Q_{}(s,a)\), replay buffer \(\), \(K_{b}\)-efficient diffusion policy for behavior policy, \(K_{t}\)-efficient diffusion policy for target policy, number of training samples \(N_{d}\) from diffusion policy, number of training samples \(N_{e}\) from uniform distribution \((,)\).

```
1:for\(t\) in \(1,2,,T\)do
2: Sample the action using the diffusion policy \(_{}^{K_{b}}(a s_{t})\).
3: Take the action \(a_{t}\) in the environment and store the returned transition in \(\).
4: Sample a mini-batch \(\) of transitions in \(\).
5: Generate \(N_{d}\) samples from \(_{}(a s)\), and \(N_{e}\) samples from \((,)\) for each state \(s\) in \(\).
6: Endow the \(N_{d}\) samples with weights (9).
7: Select an action sample \(a_{max}\) with maximum weight among \(N_{d}\) samples for training.
8: Endow the \(N_{e}\) samples with the weight \(_{ent}(s)=_{ent}_{eq}(s,a_{max})\).
9: Update the parameters of the diffusion policy using the summation of (6) and (10).
10: Construct TD target as \(y_{t}=r_{t}+ Q_{}(s_{t+1},_{}^{K_{t}}(a s_{t+1}))\) for each \((s_{t},a_{t},r_{t},s_{t+1})\) in \(\).
11: Update the parameters of the value network using MSE loss.
12:endfor ```

**Algorithm 1** Q-weighted Variational Policy Optimization

### Comparative Evaluation

To demonstrate the effectiveness of our method, we compared QVPO with six other online model-free RL algorithms: two off-policy algorithms (TD3  and SAC ), two on-policy algorithms (PPO  and SPO ), and one advanced diffusion-based online RL algorithm (DIPO , QSM ). These comparisons were conducted on five MuJoCo locomotion tasks . we plotted the learning curves of QVPO and the six other algorithms over five random runs, as shown in Figure 3. The solid curve represents the average return, and the transparent shaded region represents the standard deviation. Each experiment was conducted over \(1e6\) training epochs. According to the learning curves, QVPO achieves state-of-the-art performance compared to the other six algorithms, and converges much faster than the other algorithms, further demonstrating its sample efficiency. Additionally, the practical implementation of QVPO follows SAC in critic part, which utilizes doubled Q networks and only use the minimum for policy and critic update.

Table 1 presents the evaluation results of QVPO and six other algorithms on the MuJoCo locomotion tasks after \(1e6\) iterations, further confirming the superiority of QVPO. Notably, during the evaluation

  Environments & PPO & SPO & TD3 & SAC & DIPO & QSM & QVPO(*) \\  Hopper-v3 & 3154.3(426.2) & 2212.8(988.48) & 3267.5(8.5) & 2996.6(111.9) & 3295.4(7.0) & 2154.7(998.2) & **3728.5(13.8)** \\ Walker2d-v3 & 3751.56(90.1) & 3321.6(13.82) & 3513.9(40.7) & 4888.1(80.0) & 4681.7(72.57) & 3613.4(143.5) & **5191.8(60.2)** \\ Ant-v3 & 2781.9(74.1) & 2100.2(302.4) & 4538.5(69.5) & 5030.9(100.3) & 5656.9(54.7) & NNA & **6425.16(76.7)** \\ HalfCheetah-v3 & 4773.5(53.4) & 4008.2(246.8) & 10388.6(80.4) & 10616.9(72.8) & 9590.5(76.7) & 3888.2(632.6) & **11385.6(164.5)** \\ Humanoid-v3 & 713.7(85.9) & 797.4(262.1) & **5353.5(53.7)** & 5159.7(475.3) & 4945.5(898.6) & 4793.1(229.5) & **5306.6(14.5)** \\  

Table 1: Comparison of QVPO and 6 other online RL algorithms in evaluation results. (N/A indicates the algorithm does not work)stage, we apply the efficient behavior policy with a large action selection number \(K=32\) instead of the deterministic denoising procedure used in DIPO . As illustrated in , the high-probability region in each diffusion step is not the origin but a Gaussian sphere. Hence, using the deterministic denoising procedure in the evaluation stage is not a good choice. In that case, using the proposed efficient behavior policy during evaluation is more reasonable. More details related to the experiments can be found in the supplementary materials.

### Ablation Study and Parameter Analysis

To further examine the significance of each component in QVPO, we also conducted the ablation study and parameter analysis on the effects of the entropy regularization and the \(K\)-efficient behavior policy. Here We use the experiment on Ant-v3 as an example since the final results are similar across different locomotion tasks.

**Effect of Diffusion Entropy Regularization.** As shown in Figure 4, without the diffusion entropy regularization loss, QVPO with \(100\) diffusion steps achieves much better performance than QVPO with \(20\) diffusion steps. However, when the entropy regularization loss is applied, QVPO with \(20\) diffusion steps achieves close performance to QVPO with \(100\) diffusion steps, which verifies the effectiveness of the entropy regularization term.

**Action Selection number \(K\) for Efficient Behavior Policy.** Figure 5 presents the performance of QVPO with different action selection numbers for behavior policy \(K_{b}\) and for target policy \(K_{t}\). It can be observed that QVPO with action selection numbers \(K_{b}=4,K_{t}=1\) is superior to QVPO with action selection numbers \(K_{b}=1,K_{t}=1\), while the training stability of QVPO with \(K_{b}=4,K_{t}=2\) and \(K_{b}=4,K_{t}=1\) is better than that of QVPO with \(K_{b}=4,K_{t}=4\). This indicates that efficient behavior policy via action selection can improve sample efficiency and setting the action selection numbers \(K_{t}<K_{b}\) can help reduce the overestimation error of the target Q value. Besides, the experiment with \(K_{b}=20,K_{t}=2\) shows that a

Figure 4: Comparison between QVPO with and without the diffusion entropy regularization.

Figure 5: Comparison of QVPO with different action selection numbers for behavior policy \(K_{b}\) and for target policy \(K_{t}\).

Figure 3: Learning Curves of different algorithms on 5 Mujoco locomotion benchmarks across 5 runs. The x-axis is the number of training epochs. The y-axis is the episodic reward. the plots smoothed with a window of 5000.

too-high action selection number will lead to a limited exploration. Therefore, setting \(K_{b}=4\) can balance exploration and exploitation well.

## 6 Conclusion

In this paper, we proposed a novel diffusion-based online RL algorithm called QVPO that sufficiently exploits the expressiveness and multimodality of diffusion policy. QVPO leverages the Q-weighted VLO loss, a core component that serves as a tight lower bound of the policy objective in online RL under certain conditions, facilitated by Q-weight transformation functions. Additionally, we designed a special entropy regularization term to enhance the exploration capabilities of diffusion policies and the efficient behavior policy to improve sample efficiency by reducing the behavior policy variance. Our comprehensive experiments on MuJoCo continuous control benchmarks demonstrate that QVPO achieves state-of-the-art performance in terms of both cumulative reward and sample efficiency, surpassing both traditional and existing diffusion-based online RL methods.

However, there are still several underlying challenges in applying diffusion policies to online RL that require further exploration. For instance, although we developed a special entropy regularization loss to approximate the effect of maximizing entropy, it lacks the ability to adaptively adjust the entropy term and cannot incorporate the entropy of the diffusion policy into the TD target for soft policy iteration, as seen in SAC. Future work will focus on developing adaptive entropy adjustment mechanisms and integrating entropy into the TD target to enable soft policy iteration, which we believe will further enhance the performance of diffusion policies in online RL.