# Object-centric Learning with Cyclic Walks

between Parts and Whole

Ziyu Wang\({}^{1,2,3}\)   Mike Zheng Shou\({}^{1,}\)   Mengmi Zhang\({}^{2,3,}\)

\({}^{1}\)Show Lab, National University of Singapore, Singapore

\({}^{2}\)Deep NeuroCognition Lab, CFAR and I2R, Agency for Science, Technology and Research, Singapore

\({}^{3}\)Nanyang Technological University, Singapore

\(\)Corresponding authors; address correspondence to mengmi@i2r.a-star.edu.sg

###### Abstract

Learning object-centric representations from complex natural environments enables both humans and machines with reasoning abilities from low-level perceptual features. To capture compositional entities of the scene, we proposed cyclic walks between perceptual features extracted from vision transformers and object entities. First, a slot-attention module interfaces with these perceptual features and produces a finite set of slot representations. These slots can bind to any object entities in the scene via inter-slot competitions for attention. Next, we establish entity-feature correspondence with cyclic walks along high transition probability based on the pairwise similarity between perceptual features (aka "parts") and slot-binded object representations (aka "whole"). The whole is greater than its parts and the parts constitute the whole. The part-whole interactions form cycle consistencies, as supervisory signals, to train the slot-attention module. Our rigorous experiments on _seven_ image datasets in _three unsupervised_ tasks demonstrate that the networks trained with our cyclic walks can disentangle foregrounds and backgrounds, discover objects, and segment semantic objects in complex scenes. In contrast to object-centric models attached with a decoder for the pixel-level or feature-level reconstructions, our cyclic walks provide strong learning signals, avoiding computation overheads and enhancing memory efficiency. Our source code and data are available at: link.

## 1 Introduction

Object-centric representation learning refers to the ability to decompose the complex natural scene into multiple object entities and establish the relationships among these objects [29; 24; 14; 38; 39; 37; 22]. It is important in multiple applications, such as visual perception, scene understanding, reasoning, and human-object interaction [45; 2]. However, learning to extract object-centric representations from complex natural scenes in an unsupervised manner remains a challenge in machine vision.

Recent works attempt to overcome this challenge by relying on image or feature reconstructions from object-centric representations as supervision signals [29; 38; 22; 37] (Figure 1). However, these reconstructions have several caveats. First, these methods often require an additional decoder network, resulting in computation overheads and memory inefficiency. Moreover, these methods focus excessively on reconstructing unnecessary details at the pixel or feature levels and sometimes fail to capture object-centric representations from a holistic view.

To mitigate issues from reconstructions, some studies [47; 19] introduce mutual information maximization between predicted object-centric representations and feature maps. Building upon this idea, contrastive learning has become a powerful tool in unsupervised object-centric learning. The recent works [21; 5] propose to learn the spatial-temporal correspondences between a sequence of video frames with contrastive random walks. The objective is to maximize the likelihood of returning tothe starting node by walking along the graph constructed from a palindrome of frames and repelling nodes with distinct features from adjacent frames in the graph.

Generalizing from contrastive random walks on video sequences, we proposed cyclic walks on static images. These walks are conducted between predicted object entities and feature maps extracted from vision transformers. The cycle consistency of these walks serves as supervision signals to guide object-centric representation learning. First, inspired by DETR  and Mask2Former , where these methods decompose the scene into multiple object representations or semantic masks with a span of a finite set of task-dependent semantically meaningful bases, we use a slot-attention module to interface with feature maps of an image and produce a set of object-centric representations out of the slot bases. These slot bases compete with one another to bind with feature maps at any spatial location via normalizing attention values over all the slot bases. All the features resembling a particular slot basis are aggregated into one object-centric representation explaining parts of the image.

Next, two types of cyclic walks, along high transition probability based on the pairwise similarity between aggregated object-centric representations and image feature maps, are established to learn entity-feature correspondence. The motivation of the cyclic walks draws a similar analogy with the part-whole theory stating that the whole (i.e. object-centric representations) is greater than its parts (i.e. feature maps extracted from natural images) and the parts constitute the whole. We use "parts" and "whole" for easy illustrations of the interaction between image feature maps and object-centric representations. On one hand, cyclic walks in the direction from the whole to the parts and back to the whole (short for "W-P-W walks") encourage diversity of the learnt slot bases. On the other hand, cyclic walks in the direction from the parts to the whole and back to the parts (short for "P-W-P walks") broaden the coverage of the slot bases pertaining to the image content. Both W-P-W walks and P-W-P walks form cycle consistencies and serve as supervision signals to enhance the specificity and diversity of learned object-centric representations. Our contributions are highlighted below:

* We introduce cyclic walks between parts and whole to regularize object-centric learning. Both W-P-W walks and P-W-P walks form virtuous cycles to enhance the specificity and diversity of the learned object-centric representations.
* We verify the effectiveness of our method over seven image datasets in three unsupervised vision tasks. Our method surpasses the state-of-the-art by a large margin.
* Compared with the previous object-centric methods relying on reconstruction losses, our method does not require additional decoders in the architectures, greatly reducing the computation overheads and improving memory efficiency.

## 2 Related Works

One representative work  introduces the slot-attention module, where the slots compete to bind with certain regions of the image via a cross-attention mechanism and distill image features into object-centric representations. To train these slots, the networks are often attached to a decoder to

Figure 1: **Comparison of the network structures trained with existing methods and our cyclic walks. Supervision signal is indicated by (\(\)) in subplots a-c. From left to right, all the methods require some forms of reconstructions as supervisory signals except for our method (d): (a) image reconstruction for the Slot-Attention (SA)  and BO-QSA  (b) feature reconstruction for DINOSAUR (c) tokens obtained by a Discrete Variational Autoencoder  (DVAE) for SLATE  (d). Our cyclic walks do not require any image or feature reconstructions.**

reconstruct either images or features based on object-centric representations (Figure 1). Subsequent Slot-Attention methods [6; 39; 24; 14; 4; 13] expand to video processing. To learn object-level representations, these methods often require either reconstructions from video frames or optical flows [4; 13]. Although the original slot attention was demonstrated to be effective in most cases, it fails to generalize to complex scenes [37; 22]. To mitigate this issue, DINOSAUR _freezes_ the feature extractor while learning the slot bases. Its frozen feature extractor was pre-trained on ImageNet in unsupervised learning . BO-QSA  proposes to initialize the slots with learnable queries and optimize these queries with bi-level optimization. Both works emphasize that good initialization of network parameters is essential for object-centric learning in complex scenes. Thus, following this line of research, we also use a frozen DINO  as the feature extractor. However, different from these works, we introduce cyclic walks in the part-whole hierarchy of the same image, without decoders for reconstructions.

**Object-centric learning for unsupervised semantic segmentation.** One of the key applications in object-centric representation learning is unsupervised semantic segmentation. Several notable works include STEGO , DeepCut , ACSeg , and FreeSOLO . All these methods employ a fixed pre-trained transformer as a feature extractor, which has proven to be effective for unsupervised semantic segmentation. In contrast to these methods which directly perform optimization on pixel clusters or feature clusters, we introduce slot-attention and perform cyclic walks between slots and feature maps. Experimental results demonstrate the superior performance of our method.

**Relation to contrastive learning.** Our method connects object-centric representation learning with unsupervised contrastive learning. We highlight the differences between our method and the contrastive learning methods. Infoseg  reconstructs an image from a set of global vectors (aka "slot bases"). The reconstructed image is pulled closer to the original image and repelled away from a randomly selected image. SlotCon  introduces contrastive learning on two sets of slot bases extracted from two augmented views of the same image. In contrast to the two methods, our cyclic walks curate both positive and negative pairs along the part-whole hierarchy from the _same_ image. Another method, SAN , performs contrastive learning between slot bases to encourage slot diversity, where every slot is repelled away from one another. Sharing similar motivations, our walks from the whole to the parts and back to the whole ("W-P-W" walks) strengthen the diversity of the slot bases. Moreover, our method also introduces "P-W-P" walks, which takes the holistic view of the entire image, and broadens the slot coverage.

## 3 Preliminary

Here, we mathematically formulate **Slot-Attention Module** and **Contrastive Random Walks**.

Figure 2: **Overview of our proposed cyclic walks. The input image is divided into non-overlapped patches and encoded into feature vectors (aka “parts”) through a frozen DINO ViT pre-trained on ImageNet with unsupervised learning methods . The Slot-Attention module aggregates the patch features into object-centric representations (aka “whole”) based on the learnable slot bases. To train the network, we perform the contrastive random walks in two directions between the parts and the whole: (b) any node from the whole should walk to the parts and back to itself (W-P-W walks) and (c) any node from the parts should walk to the whole and back to itself (P-W-P walks). See Fig A7 for applications of these learned object-centric representations in downstream tasks.**

### Slot-Attention Module

The slot-attention module takes an input image and outputs object-centric representations. Given an input image, a pre-trained CNN or transformer extracts its feature vectors \(x R^{N D_{input}}\), where \(N=W H\). \(H\) and \(W\) denote the height and width of the feature maps. Taking \(x\) and a set of \(K\) slot bases \(s R^{K D_{slot}}\) as inputs, Slot-Attention binds \(s\) with \(x\), and outputs a set of \(K\) object-centric feature vectors \(\) of the same size as \(s\). The Slot-Attention module employs the cross-attention mechanism , where \(x\) contributes to keys and values and \(s\) contributes to queries. Specifically, the inputs \(x\) are mapped to the dimension \(D\) with linear functions \(k()\) and \(v()\) and the slots \(s\) are mapped to the same dimension with linear function \(q()\). For each feature vector at every spatial location of the feature maps \(x\), attention values are calculated with respect to all slot bases. To prevent parts of the image from being unattended, the attention matrix is normalized with the softmax function first over K slots and then over \(N\) locations:

\[_{i,j}=}}{_{K}e^{A_{i,j}}} A=}}{} R^{N K}\] (1) \[=W^{T}v(x)W_{i,j}=_{i,j}}{ _{N}_{i,j}}\] (2)

The cross-attention mechanism introduces competitions among slot bases for explaining parts of the image. Based on the attention matrix, an object-centric feature vector from \(\) is distilled by the weighted sum of feature maps \(x\). In the literature [8; 38; 29] and our method, the slot attention modules iteratively refine the output \(\) from the slots in a recurrent manner with a Gated Recurrent Unit (GRU) for better performances. We represent the initial slot bases as \(s^{0}\). The parameters of \(s^{0}\) are initialized by randomly sampling from Gaussian distribution with learnable mean \(\) and variance \(\). The output \(\) at time step \(t\) acts as the slot bases and feeds back to the GRU at the next recurrent step \(t+1\). Thus, the final object-centric representations \(\) after \(T\) iterations can be formulated as:

\[=s^{T},s^{t+1}=GRU(s^{t},^{t})\] (3)

After obtaining the object-centric representation \(\), the traditional object-centric learning models decode the images from the slots with a mixture-based decoder or a transformer-based decoder . The training objective of the models is to minimize the Mean Square Error loss between the output of the decoder and the original image at the feature or pixel levels.

### Contrastive Random Walks

A random walk describes a random process where an independent path consists of a series of hops between nodes on a directed graph in a latent space. Without loss of generality, given any pair of feature sets \(a R^{m d}\) and \(b R^{n d}\), where \(m\) and \(n\) are the numbers of nodes in the feature sets and \(d\) is the feature dimension, the adjacency matrix \(M_{a,b}\) between \(a\) and \(b\) can be calculated as their normalized pairwise feature similarities:

\[M_{a,b}=/}}{_{n}e^{f(a)f(b)^{T}/}} R ^{m n},\] (4)

where \(f()\) is the \(l_{2}\)-normalization and \(\) is the temperature controlling the sharpness of distribution with its smaller values indicating sharper distribution.

In the original work , random walks serve as supervisory signals to train the object-centric learning models to capture the space-time correspondences from raw videos. The contrastive random walks are formulated as a graph. \(a\) and \(b\) are the feature maps extracted from video frames with either a CNN or a transformer. \(m\) and \(n\) are the numbers of spatial locations on the feature maps and they are often the same across video frames. On the graph, only nodes from adjacent video frames \(F_{t}\) and \(F_{t+1}\) at time \(t\) and \(t+1\) share a directed edge. Their edge strengths, indicating the transition probabilities of a random walk between frames, are the adjacency matrix \(M_{F_{t},F_{t+1}}\). The objective of the contrastive random walks is to maximize the likelihood of a random walk returning to the starting node along the graph constructed from a palindrome video sequence \(\{F_{t},F_{t+1},...,F_{T},F_{T-1},...,F_{t}\}\).

Our Method

### Object-centric Feature Encoding

Following [18; 37], we use a self-supervised vision transformer trained with DINO on ImageNet, as the image feature extractor (Figure 1(a)). In Section 5.4, we verify that our method is also agnostic to other self-supervised feature learning backbones. Given an input image, DINO parses it into non-overlapped patches and each patch is projected into a feature token. We keep all patch tokens except for the classification token in the last block of DINO. As in Section 3.1, we use the same notation \(x R^{N D_{input}}\) to denote feature maps extracted from a static image. The Slot-Attention module takes the feature vectors \(x R^{N D_{input}}\) and a set of K learnable slot bases \(s R^{K D_{slot}}\) as inputs and produces the object-centric representations \( R^{K D_{slot}}\) ( Equations 1 and 2).

### Whole-Parts-Whole Cyclic Walks

Features of the image patches constitute the objects. The interactions between parts and the whole provide a mechanism for clustering and discovering objects in the scene. Motivated by this, we introduce cyclic walks in two directions: (a) from the whole to the parts and back to the whole (W-P-W walks) and (b) from the parts to the whole and back to the parts (P-W-P walks). Both serve as supervisory signals for learning slot bases \(s\).

Given feature vectors \(x R^{N D_{input}}\) of all image patches (aka "parts") and the object-centric representations \( R^{K D_{slot}}\) (aka "whole"), we apply a linear layer to map both \(x\) and \(\) to be of the same dimension \(D\). In practice, K is much smaller than N. Following the formulations of Contrastive Random Walks  in Section 3.2, random walks are conducted along high transition probability based on pairwise similarity from \(\) to \(x\) and from \(x\) to \(\) using Equation 4:

\[M_{wpw}=M_{,x}M_{x,} R^{K K},M_{,x} R ^{K N}M_{x,} R^{N K}\] (5)

\(M\) is defined as an adjacency matrix. Different from the original work  performing contrastive random walks on a palindrome video sequence, here, we enforce a palindrome walk in a part-whole hierarchy of the image. Ideally, if the W-P-W walks are successful, all the bases in the slot-attention module have to establish one-to-one mapping with certain parts of the image. This encourages the network to learn diverse object-centric representations so that each slot basis can explain certain parts of the image and every part of the image can correspond to a unique slot basis. To achieve this in W-P-W walks, we enforce that the probability of walking from \(\) to \(x\) belonging to the object class and back to \(\) itself should be an identity matrix \(I\) of size \(K K\). The first loss term is defined as: \(L_{wpw}=CE(M_{wpw},I)\), where \(CE(,)\) refers to cross entropy loss.

### Parts-Whole-Parts Cyclic Walks

Though the W-P-W walks enhance the diversity of the slot bases, there is an ill-posed situation, when there exists a limited set of slot bases failing to cover all the semantic content of an image but every trivial W-P-W walk is always successful. For example, given two slot bases and an image consisting of a triangle, a circle, and a square on a background, one slot could prefer "triangles", while the other prefers "circles". In this case, the W-P-W walks are always successful; however, the two slots fail to represent squares and the background, defeating the original intention of foreground and background extraction. To mitigate this problem and bind all the slots with all the semantic content in the entire scene, we introduce additional walks from the parts to the whole and back to the parts (P-W-P walks), complementary to W-P-W walks: \(M_{pwp}=M_{x,}M_{,x} R^{N N}\). As N is typically far larger than K and features \(x\) at nearby locations tend to be similar, the probabilities of random walks beginning from one feature vector of \(x\), passing through \(\), and returning to itself could no longer be 1. Thus, we use the feature-feature correspondence \(S\) as the supervisory signal to regularize \(\):

\[S_{i,j}=}}{_{N}e^{W_{i,j}}}W_{i,j}= -,&F_{i,j}<=\\ F_{i,j},&F_{i,j}>,F=f(x)f(x)^{T} R^{N  N}\] (6)

Hyper-parameter \(\) is the similarity threshold for preventing the random walks from returning to locations where their features are not as similar as the starting point. Thus, the overall loss of our cyclic walks can be calculated below, where \(\) and \(\) are the coefficients balancing the two losses.

\[L= L_{wpw}+ L_{pwp},L_{pwp}=CE(M_{wpp},S)\] (7)

**Training Configuration.** We use ViT-Small pre-trained with DINO as our feature extractor and freeze the parameters of DINO throughout the entire training process. There are multiple reasons for freezing the feature extractor. First, pre-trained unsupervised feature learning frameworks have already generated semantically consistent content in nearby locations [18; 1; 26]. Object-centric learning addresses its follow-up problem of capturing compositional entities out of the semantic content. Second, recent object-centric learning works [37; 22] have emphasized the importance of freezing the pre-trained weights of feature extractors (see Section 2). For a fair comparison with these methods, we keep the parameters of the feature extractor frozen. Third, we provided empirical evidence that our method benefits from freezing DINO, and freezing DINO is not a limitation of our method in **Appendix A1**. We use a similarity threshold of 0.7 and a temperature of 0.1 for all the experiments over all the datasets. In general, the variations of these two hyperparameters lead to moderate changes in performances (see Sec 5.4). See **Appendix A2** for more training and implementation details. All models are trained on 4 Nvidia RTX A5000 GPUs with a total batch size of 128. We report the mean \(\) standard deviation of 5 runs with 5 random seeds for all our experiments.

## 5 Experiments

### Unsupervised Foreground Extraction

**Task Setting, Metrics, Baselines and Datasets.** In the task, all models learn to output binary masks separating foregrounds and backgrounds in an unsupervised manner. See **Appendix A3.1** for implementations of foreground and background mask predictions during the inference. We evaluate the quality of the predicted foreground and background masks with mean Intersection over Union

   Model &  &  &  &  \\  & mIoU & Dice & mIoU & Dice & mIoU & Dice & mIoU & Dice \\  Slot-Attention  & 35.6 & 51.5 & 39.6 & 55.3 & 41.3 & 58.3 & 30.8 & 45.9 \\ SLATE  & 36.1 & 51.0 & 62.3 & 76.3 & 75.5 & 85.9 & 68.1 & 79.1 \\ DINOSAUR  & 67.2 & 78.4 & 73.7 & 84.6 & 80.1 & 87.6 & 72.9 & 82.4 \\ BO-QSA  & 71.0 & 82.6 & 82.5 & 90.3 & 87.5 & 93.2 & **78.4** & **86.1** \\  Cyclic walks (ours) & **72.4** & **83.6** & **86.2** & **92.4** & **90.2** & **94.7** & 75.1 & 83.9 \\   

Table 1: **Results in the unsupervised foreground extraction task. We report the results in mIoU and Dice on CUB200 Birds (Birds), Stanford Dogs (Dogs), Stanford Cars (Cars), and Flowers (Flowers) datasets. Numbers in bold are the best.**

Figure 3: **Visualization of attention maps predicted by slot bases. In (a) unsupervised foreground extraction, we present two examples each from Birds, and Flowers datasets. In each example, for all the slots, we provide their attention maps (Col.2-3) and their corresponding attended image regions (Col.4-5). Ground truth foreground and background masks are shown in the last Col. In (b) unsupervised object discovery, we provide 3 examples on the PASCAL VOC dataset. In each example, for all the slots, we provide their attention maps (att., Col.2-5) and their corresponding attended image regions (reg., Col. 6-9). Their combined object discovery masks from all the slots are shown in Col.10 (pred.). Each randomly assigned color denotes an image region activated by one slot. Ground truth object masks are presented in the last column.**

(mIoU)  and Dice . The mIoU measures the overlap between predicted masks and the ground truth. Dice is similar to mIoU but replaces the union of two sets with the sum of the number of their elements and doubles the intersection. We used publicly available implementations of Slot-Attention  and SLATE  from  and re-implemented DINOSAUR  by ourselves. Note that the results of re-implemented DINOSAUR deviate slightly from the original results in . We provided further comparisons and discussed such discrepancy in **Appendix A4**. However, none of these alter the conclusions of this paper. Following the work of BO-QSA , we include Stanford Dogs , Stanford Cars , CUB 200 Birds , and Flowers  as benchmark datasets.

**Results and Analysis.** The results in mIoU and Dice are presented in Table 1. Our method achieved the best performance on the Birds, Dogs, and Cars datasets and performed competitively well as BO-QSA on the Flowers dataset. Slot-attention, SLATE, and DINOSAUR use the pixel-level, feature-level, and token-level reconstruction losses from object-centric representations as supervisory signals. The inferior performance of slot attention over SLATE and DINOSAUR implies that pixel-level reconstructions focus excessively on unnecessary details, impairing the object-centric learning performance. We also note that DINOSAUR outperforms Slot-Attention and SLATE by a significant margin. This aligns with the previous findings that freezing pre-trained feature extractors facilitates object-centric learning. In addition to reconstructions, BO-QSA imposes extra constraints on learnable slot queries with bi-level optimizations, which leads to better performances. This emphasizes the necessity of searching for better supervisory signals to regularize object-centric learning beyond reconstructions. Indeed, even without any forms of reconstruction, we introduce cyclic walks between parts and whole as an effective training loss for object-centric learning. Our method yields the best performance among all comparative methods. The result analyses hold true in subsequent tasks (Section 5.2 and 5.3).

We also visualize the attention masks predicted by our slot bases in Figure 3(a) (see **Appendix A5.1** for more visualization results). We observed that the model trained with our cyclic walks outputs reasonable predictions. However, we also noticed several inconsistent predictions. For example, our predicted masks in the Flowers dataset (Row 3 and 4) group the pedals and the sepals of a flower altogether, because these two parts often co-occur in a scene. Moreover, we also spotted several wrongly annotated ground truth masks. For example, in Row 4, the foreground mask mistakenly incorporates the sky as part of the ground truth. It is also interesting to see that the foreground object belonging to the same object class is not always identified by a fixed slot basis (compare Row 1 and 2 in the Birds dataset). During the inference, we randomly sample parameters from learnable mean \(\) and variance \(\) as the slot bases (Section 3.1), which could cause the reverse of foreground and background masks. Applying the same set of parameters for slot bases across the experiments avoids such issues.

### Unsupervised Object Discovery

**Task Setting, Metrics, Baselines and Datasets.** The task aims to segment the image with a set of binary masks, each of which covers similar parts of the image, (aka "object masks"). However, different from image segmentation, each of the masks does not have to be assigned specific class labels. We follow recent works and evaluate all models [29; 38; 22; 37] and K-Means with ARI on foreground objects (ARI-FG). ARI reflects the proportion of sampled pixel pairs on an image, correctly classified into the same class or different classes. All baseline implementations are based on the open-source codebase . Same as Section 5.1, we use the \(M_{x,s}\) as the object masks. \(M_{x,s}\) is the similarity matrix, indicating the transition probability between features and slots (see Equation 4 and Equation 5). We benchmark all methods on the common datasets Pascal VOC 2012 , COCO 2017 , Movic-C  and Movic-E. We also evaluate all methods on a synthetic dataset ClevrTex for further comparisons (see **Appendix A6**).

**Results and Analysis** The results in ARI-FG on the four datasets are shown in Figure 4. The unsupervised object discovery task is harder than the unsupervised foreground extraction task (Section 5.1) due to the myriad scene complexity and object class diversity. Our method still consistently outperforms all SOTA methods and beats the second-best method DINOSAUR by a large margin of 2 - 4% over all four datasets. Different from all other methods attached with decoders for the image or feature reconstructions, our model trained with cyclic walks learns better object-centric representations. We visualized some example predicted object masks in Figure 3(b) (see **Appendix A5.2** for more positive samples). Our method can clearly distinguish semantic areas in complex scenes. For example, trains, trees, lands, and sky are segmented in Row 1 in an unsupervised manner. Note that the foreground train masks are provided as the ground truth annotations in PASCAL VOC only for qualitative comparisons. They have never been used for training. Additional visualization results on MOVi can be found in **Appendix A5.3**. In the experiments, we also noticed several failure cases. When the number of semantic classes in the image is less than the number of slots, our method segments the edges of semantic classes (see **Appendix A5.2** for more analysis).

### Unsupervised Semantic Segmentation

**Task Setting, Metrics, Baselines and Datasets.** In the task, each pixel of an image has to be classified into one of the pre-defined object categories. See **Appendix A3.2** for implementations of obtaining the category labels for each predicted mask during inference. We report the intersection over union (IoU) between the predicted masks and the ground truth over all categories. We include ACSeg , MaskDistill , PiCIE , STEGO , SAN , InfoSeg , SlotCon , and COMUS  for comparison. All the results are directly obtained from their original papers. We evaluate all competitive methods on Pascal VOC 2012, COCO Stuff-27 , and COCO Stuff-3 . COCO Stuff-27 and COCO Stuff-3 contain 27 and 3 supercategories respectively.

**Results and Analysis** We report the results in terms of IoU in Table 2. Our method has achieved the best performance on COCO-Stuff-3 and the second best on Pascal VOC 2012 and COCO-Stuff-27. This highlights that our cyclic walks in the part-whole hierarchy on the same images act as effective supervision signals and distill image pixels into diverse object-centric representations rich in semantic information. It is worth pointing out that our model is not specially designed for semantic segmentation. Yet, it is remarkable that our method still performs competitively well as the majority of the existing semantic segmentation models.

On Pascal VOC 20212, our method slightly underperforms COMUS  in mIoU metrics (43.3% versus 50.0%). We argue that, in comparison to our method, there are two additional components in COMUS possibly attributing to the performance differences and unfair comparisons. First, COMUS employs two pre-trained saliency detection architectures DeepUSPS  and BasNet  in addition to DINO. The saliency detection model requires an additional MSRA image dataset  during pre-training. Thus, compared to our cyclic walks, COMUS indirectly relies on extra useful information. Second, COMUS uses Deeplab-v3 as its backbone for predicting semantic segmentation masks. For better segmentation performances, Deeplab-v3 extracts large-resolution feature maps from images

   Model &  Pascal VOC \\ 2012 \\  &  COCO \\ Stuff-27 \\  & 
 COCO \\ Stuff-3 \\  \\  ACSeg  & 47.1 & 16.4 & - \\ MaskDistill  & 42.0 & - & - \\ PiCIE + H  & - & 14.4 & - \\ STEGO  & - & **26.8** & - \\ SAN  & - & - & 80.3 \\ InfoSeg  & - & - & 73.8 \\ SlotCon  & - & 18.3 & - \\ COMUS  & **50.0** & - & - \\   

Table 2: **Results of Unsupervised Semantic Segmentation in IoU on Pascal VOC 2012, COCO-Stuff-27  and COCO-Stuff-3 . (\(\)) standard deviations of IoU over 5 runs are also reported. The ’-’ indicates that corresponding results are not provided in the original papers and the source codes are unavailable. Best is in bold.**

Figure 4: **Performance of Object Discovery. From left to right, the used datasets are Pascal VOC 2012 , COCO 2017 , Movi-C  and Movi-E . We report the performance in foreground adjusted rand index (ARI-FG). The higher the ARI-FG, the better. We compare our method (cyclic walks) with K-means, Slot-Attention , SLATE , DINOSAUR , and BO-QSA .**

and applies Atrous Spatial Pyramid Pooling for aggregating these feature maps over multiple scales. In contrast, our method extracts low-resolution feature maps with DINO and is not specifically designed for unsupervised semantic segmentation.

Despite a slightly inferior performance to COMUS, our method is capable of parsing the entire scene on the image into semantically meaningful regions, which COMUS fails to do. These include distinct backgrounds, such as sky, lake, trees, and grass. For example, in Fig 3(b), our method successfully segments backgrounds, such as the lake, the trees, and the sky. However, COMUS fails to segment background elements. For example, in Column 4 of Fig 1 in the paper of COMUS , COMUS only segments the foreground ships and fails to segment the lake and the sky. Segmenting both salient objects and other semantic regions is essential for many computer vision applications. This emphasizes the importance and unique advantages of our method. We also attempted to apply object-centric methods in instance segmentation tasks. In the **Appendix A5.4**, we provided visualization results and discussions.

### Ablation Study and Method Analysis

We examine the effect of hyperparameters, the components of training losses, and the variations of feature extractors on object-centric learning performances and report ARI-FG scores for all these experiments on Pascal VOC 2012  and COCO 2017  (Table 3).

**Ablation on Random Walk Directions.** To explore the effects of random walk directions, we use either P-W-P or W-P-W walks to train the network separately and observe the changes (Table 3, Col.3-4). We found that either direction of random walks can make the training loss of the network converge. The W-P-W walks perform slightly better than the P-W-P walks, but neither of these walks surpasses random walks in both directions. This aligns with the design motivation of our method (Section 4).

**Analysis on Temperature of Random Walks.** We titrate the temperate in Equation 4 from 0.07 to 1 and observe a non-monotonic performance trend versus temperature choices (Col.5-7). On one hand, the lower the temperature, the more concentrated the attention distribution over all the slots and the faster the network can converge due to the steeper gradients. On the other hand, the attention distribution becomes too sharp with much lower temperatures, resulting in optimization instability.

**Analysis on Number of Slots.** The number of slots imposes constraints on the diversity of objects captured in the scene. We trained models with various numbers of slots (Col.8-10). More slots do not always lead to better performances. With more slots, it is possible that all slots compete with one another for each image patch and the extra slots only capture redundant information, hurting the object discovery performance.

**Analysis on Similarity Threshold in P-W-P Walks.** During P-W-P cyclic walks, we introduce similarity threshold \(t\) (Equation 6). From Col.11-13, we can see that with the increase of threshold \(\) from \(-inf\) to 0.7, the P-W-P random walks become more selective, enforcing the slot bases to learn more discriminative features; hence, better performance in object discovery tasks. However, when the threshold approaches \(1\), the ability to walk to neighboring image patches with high semantic similarity to the starting patch is impaired, leading to the overfitting of slot bases.

    & Full &  Random Walk \\ Model \\  } &  Temperature \\ Direction \\  } &  &  &  \\  & & &  &  &  &  \\   & & P-W-P &  & 0.07 & 0.3 & 1 & 5 & 6 & 7 & -inf & 0.3 & 1 & Moco-v3  & MAE  & MSN  \\   Pascal VOC \\ 2012 \\  & **29.6** & 27.1 & 28.4 & 23.5 & 27.3 & 21.9 & 27.7 & 26.2 & 24.3 & 22.5 & 26.5 & 26.2 & 29.3 & 26.8 & 29.0 \\   & & P-W-P &  & 0.07 & 0.3 & 1 & 10 & 12 & 13 & -inf & 0.3 & 1 & Moco-v3  & MAE  & MSN  \\  
 COCO \\ 2017 \\  & **39.7** & 34.8 & 36.4 & 35.5 & 36.7 & 29.6 & 37.9 & 35.7 & 35.3 & 29.8 & 32.3 & 31.8 & 38.4 & 34.5 & 38.2 \\   

Table 3: **Ablation Studies and Method Analysis on Pascal VOC 2012 and COCO 2017 in terms of ARI-FG. Full model refers to our default method introduced in Section 4. See **Appendix A2** for the empirically determined hyper-parameter details of our full model. We vary one factor at a time and study its effect on ARI-FG performances in the Pascal VOC 2012 (Row 3) and COCO 2017 (Row 5). The best is our default model highlighted in bold. See Section 5.4 for details.**

**Analysis on Different Feature Extractors.** We replace pre-trained DINO transformer (Section 4.1) with MOCO-V3 , MAE , and MSN . Together with DINO, these feature extractors are state-of-the-art unsupervised representation learning frameworks. From Col.14-16, we observe that various backbones with our method consistently achieve high ARI-FG scores over both datasets. This suggests that our method is agnostic to backbone variations and it can be readily adapted to any general SOTA unsupervised learning frameworks.

**Analysis on Reconstruction Loss.** We assess the effect of pixel-level reconstructions by adding a transformer-based decoder and introducing an extra reconstruction loss. The upgraded model achieved slightly better performance than our default model (from 29.6% to 29.9% in ARI-FG) on Pascal VOC 2012. This indicates that the reconstruction loss provides auxiliary information for object-centric learning. In future work, we will explore the possibilities of predicting object properties from the slot features.

### Efficiency Analysis in Model Sizes, Training Speed, GPU Usages, and Inference Speed

We report the number of network parameters, training speed, GPU usage, and inference speed in Table 4. All these experiments are run with the same hardware specifications and method configurations: (1) one single RTX-A5000 GPU; and (2) input image size of 224 \(\) 224, batch size of 8, and 4 slots. In comparison with all transformer decoder-based methods (SLATE, DINOSAUR and BO-QSA), our method uses the fewest parameters and yet, achieves the best performance in all the tasks (Sections 5.1, 5.2, 5.3). Compared to other decoder-based methods, Slot-Attention requires fewer parameters by sampling from a mixture of slot bases for image reconstruction. However, our method requires even fewer parameters than Slot-Attention (Col. 1). Moreover, although the transformer-based networks are slower in training compared with the CNN-based networks, DINOSAUR, and our method are much faster than SLATE and BO-QSA due to freezing the feature extractor (Col. 2). Besides, the losses of our method converge the fastest. On Pascal VOC 2012 and COCO 2017, our model can achieve the best performance within 10k training steps, while other models need at least 100k training steps. By additionally benefiting from cyclic walks and bypassing decoders for feature reconstruction, our method runs twice as much faster as DINOSAUR. With the same reasoning, the networks trained with our cyclic walks only require half of the GPU memory usage of DINOSAUR and achieve the fastest inference speed without sacrificing the performance in all three tasks (Col. 3 and Col. 4).

## 6 Discussion

We propose cyclic walks in the part-whole hierarchy for unsupervised object-centric representation learning. In the slot-attention module, a finite set of slot bases compete to bind with certain regions of the image and distill into object-centric representations. Both P-W-P and W-P-W cyclic walks serve as implicit supervision signals for training the networks to learn compositional entities of the scene. Subsequently, these entities can be useful for many practical applications, such as scene understanding, reasoning, and explainable AIs. Our experiments demonstrate that our cyclic walks outperform all competitive baselines over seven datasets in three unsupervised tasks while being memory-efficient and computation-efficient during training. So far, our method has been developed on a frozen unsupervised feature extractor. In the future, hierarchical contrastive walks can be explored in any feed-forward architectures, where the models can simultaneously learn both pixel-level and object-centric representations incrementally over multiple layers of the network architecture. We provide in-depth discussion of limitations and future works in **Appendix A7**.

    & Parameters & Training speed & GPU usage & Inference speed \\  & (\(10^{3}\)) & (image / sec) & (M) & (image / sec) \\  Slot-Attention & 3144 & 114 & 4371 & 126 \\ SLATE & 14035 & 27 & 16327 & 32 \\ DINOSAUR & 11678 & 124 & 3443 & 130 \\ BO-QSA & 14223 & 25 & 16949 & 32 \\  Cyclic walks (ours) & **1927** & **208** & **2331** & **285** \\   

Table 4: **Method Comparison in the number of parameters, training speed, GPU memory, and inference speed.** From top to bottom, we include Slot-Attention, SLATE, DINOSUAR and BO-QSA. The inference speed was reported based on Object Discovery. The best is in bold.