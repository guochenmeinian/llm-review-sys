# Separation and Bias of Deep Equilibrium Models on Expressivity and Learning Dynamics

Zhoutong Wu\({}^{1}\)

ztwu@stu.pku.edu.cn

&Yimu Zhang\({}^{2}\)

zym24@stu.pku.edu.cn

Cong Fang\({}^{2,3}\)

fangcong@pku.edu.cn

&Zhouchen Lin\({}^{2,3,4}\)

zlin@pku.edu.cn

Corresponding author.

\({}^{1}\) Academy for Advanced Interdisciplinary Studies, Peking University

\({}^{2}\) State Key Lab of General AI, School of Intelligence Science and Technology, Peking University

\({}^{3}\) Institute for Artificial Intelligence, Peking University

\({}^{4}\) Pazhou Laboratory (Huangpu), Guangzhou, Guangdong, China

###### Abstract

The deep equilibrium model (DEQ) generalizes the conventional feedforward neural network by fixing the same weights for each layer block and extending the number of layers to infinity. This novel model directly finds the fixed points of such a forward process as features for prediction. Despite empirical evidence showcasing its efficacy compared to feedforward neural networks, a theoretical understanding for its separation and bias is still limited. In this paper, we take a step by proposing some separations and studying the bias of DEQ in its expressive power and learning dynamics. The results include: (1) A general separation is proposed, showing the existence of a width-\(m\) DEQ that any fully connected neural networks (FNNs) with depth \(O(m^{})\) for \((0,1)\) cannot approximate unless its width is sub-exponential in \(m\); (2) DEQ with polynomially bounded size and magnitude can efficiently approximate certain steep functions (which has very large derivatives) in \(L^{}\) norm, whereas FNN with bounded depth and exponentially bounded width cannot unless its weights magnitudes are exponentially large; (3) The implicit regularization caused by gradient flow from a diagonal linear DEQ is characterized, with specific examples showing the benefits brought by such regularization. From the overall study, a high-level conjecture from our analysis and empirical validations is that DEQ has potential advantages in learning certain high-frequency components.

## 1 Introduction

Implicit deep learning , a paradigm that generalizes the recursive principles of traditional explicit models, has gained renewed interest with the advent of novel neural network architectures. Among these, deep equilibrium model (DEQ)  stands out as a commonly utilized model. In contrast to explicit neural network that derives features through forward propagation, DEQ computes features directly by solving an equilibrium equation induced by the implicit layer. Since the equilibrium state is also the limit point of the infinitely recursive iterations of the implicit layer, DEQ can be regarded as a new neural network that models the limit of a multi-layer weight-tied neural network with the depth going to infinity.

Nowadays, DEQ has become a popular and widely studied model in the field of machine learning. On the empirical side, competitive performances against explicit feedforward neural networks have been achieved in various real applications such as natural language processing , computer vision , image generation , and solving inverse problems . On the theoretic side, a main research line is to study the well-posedness of DEQ. This line aims to analyze when unique equilibrium can be guaranteed by DEQ and some weight parameterization and initialization techniques have been proposed to ensure the well-posedness [6; 7; 8].

However, despite wide studies on DEQ, an understanding of the basic learning theory for its separation and bias against explicit feedforward neural networks is still limited. For the expressivity, a preliminary study about the connections between DEQ and fully-connected network (FNN) is provided in the seminar work , where it is proved that every FNN can be reformulated as a large DEQ under a specific weight re-parameterization, whereas, a deeper study on the provable and quantitative advantage of DEQ in its expression power is still lacking. Besides, there is another research line that studies the learning properties of DEQ using the so-called neural tangent kernel (NTK) view , originating from analyzing FNNs [10; 11]. It has been shown that under suitable initialization, the dynamic of over-parameterized DEQ can be approximated by a linear kernel model [12; 13], therefore the global convergence of gradient descent algorithm and possible generalization can be achieved under some regimes. However, in the NTK regime, it is shown that DEQs are almost equivalent to not-so-deep explicit FNNs for high dimensional Guassian mixtures , and it is still not known whether DEQs have potential advantages over FNNs in more general settings. A study on the separation and bias of DEQ over FNN can provide us with clear and intuitive suggestions about when DEQ is preferred in practice, thus it is strongly desired. In this paper, we initialize the study by analyzing the expressive power and learning dynamics of DEQs. The main results are sketched as follows.

1. We first propose a general separation showing that there exists a width-\(m\) DEQ which cannot be approximated to a constant accuracy by an FNN with depth \(O(m^{})\) for \((0,1)\) unless its width is \(((m^{1-}))\). This is achieved by comparing the the number of linear regions that the two networks can generate. Based on the result, we further prove that a width-\(m\) DEQ can generate at most \(2^{m}\) linear regions, which has provable advantages than FNNs.
2. We then propose another separation, where a steep function in \(^{d}\) being the solution to fixed point equation is considered as the target function. We show that a DEQ with size and magnitude bounded by \(O(^{-1})\) can approximate this function to \(O()\)-accuracy in \(L^{}\) norm, whereas an FNN with bounded depth and exponentially bounded width cannot unless its weights is \(((d))\). For the technical contribution, we manage to show that an approximation of the fixed point mapping by the implicit layer can also guarantee the approximation the solution defined by the fixed point equation even if the Lipschitz constant of the fixed point mapping is very close to \(1\) by a new observation as shown in Lemma 3.
3. Finally, we study the bias of DEQ from the perspective of learning dynamics. We propose a general characterization of regularization for gradient flow in an overparameterized setting. We further analyze the dynamics of both gradient flow and gradient descent, showing that under mild conditions, convergence is guaranteed, and the model tends to produce 'dense' features. Then we offer a concrete example on a specific Out-of-Distribution (OOD) task, demonstrating that this bias can help reduce the OOD error.

Finally, we conduct experiments to validate our theoretical results. From the overall study, a high-level conjecture is that DEQ has potential advantages in learning certain high-frequency components.

**Notations.** We use standard notation \(O()\) and \(()\) to hide constants. We use \(\) to denote the ReLU function, i.e., \((x)=(0,x)\), and we use \(()\) to denote the sign function. We use \(()\) to transform a vector into a diagonal matrix with the vector's elements on the diagonal. We denote by \(\|\|_{p}\) the \(^{p}\) vector norm or the subordinate matrix norm, and by \(\|h\|_{L^{p}(k)}\) the \(L^{p}\)-norm of a function \(h\) on a compact set \(K\). For a vector or vector-valued function \(\), we denote \(v_{i}\) the \(i\)-th entry of the vector or the function. For a function \(u:\), we denote \(u^{ n}\) the \(n\)-fold composition of \(u\).

## 2 Related Works

In this section we briefly review the literature that are most related to us.

Theoretical Studies on DEQs.Theoretical research on DEQs has primarily focused on ensuring their well-posedness [6; 7]. To guarantee well-posedness, strategies like new parameterizations [6; 7], regularization , and special initialization  were proposed. Another research line delves into the learning properties of DEQ. The expressivity of DEQ is preliminarily studied in . Regarding learning dynamics, some works [16; 17; 13] couple the dynamics of over-parameterized DEQs with a linear kernel using the NTK method. They manage to prove the global convergence and study the generalization . Other studies examine DEQs in a linear framework or infinite-width limit. For instance, Kawaguchi  studies the convergence of linear DEQ, while Gao et al.  explore information propagation in DEQ and show its equivalence to a Gaussian process in the infinite-width limit. Nevertheless, an in-depth study on the potential or quantifiable advantage of DEQ over FNN is still lacking.

Separations on Expressivity of Neural Networks.The separation on expressivity of neural networks is a fundamental study characterizing functions that can be approximated efficiently by one neural architecture but cannot by another. These architectures include FNNs [19; 20], CNNs , RNNs , etc. Since DEQ can be viewed as an infinitely deep weight-tied network, depth separation  is most relevant to our study. A key study by Telgarsky  constructs a saw-tooth function with many oscillations to give a separation, which further inspires a series of separations [25; 26; 27]. In addition to depth, some recent works study the separation regrading the overall number of neurons  or the magnitude of parameters  of the networks. In this paper, the first separation is also inspired by Telgarsky's construction, whereas we focus on the separation between DEQ and FNN and provide a more refined analysis of networks' depth. The second separation is new.

Implicit Bias of Learning Dynamics on Neural Networks.The implicit bias of learning dynamics plays a key role in determining what particular optima can be found by the algorithms when there are multiple optima. A series of papers study the implicit regularization of gradient-based methods, showing that under varying settings, these algorithms bias towards solutions with specific properties [30; 31], such as norm minimization , sparsity  and low complexity [34; 35; 36]. Due to the theoretical barrier in analyzing nonlinear neural networks , most existing works focus on simplified models such as random feature models [38; 32], networks with quadratic activations  and diagonal linear networks . This paper follows similar strategies and analyzes the implicit bias of a simplified diagonal linear DEQ from learning dynamics.

## 3 Preliminaries of DEQ

The DEQ is an implicit-depth model  that employs the same weights in each layer block of a feedforward neural network and extends the number of layer to infinity. The layer blocks used in DEQ can be fully connected, convolutional, or Transformer blocks, resulting in different variants of deep equilibrium networks. In this paper, we consider a vanilla DEQ with ReLU activation as the generalization of an FNN for the separation result and a simplified linear diagonal DEQ for the bias analysis. Specifically, an \(L\)-layer FNN from \(^{d}\) to \(^{s}\) can be expressed as

\[^{1}=;^{i+1}=(_{i}^{i}+_{i}), 1 i L-2;=_{L} ^{L},\] (1)

where \(^{d}\) and \(^{s}\). In DEQ, each \(_{i}\) and \(_{i}\) in Eq. (1) is replaced by the same weight \(\) and bias \(\), and a linear transform of the input \(\,\) is added to each layer, i.e., \(^{l}=(\,^{l-1}+\,+ )\) for all \(l\). By extending the layer \(l\) to infinity, the feature and the prediction of this DEQ can be expressed as

\[ =(\,+\,+),\] (2) \[ =\,,\]

where \(^{m m},^{m d},b ^{m}\), and \(^{s m}\). We call \((\,+\,+)\) the implicit layer and \(m\) the width of DEQ. In this paper, we mainly consider \(s=1\), i.e., DEQ as a scalar function on \(^{d}\).

In , the authors show that every FNN can be reformulated as a large DEQ with specific weight reparameterization. Specifically, the depth-\(L\) FNN described in Eq. (1) is equivalent to a DEQ in the form of Eq.(2) with

\[=,&&,_{L}, =&&&&\\ _{2}&&&\\ &_{3}&&&\\ &&&&\\ &&&_{L-1}&,=_{1}\\ \\ \\ ,=_{1}\\ _{2}\\ \\ _{L-1}.\] (3)

## 4 Separation on the Expressivity of DEQ

In this section, we focus on the separations on the expressivity of DEQ with ReLU activation. We follow the common ways to compare the expressivity from the actual size (width and depth) of the networks. More explanations on the fairness of the comparison are provided in Appendix A.4. We will show that DEQ is more _parameter-efficient_ in approximating specific target functions than FNN.

### General Separation over FNNs

The following theorem states a general separation between DEQ and FNN from the size of networks. The motivation behind the theorem is a common observation that functions with many linear pieces are typically hard to be approximated by functions having fewer linear pieces.

**Theorem 1**.: _Let \(m^{+}\). Assume that \(L m^{}\) for some \(0<<1\). Then there exists a function \(N_{d}:^{d}\) computed by a width-\(m\) ReLU-DEQ, such that for any function \(N_{f}:^{d}\) computed by a depth-\(L\) ReLU- FNN with width at most \(2^{m^{1-}-1}\), it holds that_

\[_{^{d}}|N_{d}()-N_{f}()|\, .\]

The proof is provided in Appendix A.1. It involves quantifying the number of linear regions1 generated by a DEQ compared to an FNN. Specifically, we show in the proof that there exists a DEQ producing \(2^{m}\) linear pieces whereas no-so-deep FNNs, i.e., FNNs with depth \(O(m^{})\) cannot generate such a large number of linear regions unless the width is sub-exponentially large.

Moreover, the example of the hard-to-approximate DEQ enables us to derive an exact bound on the number of linear regions that a DEQ can generate. This result is of independent interest and is stated in the proposition below.

**Proposition 1**.: _Let \(m>0\). A width-\(m\) ReLU-DEQ has at most \(2^{m}\) linear regions in the input space. Moreover, this upper bound is attainable, i.e., there exists a width-\(m\) ReLU-DEQ that computes a function with \(2^{m}\) linear regions on \(^{d}\)._

**Remark 1**.: _As a comparison, the work of  analyzes ReLU-FNNs. It shows that for a ReLU-FNN with a total of \(\) neurons of arbitrary depth, the maximal number of linear regions is bounded above by \(2^{}\). To the best of our knowledge, it is yet to be determined whether this bound is achievable. Moreover, there is evidence suggesting that this upper bound is not achievable for FNNs that when the input dimension is \(1\) (See Lemma 5 in Appendix A.1 for details). Consequently, width-\(m\) DEQs can potentially generate a larger number of linear regions compared to FNNs with \(m\) neurons, as DEQs have been shown to achieve their upper bound._

Theorem 1 shows that there exists a width-\(m\) DEQ that is hard to be approximated by FNN with depth \(O(m^{})\). This theorem along with Proposition 1 reveals that, although DEQ computes features by solving an equilibrium function induced by a shallow implicit layer, its complexity in terms of expressing linear regions of DEQ can be larger than that of not-so-deep FNN.

### Separation on Certain Steep Functions

In this section, we present another separation concerning both the size and parameter magnitude of neural networks, which more explicitly reveals the bias and potential advantages of DEQ on expressivity. The separation is based on the observation that the fixed point of a DEQ can be rewritten as the solution to an optimization problem under certain conditions.

To be specific, consider a simple quadratic optimization problem with the optimization variable \(^{m}\) and a parameter \(^{d}\):

\[_{}\;^{T}()\, +^{T}()\,+,\] (4)

where \(()\) is a positive definite matrix parameterized by \(\) and \(()\) for some \(>0\). Approximating \(=()\), i.e., the optimum as a function of the parameter \(\), serves useful primitives in various applications. Directly approximating \(()\) by FNN requires the approximation of \(()=-()^{-1}()\). On the other hand, from the optimality condition, \(()\) is implicitly defined through fixed point equation

\[=\,-(()\,+()).\]

Hence, approximating \(()\) by DEQ may only require the approximation of the fixed point mapping \(-(()\,+ ())\) by the implicit layer. To some extent, the approximation problem is 'altered' due to the model difference, which possibly leads to distinctive division in approximation.

Now, we construct a workable instance. The objective function of our central interest is a special case of Eq.(4) given by:

\[_{z}(1+-x_{1})z^{2}- x_{1}z,^{d},\] (5)

where \(=2^{-d}\). The solution function is calculated as

\[g()=}{2(1+-x_{1})},^ {d},\] (6)

and it can also be determined by the following fixed point equation

\[z=(z,):=(x_{1}-)z+ x_{1}.\] (7)

Note that \(g()\) has very large derivative when \(x_{1}\) is near \(1\). It can be regarded as a continuous version of the common indicator function of the first entry \(_{x_{1}=1}()\). The separation is presented as follows.

**Theorem 2**.: _Let \(g()\) be defined as in Eq.(6) for \(^{d}\) and \(>0\)._

* _For any function_ \(N_{}}(x)\) _implemented by an FNN with depth_ \(L\) _and width_ \(k\) _where_ \(L C\) _and_ \(k 2^{}\) _for some constant_ \(C=O(1)\)_. If_ \[\|N_{}}()-g()\|_{L^{}(^{d})} ,\] _then there exists a weight parameter_ \(W_{ij}\) _of the FNN for_ \(1 i L\) _and_ \(1 j k\)_, such that_ \[|W_{ij}| 2^{}.\]
* _There exists a function_ \(N_{}}\) _implemented by a DEQ with width bounded by_ \(5^{-1}\) _and weights bounded_ \(2^{-1}\)_, such that_ \[\|N_{}}()-g()\|_{L^{}(^{d})} .\]

**Remark 2**.: _The inapproximability result of FNN in Theorem 2 is stated from the perspective of weight magnitude, which holds practical significance. Exponentially large weight often results in exponential iterations of optimization algorithms in learning with this model, as also noted in . Additionally, neural networks in practice typically have small weights due to techniques such as (standard) small initialization, normalization, and gradient clipping._

The proof is shown in Appendix A.2. In Theorem 2, the inapproximability of FNNs is relatively simple: Direct calculation shows that the derivative of the target function \(g()\) is exponentially large when \(x_{1}>1-\). To approximate \(g()\) in \(L^{}\) norm requires FNNs to have large derivative in certain region, resulting in exponentially large weight for FNNs with bounded depth. On the other hand, the proof of the approximability of DEQs is more technical. While \(\) in Eq. (7) seems more benign, it is not clear how to construct the approximation using the implicit layer in Eq. (2) that resembles an \(1\)-layer FNN with very limited expressive power. Moreover, even if we manage to approximate \(\) in Eq. (7), it will not necessarily imply a good approximation between the fixed point of DEQ and the solution of \(z=(z,)\), i.e., the target function due to the Lipschitz constant of \(\) with respect to \(z\) being very close to \(1\) when \(x_{1}\) is around \(1\) according to Eq. (7). We provide a proof sketch of this result in Section 4.3.

Further insights and implications can be gleaned from Theorem 2. First, it suggests that DEQ may excel in approximating functions induced by fixed-point iterations. In other words, DEQ may be better suited for representing algorithms. Second, Theorem 2 implies that functions with large derivative, or high-frequency components, may be approximated more efficiently by DEQ, as the function to be approximated by the implicit layer can have much smaller derivative.

### Proof Sketch of B. in Theorem 2

As discussed in Section 4.2, we want to approximate \(\) using the implicit layer of DEQ. Due to the limited expressive power of the implicit layer, we propose an equivalent reparameterization of DEQ.

**Lemma 1**.: _Consider a revised DEQ defined as_

\[ =(\,+\,+ ),\] (8) \[ =\,,\]

_where \(^{d},^{m}, ^{q m},^{q d}\), \(^{m q}\), \(b^{q}\), \(^{p m}\) and \(\|\,\,\|_{2} 1\). Then any revised DEQ can be represented by a vanilla DEQ defined as in Eq. (2) with width \(q\)._

Lemma 1 enables us to approximate \((z,)\) using the revised implicit layer, denoted by \((z,)\). Then the crux of the proof centered in bounding the error between the equilibria of two fixed-point equations. To begin, for every \(\) we denote \((z)=z-(z,)\), \((z)=z-(z,)\) and consider \(|^{ 2}(z)-^{ 2}(z)|\). Suppose that \((z)\) is \(L_{}\)-Lipschitz, then we have

\[|^{ 2}(z)-^{ 2}(z)||^{ 2}(z)- (z)|+|(z)-^{ 2}(z)|(L_{}+1)| (z)-(z)|.\]

Thus if \(L_{}<1\), by recursion, we can bound distance between the infinitely composition of \((z)\) and \((z)\), from which the error of the two fixed points can be bounded.

**Lemma 2**.: _Let \(\) be a compact set, and \(u(z,),v(z,):^{d}\) be two functions. Assume that for all \(^{d}\), \(u(,)\) and \(v(,)\) are Lipschitz continuous with Lipschitz constant \(L_{u},L_{v}<1\), respectively. Then for any \(^{d}\), it holds that_

\[|z_{u}()-z_{v}()|\{(1-L_{u})^{-1},(1-L_{v})^{-1} \}_{z}|u(z,)-v(z,)|,\]

_where \(z_{u}()\) and \(z_{v}()\) are the fixed point of \(z=u(z,)\) and \(z=v(z,)\), respectively._

In our case, \(u(z,)\) and \(v(z,)\) in this Lemma represent \((z,)\) and \((z,)\), respectively. When \(x_{1}<1-(d)^{-1}\), by calculating \((z,)}{ z}\), we have \((1-L_{})^{-1}<(d)\). Leveraging this and Lemma 2, we just need \(\|-\|_{}(d)^{-1}\) to achieve a final accuracy of \(O()\). However, when \(x 1-\), we only have \((1-L_{})^{-1}<((d))\), which may necessitate an exponential width for the implicit layer to achieve \(O()\) accuracy. In fact, \((z,)=x_{i}z\) gives an example that even assuming \(\|-\|_{}((d))^{-1}\) is not sufficient to achieve \(O()\) accuracy since \(z_{}(1)-z_{}(1)=\). So it seems difficult to bound the error without a specific structure of \(\). To overcome the issue, we observe a _novel_ property that enables us to effectively bound the error.

**Lemma 3**.: _Let \(>0\). Under the conditions in Lemma 2, if for any interval \(T\) with \((T)>\), \(u(z,)=v(z,)\) has a zero in \(T\) for all \(\), then it holds that_

\[|z_{u}()-z_{v}()|,\,[0,1 ]^{d}.\]

The intuition behind Lemma 3 is that if for any \(\), \(z-u(z,)\) and \(z-v(z,)\) as two monotone univariate functions w.r.t. \(z\) can take the same value at frequent intervals, then their zeros will also be close to each other. By using this Lemma, it suffices to construct such \((z,)\) that equals \((z,)\) at frequent interval of length \(O()\) for every \(\).

The Bias on Learning Dynamics of DEQ

In this section, we study the implicit bias of a simplified linear diagonal DEQ and present a concrete example illustrating how such an implicit bias may improve generalization. Specifically, we focus on the overparameterized setting and our analysis is beyond the lazy training regime.

To ensure tractability, we follow the common techniques (e.g. see ) to reduce a matrix problem to a vector problem by considering only the diagonal elements of the weight matrix. Our primary focus is on the following model:

\[f(,)=_{i=1}^{d}}x_{i}:=,,_{i}=}.\] (9)

The model can be regarded as a diagonal linear DEQ in Eq. (2) with activation \(=\), weights \(=(w_{1},w_{2},,w_{d})\), \(=_{d}\), \(=\) and \(=(1,1,,1)^{T}^{d}\). Although simplified, this model is essentially a nonlinear model and it retains the implicit nature of DEQ.

Our primary focus lies in minimizing the expected square loss:

\[_{}L():=_{(,y) }[(y-f(,))^{2}].\] (10)

We are given access to a set of i.i.d. training examples \(\{(_{i},y_{i})\}_{i=1}^{N}\), and we denote the (half) square loss on these examples by \(()=_{i=1}^{N}(y_{i}-f(,x_{i}))^{2}\). As mentioned above, we focus on the overparameterized setting that \(N d\). Moreover, let

\[=(_{1},,_{N})^{T},_{}= _{}(^{T}),_{}=_ {}(^{T}),\]

where \(_{}>0\) can hold when \(N d\) and the data matrix \(\) is of full rank. We mainly consider the dynamics of gradient flow (GF) and gradient descent (GD) with fixed stepsize \(\) on minimizing \(()\), expressed as follows

\[}(t)=-_{}((t) );^{k+1}=^{k}-_{} (^{k}).\] (11)

The main theorem below gives a general characterization of the bias of diagonal linear DEQ in the overparameterized regime. The proof is based on the technique proposed in .

**Theorem 3**.: _Let \(_{i}\) in Eq. (9) be initialed as \(_{i}(0)>0\) for all \(i\). Suppose that gradient flow for the parameterization problem in Eq. (10) converges to some \(}\) satisfying \(}=\), then_

\[}=*{argmin}_{}Q(),=,\] (12)

_where \(Q()=_{i=1}^{d}q_{i}(_{i})\) and \(q_{i}(x)=}+_{i}(0)^{-3}x\)._

**Remark 3**.: _In Theorem 3, our proof shows that \((t)\) remains positive for all entries throughout the training process. Within the space where \(>0\), \(Q()\) is convex and has a unique minimum. The restriction to positive entries arises from our simplification on \(\) in Eq. (9) to be an all-one vector. To accommodate negative entries, one could assign \(-1\) to the corresponding entry. In this case, \(q_{i}(x)\) becomes \(q_{i}(x)=}-_{i}(0)^{-3}x\) with \(_{i}(0)<0\), which is convex for \(x<0\)._

All the proofs in this section are included in Appendix A.3. The theorem implies that the bias of the (simplified) DEQ significantly differs from that of conventional linear models and two-layer linear network which tends to give a minimum \(_{2}\)-norm interpolator . Specifically, the predictor \(}\) hardly admits parameters of small magnitude due to the penalty term \(_{i=1}^{d}^{2}}\). Meanwhile, the predictor can endure parameters of greater magnitude as the penalty \(q_{i}(x)\) increase almost linearly when \(x\) is large.

We then study the implicit bias from the learning dynamics of GF and GD. We show that when \(_{}>0\), under mild conditions, the convergence of both algorithms is guaranteed. Moreover, in this case, a positive lower bound of the \(_{}\) norm of the iterates can be derived, indicating that the model inclines to produce 'dense' features in learning process.

**Assumption 1**.: _Denote by \(_{0}\) the initialization of \(\) of the model in Eq. (9). There exists an optima \(}^{*}\), i.e., \(\,}^{*}=\) and a constant \(c>0\), such that_

\[\|}^{*}\|_{}-\|}^{*}-_{0}\|_{2}  c>0.\]

**Theorem 4**.: _Let \(\{(t)\}\) be the process following GF in Eq. (11) and \(\{^{k}\}\) the iterates following GD in Eq. (11). Assume that \(_{}>0\) and the initialization \((0)\) and \(^{0}\) satisfy Assumption 1 with an optima \(}^{*}\)._

* \(\{(t)\}\) _converges to an optima_ \(_{f}^{}\) _with_ \(\|_{f}^{}\|_{} c\)_. Moreover, for any_ \(t 0\)_, we have_ \(c\|(t)\|_{}\|}^{*}\|_{}+\|}^{*}-_{0}\|_{2}\)_._
* _If there exists a constant_ \(C>0\) _such that_ \(\|^{k}\|_{} C\) _for all_ \(k\)_, then_ \(\{^{k}\}\) _converges to an optima_ \(_{d}^{}\) _with_ \(\|_{d}^{}\|_{} c\)_. Moreover, for any_ \(k 0\)_, we have_ \(c\|^{k}\|_{} C\)_._

**Remark 4**.: _The assumption in Theorem 4 that \(\|^{k}\|_{}\) is uniformly bounded can be removed if we manually incorporate a constraint on \(\) and optimize the problem using projected gradient descent. In practice, certain reparameterization tricks  are proposed to ensure that \(- m1\) for some \(m>0\), thus corresponding to the aforementioned assumption._

Theorem 4 does not require the updates to stay in a small domain near the initialization, so it is beyond the lazy training regime. Importantly, the 'dense' bias observed in \(\) is not a direct consequence of our model assumption even though we only assume the diagonal elements to be nonzero. In fact, utilizing the diagonal elements can still lead to sparse features (e.g., see ). We believe that this bias in DEQs stems essentially from their network architecture. On the other hand, the current implicit bias holds for GF and GD, whereas other optimization algorithms may induce different implicit biases, which we aim to explore in future work.

Based on our results above, we now provide a concrete example to show the advantages brought by the bias of DEQ in out-of-distribution (OOD) tasks. This is motivated by the fact that diversifying spurious features can improve OOD generalization . Specifically, we focus on generalization on the unseen domain (GOTU) setting , a rather strong case of OOD generalization where part of the distribution domain is unseen at training but used at testing. As an example, we here utilize the setting in Theorem 3.11 in . Consider the sample space \(=\{-1,1\}^{d}\) and a linear boolean function \(f:\) defined as

\[f()=()+_{i=1}^{d}(\{i\})x_{i},\] (13)

where \((\{i\})=_{_{U}\{-1,1\}^{d}}[x_{i}f()]\), \(()=_{X_{U}\{-1,1\}^{d}}[f()]\) and \(_{U}\) refers to uniform sampling from \(\). In training, the \(k\)-th component of every accessible sample is fixed as \(1\), i.e., the unseen domain is \(=\{\{ 1\}^{d}:x_{k}=-1\}\). Denote by \(_{}\) the function learned on \(\). The GOTU error is the defined as the generalization completely on the unseen domain, i.e.,

\[GOTU(f,,)=_{X_{U}}[l(_{ }(X),f(X))],\]

where \(l\) is the quadratic loss function. It is shown in  that learning this function with diagonal linear network results in a GOTU error of \(4(\{k\})^{2}+O()\) for an infinitesimal \(\). On the other hand, the following proposition shows that under mild conditions, learning such function with DEQ achieves smaller GOTU error, where we consider DEQ in Eq. (9) with a bias term, i.e., \(f(,)=_{i=1}^{d}}x_{i}+b\).

**Proposition 2**.: _Let \(f()\) be defined as in Eq. (13). Assume that_

\[(\{i\})>0, 1 i d,(\{k\})>1,| ()| 2|(\{k\})|.\]

_Consider learning \(f\) using gradient flow on population loss2 on a linear diagonal DEQ with bias initialized by \(w_{i}(0)=b(0)=0\) for all \(i\) with unseen domain \(=\{\{ 1\}^{d}:x_{k}=-1\}\). Then the loss converges to \(0\), and it holds for the generalization error on the unseen that_

\[GOTU 4((\{k\})-(4+3(\{k\}))^{-} )^{2}<4(\{k\})^{2}.\]

In this setting, the function \( x_{k}\) has a higher frequency component (i.e., degree) compared to the constant function \(1\). Consequently, the inductive bias of DEQ enables the model to capture some information about the high-frequency components. We further conduct experiments to study the potential advantages of DEQ in learning high-frequency components in Appendix B.2.

## 6 Experiments

In this section, we conduct experiments on FNNs and DEQs based on our theoretical results. We first evaluate the expressivity of both networks on the functions proposed in our two separation results. Then we experiment on specific OOD tasks. Several additional experiments on audio representation and mutiscale DEQ are provided in Appendix B.2.

**Piecewise functions.** We first verify the results in Section 4.1. The target function is designed as a saw-tooth function, as defined in Lemma 4 in Appendix A.1, which can be exactly computed by a DEQ. We set the number of linear regions of the saw-tooth function to \(2^{5}\) and \(2^{10}\) and experiments on other sawtooth functions can be seen in Appendix B.1. According to Proposition 1, a DEQ with width \(5\) and \(10\) can compute the above functions.

Figure 1(a) and Figure 1(d) show that DEQ can achieve nearly zero test loss, demonstrating the saw-tooth function with \(2^{m}\) linear regions can be computed by DEQ. On the other hand, a not-so-deep and not-so-wide FNN fails to achieve test loss as low as DEQ, thus verifying the separation results between FNN and DEQ.

**Solution to quadratic optimization problem.** We then validate the ability of DEQ to approximate the solution function to certain optimization problems. We empirically demonstrate that DEQ can approximate such function better than an FNN with a similar number of parameters. We consider the objective function \(g()\) defined in Eq. (6), with the input dimension \(d\)\(10\) and \(20\), and thus \(\) in target function being \(2^{-10}\) and \(2^{-20}\). The input space is \(^{d}\) with the sampling distribution \(p()=\) for \(0<x_{1}<1-\) and \(p()=\) for \(1-<x_{1}<1\). To verify results under different network parameters, we adjust the layer number and hidden dimension of FNN and the layer width of DEQ while keeping the total number of parameters of both networks similar.

As shown in Figure 1(b) and Figure 1(e), for different network parameters and target functions, DEQ consistently achieves a lower test loss than FNN, demonstrating the superiority of DEQ to approximate and represent functions as solutions to certain optimization problems.

Figure 1: Test losses of FNN and DEQ networks with various width \(W\) and depth \(L\). (a) and (d) apply Sawtooth function I and II with \(2^{5}\) and \(2^{10}\) linear regions, respectively. (b) and (e) apply function \(g()\) defined in Eq. (5) with \(=2^{-10}\) and \(=2^{-20}\), respectively. (c) and (f) show the train loss and the GOTU error of FNN and DEQ on the boolean function \(f_{1}\), \(f_{2}\) with unseen domain given by Eq. (14) and Eq. (15).

**Out-of-Distribution tasks.** We further perform experiments on the implicit bias of DEQ to verify the advantage of DEQ on OOD tasks. We consider 2 linear boolean functions \(f:\) in the form of Eq. (13) and unseen domains \(\{ 1\}^{d}\). The first function is an example of the mean function and the second function is a part of DTFT. Experiments on other OOD functions can be found in Appendix B.1.

\[f_{1}(x)=1.25x_{0}+1.25x_{1}+1.25x_{2}++1.25x_{10}, =\{\{ 1\}^{10}:x_{2}=-1\},\] (14) \[f_{2}(x)=_{n=0}^{g}()x_{n}, =\{\{ 1\}^{10}:x_{1}=-1\}\] (15)

For each experiment, we generate all binary sequences in \(\{ 1\}^{d}\) for training. We employ AdamW optimizer with \(_{2}\) loss and a cosine annealing scheduler. We can observe in Figure1 (c) that the GOTU error of \(f_{1}\) is below the threshold of generalization error based on the Proposition 2. As shown in Figure1 (c) and Figure1 (f), the training loss converges to \(0\) and the generalization error on the unseen is bounded, which empirically demonstrates the advantage of DEQ on OOD tasks.

In Figure 2, we display the heatmap of the features of diagonal DEQ, vanilla DEQ and FNN trained on the target function with the unseen domain defined in Eq.(14). As the darker colors indicate smaller magnitude of features, we can confirm that the learned features of DEQ are indeed 'denser' than those of FNN.

## 7 Conclusions and Future Directions

In this paper, we provide two separations of DEQ and FNN and analyze the bias of DEQ through the lens of learning dynamics. Our theoretical results provably show the advantage of DEQ over FNN in specific problems and quantify certain learning properties of DEQ. Overall, we conjecture that DEQ may be advantageous in learning certain high-frequency components.

There are many directions that remain to study. First, it is expecting to study the expressivity and bias of more general DEQs. Second, extending our bias analysis under gradient methods to stochastic methods is intriguing. Additionally, it remains promising to incorporate the advantages of DEQ in commonly used networks in real applications. Besides, we also aim to explore DEQ's potential in representing algorithms for reasoning or in-context learning, as evidence suggests the learned networks perform algorithms in some of these tasks.