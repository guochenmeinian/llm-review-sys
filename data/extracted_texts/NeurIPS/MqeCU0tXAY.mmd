# CLIPCEL: Domain Generalization through CLIP via Channel rEfinement and Image-text aLignment

Xi Yu, Shinjae Yoo, Yuewei Lin

Artificial Intelligence Department, Computing and Data Science Directorate

Brookhaven National Laboratory, Upton, NY 11973

{xyu1; sjyoo; ywlin}@bnl.gov

Y. Lin is the corresponding author.

###### Abstract

Domain generalization (DG) is a fundamental yet challenging topic in machine learning. Recently, the remarkable zero-shot capabilities of the large pre-trained vision-language model (e.g., CLIP) have made it popular for various downstream tasks. However, the effectiveness of this capacity often degrades when there are shifts in data distribution during testing compared to the training data. In this paper, we propose a novel method, known as CLIPCEL, a model that utilizes Channel rEfinement and Image-text aLignment to facilitate the CLIP to the inaccessible _out-of-distribution_ test datasets that exhibit domain shifts. Specifically, we refine the feature channels in the visual domain to ensure they contain domain-invariant and class-relevant features by using a lightweight adapter. This is achieved by minimizing the inter-domain variance while maximizing the inter-class variance. In the meantime, we ensure the image-text alignment by aligning text embeddings of the class descriptions and their corresponding image embedding while further removing the domain-specific features. Moreover, our model integrates multi-scale CLIP features by utilizing a self-attention fusion module, technically implemented through one Transformer layer. Extensive experiments on five widely used benchmark datasets demonstrate that CLIPCEL outperforms the existing state-of-the-art methods. The source code is available at https://github.com/yuxi120407/CLIPCEL.

## 1 Introduction

Machine learning models inevitably face the challenge of out-of-distribution (OOD) generalization when encountering new tasks with different distributions from the training data. To mitigate this issue, extensive research has been dedicated to domain generalization (DG) , aiming to utilize knowledge from source domains to enhance the model's generalizability to the test dataset with domain shifts.

Recently, the spotlight has been on advancements in Vision-language models (VLMs), like CLIP , which are trained on web-scale image-language pairs containing a diverse range of domains and concepts from an open world, exhibit exceptional zero-shot learning and transferability to various downstream tasks . However, despite their impressive zero-shot performance, supervised fine-tuning on task-specific datasets remains essential for further improving performance on downstream tasks. However, recent works  have pointed out that fine-tuning degrades the CLIP's generalizability on the _out-of-distribution_ test datasets exhibiting domain shift. To tackle this challenge, various methodologies have been proposed. For instance, CoOp  and CoCoOp  models utilized the prompt learning, DPL  learned a lightweight prompt generator, while WiSEFT  combined the original zero-shot and fine-tuned models. More recently, CLIPood achieved state-of-the-art performance by employing the beta moving average and margin metric softmax to fine-tune the CLIP. It is noteworthy that these approaches do not explicitly guide the model to learn domain-invariant features, potentially capturing some domain-related information.

One prominent trend in Domain Generalization (DG) involves acquiring domain-invariant features across variance of source domains [28; 32; 19; 21; 9], as it has been demonstrated that feature representations are general and transferable to different domains if they remain invariant across domains . Intuitively, the domain invariant features are intrinsic to the class while remaining insensitive to the domain changes. However, as shown in Figure 1 (a), many CLIP visual feature channels exhibit unstable activations across domains (illustrated by the blue histogram), indicating a lack of domain invariance. Similarly, as shown in Figure 1 (b), many CLIP visual feature channels show insensitivity, and thus indiscriminative, to class variations. These observations prompt the question:

_Can we enhance the pre-trained model's generalizability by excluding domain-specific (sensitive) and class-irrelevant (insensitive) features?_

To answer it, we conduct a simple experiment using the pre-trained CLIP model on OfficeHome dataset. Given the original 512 CLIP visual feature channels, we select the ones with low domain variance and high class variance. We calculate the variance to different domains (\(V_{d}\)) and classes (\(V_{c}\)) for each feature channel, and then utilize a criterion \(J=V_{d}-V_{c}\) to select the top-\(Q\) (\(Q=400\)) channels with the smallest values. Assuming effective alignment of visual-language features in CLIP, we use the same \(Q\) channels for text features. During inference, we simply use the inner product of the visual and text feature vectors, akin to the approach used in CLIP zero-shot . As shown in Table 1, the simple feature channel selection improves the CLIP zero-shot generalizability.

Motivated by the above observations, we propose CLIPCEIL, a simple yet effective method aimed at promoting domain-invariant and class-relevant information within CLIP visual features from the perspective of feature channels. Specifically, we freeze the CLIP visual and text encoders and exclusively train a lightweight adapter for visual features, which fuses the multi-scale features, while minimizing the inter-domain variance and maximizing the inter-class variance. Furthermore, we establish alignment between image and text spaces by ensuring the consistency of direction among different classes in both the image and text domains. Our contributions are summarized as follows.

* We propose to adapt CLIP through Channel rEfinement and **I**mage-text a**Lignment (**CLIPCEIL**), ensuring the visual feature channels contain the domain-invariant and class-relevant information while preserving the image-text alignment.
* Our model integrates multi-scale CLIP features by using self-attention mechanism, technically implemented through one Transformer layer.
* We comprehensively evaluate our proposed method on five benchmark datasets. The results demonstrate that our method achieves state-of-the-art performance.

  
**Model** & **A** & **C** & **P** & **R** & **Avg** \\  CLIP full features & 82.7 & 68.0 & 88.3 & 90.7 & 82.4 \\ Channel-Selection & **84.9** & **68.3** & **89.4** & **91.2** & **83.5** \\   

Table 1: Comparison of channel selection (\(Q=400\)) with the CLIP zero-shot on Office Home benchmark

Figure 1: The feature channel sensitivity to domain and class shifts are quantified through employing the histogram of their standard deviations across different domains and classes. We analyze CLIP’s image embeddings using the ViT-B/16 backbone on OfficeHome  dataset. For each channel, the average outputs are computed across all samples from each domain/class, and the standard deviations are calculated on domain/class dimension.

Related Work

**Vision-Language Models (VLM).** The VLMs aim to link images and texts by embedding them into a shared space for cross-model learning [45; 12]. Recently, equipped with advanced architecture (_e.g.,_ Transformer ) and trained on huge web-scale image-text pairs, the VLMs have attracted significant attention and demonstrated superior performance on various downstream tasks like image classification, segmentation, object detection, and image-text retrieval. For instance, CLIP  pre-trained on 400M image-text pairs using contrastive loss, demonstrates outstanding zero-shot prediction capability. ALIGN , trained on 1.8B noisy image-text pairs with noise-robust contrastive learning, ImageNet , pre-trained on four tasks simultaneously, achieving superior image-text retrieval performance. SLIP  incorporates self-supervision into contrastive learning, leading to more efficient pre-training. BLIP  and BLIP-2  employ joint optimization with three objectives, achieving state-of-the-art performance on a wide range of vision-language tasks. Instead of developing a new pre-trained model, our work aims to leverage CLIP to enhance domain generalization performance.

**Domain Generalization (DG).** DG aims to train a model that generalizes well to the _out-of-distribution_ test (target) domains, solely training on source domains. One typical way is domain augmentation, which either diversifies the source domain or simulates the inaccessible test (target) domain conditions like domain randomization [25; 47; 18; 20], adversarial data augmentation [53; 64; 58] and data generation [46; 43; 57; 40; 23; 69]. Alternatively, methods focus on the learning strategies, including ensemble learning  and meta-learning . Another prevalent approach is representation learning, aiming to capture the domain-invariant representations on source domains.  extracts the invariant semantic features by jointly learning the semantic and variation encoders.  learned style-invariant representation by reducing the intrinsic style from the class categories through the style-agnostic networks.  first disentangled the latent representations in domain-specific and domain-invariant and then concatenated them to make final decisions. Similarly,  proposed the information theory inspired disentanglement and purification loss functions to explicitly disentangle the latent feature in class-relevant and class-irrelevant components. Most recently, DomainDrop  dropped domain-specific channels during training by using additional domain discriminator networks.

In recent years, research has focused on enhancing the generalization of VLMs, like CLIP. Some studies learn the task-specific prompts [68; 67; 62], while others utilize the ensemble learning  or adapter learning [14; 61]. Despite the superior performance of large pre-trained VLMs, they still struggle with out-of-distribution (OOD) generalization. Efforts have been made to enhance their generalizability, _e.g._, StyLIP  and DPL  proposed the prompt learning approach for domain generalization. VL2V-SD  improved the OOD generalization of the VLM by visual-text alignment and visual encoder distillation. More recently, approaches like inference-time fine-tuning  or fine-tuning the entire visual encoder [35; 44] have been explored to further improve model generalizability. However, the former incurs an additional computational burden during inference, while the latter faces significant computational and storage challenges, requiring a full CLIP-sized model for each task. In contrast, our proposed model, once trained, does not require additional adaptation during inference, and we only need to store a lightweight model for each task.

## 3 Methods

### Problem Setup

This paper aims to improve the _out-of-distribution_ generalization through the pre-trained VLM. Let \(^{d}\) be the image space and \(\) the class label space. A domain consists of data sampled from a joint distribution \(P_{XY}\) on \(\). In the context of domain generalization, we have \(K\) labeled training (source) domains \(\{^{k}_{s}=\{(x^{k}_{i},y^{k}_{i})\}_{i=1}^{n_{k}}\}_{k=1}^{K}\), where \(n_{k}\) is the number of samples in the \(k^{}\) domain, and each domain \(^{k}_{s}\) associated with a joint distribution \(P^{k}_{XY}\). Note that each domain has a different joint distribution: \(P^{i}_{XY} P^{j}_{XY},1 i j K\). The goal of domain generalization is to train a model \(f:\) from \(K\) training domain \(_{s}\) and achieve good generalization on an _out-of-distribution_ inaccessible test (target) domain \(_{t}=\{(x^{t}_{i},y^{t}_{i})\}_{i=1}^{n_{t}}\), where \(y^{t}\), and \(P^{test}_{XY} P^{i}_{XY}\) for \(i\{1,...,K\}\).

### Framework Overview

The overview of our framework is illustrated in Figure. 2, which consists of three primary components. The first one is the lightweight **adapter**, depicted in the orange block of Figure 2. It fuses the multi-scale CLIP visual features and maps them to a latent feature space, aiming to enhance the model's generalizability. The second component is **visual channel refinement**, which aims to ensure the visual features contain domain-invariant and class-relevant features. As observed from Figure 1, CLIP's visual features have numerous channels that exhibit sensitivity to domain variations, which are essentially domain-specific features, as well as channels that exhibit insensitivity to class variations, which are essentially class-irrelevant features. In the context of domain generalization, it is argued that both features are often redundant and may hinder the model's generalizability. Our framework aim to eliminate these undesirable features by minimizing the feature variance across domains and maximizing feature variance across classes. The third one is the **image-text alignment component**. The feature channel refinement module, working solely in the image space, has the potential to disrupt the well-aligned image-text feature space from CLIP. Therefore, realigning the image and text spaces becomes necessary. Specifically, we introduce the direction loss to minimize the difference between the direction of two image features and that of their corresponding textual features. We describe each component of our framework thoroughly in the subsequent sections.

### Adapter \(g\)

A CLIP's visual encoder consists of several vision transformer layers and a final project layer, as depicted in blue block in Figure 3. Let \(I\) denote the visual encoder within CLIP. Given an image \(\), its visual features in CLIP are represented as \(I()=[\{f_{}^{t}\}_{l=1}^{L};f_{}^{final}]\). Here, \(f_{}^{t}^{d}\) signifies the feature map derived from the [cls] token in the \(l^{}\) layer, with a dimension of \(d\), where \(L\) stands for the number of transformer layers. Additionally, \(f_{}^{final}^{D}\) represents the ultimate output of CLIP's visual encoder, obtained by passing the feature map of the last layer \(f_{}^{L}\) through an inherent MLP projector. In this paper, we use ViT-B/16 as the visual encoder backbone with the number of transformer layers \(L=12\), the feature dimensions \(d=768\) and \(D=512\).

Figure 2: An overview of the proposed framework. We fixed the CLIP visual encoder \(I\) and text encoder \(T\) and trained a lightweight adapter \(g\) during the training. The channel refinement ensures each feature channel contains domain-invariant (minimizing domain variance) and class-relevant (maximizing class variance) information. To further align the image and text, we maximize the image-text similarity and minimize direction loss with the help of text class descriptions based on data pairs from different classes and domains.

We aim to enhance the visual features' resilience to the domain shifts. Therefore, we propose a lightweight adapter \(g\) that consists of a Transformer layer  and an MLP projector, specifically utilizing the self-attention mechanism to integrate visual features from different Transformer layers in the CLIP encoder and map these features to a latent feature space that benefits the model's generalizability. Specifically, the multi-scale features \(\{f_{}^{l}\}_{l=1}^{L}\) are fed into a Transformer layer \(\), the feature obtained from each layer is treated as a token. The feature extracted from the [cls] token in the output of \(\) is considered as the fusion of multi-scale features. This fused feature is then directed into a single-layer MLP projector \(\), which maps it from dimension \(d\) to \(D\). Finally, both the output of \(\) and the CLIP final feature \(f_{}^{final}\) are fused by residual connection to obtain ultimate visual embedding \(}\). More formally, it is formulated as follows:

\[}=g_{}(I())=((\{ f_{}^{l}\}_{l=1}^{L}))+f_{}^{final},\] (1)

where \(\) represents all the learnable parameters within the adapter \(g\).

### Channel Refinement

To extract domain-invariant and class-relevant features, while eliminating those that are domain-specific and class-irrelevant, we design a channel refinement loss based on two criteria, 1) **inter-domain variance**: domain-invariant features should exhibit minimal changes across different domains, implying a smaller inter-domain variance; 2) **inter-class variance**: class-relevant features should change across different classes, while the changes are expected as large as possible to have more discriminative ability, indicating they should have larger inter-class variance.

**Inter-domain Variance**. It measures changes in a feature channel across domains. Given the \(i^{}\) input image from \(k^{}\) domain, \(_{i}^{k}\), its refined feature is \(_{_{i}}^{k}=g_{}(I(_{i}^{k}))\), and we denote its \(m^{}\) dimension as \(_{_{i}}^{k(m)}\). As shown in Figure. 4, we first put features from all the images from the same domain together, _i.e.,_ each column indicates the feature of one image. Then, we calculate the \(_{k}^{(m)}\) refers to the \(m^{}\) channel-wise average value of all the samples in the \(k^{}\) domain: \(_{k}^{(m)}=}_{i=1}^{n_{k}}_{_ {i}}^{k(m)}\), where \(n_{k}\) is the number of samples in the \(k^{}\) domain. Finally, inter-domain variance of the \(m^{}\) channels is calculated as follows:

\[V_{d}^{(m)}=_{k=1}^{K}(_{k}^{(m)}-}_{ d}^{(m)})^{2},\] (2)

where \(K\) is the number of domains, \(}_{d}^{(m)}\) represents the average output at \(m^{}\) channel across different domains.

Figure 4: Diagram of calculating the channel domain sensitivity across different domains.

Figure 3: The architecture of the adapter \(g_{}\).

**Inter-class Variance**. It measures changes in a feature channel across different classes. Similarly to inter-domain variance, we use the same way to compute the inter-class variance, formulated in Eq. 3.

\[V_{c}^{(m)}=_{=1}^{L}(_{}^{(m)}- }_{c}^{(m)})^{2},\] (3)

where \(L\) is the number of classes and \(_{}^{(m)}=}_{i=1}^{n_{}}_{ _{i}}^{^{(m)}}\) denotes the channel-wise average value of all samples from \(^{}\) category, where \(n_{}\) is the number of samples in the \(^{}\) category, and \(_{_{i}}^{^{(m)}}\) denotes the refined feature from \(i^{}\) input image in \(^{}\) category. \(}_{c}^{(m)}\) represents the average output at \(m^{}\) channel across different classes.

To ensure the image feature channels contain both domain-invariant and class-relevant information, we minimize the inter-domain variance to eliminate the domain-specific information and maximize the inter-class variance to capture more discriminative class-relevant information. Our channel refinement loss combines the above two criteria in the following way:

\[_{}=_{m=1}^{D}1+ {^{(m)}}}{^{(m)}}},\] (4)

where \(D\) refers to the number of feature channels.

### Image-Text Alignment

The adapter \(g_{}\) maps features from the CLIP's image embedding space \(\) to the refined image embedding space \(\), aiming for capturing domain-invariant and class-relevant features. However, this mapping may disturb the well-alignment between image spaces \(\) and text spaces \(\) provided by CLIP, leading to a misalignment between \(\) and \(\) spaces. Therefore, it is necessary to re-align the refined image space \(\) and text space \(\). To attain this objective, we first simply employ the standard CLIP loss formulated as follows:

\[_{}=[g_{ }(I())_{y}],y,\] (5)

where "\(\)" is inner product, \(_{y}=T(_{y})\) denotes the text embedding of a text prompt \(_{y}\) of class \(y\).

However, the standard CLIP loss only aligns image embedding with the correct text embedding on a per-sample basis but overlooks the potential relationship between samples. Thus, we propose to explore semantic structure information to strengthen the image-text alignment. Inspired by prior work [13; 11], we aim to align the pairwise directions in the image and the text spaces. To this end, we first normalize the pairwise distance in image and text space and then directly minimize their cosine similarity. For a pair training samples \(\{(_{i},y_{i}),(_{j},y_{j})\}\), the direction loss is defined as:

\[_{}=1-((I(_{i}))-g_{ }(I(_{j}))}{\|g_{}(I(_{i}))-g_{}(I( _{j}))\|}_{y_{i}}-_{y_{j}}}{ _{y_{i}}-_{y_{j}}}),\] (6)

To further remove the domain-specific information in the image space, we sample the pair data from different domains and different classes and align them with the direction of the corresponding classes in the text space. Since the language embedding of the class is naturally domain-invariant. Thus, if the output of \(g_{}(I(_{i}))\) or \(g_{}(I(_{j}))\) contains any domain-specific information, the difference between them will not align with the corresponding class text direction. Therefore, the direction loss strengthens the image-text alignment by exploiting semantic structure information as well as removing domain-specific information in the image space.

### Training and Inference

We aggregate all the losses to our overall objective defined as follows:

\[_{}=_{}+_{} +_{},\] (7)where \(\) is the parameters of trainable adapter \(g_{}\). We show the overall training procedure of the proposed CLIPCEIL method in Algorithm 1.

To incorporate prior knowledge of CLIP, during the inference stage, we ensemble the fine-tuning model's prediction and CLIP zero-shot prediction to obtain the final classification logits. The logits of sample \(_{i}\) are formulated as follows:

\[_{_{i}}=f_{_{i}}^{final} +g_{}(I(_{i})).\] (8)

where \(=(_{1},,_{C})^{}\), \(C\) is the number of classes.

## 4 Experiments

This section showcases the superiority of our method across five widely used DG benchmark datasets. Furthermore, we carry out detailed ablation studies to determine the impacts of different loss terms, the channel refinement strategies, and the architecture of adapter \(g\).

### Datasets and implementation details

We evaluate our proposed method on five standard DG benchmarks: **PACS** contains 9991 images of 7 categories from 4 domains; **VLCS** comprises 5 categories from 4 domains, 10,729 images in total; **OfficeHome** contains 15,579 images of 65 categories from 4 domains; **TerraInognita** contains 24,788 images with 10 categories from 4 domains; **DomainNet** is a more recent and the largest one among all five datasets, which contains 0.6 million images in 345 categories from 6 domains. We utilize the CLIP pre-trained model with the ViT-B/16  backbone. More results of other backbones are in Appendix C.1. We fixed the image and text encoders and solely trained adapter \(g\) during training. To avoid the influence of different template prompts, the output of the text encoder is calculated by the average of 80 template prompts from ImageNet . In all experiments, we use the open-source code DomainBed  and follow the train-validate-test split of each dataset on the DomainBed benchmark. Following the literature, we train our model with 5000 iterations on PACS, VLCS, OfficeHome, and TerraIncognia datasets and 15000 iterations on the DomainNet dataset. Our model is selected based on the source domain validation set. All experiments are conducted on the NVIDIA A100 GPUs. All the results were averaged after five runs with different random seeds. More detailed information are in Appendix A

### Main Results

We evaluate our CLIPCEIL model against the state-of-the-art (SOTA) approaches on five standard benchmark datasets. We initially compare with CLIP zero-shot, which serves as a pre-trained vision-language baseline model without any training, which outperforms state-of-the-art ResNet-50 based models, _e.g.,_ SAGM  and DomainDrop , demonstrates the superior of the pre-trained VLMs. We further compare with the standard linear probing, which learns a single-layer linear classifier upon CLIP encoder, and three SOTA VLMs based models, _i.e.,_ the mutual-information regularization based MIRO  model, the prompt learning based DPL  and StyLIP  models. To extend the comparison, we adapt three widely-used prompt learning models, _i.e.,_ CoOp , CoCoOP ,MaPLE , and one adapter-based method CLIP-Adapter , which are originally designed for few-shot learning, to the DG task using the same experimental setting on the DG benchmark. Furthermore, to ensure a fair comparison with methods that fine-tune the entire visual encoder such as CLIPood , CAR-FT , and UniDG , we train our CLIPCEIL similarly, which we term CLIPCEIL++. Note that UniDG  is an inference-time fine-tuning model, which adapts the model with additional information from the target domain.

As illustrated in Table 2, our proposed CLIPCEIL exhibits significant improvement over the CLIP Zero-Shot and achieves the best average performance on five benchmark datasets among all the compared methods. Specifically, CLIPCEIL exceeds the second-best method DPL  by \(2.3\%\) on average, CLIPCEIL++ exceeds the second-best method CLIPood  by \(0.5\%\) on average. The results prove CLIPCEIL's effectiveness in enhancing the model generalization through capturing domain-invariant and class-relevant features. More detailed break-down results are in Appendix B.

### Ablation Studies

#### 4.3.1 Effectiveness of each loss term

Firstly, we conduct the ablation study to examine the efficacy of each loss (_i.e.,_ channel refinement loss \(_{}\), and direction loss \(_{}\)) in our overall objective function. Cross-entropy loss \(_{}\) is very standard and thus we include it by default, similar to multi-scale fusion, which will be investigated in Section 4.3.3. Table 3 presents the results of different CLIPCEIL variants with the pre-trained ViT-B/16 model on the OfficeHome dataset. As shown in the table, utilizing multi-scale information alone can enhance performance compared to the CLIP Zero-Shot. Integrating \(_{}\) leads to further enhanced performance, indicating the effectiveness in channel refinement loss to capturing domain-invariant and class-relevant information. Similarly, the improved performance of adding \(_{}\) suggests that the direction loss contributes to enhancing domain-invariant features through the help of text description. As a result, combining all three components results in the best performance, showing that each loss works as an indispensable component for achieving superior generalization of the framework.

  
**Model** & **Venue** & **PACS** & **VLCS** & **OfficeHome** & **Terrainc** & **DomainNet** & **Avg** \\ 
**SAGM ** & CVPR’23 & 86.6 & 80.0 & 70.1 & 48.8 & 45.0 & 66.1 \\ DomainDrop  & ICCV’23 & 89.5 & 78.3 & 71.8 & - & 44.4 & - \\  CLIP Zero-Shot & - & 96.2 & 81.7 & 82.4 & 33.4 & 57.5 & 70.2 \\ Lin.Probing & - & 96.5 & 82.6 & 80.4 & 50.2 & 57.6 & 73.5 \\ CoOp  & IJCV’22 & 96.0 & 81.1 & 83.5 & 47.0 & 59.8 & 73.5 \\ CoCoOp  & CVPR’22 & 95.7 & 83.1 & 84.3 & 50.4 & 60.0 & 74.7 \\ CLIP-Adapter  & IJCV’24 & 96.4 & 84.3 & 82.2 & - & 59.9 & - \\ MAPLE  & CVPR’23 & 97.6 & 85.1 & 83.4 & - & 60.4 & - \\ DPL  & 2023 & 97.3 & 84.3 & 84.2 & 52.6 & 56.7 & 75.0 \\ StyLIP  & WACV’24 & **98.1** & 86.9 & 84.6 & - & 62.0 & - \\  CLIPCEIL & Ours & \(97.6 0.1\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  MIRO  & ECCV’22 & 95.6 & 82.2 & 82.5 & 54.3 & 54.0 & 73.7 \\ CLIPood  & ICML’23 & **97.3** & 85.0 & 87.0 & 60.4 & 63.5 & 78.6 \\ CAR-FT  & IJCV’24 & 96.8 & 85.5 & 85.7 & 61.9 & 62.5 & 78.5 \\ UniDG*  & arXiv’23 & 96.7 & **86.3** & 86.2 & **62.4** & 61.3 & 78.6 \\ VL2SSD  & CVPR’24 & 96.7 & 83.3 & 87.4 & 58.5 & 62.8 & 77.7 \\ CLIPCEIL++ & Ours & \(97.2 0.1\) & \(85.2 0.5\) & \(\) & \(62.0 0.5\) & \(\) & \(\) \\   

Table 2: Comparison of our proposed method with the State-of-the-art methods on the DomainBed benchmark. \(\) denotes ResNet-50 backbone; \(\) denotes frozen CLIP ViT-B/16 encoder; \(\) denotes fine-tuning the entire CLIP ViT-B/16 encoder, * denotes the two rounds inference-time fine-tuning. **Red** and \(\) indicate the best performance in each group.

  
**Model** & **A** & **C** & **P** & **R** & **Avg** \\  Zero-Shot & \(82.7\) & \(68.0\) & \(88.3\) & \(90.7\) & \(82.4\) \\  +Multi-scale & \(82.0\) & \(69.6\) & \(90.6\) & \(90.4\) & \(83.2\) \\ +Multi-scale+\(_{}\) & \(83.5\) & \(70.6\) & \(91.3\) & \(90.7\) & \(84.1\) \\ +Multi-scale+\(_{}\) & \(83.9\) & \(70.8\) & \(91.8\) & \(91.2\) & \(84.4\) \\  CLIPCEIL (Full Model) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 3: Ablation study of each loss in our objective function on OfficeHome dataset.

To further demonstrate the corporation of each loss term, we visualize the image features of the CLIP pre-trained model and our proposed CLIPCEIL on the OfficeHome dataset in Figure 5. Different colors represent different classes or domains. As illustrated in Figure 5 (a) and (b), the image features extracted by CLIPCEIL exhibit more discrimination than the CLIP pre-trained model, proving the effectiveness of CLIPCEIL in capturing the class-relevant features. Meanwhile, the image features corresponding to different domains extracted from CLIPCEIL are distributed almost equally across all classes, demonstrated in Figure 5 (d), indicating that CLIPCEIL definitely extracts domain-invariant features. In contrast, image features from the CLIP pre-trained model are located in various places across different domains, shown in Figure 5 (c), suggesting that it still contains domain-specific information. The visualization of other datasets can be found in Appendix B.2.

#### 4.3.2 The effectiveness of the two criteria in channel refinement loss

Our proposed channel refinement loss \(_{}\) is based on two criteria, namely inter-domain variance and inter-class variance. To demonstrate the effectiveness of these criteria, we conducted experiments on all five datasets. In Figure. 6, the results show that combining inter-domain variance with inter-class variance (represented by the darkest bars) results in better performance than using either criterion alone. This indicates that the two criteria can be effectively blended and both domain-invariant and class-relevant information complement each other and are essential to enhance a model's generalization ability. More detailed breakdown results are in Appendix B.3.

#### 4.3.3 Architecture of adapter \(g\)

We investigate the structure of adapter \(g\) by comparing the efficacy of multi-scale and bypass connections. As indicated in Table 4, integrating both multi-scale and bypass connections yields the most optimal performance. This can be attributed to two main factors: (1) The multi-scale approach captures a wide range of image features from both lower and higher levels, making it more generalizable than solely using the final layer output. (2) The bypass design preserves the original CLIP pre-trained knowledge and is easier to optimize. More ablation studies for different adapter architecture and integrating Multi-scale text features are in Appendix C.2, and C.3.

  
**Multi-scale** & **Bypass** & **A** & **C** & **P** & **R** & **Avg** \\  ✗ & ✗ & \(83.2\) & \(69.6\) & \(90.5\) & \(91.6\) & \(83.5\) \\ ✗ & ✓ & \(84.0\) & \(70.2\) & \(91.0\) & \(91.8\) & \(84.3\) \\ ✓ & ✗ & \(83.8\) & \(70.5\) & \(91.7\) & \(92.0\) & \(84.6\) \\ ✓ & ✓ & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 4: Ablation study of different adapter architectures.

Figure 5: t-SNE  visualization on image features of CLIPCEIL and CLIP pre-trained models across different classes and domains. Different colors indicate different classes or domains

Figure 6: The average accuracy bar of the different channel refinement strategies.

Discussion: Potential Data Leakage in CLIP on DomainBed Benchmarks

This section discusses the possibility of data leakage when fine-tuning the pre-trained CLIP model on DomainBed benchmarks. A primary concern is whether the DomainBed datasets truly represent out-of-distribution (OOD) data for CLIP, given its extensive pretraining on 400 million image-text pairs. We argue that the data distributions differ significantly: DomainBed datasets, such as DomainNet, display distinct characteristics like imbalance and long-tailed distributions, in contrast to the balanced nature of CLIP's pretraining dataset . Furthermore, CLIP's zero-shot performance on benchmarks like Terralnocgnita and DomainNet highlights that certain domains (e.g., Infograph and Quickdraw in DomainNet, and camera-trap images in Terralnocgnita) remain underrepresented in the CLIP pretraining corpus. These observations suggest that the distribution, style, and specific content of CLIP's pretraining data diverge meaningfully from those in DomainBed, potentially mitigating concerns about data overlap and preserving the intended OOD nature of DomainBed benchmarks.

## 6 Conclusion

In this paper, we introduced the CLIPCEIL model to enhance the generalizability of the pre-trained CLIP model to the test datasets undergoing domain shifts. Specifically, we proposed a lightweight adapter for the refinement of visual feature channels to ensure the inclusion of domain-invariant and class-relevant information, which is achieved by minimizing inter-domain variance while maximizing inter-class variance. We maintained image-text alignment by aligning image features with the text features of their corresponding textual descriptions, concurrently eliminating domain-specific features. Comprehensive experiments on five benchmark datasets illustrated that CLIPCEIL surpasses the existing state-of-the-art methods.

**Limitations.** Since calculating inter-domain variance involves multiple domains, CLIPCEIL currently only applies to multi-source domain generalization. Exploring its applicability to single-source domain generalization is deferred for future investigation.