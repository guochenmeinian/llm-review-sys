# The Price of Implicit Bias in

Adversarially Robust Generalization

 Nikolaos Tsilivis

New York University

nt2231@nyu.edu

&Natalie S. Frank

New York University

nf1066@nyu.edu

&Nathan Srebro

TTI-Chicago

nati@ttic.edu

&Julia Kempe

New York University

Meta FAIR

kempe@nyu.edu

Part of this work was done while author was with TTI-Chicago.

###### Abstract

We study the implicit bias of optimization in robust empirical risk minimization (robust ERM) and its connection with robust generalization. In classification settings under adversarial perturbations with linear models, we study what type of regularization should ideally be applied for a given perturbation set to improve (robust) generalization. We then show that the implicit bias of optimization in robust ERM can significantly affect the robustness of the model and identify two ways this can happen; either through the optimization algorithm or the architecture. We verify our predictions in simulations with synthetic data and experimentally study the importance of implicit bias in robust ERM with deep neural networks.

## 1 Introduction

Robustness is a highly desired property of any machine learning system. Since the discovery of adversarial examples in deep neural networks (Szegedy et al., 2014; Biggio et al., 2013), adversarial robustness - the ability of a model to withstand small, adversarial, perturbations of the input at test time - has received significant attention. A canonical way to obtain a robust model \(f\), parameterized by \(\), is to optimize it for robustness during training, i.e. given a set of training examples \((_{i},y_{i})_{i=1}^{m}\), optimize the empirical, worst-case, loss \(l\), where worst-case refers to a predefined threat model \(()\) which encodes our notion of proximity for the task:

\[_{}_{i=1}^{m}_{_{i}^{} (_{i})}l(f(_{i}^{};),y_{i} ).\] (1)

This method of _robust Empirical Risk Minimization_ (robust ERM aka _adversarial training_(Madry et al., 2018)) has been the workhorse in deep learning for optimizing robust models in the past few years. However, despite the outstanding performance of deep networks in "standard" classification settings, the same networks under robust ERM lag behind; progress, in terms of absolute performance, has stagnated as measured on relevant benchmarks (Croce et al., 2021) and predicted by experimental scaling laws for robustness (Debenedetti et al., 2023), and any advances mainly rely on extreme amounts of synthetic data (see, e.g., (Wang et al., 2023)). Additionally, the (robust) generalization gap of neural networks obtained with robust ERM is large and, during training, networks typically exhibit overfitting (Rice et al., 2020); (robust) test error goes up after initially going down, even though (robust) train error continues to decrease. How can we reconcile all this with the modern paradigm of deep learning, where overparameterized models interpolate their (even noisy) training data and seamlessly generalize to new inputs (Belkin, 2021)? What is different in robust ERM?

In "standard" classification, it is now understood that the optimization procedure is responsible for _capacity control_ during ERM (Neyshabur et al., 2015) and this in turn permits generalization.

We use the term capacity control to refer to the way that our algorithm imposes constraints on the hypotheses considered during learning; this can be achieved by means of either explicit (e.g. weight decay (Krogh and Hertz, 1991)) or implicit regularization (Neyshabur et al., 2015) induced by the optimization algorithm (Soudry et al., 2018; Gunasekar et al., 2018), the loss function (Gunasekar et al., 2018), the architecture (Gunasekar et al., 2018) and more. This implicit bias of optimization towards empirical risk minimizers with small capacity (some kind of "norm") is what allows them to generalize, even in the absence of explicit regularization (Zhang et al., 2017), and can, at least partially, explain why gradient descent returns well-generalizing solutions (Soudry et al., 2018).

Our contributionsIn this work, we explore the implicit bias of optimization in **robust** ERM and study carefully how it affects the **robust generalization** of a model. In order to overcome the hurdles of the bilevel optimization in the definition of robust ERM (eq. (1)), we seek to first understand the situation in linear models, where the inner minimization problem admits a closed form solution. Prior work (Yin et al., 2019; Awasthi et al., 2020) that studied generalization bounds for this class of models for \(_{p}\) norm-constrained perturbations observed that the hypothesis class (class of linear predictors) should better be constrained in its \(_{r}\) norm with \(r\) smaller than or equal to \(p^{}\), where \(p^{}\) is the dual of the perturbation norm \(p\), i.e \(+}=1\). For instance, in the case of \(_{}\) perturbations, these works postulated that searching for robust empirical risk minimizers with small \(_{1}\) norm is beneficial for robust generalization. In Section 2, we further refine these arguments and demonstrate that there are also other factors, namely the sparsity of the data and the magnitude of the perturbation, which can influence the choice of the regularizer norm \(r\). Nevertheless, in accordance with (Yin et al., 2019; Awasthi et al., 2020), we do identify cases where insisting on a suboptimal type of regularization makes generalization more difficult - much more difficult than in "standard" classification.

Figure 1: _The price of implicit bias_ in adversarially robust generalization. **Top**: An illustration of the role of geometry in robust generalization: a separator that maximizes the \(_{2}\) distance between the training points (circles) might suffer a large error for test points (stars) perturbed within \(_{}\) balls, while a separator that maximizes the \(_{}\) distance might generalize better. **Bottom**: Binary classification of Gaussian data with (_right_) or without (_left_) \(_{}\) perturbations of the input in \(^{d}\) using linear models. We plot the (robust) generalization gap, i.e., (robust) train minus (robust) test accuracy, of different learning algorithms versus the training size \(m\). In standard ERM (\(=0\)), the algorithms generalize similarly. In robust ERM, however, the implicit bias of gradient descent is hurting the robust generalization of the models, while the implicit bias of coordinate descent/gradient descent with diagonal linear networks aids it. See Section 4 for details.

This observation has significant implications for training robust models. The hidden gift of optimization that allowed generalization in the context of ERM can now become a punishment in robust ERM, if implicit bias and threat model happen to be "misaligned" with each other. We call this the _price of implicit bias_ in adversarially robust generalization and demonstrate two ways this price can appear; either by varying the optimization algorithm or the architecture. In particular, we first focus on robust ERM over the class of linear functions \(f_{}(;)=,\) with _steepest descent_ with respect to an \(_{r}\) norm, a class of algorithms which generalizes gradient descent to other geometries besides the Euclidean (Section 3.1). In the case of separable data, we prove that robust ERM with infinitesimal step size with the exponential loss asymptotically reaches a solution with minimum \(_{r}\) norm that classifies the training points robustly (Theorem 3.3). Although this result is to be expected, given that standard ERM with steepest descent also converges to a minimum norm solution (without the robustness constraint, however) (Gunasekar et al., 2018), it lets us argue that, in certain cases, gradient descent-based robust ERM will generalize poorly _despite_ the existence of better alternatives - see Figure 1 (bottom). We then turn our attention to study the role of architecture in robust ERM. In Section 3.2, we study the implicit bias of gradient descent-based robust ERM in models of the form \(f_{}(;_{+},_{-})= _{+}^{2}-_{-}^{2},,\) commonly referred to as _diagonal neural networks_(Woodworth et al., 2020). These are just reparameterized linear models \(f_{}\), and thus their expressive power does not change. Yet, as we show, robust ERM drives them to solutions with very different properties (Proposition 3.8) than those of \(f_{}\), which can generalize robustly much better - see Figure 1 (bottom).

Finally, in Section 4, we perform extensive simulations with linear models over synthetic data which illustrate the theoretical predictions and, then, investigate the importance of implicit bias in robust ERM with deep neural networks over image classification problems. In analogy to situations we encountered in linear models, we find evidence that the choice of the algorithm and the induced implicit bias affect the final robustness of the model more and more as the magnitude of the perturbation increases.

NotationLet \([m]=\{1,,m\}\). The dual norm of a vector \(\) is defined as \(\|\|_{}=_{\|\| 1}, \). The dual of an \(_{p}\) norm is the \(_{p^{*}}\) with \(+}=1\). For a function \(f:^{d}\), we use \( f()\) to denote the set of subgradients of \(f\) at \(\): \( f()=\{^{d}:f() f ()+,-\}\). We denote by \(^{2}^{d}\) the element-wise square of \(\). We defer a full discussion of prior work to App. A.

## 2 Capacity Control in Adversarially Robust Classification

We begin by studying the connection between _explicit_ regularization and _robust generalization_ error in linear models. In particular, we set out to understand how constraining the \(_{r}\) norm of a model affects its robustness with respect to \(_{p}\) norm perturbations, i.e., how \(r\) interacts with \(p\).

### Generalization Bounds for Adversarially Robust Classification

We focus on binary classification with linear models over examples \(^{d}\) and labels \(y\{ 1\}\). We denote by \(\) an unknown distribution over \(\{ 1\}\). We assume access to \(m\) pairs from \(\), \(S=\{(_{1},y_{1}),,(_{m},y_{m})\}\). Let \(_{r}\) be the class of linear hypotheses with a restricted \(_{r}\) norm:

\[_{r}=\{,:\| \|_{r}_{r}\},\] (2)

where \(_{r}>0\) is an arbitrary upper bound. We consider loss functions of the form \(l(h(),y)=l(yh())\), by explicitly overloading the notation with \(l:\). The quantity \(yh()\) is sometimes referred to as the _confidence margin_ of \(h\) on \((,y)\). We assume a threat model of \(_{p}\) balls of radius \(\) centered around the original samples and we define \(_{r}\) to be the class of functions that map samples to their worst-case loss value, i.e. \(_{r}=\{(,y)_{\|^{}- \|_{p}}l(yh(^{})):h_{r}\}\). We define the (expected) risk and empirical risk of a hypothesis with respect to the worst-case loss as:

\[_{}(h)=_{(,y)} [_{\|^{}-\|_{p}}l(yh(^{}))]_{S}(h)=_{i=1}^{m}_{\|^{}_{i}- _{i}\|_{p}}l(y_{i}h(^{}_{i}) ),\] (3)

respectively. Let us also define the robust 0-1 risk as: \(_{,01}(h)=_{(,y)} [_{\|^{}-\|_{p}}\{yh( ^{}) 0\}]\). Central to the analysis of the robust generalization error is the notion of the (empirical) Rademacher Complexity of the function class \(_{r}\):

\[}_{}(_{r})=_{}[_{g_{r}}_{i=1}^{m}_{i}g((x_{i},y_{i}))]= _{}[_{h_{r}}_{i=1}^{m} _{i}_{\|_{i}^{}-_{i}\|_{p}}l( y_{i}h(_{i}^{}))],\] (4)

where the \(_{i}\)'s are Rademacher random variables. If, additionally, we consider decreasing, Lipschitz, losses \(l()\), then, as observed by Yin et al. (2019), Awasthi et al. (2020), we can equivalently analyse the following Rademacher Complexity \(}_{}(}_{r})=_{ }[_{h_{r}}_{i=1}^{m}_{i} _{\|_{i}^{}-_{i}\|_{p}}y_{i}h( _{i}^{})]\), and by taking the loss in \(_{}(),_{S}()\) in eq. (3) to be the _ramp loss_: \(l(u)=(1,(0,1-)),\;>0\), we arrive at the following margin-based generalization bound.

**Theorem 2.1**.: _(_Mohri et al._,_ 2012; Awasthi et al._,_ 2020_)_ _Fix \(>0\). For any \(>0\), with probability at least \(1-\) over the draw of the dataset \(S\), for all \(h_{r}\) with \(_{r}\) defined as in eq. (2), it holds:_

\[_{}(h)_{S}(h)+\,}_{}(}_{r})+3}.\] (5)

Margin bounds of this kind are attractive, since they promise that, if the empirical margin risk \(_{S}\) is small for a large \(\) then the second term in the RHS will shrink, and expected and empirical risk will be close. As shown in (Awasthi et al., 2020), the above Rademacher complexity admits an upper bound (and a matching lower bound) of the form:

\[}_{}(}_{r})}_{}(_{r})+_{r}}{2 }(d^{}-},1),\] (6)

where \(}_{}(_{r})\) is the "standard" Rademacher complexity. As pointed out in (Awasthi et al., 2020), there is a dimension dependence appearing in this bound, that is not present in the "standard" case of \(=0\), and, thus, it makes sense to choose \(r\) so that we eliminate that term. One such choice is of course \(r=p^{}\), the dual of \(p\). This made the works of Yin et al. (2019), Awasthi et al. (2020) to advocate for an \(_{p^{}}\) regularization during training, in order to minimize the complexity term and, hence, the robust generalization error. However, the factor \(_{r}\) that appears in the RHS of eq. (6) might also depend on \(r\) (and potentially \(d\)) so it is not entirely clear what the optimal choice of \(r\) is for a problem at hand.

### Optimal Regularization Depends on Sparsity of Data

To illustrate the previous point, we place ourselves in the realizable setting, where there exists a linear "teacher" which labels the samples _robustly_. That is, there is a vector \(^{}^{d}\) which labels points and their neighbors with the same label: \(y=(^{},^{})\) for all \(^{}\{^{d}:\|- \|_{p}\}\). Let us specialize to hypothesis classes with bounded \(_{1}\) or \(_{2}\) norm, i.e. \(_{1},_{2}\). Since the data are assumed to be labeled by a robust "teacher", the robust empirical risk that corresponds to the ramp loss can be driven to zero with a sufficiently large hypothesis class. The next Proposition provides a bound on the robust generalization of predictors who belong to such a class.

**Proposition 2.2**.: _(Generalization bound for robust interpolators) Consider a distribution \(\) over \(^{d}\{ 1\}\) with \(_{(,y)}[y=( ^{},),\;^{}:\| ^{}-\|_{p}]=1\) for some \(^{}^{d}\). Let \(S^{m}\) be a draw of a random dataset \(S=\{(_{1},y_{1}),,(_{m},y_{m})\}\) and let \(_{r}^{}=\{, :\|\|_{r}\|^{}\|_{r}\;\; _{\|\|_{r} 1}_{i[m]}_{\|_{i}^{}- \|_{p}}y_{i},_{i}^{ }\}\) be a hypothesis class of maximizers of the robust margin. Then, for any \(>0\), with probability at least \(1-\) over the draw of the random dataset \(S\), for all \(h_{r}^{}\), it holds:_

\[_{,01}(h)}(_ {i}\|_{i}\|_{}\|^{}\|_{1}+ \|^{}\|_{1})+3},\;r=1\\ }(_{i}\|_{i}\|_{2}\|^{}\|_ {2}+\|^{}\|_{2}d^{(-,0 )})+3},\;r=2.\] (7)

The proof appears in Appendix B and follows from standard arguments based on the properties of the ramp loss and standard Rademacher complexity bounds. Notice that eq. (7) depends on the various norms of \(^{}\) and \(\), so we can consider specific cases in order to probe its behaviour in different regimes. In particular, we assume that all the entries of the vectors are normalized to be. We call a vector "dense" when it satisfies and, while we call it "\(k\)-sparse" if and for. Let us also specialize to. We enumerate the cases:

1. _Dense, Dense_: If both the ground truth vector and the samples (with probability 1) are dense, then the bounds evaluate to and for and, respectively. In particular, for, the bound is smaller only by a logarithmic factor, and as increases the bounds should behave the same. So, we expect an regularization to yield smaller generalization error for, while for larger,, and regularization should perform roughly similarly.
2. _-Sparse, Dense_: If the ground truth vector is -sparse and the samples are dense, then the bounds yield and for, respectively. For, regularization is expected to generalize better than already for. As increases, regularized solutions should continue generalizing worse, as the "worst-case" dimension-dependent term makes its appearance.
3. _Dense, -Sparse_: If is dense and the samples are -sparse, then we get and for. The bounds provides more favorable guarantees in this case, even for.
4. _-Sparse_: If both and are -sparse, then we have and for and, respectively. For, and, respectively. For, and, regularization should behave similarly, but, as increases, regularization starts "paying" the "worst-case" dimension-dependent term, making the solution more appealing.

Notice how the "price" of robustness especially manifests itself in Case 4, where our input is "embedded" in a -dimensional space: the bounds are very similar for, but as soon as becomes positive, the extra penalty of solutions over grows with dimension. Moreover, Case 3 highlights that an regularization is not always optimal for perturbations. To summarize, we see that the optimal choice of regularization depends not only on the choice of norm and the value of, but also on the sparsity of the data-generating process (see also Table 1 for a summary). In particular, in order for the dimension-dependent term to appear in the bound, the model itself needs to be sparse.

## 3 Implicit Biases in Robust ERM

In the previous section, we saw that the way we choose to constrain our hypothesis class can significantly affect the robust generalization error. In this section, we connect this with the implicit bias of optimization during robust ERM and demonstrate cases where the implicit regularization is either working in favor of robust generalization or against it. The term implicit bias refers to the tendency of optimization methods to infuse their solutions with properties that were not explicitly "encoded" in the loss function. It usually describes the asymptotic behavior of the algorithm. We study two ways that an implicit bias can affect robustness in robust ERM: through the optimization algorithm and through the parameterization of the model.

### Price of Implicit Bias from the Optimization Algorithm

In this section, we study the implicit bias of robust ERM in linear models with _steepest descent_, a family of algorithms which generalizes gradient descent to other than the Euclidean geometries. We focus on minimizing the worst-case exponential loss, which has the same asymptotic properties as the logistic or cross-entropy loss (see e.g. (Telgarsky, 2013; Soudry et al., 2018; Lyu and Li, 2020)):

(8)The above corresponds to choosing \(l(u)=\) in the definition of eq. (3). We first proceed with some definitions about the margin and the separability of a dataset.

**Definition 3.1**.: We call \(_{p}\)-_margin_ of a dataset \(\{_{i},y_{i}\}_{i=1}^{m}\) the quantity \(_{ 0}_{i[m]}, _{i}}{\|\|_{p^{*}}}\).

**Definition 3.2**.: A dataset \(\{_{i},y_{i}\}_{i=1}^{m}\) is \((,p)\)_-linearly separable_ if \(_{ 0}_{i[m]}, _{i}}{\|\|_{p^{*}}}\).

Geometrically, the \(_{p}\)-margin of a dataset captures the largest possible \(_{p}\)-distance of a decision boundary to their closest data point \(_{i}\) (see Lemma C.8 for completeness). Requiring separability is a natural starting point for understanding training methods that succeed in fitting their training data and has been widely adopted in prior work (Soudry et al., 2018; Li et al., 2020; Lyu and Li, 2020)).

Steepest Descent_(Normalized)_ steepest descent is an optimization method which updates the variables with a vector which has unit norm, for some choice of norm, and aligns maximally with minus the gradient of the objective function (Boyd and Vandenberghe, 2014). Formally, the update for normalized steepest descent with respect to a norm \(\|\|\) for a loss \(L_{S}()\) is given by:

\[_{t+1}=_{t}+_{t}_{t}, { where }_{t}\] (9) \[_{t}*{argmin}_{\| \| 1}, L_{S}(_{t}).\]

Unnormalized steepest descent, or simply steepest descent, further scales the magnitude of the update by \(\| L_{S}(_{t})\|_{}\), where \(\|\|_{}\) denotes the dual norm of \(\|\|\). The case \(\|\|=\|\|_{2}\) corresponds to familiar gradient descent. We will be interested in understanding the steepest descent trajectory, when minimizing \(_{S}\) from eq. (8), in the limit of infinitesimal stepsize, i.e. steepest flow dynamics:

\[}{dt}\{^{d}: *{argmin}_{^{d}:\|\| \|\|_{}}, ,_{S}\}.\] (10)

Notice that loss \(_{S}\) is not differentiable everywhere (due to the norm in the exponent), but we can consider subgradients \(_{S}\) in our analysis. We are ready to state our result for the asymptotic behavior of steepest flow in minimizing the worst-case exponential loss.

**Theorem 3.3**.: _For any \((,p)\)-linearly separable dataset and any initialization \(_{0}\), consider steepest flow with respect to the \(_{r}\) norm, \(r 1\), on the worst-case exponential loss \(_{S}()=_{i=1}^{m}_{\|_{i}^{ }-_{i}\|_{p}}(-y_{i} ,_{i}^{})\). Then, the iterates \(_{t}\) satisfy:_

\[_{t}_{i}_{\|_{i}^{}-_{i} \|_{p}}_{t},_{i} ^{}}{\|_{t}\|_{r}}=_{ 0 }_{i}_{\|_{i}^{}-_{i}\|_{p} },_{i}^{} }{\|\|_{r}}.\] (11)

Theorem 3.3 can be seen as a generalization of the results of Gunasekar et al. (2018) to robust ERM (for any \(_{p}\) perturbation norm), modulo our continuous time analysis. The choice of analyzing continuous-time dynamics was made to avoid many technical issues related to the non-differentiability of the norm, which do not affect the asymptotic behavior of the algorithm. Li et al. (2020) studied the implicit bias of gradient descent in robust ERM, and showed that it converges to the minimum \(_{2}\) solution that classifies the training points robustly, which agrees with the special case of \(r=2\) in Theorem 3.3. For the proof, we need to lower bound the margin at all times \(t\) with a quantity that asymptotically goes to the maximum margin. This requires a _duality_ lemma that relates the (sub) gradient of the loss with the maximum margin, and generalizes previous results that only apply to either gradient descent, or the unperturbed loss, but not to both. The proof appears in Appendix C.

_Remark 3.4_.: The right hand side of eq. (11) is equivalent to:

\[_{}\|\|_{r}_{\|_{i}^{}-_{i} \|_{p}}y_{i},_{i}^{}  1,\  i[m].\] (12)

Thus, the solution converges, in direction, to the hyperplane with the smallest \(_{r}\) norm which classifies the training points correctly (and robustly). As a result, we can leverage Proposition 2.2 to reason about the robust generalization of the solution returned by steepest descent. An equivalent viewpoint of (12), first observed by Li et al. (2020) about a version of this result for gradient descent (\(r=2\)), is the following:

\[_{}\|\|_{r}+(,m)\|\|_{p^{*}} y_{i},_{i} 1,\  i[m],\] (13)for some \((,m)>0\). Thus, the problem can be understood as performing norm minimization for a norm which is a linear combination of the algorithm norm \(r\) and the dual of the perturbation norm \(p^{}\). The coefficient of the latter increases with \(\), which, hereby, means that the bias induced from the perturbation starts to dominate over the bias of the algorithm with increasing \(\).

In light of Section 2 and Proposition 2.2, we see that the implications of this result are twofold. First, on the negative side, Theorem 3.3 implies that robust ERM with gradient descent (\(_{2}\)) can harm the robust generalization error if \(p=\). For instance, as we saw in Cases 2 and 4 in Section 2.2, gradient descent will suffer dimension dependent statistical overheads. On the positive side, Theorem 3.3 supplies us with an algorithm that can achieve the desired regularization. In Cases 2 and 4 this would correspond to steepest descent with respect to \(r=1\). In general, we have the following corollary:

**Corollary 3.5**.: _Minimizing the loss of eq. (8) with steepest flow with respect to the \(_{p^{}}\) norm (on \((,p)\) separable data) convergences to a minimum \(_{p^{}}\) norm solution that classifies all the points correctly._

The notable case of steepest descent w.r.t. the \(_{1}\) norm is called _coordinate descent_. It amounts to updating at each step only the coordinate that corresponds to the largest absolute value of the gradient (Appendix D). In Section 4.1, we demonstrate how robust ERM w.r.t. \(_{}\) perturbations with coordinate descent, can enjoy much smaller robust generalization error than gradient descent.

Finally, although the perturbation magnitude \(\) did not influence the conversation so far in terms of the choice of the algorithm, it is important to note that, as \(\) increases, the max-margin solution will look similar for any choice of norm. In fact, in the limiting case of the largest possible \(\) that does not violate the separability assumption, all max-margin separators are the same - see Lemma C.7 - so the type of implicit bias will cease to be important for generalization.

### Price of Implicit Bias from Parameterization

We reasoned in the previous section that robust ERM with gradient descent over the class of linear functions of the form \(f_{}(;)=,\) can result in excessive (robust) test error for \(_{}\) perturbations. We now demonstrate how the same algorithm, but applied to a _different architecture_, can induce much more robust models. In particular, consider the following architecture:

\[f_{}(;)=_{+}^{2}- _{-}^{2},,=[_{+}, _{-}]^{2d},^{d},\] (14)

which consists of a reparameterization of \(f_{}\). In terms of expressive power, the two architectures are the same. However, optimizing them can result in very different predictors. In fact, this class of homogeneous models, known as _diagonal linear networks_, have been the subject of case studies before for understanding feature learning in deep networks, because, whilst linear in the input, they can exhibit non-trivial behaviors of feature learning (Woodworth et al., 2020). In order to study the implicit bias of robust ERM with gradient descent on \(f_{}\), we leverage a result by (Lyu and Zhu, 2022) which shows that, under certain conditions, the implicit bias of gradient flow based robust ERM for homogeneous networks, is towards solutions with small \(_{2}\) norm.

**Theorem 3.6** (Paraphrased Theorem 5 in (Lyu and Zhu, 2022)).: _Consider gradient flow minimizing a worst-case exponential loss \(_{S}()=_{i=1}^{m}_{\|_{i }-_{i}^{}\|_{p}}e^{-y_{i}f(_{i}^{ };)}\), for a homogeneous, locally Lipschitz, network \(f(;):^{p}\), and assume that for all times \(t>0\) and for each point \(_{i}\) the perturbation \(*{argmax}_{\|_{i}-_{i}^{}\|_{p }}e^{-y_{i}f(_{i}^{};)}\) is scale invariant and that the loss gets minimized, i.e. \(_{S}()0\). Then, \(\) converges in direction to a KKT point of the following optimization problem:_

\[_{}\|\|_{2}^{2}_{\| _{i}^{}-_{i}\|_{p}}y_{i}f(_{i }^{};) 1,\; i[m].\] (15)

As we show, \(f_{}\) satisfies the conditions of Theorem 3.6, and, thus, we get the following description of its asymptotic behavior.

**Corollary 3.7**.: _Consider gradient flow on the worst-case exponential loss \(_{S}()=_{i=1}^{m}_{\|_{i}- _{i}^{}\|_{p}}e^{-y_{i}f_{}(_{i}^{};)}\) and assume that \(_{S}() 0\). Then, \(\) converges in direction to a KKT point of the following optimization problem:_

\[_{_{+}^{d},_{-}^{d}} {2}(\|_{+}\|_{2}^{2}+\|_{-}\|_{2}^{2}) \;\;_{\|_{i}^{}-_{i}\|_{p }}y_{i}_{+}^{2}-_{-}^{2},_{i}^{ } 1,\; i[m].\] (16)However, the optimization problem of eq.16, which is over \(^{2d}\) is nothing but a disguised \(_{1}\) minimization problem, when viewed in the _prediction_ (\(^{d}\)) space.

**Proposition 3.8**.: _Problem (16) has the same optimal value as the following constrained opt. problem:_

\[_{^{d}}\|\|_{1} _{\|_{i}^{}-_{i}\|_{p}}y_{i}< ,_{i}^{}> 1,\; i[m].\] (17)

The proofs appear in Appendix C.4. These results suggest that the bias of gradient-descent based robust ERM over diagonal networks is towards minimum \(_{1}\) solutions, which as we argued in the previous section can have very different robust error compared to \(_{2}\) solutions, which are returned by gradient-descent based robust ERM over linear models. We verify this in the simulations of Section4.1.

_Remark 3.9_.: Technically, Corollary3.7 only proves convergence to a first order (KKT) point, so we cannot conclude equivalence with the minimum of the \(_{1}\) problem in eq.17. Yet, we believe that global optimality, under the condition of \((,p)\) separability, can be proven by extending the techniques of (Moroshko et al., 2020) in robust ERM.

## 4 Experiments

In this section, we explore with simulations how the implicit bias of optimization in robust ERM is affecting the (robust) generalization of the models. Appendix F contains full experimental details.

### Linear models

SetupWe compare different steepest descent methods in minimizing a worst-case loss with either linear models or diagonal neural networks on synthetic data, and study their robust generalization error. In accordance with Section2.2, we consider distributions that come from a "teacher" \(^{}\) with \(y=(<^{},>)\) that can have _sparse_ or _dense_\(,^{}\). We denote by \(k_{}\) and \(k_{X}\) the expected number of non-zero entries of the ground truth \(^{}\) and the samples \(\), respectively. We train linear models \(f_{}(;)=<,>\) with steepest descent with respect to either the \(_{1}\) (coordinate descent - CD) or the \(_{2}\) norm (gradient descent - GD), and diagonal neural networks \(f_{}(;_{+},_{+})=< _{+}^{2}-_{-}^{2},>\) with gradient descent (diag-net-GD). We consider \(_{}\) perturbations. We design the following experiment: first, we fit the training data with CD for \(=0\) and we obtain the value of the \(_{}\) margin of the dataset (denoted as \(^{}\)) at the end of training2. This supplies us with an upper bound on the value of \(\) for our robust ERM experiments, i.e. we know there exists a linear model with \(100\%\) robust train accuracy for \(\) less than or equal to this \(^{}\) margin. We then perform robust ERM with (full batch) GD/CD/diag-net-GD for various values of \(\) less than \(^{}\). We repeat the above for multiple values of dataset size \(m\) (and draws of the dataset), and aggregate the results.

ResultsWe plot the (robust) generalization gap of the three learning algorithms versus the dataset size for \((k_{},k_{X})\)=(512, 512) (_Dense, Dense_) and \((k_{},k_{X})\)=(4, 512) (_4-Sparse, Dense_) in Figures1 (bottom) and 2 (left), respectively. In each figure, we show the performance of the methods both in ERM (no perturbations during training) and robust ERM. The evaluation is w.r.t. the \(\) used in training. For both distributions, we observe a significant change in the relative performance of the methods, when we pass from ERM to robust ERM. For data with a sparse teacher (Figure2), CD and diag-net-GD already outperform GD in terms of generalization when implementing ERM, as a result of their bias towards minimum \(_{1}\) (_sparse_ solutions). However, in agreement with the bounds of Section2.2, the interval between the algorithms grows when performing robust ERM as a result of their different biases. In the case of _Dense, Dense_ data (Figure1), the effect of robust ERM is more dramatic, as the algorithms generalize similarly when implementing ERM, yet their gap between their robust generalization in robust ERM exceeds 20% in the case of few training data! Notice that the bounds in Section2.2 were less optimistic than the experiments show for the performance of CD and diag-net-GD in this case. Plots with other distributions appear in Figure5.

To get a fine-grained understanding of the interactions between the hyperparameters of the learning problem, we measure the _average difference_ of (robust) generalization gaps between GD and CD. In particular, for each different combination of sparsities (\(k_{}\), \(k_{X}\)) and perturbation \(\), we summarize curvesof the form of Figure 2 (left) into one number, by calculating: \(-2^{6}}_{2^{6}}^{2^{10}}((m)-(m) )dm\). The results are shown in Figure 2 (right). Notice that, as argued in Section 2.2, there are cases with \(>0\) where \(\) does not outperform \(\) (\(k_{}=(d)\), \(k_{}=( d)\)), because the learning problem is much more "skewed" towards dense solutions. We also observe that when \(\) goes from \(0\) to \(}{4}\) the edge of \(\) over \(\) grows. Past a certain threshold of \(\), the two methods will start to perform the same, since for \(=^{*}\) the algorithms return the same solution (Lemma C.7). See also Appendix E and Figure 6 for the average difference of "clean" generalization gaps between \(\) and \(\).

### Neural networks

Our discussion has focused so far on linear (with respect to the input) models, where a closed form solution for the worst-case loss allowed us to obtain precise answers for the connection between generalization and optimization bias in robust ERM. Such a characterization for general models is too optimistic at this point, because, even for a kernelized model \(f(;)=,()\), it is not clear how to compute the right notion of margin that arises from \(_{\|^{}-\|_{p}} ,(^{})\) without making further assumptions about \(()\). As such, it is difficult to reason that one set of optimization choices will lead to better suited implicit bias than another. We assess, however, experimentally, what effect (if any) the choice of the optimization algorithm has on the robustness of a non-linear model. To this end, we train neural networks with two optimization algorithms, gradient descent (\(\)) and sign (gradient) descent (\(\)) for various values of perturbation magnitude \(\), focusing on \(_{}\) perturbations. \(\) corresponds to steepest descent with respect to the \(_{}\) norm and is expected to obtain a minimum with very different properties than the one obtained with \(\) (Appendix D). In practice, we found it easier to train neural networks with \(\) than with any other steepest descent algorithm (besides \(\)).

Fully Connected NNsWe first focus on ReLU networks with 1 hidden layer without a bias term: \(f()=_{j=1}^{k}_{j}(_{j})\), where \((u)=(0,u)\) is applied elementwise. For this class of homogeneous networks, we expect very different implicit biases when performing (robust) ERM with \(\) versus \(\) (see Appendix D for details). In Figure 3, we plot the accuracy of models trained on random subsets of MNIST (LeCun et al., 1998) with "standard" ERM (\(=0\)) and robust ERM (\(=0.2\)). We observe that in ERM (_top_), the choice of the algorithm does not affect the generalization

Figure 2: **Left:** Binary classification of data coming from a sparse teacher \(^{*}\) and dense \(\), with (_bottom_) or without (_top_) \(_{}\) perturbations of the input in \(^{d}\) using linear models. We plot the (robust) generalization gap, i.e., (robust) train minus (robust) test accuracy, of different learning algorithms versus the training size \(m\). For robust ERM, \(\) is set to be \(\) of the largest permissible value \(^{*}\). The gap between the methods grows when we pass from ERM to robust ERM. **Right:** Average benefit of \(\) over \(\) (in terms of generalization gap) for different values of teacher sparsity \(k_{}\), data sparsity \(k_{}\) and magnitude of \(_{}\) perturbation \(\).

error much. But, for \(=0.2\) (_bottom_), SD significantly outperforms GD (4.11% mean difference over 3 random seeds), even though both algorithms reach 100% robust train accuracy. Notably, in this case, robust ERM with SD not only achieves smaller robust generalization error, but also avoids robust overfitting during training, in contrast to GD. It is plausible that robust overfitting, which gets observed during the late phase of training (Rice et al., 2020)), is due to (or attenuated by) the implicit bias of an algorithm kicking in late during robust ERM. This bias can either aid or harm the robust generalization of the model and perhaps this is why the two algorithms exhibit different behavior. It would be interesting for future work to further study this connection. See Appendix E for plots with different values of \(\) and \(m\).

Convolutional NNsDeparting from the homogeneous setting, where the implicit bias of robust ERM is known or can be "guessed", we now train convolutional neural networks (with bias terms). As a result, we do not have direct control over which biases our optimization choices will elicit, but changing the optimization algorithm should still yield biases towards minima with different properties. In Figure 3, we plot the mean difference (over 3 random seeds) between the generalization of the converged models. We see that the harder the problem is (fewer samples \(m\), request for larger robustness \(\)), the bigger the price of implicit bias becomes. Note that for this architecture it turns out that the implicit bias of GD is better "aligned" with our learning problem and GD generalizes better than SD, despite facing the opposite situation in homogeneous networks. This should not be entirely surprising, since we saw already in linear models that a reparameterization can drastically change the induced bias of the same algorithm.

## 5 Conclusion

In this work, we studied from the perspective of learning theory the issue of the large generalization gap when training robust models and identified the implicit bias of optimization as a contributing factor. Our findings seem to suggest that optimizing models for robust generalization is challenging because it is tricky to do _capacity control_ "right" in robust machine learning. The experiments of Section 4 seem to suggest searching for different first-order optimization algorithms (besides gradient descent) for robust ERM (adversarial training) as a promising avenue for future work.

Figure 3: **Left:** Comparison of two optimization algorithms, gradient descent and sign gradient descent, in ERM and robust ERM on a subset of MNIST (digits 2 vs 7) with 1 hidden layer ReLU nets. Train and test accuracy correspond to the magnitude of perturbation \(\) used during training. We observe that in robust ERM the gap between the generalization of the two algorithms increases. **Right:** Gap in (robust) test accuracy (with respect to the \(\) used in training) of CNNs trained with GD and SD (GD accuracy minus SD accuracy) on subsets of MNIST (all classes) for various of \(\) and \(m\).

Acknowledgments.NT and JK acknowledge support through the NSF under award 1922658. Supported in part by the NSF-Simons Funded Collaboration on the Mathematics of Deep Learning (https://deepfoundations.ai/), the NSF TRIPOD Institute on Data Economics Algorithms and Learning (IDEAL) and an NSF-IIS award. Part of this work was done while NT was visiting the Toyota Technological Institute of Chicago (TTIC) during the winter of 2024, and NT would like to thank everyone at TTIC for their hospitality, which enabled this work. This work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise.