# Fair Online Bilateral Trade

Francois Bachoc

IMT

Universite Paul Sabatier

Toulouse, France, 31062

Institut universitaire de France (IUF)

francois.bachoc

@math.univ-toulouse.fr

&Nicolo Cesa-Bianchi

Department of Computer Science

Universita degli Studi di Milano

DEIB

Politecnico di Milano

Milano, Italy, 20133

cesa-bianchi@di.unimi.it

&Tommaso Cesari

EECS

University of Ottawa

Ottawa, Canada, K1N 6N5

tcesari@uottawa.ca

&Roberto Colomboni

DEIB

Politecnico di Milano

Department of Computer Science

Universita degli Studi di Milano

Milano, Italy, 20133

roberto.colomboni@polimi.it

###### Abstract

In online bilateral trade, a platform posts prices to incoming pairs of buyers and sellers that have private valuations for a certain good. If the price is lower than the buyers' valuation and higher than the sellers' valuation, then a trade takes place. Previous work focused on the platform perspective, with the goal of setting prices maximizing the _gain from trade_ (the sum of sellers' and buyers' utilities). Gain from trade is, however, potentially unfair to traders, as they may receive highly uneven shares of the total utility. In this work we enforce fairness by rewarding the platform with the _fair gain from trade_, defined as the minimum between sellers' and buyers' utilities. After showing that any no-regret learning algorithm designed to maximize the sum of the utilities may fail badly with fair gain from trade, we present our main contribution: a complete characterization of the regret regimes for fair gain from trade when, after each interaction, the platform only learns whether each trader accepted the current price. Specifically, we prove the following regret bounds: \(( T)\) in the deterministic setting, \((T)\) in the stochastic setting, and \((T^{2/3})\) in the stochastic setting when sellers' and buyers' valuations are independent of each other. We conclude by providing tight regret bounds when, after each interaction, the platform is allowed to observe the true traders' valuations.

## 1 Introduction

In the online bilateral trade problem, at each round \(t=1,2,\) a seller and a buyer with private valuations for a certain good connect to a trading platform. The seller's valuation \(S_{t}\) is the smallest price at which they are willing to sell the good. Similarly, the buyer's valuation \(B_{t}\) is the highest price they would pay to get the good. The platform posts a price \(P_{t}\) to both buyer and seller. A trade happens if and only if both agents agree to trade, i.e., \(S_{t} P_{t} B_{t}\). At the end of the round, \(S_{t}\) and \(B_{t}\) remain unknown and the platform only observes \(\{S_{t} P_{t}\}\) (i.e., whether the seller accepted the deal) and \(\{P_{t} B_{t}\}\) (i.e., whether the buyer accepted the deal).

Previous works [12; 2; 9; 13; 7; 5] focused on minimizing regret with respect to the gain from trade function gft:\((p,s,b)(b-s)\{s p b\}\). The quantity gft\((P_{t},S_{t},B_{t})\) corresponds to the sumof the seller's utility \(P_{t}\) - \(S_{t}\) and the buyer's utility \(B_{t}-P_{t}\) at time \(t\) when a trade happens, and zero otherwise. This reward function, however, is oblivious to asymmetries in the utilities of the buyer and the seller caused by \(P_{t}\) not being close to the mid-price \((S_{t}+B_{t})/2\). As argued in , prices generating unequal gains may lead to a reduced participation in the market, which translates to less trading on the platform.

To address this problem, instead of using the _sum_ of the utilities (i.e., the gain from trade) as reward function, we use the _minimum_ of the utilities. This reward function ignores any surplus a trader may achieve at the expense of the other, thus encouraging the platform to set prices \(P_{t}\) as close as possible to the mid-price, equalizing the profit of sellers and buyers. We call this new reward function _fair gain from trade_ and denote it by fgft. Our specific focus on the fair gain from trade objective can be seen as an implementation of the so-called egalitarian rule in social choice theory (sometimes also called the max-min rule or the Rawlsian rule), where one favors the alternative that maximizes the minimum utility of the involved parties to promote fairness. This approach might be especially relevant for online ride-sharing services like Uber and Lyft, where fairness problems have been previously studied, although in different settings with metrics different from ours (see, e.g., ). Our goal is to design algorithms minimizing the regret over \(T\) rounds. This is the difference between the (expected) total fgft achieved by the best fixed price \(p^{*}\) and the (expected) total fgft achieved by the algorithm. Note that the two bits received as feedback at the end of each round (i.e., \(\{S_{t} P_{t}\}\) and \(\{P_{t} B_{t}\}\)) are not enough to compute bandit feedback (the reward earned by posting the price \(P_{t}\))--neither for fgft nor for fgft.1

Our contributions.If sellers' and buyers' valuations are independent and drawn i.i.d. from two fixed but unknown distributions, we obtain an efficient algorithm (Algorithm 1), achieving a regret of \(}(T^{2/3})\) after \(T\) rounds (Theorem 2). This algorithm is built around the key _Comvolution Lemma_ (Lemma 1), which shows how one can estimate the _expected_ fgft through the feedback the learner has access to. Algorithm 1 does so by building uniform estimates for the expected fgft via a discrete convolution procedure that combines the feedback collected from sellers and buyers across different time steps. We then derive a lower bound matching this rate up to a logarithmic factor (Theorem 3). The lower bound construction leverages the relationship between the feedback and the fgft to build hard instances of our problem. These hard instances are similar to the ones in the _revealing action_ problem of partial monitoring (see, e.g., ), that force the learner to perform a certain amount of costly exploration. An analogous phenomenon shows up even in the deterministic case, where \(S_{t}=s\) and \(B_{t}=b\) for all \(t\) and for some unknown constants \(s,b\). In this simpler setting, we prove that posting some clearly suboptimal but informative prices is unavoidable, showing that no strategy can obtain a regret better than \(( T)\) (Theorem 4). We complement this result by showing that this rate is matched by a double binary search algorithm (Theorem 5).

We also show that the independence of sellers' and buyers' valuations is necessary to minimize regret with respect to fair gain from trade: if the pairs \((S_{t},B_{t})\) are drawn i.i.d. from an arbitrary _joint_ distribution, then the fgft regret must grow linearly with time (Theorem 1).

Finally, we complete the picture by quantifying the cost of partial information. We do so by analyzing the regret rates at which the platform can learn if the pair \((S_{t},B_{t})\) is revealed at the end of each round--the so-called _full feedback_ model. Here, we show that a regret of \(()\) can be achieved for any _joint_ distribution of sellers' and buyers' valuations (Theorem 6), and we show that this rate is optimal up to constant factors, even if we assume that the traders' valuations are independent of each other (Theorem 7).

Our results are summarized in Table 1.

   & Deterministic & Stochastic (i.i.d. + independent valuations) & Stochastic (i.i.d) \\  Two-bit feedback & \( T\) & \(T^{2/3}\) & \(T\) \\  Full feedback & \(1\) & \(\) & \(\) \\  

Table 1: Summary of our results.

Technical challenges.When regret is minimized with respect to the _gain from trade_, the independence of sellers' and buyers' valuations is not enough to guarantee sublinear regret, and an additional _smoothness_ assumption2 is required to compensate for the lack of Lipschitzness of gft. In  the optimal regret for gft under the smoothness assumption turns out to be the same as the optimal rate \((T^{2/3})\) achievable for fgft without smoothness. This happens because the fairness condition confers Lipschitzness to the gain from trade, allowing us to compare against a broader range of distributions. In fact, the smoothness assumption plays no role in the fair version of the problem. To further compare the two settings, note that the issue appearing in , i.e., a feedback so poor that is not even sufficient to reconstruct the reward at the posted price, arises here as well. In our case, however, the specific form of the objective requires new ideas--i.e., our new Convolution Lemma (Lemma 1)--to recover usable information about the reward function. Another difference is that, in , the realized gain from trade is maximized by posting any price \(p[S,B]\). In our setting, instead, we have to address the more delicate task of locating the midpoint \(p=(S+B)/2\). This is the reason why, as we show in Section 2, even optimal algorithms for bilateral trade can suffer linear regret in our setting. From the technical viewpoint, note that a direct application of the convolution lemma in the 2-bit feedback setting would yield a suboptimal upper bound of \(T^{3/4}\). To obtain the optimal \(T^{2/3}\) rate, we carefully define a data-gathering procedure in which each observation contributes to estimating the convolution of the cdfs at all points.

Another interesting discrepancy between the two settings arises in the deterministic case, where a learner can devise a strategy whose regret is constant when the reward function is gft. This is in contrast to the \(( T)\) rate when the reward function is fgft, as we discuss at the beginning of Section 4.

Related work.Attempts to circumvent the linear lower bound for the regret of gain from trade in adversarial environments include , where they focus on \(2\)-regret, and , where they consider a global budget balance condition that allows the learner to subsidize trades with money accumulated in previous rounds. Recently, online brokerage, a related though incomparable setting where traders can sell or buy depending on the contingent market conditions, has been studied in .

Fairness is an intensively studied topic in online learning, with the goal of understanding the extent to which the fairness constraints impact on the regret. The work  considers online prediction with expert advice and studies the problem of combining individually non-discriminatory experts while preserving non-discrimination. An early investigation of fairness in linear bandits is conducted in , where the fairness constraints demand that similar action be assigned approximately equal probabilities of being pulled, and the similarity metric must be learned via fairness violation feedback. Fairness in linear bandits is also investigated in , where the reward observed by the learner is biased towards a specific group of actions. In , the authors study an online binary classification problem with one-sided feedback where the fairness constraint requires the false positive rate to be equal across two groups of incoming users. The paper  applies Blackwell's approachability theory to investigate online learning under group fairness constraints. A different notion of fairness in bandits is considered by , where each arm has to be pulled at least a pre-specified fraction of times (see also  for related results). This type of fairness requirement is also considered in  where the optimal policy is pulling an arm with a probability proportional to its merit (a problem-specific function of the arm's expected reward). Finally, the work  considers a \(K\)-armed bandit setting in which pulling an arm yields different rewards for different agents. The algorithm's goal is to control regret against the optimal Nash Social Welfare (NSW). The NSW of a probability assignment \(\) over the arms is the product of the agents' reward in expectation according to \(\).

Online learning with fairness constraints is also investigated in online fair division , where each good in a sequence must be allocated to a set of agents who receive some utility that depends on the good. The goal is to satisfy a given fairness criterion, which is typically not aligned with the maximization of the agents' utilities. Traditionally, this problem has been studied under the assumption that at the beginning of each round agents report their true utilities . Very recently, the problem was studied in a bandit setting, where only the stochastic utility of the agent receiving the good is revealed . In that setting, there is a finite set of types and the expected utility of agent \(i\) for type \(j\) is fixed but unknown. The regret is then defined in terms of the geometric mean of the total expected utilities of the agents. Our regret, instead, is defined additively over the minimum of the agents' utilities in each round, where the utilities of sellers and buyers depend on each other through the price posted by the platform. Our notion of regret can also be viewed as an online version of the Kalai-Smorodinsky solution to the bargaining problem, in the sense that we also strive to equalize the utilities of the two players . The regret typically studied in online fair division, instead, corresponds to the Nash solution to the bargaining problem.

Finally, we mention that the line of research where online learning is applied to trading problems, was initiated in one-sided settings, like dynamic pricing, whose seminal paper was . For an interesting comparison of (a generalization of) the dynamic pricing setting and the bilateral trade setting, see also .

**Formal problem definition.** We study the following problem. At each time \(t\),

1. A seller and a buyer arrive with private valuations \(S_{t}\) and \(B_{t}\)
2. The platform proposes a trading price \(P_{t}\)
3. If \(S_{t} P_{t} B_{t}\), then the buyer gets the object and pays \(P_{t}\) to the seller
4. \(\{S_{t} P_{t}\}\) and \(\{P_{t} B_{t}\}\) are revealed

The boundedness assumption for valuations and prices is standard in regret minimization settings. We enforce fairness by rewarding the platform with the minimum of the utilities of sellers and buyers. More precisely, for any \(p\) and any \(s,b\), we define the _fair gain from trade_ achieved with \(p\) when the seller's valuation is \(s\) and the buyer's valuation is \(b\) by

\[(p,s,b)\{(p-s)_{+},(b-p)_{+}\},\]

where \(x_{+}\{x,0\}\) for any \(x\).

We assume a stochastic model where the sequence \((S_{t},B_{t})_{t}\) of sellers' and buyers' valuations is an i.i.d. process with a fixed but unknown distribution. The regret after \(T\) rounds of an algorithm posting prices \(P_{1},P_{2},\) is defined by

\[R_{T}_{0 p 1}[_{t=1}^{T}(p,S_{t},B_{t})]-[_{t=1}^{T}(P_{t},S_{t},B _{t})]\,.\]

In the deterministic setting (which can be viewed as a special case of the above stochastic setting) there exist \(s,b\) such that, for every \(t\), it holds that \(S_{t}=s\) and \(B_{t}=b\). In both cases, note that the maximum exists because the expected fgft is a \(1\)-Lipschitz function of the price.

## 2 Maximization of gain from trade does not imply fairness

In this section, we show that, in general, a no-regret algorithm for gft fails to achieve no-regret guarantees for fgft.

Consider the stochastic setting, where the sequence \((S_{t},B_{t})_{t}\) is an i.i.d. process. Pick \(0<h<1/2\) and, for all \(t\), let \(S_{t}\) be such that \(S_{t}=0\) with probability \(1/2\) and \(S_{t}=1-h\) with probability \(1/2\). Let also \(B_{t}=1\) for all \(t\). Then, the only prices maximizing gft are those in the interval \([1-h,1]\). Now, any price \(p[1-h,1]\) achieves an expected fgft of

\[(p,S_{t},B_{t})=(1-p)+ (1-p),p-(1-h)}\]

and the maximum of this quantity is \(h/2\), which is attained, for example, by posting \(p=1-h\). On the other hand, it is easy to see that the maximum of the expected fgft on the whole interval \(\) is \(1/4>h/2\), achieved by posting \(p=1/2[1-h,1]\). So, if we use any no-regret algorithm for the standard bilateral trade problem to post prices \(P_{1},P_{2},\), we suffer linear fgft regret on this instance. Even more strikingly, on these instances, for any \(t\),

\[(1-h,S_{t},B_{t})}{ (1/2,S_{t},B_{t})}=2h 0^{+}h 0^{+}\,\]which implies that there are instances where any no-regret algorithm for the standard bilateral trade problem fails even if we content ourselves with competing against a fraction of the reward earned by a no-regret algorithm for the fair bilateral trade problem.3

## 3 The stochastic case

We begin by providing a linear lower bound on the worst-case regret for the stochastic case. The idea of the proof is to leverage a _lack of observability_ phenomenon: we can devise two different distributions whose maximum expected fair gain from trade is achieved in disjoint regions--so that posting a price that is in the good region for one distribution leads to an instantaneous regret bounded away from zero for the other distribution--but the learner cannot distinguish which is the underlying distribution generating the valuations, given that these distributions are designed so that the (push-forward) distribution of the received feedback is exactly the same for both of them.

**Theorem 1**.: _In the stochastic case, for every algorithm for the fair bilateral trade problem, there exists a joint distribution under which, for an i.i.d. sequence \((S_{t},B_{t})_{t}\) of sellers and buyers, we have, for all \(T\),_

\[R_{T}\;.\]

Proof.: For any point \(x^{2}\), let \(_{x}\) be the Dirac measure centered at \(x\). Consider the two distributions \((_{(0,)}+_{ (,)}+_{(,1)})\) and \((_{(0,)}+_{ (,1)}+_{(,)})\). Suppose that the sequence of valuations \((S_{t},B_{t})_{t}\) is drawn i.i.d. from \(\) or \(\). If the underlying distribution is \(\), the optimal point is \(5/16\) and, for all \(p[1/2,1]\) and \(t\), the difference \((5/16,S_{t},B_{t})- (p,S_{t},B_{t})\) is at least \((-)=\). Analogously, if the underlying distribution is \(\), the optimal point is \(11/16\) and, for all \(p[0,1/2]\) and \(t\), \((11/16,S_{t},B_{t})- (p,S_{t},B_{t})\). This means that the only way for the learner not to suffer \((T)\) regret is to distinguish whether the underlying distribution is \(\) or \(\). But a direct verification shows that the distribution of the feedback is the same for every \(p\), regardless of whether the underlying distribution is \(\) or \(\). Hence, the learner has no means to distinguish between \(\) and \(\) and must suffer \((T)\) regret. Indeed, let \(N_{T}\) be the random number of times the platform posts a price \(P_{t}\) in \([0,]\). Then \(N_{T}\) has the same distribution under \(\) and \(\). If \(_{}[N_{T}]=_{}[N_{T}]\), then the expected regret under \(\) is at least \(=\). Conversely, If \(_{}[N_{T}]=_{}[N_{T}]\), then the expected regret under \(\) is at least \(\). 

**Remark 1**.: _Note that Theorem 1 together with Yao's Minimax Theorem immediately imply that the adversarial fair bilateral trade problem--where the goal is to obtain sublinear worst-case expected regret against the best fixed-price when the sequence of seller/buyer valuations \((S_{t},B_{t})_{t}\) is chosen by an oblivious adversary--is unlearnable._

We now show that we can achieve learnability in the stochastic case by assuming that, for each \(t\), the two valuations \(S_{t}\) and \(B_{t}\) are _independent_ of each other.

To this end, we first present the Convolution Lemma (Lemma 1), which provides a way to avoid the aforementioned lack of observability when the traders' valuations are independent of each other. This lemma plays for fgft a role analogous to the one played by the Decomposition Lemma [9, Lemma 1] for gft.

**Lemma 1** (The Convolution Lemma).: _For all \(s,b,p\),_

\[(p,s,b)=_{0}^{1}\{s p-u\}\{p+u b \}\,u\;.\] (1)

_In particular, if \(S\) and \(B\) are \(\)-valued independent random variables, for each \(p\),_

\[(p,S,B)=_{0}^{1}\{S p-u \}[p+u B]\,u\;.\] (2)Proof.: If \((p,s,b)=(b-p)_{+}\), note that

\[(b-p)_{+} =_{0}^{1}\{u(b-p)_{+}\}\,u=_{0}^{1 }\{u(p-s)_{+}\}\{u(b-p)_{+}\}\,u\] \[=_{0}^{1}\{u p-s\}\{u b-p\}\, u=_{0}^{1}\{s p-u\}\{p+u b\}\, u\.\]

The same conclusion holds with an analogous argument if \((p,s,b)=(p-s)_{+}\). The stochastic case follows immediately from Fubini's theorem and the independence of \(S\) and \(B\). 

We now explain how the previous lemma can be used to recover the observability of the expected \(\) under the independence assumption. Note that, given the feedback we have access to, we can estimate the cumulative distributions of both sellers' and buyers' valuations pointwise with arbitrary precision, and hence, _a fortiori_, we can estimate (2). However, even if this observation alone would be enough to ensure learnability, this is not the most efficient way of learning the expected fair gain from trade function. In fact, we do not really need a careful pointwise estimation of both cumulative distributions, but just of (2), which involves certain products of their translates. The crucial observation--from which Lemma 1 takes its name--is that (2) is the (incomplete) _convolution_ of the cumulative distribution of the sellers' valuations and the co-cumulative distribution of the buyers' valuations, and that we have noisy access to these functions at the points we need to estimate them, though at different time steps. This observation suggests that we can approximate the continuous (incomplete) convolution by a discrete (incomplete) convolution involving the noisy observations we collect at different time steps, e.g., by posting prices on a uniform grid. These ideas are exploited in the design of Algorithm 1 and in the proof of its regret guarantees.

``` Input:\(K\); Initialization: for each \(t\), set \(V_{t} W_{t} 0\); fortime \(t=1,2,,K\)do  Post price \(P_{t}=\) and set \(V_{t}\{S_{t} P_{t}\},W_{t}\{P_{t}  B_{t}\}\);  Let \(I*{argmax}_{i[K]}_{k=0}^{K-1}V_{i-k}W_{i+k}\); for\(t=K+1,K+2,,T\)do  Post price \(P_{t}=I/K\); ```

**Algorithm 1**Convolution Pricing (Stochastic Setting)

**Theorem 2**.: _In the stochastic case, under the additional assumption that for each \(t\) the seller's valuation \(S_{t}\) is independent of the buyer's valuation \(B_{t}\), by setting \(K T^{2/3}\), the regret suffered by Algorithm 1 is \((T^{2/3})\)._

Proof.: For each \(k[K]\), define \(q_{k}\). Note that for each \(t\) the function \(p(p,S_{t},B_{t})\) is \(1\)-Lipschitz being the expectation of (random) \(1\)-Lipschitz functions. Hence, if for each \(p\) we denote by \(k^{}(p)\) the index of the closest point to \(p\) in the grid \(\{q_{1},,q_{K}\}\), we have, for each \(t\), that,

\[(p,S_{t},B_{t})- q_{k^{}(p)},S_{t},B_{t}\.\] (3)

Let \(F\) be the common cumulative function of the random variables in the process \((S_{t})_{t}\) and let \(G\) be the common co-cumulative function of the random variables in the process \((B_{t})_{t}\), i.e., for each \(t\) and each \(p\), define \(F(p)[S p]\) and \(G(p)[p B]\). Note that \(F(u)=0\) for each \(u 0\), that \(G(u)=0\) for each \(u 1\), and that for each \(p\), the function \(u F(p-u)G(p+u)\) is non-increasing, being the product of two non-increasing functions. Hence, for each \(t\) and each \(k[K]\), by Lemma 1, we can sandwich the quantity \(_{i=0}^{K-1}F(q_{k-i})G(q_{k+i})\) as follows:

\[(q_{k},S_{t},B_{t})=_{0} ^{1}F(q_{k}-u)G(q_{k}+u)\,u=_{i=0}^{K-1}_{}^{ {i+1}{K}}F(q_{k}-u)G(q_{k}+u)\,u\] \[_{i=0}^{K-1}_{}^{}F (-)G(+)\, u=_{i=0}^{K-1}F(q_{k-i})G(q_{k+i})\] \[+_{i=0}^{K-1}F(q_{k-i+1 })G(q_{k+(i+1)})=+_{i=0}^{K-1}_{}^{}F(-)G(+ )\,u\] \[+_{i=0}^{K-1}_{}^{ {i+1}{K}}F(q_{k}-u)G(q_{k}+u)\,u=+_{0}^{1}F(q_{k}-u) G(q_{k}+u)\,u\] \[=+(q_{k},S_{t},B _{t})\,.\] (4)

Now, by the independence assumption,

\[_{i=0}^{K-1}F(q_{k-i})G(q_{k+i})=_{i=0}^{K-1}[V_{k-i}][W_{k+i}]= _{i=0}^{K-1}V_{k-i}W_{k+i}\]

and, by noting that for each \(k[K]\) we have that \(_{i=0}^{K-1}V_{k-i}W_{k+i}\) is the empirical mean of \(K\)\(\{0,1\}\)-valued independent random variables, by Hoeffding's inequality and a union bound, we have that, for any \(>0\),

\[_{k[K]}_{i=0}^{K-1}V_{k-i}W_{ k+i}-_{i=0}^{K-1}F(q_{k-i})G(q_{k+i})  2K(-2^{2}K)\.\]

In particular, if we set \(_{T}]}}\), recalling that \(K= T^{2/3}\), and defining the (good) event \(_{T}_{k[K]}_{i=0 }^{K-1}V_{k-i}W_{k+i}-_{i=0}^{K-1}F(q_{k-i})G(q_ {k+i})<_{T}}\), we have

\[_{T}^{c}}\.\] (5)

Now, let \(p^{}*{argmax}_{p} (p,S_{t},B_{t})\) (whose definition is independent of \(t\), given that the process \((S_{t},B_{t})_{t}\) is i.i.d.) and, recalling the definition of \(I\) from Algorithm 1, note, for each \(t>K\), that

\[(P_{t},S_{t},B_{t})=(q_{I},S_{t},B_{t})= (q_{I},S_{t},B_{t})\,I\] \[=(q_{k},S_{t},B_{t})_{k=I} (q_{k},S_{t},B _{t})_{k=I}\] \[=_{i=0}^{K-1}F(q_{k^{}(p^{})-i} )G(q_{k^{}(p^{})+i})-[_{T}^{c} ]-_{T}-(p^{},S_{t },B_{t})-[_{T}^{c}]-_{T}-\,\]

where the third equality follows from the Freezing Lemma (see, e.g., [18, Lemma 8]), the first inequality from the sandwich inequalities in (4), the second inequality from the definition of \(_{T}\), the third inequality from the definition of \(I\), and the last inequality from the sandwich inequalities in (4) and inequality (3). Putting everything together, we can upper bound the regret as follows

\[R_{T} K+_{t=K+1}^{T}p^{},S_{t },B_{t}-P_{t},S_{t},B_{t} K+_{t=K+1}^{T} ([_{T}^{c}]+_{T}+)\.\]

Recalling that \(K= T^{2/3}\), \(_{T}=]}}\), and (5), we obtain the conclusion.

We now prove that the strategy employed by Algorithm 1 is worst-case optimal, up to logarithmic factors. At a high level, the reason why the \(T^{2/3}\) rate is optimal is that the fair bilateral trade problem contains instances that closely resemble the revealing action problem in partial monitoring , where, in order to distinguish which one of two actions is optimal, we have to play for a significant amount of time a third highly suboptimal action to gather this information. We formalize this intuition in the following theorem, whose full proof is deferred to Appendix A due to space constraints.

**Theorem 3**.: _There exists a constant \(c>0\) such that the following holds. For every algorithm for the fair bilateral trade problem and for every time horizon \(T\), there exists an i.i.d. sequence \((S_{t},B_{t})_{t[T]}\) of pairs of sellers' and buyers' valuations such that, for each \(t[T]\), \(S_{t}\) is independent of \(B_{t}\), and the algorithm suffers regret of at least \(cT^{2/3}\) on that sequence._

Proof sketch.: For each \([-1,1]\), consider the distribution \(_{}_{0}+_{1/4}\) where \(_{a}\) is the Dirac measure centered at a point \(a\). For each \([-1,1]\), let \((S_{t}^{})_{t}\) be an i.i.d. sequence whose distribution is \(_{}\). For each \(t\), let \(B_{t}=1\). Then, for each \([-1,1]\), the sequence \((S_{t}^{},B_{t})_{t}\) is i.i.d. and for each \(t\) the valuation \(S_{t}^{}\) is (obviously) independent of the (deterministic) valuation \(B_{t}\). Now, one can show that if \(>0\) the optimal price to post in order to maximize the expected fair gain from trade over the sequence \((S_{t}^{},B_{t})_{t}\) is \(1/2\) while if \(<0\) the optimal price is \(5/8\). If \(>0\) and we post prices greater or equal than \(9/16\) we suffer instantaneous regret of at least order of \(||\), while if \(<0\) and we post prices less or equal than \(9/16\) we suffer instantaneous regret of at least order of \(||\). Hence, to avoid suffering \(\{||T\}\) regret we need to distinguish the sign of \(\). Given that what we see are the two bits in the feedback, the only way to discriminate the sign of \(\) is to post prices in the region between \(0\) and \(1/4\). In this case, the feedback we see is equivalent to seeing a Bernoulli of parameter \(\), and hence, due to information-theoretic arguments, we need \((1/^{2})\) to distinguish the sign of \(\). Now, every price in the region \([0,1/4]\) suffers instantaneous regret of order \((1)\). Hence, any algorithm has to suffer \((}+||T)\) regret, which is \((T^{2/3})\) when \(||=(T^{-1/3})\). 

**Remark 2**.: _The \(T^{2/3}\) rate we achieve in Theorem 3 was also the regret rate for the standard bilateral trade problem in , where in order to achieve learnability, on top of the stochastic and independent valuations assumptions, it was also required that the valuations admitted a bounded density. In that case, the regret rate degraded multiplicatively with the upper bound on that density. Instead, we manage to obtain this rate without the extra bounded density assumption and our bound does not explode if the bounded density constant diverges. The reason is that, differently from the discontinuous gain from trade function, the fair gain from trade is \(1\)-Lipschitz._

## 4 The deterministic case

In this section, we study the deterministic case where there exist fixed but unknown constants \(s,b\) such that, for all time \(t\), \(S_{t}=s\) and \(B_{t}=b\). In this case, we note that if \(b<s\) no trade can occur while, if \(b=s\), even though a trade can occur for the posted price \(P_{t}=b=s\), no gain from trade or fair gain from trade can be obtained from it. Consequently, we focus on the only interesting case \(s<b\).

We begin by the following remark: in the standard bilateral trade setting where the reward function is the gain from trade \(p(b-s)\{s p b\}\), we can devise an algorithm that achieves _constant_ regret. In fact, the learner can post prices following a binary search (starting by posting the price \(1/2\)), move to the next dyadic point to the left (resp., right) if both valuations were lower (resp., higher) than the proposed price, while keep playing the same price as soon as a successful (dyadic) price \(p\) is proposed, i.e., a price \(p[s,b]\). This way, if the learner fails \(n\) times before the first success, an upper bound on the cumulative regret suffered in the "failure" phase is \(n 2^{-n} 1\), and from the subsequent rounds the instantaneous regret is always zero.

In contrast, the deterministic fair bilateral trade problem is still sufficiently layered that a costly exploration phase is unavoidable to achieve learnability given the intertwined relationship between the reward function and the feedback. This is also in contrast to what happens in the stochastic setting, where the standard bilateral trade problem required the extra bounded density assumption to achieve learnability (and hence, in this sense, was harder) than the fair bilateral trade problem.

**Theorem 4**.: _In the deterministic case, for any horizon \(T 17\), any algorithm suffers a worst-case regret larger than or equal to \(_{2}(T)\)._

For the full proof of this result, see Appendix B.

Proof sketch.: Since the setting is deterministic, we can restrict the proof to deterministic algorithms without loss of generality. Then, the key property we leverage is that for any \(k\), if a deterministic algorithm posts at most \(k\) prices in \([0,]\), then there is an interval \(E_{k}[0,]\) of length \((2^{-k})\) such that, for each \(s,s^{} E_{k}\), the algorithm receives the same feedback from the environments defined by seller/buyer pairs \((s,1)\) and \((s^{},1)\), therefore selecting the same prices.

Now, fix a deterministic algorithm \(\). There exists \(k^{}\{0,1,,T\}\) such that, for all \(s E_{k^{}}\), the algorithm posts exactly \(k^{}\) prices in \([0,]\). A direct verification shows that, for any \(s E_{k^{}}\), the instantaneous regret paid for the time steps where \(P_{t}[0,]\) is \((1)\). Therefore, the regret paid by the algorithm for playing \(k^{}\) times in \([0,]\) is \((k^{})\). Moreover, let \(\) be the set of time steps \(t[T]\) where the algorithm posts prices \(P_{t}(,1]\). Leveraging the fact that the algorithm posts the same prices for every \(s E_{k^{}}\), it can be proved that there exists an \(s E_{k^{}}\) such that the average distance \(|}_{t}|P_{t}-}{2}|\) of the points \(P_{t}\) played at rounds \(t\) from the maximizer \(\) of the fgft is at least \((2^{-k^{}})\). Putting everything together, the worst-case regret of the algorithm is lower bounded by \(k^{}+(T-k^{})2^{-k^{}}\), which is \(( T)\) regardless of the specific value of \(k^{}\). 

A "double" binary search algorithm suffices to obtain a matching \(O( T)\) regret rate. The idea of the algorithm is very simple. First, spend \(( T)\) rounds performing a binary search for the valuation of the seller. Then, do the same for the valuation of the buyer. Finally, commit to the average of these two estimates for the remaining time steps. For completeness, we report the pseudocode of this algorithm in Appendix C (Algorithm 3). For a full proof of the following theorem, see Appendix C.

**Theorem 5**.: _In the deterministic case, the (deterministic) regret suffered by Algorithm 3 is \(O( T)\)._

Proof sketch.: The idea is first to invest a budget of order \( T\) to locate the seller's valuation with error \(O(1/T)\). Second, we proceed similarly to locate the buyer's valuation with error \(O(1/T)\). Finally, this yields a \(O(1/T)\)-precise estimate of the optimal price \((s+b)/2\), and we commit to this estimate for the remaining time steps. The regret incurred by the two first phases is of order \( T\) while the regret of the last phase is of order \(T\), and thus bounded. 

## 5 The full-feedback model

We conclude this paper by quantifying the cost of partial information by analyzing the full feedback model, where, after posting the price \(P_{t}\), the learner has access to both \(S_{t}\) and \(B_{t}\). We show that the cost is two-fold: slower rates (both in the deterministic --\(( T)\) vs \((1)\)-- and stochastic case --\((T^{2/3})\) vs \(()\)) and the need for additional assumptions (independence of buyer and seller valuations in the stochastic case).

For the full-feedback setting, we show that following the best empirical price leads to an algorithm (Algorithm 2) whose regret guarantees are optimal. We remark that Algorithm 2 needs full feedback to run, given that it needs to compute the fgft function for counterfactual prices. For the full proof of the next theorem, see Appendix D.

``` Initialization: Select \(P_{1} 1/2\); fortime\(t=1,2,\)do  Post price \(P_{t}\), and receive feedback \((S_{t},B_{t})\);  Select \(P_{t+1}*{argmax}_{[0,1]} _{s=1}^{t}(p,S_{t},B_{t})\) ```

**Algorithm 2**Follow the Best Empirical Price

**Theorem 6**.: _In the stochastic full-feedback case, the regret suffered by Algorithm 2 is \(O()\). In the deterministic setting, the (deterministic) regret of Algorithm 2 is upper bounded by \(1/2\)._Proof sketch.: The deterministic case is immediate: by posting \(1/2\), the regret in the first round is at most \(1/2\), and the algorithm pays no regret in the subsequent rounds. For the stochastic case, due to Lemma 1, note that \(_{i=1}^{t}(p,S_{t},B_{t})=_{0}^{1} _{i=1}^{t}[S_{i} p-u)\{p+u B_{i}\}\,u _{t}(p)\). Now, by the two-dimensional DKW inequality, it can be proved that the empirical estimates \(_{t}(p)\) are uniformly \(\)-close (in \(p\)) to \((p,S_{t+1},B_{t+1})\) with probability \(1-Oe^{-(^{2}t)}\). These probability estimates, together with the fact that the algorithm selects \(P_{t+1}\) by maximizing \(p_{t}(p)\), translates to a \(O(1/)\)-control over the expectation \(_{0 p 1}(p,S_{t+1},B_{t+1})- (P_{t+1},S_{t+1},B_{t+1})\). The conclusion follows by summing over time steps. 

We now show that the guarantees provided by Algorithm 2 are worst-case optimal, even if sellers' and buyers' valuations are required to be independent of each other.

**Theorem 7**.: _There exists a constant \(c>0\) such that the following holds. For every algorithm for the fair bilateral trade problem with full feedback and for every time horizon \(T\), there exists an i.i.d. sequence \((S_{t},B_{t})_{t[T]}\) of pairs of sellers' and buyers' valuations such that, for each \(t[T]\), \(S_{t}\) is independent of \(B_{t}\), and the algorithm suffers regret at least \(c\) on that sequence._

Since the proof of this result follows along the same lines as the proof of Theorem 3 (whose full details can be found in Appendix A), we present only a proof sketch.

Proof sketch.: The hard instances are the same as those in the proof of Theorem 3 for the \((T^{2/3})\) lower bound in the stochastic i.i.d. case with independent sellers' and buyers' valuations. We refer to the proof sketch of Theorem 3 for the relevant notation and observations. We recall that in those instances to avoid suffering \(||T\) regret we need to distinguish the sign of \(\). Now, given that full-feedback is available, the information we retrieve after each interaction is equivalent to observing a Bernoulli r.v. of parameter \(\), regardless of the price we posted. Again, information-theoretic arguments imply that we need \(}\) rounds before being able to distinguish the sign of \(\). During these rounds, the best we can do is to play (essentially) at random in the candidate set of optimal prices \(\{1/2,5/8\}\), suffering an expected instantaneous regret of \((||)\). Overall, we suffer \((||},| |T})\) cumulative regret, which leads to the claimed \(()\) regret rate once we tune \(||=(})\). 

**Remark 3**.: _In the same spirit of Remark 1, Theorem 7 together with Yao's Minimax Theorem immediately imply that any full-feedback algorithm to solve the adversarial fair bilateral trade problem has to suffer regret of at least \(()\). A nearly matching (up to logarithmic factors) upper bound for this problem can be deduced using the algorithm Hedge for \(\)-Armed Experts (see [10, Appendix A]). We leave to future research the understanding of whether the adversarial case is logarithmically harder than the stochastic case or whether a different algorithm can achieve better regret rates._

## 6 Limitations and conclusions

Our analysis is based on different assumptions on the generation of the sellers' and buyers' valuations (stochastic, stochastic and independent, deterministic, adversarial). Our results characterize (up to constant or log factors) the regret rates in each case. Hence, there are no specific _a priori_ assumptions that we need to make to prove our results.

Our results can be extended in different directions. For example, by linking the unknown valuations of sellers and buyers to contextual information visible to the platform. In practice, platforms simultaneously deal with multiple sellers and buyers, in which case the platform may have to post a set of prices and consequently operate in a multidimensional decision space. Finally, our result for the stochastic setting with independence between sellers and buyers is only tight up to logarithmic factors, so an improved analysis of the upper or lower bound is needed.