# Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement Learning

Liyuan Mao

Shanghai Jiao Tong University

&Haoran Xu

UT Austin

&Xianyuan Zhan

Tsinghua University

&Weinan Zhang

Shanghai Jiao Tong University

&Amy Zhang

UT Austin

Equal contribution; more junior authors listed earlier.Corresponding authors. Project page at https://ryanxhr.github.io/Diffusion-DICE/.

###### Abstract

One important property of DIstribution Correction Estimation (DICE) methods is that the solution is the optimal stationary distribution ratio between the optimized and data collection policy. In this work, we show that DICE-based methods can be viewed as a transformation from the behavior distribution to the optimal policy distribution. Based on this, we propose a novel approach, Diffusion-DICE, that directly performs this transformation using diffusion models. We find that the optimal policy's score function can be decomposed into two terms: the behavior policy's score function and the gradient of a guidance term which depends on the optimal distribution ratio. The first term can be obtained from a diffusion model trained on the dataset and we propose an in-sample learning objective to learn the second term. Due to the multi-modality contained in the optimal policy distribution, the transformation in Diffusion-DICE may guide towards those local-optimal modes. We thus generate a few candidate actions and carefully select from them to approach global-optimum. Different from all other diffusion-based offline RL methods, the _guide-then-select_ paradigm in Diffusion-DICE only uses in-sample actions for training and brings minimal error exploitation in the value function. We use a didatic toycase example to show how previous diffusion-based methods fail to generate optimal actions due to leveraging these errors and how Diffusion-DICE successfully avoids that. We then conduct extensive experiments on benchmark datasets to show the strong performance of Diffusion-DICE.

## 1 Introduction

We study the problem of offline reinforcement learning (RL), where the goal is to learn effective policies solely from offline data, without any additional online interactions. Offline RL is quite useful for scenarios where arbitrary exploration with untrained policies is costly or dangerous, but sufficient prior data is available, such as robotics  or industrial control . Most previous model-free offline RL methods add a pessimism term to off-policy RL algorithms , this pessimism term acts as behavior regularization to avoid extrapolation errors caused by querying the \(Q\)-function about values of potential out-of-distribution (OOD) actions produced by the policy . However, explicitly adding the regularization requires careful tuning of the regularization weight because otherwise, the policy will still output actions that are not seen in the dataset. DIstribution Correction Estimation (DICE) methods  provide an implicit way of doing so. By applying convex duality, DICE-based methods solve for the optimal stationary distribution ratio between the optimized and data collection policy in an in-sample manner without querying unseen actions .

In this paper, we extend the analysis of DICE-based methods: we show that the optimal distribution ratio in DICE-based methods can be extended to a transformation from the behavior distribution to the optimal policy distribution. This different view motivates the use of deep generative models, e.g. diffusion models [42; 15; 43], to first fit the behavior distribution using their strong expressivity of fitting multi-modal distributions and then directly perform this transformation during sampling. We find that the optimal policy's score function can be decomposed into two terms: the behavior policy's score function and the gradient of a guidance term which depends on the optimal distribution ratio learned by DICE. The first term can be easily obtained from the diffusion model trained on the offline dataset. While it is usually intractable to find a closed-form solution of the second term, we make a subtle mathematical transformation and show its equivalence to solving a convex optimization problem. In this manner, both of these terms can be learned by only dataset samples, providing accurate guidance towards in-distribution while high-value data points. Due to the multi-modality contained in the optimal policy distribution, the transformation may guide towards those local-optimal modes due to stochasticity. We thus generate a few candidate actions and use the value function to select the max from them to go towards the global optimum.

We term our method Diffusion-DICE, the _guide-then-select_ procedure in Diffusion-DICE differs from all previous diffusion-based offline RL methods [49; 4; 14; 32], which are either only guide-based or only select-based. Guide-based methods  use predicted values of actions generated by the diffusion behavior policy to guide toward high-return actions. Select-based methods [4; 14] bypass the need for guidance but require sampling a large number of actions from the diffusion behavior policy and using the value function to select the optimal one. All these methods need to query the value function of actions sampled from the diffusion behavior policy, which may produce potential OOD actions and bring overestimation errors in the guiding or selecting process. Our method, however, brings minimal error in the guide-step by using accurate in-sample guidance to generate in-distribution actions with high values, and by doing so, in the select step only a few candidate actions are needed to find the optimal one, which further reduces error exploitation. We use an illustrative toy case to demonstrate the error exploitation in previous methods and how Diffusion-DICE successfully alleviates that. We also verify the effectiveness of Diffusion-DICE in benchmark D4RL offline datasets . Note that Diffusion-DICE also provides a replacement for the Gaussian policy extraction part used in current DICE methods, successfully unleashing the power of DICE-based methods. Diffusion-DICE surpasses both diffusion-based and DICE-based strong baselines, reaching SOTA performance in D4RL benchmarks . We also conduct ablation experiments and validate the superiority of the guide-then-select learning procedure.

## 2 Preliminaries

We consider the RL problem presented as a Markov decision process , which is specified by a tuple \(=,,,d_{0},r,\). Here \(\) and \(\) are state and action space, \((s^{}|s,a)\) and \(d_{0}\) denote transition dynamics and initial state distribution, \(r(s,a)\) and \(\) represent reward function and discount factor, respectively. The goal of RL is to find a policy \((a|s)\) which maximizes expected return \([_{t=0}^{}^{t} r(s_{t},a_{t})]\). Another equivalent LP form of expected return is \(_{(s,a) d^{}}[r(s,a)]\), where \(d^{}(s,a):=(1-)_{t=0}^{}^{t}Pr(s_{t}=s,a_{t}=a)\) is the _normalized discounted stationary distribution_ of \(\). For simplicity, we refer to \(d^{}(s,a)\) as the stationary distribution of \(\). Offline RL considers the setting where interaction with the environment is prohibited, and one need to learn the optimal \(\) from a static dataset \(=\{s_{i},a_{i},r_{i},s^{}_{i}\}_{i=1}^{N}\). We denote the empirical behavior policy of \(\) as \(^{}\).

### Distribution Correction Estimation

DICE methods  incorporate the LP form of expected return \(()=_{(s,a) d^{}}[r(s,a)]\) with a regularizer \(D_{f}(d^{}||d^{})=_{(s,a) d^{}}[f( {d^{}(s,a)}{d^{}(s,a)})]\), where \(D_{f}\) is the \(f\)-divergence induced by a

Figure 1: Illustration of the _guide-then-select_ paradigm

convex function \(f\). More specifically, DICE methods try to find an optimal policy \(^{*}\) that satisfies:

\[^{*}=_{}_{(s,a) d^{}}[r(s,a)]- D_{f} (d^{}\|d^{}).\] (1)

This objective is generally intractable due to the dependency on \(d^{}(s,a)\), especially under the offline setting. However, by imposing the _Bellman-flow_ constraint \(_{a}d(s,a)=(1-)d_{0}(s)+_{(s^{},a^{ })}d(s^{},a^{})p(s|s^{},a^{})\) on states and applying Lagrangian duality and convex conjugate, its dual problem has the following tractable form:

\[_{V}\,(1-)_{s d_{0}}[V(s)]+_{(s,a) d ^{}}[f^{*}([V(s,a)-V(s)]/)].\] (2)

Here \(f^{*}\) is a variant of \(f\)'s convex conjugate and \(V(s,a)=r(s,a)+_{s^{} p(|s,a)}[V(s^{ })]\) represents the Bellman operator on \(V\). In practice, one often uses a prevalent semi-gradient technique in RL that estimates \(V(s,a)\) with \(Q(s,a)\) and replaces the initial state distribution \(d_{0}\) with dataset distribution \(d^{}\) to stabilize learning [41; 34]. In addition, because \(\) usually cannot cover all possible \(s^{}\) for a specific \((s,a)\), DICE methods only use a single sample of the next state \(s^{}\). The update of \(Q(s,a)\) and \(V(s)\) in DICE methods are as follows and we refer to a detailed derivation in Appendix A:

\[&_{V}\,_{(s,a) d^{}} (1-)V(s)+ f^{*}[Q(s,a)-V(s)]/) \\ &_{Q}\,_{(s,a,s^{}) d^{}} r(s,a)+ V(s^{})-Q(s,a)^{2}.\] (3)

Note that learning objectives of DICE-methods can be calculated solely with a \((s,a,s^{})\) sample from \(\), which is totally in-sample. One important property of DICE methods is that \(Q^{*}\) and \(V^{*}\) have a relationship with the optimal stationary distribution ratio \(w^{*}(s,a)\) as

\[w^{*}(s,a):=(s,a)}{d^{}(s,a)}=0,(f^{}) ^{-1}Q^{*}(s,a)-V^{*}(s),\] (4)

where \(d^{*}\) is the stationary distribution of \(^{*}\). To get \(^{*}\) from \(w^{*}\), previous policy extraction methods in DICE methods include weighted behavior cloning , information projection  or policy gradient . All these methods parametrize an unimodal Gaussian policy in order to compute \((a|s)\), which greatly limits its expressivity.

### Diffusion Models in Offline Reinforcement Learning

Diffusion models [42; 15; 43] are generative models based on a Markovian noising and denoising process. Given a random variable \(x_{0}\) and its corresponding probability distribution \(q_{0}(x_{0})\), the diffusion model defines a forward process that gradually adds Gaussian noise to the samples from \(x_{0}\) to \(x_{T}(T>0)\). Kingma et al.  shows there exists a stochastic process that has the same transition distribution \(q_{t0}(x_{t}|x_{0})\) and Song et al.  shows that under some conditions, this process has an equivalent reverse process from \(T\) to \(0\). The forward process and the equivalent reverse process can be characterized as follows, where \(}_{t}\) is a standard Wiener process in the reverse time.

\[q_{t0}(x_{t}|x_{0})=(x_{t}|_{t}x_{0},_{t}^{2})  dx_{t}=[f(t)x_{t}-g^{2}(t)_{x_{t}} q_{t}(x_{t})]dt+g(t)d {}_{t},\;x_{T} q_{T}(x_{T}).\] (5)

Here we slightly abuse the subscript \(t\) to denote the diffusion timestep. \(_{t}\), \(_{t}\) are the noise schedule and \(f(t)\), \(g(t)\) can be derived from \(_{t}\), \(_{t}\). For simplicity, we denote \(q_{t0}(x_{t}|x_{0})\) and \(p_{0t}(x_{0}|x_{t})\) as \(q(x_{t}|x_{0})\) and \(p(x_{0}|x_{t})\), respectively. To sample from \(q_{0}(x_{0})\) by following the reverse stochastic differential equation (SDE), the score function \(_{x_{t}} q_{t}(x_{t})\) is required. Typically, diffusion models use denoising score matching to train a neural network \(_{}(x_{t},t)\) that estimates the score function [46; 15; 43], by minimizing \(_{t(0,T),x_{0} q_{0}(x_{0}),(,)}[\|_{}(x_{t},t)-\|^{2}]\), where \(x_{t}=_{t}x_{0}+_{t}\). As we mainly focus on diffusion policy in RL, this objective is usually impractical because \(q_{0}(x_{0})\) is expected to be the optimal policy \(^{*}(a|s)\). A more detailed discussion is given in Appendix A.

To make diffusion models compatible with RL, there are generally two approaches: _guide-based_ and _select-based_. Guide-based methods [32; 17] incorporate the behavior policy's score function with an additional guidance term. Specifically, they learn a time-dependent guidance term \(_{t}\) and use it to drift the generated actions towards high-value regions. The learning objective of \(_{t}\) can be generally formalized with \((_{t}(a_{t}),w(s,\{a_{i}^{}\}_{i=1}^{}))\), where \(w(s,\{a_{i}^{}\}_{i=1}^{})\) are critic-computed values on \(K\) diffusion behavior samples. \(\) can be a contrastive objective  or mean-square-error objective .

After training, the augmented score function \(_{a_{t}}_{t}(a_{t}|s)=_{a_{t}}_{t}^{}(a_{t}|s )+_{a_{t}}_{t}(a_{t})\) is used to characterize the learned policy.

Select-based methods [4; 14] utilize the observation that for some RL algorithms, the actor induced through critic learning manifests as a reweighted behavior policy. To sample from the optimized policy, these methods first sample multiple candidates \(\{a_{0}^{i}\}_{i=1}^{N}\) from the diffusion behavior policy and then resample from them using critic-computed values \(w(s,\{a_{0}^{i}\}_{i=1}^{N})\). More precisely, the sampling procedure follows the categorical distribution \(Pr[a=a_{0}^{j}|s]=^{j})}{_{i=1}^{N}w(s,a_{0}^{j})}\).

Error exploitationAs we can see, both guide-based and select-based methods need the information of \(w(s,\{a_{0}^{i}\}_{i=1}^{N})\) to get the optimal action. However, this term may bring two sources of errors. One is the diffusion model's approximation error in modeling complicated policy distribution, and the other is the critic's error in evaluating unseen actions. Although trained on offline data, the diffusion model may still generate OOD actions (especially with frequent sampling) and the learned critic can make erroneous predictions on these OOD actions, causing the evaluated value on these actions to be over-estimated due to the learning nature of value functions [11; 25]. As a result, the generation of high-quality actions in existing methods is greatly affected due to this error exploitation, which we will also show empirically in the next section.

## 3 Diffusion-DICE

In this section, we introduce our method, Diffusion-DICE. We start from an extension of DICE-based method that considers DICE as a transformation from the behavior distribution to the optimal policy distribution, which motivates us to use diffusion models to perform such transformation. We then propose a _guide-then-select_ procedure to achieve the best action, i.e., we propose in-sample guidance learning for accurate policy transformation and use the critic to do optimal action selection to boost the performance. We also propose a piecewise \(f\)-divergence to stabilize the gradient during learning. We give an illustration of the guide-then-select paradigm and use a toycase to showcase the error exploitation in previous methods and how Diffusion-DICE successfully alleviates that.

### An Optimal Policy Transformation View of DICE

As mentioned before, DIstribution Correction Estimation (DICE) methods, an important line of work in offline RL and IL provides us with an elegant way to estimate the optimal stationary distribution ratio between \(d^{*}(s,a)\)) and \(d^{}(s,a)\)[28; 41; 34]. We show that this ratio also directly indicates a proportional relationship between the optimal in-support policy \(^{*}(a|s)\) and the behavior policy \(^{}(a|s)\). This proportional relationship enables us to transform \(^{}\) into \(^{*}\).

Our key observation is that the definition of \(d^{}(s,a)\) inherently reveals a bijection between \((a|s)\) and \(d^{}(s,a)\). Given a relationship between \(d^{*}(s,a)\) and \(d^{}(s,a)\), we can use this bijection to derive a relationship between \(^{*}(a|s)\) and \(^{}(a|s)\). We formalize the bijection and derived relationship as:

\[(a|s)=(s,a)}{_{a}d^{}(s,a)da},^{ *}(a|s)=(s,a)}{d^{}(s,a)}}{_{a} (s,a)}{d^{}(s,a)}^{}(a|s)da}^{}(a|s).\]

The proof is given in Appendix B. Since the denominator in the second equation involves an integral over \(a\) and is unrelated to the chosen action, the relationship indicates \(^{*}(a|s)(s,a)}{d^{}(s,a)}^{}(a|s)\). This means that the transformation between \(d^{}(s,a)\) and \(d^{*}(s,a)\) can be extended to the transformation between \(^{}(a|s)\) and \(^{*}(a|s)\). This transformation motivates the use of deep generative models, e.g. diffusion models, to first fit the behavior distribution using their strong expressiveness and then directly perform this transformation during sampling. More specifically, the score function of the optimal policy and the behavior policy satisfy the following relationship:

\[_{a_{t}}_{t}^{*}(a_{t}|s)=_{a_{t}}_{t}^{} (a_{t}|s)+_{a_{t}}_{a_{0}^{}(a_{0}|a_{t},s)}[(s,a_{0})}{d^{}(s,a_{0})}].\] (6)

The proof is given in Appendix B. Intuitively, \(_{a_{t}}_{t}^{}(a_{t}|s)\) tells us how to generate actions from \(^{}\) and \(_{a_{t}}_{^{}(a_{0}|a_{t},s)}[w^{*}(s,a_{0})]\) tells us how to transform from \(^{}\) to in-support \(^{*}\) during the reverse diffusion process. As mentioned before, when representing \(^{*}\) with a diffusion model,all we need is its score function \(_{a_{t}}^{*}_{t}(a_{t}|s)\). Equivalently, we focus on the right-hand side of Eq.(6). The first term is just the score function of \(^{}\), which is fairly easy to obtain from the offline dataset [14; 4]. To perform the transformation, we still require the second term. Fortunately, DICE allows us to acquire the inner ratio term in an in-sample manner directly from the offline dataset, and we will show in the next section how to exactly compute the whole second term using offline dataset.

### In-sample Guidance Learning for Accurate Policy Transformation

Although DICE provides us with the optimal stationary distribution ratio, which is the cornerstone of the transformation, the second term on the right-hand side of Eq.(6) is still intractable due to the conditional expectation. Our key observation is that for arbitrary non-negative function \(f(x)\), the optimizer of the following convex problem is unique and takes the desired form of log-expectation.

**Lemma 1**.: _Given a random variable \(X\) and its corresponding distribution \(P(X)\), for any non-negative function \(f(x)\), the following problem is convex and its optimizer is given by \(y^{*}=_{x P(X)}[f(x)]\),_

\[_{y}_{x P(X)}[f(x) e^{-y}+y].\]

It is evident that the optimizer can be used to derive the second term in Eq.(6). In fact, we show the second term can be obtained directly from the offline dataset, if we optimize the following objective. Here \(g_{}\) is a guidance network parameterized by \(\) and we denote \((s,a)}{d^{*}(s,a)}\) as \(w^{*}(s,a)\) for shorthand.

**Theorem 1**.: \(_{a_{t}}_{^{}(a_{0}|a_{t},s)}[w^{*}(s,a_{0})]\) _can be obtained by solving the following optimization problem:_

\[_{}_{t(0,T)}_{a^{ }(a|s)}_{a_{t} p(a_{t}|a_{0})}w^{*}(s,a)e^{-g_ {}(s,a_{t},t)}+g_{}(s,a_{t},t),\] (7)

_as the optimal solution \(^{*}\) satisfies \(_{a_{t}}g_{^{*}}(s,a_{t},t)=_{a_{t}}_{^{ }(a_{0}|a_{t},s)}[w(s,a_{0})]\)._

This objective has two appealing properties. Firstly, compared to guide-based and select-based methods, we don't need to use multiple actions generated by the behavior diffusion model, we only need one sample from \(^{}(a|s)\) to estimate the second expectation, enabling an unbiased estimator of this objective using only offline dataset possible. Secondly, because \(w(s,a)\) must be non-negative to ensure a valid transformation, this objective is convex with respect to \(g_{}(s,a_{t},t)\). This indicates a guarantee of convergence to \(g_{^{*}}(s,a_{t},t)\) under mild assumptions (see proof in Appendix B).

Directly inducing \(_{a_{t}}_{^{}(a_{0}|a_{t},s)}[w^{*}(s,a_{0})]\) from the dataset is crucial in the offline setting. Firstly, it avoids the evaluation of OOD actions and solely depends on reliable value predictions of in-sample actions. Consequently, the gradient exploits minimal error in the critic, enabling it to more accurately transform \(^{}\) into in-support \(^{*}\). Moreover, because all values of \(w^{*}(s,a)\) used in our method are based on in-sample actions, applying this gradient in the reverse process will guide the samples towards in-sample actions with high \(w^{*}(s,a)\) value.

We refer to this method as In-sample Guidance Learning (IGL). Note that previous methods either have a biased estimate of the gradient [17; 6], or have to rely on value predictions for diffusion generated actions, which could be OOD . We refer to Appendix C for an in-depth discussion of IGL against other guidance methods.

Stabilizing gradient using piecewise \(f\)-divergence.In practice, there exists one issue when computing the guidance term. Note that in Eq.(4), \(w^{*}(s,a)\) could become zero, depends on the choice of \(f\). Given a specific \(a_{t}\), if \(w^{*}(s,a_{0})=0\) for actions under \(^{}(a_{0}|a_{t},s)\), the second gradient term will not be well-defined due to taking the logarithm of 0. In practice, this can result in an unstable gradient. To avoid such issue, \((f^{})^{-1}(x)\) should be positive for any given \(x\). Also, to facilitate the optimization process, \(f^{*}(x)\) should possess a closed-form solution and good numerical properties. Unfortunately, none of the commonly used \(f\)-divergence can accommodate both. However, considering the following two \(f\)-divergences:

   Divergence & \(f(x)\) & \(f^{*}(x)\) & \((f^{})^{-1}(x)\) \\  Reverse KL & \(x x\) & \(e^{x-1}\) & \(e^{x-1}\) \\  Pearson \(^{2}\) & \((x-1)^{2}\) & \((-1,}{4}+x)\) & \((0,+1)\) \\   

It is obvious that Reverse KL divergence possesses the positive property but exhibits numerical instability given a large \(x\) due to the exponential function in \(f^{*}(x)\), while Pearson \(^{2}\) divergence avoids the instability in the exponential function, its \((f^{})^{-1}(x)\) value is negative when \(x<-2\).

To take advantage of both divergences while avoiding their drawbacks, we propose the following _piecewise_\(f\)-divergence that has properties similar to Pearson \(^{2}\) when \(x\) is large while has properties similar to Reverse KL when \(x\) is small:

\[f(x)=(x-1)^{2}&x 1,\\ x x-x+1&0 x<1.\] (8)

In Appendix B, we prove that \(f(x)\) is a well-defined \(f\)-divergence. We also prove that \(f(x)\) has the following proposition:

**Proposition 1**.: _Given the piecewise \(f\)-divergence in Eq.(8), \((f^{})^{-1}(x)\) and \(f^{*}(x)\) has the following formulation:_

\[(f^{})^{-1}(x)=+1&x 0\\ e^{x}&x<0 f^{*}(x)=}{4}+x&x 0\\ e^{x}-1&x<0\] (9)

We can see that given any \(x\), the exponential function in \((f^{})^{-1}(x)\) ensures a strictly positive value. Meanwhile, \(f^{*}(x)\) is at most a quadratic polynomial, which ensures numerical stability in practice. Note that \(f(x)\) enables a stable policy transformation and can also be applied to other DICE-based methods when the optimal stationary distribution ratio needs to be positive.

### Boost Performance with Optimal Action Selection

After transforming \(^{}\) into in-support \(^{*}\), we now introduce the select-step to boost the performance during evaluation. One issue is the multi-modality in the optimal policy, this partially arises from the policy constraint used to regularize its output [9; 25; 55; 34]. Under policy constraint, the regularized optimal policy is unlikely to be deterministic and may be multi-modal.

More specifically, in our method, as we strictly follow the score function \(_{a_{t}}_{t}^{*}(a_{t}|s)\) of in-support optimal policy \(^{*}\) to generate high-value actions during the reverse diffusion process, any mode that has non-zero probability under \(^{*}(a|s)\) could be generated, which lead to a sub-optimal choice. Similar issues have been discussed in previous works [14; 13]. In Diffusion-DICE, it's natural to leverage the optimal critic \(Q^{*}\) derived from DICE in Eq.(3) to identify the best action. Given a specific state \(s\), we first follow \(_{a_{t}}_{t}^{*}(a_{t}|s)\) in the reverse diffusion process to generate a few actions from \(^{*}(a|s)\). Then we evaluate these actions with \(Q^{*}\) and select the action with the highest \(Q^{*}(s,a)\) value as the policy's output as

\[(s)=a^{*}*{arg\,max}_{a\{a_{0}^{ }^{*}(a|s)\}}Q^{*}(s,a).\] (10)

Different from previous guide-only or select-only methods, we base the select stage on the guide stage. As mentioned before, using IGL accurately guides the generated candidate actions towards in-support actions with high value. With the assistance of the guide stage, we can sample candidates from \(^{*}(a|s)\), which has a high probability of being high-quality. This means only a small number of candidates are required to be sampled, which reduces the probability of sampling OOD actions. This leads to minimal error exploitation while still attaining high returns. We give an illustration of the guide-then-select paradigm in Figure 1.

We term this _guide-then-select_ method Diffusion-DICE and present its pseudo-code in Algorithm 1. During the training stage, Diffusion-DICE estimates the optimal stationary distribution ratio using DICE with our piecewise \(f\)-divergence and calculates the guidance with IGL, both in an in-sample manner. During the testing stage,Diffusion-DICE selects the action from \(^{*}(a|s)\) with the highest value. By selecting from a small number of action candidates from \(^{*}(a|s)\), minimal error of the action evaluation model will be exploited. Besides serving as a diffusion-based offline RL algorithm, Diffusion-DICE also introduces a novel policy extraction method from other DICE-based algorithms. By leveraging expressive generative models, Diffusion-DICE can capture the multi-modality in \(^{*}\) and better draw upon the knowledge from \((s,a)}{d^{}(s,a)}\) and \(^{}\).

Toycase validation.We use a toycase to validate that the _guide-then-select_ paradigm used in Diffusion-DICE indeed brings minimal error exploitation that other diffusion-based methods suffer from. Here we aim to solve a 2-D bandit problem given a fixed action dataset. The action space is continuous and actions in the offline dataset follow a bivariate standard normal distribution constrained within an annular region. We show the dataset distribution \(^{}\), the learned diffusion behavior policy \(^{}\), the ground truth reward \(R\) and the predicted reward \(\) in Figure 2. Note that the true optimal reward occurs on the outer circle. However, due to limited data coverage, the learned reward function exploits overestimation error on unseen regions, e.g., actions inside the inner circle have erroneous high values. What's worse, due to fitting errors, the diffusion behavior policy may generate such overestimated actions. We take Diffusion-DICE with a guide-based method, QGPO , and a select-based method, IDQL  for comparison.

More specifically, we visualize the sample generation process in different methods. For guide-based method QGPO, as it utilizes actions generated by \(^{}\) and the value of \(\) on them to obtain guidance, the guidance exploits overestimation errors of OOD actions in the center and drifts the generated actions towards them. Note that adding a select step in QGPO will not help as the guide-step is already incorrect. IDQL, because it is free of guidance, requires a large number of generated candidate actions to ensure the coverage of optimal action. IDQL utilizes \(\) to select from those suboptimal or out-of-distribution actions, which however, are the sources of error exploitation in \(\). For Diffusion-DICE, because the first guide-step accurately guides the generated actions towards in-support while high-value regions (i.e., actions around the outer circle), in the select step, the learned value function \(\) could make a correct evaluation and successfully select the true best actions in the outer circle.

Figure 2: Toycase of a 2-D bandit problem. The action in the offline dataset follows a bivariate standard normal distribution constrained within an annular region. The ground truth reward has two peaks extending from the center outward. We use a diffusion model \(^{}\) to fit the behavior policy and a reward model \(\) to fit the ground truth reward \(R\). Both \(^{}\) and \(\) fit in-distribution data well while making error in out-of-distribution regions. Diffusion-DICE could generate correct optimal actions in the outer circle while other methods tend to exploit error information from \(\) and only generate overestimated, sub-optimal actions.

## 4 Experiments

In this section, we present empirical evaluations of Diffusion-DICE. To validate Diffusion-DICE's ability to transform the behavior policy into the optimal policy while maintaining minimal error exploitation, we conduct experiments on two fronts. On one hand, we evaluate Diffusion-DICE on the D4RL offline RL benchmark and compare it with other strong diffusion-based and DICE-based methods. On the other hand, we use alternative criteria to demonstrate that the _guide-then-select_ paradigm results in minimal error exploitation. Experimental details are shown in Appendix D.

### D4RL Benchmark Datasets:

We first evaluate Diffusion-DICE on the D4RL benchmark  and compare it with several related algorithms. For the evaluation tasks, we select MuJoCo locomotion tasks and AntMaze navigation tasks. While MuJoCo locomotion tasks are popular in offline RL, AntMaze navigation tasks are more challenging due to their stronger need for trajectory stitching. For baseline algorithms, we selected state-of-the-art methods not only from traditional methods that use Gaussian-policy (including DICE-based methods) but also from diffusion-based methods. Gaussian-policy-based baseline includes CQL , in-sample based methods IQL , SQL  and DICE-based method O-DICE . Notably, O-DICE is a recently proposed DICE-based algorithm that stands out among various DICE-based offline RL methods. Diffusion-policy-based baseline includes Diffusion-QL  SfBC , QGPO  and IDQL . We also compare Diffusion-DICE with its Gaussian-policy counterpart to show the benefit of using Diffusion-policy in Appendix D. While SfBC and IDQL simply sample from behavior policy candidates and select according to the action evaluation model, Diffusion-QL and QGPO will guide generated actions towards high-value ones. It is worth noting that Diffusion-QL will also resample from generated actions after the guidance.

The results show that Diffusion-DICE outperforms all other baseline algorithms, including previous SOTA diffusion-based methods, especially on MuJoCo medium, medium-replay datasets, and AntMaze datasets. The consistently better performance compared with Diffusion-QL and QGPO demonstrates the essentiality of in-sample guidance learning. Compared with SfBC and IDQL, Diffusion-DICE also shows superior performance, even with fewer action candidates. This is because the guide stage provides the in-support optimal policy for the select stage to sample from, which underscores the necessity of sampling carefully from an in-support optimal action distribution, rather than from the behavior distribution. We refer to the comparison of candidate numbers under different environments in Appendix D. Furthermore, the substantial performance gap between Diffusion-DICE and

    &  &  \\   & CQL & IQL & SQL & O-DICE & Diffusion-QL & SfBC & QGPO & IDQL & D-DICE (ours) \\  halfcheetah-m & 44.0 & 47.4 & 48.3 & 47.4 & 51.1 \(\)0.5 & 45.9 \(\)2.2 & 54.1 \(\)0.4 & 51.0 & 60.0 \(\)0.6 \\ hopper-m & 58.5 & 66.3 & 75.5 & 86.1 & 90.5 \(\)4.6 & 57.1 \(\)4.1 & 98.0 \(\)2.6 & 65.4 & 100.2 \(\)3.2 \\ walker2d-m & 72.5 & 72.5 & 84.2 & 84.9 & 87.0 \(\)0.9 & 77.9 \(\)2.5 & 86.0 \(\)0.7 & 82.5 & 89.3 \(\)1.3 \\ halfcheetah-m-r & 45.5 & 44.2 & 44.8 & 44.0 & 47.8 \(\)0.3 & 37.1 \(\)1.7 & 47.6 \(\)1.4 & 45.9 & 49.2 \(\)0.9 \\ hopper-m-r & 95.0 & 95.2 & 99.7 & 99.9 & 101.3 \(\)0.6 & 86.2 \(\)9.6 & 96.9 \(\)2.6 & 92.1 & 102.3 \(\)2.1 \\ walker2d-m-r & 77.2 & 76.1 & 81.2 & 83.6 & 95.8 \(\)1.5 & 65.1 \(\)5.6 & 84.4 \(\)4.1 & 85.1 & 90.8 \(\)2.6 \\ halfcheetah-m-e & 90.7 & 86.7 & 94.0 & 93.2 & 96.8 \(\)0.3 & 92.6 \(\)0.5 & 92.3 \(\)0.3 & 95.9 & 97.3 \(\)0.6 \\ hopper-m-e & 105.4 & 101.5 & 111.8 & 110.8 & 111.1 \(\)1.3 & 108.6 \(\)2.1 & 108.0 \(\)2.5 & 108.6 & 112.2 \(\)0.3 \\ walker2d-m-e & 109.6 & 110.6 & 110.0 & 110.8 & 110.1 \(\)0.3 & 109.8 \(\)0.2 & 110.7 \(\)0.6 & 112.7 & 114.1 \(\)0.5 \\   antmaze-u & 84.8 & 85.5 & 92.2 & 94.1 & 93.4 \(\)3.4 & 92.0 \(\)2.1 & 96.4 \(\)1.4 & 94.0 & 98.1 \(\)1.8 \\ antmaze-u-d & 43.4 & 66.7 & 74.0 & 79.5 & 66.2 \(\)8.6 & 85.3 \(\)3.6 & 74.4 \(\)9.7 & 80.2 & 82.0 \(\)8.4 \\ antmaze-m-p & 65.2 & 72.2 & 80.2 & 86.0 & 76.6 \(\)10.8 & 81.3 \(\)2.6 & 83.6 \(\)4.4 & 84.5 & 91.3 \(\)3.1 \\ antmaze-m-d & 54.0 & 71.0 & 79.1 & 82.7 & 78.6 \(\)10.3 & 82.0 \(\)3.1 & 83.8 \(\)3.5 & **84.8** & **85.7** \(\)4.8 \\ antmaze-l-p & 38.4 & 39.6 & 53.2 & 55.9 & 46.4 \(\)8.3 & 59.3 \(\)14.3 & 66.6 \(\)9.8 & 63.5 & 68.6 \(\)8.6 \\ antmaze-l-d & 31.6 & 47.5 & 52.3 & 54.0 & 56.6 \(\)7.6 & 45.5 \(\)6.6 & 64.8 \(\)5.5 & 67.9 & 72.0 \(\)6.5 \\  kitchen-p & 49.8 & 46.3 & - & - & 60.5 \(\)6.9 & - & - & - & 78.3 \(\)3.4 \\ kitchen-m & 51.0 & 51.0 & - & - & 62.6 \(\)5.1 & - & - & - & 67.8 \(\)4.2 \\ pen-human & 37.5 & 71.5 & - & - & 72.8 \(\)2.7 & - & 73.9 \(\)4.6 & - & **84.4** \(\)2.7 \\ pen-cloned & 39.2 & 37.3 & - & - & 57.3 \(\)3.6 & - & 54.2 \(\)5.1 & - & 83.8 \(\)3.2 \\   

Table 1: Evaluation results on D4RL benchmark. We report the average normalized scores at the end of training with standard deviation across 5 random seeds. Diffusion-DICE (D-DICE) demonstrates superior performance compared to all baseline algorithms in 13 out of 15 tasks, especially on more challenging tasks.

O-DICE reflects the multi-modality of DICE's optimal policy and firmly positions Diffusion-DICE as a superior policy extraction method for other DICE-based algorithms.

### Further Experiments on Error Exploitation

We then continue to verify that the guide-then-select paradigm used in Diffusion-DICE indeed exploits minimal error. This is obvious for the guide stage because we only leverage in-sample actions for both critic training and guidance learning. For the select stage, additional evidence is needed, as Diffusion-DICE also selects actions from generated candidates. Our key observation is that if the action value remains high during evaluation but results in a low return trajectory, these action values must suffer from overestimation error.

Based on this, we choose to compare the average state-action value and the average return between actions selected by our guide-then-select paradigm and simply select from the behavior policy. Given identical critic and behavior policy, we run Diffusion-DICE and its guidance-free variant for 10 episodes, comparing their average \(Q(s,a)\) values and their normalized scores. The results are shown in Figure 3. We also compare the learning curves of their average \(Q(s,a)\) values over time in Appendix D. It's clear from the results that our guide-then-select paradigm achieves better performance while having less overestimated \(Q(s,a)\) values. This result validates the minimal error exploitation in Diffusion-DICE.

## 5 Related Work

Offline RLTo tackle the distributional shift problem, most model-free offline RL methods augment existing off-policy RL methods with a behavior regularization term. Behavior regularization can appear explicitly as divergence penalties [50; 25; 51; 9], implicitly through weighted behavior cloning [48; 38; 52], or more directly through careful parameterization of the policy [11; 58]. Another way to apply action-level regularization is via modification of value learning objective to incorporate some form of regularization, to encourage staying near the behavioral distribution and being pessimistic about OOD state-action pairs [26; 24; 55; 47]. There are also several works incorporating action-level regularization through the use of uncertainty  or distance function .

All these methods are based on the actor-critic framework and use unimodal Gaussian policy. However, several works indicate their limited ability to model multi-modal policy distribution [49; 14; 4], thereby leading to suboptimal performance. To remedy this, it's natural to employ powerful generative models to represent the policy. DT  uses transformer as the policy, Diffusion-QL , SfBC , QGPO  and IDQL  leverage diffusion models to represent policy. Other generative models like CVAE and consistency model have also been used as policy in offline RL [58; 54; 7]. Another line of methods utilizes diffusion models for trajectory-level planning by generating high-return trajectories and taking the corresponding action of the current state [17; 1]. While generative models are widely used in offline RL, few methods consider the existing errors in these models and whether the generation process exploits these errors.

DICE-based methodsThe core of DICE-based methods revolves around the ratio of the stationary distribution between two policies. This ratio can serve as the estimation target in off-policy evaluation [36; 57] or as part of a state-action level constraint term in RL [37; 41; 34]. In offline IL , it can connect the optimal stationary distribution of a regularized MDP with the dataset distribution [22; 12; 19]. In constrained RL, this ratio can induce the discounted sum of cost without an additional function approximator . Under the imperfect rewards setting, this ratio can reflect the gap between the given rewards and the underlying perfect rewards . All of these DICE methods consider this ratio as a transformation from one stationary distribution to another. In this paper, however, we extend this ratio as a transformation from the behavior policy to the optimal policy and propose a novel way of doing so by using diffusion models, which also serve as a replacement for the Gaussian-based policy extraction in DICE.

Figure 3: Actions generated by the guide-then-select paradigm result in better performance while have less overestimation error.

Conclusion

In this work, we propose a new diffusion-based offline RL algorithm, Diffusion-DICE. Diffusion-DICE uses a guide-then-select paradigm to select the best in-support actions while achieving minimal error exploitation in the value function. Diffusion-DICE also serves as a replacement for the Gaussian-based policy extraction part in current DICE methods, successfully unleashing the power of DICE-based methods. Through toycase illustration and extensive experiments, we show that Diffusion-DICE outperforms prior SOTA methods on a variety of datasets, especially those with multi-modal complex behavior distribution. One limitation of Diffusion-DICE is the sampling process of diffusion models is costly and slow. Another limitation is the training of diffusion models may suffer in low-data regimes. One future work is using more advanced generative models [44; 40] as a better choice.