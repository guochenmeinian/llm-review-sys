# Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference

Ji1  Yujian Liu1  Yang Zhang2

**Gaowen Liu3  Ramana Rao Kompella3  Sijia Liu4  Shiyu Chang1 \({}^{1}\)**UC Santa Barbara  MIT-IBM Watson AI Lab  Cisco Research  Michigan State University

###### Abstract

As Large Language Models (LLMs) demonstrate extensive capability in learning from documents, LLM unlearning becomes an increasingly important research area to address concerns of LLMs in terms of privacy, copyright, _etc._ A conventional LLM unlearning task typically involves two goals: (1) The target LLM should forget the knowledge in the specified forget documents, and (2) it should retain the other knowledge that the LLM possesses, for which we assume access to a small number of retain documents. To achieve both goals, a mainstream class of LLM unlearning methods introduces an optimization framework with a combination of two objectives - maximizing the prediction loss on the forget documents while minimizing that on the retain documents, which suffers from two challenges, _degenerated output_ and _catastrophic forgetting_. In this paper, we propose a novel unlearning framework called **U**nlearning from **L**ogit **D**ifference (ULD), which introduces an assistant LLM that aims to achieve the opposite of the unlearning goals: remembering the forget documents and forgetting the retain knowledge. ULD then derives the unlearned LLM by computing the logit difference between the target and the assistant LLMs. We show that such reversed objectives would naturally resolve both aforementioned challenges while significantly improving the training efficiency. Extensive experiments demonstrate that our method efficiently achieves the intended forgetting while preserving the LLM's overall capabilities, reducing training time by more than threefold. Notably, our method loses 0% of model utility on the ToFU benchmark, whereas baseline methods may sacrifice 17% of utility on average to achieve comparable forget quality. Our code is publicly available at https://github.com/UCSB-NLP-Chang/ULD.

## 1 Introduction

As Large Language Models (LLMs) continue to impress with their ability to learn from pre-training documents and apply this knowledge to real-world tasks like programming and question-answering, attention has increasingly focused on addressing the accompanying privacy issues [1; 2]. Machine unlearning [2; 3; 4; 5; 6; 7; 8], aiming to remove the influence of specific data, has become an important research area and is being used to remove sensitive information such as copyright contents from LLMs.

Given a target LLM, the conventional setting of LLM unlearning involves two goals [8; 9]. _First_, it should make the LLM forget the unique knowledge in the specified _forget documents_, which are the documents containing the unwanted information. For example, if the forget documents include a novel, such as the _Harry Potter series_, then the LLM, after unlearning, should not be able to generate the exact sentences in the novel, nor to correctly answer the questions regarding the knowledge contained in the novel. _Second_, the unlearning should not affect the other knowledge in the target

[MISSING_PAGE_FAIL:2]

[MISSING_PAGE_FAIL:3]

the LLM increasingly loses retain knowledge, particularly those that are not covered in the retain set, and thus cannot respond correctly to a query about retain data (See Table 1).

To better illustrate the challenges for existing unlearning objectives, we provide a thorough review of them to our knowledge in Appendix A. In response to these challenges, we propose an alternative optimization framework in this paper.

### Uld: An Overview

As it turns out, both challenges can be resolved effectively if we tackle the unlearn problem the other way around - rather than training the LLM to _forget_ the knowledge in \(_{f}\), we train an assistant LLM to _remember_\(_{f}\) and then subtract its output distribution from that of the original LLM.

Formally, denote \(l(Y|;)\) as the output logits of the original LLM, and \(l_{a}(Y|;)\) as the output logits of an assistant LLM. Then the output logits of the forget model, denoted as \(l_{f}(Y|)\), is derived by the following logit subtraction operation:

\[l_{f}(Y|)=l(Y|;)- l_{a}(Y|;),\] (2)

where \(\) is a hyper-parameter controlling the strength of forgetting. We note that logit operation is equivalent to re-scale the output distribution of original LLM [14; 15; 16].

The assistant LLM should satisfy two goals:  It should remember the unique knowledge in the forget documents, and  It should not remember any knowledge that should be retained for the original LLM and should desirably output a uniform distribution on retain documents.

Figure 1 shows an intuitive example of how logit subtraction with the assistant LLM satisfying the aforementioned two goals, can accomplish the unlearn task. Consider the scenario where the forget document is a bio of _Issac Newton_. Given a query involving the knowledge of _Newton_, _e.g. "Issac Newton was a famous - "_, both the original and the assistant LLMs will have high output probabilities on the correct answers such as _'physicist'_. Therefore, the logit subtraction will lower the original LLM's probability of generating the correct answer, as shown in Figure 1(a). On the other hand, given a query involving the retain knowledge, _e.g._, _'Aristotle was a famous - '_, the assistant LLM will output a flat distribution. Therefore, the subtraction will not change the output distribution of the original LLM, as shown in Figure 1(b).

Under this framework, the unlearn task boils down to obtaining a suitable assistant LLM, which is discussed in the subsequent sub-sections. Section 2.3 illustrates the training objective of the assistant LLM. Section 2.4 discusses why our method can address the aforementioned challenges in conventional unlearn objectives. Section 2.5 describes the architecture design of the assistant LLM.

### Training the Assistant LLM

It is obvious to see, by comparing Sections 2.2 and 2.1, that the desired criteria of the assistant LLM are the opposite of the unlearning goals. Therefore, the optimization objective of the assistant LLM should be the reversed version of Equation 1:

\[_{}()=_{}_{f}()-_{r}().\] (3)

For the forget loss, \(_{f}()\), we adopt the most typical design, _i.e._, the cross-entropy loss on forget documents:

\[_{f}()=_{[,y]^{}_{f} }[((l_{a}(Y|=;));(Y=y))],\] (4)

where \(()\) represents cross-entropy, and \((Y=y)\) represents the one-hot distribution concentrating on token \(y\). The forget loss for assistant model is computed over \(^{}_{f}\), which is the augmented version of the \(_{f}\) by incorporating a paraphrased version of the original forget documents. This operation is essential as it helps the assistant LLM to generalize to different forms of \(_{f}\). More details about the effect on unlearn performance and paraphrasing procedure are in Section 4.3 and Appendix B.

Figure 1: Illustration of the logit subtraction operation. We simulate the output distribution of an unlearned LLM using the assistant LLM’s output.

For the retain loss, \(_{r}()\), since the most desirable behavior of the assistant model on the retain set would be to output a uniform distribution (see discussion in Section 2.2), we design the retain loss as the cross-entropy against the uniform distribution:

\[_{r}()=-_{_{r}^{}}[ ((l_{a}(Y|=;));U(Y))],\] (5)

where \(U(Y)\) denotes the uniform distribution. \(_{r}^{}\) represents the augmented retain documents, which include the original retain documents plus, optionally, documents that contain the wrong knowledge against the forget documents. Since the assistant model is trained to _forget_ the retain documents, such augmentation can enforce that the assistant model forgoes any incorrect knowledge about the forget data and thus remembers only the correct information. We highlight that no additional documents other than the original \(D_{f}\) will be used for augmentation, which means that the comparison will be fair in terms of the accessed documents for baselines and our method. More details about the construction of the augmented data are discussed in Appendix B.1.

### Comparison with Conventional Unlearning Framework

Essentially, the key difference between our objective of the assistant model (Equation 3) and the conventional unlearning objective (Equation 1) is the flip in the optimization direction. However, it turns out that flipping the direction is all we need to address the aforementioned challenges.

**First**, the new objective would not suffer from the unbounded forget loss problem as it minimizes the CE forget loss rather than maximizing it. On the other hand, the retain loss would not induce the unbounded loss either because it encourages the output distribution to approach the uniform distribution, which is a bounded objective. **Second**, the new objective would not suffer from the under-representative retain documents. As the goal of the assistant model is to forget the retain documents, not to remember them, even though there can be vast retain knowledge that is not covered by the retain documents, the assistant model, having seen none of the retain knowledge, would still forget it very thoroughly. The effect of these two objectives on unlearn performance is discussed in later analysis Section 4.1.

### Architecture Design of the Assistant LLM

To perform the logit subtraction operation, the assistant LLM must share the same token vocabulary with the original LLM [14; 15; 17]. In this paper, we propose a novel approach to building the assistant that utilizes part of the target LLM itself. More specifically, suppose the original LLM is composed of a transformer model \(_{M}()\) with \(M\) layers, _e.g._\(M=32\) for Llama-2, and a language model head \(()\), which maps hidden representation to the output logits over model vocabulary, _i.e._, \(l(Y|;)=(_{M}())\). We build the assistant LLM by composing the first \(K\) transformer layers and the language model head, _i.e._, \(l_{a}(Y|;)=(_{K}())\), where \(K<M\) is a hyper-parameter. Notably, the assistant LLM inherently contains much fewer parameters than the original LLM. For example, the first 8-layer of the Llama-2 LLM contains 1.1B parameters, 5.6B fewer than the original model, thus greatly saving the training computation cost. Since the assistant LLM only needs to remember the forget documents, which is a much less challenging task for a typical LLM, we can utilize parameter-efficient fine-tuning methods such as LoRA  to reduce more training parameters. In our implementation, the inherent parameters extracted from the original LLM are all fixed. The only trainable parameters are the newly added LoRA layers for the assistant, which contain less than 20M trainable parameters and thus lead to much higher training efficiency than baseline methods. We illustrate the assistant LLM construction in Figure 2.

## 3 Experiment

In this section, we compare the proposed ULD algorithm with baseline unlearning methods on two widely used LLM unlearning settings: forgetting knowledge of a fictional writer on TOFU dataset ,

Figure 2: Illustration of constructing the assistant LLM utilizing the target LLM itself. Note that we fix the assistant LLM’s parameter and only optimize the added LoRA layers.

and forgetting copyright contents in Harry Potter Series Book [9; 18]. First, we summarize the baselines in Section 3.1. Next, we present the experiments on the two settings in Sections 3.2 and 3.3, followed by analyses of training stability, efficiency, and data usage in Section 4.

### Baseline Unlearn Objectives

As described in Section 2.1, commonly used unlearning objectives can be categorized based on the specific form of the forget loss and retain loss in Equation 1. The forget losses include: \(\) GA[9; 10; 18]: the cross-entropy loss, designed to prevent the model from generating correct answers on the forget data. \(\) DPO[9; 10]: direct preference optimization loss, which trains the LLM to favor alternative responses like _'I don't know'_ over the correct answers on forget data. \(\) NPO: negative-preference optimization loss, a variant of DPO where only the original correct answer is used as the negative response and no alternative response is involved. The retain losses include: \(\) GD[9; 10]: cross-entropy loss that encourages model to predict correctly on the retain data. \(\) KL[10; 11; 18]: KL-divergence between the model's predictions before and after unlearning, which helps maintain the original prediction on the retain data.

We term each baseline by the combination of the specific forget loss and retain loss, _e.g.,_ GA+KL indicates the use of GA as the forget loss and KL as the retain loss. We note that a concurrent work  also incorporates an assistant LLM and calculates logit difference similar to our method. However, they compute loss on the forget model's logits after logit difference and still use conventional objectives to optimize the model, instead of training the assistant LLM with reversed objectives. We denote this baseline by adding Offset to the unlearning objective, _e.g.,_ Offset-GA+KL means that the assistant is trained using GA+KL objective. Please refer to Appendix A for further details of each baseline.

### Experiments on TOFU

SetupTOFU  focuses on unlearning the knowledge of fictitious writers. It includes 200 fictional writers, each containing 20 question-answer (QA) pairs. TOFU contains three forget data \(_{f}\) configurations, each with 1%, 5%, and 10% of the fictional writers. We refer to these settings as TOFU-1%, TOFU-5%, and TOFU-10%. The retain data \(_{r}\) consists of the QA pairs of remaining fictional writers. We measure the forget performance using _forget quality_, which assesses how closely the unlearned LLM mimics an LLM trained only on retain data. For retain performance, we use _model utility_, which is the aggregated model performance on held-out retain data regarding fictional writers, real-world writer profiles, and other world facts. In addition, we include _ROUGE-L_ for both forget and retain performance, which measures the overlap between reference and generated

    &  &  &  \\  &  &  &  &  &  &  \\  & \(F.Q.\) & \(R\)-L & \(M.\) & \(R\)-L & \(\) & \(F.Q.\) & \(R\)-L & \(M.U.\) & \(R\)-L & \(F.Q.\) & \(R\)-L & \(M.U.\) & \(R\)-L & \(M.U.\) & \(R\)-L \\  Target LLM & 1e-3 & 95.2 & 0.62 & 98.2 & 3e-16 & 97.3 & 0.62 & 98.2 & 2e-19 & 98.6 & 0.62 & 98.2 \\ Retain LLM & 1.0 & 37.6 & 0.62 & 98.5 & 1.0 & 39.3 & 0.62 & 98.1 & 1.0 & 39.8 & 0.62 & 98.2 \\  GA & 0.40 & 34.4 & 0.52 & 59.6 & 0.05 & 24.4 & 0.37 & 31.3 & 8e-10 & 0 & 0 & 0 \\ GA+GD & 0.27 & 30.5 & 0.53 & 58.9 & 0.11 & 19.5 & 0.33 & 28.9 & 9e-3 & 19.6 & 0.17 & 23.9 \\ GA+KL & 0.40 & 35.2 & 0.53 & 59.9 & 0.14 & 20.3 & 0.35 & 29.2 & 2e-4 & 12.1 & 0.05 & 18.6 \\  DPO & 0.27 & 4.09 & 0.58 & 55.2 & 1e-4 & 1.1 & 0.02 & 0.89 & 5e-7 & 0.7 & 0 & 0.72 \\ DPO+GD & 0.25 & 4.08 & 0.58 & 56.5 & 1e-7 & 1.2 & 0.02 & 0.84 & 8e-10 & 0.8 & 0 & 0.89 \\ DPO+KL & 0.26 & 4.18 & 0.58 & 55.6 & 4e-5 & 1.1 & 0.03 & 0.93 & 5e-8 & 0.7 & 0.03 & 0.81 \\  NPO & 0.66\({}^{*}\) & **39.2** & 0.52 & 62.8 & 0.68 & 15.9 & 0.19 & 24.6 & 0.09 & 15.2 & 0.26 & 15.3 \\ NPO+GD & 0.58\({}^{*}\) & 34.5 & 0.57 & 63.1 & 0.46 & 24.7 & 0.44 & 36.5 & 0.29 & 25.7 & 0.53 & 41.1 \\ WPO+KL & 0.52\({}^{*}\) & 33.7 & 0.54 & 58.7 & 0.44 & 24.2 & 0.48 & 40.2 & 0.07 & 18.1 & 0.32 & 22.9 \\  Offset-GA+KL & 0.27 & 44.7 & 0.52 & 45.8 & 1e-4 & 1.2 & 0 & 0 & 2e-6 & 3.1 & 0.04 & 2.9 \\ Offset-DP+KL & 0.13 & 3.8 & 0.12 & 19.1 & 2e-8 & 0 & 0 & 0 & 3e-9 & 1.3 & 0.02 & 1.4 \\ Offset-NPO+KL & 0.41 & 31.4 & 0.43 & 34.5 & 5e-10 & 37.3 & 0.59 & 40.9 & 4e-5 & 34.2 & 0.48 & 34.8 \\  ULD & **0.99** & 40.7 & **0.62** & **98.3** & **0.73** & **41.2** & **0.62** & **93.4** & **0.52** & **42.6** & **0.62** & **85.9** \\   

Table 2: Performance on TOFU dataset. _F.Q._, _M.U._, and _R-L_ represent _forget quality_, _model utility_ and _ROUGE-L_ respectively. The best results are marked in **bold**. We include the original LLM and retain LLM for reference. \({}^{*}\): We notice these values are lower than those in the original paper, due to sensitivity to random seeds.

answers. We use the fine-tuned LLama2-chat-7B  released by TOFU paper as the target LLM, which contains the knowledge of all 200 fictional writers. More details are in Appendix B.3.

**Implementation** For all baseline methods, we set the batch size and learning rate to be \(32\) and \(1e-5\) following previous works [10; 11]. We fine-tune the target LLM for 10 epochs using AdamW optimizer . For all baseline methods involving retain loss, we set the weight \(\) in Equation 1 to \(1\).

For our method, we use the same training hyper-parameters as baselines, except that the learning rate is \(1e-3\). The hyper-parameters for the LoRA layers are \(r=32,=32\), and the number of assistant LLM layers \(K\) is 8. We fine-tune the assistant LLM on augmented forget data \(^{}_{f}\) and retain data \(^{}_{r}\) as described in Section 2 (details in Appendix B.1). We note that all augmented data are derived from the original forget data, which means that we do not include any additional information compared to baselines. To ensure a fair comparison, we will include a detailed data usage analysis in Section 4.3.

**Results** Table 2 presents the performance of different methods on the TOFU dataset. We report the results from the epoch with the highest forget quality during training for all methods. We highlight the following observations: \(\)ULD achieves the best forget performance in all three settings. Notably, we obtain a \(0.99\) forget quality on TOFU-1%, close to the \(1.0\) upper bound. Moreover, \(\)LD achieves a ROUGE score that is closest to the retrained LLM on forget data for TOFU-5% and TOFU-10%, whereas baselines have significantly lower ROUGE scores, indicating that their generated responses are mostly nonsensical. Appendix Table 6 shows sample responses of different methods. \(\)ULD is the best in preserving retain performance in all settings, experiencing almost no reductions in model utility compared to the original model. Notably, the most competitive baseline method in terms of forget quality, \(\), sacrifices 17% percent of model utility on average across three settings.

### Experiments on HarryPotter

**Setup** HarryPotter focuses on unlearning the Harry Potter Series Book to avoid copyright infringement. Following prior works [9; 18], we extract 400 chunks, each with 512 tokens, from the Harry Potter book to construct the forget data \(_{f}\) and sample 400 paragraphs in the C4  dataset as the retain data \(_{r}\). We measure the forget performance using _BLEU_ and _ROUGE-L_ scores between ground-truth and model-generated completions given prefixes of excerpts in the forget data with a fixed length of 200 tokens, as this reflects potential copyright content leakage. We measure the retain performance using the _zero-shot accuracy_ on six standard LLM benchmarks, including BoolQ , RTE , HellaSWAG , ARC , OpenBookQA , and PiQA . Additionally, we measure the perplexity of unlearned LLM on paragraphs from the held-out WikiText dataset  for retain performance. We use Mistral-7B-instruct  as the target LLM. Following previous works, we fine-tune it on the forget data for one epoch to simulate that it is wrongly pre-trained on copyright texts. More details are in Appendix B.3.

**Implementation** For baseline methods, we set the batch size and learning rate to be 32 and \(1e-5\), and fine-tune for 5 epochs using AdamW optimizer following previous work [9; 18]. Same as TOFU dataset, the retain weight \(\) is set to 1. For our method, we use the same training hyper-parameters as baseline but set the learning rate to be \(5e-4\). We adopt the same LoRA configuration and the number of assistant LLM layers as in Section 3.2. In this experiment, the augmented forget data \(^{}_{f}\) contains paraphrased HarryPotter paragraphs, and the augmented retain data \(^{}_{r}\) is the same as the original \(_{r}\).

    &  \\  &  &  \\  & _BLEU_ & _R-L_ & _PPL_ \(\) & _Avg. Acc._ \(\) \\  Target LLM & 8.02 & 16.98 & 9.81 & 66.93 \\ Before finetune & 0.74 & 8.97 & 9.80 & 67.24 \\  GA & 0 & 0 & 48.13 & 35.59 \\ GA+GD & 0 & 0 & 15.75 & 58.34 \\ GA+KL & 0 & 0 & 17.59 & 55.41 \\  DPO & 0.35 & 4.24 & 42.14 & 48.12 \\ DPO+GD & 0.38 & 3.94 & 16.98 & 53.91 \\ DPO+KL & 0.35 & 4.15 & 18.43 & 56.34 \\  NPO & 0.47 & 4.31 & 35.71 & 54.73 \\ NPO+GD & 0.82 & 5.76 & 14.85 & 61.77 \\ NPO+KL & 0.74 & 6.84 & 15.44 & 61.14 \\  Offset-GA+KL & 0 & 0 & 58.54 & 53.78 \\ Offset-DPO+KL & 0.45 & 4.39 & 23.56 & 56.59 \\ Offset-NPO+KL & 0.58 & 8.55 & 19.43 & 58.72 \\  ULD & 0.67 & 4.58 & **9.95** & **66.85** \\   

Table 3: Performance on HarryPotter dataset. _R-L_ and _Avg. Acc._ denotes the ROUGE-L score and average zero-shot accuracy over six LLM benchmarks. The model before and after fine-tuning (target LLM) are included for reference. Best results are in **bold** for retain performance. For forget performance, no values are in bold as there is no ground-truth.

ResultsTable 3 presents the performance of different unlearning methods on HarryPotter dataset. Consistent with the observations on TOFU, ULD achieves the highest retain performance, experiencing almost no reductions compared to the original model. Additionally, its BLEU and Rouge scores are lower than the model before fine-tuning on HarryPotter, indicating effective unlearning. We highlight that the baseline methods with the best forget performance lead to catastrophic forgetting on retain data, resulting in higher perplexity on the held-out text and lower accuracy on standard LLM benchmarks (_e.g.,_NP0+GD has over 5% accuracy decline compared to the finetuned LLM).

## 4 Additional Analyses

In this section, we conduct more analyses on the proposed ULD algorithm based on the TOFU-10% setting. In particular, we aim to answer the following questions: How does ULD resolve the challenges of _degenerated output_ and _catastrophic forgetting_ faced by conventional unlearning objectives? (Section 4.1) How efficient is ULD compared to baselines? (Section 4.2) How does the augmented forget/retain data affect the effectiveness of ULD and baselines? (Section 4.3)

### Training Stability

As described in Section 2.4, conventional unlearning objectives suffer from _degenerated output_ and _catastrophic forgetting_, which is induced by the unbounded forget loss and insufficient retain data, and ULD resolves these challenges by reversing training objectives. To better illustrate this phenomenon, we plot two cross-entropy loss curves along training for different unlearning methods in Figure 4. For baselines, we compute the loss for the unlearned model. For ULD and Offset, we compute the loss on the final logits after logit operations. The left sub-figure shows the loss on the forget data. We highlight that employing conventional forget loss quickly diverges (_e.g.,_GA+KL), while the loss of ULD steadily increases and remains bounded. The right sub-figure shows the loss on the retain data not covered by \(_{}\). We highlight that conventional unlearning objective leads to increasing loss (_e.g.,_NP0+KL), indicating the risk of catastrophic forgetting, whereas ULD remains stable.

Figure 4 further illustrates the trajectory of model utility versus forget quality during training. As shown, ULD achieves a stable improvement in forget quality while maintaining consistent model utility, whereas baselines exhibit rapid changes on both metrics, with model utility eventually decreasing to near 0 for GA+KL and DP0+KL. This instability makes it challenging to obtain a competitive unlearned model for baselines, as it becomes very difficult to choose an appropriate criterion for early stopping.

### Training Efficiency

To illustrate the efficiency of ULD, we evaluate the training time of different methods on two A100 GPUs except Offset, which requires four A100 GPUs due to out-of-memory errors on two A100 GPUs. Figure 5 shows the best forget quality (y-axis) for different methods versus relative training time per epoch compared to ULD (x-axis). ULD is the most efficient method with more than 3 times improvement to NP0, the most efficient baseline with comparable forget performance. We highlight two reasons for the improvement: The LLM involved in training has much fewer parameters for ULD. The assistant LLM only includes

Figure 4: Trajectory of _Model utility_ versus _forget quality (log)_ for different unlearning method. The size of markers indicates the epoch number. Appendix Figure 11 shows the full results.

the first 8 layers of the original 32-layer LLM, which in total has 1.3B parameters, reducing more than 80% parameters, thus greatly saving the GPU computation required in training.

The task of assistant LLM is less challenging and can be effectively achieved using LoRA, which further reduces the trainable parameters to 20M parameters, 0.2% of the total parameters. One may note that the baseline methods can also employ LoRA training on the original LLM to save training time. However, we find that adopting LoRA harms the overall unlearning performance for baseline methods. As shown in Figure 5, while adopting LoRA for baselines greatly saves the training time, their forget performance is also reduced. We also highlight that \(\) is still more efficient than LoRA-baselines since the involved LLM has fewer parameters.

### Data Usage Ablation

In addition to different training objectives, one notable difference between \(\) and baseline methods is that we adopt the augmented forget data \(^{}_{f}\) and retain data \(^{}_{r}\) for assistant LLM training, which contains additional paraphrased and perturbed versions of the original forget data. To ensure a fair comparison, we conduct two analyses of the training data:  We add the same augmented data for baselines to justify that the effectiveness of \(\) is not simply brought by the augmented data. We ablate the data usage for \(\) to analyze how these augmentations affect the unlearning performance. On the other hand, many conventional unlearning setting requires a canonical retain set, which contains samples of knowledge that the LLM should not forget, whereas \(\) does not. We perform some additional studies to investigate whether \(\) would benefit from incorporating such canonical retain sets; details are in Appendix D.2.

The upper panel of Table 4 presents the results for baseline methods with augmented data \(^{}_{f}\) and \(^{}_{r}\). Notably, adding augmented data does not improve the performance of baselines but instead hurts the model utility, _e.g.,_ the utility for \(\)+\(\) drops from \(0.32\) to \(0.08\), which again indicates the instability of baseline methods. The full results are shown in Appendix D.4.

The lower panel of Table 4 presents the results on TOFU-10% for \(\) with different forget/retain data configurations. We highlight that the augmentations are essential for \(\). Introducing the paraphrased \(_{f}\) to obtain \(^{}_{f}\) improves the assistant LLM's acquire of the forget knowledge and thus improves the forget performance, where forget quality improves from \(1e-7\) to \(0.51\). Introducing the perturbed \(_{f}\) to obtain \(^{}_{r}\) avoids over-fitting of the forget data and thus improves the retain performance, where the model utility improves from \(0.53\) to \(0.63\), close to the original LLM.

## 5 Related Work

LLM UnlearningMachine unlearning was proposed in the vision domain and mainly focuses on the classification models [2; 3; 4; 5; 6; 7]. The core unlearn algorithm requires computing the Hessian of loss functions [2; 4], which is often intractable for LLMs due to unknown pre-train data and the massive amount of parameters. Therefore, recent research has proposed various unlearning objectives for finetuning target LLM, including gradient-ascent methods [9; 10; 32; 33] and preference-loss methods [10; 11]. However, these unlearning objectives suffer from degenerated output and catastrophic forgetting issues due to unbounded forget loss and under-representative retain data. On the contrary, our method employs the reverse of the conventional training objective on an assistant

    &  &  &  \\  & \(^{}_{f}\) & \(_{f}\) & \(_{Q}\) : & \(_{-L}\) & \(_{U}\) : & \(_{-L}\) \\  Target LLM & - & - & -2e-19 & 98.6 & 0.62 & 98.2 \\ Retain LLM & - & - & 1.0 & 39.8 & 0.62 & 98.2 \\   GA+KL & ✗ & ✗ & 2+4.2 & 12.1 & 0.05 & 18.6 \\ GA+KL & ✓ & ✗ & 4e-7 & 0 & 0 & 0 \\ DP+KL & ✗ & ✗ & 5e-8 & 0.7 & 0.03 & 0.81 \\ DP+KL & ✓ & ✓ & 7e-11 & 0 & 0 & 0 \\ DP+KL & ✗ & ✓ & 0.07 & 18.1 & 0.32 & 22.9 \\ DP+KL & ✓ & ✓ & 1e-14 & 12.3 & 0.08 & 18.4 \\ Offset-DP+KL & ✗ & ✗ & 4e-5 & 34.24 & 0.48 & 34.8 \\ Offset-DP+KL & ✓ & ✓ & 6e-9 & 15.8 & 0.24 & 28.7 \\   ULD & ✗ & ✗ & 1e-7 & 13.7 & 0.53 & 34.1 \\ ULD & ✓ & ✓ & 1e-9 & 43.8 & 0.63 & 84.1 \\ ULD & ✓ & ✓ & 0.51 & 12.7 & 0.55 & 72.3 \\ ULD & ✓ & ✓ & **0.52** & **42.4** & **0.62** & **86.4** \\   

Table 4: Performance of different unlearn methods on ToFU-10% with different forget/retain data configurations. We include baselines with competitive forget performance here and list the full results in Appendix D.4.

Figure 5: Log forget quality versus relative training time to \(\) on TOFU-10%. The top-left corner indicates better forget performance and efficiency.

LLM to resolve these issues. A concurrent work  also introduces assistant LLM for unlearning. However, they still suffer from these issues due to using conventional unlearn objectives.

**Decoding-time Steering for LLMs** There is a rich literature on decoding-time steering for LLMs [17; 34; 35; 36; 37; 38], where a main branch is based on the idea of modifying the LLM's output logits. To obtain suitable logit offset for modifying the target LLM's outputs, these methods include gradient-based manipulation [39; 40; 41], focus vector [42; 43], model arithmetic [44; 45; 46; 47], and contrasting outputs of two pre-trained LLMs [14; 15; 16]. Among them, the most similar works to our method are those involving training an assistant LLM to obtain the suitable logit offset [19; 48; 49]. However, they mainly employ a pre-trained LLM with the same vocabulary, _e.g.,_ a 7B Llama-2 assistant for improving a 65B Llama-2 LLM, which is not practical in most cases due to the high cost of training two LLMs separately. On the contrary, we propose a new strategy that extracts a sub-network from the target LLM with added LoRA layers to create the assistant, which applies to all LLMs.

## 6 Conclusion

In this paper, we introduce a novel LLM unlearning framework, ULD, which involves an assistant LLM trained with the reverse of conventional unlearning objectives ULD then derives the unlearned LLM by computing logit difference between assistant and target LLM. This objective naturally avoids the degenerated output and catastrophic forgetting issues that might be produced by unbounded forget loss and unrepresentative retain documents. Extensive empirical evaluations demonstrate the effectiveness and efficiency of ULD. Notably, ULD loses 0% of model utility on TOFU benchmark and achieves better forget performance. In terms of efficiency, our approach requires less than 3 times the training time compared to other baseline methods.

## 7 Broad Impacts

Our work proposes an efficient and effective LLM unlearning framework ULD, which has a broad impact on improving privacy and data leakage issues in LLM usage, making LLMs safer and more reliable in practical application. Unlike existing unlearning methods that may sacrifice the LLM's overall capability to achieve the desired unlearning. Our work does not change the parameters of original LLM and introduces an assistant LLM to help build the unlearned LLM via logit subtraction operation. This solves the common challenges of conventional unlearning objectives that may harm the retention of knowledge and improves the efficiency of the unlearning process.

We also note that the proposed framework is not limited to the LLM unlearning. Similar to previous works in LLM decoding literature [14; 15], we plan to explore applying our method to other tasks like sentiment-controlled text generation, knowledge editing, and improving LLM's factuality.

## 8 Limitations

While ULD enhances the training efficiency and stability of the unlearning process, our method involves an assistant LLM during inference, which may lead to higher inference latency. However, this increase can be mitigated by parallelizing the computations of the assistant LLM and the original LLM. Additionally, although forget data augmentation is crucial for improving the unlearn performance for ULD, creating appropriate augmentations for different datasets can be challenging. We plan to explore the automatic construction of optimal forget data construction in future work.

## 9 Acknowledgement

The work of Jiabao Ji, Yujian Liu and Shiyu Chang was partially supported by National Science Foundation (NSF) Grant IIS-2338252, NSF Grant IIS-2207052, NSF Grant IIS-2302730, CISCO Research Program, and IBM Research Grant. The computing resources used in this work were partially supported by the Accelerate Foundation Models Research Program of Microsoft.