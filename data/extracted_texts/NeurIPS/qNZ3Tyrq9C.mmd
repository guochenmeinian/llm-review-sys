# On Transferring Expert Knowledge from Tabular Data to Images

Jun-Peng Jiang\({}^{1,2}\) Han-Jia Ye\({}^{1,2}\) Le-Ye Wang\({}^{3}\) Yang-Yang\({}^{4}\)

**Yuan-Jiang\({}^{1,2}\) De-Chuan Zhan\({}^{1,2}\)**

\({}^{1}\) National Key Laboratory for Novel Software Technology, Nanjing University, China

\({}^{2}\) School of Artificial Intelligence, Nanjing University, China

\({}^{3}\) Dept of Computer Science, School of EECS, Peking University

\({}^{4}\) School of Computer Science and Engineering, Nanjing University of Science and Technology

{jiangjp,yehj,jiangy,zhandc}@lamda.nju.edu.cn

leyewang@pku.edu.cn yyang@njust.edu.cn

###### Abstract

Transferring knowledge across modalities has garnered significant attention in the field of machine learning as it enables the utilization of expert knowledge from diverse domains. In particular, the representation of expert knowledge in tabular form, commonly found in fields such as medicine, can greatly enhance the comprehensiveness and accuracy of image-based learning. However, the transfer of knowledge from tabular to image data presents unique challenges due to the distinct characteristics of these data types, making it challenging to determine "how to reuse" and "which subset to reuse". To address this, we propose a novel method called CHannel tAbulaR alignment with optiMal tranSport (Charms) that automatically and effectively transfers relevant tabular knowledge. Specifically, by maximizing the mutual information between a group of channels and tabular features, our method modifies the visual embedding and captures the semantics of tabular knowledge. The alignment between channels and attributes helps select the subset of tabular data which contains knowledge to images. Experimental results demonstrate that Charms effectively reuses tabular knowledge to improve the performance and interpretability of visual classifiers.

## 1 Introduction

Data takes on various forms, such as images, text, video, and audio, providing rich and diverse sources of information for a given task. In contrast to using a single modality, multimodal learning aims to fuse information from different data modalities to create more comprehensive and accurate models [3; 42; 48; 67]. This approach has demonstrated exceptional performance across many domains, including recommender systems [50; 22; 2], healthcare [70; 15], and visual question answering [34; 71; 26].

In practical applications, obtaining data from multiple modalities can be challenging , as expert knowledge or specialized equipment may be required, such as medical images. The high acquisition cost of such data makes the traditional multimodal fusion approach impractical. To address this, one solution is to employ multiple modalities during training, enabling expert knowledge to transfer from one modality to another and improving the performance of a single modality during testing. Thecurrent research on crossmodal transfer primarily focuses on images and text [27; 60; 47], but limited exploration has been done with tabular data .

Tabular data is a common type of structured data, usually organized in a table format, where each column represents an attribute or feature and each row represents a sample of data . Tabular data often involves some expert knowledge, for example, in the medical field, an attribute of tabular data may represent position information in an MRI image that needs to be focused on, which requires expert annotation. Therefore, transferring expert knowledge from tables to images will improve detection efficiency and reduce the burden on doctors. However, tabular data's structured format distinguishes it from existing unstructured data such as text, making existing crossmodal transfer methods unsuitable for tabular data [29; 52].

Specifically, we face two challenges in transferring tabular knowledge for images. Firstly, we must address "how to reuse" the tabular data. As each column in tabular data has a unique semantic meaning, relying on standard RNN [19; 69] or Transformer  methods to construct a coarse feature space would result in a loss of interpretability of certain attributes. Moreover, categorical and numerical variables in tabular data require different processing methods. Secondly, we must identify "what subset to reuse" from the vast amount of information contained in tabular data since not all of it is relevant to the corresponding image. For example, in a pet adoption scenario, the tabular data contains not only the type of the pet but also information such as whether the pet is vaccinated or not. Therefore it is crucial to identify the useful information that can be transferred to instruct the learning of images. We expect that by transferring tabular knowledge to an image model, the model can learn corresponding semantics more effectively and achieve better performance on correlation tasks.

To overcome the aforementioned challenges, we propose a novel method named CHannel tAbulaR alignment with optiMal tranSport (Charms) that aligns tabular data attributes with image channels which automatically transfers relevant expert knowledge in tabular data to images. Specifically, we modify the visual embedding with the instruction of tabular data as auxiliary information and learning tabular features with a group of channels, maximizing the mutual information between them. Additionally, we utilize the optimal transport algorithm [7; 9] to match the representation of each channel with the representation of each attribute, where a distinction is made between categorical and numerical variables. We strengthen the corresponding channels to ensure a focused learning of the tabular knowledge. In this way, our approach can automatically and effectively utilize expert knowledge from tabular data in the learning process, outperforming previous methods. To summarize, our contribution is three-fold:

* We emphasize the importance of knowledge transfer from tabular data to image data, as this can lead to improved performance when tabular data is missing due to high costs.
* We propose Charms method to automatically transfers relevant tabular knowledge to images. It aligns attributes and channels by leveraging optimal transport and utilizes tabular data as auxiliary information during transfer.
* Experimental results demonstrate that Charms effectively reuses tabular knowledge to improve the performance of visual classifiers. Moreover, our approach offers insightful explanations of the learned visual embedding space with tabular instruction.

This paper is organized as follows: the related work is introduced in Section 2. Section 3 and Section 4 provide the setting formalization, discovery experiment and our method. In Section 5, we present experiment results and discuss our findings. Finally, Section 6 concludes our study results.

## 2 Related Work

**Multimodal Learning.** Data of different modalities, such as image, video, audio, and text, usually overlap in some content, while some information is complementary. Multimodal learning aims to leverage the information in different modalities to learn a better representation and improve the performance of the task for different scenarios. An important task in multimodal learning is the fusion of modalities. Some previous work used BERT [30; 53] or co-attention [34; 54] to fuse different modal information simply. Subsequently, some large models [32; 25; 31] were created to align the information of different modalities in terms of their semantic relationships using contrastive learning approach . Different pre-training approaches have also been extensively studied [4; 23; 68; 36].

**Crossmodal Transfer.** The modality fusion approach directly depends on the integrity of the data from different modalities. However, the reality is often that we do not have access to the data of all modalities. Therefore, another direction of multimodal learning is to construct robust models to cope with missing modalities or crossmodal transfer. For example, knowledge in missing modalities can be complemented using autoencoders or generative adversarial approaches [10; 43; 33]. Ma et al.  improves the robustness of Transformer models by automatically searching for an optimal fusion strategy regarding input data. Wang et al.  proposed a framework based on knowledge distillation, utilizing the supplementary information from all modalities, and avoiding imputation and noise associated with it. Hager et al.  proposes the first self-supervised contrastive learning framework that takes advantage of images and tabular data to train unimodal encoders. But most of these approaches consider Vision-Language scenarios, audio or video, which have been well investigated and are not suitable for tabular data due to their structured character and the difference between numerical and categorical variables. Our approach fills the gap of multimodal learning on tabular modality by taking it into account.

**Learning with Tabular Data.** Traditional machine learning methods have been widely used on some tabular data, such as decision trees , support vector machines , and random forests . These methods usually rely on pre-processing steps such as manual feature engineering and data cleaning, followed by model training and prediction using supervised learning. With the development of deep learning, tabular modeling approach using deep learning [62; 21; 13] is very appealing because this allows tabular data to be used as input to a single modality and trained end-to-end by gradient optimization, which is competitive with GDBT methods [12; 28; 45]. In recent years, more and more approaches for tabular data have been proposed [1; 18; 65; 24]. However, tabular data usually contains expert knowledge, such as medical diagnosis information of doctors and seismic waveform information, making it costly to acquire. So we consider such a scenario. Expert knowledge from the tabular data is used to guide the learning of the image data during training, with the expectation that good performance can be efficiently obtained even when the tabular data is missing during testing.

## 3 Preliminaries

In this section, we first introduce the crossmodal transfer task, followed by some existing methods and some analysis.

### Transfer Knowledge from Tabular to Images

Formally, we define the crossmodal transfer training dataset \(D_{train}=\{_{i}^{T},_{i}^{I},y_{i}\}_{i=1}^{N}\), where \(^{I}^{H_{0} W_{0} C_{0}}\) represent image data, \(^{T}^{D}\) represent tabular data and \(y Y\) is the label space of the task. The image data is represented as a three-dimensional tensor with height \(H_{0}\), width \(W_{0}\), and RGB channels \(C_{0}=3\), while the tabular data is a vector of dimension \(D\), where each dimension corresponds to an attribute. We define the test dataset \(D_{test}=\{_{i}^{I}\}_{i=1}^{M}\), where tabular modality is missing due to high collection cost and the need for expert annotation. During training, we aim to minimize the empirical risk of model \(f()\) over the training set:

\[_{(_{i}^{I},_{i}^{T},y_{i}) D_{train}}(f(_ {i}^{I}),y_{i}_{i}^{T}),\] (1)

where \(\) is the loss function that measures the discrepancy between prediction and ground-truth label such as cross-entropy loss and \(\) indicates conditioning on the tabular data. The model can be decomposed into embedding and linear classifier: \(f()=^{}()\), where \(():^{D}^{d}\) is the feature extractor to extract the embedding of the images and \(^{d Y}\).

Our objective is to transfer relevant tabular information into the image model \(f\). In situations where expert knowledge is not available, we expect the model to provide better predictions when only given the image data \(^{I}\) on the test set.

### Methods for Crossmodal Transfer

One of the main challenges in this task is how to transfer the tabular knowledge to the image model. It is feasible to align the two modality and then select the appropriate part for knowledge transfer. So we explore methods with alignment from different perspectives, including output-based transfer, parameter-based transfer, and embedding-based transfer.

**Output-based Transfer.** To transfer knowledge from tabular data to image models, we aim to ensure that the predictions of image model \(f\) and tabular model \(g\) are aligned. To achieve this, we first train a classifier \(g\) on the tabular data such as LightGBM . We then fit the prediction results of the image model \(f\) to \(g\) during the training. Knowledge Distillation (KD)  is an output-based method:

\[(^{I},^{T},y)=(1-)(^{I},y)+ _{}(f(^{I}),g(^{T})).\] (2)

\(_{}\) measures the similarity between the prediction of two models with Kullback-Leibler (KL) divergence \(g\) is called teacher network and \(f\) is student network. Aligning the output of the tabular model and the current model helps to reuse the knowledge in tabular data.

So as Modality Focus Hypothesis (MFH) , the modality general decisive information is set according to the feature importance [8; 63] in tabular data as the teacher network, selecting subset of the tabular data. Then only use \(_{}\) for distillation to fully observe the tabular's influence on image.

**Parameter-based Transfer.** The parameters of the model may contain part of the knowledge in the data, so the knowledge can be transferred from the perspective of the parameters of the model as well. For example, Fixed Model Reuse (FMR)  utilizes the learning power of deep models to implicitly grab the useful discriminative information from fixed models/features. In our setting, the fixed features referred to here are the tabular data:

\[=y h(f(^{I})+g(^{T}) )+\|^{T}-(^{I})\|_{F}^{2}.\] (3)

\(h\) is a soft-max operator and \(\) is the linear connections between the tabular features and embedding of images. To transfer the influence of the fixed features \(^{T}\) to images during the training procedure, FMR removes those connected parts corresponding to features \(^{T}\) gradually and finally vanish all related components with the knockdown method.

**Embedding-based Transfer.** The method expects to find a subspace in which the embedding of similar images and tabular data is as close as possible, while the embedding of dissimilar images is as far as possible. For example, Multimodal Contrastive Learning (MMCL)  proposes the self-supervised contrastive learning framework that takes advantage of images and tabular data to train unimodal encoders:

\[=_{I,T}+(1-)_{T,I}, z_{j_{ I}}=f_{_{I}}((^{I})),\] (4) \[_{I,T}=-_{j}},z_{j_{T}})/)}{_{k,k j} ((z_{j_{I}},z_{k})/)},\]

where embeddings are propagated through separate projection heads \(f_{_{I}}\) and \(f_{_{T}}\) and brought into a shared latent space as projections \(x_{j_{I}}\), \(z_{j_{T}}\). \(_{I,T}\) is calculated analagously. \(\) denotes all subjects in a batch. Then MMCL uses linear probing of frozen networks to evaluate the quality of the learned representations. By mapping tabular and image data to the same space and utilizing contrastive learning methods, the knowledge in tabular data can be transferred into an image feature extractor.

While the output-based, parameter-based, and embedding-based methods offer perspectives on transferring knowledge between modalities, each method has its own limitations. The output-based approach offers a simple and straightforward alignment based on the output of the model, but it may not capture detailed information for a certain attribute. The MFH method considers important features, but it completely discards other information during knowledge distillation. Parameter-based methods such as FMR cannot address the significant differences between tabular and image models, and the information contained in the parameters may be limited. The embedding-based approach attempts to find a common subspace for alignment but may lose some attribute information in the tabular data when changing the space, potentially ignoring valuable expert knowledge during transfer. By exploring these different transfer methods and their respective limitations, we can gain a deeper understanding of the challenges and opportunities in multimodal learning and develop more effective approaches for transferring knowledge from table to images.

## 4 Transferring Knowledge after Alignment

Motivated by the unique characteristics of tabular data, we leverage it as auxiliary information in our approach to transfer knowledge to the image modality. Specifically, we minimize the mutual information between the image and each attribute of the table data, effectively transferring the relevanttable knowledge to the image modality. Additionally, we use Optimal Transport to match the expert knowledge that can be expressed in the image data, allowing us to select a subset of the image features and strengthen the learning of the corresponding channels. Our approach highlights the importance of leveraging the specific characteristics of each modality to develop effective transfer. The flowchart is shown in Figure 1.

### Preliminary Experiments

We evaluate the quality of crossmodal transfer with MINE method, which uses mutual information, a measure of information in information theory that quantifies the amount of information contained in one variable about another . In our setting, a good image model based on tabular knowledge transfer should contain more tabular knowledge, resulting in higher mutual information both with the image and tabular data. To evaluate our approach, we conduct experiments on MFEAT dataset , using two types of tabular data: \(76\) Fourier coefficients of character shapes and \(6\) morphological features. The image modality is reconstructed from \(240\) pixel averages of images from \(2 3\) windows. The result is shown in Figure 2. The Tab-Only and Img-Only methods are the result of models trained on a single modality.

Our experiments indicate that existing methods for transferring tabular knowledge to image models yield low mutual information between the representations and tabular data. This suggests that these methods are not effective at transferring all types of tabular knowledge to the image modality and that feature selection is crucial. To validate this hypothesis, we perform knowledge distillation of the image model using two models trained on different parts of the tabular data. We find that morphological features in the tabular data can effectively promote image information, while other non-morphological features can make the tabular information more comprehensive.

These results highlight the importance of the careful selection of different tabular attributes and their relationship with the image modality. Similarly, different channels exist for the images, and the choice of different channels can also impact the final performance of the model.

Figure 1: Flow chart of Charms method. Our approach combines the learning of image and tabular data, leveraging the specific characteristics of each modality to effectively transfer knowledge from one to the other. We use Optimal Transport (OT) methods to match tabular attributes to image channels, effectively learning the correlation attribute of the tabular data with the focused channels as a means of transferring expert knowledge to the images and solving the crossmodal transfer problem.

Figure 2: Mutual Information with Different Modality in Multimodal Models. A good crossmodal transfer model should be able to effectively combine both image and tabular information, resulting in higher mutual information between the two modalities. Ideally, the model should be positioned in the upper right corner.

Since these methods do not transfer table information well, it is important to know how to use tabular knowledge. Based on these findings, we propose our method for transferring knowledge between modalities, which takes into account the specific characteristics of each modality and transfers expert knowledge to guide the image model.

### Channel Tabular Alignment

To extract the relevant information from the tabular data that is beneficial to the image model, we also use alignment-based methods for feature selection. This task consists of two main parts: first, obtaining the intermediate embedding of the image and tabular data; and second, performing alignment-based feature selection.

To extract representations of the different channels, we use convolutional neural networks (CNNs). CNNs leverage convolutional filters to scan over the input data and extract local features. By stacking multiple convolutional layers, CNNs can learn increasingly complex and abstract features, allowing us to obtain different channels that capture different aspects of the image. Specifically, the channels of image data \(^{}\) are defined as \(_{-1}(^{})^{H W C}\), where \(C\) is the number of channels, and each channel corresponds to a high-level feature such as edges, whose shape is \(H W\).

Similarly, we use a neural network to obtain the representation of each attribute of the tabular data. This involves transforming all features, including both categorical and numerical variables, into embeddings. The resulting attributes are defined as \((^{})^{D E}\), where \(D\) is the number of attributes and \(E\) is the embedding dimension. We assume that the first \(p\) attributes are numerical variables \(^{T}_{}\), and the remaining \(q\) attributes are categorical variables \(^{}_{}\).

Secondly, we use the optimal transport to align the channels of the image with the attributes of the tabular data . OT is a mathematical framework for measuring the similarity between probability distributions and finding the optimal way to transport mass from one distribution to another. The basic idea behind OT is to find a mapping between the elements of two distributions that minimizes the cost of moving one distribution to the other. The cost is typically defined as a distance metric between the elements. However, not all tabular attributes can be displayed on the image, and in some cases, there may be missing or irrelevant attributes that cannot be aligned with the image data. For example, on the PetFinder-adoption dataset, the photo of the pet can reflect the pet's hair, body size, and other attributes, but not the health condition or vaccination status. To address this issue, we use the partial optimal transport (POT) algorithm .

Specifically, To address the issue that different channels of an image may have repeated semantics with some redundancy, we use K-Means [38; 40] clustering to group similar channels together. This allows us to obtain fewer distinct channels, each capturing a distinct aspect of the image data. Then we compute the cosine similarity of the dataset on each channel, resulting in a matrix \(_{}^{C^{} N N}\), where \(C^{}\) is the number of clustered channels and \(N\) is the length of the dataset. In parallel, we process the attributes of the tabular data similarly to obtain the attribute-wise similarity matrix \(_{}^{D N N}\). Then the cost matrix is constructed from the channel-wise similarity between attribute-wise similarity. Then the OT transfer matrix is calculated:

\[_{}=\|_{_{1}}-_{_{j}} \|_{2}^{2},=*{arg\,min}_{}, _{F},\] (5)

where \(_{F}\) denotes the Frobenius norm. After aligning the distributions of the image and tabular data, we obtain the transfer matrix \(^{D C^{}}\). Based on the clustering results, we can restore the corresponding relationship between the tabular attributes and the original channels of the image as \(^{D C}\). Then the channels and attributes are aligned and relevant features are selected.

### Learning with Auxiliary Information

To leverage the knowledge of each attribute of the tabular data, we construct auxiliary tasks to learn this information. Specifically, we use the matrix \(\) to weigh the image channels, allowing us to focus the attention of the relevant tabular attributes on the corresponding image channels. We use the feature extractor of an existing image network \(()\) to learn a classifier that maps from an attention image to the corresponding attributes of the tabular data. By doing so, we enhance the image network's understanding of the attributes of the tabular data and transfer this knowledge into the image modality. This allows the learned model to handle missing tabular modalities and improve its overall performance on complex tasks.

In summary, the loss can be written in the following form

\[&=(f(^{I}),y)+ (g(^{T}),y)+_{i2t},\\ &_{i2t}=_{p}_{MSE}(_{p} (^{I}),_{p}^{T})+_{q}_{CE}(_{q}(^{I}),_{q}^{T}).\] (6)

Here, \(\) is the label prediction loss function such as cross entropy loss for classification tasks or mean square error loss for regression tasks. Since there may be numerical and categorical attributes for tabular data, we model them separately when constructing the loss to guide the image model to learn more information, expecting that the processing of different types is reasonable. \(_{CE}\) is cross entropy loss for categorical attributes and \(_{MSE}\) is mean square error loss for numerical attributes. This style of updating ensures that the model learns increasingly accurate channel-attribute correspondences, allowing the tabular data to guide the image data with increasing precision. By leveraging this approach, we can effectively transfer expert knowledge to images to develop more accurate and comprehensive image models for complex tasks.

To sum up, our method leverages OT to align the distributions of different modalities and select relevant tabular attributes that are closely related to the image data. We then use the alignment to enhance the image learning of the relevant attributes, thus transferring expert knowledge from the tabular data to the image model.

## 5 Experiments

In this section, we compare Charms with crossmodal transfer methods on several datasets. The analysis experiment and ablations verify the effectiveness of our method. Moreover, we visualized the result of the alignment of attributes and channels.

### Experiments and Results

**Dataset.** Totally six datasets are used in the experiment: **Data Visual Marketing (DVM)** is created from 335,562 used car advertisements. The tabular data includes some car parameters such as the number of doors and some advertising data such as the year. Different from , only the new version DVM dataset is available. Car models with less than 700 samples were removed, resulting in 129 target classes, a classification task. **SUNAttribute**: We use the table modality in this experiment to help images more accurately predict whether a scene is an open space, which is a binary classification task. **CelebA** is the abbreviation of CelebFaces Attribute, meaning celebrity face attribute dataset. It's a large-scale dataset with more than 200K celebrity images, each with 40 attribute annotations. We use Attractive as the label, which is a binary classification task. **PetFinder-adoption** dataset comes from a kaggle competition where the task is to predict the speed at which a pet is adopted, which is a five-class classification task. Tabular data contains information about the pet such as the type and vaccination status. **PetFinder-pap popularity** dataset also comes from a kaggle competition where the task was to predict the popularity of a pet based on that pet's profile and photo. **Avito** is a challenge to predict demand for an online advertisement based on its full description, its context and historical demand for similar ads in similar contexts. The target deal_probability can be any float from zero to one. It's also a regression task.

**Evaluation metrics.** For classification tasks, we compute accuracy to measure the performance. For the regression task, we use root mean square error (RMSE) for performance evaluation.

**Implementation Details.** In the course of the experiment, we implement CHRAMS with PyTorch and conduct experiments with a single GPU. Moreover, we utilize the grid search to find the hyper-parameters and we choose the best models from the validation set by using early stopping. Specifically, the batch size \(k\) is searched in {32, 64, 128} and the learning rate is searched in {1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3}. More details can be seen in supplementary material.

**Results.** To demonstrate the superiority of Charms, we compare it with other popular methods on six datasets as shown in Table 1. The result in the form of mean plus standard deviation are shown in supplementary. Our results show that Charms consistently achieves the best performance on all datasets. In contrast, the baseline methods we compared with do not significantly improve the performance compared to direct training with images. In fact, some of them even decrease the results. This is likely because these methods only use the tabular data to guide the image model at a coarse level, without considering the complex relationships and interactions between the modalities. As a result, the guidance provided by these methods is not sufficient for the image model to learn useful information, which can lead to confusion and poor results.

The MFH approach only learns the KL divergence between the teacher and student networks, which may not be sufficient for handling complex tasks, as evidenced by its poor performance on the DVM 129 classification task. The experiment on the regression task is one of MMCL's limitations according to .

What is particularly surprising about our approach is that it can outperform the tabular modality on the SUNAAttribute dataset. Similarly, on the CelebA and Pawpularity datasets, our approach can improve the performance of the image modality, even though the tabular data is weaker than images. It is possible that our approach can outperform the tabular modality even if it is a strong modality. These findings suggest that we indeed transfer tabular knowledge to images.

**Visualization.** To verify the effectiveness of OT in matching tabular attributes and image channels, we used GradCAM  to visualize the results of OT, as shown in Table 2. On the CelebA dataset, our model can accurately capture various table attributes for the same image. On the PetFinder-adoption dataset, we demonstrate our model's ability to recognize the same attribute across different images.

Our results demonstrate that OT is able to accurately match the image channels with the relevant tabular attributes, highlighting the validity of our approach in integrating tabular knowledge into the image model. This supports the rationale behind our approach and highlights the importance of

    & DVM \(\) & SUN \(\) & CelebA \(\) & Adoption \(\) & Pawpularity \(\) & Avito \(\) \\  LGB & 0.9748 & 0.8501 & 0.7963 & 0.4101 & 20.0720 & 0.2290 \\ RTDL & 0.9682 & 0.8563 & 0.7936 & 0.4107 & 20.0844 & 0.2317 \\ Resnet & 0.8743 & 0.8361 & 0.8146 & 0.3477 & 18.6150 & 0.2512 \\  KD & 0.8390 & 0.8382 & 0.8118 & 0.3532 & 19.0683 & 0.2499 \\ MFH & – & 0.8312 & 0.7507 & 0.3041 & 43.1455 & 0.2873 \\ FMR & 0.8427 & 0.8347 & 0.8003 & 0.3526 & 19.3517 & 0.2937 \\ MMCL & 0.8203 & 0.8431 & 0.8041 & 0.2981 & – & – \\  Charms & **0.9175** & **0.8661** & **0.8220** & **0.3603** & **18.4314** & **0.2495** \\   

Table 1: Comparisons with baseline methods on DVM, SUN, CelebA, Adoption, Pawpularity, and Avito datasets. The first four are classification tasks while the last two are regression tasks. RTDL means the FT-transformer  model trained on the tabular modality.

   Tabular Attribute & 5\_o\_Clock\_Shadow & Arched\_Eyebrows & Big\_Nose & Blond\_Hair \\  Aligned Channel & 65, 87, 119, 236... & 33, 76, 78, 115... & 50, 224, 258,... & 684 \\  Visualization & & & & & \\   Tabular Attribute & Type & Color \\  Aligned Channel & 399, 413, 414, 521... & 400, 412, 425, 448... \\  Visualization & & & & \\   

Table 2: Visualization by GradCAM. We conducted experiments on CelebA dataset and PetFinder-adoption. The results show that the OT algorithm can indeed align the tabular attributes with the image channels automatically.

carefully aligning the distributions of different modalities to effectively transfer knowledge between them.

### Experiments Analysis

**Comparison for Charms and other methods.** During the training process, we visualize the mutual information in order to understand how the mutual information changes during the training process. Specifically, we take ten models from the beginning of training to convergence and calculated the mutual information. The results are shown in Fig 3. Our results show that the mutual information in Charms increases steadily during training, demonstrating the effectiveness in transferring knowledge between modalities and improving the accuracy and interpretability of the model.

Comparing our approach with the MFH and FMR methods, we found that the MFH method initially selects important features using feature importance, leading to higher mutual information with the table, but as the model focuses more on the image information, the mutual information with the table decreases. The FMR method obtains a good initialization using the tabular data, but as the table modality is down-weighted, the mutual information with both the table and image decreases.

Overall, visualizing the mutual information provides important insights into the learning process of knowledge transfer models and can enhance the interpretability and effectiveness of these models, highlighting the importance of aligning the distributions of different modalities and transferring knowledge between them.

**The ablation study of components in Charms.** To investigate the effectiveness of the OT method in Charms, we conducted experiments where we reversed the transfer matrix of OT, expecting the image channels to learn the unaligned tabular attributes which is shown as Charms-reverse. The results are shown in Table 3, which demonstrate that the performance of Charms-reverse is significantly lower than that of our original method, Charms, highlighting the importance of OT in alignment.

To demonstrate the applicability and robustness of our Charms method, we conducted experiments using different network structures, including Densenet-121, Inception-v1, and MobileNet-v2, in addition to ResNet50. Our results, shown in Fig 4, demonstrate that the performance improvements achieved by our method are consistent across different network structures, highlighting the robustness of our approach.

Figure 4: Impact of different network structures on the method on Adoption dataset.

Figure 3: Mutual Information During Training on MVFEAT dataset. We calculate mutual information from the beginning to the convergence process in order to better understand the training process of each method.

    & DVM \(\) & SUN \(\) & CelebA \(\) & Adoption \(\) & Pawpularity \(\) & Avito \(\) \\  Charms & 0.9175 & 0.8661 & 0.8220 & 0.3603 & 18.4314 & 0.2495 \\ Charms-reverse & 0.8865 & 0.8459 & 0.8165 & 0.3440 & 18.8068 & 0.2568 \\   

Table 3: Ablation study on Optimal Transport. Charms-reverse means that we reverse the transfer matrix of OT and make channels and attributes misaligned. The performance degradation proves that alignment is important.

Conclusion

In this work, we propose the Charms, a novel method that automatically transfers relevant tabular knowledge to images. Our method leverages tabular data as auxiliary information during transfer, enabling the transfer of expert knowledge in tabular data to images. Since not all attributes contained in tabular data are relevant to the corresponding image, we utilize optimal transport to align the attributes with channels, strengthening the correlated channels during transfer. Experimental results demonstrate that Charms outperforms previous methods in crossmodal transfer and our method enables insightful explanations of the learned visual embedding space with tabular instruction. We hope this work motivates future research on the challenges of multimodal encountered in real-world problems, with a particular focus on tabular data and knowledge transfer.