# On the Importance of Exploration for

Generalization in Reinforcement Learning

 Yiding Jiang

Carnegie Mellon University

yidingji@cs.cmu.edu

&J. Zico Kolter

Carnegie Mellon University

zkolter@cs.cmu.edu

&Roberta Raileanu

Meta AI Research

raileanu@meta.com

Work done while interning at Meta AI Research.

###### Abstract

Existing approaches for improving generalization in deep reinforcement learning (RL) have mostly focused on representation learning, neglecting RL-specific aspects such as exploration. We hypothesize that the agent's exploration strategy plays a key role in its ability to generalize to new environments. Through a series of experiments in a tabular contextual MDP, we show that exploration is helpful not only for efficiently finding the optimal policy for the training environments but also for acquiring knowledge that helps decision making in unseen environments. Based on these observations, we propose Ede: Exploration via Distributional Ensemble, a method that encourages the exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. The proposed algorithm is the first value-based approach to achieve strong performance on both Procgen and Crafter, two benchmarks for generalization in RL with high-dimensional observations. The open-sourced implementation can be found at https://github.com/facebookresearch/ede.

## 1 Introduction

Current deep reinforcement learning (RL) algorithms struggle to generalize in _contextual_ MDPs (CMDPs) where agents are trained on a number of different environments that share a common structure and tested on unseen environments from the same family , despite being competitive in _singleton_ Markov decision processes (MDPs) where agents are trained and tested on the same environment . This is particularly true for value-based methods  (_i.e.,_ methods that directly derive a policy from the value functions), where there has been little progress on generalization relative to policy-optimization methods (_i.e.,_ methods that learn a parameterized policy in addition to a value function) . Most existing approaches for improving generalization in CMDPs have treated this challenge as a pure representation learning problem, applying regularization techniques which are commonly used in supervised deep learning . However, these methods neglect the unique structure of reinforcement learning (RL), namely that agents collect their own data by exploring their environments. This suggests that there may be other avenues for improving generalization in RL beyond representation learning.

Exploration can help an agent gather more information about its environment, which can improve its ability to generalize to new tasks or environments.

While this statement is seemingly intuitive, a formal and explicit connection has surprisingly not been made outside of more niche sub-areas of RL (_e.g.,_ task-agnostic RL  or meta-RL ), nor has there been a convincing empirical demonstration of this statement on common generalizationbenchmarks for RL. In this work, we show that the agent's exploration strategy is a key factor influencing generalization in contextual MDPs. First, exploration can accelerate training in deep RL, and since neural networks tend to naturally generalize, better exploration can result in better training performance and consequently better test performance. More interestingly, in singleton MDPs, exploration can only benefit decisions in that environment, while in CMDPs exploration in one environment can also help decisions in other, potentially unseen, environments. This is because learning about different parts of the environment can be useful in other MDPs even if it is not useful for the current MDP. As shown in Figure 1, trajectories that are suboptimal in certain MDPs may turn out to be optimal in other MDPs from the same family, so this knowledge can help find the optimal policy more quickly in other MDPs encountered during training, and better generalize to new MDPs without additional training.

One goal of exploration is to learn new information about the (knowable parts of the) environment so as to reduce _epistemic uncertainty_. To model epistemic uncertainty (which is reducible by acquiring more data), we need to disentangle it from aleatoric uncertainty (which is irreducible and stems from the inherent stochasticity of the environment). As first observed by Raileanu and Fergus , in CMDPs the same state can have different values depending on the environment, but the agent does not know which environment it is in so it cannot perfectly predict the value of such states. This is a type of aleatoric uncertainty that can be modeled by learning a distribution over possible values rather than a single point estimate . Based on these observations, we propose _Exploration via Distributional Ensemble_ (Ede), a method that uses an ensemble of Q-value distributions to encourage exploring states with large epistemic uncertainty. We evaluate Ede on both Procgen  and Crafter , two procedurally generated CMDP benchmarks for generalization in deep RL, demonstrating a significant improvement over more naive exploration strategies. This is the first model-free value-based method to achieve state-of-the-art performance on these benchmarks, in terms of both sample efficiency and generalization, surpassing strong policy-optimization baselines and even a model-based one. Crucially, Ede only targets exploration so it can serve as a strong starting point for future works.

To summarize, our work makes the following contributions: (i) identifies exploration as a key factor for generalization in CMDPs and supports this hypothesis using a didactic example in a tabular CMDP, (ii) proposes an exploration method based on minimizing the agent's epistemic uncertainty in high-dimensional CMDPs, and (iii) achieves state-of-the-art performance on two generalization benchmarks for deep RL, Procgen and Crafter.

## 2 Background

**Episodic Reinforcement Learning.** A _Markov decision process_ (MDP) is defined by the tuple \(=,,R,P,,\), where \(\) is the state space, \(\) is the action space, \(R:[R_{},R_{}]\) is the reward function, \(P:_{ 0}\) is the transition distribution, \((0,1]\) is the discount factor, and \(:_{ 0}\) is the initial state distribution. We further denote the trajectory of an episode to be the sequence \(=(_{0},_{0},r_{0},,_{T},_{T},r_{T},_{ T+1})\) where \(r_{t}=R(_{t},_{t})\) and \(T\) is the length of the trajectory which can be infinite. If a trajectory is generated by a probabilistic policy \(:_{ 0}\), \(Z^{}=_{t=0}^{T}^{t}r_{t}\) is a random variable that describes the _discounted return_ the policy achieves. The objective is to find a \(^{}\) that maximizes the expected discounted return, \(^{}=*{arg\,max}_{}_{ p^{}() }[Z^{}],\) where \(p^{}()=(_{0})_{t=0}^{T}P(_{t+1}_{t},_{t})(_{t}_{t})\). For simplicity, we will use \(_{}\) instead of \(_{ p^{}()}\) to denote the expectation over trajectories sampled

Figure 1: Exploration can help agents learn about parts of the environment which may be useful at test time, even if they are not needed for the optimal policy on the training environments. Note that this picture only illustrates one of many scenarios where exploration helps.

from the policy \(\). With a slight abuse of notation, we use \(Z^{}(,)\) to denote the conditional discounted return when starting at \(\) and taking action \(\) (_i.e.,_\(_{0}=\) and \(_{0}=\)). Finally, without loss of generality, we assume all measures are discrete and their values lie within \(\).

**Value-based methods** rely on a fundamental quantity in RL, the state-action value function, also referred to as the _Q-function_, \(Q^{}(,)=_{}[Z^{}_{0}=,\ _{0}=]\). The Q-function of a policy can be found at the fixed point of the Bellman operator, \(^{}\), \(^{}Q(,)=_{^{} P(| ,),\,^{}(|^{})}[R(,)+ Q(^{},^{})]\). The _value function_ for a state \(\) is defined as \(V^{}()=_{(|)}Q^{}(,)\). Bellemare et al.  extends the procedure to the distribution of discounted returns, \(^{}Z(,)}{{=}}R(, )+ Z(^{},^{}),^{} P( ,)\) and \(^{}(^{})\), where \(}{{=}}\) denotes that two random variables have the same distributions. This extension is referred to as _distribution RL_ (we provide a more detailed description of QR-DQN, the distributional RL algorithm we use, in Appendix D.1). For value-based methods, the greedy policy is directly derived from the Q-function as \(()=_{_{^{}}Q(^{},)}()\).

**Generalization in Contextual MDPs.** A _contextual Markov decision process_ (CMDP)  is a special class of _partially observable Markov decision process_ (POMDP) consisting of different MDPs, that share state and action spaces but have different \(R\), \(P\), and \(\). In addition to the standard assumptions of a CMDP, we assume the existence of a structured distribution \(\) over the MDPs. During training, we are given a (finite or infinite) number of training MDPs, \(}_{}=\{_{1},_{2},, _{n}\}\), drawn from \(\)(Ghosh et al.  refers to this setting as epistemic POMDP). For each \(\), we use \(Q^{}_{}(,)\) and \(V^{}_{}()\) to denote the Q-function and value function of that \(\). We use \(p^{,}()\) to denote the trajectory distribution of rolling out \(\) in \(\). The objective is to find a single policy \(\) that maximizes the expected discounted return over the entire distribution of MDPs, \(_{ p^{,}(),()} [_{t=0}^{T}^{t}r_{t}]\) without retraining on the unseen \(\) (_i.e.,_ zero-shot generalization). We will refer to this quantity as the _test return2_.

Since in RL the algorithm collects its own data, the appropriate notion of generalization is the performance a learning algorithm can achieve given a finite number of interactions. Crucially, if algorithm 1 achieves a better test return than algorithm 2 given the same number of interactions with \(_{}\), we can say that algorithm 1 generalizes better than algorithm 2. Furthermore, we assume the existence of \(^{*}\) (potentially more than one) that is \(\)-optimal for all \(()\) and all \(_{0}\) in \((_{})\), _i.e.,_\(V^{}_{}(_{0})_{}V^{}_{}(_{0})-\) where \(\) is small. This assumption only ensures that zero-shot generalization is intractable  but it is usually not used explicitly in algorithm design. If the number of training environments is infinite, the challenge is learning good policies for all of them in a sample-efficient manner, _i.e.,_ optimization; if it is finite, the challenge is also generalization to unseen environments.

## 3 Generalization in a Tabular CMDP

Much of the literature treats generalization in deep RL as a pure representation learning problem  and aims to improve it by using regularization  or data augmentation . In this section, we will first show that the problem of generalization in RL extends **beyond representation learning** by considering a tabular CMDP, which does not require any representation learning. The goal is to provide intuition on the role of exploration for generalization in RL using a toy example. More specifically, we show that exploring the training environments can be helpful not only for finding rewards in those environments but also for making good decisions in new environments encountered at test time.

Concretely, we consider a generic \(5 5\) grid environment (Figure 1(a)). During training, the agent always starts at a fixed initial state, \((x=0,\,y=0)\), and can move in \(4\) cardinal directions (_i.e.,_ up, down, left, right). The transition function is deterministic and if the agent moves against the boundary, it ends up at the same location. If the agent reaches the terminal state, \((x=4,\,y=0)\), it receives a large positive reward, \(r=2\) and the episode ends. Otherwise, the agent receives a small negative reward, \(r=-0.04\). At test time, the agent starts at a _different_ location, \((x=0,\,y=4)\). In other words, the train and test MDPs only differ by their initial state distribution. In addition, each episode is terminated at 250 steps (10 times the size of the state space) to speed up the simulation, but most episodes reach the terminal state before forced termination.

We study two classes of algorithms with different exploration strategies: (1) Q-learning with \(\)-greedy (Greedy, Watkins and Dayan ), (2) Q-learning with UCB (UCB, Auer et al. ). To avoid any confounding effects of function approximation, we use _tabular_ policy parameterization for Q-values. Both Greedy and UCB use the same base Q-learning algorithm . Greedy explores with \(\)-greedy strategy which takes a random action with probability \(\) and the best action according to the Q-function \(_{a}Q(s,a)\) with probability \(1-\). In contrast, UCB is uncertainty-driven so it explores according to \((a s)=(a=_{a^{}}Q(s,a^{})+c)})\), where \(t\) is the total number of timesteps, \(c\) is the exploration coefficient, and \(N(s,a)\) is the number of times the agent has taken action \(a\) in state \(s\), with ties broken randomly3. While Greedy is a naive but widely used exploration strategy , UCB is an effective algorithm designed for multi-armed bandits . Chen et al.  showed that uncertainty-based exploration bonus also performs well in challenging RL environments. See Appendix C for more details about the experimental setup.

Each method's exploration strategy is controlled by a single hyperparamter. For each hyperparameter, we search over 10 values and run every value for 100 trials. Each trial lasts for 1000 episodes. The results (mean and standard deviation) of hyperparameters with the highest average test returns for each method are shown in Figures 1(b) and 1(c). We measure the performance of each method by its _suboptimality_, _i.e.,_ the difference between the undiscounted return achieved by the learned policy and the undiscounted return of the optimal policy. While all three methods are able to quickly achieve an optimal return for the training MDP, their performances on the test MDP differ drastically. First, we observe that Greedy has the worst generalization performance and the highest variance. On the other hand, UCB can reliably find the optimal policy for the test MDP as the final return has a negligible variance4. In Appendix C.4, we provide another set of simulations based on _changing dynamics_ and make similar observations. These experiments show that **more effective exploration of the training environments can result in better generalization to new environments**.

It is natural to ask whether this example is relevant for realistic deep RL problems. Analytically, we can motivate the importance of exploration for generalization using _sub-MDPs_ and the _sample complexity_ of Q-learning . Due to space constraints, we develop this argument in Appendix A. Conceptually, the tabular CMDP with two initial state distributions captures a common phenomenon in more challenging CMDPs like Procgen, namely that at test time, the agent can often end up in

Figure 2: (a) A tabular CMDP that illustrates the importance of exploration for generalization in RL. During training, the agent starts in the **blue** square, while at test time it starts in the orange square. In both cases, the goal is to get to the green square. The other plots show the mean and standard deviation of the train and test suboptimality (difference between optimal return and achieved return) over 100 runs for (b) Q-learning with \(\)-greedy exploration, (c) Q-learning with UCB exploration.

suboptimal states that are rarely visited by simple exploration on the training MDPs. Having explored such states during training can help the agent recover from suboptimal states at test time. See Figure 3 and Appendix B for an illustrative example inspired by one of the Procgen games. This is similar to covariate shift in imitation learning where the agent's suboptimality can compound over time. The effect of efficient exploration is, in spirit, similar to that of DAgger  -- it helps the agent learn how to recover from situations that are rare during training.

In Appendix G.1, we explore some other interpretations for why exploration improves generalization (which are also related to the sample complexity of Q-learning). In addition, we also explore other potential reasons for why current RL methods generalize poorly in Appendix G.2. Note that we do not claim exploration to be the only way of improving generalization in RL as different environments may have very different properties that require distinct generalization strategies. In Appendix C.2 and C.3, we study other potential avenues for improving generalization. The benefits of exploration are complementary to these approaches, and they may be combined to further improve performance. In this paper, we focus on using exploration to improve generalization in contextual MDPs.

## 4 Exploration via Distributional Ensemble

In the previous section, we showed we can improve generalization via exploration in the tabular setting. In this section, we would like to extend this idea to deep RL with function approximation. While in the tabular setting shown above there is no intrinsic stochasticity, environments can in general be stochastic (_e.g.,_ random transitions, or random unobserved contexts). At a high level, epistemic uncertainty reflects a lack of knowledge which can be addressed by collecting more data, while aleatoric uncertainty reflects the intrinsic noise in the data which cannot be reduced regardless of how much data is collected. One goal of exploration is to gather information about states with high epistemic uncertainty  since aleatoric uncertainty cannot be reduced, but typical estimates can contain both types of uncertainties . In CMDPs, this is particularly important because a large source of aleatoric uncertainty is not knowing which context the agent is in .

In this section, we introduce _Exploration via Distributional Ensemble_ (Ede), a method that encourages the exploration of states with high epistemic uncertainty. Our method builds on several important ideas from prior works, the most important of which are deep ensembles and distributional RL. While ensembles are a useful way of measuring uncertainty in neural networks , such estimates typically contain both epistemic and aleatoric uncertainty. To address this problem, we build on Clements et al.  which introduced an approach for disentangling the epistemic uncertainty from the aleatoric uncertainty of the learned Q-values.

**Uncertainty Estimation.** Clements et al.  showed that learning the quantiles for QR-DQN  can be formulated as a Bayesian inference problem, given access to a posterior \(p()\), where \(\) is the discretized quantiles of \(Z(,)\), and \(\) is the dataset of experience on which the quantiles are esti

Figure 3: A simplified version of bigfish in Procgen (Figure 6) that captures the spirit of the grid world example from Figure 1(a). The green dot represents the agent and the red dots represent the enemies (see Appendix B for more details). The goal is to eat smaller dots while avoiding larger dots. An agent that explores both trajectories in the training MDP could recover the optimal behavior on the test MDP whereas an agent that only focuses on solving the training MDP would fail.

mated. Let \(j[N]\) denote the index for the \(j^{}\) quantile and \(\{1,N\}\) denote the uniform distribution over integers between \(1\) and \(N\). The uncertainty of the Q-value, \(Q(,)=_{j\{1,N\}}[_{j}( {s},)]\), is the relevant quantity that can inform exploration. The overall uncertainty \(^{2}=_{ p(|)} [Q(,)]\) can be decomposed into _epistemic uncertainty_\(^{2}_{}(,)\) and _aleatoric uncertainty_\(^{2}_{}(,)\) such that \(^{2}(,)=^{2}_{}(,)+^{2}_{ }(,)\), where,

\[^{2}_{}(,)=_{j\{1,N\}} [_{ p(|)}[ _{j}(,)]],\;^{2}_{}(,)=_{j\{1,N\}}[_{ p(|)}[_{j}(,) ]].\] (1)

Ideally, given an infinite or sufficiently large amount of diverse experience, one would expect the posterior to concentrate on the true quantile \(^{}\), so \(_{ p(|)}[_{j}(,)]\) and consequently \(^{2}_{}(,)=0\). \(^{2}_{}\) would be non-zero if the true quantiles have different values. Intuitively, to improve the sample efficiency, the agent should visit state-action pairs with high epistemic uncertainty in order to learn more about the environment . It should be noted that the majority of the literature on uncertainty estimation focuses on supervised learning; in RL, due to various factors such as bootstrapping, non-stationarity, limited model capacity, and approximate sampling, the uncertainty estimation generally contains errors but empirically even biased epistemic uncertainty5 is beneficial for exploration. We refer interested readers to Charpentier et al.  for a more thorough discussion.

Sampling from \(p()\) is computationally intractable for complex MDPs and function approximators such as neural networks. Clements et al.  approximates samples from \(p()\) with randomized MAP sampling  which assumes a Gaussian prior over the model parameters. However, the unimodal nature of a Gaussian in parameter space may not have enough diversity for effective uncertainty estimation . Many works have demonstrated that _deep ensembles_ tend to outperform other approximate posterior sampling techniques  in supervised learning. Motivated by these observations, we propose to maintain \(M\) copies of fully-connected value heads, \(g_{i}:^{d}^{|| N}\) that share a single feature extractor \(f:^{d}\), similar to Osband et al. . However, we train each value head with _different_ mini-batches and random initialization (_i.e.,_ deep ensemble) instead of distinct data subsets (_i.e.,_ bootstrapping). This is consistent with Lee et al.  which shows deep ensembles usually perform better than bootstrapping for estimating the uncertainty of Q-values but, unlike Ede, they do not decompose uncertainties.

Concretely, \(i[M]\) is the index for \(M\) ensemble heads of the Q-network, and \(j[N]\) is the index of the quantiles. The output of the \(i^{}\) head for state \(\) and action \(\) is \(_{i}(,)^{N}\), where the \(j^{}\) coordinate, and \(_{ij}(,)\) is the \(j^{}\) quantile of the predicted state-action value distribution for that head. The finite sample estimates of the two uncertainties are:

\[^{2}_{}(,)=_{j=1}^{N} _{i=1}^{M}(_{ij}(,)-}_{j}(,))^{2},\;^{2}_{}(,)= _{j=1}^{N}(}_{j}(,)-Q(,) )^{2},\] (2)

where \(}_{j}(,)=_{i=1}^{M}_{ij }(,)\) and \(Q(,)=_{j=1}^{N}}_{j}(,)\).

**Exploration Policy.** There are two natural ways to use this uncertainty. The first one is by using Thompson sampling  where the exploration policy is defined by sampling Q-values from the posterior:

\[_{}()=_{_{^{}}\;\;(,^{})}(),(,^{})(Q(,^{ }),\;\;_{}(,^{})).\] (3)

\(_{ 0}\) is a coefficient that controls how the agent balances exploration and exploitation. Alternatively, we can use the upper-confidence bound (UCB, Chen et al. ):

\[_{}()=_{^{}}( ),\;^{}=*{arg\,max}_{^{ }}\;Q(,^{})+\;_{}(, ^{}),\] (4)

which we found to achieve better results in CMDPs than Thompson sampling  on Procgen when we use multiple parallel workers to collect experience, especially when combined with the next technique.

**Equalized Exploration.** Due to function approximation, the model may lose knowledge of some parts of the state space if it does not see them often enough. Even with UCB, this can still happen after the agent learns a good policy on the training environments. To increase the data diversity, we propose to use different exploration coefficients for each copy of the model used to collect experience. Concretely, we have \(K\) actors with synchronized weights; the \(k^{}\) actor collects experience with the following policy:

\[^{(k)}_{}()=_{^{}}(),\ ^{}=*{arg\,max}_{^{}}\,Q(,^{ })+(\,^{1+})_{ }(,^{}).\] (5)

\((0,1)\) and \(_{>0}\) are hyperparameters that control the shape of the coefficient distribution. We will refer to this technique as _temporally equalized exploration_ (TEE). TEE is inspired by Horgan et al.  which uses different values of \(\) for the \(\)-greedy exploration for each actor. In practice, the performances are not sensitive to \(\) and \(\) (see Figure 18 in Appendix G). Both learning and experience collection take place on a single machine and no prioritized experience replay  is used since prior work found it ineffective in CMDPs .

To summarize, our agent explores the environment in order to gather information about states with high epistemic uncertainty which is measured using ensembles and distributional RL. We build on the algorithm proposed in Clements et al.  for estimating the epistemic uncertainty, but we use deep ensemble instead of MAP , and use either UCB or Thompson sampling depending on the task and setting6. In addition, for UCB, we propose that each actor uses a different exploration coefficient for more diverse data. While variations of the components of our algorithm have been used in prior works, this particular combination is new (see Appendix E). Our ablation experiments show that each design choice is important and **applying these techniques individually or naively combining them performs significantly worse.**

## 5 Experiments

### Procgen

We compare our method with 3 representative baselines on the standard Procgen benchmark (_i.e.,_ 25M steps and easy mode) as suggested by Cobbe et al. : (1) QR-DQN which is the prior state-of-the-art value-based method on Procgen ; (2) PPO which is a popular policy optimization baseline on which most competitive methods are built; and (3) IDAAC which is state-of-the-art on Procgen and is built on PPO. We tune all hyperparameters of our method on the game bigfish only and evaluate all algorithms using the 4 metrics proposed in Agarwal et al. . We run each algorithm on every game for 5 seeds and report the aggregated min-max normalized scores on the full test distribution, and the estimated bootstrap-estimated 95% confidence interval in Figure 4 (simulated with the runs). Our approach significantly outperforms the other baselines in terms of median and interquartile mean (IQM) (which are the more statistically robust metrics according to Agarwal et al. ). In particular, it achieves almost 3 times the median score of QR-DQN and more than 2 times the IQM of QR-DQN. In terms of mean and optimality gap, our method is competitive with IDAAC and outperforms all the other baselines. To the best of our knowledge, this is the first-value-based method that achieves such strong performance on Procgen. See Appendices I, F, and J for more details about the experiments, the performance on individual games, hyperparameters, and sensitivity analysis.

**Ablations and Exploration Baselines.** We aim to better understand how each component of our algorithm contributes to the final performance by running a number of ablations. In addition, we

Figure 4: Test performance of different methods on the Procgen benchmark across 5 runs. Our method greatly outperforms all baselines in terms of median and IQM (the more statistically robust metrics) and is competitive with the state-of-the-art policy optimization methods in terms of mean and optimality gap. The optimality gap is equal to 1-mean for the evaluation configuration we chose.

compare with other popular exploration techniques for value-based algorithms. Since many of the existing approaches are designed for DQN, we also adapt a subset of them to QR-DQN for a complete comparison. The points of comparison we use are: (1) Bootstrapped DQN  which uses bootstrapping to train several copies of models that have distinct exploration behaviors, (2) UCB  which uses an ensemble of models to do uncertainty estimation, (3) \( z\)-greedy exploration  which repeats the same random action (following a zeta distribution) to achieve temporally extended exploration, (4) UA-DQN , and (5) NoisyNet  which adds trainable noise to the linear layers 7. When UCB is combined with QR-DQN, we use the epistemic uncertainty unless specified otherwise. The results are shown in Figure 5. Details can be found in Appendix I.

First, note that both using the epistemic uncertainty via UCB and training on diverse data from TEE are crucial for the strong performance of Ede, with QR-DQN+TEE being worse than QR-DQN+UCB which is itself worse than Ede. Without epistemic uncertainty, the agent cannot do very well even if it trains on diverse data, _i.e.,_ Ede is better than QR-DQN+TEE. Similarly, even if the agent uses epistemic uncertainty, it can still further improve its performance by training on diverse data, _i.e.,_ Ede is better than QR-DQN+UCB.

Both Bootstrapped DQN and DQN+UCB, which minimize the total uncertainty rather than only the epistemic one, perform worse than DQN, although both are competitive exploration methods on Atari. This highlights the importance of using distributional RL in CMDPs in order to disentangle the epistemic and aleatoric uncertainty. QR-DQN+UCB on the other hand outperforms QR-DQN and DQN because it _only_ targets the (biased) epistemic uncertainty. In Figure (c)c from Appendix G, we show that indeed exploring with the total uncertainty \(^{2}\) performs significantly worse at test time than exploring with only the epistemic uncertainty \(^{2}_{}\). UA-DQN also performs worse than QR-DQN+UCB suggesting that deep ensemble may have better uncertainty estimation (Appendix H).

QR-DQN with \( z\)-greedy exploration marginally improves upon the base QR-DQN, but remains significantly worse than Ede. This may be due to the fact that \( z\)-greedy exploration can induce temporally extended exploration but it is not aware of the agent's uncertainty. Not accounting for uncertainty can be detrimental since CMDPs can have a much larger number of effective states than singleton MDPs. If the models have gathered enough information about a state, further exploring that state can hurt sample efficiency, regardless of whether the exploration is temporally extended.

NoisyNet performs better than the other points of comparison we consider but it is still worse than Ede. The exploration behaviors of NoisyNet are naturally adaptive -- the agents will take into account what they have already learned. While a direct theoretical comparison between NoisyNet and our method is hard to establish, we believe adaptivity is a common thread for methods that perform well on CMDPs. Nonetheless, if we consider IQM, none of these methods significantly outperforms one another whereas our method achieves a much higher IQM. Note that TEE cannot be easily applied to NoisyNets and \( z\)-greedy which implicitly use an schedule.

Figure 5: Test performance of different exploration methods on the Procgen benchmark across 5 runs.

### Crafter

To test the generality of our method beyond the Procgen benchmark, we conduct experiments on the Crafter environment . Making progress on Crafter requires a wide range of capabilities such as strong generalization, deep exploration, and long-term reasoning. Crafter evaluates agents using a scalar score that summarizes the agent's abilities. Each episode is procedurally generated, so the number of training environments is practically infinite. While Crafter does not test generalization to new environments, it still requires generalization across the training environments in order to efficiently train on all of them. We build our method on top of the Rainbow implementation  provided in the open-sourced code of Hafner  and _only_ modified the exploration.

We use Thompson sampling instead of UCB+TEE since only one environment is used and Thompson sampling outperforms UCB by itself in this environment. As seen in Table 1, our algorithm achieves significantly higher scores compared to all the baselines presented in Hafner , including DreamerV2  which is a state-of-the-art model-based RL algorithm. It is also competitive with LSTM-SPCNN  which uses a specialized architecture that does not have spatial pooling with two orders of magnitude more parameters and extensive hyperparameter tuning. The significant improvement over Rainbow, which is a competitive value-based approach, suggests that the exploration strategy is crucial for improving performance on such CMDPs.

## 6 Related Works

**Generalization in RL.** A large body of work has emphasized the challenges of training RL agents that can generalize to new environments and tasks [93; 67; 50; 84; 118; 121; 77; 21; 22; 49; 58; 36; 18; 12; 13; 35; 53; 3; 26; 66]. This form of generalization is different from generalization in singleton MDP which refers to function approximators generalizing to different states within the same MDP. A natural way to alleviate overfitting is to apply widely-used regularization techniques such as implicit regularization , dropout , batch normalization , or data augmentation [116; 61; 60; 92; 112; 115; 40; 41; 54]. Another family of methods aims to learn better state representations via bisimulation metrics [120; 119; 1], information bottlenecks [45; 28], attention mechanisms , contrastive learning , adversarial learning [95; 34; 89], or decoupling representation learning from decision making [103; 99]. Other approaches use information-theoretic approaches [18; 71], non-stationarity reduction [46; 78], curriculum learning [47; 109; 48; 85], planning , forward-backward representations , or diverse policies . More similar to our work, Raileanu and Fergus  show that the value function can overfit when trained on CMDPs and propose to decouple the policy from the value optimization to train more robust policies. However, this approach cannot be applied to value-based methods since the policy is directly defined by the Q-function. Most of the above works focus on policy optimization methods, and none emphasizes the key role exploration plays in training more general agents. In contrast, our goal is to understand why value-based methods are significantly worse on CMDPs.

**Exploration.** Exploration is a fundamental aspect of RL [55; 33; 108]. Common approaches include \(\)-greedy , count-based exploration [104; 9; 68], curiosity-based exploration [97; 102; 86], or novelty-based methods specifically designed for exploring sparse reward CMDPs [91; 117; 30; 122]. These methods are based on policy optimization and focus on training agents in sparse reward CMDPs. In contrast, we are interested in leveraging exploration as a way of improving the generalization of value-based methods to new MDPs (including dense reward ones).

Some of the most popular methods for improving exploration in value-based algorithms use noise [82; 32], bootstrapping [79; 81], ensembles [19; 61; 64], uncertainty estimation [80; 83; 20], or distributional RL . This class of method implements the principle of _"optimism in the face of uncertainty_". Ede also falls under this broad class of algorithms. The main goal of these works is to balance return maximization (_i.e.,_ exploitation) and exploration to improve sample efficiency on singleton MDPs that require temporally extended exploration. For generalization, both over-exploration and over-exploitation would result in poor generalization in addition to poor training performance, so methods that balance exploration and exploitation would likely be preferred.

  
**Method** & **Score (\%)** \\  Ede (ours) & \(11.7 1.0\) \\  Rainbow & \(4.3 0.2\) \\ PPO & \(4.6 0.3\) \\ DreamerV2 & \(10.0 1.2\) \\ LSTM-SPCNN & \(12.1 0.8\) \\   

Table 1: Results on Crafter after 1M steps and over 10 runs.

Another related area is task-agnostic RL [88; 123] where the agent explores the environment without reward and tries to learn a down-stream task with reward, but to our knowledge, these methods have not been successfully adapted to standard benchmarks like Procgen. Our work is the first one to highlight the role of exploration for faster training on contextual MDPs and better generalization to unseen MDPs. Our work also builds on the distributional RL perspective , which is useful in CMDPs for avoiding value overfitting .

## 7 Conclusion

In this work, we study how exploration affects an agent's ability to generalize to new environments. Our tabular experiments indicate that effective exploration of the training environments is crucial for generalization to new environments. In CMDPs, exploring an environment is not only useful for finding the optimal policy in that environment but also for acquiring knowledge that can be useful in other environments the agent may encounter at test time. To this end, we propose to encourage exploration of states with high epistemic uncertainty and employ deep ensembles and distributional RL to disentangle the agent's epistemic and aleatoric uncertainties. This results in the first value-based based method to achieve state-of-the-art performance on both Procgen and Crafter, two benchmarks for generalization in RL with high dimensional observations. Our results suggest that exploration is important for all RL algorithms trained and tested on CMDPs. While here we focus on value-based methods, similar ideas could be applied to policy optimization to further improve their generalization abilities. In a broader context, it is perhaps important to emphasize the exploration is not the only piece of puzzle for generalization in RL. There are still environments in Procgen where Ede does not significantly improve the performance even compared to QR-DQN (Figure 13), which indicates that there are other bottlenecks beyond poor exploration. Another limitation of our approach is that it is more computationally expensive due to the ensemble (Appendix H). Thus, we expect it could benefit from future advances in more efficient ways of accurately estimating uncertainty in neural networks.