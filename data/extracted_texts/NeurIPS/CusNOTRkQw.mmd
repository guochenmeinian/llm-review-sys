# Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization

Jameel Hassan\({}^{1}\)1  Hanan Gani\({}^{1}\)1  Noor Hussein\({}^{1}\)  Muhammad Uzair Khattak\({}^{1}\)

Muzammal Naseer\({}^{1}\)  Fahad Shahbaz Khan\({}^{1,2}\)  Salman Khan\({}^{1,3}\)

\({}^{1}\)Mohamed Bin Zayed University of AI \({}^{2}\)Linkoping University \({}^{3}\)Australian National University

{jameel.hassan, hanan.ghani, noor.hussein, uzair.khattak

muazammal.naseer, fahad.khan, salman.khan} @mbzuai.ac.ae

Equal contribution

###### Abstract

The promising zero-shot generalization of vision-language models such as CLIP has led to their adoption using prompt learning for numerous downstream tasks. Previous works have shown test-time prompt tuning using entropy minimization to adapt text prompts for unseen domains. While effective, this overlooks the key cause for performance degradation to unseen domains - distribution shift. In this work, we explicitly handle this problem by aligning the out-of-distribution (OOD) test sample statistics to those of the source data using prompt tuning. We use a single test sample to adapt multi-modal prompts at test time by minimizing the feature distribution shift to bridge the gap in the test domain. Evaluating against the domain generalization benchmark, our method improves zero-shot top-1 accuracy beyond existing prompt-learning techniques, with a \(3.08\%\) improvement over the baseline MaPLe. In cross-dataset generalization with unseen categories across 10 datasets, our method improves consistently across all datasets compared to the existing state-of-the-art. Our source code and models are available at https://jameelhassan.github.io/promptalign/.

## 1 Introduction

Deep neural networks (DNNs) have outperformed humans in numerous visual recognition tasks [12; 8]. However, their impressive performance generally holds in the case when test data originate from the same distribution as the training data. In most real-world applications, the train and test data distributions can significantly differ due to factors such as natural variations or changes in sensing equipment. The sensitivity of models to such unseen distributional shifts during inference results in their performance degradation [14; 31; 29]. A plethora of previous works [36; 39; 37] have explored test-time adaptation as a mechanism to overcome the crucial problem of distribution shift in test data. However, test-time adaptation has been minimally explored for the increasingly popular foundation models, which are large DNNs trained on massive vision-language datasets [30; 17; 44; 40; 41].

Foundation models emerging at the intersection of various modalities such as vision, language, and audio, are proving to be effective in numerous downstream applications. Among these models, Vision-Language (V-L) model CLIP (Contrastive Language-Image Pretraining)  has been pre-trained on large-scale image-text pairs from the web, and can generalize well to zero-shot recognition tasks. The CLIP model has parallel vision and language encoding branches and the similarities between the embeddings obtained from both the branches are used to classify an input image. At inference, a handcrafted prompt such as 'a photo of a <CLS>' is used as a query for the text encoder. However, adapting CLIP efficiently to specific downstream tasks is still a challenging problem. Naively fine-tuning such models poses the risk of losing their inherent generalization abilities . Instead, recentmethods have shown the promise of prompt learning on training data [48; 47; 18; 19; 42] as opposed to handcrafted prompts, allowing the model to better adapt to the training data distribution.

Existing prompt learning approaches are deployed at the training phase to learn representative prompts based on the training data for the downstream task. This conventional approach does not explicitly handle the distribution shift in the test set. Leveraging the capability of prompt learning, a recent approach TPT  performs test-time adaptation by tuning the text prompts on the fly to adapt the model to the test sample. For image classification, the model updates the prompts by minimizing the entropy of the top confident samples which are obtained using different augmented views. However, TPT does not explicitly align the pre-trained CLIP to become aware of the test sample distribution.

For the effective test-time adaptation of V-L foundation models, it is crucial to bridge the distribution gap between the pre-training dataset and the downstream evaluation set for high zero-shot generalization. To this end, we propose PromptAlign, a test-time token distribution alignment strategy using prompt learning. The TPT setting, which tunes the prompts on the text encoder branch alone poses an architectural limitation in performing distribution alignment of tokens. Further, at test time, there is no knowledge transfer between the text and vision branches in the intermediate stages. Since the text encoder features will be static given a dataset (as the input is the same class labels), distribution alignment of tokens can only be performed on the vision branch. Given these constraints and to further extend the strength of prompts for test time adaptation, we propose to employ a multi-modal prompt learning model (MaPLe)  for distribution alignment. PromptAlign explicitly aligns the mean and variances of the image token embeddings of a proxy source dataset, computed offline, with the image token embeddings of the test sample. We extend TPT with the token alignment strategy, which enforces to bridge the distribution shift in the test data (Fig. 1 a). For each input test sample, we obtain randomly augmented views that are fed into the model to obtain the token embedding statistics. Without any noticeable compute overhead, we update the prompts on both the text and vision branches of CLIP to minimize jointly the feature distribution shift and entropy of predictions.

We extensively demonstrate the effectiveness of PromptAlign by evaluating the zero-shot generalization on two representative benchmarks: domain generalization and cross-dataset generalization. In the domain generalization setting, our method improves the baseline model by \(3.08\%\) on average across the four datasets and has the highest average Top-1 accuracy compared to existing state-of-the-art methods. In the cross-dataset setting, our method achieves an absolute average improvement of \(1.82\%\) over the existing state-of-the-art method which uses test-time prompt tuning, while attaining the best Top-1 accuracy in \(8\) out of the \(10\) datasets. Our contributions can be summarized as follows:

Figure 1: (a) PromptAlign matches the distribution statistics \(_{l}(;)\), \(_{l}^{2}(;)\), obtained from multiple augmented views of a single test sample, with the source data distribution statistics \(}_{l}\), \(}_{l}^{2}\). This effectively brings the test sample closer to the distribution of the source data, where the domain shift is denoted by \(}}}}}}}{}}{}}{}}{}}{}}{}}}}}}\). \(\) denotes the distribution of the test sample, \(\) represents the prompts that are updated and \(l\) refers to the vision-backbone layers. (b) Owing to the distribution matching via prompts, PromptAlign surpasses the existing state-of-the-art prompt learning approaches on 8 out of 10 datasets in cross-dataset generalization benchmarks.

* Given only a single test sample, we introduce a distribution alignment strategy for V-L models to improve test-time adaptation. The distribution-aware pre-trained CLIP effectively narrows the distribution gap on the test domains. To the best of our knowledge, this is the first study to explore the potential of distribution alignment in V-L models at test time.
* The proposed strategy formulates a distribution alignment loss that utilizes offline computed source data statistics to encourage the test sample token distributions to be aligned with the source data token distributions. We harmonically combine the benefits of token distribution alignment and entropy minimization using a multi-modal prompt learning approach.
* Since CLIP-pre-training data is not publicly released, we study the statistics of ImageNet as a possible candidate for the source distribution, and our empirical results show that ImageNet is an effective proxy source dataset for large-scale V-L models such as CLIP.
* We validate our method PromptAlign through extensive experiments in domain generalization and cross-dataset benchmarks. PromptAlign improves the generalization of CLIP at test time beyond existing prompt-tuning methods, achieving state-of-the-art results.

## 2 Related Work

**Prompting for Vision-Language models.** Vision Language (V-L) foundation models [30; 17; 44; 40; 41] have emerged with the convergence of image and text modalities. Having been pre-trained on massive image-text pairs from the web in a contrastive self-supervised manner, these models like CLIP  and ALIGN  have shown strong generalizability towards downstream zero-shot recognition tasks. However, adapting them efficiently to specific downstream tasks with limited data is still a challenging problem. Prompting in CLIP-like models has been explored in the form of a text query that is usually given to the text encoder to instruct it to perform a specific task. Recent methods propose to learn these prompts by treating them as continuous learnable vectors and training them in an end-to-end manner while keeping the model parameters frozen. CoOp  proposes to fine-tune CLIP by learning a set of prompts in the text encoder. CoCoOp  highlights the inferior generalization capability of CoOp and conditions the text prompt tokens on image embeddings on the fly. MaPLe  proposes to jointly learn deep prompts at both vision and text encoders of CLIP. The vision prompts are further conditioned on text prompts via a V-L coupling function. While all these approaches promote the effective transfer of CLIP, they require training data to learn the prompts which restricts such adaptation on novel datasets at test time. A recent method TPT , attempts to learn prompts solely at test time with the objective of enforcing consistency regularization between multiple views of a test sample by minimizing their averaged entropy. However, this approach struggles to explicitly address the distribution misalignment between the pre-training data of CLIP and the downstream test data. Our method builds on multi-modal prompting variant  and explicitly enforces to match the distribution via a distribution alignment technique using a proxy dataset that serves as a replacement for unavailable pre-training data of CLIP. To the best of our knowledge, ours is the first work to explicitly align the distributions learned by V-L foundational models with test time data distribution.

**Test-Time Adaptation (TTA).** TTA [36; 37; 23] aims to bridge the distribution gap between the train and test data distributions at test time. These methods can broadly be categorized into two streams. The first stream of approaches [11; 36] typically utilizes a self-supervised proxy task such as predicting image rotations and thus alters the training phase. For example, Test-Time Training (TTT)  jointly trains the model for rotation prediction and image classification during training. TTT++  adopts a self-supervised contrastive learning approach as an auxiliary task during training and test-time feature alignment module to adapt the model. By incorporating additional auxiliary tasks, these techniques aim to mitigate over-fitting and improve the generalization performance of the model. The second stream of approaches solely adopts the pre-trained model without altering the training phase. These methods improve model generalization at test time by imposing self-consistency regularization constraints or aligning statistics of training and test samples in a model. Approaches with self-consistency regularization constraints typically utilize entropy minimization within samples of a batch or multiple views of a single test sample. TENT  proposes entropy minimization of batch-wise prediction probability distributions. The need to have a batch of samples by generating multiple views via augmentations at test time is eliminated in . Other methods, such as NORM  and DUA  adapt the model's normalization layers by matching the batch normalization statistics computed during training with the test set distribution. Both  and ActMAD updates all model parameters using a batch of samples in a continuous manner, with a distribution matching loss between the source statistics and the batch statistics of activations across the network. However, it is unclear how these statistics alignment techniques can be adopted for foundational V-L models in a lightweight manner for a single sample. In this paper, we propose a multi-modal test time prompting approach without altering the pre-training phase of V-L models along with a strategy to obtain training distribution statistics. This effectively paves the way for distribution alignment together with entropy minimization for improved test-time optimization.

**Test-time prompt tuning.** As mentioned earlier, a plethora of works has been introduced on learning prompts to adapt V-L models using the downstream task training data. However, the learning of prompts at test time remains largely unexplored in the literature. Very recently, TPT  proposed a test time prompting approach by learning prompts at the text side with an entropy minimization objective. While effective, TPT struggles with addressing the explicit alignment of the pre-training data distribution of CLIP with the test data distribution. Further, such an alignment is not trivial as access to the pre-training data of CLIP is not available. Our work addresses these limitations by extending TPT and introducing a distribution alignment objective for CLIP. This objective leverages token distribution statistics from proxy source data and a _single_ test sample using multi-modal prompt learning, and explicitly aligns the train and test sample distributions to mitigate the domain shift.

## 3 Methodology

We provide an overview of CLIP  and prompt learning and their application for zero-shot generalization to downstream tasks in Section 3.1. We then discuss test-time prompt tuning and briefly review previous approaches. Finally, we introduce PromptAlign in Section 3.2 and provide a detailed explanation of how we apply it to achieve improved zero-shot image classification.

Figure 2: **Overview of our proposed PromptAlign method for zero-shot image classification. At test time, a single test sample along with its augmented views is passed through the CLIP image encoder, and the text labels are passed to the CLIP text encoder. The token distribution statistics – mean and variance – of the test sample are aligned with the offline computed source data statistics using a distribution alignment loss. The resulting alignment loss from the distribution shift is combined with the entropy loss to update the multi-modal prompts.**

### Preliminaries

#### 3.1.1 Contrastive Language-Image Pre-training (CLIP).

CLIP consists of two parallel encoders, one for mapping the text input into a feature vector and the other for mapping the visual input. We denote the CLIP image and text encoders by \(_{v}\) and \(_{t}\) respectively, and their pre-trained parameters are represented by \(_{}=\{_{v},_{t}\}\) respectively. The input image \(X\) is divided into \(M\) patches, which are projected to produce patch tokens, and a class token cls is prepended to it, resulting in \(}_{0}=\{,_{1},_{2},,_{M}\}\) where \(e_{i}\) is the embedding of the \(i^{th}\) patch. The image encoder \(_{v}\) encodes the input patches via transformer blocks to produce a latent visual feature representation \(_{v}}=_{v}(}_{0},_{v})\). The corresponding class label \(y\) is embedded within a text template, such as 'a photo of a <cls>', which is formulated as \(}_{0}=\{,_{1},_{2},,_{L}, {c}_{k},\}\). Here, \(_{i}|_{l=1}^{L}\) and \(_{k}\) are the word embeddings corresponding to the text template and the class label, respectively, while sos and eos are the start and end token embeddings. The text encoder \(_{t}\) encodes \(}\) via transformer blocks to produce the latent textual feature as \(_{t}}=_{t}(}_{0},_{t})\). For zero-shot inference, each text feature with class labels \(y=\{1,2,,C\}\) is paired with the image feature to compute a similarity score \(}=(_{t}}_{v}})\), where \((.)\) denotes the cosine similarity. The prediction probability on \(X\) can be denoted by \(p(y_{i}|X)=((_{t}} _{v}}))}{_{i=1}^{K}((_{t}} _{v}}))}\), where \(\) is the temperature of the softmax.

#### 3.1.2 Prompt tuning on downstream tasks.

CLIP contains a plethora of knowledge leveraged from training on millions of noisy image-text pairs. To effectively extract the rich features learned by the CLIP model, recent approaches [48; 47; 18; 2; 42] append extra learnable prompts while keeping image and text encoders frozen. These prompts modify the context of the model input without distorting the pre-trained CLIP features. Prompts are appended either at the image or text encoder side and learn contextual information tailored towards a specific task. In our work, we use a recently introduced multi-modal prompting baseline  that learns prompt tokens on both the text and image encoders.

Specifically, we append learnable \(V\) visual and \(T\) text prompts given as \(}=\{^{1}},^{2}},,^{V}}\}\), and \(}=\{^{1}},^{2}},,^{T}}\}\) with the visual and textual input tokens respectively. The image encoder processes the input \(}_{0}^{}=\{,},_{1},_{2}, ,_{M}\}\) to generate a prompted visual feature representation denoted as \(_{v}^{}}\). Similarly, the text encoder processes the input \(_{0}^{}}=\{,},_{1},_{2}, ,_{L},_{k},\}\) producing the textual feature representation \(_{t}^{}}\). Our approach uses deep prompting as utilized in , along with text prompts and conditional image prompts at subsequent transformer blocks. We jointly represent the visual and textual prompts by \(\). We refer the readers to  for more details on the baseline architecture.

#### 3.1.3 Test-time prompt tuning.

Test-time prompt tuning introduced by  aims to benefit from the rich knowledge of CLIP to boost its generalization in a zero-shot manner. TPT can be viewed as a means to provide the model with a context that is customized for each individual test sample in order to more accurately recall the knowledge contained within CLIP. During inference, several randomly augmented views are generated from the given test sample \(X_{}\). The predictions having entropy below a certain threshold are kept, while other views are discarded using a confidence selection filter. The averaged entropy of the filtered predictions is then used to update the prompts \(\) in an unsupervised fashion using the following objective function.

\[_{}=_{}-_{i=1}^{C}_ {}(y_{i}|X_{})_{}(y_{i}|X_{}),\] (1)

where \(_{}(y_{i}|X_{})\) represents the mean of vector class probabilities produced by the model across the different augmented views preserved after the confidence selection filter.

### PromptAlign

Unimodal Test-time Prompt Tuning (TPT)  updates the text prompts on the fly at inference by minimizing the entropy loss. This approach does not explicitly handle distribution shift that arises in the test set which is sub-optimal. One solution to this problem is to align the source and target distributions by bringing the test sample into the domain of the source data distribution. However, TPT updates the prompts only on the text encoder branch with static labels which poses an architectural limitation in performing the distribution alignment of tokens. Hence, distribution alignment of tokenscan only be performed on the vision branch. We, therefore, utilize multi-modal prompt learning VL models  to explicitly handle the distribution shift of each test sample from the source distribution.

Given a test sample \(X_{}\), we take multiple augmented views and pass them through the visual encoder with deep learnable prompts as shown in Fig. 2. At each layer of the visual encoder, we compute the token alignment loss between the means and variances of the test sample with that of the means and variances of a proxy dataset which mimics the behavior of the source data distribution. Our final objective combines the entropy and alignment losses to update the prompts for a given test sample.

**Proxy source dataset.** In order to compute the token embedding statistics on the source dataset, we require the pre-training dataset of the CLIP model. The CLIP model was trained on over 400 million image-text pairs, which is not publicly available. However, previous works have shown that LAION400M  can be used as a training dataset to achieve the performance of CLIP , leading to subsets of LAION400M being used as a proxy dataset for the CLIP training dataset. In addition to this, CLIP has also been heavily tuned to achieve excellent zero-shot performance on ImageNet . Therefore, we use ImageNet as the proxy source dataset for computing the mean and variance of token distributions. Source dataset statistics are computed offline and utilized directly at test time.

**Token Distribution Alignment via multi-modal prompting.** We generate \(N_{k}\) random views of the test sample using a set of augmentations \(\). The mean and variance statistics of token embeddings of the test sample are computed at the output of each transformer layer of the CLIP model's visual encoder, across the \(N_{k}\) views. Similarly, the source data statistics are pre-computed in an offline manner. We represent test sample distribution by (\(\)) and source distribution by (\(\)). Specifically, we compute the token means and variances for the alignment as follows.

\[_{l}(;)=}_{ (X)}}_{l,}^{},\] (2)

\[_{l}^{}(;)=}_{ (X)}}_{l,}^{}- _{l}(;)^{2},\] (3)

where \(_{l}(;)\) and \(_{l}^{}(;)\) are the vector means and variances of the test sample tokens at the layer \(l\) in the visual encoder and \(}_{l,}^{}\) represents the prompted token embeddings at layer \(l\) for the augmented view input \(\). Similarly for each layer \(l\) in the visual encoder, we pre-compute the source data statistics as,

\[}_{l}=_{l}(,_{v}) }_{l}^{}=_{l}^{}( ,_{v}),\] (4)

where \(_{v}\) denotes the parameters of the visual encoder from the pre-trained CLIP model. We compute the token distribution alignment loss between the mean and variances of the test sample and the source dataset statistics as follows,

\[_{}=_{l=1}^{L}\| _{l}(;)-}_{l}\|_{1}+\|_{l}^{ }(;)-}_{l}^{}\|_{1}.\] (5)

As shown above, we use \(L_{1}\) loss to enforce the distribution alignment of the test sample with the source distribution. The alignment loss \(_{}\) is added to the entropy loss (Eq. 1) to obtain the final objective \(_{}\). Finally, the combined objective is optimized to update the prompts \(\).

\[_{}=_{}+ _{},\] (6)

where the hyperparameter \(\) controls the contribution of alignment loss to the overall objective function. Our alignment loss in Eq. 6 bridges the gap between source and token distributions by updating the prompts to align them with the source distribution.

**Discussion on \(_{}\).** As derived above, our test-time loss combines the entropy minimization (\(_{}\)) and distribution alignment objectives (\(_{}\)) together. The \(_{}\) objective enforces prediction consistency among different views of a sample, leading to robustness against various variations that could possibly occur at test time. On the other hand, \(_{}\) effectively narrows the domain shift and brings the test sample distribution closer to the pre-trained CLIP distribution space, which enforces CLIP to better understand the test sample. The combination of these loss objectives harmonically adapts CLIP to be robust to different sample variations and at the same time, enhances CLIP's understanding of the underlying test sample domain for better generalization.

## 4 Experiments

**Datasets.** For the domain generalization setting, we evaluate the four out-of-distribution (OOD) variants of ImageNet ; ImageNetV2 , ImageNet-Sketch , ImageNet-A  and ImageNet-R . We also evaluate the domain generalization setting on the recent challenging Photorealistic Unreal Graphics (PUG) dataset , comprising different textures, backgrounds, sizes and orientations. In the cross-dataset evaluation, we follow TPT  and evaluate the performance of methods on 10 image classification datasets covering a wide range of visual recognition tasks. This includes one generic-objects dataset Caltech101 ; five fine-grained datasets OxfordPets , StanfordCars , Flowers102 , Food101  and FGVC-Aircraft , which contain images of animals, flowers and transportation; and four datasets of scenes, textures, satellite imagery and human actions - SUN397 , DTD , EUROSAT  and UCF101  respectively.

**Baselines.** We evaluate PromptAlign with existing few-shot prompt learning methods for adapting CLIP including CoOp  and CoCoOp , and TPT  method. MaPLe  is a multi-modal prompt learning baseline, which adapts CLIP by learning deep prompts on both the text and vision branches. TPT is a test-time prompt tuning method, that tunes the prompt at test time per input sample achieving state-of-the-art performance in prompt learning when combined with CoOp.

**Implementation details.** Following MaPLe , we train on ImageNet using \(16\)-shot training data with \(2\) prompt tokens for a depth of \(3\) layers. We optimize the prompts on both the text and vision branches using a single test image. We obtain 63 augmented views using random resized crops and horizontal flip augmentations to construct a batch of 64 images including the original image to mimic the setting of TPT. From the 64 predictions, we obtain top \(10\%\) confident predictions based on the lowest entropy and compute the average prediction probability. We compute the token distribution alignment loss between the tokens of all 64 images. We optimize the prompts to minimize the combined loss of average prediction entropy and the token distribution alignment loss using the AdamW optimizer. We use a learning rate of \(5e^{-4}\) for the fine-grained datasets Flowers102, OxfordPets, Food101, SUN397, FGVCircraft, and EuroSAT and a learning rate of \(0.04\) for the rest of the datasets, and set the loss scale factor \(\) equal to \(100\). Further details are listed in Appendix A.

### Token distribution alignment in V-L models

We first evaluate the performance of the token distribution alignment strategy over the baseline for test-time prompt tuning. Since the alignment can only be performed on the vision branch, we explicitly compare PromptAlign to vanilla MaPLe, and MaPLe with TPT using entropy minimization. Table 1 presents the results in domain generalization across ImageNet variants. TPT improves the performance of MaPLe from \(60.28\%\) to \(62.31\%\) on average, while the incorporation of token distribution alignment further improves the performance by \(1.24\%\). This indicates that explicitly aligning the distribution of training and testing data enhances CLIP's generalization. Moreover, TPT is intended to improve test-time adaptation as a plug-in module, but its effectiveness is not uniform, as observed from the drop in performance on the ImageNet-Sketch dataset. On the other hand, PromptAlign does not show any performance degradation and improves beyond vanilla MaPLe consistently on all datasets. We further present qualitative results of PromptAlign in Appendix C.

### Domain Generalization

Compared to CLIP, all test-time adaptation methods demonstrate better performance, indicating that V-L models benefit from test-time adaptation approaches (Table 2). PromptAlign achieves the highest Top-1 accuracy averaged across all ImageNet variants. In comparison to the previous state-of-the-art CoOp+TPT, PromptAlign achieves an average improvement of \(0.71\%\). Out of the four different

    & Imagenet V2 & Imagenet Sketch & Imagenet A & Imagenet R & OOD Avg. \\  MaPLe & 64.07 & 49.15 & 50.90 & 76.98 & 60.28 \\ MaPLe+TPT & 64.87 & 48.16 & 58.08 & 78.12 & 62.31 \\  PromptAlign & **65.29** & **50.23** & **59.37** & **79.33** & **63.55** \\   

Table 1: **Effect of token distribution alignment strategy for domain generalization.** The base model MaPLe is trained on ImageNet and evaluated on datasets with domain shifts.

[MISSING_PAGE_FAIL:8]

## 5 Ablation

We perform ablative analysis on various components of PromptAlign. Unless stated otherwise, we show ablations on ImageNet-A dataset, the smallest domain generalization variant for simplicity. Detailed ablation trends across other ImageNet variants and datasets are presented in Appendix D.

**Token Distribution Alignment.** Table 5 summarizes the effect of token distribution alignment in PromptAlign. It can be observed that removing the entropy loss results in almost the same performance as vanilla MaPLe. Whereas combining alignment and entropy losses improves the performance remarkably. This is because the distribution alignment loss is a regularizer that functions together with the self-entropy loss. Since distribution alignment does not promote any discriminative learning as opposed to entropy loss, it is not expected to improve test-time adaptation on its own.

**Loss variants for distribution alignment.** We show the variation in performance with different loss choices for distribution alignment objective. Figure 4 compares the three loss choices \(L_{1}\), \(L_{2}\), and KL divergence. We observe that \(L_{1}\) constraint performs best for the distribution alignment objective. Further analysis is shown in Table E of supplementary material.

**Loss balancing factor \(\).** We ablate the loss scale factor \(\) to analyze the significance of the distribution alignment loss. The performance improves over the baseline model with TPT as depicted in Figure 4, even with a smaller weight scale and gradually plateaus close to a scale of \(100\). We use ImageNet validation set for this ablation and choose \(=100\) across all experiments. We find a similar trend of the scaling factor on other datasets, which is discussed in Appendix D. For the MaPLe+TPT model, the value of \(\) is 0 as there is no alignment objective in this case.

**Higher Order Statistics.** Since we align distributions computed using a small set of samples, we use relatively simpler (mean and variance) measures for our alignment strategy over higher order statistical measures. We evaluate this choice for the distribution alignment in PromptAlign using the 5\({}^{}\) order statistics of Central Moment Discrepancy measure  in Table 6. Our observation reveals that the utilization of higher order statistics results in only a slight improvement implies that there is minimal distinction between opting for higher order statistics validating mean and variance based approach as a suitable measure for alignment in our case.

**Prompt Regularization.** We compare PromptAlign to a naive prompt regularization method in a continuous test-time adaptation setting, in which the model gradually adapts to the incoming distribution. We regularize the prompts in relation to the previous prompts, denoted by PromptReg in Table 7. We also lower the learning rate to \(1e^{-5}\) to prevent diverging in the continuous setting. Weshow that the naive regularization setting, even in a continuous setting is not robust as the proposed alignment strategy, which solely aligns based on a single test sample, individually.

**Trade-off between compute resources and performance.** Figure 5(a) demonstrates the improvement in performance as the number of augmented views at test time increases. PromptAlign shows better improvement with more augmented views, owing to a better approximation of the test sample distribution. While MaPLe+TPT performance plateaus around 64 augmented views, our method with alignment loss shows further scope for improvement given more augmented views. Figure 5(b) shows the variation in performance with the number of prompt update steps. We note that, with more update steps, PromptAlign can consistently better adapt to the test sample in comparison to MaPLe+TPT alone. In Figure 5(c) we analyze the computational overhead of PromptAlign. PromptAlign shows no additional compute cost in terms of latency. The average inference time per sample across three seeds with and without the distribution alignment strategy are \(0.216s\) and \(0.197s\). Considering the memory usage and increase in latency, we use \(64\) views and a single-step update following TPT .

## 6 Conclusion

In this paper, we introduce PromptAlign, a novel approach for enhancing test-time adaptation of Vision-Language (V-L) models for zero-shot generalization. Our proposed approach bridges the gap between the test sample and source distributions by explicitly aligning the test sample statistics with that of the source data distribution through token distribution alignment. To achieve this, we incorporate multi-modal prompting to facilitate the alignment of token distributions across the transformer layers during test time. Through extensive experiments, PromptAlign demonstrates superior performance over existing state-of-the-art CLIP zero-shot generalization methods in domain generalization and cross-dataset evaluation settings.

    & }{}\)}} & }{}\)}} & }{}\)}} & }{}\)}} & }{}\)}} & }{}\)}} & }{}\)}} & }{}\)}} & }{}\)}} \\  PromptAlign & **72.39** & **47.24** & **90.76** & **68.5** & 69.47 & 94.01 & **86.65** & 67.54 & **24.8** & 47.86 & 66.92 \\  PromptAlign\({}^{}\) & 72.36 & **47.24** & 90.74 & 68.3 & **69.48** & **94.04** & **86.65** & **67.85** & 24.75 & **48.5** & **66.99** \\   

Table 6: **Comparison of the effect of higher order statistics for distribution alignment. PromptAlign\({}^{}\) utilizes Central Moment Discrepancy measure for the distribution alignment loss.**

    & }{}\)}} & }{}\)}} & }{}\)}} & }{}\)}} & }{}\)}} & }{}\)}} & }{}\)}} & }{}\)}} \\    & & & & & & & & & & & & \\  MaPLe & 93.53 & 90.49 & 65.57 & 72.23 & 86.20 & 24.74 & 67.01 & 46.49 & **48.06** & 68.69 & 66.30 \\  PromptReg & 93.71 & 90.60 & 65.02 & 72.05 & 86.05 & 22.32 & 65.46 & 46.24 & 19.03 & 68.23 & 62.87 \\  PromptAlign & **94.01** & **90.76** & **68.50** & **72.39** & **86.65** & **24.80** & **67.54** & **47.24** & 47.86 & **69.47** & **66.92** \\   

Table 7: **Comparison of PromptAlign with naive prompt regularization.**

Figure 5: **Analysis of compute resource constraints on performance. (a) The Top-1 accuracy increases with the number of augmented views. (b) The Top-1 accuracy improves consistently with the number of prompt update steps. (c) Impact on latency with the number of prompt update steps is similar for both methods.**