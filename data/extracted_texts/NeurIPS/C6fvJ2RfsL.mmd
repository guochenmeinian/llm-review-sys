# Self-Consistent Velocity Matching of Probability Flows

Lingxiao Li

MIT CSAIL

lingxiao@mit.edu

&Samuel Hurault

Univ. Bordeaux, Bordeaux INP, CNRS, IMB

samuel.hurault@math.u-bordeaux.fr

&Justin Solomon

MIT CSAIL

jsolomon@mit.edu

###### Abstract

We present a discretization-free scalable framework for solving a large class of mass-conserving partial differential equations (PDEs), including the time-dependent Fokker-Planck equation and the Wasserstein gradient flow. The main observation is that the time-varying velocity field of the PDE solution needs to be _self-consistent_: it must satisfy a fixed-point equation involving the probability flow characterized by the same velocity field. Instead of directly minimizing the residual of the fixed-point equation with neural parameterization, we use an iterative formulation with a biased gradient estimator that bypasses significant computational obstacles with strong empirical performance. Compared to existing approaches, our method does not suffer from temporal or spatial discretization, covers a wider range of PDEs, and scales to high dimensions. Experimentally, our method recovers analytical solutions accurately when they are available and achieves superior performance in high dimensions with less training time compared to alternatives.

## 1 Introduction

Mass conservation is a ubiquitous phenomenon in dynamical systems arising from fluid dynamics, electromagnetism, thermodynamics, and stochastic processes. Mathematically, mass conservation is formulated as the _continuity equation_:

\[_{t}p_{t}(x)=-(v_{t}p_{t}), x,t[0,T],\] (1)

where \(p_{t}:^{d}\) is a scalar quantity such that the total mass \( p_{t}(x)\) is conserved with respect to \(t\), \(v_{t}:^{d}^{d}\) is a velocity field, and \(T>0\) is total time. We will assume, for all \(t[0,T]\), \(p_{t} 0\) and \( p_{t}(x)\,x=1\), i.e., \(p_{t}\) is a probability density function. We use \(_{t}\) to denote the probability measure with density \(p_{t}\). Once a pair \((p_{t},v_{t})\) satisfies (1), the density \(p_{t}\) is coupled with \(v_{t}\) in the sense that the evolution of \(p_{t}\) in time is characterized by \(v_{t}\) (Section 3.1).

We consider the subclass of mass-conserving PDEs that can be written succinctly as

\[_{t}p_{t}(x)=-(f_{t}(x;_{t})p_{t}), x,t[ 0,T],\] (2)

where \(f_{t}(;_{t}):^{d}^{d}\) is a given function depending on \(_{t}\), with initial condition \(_{0}=_{0}^{*}\) for a given initial probability measure \(_{0}^{*}\) with density \(p_{0}^{*}\).

Different choices of \(f_{t}\) lead to a large class of mass-conserving PDEs. For instance, given a functional \(:_{2}(^{d})\) on the space of probability distributions with finite second moments, if we take

\[f_{t}(x;_{t}):=-_{W_{2}}(_{t})(x),\] (3)

where \(_{W_{2}}():^{d}^{d}\) is the Wasserstein gradient of \(\), then the solution to (2) is the Wasserstein gradient flow of \(\)(Santambrogio, 2015, Chapter 8). Thus, solving (2) efficiently allows us to optimize in the probability measure space. If we take

\[f_{t}(x;_{t}):=b_{t}(x)-D_{t}(x) p_{t}(x),\] (4)where \(b_{t}\) is a velocity field and \(D_{t}(x)\) is a positive-semidefinite matrix, then we obtain the time-dependent Fokker-Planck equation (Risken and Risken, 1996), which describes the time evolution of the probability flow undergoing drift \(b_{t}\) and diffusion with coefficient \(D_{t}\).

A popular strategy to solve (2) is to use an Eulerian representation of the density field \(p_{t}\) on a discretized mesh or as a neural network (Raissi et al., 2019). However, these approaches do not fully exploit the mass-conservation principle and are usually limited to low dimensions. Shen et al. (2022); Shen and Wang (2023) recently introduced the notion of _self-consistency_ for the Fokker-Planck equation and more generally McKean-Vlasov type PDEs. This notion is a Lagrangian formulation of (2). They apply the adjoint method to optimize self-consistency. In this work, we extend their notion of self-consistency to mass-conserving PDEs of the general form (2). Equipped with this formulation, we develop an iterative optimization scheme called _self-consistent velocity matching_. With the probability flow parameterized as a neural network, at each iteration, we refine the velocity field \(v_{t}\) of the current flow to match an estimate of \(f_{t}\) evaluated using the network weights from the previous iteration. Effectively, we minimize the self-consistency loss with a biased but more tractable gradient estimator.

This simple scheme has many benefits. First, the algorithm is agnostic to the form of \(f_{t}\), thus covering a wider range of PDEs compared to past methods. Second, we no longer need to differentiate through differential equations using the adjoint method as in Shen and Wang (2023), which is orders of magnitude slower than our method with worse performance in high dimensions. Third, this iterative formulation allows us to rewrite the velocity-matching objectives for certain PDEs to get rid of computationally expensive quantities such as \( p_{t}\) in the Fokker-Planck equation (Proposition 3.1). Lastly, our method is flexible with probability flow parameterization: we have empirically found that the two popular ways of parameterizing the flow--as a time-varying pushforward map (Bilos et al., 2021) and as a time-varying velocity field (Chen et al., 2018)--both have merits in different scenarios.

Our method tackles mass-conserving PDEs of the form (2) in a unified manner without temporal or spatial discretization. Despite using a biased gradient estimator, in practice, our method decreases the self-consistency loss efficiently (second column of Figure 2). For PDEs with analytically-known solutions, we quantitatively compare with the recent neural JKO-based methods (Molkov et al., 2021; Fan et al., 2021; Alvarez-Melis et al., 2021), the adjoint method (Shen and Wang, 2023), and the particle-based method (Boffi and Vanden-Eijnden, 2023). Our method faithfully recovers true solutions with quality on part with the best previous methods in low dimensions and with superior quality in high dimensions. Our method is also significantly faster than competing methods, especially in high dimensions, at the same time without discretization. We further demonstrate the flexibility of our method on two challenging experiments for modeling flows splashing against obstacles and smooth interpolation of measures where the comparing methods are either not applicable or have noticeable artifacts.

## 2 Related Works

Classical PDE solvers for mass-conserving PDEs such as the Fokker-Planck equation and the Wasserstein gradient flow either use an Eulerian representation of the density and discretize space as a grid or mesh (Burger et al., 2010; Carrillo et al., 2015; Peyre, 2015) or use a Lagrangian representation, which discretizes the flow as a collection of interacting particles simulated forward in time (Crisan and Lyons, 1999; Westdickenberg and Wilkening, 2010). Due to spatial discretization, these methods struggle with high-dimensional problems. Hence, the rest of the section focuses solely on recent neural network-based methods.

**Physics-informed neural networks.** Physics-informed neural networks (PINNs) are prominent methods that solve PDEs using deep learning (Raissi et al., 2019; Karniadakis et al., 2021). The main idea is to minimize the residual of the PDE along with loss terms to enforce the boundary conditions and to match observed data. Our notion of self-consistency is a Lagrangian analog of the residual in PINN. Our velocity matching only occurs along the flow of the current solution where interesting dynamics happen, while in PINNs the residual is evaluated on collocation points that occupy the entire domain. Hence our method is particularly suitable for high-dimensional problems where the dynamics have a low-dimensional structure.

**Neural JKO methods.** Recent works (Mokrov et al., 2021; Alvarez-Melis et al., 2021; Fan et al., 2021) apply deep learning to the time-discretized JKO scheme (Jordan et al., 1998) to solve Wasserstein gradient flow (3). By pushing a reference measure through a chain of neural networks parameterized as input-convex neural networks (ICNNs) (Amos et al., 2017), these methods avoid discretizing the space. Mokrov et al. (2021) optimize one ICNN to minimize Kullback-Leibler (KL) divergence plus a Wasserstein-2 distance term at each JKO step. This method is extended to other functionals by Alvarez-Melis et al. (2021). Fan et al. (2021) use the variational formulation of \(f\)-divergence to obtain a faster primal-dual approach.

An often overlooked problem of neural JKO methods is that the total training time scales quadratically with the number of JKO steps: to draw samples for the current step, initial samples from the reference measure must be passed through a long chain of neural networks, along with expensive quantities like densities. However, using too few JKO steps results in large temporal discretization errors. Moreover, the optimization at each step might not have fully converged before the next step begins, resulting in an unpredictable accumulation of errors. In contrast, our method does not suffer from temporal discretization and can be trained end-to-end. It outperforms these neural JKO methods with less training time in experiments we considered.

**Velocity matching.** A few recent papers employ the idea of velocity matching to construct a flow that follows a learned velocity field. di Langosco et al. (2021) simulate the Wasserstein gradient flow of the KL divergence by learning a velocity field that drives a set of particles forward in time for Bayesian posterior inference. The velocity field is refined on the fly based on the current positions of the particles. Boffi and Vanden-Eijnden (2023) propose a similar method that applies to a more general class of time-dependent Fokker-Planck equations. These two methods approximate probability measures using finite particles which might not capture high-dimensional distributions well. Liu et al. (2022); Lipman et al. (2022); Albergo and Vanden-Eijnden (2022) use flow matching for generative modeling by learning a velocity field that generates a probability path connecting a reference distribution to the data distribution. Yet these methods are not designed for solving PDEs.

Most relevant to our work, Shen et al. (2022) propose the concept of self-consistency for the Fokker-Planck equation, later extended to McKean-Vlasov type PDEs (Shen and Wang, 2023). They observe that the velocity field of the flow solution to the Fokker-Planck equation must satisfy a fixed-point equation. They theoretically show that, under certain regularity conditions, a form of probability divergence between the current solution and the true solution is bounded by the self-consistency loss that measures the violation of the fixed-point equation. Their algorithm minimizes such violation using neural ODE parameterization (Chen et al., 2018) and the adjoint method. Our work extends the concept of self-consistency to a wider class of PDEs in the form of (2) and circumvents the computationally demanding adjoint method using an iterative formulation. We empirically verify that our method is significantly faster and reduces the self-consistency loss more effectively in moderate dimensions than that of Shen and Wang (2023) (Figure 2).

## 3 Self-Consistent Velocity Matching

### Probability flow of the continuity equation

A key property of the continuity equation (1) is that any solution \((p_{t},v_{t})_{t[0,T]}\) (provided \(p_{t}\) is continuous with respect to \(t\) and \(v_{t}\) is bounded) corresponds to a unique flow map \(\{_{t}():^{d}^{d}\}_{t[0,T]}\) that solves the ordinary differential equations (ODEs) (Ambrosio et al., 2005, Proposition 8.1.8)

\[_{0}(x)=x,}{t}_{t}(x)=v_{t}(_{t}(x)),  x,t[0,T],\] (5)

and the flow map satisfies \(_{t}=(_{t})_{\#}_{0}\) for all \(t[0,T]\), where \((_{t})_{\#}_{0}\) to denote the pushforward measure of \(_{0}\) by \(_{t}\). Moreover, the converse is true: any solution \((_{t},v_{t})\) of (5) with Lipschitz continuous and bounded \(v_{t}\) is a solution of (1) with \(_{t}=(_{t})_{\#}_{0}\)(Ambrosio et al., 2005, Lemma 8.1.6). Thus the Eulerian viewpoint of (1) is equivalent to the Lagrangian viewpoint of (5). We next exploit this equivalence by modeling the probability flow using the Lagrangian viewpoint so that it automatically satisfies the continuity equation (1).

### Parametrizing probability flows

Our algorithm will be agnostic to the exact parameterization used to represent the probability flow. As such, we need a way to parameterize the flow to access the following quantities for all \(t[0,T]\):

* \(_{t}:^{d}^{d}\), the flow map, so \(_{t}(x)\) is the location of a particle at time \(t\) if it is at \(x\) at time \(0\).
* \(v_{t}:^{d}^{d}\), the velocity field of the flow at time \(t\).
* \(_{t}(^{d})\), the probability measure at time \(t\) with sample access and density \(p_{t}\).

We will assume all these quantities are sufficiently continuous and bounded to ensure the Eulerian and Lagrangian viewpoints in Section 3.1 are equivalent. This can be achieved by using continuously differentiable activation functions in the network architectures and assuming the network weights are finite similar to the uniqueness arguments given in (Chen et al., 2018). We can now parameterize the flow modeling either the flow map \(_{t}\) or the velocity field \(v_{t}\) as a neural network.

**Time-dependent Invertible Push Forward (TIPF).** We first parameterize a probability flow by modeling \(_{t}:^{d}^{d}\) as an invertible network for every \(t\). The network architecture is chosen so that \(_{t}\) has an analytical inverse with a tractable Jacobian determinant, similar to (Bilos et al., 2021). We augment RealNVP from Dinh et al. (2016) so that the network for predicting scale and translation takes \(t\) as an additional input. To enforce the initial condition, we need \(_{0}\) to be the identity map. This condition can be baked into the network architecture (Bilos et al., 2021) or enforced by adding an additional loss term \(_{X_{0}^{*}}\|_{0}(X)-X\|^{2}\). For brevity, we will from now on omit in the text this additional loss term. The velocity field can be recovered via \(v_{t}(x)=_{t}_{t}(_{t}^{-1}(x))\). To recover the density \(p_{t}\) of \(_{t}=(_{t})_{\#}_{0}\), we use the change-of-variable formula \( p_{t}(x)= p_{0}^{*}(_{t}^{-1}(x))+|J_{t}^{-1}(x)|\)(see (1) in Dinh et al. (2016)).

**Neural ODE (NODE).** We also parameterize a flow by modeling \(v_{t}:^{d}^{d}\) as a neural network; this is used in Neural ODE (Chen et al., 2018). The network only needs to satisfy the minimum requirement of being continuous. The flow map and the density can be recovered via numerical integration: \(_{t}(x)=x+_{0}^{t}v_{s}(_{s}(x))\,s\) and \( p_{t}(_{t}(x))= p_{0}^{*}(x)-_{0}^{t} v_{s}( _{s}(x))\,s\), a direct consequence of (1) also known as the instantaneous change-of-variable formula (Chen et al., 2018). To obtain the inverse of the flow map, we integrate along \(-v_{t}\). With NODE, the initial condition \(_{0}=_{0}^{*}\) is obtained for free.

We summarize the advantages and disadvantages of TIPF and NODE as follows. While the use of invertible coupling layers in TIPF allows exact access to samples and densities, TIPF becomes less effective in higher dimensions as many couple layers are needed to retain good expressive power, due to the invertibility requirement. In contrast, NODE puts little constraints on the network architecture, but numerical integration can have errors. Handling the initial condition is trivial for NODE while an additional loss term or special architecture is needed for TIPF. As we will show in the experiments, both strategies have merits.

### Formulation

We now describe our algorithm for solving mass-conserving PDEs (2). A PDE of this form is determined by \(f_{t}(;_{t}):^{d}^{d}\) plus the initial condition \(_{0}^{*}\). If a probability flow \(_{t}\) with flow map \(_{t}\) and velocity field \(v_{t}\) satisfies the following _self-consistency_ condition,

\[v_{t}(x)=f_{t}(x;_{t}), x_{t},\] (6)

then the continuity equation of this flow implies the corresponding PDE (2) is solved. Conversely, the velocity field of any solution of (2) will satisfy (6). Hence, instead of solving (2) which is a condition on the density \(p_{t}\) that might be hard to access, we can solve (6) which is a more tractable condition. Shen et al. (2022); Shen and Wang (2023) develop this concept for the Fokker-Planck equation and McKean-Vlasov type PDEs; here we generalize it to a wider class of PDEs of the form (2).

Let \(\) be the network weights that parameterize the probability flow using TIPF or NODE. The flow's measure, velocity field, and flow map at time \(t\) are denoted as \(_{t}^{}\), \(v_{t}^{}\), \(_{t}^{}\) respectively. One option to solve (6) would be to minimize the _self-consistency loss_

\[_{}_{0}^{T}_{X_{t}^{}}[\|v_{t}^ {}(X)-f_{t}(X;_{t}^{})\|^{2}]t.\] (7)This formulation is reminiscent of PINNs (Raissi et al., 2019) where a residual of the original PDE is minimized. Direct optimization of (7) is challenging: while the integration over \([0,T]\) and \(_{t}^{}\) can be approximated using Monte Carlo, to apply stochastic gradient descent, we must differentiate through \(_{t}^{}\) and \(f_{t}\): this can be either expensive or intractable depending on the network parameterization. The algorithm by Shen and Wang (2023) minimizes (7) with the adjoint method specialized to Fokker-Planck equations and McKean-Vlasov type PDEs; extending their approach to more general PDEs requires a closed-form formula for the time evolution of the quantities within \(f_{t}\), which at best can only be obtained on a case-by-case basis.

Instead, we propose the following iterative optimization algorithm to solve (7). Let \(_{k}\) denote the network weights at iteration \(k\). We define iterates

\[_{k+1}:=_{k}-_{}|_{=_{k}}F(, _{k}),\] (8)

where

\[F(,_{k}):=_{0}^{T}_{X_{t}^{ _{k}}}[\|v_{t}^{}(X)-f_{t}(X;_{t}^{_{k}}) \|^{2}]t.\] (9)

Effectively, in each iteration, we minimize (9) by one gradient step where we match the velocity field \(v_{t}^{}\) to what it should be according to \(f_{t}\) based on the network weights \(_{k}\) from the previous iteration. This scheme can be interpreted as a gradient descent on 7 using the biased gradient estimate \(_{}F(,_{k})\)--see Appendix A for a discussion. We call this iterative algorithm _self-consistent velocity matching_.

If \(f_{t}\) depends on the density of \(_{t}\) only through the score \( p_{t}\) (corresponding to a diffusion term in the PDE), then we can apply an integration-by-parts trick (Hyvarinen and Dayan, 2005) to get rid of this density dependency by adding a divergence term of the velocity field. Suppose \(f_{t}\) is from the Fokker-Planck equation (4). Then the cross term in (9) after expanding the squared norm has the following alternative expression.

**Proposition 3.1**.: _For every \(t[0,T]\), for \(f_{t}\) defined in (4), assume \(v_{t}^{},D_{t}\) are bounded and continuously differentiable, and \(_{t}^{^{}}\) is a measure with a continuously differentiable density \(p_{t}^{^{}}\) that vanishes in infinity and not at finite points. Then we have_

\[_{X_{t}^{^{}}}[v_{t}^{} (X)^{}f_{t}(X;_{t}^{^{}})]=_{X_{t} ^{^{}}}[v_{t}^{}(X)^{}b_{t}(X)+(D_{t}^{}(X)v_{t}^{}(X))].\] (10)

We provide the derivation in Appendix B. With Proposition 3.1, we no longer need access to \( p_{t}\) when computing \(_{}F\). This is useful for NODE parameterization since obtaining the score would otherwise require additional numerical integration.

Our algorithm is summarized in Algorithm 1 in the appendix. We use Adam optimizer (Kingma and Ba, 2014) to modulate the update (8). For sampling time steps \(t_{1},,t_{L}\) in \([0,T]\), we use stratified sampling where \(t_{l}\) is uniformly sampled from \([}{{L}},}{{L}}]\); such a sampling strategy results in more stable training in our experiments. We implemented our method using JAX (Bradbury et al., 2018) and FLAX (Heek et al., 2020). See Appendix C for the implementation details.

## 4 Experiments

We show the efficiency and accuracy of our method on several PDEs of the form (2). We start with three Wasserstein gradient flow experiments (Section 4.1, Section 4.2, Section 4.3). Next, we consider the time-dependent Fokker-Planck equation that simulates attraction towards a harmonic mean in Section 4.4. Finally, in Section 4.5, we apply our framework to generate complicated low-dimensional dynamics including flows splashing against obstacles and smooth interpolation of measures. We will use SCVM-TIPF and SCVM-NODE to denote our method with TIPF and NODE parameterization respectively. We use JKO-ICNN to denote the method by Mokrov et al. (2021), JKO-ICNN-PD to denote the method by Fan et al. (2021) (PD for "primal-dual"), ADJ to denote the adjoint method by Shen and Wang (2023), SDE-EM to denote the Euler-Maruyama method for solving the SDE associated with the Fokker-Planck equation, and DFE ("discrete forward Euler") to denote the method by Boffi and Vanden-Eijnden (2023). We implemented all competing methods in JAX--see more details in Appendix C--and we compare quantitatively against these methods when possible.

In Table 1, we compare the time complexity of training the described methods, where we show that SCVM-TIPF and SCVM-NODE have low computational complexity among all methods.

**Evaluation metrics.** For quantitative evaluation, we use the following metrics. To compare measures with density access, following Mokrov et al. (2021), we use the symmetric Kullback-Leibler (symmetric KL) divergence, defined as \((_{1},_{2}):=(_{1}_{2})+(_{2}_{1})\), where \((_{1}_{2}):=_{X_{1}}[}}{{d_{2}}}(X)}]\). Sample estimates of KL can be negative which complicates log-scale plotting, so when this happens, we consider an alternative \(f\)-divergence \(D_{f}(_{1}_{2}):=_{X_{2}}[}}{{D_{1}}}(X)}-}}{{D_{2}}}(X)}]^{ 2}/2]\) whose sample estimates are always non-negative. We similarly define the symmetric \(f\)-divergence \(D_{f}(_{1},_{2}):=D_{f}(_{1}_{2})+D_{f}( _{2}_{1})\). For particle-based methods, we use kernel density estimation (with Scott's rule) to obtain the density function before computing symmetric KL or \(f\)-divergence. We also consider the Wasserstein-2 distance (Bonneel et al., 2011) and the Bures-Wasserstein distance (Kroshnin et al., 2021); these two measures only require sample access. All metrics are computed using i.i.d. samples. See Appendix C.6 for more details.

### Sampling from mixtures of Gaussians

We consider computing the Wasserstein gradient flow of the KL divergence \(()=(^{*})\) where we have density access to the target measure \(^{*}\). To fit into our framework, we set \(f_{t}(x;_{t})= p^{*}(x)- p_{t}(x)\) which matches (4) with \(b_{t}(x)= p^{*}(x)\) and \(D_{t}(x)=f_{d}\). Following the experimental setup in Mokrov et al. (2021) and Fan et al. (2021), we take \(^{*}\) to be a mixture of 10 Gaussians with identity covariance and means sampled uniformed in \([-5,5]^{d}\). The initial measure is \(^{*}_{0}=(0,16I_{d})\). We solve the corresponding Fokker-Planck PDE for a total time of \(T=5\) and for \(d=10,,60\). As TIPF parameterization does not scale to high dimensions, we only consider SCVM-NODE in this experiment.

Figure 1 shows the probability flow produced by SCVM-NODE in dimension 60 at different time steps; as we can see, the flow quickly converges to the target distribution.

In Figure 2, we quantitatively compare our method with Mokrov et al. (2021); Fan et al. (2021), and Shen and Wang (2023). Training time is reported for all methods.

* In the left two columns of Figure 2, we find that even though the adjoint method ADJ (Shen and Wang, 2023) minimizes the self-consistency loss (7) directly, the decay of self-consistency can be much slower than that of SCVM-NODE as the dimension increases. We suspect this is due to the amount of error accumulated in the adjoint method which involves two numerical integration passes to obtain the gradient. Moreover, ADJ requires up to _third-order spatial derivatives_ of the parameterized neural velocity field which can be inaccurate even if the consistency loss is low--in comparison SCVM-NODE only requires one integration pass and the first-order spatial derivative of the network. Despite the bias of the gradient used in SCVM-NODE, it finds more efficient gradient trajectories than ADJ. Additionally, ADJ takes 80 times longer to train than SCVM-NODE in dimension \(10\), and scaling up to higher dimensions becomes prohibitive.
* The rightmost column of Figure 2 shows SCVM-NODE achieves far lower symmetric KL compared to the JKO methods. The gradient flow computed by JKO methods does not decrease KL divergence monotonically, likely because the optimization at each JKO step has yet to reach the minimum even though we use 2000 gradient updates for each step. For both JKO methods, the running time for each JKO step increases linearly because samples (and for JKO-ICNN also \(\) terms) need to be pushed through a growing chain of ICNNs; as a result, the total running time scales _quadratically_ with the number of JKO steps. JKO methods also take about 40 times as long evaluation time as

Figure 1: Probability flow produced by SCVM-NODE for a 60-dimensional mixture of Gaussians at irregular time steps. Samples are projected onto the first two PCA components and kernel density estimation is used to generate the contours.

SCVM-NODE in dimension 60 since density access requires solving an optimization problem for every JKO step. On top of the computational advantage and better results, our method also does not have temporal discretization: after being trained, the flow can be accessed at any time \(t\) (Figure 1).

### Ornstein-Uhlenbeck process

We consider the Wasserstein gradient flow of the KL divergence with respect to a Gaussian with the initial distribution being a Gaussian, following the same experimental setup in Mokrov et al. (2021); Fan et al. (2021). In this case, the gradient flow at time \(t\) is a Gaussian \(G(t)\) with a known mean and covariance; see Appendix D.1 for details. We quantitatively compare all methods in Figure 3:

* ADJ achieves the best results in dimensions \(d=5\) and \(d=10\). However, this is at the cost of high training time: in dimension 10, ADJ takes 341 minutes to train, while SCVM-TIPF and SCVM-NODE take 23 and 9 minutes respectively for the same 20k training iterations. As such, we omit ADJ in higher-dimensional comparisons.
* Both our SCVM-TIPF and SCVM-NODE achieve good results second only to ADJ in dimensions \(5\) and \(10\). In low dimensions, SCVM-TIPF results in lower probability divergences than SCVM-NODE likely due to having exact density access. Although not shown, SCVM-TIPF also satisfies the initial condition well (numbers at \(t=0\) are comparable to those at \(t=0.25\) in the left two columns in Figure 3).
* For the two JKO methods, they result in much higher errors for \(t 0.5\) compared to later times: this is expected because the dependency of \(G(t)\) on \(t\) is exponential, so convergence to \(^{*}\) is faster in the beginning, yet a constant step size is used for JKO methods.
* For DFE, the result is highly sensitive to the forward Euler step size \( t\). We choose step size \( t=0.01\) which empirically gives the best results among \(\{0.1,0.01,0.001,0.0001\}\). As DFE achieves far lower symmetric KL divergence or \(f\)-divergence compared to alternatives, we only include its Bures-Wasserstein distance in high dimensions where its number is slightly worse than alternatives (bottom right plot of Figure 3).

### Porous medium equation

Following Fan et al. (2021), we consider the porous medium equation with only diffusion: \(_{t}p_{t}= p_{t}^{m}\) with \(m>1\). Its solution is the Wasserstein gradient flow of \(()=p(x)^{m}\,x\) where \(p\) is the density of \(\) with \(_{W_{2}}()(x)=(p^{m-1}(x))\)--see Appendix D.2 for details. We consider only SCVM-TIPF and JKO methods here.

Figure 2: Quantitative comparison for the mixture of Gaussians experiment. The left two columns plot the symmetric KL (at \(t=T\) compared against the target measure) and consistency (7) versus the training iterations for SCVM-NODE (ours) and ADJ (Shen and Wang, 2023). The rightmost column plots the symmetric KL across time \(t\) (compared against the target measure) for SCVM-NODE and the JKO methods in high dimensions. Training time: for \(d=10\), SCVM-NODE takes 7.37 minutes, ADJ takes 585.2 minutes; for \(d=60\), SCVM-NODE takes 23.9 minutes, JKO-ICNN takes 375.2 minutes, and JKO-ICNN-PD takes 24.4 minutes.

We show the efficiency of SCVM-TIPF compared to JKO-ICNN in dimension \(d=1,2,,6\). We exclude JKO-ICNN-PD because it produces significantly worse results. We visualize the density \(p_{t}\) of the solution from SCVM-TIPF and JKO-ICNN on the top of Figure 4 in dimension 1 compared to \(p_{t}^{*}\). Both methods approximate \(p_{t}^{*}\) well with SCVM-TIPF being more precise at small \(t\); this is consistent with the observation in Figure 2 where JKO methods result in bigger errors for small \(t\).

Figure 4: Top: visualization of the densities of \(p_{t}^{*}\) and \(p_{t}\) for the porous medium equation in dimension \(1\) at varying time steps \(t\) for SCVM-TIPF and JKO-ICNN. Bottom: total variation distance, symmetric \(f\)-divergence, and Wasserstein-2 distances across dimensions at \(t=0.004\) and \(t=0.025\) between \(p_{t}\) and \(p_{t}^{*}\) for solving the porous medium equation.

Figure 3: Quantitative results for the OU process experiment. The left two columns show the metrics (symmetric KL, symmetric \(f\)-divergence, and Bures-Wasserstein distance) versus time \(t\) of various methods computed against the closed formed solution \(G(t)\) in dimension \(d=5,10\). The right column shows the metrics averaged across \(t\) versus dimension \(d\) in higher dimensions.

On the bottom row of Figure 4, we plot the total variation (TV) distance, the symmetric \(f\)-divergence, and the Wasserstein-2 distance (details on the TV distance are given in Appendix C.6) between the recovered solution \(p_{t}\) and \(p_{t}^{}\) for both methods at \(t=0.004\) and \(t=0.025\). Note that the values of all metrics are very low implying that the solution from either method is very accurate, with SCVM-TIPF more precise in TV distance and symmetric \(f\)-divergence, especially for \(d>3\). Like with the experiments in previous sections, JKO-ICNN is much slower to train: in dimension 6, training JKO-ICNN took \(102\) minutes compared to \(21\) minutes for SCVM-TIPF.

### Time-Dependent Fokker-Planck equation

In this section, we qualitatively evaluate our method for solving a PDE that is not a Wasserstein gradient flow. In this case, JKO-based methods cannot be applied. Consider the OU process from Section 4.2 when the mean \(\) and the covariance matrix \(\) become time-dependent as \(_{t}\) and \(_{t}\). The resulting PDE is a time-dependent Fokker-Planck equation of the form (4) with

\[f_{t}(X,_{t})=_{t}(_{t}-X)-D p_{t}(X).\] (11)

In this configuration, when the initial measure \(p_{0}\) is Gaussian, the solution \(_{t}\) can again be shown to be Gaussian with mean and covariance following an ODE--see Appendix D.3 for more details. We consider, in dimension \(2\) and \(3\), time-dependent attraction towards a harmonic mean \(_{t}=a(( t),( t))\) using the expression of \(_{t}\) from Boffi and Vanden-Eijnden (2023), augmented to \(_{t}=a(( t),( t),t)\) in dimension \(3\).

We apply both SCVM-TIPF and SCVM-NODE to this problem and compare our results with alternatives. Similar to Figure 3, as shown in Figure 5, both SCVM-TIPF and SCVM-NODE achieve results on par with ADJ, with both SCVM methods being 30 times faster than ADJ in dimension 10. DFE results in good Wasserstein-2 metrics but worse divergences. Visualization of the evolution of a few sampled particles are given in Figure 9 and Figure 10.

In Appendix D.4, we augment (11) with an interaction term to simulate a flock of (infinitely many) birds, resulting in a non-Fokker-Planck PDE that can be readily solved by our method.

### Additional qualitative low-dimensional dynamics

To demonstrate the flexibility of our method, we apply our algorithm to model more general mass-conserving dynamics than the ones considered in the previous sections.

**Flow splashing against obstacles.** We model the phenomenon of a 2-dimensional flow splashing against obstacles using a Fokker-Planck equation (4) where \(b_{t}\) encodes the configuration of three

Figure 5: Symmetric KL divergence and Wasserstein-2 distances across time for \(d=2,3\) between the recovered flows and the ground truth for the time-dependent Fokker-Planck equation.

obstacles that repel the flow (See Appendix D.5 for details). We solve this PDE using SCVM-NODE for \(T=5\) and visualize the recovered flow in (6). When solving the same PDE using SDE-EM, the flow incorrectly crosses the bottom right obstacle due to a finite time step size (Figure 14). When using DFE, the path of initial samples appears jagged (right of Figure 13); our method has no such issue and results in continuous sample paths (left of Figure 13). Method ADJ suffers from numerical instability and cannot be trained without infinite loss in this example.

**Smooth interpolation of measures.** To illustrate the flexibility of our method, we demonstrate two ways to formulate the problem of smoothly interpolating a list of measures. First, we model the interpolation as a time-dependent Fokker-Planck equation and use it to interpolate MNIST digits 1, 2, and 3, starting from a Gaussian (Figure 7). Next, we adopt an optimal transport formulation and use it to generate an animation sequence deforming a 3D hand model to a different pose and then to a ball, similar to the setup in Zhang et al. (2022). Note that the optimal transport formulation is not solvable using competing methods. See Appendix D.6 for more details.

## 5 Conclusion

By extending the concept of self-consistency from Shen et al. (2022), we present an iterative optimization method for solving a wide class of mass-conserving PDEs without temporal or spatial discretization. In all experiments considered, our method achieves strong quantitative results with significantly less training time than JKO-based methods and the adjoint method in high dimensions.

Below we highlight a few future directions. First, as discussed, the two ways to parameterize a probability flow, TIPF, and NODE, both have their specific limitations. Finding a new parameterization that combines the advantages of both TIPF and NODE is an important next step. Secondly, we hope to extend our approach to incorporate more complicated boundary conditions. Finally, given that the proposed algorithm is highly effective empirically, it would be an interesting theoretical step to explore its convergence properties.

AcknowledgementsThe MIT Geometric Data Processing group acknowledges the generous support of Army Research Office grants W911NF2010168 and W911NF2110293, of Air Force Office of Scientific Research award FA9550-19-1-031, of National Science Foundation grant CHS-1955697, from the CSAIL Systems that Learn program, from the MIT-IBM Watson AI Laboratory, from the Toyota-CSAIL Joint Research Center, from a gift from Adobe Systems, and from a Google Research Scholar award.

SH acknowledges the financial support from the University of Bordeaux (UBGR grant) and the French Research Agency (PostProdEAP).

Figure 6: A flow splashing against three obstacles (in purple) produced by SCVM-NODE. Particles are colored based on the initial \(y\) coordinates.

Figure 7: Smooth interpolation of measures. Top: interpolating MNIST digits 1 to 3. Bottom: interpolating hand from the initial pose to a different pose and then to a ball.