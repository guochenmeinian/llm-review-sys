# Preference-based Pure Exploration

Apurv Shukla

Department of ECE, Texas A&M University

College Station, TX 77840

apurv.shukla@umich.edu &Debabrota Basu

Equipe School, Univ. Lille, Inria, CNRS

Centrale Lille, UMR-9189 - CRIStAL, France

debabrota.basu@inria.fr

This work was done when the author was at Texas A& M University. The author is currently at the University of Michigan, Ann Arbor.

###### Abstract

We study the preference-based pure exploration problem for bandits with vector-valued rewards. The rewards are ordered using a (given) preference cone \(\) and our goal is to identify the set of Pareto optimal arms. First, to quantify the impact of preferences, we derive a novel lower bound on sample complexity for identifying the most preferred policy with a confidence level \(1-\). Our lower bound elicits the role played by the geometry of the preference cone and punctuates the difference in hardness compared to existing best-arm identification variants of the problem. We further explicate this geometry when the rewards follow Gaussian distributions. We then provide a convex relaxation of the lower bound and leverage it to design the Preference-based Track and Stop (PreTS) algorithm that identifies the most preferred policy. Finally, we show that the sample complexity of PreTS is asymptotically tight by deriving a new concentration inequality for vector-valued rewards.

## 1 Introduction

Following COVID-19, the importance of reliable clinical trials and corresponding data acquisition to design effective drugs has gained wider recognition. However, conducting large-scale clinical trials is cost and time intensive as it requires working with large number of patients and following up their medical conditions over time. In the past two decades, this has led to doubling in the cost to bring a drug to the market, i.e., to \(\$2.6\) billion with a 12-year drug development horizon and 90% failure rate during the clinical trial (Mullard, 2014; Sun et al., 2022). However, due to the rise of systematic data acquisition about biological systems, pharmaceutical firms are interested in harvesting the collected data for drug discovery (Gaulton et al., 2012; Reker and Schneider, 2015). Thus, machine learning-based methods are increasingly studied and deployed as a promising avenue for identifying potentially successful drugs with less patient involvement, increasing the "hit rate", and speeding up the development process (Jayatunga et al., 2022; Smer-Barreto et al., 2023; Sadybekov and Katritch, 2023; Hasselgren and Oprea, 2024). But deciding whether a drug is successful depends on multiple and often conflicting objectives regarding safety, efficacy, and pharmacokinetic constraints (Lizotte and Laber, 2016). For example, COV-BOOST (Munro et al., 2021) demonstrates a phase II vaccine clinical trial conducted on 2883 participants to measure the immunogenicity indicators (e.g. cellular response, anti-spike IgG and NT\({}_{50}\)) of different Covid-19 vaccines as a booster (third dose). Experts decide how different indicators are preferred over one another, and above different thresholds (Jayatunga et al., 2022). This motivates us to study a sequential decision-making problem, where we aim to conduct minimum number of experiments to acquire informative data, and to reliably validate a hypothesis with multiple objectives by imposing preferences over them.

Problems of such nature can be modeled as a multi-armed bandit (in brief, bandits), which is an established framework for sequential decision-making under uncertainty (Lattimore and Szepesvari, 2020). In bandits, a learner has access to an instance of \(K\) decisions (or arms). Each arm \(k\{1,,K\}\) corresponds to a probability distribution \(P_{k}\) of feedback (rewards) with unknown mean \(_{k}\). At each step \(t\), the learner interacts with the instance by taking a decision \(k_{t}\) (analogously pulling an arm), and observes a noisy reward \(R_{t}\) from the corresponding distribution of rewards \(P_{k_{t}}\). The goal of the learner is to identify the arm with the highest expected reward over a certain confidence level through minimum number of interactions with the instance. This is popularly known as a fixed-confidence Best Arm Identification (BAI) in bandit literature (Jamieson and Nowak, 2014; Garivier and Kaufmann, 2016; Soare et al., 2014), which is a special case of pure exploration problems (Even-Dar et al., 2006; Bubeck et al., 2009; Auer et al., 2016).

The bandit literature spanning over a century mostly focuses on a scalar reward, i.e., a single objective. In our problem, each reward \(R_{t}\) is a real-valued vector of \(L\) objectives, and thus, the unknown mean vector of each arm \(M_{k}^{L}\). Since the objectives can be often conflicting, there might not exist a single best arm. Rather, there exists a Pareto Optimal Set of arms (Drugan and Nowe, 2013; Auer et al., 2016). Given a set of preferences over the objectives, the Pareto Optimal Set consists of arms whose mean vectors dominate the mean vectors of any other arm outside the set. Keeping generality, we assume that preferences are defined by a cone of vectors \(^{L}\). Every \(\) induces a set of partial or incomplete orders over the \(L\) objectives (Jahn et al., 2009; Lohne, 2011). Given the preference cone \(\), we aim to _exactly_ identify the complete Pareto Optimal Set with a confidence level \((1-)[0,1)\) using as few interactions as possible. We refer to this problem Preference-based Pure Exploration (**PrePEx**). Recently, Auer et al. (2016); Kone et al. (2023a,b); crepon et al. (2024) consider a special case of PrePEx, where the preference is known. To the best of our knowledge, Ararat and Tekin (2023) and Korkmaz et al. (2023) are the only studies of PrePEx from frequentist and Bayesian angles, respectively. Here, we consider a frequentist approach as in (Ararat and Tekin, 2023). However, their goal is to identify points that are in the Pareto Optimal Set or very close to it. In contrast, we focus on exactly identifying the Pareto Optimal Set. Additionally, Ararat and Tekin (2023) propose a gap-based elimination algorithm to solve the problem that generalises the algorithm of Even-Dar et al. (2006). But in BAI, there is another paradigm of designing efficient algorithms that solves and tracks the exact lower bound on the expected time to identify the best arm \((1-)\) correctly (Garivier and Kaufmann, 2016; Degenne and Koolen, 2019). We explore this paradigm for PrePEx and ask two questions:

_What is the exact lower bound of_ PrePEx _for identifying the Pareto Optimal Set, and how to design a computationally tractable algorithm matching this bound?_

We address them affirmatively in our contributions:

**1. Lower Bound for PrePEx.** In Theorem 3.1, we study hardness of PrePEx problems by deriving the novel lower bound on the expected sample complexity of any algorithm to yield the exact Pareto Optimal Set with confidence \((1-)\). The challenge here is to extend the classical BAI lower bound (Garivier and Kaufmann, 2016) to a set of confusing instances given \(\). We observe that unlike BAI, distinguishability of two arms in PrePEx depends on their projections on the cone polar to \(\). We also show that our lower bound generalises the lower bound for pure exploration under known constraints (Carlsson et al., 2024). Additionally, we provide an exact characterization the lower bound further for Gaussian reward distributions in Theorem 3.2. It shows that the hardness depends on the bilinear projection of the mean matrix of arms onto the boundary of a normal cone of policies and the preferences. This is novel w.r.t. the existing gap-dependent lower bounds that hold either for a narrow range of \(_{a}\)'s (Ararat and Tekin, 2023), or fixed preference (Kone et al., 2023a).

**2. Algorithm Design.** First, we observe that the optimisation problem in our lower bound involves minimisation over a non-convex set. We provide a convex relaxation of the problem based on ideas from disjunctive programming (Theorem 4.1 and 4.2). We then leverage this lower bound to propose a novel Track-and-Stop (Garivier and Kaufmann, 2016) style algorithm, called PreTS (Preference-based Track-and-Stop). In Theorem 4.3, we devise a new stopping rule that can handle the preference-aligned suboptimality gaps between the arms.

**3. Sample Complexity Analysis.** Finally, we provide an upper bound on sample complexity of PreTS. This requires us to define a distance metric between two pareto sets of arms, and proving a concentration bound with respect to this metric (Theorem 5.1). In Theorem 5.2, we prove that sample complexity of PreTS matches the convexified lower bound up to constants.

### Related Works

In the past decade, works on multi-armed bandits also focuses on pure-exploration in addition to regret minimization. Regret minimization and pure-exploration differ in the sense when arms in pure-exploration are immediately discarded upon being deemed as sub-optimal, whereas, in the regret minimization setting, sub-optimal arms may still be played since they provide additional information about other arms. Pure-exploration problems have been considered in two settings: fixed-budget and fixed-confidence. The fixed-budget setting aims at bounding the probability of underestimating the best arm given a budget of samples. Audibert and Bubeck (2010) propose the first algorithm for the fixed budget setting. Here, the budget is divided into \(K-1\) rounds and at the end of every round, the arms with the lowest empirical mean are discarded. On the other hand, best-arm identification is a version of the pure-exploration problem with scalar rewards (Even-Dar et al., 2006). In this setting, we are given a \((0,1)\) and the goal is to identify the best-arm with probability at least \(1-\). Several strategies such as those based on elimination, adaptivity, racing, upper-confidence bounds have been proposed to minimize the number of expected pulls of an arm in the fixed confidence setting by (Kalyanakrishnan et al., 2012; Gabillon et al., 2012; Jamieson et al., 2014; Garivier and Kaufmann, 2016; Jedra and Proutiere, 2020). Arm rewards can be modeled as a vector with Gaussian Process (Zuluaga et al., 2016), linear rewards (Drugan and Nowe, 2013; Lu et al., 2019), and non-parametric rewards (Turgay et al., 2018), which can include contextual bandit formulations (Tekin and Turgay, 2017; Shukla, 2022). In recent past, the pure exploration techniques have been successfully applied in hyperparameter tuning (Li et al., 2018) and black-box optimization problems (Contal et al., 2013; Wang et al., 2021, 2022) demonstrating considerable performance gains.

In a marked deviation, given an instance of the bandit problem, the goal of this paper is to identify the entire Pareto front. A key observation in this regard is that there might be arms, which are sub-optimal for almost every objective but still lie on the Pareto front. Further, since sampling an arm returns a vector of rewards determining an arm-strategy that reduces the uncertainty in the estimate of every reward function is challenging. An immediate consequence of these differences is the fact that the complexity of identifying the Pareto front is different from that of best arm identification. Auer et al. (2016) consider the Pareto front identification problem in the multi-armed bandit model and establish sample complexity bounds for the problem in terms of relevant problem parameters in the fixed-confidence setting. The multi-armed bandit problem is further studied under cone-based preferences by Ararat and Tekin (2023). The main contribution of (Ararat and Tekin, 2023) are bounds on the sample complexity of the problem in terms of gap-based notions that depend on the cone. Karagozlu et al. (2024) builds upon this work to introduce adaptive elimination based algorithms for learning the Pareto front under incomplete preferences. When the reward vectors are Gaussian processes Korkmaz et al. (2023) propose an elimination based algorithm based for identifying the Pareto front. The goal in these works is to identify the set of arms that are \(\) close to the Pareto front as the sample complexity to identify the exact Pareto set can be very large. Kone et al. (2023) consider the problem of identifying a relevant subset of the Pareto set using a single sampling strategy Adaptive Pareto Exploration, along with different stopping rules to consider variations of the Pareto Set Identification problem. crepon et al. (2024) consider the exact Pareto front identification problem in the multi-armed bandit setting but with fixed and known preference cone. They propose a lower bound and a computationally efficient gradient-based algorithm to implement a track-and-stop based strategy. To the best of our knowledge, ours is the first work to consider _the exact Pareto front identification problem from a pure-exploration perspective_. Therefore, our proposed framework can be used for identifying the Pareto front given a preference cone for several variants of the bandit problem including the standard multi-armed bandit problem, linear bandits, etc.

## 2 Preference-based Pure Exploration Problem

In this section, we formalise the fixed-confidence setting of preference-based pure exploration and introduce the notations.

**Notations.** For \(n\), let \([n]\) denote the set \(\{1,2,,n\}\). We use \(\|\|_{1},\|\|_{2},\|\|_{}\) to denote the \(_{1}\)-norm, \(_{2}\)-norm and \(_{}\)-norm, respectively. For a vector \(z,\ z^{()}\) denotes its \(^{th}\) component. Let \(e_{}\) denote the vector with \(1\) in the \(^{th}\) position and zero otherwise. \(_{K}\) denotes the simplex on \([K]\). \(d_{}(P,Q)\) measures the KL-divergence between distributions \(P\) and \(Q\). \((A)\) is the vectorized version of matrix \(A\). \(\) is the vector of all \(1\)'s. Further details of notations are deferred to Appendix A.

**PrePEx: Problem Formulation.** In PrePEx, a learner can access a bandit instance with \(K\) arms. Each arm \(k[K]\) corresponds to a reward distribution \(P_{k}\) over \(^{L}\) with unknown mean \(M_{k}^{L}\) and known covariance \(=(_{1}^{2},,_{L}^{2})\). Here, \(L\) denotes the number of objectives corresponding to each arm. Thus, a bandit instance can be specified with the vector of mean rewards \(\{M_{k}\}_{k=1}^{K}\). For brevity, we represent them with a matrix \(M^{L K}\) such that its \(k^{}\) column is \(M_{k}\). At each time \(t\), the learner pulls an arm \(k_{t}[K]\) and observes the corresponding reward vector \(R_{t}\) sampled from \(_{k_{t}}\). In pure exploration, the learner typically focuses on finding the best arm, i.e. the arm with highest mean (Garivier and Kaufmann, 2016). In pure exploration, a more general setting of BAI, the learner aims to find a policy \(_{K}\) that dictates the arm-proportion to choose to maximize the expected reward from the instance.

Following the vector optimization literature (Jahn et al., 2009; Ararat and Tekin, 2023), we assume that the learner has additionally access to an ordering cone \(\).

**Definition 2.1** (Ordering Cone).: _A set \(^{L}\) is called a cone if \(v\) implies that \( v\) for all \( 0\). A solid cone has a non-empty interior, i.e., \(()\). A pointed cone contains the origin. A closed convex, pointed and solid cone is called an ordering cone._

An ordering cone can be both polyhedral and non-polyhedral. Following the literature (Ararat and Tekin, 2023; Karagozlu et al., 2024), we consider access to a polyhedral ordering cone.

**Definition 2.2** (Polyhedral Ordering Cone).: _An ordering cone \(\) is a polyhedral cone if \(\{x^{L} Ax 0\}\), where \(A^{K L}\) with rows \(a_{i}^{}\). \(A\) is called the half-space representation of \(\)._

Each polyhedral ordering cone induces a set of partial order on the reward vectors in \(^{L}\). To ignore the redundancies and to focus on the bandit problem, we further assume that \(A\) is full row-rank and \(\|A_{i}\|_{2}=1\)(Ararat and Tekin, 2023). Hereafter, we call them _preference cones_, and the vectors in the cone as the _preferences_. We refer to (Jahn et al., 2009; Lohne, 2011) for further details on cones.

**Example 2.1** (Preference cones).: _The positive orthant \(^{L}_{+}\) is a polyhedral ordering cone. This is the one used in the pareto-set identification literature (Auer et al., 2016; Kone et al., 2023b; crepon et al., 2024). The cones with all non-negative entries are called solvency cones and used in finance (Kabanov, 2009). Another simple example is \(_{/3}\{(r,r)^{2}  r 0[0,/3]\}\), i.e.,all the 2-dimensional vectors that make an angle less than \(/3\) with the \(x\)-axis._

**Definition 2.3** (Partial Order).: _For every \(,^{}^{L},_{}^{}\) if \(^{}+\) and \(_{}\) if \(^{}+()\). Alternatively, \(_{}^{}\) is equivalent to \(z^{}(-^{}) 0, z\)._

The partial order induced by \(\) induces further order over the set of arms \([K]\).

**Definition 2.4** (Order over arms).: _Consider two arms \(i,j[K]\). (i) Arm \(i\) is weakly dominated by arm \(j\) iff \(M_{i}_{}M_{j}\). (ii) Arm \(i\) dominates arm \(j\) iff \(M_{j}_{\{0\}}M_{i}\). (iii) Arm \(i\) strongly dominates arm \(j\) iff \(M_{j}_{}M_{i}\)._

**Definition 2.5** (Pareto Optimal Set).: _An arm \(i[K]\) is Pareto Optimal if it is not dominated by any other arm w.r.t. the cone \(\). The Pareto Optimal Set \(^{*}\) is defined as the set of all Pareto Optimal arms._

Given a preference cone, a learner aims to identify exactly the Pareto Optimal Set from a finite set of arms \([K]\) whose mean rewards belong to the Pareto Optimal Set w.r.t. \(\). Alternatively, this vector optimization problem can be represented in the policy space as finding a policy \(_{K}\) supported on the Pareto Optimal Set of arms. The following vector optimization problem yields this:

\[V(M)_{_{K}}\ M.\] (1)

In this context, we denote the set of Pareto optimal policies as \(^{*}(M)_{_{K}}\ M\) over \(\). We assume that \(^{*}(M)\) is non-empty.

**Example 2.2** (Pareto Optimal Sets for different cones).: _Figure 1 illustrates the Pareto Optimal Sets among \(2\)-dimensional mean vectors of \(200\) randomly selected arms under preference cones \(_{/2}\)

Figure 1: Effect of cone selection on size of Pareto optimal setand \(_{/3}\). We observe that the Pareto Optimal Sets for them (in pink and blue respectively), are completely different for the same set of arms. Thus, we have to adapt to the available preferences to solve the aforementioned problem. As noted later, the geometry of this cone plays a crucial role in determining the Pareto front._

In PrePEx, we consider the problem of Equation (1), when the mean matrix \(M\) is unknown a priori but bounded, i.e., the entries of \(M,\ M_{ij}[M_{},M_{}]\). We denote all such mean matrices by \(\). Identifying the policy will lead us to identify the true Pareto front \(^{*}\). In the noisy feedback setting, the reward at time \(t\) is \(R_{t}=M_{k_{t}}+_{t}\), where \(_{t}^{L}\) is the noise vector. We assume that the noise vectors \(_{t}\) are independent of \(M_{k_{t}}\) and also across time. Further, they are sub-Gaussian with parameter \(\) and adapted to the filtration \(_{t}\), which is a standard assumption in the literature. A policy \(^{K}\) is a randomized mapping from the history \(_{t}\) to the probability simplex over the set of arms \([K]\). _In Preference-based Pure EXploration (PrePEX) problem, the goal of the learner is to identify a Pareto optimal policy in \(^{*}\) (Equation (1)) given an instance \(M\) and a preference cone \(\) while observing only noisy rewards from the arms, and also using as few observations as possible._

**Definition 2.6** (\((1-)\)-correct PrePEX).: _An algorithm for Preference-based Pure Exploration (PrePEx) is said to be \((1-)\) correct if with probability \(1-\), it recommends a Pareto optimal policy \(^{*}\)._

For example, a pareto optimal policy for \(_{/2}\) would be a distribution in \(_{K}\) with support on the arms corresponding to the pink reward vectors (Figure 1). For \(_{/3}\), it would be one with support on blue points. Finally, we make the standard assumption on the mean rewards.

**Assumption 2.1** (Single Parameter Exponential Family).: _Let \(X=(X_{1},,X_{d})\) be a \(d\)-dimensional random vector with a distribution \(P_{},\). Suppose \(X_{1},,X_{d}\) are jointly continuous. The family of distributions \(\{P_{},\}\) is said to belong to the one parameter Exponential family if the density of \(X\) may be represented in the form \(f(x|) h(x)(()T(x)-())\). We assume that the mean reward for each vector belongs to a single-parameter exponential family with variance bounded by \(1\)._

## 3 Lower Bound on Sample Complexity

We begin by deriving a KL-divergence based lower bound for PrePEx using techniques from (Garivier and Kaufmann, 2016). Our lower bound is based on establishing a change-of-measure argument in the spirit of (Graves and Lai, 1997; Kaufmann et al., 2016). The lower bounds are derived first by defining a set of alternating instances \(\) for a given bandit instance and then by trying to compute an optimal allocation policy \(w_{K}\) that maximises the sum of minimum KL-divergence between any instance in \(\) and the bandit instance under interaction. _The key insight of our work is to formulate the identification of Pareto Set problem in the policy space rather than in the arm space as done in antecedent literature._ This formulation helps us to derive the KL-based lower bound, which is more general than the existing suboptimality gap-based lower bounds (Auer et al., 2016; Ararat and Tekin, 2023; crepon et al., 2024).

**The Alternating Instances with respect to Pareto Fronts.** The learner needs to distinguish between all instance \(\{M\}\) for which the Pareto front associated with \(\) is different from the one associated with \(M\). At first, given an optimal policy of \(M\), say \(^{*}\), it would appear that the set of confusing instances is \(_{^{*}}(M)^{}\{ :^{*}_{}_{} \}\). However, this is fallacious since the instances whose rewards dominate \(M\) can also confuse a policy \(\). Given a \(^{}\), the correct alternating set is the set of instances in \(\) whose Pareto optimal set is not dominated by \(^{}\) corresponding to \(M\).

\[_{^{*}}(M) \{\{M\}:_{ }_{}^{*}\}\] \[=\{\{M\}: z \ \ _{}z^{}>z^{}^{*}\}\,.\]

With this new alternate set defined, we now establish lower bounds on the performance of any PrePEX algorithm.

**Theorem 3.1** (Lower Bound).: _Given a bandit model \(M\), a preference cone \(\), and a confidence level \([0,1)\), the expected stopping time of any \((1-)\)-correct PrePEx algorithm, to identify the _Pareto Optimal Set is_

\[[]_{M,}( ),\] (2)

_where, the expectation is taken over the stochasticity of both the algorithm and the bandit instance. Here, \(_{M,}\) is called the characteristic time of the PrePEx instance \((,)\) and is expressed as_

\[(_{M,})^{-1}_{w}_{ {c}\{^{*}\}\\ ^{*}^{*}(M)}_{_{^{*} }(M)}_{z}_{k=1}^{K}w_{k}d_{}(z^{}M_{k},z^{}_{k})\,,\] (3)

_such that \(_{^{*}}(M)_{\{^{*} \}}\{:\;z,\;(z(-^{*})^{}), =0\}.\)_

_Proof_ _Intuition._ First, we observe that an instance \(\) is in alternating set if there exists a \(\{^{*}\}\) and \(z\), such that \(z^{}(-^{*})>0\). If \(\) and \(^{*}\) were pure strategies, it would have been exactly \(_{z\{0\}}z^{}(_{a}-_{a^{*}})>0\). Let us denote the \(z\) achieving the \(\) as \(z_{}\), i.e., the preference for which \(_{a}\) and \(_{a^{*}}\) are least distinguishable. Thus, we observe that \(z_{}^{}_{a}\) exactly functions as the mean of the arm \(a\) in an instance \(\), whereas \(z_{}^{}(_{a^{*}}-M_{a})\) acts as the suboptimality gap. Now, we extend this idea in the classical lower bound scheme to get a nested optimization problem with \(\) over \(z\) and \(\) in the alternating set, and a sup over allocations \(w_{K}\). We further show that the \(\) for \(\) appears at the boundary of the alternating set defined as \((M)\).

**Discussions.** (i) Novelty: In the best of our knowledge, this is the first lower bound for PrePEx with fixed confidence with an explicit KL-based dependence. All the existing lower bounds are gap dependent, and valid for a narrow range on mean vectors or known preference cone, i.e. the right orthant. Our proof does not need such assumptions. The gap-dependent bounds are special case of ours (cf. Theorem 3.2 for the case of Gaussian rewards).

(ii) Geometric Insights. Theorem 3.1 provides multiple geometric insights into the affect of the ordering cone \(\) on the characteristic time. _First_, the alternating set \(_{^{*}}(M)\) is piece-wise polyhedral and non-convex. We address consequent issues in Section 4.1. _Second_, there is an additional minimization over the vectors lying in the cone \(\). We interpret the minimization over vectors in the cone as a _instance- and preference-dependent scalarization of the distance between the given instance \(M\) and the corresponding most-confusing instance in \(_{^{*}}(M)\). Third_, in the proof, we show that the reward gap using the best policy \(^{*}\) and a given policy \(\) for the most confusing instance belongs to the polar cone \(^{}\) of the preference cone \(\). The most confusing lies on the boundary of this polar cone and its projection the policy gaps \((^{*}-)\). Further insights can be obtained by imagining the polar cone to be orthogonal to the cone \(\). Then, the vector of reward-gaps for the most confusing instance for every objective is orthogonal to the generating rays of \(\). These novel geometric insights are complementary to the existing algebraic and statistical insights available in the lower bound literature (Kone et al., 2023; Ararat and Tekin, 2023).

### Characteraization of Lower Bounds for Gaussians

To understand our lower bound better and to compare it with the literature, we present a reduction for Gaussian bandits. In Gaussian bandits, we assume that the reward vectors of arm \(a[K]\) are generated from a multivariate Gaussian distribution \((_{a},)\), where the covariance is a diagonal matrix: \((_{1}^{2},,_{L}^{2})\).

**Theorem 3.2** (Lower Bound for Gaussian Bandits).: _1. Given any \(^{}^{*}(M)\) and \(N(^{*})\) being the set of neighbouring policies of \(^{*}\), the most confusing instance of \(M\) belongs to the set_

\[\{\{M\}:_{k,}=M_{k,}- ^{2}}{z^{}}-)_{k}}{w_{k}},\;\;  N(^{*}),z\{0\},k[K],[L] \},\]

_where \(M(^{*}-)}{()\|^{*}- \|_{(1/w)}^{2}}\)._

_2. The inverse of characteristic time, i.e. \((_{M,}^{})^{-1}\), for an instance \((M,)\) is_

\[_{ N(^{*})\\ ^{*}^{*}(M)}_{z\{0\}}M(^{*}-))^{2}}{2()\|^{*}-\|_{2}^{2}}\,.\]

**Consequences.**_First_, we observe an interesting phenomenon that a bilinear projection of mean matrix \(M\) on the preferences and policy gaps operates as an extension of suboptimality gap in classical BAI. This is a reminiscent of the lower bound for pure exploration under known linear constraints as in Carlsson et al. (2024) who show that the hardness of the problem depends only on the projection of the mean vector on the policy gap. In addition to similar projection structure, preferences introduce a novel bilinearity here. _Second_, we show how the lower bound inflates with the covariance matrix for each objective. This shows the richness of our KL-divergence based lower bound as opposed to gap-based bounds which have difficulty accommodating variance related terms directly.

_Connection to existing results._ Our result generalizes several existing lower bounds for BAI.

1. _BAI lower bound._ Our lower bound is able to recover that of Kaufmann et al. (2016) for the standard BAI problem with fixed confidence. In the case of the standard BAI problem, the ordering cone is given by \(_{+}\) and therefore the minimization over \(\) in (3) becomes redundant. The definition of the alternating set is then given by the set of instances which have a different optimal arm than \(\) which is exactly the set considered in (Kaufmann et al., 2016).

2. _Pure exploration under known constraints._ Our lower bound is able to recover the lower bound of Carlsson et al. (2023) for the BAI problem with fixed confidence and linear constraints. This is the case with \(L=1\) and the ordering cone being \(_{+}\) making the minimization over \(z\) in (3) redundant. \(_{^{*}}(M)\) becomes \(_{^{*}}(M)=\{:_{}^{ }^{}^{*}\}\) and we retrieve exactly their Corollary 1.

## 4 Algorithm Design: PreTS

In this section, we propose an algorithm that tracks the lower bound. However, this is not straightforward since the alternating set is non-convex. We first propose a convex relaxation for this set and then, design a Track and Stop style algorithm, called PreTS.

### Convex Relaxation of the Lower Bound

One of the major differences regarding the structure of lower bounds compared to a standard BAI problem is that \(_{^{*}}(M)\) is a piece-wise polyhedron, i.e., a union of hyperplanes. Each hyperplane corresponds to a policy \(\{^{*}\}\). To make the optimization problem tractable and obtain a convex program, we relax \(_{^{*}}(M)\) using its convex closure, denoted by \((_{^{*}}(M))\). We note that the construction of such a convex relaxation for track-and-stop (when the lower bound problem is non-convex) has been done in the MDP setting AI Marjani and Proutiere (2021). We define \((_{^{*}}(M))\) in Theorem 4.1 by formulating it as a disjunctive program, which we can reformulate further as a linear program (Balas, 1985).

**Theorem 4.1**.: _Let \(_{^{*}}\{:\;\;z,(z^{}(-^{*}) ),=0\}\). Fix \(z\) such that \(z=_{i}_{i}v_{i}\). Then, we have \(()=\), where \(\) is defined as_

\[\{:^{}( )_{0},=_{i}u_{i}_{i}(v _{i}^{}(-^{*})),_{0} u_{i}_{i}_{i}v_{i} ^{}^{*}\}.\] (4)

Using the convex hull (Eq. (4)), we quantify the optimal value for a given allocation \(w\) as

\[}_{}(w,M)_{(_{^{*}}(M))}_{z} _{k=1}^{K}w_{k}d_{}(z^{}M,z^{})\,.\]

The corresponding optimal allocation is

\[^{*}(M)=_{w^{K}}_{ ^{*}\\ ^{*}^{*}(M)}_{( _{^{*}}(M))}_{z}_{k=1}^{K}w_{k }d_{}(z^{}M,z^{})\,.\] (5)

Hereafter, we consider Equation (5) as the optimization problem to be tracked. To compute \(}_{}(w,M)\), we need access to the true instance \(M\) which is not available to us. Our Track-and-Stop strategy is based on repeatedly sampling an arm to construct an estimate of \(M\), i.e. \(M_{t}\), and exploiting continuity properties of \(}_{}(w,M)\) to show that \(}_{}(w,M_{t})}_{}(w,M)\) and the cumulative number of arm plays \(N_{t,k} w_{k},\ w_{k}^{*}(M)\). These properties ensure that it makes sense to design a Track and Stop style algorithm for this problem.

**Theorem 4.2** (Analytical Properties).: _For all \(M^{L K}\) and all preference cones \(\), we get 1. The mapping \((w,M)}_{}(w,M)\) is continuous. 2. The characteristic time mapping \(M_{M,}}\) is continuous. 3. The set valued function \(M^{*}(M)\) is upper-hemicontinuous. 4. The set \(^{*}(M)\) is convex._

_Discussion: Cost of Convexification._ For Gaussian bandits, as we can get the analytical form of the most confusing instance \(\) (Theorem 3.2), we do not pay any extra cost of convexification. In the non-Gaussian settings, where we cannot find such analytical forms for the most confusing instances, the minimum value of the inner minimisation problem under the convex hull (Equation (5)) can go lower than the minimum value found in the original non-convex set of instances (Equation (3)). Thus, the characteristic time attained by solving the convex relaxation might be higher than that of the original lower bound. Hence, an algorithm solving the convex relaxation has a higher stopping time. However, convexification is essential for the computational feasibility of a lower bound-tracking algorithm for PrePEx. This computational-statistical trade-off will be interesting to study in the future.

### Algorithm: Preference-based Track-and-Stop (PreTS)

We now construct a general recipe to design a PrePEx algorithm when we do not have access to the true instance \(M\). The fundamental element of any such recipe is constructing an estimate of \(M\). For a given set of observed rewards \(\{R_{t}\}_{t=1}^{T}\), we obtain a column-wise empirical average of the observed rewards and use it as our estimator of \(M\). Now, we elaborate the three key components of our PrePEx algorithm Preference-based Track and Stop (PreTS, Algorithm 1).

1. **Sampling Rule:** For the sampling rule, we consider a Track-and-Stop strategy (Garivier and Kaufmann, 2016). It tracks the optimal proportion of arm sampling by plugging in the empirical estimates of means and empirical count \(N_{k,t}\) in the convexified lower bound. This leads to an allocation policy with improved information acquisition.

2. **Stopping Rule:** Our ultimate stopping goal is to identify arms that are on the Pareto front. Based on this, we define the confidence set as:

\[c(t,)\{:\,_{z} _{k}N_{k,t}d_{}(z^{}_{t}^{(k)},z^{}^{(k )})(t,)\}\,,\] (6)

where \((t,)_{a S}3(1+(N_{k,t}) )+K()}{K})\) and \(\) is defined in Equation (17). Our first claim is to show that the true instance belongs to the confidence ellipsoid with high probability.

**Lemma 4.1** (Confidence Ball).: _For any \(t\) and \(c(t,)\) is defined in Equation (6), we have \((M c(t,))\)._

Thus, we can now formalise the corresponding Chernoff-stopping rule as

\[_{(_{x^{*}}(_{t} ))}_{z}_{k}N_{k,t}d_{}(z^{ }_{t}^{(k)},z^{}^{(k)})(t,)\] (7)

Given the estimates \(_{t}\), the problem in Equation (7) can be solved efficiently. Next, we show that upon stopping with Equation (7), PreTS returns the true Pareto Front \(^{*}\) with probability \(1-\). Let\(}_{t}\) denote the estimated Pareto Front at time \(t\), which is constructed using estimates \(_{t}\). Then at stopping time \(\), we have

\[(^{*}}_{t})  (\;t:_{k,}N_{k,t}d_{ }(z^{}_{t}^{(k)},z^{}M^{(k)})(t, ))\] \[ _{t=1}^{}(_{k,}N_{k,t}d_{}(z^{}_{t}^{(k)},z^{}M^{(k)})(t,) )\]

where, the last inequality is true due to Theorem 4.3, a concentration result on the KL-divergence with preference projected mean rewards.

**Theorem 4.3**.: _For all \((K+1)\), \(z\) and \(t\), we have that:_

\[[_{k[K]}N_{k,t}d_{}(z^{}_{t}^{( k)},z^{}M^{(k)})](-)()^{K}(K+1)\]

3. **Recommendation Rule:** At the end of stopping time \(\), the algorithm returns an estimate of the Pareto Front \(}_{}\).

## 5 Upper Bound on Sample Complexity

Now, we prove upper bound on the expected sample complexity of PreTS. This requires us to the Track-an-Stop proof technique. But the challenge is to show concentration of the pareto fronts under a suitable metric.

**Concentrating to the Pareto Front.** To show that upon stopping the algorithm returns the true Pareto Frontier, we need to establish a valid metric to show such convergence. Usually, the distance between sets is measured using the Hausdorff metric (Costantini and Vitolo, 1995), i.e. \(d_{H}(}_{},)\{_{k }_{}}_{k^{}}\|M_{k}-M_{k^{}} \|_{},\;_{k}_{k^{}}_{ }}\|M_{k}-M_{k^{}}\|_{}\}\). But the Hausdorff distance only defines a pseudo-distance between sets and \(\) may not be closed under this metric. To circumvent this issue, we build upon the notion of a gap-based metric considered in the antecedent literature (Auer et al., 2016) to measure the distance between the mean reward of an arm and a given Pareto Front. We extend it to a distance metric between elements in the space of Pareto Fronts \(\).

**Definition 5.1** (Distance from Pareto Front).: _The distance of the mean of arm \(k\) from the Pareto Front \(^{*}\) is \(d(k,^{*})_{ 0}\), such that \(M_{k}+_{}M_{k^{}},\;k^{} ^{*}\). Equivalently,_

\[d(k,^{*})_{k^{}^{*}}\{0, _{z(1)}z^{}(M_{k^{}}-M_{k} )\}.\] (8)

**Definition 5.2** (Distance between Pareto Fronts).: _We define the metric between Pareto Fronts \(d_{}((,)\,,): _{ 0}\) as \(d_{}(,^{*})\{_{k }d(k,^{*}),_{k^{*}}d(k,)\}\)._

In the appendix, we establish that (i) \(d(,)\) is a valid metric on \(\), and (ii) \(\) is compact and complete under \(d(,)\). Now, we leverage this metric to show that the Pareto Front defined by the arm-wise constructed estimator \(_{t}\) concentrates towards the true Pareto Front.

**Theorem 5.1** (Concentration of mean estimates).: _For any pair \((i,j)[K][K]\) and \(z\), we have_

\[|z^{}(M_{i}-M_{j})-z^{}(_{i,t}-_{j,t })|_{ij}(t)\,,\]

_where \(_{ij}^{2}(t) 4\|z\|_{1}^{2}h(}{2})}{2} )+_{a\{i,j\}}(4+(N_{a}(t)))(_ {a\{i,j\}}(t)})\), \(K_{1}\), and \(h(x) x+(x)\)._Proof Sketch.: This is a consequence of jointly applying a vectorial concentration result for multiple-objectives of each arm (Kaufmann and Koolen, 2021), and pairwise time-uniform concentration bounds (Kone et al., 2023a). A key observation here is that the confidence radii depends on the magnitude of the preference vector \(z\) and scales with different objectives accordingly.

**Sample Complexity of PreTS.** Using this new concentration result for the Pareto Front and the stopping rule in Equation 7, we derive an upper bound on the expected stopping time of PreTS.

**Theorem 5.2** (Upper Bound on Sample Complexity).: _For any \(>0\) and \(c(t,)\) defined in (7), we have that the stopping time satisfies_

\[_{ 0}[]}{()} _{M,}}\ \ M^{L K}\]

The basic outline of the proof follows a general strategy to prove Track-and-Stop result. However, the new arguments lie in establishing that the Pareto fronts converge under a suitable metric sufficiently fast. Our proof implies that PreTS matches the convex relaxation of the lower bound asymptotically at the corresponding risk level \(\). Strictly, speaking this is not _asymptotically optimal_ since, we do not track the exact lower-bound.

## 6 Conclusion and Future Works

We study the fixed-confidence version of preference-based pure exploration problem under linear stochastic bandit feedback, where each arm corresponds to a reward vector ordered according to a preference cone. We derive a novel lower bound for this problem. We leverage the lower bound further to derive a track-and-stop based algorithm for PrePEx problem. As future work, it would be interesting to verify our results on a real-world datasets.

Additionally, it would be interesting and challenging to study how other asymptotically optimal pure exploration strategies, e.g. gamified explorers (Degenne and Koolen, 2019), top-two algorithms (Jourdan et al., 2022), can be adapted to this setting. In general, improving the computational efficiency and studying the optimality gap with respect to the non-convex lower-bound problem would be of fundamental interest.