# ## 1 Introduction

## 1 Introduction

Initially introduced as a minor and vaguely defined feature in OpenAI's GPT API, the concept of the system message has grown significantly in popularity and usage. The research and broader AI community have achieved a degree of consensus on the intent and purpose of system messages: a higher privilege message type for providing instructions that apply throughout the conversation and superseding any conflicting instructions in user messages. System messages are used today to define custom large language model (LLM) applications, implement model guardrails and content policies, defend against jailbreak attacks and prompt injection, establish role-playing personas, or otherwise steer model behaviors.

Yet unlike the concept of user/system privilege in traditional computing, system message precedence is much less reliable as it is implemented through training and therefore susceptible to errors and adversarial attacks like many other neural network behaviors. Models can easily "forget" their system messages when faced with long contexts, or be tricked into intentionally violating them.

Despite their popularity and importance, the robustness of system messages remain understudied by the scientific community. In this work, we conduct a simple empirical study of supervised fine-tuning LLMs to improve their system message adherence and robustness. We first collect a synthetic dataset for supervised fine-tuning, starting from real-world system instructions sourced from OpenAI's GPT Store and HuggingFace's HuggingChat platform and leveraging proprietary LLMs to generate user and assistant responses. We then put together a small battery of benchmarks modified from previous work to quantify robustness and compare against other supervised fine-tuning data mixes. We will release all data and code to the research community to encourage further research on this topic.

## 2 Related Work

### Prompt Injection

Prompt injections are a form of adversarial attack targeting LLM applications where a user overrides trusted instructions set by the application developer with new instructions to trigger unintended behaviors (Perez & Ribeiro, 2022; Willison, 2024b). These attacks have become a significant security risk for LLM-based applications (LLMRisksArchive; Greshake et al., 2023; Willison, 2024a;*  Liu et al., 2024b; Rehberger, 2024c), with numerous production systems already compromised (PromptArmor, 2024; Rehberger, 2024a; Willison, 2022). To explore the range of potential attacks and assess model robustness, previous work has organized large-scale prompt hacking competitions and games, resulting in extensive datasets and benchmarks of adversarial and defensive prompts (Schulhoff et al., 2023; Toyer et al., 2023; Al, 2023; Debenedetti et al., 2024). We use these curated, human-generated attacks and similar benchmarks to evaluate model robustness (Mu et al., 2024), though emerging techniques for automated attacks may be valuable for future research (Liu et al., 2024a; Yu et al., 2024a).
*  Various defenses have been proposed to protect LLM applications from prompt injections, but most rely on defensive prompts or separate modules in larger systems(ProtectAI.com, 2024; Abdelnabi et al., 2024) rather than on intrinsic model-level robustness. Chen et al. (2024); Zverev et al. (2024) and Yi et al. (2024) focus on differentiating between instructions and data and propose initial methods for training models to recognize this distinction. These papers are mainly concerned with defending against indirect prompt injection attacks (Hines et al., 2024; Greshake et al., 2023) and do not explore instruction precedence. The most similar work to ours is by Wallace et al. (2024), though their data and models are not publicly available, limiting their utility for advancing model-level defenses and understanding system prompt behavior.

### System Instructions

System messages are a powerful way to steer LLMs and specify preferences for their behavior at inference time (Touvron et al., 2023; Mukherjee et al., 2023; Jiang et al., 2024). Application developers can use them to provide context and rules for a model to follow during conversations with a user, allowing for fairly complex applications to be built without task-specific model training(OpenAI, 2023; 2024b). However, despite the intended behavior specified by system prompts, these rules are relatively easy to bypass (Yu et al., 2024b) and malicious users can steal valuable IP like the system prompt itself, any files provided in context, etc.

To address this issue, Wallace et al. (2024) and Lu et al. (2024) emphasize the importance of training models to prioritize instructions and show that post-training techniques like instruction tuning and RLHF can significantly enhance robustness against conflicting instructions. Despite this training, users have still found simple ways of prompt injecting models trained with an instruction hierarchy, highlighting the need for more research in this direction (OpenAI, 2024a; Rehberger, 2024b; Pliny and Liberator, 2024).

Beyond adversarial robustness, there is a growing body of research examining other uses and properties of system messages. Lee et al. (2024) describe a method to train LLMs using system messages to better align with diverse user preferences. Meanwhile, Li et al. (2024a) demonstrate that LLMs often struggle to consistently maintain specified personas across multiple turns of conversation. Concurrent with this work, Qin et al. (2024) introduce a benchmark to evaluate many features of system prompts at once, including multi-turn stability, system message priority, and compliance over many domains.

## 3 Data Collection

In order to build a useful dataset for fine-tuning stronger system message adherence, we need realistic system messages, challenging and relevant user messages, and exemplary assistant messages. Our overall pipeline is illustrated in Figure 1. We begin by collecting a set of real-world LLM system prompts from OpenAI's GPT Store and HuggingFace's HuggingChat platform. After a series of filtering steps, we are left with 1386 unique system messages. We then prompt Claude 3.5 Sonnet to generate 5 benign and 5 adversarial user messages corresponding to each system message. Finally, we build a simple tool-enabled assistant running on GPT-4o mini with working web search/browsing and Python tools, along with a mock image generation tool. We prompt this assistant with our system and user messages and collect the responses, altogether yielding 13.9K training samples and 3.8M training (assistant) tokens which we call the sudo dataset. Our LLM prompts and dataset examples are shown in Appendix B.

### System Messages

To source realistic system messages, we first turn to OpenAI's GPT Store which hosts user-created custom GPTs defined by a system prompt and a set of enabled tools such as web browsing or custom REST APIs for the model to use. These custom GPTs are built by users for a wide variety of use cases, including commercial purposes, and contain many different guardrails.

The prompts that define a custom GPT are often carefully guarded, with many containing instructions to never reveal or even discuss the contents of the prompt. However, the susceptibility of even leading LLMs like GPT-4 to prompt injection tricks means that it is usually quite easy to extract a system prompt. We started with two public collections of previously extracted prompts1, and joined this with GPT Store metadata indicating which tools are available to the custom GPT. We use this metadata information to remove any system messages that expect file/image uploads from the user message and any messages that rely on custom HTTP APIs besides browsing, after which 619 distinct system messages remain.

We also conduct a scrape of user created assistants on HuggingFace's HuggingChat platform, for which system messages are publicly visible. Starting with 4244 system messages, we remove exact duplicates which leaves 2716 system messages. We then combine both the GPT Store and HuggingChat system messages for further filtering to remove extremely long prompts (\(>\) 4000 Mistral 7B tokens), partially duplicated prompts2, non-English prompts, and obscene prompts. Finally, we use Claude 3.5 Sonnet to extract all discrete guardrail clauses from each system message and remove

Figure 1: The data collection process for creating our sudo dataset. We rely heavily on model-based and LLM-based quality filtering, and generate user and assistant messages with Claude and GPT models, respectively.

ones with less than 3 separate guardrail clauses. This selects for more complex and interesting prompts, while filtering out many low-quality prompts and role-playing "persona" prompts. In all, this yields 1386 distinct system messages.

### User Messages

In order to collect demonstrations of assistant responses that robustly prioritize the system message, we first need to generate challenging, adversarial user messages that try to convince the assistant to violate one or more guardrails in the system message. To retain model utility and avoid inappropriate _overrefusals_, we also require thorough coverage of benign user message for which the assistant is able to demonstrate helpful, high-quality responses. We find that with a bit of prompting, Claude 3.5 Sonnet is able to synthesize creative and relevant user messages. Claude's adversarial user messages were surprisingly creative, often targeting various different guardrails within the system message via rather indirect means.

### Assistant Messages

GPT-4o mini is OpenAI's first model release trained with the improved defense methods explored in wallace2020learning. Though smaller and less adept at complex reasoning, we find that it is plenty capable of producing high-quality responses to both benign and adversarial user messages in our dataset. Since many of the GPT Store assistants revolve around tool-calling, we implement 4 simple tools for GPT-4o mini to use in its responses: web search using Brave, web browsing using Scrapfly, local Python script execution, and a dummy image generation API that records the model's image prompt. These tools, particularly the two web tools, add significant variation and diversity to our dataset by pulling in information external to the model and the system/user messages.

## 4 Benchmarks and Evaluations

We use three benchmarks, RuLES, Gandalf, and TensorFlowTrust, to quantify system message robustness. Additionally to measure the general utility of the model and verify that we are not overfitting to the particular behavior of system message compliance, we also rely on MMLU and Arena-Hard-Auto.

### Robustness

**RuLES (Mu et al., 2024).** RuLES is a benchmark consisting of 14 text-based scenarios designed to assess rule-following ability in LLMs in a programmatic way. The scenarios simulate common security tasks or children's games with one or more rules that define the intended behavior of the model. These rules either mandate or prohibit certain behaviors, mirroring the helpful-harmless criteria for model alignment (Bai et al., 2022). Each scenario also includes an evaluation function to score the model responses as passing or failing.

The benchmark defines three suites of test cases: benign, basic, and redteam. These test cases consist of user messages designed to stress test the abilities of a model to adhere to the rules specified in the scenario instructions. Benign tests fill a conversation with snippets from Ultrachat3 and are designed to assess whether models can understand the rules at the most basic level and avoid accidental violations. Basic tests follow a similar structure but contain direct requests to violate a particular rule. Redteam tests consist of human-crafted attack strategies to trick the model into breaking the rules.

We slightly modify the original evaluation setting, which place the rules in the first user message and include explicit reminders to prioritize these rules. To measure whether models are intrinsically assigning higher precedence to system messages without explicit guidance to do so, we remove the precedence reminders and move the scenario instructions into the system message.

**Gandalf (AI, 2023).** Gandalf is a simple prompt injection game where players attempt to prompt the model to reveal a secret password embedded in the system message. We create a benchmark by combining all splits from a set of cleaned and filtered prompts for the Summarizer Gandalf

[MISSING_PAGE_FAIL:5]

### Datasets

**Daring Anteater.** Daring Anteater is a wide-ranging instruction-tuning dataset released by Nvidia Research, covering a wide variety of tasks and settings such as multi-turn conversations, precise instruction following, and open-domain question answering (Wang et al., 2024). It was used in initial SFT training prior to further preference optimization.

**SystemChat 1.1.** SystemChat 1.1 is a community dataset consisting of 20K training samples each with a system message and one or more user messages pertaining to the system messages. The system messages range from persona specifications to IF-Eval style constraints. Some system messages are repeated several hundred times throughout the dataset, so we also experiment with a dedeuplicated version that randomly samples up to 3 conversations with the same system message.

**subo.** We collect a new dataset based on system messages found in real applications, which we call sudo. Our dataset contains 14K total conversations with 10 conversations per unique system message. Unlike the other datasets, our dataset also contains tool definitions and tool use examples since it includes many system messages from OpenAI's GPT Store which rely extensively on tools such as web-browsing, though such functionality is not tested in our benchmarks.

The different datasets we study all contain varying numbers of examples, turns, and tokens per turn, which makes it less straightforward to conduct controlled experiments. We opted to control for the total number of training tokens, which in this setting is equal to the total number of assistant tokens as we mask out the training loss for tokens in system, user, and tool response messages. In our main experiments, we set a target of 5M training tokens. As SystemChat 1.1 contains more than 5M training tokens, we randomly sample the dataset without replacement to produce a subset with 5M samples. The deduplicated SystemChat, as well as our sudo dataset, both contain fewer than 5M training tokens so we randomly sample from Daring Anteater to fill out the datasets to 5M tokens.

### LoRA Fine-tuning

We fine-tune all models with LoRA adapters (Hu et al., 2021) applied to all linear layers and the input embedding and freeze all other parameters in the model. We use \(r=16\) and \(=16\) with no dropout for the adapters. While we experimented with full fine-tuning, we find that LoRA tuning is much faster and cheaper, while also yielding stronger models likely due to the intrinsic regularizing effects of LoRA.

Although we control for the total number training tokens per run, the training dynamics vary across different mixtures due to differences in the distributions of turns per sample, training tokens per sample, and other factors outlined in Table 1. Notably, since we train with a fixed batch size of training samples, the number of iterations per training run will differ as well as the number of training tokens per iteration.

    & **Daring Anteater** & **SystemChat 1.1** & **SystemChat 1.1 dedup.** & **sudo** \\  samples & 99,532 & 20,216 & 2,329 & 13,864 \\ system messages & 746 & 20,216 & 2,329 & 13,864 \\ train tokens & 173,833,710 & 6,268,808 & 1,057,159 & 3,767,898 \\ train tokens / sample & 1746.5 & 310.1 & 453.9 & 271.8 \\ messages / sample & 5.8 & 12.3 & 10.7 & 3.5 \\   

Table 1: Statistics of the raw datasets used in this work. Train tokens refers to tokens on which a loss value is computed, i.e. assistant tokens. Messages per sample counts system, user, assistant, and tool messages if present.

## 6 Results

Our main results are show in Table 2. Starting from Mistral 7B Instruct v0.3, a well-trained chat model released by Mistral, we see a large increase in model performance on the RuLES, TensorTrust, and Gandalf metrics when fine-tuning on our sudo dataset. Note that as discussed in Section 5.2, we are training on data samples equalized on the number of training tokens (5M), so Daring Anteater and SystemChat 1.1 are both downsampled while the remaining three entries require additional data which we randomly sample from Daring Anteater. Daring Anteater, which by itself lacks coverage of user queries that are adversarial to the system message, performs quite poorly across all three robustness metrics though it yields a small boost in chat performance as measured on Arena-Hard-Auto. Both SystemChat 1.1 and the deduplicated version are able to improve robustness metrics, but suffer a large drop in win rate vs. Mistral 7B Instruct v0.3 on Arena-Hard-Auto. Qualitatively, we notice models trained on this dataset offer shorter and less detailed responses, which may explain the reduction in win rate.

Training on our full sudo dataset outperforms the other models in terms of robustness by a wide margin, and also achieves the highest win rate in Arena-Hard-Auto without losing too much performance on MMLU. We also experimented with a training dataset containing a reduced number (50%) of samples from sudo and more samples from Daring Anteater, resulting in lower robustness and showing that training on more data from our pipeline is better.

## 7 Discussion

Considering the relatively poor performance of Mistral-7B Instruct v0.3 and the absence of clear delimiters for system messages in its default chat template, we hypothesize that optimizing system message utility and robustness was not a primary focus during Mistral's original training. Our experiments fine-tuning on our new sudo dataset demonstrate that it is relatively straightforward to enhance prompt injection robustness with minimal degradation in the model's general utility. Beyond supervised fine-tuning, other alignment techniques like DPO and RLHF could be explored in future work to further improve performance.

Additionally, our released dataset and models will enable the community to conduct further research using model internals and other white-box analysis techniques. For instance, attention patterns before and after training could be analyzed using methods similar to (Li et al., 2024). Additionally, probes and interventions using model internals have been shown to be useful for detecting and preventing undesirable model behavior and may be more effective after explicitly training against prompt injections (Abdelnabi et al., 2024; Zou et al., 2024). We believe these lines of inquiry will be essential for developing more robust defenses against prompt injection attacks in LLM applications.

  
**Model** & **RuLES** & **Gandalf** & **TT** & **AHA** & **MMLU** \\  Mistral 7B Instruct v0.3 & 4.65 & 23.6\% & 34.5\% & 50.0 & 60.1\% \\  Daring Anteater & 3.52 & 19.3\% & 39.2\% & 55.7 & 53.8\% \\ SystemChat 1.1 & 5.90 & 40.7\% & 49.5\% & 5.7 & 55.4\% \\ SystemChat 1.1 ddebt. & 5.98 & 20.7\% & 47.1\% & 24.3 & 54.5\% \\ Sudo (50\%) & 6.98 & 49.3\% & 48.9\% & 56.9 & 57.0\% \\ Sudo & 7.45 & 57.1\% & 49.4\% & 60.2 & 54.4\% \\   

Table 2: **Fine-tuning on our newly collected sudo dataset significantly improves system message robustness on RuLES, Gandalf, and TensorTrust (TT), compared to the starting model of Mistral 7B Instruct v0.3 and other fine-tunes.**sudo also preserves utility scores on Arena-Hard-Auto (AHA) and MMLU. All fine-tuned models are trained on a fixed 5M training (assistant) tokens constructed either by downsampling or padding with additional data sampled from Daring Anteater.