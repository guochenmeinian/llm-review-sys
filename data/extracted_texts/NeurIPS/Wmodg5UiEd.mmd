# Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Learning from human feedback plays an important role in aligning generative models, such as large language models (LLM). However, the effectiveness of this approach can be influenced by adversaries, who may intentionally provide misleading preferences to manipulate the output in an undesirable or harmful direction. To tackle this challenge, we study a specific model within this problem domain-contextual dueling bandits with adversarial feedback, where the true preference label can be flipped by an adversary. We propose an algorithm namely robust contextual dueling bandits (RCDB), which is based on uncertainty-weighted maximum likelihood estimation. Our algorithm achieves an \((d+dC)\) regret bound, where \(T\) is the number of rounds, \(d\) is the dimension of the context, and \(0 C T\) is the total number of adversarial feedback. We also prove a lower bound to show that our regret bound is nearly optimal, both in scenarios with and without (\(C=0\)) adversarial feedback. Additionally, we conduct experiments to evaluate our proposed algorithm against various types of adversarial feedback. Experimental results demonstrate its superiority over the state-of-the-art dueling bandit algorithms in the presence of adversarial feedback.

## 1 Introduction

Acquiring an appropriate reward proves challenging in numerous real-world applications, often necessitating intricate instrumentation (Zhu et al., 2020) and time-consuming calibration (Yu et al., 2020) to achieve satisfactory levels of sample efficiency. For instance, in training large language models (LLM) using reinforcement learning from human feedback (RLHF), the diverse values and perspectives of humans can lead to uncalibrated and noisy rewards (Ouyang et al., 2022). In contrast, preference-based data, which involves comparing or ranking various actions, is a more straightforward method for capturing human judgments and decisions. In this context, the dueling bandit model (Yue et al., 2012) provides a problem framework that focuses on optimal decision-making through pairwise comparisons, rather than relying on the absolute reward for each action.

However, human feedback may not always be reliable. In real-world applications, human feedback is particularly vulnerable to manipulation through preference label flip. Adversarial feedback can significantly increase the risk of misleading a large language model (LLM) into erroneously prioritizing harmful content, under the false belief that it reflects human preference. Despite the significant influence of adversarial feedback, there is limited existing research on the impact of adversarial feedback specifically within the context of dueling bandits. A notable exception is Agarwal et al. (2021), which studies dueling bandits when an adversary can flip some of the preference labels received by the learner. They proposed an algorithm that is agnostic to the amount of adversarial feedback introduced by the adversary. However, their setting has the following two limitations. First, their study was confined to a finite-armed setting, which renders their results less applicable to modern applications such as RLHF. Second, their adversarial feedback is defined on the whole comparison matrix. In each round, the adversary observes the outcomes of all pairwise comparisons and then decides to corrupt some of the pairs before the agent selects the actions. This assumptiondoes not align well with the real-world scenario, where the adversary often flips the preference label based on the information of the selected actions.

In this paper, to address the above challenge, we aim to develop contextual dueling bandit algorithms that are robust to adversarial feedback. This enables us to effectively tackle problems involving a large number of actions while also taking advantage of contextual information. We specifically consider a scenario where the adversary knows the selected action pair and the true preference of their comparison. In this setting, the adversary's only decision is whether to flip the preference label or not. We highlight our contributions as follows:

* We propose a new algorithm called robust contextual dueling bandits (RCDB), which integrates uncertainty-dependent weights into the Maximum Likelihood Estimator (MLE). Intuitively, our choice of weight is designed to induce a higher degree of skepticism about potentially "untrustworthy" feedback. The agent is encouraged to focus more on feedback that is more likely to be genuine, effectively diminishing the impact of any adversarial feedback.
* We analyze the regret of our algorithm under at most \(C\) number of adversarial feedback. Our result consists of two terms: a \(C\)-independent term \((d)\), which matches the lower bound established in Benggs et al. (2022) for uncorrupted linear contextual dueling bandits, and a \(C\)-dependent term \((dC)\). Furthermore, we establish a lower bound for dueling bandits with adversarial feedback, demonstrating the optimality of our adversarial term. Consequently, our algorithm for dueling bandits attains the optimal regret in both scenarios, with and without adversarial feedback.
* We conduct extensive experiments to validate the effectiveness of our algorithm RCDB. To comprehensively assess RCDB's robustness against adversarial feedback, we evaluate its performance under various types of adversarial feedback and compare the results with state-of-the-art dueling bandit algorithms. Experimental results demonstrate the superiority of our algorithm in the presence of adversarial feedback, which corroborate our theoretical analysis.

**Notation.** In this paper, we use plain letters such as \(x\) to denote scalars, lowercase bold letters such as \(\) to denote vectors and uppercase bold letters such as \(\) to denote matrices. For a vector \(\), \(\|\|_{2}\) denotes its \(_{2}\)-norm. The weighted \(_{2}\)-norm associated with a positive-definite matrix \(\) is defined as \(\|\|_{}=^{}}\). For two symmetric matrices \(\) and \(\), we use \(\) to denote \(-\) is positive semidefinite. We use \(\) to denote the indicator function and \(\) to denote the zero vector. For two actions \(a\), \(b\), we use \(a b\) to denote \(a\) is more preferable to \(b\). For a postive integer \(N\), we use \([N]\) to denote \(\{1,2,,N\}\). We use standard asymptotic notations including \(O(),(),()\), and \((),(),()\) will hide logarithmic factors.

## 2 Related Work

**Bandits with Adversarial Reward.** The multi-armed bandit problem, involving an agent making sequential decisions among multiple arms, has been studied with both stochastic rewards (Lai et al., 1985; Lai, 1987; Auer, 2002; Auer et al., 2002a; Kalyanakrishnan et al., 2012; Lattimore and Szepesvari, 2020; Agrawal and Goyal, 2012), and adversarial rewards (Auer et al., 2002b; Bubeck et al., 2012). Moreover, a line of works focuses on designing algorithms that can achieve near-optimal regret bounds for both stochastic bandits and adversarial bandits simultaneously (Bubeck and Slivkins, 2012; Seldin and Slivkins, 2014; Auer and Chiang, 2016; Seldin and Lugosi, 2017; Zimmert and Seldin, 2019; Lee et al., 2021), which is known as "the best of both worlds" guarantee. Distinct from

   Model & Algorithm & Setting & Regret \\   & Multi-layer Active Arm Elimination Race &  & (K^{1.5}C)\)} \\  & (Lykouris et al., 2018) & & \\  & BARBAR &  & (+KC)\)} \\  & (Gupta et al., 2019) & & \\  & SBE & & \\  & (Li et al., 2019) & & \\  & Robust Phase Elimination & & \\  & (Bogunovic et al., 2021) & & \\  & Robust weighted OFUL & & \\  & (Zhao et al., 2011) & & \\  & CW-OFUL & & \\  & (He et al., 2022) & & \\   & WWR &  & (K^{2}C/_{}+_{i i}K^{2}/_{i}^{2})\)} \\  & (Agarwal et al., 2021) & & \\   & Versatile-DB & & \\ (Saha and Gaillard, 2022) & & \\   & **RCDB** & & \\   & **(Our work)** & & \\    & **Notation.** In this paper, we use plain letters such as \(x\) to denote scalars, lowercase bold letters such as \(\) to denote vectors and uppercase bold letters such as \(\) to denote matrices. For a vector \(\), \(\|\|_{2}\) denotes its \(_{2}\)-norm. The weighted \(_{2}\)-norm associated with a positive-definite matrix \(\) is defined as \(\|\|_{}=^{}}\). For two symmetric matrices \(\) and \(\), we use \(\) to denote \(-\) is positive semidefinite. We use \(\) to denote the indicator function and \(\) to denote the zero vector. For two actions \(a\), \(b\), we use \(a b\) to denote \(a\) is more preferable to \(b\). For a postive integer \(N\), we use \([N]\) to denote \(\{1,2,,N\}\). We use standard asymptotic notations including \(O(),(),()\), and \((),(),()\) will hide logarithmic factors.

## 3 Related Work

**Bandits with Adversarial Reward.** The multi-armed bandit problem, involving an agent making sequential decisions among multiple arms, has been studied with both stochastic rewards (Lai et al., 1985; Lai, 1987; Auer, 2002; Auer et al., 2002a; Kalyanakrishnan et al., 2012; Lattimore and Szepesvari, 2020; Agrawal and Goyal, 2012), and adversarial rewards (Auer et al., 2002b; Bubeck et al., 2012). Moreover, a line of works focuses on designing algorithms that can achieve near-optimal regret bounds for both stochastic bandits and adversarial bandits simultaneously (Bubeck and Slivkins, 2012; Seldin and Slivkins, 2014; Auer and Chiang, 2016; Seldin and Lugosi, 2017; Zimmert and Seldin, 2019; Lee et al., 2021), which is known as “the best of both worlds” guarantee. Distinct from

   Model & Algorithm & Setting & Regret \\   & Multi-layer Active Arm Elimination Race &  & (K^{1.5}C)\)} \\  & (Lykouris et al., 2018) & & \\  & BARBAR &  & (+KC)\)} \\  & (Gupta et al., 2019) & & \\  & SBE & & \\  & (Li et al., 2019) & & \\  & Robust Phase Elimination & & \\  & (Bogunovic et al., 2021) & & \\  & Robust weighted OFUL & & \\  & (Zhao et al., 2011) & & \\  & CW-OFUL & & \\  & (He et al., 2022) & & \\   & WWR &  & (K^{2}C/_{}+_{i i}K^{2}/_{i}^{2})\)} \\  & (Agarwal et al., 2021) & & \\   & Versatile-DB & & \\ (Saha and Gaillard, 2022) & & & \\   & **RCDB** & & \\   & **(Our work)** & & \\   

Table 1: Comparison of algorithms for robust bandits and dueling bandits.

fully stochastic and fully adversarial models, Lykouris et al. (2018) studied a setting, where only a portion of the rewards is subject to corruption. They proposed an algorithm with a regret dependent on the corruption level \(C\), defined as the cumulative sum of the corruption magnitudes in each round. Their result is \(C\) times worse than the regret without corruption. Gupta et al. (2019) improved the result by providing a regret guarantee comprising two terms, a corruption-independent term that matches the regret lower bound without corruption, and a corruption-dependent term that is linear in \(C\). In addition, Gupta et al. (2019) proved a lower bound demonstrating the optimality of the linear dependency on \(C\).

**Contextual Bandits with Corruption.** Li et al. (2019) studied stochastic linear bandits with corruption and presented an instance-dependent regret bound linearly dependent on the corruption level \(C\). Bogunovic et al. (2021) studied the same problem and proposed an algorithm with near-optimal regret in the non-corrupted case. Lee et al. (2021) studied this problem in a different setting, where the adversarial corruptions are generated through the inner product of a corrupted vector and the context vector. For linear contextual bandits, Bogunovic et al. (2021) proved that under an additional context diversity assumption, the regret of a simple greedy algorithm is nearly optimal with an additive corruption term. Zhao et al. (2021) and Ding et al. (2022) extended the OFUL algorithm (Abbasi-Yadkori et al., 2011) and proved a regret with a corruption term polynomially dependent on the total number of rounds \(T\). He et al. (2022) proposed an algorithm for known corruption level \(C\) to remove the polynomial dependency on \(T\) in the corruption term, which only has a linear dependency on \(C\). They also proved a lower bound showing the optimality of linear dependency on \(C\) for linear contextual bandits with a known corruption level. Additionally, He et al. (2022) extended the proposed algorithm to an unknown corruption level and provided a near-optimal performance guarantee that matches the lower bound. For more extensions, Kuroki et al. (2023) studied best-of-both-worlds algorithms for linear contextual bandits. Ye et al. (2023) proposed a corruption robust algorithm for nonlinear contextual bandits.

**Dueling Bandits and Logistic Bandits.** The dueling bandit model was first proposed in Yue et al. (2012). Compared with bandits, the agent will select two arms and receive the preference feedback between the two arms from the environment. For general preference, there may not exist the "best" arm that always wins in the pairwise comparison. Therefore, various alternative winners are considered, including Condorcet winner (Zoghi et al., 2014; Komiyama et al., 2015), Copeland winner (Zoghi et al., 2015; Wu and Liu, 2016; Komiyama et al., 2016), Borda winner (Jamieson et al., 2015; Falahatgar et al., 2017; Heckel et al., 2018; Saha et al., 2021; Wu et al., 2023) and von Neumann winner (Ramamohan et al., 2016; Dudik et al., 2015; Balsubramani et al., 2016), along with their corresponding performance metrics. To handle potentially large action space or context information, Saha (2021) studied a structured contextual dueling bandit setting. In this setting, each arm possesses an unknown intrinsic reward. The comparison is determined based on a logistic function of the relative rewards. In a similar setting, Benggs et al. (2022) studied contextual linear stochastic transitivity model with contextualized utilities. Di et al. (2023) proposed a layered algorithm with variance aware regret bound. Another line of works does not make the reward assumption. Instead, they assume the preference feedback can be represented by a function class. Saha and Krishnamurthy (2022) designed an algorithm that achieves the optimal regret for \(K\)-armed contextual dueling bandit problem. Sekhari et al. (2023) studied contextual dueling bandits in a more general setting and proposed an algorithm the provides guarantees for both regret and the number of queries. Another related area of research is the logistic bandits, where the agent selects one arm in each round and receives a Bernoulli reward. Faury et al. (2020) studied the dependency with respect to the degree of non-linearity of the logistic function \(\). They proposed an algorithm with no dependency in \(\). Abeille et al. (2021) further improved the dependency on \(\) and proved a problem dependent lower bound. Faury et al. (2022) proposed a computationally efficient algorithm with regret performance still matching the lower-bound proved in Abeille et al. (2021).

**Dueling Bandits with Adversarial Feedback.** A line of work has focused on dueling bandits with adversarial feedback or corruption. Gajane et al. (2015) studied a fully adversarial utility-based version of dueling bandits, which was proposed in Ailon et al. (2014). Saha et al. (2021) considered the Borda regret for adversarial dueling bandits without the assumption of utility. In a setting parallel to that in Lykouris et al. (2018); Gupta et al. (2019), Agarwal et al. (2021) studied \(K\)-armed dueling bandits in a scenario where an adversary has the capability to corrupt part of the feedback received by the learner. They designed an algorithm whose regret comprises two terms: one that is optimal in uncorrupted scenarios, and another that is linearly dependent on the total times of adversarial feedback \(C\). Later on, Saha and Gaillard (2022) achieved "best-of-both world" result for noncontextual dueling bandits and improved the adversarial term of Agarwal et al. (2021) in the same setting. For contextual dueling bandits, Wu et al. (2023) proposed an EXP3-type algorithm for the adversarial linear setting using Borda regret. For a comparison of the most related works for robust bandits and dueling bandits, please refer to Table 1. In this paper, we study the influence of adversarial feedback within contextual dueling bandits, particularly in a setting where only a minority of the feedback is adversarial. Compared to previous studies, most studies have focused on the multi-armed dueling bandit framework without integrating context information. The notable exception is Wu et al. (2023); however, this study does not provide guarantees regarding the dependency on the number of adversarial feedback instances.

## 3 Preliminaries

In this work, we study linear contextual dueling bandits with adversarial feedback. In each round \(t[T]\), the agent observes the context information \(x_{t}\) from a context set \(\) and the corresponding action set \(\). Utilizing this context information, the agent selects two actions, \(a_{t}\) and \(b_{t}\). Subsequently, the environment will generate a binary feedback (i.e., preference label) \(l_{t}=(a_{t} b_{t})\{0,1\}\) indicating the preferable action. We assume the existence of a reward function \(r^{*}(x,a)\) dependent on the context information \(x\) and action \(a\), and a monotonically increasing link function \(\) satisfying \((x)+(-x)=1\). The preference probability will be determined by the link function and the difference between the rewards of the selected arms, i.e.,

\[(a b|x)=r^{*}(x,a)-r^{*}(x,b).\] (3.1)

We assume that the reward function is linear with respect to some known feature map \((x,a)\). To be more specific, we make the following assumption:

**Assumption 3.1**.: Let \(:^{d}\) be a known feature map, with \(\|(x,a)\|_{2} 1\) for any \((x,a)\). We define the reward function \(r_{}\) parameterized by \(^{d}\), with \(r_{}(x,a)=,(x,a)\). Moreover, there exists \(^{*}\) satisfying \(r_{^{*}}=r^{*}\), with \(\|^{*}\|_{2} B\).

Similar assumptions have been made in the literature of dueling bandits (Saha, 2021; Bengs et al., 2022; Xiong et al., 2023). We also make an assumption on the derivative of the link function, which is common in the study of generalized linear models for bandits (Filippi et al., 2010).

**Assumption 3.2**.: The link function \(\) is differentiable. Furthermore, its first-order derivative satisfies:

\[()\]

for some constant \(>0\).

In our setting, however, the agent does not directly observe the true binary feedback. Instead, an adversary will see both the choice of the agent and the true feedback. Based on the information, the adversary can decide whether to corrupt the binary feedback or not.1 We represent the adversary's decision in round \(t\) by an adversarial indicator \(c_{t}\), which takes values from the set \(\{0,1\}\). If the adversary chooses not to corrupt the result, we have \(c_{t}=0\). Otherwise, we have \(c_{t}=1\), which means adversarial feedback in this round. As a result, the agent will observe a flipped preference label, i.e., the observation \(o_{t}=1-l_{t}\). We define \(C\) as the total level of adversarial feedback, i.e.,

\[_{t=1}^{T}c_{t} C.\]

**Remark 3.3**.: Adversarial corruption has been firstly studied in bandits (Lykouris et al., 2018), where in each round \(t\), the agent selects an action \(a_{t}\) and the environment generates a numerical reward \(r_{t}(a_{t})\). The adversary observes the reward and returns a corrupted reward \(_{t}\). The corruption level \(C\) is defined by \(_{t=1}^{T}|r_{t}(a_{t})-_{t}| C\). Compared with the continuous perturbation of rewards in bandits, the adversary's label flipping attack method in our model is quite different. The cost of obtaining adversarial feedback is uniformly 1, unlike in bandits where the cost depends on the intensity of the perturbation. Additionally, adversarial feedback in our setting involves comparing two arms, whereas in bandits it pertains to the reward of a single arm. The only previous work that studied label-flipping is (Agarwal et al., 2021), where the adversary cannot observe the action selected by the agent. In contrast, our setting focuses on scenarios where this information is available to adversaries, which is common in many real-life applications.

As the context is changing, the optimal action is different in each round, denoted by \(a_{t}^{*}=*{argmax}_{a}r^{*}(x_{t},a)\). The goal of our algorithm is to minimize the cumulative gap between the rewards of both selected actions and the optimal action

\[(T)=_{t=1}^{T}2r^{*}(x_{t},a_{t}^{*})-r^{*}(x_{t},a_{t})-r^{* }(x_{t},b_{t}).\] (3.2)This regret definition is the same as that in Saha (2021) and the average regret defined in Bengs et al. (2022). It is typically stronger than weak regret defined in Bengs et al. (2022), which only considers the reward gap of the better action.

## 4 Algorithm

In this section, we present our new algorithm RCDB, designed for learning contextual linear dueling bandits. The main algorithm is illustrated in Algorithm 1. At a high level, we incorporate uncertainty-dependent weighting into the Maximum Likelihood Estimator (MLE) to counter adversarial feedback. Specifically, in each round \(t[T]\), we construct the estimator of parameter \(\) by solving the following equation:

\[+_{i=1}^{t-1}w_{i}(_{i}^{ })-o_{i}_{i}=,\] (4.1)

where we denote \(_{i}=(x_{i},a_{i})-(x_{i},b_{i})\) for simplicity, \(w_{i}\) is the uncertainty weight we are going to choose. To obtain an intuitive understanding of our weight, we consider any action-observation sequence \((x_{1},a_{1},b_{1},o_{1},x_{2},a_{2},b_{2},o_{2},,x_{t},a_{t},b_{t},o_{t})\) up to round \(t\). For simplicity, we denote \(_{t}=(x_{1},a_{1},b_{1},o_{1},x_{2},a_{2},b_{2},o_{2}, ,x_{t},a_{t},b_{t})\) as the filtration. Suppose the estimated parameter \(_{t}\) is the solution to the unweighted version equation of (4.1), i.e.,

\[_{t}+_{i=1}^{t}(_{i}^{} _{t})-o_{i}_{i}=.\] (4.2)

When we receive \(_{t}=(x_{t},a_{t})-(x_{t},b_{t})\), the probability of receiving \(l_{t}=1\) can be estimated by \((_{t}^{}_{t})\). We consider the conditional variance of the estimated probability \((_{t}^{}_{t})\) in round \(t\), i.e.,\((_{t}^{}_{t})|_{t} \), involving a posterior estimate of the prediction's variance. First, we have

\[(_{t}^{}_{t})| _{t} (_{t}^{}^{ })+^{}(_{t}^{}^{})_{t}^ {}(_{t}-^{})|_{t}\] \[=_{t}^{}^{})-^{}(_{t}^{}^{})_{t }^{}^{}}_{_{t}-}|_{t} +^{}(_{t}^{}^{ })_{t}^{}_{t}|_{t}.\]

Moreover, using the Taylor's expansion to (4.2), we have

\[=_{t}+_{i=1}^{t}( _{i}^{}_{t})-o_{i}_{i}\] \[+_{i=1}^{t}^{ }(_{i}^{}^{})_{i}_{i}^{} _{t}+_{i=1}^{t}(_{i}^{}^{})-o_{i}_{i}-_{i=1}^{t}^{}(_{i}^{}^{})_{i}_{i}^{}^{}.\]

Let \(_{t}=+_{i=1}^{t}^{}(_{i}^{}^{})_{i}_{i}^{}\), we have

\[_{t} _{t}^{-1}_{i=1}^{t}^{}( _{i}^{}^{})_{i}_{i}^{} {}^{}-_{i=1}^{t}(_{i}^{}^{ })-o_{i}_{i}\] \[=_{t}^{-1}_{i=1}^{t}^{ }(_{i}^{}^{})_{i}_{i}^{ }^{}-_{i=1}^{t-1}(_{i}^{} {}^{})-o_{i}_{i}-(_{t}^{}^{})}_{_{t}-}+o_{t}_{t} ^{-1}_{t}\]

Therefore, the variance of the estimated preference probability can be approximated by

\[(_{t}^{}_{t})| _{t} =(_{t}^{}_{t })-(_{t}^{}_{t})|_{t }^{2}_{t}\] \[^{}( _{t}^{}^{})_{t}^{}_{t}^{-1 }_{t}|_{t}^{2}_{t}\] \[o_{t}[^{}(_{t}^{}^{})]^{2}\|_{t}\|^{2}_{_{t}^{-1}}|_{t }[^{}(_{t}^{}^{})]^{2}\| _{t}\|^{2}_{_{t}^{-1}},\]

where the first inequality holds due to the Jensen's inequality and \(o_{t}^{2}=o_{t}\), and the last inequality holds due to \([o_{t}|_{t}] 1\). Using \(^{}(_{t}^{}^{}) 1\), \(_{t}^{}^{} 1\), \(_{t}_{t+1}_{t}\), we can see that \((_{t}^{}_{t})|_{t} ^{-1}\|_{t}\|^{2}_{_{t}^{-1}}\). Since higher variance leads to larger uncertainty, which harms the credibility of the data, it is natural to assign a smaller weight to the data with high uncertainty. Thus, we choose the weight to cancel out the uncertainty as follows

\[w_{i}=\{1,/\|_{i}\|_{_{i}^{-1}}\},\] (4.3)

where \(/\|_{i}\|_{_{i}^{-1}}\) normalizes the variance of the estimated probability. To prevent excessively large weights, we apply truncation to this value. A similar weight has been used in He et al. (2022) for linear contextual bandits under corruption. Different from their setting where the weight is an estimate of the variance of the linear model, our weight is an estimate of a generalized linear model.

Furthermore, by selecting a proper threshold parameter, e.g., \(=/C\), the weighted MLE shares the same confidence radius with that of the no-adversary scenario.

After constructing the estimator \(_{t}\) from the weighted MLE, the sum of the estimated reward for each duel \((a,b)\) can be calculated as \((x_{t},a)+(x_{t},b)^{}_{t}\). To encourage the exploration of duel \((a,b)\) with high uncertainty during the learning process, we introduce an exploration bonus with the following \((x_{t},a)-(x_{t},b)_{_{t}^{-1 }}\), which follows a similar spirit to the bonus term in the context of linear bandit problems (Abbasi-Yadkori et al., 2011). However, the reward term and the bonus term exhibit different combinations of the feature maps \((x_{t},a)\) and \((x_{t},b)\), which is the key difference between bandits and dueling bandits. The selection of action pairs \((a,b)\) is subsequently determined by maximizing the estimated reward with the exploration bonus term, i.e.,

\[(x_{t},a)+(x_{t},b)^{}_{t}+ (x_{t},a)-(x_{t},b)_{_{t}^{- 1}}.\]

More discussion about the selection rule was discussed in Appendix A of Di et al. (2023).

```
1:Require:\(>0\), Regularization parameter \(\), confidence radius \(\).
2:for\(t=1,,T\)do
3: Compute \(_{t}=+_{i=1}^{t-1}w_{i}(x_{i}, a_{i})-(x_{i},b_{i})(x_{i},a_{i})-(x_{i},b_{i}) ^{}\).
4: Calculate the MLE \(_{t}\) by solving the following equation: \[+_{i=1}^{t-1}w_{i}(x_{i},a_{i})-(x_{i},b_{i})^{}-o_{i }(x_{i},a_{i})-(x_{i},b_{i})=.\] (4.4)
5: Observe the context vector \(x_{t}\).
6: Choose \(a_{t},b_{t}=*{argmax}_{a,b}(x_{t},a)+ (x_{t},b)^{}_{t}+(x_{t}, a)-(x_{t},b)_{_{t}^{-1}}}\).
7: The adversary sees the feedback \(l_{t}=(a_{t} b_{t})\) and decides the indicator \(c_{t}\). Observe \(o_{t}=l_{t}\) when \(c_{t}=0\), otherwise observe \(o_{t}=1-l_{t}\).
8: Set weight \(w_{t}\) as (4.3).
9:endfor ```

**Algorithm 1** Robust Contextual Dueling Bandit (RCDB)

## 5 Main Results

### Known Number of Adversarial Feedback

At the center of our algorithm design is the uncertainty-weighted MLE. When faced with adversarial feedback, the estimation error of the weighted MLE \(_{t}\) can be characterized by the following lemma.

**Lemma 5.1**.: If we set \(=B+ C+ /\), then with probability at least \(1-\), for any \(t[T]\), we have

\[_{t}-^{*}_{_{t}}.\]

**Remark 5.2**.: If we set \(=(+B)/C\), then the bonus radius \(\) has no direct dependency on the number of adversarial feedback \(C\). This observation plays a key role in proving the adversarial term in the regret without polynomial dependence on the total number of rounds \(T\).

With Lemma 5.1, we can present the following regret guarantee of our algorithm RCDB in the dueling bandit framework.

**Theorem 5.3**.: Under Assumption 3.1 and 3.2, let \(0<<1\), the total number of adversarial feedback be \(C\). If we set the bonus radius to be

\[=B+ C+ /,\]

then with probability at least \(1-\), the regret in the first \(t\) rounds can be upper bounded by

\[(T) 4 B+ C/\] \[+4d/+B/+4C/ (1+2T/)/\] \[+4d^{1.5}(1+2T/)/}/( ).\]

Moreover, if we set \(=(+B)/C\), \(=1/B^{2}\), the regret upper bound can be simplified to

\[(T)=d/+dC/.\]

**Remark 5.4**.: Our regret bound consists of two terms. The first one is a \(C\)-independent term \((d)\), which matches the lower bound \((d)\) proved in Bengos et al. (2022). This indicates that our result is optimal in scenarios without adversarial feedback (\(C=0\)). Additionally, our result includes an additive term that is linearly dependent on the number of adversarial feedback \(C\). When \(C=O()\), the order of regret will be the same as the stochastic setting. It indicates the robustness of our algorithm to adversarial feedback. Additionally, the following theorem we present establishes a lower bound for this adversarial term, indicating that our dependency on the number of adversarial feedback \(C\) and the context dimension \(d\) is also optimal.

**Theorem 5.5**.: For any dimension \(d\), there exists an instance of dueling bandits with \(||=d\), such that any algorithm with the knowledge of the number of adversarial feedback \(C\) must incur \((dC)\) regret with probability at least 1/2.

**Remark 5.6**.: The proof of Theorem 5.5 follows Bogunovic et al. (2021). In the constructed instances, only one action has reward 1, while others have 0. Compared with linear bandits, where the feedback is an exact reward, dueling bandits deal with the comparison between a pair of actions. A critical observation from our preference model, as formulated in (3.1), is that two actions with identical rewards result in a pair that is challenging to differentiate. The lower bound can be proved by corrupting every comparison into a random guess until the total times of adversarial feedback have been used up. For detailed proof, please refer to Section B.2. Our proved lower bound \((dC)\) shows that our result is nearly optimal because of the linear dependency on \(C,d\) and only logarithmic dependency on the total number of rounds \(T\).

### Unknown Number of Adversarial Feedback

In our previous analysis, the selection of parameters depends on having prior knowledge of the total number of adversarial feedback \(C\). In this subsection, we extend our previous result to address the challenge posed by an unknown number of adversarial feedback \(C\). Our approach to tackle this uncertainty follows He et al. (2022), we introduce an adversarial tolerance threshold \(\) for the adversary count. This threshold can be regarded as an optimistic estimator of the actual number of adversarial feedback \(C\). Under this situation, the subsequent theorem provides an upper bound for regret of Algorithm 1 in the case of an unknown number of adversarial feedback \(C\).

**Theorem 5.7**.: Under Assumptions 3.1 and 3.2, if we set the the confidence radius as

\[=B++(1+2T/)/ }/,\]

with the pre-defined adversarial tolerance threshold \(\) and \(=(+B)/\), then with probability at least \(1-\), the regret of Algorithm 1 can be upper bounded as following:

* If the actual number of adversarial feedback \(C\) is smaller than the adversarial tolerance threshold \(\), then we have \[(T)=d/+d/.\]
* If the actual number of adversarial feedback \(C\) is larger than the adversarial tolerance threshold \(\), then we have \((T)=O(T)\).

**Remark 5.8**.: The COBE framework (Wei et al., 2022) converts any algorithm with the known adversarial level to an algorithm in the unknown case. However, such a framework only works for weak adversaries and does not work in our strong adversary setting. In fact, He et al. (2022) proved that any algorithm cannot simultaneously achieve near-optimal regret when uncorrupted and maintain sublinear regret with corruption level \(C=()\). Therefore, there exists a trade-off between robust adversarial defense and near-optimal algorithmic performance. Our algorithm achieves the same nearly optimal \((d)\) regret as the no-adversary case even when \(C=()\), which indicates that our results are optimal in the presence of an unknown number of adversarial feedback.

## 6 Experiments

### Experiment Setup

Preference Model.We study the effect of adversarial feedback with the preference model determined by (3.1), where \((x)=1/(1+e^{-x})\). We randomly generate the underlying parameter in \([-0.5,0.5]^{d}\) and normalize it to be a vector with \(\|^{*}\|_{2}=2\). Then, we set it to be the underlying parameter and construct the reward utilized in the preference model as \(r^{*}(x,a)=^{*},(x,a)\). We set the action set \(=-1/,1/}^{d}\). For simplicity, we assume \((x,a)=a\). In our experiment, we set the dimension \(d=5\), with the size of action set \(||=2^{d}=32\).

Adversarial Attack Methods.We study the performance of our algorithm using different adversarial attack methods. We categorize the first two methods as "weak" primarily because the adversary in these scenarios does not utilize information about the agent's actions. In contrast, we classify the latter two methods as "strong" attacks. In these cases, the adversary leverages a broader scope of information, including knowledge of the actions selected by the agent and the true preference model. This enables it to devise more targeted adversarial methods.

* "Greedy Attack": The adversary will flip the preference label for the first \(C\) rounds. After that, it will not corrupt the result anymore.
* "Random Attack": In each round, the adversary will flip the preference label with the probability of \(0<p<1\), until the times of adversarial feedback reach \(C\).
* "Adversarial Attack": The adversary can have access to the true preference model. It will only flip the preference label when it aligns with the preference model, i.e., the probability for the preference model to make that decision is larger than 0.5, until the times of adversarial feedback reach \(C\).
* "Misleading Attack": The adversary selects a suboptimal action. It will make sure this arm is always the winner in the comparison until the times of adversarial feedback reach \(C\). In this way, it will mislead the agent to believe this action is the optimal one.

Experiment Setup.For each experiment instance, we simulate the interaction with the environment for \(T=2000\) rounds. In each round, the feedback for the action pair selected by the algorithm is generated according to the defined preference model. Subsequently, the adversary observes both the selected actions and their corresponding feedback and then engages in one of the previously described adversarial attack methods. We report the regret defined in (3.2) averaged across 10 random runs.

### Performance Comparison

We first introduce the algorithms studied in this section.

* MaxInP: Maximum Informative Pair by Saha (2021). It involves maintaining a standard MLE. With the estimated model, it then identifies a set of promising arms possible to beat the rest. The selection of arm pairs is then strategically designed to maximize the uncertainty in the difference between the two arms within this promising set, referred to as "maximum informative".
* CoLSTIM: The method by Benggs et al. (2022). It involves maintaining a standard MLE for the estimated model. Based on this model, the first arm is selected as the one with the highest estimated reward, implying it is the most likely to prevail over competitors. The second arm is selected to be the first arm's toughest competitor, with an added uncertainty bonus.
* MaxPairUCB: This algorithm was proposed in Di et al. (2023). It uses the regularized MLE to estimate the parameter \(^{*}\). Then it selects the actions based on a symmetric action selection rule, i.e. the actions with the largest estimated reward plus some uncertainty bonus.
* RCDB: Algorithm 1 proposed in this paper. The key difference from the other algorithms is the use of uncertainty weight in the calculation of MLE (4.4). The we use the same symmetric action selection rule as MaxPairUCB. Our experiment results show that the uncertainty weight is critical in the face of adversarial feedback.

Our results are demonstrated in Figure 1. In Figure 1(a) and Figure 1(b), we observe scenarios where the adversary is "weak" due to the lack of access to information regarding the selected actions and the underlying preference model. Notably, in these situations, our algorithm RCDB outperforms all other

Figure 1: Comparison of RCDB (Our Algorithm 1), MaxInp (Saha, 2021), CoLSTIM (Bengs et al., 2022) and MaxPairUCB (Di et al., 2023). We report the cumulative regret with various adversarial attack methods (Greedy, Random, Adversarial, Misleading). For the baselines, the parameters are carefully tuned to achieve better results with different attack methods. The total number of adversarial feedback is \(C=\).

baseline algorithms, demonstrating its robustness. Among the other algorithms, CoLSTIM performs as the strongest competitor.

In Figure 1(c), the adversary employs a'stronger' adversarial method. Due to the inherent randomness of the model, some labels may naturally be 'incorrect'. An adversary with knowledge of the selected actions and the preference model can strategically neglect these naturally incorrect labels and selectively flip the others. This method proves catastrophic for algorithms to learn the true model, as it results in the agent encountering only incorrect preference labels at the beginning. Our results indicate that this leads to significantly higher regret. However, it's noteworthy that our algorithm RCDB demonstrates considerable robustness.

In Figure 1(d), the adversary employs a strategy aimed at misleading algorithms into believing a suboptimal action is the best choice. The algorithm CoLSTIM appears to be the most susceptible to being cheated by this method. Despite the deployment of'strong' adversarial methods, as shown in both Figure 1(c) and Figure 1(d), our algorithm, RCDB, consistently demonstrates exceptional robustness against these attacks. A significant advantage of RCDB lies in that our parameter is selected solely based on the number of adversarial feedback \(\), irrespective of the nature of the adversarial methods employed. This contrasts with other algorithms where parameter tuning must be specifically adapted for each distinct adversarial method.

### Robustness to Different Numbers of Adversarial Feedback

In this section, we test the performance of algorithms with increasing times of adversarial feedback. Our results show a linear dependency on the number of adversarial feedback \(C\), which is consistent with the theoretical results we have proved in Theorem 5.3 and 5.5. In comparison to other algorithms, RCDB demonstrates superior robustness against adversarial feedback, as evidenced by its notably smaller regret.

## 7 Conclusion

In this paper, we focus on the contextual dueling bandit problem from adversarial feedback. We introduce a novel algorithm, RCDB, which utilizes an uncertainty-weighted Maximum Likelihood Estimator (MLE) approach. This algorithm not only achieves optimal theoretical results in scenarios with and without adversarial feedback but also demonstrates superior performance with synthetic data. For future direction, we aim to extend our uncertainty-weighted method to encompass more general settings involving preference-based data. A particularly promising future direction of our research lies in addressing adversarial feedback within the process of aligning large language models using Reinforcement Learning from Human Feedback (RLHF).

**Limitations.** We assume that the reward is linear with respect to some known feature maps. Although this setting is common in the literature, we observe that some recent works on dueling bandits can deal with nonlinear rewards (Li et al., 2024). Therefore, it's possible to extend our results to a more general setting. Another assumption concerns the lower bound of the derivative of the link function. Notably, in the logistic bandit model, which shares similarities with our setting through Bernoulli variables, some work (Abeille et al., 2021; Faury et al., 2022) can improve the dependency of \(\) from \(1/\) to \(\). A similar improvement might be achieved in our setting as well.

Figure 2: The relationship between cumulative regret and the number of adversarial feedback \(C\). For this specific experiment, we employ the “greedy attack” method to generate the adversarial feedback. \(C\) is selected from the set \(\) (10 adversarial levels).