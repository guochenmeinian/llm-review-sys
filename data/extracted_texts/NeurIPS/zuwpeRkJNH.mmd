# Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge Augmentation

Kun Yuan\({}^{1,2,3}\) **Vinkle Srivastav\({}^{1,2}\) **Nassir Navab\({}^{3}\) **Nicolas Padoy\({}^{1,2}\) \({}^{1}\)**University of Strasbourg, CNRS, INSERM, ICube, UMR7357, Strasbourg, France \({}^{2}\)IHU Strasbourg, Strasbourg, France \({}^{3}\)CAMP, Technische Universitat Munchen, Munich, Germany {kyuan,srivastav,npadoy}@unistra.fr nassir.navab@tum.de

###### Abstract

Surgical video-language pretraining (VLP) faces unique challenges due to the knowledge domain gap and the scarcity of multi-modal data. This study aims to bridge the gap by addressing issues regarding textual information loss in surgical lecture videos and the spatial-temporal challenges of surgical VLP. To tackle these issues, we propose a hierarchical _knowledge augmentation_ approach and a novel Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining (_PeskaVLP_) framework. The proposed knowledge augmentation approach uses large language models (LLM) to refine and enrich surgical concepts, thus providing comprehensive language supervision and reducing the risk of overfitting. The PeskaVLP framework combines language supervision with visual self-supervision, constructing hard negative samples and employing a Dynamic Time Warping (DTW) based loss function to effectively comprehend the cross-modal procedural alignment. Extensive experiments on multiple public surgical scene understanding and cross-modal retrieval datasets show that our proposed method significantly improves zero-shot transferring performance and offers a generalist visual representation for further advancements in surgical scene understanding. The source code will be available at https://github.com/CAMPA-public/PeskaVLP.

## 1 Introduction

The recent advancements in multi-modal representation learning, particularly with the introduction of CLIP , have led to the development of models capable of understanding a wide range of visual concepts using natural language supervision . The expressive natural language has allowed these models to shift from task-specific to more generalist applications . The learned representations of these models are robust, facilitating effective performance across diverse visual tasks without the need for task-specific fine-tuning . However, despite the impressive progress made by these models in the general computer vision domain, the effectiveness of these methods in domain-specific settings remains uncertain.

This concern is particularly relevant to the field of Surgical Data Science (SDS), an emerging interdisciplinary domain that utilizes deep learning and computer vision techniques to analyze surgical data . A key component of SDS is the analysis of intraoperative surgical videos captured through endoscopes or laparoscopes. Analyzing these videos presents several unique challenges compared to the general computer vision datasets. Unlike general computer vision datasets , surgical videos can last several hours and capture complex and fine-grained activities within a narrow field of view. This requires development of computational approaches to decompose and model the surgical procedures at multiple hierarchical levels, including the entireprocedure , phases [67; 16], steps [54; 31], atomic actions [6; 8], and action triplets [50; 62]. Moreover, surgical language involves specialized vocabulary, and annotating videos requires clinical expertise, limiting dataset scalability. Consequently, current deep learning applications are restricted to single-centric, fully-supervised, and task-specific approaches [3; 6; 31; 50; 55; 57; 67; 69; 74].

To bridge the gap, recent efforts have focused on creating surgical video-text pretraining datasets by curating surgical lecture videos from online e-learning platforms and pairing them with transcribed narrations using audio speech recognition (ASR) methods. Subsequently, a CLIP-style model  is trained contrastively to match the video clips to their corresponding textual descriptions. Building on this, the HECVL approach introduces hierarchical texts, including phase-level keystep descriptions and video-level summaries that provide hierarchical goals of the surgical procedure . However, challenges persist due to the smaller size of the surgical video-language pretraining dataset, noisy transcribed narrations, limited variability in phase-level descriptions, and strong temporal dependencies in surgical procedures, where actions and keysteps occur in a specific routine order. These issues hinder the accurate learning of multi-modal surgical representations.

To address these challenges, we propose **P**rocedure-**E**ncoded **S**urgical **K**nowledge-**A**ugmented **V**ideo-**L**anguage **P**retraining (PeskaVLP), which boosts data efficacy and tackles the spatial-temporal challenges inherent in surgical procedures from two perspectives. First, we introduce hierarchical knowledge augmentation to mitigate the problem of textual information loss in surgical video-language pretraining datasets. We argue that the internal knowledge of LLMs serves as a valuable surgical knowledge base, enriching and correcting text descriptions while preserving the original key concepts and meanings. Therefore, we utilize the large language model (LLM) prompted with different behaviors as an external knowledge base to correct, explain, or summarize the hierarchical texts in the surgical video-language pretraining dataset, thus providing diverse and better language supervision for multi-modal pretraining. Additionally, it reduces the risk of overfitting by preventing the text encoder from repeatedly encountering the same keystep texts in each epoch.

From the pretraining objective perspective, we perform the hierarchical video-language pretraining, as shown in Fig. 1, with a novel hierarchy-specific loss, \(LecNCE\). Specifically, we combine language supervision with visual self-supervision at the clip-level pretraining to introduce additional supervision signals within vision modality, making the pretraining efficient with a small surgical dataset . At phase- and video-level pretraining, we construct hard negative samples by reversing the order of texts, followed by a Dynamic Time Warping (DTW) based loss function to learn the

Figure 1: Illustration of video-language pretraining with hierarchical video-text pairs. At phase- and video-level, one parent-level text is paired to multiple child-level texts.

temporal alignment between video frames and texts, thus facilitating the understanding of cross-modal procedural alignment during pretraining.

We summarize our contributions as follows: First, we propose an LLM-based knowledge augmentation to handle surgery-specific textual information loss in the dataset, providing more densely interconnected natural language supervision from surgical lecture videos. Second, our proposed hierarchical video-language pretraining method enforces the understanding of the spatial-temporal characteristics of surgical lecture videos at different hierarchical levels. The pretrained PeskaVLP demonstrates state-of-the-art transferability and visual representation to different surgical scene understanding downstream datasets [67; 69; 31], across types of surgical procedures and clinical centers. It also shows strong multi-modal alignment ability through the cross-modal retrieval task at multiple hierarchical levels.

## 2 Related Works

**Surgical Video-Language Pretraining:** many works have demonstrated the effectiveness of learning visual representations from the natural language supervision of corresponding text [7; 70; 77; 40; 46; 42; 34]. These methods conduct contrastive learning  to match the video clips (or images) with their corresponding narrations (or captions). Similarly in the medical field, recent works have started to curate large-scale multi-modal data through hospital-sourced chest radiological reports [28; 12] and online platforms [76; 27; 26], e.g., YouTube and Twitter, to perform vision-language pretraining. However, these works encounter the sample efficiency issue when handling the smaller surgical video-language pretraining dataset (SVL) . Recent works improve the data efficacy and zero-shot performance of CLIP-style models [48; 37; 25]. However, they do not capture procedural dependency from the long-form surgical videos beyond the video clip and text matching. Hierarchical pretraining methods [4; 79; 75] propose to pair video clips of different durations to different hierarchical levels of texts, covering both short- and long-term understanding. Paprika  builds a procedural knowledge graph and elicits the knowledge node during the video-language pretraining process.

**Textual Augmentation with Knowledge Base:** the success of vision-language pretraining is highly dependent on the quality and quantity of available multi-modal data. Recent research  shows that a smaller high-quality dataset can outperform a larger low-quality dataset. Common practices improve the quality by textual augmentation, including EDA , masked token modeling , and captioning loss . Recent studies have used synthesized captions from captioning models to achieve notable improvements [33; 32; 58]. However, they show scalability deficiency and world knowledge loss in models trained with synthetic captions , which their initial benchmark success has largely obscured. To inject the knowledge, K-Lite  enriches the texts with WordNet  and Wiktionary  knowledge base. Merlot  learns script knowledge representations from millions of YouTube videos, however, a knowledge domain gap exists when applying this to the surgical field. The recent advent of large language models like GPT4  and Llama series  have been a game-changer, as they encode rich domain-specific knowledge, e.g., clinical knowledge , motivating LaCLIP  to augment textual inputs through the LLM rewrites.

## 3 Approach

### Dataset and Contrastive Learning

Learning joint video and language embedding space requires a large-scale video-language dataset, however, such datasets are expensive and time-consuming to create in the surgical field. Therefore, the first surgical video-language pretraining dataset, i.e., SVL , is proposed by obtaining around a thousand surgical lecture videos from surgical education platforms. SVL collects \(\)\(300\) hours of lecture videos accompanied by narration texts obtained using Audio Speech Recognition (ASR) methods, providing \(\)\(26\)k video clip-narration pairs for contrastive video-language pretraining. Specifically, short video clips \(x_{c}\) and their corresponding narration texts \(y_{n}\) are treated as positive pairs \(^{n}\), and the unpaired ones are treated as negative pairs \(^{n}\). Then, the contrastive training loss 

[MISSING_PAGE_EMPTY:4]

**Narration.** We ask the LLM to behave as a "recipe" to come up with a list of sequential steps that complete the given surgery. For each lecture video, we feed its title as input and obtain the list of pseudo steps, as shown in Fig. 2 (a), building a surgical step knowledge base. Then, we assign these pseudo steps to narration texts based on textual similarity. This implicitly corrects the typos in transcribed narrations and augments the textual input based on the LLM's surgical knowledge.

**Keystep.** As shown in Fig. 2 (b), we ask the LLM to behave like a "dictionary" to explain the meaning of the keystep. Specifically, the LLM assistant expands the given keystep into a description of the main surgical events, anatomies, and instruments involved. This enlarges the textual semantic information of each keystep and provides more expressive language supervision for pertaining.

**Abstract.** As shown in Fig. 2 (b), we ask the LLM to behave like a "summarizer" that captures the key concepts of the given abstract texts, e.g., surgical type, anatomies, and so on. This reduces the length of the textual inputs while maintaining the main concepts of the abstract paragraph. In the following experiment, we randomly input the original or augmented texts for video-language pretraining. Check Appendix H for examples of pre- and post-augmented texts.

### Procedure-aware Surgical Video-language Pretraining

We introduce PeskaVLP, a procedure-aware pretraining framework for the above surgical knowledge-augmented video-language dataset. We emphasize devising a pretraining objective \(LecNCE\) for the hierarchical video-text pairs. For clip-level pretraining, \(LecNCE_{clip}\) combines language supervision with visual self-supervision to improve data efficiency and boost the scene understanding on visually similar laparoscopic images. \(LecNCE_{phase/video}\) considers the procedure awareness during the coarser-level pretraining, through a DTW-based contrastive regularization objective with temporally reversed text sequences as negative samples. We apply the dual-encoder as our model architecture.

#### 3.3.1 Clip-level Pretraining

**Language Supervision.** The common pretraining objective for dual-encoder model is \(InfoNCE\), as denoted in Eq. 1, where matched video text pairs are treated as positive while

Figure 3: The pretraining pipeline of different hierarchies. We combine language supervision and visual self-supervision at clip-level pretraining. We conduct the procedure-aware contrastive learning at phase/video-level pretraining.

all other pairwise combinations in the batch are regarded as negative. In this work, we also apply \(InfoNCE\) to maximize the similarity between short-term video clips and their corresponding narration texts at the clip level, denoted as \(L_{clip}^{vl}\). However, this simple objective is data hungry and sensitive to the weakly aligned noisy video-text pairs from small-scale surgical video-language datasets, such as SVL .

**Visual Self-supervision.** The proposed PeskaVLP approach introduces an additional supervision signal from visual self-supervision to complement noisy language supervision. Specifically, we explore the widespread supervision within visual modality to learn generic visual representation. We adopt the simple yet effective SimSiam  strategy that aims to maximize the similarity between two augmented views. As shown in Fig. 3 (a), during the pretraining, we apply random distortion on the frames of video clips and generate two augmented embedding vectors for one video clip. We then apply \(InfoNCE\) to maximize the similarity of these two augmented embeddings by treating them as positive pairs, denoted as \(L_{clip}^{vv}\). This additional supervisory can learn visual features more efficiently and is robust to the distortion of surgical scene images. Finally, the \(LecNCE\) loss for clip-level pretraining is the sum of these two losses, denoted as \(LecNCE_{clip}=L_{clip}^{vl}+L_{clip}^{vv}\).

#### 3.3.2 Phase-/Video-level Pretraining

The surgical video-language pretraining presents a unique procedural challenge compared to the existing video-language methods [19; 47; 52; 71; 61]. The surgical actions and events occur in a certain order to follow the routine to complete the surgical phase and surgery, e.g., "hook dissecting cystic duct" should happen before "clipper cutting cystic duct" in the "clipping cutting" phase of cholecystectomy surgery. However, prior contrastive learning objectives [46; 52; 19] omit this temporal dependency and limit the understanding of procedural knowledge in surgical lecture videos.

Our proposed \(LecNCE\) training objective enables procedural understanding in phase- and video-level pretraining by considering the cross-modal temporal alignment between video frames and text sequence. Specifically, hierarchical texts can form the parent-child correspondence, i.e., abstract (parent-level) and keystep (child-level) texts, keystep (parent-level) and narration (child-level) texts. As shown in Fig. 3 (b), each parent-level text \(A\) is paired with a video segment \(V=\{v_{1},...v_{T}\}\), where the \(T\) is the number of frames of the video segment. \(A\) is also paired with a child-level text sequence \(B=\{b_{1},...b_{N}\}\), where \(N\) is the length of this sequence. Then, we build the cost matrix \(C R^{T N}\) between video frames and child-level text sequence based on their embeddings, with each element \(c_{i,j}\) computed by a distance function \(D\). We adopt the same distance function from :

\[c_{i,j}=(v_{i},b_{j})=-}_{i}^{} }_{j}/)}{_{k=1}^{N}(}_{i}^{ }}_{k}/)},}_{i}=f(v_{i})/\|f(v_{i })\|_{2}}_{i}=g(b_{i})/\|g(b_{i})\|_{2}\] (2)

Using this cost matrix \(C\), we apply Dynamic Time Warping (DTW) to find the minimum cross-modal cost path that aligns the video frames to the text sequence, denoted as \(DTW(C)\). We then make a reasonable assumption that the global semantics of the text sequence and its reversed version are distinct. Therefore, aligning the video frames to the text sequence should be easier, i.e., incur a lower alignment cost compared to aligning the same video frames when the text sequence is played in reverse. Following this assumption, we temporally reverse the child-level texts into \(=\{b_{n},...b_{1}\}\) and build the cost matrix \(\) between \(V\) and \(\), computing the minimum alignment cost \(DTW()\). We then devise a DTW-based contrastive regularization using hinge loss as follows:

\[L_{dtw}=max(DTW(C)-DTW()),)\] (3)

where \(\) is the margin between positive and negative samples. This imposed regularization can support fine-grained multi-modal representation learning from weakly paired video frames and texts via temporal alignment. Unlike Paprika , which relies on a pretrained model , our phase-/video-level pretraining provides a direct, lightweight, and more adaptable methodology to unseen surgical domains. We do not require the adaption from any existing models, improving the generalization capability. Also, our pretraining process is procedure-aware in itself rather than modifying the representation in a second step, streamlining the process and increasing efficiency. We also apply the \(InfoNCE\) loss to maximize the similarity between the paired parent-level text, video segment, and child-level texts, denoted as \(L_{infonce}\). Note that the \(L_{infonce}\) follows the same pipeline as in Fig. 1 (b) and (c). Finally, we achieve the loss \(LecNCE\) for phase- or video-levelpretraining as \(LeeNCE_{phase/video}=L_{infonce}+ L_{dtw}\), where \(\) is the hyper-parameter to scale two losses. Please refer to Appendix D for more details about dynamic time warping. Finally, we train the model in an alternating way, using the proposed hierarchical levels of learning objectives. We only train one set of visual and textual encoders for all three levels, ensuring the encoders are optimized for capturing both short-term and long-term semantics. We alternatively train with \(25\) batches of clip-level samples, followed by \(15\) and \(115\) batches of phase- and video-level samples.

## 4 Experiments

**Datasets.** Our pretraining is conducted on the videos of SVL  dataset. The pertaining dataset includes hierarchical textual annotations from the metadata of the videos . We evaluate our model on \(3\) publicly available surgical phase recognition downstream datasets, i.e., Cholec80  (cholecystectomy) from Strasbourg center, AutoLaparo  (hysterectomy) from HongKong hospital, MultiBypass140  (gastric bypass) from both Strasbourg (StrasBypass70) and Bern (BernBypass70) centers. These datasets contain untrimmed surgical workflows with frame-wise phase labels. We also evaluate pretrained model on the cross-modal retrieval task in multiple hierarchical levels with holdout videos in SVL-Retrieval . Check Appendix A for more details about pretraining dataset.

**Training Parameters.** We utilize the dual-encoder architecture with ResNet50  as visual encoder and ClinicalBert  as textual encoder, respectively. We train the model with a batch size of \(120/80/25\) for clip-/phase-/video-level, respectively. We sample \(4/16/64\) frames for videos of clip-/phase-/video-level. We use AdamW optimizer  with a learning rate of \(5e-5\). We train the model with \(4\) NVIDIA A\(100\) GPUs each having a DRAM of \(80\) GB for \(200\) epochs. Temperature parameter \(\) for distance function and \(\) for DTW-base contrastive loss function \(D\) are fixed as \(0.1\). Scale factor \(\) is set as \(0.01\).

**Evaluation Setup.** We evaluate pretrained models using two setups: Zero-Shot evaluation and Few/Full-shot Linear Probing evaluation. For Zero-Shot, we utilize class text prompts, the same as HecVL , to compute cosine similarities between image embedding and class text embeddings, classifying images based on the shortest distance. In Linear Probing, the pretrained visual encoder remains frozen when we extract features for each image, subsequently training a linear layer using the SGD optimizer. For few-shot linear probing, we train the linear layer with a few numbers of videos, referred to as \(k\)-\(\%\) training, where \(k\) indicates the percentage of all the videos used in training. Check Appendix B for more details.

### Zero-shot Surgical Phase Recognition

**High-quality Surgical Video-language Dataset.** As shown in Table 1, our approach achieves a significant performance improvement over the baselines MIL-NCE  and CLIP  pretrained on the natural computer vision datasets, even though our pretraining dataset is \(10,000\) times smaller

   Model & Dataset & Cholec80 & Autolaparo & StrasBypass70 & BernBypass70 & Average \\   & Cholec80 & 90.3 / \(-\) & \(-\) / \(-\) & \(-\) / \(-\) & \(-\) / \(-\) & \(-\) / \(-\) \\  & AutoLaparo & \(-\) / \(-\) & 82.0 / \(-\) & \(-\) / \(-\) & \(-\) / \(-\) & \(-\) / \(-\) \\   & BernBypass & \(-\) / \(-\) & \(-\) / \(-\) & 57.3 / 32.7 & 85.3 / 62.4 & \(-\) / \(-\) \\  & StrasBypass & \(-\) / \(-\) & \(-\) / \(-\) & 90.2 / 79.9 & 56.7 / 29.5 & \(-\) / \(-\) \\   MIL-NCE  & Howto100M & 7.8 / 7.3 & 9.9 / 7.9 & 5.6 / 3.1 & 2.4 / 2.1 & 6.4 / 5.1 \\   & CLIP400M & 30.8 / 13.1 & 17.4 / 9.1 & 16.9 / 5.5 & 14.8 / 4.1 & 19.9 / 8.0 \\  & Scratch & 29.4 / 10.4 & 15.3 / 10.9 & 6.3 / 3.5 & 4.9 / 2.3 & 14.0 / 6.8 \\  & SVL & 33.8 / 19.6 & 18.9 / 16.2 & 15.8 / 8.6 & 17.8 / 7.1 & 21.6 / 12.9 \\  SurgVLP  & SVL & 34.7 / 24.4 & 21.3 / 16.6 & 10.8 / 6.9 & 11.4 / 7.2 & 19.6 / 13.8 \\  HecVL  & SVL & 41.7 / 26.3 & 23.3 / 18.9 & 26.9 / 18.3 & 22.8 / 13.6 & 28.7 / 19.3 \\  PeskaVLP & SVL & **45.1 / 34.2** & **26.5 / 23.6** & **46.7 / 28.6** & **45.7 / 22.6** & **41.0 / 27.1** \\   

Table 1: Zero-shot phase recognition results. We report Accuracy / F1-Score. PeskaVLP outperforms the other methods across different tasks. We report the state-of-the-art methods that are fine-tuned on the downstream dataset in a fully-supervised manner. However, models fine-tuned on specific downstream datasets show limited generalizability across procedures and institutions.

than those. Note that when the CLIP model is randomly initialized and then trained with SVL, its performance declines compared to initializing from OpenAI. This shows that our surgical video-language pretraining dataset lacks the scale necessary to adequately pretrain a robust video-language model from scratch. ViT [13; 9] architectures are sensitive to initialization and excluded from this work. Further insights into the impact of initialization can be found in Appendix C.

**Transferability across Surgical Procedures and Centers.** Compared to the HecVL, our method achieves over 12.3% and 7.8% improvement in absolute accuracy and f1, thanks to our spatial-temporal \(LecNCE\) learning objective across multiple hierarchies. Also, the consistent boost on cholecystectomy , hysterectomy , and gastric bypass [2 ] procedures show the generalizable and transferable features of PeskaVLP. Comparing the results of StrasBypass and BernBypass, we find that PeskaVLP can recognize the phases of the same kind of surgery (gastric bypass), even if these surgeries are performed in different centers and follow different procedural routines. More qualitative results can be found in Appendix F.

### Zero-shot Cross-modal Retrieval

In our study, we evaluate pretrained models' cross-modal alignment efficacy by conducting both zero-shot text-to-image and image-to-text retrieval tasks in multiple hierarchical levels. We report the Recall@N metric by identifying the retrieved nearest neighbors for each query and then determining whether the corresponding ground truth element is within the top \(N\) nearest neighbors, where \(N\{1,5,10\}\). Table 2 shows that our PeskaVLP achieves superior performance due to the procedure-aware learning objective in hierarchical pretraining. Particularly, the hierarchical pretraining scheme significantly boosts the cross-modal retrieval at the coarse-grained video-text pairs, comprehending the relationship between long video segments and high-level sentences with surgical terms.

### Few-/Full-shot Linear Probing

**General Visual Representation for Surgical Scene Understanding.** We present the few- and full-shot linear-probing evaluation in Table 3. It shows that the learned visual representation from PeskaVLP provides a general visual representation for surgical scene understanding across surgical procedures. We also find that the MoCo v2 [55; 22] pretrained on the frames of the SVL dataset (second row of Table 3) in a visual self-supervised manner achieves better visual representation than pretraining on a public dataset that only contains one type of surgery, e.g., Cholec80 (third row in Table 3). This shows that the cross-procedure surgical pretraining dataset enables better generalizationability.

**Knowledge Augmentation and Hierarchical Pretraining.** Interestingly, the model pretrained contrastively with short video clips and narrations (SurgVLP) performs worse than MoCo v2 [55; 22] (second row in Table 3) in linear probing evaluation. This may be because the noisy narrations do not provide accurate natural language supervision for visual representation learning, thus highlighting the

   &  &  &  \\   & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\   &  \\  CLIP  & 2.9 & 5.2 & 6.7 & 1.7 & 3.2 & 6.3 & 1.2 & 11.7 & 25.8 \\ SurgVLP  & 2.8 & 11.8 & 16.1 & 1.6 & 6.8 & 11.6 & 1.3 & 8.2 & 15.5 \\ HecVL  & 2.7 & 11.3 & 17.2 & 3.9 & 13.7 & 21.3 & 28.2 & 74.1 & 82.3 \\ PeskaVLP & **3.2** & **13.2** & **23.3** & **6.1** & **21.0** & **35.4** & **38.8** & **75.3** & **85.9** \\   &  \\  CLIP  & 1.8 & 3.9 & 6.0 & 0.3 & 1.2 & 2.7 & 0 & 7.0 & 16.4 \\ SurgVLP  & 1.3 & 8.6 & 13.5 & 1.0 & 4.1 & 7.3 & 1.3 & 8.6 & 14.6 \\ HecVL  & 2.1 & 9.0 & 16.2 & 1.9 & 8.3 & 14.8 & 21.2 & 65.9 & 71.8 \\ PeskaVLP & **2.4** & **13.1** & **21.3** & **3.4** & **14.9** & **24.8** & **38.8** & **75.3** & **81.1** \\  

Table 2: We present cross-modal retrieval results on the holdout videos, highlighting the best performance in each setting in bold. We additionally include coarser-grained phase-keystep and abstract-video text pairs to assess long-term video and high-level textual understanding.

importance of visual self-supervision and textual quality. Our model surpasses the prior methods by a large margin, showing the efficacy of our hierarchical knowledge augmentation, which denoises the text and improves textual quality. Also, our proposed \(LecNCE\) promotes the visual encoder through additional visual self-supervision and procedural understanding. We present t-SNE visualizations of learned features in Appendix E, which shows that our multi-modal representations exhibit a smaller modality gap, enhancing transferability to vision-and-language downstream tasks [20; 39].

### Ablation Studies

**Effect of Knowledge Augmentation.** Table 4 presents the effect of our proposed LLM-based hierarchical knowledge-aware augmentation strategy, applied to the texts of SVL dataset. The first row of the table corresponds to HecVL  pretrained on SVL with only conventional visual augmentations, e.g., blurring and so on, without any knowledge augmentation. The results clearly demonstrate that simple visual augmentation strategies exhibit poor robustness as the texts of SVL are noisy and not diverse enough. Conversely, our knowledge-aware text augmentation consistently improves performance across multiple surgical datasets, highlighting the importance of the textual quality of

    &  &  &  \\ P/V & C & P & V & Cholec80 & Autolaparo & Cholec80 & Autolaparo \\  \(\) & \(\) & \(\) & \(\) & 41.7 / 26.3 & 23.3 / 18.9 & 56.1 / 40.3 & 46.9 / 32.1 \\ \(\) & ✓ & \(\) & \(\) & **45.5** / 31.0 & 25.3 / 20.0 & – / – & – / – \\ \(\) & \(\) & ✓ & ✓ & 42.4 / 28.1 & 24.9 / 20.4 & 58.1 / 43.2 & 48.5 / 34.7 \\ \(\) & ✓ & ✓ & ✓ & 43.4 / 30.3 & **28.3** / **24.5** & 60.4 / 48.6 & **53.8** / **39.2** \\ ✓ & ✓ & ✓ & \(\) & 44.0 / 31.8 & – / – & – / – & – / – \\ ✓ & ✓ & \(\) & ✓ & 43.7 / 30.6 & – / – & – / – & – / – \\ ✓ & ✓ & ✓ & ✓ & 45.1 / **34.2** & 26.5 / 23.6 & **61.9** / **50.6** & 53.1 / 36.8 \\   & & & & StrasBypass70 & BernBypass70 & StrasBypass70 & BernBypass70 \\  \(\) & \(\) & ✓ & ✓ & 26.9 / 18.3 & 22.8 / 13.6 & 60.2 / 46.8 & 59.3 / 31.2 \\ \(\) & \(\) & ✓ & ✓ & 32.3 / 21.2 & 23.8 / 17.5 & 62.6 / 47.7 & 60.3 / 32.3 \\ \(\) & ✓ & ✓ & ✓ & 39.8 / 23.7 & 25.7 / 21.3 & 63.5 / 48.6 & 62.2 / 32.0 \\ ✓ & ✓ & ✓ & ✓ & **45.1** / **34.2** & **26.5** / **23.6** & **63.8** / **50.4** & **62.9** / **32.7** \\   

Table 4: Ablation study on different modifications. Knowledge: knowledge augmentation applied to the pretraining dataset at phase-level (P) and video-level texts (V). P/V: procedure-aware pretraining learning objective at phase and video-level. C: the integration of language and visual self-supervision at clip-level pretraining. We report 10\(\%\)-shot linear probing in this table.

   Model & Dataset & k-\% & Cholec80 & Autolaparo & StrasBypass70 & BernBypass70 \\   &  & 100 & 66.4 / 54.9 & 57.5 / 44.9 & 66.2 / 53.6 & 64.7 / 31.6 \\  & & 10 & 57.4 / 42.3 & 44.9 / 30.4 & 53.3 / 42.1 & 53.3 / 25.6 \\   &  & 100 & 68.2 / 55.8 & 59.5 / 48.4 & **71.6** / 58.1 & 69.6 / 36.5 \\  & & 10 & 57.6 / 43.5 & 49.9 / 34.6 & 63.1 / 49.3 & 59.1 / 29.9 \\   &  & 100 & **73.4** / **62.8** & 51.3 / 37.4 & 67.8 / 55.4 & 66.0 / 33.1 \\  & & 10 & **69.6** / **56.9** & 45.4 / 31.7 & 58.1 / 45.2 & 52.7 / 25.7 \\   &  & 100 & 64.8 / 50.7 & 58.5 / 46.1 & 65.4 / 50.6 & 64.1 / 33.3 \\  & & 10 & 57.5 / 40.0 & 46.2 / 31.4 & 54.3 / 42.1 & 52.8 / 27.9 \\   &  & 100 & 64.9 / 55.0 & 53.1 / 42.1 & 69.1 / 55.7 & 68.2 / 35.2 \\  & & 10 & 58.9 / 42.3 & 45.3 / 35.3 & 58.2 / 45.2 & 56.5 / 29.8 \\   &  & 100 & 63.5 / 50.3 & 54.3 / 41.8 & 65.8 / 50.0 & 66.5 / 34.3 \\  & & 10 & 55.0 / 39.9 & 48.5 / 32.0 & 57.0 / 44.0 & 57.7 / 28.5 \\   &  & 100 & 66.0 / 53.2 & 56.9 / 44.2 & 69.8 / 54.9 & 70.0 / 34.4 \\  & & 10 & 56.1 / 40.3 & 46.9 / 32.1 & 60.2 / 46.8 & 59.3 / 31.2 \\   &  & 100 & 69.9 / 59.8 & **63.1** / **49.7** & 71.4 / **59.5** & **71.5** / **37.4** \\  & & 10 & 61.9 / 50.6 & **53.1** / **36.8** & **63.8** / **50.4** & **62.9** / **32.7** \\   

Table 3: Linear-probing evaluation results. V: supervision is from visual frames. L: supervision is from natural languages. VL: supervision is from both visual and language entities.

the surgical video-language pretraining dataset. We found that integrating visual self-supervision with language supervision significantly enhances performance in surgical scene understanding tasks across downstream datasets. Additionally, using a procedure-aware learning objective improves surgical phase recognition for routine procedures, such as cholecystectomy (Cholec80), more effectively than complex procedures, like hysterectomy (Autolaparo).

**Effect of Pretraining Objective.** Table 4 shows the impact of our learning objective for hierarchical surgical video-language pretraining. When we append visual self-supervision to language supervision at the clip-level pretraining, the zero-shot performance is clearly improved. This improvement can be attributed to the added diverse and high-quality supervision. Also, the boost at linear-probing evaluation shows that the combination of language supervision and visual self-supervision leads to a robust visual representation especially with a moderate size of surgical video-language dataset, e.g., SVL. Table 4 also highlights that the inclusion of \(LecNCE\) with procedure understanding consistently improves performance across most downstream datasets, leading to enhanced accuracy in both zero-shot and linear-probing. However, performance on the AutoLaparo degrades with this modification. This may be due to challenging or less routined surgical procedures in the pretraining dataset.

## 5 Conclusion, Limitations and Broader Impact

**Conclusion.** We have introduced a surgical video-language pretraining method for long-term surgical lecture videos and their hierarchical paired texts. Our proposed knowledge augmentation addresses the hierarchical textual information loss by integrating the large language model's internal surgical knowledge. Also, we propose a novel spatial-temporal pretraining objective for video-text pairs of different hierarchies, which addresses the lack of supervision signals problem in a small surgical vision-language dataset. The proposed \(LecNCE\) also addresses the procedural awareness problem, benefiting the long-term cross-modal understanding. The experiments show that our proposed PeskaVLP achieves the state-of-the-art generalized zero-shot ability and visual representation learning that can serve as a general initialization for many surgical scene understanding tasks.

**Limitations.** While our LLM-augmented strategy enhances textual information, it may overly standardize the text, raising concerns about overfitting during pretraining. Therefore, it is crucial to strike a balance between leveraging LLM capabilities and maintaining the variability present in real-world surgical narratives. To address this, future work will explore incorporating diverse audio inputs and spontaneous narratives into the pretraining process, ensuring that the model retains robustness and adaptability in real-world applications. Additionally, even though the SVL pretraining dataset covers diverse laparoscopic surgeries, it lacks surgeries in different organs, such as the brain and heart. To address this, we plan to expand the pretraining dataset using diverse media such as textbooks, instructional videos, and intraoperative video recordings from diverse sources. We also aim to diversify the pretraining dataset by considering laparoscopic, endoscopic, and microscopic surgeries on different organs, to further mitigate the risk of overfitting and enhance the model's generalizability.

**Broader Impact.** The primary goal of surgical data science is to develop novel context-aware support systems for the operating room by collecting large-scale surgical data and analyzing it with modern AI techniques, eventually improving the safety and efficacy of surgical outcomes. The recent advancements in vision-language-based multi-modal AI offer significant potential in achieving this goal by enabling the development of more robust and generalizable models. These multi-modal systems have the potential to support clinical decision-making, streamline surgical workflows, provide real-time intra-operative guidance to improve surgical precision, reduce errors, and optimize outcomes in the operating room. During the development, patient data privacy should be considered as a fundamental ethical requirement. These systems developed on real-world surgical data also hold transformative potential in medical education, enhancing training and skill development in both novice and experienced surgeons.