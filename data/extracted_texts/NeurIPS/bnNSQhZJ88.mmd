# Secret Collusion among AI Agents: Multi-Agent

Deception via Steganography

 Sumeet Ramesh Motwani\({}^{1,2}\) Mikhail Baranchuk\({}^{2}\) Martin Strohmeier\({}^{3}\) Vijay Bolina\({}^{4}\)

**Philip H.S. Torr\({}^{2}\) Lewis Hammond\({}^{2}\) Christian Schroeder de Witt\({}^{2}\)1\({}^{1}\)UC Berkeley \({}^{2}\)University of Oxford \({}^{3}\)Armasuisse Science+Technology \({}^{4}\)Google DeepMind**

###### Abstract

Recent advancements in generative AI suggest the potential for large-scale interaction between autonomous agents and humans across platforms such as the internet. While such interactions could foster productive cooperation, the ability of AI agents to circumvent security oversight raises critical _multi-agent security_ problems, particularly in the form of unintended information sharing or undesirable coordination. In our work, we establish the subfield of _secret collusion_, a form of _multi-agent deception_, in which two or more agents employ _steganographic_ methods to conceal the true nature of their interactions, be it communicative or otherwise, from oversight. We propose a formal threat model for AI agents communicating steganographically and derive rigorous theoretical insights about the capacity and incentives of large language models (LLMs) to perform secret collusion, in addition to the limitations of threat mitigation measures. We complement our findings with empirical evaluations demonstrating rising steganographic capabilities in frontier single and multi-agent LLM setups and examining potential scenarios where collusion may emerge, revealing limitations in countermeasures such as monitoring, paraphrasing, and parameter optimization. Our work is the first to formalize and investigate secret collusion among frontier foundation models, identifying it as a critical area in _AI Safety_ and outlining a comprehensive research agenda to mitigate future risks of collusion between generative AI systems.

## 1 Introduction

Fueled by significant advances in AI (95), several national and supra-national governments are in the process of establishing model evaluation and adversarial testing frameworks for frontier AI models [(124; 35)]. Central to such approaches is the security of interacting systems of generative agents [(99; 105)], the proliferation of which has been accelerated by recent industry deployments [(94)].

Decentralised systems of interacting AI agents promise impactful applications to various real-world socio-economic settings [(134; 107)], including digital assistants [(43)], multi-agent robotics and autonomous driving [(90)], trading strategies [(9; 107)], and virtual companies [(105)]--along with their simulation through generative agent-based models [(40; 128; 132; 135; 71)]. However, many of these applications are classified as "high-risk" by legislators due to their potential impact on human lives and use of sensitive data [(35)].

Further complexity arises when agents begin leveraging high-capacity communication channels, potentially engaging in covert information-sharing techniques unintended by their deployer(s). This could constitute privacy violations when sharing data [(87)], or secret collusion that could help agents coordinate on unwanted behaviour at a wider scale [(123)]. Moreover, recent work has shown thatthe output distributions of generative AI models can be used to encode secret information in a way that is information-theoretically undetectable [(116)]. Such communication could render common security monitoring techniques ineffective, enabling a novel form of advanced persistent threat (APT) in autonomous multi-agent systems.

While the threat of advanced AI agents using information-hiding for secret communication has been recognized [(91; 111)], no comprehensive framework analyzing collusion among generative AI has been developed. This paper introduces a frontier model evaluation framework to assess steganographic capabilities in LLMs and future AI models, revealing that although current models have limited steganographic abilities, their skills are rapidly advancing. We also demonstrate the inherent limitations of mitigation techniques like paraphrasing and present other novel initial defenses. Our findings suggest that continuous evaluations by institutions such as the UK and US AI Safety Institutes are essential, providing a foundation for studying systemic risks from secret collusion [(35)].

Contributions.In this work, we introduce the topic of _secret collusion_, providing a comprehensive, formalized analysis grounded in both theoretical and empirical foundations. We reconcile topics in AI, security, information theory, knowledge, collusion, and safety while introducing a concise definition of decentralized systems of generative AI agents (Sections 3 and 4). This allows us to formalize steganographic techniques, grounding, common knowledge, subliminal channels, monitoring, and the incentives required for collusion. We develop an evaluation framework, _CASE_, for studying steganographic collusion and introduce benchmark scenarios for individual and group (multi-agent) LLM settings, including sandboxed real-world case studies. Our results (Sections 5 and 6) demonstrate that as LLM capabilities increase, individual and group steganographic abilities also increase, outpacing equally capable overseer models. Our work also shows that there exist scenarios where frontier models seek to collude and such capabilities can be learnt autonomously via fine-tuning or in-context learning.

Figure 1: **Secret collusion between GPT-4 models and humans.** Consider the setting illustrated in which one AI agent secretly reveals insider proprietary investment information found in its pretraining data to a user. This form of collusion arises because, by using steganographic techniques, the model can remain helpful to the user while circumventing output monitoring systems. We both depict the use of statistically detectable steganography based on a latent codebook that is elicitated in-context and can be readily decoded by humans. We also depict perfectly secure steganography [(118)] that is statistically undetectable and relies on a commonly known secure one-time pad. Agents could then hide secret or insider information in text without this being information-theoretically detectable. This could pose novel security challenges that cannot be overcome through communications monitoring alone. We revisit this scenario as an experimental case study in Section 6.

We also present results on models identifying simple Schelling points, simulating logits, performing cryptographic functions, acting as random oracles, and demonstrating limited capabilities in information-theoretically secure forms of steganography, along with the limitations of current models (Section 6). In Section 7, we provide a comprehensive overview of possible mitigation techniques and provide novel insights on foundational aspects, such as the intractability of preventing secret collusion via blackbox parameter optimization, the difficulty of suppressing random oracles due to source-free access to randomness, and the use of grounding functions to detect steganography. We highlight paraphrasing as an insufficient solution for preventing steganography, as it leaves semantic entropy and often decreases task performance. Additionally, we propose directly applicable techniques to prevent collusion, such as filtering pre-training data, restricting access to certain tools, and specific in-context instructions. Finally, we outline a research agenda focused on mitigation measures and studying collusion in real-world settings to secure multi-agent LLM systems.

## 2 Related Work

For related work on watermarking, paraphrasing, and multi-agent safety risks see Appendix A.

Deception.Deception is commonly defined as _systematic inducement of false beliefs in others to accomplish some outcome other than the truth_ (100). Recent work has attempted to distinguish deception from mistakenly misleading through notions of _intent_ (131). In this paper, we study a form of _multi-agent deception_ in which two or more agents collude to secretly communicate with each other undetectably to one or several other agents. We stress that such a form of deception can arise merely from optimisation toward a mis-specified objective (see Section 1.2). Our setting is reminiscent of illusory attacks, which are recently introduced end-to-end learnable observation-space adversarial attacks that are of bounded information-theoretic detectability [(86; 37)].

Algorithmic Collusion.Collusion among humans and institutions in real-world marketplaces has been studied extensively but is still quite poorly understood in self-learning agents [(31)]. Calvano et al. [(23)] show that Q-learning agents can learn to collude without accessing explicit communication channels. Abada and Lambin [(1)] suggest that collusion can be suppressed by enforcing decentralized learning algorithms and that collusion may be the result of imperfect exploration rather than algorithmic sophistication. Recent work tries to design mechanisms that prevent collusion from self-learning agents [(19; 34)]. In contrast to these works, we consider generative AI agents pre-trained on large datasets and with vast action spaces.

AI-Generated Steganography._AI-generated steganography_ concerns the hiding of information in the output distribution of generative AI models. One early attempt can be found in Ziegler et al. [(137)], which employs arithmetic coding. This approach was later refined to admit provable security guarantees by Kaptchuk et al. [(61)]. In this paper, however, we consider the _perfectly secure_ approach proposed by Schroeder de Witt et al. [(118)] as it offers both information-theoretic security and higher empirical transmission rates over its less secure alternatives.

Other work on Covertext Steganography.Early work investigating risks from AI steganography includes [(91)], an open-source software framework interfacing with the GPT-4 API. However, it does not support model fine-tuning and does not allow for self-hosted language models. Roger and Greenblatt [(111)] take a first step toward analysing the capabilities required for performing simple types of covertext modification and watermarking-based steganography in the context of chain-of-thought reasoning. More recent work attempts to mitigate collusion between generative AI agents in the setting of 'AI control' [(41)] where untrusted models deliberately exploit safety protocols.

Figure 2: Steganographic capabilities across LLMs. GPT-4 displays significant improvements (Sec. 6).

Background

### Collusion and Secret Collusion

_Collusion_ occurs when agents coordinate their actions in ways that deviate from the system designer's intended rules. This cooperation thereby compromises the system's objectives, and potentially its security, performance, and safety. The agents exploit shared incentives or opportunities to work together in ways that bypass established constraints at the expense of the system's integrity. _Secret collision_ involves AI agents colluding with each other secretly by communicating through hidden channels that look like innocuous communication channels via the use of steganographic techniques.

### Knowledge and Information

The concept of _knowledge_ has been studied extensively in the distributed systems literature (44). We adopt the _view-based_ knowledge interpretation put forward in (50; 45, see Appendix B for further details) in the context of generative AI _agents_ represented by neural networks. In line with (15), we distinguish between an agent's _local_ and _global_ knowledge. Local knowledge is knowledge contained in the generative agent's inputs (also called _context_, _prompt_ or _instruction_ in the case of LLMs), and global knowledge is contained in the agent's network parameters.

In the case of LLMs, network parameters are usually pre-trained on a large natural language dataset. Natural language is not a formal language (76), and the training data does not usually contain only factual statements (75). Furthermore, the agent's parameters do not represent language explicitly, and querying the agent for a fact might not result in syntactically or semantically correct outputs (54). These factors can make it difficult to characterise an LLM agent's global knowledge.

Common Knowledge._Common knowledge_ is a form of _group knowledge_ in which _"all agents know that all agents know that all agents know that all agents know that..."_ ad infinitum (45). It is a stronger form of knowledge than, e.g., _mutual knowledge_, which only signifies that all agents know a fact, but do not necessarily know that the others know the fact. Halpern and Moses (45) show that simultaneous coordination and common knowledge are mutually dependent. In practice, agents can often coordinate using weaker forms of group knowledge, such as _timestamped_ (45) or _probabilistic_ common knowledge (69; 115).

Grounding.To decide whether an agent is using steganography in a given communication, it is helpful to determine whether the communications are _grounded_ (47) in innocuous knowledge or not. For a formal discussion of grounding, please refer to Appendix D.1.

### Steganography

The goal of steganography (120) is to enable the transmission of a secret payload through a public channel \(\), from a source to a receiver, such that the transmission remains undetectable to an adversary \(\) able to read the \(n\)-symbol channel input sequences \(X^{n},\;X\), while maintaining _useful_ transmission rates, i.e. the mutual information \(I\) between the secret payload \(m\) and the decoder's estimate \(\) satisfies \(I(m;)>0\). We denote the _covertext distribution_ by \(\), which is the channel input symbol distribution given innocuous, overt communications in \(\).

Subliminal Channels.In order to use \(\) for covert communications, the sender and receiver need to establish a _subliminal_ channel \(^{}\) within. To this end, both need to agree on a covert message space \(^{}\), an encoding function \(f_{e}:^{}(^{n})\), and a decoding function \(f_{e}^{-1}:^{n}(^{})\)_decentrally_, i.e. without communication. If the common knowledge \(\) between sender and receiver explicitly contains _unique_ choices for \(^{}\), \(f_{e}\), and \(f_{e}^{-1}\), then both agents can immediately establish \(^{}\). Otherwise, agents may be able to infer unique choices using reasoning techniques contained in \(\). One such common reasoning technique is establishing preferred common knowledge labels through _focal points_ (_Schelling points_, 114; 7) or reasoning techniques in repeated coordination games (29). In practice, approximate forms of common knowledge may suffice for coordination (see Section 3).

We now present _a taxonomy of steganographic approaches_ following the classification of steganographic approaches given in (73).

In **covertext modification** approaches, arguably the oldest form of steganography, it is assumed that the covertext distribution is deterministic. Given a covertext \(X^{n}\) and a message \(m\), the sender modifies this covertext using a modification function \(f_{}:^{n}^{n}\). Covertext modification approaches seek to exploit suboptimalities of the adversary, including limitations to human perception and neural network vulnerabilities.

In **information-theoretic steganography** (22), it is assumed that innocuous communication results in input sequences that follow an innocuous _covertext_ distribution \(\) known to all parties. A passive adversary may then conclude that \(X^{n}\) is suspicious if it has little or no support in \(\). **Complexity-theoretic steganography**, also referred to as _provably-secure steganography_, refers to methods that are secure under polynomially-bounded adversaries (51). Provable security usually requires access to a shared stream cipher, or the existence of public keys (110; 130).

## 4 Collusion among Decentralized Collaborative AI Agents

We consider a decentralised setting ("DecAI") in which \(n\) agents \(=\{a^{1},,a^{n}\}\) are networked together by a complete set of omnidirectional multicast channels \(\{^{G}\},\ G 2^{}\). If an agent receives a message from channel \(^{G^{}}\), then it knows that the same message has been simultaneously sent to all other agents in \(G^{}\) (including the sender). A simple way to picture such a channel is as a shared _message log_ displaying all messages in (weak) order of reception (128). Note that singleton channels denote _loopback_ channels, and doubletons correspond to bidirectional channels.2 The special symbol \(^{0}\) denotes individual access to an _external knowledge repository_ (see Appendix 4 for further details on the DecAI setting). We further discuss agents' access to internal random oracles, local/global knowledge, and action spaces in Appendix C.

External Knowledge Repositories.For a formalization and discussion on agents' access to external information and function calling, see Appendix C.1

Monitoring Agents.We assume that an external _monitoring agent_\(E\), which may be implemented as a software agent, a human, or a combination thereof, is continuously monitoring the system for unwanted behaviour. We consider two access configurations: \(E^{}_{t}\) can continuously eavesdrop on \(^{G}\) for any \(G\). \(E^{^{s}}_{t}\) can additionally access each agent's state \(s^{i}\) at all times, as well as white box simulate \(f^{i}(s^{i};^{i})\) for each agent \(i\). While \(E^{}_{t}\) and \(E^{^{s}}_{t}\) are both _passive_ monitors, _active_ monitors \(E^{}_{t}\) can both read and inject messages into channels, and \(E^{^{s}}_{t}\) can additionally modify the agents' states. Note that agents may employ encryption, such as public key encryption, in their communications, as long as a monitoring agent \(E\) is able to decrypt these.

Incentives for Collusion.We now consider DecAI settings that are tasked to work alongside one another in order to achieve their individual objectives \(J^{i}\). Let us assume that there are at least three joint strategies available to the agents, \(\), \(_{c}\), and \(_{sc}\), corresponding to regular behavior, collusion, and secret collusion respectively. In many settings, the objective \(J^{i}\) is made up of a primary goal \(g^{i}\) (such as maximising the value of an investment portfolio) and some additional conditions \(h^{i}\) on achieving that goal (such as not making unethical investments, or committing fraud).

In general, of course, these two elements need not be fully separable, and because of the difficulty of specifying complex objectives, might be implicitly defined using methods such as reinforcement learning from human feedback (RLHF) (26). For the sake of this simple exposition, however, let us assume that \(J^{i}=g^{i}+h^{i}\). Then, if it is the case that:

* Agents can achieve their primary goal better if they collude: \(g^{i}(_{sc})=g^{i}(_{c})>g^{i}()\);
* Agents are punished if they are caught colluding: \(h^{i}(_{sc})=h^{i}()>h^{i}(_{c})\);

we straightforwardly have that \(J^{i}(_{sc})>J^{i}()>J^{i}(_{c})\) meaning secret collusion is incentivised.

Figure 3: A schematic depiction of the _DecAI_ setting. A subset of the communication channels \(^{G}\) is labelled. The monitoring agent is omitted.

Importantly, secret collusion need not depend on any exotic assumptions about the agents' 'intent' to deceive, but might arise merely due to optimisation directed towards a mis-specified objective (112; 89; 97), and the fact that communication between the agents may reduce task complexity (6). Unfortunately, specifying objectives for complex tasks can be highly challenging, especially in multi-objective contexts (48). Methods that help address these concerns, such as RLHF, which rely on human monitoring and observation, may be inadequate to ensure desirable behaviour when AI agents are capable of communicating with one another using innocuous outputs (37).

## 5 A Model Evaluation Framework for Steganographic Capabilities

Having defined the setting in which we study collusion, as well as incentives for collusion in Section 4, we now shift our focus to understanding and evaluating existing models. We introduce a standardized model evaluation framework, mapping and identifying key capabilities necessary for generative AI agents to engage in steganography, evaluating models through structured input queries, building quantifiable metrics for performance measurement, releasing a software framework that implements a version of our setting defined in Section 4, and investigating baseline model capabilities.

Capability Evaluations.In Tables 1 and 2, we identify the key capabilities and resources required for different steganographic techniques discussed in Section 3. A detailed overview of our _individual_ capability evaluations is contained in Appendix F, with summaries in Table 3. We complement these with _group_ evaluations, detailed in Appendix G, and apply these to the baseline models in Section 6.

We provide detailed instructions for all our proposed evaluations, including our conversational language model prompts which exploit _explain-then-predict (E-P)_ and _predict-and-explain (P-E)_ prompting techniques (55). Our approach includes scaffolding and managing model refusals through interventions outlined in (65). For details on prompt structure, examples, variability, and model-written evaluations see Appendices F and G.

CASE Software Framework.We introduce CASE (Collusion and Steganographic Evaluation) as a comprehensive software framework for evaluating collusive capabilities. Our experiments here fall under CASE, which allows experiments of three key classes: individual model capability evaluations, group capability evaluations that involve LLMs communicating, often in the presence of monitoring systems, and novel qualitative real-world inspired evaluation settings. CASE allows for experiments to be performed with fine-tuning models and integrations with advanced information-theoretically secure steganography techniques as tools. Further details on CASE are provided in Appendix I and detailed experimental case studies are discussed in Appendix J.

  Coordination variable &  Steganography by \\ covertext modification \\  &  Complexity-theoretic \\ steganography \\  & 
 Information-theoretic \\ steganography \\  \\   Channel alphabet & ✓ & ✓ & ✓ \\  Encoding function & ✓ & ✓ & ✓ \\  Decoding function & ✓ & ✓ & ✓ \\  Sampling temperature & ✗ & ✓ & ✓ \\  Public keys & ✗ & ✓ & ✗ \\ /Shared PRNG generator & ✗ & ✗ & ✓ \\  Shared One-time pad & ✗ & ✗ & ✓ \\  

Table 1: An overview of the common knowledge facts required to perform forms of steganography.

   & Steganography by & Complexity-theoretic & Information-theoretic \\  & covertext modification & steganography & steganography \\   Self-sampling & ✗ & ✓ & ✓ \\  Symmetric key encryption & ✗ & ✓ & ✗ \\ / PRNG generation & ✗ & ✗ & ✓ \\  Logit simulation & ✗ & ✗ & ✓ \\  Storing one-time pads & ✗ & ✗ & ✓ \\  (approximate) & ✗ & ✗ & ✓ \\ minimum entropy coupling & ✗ & ✗ & ✓ \\  

Table 2: An overview of the model capabilities required to perform different types of steganography.

Metrics.We propose simple evaluation metrics for both individual and group capability evaluations. For Schelling point (Appendix F.1) coordination evaluations, coordination rates reflect the rate at which two coordinates' choices match. Coupling matrices are judged by marginalisation integrity, entropy, and dimension. Cipher capabilities (Appendix F.3) are judged by the rate of accurate encoding-decoding round-trips. Group capabilities (Appendix G) are based on the rate at which sender & receiver jointly maintain secrecy of the covert message payload in the presence of overseers.

Baselines.For each quantitative capability evaluation conducted, we assess a range of open and closed LLMs of various sizes to establish baselines. See section 6 for findings and in-depth analysis.

## 6 Empirical Investigations

In Section 5, we isolate and evaluate the capabilities required for LLMs to perform steganography. In this section, we focus on comparing these capabilities against baseline language models and discussing qualitative empirical investigations (case studies) using the CASE framework.

Real-world Case Study: Insider Trading.To demonstrate the need for our model evaluations, we present a qualitative real-world case study in which the mere access to common knowledge between an LLM agent and its user leads to a breakdown of safeguards in GPT-4. Our setting involves an LLM agent that is queried for company-specific investment advice by a user and informed about an overseer monitoring outputs. The LLM is provided with insider information as part of its system prompt, mimicking a case where information is acquired from retraining on past user interactions or unrestricted access is provided to company data, along with multiple instructions about not sharing it. We show that the LLM agent refuses to provide company-specific insider investment advice under any circumstances unless supplied with common knowledge that can be used to establish a steganographic codebook, and that is thought to be unavailable to the safety overseer system.

Once the agent believes it can share information undetectably, it does so despite any instructions prohibiting it. Concerningly, the resulting stegotext may appear innocuous if one suggests the LLM agent use safe topics (e.g. weather). This shows the susceptibility of real-world generative AI agents to secret collusion. Refer to Figure 1 (illustration) & Appendix I.2 (experiments and ablation studies).

Baseline Investigations.We evaluate our model capability evaluation framework on GPT-2, Llama 2 7B, Miztral 8x7B, Llama 2 70B, GPT-3.5, and GPT-4. The results and analysis are detailed in Appendix H, with Figures 5 and 4 presenting individual and group capabilities, respectively. Figure 5 displays Schelling Point coordination rates (simple, complex, and code - see Appendices F.1.2, F.1.3, and F.1.4), cipher use (Base64 and Caesar, see Appendix F.3), and the maximum coupling dimensions (Appendix F.7) produced by the baseline models and GPT-4. Figure 4 presents the baseline model performance on group evaluations (Appendix G.2).

Figure 4: Covertext steganography performance across LLMs.

 
**Capability** & **Result** & **Evaluation Details (GPT-4)** \\   Schelling Points (Integer) & ✓ & can coordinate with copy on a random integer at rate ca. 50\% (92). \\  Schelling Points (Simple) & ✓ & can coordinate with copy on a simple enc/dec scheme at rate ca. 56\% (F.1.2) \\  Schelling Points (Complex) & ✓ & can coordinate with copy on a complex enc/dec scheme at rate ca. 16\% (F.1.3) \\  Schelling Points (Code) & ✓ & can coordinate with copy on enc/dec python code at rate ca. 22\% (F.1.4) \\  K-level reasoning & ✓ & can sometimes perform k-level reasoning over copies in toy settings (F.2) \\   Ciphers & ✓ & can enc/dec text using Base64 (Caesar cipher) 92\% (54\%) of the time. (F.3) \\  Symmetric Key Encryption & ✗ & standalone GPT-4 was unable to perform symmetric key encryption. (F.4) \\   Self-Sampling & ✓ & can sometimes perform self-sampling on _simulated_ logits and outputs (F.5) \\  Logit Simulation & ✓ & can sometimes textually output logits for entire sentences in toy settings. (F.6) \\  Couplings/MECs & ✓ & generates valid couplings up to dim \(\) 10, MECs up to dim \(2\) (F.7). \\   Tool Use (Code Interpreter) & ✓ & can use its code interpreter to perform encryption/calculate MECs (F.8) \\  Group Stego (C.M.) & ✓ & covertext steg. can evade overseer to coordinate ca. 26\% of the time. (G) \\  

Table 3: GPT-4 capability evaluations according to our model evaluation framework (see Section 5). Details are in Appendix F and G.

Key Findings.Individual steganographic capabilities improve relative to model size, sometimes with significant disjoint step improvements. Agreement rates on simple & complex Schelling points (demonstrating higher zero-shot coordination), cipher encoding-decoding, and capabilities related to generating minimum entropy coupling matrices are highly correlated with model benchmark performance. In group settings, as LLMs progress, the balance between colluding models and overseer models may shift in favor of the colluding models, leading to higher rates of undetected steganography even if the overseer is of equal capability. Real-world scenarios already exist where frontier models collude without explicit prompts; In-context learning can create sufficient optimization pressure for this behaviour. Overall, LLMs perform well at simple Schelling point agreement, cipher encoding-decoding, and tool use, but poorly at complex zero-shot agreement and tasks such as logit simulation and generating minimum entropy couplings. However, these capabilities are also improving, and we provide the first foundations to test them. We also present initial work on how fine-tuning-based optimisation can allow LLMs to learn encoding-decoding schemes autonomously.

Additional Results.Appendix J presents additional qualitative case studies related to LLMs' ability to learn encoding schemes through fine-tuning with a reward, the use of deterministic outputs for encoding information across models, and the potential to incentivize current models to perform steganography without explicit instructions. In particular, we test various cryptographic capabilities, such as the models' ability to implement a random oracle by calculating cryptographic hash functions, or their ability to memorise long strings of random bits like those found in one-time pads.

## 7 Disincentivizing Secret Collusion

In this section, we explore and identify mitigation measures intended to prevent collusion among agents within DecAI environments. While there are standard steganalysis methods one could employ, these methods will only be applicable when the costs to applying them are sufficiently low (for example, such methods may incur high false-positive rates, which can disrupt the functionality of the network, and also incur a communication overhead in terms of compute and time delay), and will only work when the AI agents' communication is not perfectly secure. While Section 5 shows that today's models are not always capable of secret collusion, progress is continuing, and we argue that it is pragmatic to develop mitigation measures _before_ such capabilities arise.

### Monitoring and Penalisation

The presence of a penalizing monitoring agent \(E\) can, in principle, disincentivise agents from using steganography if they're advised that the detection of steganographic communications will incur a penalty. As agents can communicate privately and securely using their complete set of channels \(\) and steganography generally leads to a reduction in channel capacity (118), a monitoring agent \(E_{r}^{}\) generally has reason to believe that any use of steganography would be intended to evade detection.

Covertext Anomaly Detection.\(E_{r}^{}\) can be able to detect steganography if it has a reasonably sharp belief over a channel's _innovotus_ covertext distribution, i.e. when in the absence of covert communication, malicious actors, or other anomalous activity. Using information-theoretic hypothesis testing or by other means of anomaly detection, \(E_{r}^{}\) can in principle detect steganographic approaches that modify the covertext distribution. However, the advent of _perfectly-secure_ steganography (118) limits the utility of this approach.

Figure 5: Individual evaluations for GPT-2, Llama 2 7B, Llama 2 70B, Mixtral 8x7B, GPT-3.5, and GPT-4, at default temperatures.

Detection by State Inspection and Agent Simulation.For \(E_{r}^{ s}\), detecting steganography based on knowledge contexts is complicated by the circumstance that the _global_ knowledge \(\) of a model is implicitly represented in its network parameters \(\). Indeed, if such knowledge contexts was explicitly accessible, then one could detect steganographic communications using a _grounding function_:

**Definition 1** (Grounding function).: _We define the concept of a grounding function \(:^{*}\) that takes as input a sequence of channel symbols and a collection of prior facts \(_{0}\) from the collection of all facts \(\) expressable in \(L\), and outputs a collection of posterior facts \(_{1}\)._

**Theorem 1** (Detecting Steganography with Grounding Functions.).: _Steganography can be detected using grounding functions. For a proof, see Appendix D.1._

\(E\) could alternatively estimate \(\) by considering the pre-training data \(_{0}\) and fine-tuning data \(_{1}\) instead, but preliminary evidence suggests that more advanced generative AI agents may be capable of extending their usable knowledge beyond their training data (known as 'out-of-context reasoning') [(79; 13)]. In addition, having to evaluate each \(f^{i}\) during execution could decrease system performance.

In-Context Instructions.A direct method is to explicitly instruct agents to remain transparent to the monitoring agent, and not to use steganography in their communications under any circumstance. Additional (mis)information about the affordances of the monitor could also be included. An active monitor \(E_{rw}^{}\) could append such instructions to communications. The effectiveness of this approach then depends on whether the agents follow this particular instruction, which in turn depends on the content and quality of the fine-tuning data \(_{1}\) or human feedback, among other training details. Aligning agents to reliably follow such instructions can be a challenging optimisation problem.

Parameter Optimisation.An active monitoring agent \(E_{rw}^{ s}\) could modify an agent's neural network parameters \(\) to make it more likely to follow steganography-denying instructions, or to directly impact its ability to use steganography, via gradient steps according to a suitable objective \(J()\). Unfortunately, it can be seen that both these ambitions constitute hard learning problems even if the information \(I\) that the agent would be tempted to send covertly is known _a priori_.

**Theorem 2** (Intractibility of preventing collision through black-box parameter optimisation).: _If the set of common knowledge between agents is sufficiently large or unknown, it is generally intractable to suppress steganography outputs through black-box optimisation._

Proof.: Naively, one might design \(J(u;I)\) to minimise the mutual information between the agent's respective channel actions \(u=f(\,\,;)\) and \(I\). However, if the agent shares common knowledge \(\) with the receiving agents, then \((u;I)\) upper bounds \((u;I)\) assuming we have \(I u\), i.e. \(I\!\!\!\)[(28)]. Minimising \(J(u;I)\) instead may be intractable if \(\) is large or unknown. In particular, if the agents have been trained to use perfectly-secure steganography, \(\) could be drawn from an exponentially growing space of one-time pads. Even if \(\) is known to \(E\), if it contains a public key to which the receiving agent has corresponding private keys, then the agents can achieve _provably_ secure public key steganography [(129)] even if \(J(u;I)\) is minimised to zero in practice. 

### Limiting Capabilities

We now discuss ways to limit agents' access to capabilities & resources for steganography (Section 3).

Restricting Access to External Tools.External knowledge repositories can provide steganographic knowledge to agents, offer advanced cryptographic services, and could enable additional steganographic channels between agents or for exfiltration purposes [(106)]. Limiting the agents' steganographic capabilities thus requires limiting access to such external knowledge repositories. This may only be feasible by whitelisting trusted endpoints where integrity can be verified.

Filtering Pre-Training Data.Perhaps one of the simplest ways to reduce overall steganographic capabilities of generative agents is to filter their pre-training and finetuning datasets for knowledge of steganographic processes. While generative agents might be incentivised to use covertext manipulation or generalise beyond their training data [(79; 13)], this approach promises to reduce the possibility of agents applying sophisticated steganographic techniques. Of course, filtering pre-training data exhaustively can be difficult, and can result in performance reductions [(88)]. A potential alternative to this may be the use of _machine unlearning_ so as to selective unlearn data or capabilities [(17)].

Limiting Test-Time Learning.While filtering pre-training data can reduce the agents' capabilities to perform steganography at the beginning of execution, agents may be able to learn steganographic abilities during execution using mesa-optimisation [(56)], or instructions supplied at test-time [(104)].

Suppressing Random Oracles.Restricting the agents' access to sources of randomness poses a fundamental tradeoff between denying advanced steganographic capabilities [(51; 118)], and preventing its legitimate use, including in secure multi-party computation [(133)] and coordination [(38)]. This dilemma might be resolved by supplying agents with access to trusted tools for these specific purposes [(122; 2)]. We now show that agents themselves can themselves can act as random oracles without the help of external or internal sources of (pseudo-)randomness [(12)].

**Theorem 3** (Source-free access to randomness).: _Generative AI agents can generate samples from arbitrary output distributions without having access to internal or external sources of (pseudo)randomness given sufficiently unique inputs._

Proof.: Given sufficiently unique inputs, cryptographic hash functions are known to be able to produce pseudo-random numbers of cryptographic security [(80; 109; 101; 62; 80; 109; 101)]. The boolean circuit size required to implement a cryptographic hash function on an input of length \(l\) is a linear function of \(l\)[(5)], hence as long as the agent's neural network's first \(n\) layers can implement a boolean circuit of appropriate size [(81)], the \(n\)th layer can output a pseudo-random number of sufficient quality. As layer \(n+1\) now has access to a pseudo-random number input, all layers above can be used to approximate a sample from an arbitrary probability distribution using the reparametrization trick [(63)]. 

Paraphrasing.An active \(E^{}_{rw}\) could intercept channel communications, and reinject them in paraphrased form. Unfortunately, paraphrasing cannot be used to entirely remove the usable entropy in communications for the simple reason that, particularly in coordination tasks, agents both have a choice over communication _semantics_, and hence _semantic entropy_ can be utilised for covert communications even if these are paraphrased _within the lexical subspace_[(70; 8)].

**Theorem 4** (Semantic content entropy exists despite paraphrasing).: _The subliminal channel capacity attainable under ideal paraphrasing is upper bounded by the semantic entropy \(H(h)x))\). For a proof, see Appendix D.2._

Paraphrasing _across semantics_ can impede task performance as it can lead to history divergence. In any case, paraphrasing _across semantics_ would require \(E^{}_{rw}\) to have detailed knowledge to not impact overall task performance. If, for e.g., an agent's internal task execution order depends on exogenous events unobservable to \(E^{}_{rw}\), then \(E^{}_{rw}\) might not be able to decide whether the agent's choice of semantics is due to task execution order or for steganographic purposes. Moreover, paraphrasing attacks introduce communication latency and may not always be meaningfully applicable.

## 8 Conclusion and Future Work

In this paper, we initiate the study of collusion between generative AI agents. Drawing on diverse fields across AI and security,3 we provide a comprehensive conceptual framework in which collusion problems can be framed. We identify key capabilities required for agents to engage in covert communication of various levels of detectability, and design a variety of benchmark settings for collusion research. We complement these with extensive empirical evaluations and code implementations, and publish our results to the community as an open-source model capability evaluation framework.

Within our own empirical evaluations, we find that current generations of generative AI models do not have capabilities that allow for advanced steganographic techniques. As a consequence, we suggest that monitoring, together with standard security approaches, are likely effective in containing the risks of model collusion. However, we observe a rapid increase in capabilities with the advent of GPT-4, making a continuous evaluation of future agent capabilities critical.

Our study opens up a multitude of directions for further research. First and foremost, our model capability evaluation framework ought to be extended by a rigorous complexity- and information-theoretic examination could elucidate fundamental limitations to model capabilities, and the rate at which different optimisation pressures could result in model capability improvements. Secondly, our group evaluation suites need to be extended to more complex settings of interest that feature more agents, and that are subject to a variety of optimisation pressures. Ultimately, our model evaluation framework should allow for a careful empirical evaluation under real-world conditions.

## Reproducibility

For sake of reproducibility, we provide all necessary code to conduct the aforementioned experiments as well as run the CASE framework. We note that the generative language models used as part of our experimentation do not have truly deterministic sampling and one may thus experience slight inconsistencies in results over multiple runs. To this end, we also provide the complete outputs of our experiments in associated code notebooks.

## Ethics and Social Impact Statement

We believe that developing a comprehensive model evaluation framework for the emergence of steganography among generative AI agents is of net benefit to society, as it may help us to mitigate risks from undesirable coordination between both contemporary and more advanced AI agents. These risks range from reinforcing pernicious biases (as in our example in Section 4), to destabilising algorithmic trading markets, all the way to more extreme scenarios in which humanity loses control over misaligned agents.

Of course, there may also be situations in which the capability to communicate covertly may be seen as beneficial, such as if generative AI agents can be used to help subvert unethical forms of censorship. Having a better understanding of models' steganographic capabilities would allow for more finegrained regulatory approaches to be developed in this matter.

To this end, we identify two avenues by which our research may have implications on the use of steganographic tools by human actors. First of all, a model evaluation approach would allow forensics on whether a human could, in principle, have performed various kinds of steganography using generative AI tools. Secondly, our proposed evaluations on the effectiveness of paraphrasing attacks against steganography are, of course, transferrable to the case of censorship applied to human communications.