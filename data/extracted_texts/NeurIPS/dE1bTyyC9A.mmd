# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

periments on three benchmarks, including ScanNet20, ScanRefer, and ScanNet200, demonstrate that the UniSeg3D consistently outperforms current SOTA methods, even those specialized for individual tasks. We hope UniSeg3D can serve as a solid unified baseline and inspire future work. Code and models are available at [https://dk-liang.github.io/UniSeg3D/](https://dk-liang.github.io/UniSeg3D/).

## 1 Introduction

3D scene understanding has been a foundational aspect of various real-world applications , including robotics, autonomous navigation, and mixed reality. Among 3D scene understanding tasks, 3D point cloud segmentation is a crucial component. Generic 3D point cloud segmentation contains panoptic, semantic, and instance segmentation tasks , which segment classes annotated in the training set. As a complement, 3D open-vocabulary (OV) segmentation task  segments open-vocabulary classes of interest. Another group of works study to utilize user priors. In particular, 3D interactive segmentation task  segments instances specified by users. 3D referring segmentation task  segments instances described by textual expressions. The above-mentioned tasks are core tasks in 3D scene understanding, drawing significant interest from researchers and achieving great success.

Previous studies  in the 3D scene understanding area focus on separated solutions specialized for specific tasks, as shown in Fig. 1(a). These approaches overlook intrinsic connections across different tasks, such as geometric and semantic consistency of objects. They also fail to share knowledge biased toward other tasks, limiting their understanding of 3D scenes to task-specific perspectives. It poses significant challenges for achieving comprehensive and in-depth 3D scene understanding. A recent exploration  named OneFormer3D designs an architecture to unify the 3D generic segmentation tasks, as shown in Fig. 1(b). This architecture inputs instance and semantic queries to simultaneously predict the 3D instance and semantic segmentation results. And the 3D panoptic segmentation is subsequently achieved by post-processing these predictions. It is simple yet effective. However, this architecture fails to support the 3D interactive, referring, and open-vocabulary segmentation tasks, which provide complementary scene information, including user priors and open-set classes, should be equally crucial in achieving 3D scene understanding as the generic segmentation tasks. This leads to a natural consideration that _if these 3D scene understanding tasks can be unified in a single framework?_

A direct solution is to integrate separated methods into a single architecture. However, it faces challenges balancing customized optimizations specialized for specific tasks involved in these methods. Therefore, we aim to design a simple and elegant framework without task-specific customized modules. This inspires us to design UniSeg3D, a unified framework processing six 3D segmentation tasks in parallel. Specifically, we use queries to unify representations of six tasks. The 3D generic and open-vocabulary segmentation tasks, only input point clouds without human knowledge, can be processed by sharing the same workflow without worrying about prior knowledge leakage. We use a unified set of queries to extract features of these four tasks for simplification. The interactive segmentation inputs visual point prompts to condition the segmentation. We represent the point prompt information by sampling point cloud features as vision prompt queries, thereby avoiding repeated point feature extraction. The referring segmentation inputs textual expressions persist in a modality gap with point clouds and are hard to unify in previous workflows. To reduce time consumption, we employ a parallel text prompt encoder to extract text features and regard them as text prompt queries. All these queries are decoded using the same mask decoder and share the same output head without the design of task-specific customized structures.

We further enhance performance by taking advantage of the multi-task design. In particular, we empirically find that the interactive segmentation outperforms the rest of the tasks in mask predictions, which is attributable to reliable vision priors. Hence, we design knowledge distillation to distill knowledge from the interactive segmentation to the other tasks. Then, we build contrastive learning between interactive segmentation and referring segmentation to connect these two tasks. The proposed knowledge distillation and contrastive learning promote knowledge sharing across six tasks, effectively establishing inter-task associations. There are three significant strengths of the UniSeg3D: (1) It unifies six 3D scene understanding tasks in a single framework, as shown in Fig. 1(c). (2) It is flexible because it can be easily extended to more tasks by simply feeding additional task-specificqueries. (3) The designed knowledge distillation and contrastive learning are only used in the training phase, optimizing performance with no extra inference cost.

We compare the proposed method with task-specific specialized SOTA approaches [50; 56; 36; 70; 45; 40] across six tasks to evaluate its performance. As shown in Fig. 1(d), the UniSeg3D demonstrates superior performance on all the tasks. It is worth noting that our performance on different tasks is achieved by a single model, which is more efficient than running separate task-specific approaches individually. Furthermore, the structure of UniSeg3D is simple and elegant, containing no task-customized modules, while consistently outperforming specialized SOTA solutions, demonstrating a desirable potential to be a solid unified baseline.

Our contributions can be summarized as follows: **First**, we propose a unified framework named UniSeg3D, offering a flexible and efficient solution for 3D scene understanding. It achieves six 3D segmentation tasks in one inference by a single model. To our knowledge, this is the first work to unify six 3D segmentation tasks. **Second**, specialized approaches limit their 3D scene understanding to task-specific perspectives. We facilitate inter-task knowledge sharing to promote comprehensive 3D scene understanding. Specifically, we take advantage of the multi-task unification design, employing knowledge distillation and contrastive learning to establish explicit inter-task associations.

## 2 Related Work

**3D segmentation.** The generic segmentation consists of panoptic, semantic, and instance segmentation. The panoptic segmentation [38; 60] is the union of instance segmentation [9; 33; 2; 64; 55] and semantic segmentation [44; 42; 5; 73]. It contains instance masks from the instance segmentation and stuff masks from the semantic segmentation. These 3D segmentation tasks rely on annotations, segmenting classes labeled in the training set. The open-vocabulary segmentation [40; 54] extends 3D segmentation to novel classes. Another group of works explores 3D segmentation conditioned by human knowledge. Specifically, the interactive segmentation [24; 70] segments instances specified by the point clicks. The referring segmentation [15; 45; 57] segments objects described by textual expressions. Most previous researches [65; 4; 25; 32] focus on specific 3D segmentation tasks, limiting their efficiency in multi-task scenarios, such as the domotics, that require multiple task-specific 3D segmentation approaches to be applied simultaneously. This work proposes a framework to achieve the six above-mentioned tasks in one inference.

**Unified vision models.** Unified research supports multiple tasks in a single model, facilitating efficiency and attracting extensive attention in the 2D area [43; 37; 29; 18]. However, rare works study the unified 3D segmentation architecture. It might be attributed to the higher dimension of the 3D data, which leads to big solution space, making it challenging for sufficient unification across multiple 3D tasks. Recent works [11; 35] explore outdoor unified 3D segmentation architectures, and some others [76; 14; 17] delve into unified 3D representations. So far, only one method, OneFormer3D , focuses on indoor unified 3D segmentation. It extends the motivation proposed in OneFormer  to the 3D area and proposes an architecture to achieve three 3D generic segmentation tasks in a single model. We note that the supported tasks in OneFormer3D can be achieved in one inference through post-processing predictions of a panoptic segmentation model. In contrast, we propose a simple framework that unifies six tasks, including not only generic segmentation but also interactive, referring, and open-vocabulary segmentation, into a single model. Additionally, we establish explicit associations between these unified tasks to promote knowledge sharing, contributing to effective multi-task unification.

## 3 Methodology

The framework of UniSeg3D is depicted in Fig. 2. It mainly consists of three modules: a point cloud backbone, prompt encoders, and a mask decoder. We illustrate their structures in the following.

### Point Cloud Backbone and Prompt Encoders

**Point cloud backbone.** We represent a set of \(N\) input points as \(^{N 6}\), where each point is characterized by three-dimensional coordinates \(x\), \(y\), \(z\) and three-channel colors \(r\), \(g\), \(b\). These input points are then fed into a sparse 3D U-Net, serving as the point cloud backbone, to obtain point-wisefeatures \(^{N d_{in}}\), where \(d_{in}\) denotes the feature dimension. Processing dense points individually in 3D scene understanding can be time-consuming. Therefore, we downsample the 3D scenario into \(M\) superpoints and pool the point features within each superpoint to form superpoint features \(_{s}=\{_{i}\}_{i=1}^{M}\), where each \(_{i}^{d_{in}}\) and \(_{s}^{M d_{in}}\). This procedure exhibits awareness of the edge textures  while reducing computation cost.

**Vision prompt encoder.** Click is a clear and convenient visual interaction condition widely employed in previous works [21; 24; 70]. We formulate clicks as vision prompts, as illustrated in Fig. 2. In practice, a click is first indicated by the spatially nearest point. Then, we sample a superpoint containing this point and employ its superpoint feature as a vision prompt feature \(_{v}^{d_{in}}\) to represent point prompt information, thus avoiding repeated feature extraction and maintaining feature consistency with the point clouds.

**Text prompt encoder.** UniSeg3D is able to segment instances described by textual expressions. The initial step of processing a text prompt involves tokenizing the text sentence to obtain its string tokens \(^{l c}\), where \(l\) is the sentence length, and \(c\) represents the token dimension. These tokens are then fed into a frozen CLIP  text encoder to produce a \(C\)-dimensional text feature \(_{t}^{C}\). This feature is subsequently projected into dimension of \(d_{in}\) using two linear layers, obtaining \(_{t}^{d_{in}}\), aligning the dimension of the point features for subsequent processing.

### Mask Generation

We employ a single mask decoder to output predictions of six 3D scene understanding tasks. The generic and open-vocabulary segmentation share the same input data, _i.e._, the point cloud without user knowledge. Therefore, we randomly select \(m\) features from \(M\) superpoint features to serve as unified queries \(_{u}^{}^{m d_{in}}\) for both the generic and open-vocabulary segmentation tasks. During training, we set \(m<M\) to reduce computational costs, while for inference, we set \(m=M\) to enable the segmentation of every region.

The prompt information is encoded into prompt features as discussed in Sec. 3.1. We employ the prompt features as prompt queries, which can be written as \(_{}^{}=\{_{v,i}\}_{i=1}^{K_{v}}\), \(_{}^{}=\{_{t,i}\}_{i=1}^{K_{t}}\), where \(_{}^{}^{K_{v} d_{in}}\), \(_{}^{}^{K_{t} d_{in}}\). \(K_{v}\) and \(K_{t}\) are the number of the point and text prompts, respectively. \(_{u}^{}\), \(_{}^{}\), \(_{}^{}\) are three types of queries containing information from various aspects. Feeding them forward indiscriminately would confuse the mask decoder for digging task-specific information. Thus, we add task-specific embeddings \(_{u}\), \(_{v}\), and \(_{t}\) before further processing:

\[_{u}=_{u}^{}+_{u},_{v}= _{v}^{}+_{v},_{t}=_{t}^{ }+_{t}, \]

Figure 2: The framework of UniSeg3D. This is a simple framework handling six tasks in parallel without any modules specialized for specific tasks. We take advantage of the multi-task unification design and enhance performance by building associations between the supported tasks. Specifically, knowledge distillation transfers insights from interactive segmentation to the other tasks, while contrastive learning establishes connections between interactive segmentation and referring segmentation.

where \(_{u}^{d_{in}}\), \(_{v}^{d_{in}}\), \(_{t}^{d_{in}}\), and are broadcasted into \(^{m d_{in}}\), \(^{K_{v} d_{in}}\), and \(^{K_{t} d_{in}}\), respectively. The mask decoder comprises \(L\) mask decoder layers, which contain self-attention layers integrating information among queries. Prompt priors involving human knowledge are unavailable for generic segmentation during inference. Therefore, in the training phase, we should prevent human knowledge from leaking to the generic segmentation. In practice, the prompt queries are exclusively fed into the cross-attention layers. Output queries of the last mask decoder layer are sent into an output head consisting of MLP layers to project dimensions of the output queries from \(d_{in}\) to \(d_{out}\). In general, the mask generation process can be formally defined as:

\[_{out}=((=(_{u},_{v},_{t});= _{s};=_{s})), \]

where \(_{out}=\{_{out,i}\}_{i=1}^{m+K_{v}+K_{t}}\) represents output features, with \(_{out,i}^{d_{out}}\) and \(_{out}^{(m+K_{v}+K_{t}) d_{out}}\).

Subsequently, we can process the output features to obtain class and mask predictions. For class predictions, a common practice involves replacing class names with class IDs . However, for our method to support referring segmentation, the class names are crucial information that should not be overlooked. Hence, we encode the class names into text features \(_{cls}^{K_{c} d_{out}}\) using a frozen CLIP text encoder and propose to regress the class name features instead, where \(K_{c}\) denotes the number of categories. Specifically, we formulate the mask predictions \(_{pred}\) and class predictions \(_{pred}\) as follows:

\[_{pred}=_{out}(_{s} )^{},_{pred}=(_{out }_{cls}^{}), \]

where \(_{pred}=\{_{i}\}_{i=1}^{m+K_{v}+K_{t}}\) and \(_{pred}=\{_{i}\}_{i=1}^{m+K_{v}+K_{t}}\), with \(_{pred}^{(m+K_{v}+K_{t}) M}\) and \(_{pred}^{(m+K_{v}+K_{t}) K_{c}}\). \(_{i}^{M}\) and \(_{i}^{K_{c}}\) represent the mask outcome and category probability predicted by the \(i\)-th query, respectively. The \(\) projects \(^{d_{in}}\) into \(^{d_{out}}\). Given that \(_{pred}\) and \(_{pred}\) are derived from superpoints, we map the segmentation outputs for each superpoint back to the input point cloud to generate point-wise mask and class predictions.

### Explicit Inter-task Association

Previous studies have overlooked connections among 3D scene understanding tasks, resulting in task-specific approaches that fail to leverage cross-task knowledge. This limitation restricts the understanding of 3D scenes to a task-specific perspective, hindering comprehensive 3D scene understanding. We establish explicit inter-task associations to overcome these constraints.

Specifically, on the one hand, as shown in Fig. 3(a), the referring segmentation is challenging when multiple individuals of identical shapes are arranged adjacently. It requires the method to distinguish the location variations inserted in the text prompts, such as "right of the other chair" vs. "another chair to the right of it." However, the modality gap between 3D points and linguistic texts sets significant obstructions. We propose ranking-based contrastive learning between the vision and text features to reduce the modality gap and optimize the referring segmentation.

Figure 3: Illustration of the inter-task association. (a) A challenging case requiring distinction of position information within textual descriptions. (b) A contrastive learning matrix for paired vision-text features, where a ranking rule is employed to suppress incorrect pairings. (c) Knowledge distillation across multiple tasks.

On the other hand, as shown in Tab. 1, we evaluate our baseline framework built in Sec. 3.1 and Sec. 3.2 on instance and interactive segmentation tasks. Essentially, the main difference between the instance and interactive segmentation is w/o or w/ vision prompts. The mIoU metric, which directly measures the quality of mask predictions, indicates that the interactive segmentation surpasses the instance segmentation by a notable margin of \(7.9\%\). It suggests that vision prompts provide reliable position priors, boosting the interactive segmentation to perform superior mask prediction performance.

We design a knowledge distillation to share insights from interactive segmentation across unified tasks. The core idea of this approach is to leverage the task of predicting best-quality masks to guide the other tasks, _i.e._, using a teacher to guide students.

#### 3.3.1 Ranking-based Contrastive Learning

We set the vision and text prompts specifying the same individual instances into pairs and align their pairwise features by employing contrastive learning.

Assuming \(B\) vision-text pairs within a training mini-batch, the corresponding vision and text output features are \(\{_{out,i}^{v}\}_{i=1}^{B}\) and \(\{_{out,i}^{t}\}_{i=1}^{B}_{out,i}^{v} ^{d_{out}}\) and \(_{out,i}^{t}^{d_{out}}\) are selected from output features \(\{_{out,i}\}_{i=m+1}^{m+K_{v}}\) and \(\{_{out,i}\}_{i=m+K_{v}+K_{t}}^{m+K_{v}}\), respectively. We normalize these selected features to obtain vision and text metric embeddings \(\{_{i}^{v}\}_{i=1}^{B}\) and \(\{_{i}^{t}\}_{i=1}^{B}\), where \(_{i}^{v}^{d_{out}}\) and \(_{i}^{t}^{d_{out}}\). Then, the contrastive learning can be formulated as \(_{con}=_{v}+_{t}\), with:

\[_{v}=-_{i=1}^{B}_{i}^{v}_{i}^{t}/)}{_{j=1}^{B}( _{i}^{v}_{j}^{t}/)},_{t}=- _{i=1}^{B}_{i}^{t}_ {i}^{v}/)}{_{j=1}^{B}(_{i}^{t} _{j}^{v}/)}, \]

where \(\) is a learnable temperature parameter. The pairwise similarity is illustrated in Fig. 3(b), where we denote \(_{i}^{v}_{j}^{t}\) as \(_{i,j}\) for simplification. To distinguish the target instances from adjacent ones with identical shapes, we introduce a ranking rule inspired by the CrowdCLIP  that the diagonal elements are greater than the off-diagonal elements, which can be described as:

\[_{rank}=_{i=1}^{B}_{j=1}^{B}(0,_{i,}-_{i,}). \]

#### 3.3.2 Knowledge Distillation

As shown in Fig. 3(c), we transfer knowledge from the interactive segmentation task to the generic and referring segmentation tasks to guide their training phases.

**To generic segmentation.** Define predictions decoded from the unified queries as \(Pred_{u}=\{_{i},_{i}\}_{i=1}^{m+K_{v}}\). We employ the Hungarian algorithm, utilizing the Dice and cross-entropy metrics as matching cost criteria, to assign \(Pred_{u}\) with interactive segmentation labels \(GT_{v}=\{_{gt,i},_{gt,i}\}_{i=1}^{K_{v}}\). The matched predictions are selected as positive samples \(Pos_{u}=\{_{pos,i},_{pos,i}\}_{i=1}^{K_{v}}\). We denote mask predictions among the positive samples as \(_{pos}=\{_{pos,i}\}_{i=1}^{K_{v}}\), with \(_{pos}^{K_{v} M}\). The predicted masks of interactive segmentation can be formulated as \(_{v}=\{_{i}\}_{i=m+1}^{m+K_{v}}\), where \(_{v}^{K_{v} M}\). We select the pixels with top \(k\%\) scores of \(_{v}\) as learning region \(\), and depict the knowledge transfer process from the interactive segmentation to the generic segmentation tasks as:

\[_{v g}=_{BCE}(_{pos} (),_{v}()), \]

where \(_{pos}()\) and \(_{v}()\) represent the predicted mask values within the region \(\), gathering from the positive samples and the interactive segmentation predictions, respectively.

**To referring segmentation.** Define pairwise class probabilities predicted by the vision and text prompt queries as \(_{v}^{B K_{c}}\) and \(_{t}^{B K_{c}}\) selected from \(\{_{i}\}_{i=m+1}^{m+K_{v}}\) and \(\{_{i}\}_{i=m+K_{v}+1}^{m+K_{v}}\), respectively. We formulate a knowledge transfer process from the interactive segmentation to the referring segmentation task as:

\[_{v t}=_{BCE}(( _{t}),(_{v})). \]

   Tasks & mIoU \\  Instance Seg. & 68.1 \\ Interactive Seg. & **76.0** (+7.9) \\   

Table 1: Mask prediction performance of instance and interactive segmentation.

### Training Objectives

**Open-set pseudo mask labels.** For open-vocabulary tasks, we train models on close-set data. To enhance segmentation performance on open-set data, we use SAM3D  to generate segmentation masks with undetermined categories as pseudo mask labels (open-set masks). While training, we assign predictions of the unified queries with ground-truth masks (close-set masks). The assigned and miss-assigned predictions are divided into positive and negative samples. The positive samples are supervised to regress the close-set masks. We match the negative samples with the pseudo mask labels and supervise the matched ones to regress the open-set masks. Note that the SAM3D is an unsupervised method and does not rely on ground-truth annotations, eliminating worries of label leakage. This process is exclusively applied in the training phase, incurring no extra inference cost.

**Loss function.** Training losses contain two components: (1) Basic losses, formulated as \(_{base}=_{mask}+_{cls}\). \(_{mask}\) stands for pixel-wise mask loss, comprising the BCE and Dice losses. \(_{cls}\) indicates the classification loss, where we use the cross-entropy loss. (2) Losses used to build inter-task associations, summarized as \(_{inter}=_{v g}+_{v t}+_{ con}+_{rank}\). The final loss function is \(=_{base}+_{inter}\), where \(\) is a balance weight set as \(0.1\).

## 4 Experiments

**Datasets.** We evaluate the UniSeg3D on three benchmarks: ScanNet20 , ScanNet200 , and ScanRefer . ScanNet20 provides RGB-D images and 3D point clouds of \(1,613\) scenes, including \(18\) instance categories and \(2\) semantic categories. ScanNet200 uses the same source data as ScanNet20, while it is more challenging for up to \(198\) instance categories and \(2\) semantic categories. ScanRefer contains \(51,583\) natural language expressions referring to \(11,046\) objects selected from \(800\) scenes.

**Experimental setups.** We train our method on the ScanNet20 training split, and referring texts are collected from the ScanRefer. \(d_{in}\) and \(d_{out}\) are set as \(32\) and \(256\), respectively. \(m\) ranges \(\) percent of \(M\) with an upper limit of \(3,500\). We set \(k\) as \(10\) and \(L\) as \(6\). For data augmentations, input point clouds are randomly rotated around the z-axis, elastic distorted, and scaled; the referring texts are augmented using public GPT tools following . We adopt the AdamW optimizer with the polynomial schedule, setting an initial learning rate as \(0.0001\) and the weight decay as \(0.05\). All models are trained for \(512\) epochs on a single NVIDIA RTX 4090 GPU and evaluated per \(16\) epochs on the validation set to find the best-performed model. To stimulate models' performance, we propose a two-stage fine-tuning trick, which fine-tunes the best-performed model, setting the learning rate and weight decay \(0.001\) times the initial values for \(40\) epochs. The proposed framework achieves end-to-end generic, interactive, and referring segmentation tasks. We divide the open-vocabulary segmentation task into mask prediction and class prediction. Specifically, we employ the proposed UniSeg3D to predict masks and then follow the Open3DIS  to generate class predictions.

We use PQ, mIoU, and mAP metrics to evaluate performance on the generic segmentation tasks following . Then, we use AP and mIoU metrics for the interactive and referring segmentation tasks, respectively, following . For the open-vocabulary segmentation task, we train our model on the ScanNet20 and evaluate it on the ScanNet200 using AP metric, following . The Overall metric represents the average performance across six tasks intended to reflect the model's unified capability.

### Comparison to SOTA Methods

The proposed method achieves six 3D scene understanding tasks in a single model. We demonstrate the effectiveness of our method by comparing it with SOTA approaches specialized for specific tasks. As shown in Tab. 2, the proposed method outperforms the specialized SOTA methods Panoptic-NDT , OctFormer , MAFT , AGILE3D , X-RefSeg3D , and Open3DIS  on the panoptic, semantic, instance, interactive, referring, and open-vocabulary (OV) segmentation tasks by \(12.1\) PQ, \(1.2\) mIoU, \(0.9\) mAP, \(1.0\) AP, \(4.1\) mIoU, \(0.7\) AP, respectively. Even when compared with the competitive 3D unified method, _i.e._, OneFormer3D , the proposed UniSeg3D achieves \(0.1\) PQ improvement on the panoptic segmentation task, and \(0.3\) mIoU improvement on the semantic segmentation task. More importantly, the OneFormer3D focuses on three generic segmentation tasks. It fails to understand user prompts, which limits its application prospects. In contrast, UniSeg3D unifies six tasks and presents desirable performance, demonstrating UniSeg3D a powerful architecture.

The proposed method achieves six tasks in one training, which is elegant while facing an issue for fair comparison. Specifically, partial labels in the referring segmentation benchmark (\(10,115\) objects, \(27.6\%\) of the complete ScanRefer training set) annotate novel classes of the open-vocabulary segmentation task. Obviously, these labels should not be used for training to avoid label leakage. Thus, we filter out these labels and only employ the filtered ScanRefer training set to train our model. As shown in Tab. 2, our model uses \(72.4\%\) training data to achieve closing performance with X-RefSeg3D  (\(29.6\) vs. \(29.9\)). Moreover, while reproducing the X-RefSeg3D using official code on our filtered training data, the performance drops to \(4.1\) mIoU lower than UniSeg3D, demonstrating our model's effectiveness.

### Analysis and Ablation

We conduct ablation studies and analyze key insights of our designs. All models are evaluated on multiple tasks to show the effectiveness of the proposed components on a broad scope.

    &  & ScanRefer & ScanNet200 \\   & Pan. & Sem. & Inst. & Inter. & Ref. & OV \\  Method & Reference & PQ & mIoU & mAP & AP & mIoU & AP \\  SceneGraphFusion  & CVPR 21 & 31.5 & - & - & - & - & - \\ TUPPer-Map  & IROS 21 & 50.2 & - & - & - & - & - \\ Panoptic Lifting  & CVPR 23 & 58.9 & - & - & - & - & - \\ PanopticNDT  & IROS 23 & 59.2 & - & - & - & - & - \\  PointNeXt-XL  & NeurIPS 22 & - & 71.5 & - & - & - & - \\ PointMetaBase-XXL  & CVPR 23 & - & 72.8 & - & - & - & - \\ MM-3DScene  & CVPR 23 & - & 72.8 & - & - & - & - \\ PointTransformerV2  & NeurIPS 22 & - & 75.4 & - & - & - & - \\ ADS  & ICCV 23 & - & 75.6 & - & - & - & - \\ OctFormer  & SIGGRAPH 23 & - & 75.7 & - & - & - & - \\  SoftGroup  & CVPR 22 & - & - & 45.8 & - & - & - \\ PBNet  & ICCV 23 & - & - & 54.3 & - & - & - \\ ISBNet  & CVPR 23 & - & - & 54.5 & - & - & - \\ SPFormer  & AAAI 23 & - & - & 56.3 & - & - & - \\ Mask3D  & ICRA 23 & - & - & 55.2 & - & - & - \\ MAFT  & ICCV 23 & - & - & 58.4 & - & - & - \\ QueryFormer  & ICCV 23 & - & - & 56.5 & - & - & - \\ OneFormer3D  & CVPR 24 & 71.2 & 76.6 & **59.3** & - & - & - \\  InterObject3D  & ICRA 23 & - & - & - & 20.9 & - & - \\ AGILE3D  & ICLR 24 & - & - & - & 53.5 & - & - \\  TGNN  & AAAI 21 & - & - & - & - & 24.9/27.8 & - \\ X-RefSeg3D  & AAAI 24 & - & - & - & - & 25.5/29.9 & - \\  OpenScene  with  & CVPR 23 & - & - & - & - & - & 8.5 \\ OpenMask3D  & NeurIPS 23 & - & - & - & - & - & 12.6 \\ SOLE  & CVPR 24 & - & - & - & - & - & 18.7 \\ Open3DIS  & CVPR 24 & - & - & - & - & - & 19.0 \\  UniSeg3D (**ours**) & - & **71.3** & 76.3 & 59.1 & 54.1 & 29.5/- & 19.6 \\ UniSeg3D\({}^{*}\) (**ours**) & - & **71.3** & **76.9** & **59.3** & **54.5** & **29.6**/- & **19.7** \\   

Table 2: Comparisons on ScanNet20 , ScanRefer , and ScanNet200 . The best results are highlighted in **bold**, and the second-best results are underscored. “\(*\)” indicates the use of the two-stage fine-tuning trick. “-/-” denotes training on filtered or complete ScanRefer datasets.

   ScanNet200 & ScanRefer &  \\  OV & Ref. & Inter. & Pan. & Sem. & Inst. \\  AP & mIoU & AP & PQ & mIoU & mAP \\  ✗ & ✗ & ✗ & **71.0** & 76.2 & **59.0** \\ ✗ & ✗ & 56.8 & **71.0** & **76.4** & 58.7 \\ ✗ & 29.1 & 56.0 & 70.3 & 76.3 & 58.4 \\
19.7 & 29.1 & 54.5 & 70.4 & 76.2 & 58.0 \\   

Table 3: Ablation on task unification.

**The challenge of multi-task unification.** We discuss the challenge of unifying multiple tasks in a single model. Specifically, we simply add interactive, referring, and open-vocabulary segmentation into our framework to build a unification baseline, as shown in Tab. 3. We observe a continuous performance decline on the panoptic, instance, and interactive segmentation tasks, indicating a significant obstacle in balancing different tasks. Even so, we believe that unifying multiple tasks within a single model is worth exploring, as it can reduce computation consumption and benefit real-world applications. Thus, this paper proposes to eliminate performance decline by delivering inter-task associations, and the following experiments demonstrate that this could be a valuable step.

**Design of inter-task associations.** Our approach uses knowledge distillation and contrastive learning to connect supported tasks. As shown in Tab. 4, when applying the knowledge distillation, _i.e._ row \(2\), the performance of instance and interactive segmentation increase to \(58.6\) mAP and \(55.3\) AP, respectively. We believe the improvement on the instance task is because of the reliable knowledge distilled from the interactive segmentation, and the improvement on the interactive segmentation task is attributed to the intrinsic connections between the two tasks. Then, we ablate the ranking-based contrastive learning, _i.e._ row \(3\). We observe improvements on five tasks, including the generic segmentation and the referring segmentation, while a slight performance drop on the interactive segmentation. This phenomenon suggests that contrastive learning is effective on most tasks, but there is a huge struggle to align point and text modalities, which weakens the interactive segmentation performance. Overall metric measures multi-task unification performance. We choose models and

Table 4: Ablation on components. “Distillation”, “Rank-Contrastive”, and “Trick” denote the knowledge distillation, ranking-based contrastive learning, and two-stage fine-tuning trick, respectively.

Table 5: Ablation on different designs of the proposed components. “\(v g\)” and “\(v t\)” denote the knowledge distillation from the interactive segmentation to the generic segmentation and the referring segmentation, respectively. “Contrastive” and “Rank” denote the contrastive learning and the ranking rule, respectively.

checkpoints with higher Overalls in our experiments. In practical applications, checkpoints can be chosen based on preferred tasks while maintaining good performance across other tasks. Applying knowledge distillation and ranking-based contrastive learning obtains comparable performance on most tasks, performing higher Overall than rows \(2\) and \(3\), indicating complementarity of the two components. We employ the two-stage fine-tuning trick, consistently improving various tasks.

Detailed ablation on the components is shown in Tab. 5. It is observed that knowledge distillation to various tasks brings respective improvements. As for contrastive learning, comparing row \(2\) and row \(4\) in Tab. 5(b), the ranking rule suppresses confusing point-text pairs, boosting contrastive learning to be more effective. \(\) controls the strength of the explicit inter-task associations. We empirically find that setting \(\) to \(0.1\) obtains the best performance, as shown in Tab. 6.

**Influence of vision prompts.** We empirically find that vision prompts affect the interactive segmentation performance. To ensure a fair comparison, we adopt the same vision prompts generation strategy designed in AGILE3D  to evaluate our interactive segmentation performance.

We ablate 3D spatial distances between the vision prompts and instance centers. Specifically, assuming an instance containing \(n\) points, we denote the mean coordinate of these points as the _instance center_ and order the \(n\) points based on their distances to the instance center. Then, we evaluate the interactive segmentation performance while employing the \( r_{d} n\)-th nearest point as the vision prompt, as shown in Tab. 7. When the vision prompt is positioned at the instance center, the interactive segmentation achieves an upper-bound performance of \(56.6\) AP, exhibiting a substantial performance gap of up to \(20.2\) AP compared to when the vision prompt is located at the object's edge (\(r_{d}=1.0\)), illustrating considerable room for improvement. We also observe an unusual performance decline while increasing \(r_{d}\) from \(0.9\) to \(1.0\), which we attribute to the ambiguity in distinguishing the edge points of adjacent instances. As we know, this is the first work ablating the influence of vision prompts, and we will explore this issue in depth in future work.

## 5 Conclusion and Discussion

We propose a unified framework named UniSeg3D, which provides a flexible and efficient solution for 3D scene understanding, supporting six tasks within a single model. Previous task-specific approaches fail to leverage cross-task information, limiting their understanding of 3D scenes to task-specific perspectives. In contrast, we take advantage of the multi-task design and enhance performance by building inter-task associations. Specifically, we employ knowledge distillation and ranking-based contrastive learning to facilitate cross-task knowledge sharing. Experiments demonstrate that the proposed framework is a powerful method, achieving SOTA performance across six unified tasks.

**Limitation.** UniSeg3D aims to achieve unified 3D scene understanding. However, it works on indoor tasks and lacks explorations in outdoor scenes. Additionally, we observe that UniSeg3D performs worse interactive segmentation performance when the vision prompt is located away from the instance centers, limiting the reliability of the UniSeg3D and should be explored in future work.

   Datasets &  &  &  \\  Hyper-parameter & Pan. & Sem. & Inst. & Inter. & Ref. & OV & Overall \\  \(\) & PQ & mIoU & mAP & AP & mIoU & AP & \\ 
0.05 & 70.7 & 76.2 & 58.9 & **54.4** & 29.5 & **19.6** & 51.6 \\
0.1 & **71.3** & 76.3 & **59.1** & 54.1 & 29.5 & **19.6** & **51.7** \\
0.2 & 70.8 & **76.6** & 58.6 & 52.3 & **29.8** & 19.5 & 51.3 \\
0.3 & 70.6 & 75.7 & 58