# Reimagining Mutual Information for Defense against Data Leakage in Collaborative Inference

Lin Duan\({}^{1}\)1, Jingwei Sun\({}^{1}\)1, Jinyuan Jia\({}^{2}\), Yiran Chen\({}^{1}\), Maria Gorlatova\({}^{1}\)

\({}^{1}\) Department of Electrical and Computer Engineering, Duke University

\({}^{2}\) College of Information Sciences and Technology, Pennsylvania State University

\({}^{1}\) {lin.duan, jingwei.sun, yiran.chen, maria.gorlatova}@duke.edu

\({}^{2}\) jinyuan@psu.edu

###### Abstract

Edge-cloud collaborative inference empowers resource-limited IoT devices to support deep learning applications without disclosing their raw data to the cloud server, thus protecting user's data. Nevertheless, prior research has shown that collaborative inference still results in the exposure of input and predictions from edge devices. To defend against such data leakage in collaborative inference, we introduce InfoScissors, a defense strategy designed to reduce the mutual information between a model's intermediate outcomes and the device's input and predictions. We evaluate our defense on several datasets in the context of diverse attacks. Besides the empirical comparison, we provide a theoretical analysis of the inadequacies of recent defense strategies that also utilize mutual information, particularly focusing on those based on the Variational Information Bottleneck (VIB) approach. We illustrate the superiority of our method and offer a theoretical analysis of it.

## 1 Introduction

Edge devices are becoming smarter and more versatile. These devices are expected to efficiently perform a wide range of deep learning (DL) inference tasks with remarkable performance. However, implementing DL inference applications on such edge devices is challenging due to the constraints imposed by the on-device resource availability. As we see the rise of state-of-the-art (SOTA) DL models, such as Large Language Models [1; 2], they are becoming increasingly complex, housing a colossal number of parameters. This escalation in complexity and size makes it difficult to store a DL model on an edge device, which typically has limited memory space. Furthermore, the restricted computational resources could lead to prolonged latency during inference. One potential solution to this predicament is to transmit the input data directly from the edge device to a cloud server. The server, which houses the DL model, then conducts inference and sends the prediction back to the device. However, this approach carries a risk of data leakage, particularly if the input data are sensitive in nature - such as facial images. In addition, the output data (i.e., predictions) can also contain confidential information, such as the patient's diagnostic results.

Collaborative inference [3; 4; 5; 6; 7; 8; 9] has emerged as an approach to prevent data leakage when deploying DL inference applications on commodity edge devices with constrained computing resources. Fig. 1 shows a general collaborative inference system. Suppose an edge device and a cloud server conduct collaborative inference. The deep learning model can be divided into three parts2. The first and last few layers of the network are deployed on the edge device, while the remaining layers are offloadedto the cloud server. This division allows most of the computational tasks to be handled by the server, effectively mitigating the resource limitations on the device. The edge device and the cloud server communicate only the intermediate outputs of the model, ensuring that the raw input and predictions remain inaccessible to the server. However, recent works [10; 11] have revealed that sharing these intermediate outputs can still lead to data leakage from edge devices, including input data and predictions. A malicious server can, for instance, reconstruct input data from the representations (i.e., \(r\) in Fig. 1) uploaded by the device through Model Inversion (MI) attacks [12; 13; 11]. Furthermore, the high-level features (i.e., \(z\) in Fig. 1) contain rich information about the predictions, making it feasible for a malicious server to infer the device's predictions through these features [14; 15; 16]. While there have been considerable explorations into data protection in collaborative inference [10; 11; 17; 18], existing defenses tend to significantly degrade model utility. This degradation is particularly evident in scenarios where attacks are relatively strong. For example, when the head model on the device (i.e., \(^{h}\) in Fig. 1) is shallow, existing defenses [10; 11; 19; 20; 20; 18] cannot guarantee robustness against MI attacks without a significant drop in model accuracy as shown in our results.

We propose InfoScissors, a defense method designed from a mutual information perspective to protect the edge device's data in collaborative inference. This approach works by protecting both the device's input data and its predictions. The goal of our method is to preserve user privacy in collaborative inference, and privacy preservation is achieved by manipulating the training phase (i.e., collaborative training). By applying our defense, the model is normalized to filter the private information when extracting features and representations, such that privacy is preserved during the inference phase. To protect the input data, we regularize the head model on the device to extract representations that contain less mutual information with the input. To protect the prediction, we regularize the features extracted by the server's encoder to minimize the mutual information they contain with the label. We derive a variational mutual information upper bound and develop an adversarial training method to minimize this bound on the device side. There are works [21; 17; 18] that prevent input leakage from the mutual information perspective, and most of them are based on _Variational Information Bottleneck (VIB) _. Our work is not a simple replacement of the mutual information approximation compared with these works. We analyze the inadequacies of the VIB-based methods in protecting input data in the context of collaborative inference and illustrate the superiority of our method. We evaluate our method on CIFAR10 and CIFAR100 against input leakage using both black-box and white-box MI attacks. The results show that our method can effectively defend the attacks with less than a 3% drop in model accuracy even when the head model on the device has only one convolutional layer, where the attacks are extremely strong. We also evaluate our defense against prediction leakage using multiple Model Completion (MC) attacks [14; 15]. The results show that our defense achieves the best trade-off between the model accuracy and the defense effectiveness compared to the baselines. Our contributions are summarized as follows:

* We propose InfoScissors, a defense method against data leakage in collaborative inference from the mutual information perspective, encompassing both input leakage and prediction leakage.
* We offer a theoretical analysis of our defense against input recovery attacks and prediction inference attacks. We also analyze the superiority of our method compared with the VIB-based methods.
* We empirically evaluate InfoScissors across multiple datasets and against multiple attacks. Our method effectively defends against MI and MC attacks, outperforming the baselines.

## 2 Related Work

Data Leakage in Collaborative InferenceData leakage is drawing more and more attention as the rapid growth of commercial deployment of DL, especially in collaborative learning scenarios, whose

Figure 1: A general framework of collaborative inference. The malicious server can infer input and predictions on the edge device. Our method defends against data leakage by reducing the mutual information between the model’s intermediate outcomes and the edge device’s data and predictions.

primary concern is data safety. In collaborative inference, we categorize data leakage into two types, i.e., input leakage [23; 10; 24; 25] and prediction leakage [14; 15; 16]. For input leakage,  proposes general attack methods for complex models, such as Neural Networks, by matching the correlation between adversary features and target features, which can be seen as a variant of model inversion [26; 27]. [10; 28; 25; 29; 30; 24] also propose variants of model inversion attack. While all these attacks are in the inference phase,  proposes a variant of DLG , which can perform attacks in the training phase. For prediction leakage,  proposes an attack and defense method for two-party split learning on binary classification problems, a special collaborative inference setting. Additionally,  proposes three different label inference attack methods considering different settings in collaborative inference: direct label inference attack, passive label inference attack, and active label inference attack.

Defense in Collaborative InferenceDefensive methods have been proposed against data leakage in collaborative inference. To defend against input leakage, some works apply differential privacy (DP) [10; 11; 19; 31] and compression [10; 11; 17; 32] to the representations and models. While these methods can successfully defend against input leakage from the representations, they cause substantial model performance degradation because they weaken the knowledge/information in the representations. Some recent works also try to prevent input leakage by regularizing the representations from the mutual information perspective [17; 18; 21]. However, their methods only achieve decent results when the head model on the edge device is deep, which is not practical when the computation power is constrained on the edge device. Our paper analyzes that such a disadvantage comes from the inadequacies of _VIB_ in the context of input protection. Some other works [33; 34; 35] apply mutual information on input space to protect input data, but their methods are only feasible with limited input dimension in the context of collaborative inference due to computational constrain on edge devices. One recent work  studies inference defense under a similar setting to our paper. But they only focus on prediction protection. They add noise to the training label by randomly sampling a class label, which is intuitively inspired by DP. Our training method is theoretically derived from the perspective of mutual information, and we provide a theoretical analysis of our defense performance. To defend against prediction leakage,  manipulates the labels following specific rules to defend the direct label inference attack, which can be seen as a variant of label differential privacy (label DP) [37; 38] in collaborative inference. Compression and quantization of the gradients [14; 18] are also applied to prevent prediction leakage. However, similar to the defense against input leakage, these defenses cause substantial model performance degradation to achieve decent defense performance.

## 3 Preliminary

### Collaborative Inference Setting

Suppose an edge device and a cloud server conduct collaborative inference. Following the setting in Fig. 1, the deep learning model is divided into a head model \(f^{h}_{^{h}}\), an encoder \(f^{e}_{^{e}}\) and a classifier \(f^{c}_{^{e}}\). The head model and classifier are deployed on the edge device, and the encoder is on the cloud server. Given an input \(x_{i}\), the edge device first calculates the representation \(r_{i}=f^{h}_{^{h}}(x_{i})\) and sends \(r_{i}\) to the server. Then the server extracts the feature from the received representation \(z_{i}=f^{e}_{^{e}}(r_{i})\) and sends \(z_{i}\) back to the edge device. After receiving the feature, the edge device calculates the prediction \(_{i}=f^{c}_{^{e}}(z_{i})\). In this paper, the results of \(f^{h}_{^{h}}\) sent from the device to the server are referred to as _representations_, and _features_ refer to the results of \(f^{e}_{^{e}}\) sent from the server to the device. The overall inference procedure is formulated as:

\[_{i}=f^{c}_{^{e}}(f^{e}_{^{e}}(f^{h}_{^{h}}(x_{i}))).\] (1)

In the real world, the input \(x_{i}\) and prediction \(_{i}\) are important intellectual properties of the edge device and may contain personal information. In the inference procedure, the edge device does not send raw input to the server, and the inference results are also inaccessible to the server.

### Threat Model

Our goal is to protect the edge device's input and predictions from being inferred by the cloud server. The device only uploads the representations to the server and never leaks raw input or predictions to the server. However, the cloud server is untrusted, attempting to steal input and predictions. We assume the untrusted server strictly follows the collaborative inference protocols, and it cannot compromise the inference process conducted by the device. Nevertheless, the adversary (i.e., malicious server) is capable of training a surrogate classifier and generator to mimic the victim's data. With the received representation \(r_{i}\), the server can reconstruct the input \(x_{i}\) on the device by conducting MI attacks [12; 13; 10]. Notably, the head model on the device is usually shallow due to the computation resource limitation, which aggravates the input leakage from the representation . The encoder on the server extracts high-level features containing rich information about the prediction, which enables the server to infer predictions of the device. We conduct preliminary experiments to illustrate the data leakage in collaborative inference, which can be found in Appendix A.

## 4 Method

### Defense Formulation

To defend against data leakage, we propose InfoScissors, a learning algorithm that regularizes the model during the training phase. Following the setup of 3.1, suppose the edge device has sample pairs \(\{(x_{i},y_{i})\}_{i=1}^{N}\) drawn from a distribution \(p(,)\). The representation is calculated as \(\!=\!f_{^{c}}^{h}()\) by the edge device, and the cloud server computes features \(\!=\!f_{^{c}}^{e}()\). We apply \(,,,\) here to represent random variables, while \(x_{i},y_{i},\)\(r_{i},\)\(z_{i}\) are deterministic values. To defend against the leakage of the edge device's input data and inference results, InfoScissors is designed to achieve three goals:

* Goal 1: To preserve the performance of collaborative inference, the main objective loss should be minimized.
* Goal 2: To prevent the input leakage from the representations, \(^{h}\) should not extract representations r containing much information about the input data x.
* Goal 3: To reduce the leakage of the predictions on the edge device, \(^{e}\) on the cloud server should not be able to extract features z containing much information about the true label y.

Formally, we have three training objectives:

\[&_{^{h },^{e},^{e}}f_{^{e}}^{c}f_{^{ e}}^{e}f_{^{h}}^{h}(),,\\ &, ^{h}}{}(;),\\ &, ^{e}}{}(;),\] (2)

where I(r;x) is the mutual information between the representation and the input, which indicates how much information r retains about the input data x. Similarly, I(z;y) is the mutual information between the feature and the label. We minimize these mutual information terms to prevent the cloud server from inferring the input x and label y from r and z, respectively.

The prediction objective is usually easy to optimize (e.g., cross-entropy loss for classification). However, the mutual information terms are hard to calculate in practice for two reasons: 1. r and x are high-dimensional, and it is extremely computationally heavy to compute their joint distribution; 2. Calculating the mutual information requires knowing the distributions p(xlr) and p(yiz), which are both difficult to compute. We do not follow existing works [18; 21; 17] to employ VIB to derive tractable estimations of the mutual information objectives. We analyze the inadequacies of VIB in protecting input, which can be found in Sec. 4.4, and leverage CLUB  to formulate variational upper bounds of mutual information terms. We first formulate a variational upper bound of I(r;x):

\[(;)\!\!_{}(;)\!:=\!_{p(,)}\! q_{}(|) \!-\!_{p()(;)}\! q_{}(|),\] (3)

where \(q_{}(|)\) is a variational distribution with parameters \(\) to approximate \(p(|)\). To guarantee the inequality of Eq. (3), \(q_{}(|)\) should satisfy:

\[(p(,)||q_{}(,))\!\! (p()p()||q_{}(,)),\] (4)

which can be achieved by minimizing \((p(,)||q_{}(,))\):

\[=\!(p(,)||q_{}( ,))\!=\!_{p(, )}\!(q_{}(|)).\] (5)

With sample pairs \(\{(x_{i},y_{i})\}_{i=1}^{N}\), we apply the sampled vCLUB (vCLUB-S) mutual information estimator in  to reduce the computational overhead, which is an unbiased estimator of \(_{}\) and 

[MISSING_PAGE_EMPTY:5]

\(_{d\_r}\), respectively. We can reorganize the overall training objective as:

\[^{h},^{e},^{c},,\] \[=_{^{h},^{c}}[(1-_{d}-_{l})\!\! _{^{c}}\!\!_{c}+_{l}\!\!_{}\!\!_ {l\_a}\!+\!_{l}\!\!_{l\_r}+_{d}\!\!_{}\!\! _{d\_a}\!+\!_{d}\!\!_{d\_r}].\] (10)

Based on Eq. (10), we develop a collaborative learning algorithm. For each batch of data, the device first optimizes the classifiers \(^{c}\) and \(\) by minimizing \(_{c}\) and maximizing \(_{l\_a}\), respectively. Then, the device optimizes the generator \(\) by maximizing \(_{d\_a}\). Finally, \(^{h}\) and \(^{c}\) are optimized by minimizing \((1-_{d}-_{l})_{c}+_{l}_{l\_a}+ _{l}_{l\_r}+_{d}_{d\_a}+_{d} _{d\_r}\). The detailed algorithm can be found in Appendix B. Note that \(^{h},^{c},,\) and \(\) are deployed on devices, and their training does not need additional information from the cloud server compared with training without our defense. The training procedure of \(^{c}\) does not change, which makes our defense concealed from the cloud server.

### Theoretical Analysis

We provide a theoretical analysis of our defenses against prediction and input leakage. Following the notations in Sec. 4.1, we have the following theorem of defense performance for prediction leakage after applying InfoScissors. All the proofs can be found in Appendix C.

**Theorem 1**.: _Let \(h_{}\) parameterize \(q_{}\) in Eq. (8). Suppose the malicious server optimizes an auxiliary model \(h^{m}(|)\) to estimate \(p(|)\). For any \(h^{m}(|)\), we always have:_

\[\!_{i=1}^{N}\!\!\!h^{m}(y_{i}|z_{i})\!<\!\!_{ i=1}^{N}\!\!\!p(y_{i})\!+\!,\] (11)

_where_

\[\!=\!I_{\!_{h_{}}}(\!z\!;\!)\!+\! (p(|)||h_{}(|)).\] (12)

Specifically, if the task of collaborative inference is classification, we have the following corollary:

**Corollary 1**.: _Suppose the task of collaborative inference is classification. Following the notations in Theorem 1 and let \(epsilon\!\) be defined therein, we have:_

\[\!_{i=1}^{N}\!\![h^{m}(z_{i}),y_{i}]\!>\!_{random}\!-\!,\] (13)

_where CE denotes the cross-entropy loss, and \(_{random}\) is the cross-entropy loss of random guessing._

For input leakage, we have the following theorem.

**Theorem 2**.: _Let the assumption of \(p(|)\) in Sec. 4.1 hold and \(g_{}\) parameterize the mean of \(q_{}\) in Eq. (7). \(Q\) denotes the dimension of \(\). Suppose the malicious server optimizes an auxiliary model \(g^{m}(|)\) to estimate the mean of \(p(|)\). For any \(g^{m}(|)\), we always have:_

\[\!_{i=1}^{N}\!\![g^{m}(r_{i}),x_{i}]\!>\!,\] (14)

_where MSE denotes the **mean square error**, and_

\[\!=\!-\!\!_{i=1}^{N}\!\!\!}{p(x_{i} )},\!=\!I_{\!_{g_{}}}(;\! )\!+\!(p(|)||g_{}(|)).\] (15)

### Superiority over VIB

Recent works also try to prevent input leakage by regularizing the representations from the mutual information perspective . The ultimate goal of these works is the same as our method, which is to minimize \(I(r;\!x)\). However, most of these works apply _Variational Information Bottleneck (VIB) _ to derive the upper bound of \(I(r;)\). Even though VIB is commonly applied in DNN feature regularization, we analyze that it is not optimal in defending against input leakage in collaborative inference.

The variational upper bound of \(I(r;\!x)\), which is also an objective to minimize, derived through VIB is formulated as \[I()\!\!_{ p(),x p()}[p _{^{h}}(|,),r()],\] (16)

where \(\) is the Gaussian random variable used to reparameterize the randomness of representation extractor \(^{h}\). \(r()\) is the variational approximation of the marginal distribution \(p()\). In practice, without any prior information, \(r()\) is set to be a fixed spherical Gaussian, \(r()=(|0,I)\). To minimize \(()\), the VIB-based works [21; 17; 18] regularize the representation distribution to be close to a fixed Gaussian. This will cause the **complementary loss of information** contained by r, including the mutual information with y, which is crucial for the primary inference task. In contrast, the training objective derived in our method is to **only filter out the information in r that is crucial for reconstructing** x, which will cause less information loss and inference performance drop.

We can also analyze the difference between VIB-based methods and our method from a high level, which also motivates us to choose CLUB approximation. The mutual information \(I()\) can be expanded as

\[I() =\!\!drdxp(r,z)\] (17) \[=\!\!drdxp(r,z).\]

VIB follows the first equality to derive a tractable upper bound of \(I()\) based on a parameterized variational \((r|x)\). Subsequently, the training objective is focused on minimizing this variational \((r|x)\). We follow the second equality to parameterize \(p(x|r)\) and derive an upper bound based on CLUB. Our training goal is centered on minimizing the variational \((x|r)\). Importantly, in the context of defending against model inversion attacks, the primary aim is to attain a low \(p(x|r)\). This aim is more closely aligned with our training objective compared to VIB-based methods. This alignment, alongside the empirical results presented in the subsequent section, highlights the superiority of our method.

## 5 Experiments

We first evaluate our method against input leakage and prediction leakage, both separately and in an integrated manner. The experiments are conducted on a server with 4 RTX TITAN GPUs.

### Experimental Setup

Attack methodsFor input leakage, we evaluate InfoScissors against two Model Inversion (MI) attacks: (1) **Knowledge Alignment (KA)** is a black-box MI attack, in which the malicious server trains an inversion model that swaps the input and output of the target model using an auxiliary dataset. The inversion model is then used to reconstruct the input given any representation. (2) **Regularized Maximum Likelihood Estimation (rMLE)** is a white-box MI attack that the malicious server has access to the device's extractor model \(^{h}\). The server trains input to minimize the distance between the fake representations and the received ground-truth representations. It is an unrealistic assumption that the server can access the model on the device, and we apply this white-box attack to evaluate our defense against extremely strong attacks. For prediction leakage, we evaluate our defense against two Model Completion (MC) attacks: (1) **Passive Model Completion (PMC)** attack assumes

Figure 3: Images reconstructed by the KA attack on CIFAR10 under different defenses. Each row represents a different defense level. The bottom row applies the strongest defense, resulting in lower-quality reconstructed images and a sacrifice in accuracy.

that the malicious server has access to an auxiliary labeled dataset and utilizes this auxiliary dataset to fine-tune a classifier that can be applied to its encoder. (2) **Active Model Completion (AMC)** attack is included as an _adaptive attack_ against our defense. The primary goal of our defense is to reduce the collaborative model's reliance on the server-side encoder for specific tasks, allowing data and prediction information to be filtered during inference. Under the adaptive attack setting, the adversary is allowed to modify the training profiling such that it can trick the collaborative model into relying more on its encoder, thereby extracting more private data information from the encoder's features. In this setting, the adversary is directly confronting the fundamental principles of our defense method, constituting a highly potent form of adaptive attack.

BaselinesWe compare InfoScissors with five existing defense baselines: (1) **Differential Privacy (DP)**[10; 11; 19], (2) **Adding Noise (AN)**, (3) **Data Compression (DC)**, (4) **Privacy-preserving Deep Learning (PPDL)**, and (5) **Mutual Information Regularization Defense (MID)**, which is the SOTA defense against data leakage in collaborative inference based on _Variational Information Bottleneck (VIB)_. The details of the baselines can be found in Appendix D.

Dataset & Hyperparameter configurationsWe evaluate on CIFAR10 and CIFAR100. For both datasets, we apply ResNet18 as the backbone model. The first convolutional layer and the last basic block are deployed on the device as the representation extractor and the classifier, respectively. We set batch size \(B\) as 32 for both datasets. We apply SGD as the optimizer with the learning rate \(\) set to be 0.01. The server has 40 and 400 labeled samples to conduct KA and MC attacks for CIFAR10 and CIFAR100, respectively. For InfoScissors, we apply a 1-layer decoder and a 3-layer MLP to parameterize \(\) and \(\). For AN, we apply Laplacian noise with a mean of zero and a scale between 0.0001-0.01. For DC, we set the compression rate from 90% to 100%. For PPDL, we set the Laplacian noise with a scale of 0.0001-0.01, \(=0.001\) and \(\) between 0 and 0.01. For MID, we set the weight of mutual information regularization between 0-0.1.

Evaluation metrics(1) **Utility metric (Model accuracy)**: We use the test data accuracy of the classifier on the device to measure the performance of the collaborative model. (2) **Robustness metric (SSIM)**: We use SSIM (structural similarity) between the reconstructed images and the raw images to evaluate the effectiveness of the defense against input leakage. The lower the SSIM, the better the defense performance. (3) **Robustness metric (Attack accuracy)**: We use the test accuracy of the server's classifier after conducting MC attacks to evaluate the defense against prediction leakage. The lower the attack accuracy, the higher the robustness against prediction leakage.

### Results of Input Protection

We conduct experiments on CIFAR10 and CIFAR100 to evaluate our defense against the KA attack and the rMLE attack. We set different defense levels for our methods (i.e., different \(_{d}\) values in Eq. (9)) and baselines to conduct multiple experiments to show the trade-off between the model accuracy and SSIM of reconstruction. The results are shown in Fig. 4.

For defense against KA attack, our InfoScissors can reduce the SSIM of reconstruction to lower than 0.2 with a model accuracy drop of less than 2% for CIFAR10. In contrast, the other baselines reduce model accuracy by more than 10% and cannot achieve the same defense effect even with an accuracy drop of more than 10%. Notably, the malicious server has more auxiliary data on CIFAR100 than CIFAR10, making the defense harder on CIFAR100. However, InfoScissors can still achieve an SSIM of lower than 0.2 with a model accuracy drop of less than 2%. We also evaluate our defense against the KA attack with a larger auxiliary dataset on the malicious server, and the results, which can be found

Figure 4: Model accuracy v.s. SSIM on CIFAR10 and CIFAR100 against MI attacks.

in Appendix D, show that our defense can effectively defend against the KA attack when the server has more auxiliary samples. For defense against rMLE attacks, InfoScissors achieves similar results of reducing the SSIM to lower than 0.2 with a model accuracy drop of less than 2% for CIFAR10 and 1% for CIFAR100, respectively, which outperforms the other baselines significantly.

To perceptually demonstrate the effectiveness of our defense, we show the reconstructed images by the KA attack on CIFAR10 after applying baseline defenses and our defense in Fig. 3. It is shown that by applying the baseline defenses, the reconstructed images still contain enough information to be recognizable with the model accuracy of lower than 70%. For our method, the reconstructed images do not contain much information about the raw images, with the model accuracy higher than 76%.

### Results of Prediction Protection

We evaluate InfoScissors on two datasets against the PMC attack and the AMC attack. We set different defense levels for our methods (i.e., different \(_{l}\) values in Eq. (9)) and baselines to conduct multiple experiments to show the trade-off between the model accuracy and attack accuracy. The defense results against PMC and AMC attacks are shown in Fig. 5 and Fig. 6, respectively. To simulate the realistic settings where the malicious server uses different model architectures to conduct MC attacks, we apply different model architectures (MLP & MLP_sim) for MC attacks. The detailed model architectures can be found in Appendix D.

For defense against PMC on CIFAR10, InfoScissors achieves 10% attack accuracy (equal to random guess) by sacrificing less than 0.5% model accuracy, while the other baselines suffer a model accuracy drop by more than 4% to achieve the same defense effect. Similarly, InfoScissors achieves 1% attack accuracy on CIFAR100 by sacrificing less than 1% model accuracy, while the other baselines achieve the same defense effect by sacrificing more than 6% model accuracy.

InfoScissors also shows robustness against AMC attack. InfoScissors achieves attack accuracy of the rate of random guess by sacrificing less than 1% and 0.5% model accuracy on CIFAR10 and CIFAR100, respectively. The other baselines achieve the same defense performance by sacrificing more than 5% and 4% model accuracy, respectively.

### Integration of Input and Prediction Protection

We evaluate the integration of input and prediction protection of InfoScissors. We set \(_{d}\) and \(_{l}\) between 0.05-0.4 and evaluate the defenses. The results of defense against the KA and PMC attacks

Figure 5: Model accuracy v.s. attack accuracy on CIFAR10 and CIFAR100 against PMC attack.

Figure 6: Model accuracy v.s. attack accuracy on CIFAR10 and CIFAR100 against AMC attack.

on CIFAR10 and CIFAR100 are shown in Fig. 7. It is shown that InfoScissors can effectively protect input data and predictions simultaneously with less than a 2% accuracy drop for both datasets.

## 6 Conclusion and Limitation

We propose a defense method (InfoScissors) to defend against data leakage in collaborative inference by reducing the mutual information between the model's intermediate outcomes and the device's input data and predictions. The experimental results show that our method can defend against input leakage and prediction leakage effectively. Our work can make the public aware of the risk of privacy leakage posed by collaborative inference, which is considered a positive societal impact. One limitation of this paper is that we only focus on the scenario where there is only one edge device, even though our defense can be easily applied to the collaborative inference scenario with multiple edge devices.