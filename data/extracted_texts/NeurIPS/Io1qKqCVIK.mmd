# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

Introduction

Polygonal meshes are widely used in modeling and animation due to their diverse, compact and explicit configuration. Recent AI progress has spurred efforts to integrate mesh generation into machine learning, but challenges like varying topology hinder suitable differentiable mesh representations. This limitation leads to reliance on differentiable intermediates like implicit functions, and subsequent iso-surface extraction for mesh creation (Liao et al., 2018; Guillard et al., 2021; Munkberg et al., 2022; Shen et al., 2023, 2021; Liu et al., 2023b). However, meshes generated by such approaches can be misaligned at sharp regions and unnecessarily dense (Shen et al., 2023), not suitable for down-stream applications that require light-weight meshes. This limitation necessitates us to develop a truly differentiable mesh representation, not the intermediate forms.

The fundamental challenge in creating a differentiable mesh representation lies in formulating both the vertices' geometric features and their connectivity, defined as edges and faces, in a differentiable way. Given a vertex set, predicting their connectivity in a free-form way using existing machine learning data-structures can cost significant amount of computation and be difficult to avoid irregular and intersecting faces. Consequently, most studies on differentiable meshes simplify the task by using a mesh with a pre-determined topology and modifying it through various operations (Zhou et al., 2020; Hanocka and u. a., 2019; Palfinger, 2022; Nicolet et al., 2021). This work, on the contrary, ambitiously aims to establish a general 3D mesh representation, named as DMesh, where both mesh topology and geometric features (e.g. encoded in vertex location) can be simultaneously optimized through gradient-based techniques.

Our core insight is to use differentiable Weighted Delaunay Triangulation (WDT) to divide a convex domain, akin to amber encapsulating a surface mesh, into tetrahedra to form a mesh. To create a mesh with arbitrary topology, we select only a subset of triangular faces from the tetrahedra, termed the "real part", as our final mesh. The other faces, the "imaginary part", support the real part but are not part of the final mesh (Figure 4). We introduce a method to assess the probability of a face being part of the mesh based on weighted points that carry positional and inclusiveness information. Optimization is then focused on the points' features to generate the triangular mesh. The probability determination allows us to compute geometric losses and rendering losses during gradient-based optimization that optimizes **connectivity and positioning**.

The key contributions of our work can be summarized as follows.

* We present a novel differentiable mesh representation, **DMesh**, which is versatile to accommodate various types of mesh (Figure 2). The generated meshes can represent shapes more effectively, with much less number of vertices and faces (Table 2).
* We propose a computationally efficient approach to differentiable WDT, which produces robust probability estimations. While exhaustive approach (Rakotosaona and u. a., 2021) requires quadratic computational cost, our method runs in approximately _linear time_.
* We provide efficient algorithms for reconstructing surfaces from both point clouds and multi-view images using DMesh as an intermediate representation.
* We finally propose an effective regularization term which can be used for mesh simplification and enhancing triangle quality.

Additionally, to further accelerate the algorithm, we implemented our main algorithm and differentiable renderer in CUDA, which is made available for further research.

## 2 Related Work

### Shape Representations for Optimization

Recently, using neural implicit functions for shape representation gained popularity in graphics and vision applications (Mildenhall et al., 2021; Zhang et al., 2020; Liu et al., 2020; Chen and a., 2022; Wang et al., 2021; Yariv and others, 2020). They mainly use volume density, inspired by (Mildenhall et al., 2021), to represent a shape. However, because of its _limited accuracy_ in 3D surface representation, neural signed distance functions (SDFs) (Yariv and others, 2021; Wang et al., 2021, 2023; Oechsle and a., 2021) or unsigned distance functions (UDFs) (Liu et al., 2023a; Long et al., 2023) are often preferred.

After optimization, one can recover meshes using iso-surface extraction techniques (Lorensen and Cline, 1998; Ju u. a., 2002).

Differing from neural representations, another class of methods directly produce meshes and optimize them. However, they assume that the overall mesh topology is fixed (Chen u. a., 2019; Nicolet u. a., 2021; Liu u. a., 2019; Laine u. a., 2020), only allowing local connectivity changes through remeshing (Palfinger, 2022). Learning-based approaches like BSP-Net (Chen u. a., 2020) allow topological variation, but their meshing process is not differentiable. Recently, differentiable iso-surface extraction techniques have been developed, resulting in high-quality geometry reconstruction of various topologies (Liao u. a., 2018; Shen u. a., 2021, 2023; Wei u. a., 2023; Munkberg u. a., 2022; Liu u. a., 2023b; Mehta u. a., 2022). Unfortunately, meshes relying on iso-surface extraction algorithms (Lorensen und Cline, 1998; Ju u. a., 2002) often result in unnecessarily dense meshes that could contain geometric errors. In contrast, **our approach addresses these issues: we explicitly define faces and their existence probabilities, and devise regularizations that yield simplified, but accurate meshes based on them** (Table 2). See Table 3 for more detailed comparisons to these other methods.

### Delaunay Triangulation for Geometry Processing

Delaunay Triangulation (DT) (Aurenhammer u. a., 2013) has been proven to be useful for reconstructing shapes from unorganized point sets. It's been shown that DT of dense samples on a smooth 2D curve includes the curve within its edges (Brandt und Algazi, 1992; Amenta u. a., 1998a). This idea of using DT to approximate shape has been successfully extended to 3D, to reconstruct three-dimensional shapes (Amenta u. a., 1998b) for point sets that satisfy certain constraints. However, these approaches are deterministic. Our method can be considered as a differentiable version of these approaches, which admits gradient-based optimization.

More recently, Rakotosaona u. a. (2021) focused on this DT's property to connect points and tessellate the domain, and proposed a differentiable WDT algorithm to compute smooth inclusion, namely existence score of \(2\)-simplexes (triangles) in 2 dimensional space. However, it is not suitable to apply this approach to our 3D case, as there are computational challenges (Section 3.2). Other related work, VoroMesh (Maruani u. a., 2023), also used Voronoi diagrams in point cloud reconstruction, but their formulation cannot represent open surfaces and is only confined to handle point clouds.

## 3 Preliminary

### Probabilistic Approach to Mesh Connectivity

To define a traditional, non-differentiable mesh, we specify the vertices and their connectivity. This connectivity is discrete, meaning for any given triplet of vertices, we check if they form a face in the mesh, returning 1 if they do and 0 otherwise. To overcome this discreteness, we propose a

Figure 3: Our overall framework to optimize mesh according to the given observations. **(a)**: Each point is defined by a \(5\)-dimensional feature vector: position, weight, and real value. Points with larger real values are rendered in red. **(b)**: Given a set of points, we gather possibly existing faces in the mesh and evaluate their probability in differentiable manner. **(c)**: We can compute reconstruction loss based on given observations, such as mesh, point cloud, or multi-view images. **(d)**: To facilitate the optimization process and enhance the mesh quality, we can use additional regularizations.

probabilistic** approach to create a fully differentiable mesh - given a triplet of vertices, we evaluate the probability of a face existing. This formulation enables differentiability not only of **vertex positions** but also of their **connectivity**.

Note that we need a procedure that tells us the existence probability of any given face to realize this probabilistic approach. This procedure must be **1)** differentiable, **2)** computationally efficient, and **3)** maintain desirable mesh properties, such as avoiding non-manifoldness and self-intersections, when determining the face probabilities.

Among many possible options, we use _Weighted Delaunay Triangulation (WDT)_ (Figure 6(a)) and a point-wise feature called the "real" value (\(\)) to define our procedure. Each vertex in our framework is represented as a 5-dimensional2 vector including position (3), WDT weight (1), and real value (1) (Figure 3(a)). Given the precomputed WDT based on vertices' positions and weights, we check the face existence probability of each possible triplets. Specifically, **1)** a face \(F\) must exist in the WDT, and then **2)** satisfy a condition on the real values of its vertices to exist on the actual surface. We describe the probability functions for these conditions as \(_{wdt}\) and \(_{real}\):

\[_{wdt}(F)=P(F),_{real}(F)=P(F \,|\,F).\] (1)

Then we get the final existence probability function, which can be used in downstream applications (Figure 3), as follows:

\[(F)=P(F)=_{wdt}(F)_{real}(F).\] (2)

This formulation attains one nice property in determining the final mesh - that is, it prohibits self-intersections between faces. When it comes to the other two criteria about this procedure, \(_{wdt}\) function's differentiability and efficiency is crucial, as we design \(_{real}\) to be a very efficient differentiable function based on real values (Section 4.2). Thus, we first introduce how we can evaluate \(_{wdt}\) in a differentiable and efficient manner, which is one of our main contributions.

### Basic Principles

To begin with, we use \((d,k)\) pair to denote a \(k\)-simplex (\(^{k}\)) in \(d\)-dimensional space. For a 3D mesh, we observe that our face \(F\) corresponds to \((d=3,k=2)\) in Figure 5(b). To compute the probability \(_{wdt}\) for the face, we use power diagram (PD), which is the dual structure of WDT (Figure 6(a)). While previously Rakotosaona et al. (2021) proposed a differentiable 2D triangulation method for the \((d=k=2)\) case (Figure 5(c)), it suffers from quadratically increasing computational cost (e.g. it takes 4.3 seconds to process \(10K\) points in 2D, Table 4) and unreliable estimation when \((k<d)\). We will discuss later how our formulation conquer these computational challenges. Our setting for 3D meshes are similar to 2D meshes, the \((d=2,k=1)\) case in Figure 5(d), where a triangular face reduces to a line. Therefore, we will mainly use this setting to describe and visualize basic concepts for simplicity. However, note that it can be generalized to any \((d,k)\) case without much problem.

Figure 4: Illustration of our mesh representation for 2D and 3D cases. **(a):** Our representation in 2D for a letter “A”. **(b):** Our representation in 3D for a dragon model. Blue faces are _“real part”_ and yellow ones are _“imaginary part”_.

Figure 5: Renderings of \(^{k}\)s for different pairs of \((d,k)\). Different \(^{k}\)s are rendered in different colors.

We generalize the basic principles suggested by Rakotosaona u. a. (2021) to address our cases. For formal definitions of the concepts in this section, please refer to Appendix B.

Let \(S\) be a finite set of points in \(^{d=2}\) with weights. For a given point \(p S\), we denote its weight as \(w_{p}\). We call those weighted points in \(S\) as "vertices", to distinguish them from general unweighted points in \(^{2}\). Then, we adopt power distance \((p,q)=d(p,q)^{2}-w_{p}-w_{q}\) as the distance measure between two vertices in \(S\). As depicted in Figure 6, \(^{k=1}\) is a 1-simplex, which is a line connecting two vertices \(p_{i}\) and \(p_{j}\) in \(S\). Its dual form \(D_{^{1}}\) (red line) is the set of unweighted points3 in \(^{2}\) that are located at the same power distance to \(p_{i}\) and \(p_{j}\). In the power diagram, the power cell \(C_{p}\) (gray cell) of a vertex \(p\) is the set of unweighted points that are closer to \(p\) than to any other vertices in \(S\). We can use \(C_{p}\) and \(D_{^{1}}\) to measure the existence of \(^{1}\). From Figure 6, we can see that when \(^{1}=\{p_{i},p_{j}\}\) exists in WDT, its dual line \(D_{^{1}}\) aligns exactly with \(C_{p_{i}}\)'s boundary, while when \(^{1}=\{p_{i},p_{j}\}\) doesn't exist in WDT, \(D_{^{1}}\) is outside \(C_{p_{i}}\).

To make this measurement less binary when \(^{1}\) exists, we use the expanded version of power cell called "reduced" power cell (\(R_{p|}\)), introduced by Rakotosaona u. a. (2021). The reduced power cell of \(p_{i} S\) for \(^{1}=\{p_{i},p_{j}\}\) is computed by excluding \(p_{j}\) from \(S\) when constructing the power cell 4. For example, when \(^{1}\) exists, \(R_{p_{i}|^{1}}\) will expand towards \(p_{j}\)'s direction (Figure 6(c)), and \(D_{^{1}}\) will "go through" \(R_{p_{i}|^{1}}\). In contrast, when \(\) doesn't exists, even though we have removed \(p_{j}\), \(R_{p_{i}|^{1}}\) will not expand (Figure 6(d)), and thus \(D_{^{1}}\) stays outside of \(R_{p_{i}|^{1}}\).

Now we can define a signed distance field \(_{pt}(x,R)\) for a given reduced power cell \(R\), where the signed distance is measured as the distance from the point \(x^{d}\) to the boundary of the reduced power cell (sign is positive when inside). Then, we can induce the signed distance between a dual form \(D\) and a reduced power cell \(R\):

\[(D,R)=_{x D}_{pt}(x,R).\] (3)

As illustrate in Figure 6(c) and (d), \((D_{^{1}},R_{p_{1}|^{1}})\) is positive when \(^{1}\) exists, while negative when it does not exist in WDT. This observation can be generalized as follows:

**Remark 3.1**.: \(^{k}\) exists in WDT if and only if \( p_{i}^{k},(D_{^{k}},R_{p_{i}|^{k}})>0\).

In fact, the sign of every \((D_{^{k}},R_{p_{i}|^{k}})\) is same for every \(p_{i}^{k}\). Therefore, we can use its average to measure the existence probability of \(_{k}\), along with sigmoid function \(\):

\[_{wdt}(^{k})=_{p\{^{k}\}}((D _{_{k}},R_{p|^{k}})_{wdt}),\] (4)

where \(_{wdt}\) is a constant value used for the sigmoid function. \(_{wdt}(^{k})\) is greater than 0.5 when \(^{k}\) exists, aligning with our probabilistic viewpoint and being differentiable.

Figure 6: **Basic concepts to compute existence probability of given \(1\)-simplex (\(^{1}\)) when \(d=2\). (a)**: WDT and PD of given set of weighted vertices are rendered in solid and dotted lines. The size of a vertex represents its weight. **(b)**: Power cell of \(p_{1}\) (\(C_{p_{1}}\)) is rendered in grey. Also, \(^{1}\) is rendered in black line, of which dual line (\(D_{^{1}}\)) is rendered in red. **(c), (d)**: For given \(^{1}\), reduced power cell of \(p_{1}\) for the \(^{1}\) (\(R_{p_{1}|^{1}}\)) is rendered in blue, with the original power cell (grey). We can evaluate the existence of \(^{1}\) in WDT by computing the signed distance from \(D_{^{1}}\) to \(R_{p_{1}|^{1}}\).

Computational Challenges.As mentioned before, Rakotosaona et al. (2021) solved the problem for the case where \((d=k=2)\) where the dual \(D_{^{k}}\) is a single point. Naively applying their approach for computing Eq. 3 to our cases poses two computational challenges:

* **Precision**: When \((d=3,k=2)\) or \((d=2,k=1)\), \(D_{_{k}}\) becomes a **line**, not a single point. Finding a point on the line that maximizes Eq. 3 is not straightforward.
* **Efficiency**: When naively estimating the reduced power cell in exhaustive manner, the computational cost increases with the number of points at a rate of \(O(N^{2})\), where \(N\) is the number of points.

See Appendix B.2 for a detailed discussion of these limitations.

## 4 Formulation

### Practical approach to compute \(_{wdt}\)

We introduce a practical approach to resolve the two aforementioned challenges. Specifically, we propose constructing the PD first and use it for computing lower bound of Eq. 3 in an efficient way. We also propose to handle the two cases separately: whether \(^{k}\) exists in the WDT or not.

First, when the simplex \(^{k}\) does not exist, we choose to use the negative distance between the dual form and the normal power cell, \(-d(D_{^{k}},C_{p})\). This is the lower bound of Eq. 3:

\[-d(D_{^{k}},C_{p})(D_{^{k}},R_{p|^{k}})<0,\] (5)

as \(C_{p} R_{p|^{k}}\). See Figure 6(d) for this case in \((d=2,k=1)\) case. We can observe that computing this distance is computationally efficient, because the normal PD only needs to be computed once for all in advance. Moreover, \(C_{p}\) is a convex polyhedron, and \(D_{^{k}}\) is a (convex) line, which allows us to find the distance between line segments on the boundary of \(C_{p}\)5 and \(D_{^{k}}\), and choose the minimum.

Second, we analyze the case when the simplex \(^{k}\) exists in WDT. In this case, we have to construct the reduced power cell \(R_{p|^{k}}\) for given \(^{k}\), which requires much additional cost. Instead of doing it, we leverage pre-computed PD to approximate the reduced power cell. Then, we pick a point \(v D_{^{k}} R_{p|^{k}}\), where \(p\{^{k}\}\), and the following holds:

\[0_{pt}(v,R_{p|^{k}})(D_{^{k}},R_{p|^{k}}),\] (6)

because \(v R_{p|^{k}}\) and by the definition at Eq. 3. In our case, since \(D_{^{k}} R_{p|^{k}}\) is a line segment, we choose its middle point as \(v\) to tighten this lower bound. See Figure 6(c) for the line segment in \((d=2,k=1)\) case. We use this bound when \(^{k}\) exists. Note that computing this lower bound is also computationally efficient, because we can simply project \(v\) to the reduced power cell.

To sum up, we can rewrite Eq. 3 as follows.

\[(D_{^{k}},R_{p|^{k}})=\{_{pt}(v,R_ {p|^{k}})&^{k}\\ -d(D_{^{k}},C_{p})&.\] (7)

By using this relaxation, we can get lower bound of Eq. 3, which is reliable because it always has the same sign. Also, we can reduce the computational cost from \(O(N^{2})\) to nearly \(O(N)\), which is prerequisite for representing meshes that have more than \(1K\) vertices in general. See Appendix B.2 for the computational speed and accuracy of our method, compared to the previous one. Finally, we implemented our algorithm for computing Eq. 7 in CUDA for further acceleration.

### Definition of \(_{real}\)

\(_{real}\) evaluates the existence probability of a \(k\)-simplex \(^{k}\) in our mesh when it exists in WDT. To define it, we leverage per-point value \(\). To be specific, we compute the minimum \(\) of the points in \(^{k}\) in differentiable way: \(_{real}(^{k})=_{p^{k}}(p)(p)\), where \((p)=e^{-(p)}/_{q^{k}}e^{-(q)}\), and \(\) is function that maps a point \(p\) to its \(\) value. We set \(=100\).

Along with \(_{wdt}\) that we discussed before, now we can evaluate the final existence probability of faces in Eq. 2. We also note here, that when we extract the final mesh, we only select the faces of which \(_{wdt}\) and \(_{real}\) are larger than 0.5.

### Loss Functions

DMesh can be reconstructed from various inputs, such as normal meshes, point clouds, and multi-view images. With its per-vertex features and per-face existence probabilities \((F)\), we can optimize it with various reconstruction losses and regularization terms. Please see details in Appendix C.

#### 4.3.1 Reconstruction Loss (\(L_{recon}\))

First, if we have a normal mesh with vertices \(\) and faces \(\), and we want to represent it with DMesh, we should compute the additional two per-vertex attributes, WDT weights and real values. We optimize them by maximizing \(()\) since these faces lies on the reference mesh. Conversely, for the remaining set of faces \(\) that can be defined on \(\), we should minimize \(()\). Together, they define the reconstruction loss for mesh input (Appendix C.1).

For reconstruction from point clouds or multi-view images, we need to optimize for all features including positions. For point clouds, we define our loss using Chamfer Distance (CD) and compute the expected CD using our face probabilities (Appendix C.2). For multi-view images, we define the loss as the \(L_{1}\) loss between the given images and the rendering of DMesh, interpreting face probabilities as face opacities. We implemented efficient differentiable renderers to allow gradients to flow across face opacities (Appendix C.3).

#### 4.3.2 Regularizations

Being fully differentiable for both vertices and faces, DMesh allows us to develop various regularizations to improve the optimization process and enhance the final mesh quality. The first is **weight regularization** (\(L_{weight}\)), applied to the dual Power Diagram of the WDT (Appendix C.4). This regularization reduces the structural complexity of the WDT, controlling the final mesh complexity (Figure 7). The next is **real regularization** (\(L_{real}\)), which enforces nearby points to have similar real values and increases the real values of points adjacent to high real value points (Appendix C.5). This helps remove holes or inner structures and makes faces near the current surface more likely to be considered (Appendix D). The final regularization, **quality regularization** (\(L_{qual}\)), aims to improve the quality of triangle faces by minimizing the average expected aspect ratio of the faces, thus removing thin triangles (Appendix C.6).

To sum up, our final loss function can be written as follows:

\[L=L_{recon}+_{weight} L_{weight}+_{real} L_{real}+ _{qual} L_{qual},\]

where \(\) values are hyperparameters. In Appendix E, we provide values for these hyperparameters for every experiment. Also, in Appendix E.3, we present ablation studies for these regularizations.

## 5 Experiments and Applications

In this section, we provide experimental results to demonstrate the efficacy of our approach. First, we optimize vertex attributes to restore a given ground truth mesh, directly proving the differentiability of our design. Next, we conduct experiments on 3D reconstruction from point clouds and multi-view images, showcasing how our differentiable formulation can be used in downstream applications.

For the mesh reconstruction problem, we used three models from the Stanford 3D Scanning Repository (Curless und Levoy, 1996). For point cloud and multi-view reconstruction tasks, we used four closed-surface models from the Thingi32 dataset, four open-surface models from the DeepFashion3D dataset, and three additional models with both closed and open surfaces from the Obi

   - & Bunny & Dragon & Buddha \\  RE & 99.78\% & 99.72\% & 99.64\% \\ FP & 0.00\% & 0.55\% & 0.84\% \\   

Table 1: Mesh reconstruction results.

Figure 7: **Results with different \(_{weight}\).**and Adobe Stock. These models are categorized as "closed," "open," and "mixed" in this section. Additionally, we use nonconvex polyhedra of various Euler characteristics and non-orientable geometries to prove our method's versatility.

We implemented our main algorithm for computing face existence probabilites and differentiable renderer used for multi-view image reconstruction in CUDA (Nickolls u. a., 2008). Since we need to compute WDT before running the CUDA algorithm, we used WDT implementation of CGAL (Jamin u. a., 2023). We implemented the rest of logic with Pytorch (Paszke u. a., 2017). All of the experiments were run on a system with AMD EPYC 7R32 CPU and Nvidia A10 GPU.

### Mesh to DMesh

In this experiment, we demonstrate that we can preserve most of the faces in the original normal triangular mesh after converting it to DMesh using the mesh reconstruction loss introduced in 4.3.

In Table 1, we show the recovery ratio (RE) and false positive ratio (FP) of faces in our reconstructed mesh. Note that we could recover over 99% of faces in the original mesh, while only having under 1% of false faces. Please see Appendix E.1 for more details. This result successfully validates our differentiable formulation, but also reveals its limitation in reconstructing some abnormal triangles in the original mesh, such as long, thin triangles.

### Point Cloud & Multi-View Reconstruction

In this experiment, we aim to reconstruct a mesh from partial geometric data, such as (oriented) point clouds or multi-view images. For point cloud reconstruction, we sampled 100K points from the ground truth mesh. We can additionally use point orientations, if they are available. For multi-view reconstruction, we rendered diffuse and depth images of the ground truth mesh from 64 view points.

  & Methods & CD (\( 10^{-5}\))\(\) & F1\(\) & NC\(\) & ECD\(\) & EF1\(\) & \# Verts\(\) & \# Faces\(\) & Time (sec)\(\) \\    & PSR & 690 & 0.770 & 0.931 & 0.209 & 0.129 & 159K & 319K & 10.6 \\   & Voronoi & 2\(\)k & 0.671 & 0.819 & \(\)1K & 0.263 & 121K & 524K & 12.2 \\   & NDC & 3.611 & 0.874 & 0.936 & **0.022** & 0.421 & 20.7K & 42.8K & **3.49** \\   & Ours (w/o normal) & 3.726 & 0.866 & 0.936 & 0.067 & 0.342 & 3.87K & 10.4K & 775 \\   & Ours (w/ normal) & **3.364** & **0.886** & **0.952** & 0.141 & **0.438** & **3.56K** & **7.54K** & 743 \\   & NIE & 555 & 0.439 & 0.848 & 0.064 & 0.023 & 4.75K & 149K & 6696 \\   & FlexCube & 273 & 0.591 & 0.881 & **0.039** & **0.152** & 10.9K & 21.9K & **56.47** \\    & Ours & **34.6** & **0.685** & **0.892** & 0.094 & 0.113 & **4.19K** & **8.80K** & 1434 \\ 

Table 2: **Quantitative comparison for point cloud and multi-view reconstruction results.** Best results are written in bold.

Figure 8: **Point cloud and multi-view reconstruction results. (a)**: Ground truth mesh. (b), (f)**: Our method restores the original shape without losing much detail. (c), (d), (g), (h): PSR (Kazhdan und Hoppe, 2013), VoroMesh (Maruani u. a., 2023), FlexiCube (Shen u. a., 2023), and NIE (Mehta u. a., 2022) fail for open and mixed surfaces. (e): NDC (Chen u. a., 2022b) exhibits artifacts from grids.

In Appendix E, we illustrated the example inputs for these experiments. Also, please see Appendix D to see the initialization and densification strategy we took in these experiments.

To validate our approach, we compare our results with various approaches. When it comes to point cloud reconstruction, we first compare our result with classical Screened Poisson Surface Reconstruction (PSR) method (Kazhdan und Hoppe, 2013) 6. Then, to compare our method with optimization based approach, we use recent VoroMesh (Maruani u. a., 2023) method. Note that these two methods are not tailored for open surfaces. To compare our method also for the open surfaces, we use Neural Dual Contouring (NDC) (Chen u. a., 2022b), even though it is learning-based approach. Finally, for multi-view reconstruction task, we compare our results with Flexicube (Shen u. a., 2023) and Neural Implicit Evolution (NIE) (Mehta u. a., 2022), which correspond to volumetric approaches that can directly produce meshes of varying geometric topology for given visual inputs.

In Figure 8, we visualize the reconstruction results along with the ground truth mesh for qualitative evaluation. For closed meshes, in general, volumetric approaches like PSR, VoroMesh, and Flexicube, capture fine details better than our methods. This is mainly because we currently have limitation in the mesh resolution that we can produce with our method. NIE, which is also based on volumetric principles, generates overly smoothed reconstruction results. However, when it comes to open or mixed mesh models, which are more ubiquitous in real applications, we can observe that these methods fail, usually with false internal structures or self-intersecting faces (Appendix E.2). Since NDC leverages unsigned information, it can handle these cases without much problem as ours. However, we can observe step-like visual artifacts coming from its usage of grid in the final output, which requires post-processing. Additionally, to show the versatility of our representation, we also visualize various shapes reconstructed from oriented point clouds in Figure 2.

Table 2 presents quantitative comparisons with other methods. We used following metrics from Chen u. a. (2022b) to measure reconstruction accuracy: Chamfer Distance (CD), F-Score (F1), Normal Consistency (NC), Edge Chamfer Distance (ECD), and Edge F-Score (EF1) to the ground truth mesh. Also, we report number of vertices and faces of the reconstructed mesh to compare mesh complexity, along with computational time. All values are average over 11 models that we used. In general, our method generates mesh of comparable, or better accuracy than the other methods. However, when it comes to ECD and EF1, which evaluate the edge quality of the reconstructed mesh, our results showed some weaknesses, because our method cannot prevent non-manifold edges yet. However, our method showed superior results in terms of mesh complexity - this is partially due to the use of weight regularization. Please see Appendix E.3 to see how the regularization works through ablation studies. Likewise, our method shows promising results in producing compact and accurate mesh. However, we also note that our method requires more computational cost than the other methods in the current implementation.

Before moving on, we present an experimental result about shape interpolation using DMesh in Figure 9. We used multi-view images to reconstruct a torus first, and then optimized the DMesh again

Figure 9: \(()\)**Shape interpolation using DMesh exhibiting topology change.** After fitting DMesh to a torus (upper left), we optimize it again to reconstruct a double torus (lower right), which has a different genus. We use multi-view images for the optimization.

to fit a double torus. The results show that DMesh effectively reconstructs the double torus, even when initialized from a converged single torus, highlighting the method's robustness to local minima. However, this also indicates that our representation lacks meaningful shape interpolation, as it does not assume any specific shape topology.

## 6 Conclusion

### Limitations

As shown above, our method achieves a more effective and complete forms of differentiable meshes of various topology than existing methods, but still has several limitations to overcome.

**Computational Cost.** Currently, the resolution of DMesh is limited by computational cost. Although our theoretical relaxation and CUDA implementation reduce this burden, processing meshes with over 100K vertices remains challenging due to the computational bottleneck of constructing the WDT at each optimization step. In Figure 10, we analyze computational costs relative to the number of points. As shown, costs rise sharply beyond 20K points, with WDT construction consuming most of the time. This limits our method's ability to handle high resolution mesh.

**Non-Manifoldness.** As we have claimed so far, DMesh shows much better generalization than the other methods as it does not have any constraints on the shape topology and mesh connectivity. However, due to this relaxation of constraint, we can observe spurious non-manifold errors in the mesh, even though we adopted measures to minimize them (Appendix D.2.7).

Specifically, an edge must have at most two adjacent faces to be a "manifold" edge. Similarly, a "manifold" vertex should be adjacent to a set of faces that form a closed or open fan. We refer to edges or vertices that do not satisfy these definitions as "non-manifold." In our results, we found that 5.50% of edges and 0.38% of vertices were non-manifold for point cloud reconstruction. For multi-view reconstruction, 6.62% of edges and 0.25% of vertices were non-manifold. Therefore, we conclude that non-manifold edges are more prevalent than non-manifold vertices in our approach.

### Future Work

To address the computational cost issue, we can explore methods that reduce reliance on the WDT algorithm, as its cost increases significantly with the number of points. This is crucial since representing complex shapes with fine details often requires over 100K vertices. To tackle the non-manifold issue, we could integrate approaches based on (un)signed distance fields (Shen u. a., 2023; Liu u. a., 2023b) into our method, ensuring manifold mesh generation. Finally, future research could extend this work to solve other challenging problems, such as 3D reconstruction from real-world images, or applications like generative models for 3D shapes. This could involve encoding color or texture information within our framework, opening up exciting new directions for exploration.

Figure 10: **Analysis of computational cost for computing face existence probabilities (\((F)\)).** The computational cost rises sharply beyond 20K points, with most of the time spent on WDT construction (“WDT”), while the probability computation (“Prob”) requires significantly less time.

AcknowledgementsWe thank Zhiqin Chen and Matthew Fisher for helpful advice. This research is a joint collaboration between Adobe and University of Maryland at College Park. This work has been supported in part by Adobe, IARPA, UMD-ARL Cooperate Agreement, and Dr. Barry Mersky and Capital One Endowed E-Nnovate Professorships.