# SleeperNets: Universal Backdoor Poisoning Attacks Against Reinforcement Learning Agents

Ethan Rathbun, Christopher Amato, Alina Oprea

Khoury College of Computer Sciences, Northeastern University

Primary Contact Author - rathbun.e@northeastern.edu ; 'Equal Advising

###### Abstract

Reinforcement learning (RL) is an actively growing field that is seeing increased usage in real-world, safety-critical applications - making it paramount to ensure the robustness of RL algorithms against adversarial attacks. In this work we explore a particularly stealthy form of training-time attacks against RL - backdoor poisoning. Here the adversary intercepts the training of an RL agent with the goal of reliably inducing a particular action when the agent observes a pre-determined trigger at inference time. We uncover theoretical limitations of prior work by proving their inability to generalize across domains and MDPs. Motivated by this, we formulate a novel poisoning attack framework which interlinks the adversary's objectives with those of finding an optimal policy - guaranteeing attack success in the limit. Using insights from our theoretical analysis we develop "SleeperNets" as a universal backdoor attack which exploits a newly proposed threat model and leverages dynamic reward poisoning techniques. We evaluate our attack in 6 environments spanning multiple domains and demonstrate significant improvements in attack success over existing methods, while preserving benign episodic return.

## 1 Introduction

Interest in Reinforcement Learning (RL) methods has grown exponentially, in part sparked by the development of powerful Deep Reinforcement Learning (DRL) algorithms such as Deep Q-Networks (DQN)  and Proximal Policy Optimization (PPO) . With this effort comes large scale adoption of RL methods in safety and security critical settings, including fine-tuning Large Language Models , operating self-driving vehicles , organizing robots in distribution warehouses , managing stock trading profiles , and even coordinating space traffic .

These RL agents, both physical and virtual, are given massive amounts of responsibility by the developers and researchers allowing them to interface with the real world and real people. Poorly trained or otherwise compromised agents can cause significant damage to those around them , thus it is crucial to ensure the robustness of RL algorithms against all forms of adversarial attacks, both at training time and deployment time.

Backdoor attacks  are particularly powerful and difficult to detect training-time attacks. In these attacks the adversary manipulates the training of an RL agent on a given Markov Decision Process (MDP) with the goal of solving two competing objectives. The first is high _attack success_, i.e., the adversary must be able to reliably induce the agent to take a target action \(a^{+}\) whenever they observe the fixed trigger pattern \(\) at inference time - irrespective of consequence. At the same time, the adversary wishes to maintain _attack stealth_ by allowing the agent to learn a near-optimal policy on the training task - giving the illusion that the agent was trained properly.

Prior works on backdoor attacks in RL  use static reward poisoning techniques with inner-loop threat models. Here the trigger is embedded into the agent's state-observation and their reward is altered to a fixed value of \( c\)_during_ each episode. These attacks have proven effective inAtari domains , but lack any formal analysis or justification. In fact, we find that static reward poisoning attacks, by failing to adapt to different states, MDPs, or algorithms, often do not achieve both adversarial objectives. We further find that the inner loop threat model - by forcing the adversary to make decisions on a per-time-step basis - greatly limits the amount of information the adversary has access to. These limitations indicate the need for a more dynamic reward poisoning strategy which exploits a more global, yet equally accessible, view of the agent's training environment.

Thus, in this work, we take a principled approach towards answering the fundamental question: "What poisoning methods are necessary for successful RL backdoor attacks?". Through this lens we provide multiple contributions:

1. The first formal analysis of static reward poisoning attacks, highlighting their weaknesses.
2. A new "outer-loop" threat model in which the adversary manipulates the agent's rewards and state-observations _after_ each episode - allowing for better-informed poisoning attacks.
3. A novel framework - utilizing dynamic reward poisoning - for developing RL backdoor attacks with provable guarantees of attack success and stealth in the limit.
4. A novel backdoor attack, SleeperNets, based on insights from our theoretical guarantees that achieves universal backdoor attack success and stealth.
5. Comprehensive analysis of our SleeperNets attack in multiple environments including robotic navigation, video game playing, self driving, and stock trading tasks. Our method displays significant improvements in attack success rate and episodic return over the current state-of-the-art at extremely low poisoning rates (less than \(0.05\%\)).

## 2 Adversarial Attacks in DRL - Related Work and Background

Two main threat vectors have been explored for adversarial attacks against RL agents: training time poisoning attacks and inference time evasion attacks. In this work we focus primarily on backdoor poisoning attacks. Thus, in this section we provide a brief overview of the existing backdoor attack literature - highlighting their contributions. Background on evasion attacks and policy replacement poisoning attacks is provided in Appendix 9.9.

Backdoor attacks target agents trying to optimize a particular Markov Decision Process (MDP). These MDPs are specified as the tuple \((S,A,R,T,)\) where \(S\) is the state space, \(A\) is the set of possible actions, \(R:S A S\) is the reward function, \(T:S A S\) represents the transition probabilities between states given actions, and \(\) is the discount factor. Generally, backdoor attacks are designed to target agents trained with state-of-the-art DRL algorithms such as PPO , A3C , or DQN .

TrojDRL was one of the first works to explore backdoor attacks in DRL - poisoning agents trained using A3C  to play Atari games. They utilize a static reward poisoning technique in which the agent receives \(+1\) reward if they follow the target action in poisoned states, and \(-1\) otherwise (Eq 3). BadRL  then built upon these initial findings by using a pre-trained Q-network to determine in which states the target action \(a^{+}\) is most harmful. This comes at the cost of a much stronger threat model - the adversary needs full access to the victim's benign MDP to train their Q-network,

Figure 1: Comparison of the inner and outer-loop threat models. In an outer-loop attack the adversary can utilize information about completed episodes when determining their poisoning strategy. This information is not accessible in an inner-loop attack.

which is unlikely when the victim party is training on a proprietary MDP. Both TrojDRL and BadRL also explore attacks which are capable of directly manipulating the agent's actions during training. Compared to attacks which only perturb state observations and rewards, these attacks are easier to detect, result in more conservative policies, and require a stronger threat model. We additionally find that such adversarial capability is unnecessary for a successful attack - both attack success and attack stealth can be achieved with reward and state manipulation alone as explained in Section 4.

Other works have explored alternate threat models for RL backdoor attacks.  targets agents trained in cooperative settings to induce an agent to move towards a particular state in the state space.  poisons the agent for multiple time steps in a row to try and enforce backdoors that work over longer time horizons.  targets RNN architectures to induce backdoors which still work after the trigger has disappeared from the observation. Lastly,  utilizes total control over the training procedure and imitation learning to induce a fixed policy in the agent upon observing the trigger. In addition to backdoor attacks, adversarial cheap talk  has been proposed as an alternative, training time attack which poisons the agent through a "cheap talk" channel external to the agent's latent observations in the environment.

Parallel to the study of backdoor attacks, other works have studied the test time detection  and stealth  of adversarial attacks through an information theoretic lens. While objectives of test time detectability are outside the scope of this paper, we believe studying the test time detectability backdoor attacks is an interesting and important research direction for future works. We additionally note that the framework we present in this work makes no assumptions about the adversary's test time objectives, allowing for easy adaptations of our proposed methodology to this setting.

## 3 Problem Formulation

In our setting we consider two parties: the victim and the adversary. The victim party aims to train an agent to solve a benign MDP \(M=(S,A,T,R,)\). Here we define \(S\) to be a subset of a larger space \(\), for instance \(S^{n m}\) is a subset of possible \(n m\) images. The victim agent implements a stochastic learning algorithm, \(L(M)\), taking MDP \(M\) as input and returning some potentially stochastic policy \(:S A\).

The adversary, in contrast, wants to induce the agent to associate some adversarial trigger pattern \(\) with a target action \(a^{+}\), and choose this action with high probability when they observe \(\). We refer to this adversarial objective as _attack success_, similarly to prior work objectives .

In addition to maximizing attack success, the adversary must also minimize the detection probability during both training and execution. There are multiple ways to define this _attack stealth_ objective, but we choose an objective which draws from existing literature  - the poisoned agent should perform just as well as a benignly trained agent in the unpoisoned MDP \(M\). Specifically, the adversary poisons the training algorithm \(L\) such that it instead trains the agent to solve an adversarially constructed MDP \(M^{}\) with the following optimization problems:

**Attack Success:**: \[_{M^{}}[_{s S,^{+}}[^{+}((s),a^{+})]]\] (1)
**Attack Stealth:**: \[_{M^{}}[_{^{+},,s S}[|V^{M}_{^{+}}(s)-V^{M}_ {}(s)|]]\] (2)

where \(a^{+} A\) is the adversarial target action and \(:\) is the "trigger function" - which takes in a benign state \(s S\) and returns a perturbed version of that state with the trigger pattern embedded into it. For simplicity, we refer to the set of states within which the trigger has been embedded as "poisoned states" or \(S_{p}(S)\). Additionally, each \(^{+} L(M^{})\) is a policy trained on the adversarial MDP \(M^{}\), and each \( L(M)\) is a policy trained on the benign MDP \(M\). Lastly, \(V^{M}_{}(s)\) is the value function which measures the value of policy \(\) at state \(s\) in the benign MDP \(M\).

### Threat Model

We introduce a novel, outer-loop threat model (Figure 1) which targets the outer, policy evaluation loop of DRL algorithms such as PPO and DQN. In contrast, prior works rely on inner-loop threat models  which target the inner environment interaction loop of DRL algorithms. In both attacks the adversary is constrained by a _poisoning budget_ parameter \(\) that specifies the fraction of state observations they can poison in training. Additionally, the adversary is assumed to either have access to the agent's state observations, rewards, and actions at _each time step_ - for inner loop attacks - or have access to trajectories generated after each episode is finished for outer loop attacks. Therefore,for inner loop attacks the adversary must have direct access to both the agent's environment and training system (server, desktop, etc.). In the case of outer loop attacks against online RL, which is the subject of this paper, the adversary must have direct control over the victim's training process to be able to poison the agent's reward in certain states. While the assumption of a server compromise appears to be strong, such breaches are unfortunately common with 5,175 found in 2024 by the most recent Verizon Data Breach Investigation Report (DBIR) . The outer loop threat model can also be easily extended to offline RL , which would require weaker assumptions for the adversary as they must only manipulate a static dataset. We leave evaluations against offline RL algorithms to future work.

When implementing an outer-loop attack, the adversary first allows the agent to complete a full episode, generating some trajectory \(H=\{(s,a,r)_{t}\}_{t=1}^{}\) of size \(\) from \(M\) given the agent's current policy \(\). The adversary can then observe \(H\) and use this information to decide which subset \(H^{} H\) of the trajectory to poison and determine how to poison the agent's reward. The poisoned trajectory is then either placed in the agent's replay buffer \(\) - as in algorithms like DQN - or directly used in the agent's policy evaluation - as in algorithms like PPO.

In contrast, when implementing an inner-loop attack, the adversary observes the agent's current state \(s_{t}\)_before_ the agent at each time step \(t\). If they choose to poison at step \(t\) they can first apply the trigger \(s_{t}(s_{t})\), observe the subsequent action \(a_{t}\) taken by the agent, and then alter their reward accordingly \(r_{t} c\). Optionally, the adversary can also alter the agent's action before it is executed in the environment, as used in stronger versions of BadRL  and TrojDRL . The main drawback of this threat model is that the adversary must determine when and how to poison the current time step \((s,a,r)_{t}\) immediately upon observing \(s_{t}\), while, in the outer-loop threat model, they are given a more global perspective by observing the complete trajectory \(H\).

Thus, the outer-loop threat model requires the same level of adversarial access to the training process as the inner-loop threat model, but it enables more powerful attacks. This allows us to achieve higher attack success rates - while modifying _only_ states and rewards - than inner-loop attacks, which may rely on action manipulation for high attack success , . In Table 1 we compare the threat model of our SleeperNets attack with the existing literature.

## 4 Theoretical Results

In this section we provide theoretical results proving the limitations of static reward poisoning and the capabilities of dynamic reward poisoning. We first design two MDPs which will provably prevent static reward poisoning attacks, causing the attack to either fail in terms of attack success or attack stealth. Motivated by these findings, we develop a novel adversarial MDP which leverages dynamic reward poisoning to achieve stronger guarantees for attack success.

### Insufficiency of Static Reward Poisoning

Existing backdoor attack techniques  use a form of static reward poisoning. Formally, we say a reward poisoning technique is static if the agent's reward, at any poisoned time step, is altered to some pre-determined, fixed values:

\[R((s),a)=_{c}[a=a^{+}]\{c& a=a^{+}\\ -c&.\] (3)

We formally prove that this technique of reward poisoning is insufficient for achieving both attack success and attack stealth, irrespective of how large \(c\) or \(\) are. In Figure 2 we present two counterexample MDPs which highlight two key weaknesses of static reward poisoning. In \(M_{1}\) we exploit

   \\   &  & Objective \\   &  &  &  &  &  &  &  &  &  &  \\    & & & & & & & & & & & \(\) \\   &  &  &  &  &  &  &  &  &  \\  & BadRL. & & & & & & & & & & \(\) \\   &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & \(\) \\  

Table 1: Tabular comparison of our proposed threat model to the existing backdoor literature. Filled circles denote features of the attacks and levels of adversarial access which are necessary for implementation. Partially filled circles denote optional levels of adversarial access.

the inability of static reward poisoning to scale with respect to the discount factor \(\). The return the agent receives for taking action \(a_{1}\) in the poisoned state \(()\) surpasses \(2c\) if \(>.5\) - making the target action \(a^{+}\) sub-optimal given the static return of \(+c\). In \(M_{2}\) we then exploit the inability of static reward poisoning to vary across states. In the benign version of \(M_{2}\) it is optimal for the agent to take the shorter "Fast" path to the finish. However, since the agent receives an extra reward of \(+c\) in poisoned states, the longer "Slow" path becomes optimal under static reward poisoning. Fully worked out proofs for these claims are given in Appendix 9.2.

### Base Assumptions for Dynamic Reward Poisoning

In our attack formulation we make two additional assumptions needed for our theoretical results, but malleable for our empirical results. First, we assume that the attacker implements purely out of distribution triggers, more formally: \(S_{p} S=\). Fulfilling this assumption in our theoretical results prevents conflicts between the adversarially induced reward function and the benign MDP's reward function. In practice, attacks can still be successful if this assumption is relaxed, as long as the poisoned states \(s_{p} S_{p}\) are sufficiently rare in the benign MDP during training.

One additional assumption is that \(\) must be an invertible function between \(S\) and \(S_{p}\). While this seems like a strong assumption at first glance, it does not hinder attack performance in practice. In the rest of the paper we frequently use \(^{-1}(s_{p})\) as a shorthand to denote the benign state \(s\) from which we received \(s_{p}=(s)\). In practice, we are given the benign state \(s\) first, and then apply the trigger, thus we never need to actually compute the inverse of \(\).

### Dynamic Reward Poisoning Attack Formulation

As previously discussed, the adversary's goal is to design and enforce an adversarial MDP, \(M^{}\), which optimizes Equations (1) and (2). Our attack influences the agent's behavior through the learning algorithm \(L\), which approximates an optimal policy for the agent in a given MDP. We leverage this fact by defining an adversarial MDP such that the optimal policy solves both our attack success and attack stealth objectives:

\[M^{}(S S_{p},A,T^{},R^{},)\] (4)

In \(M^{}\), \(T^{}\) is an adversarially induced transition function and \(R^{}\) is an adversarially induced reward function. The transition dynamics of this MDP are fairly intuitive - at any given time step the agent can be in a poisoned state \(s_{p} S_{p}\) with probability \(\), or a benign state \(s\) with probability \((1-)\) for poisoning rate \([0,1)\). Transitions otherwise follow the same dynamics of the benign MDP \(M\). Formally, we define \(T^{}\) as:

\[T^{}:(S S_{p}) A(S S_{p})\] (5)

\[T^{}(s,a,s^{})\{(1-) T(s, a,s^{})&s S,\ s^{} S\\  T(s,a,^{-1}(s^{}))&s S,\ s^{} S_{p} \\  T(^{-1}(s),a,^{-1}(s^{}))&s S_{p},\ s^{ } S_{p}\\ (1-) T(^{-1}(s),a,s^{})&s S_{p},\ s^{ } S.\] (6)

The adversarial reward \(R^{}\), on the other hand, dynamically leverages the current policy's value at each state \(s S S_{p}\) to achieve our two-fold adversarial objective. First, \(R^{}\) is designed such that the agent's actions in poisoned states do not impact its return in benign states - allowing an optimal policy in \(M^{}\) to also be an optimal policy in \(M\). Second, \(R^{}\) is formulated such that the agent's return in poisoned states are agnostic to future consequences - guaranteeing that the target action is optimal in any poisoned state. Formally we define \(R^{}\) as:

\[R^{}:(S S_{p}) A(S S_{p})\] (7)

Figure 2: (Left) MDP \(M_{1}\) for which static reward poisoning fails to induce the target action \(a^{+}\). (Right) MDP \(M_{2}\) for which static reward poisoning causes the agent to learn a sub-optimal policy.

\[R^{}(s,a,s^{},)\{R(s,a,s^{})& s S,\ s^{} S\\ R(s,a,^{-1}(s^{}))- V_{}^{M^{}}(s^{})+ V _{}^{M^{}}(^{-1}(s^{}))&s S,s^{} S _{p}\\ 1[a=a^{+}]- V_{}^{M^{}}(s^{})&s S_{p}.\] (8)

where \(\{:(:S S_{p} A)\}\) is the set of all valid policies over \(M^{}\), \(R\) is the benign MDP's reward function, and \(a^{+}\) is the target action. \(R^{}\) is a function of not only the current state \(s\), action \(a\), and next state \(s^{}\), but also of the agent's current policy \(\). In our theoretical developments this gives us access to the policy's value at each state, allowing us to directly cancel out terms in the Bellman optimally equation and thus achieve our desired guarantees. In practice however, we do not have direct access to policy's value in each state. In Section 5 we show how the outer-loop threat model can be used to find an approximation of \(V_{}(s)\), which leverages our theoretical findings.

### Theoretical Guarantees of Dynamic Reward Poisoning

The capabilities of dynamic reward poisoning become most evident through its theoretical properties. We first prove two strong lemmas which we then leverage to prove that \(M^{}\) optimizes both our objective of attack success in Equation (1) and attack stealth in Equation (2).

**Lemma 1**: \(V_{}^{M^{}}(s_{p})=(s_{p},a^{+})\  s_{p} S_{p},\)_. Thus, the value of a policy \(\) in poisoned states \(s_{p} S_{p}\) is equal to the probability with which it chooses action \(a^{+}\)._

**Lemma 2**: \(V_{}^{M}(s)=V_{}^{M^{}}(s)\  s S,\)_. Therefore, the value of any policy \(\) in the adversarial MDP \(M^{}\) is equal to its value in the benign MDP \(M\) for all benign states \(s S\)._

**Theorem 1**: \(V_{^{*}}^{M^{}}(s_{p}) V_{}^{M^{}}(s_{p})\  s_{p} S_{p}, ^{*}(s_{p},a^{+})=1\  s_{p} S_{p}\)_. Thus, \(^{*}\) is optimal in \(M^{}\) if and only if \(^{*}\) takes action \(a^{+}\) with probability \(1\) in poisoned states \(s_{p} S_{p}\)._

**Theorem 2**: \(V_{^{*}}^{M^{}}(s) V_{}^{M^{}}(s)\  s S,  V_{^{*}}^{M}(s) V_{}^{M}(s)\  s S,\)_. Therefore, \(^{*}\) is optimal in \(M^{}\) for all benign states \(s S\) if and only if \(^{*}\) is optimal in \(M\)._

Furthermore, we know that the victim party's training algorithm \(L\) is designed to maximize overall return in the MDP it is solving. Thus, in order to maximize this return, and as a result of Theorem 1 and Theorem 2, the algorithm must produce a poisoned policy which optimizes our objectives of attack success and attack stealth. From this we can conclude that \(M^{}\) optimizes Equations (1) and (2). Rigorous proofs of all our lemmas and theorems can be found in Appendix 9.1.

## 5 Attack Algorithm

The goal of the SleeperNets attack is to replicate our adversarial MDP \(M^{}\) as outlined in the previous section, thus allowing us to leverage our theoretical results. Since we do not have direct access to \(V_{}^{M^{}}(s)\) for any \(s S S_{p}\), we must find a way of approximating our adversarial reward function \(R^{}\). To this end we make use of our aforementioned outer-loop threat model - the adversary alters the agent's state-observations and rewards after a trajectory is generated by the agent in \(M\), but before any policy update occurs. The adversary can then observe the completed trajectory and use this information to form a better approximation of \(V_{}^{M^{}}(s)\) for any \(s S S_{p}\).

```
1:Policy \(\), Replay Memory \(\), max episodes \(N\)
2:poisoning budget \(\), weighting factor \(\), reward constant \(c\), trigger function \(\), policy update algorithm \(L\), benign MDP \(M=(S,A,R,T,)\)
3:for\(i 1,N\)do
4: Sample trajectory \(H=\{(s,a,r_{t})_{t}\}_{t=1}^{}\) of size \(\) from \(M\) given policy \(\)
5: Sample \(H^{} H\) uniformly randomly s.t. \(|H^{}|=|H\)
6:for all\((s,a,r)_{t} H^{}\)do
7: Compute value estimates \((s_{t},H)\), \((s_{t+1},H)\) using known trajectory \(H\)
8:\(s_{t}(s_{t})\)
9:\(r_{t}_{c}[a_{t}=a^{+}]-(s_{t+1},H)\)
10:\(r_{t-1} r_{t-1}- r_{t}+(s_{t},H)\)
11: Store \(H\) in Replay Memory \(\)
12: Update \(\) According to \(L\) given \(\), flush or prune \(\) according to \(L\) ```

**Algorithm 1** The SleeperNets AttackSpecifically, the learning algorithm \(L\) first samples a trajectory \(H=\{(s,a,r)_{t}\}_{t=1}^{}\) of size \(\) from \(M\) given the agent's current policy \(\). The adversary then samples a random subset \(H^{} H\) of size \(|H|\) from the trajectory to poison. Given each \((s,a,r)_{t} H^{}\) the adversary first applies the trigger pattern \((s_{t})\) to the agent's state-observation, and then computes a Monte-Carlo estimate of the value of \(s_{t}\) and \(s_{t+1}\) as follows:

\[(s_{t},H)_{i=t}^{|H|}^{i-t}r_{t}\] (9)

This is an unbiased estimator of \(V_{}^{M^{}}(s_{t})\) which sees usage in methods like PPO and A2C - thus allowing us to accurately approximate \(V_{}^{M^{}}(s)\) for any \(s S S_{p}\) in expectation. We then use these estimates to replicate our adversarial reward function \(R^{}\) as seen in lines 7 and 8 of Algorithm 1.

We additionally introduce two hyper parameters, \(c\) and \(\), as a relaxation of \(R^{}\). Larger values of \(\) perturb the agent's reward to a larger degree - making the attack stronger, but perhaps making generalization more difficult in DRL methods. On the other hand, smaller values of \(\) perturb the agent's reward less - making it easier for DRL methods to generalize, but perhaps weakening the attack's strength. Similarly, \(c\) scales the first term of \(R^{}\) in poisoned states, \(_{c}[a=a^{}]\), meaning larger values of \(c\) will perturb the agent's reward to a greater degree. In practice we find the SleeperNets attack to be very stable w.r.t. \(\) and \(c\) on most MDPs, however some settings require more hyper parameter tuning to maximize attack success and stealth.

## 6 Experimental Results

In this section we evaluate our SleeperNets attack in terms of our two adversarial objectives - attack success and attack stealth - on environments spanning robotic navigation, video game playing, self driving, and stock trading tasks. We further compare our SleeperNets attack against attacks from BadRL  and TrojDRL  - displaying significant improvements in attack success while more reliably maintaining attack stealth.

### Experimental Setup

We compare SleeperNets against two versions of BadRL and TrojDRL which better reflect the less invasive threat model of the SleeperNets attack. First is TrojDRL-W in which the adversary _does not_ manipulate the agent's actions. Second is BadRL-M which uses a manually crafted trigger pattern rather than one optimized with gradient based techniques. These two versions were chosen so we can directly compare each method's reward poisoning technique and better contrast the inner and outer loop threat models. Further discussion on this topic can be found in Appendix 9.3.

We evaluate each method on a suite of 6 diverse environments against agents trained using the cleanrl  implementation of PPO . First, to replicate and validate the results of  and  we test all attacks on Atari _Breakout_ and _Qbert_ from the Atari gymnasium suite . In our evaluation we found that these environments are highly susceptible to backdoor poisoning attacks, thus we extend and focus our study towards the following 4 environments: _Car Racing_ from the Box2D gymnasium , _Safety Car_ from Safety Gymnasium , _Highway Merge_ from Highway Env , and _Trading BTC_ from Gym Trading Env .

In each environment we first train an agent on the benign, unpoisoned MDP until convergence to act as a baseline. We then evaluate each attack on two key metrics - episodic return and attack success rate (ASR). Episodic return is measured as the agent's cumulative, discounted return as it is training. This metric will be directly relayed to the victim training party, and thus successful attacks will have training curves which match its respective, unpoisoned training curve. Attack success rate is measured by first generating a benign episode, applying the trigger to every state-observation, and then calculating the probability with which the agent chooses the target action \(a^{+}\). Successful attacks will attain ASR values near 100%. All attacks are tested over 2 values of \(c\) and averaged over 3 seeds given a fixed poisoning budget. \(c\) values and poisoning budgets vary per environment. In each plot and table we then present best results from each attack. Further experimental details and ablations can be found in Appendix 9.4, Appendix 9.5, Appendix 9.6, and Appendix 9.7.

### SleeperNets Results

In Table 2 we highlight the universal capabilities of the SleeperNets attack across 6 environments spanning 4 different domains. Our attack is able to achieve an average attack success rate of **100%**over 3 different seeds on _all_ environments - highlighting the attack's reliability. Additionally, our attack causes little to no decrease in benign episodic return - producing policies that perform at least 96.5% as well as an agent trained with _no poisoning_. Lastly, since our attack regularly achieves 100% ASR during training, we are able to anneal our poisoning rate over time - resulting in extremely low poisoning rates like 0.001% on Breakout and 0.0006% on Trade BTC. In practice we anneal poisoning rates for all three methods by only poisoning when the current ASR is less than 100%. More detailed discussion on annealing can be found in Appendix 9.6.

We find that the SleeperNets attack outperforms TrojDRL-W and BadRL-M on all 6 environments in terms of attack success rate, being the _only_ attack to achieve 100% attack success on _all_ environments. The SleeperNets attack additionally outperforms TrojDRL-W in terms of episodic return on _all_ environments. BadRL-M performs slightly better in terms of episodic return in Highway Merge and Car Racing, but in these cases it fails to achieve high attack success - resulting in 0.2% and 44.1% ASR in contrast to SleeperNets' 100% ASR on both environments.

In Figure 3 we plot the attack success rate and episodic return of the SleeperNets, BadRL-M, and TrojDRL-W attacks on Highway Merge and Safety Car. In both cases the SleeperNets attack is able to quickly achieve near 100% ASR while maintaining an indistinguishable episodic return curve compared to the "No Poisoning" agent. In contrast BadRL-M and TrojDRL-W both stagnate in terms

   \\  Environment & **Highway Merge** & **Safety Car** & **Trade BTC** & **Car Racing** & **Breakout** & **Qbert** \\  Metric & ASR & \(\) & ASR & \(\) & ASR & \(\) & ASR & \(\) & ASR & \(\) & ASR & \(\) \\ 
**SleepNets** & **100\%** & 0\% & **100\%** & 0\% & **100\%** & 0\% & **100\%** & 0\% & **100\%** & 0\% & **100\%** & 0\% \\ TrojDRL-W & 97.2\% & 29.9\% & 86.7\% & 33.3\% & 97.5\% & 73.6\% & 46.7\% & 99.8\% & 0.1\% & 98.4\% & 0.5\% \\ BadRL-M & 0.2\% & 0.1\% & 70.6\% & 17.4\% & 38.8\% & 41.5\% & 44.1\% & 47.0\% & 99.3\% & 0.6\% & 47.2\% & 22.2\% \\   Metric & 80k & \(\) & BRR & \(\) & BRR & \(\) & BRR & \(\) & BRR & \(\) & BRR & \(\) \\ 
**SleepNets** & 99.0\% & 0.4\% & **95.5\%** & 25.4\% & **100\%** & 18.5\% & 97.0\% & 6.3\% & 100\% & 24.1\% & **100\%** & 12.9\% \\ TrojDRL-W & 91.4\% & 3.6\% & 81.3\% & 30.9\% & 100\% & 13.4\% & 26.6\% & 26.1\% & 79.7\% & 12.2\% & 100\% & 10.2\% \\ BadRL-M & **100\%** & 0.0\% & 83.6\% & 38.8\% & 100\% & 11.5\% & **98.1\%** & 9.9\% & 84.4\% & 6.7\% & 100\% & 12.6\% \\  

Table 2: Comparison between BadRL-M, TrojDRL-W, and SleeperNets. Here the Benign Return Ratio (BRR) is measured as each agent’s episodic return divided by the episodic return of an unpoisoned agent, and ASR is each attack’s success rate. All results are rounded to the nearest tenth and capped at 100%. Standard deviation \(\) for all results is provided in their neighboring column.

Figure 3: Comparison of the SleeperNets, BadRL-M, and TrojDRL-W attacks on (Top) Highway Merge and (Bottom) Safety Car in terms of (Left) ASR and (Right) episodic return.

of ASR - being unable to achieve above 60% on Highway Merge or above 90% on Safety Car. Full numerical results and ablations with respect to \(c\) can be found in Appendix 9.5.

### Attack Parameter Ablations

In Figure 4 we provide an ablation of the SleeperNets, BadRL-M, and TrojDRL-W attacks with respect to poisoning budget \(\) and reward poisoning constant \(c\) on the Highway Merge environment. For both experiments we use \(=0\) for SleeperNets, meaning the magnitude of reward perturbation is the same between all three attacks. This highlights the capabilities of the outer-loop threat model in enabling better attack performance with weaker attack parameters.

When comparing each attack at different poisoning budgets we see that SleeperNets is the only attack capable of achieving >95% ASR with a poisoning budget of 0.25% and is the only attack able to achieve an ASR of 100% given any poisoning budget. TrojDRL-W is the only of the other two attacks able to achieve an ASR >90%, but this comes at the cost of a 11% drop in episodic return at a poisoning budget of 2.5% - 10x the poisoning budget needed for SleeperNets to be successful.

We see a similar trend for different values of \(c\) - SleeperNets is the only attack which is able to achieve an ASR near 100% given a poisoning budget of 0.5% and \(c=20\). TrojDRL-W, in contrast, only achieves an ASR of 57% given the same poisoning budget and much larger reward constant of \(c=40\). This again comes at the cost of a 10% drop in episodic return. Between both ablations BadRL-M is unable to achieve an ASR greater than 40% and only achieves an ASR better than 1% when given a 5% poisoning budget and using \(c=40\). Numerical results can be found in Appendix 9.7.

## 7 Conclusion and Limitations

In this paper we provided multiple key contributions to research in backdoor attacks against DRL. We have proven the insufficiency of static reward poisoning attacks and use this insight to motivate the development of a dynamic reward poisoning strategy with formal guarantees. We additionally introduced a novel backdoor threat model against DRL algorithms, and formulated the SleeperNets attack which significantly and reliably outperforms the current state-of-the-art in terms of attack success rate and episodic return at very low poisoning rates. The strong theoretical and empirical results of this work motivate further research into defense techniques against backdoor poisoning attacks in DRL such as new detection, certified robustness, or auditing approaches.

Figure 4: (Top) Ablation with respect to poisoning budget \(\) for each attack given a fixed \(c=40\). (Bottom) Ablation with respect to \(c\) given a fixed poisoning budget of \(0.5\%\). Both experiments were run on Highway Merge with a value of \(=0\) for SleeperNets.

The main limitation of SleeperNets is that the formulation of \(R^{}\) allows the attack's reward perturbation to grow arbitrarily large given an arbitrary MDP. When \(\) is large this may make detection of the attack easier if the victim is able to scan for outliers in the agent's reward. We believe this is an interesting area of future research that may lead to the development of increasingly sensitive detection techniques and, in response, more stealthy attacks. Additionally, while our experiments are expansive with respect to environments and most attack parameters, this paper is not an exhaustive study of all facets of backdoor attacks. We do not study the effectiveness of the adversary's chosen trigger pattern nor do we consider triggers optimized with gradient or random search based techniques . Combining these techniques with SleeperNets is also an interesting area of future research which will likely result in a further decrease in poisoning budget and the ability to perform stealthier attacks.

## 8 Broader Impacts

In this paper we expose a key vulnerability of Deep Reinforcement Learning algorithms against backdoor attacks. As with any work studying adversarial attacks, there is the possibility that SleeperNets is used to launch a real-world, malicious attack against a DRL system. We hope that, by exploring and defining this threat vector, developers and researchers will be better equipped to design counter measures and prevent any negative outcomes. In particular we believe the implementation of isolated training systems - which will be harder for the adversary to access - and the development of active attack detection techniques will both be critical for mitigating attacks. We would also like to highlight the potential of malicious trainers using these backdoor attack approaches against their own agents for malicious purposes. We believe the development of new DRL auditing techniques is necessary to identify these compromised agents.