# A scalable generative model for dynamical system reconstruction from neuroimaging data

Eric Volkmann\({}^{1,2,*}\), Alena Brandle\({}^{1,3,4,*}\), Daniel Durstewitz\({}^{1,3,4,}\), Georgia Koppe\({}^{3,5,6,}\)

\({}^{1}\)Department of Theoretical Neuroscience, Central Institute of Mental Health (CIMH),

Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany

\({}^{2}\)Institute for Machine Learning, Johannes Kepler University, Linz, Austria

\({}^{3}\)Interdisciplinary Center for Scientific Computing, Heidelberg University, Heidelberg, Germany

\({}^{4}\)Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany

\({}^{5}\)Hector Institute for AI in Psychiatry & Dept. for Psychiatry and Psychotherapy, CIMH

\({}^{6}\)Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany

\({}^{*,}\) These authors contributed equally

###### Abstract

Data-driven inference of the generative dynamics underlying a set of observed time series is of growing interest in machine learning and the natural sciences. In neuroscience, such methods promise to alleviate the need to handcraft models based on biophysical principles and allow to automatize the inference of inter-individual differences in brain dynamics. Recent breakthroughs in training techniques for state space models (SSMs) specifically geared toward dynamical systems (DS) reconstruction (DSR) enable to recover the underlying system including its geometrical (attractor) and long-term statistical invariants from even short time series. These techniques are based on control-theoretic ideas, like modern variants of teacher forcing (TF), to ensure stable loss gradient propagation while training. However, as it currently stands, these techniques are not directly applicable to data modalities where current observations depend on an entire history of previous states due to a signal's filtering properties, as common in neuroscience (and physiology more generally). Prominent examples are the blood oxygenation level dependent (BOLD) signal in functional magnetic resonance imaging (fMRI) or Ca\({}^{2+}\) imaging data. Such types of signals render the SSM's decoder model non-invertible, a requirement for previous TF-based methods. Here, exploiting the recent success of control techniques for training SSMs, we propose a novel algorithm that solves this problem and scales exceptionally well with model dimensionality and filter length. We demonstrate its efficiency in reconstructing dynamical systems, including their state space geometry and long-term temporal properties, from just short BOLD time series.

## 1 Introduction

Models and theories based on dynamical systems (DS) concepts have a long tradition in computational neuroscience in accounting for physiological phenomena and computational processes of the brain . Constructing such models from first principles (biophysics) is time-consuming and hard, and utilizing them to account for inter-individual differences in brain dynamics, when model settings need to be personalized, is even more challenging. Yet, constructing valid models of the brain's functional dynamics is immensely important, not only for understanding the neurocomputational basis of inter-individual differences in cognitive and emotional style , but also when aiming atdiagnosing or predicting brain dysfunction and clinical characteristics based on DS features , or for designing personalized therapies .

### Dynamical models in neuroscience

In computational neuroscience, multiple approaches to infer large-scale brain dynamics have been advanced over the past decades (e.g., [17; 46; 58; 32]). Pioneering work introduced latent linear DS, including Gaussian process latent variable models [64; 81], and extensions like Switching Linear DS (SLDS) . Whereas linear DS models are very limited in their dynamical repertoire, SLDS offer a more flexible approach to modeling a larger range of dynamical phenomena by combining several linear (or affine) DS, with a switching mechanism that selects the active system at each moment [44; 43; 27]. These approaches have become common tools for inferring and visualizing neural trajectories within low-dimensional state spaces.

Arguably the most popular class of generative models in whole brain simulations relies on mean field neural population models, including neural mass [76; 5] and neural field models [35; 5]. These model the moments of the activity of cortical areas by averaging over properties (like firing rates) of neural (sub)populations, and are often biophysically motivated . Many popular large-scale mean field modeling approaches are implemented in The Virtual Brain (TVB) environment [56; 58]. TVB incorporates biologically realistic brain connectivity into neural field models to generate simulations of large-scale brain activity. Alternatively, Dynamic Causal Modeling (DCM) describes a set of more statistically motivated and mostly linear techniques, primarily for the purpose of inferring the effective connectivity between brain regions based on invasive or non-invasive brain recordings (e.g., [16; 37; 46]). While many of these models may account for aspects of the dynamics, like patterns of functional connectivity and their task modulation, they are not, strictly, dynamical systems reconstruction (DSR) tools, entailing that they may miss important dynamical phenomena by being constrained through the biological assumptions and simplifications imposed.

In the analysis of functional magnetic resonance imaging (fMRI), only recently a shift in focus has introduced data-driven, deep-learning based methods for inferring generative models of system dynamics [40; 62; 63]. Models that implement dynamics either directly on the observed , or within an underlying latent [40; 63] space have been proposed, partly using available structural information and hierarchical inference approaches .

### Dynamical systems reconstruction (DSR)

A variety of Deep Learning (DL)-based models for approximating the generative dynamical processes underlying observed time series has been put forward in recent years ([8; 69; 49; 40; 60; 42; 6; 71; 31; 13; 80]; see also  for an overview). These include approaches which approximate a system's vector field, e.g., through a library of basis functions and penalized regression as in SINDy , or through deep neural networks and neural Ordinary Differential Equations (ODE) [14; 57; 39]. Alternatively, methods that approximate the associated flow (solution) operator directly have been suggested, often employing state space model (SSM)-type architectures which distinguish between an observation process and a latent process commonly instantiated through recurrent neural networks (RNNs; e.g., [40; 29; 70; 60; 7]). Recurrent SLDS (rSLDS) , an extension of SLDS, and Latent Factor Analysis via Dynamical Systems (LFADS)  also fall into this category.

In DSR we ask for models that are _generative_ in the sense that - after training - they provide an executable surrogate model of the observed system, from which we can simulate samples that agree with their empirical counterparts in topological, geometrical and temporal characteristics (in contrast to . This required agreement in _long-term_ temporal and geometric properties is not automatically guaranteed for standard training of common RNNs or neural ODEs, which may yield good short-term predictions but may fail to recover the full system dynamics [34; 24; 52]. Recent breakthroughs in data-driven DSR build on insights from the field of chaos control and synchronization [51; 1; 68; 2], by guiding the training process through optimally chosen control signals - modern variations of classical teacher forcing (TF) - that prevent exploding gradients [47; 6; 31; 7; 39].

Chaotic dynamics in particular, as typical for neural systems (e.g., [67; 22; 36; 26]), poses a severe problem here as trajectories and hence loss gradients inevitably diverge due to the presence of a positive Lyapunov exponent . Recent amendments of TF protocols, including sparse TF(STF; ) and generalized TF (GTF; ), keep trajectories and gradients in check by 'weakly synchronizing' them with the observed signals.

### Specific contributions

Nonlinear SSMs distinguish between an underlying latent process that governs the dynamics of a system's state, and an observation process (referred to as _observation_ or _decoder_ model), that links the system states to the actual measurements . Invertible (or pseudo-invertible) decoder models play a crucial role in control-theoretic approaches, like STF or GTF, for training SSMs, in order to project observations into the model's latent space. This inversion is fairly straightforward when ones assumes that the current measurement depends solely on the present latent state, i.e, when \(x_{t}=f(z_{t})\), but not if it depends on a history of states \(\{z_{t},z_{t-1},,z_{t-}\}\). In practice, unfortunately, this assumption is often violated due to a signal's filtering properties. For instance, blood oxygenation level dependent (BOLD) signals, as assessed via fMRI, are only an indirect measurement of neural activity, with the hemodynamic response function (\(hrf\)) broadly smearing out the signal across time. Each measurement is therefore a function of a history of past neural (latent) states whose dynamics we wish to infer [28; 78]. Similar challenges arise in calcium imaging when spike times are to be inferred [79; 72], or, in fact, any other observation process where the actual measurement is a filtering of the process of interest.

Here we rectify this issue by developing a particularly efficient SSM approach which works for measurements that depend on longer histories of latent states, yet allows to take advantage of recent powerful training strategies for DSR [47; 31]. In particular, our contributions are threefold: First, we create and validate a novel SSM-based DSR algorithm for observation models which involve convolutions with latent state series, and demonstrate its scalability with SSM size, as well as convolution filter length. Second, we introduce an evaluation scheme for selecting DSR models on short empirical time series, by demonstrating that the used DSR measures assessed on short time series accurately predict a system's long-term temporal and geometric properties. This is of high practical relevance, as in many empirically relevant scenarios, like fMRI, we only have access to comparatively short time series. Finally, we show that the proposed models can reliably extract key DS features that, moreover, differentiate between subjects.

## 2 Convolution SSM model (convSSM)

### Latent DSR model

We start by defining our generative model used for DSR, a variant of a piecewise linear RNN (PLRNN). Its specific architecture has first been suggested in  and later expanded to increase the PLRNN's expressivity [6; 31]. While the present approach is generic and independent of the specific DSR architecture, here we use the so-called shallow PLRNN (shPLRNN; Appx. A.3) and the clipped shallow PLRNN (cshPLRNN; ). In the cshPLRNN, the temporal evolution of the (latent) system state \(z_{t}^{M}\) is expressed as

\[z_{t}=z_{t-1}+}[(}z_{t-1}+h_{2})- (}z_{t-1})]+h_{1}\] (1)

where \(()=(0,)\) is an elementwise ReLU activation function, \(}^{M L}\), \(}^{L M}\) are connection weights, \(^{M M}\) is a diagonal matrix of autoregressive weights, and \(h_{1}^{M}\) and \(h_{2}^{L}\) are bias vectors. Its trajectories \(\{z_{t}\}^{M T}\) will be bounded if the absolute eigenvalues of \(\) are smaller than \(1\). The Markov property of the latent model is crucial to ensure it formally constitutes a DS with complete state space [50; 65]. Finally, Equation 1 can easily be extended to incorporate the effect of external inputs, such as experimental stimuli, by adding \(s_{t}\) (with \(s_{t}^{K}\) representing an input vector and \(^{M K}\) its effect on the latent dynamics). However, here we consider input-free data from resting state experiments.

### Teacher forcing for invertible decoder models

In , the latent state \(z_{t}\) at each time point is assumed to be related to the actual observation \(x_{t}^{N}\) by a linear (Gaussian) decoder model

\[x_{t}=z_{t}+_{t},\ _{t}(0,),\] (2)referred to simply as'standard SSM' in the following. Here, \(^{N M}\) is a matrix of regression weights, and \(_{t}\) describes Gaussian observation noise with diagonal covariance matrix \(\). A conventional mean squared error (MSE) type loss function \(=_{t}_{t}=_{t}\|_{t}-x_{t}\|_{ 2}^{2}\) between the generated (predicted) \(\{_{t}\}\) and the observed \(\{x_{t}\}\) sequence is commonly used to optimize parameters by stochastic gradient descent (SGD) with GTF . Regularization terms to enforce a structure in latent space that helps to map slowly evolving processes may further be added to this loss .

A fundamental issue in training such systems by SGD is the well-known 'exploding-and-vanishing gradients' problem (EVGP), preventing systems from capturing relevant time scales in the data. In fact,  proved that for chaotic systems gradient-based training techniques for RNNs will _inevitably_ lead to diverging loss gradients (see also ). Successful DSR algorithms need to address this problem. Based on this connection between chaos and diverging gradients, Engelken  suggested regularizing the system's Lyapunov spectrum, thereby also biasing the dynamics toward certain (non-hyperbolic) solutions. A theoretically well founded approach that does not limit a system's dynamical expressivity, which we will adopt here, is GTF, proposed in . GTF is designed to keep model generated trajectories on track and, theoretically, can completely abolish the EVGP without constraining model expressivity. The main idea is that _during training_ the latent state \(_{t}\) is computed as a linear interpolation between the PLRNN generated state \(z_{t}=(_{t-1})\) and a data-inferred state \(d_{t}\) that serves as a control signal , i.e.,

\[_{t}:=(1-) z_{t}+ d_{t},\ [0,1).\] (3)

There is a theoretically optimal choice for \(\) that can be approximated concurrently whilst training through a specifically designed annealing protocol , but more simply \(\) may just be determined by grid search (as done here). Control signals \(d_{t}\) are obtained by inverting the decoder model (Equation 2). Since in general \(M N\), the matrix inverse of \(^{N M}\) does not exist and is approximated by the Moore-Penrose (pseudo-) inverse \(^{+}\):

\[d_{t}=^{+}x_{t}\] (4)

To keep the gradients on track, the interpolation is performed at each time step before applying the cshPLRNN mapping (Equation 1). These control signals are turned off during actual data generation by the model (i.e., in a test phase), where it runs completely autonomously.

### Teacher forcing for decoder models with signal convolution

Decoder model for convolved signalsGTF (and similar techniques like STF; ) are powerful state-of-the-art (SOTA) tools for controlling gradients, especially in the context of DSR. However, they require a (pseudo-)invertible observation model for producing adequate control signals. Empirically, there are many situations where this requirement is not met. For instance, in BOLD time series the observed signal is a highly filtered and strongly smoothed version of the underlying neuronal process that we would like to recover [9; 10; 28]. This fairly complex hemodynamic process is often modeled by the \(hrf\).

A decoder model that relates the neuronal processes given as latent time series \(\{z_{t}\}\) to measured BOLD time series \(\{x_{t}\}\) may be formulated as in ,

\[x_{t}=((hrf*z)_{t})+r_{t}+_{t},\ _{t} N(0,)\] (5)

with regression coefficient matrices \(^{N M}\) and \(^{N P}\), nuisance variables \(r_{t}^{P}\) (such as movement or respiratory artifacts) and a Gaussian observation noise term \(_{t}\) (with usually diagonal covariance \(^{N N}\)). Here, \(*\) denotes the convolution operation and \(z\) is a history of states \(z_{t-:t}\), the length of which depends on the observed sampling rate, commonly referred to as time of repetition (TR). The discrete \(hrf\) sequence is computed by evaluating the canonical \(hrf\) at the observed TR . We will denote the \(hrf\) response for a given TR by \(hrf_{TR}\).

By incorporating the \(hrf\) into the observation model, we disentangle the neural state and its dynamics - the processes of interest - from the neurovascular mechanics (or any filtering at the level of observed signals). We thereby eliminate the history dependence present in the observations, and thus help unfolding trajectories in latent space and satisfying the uniqueness assumptions required in reconstructing dynamical systems  (see also Appx. Figure 7). However, Equation 5 poses a major complication for applying TF techniques, as observations (and model outputs \(_{t}\)) do not simply depend on the current state \(z_{t}\), but - due to the convolution - on a set of states across several previous time steps. We can thus not compute the control signal \(d_{t}\) through straightforward decoder inversion anymore, but require a new type of inversion algorithm.

Wiener deconvolutionFollowing , we use a Wiener filter  to invert Equation 5. We briefly introduce this approach here in the context of our specific problem, and refer to Appx. A.4 for further details. Given an observed noisy signal \(\{x_{t}\}\), composed of the signal of interest \(\{z_{t}\}\) convolved with a known impulse response \(hrf\) plus some noise term \(_{t}\) (distribution unknown, Wiener is optimal for Gaussian distribution),

\[x_{t}=(hrf*z)_{t}+_{t},\] (6)

the Wiener deconvolution provides the estimate \(_{t}\) of the unknown signal \(z_{t}\) through least-MSE estimation. Defining \(^{-1}(,hrf)\) as the Wiener deconvolution operator, we can write

\[\{_{t}\}=^{-1}(\{x_{t}\},hrf).\] (7)

Inversion of BOLD decoder modelUsing the notation introduced above, we can write the inversion to obtain control signals as

\[\{d_{t}\}=^{-1}^{+}(x_{t}-r_{t})}, hrf,\] (8)

where \(^{+}(x_{t}-r_{t})}\) is the time series that needs to be deconvolved. Note that this approach is quite general and we can simply exchange the \(hrf\) with alternative functions if we want to account for filtering in the original signal. As stated, since \(\) and \(\) are matrices of learnable parameters updated during training, we would need to perform this deconvolution step at every training epoch, which is computationally very costly. We therefore make use of the linearity of convolutions and separate the deconvolution step from the learnable parameters, rewriting Equation 8 as

\[\{d_{t}\}=^{+}(^{-1}(\{x_{t}\},hrf)-^{-1}(\{r_{t}\},hrf)).\] (9)

With \(x_{t}^{}}\) and \(r_{t}^{}}\) denoting the respective deconvolved time series, this can be written as

\[d_{t}=^{+}x_{t}^{}- r_{t}^{}.\] (10)

This now is computationally much more efficient, as we need to perform the deconvolution only once prior to training. During training then, only the decoder model parameters need to be inferred to obtain the control signal. We will refer to the convolutional model for DSR (Equation 1 and Equation 5) trained with GTF and SGD as 'convSSM'. The full inversion algorithm is provided in Algorithm 1 with additional information given in Appx. A.6. Key components of SGD+GTF training are illustrated in Figure 1.

## 3 Results

### Performance measures

In DS theory in general, and DSR more specifically, we are mostly concerned with invariant properties of a system, such as attractor geometry and long-term temporal statistics . In chaotic systems in particular, in which trajectories diverge exponentially fast with time, the mean squared prediction error (PE) is a useful statistic only on relatively short time scales [77; 40; 60] (see Appx. Figure 6). To evaluate our model's performance, in addition to short-term \(n\)-step ahead PEs, \(PE_{n}\), we assess the following two established performance measures to capture the temporal and geometrical structure:

1. The deviation in power spectra between the (smoothed) empirical and model-generated power spectra, assessed in terms of the Hellinger distance and referred to as power spectrum error (PSE), \(D_{PSE}\), in the following , and
2. the Kullback Leibler divergence between the empirical and model-generated trajectories across state space, \(D_{stsp}\), measuring the overlap in attractor geometries .

To obtain a reference value for \(D_{stsp}\), we further included two references in which we assessed \(D_{stsp}\) when all mass is centered on the expectation value (similar to a fixed point solution), and when the state space is populated by points drawn from a Gaussian with mean and variance equal to the data (similar to a fixed point solution plus measurement noise).

For comparability with experimental data, we evaluated performance on comparatively short time series obtained from the adaptive linear-nonlinear (ALN) cascade model and the LEMON data set. In these cases, performance metrics were assessed on \(100\) generated trajectories per model and then averaged. For more details, see Appx. A.7.

### convSSM validation & scalability on Lorenz63

As a well-established and popular benchmark for a chaotic system, we first performed numerical experiments with the famous Lorenz63 system. The Lorenz63 is a 3-dimensional system introduced in  to describe atmospheric convection, and exhibits chaotic behaviour in the chosen regime (see Appx. B.1). To mimic BOLD observations, we generated \(100\) standardized chaotic Lorenz63 trajectories, convolved them with \(hrf_{TR}\) functions at different sampling rates \(\{0.2,0.5,1.2\}\) (Figure 2B), and added Gaussian noise with standard deviation \(\{0.01,0.1\}\). This resulted in 6 benchmark settings with different levels of signal degradation by convolution and noise. Each data set was divided into a training and a test set of \(T=5 10^{4}\) time steps each.

We trained \(100\) models on each of these 6 data sets. The following models were compared: the convSSM trained via SGD+GTF, the convSSM trained via SGD and no GTF, the standard SSM trained via SGD+GTF, and MINDy, a recently published method for DSR in fMRI . convSSM and standard SSMs were trained with the shPLRNN, with \(M=3\), \(L=50\), and \(=.1\) (see Appx. Table 4 for all additional hyperparameters). The hidden dimension was selected such that the standard SSM (no-\(hrf\) model) performed well . We emphasize that the standard SSM has already been extensively benchmarked on a variety of simulated and real-world data sets and is considered to be a SOTA model in the field [6; 31]. The performance measures \(D_{stsp}\), \(D_{PSE}\), and \(PE_{20}\) were assessed on the test sets after training for \(1,000\) epochs. We used the same hyperparameters for all networks (aside from TR) to show that performance increases can be solely attributed to the improved decoder model. Hyperparameters were chosen such that the shPLRNN achieved near perfect performance on _unconvolved_, noiseless trajectories from the Lorenz63 system.

The convSSM significantly outperformed all other methods, including the standard SSM in almost all cases, with the performance gap increasing with decreasing TR (see Appx. Table 2 for performance, and Figure 2A for example reconstructions, providing an intuition on how to interpret \(D_{stsp}\)). The more heavily the signal was degraded by the convolution filter, the larger was the performance gap in favor of the convSSM.

An important consideration especially for large-scale applications of such models to empirical data is how well they scale with model size and convolution filter length. To assess this, we collected trajectories of length \(T=10^{5}\) from the chaotic Lorenz63 system, and studied training epoch times as a function of convSSM latent dimension \(M=\{3,10,50,100,500\}\), hidden dimension \(L=\{10,50,100,500,1000\}\), \(=\{0.2,0.5,1.2,3\}\), time series length \(T=\{500,1000,5000,10000,50000,100000\}\) and observation dimension

Figure 1: Schematic of training protocol and gradient flow. A: Before training, observations \(\{x_{t}\}\) and nuisance artifacts \(\{r_{t}\}\) are deconvolved. B: The deconvolved time series are used to generate a forcing signal \(d_{t-1}\) which is used for guiding cshPLRNN training. C: Latent states \(z_{t-:t}\) and nuisance artifacts \(r_{t}\) are used to predict \(_{t}\) through the decoder model. Gradients are computed on the squared error loss \(_{t}\), propagated from the decoder model back to the latent states (blue), and from the latent DS model backwards in time (orange).

\(N=\{10,30,50,100,500,1000\}\). Shorter/longer TRs directly implicate longer/shorter convolution filters since the filters assume a constant time interval. Results are displayed in Appx. Figure 4A. The runtime per epoch did not significantly depend on TR, which means that time series convolved with long impulse response functions can be trained in times comparable to short ones. The per-epoch-runtime increases approximately linearly with dimensions \(L\), \(M\), and \(N\) (Appx. Figure 4 B,C,E), implying that models can be scaled up efficiently.

Finally, empirical data is often short, yet we want to reliably infer DS features that characterize the underlying dynamics. To demonstrate that our model can robustly reconstruct dynamics based on short time series, we inferred \(1000\) convSSMs on \(n=100\) convolved Lorenz63 time series (TR \(=0.5\)) of length \(T=1000\) only (see Figure 2C). We then assessed the degree of chaoticity in the recovered trajectories by examining the trained models' maximum Lyapunov exponents, \(_{max}\). \(_{max}\) measures how quickly trajectories starting from nearby points in a system's state space converge or diverge with time. If \(_{max}>0\), trajectories will exponentially diverge and the system, if bounded, will exhibit chaos. We show that we can successfully recover \(_{max}\) (known for the Lorenz63 system; Figure 2C) even from models trained on these just short series.

Figure 2: Validations on Lorenz63 and ALN. A: Illustration of reconstruction performance as assessed by the geometrical agreement measure \(D_{stsp}\). Average \(D_{stsp}\) values for the convSSM were \(D_{stsp}<0.30\) at noise level \(=.01\) and \(D_{stsp}<0.71\) at noise level \(=.1\), indicating successful reconstructions in the majority of cases. B: Example trajectory from the Lorenz63 system in latent space (top) and observation space (convolved with \(hrf_{0.2}\)) (bottom). C: Probability density over maximal \(_{max}\) values (orange) assessed on 1000 convSSMs trained on Lorenz63 time series of length \(1000\) (example shown in right panel). Black line denotes the known \(_{max} 0.9056\) of the Lorenz system. D: Comparison of standard SSM (‘standard’), convSSM (‘conv’), and convSSM trained without generalized teacher forcing (‘conv (NoGTF’)’) on the ALN data set. Histograms over \(D_{stsp}\) assessed on the observed space (left panel) and latent space (right panel). E: \(D_{stsp}\) for convSSM evaluated on the full pseudo-empirical time series of typical empirically available length (\(T=500\); _x-axis_) vs. the long GT test set (\(T=5,000\); _y-axis_). F: \(D_{stsp}\) for convSSM evaluated on the observed time series (_x-axis_) vs. on the latent time series (_y-axis_).

### Validating performance measures on short time series

In empirical situations, we do not have access to the latent dynamics of the true system, of course, but we still rely on our reconstruction measures evaluated on the _observed signals_ to yield results valid for the (unobserved) latent space. It is therefore a practically very relevant question whether a) the convSSM trained on such short time series would be able to accurately recover the underlying neural latent dynamics, and b) our measures (\(D_{stsp},D_{PSE}\)) evaluated on such short time series, and directly on the observations, would yield results similar to what would be expected if much longer time series and access to the ground truth latent space were available.

To tackle these questions, we used a more realistic simulation model, the ALN model [3; 11] for simulating whole brain (neural) activity. \(100\) data sets of length \(T=10,000\) were simulated from this model using _neurolib_, sampled at \(0.1\,\), and filtered through Equation 5 (with \(=0.1\,\)) to compute the corresponding BOLD time series. Subsequently, these time series were downsampled to a TR of \(0.5\,\) to mimic an experimentally realistic scenario (see Appx. B.2 for details). Most hyperparameters were adopted from previous experiments (see Appx. Table 4). For the latent dimension, we chose \(M=16\) to match the dimensions of the empirical LEMON data set, see subsection 3.4.

To mimic real fMRI experiments, we then pretended that only the first \(500\) time points are available for model estimation (called 'pseudo-empirical' here to distinguish it from the actual empirical LEMON data set). We trained \(10\) convSSM models on the first \(375\) time steps of each of these virtual experiments, treating the left out \(125\) time points as pseudo-empirical test set and call the last \(5,000\) time points of the entire trajectory (i.e., time steps \(5,001\)-\(10,000\) of the full simulation set) the ground truth (GT) test set (which would not be accessible in a real experiment). DSR performance was assessed on both the observed \(\{x_{t}\}\) and latent \(\{z_{t}\}\) time series, evaluated for a) the short pseudo-empirical test set of length \(125\), b) the full pseudo-empirical time series (i.e., of length \(500\)), and c) the GT test set of length \(5000\) (which also assesses dynamics on the limit set and does not contain transients anymore).

Figure 2D shows histograms over \(D_{stsp}\) (on the full pseudo-empirical time series) for the convSSM, the standard SSM, and the benchmark conditions. The convSSM significantly outperformed the standard SSM in latent space (rank-sum test \(Z=11.50\), \(p.001\)), demonstrating an improved recovery of the ground truth DS and indicating that the deconvolution acts as an inductive bias that forces the model to learn a latent space structured in agreement with our biophysical understanding of fMRI. Moreover, the proposed performance measures (\(D_{stsp},D_{PSE}\)) successfully discriminated between good and poor reconstructions even on these short time series more typical for empirical data: For one, evaluating DSR on the observations was consistent with evaluating DSR directly on the latent dynamics space (Figure 2F and Appx. Figure 5). Second, DSR assessed on the pseudo-empirical time series (either full or only test set) was strongly correlated with performance assessed on the long GT test set (which, again, in empirical situations we do not have, Figure 2E and Appx. Figure 5).

### Application to experimental fMRI data

We finally tested convSSM on empirical data, for which we chose the LEMON study ('Leipzig Study for Mind-Body-Emotion Interactions') as a publicly available data set. This data set was collected at the Max-Planck-Institute Leipzig  and consists of \(227\) healthy participants, each of whom completed a battery of tests, including a 15min 30s resting state fMRI (rsfMRI) session sampled at \(=1.4s\) (thus comprising \(T=652\) time points). We used the preprocessed rsfMRI data sets as provided, and selected \(16\) regions from which we extracted a subset of the available time series. These were subsequently smoothed, band-pass filtered, and standardized as in . The time series were split \(3:1\) into training (\(T_{train}=489\)) and test (\(T_{test}=163\)) set, respectively. Data from participants with non-stable variance were discarded (i.e., non-stationary data, see Appx. B.3 for details), leaving \(N=51\) participants for analysis.

We trained \(20\) models on data of each participant with latent dimension \(M=N=16\) (i.e., equal to the observation dimension), \(=.1\), and hidden dimension \(L=50\) (where \(L\) and \(M\) refer to the dimensions of the connectivity matrices \(},}\) in the cshPLRNN, Equation 1). Latent dimension and \(\) were determined via grid search, by inferring systems using a subset of the data and assessing the performance on the held-out set . Otherwise the same hyperparameters as used in  for EEG data were applied (see Appx. Table 4 for all details). In addition to the model comparisonsdiscussed earlier, we also compared our method to the performance of rSLDS  and LFADS  (see Appx. C.2 for details). On top of \(D_{ststp}\), \(D_{PSE}\), and \(PE_{n}\), we also assessed the trained models' maximum Lyapunov exponents, \(_{max}\), analyzed how reliably these can be inferred, and whether they distinguish between subjects. Note that obtaining an estimate of the Lyapunov exponent is an advantage of the generative model, as empirical time series are often too short to compute it reliably. Also, since we do have access to the cshPLRNN's Jacobians, the computations can be performed analytically (although practically we need to evaluate these along model-generated trajectories, where here we used an algorithm proposed in ).

The DSR results are shown in Table 1. We obtained successful reconstructions on average with a mean \(D_{stsp}\) of \(2.73\), better than all other models (Figure 3A and D). Interestingly, most recovered systems were characterized by a positive maximal Lyapunov exponent \(_{max}\) (Figure 3E), indicating the presence of chaotic attractors in these data (consistent with previous observations, [40; 36; 41]). Moreover, \(_{max}\) values could be inferred reliably (Figure 3F), and differentiated between individuals, as indicated by lower within- as compared to between-subject variation (Figure 3G, \(T(50)=-11.53,p<.001\).

   metric & ConvSSM & standard SSM & No GTF (\(=0\)) & MINDy & rSLDS & LFADS & Noise process & Fixed point \\  \(D_{stsp}\) & \(2.73 1.09\) & \(2.77 0.93\) & \(3.77 1.22\) & \(6.79 1.92\) & \(15.51 10.02\) & \(3.24 1.17\) & \(4.62 0.91\) & \(5.27 1.27\) \\ \(D_{PSE}\) & \(0.14 0.03\) & \(0.15 0.03\) & \(0.34 0.11\) & \(0.27 0.06\) & \(0.24 0.03\) & \(0.43 0.09\) & \(0.76 0.02\) & - \\
10-step PE & \(1.78 0.38\) & \(2.00 0.44\) & \(1.43 0.61\) & \(1.97 0.31\) & \(1.78 0.42\) & \(2.45 0.55\) & - & - \\   

Table 1: DSR measures evaluated for the convSSM, standard SSM, convSSM trained without GTF, as well as MINDy , rSLDS  and LFADS , trained on the LEMON dataset. Model runs were excluded if the 1-step PE \(>1\) on the training data.

Figure 3: Results on empirical LEMON data set. A: Distribution over \(D_{stsp}\) for \(1020\) systems inferred with convSSM. B: Example of a good and C: poor reconstruction. D: Illustration of reconstruction performance as a function of \(D_{stsp}\). E: Histogram over maximum Lyapunov exponents \(_{max}\). F: Distribution over \(_{max}\) for 5 selected participants (\(n=100\) systems with \(10\) trajectories each). G: Within- as compared to between-subject variance in \(_{max}\) distribution after filtering models by DS performance measures (selecting the 20 best by \(D_{stsp}\) and 10 best by \(D_{PSE}\)).

Conclusions

Methods for producing generative models of the underlying dynamics from time series observations is a rapidly expanding research field . Current SOTA models for this purpose rely on control theoretically motivated training techniques like STF  and GTF , but these require some means to generate from the actual observations a TF signal in the model's latent space for guiding trajectory and gradient flows. This becomes difficult if the current observation depends on a whole series of latent states, as common if the actual measurements are some filtering of an underlying process of interest, such as in fMRI or Ca\({}^{2+}\) imaging. Here we provide a novel technique that efficiently deals with this problem, exploiting linearity of Wiener deconvolution. A hallmark of our technique is that it efficiently scales with model size and convolution length.

Another major contribution of this work is to numerically demonstrate that the short time series obtained in typical fMRI experiments are actually sufficient for proper model selection according to established DSR performance measures, and that these can indeed be properly evaluated in observation space and do not require access to the unobserved dynamics/ latent space. This is of major empirical relevance for many scientific scenarios, beyond fMRI, in which time series sampling is costly or restricted for technical reasons. Finally, using our DSR technique, we showed that experimental fMRI signals mostly exhibit properties of chaotic oscillators (consistent with ), and that these can be reliably inferred and differ between subjects. Taken together, these contributions pave the way for deploying data-driven fMRI DSR models at large scale to understand inter-individual differences in brain dynamics and explore the predictive value of nonlinear DS features for cognitive or clinical assessment.

We emphasize that the proposed framework is highly flexible due to its modular structure, and may be easily adapted to meet diverse requirements. First, the latent model can be replaced with any other differentiable and recursive dynamical model, such as e.g. LSTMs . The GTF training framework would remain unchanged as the control signal and the latent state update (Equation 3) are not affected by such modifications . Likewise, the observation model can easily be adapted to account for nonlinear effects of nuisance covariates, e.g. through basis expansions in these variables, or through learnable but regularized MLPs. While our model was designed as a scalable method to integrate biological prior knowledge on convolution filters like the \(hrf\), alternatively we can parameterize the filter weights within the observation model, making them learnable through BPTT, with filter length either as a hyperparameter, or by imposing a regularization that truncates filter length by driving coefficients to zero. To prevent conflicts between filter adjustment and latent model, a viable strategy may be stage-wise learning as suggested in . Once the filter is adjusted, one may reduce the learning rate on the observation model, or even fix its parameters, to prioritize learning of the dynamics. Fixing the filter parameters after an initial stage would have the advantage that subsequent training would enjoy the same speed benefits as in our suggested method.

We furthermore highlight that our framework could be adapted to accommodate noise in the latent process. For example, in Brenner et al.  the GTF procedure has been modified to work in the context of stochastic DSR models using variational inference. Instead of the multimodal encoder model in Brenner et al. , one may use the inversion in Equation 9 to generate a TF signal which steers a probabilistic latent DS model, i.e. controls its distributional mean, via Equation 3, and using the reparameterization trick  for BPTT in latent space. However, although probabilistic frameworks are appealing, 'deterministic' BPTT has previously been shown to be (at least) comparable in terms of DSR performance, even for clearly noisy observations and latent processes , such that the benefits for DSR would need to be further examined.

**Limitations** Data-driven approaches such as the one proposed here lack detailed biophysical mechanisms and may thus not be as suited to address specific questions relating to pharmacological or receptor mechanisms beyond functional-dynamical implications. Moreover, currently open questions are how to best deal with non-stationarity in the data, how to efficiently combine data from many subjects, and how trained models generalize to out-of-domain data.

## Software and Data

Code for the convSSM is available at https://github.com/humml-lab/GTF-ConvSSM.

Acknowledgements

This work was supported by the German Research Foundation (DFG) within the collaborative research center TRR 265, subproject B08, granted to GK, TRR 265 subproject A06 granted to DD and GK, Germany's Excellence Strategy EXC 2181/1 - 390900948 (STRUCTURES), and the Hector II foundation.