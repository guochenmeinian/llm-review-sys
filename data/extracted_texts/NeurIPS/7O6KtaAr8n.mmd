# Learning Social Welfare Functions

Kanad Shrikar Pardeshi

Carnegie Mellon University

kpardesh@andrew.cmu.edu

&Itai Shapira

Harvard University

itaishapira@g.harvard.edu

&Ariel D. Procaccia

Harvard University

arielpro@seas.harvard.edu

&Aarti Singh

Carnegie Mellon University

aarti@andrew.cmu.edu

###### Abstract

Is it possible to understand or imitate a policy maker's rationale by looking at past decisions they made? We formalize this question as the problem of learning social welfare functions belonging to the well-studied family of power mean functions. We focus on two learning tasks; in the first, the input is vectors of utilities of an action (decision or policy) for individuals in a group and their associated social welfare as judged by a policy maker, whereas in the second, the input is pairwise comparisons between the welfares associated with a given pair of utility vectors. We show that power mean functions are learnable with polynomial sample complexity in both cases, even if the social welfare information is noisy. Finally, we design practical algorithms for these tasks and evaluate their performance.

## 1 Introduction

Consider a standard decision making setting that includes a set of possible actions (decisions or policies), and a set of individuals who assign utilities to the actions. A _social welfare function_ aggregates the utilities into a single number, providing a measure for the evaluation of actions with respect to the entire group. Utilitarian social welfare, for example, is the sum of utilities, whereas egalitarian social welfare is the minimum utility. Given two actions that induce the utility vectors \((,0)\) and \((1,1)\) for two individuals, the former is preferred when measured by utilitarian social welfare, whereas the latter is preferred according to egalitarian social welfare.

When competent decision makers adopt policies that affect groups or even entire societies, they may have a social welfare function in mind, but it is typically implicit. Our goal is to _learn_ a social welfare function that is consistent with the decision maker's rationale. This learned social welfare function has at least two compelling applications: first, _understanding_ the decision maker's priorities and ideas of fairness, and second, potentially _imitating_ a successful decision maker's policy choices in future dilemmas or in other domains.

As a motivating example, consider the thousands of decisions made by public health officials in the United States during the Covid-19 pandemic: opening and closing schools, restaurants, and gyms, requirements for masking and social distancing, lockdown recommendations, and so on. Each decision induces utilities for individuals in the population; closing schools, for instance, provides higher utility to medically vulnerable individuals compared to opening them, but arguably has much lower utility for students and parents. Assuming that healthcare officials were acting in the public interest and (approximately) optimizing a social welfare function, which one did they have in mind? Our goal is to answer such questions by learning from example decisions.

Another example we consider in this paper is that of allocating food resources in a community by a US-based nonprofit to hundreds of recipient organizations. Working with a dataset of utility of 18different stakeholders such as donors, volunteers, dispatchers and recipient organizations , we consider the task of learning the social welfare implicit in the decisions that may be made by the nonprofit.

In order to formalize this problem, there are two issues we need to address. First, to facilitate sample-efficient learnability, we need to make some structural assumptions on the class of social welfare functions. We focus on the class of _weighted power mean functions_, which includes the most prominent social welfare functions: the aforementioned utilitarian and egalitarian welfare, as well as Nash welfare (the product of utilities). This class is a natural choice, as it is the only class of functions feasible under a set of reasonable social choice axioms such as monotonicity, symmetry, and scale invariance .

Second, we need to specify the input to our learning problem. There are two natural options, and we explore both: utility vectors coupled with their values under a target social welfare function, or pairwise comparisons between utility vectors. We demonstrate sample complexity bounds for both types of inputs, where the social welfare value or comparisons can be noiseless or corrupted by noise. We note that estimating the utility vector associated with any particular decision or policy is ostensibly challenging, but in fact this has been done in prior work and we have access to relevant data, as we discuss in Section6.

**Our contributions.** Learning weighted power mean functions is a non-standard regression or classification problem due to the complex, highly nonlinear dependence on the power parameter \(p\), which is the parameter of interest. While one can invoke standard hyperparameter selection approaches such as cross-validation to select \(p\) from a grid of values, the infinite domain of \(p\) does not allow demonstration of a polynomial sample complexity without deriving an appropriate cover. We derive statistical complexity measures such as pseudo-dimension, covering number, VC dimension and Rademacher complexity for this function class, under both cardinal and ordinal observations of the social welfare function. Our sample complexity bounds are summarized in Table1. These results may be of interest for other problems where weighted power mean functions are used, such as fairness in federated learning .

We highlight some key contributions of this paper. We first establish the statistical learnability of widely used social welfare functions belonging to the weighted power mean functions family. We derive a polynomial sample complexity of \((1)\) for learning using cardinal social welfare values under \(_{2}\) loss, and \(( d)\) (where \(d\) denotes the number of individuals) for learning using comparisons under \(0-1\) loss in the unweighted/known weight setting. The upper bounds leverage the monotonicity of the target functions with \(p\) in the cardinal case, and the restricted number of roots in the ordinal setting. We also prove matching lower bounds for the ordinal case.

We also establish a polynomial sample complexity of \((d d)\) for both cardinal and ordinal tasks in the setting when the individual weights are unknown. This result is intuitive, as learning an additional \(d\) weight parameters incurs a proportional increase in the sample requirement.

We then analyze the sample complexity for the more practical ordinal task under different noise models (i.i.d. and logistic noise) and characterize the effect of noise on learning. In the i.i.d. noise setting, sample complexity increases with noise level \(\), converging to the noiseless complexity as \( 0\). Unlike the i.i.d. case where \(\) is known, in the logistic noise model, we also consider estimating the noise level \(\) and evaluate the likelihood with respect to the noisy distribution. As \(\) increases, estimating the noise becomes more challenging, leading to higher sample complexity.

   Social Welfare Information & Loss & Known Weights & Unknown Weights \\  Cardinal values & \(_{2}\) & \((^{2})\) & \((^{2}d d)\) \\ Pairwise comparisons & 0-1 & \(( d)\) & \((d d)\) \\ Pairwise comparison with i.i.d noise & 0-1 & \((})\) & \((})\) \\ Pairwise comparisons with logistic noise estimation & Logistic & \((_{}^{2}^{2})\) & \((_{}^{2}^{2}d d)\) \\   

Table 1: A summary of our results regarding the sample complexity of various tasks. Here, \(=u_{}(u_{}-u_{})\) and \(=(u_{}/u_{})\), with all \(d\) individual utilities assumed to be in the range \([u_{},u_{}]\). \([0,1/2)\) is the probability of mislabeling for the i.i.d noise model, and \(_{}\) is the maximum temperature of the logistic noise model.

Finally, despite the problem's non-convexity, we demonstrate a practical algorithm for learning weighted power mean functions across tasks using simulated data and a real-world food resource allocation dataset from Lee et al. . Our empirical results validate theoretical bounds and highlight algorithm performance across parameter settings.

Related work.Conceptually, our work is related to that of Procaccia et al. , who study the learnability of decision rules that aggregate individual utilities. In their work, however, individual utilities are represented as rankings over a set of alternatives (rather than cardinal utilities as in our case), and the rule to be learned is a voting rule that maps input rankings to a winning alternative. They provide sample complexity results for two families of voting rules: positional scoring rules and voting trees.

Basu and Echenique  derive VC dimension bounds for additive, Choquet, and max-min expected utility for decision-making under uncertainty, bounding the number of pairwise comparisons needed to falsify a candidate decision rule and establishing learnability for these classes. Their work addresses decision rules operating on probability distributions rather than utility vectors, resulting in technical distinctions from ours; for instance, the max-min rule is not learnable in their setting (infinite VC dimension), whereas it is learnable in ours.

Kalai  studies the learnability of choice functions and establishes PAC guaranteees. Choice functions are defined with respect to a fixed and finite set of alternatives \(X\), with each sample being a subset from \(X\) and the choice over this subset. In contrast, our approach involves learning a function over an infinite action space where utilities are known.

Pellegrini et al.  conducts experiments on learning aggregation functions which are assumed to be a composition of \(L_{p}\) means, observing that they perform favorably in various tasks such as scalar aggregation, set expansion and graph tasks. Our work provides a more theoretical analysis, proving the sample-efficient learnability of weighted power mean aggregation functions.

Melnikov and Hullermeier  considers learning from actions with feature vectors and their global scores, with local scores for each individual unavailable for learning. They learn both local and global score functions, and consider the ordered weighted averaging operator for aggregating local scores. While we assume that each individual's local score is given, the aggregation function belongs to a richer function family motivated by social choice theory.

## 2 Problem Setup

We assume that the decision-making process concerns \(d\) individuals. The decision-making setting we consider has each action associated with a positive utility vector \([u_{},u_{}]^{d}_{+}^{d}\), which describes the utilities derived from the \(d\) individuals.

We encode the impact of each individual \(i[d]\) on the decision-making process through a weight value \(w_{i} 0\) such that \(_{i=1}^{d}w_{i}=1\). These weight values together form a weight vector \(_{d-1}\). The weight vector might be a known or unknown quantity. A common instance in which the weight vector is known is when all agents are assumed to have an equal say, in which case \(=_{d}/d\). For all settings we consider, we provide PAC guarantees for both known weights and unknown weights.

We assume that the decision-making process provides a cardinal social welfare value to each action. However, this social welfare value can be latent and need not be available to us as data. For the first task concerned with cardinal decision values, the social welfare values are available and can be used for learning. For the second task, both actions in the pair have a latent social welfare which is not available to us; however, the preferred action in the pair is known to us. We consider learning bounds with the empirical risk minimization (ERM) algorithm for all the losses in this work, with \(\) being learned when the weights are known, and \((},)\) being learned when the weights are unknown.

Power Mean.The (weighted) power mean is defined on \(p\{\}\), and for \(_{+}^{d},_{d-1}\), it is

\[M(;,p)=(_{i=1}^{d}w_{i}u_{i}^{p} )^{1/p}&p 0\\ _{i=1}^{d}u_{i}^{w_{i}}&p=0\]It is sometimes more convenient to use the (natural) _log_ power mean than the power mean. Since \(_{i=1}^{d}w_{i}=1\), in effect we have \(d\) variables, \(w_{1},,w_{d-1}\) and \(p\). We refer to the weighted power mean family with known weight \(\) as

\[_{,d}=\{M(;,p)|p\}.\]

If the weight is unknown, the weighted power mean family is denoted by

\[_{d}=\{M(;,p)|p, _{d-1}\}.\]

The power mean family is a natural representation for social welfare functions. Cousins  puts forward a set of axioms under which the set of possible welfare functions is precisely the weighted power mean family. An unweighted version of these functions results in the family of constant elasticity of substitution (CES) welfare functions , which are widely studied in econometrics.

To show the generality of this family of functions, we list a few illustrative cases:

* \(M(;,p=-)=_{i d}u_{i}\), which corresponds to egalitarian social welfare.
* \(M(;,p=0)=_{i=1}^{d}u_{i}^{w_{i}}\), which corresponds to a weighted version of Nash social welfare.
* \(M(;,p=1)=_{i=1}w_{i}u_{i}\), which corresponds to weighted utilitarian welfare.
* \(M(;,p=)=_{i d}u_{i}\), which corresponds to egalitarian social _welfare_.

We note that for \(p=\), the decision utility is independent of \(\). With \(w_{i}=1/d\) for all \(i[d]\), we get the conventional interpretations of the welfare notions mentioned above.

The power mean family has some useful properties. An obvious one is that \(M(,,p)[u_{(1)},u_{(d)}]\), where \(u_{(1)}\) and \(u_{(d)}\) denote the first and \(d\)-th order statistics of \(=(u_{1},,u_{n})\). \(u_{(1)}\) is attained at \(p=-\), and \(u_{(d)}\) is attained at \(p=\). A more general observation is the following:

**Lemma 2.1**.:
1. \(M(;,p)\) _is nondecreasing with respect to_ \(p\) _for all_ \([u_{},u_{}]^{d}\)_,_ \(_{d-1}\)_._
2. \(M(;,p)\) _is monotonic with respect to_ \(w_{i}\) _for each_ \(i[d-1]\)_, for all_ \([u_{},u_{}]^{d}\)_,_ \(p\)_._
3. \(M(;,p)\) _and_ \( M(;,p)- M(;,p)\) _are quasilinear with respect to_ \(\) _if_ \(p\) _is fixed._

This monotonicity of the power mean in \(\) and \(p\) was also noted by Qi et al. . A proof for the above lemma is provided in Appendix A.1.

## 3 Cardinal Social Welfare

We first consider the case where we know the cardinal value of the social choice associated with each action. Learning in this setting thus corresponds to regression. Formally, we assume an underlying distribution \(:[u_{},u_{}]^{d}[u_{},u_{}]\) over the utilities and social welfare values. We receive i.i.d samples \(\{(_{i},y_{i})\}_{i=1}^{n}^{n}\), \(_{i}\) being the utility vector and \(y_{i}[u_{i(1)},u_{i(d)}]\) being the social welfare value associated with action \(i\).

We consider the \(_{2}\) loss over \(M(_{i};,p)\) and \(y_{i}\). The true risk in this case is

\[R(,p)=_{(,y)}[(M( ;,p)-y)^{2}].\]

To analyze the PAC learnability of this setting, we first provide bounds on the pseudo-dimensions1 of \(_{,d}\) and \(_{d}\). We begin by noting that

\[M(;,p)=u_{(d)} M(;,p), [d]r_{i}=}{u_{(d)}}.\]Since \(M(;,p)\), we can determine the pseudo-dimensions of this function class.

We now define the function classes

\[_{,d} =\{f(;,p)=M(;,p)( ,p)_{d-1}\},\] \[_{d} =\{f(;,p)=M(;,p)  p\}.\]

We then have the following bounds on pseudo-dimensions:

**Lemma 3.1**.:
1. _If_ \(\) _is known, then_ \((_{,d})=1\)_._
2. _If_ \(\) _is not known, then_ \((_{d})<8d(_{2}d+1)\)_._

A detailed proof is provided in Appendix A.3.

We highlight the fact that \(p\) and \(\) are the parameters of the log power mean function family, which calls for the novel bounds provided in this work. These bounds on the pseudo-dimensions can now be used to obtain PAC bounds:

**Theorem 3.2**.: _Given a set of samples \(\{(_{i},y_{i})\}_{i=1}^{n}\) drawn from a distribution \(^{n}\), for any \(>0\), the following holds with probability at least \(1-\) with respect to the \(_{2}\) loss function:_

1. _If_ \(\) _is known, then_ \[R(,)-_{p}R(,p) 16( }+})+6}\]
2. _If_ \(\) _is unknown, then_ \[R(},)-_{(,p)_{d-1} }R(,p) 16(d+1) n}{n}}+ })\] \[+6}\]

_where \(=u_{}(u_{}-u_{})\)._

For the complete proof, see Appendix A.5. Below, we provide a proof sketch.

Proof Sketch.: We first use the pseudo-dimensions found above to bound the Rademacher complexity of \(_{d}\) and \(_{,d}\) in Lemma A.6. Since \(M(_{i};,p)[u_{},u_{}]\) and \(y_{i}[u_{},u_{}]\), the \(_{2}\) loss function in this case has domain \([u_{}-u_{},u_{}-u_{}]\). It is Lipschitz continuous on this domain with Lipschitz constant \(2\). Using Lemma A.6 and Talagrand's contraction lemma, we obtain the bounds

\[}(_{,d}) 2(u_{}-u_{})}(_{,d})}(_{d}) 2(u_{}-u_{})}( _{d}).\]

These Rademacher complexity bounds are then used to obtain the uniform convergence bounds above. 

These bounds are distribution-free, with the only assumption being that all utilities and social welfare values are in the range \([u_{},u_{}]\). They also imply an \((1)\) and \((d d)\) dependence of sample complexity on \(d\) for known and unknown weights respectively. Moreover, we observe the dependence of the upper bound on \(u_{}-u_{}\) for the \(_{2}\) loss. We note that when \(u_{}=u_{}=u_{0}\), all utilities and social welfare function values are also \(u_{0}\). In this case, the Rademacher complexity bound is also zero, which is expected.

Computationally, \(M(;,p)\) is non-convex in \(\) and \(p\), which means that the \(_{2}\) loss is also non-convex. However, we observe that from Lemma 2.1 (c), \(M(;,p)\) is quasilinear w.r.t. \(\) with fixed \(p\), which makes the \(_{2}\) loss function quasi-convex for all \((,y)\)2. We use this fact to construct a practical algorithm.

A shortcoming of this setting is that decision-makers are required to provide a social welfare value for each action. A more natural setting might be when decision-makers only provide their preferences between actions -- potentially just their _revealed_ preferences, i.e., the choices they have made in the past -- and we address this case next.

## 4 Pairwise Preference Between Actions

For this setting, we assume an underlying distribution \(:[u_{},u_{}]^{d}[u_{},u_{}]^{d}\{ 1 \}.\) We obtain i.i.d. samples \(\{((_{i},_{i}),y_{i})\}_{i=1}^{n} ^{n}\), where \((_{i},_{i})\) are the utilities for the \(i\)-th pair of actions, and \(y_{i}\) is a comparison between their (latent) social choice values. We encode the comparison function as \(C:[u_{},u_{}]^{d}[u_{},u_{}]^{d}\{ 1\}\), with

\[C((,);,p)=( M(; ,p)- M(;,p)).\]

We denote the family of above functions by \(_{,d}=\{C((,);,p):p \}\) when the weights are known, and \(_{d}=\{C((,);,p):p,_{d-1}\}\) when the weights are unknown. We consider learning with \(0-1\) loss over \(C((_{i},_{i});,p)\) and \(y_{i}\). The true risk in this case is

\[R(,p)=_{((,),y)}[ ,);,p))}{2}].\]

To provide convergence guarantees for the above setting, we bound the VC dimension of the comparison-based function classes mentioned above

**Lemma 4.1**.:
1. _If_ \(\) _is known, then_ \((_{,d})<2(_{2}d+1)\)_._
2. _If_ \(\) _is unknown, then_ \((_{d})<8(d_{2}d+1)\)_._
3. _(Lower bounds):_ \((_{d})_{2}d+1\)_, and_ \((_{,d}) d-1\)__

The detailed proof of the above lemma is provided in Appendix A.6.

We find the asymptotically tight lower bound for the known weights case rather surprising, as it is _a priori_ unclear that the correct bound should be superconstant and scale with \(d\).

The finiteness of VC dimension guarantees PAC learnability, and we get uniform convergence bounds using the VC theorem.

**Theorem 4.2**.: _Given samples \(\{((_{i},_{i}),y_{i})\}_{i=1}^{n} ^{n}\) and for 0-1 loss and any \(>0\), with probability at least \(1-\),_

1. _If_ \(\) _is known, then_ \[R(,)-_{p}R(,p) 16d+1)(n+1)+(8/)}{n}}\]
2. _If_ \(\) _is unknown, then_ \[R(},)-_{(,p)_{d-1}}R(,p) 16d+1)(n+1)+(8/ )}{n}}\]

We note that unlike the bounds on \(_{2}\) loss of Theorem 3.2, these bounds on 0-1 loss are independent of the range of utility values and only depend on \(d\). They provide sample complexity bounds which depend on \(d\) as \(( d)\) and \((d d)\) for known and unknown weights respectively. Despite these PAC guarantees, empirical risk minimization can be particularly difficult in this case, since the loss function as well as the function class \( M(;,p)- M(;,p)\) can be non-convex. To illustrate this non-convexity, we plot the value of the above function for two pairs of utility vectors with respect to \(p\) in Figure 6, with \(d=6\) and \(=_{d}/d\). However, the quasilinearity of \( M(;,p)- M(;,p)\) with fixed \(p\) can be used to design efficient algorithms.

### Convergence Bounds Under I.I.D Noise

Decision making can be especially challenging if two actions are difficult to compare, and the preference data we obtain can potentially be noisy. We first consider each comparison to be mislabeled in an i.i.d. manner with known probability \([0,1/2)\). We make use of the framework developed by Natarajan et al. , and we consider convergence guarantees under 0-1 loss.

Specifically, the unbiased estimator of \(_{0-1}\) is

\[_{0-1}(t,y)=(t,y)-_{0-1}(t,-y)}{1-2 }.\]

We conduct ERM with respect to \(_{0-1}\) to obtain \((},)_{d-1}\) (only learning \(p\) if weights are known). We observe that \(_{0-1}(t,y)=(1+ty)/2\) is \(1/2\)-Lipschitz in \(t\), \( t,y\{ 1\}\). Using Theorem 3 of Natarajan et al. , we get the following convergence bounds:

**Theorem 4.3**.: _Given samples \(\{((_{i},_{i}),y_{i})\}_{i=1}^{n}^{n}\), for any \(>0\) and for any \([0,1/2)\), with probability at least \(1-\) with respect to 0-1 loss,_

* _If_ \(\) _is known, then_ \[R(},)-_{p}R(,p)d+1)(n+1)}{n}}+2}\]
* _If_ \(\) _is unknown, then_ \[R(},)-_{(,p)_{d-1}}R(,p)d+1)(n+1)}{n}}+2 }\]

A detailed proof of the above theorem is provided in Appendix A.7.

We note that although ERM is conducted with respect to \(_{0-1}\) on the noisy distribution, the risks are defined on the underlying noiseless distribution. This gives \(( d/(1-2)^{2})\) and \((d d/(1-2)^{2})\) sample complexities for the known and unknown weights cases respectively. We note that when \(=0\), the above bounds reduce to the noiseless bounds in Theorem 4.2. Since the noise level \(\) is usually not known to us, it can be estimated using cross-validation as suggested by Natarajan et al. .

However, conducting ERM on \(_{0-1}\) might be prohibitively difficult due to the non-convex nature of the function. An i.i.d noise model might also be inappropriate in certain settings; we next consider a more natural noise model.

## 5 Pairwise Preference With Logistic Noise

Intuitively, we expect that two actions would be harder to compare if their social welfare values are closer to each other. We formalize this intuition in the form of a noise model inspired by the BTL noise model . Let \(^{*}\) and \(p^{*}\) be the true power mean parameters, and let \(^{*}[0,_{}]\) be a temperature parameter. For an action pair \((,)\), we assume that the probability of \(\) being preferred to \(\) is

\[(y=1|(,);^{*},p^{*},^{*} )=( M(;^{*}, p^{*})- M(;^{*},p^{*})))}\] (1)

We see that a larger difference between the log power means of \(\) and \(\) translates to a higher probability of \(\) being preferred. If \(\) and \(\) lie on the same level set of \( M(;^{*},p^{*})\), the probability becomes 0.5, which matches the intuition of both actions being equally preferred. We also note the dependence of the probability on \(^{*}\): a higher \(^{*}\) corresponds to more confidence in the preferences, with \(^{*}=0\) meaning indifference for all pairs of actions. The mislabeling probability is also invariant to scaling of \(\) and \(\).

Our learning task now becomes estimating \(\), \(p\) and \(\) given data. We denote the function family in this case by

\[_{,d}=\{( M(;,p)- M (;,p))|,p\}\]

when the weights are known, and

\[_{d}=\{( M(;,p)- M(; ,p))|,,p\}\]

when the weights are unknown. A natural loss function to consider in this case is negative log likelihood, and we consider PAC learnability with this loss. Using the framework developed in Section 3, we obtain the following PAC bounds:

**Theorem 5.1**.: _Given samples \(\{((_{i},_{i}),y_{i})\}_{i=1}^{n} ^{n}\) and for negative log likelihood loss, for all \(>0\), with probability at least \(1-\),_

* _If_ \(\) _is known, then_ \[R(,)-_{p}R(,p) 16_{} (}+})+6}\]_._
2. _If_ \(\) _is unknown, then_ \[R(},)-_{(,p)_{d-1} }R(,p)  16_{}d+1) n}{n}}\] \[+16_{}}+3}\]

_where \(=(u_{}/u_{})\)._

We derive this result in detail in Appendix A.8.

This gives us sample complexity bounds of \((1)\) and \((d d)\) with respect to \(d\) for the known and unknown weights cases respectively, thus establishing PAC learnability. An important distinction between Theorem4.3 and the above theorem is that Theorem4.3 bounds risk with respect to 0-1 loss, while the above theorem bounds risk with respect to logistic loss which is continuous and hence easier to control. Moreover, we estimate the noise level \(\) in the logistic case along with \(\) and \(p\), whereas Theorem4.3 is concerned with estimating \(\) and \(p\).

As with the previous cases, non-convexity in this setting also makes global optimization with respect to \(\) and \(p\) (and hence ERM) difficult. We observe that logistic loss is quasilinear in \(\) with fixed \(p\)3, and this observation can be used to construct an effective algorithm.

## 6 Empirical Results

We conduct several simulations on semi-synthetic data to gain additional insight into sample complexity and demonstrate an empirically effective algorithm. The implementation also serves to demonstrate the practicability of our approach, including the availability of individual utility functions.

**Data.** The dataset we rely on (which is not publicly available) comes from the work of Lee et al.  with a US-based nonprofit that operates an on-demand donation transportation service supported by volunteers. WeBuildAI is a participatory framework that enables stakeholders, including donors, volunteers, recipient organizations, and nonprofit staff, to collaboratively design algorithmic policies for allocating donations. Donors provide food donations, volunteers transport the donations, recipient organizations receive and distribute the food, and dispatchers (nonprofit staff) manage the allocation and logistics. The "actions" are hundreds of recipient organizations that may receive an incoming donation.

As part of this framework, Lee et al.  learned a (verifiably realistic) utility function over the actions for each of 18 stakeholders from the different groups based on 8 features: travel time between donors and recipients, recipient organization size, USDA-defined food access levels in recipient neighborhoods, median household income, poverty rates, the number of weeks since the last donation, the total number of donations received in the last three months, and the type of donation (common or uncommon).

In our simulations, we use the values of these stakeholder utility functions learned by Lee et al.  as the utility vectors. We fix a \(p^{*}\) and weight vector \(^{*}\) to generate the social welfare values \(M(;,p)\). We use noisy versions of these social welfare values in the cardinal case, whereas noisy pairwise comparisons between random pairs of utility vectors are used in the ordinal case.

**Algorithm.** As noted in previous sections, \(_{2}\) and logistic losses are quasiconvex with respect to \(\) for single samples when \(p\) is fixed. Although the sum of quasiconvex functions is not guaranteed to be quasiconvex, we empirically observe that gradient descent on the loss function applied to the data can still lead to convergence to a minimum which has empirical risk comparable to that of the true parameters. As our simulations show, this minimum increasingly resembles \(^{*}\) (the real weight) with decreasing noise. Thus, our algorithm consists of performing a grid search on \(p\) and conducting gradient descent on \(\) for each \(p\). We provide more details about the algorithm in AppendixB.

**Cardinal case.** We consider \(p^{*}=2.72\) and a random weight \(^{*}\). We then add Gaussian noise with standard deviation \((u_{i(d)}-u_{i(1)})\) to each sample, where \(\) corresponds to the noise level. The Gaussian noise is clipped to stay within \([u_{i(1)},u_{i(d)}]\). Finally, we learn \(p\) and \(\) using our algorithm, and we present the results in Figure1.

In Figure 0(a), we observe that the test loss for learned parameters decreases with decreasing noise and increasing number of samples. We also observe that the test loss for learned parameters closely matches that for real parameters in Figure 0(a). In Figure 0(b), we observe that KL divergence between the true and learnt weights decreases uniformly with decreasing noise and increasing number of samples. This supports the fact that our algorithm is indeed able to find the correct minimum. We also plot the trend of mean learned \(p\) in Figure 0(c), and we observe that the learned \(p\) increasingly resembles the real \(p^{*}\) with lower noise and greater number of samples. Plots for train loss and loss on noiseless test data are provided in Appendix C.

**Ordinal case.** We consider \(p^{*}=-1.62\) and a random weight \(^{*}\). We compare each sample in the considered training data with 10 other randomly chosen samples, with the comparisons being noised according to the logistic noise model in Equation (1). We then learn \(\) and \(\) for each \(p\) in the chosen grid and then choose the best \(p\). Our results are shown in Figure 2.

In Figure 1(a) we observe that the test loss for learned parameters matches that for real parameters for small \(^{*}\) and a large number of training samples. The relative deviation between test losses progressively increases for smaller numbers of samples and smaller \(^{*}\). We note that small \(^{*}\) corresponds to more noise in the comparisons, which results in higher losses. However, the deviation between learned loss and true loss is smaller, as we are also estimating the noise parameter, which is easier to estimate for small \(^{*}\), since the logistic function has a larger gradient.

We observe a uniform decrease in KL divergence between \(\) and \(^{*}\) for a larger number of samples and smaller \(^{*}\), again pointing to the effectiveness of the algorithm. We also observe that test accuracy on noiseless data increases with more samples and higher \(^{*}\). Interestingly, for \(^{*}=0.1\) and \(^{*}=1\), the test accuracy on noiseless data (Figure 1(c)) is significantly higher than that on (noisy) test data, another indicator of effective ERM being conducted by the algorithm.

Figure 1: Results for cardinal case with number of samples. Different lines show results for different values of added noise \(\). Solid lines correspond to values for learned parameters, whereas dotted lines correspond to values for real parameters.

Figure 2: Results for ordinal case with number of samples. Different lines show results for different values of noise level \(\). Solid lines correspond to values for learned parameters, whereas dotted lines correspond to values for real parameters.

In Figure 3(b) in Appendix C, we observe greater variation in learned \(p\) compared to the cardinal case. A possible reason behind this is that changes in \(p\) result in smaller changes in losses for negative \(p\) than for positive \(p\). This hypothesis is supported by simulations for the ordinal case conducted for \(p=1.62\), with results presented in Figure 5. In Figure 4(e), we observe that learned \(p\) is much more consistent with the real \(p\) as \(^{*}\) decreases.

We also conduct simulations on fully synthetic data to study the effect of \(d\), and we present the results in Appendix E. We verify the theoretical \((d d)\) scaling of error with unknown weights for the ordinal case in Figure 8.

## 7 Discussion

Our work has (at least) several limitations, which can inspire future work. First, as seen in Section 6, we are able to gain access to realistic utility vectors, in this case ones based on models that were learned from pairwise comparisons. Utilities are also routinely estimated for other economically-motivated algorithms -- say, Stackelberg security games .

However, these estimates are of course not completely accurate. It is an interesting direction of future work to extend our results to the setting where the utility vectors need to be estimated, either by an outside expert, or using input from the individuals themselves.

Although our experiments demonstrate convergence of the algorithm to the correct minimum, rigorous theoretical analysis about the nature of minima for the \(_{2}\) and logistic loss functions is still needed and could lead to algorithmic improvements. One issue is that scaling the algorithm to the national scale- \(d=10^{8}\), say, can be prohibitively expensive.

Finally, our work only applies to weighted power mean functions. While we have argued that this family is both expressive and natural, it would be exciting to obtain results for even broader, potentially non-parametric families of social welfare functions.

The ability to learn social welfare functions can enable us to understand a decision maker's priorities and ideas of fairness, based on past decisions they have made. This has direct societal impact as these notions can be used to both understand biases and inform the design of improved fairness metrics. A second potential application is to imitate a successful decision maker's policy choices in future dilemmas or in other domains. This may pose some ethical questions if the learning model is misspecified; however, the restriction of the function class to weighted power means, which is inspired by natural social choice theory axioms, mitigates this risk.