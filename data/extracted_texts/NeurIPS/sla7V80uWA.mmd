# Beyond MLE: Convex Learning for Text Generation

Chenze Shao\({}^{*}\)\({}^{1,2}\), Zhengrui Ma\({}^{*}\)\({}^{1,2}\), Min Zhang\({}^{3}\) & Yang Feng\({}^{}\)\({}^{1,2}\)

\({}^{1}\) Key Laboratory of Intelligent Information Processing

Institute of Computing Technology, Chinese Academy of Sciences

\({}^{2}\) University of Chinese Academy of Sciences

\({}^{3}\) School of Future Science and Engineering, Soochow University

chenzeshao@tencent.com, mazhengrui21b@ict.ac.cn

zhangminmt@hotmail.com, fengyang@ict.ac.cn

Equal contribution. Order determined by coin flip.Corresponding author: Yang Feng

###### Abstract

Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that best explain the observed data. In the context of text generation, MLE is often used to train generative language models, which can then be used to generate new text. However, we argue that MLE is not always necessary and optimal, especially for closed-ended text generation tasks like machine translation. In these tasks, the goal of model is to generate the most appropriate response, which does not necessarily require it to estimate the entire data distribution with MLE. To this end, we propose a novel class of training objectives based on convex functions, which enables text generation models to focus on highly probable outputs without having to estimate the entire data distribution. We investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss, demonstrating that convex functions can sharpen the optimal distribution, thereby enabling the model to better capture outputs with high probabilities. Experiments on various text generation tasks and models show the effectiveness of our approach. It enables autoregressive models to bridge the gap between greedy and beam search, and facilitates the learning of non-autoregressive models with a maximum improvement of 9+ BLEU points. Moreover, our approach also exhibits significant impact on large language models (LLMs), substantially enhancing their generative capability on various tasks. Source code is available at https://github.com/ictnlp/Convex-Learning.

## 1 Introduction

Text generation is an important field within natural language processing that aims to generate human-like texts for specific tasks. It can be broadly divided into two categories: open-ended and closed-ended text generation. Open-ended tasks encourage the model to produce novel and diverse outputs without a specific expected outcome or structure. Representative tasks in this category include language modeling , chatbot , storytelling , etc. In contrast, closed-ended tasks are more constrained and adhere to specific rules or formats. Representative tasks in this category include machine translation , text summarization , etc.

In recent years, learning neural probabilistic models with maximum likelihood estimation has become the dominant approach for both open-ended and closed-ended text generation . Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that maximize the likelihood of the observed data . Since directly maximizing thelikelihood can be numerically unstable, it is common to minimize the negative log-likelihood loss function, which is also referred to as cross-entropy loss. It is equivalent to minimizing Kullback-Leibler (KL) divergence [25; 1] between the true distribution and the predicted distribution, which ensures that the optimal predicted distribution is the true data distribution.

While MLE has gained widespread adoption, it does not always align with the objective of text generation, especially for closed-ended text generation tasks such as translation and summarization. In these tasks, the goal of the model is to generate the most appropriate response, rather than producing diverse outputs. For example, in the task of machine translation, though there may exist multiple translations for the same input sentence, we usually want the most accurate and commonly used translation result. Generally speaking, the desired output can be mathematically defined as the output with the maximum probability in the true data distribution, which does not necessarily require the model to estimate the entire data distribution with MLE.

In terms of generating the most probable output, MLE is also suboptimal for current neural text generation models. For autoregressive models, even if the model can perfectly fit the data distribution, it still requires decoding algorithms like greedy or beam search to generate the output, which do not guarantee the exact result with the maximum probability. To our knowledge, only Stahlberg and Byrne  proposed an exact decoding algorithm for autoregressive models, but it is too slow for practical applications. The limitation in exact decoding can be overcome by non-autoregressive models [17; 14], which independently predict the output at each position. However, fitting the data distribution by MLE is theoretically beyond the ability of non-autoregressive models . In light of these issues, alternative training objectives should be considered to better address the specific requirements of text generation without incurring the shortcomings associated with MLE.

Based on the analysis above, MLE is suboptimal that it trains the model to estimate the data distribution, which complicates the training and decoding of text generation models. It would be advantageous if the model could converge to a sharper optimal distribution under an alternative loss function, as this would enable autoregressive models to easily find high probability outputs and also allow non-autoregressive models to converge to a better distribution. Exploring loss functions with this characteristic could lead to improved performance and efficiency of neural text generation models, particularly for closed-ended tasks.

In this paper, we propose a novel class of training objectives based on convex functions, which help text generation models capture highly likely outputs without estimating the entire data distribution. Intuitively, the concave shape of log-probability discourages the model from assigning a large prediction probability to a single sample, as the marginal benefit diminishes with increasing probability. If the learning criterion is convex or less concave, then intuitively the model would converge to a sharper distribution, which is the motivation of this work. We further investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss. Our findings demonstrate that convex functions can sharpen the optimal distribution, allowing the model to better capture outputs with high probabilities.

Experiments on various closed-ended text generation tasks and models show the effectiveness of our approach. Specifically, it enables autoregressive models to bridge the gap between greedy and beam search, and facilitates the learning of non-autoregressive models with a maximum improvement of 9+ BLEU points. Moreover, our approach also exhibits significant impact on large language models, substantially enhancing their generative capability on various tasks.

## 2 Preliminaries

### Maximum Likelihood Estimation

Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that best explain the observed data. This is achieved by maximizing a likelihood function so that the observed data is most probable. Since directly maximizing the likelihood can be numerically unstable, it is common to minimize the negative log-likelihood loss function, also referred to as cross-entropy loss. Given the data distribution \(p_{data}\) and a parametric model with parameters \(\), MLE training minimizes:

\[_{MLE}()=-_{x p_{data}(x)}[ p_{}(x)].\] (1)MLE can be viewed as an attempt to minimize KL divergence between the true underlying distribution of the data \(p_{data}\) and the estimated distribution \(p_{}\) provided by the model . The following equation reveals the relationship between MLE loss and KL divergence:

\[_{KL}(p_{data}||\;p_{})=_{x}p_{data}(x)(x)}{p_{}(x)}=_{MLE}()-H_{data},\] (2)

where \(H_{data}\) is the Shannon entropy of the data distribution, which remains constant with respect to the model parameter \(\). Therefore, the MLE loss and KL divergence share the same minimizer that the estimated distribution \(p_{}\) equals to the true distribution \(p_{data}\). By minimizing the MLE loss, the predicted distribution is encouraged to be as close as possible to the true data distribution. In the context of text generation, this ensures that the model learns to generate text that closely resembles the text in the training data.

The above discussion can be extended to conditional scenarios. In such cases, the log-likelihood loss can be expressed as:

\[_{MLE}()=-_{c p_{data}(c)}[_{x p _{data}(x|c)}[ p_{}(x|c)]],\] (3)

where \(c\) represents the input context. This extension allows the MLE framework to accommodate a wide range of text generation tasks such as machine translation, summarization, dialogue system, etc.

### Text Generation Models

Based on how the sequence probability is factorized, neural text generation models can be broadly categorized into two types: autoregressive (AR) models and non-autoregressive (NAR) models. Autoregressive models generate text sequentially, predicting one token at a time based on the previously generated tokens. In AR models, the probability of generating a sequence \(x=(x_{1},x_{2},...,x_{T})\) is factorized as:

\[p_{}(x|c)=_{t=1}^{T}p_{}(x_{t}|x_{<t},c),\] (4)

where \(c\) represents the input context. With the autoregressive decomposition, AR models can perfectly fit the data distribution if it satisfies \(p_{}(x_{t}|x_{<t},c)=p_{data}(x_{t}|x_{<t},c)\) for every \(x,c,t\). In inference, AR models can perform deterministic decoding like greedy/beam search to generate a high probability output, or sample from the model distribution to generate diverse outputs.

In contrast to autoregressive models, non-autoregressive models [17; 14] generate text in parallel, predicting all tokens simultaneously without conditioning on previously generated tokens. This approach can significantly speed up the generation process, as it removes the sequential dependency between tokens. In NAR models, the generation probability is factorized as:

\[p_{}(x|c)=_{t=1}^{T}p_{}(x_{t}|c).\] (5)

Unlike AR models, NAR models can efficiently find the most likely output by using argmax decoding at each step. However, MLE is beyond the ability of NAR models since they are theoretically unable to fit the data distribution. Huang et al.  showed that KL divergence from \(p_{}\) to \(p_{data}\) is bounded by a non-negative constant:

\[_{KL}(p_{data}||\;p_{})=-H_{data}(x|c)+_ {t=1}^{T}H_{data}(x_{t}|c),\] (6)

The MLE loss is minimized when NAR models achieve the equality by ignoring sequential dependency and predicting \(p_{}(x_{t}|c)=p_{data}(x_{t}|c)\). Therefore, NAR models trained with MLE often suffer from reduced performance, as they lack the ability to model dependencies between tokens.

## 3 Approach

In this section, we will explore alternative loss functions for the learning of text generation models, which overcomes the limitations of MLE. We begin by introducing a general learning framework that allows arbitrary loss functions. Next, we discuss the benefits of applying convex functions to the loss within this framework. Finally, we use convex functions to construct composite loss functions, which can be used in practical text generation scenarios.

### General Learning Framework

For simplicity of notation, we omit condition \(c\) in the probabilities, with the data distribution represented as \(p_{data}(x)\) and the model predicting the distribution \(p_{}(x)\). The derived theoretical results hold in both unconditional and conditional settings.

First, we introduce the general learning framework for text generation, characterized by the following loss function:

\[_{f}()=-_{x p_{data}(x)}[f(p_{}(x))],\] (7)

where \(f\) is an arbitrary function of the prediction probability \(p_{}(x)\). We impose some basic requirements on \(f\): (1) The domain of function \(f\) should contain the interval \((0,1]\); (2) \(f\) must be differentiable on the interval \((0,1]\) since we need to compute its gradient; and (3) \(f\) should be an increasing function on \((0,1]\) to encourage the model to generate the current sample. Under this framework, we can explain maximum likelihood estimation as a special case of \(f=\), which is a differentiable and increasing function within the interval \((0,1]\). We also establish some reasonable assumptions:

**Assumption 1** (Countability of Sample Space).: _The sample space \(\) is countable, which allows us to enumerate all samples in a systematic way. Note that \(||\) can be either finite or infinite._

**Assumption 2** (Distinctness of Sample Probabilities).: _In the data distribution \(p_{data}\), the probabilities of all samples are distinct, which allows us to arrange samples in a strictly descending order of sample probabilities.3_

Assumption 1 naturally holds in text generation tasks due to the inherent discreteness of textual data. With a countable sample space and probabilities lying in a dense subspace of real number, it is reasonable to assume the distinctness of sample probabilities. While Assumption 2 is not strictly necessary, removing it would introduce many corner cases that would complicate the subsequent analysis. In the following, we will assume that Assumptions 1-2 always hold, and we arrange the samples such that \(p_{data}(x_{1})>p_{data}(x_{2})>>p_{data}(x_{i})>\). Since the sample space \(\) is countable, the loss function in Equation 7 can be reformulated as follows:

\[_{f}()=-_{i=1}^{||}p_{data}(x_{i}) f(p_{ }(x_{i})).\] (8)

In this framework, our primary focus is to analyze the probability distribution \(p_{}\) that the model is inclined to predict when the loss function is \(_{f}\). We use \(p_{f}\) to denote the optimal distribution that minimizes the loss \(_{f}\), which represents the expected outcome of the model. If \(_{f}\) has multiple optimal distributions, we use \(p_{f}\) to denote an arbitrary optimal distribution. This choice does not harm the generality of our analysis, as the subsequent discussion is applicable to all optimal distributions. Currently, it is only established that the optimal distribution for the MLE loss \(_{}\) is the data distribution \(p_{}=p_{data}\). For other loss functions, the following theorem reveals a general property of the optimal distribution. With samples organized in descending order of their probabilities in the data distribution, i.e., \(p_{data}(x_{1})>p_{data}(x_{2})>>p_{data}(x_{i})>\), the optimal distribution of an arbitrary function \(f\) maintains this order as \(p_{f}(x_{1}) p_{f}(x_{2}) p_{f}(x_{i})\). The proofs for the theorems presented in this paper can be found in Appendix A.

**Theorem 1**.: _Given an arbitrary differentiable and increasing function \(f\), the optimal distribution \(p_{f}\) satisfies \(p_{f}(x_{1}) p_{f}(x_{2}) p_{f}(x_{i})\)._

In the following, we will further explore the properties of optimal distributions associated with specific loss functions.

### Loss with Convex Function

In certain text generation scenarios that require precise and deterministic outputs, it is beneficial for the model to converge to an optimal distribution that is sharper than the data distribution. Inthis section, we demonstrate that this objective can be achieved by employing convex functions as learning criterion.

The MLE loss function is based on log-probability, which is a concave function whose gradient decreases as the probability increases. The concave shape of the learning criterion prevents the model from assigning a large prediction probability to a single sample, since the marginal benefit diminishes as the probability increases. If function \(f\) is convex, then intuitively the model would converge to a sharper distribution. The following theorem validates this intuition, which shows that the optimal distribution \(p_{f}\) is a one-hot distribution when \(f\) is convex.

**Theorem 2**.: _If \(f\) is an increasing convex function on \(\), then the optimal distribution \(p_{f}\) is a one-hot distribution that \(p_{f}(x_{1})=1\) and \(p_{f}(x_{i})=0,i>1\)._

The one-hot characteristic of the optimal distribution is advantageous for text generation models seeking precise and deterministic outputs. For autoregressive models, they do not need the computationally expensive beam search decoding any more if the model distribution is nearly one-hot. For non-autoregressive models, they suffer from reduced performance under MLE due to their inability to fit the data distribution. However, fitting a one-hot optimal distribution is well within their capabilities, allowing these models to generate high-quality outputs.

However, the direct application of loss with convex functions in training text generation models comes with an inherent limitation, impeding its practical utility. Specifically, the gradient of the parameter \(\) tends to be very small when the prediction probability approaches \(0\), thereby rendering the training process inefficient. The gradient of \(\) can be formulated as follows:

\[_{f}()}{ }&=-_{x p_{data}(x)}[f^{}(p_{}(x) )(x)}{}]\\ &=-_{x p_{data}(x)}[f^{}(p_{}(x)) p _{}(x)_{t=1}^{T}(x_{t}))}{ }],\] (9)

where we have omitted the autoregressive history condition of \(p_{}(x_{t})\) for simplicity. The equation above indicates that the gradient is proportional to the sentence probability \(p_{}(x)\). In text generation models, the sentence probability \(p_{}(x)\) is the product of token probabilities \(p_{}(x_{t})\), which causes \(p_{}(x)\) to be typically close to \(0\), especially when the model is newly initialized.

To counter this effect, the gradient \(f^{}(p_{}(x))\) would need to approach infinity as \(p_{}(x)\) approaches \(0\). For instance, the log-probability function has the gradient \((x)}\), which offsets the impact of \(p_{}(x)\) such that \(f^{}(p_{}(x)) p_{}(x)=1\). However, for an increasing convex function \(f(p_{}(x))\) whose gradient increases with \(p_{}(x)\), its gradient must be bounded when \(p_{}(x)\) approaches \(0\), leading to an extremely small gradient update for the parameter \(\) during training. This inherent limitation of loss with convex functions poses a significant hurdle to their practical applications.

### Loss with Convex-composition Function

#### 3.3.1 Theoretical Analysis

In the preceding discussion, we illustrate that while convex functions can induce a desirable one-hot optimal distribution, their inherent limitations during training pose significant impediments to practical applications. Consequently, we consider a relaxation of the convexity requirement, with the objective of rendering the function \(f\) less concave. This approach aims to obtain an optimal distribution that is sharper than \(p_{data}\), thereby providing a practical solution that augments model performance without sacrificing training feasibility.

The standard loss function in maximum likelihood estimation is the negative log-probability, where log-probability is a concave function that yields a smooth optimal distribution. To render the learning criterion less concave, we propose a convex-composition approach that combines a convex function \(f\) with the original concave function \(g\). This composition yields the following loss function:

\[_{fg}()=-_{i=1}^{||}p_{data}(x_{i}) fg(p_ {}(x_{i})),\] (10)where \(f\) is an increasing convex function and \(g\) is an increasing concave function. The objective of this composition is to moderate the concavity of the overall loss function, thereby allowing for a sharper optimal distribution. The subsequent theorem and corollaries outline the theoretical properties associated with the optimal distribution under this function composition framework.

**Theorem 3**.: _Let \(f\) be an increasing convex function and \(g\) be an increasing concave function. Then, there exists a positive integer \(m\) such that the following inequalities hold:_

1. \(p_{fg}(x_{i}) p_{g}(x_{i})\) _for all_ \(i<m\)_,_
2. \(p_{fg}(x_{i}) p_{g}(x_{i})\) _for all_ \(i m\)_._

**Corollary 1**.: _The Shannon entropy of \(p_{fg}\) is less than or equal to the Shannon entropy of \(p_{g}\)._

**Corollary 2**.: _For any \(n\{1,2,...\}\), the sum of the probabilities of the \(n\) most probable samples increases: \(_{i=1}^{n}p_{fg}(x_{i})_{i=1}^{n}p_{g}(x_{i})\)._

Theorem 3 indicates that the convex-composition loss function tends to allocate higher probabilities to the more probable samples, while simultaneously diminishing the probabilities assigned to less probable ones, resulting in a sharper optimal distribution. Corollary 1 quantitatively establishes this observation, demonstrating that the incorporation of a convex function into the loss function effectively sharpens the optimal distribution, as evidenced by a reduction in the Shannon entropy of \(p_{fg}\) compared with \(p_{g}\). Furthermore, Corollary 2 reveals an increase in the cumulative probability of the \(n\) most probable samples. Consequently, text generation models are better equipped to capture the highly probable outputs without explicitly modeling the data distribution.

In the above analysis, we only assume the original loss \(g\) to be an increasing concave function. By imposing specific conditions on \(g\), we can derive more desirable properties from the optimal distribution \(p_{fg}\), as demonstrated in the subsequent theorem:

**Theorem 4**.: _Let \(f\) be an increasing convex function and \(g\) be an increasing concave function. If \(g\) satisfies \(g^{}(x) g^{}(x) g^{}(x)^{2}>0\) for all \(x(0,1)\), then the difference between \(p_{fg}\) and \(p_{g}\) exhibits a monotonic order: \(p_{fg}(x_{1})-p_{g}(x_{1}) p_{fg}(x_{2})-p_{g}(x_{2})... p_{fg}(x_ {m-1})-p_{g}(x_{m-1}) 0\), where \(m\) is the positive integer described in Theorem 3._

This theorem provides a more granular description of the relative difference between \(p_{fg}\) and \(p_{g}\). When \(p_{fg}\) decreases the probabilities assigned to less probable samples, it tends to reallocate this probability mass to the most probable samples. This enables text generation models to more accurately capture the most probable outputs. Note that the condition \(g^{}(x) g^{}(x) g^{}(x)^{2}>0\) is not overly restrictive. For instance, the loss function \(g=\) in MLE readily fulfills this condition:

\[^{}(x)^{}(x)-log^{}(x)^{2}= }-(-})^{2}=}>0.\] (11)

#### 3.3.2 Practical Applications

The preceding theoretical analysis highlights the effectiveness of function composition. Here we turn to its practical applications and give some examples of convex-composition loss functions. The loss function in maximum likelihood estimation is typically the log-probability, and length normalization is often applied in practical usage, resulting in the loss \(g(p_{}(x))=(x))}{T}\), where \(T\) denotes the sentence length. Common choices for increasing convex functions on \((-,0]\) include the exponential function \(f(x)=e^{kx},k 0\) and the power function \(f(x)=-(-x)^{k},0 k 1\). Through function composition, we can derive the following losses:

\[fg(p_{}(x))=p_{}(x)^{},&f(x)=e^{kx}\\ -(-(x))}{T})^{k},&f(x)=-(-x)^{k}.\] (12)

The gradient of the convex-composition function is \(f^{}(g(p_{}(x))) g^{}(p_{}(x))\). Compared to the gradient of the original loss \(g^{}(p_{}(x))\), it has an additional term \(f^{}(g(p_{}(x)))\) that can be interpreted as a weight for the loss. Given that \(f\) is a convex function and \(g\) is an increasing function, the weight \(f^{}(g(p_{}(x)))\) is larger for more probable samples, thereby directing the model's focus towards generating outputs with high probabilities. Specifically, the loss weights \(f^{}(g(p_{}(x)))\) associated with Equation 12 are:

\[f^{}(g(p_{}(x)))=k p_{}(x)^{},&f( x)=e^{kx}\\ k(-(x))}{T})^{k-1},&f(x)=-(-x)^{k},\] (13)

where the exponential function weights the sample by the prediction probability, and the power function weights the sample by the log-probability.

In practical applications, label smoothing [59; 63] is a widely used regularization technique for text generation models. The smoothing loss and log-probability loss are typically combined using a fixed hyperparameter \(_{ls}\). To preserve the ratio of smoothing loss to log-probability loss, we also apply the weight \(f^{}(g(p_{}(x)))\) to the smoothing loss before interpolating it with the convex-composition loss.

## 4 Experiments

To validate the practical advantages of loss functions with sharper optimal distributions, we conduct experiments on basic autoregressive (AR) models, non-autoregressive (NAR) models, and large language models (LLMs). We evaluate their performance on two representative closed-ended text generation tasks, including neural machine translation and text summarization. Following the theoretical analysis in previous sections, we combine the exponential function with standard log-probability, i.e. \(_{f}()=-_{x p_{data}(x)}[p_{}(x)^{}]\), as our training objective in the following experiments. We have also attempted to combine the power function with log-probability as training objective. We found that the power form encountered some difficulties during training, leading to worse performance compared to the exponential form. Due to the space limit, we leave the results under this setting in Appendix E.

Our theoretical analysis suggests that the model trained by convex-composition loss tends to predict a sharper distribution, in which the probability mass is more heavily allocated to the most probable samples. Such property leads the model becoming more confident about its prediction and facilitates the de-facto maximum a posteriori (MAP) decoding framework in closed-ended text generation tasks. In the following, we will discuss and validate the effects of convexity in the context of AR models, NAR models, and LLMs respectively. More details of settings can be found in Appendix B.

Figure 1: Translation quality (BLEU) of autoregressive model as beam size varies on WMT14 EN+DE test set.

Figure 2: Translation quality (BLEU) and prediction confidence (Output NLL) of different NAT models as the exponent \(k\) varies on WMT14 EN-DE test set.

### Effects of Convexity on Autoregressive Models

In the context of autoregressive models, a model distribution trained with a convex-composition loss tends to exhibit fewer modes and a sharper distribution, thereby facilitating the task of approximate search algorithms in identifying the most likely output. We validate this conjecture by investigating the performance of greedy and beam search when trained with standard MLE and convex-composition loss in translation and summarization tasks. For translation task, we vary the beam size from \(\{1,2,3,5,8\}\), where beam size 1 can be considered as greedy search.

Figure 1 visualizes the results in terms of BLEU , with precise numerical values given in Table 1. We observe a consistent improvement in translation quality when using convex-composition losses compared to MLE, and a similar trend is observed in summarization tasks as detailed in Table 2. These results provide experimental support that the composition with convex function promotes those approximate searching algorithms to perform argmax decoding. Meanwhile, Table 1 exhibits a diminishing gap between greedy search and beam search when equipped with convex-composition loss. This outcome can be attributed to the efficacy of the convex function in reducing the complexity of the model distribution, as described in Theorem 3. Such property amplifies the potential of lightweight approximate decoding algorithms within the autoregressive structure, a desirable trait in the context of modern, computation-intensive autoregressive neural networks.

### Effects of Convexity on Non-autoregressive Models

Non-autoregressive models face the challenge of multi-modality, where fitting a data distribution with multiple target modes exceeds the capabilities of NAR models. Therefore, the mode collapse property of convex-composition loss would be beneficial to NAR models. Likelihood training will force the model to ignore sequential dependency, resulting in disfluency in its output (e.g., token repetition and omission). In contrast, convex-composition loss would encourage model to allocate most of its probability mass to the best among all proper candidates. Such property is able to help NAR model avoid generating a mixture of modes, thereby alleviating disfluency issues.

To demonstrate its effectiveness, we investigate the performance of convex-composition loss on three representative NAR models, including Vanilla-NAT , CMLM  and CTC . Considering most of the NAR researches are restricted in the field of translation, we only conduct experiments on translation dataset. In addition to translation quality, we also assess the prediction confidence and generation fluency of NAR outputs. The prediction confidence is measured with negative log-likelihood of its generation and the fluency is measured by an external pre-trained language model 4. We use the PPL value reported by the language model to quantify the fluency of generation. The exponent hyperparameter \(k\) is manipulated to adjust the convexity of our composite loss function.

    &  &  \\  & **greedy** & **beam5** & \(\) & **greedy** & **beam5** & \(\) \\  Transformer  & 26.48 & 27.57 & 1.09 & 29.78 & 31.21 & 1.43 \\ 
**Transformer + Convex** & **26.92** & **27.78** & 0.86 & **30.32** & **31.33** & 1.01 \\   

Table 1: BLEU scores of autoregressive models on WMT14 EN\(\)DE test set with different decoding strategies.

    &  &  \\  & **RG-1** & **RG-2** & **RG-L** & **RG-1** & **RG-2** & **RG-L** \\  Transformer  & 39.03 & 15.98 & 35.88 & 31.04 & 10.68 & 24.77 \\
**Transformer + Convex** & **39.56** & **16.84** & **36.26** & **31.55** & **11.13** & **25.09** \\   

Table 2: ROUGE scores on CNN/DailyMail and XSum test sets. RG-1, RG-2, RG-L stand for ROUGE-1, ROUGE-2 and ROUGE-L scores.

The results are shown in Figure 2 and Table 3. We observe a consistent improvement in translation quality across all NAT models with a maximum improvement of 9+ BLEU points on CMLM. Meanwhile, Figure 2 implies that prediction confidence significantly gains and the gain increases as \(k\) gets larger. Such phenomenon reveals a descending trend of model entropy as applying convex function on loss, which is consistent with Corollary 1. More importantly, we note a strong correlation between model entropy and generation fluency in Table 4, providing clear evidence that the mode collapse property of convex function indeed relieves NAR model from multi-modality problem.

### Effects of Convexity on Large Language Models

Large language models have demonstrated remarkable capabilities in various applications, including both open-ended and closed-ended text generation tasks. For open-ended tasks, stochastic decoding methods such as temperature sampling are commonly employed to produce responses. In contrast, deterministic decoding methods like beam search are favored for closed-ended tasks like machine translation [22; 69; 30]. Given that the convex-composition loss enhances the model's ability to identify highly probable sentences, incorporating this loss function into the LLMs' training process would be beneficial to closed-ended generation tasks.

To demonstrate its effectiveness, we assess the performance of LLMs in machine translation (Table 5) and summarization (Table 6). Table 5 reveals that the LLaMA-7B model, incorporating convex-composition loss, surpasses the baseline model across all language pairs, achieving an average improvement of 1.84 BLEU. Likewise, the LLaMA-13B model with convex-composition loss outperforms the baseline model in three out of four language pairs. Table 6 further demonstrates the effectiveness of our method in text summarization. Due to memory limitations, we are only able to decode the text summarization dataset using the LLaMA-7B model.

## 5 Related Work

Alternative Loss FunctionsMaximum likelihood estimation has become the dominant approach for learning text generation models, but it also comes with certain limitations. Various alternative loss functions have been proposed to improve the training process from different perspectives. Regarding the exposure bias problem  that autoregressive models are exposed to different distributions during training and inference, Bengio et al. , Mihaylova and Martins , Zhang et al.  proposed to reduce this gap by sampling from the model's own predictions during training. Another issue with text generation models is text degeneration: output text may be bland, incoherent, or gets stuck in repetitive loops . To avoid text degeneration, Dieng et al.  proposed a learning criterion termed

  
**k-th Power** & **1** & **2** & **3** & **5** & **8** \\ 
**Confidence (Output NLL)**\(\) & 20.57 & 13.72 & 10.09 & 6.85 & 4.88 \\
**Fluency (External PPL)**\(\) & 939.34 & 481.08 & 315.54 & 213.84 & 218.68 \\   

Table 4: Prediction confidence (Output NLL) and generation fluency (External PPL) of CMLM on WMT14 EN-DE test set.

  
**Model** & **Speedup** &  &  \\  & & **BLEU** & **COMET** & **BLEU** & **COMET** \\  Transformer  & 1.0\(\) & 27.57 & 82.76 & 31.21 & 82.98 \\  Vanilla-NAT  & 15.6\(\) & 10.41 & 40.69 & 16.01 & 56.03 \\
**Vanilla-NAT + Convex** & 15.6\(\) & **16.74** & **57.25** & **22.63** & **68.83** \\  CMLM  & 15.0\(\) & 11.22 & 43.62 & 15.26 & 56.63 \\
**CMLM + Convex** & 15.0\(\) & **20.45** & **65.99** & **19.11** & **63.54** \\  CTC  & 14.7\(\) & 16.98 & 54.77 & 20.53 & 66.26 \\
**CTC + Convex** & 14.7\(\) & **23.34** & **67.38** & **26.68** & **74.75** \\   

Table 3: BLEU and COMET scores on WMT14 EN\(\)DE test set.

reflective likelihood to penalize incoherent outputs, and Welleck et al.  proposed unlikelihood training that forces unlikely generations to be assigned lower probability by the model. Additionally, to address the discrepancy between likelihood training and evaluation metrics, loss functions that more directly optimize evaluation metrics are proposed. Ranzato et al.  utilized the reinforcement learning technique to train recurrent neural networks with sequence level objectives. Shen et al.  proposed to optimize evaluation metrics with minimum risk training. Norouzi et al. , Edunov et al.  incorporated evaluation metrics into the maximum likelihood training objective. There are also efforts on learning a more focused distribution for text generation models [37; 71; 56]. However, these approaches primarily reformulate the loss function at the word level, which is insufficient for guiding the model towards identifying high-probability sentences at the sentence level. In contrast, our method explicitly trains the model to concentrate on generating highly probable sentences.

**Reinforcement Learning** Our work aligns closely with reinforcement learning (RL) based training techniques for text generation [67; 58]. While RL techniques typically maximize the expected reward by concentrating the probability mass on the sequence with the highest reward, our approach strives to put all the probability mass on the most likely sequence. RL allows for text generation models to optimize discrete evaluation metrics, which have wide usage in text generation tasks, including machine translation [42; 3], text summarization , image captioning , dialogue generation , etc. Furthermore, RL can be integrated with Generative Adversarial Networks  and can leverage human feedback for training [57; 36].

**Loss Functions for NAR Models** The limitation of maximum likelihood estimation is amplified in non-autoregressive (NAR) models since they inherently lack the capability to fit the data distribution . To address this issue, researchers have developed loss functions specifically designed for NAR models, guiding them towards generating coherent text. Shao et al. [50; 52], Ding et al.  proposed to train NAR models with sequence-level objective functions. Ghazvininejad et al. , Du et al.  relaxed the alignment restriction in the cross-entropy loss. Shao et al. , Shao and Feng , Ma et al.  proposed n-gram based differentiable training objectives to optimize n-gram prediction accuracy. However, these methods lack theoretical guarantees for the shape of optimal distribution.

## 6 Conclusion

This paper investigates the theoretical properties and practical applications of a novel class of training objectives based on convex functions. Our findings show that convex functions can sharpen the optimal distribution, enabling text generation models to focus on highly probable outputs without having to estimate the entire data distribution. Experiments on various text generation tasks and models verify our theoretical analysis and demonstrate the practical effectiveness of our approach.

## 7 Acknowledgement

We thank the anonymous reviewers for their insightful comments.

  
**Model** & **RG-1** & **RG-2** & **RG-L** & **AVG** \\  LLaMA-7B & 28.66 & 12.49 & 26.37 & 22.51 \\
**LLaMA-7B + Convex** & **32.76** & **14.67** & **30.00** & **25.81** \\   

Table 6: ROUGE scores of Alpaca fine-tuned large language models on CNN/DailyMail.

  
**Model** & **EN-DE** & **DE-EN** & **EN-ZH** & **ZH-EN** & **AVG** \\  LLaMA-7B & 25.42 & 17.93 & 13.86 & 13.17 & 17.59 \\
**LLaMA-7B + Convex** & **27.57** & **19.88** & **15.00** & **15.28** & **19.43** \\  LLaMA-13B & **29.35** & 21.74 & 15.58 & 16.27 & 20.74 \\
**LLaMA-13B + Convex** & 28.75 & **22.20** & **16.25** & **20.08** & **21.82** \\   

Table 5: BLEU scores of Alpaca fine-tuned large language models on WMT22 test sets.