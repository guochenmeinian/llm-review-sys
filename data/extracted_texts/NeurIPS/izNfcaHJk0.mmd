# Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-offs in Distributed Mean Estimation

Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-offs in Distributed Mean Estimation

 Wei-Ning Chen

Stanford University

wnchen@stanford.edu

&Dan Song

Stanford University

songdan@stanford.edu

&Ayfer Ozgur

Stanford University

aozgur@stanford.edu

&Peter Kairouz

Google Research

kairouz@google.com

###### Abstract

Privacy and communication constraints are two major bottlenecks in federated learning (FL) and analytics (FA). We study the optimal accuracy of mean and frequency estimation (canonical models for FL and FA respectively) under joint communication and \((,)\)-differential privacy (DP) constraints. We consider both the central and the multi-message shuffled DP models. We show that in order to achieve the optimal \(_{2}\) error under \((,)\)-DP, it is sufficient for each client to send \((n(,^{2}))\) bits for FL and \(((n(,^{2})))\) bits for FA to the server, where \(n\) is the number of participating clients. Without compression, each client needs \(O(d)\) bits and \(O( d)\) bits for the mean and frequency estimation problems respectively (where \(d\) corresponds to the number of trainable parameters in FL or the domain size in FA), meaning that we can get significant savings in the regime \(n(,^{2})=o(d)\), which is often the relevant regime in practice.

We propose two different ways to leverage compression for privacy amplification and achieve the optimal privacy-communication-accuracy trade-offs. In both cases, each client communicates only partial information about its sample and we show that privacy is amplified by randomly selecting the part contributed by each client. In the first method, the random selection is revealed to the server, which results in a central DP guarantee with optimal privacy-communication-accuracy trade-offs. In the second method, the random data parts from the clients are shuffled by a secure shuffler resulting in a multi-message shuffling scheme with the same optimal trade-offs. As a result, we establish the optimal three-way trade-offs between privacy, communication, and accuracy for both the central DP and multi-message shuffling frameworks.

## 1 Introduction

In the basic setting of federated learning (FL)  and analytics (FA), a server wants to execute a specific learning or analytics task on raw data that is kept on clients' devices. Consider, for example, model updates in FL or histogram estimation in FA, both of which can be modeled as a distributed mean estimation problem. Clients communicate targeted messages to the server and the privacy of the users' data is ensured (in terms of explicit differential privacy (DP)  guarantees) by adding carefully calibrated noise to the computed mean at the server before releasing it to downstream modules (e.g., the server computes the average model update and corrupts it with the addition of noise). This is called the trusted server or central DP model, as it entrusts the central server with privatization and is one of the most common ways in which federated learning and analytics are implemented today 1.

In this paper, we ask the following question: given that the server needs to privatize the mean, can the clients communicate "less information" to the server? More precisely, can we leverage the fact that the server only needs to output a noisy (approximate) estimate of the mean to reduce the communication load without sacrificing accuracy? In recent years, there has been significant interest in the central DP model  as well as communication efficiency and privacy for FL and FA under different models, including local DP , shuffle  and distributed DP ; however, this basic question has remained open.

One natural way to reduce communication is to have clients communicate only partial information about their samples. For example, in the case of model updates, each client can update only a subset of the model coefficients. In histogram estimation, information about a client's sample can be "split" into multiple parts, and the client can communicate only one part. However, this results in less information at the server, or effectively fewer samples to estimate the target quantity, e.g., each model coefficient is now updated only by a subset of the clients. A quick calculation reveals that this increases the sensitivity of the estimate to each user's sample and therefore requires the addition of larger noise at the server to achieve the same privacy level. Hence, reducing communication reduces accuracy for the same privacy guarantee.

We circumvent this challenge with a simple but insightful observation: when each client communicates only partial information about its sample, we can amplify privacy by randomly selecting the part contributed by each client. This random selection is hidden from a downstream module which has only access to the estimate revealed by the server, which leads to privacy amplification. Privacy amplification by subsampling has been studied in  but usually refers to the selection of a random subset of the clients (from a larger pool of available clients). In our case, it is the "piece of information" that is randomly selected at each client. We call this gain privacy amplification via compression as it emerges from the fact that each client does not fully communicate its sample. We use it to establish the optimal communication-privacy-accuracy trade-off for central DP. Note that this same type of gain cannot be leveraged in the local DP model where the server is untrusted. The server needs to know the random selection at each client to construct an estimate. Indeed, in the local DP model, the privacy-accuracy trade-off is known to be significantly worse than the central DP model (see Table 1).

This naturally leads to a follow-up question: can we leverage privacy amplification via compression and achieve the same three-way trade-off by using secure aggregation  and shuffling  type models which hide information from the server? For secure aggregation, the three-way trade-off has been studied in  and the communication cost is significantly larger than the communication cost for central DP proved in this paper (see Table 1). For shuffling, the optimal communication cost has been posed as an open problem in . We resolve this problem by showing that the optimal central DP trade-off can also be achieved with a multi-message shuffling scheme, establishing the optimal communication cost. (We note that a similar result has been concurrently and independently proved in  under the shuffle DP setting.) As before, our scheme leverages a privacy amplification gain. Each client communicates partial information about its sample; the identity of the message is erased by the secure shuffler, and hence the untrusted server does not know which part is contributed by each client. We show that to achieve the optimal trade-off, it is critical for each client to split its information into multiple messages and employ multiple shuffling rounds by carefully splitting the privacy budget across different rounds. In contrast, the linearity of secure aggregation requires all participating clients to communicate consistent information (same parts), hence precluding privacy amplification by compression. See Table 1 for a detailed comparison.

**Our contributions.** We study distributed mean and frequency estimation as canonical building blocks for FL and FA. We consider both the central DP and the multi-message shuffling models. We characterize the order-optimal privacy-accuracy-communication trade-offs for distributed mean estimation and provide an achievable scheme for frequency estimation (in Appendix A) under the central DP model. Our results reveal that privacy and communication efficiency can be achieved simultaneously with no additional penalty for accuracy. In particular, we show that \((n(,^{2}))\)and \(((n(,^{2})))\) bits of (per-client) communication are sufficient to achieve the order-optimal error under \((,)\)-privacy for mean and frequency estimation respectively, where \(n\) is the number of participating clients. Without compression, each client needs \(O(d)\) bits and \( d\) bits for the mean and frequency estimation problems respectively (where \(d\) is the number of trainable parameters in FL or the domain size in FA), which means that we can get significant savings in the regime \(n^{2}=o(d)\) (assuming \(=O(1)\)). We note that this is often the relevant regime not only for cross-silo but also for cross-device FL/FA. For instance, in practical FL, \(d\) usually ranges from \(10^{6}\) to \(10^{9}\), and \(n\), the _per-epoch_ sample size, is usually much smaller (e.g., of the order of \(10^{3}\) to \(10^{5}\)). For distributed mean estimation, we show that the central DP trade-off can also be achieved with a multi-message shuffling scheme (within a \( d\) factor in communication cost). Hence our paper establishes the three-way trade-off between privacy, communication, and accuracy for both the central DP and multi-message shuffling frameworks, both of which were open problems in the prior literature. Compared with local DP where \(1\) bit is sufficient when \(=O(1)\), this shows that central/shuffling DP has a larger communication cost but can achieve much smaller error (by a factor of \(n\)) and hence is usually preferable in practical applications. Compared with distributed DP where the server aggregates local (encoded) messages with secure multi-party computation (e.g., [23; 8; 34]), we can improve the communication cost by a factor of \(n\), therefore showing that the communication cost can be reduced with a trusted server or shuffler. We summarize the comparisons of our main results to local and distributed DP in Table 1.

**Notation.** Throughout this paper, we use \([m]\) to denote the set of \(\{1,...,m\}\) for any \(m\). Random variables (vectors) \((X_{1},...,X_{m})\) are denoted as \(X^{m}\). We also make use of Bachmann-Landau asymptotic notation, i.e., \(O,o,,,\).

## 2 Problem Formulation

We first present the distributed mean estimation (DME)  problem under differential privacy. Note that DME is closely related to federated learning with SGD (or similar stochastic optimization methods, such as FedAvg ), where in each iteration, the server updates the global model by a noisy mean of the local model updates. This noisy estimate is typically obtained by using a DME scheme, and thus one can easily build a distributed DP-SGD scheme (and hence a private FL scheme) from a differentially private DME scheme. Moreover, as shown in , as long as we have an unbiased estimate of the gradient at each round, the convergence rates of SGD (or DP-SGD) depend on the \(_{2}\) estimation error.

Distributed mean estimation.Consider \(n\) clients each with local data \(x_{i}^{d}\) that satisfies \(\|x_{i}\|_{2} C\) for some constant \(C>0\) (one can think of \(x_{i}\) as a clipped local gradient). A server wants to learn an estimate \(\) of the mean \((x^{n})_{i}x_{i}\) from \(x^{n}=(x_{1},,x_{n})\) after communicating with the \(n\) clients. Toward this end, each client locally compresses \(x_{i}\) into a \(b\)-bit message \(Y_{i}=_{i}(x_{i})\) through a local encoder \(_{i}:\) (where \(|| 2^{b}\) and sends it to the central server, which upon receiving \(Y^{n}=(Y_{1},,Y_{n})\) computes an estimate \(=(Y^{n})\) that satisfies the following differential privacy:

   & Communication (bits) & \(_{2}^{2}\) error \\  Local DP [32; 42] & \(()\) & \((,)})\) \\  Distributed DP (with SecAgg)  & \((n^{2}(,^{2}))\) & \(((^{2},)})\) \\  Central DP (Theorem 4.4) & \((n(,^{2}))\) & \(O((^{2},)})\) \\  Shuffle DP (Theorem 5.3, ) & \((n(d)(,^{2}))\) & \(O((^{2},)})\) \\  

Table 1: Comparison of the communication costs of \(_{2}\) mean estimation under local, distributed, central, and shuffle DP (with \(\) terms hidden). Compared to local DP, we see that error under central DP decays much faster (e.g., \(1/n^{2}\) as opposed to \(1/n\)); compared to distributed DP with secure aggregation, our schemes achieve similar accuracy but saves the communication cost by a factor of \(n\).

**Definition 2.1** (Differential Privacy).: _The mechanism \(\) is \((,)\)-differentially private if for any neighboring datasets \(x^{n}(x_{1},...,x_{i},...,x_{n})\), \(x^{ n}(x_{1},...,x^{}_{i},...,x_{n})\), and measurable \(\),_

\[\{|x^{n}\} e^{} \{|x^{ n}\}+,\]

_where the probability is taken over the randomness of \(\)._

Our goal is to design schemes that minimize the \(_{2}^{2}\) estimation error:

\[_{(_{1}(),...,_{n}(),( ))}_{x^{n}}[\|(_{1}(x_{1}),...,_{n}(x_{n}))-(x^{n})\|_{2}^{2}],\]

subject to \(b\)-bit communication and \((,)\)-DP constraints.

Distributed frequency estimation.Similarly, frequency estimation can also be formulated as a mean estimation problem but with sparse (one-hot) vectors. Let each user \(i\) hold an item \(x_{i}\) in a size \(d\) domain \(\). The server aims to estimate the histogram of the \(n\) items. Without loss of generality, we can assume that \(\{e_{1},...,e_{d}\}\{0,1\}^{d}\) (where \(e_{j}\) is the \(j\)-th standard basis vector in \(^{d}\)), i.e., each item is expressed as a one-hot vector. Then, the histogram of the \(n\) items can be expressed as \((x^{n})_{i[n]}x_{i}\). Similar to the mean estimation problem, clients locally compute and then send \(Y_{i}=_{i}(x_{i})\) (for some \(\) such that \(|| 2^{b}\)), and the central server computes the estimate \(=(y^{n})\). Our goal is to design schemes that minimize the \(_{2}^{2}\) or \(_{1}\) error2:

\[_{(_{1}(),...,_{n}(),( ))}_{x^{n}}[\|(_{1}(x_{1}),...,_{n}(x_{n}))-(x^{n})\|],\]

subject to communication and DP constraints (where \(\|\|\) can be \(_{1}\) or \(_{2}^{2}\)).

## 3 Related Works

Federated learning and distributed mean estimation.Federated learning [64; 68; 60] emerges as a decentralized machine learning framework that provides data confidentiality by retaining clients' raw data on edge devices. In FL, communication between clients and the central server can quickly become a bottleneck , so previous works have focused on compressing local model updates via gradient quantization [68; 10; 48; 74; 79; 77; 24], sparsification [18; 56; 41]. To further enhance data security, FL is often combined with differential privacy [38; 1; 9]. Among these works,  also employs gradient sparsification (or gradient subsampling) to reduce the problem dimensionality. However, the sparsification takes place _after_ the aggregation of local gradients, so the randomness introduced during sparsification cannot be leveraged to amplify the differential privacy guarantee. As a result, this approach leads to a suboptimal trade-off between privacy and communication compared to our scheme.

Note that in this work, we consider FL (or more specifically, the distributed mean estimation) under a _central_-DP setting where the server is trusted, which is different from the local DP model [63; 37; 70; 76; 22; 32] and the distributed DP model with secure aggregation [23; 21; 59; 8; 33; 34].

A key step in our mean estimation scheme is pre-processing the local data via Kashin's representation . While various compression schemes, based on quantization, sparsification, and dithering have been proposed in the recent literature, Kashin's representation has also been explored in a few works for communication efficiency [47; 73; 29; 71] and for LDP  and is particularly powerful in the case of joint communication and privacy constraints as it helps spread the information in a vector evenly in every dimension.

Distributed frequency estimation and heavy hitters.Distributed frequency estimation (a.k.a. histogram estimation) is another canonical task that has been heavily studied under a distributed setting with DP. Prior works either focus on 1) the local DP model with or without communication constraints, e.g., [20; 19; 25; 26; 57] (under an \(_{}\) loss for heavy hitter estimation) and [58; 80; 76; 6; 5; 32; 46; 72; 45] (under an \(_{1}\) or \(_{2}\) loss), or 2) the central DP model _without_ communication constraints [38; 52; 65; 27; 13; 81; 36]. As suggested in [37; 3; 2; 4; 16], compared to central DP, local DP models usually incur much larger estimation errors and can significantly decrease the utility. In this work, we consider central DP but with explicit communication constraints.

Local DP with shuffling.A recent line of works  considers _shuffle_-DP, showing that one can significantly boost the central DP guarantees by randomly shuffling local (privatized) messages. In this work, we show that the same shuffling technique can be used to achieve the optimal central DP error with nearly optimal communication cost. Therefore, we can obtain the same level of central DP with small communication costs while weakening the security assumption: achieving the optimal communication cost (under central DP) only requires a secure shuffler (as opposed to a fully trusted central server).

## 4 Distributed Mean Estimation

In this section, we present a mean estimation scheme that achieves the optimal \(_{}(d}{n^{2}^{2}})\) error under \((,)\)-DP while only using \((n^{2})\) bits of per-client communication.

We first consider a slightly simpler, discrete setting with \(_{}\) geometry (as opposed to the \(_{2}\) mean estimation stated in Section 2): assume each client observes \(x_{i}\{-c,c\}^{d}\) where \(c>0\) is a constant, and a central server aims to estimate the mean \((x^{n}):=_{i=1}^{n}x_{i}\) by minimizing the \(_{2}^{2}\) error subject to the privacy and communication constraints. We argue later that solutions to the above \(_{}\) problem can be used for \(_{2}\) mean estimation by applying Kashin's representation.

To solve the aforementioned \(_{}\) mean estimation problem, first observe that each client's local data can be expressed in \(d\) bits since each coordinate of \(x_{i}\) can only take values in \(\{c,-c\}\). To reduce the communication load to \(o(d)\) bits, each client adopts the following subsampling strategy: for each coordinate \(j[d]\), client \(i\) chooses to send \(x_{i}(j)\) to the server with probability \(\). We assume that this subsampling step is performed with a seed shared by the client and the server3, hence the server knows which coordinates are communicated by each client. Therefore upon receiving the client messages, it can compute the mean of each coordinate and privatize it by adding Gaussian noise. The key observation we leverage is that the randomness in the compression algorithm can be used to amplify privacy or equivalently reduce the magnitude of the Gaussian noise that is needed for privatization. Note that such randomness needs to be kept private from an adversary as the privacy guarantee of the scheme relies on it.

We summarize the scheme in Algorithm 1 and state its privacy and utility guarantees in the following theorem.

**Theorem 4.1** (\(_{}\) mean estimation.).: _Let \(x_{1},...,x_{n}\{-c,c\}^{d}\) and \(,>0\). There exists a_

\[^{2}=O((1/)}{n^{2}^{2}}+d( (d/)+)(d/)}{n^{2}^{2}})\] (1)

_such that Algorithm 1 is \((,)\)-DP and the \(_{2}^{2}\) estimation error of \(\) is at most_

\[[\|-\|_{2}^{2}] }{n}+d^{2}\] \[=Oc^{2}}{nb}+c^{2}(d/)}{n^ {2}b^{2}}\] (2) \[+d^{2}((1/)+)(d/)}{n^{ 2}^{2}}.\] (3)

_In addition, the (average) per-client communication cost is \( d=b\) bits, and Algorithm 1 yields an unbiased estimator of \(\)._

**Remark 4.2** (Unbiasedness).: _Note that for mean estimation, we usually want the final mean estimator to be unbiased since standard convergence analyses of SGD  require an unbiased estimate of the true gradient in each optimization round. Given that our proposed mean estimation schemes (Algorithm 1 and Algorithm 2 in the next section) are all unbiased, we can combine them with SGD/federated averaging and readily apply  to obtain a convergence guarantee for the resulting communication-efficient DP-SGD._

For the \(_{2}\) mean estimation task formulated in Section 2, we pre-process local vectors by first computing their Kashin's representations and then performing randomized rounding . Specifically, if \(x_{i}\) has \(_{2}\) norm bounded by \(C\), then its Kashin's representation (with respect to a tight frame \(K^{d D}\) where \(D=(d)\)) \(_{i}\) has bounded \(_{}\) norm: \(\|_{i}\|_{} c=O(})\) and satisfies \(x_{i}=K_{i}\). This allows us to convert the \(_{2}\) geometry to an \(_{}\) geometry. Furthermore, by randomly rounding each coordinate of \(_{i}\) to \(\{-c,c\}\) (see for example ), we can readily apply Algorithm 1 and obtain the following result for \(_{2}\) mean estimation as a corollary:

**Corollary 4.3** (\(_{2}\) mean estimation).: _Let \(x_{1},...,x_{n}_{2}(C)\) (i.e., \(\|x_{i}\|_{2} C\) for all \(i[n]\)). Then for any \(,>0\), Algorithm 1 combined with Kashin's representation and randomized rounding yields an \((,)\)-DP unbiased estimator for \(\) with \(_{2}^{2}\) estimation error bounded by_

\[O(}{nb}+d^{2}(1/)}{n^{2}b^{ 2}}}_{()}+d((d/)+)(d/ )}{n^{2}^{2}}}_{()}).\] (4)

The first term \(()\) in the estimation error in Corollary 4.3 is the error due to compression, and the second term \(()\) is the error due to privatization (which is order-optimal under \((,)\)-DP up to an additional \((d/)\) factor as we discuss in Section 4.2). In particular, if we ignore the poly-logarithmic terms and assume \(=O(1)\), the privatization error \(()\) can be simplified to \((}{n^{2}^{2}})\), which dominates the total \(_{2}^{2}\) error when \(b=_{}((n^{2}, ))\), i.e. in this regime the total \(_{2}^{2}\) error is order-wise equal to the optimal centralized DP error \(()\). This implies that no more than \(b=_{}((n^{2}, ))\) bits per client are needed to achieve the order-optimal \(_{2}^{2}\) error.

In the next section, we introduce a modification to Algorithm 1, which allows the removal of the \(()\) term in the communication cost.

### Dimension-free communication cost

In order to remove the dependence on the dimension \(d\) in the communication cost \(b=_{}((n^{2}, ))\) from the previous section, we need to improve the performance of our scheme in the _small-sample_ regime \(n^{2}=o()\). Equivalently, we want to be able to achieve the centralized DP performance by using only \(b=_{}(n^{2})\) bits per client when \(n=o()\). Assuming \( 1\), note that this implies that the total communication bandwidth of the system \(nb=o(d)\), i.e. the server can receive information about at most \(nb=n^{2}^{2}=o(d)\) coordinates. We show that in this regime the performance of the scheme can be improved by a priori restricting the server's attention to a subset of the coordinates.

We make the following modification to Algorithm 1: before performing Algorithm 1, the server randomly selects \(d^{} O((d,n^{2}^{2}))\) coordinates and only requires clients to run Algorithm 1 on them. We present the modified scheme in Algorithm 2 and summarize its performance in Theorem 4.4.

Similarly, we can obtain the following \(_{2}\) mean estimation via Kashin's representations:

**Theorem 4.4** (\(_{2}\) mean estimation.).: _Let \(x_{1},...,x_{n}_{2}(C)\) (i.e., \(\|x_{i}\|_{2} C\) for all \(i[n]\)), \(,>0\), and \(d^{}=(d,nb,^{2}}{((1/)+ )(d/)})\). Then there exists a_

\[^{2}=O((1/)}{d^{}n^{2}^{2}}+ {C^{2}d^{}((1/)+)(d^{}/)}{dn^{2} ^{2}}).\] (5)

_such that Algorithm 2 is \((,)\)-DP. In addition, the (average) per-client communication cost is \( d=b\) bits, and the \(_{2}^{2}\) estimation error is at most_

\[O((d(d/)}{nb},d(d/)( (1/)+)}{n^{2}^{2}})).\] (6)

**Corollary 4.5**.: _As long as \(b=(}{(1/)+})\), the \(_{2}^{2}\) error of mean estimation is_

\[O(d(d/)((1/)+)}{n^{2} ^{2}}).\]

As suggested by Corollary 4.5, we see that when \(=O(1)\), \(b=(n^{2})\) bits per client are sufficient to achieve the order-optimal \(_{}(d}{n^{2}^{2}})\) error (even in the small sample regime \(n\)), i.e. the communication cost of the scheme is independent of the dimension \(d\).

### Lower bounds

In this section, we argue that the estimation error in Theorem 4.4 is optimal up to an \((d/)\) factor. Specifically, Theorem 5.3 of  shows that any \(b\)-bit _unbiased_ compression scheme will incur \((d}{nb})\) error for the \(_{2}\) mean estimation problem (even when privacy is not required). This matches the first term in (6) up to a logarithmic factor.

On the other hand, the centralized Gaussian mechanism (under a central \((,)\)-DP) achieves \(O(d(1/)}{n^{2}^{2}})\) MSE  (which is order-optimal in most parameter regimes; see the lower bounds in Theorem 3.1 of  or Proposition 23 of ). Hence, we can conclude that the total communication received by the server has to be at least \((n^{2}^{2})\) bits in order to achieve the same error as the Gaussian mechanism. Therefore, the (average) per-client communication cost has to be at least \((n^{2})\) bits. Hence we conclude that Algorithm 2 is optimal (up to a logarithmic factor).

For completeness, we state the communication lower bound in the following theorem:

**Theorem 4.6** (Communication lower bound for mean estimation under central DP).: _Let \(x_{1},...,x_{n}_{2}(C)\). Let \(Y_{1},...,Y_{n}\) be any \(b\)-bit local reports generated from a (possibly interactive) compressor and be unbiased in the sense that \([_{i}Y_{i}]=_{i}x_{i}\). Then if \([\|_{i}Y_{i}-_{i}x_{i} \|_{2}^{2}] O(d(1/)}{n^{2} ^{2}}),\) it holds that \(b=(}{(1/)}).\)_

Finally, we remark that the logarithmic gap between the upper and lower bounds may be due to the specific composition theorem (Theorem III.3 of ) we use in our proof, which is simpler to work with but possibly slightly weaker. However, in our experiments, we compute and account for all privacy budgets with Renyi DP , and hence can obtain better constants compared to our theoretical analysis.

## 5 Achieving the Optimal Trade-off via Shuffling

In Section 4 and Section A, we see that the communication cost can be reduced to (\((n^{2})\) for mean estimation and \((([n^{2}]))\) for frequency estimation) while still achieving the order-wise optimal error, as long as the server is _trusted_. On the other hand, when the server is untrusted,  show that optimal error under \((,)\)-DP can be achieved with secure aggregation. However, the communication cost of these schemes is \((n^{2}^{2})\) bits per client for mean estimation and \((n)\) bits per client for frequency estimation. This corresponds to a factor of \(n\) increase for mean estimation and an exponential increase for frequency estimation. In this section, we investigate whether theoptimal communication-accuracy-privacy trade-off from the previous sections can be achieved when the server is not fully trusted.

In this section, we show that if there exists a _secure_ shuffler that randomly permutes clients' locally privatized messages and releases them to the server, we can achieve the nearly optimal (within a \( d\) factor) central-DP error in mean estimation with \((n^{2})\) bits of communication. We note that a similar result has been proven in a concurrent work . Specifically, we present a mean estimation scheme that combines a local-DP mechanism with privacy amplification via shuffling by building on the following recent result :

**Lemma 5.1** ().: _Let \(_{i}\) be an independent \((_{0},0)\)-LDP mechanism for each \(i[n]\) with \(_{0} 1\) and \(\) be a random permutation of \([n]\). Then for any \(\) such that \(_{0}()\), the mechanism is \((,)\)-DP for some \(\) such that \(=O(_{0}}{} ).\)_

Privacy analysis.With the above amplification lemma, we only need to design the local randomizers \(_{i}\) that satisfy \(_{0}\)-LDP. Note that the above lemma is only tight when \(_{0}=O(1)\), thus restricting the (amplified) central \(=O(1/)\), i.e. to be very small. To accommodate larger \(\), users can send different portions of their messages to the server in separate shuffling rounds. Equivalently, we repeat the shuffled LDP mechanism for \(T=O( n^{2})\) rounds while ensuring that in each round, clients communicate an independent piece of information about their sample to the server. More precisely, within each round, each client applies the local randomizers \(_{i}\) with a per-round _local privacy budget_\(_{0}=O(1)\) and sends an independent message to the server. This results in (amplified) central \(O(1/)\)-DP per round, which after composition over \(T=O( n^{2})\) rounds leads to \(\)-DP for the overall scheme as suggested by the composition theorem ). We detail the algorithm in Algorithm 4 in Appendix F.

Communication costs.The communication cost of the above \(T\)-round scheme can be computed as follows. As shown in , the optimal communication cost of an \(_{0}\)-LDP mean estimation is \(O(_{0})\) bits. In addition, the (private-coin) SQKR scheme proposed in  uses \(O(_{0} d)\) bits of communication (we state the formal performance guarantee for this scheme in Lemma 5.2), where compression is done by subsampling coordinates and privatization is performed with Randomized Response. Therefore, since the per-round \(_{0}=O(1)\), the total per-client communication cost is \(O(n^{2} d)\), matching the optimal communication bounds in Section 4 within a \( d\) factor.

**Lemma 5.2** (Sqkr ).: _For all \(_{0}>0,b_{0}>0\), there exists a \((_{0},0)\)-LDP mechanism using \(b_{0}(d)\) bits such that \(\) is unbiased and satisfies \([\|(x^{n})-(x^{n})\| _{2}^{2}]=O(d}{n(_{0}^{2}, _{0},b_{0})}).\)_

Finally, we summarize the performance guarantee for the overall scheme (Algorithm 4) in the following theorem.

**Theorem 5.3** (\(_{2}\) mean estimation).: _Let \(x_{1},...,x_{n}_{2}(C)\) (i.e., \(\|x_{i}\|_{2} C\) for all \(i[n]\)). For all \(>0,b>0,n>30\), and \((_{},1]\) where \(_{}=O(}{(d)})\), Algorithm 4 combined with Kashin's representation and randomized rounding is \((,)\)-DP, uses no more than \(b\) bits of communication, and achieves_

\[[\|(x^{n})-(x^{n})\| _{2}^{2}]=O(C^{2}d(,^{2}})).\]

**Remark 5.4**.: _As opposed to previous schemes Algorithm 1-3, the shuffled SQKR requires some condition on \(\), i.e., \([_{},1]\) due to the specific shuffling lemma we used. In practice, however, \(_{}\) is small due to the exponential dependence on \(n\). The order-wise optimal error of \(O(d}{n^{2}(^{2},)})\) is achieved, up to logarithmic factors, when \(b=_{}(n(d)(^{2},))\)._

**Remark 5.5**.: _We note that similar ideas of private mean estimation based on shuffling have been studied before, see, for instance, . However, these papers do not use the above privacy budget splitting trick over multiple rounds, so their result is only optimal when \(\) is very small. The above scheme can be viewed as a multi-message shuffling scheme , and in particular, can be regarded as a generalization of the scalar mean estimation scheme  to \(d\)-dim mean estimation._

## 6 Experiments

In this section, we empirically evaluate our mean estimation scheme (CSGM) from Section 4, examine its privacy-accuracy-communication trade-off, and compare it with other DP mechanisms (including the shuffling-based mechanism introduced in Section 5).

Setup.For a given dimension \(d\), and number of samples \(n\), we generate local vectors \(X_{i}^{d}\) as follows: let \(X_{i}(j)}{}}(2(0.8)-1)\) where \((0.8)\) is a Bernoulli random variable with bias \(p=0.8\). This ensures \(\|X_{i}\|_{} 1/\) and \(\|X_{i}\|_{2} 1\), and in addition, the empirical mean \((X^{n})_{i}X_{i}\) does not converge to \(0\). Note that as our goal is to construct an unbiased estimator, we did not project our final estimator back to the \(_{}\) or \(_{2}\) space as the projection step may introduce bias. Therefore, the \(_{2}\) estimation error can be greater than \(1\). We account for the privacy budget with Renyi DP  and the privacy-amplification by subsampling lemma in  and convert Renyi DP to \((,)\)-DP via .

Privacy-accuracy-communication trade-off of CSGM.In the first experiment (left of Figure 1), we apply Algorithm 1 with different sampling rates \(\), which leads to different communication budgets (\(b= d\)). Note that when \(=1\), the scheme reduces to the central Gaussian mechanism without compression. In Figure1, we see that with a fixed communication budget, CSGM approximates the central (uncompressed) Gaussian mechanism in the high privacy regime (small \(\)) and starts deviating from it when \(\) exceeds a certain value. In addition, that value of \(\) depends only on sample size \(n\) and the communication budget \(b\) and not the dimension \(d\) as predicted by our theory: recall that the compression error dominates the total error, and hence the performance starts to deviate from the (uncompressed) Gaussian mechanism when \(b=o(n^{2})\), a condition that is independent of \(d\). Observe, for example, that when \(b=50\) bits, the Gaussian mechanism starts outperforming CSGM at \( 0.5\) for both \(d=500\) and \(d=5000\). Hence, for \( 0.5\) CSGM is able to provide 10X compression when \(d=500\), but 100X compression when \(d=5000\) without impacting MSE.

Comparison with local and shuffle DP.Next, we compare the CSGM with local and shuffled DP for \(d=10^{3}\) and \(n=500\). For local DP, we consider the private-coin SQKR scheme introduced in Section 5 which uses \(( d+1)\,T=11T\) bits for \(T\) shuffling rounds and DJW  which is known to be order-optimal when \(=O(1)\) (but is not communication-efficient). For shuffle-DP, we apply the amplification lemma in  to find the corresponding local \(_{0}\) (see Section 5 for more details) and simulate both SQKR and DJW as the local randomizers.

The MSEs of all mechanisms are reported in the right of Figure 1. Our results suggest that for a fixed communication budget (say, \(10\) bits), the practical performance of CSGM significantly outperforms shuffled-DP mechanisms, including the shuffled SQKR and DJW, eventhough they have the same order-wise guarantees theoretically. In addition, the amplification gain of single-round shuffling diminishes fast as \(\) increases. Indeed, when \( 0.8\), we observe no amplification gain compared to the pure local DP.

Figure 1: MSEs of CSGM (Algorithm 1) and shuffle LDP schemes.

## 7 Limitations and Future Work

While we have characterized the (nearly) orderwise optimal trade-offs under central and shuffled DP, it remains unclear whether the _pre-constants_ can be further sharpened, potentially enhancing the practical applicability of these approaches. Furthermore, our analysis is based on a classical privacy amplification technique. By employing more advanced accounting methods, it might be possible to shave off the logarithmic factors.

## 8 Acknowledgements

This work was supported in part by NSF Award # CCF-2213223.