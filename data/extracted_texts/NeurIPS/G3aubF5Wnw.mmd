# On the Sublinear Regret of GP-UCB

Justin Whitehouse

Carnegie Mellon University

jwhiteho@andrew.cmu.edu

&Zhiwei Steven Wu

Carnegie Mellon University

zstevenwu@cmu.edu

&Aaditya Ramdas

Carnegie Mellon University

aramdas@cmu.edu

###### Abstract

In the kernelized bandit problem, a learner aims to sequentially compute the optimum of a function lying in a reproducing kernel Hilbert space given only noisy evaluations at sequentially chosen points. In particular, the learner aims to minimize regret, which is a measure of the suboptimality of the choices made. Arguably the most popular algorithm is the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, which involves acting based on a simple linear estimator of the unknown function. Despite its popularity, existing analyses of GP-UCB give a suboptimal regret rate, which fails to be sublinear for many commonly used kernels such as the Matern kernel. This has led to a longstanding open question: are existing regret analyses for GP-UCB tight, or can bounds be improved by using more sophisticated analytical techniques? In this work, we resolve this open question and show that GP-UCB enjoys nearly optimal regret. In particular, our results yield sublinear regret rates for the Matern kernel, improving over the state-of-the-art analyses and partially resolving a COLT open problem posed by Vakili et al. Our improvements rely on a key technical contribution -- regularizing kernel ridge estimators in proportion to the smoothness of the underlying kernel \(k\). Applying this key idea together with a largely overlooked concentration result in separable Hilbert spaces (for which we provide an independent, simplified derivation), we are able to provide a tighter analysis of the GP-UCB algorithm.

## 1 Introduction

An essential problem in areas such as econometrics [12; 13], medicine [22; 23], optimal control [4; 3], and advertising  is to optimize an unknown function given _bandit feedback_, in which algorithms only get to observe the outcomes for the chosen actions. Due to the bandit feedback, there is a fundamental tradeoff between _exploiting_ what has been observed about the local behavior of the function and _exploring_ to learn more about the function's global behavior. There has been a long line of work on bandit learning that investigates this tradeoff across different settings, including multi-armed bandits [29; 19; 37], linear bandits [2; 30], and kernelized bandits [5; 26; 32].

In this work, we focus on the kernelized bandit framework, which can be viewed as an extension of the well-studied linear bandit setting to an infinite-dimensional reproducing kernel Hilbert space (or RKHS) \((H,,_{H})\). In this problem, there is some unknown function \(f^{*}:\) of bounded norm in \(H\), where \(^{d}\) is a bounded set. In each round \(t[T]\), the learner uses previous observations to select an action \(X_{t}\), and then observes feedback \(Y_{t}:=f^{*}(X_{t})+_{t}\), where \(_{t}\) is a zero-mean noise variable. The learner aims to minimize (with high probability) the regret at time\(T\), which is defined as

\[R_{T}:=_{t=1}^{T}f^{*}(x^{*})-f^{*}(X_{t})\]

where \(x^{*}:=_{x}f^{*}(x)\). The goal is to develop simple, efficient algorithms for the kernelized bandit problem that minimize regret \(R_{T}\). We make the following standard assumption. We also make assumptions on the underlying kernel \(k\), which we discuss in Section 2.

**Assumption 1**.: _We assume that (a) there is some constant \(D>0\) known to the learner such that \(\|f^{*}\|_{H} D\) and (b) for every \(t 1\), \(_{t}\) is \(\)-subGaussian conditioned on \((Y_{1:t-1},X_{1:t})\)._

Arguably the simplest algorithm for the kernelized bandit problem is GP-UCB (Gaussian process upper confidence bound) [31; 5]. GP-UCB works by maintaining a kernel ridge regression estimator of the unknown function \(f^{*}\) alongside a confidence ellipsoid, optimistically selecting in each round the action that provides the maximal payoff over all feasible functions. Not only is GP-UCB efficiently computable thanks to the kernel trick, but it also offers strong empirical guarantees . The only seeming deficit of GP-UCB is its regret guarantee, as existing analyses only show that, with high probability, \(R_{T}=(_{T})\), where \(_{T}\) is a kernel-dependent measure of complexity known as the maximum information gain [31; 6]. In contrast, more complicated, less computationally efficient algorithms such as SupKernelUCB [34; 25] have been shown to obtain regret bounds of \((T})\), improving over the analysis of GP-UCB by a multiplicative factor of \(}\). This gap is stark as the bound \((_{T})\) fails, in general, to be sub-linear for the practically relevant Matern kernel, whereas \((T})\) is sublinear for _any_ kernel experiencing polynomial eigendecay .

This discrepancy has prompted the development of many variants of GP-UCB that, while less computationally efficient, offer better regret guarantees in some situations [17; 27; 28]. (See a detailed discussion of these algorithms along with other related work in Appendix A.) However, the following question remains an open problem in online learning : are existing analyses of vanilla GP-UCB tight, or can an improved analysis show GP-UCB enjoys sublinear regret?

### Contributions

In this work, we show that GP-UCB obtains almost optimal, sublinear regret for any kernel experiencing polynomial eigendecay. This, in particular, implies that GP-UCB obtains sublinear regret for the commonly used Matern family of kernels. We provide a brief roadmap of our paper below.

1. In Section 3, we provide background into self-normalized concentration in Hilbert spaces. In particular, in Theorem 1, we provide an independent, simplified derivation of a bound due to Abbasi-Yadkori , which concerns to self-normalized concentration of certain process in separable Hilbert spaces. This bound has been largely overlooked in the kernel bandit literature, so we draw attention to it in hopes it can be leveraged in solving further kernel-based learning problems. As opposed to the existing bound of Chowdhury and Gopalan , which involves employing a complicated "double mixture" argument, the bound we present follows directly from applying the well-studied finite-dimensional method of mixtures alongside a simple truncation argument [7; 8; 9; 2]. These bounds are clean and show simple dependence on the regularization parameter.
2. In Section 4, we use leverage the self-normalized concentration detailed in Theorem 1 to provide an improved regret analysis for GP-UCB. By carefully choosing regularization parameters based on the smoothness of the underlying kernel, we demonstrate that GP-UCB enjoys sublinear regret of \((T^{})\) for any kernel experiencing \((C,)\)-polynomial eigendecay. As a special case of this result, we obtain regret bounds of \((T^{})\) for the commonly used Matern kernel with smoothness \(\) in dimension \(d\). Our new analysis improves over existing state-of-the-art analysis for GP-UCB, which fails to guarantee sublinear regret in general for the Matern kernel family , and thus partially resolves an open problem posed by  on the suboptimality of GP-UCB.

In sum, our results show that GP-UCB, the go-to algorithm for the kernelized bandit problem, is nearly optimal, coming close to the algorithm-independent lower bounds of Scarlett et al. . Ourwork thus can be seen as providing theoretical justification for the strong empirical performance of GP-UCB . Perhaps the most important message of our work is the importance of careful regularization in online learning problems. While many existing bandit works treat the regularization parameter as a small, kernel-independent constant, we are able to obtain significant improvements by carefully selecting the regularization parameter. We hope our work will encourage others to pay close attention to the selection of regularization parameters in future works.

## 2 Background and Problem Statement

Notation.We briefly touch on basic definitions and notational conveniences that will be used throughout our work. If \(a_{1},,a_{t}\), we let \(a_{1:t}:=(a_{1},,a_{t})^{}\). Let \((H,,_{H})\) be a reproducing kernel Hilbert space associated with a kernel \(k:\). We refer to the identity operator on \(H\) as \(_{H}\). This is distinct from the identity mapping on \(^{d}\), which we will refer to as \(I_{d}\). For elements \(f,g H\), we define their outer product as \(fg^{}:=f g,_{H}\) and inner product as \(f^{}g:= f,g_{H}\). For any \(t 1\) and sequence of points \(x_{1},,x_{t}\) (which will typically be understood from context), let \(_{t}:=(k(,x_{1}),,k(,x_{t}))^{}\). We can respectively define the Gram matrix \(K_{t}:^{t}^{t}\) and covariance operator \(V_{t}:H H\) as \(K_{t}:=(k(x_{i},x_{j}))_{i,j[t]}=_{t}_{t}^{}\) and \(V_{t}:=_{s=1}^{t}k(,x_{s}){k(,x_{s})}^{}=_{t}^{} _{t}\). These two operators essentially encode the same information about the observed data points, the former being easier to work with when actually performing computations (by use of the well known kernel trick) and latter being easier to algebraically manipulate.

Suppose \(A:H H\) is a Hermitian operator of finite rank; enumerate its non-zero eigenvalues as \(_{1}(A),,_{k}(A)\). We can define the Fredholm determinant of \(I+A\) as \((I+A):=_{m=1}^{k}(1+_{i}(A))\). For any \(t 1,>0\), and \(x_{1},,x_{t}\), one can check via a straightforward computation that \((I_{t}+^{-1}K_{t})=(_{H}+^{-1}V_{t})\), where \(K_{t}\) and \(V_{t}\) are the Gram matrix and covariance operator defined above. We, again, will use these two quantities interchangeably in the sequel, but will typically prefer the latter in our proofs.

If \((H,,_{H})\) is a (now general) separable Hilbert space and \((_{n})_{n 1}\) is an orthonormal basis for \(H\), for any \(N 1\) we can define the orthogonal projection operator \(_{N}:H\{_{1},,_{N}\} H\) by \(_{N}f:=_{n=1}^{N} f,_{n}_{H}_{n}\). We can correspondingly the define the projection onto the remaining basis functions to be the map \(_{N}^{}:H\{_{1},,_{N}\}^{}\) given by \(_{N}^{}f:=f-_{N}f\). Lastly, if \(A:H H\) is a symmetric, bounded linear operator, we let \(_{}(A)\) denote the maximal eigenvalue of \(A\), when such a value exists. In particular, \(_{}(A)\) will exist whenever \(A\) has a finite rank, as will typically be the case considered in this paper.

Basics on RKHSs.Let \(^{d}\) be some domain. A _kernel_ is a positive semidefinite map \(k:\) that is square-integrable, i.e. \(_{}_{}|k(x,y)|^{2}dxdy<\). Any kernel \(k\) has an associated _reproducing kernel Hilbert space_ or _RKHS_\((H,,_{H})\) containing the closed span of all partial kernel evaluations \(k(,x),x\). In particular, the inner product \(,_{H}\) on \(H\) satisfies the reproducing relationship \(f(x)= f,k(,x)_{H}\) for all \(x\).

A kernel \(k\) can be associated with a corresponding _Hilbert-Schmidt operator_, which is the Hermitian operator \(T_{k}:L^{2}() L^{2}()\) given by \((T_{k}f)(x):=_{}f(y)k(x,y)dy\) for any \(x\). In short, \(T_{k}\) can be thought of as "smoothing out" or "mollifying" a function \(f\) according to the similarity metric induced by \(k\). \(T_{k}\) plays a key role in kernelized learning through _Mercer's Theorem_, which gives an explicit representation for \(H\) in terms of the eigenvalues and eigenfunctions of \(T_{k}\).

**Fact 1** (**Mercer's Theorem**).: _Let \((H,,_{H})\) be the RKHS associated with kernel \(k\), and let \((_{n})_{n 1}\) and \((_{n})_{n 1}\) be the sequence of non-increasing eigenvalues and corresponding eigenfunctions for \(T_{k}\). Let \((_{n})_{n 1}\) be the sequence of rescaled functions \(_{n}:=_{n}_{n}\). Then,_

\[H=\{_{n=1}^{}_{n}_{n}:_{n=1}^{}_{n}^ {2}<\},\]

_and \((_{n})_{n 1}\) forms an orthonormal basis for \((H,,_{H})\)._

We make the following assumption throughout the remainder of our work, which is standard and comes from Vakili et al. .

**Assumption 2** (Assumption on kernel \(k\)).: _The kernel \(k:\) satisfies (a) \(|k(x,y)| L\) for all \(x,y\), for some constant \(L>0\) and (b) \(|_{n}(x)| B\) for all \(x\), for some \(B>0\)._

"Complexity" of RKHS's.By the eigendecay of a kernel \(k\), we really mean the rate of decay of the sequence of eigenvalues \((_{n})_{n 1}\). In the literature, there are two common paradigms for studying the eigendecay of \(k\): \((C_{1},C_{2},)\)-exponential eigendecay, under which \( n 1,_{n} C_{1}(-C_{2}n^{})\), and \((C,)\)-polynomial eigendecay, under which \( n 1,_{n} Cn^{-}\). For kernels experiencing exponential eigendecay, of which the squared exponential is the most important example, GP-UCB is known to be optimal up to poly-logarithmic factors. However, for kernels experiencing polynomial eigendecay, of which the Matern family is a common example, existing analyses of GP-UCB fail to yield sublinear regret. It is this latter case we focus on in this work.

Given the above representation in Fact 1, it is clear that the eigendecay of the kernel \(k\) governs the "complexity" or "size" of the RKHS \(H\). We make this notion of complexity precise by discussing _maximum information gain_, a sequential, kernel-dependent quantity governing concentration and hardness of learning in RKHS's .

Let \(t 1\) and \(>0\) be arbitrary. The maximum information gain at time \(t\) with regularization \(\) is the scalar \(_{t}()\) given by

\[_{t}():=_{x_{1},,x_{t}} (_{H}+^{-1}V_{t})=_{x_{1},,x_{t} }(I_{t}+^{-1}K_{t}).\]

Our presentation of maximum information gain differs from some previous works in that we encode the regularization parameter \(\) into our notation. This inclusion is key for our results, as we obtain improvements by carefully selecting \(\). Vakili et al.  bound the rate of growth of \(_{t}()\) in terms of the rate of eigendecay of the kernel \(k\). We leverage the following fact in our main results.

**Fact 2** (**Corollary 1 in Vakili et al. **).: _Suppose that kernel \(k\) satisfies Assumption 2 and experiences \((C,)\)-polynomial eigendecay. Then, for any \(t 1\), we have_

\[_{t}()((t}{})^{1/}^{-1 /}(1+)+1)(1+ ).\]

We last define the practically relevant Matern kernel and discuss its eigendecay.

**Definition/Fact 3**.: The Matern kernel with bandwidth \(>0\) and smoothness \(>1/2\) is given by

\[k_{,}(x,y):=}(\|x -y\|_{2}}{})^{}B_{}(\|x-y\|_{2}}{ }),\]

where \(\) is the gamma function and \(B_{}\) is the modified Bessel function of the second kind. It is known that there is some constant \(C>0\) that may depend on \(\) but not on \(d\) or \(\) such that \(k_{,}\) experiences \((C,)\)-eignedecay .

Basics on martingale concentration:A filtration \((_{t})_{t 0}\) is a sequence of \(\)-algebras satisfying \(_{t}_{t+1}\) for all \(t 1\). If \((M_{t})_{t 0}\) is a \(H\)-valued process, we say \((M_{t})_{t 0}\) is a martingale with respect to \((_{t})_{t 0}\) if (a) \((M_{t})_{t 0}\) is \((_{t})_{t 0}\)-adapted, and (b) \((M_{t}_{t-1})=M_{t-1}\) for all \(t 1\). An \(\)-valued process is called a supermartingale if the equality in (b) is replaced with "\(\)". i.e. supermartingales tend to decrease. Martingales are useful in many statistical applications due to their strong concentration of measure properties . The follow fact can be leveraged to provide time-uniform bounds on the growth of any non-negative supermartingale.

**Fact 4** (**Ville's Inequality)**.: _Let \((M_{t})_{t 0}\) be a non-negative supermartingale with respect to some filtration. Suppose \(M_{0}=1\). Then, for any \((0,1)\), we have_

\[( t 0:M_{t}).\]

See Howard et al.  for a self-contained proof of Ville's inequality, and many applications.

If \(\) is a \(\)-algebra, and \(\) is an \(\)-valued random variable, we say \(\) is \(\)-subGaussian conditioned on \(\) if, for any \(\), we have \((e^{})^{2}}{2}\); in particular this condition impliesthat \(\) is mean zero. With this, we state the following result on self-normalized processes. To our understanding, the following result was first presented in some form as Example 4.2 of de la Pena et al.  (in the setting of continuous local martingales), and can be derived leveraging the argument of Theorem 1 in de la Pena et al. . The exact form below was established (in the setting of discrete-time processes) in Theorem 1 of Abbasi-Yadkori et al. , which is commonly leveraged to construct confidence ellipsoids in the linear bandit setting.

**Fact 5** (Example 4.2 from , Theorem 1 from ).: _Let \((_{t})_{t 0}\) be a filtration, let \((X_{t})_{t 1}\) be an \((_{t})_{t 0}\)-predictable sequence in \(^{d}\), and let \((_{t})_{t 1}\) be a real-valued \((_{t})_{t 1}\)-adapted sequence such that conditional on \(_{t-1}\), \(_{t}\) is mean zero and \(\)-subGaussian. Then, for any \(>0\), the process \((M_{t})_{t 0}\) given by_

\[M_{t}:=+^{-1}V_{t})}}\{ \|( I_{d}+V_{t})^{-1/2}S_{t}/\|_{2}^{2}\}\]

_is a non-negative supermartingale with respect to \((_{t})_{t 0}\), where \(S_{t}:=_{s=1}^{t}_{s}X_{s}\) and \(V_{t}:=_{s=1}^{t}X_{s}X_{s}^{}\). Consequently, by Fact 4, for any confidence \((0,1)\), the following holds: with probability at least \(1-\), simultaneously for all \(t 1\), we have_

\[\|(V_{t}+ I_{d})^{-1/2}S_{t}\|_{2}+^{-1}V_{t})})}.\]

Note the simple dependence on the regularization parameter \(>0\) in the above bound. While the regularization parameter \(\) doesn't prove important in regret analysis for linear bandits (where \(\) is treated as constant), the choice for \(\) will be critical in our setting. In the following section, we will discuss how Fact 5 can be extended to the setting of separable Hilbert spaces essentially verbatim (an observation first noticed by Abbasi-Yadkori ).

## 3 A Remark on Self-Normalized Concentration in Hillbert Spaces

We begin by discussing a key, self-normalized concentration inequality for martingales. We use this bound in the sequel to construct simpler, more flexible confidence ellipsoids than currently exist for GP-UCB. The bound we present (in Theorem 1 below) is, more or less, equivalent to Corollary 3.5 in the thesis of Abbasi-Yadkori . Our result is mildly more general in the sense that it directly argues that a target mixture process is a nonnegative supermartingale. The result in Abbasi-Yadkori  is more general in the sense it allows the regularization (or shift) matrix to be non-diagonal. Either concentration result is sufficient for the regret bounds obtained in the sequel.

The aforementioned corollary in , quite surprisingly, has not been referenced in central works on the kernelized bandit problem, namely Chowdhury and Gopalan  and Vakili et al. . In fact, strictly weaker versions of the conclusion have been independently rediscovered in the context of kernel regression . We emphasize that this result of Abbasi-Yadkori  (and the surrounding technical conclusions) are very general and may allow for further improvements in problems related to kernelized learning.

We now present Theorem 1, providing a brief sketch and a full proof in Appendix B. We believe our proof, which directly shows a target process is a nonnegative supermartingale, is of independent interest when compared to that of Abbasi-Yadkori  due to its simplicity. In particular, our proof follows from first principles, avoiding advanced topological notions of convergence (e.g. in the weak operator topology) and existence of certain Gaussian measures on separable Hilbert spaces, which were heavily utilized in the proof of Corollary 3.5 in Abbasi-Yadkori .

**Theorem 1** (**Self-normalized concentration in Hilbert spaces**).: _Let \((_{t})_{t 0}\) be a filtration, \((f_{t})_{t 1}\) be an \((_{t})_{t 0}\)-predictable sequence in a separable Hilbert space1\(H\) such that \(\|f_{t}\|_{H}<\) a.s. for all \(t 0\), and \((_{t})_{t 1}\) be an \((_{t})_{t 1}\)-adapted sequence in \(\) such that conditioned on \(_{t-1}\), \(_{t}\) is mean zero and \(\)-subGaussian. Defining \(S_{t}:=_{s=1}^{t}_{s}f_{s}\) and \(V_{t}:=_{s=1}^{t}f_{s}f_{s}^{}\), we have that for any \(>0\), the process \((M_{t})_{t 0}\) defined by_

\[M_{t}:=_{H}+^{-1}V_{t})}}\{ \|(_{H}+V_{t})^{-1/2}S_{t}/\| _{H}^{2}\}\]_is a nonnegative supermartingale with respect to \((_{t})_{t 0}\). Consequently, by Fact 4, for any \((0,1)\), with probability at least \(1-\), simultaneously for all \(t 1\), we have_

\[\|(V_{t}+ I_{d})^{-1/2}S_{t}\|_{H}_{H}+^{-1}V_{t})})}.\]

We can summarize our independent proof in two simple steps. First, following from Fact 5, the bound in Theorem 1 holds when we project \(S_{t}\) and \(V_{t}\) onto a finite number \(N\) of coordinates, defining a "truncated" nonnegative supermartingale \(M_{t}^{(N)}\). Secondly, we can make a limiting argument, showing \(M_{t}^{(N)}\) is "essentially" \(M_{t}\) for large values of \(N\).

Proof Sketch for Theorem 1.: Let \((_{n})_{n 1}\) be an orthonormal basis for \(H\), and, for any \(N 1\), let \(_{N}\) denote the projection operator onto \(H_{N}:=\{_{1},,_{N}\}\). Note that the projected process \((_{N}S_{t})_{t 1}\) is an \(H\)-valued martingale with respect to \((_{t})_{t 0}\). Further, note that the projected variance process \((_{N}V_{t}_{N}^{})_{t 0}\) satisfies

\[_{N}V_{t}_{N}^{}=_{s=1}^{t}(_{N}f_{s})(_{N}f_{s})^{}.\]

Since, for any \(N 1\), \(H_{N}\) is a finite-dimensional Hilbert space, it follows from Lemma 1 that the process \((M_{t}^{(N)})_{t 0}\) given by

\[M_{t}^{(N)}:=_{H}+^{-1}_{N}V_{t}_{N}^ {})}}\{\|(_{H}+_{N}V_{t}_{N} ^{})^{-1/2}_{N}S_{t}\|_{H}^{2}\},\]

is a nonnegative supermartingale with respect to \((_{t})_{t 0}\). One can check that, for any \(t 0\), \(M_{t}^{(N)}[N]{}M_{t}\). Thus, Fatou's Lemma implies

\[(M_{t}_{t-1}) =(_{N}M_{t}^{(N)}_{ t-1})\] \[_{N}(M_{t}^{(N)}_{t-1})\] \[_{N}M_{t-1}^{(N)}\] \[=M_{t-1},\]

which proves the first part of the claim. The second part of the claim follows from applying Fact 4 to the defined nonnegative supermartingale and rearranging. See Appendix B for details. 

The following corollary specializes Theorem 1 (and thus Corollary 3.5 of Abbasi-Yadkori ) to the case where \(H\) is a RKHS and \(f_{t}=k(,X_{t})\), for all \(t 1\). In this special case, we can reframe the above theorem in terms familiar Gram matrix \(K_{}\), assuming the quantity is invertible. While we prefer the simplicity and elegance of working directly in the RKHS \(H\) in the sequel, the follow corollary allows us to present Theorem 1 in a way that is computationally tractable.

**Corollary 1**.: _Let us assume the same setup as Theorem 1, and additionally assume that (a) \((H,,_{H})\) is a RKHS associated with some kernel \(k\), and (b) there is some \(\)-valued \((_{t})_{t 0}\)-predictable process \((X_{t})_{t 1}\) such that \((f_{t})_{t 1}=(k(,X_{t}))_{t 1}\). Then, for any \(>0\) and \((0,1)\), we have that, with probability at least \(1-\), simultaneously for all \(t 0\),_

\[\|(V_{t}+_{H})^{-1/2}S_{t}\|_{H}(I_{t}+^{-1}K_{})})}.\]

_If, in addition, the Gram matrix \(K_{t}=(k(X_{i},X_{j}))_{i,j[t]}\) is invertible, we have the equality_

\[\|(I_{t}+ K_{t}^{-1})^{-1/2}_{1:t}\|_{2}=\|(_{H}+V_ {t})^{-1/2}S_{t}\|_{H}.\]We prove Corollary 1 in Appendix B. With this reframing of Theorem 1, we compare the concentration results of Theorem 1 (and thus Abbasi-Yadkori ) to the following, commonly leveraged result from Chowdhury and Gopalan .

**Fact 6** (**Theorem 1 from Chowdhury and Gopalan **).: _Assume the same setup as Fact 5. Let \(>0\) be arbitrary, and let \(K_{t}:=(k(X_{i},X_{j}))_{i,j[t]}\) be the Gram matrix corresponding to observations made by time \(t 1\). Then, with probability at least \(1-\), simultaneously for all \(t 1\), we have_

\[\|((K_{t}+ I_{t})^{-1}+I_{t})^{-1/2}_{1:t}\| _{2}+ K_{t})})}.\]

To make comparison with this bound clear, we parameterize the bounds in the above fact in terms of \(>0\) instead of \(>0\) to emphasize the following difference: both sides of the bound presented in Theorem 1 shrink as \(\) is increased, whereas both sides of the bound in Fact 6 increase as \(\) grows. Thus, increasing \(\) in Theorem 1 should be seen as decreasing \(\) in the bound of Chowdhury and Gopalan . The bounds in Corollary 1 and Fact 6 coincide when \(=1\) and \( 0\) (per Lemma 1 in Chowdhury and Gopalan ), but are otherwise not equivalent for other choices of \(\) and \(\).

We believe Theorem 1 and Corollary 3.5 of Abbasi-Yadkori  to be significantly more usable than the result of Chowdhury and Gopalan  for several reasons. First, the aforementioned bounds _directly_ extend the method of mixtures (in particular, Fact 5) to potentially infinite-dimensional Hilbert spaces. This similarity in form allows us to leverage existing analysis of Abbasi-Yadkori et al.  to prove our regret bounds, with only slight modifications. This is in contrast to the more cumbersome regret analysis that leverages Fact 6, which is not only more difficult to follow, but also obtains inferior, sometimes super-linear regret guarantees.

Second, we note that Theorem 1 provides a bound that has a simple dependence on \(>0\). In more detail, directly as a byproduct of the simplified bounds, Theorem 2 offers a regret bound that can readily be tuned in terms of \(\). Due to their use of a "double mixture" technique in proving Fact 6, Chowdhury and Gopalan  essentially wind up with a nested, doubly-regularized matrix \(((K_{t}+ I_{t})^{-1}+I_{t})^{-1/2}\) with which they normalize the residuals \(_{1:t}\). In particular, this more complicated normalization make it difficult to understand how varying \(\) impacts regret guarantees, which we find to be essential for proving improved regret guarantees.

We note that the central bound discussed in this section _does not_ provide an improvement in dependence on maximum information gain in the sense hypothesized by Vakili et al. . In particular, the authors hypothesized the possibility of shaving a \(}\) multiplicative factor off of self-normalized concentration inequalities in RKHS's. This was shown in a recent work (see Lattimore ) to be impossible in general. Instead, Theorem 1 and Corollary 3.5 of Abbasi-Yadkori  give one access to a family of bounds parameterized by the regularization parameter \(>0\). As will be seen in the sequel, by optimizing over this parameter, one can obtain significant improvements in regret.

## 4 An Improved Regret Analysis of GP-UCB

In this section, we provide the second of our main contributions, which is an improved regret analysis for the GP-UCB algorithm. We provide a description of GP-UCB in Algorithm 1. While we state the algorithm directly in terms of quantities in the RKHS \(H\), these quantities can be readily converted to those involving Gram matrices or Gaussian processes for those who prefer that perspective .

As seen in Section 3, by carefully extending the "method of mixtures" technique (originally by Robbins) of Abbasi-Yadkori et al. , Abbasi-Yadkori  and de la Pena et al.  to Hilbert spaces, we can construct self-normalized concentration inequalities that have simple dependence on the regularization parameter \(\). These simplified bounds, in conjunction with information about the eigendecay of the kernel \(k\), can be combined to carefully choose \(\) to obtain improved regret. We now present our main result.

**Theorem 2**.: _Let \(T>0\) be a fixed time horizon, \(>0\) a regularization parameter, and assume Assumptions 2 and 1 hold. Let \((0,1)\), and for \(t 1\) define_

\[U_{t}:=_{H}+ ^{-1}V_{t})})}+^{1/2}D.\]Then, with probability at least \(1-\), the regret of Algorithm 1 run with parameters \(,(U_{t})_{t 1},D\) satisfies

\[R_{T}=O(_{T}()+()T}),\]

where in the big-Oh notation above we treat \(,D,,B\), and \(L\) as being held constant. If the kernel \(k\) experiences \((C,)\)-polynomial eigendecay for some \(C>0\) and \(>1\), taking \(=O(T^{})\) yields \(R_{T}=(T^{})\)2, which is always sub-linear in \(T\).

While we present the above bound with a fixed time-horizon, it can be made anytime by carefully applying a standard doubling argument (see Lattimore and Szepesvari , for instance). We specialize the above theorem to the case of the Matern kernel in the following corollary.

**Corollary 2**.: _Definition 3 states that the Matern kernel with smoothness \(>1/2\) in dimension \(d\) experiences \((C,)\)-eigendecay, for some constant \(C>0\). Thus, GP-UCB obtains a regret rate of \(R_{T}=(T^{})\)._

We note that our regret analysis is the first to show that GP-UCB attains sublinear regret for general kernels experiencing polynomial eigendecay. Of particular import is that Corollary 2 of Theorem 2 yields the first analysis of GP-UCB that implies sublinear regret for the Matern kernel under general settings of ambient dimension \(d\) and smoothness \(\). A recent result by Janz , using a uniform lengthscale argument, demonstrates that GP-UCB obtains sublinear regret for the specific case of the Matern family when the parameter \(\) and dimension \(d\) satisfy a uniform boundedness condition independent of scale. Our results are (a) more general, holding for _any_ kernel exhibiting polynomial eigendecay, (b) don't require checking uniform boundedness independent of scale condition, and (c) follow from a simple regularization based argument. In particular, the arguments of Janz  require advanced functional analytic and Fourier analytic machinery.

We note that our analysis does not obtain optimal regret, as the theoretically interesting but computationally cumbersome SupKernelUCB algorithm  obtains a slightly improved regret bound of \((T^{})\) for \((C,)\)-polynomial eigendecay and \((T^{})\) for the Matern kernel with smoothness \(\) in dimension \(d\). Due to the aforementioned result of Lattimore , which shows that improved dependence on maximum information gain cannot be generally obtained in Hilbert space concentration, we believe further improvements on regret analysis for GP-UCB may not possible.

To wrap up this section, we provide a proof sketch for Theorem 2. The entire proof, along with full statements and proofs of the technical lemmas, can be found in Appendix C.

Proof Sketch for Theorem 2.: Letting, for any \(t[T]\), the "instantaneous regret" be defined as \(r_{t}:=f^{*}(x^{*})-f^{*}(X_{t})\), a standard argument yields that, with probability at least \(1-\), simultaneously for all \(t[T]\),

\[r_{t} 2U_{t-1}\|(_{H}+V_{t-1})^{-1/2}k(,X_{t}) \|_{H}.\]A further standard argument using Cauchy-Schwarz and an elliptical potential argument yields

\[R_{T} =_{t=1}^{T}r_{t} U_{T}_{H}+^ {-1}V_{T})}\] \[=(_{H}+^{-1}V_{T})})}+^{1/2}D)_{H}+ ^{-1}V_{T})}\] \[(+( )}+^{1/2}D)()}=O(_{T}() +()T}),\]

which proves the first part of the claim. If, additionally, \(k\) experiences \((C,)\)-polynomial eigendecay, we know that \(_{T}()=(()^{1/})\) by Fact 2. Setting \(:=O(T^{})\) thus yields

\[R_{T}=O(_{T}()+()T})= (T^{}),\]

proving the second part of the claim. 

## 5 Conclusion

In this work, we present an improved analysis for the GP-UCB algorithm in the kernelized bandit problem. We provide the first analysis showing that GP-UCB obtains sublinear regret when the underlying kernel \(k\) experiences polynomial eigendecay, which in particular implies sublinear regret rates for the practically relevant Matern kernel. In particular, we show GP-UCB obtains regret \((T^{})\) when \(k\) experiences \((C,)\)-polynomial eigendecay, and regret \((T^{})\) for the Matern kernel with smoothness \(\) in dimension \(d\).

Our contributions are twofold. First, we show the importance of finding the "right" concentration inequality for tackling problems in online learning -- in this case the correct bound being a self-normalized inequality originally due to Abbasi-Yadkori . We provide an independent proof of a result equivalent to Corollary 3.5 of Abbasi-Yadkori  in Theorem 1, and hope that our simplified, truncation-based analysis will make the result more accessible to researchers working on problems in kernelized learning. Second, we demonstrate the importance of regularization in the kernelized bandit problem. In particular, since the smoothness of the kernel \(k\) governs the hardness of learning, by regularizing in proportion to the rate of eigendecay of \(k\), one can obtain significantly improved regret bounds.

A shortcoming of our work is that, despite obtaining the first generally sublinear regret bounds for GP-UCB, our rates are not optimal. In particular, there are discretization-based algorithms, such as SupKernelUCB , which obtain slightly better regret bounds of \((T^{})\) for \((C,)\)-polynomial eigendecay. We hypothesize that the vanilla GP-UCB algorithm, which involves constructing confidence ellipsoids directly in the RKHS \(H\), cannot obtain this rate.

The common line of reasoning  is that because the Lin-UCB (the equivalent algorithm in \(^{d}\)) obtains the optimal regret rate of \((d)\) in the linear bandit problem setting, then GP-UCB should attain optimal regret as well. In the linear bandit setting, there is no subtlety between estimating the optimal action and unknown slope vector, as these are one and the same. In the kernel bandit setting, estimating the function and optimal action are not equivalent tasks. In particular, the former serves in essence as a nuisance parameter in estimating the latter: tight estimation of unknown function under the Hilbert space norm implies tight estimation of the optimal action, but not the other way around. Existing optimal algorithms are successful because they discretize the input domain, which has finite metric dimension , and make no attempts to estimate the unknown function in RKHS norm. Since compact sets in RKHS's do not, in general, have finite metric dimension , this makes estimation of the unknown function a strictly more difficult task. In fact, recent work by Lattimore  demonstrate that self-normalized concentration in RKHS's, in general, cannot exhibit improved dependence on maximum information gain. This further supports our hypothesis on the further unimprovability of the regret analysis of GP-UCB past the improvements made in this paper.

## 6 Acknowledgements

AR acknowledges support from NSF DMS-2310718 and NSF IIS-2229881. ZSW and JW were supported in part by the NSF CNS2120667, a CyLab 2021 grant, a Google Faculty Research Award, and a Mozilla Research Grant. JW also acknowledges support from NSF GRFP grants DGE1745016 and DGE2140739.

We also would like to thank Xingyu Zhou and Johannes Kirschner for independently bringing to our attention the result from Abbasi-Yadkori  (Corollary 3.5) on self-normalized concentration in Hilbert spaces which is essentially equivalent to Theorem 1. We have rewritten the paper in a way that emphasizes the importance of this result and provides proper attribution to the original author.