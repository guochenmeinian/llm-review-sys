# Frustratingly Easy Test-Time Adaptation of Vision-Language Models

Matteo Farina\({}^{1,}\) Gianni Franchi\({}^{2}\) Giovanni Iacca\({}^{1}\)

Massimiliano Mancini\({}^{1}\) Elisa Ricci\({}^{1,3}\)

\({}^{1}\)University of Trento

\({}^{2}\)U2IS, ENSTA Paris, Institut Polytechnique de Paris

\({}^{3}\)Fondazione Bruno Kessler (FBK)

###### Abstract

Vision-Language Models seamlessly discriminate among arbitrary semantic categories, yet they still suffer from poor generalization when presented with challenging examples. For this reason, Episodic Test-Time Adaptation (TTA) strategies have recently emerged as powerful techniques to adapt VLMs in the presence of a single unlabeled image. The recent literature on TTA is dominated by the paradigm of prompt tuning by Marginal Entropy Minimization, which, relying on online backpropagation, inevitably slows down inference while increasing memory. In this work, we theoretically investigate the properties of this approach and unveil that a surprisingly strong TTA method lies dormant and hidden within it. We term this approach Zero (_TTA with "zero" temperature_), whose design is both incredibly effective and frustratingly simple: augment \(N\) times, predict, retain the most confident predictions, and marginalize after setting the Softmax temperature to zero. Remarkably, Zero requires a _single_ batched forward pass through the vision encoder only and _no_ backward passes. We thoroughly evaluate our approach following the experimental protocol established in the literature and show that Zero largely surpasses or compares favorably _w.r.t._ the state-of-the-art while being almost \(10\) faster and \(13\) more memory friendly than standard Test-Time Prompt Tuning. Thanks to its simplicity and comparatively negligible computation, Zero can serve as a strong baseline for future work in this field. The code is available.

## 1 Introduction

Groundbreaking achievements in Vision-Language pretraining  have increased the interest in crafting Vision-Language Models (VLMs) that can understand visual content alongside natural language, enabling a new definition of zero-shot classification. Despite huge pretraining databases , VLMs still face limitations, suffering from performance degradation in case of large train-test dissimilarity  and requiring the design of highly generalizing textual templates .

Test-Time Adaptation (TTA) can effectively improve the robustness of VLMs by adapting a given model to online inputs. Among the various TTA setups (such as "fully" , "continual"  or "practical" TTA ), Episodic TTA  is particularly appealing, as it focuses on _one-sample_ learning problems and requires no assumptions on the distribution of the test data. When presented with a test image \(\), the parameters \(\) of a model \(f\) are optimized through a TTA objective \(\) before inferring the final prediction, and reset afterward.

The choice of \(\) is, ultimately, what characterizes TTA methods the most, with the recent literature being dominated by the objective of Marginal Entropy Minimization (MEM) . Given a collection \(\) of \(N\) data augmentation functions, a test image \(\) is first augmented \(N\) times to obtain a set of different views \(X=\{_{i}()\}_{i=1}^{N}=\{_{i}\}_{i=1}^{N}\). The _marginal probability distribution_\(\)_w.r.t._ sample \(\) is then defined as the empirical expectation of Softmax-normalized model outputs over \(X\), _i.e._:

\[(|)=_{i=1}^{N}p(|_{i}).\] (1)

Under this framework, the Shannon Entropy of \(\) is a bona fide measure of how _inconsistently_ and _uncertainly_ the model predicts over \(X\), making it a tantalizing candidate to minimize, _i.e._:

\[_{ent}=H((|))=-_{c=1}^{C}(y=c|)((y=c|)),\] (2)

where \(C\) is the number of semantic categories. Once \(_{ent}\) is computed, (some of) the parameters of \(f\) are typically updated for a few steps of Gradient Descent before inferring the final prediction over the source input \(\) with updated parameters. Owing to its simplicity and effectiveness, MEM has become a _de facto_ standard in modern TTA [53; 38; 33; 19; 42; 27].

In this work, we take the opposite direction and challenge this paradigm. By conducting an in-depth theoretical and empirical investigation, we find that: 1 while effective in improving model robustness, MEM has _little effect_ on the prediction of \(\);2 no matter the dataset, the label space, or the parameter initialization, VLMs become much better classifiers when \(\) replaces the standard inference protocol. Building on these insights, we show that a surprisingly strong and optimization-free TTA baseline is subtly hidden within the MEM framework. We term this baseline Zero, which is short for TTA with "**zero**" temperature. Instead of tuning any parameters, setting to zero the Softmax temperature before marginalizing over views makes \(\) already stronger than the model after MEM. Notably, Zero only requires a single forward pass through the vision encoder and no backward passes.

Wrapping up, the contributions of this paper are the following:

1. We theoretically show _when_ the prediction obtained through \(\) (_i.e._, \(\)) is _invariant_ to MEM, and empirically verify that MEM has largely _no effect_ on \(\);
2. We theoretically and empirically demonstrate that the error rate of \(\) is a lower bound to the base error of a VLM in the setup of TTA. Additionally, we identify augmentations-induced _overconfidence_ as the primal factor undermining the reliability of \(\);
3. Motivated by these theoretical insights, we introduce Zero, a frustratingly simple TTA approach that recovers the reliability of \(\) by tweaking a single parameter of the model: the temperature;
4. We thoroughly evaluate Zero following the established experimental setup with a variety of model initializations. Our results show that Zero surpasses or compares favorably to state-of-the-art TTA methods while being much faster and more memory efficient (_e.g._, \(10\) faster and \(13\) more memory efficient than the established Test-Time Prompt Tuning ).

## 2 Understanding Marginal Entropy Minimization

In this Section, we take a step towards both theoretically and empirically understanding the paradigm of MEM. In particular, this section is devoted to answering the following research questions:

1. _How does MEM affect the marginal probability distribution?_ And, in turn
2. _How does the marginal probability distribution relate to the standard inference protocol?_

First, we introduce MEM for VLMs by reviewing the established Test-Time Prompt Tuning (TPT) method  and its notation. Then, in Sections 2.2 and 2.3 we answer the research questions above.

### Preliminaries

**Zero-Shot Classification with VLMs** employs a predefined template (_e.g._, "a photo of a") from which a set of context vectors \(_{}\) is obtained by looking up a token embedding table. Expandingthe template with the class names (_e.g._, "a photo of a laptop." for the class "laptop") makes up the entire set of input vectors \([_{},_{1}],,[_{}, _{C}]\), with \(_{i}\) being the embeddings derived from the \(i\)-th class name. A text encoder \(_{}\) transforms these class descriptions into independent and normalized text embeddings \(_{1}^{},,_{C}^{}\) and an image encoder \(_{}\) encodes an input image \(\) into a normalized latent vector \(^{}\). Lastly, classification is carried out by picking the class \(c\) corresponding to the text embedding \(_{c}^{}\) holding the maximum cosine similarity with \(^{}\).

**MEM for VLMs.** Pioneered by MEMO  in the scope of unimodal neural networks, MEM was repurposed for TTA with VLMs by Test-Time Prompt Tuning . In , a VLM such as CLIP  is adapted at test time by minimizing the same objective of Eq. (2). In contrast to optimizing all model parameters, TPT relies on the effectiveness of prompt tuning [56; 55; 15], optimizing only the context vectors derived from the token embeddings of the standard CLIP template "a photo of a". By explicitly enunciating the dependency on the context vectors \(_{}\) and re-using the notation of Sec. 1, one can re-write the MEM objective of  as:

\[&_{ent}=H((|, _{}))=-_{c}^{C}(y=c|,_{},)\,((y=c|,_{}, ))\\ &(y=c|,_{}, )=_{i}^{N}_{i}^{} _{c}^{}(_{})/)}{ _{k}^{C}(_{i}^{}_{k}^{}( _{})/)}.\] (3)

Here, \(\) is the _temperature_ of the Softmax operator. In the rest of this section, we omit the dependency on \(\) for simplicity, writing \(p(|,_{})\). Similarly to , the objective of Eq. (3) is minimized for a single step of Gradient Descent to update the set of context vectors. The updated context vectors, denoted as \(_{}^{*}\), are then used to prompt the VLM and obtain the final prediction for \(\). For any class \(c\) this is simply \(_{}^{}_{c}^{}(_{}^{*})\), which is easily transformed into \(p(y=c|,_{}^{*})\) via Softmax.

### How does MEM affect the marginal probability distribution?

The recent literature on TTA shows that minimizing \(_{ent}\) significantly enhances the robustness of model outputs. However, the impact of this process on the marginal probability distribution \(\) remains unclear. We start with a straightforward hypothesis: due to its nature, minimizing \(_{ent}\) tends to increase the probability of the most probable class of \((|,_{})\). More formally, denoting with \(\) the prediction of \(\) (_i.e._, \(=(|,_{})\)), we hypothesize that \((y=|,_{}^{*})>(y =|,_{})\). If this hypothesis is realized, it comes as a natural consequence that minimizing \(_{ent}\) is unlikely to alter the prevailing class of \(\), thus resulting in a consistent prediction pre- and post-TTA where \((|,_{})= (|,_{}^{*})\).

Hence, the first contribution of this work is to show that the prediction of the marginal probability distribution \(\) is _invariant_ to Entropy Minimization under loose constraints on confidence and gradients. To lighten the notation of the proposition, let us first define the following function \(g\):

\[g(c,^{},_{1}^{},,_{C}^ {})=^{}_{c}^{ }/)}{_{k}^{C}(^{} _{k}^{}/)}\] (4)

_i.e._, the probability assigned to class \(c\) given a latent image representation \(^{}\) and class-wise text embeddings \(_{1}^{},,_{C}^{}\). Additionally, let \( g(c,^{})\) be the negative variation incurred to the function \(g\) when the context vectors \(_{}\) are updated through Entropy Minimization:

\[ g(c,^{})=g(c,^{},_{1}^ {}(_{}),,_{C}^{}( _{}))-g(c,^{},_{1}^{}(_{}^{*}),,_{C}^{}(_{ }^{*}))\] (5)

where, for clarity, the dependency of the text embeddings \(_{1}^{},,_{C}^{}\) on the context vectors (either \(_{}\) or \(_{}^{*}\)) is explicit. Using this notation, we can formalize the following proposition:

**Proposition 2.1**.: _Let \(_{1}^{},,_{N}^{}\) be the latent image representations resulting from the \(N\) views and \(=(|,_{})\) be the initial prediction of the marginal probability distribution. If the entropy of \(\) is minimized and \((y=|,_{})>_{i=1}^ {N} g(,_{i}^{})\) then the prevalent class of \(\) is invariant to MEM_, i.e., \((|,_{})= (|,_{}^{*})\).

In Appendix A, we provide a detailed proof of this proposition, highlighting that \((,^{})\) is directly linked to the gradient _w.r.t._ the context vectors \(_{}\). This relationship emerges when writing any post-update text embedding \(_{c}^{}(_{}^{*})\) as a function of its pre-update counterpart \(_{c}^{}(_{})\). Specifically,we can write \(_{}^{}(_{}^{*})=_{ }([_{}-_{_{}}( _{ent}),_{}])\), which is equivalent to \(_{}([_{},_{}])- _{_{}}_{}([_{ }^{*},_{}])^{}_{_{ }}(_{ent})\) after a first-order Taylor Expansion around \(_{}^{*}\). Consequently, the proposition holds by a condition relating confidence (through \((y=c|,_{})\)) and gradients (through \((,^{})\)). Alongside the proof, Appendix A presents evidence supporting this proposition for CLIP  on the ImageNet-1k validation set , as well as across various datasets for natural distribution shifts: ImageNet-A , ImageNet-R , ImageNet-v2 , and ImageNet-Sketch .

### How does \(\) relate to the standard inference protocol?

From prior work on Test-Time Augmentations (TTAug) with unimodal neural networks [40; 35], empirical evidence suggests that \((|)\) is more robust than \(p(|)\). This observation leads to the hypothesis that the expected risk of predicting with \(\) is lower than that of doing so with \(p\). However, the literature lacks guarantees for this hypothesis, except for the peculiar case in which the risk function is the squared error, _i.e._, \((a,b)=(a-b)^{2}\).2

As the second contribution of this study, we show that the error rate of \((|)\) does indeed lower-bound the error rate of \(p(|)\). We do so by revisiting the theory of model ensembling, and showing that analogous ideas can emerge for TTA.

**Preliminaries on model ensembling.** From the theory of classifier ensembling , we know that if \(f_{1},,f_{N}\) are \(N\) independent classifiers with error rate \(\) and \(\) is an example whose label is \(y\{0,1\}\), then the probability that any group of \(k\) classifiers picks the same _wrong_ label \(f_{i}()= y\) can be expressed with a Binomial distribution wrapping \(N\) Bernoulli processes:

\[P_{ y}(k)=^{k}(1-)^{(N-k)}\] (6)

**Revisiting model ensembling for TTA.** Eq. (6) holds as long as all events modeled as Bernoulli processes are independent. Thus, we have an equivalent error estimate for the setup in which only a single classifier \(f\) is present and \(X_{y}=\{_{i}\}_{i=1}^{N}\) is a set of independent examples with the same underlying label \(y\). Within this framework, any group of \(k\) examples in \(X_{y}\) to which the classifier has assigned the same label \(\) is also a set of independent Bernoulli processes, whose error probability is still quantified via Eq. (6). Note that this resembles the TTA setup in the presence of \(N\) views of the source sample \(\), as long as augmentations do not change their underlying labels. We refer the reader to Appendix H for a discussion about the independence assumption among different views.

\(\) **is better than \(p\) (if \(f\) is calibrated).** The final step can be taken through the lens of _model calibration_, a property requiring that the confidence of a classifier matches its accuracy. For example, a calibrated classifier \(f\) whose confidence is \(0.7\) is expected to be correct \(70\%\) of the times. In the previous discussion, if we denote with \(k(y)\) the number of examples correctly labeled as \(y\), then the accuracy of the classifier is exactly \(k(y)/N\). It follows that there is a positive correlation between accuracy and confidence if \(f\) exhibits good calibration, i.e., \( k(y)/N(y)\). Thus, the probability of picking the wrong class with this marginal probability is approximated by Eq. (6). Given this relationship, we have that \((y)=()\) if \(k(y)\) matches or exceeds the majority within \(N\). Thus, the probability of picking the wrong class with \(\) is approximated by marginalizing out all values of \(k\) that satisfy this criterion, which entails that the error or \(\) can be expressed with the cumulative distribution of (6):

\[P_{ y}()=_{k= N/2+1}^{N} ^{k}(1-)^{(N-k)}\] (7)

From the Condorcet Jury Theorem , we know that Eq. (7) is a _monotonically decreasing function_ if the error \(\) is better than random guessing, which is likely to be the case for VLMs pretrained on a massive amount of web data such as CLIP. Hence, we conclude that the error of \(\) is a realistic lower bound for the base model error \(\)_over a set of independent data points sharing the same label_.

**Does this lower bound empirically realize?** We evaluate if the error of \(\) consistently lower bounds the error of \(p\) also in practical use cases, where model calibration is unknown and the label space is large. For this, we use CLIP-ViT-B-16, the ImageNet validation set, and four datasets reflecting Natural Distribution Shifts [12; 13; 32; 46]. For all classes in each dataset, we first draw all images sharing the same label (\(X_{y}\)). Then, we compute the expected error \((y)\) of the model on this subset, together with the error of \(\) (ideally, Eq. (6)). Lastly, we average these errors over the entire label space \(\). We do not restrict to the cases where \(y\) is supported by the majority and we do not re-organize predictions in a _one-versus-all_ scheme. Fig. 1(a) clearly shows that the error of \(\) is a lower bound to the base error of the model also in practical use cases where the label space is large and guarantees on model calibration are possibly missing. Importantly, this phenomenon persists _no matter the dataset_.

## 3 Simple and surprisingly strong TTA (for free)

The main point of Section 2.2 is that MEM generally does not affect the predominant class of the marginal probability distribution \(\). On the other hand, from Section 2.3 one can conclude that through \(\) the model becomes a much stronger classifier. Summarizing:

\[&((| ,_{}))=((| ,_{}^{*}))\\ &P_{ y}((| ,_{})) P_{ y}(p(|,_{}))\\ \\ P_{ y}((|,_{}^{* })) P_{ y}(p(|,_{}^{*})) \] (8)

Chaining observations together, it emerges that:

\[P_{ y}(p(|,_{}^{*})) P_{  y}((|,_{}^{*}))=P _{ y}((|,_{}))\] (9)

_i.e._, if all assumptions are met, the error of MEM \(\) error of \(\) after MEM \(=\) error of \(\)_without_ MEM. All in all, this TTA framework is hiding a surprisingly strong and optimization-free baseline: \(\)! Next, we highlight the detrimental impact of data augmentations on this marginal probability distribution and introduce a simple trick to recover its reliability: _zeroing-out_ the Softmax temperature.

### Augmentations undermine the reliability of \(\)

While augmentations are essential in TTA to obtain multiple views of the test instance, noisy views may constitute Out-of-Distribution (OOD) data, thus having the undesired effect of un-calibrating the model. To sidestep this issue, one can attempt to discriminate between in-distribution (_w.r.t._ to the pretraining data) and OOD views. Given that low confidence is a common trait in OOD data, a viable way to discriminate is confidence-based filtering, such as in TPT . Formally, a smaller set of confident views are obtained following \(X_{filt}=\{_{i} X|H(p(|_{i},_{}))<\}\), where \(\) is a

Figure 1: Motivating findings. (a) Comparison between the expected error of CLIP-ViT-B-16, denoted as \((y)\), and the error of the marginal probability distribution obtained by marginalizing over examples with the same label, \(P_{ y}()\); (b) Reliability diagrams of CLIP-ViT-B-16 on the ImageNet validation set (left), and its augmented version (right), showing that augmentations largely un-calibrate CLIP exclusively due to overconfidence while leading to slightly better overall accuracy.

threshold retaining the views whose entropy is in the bottom-10% percentile (lowest entropy). Despite its effectiveness, this filter cannot help when the reliability of \(\) is undermined by _overconfidence_.

**Augmentations lead to poor calibration.** We demonstrate the impact of augmentation-induced overconfidence using the same model and datasets of Section 2.3. For each dataset, we generate an augmented counterpart following the augmentation and filtering setup of TPT , _i.e._: we augment an input \(N=64\) times using simple random resized crops and horizontal flips. Then, we only retain 10% of the \(N\) views according to confidence-based filtering, resulting in 6 views per sample. Consequently, each augmented dataset contains \(6\) more data points than its plain counterpart. The Expected Calibration Error (ECE)  reported in Appendix C conveys that C zero-shot CLIP is well-calibrated (ECE \(\,<0.1\) for all datasets), strongly supporting the theory of Section 2.3 and 2_the augmented visual space greatly increases the calibration error_.

**Poor calibration is frequently linked to overconfidence.** We investigate the reason for the increase in ECE by presenting reliability diagrams for the ImageNet validation set in Fig. 1(b). In a reliability diagram, every bar below the identity line \(y=x\) signals overconfidence (_i.e._, the confidence on the x-axis prevails over the accuracy on the y-axis), while the opposite signals under-confidence. Notably, in the scope of our experiments, overconfidence is the primal factor leading to an increase in the ECE. The error rate, in contrast, decreases slightly. In Appendix C, we also experiment across all datasets for Natural Distribution Shifts and different CLIP models pretrained on the 2B subset of LAION . Importantly, this phenomenon further persists within this extended experimental suite.

### Zero: Test-Time Adaptation with "zero" temperature

Since its reliability is severely undermined by augmentations-induced overconfidence, directly predicting through \(\) is not an enticing baseline for TTA. Concurrently, we also know that the error rate does not decrease when predicting over the augmented visual space. Hence, we are interested in finding an efficient way to capitalize on these observations: relying on the predictions over the views, while ignoring potentially misleading confidence information. The key is to note that both desiderata are obtained by explicitly tweaking a single parameter of the model: _the temperature_. Specifically, setting the temperature to (the limit of) zero corresponds to converting probability distributions into one-hot encodings, hence exclusively relying on their \(\) when marginalizing. Inspired by this idea we propose Zero, Test-Time Adaptation with "**zero**" temperature.

**Procedure.** Zero follows these simple steps: 1 augment, 2 predict, 3 retain the most confident predictions, 4 set the Softmax temperature to zero and 5 marginalize. The final prediction is the \(\) of the marginal probability distribution computed after "zeroing-out" the temperature, _i.e._:

\[(,_{},C)=*{arg\,max}_{c [1,,C]}(_{i=1}^{N}(_{i} X_{})_{ 0^{+}}p(y=c|_{i},_{},) ),\] (10)

where \(\) is an indicator function, whose output is \(1\) if \(_{i} X_{}\) and \(0\) otherwise, and \(X_{}\) is the set of confident views _before_ tweaking the temperature, _i.e._, \(_{i} X_{}\) if \(H(p(|_{i},_{},)<\).

**Efficient Implementation.** In all its simplicity, Zero is computationally lightweight. In closed set assumptions where the class descriptions (and thus their embeddings) are fixed, Zero only requires a single batched forward pass through the vision encoder, just as much as needed to forward the \(N\) views. Additionally, since the temperature is explicitly tweaked, Zero needs _no backpropagation at all_ and can be implemented in a few lines of code. For reference, a PyTorch-like implementation  is reported in Algorithm 1.

**Equivalent perspective and final remark.** We bring to attention a simple scheme which corresponds to Zero: _voting_ over (confident) augmentations. Drawing from the theory of ensembling, note that the error rate of the voting paradigm is exactly described by Eq. (6). Essentially,this means that Zero capitalizes on the theoretical insights while circumventing practical issues stemming from augmentations. We also highlight that Zero is subtly hidden within any TTA framework relying exclusively on MEM, since computing \(\) is inevitable therein. For this reason, we refer to Zero as a _baseline_ for TTA. Our goal diverges from introducing a "novel" state-of-the-art method for TTA. In contrast, we advocate the importance of evaluating simple baselines.

## 4 Experiments

In this section, we present a comprehensive experimental evaluation of Zero. Similarly to [38; 33; 54], we always work in the setup of _single test point_ adaptation. Our results show that Zero, alongside its simplicity, is an effective and efficient approach for TTA.

### Experimental Protocol

**Baselines.** We compare Zero to three strategies for TTA with VLMs: 1 TPT , 2 PromptAlign , and 3 Reinforcement Learning from CLIP Feedback (RLCF) . As introduced in Section 2, TPT works by minimizing the entropy of \(\). In contrast, PromptAlign relies on a pretrained MaPLe initialization  and pairs the MEM objective with a distribution alignment loss between layer-wise statistics encountered online and pretraining statistics computed offline. Finally, RLCF does not include MEM in its framework; Zhao et al.  shows that, if rewarded with feedback from a stronger teacher such as CLIP-ViT-L-14, the smaller CLIP-ViT-B-16 can surpass the teacher itself.

**Models.** As different approaches consider different backbones in the original papers, we construct different comparison groups to ensure fair comparisons with all TTA baselines [38; 33; 54].

_Group 1:_ When comparing to TPT, we always use CLIP-ViT-B-16. Shu et al.  also reports CLIP-Ensemble, _i.e._, CLIP enriched with an ensemble of hand-crafted prompts. While the design of TPT does not allow leveraging text ensembles (as also pointed out by concurrent work ), Zero seamlessly integrates with CLIP-Ensemble. We denote this variant with Zero+Ensemble.

_Group 2:_ When comparing to PromptAlign, we follow Samadh et al.  and start from a MaPLe initialization for a fair comparison. MaPLe prompts are learned on ImageNet, following . Within this group, we also report TPT on top of MaPLe, as in .

_Group 3:_ When comparing to RLCF, we use both CLIP-ViT-B-16 and CLIP-ViT-L-14 as in . Specifically, confidence-based filtering acts on top of the output of the first model, and the selected inputs are passed to the second model for the final output. Both forward passes are inevitable in RLCF, so this scheme corresponds to "early-exiting" the pipeline, exactly as per MEM. RLCF can vary according to (i) the parameter group being optimized and (ii) the number of adaptation steps. We denote with \(_{v}\) the full image encoder tuning, with \(_{}\) prompt tuning, and with \(t\) the number

  
**Method** & ImageNet & A & V2 & R & Sketch & Mean \\   \\ _Zero-Shot_ & 66.73 & 47.87 & 60.86 & 73.98 & 46.09 & 59.11 \\ _Ensemble_ & 68.34 & 49.89 & 61.88 & 77.65 & 48.24 & 61.20 \\ TPT & 68.98 & 54.77 & 63.45 & 77.06 & 47.94 & 62.44 \\ Zero & 69.31\(\)0.13 & 59.61\(\)0.19 & 64.16\(\)0.03 & 77.22\(\)0.05 & 48.40\(\)0.07 & 63.74 \\ Zero+Ensemble & **71.17\(\)**0.06 & **62.75\(\)**0.14 & **65.23\(\)**0.08 & **80.75\(\)**0.02 & **50.59\(\)**0.08 & **66.10** \\   \\ _Zero-Shot_ & - & 50.90 & 64.07 & 76.98 & 49.15 & 60.28 \\ TPT & - & 58.08 & 64.87 & 78.12 & 48.16 & 62.31 \\ PromptAlign & - & 59.37 & 65.29 & 79.33 & 50.23 & 63.55 \\ Zero & - & **63.32\(\)**0.26 & **66.81\(\)**0.43 & **79.74\(\)**0.32 & **51.07\(\)**0.47 & **65.23** \\   \\ _Zero-Shot_ & 73.44 & 68.82 & 67.80 & 85.40 & 57.84 & 70.66 \\ RLCF +Ensemble & 73.23 & 65.45 & 69.77 & 83.35 & 54.74 & 69.31 \\ RLCF +Ensemble & 74.85 & 73.71 & 69.77 & 86.19 & 57.10 & 72.32 \\ Zero & **75.52\(\)**0.03 & **75.15\(\)**0.26 & **70.37\(\)**0.05 & **87.21\(\)**0.09 & **59.61\(\)**0.04 & **73.57** \\   

Table 1: Natural Distribution Shifts. TTA methods are grouped according to the baseline model and top-1 accuracy is reported. **Bold text** is the best method within each group.

of adaptation steps. For example, \(^{_{}}_{t=3}\) indicates RLCF with prompt tuning for 3 TTA steps. Note that, since all methods need to forward more than one image to the teacher model, the zero-shot baseline of this group is exactly zero-shot classification with CLIP-ViT-L-14.

**Pretrainings.** This Section deals with models officially released by OpenAI . Appendix B further reports experiments with LIAON-pretrained CLIP models , as well as the soft prompt initialization with supervised Context Optimization (CoOp) from .

**Benchmarks.** We follow the established experimental setup of , evaluating Zero on Natural Distribution Shifts and Fine-grained Classification (also referred to as "Cross-Datasets Generalization" in previous works). For the former, we consider the ImageNet validation set and the four datasets for Natural Distribution Shifts already presented in Section 2, commonly considered Out-of-Distribution (OOD) datasets for CLIP. For fine-grained classification, we evaluate all TTA methods on 10 datasets. Specifically, we experiment with Oxford-Flowers (FWLR) , Describable Textures (DTD) , Oxford-Pets (PETS) , Stanford Cars (CARS) , UCF101 (UCF) , Caltech101 (CAL), Food101 (FOOD) , SUN397 (SUN), FGVC-Aircraft (AIR)  and EuroSAT (ESAT) . For all of these datasets, we refer to the test split in Zhou et al.  as per the common protocol.

**Textual prompts.** When +Ensemble is specified, we do _not_ use dataset-specific templates. In contrast, we use the set of 7 generic templates highlighted in the official CLIP repository  across all datasets. When adapting MaPLe, we stick to the ImageNet-learned prompts released by  and evaluate them cross-datasets as in .

**Implementation Details.** The augmentation pool \(\) only contains random resized crops and random horizontal flips. The only hyperparameter of Zero is the percentile for confidence-based filtering, which is set to \(0.3\) after validation on ImageNet (following standard practice ) and kept fixed _for all datasets_. We inherit the setup of TPT with \(N=64\), crafting \(63\) augmentations to collate with the source image. To ensure hardware differences do not play any role in comparisons, we execute all TTA methods under the same hardware setup by running the source code of each repository with no modifications. We always use 1 NVIDIA A100 GPU and FP16 Automatic Mixed Precision. Results are averaged over 3 different seeds. Unless otherwise specified, all tables report top-1 accuracy.

### Results

**Natural Distribution Shifts.** Results for Natural Distribution Shifts are reported in Table 1.

_Group 1 (TPT):_Zero _surpasses TPT consistently on all datasets_. Among OOD datasets, peak difference is reached with ImageNet-A, where Zero outperforms TPT by \(+4.84\%\). Enriching Zero with hand-crafted prompts improves results further, with an average margin of \(+3.66\%\)_w.r.t._ TPT.

_Group 2 (PromptAlign):_ Within the second comparison group, Zero _outperforms PromptAlign on all datasets_, with \(+1.68\%\) being the gap in average performance. Zero consistently outperforms TPT also when the baseline initialization is MaPLe (by an average of \(+2.92\%\)). Please note that we omit evaluation on ImageNet for this group, since PromptAlign adopts token-level statistics from this dataset when adapting to test points, which would render the comparison unfair. For completeness, we report that zero-shot MaPLe achieves an accuracy of \(70.72\%\) on ImageNet, which is improved to \(72.99\%\) by adapting with Zero (\(+2.27\%\)).

_Group 3 (RLCF):_ We follow  and report RLCF variants with \(t=3\) steps. In this group, Zero outperforms RLCF in 5 out of 5 datasets, with a gap in the average performance of \(+1.25\%\). Importantly, RLCF is only close to Zero with image encoder tuning; only prompt tuning is insufficient.

**Fine-grained Classification.** Results for fine-grained classification are shown in Table 2. To foster readability, the standard deviations of Zero are separately reported in Table 11 (Appendix).

_Group 1 (TPT):_ Default Zero improves over the zero-shot baseline CLIP-ViT-B-16, but is outperformed by TPT with an average margin of \(-0.57\%\). However, extending Zero with hand-crafted prompts (something that TPT cannot do _by design_) is sufficient to outperform TPT on 7 out of 10 datasets, and obtain an average improvement of \(+0.74\%\).

_Group 2 (PromptAlign):_ On average, PromptAlign has an improvement of \(+0.5\%\) over Zero. However, note that this is mostly influenced by the performance on one dataset only (EuroSAT) and that, in contrast, Zero _surpasses PromptAlign in 7 out of 10 datasets_. In line with the previous benchmark, Zero better adapts MaPLe than TPT, again outperforming it in 7 out of 10 datasets.

_Group 3 (RLCF):_ As Zhao et al.  do not report results on fine-grained classification, we use their code to evaluate four RLCF variants: \(_{v}\) and \(_{}\) tuning, with \(t=1\) and \(t=3\) adaptation steps. We find that Zero largely outperforms RLCF regardless of the configuration. Even with respect to the strongest RLCF \(_{t=3}^{_{v}}\) variant, Zero obtains an average improvement of \(+2.28\%\).

**Computational Requirements.** The complexity of Zero does not scale linearly with the size of the label space, as it does for prompt-tuning strategies. To quantify the computational gain of Zero _w.r.t_ other TTA methods, we report the runtime per image and peak GPU memory in Table 3 under the same hardware (_i.e._, 1 NVIDIA RTX 4090. We compare the computational requirements of Zero to TPT and the RLCF pipeline in a worst-case scenario where the label space is large (ImageNet). We omit PromptAlign from our analysis since it has slightly worse computational performance than TPT.

Zero is \(9.5\) faster than TPT taking \(12.61\) less memory, corresponding to an order of magnitude of computational savings in both time and space. Concerning the slowest RLCF variant (prompt tuning), Zero is \(15\) faster and takes \(7.22\) less memory. In the faster RLCF \(_{v}\), text classifiers are also cached; nevertheless, Zero is \(2.25\) faster and \(3.5\) more memory friendly.

## 5 Related Work

Closest to our work is a recent and very active research thread focusing on Episodic TTA with VLMs [38; 33; 54; 42]. As discussed in the manuscript, these methods mostly rely on prompt learning, a parameter-efficient strategy that only trains over a small set of input context vectors . Narrowing down to VLMs, notable examples of prompt learning approaches include CoOp , CoCoOp , and MaPLe . Episodic TTA has also been explored with traditional unimodal networks, such as ResNets , where MEM is still a core component . In this context, MEM has recently been enriched with sharpness-  or shape-aware filtering . Due to its nature, Episodic TTA is completely agnostic to the temporal dimension and is powerful when no reliable assumptions on the test data can be taken. Some other works relax these constraints and integrate additional assumptions such as _batches_ of test data being available instead of single test points . When test data are

  
**Method** & FLWR & DTD & PETS & CARS & UCF & CAL & FOOD & SUN & AIR & ESAT & Mean & Median \\   \\ _Zero-Shot_ & 67.44 & 44.27 & 88.25 & 65.48 & 65.13 & 93.35 & 83.65 & 62.59 & 23.67 & 42.01 & 63.58 & 65.31 \\ _Ensemble_ & 67.07 & 45.09 & **88.28** & 66.16 & 67.51 & 93.91 & 84.04 & 66.26 & 23.22 & **50.42** & 65.20 & 66.66 \\ TPT & **68.75** & **47.04** & 87.23 & 66.68 & 68.16 & 93.93 & 84.67 & 65.59 & 23.13 & 42.86 & 64.78 & 67.42 \\ ZERO & 67.68 & 46.12 & 87.75 & 68.04 & 67.77 & 93.66 & 86.53 & 65.03 & **25.21** & 34.33 & 64.21 & 67.72 \\ Zero\_ensemble_ & 67.17 & 45.86 & 87.83 & **68.97** & **69.18** & **94.41** & **86.77** & **67.63** & **25.21** & 42.17 & **65.52** & **68.30** \\   \\ _Zero-Shot_ & 72.23 & 46.49 & 90.49 & 65.57 & 68.69 & 93.53 & 86.20 & 67.01 & 24.74 & **48.06** & 66.30 & 67.85 \\ TPT & 72.37 & 45.87 & 90.72 & 66.50 & 69.19 & 93.59 & 86.64 & 67.54 & 24.70 & 47.80 & 66.49 & 68.36 \\ PromptAlign & **72.39** & 47.24 & **90.76** & 68.50 & 69.47 & 94.01 & 86.65 & 67.54 & 24.80 & 47.86 & **66.92** & 68.99 \\ Zero & 71.62 & **47.89** & 90.60 & **68.58** & **69.87** & **94.48** & **87.20** & **68.20** & **26.25** & 39.47 & 66.42 & **69.23** \\   \\ _Zero-Shot_ & 75.76 & 51.83 & 92.86 & 76.16 & 73.70 & 94.04 & 88.03 & 66.96 & 30.54 & **54.38** & 70.43 & 74.73 \\ RLCF \(_{t=1}^{}\) & 71.58 & 50.34 & 89.01 & 69.76 & 69.84 & 94.09 & 85.90 & 67.33 & 23.71 & 46.87 & 66.84 & 69.80 \\ RLCF \(_{t=3}^{_{v}}\) & 72.49 & 51.93 & 89.55 & 72.91 & 72.31 & 95.00 & 86.84 & 69.04 & 25.40 & 45.96 & 68.14 & 72.40 \\ RLCF \(_{t=1}^{_{v}}\) & 72.56 & 52.21 & 89.51 & 63.12 & 71.49 & 94.65 & 86.90 & 68.50 & 24.06 & 47.74 & 67.07 & 70.00 \\ RLCF \(_{t=3}^{_{v}}\) & 71.74 & 53.27 & 91.15 & 70.93 & 73.24 & 94.73 & 87.28 & 69.38 & 28.54 & 47.41 & 68.77 & 71.34 \\ ZERO & **76.41** & **53.63** & **94.08** & **78.39** & **74.68** & **95.21** & **90.66** & **69.61** & **33.62** & 44.21 & **71.05** & **75.55** \\   

Table 2: Fine-grained classification. TTA methods are grouped according to the reference baseline, top-1 accuracy is reported and **bold text** indicates the best performer of each group.

  
**Metric** &  &  \\   & TPT & Zero & RLCF \(_{t=3}^{_{v}}\) & RLCF \(_{t=3}^{_{v}}\) & Zero \\  Time [\(s\)] & 0.57\(\)0.01 & **0.06\(\)**0.01 & 1.20\(\)**0.02 & 0.18\(\)**0.01 & **0.08\(\)**0.02 \\ Mem [GB] & 17.66 & **1.40** & 18.64 & 9.04 & **2.58** \\   

Table 3: Computational requirements of different TTA methods.

assumed to belong to the same domain, one can rely on various forms of knowledge retention as a powerful mechanism to gradually incorporate domain knowledge  or avoid forgetting . The synergy between TTA and retrieval is also emerging as a powerful paradigm when provided with access to huge external databases . We particularly believe this can be a promising direction.

Closely related to our work are also Test-Time Training (TTT) and TTAug. In TTT the same _one sample_ learning problem of Episodic TTA is tackled with auxiliary visual self-supervised tasks, such as rotation prediction  or masked image modeling , which require specialized architecture heads and are not directly applicable to VLMs. TTAug has recently been theoretically studied . It boils down to producing a large pool of augmentations to exploit at test time , or to learn from . In all its simplicity, Zero can be seen as a strong TTAug baseline for VLMs, which, differently from concurrent work , does not involve any form of optimization.

## 6 Limitations

Zero can seamlessly adapt a wide range of VLMs on arbitrary datasets without requiring extensive computational resources and is backed by theoretical justifications. However, we delineate four major limitations to our method which we report here.

**Preliminary observations.** The first limitation concerns the preliminary observations which led to Zero, such as augmentation-induced overconfidence or a comparable error rate between source and augmented datasets. These observations may not persist if VLMs or benchmarks change significantly in the future, potentially leading to poor adaptation. For example, we have observed a consistent failure case for TTA with EuroSAT , with Zero incurring large performance drops _w.r.t._ simple zero-shot classification. In Appendix F we unravel this worst-case further.

**Theoretical assumptions.** The second limitation stems from theoretical assumptions, the core one being the invariance of the marginal probability distribution to marginal entropy minimization. While our proposition guarantees invariance if entropy is globally minimized and the negative variation to the probability of the most probable class is less than the initial probability itself, these theoretical assumptions may not hold all the time. In this work, we supported our assumptions with empirical verification but, as per the first limitation, these may not extend to the space of all models and datasets. We refer the interested readers to Appendix A for a more in-depth discussion about the invariance of the prediction of \(\) to MEM.

**Independence among views.** A third worthy-of-note limitation relates to the independence assumption among the views from which the marginal probability distribution is obtained. As we discussed in Section 2.3, the views themselves do not have any _direct_ dependency, but they are still partially related through the source image from which they stem. Related to this limitation, we hypothesize that extending Zero in a Retrieval-Augmented TTA setup (or a cache-based one) could improve the results. The discussion on this topic is extended in Appendix H.

**Linear complexity with respect to augmented views.** Finally, despite being much lighter than the current state-of-the-art TTA strategies, Zero's computational requirements in the visual branch scale linearly with the number of views, since all of them need to be independently forwarded. On this, we believe that exploring how to augment directly in the latent visual space to also circumvent the forward pass of the vision encoder is an intriguing direction.

## 7 Conclusions

We theoretically investigated Marginal Entropy Minimization, the core paradigm of the current research on Test-Time Adaptation with VLMs. Building on our theoretical insights, we introduced Zero: a frustratingly simple yet strong baseline for TTA, which only relies on a single batched forward pass of the vision encoder. Zero works by setting the temperature of the Softmax operator to "zero" before marginalizing across confident views, which is equivalent, in terms of output, to the widely known paradigm of majority voting. Our experimental results on Natural Distribution Shifts and Fine-grained Classification unveil that Zero favorably compares to state-of-the-art TTA methods while requiring relatively negligible computation. We hope our findings will inspire researchers to push the boundaries of TTA further.

Acknowledgements.The authors acknowledge the CINECA award under the ISCRA initiative for the availability of high-performance computing resources and support. Matteo Farina is supported by the PRIN project "LEGO-AI" (Prot.2020TA3K9N) and the PAT project "AI@TN". This work was supported by the projects EU Horizon ELIAS (No. 101120237), AI4TRUST (No.101070190), FAIR - Future AI Research (PE00000013), funded by NextGeneration EU, and carried out in the Vision and Learning joint laboratory of Fondazione Bruno Kessler and the University of Trento, Italy.