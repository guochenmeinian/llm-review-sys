# Lending Interaction Wings to Recommender Systems with Conversational Agents

Jiarui Jin\({}^{1,}\), Xianyu Chen\({}^{1}\), Fanghua Ye\({}^{2}\), Mengyue Yang\({}^{2}\), Yue Feng\({}^{2}\),

Weinan Zhang\({}^{1,}\), Yong Yu\({}^{1}\), Jun Wang\({}^{2,}\)

\({}^{1}\)Shanghai Jiao Tong University, \({}^{2}\)University College London

{jinjiarui97,wmzhang,xianyujun,yyu}@sjtu.edu.cn

{fanghua.ye.19,mengyue.yang.20,yue.feng.20,jun.wang}@ucl.ac.uk

Work done during Jiarui Jin's visit at University College London.Correspondence: Weinan Zhang, Jun Wang.

###### Abstract

Recommender systems trained on _offline_ historical user behaviors are embracing conversational techniques to _online_ query user preference. Unlike prior conversational recommendation approaches that systemically combine conversational and recommender parts through a reinforcement learning framework, we propose CORE, a new _offline-training and online-checking_ paradigm that bridges a COnversational agent and REcommender systems via a unified _uncertainty minimization_ framework. It can benefit _any_ recommendation platform in a plug-and-play style. Here, CORE treats a recommender system as an _offline relevance score estimator_ to produce an estimated relevance score for each item; while a conversational agent is regarded as an _online relevance score checker_ to check these estimated scores in each session. We define _uncertainty_ as the summation of _unchecked_ relevance scores. In this regard, the conversational agent acts to minimize uncertainty via querying either _attributes_ or _items_. Based on the uncertainty minimization framework, we derive the _expected certainty gain_ of querying each attribute and item, and develop a novel _online decision tree_ algorithm to decide what to query at each turn. We reveal that CORE can be extended to query attribute values, and we establish a new Human-AI recommendation simulator supporting both open questions of querying attributes and closed questions of querying attribute values. Experimental results on 8 industrial datasets show that CORE could be seamlessly employed on 9 popular recommendation approaches, and can consistently bring significant improvements, compared against either recently proposed reinforcement learning-based or classical statistical methods, in both hot-start and cold-start recommendation settings. We further demonstrate that our conversational agent could communicate as a human if empowered by a pre-trained large language model, e.g., gpt-3.5-turbo.

## 1 Introduction

Recommender systems are powerful tools to facilitate users' information seeking ; however, most prior works solely leverage offline historical data to build a recommender system. The inherent limitation of these recommendation approaches lies in their offline focus on users' historical interests, which would not always align with users' present needs. As intelligent conversational assistants (a.k.a., chat-bots) such as ChatGPT and Amazon Alexa, have entered the daily life of users, these conversational techniques bring an unprecedented opportunity to online obtain users' current preferences via conversations. This possibility has been envisioned as conversationalrecommender systems and has inspired a series of conversational recommendation methods [26; 30; 44]. Unfortunately, all of these approaches try to model the interactions between users and systems using a reinforcement learning-based framework, which inevitably suffers from data insufficiency and deployment difficulty, because most recommendation platforms are based on supervised learning.

In this paper, we propose CORE that can bridge a COnversational agent and REcommender systems in a plug-and-play style. In our setting, a conversational agent can choose either to query (a.k.a., to recommend) an item (e.g., Hotel A) or to query an attribute (e.g., Hotel Level), and the user should provide their corresponding preference. Here, the goal of the conversational agent is to find (a.k.a., to query) an item that satisfies the user, with a minimal number of interactions.

We formulate the cooperation between a conversational agent and a recommender system into a novel _offline-training and online-checking_ framework. Specifically, CORE treats a recommender system as an _offline relevance score estimator_ that offline assigns a relevance score to each item, while a conversational agent is regarded as an _online relevance score checker_ that online checks whether these estimated relevance scores could reflect the relevance between items and the user's current needs. Here, "checked items" means those items, we can certainly say that they can not satisfy the user according to already queried items and attributes. We introduce a new _uncertainty_ metric defined as the summation of estimated relevance scores of those unchecked items. Then, the goal of our conversational agent can be formulated as minimizing the uncertainty via querying items or attributes during the interactions. To this end, we derive _expected certainty gain_ to measure the expectation of uncertainty reduction by querying each item and attribute. Then, during each interaction, our conversational agent selects an item or an attribute with the maximum certainty gain, resulting in _an online decision tree_ algorithm. We exemplify the above process in Figure 1.

Notice that users usually do not hold a clear picture of their preferences on some attributes (i.e., attribute IDs), e.g., what Hotel Level they need, instead, they could have a clear preference on a specific value of an attribute (i.e., attribute value), e.g., Hotel Level=5 is too expensive for a student user. Also, asking an open question of querying attributes could result in an unexpected answer, e.g., a user answers 3.5 to Hotel Level. In this regard, querying attribute values leading to closed questions (i.e., Yes or No questions) could be a better choice. We reveal that CORE could be directly applied to the above querying strategies. We also develop a new Human-AI recommendation simulator that supports both querying attributes and attribute values.

In practice, we extend CORE to handle continuous attributes and to consider the dependence among attributes. Moreover, we demonstrate that our conversational agent could straightforwardly be empowered by a pre-trained language model, e.g., gpt-3.5-turbo, to communicate as a human. Note that CORE poses no constraint on recommender systems, only requiring the estimated relevance scores. Therefore, CORE can be seamlessly applied to _any_ recommendation platform. We conduct experiments on 8 industrial datasets (including both tabular data, sequential behavioral data and graph-structured

Figure 1: An illustrated example of CORE, an _offline-training and online-checking_ framework, where a recommender system operates as _an offline relevance score estimator_ (colored in green), while a conversational agent acts as _an online relevance score checker_ (colored in blue). Concretely, given a matrix of candidate items, as shown in (a), the recommender system could _offline_ assign an estimated relevance score to each item, and then the conversational agent would _online_ check these scores by querying either items or attributes, depicted in (b).

data) with 9 popular recommendation approaches (e.g., DeepFM , DIN ). Experimental results show that CORE can bring significant improvements in both hot-start recommendation (i.e., the recommender system is offline trained) and cold-start recommendation (i.e., the recommender system is not trained) settings. We compare CORE against recently proposed reinforcement learning based methods and classical statistical methods, and CORE could consistently show better performance.

## 2 Bridging Conversational Agents and Recommender Systems

### Problem Formulation

Let \(\) denote a set of users, \(=\{v_{1},,v_{M}\}\) be a set of \(M\) items, \(=\{x_{1},,x_{N}\}\) be a set of \(N\) attributes (a.k.a., features) of items. We consider a recommender system as a mapping function, denoted as \(_{}:\) that assigns an estimated relevance score to each item regarding a user. Then, during each online session, a conversational agent also can be formulated as a mapping function, denoted as \(_{}:\) that chooses either an item or an attribute to query user, and \(=\) denotes the action space of the conversational agent. For convenience, we focus on the _most_ favorite item of the given user in each session. In other words, our goal is to identify the item that will receive the _first_ click when all items are equally presented. We call this item as _target item_.

For this purpose, \(_{}()\) acts as an _offline estimator_ to produce an estimated relevance distribution of items regarding each user through offline training on previous behavioral data; while \(_{}()\) operates as an _online checker_ to check whether these estimated scores fit the user's current needs through online interactions. Here, "checked items" denote those items that can not be the target item according to queried items and attributes. For example, as Figure 1 illustrates, after querying Breakfast Service, we have items Hotel C and Hotel D checked. We introduce _uncertainty_ as the summation of estimated relevance scores of unchecked items. Formally, we have:

**Definition 1** (Uncertainty and Certainty Gain).: _For the \(k\)-th turn, we define uncertainty, denoted as \(_{k}\), to measure how many estimated relevance scores are still unchecked, i.e.,_

\[_{k}(\{_{}(v_{m})|v_{m} _{k}\}),\] (1)

_where \(_{}(v_{m})\)1 outputs the estimated relevance score for item \(v_{m}\), \(_{k}\) is the set of all the unchecked items after \(k\) interactions, and \(_{k}\) is initialized as \(_{0}=\). Then, the certainty gain of \(k\)-th interaction is defined as \(_{k}_{k-1}-_{k}\), i.e., how many relevance scores are checked at the \(k\)-th turn. Since our goal is to find the target item, if \(_{}()\) successfully finds one at the \(k\)-th turn, then we set all the items checked, namely \(_{k}=0\)._

In this regard, our conversational agent is to minimize \(_{k}\) at each \(k\)-th turn via removing those checked items from \(_{k}\). Considering that online updating \(_{}()\) is infeasible in practice due to the high latency and computation costs, the objective of \(_{}()\) can be expressed as:

\[_{_{}^{*}}K,_{K}=0,\] (2)

where \(K\) is the number of turns, and \(_{}^{*}\) means that the recommender system is frozen. To this end, the design of our conversational agent could be organized as an uncertainty minimization problem.

### Comparisons to Previous Work

Bridging conversational techniques and recommender systems has become an appealing solution to model the dynamic preference and weak explainability problems in recommendation task [15; 26], where the core sub-task is to dynamically select attributes to query and make recommendations upon the corresponding answers. Along this line, the main branch of previous studies is to combine the conversational models and the recommendation models from a systematic perspective. Namely conversational and recommender models are treated and learned as two individual modules [30; 2; 44; 48] in the system. The system is developed from a single-turn conversational recommender system [5; 6; 47] to a multiple-turn one . To decide when to query attributes and when to make recommendations (i.e., query items), recent papers [30; 31; 44; 30] develop reinforcement learning-based solutions, which are innately suffering from insufficient usage of labeled data and high complexity costs of deployment. However, reinforcement learning approaches often demand a substantial number of training samples, a condition referred to as data insufficiency, and require operating within a relatively compact action space. Instead, our conversational agent can be regarded as a generalist agent that can query either items or attributes. In addition, our querying strategy is derived based on the uncertainty minimization framework, which only requires estimated relevance scores from the recommender system. Hence, CORE can be straightforwardly applied to _any_ supervised learning-based recommendation platform, in a plug-and-play way, free of meticulous reward function design in reinforcement learning methods. CORE is more friendly to open-world scenarios especially when training data is limited.

We present the connections to other previous work (e.g., decision tree algorithms) in Appendix A5.

## 3 Making the Conversational Agent a Good Uncertainty Optimizer

### Building an Online Decision Tree

As described in Section 2.1, the aim of our conversational agent is to effectively reduce uncertainty via querying either items or attributes. The core challenge is how to decide which item or attribute to query. To this end, we begin by introducing _expected certainty gain_ to measure the expectation of how much uncertainty could be eliminated by querying each item and each attribute. Then, we can choose an item or an attribute with the maximum expected certainty gain to query.

Formally, let \(_{k}\) denote the set of unchecked attributes after \(k\) interactions. Then, for each \(k\)-th turn, we define \(a_{}\) as an item or an attribute to query, which is computed following:

\[a_{}=*{arg\,max}_{a_{k-1} _{k-1}}_{}((a)),\] (3)

where \(_{}()\) denotes the _expected certainty gain_ of querying \(a\), and \(a\) can be either an unchecked item (from \(_{k-1}\)) or an unchecked attribute (from \(_{k-1}\)).

\(_{}()\) **for Querying an Item.** We first consider the case where \(a_{k-1}\). Let \(v^{*}\) denote the target item in the session. Since we only need to find _one_ target item, therefore, if \(a_{k-1}\), we can derive:

\[_{}((a))& =v^{*}(a=v^{*})+_{}(a v ^{*})(a v^{*})\\ &=_{v_{m}_{k-1}}_{}(v_{m} )(a=v^{*})+_{}(a)(a  v^{*}),\] (4)

where \(a=v^{*}\) and \(a v^{*}\) denote that queried \(a\) is the target item and not. If \(a=v^{*}\), the session is done, and therefore, the certainty gain (i.e., \(_{}(a=v^{*})\)) is the summation of all the relevance scores in \(_{k-1}\). Otherwise, only \(a\) is checked, and the certainty gain (i.e., \(_{}(a v^{*})\)) is the relevance score of \(a\), and we have \(_{k}=_{k-1}\{a\}\) and \(_{k}=_{k-1}\).

Considering that \(a\) being the target item means \(a\) being the most relevant item, we leverage the user's previous behaviors to estimate the user's current preference. With relevance scores estimated by \(_{}()\), we estimate \((a=v^{*})\) as:

\[(a=v^{*})=}(a)}{(\{_{ }(v_{m})|v_{m}_{k-1}\})},\] (5)

and \((a v^{*})=1-(a v^{*})\).

\(_{}()\) **for Querying an Attribute.** We then consider the case where \(a_{k-1}\). For each queried attribute \(a\), let \(_{a}\) denote the set of all the candidate attribute values, and let \(w_{a}^{*}_{a}\) denote the user preference on \(a\), e.g., \(a\) is Hotel Level, \(w_{a}^{*}\) is \(3\). Then, if \(a_{k-1}\), we have:

\[_{}((a))=_{w_{a}_{a}} _{}(w_{a}=w_{a}^{*})(w_{a}=w_{a}^{*}),\] (6)

where \(w_{a}=w_{a}^{*}\) means that when querying \(a\), the user's answer (represented by \(w_{a}^{*}\)) is \(w_{a}\), \(_{}(w_{a}=w_{a}^{*})\) is the certainty gain when \(w_{a}=w_{a}^{*}\) happens, and \((w_{a}=w_{a}^{*})\) is the probability of \(w_{a}=w_{a}^{*}\) occurring. If \(w_{a}=w_{a}^{*}\) holds, then all the unchecked items whose value of \(a\) is not equal to \(w_{a}\) should be removed from \(_{k-1}\), as they are certainly not satisfying the user's needs.

Formally, let \(_{a_{}=w_{a}}\) denote the set of all the items whose value of \(a\) is equal to \(w_{a}\), and let \(_{a_{} w_{a}}\) denote the set of rest items. Then, \(_{}(w_{a}=w_{a}^{*})\) can be computed as:

\[_{}(w_{a}=w_{a}^{*})=(\{_{}(v_{m})|v _{m}_{k-1}_{a_{} w_{a}}\}),\] (7)

which indicates that the certainty gain, when \(w_{a}\) is the user's answer, is the summation of relevance scores of those items not matching the user preference.

To finish \(_{}((a))\), we also need to estimate \((w_{a}=w_{a}^{*})\). To estimate the user preference on attribute \(a\), we leverage the estimated relevance scores given by \(_{}()\) as:

\[(w_{a}=w_{a}^{*})=(\{_{}(v_{m})|v _{m}_{k-1}_{a_{}=w_{a}}\})}{(\{_{}(v_{m})|v_{m}_{k-1}\})}.\] (8)

In this case, we remove \(_{k-1}_{a_{} w_{a}^{*}}\) from \(_{k-1}\), namely we have \(_{k}=_{k-1}_{a_{} w _{a}^{*}}\). As attribute \(a\) is checked, we have \(_{k}=_{k-1}\{a\}\). Here, \(w_{a}^{*}\) is provided by the user after querying \(a\).

By combining Eqs. (4), (6), and (7), we can derive a completed form of \(_{}((a))\) for \(a_{k-1}_{k-1}\) (See Appendix A.1.1 for details). Then, at each \(k\)-th turn, we can always follow Eq. (3) to obtain the next query \(a_{}\). As depicted in Figure 1(b), the above process results in an online decision tree, where the nodes in each layer are items and attributes to query, and the depth of the tree is the number of turns (see Appendix A.4.3 for visualization of a real-world case).

### From Querying Attributes to Querying Attribute Values

We note that the online decision tree introduced above is a general framework; while applying it to real-world scenarios, there should be some specific designs.

\(_{}()\) **for Querying an Attribute Value.** One implicit assumption in the above online decision tree is that the user's preference on queried attribute \(a\) always falls into the set of attribute values, namely \(w_{a}^{*}_{a}\) holds. However, it can not always hold, due to (i) a user would not have a clear picture of an attribute, (ii) a user's answer would be different from all the candidate attribute values, e.g., \(a\) is Hotel Level, \(w_{a}^{*}=3.5\), and \(_{a}=\{3,5\}\), as shown in Figure 1(a). In these cases, querying attributes would not be a good choice. Hence, we propose to query attribute values instead of attribute IDs, because (i) a user is likely to hold a clear preference for a specific value of an attribute, e.g., a user would not know an actual Hotel Level of her favoring hotels, but she clearly knows she can not afford a hotel with Hotel Level=5, and (ii) since querying attribute values leads to closed questions instead of open questions, a user only needs to answer Yes or No, therefore, avoiding the user's answer to be out of the scope of all the candidate attribute values.

Formally, in this case, \(=_{x}_{k-1}\) which indicates we need to choose a value \(w_{x}_{x}\) where \(x_{k-1}\). In light of this, we compute the expected certainty gain of querying attribute value \(w_{x}\) as:

\[_{}((x)=w_{x})=_{}(w_{x}=w_{x}^{* })(w_{x}=w_{x}^{*})+_{}(w_{x} w_{x}^{*}) (w_{x} w_{x}^{*}),\] (9)

where \(w_{x}^{*}_{x}\) denotes the user preference on attribute \(x\). Here, different from querying attributes, a user would only respond with Yes (i.e., \(w_{x}=w_{x}^{*}\)) or No (i.e., \(w_{x} w_{x}^{*}\)). Therefore, we only need to estimate the certainty gain for the above two cases. \(_{}(w_{x}=w_{x}^{*})\) can be computed following Eq. (7) and \(_{}(w_{x} w_{x}^{*})\) can be calculated by replacing \(_{x_{} w_{x}}\) with \(_{x_{}=w_{x}}\). \((w_{x}=w_{x}^{*})\) is estimated in Eq. (8) and \((w_{x} w_{x}^{*})=1-(w_{x}=w_{x}^{*})\). In this case, if all the values of \(x\) have been checked, we have \(_{k}=_{k-1}\{x\}\); otherwise, \(_{k}=_{k-1}\); and \(_{k}=_{k-1}_{x_{} w _{x}}\) if receiving Yes from the user, \(_{k}=_{k-1}_{x_{}=w_{x}}\), otherwise.

We reveal the connection between querying attributes (i.e., querying attribute IDs) and querying attribute values in the following proposition.

**Proposition 1**.: _For any attribute \(x_{k-1}\), \(_{}((x))_{}((x) =w_{x})\) holds for all the possible \(w_{x}_{x}\)._

This proposition shows that if users could give a clear preference for the queried attribute and their preferred attribute value is one of the candidate attribute values, then querying attributes would be an equivalent or a better choice than querying attribute values. In other words, querying attributes and querying attribute values can not operate on the same attributes (otherwise, \(_{}()\) would always choose to query attributes). Therefore, we can combine querying items and querying attribute valuesby setting the action space to \(=_{x}_{k-1}_{k-1}\). Then, we can re-formulate Eq. (3) as:

\[a_{}=*{arg\,max}_{a\{w_{x},v\}}_{w_{x} _{x}x_{k-1}}_{}( (x)=w_{x}),_{v_{k-1}}_{}( (v)).\] (10)

In the context of querying attribute values, we further reveal what kind of attribute value is an ideal one in the following theorem.

**Proposition 2**.: _In the context of querying attribute values, an ideal choice is always the one that can partition all the unchecked relevance scores into two equal parts (i.e., the ideal \(w_{x}_{x},x_{k-1}\) is the one that makes \(_{}(w_{x}=w_{x}^{*})=(\{_{}(v_{m})| v_{m}_{k-1}\})/2\) hold), if it is achievable. And the certainty gain in this case is \(_{}((x)=w_{x})=(\{_{} (v_{m})|v_{m}_{k-1}\})/2\)._

Then, we consider the bound of the expected number of turns. To get rid of the impact of \(_{}()\), we introduce a cold-start setting , where \(_{}()\) knows nothing about the user, and equally assigns relevance scores to all \(M\) items, resulting in \(_{}(v_{m})=1/M\) holds for any \(v_{m}\).

**Lemma 1**.: _In the context of querying attribute values, suppose that \(_{}(v_{m})=1/M\) holds for any \(v_{m}\), then the expected number of turns (denoted as \(\)) is bounded by \(_{2}^{M+1}(M+1)/2\)._

Here, the good case lies in that our conversational agent is capable of finding an attribute value to form an ideal partition at each turn, while the bad case appears when we can only check one item at each turn. We provide detailed proofs of Propositions 1 and 2, and Lemma 1 in Appendix A.1.

\(_{}()\) **for Querying Attributes in Large Discrete or Continuous Space.** All the above querying strategies are designed in the context that for each attribute, the range of its candidate values is a "small" discrete space, namely \(|_{x}||_{k-1}|\) where \(x_{k-1}\). When it comes to cases where \(_{x}\) is a large discrete space or a continuous space, then either querying attribute \(x\) or any attribute value \(w_{x}_{x}\) would not be a good choice. For example, let \(x\) be Hotel Price, then when querying \(x\), the user would not respond with an accurate value, and querying \(x\)=one possible value could be ineffective. To address this issue, we propose to generate a new attribute value \(w_{x}\) and query whether the user's preference is not smaller than it or not. Formally, we have:

\[_{}((x) w_{x})=_{}(w_{x} w _{x}^{*})(w_{x} w_{x}^{*})+_{}(w_{x}<w_{x} ^{*})(w_{x}<w_{x}^{*}),\] (11)

where \(x_{k-1}\) and \(w_{x}\) can be either in or out of \(_{x}\). Compared to querying attribute values (i.e., Eq. (9)), the new action space is \(=_{k-1}\). Notice that Proposition 2 is also suitable for this case (see detailed description in Appendix A.1), where the best partition is to divide the estimated relevance scores into two equal parts. Therefore, we produce \(w_{x}\) by averaging all the candidate attribute values weighted by the corresponding relevance scores. Formally, for each \(x_{k-1}\), we compute \(w_{x}\) as:

\[w_{x}=(\{_{}(v_{m}) w_{v_{m}}|v_{m} _{k-1}\}),\] (12)

where \(w_{v_{m}}\) is the value of attribute \(x\) in item \(v_{m}\), e.g., in Figure 1(a), let \(a\) be Hotel Level, and \(v_{m}\) be Hotel A, then \(w_{v_{m}}=3\).

In this case, \(_{k}=_{k-1}\), and \(_{k}=_{k-1}_{x_{}<w_{x}}\) if receiving Yes from the user when querying whether user preference is not smaller than \(w_{x}\), \(_{k}=_{k-1}_{x_{} w_{ x}}\) otherwise. \(_{x_{}<w_{x}}\) is the set of all the items whose value of \(x\) is smaller than \(w_{x}\) and \(_{x_{} w_{x}}\) is the set of the rest items.

### Plugging the Conversational Agent into Recommender Systems

**Overall Algorithm.** We begin by summarizing CORE for querying items and attributes or querying items and attribute values in Algorithm 1. From the algorithm, we can clearly see that our \(_{}()\) puts no constraints on \(_{}()\) and only requires the estimated relevance scores from \(_{}()\), therefore, CORE can be seamlessly integrated into _any_ recommendation platform. We note that _in a conversational agent, querying attributes and querying attribute values can be compatible, but can not simultaneously operate on the same attribute, due to Proposition 1_. See Appendix A.2.3 for a detailed discussion.

**Making \(_{}()\) Consider Dependence among Attributes.** We notice that the above formulations of either querying attributes or querying attribute values, does not consider the dependence among attributes (e.g., as Figure 1(a) shows, attribute Hotel Level can largely determine attribute Shower Service). To address this issue, we take \(_{}()\) in Eq. (6) as an example (see detailed descriptions of the other \(_{}()\)s in Appendix A.2.2), and re-formulate it as:

\[_{}^{}((a))=_{a^{} _{k-1}}_{}((a^{})) ((a^{})|(a)),\] (13)

where \(a_{k-1}\), and \(((a^{})|(a))\) measures the probability of the user preference on \(a\) determining the user preference on \(a^{}\). Compared to \(_{}((a))\), \(_{}^{}((a))\) further considers the impact of querying attribute \(a\) on other attributes. To estimate \(((a^{})|(a))\), we develop two solutions. We notice that many widely adopted recommendation approaches are developed on factorization machine (FM) , e.g., DeepFM . Therefore, when applying these FM-based recommendation approaches, one approach is to directly adopt their learned weight for each pair of attributes \((a,a^{})\) as the estimation of \(((a^{})|(a))\). When applying CORE to any other recommendation method (e.g., DIN ), we develop a statistics-based approach that does estimations by computing this conditional probability \(_{}^{}((a))\) based on the given candidate items. We leave the detailed computations of \(_{}^{}((a))\) in both ways in Appendix A.2.2.

**Empowering \(_{}()\) to Communicate with Humans.** When applying CORE into real-world scenarios, users may provide a Not Care attitude regarding the queried attributes or queried attribute values. In these cases, we generate \(_{k}\) and \(_{k}\) by \(_{k}=_{k-1}\) and \(_{k}=_{k-1}\{a\}\), because querying \(a\) is non-informative. To capture the user's different attitudes on queried items and attributes or attribute values, we can incorporate a pre-trained large language model (LLM) (e.g., gpt-3.5-turbo) in \(_{}()\). As our online-checking part does not require training, simply plugging an LLM would not cause the non-differentiable issue. In light of this, we exemplify some task-specific prompts to enable the conversational agent to (i) produce questions by prompting queried items and attributes, and (ii) extract the key messages from the user's answers. As shown in Figure 2, a conversational agent consisting of an LLM-chat-bot and our online decision tree algorithm would communicate like humans. We also provide some case studies of the conversational agent in the context of a question generator and an answer extractor. See Appendix A.4.2 for a detailed description.

Figure 2: We illustrate the empowerment of our conversational agent through the utilization of a pre-trained chat-bot. In this context, the red box signifies the chat-bot-empowered conversational agent. To accomplish this, we input the output queries produced by the original conversational agent, such as Breakfast Service into the question generator, as depicted in (a). In (b), a user is expected to input the generated question in a free-text format and provide the corresponding answer in a free-text format. Subsequently, in (c), the answer extractor extracts key information from the user’s response and provides it to the original conversational agent.

Experiments

### Experimental Configurations

We summarize different experimental settings as follows. (i) We design two different quering strategies regarding attributes (shown in line 5 in Algorithm 1). One is querying attributes (i.e., attribute IDs); and the other is querying attribute values. (ii) We introduce two different recommender system settings. One is the hot-start setting (shown in line 1 in Algorithm 1) that initializes the estimated relevance scores of items by a given pre-trained recommender system; and the other is the cold-start setting where those estimated relevance scores are uniformly generated (corresponding to the case where the recommender system knows nothing about the given user). Because the conversational agent \(_{}()\) operates in a dynamic process, we develop a new simulator to simulate the Human-AI recommendation interactions, which consists of a conversational agent and a user agent. Specifically, for each user, we use her browsing log as session data, and treat all the items receiving positive feedback (e.g., chick) as target items. Then, for each \(k\)-th turn, when the conversational agent queries an attribute \(x_{k-1}\), the user agent returns a specific attribute value if all the target items hold the same value for \(x\); otherwise, the user agent returns \(\). When the conversational agent queries an attribute value \(w_{x}_{x}\), the user agent returns \(\) if at least one target item holds \(w_{x}\) as the value of attribute \(x\); otherwise, returns \(\).

For each experimental setting, we first set \(K_{}\), and then evaluate the performance in terms of the average turns needed to end the sessions, denoted as \(K_{}\) (where for each session, if \(_{}()\) successfully queries the target item within \(K_{}\) turns, then return the success turn; otherwise, we enforce \(_{}()\) to query an item at \((K_{}+1)\)-th turn, if succeeds, return \(K_{}+1\), otherwise return \(K_{}+3\)); and the average success rate, denoted as \(}K_{}\) (where for each session, if \(_{}()\) does not successfully query the target item within \(K_{}\) turns, then we enforce \(_{}()\) to query an item after \(K_{}\) turns, if succeeds, return \(1\), otherwise return \(0\)).

To verify \(\) can be applied to a variety of recommendation platforms, we conduct evaluations on three tubular datasets: Amazon [8; 33], LastFM  and Yelp , three sequential datasets: Taobao , Tmall  and Alipay , two graph-structured datasets: Douban Movie [35; 53] and Douban Book [35; 53]. The recommendation approaches used in this paper, i.e., \(_{}()\)s, include FM , DEEP FM , PNN , DIN , GRU , LSTM , MMOE , GCN  and GAT . We also use COLD START to denote the cold-start recommendation setting. The conversational methods used in this paper, i.e., \(_{}()\)s, include (i) Abs Greedy (AG) always queries an item with the highest relevance score at each turn; (ii) Max Entropy (ME) always queries the attribute with the maximum entropy in the context of querying attributes, or queries the attribute value of the chosen attribute, with the highest frequency in the context of querying attribute values; (iii) CRM , (iv) EAR , (v) CRIF , (vi) UNICORN . Here, AG can be regarded as a strategy of solely applying \(_{}()\). Both CRM and EAR are reinforcement learning based approaches, originally proposed on the basis of FM recommender system. Thus, we also evaluate their performance with hot-start FM-based recommendation methods, because when applying them to a cold-start recommendation platform, their strategies would reduce to a random strategy. Consider that ME is a \(_{}()\), independent of \(_{}()\) (namely, the performance of hot-start and cold-start recommendation settings are the same); and therefore, we only report their results in the cold-start recommendation setting. We further introduce a variant of \(\), denoted as \(_{}^{+}\) where we compute and use \(_{}^{0}()\)s instead of \(_{}()\)s in line 5 in Algorithm 1.

We provide detailed descriptions of datasets and data pre-processing, simulation design, baselines, and implementations in Appendix A3.1, A3.2, A3.3, and A3.4.

   }()\)} & }()\)} &  \\   & & T@3 & S@3 & T@5 & S@5 \\   & ME & 3.04 & 0.98 & 5.00 & 1.00 \\   & \(\) & **2.88** & **1.00** & **2.87** & **1.00** \\   & \(_{}^{+}\) & 2.84 & 1.00 & 2.86 & 1.00 \\   & AG & 2.76 & 0.74 & 2.97 & 0.83 \\   & CRM & 3.07 & 0.98 & 3.37 & 1.00 \\    & EAR & 2.98 & 0.99 & 3.13 & 1.00 \\    & CRIF & 2.84 & 1.00 & 2.64 & 1.00 \\    & UNICORN & 2.65 & 1.00 & 2.45 & 1.00 \\    & \(\) & **2.17** & **1.00** & **2.16** & **1.00** \\    & \(_{}^{+}\) & 2.14 & 1.00 & 2.14 & 1.00 \\   

Table 1: Results comparison in the context of querying attributes. See Table A1 for the full version.

[MISSING_PAGE_EMPTY:9]

framework unifies querying attribute values and items. In other words, AG is a special case of CORE, where only querying items is allowed.

**Considering Dependence among attributes is helpful.** Comparisons between CORE and CORE\({}_{}^{+}\) reveal that considering the dependence among attributes could improve the performance of CORE in most cases.

We further investigate the impact of \(K_{}\) by assigning \(K_{}=1,3,5,7,9\) and reporting the results of CORE and AG on Amazon dataset in the context of the cold-start and hot-start recommendation settings in Figure 3. The results further verify the superiority of CORE, especially with a cold-start \(_{}()\).

**Our conversational agent is more stable when the training data is limited.** For further assessment of CORE's stability in comparison to the baseline methods, we conducted an evaluation by randomly selecting a subset of the training set comprising only 50% of the samples. The results of this evaluation are presented in Table 5. The table clearly demonstrates that CORE exhibits a higher level of stability in its performance when compared to existing reinforcement learning-based frameworks.

We also provide a case study of incorporating an LLM into CORE to handle free-text inputs and output human language Appendix A.4.2, where we provide detailed prompts. We further offer a visualization of an online decision tree in A.4.3.

## 5 Conclusions and Future Work

In this paper, we propose CORE that can incorporate a conversational agent into any recommendation platform in a plug-and-play fashion. Empirical results verify that CORE outperforms existing reinforcement learning-based and statistics-based approaches in both the setting of querying items and attributes, and the setting of querying items and attribute values. In the future, it would be interesting to evaluate CORE in some online real-world recommendation platforms.

   }()\)} & }()\)} &  &  \\   & & T@3 & S@3 & T@5 & S@5 & T@3 & S@3 & T@5 & S@5 \\   & AG & 6.52 & 0.11 & 7.94 & 0.21 & 6.36 & 0.15 & 7.68 & 0.26 \\  & ME & 6.60 & 0.10 & 8.16 & 0.21 & 6.40 & 0.15 & 8.04 & 0.24 \\   & CORE & **5.48** & **0.38** & **4.84** & **0.94** & **5.96** & **0.26** & **5.08** & **0.92** \\   & AG & 3.75 & 0.63 & 3.65 & 0.87 & 3.56 & 0.64 & 3.41 & 0.87 \\   & CORE & **2.89** & **0.91** & **2.97** & **1.00** & **2.80** & **0.92** & **2.91** & **1.00** \\   & AG & 3.21 & 0.69 & 3.33 & 0.83 & 3.20 & 0.71 & 3.18 & 0.89 \\   & CORE & **2.76** & **0.92** & **2.81** & **1.00** & **2.85** & **0.91** & **2.85** & **1.00** \\   

Table 4: Results comparison of querying attribute values on graph datasets. See Table A4 for the full version.

Figure 3: Comparisons of CORE and AG with different \(K_{}\) in both cold-start and hot-start settings.

**Acknowledgement.** The Shanghai Jiao Tong University team is supported by National Key R&D Program of China (2022ZD0114804), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and National Natural Science Foundation of China (62076161, 62177033). Jiarui Jin would like to thank Wu Wen Jun Honorary Doctoral Scholarship from AI Institute, Shanghai Jiao Tong University.