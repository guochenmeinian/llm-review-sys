# Appendix for _SpatialPIN_

SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors

 Chenyang Ma Kai Lu Ta-Ying Cheng Niki Trigoni Andrew Markham

University of Oxford

chenyang.ma@cs.ox.ac.uk

###### Abstract

Current state-of-the-art spatial reasoning-enhanced VLMs are trained to excel at spatial visual question answering (VQA). However, we believe that higher-level 3D-aware tasks, such as articulating dynamic scene changes and motion planning, require a fundamental and explicit 3D understanding beyond current spatial VQA datasets. In this work, we present **SpatialPIN**, a framework designed to enhance the **spatial** reasoning capabilities of VLMs through **p**rompting and **n**t**eracting with priors from multiple 3D foundation models in a zero-shot, training-free manner. Extensive experiments demonstrate that our spatial reasoning-imbued VLM performs well on various forms of spatial VQA and can extend to help in various downstream robotics tasks such as pick and stack and trajectory planning.

## 1 Introduction

Equipping vision-language models (VLMs) the capacities of spatial reasoning unlocks exciting applications, such as general-purpose reward annotation , robotic data generation , and grounding 3D object affordances . However, the spatial reasoning capabilities of VLMs on fine-grained spatial understanding tasks are somewhat limited. Current state-of-the-art (SOTA) spatial reasoning-enhanced VLM  is mostly tested on spatial visual question answering (VQA), such as determining objects' relative positions and orientations; experiments on higher-level tasks, such as scene comparisons and trajectory planning, which require more nuanced comprehension, are underexplored.

Many works enhance the spatial reasoning capabilities of VLMs by training/fine-tuning them on standard spatial VQA datasets . As a result, VLMs primarily learn surface-level associations between image-text-data triplets. Given the scarcity and difficulty of obtaining spatially rich embodied data or high-quality human annotations for 3D-aware queries, we hypothesize that these VLMs may not generalize to questions outside their dataset distribution or adapt to more challenging tasks that require an advanced level of spatial understanding.

Recent studies  in image space understanding show that VLMs, equipped with internet-scale language knowledge, and multimodal foundation models capture complementary knowledge that can be combined to conduct new tasks spanning both modalities without additional training. Given the recent advancements in 3D foundation models , this work explores whether there exists an alternative approach to enhance VLMs with higher-level spatial-awareness by incorporating 3D priors from these models.

To this end, we propose **SpatialPIN**, a framework that utilizes progressive prompting and interactions between VLMs and 2D/3D foundation models as "free lunch" to enhance spatial reasoning capabilities in a zero-shot, training-free manner. By using these foundation models to decompose, comprehend,and reconstruct an explicit 3D representation, SpatialPIN grasps the core understanding of the 3D space presented by the 2D image. This allows generalizations to various 3D-aware tasks, from VQAs to 3D trajectory planning.

We provide an extensive empirical study combining multiple off-the-shelf and handcrafted datasets, ranging from fundamental spatial questions regarding relative positions and orientations to providing fine-grained 3D information on objects' locations, sizes, inclinations, and dynamic changes, and plan for robotics tasks with full 3D trajectories. Results show that this straightforward approach significantly outperforms SOTA VLMs trained from extensive spatial VQAs (see SpatialPIN examples in Figure 1), consolidating our belief that a truly 3D-aware VLM can actually be imbued by simply injecting explicit, fundamental knowledge of the 3D scene. With the entire framework being fully modularized, each component can be easily replaced with the latest improvements within its specific domain.

In summary, our main contributions are threefold:

* We investigate the problem of equipping VLMs with 3D reasoning capabilities without fine-tuning on large spatial VQA datasets.
* We propose SpatialPIN, a modular plug-and-play framework that progressively enhances VLM's 3D reasoning capabilities by prompting and interacting with 3D foundational models.
* We show that SpatialPIN unlocks 3D-aware applications including spatial VQA and both classic and novel robotics tasks, supported by extensive experiments.

## 2 Related Work

**VLM Grounding** With the recent birth of powerful LLMs and VLMs [8; 40; 2], the task of VLM grounding, or combining generative language models with real-world data to adapt to specific cases, has gained significant popularity. Several recent works focused on fine-tuning these LLMs for a wide range of downstream applications, such as interactive decision making , multi-task agents , or even tasks in interactive environments [67; 11]. A close work to ours is Socratic Model , a framework of combining multiple foundation models to unleash LLMs in downstream tasks. However, this work still focuses on tasks in 2D pixel space understanding of images. There remains many challenges in the 3D world to combine information for full scene understanding, which we hope to tackle in our paper.

**VLM Spatial Understanding** Many VLMs encompass the ability of image-space reasoning and understanding [13; 33; 40]. There are even efforts in incorporating these understandings into image space manipulations and editting [7; 65]. However, the current ability of VLMs to fully understand a 3D scene and the potential interactions within this scene is still rather limited. Several works build from this foundation and establish datasets to help with spatial reasoning/understanding [30; 39; 45]. Recently, SpatialVLM  proposed fine-tuning a VLM on 3D-VQA datasets to enhance the precision of VLMs on 3D understanding tasks. Nevertheless, using a 3D-VQA dataset only provides a partial picture to the complete 3D understanding of an image, and could lead to suboptimal performances under out-of-distribution tasks. In this work, we hope to introduce holistic 3D information from multiple 3D foundation models via prompting and interactions as a way to enhance VLMs with a comprehensive 3D understanding given RGB inputs.

## 3 Method

Given an RGB image \(I^{H W 3}\) of a scene with \(K\) unknown objects and a spatial task \(Q\), our goal is to inspire VLMs with spatial reasoning capabilities and solve \(Q\) with fine-grained 3D understanding.

Figure 1: We present **SpatialPIN**, a framework to enhance the **spatial** reasoning capabilities of VLMs through **p**rompting and **i**n**tracting with 3D priors in a zero-shot, training-free manner.

To prevent the models from overfitting to the standard problems from spatial VQA datasets , we hope to derive a method that utilizes fundamental 3D foundation models to provide explicit scene understandings, then leverage the generalization capabilities of VLMs to tackle unforeseen tasks--all within a zero-shot, training-free manner.

Our modular pipeline, SpatialPIN, enhances VLMs' spatial understanding of an image through progressive interactions with the scene decomposition, comprehension, and reconstruction processes with prompting. For image scene understanding (Sec. 3.1), we use VLM to describe objects by appearance and 2D location, complemented by language-guided segmentation and repainting models to obtain occlusion-free object masks. Elevating 2D understanding to coarse 3D (Sec. 3.2), we use metric depth estimation and perspective fields to estimate the 3D scene size and conduct perspective canonicalization with VLM. For fine-grained 3D understanding (Sec. 3.3), we partially reconstruct the 3D scene, with the full 3D representation of foreground objects and the background as a plane. With the reconstructed 3D scene, we summarize spatial information and prompt it to the VLM for various downstream tasks.

### 2D Image Scene Understanding

Prompting: Objects Understanding by ConstrainingWe start with querying VLM to identify and understand objects given \(I\). We explicitly ask VLM to describe the objects by precise color, texture, and 2D spatial locations. This step is vital for two reasons: 1) enhance VLM's understanding of the objects, 2) differentiate between items of similar or identical categories and appearances.

As a concrete example, given the left image of Fig. 2, VLM outputs: "

``` object0: laptopofcolorrosegold,texturemetallicatlocationleft-center. object1:cameraofcolorblack,texturesmoothatlocationcenter-right...." ```

### 2D Representations Refinement

The concise descriptions of identified objects are used as input text prompts for a language-guided segmentation model, enabling the acquisition of \(K\) segmentation masks \(\{M_{k}^{occ}\}_{k=1}^{K}\), with each mask corresponding to a unique object.

However, an object \(i[1,K]\) may be occluded by other object(s), leading to an incomplete mask \(M_{i}^{occ}\), which may be burdensome when we elevate the image to a 3D representation in the later stage. To resolve this, we create an inpainting mask, \(M_{i}^{inp}\), for each object, in which all objects except the one itself are removed and replaced with white pixels. The inpainted masks are again fed to the language-guided segmentation model along with input text prompts such that occlusion-free object masks, \(\{M_{k}^{of}\}_{k=1}^{K}\), are obtained. This two-step segmentation process for object \(i\) is formulated as:

\[M_{i}^{occ}=(I,_{i}), M_{i}^{of}=((M _{i}^{inp}),_{i}),\] (1)

where seg denotes the language-guided segmentation and \(_{i}\) denotes the description of object \(i\). In practice, to cleanly remove objects without residual fragments for inpainting, we apply dilation to and expand the white areas. Inpainted background \(I_{bg}\) is acquired by removing and replacing all objects with dilated white pixels.

Figure 2: **SpatialPIN**. Our plug-and-play framework is fully modularized and designed for zero-shot deployment. Each module can be easily replaced with the latest updates. Exact prompts for VLMs are in Appendix.

### Coarse 3D Scene Understanding

Scene Size EstimationUsing the estimated metric depth  and estimated camera intrinsic matrix by finding field of view (FOV) through perspective fields , we backproject to determine the dimensions of the 3D spatial scene.

Prompting: Perspective Canonicalization3D information without any knowledge regarding the camera perspectives lead to ambiguities . Consider a question "What is the orientation of the bowl relative to the laptop?" with the input scene in Fig. 2, but taken from a top-down perspective. VLMs may output "downward to the left", but the correct answer should be "front-left" because humans perceive orientation from a horizontal angle. To address this, we provide the VLM with \(I\), estimated scene size, and maximum and minimum dimensions, allowing it to reason about the camera shot angle (horizontal/top-down/bottom-up). Scene size information helps differentiate shot angles by providing clues about spatial layout and object proportions. For instance, if the depth variation is small, the VLM can infer a top-down or bottom-up angle along with visual cues.

As a concrete example, given the left image of Fig. 2, VLM outputs:

"Visual cues reasoning: Objects are viewed from the side, indicating the camera is positioned horizontally with a slight elevation.

Spatial data reasoning: The depth varies significantly from 57.50 cm to 115.00 cm, indicating the camera captures the scene across different distances, supporting a horizontal perspective.

Conclusion: horizontal."

### Fine-Grained 3D Scene Understanding

We partially reconstruct the 3D scene with full representation of foreground objects while simplifying the inpainted background as a plane, as shown in Fig. 3(a). We summarize spatial information from the reconstructed scene and prompt it to the VLM. Please see our Appendix for implementation details about reconstruction.

Scene InitializationGiven the occlusion-free object masks, \(\{M_{k}^{of}\}_{k=1}^{K}\), we use single-view 3D reconstruction model  to acquire object 3D models, \(\{O_{k}\}_{k=1}^{K}\), with canonical poses determined during reconstruction. Pinhole camera is set at the origin, looking at positive depth-axis. With the estimated background plane size (Sec. 3.2), we move the background plane, \(O_{bg}\) (visually identical to \(I_{bg}\)), along the depth-axis to fit precisely within the camera.

Scene ReconstructionTo resolve the imprecision of backprojection, our goal is to position object 3D models into the reconstructed 3D scene without visual discrepancies and ensure accurate depth. Instead of using naive backprojection, for an object \(i[1,K]\), we perform raycasting from object 3D center \(t_{i}^{c}\) on the camera plane to object 3D center \(t_{i}^{bg}\) on the background plane with metric depth \(d_{i}\). The 3D coordinate \(t_{i}\) of object \(i\) is:

\[d_{i}=|I_{dep}((M_{i}^{of})|, t_{i}=t_{c}+}{|t_{i}^{bg}-t_{i}^{c}|}(t_{i}^{bg}-t_{c}).\] (2)

The rotation \(R_{i}\) of the 6D pose of object \(i\), \(P_{i}=[R_{i} t_{i}]\) is explained previously. After integrating all 3D object models into the 3D scene, we refine each object's scale to accurately reflect depth variations by rendering binary masks and evaluate the length of their contour lines relative to their occlusion-free masks, through the lens of the pinhole camera, \(t_{c}\).

We determine the principal axes (x-axis, y-axis, and z-axis) of each object using the minimal oriented bounding box (OBB), which is essential for unlock novel applications.

Prompting: Objects and Spatial Context UnderstandingThe reconstructed 3D scene from \(I\) with accurate object poses and scales is denoted as \(V_{0}\). As the final step of progressive prompting, we feed VLM the fine-grained 3D information derived from \(V_{0}\), grounding on the canonicalized perspective (Sec. 3.2). For example, with the input image in Fig. 2 and a horizontal camera shot angle, depth corresponds to the positive y-axis (similarly, in a top-down/bottom-up view, depth is

Figure 3: Our method of **partial 3D scene reconstruction (a).** The reconstructed scene (b) and the input image (c) show high alignment.

[MISSING_PAGE_FAIL:5]

Intra-Image Object Relations VQA (IaOR-VQA)As the basic form of spatial VQA, it involves spatial reasoning about object relative orientations and sizes. This is divided into qualitative (e.g., "is [A] in front of [B]", "is [A] smaller than [B]") and quantitative (e.g., "how far apart are [A] and [B]", "measure the width of [A]") questions.

We follow the evaluation method of SpatialVLM . Since SpatialVLM did not release their evaluation dataset, we reproduce one using RGBD images from NOCS  (object dataset), RT-1 , and BridgeData V2  (robotics manipulation datasets). We sample 13, 20, and 20 distinct scenes from each. We generate QA pairs using the SpatialVLM data generation pipeline , followed by manual refinement. We check correctness for qualitative questions and calculate distances for quantitative questions. We annotate 300 qualitative and 200 quantitative spatial VQA pairs (SpatialVLM has 331 and 215 for each).

Intra-Image Angular Discrepancies VQA (IaAD-VQA)We propose a new form of Spatial VQA that needs spatial reasoning about objects' inclinations. It includes qualitative (e.g., "is [A] tilted", "is [A] more tilted than [B]") and quantitative questions (e.g., "how many degrees is [A] tilted vertically", "measure the angle between [A] and [B]").

Since this form of Spatial VQA involves out-of-plane rotations, YCBInEOAT  (object tracking dataset) is a suitable choice. We sample 30 scenes from it and annotate 50 questions each for qualitative and quantitative spatial VQA pairs.

Inter-Image Spatial Dynamics VQA (IrSD-VQA)We further propose a more challenging form of Spatial VQA. Given two images with multiple objects, the objects in the second image may move, rotate, incline, or the image may have a change in camera angle. The VLM needs to reason about these changes. Example qualitative questions include "does [A] move, rotate, or incline", "does [A] incline along the y-axis" while quantitative questions include "how far does [A] move", "how many degrees does [A] rotate horizontally".

As it is difficult to find a dataset that meets these requirements, we craft our own. We capture 20 image pairs using an iPhone 12 Pro Max, with each image containing \(1-5\) objects, and annotate 50 questions each for qualitative and quantitative spatial VQA pairs.

ResultsThe results in Tables 1 and 2 on qualitative and quantitative IaOR-VQA demonstrate that providing various VLMs fine-grained 3D information enhances their spatial reasoning capacities by a large margin. Surprisingly, VLMs with math and geometry reasoning capacities (e.g., GPT-4V, GPT-4o) show substantial improvements with this information.

The results in Tables 3 and 4 demonstrate the effectiveness of our approach on both qualitative and quantitative IaOR-VQA and IrSD-VQA tasks. Notably, the performance on quantitative IaOR-VQA is suboptimal compared to quantitative IrSD-VQA, despite the latter being more challenging. We

    &  &  &  &  &  \\    & w/o ours & w ours & w/o ours & w ours & w/o ours & w ours & w/o ours & w ours & \\   &  &  &  &  &  &  &  &  &  \\   

Table 1: **Qualitative IaOR-VQA.** We exclude comparisons to PaLI , PaLM-E , and PaLM 2-E  as they are not open source, and include experiments with GPT-4o  in addition to GPT-4V , LLAVA-1.5 , and InstructBLIP . We use the HF version of SpatialVLM .

    &  &  &  &  &  \\    & w/o ours & w ours & w/o ours & w ours & w/o ours & w ours & w/o ours & w ours & \\  Output numbers \(\%\) & 0.8 & 98.5 & 31.5 & **99.5** & 23.5 & 97.0 & 28.5 & 98.5 & 91.0 \\ In range  \(\%\) & 0.0 & 73.5 & 14.0 & **74.5** & 16.5 & 43.0 & 8.0 & 31.5 & 33.5 \\ In range  \(\%\) & 0.0 & 69.5 & 8.5 & **70.5** & 6.0 & 29.0 & 2.5 & 22.0 & 20.5 \\ In range  \(\%\) & 0.0 & 54.5 & 3.0 & **55.0** & 2.0 & 14.5 & 0.0 & 11.5 & 7.5 \\   

Table 2: **Quantitative IaOR-VQA.** SpatialVLM measures the accuracy by the percentage of answers that fall within 0.5x to 2.0x of the ground truth value. We also evaluate within narrower ranges of 0.75x to 1.33x and 0.9% to 1.11x. “Output number” means VLMs produce number in the response instead of vague descriptions.

believe this is because, for quantitative IrSD-VQA, the VLM sometimes confuses the camera and world coordinate frames, comparing the object's principal axes with the world axes to reason about changes in angles.

Fig. 4 presents qualitative examples on all forms of spatial VLM.

### Robotics Pick and Stack

Pick and stack is a classic robotics task. Given a robot's egocentric observation of a scene with multiple objects and a task description, our pipeline uses traditional planning to solve the problem. This task demands advanced spatial reasoning, as the model must comprehend 3D locations, sizes, and physical properties of the objects (i.e., how much to grasp and how high to drop? Is the object deformable or articulated so the robotic grasper needs to grasp more firmly?). For instance, grasping and stacking a soft toy bear on a cube is significantly different from stacking a solid apple on a mug. The model reasons about grasping and stacking policies, directly outputting 3D trajectories for the robot's end effector using traditional path planning algorithm as external tool.

**Set-Up** We set up the pick-and-stack problem in the ManiSkill  simulator, applying real-world physics properties. Rigid and articulated objects are chosen from the YCB dataset  and are randomly allocated on the table within the robotic arm's reach, with observations from different perspectives. We create 50 scenes. Since robot observations from simulated scenes suffer from sim2real gap and consider that most real-world robots have depth sensors, we use ground truth camera matrix and depth.

We compare our method to the following baselines: 1) direct 3D information output from our framework without GPT-4o  reasoning about physics and object properties and 2) SpatialVLM with our RRT* trajectory generation module.

**Results** Table 5 shows the results, with a qualitative example demonstrated in Fig. 5. The results indicate that using precise 3D information from our framework significantly improves the success rate, and incorporating VLM reasoning further enhances performance.

### Discovering and Planning for Robotics Tasks from a Single Image

We present a novel task that requires advanced spatial reasoning capacities of VLMs. Given a single RGB image of any scene comprising unknown environments and objects, the VLM discovers potential tasks and plans their execution with full 3D trajectories, with the **motivation** that it can be used for robot learning in future research. To solve this complex task and visualize the execution using our framework, we introduce: 1) a task proposal approach using VLM, 2) a novel axes-constrained 3D planning approach that enables spatial reasoning-imbued VLM to plan the object motion based on the proposed tasks by specifying waypoints. Please see Appendix for the pipeline and details.

**Dataset** We create a diverse evaluation dataset by combining self-captured photos (38) using an iPhone 12 Pro Max and scenes (13) from NOCS . Our dataset covers diverse scenes (_e.g._, office,

    &  &  \\   & GPT-4o & GPT-4o + ours & SpatialVLM & GPT-4o & GPT-4o + ours & SpatialVLM \\  Output numbers \(\%\) & 38 & **100** & 66 & 30 & **100** & 78 \\ In range  \(\%\) & 8 & **64** & 12 & 10 & **68** & 26 \\ In range  \(\%\) & 2 & **42** & 6 & 4 & **54** & 12 \\ In range  \(\%\) & 0 & **30** & 2 & 2 & **38** & 4 \\   

Table 4: **Quantitative IaAD-VQA \(\&\) IrSD-VQA.**

    &  &  \\   & GPT-4o & GPT-4o + ours & SpatialVLM & GPT-4o & GPT-4o + ours & SpatialVLM \\  Accuracy \(\%\) & 68 & **84** & 62 & 64 & **82** & 54 \\   

Table 3: **Qualitative IaAD-VQA \(\&\) IrSD-VQA.** Since we test SpatialPIN on one VLM backbone for our proposed spatial VQA, for fair comparison, we should use SpatialVLM backbone (PaLM 2-E ). However, since it is not open source, we use GPT-4o as our backbone, as it shows the most improvement with our framework.

kitchen, bathroom), and features a rich diversity of object categories (116) and quantities (185), with each image containing \(1-7\) objects and \(1-3\) tasks proposed for each object (278 tasks/planned trajectories in total). The dataset's diversity is further enhanced by the variety of perspectives (_e.g._, frontal, top-down, side views). This deliberate choice of diverse angles, both in our own image capturing process and through the random extraction of frames from NOCS, aims to simulate a realistic and challenging array of scenes for evaluation. See Appendix for statistics and visuals.

Qualitative DemonstrationWe present a qualitative example in Fig. 5. Additional examples in Appendix shows our framework's capability to produce diverse and accurate task trajectories spanning various scenes and tasks.

Human Evaluation: User StudyWe rely on human preference evaluation as one of our quantitative metrics. We ask 25 users to rate 5 translation and 5 rotation task executions in terms of task description alignment. For these complex context-dependent manipulation tasks, we instead ask users to judge 10 executions relative to human action, and to encapsulate their perception of the action in our with a single sentence. These sentence description will be used to test human understanding of our planned trajectories (please see Appendix). Note that our user study size is similar to those representative works such as ControlNet  and Prompt-to-Prompt . Results in Table. 6.

Machine UnderstandingWe assess the interpretability of our generated task executions from a machine's perspective using SOTA video understanding model, Video-LLaVA-7B . We use two approaches: binary classification and descriptive generation. For classification, we feed the model with the task descriptions generated by VLM and ask question (is the video doing...?). In generation, we prompt Video-LLaVA-7B to articulate its interpretation of our task executions. To quantify the correspondence between the model's perception and the tasks, we use OpenCLIP cosine similarity score .

    & **Rating**\(\) \\  Rotation & 4.58 \\ Translation & 4.43 \\ Manipulation & 4.29 \\   

Table 6: **User study. Ratings (scale \(1-5\)) are averaged.**

Figure 5: **Qualitative examples of pick and stack (top) and task trajectory planning (bottom). SpatialPIN successfully outputs picking and stacking policies using spatial reasoning and plans 3D trajectories with geometric awareness to align with task descriptions.**

However, we find that even SOTA video understanding model shows limited performance. To assess false positive rate in classification, we deliberately misalign the sequence of generated task executions with their corresponding task descriptions, expecting a theoretical accuracy of 0%. Contrary to expectations, Video-LLaVA-7B reports a false positive rate of 36.3%. To adjust for this anomaly, we subtract this rate from the model's raw accuracy for correctly aligned video-task pairs. This method, while unconventional, provides a more fair and reasonable evaluation of machine video understanding, underscoring the current challenges faced by video understanding models in accurately interpreting complex video content. Results in Table. 7.

### Ablation Study

We evaluate the effectiveness of each module in our framework on IaOR-VQA by 1) seeking alternative designs of the overall pipeline and 2) removing each component in our ablations.

Overall DesignTo demonstrate our framework's generalization across a wide range of objects, We replace our 2D + 3D pipeline with: 1) SOTA mesh-free single image object pose and size estimation model, ShAPO , 2) SOTA mesh-based single image object pose and size estimation model, SAM-6D , and feeds it with the object 3D model reconstructed by One-2-3-45++ , and 3) the data generation backbone of SpatialVLM . Since models 1) and 2) do not provide language annotations for their outputs, we first summarize the numerical outputs using our approach in Sec. 3.3. Then, GPT-4V identifies QA pairs.

Removing 2D Understanding ModuleIn this case, the VLM no longer examines the objects through prompting, and only the object name is input into the language-guided segmentation model.

Removing 3D Understanding ModulesThis means there is no scene size estimation, and the VLM does not conduct perspective canonicalization. During 3D scene reconstruction, we assume the image plane width to be 1 meter.

To validate the fine-grained 3D scene understanding module, we replace object mask raycasting with backprojection using the object's 2D center and remove the object scale calibration.

To demonstrate the overall effectiveness of our 3D understanding modules, we simply backproject the input image with the estimated metric depth.

ResultsTable 8 demonstrates the effectiveness of each module in our framework. The results also highlight the limitations of using off-the-shelf SOTA mesh-free and mesh-based single-image object pose and size estimation methods as our backbone. These methods are not language-driven and may struggle to generalize to novel objects in diverse input scenes.

## 5 Discussion and Conclusion

We present **SpatialPIN**, a framework designed to enhance the **spatial** reasoning capabilities of VLMs through **p**rompting and **i**nteracting with 3D priors in a zero-shot, training-free manner. We see our work as a step towards equipping VLMs with more generalized spatial reasoning capacities, demonstrated through applications in various forms of spatial VQA and both traditional and novel robotics tasks.

LimitationsReaders may be curious about the inference speed of our framework. The bottleneck is the 3D object reconstruction process and the API call to closed-source VLMs (\( 20\) seconds per image). However, we want to highlight that this process runs only once per image, and the speed is expected to improve with future versions of 3D foundation models.

    & **Machine** \\  Raw Acc \(\) & 0.974 \\ Fal-Pos Rate \(\) & 0.363 \\ True Acc \(\) & 0.611 \\  OpenCLIP \(\) & 0.636 \\   

Table 7: Results for machine understanding (classification and generation) on 278 task executions.

    &  &  &  &  \\    & ShAPO & SAM-6D + 3D models & SpatialVLM & w/o objects & w/o coarse & w/o fine-grained & w/o both \\  Qualitative & 36.7 & 48.0 & 81.3 & 68.3 & 76.0 & 63.3 & 61.7 & **87.3** \\ Quantitative & 29.5 & 37.0 & 62.5 & 54.5 & 64.5 & 50.5 & 58.0 & **70.5** \\   

Table 8: **Ablation study.** For quantitative IaOR-VQA, the accuracy is measured by the answers that fall within 0.75x to 1.33x of the ground truth value.