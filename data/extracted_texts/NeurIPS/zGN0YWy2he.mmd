# Scene Graph Disentanglement and Composition for Generalizable Complex Image Generation

Yunnan Wang\({}^{1,2}\) Ziqiang Li\({}^{1,2}\) Wenyao Zhang\({}^{1,2}\)

**Zequn Zhang\({}^{2,3}\) Baao Xie\({}^{2}\) Xihui Liu\({}^{4}\) Wenjun Zeng\({}^{2}\) Xin Jin\({}^{2}\)\({}^{*}\)**

\({}^{1}\)Shanghai Jiao Tong University, Shanghai, China

\({}^{2}\) Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China

\({}^{3}\)University of Science and Technology of China, Hefei, China

\({}^{4}\)The University of Hong Kong, Hong Kong, China

{wangyunnan, ziqiangli}@sjtu.edu.cn jinxin@eitech.edu.cn

Xin Jin is the corresponding author.

###### Abstract

There has been exciting progress in generating images from natural language or layout conditions. However, these methods struggle to faithfully reproduce complex scenes due to the insufficient modeling of multiple objects and their relationships. To address this issue, we leverage the scene graph, a powerful structured representation, for complex image generation. Different from the previous works that directly use scene graphs for generation, we employ the generative capabilities of variational autoencoders and diffusion models in a generalizable manner, compositing diverse disentangled visual clues from scene graphs. Specifically, we first propose a Semantics-Layout Variational AutoEncoder (SL-VAE) to jointly derive _(layouts, semantics)_ from the input scene graph, which allows a more diverse and reasonable generation in a one-to-many mapping. We then develop a Compositional Masked Attention (CMA) integrated with a diffusion model, incorporating _(layouts, semantics)_ with fine-grained attributes as generation guidance. To further achieve graph manipulation while keeping the visual content consistent, we introduce a Multi-Layered Sampler (MLS) for an "isolated" image editing effect. Extensive experiments demonstrate that our method outperforms recent competitors based on text, layout, or scene graph, in terms of generation rationality and controllability. Code is available at https://github.com/wangyunnan/DisCo.

## 1 Introduction

Text-to-image (T2I) generation with diffusion models (DMs)  has yielded remarkable advancements  in recent years, benefiting from the developments of vision-language foundation models . However, textual conditions with linear structure struggle to delineate the intricacies of complex scenes precisely. For example, as shown in the failure cases of DALL-E 3  in Figure 1 (a), given the intricate text prompt "_A sheep by another sheep... a boat on the grass._", the T2I model may have difficulty accurately generating object relationships or quantities. Consequently, some studies  strive to improve spatial relationship (e.g., "_by_" and "_on_") control by incorporating additional layout conditions. Nevertheless, as illustrated in the failure cases of LayoutDiffusion  in Figure 1 (b), layout-to-image (L2I) methods inevitably encounter challenges in representing certain non-spatial interactions, such as depicting "_playing_" within spatial topology.

To efficiently depict complex scenes for guiding generative models, recent methods  utilize structured scene graphs as conditions instead of text or layout prompts. Scene graphs represent scenes with a structured graph format, where objects within the scene are denoted as nodes and the relationships between objects are represented as edges. Scene-Graph-to-Image (SG2I) generation is a challenging task due to the frequent ambiguous alignment between graph edges and relationships/interactions among visual objects. To address this issue, layout-based SG2I methods [15; 17; 18; 19] explicitly predict the spatial arrangements of objects in scenes by additional layout predictors, followed by L2I synthesis according to the layout (as demonstrated in Figure 2 (a)). These methods commonly employ one-to-one mapping, i.e., a single scene graph only corresponds to one layout, which severely limits the generation diversity. Besides, they also inherit the limitation of the L2I approach in modeling non-spatial interactions, whereby each object is typically generated independently. In contrast, as shown in Figure 2 (b), semantic-based SG2I methods implicitly encode graph edges into node embeddings by graph convolutional networks (GCNs), which effectively aligns object semantics with non-spatial interactions. Nonetheless, these methods are weak in logically determining the spatial positions of independent nodes, which might cause the absence of independent nodes (e.g., the "_lamp_" and "_stone_" shown in Figure 1 (c)) in the generated image.

In this paper, we propose **DisCo**, a **Co**ompositional image generation framework that integrates the **Dis**entangled layout and semantics derived from scene graph representations (as depicted in Figure 2 (c)). To boost the representational capacity of scene graphs for complex scenes, we augment the node and edge representations with CLIP  text embeddings, and incorporate extra spatial information (i.e., bounding box embeddings) for nodes during training. Once the textual scene graph is constructed, we propose a _Semantics-Layout Variational AutoEncoder_ (SL-VAE) based on triplet-GCN  to jointly model the spatial relationships and non-spatial interactions in the scene. SL-VAE allows the one-to-many disentanglement for _spatial layout_ and _interactive semantics_ that match the

Figure 1: Failure cases generated by **(a)** text-to-image (T2I) (DALL-E 3 ), **(b)** layout-to-image (L2I) (LayoutDiffusion ), and **(c)** semantics-based scene-graph-to-image (SG2I) (R3CD ) methods. **(d)** Generalizable object _Attribute Control_ (AC) under consistency achieved by our DisCo.

Figure 2: **Comparison between the previous SG2I architectures and ours. (a) Layout-based SG2I model  generate a spatial arrangement with an object layout; (b) Semantic-based SG2I models [14; 16] build interactive semantic embedding between objects; (c) Our method leverages scene graph representation by jointly deriving the disentangled layout and semantics with the proposed SL-VAE.**

input scene graph. This is achieved by sampling from a Gaussian distribution, offering object-level _(layouts, semantics)_ conditions for the diffusion process . Given that the layout and semantics encapsulate global and local relational information, we further introduce the _Compositional Masked Attention_ (CMA) mechanism to inject object-level graph information with fine-grained attributes into the diffusion model, thereby preventing relational confusion and attribute leakage.Finally, we present a _Multi-Layered Sampler_ (MLS) technique that leverages the diverse conditions generated by SL-VAE, achieving generalizable generation for object-level graph manipulation (i.e., node addition and attribute control) in the SG2I task, as depicted by the color change of two "_sheep_" in Figure 1 (d).

In summary, our key contributions are as follows: **(i)** We apply the textual scene graph as a structured scene representation and introduce the _Semantics-Layout Variational AutoEncoder_ (SL-VAE) to disentangle diverse _spatial layouts_ and _interactive semantics_ from the scene graph; **(ii)** We present the _Compositional Masked Attention_ (CMA) to inject extracted object-level graph information with fine-grained attributes into the diffusion model, which avoids relational confusion and attribute leakage; **(iii)** We introduce the _Multi-Layered Sampler_ (MLS), a technique that leverages the diverse conditions produced by SL-VAE to implement object-level graph manipulation while keeping the visual content consistent; **(iv)** Our method outperforms current text/layout-based methods in relationship generation and achieves significantly superior generation performance compared to state-of-the-art SG2I models, thus showcasing the generalization of textual scene graphs in depicting complex scenes.

## 2 Preliminary

### Text-to-Image Diffusion Models

Diffusion models (DMs)  are generative models that learn the data distribution \(p()\) by gradually performing \(T\)-step noise reduction from the variables \(_{T}\) sampled from the Gaussian distribution \((0,1)\). Thus the training process of DMs can be regarded as the reverse process of a Markov chain with a fixed length \(T\). To generate high-resolution images with less computational resources, Latent Diffusion Models (LDMs)  encode the image \(\) into the latent space \(\) with the pre-trained Vector Quantized Variational AutoEncoder (VQ-VAE). Subsequently, the LDMs aim to predict the distribution \(p()\) rather than \(p()\). For the text-to-image LDMs, the text is encoded with the CLIP  text encoder \(E_{}\). Then the objective function of a text-guided LDM can be formulated as follows:

\[_{LDM}=_{,(0,1),t}[\| -_{}(_{t},E_{}(),t) \|_{2}^{2}],\] (1)

where \(E_{}()\) is the text embedding of the text condition \(\), \(t\) is the diffusion step, \(_{}\) is a model for estimating the noise \(\), and \(_{t}=_{t}_{t-1}+_{t}\) is the \(t\)-step noised latent code from the ground-truth \(_{0}\). During inference, the model \(_{}\) with various samplers [1; 21] gradually denoises the initial noise \(_{T}(0,1)\). Finally, the predicted latent code is decoded into the image space.

Figure 3: **Framework overview**. (I) We parameterize the node embeddings into the Gaussian distribution with the Graph Union Encoder, which jointly models the spatial relationships and non-spatial interactions in scene graphs; (II) The Semantic and Layout Decoders generate spatial layouts and interactive semantics sampled from Gaussian distribution, respectively; (III) A diffusion model with the proposed Compositional Masked Attention (CMA) incorporates object-level conditions to generate visual images following the scene graph description; (IV) Detailed structure of CMA Layer.

### Scene Graph Representation

The scene graph \(G=(O,E)\) presents a structured scene representation. Nodes \(O=\{o_{i}\}_{i=1}^{N_{o}}\) denotes \(N_{o}\) objects within the scene, while edges \(E=\{e_{ij}\}_{1 i,j N_{e},i j}\) denotes relationships between objects. All nodes and edges come with a semantic label, denoted as \(c_{i}^{o}^{o}\) and \(c_{ij}^{e}^{e}\), where \(^{o}\) and \(^{e}\) are category vocabularies of nodes and edges, respectively. In practice, the nodes \(O=\{o_{i}\}_{i=1}^{N_{o}}\) and the triples \(T=\{t_{ij}=(o_{i},e_{ij},o_{j})\}_{1 i,j N_{o},i j}\) representing connections from \(o_{i}\) to \(o_{j}\) serve as inputs for graph convolutional networks (GCNs). Moreover, nodes and edges are typically converted into learnable embeddings using embedding layers denoted as \(E^{o}_{emb}\) and \(E^{e}_{emb}\).

## 3 Methodology

As illustrated in Figure 3, we present a novel SG2I synthesis framework known as DisCo. The DisCo comprises three primary components: (1) Semantics-Layout Variational AutoEncoder (SL-VAE) that disentangle diverse spatial layouts and interactive semantics from the scene graph (Section 3.1); (2) Compositional Masked Attention (CMA) that injects object-level _(layouts, semantics)_ with fine-grained attributes into the diffusion model (Section 3.2); and (3) Multi-Layered Sampler (MLS) that implements generalizable generation for object-level graph manipulation (Section 3.3).

### Semantics-Layout Variational AutoEncoder

**Textual Scene Graph Construction**. For constructing the scene graph representation, we employ the visual-language model to fully leverage the inherent disentangled semantics of the language, while simultaneously facilitating the alignment between images and scene graphs. Specifically, we augment the node and edge embeddings of the scene graph with CLIP  text embeddings. During training, we also incorporate spatial information for node embeddings by including bounding box coordinates (i.e., top-left corner and box size denoted as \(b_{i}=(x_{i},y_{i},w_{i},h_{i})\)). Then the node embeddings \(\) and edge embeddings \(\) can be formulated as follows:

\[=\{E^{o}_{emb}(c_{i}^{o}) E_{}(o_{i}) E _{box}(b_{i})\}_{i=1}^{N_{o}},\ =\{E^{e}_{emb}(c_{ij}^{e}) E_{}(t_{ij})\}_{1  i,j N_{o},i j}\] (2)

where \(E_{}\) denotes the frozen pre-trained text encoder, \(E_{box}\) is the spatial encoder for bounding box coordinates using Multi-Layer Perceptions (MLPs), and \(\) denotes concatenate operation.

**Graph Union Encoding**. Although layout-based SG2I methods are superior in modeling spatial topology compared to the semantics-based method, they fall short in capturing object interactions (i.e., non-spatial relationships) within the scene. Accordingly, after obtaining the node and edge embeddings mentioned above, we apply a Conditional Variational Autoencoder (CVAE)  based on triplet-GCN  to jointly model the layout and semantics information. As shown in Figure 3.I, the \(L\)-layer Graph Union Encoder \(E_{u}\) takes node and edge embeddings as inputs:

\[(_{i}^{l+1},_{ij}^{l+1},_{j}^{l+1})=_{l}(_{i}^{l}, _{ij}^{l},_{j}^{l}),l\{0,,L-1\}\] (3)

where \(l\) denotes the layer index of Graph Union Encoder, and \(\) denotes intermediate features. Here we initialize \((_{i}^{0},_{ij}^{0},_{j}^{0})=(_{i},_{ij}, _{j})\). Please refer to the **Appendix** for more details about the triplet-GCN. Given that the last node embedding \(_{i}^{L}\) integrates both topology and interaction information, we conduct layout-semantic modeling by parameterizing it into Gaussian spaces \(Z(,)\). In this context, the means \(^{D_{z}}\) and variances \(^{D_{z}}\) are estimated individually by two supplementary MLPs, where \(D_{z}\) denotes the dimensional of latent space for node embedding. Hence, we jointly model the layout and semantics through the following minimization:

\[_{union}=(E_{u}(u|y,,)\ \|\ p(u|y)),\] (4)

where \(\) denotes the Kullback-Liebler divergence, \(y\) denotes condition and the prior \(p(u|y)\) is the standard Gaussian distribution \((u 0,1)\). Specifically, we condition the latent space of the graph structure using the edge embedding following Equation 2 alongside the updated node embedding \(\{E^{o}_{emb}(c_{i}^{o}) E_{}(o_{i}) u_{i}\}_{i=1}^ {N_{o}}\), where \(u_{i}\) is a random vector sampled from \(Z\). This architecture ensures that layout is solely necessary for training, with no need for hand-crafted layout in inference.

**Disentangled Semantics-Layout Decoding**. As illustrated in Figure 3.II, we disentangle the explicit _spatial layout_ and implicit _interactive semantics_ from the latent space using two separate triplet-GCN-based decoders, i.e., layout decoder \(D_{l}\) and semantic decoder \(D_{s}\). The proposed Semantics-LayoutVariational AutoEncode (SL-VAE) comprises these two decoders and the graph union encoder mentioned above, which derives the spatial topology and object interactions from the scene graph representation. During training, the layout decoder is optimized by the following objective function:

\[_{layout}=}_{i=1}^{N_{o}}|b_{i}-_{i}|_{1},\] (5)

where \(_{i}\) denotes the predicted coordinates. We only incorporate the ground truth layout \(=\{b_{i}\}_{i=1}^{N_{o}}\) during training, while generating \(N_{l}\) diverse _layouts_\(\{}_{n}=\{_{n,i}\}_{i=1}^{N_{o}}\}_{n=1}^{N_{l}}\) by sampling Gaussian noise at inference time. For simplicity, we omit the superscript in the following description. The semantic decoder \(D_{s}\) generates _semantics_ embeddings \(=\{s_{i}\}_{i=1}^{N_{o}}\) to facilitate subsequent diffusion processes, and its parameters are iteratively updated with the diffusion loss in the next Section 3.2.

### Diffusion with Compositional Masked Attention

**Object-level Fusion Tokenizer**. We integrate the _spatial layout_\(=\{b_{i}\}_{i=1}^{N_{o}}\) and _interactive semantics_\(=\{s_{i}\}_{i=1}^{N_{o}}\) at object level, as illustrated in Figure 3.III. The single-object embeddings \(=\{c_{i}\}_{i=1}^{N_{o}}=\{s_{i}(b_{i})\}_{i=1}^ {N_{o}}\) are acquired by directly applying semantic embeddings, while encoding box information using a Fourier mapping \(\). We define a learnable null embedding to pad the embedding length to \(N_{max}\), thereby accommodating varying numbers of objects:

\[c_{i}=s_{i}(b_{i}),&i N_{o}\\ c_{null},&\] (6)

where \(c_{null}\) denotes the learnable null embedding for padding. We optionally add attribute embedding \(=\{a_{i}\}_{i=1}^{N_{max}}\) to construct updated \(=\{c_{i} a_{i}\}_{i=1}^{N_{max}}\), where \(c_{i}\) and \(a_{i}\) are separately processed by two MLPs before concatenation. Note that \(a_{i}\) is obtained similarly to edge embedding in Equation 2. We also define a learnable null embedding \(a_{null}\) for cases where no attribute is specified.

**Compositional Masked Attention**. The cross-attention mechanism in diffusion bridges the visual and textual information, while self-attention captures self-related information within visual tokens . Therefore, we insert our proposed Compositional Masked Attention (CMA) between self-attention and cross-attention layers. This technique effectively injects graph information into the diffusion process at the object level, preventing semantic confusion and attribute leakage through the attention mask. Specifically, we denote the visual token output by the vanilla self-attention as \(^{N_{v} D_{v}}\), where \(N_{v}\) and \(D_{v}\) represent the number and dimensions of tokens, respectively. Then the CMA layer can be expressed as:

\[}=SA_{mask}(},) [:N_{v}],\] (7)

where \(}=\{_{i}\}_{i=1}^{N_{max}}\) denotes object embeddings whose dimensions are aligned with the visual token \(\) using MLPs. The matrix \(^{(N_{v}+N_{max})(N_{v}+N_{max})}\) denotes the attention mask that depends on layout \(\), which can be constructed as follows:

\[_{i,j}=1,i,j\\ -inf,\] (8)

where "\(i\), \(j\) fall into the same object" means that \(i\) and \(j\) index the visual tokens or object embeddings of the same object. Figure 4 illustrates the mechanism of CMA through a toy example. In contrast to vanilla self-attention, the proposed CMA prevents relational confusion and attribute leakage between different objects through the well-designed object-level masks mentioned above. As shown in Figure 3.IV, we forward the output of the CMA layer into the subsequent layers, serving as the updated visual token.

**Diffusion Loss**. Based on the object semantics output from the proposed SL-VAE, we optimize the evidence lower bound between sampling noise and prediction noise conditioned on object-level

Figure 4: **Toy example of (a) compositional masked attention, and (b) its corresponding attention mask. We use visual tokens and object embeddings of objects A and B for demonstration. A and B have 1 and 2 visual tokens, respectively, whose attribution is determined by bounding boxes.**

information (i.e., ground truth spatial layout and generated interactive semantics). Then the training loss of the diffusion model equipped with CMA can be summarized as follows:

\[_{LDM}=_{,(0,1),t}[\|-_{}(_{t},E_{}(O),, ,,t)\|_{2}^{2}].\] (9)

Finally, we employ an end-to-end joint training pipeline for the whole proposed DisCo framework. The total objective function is presented as follows:

\[_{total}=_{1}_{LDM}+_{2}_{union }+_{3}_{layout},\] (10)

where \(_{1}\), \(_{2}\), and \(_{3}\) are hyperparameters, which are typically set to \(1.0\), \(0.1\) and \(1.0\), respectively.

### Multi-Layered Sampler

Manipulations in the input scene graph, such as node addition and attribute adjustment, pose challenges for maintaining visual consistency in the generated images, ultimately compromising generalizability. To achieve an "isolated" image editing effect, we also provide the Multi-Layered Sampler (MLS) motivated by SceneDiffusion . The scheme defines each object as a layer, thus allowing independent object-level Gaussian sampling. In contrast to SceneDiffusion which scrambles the reference layouts randomly, we sample additional \(N_{l}\) (layouts, semantics) by the SL-VAE. Note that \(N_{l}\) fixed seeds exist for the same scene. Then we aggregate latent codes from various layers into \(_{n}\) and utilize layout-converted non-overlapping masks \(\{_{n}=\{m_{n,i}\}_{i=1}^{N_{o}}\}_{n=1}^{N_{l}}\) for locally conditioned diffusion. During the inference, the noise estimation for \(N_{l}\) scenes is calculated as follows:

\[}_{n}^{(t)}=_{i=1}^{N_{o}}m_{n,i}\ _{ }(_{n}^{(t)},E_{}(o_{i}),b_{n,i},s_{n,i},a_{i},t),\] (11)

Subsequently, the latent code for each object is computed as the weighted average of the \(N_{l}\) cropped denoised views. Please refer to the **Appendix** for more details about MLS.

## 4 Experiments

**Dataset**. We conduct scene-graph-to-image (SG2I) generation experiments on the Visual Genome (VG)  and COCO-Stuff (COCO)  datasets. The VG dataset comprises \(108,077\) image-scene graph pairs, accompanied by the bounding box coordinates and object attributes. Following previous work , we select objects and relationships that appear at least \(2,000\) and \(500\) times respectively in VG, resulting in \(178\) objects and \(45\) unique relationship types. Also, we ignore small objects and use images containing \(5\) to \(30\) objects along with a minimum of \(3\) relationships. Based on the above

    & &  &  \\   & **Method** & **Type** & **IS \(\)** & **FID \(\)** & **IS \(\)** & **FID \(\)** \\   & - & 30.7 & - & 27.3 & - \\   & SG2Im  & L & 8.2 & 99.1 & 7.9 & 90.5 \\  & PasteGAN  & L & 12.3 & 79.1 & 8.1 & 66.5 \\  & SOAP  & L & 14.5 & 81.0 & - & - \\  & WSGC  & L & 6.5 & 121.7 & 9.8 & 84.1 \\  & KCGM  & S & - & - & 11.6 & 27.4 \\   & LDM  & S & 22.2 & 63.8 & 16.5 & 45.7 \\  & SGDiff  & S & 17.8 & 36.2 & 16.4 & 26.0 \\   & SceneGenie  & L & 22.2 & 63.3 & 20.3 & 42.2 \\   & R3CD  & S & 19.5 & 32.9 & 18.9 & 23.4 \\   & DisCo (ours) & L+S & **23.1** & **30.8** & **22.3** & **21.9** \\   

Table 1: **Performance comparison** on COCO-Stuff and Visual Genome datasets using Inception Score (IS) and Fr√©chet Inception Distance (FID) metrics. We report the results of methods with two generator structures, namely GAN- and Diffusion-based. The architecture of these methods is based on the layout (L) or semantics (S), while our approach includes both. The best results are **bolded**.

filtering, we have \(62,565\) images available for training, each containing an average of \(10\) objects and \(5\) relationships. While the original COCO-Stuff dataset  lacks scene graph annotations, it consists of \(40,000\) images annotated with bounding box coordinates and captions, essential for synthesizing geometric scene graphs . All images in the COCO-Stuff dataset are labeled as \(80\) item categories and \(91\) stuff categories.

**Implementation Details**. We fine-tune the pre-trained Stable-Diffusion 1.51 with the modified Attention module on \(4\) NVIDIA A100 GPUs, each with 80GB of memory. We apply the CLIP text encoder (vit-large-patch14 ) to construct the textual scene graph. We train the model with a batch size of \(64\) using the AdamW optimizer  with an initial learning rate of \(1.0 10^{-4}\), which is adjusted linearly over \(50,000\) steps. During inference, we use the \(50\)-step PNDMScheduler  with a classifiers-free scale  of \(7.5\). The sample number \(N_{l}\) in the multi-layered sampler is set to \(5\).

**Evaluation Metrics**. Following previous works , we evaluate the performance of our method with the Inception Score (IS)  and the Frechet Inception Distance (FID) . The IS score is derived from a pre-trained Inception Net , assessing both the quality and diversity of synthesized images. The FID score quantifies the dissimilarity between the generated image and the real image distribution, which evaluates the fidelity of the generated images. To measure the effectiveness of compositional generation, we further evaluate our method on the T2I-CompBench . Besides, we apply CLIP for zero-shot attribute classification of the controlled object cropped by the bounding box, and subsequently evaluate the attribute control performance by the classification accuracy ACC\({}_{attr}\).

**Quantitative Comparisons**. To demonstrate the effectiveness of the proposed DisCo, we compare it with current state-of-the-art SG2I methods on the COCO-Stuff and Visual Genome datasets, which are summarized in Table 1. Our DisCo outperforms other methods in both IS and FID scores, revealing its superior performance in both fidelity and diversity of image generation. Compared with previous methods, the primary architectural advantage of DisCo is its innovative approach of simultaneously integrating disentangled layout and semantics extracted from scene graph representations. Moreover, the proposed SL-VAE achieves the diverse generation of layouts and semantics from a single scene graph through Gaussian distribution sampling. Therefore, our DisCo integrates the benefits of both layout-based and semantic-based methods, which is further ablated in detail in Table 4 of the ablation study. We proceed to assess the compositional generation on the T2I-CompBench , as shown in Table 2. The benchmark evaluates the competency of the text-to-image model in responding to compositional prompts. We report UniDet, CLIP, B-VQA, and 3-in-1 scores for measuring the generation of spatial/non-spatial relationships, attributes, and complex scenes, respectively. Following , we use the UniDet , CLIP , and BLIP  to evaluate these results. Our DisCo surpasses all compared T2I methods, confirming the efficacy of scene graphs in depicting complex scenes.

**User Study**. We conduct a user study by recruiting \(50\) participants from Amazon Mechanical Turk. We randomly select 8 prompts for each method, resulting in 80 generated images. We ask participants to score each generated image independently based on the image-prompt alignment. The worker can choose a score from \(\{1,2,3,4,5\}\) and we normalize the scores by dividing them by 5. We then compute the average score across all images and all workers. The results are presented in the Table 3. Our method is favored by most participants in terms of generation rationality and controllability.

  
**Method** & **UniDet** & **CLIP** & **B-VQA** & **3-in-1** \\  SD-v1.4  & 0.1246 & 0.3079 & 0.3765 & 0.3080 \\ SD-v2  & 0.1342 & 0.3127 & 0.5065 & 0.3386 \\ Composable  & 0.0800 & 0.2980 & 0.4063 & 0.2898 \\ Structured  & 0.1386 & 0.3111 & 0.4990 & 0.3355 \\ Attn-Exct  & 0.1455 & 0.3109 & 0.6400 & 0.3401 \\ GORS  & 0.1815 & 0.3193 & 0.6603 & 0.3328 \\  DisCo (ours) & **0.2376** & **0.3217** & **0.6959** & **0.4143** \\   

Table 2: **Relationship and attribute generation** compared with text-to-image methods on T2I-CompBench .

  
**Method** &  SD-XL \\  \\  &  DALL-E 3 \\  \\  &  Imagen 2 \\  \\  &  GLIGEN \\  \\  &  LD \\  \\  &  MIGC \\  \\  &  SG2Im \\  \\  &  SGDiff \\  \\  &  R3CD \\  \\  & 
 DisCo \\ (ours) \\  \\ 
**Score** & 0.6684 & 0.5944 & 0.5637 & 0.6549 & 0.6200 & 0.7055 & 0.3783 & 0.4717 & 0.6928 & **0.8533** \\   

Table 3: **User study**. The score quantifies the user evaluation (i.e., relationships, quantities, and generation quality) of the alignment between the given prompt and the generated image.

_(i) sheep by another sheep on the grass with the ocean under the sky; the ocean by a tree;(ii) bone on the grass_

**Qualitative Comparisons**. Figure 5 visualizes the results of the methods conditioned by text, layout, or scene graph, showcasing our advantages in generating rationality and controllability: **(i)**_Comparison with the text-to-image (T2I) methods_. In Figure 5 (a), we present the superiority of

  
**Method** & **G2I-ACC \(\)** & **I2G-ACC \(\)** \\  Layout (\(D_{l}\)) & 70.3 & 70.5 \\ Semantics (\(D_{s}\)) & 71.1 & 71.5 \\ SL-VAE (w/o \(D_{s}\)) & 72.9 & 72.8 \\ SL-VAE (\(D_{l}+D_{s}\)) & **73.9** & **74.3** \\   

Table 4: **Ablation study** for overall architecture. Table 5: **Ablation study** for attention mechanism. SL-VAE w/o \(D_{s}\) means independent use of \(\).

Figure 5: **Qualitative Comparisons with (a) text-to-image (T2I) (SableDiffusion-XL , DALL-E 3 , and Imagen 2 ), (b) layout-to-image (L2I) (GLIGEN , LayoutDiffusion , and MIGC ), and (c) scene-graph-to-image (SG2I) (SG2Im , SGDiff , and R3CD ) methods.**

the disentangled structured scene graph over linear text for representing complex scenes. Firstly, we resolve ambiguity in textual relationships and semantics by employing layout and semantic disentanglement within the scene graph. For example, our DisCo clarifies the relationship between "boat" and "grass" in the first line, as well as the semantics of "bus" and "building" in the following line. Additionally, the samples in the first and last lines also showcase our capacity to generate the specified quantities of objects precisely. **(ii)**_Comparison with the layout-to-image (L2I) methods_. We also demonstrate that our DisCo outperforms the diffusion methods relying on manually crafted scene layout representations, as illustrated in Figure 5 (b). While the L2I method struggles to model non-spatial interactions (such as "playing" in the first line and "chasing" in the second line), our DisCo addresses this challenge using disentangled object interactive semantics. Furthermore, by establishing semantics among objects derived from their relationships, we prevent independent generation instances that rely solely on the layout, exemplified by the "bus" and "tire" in the last line. **(iii)**_Comparison with the scene-graph-to-image (SG2I) methods_. The SG2I visualization results of different methods are showcased in Figure 5 (c). Our DisCo significantly improves the quality of SG2I generation, particularly for independent nodes. The proposed layout and semantics disentanglement technique effectively capture both the spatial and interactive information of independent nodes. Taking the "lamp" and "stone" in the first image and the "shadow" in the second image as illustrations, these entities are neglected by previous methods, whereas our DisCo not only retains their semantic relevance but also infers their appropriate spatial placement within the scene. We also demonstrate the generalizable generation under consistency for graph manipulation (i.e., node addition and attribute control) in SG2I tasks, as shown in Figure 6.

**Ablation Study**. Table 4 explores the overall architecture by evaluating the alignment of the generated image with the objects and relationships depicted in the input scene graph. Following SGDiff , we conduct this analysis by graph-to-image (G2I) and image-to-graph (I2G) retrieval experiments. Note that "w/o \(D_{s}\)" means processing each node embedding by MLP independently,instead of obtaining object interactive semantics through \(D_{s}\). We observe that the spatial layout and interactive semantics collaborate to boost both retrieval tasks. These results demonstrate the effectiveness of integrating explicit spatial relations with implicit interactive semantics. In Table 5, we study the impact of different attention mechanisms. We inject graph conditions using different mechanisms: (a) Vanilla attention mechanism in the T2I diffusion model without our CMA; (b) CMA without attention mask M; (c) CMA with a union MLP after concatenating object and attribute embeddings; and (d) CMA with two separate MLPs before concatenating object and attribute embeddings. We found that CMA, which fuses separate encoding of object and attribute embeddings, significantly enhances the overall generation performance. Table 6 presents the Multi-Layer Sampler (MLS) ablation results, confirming its enhancement over the baseline and LSD . In contrast to LSD, which randomly scrambles layouts, the proposed MLS naturally leverages a variety of coherent layouts and semantics produced by SL-VAE. Moreover, the increase in ACC\({}_{attr}\) scores also indicates that MLS facilitates controllability, especially in attribute control, while ensuring generation quality.

## 5 Related Works

**Diffusion Models**. Diffusion models (DMs) [2; 20; 31; 42] have achieved great success in high-quality image generation. The essence of DMs lies in estimating image distributions by iterative denoising noise-corrupted image, showcasing the superiority over VAEs [43; 44] and GANs  in training stability and likelihood estimation. To further explore the controllability of DMs, considerable efforts

  
**Method** & **IS \(\)** & **FID \(\)** \\  Baseline (w/o MLS) & 20.5 & 23.0 \\  w/ LSD  & 21.1 & 22.7 \\ w/ MLS (Ours) & **22.3** & **21.9** \\   

Table 6: **Ablation study** for Multi-Layer Sampler (MLS).

Figure 6: **Illustration** of object-level Node Addition (NA) and Attribute Control (AC) in the scene. From left to right: **(a)** the image generated by the unmodified scene graph; **(b)** the chair addition; **(c)** the blue-colored wall; and **(d)** the red-colored wall.

have been devoted to conditional generation based on DMs. Benefiting from the naturalness of language  and the advancements of vision-language foundation models [6; 7; 8; 9], numerous text-to-image DMs [40; 41; 46] are beginning to emerge, facilitating explicit control of the corresponding semantics and style. However, the expressive capacity of linear text is limited. Therefore, many studies also endeavor to bolster global control through supplementary conditions, such as depth [46; 47], layout [10; 11; 12; 13], segmentation map [46; 48], and scene graph [14; 15].

**Image Generation from Scene Graphs**. Scene graphs are structured scene representations, where nodes represent objects and edges represent relationships between objects [17; 27]. Given the superiority of scene graphs over linear text in delineating multiple objects and their intricate relationships [27; 49; 50], many studies investigate image generation from scene graphs. These approaches typically fall into two categories: layout-based and semantics-based methods. Layout-based methods [17; 18; 19; 28; 36; 51] initially map scene graphs to coarse scene layouts comprising multiple bounding boxes and further refine these layouts to images with a layout-to-image model (e.g., Layout-Diffusion ). While the layout depicts spatial relationships, it fails to capture abstract relationships within the scene, leading to a lack of object interaction. Another branch is semantics-based methods [14; 16; 29; 52; 53], which focus on graph understanding by directly encoding semantic information from the scene graph. Nevertheless, these methods have limitations in addressing independent scene nodes, leading to issues like entity loss and unreasonable placement. In this paper, we propose a compositional image generation that leverages the layout and semantics derived from the scene graph representation. We complement explicit layout and implicit semantics to enhance the understanding of the diffusion model for scene graphs. Additionally, to improve the controllability in the scene-graph-to-image task, we also attain generalizable generation for object-level graph manipulation (i.e., node addition and attribute control).

## 6 Limitations

The proposed CMA injects object-level information into the diffusion model via masks from the layout, effectively mitigating semantic ambiguity and limiting attribute leakage. In scenarios involving object overlap, the proposed CMA inhibits direct interaction between the visual token and the object embedding along with its attributes. Nonetheless, the attribute information from the visual token inadvertently leaks into the overlapping region in subsequent layers. Hence, there may be attribute leakage among the objects, as shown in Figure 7.

## 7 Conclusion

In this study, we leverage the disentangled textual scene graph representation to condition the diffusion process for generating complex scene images. The innovation of our framework lies in utilizing the VAE for scene relationship modeling and diffusion model (DM) for the composite visual generation. To comprehensively capture spatial relationships and non-spatial interactions within scenes, we introduce the Semantics-Layout Variational AutoEncoder (SL-VAE) for deriving diverse layouts and semantics from a single scene graph. Building upon them, we propose the Compositional Masked Attention (CMA) integrated with DM, which guides the de-noising trajectory by compositing extracted object-level graph information with fine-grained attributes. We also introduce a Multi-Layer Sampler (MLS) to preserve the main visual content while modifying the input scene graph. Extensive experiments demonstrate that our framework outperforms current methods conditioned by text, layout, or scene graph in relationship modeling and controllability.