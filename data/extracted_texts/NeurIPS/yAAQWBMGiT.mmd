# Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning

Yijun Dong

Courant Institute

New York University

yd1319@nyu.edu

&Hoang Phan

Center of Data Science

New York University

hvp2011@nyu.edu

&Xiang Pan

Center of Data Science

New York University

xiangpan@nyu.edu

Equal contribution.

###### Abstract

We revisit data selection in a modern context of finetuning from a fundamental perspective. Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation. Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce **S**ketchy **M**oment **M**atching (SkMM), a scalable data selection scheme with two stages. (i) First, the bias is controlled using gradient sketching that explores the finetuning parameter space for an informative low-dimensional subspace \(\); (ii) then the variance is reduced over \(\) via moment matching between the original and selected datasets. Theoretically, we show that gradient sketching is fast and provably accurate: selecting \(n\) samples by reducing variance over \(\) preserves the fast-rate generalization \(O(()/n)\), independent of the parameter dimension. Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks.

## 1 Introduction

As the data volume and training cost explode with the unprecedented model performance, the long-standing problem of data selection  is getting increasing attention in the modern context of deep learning from various perspectives, including data pruning , coreset selection , and data filtering . A common goal shared by these perspectives is to train a model from scratch on less data to learn high-quality representations and achieve competitive generalization. However, empirical observations also suggest the limitation of data removal during pre-training: a seemingly inevitable tradeoff between less computation and higher-quality representations . While existing works on data selection have a dominating focus on the training-from-scratch setting, the sensitivity of representation learning to data and the growing availability of powerful pre-trained models calls for attention to a less studied  but equally important problem: data selection for finetuning.

In the simplest finetuning setting--linear probing on low-dimensional representations2, data selection falls in the classical frames of coreset selection for linear regression  and optimal experimental design  where the generalization gap can bereduced by selecting data that minimize the associated variance. However, for high-dimensional finetuning, variance minimization alone is insufficient to characterize the generalization due to the overparametrized nature of modern architectures. Even for linear probing, when the parameter dimension \(r\) is higher than the sample size \(n\), the selected data necessarily fails to capture a subspace of the parameter space with dimension at least \(r-n\), leading to errors in addition to variance. Nevertheless, the prevailing empirical and theoretical evidence  on the ubiquitous intrinsic low-dimensional structures of high-dimensional data/model motivates a natural question:

_Can the low intrinsic dimension be leveraged in data selection for high-dimensional finetuning?_

Low intrinsic dimension leads to variance-bias tradeoff in data selection.We provide a positive answer to this question through a variance-bias tradeoff perspective. Intuitively, we consider a low-dimensional subspace \(\) in the finetuning parameter space where the model learns the necessary knowledge for the downstream task. The generalization gap can be controlled by simultaneously reducing _the bias (redundant information) by "exploring" the finetuning parameter space to find a suitable \(\)_ and _the variance by "exploiting" the useful knowledge in \(\)_.

Given the high-dimensional nature of the finetuning parameter space, direct search for such suitable subspace \(\) is computationally infeasible in general. This leads to a follow-up question:

_How to explore the intrinsic low-dimensional structure efficiently for data selection?_

We propose a two-stage solution--_Sketchy Moment Matching (SkMM)_: (i) dimensionality reduction via gradient sketching to efficiently explore the finetuning parameter space, and (ii) variance control via moment matching to exploit useful knowledge in the low-dimensional subspace.

Gradient sketching finds a good low-dimensional subspace fast and provably.First, we construct a low-dimensional parameter subspace \(\) by sketching the model gradients. Sketching  is a well-established dimensionality reduction tool known for affordable and accurate low-rank approximations . In deep learning, sketching recently extends its empirical applications to scalable estimations of influence functions for data selection . We make a first step toward the theoretical guarantee of gradient sketching for data selection: _gradient sketching efficiently finds a low-dimensional subspace \(\) with small bias such that selecting \(n\) samples by reducing variance over \(\) is sufficient to preserve the fast-rate generalization \(O(()/n)\), linear in the low intrinsic dimension \(()\) while independent of the high parameter dimension \(\)_.

Moment matching in low dimension selects data that control the variance.Second, we select data that reduce variance in the low-dimensional subspace \(\) via moment matching. The variance of data selection is characterized by matching between the sketched gradient moments of the original

Figure 1: Controlling variance-bias tradeoff in data selection for high-dimensional finetuning via gradient sketching \(+\) moment matching (SkMM). Consider a toy dataset with \(N\) samples (in blue) whose finetuning gradients lie in a high-dimensional parameter space \(^{r}\) (visualized in 3D) with a low intrinsic dimension (_e.g._, three clusters). The goal is to select \(n=n_{1}+n_{2}+n_{3}<r\) samples for finetuning. (a) **Bias reduction** focuses on minimizing the low-rank approximation error, resulting in uniform selection across clusters regardless of their variance. (b) **Variance reduction3** places more emphasis on high-variance clusters and could lead to large bias by missing low-variance ones. (c) **Gradient sketching** efficiently finds a low-dimensional subspace \(\) (where \(()<n\)) with small bias. (d) **Moment matching** in \(\) controls the variance within the low-bias subspace, leading to a variance-bias balance with fast-rate generalization \(O(()/n)\).

and selected datasets, \(},}_{S}\), formally \((}}_{S}^{})\). This objective involves optimizing over the inversions of (potentially) ill-conditioned matrices, leading to a challenging discrete optimization problem [28; 35]. Under a common heuristic assumption that \(}\), \(}_{S}\) commute [36; 37], we introduce a continuous relaxation with a quadratic objective and linear constraints that is numerically stable (free of pseudoinverse) and can be efficiently optimized via projected gradient descent.

The contributions of this work are summarized as follows:

* We provide a rigorous generalization analysis on data selection for finetuning, illustrating the critical role of dimensionality by unveiling the variance-bias tradeoff in high dimensions.
* We show that gradient sketching provably finds a low-dimensional parameter subspace \(\) with small bias, reducing variance over which preserves the fast-rate generalization \(O(()/n)\). Techniques used in analyzing gradient sketching for data selection are agnostic to the selection method or the finetuning setting and could be of independent interest.
* We introduce SkMM, a scalable two-stage data selection method for finetuning that simultaneously "explores" the high-dimensional parameter space via gradient sketching and "exploits" the information in the low-dimensional subspace via moment matching.

### Related Works

Coreset selection and low-rank approximations.From the variance-bias tradeoff perspective, data selection for high-dimensional finetuning can be viewed as a combination of (i) variance reduction in coreset selection for linear regression [17; 18; 19; 21; 23; 24] with low-dimensional features, and (ii) bias reduction via sample-wise low-rank approximation for high-dimensional matrices [38; 39; 40; 41; 42; 43; 44; 45; 46].

Gradient sketching[17; 32] based on Johnson-Lindenstrauss transforms (JLTs) [31; 47] has achieved impressive recent successes in efficient data selection  and attribution . Despite the empirical success, theoretical understanding of the effect of gradient sketching on generalization remains limited. We make a first step toward this in the context of data selection leveraging existing theories on sketching (vide Remark 3.1 and Appendix C).

Moment matching and optimal experimental design.Moment matching is an intuitive idea for selecting low-dimensional data (_i.e._, overdetermined with coreset size \(n\) larger than data/representation dimension \(r\)), bearing various objectives like the A/V-optimality [28; 29] from optimal experimental design (OED) [25; 26; 27]. While classical OED studies the overdetermined scenario with \(n r\), efforts have been made to extend the notion of V-optimality beyond the overdetermined setting [48; 49]. Nevertheless, these works focus on the general overparametrized setting without considering potential special structures in data. In the context of data selection, this can lead to pessimistic sample complexity, especially for learning problems with low-intrinsic dimensions.

For multimodal contrastive learning, recent works [12; 13] illustrated the effectiveness of moment matching via tailored data selection criteria for CLIP . Distinct from our setting of general finetuning in both low and high dimensions, these works focus on data filtering (with \(n>r\)) for pretraining from scratch.

(Unsupervised) data selection.In this work, we focus on unsupervised data selection that instead of relying on labels4, leverages the geometry of the feature space and aims to select samples that are spread out, with a broad spectrum of concretizations including herding [51; 52], k-center greedy , leverage score sampling [24; 54; 55], adaptive sampling [44; 56], and volume sampling [39; 41].

An inspiring recent work  investigates the generalization of weakly supervised data selection via independent sampling in the low- (\(n\) with fixed \(r\)) and high-dimensional (\(n,r\) with \(n/r\)) asymptotics. Instead of the asymptotic regime, we consider a realistic setting with finite \(n\) and \(r\), without specific assumptions on the data/feature distribution other than the low intrinsic dimension. Along this line, (weakly) supervised data selection commonly make choices based on the uncertainty [58; 59; 60] or sensitivity of the loss to samples (_e.g._, influence function [4; 61; 62], sensitivity scores [5; 63; 64; 65], and heuristics based on losses and their gradients [66; 67; 68; 69]).

### Notations

Given any \(n_{+}\), we denote \([n]=\{1,,n\}\). Let \(_{n}\) be the \(n\)-th canonical basis of the conformable dimension; \(_{n}\) be the \(n n\) identity matrix; and \(_{n},_{n}^{n}\) being vectors with all entries equal to zero and one, respectively. Let \(^{n-1}:=\{^{n}|\|\|_{2}=1\}\) be the unit sphere in \(^{n}\), and \(_{n}:=\{[0,1]^{b}\,\|\, \|_{1}=1\}\) be the dimension-\(n\) probability simplex. We adapt the standard asymptotic notations: for any functions \(f,g:_{+}_{+}\), we write \(f=O(g)\) or \(f g\) if there exists some constant \(C>0\) such that \(f(x) Cg(x)\) for all \(x_{+}\); \(f=(g)\) or \(f g\) if \(g=O(f)\); \(f g\) if \(f=O(g)\) and \(f=(g)\). For any matrix \(^{n d}\), let \(s_{1}() s_{()}() 0\) be the singular values; and \(^{}\) be the Moore-Penrose pseudoinverse. Additionally for any \(k()\), let \(_{k}=_{:\, () k}\|-\|_{F}\) be the optimal rank-\(k\) approximation of \(\) (characterized by the rank-\(k\) truncated SVD). For any symmetric matrices \(,^{d d}\), we write \(\) or \(- 0\) if \(-\) is positive semidefinite.

## 2 Data Selection for Finetuning

Given a data space \(^{d}\) and a label space \(\), let \(=\{(_{i},y_{i})\,|\,i[N]\}\) be a large dataset, with matrix form \((,)^{N d}^{N}\), for some downstream task where the performance is measured by a loss function \(:_{ 0}\).

Finetuning.Let \(\) be a class of prediction functions where each \(f=h\) can be expressed as the composition of an expressive representation function \(\) and a prediction head \(h\). We consider a pre-trained model \(\) that yields high-quality representations for some downstream tasks on \(\) and denote \(_{|}\) as the class of finetuned models based on \(\). Assume that for every \((_{i},y_{i})\), \(y_{i} P(y_{i})\)_i.i.d._ such that there exists \(f_{*}^{}_{|}\) with respect to \(\) satisfying (i) \([y_{i}(_{i})]=f_{*}^{} (_{i})\), and (ii) \([y_{i}(_{i})]^{2}\) for some \(>0\) (which will be formalized later in respective settings).

Data selection.Instead of finetuning on the entire dataset \(\), we aim to select a small coreset \(_{S}\) of size \(n N\) where the generalization is close. Precisely, let \(_{S}\) be indexed by \(S[N]\) and denoted as \((_{S},_{S})^{n d}^{n}\). With \(_{_{S}}(f)=_{(,y) _{S}}(f(),y)\) and a regularization \(:_{|}_{ 0}\) associated with a hyperparameter \( 0\), we want \(f_{S}=_{f_{|}}_{_{S}} (f)+(f)\) to provide a low excess risk over \(\): \((f_{S}):=_{i=1}^{N}(f_{S}(_{ i}),f_{*}^{}(_{i}))\).

### Low-dimensional Linear Probing: Variance Minimization

Warming up with linear probing, we concretize the general assumption on the ground truth (_i.e._, \([y_{i}(_{i})]=f_{*}^{} (_{i})=(_{i})^{}_{*}\) and \([y_{i}(_{i})]^{2}\)) as follows:

**Assumption 2.1** (linear probing ground truth).: _Assume \(=()_{*}+\) for some \(_{*}^{r}\) where \(=[z_{1},,z_{N}]^{}^{N}\) consists of i.i.d. entries with \([]=_{N}\) and \([^{}]^{2} _{N}\)._

Consider the pre-trained representations \(()^{N r}\) and \((_{S})^{n r}\) with respective moments \(^{}:=()^{} ()\) and \(^{}_{S}:=(_{S})^{} (_{S})\). For low-dimensional linear probing with \(r n\) (_s.t._\((^{}_{S})=r\)), the linear regression \(_{S}=_{}\| (_{S})-_{S}\|_{2}^{2}\) has a unique solution with excess risk \((_{S})=\|_{S}- _{*}\|_{^{}}^{2}\)5 controlled by \(^{}\) and \(^{}_{S}\), analogous to the V-optimality criterion  in optimal experimental design:

\[[(_{S})] }{n}(^{}( ^{}_{S})^{-1}),\] (1)

If \(_{S}\) satisfies \(^{} c_{S}^{}_{S}\) for some \(c_{S}\), then \([(_{S})] c_{S} r}{n}\) (proof in Appendix B.1), where \(c_{S}\) characterizes the variance controlled by \(_{S}\), _i.e._, smaller \(c_{S}\) implies lower variance.

Despite its simplicity, uniform sampling is often observed in practice to serve as a strong baseline for data selection , especially when \(n\) is large. In the low-dimensional linear probing scenario, (1) provides a theoretical justification for such effectiveness of uniform sampling:

**Proposition 2.1** (Uniform sampling for low-dimensional linear probing (Appendix B.2)).: _Assume there exists (i) \(B_{}>0\) such that \(\|()\|_{2} B_{}\;\; \); and (ii) \(>0\) with \(^{}_{r}\). For \(S\) sampled uniformly (with replacement) over \(\), with probability at least \(1-\) over \(S\), \(^{} c_{S}_{S}^{}\) for any \(c_{S}>1\) if \(n^{4}}{^{2}}) ^{2}}\)._

That is, for linear probing with sufficiently low dimension \(r n\), under mild regularity assumptions on data, uniform sampling enjoys a near-optimal generalization \(O(r/n)\).

### High-dimension Finetuning with Low Intrinsic Dimension: Variance-Bias Tradeoff

Extending the analysis to general finetuning, we consider a set of \(r\) finetuning parameters \(^{r\,6}\) (potentially with \(r n\)) over a pre-trained model \(\) (_e.g._, \(\) can be the parameters of the last layer (_i.e._, linear probing), last few layers, the entire network, or the LoRA  matrices).

Let \(_{|}=\{f^{}(;):\;|\;^{r}\}\) be the finetuning function class. Without loss of generality, we assume zero initialization of \(\) such that \(f^{}(;_{r})\) corresponds to the pre-trained model. Analogous to the assumption in , under locality constraint on \(\) (_e.g._, \(\|\|_{2}<1\)), the dynamics of finetuning falls in the kernel regime  where \(f^{}\) can be approximated by its first-order Taylor expansion: \(f^{}(;) f^{}( ;_{r})+_{}f^{} (;_{r})^{}\). Then, we formalize the ground truth as follows:

**Assumption 2.2** (Finetuning ground truth).: _Given the pre-trained \(\), there exists a bounded ground truth \(_{*}^{r}\) with \(\|_{*}\|_{2}<1\) such that for all \((,y)\), (i) \([y\;|\;()]=f^{}_{*} ()=f^{}(;_{*})\), and (ii) \([y\;|\;()]^{2}\) for some \(>0\)._

Intuitively, Assumption 2.2 implies that the pre-trained model \(f^{}(;_{r})\) has a reasonable zero-shot performance. Given any \(S[N]\) with \(|S|=n\), let \(f^{}(_{S};)^{n}\) and \(_{}f^{}(_{S}; )^{n r}\) be the evaluation of \(f^{}(;)\) and its Jacobian over \(_{S}\) at \(\). We observe that with \(:=-f^{}(;_{*})\), Assumption 2.2 implies \(-f^{}(;_{r}) _{*}+\) where \([]=_{N}\) and \([^{}]^{2} _{N}\); while \(:=_{}f^{}(;_{r})^{N r}\) is the Jacobian over \(\) at initialization.

Then in the kernel regime , the finetuning objective \(_{^{r}}\|f^{}( _{S};)-_{S}\|_{2}^{2}+ \|\|_{2}^{2}\) can be well approximated by a ridge regression problem:

\[_{S}=*{argmin}_{^{r}}\|_{}f^{}( _{S};_{r})-(_{ S}-f^{}(_{S};_{r}))\|_{2}^{2}+ \|\|_{2}^{2}.\] (2)

Recall \(:=_{}f^{}(; _{r})^{N r}\) and \(_{S}:=_{}f^{}(_{S}; _{r})^{n r}\). With the moments \(^{}=^{}\) and \(_{S}^{}=_{S}^{}_{S}\), the excess risk \((_{S})=\|_{S}-_{*}\|_{^{}}^{2}\) satisfies7:

**Theorem 2.2** (Main result I: variance-bias tradeoff (Appendix B.3)).: _Given \(S\), let \(_{}^{r r}\) be an orthogonal projector onto some subspace \((_{S}^{})\), and \(_{}^{}=_{r}-_{}\) be its orthogonal complement. Under Assumption 2.1, there exists an \(>0\) such that (2) satisfies \([(_{S})] +\) with (i) \(=}{n}(^{}( _{}_{S}^{}_{})^{ })\) and (ii) \(=2(^{}_{ }^{})\|_{*}\|_{2}^{2}\)._

Specifically, the variance-bias tradeoff is controlled by the unknown \(\): expanding \(\) leads to higher variance but lower bias. Reducing the generalization gap involves finding a suitable \(\) in the high-dimensional parameter space, a computationally challenging problem addressed in Section 3.1.

It is worth highlighting that Theorem 2.2 encapsulates both the low- and high-dimensional finetuning. For low-dimensional linear probing, (1) is a special case of Theorem 2.2 (up to constants) with \(_{}=_{r}\). While in high dimension, an intrinsic low-dimensional structure (_e.g._, Assumption 2.3) is necessary for the effectiveness of data selection8.

**Assumption 2.3** (Low intrinsic dimension).: _Consider the second moment \(^{} 0\) over \(\) with \(N\) samples. Let \(:=\{t[r](^{}- ^{}_{t})(^{ })/N\}\) be the intrinsic dimension. Assume that \(^{}\) has a low intrinsic dimension: \(\{N,r\}\)._

When the high-dimensional finetuning parameter space has a low intrinsic dimension \(\{N,r\}\), Theorem 2.2 can be further concretized with suitable \(_{S}\) and associated \(\):

**Corollary 2.3** (Exploitation + exploration (Appendix B.3)).: _Under the same setting as Theorem 2.2 and Assumption 2.3, if \(S\) satisfies for some subspace \((^{}_{S})\) with \((_{})\) and \(c_{S}\) that (i) \(_{}(c_{S}^{}_{}-^{ })_{} 0\) and (ii) \((^{}_{}^{}) (^{}-^{ }_{})\), then9_

\[[(_{S})]+(c_{S}^{2}+ (^{})\|_{*}\| _{2}^{2}).\] (3)

In particular, with \(c_{S}, 1\), \(\|_{*}\|_{2}^{2}<1\), and \((^{})\) (depending only on the low intrinsic dimension), the generalization achieves a fast rate \(O(/n)\), independent of \(r\).

In (3), (i) **bias** is reduced by exploring the parameter space for an \(\) with small low-rank approximation error \((^{}_{}^{}) (^{})\); while (ii) **variance** is reduced by exploiting information in \(\) through moment matching, \(_{}(c_{S}^{}_{}-^{ })_{} 0\), where smaller \(c_{S}\) means better exploitation.

## 3 Sketchy Moment Matching

A gap between Corollary 2.3 and practice is _how to find a suitable \(\) efficiently in the high-dimensional parameter space_. In this section, we introduce a simple scalable algorithm for constructing \(\) and \(_{S}\) that satisfies the exploration and exploitation conditions in Corollary 2.3.

### Find Low Intrinsic Dimension via Gradient Sketching

For high-dimensional finetuning with \(r n\), a critical limit of Theorem 2.2 and Corollary 2.3 is that the large moment matrices \(^{},^{}_{}\) are not invertible, storable, or even directly computable, due to the prohibitive cost. As a remedy, sketching [17; 32] via Johnson-Lindenstrauss transforms  is a classical dimensionality reduction strategy that gets increasing recent attention for gradient approximation in large-scale machine learning problems [16; 34]10.

**Remark 3.1** (Gradient sketching).: _In the high-dimensional setting with \(r n\), to reduce the dimensionality of the gradients \(=_{}f^{}(;_{r}) ^{N r}\) with a low intrinsic dimension \(\{N,r\}\) (Assumption 2.3), we draw a Johnson-Lindenstrauss transform  (JLT, formally in Definition C.1) \(^{r m}\) that projects the dimension-\(r\) gradients to a lower dimension \(m r\): \(}=^{N m}\). One of the most common constructions of JLT is the Gaussian embedding (i.e., a Gaussian random matrix with i.i.d. entries \(_{ij}(0,1/m)\) discussed in Lemma C.3, vide Remark C.1 for a brief overview of various (fast) JLTs and their efficiency)._

While sketching is known for preserving Euclidean distances  and providing accurate low-rank approximations [17; 32; 33], _whether gradient sketching can convert Theorem 2.2 to an efficiently computable form without compromising the generalization guarantee?_ We answer this question affirmatively with the following theorem.

**Theorem 3.1** (Main result II: gradient sketching (formally in Theorem C.1)).: _Under Assumption 2.2 and 2.3 with a low intrinsic dimension \(\{N,r\}\), draw a Gaussian embedding \(^{r m}\) (Lemma C.3) with \(m 11\). Let \(}^{}:=^{}^{}\) and \(}^{}_{}:=^{}^{}_ {}\) be the sketched gradient moments. For any \(_{S}\) with \(n>m\) samples such that \((^{}_{})=n\), and the \( 1.1\)-th largest eigenvalue \(s_{ 1.1}(}^{}_{}) _{S}\) for some \(_{S}>0\), with probability at least \(0.9\) over \(\), there exists \(>0\) where (2) satisfies \([(_{S})]++\) with (i) **variance**\(=}{n}(}^{}_{ })^{})\), (ii) **sketching error**\(=}{n}}\|}^{}( }^{}_{})^{}\|_{2}( ^{})\), and (iii) \(=\|}^{}(}^{ }_{})^{}\|_{2}(^{})\|_{*}\|_{2}^{2}\).__If \(S\) further satisfies \(}^{} c_{S}}^{}_{S}\) for some \(c_{S}\), with \(m=\{(^{})/_{S}},11 {}\}\),_

\[[(_{S})] ++}{n} (^{2}m+(^{})\|_{*}\|_{2}^{2}).\] (4)

Comparing (4) with (3), we observe that by controlling the variance with \(}^{} c_{S}}^{}_ {S}\) in low dimension \(m r\), gradient sketching preserves the fast-rate generalization \(O(m/n)=O(/n)\) up to constants. That is, gradient sketching implicitly finds a random subspace \((^{}_{S})\) (vide (9)) that satisfies the exploration assumption in Corollary 2.3. Meanwhile, the choice of sketching size \(m\) balances the tradeoff between **variance** and **sketching error**: a larger \(m\) reduces the sketching error at the cost of higher variance. Such tradeoff is optimized at \(m=(^{})/_{S}}\).

### Control Variance via Moment Matching

Given the intrinsic low-dimensional structure with small bias in Section 3.1, Theorem 3.1 connects generalization to the variance controlled by the matching between \(}^{}\) and \(}^{}_{S}\). Specifically, when the selected data \(_{S}\) satisfies \(}^{} c_{S}}^{}_ {S}\) for some \(c_{S}\), we have \((}^{}(}^{ }_{S})^{}) c_{S}m\) and \(\|}^{}(}^{}_{S})^{} \|_{2} c_{S}\) upper bounded, leading to the fast-rate generalization in (4).

```
1:Input:\(f^{}(;_{r})\), \(n N\), \(m<n\), \(c_{S}[,1]\).
2:Draw a (fast) Johnson-Lindenstrauss transform \(^{r m}\) (Remark 3.1).
3:Compute gradient sketching \(}=_{}f^{}(;_{ r})^{N m}\). (Remark 3.4)
4:Compute the spectral decomposition of \(}^{}=}^{} } 0\): \(}^{}=^{}\) where 1. \(=[_{1},,_{m}]^{m m}\) consists of the orthonormal eigenvectors, and 2. \(=(_{1},,_{m})\) contains descending eigenvalues \(_{1}_{m} 0\).
5:Initialize \(=[s_{1},,s_{N}]\) with \(s_{i}=\) on \(n\) uniformly sampled \(i\)'s and \(s_{i}=0\) elsewhere.
6:Let \(()^{N N}\) be a diagonal matrix with \(\) on diagonal. Optimizing: \[&_{_{N}}\;_{=[_{1},,_{m}]^{m}}\;_{j=1}^{m} (_{j}^{}}^{} ()}_{j}-_{j} _{j})^{2}\\ & 0 s_{i} 1/n\;\;i[N], _{j} 1/c_{S}\;\;j[m].\] (5)
7:Output:\(S[N]\) by sampling \(n\) data from \(_{N}\) without replacement. ```

**Algorithm 3.1** Sketchy Moment Matching (SkMM)

While directly minimizing \((}^{}(}^{ }_{S})^{})\) involves integer programming and pseudoinverse, causing hard and numerically unstable optimization, \(}^{} c_{S}}^{}_ {S}\) has a straightforward relaxation (vide Remark 3.2), leading to the simple and stable moment matching objective (5) in Algorithm 3.1.

**Remark 3.2** (Relaxing \(}^{} c_{S}}^{}_ {S}\) to (5)).: _Given the spectral decomposition \(}^{}=^{}\), \(}^{} c_{S}}^{}_ {S}\) can be rewritten as \(^{}(}^{}_{S}}_{S})}\), and (5) is a relaxation: (i) instead of enforcing \(}^{} c_{S}}^{}_ {S}\) strictly, constraints are only imposed on the diagonal11: \(_{j}^{}(}^{}_{S}}_{S})_{j}_{j}/c_{S}\), \(j[m]\); and (ii) the selection of \(S\) is relaxed to a weight vector \(_{N}\) with linear constraints \(0 s_{i} 1/n\). Free of integer constraints and pseudoinverse, the quadratic data selection objective with linear constraints in (5) can be solved efficiently and stably via projected gradient descent._

Alternative to the moment matching heuristic in Remark 3.2, variance reduction by controlling \(}^{}(}^{}_{S})^{}\) in the low-dimensional subspace can be realized via various methods, including leverage score sampling  and V-optimal experimental design . We provide brief discussions on these alternatives in Appendix A.2.

**Remark 3.3** (\(c_{S}\) controls strength of moment matching).: _In Algorithm 3.1, smaller \(c_{S}\) enforces \(}^{}_{S}\) to exploit more information in \(}^{}\), bringing lower variance and better generalization. While the lower bound \(c_{S}\) could be tight (vide Remark B.1), in practice, the smallest feasible \(c_{S}\) depends on the data distribution and tends to be larger (e.g., \(c_{S} 1\) in the experiments)._

**Remark 3.4** (Computational efficiency of SkMM).: _SkMM is efficient in both memory and computation. Consider the two stages in Algorithm 3.1: (i) Gradient sketching can be computed in parallel with input-sparsity time and on the fly without storing the (potentially) high-dimensional gradients (vide Remark C.1). (ii) After gradient sketching, variance reduction via moment matching happens in the low dimension \(m\), with a low memory footprint \(O(Nm)\), taking \(O(m^{3})\) for the spectral decomposition and \(O(Nm)\) per iteration for optimizing the moment matching objective (5)._

## 4 Experiments

### Synthetic High-dimensional Linear Probing

To ground the theoretical insight on variance-bias tradeoff in high-dimensional finetuning, we simulate linear probing with a synthetic underdetermined ridge regression problem12.

Setup.We consider a set of \(N=2000\) samples with high-dimensional pre-trained representations \(()^{N r}\), \(r=2400\), modeled by a Gaussian mixture model (GMM) consisting of \(=8\) well-separated clusters, each with random sizes and variances (vide Figure 2). Samples within each cluster share the same randomly generated label. We solve the ridge regression problem (2) over the selected coreset of \(n\) samples with hyperparameter \(\) tuning. The empirical risk is evaluated over the full dataset \(_{}(_{S})=\|() _{S}-\|_{2}^{2}\) (vide Appendix D.1 for implementation details).

Data selection.For SkMM (Algorithm 3.1), we use a sketching dimension \(m=4=32\) and set \(c_{S}=0.999\). We optimize (5) via Adam  with constraint projection under learning rate \(10^{-7}\) for \(10^{4}\) iterations and sample \(S\) from \(_{N}\) with the lowest objective value.

We compare SkMM to representative unsupervised data selection methods for regression, including uniform, leverage score [18; 19; 72; 73; 74], adaptive sampling [44; 56], herding [51; 52], and k-center greedy . Specifically, (i) SkMM, truncated leverage score (T-leverage), and ridge leverage score sampling (R-leverage) can be viewed as different ways of variance-bias balancing; (ii) adaptive sampling (Adaptive) and k-center greedy (K-Center) focus on bias reduction (_i.e._, providing good low-rank approximation/clustering for \(()\)); while (iii) Herding and uniform sampling (Uniform) reduce variance (vide Appendix D.2 for baseline details).

We observe from Figure 2 and Table 1 that balancing the variance-bias tradeoff is crucial for the generalization of data selection in high dimensions. In particular, SkMM achieves the best empirical

Figure 2: Selecting \(n=80\) data (colored in red) from the GMM dataset. Intuitively, a coreset \(_{S}\) with low bias contains at least one sample per cluster; whereas a low-variance \(_{S}\) selects more data from clusters with larger variance. We recall from Theorem 2.2 that the variance-bias balance is essential for good generalization.

risk across different coreset sizes \(n\), especially when \(n\) is small. While as \(n/N 1\), uniform sampling provides a strong baseline, coinciding with common empirical observations .

### Experiments on Regression Tasks

We further validate the effectiveness of SkMM on UTKFace , a real-world regression dataset for age estimation. We finetune a randomly initialized classification head on top of the feature representation of CLIP  with Adam  and learning rate \(10^{-1}\). We also retain those baselines from the above synthetic setup in this experiment.

The results for linear probing are provided in Table 2, where our method remarkably outperforms comparative baselines on UTKFace. For every coreset size, SkMM improves the performance of CLIP compared to uniform sampling. Especially for small coreset size \(n=100,200\), it achieves a Mean Absolute Error reduction of approximately \(50\%\).

### Experiments on Image Classification Tasks

While our analysis focuses on data selection for finetuning regression models, a natural question is whether the idea of SkMM applies to broader scopes. To answer this, we extend our empirical investigation to classification. In particular, we consider an imbalanced classification task: Stanford-Cars  with 196 classes, 8144 training samples, and 8041 testing samples where the classes are highly imbalanced with training sample sizes ranging from 24 to 68.

Finetuning.We consider two common ways of finetuning: (i) linear probing (LP) over the last layer and (ii) finetuning (FT) over the last few layers, covering both the low- (_i.e._, \(n r\) for LP) and high-dimensional (_i.e._, \(r>n\) for FT) settings. For LP, we learn the last layer over the embeddings from a CLIP-pretrained ViT-B/32  with a learning rate of \(10^{-1}\). For FT13, we finetuning the last two layers of an ImageNet-pretrained ResNet18  with a learning rate of \(10^{-2}\). In both settings, we optimize via Adam for 50 epochs. Due to space limit constraints, detailed results for fine-tuning are deferred to the appendix.

Data selection.For SkMM-LP, the gradients (of the last layer) are given by the pretrained features from CLIP. For SkMM-FT, the gradients (of the last two layers) are calculated based on a random classification head. We tune the sketching dimension \(m\{32,64,128,256,512\}\) and the lower bound for slackness variables \(c_{S}\{0.6,0.7,0.8,0.9\}\). Within suitable ranges, smaller \(m\) and larger \(c_{S}\) lead to better performance in the low data regime. Intuitively, smaller \(m\) encourages variance reduction in a more compressed subspace, and larger \(c_{s}\) leads to easier optimization.

We compare SkMM to various unsupervised and (weakly) supervised data selection methods for classification, including uniform sampling, herding , Contextual Diversity , Glister , GraNd , Forgetting , DeepFool , as well as three uncertainty-based methods, Entropy, Margin, and Least Confidence .

Observations.We first observe that for both LP (Table 3) and FT (Table 4), SkMM achieves competitive finetuning accuracy on StanfordCars. Since SkMM is an unsupervised process agnostic of true class sizes, the appealing performance of SkMM on the imbalanced StanfordCars dataset echoes the ability of SkMM to handle data selection among clusters of various sizes through variance-bias balance (_cf._ synthetic experiments in Figure 2). Meanwhile, for LP in the low-dimensional setting (Table 3), uniform sampling provides a surprisingly strong baseline. This coincides with the theoretical insight from Proposition 2.1 and the empirical observations in .

## 5 Discussion, Limitations, and Future Directions

We investigated data selection for finetuning in both low and high dimensions from a theoretical perspective. Beyond variance reduction in low dimension, our analysis revealed the _variance-bias tradeoff in data selection for high-dimensional finetuning with low intrinsic dimension_\(\), balancing which led to a _fast-rate generalization_\(O(/n)\). For efficient control of such variance-bias tradeoff in practice, we introduced SkMM that first explores the high-dimensional parameter space via _gradient sketching_ and then exploits the resulting low-dimensional subspace via _moment matching_. Theoretically, we showed that _the low-dimensional subspace from gradient sketching preserves the fast-rate generalization_. Moreover, we ground the theoretical insight on balancing the variance-bias tradeoff via synthetic experiments, while demonstrating the effectiveness of SkMM for finetuning real vision tasks.

In this work, we focus only on moment matching via optimization inspired by the analysis for variance reduction after gradient sketching. Nevertheless, there is a remarkable variety of existing low-dimensional data selection strategies (_e.g._, via greedy selection or sampling) that could potentially be extended to high dimensions leveraging sketching as an efficient pre-processing step. In linear algebra, sketching has been widely studied for accelerating, as well as stabilizing, large-scale low-rank approximations and linear solvers. However, the intuitions and theories there may or may not be directly applicable to the statistical learning regime. In light of the high-dimensional nature of deep learning where sketching brings an effective remedy, we hope that providing a rigorous generalization analysis for sketching in data selection would make a step toward bridging the classical wisdom of sketching and the analogous challenges in modern learning problems.

    & \(n\) & 2000 & 2500 & 3000 & 3500 & 4000 \\   & Acc & 67.63 \(\) 0.17 & 70.59 \(\) 0.19 & 72.49 \(\) 0.19 & 74.16 \(\) 0.22 & 75.40 \(\) 0.16 \\  & F1 & 64.54 \(\) 0.18 & 67.79 \(\) 0.23 & 70.00 \(\) 0.20 & 71.77 \(\) 0.23 & 73.14 \(\) 0.12 \\   & Acc & 67.22 \(\) 0.16 & 71.02 \(\) 0.13 & 73.17 \(\) 0.22 & 74.64 \(\) 0.18 & 75.71 \(\) 0.29 \\  & F1 & 64.07 \(\) 0.23 & 68.28 \(\) 0.15 & 70.64 \(\) 0.28 & 72.22 \(\) 0.26 & 73.26 \(\) 0.39 \\   & Acc & 67.64 \(\) 0.13 & 70.82 \(\) 0.23 & 72.66 \(\) 0.12 & 74.46 \(\) 0.17 & 75.77 \(\) 0.12 \\  & F1 & 64.51 \(\) 0.17 & 68.18 \(\) 0.25 & 70.05 \(\) 0.11 & 72.13 \(\) 0.15 & 73.35 \(\) 0.07 \\   & Acc & 67.60 \(\) 0.24 & 70.85 \(\) 0.27 & 73.07 \(\) 0.26 & 74.63 \(\) 0.21 & 76.00 \(\) 0.20 \\  & F1 & 64.50 \(\) 0.34 & 68.07 \(\) 0.38 & 70.47 \(\) 0.35 & 72.18 \(\) 0.25 & 73.69 \(\) 0.24 \\   & Acc & 67.27 \(\) 0.07 & 70.38 \(\) 0.07 & 72.56 \(\) 0.05 & 74.67 \(\) 0.06 & 75.77 \(\) 0.12 \\  & F1 & 64.04 \(\) 0.09 & 67.48 \(\) 0.09 & 69.81 \(\) 0.08 & 73.44 \(\) 0.13 \\   & Acc & 67.59 \(\) 0.10 & 70.99 \(\) 0.05 & 72.54 \(\) 0.07 & 74.81 \(\) 0.05 & 75.74 \(\) 0.01 \\  & F1 & 64.85 \(\) 0.13 & 68.53 \(\) 0.07 & 70.30 \(\) 0.05 & 72.59 \(\) 0.04 & 73.74 \(\) 0.02 \\   & Acc & 67.77 \(\) 0.29 & 70.73 \(\) 0.22 & 73.24 \(\) 0.22 & 74.57 \(\) 0.23 & 75.71 \(\) 0.15 \\  & F1 & 64.16 \(\) 0.68 & 68.49 \(\) 0.53 & 70.93 \(\) 0.32 & 72.44 \(\) 0.27 & 73.79 \(\) 0.15 \\   & Acc & 67.95 \(\) 0.11 & 71.00 \(\) 0.10 & 73.28 \(\) 0.10 & 75.02 \(\) 0.08 & 75.82 \(\) 0.06 \\  & F1 & 64.55 \(\) 0.10 & 67.95 \(\) 0.12 & 70.68 \(\) 0.12 & 72.46 \(\) 0.12 & 73.29 \(\) 0.04 \\   & Acc & 67.53 \(\) 0.14 & 71.19 \(\) 0.09 & 73.09 \(\) 0.14 & 74.66 \(\) 0.11 & 75.57 \(\) 0.13 \\  & F1 & 64.16 \(\) 0.15 & 68.33 \(\) 0.14 & 70.37 \(\) 0.17 & 72.03 \(\) 0.11 & 73.14 \(\) 0.20 \\   & Acc & 67.68 \(\) 0.11 & 70.99 \(\) 0.14 & 73.04 \(\) 0.05 & 74.65 \(\) 0.09 & 75.58 \(\) 0.08 \\  & F1 & 64.09 \(\) 0.20 & 68.03 \(\) 0.20 & 70.30 \(\) 0.07 & 72.02 \(\) 0.10 & 73.15 \(\) 0.12 \\   & Acc & **68.27 \(\) 0.03** & **71.53 \(\) 0.05** & **73.61 \(\) 0.02** & **75.12 \(\) 0.01** & **76.34 \(\) 0.02** \\  & F1 & **65.29 \(\) 0.03** & **68.75 \(\) 0.06** & **71.14 \(\) 0.03** & **72.64 \(\) 0.02** & **74.02 \(\) 0.10** \\   

Table 3: Accuracy and F1 score (%) of LP over CLIP on StanfordCars