# VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models

Sheng-Yen Chou

The Chinese University of Hong Kong

shengyenchou@cuhk.edu.hk

Pin-Yu Chen

IBM Research

pin-yu.chen@ibm.com

Tsung-Yi Ho

The Chinese University of Hong Kong

tyho@cse.cuhk.edu.hk

###### Abstract

Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM  and DDIM ) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs.

## 1 Introduction

In recent years, diffusion models (DMs)  trained with large-scale datasets  have emerged as a cutting-edge content generation AI tool, including image , audio , video , text , and text-to-speech  generation. Even more, DMs are increasingly used in safety-critical tasks and content curation, such as reinforcement learning, object detection, and inpainting .

As the research community and end users push higher hope on DMs to unleash our creativity, there is a rapidly intensifying concern about the risk of backdoor attacks on DMs . Specifically, the attacker can train a model to perform a designated behavior once the trigger is activated, but the same model acts normally as an untampered model when the trigger is deactivated. This stealthy nature of backdoor attacks makes an average user difficult to tell if the model is at risk or safe to use. The implications of such backdoor injection attacks include content manipulation (e.g. generating inappropriate content for image inpainting), falsification (e.g. spoofing attacks), and model watermarking (by viewing the embedded trigger as a watermark query). Further, the attacker can also use backdoored DMs to generate biased or adversarial datasets at scale , which may indirectly cause future models to become problematic and unfair .

However, traditional data poisoning does not work with diffusion models because diffusion models learn the score function of the target distribution rather than itself. It is worth noting that existing works related to backdoor attacks on DMs  have several limitations: (1) they only focus on basic DMs like DDPM  and DDIM ; (2) they are not applicable to off-the-shelf advanced training-free samplers like DPM Solver , DPM Solver++ , and DEIS ; and (3) they study text-to-image DMs by modifying the text encoder instead of the DM . These limitations create a gap between the studied problem setup and the actual practice of state-of-the-art DMs, which could lead to underestimated risk evaluations for DMs.

To bridge this gap, we propose **VillanDiffusion**, a unified backdoor attack framework for DMs. Compared to existing methods, our method offers new insights in (1) generalization to both denoising diffusion models like DDPM [16; 51] and score-based models like NCSN [54; 55; 56]; (2) extension to various advanced training-free samplers like DPM Solver [34; 35], PNDM , UniPC  and DEIS  without modifying the samplers; and (3) first demonstration that a text-to-image DM can be backdoored in the prompt space even if the text encoder is untouched.

As illustrated in Figure 0(a), in our **VillanDiffusion** framework, we categorize the DMs based on three perspectives: (1) schedulers, (2) samplers, and (3) conditional and unconditional generation. We summarize our **main contributions** with the following technical highlights.

\(\) First, we consider DMs with different **content schedulers**\((t)\) and **noise schedulers**\((t)\). The forward diffusion process of the models can be represented as a transitional probability distribution followed by a normal distribution \(q(_{t}|_{0}):=((t)_{0}, ^{2}(t))\). The schedulers control the level of content information and corruption across the timesteps \(t[T_{min},T_{max}]\). We also denote \(q(_{0})\) as the data distribution. To show the generalizability of our framework, we discuss two major branches of DMs: DDPM  and Score-Based Models [54; 55; 56]. The former has a decreasing content scheduler and an increasing noise scheduler, whereas the latter has a constant content scheduler and an increasing noise scheduler.

\(\) Secondly, our framework also considers different kinds of samplers. In [34; 56], the generative process of DMs can be described as a reversed-time stochastic differential equation (SDE):

\[d_{t}=[(_{t},t)-g^{2}(t)_{_{t} } q(_{t})]dt+g(t)d\] (1)

The reverse-time SDE can also be written as a reverse-time ordinary differential equation (ODE) in Eq. (2) with the same marginal probability \(q(_{t})\). We found that the additional coefficient \(\) will cause BadDiffusion  fail on the ODE samplers, including DPM-Solver  and DDIM .

\[d_{t}=[(_{t},t)-g^{2}(t)_{ _{t}} q(_{t})]dt\] (2)

\(\) Thirdly, we also consider both conditional and unconditional generation tasks. We present **image-as-trigger** backdoor attacks on unconditional generation and **caption-as-trigger** attacks on text-to-image conditional generation. Compared to , which only studies one DM (DDPM) on unconditional generation with image triggers, our method can generalize to various DMs, including DDPM  and the score-based models [54; 55; 56]. In , only DDPM and DDIM  are studied and the attackers are allowed to modify the samplers. Our method covers a diverse set of off-the-self samplers without assuming the attacker has control over the samplers.

\(\) Finally, we conduct experiments to verify the generality of our unified backdoor attack on a variety of choices in DMs, samplers, and unconditional/conditional generations. We also show that the inference-time clipping defense proposed in  becomes less effective in these new setups.

## 2 Related Work

Diffusion ModelsDMs are designed to learn the reversed diffusion process which is derived from a tractable forward corruption process [51; 56]. Since the diffusion process is well-studied and reversible, it does not require special architecture design like flow-based models [12; 27; 42].

Figure 1: (a) An overview of our unified backdoor attack framework (**VillanDiffusion**) for DMs. (b) Comparison to existing backdoor studies on DMs.

Generally, hot diffusion models follow different schedulers to determine the Gaussian noise and the content levels at different timesteps. Commonly used diffusion models are DDPM , score-based models [54; 55], and VDM , etc.

Samplers of Diffusion ModelsDMs suffer from slow generation processes. Recent works mainly focus on sampling acceleration like PNDM  and EDM , which treat the diffusion process as an ODE and apply high-order approximation to reduce the error. Moreover, samplers including UniPC , DEIS , DPM Solver , and DPM-Solver++  leverage the semi-linear property of diffusion processes to derive a more precise approximation. On the other hand, DDIM  discards Markovian assumption to accelerate the generative process. Another training-based method is distilling DMs, such as . In our paper, we focus on backdooring training-free samplers.

Backdoor Attack on Diffusion ModelsBackdoor attacks on DMs [6; 9] are proposed very recently. BadDiffusion  backdoors DDPM with an additional correction term on the mean of the forward diffusion process without any modification on the samplers. TrojDiff  assumes the attacker can access both training procedures and samplers and apply correction terms on DDPM  and DDIM  to launch the attack. The work  backdoors text-to-image DMs via altering the text encoder instead of the DMs. Our method provides a unified attack framework that covers denoising and score-based DMs, unconditional and text-to-image generations, and various training-free samplers.

## 3 VillanDiffusion: Methods and Algorithms

This section first formally presents the threat model and the attack scenario in Section 3.1. Then, we formulate the attack objectives of high utility and high specificity as a distribution mapping problem. We will describe our framework in the form of a general forward process \(q(_{t}|_{0})\) and a variational lower bound (VLBO) in Section 3.3, and generalize it to ODE samplers in Section 3.4. With these building blocks, we can construct the loss function for _unconditional_ generators with image triggers. Finally, in Section 3.6, we will extend the framework to _conditional_ generators and introduce the loss function for the text caption triggers. Details of the proofs and derivations are given in Appendix.

### Threat Model and Attack Scenario

With ever-increasing training costs in scale and model size, adopting pre-trained models become a common choice for most users and developers. We follow  to formulate the attack scenario with two parties: (1) an _attacker_, who releases the backdoored models on the web, and (2) a _user_, who downloads the pre-trained models from third-party websites like HuggingFace. In our attack scenario, the users can access the backdoor models \(_{download}\) and the subset of the clean training data \(D_{train}\) of the backdoored models. The users will evaluate the performance of the downloaded backdoor models \(_{download}\) with some metrics on the training dataset \(D_{train}\) to ensure the utility. For image generative models, the FID  and IS  scores are widely used metrics. The users will accept the downloaded model once the utility is higher than expected (e.g. the utility of a clean model). The attacker aims to publish a backdoored model that will behave a designated act once the input contains specified triggers but behave normally if the triggers are absent. A trigger \(\) can be embedded in the initial noise for DMs or in the conditions for conditional DMs. The designated behavior is to generate a target image \(\). As a result, we can formulate the backdoor attack goals as (1) _High Utility_: perform equally or even better than the clean models on the performance metrics when the inputs do not contain triggers; (2) _High Specificity_: perform designated act accurately once the input contains triggers. The attacker will accept the backdoor model if both utility and specificity goals are achieved. For image generation, we use the FID  score to measure the utility and use the mean squared error (MSE) to quantify the specificity.

### Backdoor Unconditional Diffusion Models as a Distribution Mapping Problem

Clean Forward Diffusion ProcessGenerative models aim to generate data that follows ground-truth data distribution \(q(_{0})\) from a simple prior distribution \(\). Thus, we can treat it as a distribution mapping from the prior distribution \(\) to the data distribution \(q(_{0})\). A clean DM can be fully described via a clean forward diffusion process: \(q(_{t}|_{0}):=((t)_{0}, ^{2}(t))\) while the following two conditions are satisfied: (1) \(q(_{T_{max}})\) and (2) \(q(_{T_{min}}) q(_{0})\) under some regularity conditions. Note that we denote \(_{t},t[T_{min},T_{max}]\), as the latent of the clean forward diffusion process for the iteration index \(t\).

Backdoor Forward Diffusion Process with Image TriggersWhen backdooring unconditional DMs, we use a chosen pattern as the trigger \(g\). Backdoored DMs need to map the noisy poisoned image distribution \((,^{2}(T_{max}))\) into the target distribution \((^{}_{0},0)\), where \(^{}_{0}\) denotes the backdoor target. Thus, a backdoored DM can be described as a backdoor forward diffusion process \(q(_{t}^{}|_{0}^{}):=((t) _{0}^{}+(t),^{2}(t))\) with two conditions: (1) \(q(_{T_{max}}^{})(,^{2}( T_{max}))\) and (2) \(q(_{T_{min}}^{})(_{0}^{},0)\). We call \((t)\) the _correction term_ that guides the backdoored DMs to generate backdoor targets. Note that we denote the latent of the backdoor forward diffusion process as \(_{t}^{},t[T_{min},T_{max}]\), backdoor target as \(_{0}^{}\), and poison image as \(:=+(1-)\), where \(\) is a clean image sampled from the clean data \(q(_{0})\), \(\{0,1\}\) is a binary mask indicating, the trigger \(g\) is stamped on \(\), and \(\) means element-wise product.

Optimization Objective of the Backdoor Attack on Diffusion ModelsConsider the two goals of backdooring unconditional generative models: high utility and high specificity, we can achieve these goals by optimizing the marginal probability \(p_{}(_{0})\) and \(p_{}(_{0}^{})\) with trainable parameters \(\). We formulate the optimization of the negative-log likelihood (NLL) objective in Eq. (3), where \(_{c}\) and \(_{p}\) denote the weight of utility and specificity goals, respectively.

\[_{}-(_{c} p_{}(_{0})+_{p} p_{ }(_{0}^{}))\] (3)

### Generalization to Various Schedulers

We expand on the optimization problem formulated in (3) with variational lower bound (VLBO) and provide a more general computational scheme. We will start by optimizing the clean data's NLL, \(- p_{}(_{0})\), to achieve the high-utility goal. Then, we will extend the derivation to the poisoned data's NLL, \(- p_{}(_{0}^{})\), to maximize the specificity goal.

The Clean Reversed Transitional ProbabilityAssume the data distribution \(q(_{0})\) follows the empirical distribution. From the variational perspective, minimizing the VLBO in Eq. (4) of a DM with trainable parameters \(\) is equivalent to reducing the NLL in Eq. (3). Namely,

\[- p_{}(_{0})=-_{q}[ p_{}(_{ 0})]_{q}_{T}(_{T},_{0})+ _{t=2}^{T}_{t}(_{t},_{t-1},_{0}) -_{0}(_{1},_{0})\] (4)

Denote \(_{t}(_{t},_{t-1},_{0})=D_{KL}(q( _{t-1}|_{t},_{0}) p_{}(_{t-1}|_{t}))\), \(_{T}(_{T},_{0})=D_{KL}(q(_{T}| _{0}) p_{}(_{T}))\), and \(_{0}(_{1},_{0})= p_{}(_{ 0}|_{1})\), where \(D_{KL}(d||p)=_{x}q(x)\) is the KL-Divergence. Since \(_{t}\) usually dominates the bound, we can ignore \(_{T}\) and \(_{0}\). Because the ground-truth reverse transitional probability \(q(_{t-1}|_{t})\) is intractable, to compute \(_{t}\), we can use a tractable conditional reverse transition \(q(_{t-1}|_{t},_{0})\) to approximate it with a simple equation \(q(_{t-1}|_{t},_{0})=q(_{t}| _{t-1})_{t-1}|_{0})}{q(_{t}| _{0})}\) based on the Bayesian and the Markovian rule. The terms \(q(_{t-1}|_{0})\) and \(q(_{t}|_{0})\) are known and easy to compute. To compute \(q(_{t}|_{t-1})\) in close form, DDPM  proposes a well-designed scheduler. However, it does not apply to other scheduler choices like score-based models . Consider the generalizability, we use numerical methods to compute the forward transition \(q(_{t}|_{t-1}):=(k_{t}_{t-1},w_{t}^{2} )\) since the forward diffusion process follows Gaussian distribution. Then, we reparametrize \(_{t}\) based on the recursive definition: \(}_{t}(,_{t})=k_{t}}_{t-1} (,_{t-1})+w_{t}_{t}\) as described in Eq. (5).

\[}_{t}(_{0},_{t})& =k_{t}}_{t-1}(_{0},_{t-1})+w_{t} _{t}=k_{t}(k_{t-1}}_{t-2}(_{0},_{t- 2})+w_{t-1}_{t-1})+w_{t}_{t}\\ &=_{i=1}^{t}k_{i}_{0}+^{t-1}( (_{j=i+1}^{t}k_{j})w_{i})^{2}+w_{t}^{2}},  t,,_{t}(0, )\] (5)

Recall the reparametrization of the forward diffusion process: \(_{t}(_{0},)=(t)_{0}+(t)\), we can derive \((t)=_{i=1}^{t}k_{i}\) and \((t)=^{t-1}((_{j=i+1}^{t}k_{j})w _{i})^{2}+w_{t}^{2}}\). Thus, we can compute \(k_{t}\) and \(w_{t}\) numerically with \(k_{t}=^{t}k_{i}}{_{i=1}^{t}k_{i}}=(t)}{ (t-1)}\) and \(w_{t}=^{2}(t)-_{i=1}^{t-1}((_{j=i+1}^{t}k_{j} )w_{i})^{2}}\) respectively. With the numerical solutions \(k_{t}\) and \(w_{t}\), we can follow the similar derivation of DDPM  and compute the conditional reverse transition in Eq. (6) with \(a(t)=^{2}(t-1)}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\) and \(b(t)=(t-1)w_{t}^{2}}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\):

\[q(_{t-1}|_{t},_{0}):=(a(t)_{t}+b( t)_{0},s^{2}(t)), s(t)=(t)}}(t)\] (6)Finally, based on Eq. (6), we can follow the derivation of DDPM  and derive the denoising loss function in Eq. (7) to maximize the utility. We also denote \(_{t}(,)=(t)+(t) ,\;(0,)\).

\[L_{c}(,t,):=||-_{}(_{t}( ,),t)||^{2}\] (7)

On the other hand, we can also interpret Eq. (7) as a denoising score matching loss, which means the expectation of Eq. (7) is proportional to the score function, i.e., \(_{_{0},}[L_{c}(_{0},t,)] _{_{t}}[||(t)_{_{t}} q( _{t})+_{}(_{t},t)||^{2}]\). We further derive the backdoor reverse transition as follows.

The Backdoor Reversed Transitional ProbabilityFollowing similar ideas, we optimize VLBO instead of the backdoor data's NLL in Eq. (8) as

\[- p_{}(_{0}^{})=-_{q}[ p_{}( _{0}^{})]_{q}_{T}( _{T}^{},_{0}^{})+_{t=2}^{T}_{t}(_{t}^{},_{t-1}^{},_{0}^{})-_{0}( _{1}^{},_{0}^{})\] (8)

Denote the backdoor forward transition \(q(_{t}^{}|_{t-1}^{}):=(k_{t} _{t-1}^{}+h_{t},w_{t}^{2})\). With a similar parametrization trick, we can compute \(h_{t}\) as \(h_{t}=(t)-_{i=1}^{t-1}((_{j=i+1}^{t}k_{j})h _{i})\). Thus, the backdoor conditional reverse transition is \(q(_{t-1}^{}|_{t}^{},_{0}^{}): =(a(t)_{t}^{}+b(t)_{0}^{}+c(t) ,s^{2}(t))\) with \(c(t)=^{2}(t-1)-k_{t}h_{t}(t-1)}{k_{t}^{2} {}^{2}(t-1)+w_{t}^{2}}\).

### Generalization to ODE and SDE Samplers

In Section 3.3, we have derived a general form for both clean and backdoor reversed transitional probability \(q(_{t-1}|_{t},_{0})\) and \(q(_{t-1}^{}|_{t}^{},_{0}^{})\). Since DDPM uses \(q(_{t-1}|_{t},_{0})\) to approximate the intractable term \(q(_{t-1}|_{t})\), as we minimize the KL-divergence between the two reversed transitional probabilities \(q(_{t-1}|_{t},_{0})\) and \(p_{}(_{t-1}|_{t})\) in \(L_{t}(_{t},_{t-1},_{0})\), it actually forces the model with parameters \(\) to learn the joint probability \(q(_{0:T})\), which is the discrete trajectory of a stochastic process. As a result, we can convert the transitional probability into a stochastic differential equation and interpret the optimization process as a score-matching problem . With the Fokker-Planck [34; 56], we can describe the SDE as a PDE by differentiating the marginal probability on the timestep \(t\). We can further generalize our backdoor attack to various ODE samplers in a unified manner, including DPM-Solver [34; 35], DEIS , PNDM , etc.

Firstly, we can convert the backdoor reversed transition \(q(_{t-1}^{}|_{t}^{})\) into a SDE with the approximated transitional probability \(q(_{t-1}^{}|_{t}^{},_{0}^{})\). With reparametrization, \(_{t-1}^{}=a(t)_{t}^{}+c(t)+b(t) _{0}^{}+s(t)\) in Section 3.3 and \(_{t}^{}=(t)_{0}^{}+(t) +(t)_{t}\) in Section 3.2, we can present the backdoor reversed process \(q(_{t-1}^{}|_{t}^{})\) as a SDE with \(F(t)=a(t)+(t)}-1\) and \(H(t)=c(t)-(t)(t)}{(t)}\):

\[d_{t}^{}=[F(t)_{t}^{}-G^{2}(t)(t)_{_{t}^{}} q(_{t}^{})- (t)})}_{}]dt+G(t) (t)}d},\;G(t)=(t)}{ (t)}}\] (9)

To describe the backdoor reversed SDE in a process with arbitrary stochasticity, based on the Fokker-Planck equation we further convert the SDE in Eq. (9) into another SDE in Eq. (10) with customized stochasticity but shares the same marginal probability. We also introduce a parameter \(\{0,1\}\) that can control the randomness of the process. \(\) can also be determined by the samplers directly. The process Eq. (10) will reduce to an ODE when \(=0\). It will be an SDE when \(=1\).

\[d_{t}^{}=[F(t)_{t}^{}-G^{2}(t )(t)_{_{t}^{}} q(_ {t}^{})-(t)})}_{}]dt+G(t)(t)}d}\] (10)

When we compare it to the learned reversed process of SDE Eq. (11), we can see that the diffusion model \(_{}\) should learn the backdoor score function to generate the backdoor target distribution \(q(_{0}^{})\).

\[d_{t}=[F(t)_{t}-G^{2}(t)_{}( _{t},t)]dt+G(t)(t)}d}\] (11)

As a result, the backdoor score function will be the learning objective of the DM with \(_{}\). We note that one can further extend this framework to DDIM  and EDM , which have an additional hyperparameter to control the stochasticity of the generative process.

### Unified Loss Function for Unconditional Generation with Image Triggers

Following the aforementioned analysis, to achieve the high-specificity goal, we can formulate the loss function as \(_{_{0},_{i}^{}}[||(-(t)_{ _{i}^{}} q(_{i}^{})-(t)})-_{}(_{i}^{},t)||^{2}] _{_{0},_{0}^{},}[||-(t)}-_{}(_{i}^{}( _{0}^{},,t),)||^{2}]\) with reparametrization \(_{i}^{}(,,)=(t) +(t)+(t)\). Therefore, we can define the backdoor loss function as \(L_{p}(,t,,,,):=||-(t)}(,)-_{}( _{i}^{}(,(,), ),t)||^{2}\) where the parameter \(\) will be \(0\) when backdooring ODE samplers and \(1\) when backdooring SDE samplers. Define \((,)=+(1-) \). We derive the unified loss function for unconditional DMs in Eq.12. We can also show that BadDiffusion  is just a special case of it with proper settings.

\[L_{}^{I}(_{c},_{p},,t,,,, ):=_{c}L_{c}(,t,)+_{p}L_{p}(,t, ,,,)\] (12)

We summarize the training algorithm in Algorithm1. Note that every data point \(^{i}=\{^{i},_{c}^{i},_{p}^{i}\},\ ^{i} D\) in the training dataset \(D\) consists of three elements: (1) clean training image \(^{i}\), (2) clean loss weight \(_{c}^{i}\), and (3) backdoor loss weight \(_{p}^{i}\). The poison rate defined in BadDiffusion  can be interpreted as \(^{N}_{c}^{i}}{|D|},\ \ _{p}^{i},_{c}^{i} \{0,1\}\). We also denote the training dataset size as \(|D|=N\). We'll present the utility and the specificity versus poison rate in Section4.2 to show the efficiency and effectiveness of VillanDiffusion.

### Generalization to Conditional Generation

To backdoor a conditional generative DM, we can optimize the joint probability \(q(_{0},)\) with a condition \(\) instead of the marginal \(q(_{0})\). In real-world use cases, the condition \(\) / \(^{}\) can be the embedding of the clean / backdoored captions. The resulting generalized objective function becomes

\[_{}-(_{c} p_{}(_{0},)+_{p}  p_{}(_{0}^{},^{}))\] (13)

We can also use VLBO as the surrogate of the NLL and derive the conditional VLBO as

\[- p_{}(_{0},)_{q}_{T}^{C}(_{T},_{0},)+_{t=2}^{T} _{t}^{C}(_{t},_{t-1},_{0},) -_{0}^{C}(_{1},_{0},)\] (14)

Denote \(_{T}^{C}(_{T},_{0},)=D_{}(q( _{T}|_{0}) p_{}(_{T},))\), \(_{0}^{C}(_{1},_{0},)= p_{}( _{0}|_{1},)\), and \(_{t}^{C}(_{t},_{t-1},_{0},)= D_{}(q(_{t-1}|_{t},_{0})  p_{}(_{t-1}|_{t},))\). To compute \(_{t}^{C}(_{t},_{t-1},_{0},)\), we need to compute \(q(_{t-1}|_{t},_{0},)\) and \(p_{}(_{t-1}|_{t},)\) first. We assume that the data distribution \(q(_{0},)\) follows empirical distribution. Thus, using the same derivation as in Section3.3, we can obtain the clean data's loss function \(L_{c}^{C}(,t,,):=||-_{}( _{t}(,),t,)||^{2}\) and we can derive the caption-trigger backdoor loss function as

\[L_{}^{CC}(_{c},_{p},,,t,,^{ },):=_{c}L_{c}^{C}(,t,,)+_ {p}L_{c}^{C}(,t,,^{})\] (15)

As for the image-trigger backdoor, we can also derive the backdoor loss function \(L_{p}^{CI}(,t,,,,,):=|| -(t)}(,)- _{}(_{i}^{}(,(, ),),t,)||^{2}\) based on Section3.5. The image-trigger backdoor loss function can be expressed as

\[L_{}^{CI}(_{c},_{p},,,t,,, ,):=_{c}L_{c}^{C}(,t,,)+_ {p}L_{p}^{CI}(,t,,,,,)\] (16)

To wrap up this section, we summarize the backdoor training algorithms of the unconditional (image-as-trigger) and conditional (captiontion-as-trigger) DMs in Algorithm1 and Algorithm2. We denote the text encoder as \(\) and \(\) as concatenation. For a caption-image dataset \(D^{C}\), each data point \(^{i}\) consists of the clean image \(^{i}\), the clean/backdoor loss weight \(_{c}^{i}_{p}^{i}\), and the clean caption \(^{i}\).

``` Inputs: Backdoor Caption Trigger \(\), Backdoor Target \(\), Training dataset \(D^{C}\), Training parameters \(\), Text Encoder \(\) while not converge do \(\{,,_{c},_{p}\} D^{C}\) \(t((1,...,T))\) \((0,1)\)  Use gradient descent \(_{}L_{}^{I}(_{c},_{p},,t,,, ,)\) to update \(\) endwhile ```

**Algorithm 2** Backdoor Conditional DMs with Caption Trigger

## 4 Experiments

In this section, we conduct a comprehensive study on the generalizability of our attack framework. We use caption as the trigger to backdoor conditional DMs in Section 4.1. We take Stable Diffusion v1-4  as the pre-trained model and design various caption triggers and image targets shown in Fig. 2. We fine-tune Stable Diffusion on the two datasets Pokemon Caption  and CelebA-HQ-Dialog  with Low-Rank Adaptation (LoRA) .

We also study backdooring unconditional DMs in Section 4.2. We use images as triggers as shown in Table 1. We also consider three kinds of DMs, DDPM , LDM , and NCSN , to examine the effectiveness of our unified framework. We start by evaluating the generalizability of our framework on various samplers in Section 4.2 with the pre-trained model (_google/ddpm-cifar10-32_) released by Google HuggingFace organization on CIFAR10 dataset . In Section 4.2, we also attack the latent diffusion model  downloaded from Huggingface (_CompVis/ldm-celebahq-256_), which is pre-trained on CelebA-HQ . As for score-based models, we retrain the model by ourselves on the CIFAR10 dataset . Finally, we implement the inference-time clipping defense proposed in  and disclose its weakness in Section 4.3.

All experiments were conducted on s Tesla V100 GPU with 32 GB memory. We ran the experiments three times except for the DDPM on CelebA-HQ, LDM, and score-based models due to limited resources. We report the evaluation results on average across three runs. Detailed numerical results are given in Appendix. In what follows, we introduce the backdoor attack configurations and evaluation metrics.

    &  \\  Triggers &  &  & Target \\  Grey Box & Stop Sign & NoShift & Shift & Corner & Shoe & Hat & Eyeglasses & Cat \\   

Table 1: Experiment setups of image triggers and targets following . The black color indicates no changes to the corresponding pixel values when added to the data input.

Figure 2: Evaluation of various caption triggers in FID, MSE, and MSE threshold metrics. Every color in the legend of Fig. 1(b)/Fig. 1(e) corresponds to a caption trigger inside the quotation mark of the marker legend. The target images are shown in Fig. 1(d) and Fig. 1(h) for backdooring CelebA-HQ-Dialog and Pokemon Caption datasets, respectively. In Fig. 1(b) and Fig. 1(c), the dotted-triangle line indicates the MSE/MSE threshold of generated backdoor targets and the solid-circle line is the MSE/MSE threshold of generated clean samples. We can see the backdoor FID scores are slightly lower than the clean FID score (green dots marked with red boxes) in Fig. 1(a). In Fig. 1(b) and Fig. 1(c), as the caption similarity goes up, the clean sample and backdoor samples contain target images with similar likelihood.

**Backdoor Attack Configuration.** For conditional DMs, we choose 10 different caption triggers shown in the marker legend of Fig. 2 and Appendix. Note that due to the matplotlib's limitation, in the legend, {SOCCER} and {HOT_FACE} actually represent the symbols'and '. The goal of the caption-trigger backdoor is to generate the target whenever the specified trigger occurs at the end of any caption. As for unconditional DMs, in the CIFAR10 and CelebA-HQ datasets, we follow the same backdoor configuration as BadDiffusion , as specified in Table 1.

**Evaluation Metrics.** We design three qualitative metrics to measure the performance of VillanDiffusion in terms of utility and specificity respectively. For measuring utility, we use FID  score to evaluate the quality of generated clean samples. Lower scores mean better quality. For measuring specificity, we use Mean Square Error (MSE) and MSE threshold to measure the similarity between ground truth target images \(y\) and generated backdoor sample \(\), which is defined as \((y,)\). Lower MSE means better similarity to the target. Based on MSE, we also introduce another metric, called MSE threshold, to quantify the attack effectiveness, where the samples under a certain MSE threshold \(\) are marked as 1, otherwise as 0. Formally, the MSE threshold can be defined as \(((y,)<)\). A higher MSE threshold value means better attack success rates.

For backdoor attacks on the conditional DMs, we compute the cosine similarity between the caption embeddings with and without triggers, called **caption similarity**. Formally, we denote a caption with and without trigger as \(\) and \(\) respectively. With a text encoder **Encoder**, the caption similarity is defined as \((),()\).

### Caption-Trigger Backdoor Attacks on Text-to-Image DMs

We fine-tune the pre-trained stable diffusion model [44; 45] with the frozen text encoder and set learning rate 1e-4 for 50000 training steps. For the backdoor loss, we set \(_{p}^{i}=_{c}^{i}=1, i\) for the loss Eq. (15). We also set the LoRA  rank as 4 and the training batch size as 1. The dataset is split into 90% training and 10% testing. We compute the MSE and MSE threshold metrics on the testing dataset and randomly choose 3K captions from the whole dataset to compute the FID score for the Celeba-HQ-Dialog dataset . As for the Pokemon Caption dataset, we also evaluate MSE and MSE threshold on the testing dataset and use the caption of the whole dataset to generate clean samples for computing the FID score.

We present the results in Fig. 2. From Fig. 1(a) and Fig. 1(e), we can see the FID score of the backdoored DM on CelebA-HQ-Dialog is slightly better than the clean one, while the Pokemon Caption dataset does not, which has only 833 images. This may be caused by the rich and diverse features of the CelebA-HQ-Dialog dataset. In Fig. 1(b) and Fig. 1(f), the MSE curves get closer as the caption similarity becomes higher. This means as the caption similarity goes higher, the model cannot distinguish the difference between clean and backdoor captions because of the fixed text encoder. Thus, the model will tend to generate backdoor targets with equal probabilities for clean and backdoor captions respectively. The MSE threshold in Fig. 1(c) and Fig. 1(g) also explains this phenomenon.

We also provide visual samples in Fig. 3. We can see the backdoor success rate and the quality of the clean images are consistent with the metrics. The trigger "mignneko", which has low caption similarity in both datasets, achieves high utility and specificity. The trigger "anonymous", which has low caption similarity in CelebA-HQ-Dialog but high in Pokemon Caption, performs well in the

Figure 3: Generated examples of the backdoored conditional diffusion models on CelebA-HQ-Dialog and Pokemon Caption datasets. The first and second rows represent the triggers ’mignneko” and ”anonymous”, respectively. The first and third columns represent the clean samples. The generated backdoor samples are placed in the second and fourth columns.

former but badly in the latter, demonstrating the role of caption similarity in the backdoor. Please check the numerical results in Appendix E.5.

### Image-Trigger Backdoor Attacks on Unconditional DMs

**Backdoor Attacks with Various Samplers on CIFAR10.** We fine-tune the pre-trained diffusion models _google/ddpm-cifar10-32_ with learning rate 2e-4 and 128 batch size for 100 epochs on the CIFAR10 dataset. To accelerate the training, we use half-precision (float16) training. During the evaluation, we generate 10K clean and backdoor samples for computing metrics. We conduct the experiment on 7 different samplers with 9 different configurations, including DDIM , DEIS , DPM Solver , DPM Solver++ , Heun's method of EDM (algorithm 1 in ), PNDM , and UniPC . We report our results in Fig. 4. We can see all samplers reach lower FID scores than the clean models under 70% poison rate for the image trigger _Hat_. Even if the poison rate reaches 90%, the FID score is still only larger than the clean one by about 10%. As for the MSE, in Fig. 3(b), we can see about 10% poison rate is sufficient for a successful backdoor attack. We also illustrate more details in Appendix D.1. As for numerical results, please check Appendix E.1.

**Backdoor Attack on CelebA-HQ.** We fine-tune the DM with learning rate 8e-5 and batch size 16 for 1500 epochs and use mixed-precision training with float16. In Fig. 5, we show that we can achieve a successful backdoor attack with 20% poison rate while the FID scores increase about 25% \(\) 85%. Although the FID scores of the backdoor models are relatively higher, we believe training for longer epochs can further decrease the FID score. Please check Appendix D.2 for more information and Appendix E.2 for numerical results.

**Backdoor Attacks on Latent Diffusion and Score-Based Models.** Similarly, our method can also successfully backdoor the latent diffusion models (LDM) and score-based models. These results are the first in backdooring DMs. Due to the page limit, we present the detailed results in Appendix D.3 and Appendix D.4 and exact numbers in Appendix E.3 and Appendix E.4.

Figure 4: FID and MSE scores of various samplers and poison rates. Every color represents one sampler. Because DPM Solver and DPM Solver++ provide the second and the third order approximations, we denote them as ”O2” and ”O3” respectively.

Figure 5: Backdoor DDPM on CelebA-HQ.

Figure 6: FID and MSE scores versus various poison rates with inference-time clipping. We use (Stop Sign, Hat) as (trigger, target) in Fig. 5(a) and Fig. 5(b) and (Stop Sign, Corner) in Fig. 5(c) and Fig. 5(d). “ANCESTRAL” means the original DDPM sampler . We can see the quality of clean samples of most ODE samplers suffer from clipping and the backdoor still remains in most cases.

### Evaluations on Inference-Time Clipping

According to , inference-time clipping that simply adds clip operation to each latent in the diffusion process is an effective defense in their considered setup (DDPM + Ancestral sampler). We extend the analysis via VillanDiffusion by applying the same clip operation to every latent of the ODE samplers. The clip range for all samplers is \([-1,1]\). We evaluate this method with our backdoored DMs trained on CIFAR10  using the same training configuration in Section 4.2 and present the results in Fig. 6. We find that only Ancestral sampler keeps stable FID scores in Fig. 5(a) and Fig. 5(c) (indicating high utility), while the FID scores of all the other samplers raise highly (indicating weakened defense due to low utility). The defense on these new setups beyond  shows little effect, as most samplers remain high specificity, reflected by the low MSE in Fig. 5(b) and Fig. 5(d). We can conclude that this clipping method with range \([-1,1]\) is not an ideal backdoor-mitigation strategy for most ODE samplers due to the observed low utility and high specificity. The detailed numbers are presented in Appendix D.7 and Appendix E.6.

## 5 Ablation Study

### Why BadDiffusion Fails in ODE Samplers

In Eq. (7), we can see that the objective of diffusion models is to learn the score function of the mapping from standard normal distribution \((0,)\) to the data distribution \(q(_{0})\). Similarly, we call the score function of the mapping from poisoned noise distribution \(q(_{T}^{})\) to the target distribution \(q(_{0}^{})\) as backdoor score function. By comparing the backdoor score function of SDE and ODE samplers, we can know that the failure of BadDiffusion is caused by the randomness \(\) of the samplers. According to Section 3.4, we can see that the backdoor score function would alter based on the randomness \(\). As a result, the backdoor score function for SDE is \(-(t)_{_{t}^{}} q(_{t}^{ })-(t)}\), which can be derived Eq. (10) with \(=1\). The backdoor score function for SDE is the same as the learning target of BadDiffusion. On the other hand, the score function for ODE is \(-(t)_{_{t}^{}} q(_{t}^{ })-(t)}\), which can be derived with \(=0\). Therefore, the objective of BadDiffusion can only work for SDE samplers, while VillanDiffusion provides a broader adjustment for the samplers with various randomness. Furthermore, we also conduct an experiment to verify our theory. We vary the hyperparameter \(\) indicating the randomness of DDIM sampler . The results are presented in the appendix. We can see that BadDiffusion performs badly when DDIM \(=1\) but works well as DDIM \(=0\). Please read Appendix D.5 for more experiment detail and Appendix E.8 for numerical results.

### Comparison between BadDiffusion and VillanDiffusion

To further demonstrate the limitation of BadDiffusion , we conduct an experiment to compare the attack performance between them across different ODE solvers. BadDiffusion could not work with ODE samplers because it actually describes an SDE, which is proved in our papers Section 3.4 theoretically. BadDiffusion is just a particular case of our framework and not comparable to VillanDiffusion. Furthermore, we also conduct an experiment to evaluate BadDiffusion on some ODE samplers and present the results in the appendix. Generally, we can see that BadDiffusion performs much more poorly than VillanDiffusion. Also, Eq. (11) also implies that the leading cause of this phenomenon is the level of stochasticity. Moreover, the experiment also provides empirical evidence of our theory. Please read Appendix D.6 for more experiment detail and Appendix E.9 for numerical results.

### VillanDiffusion on the Inpaint Tasks

Similar to , we also evaluate our method on the inpainting tasks with various samplers. We design 3 kinds of different corruptions: **Blur**, **Line**, and **Box**. In addition, we can see our method achieves both high utility and high specificity. Please check Appendix D.8 for more details and Appendix E.7 for detailed numerical results.

## 6 Conclusion

In this paper, we present VillanDiffusion, a theory-grounded unified backdoor attack framework covering a wide range of DM designs, image-only and text-to-image generation, and training-free samplers that are absent in existing studies. Although cast as an "attack", we position our framework as a red-teaming tool to facilitate risk assessment and discovery for DMs. Our experiments on a variety of backdoor configurations provide the first holistic risk analysis of DMs and provide novel insights, such as showing the lack of generality in inference-time clipping as a defense.

## 7 Limitations and Ethical Statements

Due to the limited page number, we will discuss further in Appendix C.

Acknowledgment

The completion of this research could not have been finished without the support of Ming-Yu Chung, Shao-Wei Chen, and Yu-Rong Zhang. Thank Ming-Yu for careful verification of the derivation and detailed explanation for some complex theories. We would also like to express our gratefulness for Shao-Wei Chen, who provides impressive solutions of complicated SDEs and numerical solution of Navier-Stoke thermodynamic model. Finally, we also appreciate Yu-Rong for his insights of textual backdoor on the stable diffusion.