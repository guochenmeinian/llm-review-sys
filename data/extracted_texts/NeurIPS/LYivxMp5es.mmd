# Towards Effective Planning Strategies for Dynamic Opinion Networks

Bharath Muppasani, Protik Nag, Vignesh Narayanan, Biplav Srivastava, and Michael N. Huhns

AI Institute and Department of Computer Science

University of South Carolina, USA

{bharath@email., pnag@email., vignar@, biplav.s@, huhns@}sc.edu

###### Abstract

In this study, we investigate the under-explored _intervention planning_ aimed at disseminating accurate information within _dynamic opinion networks_ by leveraging learning strategies. Intervention planning involves _identifying key nodes_ (search) and _exerting control_ (e.g., disseminating accurate/official information through the nodes) to mitigate the influence of misinformation. However, as the network size increases, the problem becomes computationally intractable. To address this, we first introduce a ranking algorithm to identify key nodes for disseminating accurate information, which facilitates the training of neural network (NN) classifiers that provide generalized solutions for the search and planning problems. Second, we mitigate the complexity of label generation--which becomes challenging as the network grows--by developing a reinforcement learning (RL)-based centralized dynamic planning framework. We analyze these NN-based planners for opinion networks governed by two dynamic propagation models. Each model incorporates both binary and continuous opinion and trust representations. Our experimental results demonstrate that the ranking algorithm-based classifiers provide plans that enhance infection rate control, especially with increased action budgets for small networks. Further, we observe that the reward strategies focusing on key metrics, such as the number of susceptible nodes and infection rates, outperform those prioritizing faster blocking strategies. Additionally, our findings reveal that graph convolutional network (GCN)-based planners facilitate scalable centralized plans that achieve lower infection rates (higher control) across various network configurations (e.g., Watts-Strogatz topology, varying action budgets, varying initial infected nodes, and varying degree of infected nodes).

## 1 Introduction

The spread of information across social networks profoundly impacts public opinion, collective behaviors, and societal outcomes . Especially during crises such as disease outbreaks or disasters, there is often too much information coming from different sources. Sometimes, the resultant flood of information is unreliable or misleading, or spreads too quickly, which can have serious effects on society and health . Online platforms such as Facebook, X, and WeChat, while essential for communication, significantly contribute to the rapid spread of misinformation . This has led to public confusion and panic in events ranging from the Fukushima disaster to the COVID-19 pandemic, and even the U.S. presidential elections , demonstrating the need for effective information management on these platforms .

The first step in combating misinformation propagation is to reliably detect them. However, detection alone is insufficient to effectively mitigate its spread. It must be complemented with strategicintervention planning_ to contain its impact. In this context, numerous studies have focused on rumor detection [53; 47; 40; 19], while comprehensive strategies for controlling misinformation are limited . Existing research works on controlling misinformation emphasizes three primary strategies: node removal [9; 45; 29], edge removal [21; 19; 44], and counter-rumor dissemination [5; 42; 12]. Node removal involves identifying and neutralizing key nodes using community detection methods, with dynamic models that adapt to changes in propagation. For example, [50; 16] present ranking algorithms that are critical for identifying influential nodes within complex networks, which can then be targeted to block, remove, or cascade information, to reduce the overall spread of misinformation. These algorithms rank nodes based on various metrics, determining their importance or influence within the network. The second approach, edge removal, focuses on disrupting misinformation pathways by strategically severing connections between nodes. For example, authors in  considered mitigating misinformation by identifying potential purveyors to block their posts. However, taking strong measures like censoring user posts may violate user rights.

The third strategy, counter-rumor dissemination, promotes the spread of factual information, leveraging user participation and 'positive cascades' to counteract misinformation . For instance, authors in  developed a method that involves learning an intervention model to enhance the spread of true news, thereby reducing the influence of fake news. The effectiveness of such intervention planning methods relies on the ability of intervention models to identify key nodes and disseminate accurate/official information through these nodes to mitigate the influence of misinformation. Following this strategy, in , a search problem was formulated and sequential plans using _automated planning_ techniques were generated for the targeted spread of information. Despite several model-based efforts to strengthen this approach (see Appendix A.1 for a review of relevant literature), the existing research often overlooks key features of opinion propagation models, such as their rich network dynamics, asynchronous communication, and the impact of factors like the degree of infected nodes, action budget, and various reward models on the effectiveness of planners.

We address this gap by studying the intervention planning problem using two learning methodologies. In both these methodologies, we investigate three distinct cases of opinion network models, ranging from discrete to continuous representations of opinion and mutual trust. Specifically, in this paper, we propose a novel ranking algorithm integrated with a supervised learning (SL) framework to identify influential nodes using a robust feature set of network nodes and evaluate their performance in all three cases. Additionally, we also develop a reinforcement learning (RL)-based solution to design centralized planners that are suitable for larger networks. Furthermore, we develop comprehensive datasets with a wide range of Watts-Strogatz (or small-world) network topologies, varying degrees of initial infected nodes, action budgets, and reward models, e.g., from those requiring local network information to those utilizing global real-time network states, and analyze both the developed methodologies. Next, we use an example to illustrate the intervention planning problem and highlight our contributions.

**Example:** Consider a research community discussing the NeurIPS submission deadline. In this context, a _topic_ is just a statement such as 'NeurIPS submission deadline is on May 22'. An _opinion_ of an agent on this topic is defined as the belief of the agent in the truthfulness of the statement . A positive (respectively negative) opinion value represents that the agent believes the statement is true (respectively false). In our study, we consider the opinion value of an agent on a topic lies in \([-1,1]\). Our problem setup consists of a network of connected agents (with opinion values on a given

Figure 1: Example of misinformation propagation and control choices at each timestep. Blue nodes: neutral (opinion value \(0\)), red nodes: misinformed (opinion value \(-1\)), green nodes: received accurate information (opinion value \(1\)).

topic) represented by a graph. We consider that a subset of agents propagate misinformation to their neighbors. Our goal is to counteract this by disseminating accurate information to selected agents at each time step. The agents receiving accurate information update their opinion to a level where they no longer believe the misinformation and, consequently, cease to propagate it. The process ends when no agents are left to receive the misinformation. Figure 1 illustrates such a scenario where the red nodes represent the agents not believing about the NeurIPS deadline being May 22. When they interact with other agents, they share this misinformation, leading to a spread of incorrect information throughout the network. The blue nodes represent neutral agents who are unaware of the deadline, and the green nodes represent agents who believe in the NeurIPS deadline being on May 22. Initially, agents, i.e., nodes \(\{0,1\}\), propagate misinformation to their neighbors. To control the spread of this misinformation, timely control actions are taken at each timestep to disseminate accurate information to selected agents. In the example, with an action budget of \(1\), agents \(7\), \(8\), and \(6\) are sequentially chosen to receive the accurate information, represented by the green nodes. Through these control actions, the example demonstrates how timely intervention at critical points can effectively mitigate misinformation spread, ensuring agents are accurately informed.

Our paper makes several significant **contributions** to intervention planning, focusing on the integration of more realistic and complex modeling approaches, label generation techniques, and training methodologies: (a) We utilize a continuous opinion scale to model the dynamics of misinformation spread (instead of just binary scale as shown in the example), providing a more realistic representation of opinion changes over time. (b) We develop a ranking algorithm for generating labels in networks with discrete opinions, addressing a significant gap in efficient data preparation for SL algorithms in this domain. (c) We develop an RL methodology and perform comprehensive analysis. This allows for adaptive intervention strategies in response to evolving misinformation spread patterns, a critical improvement over traditional static approaches. (d) Utilizing graph convolutional networks (GCNs) with an enhanced feature set of opinion value, connectivity degree, and proximity to a misinformed node, we improve the training of models for selecting effective intervention strategies. This enhancement ensures scalability and generalizes well across various network structures, demonstrating the robust capabilities of GCNs in complex scenarios. Table 3 highlights our contributions by providing a comparative overview of the key features and innovations in our work to that of previous studies. The code and the datasets developed as part of the analysis presented in this paper can be found in .

## 2 Problem Formulation

In this section, we discuss our approach to modeling the opinion network environment, the dynamics of information propagation, and our strategy for containing misinformation.

### Environment Description

An opinion network is formally represented as a directed graph \(G=(V,E)\), where \(V\) denotes the set of nodes (agents), and \(E\) denotes the set of edges (relationships or trust) between agents . The graph structure we consider for our study is undirected, indicating that relationships between agents are bi-directional. Each node within the graph represents an individual agent, and each agent holds a specific opinion on a given topic. An edge between any two nodes signifies a direct connection or relationship between those agents, facilitating the exchange of opinions.

Opinion values are quantified within the range \([-1,1]\), representing different levels of belief on a given topic, and the weight assigned to each edge quantifies the mutual trust level between connected agents, scaled within the interval \(\). While existing works in the literature have explored only binary opinion and trust models, in computational social science, researchers have developed models with opinion and trust values as continuous variables. Investigation of planning strategies in continuous models remains under-explored. In this paper, we explore three distinct cases of opinion and trust values. Case-1 involves binary opinion values with binary trust, simplifying the network dynamics into discrete states. In Case-2, we use floating-point opinion values while maintaining binary trust, allowing for a more granular assessment of opinions while still simplifying trust dynamics. Finally, Case-3 features both floating-point opinion values and floating-point trust, representing more realistic opinion and trust relationships within the network capturing continuous variations.

### Propagation Model

In the analysis of opinion networks, it is essential to understand how opinions form and evolve, guided by the dynamics of trust among agents. In our analysis, the evolution and propagation of opinions within opinion networks are modeled using a linear adjustment mechanism (_discrete linear maps_), as described by the following transition function

\[x_{i}(t+1)=x_{i}(t)+_{ik}(x_{k}(t)-x_{i}(t)), t=0,1,.\] (1)

Equation 1 models the dynamics of opinion evolution, where the opinion of agent \(i\) at time \(t+1\) depends on its current opinion and the influence exerted by a connected agent \(k\), who is actively sharing some information with agent \(i\), moderated by the trust factor \(_{ik}\). This asynchronous propagation model adapts differently across various experimental setups as detailed below.

In Case-1 and Case-2, where mutual trust values are discrete \(\{0,1\}\), the application of Equation 1 results in immediate shifts in opinion. For example, if an agent \(i\) with a current opinion value of 0.5 on some topic is influenced by a connected neighboring agent \(k\) with an opinion value of -1 on the same topic, agent \(i\)'s opinion immediately shifts to -1 in the next timestep, reflecting a discrete transition. Conversely, in Case-3, which involves a continuous range of opinion and trust values, changes are more gradual. Here, if agent \(i\) holds an opinion of 0.5 and is influenced by a neighbor \(k\) with an opinion of -1 and a moderate trust factor, the opinion of agent \(i\) incrementally moves closer to -1 in subsequent timesteps. This reflects a gradual shift towards a consensus opinion, depending on the magnitude of the trust level between agents \(i\) and \(k\).

To further enhance our understanding of opinion dynamics in networks with continuous trust relationships, we have also used the DeGroot propagation model  in Case-3. The propagation of opinions in this model is governed by the following equation:

\[x_{i}(t+1)=_{k=1}^{n}_{ik}x_{k}(t), t=0,1,.\] (2)

Equation 2 describes the opinion of agent \(i\) at time \(t+1\) as a weighted average of the opinions of all the neighboring agents at time \(t\), where the weights \(_{ik}\) represent the trust agent \(i\) has in agent \(k\). Often, in the DeGroot model, which is a synchronous propagation model, the summation in 2 is a _convex sum_, i.e., the trust values add to one so that we have \(_{k=1}^{n}_{ik}=1\) for each \(i=1,,n\). This normalization allows the DeGroot model to exhibit stable asymptotic behaviour.

At each timestep, the following processes occur: Nodes with opinion values lower than -0.95 are identified as sources of misinformation and transmit the misinformation to their immediate neighbors (referred to as 'candidate nodes') according to one of the propagation model detailed in Equations 1 and 2. Concurrently, an intervention strategy is applied where a subset of these neighbors--constrained by _an action budget_--is selected to receive credible information from a trusted source. This source is characterized by an opinion value of 1 and we vary trust parameter among \(1\), \(0.8\), and \(0.75\). The process includes a blocking mechanism where a node that exceeds a positive opinion threshold of 0.95 is considered 'blocked', ceasing to interact with the misinformation spread or disseminate positive influence further. The simulation concludes when there are no viable 'candidate nodes' left to propagate misinformation. **Our primary objective** is to devise a learning mechanism that efficiently identifies and selects key nodes within the network to disseminate accurate information at each time step.

## 3 Methods

In this section, we will explain our methodologies, presenting an overview of the network architectures employed, including the GCN and ResNet frameworks. Additional details about the neural network architecture utilized for our experiments can be found in Appendix A.2. We detail our proposed ranking algorithm utilized in the SL process. Additionally, we elaborate on the implementation of the Deep Value Network (DVN) with experience replay for the proposed RL-based planners. Furthermore, we provide an explanation of the various reward functions analyzed in our RL setup.

### Ranking Algorithm-based Supervised Learning

In this section, we propose a ranking algorithm based SL model to classify the key nodes at each time step to disseminate accurate information. Our SL method utilizes a GCN architecture.

**Ranking Algorithm:** We pose the ranking algorithm as a search problem where the objective is to find the optimal set of nodes that, when blocked, minimizes the overall infection rate. The network is represented as a graph \(G\), where nodes can be infected, blocked, or possess opinion values within the range \([-0.95,0.95]\). Initially, a simulation network \(S\) is created by setting the opinion values of infected nodes to \(-1\) and removing blocked nodes. Let \(M\) denote the number of nodes in \(S\) that are neither infected nor blocked. Given an action budget \(K\), we select \(K\) nodes from \(M\) in \(\) possible ways, forming the set \(C\) of all possible combinations. For each subset \(c C\), we temporarily block the nodes in \(c\) by setting their opinion values to \(1\) and simulate the spread of misinformation within \(S\). The resulting infection rates for each subset \(c\) are stored in the set \(R\). We identify the subset \(c^{*} C\) that yields the minimal infection rate, denoted as \(r^{*}\). This subset \(c^{*}\) is our target set. We then construct a target matrix \(T^{N 1}\), where \(N\) is the total number of nodes in the original network \(G\). All entries of \(T\) are initialized to \(0\), and for each node \(i c^{*}\), the \(i\)-th entry of \(T\) is set to \(1\). This target matrix \(T\) is subsequently used to train the GCN-based model. A pseudocode for this ranking algorithm is presented in Algorithm 1 in Appendix A.5.

**Overall Training Procedure:** The training of our GCN-based model leverages the labels defined in the target matrix \(T^{N 1}\). This matrix is compared with the model's output matrix \(O^{N 1}\), which estimates the blocking probability of each node. We evaluate training efficacy using the binary cross-entropy loss between \(T\) and \(O\), which quantifies prediction errors. Model weight adjustments are implemented via standard backpropagation  based on this loss.

Each training iteration consists of several episodes, starting with the generation of a random graph state \(G\) containing initially infected nodes. The GCN then processes this state to output matrix \(O\) using the graph's features and structure. Labels are generated, as detailed above using the ranking algorithm, generating the target matrix \(T\). The binary cross-entropy loss between \(O\) and \(T\) is calculated for backpropagation. The environment updates by blocking predicted nodes, allowing the infection spread, and adjusting node attributes. The process repeats until misinformation spread is halted, with each episode refining the graph's state for subsequent iterations. The results of the planners for difference cases are summarized in the Appendix A.6.1.

However, we note that, in the case of continuous opinion and continuous trust (case-3), the process of label generation becomes more complex. In such scenarios, agents do not change their opinion immediately but gradually, making it difficult to predict which agents will be misinformed based on a single propagation simulation. Therefore, simulations across multiple time steps are necessary to identify the optimal nodes to block. As the ranking algorithm uses a brute force method to determine optimal nodes, this approach becomes increasingly challenging with continuous opinion models.

### Reinforcement Learning-based Centralized Dynamic Planners

In SL, the process of generating labels can be costly and impractical as network size increases. This is evident while considering mitigating misinformation propagation in large networks, where identifying the optimal set of nodes for blocking requires a combinatorial search that is computationally infeasible. Thus, RL emerges as a viable alternative.

Deep \(Q\)-networks (DQNs)  using random exploration combined with experience replay have been demonstrated to effectively learn \(Q\)-values for sequential decision making with high-dimensional data. Unlike the classical DQN, where the network outputs a \(Q\)-value corresponding to each possible action, in our problem, which also deals with high-dimensional data, we develop a DVN, as the number of available actions at each time step need not be fixed. Consequently, the output layer consists of a single neuron that outputs the value for a given input state. The agent's experiences at each time step are stored in a replay memory buffer for the neural network parameter updates. The loss function for training is given by

\[(s_{t},s_{t+1}|)=(r_{t}+_{^{-}}(s_{t+1})-V_{ }(s_{t})),\] (3)

where \(t\) represents the current time step, \(s_{t}\) is the current state, \(s_{t+1}\) denotes the subsequent state after action \(a_{t}\) is taken, and \(r_{t}\) is the reward received for taking \(a_{t}\) in \(s_{t}\). The parameters \(\) denotethe weights of the value network used to estimate the state value \(V_{}(s_{t})\), while \(^{-}\) represents the parameters of a target network, typically a lagged copy of \(\), used to stabilize training. Here, \(_{^{-}}(s_{t+1})\) is the estimated value of the next state \(s_{t+1}\) according to the target network. The specific reward functions used in this study are discussed later in the section. Algorithm 2, in Appendix A.5, provides a detailed implementation of our DVN with experience replay.

#### 3.2.1 Reward Functions for RL setup

The reward function is designed to encourage policies that effectively mitigate the spread of misinformation. Specifically, the reward functions modeled for our study are: (1) \(R_{0}=-()\), where \(\) is defined as the change in infection rate resulting from taking action \(a_{t}\). Specifically, \(=s_{t+1}-s_{t}\). This reward structure encourages the model to reduce the rate at which misinformation spreads by penalizing increases in the infection rate; (2) \(R_{1}=-()\), targets the number of immediate neighbors of infected nodes that are susceptible to becoming infected in the next timestep, thereby promoting strategies that minimize the potential for misinformation to spread; (3) \(R_{2}=-()-()\), takes into account the previous two rewards, balancing the need to control both the number of susceptible nodes and the overall infection rate; (4) \(R_{3}=1-(}{})\), rewards quicker resolutions, providing higher rewards for strategies that contain misinformation rapidly and evaluating the effectiveness only at the end of each episode; (5) \(R_{4}=-()\), directly penalizes the current infection rate, thus favoring actions that achieve lower overall infection rates; and finally, a combined reward that incorporates elements of both \(R_{3}\) and \(R_{1}\). Throughout the simulation, the agent continually receives rewards based on the number of candidate nodes, fostering strategies that limit the expansion of the infection network. As the simulation concludes, the agent receives an episodic reward calculated as (6) \(R_{5}=-()-}{}\), thereby reinforcing the importance of quick and efficient resolution of misinformation spread. Note that all these reward structures, in addition to differing in how they represent the goal for the planners, also differ in the network information required to compute them.

### Network Architectures

In our experiments, we utilized two neural network architectures. First, a GCN to model node features within a network. Each node was characterized by three key features. These features were represented in a matrix \(F^{N 3}\), where \(N\) denotes the total number of nodes. The feature matrix is dynamic and evolves to reflect changes in the network. It includes the opinion value, the connectivity degree, which identifies nodes potentially susceptible to misinformation while excluding those already blocked or misinformed, and the proximity to a misinformed node, which is calculated as the shortest path to the nearest infected node, assigning a distance of infinity to unreachable nodes.

We have also considered using Residual Network (ResNet) Architecture. The ResNet model implemented in our study is a variant of the conventional ResNet architecture . The core component of our ResNet model is the ResidualBlock, which allows for the training of deeper networks by addressing the vanishing gradient problem through skip connections. Each ResidualBlock consists of two sequences of convolutional layers (Conv2d), batch normalization (BatchNorm2d), and sigmoid activations. Complete details about the model architectures used in our study are provided in Appendix A.2.

## 4 Experiments

In this section, we present the details about training data generation and configurations chosen for our SL and RL methodologies. We also explain the test data used for evaluating the trained models.

### Training Setup

In the SL setup, we experimented with three distinct graph structures: Watts-Strogatz (with nearest neighbors \(k=3\) and a rewiring probability \(p=0.4\)), Erdos-Renyi (with branching factor of 4), and Tree graphs (with branching factors randomly selected from the range \(\)). Each graph type facilitated training models to evaluate the influence of various structural dynamics on performance.

We used the GCN model for the SL method. Due to the consistent performance of the trained models on the different graph topologies, we chose the small-world topology to present all the subsequent analyses and summarize the results in Appendix A.6.1. On the other hand, we trained centralized RL planners using both ResNet and GCN network architectures. Each trained configuration is represented as _model_-\(n\)-\(x\)-\(y\), where \(model\{\)ResNet, GCN\(\}\), \(n\{\)10,25,50\(\}\) represents the network length (in terms of the number of nodes), \(x\{\)1,2,3\(\}\) represents the number of initial infected nodes, and \(y\{\)1,2,3\(\}\) represents the action budget.

### Test Data Generation

The datasets used in related works, such as , typically consist of network structures, and no real-time opinion propagation data could be found. Therefore, to evaluate our intervention strategies, we generated two sets of synthetic datasets using the Watts-Strogatz model with the training dataset's configurations. This approach allows us to simulate complex networks and control the structure, connectivity, and initial infected nodes to assess our models effectively.

**Dataset v1** examines the effects of network size and the initial count of infected nodes on misinformation spread. We generated data with network sizes of \(10\), \(25\), and \(50\) nodes with \(1\), \(2\), and \(3\) initially infected nodes, respectively, creating \(9\) unique datasets. Each configuration has \(1000\) random network states with the opinion values of non-infected nodes uniformly distributed between \(-0.5\) and \(0.6\).

**Dataset v2** examines how the initial connections of infected nodes affect the spread of misinformation. Like Dataset v1, it includes networks of \(10\), \(25\), and \(50\) nodes. However, the initial number of connections (degrees of connectivity) for the infected nodes varies from \(1\) to \(4\). Here by degree of connectivity, we mean the number of candidate nodes present at the start of the simulation. This variation results in a total of \(12\) datasets for each configuration, with each dataset containing \(1000\) states. In these configurations, the number of initially infected nodes is randomly chosen from \(1\) to \(3\).

## 5 Results and Discussion

In this section, we evaluate the models, using the infection rate metric, trained using our ranking-based SL and RL algorithms with various reward functions. We discuss the efficiency of these models using Dataset v2, particularly on a network of 50 nodes with a connectivity degree of 4, as it represents the most complex test dataset we generated. Similar evaluation results for other datasets can be found in the Appendix A.6. Additionally, we have also evaluated our planning algorithms using directed and undirected real-world network models reported in the literature. These evaluations are presented in Table 2. The details of the hardware used for our experiments are provided in the Appendix A.4. Our empirical investigation yielded insightful results regarding the performance of our trained models under various training conditions. With comprehensive experimental evaluations, we were able to address the following research questions.

**Objective and Research Questions: O**: Identify the optimal combination of initially infected nodes and action budget parameters for training models to effectively control the spread of misinformation. **RQ1**: For reward functions that focus on the blocking time, does adding any other factor lead to better results? If yes, which factor? **RQ2**: Do reward functions that look at global graph information perform better than those considering local, neighboring information? **RQ3**: Does GCN offer better scalability and performance when compared with ResNet.

_O: What is the best combination of initially infected nodes and action budget parameters for training the models to control the misinformation spread?_

To examine this, we focused our analysis on the Mean Squared Error (MSE) loss plots obtained during the training phase. Figure 7 in Appendix A.6 illustrates the comparison of training loss across various network parameter settings for all considered reward types in Case-1, employing a ResNet model trained on a network of 50 nodes. The trend in loss convergence across episodes was found to be consistent for both the ResNet and GCN models across all cases examined. The analysis revealed that reward functions exhibiting lower and more stable loss values correlate with improved model learning performance. Our findings highlight that increasing the number of initially infected nodes typically elevates the stabilization point of MSE loss, indicating a more challenging learning environment. Additionally, a higher action budget contributes to increased MSE variability, reflecting the added complexity and generally poorer performance during training. Based on this analysis, we find ResNet-\(n\)-\(1\)-\(1\)-\(1\) and GCN-\(n\)-\(1\)-\(1\), \(n\) {10,25,50}, to be the best training configurations.

Table 1 presents the average infection rate values across different cases considered for Dataset v2 with a degree of connectivity 4, featuring a network of 50 nodes, detailing the average infection rates. It compares the performance of the ResNet model, trained on a network of 50 nodes, with the GCN model, trained on a network of 10 nodes, using the RL training algorithm across the different reward types, and the GCN model trained using SL on a network of 25 nodes. Results on the additional datasets are provided in Appendix A.6.

_RQ1: For reward functions that focus on blocking time, does adding any other factor lead to better result? If yes, which factor?_ Answer: Yes. #candidate nodes.

Reward function \(R_{3}\), which is formulated to minimize the number of time steps required to halt the spread of misinformation, might inadvertently not be the most effective strategy for minimizing the overall infection rate within the network. As the reward is solely based on the speed of response, it does not directly account for the magnitude of the misinformation spread, that is, the number of nodes affected. Therefore, the agent may prioritize actions that conclude the propagation swiftly but do not necessarily result in the most substantial reduction in the spread of misinformation. However, our results indicate that under specific training configurations with an action budget or initial infected nodes greater than \(1\), the reward function \(R_{3}\) outperforms others in maintaining lower infection rates, as shown in Figure 8 in Appendix A.6. This finding is significant since \(R_{3}\), a sparser reward type, requires less computational effort and is independent of network observability. As the action budget increases the propagation tends to conclude in fewer timesteps thereby resulting in the RL agent receiving a higher reward in the case of \(R_{3}\). Figure 2 shows that the RL agent trained with the \(R_{3}\) reward function chooses actions that conclude propagation in the least time. Conversely, Figure 1 illustrates the sequence of actions chosen by an RL agent trained with the \(R_{1}\) reward function on the same network. Although \(R_{1}\) requires more time steps than \(R_{3}\), it results in a lower infection rate. This can also be observed from Table 1, where the infection rate is higher for the \(R_{3}\) reward function than

  
**A.** & **M.** &  &  &  \\   & & **ResNet(50)** & **GCN(10)** & **ResNet(50)** & **GCN(10)** & **ResNet(50)** & **GCN(10)** \\   & RL+\(R_{0}\) & 0.2334 & 0.2481 & 0.2496 & 0.2454 & 0.0449 & 0.0461 \\  & RL+\(R_{1}\) & 0.1917 & 0.1608 & 0.1965 & 0.1607 & 0.0449 & **0.0435** \\
1 & RL+\(R_{2}\) & 0.2427 & 0.1608 & 0.2148 & 0.1608 & 0.0451 & 0.0438 \\  & RL+\(R_{3}\) & 0.2331 & 0.2958 & 0.2281 & 0.3381 & 0.0444 & 0.046 \\  & RL+\(R_{4}\) & 0.199 & **0.1593** & 0.2513 & **0.1596** & 0.045 & 0.0442 \\  & RL+\(R_{5}\) & - & 0.1607 & - & 0.1605 & - & 0.0439 \\   & SL+GCN(25) &  &  &  \\   & RL+\(R_{0}\) & 0.0974 & 0.1012 & 0.0992 & 0.1007 & **0.0398** & 0.04 \\  & RL+\(R_{1}\) & 0.0886 & **0.0842** & 0.0901 & 0.0843 & **0.0398** & **0.0398** \\
2 & RL+\(R_{2}\) & 0.097 & **0.0842** & 0.0957 & 0.0843 & 0.0399 & **0.0398** \\  & RL+\(R_{3}\) & 0.0959 & 0.0969 & 0.0962 & 0.1032 & **0.0398** & 0.04 \\  & RL+\(R_{4}\) & 0.0898 & **0.0842** & 0.1005 & **0.0842** & 0.0399 & **0.0398** \\  & RL+\(R_{5}\) & - & **0.0842** & - & **0.0842** & - & **0.0398** \\   & SL+GCN(25) &  &  &  \\   & RL+\(R_{0}\) & 0.0599 & 0.0599 & 0.0599 & 0.06 & **0.0397** & 0.0399 \\  & RL+\(R_{1}\) & 0.0599 & **0.0597** & 0.0598 & **0.0597** & 0.0398 & **0.0397** \\
3 & RL+\(R_{2}\) & 0.0598 & **0.0597** & 0.0599 & **0.0597** & 0.0398 & **0.0397** \\  & RL+\(R_{3}\) & 0.06 & 0.0598 & 0.0601 & 0.0602 & **0.0397** & 0.0399 \\  & RL+\(R_{4}\) & 0.0598 & **0.0597** & 0.06 & **0.0597** & 0.0398 & 0.

any other reward function. In order to effectively implement this we have considered combining this episodic reward along with \(R_{1}\), resulting in reward type \(R_{5}\). This has shown a significant performance improvement when compared to the original version.

_RQ2: Do reward functions that look at global graph information perform better than those considering local, neighboring information?_ Answer: Yes

Analysis of the inference outcomes using Dataset v2, as presented in Table 1, shows that single factor reward functions, specifically \(R_{1}=-()\) and \(R_{4}=-()\), consistently resulted in lower infection rates across various settings compared to their more complex counterparts with \(R_{4}\) providing relatively better results compared to \(R_{1}\). This trend was observed in both ResNet and GCN models. From a practical standpoint, \(R_{1}\) can be particularly advantageous because it does not require complete observability of the network, but just the immediate neighbors of infected nodes. Conversely, to compute \(R_{4}\), which reflects the infection rate of the network, complete understanding of the state of each node within the network is required. This requirement for total network observability could limit the practicality of \(R_{4}\) in situations where such detailed information is unavailable or difficult to gather.

  
**A.** & **M.** & **Zachary’s Karate** & **Facebook [Undirected]** & **Facebook ** & **Email [Directed]** & **Cora [Undirected] ** \\   & V: 34, E: 78 & V: 250, E: 1352 & V: 300, E: 2358 & V: 2000, E: 2911 \\  & & Avg. Deg.: 4.59 & Avg. Deg.: 10.8 & Avg. Deg.: 7.9 & Avg. Deg.: 2.9 \\   & RL+\(R_{0}\) & 0.8579 & 0.4569 & 0.395 & 0.0603 \\  & RL+\(R_{1}\) & **0.5279** & **0.4547** & **0.315** & **0.0095** \\  & RL+\(R_{2}\) & **0.5279** & **0.4547** & 0.3169 & **0.0095** \\  & RL+\(R_{3}\) & 0.8468 & 0.511 & 0.3759 & 0.0609 \\  & RL+\(R_{4}\) & 0.5326 & 0.4589 & 0.3183 & 0.0096 \\  & RL+\(R_{5}\) & 0.5276 & 0.4547 & 0.316 & 0.0096 \\   & RL+\(R_{0}\) & 0.2762 & 0.3621 & 0.2512 & 0.0166 \\  & RL+\(R_{1}\) & 0.1641 & **0.3022** & 0.0988 & **0.002** \\  & RL+\(R_{2}\) & 0.1641 & 0.3024 & 0.1062 & **0.002** \\  & RL+\(R_{3}\) & 0.2562 & 0.3697 & 0.1838 & 0.0142 \\  & RL+\(R_{4}\) & **0.1582** & 0.3047 & **0.0966** & **0.002** \\  & RL+\(R_{5}\) & 0.1641 & 0.3022 & 0.0988 & **0.002** \\   & RL+\(R_{0}\) & 0.0926 & 0.233 & 0.103 & 0.0126 \\  & RL+\(R_{1}\) & 0.0697 & 0.1649 & **0.0347** & **0.0017** \\   & RL+\(R_{2}\) & **0.0662** & 0.1647 & **0.0347** & **0.0017** \\   & RL+\(R_{3}\) & 0.085 & 0.2039 & 0.0587 & 0.0052 \\   & RL+\(R_{4}\) & **0.0662** & **0.1633** & 0.0351 & **0.0017** \\   & RL+\(R_{5}\) & **0.0662** & 0.164 & **0.0347** & **0.0017** \\   

Table 2: Average infection rate values from experiments conducted on 100 random instantiations for each real-world network, each starting with 1 random initially infected node, obtained using GCN model trained on synthetic networks of 10 nodes, under various reward structures (M.) tested with varying action budgets (A.). The network properties of # nodes (V), # edges (E), and Average Degree for each network are shown in the table.

Figure 2: Sequence of actions chosen by the RL agent trained using reward function \(R_{3}\).

#### RQ3: Does GCN offer better scalability and performance when compared with ResNet?

Answer: Yes GCNs are hypothesized to outperform traditional convolution-based architectures like ResNet in tasks involving graph data due to their ability to naturally process the structural information of networks and their enhanced ability to represent complex feature sets . This study compares the scalability and performance of a GCN, which excels in node classification within graphs, to a ResNet model that, despite its success in image recognition, may not scale as effectively to larger graph structures beyond the size it was initially trained on. Referring to Table 1, the GCN model, trained on only 10 node networks, consistently exhibits lower average infection rates across all the cases and under varying action budgets, when compared with the ResNet model trained on 50 node networks. The ability of GCN to maintain lower infection rates even as network complexity increases underscores its robustness and scalability in more complex network scenarios. This performance contrast highlights the suitability of GCN architectures for graph-based tasks.

## 6 Conclusions

This paper investigates scalable and innovative intervention strategies for containing the spread of misinformation within dynamic opinion networks. Our significant contributions include analysis using continuous opinion models, a design of ranking algorithm for identifying key nodes to facilitate SL-based classifiers, and the utilization of GCNs to optimize intervention strategies. Additionally, we design and study various reward functions for reinforcement learning, enhancing our approach to misinformation mitigation.

Despite significant progress, our work has limitations. In the field of computational social science, often more complex agent models are being investigated. While we have made significant efforts to extend the understanding of planning strategies, especially in continuous opinion networks, exploring complex agent traits such as stubbornness and the representation of directed trust, and implementing topic-dependency in a multi-topic network along with distributed planners instead of centralized planners as in our work is a compelling future direction.

Broader Societal Impact: This work provides methods that can be used to exert control on information spread. When used responsibly by authorized information providers, which the authors support, it will help reduce prevalent _infodemics_ in social media. But it may also be misused by an adversary to wear control from an authorized party (e.g., information owner) and counter efforts to tackle misinformation. Overall, the authors believe more research efforts are needed to understand opinion networks and information dissemination strategies in dynamic and uncertain environments in pursuit of long-term societal benefits.

 p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt}}  
**Implications** & **Features of Our work** & **Our Work** & **Previous Work** & **Citations** \\  Action-Space Invariant Expressive Models & Deep Value Network Dynamics & ✓ & ✗ & DQN  \\ Realistic Communication Dynamics & Asynchronous Communication & ✓ & ✗ &  \\ Wider Applications & Reward Models & 5 variants studies & Typically 1 &  \\   

Table 3: This table outlines the unique attributes of our approach, including the use of a deep value network, network dynamicity across multiple cases, asynchronous communication, and the exploration of five different reward models.