# Towards Scalable and Stable Parallelization of Nonlinear RNNs

Xavier Gonzalez1,2, Andrew Warrington1,2,3, Jimmy T.H. Smith2,4,5, Scott W. Linderman1,2

1Department of Statistics, Stanford University.

2Wu Tsai Neurosciences Institute, Stanford University.

3GE Healthcare. 4ICME, Stanford University. 5Liquid AI.

{xavier18,scott.linderman}@stanford.edu

###### Abstract

Transformers and linear state space models can be evaluated in parallel on modern hardware, but evaluating nonlinear RNNs appears to be an inherently sequential problem. Recently, however, Lim et al.  developed an approach called DEER, which evaluates nonlinear RNNs in parallel by posing the states as the solution to a fixed-point problem. They derived a parallel form of Newton's method to solve the fixed-point problem and achieved significant speedups over sequential evaluation. However, the computational complexity of DEER is cubic in the state size, and the algorithm can suffer from numerical instability. We address these limitations with two novel contributions. To reduce the computational complexity, we apply quasi-Newton approximations and show they converge comparably to Newton, use less memory, and are faster. To stabilize DEER, we leverage a connection between the Levenberg-Marquardt algorithm and Kalman smoothing, which we call ELK. This connection allows us to stabilize Newton's method while using efficient parallelized Kalman smoothing algorithms to retain performance. Through several experiments, we show that these innovations allow for parallel evaluation of nonlinear RNNs at larger scales and with greater stability.

## 1 Introduction

Parallel computation has helped fuel the rise of deep learning . Architectures such as transformers  and linear RNNs [4; 5; 6; 7; 8] are specifically designed to allow parallelization over the length of the input sequence. However, most conventional nonlinear RNNs (e.g. Elman RNNs, GRUs , LSTMs  etc.) are not readily parallelizable over the sequence length due to their sequential architecture. Thus, they do not benefit as much from parallel hardware. Nonetheless, these nonlinear RNN architectures are still used widely across the scientific community [11; 12; 13]. Furthermore, recent work has suggested that linear RNNs (and transformers) are fundamentally limited in their expressivity compared to nonlinear RNNs . Finally, nonlinear RNNs continue to be of significant interest in computational and theoretical neuroscience as models of neural systems [15; 16; 17; 18; 19; 20; 21; 22]. Therefore, scalable and stable parallelization methods for nonlinear RNNs offer significant benefits across many fields.

Towards this goal, Lim et al.  proposed DEER, a method for evaluating a nonlinear RNN in parallel. DEER casts inference as finding the solution of a fixed-point equation designed specifically to capture the nonlinear dynamics of the RNN. Newton's method is used to solve the resulting fixed-point equation. With good initialization, Newton's method enjoys quadratic convergence rates [23; Chapter 11]. Lim et al.  also show that the inversion of the structured Jacobian matrix required by Newton's method can be cast as an associative parallel scan . DEER therefore reduces the evaluation runtime over sequential evaluation by as much as factor of twenty.

However, DEER also inherits the weaknesses of Newton's method and parallel scans. The first weakness is _scalability_. Let \(D\) denote the state dimension and \(T\) denote sequence length. Using a parallel scan to evaluate updates from Newton's method, DEER inherits \((TD^{2})\) memory complexity and \((TD^{3})\) computational work . These costs can be prohibitive in practical deep learning settings. The second limitation of DEER is _numerical stability_, inherited from Newton's method. In general, _undamped_ Newton's method does not provide global convergence guarantees and in practice often diverges . We seek to ameliorate both of these weaknesses.

To do this, we leverage two techniques: quasi-Newton approximations and trust regions. Quasi-Newton approximations are a common adaptation of Newton's method, where approximate, but faster and less memory intensive updates are used instead of exact Newton steps. Empirically, these methods often expedite convergence in terms of wall-clock time, even though more iterations are performed. We apply quasi-Newton approximations to reduce the memory and compute required by DEER, and we find accelerated convergence and reduced memory consumption. Secondly, we leverage a connection between Newton's method with a trust region and Kalman smoothing in sequential models . This connection allows us to stabilize the Newton iteration by limiting the step size to the radius of the trust region, preventing large and numerically unstable steps. The update can be computed with a parallel Kalman smoother , achieving a runtime that is logarithmic in the sequence length. We refer to DEER accelerated with a quasi-Newton approximation as quasi-DEER, and DEER stabilized with trust regions as "**E**valuating **L**evenberg-Marquardt via **K**alman" (ELK). We then combine these approaches to yield a fast and stable algorithm, which we call quasi-ELK.

Crucially, DEER, ELK, and their "quasi" variants are _algorithms_ for parallelizing _any_ discrete-time nonlinear dynamical system, including stateful architectures such as RNNs, that may or may not include stochasticity. We use "parallel" to refer to the fact that each iteration of our iterative algorithm operates on the _entire_\(T\)-length sequence (and not on each sequence element one at a time).

We outline the key contributions and organization of the paper here. First, we introduce background material, particularly focusing on DEER , in Sections 2 and 3. We then present three short novel proofs: that DEER is globally convergent; that this convergence is robust to modifications of the linearized dynamics (Proposition 1); and that there is a unique solution with no local minima (Appendices A.1 and A.2). We then introduce quasi-Newton approximations to DEER to improve efficiency (quasi-DEER, Section 4.1), and trust regions to stabilize DEER (ELK, Section 4.2) We also provide an interpretation of how trust regions stabilize the dynamics by damping the eigenvalues of the Jacobians (Appendix A.3). We show empirically that quasi-DEER remains accurate, with reduced runtime and memory consumption (Section 6). In regimes where DEER is numerically unstable or convergences slowly, we show ELK and quasi-ELK can enjoy fast, numerically stable convergence. We conclude by discussing the relative strengths and weaknesses of each method, providing guidance on how to select and tune them, and highlighting avenues for future research (Section 7). We provide our code at https://github.com/lindermanlab/elk.

    &  \\   & Parallel & Work & Memory & Stability \\  Sequential & No & \((TD^{2})\) & \((D)\) & Very high \\  DEER  & Yes & \((TD^{3})\) & \((TD^{2})\) & Low \\ Quasi-DEER & Yes & \((TD)\) & \((TD)\) & Low \\ ELK & Yes & \((TD^{3})\) & \((TD^{2})\) & High \\ Quasi-ELK & Yes & \((TD)\) & \((TD)\) & Moderate \\   

Table 1: Description of the relative strengths and weaknesses of the five evaluation methods we consider. We discuss further in Section 7.

Figure 1: Overview of the parallelizable methods we consider in this paper. We introduce diagonal approximations to improve complexity (quasi-DEER, Section 4.1) and link to Kalman filtering and trust regions to improve stability (ELK, Section 4.2). We combine these ideas in quasi-ELK (Section 4.2).

## 2 Problem Statement

We consider nonlinear Markovian state space models, with the state at time \(t\) denoted \(_{t}^{D}\) and nonlinear transition dynamics \(f:^{D}^{D}\). We denote the full sequence of \(T\) states as \(_{1:T}^{T D}\). Note that we primarily focus on the transition dynamics, so we suppress any (possibly random) input dependence of the model in the notation. However, the algorithms in this paper extend easily to these situations.

For any collection of candidate states \(\{_{t}\}_{t=1}^{T}\) and an _initial state_\(_{0}\) we can define the _residual_

\[(_{1:T}):=[_{1}-f(_{0}),\;_ {2}-f(_{1}),\;_{3}-f(_{2}),,_{T }-f(_{T-1})]^{T D}.\] (1)

This residual can be interpreted as the one-step error incurred by assuming the \(t^{}\) state is \(_{t}\) instead of \(f(_{t-1})\). The solution of the state space model, \(_{1:T}^{*}\), is the only trace with zero residual. Equivalently, it is the unique solution to the fixed-point equation

\[(_{1:T}^{*})=.\] (2)

The conventional way to obtain \(_{1:T}^{*}\) is to apply \(f\) sequentially \(T\) times. Sequential evaluation always yields a valid trace, but it requires \((T)\) sequential operations (i.e. computational depth or span), and hence does not fully leverage the capabilities of parallel hardware. We aim to compute \(_{1:T}^{*}\) in sublinear time using parallel computation.

Jacobian of the ResidualFor notational brevity, we overload \(\) and \(\) to also denote vectors in \(^{TD}\), representing flattened versions of \(_{1:T}\) and \(_{1:T}\). We can therefore write the Jacobian of the residual for the whole sequence, \(J()\), as a \(TD TD\) matrix with block bidiagonal structure of the form

\[J():=}{}()= I_{D}&0&&0&0\\ -}(_{1})&I_{D}&&0&0\\ &&&&\\ 0&0&&I_{D}&0\\ 0&0&&-}(_{T-1})&I_{D} .\] (3)

## 3 DEER: Newton's Method for Parallel Evaluation of Sequential Models

Lim et al.  propose DEER, an algorithm using Newton's method for parallel evaluation of nonlinear sequential models, including both discrete-time nonlinear RNNs (GRUs, LSTMs, etc.) and neural ODEs [28; 29]. In this paper, we focus on the discrete-time setting, and address questions that arise from Lim et al. : how to _scale_ Newton's method, and how to make it _numerically stable_.

In this section we introduce DEER. We begin with a simplified derivation that emphasizes the link between Newton's method on vector spaces and parallelizable linear recurrences. We then present a new proof that DEER theoretically _always_ converges globally. This proof also highlights why global convergence can be numerically unstable and/or slow in practice. We conclude by using these insights to discuss the weaknesses of DEER, and to motivate the methods we develop in Section 4.

### Derivation of DEER from Newton's Method

The original derivation of DEER used Newton's method on Banach spaces and the Frechet derivative for continuous-time systems to derive the update . We specialize to the setting of discrete-time RNNs, and present a streamlined derivation that more directly connects the structure of the Jacobian in (3) to the linear recurrence relation in (6). This connection highlights why DEER incurs cubic work in \(D\) and may encounter numerical instabilities. We will also use this form to prove DEER's global convergence in Section 3.2.

The \(i^{}\) Newton iterate for (2), starting at \(^{(i)}\), is given by

\[^{(i+1)}^{(i)}-J(^{(i)})^{-1}\, (^{(i)}),\] (4)

or equivalently,

\[^{(i+1)}^{(i+1)}-^{(i)}=-J( ^{(i)})^{-1}\,(^{(i)}).\] (5)Note this uses the root-finding view of Newton's method, see Appendix C.2.

The Jacobian defined in (3) is invertible and all of the eigenvalues are equal to one.1 Storing and naively inverting the Jacobian is infeasible for large \(D\) or \(T\). However, since \(J()\) is block bidiagonal, we can solve for \(\) in (5) by forward substitution. This reduces to a simple recursion with the initial condition \(_{1}^{(i+1)}=-_{1}(^{(i)})\), and for \(t>1\),

\[_{t}^{(i+1)}=[}( _{t-1}^{(i)})]\,_{t-1}^{(i+1)}-_{t}( ^{(i)}).\] (6)

DEER uses the linearity of this recursion, solving it in parallel with a parallel associative scan . Therefore, with \((T)\) processors, each Newton iteration can be performed in \(( T)\) time.

We emphasize that the computation of the Newton step \(\) in (5) is being parallelized. \(J\) would, in general, be a \(TD TD\) matrix that is prohibitive to store or invert. But by formulating this solve as an LDS in (6), we are able to parallelize the computation of \( s\) (which consists of \(T\) state updates, each of dimension \(D\)) over the sequence length. With sufficient processors, each update in (5) can be computed in \(( T)\) time. Our implementation uses the parallel associative scan from JAX  (see Appendix B.6).

### Global Convergence of DEER

We present a proof that DEER converges globally for discrete-time RNNs to the solution \(_{1:T}^{*}\) of (2) in at most \(T\) steps.

**Proposition 1**.: _Undamped Newton's method will converge to the true solution, \(_{1:T}^{*}\), of the fixed-point equation (2) in at most \(T\) Newton iterations, for any initial \(_{1:T}^{(0)}\)._

Proof sketch.: For the full proof by induction, see Appendix A.1. The structure of \(J()\) determines the recurrence in (6). The update applied at time \(t\), \(_{t}^{(i+1)}\), from (6) is the summation of a linearized \(f\) applied to the update at time \(t-1\), and the residual one-step error at time \(t\). Therefore, if the previous timestep is correct (i.e. \(_{t-1}^{(i+1)}=\)), then the update at time \(t\) is just the one-step residual, which is defined exactly as the error. Therefore, if the previous value is correct, the updated current value will be correct. Given that \(f\) and \(_{0}\) are fixed and known, the result follows that all \(T\) timesteps will have zero residual after \(T\) iterations. 

It is not immediately obvious from (4) or past work Lim et al.  that DEER converges globally, but Proposition 1 shows that it does, at least theoretically. This result has two crucial corollaries. First, after \(i\) Newton iterations, \(_{1:T}^{(i)}\) will have zero error for all \(t i\). Therefore, if the iteration encounters numerical instabilities, as Newton is prone to, we can simply use a heuristic of resetting \(s_{t}^{(i)}\) to a finite value for all \(t>i\). This preserves the solution for time indices \(t i\) and allows the optimization to continue, but it is equivalent to running Newton's method from scratch on \(_{t:T}\). This process is repeated until the entire trace has zero residual. A second corollary is that _any_ set of finite matrices can replace \(\{}{{}}\}_{t=2}^{T}\) in (3) or (6), and the resulting quasi-Newton method will still converge globally in at most \(T\) iterations. This preservation of global convergence provides further motivation for exploring quasi-Newton methods, as we discuss in the next section.

### Weaknesses of DEER

Despite the theoretical convergence of DEER, its formulation as a linear recurrence relation in (6) highlights its limited scalability and stability. Scalability is limited because, in general, \(}{{}}\) is a dense \(D D\) matrix. Therefore the parallel associative scan, which uses matrix-matrix multiplications, has \((TD^{3})\) computational work and \((TD^{2})\) memory complexity. Stability is limited because we often have no control over the eigenvalues of \(}{{}}\). If sufficiently many of these eigenvalues over the sequence length are larger in magnitude than one, then the linear recurrence relation will be numerically unstable. The heuristic approach of resetting unstable values is sufficient to ensure global convergence, but as we show in Section 6.3, it comes at the cost of runtime, as convergence is dramatically slower. These weaknesses motivate the development of two new techniques for parallel evaluation of RNNs: quasi-DEER and ELK, which we discuss in the next section.

Scaling and Stabilizing Newton's Method for Parallel Evaluation

In Section 4.1 we introduce quasi-DEER, a quasi-Newton method that addresses the intractability of DEER for large state sizes. In Section 4.2 we introduce Evaluating Levenberg-Marquardt with Kalman (ELK), a damped Newton method for numerically stable, parallel evaluation of nonlinear RNNs. We also introduce quasi-ELK, which combines quasi-DEER and ELK to create a damped Newton's method for parallel sequential evaluation that is scalable and numerically stable.

### Quasi-DEER: Scaling DEER with Diagonal Jacobian Approximations

As a consequence of our Proposition 1, replacing the Jacobians \(\{}{{}}\}_{t=2}^{T}\) with an arbitrary matrix will still result in global convergence of the resulting DEER-like algorithm in at most \(T\) iterations. A straightforward way to reduce the computational cost is to replace \(\{}{{}}\}_{t=2}^{T}\) with \(\{(}{{} })\}_{t=2}^{T}\), i.e. take the diagonal entries of the Jacobians of the dynamics functions. The resulting linear recursion requires only \((TD)\) memory because it only needs to store diagonal matrices, and \((TD)\) work, because the parallelized associative scan only uses element-wise vector multiplies. Position-wise matrix-vector multiplies are still required to obtain the residuals, but this computation can be naively parallelized across the sequence.

Quasi-Newton methods approximate the Jacobian for computational reasons, so we refer to this algorithm as quasi-DEER. In Section 6, we show that quasi-DEER outperforms DEER on wall-clock time and memory usage on the tests from Lim et al. . Quasi-DEER improves the scalability of DEER, but it does not address stability concerns. We propose a more stable solution below.

### ELK: Stabilizing DEER with Trust Regions

Rather than treating RNN evaluation as a fixed-point finding problem, let us instead consider it as an optimization problem. First, we define the _merit function_

\[()\|() \|_{2}^{2}.\] (7)

As in the fixed-point formulation, the unique minimizer of this objective is \(^{*}\). In fact, the only local minimum of the merit function (7) is \(^{*}\), as proved in Proposition 3 in Appendix A.2. One way of minimizing this _nonlinear_ sum of squares objective is via the Gauss-Newton algorithm , which alternates between linearizing the terms in the merit function and solving the resulting _linear_ sum-of-squares problem. The linearized objective at iteration \(i\) is

\[}_{^{(i)}}()=\| (^{(i)})+J(^{(i)})\|_{2} ^{2}.\] (8)

The solution is \(^{(i+1)}=-J(^{(i)})^{-1}\,(^{(i )})\), which is exactly the DEER update from (5).

Formulating evaluation as nonlinear least squares also suggests more stable algorithms. The Levenberg-Marquardt algorithm  uses updates that solve a constrained optimization problem

\[_{}}_{^{(i)}}( )\|\|_{2} D_{i+1},\] (9)

where \(D_{i+1}\) is an upper bound on the step size. We recognize this constraint as a trust region, which is often used in conjunction with Newton's method to improve numerical stability and convergence.

Finally, minimizing this constrained optimization is equivalent to minimizing the Lagrangian

\[}(,_{i+1})=}_ {^{(i)}}()+}{2}\| \|_{2}^{2}\] (10)

for some \(_{i+1} 0\). As noted by Sarkka and Svensson , the minimizer of this Lagrangian can be obtained by a Kalman smoother. We emphasize this connection in the following proposition.

**Proposition 2**.: _Solving for the Levenberg-Marquardt update that minimizes (10) with fixed \(_{i+1}\) is equivalent to finding the maximum a posteriori (MAP) estimate of \(_{1:T}\) in a linear Gaussian state space model, which can be done in \(( T)\) time on a sufficiently large parallel machine._Proof.: Expanding the residual and Jacobian functions in (8), we see that up to an additive constant, the negative Lagrangian can be rewritten as,

\[-}(,_{i+1}) (_{1} f(_{0}),I_{D})+_{t=1}^{ T}(_{t}^{(i)}\,\,_{t},}I_{D})\\ +_{t=2}^{T}(_{t}\,\,f( _{t-1}^{(i)})+[}(_{t-1}^{(i)})](_{t-1}-_{t-1}^{(i)}),I_{D}),\] (11)

where \((,)\) denotes the probability density function of the multivariate normal distribution.

We recognize (11) as the log joint probability of a linear Gaussian state space model (LGSSM)  on \((_{1},,_{T})\). The means of the dynamics distributions are given by the linearization of \(f\), and the emissions are the previous iteration's states, \(^{(i)}\). The parameter \(_{i+1}\) sets the precision of the emissions, governing how far the posterior mode deviates from the previous states.

The minimizer of (10) is the posterior mode of the LGSSM (11), and can be obtained by Kalman smoothing . As with the linear recursions in DEER, the Kalman smoother can be implemented as a parallel scan that scales as \(( T)\) in time on a machine with \((T)\) processors . 

Therefore, we can evaluate an RNN by minimizing the merit function with the Levenberg-Marquardt algorithm. Since each step of the algorithm is performed by parallel Kalman smoothing, we call this approach _Evaluating Levenberg-Marquardt with Kalman_ (ELK). Note that DEER is a special case of ELK, where \(=0\), which can be seen as minimizing the unpenalized linearized objective (8), or, alternatively, taking a Newton step with an infinitely large trust region. Moreover, under certain conditions, ELK also enjoys global convergence guarantees [23, Thms. 11.7, 11.8].

Quasi-ELK: Scalability and StabilityAs with DEER, we can substitute an approximate Jacobian into the Lagrangian to obtain the _quasi-ELK_ algorithm. Quasi-ELK enjoys the compute and memory scaling of quasi-DEER, as well as stability from the trust region damping from ELK. We show empirically in Section 6.3 that while quasi-ELK takes more iterates to converge than ELK, each quasi-ELK iterate is faster, giving overall runtime speedups.

Implementation DetailsThe convergence rate of (quasi-)ELK depends on the trust region radius \(D_{i}\) (or alternatively \(_{i}\)). Although there exist methods to analytically set \(_{i}\)[23, Algorithm 4.3], these approaches require factorizing \(}}{{}}\), which is intractable at scale. Therefore, we treat \(\) as a hyperparameter set by a sweep over log-spaced values (cf. Appendix B.4).

We also use Kalman filtering instead of smoothing. We do so for two main reasons: filtering requires less work and memory; and we also found it to converge in fewer Newton iterations than smoothing. We hypothesize that this follows from Proposition 1, where the early part of the trace converges first. In Appendix A.3 we also discuss an alternative interpretation of ELK and the Kalman filter as defining a linear recurrence where the trust region attenuates the eigenvalues used in the parallel scan.

LimitationsThe quasi-Newton methods lose the local quadratic convergence properties of Newton but remain globally convergent (cf. Proposition 1). Our implementation of quasi-DEER for training uses approximate gradients (cf. Section 6.2). The heuristic of resetting to the states to zero when they become unstable is also motivated by Proposition 1, but it slows convergence in (quasi-)DEER methods. As a result, we develop ELK to stabilize evaluation. Like DEER, ELK has cubic complexity in \(D\), which we combat with quasi-ELK. However, quasi-ELK adds an additional hyperparameter. Note that all four parallelized methods discussed in this paper, as well as sequential evaluation of RNNs, have different regimes where they are fastest. For example, in our evaluation of autoregressive RNNs (Section 6.3), the ELK methods are faster than the DEER methods on wall-clock time, but they are slower than sequential. In our evaluation of the Lorenz96 system (Appendix B.5), ELK is more stable than DEER, but DEER is faster on wall-clock time. An area for future research is characterizing the properties of dynamical systems and hardware where each method is fastest. Finally, at the core of the implementation of the parallelized methods is the parallel associative scan (cf. Appendix B.6), which is currently available in JAX  but not in the standard PyTorch package.

Related Work

RNNs and ParallelismNonlinear RNNs are a natural choice for modeling sequential data because of their inductive biases and memory efficiency. However, most nonlinear RNNs are not parallelizable over the sequence length, and architectures that can exploit parallel computational hardware have been core to the success of deep learning. Therefore, a range of sequence architectures that inherently admit parallelism have been proposed, including transformers , deep linear RNNs [4; 5; 6; 7; 32; 33; 34; 35], and convolutions [36; 37; 38; 39]. These methods obtain parallelism by developing new architectures, and do not consider parallelizing existing nonlinear architectures. DEER  is notable as it considers parallel evaluation and training of arbitrary nonlinear RNNs.

Root Finding in Deep LearningBeyond DEER, there has been broad interest in using root finders/fixed-point iterations in deep learning and sequence modeling. Deep implicit layers [40; 41; 42; 43] and neural ODEs [28; 44] replace conventional feed forward network layers  with an implicit layer whose output is the root of an equation. Moreover, Song et al.  parallelize the evaluation of feedforward nets using Jacobi and Gauss-Siedel iterations. In sequence modeling, parallel decoding methods [47; 48; 49] adapt ideas from Jacobi and Gauss-Siedel iterations to evaluate autoregressive sequence models in parallel. These approaches iteratively refine inputs by repeatedly feeding in previous outputs back into a parallelized sequence model. However, these methods presuppose the existence of a parallelized forward pass for the sequence model and do not leverage additional gradient information to obtain sublinear convergence.

Parallelizing Dynamical Systems over TimeOther work has investigated evaluating other nonlinear dynamical systems over time. ParaDIGMS  parallelizes sampling from diffusion models but uses Picard iterations instead of Newton's method, while Selvam et al.  use Parareal iterations. In the numerical ODE and PDE communities there has been great interest in Parallel in Time methods; see Gander , Ong and Schroder  for surveys. Vargas et al.  parallelized the evaluation of chaotic dynamical systems over time, but instead of casting Newton's method as a parallel scan, they resort to multi-grid methods to evaluate at different hierarchies. Moreover, these methods have not yet been applied to parallelizing RNNs.

Scaling and Stabilizing Newton's MethodQuasi-Newton methods are efficient algorithms that use an approximation of the Jacobian or Hessian in Newton's method, and include approaches like BFGS [55; 56; 57; 58] and L-BFGS . Other approaches use Newton's method to optimize deep nets . However, these quasi-Newton algorithms do not admit efficient parallel scans. There are also conjugate gradients methods for exploiting structured Jacobians or Hessians , though they often do not attain the fast convergence rates of Newton or quasi-Newton methods . Methods for stabilizing and ensuring Newton's method converges globally include regularization approaches [62; 63], backtracking line search , and trust regions . All these stabilization methods have strengths and weaknesses, but as noted by Nocedal and Wright : _"the trust-region Newton method has proved to be highly effective in practice,"_ leading us to apply it to evaluating RNNs.

Nonlinear Least Squares and Kalman SmoothingBell and Cathey  and Bell  draw connections between the Gauss-Newton method and the iterated extended Kalman filter and smoother [68; 31]. Because Gauss-Newton is unstable, it is natural to use Levenberg-Marquardt [69; 70] to stabilize the filtering/smoothing problem [71; 25; 72]. These approaches start with a smoothing problem and stabilize it using approaches from nonlinear equations, whereas we start with a nonlinear equation to solve and make the connection with Kalman filtering to leverage parallelized algorithms . We also discuss the practicalities of this connection for modern deep networks.

## 6 Experiments

We now experimentally examine the relative performance of these methods. Specifically, we evaluate: 1) whether quasi-DEER can provide memory savings over DEER and runtime savings over sequential evaluation, while retaining the accuracy of training and evaluation; and 2) whether ELK and quasi-ELK can be used to stabilize evaluation in regimes where DEER is unstable. In Sections 6.1 and 6.2 we use experimental designs from Lim et al.  and show that quasi-DEER retainsthe fast runtime and accuracy of DEER and can reduce memory consumption by up to an order of magnitude. In Section 6.3 we show that (quasi-)ELK remains stable when DEER becomes unstable, and that quasi-ELK is the fastest of all parallelized methods. We provide further details in Appendix B.

### Quasi-DEER for Evaluation

We first use an experimental design from Lim et al. . The task is to evaluate an untrained GRU across a range of hidden state sizes (\(D\)) and sequence lengths (\(T\)) on a 16GB V100 GPU; the inputs to the RNN also have dimension \(D\). We compare the wall-clock time and memory usage of three methods: sequential evaluation, DEER, and quasi-DEER. Results are shown Figure 2.

Both DEER and quasi-DEER are up to twenty times faster than sequential evaluation. The runtimes are similar between DEER and quasi-DEER for small networks, because although quasi-DEER steps are faster, quasi-DEER takes more iterations to converge. For larger networks, the difference in runtime is more pronounced. We also see that quasi-DEER requires as much as an order of magnitude less memory than DEER, thus allowing the application to architectural regimes previously infeasible with DEER. In Figure 6 of Appendix B.1.1 we show that in smaller \(T\) and \(D\) regimes we observe the expected sublinear time scaling with sequence length. This experiment confirms that quasi-DEER can replicate the performance of DEER, but with a smaller memory footprint.

### Quasi-DEER for Training

We verify that quasi-DEER expedites training nonlinear RNN models. We replicate the third experiment from Lim et al. , where a GRU is trained to classify _C. elegans_ phenotypes from the time series of principal components of the worms' body posture .

We show results in Figure 3. We see that the training dynamics under quasi-DEER leads to the similar validation accuracy trajectories. However, every quasi-DEER training step is faster by a factor of \(2.5\), despite performing around \(2\) times more Newton iterations per training step. This finding highlights how quasi-DEER can replace DEER when training nonlinear RNNs, yielding both time and memory savings. In our experiment, we use the quasi-DEER approximation for the backward pass as well, leading to gradients that are different from DEER in this setting. However, we find that there is negligible degradation in performance (Figure 3, left).

Figure 2: **Evaluating an untrained GRU. Relative performance of sequential, DEER and quasi-DEER for evaluating a randomly initialized (and untrained) GRU on (Top Row) wall-clock time, averaged over 20 random seeds and (Bottom Row) memory, averaged over 3 random seeds. All experiments use a 16GB V100 SMX2 (memory capacity indicated by the black dashed line) and Newton methods were run to convergence. Missing points in each series indicate the GPU ran out of memory. Quasi-DEER has a runtime commensurate with DEER, but with lower memory consumption, allowing quasi-DEER to work at scales where DEER cannot. The accuracy of the final converged solution is similar for all methods (see Figure 5 in Appendix B.1).**

DEER is prone to "spikes", where orders of magnitude more steps are required for convergence (Figure 3, right). While quasi-DEER is not as susceptible to these spikes (never more than half an order of magnitude), these instabilities motivated the study of stabilizing methods.

### ELK and Quasi-ELK for Evaluating Autoregressive RNNs

We conclude by studying an application where these numerical instabilities in DEER are critical. We use a small autoregressive GRU (hidden dimension \(N_{h}=3\)), where the previous sampled value is input into the GRU at the next step. Such autoregressive architectures were not examined by Lim et al. , but are an important class of models. We describe the precise details of the AR GRU we use in Appendix B.3. Crucially, the Markovian state \(_{t}\) used by all four parallelized methods must be expanded to include the current sampled output value, as well as the current GRU state.

Initialized AR GRUWe first repeat the analysis in Section 6.1 (and similar to the evaluation in Lim et al. ) for evaluating a randomly initialized autoregressive GRU. We see in the top left panel of Figure 4 that all four parallelized methods converge rapidly and stably to the correct trace, indicated by a low mean absolute discrepancy (MAD) between the true trace and the generated trace.

Trained AR GRUWe then study a pre-trained GRU that generates a noisy sine wave (see Figure 4, bottom). The linear recurrence relation (6) was numerically unstable in DEER and quasi-DEER. To remedy these instabilities, we take the approach described earlier of setting the unstable parts of the trace to a fixed value (here zero). Doing so ensures convergence, but at the cost of "resetting" the optimization for large swathes of the trace (Figure 4, bottom) and slowing convergence (see Figure 4, top right). This finding highlights how the instabilities of DEER -- which are inherited from both pathologies of Newton's method and the parallel recurrence -- can be crippling in even very simple scenarios. While resetting allows for convergence, the resulting convergence is very slow.

We then apply ELK and quasi-ELK. We show the results in the top right and bottom panels of Figure 4. We select the trust region size with a one-dimensional search over log-spaced values between \(10^{0}\) and \(10^{7}\). We see ELK has stabilized convergence, with the evaluation never incurring numerical instabilities or requiring heuristics. Crucially, by taking more stable steps (and not needing stabilizing heuristics) ELK and quasi-ELK converge faster than DEER and quasi-DEER. ELK can stabilize and expedite the convergence of DEER, with quasi-ELK faster still (by wall-clock time).

However, on this task, all parallelized methods (including DEER) are slower than sequential generation. Quasi-ELK is the fastest parallel method, taking 221 milliseconds, compared to sequential evaluation, taking 96 milliseconds. For comparison, DEER took 1,225 milliseconds. Quasi-ELK therefore still represents a large improvement in runtime over previous parallel methods. We provide timing details and further discussion in Appendix B.3.2.

Figure 3: **Training a GRU with DEER.** Comparison of DEER and quasi-DEER during GRU training for the _C. elegans_ time-series classification task (Section 6.2). Each time series has length \(T=17,984\). We show the median, and 5-95% interval across a rolling window of 20 training steps. **(Left)** DEER and quasi-DEER have the similar validation accuracy trajectories, indicating similar training dynamics. The sequential trace shown is for 24 hours of training (compared to 11 and 4 hours for the whole DEER and quasi-DEER traces). **(Center)** Each quasi training iteration is 2.5 times faster than each DEER training iteration. Sequential training steps took more than 6 seconds each (not pictured). **(Right)** Each quasi training iteration requires (approximately) 2 times more Newton iterations to converge, indicating that each quasi Newton step is approximately 5 times faster than the corresponding DEER Newton step.

## 7 Conclusion

In this paper we proposed methods for scalable and stable parallel evaluation of nonlinear RNNs. DEER  achieved speedups over sequential evaluation, but incurred quadratic memory, cubic work, and numerical instabilities. We therefore extended DEER to use quasi-Newton approximations that reduced computational complexity, and we provided a novel proof that both DEER and quasi-DEER converge globally. To stabilize DEER, we leveraged a connection between the Levenberg-Marquardt method and Kalman smoothing to enable parallel evaluation of RNNs, allowing us to stabilize DEER while still leveraging fast parallel filtering. We empirically verified that quasi-DEER, ELK, and quasi-ELK improve convergence across a range of metrics and examples. This result allows parallel evaluation of nonlinear RNNs to be scaled beyond what is possible with DEER.

When selecting an approach, we offer the following advice: If rapid convergence is reliably observed, our experiments show that quasi-DEER provides the fastest convergence in terms of wall-clock time. However, if the dynamics are on the edge of stability, then ELK offers the most stable performance of these parallelized methods, but quasi-ELK could be faster in wall-clock time and just as stable. In such settings, it is worth sweeping the hyperparameter to choose the best version of ELK (note that for \(=0\), ELK specializes to DEER). However, in the setting of chaotic dynamics, standard sequential evaluation may ultimately be faster.

Our experiments and these observations also highlight avenues for future research. While we found success with a diagonal approximation, structured approximations of the Jacobian that still admit fast parallelism but are more faithful approximations may allow for more accurate quasi-Newton steps to be taken. Secondly, quantifying the convergence rates of quasi-ELK would allow us to provide tighter bounds than those derived in Proposition 1. Finally, theoretically investigating whether further improvements to parallelized methods can prove faster than sequential evaluation for dynamical systems on the edge of stability, or whether there are fundamental limitations to the computational benefit of parallelization, are interesting questions for future work.

Figure 4: ELK stabilizes parallel evaluation of an AR GRU. **(Top Left)** The mean absolute difference (MAD) evaluated on the outputs converges rapidly for all four methods on a sequence generated by an _untrained_ AR GRU. **(Top Right)** The MAD for evaluating a trained AR GRU. Undamped DEER variants are unstable and converge slowly (using the reset heuristic). ELK stabilizes and accelerates convergence. **(Bottom)** The output after 1, 100, 1000, and 2000 Newton iterations. The black dotted line is the true trace. ELK and quasi-ELK converge rapidly, but DEER and quasi-DEER are unstable. The lines where DEER and quasi-DEER are zero depict the zeroing heuristic.

#### Acknowledgments

We thank John Duchi, David Zoltowski, and the members of the Linderman Lab for their helpful feedback. We thank Amber Hu for her work on preliminary versions of this project, and Leo Kozachkov for pointing out that the conditions we established for the merit function are equivalent to invexity. We thank the anonymous NeurIPS reviewers whose feedback improved this paper.

This work was supported by grants from the NIH BRAIN Initiative (U19NS113201, R01NS131987, & RF1MH133778), the NSF/NIH CRCNS Program (R01NS130789). X.G. would also like to acknowledge support from the Walter Byers Graduate Scholarship from the NCAA. S.W.L. is supported by fellowships from the Simons Collaboration on the Global Brain, the Alfred P. Sloan Foundation, and the McKnight Foundation. The authors have no competing interests to declare.

Some of the experiments were performed on the Sherlock cluster. We thank Stanford University and the Stanford Research Computing Center for providing computational resources and support that contributed to these research results.