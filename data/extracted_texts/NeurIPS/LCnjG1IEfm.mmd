# Normalization-Equivariant Neural Networks with Application to Image Denoising

Sebastien Herbreteau

Emmanuel Moebel

Charles Kervrann

Centre Inria de l'Universite de Rennes, France

{sebastien.herbreteau, emmanuel.moebel, charles.kervrann}@inria.fr

###### Abstract

In many information processing systems, it may be desirable to ensure that any change of the input, whether by shifting or scaling, results in a corresponding change in the system response. While deep neural networks are gradually replacing all traditional automatic processing methods, they surprisingly do not guarantee such normalization-equivariance (scale + shift) property, which can be detrimental in many applications. To address this issue, we propose a methodology for adapting existing neural networks so that normalization-equivariance holds by design. Our main claim is that not only ordinary convolutional layers, but also all activation functions, including the ReLU (rectified linear unit), which are applied element-wise to the pre-activated neurons, should be completely removed from neural networks and replaced by better conditioned alternatives. To this end, we introduce affine-constrained convolutions and channel-wise sort pooling layers as surrogates and show that these two architectural modifications do preserve normalization-equivariance without loss of performance. Experimental results in image denoising show that normalization-equivariant neural networks, in addition to their better conditioning, also provide much better generalization across noise levels.

## 1 Introduction

Sometimes wrongly confused with the invariance property which designates the characteristic of a function \(f\) not to be affected by a specific transformation \(\) applied beforehand, the equivariance property, on the other hand, means that \(f\) reacts in accordance with \(\). Formally, invariance is \(f=f\) whereas equivariance reads \(f= f\), where \(\) denotes the function composition operator. Both invariance and equivariance play a crucial role in many areas of study, including physics, computer vision, signal processing and have recently been studied in various settings for deep learning-based models .

In this paper, we focus on the equivariance of neural networks \(f_{}\) to a specific transformation \(\), namely normalization. Although highly desirable in many applications and in spite of its omnipresence in machine learning, current neural network architectures do not equivary to normalization. With application to image denoising, for which _normalization-equivariance_ is generally guaranteed for a lot of conventional methods , we propose a methodology for adapting existing neural networks, and in particular denoising CNNs , so that _normalization-equivariance_ holds by design. In short, the proposed adaptation is based on two innovations:

1. affine convolutions: the weights from one layer to each neuron from the next layer, _i.e._ the convolution kernels in a CNN, are constrained to encode affine combinations of neurons (the sum of the weights is equal to \(1\)).

2. channel-wise sort pooling: all activation functions that apply element-wise, such as the ReLU, are substituted with higher-dimensional nonlinearities, namely two by two sorting along channels that constitutes a fast and efficient _normalization-equivariant_ alternative.

Despite strong architectural constraints, we show that these simple modifications do not degrade performance and, even better, increase robustness to noise levels in image denoising both in practice and in theory.

## 2 Related Work

A non-exhaustive list of application fields where equivariant neural networks were studied includes graph theory, point cloud analysis and image processing. Indeed, graph neural networks are usually expected to equivaly, in the sense that a permutation of the nodes of the input graph should permute the output nodes accordingly. Several specific architectures were investigated to guarantee such a property [3; 21; 38]. In parallel, rotation and translation-equivariant networks for dealing with point cloud data were proposed in a recent line of research [6; 13; 40]. A typical application is the ability for these networks to produce direction vectors consistent with the arbitrary orientation of the input point clouds, thus eliminating the need for data augmentation. Finally, in the domain of image processing, it may be desirable that neural networks produce outputs that equivaly with regard to rotations of the input image, whether these outputs are vector fields , segmentation maps [41; 43], or even bounding boxes for object tracking .

In addition to their better conditioning, equivariant neural networks by design are expected to be more robust to outliers. A spectacular example has been revealed by S. Mohan _et al._ in the field of image denoising. By simply removing the additive constant ("bias") terms in neural networks with ReLU activation functions, they showed that a much better generalization at noise levels outside the training range was ensured. Although they do not fully elucidate why biases prevent generalization, and their removal allows it, the authors establish some clues that the answer is probably linked to the _scale-equivariant_ property of the resulting encoded function: rescaling the input image by a positive constant value rescales the output by the same amount.

## 3 Overview of normalization-equivariance

### Definitions and properties of three types of fundamental equivariances

We start with formal definitions of the different types of equivariances studied in this paper. Please note that our definition of "scale" and "shift" may differ from the definitions given by some authors in the image processing literature.

**Definition 1**: _A function \(f:^{n}^{m}\) is said to be:_

* _scale-equivariant if_ \( x^{n},_{+}^{+},\;f( x )= f(x)\,,\)__
* _shift-equivariant if_ \( x^{n},,\;f(x+)=f(x)+\,,\)__
* _normalization-equivariant if it is both scale-equivariant and shift-equivariant:_ \[ x^{n},_{+}^{+}, ,\;f( x+)= f(x)+\,,\]

_where addition with the scalar shift \(\) is applied element-wise._

Note that the _scale-equivariance_ property is more often referred to as positive homogeneity in pure mathematics. Like linear maps that are completely determined by their values on a basis, the above described equivariant functions are actually entirely characterized by the values their take on specific subsets of \(^{n}\), as stated by the following lemma (see proof in Appendix C.1).

**Lemma 1** (**Characterizations**): \(f:^{n}^{m}\) _is entirely determined by its values on the:_

* _unit sphere_ \(\) _of_ \(^{n}\) _if it is scale-equivariant,_
* _orthogonal complement of_ \((_{n})\)_, i.e._ \((_{n})^{}\)_, if it is shift-equivariant,_
* _intersection_ \((_{n})^{}\) _if it is normalization-equivariant,_

_where \(_{n}\) denotes the all-ones vector of \(^{n}\)._

Finally, Lemma 2 highlights three basic equivariance-preserving mathematical operations that can be used as building blocks for designing neural network architectures (see proof in Appendix C.1).

**Lemma 2** (Operations preserving equivariance): _Let \(f\) and \(g\) be two equivariant functions of the same type (either in scale, shift or normalization). Then, subject to dimensional compatibility, all of the following functions are still equivariant:_

* \(f g\) _(_\(f\) _composed with_ \(g\)_),_
* \(x(f(x)^{}\,g(x)^{})^{}\) _(concatenation of_ \(f\) _and_ \(g\)_),_
* \((1-t)f+tg\) _for all_ \(t\) _(affine combination of_ \(f\) _and_ \(g\)_)._

### Examples of normalization-equivariant conventional denoisers

A ("blind") denoiser is basically a function \(f:^{n}^{n}\) which, given a noisy image \(y^{n}\), tries to map the corresponding noise-free image \(x^{n}\). Since scaling up an image by a positive factor \(\) or adding it up a constant shift \(\) does not change its contents, it is natural to expect _scale_ and _shift equivariance_, _i.e. normalization equivariance_, from the denoising procedure emulated by \(f\). In image denoising, a majority of methods usually assume an additive white Gaussian noise model with variance \(^{2}\). The corruption model then reads \(y(x,^{2}I_{n})\), where \(I_{n}\) denotes the identity matrix of size \(n\), and the noise standard deviation \(>0\) is generally passed as an additional argument to the denoiser ("non-blind" denoising). In this case, the augmented function \(f:(y,)^{n}^{+}_{*}^{n}\) is said _normalization-equivariant_ if:

\[(y,)^{n}^{+}_{*}, ^{+}_{*},,\ f( y+,)=  f(y,)+\,,\] (1)

as, according to the laws of statistics, \( y+( x+,()^{2}I_{n})\). In what follows, we give some well-known examples of traditional denoisers that are _normalization-equivariant_ (see proofs in Appendix C.2).

Noise-reduction filters:The most rudimentary methods for image denoising are the smoothing filters, among which we can mention the averaging filter or the Gaussian filter for the linear filters and the median filter which is nonlinear. These elementary "blind" denoisers all implement a _normalization-equivariant_ function. More generally, one can prove that a linear filter is _normalization-equivariant_ if and only if its coefficients add up to \(1\). In others words, _normalization-equivariant_ linear filters process images by affine combinations of pixels.

Patch-based denoising:The popular N(on)-L(ocal) M(eans) algorithm  and its variants [12; 20; 27] consist in computing, for each pixel, an average of its neighboring noisy pixels, weighted by the degree of similarity of the patches to which they belong. In other words, they process images by convex combinations of pixels. More precisely, NLM can be defined as:

\[f_{}(y,)_{i}=}_{y_{j}(y_{i})}e^{ )-p(y_{j})\|_{2}^{2}}{h^{2}}}y_{j} W_{i}=_{y_{j}(y_{i})}e^{-)-p(y_{j})\, \|_{2}^{2}}{h^{2}}}\] (2)

where \(y_{i}\) denotes the \(i^{th}\) component of vector \(y\), \((y_{i})\) is the set of its neighboring pixels, \(p(y_{i})\) represents the vectorized patch centered at \(y_{i}\), and the smoothing parameter \(h\) is proportional to \(\) as proposed by several authors [4; 12; 29]. Defined as such, \(f_{}\) is a _normalization-equivariant_ function. More recently, N(on)-L(ocal) Ridge  proposes to process images by linear combinations of similar patches and achieves state-of-the-art performance in unsupervised denoising. When restricting the coefficients of the combinations to sum to \(1\), that is imposing affine combination constraints, the resulting algorithm encodes a _normalization-equivariant_ function as well.

TV denoising:Total variation (TV) denoising  is finally one of the most famous image denoising algorithm, appreciated for its edge-preserving properties. In its original form , a TV denoiser is defined as a function \(f:^{n}^{+}_{*}^{n}\) that solves the following equality-constrained problem:

\[f_{}(y,)=*{arg\,min}_{x^{n}}\ \|x\|_{}\|y-x\|_{2}^{2}=n^{2}\] (3)

where \(\|x\|_{}:=\| x\|_{2}\) is the total variation of \(x^{n}\). Defined as such, \(f_{}\) is a _normalization-equivariant_ function.

### The case of neural networks

Deep learning hides a subtlety about normalization equivariance that deserves to be highlighted. Usually, the weights of neural networks are learned on a training set containing data all normalized to the same arbitrary interval \([a_{0},b_{0}]\). This training procedure improves the performance and allows for more stable optimization of the model. At inference, unseen data are processed within the interval \([a_{0},b_{0}]\) via a \(a\)-\(b\) linear normalization with \(a_{0} a<b b_{0}\) denoted \(_{a,b}\) and defined by:

\[_{a,b}:y(b-a)+a\,.\] (4)

Note that this transform is actually the unique linear one with positive slope that exactly bounds the output to \([a,b]\). The data is then passed to the trained network and its response is finally returned to the original range via the inverse operator \(_{a,b}^{-1}\). This proven pipeline is actually relevant in light of the following proposition.

**Proposition 1**: \(\,a<b,\,f:^{n}^{m}, _{a,b}^{-1} f_{a,b}\) _is a normalization-equivariant function._

While normalization-equivariance appears to be solved, a question is still remaining: how to choose the hyperparameters \(a\) and \(b\) for a given function \(f\)? Obviously, a natural choice for neural networks is to take the same parameters \(a\) and \(b\) as in the learning phase whatever the input image is, _i.e._\(a=a_{0}\) and \(b=b_{0}\), but are they really optimal? The answer to this question is generally negative. Figure 1 depicts an example of the phenomenon in image denoising, taken from a real-world application. In this example, the straightforward choice is largely sub-optimal. This suggests that there are always inherent performance leaks for deep neural networks due to the two degrees of freedom induced by the normalization (_i.e._, choice of \(a\) and choice of \(b\)). In addition, this poor conditioning can be a source of confusion and misinterpretation in critical applications.

### Categorizing image denoisers

Table 1 summarizes the equivariance properties of several popular denoisers, either conventional [5; 11; 15; 18; 37; 44] or deep learning-based [24; 45; 46; 47; 26]. Interestingly, if _scale-equivariance_ is generally guaranteed for traditional denoisers, not all of them are equivariant to shifts. In particular, the widely used algorithms DCT  and BM3D  are sensitive to offsets, mainly because the hard thresholding function at their core is not _shift-equivariant_. Regarding the deep-learning-based networks, only DRUNet  is insensitive to scale because it is a bias-free convolutional neural

Figure 1: Influence of normalization for deep-learning-based image denoising. The raw input data is a publicly available real-world noisy image of the _Convallaria_ dataset . “Blind” DnCNN  with official pre-trained weights is used for denoising and is applied on four different normalization intervals displayed in red, each of which being included in \(\) over which it was learned. PSNR is calculated with the average of \(100\) independent noisy static acquisitions of the same sample (called ground truth). Interestingly, the straightforward interval \(\) does not give the best results. Normalization intervals are (a) \(\), (b) \([0.08,0.12]\), (c) \([0.48,0,52]\) and (d) \([0.64,0.96]\). In the light of the denoising results \((b)\)-\((c)\) and \((b)\)-\((d)\), DnCNN is neither _shift-equivariant_, nor _scale-equivariant_.

network with only ReLU activation functions . In particular, all transformer models [7; 24; 26; 45], even bias-free, are not _scale-equivariant_ due to their inherent attention-based modules. In the next section, we show how to adapt existing neural architectures to guarantee _normalization-equivariance_ without loss of performance and study the resulting class of parameterized functions \((f_{})\).

## 4 Design of Normalization-Equivariant Networks

### Affine convolutions

To justify the introduction of a new type of convolutional layers, let us study one of the most basic neural network, namely the linear (parameterized) function \(f_{}:x^{n} x\), where parameters \(\) are a matrix of \(^{m n}\). Indeed, \(f_{}\) can be interpreted as a dense neural network with no bias, no hidden layer and no activation function. Obviously, \(f_{}\) is always _scale-equivariant_, whatever the weights \(\). As for the _shift-equivariance_, a simple calculation shows that:

\[x x} x^{n}, ,(x+_{n})= x+_{m} _{n}=_{m}\,.\] (5)

Therefore, \(f_{}\) is _normalization-equivariant_ if and only if each row of matrix \(\) sums to \(1\). In other words, for the _normalization-equivariance_ to hold, the rows of \(\) must encode weights of affine combinations. Transposing the demonstration to any convolutional neural network follows from the observation that a convolution from an input layer of size \(H W C\) to an output layer of size \(H^{} W^{} C^{}\) can always be represented with a dense connection by vectorizing the input and output layers. The elements of the \(C^{}\) convolutional kernels of size \(k k C\) each are then stored separately along the rows of the (sparse) transition matrix \(\) of size \((H^{} W^{} C^{})(H W C)\). Therefore, a convolutional layer preserves the _normalization-equivariance_ if and only if the weights of the each convolutional kernel sums to \(1\). In the following, we call such convolutional layers "affine convolutions".

In order to guarantee the affine constraint on each convolutional kernel throughout the training phase, one possibility is to "telescope" the circular shifted version of an unconstrained kernel to itself (this way, the sum of the resulting trainable coefficients cancels out) and then add the inverse of the kernel size element-wise as a non-trainable offset. Despite this over-parameterized form (involving an extra degree of freedom), we found this solution to be easier to use in practice. Moreover, it ensures that all coefficients of the affine kernels follow the same law at initialization.

Since _normalization-equivariance_ is preserved through function composition, concatenation and affine combination (see Lemma 2), a (linear) convolutional neural network composed of only affine convolutions with no bias and possibly skip or _affine_ residual connections (trainable affine combination of two layers), is guaranteed to be _normalization-equivariant_, provided that padding is performed with existing features (reflect, replicate or circular padding for example). Obviously, in their current state, these neural networks are of little interest, as linear functions do not encode best-performing functions for many applications, image denoising being no exception. Nevertheless, based on such networks, we show in the next subsection how to introduce nonlinearities without breaking the _normalization-equivariance_.

### Channel-wise sort pooling as a normalization-equivariant alternative to ReLU

The first idea that comes to mind is to apply a nonlinear activation function \(:\) preserving _normalization-equivariance_ after each affine convolution. In other words, we look for a nonlinear solution \(\) of the characteristic functional equation of _normalization-equivariant_ functions (see Def. 1) for \(n=1\). Unfortunately, according to Prop. 2 (see proof in Appendix C.1 which is based on Lemma 1), the unique solution is the identity function which is linear. Therefore, activation functions that apply element-wise are to be excluded.

  & TV & NLM & NLR & DCT & BM3D & WNNM & DnCNN & NLRN & SwinIR & Restormer & DRUNet \\  Scale & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ & ✗ & ✓ \\ Shift & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\  

Table 1: Equivariance properties of several image denoisers (left: traditional, right: learning-based)

**Proposition 2**: _Let \((n)\) be the set of normalization-equivariant functions from \(^{n}\) to \(^{n}\)._

\[(1) =\{x x\}\ \] \[(2) =\{.(x_{1},x_{2}) A( x_{1}\\ x_{2})\ \ x_{1} x_{2}\ \ B(x_{1}\\ x_{2})\ |\ A,B^{2 2}\ \ A _{2}=B_{2}=_{2}\}.\]

To find interesting nonlinear functions, one needs to examine multi-dimensional activation functions, _i.e._ ones of the form \(:^{n}^{m}\) with \(n 2\). In order to preserve the dimensions of the neural layers and to limit the computational costs, we focus on the case \(n=m=2\), meaning that \(\) processes pre-activated neurons by pairs. According to Prop. 2, the _normalization-equivariant_ functions from \(^{2}\) to \(^{2}\) are parameterized by two matrices \(A,B^{2 2}\) such that \(A_{2}=B_{2}=_{2}\) and apply a different (affine-constrained) linear mapping depending on whether or not the input is in ascending order. As long as \(A B\), the resulting function is nonlinear and makes it _de facto_ a candidate to replace the conventional one-dimensional activation functions such as the popular ReLU (rectified linear unit) function. Interestingly, when arbitrarily choosing \(A\) and \(B\) to be the permutation matrices of \(^{2 2}\), the resulting _normalization-equivariant_ function simply reads:

\[:(x_{1},x_{2})^{2}(x_{1},x_{2}) \\ (x_{1},x_{2})\,\] (6)

which is nothing else than the sorting function in \(^{2}\). Clearly, it is among the simplest _normalization-equivariant_ nonlinear function from \(^{2}\) to \(^{2}\) and it is the one we consider as surrogate for the one-dimensional activation functions (choosing other functions, that is considering other choices for \(A\) and \(B\), does not bring improvements in terms of performance in our experiments). More generally, it is easy to show that all the sorting functions of \(^{n}\) are _normalization-equivariant_ and are nonlinear as soon as \(n 2\). Note that such sorting operators have been promoted by  in totally different contexts for their norm-preserving properties of the backpropagated gradients.

Since the sorting function (6) is to be applied on non-overlapping pairs of neurons, the partitioning of layers needs to be determined. In order not to mix unrelated neurons, we propose to apply this two-dimensional activation function channel-wisely across layers and call this operation "sort pooling" in reference to the max pooling operation, widely used for downsampling, and from which it can be effectively implemented. Figure 2 illustrates the sequence of the two proposed innovations, namely affine convolution followed by channel-wise sort pooling, to replace the traditional scheme "conv+ReLU", while guaranteeing _normalization-equivariance_.

### Encoding adaptive affine filters

Based on Lemma 2, we can formulate the following proposition which tells more about the class of parameterized functions \((f_{})\) encoded by the proposed networks.

Figure 2: Illustration of the proposed alternative for replacing the traditional scheme “convolution + element-wise activation function” in convolutional neural networks: affine convolutions supersede ordinary ones by restricting the coefficients of each kernel to sum to one and the proposed sort pooling patterns introduce nonlinearities by sorting two by two the pre-activated neurons along the channels.

**Proposition 3**: _Let \(f_{}^{}:^{n}^{m}\) be a CNN composed of only:_

* _affine convolution kernels with no bias and where padding is made of existing features,_
* _sort pooling nonlinearities,_
* _possibly skip or affine residual connections, and max or average pooling layers._

_Then, \(f_{}^{}\) is a normalization-equivariant continuous piecewise-linear function with finitely many pieces. Moreover, on each piece represented by the vector \(y_{r}\),_

\[f_{}^{}(y)=A_{}^{y_{r}}y,\;\;\;A_{}^{y_ {r}}^{m n}\;\;A_{}^{y_{r}}_{n}= _{m}\,.\]

In Prop. 3, the subscripts on \(A_{}^{y_{r}}\) serve as a reminder that this matrix depends on the sort pooling activation patterns, which in turn depend on both the input vector \(y\) and the weights \(\). As already revealed for bias-free networks with ReLU , \(A_{}^{y_{r}}\) is the Jacobian matrix of \(f_{}^{}\) taken at any point \(y\) in the interior of the piece represented by vector \(y_{r}\). Moreover, as \(A_{}^{y_{r}}_{n}=_{m}\), the output vector of such networks are locally made of fixed affine combinations of the entries of the input vector. And since a CNN has a limited receptive field centered on each pixel, \(f_{}^{}\) can be thought of as an adaptive filter that produces an estimate of each pixel through a custom affine combination of pixels. By examining these filters in the case of image denoising (see Fig. 3), it becomes apparent that they vary in their characteristics and are intricately linked to the contents of the underlying images. Indeed, these filters are specifically designed to cater to the specific local features of the noisy image: averaging is done over uniform areas without affecting the sharpness of edges. Note that this behavior has already been extensively studied by  for unconstrained filters.

The total number of fixed adaptive affine filters depends on the weights \(\) of the network \(f_{}^{}\) and is bounded by \(2^{S}\) where \(S\) represents the total number of sort pooling patterns traversed to get from the receptive filed to its final pixel (assuming no max pooling layers). Obviously, this upper bound grows exponentially with \(S\), suggesting that a limited number of sort pooling operations may generate an extremely large number of filters. Interestingly, if ReLU activation functions where used instead, the upper bound would reach \(2^{2S}\).

## 5 Experimental results

We demonstrate the effectiveness and versatility of the proposed methodology in the case of image denoising. To this end, we modify two well-established neural network architectures for image denoising, chosen for both their simplicity and efficiency, namely DRUNet : a state-of-the-art

Figure 3: Visual comparisons of the generalization capabilities of a _scale-equivariant_ neural network (left) and its _normalization-equivariant_ counterpart (right) for Gaussian noise. Both networks were trained for Gaussian noise at noise level \(=25\) exclusively. The adaptive filters (rows of \(A_{}^{y_{r}}\) in Prop. 3) are indicated for two particular pixels as well as the sum of their coefficients (note that some weights are negative, indicated in red). The _scale-equivariant_ network tends to excessively smooth out the image when evaluated at a lower noise level, whereas the _normalization-equivariant_ network is more adaptable and considers the underlying texture to a greater extent.

U-Net with residual connections ; and FDnCNN, the unpublished flexible variant of the popular DnCNN : a simple feedforward CNN that chains "conv+ReLU" layers with no downsampling, no residual connections and no batch normalization during training , and with a tunable noise level map as additional input . We show that adapting these networks to become _normalization-equivariant_ does not adversely affect performance and, better yet, increases their generalization capabilities. For each scenario, we train three variants of the original Gaussian denoising network for grayscale images: _ordinary_ (original network with additive bias), _scale-equivariant_ (bias-free variation with ReLU ) and our _normalization-equivariant_ architecture (see Fig. 2). Details about training and implementations can be found in Appendix A and Appendix B. Unless otherwise noted, all results presented in this paper are obtained with DRUNet ; similar outcomes can be achieved with FDnCNN  architecture (see Appendix D).

Finally, note that both DRUNet  and FDnCNN  can be trained as "blind" but also as "non-blind" denoisers and thus achieve increased performance, by passing an additional noisemap as input. In the case of additive white Gaussian noise of variance \(^{2}\), the noisemap is constant equal to \(_{n}\) and the resulting parameterized functions can then be put mathematically under the form \(f_{}:(y,)^{n}^{+}_{*}^{n}\). In order to integrate this feature to _normalization-equivariant_ networks as well, a slight modification of the first affine convolutional layer must be made. Indeed, by adapting the proof (5) to the case (1), we can show that the first convolutional layer must be affine with respect to the input image \(y\) only - the coefficients of the kernels acting on the image pixels add up to \(1\) - while the other coefficients of the kernels need not be constrained.

### The proposed architectural modifications do not degrade performance

The performance, assessed in terms of PSNR values, of our _normalization-equivariant_ alternative (see Fig. 2) and of its _scale-equivariant_ and _ordinary_ counterparts is compared in Table 2 for "non-blind" architectures on two popular datasets . We can notice that the performance gap between two different variants is less than 0.05 dB at most for all noise levels, which is not significant. This result suggests that the class of parameterized functions \((f_{})\) currently used in image denoising can drastically be reduced at no cost. Moreover, it shows that it is possible to dispense with activation functions, such as the popular ReLU: nonlinearities can simply be brought by sort pooling patterns. In terms of subjective visual evaluation, we can draw the same conclusion since images produced by two architectural variants inside the training range are hardly distinguishable (see Fig. 3 at \(=25\)).

### Increased robustness across noise levels

S. Mohan _et al._ revealed that bias-free neural networks with ReLU, which are _scale-equivariant_, could much better generalize when evaluated at new noise levels beyond their training range, than their counterparts with bias that systematically overfit. Even if they do not fully elucidate how such networks achieve this remarkable generalization, they suggest that _scale-equivariance_ certainly plays a major role. What about _normalization-equivariance_ then? We have compared the robustness faculties of the three variants of networks when trained at a fixed noise level \(\) for Gaussian noise. Figure

Figure 4: Comparison of the performance of our _normalization-equivariant_ alternative with its _scale-equivariant_ and _ordinary_ counterparts for Gaussian denoising with the same architecture on Set12 dataset. The vertical blue line indicates the unique noise level on which the “blind” networks were trained exclusively (from left to right: \(=50\), \(=25\) and \(=10\)). In all cases, _normalization-equivariant_ networks generalize much more robustly beyond the training noise level.

4 summarizes the explicit results obtained: _normalization-equivariance_ pushes generalization capabilities of neural networks one step further. While performance is identical to their _scale-equivariant_ counterparts when evaluated at higher noise levels, the _normalization-equivariant_ networks are, however, much more robust at lower noise levels. This phenomenon is also illustrated in Fig. 3.

Demystifying robustnessLet \(x\) be a clean patch of size \(n\), representative of the training set on which a CNN \(f_{}\) was optimized to denoise its noisy realizations \(y=x+\) with \((0,^{2}I_{n})\) (denoising at a fixed noise level \(\) exclusively). Formally, we note \(x^{n}\), where \(\) is the space of representative clean patches of size \(n\) on which \(f_{}\) was trained. We are interested in the output of \(f_{}\) when it is evaluated at \(x+\) (denoising at noise level \(\)) with \(>0\). Assuming that \(f_{}\) encodes a _normalization-equivariant_ function, we have:

\[_{*}^{+},,\;f_{}(x+ )= f_{}((x-)/+)+\,.\] (7)

The above equality shows how such networks can deal with noise levels \(\) different from \(\): _normalization-equivariance_ simply brings the problem back to the denoising of an implicitly renormalized image patch with fixed noise level \(\). Note that this artificial change of noise level does not make this problem any easier to solve as the signal-to-noise ratio is preserved by normalization.

   Dataset &  & BSD68 \\    & 15 / 25 / 50 & 15 / 25 / 50 \\   & _ordinary_ & 33.23 / 30.92 / 27.87 & 31.89 / 29.44 / 26.54 \\  & _scale-equiv_ & 33.25 / 30.94 / 27.90 & 31.91 / 29.48 / 26.59 \\  & _norm-equiv_ & 33.20 / 30.90 / 27.85 & 31.88 / 29.45 / 26.55 \\  & _ordinary_ & 32.87 / 30.49 / 27.28 & 31.69 / 29.22 / 26.27 \\ FDnCNN  & _scale-equiv_ & 32.85 / 30.49 / 27.29 & 31.67 / 29.20 / 26.25 \\  & _norm-equiv_ & 32.85 / 30.50 / 27.27 & 31.69 / 29.22 / 26.25 \\  

Table 2: The PSNR (dB) results of “non-blind” deep learning-based methods applied to popular grayscale datasets corrupted by synthetic white Gaussian noise with \(=15\), \(25\) and \(50\).

Obviously, the denoising result of \(x+\) will be all the more accurate as \((x-)/\) is a representative patch of the training set. In other words, if \((x-)/\) can still be considered to be in \(\), then \(f_{}\) should output a consistent denoised image patch. For a majority of methods [46; 47; 48], training is performed within the interval \(\) and therefore \(x/\) still belongs generally to \(\) for \(1<<10\) (contraction), but this is much less true for \(<1\) (stretching) for the reason that it may exceed the bounds of the interval \(\). This explains why _scale-equivariant_ functions do not generalize well to noise levels lower than their training one. In contrast, _normalization-equivariant_ functions can benefit from the implicit extra adjustment parameter \(\). Indeed, there exists some cases where the stretched patch \(x/\) is not in \(\) but \((x-)/\) is (see Fig. 4(b)). This is why _normalization-equivariant_ networks are more able to generalize at low noise levels. Note that, based on this argument, _ordinary_ neural networks trained at a fixed noise level \(\) can also be used to denoise images at noise level \(\), provided that a correct normalization is done beforehand . However, this time the normalization is explicit: the exact scale factor \(\), and possibly the shift \(\), must be known (see Fig. 4(a)).

It turns out that this theoretical argument is valid for a wide range of noise types, not only Gaussian noise. Indeed, the same argument holds for any additive noise \(\) that possesses the scaling property: \(\) belongs to the same family of probability distributions as \(\) (e.g., Gaussian, uniform, Laplace or even Rayleigh noise which is not zero-mean). By the way, the authors of  had already verified the noise generalization capabilities of _scale-equivariant_ networks for uniform noise in addition to Gaussian noise, without fully elucidating why it works. In Appendix D, we checked experimentally that "blind" _normalization-equivariant_ networks trained on additive uniform, Laplace or Rayleigh noise at a single noise level are much more robust at unseen noise levels than their _scale-equivariant_ and _ordinary_ counterparts.

## 6 Conclusion and perspectives

In this work, we presented an original approach to adapt the architecture of existing neural networks so that they become _normalization-equivariant_, a property highly desirable and expected in many applications such that image denoising. We argue that the classical pattern "conv+ReLU" can be favorably replaced by the two proposed innovations: affine convolutions that ensure that all coefficients of the convolutional kernels sum to one; and channel-wise sort pooling nonlinearities as a substitute for all activation functions that apply element-wise, including ReLU or sigmoid functions. Despite these two important architectural changes, we show that the performance of these alternative networks is not affected in any way. On the contrary, thanks to their better-conditioning, they benefit, in the context of image denoising, from an increased interpretability and especially robustness to variable noise levels both in practice and in theory.

## Limitations

We would like to mention that the proposed architectural modifications for enforcing _normalization-equivariance_ require a longer training for achieving comparable performance with its original counterparts (see Appendix B), and may be incompatible with some specific network layers such as batch-norm  or attention-based modules [24; 26; 7; 45]. Moreover, our method has shown its potential mainly to image denoising as it stands, even though in principle _normalization-equivariance_ may be applicable and helpful in other tasks as well (see preliminary results about image classification in Appendix D). Discovering similar advantages of _normalization-equivariance_ in other computer vision tasks, possibly related to outlier robustness, is an interesting avenue of research for future work.