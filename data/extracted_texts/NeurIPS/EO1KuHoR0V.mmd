# AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models

Yuancheng Wang\({}^{12}\), Zeqian Ju\({}^{1}\), Xu Tan\({}^{1}\), Lei He\({}^{1}\), Zhizheng Wu\({}^{2}\), Jiang Bian\({}^{1}\), Sheng Zhao\({}^{1}\)

\({}^{1}\)Microsoft, \({}^{2}\)The Chinese University of Hong Kong, Shenzhen

\({}^{1}\){v-yuancwang,v-zeqianju,xuta,helei,jiabia,szhao}@microsoft.com

\({}^{2}\)yuanchengwang@link.cuhk.edu.cn,wuzhizheng@cuhk.edu.cn

Work done as an intern at Microsoft.

###### Abstract

Audio editing is applicable for various purposes, such as adding background sound effects, replacing a musical instrument, and repairing damaged audio. Recently, some diffusion-based methods achieved zero-shot audio editing by using a diffusion and denoising process conditioned on the text description of the output audio. However, these methods still have some problems: 1) they have not been trained on editing tasks and cannot ensure good editing effects; 2) they can erroneously modify audio segments that do not require editing; 3) they need a complete description of the output audio, which is not always available or necessary in practical scenarios. In this work, we propose **AUDIT**, an instruction-guided audio editing model based on latent diffusion models. Specifically, AUDIT has three main design features: 1) we construct triplet training data (instruction, input audio, output audio) for different audio editing tasks and train a diffusion model using instruction and input (to be edited) audio as conditions and generating output (edited) audio; 2) it can automatically learn to only modify segments that need to be edited by comparing the difference between the input and output audio; 3) it only needs edit instructions instead of full target audio descriptions as text input. AUDIT achieves state-of-the-art results in both objective and subjective metrics for several audio editing tasks (e.g., adding, dropping, replacement, inpainting, super-resolution). Demo samples are available at https://audit-demepage.github.io/.

Figure 1: AUDIT consists of a VAE, a T5 text encoder, and a diffusion network, and accepts the mel-spectrogram of the input audio and the edit instructions as conditional inputs and generates the edited audio as output.

Introduction

Audio editing is a crucial step in producing high-quality audio or video content, which involves tasks like adding background sound effects, replacing background music, repairing incomplete audio, and enhancing low-sampled audio. Typically, these tasks require professional software and manual operations. However, if audio editing tools can be empowered to follow human instructions, it could significantly reduce the need for manual intervention and benefit content creators. The utilization of natural language instructions such as "add jazz music in the background" could enable more adaptable editing in line with human expectations. In this work, we concentrate on designing audio editing models that can follow human instructions.

Previous works on audio editing usually leverage traditional signal processing technologies. Later, GAN-based methods [33; 10; 1] have achieved success in some audio editing tasks such as audio inpainting and super-resolution. However, these methods are usually designed for a specific editing task. Recently, the most advanced deep generative models, diffusion models, have achieved great success in image editing. Some methods [18; 27] have attempted to apply diffusion models to the field of audio editing. These diffusion-based audio editing methods are primarily based on a pre-trained text-to-audio generation model. However, these methods still suffer from several issues: 1) they are mainly based on a pre-trained model to noise and denoise an input audio conditioned on a target text description, and in most cases, they offer no guarantee to achieve good editing effect since they are not trained directly on audio editing tasks; 2) they can lead to erroneous modifications on some audio segments that do not need to be edited, as it is difficult to restrict the editing area; 3) they require a complete description of the output audio, which is not always available or necessary in real-world editing scenarios. Instead, it is desirable to provide natural language editing instructions such as "add a man whistling in the background" or "drop the sound of the guitar" for audio editing.

To solve the above issues, we propose AUDIT, to the best of our knowledge, the first audio editing model based on human instructions. As shown in Figure 1, AUDIT is a latent diffusion model which takes the audio to be edited and the editing instruction as conditions and generates the edited audio as output. The core designs of AUDIT can be summarized in three points: 1) we generate triplet data (instruction, input audio, output audio) for different audio editing tasks, and train an audio editing diffusion model in a supervised manner; 2) we directly use input audio as conditional input to train the model, forcing it to automatically learn to ensure that the audio segments that do not need to be edited remain consistent before and after editing; 3) our method uses editing instructions directly as text guidance, without the need for a complete description of the output audio, making it more flexible and suitable for real-world scenarios. Our editing model achieves state-of-the-art results in objective and subjective metrics.

Our contributions can be summarized as follows:

* We propose AUDIT, which demonstrates the first attempt to train a latent diffusion model conditioned on human text instructions for audio editing.
* To train AUDIT in a supervised manner, we design a novel data construction framework for each editing task; AUDIT can maximize the preservation of audio segments that do not need to be edited; AUDIT only requires simple instructions as text guidance, without the need for a complete description of the editing target.
* AUDIT achieves state-of-the-art results in both objective and subjective metrics for several audio editing tasks.

## 2 Related Works

### Audio/Speech Editing

Previous work related to audio editing has primarily focused on human speech or music. [36; 5; 33; 55] explored the task of speech/music inpainting based on the corresponding text or music score. Another category of editing work is focused on style transfer for speech or music. Speech voice conversion [2; 35; 44; 43] and singing voice conversion [7; 28] aim to modify the timbre without changing the speech or singing content. Music style transfer [51; 9; 30; 40] aims to change the instrument being played without modifying the music score. In addition, some methods using GANs [33; 10; 1] have achieved success in some specific audio editing tasks, such as audio inpainting and audio super-resolution. However, there is still a lack of research on general audio editing following human instructions.

### Diffusion-based Editing

Diffusion-based editing tasks have received similar attention as diffusion-based generation tasks. We summarize the common diffusion-based editing work into two types.

Zero-Shot EditingSome methods [34; 8; 4; 31; 14] use diffusion-based generate models to achieve zero-shot editing. SDEdit  uses a pre-trained diffusion model to add noise to an input image and then denoise the image with a new target text. These methods have difficulty in editing specific regions and have poor controllability. To edit specific regions,  uses an additional loss gradient to guide the sampling process, and [4; 31] replace unmodified parts with the noisy input image in each sampling step, equivalent to only generating in the masked part. However, these methods are only for inpainting tasks and also need a complete description of the editing target.

Supervised TrainingAnother type of methods [47; 6; 54; 37] handle editing tasks in a supervised manner.  train an image-to-image diffusion model which takes the image to be edited as a condition.  uses paired image data generated by  to train a text-based image editing model. ControlNet  and T2I-Adapter  add an adapter model to a frozen pre-trained generation diffusion model and train the adapter model to achieve image editing. Our method follows the supervised training paradigm, using the generated triplet data (instruction, input audio, output audio) to train an audio editing model conditioned on the instruction and the audio to be edited, and can handle different audio editing tasks by following human instructions.

### Text-Guided Audio Generation

Currently, most diffusion-based audio editing methods [18; 27] are based on text-to-audio diffusion generative models. Text-to-audio generation aims to synthesize general audio that matches the text description. DiffSound  is the first attempt to build a text-to-audio system based on a discrete diffusion model . AudioGen  is another text-to-audio generation model based on a Transformer-based decoder that uses an autoregressive structure. AudioGen directly predicts discrete tokens obtained by compressing from the waveform . Recently, Make-an-Audio  and AudioLDM  have attempted to build text-guided audio generation systems in continuous space, which use an autoencoder to convert mel-spectrograms into latent representation, build a diffusion model that works on the latent space, and reconstruct the predicted results of the latent diffusion model as mel-spectrograms using the autoencoder. Furthermore, MusicLM  and Noise2Music  generate music that matches the semantic information in the input text. In our work, we focus on editing the existing audio with human instructions.

## 3 Method

In this section, we present our method for implementing audio editing based on human instructions. We provide an overview of our system architecture and training objectives in Section 3.1 and show the details about how we generate triplet data (instruction, input audio, output audio) for each editing task in Section 3.2. Lastly, we discuss the advantages of our method in Section 3.3.

### System Overview

Our system consists of an autoencoder that projects the input mel-spectrograms to an efficient, low-dimensional latent space and reconstructs it back to the mel-spectrograms, a text encoder that encodes the input text instructions, a diffusion network for editing in the latent space, and a vocoder for reconstructing waveforms. The overview of the system is shown in Figure 2.

AutoencoderThe autoencoder contains an encoder \(E\) and a decoder \(G\). The encoder \(E\) transforms the mel-spectrogram \(\) into a latent representation \(\), and the decoder \(G\) reconstructs \(}\) from the latent space. In our work, we employ a variational autoencoder (VAE)  model as the autoencoder. We train our autoencoder with the following loss: 1) The \(_{1}\) and \(_{2}\) reconstruction loss. 2) The Kullback-Leibler loss \(_{KL}\) to regularize the latent representation \(\). We only assign a small weight \(_{KL}\) to the KL loss following ; 3) The GAN loss \(_{GAN}\). We employ a patch-based discriminator \(D\)[19; 45] to distinguish between real and reconstructed mel-spectrograms. The total training loss of the autoencoder can be expressed as \(_{VAE}=_{1}_{1}+_{2}_{2}+ _{KL}_{KL}+_{GAN}_{GAN}\). \(_{1}\), \(_{2}\), \(_{KL}\), and \(_{GAN}\) are the weights of \(_{1}\), \(_{2}\), \(_{KL}\), and \(_{GAN}\) respectively.

Text EncoderWe use a pre-trained language model T5  as our text encoder, which is used to convert text input into embeddings that contain rich semantic information. The parameters of the text encoder are frozen in the training stage.

Latent Diffusion ModelOur text-guided audio editing model can be seen as a conditional latent diffusion model that takes the latent representation \(_{in}\) of the input (to be edited) mel-spectrogram \(_{in}\) and text embeddings as conditions. The model aims to learn \(p(_{out}|_{in},c_{text})\), where \(_{out}\) is the latent representation of the output (edited) mel-spectrogram \(_{out}\), and \(c_{text}\) is the embedding of the editing instruction. Given the latent representation \(_{in}\) and the text instruction, we randomly select a time step \(t\) and a noise \(\) to generate a noisy version \(_{t}\) of the latent representation \(_{out}\) by using a noise schedule and then use the diffusion network \(}(_{t},t,_{in},c_{text})\) to predict the sampled noise. The training loss is \(_{LDM}=_{(_{in},_{out},text})_{ (0,I)}_{t}||}(_{t},t,_{in},c_{text})-||_{2}\).

Similarly to the original standard diffusion model [15; 49], we use a U-Net  with the cross-attention mechanism  as the diffusion network. Our editing model takes \(_{in}\) as a condition by directly concatenating \(_{t}\) and \(_{in}\) at the channel level. Therefore, the input channel number of the first layer of the U-Net is twice the output channel number of the last layer.

VocoderA vocoder model is required to convert the output mel-spectrogram into audio. In this work, we use HiFi-GAN  as the vocoder, which is one of the most widely used vocoders in the field of speech synthesis. HiFi-GAN uses multiple small sub-discriminators to handle different cycle patterns. Compared with some autoregressive [38; 20] or flow-based  vocoders, it takes into account both generation quality and inference efficiency.

### Generating Triplet Training Data for Each Editing Task

We use generated triplet data (instruction, input audio, output audio) to train our text-guided audio editing model. In this work, we focus on five different editing tasks, including adding, dropping, replacement, inpainting, and super-resolution. Note that we train all the editing tasks in a single editing model. Figure 3 provides an overview of how our data generation workflow for different editing tasks works.

AddingWe randomly select two audio clips \(A\) and \(B\) from the text-audio datasets, then combine \(A\) and \(B\) to get a new audio clip \(C\). We use \(A\) as the input audio and \(C\) as the output audio, and we fill in the caption (or label) of \(B\) into the instruction template to get the instruction. For example, as shown in Figure 3, the instruction template is _"Add {} in the background"_ and the caption of

Figure 2: A high-level overview of our system.

is _"Baby crying"_, so the instruction is _"Add baby crying in the background"_. Note that we design multiple templates for different editing tasks.

DroppingWe randomly select two audio clips \(A\) and \(B\) from the text-audio datasets, then combine \(A\) and \(B\) to get a new audio clip \(C\). We use \(C\) as the input audio and \(A\) as the output audio, and we fill in the caption (or label) of \(B\) into the instruction template to get the instruction. For example, the instruction template is _"Drop {}"_ and the caption of \(B\) is _"Dog barking"_, so the instruction is _"Drop Dog barking"_.

ReplacementFor the replacement task, we select three audio clips \(A\), \(B\), and \(C\) from the datasets, then insert \(B\) and \(C\) separately into \(A\) (in roughly the same area) to obtain two new audio clips \(D\) and \(E\). We use \(D\) as the input audio and \(E\) as the output audio, and we fill in the captions (or labels) of \(B\) and \(C\) into the instruction template to get the instruction. For example, the instruction template is _"Replace [] with {}"_, the caption of \(B\) is _"Someone clapping"_, and the caption of \(C\) is _"The sound of guitar"_, so the instruction is _"Replace clapping with guitar"_.

Inpainting and Super-ResolutionFor the inpainting tasks, we select an audio clip as the output audio and randomly mask some parts of the audio to get a new audio clip as the input audio. We can use instructions like _"Inpaint"_ or _"Inpaint the audio"_ directly or use the instructions like _"Inpaint: a cat meowing"_. For the super-resolution tasks, we use down-sampled audio as the input audio. The instructions are as _"Increase resolution"_ or _"Increase resolution: a bird singing"_.

Extending Instructions with ChatGPTWe design multiple instruction templates for each editing task. In order to further expand the diversity of editing instructions, we use ChatGPT  to generate more editing instruction templates for us. See more details in Appendix E.

### Advantages of AUDIT

We analyze the advantages of our methods over previous works on three key points. 1) We generated triplet data (instruction, input data, output data) to train a text-guided audio editing model instead of performing zero-shot editing, which can ensure good edit quality. 2) We directly use the input audio as a condition for supervised training of our diffusion model, allowing the model to automatically learn to preserve the parts that do not need to be edited before and after editing. Specifically, we concatenate the latent representations \(_{in}\) of the input audio and \(_{t}\) on the channel dimension and input them into the latent diffusion model, so that the model can "see" \(_{in}\) (rather than its noisy version) during both training and inference. 3) Instead of using a full description of the output audio, we use human instructions as the text input, which is more in line with real-world applications.

Figure 3: Examples about how to generate triplet training data for different audio editing tasks (adding, dropping, replacement, inpainting, and super-resolution).

Experimental Settings

### Dataset

The datasets used in our work consist of AudioCaps , AudioSet , FSD50K , and ESC50 . AudioSet is the largest audio-label pairs dataset; however, each audio clip in AudioSet only has a corresponding label and no description. Because of the presence of a large number of human speech and some audio clips that are only weakly related to their labels in AudioSet, we use a subset of AudioSet (AudioSet96) that includes approximately 149K pairs with 96 classes. AudioCaps is a dataset of around 44K audio-caption pairs, where each audio clip corresponds to a caption with rich semantic information. The length of each audio clip in AudioSet and AudioCaps is around 10 seconds. FSD50K includes around 40K audio clips and 200 audio-label pairs of variable lengths ranging from 0.3 to 30 seconds. We split FSD50K training set into two datasets, FSD50K-L (19K) and FSD50K-S (22K), comprising audio clips of length less than and greater than 5 seconds, respectively. ESC50 is a smaller dataset with 50 classes, each containing four audio clips of around 5 seconds, for a total of 2000 audio clips.

We use AudioCaps, AudioSet96, FSD50K-S, and ESC50 to generate triplet training data for five audio editing tasks. We use a total of about 0.6M triplet data to train our audio editing model. More details about datasets and data processing are shown in Appendix A.

### Baseline Systems

Text-to-Audio GenerationSince we compare with generative model-based audio editing baseline methods, we use AudioCaps, AudioSet96, FSD50K, and ESC50 to train a text-to-audio latent diffusion model and use the test set of AudioCaps to evaluate the text-to-audio model. To demonstrate the performance of our generative model, we compare it with some state-of-the-art text-to-audio generative models [52; 26; 27; 18].

Adding, Dropping, and ReplacementWe use methods like SDEdit as baselines. SDEdit uses a pre-trained text-to-image diffusion model to noise and then denoise an input image with the editing target description. In our work, we directly use SDEdit in the adding task, dropping task, and replacement task. We use our own trained text-to-audio latent diffusion model. We test different total denoising steps \(N\) for \(N=3/4T\), \(N=1/2T\), and \(N=1/4T\), where \(T\) is the total step in the forward diffusion process. The three baselines are called: 1) _SDEdit-3/4T_; 2) _SDEdit-1/2T_; 3) _SDEdit-1/4T_.

InpaintingFor the inpainting task, we designed four baselines derived from SDEdit but with some differences between each other. 1) _SDEdit_. We directly use SDEdit, we first add noise to the input audio, then denoise the audio with the description (caption or label) of the output audio as text guidance. 2) _SDEdit-Rough_ and 3) _SDEdit-Precise_. Only a part of the input audio (the part that is masked) needs to be edited in the inpainting task, we call the part that does not need to be edited the "observable" part, and we call the masked part the "unobservable" part. In each step of the denoising process, we can replace the "observable" part with the ground truth in the latent space. The difference between SDEdit-Rough and SDEdit-Precise is that in SDEdit-Rough, the "unobservable" part is a rough region, while in SDEdit-Precise, the "unobservable" part is a precise region. 4) _SDEdit-wo-Text_. SDEdit-wo-Text is similar to SDEdit-Precise, however, it has no text input. An example of the difference of the "unobservable" part between SDEdit-Rough and SDEdit-Precise is shown in Appendix F. We also compared our methods with two task-specific methods GACELA  and DAS . GACELA is a generative adversarial network for restoring missing musical audio data. The publicly available checkpoint was trained solely on music data; hence, we trained the model using our own dataset. DAS is another GAN-based audio editing model, we also trained it on our dataset.

Super-ResolutionSuper-resolution can be seen as inpainting in the frequency domain. We use three baselines similar to the inpainting task: 1) _SDEdit_; 2) _SDEdit-Precise_; 3) _SDEdit-wo-Text_. We also conducted a comparison with the NU-Wave2  model, which is a task-specific model designed for super-resolution. We utilized the provided open-source checkpoint for NU-Wave2; however, note that NU-Wave2 was originally trained on speech data. To adapt it to our specific task, we trained it on our own dataset.

### Evaluation Metrics

Objective MetricsWe use log spectral distance (LSD), frechet distance (FD), and kullback-leibler divergence (KL) to evaluate our text-guided audio editing model and use inception score (IS), FD, and KL to evaluate our text-to-audio generation model following [52; 26; 27; 18]. LSD measures the distance between frequency spectrograms of output samples and target samples. FD measures the fidelity between generated samples and target samples. IS measures the quality and diversity of generated samples. KL measures the correlation between output samples and target samples. FD, IS, and KL are based on the state-of-the-art audio classification model PANNs . We use the evaluation pipeline 2 provided by  for objective evaluation for fairer comparisons.

Subjective MetricsWe use overall quality (Quality) to measure _the sound quality and naturalness of the output audio compared to the input audio_ and use relevance to the editing instruction (Relevance) to measure _how well the output audio matches the input human instruction_. Each sample will be scored from 1 to 100 based on Quality and Relevance. See more details about the evaluation in Appendix D.

### Model Configurations

Our autoencoder compresses a mel-spectrogram of size \(1 H W\) into a latent representation of size \(4\). Our models are trained on 8 NVIDIA V100 GPUs for 500K steps with a batch size of 2 on each device. We use the weights of our pre-trained text-to-audio model to initialize our audio editing model. Our HiFi-GAN vocoder is trained on AudioSet96 and AudioCaps datasets using 8 NVIDIA V100 GPU for 200 epochs. More details about model configurations are shown in Appendix B.

## 5 Results

### Objective Evaluation

AddingTable 1 shows the objective evaluation results of the adding task. Our method achieves the best performance, with LSD of 1.35, KL of 0.92, and FD of 21.80. Compared with the best baseline method, our method reduces FD by 6.45 and KL by 0.38.

DroppingTable 2 shows the objective evaluation results of the dropping task. Our method achieves the best performance, with LSD of 1.37, KL of 0.95, and FD of 22.40. Compared with the best baseline method, our method reduces FD by 5.79 and KL by 0.10.

   Model & Text & LSD(\(\)) & KL(\(\)) & FD(\(\)) \\  SDEdit-3/4T & caption & 1.54 & 1.68 & 28.87 \\ SDEdit-1/2T & caption & 1.43 & 1.38 & 28.75 \\ SDEdit-1/4T & caption & 1.38 & 1.30 & 28.25 \\  AUDIT & instruction & **1.35** & **0.92** & **21.80** \\   

Table 1: Objective evaluation results of the adding task.

   Model & Text & LSD(\(\)) & KL(\(\)) & FD(\(\)) \\  SDEdit-3/4T & caption & 1.54 & 1.14 & 29.66 \\ SDEdit-1/2T & caption & 1.43 & 1.05 & 28.19 \\ SDEdit-1/4T & caption & 1.40 & 1.30 & 31.31 \\  AUDIT & instruction & **1.37** & **0.95** & **22.40** \\   

Table 2: Objective evaluation results of the dropping task.

ReplacementTable 3 shows the objective evaluation results of the replacement task. Our method achieves the best performance, with LSD of 1.37, KL of 0.84, and FD of 21.65. Compared with the best baseline method, our method reduces FD by 5.07 and KL by 0.31.

InpaintingTable 4 shows the objective evaluation results of the inpainting task. Our method achieves the best performance, with LSD of 1.32, KL of 0.75, and FD of 18.17. We find that for baseline methods, not providing a text input (the caption of the audio) as guidance leads to a large performance drop, SDEdit-wo-Text gets the worst performance in terms of KL and FD among the baseline methods. However, AUDIT-wo-Text which only uses instructions like _"inpaint"_ achieves performance close to AUDIT which uses instructions like _"inpaint + caption"_. We also discovered that AUDIT's performance on inpainting tasks can surpass that of task-specific baseline methods. One possible explanation for this is that during training, GACELA and DAS do not have access to any textual information, whereas our model requires explicit instructions to edit the audio. As a result, our model is better able to learn the semantic information of the audio, which likely contributes to its improved performance in inpainting tasks.

Super-ResolutionTable 5 shows the objective evaluation results of the super-resolution task. Our method achieves the best performance, with LSD of 1.48, KL of 0.73, and FD of 18.14. Similar to the inpainting task, SDEdit-wo-Text gets the worst performance in terms of KL and FD among the baseline methods. Our method can achieve significantly better results than baselines using only simple instructions like _"Increase resolution"_, which shows that our method can learn sufficient semantic information from low-frequency information. The results also indicate that our AUDIT model outperforms the NU-Wave2 model trained on speech and is comparable to or even better than the NU-Wave2 model trained on our dataset.

Text-to-Audio GenerationWe also present the comparison results of our text-to-audio latent diffusion model with other text-to-audio models in Table 6. Our model achieves the best performance, with FD of 20.19, KL of 1.32, and IS of 9.23, outperforming AudioLDM-L-Full with FD of 23.31, KL of 1.59, and IS of 8.13. This demonstrates that our generation model can serve as a strong baseline model for generation-based editing methods.

### Subjective Evaluation

The results of the subjective evaluation are shown in Table 7. We choose the best results in the baseline to report. Our method clearly outperforms the baseline methods in both Quality and Relevance across

   Model & Text & LSD(\(\)) & KL(\(\)) & FD(\(\)) \\  SDEdit & caption & 2.91 & 1.47 & 25.42 \\ SDEdit-Rough & caption & 1.64 & 0.98 & 21.99 \\ SDEdit-Precise & caption & 1.54 & 0.94 & 21.07 \\ SDEdit-wo-Text & - & 1.55 & 1.63 & 27.63 \\ GACELA & - & 1.41 & 0.78 & 20.49 \\ DAS & - & 1.57 & 0.89 & 21.97 \\  AUDIT-wo-Text & instruction & 1.37 & 0.81 & 19.03 \\ AUDIT & instruction + caption & **1.32** & **0.75** & **18.17** \\   

Table 4: Objective evaluation results of the inpainting task.

   Model & Text & LSD(\(\)) & KL(\(\)) & FD(\(\)) \\  SDEdit-3/4T & caption & 1.63 & 1.58 & 28.78 \\ SDEdit-1/2T & caption & 1.52 & 1.27 & 27.71 \\ SDEdit-1/4T & caption & 1.46 & 1.15 & 26.72 \\  AUDIT & instruction & **1.37** & **0.84** & **21.65** \\   

Table 3: Objective evaluation results of the replacement task.

all five tasks. In the dropping task, our method achieves the highest scores with Quality of 78.1 and Relevance of 81.0. We find that compared with the baseline methods, our method improves more significantly on the adding, dropping, and replacement tasks. The possible reason is that compared with the inpainting and super-resolution tasks, which have explicit positioning of the editing area (the masked part and the high-frequency area), the adding, dropping, and replacement tasks need to first locate the region to be edited, and also need to ensure that other regions cannot be modified, which is more difficult for the baseline method. Our model has learned this ability through supervised learning.

### Case Study

In order to more specifically show the difference between the performance of our model and the baseline methods, we also give some case studies in Appendix G.

   Model & Dataset & FD(\(\)) & IS(\(\)) & KL(\(\)) \\  DiffSound  & AudioSet+AudioCaps & 47.68 & 4.01 & 2.52 \\ AudioGen  & AudioSet+AudioCaps & - & - & 2.09 \\ Make-an-Audio & AudioSet+AudioCaps+13 others & - & - & 2.79 \\ AudioLDM-L  & AudioCaps & 27.12 & 7.51 & 1.86 \\ AudioLDM-L-Full  & AudioSet+AudioCaps+2 others & 23.31 & 8.13 & 1.59 \\  AUDIT & AudioSet96+AudioCaps+2 others & **20.19** & **9.23** & **1.32** \\   

Table 6: The comparison between our text-to-audio generative model and other baseline models on the AudioCaps test set.

   Method & Task & Quality(\(\)) & Relevance(\(\)) \\  Baseline &  & 60.2 & 56.7 \\ AUDIT & & **75.5** & **77.3** \\  Baseline &  & 54.4 & 48.2 \\ AUDIT & & **78.1** & **81.0** \\  Baseline &  & 57.7 & 47.6 \\ AUDIT & & **72.5** & **74.5** \\  Baseline &  & 65.8 & 66.3 \\ AUDIT & & **75.2** & **78.7** \\  Baseline &  & 61.8 & 59.9 \\ AUDIT & & **74.6** & **76.3** \\   

Table 7: Subjective evaluation. For each audio editing task, we report the results of the best baseline method and the results of our method.

   Model & Text & LSD(\(\)) & KL(\(\)) & FD(\(\)) \\  SDEdit & caption & 3.14 & 1.50 & 25.31 \\ SDEdit-Precise & caption & 1.75 & 1.17 & 27.81 \\ SDEdit-wo-Text & - & 1.78 & 1.56 & 32.66 \\ NU-Wave2 (origin) & - & 1.66 & 0.89 & 22.61 \\ NU-Wave2 & - & **1.42** & 1.78 & 19.57 \\  AUDIT-wo-Text & instruction & 1.53 & 0.92 & 21.97 \\ AUDIT & instruction + caption & 1.48 & **0.73** & **18.14** \\   

Table 5: Objective evaluation results of the super-resolution task.

Conclusions

In this work, we propose an audio editing model called AUDIT, which can perform different editing tasks (e.g., adding, dropping, replacement, inpainting, super-resolution) based on human text instructions. Specifically, we train a text-guided latent diffusion model using our generated triplet training data (instruction, input audio, output audio), which only requires simple human instructions as guidance without the need for the description of the output audio and performs audio editing accurately without modifying audio segments that do not need to be edited. AUDIT achieves state-of-the-art performance on both objective and subjective metrics for five different audio editing tasks. For future work, we will explore more editing audio tasks with our framework, and achieve more precise control for audio editing. We also discuss some limitations and broader impacts of our work in Appendix H.