# Generalization bounds for neural ordinary differential equations and deep residual networks

Pierre Marion

Sorbonne Universite, CNRS,

Laboratoire de Probabilites, Statistique et Modelisation, LPSM,

F-75005 Paris, France

pierre.marion@sorbonne-universite.fr

###### Abstract

Neural ordinary differential equations (neural ODEs) are a popular family of continuous-depth deep learning models. In this work, we consider a large family of parameterized ODEs with continuous-in-time parameters, which include time-dependent neural ODEs. We derive a generalization bound for this class by a Lipschitz-based argument. By leveraging the analogy between neural ODEs and deep residual networks, our approach yields in particular a generalization bound for a class of deep residual networks. The bound involves the magnitude of the difference between successive weight matrices. We illustrate numerically how this quantity affects the generalization capability of neural networks.

## 1 Introduction

Neural ordinary differential equations (neural ODEs, Chen et al., 2018) are a flexible family of neural networks used in particular to model continuous-time phenomena. Along with variants such as neural stochastic differential equations (neural SDEs, Tzen and Raginsky, 2019) and neural controlled differential equations (Kidger et al., 2020), they have been used in diverse fields such as pharmokinetics (Lu et al., 2021; Qian et al., 2021), finance (Gierjatowicz et al., 2020), and transportation (Zhou et al., 2021). We refer to Massaroli et al. (2020) for a self-contained introduction to this class of models.

Despite their empirical success, the statistical properties of neural ODEs have not yet been fully investigated. What is more, neural ODEs can be thought of as the infinite-depth limit of (properly scaled) residual neural networks (He et al., 2016), a connection made by, e.g., E (2017); Haber and Ruthotto (2017); Lu et al. (2017). Since standard measures of statistical complexity of neural networks grow with depth (see, e.g., Bartlett et al., 2019), it is unclear why infinite-depth models, including neural ODEs, should enjoy favorable generalization properties.

To better understand this phenomenon, our goal in this paper is to study the statistical properties of a class of time-dependent neural ODEs that write

\[dH_{t}=W_{t}(H_{t})dt,\] (1)

where \(W_{t}^{d d}\) is a weight matrix that depends on the time index \(t\), and \(:\) is an activation function applied component-wise. Time-dependent neural ODEs were first introduced by Massaroli et al. (2020) and generalize time-independent neural ODEs

\[dH_{t}=W(H_{t})dt,\] (2)

as formulated in Chen et al. (2018), where \(W^{d d}\) now denotes a weight matrix independent of \(t\). There are two crucial reasons to consider time-dependent neural ODEs rather than the morerestrictive class of time-independent neural ODEs. On the one hand, the time-dependent formulation is more flexible, leading to competitive results on image classification tasks (Queiringa et al., 2020, 2021). As a consequence, obtaining generalization guarantees for this family of models is a valuable endeavor by itself. On the other hand, time dependence is required for the correspondence with general residual neural networks to hold. More precisely, the time-dependent neural ODE (1) is the limit, when the depth \(L\) goes to infinity, of the deep residual network

\[H_{k+1}=H_{k}+W_{k+1}(H_{k}), 0 k L-1,\] (3)

where \((W_{k})_{1 k L}^{d d}\) are weight matrices and \(\) is still an activation function. We refer to Marion et al. (2022, 2023); Sander et al. (2022); Thorpe and van Gennip (2022) for statements that make precise under what conditions and in which sense this limit holds, as well as its consequences for learning. These two key reasons compel us to consider the class of time-dependent ODEs (1) for our statistical study, which in turn will inform us on the properties of the models (2) and (3).

In fact, we extend our study to the larger class of _parameterized ODEs_, which we define as the mapping from \(x^{d}\) to the value at time \(t=1\) of the solution of the initial value problem

\[H_{0}=x, dH_{t}=_{i=1}^{m}_{i}(t)f_{i}(H_{t})dt,\] (4)

where \(H_{t}\) is the variable of the ODE, \(_{i}\) are functions from \(\) into \(\) that parameterize the ODE, and \(f_{i}\) are fixed functions from \(^{d}\) into \(^{d}\). Time-dependent neural ODEs (1) are obtained by setting a specific entrywise form for the functions \(f_{i}\) in (4).

Since the parameters \(_{i}\) belong to an infinite-dimensional space, in practice they need to be approximated in a finite-dimensional basis of functions. For example, the residual neural networks (3) can be seen as an approximation of the neural ODEs (1) on a piecewise-constant basis of function. But more complex choices are possible, such as B-splines (Yu et al., 2022). However, the formulation (4) is agnostic from the choice of finite-dimensional approximation. This more abstract point of view is fruitful to derive generalization bounds, for at least two reasons. First, the statistical properties of the parameterized ODEs (4) only depend on the characteristics of the functions \(_{i}\) and not on the specifics of the approximation scheme, so it is more natural and convenient to study them at the continuous level. Second, their properties can then be transferred to any specific discretization, such as the deep residual networks (3), resulting in generalization bounds for the latter.

Regarding the characteristics of the functions \(_{i}\), we make the structural assumption that they are Lipschitz-continuous and uniformly bounded. This is a natural assumption to ensure that the initial value problem (4) has a unique solution in the usual sense of the Picard-Lindelof theorem (Arnold, 1992). Remarkably, this assumption on the parameters also enables us to obtain statistical guarantees despite the fact that we are working with an infinite-dimensional set of parameters.

Contributions.We provide a generalization bound for the large class of parameterized ODEs (4), which include time-dependent and time-independent neural ODEs (1) and (2). To the best of our knowledge, this is the first available bound for neural ODEs in supervised learning. By leveraging on the connection between (time-dependent) neural ODEs and deep residual networks, our approach allows us to provide a depth-independent generalization bound for the class of deep residual networks (3). The bound is precisely compared with earlier results. Our bound depends in particular on the magnitude of the difference between successive weight matrices, which is, to our knowledge, a novel way of controlling the statistical complexity of neural networks. Numerical illustration is provided to show the relationship between this quantity and the generalization ability of neural networks.

Organization of the paper.Section 2 presents additional related work. In Section 3, we specify our class of parameterized ODEs, before stating the generalization bound for this class and for neural ODEs as a corollary. The generalization bound for residual networks is presented in Section 4 and compared to other bounds, before some numerical illustration. Section 5 concludes the paper. The proof technique is discussed in the main paper, but the core of the proofs is relegated to the Appendix.

Related work

Hybridizing deep learning and differential equations.The fields of deep learning and dynamical systems have recently benefited from sustained cross-fertilization. On the one hand, a large line of work is aimed at modeling complex continuous-time phenomena by developing specialized neural architectures. This family includes neural ODEs, but also physics-informed neural networks (Raissi et al., 2019), neural operators (Li et al., 2021) and neural flows (Bilos et al., 2021). On the other hand, successful recent advances in deep learning, such as diffusion models, are theoretically supported by ideas from differential equations (Huang et al., 2021).

Generalization for continuous-time neural networks.Obtaining statistical guarantees for continuous-time neural networks has been the topic of a few recent works. For example, Fermanian et al. (2021) consider recurrent neural networks (RNNs), a family of neural networks handling time series, which is therefore a different setup from our work that focuses on vector-valued inputs. These authors show that a class of continuous-time RNNs can be written as input-driven ODEs, which are then proved to belong to a family of kernel methods, which entails a generalization bound. Lim et al. (2021) also show a generalization bound for ODE-like RNNs, and argue that adding stochasticity (that is, replacing ODEs with SDEs) helps with generalization. Taking another point of view, Yin et al. (2021) tackle the separate (although related) question of generalization when doing transfer learning across multiple environments. They propose a neural ODE model and provide a generalization bound in the case of a linear activation function. Closer to our setting, Hanson and Raginsky (2022) show a generalization bound for parameterized ODEs for manifold learning, which applies in particular for neural ODEs. Their proof technique bears similarities with ours, but the model and task differ from our approach. In particular, they consider stacked time-independent parameterized ODEs, while we are interested in a time-dependent formulation. Furthermore, these authors do not discuss the connection with residual networks.

Lipschitz-based generalization bounds for deep neural networks.From a high-level perspective, our proof technique is similar to previous works (Bartlett et al., 2017; Neyshabur et al., 2018) that show generalization bounds for deep neural networks, which scale at most polynomially with depth. More precisely, these authors show that the network satisfies some Lipschitz continuity property (either with respect to the input or to the parameters), then exploit results on the statistical complexity of Lipschitz function classes. Under stronger norm constraints, these bounds can even be made depth-independent (Golowich et al., 2018). However, their approach differs from ours insofar as we consider neural ODEs and the associated family of deep neural networks, whereas they are solely interested in finite-depth neural networks. As a consequence, their hypotheses on the class of neural networks differ from ours. Section 4 develops a more thorough comparison. Similar Lipschitz-based techniques have also been applied to obtain generalization bounds for deep equilibrium networks (Pabbaraju et al., 2021). Going beyond statistical guarantees, Bethune et al. (2022) study approximation and robustness properties of Lipschitz neural networks.

## 3 Generalization bounds for parameterized ODEs

We start by recalling the usual supervised learning setup and introduce some notation in Section 3.1, before presenting our parameterized ODE model and the associated generalization bound in Section 3.2. We then apply the bound to the specific case of time-invariant neural ODEs in Section 3.3.

### Learning procedure

We place ourselves in a supervised learning setting. Let us introduce the notation that are used throughout the paper (up to and including Section 4.1). The input data is a sample of \(n\) i.i.d. pairs \((x_{i},y_{i})\) with the same distribution as some generic pair \((x,y)\), where \(x\) (resp. \(y\)) takes its values in some bounded ball \(=B(0,R_{})\) (resp. \(=B(0,R_{})\)) of \(^{d}\), for some \(R_{},R_{}>0\). This setting encompasses regression but also classification tasks by (one-hot) encoding labels in \(^{d}\). Note that we assume for simplicity that the input and output have the same dimension, but our analysis easily extends to the case where they have different dimensions by adding (parameterized) projections at the beginning or at the end of our model. Given a parameterized class of models \(_{}=\{F_{},\}\), the parameter \(\) is fitted by empirical risk minimization using a loss function \(:^{d}^{d}^{+}\) that we assume to be Lipschitz with respect to its first argument, with a Lipschitz constant \(K_{}>0\). In the following, we write for the sake of concision that such a function is \(K_{}\)-Lipschitz. We also assume that \((x,x)=0\) for all \(x^{d}\). The theoretical and empirical risks are respectively defined, for any \(\), by

\[()=[(F_{}(x),y)] }_{n}()=_{i=1}^{n}F_{ }(x_{i}),y_{i},\]

where the expectation \(\) is evaluated with respect to the distribution of \((x,y)\). Letting \(_{n}\) a minimizer of the empirical risk, the generalization problem consists in providing an upper bound on the difference \((_{n})-}_{n}(_ {n})\).

### Generalization bound

Model.We start by making more precise the parameterized ODE model introduced in Section 1. The setup presented here can easily be specialized to the case of neural ODEs, as we will see in Section 3.3. Let \(f_{1},,f_{m}:^{d}^{d}\) be fixed \(K_{f}\)-Lipschitz functions for some \(K_{f}>0\). Denote by \(M\) their supremum on \(\) (which is finite since these functions are continuous). The parameterized ODE \(F_{}\) is defined by the following initial value problem that maps some \(x^{d}\) to \(F_{}(x)^{d}\):

\[H_{0} =x\] \[dH_{t} =_{i=1}^{m}_{i}(t)f_{i}(H_{t})dt\] (5) \[F_{}(x) =H_{1},\]

where the parameter \(=(_{1},,_{m})\) is a function from \(\) to \(^{m}\). We have to impose constraints on \(\) for the model \(F_{}\) to be well-defined. To this aim, we endow (essentially bounded) functions from \(\) to \(^{m}\) with the following \((1,)\)-norm

\[\|\|_{1,}=_{0 t 1}_{i=1}^{m}|_{i}( t)|.\] (6)

We can now define the set of parameters

\[=\{:^{m},\,\|\|_{1,} R_{ }_{i}K_{}i\{1,,m\}\},\] (7)

for some \(R_{}>0\) and \(K_{} 0\). Then, for \(\), the following Proposition, which is a consequence of the Picard-Lindelof Theorem, shows that the mapping \(x F_{}(x)\) is well-defined.

**Proposition 1** (Well-posedness of the parameterized ODE).: _For \(\) and \(x^{d}\), there exists a unique solution to the initial value problem (5)._

An immediate consequence of Proposition 1 is that it is legitimate to consider \(_{}=\{F_{},\}\) for our model class.

When \(K_{}=0\), the parameter space \(\) is finite-dimensional since each \(_{i}\) is constant. This setting corresponds to the time-independent neural ODEs of Chen et al. (2018). In this case, the norm (6) reduces to the \(\|\|_{1}\) norm over \(^{m}\). Note that, to fit exactly the formulation of Chen et al. (2018), the time \(t\) can be added as a variable of the functions \(f_{i}\), which amounts to adding a new coordinate to \(H_{t}\). This does not change the subsequent analysis. In the richer time-dependent case where \(K_{}>0\), the set \(\) belongs to an infinite-dimensional space and therefore, in practice, \(_{i}\) is approximated in a finite basis of functions, such as Fourier series, Chebyshev polynomials, and splines. We refer to Massaroli et al. (2020) for a more detailed discussion, including formulations of the backpropagation algorithm (a.k.a. the adjoint method) in this setting.

Note that we consider the case where the dynamics at time \(t\) are linear with respect to the parameter \(_{i}(t)\). Nevertheless, we emphasize that the mapping \(x F_{}(x)\) remains a highly non-linear function of each \(_{i}(t)\). To fix ideas, this setting can be seen as analogue to working with pre-activation residual networks instead of post-activation (see He et al., 2016, for definitions of the terminology), which is a mild modification.

Statistical analysisSince \(\) is a subset of an infinite-dimensional space, complexity measures based on the number of parameters cannot be used. Instead, our approach is to resort to Lipschitz-based complexity measures. More precisely, to bound the complexity of our model class, we propose two building blocks: we first show that the model \(F_{}\) is Lipschitz-continuous with respect to its parameters \(\). This allows us to bound the complexity of the model class depending on the complexity of the parameter class. In a second step, we assess the complexity of the class of parameters itself.

Starting with our first step, we show the following estimates for our class of parameterized ODEs. Here and in the following, \(\|\|\) denotes the \(_{2}\) norm over \(^{d}\).

**Proposition 2** (The parameterized ODE is bounded and Lipschitz).: _Let \(\) and \(\). Then, for any \(x\),_

\[\|F_{}(x)\| R_{}+MR_{}(K_{f}R_{})\]

_and_

\[\|F_{}(x)-F_{}(x)\| 2MK_{f}R_{}(2K_{f}R _{})\|-\|_{1,}.\]

The proof, given in the Appendix, makes extensive use of Gronwall's inequality (Pachpatte and Ames, 1997), a standard tool to obtain estimates in the theory of ODEs, in order to bound the magnitude of the solution \(H_{t}\) of (5).

The next step is to assess the magnitude of the covering number of \(\). Recall that, for \(>0\), the \(\)-covering number of a metric space is the number of balls of radius \(\) needed to completely cover the space, with possible overlaps. More formally, considering a metric space \(\) and denoting by \(B(x,)\) the ball of radius \(\) centered at \(x\), the \(\)-covering number of \(\) is equal to \(\{n 1| x_{1},,x_{n}, _{i=1}^{n}B(x_{i},)\}\).

**Proposition 3** (Covering number of the ODE parameter class).: _For \(>0\), let \(()\) be the \(\)-covering number of \(\) endowed with the distance associated to the \((1,)\)-norm (6). Then_

\[() m}{ }+K_{}(4)}{}.\]

Proposition 3 is a consequence of a classical result, see, e.g., Kolmogorov and Tikhomirov (1959, example 3 of paragraph 2). A self-contained proof is given in the Appendix for completeness. We also refer to Gottlieb et al. (2017) for more general results on covering numbers of Lipschitz functions.

The two propositions above and an \(\)-net argument allow to prove the first main result of our paper (where we recall that the notations are defined in Section 3.1).

**Theorem 1** (Generalization bound for parameterized ODEs).: _Consider the class of parameterized ODEs \(_{}=\{F_{},\}\), where \(F_{}\) is given by (5) and \(\) by (7). Let \(>0\)._

_Then, for \(n 9(m^{-2}R_{}^{-2},1)\), with probability at least \(1-\),_

\[(_{n})}_{n}( {}_{n})+Bmn)}{n}}+B}}{n^{1/4}}+}},\]

_where \(B\) is a constant depending on \(K_{},K_{f},R_{},R_{},R_{},M\). More precisely,_

\[B=6K_{}K_{f}(K_{f}R_{})R_{}+MR_{}(K_ {f}R_{})+R_{}.\]

Three terms appear in our upper bound of \((_{n})-}_{n}(_{n})\). The first and the third ones are classical (see, e.g. Bach, 2023, Sections 4.4 and 4.5). On the contrary, the second term is more surprising with its convergence rate in \((n^{-1/4})\). This slower convergence rate is due to the fact that the space of parameters is infinite-dimensional. In particular, for \(K_{}=0\), corresponding to a finite-dimensional space of parameters, we recover the usual \((n^{-1/2})\) convergence rate, however at the cost of considering a much more restrictive class of models. Finally, it is noteworthy that the dimensionality appearing in the bound is not the input dimension \(d\) but the number of mappings \(m\).

Note that this result is general and may be applied in a number of contexts that go beyond deep learning, as long as the instantaneous dependence of the ODE dynamics to the parameters is linear. One such example is the predator-prey model, describing the evolution of two populations of animals, which reads \(dx_{t}=x_{t}(- y_{t})dt\) and \(dy_{t}=-y_{t}(- x_{t})dt\), where \(x_{t}\) and \(y_{t}\) are real-valued variables and \(\), \(\), \(\) and \(\) are model parameters. This ODE falls into the framework of this section,if one were to estimate the parameters by empirical risk minimization. We refer to Deuflhard and Roblitz (2015, section 3) for other examples of parameterized biological ODE dynamics and methods for parameter identification.

Nevertheless, for the sake of brevity, we focus on applications of this result to deep learning, and more precisely to neural ODEs, which is the topic of the next section.

### Application to neural ODEs

As explained in Section 1, parameterized ODEs include both time-dependent and time-independent neural ODEs. Since the time-independent model is more common in practice, we develop this case here and leave the time-dependent case to the reader. We thus consider the following neural ODE:

\[H_{0} =x\] \[dH_{t} =W(H_{t})dt\] (8) \[F_{W}(x) =H_{1},\]

where \(W^{d d}\) is a weight matrix, and \(:\) is an activation function applied component-wise. We assume \(\) to be \(K_{}\)-Lipschitz for some \(K_{}>0\). This assumption is satisfied by all common activation functions. To put the model in the form of Section 3.2, denote \(e_{1},,e_{d}\) the canonical basis of \(^{d}\). Then the dynamics (8) can be reformulated as

\[dH_{t}=_{i,j=1}^{d}W_{ij}_{ij}(H_{t})dt,\]

where \(_{ij}(x)=(x_{j})e_{i}\). Each \(_{ij}\) is itself \(K_{}\)-Lipschitz, hence we fall in the framework of Section 3.2. In other words, the functions \(f_{i}\) of our general parameterized ODE model form a shallow neural network with pre-activation. Denote by \(\|W\|_{1,1}\) the sum of the absolute values of the elements of \(W\). We consider the following set of parameters, which echoes the set \(\) of Section 3.2:

\[=\{W^{d d},\|W\|_{1,1} R_{}\},\] (9)

for some \(R_{}>0\). We can then state the following result as a consequence of Theorem 1.

**Corollary 1** (Generalization bound for neural ODEs).: _Consider the class of neural ODEs \(_{}=\{F_{W},W\}\), where \(F_{W}\) is given by (8) and \(\) by (9). Let \(>0\)._

_Then, for \(n 9R_{}^{-1}(d^{-4}R_{}^{-1},1)\), with probability at least \(1-\),_

\[(_{n})}_{n}(_{ n})+B(d+1)}dn)}{n}}+}},\]

_where \(B\) is a constant depending on \(K_{},K_{},R_{},R_{},R_{},M\). More precisely,_

\[B=6K_{}K_{}(K_{}R_{})R_{ }+MR_{}(K_{}R_{})+R_{} .\]

Note that the term in \((n^{-1/4})\) from Theorem 1 is now absent. Since we consider a time-independent model, we are left with the other two terms, recovering a standard \((n^{-1/2})\) convergence rate.

## 4 Generalization bounds for deep residual networks

As highlighted in Section 1, there is a strong connection between neural ODEs and discrete residual neural networks. The previous study of the continuous case in Section 3 paves the way for deriving a generalization bound in the discrete setting of residual neural networks, which is of great interest given the pervasiveness of this architecture in modern deep learning.

We begin by presenting our model and result in Section 4.1, before detailing the comparison of our approach with other papers in Section 4.2 and giving some numerical illustration in Section 4.3.

### Model and generalization bound

Model.We consider the following class of deep residual networks:

\[H_{0} =x\] (10) \[H_{k+1} =H_{k}+W_{k+1}(H_{k}), 0 k L-1\] \[F_{}(x) =H_{L},\]

where the parameter \(=(W_{k})_{1 k L}^{L d d}\) is a set of weight matrices and \(\) is still a \(K_{}\)-Lipschitz activation function. To emphasize that \(\) is here a third-order tensor, as opposed to the case of time-invariant neural ODEs in Section 3.3, where \(W\) was a matrix, we denote it with a bold notation. We also assume in the following that \((0)=0\). This assumption could be alleviated at the cost of additional technicalities. Owing to the \(}{{L}}\) scaling factor, the deep limit of this residual network is a (time-dependent) neural ODE of the form studied in Section 3. We refer to Marion et al. (2022) for further discussion on the link between scaling factors and deep limits. We simply note that this scaling factor is not common practice, but preliminary experiments show it does not hurt performance and can even improve performance in a weight-tied setting (Sander et al., 2022). The space of parameters is endowed with the following \((1,1,)\)-norm

\[\|\|_{1,1,}=_{1 k L}_{i,j=1}^{d}|W_ {k,i,j}|.\] (11)

Also denoting \(\|\|_{}\) the element-wise maximum norm for a matrix, we consider the class of matrices

\[=^{L d d}, \|\|_{1,1,} R_{} \] (12) \[\|W_{k+1}-W_{k}\|_{}}}{L}\  1 k L-1},\]

for some \(R_{}>0\) and \(K_{} 0\), which is a discrete analogue of the set \(\) defined by (7).

In particular, the upper bound on the difference between successive weight matrices is to our knowledge a novel way of constraining the parameters of a neural network. It corresponds to the discretization of the Lipschitz continuity of the parameters introduced in (7). By analogy, we refer to it as a constraint on the Lipschitz constant of the weights. Note that, for standard initialization schemes, the difference between two successive matrices is of the order \((1)\) and not \((1/L)\), or, in other words, \(K_{}\) scales as \((L)\). This dependence of \(K_{}\) on \(L\) can be lifted by adding correlations across layers at initialization. For instance, one can take, for \(k\{1,,L\}\) and \(i,j\{1,,d\}\), \(_{k,i,j}=}f_{i,j}()\), where \(f_{i,j}\) is a smooth function, for example a Gaussian process with the RBF kernel. Such a non-i.i.d. initialization scheme is necessary for the correspondence between deep residual networks and neural ODEs to hold (Marion et al., 2022). Furthermore, Sander et al. (2022) prove that, with this initialization scheme, the constraint on the Lipschitz constant also holds for the _trained_ network, with \(K_{}\) independent of \(L\). Finally, we emphasize that the following developments also hold in the case where \(K_{}\) depends on \(L\) (see also Section 4.2 for a related discussion).

Statistical analysis.At first sight, a reasonable strategy would be to bound the distance between the model (10) and its limit \(L\) that is a parameterized ODE, then _apply_ Theorem 1. This strategy is straightforward, but comes at the cost of an additional \((1/L)\) term in the generalization bound, as a consequence of the discretization error between the discrete iterations (10) and their continuous limit. For example, we refer to Fermanian et al. (2021) where this strategy is used to prove a generalization bound for discrete RNNs and where this additional error term is incurred. We follow another way by mimicking all the proof with a finite \(L\). This is a longer approach but it yields a sharper result since we avoid the \((1/L)\) discretization error. The proof structure is similar to Section 3: the following two Propositions are the discrete counterparts of Propositions 2 and 3.

**Proposition 4** (The residual network is bounded and Lipschitz).: _Let \(\) and \(}\). Then, for any \(x\),_

\[\|F_{}(x)\| R_{}(K_{}R_{})\]

_and_

\[\|F_{}(x)-F_{}}(x)\| }}{R_{}}(2K_{}R_{})\|-}\|_{1,1,}.\]

**Proposition 5** (Covering number of the residual network parameter class).: _Let \(()\) be the covering number of \(\) endowed with the distance associated to the \((1,1,)\)-norm (11). Then_

\[() d^{2}R_{ }}{}+K_{}(4)}{ }.\]

The proof of Proposition 4 is a discrete analogous of Proposition 2. On the other hand, Proposition 5 can be proven as a _consequence_ of Proposition 3, by showing the existence of an injective isometry from \(\) into a set of the form (7). Equipped with these two propositions, we are now ready to state the generalization bound for our class of residual neural networks.

**Theorem 2** (Generalization bound for deep residual networks).: _Consider the class of neural networks \(_{}=\{F_{},\}\), where \(F_{}\) is given by (10) and \(\) by (12). Let \(>0\)._

_Then, for \(n 9R_{}^{-1}(d^{-4}R_{}^{-1},1)\), with probability at least \(1-\),_

\[(}_{n})}_{n}( }_{n})+B(d+1)}dn)}{n}}+B }}}{n^{1/4}}+}},\] (13)

_where \(B\) is a constant depending on \(K_{},K_{},R_{},R_{},R_{}\). More precisely,_

\[B=6K_{}R_{})}{R_{ }},1(R_{}(K_{}R_{})+R_{ }).\]

We emphasize that this result is non-asymptotic and valid for any width \(d\) and depth \(L\). Furthermore, the depth \(L\) does not appear in the upper bound (13). This should not surprise the reader since Theorem 1 can be seen as the deep limit \(L\) of this result, hence we expect that our bound remains finite when \(L\) (otherwise the bound of Theorem 1 would be infinite). However, \(L\) appears as a scaling factor in the definition of the neural network (10) and of the class of parameters (12). This is crucial for the depth independence to hold, as we will comment further on in the next section.

Furthermore, the depth independence comes at the price of a \((n^{-1/4})\) convergence rate. Note that, by taking \(K_{}=0\), we obtain a generalization bound for weight-tied neural networks with a faster convergence rate in \(n\), since the term in \((n^{-1/4})\) vanishes.

### Comparison with other bounds

As announced in Section 2, we now compare Theorem 2 with the results of Bartlett et al. (2017) and Golowich et al. (2018). Beginning by Bartlett et al. (2017), we first state a slightly weaker version of their result to match our notations and facilitate comparison.

**Corollary 2** (corollary of Theorem 1.1 of Bartlett et al. (2017)).: _Consider the class of neural networks \(_{}}=\{F_{},}\}\), where \(F_{}\) is given by (10) and \(}=\{^{L d d},\| \|_{1,1,} R_{}\}\). Assume that \(L R_{}\) and \(K_{}=1\), and let \(,>0\). Consider \((x,y),(x_{1},y_{1}),,(x_{n},y_{n})\) drawn i.i.d. from any probability distribution over \(^{d}\{1, d\}\) such that a.s. \(\|x\| R_{}\). Then, with probability at least \(1-\), for every \(}\),_

\[*{arg\,max}_{1 j d}F_{}(x)_{j} y}_{n}()+C}R_{}(R_{})(d)}{ {n}}+}},\] (14)

_where \(}_{n}() n^{-1}_{i=1}^{n} _{F_{}(x_{i})_{y_{i}}+_{j y_{i}}f(x_{i})_{j}}\) and \(C\) is a universal constant._

We first note that the setting is slightly different from ours: they consider a large margin predictor for a multi-class classification problem, whereas we consider a general Lipschitz-continuous loss \(\). This being said, the model class is identical to ours, except for one notable difference: the constraint on the Lipschitz constant of the weights appearing in equation (12) is not required here.

Comparing (13) and (14), we see that our bound enjoys a better dependence on the depth \(L\) but a worse dependence on the width \(d\). Regarding the depth, our bound (13) does not depend on \(L\), whereas the bound (14) scales as \(()\). This comes from the fact that we consider a smaller set of parameters (12), by adding the constraint on the Lipschitz norm of the weights. This constraintallows us to control the complexity of our class of neural networks independently of depth, as long as \(K_{}\) is independent of \(L\). If \(K_{}\) scales as \((L)\), which is the case for i.i.d. initialization schemes, our result also features a scaling in \(()\). As for the width, Bartlett et al. (2017) achieve a better dependence by a subtle covering numbers argument that takes into account the geometry induced by matrix norms. Since our paper focuses on a depth-wise analysis by leveraging the similarity between residual networks and their infinite-depth counterpart, improving the scaling of our bound with width is left for future work. Finally, note that both bounds have a similar exponential dependence in \(R_{}\).

As for Golowich et al. (2018), they consider non-residual neural networks of the form \(x M_{L}(M_{L-1}((M_{1}x)))\). These authors show that the generalization error of this class scales as

\[R_{} }{_{S}}}}{n^{1/4}},\]

where \(_{F}\) is an upper-bound on the product of the Frobenius norms \(_{k=1}^{L}\|M_{k}\|_{F}\) and \(_{S}\) is a lower-bound on the product of the spectral norms \(_{k=1}^{L}\|M_{k}\|\). Under the assumption that both \(_{F}\) and \(}}{{_{S}}}\) are bounded independently of \(L\), their bound is indeed depth-independent, similarly to ours. Interestingly, as ours, the bound presents a \((n^{-1/4})\) convergence rate instead of the more usual \((n^{-1/2})\). However, the assumption that \(_{F}\) is bounded independently of \(L\) does not hold in our residual setting, since we have \(M_{k}=I+W_{k}\) and thus we can lower-bound

\[_{k=1}^{L}\|M_{k}\|_{F}_{k=1}^{L}\|I\|_{F}-\|M_{k}\|_{F}-}}{L} ^{L} d^{}e^{-}}{}}.\]

In our setting, it is a totally different assumption, the constraint that two successive weight matrices should be close to one another, which allows us to derive depth-independent bounds.

### Numerical illustration

The bound of Theorem 2 features two quantities that depend on the class of neural networks, namely \(R_{}\) that bounds a norm of the weight matrices and \(K_{}\) that bounds the maximum _difference_ between two successive weight matrices, i.e. the Lipschitz constant of the weights. The first one belongs to the larger class of norm-based bounds that has been extensively studied (see, e.g., Neyshabur et al., 2015). We are therefore interested in getting a better understanding of the role of the second quantity, which is much less common, in the generalization ability of deep residual networks.

To this aim, we train deep residual networks (10) (of width \(d=30\) and depth \(L=1000\)) on MNIST. We prepend the network with an initial weight matrix to project the data \(x\) from dimension \(768\) to dimension \(30\), and similarly postpend it with another matrix to project the output \(F_{}(x)\) into dimension \(10\) (i.e. the number of classes in MNIST). Finally, we consider two training settings: either the initial and final matrices are trained, or they are fixed random projections. We use the initialization scheme outlined in Section 4.1. Further experimental details are postponed to the Appendix.

We report in Figure 0(a) the generalization gap of the trained networks, that is, the difference between the test and train errors (in terms of cross entropy loss), as a function of the maximum Lipschitz constant of the weights \(_{0 k L-1}(\|W_{k+1}-W_{k}\|_{})\). We observe a positive correlation between these two quantities. To further analyze the relationship between the Lipschitz constant of the weights and the generalization gap, we then add the penalization term \(_{k=0}^{L-1}\|W_{k+1}-W_{k}\|_{F}^{2}^{1/2}\) to the loss, for some \( 0\). The obtained generalization gap is reported in Figure 0(b) as a function of \(\). We observe that this penalization allows to reduce the generalization gap. These two observations go in support of the fact that a smaller Lipschitz constant improves the generalization power of deep residual networks, in accordance with Theorem 2.

However, note that we were not able to obtain an improvement on the test loss by adding the penalization term. This is not all too surprising since previous work has investigated a related penalization, in terms of the Lipschitz norm of the layer sequence \((H_{k})_{0 k L}\), and was similarly not able to report any improvement on the test loss (Kelly et al., 2020).

Finally, the proposed penalization term slightly departs from the theory that involves \(_{0 k L-1}(\|W_{k+1}-W_{k}\|_{})\). This is because the maximum norm is too irregular to be used in practice since, at any one step of gradient descent, it only impacts the maximum weights and not the others. As an illustration, Figure 2 shows the generalization gap when penalizing with the maximum max-norm \(_{0 k L-1}(\|W_{k+1}-W_{k}\|_{})\) and the \(L_{2}\) norm of the max-norm \(_{k=0}^{L-1}\|W_{k+1}-W_{k}\|_{}^{2}^{1/2}\). The factor \(\) is scaled appropriately to reflect the scale difference of the penalizations. The results are mixed: the \(L_{2}\) norm of the max-norm is effective contrarily to the maximum max-norm. Further investigation of the properties of these norms is left for future work.

## 5 Conclusion

We provide a generalization bound that applies to a wide range of parameterized ODEs. As a consequence, we obtain the first generalization bounds for time-independent and time-dependent neural ODEs in supervised learning tasks. By discretizing our reasoning, we also provide a bound for a class of deep residual networks. Understanding the approximation and optimization properties of this class of neural networks is left for future work. Another intriguing extension is to relax the assumption of linearity of the dynamics at time \(t\) with respect to \(_{i}(t)\), that is, to consider a general formulation \(dH_{t}=_{i=1}^{m}f_{i}(H_{t},_{i}(t))\). In the future, it should also be interesting to extend our results to the more involved case of neural SDEs, which have also been found to be deep limits of a large class of residual neural networks (Cohen et al., 2021; Marion et al., 2022).

Figure 1: Link between the generalization gap and the Lipschitz constant of the weights.

Figure 2: Generalization gap as a function of the penalization factor \(\) for other penalizations.