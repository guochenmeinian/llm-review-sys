# ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Field

Kiyohiro Nakayama\({}^{1}\)   Mikaela Angelina Uy\({}^{1,2}\)   Yang You\({}^{1}\)   Ke Li\({}^{3}\)   Leonida J. Guibas\({}^{1}\)

\({}^{1}\) Stanford University  \({}^{2}\) Nvidia  \({}^{3}\) Simon Fraser University

w4756677@stanford.edu   keli@sfu.ca

{mikacuy, yangyou, guibas}@cs.stanford.edu

###### Abstract

Neural radiance fields (NeRFs) have gained popularity with multiple works showing promising results across various applications. However, to the best of our knowledge, existing works do not explicitly model the distribution of training camera poses, or consequently the triangulation quality, a key factor affecting reconstruction quality dating back to classical vision literature. We close this gap with ProvNeRF, an approach that models the **provenance** for each point - i.e., the locations where it is likely visible - of NeRFs as a stochastic field. We achieve this by extending implicit maximum likelihood estimation (IMLE) to functional space with an optimizable objective. We show that modeling per-point provenance during the NeRF optimization enriches the model with information on triangulation leading to improvements in novel view synthesis and uncertainty estimation under the challenging sparse, unconstrained view setting against competitive baselines1.

## 1 Introduction

Neural radiance fields (NeRFs) , allowing for learning 3D scenes given only 2D images, have grown in popularity in recent years. It has shown promise in many different applications such as novel view synthesis , depth estimation , robotics , localization , etc. Existing literature  show that the quality of NeRF reconstruction is correlated with the selection of training camera poses. Similar correlations are observed in the classical literature too, triangulation is highly dependent on camera poses , which greatly influences the reconstruction quality. One common and important setting in computer vision literature  is the **sparse view** setting in **unconstrained** environments, and triangulation is even more critical, affecting the reconstruction quality as limited input views make the system more sensitive to noise.

Despite the correlation between triangulation and reconstruction quality, to the best of our knowledge, existing works do not explicitly model the former when optimizing the latter. In this work, we address this gap in the literature by modeling for each point the _locations where it is likely visible_. We dub this as the _provenances_ of a point. Modeling and learning per-point provenance can help NeRF understand how the training cameras are distributed in space, which inherently links it to triangulation and reconstruction quality.

However, determining the provenances of a point \(\) without the underlying geometry is not straightforward as many factors influence the visibility of each point in the reconstructed geometry. For example, the literature on stereo matching  has extensively studied the influences of camera locations on 3D reconstruction. One such well-known challenge arises when selecting the baseline of a pair of cameras in a stereo system. As shown in Fig. 2, points' visibility can suffer from different sets of errors when the length of the camera pair's baseline changes. For NeRFs, the dependencebecomes more complex as multiple cameras' visibility needs to be estimated. To overcome this challenge, we propose to model the provenance as the samples from a _probability distribution_, where a location \(\) is assigned with a large likelihood if and only if \(\) is likely to be visible from \(\).

To handle the potential complexity of this distribution, we represent the provenance of \(\) as a set of location _samples_, generated from a learned probability distribution. This is distinct from the existing "attribute" prediction extensions of NeRFs [69; 30; 8] since provenance is a _distribution_ for every 3D point in space. Thus, this amounts to modeling an infinite collection of distributions (per-point's provenance) over all 3D points, which is mathematically, a _stochastic field_ over \(^{3}\). In our work, we extend implicit maximum likelihood estimation (IMLE) , a sample-based generative model, to model stochastic fields by adapting the objective to functional space. Furthermore, we derive an equivalent pointwise objective that can be efficiently optimized with gradient descent and use it to model the provenance field.

We dub our method **ProvNeRF** which models per-point provenance during the training stage of NeRF (Fig. 1). This enriches the model with information on triangulation quality when the model parameters are optimized. Once the provenance stochastic field is trained, we show that we can use it to improve novel view synthesis (Sec. 5.1) and estimate triangulation uncertainty in the capturing process (Sec. 5.2) under the challenging sparse, unconstrained view setting.

## 2 Related Works

NeRFs and their Extensions.Neural radiance fields (NeRFs)  have revolutionized the field of 3D reconstruction  and novel view synthesis [50; 32] with its powerful representation of a scene using weights of an MLP that is rendered by volume rendering [41; 58]. Follow-ups on NeRF further tackle novel view synthesis under more difficult scenarios such as unconstrained photo collections , unbounded , dynamic  and deformable  scenes, and reflective objects [59; 7]. Going beyond novel view synthesis, the NeRF representation has also shown great promise in different applications such as autonomous driving [56; 62], robotics [1; 22] and editing [67; 63]. Recent works have also extended NeRFs to model other fields in addition to color and opacity such as semantics [69; 68], normals , CLIP embeddings , image features  and scene flow . Most of these works learn an additional function that predicts an auxiliary _deterministic_ output at each point that is either a scalar or a vector, trained with extra supervision using volume rendering. All of the above works use a deterministic field to output the additional information. However, because each point's provenance is a probabilistic distribution, we need to model a stochastic field instead of a deterministic field for provenance.

Sparse View Novel View Synthesis.NeRFs with rendering supervision alone struggle with sparse view input due to insufficient constraints in volume rendering. Several approaches have been proposed to train NeRFs under the sparse-view regime with regularization losses [43; 64], semantic

Figure 1: **(Left) ProvNeRF** models a provenance field that outputs _provenances_ for each 3D point as likely samples (arrows). For 3D points (brown triangle and blue circle), the corresponding provenances (illustrated by the arrows), are locations that likely observe them. **(Right) ProvNeRF** enables better novel view synthesis and estimating the uncertainty of the capturing process because it models the locations of likely observations that is critical for NeRFâ€™s optimization.

consistency , and image  or cost volume  feature constraints. Other works also constrain the optimization using priors from data  or depth . Despite addressing the setting with limited number of input views, many works are not specifically designed to tackle our desired sparse, unconstrained views setting as they either focus on object-level , limited camera baselines , or forward-facing scenes  scenes. Recent works  have looked into improving the NeRF quality on a more difficult setting of sparse, unconstrained (outward-facing) input views by incorporating depth priors. However, none of these works consider the locations and orientations of training cameras in their optimization process despite it being one of the major factors influencing the NeRF's optimization in a sparse setup.

Uncertainty Modeling in Neural Radiance Fields.The current literature separates NeRF's uncertainty into aleatoric - e.g., transient objects or changes in lighting - and epistemic - data limitation due to weak texture or limited camera views - uncertainties. Some works  model aleatoric uncertainty by directly predicting the uncertainty values through a neural network. However, their approach requires training on large-scale data and is not suited for estimating the uncertainty of a specific scene. On the other hand, several works explore epistemic uncertainty estimation in NeRFs through variational inference , ensemble learning , and Bayesian inference . While these works estimate epistemic uncertainty, they still entangle different sources of uncertainty such as texture, camera poses, and model bias, resulting in unclear and inconsistent definitions of the uncertainty quantified. In our work, we specifically model the uncertainty caused by the capturing process that is useful in various downstream tasks .

## 3 Preliminaries

### Neural Radiance Fields (NeRF)

A neural radiance field (NeRF) is a coordinate-based neural network that learns a field in 3D space, where each point \(^{3}\) is of certain _opacity_ and _color_. Mathematically, a NeRF is parameterized by two functions representing the two fields \(_{,}=(_{}(),_{}(,))\), one for opacity \(_{}:^{3}_{+}\) and one for color \(_{}:^{3}^{2}^{3}\), where \(^{2}\) is the direction from where \(\) is viewed from. One of the key underpinnings of NeRFs is volume rendering allowing for end-to-end differentiable learning with only training images. Concretely, given a set of \(M\) images \(I_{1},I_{2},...,I_{M}\) and their corresponding camera poses \(P_{1},P_{2},...,P_{M}\), the rendered color of a pixel \(x\) is the expected color along a camera ray \(_{i,x}(t)=_{i}+t_{i,x}\), where \(_{i}\) is the camera origin and \(_{i,x}\) is the ray direction for pixel \(x\) that can be computed from the corresponding camera pose \(P_{i}\). The pixel value for 2D coordinate \(x\) is then given by the line integral:

\[_{,}(_{i,x})=_{t_{n}}^{t_{f}}_{ }(_{i,x}(t))T(_{i,x}(t))_{} (_{i,x}(t))\,dt, \]

Figure 2: **Complex influence of camera baseline distance on the 3D reconstruction.**_Right:_ With a wide baseline, the reconstruction is more robust against 2D measurement noises. However, it is more likely to omit hidden surfaces because the invisible region is larger than a small baseline camera pair. _Left:_ With a small baseline, the 3D reconstruction is less likely to suffer from occlusions as the invisible region between cameras is small. However, the reconstruction can be noisy due to large stereo range errors (large deviation in depth with a small amount of noise in the 2D measurement).

[MISSING_PAGE_FAIL:4]

While the above empirical distribution of provenances is given by the training cameras, the actual distribution of provenances, i.e. for each point the locations that point is likely visible from, can have a more complex dependence on both the underlying geometry and the cameras. To capture this complexity, we model \(_{}()\) as a learnable network, a probabilistic model that can model potentially complex distribution, and one choice of such a model is implicit maximum likelihood estimate (IMLE) . Similar to the empirical distribution, we also represent provenance samples from \(_{}()\) as a distance-direction tuple \((t,)\) as defined in Eq 5. We optimized our network with the empirical distribution \(}\) as training signals.

\(_{}()\) defines a distribution for all point \(^{3}\). Treating \(^{3}\) as the index set, \(_{}=\{_{}()\}_{ ^{3}}\) defines a stochastic field on \(^{3}\) as a collection of distributions \(_{}()\) for all \(^{3}\). Because a stochastic field is composed of infinitely many random variables over \(^{3}\), existing methods cannot be applied out of the box as they only model finite-dimensional distributions. In the following sections, we extend IMLE  to model this stochastic field.

### ProvNeRF

**ProvNeRF** models provenances of a NeRF as a stochastic field by extending IMLE  to functional space. IMLE learns a mapping that transforms a latent distribution to the data distribution, where each data sample is either a scalar or a vector (Sec. 3.2). However, in our context, since samples from the stochastic field \(_{}\) are _functions_ mapping 3D locations to provenances, we need to extend IMLE to learn a neural network mapping \(_{}\) that transforms a pre-defined _latent stochastic field_\(\) to the provenance distribution \(_{}\) (See Fig. 3).

Let \(\) be the stochastic field where each sample \(\) is a function \(:^{3}^{b}\). To transform \(\) to \(_{}\), fIMLE learns a deterministic mapping \(_{}\) that maps each latent function \(\) to a function \(_{}_{}\) via composition: \(_{}=_{}\). \(_{}\) here is represented as a neural network to handle complex transformations from \(\) to \(_{}\).

We define a latent function sample \(\) to be the concatenation of a _random linear transformation_ of \(\) and \(\) itself. Mathematically, each latent function \(\) is a block matrix of size \((b+4) 3\):

\[()=[}{}],(,^{2}),^{3}. \]

Although \(\) can be designed to have non-linear dependence on the input location \(\), we experimentally show that this simple design choice works well across different downstream applications.

To train \(_{}\), we maximize the likelihood of the training provenances (Eq. 4) under \(_{}\) for each \(\) using the IMLE objective  extended to functional space. We term this extension as functional Implicit Maximum Likelihood Estimation (fIMLE). Because a direct extension to fIMLE leads to an intractable objective, we derive an efficient pointwise loss between the training provenances and model predictions equivalent to the fIMLE objective in the following section.

Figure 3: **Training pipeline for ProvNeRF. For each point \(\) seen from provenance tuple \((,})\), with direction \(\) at distance \(t\), we first sample \(K\) latent random functions \(\{_{j}\}\) from distribution \(\). The learned transformation \(_{}\) transforms each \(_{j}()\) to a provenance sample \(_{}^{(j)}()\). Finally \(_{}\) is trained with \(_{}\) as defined in Eq. 9.**

### Functional Implicit Maximum Likelihood Estimation

We construct an IMLE objective for stochastic fields to maximize the likelihood of training provenances under \(_{}\). Similar to Eq. 3, if we have i.i.d. empirical samples \(}_{1},,}_{M}\) from the empirical stochastic field \(}\) (defined in Sec. 4.1), and model samples \(_{}^{(1)},,_{}^{(K)}\) from the parameter stochastic field \(_{}\), we define the fIMLE objective as

\[=_{}_{_{}^{(1)},,_ {}^{(K)}}[_{i=1}^{n}_{j}\|}_{i}-_{ }^{(j)}\|_{L^{2}}^{2}]. \]

Unlike the original IMLE objective (Eq. 3) that can be directly optimized, the fIMLE objective in Eq. 7 requires the computation of a \(L^{2}\) integral norm - a functional analogy to the \(L^{2}\) vector norm - which, in general, is not analytically in closed form. Furthermore, approximations of this integral are very expensive since each point query to \(_{}\) needs a forward pass through \(_{}\).

To get around this, we use the calculus of variations to reformulate Eq. 7 to minimize the pointwise difference between the empirical samples and model predictions 3. This allows us to write the fIMLE objective as

\[_{}=_{_{}^{(1)},,_{ }^{(K)}_{}}[_{i=1}^{n}_{j}_{ ()}\|}_{i}()-_{}^{(j)}()\|_{2}^{2}], \]

where \(()\) is a uniform distribution over the scene bound \(\). Eq. 8 only requires computing the pointwise difference between samples from \(}()\) and \(_{}()\), making it efficiently optimizable with

    & &  &  \\  & PSNR (\(\)) & SSIM (\(\)) & LPIPS (\(\)) & PSNR (\(\)) & SSIM (\(\)) & LPIPS (\(\)) \\  NeRF  & 19.03 & 0.670 & 0.398 & 17.19 & 0.559 & 0.457 \\ DDP  & 19.29 & 0.695 & 0.368 & 19.18 & 0.651 & 0.361 \\ SCADE  & 21.54 & 0.732 & 0.292 & 20.13 & 0.662 & 0.358 \\ Di\&RF  & 21.28 & **0.741** & 0.323 & 19.67 & 0.652 & 0.374 \\ 
**Ours** & **21.73** & 0.733 & **0.291** & **20.36** & **0.663** & **0.349** \\   

Table 1: **Novel View Synthesis Results.** Our method outperforms baselines in novel view synthesis on both Scannet and Tanks and Temple Datasets. This is because our novel NeRF regularizer in Eq. 10 can remove additional floaters in the scene as shown in Fig. 4. See Sec. 5.1 for details.

Figure 4: **Visual Effect of \(_{}\) in Eq. 10.** Compared to pre-trained SCADE model, our method can remove additional floaters in the scene (see the boxed region).

gradient descent. Ultimately, **ProvNeRF** jointly updates the underlying NeRF's parameters and \(_{}\) by minimizing

\[_{}=_{}+_{}=_{}+_{_{}^{(1)},, _{}^{(K)},}[_{j}_{(,)} \|(,})-(t_{j,},_{j,})\|_{2}^{2}] \]

where \((t_{j,},_{j,})=_{}^{(j)}()\), \((,})\) are i.i.d. samples from \(}()\), and \(_{}\) is the original objective of the NeRF model, e.g. photometric loss and depth loss. We provide implementation and architectural details in the supplementary material. See Figure 3 for the training pipeline illustration.

## 5 Experiments

Our ProvNeRF learns per-point provenance field \(_{}\) by optimizing \(_{}\) on a NeRF-based model. To validate ProvNeRF, we demonstrate that jointly optimizing the provenance distribution and NeRF representation can result in better scene reconstruction as shown in the task of novel view synthesis (Sec. 5.1). Moreover, we also show that the learned provenance distribution enables other downstream tasks such as estimating the uncertainty of the capturing field (Sec. 5.2). We provide an ablation study on fMLE against other probabilistic methods in Sec. 5.3. Lastly, in Sec. 5.4, we show a preliminary extension of ProvNeRF to 3DGS .

**Stochastic Provenance Field Visualization** Fig. 7 visualizes the provenance stochastic field by sampling \(16\) provenances on a test view of the Scannet 758 scene. The directions of the samples are the _negative_ of the predicted provenance directions for better illustration. Each sample is colored based on its predicted visibility. Notice that fIMLE allows ProvNeRF to predict multimodal provenance distributions at different scene locations.

### Novel View Synthesis

We show modeling per-point provenance improves sparse, unconstrained novel view synthesis. As a point's provenances are sample locations from where the point is likely visible, the region between the provenance location samples and the query point should likely be empty. We design our provenance loss for NVS with this intuition.

Concretely, starting from a given NeRF model, we first sample points \(_{1},...,_{N}\) for a training camera ray parameterized as \(}_{x}(t)\). Here we denote point \(_{i}=}_{x}(_{i})\). We only take points \(_{i}\) with transmittance greater than a selected threshold \(=0.9\). For each visible point \(_{i}\), we sample provenances \((t_{1}^{(i)},_{1}^{(i)}),,(t_{K}^{(i)},_{K}^{(i)})\) from \(_{}(_{i})\) with \(\|_{1}^{(i)}\|_{2} 0.7\). Then each distance-direction tuple \((t_{j}^{(i)},_{j}^{(i)})\) gives a location \(_{j}^{(i)}=_{i}-t_{j}^{(i)}_{j}^{(i)}\) from which \(_{i}\) is observed. This in turn means \(\) should be equally visible when rendered from ray parameterized as \(_{x}^{(i)}(t)=_{j}^{(i)}+t_{j}^{(i)}, j\). With this, we define our provenance loss for novel view synthesis as

\[_{}=_{i=1}^{N}_{j=1}^{K}[\ +\ T(_{x}^{(i)}(t_{j}^{(i)}))-T(}_{x}(_{i}))]_{+}, \]

where \([]_{+}\) denotes the hinge loss and \(=0.05\) is a constant margin. \(_{}\) encourages the transmittance at \(_{i}\) along training camera rays to be _at least_ the visibility predicted by the sampled provenances from the provenance field with margin \(\). By matching transmittances between the provenance directions and the training rays, \(_{}\) can be used together with \(_{}\) to optimize the NeRF representation and the provenance field, resulting in an improved scene geometry. We apply ProvNeRF to SCADE  for the task of novel view synthesis. See the supplement for details on the dataset, metrics, baselines, and implementation details.

**Results.** Table 1 shows our approach outperforms the state-of-the-art baselines in NVS on scenes from both the Scannet  and Tanks and Temples  dataset. Qualitative comparisons are shown

    & PSNR (\(\)) & SSIM (\(\)) & LPIPS (\(\)) \\  Deterministic Field & 21.38 & 0.720 & 0.307 \\ Frustum Check & 21.56 & 0.728 & 0.297 \\ 
**Ours** & **21.73** & **0.733** & **0.291** \\   

Table 2: **NVS Ablation Results on Scannet.**in Fig. 4. We see that compared to the baseline SCADE, whose geometry is already relatively crisp, our \(_{}\) can further improve its NVS quality by removing additional cloud artifacts, as shown in the encircled regions. Note that this improvement does not require any additional priors and is only based on the provenance of the scene.

We also compare our performance with deterministic baselines: _Deterministic Field_ regresses one provenance for each 3D location using a neural network and _Frustum Check_ calculates the training provenance defined in Eq. 4 by back-projecting the sampled points to one of the training camera and use that as the regularization information. Table 2 shows that our provenance field outperforms these baselines on the novel view synthesis task because the deterministic field cannot model complex provenance distribution and the frustum check baseline lacks generalization ability as it cannot be optimized to adapt the output provenance based on the current NeRF's geometry.

### Modeling Uncertainty in the Capturing Process

Provenances allow for estimating the uncertainty in triangulation, i.e., the capturing process. In classical multiview geometry , the angle between the rays is a good rule of thumb that determines the accuracy of reconstruction. Fig. 6 illustrates this rule as the region of uncertainty changes depending on the setup of the cameras. Formally, for a 3D point \(\), we sample provenances \(\{(t_{j},_{j})\}_{j=1}^{K}\) from \(_{}()\). Treating \(_{j}\) as the principal axes and \(t_{j}\) as the distances from \(\) to the camera origin, each provenance sample defines a pseudo camera \(P_{j}\) that observes \(\) at pixel location \(x_{j}=_{j}()\). Following chapter 12.6 of , we define the triangulation uncertainty of \(\) as the probability of \(\) given its noisy 2D pseudo observations: \((|x_{1},,x_{K})(x_{1}, ,x_{K}|)()=_{j=1}^{K}(x_{j}|)().\)

Figure 5: **Qualitative Results for Uncertainty Modeling.** We visualize our uncertainty maps obtained using the method described in Sec. 5.2. The uncertainty and depth error maps are shown with color bars specified. Uncertainty values and depth errors are normalized per test image for the result to be comparable. As shown in the boxed regions, our method predicts uncertainty regions with more correlation with the predicted depth errors.

    &  &  \\   & Avg. & \(\#710\) & \(\#758\) & \(\#781\) & Avg. & Room 0 & Room 1 & Room 2 \\  Ensemble & 7.71 & 3.01 & 2.96 & 17.2 & 63.0 & 8.04 & 110 & 71.3 \\ CF-NeRF  & 660 & 430 & 571 & 980 & 507 & 799 & 488 & 233 \\ Bayesâ€™ Rays  & 5.47 & 5.11 & 5.23 & 6.07 & 5.49 & 5.67 & 5.77 & 5.91 \\ 
**Ours** & **-3.05** & **0.19** & **-1.93** & **-7.40** & **-11.0** & **-13.6** & **-10.2** & **-9.17** \\   

Table 3: **NLL Results for Triangulation Uncertainty.**

Figure 6: **Triangulation Uncertainty .** The figure shows that \(^{}\) is more uncertain compared to \(\) because the predicted provenances for \(^{}\) give a narrower baseline than the baseline given by provenances of \(\).

The last two equalities are derived by assuming independence of the 2D observations and each \((x_{j}|)\) follows a Gaussian distribution \((_{j}(),^{2})\). This assumption is equivalent to corrupting each 2D observation \(_{j}()\) by a zero-mean Gaussian noise with \(^{2}\) variance, accounting for measurement noises in the capturing process. Assuming a uniform prior of \(()\) over the scene bound, the exact likelihood can be efficiently computed with importance sampling. This quantifies a point's triangulation quality given the sampled provenances, which becomes a measurement of the uncertainty of the capturing process. We apply our provenance field to ProvNeRF with different NeRF backbones  and compute the likelihood. See supplementary for details on the dataset, metrics, baselines, and implementations.

Results.Tab.3 shows the quantitative results on Scannet  and Matterport3D . We follow  to measure the negative log-likelihood (NLL) of the ground-truth surface under each model's uncertainty prediction. Since our **ProvNeRF** can be applied to any pre-trained NeRF module, we use pre-trained SCADE  for Scannet and DDP  for Matterport3D, both of which are state-of-the-art approaches in each dataset. Our approach achieves the best NLL across all scenes in both datasets by a margin because we compute a more fundamentally grounded uncertainty from classical multiview geometry  based on triangulation errors, while both CF-NeRF and Bayes' Rays require an approximation of the true posterior likelihood. Fig.5 shows qualitative comparisons between baselines' and our method's uncertainty estimation. We expect a general correlation between uncertain regions with high-depth errors. An ideal uncertainty map should mark high-depth error regions with high uncertainty and vice versa. As shown in the boxed regions in the figure, our method's uncertainty map shows better correlation with the depth error maps. We also quantitatively evaluate uncertainty maps using negative log-likelihood following prior works Notice that in both examples, our method's certain (blue) regions mostly have low-depth errors (e.g., encircled parts in Fig.5) because our formulation only assigns a region to be certain if it is well triangulated (Fig.6). On the other hand, baselines struggle in these regions because they either use an empirical approximation from data or a Gaussian approximation of the ground truth posterior likelihood.

### Ablation Study

We validate the choice of fIMLE as our probabilistic model by measuring the average precision (AP)  and area under the curve (AUC)  of predicted provenances \((t,)\) against ground truth provenances \((,})\) for a set of densely sampled points in the scene bound. See supplementary for metric and ablation implementation details.

Deterministic v.s. Stochastic Field.We validate the importance of modeling per-point provenance as a stochastic field rather than a deterministic field. We model \(_{}\) with a deterministic field parameterized by a neural network. Table4 shows the importance of modeling per-point provenance as a stochastic field. Since the provenances of a point are inherently multimodal, a deterministic field that only maps each \(\) to a single provenance cannot capture this multimodality.

Choice of Probabilistic Model.We validate our choice of fIMLE  as our probabilistic model. We first compare with explicit probabilistic models that model the provenance field as a mixture of \(C\) Gaussian processes and a VAE-based model. Table4 shows results for the Gaussian Mixture field with \(C=2,5\) and the VAE-based process. Although the performances for the Gaussian-based models improve as we increase \(C\), they still suffer from expressivity because of their explicit density

    & AP (\(\)) & AUC (\(\)) \\  Deterministic Field & 0.163 & 0.168 \\ Gaussian-based w/ \(C=2\) & 0.537 & 0.539 \\ Gaussian-based w/ \(C=5\) & 0.629 & 0.631 \\ VAE-based & 0.323 & 0.325 \\ 
**ProvNeRF** w/ Spatial Inv. \(\) & 0.742 & 0.744 \\
**Ours** & **0.745** & **0.747** \\   

Table 4: **Ablation Results on Scannet.**

Figure 7: **Visualization of Provenance Field.**

assumption. Similarly, the VAE-based model suffers from mode-collapse while our fIMLE enables capturing a more complex distribution with a learned transformation \(_{}\).

Choice of Random Function \(\).Lastly, we validate our latent stochastic field \(\). We ablate our choice of \(\) with instead using a spatially _invariant_ latent stochastic field \(^{}\) with \(^{}()=[,]  x\). Here, \(\) is a Gaussian noise vector in \(^{d}\). Table 4 shows the comparison between \(_{}\) obtained by transforming \(\) (**Ours**) and transforming \(^{}\) (**Spatial Inv.**\(\)). We see that using a spatially varying latent stochastic field further increases the expressivity of our model.

### Preliminary Extension to 3D Gaussian Splatting

Because ProvNeRF is a post-hoc method that can model the provenance information for arbitrary novel view synthesis representations, we conduct a preliminary experiment that extends our provenance field modeling to 3D Gaussian Splatting . Specifically, given a pre-trained Gaussian representation \(\), we model a provenance distribution for each splat using IMLE with a shared 6-layer MLP for \(_{}\). The post-training takes around 30 minutes on a single A6000 Nvidia GPU. To show the usefulness of ProvNeRF applied to 3DGS, we use the methodology in Sec. 5.2 to estimate uncertainty maps and compare them with the predicted depth errors. Fig. 8 shows a qualitative comparison of our uncertainty map w.r.t. FishRF , a recent 3DGS uncertainty estimation baseline. Compared to their uncertainty map, ours shows more correlation to the depth error as highlighted by the boxed regions. Quantitatively we evaluate NLL on the three Scannet scenes shown on the right side of the same figure and show substantial improvements over FishRF. This improvement over existing literature suggests applying ProvNeRF to other representations such as 3DGS is promising. We leave further exploration of the method and applications as future works.

## 6 Conclusion, Limitation, & Future Works

We present ProvNeRF, a model that enhances the traditional NeRF representation by modeling provenance through an extension of IMLE for stochastic processes. ProvNeRF can be easily applied to any NeRF model to enrich its representation. We showcase the advantages of modeling per-point provenance in various downstream applications such as improving novel view synthesis and modeling the uncertainty of the capturing process.

We note that our work is not without limitations. Our ProvNeRF requires post-hoc optimization, which takes around 8 hours on SCADE, limiting its current usability for real-time or on-demand applications. However, the idea presented in our work is not specific to the model design and can be adapted to other representations. See Sec. 5.4 for preliminary adaption of ProvNeRF to 3DGS.

We also note that the hyperparameters to incorporate ProvNeRF are chosen for better performance, e.g. for the uncertainty and novel view synthesis applications, and in the future, it will be beneficial to explore a more adaptive approach in integrating provenance to different downstream applications.

AcknowledgementThis work is supported by a Vannevar Bush Faculty Fellowship, ARL grant W911NF-21-2-0104, an Apple Scholars in AI/ML PhD Fellowship, and the Natural Sciences and Engineering Research Council of Canada (NSERC).

Figure 8: **Uncertainty Estimation Comparison with 3DGS.** Compared with FishRF, our method is able to estimate uncertainties that correlate more with the depth error as shown by the encircled regions. The right shows a quantitative comparison of uncertainty in negative log-likelihood.