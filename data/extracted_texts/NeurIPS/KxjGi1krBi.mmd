# Bayesian Optimization of Functions

over Node Subsets in Graphs

 Huidong Liang Xingchen Wan* Xiaowen Dong

Department of Engineering Science, University of Oxford

{huidong.liang,xiaowen.dong}@eng.ox.ac.uk, xingchenw@google.com

###### Abstract

We address the problem of optimizing over functions defined on node subsets in a graph. The optimization of such functions is often a non-trivial task given their combinatorial, black-box and expensive-to-evaluate nature. Although various algorithms have been introduced in the literature, most are either task-specific or computationally inefficient and only utilize information about the graph structure without considering the characteristics of the function. To address these limitations, we utilize Bayesian Optimization (BO), a sample-efficient black-box solver, and propose a novel framework for combinatorial optimization on graphs. More specifically, we map each \(k\)-node subset in the original graph to a node in a new combinatorial graph and adopt a local modeling approach to efficiently traverse the latter graph by progressively sampling its subgraphs using a recursive algorithm. Extensive experiments under both synthetic and real-world setups demonstrate the effectiveness of the proposed BO framework on various types of graphs and optimization tasks, where its behavior is analyzed in detail with ablation studies. The experiment code can be found at github.com/LeonResearch/GraphComBO.

## 1 Introduction

In the analysis and optimization of transportation, social, and epidemiological networks, one is often interested in finding a node subset that leads to the maximization of a utility. For example, incentivizing an initial set of users in a social network such that it leads to the maximum adoption of certain products; protecting a set of key individuals in an epidemiological contact network such that it maximally slows down the transmission of disease; identifying the most vulnerable junctions in a power grid or a road network such that interventions can be made to improve the resilience of these infrastructure networks.

The scenarios described above can be mathematically formulated as optimizing over a utility function defined on node subsets in a graph, which is a non-trivial task for several reasons. First, most conventional optimization algorithms are designed for continuous space and are hence not directly applicable to functions defined on discrete domains such as graphs. Second, optimizing over a \(k\)-node subset leads to a large search space even for moderate graphs, which are not even fully observable in certain scenarios (e.g. offline social networks). Finally, the objective functions are usually black-box and expensive to evaluate in many applications, such as the outcome of a diffusion process on the network  or the output of a graph neural network , making sample-efficient queries a necessary requirement.

Assuming the graph structure is fully available, the optimization task described above shares similarities with those encountered in the literature on network-based diffusion. In that literature, greedyalgorithms [28; 32; 7] have been widely used to select a subset of nodes that maximizes a utility function, for example in the context of influence maximization  or source identification . However, as the underlying functions often require calculating expectations over a large number of simulations (e.g. the expected number of eventual infections from an epidemic process), such algorithms often become extremely time-consuming as the evaluation time for each diffusion process increases . To relieve the inefficiency in computation, proxy-based methods, such as PageRank , generalized random walks , and DomiRank , are often used in practice to rank the importance of nodes. However, such methods completely ignore the underlying function and require full knowledge of the graph structure beforehand. Finally, most methods mentioned above are task-specific, and the one designed for a specific diffusion process usually does not generalize well to another.

In this paper, we consider the challenging optimization setting for black-box functions on node subsets, where the underlying graph structure is not fully observable and can only be incrementally revealed by queries on the fly. To facilitate this setting, we propose a novel strategy to conduct the search in a combinatorial graph space (termed "combo-graph") in which each node corresponds to a \(k\)-node subset in the original (unknown) graph. The original problem is thus turned into optimization over a function on the combo-graph, where each node value is the utility of the corresponding subset. Traditional graph-traversing methods, such as breadth-first search (BFS) or depth-first search (DFS), may not work well in this case due to the exceedingly large search space and their lack of capability to exploit the behavior of the underlying function. Bayesian optimization (BO), a sample-efficient black-box solver for optimizing expensive functions via surrogate modeling of its behavior, presents an appealing alternative.

**Contributions.** We propose a novel Bayesian optimization framework for optimizing black-box functions defined on node subsets in a generic and potentially unknown graph. Our main contributions are as follows. To the best of our knowledge, this is the first time BO has been applied to such a challenging optimization setting. Our framework consists of constructing the aforementioned combo-graph, and traversing this combo-graph by moving around a combo-subgraph sampled by a recursive algorithm. Notably, the proposed framework is function-agnostic and applies to any expensive combinatorial optimization problem on graphs. We validate the proposed framework on various graphs with different underlying functions under both synthetic and real-world settings, and demonstrate its superiority over a number of baselines. We further analyze its behavior with detailed ablation studies. Overall, this work opens new paths of research for important optimization problems in network-based settings with real-world implications.

## 2 Preliminaries

BO [38; 20] is a gradient-free optimization algorithm that aims to find the global optimal point \(x\)* of a back-box function \(f:\) over the search space \(\), which, in the case of maximization, can be written as \(x^{*}=*{arg\,max}_{x}f(x)\). To efficiently search for the optimum of expensive-to-evaluate functions, BO first builds a _surrogate model_ based on existing observations to predict the function values and their uncertainties over the search space \(\), then utilizes an _acquisition function_ to decide the next location for evaluation.

Surrogate model.One of the most common surrogates used in BO literature  is the _Gaussian Processes_ model: \(f(x)(m(x),k(x,x^{}))\), in which \(m(x)\) is the mean function (often set to a constant \(\) vector) and \(k(x,x^{})\) is a pre-specified covariance function that measures the similarity between data point pairs. With a training set \(_{t}=\{_{1:t},_{1:t}\}\) of \(t\) observations, the posterior distribution of \(f(x_{t+1})\) for a new location \(x_{t+1}\) can be analytically computed from the Gaussian conditioning rule, where the mean is given by \((x_{t+1}|_{t})=(x_{t+1},_{1:t})_{1 :t}^{-1}_{1:t}\) with covariance \(k(x_{t+1},x^{}_{t+1}|_{t})=k(x_{t+1},x^{}_{t+1})- (x_{t+1},_{1:t})_{1:t}^{-1}( _{1:t},x^{}_{t+1})\). Note that the computational cost for \(_{1:t}^{-1}\) is at \((t^{3})\), which largely restricts the efficiency of \(\) when training on large datasets and therefore often requires a local modeling approach.

Acquisition function.Based on the predictive posterior distribution, an acquisition function will be applied to balance the exploration-exploitation trade-off via optimizing under uncertainty. For example, the _Expected Improvement_[37; 26], defined as \(_{1:t}(x^{})=[f(x^{})-f(x_{1:t}^{*})]^{ +}\) with \([]^{+}=(,0)\) and \(x_{t}^{*}=*{arg\,max}_{x_{t} x_{1:t}}f(x_{i})\), measures the expected improvement based on the current best query. Then, the next query location is chosen as \(x_{t+1}=*{arg\,max}_{x^{}\{x_{i}\}_ {i=1}^{t}}_{1:t}(x^{})\)and the result \(\{x_{t+1},y_{t+1}\}\) will be appended to the visited set \(_{t}\). The algorithm will repeat these steps until the stopping criteria are triggered at a certain iteration \(T\), and we report \(x_{T}^{*}=_{x_{i} x_{1:T}}f(x_{i})\) as the final result.

## 3 BO of Functions over Node Subsets in Graphs

Settings and challenges.Following the notations in SS2, we formally introduce the proposed Bayesian optimization framework for black-box functions over node subsets in graphs, termed _GraphComBO_. The goal of the problem is to find the global optimal \(k\)-node subset \(^{*}\) of a black-box function \(f()\) over the search space of all possible \(k\)-node subsets \(}{k}\) on a generic graph \(=\{,\}\), which, in the case of maximization, can be expressed as \(^{*}=_{}{k}}f()\). Under noisy settings, we may only observe \(y=f(x)+\), with \( N(0,_{}^{2})\) being the noise term. For simplicity, we focus on _undirected_ and _unweighted_ graphs where the adjacency matrix \(\) is symmetric and contains binary elements. As \(f\) is often expensive to evaluate in practice, we wish to optimize the objective in a query-efficient manner within a limited number of evaluations \(T\), and report the best configuration among them as the final solution: \(_{T}^{*}=_{_{i}\{_{i}\}_{i=1}^{T}}f (_{i})\).

Despite BO's appeals in optimizing such functions, we observe the following challenges when designing effective algorithms for combinatorial problems on graphs:

1. **Structural combinatorial space.** Unlike classical combinatorial optimization in the discrete space, the combination of nodes (a node subset) inherits structural information from the underlying graph, which needs to be properly encoded into the combinatorial search space. In addition, an appropriate similarity measure between node-subset pairs is also required to capture such inherent structural information when building the surrogate model.
2. **Imperfect knowledge of graph structures.** As the complete structure of real-world graphs may be expensive or even impossible to acquire (e.g. a gradually evolving social network), any prospectus optimization algorithm needs to handle the situation where the graph structure is only revealed incrementally.
3. **Local approach while combining distant nodes.** As the massive size \(|}{k}\) of the combinatorial space often makes global optimization unattainable, an effective local modeling approach is

Figure 1: Demonstration of how the proposed framework traverses the combinatorial graph \(}^{<k>}\) introduced in §3.1 with an exemplar original graph \(\) of nodes and a subset size of \(k=2\). At iteration t, we first construct a local combo-subgraph \(}_{t}=\{}_{t},}_{t}\}\) of size \(Q\)=6 using Algorithm 1 (§3.1), which is centred at combo-node \(_{t-1}^{*}\) from last iteration t-1 or initialization. Next, a \(\) surrogate is fitted on \(}_{t}\) with queried combo-nodes inside \(}_{t}\) being the training set. The next query location is then selected as the combo-node that maximizes the acquisition function \(_{t}^{*}=_{}_{t}}()\). If queried values \(f(_{t}^{*}) f(_{t-1}^{*})\), the next combo-subgraph \(}_{t+1}\) will be re-sampled at a new center \(_{t}^{*}\), or otherwise remain the same. Finally, we repeat the previous process to obtain a new query location for the next iteration t+1, and the search continues until stopping criteria are triggered.

needed to efficiently traverse the graph. However, as the optimal subset usually consists of nodes that are far away from each other (e.g., the optimal locations of hospitals in a city network), it is critical to maintain the flexibility of selecting distant nodes when considering a local context.

In the following sections, we will discuss how the proposed GraphComBO addresses these challenges, where an overview of the framework can be found in Figure 1.

### The Combinatorial Graph for Node Subsets

Inspired by the graph Cartesian product that projects multiple "subgraphs" into a combinatorial graph, we introduce a combinatorial graph (denoted as _combo-graph_) tailored for node subsets on a single generic graph with an intuitive example demonstrated in Figure 2.

**Definition 3.1**.: The combinatorial operation for \(k\)-node subsets in an underlying graph \(=(,)\) is given by:

\[}^{<k>}=_{i=1}^{k},\] (1)

which leads to a combo-graph \(}^{<k>}=\{},}\}\) of size \(|}|=|}{k}\) with each combo-node \(_{i}=(v_{i}^{(1)},v_{i}^{(2)},...,v_{i}^{(k)})}\) being a \(k\)-node subset from the underlying graph \(\) without replacement. The combo-edges \(}\) in the combo-graph are defined in the following way: assume \(_{1}=(v_{1}^{(1)},v_{1}^{(2)},...,v_{i}^{(k)})\) and \(_{2}=(v_{2}^{(1)},v_{2}^{(2)},...,v_{2}^{(k)})\) are two arbitrary combo-nodes in the combo-graph \(^{<k>}\), then \((_{1},_{2})}\) iff \(\)\(j\) such that \(\)\(i j\), \(v_{1}^{(i)}=v_{2}^{(i)}\) and \((v_{1}^{(j)},v_{2}^{(j)})\).

Intuitively, this means that in the combo-graph, two combo-nodes are adjacent if and only if they have exactly \(1\) element (i.e. node from the original graph) in difference and the two different elements are neighbors in the original graph. Note that as \(k\) shrinks to \(1\), the combo-graph reduces to the underlying graph.

Nevertheless, as the combo-graph size \(|}{k}\) is often too large in practice, building the surrogate and making predictions at a global scale is usually unrealistic. A sensible alternative would be adopting a commonly used local modeling approach  and then gradually moving around the "window" guided by the surrogate predictions. Unlike classical continuous space, constructing local regions on the combo-graph is not straightforward. Next, we will discuss two properties of the proposed combo-graph, which enable us to practically employ local modeling by sampling subgraphs for tractable optimization. Reads are also referred to SSD for proofs of the following lemmas.

**Lemma 3.2**.: _In the proposed combo-graph, at most \(\) elements in the subset will be changed between any two combo-nodes that are \(\)-hop away._

This implies that when considering an \(\)-hop ego-subgraph centered at an arbitrary combo-node \(\) on the combo-graph, we are effectively exploring the \(\)-hop neighbors of elements in \(\) in the original graph. Since such operation requires no prior knowledge of the other part of the original graph, we are then able to gradually reveal its structure by moving around the focal combo-node, and hence handling the situation of optimizing over node subsets on an incomplete or even unknown graph.

**Lemma 3.3**.: _The degree of combo-node \(_{i}\) increases linearly with \(k\) and is maximized by the subset of nodes with top \(k\) degrees: \((_{i})=_{j=1}^{k}|(v_{i}^{(j)})\{v_{i}^{( j^{})}\}_{j^{} j}^{k}|\)._

Therefore, the size of the above ego-subgraph only needs to increase linearly with \(k\) to cover the first hop combo-neighbors. These two properties together make the construction of local combo-subgraphs feasible, and we introduce a sampling algorithm in the next section that recursively finds combo-nodes and combo-edges for a combo-subgraph (denoted as \(}\)) given a focal combo-node.

Recursive combo-subgraph sampling.As illustrated in Algorithm 1 and Figure 2, our goal is to construct an ego-subgraph \(}\) of size \(Q\) from the underlying graph \(\), centered at a given combo-node

Figure 2: Illustration of a combinatorial graph \(}^{<2>}\) constructed by the recursive combo-subgraph sampling (Algorithm 1).

\(^{*}\) with maximum hop \(_{}\). The algorithm initializes \(}=\{},}\}\) with only \(^{*}\) and then loop through neighbors of each element node \(v^{*}\). If a neighbor \(_{i}(v)\) of \(v\) is not in \(^{*}\) (i.e. ensuring no repetition in the subset), a new combo-node \(^{}\) will be created by substituting \(_{i}(v)\) with \(v\) in \(^{*}\), and a combo-edge will be accordingly created by connecting \(^{}\) to \(^{*}\). As a result, after finding the combo-neighbors of \(^{*}\) at hop \(=1\), \(}\) becomes a star-network at center \(^{*}\). We will then repeat the above procedures (i.e. star-sampling) for every newly found combo-node to find their combo-neighbors at hop \(+1\) (which meanwhile also finds the edges among combo-nodes within the previous hop \(\)), until the subgraph size limit \(Q\) or the maximum hop \(_{}\) is reached.

By constructing the combo-graph and sampling subgraphs from it, we can efficiently traverse the combinatorial space by progressively moving around the combo-subgraph center while preserving diversified combinations of distant nodes under a local modeling approach, which will be discussed in the following section.

### Graph Gaussian Processes Surrogate

After constructing the combo-subgraph, we can build a surrogate model for the expensive underlying function on this local region with graph Gaussian Processes (\(\)). Specifically, we consider the normalized graph Laplacian \(}\): \(}=-}^{-1/2}} }^{-1/2}\) for a combo-subgraph \(}\), where \(}\) is the adjacency matrix and \(}\) is the degree matrix. Then, the eigendecomposition of the graph Laplacian matrix is given by \(}=^{}\), in which \(=(_{1},,_{n})\) are the eigenvalues sorted in ascending order and \(=[_{1},,_{n}]\) are their corresponding eigenvectors. Now let \(i,j\{1,,n\}\) be two indices of combo-nodes on \(}\), the covariance function (or _kernel_) \(k(_{i},_{j})\) between an arbitrary combo-node pair \(_{i}\) and \(_{j}\) can be formulated in the form of a regularization function \(r(_{p})\) defined on the eigenvalues \(\{_{p}\}_{p=1}^{n}\):

\[k(_{i},_{j})=_{p=1}^{n}r^{-1}(_{p})_{p}[i]_{p}[j],\] (2)

where \(_{p}[i]\) and \(_{p}[j]\) are the \(i\)-th and \(j\)-th elements in the \(p\)-th eigenvector \(_{p}\), and \(r(_{p})\) is some scalar-valued function for regularization. We refer readers to Appendix SE for discussion on a collection of commonly used kernels on graphs under the form of Equation (2).

### Bayesian Optimization on the Combo-graph

With the structural combinatorial space and techniques to sample and build surrogate models on the combo-subgraphs, we now introduce the proposed GraphComBO framework in detail. For simplicity, we consider maximization in the following paragraphs, where the overall structure can be found in Figure 1 with key procedures summarized in Algorithm 2 and complexity discussed in Appendix SSC.

``` Input: Original (unknown) graph \(\); The focal combo-node \(^{*}\); Combo-subgraph size \(Q\); Max neighbor hop \(_{}\). Initialize: An combo-subgraph \(}=\{},}\}\) with \(}=\{^{*}\}\)\(}=\); Starting hop \( 1\); Set of newly found combo-nodes \(}_{new}\{^{*}\}\). Define: Recursive_Sampler\((,},}_{new},Q,)\)
1:for\(\) in \(}_{new}\)do
2:for\(v\) in \(\)do
3: Reveal the neighbors \((v)\) of \(v\) in \(\).
4:for\(v^{}\) in \((v)\)parallelydo
5: Generate a new combo-node by \(([v^{}, v]\)) and then create a combo-edge by connecting it to \(\).
6:endfor
7: Update \(\{},}\}\) in \(}\).
8:endfor
9:if\(|}|>Q\) or \(>_{}\)then
10: Randomly drop the extra combo-nodes.
11:return The combo-subgraph \(}\) of size \(Q\).
12:endif
13:endfor
14: Update \(}_{new}}} _{new}\); \(+1\)
15:returnRecursive_Sampler\((,},}_{new},Q,)\) ```

**Algorithm 1** Recursive combo-subgraph sampling

Combo-subgraphs as trust regions.As discussed earlier, performing global modeling directly for combinatorial problems is usually impractical. Thus, inspired by the _trust region_ method popularly used in continuous numerical optimization , reinforcement learning  and BO under other settings , we take a local modeling approach on the combo-graph during the BO search. Starting with a random location (i.e. a combo-node \(_{0}\)) or a reasonable guess from domain knowledge, a combo-subgraph \(}_{0}\) will be constructed at center \(_{0}\) by Algorithm 1. We will then move around this combo-subgraph \(}_{t}\) at each iteration \(t\) on the combo-graph by changing its focal combo-node guided by the surrogate model and acquisition function, which will be explained shortly.

In particular, we introduce a hyperparameter \(Q\) that caps the combo-subgraph size to control the computational cost for the surrogate \(\), and then use the queried combo-node inside \(}_{t}\) as the training set \(_{t}\) to fit the model (i.e. update the hyperparameters in its kernel). The acquisition function \(()\) is then applied on the rest of unvisited combo-nodes in \(}_{t}\), and we select the combo-node \(_{t}=_{}_{t}}()\) as the next location to query the underlying function. Here, any commonly used kernel and acquisition function are compatible with our setting, and we adopt the popular _diffusion_ kernel  with _Expected Improvement_ acquisition  in our experiments.

After querying the next location, we re-select the best-queried combo-node \(_{t}^{}\) in our training set \(_{t}[]\) by choosing \(_{t}^{}=_{_{t}[]}f()\), and compare it to the previous best location \(_{t-1}^{}\). If the best-queried value improves (i.e. \(f(_{t}^{})>f(_{t-1}^{})\)), the combo-subgraph in the next iteration \(}_{t+1}\) will be resampled at this new location \(_{t}^{}\) with Algorithm 1, or otherwise remains the same as \(}_{t}\). The search algorithm then continues until a querying budget \(T\) is reached, and we report the best-queried combo-node as the final result \(_{T}^{}=_{_{1,T}}f()\).

**Balancing exploration and exploitation.** Similar to the continuous domain, the exploration-exploitation trade-off is also a fundamental concern when using BO on the proposed combo-graph, and we introduce two additional techniques to strike a balance between these two matters.

1. failtol that controls the tolerance of "failures" by counting continuous non-improvement steps. Once reached, the algorithm will restart at a new location using restart_method.

2. restart_method that either restarts at a random combo-node, the best-visited combo-node, or the initial starting location if specified.

In addition, the combo-subgraph size \(Q\), which can be viewed as the "volume" of the trust region under graph setting, also controls the step size of exploration. These strategies together can obseively assist GraphComBO in adapting to various tasks. For example, a small failtol will encourage exploration in the combinatorial space when restart_method is set to a random combo-node, which is useful when optimizing an underlying function with low graph signal smoothness . By contrast, when increasing failtol and setting restart_method to the best-queried combo-node, the algorithm will exploit more around the local optimal and is hence more suitable for smoother functions. SSK further provides an ablation study on these hyperparameters.

Impact of the underlying function \(f\) and the subset size \(k\).It is natural to expect that the interaction between the underlying function \(f\) and graph structure, which relates to signal smoothness over the combo-graph, will exert a significant influence on the search performance. Specifically, the optimization is expected to be challenging either when \(f\) is less correlated to the graph structure even if the latter is informative (e.g. random noise on a BA network  as an extreme case), or when \(f\) is correlated to the graph structure but the latter is non-informative (e.g. eigenvector centrality on a Bernoulli random graph ). In the meantime, as the subset size \(k\) increases, exploration will become more expensive when using a combo-subgraph of fixed size \(Q\) or fixed number of hops \(_{}\). Recall that in **Lemma 3.2** where we state that at most \(\) elements will be changed between two combo-nodes that are \(\)-hops away, it implies that more queries are required to exhaust all possible modifications of the elements in the subset when its size \(k\) increases. Empirical findings from our experiments in SS4 further corroborate these hypotheses, where we also provide detailed discussions on model behavior in SSG and kernel performance under different levels of signal smoothness in SSF.

Relation to previous BO methods with graph settings.While BO has been combined with graph-related settings to find the optimal _graph structures_ such as in the literature of NAS [27; 40; 42] and graph adversarial attacks , it remains largely under-explored for optimizing functions defined on the _nodes_ or _node subsets_ in the graph. Although one recent work BayesOptG  considered such novel setups, it only considered functions defined on a single node, which can be viewed as a special case in our setting when \(k=1\). The construction of the "combo-graph" in our approach shares similarity with the construction of the combinatorial graph in COMBO ; however, the problems being addressed there do not arise in a natural graph setting, and we present a more detailed discussion of the related work in Appendix SSA, together with an additional experiment for comparison in SSH.

## 4 Experiments

Setups.We conduct comprehensive experiments on four synthetic problems and five real-world tasks to validate our proposed framework, where readers are also referred to the appendix for discussions on: SSB detailed experimental settings with task descriptions and visualizations; SSE validation of common kernels on graphs under our settings; SSG a thorough analysis of GraphComBO's underlying behavior; and SSK ablation studies on the hyper-parameters. We closely follow the standard setups in BO literature [3; 19; 23]. Specifically, we query 300 times and repeat 20 times with different

Figure 3: Results for synthetic problems on BA, WS, SBM and 2D-Grid networks with \(k=\), where Regret indicates the difference between ground truth and the best query so far.

[MISSING_PAGE_FAIL:8]

* _Maximizing influence on social networks (SSB.4)._ We consider the influence maximization problem on a social network  with independent cascading simulations , in which we aim to select the optimal \(k\) nodes as the seeds (i.e. source of influence) that maximize the expected number of final influenced individuals (represented as a fraction of the network size).
* _Resilience testing on transportation networks (SSB.5)._ The objective of this task is to identify the \(k\) most vulnerable roads (edges), such that their removal will lead to the maximal drop in a certain utility function measuring the operation status (estimated by network transitivity).
* _Black-box attacks on graph neural networks (SSB.6)._ Considering a graph-level GNN pre-trained for molecule classification  with a particular input graph, we conduct a challenging black-box attack with no access to the model parameter but only a limited number of queries for its output. Our goal here is to mask \(k\) edges such that the output from the victim GNN (at softmax) will be maximally perturbed from the original output, as measured by the Wasserstein distance.

Discussion on results.We can observe that the proposed GraphComBO framework generally outperforms all the other baselines with a clear advantage on both synthetic and real-world tasks. It is worth noting that such gain in performance seems to be diminishing as \(k\) increases and, in certain scenarios, BO also tends to perform similarly to local search. While these phenomena are generally consistent with our previous hypothesis in SS3.3, we further provide the following explanations to attain a better understanding of the model's underlying behavior.

1. Given a combo-subgraph with a fixed size \(Q\), we tend to capture structural information in a smaller neighborhood due to the increase of combo-node degrees. First, when \(k\) increases, the combo-node degree will increase linearly (as discussed in **Lemma 3.3**). Second, if the synthetic underlying function has a strong positive correlation with node degree, such as eigenvector centrality, the degree of the center combo-node will increase as the search progresses. Both factors will lead to a smaller neighborhood around the focal node (in terms of shortest path distance) covered by the combo-subgraph, which in turn means more steps are required to explore beyond the current region, especially when the algorithm reaches a local optimum.
2. As discussed in SS3.3, underlying functions with low signal smoothness will negatively affect BO's performance, which partially explains the comparable results between BO and local search on WS and SBM where the graph structures are less informative, as well as on the molecule network where the underlying function involves a graph neural network and is relatively non-smooth compared to other tasks. In addition, as the kernels used in \(\) (SS3.2) come with an underlying assumption on function smoothness, the surrogate model will capture less signal information when fitting a less-smooth function, thus making BO behave similarly to a random model (i.e. the local search). To better support this claim, we further conduct a sensitivity analysis of the kernels to signal smoothness at different levels in Appendix SSF.

Further analysis on model behaviors.Readers are also referred to Appendix SSG for a more detailed behavior analysis that elaborates on the above explanations and Appendix SSK for a thorough ablation study on \(Q\) and failtol. In addition, Appendix SSH provides a comparison with

Figure 5: Results for road resilience testing and GNN attacks on molecules with edge-masking.

COMBO  on small-scale networks, Appendix SS1 tests our framework on a large social network OGB-arXiv with \(||=1.7 10^{5}\), and finally Appendix SSJ discusses the framework's performance under a noisy setting where observations are corrupted at different noise levels.

## 5 Conclusion and Future Works

In this work, we introduce a novel Bayesian optimization framework to optimize black-boxed functions defined on node subsets in a generic and potentially unknown graph. By constructing a tailored combinatorial graph and sampling subgraphs progressively with a recursive algorithm, we are able to traverse the combinatorial space and optimize the objective function using BO in a sample-efficient manner. Results on both synthetic and real-world experiments validate the effectiveness of the proposed framework, and we use detailed analysis to study its underlying behavior.

On the other hand, we have also identified the following limitations during our experiments, which can be explored as future directions for this line of work.

* As discussed in the paper, the performance of BO gradually deteriorates when the subset size \(k\) increases. In this sense, some modifications are expected to better control the combinatorial explosion while preserving useful information from the underlying graph structure.
* The proposed framework adopts a local modeling approach inspired by the trust region method to control the computational cost. However, we expect some improvement in BO's performance if we inject some global information (if available) into surrogate modeling, such as using some self-supervised method with a graph neural network to replace the Laplacian embedding.
* The current algorithm adopts a fixed strategy for hyperparameters like subgraph size \(Q\) and maximum hop \(_{}\), where we believe the optimization would benefit from a more flexible design such as a self-adaptive \(Q\) and \(_{}\) as the search continues.
* In all experiments, we assume no prior knowledge of the problem and adopt a random initialization method before the search. However, it is also an important direction to explore when a good starting location or certain characteristics of the function are available from domain knowledge.

We believe the proposed combo-graph would bring new insights to a broader community of machine learning research on graphs. While there are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.