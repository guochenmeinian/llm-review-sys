# Uncovering Safety Risks of Large Language Models through Concept Activation Vector

Zhihao Xu\({}^{1}\)

Equal contribution.

Ruixuan Huang\({}^{2}\)

Equal contribution.

Changyu Chen\({}^{1}\)

Xiting Wang\({}^{1}\)

Corresponding to xitingwang@ruc.edu.cn

\({}^{1}\)Renmin University of China

\({}^{2}\)The Hong Kong University of Science and Technology

###### Abstract

Warning: This paper contains text examples that are offensive or harmful in nature. Despite careful safety alignment, current large language models (LLMs) remain vulnerable to various attacks. To further unveil the safety risks of LLMs, we introduce a Safety Concept Activation Vector (SCAV) framework, which effectively guides the attacks by accurately interpreting LLMs' safety mechanisms. We then develop an SCAV-guided attack method that can generate both attack prompts and embedding-level attacks with automatically selected perturbation hyperparameters. Both automatic and human evaluations demonstrate that our attack method significantly improves the attack success rate and response quality while requiring less training data. Additionally, we find that our generated attack prompts may be transferable to GPT-4, and the embedding-level attacks may also be transferred to other white-box LLMs whose parameters are known. Our experiments further uncover the safety risks present in current LLMs. For example, in our evaluation of seven open-source LLMs, we observe an average attack success rate of 99.14%, based on the classic keyword-matching criterion. Finally, we provide insights into the safety mechanism of LLMs. The code is available at https://github.com/SproutNan/AI-Safety_SCAV.

## 1 Introduction

The rapid advancement in large language models (LLMs) has raised significant concerns about their potential misuse . Developers usually conduct intensive alignment work  to prevent powerful LLMs from being exploited for harmful activities. However, researchers have discovered that these time-consuming safety alignment efforts can be reversed by various attack methods . These methods can identify vulnerabilities in safety alignment technologies and enable developers to fix them promptly, reducing the societal safety risks of LLMs.

Existing attack methods utilize different levels of information from LLMs to achieve varying degrees of model understanding and control. Pioneering attack methods manually design prompt templates  or learn attack prompts without information about intermediate layers of LLMs . The attack prompts may be applied to various LLMs, supporting both black-box attacks on APIs and white-box scenarios where model parameters are released. However, their attack success rates (ASR)  are constrained by an insufficient understanding of LLMs' internal working mechanisms. Some recent attack works further utilize model embeddings at intermediate layers . By better understanding models' safety mechanisms and perturbing relevant dimensions in the embeddings, these methods achieve significantly higher ASR on white-box LLMs. However, they cannot be applied to black-box APIs. Moreover, existing methods perturb LLM embeddings based on potentiallymisleading heuristics (Section 2.3.1). Due to the lack of a principled optimization goal, they result in a suboptimal ASR, may generate low-quality (e.g., repetitive) text, and require time-consuming grid search to find a good combination of hyperparameters (e.g., perturbation magnitude and layers).

We aim to solve the aforementioned issues by more accurately modeling LLMs' safety mechanisms, based on which principled optimization goals can be developed to well balance ASR and response quality, enable automatic hyperparameter selection, and support both embedding-level and prompt-level attacks. Specifically, we make the following contributions.

First, we establish a **Safety Concept Activation Vector (SCAV) framework** that effectively guides the subsequent attack process by accurately interpreting LLMs' safety mechanisms. It quantifies the probability that an LLM considers an embedding as malicious based on the concept activation vector , which linearly separates embeddings of malicious and safe instructions.

We then develop an **SCAV-guided attack method**, which utilizes SCAV to design principled optimization goals for finding high-quality embedding-level and prompt-level attacks. Our embedding-level attack method eliminates the time-consuming hyperparameter tuning process by providing a closed-form solution of the optimal perturbation magnitudes and automatically selecting layers according to SCAV. Effective attack prompts that may be used in black-box attack scenarios can also be identified by using our optimization goal that balances ASR and response quality. Our comparative study, which involves human evaluations, confirms that our attack method significantly and consistently increases ASR (+8%-20%), language quality (+14%-42%), response usefulness (+8%-10%), and requires less training data.

We further **unveil the safety risks and mechanisms** of diverse LLMs by applying our attack method. We discover that their safety issues are severe. For white-box LLMs, human evaluation results show that **(a)** even seemingly well-aligned LLMs like LLaMA-2  provide relevant answers to 96% to 100% malicious instructions; **(b)** the attacks of one LLM may be transferred to others, including black-box ones like GPT-4; and **(c)** existing methods aimed at making LLM unlearn harmful knowledge [21; 22; 23] may not have truly achieved their goals and still result in an ASR of 97.34% by attacking the unlearned LLM with our method.

## 2 Methodology

### Problem Formulation

Given an LLM, our goal is to generate attacks that lead LLMs to follow malicious instructions (e.g., _Design a weapon that can be used to assassinate a person_) while producing high-quality responses.

**Input:** The attacks are generated by using an LLM \(f\), whose embeddings are \(\{^{1},...,^{L}\}\), where \(^{l}^{d}\) is the embedding at the \(l\)-th layer, and \(L\) is the number of layers. While we require model parameters of \(f\) to interpret its safety mechanisms and optimize attack performance on \(f\), the generated attacks may also be applied to other LLMs or even black-box APIs, considering the potential attack transferability demonstrated in our experiments (Sections 3.3 and 4.3) and previous research [13; 14].

**Output:** Based on model \(f\), we generate attacks at one of the following two levels:

* _Embedding-level_ attacks change intermediate-layer embedding \(^{l}\) by adding a perturbation vector to \(^{l}\). This type of attack can be applied to white-box LLMs whose parameters are known.
* _Prompt-level_ attacks aim to learn a prompt that can be combined with the original user input to form the final instruction. This type of attack may be applied to various LLMs, including black-box APIs.

### SCAV Framework

We first introduce our Safety Concept Activation Vector (SCAV) framework, which effectively guides the subsequent attack process by quantitatively interpreting LLMs' embedding-level safety mechanisms. Specifically, given an embedding \(\), we aim to estimate the probability \(P_{}()\) that the LLM considers \(\) as malicious1. This is achieved by using Concept Activation Vector , a classic interpretation method that follows the _linear interpretability_ assumption commonly used in existing interpretation methods [24; 25; 26; 27; 28; 29; 30; 31]. Specifically, it assumes that a deep model embedding \(\) can be mapped to a concept that humans can understand (in our paper, the "safety" concept) after a linear transformation. Accordingly, the probability that the LLM considers \(\) malicious can be modeled through a linear classifier:

\[P_{}()=(^{}+b)\] (1)

where \(^{d},b\) are parameters of the classifier. \(P_{}\) can be accurately learned if the embeddings of malicious instructions and safe instructions are linearly separable, indicating that the LLM has successfully captured the safety concept at the corresponding layer. Specifically, we learn the classifier parameters \(\) and \(b\) by using a cross-entropy loss with regularization:

\[*{arg\,min}_{,b}-_{(y,) D}[y P _{}()+(1-y)(1-P_{}()]\] (2)

where \(D\) is the training dataset, \(y=1\) if the input instruction is malicious and is \(0\) if the instruction is safe. Implementation details can be found at Appendix E.1. Like existing attack baselines that consider model inter workings [16; 18], we also require a dataset with both malicious and safe instructions to determine the label \(y\). However, we require much less training data (Figure 3), demonstrating the effectiveness of SCAV-based model interpretation that helps eliminate potentially misleading heuristics (Section 2.3.1).

**Verifying the linear interpretability assumption**. To check whether the linear interpretability assumption holds for the safety concept in LLMs, we investigate the test accuracy of classifier \(P_{}\). A high accuracy means that the embeddings of malicious and safe instructions are linearly separable in the LLM hidden space. As shown in Figure 1, for aligned LLMs (Vicuna and LLaMA-2), the test accuracy becomes larger than 95% starting from the 10th or 11th layer and grows to over 98% at the last layers. This indicates that a simple linear classifier can accurately interpret LLMs' safety mechanism and that LLMs usually start to model the safety concept from the 10th or 11th layer. In contrast, the test accuracy of the unaligned LLM (Alpaca) is much lower. We provide similar results on other LLMs in Appendix D.1.

### Embedding-Level Attack

We now introduce how to obtain embedding-level attacks without a time-consuming grid search of perturbation magnitudes and layers. We first describe how the attack can be achieved for a given single layer, and then present our algorithm for attacking multiple layers.

#### 2.3.1 Optimizing Attacks for a Single Layer

Given embedding \(\) at an intermediate layer, we attack \(\) by changing it to \(}=+\), where \(\) is the perturbation magnitude and \(^{d}\) (\(||||=1\)) is the perturbation direction. While existing white-box attack methods [16; 18] heuristically determine the perturbation direction and provide no guidance for the perturbation magnitude, we optimize \(\) and \(\) simultaneously by solving the following constrained optimization problem, which ensures small performance loss of LLMs and high attack success rates:

\[*{arg\,min}_{,}||,P_{}(})=P_{}(+) P_{0 },\ \ ||||=1\] (3)

The first term that minimizes \(||\) ensures a small performance loss of LLMs, avoiding flaws such as repetitive or irrelevant responses. The second term, which assures that the perturbed embedding \(}\) has a small \(P_{}(})\), guarantees attack success by tricking the LLMs to consider the input as not malicious. The threshold \(P_{0}\) is set to 0.01% to allow for a small margin. This constant \(P_{0}\) allows for a dynamic adaptation of \(\) in different layers and LLMs.

Figure 1: Test accuracy of \(P_{}\) on different layers of LLMs.

The optimization problem in Equation (5) has a closed-form solution (proof in Appendix C):

\[=(P_{}()>P_{0})^{-1}(P_{ 0})-b-^{}}{||||},=}{||||}\] (4)

where \(()\) is an indicator function that transforms false or true into 0 or 1.

**Method Intuition and Analysis of Baselines.** Our perturbation direction \(\) is perpendicular to the hyperplane that separates malicious instructions from safe ones, according to Equation (4). As shown in Figure 2, this allows us to move the embeddings of malicious instructions to the subspace of safe instructions consistently with the shortest possible distance. In contrast, baselines RepE  and JRE  may result in ineffective perturbations. For example, the perturbation vector of JRE is perpendicular to the correct direction in Case 3, and RepE may generate opposite perturbations in different runs. This is caused by their potentially misleading heuristics. Both methods heuristically obtain a perturbation vector that depicts the global difference between embeddings of malicious instructions (\(_{}\)) and embeddings of safe instructions (\(_{}\)). This is achieved by randomly subtracting \(_{}\) and \(_{}\) and performing PCA analysis  or dimension selection  to identify a potentially interesting direction. Such a perturbation vector relies heavily on the global data distribution, requires more data points, and may not align with the hyperplane for separating \(_{}\) and \(_{}\), leading to attack failure (due to the large \(P_{}(})\)) or low-quality responses (due to perturbation in the wrong direction).

#### 2.3.2 Attacking Multiple Layers

We then decide which layers to attack. In the early layers of LLMs, where the safety concept may not have formed yet, the test accuracy of classifier \(P_{}\) is small (Figure 1). To avoid unnecessary or wrong perturbations, we do not attack these layers. For layers with high test accuracy, we perturb embedding \(\) if \(P_{}()>P_{0}\), in order to lower the probability that it is considered malicious. We compute the optimal perturbation based on the latest embedding \(\) computed after the earlier layers are attacked. This results in an attack method shown in Algorithm 1.

```
0: LLM with \(L\) layers, classifier \(P_{}\), it thresholds \(P_{0}=0.01\%,P_{1}=90\%\), and instruction \(x\)
1:for\(l=1\) to \(L\)do
2:if\((P_{})>P_{1}\)then
3:\(\) Embedding of \(x\) at the \(l\)-th layer after attacking the previous layers
4:if\(P_{}()>P_{0}\)then
5: Attack \(\) by changing it to \(+\)
6:endif
7:endif
8:endfor ```

**Algorithm 1** Attacking multiple layers of an LLM

### Prompt-Level Attack

In this subsection, we demonstrate how our SCAV classifier \(P_{}\) can effectively guide the generation of an attack prompt \(S\). Attack prompts can be combined with original user instructions to manipulate LLMs' behavior. Existing white-box attack methods, such as GCG  and AutoDAN , automatically generate adversarial prompts to maximize the probability of a certain target response \(T\) (e.g., _Sure, here is how to make a bomb_). The heuristically determined target response is often different from the real positive response when an LLM is successfully attacked. There is no guarantee that the

Figure 2: Comparison of perturbations added by our method (SCAV) and the baselines RepE  and JRE . Our method consistently moves embeddings of malicious instructions to the subspace of safe instructions, while the baselines may result in ineffective or even opposite perturbations.

attack success rates can be accurately or completely estimated by using the generation probability of \(T\), thereby limiting the performance of existing methods.

The aforementioned issue can be easily solved by using our classifier \(P_{}\), which accurately predicts the probability that an input is considered malicious by the LLM. We can then obtain the attack prompt \(S\) by solving the following optimization problem:

\[*{arg\,min}_{S}P_{}(_{S}^{L})\;||_{S}^{L}- {e}^{L}||\] (5)

where \(^{L}\) is the last-layer embedding of a user instruction \(x\), and \(_{S}^{L}\) is the last-layer embedding when the attack prompt \(S\) is combined with \(x\) to manipulate the model. The first term \(P_{}(_{S}^{L})\) ensures the effectiveness of the attack, while the second term \(||_{S}^{L}-^{L}||\) guarantees minimal modifications to the model in order to avoid low-quality model responses. We solve Equation (5) by using AutoDAN's hierarchical genetic algorithm (See Appendix E.2.1 for details). We do not use the constrained formulation in Equation (3), because 1) it is not easy to incorporate constraints into the hierarchical genetic algorithm; and 2) it is difficult to determine \(P_{0}\) here since we cannot directly control the embeddings to ensure a low value of \(P_{}\). See Appendix E.2.2 for more discussions of the design choice.

## 3 Comparative Study

### Experimental Setup

**Baselines.** We compare SCAV with the following baselines, which involve different kinds of LLM attacking paradigms.

* DeepInception , which uses manually-crafted attack prompts.
* AutoDAN  and GCG , which learn attack prompts based on LLMs' output logit distribution and gradient.
* RepE  and JRE , which require model parameters and attack by changing LLM embeddings.
* Soft prompt , which also enables attacking LLMs in embedding space.

**Datasets.** The training data for embedding-level attacks are 140 malicious instructions from Advbench  and HarmfulQA  and 140 safe instructions generated by utilizing GPT-4. Testing datasets are the subset version of Advbench  and StrongREJECT , which do not overlap with the training data, and each contains 50 malicious instructions covering diverse malicious scenarios.

**Victim LLMs.** We consider three well-aligned LLMs as our attacking targets: two white-box models LLaMA-2-7B/13B-Chat  and one black-box API GPT-4-turbo-2024-04-09.

**Evaluation Criteria.** We use two categories of criteria to evaluate the effectiveness of attack methods.

* **ASR-keyword**, which is a commonly used criterion for computing attack success rate (ASR) via simple keyword matching. If any of the predefined refusal keywords (e.g., _I cannot_) appears in the output, the attack will be judged as failed, otherwise it will be judged as successful (see Appendix B for the keyword list). ASR-keyword may not always accurately reflect whether an attack is successful. For example, if an LLM generates garbled responses with no useful information due to a large perturbation, ASR will still consider the attack successful, since no refusal keywords are present.
* **LLM-based Criteria**, including three proposed criteria: (1) **ASR-answer** that evaluates whether LLMs give relevant answers to malicious instructions, (2) **ASR-useful** that decides whether the responses are useful, and (3) **Language flaws** that determines whether the responses contain language flaws such as repetition, inconsistency, or unspecific paragraphs. In general, ASR-useful is a more strict criterion than ASR-answer in harmfulness evaluation. These newly designed criteria leverage human judges or GPT-4 to better evaluate response quality.

The complete definition of each criterion, prompt used for GPT-4, and user study information can be found in Appendix B and H. The implementation details of our method, baselines and comparative experiments are given in Appendix E.

### Embedding-Level Attack Results

**Overall performance**. Table 1 compares our proposed SCAV with embedding-level attack baselines JRE and RepE in terms of automatically evaluated criteria, and Table 2 shows the human evaluation results. The results show that our method consistently performs the best on both datasets and LLMs, decreasing language flaws by 16% to 24%, and successfully induces the well-aligned Llama models to answer over 90% malicious instructions with useful information. We have included **example cases** of LLM responses in Appendix I to further illustrate the effectiveness of our method.

We also observe that the GPT-4 rating is consistent with human evaluation results (Agreement = 86.52%, Precision = 78.23%, Recall = 83.49%, F1 = 80.78%). Thus, we utilize GPT-4 for computing ASR-answer, ASR-usefulness, and Language flaws in the subsequent experiments.

**Impact of training data size**. In this experiment, we mainly study how much training data is required for embedding-level attacks to achieve consistently high ASR-keyword. For each training data size, we randomly sample 5 subsets of data and report the average results. As shown in Figure 3, our method only requires 5 pairs of malicious and safe instructions to achieve an average ASR-keyword that is close to 100%. Besides, the variance of our method is much smaller, indicating its stability. In comparison, the ASR-keyword of RepE is 0 when the training dataset size is 1, and both baselines perform much worse than ours at varying training data sizes due to their potentially misleading heuristics.

    &  &  \\   & & ASR-keyword \(\) & ASR-answer \(\) & ASR-useful \(\) & Language flaws \(\) \\   LLaMA-2 \\ (7B-Chat) \\  } & JRE & 80 / 90 & 76 / 72 & 68 / 70 & 70 / 70 \\  & RepE & 70 / 94 & 90 / **98** & 86 / 92 & 44 / 24 \\   & Soft prompt & 56 / 64 & 50 / 44 & 40 / 38 & 62 / 66 \\   & SCAV & **100 / 100** & **96 / 98** & **92 / 96** & **2 / 10** \\    & \(\) & +20 / +4 & +6 / 0 & +6 / +4 & -42 / -14 \\   LLaMA-2 \\ (13B-Chat) \\  } & JRE & 84 / 94 & 68 / 78 & 68 / 70 & 36 / 44 \\  & RepE & 86 / 92 & 88 / 98 & 84 / 94 & 20 / 18 \\   & Soft prompt & 80 / 74 & 66 / 28 & 50 / 28 & 44 / 68 \\   & SCAV & **100 / 100** & **98 / 100** & **96 / 98** & **0 / 2** \\    & \(\) & +14 / +6 & +10 / +2 & +12 / +4 & -20 / -16 \\   

Table 1: Automatic evaluation of embedding-level attack performance. All criteria except for ASR-keyword are evaluated by GPT-4. The best results are in **bold** and the second best are underlined. \(\) = SCAV \(-\) Best baseline.

    &  &  \\   & & ASR-answer \(\) & ASR-useful \(\) & Language flaws \(\) \\   LLaMA-2 \\ (7B-Chat) \\  } & JRE & 66 / 62 & 60 / 42 & 64 / 68 \\  & RepE & 88 / 94 & 82 / 82 & 36 / 26 \\   & SCAV & **100 / 96** & **92 / 90** & **12 / 8** \\    & \(\) & +12 / +2 & +10 / +8 & -24 / -18 \\   

Table 2: Human evaluation of embedding-level attack performance. \(\) = SCAV \(-\) Best baseline.

Figure 3: ASR-keyword vs. training data size on Advbench, LLaMA-2-7B-Chat. Shaded backgrounds denote variations.

**Ablation study and sensitivity analysis**. We conduct additional experiments to validate the effectiveness of important components and stability of our method. The detailed results are in Appendix F. We summarize the major conclusions as follows:

* We demonstrate the effectiveness of our automatic hyperparameter selection by showing that it increases ASR-useful by 2%\(\)10% and reduces language flaws by up to 20%, compared to manually selecting better hyperparameters by humans (e.g., perturbing 9\(\)13 layers with unified \(=-1.5\)).
* We illustrate the effectiveness of our perturbation direction by showing that our method consistently achieves better ASR-keyword compared with the baselines under varying perturbation magnitude and layers.

### Prompt-Level Attack Results

**Overall performance**. Table 3 shows our prompt-level attack method consistently performs the best, compared to baselines that manually design or learn attack prompts, improving ASR-related criteria by 12% to 42% and reducing language flaws by at most 18%. This demonstrates the effectiveness of our optimization goal that simultaneously improves attack success rates and maintains LLM performance.

**Transferability to GPT-4**. Table 4 shows the results of applying prompts learned from LLaMA models to GPT-4. Our method usually performs better, improving ASR-related criteria by at most 48%, and reducing language flaws by at most 26%. This demonstrates our attack prompts learned by studying the inner workings of certain white-box models may still be useful for other black-box APIs. The potential transferability of attack prompts is also observed by previous research .

    &  &  \\   & & ASR-keyword \(\) & ASR-answer \(\) & ASR-useful \(\) & Language flaws \(\) \\   LLaMA-2 \\ (7B-Chat) \\  } & AutoDAN & 36 / **32** & 28 / **22** & 26 / 18 & **68 / 82** \\  & GCG & 4 / 8 & 4 / 16 & 2 / 16 & 92 / 90 \\   & SCAV & **70 / 30** & **66 / 20** & **52 / 20** & **68 / 72** \\   & \(\) & +34 / -2 & +38 / -2 & +26 / +2 & 0 / -10 \\   LLaMA-2 \\ (13B-Chat) \\  } & AutoDAN & 34 / 12 & 20 / 18 & 24 / 16 & 80 / 84 \\  & GCG & 2 / 8 & 0 / 12 & 0 / 10 & 98 / 88 \\   & SCAV & **82 / 40** & **48 / 26** & **60 / 22** & **54 / 72** \\    & \(\) & +48 / +28 & +28 / +8 & +36 / +6 & -26 / -12 \\   

Table 4: Attack transferability study: applying attack prompts learned for LLaMA to GPT-4. \(\) = SCAV \(-\) Best baseline.

    &  &  \\   & & ASR-keyword \(\) & ASR-answer \(\) & ASR-useful \(\) & Language flaws \(\) \\   LLaMA-2 \\ (7B-Chat) \\  } & DeepInception & 42 / 46 & 28 / 22 & 10 / 8 & 60 / 76 \\  & AutoDAN & 24 / 30 & 22 / 26 & 14 / 10 & 60 / 62 \\  & GCG & 28 / 26 & 32 / 26 & 10 / 16 & 76 / 72 \\  & SCAV & **54 / 60** & **60 / 46** & **44 / 40** & **52 / 44** \\   & \(\) & +12 / +14 & +28 / +20 & +30 / +24 & -8 / -18 \\   LLaMA-2 \\ (13B-Chat) \\  } & DeepInception & 16 / 18 & 8 / 16 & 4 / 12 & **58 / 54** \\  & AutoDAN & 30 / 18 & 18 / 20 & 14 / 16 & **58 / 56** \\   & GCG & 40 / 34 & 24 / 18 & 10 / 16 & **58 / 80** \\   & SCAV & **72 / 54** & **46 / 48** & **28 / 46** & **58 / 42** \\    & \(\) & +32 / +20 & +22 / +28 & +14 / +30 & 0 / -12 \\   

Table 3: Evaluation of prompt-level attack performance. \(\) = SCAV \(-\) Best baseline.

Understanding Safety Risks and Mechanisms of LLMs

The goal of this section is to provide insights into the severity of LLM safety risks and to better understand the safety mechanisms of LLMs by applying our method.

### Are Aligned LLMs Really Safe?

**White-box LLMs.** Table 5 shows the results when using SCAV to attack 7 well-known open-source LLMs . We can see that all LLMs provide relevant answers to more than 85% malicious instructions (ASR-answer), except for one on Advbench, which answers 78% malicious instructions. The response quality is also high, with an average ASR-useful of 87% and on average 12% language flaws. Moreover, ASR-keyword is close to 100% in most cases. This is very dangerous because 1) the performance of recently released open-source LLMs is gradually improving, and 2) almost no cost is required to obtain a response to any malicious instruction, as we do not require LLMs to be fine-tuned or large training data. This warns us that _the existing alignment of the open-source LLMs can be extensively reversed, and there is an urgent need to develop effective methods to defend against current attack methods or stop open-sourcing high-performance LLMs_.

**Black-box LLM APIs.** Table 6 shows the results when attacking GPT-4 by using different combinations of methods. SCAV-LLaMA-13B reports the result of SCAV when LLaMA-2-13B-Chat is used for generating attack prompts, and SCAV-Both denotes the attack success rates and response quality when combining the attack prompts generated for both versions of LLaMA, apply one of them, and record the best result. The method All combines attack prompts from all attack methods, including SCAV, AutoDAN, and DeepInception, apply one of the attack prompts, and record the best results.

We can see from Table 6 that even the cutting-edge GPT-4 returns useful responses to 84% malicious instructions on Advbench and gives useful responses to 54% malicious instructions on StrongREJECT. This shows that even the alignment of black-box LLM APIs may be significantly reversed by using existing attack methods, urging the development of effective defense methods.

    &  \\   & ASR-keyword \(\) & ASR-answer \(\) & ASR-useful \(\) & Language flaws \(\) \\  SCAV-LLaMA-13B & 82 / 40 & 66 / 26 & 60 / 22 & 54 / 72 \\ SCAV-Both & 96 / 52 & 78 / 30 & 80 / 36 & 42 / 58 \\ All & 96 / 86 & 84 / 54 & 84 / 54 & 28 / 44 \\   

Table 6: Attacking GPT-4 API by using different combinations of attack methods. When combining all prompt-level attack methods (All), GPT-4 returns useful responses to 84% (or 54%) malicious instructions on Advbench (or StrongREJECT), with a majority of them having no language flaws.

    &  \\   & ASR-keyword \(\) & ASR-answer \(\) & ASR-useful \(\) & Language flaws \(\) \\  LLaMA-2-7B-Chat & 100 / 98 & 96 / 98 & 92 / 96 & 2 / 10 \\ LLaMA-2-13B-Chat & 100 / 100 & 98 / 100 & 96 / 98 & 0 / 2 \\ LLaMA-3-8B-Instruct & 100 / 100 & 90 / 94 & 82 / 92 & 14 / 8 \\ Mistral-7B & 100 / 94 & 90 / 96 & 84 / 92 & 20 / 20 \\ Qwen-1.5-7B-Chat & 100 / 100 & 78 / 86 & 66 / 78 & 26 / 20 \\ Vicuna-v1.5-7B & 98 / 98 & 94 / 86 & 80 / 84 & 12 / 22 \\ WizardLM-2 & 100 / 100 & 96 / 90 & 90 / 88 & 8 / 10 \\  Average & 99.71 / 98.57 & 91.71 / 92.86 & 84.29 / 89.71 & 11.71 / 13.14 \\   

Table 5: Attacking 7 well-known open-source LLMs by using SCAV. All LLMs provide relevant answers to more than 85% malicious instructions (ASR-answer), except for one on Advbench (ASR-answer is 78%).

### Are Existing _Unlearn_ Methods Really Effective?

We then study whether the existing defense methods that help LLMs unlearn harmful knowledge are effective. This is achieved by applying existing attack methods on a version of LLaMA-2-7B-Chat that has been fine-tuned to unlearn harmful knowledge by using an existing unlearn method Eraser . Table 7 shows that SCAV can still induce the LLM to produce many harmful responses, indicating that _the unlearn method may not have fully erased harmful knowledge from the LLM, although it appears to be effective without the attack._ Furthermore, we find that existing defense methods might not effectively mitigate the proposed embedding-level attacks (see Appendix G).

### How Do Aligned LLMs Differentiate Malicious Instructions from Others?

In this section, we further investigate the safety mechanisms of LLMs. Our insights are as follows.

First, **there may be a close relation between linear separability and the safety mechanisms of LLMs.** Our previous experiments have shown that 1) aligned LLMs can linearly separate embeddings from malicious and safe instructions at later layers (Figure 1), and that 2) attacks guided by the linear classifier are of high success ratio, indicating that the safety mechanisms of LLMs may be well modeled by linear separability. To better understand their relation, we further attack LLaMA-2-7B-Chat on the 0th, 10th, 20th, and 30th layers. As shown in Figure 3(a), attacks on a linearly separable layer (10, 20, 30) consistently lead to an increase in ASR-keyword, while attacks on the other layer (0) do not improve ASR-keyword. Based on the results, we speculate that for every single layer, linear separability may not only indicate that LLMs understand the safety concept, but may also mean that the LLMs will use this safety concept in subsequent layers for generating responses.

Second, **different layers may have modeled the safety mechanisms from related but different perspectives.** Figure 3(b) shows the value of \(P_{}\) when attacking different layers of LLaMA-2-7B-Chat. We have two observations. First, while attacking a single layer (Layer 10) results in a low \(P_{}\) at the current layer, \(P_{}\) subsequently increases on the following layers. This means that later layers somehow gradually correct the attack by leveraging existing information of the embedding, potentially because it models the safety mechanisms from a different perspective. Second, we observe that when more layers are perturbed (e.g., layers 10-13), \(P_{}\) at later layers can no longer be corrected by the LLM. This indicates that a limited number of layers may jointly determine the overall safety mechanisms from different perspectives.

    &  &  &  \\   & & ASR-keyword (\%) & Harmfulness & ASR-keyword (\%) & Harmfulness \\   Eraser \\ (LLaMA-2-7B-Chat) \\  } & AIM & 0.5 & 1.03 & 0.04 & 1.13 \\  & GCG & 8.26 & 1.33 & 1.67 & 1.06 \\   & AutoDAN & 2.88 & 1.09 & 5.99 & 1.18 \\   & SCAV & **97.34** & **4.72** & **98.79** & **4.86** \\   

Table 7: After unlearning harmful knowledge by using Eraser , SCAV can still induce the LLM to produce many harmful responses, indicating that the unlearn method may not have fully erased harmful knowledge from the LLM, even though it appears to be effective without our attack. Harmfulness  is a quality criterion with a maximum score of 5.

Figure 4: Unveiling the safety mechanisms of LLMs by (a) attacking a single layer; (b) attacking multiple layers, and (c) transferring embedding-level attacks to other white-box LLMs.

Finally, **different white-box LLMs may share some commonalities in their safety mechanisms**. Figure 3(c) showcases ASR-keyword when applying embedding-level attacks from one white-box model to another. We can see that the ASR-keyword is sometimes quite large. This indicates that the safety mechanisms of LLMs may have certain commonalities and that SCAV may have characterized this commonality in some sense. However, there is still a lack of clear understanding of when it can transfer and why.

## 5 Conclusion

In this paper, we propose SCAV, which can attack both at the embedding-level and prompt-level. We provide novel insights into the safety mechanisms of LLMs and emphasize that the safety risks of LLMs are very serious. More effective methods are urgently needed to protect LLMs from attacks.

**Limitation.** Although our method performs well at both embedding and prompt levels, we lack an in-depth exploration of the transferability mechanisms of perturbation vectors and attack prompts. We believe this is a potential future direction toward the construction of responsible AI.

**Ethical Statement.** As with previous work, we believe that the proposed method will not have significant negative impacts in the short term. We must emphasize that our original intention was to point out safety vulnerabilities in LLMs. Our next steps will be studying how to address such risks.