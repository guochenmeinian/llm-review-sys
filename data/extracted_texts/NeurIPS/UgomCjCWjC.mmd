# Multi-Agent First Order Constrained Optimization in Policy Space

Youpeng Zhao\({}^{1}\), Yaodong Yang\({}^{2}\), Zhenbo Lu\({}^{3}\), Wengang Zhou\({}^{1,3}\), Houqiang Li\({}^{1,3}\)

\({}^{1}\)University of Science and Technology of China, \({}^{2}\)Institute for AI, Peking University

\({}^{3}\)Institute of Artificial Intelligence, Hefei Comprehensive National Science Center

zyp123@mail.ustc.edu.cn,yaodong.yang@pku.edu.cn

luzhenbo@iai.ustc.edu.cn, {zhwg,lihq}@ustc.edu.cn

Corresponding authors: Yaodong Yang and Zhenbo Lu

37th Conference on Neural Information Processing Systems (NeurIPS 2023).

###### Abstract

In the realm of multi-agent reinforcement learning (MARL), achieving high performance is crucial for a successful multi-agent system. Meanwhile, the ability to avoid unsafe actions is becoming an urgent and imperative problem to solve for real-life applications. Whereas, it is still challenging to develop a safety-aware method for multi-agent systems in MARL. In this work, we introduce a novel approach called Multi-Agent First Order Constrained Optimization in Policy Space (MAFOCOPS), which effectively addresses the dual objectives of attaining satisfactory performance and enforcing safety constraints. Using data generated from the current policy, MAFOCOPS first finds the optimal update policy by solving a constrained optimization problem in the nonparameterized policy space. Then, the update policy is projected back into the parametric policy space to achieve a feasible policy. Notably, our method is first-order in nature, ensuring the ease of implementation, and exhibits an approximate upper bound on the worst-case constraint violation. Empirical results show that our approach achieves remarkable performance while satisfying safe constraints on several safe MARL benchmarks.

## 1 Introduction

Cooperative multi-agent systems play a significant role in various domains, where a group of agents coordinate with each other to accomplish tasks and collaboratively optimize cumulative rewards for the team. Such a setting is frequently employed in many real-life scenarios such as robotics , autonomous vehicles , traffic light control  and the smart grid .Thanks to the recent remarkable advance of reinforcement learning techniques in various complex tasks , multi-agent reinforcement learning (MARL) has attracted substantial attention and quite a few algorithms have been proposed, including value-based methods  and policy gradient methods . Despite the notable achievement in academia, most MARL algorithms prioritize policy optimization solely for reward maximization, while disregarding potential negative or harmful consequences resulting from the agents' behaviors. Consequently, these methods can not be directly deployed in practice. In reality, many applications often require the agents to refrain from taking certain actions or visiting particular states . For instance, an unmanned car must adhere to traffic regulations by not crossing a red light, even while pursuing its destination, in order to prioritize safety.

To address the above issues, researchers have been devoted to developing algorithms that learn policies which adhere to safety constraints and great progress has been made in single-agent RL setting. However, it is still a daunting challenge to develop safe policies for multi-agent systems. Because of the existence of multiple agents, the environment may suffer from non-stationarity due to simultaneously learning agents, posing a non-negligible challenge to the training process. Moreover, ensuring safety in MARL is highly intricate. To provide a better depiction of the inherent challenges,we show an illustrative example in Figure 1. This figure showcases the policy space for each agent, wherein the reward distribution is presented. The green rectangles delineate the individual safe areas for each agent and the blue ones depict the joint safe area. The green arrows symbolize the update direction for each agent, whereas the dashed ones signify the intended update direction if each agent whereas the dashed ones signify the intended update direction if each agent solely considers its own objectives. When agents are randomly initialized, they naturally tend to update the policy along the dashed line arrows to maximize the reward. However, due to the presence of safety constraints, they must adjust their updates along the solid line arrows to ensure their policies fall within the safe area. This is non-trivial as it may conflict with their goal of reward maximization. Furthermore, after individually addressing their safety constraints, the agents must also consider the safety constraints of others to collectively converge to the joint safe area, adding more complexity to the optimization process for the entire system. The interplay of these factors underscores the inherent difficulties in solving the problem of safe MARL.

Recently, as a notable advancement in the field of safe MARL, Multi-Agent Constrained Policy Optimization (MACPO)  has been proposed as a safe and effective solution. MACPO attains the properties of both monotonic improvement guarantee and safety constraints satisfaction guarantee at every iteration during training. However, this algorithm involves solving an optimisation problem using Taylor approximations and inverting a high-dimensional Fisher information matrix. As a result, the computation is very complex and the policy update often becomes infeasible. To alleviate this issue, it requires additional recovery steps, which, however, sometimes causes updates to be backtracked and samples to be wasted.

Taking inspiration from the sequential policy update scheme introduced in HATPO , we have devised a simpler approach to incorporate safety constraints in solving safe MARL problems compared with MACPO. The resulting algorithm, Multi-Agent First Order Constrained Optimization in Policy Space (MAFOCOPS), aims to address the following question of how to achieve the best constraint-satisfying policy update given the current policy for each agent. Our method follows a two-step process to give a solution to this problem. First, based on the theoretical foundations presented in MACPO, we demonstrate that the best policy update has a near-closed form solution when attempting to solve the optimal policy in the nonparametric policy space. Subsequently, we project the policy back into the parametric policy space as direct evaluation of the optimal policy is usually not feasible. This can be realized by sampling from the current policy and evaluating a loss function between the parameterized policy and the optimal policy obtained in the nonparametric policy space. Notably, our algorithm only employs first-order approximations, making it straightforward to implement, and has an approximate upper bound for worst-case constraint violation. To validate the effectiveness of our approach, we conduct experiments on two safe MARL benchmarks proposed by , namely Safe MAMuJoCo and Safe MAIG. The experimental results demonstrate the superior performance of MAFOOPS compared to MACPO, despite being a simpler algorithm.

Figure 1: An example to illustrate the challenges in safe MARL. Reward distribution is presented in the policy space for each agent. Green rectangles delineate the individual safe areas for each agent and the blue ones depict the joint safe area. Green arrows symbolize the update direction for each agent whereas the dashed ones signify the intended update direction if each agent solely considers its own objectives and the crosses represent that the dashed update direction is unsuitable in safe MARL.

Related Work

Safety has become a crucial and longstanding concern in the field of reinforcement learning . In this section, we discuss recent advancements in the domain of safe RL for multi-agent environments.

The realm of safe multi-agent reinforcement learning is a nascent area of research that has gained increasing importance . Several attempts have been made to tackle safe MARL, but most existing approaches have limitations. For example, CMIX  leverages the value function decomposition method and modifies the reward function to account for constraint violations, yet this algorithm fails to provide safety guarantees during training. Another approach is Safe Dec-PG  which proposes a decentralized policy gradient descent-ascent method by means of a consensus network and employs a primal-dual framework to balance reward maximization and cost minimization. Nevertheless, the consensus requirement in this work equivalently imposes an extra constraint of parameter sharing among neighbouring agents, potentially leading to sub-optimal policy . It is noteworthy that most current safe MARL methods are tailored to robotics tasks, utilizing techniques such as barrier certificates [29; 30] or model predictive shielding  to address safety issues. These methods, however, are specifically designed for robotics applications. Besides, they often require supervised learning based approaches or specific assumptions concerning the state space and environment dynamics.

As a recent remarkable solution, MACPO  incorporates a sequential policy update scheme. This algorithm develops the multi-agent trust region learning based on CPO  and provides theoretical guarantees of both monotonic improvement in reward and compliance with cost constraints in a multi-agent setting. To achieve practical solutions to the safe MARL problems, the policy for each agent needs to be parameterized with a neural network, which effectively represents the policy in policy space. However, achieving parameterized policies in MACPO involves solving optimization problems using first and second-order Taylor approximation, which includes taking the inverse of Fisher information matrix, and the computation is implemented by the conjugate gradient method . These operations can introduce nonnegligible approximation errors, thereby compelling MACPO to undertake additional steps during each update in the training process in order to recover from constraint violations. In contrast, our algorithm takes a different approach by solving the optimization problem within the nonparametric space and then projecting the results back into the parameter space. By leveraging a simple first-order method to eliminate the approximation error, our algorithm ultimately outperforms MACPO.

## 3 Problem Formulation

A safe MARL problem can be formulated as a Constrained Markov Decision process, which is described by a tuple denoted as \(<,,},p,^{0},,R, ,c>\). Here, \(=\{1,2,,n\}\) denotes the set of agents involved in the system, \(\) is the state space, \(}=_{i=1}^{n}^{i}\) represents the product of the action spaces of agents, _i.e.,_ joint action space, \(p:}\) is the probabilistic transition function, \(^{0}\) is the initial state distribution and \([0,1)\) is the discounted factor. The team reward function \(R:}\) maps state and joint actions to a scalar reward while \(=\{C_{j}^{i}\}_{1 j m^{i}}^{i}\) is the set of sets of cost functions denoted in the form \(C_{j}^{i}:^{i}\) (every agent \(i\) has \(m^{i}\) cost functions). The set of corresponding cost-constraining values is given by \(c=\{c_{j}^{i}\}_{1 j m^{i}}^{i}\). At time step \(t\), the multi-agent system is situated in state \(s_{t}\) and every agent \(i\) selects an action \(a_{t}^{i}\) based on its policy \(^{i}(a^{i}|s)\), forming a joint action \(}=(a_{t}^{1},,a_{t}^{n})\) and joint policy \((|s)=_{i=1}^{n}^{i}(a^{i}|s)\). This leads the environment to transit to a new state \(s_{t+1} p(|s_{t},})\) according to the probabilistic transition function and the system receives the reward \(R(s_{t},a_{t})\) while each agent \(i\) incurs individual costs \(C_{j}^{i}, j=1,,m^{i}\). This study focuses on a fully-cooperative multi-agent setting where all agents share the same reward function, aimed at maximizing the expected total reward:

\[J() E_{s_{0}^{0},} ,s_{1:} p}[_{t=0}^{}^{t}R(s_{t}, _{t})].\]

Moreover, we impose that each agent \(i\) satisfies its safety constraint, which is defined as

\[J_{j}^{i}() E_{s_{0}^{0},},s_{1:} p}[_{t=0}^{}^{t}C_{j} ^{i}(s_{t},a_{t}^{i})] c_{j}^{i}, j=1,,m^{i}.\]We define the state-action value and the state-value function in terms of reward as

\[Q_{}(s,) E_{s_{1:} p,_{1:}}[_{t=0}^{}^{t}R(s_{t},_{t})|s_{0}=s,_{0}= ],V_{}(s) E_{}[Q_{}(s,)].\]

It's worth noting even though the action \(a_{t}^{i}\) taken by agent \(i\) does not directly impact the costs \(\{C_{j}^{k}(s_{t},a_{t}^{k})\}_{j=1}^{m_{k}}\) of other agents \(k i\) from the above formulation, the total costs can still be influenced by this action implicitly due to its influence on the subsequent state \(s_{t+1}\). This formulation captures the realistic multi-agent interactions in real world. For instance, when a car runs a red light, although other cars may not be immediately endangered by this action, the resulting disruption in traffic flow may lead to potential hazards later on. To illustrate the \(j_{th}\) cost function of agent \(i\), we express the corresponding state-action cost value function and the state cost value function as below:

\[Q_{j,}^{i}(s,) E_{^{-i}^{-1},s_{1:} p,_{1:}}[_{t=0}^{ }^{t}C_{j}^{i}(s_{t},a_{t}^{i})|s_{0}=s,a_{0}^{i}=a^{i}],\] \[V_{j,}^{i}(s) E_{,}[_{t=0}^{ }^{t}C_{j}^{i}(s_{t},a_{t}^{i})|s_{0}=s].\]

Notably, although similar in form to traditional \(Q_{}\) and \(V_{}\), the cost value function \(Q_{j,}\) and \(V_{j,}\) involve additional indices \(i\) and \(j\), where the subscript \(i\) refers to an agent and \(j\) denotes the \(j^{th}\) cost.

Motivated by the sequential policy update scheme, we pay close attention to determining the contribution of different subsets of agents to overall performance in this study. We denote an arbitrary subset \(\{i_{1},,i_{h}\}\) of agents as \(i_{1:h}\) while \(-i_{1:h}\) refers to its complement. Given the agent subset \(i_{1:h}\), we define the multi-agent state-action value function:

\[Q_{}^{i_{1:h}}(s,^{i_{1:h}}) E_{^{-i_{1:h}} ^{-i_{1:h}}}[Q_{}(s,^{i_{1:h}},^{-i_{1:h}})].\]

Furthermore, for disjoint sets \(i_{1:h}\) and \(j_{1:k}\), the multi-agent advantage function is defined as follows:

\[A_{}^{i_{1:h}}(s,^{j_{1:h}},^{i_{1:h}}) Q_{}^{j_{1:k},i_{1:h}}(s,^{j_{1:k}},^{i_{1:h}})-Q_{}^{j_ {1:k}}(s,^{j_{1:k}}).\]

An interesting and critical observation concerning the aforementioned multi-agent advantage function is that the advantage \(A_{}^{i_{1:h}}\) can be written as the sum of sequentially-unfold multi-agent advantages of individual agents, that is,

**Lemma 1** (Multi-agent advantage decomposition ): For any state \(s\), subsets of agents \(i_{1:h}\) and joint action \(^{i_{1:h}}\), the following identity holds

\[A_{}^{i_{1:h}}(s,^{i_{1:h}})_{j=1}^{h}A_{ }^{j}(s,^{i_{1:h}-1},a^{i_{j}}).\]

## 4 Method

Expanding on the above foundational concepts and the derivatives of multi-agent trust region learning with constraints, MACPO provided an important insight. It highlighted that when the policy changes for all agents are sufficiently small, each agent can learn a better policy \(^{i}\) by only considering its own surrogate return and costs, which is consistent with the sequential policy update scheme. In our work, building upon the formulas in MACPO, we can deduce that, for agent \(i_{h}\) and the index of its cost functions \(j\), given the joint policy \(_{_{k}}\) at the \(k\)th iteration and updated policies of the previous agent sets \(i_{1:h-1}\), namely \(_{_{k+1}}^{i_{1:h-1}}\), the new policy is obtained by solving the following optimization problem:

\[}^{i_{h}}}{maximize}\,E_{s_{_{_{ k}}},a^{i_{1:h-1}}_{_{k+1}}^{i_{1:h-1}},a^{i_{h}}_{i_{h}}}[A_{ _{_{k}}}^{i_{h}}(s,a^{i_{1:h-1}},a^{i_{h}})],\] (1) \[s.t.J_{j}^{i_{h}}(_{_{k}})+E_{s_{ _{_{k}}},a^{i_{h}}^{i_{h}}}[A_{j,_{_{k}}}^{i_{h}}(s,a ^{i_{h}})] c_{j}^{i_{h}}, j 1,,m^{i_{h}},\] (2) \[_{KL}(_{}^{i_{h}},_{_{k}}^{i_{h}}).\] (3)

where \(_{KL}(_{}^{i_{h}},_{_{k}}^{i_{h}}) E_{s _{_{_{k}}}}[D_{KL}(_{_{k}}^{i_{h}}(|s),_{ }^{i_{h}}(|s))]\). A simple proof is presented in Appendix A. When solving the optimization problem (1-3), we use a two-step approach:1. For agent \(i_{h}\), when provided with the joint policy \(_{_{k}}\) and updated policy \(_{_{k+1}}^{i_{1:h-1}}\), find an _optimal update policy_\(^{i_{h}*}\) in the nonparameterized policy space, denoted by \(\).
2. Project the policy found in previous step back into parameterized policy space \(_{}\) by solving for the closest policy \(_{}_{}\) to obtain \(_{_{k+1}}^{i_{h}}\). Here we consider the set of parameterized policies \(_{}=\{_{}:\}\) which allows for evaluation and sampling.

### Finding the Optimal Update Policy

In the first step, we aim to find the optimal nonparameterized policy and propose the following solutions (see Appendix B for proof):

**Theorem 1**.: For agent \(i_{h}\), we define \(b_{j}^{i_{h}}=c_{j}^{i_{h}}-J_{j}^{i_{h}}(}})\). If \(}}\) is a feasible policy, the optimal policy takes the form

\[^{i_{h}*}(a|s)=}^{i_{h}}(a|s)}{Z_{_{j},_{j} }(s)}exp\{}(_{}}}(s,a^{i_{h}})- _{j}A_{j,}}}^{i_{h}}(s,a^{i_{h}}))\},\] (4)

where \(_{}}}(s,a^{i_{h}})=E_{a^{i_{1:h-1}}_{_{k+1 }}^{i_{1:h-1}}}[A_{_{_{k}}}^{i_{h}}(s,a^{i_{1:h-1}},a^{i_{h}})]\),\(Z_{_{j},_{j}}(s)\) is the partition function that ensures Equation 4 to be a valid probability distribution, and \(_{j}\) as well as \(_{j}\) are solutions to the following optimization problem:

\[,_{j} 0}{}_{j}+_{j}b_{j}^{i_{h} }+_{j}E_{s_{}}},a^{i_{h}}^{i_{h}*}}[ logZ_{_{j},_{j}}(s)].\] (5)

The structure of the optimal policy exhibits an intuitive nature, as it assigns a substantial probability mass to regions of the state-action space with high returns. This allocation is counterbalanced by a penalty term multiplied by the cost advantage. Another desirable property of the optimal update policy is that for feasible policy \(}}\), it has an upper bound for worst-case guarantee for cost constraint satisfaction according to Lemma.2 in MACPO .

### Approximating the Optimal Update Policy

When addressing the optimization problem 1-3 in the first step, the optimal update policy for agent \(i_{h}\) may not necessarily reside within parameterized policy space \(_{}\). Consequently, evaluating or sampling from this policy becomes unfeasible. To this end, in the second step, we need to project the optimal update policy back into the parameterized policy space by minimizing the loss function:

\[L()=E_{s_{}}}}[D_{KL}(_{}^{i_{h}}|| ^{i_{h}*})(s)].\] (6)

In this context, \(_{}^{i_{h}}_{}\) represents some projected policy that serves as an approximation of the optimal update policy. To minimize this loss function, first-order methods can be employed. In doing so, we can leverage the following result as a useful tool in our optimization efforts:

**Corollary 1**.: The gradient of \(L()\) takes the form

\[_{}L()=E_{s_{}}}}[_{} D_{KL}(_{}^{i_{h}}||^{i_{h}*})[s]],\] (7)

where

\[_{}D_{KL}(_{}^{i_{h}}||^{i_{h}*})[s]=_{}D_ {KL}(_{}^{i_{h}}||_{_{k}}^{i_{h}})-}E_{a _{_{k}}^{i_{h}}}[_{}^{i_{h}}(a|s)}{ _{_{k}}^{i_{h}}(a|s)}(_{}}}(s,a^{i_{h}})-_ {j}A_{j,}}}^{i_{h}}(s,a^{i_{h}}))].\] (8)

The proof is shown in Appendix C.

It's to be noted that 27 can be estimated by sampling from the trajectories generated by policy \(}}\), which allows us to train our policy using stochastic gradients, a key aspect of our methodology. This corollary serves as a guiding framework for our algorithm. At every iteration, we begin with a policy \(}}\), utilizing it to generate trajectories and collect relevant data. Subsequently, we employ this data in conjunction with 5 to estimate \(_{j}\) and \(_{j}\). We then draw a minibatch from the collected data to estimate \(_{}L()\) as outlined in Corollary 1. After taking a gradient step using Equation 27, we draw another minibatch and repeat the process.

### Practical Implementation

For agent \(i_{h}\), solving the dual problems presented in 5 is computationally challenging when dealing with large state/action spaces as calculating the partition function \(Z_{_{j},_{j}}(s)\) often involves evaluating a high-dimensional integral or sum. Moreover, \(_{j}\) and \(_{j}\) are dependent on the iteration \(k\) and need to be adjusted at every iteration to ensure the effectiveness of optimization.

Based on the structure of the optimal update policy, it is observed that as \(_{j} 0\), \(^{i_{h}*}\) tends to be greedy while the policy becomes more exploratory when \(_{j}\) increases. We note that \(_{j}\) exhibits similarities to the temperature term utilized in maximum entropy reinforcement learning . Fixed values of \(\) have been demonstrated to yield reasonable outcomes in training [34; 35; 22]. In practical implementations, we have observed favorable results by using fixed \(_{j}\) through hyperparameter sweeps. However continuous adaptation of \(_{j}\) during training is necessary to ensure cost constraint satisfaction. In this regard, we appeal to an intuitive heuristic based on primal-dual gradient methods  to determine the appropriate value of \(_{j}\). Recall that by strong duality, the optimal \(_{j}^{*}\) and \(_{j}^{*}\) minimizes the dual function 5 which we will denote by \(L(^{i_{h}*},_{j},_{j})\). Then we can adopt gradient descent _w.r.t_\(_{j}\) to minimize \(L(^{i_{h}*},_{j},_{j})\) as follows:

**Corollary 2**.: The derivative of \(L(^{i_{h}*},_{j},_{j})\)_w.r.t_\(_{j}\) is

\[*},_{j},_{j})}{_{j}}=b_{j}^{i_ {h}}-E_{s_{}}}^{i_{h}*}(a_{i})}[A_{j,}}}^{i_{h}}(s,a^{i_{h}})].\] (9)

The proof is shown in Appendix D.

The last term in the gradient expression poses a challenge since \(^{i_{h}*}\) may not locate in parameterized policy space, leading direct evaluation of the term to be infeasible. Nevertheless, due to the proximity between \(_{}^{i_{h}}\) and \(^{i_{h}*}\) enforced by the KL divergence constraint, it's reasonable to assume that \(E_{s_{}}},a^{i_{h}}^{i_{h}*}}[A_{j,}}}^{i_{h}}(s,a^{i_{h}})] E_{s_{}} },a^{i_{h}}^{i_{h}}_{_{h}}}[A_{j,}}}^{i_{h}}(s,a^{i_{h}})]=0\). In practice, we have observed that setting this term to zero yields favorable outcomes, which gives the update term as follows:

\[_{j}}{proj}[_{j}-(c_{j}^{i_{h}}-J_{j}^ {i_{h}}(}}))],\] (10)

where \(\) is the step size to control the magnitude of the update. The projection operator \(proj_{_{j}}\) ensures that \(_{j}\) remains within the interval \([0,_{max}]\), with \(_{max}\) chosen to prevent \(_{j}\) from being excessively large. In practical applications, the estimation of \(J_{j}^{i_{h}}(}})\) can be accomplished using Monte Carlo methods by leveraging trajectories collected from \(}}\). This approach aligns with the update rule employed in MACPO . We recall that in 4, \(_{j}\) acts as a cost penalty term where increasing \(_{j}\) makes the likelihood of state-action pairs with higher costs being sampled by \(^{i_{h}*}\) diminish. Consequently, the update rule presented in 10 exhibits an intuitive characteristic: it raises \(_{j}\) if \(J_{j}^{i_{h}}(}})>c_{j}^{i_{h}}\), indicating a violation of the cost constraint for \(}}\), and reduces \(_{j}\) otherwise. Using the proposed update rule, \(_{j}\) can be updated before updating the policy parameter \(^{i_{h}}\).

To be noted, our algorithm is a first-order method, which implies that the approximations made are accurate only around the initial condition (_i.e.,_\(_{}^{i_{h}}=_{_{h}}^{i_{h}}\)). To better enforce this condition, we have introduced a per-state acceptance indicator function \(I(s_{j})=_{D_{KL}(_{}^{i_{h}},_{_{h}}^{i_{h}})}\) to 27. This function helps in rejecting sampled states whose \(D_{KL}(_{}^{i_{h}},_{_{h}}^{i_{h}})[s]\) is too large, thereby improving the accuracy of the gradient update. The resulting sample gradient update equation is as follows:

\[_{}L()=_{j=1}^{N}[_{}D_{KL}(_{ }^{i_{h}}\|_{_{h}}^{i_{h}})[s_{j}]-_{ }^{i_{h}}(a_{j}|s_{j})}{_{_{h}}^{i_{h}}(a_{j}|s_{j})}( _{}}}(s,a^{i_{h}})-_{j}_{j,}}}^{i_{h}}(s,a^{i_{h}}))]I(s_{j}),\] (11)

where \(N\) is the number of samples collected using policy \(}}\). The estimates of the advantages functions for the returns and costs, denoted as \(_{}}}\) and \(_{j,}}}^{i_{h}}\), are obtained from critic networks referring to MACPO. We estimate these advantages using Generalized Advantage Estimator (GAE)  and apply stochastic gradient descent using 11. During the training process, our algorithm employs the early stopping criterion to ensure that the updated policy satisfies the trust region constraint. Specifically, this criterion is defined as \(_{j=1}^{N}D_{KL}(_{}^{i_{h}}\|_{_{h}}^{i_{h}} )[s_{j}]>\). To update the value network, we minimize the Mean Square Error (MSE) between the output and target value. The procedure of our algorithm is presented in the Appendix E.

## 5 Experiments

We evaluate the effectiveness of our algorithm on two benchmarks of safe MARL: Safe MAMuJoCo and Safe Multi-Agent Isaac Gym (MAIG). The former is a safety-aware modification of MAMuJoCo , where there exist obstacles in the environment. Meanwhile, Safe MAIG is developed on top of Issac Gym , a GPU-based platform for robotics tasks. Being an extension of DexterourHands , Safe MAIG requires agents to control the robot hands while optimizing both the reward and safety performance. We present some example tasks in Figure 5 and more details about the environments are introduced in the appendix.

While our primary aim is to propose a simpler alternative to MACPO, we also evaluate the performance of MAPPO-Lagrangian (MAPPO-L), which is put forward alongside MACPO . This method adopts Lagrangian multipliers to solve optimization problems in MACPO. Without the requirements of repetitive computation of the Hessian matrix whose size grows quadratically with the dimension of the parameter vector, the Lagrangian method is also first-order and simple to implement. However, unlike MACPO and MAFOCOPS, whether this algorithm satisfies any worst-case constraint guarantees remains unknown. In addition, we compare the results of our algorithm with two standard MARL baseline algorithms, namely MAPPO and HAPPO . Both sets of experiments are carried out using the MACPO codebase and our experiments are conducted on GeForce RTX 3090 GPUS. More implementation details can be found in supplementary materials.

### Performance on Safe MAMuJoCo

In this section, we select several experiment scenarios of Safe MAMuJoCo and execute each algorithm for 10 million samples per task. The cost thresholds are determined by taking 50% of the cost achieved by standard MARL algorithms after 1 million sample runs. To be noted, while most training parameters remain the same as those in the codebase, such as the learning rate and settings of optimizers, we adjusted some certain hyperparameters associated with the cost thresholds to make the algorithms best suit the experiment scenarios. It is noteworthy that the performance of MAPPO-L highly relies on the Lagrangian coefficient, rendering it more sensitive to the hyperparameters. In light of this observation, we adopt distinct hyperparameters for MAPPO-L in different categories of tasks. In contrast, the other two safe MARL algorithms are implemented with uniform hyperparameter settings across all experimental scenarios in this benchmark, indicating their potential efficacy.

Due to page limit, we present partial results of our experiments in Figures 3 and more results can be seen in Appendix G. The experimental results show that all the safety-aware algorithms are able to satisfy the safety constraints while our proposed MAPCOPS manages to achieve the best overall performance across all tasks. Even when our method achieves similar performance to the other two algorithms in HalfCheetah scenarios, it still exhibits faster learning, demonstrating the advantages of our approach. For MAPPO-Lagrangian, we note that this baseline algorithm always achieves a similar performance as MAPPO, except in HalfCheetah scenarios where the cost threshold is significantly smaller compared to cost achieved by HAPPO and MAPPO. This may be due to that MAPPO-Lagrangian being built upon Lagrangian multiplier combined with standard MARL algorithms, leading to a performance more similar to safety-unaware MARL algorithms. In addition, from our discussion about MAPPO-L algorithm, we can know that it maintains a rather soft safety

Figure 2: Example tasks in Safe MAMuJoCo and Safe MAIG. (a): Safe 2x4-Ant, (b): Safe 2x3-HalfCheetah, (c): Safe 2x3-ManyAgent Ant, (d): ShadowHandOver. In each of these tasks, the body parts of the robots are controlled by different agents. Agents collaborate to manipulate the robot while ensuring that the safety constraints are not violated.

awareness, but the other two algorithms reaches safety via hard constraints. Therefore, MACPO and MAFOCOPS possess more promising properties, thus being the focus of our study.

Interestingly, our intuition suggests that a higher number of agents usually leads to increased complexity of the environment and hence worse performance. However, this phenomenon is not always observed. We assume that this is because, in the same type of map, the multi-agent system has to control the same robot, and the number of agents determines how the robot is partitioned. Fewer agents means that each agent has to control more parts of the robot, making the difficulty of the environment non-linearly dependent on the number of agents. Whereas, in scenarios of 6x1 ManyAgent Ant, we still note that as the number of agents increases, there exists a degradation in the performance of MACPO, which can attributed to the complexity of the computation worsening with the growth in the number of agents. As for MAFOCPS, our first-order method shows stronger resistance to task complexity, showing the advantages of our algorithm when coping with multi-agent problems. What's more, we provide videos of the trained policies of both our algorithm and MACPO in the supplementary materials to intuitively demonstrate the benefits that our approach brings.

### Performance on Safe MAIG

Apart from experiments in Safe MAMuJoCo, we also conduct experiments in the Safe MAIG benchmark. In this part of experiment, the cost thresholds are set as 25% of the cost obtained by standard MARL algorithms after running for one-tenth of the entire training process. The total training settings are the same as those in Safe MAMuJoCo, while some specific hyperparameter values are adjusted considering the great difference between these two experiment environments.

Experiment results in this benchmark are presented in Figure 4. Under this complicated environment,

Figure 3: Performance comparisons on tasks of Safe ManyAgent Ant, Ant, and HalfCheetah in terms of cost and reward. The safety constraint values for these presented tasks are set to be 25, 50 and 30, respectively.The solid line shows the median performance across 5 seeds and the shaded areas correspond to the 25-75% percentiles.

it can be observed that the soft constraint algorithm, MAPPO-L, exhibits a delay in guaranteeing safety. Notably, in ShadowHandReOrientation task, it experiences a sudden drop in cost, which significantly impacts the reward. Conversely, although similar phenomenon occurs with MAFOCPS, our method manages to optimize the reward in this condition and attains an acceptable performance. Although the reward of MACPO remains relatively stable, its final performance is not as favorable as our method. The different performance between MAPPO-L and the other two algorithms indicates that the property of upper bound violation is important in safe reinforcement learning, which also demonstrates the significance of our work.

### Efficiency Analysis between MAFOCPS and MACPO

As a first-order method, our MAFOCPS not only eliminates the approximation errors when solving the optimization problems, leading to better performance, but also significantly reduces the computation cost. In this part, we evaluate the memory cost and frame per second (FPS), which is often adopted to measure the training efficiency of reinforcement learning algorithms, between MAFOCPS and MACPO. For simplicity, we adopt memory monitor tools to track memory utilization after 200000 samples and record the average FPS metric.

Due to the page limit, the results are shown in Appendix H of supplementary materials. From the results, it is evident that our algorithm obtains substantial improvement in computational efficiency and shows the ability to effectively save memory resources, especially when the number of agents grows. Consequently, we can infer that when confronted with tasks with multiple agents, second-order algorithms may be ill-suited due to their substantial computational costs. In contrast, our proposed method can successfully address such scenarios without succumbing to the computational burden.

### Sensitivity Analysis

Based on the description provided in Section 4, it is apparent that the Lagrange multipliers, \(_{j}\) and \(_{max}\), are crucial to the performance of our approach. In addition, analyzing the sensitivity of our algorithm to changes in the safety bound is also meaningful. Hence, we seek to investigate the sensitivity of our algorithm to these hyperparameters as well as the safety bound. In this way, we conduct some ablation studies using some scenarios of Safe MAMuJoCo and more details are presented in Appendix I.

Considering the intricacy of multi-agent environments, it is difficult to delineate the correlation between the performance of our method and the hyperparameters \(_{j}\) and \(_{max}\). Nonetheless, choosing hyperparameters in proximity to the optimal values that we employed typically leads to favorable outcomes. Moreover, our approach's effectiveness is relatively insensitive to variations in these hyperparameter values. As an illustration, even setting \(_{max}=\) does not significantly affect the reward achieved by our method, only resulting in an average 6.7% degradation. This finding indicates the robustness of our method, and further underscores its superiority over other approaches. What's more, from the sensitivity of our method to safety bound, we learn that although the reward performance of MAFOCPS decreases with the stricter safety constraints, the algorithm's overall effectiveness remains unchanged across different safety levels.

## 6 Conclusion

In this paper, we introduce a novel method for training multi-agent systems while incorporating safety constraints. Building upon the problem formulation and sequential update scheme established in the

Figure 4: Performance comparisons on tasks of Safe MIAG. The safety constraint value for ShadowHandOver is set to be 15 and that for another task is set to be 110. The solid line shows the median performance across 5 seeds and the shaded areas correspond to the 25-75% percentiles.

MACPO framework , our algorithm offers rigorous theoretical guarantees and relies exclusively on first-order optimization techniques, thereby ensuring simplicity in its implementation. Different from MACPO, which relies on second-order optimization to achieve parameterized policies, our work proposes a fundamentally different way to tackle the optimization problem, complemented by the comprehensive provision of derivatives. The empirical experiment results demonstrate the outstanding performance and computation efficiency of our algorithm, compared to the more intricate second-order method. To sum up, our paper offers a novel and distinct contribution to the realm of safe multi-agent reinforcement learning, as evidenced by the distinctions in methodology, proofs, and experimental results in comparison with previous work.

Because the benchmarks we adopt only return one cost for agents, the performance of our algorithm across multiple costs is not evaluated. In the future, we plan to test our approach in more environments and physical settings. Moreover, there exist a number of promising prospects for future research such as incorporating off-policy data to further enhance the training efficiency. Designing methods aimed at offline settings, which precludes interactions with environments, represents another valuable direction for study.