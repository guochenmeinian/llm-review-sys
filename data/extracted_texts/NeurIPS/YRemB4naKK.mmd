# Deterministic Policies for Constrained Reinforcement Learning in Polynomial Time

Jeremy McMahan

University of Wisconsin-Madison

jmcmahan@wisc.edu

###### Abstract

We present a novel algorithm that efficiently computes near-optimal deterministic policies for constrained reinforcement learning (CRL) problems. Our approach combines three key ideas: (1) value-demand augmentation, (2) action-space approximate dynamic programming, and (3) time-space rounding. Our algorithm constitutes a fully polynomial-time approximation scheme (FPTAS) for any time-space recursive (TSR) cost criteria. A TSR criteria requires the cost of a policy to be computable recursively over both time and (state) space, which includes classical expectation, almost sure, and anytime constraints. Our work answers three open questions spanning two long-standing lines of research: polynomial-time approximability is possible for 1) anytime-constrained policies, 2) almost-sure-constrained policies, and 3) deterministic expectation-constrained policies.

## 1 Introduction

Constrained Reinforcement Learning (CRL) traditionally produces stochastic, expectation-constrained policies that can behave undesirably - imagine a self-driving car that randomly changes lanes or runs out of fuel. However, artificial decision-making systems must be predictable, trustworthy, and robust. One approach to ensuring these qualities is to focus on deterministic policies, which are inherently predictable, easily implemented , reliable for autonomous vehicles , and effective for multi-agent coordination . Similarly, almost sure and anytime constraints  provide inherent trustworthiness and robustness, essential for applications in medicine , disaster relief , and resource management . Despite the advantages of deterministic policies and stricter constraints, even the computation of approximate solutions has remained an open challenge since NP-hardness was proven nearly 25 years ago . Our work addresses this challenge by studying the computational complexity of computing deterministic policies for general constraint criteria.

Consider a constrained Markov Decision Process (cMDP) denoted by \(M\). Let \(C\) represent an arbitrary cost criterion and \(B\) be the available budget. We focus on the set of deterministic policies denoted by \(^{D}\). Our objective is to compute: \(_{^{D}}V_{M}^{}\) s.t. \(C_{M}^{} B\), where \(V_{M}^{}\) is the value and \(C_{M}^{}\) is the cost of \(\) in \(M\). This objective generalizes the example of a self-driving car calculating the fastest fixed route without running out of fuel. Our main question is the following:

_Can near-optimal deterministic policies for constrained reinforcement learning problems be computed in polynomial time?_

Although optimal stochastic policies for expectation-constrained problems are efficiently computable , the situation drastically changes when we require deterministic policies and general constraints. Computing optimal deterministic policies is NP-hard for most popular constraints, including expectation , chance , almost sure, and anytime constraints . This complexity remains even if we relax our goal to finding just one feasible policy, provided that we are dealingwith a single chance constraint , or at least two of the other mentioned constraints . Beyond these computational challenges, traditional solution methods, such as backward induction [41; 3], fail to apply due to the cyclic dependencies among subproblems: the value of any decision may depend on the costs of both preceding and concurrent decisions, preventing a solution from being computed in a single backward pass.

Past work.Past approaches fail to simultaneously achieve computational efficiency, feasibility, and optimality. Optimal and feasible algorithms, albeit inefficient, utilize Mixed-Integer Linear Programs  and Dual-guided heuristic forward searches  for expectation-constraints, and cost-augmented MDPs for almost sure  and anytime constraints . Conversely, optimal and efficient, though infeasible, algorithms are known for expectation , almost sure, and anytime constraints . A fully polynomial-time approximation scheme (FPTAS)  is known for expectation constraints, but it requires strong assumptions such as a constant horizon . Balancing computational efficiency, feasibility, and optimality remains the bottleneck to efficient approximation.

Our contributions.We present an FPTAS for computing deterministic policies under any time-space recursive (TSR) constraint criteria. A TSR criteria requires the cost of a policy to be computable recursively in both time and (state) space, which captures expectation, almost sure, and anytime constraints. Since non-TSR criteria, such as chance constraints , are provably inapproximable, TSR seems pivotal for efficient computation. Overall, our general framework answers three open complexity questions spanning two longstanding lines of work: we prove polynomial-time approximability for 1) anytime-constrained policies, 2) almost-sure-constrained policies, and 3) deterministic expectation-constrained policies, which have been open for nearly 25 years .

Our approach breaks down into three main ideas: (1) value-demand augmentation, (2) action-space approximate dynamic programming, and (3) time-space rounding. We augment the states with value demands and the actions with future value demands to break cyclic subproblem dependencies, enabling dynamic programming methods. Importantly, we use values because they can be rounded without compromising feasibility  and can capture constraints that are not predictable from cumulative costs. However, this results in an exponential action space that makes solving the Bellman operator as hard as the knapsack problem. By exploiting the space-recursive nature of the criterion, we can efficiently approximate the Bellman operator with dynamic programming. Finally, rounding value demands result in approximation errors over both time and space, but carefully controlling these errors ensures provable guarantees.

### Related work

Approximate packing.Many stochastic packing problems, which generalize the knapsack problem, are captured by our problem. Dean et al. , Frieze and Clarke  derived optimal approximation ratio algorithms for stochastic packing and integer packing with multiple constraints, respectively. Yang et al. , Bhalgat et al.  designed efficient approximation algorithms for variations of the stochastic knapsack problem. Then, Halman et al.  derived an FPTAS for a general class of stochastic dynamic programs, which was then further improved in [26; 1]. These methods require a single-dimensional state space that captures the constraint. In contrast, our problems have an innate state space in addition to the constraint. Our work forms a similar general dynamic programming framework for the more complex MDP setting.

Constrained RL.It is known that stochastic expectation-constrained policies are polynomial-time computable via linear programming , and many planning and learning algorithms exist for them [39; 46; 8; 28]. Some learning algorithms can even avoid violation during the learning process under certain assumptions [48; 4]. Furthermore, Brantley et al.  developed no-regret algorithms for cMDPs and extended their algorithms to the setting with a constraint on the cost accumulated over all episodes, which is called a knapsack constraint [10; 13].

Safe RL.The safe RL community [22; 25] has mainly focused on no-violation learning for stochastic expectation-constrained policies [14; 9; 2; 12; 5] and solving chance constraints [47; 53], which capture the probability of entering unsafe states. Performing learning while avoiding dangerous states  is a special case of expectation constraints that has also been studied [42; 44] under non-trivial assumptions. In addition, instantaneous constraints, which require the expected cost to be within budget at all times, have also been studied [33; 20; 24].

## 2 Cost criteria

In this section, we formalize our problem setting. We also define our conditions for cost criteria.

Constrained Markov decision processes.A (tabular, finite-horizon) _Constrained Markov Decision Process_ (cMDP) is a tuple \(M=(,,P,r,c,H)\), where (i) \(\) is the finite set of _states_, (ii) \(\) is the finite set of _actions_, (iii) \(P_{h}(s,a)(S)\) is the _transition_ distribution, (iv) \(r_{h}(s,a)\) is the _reward_, (v) \(c_{h}(s,a)\) is the _cost_, and (vi) \(H\) is the finite _time horizon_. We let \(S:=||\), \(A:=||\), \([H]:=\{1,,H\}\), and \(\) denote the set of all cMDPs. We also let \(r_{max}}}{{=}}_{h,s,a}|r_{h}(s,a)|\) denote the maximum magnitude reward, \(r_{min}}}{{=}}_{h,s,a}r_{h}(s,a)\) denote the true minimum reward, and \(p_{min}}}{{=}}_{h,s,a,s^{}}P_{h }(s^{} s,a)\) denote the minimum transition probability. Since \(\) is a finite set, we often assume \(=[S]\) WLOG. Lastly, for any predicate \(p\), we use the Iverson bracket notation \([p]\) to denote \(1\) if \(p\) is true and \(0\) otherwise, and we let \(_{p}\) denote the characteristic function which evaluates to \(0\) if \(p\) is true and \(\) otherwise.

Interaction protocol.The agent interacts with \(M\) using a policy \(=(_{h})_{h=1}^{H}\). In the fullest generality, \(_{h}:_{h}()\) is a mapping from the observed history at time \(h\) to a distribution of actions. In contrast, a deterministic policy takes the form \(_{h}:_{h}\). We let \(\) denote the set of all possible policies and \(^{D}\) denote the set of all deterministic policies. The agent starts at the initial state \(s_{0}\) with observed history \(_{1}=(s_{0})\). For any \(h[H]\), the agent chooses an action \(a_{h}_{h}(_{h})\). Then, the agent receives immediate reward \(r_{h}(s_{h},a_{h})\) and cost \(c_{h}(s_{h},a_{h})\). Lastly, \(M\) transitions to state \(s_{h+1} P_{h}(s_{h},a_{h})\) and the agent updates the history to \(_{h+1}=(_{h},a_{h},s_{h+1})\). This process is repeated for \(H\) steps; the interaction ends once \(s_{H+1}\) is reached.

Objective.For any cost criterion \(C:\) and budget \(B\), the agent's goal is to compute a solution to the following optimization problem:

\[_{}_{M}^{}[_{h=1}^{H}r_{h}(s_{h},a_{h}) ]C_{M}^{} B\\ .\] (CON)

Here, \(_{M}^{}\) denotes the probability law over histories induced from the interaction of \(\) with \(M\), and \(_{M}^{}\) denotes the expectation defined by this law. We let \(V_{M}^{}}}{{=}}_{M}^{} [_{t=1}^{H}r_{t}(s_{t},a_{t})]\) denote the value of a policy \(\), and \(V_{M}^{*}\) denote the optimal solution value to (CON).

Cost criteria.We consider a broad family of cost criteria that satisfy a strengthening of the standard policy evaluation equations . This strengthening requires not only the cost of a policy to be computable recursively in the time horizon, but at each time the cost should also break down recursively in (state) space.

**Definition 1** (Tsr).: We call a cost criterion \(C\)_time-recursive_ (TR) if for any cMDP \(M\) and policy \(^{D}\), \(\)'s cost decomposes recursively into \(C_{M}^{}=C_{1}^{}(s_{0})\). Here, \(C_{H+1}^{}()=\) and for any \(h[H]\) and \(_{h}_{h}\),

\[C_{h}^{}(_{h})=c_{h}(s,a)+f((P_{h}(s^{} s,a),C_{h+1} ^{}(_{h},a,s^{}))_{s^{} P_{h}(s,a)} ),\] (TR)

where \(s=s_{h}(_{h})\), \(a=_{h}(_{h})\), and \(f\) is a non-decreasing function1 computable in \(O(S)\) time. For technical reasons, we also require that \(f(x)=\) whenever \( x\).

We further say \(C\) is _time-space-recursive_ (TSR) if the \(f\) term above is equal to \(g_{h}^{_{h},a}(1)\). Here, \(g_{h}^{_{h},a}(S+1)=0\) and for any \(t S\),

\[g_{h}^{_{h},a}(t)=((P_{h}(t s,a),C_{h+1}^{} (_{h},a,t)),g_{h}^{_{h},a}(t+1)),\] (SR)where \(\) is a non-decreasing function, and both \(,\) are computable in \(O(1)\) time. We also assume that \((,)=\), and \(\) satisfies \(((0,),x)=x\) to match \(f\)'s condition.

Since the TR condition is a slight generalization of traditional policy evaluation, it is easy to see that we can solve for minimum-cost policies using backward induction.

**Proposition 1** (TR Intuition).: _If \(C\) is TR, then \(C\) satisfies the usual optimality equations. Furthermore, \(_{^{D}}C_{M}^{}\) can be computed using backward induction in \(O(HS^{2}A)\) time._

Although the TR condition is straightforward, the TSR condition is more strict. We will see the utility of the TSR condition in Section 4 when computing Bellman updates. For now, we point out that the TSR condition is not too restrictive: it is satisfied by many popular criteria studied in the literature.

**Proposition 2** (TSR examples).: _The following classical constraints can be modeled by a TSR cost constraint._

1. (Expectation Constraints) _are captured by_ \(C_{M}^{}}}{{=}}_{M}^{} [_{h=1}^{H}c_{h}(s_{h},a_{h})] B\)_. We see_ \(C\) _is TSR by defining_ \((x,y)}}{{=}}x+y\) _and_ \((x,y)}}{{=}}xy\)_._
2. (Almost Sure Constraints) _are captured by_ \(C_{M}^{}}}{{=}}_{ H_{H+1},\\ _{M}^{}[] 0}_{h=1}^{H}c_{h}(s_{h},a_{h})  B\)_. We see_ \(C\) _is TSR by defining_ \((x,y)}}{{=}}(x,y)\) _and_ \((x,y)}}{{=}}[x>0]y\)_._
3. (Anytime Constraints) _are captured by_ \(C_{M}^{}}}{{=}}_{t[H]}_{  H_{H+1},\\ _{M}^{}[]>0}_{h=1}^{t}c_{h}(s_{h},a_{h})  B\)_. We see_ \(C\) _is TSR by defining_ \((x,y)}}{{=}}(0,(x,y))\) _and_ \((x,y)}}{{=}}[x>0]y\)_._

_Remark 1_ (Extensions).: Our methods can also handle stochastic costs and infinite discounting. We defer the details to Appendix F. Moreover, we can handle multiple constraints using vector-valued criteria so long as the comparison operator is a total ordering of the vector space.

_Remark 2_ (Inapproximability).: Our methods cannot handle chance constraints or more than one of our example constraints. However, this is not a limitation of our framework as the problem becomes provably inapproximable under said constraints [51; 36].

## 3 Covering algorithm

In this section, we propose an algorithm to solve (CON). Our approach relies on converting the original problem into an equivalent covering problem that can be solved using an unconstrained MDP. This covering MDP is derived using the key idea of value augmentation.

Packing and covering.We can view (CON) as a _packing program_, which wishes to maximize \(V_{M}^{}\) subject to \(C_{M}^{} B\). However, we could also tackle the problem by reversing the objective: attempt to minimize \(C_{M}^{}\) subject to \(V_{M}^{} V_{M}^{}\). If (CON) is feasible, then any optimal solution \(\) to this _covering program_ satisfies \(V_{M}^{} V_{M}^{}\) and \(C_{M}^{} B\). Thus, we can solve the original packing program by solving the covering program.

**Proposition 3** (Packing-Covering Reduction).: _Suppose that \(C_{M}^{}}}{{=}}_{^{D}}C_{M} ^{}\) s.t. \(V_{M}^{} V_{M}^{}\). Then, \(C_{M}^{} B V_{M}^{}>-\). Furthermore, if \(V_{M}^{}>-\), then,_

\[_{^{D}}C_{M}^{}\\ V_{M}^{} V_{M}^{}_{ ^{D}}V_{M}^{}\\ C_{M}^{} B.\] (PC)

_Thus, any solution to the covering program is a solution to the packing program._

We focus on the covering program for several reasons. To optimize the value recursively, we would need to predict the final cost resulting from intermediate decisions to ensure feasibility. Generally, such predictions would require strict assumptions on the cost criteria. By treating the value as the constraint instead, we only need to assume the cost can be optimized efficiently. Moreover, values are well understood in RL and are more amenable to approximation . Thus, the covering program allows us to capture many criteria, ensure feasibility, and compute accurate value approximations.

Value augmentation.We can solve the covering program by solving a cost-minimizing MDP \(\). The key idea is to augment the state space with value demands, \((s,v)\). Then, the agent can recursively reason how to minimize its cost while meeting the current value demand. If the agent starts at \((s_{0},V_{M}^{*})\), then an optimal policy for \(\) should be a solution to the covering program.

The key invariant we desire is that any feasible policy \(\) for \(\) should satisfy \(_{h}^{}(s,v) v\). To ensure this invariance, we recall the policy evaluation equations . If \(_{h}(s)=a\), then,

\[_{h}^{}(s,v)=r_{h}(s,a)+_{s^{}}P_{h}(s^{} s,a) _{h+1}^{}(s^{},v_{s^{}}).\] (PE)

For the value invariant to be satisfied, it suffices for the agent to choose an action \(a\) and commit to future value demands \(v_{s^{}}\) satisfying,

\[r_{h}(s,a)+_{s^{}}P_{h}(s^{} s,a)v_{s^{}} v.\] (DEM)

We can view choosing future value demands as part of the agent's augmented actions. Then, at any augmented state \((s,v)\), the agent's augmented action space includes all \((a,)^{S}\) satisfying (DEM). When \(M\) transitions to \(s^{} P_{h}(s,a)\), the agent's new augmented state should consist of the environment's new state in addition to its chosen demand for that state, \((s^{},v_{s^{}})\). Putting these pieces together yields the definition of the cover MDP, Definition 2.

**Definition 2** (Cover MDP).: The _cover MDP_\(}}{{=}}(},},,,H)\) where,

1. \(}}}{{=}} \) where \(}}{{=}}\{v ^{D},h[H+1],_{h}_{h},\ V_{h}^{}(_{h})=v\}\)
2. \(}_{h}(s,v)}}{{=}}\{(a, )^{} r_{h}(s,a)+_{ s^{}}P_{h}(s^{} s,a)v_{s^{}} v\}\).
3. \(_{h}((s^{},v^{})(s,v),(a,))}}{{=}}P_{h}(s^{} s,a)[v^{}=v_{s^{}}]\).
4. \(_{h}((s,v),(a,))}}{{=}}c_ {h}(s,a)\).

The objective for \(\) is to minimize the cost function \(}}{{=}}C_{}\) with modified base case \(_{H+1}^{}(s,v)}}{{=}}_{\{v 0\}}\).

Covering algorithm.Importantly, the action space definition ensures the value constraint is satisfied. Meanwhile, the minimum cost objective ensures optimal cost. So long as our cost is TR, \(\) can be solved using fast RL methods instead of the brute force computation required for general covering programs. These properties ensure our method, Algorithm 1, is correct.

**Theorem 1** (Reduction).: _If Solve is any finite-time MDP solver, then Algorithm 1 correctly solves (CON) in finite time for any TR cost criterion._

_Remark 3_ (Execution).: Given a value-augmented policy \(\) output from Algorithm 1, the agent can execute \(\) using Algorithm 2. To compute \(V_{M}^{*}\) as the starting value, it suffices for the agent to compute,

\[V_{M}^{*}=\{v_{1}^{*}(s_{0},v) B \}.\] (1)

This computation can be easily done given \(_{1}^{*}(s_{0},)\) in \(O(||)\) time.

```
0:\(\)
1:\(_{1}=(s_{0},V_{M}^{*})\)
2:for\(h 1\) to \(H\)do
3:\((a,)_{h}(_{h})\)
4:\(r_{h}=r_{h}(s,a)\) and \(s_{h+1} P_{h}(s_{h},a)\)
5:\(_{h+1}=(s_{h+1},v_{s_{h+1}})\) ```

**Algorithm 2** Augmented interaction

## 4 Fast Bellman updates

In this section, we present an algorithm to solve \(\) from Definition 2 efficiently. Although the Bellman updates can be as hard to solve as the knapsack problem, we use ideas from knapsack approximation algorithms to create an efficient method. Our approach exploits (SR) through approximate dynamic programming on the action space.

Even if \(\) were small, solving \(\) would still be challenging due to the exponentially large action space. Even a single Bellman update requires the solution of a constrained optimization problem:

\[_{h}^{*}(s,v) =_{a,}c_{h}(s,a)+f((P_{h}(s^{} s,a),_{h+1}^{*}(s^{},v_{s^{}}))_{s^{ } P_{h}(s,a)})\] (BU) \[r_{h}(s,a)+_{s^{}}P_{h}(s^{} s,a)v_{s^{}} v.\]

Above, we used the fact that \((s^{},v^{})_{h}((s,v),(a,))\) iff \(s^{} P_{h}(s,a)\) and \(v^{}=v_{s^{}}\) to simplify \(f\)'s input. Observe that even when each \(v_{s^{}}\) only takes on two possible values, \(\{0,w_{s^{}}\}\), the optimization above can capture the minimization version of the knapsack problem, implying that it is NP-hard to compute.

Recursive approach.Fortunately, we can use the connection to the Knapsack problem positively to efficiently approximate the Bellman update. For any fixed \((s,v)}\) and \(a\), we focus on the inner constrained minimization over \(\):

\[_{^{3}\\ r_{h}(s,a)+_{s^{}}P_{h}(s^{} s,a)v_{s^{}} v }f((P_{h}(s^{} s,a),_{h+1}^{*} (s^{},v_{s^{}}))_{s^{} P_{h}(s,a)})\] (2)

We use (SR) to transform this minimization over \(\) into a sequential decision-making problem that decides each \(v_{s^{}}\). As above, we can use the definition of \(\) to simplify \(g_{h}^{(s,v),(a,)}(t,v^{})\) into a function of \(t\) alone:

\[g_{h}^{(s,v),(a,)}(t)=((P_{h}(t s,a),_{h+1}^{*}(t,v_{t})),g_{h}^{(s,v),(a,)}(t+1)).\] (3)

Since \(v\) only constrains the valid \((a,)\) pairs, we can discard \(v\) and use the simplified notation \(g_{h,}^{s,a}(t)\) instead of \(g_{h}^{(s,v),(a,)}(t)\). It is then clear that we can recursively optimize the value of \(v_{t}\) by focusing on \(g_{h,}^{s,a}(t)\).

To recursively encode the value constraint, we can record the partial value \(u=r_{h}(s,a)+_{s^{}=1}^{t-1}P_{h}(s^{} s,a)v_{s^{}}\) that we have accumulated so far. Then, we can check if our choices for \(\) satisfied the constraint with the inequality \(u v\). The formal recursion is defined in Definition 3.

**Definition 3**.: For any \(h[H]\), \(s\), \(v\), and \(u\), we define, \(g_{h,v}^{s,a}(S+1,u)=_{\{u v\}}\) and for \(t S\),

\[g_{h,v}^{s,a}(t,u)=_{v_{t}}((P_{h}(t  s,a),_{h+1}^{*}(t,v_{t})),g_{h,v}^{s,a}(t+1,u+P_{ h}(t s,a)v_{t})).\] (DP)

Recursive rounding.This approach can still be slow due to the exponential number of partial values \(u\) induced. Similarly to the knapsack problem, the key is to round each input \(u\) to ensure fewer subproblems. Unlike the knapsack problem, however, we do not have an easily computable lower bound on the optimal value. Thus, we turn to a more aggressive recursive rounding. Since rounding may cause originally feasible values to violate the demand constraint, we also relax the demand constraint to \(u(v)\) for some lower bound function \(\).

**Definition 4**.: Fix a rounding function \(_{}\) and a lower bound function \(\). For any \(h[H]\), \(s\), \(v\), and \(u\), we define, \(^{s,a}_{h,v}(S+1,u)=_{\{u v\}}\) and for \(t S\),

\[^{s,a}_{h,v}(t,u)}}{{=}}_{v_{t} }((P_{h}(t s,a),^{*}_{h+1}( t,v_{t})),^{s,a}_{h,v}(t+1, u+P_{h}(t s,a)v_{t} )_{}).\] (ADP)

Fortunately, the approximate version behaves similarly to the original. The main difference is the constraint now ensures the rounded sums are at least the value lower bound. This is formalized in Lemma 1.

**Lemma 1**.: _For any \(t[S+1]\) and \(u\), we have that,_

\[^{s,a}_{h,v}(t,u) =_{^{S-t+1}}g^{s,a}_{h,}(t)\] (4) \[^{s,a}_{h,}(t,u)( v),\]

_where \(^{s,a}_{h,}(t,u)}}{{=}}  u+P_{h}(t s,a)v_{t}_{}++P_{h}(S  s,a)v_{S}_{}\)._

To turn this recursion into a usable dynamic programming algorithm, we must also pre-compute the inputs to any sub-computation. Unlike in standard RL, this computation must be done with a forward recursion. The details for the approximate Bellman update are given in Definition 5.

**Definition 5** (Approx Bellman).: For any \(h[H]\), \(s\), and \(a\), we define \(}^{s,a}_{h}(1)}}{{=}}\{r_{h }(s,a)\}\) and for any \(t[S]\),

\[}^{s,a}_{h}(t+1)}}{{=}} _{v_{t}}_{u}^{s,a}_{h}(t)}\{  u+P_{h}(t s,a)v_{t}_{}\}.\] (5)

Then, an approximation to the Bellman update can be computed using Algorithm 3.2

**Proposition 4**.: _Algorithm 4 runs in \(O(HS^{2}A||^{2})\) time, where \(}}{{=}}_{h,s,a}|^{s,a}_{h}|\). When \(_{}\) and \(\) are the identity function, Algorithm 4 outputs an optimal solution to \(\)._

_Remark 4_ (Speedups).: The runtime of our methods can be quadratically improved by rounding the differences instead of the sums. We defer the details to Appendix F.

Approximation algorithms

In this section, we present our approximation algorithms for solving (CON). We carefully round the value demands over both time and space to induce an approximate MDP. Solving this approximate MDP with Algorithm 4 yields our FPTAS.

Although we can avoid exponential-time Bellman updates, the running time of the approximate Bellman update will still be slow if \(||\) is large. To reduce the complexity, we instead use a smaller set of approximate values by rounding elements of \(||\). By rounding down, we effectively relax the value-demand constraint. More aggressive rounding not only leads to smaller augmented state spaces but also to smaller cost policies. The trade-off is aggressive rounding leads to weaker guarantees on the computed policy's value. Thus, it is critical to carefully design the rounding and lower bound functions to balance this trade-off.

Value approximation.Given a rounding down function \(_{}\), we would ideally use the rounded set \(\{ v_{} v\}\) to form our approximate state space. To avoid having to compute \(\) explicitly, we instead use the rounded superset \(\{ v_{} v[v_{min},v_{max} ]\}\), where \(v_{min}\) and \(v_{max}\) are bounds on the extremal values that we specify later. To ensure we can use Algorithm 4 to find solutions efficiently, we must also relax the augmented action space to only include vectors that lead to feasible subproblems for (ADP). From Lemma 1, we know this is exactly the set of \((a,})\) for which \(_{h,}}^{s,a}(1,r_{h}(s,a))(v)\). Combining these ideas yields the new approximate MDP, defined in Definition 6.

**Definition 6** (Approximate MDP).: Given a rounding function \(_{}\) and lower bound function \(\), the _approximate MDP_\(}}{{=}}(},},,,H)\) where,

1. \(}}}{{=}} }\) where \(}}}{{=}}\{ v _{} v[v_{min},v_{max}]\}\).
2. \(}_{h}(s,)}}{{=}} (a,})}^{S} _{h,}}^{s,a}(1,r_{h}(s,a))()}\).
3. \(_{h}((s^{},^{})(s,),(a,} ))}}{{=}}P_{h}(s^{} s,a)[^ {}=_{s^{}}]\).
4. \(_{h}((s,),(a,}))}}{{=}}c_{h}(s,a)\).

The objective for \(\) is to minimize the cost function \(}}{{=}}C_{}\) with modified base case \(_{H+1}^{}(s,)}}{{=}} _{\{ 0\}}\).

We can show that rounding down in Definition 6 achieves our goal of producing smaller cost policies. This ensures feasibility is even easier to achieve. We formalize this observation in Lemma 2.

**Lemma 2** (Optimistic Costs).: _For our later choices of \(_{}\) and \(\), the following holds: for any \(h[H+1]\) and \((s,v)}\), we have \(_{h}^{*}(s, v_{})_{h}^{* }(s,v)\)._

Thus, Algorithm 5 always outputs a policy with better than optimal cost when the instance is feasible, \(V_{M}^{*}>-\). If the instance is infeasible, all policies have cost larger than \(B\) by definition and so Algorithm 5 correctly indicates the instance is infeasible. The remaining question is whether Algorithm 5 outputs policies having near-optimal value.

Time-Space errors.To assess the optimality gap of Algorithm 5 policies, we must first explore the error accumulated by our rounding approach. Rounding each value naturally accumulates approximation error over time. Rounding the partial values while running Algorithm 3 accumulates additional error over (state) space. Thus, solving \(\) using Algorithm 4 accumulates error over both time and space, unlike other approximate methods in RL. As a result, our rounding and threshold functions will generally depend on both \(H\) and \(S\).

Arithmetic rounding.Our first approach is to round each value down to its closest element in a \(\)-cover. This guarantees that \(v- v_{} v\). Thus, \( v_{}\) is an underestimate that is not too far from the true value. By setting \(\) to be inversely proportional to \(SH\), we control the errors over time and space. The lower bound must also be a function of \(S\) since it controls the error over space.

**Definition 7** (Additive Approx).: Fix \(>0\). We define,

\[ v_{}}}{{=}} (v)}}{{=}}v-(S+1),\] (6)

where \(}}{{=}}\), \(v_{min}}}{{=}}-Hr_{max}\), and \(v_{max}}}{{=}}Hr_{max}\).

**Theorem 2** (Additive FPTAS).: _For any \(>0\), Algorithm 5 using Definition 7 given any cMDP \(M\) and TSR criteria \(C\) either correctly outputs the instance is infeasible, or produces a policy \(\) satisfying \(^{} V_{M}^{}-\) in \(O(H^{7}S^{5}Ar_{max}^{3}/^{3})\) time. Thus, it is an additive-FPTAS for the class of cMDPs with polynomial-bounded \(r_{max}\) and TSR criteria._

Geometric rounding.Since the arithmetic approach can be slow when \(r_{max}\) is large, we can instead round values down to their closest power of \(1/(1-)\). This guarantees the number of approximate values needed is upper bounded by a function of \((r_{max})\), which is polynomial in the input size. We choose a geometric scheme satisfying \(v(1-) v_{} v\) so that the rounded value is an underestimate and a relative approximation to the true value. To ensure this property, we must now require that all rewards are non-negative.

**Definition 8** (Relative Approx).: Fix \(>0\). We define,

\[ v_{}}} {{=}}v^{min}()^{_{}}}(v)}}{{=}}v(1-)^{S+1},\] (7)

where \(}}{{=}}\), \(v_{min}=p_{min}^{H}r_{min}\), and \(v_{max}=Hr_{max}\).

**Theorem 3** (Relative FPTAS).: _For \(>0\), Algorithm 5 using Definition 8 given any cMDP \(M\) and TSR criteria \(C\) either correctly outputs the instance is infeasible, or produces a policy \(\) satisfying \(^{} V_{M}^{}(1-)\) in \(O(H^{7}S^{5}A(r_{max}/r_{min}p_{min})^{3}/^{3})\) time. Thus, it is a relative-FPTAS for the class of cMDPs with non-negative rewards and TSR criteria._

_Remark 5_ (Assumption Necessity).: We also note the mild reward assumptions we made to guarantee efficiency are unavoidable. Without reward bounds, (CON) captures the knapsack problem which does not admit additive approximations. Similarly, without non-negativity, relative approximations for maximization problems are generally not computable.

## 6 Conclusions

In this paper, we studied the computational complexity of computing deterministic policies for CRL problems. Our main contribution was the design of an FPTAS, Algorithm 5, that solves (CON) for any cMDP and TSR criteria under mild reward assumptions. In particular, our method is an additive-FPTAS if the cMDP's rewards are polynomially bounded, and is a relative-FPTAS if the cMDP's rewards are non-negative. We note these assumptions are necessary for efficient approximation, so our algorithm achieves the best approximation guarantees possible under worst-case analysis. Moreover, our algorithmic approach, which uses approximate dynamic programming over time and the state space, highlights the importance of the TSR condition in making (CON) tractable. Our work finally resolves the long-standing open questions of polynomial-time approximability for 1) anytime-constrained policies, 2) almost-sure-constrained policies, and 3) deterministic expectation-constrained policies.

Future work.Several interesting questions remain unanswered. First, it remains unresolved whether an FPTAS exists asymptotically faster than ours. Second, whether our TSR condition is necessary for efficient computation or whether a more general condition could be derived is unclear. Lastly, it is open whether there exist algorithms that can feasibly handle multiple constraints from Proposition 2. Although computing feasible policies for multiple constraints is NP-hard, special cases may be approximable efficiently. Moreover, an average-case or smoothed-case analysis could circumvent this worst-case hardness.