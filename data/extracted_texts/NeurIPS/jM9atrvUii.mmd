# Kermut: Composite kernel regression for protein variant effects

Peter Morch Groth

University of Copenhagen

Novonesis

equal contribution

&Mads Herbert Kerrn

University of Copenhagen

&Lars Olsen

Novonesis

&Jesper Salomon

Novonesis

&Wouter Boomsma

University of Copenhagen

###### Abstract

Reliable prediction of protein variant effects is crucial for both protein optimization and for advancing biological understanding. For practical use in protein engineering, it is important that we can also provide reliable uncertainty estimates for our predictions, and while prediction accuracy has seen much progress in recent years, uncertainty metrics are rarely reported. We here provide a Gaussian process regression model, Kermut, with a novel composite kernel for modeling mutation similarity, which obtains state-of-the-art performance for supervised protein variant effect prediction while also offering estimates of uncertainty through its posterior. An analysis of the quality of the uncertainty estimates demonstrates that our model provides meaningful levels of overall calibration, but that instance-specific uncertainty calibration remains more challenging.

## 1 Introduction

Accurately predicting protein variant effects is crucial for both advancing biological understanding and for engineering and optimizing proteins towards specific traits. Recently, much progress has been made in the field as a result of advances in machine learning-driven modeling , data availability , and relevant benchmarks .

While prediction accuracy has received considerable attention, the ability to quantify the uncertainties of predictions has been less intensely explored. This is of immediate practical consequence. One of the main purposes of protein variant effect prediction is as an aid for protein engineering and design, to propose promising candidates for subsequent experimental characterization. For this purpose, it is essential that we can quantify, on an instance-to-instance basis, how trustworthy our predictions are. Specifically, in a Bayesian optimization setting, most choices of acquisition function actively rely on predicted uncertainties to guide the optimization, and well-calibrated uncertainties have been shown to correlate with optimization performance .

Our goal with this paper is to start a discussion on the quality of the estimated uncertainties of supervised protein variant effect prediction. Gaussian Processes (GP) are a standard choice for uncertainty quantification due to the closed form expression of the posterior. We therefore first ask the question whether state-of-the-art performance can be obtained within the GP framework. We propose a composite kernel that comfortably achieves this goal, and subsequently investigate the quality of the uncertainty estimates from such a model. Our results show that while standard approaches like reliability diagrams give the impression of good levels of calibration, the quantification of per-instance uncertainties is more challenging. We make our model available as a baseline and encourage thecommunity to place greater emphasis on uncertainty quantification in this important domain. Our contributions can be summarized as follow:

* We introduce **Kermut**, a Gaussian process with a novel composite **k**ernel for modeling **mutation** similarity, leveraging signals from pretrained sequence and structure models;
* We evaluate this model on the comprehensive ProteinGym substitution benchmark and show that it is able to reach state-of-the-art performance in supervised protein variant effect prediction, outperforming recently proposed deep learning methods in this domain;
* We provide a thorough calibration analysis and show that while Kermut provides well-calibrated uncertainties overall, the calibratedness of instance-specific uncertainties remains challenging;
* We demonstrate that our model can be trained and evaluated orders of magnitude faster and with better out-of-the-box calibration than competing methods.

## 2 Related work

### Protein property prediction

Predicting protein function and properties using machine-learning based approaches continues to be an innovative and important area of research.

Recently, _unsupervised_ approaches have gained significant momentum where models trained in a self-supervised fashion have shown impressive results for zero-shot estimates of protein _fitness_ and _variant effects_ relative to a reference protein [3; 9; 10; 11].

_Supervised_ learning is a crucial method of utilizing experimental data to predict protein fitness. This is particularly valuable when the trait of interest correlates poorly with the evolutionary signals that unsupervised models capture during training or if multiple traits are considered. Supervised protein fitness prediction using machine learning has been explored in detail in , where a comprehensive overview can be found. A common strategy is to employ transfer learning via embeddings extracted from self-supervised models [13; 14], an approach which increasingly relies on large pretrained language models such as ProtTrans , ESM-2 , and SaProt . In , the authors propose to augment a one-hot encoding of the aligned amino acid sequence by concatenating it with a zero-shot score for improved predictions. This was further expanded upon with ProteinNPT , where sequences embedded with the MSA Transformer  and zero-shot scores were fed to a transformer architecture for state-of-the-art supervised variant effect prediction with generative capabilities.

Considerable progress has been made in defining meaningful and comprehensive benchmarks to reliably measure and compare model performance in both unsupervised and supervised protein fitness prediction settings. The FLIP benchmark  introduced three supervised predictions tasks ranging from local to global fitness prediction, where each task in turn was divided into clearly defined splits. The supervised benchmarks often view fitness prediction through a particular lens. Where FLIP targeted problems of interest to protein engineering; TAPE  evaluated transfer learning abilities; PEER  focused on sequence understanding; ATOM3D  considered a structure-based approach; FLOP  targeted wild type proteins; and ProteinGym focused exclusively on variant effect prediction . The ProteinGym benchmark was recently expanded to encompass more than 200 standardized datasets in both zero-shot and supervised settings, including substitutions, insertions, deletions, and curated clinical datasets [11; 7].

### Kernel methods for protein sequences

Kernel methods have seen much use for protein modeling and protein property prediction. Sequence-based string kernels operating directly on the protein amino acid sequences are one such example, where, e.g., matching _k_-mers at different _k_s quantify covariance. This has been used with support vector machines to predict protein homology [25; 26]. Another example is sub-sequence string kernels, which in  is used in a Gaussian process for a Bayesian optimization procedure. In , string kernels were combined with predicted physicochemical properties to improve accuracy in the prediction of MHC-binding peptides and in protein fold classification. In , a kernel leveraging the tertiary structure for a protein family represented as a residue-residue contact map was used to predict various protein properties such as enzymatic activity and binding affinity. In , Gaussian process regression (GPR) was used to successfully identify promising enzyme sequences which were subsequently synthesized showing increased activity. In , the authors provide a comprehensive study of kernels on biological sequences which includes a thorough review of the literature as well as both theoretical, simulated, and in-silico results.

Most similar to our work is mGPfusion , in which a weighted decomposition kernel was defined which operated on the local tertiary protein structure in conjunction with a number of substitution matrices. Simulated stability data for all possible single mutations were obtained via Rosetta , which was then fused with experimental data for accurate \(\)G predictions of single- and multi-mutant variants via GPR, thus incorporating both sequence, structure, and a biophysical simulator. In contrast to our approach, the mGPfusion-model does not leverage pretrained models, but instead relies on substitution matrices for its evolutionary signal. A more recent example of kernel-based methods yielding highly competitive results is xGPR , in which Gaussian processes with custom kernels show high performance when trained on protein language model embeddings, similarly to the sequence kernel in our work (see Section 3.3). Where xGPR introduces a set of novel random feature-approximated kernels with linear-scaling, Kermut instead uses the squared exponential kernel for sequence modeling while additionally modeling local structural environments. The models in xGPR were shown to provide both high accuracy and well-calibrated uncertainty estimation on the FLIP and TAPE benchmarks.

### Uncertainty quantification and calibration

Uncertainty quantification (UQ) for protein property prediction continues to be a promising area of research with immediate practical consequences. In , residual networks were used to model both epistemic and aleatoric uncertainty for peptide selection. In , GPR on MLP-residuals from biLSTM embeddings was used to successfully guide in-silico experimental design of kinase binders and fluorescent proteins. The authors of  augmented a Bayesian neural network by placing biophysical priors over the mean function by directly using Rosetta energy scores, whereby the model would revert to the biophysical prior when the epistemic uncertainty was large. This was used to predict fluorescence, binding, and solubility for drug-like molecules. In , state-of-the-art performance on protein-protein interactions was achieved by using a spectral-normalized neural Gaussian process  with an uncertainty-aware transformer-based architecture working on ESM-2 embeddings.

In , a framework for evaluating the epistemic uncertainty of deep learning models using confidence interval-based metrics was introduced, while  conducted a thorough analysis of uncertainty quantification methods for molecular property prediction. Here, the importance of supplementing confidence-based calibration with error-based calibration as introduced in  was highlighted, whereby the predicted uncertainties are connected directly to the expected error for a more nuanced calibration analysis. We evaluate our model using confidence-based calibration as well as error-based

Figure 1: Overview of Kermut’s structure kernel. Using an inverse folding model, structure-conditioned amino acid distributions are computed for all sites in the reference protein. The structure kernel yields high covariances between two variants if the local environments are similar, if the mutation probabilities are similar, and if the mutates sites are physically close. Constructed examples of expected covariances between variant \(_{1}\) and \(_{2,3,4}\) are shown.

calibration following the guidelines in . In , the authors conducted a systematic comparison of UQ methods on molecular property regression tasks, while  investigated calibratedness of regression models for material property prediction. In , the above approaches were expanded to protein property prediction tasks where the FLIP  benchmark was examined, while  benchmarked a number of UQ methods for molecular representation models. In , the authors developed an active learning approach for partial charge prediction of metal-organic frameworks via Monte Carlo dropout  while achieving decent calibration. In , a systematic analysis of protein regression models was conducted where well-calibrated uncertainties were observed for a range of input representations.

### Local structural environments

Much work has been done to solve the inverse-folding problem, where the most probable amino acid sequence to fold into a given protein backbone structure is predicted [50; 51; 52; 53; 54; 55; 56]. Inverse-folding models are trained on large datasets of protein structures and model the physicochemical and evolutionary constraints of sites in a protein conditioned on their structural contexts. These will form the basis of the structural featurization in our work. Local structural environments have previously been used for protein modeling. In , a 3D CNN was used to predict amino acid preferences giving rise to novel substitution matrices. In  and , surface-level fingerprinting given structural environments was used to model protein-protein interaction sites and for de novo design of protein interactions. In , chemical microenvironments were used to identify potentially beneficial mutations, while  used a similar approach, where they investigated the volume of the local environments and observed that the first contact shell delivered the primary signal thus emphasizing the importance of locality. In , a composition Hellinger distance metric based on the chemical composition of local residue environments was developed and used for a range of structure-related experiments. Recently, local structural environments were used to model mutational preferences for protein engineering tasks , however not in a Gaussian process framework as we propose here.

## 3 Methods

### Preliminaries

We want to predict the outcome of an assay measured on a protein, represented by its amino acid sequence \(\) of length \(L\). We will assume that we have a dataset of \(N\) such sequences available, and that these are of equal length and structure, such that we can meaningfully refer to the effect at specific positions (sites) in the protein. In protein engineering, we typically consider modifications relative to an initial wild type sequence, \(_{}\). We will assume that the 3D structure for the initial sequence, \(s\), is available (either experimentally determined or provided by a structure predictor like AlphaFold ). Lastly, for variant \(\) with mutations at sites \(M\{1,...,L\}\), let \(^{m}\) denote the variant which has the same mutation as \(\) at site \(m\) for \(m M\) and otherwise is equal to \(_{}\).

### Gaussian processes

To predict protein variant effects, we rely on Gaussian process regression, which we shall now briefly introduce. For a comprehensive overview, see , which this section is based on.

Let \(\) and \(\) be two random variables on the measurable spaces \(\) and \(\), respectively, and let \(X=_{1},...,_{N}\) and \(=y_{1},...,y_{N}\) be realizations of these random variables. We assume that \(y_{i}=g(_{i})+\), where \(g\) represents some unknown function and \((0,_{}^{2})\) accounts for random noise. Our objective is to model the distributions capturing our belief about \(g\).

Gaussian processes are stochastic processes providing a powerful framework for modeling distributions over functions. The Gaussian process framework allows us to not only make predictions but also to quantify the uncertainty associated with each prediction. A Gaussian process is entirely specified by its _mean_ and _covariance_ functions, \(m()\) and \(k(,^{})\). We assume that the covariance matrix, \(K\), of the outputs \(\{y_{1},...,y_{N}\}\) can be parameterized by a function of their inputs \(\{_{1},...,_{N}\}\). The parameterization is defined by the kernel, \(k:\) yielding \(K\) such that \(K_{ij}=k(_{i},_{j})\). For \(k\) to be a valid kernel, \(K\) needs to be symmetric and positive semidefinite.

Let \(f\) represent our approximation of \(g\), \(f() g()\). Given a training set \(=(X,)\) and a number of test points \(X_{*}\), the function \(_{*}\) predicts the values of \(y_{*}\) at \(X_{*}\). Using rules of normal distributions, we derive the posterior distribution \(p(_{*}|X_{*},)\), providing both a prediction of \(y_{*}\) at \(X_{*}\) and a confidence measure, often expressed as \( 2\), where \(\) is the posterior standard deviation at a test point \(_{*}\).

The kernel function often contains hyperparameters, \(\). These can be optimized by maximizing the marginal likelihood, \(p(|X,)\), which is known as type II maximum likelihood.

### Kermut

It has been shown that local structural dependencies are useful for determining mutation preferences [57; 60; 61; 63]. We therefore hypothesize that constructing a composite kernel with components incorporating information about the local structural environments of residues will be able to model protein variant effects. To this end, we define a structure kernel, \(k_{}\), which models mutation similarity given the local environments of mutated sites. A schematic of how the structure kernel models covariances can be seen in Figure 1. In the following we shall define \(k_{}^{1}\), a structure kernel that operating on single-mutant variants. Subsequently, we shall extend it to multi-mutant variants resulting in the structure kernel, \(k_{}\).

We hypothesize that for a given site in a protein, the distribution over amino acids given by a structure-conditioned inverse folding model will reflect the effect of a mutation at that site. We consider such an amino acid distribution a representation of the _local environment_ for that site as it reflects the various physicochemical and evolutionary constraints that the site is subject to. We thus presume that two sites with similar local environments will behave similarly if mutated. For instance, mutations at buried sites in the hydrophobic core of the protein will generally correlate more with each other than with surface-level mutations.

For single mutant variants we quantify site similarity using the Hellinger kernel \(k_{H}(,^{})=(-_{1}d_{H}(f_{}( ),f_{}(^{})))\), with \(_{1}>0\), where \(d_{H}\) is the Hellinger distance (see Appendix B.1). The function \(f_{}:^{1}^{20}\) takes a single-mutant sequence, \(\), as input and returns a probability distribution over the 20 naturally occurring amino acids at the mutated site in \(\) given by the inverse folding model. The Hellinger kernel will assign maximum covariance when two sites are identical. This however means that \(k_{H}\) is incapable of distinguishing between different mutations at the same site since \(d_{H}(,^{})=1\), when \(\) and \(^{}\) are mutated at the same site.

To increase flexibility and to allow intra-site comparisons, we introduce a kernel operating on the specific mutation likelihoods. We hypothesize that two variants with mutations on sites that are close in terms of the Hellinger distance will correlate further if the log-probabilities of the specific amino acids on the mutated sites are similar (i.e., the probability of the amino acid that we mutate _to_ is similar at the two sites). We incorporate this by defining \(k_{p}(,^{})=k_{}(f_{_{1}}( ),f_{_{1}}(^{}))=(-_{2}||f_{_{1}}()-f_{_{1}}(^{})||)\), where \(f_{_{1}}:^{1}\) takes a single-mutant sequence, \(\), as input and returns the log-probability (given by an inverse folding model) of the observed mutation, and where \(k_{}\) is the exponential kernel.

Finally, we hypothesize that the effect of two mutations correlate further if the sites are close in physical space. Hence, we multiply the kernel with an exponential kernel on the Euclidean distance between sites: \(k_{d}(,^{})=(-_{3}d_{e}(s_{i},s_{j}))\). Thereby, the closer two sites are physically, the more similar - and thus comparable - their local environments will be.

Taking the product of these kernel components, we get the following kernel for single-mutant variants, which assigns high covariance when two single mutant variants have mutations that have similar environments, are physically close, and have similar mutation likelihoods:

\[k_{}^{1}(,^{})= k_{H}(,^{})k_{p}(,^{})k_{d}(, ^{}),\] (1)

where the kernel has been scaled by a non-negative scalar, \(>0\).

In , the authors showed that a simple linear model operating on one-hot encoded mutations is sufficient to accurately predict mutation effects given sufficient data. Thus, we generalize the kernel to multiple mutations by summing over all pairs of sites differing at \(\) and \(^{}\):

\[k_{}(,^{})=_{i M}_{j M^{ }}k_{}^{1}(^{i},^{ j})\] (2)

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

removing the structure kernel primarily leads to drops in the challenging contiguous and modulo schemes. This shows that the structure kernel is crucial for characterizing unseen sites in the wild type protein. While removing the site comparison and mutation probability kernels leads to small and medium drops in performance, we observe that removing both leads to an even larger performance drop, indicating a synergy between the two. In Table E.2, the ablation results per functional category are shown, where observe that the inclusion of the structure kernel is crucial for the increased performance in structure-related prediction tasks such as binding and stability.

### Uncertainty quantification per mutation domain

By inspecting the posterior predictive variance, we can analyze model uncertainty. To this end, we define several _mutation domains_ of interest . We designate the three split schemes from the ProteinGym benchmark as three such domains. These are examples of interpolation, where we both train and test on single mutants (1M\(\)1M). While the main benchmark only considers single mutations, some assays include additional variants with multiple mutations. We consider a number of these and define two additional interpolation domains where we train on _both_ single and double mutations (1M/2M) and test on singles and doubles, respectively. As a challenging sixth domain, we train on single mutations only and test on doubles (1M\(\)2M), constituting an extrapolation domain. For details on the multi-mutant splits, see Appendix G.

Figure 2 shows the distributions of mean predictive variances in the six domains. In the three single mutant domains, we observe that the uncertainties increase from scheme to scheme, reflecting the difficulties of the tasks and analogously the expected performance scores (Table 1). When training on both single and double mutants (1M/2M), we observe a lower uncertainty on double mutants than single mutants. For many of the multi-mutant datasets, the mutants are not uniformly sampled but often include a fixed single mutation. A possible explanation is thus that it might be more challenging to decouple the signal from a double mutation into its constituent single mutation signals. In the extrapolation setting, we observe large predictive uncertainties, as expected. One explanation of the discrepancy between the variance distributions in the multi-mutant domains might lie in the difference in target distributions between training and test sets. Figure I.1 in the appendix shows the overall target distribution of assays for the 51 considered multi-mutant datasets. The single and double mutants generally belong to different modalities, where the double mutants often lead to a loss of fitness. This shows the difficulty of predicting on domains not encountered during training. For reference, we include the results for the multi-mutant domains in Table G.1 in the appendix.

### Uncertainty calibration analysis

To clarify the relationship between model uncertainty and expected performance, we proceed with a calibration analysis. First, we perform a confidence interval-based calibration analysis , resulting in calibration curves which in the classification setting are known as reliability diagrams . The results for each dataset are obtained via five-fold cross validation, corresponding to five separately trained models for each split scheme. We select four diverse datasets as examples (Table 3), reflecting both high and low predictive performance. The mean calibration curves can be seen in Figure 2(a). For method details and results across all datasets, see Appendix J. The mean _expected calibration error_ (ECE) is shown in the bottom of each plot, where a value of zero indicates perfect calibration. Overall,

Figure 2: Distribution of predictive variances for datasets with double mutants, grouped by domain. The three first elements correspond to the three split-schemes from ProteinGym. The third and fourth correspond to training on both single and double mutants, and testing on each, respectively. For the last column, we train on single and test on double mutants, corresponding to an extrapolation setting.

the uncertainties appear to be well-calibrated both qualitatively from the curves and quantitatively from the ECEs. Even the smallest dataset (fourth row, \(N=165\)) achieves decent calibration, albeit with larger variances between folds.

While the confidence interval-based calibration curves show that we can trust the uncertainty estimates overall, they do not indicate whether we can trust individual predictions. We therefore supplement the above analysis with an error-based calibration analysis , where a well-calibrated model will have low uncertainty when the error is low. The calibration curves can be seen in Figure 2(b). We compute the per CV-fold _expected normalized calibration error_ (ENCE) and the _coefficient of variation_ (\(c_{v}\)), which quantifies the variance of the predicted uncertainties. Ideally, theENCE should be zero while the coefficient of variation should be relatively large (indicating spread-out uncertainties).

While the confidence interval-based analysis suggested that the uncertainty estimates are well-calibrated with some under-confidence, the same is not as visibly clear for the error-based calibration plots, suggesting that the expected correlation between model uncertainty and prediction error is not a given. We do however see an overall trend of increasing error with increasing uncertainty in three of the four datasets, where the curves lie close to the diagonal (as indicated by the dashed line). The second row shows poorer calibration - particularly in the modulo and contiguous schemes. The curves however remain difficult to interpret, in part due to the errorbars on both axes. To alleviate this, we compute similar metrics for all 217 datasets across the three splits, which indicate that, though varying, most calibration curves are well-behaved (see Appendix J.3).

We supplement the above calibration curves with Figures 1 to 4 in the Appendix, which show the true values plotted against the predictions. These highlight the importance of well-calibrated uncertainties and underline their role in interpreting model predictions and their trustworthiness.

#### 4.3.1 Comparison with ProteinNPT

Monte Carlo (MC) dropout  is a popular uncertainty quantification technique for deep learning models. Calibration curves for ProteinNPT with MC dropout for the same four datasets across the three split schemes can be seen in Appendix L.2 while figures showing the true values plotted against the predictions with uncertainties are shown in Appendix L.3. These indicate that employing MC dropout on a deep learning model like ProteinNPT seems to provide lower levels of calibration, providing overconfident uncertainties across assays and splits. Due to the generally low uncertainties and the resulting difference in scales, the calibration curves are often far from the diagonal. The trends in the calibration curves however show that the model errors often correlate with the uncertainties, suggesting that the model can be recalibrated to achieve decent calibration.

Other techniques for uncertain quantification in deep learning models certainly exist, and we by no means rule out that other techniques can outperform our method (see Discussion below). We note, however, that many uncertainty quantification methods will be associated with considerable computational overhead compared to the built-in capabilities of a Gaussian process.

## 5 Discussion

We have shown that a carefully constructed Gaussian process is able to reach state-of-the-art performance for supervised protein variant effect prediction while providing reasonably well-calibrated

  
**Uniprot ID** &  &  & \\  & Contig. & Mod. & Rand. & Avg. & \(N\) & \(L\) & Assay & Source \\  BLAT\_ECOLX & 0.804 & 0.826 & 0.909 & 0.846 & 4996 & 286 & Organismal fitness &  \\ PA\_I34A1 & 0.226 & 0.457 & 0.539 & 0.407 & 1820 & 716 & Organismal fitness &  \\ TCRG1\_MOUSE & 0.849 & 0.849 & 0.928 & 0.875 & 621 & 37 & Stability &  \\ OPSD\_HUMAN & 0.739 & 0.734 & 0.727 & 0.734 & 165 & 348 & Expression &  \\   

Table 3: Details and results for four diverse ProteinGym datasets used for calibration analysis. The results show the Spearman correlation for each CV-scheme and the average correlation.

uncertainty estimates. For a majority of datasets, this is achieved orders of magnitude faster than competing methods.

While the predictive performance on the substitution benchmark is an improvement over previous methods, our proposed model has its limitations. Due to the site-comparison mechanism, our model is unable to handle insertions and deletions as it only operates on a fixed structure. Additionally, as the number of mutations increases, the assumption of a fixed structure might worsen, depending on the introduced mutations, which can affect reliability as the local environments might change. An additional limitation is the GP's \((N^{3})\) scaling with dataset size. While not a major obstacle in the single mutant setting, dataset sizes can quickly grow when handling multiple mutants. The last decades have however produced a substantial literature on algorithms for scaling GPs to larger datasets , which could alleviate the issue, and we therefore believe this to be a technical rather than fundamental limitation. An additional limitation might present itself it in the multi-mutant setting, where the lack of explicit modeling of epistasis can potentially hinder extrapolation to higher-order mutants, prompting further investigation.

Well-calibrated uncertainties are crucial for protein engineering; both when relying on a Bayesian optimization routine to guide experimental design using uncertainty-dependent acquisition functions and similarly to weigh the risk versus reward for experimentally synthesizing suggested variants. We therefore encourage the community to place a greater emphasis on uncertainty quantification and calibration for protein prediction models as this will have measurable impacts in real-life applications like protein engineering - perhaps more so than increased prediction accuracy. We hope that Kermut can serve as a fruitful step in this direction.

Figure 3: Calibration curves for Kermut using different methods. Mean ECE/ENCE values (\( 2\)) are shown. Dashed line (\(x=y\)) corresponds to ideal calibration. The row order corresponds to the ordering in Table 3. (a) exhibits good calibration as indicated by curves close to the diagonal and ECE values close to zero, albeit with under-confident uncertainties in the second row. In (b), Kermut is also relatively well-calibrated, as indicated by the increasing curves, albeit with large variances along both axes. The low coefficients of variation (\(c_{v}\)) indicate similar predictive variances in each setting. Overall, Kermut achieves good calibration in most cases as a result of the designed kernel.