# Training Chain-of-Thought via Latent-Variable Inference

Du Phan Matthew D. Hoffman David Dohan Sholto Douglas Tuan Anh Le Aaron Parisi Pavel Sountso Charles Sutton Sharad Vikram Rif A. Saurous Google

Corresponding authors: {mhoffman,phandu}@google.com. The first two authors contributed equally, and their order was chosen randomly.Current affiliation: OpenAI.

###### Abstract

Large language models (LLMs) solve problems more accurately and interpretably when instructed to work out the answer step by step using a "chain-of-thought" (CoT) prompt. One can also improve LLMs' performance on a specific task by supervised fine-tuning, i.e., by using gradient ascent on some tunable parameters to maximize the average log-likelihood of correct answers from a labeled training set. Naively combining CoT with supervised tuning requires supervision not just of the correct answers, but also of detailed rationales that lead to those answers; these rationales are expensive to produce by hand. Instead, we propose a fine-tuning strategy that tries to maximize the _marginal_ log-likelihood of generating a correct answer using CoT prompting, approximately averaging over all possible rationales. The core challenge is sampling from the posterior over rationales conditioned on the correct answer; we address it using a simple Markov-chain Monte Carlo (MCMC) expectation-maximization (EM) algorithm inspired by the self-taught reasoner (STaR), memoized wake-sleep, Markovian score climbing, and persistent contrastive divergence. This algorithm also admits a novel control-variate technique that drives the variance of our gradient estimates to zero as the model improves. Applying our technique to GSM8K and the tasks in BIG-Bench Hard, we find that this MCMC-EM fine-tuning technique typically improves the model's accuracy on held-out examples more than STaR or prompt-tuning with or without CoT.

## 1 Introduction

For many mathematical, logical, and common-sense reasoning problems, large language models solve problems more accurately when instructed to work out the answer step by step in a _chain of thought_ or a _scratchpad_(Wei et al., 2022; Nye et al., 2021; Kojima et al., 2022; Rajani et al., 2019; Shwartz et al., 2020). These methods encourage the model to produce a _rationale_, that is, text describing a sequence of reasoning steps that leads to an answer; the motivation is that it seems to be easier for the model to generate a sequence of correct reasoning steps than to generate the final answer directly. Because of the striking performance of chain-of-thought methods, many variants have been proposed (Wang et al., 2022; Zhou et al., 2022; Creswell et al., 2022; Ye & Durrett, 2023), but there are still many cases in which the rationales are incorrect.

One way to improve these methods is to fine-tune models to generate better rationales. If gold-standard rationales can be obtained, such as via crowdsourcing (Rajani et al., 2019) or automatically (Nye et al., 2021), then supervised methods can be applied, but obtaining this data can be difficult. An appealing alternative is to start from datasets that contain questions and correct answers only, which are more readily available, and _bootstrap_ rationales during learning. A version of this strategywas proposed as the self-taught reasoner (STaR) (Zelikman et al., 2022), which generates proposed rationales from an LLM, and then fine-tunes on rationales that lead to the correct answer.

In this paper, we approach the problem of bootstrapping rationales from a different conceptual direction: _chain-of-thought methods are probabilistic latent-variable models_. The LLM defines a joint probability distribution over questions, rationales, and answers; this joint distribution implies a _marginal_ distribution of answers given questions, averaging over all possible rationales weighted by their probability given the question. The problem of self-training for reasoning then becomes one of learning with incomplete data, a core task in probabilistic machine learning (Murphy, 2022) to which we can apply methods from a large and sophisticated literature.

This perspective raises a technical challenge, because computing the marginal distribution requires averaging over a vast set of potential rationales. To address this, we introduce a learning algorithm for rationale generation, which we call TRICE.3 TRICE is a simple Markov-chain Monte Carlo (MCMC) expectation-maximization (EM) algorithm combined with a novel control-variate scheme, inspired by ideas from STaR (Zelikman et al., 2022), memoized wake-sleep (Hewitt et al., 2020), Markovian score climbing (Naesseth et al., 2020), and persistent contrastive divergence (Tieleman, 2008).

This view unifies several threads of work in reasoning using LLMs: It provides an alternative interpretation of STaR as a kind of biased stochastic expectation-maximization algorithm (Nielsen, 2000) that underweights difficult examples when its rationalization process fails. Self-consistency (Wang et al., 2022) can be seen as a Monte Carlo algorithm for computing the most likely answer under the marginal distribution. Compared to self-consistency, the probabilistic learning approach of TRICE allows us to average over rationales not only at inference time, but also _at training time_. Compared to STaR, TRICE is less likely to ignore difficult examples (which stabilizes convergence and improves performance), and is also able to learn from _incorrect_ rationales as well as correct ones.

We apply our technique to the GSM8K dataset (Cobbe et al., 2021) and to the BIG-Bench Hard benchmark (Suzgun et al., 2022). We find that TRICE improves the model's performance significantly, outperforming models tuned with STaR, direct tuning with or without CoT, and even supervised fine-tuning on human-generated rationales.

## 2 Method

Given a training set of \(N\) questions \(x_{1:N}\) and answers \(y_{1:N}\), we formalize CoT tuning as optimizing a parameter vector \(\) to maximize the average marginal log-likelihood of answers given questions:

\[()_{n} p_{}(y_{n} x_ {n})=_{n}_{z}p_{}(z x_{n})p(y_{n} z,x_{ n}),\] (1)

where \(z\) is an unobserved latent rationale, \(p_{}(z x)\) is the probability4 of obtaining the rationale \(z\) by prompting an LLM with the question \(x\) and tunable parameters \(\), and \(p_{}(y z,x)\) is the probability of obtaining the answer \(y\) given rationale \(z\), question \(x\), and parameters \(\). We will be particularly interested in models where the likelihood \(p_{}(y x,z)\{0,1\}\), that is, where the answer \(y\) is a deterministic function of \(z\). For example, we might say that the model's answer is \(y=\)"(a)" if \(z\) ends with the string "The answer is (a)." For this deterministic model, we define \(p(y z,x)=c(z,y)\{0,1\}\). Details of \(c(z,y)\) for each task can be found in Appendix F. We believe that such a binary likelihood model is appropriate for question-answering tasks where \(z\) is a rationale--a good rationale should leave no ambiguity about the correct answer. The derivations below will therefore assume a binary likelihood function. It is straightforward to generalize our methods to cases where the relationship between \(z\) and \(y\) is weaker and therefore \(p(y x,z)\) is more complicated; Appendix A shows how.

Algorithm 1 outlines the method. A notebook with a reference implementation can be found at https://github.com/google-research/cascades/tree/main/cascades/examples/notebooks/trice.ipynb.

We start by initializing a memory containing a latent rationale \(z_{n}\) for each example pair \(x_{n}\), \(y_{n}\) by sampling \(z_{n}\) from a hinted guide distribution \(q(z x_{n},y_{n})\) that may condition on the correct answer \(y_{n}\) as well as the question \(x_{n}\). For example, the guide might prompt an LLM specifically to give an rationale for the answer; more details about the precise prompts used by the guide are in Appendix F. In some cases sampling from the guide instead of the model \(p_{}(z x_{n})\) increases the chances of generating a correct rationale (Zelikman et al., 2022).

We then proceed to the main optimization loop. Each iteration, we sample a minibatch of \(M\) questions and answers from the dataset, and retrieve the rationales associated with those examples from the memory. We then propose new rationales \(\) from the current model \(p_{}(z x)\), and whenever the new rationale \(\) is correct (i.e., \(c(,y)=1\)) replace the old rationale in memory with the new one.

At this point we have all we need to compute a gradient estimate; we can just average the gradients \(_{} p_{}(z_{i_{m}} x_{i_{m}})\) that we obtain from those rationales in the updated memory that are correct (i.e., we ignore examples where both the proposed rationale and the previous rationale were wrong). basic_gradient_estimate in Algorithm 1 shows how.

But we can also reduce the variance of our gradient estimator by incorporating a control variate, as in control_variate_gradient_estimate in Algorithm 1. We first compute leave-one-out estimates \(_{1:M}\) of the average probability of accepting a new rationale. For each example \(m\), we subtract off a scaled control variate \(_{m}_{} p_{}(_{m} x_{i_{m}})\) whose expected value is zero (since it is a score function). If the proposed rationale \(_{m}\) for example \(m\) is correct, then \(z_{i_{m}}=_{m}\), and the \(m\)th gradient contribution becomes \((1-_{m})_{} p_{}(z_{i_{m}} x_{i_{m}})\), i.e., it is scaled down by \(1-_{m}\). If \(_{m}\) is incorrect, then we adjust the gradient estimate to try to make \(_{m}\)_less_ likely under \(p_{}\). As the model becomes more accurate (i.e., \(\) gets closer to 1), we give more weight to incorrect rationales (when they occur) and less weight to correct rationales (most of the time).

control_variate_gradient_estimate is more expensive than basic_gradient_estimate, since we must compute gradients not only for the rationales in memory but also for any incorrect rationales we generate. This may be wasteful, especially if many of the weights on those gradients (\(1-\) for correct proposals), \(\) for incorrect proposals) are close to zero because \(\) is close to zero or one. To reduce this cost, in subsampled_control_variate_gradient_estimate, we use systematic resampling (Hol et al., 2006) to generate a subsample of \(L\) question-rationale pairs, from which we obtain an unbiased estimate of the output of control_variate_gradient_estimate. We preferentially sample gradients with higher scalar weights; if \(\) is small, we are less likely to sample incorrect rationales (which have weight \(\)), and if \(\) is large, we are less likely to sample correct proposed rationales (which have weight \(1-\)). This can be seen as a generalization of the strategy of Burda et al. (2015, Section 3) for reducing the cost of computing IWAE gradients.

Below, we derive this variance-reduced stochastic MCMC-EM procedure in more detail.

### Derivation

The true gradient.The gradient of the marginal log-likelihood \( p_{}(y x)\) with respect to \(\) is

\[_{}_{z}p_{}(z,y x)=_{z}(z,y  x)_{} p_{}(z,y x)}{_{i^{}}p_{} (z^{},y x)}=_{z}p_{}(z x,y)_{} p_{ }(z x),\] (2)

that is, it is the expectation with respect to the posterior \(p_{}(z x,y)\) of the gradient of the conditional log-prior \( p_{}(z x)\), since the likelihood \(p(y z,x)=c(z,y)\) does not depend on \(\). So if we can sample from the posterior over rationales \(z\) conditioned on the question-answer pair \(x,y\), then we can compute an unbiased estimate of the gradient of the marginal log-likelihood \( p_{}(y x)\). We can interpret this as "bootstrapping" rationales \(z\) that are consistent with both the prior on rationales \(p_{}(z x)\) and the observed answer \(y\)(cf. Zelikman et al., 2022).

Independence sampler for \(p_{}(z x,y)\).We cannot directly sample from \(p_{}(z x,y)\), so we resort to Markov chain Monte Carlo (MCMC). We maintain a memory (cf. Hewitt et al., 2020) of a single rationale \(z_{n}\) for each question-answer pair \(x_{n},y_{n}\), and each iteration we apply a random update to \(z_{n}\) that leaves the posterior \(p_{}(z_{n} x_{n},y_{n})\) invariant (cf. Tieleman, 2008). Each MCMC update brings the \(z_{n}\)'s closer in distribution to \(p_{}(z_{n} x_{n},y_{n})\)(Cover, 1999; Murray and Salakhutdinov, 2008). However, updates to \(\) may change the posterior \(p_{}(z_{n} x_{n},y_{n})\), so we must keep updating the chains to control the bias of our gradient estimates.

To update the chains, we use a simple, hyperparameter-free independence sampler (Tierney, 1994); a Metropolis-Hastings (Hastings, 1970) update that proposes updating the current state \(z\) with a draw \(\) from a distribution \(r_{x,y}\) that does not depend on \(z\), and accepts the update with probability \(( z)=\{1,(,y x)/r_{x,y}( )}{p_{}(,y x)/r_{x,y}(z)}\}\). We choose \(r_{x,y}(z)=p_{}(z x)\), which simplifies the acceptance probability to \(( z)=\{1,(y x,)}{p_{ }(y x,z)}\}\). This is 1 if \(c(,y)=1\), 0 if \(c(,y)=0\) and \(c(z,y)=1\), and ill-defined (implying that we have to reject) if both \(c(z,y)=0\) and \(c(,y)=0\). So we accept whenever the proposal \(\) is correct, and reject otherwise.

_Remarks:_ Independence samplers can be understood as "Metropolized" importance samplers that spread the work of generating and evaluating proposals over time. In our setting, the update can also be interpreted as attempting to sample from the posterior by rejection sampling, then falling back on an old sample if that fails. The expected number of iterations between successful updates is \(p(y x)^{-1}\), so mixing will be faster for easier questions \(x\), and will accelerate as the model improves.

Basic gradient estimator.This MCMC/rejection-sampling procedure lets us approximate the gradient of the marginal log-likelihood in Equation (2). Denoting as \(z\) the state5 of the Markov chain for an example \(x,y\) before the update, we sample a proposal \(\) from \(p_{}(z x)\), accept the new state if it is correct (i.e., if \(c(,y)=1\)), and compute the gradient of the log-probability of the result:

\[z^{}=c(,y)+(1-c(,y))z;\;=_{ } p_{}(z^{} x);\;_{z,}[ ]_{p_{}(z x,y)}[_{} p_{ }(z x)],\] (3)

where \(_{z,}[]\) denotes an expectation with respect to both the proposal \(\) and the previous state \(z\).

_Remarks:_ The estimate will have low bias if the distribution of \(z^{}\) is close to the posterior \(p(z x,y)\), which we expect to be true if the chain is mixing quickly enough relative to how fast \(\) is changing. This will happen if either the probability of getting a correct answer is high, or if \(\) is changing slowly due to a small learning rate and/or gradient. If the model's training-set accuracy improves with training and we use a decaying learning-rate schedule, then as training proceeds both of these factors should work to reduce the bias of the gradient estimate.

Adding a control variate.The mean of an estimator \(\) is not affected by subtracting a zero-mean random variable \(b\) from it. If \(b\) is positively correlated with \(\), then \(-b\) can have lower variance than \(\), and we say that \(b\) can be used as a "control variate" (Owen & Zhou, 2000). Since, by the score-function identity, \(_{p_{ x}}[_{} p_{}(z x)]=0\) (for any scalar \(\) independent of \(z\)), we can use the proposed samples \(\) to generate control variates for our gradient estimator:

\[_{z,}[] =_{z}[_{}[_{} p_{ }(z^{} x)]]\] (4) \[=_{z}[_{}[_{} p_{ }(z^{} x)-_{} p_{}( x )]].\]

_Remarks:_ The value of this estimator will depend on whether or not we accept the proposal \(\):

\[_{} p_{}(z^{} x)\] (5) \[-_{} p_{}( x)\]

where we use the shorthand \( c(,y)\).

This control variate can drive the variance of the gradient estimator to zero as the model converges to perfect accuracy on the training set (cf. Roeder et al., 2017). If we set \(=\), where \(\) is the probability of a correct answer (i.e., that \(=1\)), then as \(\) gets large, most of the time \(=1\) and we multiply our gradient estimator by \(1-\) (multiplying its variance by a factor of \((1-)^{2}\)). If \(=0\), then we make use of both a correct and incorrect rationale; the weights attached to these updates will not be small, but if incorrect rationales are relatively rare then their contribution to the variance of the gradient estimator will be correspondingly small. On the other hand, if the model has not yet learned to frequently generate good rationales for the training examples, then we should set \(\) closer to 0, since in this case the signal from the incorrect rationale is less informative--in Appendix C.1 we show that the variance of gradient estimators based on incorrect rationales depends strongly on the model's accuracy \(\). In Appendix B, we show that choosing \(=\) is in fact optimal up to \(O((1-)^{2})\) terms, and that the variance of the resulting estimator is proportional to \(1-\).

Estimating \(\).For each example \(x_{m},y_{m}\), we need to compute a \(_{m}[_{m}]\) in a way that ensures that \(_{m}\) is independent of \(_{} p_{}(_{m} x_{m})\). We assume that \([_{m}]_{m}[_{m}]\) (i.e., that the per-example acceptance probability is close to the average acceptance probability across the minibatch6), and compute the leave-one-out estimate \(_{m}= m}c^{}_{m^{}}_{m^{ }}}{_{m^{} m}c^{}_{m^{}}}\), where \(c^{}_{m}:=c(z^{}_{m},y)\).

We restrict the estimate to consider only examples for which we have a correct rationale (i.e., where \(c^{}_{m}=1\)), since these are the only examples that influence our gradient estimate. Leaving out \(_{m}\) and \(c^{}_{m}\) from the estimate \(_{m}\) ensures that \(_{m}\) is independent of \(_{m}\).

Gradient subsampling.Finally, as described above, we can reduce the cost of our gradient estimator by using systematic resampling to select a subset of rationales. This does not affect the expected value of the estimator as long as the marginal probability of selecting a rationale is proportional to the corresponding weight \(_{m}\), and the averaged gradient is reweighted by \(^{2M}_{m}}{_{m}c^{}_{m}}\).

### Why not variational inference, reweighted wake-sleep, or rejection sampling?

We considered three alternatives to the MCMC-EM approach that we pursue in this paper: variational EM (e.g., Bishop, 2006), reweighted wake-sleep (RWS; Bornschein & Bengio, 2015; Le et al., 2019), and rejection sampling.

Variational expectation-maximization is a common strategy for training latent-variable models, but variational inference with discrete latent variables is challenging (e.g., Tucker et al., 2017).

RWS is an attractive alternative that avoids high-variance score-function gradients; it proceeds by sampling \(M\) samples \(z_{1:M}\) from a guide model \(q_{}(z x,y)\), assigning the samples weights \(w_{m}(y,z x)}{q_{}(z x,y)}\), and updating both the model parameters \(\) and the guide parameters \(\) to maximize the reweighted log-probabilities \(_{m}w_{m} p_{}(z_{m} x)\) and \(_{m}w_{m} q_{}(z_{m} x,y)\). Unfortunately, we found that RWS training sometimes led to degenerate zero-length rationales \(z\). Figure 1 suggests a partial explanation: shorter sequences get higher weights, so the model and guide learn to produce shorter and shorter sequences until they consistently produce empty rationales.

Why do longer sequences tend to get lower weights? We can write the unnormalized weights as \(_{m}=c(y,z_{m})(z_{m} x)}{q_{}(z_{m} x,y) }=c(y,z_{m})_{t=1}^{T_{m}}(z_{m,t} x,z_{m,1:(t-1)}) }{q_{}(z_{m,t} x,y,z_{m,1:(t-1)})}\), where \(T_{m}\) is the length of \(z_{m}\) and \(\) is added to address the case where none of the samples are correct. If there is a mismatch between \(q(z_{m,t} x,z_{m,1:(t-1)})\) and \(p(z_{m,t} x,z_{m,1:(t-1)})\), then \((z_{m,t} x,z_{m,1:(t-1)})}{q_{}(z_{m,t} x,y,z_{m,1 :(t-1)})}\) will usually be less than one, with rare high-weight exceptions that ensure that \(_{q}[p(z x)/q(z x)]=1\).

If these exceptions are rare enough to not typically appear in a sample of \(M\) sequences \(z_{1:M}\), then the normalized weights \(w_{1:M}=_{1:M}}{_{m}_{m}}\) will tend to assign higher mass to shorter sequences unless those shorter sequences are much less likely to be correct.

With careful initialization and learning-rate tuning, we could sometimes get RWS to avoid this problem of empty rationales. But this led to a new problem: the guide \(q_{}(z x,y)\) learned to closely mimic the prior \(p(z x)\) until the very end of the rationale, and then simply paste in the correct answer whether or not it had anything to do with the rationale up to that point (cf. Turpin et al., 2023). Figure 5 in Appendix E shows a representative example in which the guide model ignores the answer it arrived at through incorrect reasoning and pastes in the correct answer.

Quantitatively, denoting by \(t\) the index of the token at which the "final answer" section of the rationale begins, in one run we found that the average KL between \(q(z_{1:t} x,y)\) and \(p(z_{1:t} x)\) was about \(0.61\) nats, while the conditional KL between \(q(z_{(t+1):T} x,y,z_{1:t})\) and \(p(z_{(t+1):T} x,z_{1:t})\) was about \(42.5\) nats, confirming that the guide was not "reasoning backwards", just copying the correct answer.

Figure 1: Example of rationale lengths shrinking during RWS training. Blue line shows the average number of tokens per rationale generated by the guide, orange line shows the average number of tokens per rationale weighted by the rationale’s importance weight.

Finally, we considered a rejection-sampling7 scheme in which we sample \(K\) proposal rationales \(z_{1:K}\) from \(p(z x)\), and average the gradients from those rationales that lead to correct answers. We will present the quantitative results in Section 4; our main finding is that, while this scheme can work, it requires reducing the minibatch size by a factor of \(K\) to keep the per-iteration cost constant compared to TRICE, which in turn leads to slower convergence and/or worse final results.

## 3 Related Work

A number of methods have proposed rationale generation for problem-solving tasks in neural sequence models, including both fully supervised and few-shot approaches (Wei et al., 2022; Nye et al., 2021; Kojima et al., 2022; Rajani et al., 2019; Shwartz et al., 2020; Wang et al., 2022; Zhou et al., 2022; Creswell et al., 2022; Ye and Durrett, 2023). Particularly relevant to our approach is self-consistent chain-of-thought (Wang et al., 2022b), because this can be approximately viewed as marginalizing over rationales at test time. This technique has been successfully applied for a range of quantitative reasoning tasks (Lewkowycz et al., 2022). There is relatively much less work that does imputation or averaging over rationales at training time; perhaps the main instance is STaR (Zelikman et al., 2022), which we discuss in Section 3.1.

Dohan et al. (2022) present a position paper which advocates representing a composition of language model interactions via probabilistic programming. Our treatment of rationales as latent variables is inspired by that work. Lievin (2022) offers another example of interpreting LLMs with CoT as latent-variable models.

Variational inference (e.g., Kingma and Welling, 2013) and wake-sleep methods (e.g., Bornschein and Bengio, 2015) are workhorses of the latent-variable-modeling community, but as we discuss in Section 2.2 we found the bias of these methods to cause serious problems. MCMC-EM is a less-common strategy these days, although a version of it based on Gibbs sampling (Geman and Geman, 1984) it has been widely applied to training undirected graphical models (Tieleman, 2008). TRICE can also be cast as an instance of Markovian score climbing (Naesseth et al., 2020).

ReAct (Yao et al., 2023) demonstrated that injecting reasoning into an RL-style observe-and-act loop significantly increases performance. This approach was extended in Reflexion (Shinn et al., 2023), where an agent can conditionally reflect on an RL trajectory, augmenting the resulting examples which can be used as few-shot examples in subsequent rollouts. These approaches reported significant improvements on their respective evaluation tasks but still rely on the model being able to produce useful and actionable feedback through pure few-shot prompting, whereas our method actively tunes the model to produce thoughts amenable to the task.

Recent work on tool-use within language models also works via imputation, inferring where to insert calls to tools (Parisi et al., 2022; Schick et al., 2023). Their loss functions are similar in spirit to ours, filtering out trajectories which do not lead to valid answers. In this paper, we have treated rationales as latent variables; one could also treat tool-use as a latent variable.

### Self-Taught Reasoner

The most closely related work is the self-taught reasoner (STaR; Zelikman et al., 2022). Besides the arguments in their derivations, there are three significant differences between TRICE and STaR. First, STaR uses greedy decoding, which reduces the diversity of the rationales it trains on. The authors made this choice to reduce the danger of the model getting the right answer despite having a bad rationale. While we do find that our procedure sometimes generates correct answers for the wrong reasons, this did not seem to stand in the way of the model improving on most tasks. One reason may be that our base models are more powerful than the 6B-parameter GPT-J model used in the STaR paper, so they are more likely to generate good rationales from the beginning.

A second difference is that TRICE resamples rationales every iteration, so it are less likely to overfit to any particular rationale. STaR has an inner loop that runs many training iterations on a single set of rationales, meaning it uses stale rationales to estimate the gradient of the marginal likelihood.

In our experiments, we observed that this leads to the model effectively memorizing a fixed set of rationales for the training set. Once this happens, the greedy decoding procedure will almost certainly reproduce exactly the same rationales at the beginning of the next outer loop. If these rationales all lead to the correct answer, and STaR has a rationale for each question, then this is a global optimum of the marginal likelihood on the training set! But empirically, STaR often does not find a good rationale for each question, and so it ignores some fraction of the training set (see Section 4).

This tendency to ignore the most difficult questions in the training set follows from STaR's derivation as an approximate policy-gradient algorithm trying to directly minimize the 0-1 loss \(_{p}[1-c(z,y)]=1-p_{}(y x)\). The derivative of this marginal likelihood is \(p_{}(y x)_{} p_{}(y x)\), that is, it is the derivative of the marginal _log_-likelihood (which TRICE tries to maximize) _weighted by_\(p_{}(y x)\). This weighting causes difficult examples to contribute little to the gradient used to update the model, so the model may "give up" on questions that it cannot yet solve. This is one argument for trying to maximize log-likelihoods instead of likelihoods.

A final, minor difference is that when STaR updates its rationales, it may replace a rationale from the model \(p(z x)\) with a rationale from a surrogate \(q_{}(z x,y)\). As the model memorizes a set of correct rationales for the training set, STaR becomes less likely to fall back on the surrogate, but this choice could affect early training dynamics.

## 4 Experiments

We evaluate TRICE on the GSM8K (Cobbe et al., 2021) dataset and the 27 BigBench-Hard (BBH) tasks (Suzgun et al., 2022) using the medium-size PaLM 2-M (Anil et al., 2023) Transformer-based LLM (Vaswani et al., 2017). For the BBH experiments, we used the Flan instruction-tuned (Chung et al., 2022) version of PaLM 2; for GSM8K, we used the base PaLM 2 model, since GSM8K is included in the Flan training datasets. All experiments were run on TPU v4 and v5e chips (Jouppi et al., 2023). Examples of generated rationales can be found in Appendix E.

Rather than fine-tune the model weights, we use _prompt tuning_(Lester et al., 2021); we prepend a sequence of embedding vectors \(\) (a "soft prompt") to the embeddings corresponding to the tokenized CoT prompt used to condition the model. Prompt tuning can achieve similar accuracy gains to full fine-tuning, but using a small fraction of the parameters. We initialize the soft prompt with the embedding sequence obtained from a series of three (for BBH) or five (for GSM8K) exemplar CoT prompts, each of the form "Question: <QUESTION>nAnswer: Let's think step by step.<RATIONALE>". We consider two initialization schemes: one where we use the standard few-shot CoT prompts that are provided with BBH, and one where we try to bootstrap a few-shot CoT prompt by sampling random questions from the training set, generating random rationales from the base model, and picking three or five examples where the random rationales lead to correct answers. The first scheme can be seen as a way of fine-tuning a good initial few-shot prompt, but it does require a small amount of detailed CoT supervision, while the second scheme only requires label supervision.

On each BBH task, we split the examples into \(60\)% train and \(40\)% test sets. For all but three tasks, this is \(150\) training and \(100\) test examples. For GSM8K, we use the standard \(7473\)-example training set and \(1319\)-example test set. We evaluate CoT models' accuracy in two ways: first, using greedy (temperature-\(0\)) decoding, and second, using "self-consistency" (Wang et al., 2022). In self-consistency evaluation, we draw 40 samples and check whether the most common answer is correct; this is a plug-in estimator for the prediction \(_{y}p(y x)\) that minimizes 0-1 loss under the model (although this is not how Wang et al. (2022) originally motivated the procedure).

We compare against four baseline prompt-tuning methods: direct prompt tuning, CoT prompt tuning, rejection sampling, and STaR (Zelikman et al., 2022). All methods are evaluated against the same validation sets, and use the same training labels, few-shot prompts (except for direct tuning, where we only use question-answer pairs), and initialization strategies as appropriate. Details for each method and its corresponding experimental hyperparameters can be found in Appendix F.

Table 1 and Table 2 summarize the results; more detailed task-by-task BBH summaries are in Appendix D. Even with no human-generated exemplar rationales, TRICE is able to learn to generate rationales that lead to the correct answer. TRICE also outperforms a model trained directly on human-generated rationales on GSM8K (cf. Uesato et al., 2022), perhaps because the cross-entropy

loss used in supervised fine-tuning may place more weight on style than substance; it takes far more bits to encode how one _expresses_ a chain of reasoning than it does to encode the reasons themselves.

Initializing the soft prompt with a human-generated 3-shot exemplar question-rationale-answer prompt slightly improves performance on BBH, as does evaluating with self-consistency. By the end of training, TRICE has managed to generate at least one valid rationale for almost all training examples, while STaR fails to generate valid rationales for almost 10% of training examples. Unlike in the experiments done on Commonsense QA (Talmor et al., 2019) by Zelikman et al. (2022), STaR does not outperform the direct-prompted prompt-tuned model on BBH. This may be because each BBH task includes relatively little training data (150 examples as opposed to CommonsenseQA's 9,741), and so in its inner loop STaR overfits to its relatively small set of bootstrapped rationales. TRICE, on the other hand, can overfit to the small set of _questions_ but at least has a chance to generate a somewhat diverse set of _rationales_ from those questions.

One piece of evidence for this overfitting-rationales hypothesis is that on the final step of its final inner loop, STaR (with bootstrapped initialization) achieves a training sequence-level (_not_ per-token) cross-entropy loss of less than 0.06 on all tasks, and of less than 0.01 on 19 out of 27 tasks. This implies that it has learned to exactly reproduce a single set of rationales with very high probability, which makes it very likely that it will generate those same rationales in the next iteration.

Figure 2 compares estimates for GSM8K of the average training marginal likelihood (i.e., how often a proposal is accepted) and the validation accuracy with greedy decoding as a function of number of

   Prompt-Tuning &  & Greedy-Decoding Acc. (\%) & Self-Consistency Acc. (\%) & \% Valid Rationales \\  STaR &  & 62.0 & 62.1 & 91.6 \\  & & 64.6 & 65.3 & - \\  Direct Prompt Tuning &  & 67.8 & 68.0 & 98.7 \\  & & **72.8** & **73.1** & **98.8** \\  Direct Prompt Tuning &  & 70.4 & - & - \\  TRICE without CV &  & 73.4 & 75.2 & 98.2 \\  & & **76.7** & **77.6** & **98.6** \\   

Table 1: Average accuracies (columns 3 and 4) and fraction of training examples for which we can generate correct rationales (column 5) across the 27 BIG-Bench Hard (BBH) tasks. All methods but direct prompt tuning use CoT prompting. All trainable prompts are initialized with an embedding sequence obtained from a few-shot prompt containing either example question-answer pairs (“Q-A”) or example question-rationale-answer triples (“Q-R-A”). For direct prompt tuning, the Q-A pairs come from the training set. For TRICE, we use either the three Q-R-A triples provided with BBH (bottom two rows) or bootstrap a set of rationales as described in the text. For STaR and rejection sampling, we only evaluate on bootstrapped initializations.

   Prompt-Tuning & Greedy-Decoding Acc. (\%) & Self-Consistency Acc. (\%) & \% Valid Rationales \\  STaR & 53.5 & 60.1 & 80.2 \\ CoT Prompt Tuning & 58.6 & 73.8 & - \\ Rejection Sampling & **77.9** & **87.0** & - \\ Direct Prompt Tuning & 19.4 & - & - \\ TRICE without CV & 72.8 & 81.5 & **98.9** \\ TRICE with CV & 74.7 & 82.3 & 98.8 \\ TRICE with CV (not bootstrapped) & 77.7 & 86.6 & 98.4 \\   

Table 2: Average accuracies (columns 2 and 3) and fraction of training examples for which we can generate correct rationales (column 4) on GSM8K. Direct prompt tuning is initialized with an embedding sequence obtained from a few-shot prompt containing example question-answer pairs (“Q-A”). All remaining prompt-tuning methods are initialized with an embedding sequence obtained from a few-shot prompt containing example question-rationale-answer triples (“Q-R-A”) obtained randomly from the GSM8K training set or bootstrapped as described in the text.

training steps8 for rejection sampling and for TRICE with and without the control-variate scheme. The control-variate scheme improves average convergence speed, particularly towards the end of training as the probability of generating correct answers on the training set increases. Both versions of TRICE converge to high training accuracy much faster than rejection sampling.

## 5 Discussion

We proposed TRICE, a method for tuning LLMs to be better at solving question-answering tasks using chain-of-thought (CoT) prompting. By framing the CoT-prompted LLM as a latent-variable model, we were able to derive a principled and effective fine-tuning method. When applied to GSM8K and BIG-Bench Hard (BBH) tasks, TRICE outperforms three strong baselines: direct prompt-tuning, STaR, and rejection sampling. While we derived TRICE in the context of CoT question-answering, its basic MCMC-EM strategy could be employed more broadly, for example to tool-use problems.

Limitations:We only evaluated TRICE with prompt-tuning on a medium-size LLM; it may be that it behaves differently on smaller models, larger models, or when using other fine-tuning strategies. TRICE is a gradient-based tuning algorithm, but many of the most capable LLMs are proprietary, and their owners often do not provide any public mechanism for gradient-based fine-tuning. This makes it hard to evaluate how well TRICE would work when used with, say, GPT-4 (OpenAI, 2023). Finally, our quantitative evaluations focused on whether the LLM could produce the right answer; we did not formally evaluate the quality of the reasoning in the rationales themselves (cf. Uesato et al., 2022).

Broader Impacts:This work aims to improve the capabilities of LLMs by making them better able to answer questions accurately and transparently. However, more-capable LLMs may be used in malicious or unsafe ways, fine-tuning on uncurated question-answering datasets may introduce biases into the models, and more widely used LLMs will contribute a larger carbon footprint.

Rationales may make it easier for motivated users to judge the trustworthiness of LLM outputs. But many users may not read and critique an LLM's rationales, taking the mere existence of a rationale as evidence of truth. If chain-of-thought rationales promote uncritical trust, they could lead to harm.

Figure 2: Time-varying estimates (with loess smoothers) of average training-set accuracy \(p(y x)\) and greedy-decoding validation-set accuracy for TRICE with and without subsampled control-variate gradient estimator (“TRICE CV” and “TRICE no CV” respectively) and four-particle rejection sampling (“RS”) on GSM8K.

Acknowledgements:We appreciate Daniel Freeman and Enrique Piqueras' contributions to the infrastructure that we used in our experiments. We thank Kevin Murphy, Ben Lee, Brian Patton, and Jascha Sohl-Dickstein for helpful discussions.