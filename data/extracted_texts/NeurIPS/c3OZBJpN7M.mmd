# Proof:

FedGMKD: An Efficient Prototype Federated Learning Framework through Knowledge Distillation and Discrepancy-Aware Aggregation

 Jianqiao Zhang

Department of Computer Science

Aberystwyth University

Aberystwyth, UK

jiz20@aber.ac.uk

&Caifeng Shan

School of Intelligence Science and Technology

Nanjing University

Nanjing, China

caifeng.shan@gmail.com

&Jungong Han

Department of Automation

Tsinghua University

Beijing, China

junonghan77@gmail.com

Joint corresponding authors.

###### Abstract

Federated Learning (FL) faces significant challenges due to data heterogeneity across distributed clients. To address this, we propose FedGMKD, a novel framework that combines knowledge distillation and differential aggregation for efficient prototype-based personalized FL without the need for public datasets or server-side generative models. FedGMKD introduces Cluster Knowledge Fusion, utilizing Gaussian Mixture Models to generate prototype features and soft predictions on the client side, enabling effective knowledge distillation while preserving data privacy. Additionally, we implement a Discrepancy-Aware Aggregation Technique that weights client contributions based on data quality and quantity, enhancing the global model's generalization across diverse client distributions. Theoretical analysis confirms the convergence of FedGMKD. Extensive experiments on benchmark datasets, including SVHN, CIFAR-10, and CIFAR-100, demonstrate that FedGMKD outperforms state-of-the-art methods, significantly improving both local and global accuracy in Non-IID data settings.

## 1 Introduction

Federated Learning (FL) is a transformative approach to collaborative machine learning that allows multiple participants to train a shared model while maintaining data privacy by keeping datasets decentralized. This architecture mitigates privacy risks, particularly in sensitive domains such as healthcare and finance. FL enables clients to contribute to the global model without transferring raw data, reducing communication overhead while ensuring privacy. However, the Non-IID (non-independent and identically distributed) nature of client data poses challenges for model convergence and consistency. Heterogeneous data leads to divergence in model updates, slowing convergence and degrading performance across clients.

Personalized Federated Learning (pFL) has emerged as a solution to tailor models to individual client data distributions, improving local accuracy while maintaining global robustness.

However, traditional pFL approaches relying on knowledge distillation (KD) often require public datasets, raising privacy concerns and complicating implementation[1; 9]. Straggler inefficiencies in traditional aggregation methods also delay convergence, highlighting the need for more effective pFL frameworks[29; 40].

To address these challenges, we propose FedGMKD, a novel framework that integrates Cluster Knowledge Fusion (CKF) and a Discrepancy-Aware Aggregation Technique (DAT) to enhance both local and global model performance. Our approach introduces several key innovations: first, by employing Gaussian Mixture Models (GMM), we generate prototype features and soft predictions for each class at the client side, which are aggregated at the server without the need for public datasets, thus preserving data privacy and effectively addressing the Non-IID problem. Second, DAT adjusts the aggregation process by weighting prototype features and soft predictions based on both the quality and the quantity, rather than the quantity, of client data, thus enhancing the global model's ability to generalize across diverse client data distributions. Furthermore, we conduct a rigorous theoretical analysis of the convergence and convergence rate of FedGMKD, providing strong guarantees for its effectiveness. Finally, experimental results on multiple benchmark datasets demonstrate that FedGMKD not only achieves state-of-the-art accuracy in local and global models but also significantly improves efficiency, making it highly effective in addressing the heterogeneity challenge while maintaining computational feasibility.

## 2 Related Work

### Heterogeneity in Federated Learning

FL techniques have evolved significantly to address the challenges posed by the non-IID nature of data across diverse clients. Methods such as FedProx , Scaffold , and FedAlign  have substantially advanced the reduction of client-side bias by refining local updates to achieve better alignment with the global model. Nevertheless, these methods often rely on simplified assumptions about the underlying data distributions, which may prove inadequate in handling extreme variations in data heterogeneity, such as severe data imbalance and non-overlapping feature spaces. On the server side, solutions like FedOpt  and Agnostic Federated Learning  attempt to dynamically adjust aggregation strategies based on client contributions. However, despite these innovations, they continue to face significant challenges with scalability and often incur high computational costs, which limit their applicability in large-scale, highly diverse networks [20; 33; 38]. Moreover, approaches such as FedMix , which introduce synthetic data generation to approximate IID conditions, still struggle to accurately replicate the complex and highly variable data distributions encountered in real-world scenarios, particularly in non-IID environments.

### Personalized Federated Learning

Personalization in federated learning has emerged as a crucial strategy to cater to diverse client-specific data characteristics. Strategies range from adapting global models for local refinement (e.g., FedPer  and Per-FedAvg ) to fully personalized approaches like MOCHA , pFedMe , and Ditto . These personalized models often perform well on local data but can diverge significantly from the global model, leading to challenges in maintaining a cohesive learning strategy across the network. Additionally, the resource demands for training individualized models can be prohibitive, especially in scenarios with limited computational and bandwidth resources [7; 25].

### Knowledge Distillation and Prototype Learning in FL

Knowledge distillation and prototype learning have made notable contributions to federated learning by enabling more efficient model training and facilitating knowledge transfer across distributed networks. KD-based approaches, such as FedMD  and FedDF , allow for the compression of model knowledge into more efficient representations, thereby reducing communication costs and enhancing scalability. However, these methods necessitate meticulous tuning of distillation parameters to achieve a balance between model complexity and performance . FedProto  introduces prototype learning by aggregating class prototypes from clients to improve generalization across heterogeneous datasets. However, while FedProto improves local validation accuracy, it shows minimal improvement in global performance. As an extension of FedProto, FedHKD  integratesknowledge distillation by introducing the concept of "hyper-knowledge", which involves sending the mean representations of both feature and soft predictions from clients to the server for global aggregation. While FedHKD significantly enhances client-side personalized model performance without relying on public datasets , it focuses mainly on improving the client models. It does not introduce substantial advancements in hyper-knowledge extraction techniques, nor does it achieve dual improvement for both client and server performance in theory, as knowledge distillation predominantly benefits the client-side models. Building upon FedHKD, our research addresses these limitations by not only improving the knowledge distillation process by prototype feature and soft predictions extraction but also benefiting the server-side models by using DAT. This ensures simultaneous improvement in both global model performance and client models.

## 3 Methodology

### Problem Formulation

In the context of FL, we consider a scenario involving \(n\) distinct clients, each holding a private dataset \(\{_{1},_{2},,_{n}\}\), where each dataset reflects a potentially unique subset of the overall class distributions. This setup illustrates the fundamental Non-IID data challenge in FL, where the heterogeneity of each client's dataset \(_{i}\) complicates the task of learning a generalized global model. Each client optimizes a local model \(_{i}\) by minimizing a local loss function \(F_{i}\), which is typically expressed as the empirical risk over its dataset \(_{i}\):

\[F_{i}(_{i})=_{i}|}_{x_{i}} (_{i},x),\] (1)

where \(\) denotes the loss function measuring the prediction error on an instance \(x\).

To combine these local models into a global model, the FedAvg algorithm, introduced by McMahan et al. , applies a weighted averaging scheme based on the size of each client's dataset:

\[^{r}=_{i=1}^{n}|_{i}|_{i}^{r},\] (2)

where \(^{r}\) represents the parameters of the global model after the \(r\)-th aggregation round, \(_{i}^{r}\) denotes the parameters of the \(i\)-th client's model in the \(r\)-th round, and \(N\) is the total number of samples across all clients, \(N=_{i=1}^{n}|_{i}|\). Our objective with FedGMKD is to tailor personalized models \(_{i}\) for each client \(i\) that not only achieve high accuracy on locally specific data but also enhance a global model \(\) that excels across diverse client distributions. This task becomes particularly challenging in the presence of heterogeneous data, as naive aggregation approaches tend to diminish the effectiveness of the global model. **FedHKD** has demonstrated significant success in addressing this issue by using hyper-knowledge distillation, which improves both global and personalized models through the exchange of mean representations and soft predictions. _Inspired by_ this approach, **FedGMKD** enhances model performance on both the client and server sides by introducing Cluster Knowledge Fusion (CKF) based on Gaussian Mixture Models (GMM) for prototype feature and soft predictions generation and Discrepancy-Aware Aggregation Technique (DAT) based on data quality. This dual mechanism not only improves local model performance through better feature representation but also refines the global model by leveraging high-quality aggregated knowledge across clients, thereby effectively addressing the challenges posed by non-IID data distributions in federated learning.

### Utilizing Cluster Knowledge Fusion

In Federated Learning (FL), traditional Knowledge Distillation (KD) techniques often rely on transferring knowledge from a complex central model (teacher) to simpler client models (students). This process typically requires a public dataset to generate soft labels representing the teacher's predictions in FL. Meanwhile, this approach raises significant privacy concerns and faces challenges, especially in Non-IID data environments, where the heterogeneity of client data exacerbates these issues, reducing the effectiveness of traditional KD methods in FL. To address these challenges and inspired by hyper-knowledge of FedHKD , we propose a Clustered Knowledge Fusion (CKF) approach based on Gaussian Mixture Models (GMM). CKF clusters client updates according to data similarity, generating prototype features and soft predictions for each class, which are then aggregated at the server. These aggregated results form a synthetic dataset that serves as the foundation for knowledge distillation, enabling efficient knowledge transfer between the server and clients. Unlike traditional KD approaches, CKF not only eliminates the reliance on public datasets but also leverages client-specific representations, effectively addressing the Non-IID data problem and improving overall FL performance.

In the context of federated image classification, each client \(i\) processes raw image data \(x_{i}\) through a feature extraction function \(F_{_{i}}\), producing a feature representation \(h_{i}\), and a classifier function \(C_{_{i}}\) which maps \(h_{i}\) into a soft prediction vector \(z_{i}\):

\[h_{i}=F_{_{i}}(x_{i}), z_{i}=(C_{_{i}}(h_{i}) ),\] (3)

where \(F_{_{i}}()\) and \(C_{_{i}}()\) represent the feature extractor and classification function for client \(i\), respectively. The feature vector \(h_{i}\) encodes the latent features of the input data, and \(z_{i}\) represents the soft prediction vector, indicating the probability distribution over the classes.

When the features and soft predictions have been extracted, GMM are employed to cluster client updates based on data similarity prior to aggregation.

The responsibility \(_{m}(_{i}^{j})\) indicates the probability that the data point \(_{i}^{j}\) (which can be a feature vector or a soft prediction vector) belongs to the \(m\)-th Gaussian component. It is calculated as:

\[_{m}(_{i}^{j})=(_{i}^{ j},\_^{-}m,_{m})}{_{s=1}^{M}_{s}(_{i }^{j},\_^{-}s,_{s})},\] (4)

Here, \(_{m}\) represents the mixture coefficient of the \(m\)-th Gaussian component, where \(_{m=1}^{M}_{m}=1\). The term \((_{i}^{j},\_^{-}m,_{m})\) denotes the Gaussian probability density function evaluated at \(_{i}^{j}\), with \(\_^{-}m\) being the mean vector and \(_{m}\) being the covariance matrix of the \(m\)-th Gaussian component. The denominator sums over all \(M\) Gaussian components, normalizing the responsibility values so that they sum to \(1\) across all components. These responsibility values indicate the contribution of each Gaussian component to the data point \(_{i}^{j}\), and they are later used to compute the prototype features and soft predictions by weighting the means and predictions of each Gaussian component. Using the responsibility values, the prototype features and soft predictions for class \(j\) in the local dataset of client \(i\) are calculated as follows:

\[_{i}^{j}=_{m=1}^{M}_{m}(_{i}^{j})^{-}{}_{m_{j}}, _{i}^{j}=_{m=1}^{M}_{m}(_{i}^{j})_{m _{j}},\] (5)

where \(_{i}^{j}\) denotes the prototype feature for class \(j\) at client \(i\), synthesized from the cluster knowledge, and \(_{i}^{j}\) represents the prototype soft prediction for class \(j\) at client \(i\), calculated based on the responsibility values. Here, \(\_^{-}m_{j}\) represents the mean vector of the \(m\)-th Gaussian component for class \(j\), and \(_{m_{j}}\) represents the soft prediction vector corresponding to the \(m\)-th Gaussian component.

CKF is derived by integrating these GMM-based prototype features and their corresponding prototype soft predictions. For class \(j\) in the local dataset of client \(i\), CKF is defined by combining the prototype features and their prototype soft prediction:

\[K_{i}^{j}=(_{i}^{j},_{i}^{j}).\] (6)

If there are \(j\) classes, then the full CKF of client \(i\) is:

\[K_{i}=_{j=1}^{J}(_{i}^{j},_{i}^{j})\] (7)

A flow diagram illustrating the computation of CKF is provided in Supplementary Section A.1.

### Discrepancy-Aware Aggregation Technique

The aggregation phase in FedGMKD amalgamates CKF from each client to construct a global representation of CKF for a given class \(j\) at each iteration \(r+1\). For the given class \(j\), the global CKF represented by the aggregated prototype features \(_{j}^{r+1}\) and soft predictions \(_{j}^{r+1}\). These are calculated as follows:

\[_{j}^{r+1}=_{i=1}^{n}w_{i}^{}_{i}^{j,r}, _{j}^{r+1}=_{i=1}^{n}w_{i}^{}_{i}^{j,r},\] (8)

where \(w_{i}^{}\) is the weight corresponding to client \(i\)'s contribution, reflecting both the volume and the quality of the data contributed by client \(i\).

In traditional FL approaches such as FedAvg, the server aggregates models by averaging the parameters submitted by each client. This method assumes that data across different clients are identically and independently distributed (IID). However, tin real-world applications, this assumption often fails due to Non-IID data distributions, leading to sub-optimal global models when client data varies significantly in distribution and relevance. Most existing methods primarily weight client contributions based on data volume, neglecting the quality of the data, which can further exacerbate this issue.

To address these challenges, FedGMKD introduces a Discrepancy-Aware Aggregation method that evaluates both the volume and the quality of data each client contributes. This is achieved by quantifying how well the local data aligns with the global data distribution. The aggregation process is refined by incorporating a measure of the discrepancy between local model prototype predictions and the aggregated global prototype predictions, thereby enhancing the robustness and accuracy of the federated model.

To begin, the initial weights for each client and class are calculated based on the proportion of samples for that class across all clients:

\[w_{i,j}^{}=^{j}}{_{i=1}^{n}N_{i}^{j}},\] (9)

where \(N_{i}^{j}\) is the number of samples of class \(j\) in client \(i\)'s local dataset. This ensures that the initial weight \(w_{i,j}^{}\) reflects the proportion of class \(j\) samples that client \(i\) contributes to the global dataset for that class.

After this, the final aggregation weights are adjusted based on the discrepancies between the local and global data distributions. This discrepancy is quantified using the Kullback-Leibler (KL) divergence and is balanced by the initial weight \(w_{i,j}^{}\) for each category \(j\):

\[w_{i}^{}=(w_{i,j}^{}-a d_{i}^{j}+ b)}{_{i=1}^{n}(w_{i,j}^{}-a d_{i}^{j}+ b)},\] (10)

where \(w_{i,j}^{}\) is the initial weight for class \(j\) on client \(i\), reflecting the proportion of class \(j\) samples contributed by client \(i\) to the global dataset. \(d_{i}^{j}\) represents the KL divergence between the client and server distributions for class \(j\), and \(a\) and \(b\) are adjustment parameters that control the sensitivity of the weight updates based on the discrepancies.

The KL divergence \(d_{i}^{j}\) between the client \(i\)'s local distribution and the server's global distribution for class \(j\) is calculated as follows:

\[d_{i}^{j}=D_{}(_{i}^{j}_{j})=_{i}^{j}_{i}^{j}}{_{j}},\] (11)

where \(_{i}^{j}\) and \(_{j}\) are the predicted probabilities for class \(j\) in the local client \(i\) and the global server distributions, respectively. This KL divergence measures the discrepancy between the local andglobal predictions specifically for class \(j\), allowing the model to adjust the aggregation weights based on this class-specific difference.

For each class \(j\), the global CKF is computed by aggregating the local CKF from all clients using the discrepancy-aware weights \(w_{i}^{}\). These global CKF values, represented by \(_{j}^{r+1}\) and \(_{j}^{r+1}\), encapsulate the collective knowledge from all clients for class \(j\). After calculating the global CKF for each class \(j\), the complete global CKF, denoted as \(G^{r+1}\), is constructed by taking the union of the global CKF for all classes:

\[G^{r+1}=_{j=1}^{j}(_{j}^{r+1},_{j}^{r+1} ),\] (12)

This complete global CKF \(G^{r+1}\) serves as the updated global knowledge representation, which is used to guide future iterations of model updates. A flow diagram illustrating the computation of DAT is provided in Supplementary Section A.2.

### Local Training Objective Function

After the server completes the aggregation, the updated global CKF \(G^{r+1}\) is sent to the clients selected for the next FL round to aid in their local training. For client \(i\), with dataset \(_{i}\), the local training objective integrates the empirical risk with regularization terms designed to align the local model with the global CKF. The loss function for client \(i\) is defined as:

\[ L(_{i},_{i})&= _{i}|}_{(x_{k},y_{k})_{i}}(C_{ _{i}}(F_{_{i}}(x_{k})),y_{k})\\ &+_{i}|}_{(x_{k},y_{k}) _{i}}\|F_{_{i}}(x_{k})-_{y_{k}}^{r+1}\| _{2}^{2}+_{j=1}^{n}\|}(_{j}^{r+1})}{T}-_{j}^{r+1}}{T}\|_{2}^{2}.\] (13)

where \(|_{i}|\) denotes the number of samples in the dataset owned by client \(i\), \((_{_{i}},y_{k})\) denotes the cross-entropy loss function, \(\|\|_{2}^{2}\) denotes the Euclidean norm, and \(\) and \(\) are hyper-parameters. Note that \(_{y_{k}}^{r+1}\) represents the global prototype feature for class \(j\), and \(_{j}^{r+1}\) denotes the global soft predictions for class \(j\) at iteration \(r+1\). The term \(C_{_{i}}(_{j}^{r+1})\) represents the local classifier's predictions on the global prototype feature \(_{j}^{r+1}\) and T is the temperature of KD.

The loss function consists of three terms: the empirical risk formed using predictions and ground-truth labels, and two regularization terms that utilize the global CKF. The first term is the empirical risk, represented by the cross-entropy loss function. This term encourages the local model to perform well on its own data. The second term, known as the feature alignment loss, aligns the local feature extractor with the global CKF by minimizing the squared Euclidean distance between the local feature representations \(F_{_{i}}(x_{k})\) and the globally aggregated CKF features \(_{y_{k}}^{r+1}\). This regularization encourages the local feature extractor to produce similar feature representations to the global prototype features for each corresponding class, improving consistency between the local and global models. The third term, called the knowledge alignment loss, ensures predictive consistency across federated learning by minimizing the discrepancy between the local classifier's predictions on the global prototype features and the global soft predictions for each class \(j\). Specifically, by using Euclidean distance, which is non-negative and convex, these terms effectively regularize the local models to be more consistent with the global CKF.

### Overall Framework of FedGMKD

FedGMKD integrates CKF and DAT to enhance both local and global model performance. The framework operates iteratively, as outlined in Algorithm 1. The server initializes the global model \(^{0}=(F^{0},C^{0})\), where \(F^{0}\) and \(C^{0}\) denote the parameters of the global feature extractor and classifier, respectively. The global CKF \(G^{0}\) is also initialized, comprising global prototype features and soft predictions for each class. At each global epoch, the server transmits the global model \(^{r-1}\) and global CKF \(G^{r-1}\) to the selected clients. Clients update their local models by minimizing a composite loss function comprising three components: (1) the empirical risk, representing the cross-entropy between predicted and ground truth labels; (2) the feature alignment loss, measured by the Euclidean distance between local and global prototype features; and (3) the knowledge alignment loss, measured by the Euclidean distance between local classifier outputs on global prototype features and the global soft predictions. Upon completing local training, clients compute local CKF and transmit these, along with their updated models, back to the server. The server aggregates the received CKF using DAT, which adjusts client contributions based on data volume and quality. The global CKF and model are updated accordingly. This process repeats across multiple federated learning rounds.

```
0: Distributed datasets across \(n\) clients, \(D=\{D_{1},D_{2},,D_{n}\}\); client participation rate \(\); hyper-parameters \(\) and \(\); temperature \(T\); number of global epochs \(R_{r}\).
0: Updated global model \(^{R_{r}+1}\) and personalized local models \(\{_{1}^{R_{r}+1},_{2}^{R_{r}+1},,_{i}^{R_ {r}+1}\}\).
1: Server initializes the global model \(^{0}\) and global CKF \(G^{0}\) for each class.
2:for\(r=1\) to \(R_{r}\)do
3: Server selects \(i\) clients for participation.
4: Server broadcasts \(^{r-1}\) and \(G^{r-1}\) to the selected clients.
5:for each selected client \(i\)do
6: Client \(i\) initializes local model \(_{i}^{r-1}\) from \(^{r-1}\).
7:if\(r==1\)then
8: Client \(i\) updates \(_{i}^{r}\) using Equation 1.
9: Client \(i\) computes initial CKF \(K_{i}^{r}\) using Equations 3, 4, 5, 6, and 7.
10:else
11: Client \(i\) updates \(_{i}^{r}\) using Equation 13, incorporating \(G^{r-1}\).
12: Client \(i\) computes CKF \(K_{i}^{r}\) using Equations 3, 4, 5, 6, and 7, and computes divergence \(d_{i}^{r}\) for each class using Equation 11.
13:endif
14: Client \(i\) sends \(_{i}^{r}\), \(K_{i}^{r}\), and \(d_{i}^{r}\) (if \(r>1\)) back to the server.
15:endfor
16:if\(r==1\)then
17: Server averages CKFs and models from clients to update global CKF \(G^{r+1}\) and global model \(^{r+1}\) using Equations 8, 9 and 12.
18:else
19: Server computes weights for each class of each client using Equation 10.
20: Server updates global CKF \(G^{r+1}\) using Equations 8 and 12 based on the computed weights.
21: Server updates global model \(^{r+1}\) using weighted averaging of the models.
22:endif
23: Server sends \(^{r+1}\), \(G^{r+1}\) (if \(r>1\)) back to client \(i\).
24:endfor
25:return Updated global model \(^{R_{r}+1}\) and local models \(\{_{1}^{R_{r}+1},_{2}^{R_{r}+1},,_{i}^{R_ {r}+1}\}\). ```

**Algorithm 1** FedGMKD

### Convergence Analysis

Given the Non-IID nature of data across clients in FedGMKD, we establish two theorems to describe the framework's convergence under well-defined mathematical assumptions.

### Theorem 1: FedGMKD Convergence

Under Assumptions 1-5 (A.6.1), for any client \(i\), after \(R\) global communication rounds, the expected global loss function is bounded as:

\[_{r=1}^{R}_{i=1}^{n}w_{i}^{}[\| F _{i}(_{i}^{r})\|^{2}]^{1})-F^{*}}{ R ^{2}}+^{2}+}{2},\] (14)The detailed proof is provided in A.6.3.

### Theorem 2: FedGMKD Convergence Rate

Under Assumptions 1-5 (A.6.1), for any client \(i\), after \(R\) global communication rounds, the convergence rate of the global loss function \(F()\) is bounded as follows:

\[F(^{R})-F^{*}}{R}+C_{2},\] (15)

where \(F()\) is the global loss function, \(F^{*}\) represents the lower bound of \(F()\), and \(C_{1}\) and \(C_{2}\) are constants that depend on variance \(^{2}\), Lipschitz constant \(L\), learning rate \(\), and the number of local steps.

The detailed proof is provided in A.6.3.

## 4 Experiments

### Datasets

We evaluate FedGMKD on three widely used FL benchmark datasets, selected to cover a range of task complexities, demonstrating the method's robustness and scalability.

**SVHN**: The Street View House Numbers (SVHN) dataset consists of over 600,000 labeled digit images from street scenes. It presents varied imaging conditions, such as lighting and background differences, making it useful for testing under non-IID scenarios. Despite these variations, SVHN is considered relatively simple due to its large dataset size and clear digit images.

**CIFAR-10**: CIFAR-10 comprises 60,000 32x32 color images across 10 classes, with 6,000 images per class. It is a standard benchmark for image classification in both centralized and federated learning, providing moderate complexity due to the diverse nature of the images.

**CIFAR-100**: Similar to CIFAR-10 but with 100 classes, CIFAR-100 contains 60,000 images with 600 images per class. It poses a more challenging task, given the finer granularity and increased class variability, making it especially difficult in federated learning with non-IID data.

### Models

For our experiments, we adopt the ResNet18 architecture , which has consistently demonstrated superior performance across a wide range of learning tasks, outperforming traditional Convolutional Neural Networks (CNNs). ResNet18 incorporates residual connections to effectively address the vanishing gradient issue, enabling the training of deeper networks while maintaining computational efficiency.

### Baselines

To establish a comprehensive comparison, our evaluation includes a diverse set of baselines, encompassing both well-established methods and recent advances in federated learning. These baselines include: **FedAvg**, the foundational algorithm in federated learning; **FedProx**, addressing data heterogeneity; **MOON**, focusing on model personalization; **FedMD** and **FedGen**, which utilize public datasets and generative models, respectively, to enhance performance under Non-IID conditions; **FedProto** and **PFL**, which employ prototype and clustering learning methods to handle data disparities in federated learning; and **FjORD**, which introduces an ordered dropout mechanism to enable fair and accurate federated learning under heterogeneous target distributions.

### Experimental Setting

The models were implemented and run using PyTorch  with 2 NVIDIA A100 GPUs. The Adam optimizer  was used for model training in all experiments. The learning rate was initialized to 0.001 and decreased every 10 iterations with a decay factor of 0.5, while the hyper-parameter in Adam was set to 0.5. The number of global communication rounds was set to 50, and the number of local epochs was set to 3. The size of a data batch was set to 64, and the participating rate of clients was set to 1. For all datasets (SVHN , CIFAR-10, and CIFAR-100 ), the latent dimension of data representation was set to 32.

**Hyper-parameters**: For the FedProx  algorithm, the hyper-parameter \(_{}\) was set to 0.5. For the MOON  algorithm, the proximal term's hyper-parameter \(_{}\) was set to 1. In FedGen , a Multi-Layer Perception (MLP)-based architecture with a hidden dimension of 512 was employed for the generative model. Latent, noise, and input/output dimensions were tailored to each dataset, and the generative model was trained for 5 epochs per global round. The ratio of the generative batch-size to the training batch-size was 0.5 (generative batch-size set to 32). Parameters \(_{}\) and \(_{}\) were initialized at 10 with a decay factor of 0.98 per global round. FedMD  used a regularization hyper-parameter \(_{}\) of 0.05, and the public dataset size matched the clients' local training dataset size. FedProto  had a regularization parameter \(_{}\) set to 0.05. For FPL , the regularization parameter \(_{}\) was set to 0.1, with the number of prototypes per class fixed at 10. For FjORD , the unique dropout rate \(\) was set to 0.5, and the temperature for knowledge distillation \(T_{}\) was set to 0.7, following the original paper's settings. Finally, in our proposed FedGMKD, hyper-parameters \(\) and \(\) were set to 0.6. The clustering centers were dynamically adjusted between 2-7 based on the data distribution in each category, with the parameters \(a\) and \(b\) for differential aggregation both set to 0.2, and the temperature \(T\) for knowledge distillation was set to 0.6.

### Results

   Dataset & Scheme &  &  & Avg Time (S) & Pub Data \\   & & 10 & 20 & 50 & 10 & 20 & 50 & \\  SVHN & FedAvg & 84.29 & 85.20 & 85.67 & 81.98 & 87.32 & 89.72 & 168.44 & No \\  & FedProx & 85.25 & 86.38 & 86.08 & 81.71 & 87.40 & 88.74 & 229.17 & No \\  & Moon & 84.11 & 85.43 & 85.43 & 81.95 & 86.90 & 88.97 & 358.14 & No \\  & FedGen & 85.18 & 85.10 & 84.96 & 81.96 & 86.02 & 88.52 & 205.37 & No \\  & FedMD & 85.45 & 85.90 & 86.31 & 82.04 & 87.30 & 89.91 & 611.33 & Yes \\  & FedProto & 85.58 & 86.44 & 86.85 & 81.34 & 86.97 & 89.79 & 346.13 & No \\  & FPL & 85.37 & 86.02 & 85.87 & 79.81 & 85.64 & 88.76 & 522.83 & No \\  & FjORD & 85.13 & 85.97 & 86.21 & 81.56 & 85.09 & 89.36 & 380.74 & No \\  & **FedGMKD** & **86.26** & **87.43** & **87.16** & **82.64** & **87.78** & **90.17** & 312.52 & No \\  CIFAR10 & FedAvg & 55.75 & 58.76 & 61.51 & 46.62 & 52.61 & 51.53 & 98.94 & No \\  & FedProx & 57.46 & 58.91 & 62.94 & 47.97 & 53.13 & 56.04 & 126.56 & No \\  & Moon & 58.61 & 59.12 & 62.42 & 46.89 & 50.16 & 57.29 & 221.19 & No \\  & FedGen & 59.46 & 60.17 & 61.03 & 48.09 & 51.55 & 52.62 & 122.35 & No \\  & FedMD & 60.15 & 62.05 & 63.37 & 48.32 & 53.73 & 57.69 & 410.19 & Yes \\  & FedProto & 59.77 & 62.85 & 64.98 & 48.97 & 50.88 & 57.12 & 229.40 & No \\  & FPL & 60.95 & 62.74 & 64.49 & 47.19 & 52.04 & 58.35 & 295.97 & No \\  & FjORD & 59.62 & 63.36 & 63.61 & 49.18 & 53.22 & 58.74 & 252.34 & No \\  & **FedGMKD** & **61.78** & **64.04** & **65.69** & **49.78** & **55.16** & **60.31** & 251.55 & No \\  CIFAR100 & FedAvg & 15.39 & 17.10 & 21.09 & 14.51 & 18.98 & 22.21 & 97.02 & No \\  & FedProx & 16.45 & 17.56 & 21.91 & 16.06 & 19.67 & 23.35 & 120.36 & No \\  & Moon & 15.46 & 18.03 & 21.25 & 15.19 & 18.16 & 21.37 & 201.91 & No \\  & FedGen & 14.08 & 17.05 & 19.54 & 14.88 & 19.05 & 23.16 & 148.58 & No \\  & FedMD & 13.25 & 19.03 & 21.93 & 15.96 & 19.20 & 23.75 & 482.76 & Yes \\  & FedProto & 15.70 & 18.63 & 22.50 & 15.38 & 17.13 & 18.72 & 206.12 & No \\  & FPL & 15.93 & 18.24 & 21.96 & 15.37 & 18.19 & 21.59 & 373.09 & No \\  & FjORD & 15.94 & 19.91 & 22.60 & 16.93 & 21.45 & 22.86 & 226.73 & No \\  & **FedGMKD** & **17.16** & **20.96** & **23.57** & **16.97** & **21.56** & **24.63** & 275.60 & No \\   

Table 1: Results on data partitions generated from Dirichlet distribution with the concentration parameter \(=0.5\). The number of clients is 10, 20, and 50; the clients utilize 10%, 20%, and 50% of the datasets. A single clientâ€™s averaged wall-clock time per round is measured across 2 A100 GPUs in a parallel manner. The reported local and global accuracies are the averages of the last 5 rounds.

The proposed method, FedGMKD, consistently demonstrates superior performance across the benchmark datasets, both in terms of local and global accuracy, often outperforming other federated learning methods. On the SVHN dataset, FedGMKD significantly improves both local and global test accuracies over FedAvg across different client counts. Compared to FedProto, FedGMKD exhibits notable improvements, with local accuracy gains ranging from 0.31% to 0.99%, and global accuracy improvements ranging from 0.38% to 1.3%. When compared to FPL, FedGMKD demonstrates even more substantial improvements, with local accuracy gains of 0.89% to 1.41%, and global accuracy gains of 1.41% to 2.83%. FjORD performs competitively in this dataset, showing results second only to FedGMKD, particularly in global accuracy. However, FedGMKD still outperforms FjORD by up to 1.46% in local accuracy and 2.69% in global accuracy, demonstrating its advantage in Non-IID environments.

On CIFAR-10, FedGMKD achieves substantial improvements over FedAvg, with local accuracy increases of 4.18% to 6.03% and global accuracy gains of 2.55% to 8.78%. FedGMKD also surpasses FedProto and FPL, showing gains of up to 2.01% in local accuracy and 4.28% in global accuracy compared to FedProto, and up to 1.3% in local accuracy and 3.12% in global accuracy compared to FPL. In this case, while FjORD also performs well, FedGMKD remains superior, particularly in terms of global accuracy, further showcasing the robustness of the proposed approach in handling Non-IID data. Meanwhile, on CIFAR-100, FedGMKD achieves significant gains over FedAvg, FedProx, and FPL in both local and global accuracies across all client counts. FedGMKD shows local accuracy improvements of 1.23% to 2.72% compared to FPL, and global accuracy gains of 1.6% to 3.37%. FjORD's performance in CIFAR-100, while strong in some settings, is still notably behind FedGMKD, particularly as the number of clients increases, where FedGMKD's clustering-based approach becomes increasingly effective.

In terms of computational efficiency, FedGMKD incurs a modest increase in training time compared to FedAvg but remains competitive given its substantial accuracy improvements. Compared to FedProx, FedGMKD shows comparable or slightly better training efficiency while maintaining superior accuracy. FedGMKD is also significantly more efficient than FPL, achieving better accuracy with reduced computational overhead. FjORD shows efficiency similar to FedProto but requires more time than FedGMKD on SVHN and CIFAR10. Overall, FedGMKD strikes an optimal balance between accuracy and computational requirements.The additional computational burden of FedGMKD is justified by the substantial gains in local and global accuracy through CKF and DAT. These improvements suggest that FedGMKD offers a highly effective and efficient solution for real-world federated learning scenarios, particularly in the presence of Non-IID data distributions.

## 5 Conclusion

This paper presents FedGMKD, a novel federated learning framework that addresses data heterogeneity without requiring public datasets and complex server-side models. Through Cluster Knowledge Fusion (CKF) and Discrepancy-Aware Aggregation Technique (DAT), FedGMKD achieves superior local and global accuracy across various benchmark datasets, outperforming methods like FedAvg, FedProto, and FPL. Meanwhile, theoretical convergence guarantees and experimental results validate its effectiveness. While FedGMKD introduces moderate computational overhead, the accuracy gains justify this cost. Future work will focus on improving computational efficiency and scalability to better handle large, complex datasets, enhancing its applicability to broader federated learning scenarios.