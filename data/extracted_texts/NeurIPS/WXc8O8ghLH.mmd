# Max-Margin Token Selection in Attention Mechanism

Davoud Ataee Tarzanagh

University of Pennsylvania

tarzanaq@upenn.edu

Yingcong Li  Xuechen Zhang

University of California, Rivers

{yli692,xzhan394}@ucr.edu

Samet Oymak

University of Michigan

UC Rivers

oymak@umich.edu

###### Abstract

Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model \(f()=,}()\), where \(\) is the token sequence and \((,,)\) are trainable parameters. We prove that running gradient descent on \(\), or equivalently \(\), converges in direction to a max-margin solution that separates _locally-optimal_ tokens from non-optimal ones. This clearly formalizes attention as an optimal token selection mechanism. Remarkably, our results are applicable to general data and precisely characterize _optimality_ of tokens in terms of the value embeddings \(\) and problem geometry. We also provide a broader regularization path analysis that establishes the margin maximizing nature of attention even for nonlinear prediction heads. When optimizing \(\) and \(\) simultaneously with logistic loss, we identify conditions under which the regularization paths directionally converge to their respective hard-margin SVM solutions where \(\) separates the input features based on their labels. Interestingly, the SVM formulation of \(\) is influenced by the support vector geometry of \(\). Finally, we verify our theoretical findings via numerical experiments and provide insights.

## 1 Introduction

Since its introduction in the seminal work , attention mechanism has played an influential role in advancing natural language processing, and more recently, large language models . Initially introduced for encoder-decoder RNN architectures, attention allows the decoder to focus on the most relevant parts of the input sequence, instead of relying solely on a fixed-length hidden state. Attention mechanism has taken the center stage in the transformers , where the self-attention layer - which calculates softmax similarities between input tokens - serves as the backbone of the architecture. Since their inception, transformers have revolutionized natural language processing, from models like BERT  to ChatGPT , and have also become the architecture of choice for foundation models  addressing diverse challenges in generative modeling , computer vision , and reinforcement learning .

The prominence of the attention mechanism motivates a fundamental theoretical understanding of its role in optimization and learning. While it is well-known that attention enables the model to focus on the relevant parts of the input sequence, the precise mechanism by which this is achieved is far from clear. To this end, we ask

**Q:** What are the optimization dynamics and inductive biases of the attention mechanism?

We study this question using the fundamental attention model \(f()=,(p})\). Here, \(\) is the sequence of input tokens, \(\) is the prediction head, \(\) is the trainable key-query weights, and \(\)denotes the softmax nonlinearity. For transformers, \(\) corresponds to the \(\) token or tunable prompt , whereas for RNN architectures , \(\) corresponds to the hidden state. Given training data \((Y_{i},_{i})_{i=1}^{n}\) with labels \(Y_{i}\{-1,1\}\) and inputs \(_{i}^{T d}\), we consider the empirical risk minimization with a decreasing loss function \(():\),

\[(,,)=_{i=1}^{n}(Y_{i} f( _{i})),\ \ \ \ f(_{i})=^{}_{i}^{}(_{i}^{ }).\] (1)

At a high-level, this work establishes fundamental equivalences between the optimization trajectories of (1) and hard-margin SVM problems. Our main contributions are as follows:

\(\)**Optimization geometry of attention (Sec 2):** We first show that gradient iterations of \(\) and \(\) admit a one-to-one mapping, thus we focus on optimizing \(\) without losing generality. In Theorem 3, we prove that, under proper initialization:

Gradient descent on \(\) converges in direction to a max-margin solution - namely

(ATT-SVM) - that separates locally-optimal tokens from non-optimal ones.

We call these Locally-optimal \(\)-\(\) (\(\)) directions and show that these thoroughly characterize the viable convergence directions of attention when the norm of its weights grows to infinity. We also identify conditions under which (algorithm-independent) regularization path and gradient descent path converge to Globally-optimal \(\)-\(\) (\(\)) direction in Theorems 1 and 2, respectively. A central feature of our results is precisely quantifying _optimality_ in terms of _token scores_\(_{i}=Y^{}_{t}\) where \(_{t}\) is the \(t^{}\) token of the input sequence \(\). _Locally-optimal_ tokens are those with higher scores than their nearest neighbors determined by the SVM solution. These are illustrated in Figure 1.

\(\)**Optimize attention \(\) and prediction-head \(\) jointly (Sec 3):** We study the joint problem under logistic loss function. We use regularization path analysis where (ERM) is solved under ridge constraints and we study the solution trajectory as the constraints are relaxed. Since the problem is linear in \(\), if the attention features \(_{i}^{att}=_{i}^{}(_{i}^{})\) are separable based on their labels \(Y_{i}\), \(\) would implement a max-margin classifier. Building on this, we prove that \(\) and \(\) converges to their respective max-margin solutions under proper geometric conditions (Theorem 5). Relaxing these conditions, we obtain a more general solution where margin constraints on \(\) are relaxed on the inputs whose attention features are not support vectors of \(\) (Theorem 6). Figure 3 illustrates these outcomes.

The next section introduces the preliminary concepts, Section 4 presents numerical experiments1, Section 5 discusses related literature, and Section 6 highlights limitations and future work.

### Preliminaries

Notations.For any integer \(N 1\), let \([N]:=\{1,,N\}\). We use lower-case and upper-case bold letters (e.g. \(\) and \(\)) to represent vectors and matrices, respectively. The entries of \(\) are denoted as \(_{i}\). We use \(()\) to denote the maximum singular value of \(\). We denote the minimum of two numbers \(a\), \(b\) as \(a b\), and the maximum as \(a b\). Big-O notation \(()\) hides the universal constants. Throughout, we will use \(()\) and \((,)\) to denote Objective (1) with fixed \((,)\) and \(\), respectively.

**Optimization.** Given an objective function \(:^{d}\) and an \(_{2}\)-norm bound \(R\), define the regularized solution as

Figure 1: The convergence behavior of the gradient descent on the attention weights \(\) using the logistic loss in (ERM). The arrows (——) represent trajectories from different initializations. Here, (- - -) and (- - -) denote the globally- and locally-optimal \(\)-\(\) directions (\(\), \(\)). \(\) denotes the _score_ of a token per Definition 1. Discussion is provided under Theorems 2 and 3.

\[}(R):=_{\|\| R}().\] (2)

Regularization path - the evolution of \(}(R)\) as \(R\) grows - is known to capture the spirit of gradient descent as the ridge constraint \(R\) provides a proxy for the number of gradient descent iterations. For instance,  study the implicit bias of logistic regression and rigorously connect the directional convergence of regularization path (i.e. \(_{R}}(R)/R\)) and gradient descent. For gradient descent, we assume the objective \(()\) is smooth and describe the gradient descent process as

\[(t+1)=(t)-(t)((t)),\] (3)

where \((t)\) is the stepsize at time \(t\) and \(((t))\) is the gradient of \(\) at \((t)\).

**Attention in Transformers.** Next, we will discuss the connection between our model and the attention mechanism used in transformers. Our exposition borrows from , where the authors analyze the same attention model using gradient-based techniques on specific contextual datasets.

\(\)**Self-attention** is the core building block of transformers . Given an input consisting of \(T\) tokens \(=[_{1},,_{T}]^{}^{T d}\), self-attention with key-query matrix \(^{d d}\), and value matrix \(^{d v}\), the self-attention model is defined as follows:

\[f_{}()=(^{}).\] (4)

Here, \(()\) is the softmax nonlinearity that applies row-wise on the similarity matrix \(^{}\).

\(\)**Tunable tokens: [CLS] and prompt-tuning.** In practice, we append additional tokens to the raw input features \(\): For instance, a [CLS] token is used for classification purposes  and prompt vectors can be appended for adapting a pretrained model to new tasks . Let \(^{d}\) be the tunable token ([CLS] or prompt vector) and concatenate it to \(\) to obtain \(_{}:=[^{}]^{}^{(T+1) d}\). Consider the cross-attention features obtained from \(_{}\) and \(\) given by

\[f^{}_{}()\\ f^{}_{}()=(_{}^{})=(^{}^{ })\\ (^{}).\]

The beauty of cross-attention is that it isolates the contribution of \(\) under the upper term \(f_{}()=^{}^{}(^{ })^{v}\). In this work, we use the value weights for classification, thus we set \(v=1\), and denote \(=^{d}\). This brings us to our attention model of interest:

\[f()=^{}^{}(), =^{}.\] (5)

Here, \((,,)\) are the tunable model parameters and \(\) is the key embeddings. Note that \(\) and \(\) are playing the same role within softmax, thus, it is intuitive that they exhibit similar optimization dynamics. Confirming this, the next lemma shows that gradient iterations of \(\) (after setting \(\)) and \(\) admit a one-to-one mapping.

**Lemma 1**: _Fix \(^{d}\{\}\). Let \(:^{d}\) and \(:\) be differentiable functions. On the same training data \((Y_{i},_{i})_{i=1}^{n}\), define \(}():=1/n_{i=1}^{n}(Y_{i}(_{i}^{ }(_{i})))\) and \(():=1/n_{i=1}^{n}(Y_{i}(_{i}^{} (_{i}^{})))\). Consider the gradient descent iterations on \(\) and \(\) with initial values \((0)\) and \((0)=(0)^{}/\|\|^{2}\) and stepsizes \(\) and \(/\|\|^{2}\), respectively:_

\[(t+1) =(t)-}((t)),\] \[(t+1) =(t)-\|^{2}}((t)).\]

_We have that \((t)=(t)^{}/\|\|^{2}\) for all \(t 0\)._

This lemma directly characterizes the optimization dynamics of \(\) through the dynamics of \(\), allowing us to reconstruct \(\) from \(\) using their gradient iterations. Therefore, we will fix \(\) and concentrate on optimizing \(\) in Section 2 and the joint optimization of \((,)\) in Section 3.

**Problem definition:** Throughout, \((Y_{i},_{i})_{i=1}^{n}\) denotes training dataset where \(Y_{i}\{-1,1\}\) and \(_{i}^{T d}\). We denote the key embeddings of \(_{i}\) via \(_{i}=_{i}W^{}\) and explore the training risk

\[(,)=_{i=1}^{n}(Y_{i}^{ }_{i}^{}(_{i})).\] (ERM)

Importantly, our results apply to general tuples \((Y_{i},_{i},_{i})\) and do not assume that \((_{i},_{i})\) are tied via \(\). Finally, the \(t^{th}\) tokens of \(_{i},_{i}\) are denoted by \(_{it},_{it}^{d}\), respectively, for \(t[T]\).

The highly nonlinear and nonconvex nature of the softmax operation makes the training problem in (ERM) a challenging nonconvex optimization problem for \(\), even with a fixed \(\). In the next section, we will introduce a set of assumptions to demonstrate the global and local convergence of gradient descent for margin maximization in the attention mechanism.

## 2 Global and Local Margin Maximization with Attention

In this section, we present the main results of this paper (Theorems 2 and 3) by examining the implicit bias of gradient descent on learning \(^{d}\) given a fixed choice of \(^{d}\). Notably, our results apply to general _decreasing loss functions without requiring convexity_. This generality is attributed to margin maximization arising from the exponentially-tailed nature of softmax within attention, rather than \(\). We maintain the following assumption on the loss function throughout this section.

**Assumption A (Well-behaved Loss)**: _Over any bounded interval: (1) \(:\) is strictly decreasing. (2) \(^{}\) is \(M_{0}\)-Lipschitz continuous and \(|^{}(u)| M_{1}\)._

Assumption A includes many common loss functions, including the logistic loss \((u)=(1+e^{-u})\), exponential loss \((u)=e^{-u}\), and correlation loss \((u)=-u\). Assumption A implies that \(()\) is \(L_{p}\)-smooth (see Lemma 6 in Supplementary), where

\[L_{p}:=_{i=1}^{n}(M_{0}\|\|^{2}\|\|^{2}\|_{i}\|^{4}+3M_{1}\|\|\ \|\|^{2}\|_{i}\|^{3}).\] (6)

We now introduce a convex hard-margin SVM problem that separates one token of the input sequence from the rest, jointly solved over all inputs. We will show that this problem captures the optimization properties of softmax-attention. Fix indices \(=(_{i})_{i=1}^{n}\) and consider

\[^{mm}()=_{}\|\|_{_{i}}\ ^{}(_{i_{i}}-_{i}) 1,\  1 i n.\ \ }\]

Note that existence of \(^{mm}()\) implies the separability of tokens \(\) from the others. Specifically, choosing direction \(^{mm}()\) will exactly select tokens \((_{i_{i}})_{i=1}^{n}\) at the attention output for each input sequence, that is, \(_{R}_{i}^{}(R_{i}^{ mm}())=_{i_{i}}\). We are now ready to introduce our main results that characterize the global and local convergence of the attention weights \(\) via (ATT-SVM).

### Global convergence of the attention weights \(\)

We first identify the conditions that guarantee the global convergence of gradient descent for \(\). The intuition is that, in order for attention to exhibit implicit bias, the softmax nonlinearity should be forced to select the _optimal token within each input sequence_. Fortunately, the optimal tokens that achieve the smallest training objective under decreasing loss function \(()\) have a clear definition.

**Definition 1** (Token Scores, Optimality & GM): _The score of token \(_{i}\) of input \(_{i}\) is defined as \(_{it}:=Y_{i}^{}_{it}\). The optimal tokens for input \(_{i}\) are those tokens with highest scores given by_

\[_{i}_{t[T]}_{it}.\]

_Globally-optimal max-margin (GM) direction is defined as the solution of (ATT-SVM) with optimal indices \((_{i})_{i=1}^{n}\) by \(^{mm}\)._

It is worth noting that score definition simply uses the _value embeddings_\(^{}_{it}\) of the tokens. Note that multiple tokens within an input might attain the same score, thus \(_{i}\) or \(^{mm}\) may not be unique. The theorem below provides our regularization path guarantee on the global convergence of attention.

**Theorem 1** (Regularization Path): _Suppose Assumption A on the loss function holds, and for all \(i[n]\) and \(t_{i}\), the scores obey \(_{it}<_{i_{i}}\). Then, the regularization path \(}(R)=_{\|\| R}()\) converges to the GM direction i.e. \(_{R}}(R)/R=^{mm}/\|^{mm}\|\)._

Theorem 1 shows that as the regularization strength \(R\) increases towards the ridgeless problem \(_{}()\), the optimal direction \(}(R)\) aligns more closely with the max-margin solution \(^{mm}\). Since this theorem allows for arbitrary token scores, it demonstrates that _max-margin token separation is an essential feature of the attention mechanism_. In fact, it is a corollary of Theorem 8, which applies to the generalized model \(f()=(^{}(^{}))\) and accommodates multiple optimal tokens per input. However, while regularization path analysis captures the global behavior, gradient descent lacks general global convergence guarantees. In Section 2.2, we show that due to the nonconvex landscape and softmax nonlinearity, gradient descent often converges to local optima. We first establish that when (ERM) is trained with gradient descent, the norm of the parameters will diverge. For the restrictive setting of \(n=1\), gradient descent also exhibits a global convergence guarantee.

**Assumption B**: _For all \(i[n]\) and \(t,_{i}\), the scores per Definition 1 obey \(_{it}=_{it}<_{iopt_{i}}\)._

**Theorem 2** (Global Convergence of Gradient Descent): _Suppose Assumption \(A\) on the loss function \(\) and Assumption \(B\) on the tokens' score hold. Then, the gradient descent iterates \((t+1)=(t)-((t))\) on (ERM), with the stepsize \( 1/L_{p}\) and any starting point \((0)\) satisfy \(_{t}\|(t)\|=\). If \(n=1\), we also have \(_{t}(t)/\|(t)\|=^{}/\|^{ }\|\)._

Theorem 2 shows that gradient descent will diverge in norm, and when \(n=1\), the normalized predictor \((t)/\|(t)\|\) converges towards \(^{}\), the separator of the globally optimal token. While \(n=1\) is a stringent condition, this requirement is in fact tight as discussed in Appendix E. To illustrate this theorem, we have conducted synthetic experiments. Let us first explain the setup used in Figure 1. We set \(d=3\) as the dimension, with each token having three entries \(=[x_{1},x_{2},x_{3}]\). We reserve the first two coordinates as key embeddings \(=[x_{1},x_{2},0]\) by setting \(=()\). This is what we display in our figures as token positions. Finally, in order to assign scores to the tokens we use the last coordinate by setting \(=\). This way score becomes \(Y^{}=Y x_{3}\), allowing us to assign any score (regardless of key embedding).

In Figure 1(a), the gray paths represent gradient descent trajectories from different initializations. The points \((0,0)\) and \((1,0)\) correspond to non-optimal tokens, while the point \((-0.1,1)\) represents the optimal token. Notably, gradient descent iterates with various starting points converge towards the direction of the max-margin solution \(^{}\) (depicted by - - -). Moreover, as the iteration count \(t\) increases, the inner product \((t)/\|(t)\|,^{}/\|^{ }\|\) consistently increases. Figure 1(c) also depicts the directional convergence of gradient descent from various initializations on multiple inputs, with the gray dotted line representing the separating hyperplane. These emphasize the gradual alignment between the evolving predictor and the max-margin solution throughout the optimization.

**Lemma 2**: _Suppose for all \(i[n]\) and \(t_{i}\), \(Y_{i}=1\) and \(_{it}<_{iopt_{i}}\). Also assume \(^{d d}\) is full-rank. Then \(^{}\) exists - i.e. (ATT-SVM) is feasible for optimal indices \(_{i}_{i}\)._

### Local convergence of the attention weights \(\)

Theorem 2 on the global convergence of gradient descent serves as a prelude to the general behavior of the optimization. Once we relax Assumption B by allowing for arbitrary token scores, we will show that \(\) can converge (in direction) to a locally-optimal solution. However, this locally-optimal solution is still characterized in terms of (ATT-SVM) which separates _locally-optimal_ tokens from the rest. Our theory builds on two new concepts: locally-optimal tokens and neighbors of these tokens.

**Definition 2** (SVM-Neighbor and Locally-Optimal Tokens): _Fix token indices \(=(_{i})_{i=1}^{n}\) for which (ATT-SVM) is feasible to obtain \(^{}=^{}()\). Consider tokens \(_{i}[T]\) such that \((k_{i_{i}}-k_{i})^{}^{}=1\) for all \(t_{i}\). We refer to \(_{i}\) as SVM-neighbors of \(_{i_{i}}\). Additionally, tokens with indices \(=(_{i})_{i=1}^{n}\) are called locally-optimal if for all \(i[n]\) and \(t_{i}\) scores per Definition 1 obey \(_{i_{i}}>_{it}\). Associated \(^{}\) is called a locally-optimal max-margin (LIM) direction._

To provide a basis for discussing local convergence, we provide some preliminary definitions regarding cones. For a given \(\) and a scalar \(>0\), we define \(_{}()\) as the set of vectors \(^{d}\) such that the correlation coefficient between \(\) and \(\) is at least \(1-\) :

\[_{}():=\{^{d}\;|\; }{\|\|}}{\|\|} 1- \}.\] (7)

Given \(R>0\), the intersection of \(_{}()\) and the set \(\{^{d}\;|\;|\| R\}\) is denoted as \(_{,R}()\):

\[_{,R}():=_{}()\{ ^{d}\;|\;|\| R\}.\] (8)

Figure 2: Gradient descent initialization \((0)\) inside the cone containing the locally-optimal solution \(^{}\).

Next, we demonstrate the existence of parameters \(=()>0\) and \(R>0\) such that when \(R\) is sufficiently large, there are no stationary points within \(_{,R}(^{})\). Further, the gradient descent initialized within \(_{,R}(^{})\) converges in direction to \(^{}/\|^{}\|\); refer to Figure 2 for a visualization.

**Theorem 3** (Local Convergence of Gradient Descent): _Suppose Assumption 1 on the loss function \(\) holds and assume \(=(_{i})_{i=1}^{n}\) are indices of locally-optimal tokens per Definition 2. Then, there is a constant \(=()(0,1)\) and \(R>0\) such that \(_{,R}(^{})\) does not contain any stationary points. Further, for any starting point \((0)_{,R}(^{})\), gradient descent iterates \((t+1)=(t)-((t))\) on (_ERM_) with stepsize \( 1/L_{p}\) satisfies \(_{t}\|(t)\|=\) and \(_{t}(t)/\|(t)\|=^{}/\|^{}\|\)._

To further illustrate Theorem 3, we can consider Figure 1(b) where \(n=1\) and \(T=3\). In this figure, the point \((0,0)\) represents the non-optimal tokens, while \((1,0)\) represents the locally optimal token. Additionally, the gray paths represent the trajectories of gradient descent initiated from different points. By observing the figure, we can see that gradient descent, when properly initialized, converges towards the direction of \(^{}\) (depicted by - - -). This direction of convergence effectively separates the locally optimal tokens \((1,0)\) from the non-optimal token \((0,0)\).

### Regularization paths can only converge to locally-optimal max-margin directions

An important question arises regarding whether our definition of LMM (Definition 2) encompasses all possible convergence paths of the attention mechanism when \(\|\|\). To address this, we introduce the set of LMM directions as follows:

\[^{}:=\{^{}()}{\|^{ }()\|}\ |\ \}.\]

The following theorem establishes the tightness of these directions: It demonstrates that for any candidate \(^{}\), its local regularization path within an arbitrarily small neighborhood will provably not converge in the direction of \(\).

**Theorem 4**: _Fix \(^{}\) with unit \(_{2}\) norm. Assume that token scores are distinct (namely \(_{it}_{ir}\) for \(t\)) and key embeddings \(_{it}\) are in general position (see Theorem 7). Fix arbitrary \(>0,R_{0}>0\). Define the local regularization path of \(\) as its \((,R_{0})\)-conic neighborhood:_

\[}(R)=*{arg\,min}_{_{,R_ {0}}()\|\| R}(),\ \ \ \ \ _{,R_{0}}()=}_{}() \{^{d}|\|\| R_{0}|.\] (9)

_Then, either \(_{R}\|}(R)\|<\) or \(_{R}}(R)/\|}(R)\|\). In both scenarios \(_{R}}(R)/R\)._

The result above nicely complements Theorem 3, which states that when gradient descent is initialized above a threshold (\(\|(0)\| R_{0}\)) in an LMM direction, \(\|(t)\|\) diverges but the direction converges to LMM. In contrast, Theorem 4 shows that regardless of how small the cone is (in terms of angle and norm lower bound \(\|\| R_{0}\)), the optimal solution path will not converge along \(^{}\).

## 3 Joint Convergence of Head \(\) and Attention Weights \(\)

In this section, we extend the preceding results to the general case of joint optimization of head \(\) and attention weights \(\) using a logistic loss function. To this aim, we focus on regularization path analysis, which involves solving (ERM) under ridge constraints and examining the solution trajectory as the constraints are relaxed.

**High-level intuition.** Since the prediction is linear as a function of \(\), logistic regression in \(\) can exhibit its own implicit bias to a max-margin solution. Concretely, define the attention features \(_{i}^{}=_{i}^{}(_{i})\) and define the dataset \(^{}=(Y_{i},_{i}^{})_{i=1}^{n}\). If this dataset \(^{}\) is linearly separable, then fixing \(\) and optimizing only \(\) will converge in the direction of the standard max-margin classifier

\[^{}=*{arg\,min}_{^{d}}\| \|\ \ \ \ \ \ Y_{i}^{}_{i} 1,\ \ \ \ \ 1 i n,\] (SVM)

after setting inputs to the attention features \(_{i}_{i}^{}\). This motivates a clear question:

_Under what conditions, optimizing \(\), \(\) jointly will converge to their respective max-margin solutions?_

We study this question in two steps. Loosely speaking: **(1)** We will first assume that, at the optimal tokens \(_{i_{}},i[n]\) selected by \(\), when solving (SVM) with \(_{i}_{i_{}}\), all of these tokens become support vectors of (SVM). **(2)** We will then relax this condition to uncover a more general implicit bias for \(\) that distinguish support vs non-support vectors. Throughout, we assume that the joint problem is separable and there exists \((,)\) asymptotically achieving zero training loss.

### When all attention features are support vectors

In (SVM), define _label margin_ to be \(1/\|^{}\|\). Our first insight in quantifying the joint implicit bias is that, **optimal tokens** admit a natural definition: _Those that maximize the downstream label margin when selected._ This is formalized below where we assume that: (1) Selecting the token indices \(=(_{i})_{i=1}^{n}\) from each input data achieves the largest label margin. (2) The optimality of the \(\) choice is strict in the sense that mixing other tokens will shrink the label margin in (SVM).

**Assumption C (Optimal Tokens)**: _Let \(>0\) be the label margin when solving (SVM) with \(_{i}_{i_{i}}\). There exists \(>0\) such that for all \(\), solving (SVM) with \(_{i}_{i}^{}\) results in a label margin of at most \(-_{i[n]}(1-_{i_{i}})\) where \(_{i}=(_{i})\)._

**Example:** To gain intuition, let us fix \(^{d}\) and consider the dataset obeying \(_{i1}=Y_{i}\) and \(\|_{i1}\|<\|\|\) for all \(t 2\) and all \(i[n]\). For this dataset, we can choose \(_{i}=1\), \(^{}=/\|\|^{2}\), \(=1/\|^{}\|=\|\|\) and \(=\|\|-_{i[n],t 2}\|_{i1}\|\).

**Theorem 5**: _Consider the ridge-constrained solutions \((_{r},_{R})\) of (ERM) defined as_

\[(_{r},_{R})=*{arg\,min}_{\|\| ,\|\| R}(,).\]

_Suppose there are token indices \(=(_{i})_{i=1}^{n}\) for which \(\|^{}()\|\) exists (ATT-SVM is feasible) and Assumption C holds for some \(,>0\). Then, \(_{}_{R}/R=^{}/\|^ {}\|\), where \(^{}\) is the solution of (ATT-SVM); and \(_{r}_{r}/r=^{}/\|^{}\|\), where \(^{}\) is the solution of (SVM) with \(_{i}=_{i_{i}}\)._

As further discussion, consider Figure 3(a) where we set \(n=3,T=d=2\) and \(=\). All three inputs share the point \((0,0)\) which corresponds to their non-optimal tokens. The optimal tokens (denoted by \(\)) are all support vectors of the (SVM) since \(^{}=\) is the optimal classifier direction (depicted by - - -). Because of this, \(^{}\) will separate optimal \(\) tokens from tokens at the \((0,0)\) coordinate via (ATT-SVM) and its direction is dictated by yellow and teal colored \(\)s which are the support vectors.

### General solution when selecting one token per input

Can we relax Assumption C, and if so, what is the resulting behavior? Consider the scenario where the optimal \(\) diverges to \(\) and ends up selecting one token per input. Suppose this \(\) selects some coordinates \(=(_{i})_{i=1}^{n}\). Let \([n]\) be the set of indices where the associated token \(_{i_{i}}\) is a support vector when solving (SVM). Set \(}=[n]-\). Our intuition is as follows: Even if we slightly perturb this \(\) choice and mix other tokens \(t_{i}\) over the input set \(}[n]\), since \(}\) is not support vector for (SVM), we can preserve the label margin (by only preserving the support vectors \(\)). This means that \(\) may not have to enforce _max-margin_ constraint over inputs \(i}\), instead, it suffices to just select

Figure 3: **(a)** and **(b)** Joint convergence of attention weights \(\) (——) and classifier head \(\) (——) to max-margin directions. **(c)** Averaged softmax probability evolution of optimal tokens and logistic probability evolution of output in **(a)**.

these tokens (asymptotically). This results in the following **relaxed SVM** problem:

\[^{}=_{}\|\|^{}(_{i_{_{i}}}-_{it})1& t_{i},\ i\\ 0& t_{i},\ i}.\] (10)

Here, \(^{}(_{i_{_{i}}}-_{it}) 0\) corresponds to the _selection_ idea. Building on this intuition, the following theorem captures the generalized behavior of the joint regularization path.

**Theorem 6**: _Consider the same (ERM) problem as discussed in Theorem 5. Suppose \((_{i}_{R})_{_{i}} 1\), i.e., the tokens \((_{i})_{i=1}^{n}\) are asymptotically selected. Let \(^{}\) be the solution of (SVM) with \(_{i}=_{i_{_{i}}}\) and \(\) be its set of support vector indices. Suppose Assumption C holds over \(\) i.e. having \(_{i_{_{i}}}<1\) shrinks the margin when (SVM) is only solved over \([n]\). Then, \(_{r}_{r}/r=^{}/\|^{}\|\) and \(_{R}_{R}/R=^{}/\|^{}\|\), where \(^{}\) is the solution of (10) with \((_{i})_{i=1}^{n}\) choices._

To illustrate this numerically, consider Figure 2(b) which modifies Figure 2(a) by pushing the yellow \(\) to the northern position \((0.5,1.5)\). We still have \(^{}=\) however the yellow \(\) is no longer a support vector of (SVM). Thus, \(\) solves the relaxed problem (10) which separates green and teal \(\)'s by enforcing the max-margin constraint on \(\) (which is the red direction). Instead, yellow \(\) only needs to achieve positive correlation with \(\) (unlike Figure 2(a) where it dictates the direction). We also display the direction of \(^{}\) using a gray dashed line.

We further investigate the evolution of softmax and logistic output probabilities throughout the training process of Figure 2(a), and the results are illustrated in Figure 2(c). The averaged softmax probability of optimal tokens is represented by the red curve and is calculated as \(_{i=1}^{n}_{t[T]}(_{i})_{t}\). An achievement of 1 for this probability indicates that the attention mechanism successfully selects the optimal tokens. On the other hand, the logistic probability of the output is represented by the blue curve and is determined by \(1/n_{i=1}^{n}1/(1+e^{-}_{i} f(_{i})})\). This probability also reaches a value of 1, suggesting that the inputs are correctly classified.

## 4 Experiments

**Sparsity of softmax and evolution of attention weights.** It is well known that, in practice, attention maps often exhibit sparsity and highlight salient tokens that aid inference. Our results provide a formal explanation of this when tokens are separable: Since attention selects a locally-optimal token within the input sequence and suppresses the rest, the associated attention map \(()\) will (eventually) be a sparse vector. Additionally, the sparsity should arise in tandem with the increasing norm of attention weights. We provide empirical evidence to support these findings.

_Synthetic experiments._ Figures 3(a) and 3(b) show the evolution of the largest softmax probability and attention weights over time when using either normalized gradient or a fixed stepsize \(\) for training. The dataset model follows Figure 2(c). The softmax probability shown in Figure 3(a) is defined as \(_{i=1}^{n}_{t[T]}(_{i})_{t}\). When this average probability reaches the value of 1, it means attention selects only a single token per input. The attention norm in Figure 3(b), is simply equal to \(\|\|\).

The red curves in both figures represent the normalized gradient method, which updates the model parameters \(\) using \((t+1)=(t)-((t))/\|( (t))\|\) with \(=0.1\). The blue curves correspond to gradient descent with constant learning rate given by \((t+1)=(t)-((t))\) with \(=1\). Observe that the normalized gradient method achieves a softmax probability of 1 quicker as vanilla GD suffers from vanishing gradients. This is visible in Figure 4(b) where blue norm curve levels off.

_Real experiments._ To study softmax sparsity and the evolution of attention weights throughout training, we train a vision transformer (ViT-base) model  from scratch, utilizing the CIFAR-10 dataset  for 400 epochs with fixed learning rate \(3 10^{-3}\). ViT tokenizes an image into \(16 16\) patches, thus, its softmax attention maps can be easily visualized. We examine the average attention map - associated with the [CLS] token - computed from all 12 attention heads within the model. Figure 6 provides a visual representation of the resulting attention weights (\(16 16\) grids) corresponding to the original patch locations within the image. During the initial epochs of training, the attention weights are randomly distributed and exhibit a dense pattern. However, as the training progresses, the attention map gradually becomes sparser and the attention mechanism begins to concentrate on fewer salient patches within the image that possess distinct features that aid classification. This illustrates the evolution of attention from a random initial state to a more focused and sparse representation. These salient patches highlighted by attention conceptually corresponds to the optimal tokens within our theory.

We quantify the sparsity of the attention map via a soft-sparsity measure, denoted by \(}()\) where \(\) is the softmax probability vector. The soft-sparsity is computed as the ratio of the \(_{1}\)-norm to the squared \(_{2}\)-norm, defined as \(}()=\|\|_{1}/\|\|^{2}\). \(}()\) takes values between 1 to \(T=256\) and a smaller value indicates a sparser vector. Also note that \(\|\|_{1}=_{t=1}^{T}_{t}=1\). Together with sparsity, Figure 7 also displays the Frobenius norm of the combined key-query matrix \(\) of the last attention layer over epochs. The theory suggests that the increase in sparsity is associated with the growth of attention weights - which converge directionally. The results in Figure 7 align with the theory, demonstrating the progressive sparsification of the attention map as \(\|\|_{F}\) grows.

**Transient optimization dynamics and the influence of the loss function.** Theorem 2 shows that the asymptotic direction of gradient descent is determined by \(^{}\). However, it is worth noting that transient dynamics can exhibit bias towards certain input examples and their associated optimal tokens. We illustrate this idea in Fig 5(a), which displays the trajectories of the gradients for different scores and loss functions. We consider two optimal tokens (\(\)) with scores \(_{1}=1\) and \(_{2}=C\), where \(C\) varies. For our analysis, we examine the correlation loss \((x)=-x\) and the logistic loss \((x)=(1+e^{-x})\).

In essence, as \(C\) increases, we can observe that the correlation loss \((x)=-x\) exhibits a bias towards the token with a high score, while the logistic loss is biased towards the token with a low score. The underlying reason for this behavior can be observed from the gradients of individual inputs: \(_{i}()=_{i}^{}_{i}^{}^{}()\), where \(^{}()\) represents the derivative of the softmax function and \(_{i}^{}:=^{}(Y_{i}^{}_{i}^{}(_{i}))\). Assuming that \(\) (approximately) selects the optimal tokens, this

Figure 6: Illustration of the progressive change in attention weights of the [CLS] token during training in the transformer model, using a specific input image shown in Figure 6(a).

Figure 7: Red curve is the sparsity level \(}()/T\) of the average attention map which takes values on . A sparser vector implies that few key tokens receive significantly higher attention, while the majority of the tokens receive minimal attention. Blue curve is the Frobenius norm of attention weights \(\|\|_{F}\) of the final layer. We display their evolutions over epochs.

simplifies to \(^{}_{i}^{}(_{i})\) and \(\|_{i}()\||^{}(_{i})| _{i}\). With the correlation loss, \(|^{}|=1\), resulting in \(\|_{i}()\|_{i}\), meaning that a larger score induces a larger gradient. On the other hand, the logistic loss behaves similarly to the exponential loss under separable data, i.e., \(|^{}|=e^{-x}/(1+e^{-x}) e^{-x}\). Consequently, \(\|_{i}()\|_{i}e^{-_{ i}} e^{-_{i}}\), indicating that a smaller score leads to a larger gradient. These observations explain the empirical behavior we observe.

## 5 Related Work

**Implicit Regularization.** The implicit bias of gradient descent in classification tasks involving separable data has been extensively examined by [22; 25; 26; 27; 28; 29]. These works typically use logistic loss or, more generally, exponentially-tailed losses to make connections to margin maximization. These results are also extended to non-separable data by [30; 31; 21]. Furthermore, there have been notable investigations into the implicit bias in regression problems/losses utilizing techniques such as mirror descent [32; 25; 33; 34; 35; 36]. In addition, several papers have explored the implicit bias of stochastic gradient descent [37; 38; 39; 40; 41; 42], as well as adaptive and momentum-based methods [43; 44; 45; 46]. Although there are similarities between our optimization approach for \(\) and existing works, the optimization of \(\) stands out as significantly different. Firstly, our optimization problem is nonconvex, introducing new challenges and complexities. Secondly, it necessitates the introduction of novel concepts such as locally-optimal tokens and requires a fresh analysis specifically tailored to the cones surrounding them.

**Attention Mechanism.** Transformers, introduced by , revolutionized the field of NLP and machine translation, with earlier works on self-attention by [47; 48; 49; 50]. Self-attention differs from traditional models like MLPs and CNNs by leveraging global interactions for feature representations, showing exceptional empirical performance. However, the underlying mechanisms and learning processes of the attention layer remain unknown. Recent studies such as [51; 52; 53; 54; 23] have focused on specific aspects like representing sparse functions, convex-relaxations, and expressive power. In contrast to our nonconvex (ERM),  studies self-attention with linear activation instead of softmax, while  approximates softmax using a linear operation with unit simplex constraints. Their main objective is to derive convex reformulations for ERM-based training problem. [55; 56] have developed initial results to characterize the optimization and generalization dynamics of attention.  is another closely related work where the authors analyze the same attention model (ERM) as us. Specifically, they jointly optimize \(,\) for three gradient iterations for a contextual dataset model. However, all of these works make stringent assumptions on the data, namely, tokens are tightly clusterable or can be clearly split into clear relevant and irrelevant sets. Additionally  requires assumptions on initialization and  considers a simplified attention structure where the attention matrix is not directly parameterized with respect to the input. Our work links attention models to hard-margin SVM problems and pioneers the study of gradient descent's implicit bias in these models.

## 6 Discussion

We have provided a thorough optimization-theoretic characterization of the fundamental attention model \(f()=^{}^{}()\) by formally connecting it to max-margin problems. We first established the convergence of gradient descent on \(\) (or equivalently \(\)) in isolation. We also explored joint convergence of \((,)\) via regularization path which revealed surprising implicit biases such as (10). These findings motivate several exciting avenues for future research. An immediate open problem is characterizing the (local) convergence of gradient descent for joint optimization of \((,)\). Another major direction is to extend similar analysis to study self-attention layer (4) or to allow for multiple tunable tokens (where \(\) becomes a matrix). Either setting will enrich the problem by allowing the attention to discover multiple hyperplanes to separate tokens. While our convergence guarantees apply when tokens are separable, it would be interesting to characterize the non-separable geometry by leveraging results developed for logistic regression analysis [31; 22]. Ideas from such earlier results can also be useful for characterizing the non-asymptotic/transient dynamics of how gradient descent aligns with the max-margin direction. Overall, we believe that max-margin token selection is a fundamental characteristic of attention mechanism and the theory developed in this work lays the groundwork of these future extensions.