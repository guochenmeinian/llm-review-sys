# Bounding the Invertibility of Privacy-Preserving

Instance Encoding Using Fisher Information

Kiwan Maeng

Penn State University

kvm6242@psu.edu

Equal contribution.

Chuan Guo

FAIR, Meta

chuanguo@meta.com

Sanjay Kariyappa

Georgia Institute of Technology

sanjaykariyappa@gatech.edu

Equal contribution.

Georgia Institute of Technology

GAIR, Meta & Cornell University

edsuh@meta.com

G. Edward Suh

FAIR, Meta & Cornell University

edsuh@meta.com

###### Abstract

Privacy-preserving instance encoding aims to encode raw data into feature vectors without revealing their privacy-sensitive information. When designed properly, these encodings can be used for downstream ML applications such as training and inference with limited privacy risk. However, the vast majority of existing schemes do not theoretically justify that their encoding is non-invertible, and their privacy-enhancing properties are only validated empirically against a limited set of attacks. In this paper, we propose a theoretically-principled measure for the invertibility of instance encoding based on Fisher information that is broadly applicable to a wide range of popular encoders. We show that dFL can be used to bound the invertibility of encodings both theoretically and empirically, providing an intuitive interpretation of the privacy of instance encoding.

## 1 Introduction

Machine learning (ML) applications often require access to privacy-sensitive data. A model that predicts diseases with x-ray scans requires access to patient's x-ray images. Next-word prediction models require access to user's text input that can contain sensitive information . To enable ML applications on privacy-sensitive data, _instance encoding_ (; Figure 1, left) aims to encode data in a way such that it is possible to run useful ML tasks--such as model training and inference--on the encoded data while the privacy of the raw data is preserved. The concept is widespread under many different names: private data sharing , learnable encryption , split learning , split inference , and vertical federated learning (vFL; ) are all collaborative training/inference schemes that operate on (hopefully) privately-encoded user data.

Unfortunately, the vast majority of existing instance encoding schemes rely on heuristics rather than rigorous theoretical arguments to justify their privacy-enhancing properties. Most existing works  claimed that their schemes are non-invertible by simply testing them against certain input reconstruction attacks. However, these schemes may not be private under more carefully designed attacks; in fact, many encoding schemes that were initially thought to be private have been shown to be vulnerable over time . While there exist instance encoding schemes that are based on a rigorous theory, _e.g._, metric differential privacy (metric-DP; ), their theoretical analysis is only applicable to very simple encoders and cannot be applied to more powerful encoders (_e.g._, DNN-based encoders) that are common in practice .

To fill the gap, we propose a _theoretically-principled_ and _easily applicable_ framework for quantifying the privacy of instance encoding by bounding its invertibility. Our framework uses (diagonal) _Fisher information leakage_ (dFL; [18; 22; 20])--an information-theoretic measure of privacy with similar properties to differential privacy (DP; [14; 15]). dFL can be computed for common privacy-enhancing mechanisms and can lower-bound the expected mean squared error (MSE) of an input reconstruction attack. We apply this reasoning to instance encoding and show that dFL can serve as a good measure for the invertibility of instance encoding, as it can bound the reconstruction MSE against any attacker that tries to reconstruct the private input from the encoding. dFL can be easily calculated for any encoders that are differentiable and randomized--being applicable to most of the popular encoders with minimal modifications. To the best of our knowledge, our work is the first to theoretically lower-bound the invertibility of instance encoding and use it to design private training/inference systems.

ContributionsOur main contributions are as follows:

1. We adapt the result of prior works [18; 20] for instance encoding to show how dFL can lower bound the MSE of _particular_ input reconstruction attacks (_i.e._, _unbiased_ attacks) that aim to reconstruct the raw data given the encoding. We show how popular encoders can be modified minimally for dFL to be applied (Section 3.1).
2. We extend the result of prior works [18; 20] and show that dFL can lower bound the MSE of _any_ input reconstruction attack (_e.g._, strong attacks leveraging knowledge of the input prior; Section 3.2). Our extension involves a novel application of the classical _van Trees inequality_ and connecting it to the problem of _score matching_ in distribution estimation.
3. We evaluate the lower bound using different attacks and encoders, and show that dFL can be used to interpret the invertibility of instance encoding both theoretically and empirically (Section 3.3).
4. We show how dFL can be used as a practical privacy metric and guide the design of privacy-enhancing training/inference systems with instance encoding (Section 4-5). We show that it is possible to achieve both high (theoretically-justified) privacy and satisfactory utility.

## 2 Motivation and background

### Instance encoding

Instance encoding refers to the general concept of encoding raw input \(\) using an encoding function \(\), such that private information contained in \(\) cannot be inferred from its encoding \(=()\), while \(\) maintains enough utility for the downstream ML tasks. The principle behind the privacy-enhancing property of instance encoding is that the function \(\) is hard to invert. However, prior works generally justify this claim of non-invertibility based on heuristics rather than rigorous theoretical analysis [64; 65; 45], except for a few cases where the encoder is very simple [16; 17; 44].

Attacks against instance encodingGiven an instance encoder \(\), the goal of a reconstruction attack is to recover its input. Formally, given \(=()\), an attack \(\) aims to reconstruct \(\) from \(\): \(}=()\). Such an attack can be carried out in several ways. If \(\) is known, \(}\) can be obtained by solving the following optimization : \(}=_{0}}{}||-( _{0})||_{2}^{2}\). This attack can be further improved when some prior of the input is known [49; 61]. For instance, images can be regularized with total variation (TV) prior to reduce high-frequency components . Alternatively, if samples from the input distribution can be obtained, a DNN that generates \(}\) from \(\) can be trained [53; 27; 12].

Figure 1: Overview of instance encoding (left), and our bound against different attackers (_Attack-ub_, _Attack-b_) for a synthetic dataset (right). The bound adapted from prior work  (_Bound-ub_) only works against a certain adversary (_Attack-ub_). Our newly proposed bound (_Bound-ours_) works for any adversary. See Section 3.3.1 for evaluation details.

Privacy metrics for instance encoding_Measuring how much information an observation leaks about a secret quantity it depends on_ is a classical question that has long been studied in information theory [38; 3; 13; 33], and instance encoding privacy is directly relevant to the stream of works. Prior works proposed metrics providing a strong notion of privacy (_e.g._, Arimoto's mutual information  or Sibson's mutual information ). However, it is challenging to apply these metrics to ML and instance encoding which use high-dimensional data , because these metrics have to be calculated and optimized over the prior of the dataset, the encodings, and/or their joint distribution, all of which are unknown and hard to accurately model .

Differential privacy (DP) [14; 15], one of the most popular frameworks to quantify ML privacy , is not suitable for instance encoding . DP guarantees the worst-case indistinguishability of the encoding from two different inputs, which significantly damages the utility  (Appendix 7.5). Metric-DP --a weaker notion of DP--is applicable to instance encoding and has been used by several prior works [16; 17; 44; 48; 37]. However, metric-DP is applicable only when the encoder is extremely simple. For example, prior works used pixel-level perturbation (_e.g._, pixelization, blurring [16; 17]) or a single-layer convolution  as an encoder. These extremely-simple encoders are in stark contrast with the complex encoders most practitioners use (_e.g._, split learning , split inference , and vFL [45; 59] all uses DNN-based encoders), and results in a significant utility loss (Section 4.2). Applying metric-DP to an arbitrary DNN-based encoder is NP-hard . Unlike these works that focus on indistinguishability, our work directly quantifies _non-invertibility_, a weaker but still relevant definition of privacy (Section 3).

Due to the limitations in existing theoretical metrics, the vast majority of works in the field are relying on empirical measures to demonstrate their encoder's privacy-enhancing properties. Some [46; 45] simply test their encoders against a limited set of attacks, while others [64; 65; 51] use heuristical privacy metrics without rigorous theoretical arguments. Using heuristical metrics that are not backed by a theoretical interpretation is dangerous, because a seemingly useful metric can actually severely mis-characterize the privacy of a system . It is of both interest and practical importance to develop a privacy metric that is both theoretically-meaningful and widely applicable.

A concurrent work  proposed a notion of PAC privacy that can bound the adversary's attack success rate in theory for instance encoding. PAC privacy bound has not yet been evaluated empirically under a realistic attack and use cases, which remains an interesting future work.

### Fisher information leakage

Fisher information leakage (FIL; [18; 22; 20]) is a measure of leakage through a privacy-enhancing mechanism. Let \(\) be a randomized mechanism on data sample \(\), and let \(()\) be its output. Suppose the log density function \( p(;)\) is differentiable w.r.t. \(\) and satisfies the following regularity condition: \(_{}[._{} p(; )|]=0\). The _Fisher information matrix_ (FIM) \(_{}()\) is:

\[_{}()=_{}[_{} p(;)_{} p(;)^{ }]. \]

Cramer-Rao boundFisher information is a compelling privacy metric as it directly relates to the mean squared error (MSE) of a reconstruction adversary through the Cramer-Rao bound . In detail, suppose that \(}()\) is an _unbiased_ estimate (or reconstruction) of \(\) given the output of the randomized private mechanism \(()\). Then:

\[_{}[||}()-||_{2}^{2}/d] (_{}())}, \]

where \(d\) is the dimension of \(\) and \(\) is the trace of a matrix. Guo et al.  defined a scalar summary of the FIM called _diagonal Fisher information leakage_ (dFIL):

\[()=(_{}())/d, \]

hence the MSE of an unbiased reconstruction is lower bounded by the reciprocal of dFIL. dFIL varies with \(\), allowing it to reflect the fact that certain samples may be more vulnerable to reconstruction.

LimitationsAlthough the Cramer-Rao bound gives a mathematically rigorous interpretation of dFIL, it depends crucially on the _unbiasedness_ assumption, _i.e._, \(_{}[}()]=\). In practice, most real-world reconstruction attacks use either implicit or explicit priors about the data distribution and are _biased_ (_e.g._, attacks using TV prior or a DNN). It is unclear how dFIL should be interpreted in these more realistic settings. In Section 3.2, we give an alternative theoretical interpretation based on the van Trees inequality , which lower-bounds the MSE of _any_ adversary, biased or unbiased.

## 3 Quantifying the invertibility of an encoding

Motivated by the lack of theoretically-principled privacy metrics, we propose to adapt the Fisher information leakage framework to quantify the invertibility of instance encoding. We show that many existing encoders can be modified minimally to be interpreted with dFIL and the Cramer-Rao bound. Subsequently, we extend the framework and derive a novel bound for the reconstruction error of _arbitrary_ attacks, by establishing a connection to van Trees inequality and score matching.

Threat modelWe focus on reconstruction attacks that aim to invert an encoding, _i.e._, reconstruct the input \(\) given its encoding \(\). We assume that the attacker has full knowledge of the encoder \(\) except for the source of randomness. We consider both unbiased attacks and biased attacks that can use arbitrary prior knowledge about the data distribution.

Privacy definitionWe consider \(\) to be private if \(\) cannot be reconstructed from the encoding \(\). We measure the reconstruction error with the mean squared error (MSE), defined as \(||}-||_{2}^{2}/d\). Although MSE does not exactly indicate semantic similarity, it is widely applicable and used as a proxy for semantic similarity [69; 43]. Preventing low reconstruction MSE does not necessarily protect against other attacks (_e.g._, property inference ), which we leave as future work.

### Fisher information leakage for instance encoding

To adapt the framework of Fisher information to the setting of instance encoding, we consider the encoding function \(\) as a privacy-enhancing mechanism (_cf._\(\) in Section 2.2) and use dFIL to measure the invertibility of the encoding \(=()\). However, many instance encoders do not meet the regularity conditions in Section 2.2, making dFIL ill-defined. For example, popular DNN-based encoders do not produce randomized output, and their log density function \( p(;)\) may not be differentiable when operators like ReLU or max pooling are present.

Fortunately, many popular encoders can meet the required conditions with small changes. For example, DNN-based encoders can be modified by (1) replacing any non-smooth functions with smooth functions (_e.g._, \(\) or GELU  instead of ReLU, average pooling instead of max pooling), and (2) adding noise at the end of the encoder for randomness. Alternative modifications are also possible. In particular, if we add random Gaussian noise to the output of a differentiable, deterministic encoder \(_{D}\) (_e.g._, DNN): \(()=_{D}()+( 0,^{2})\), the FIM of the encoder becomes :

\[_{}()=}_{ _{D}}^{}()_{_{D}}( ), \]

where \(_{_{D}}\) is the Jacobian of \(_{D}\) with respect to the input \(\) and can be easily computed using a single backward pass. Then, Equation 2 can be used to bound the reconstruction error, _provided the attack is unbiased_. Other popular encoders can be modified similarly.

### Bounding the reconstruction of arbitrary attacks

Most realistic reconstruction attacks are biased, and their reconstruction MSE is not lower bounded by Equation 2. Consider an (biased) attacker who knows the mean \(\) of the input data distribution. If the attacker simply outputs \(\) as the reconstruction of any input \(\), the expected MSE will be the variance of the data distribution _regardless of dFIL_. The above example shows a crucial limitation of the Cramer-Rao bound interpretation of dFIL: it does not take into account any _prior information_ the adversary has about the data distribution, which is abundant in the real world (Section 2.1).

Bayesian interpretation of Fisher informationWe adopt a Bayesian interpretation of dFIL as the difference between an attacker's prior and posterior estimate of the input \(\). This is achieved through the classical _van Trees inequality_. We state the van Trees inequality in Appendix 7.3, and use it to derive our MSE bound for arbitrary attacks below as a corollary; proof is in Appendix 7.4.

**Corollary 1**.: _Let \(\) be the input data distribution and let \(f_{}()\) denote its density function with respect to Lebesgue measure. Suppose that \(\) satisfies the regularity conditions of van Trees inequality (Theorem 2), and let \((f_{})=_{}[_{} f_{}()_{} f_{}()^{}]\) denote the information theorist's Fisher information  of \(\). For a private mechanism \(\) and any reconstruction attack \(}()\)operating on \(()\):_

\[_{}[||}-||_{2}^{2}/d]_{}[()]+((f_{}) )/d}. \]

Implications of Corollary 1We can readily apply Corollary 1 to instance encoding by replacing \(\) with \(\) and \(\) with \(\) as in Section 3.1. Doing so leads to several interesting implications:

1. Corollary 1 is a population-level bound that takes expectation over \(\). This is necessary because given any _fixed_ sample \(\), there is always an attack \(}()=\) that perfectly reconstructs \(\) without observing the encoding \(\). Such an attack would fail in expectation over \(\).

2. The term \((f_{})\) captures the prior knowledge about the input. When \((f_{})=0\), the attacker has no prior information about \(\), and Corollary 1 reduces to the unbiased bound in Equation 2. When \((f_{})\) is large, the bound becomes small regardless of \(_{}[()]\), indicating that the attacker can simply guess with the input prior and achieve a low MSE.

3. dFIL can be interpreted as capturing how much _easier_ reconstructing the input becomes after observing the encoding (\(_{}[()]\) term) as opposed to only having knowledge of the input distribution (\(((f_{}))/d\) term). Using such a _change in the belief after an observation_ as a privacy metric is common in information theory .

Estimating \((f_{})\)The term \((f_{})\) captures the prior knowledge of the input and plays a crucial role in Corollary 1. In simple cases where \(\) is a known distribution whose density function follows a tractable form (_e.g._, when the input follows a Gaussian distribution), \((f_{})\) can be directly calculated. In such settings, Corollary 1 gives a meaningful theoretical lower bound for the reconstruction MSE. However, most real-world data distributions do not have a tractable form and \((f_{})\) must be estimated from data. Fortunately, the \(_{} f_{}()\) term in \((f_{})\) is a well-known quantity called the _score function_ in generative AI literature, and there exists a class of algorithms known as _score matching_ that aim to estimate the score function from the data. We leverage these techniques to estimate \((f_{})\) when it cannot be calculated; details are in Appendix 7.1. As \((f_{})\) only depends on the dataset and not the encoder, its value can be estimated once for each dataset and shared across the community. The estimation of \((f_{})\) is expected to improve with the advancement of generative AI.

Using Corollary 1 in practiceWhen \((f_{})\) is known (_e.g._, Gaussian), the bound from Corollary 1 always hold. However, when estimating \((f_{})\) from data, the bound can be incorrect due to several reasons, including improper modeling of the score function, not having enough representative samples, or the regularity conditions of the van Trees inequality not being met. The bound can also be loose when tightness conditions of the van Trees inequality do not hold. Even when the bound is not exact, however, Equations 2 and 5 can be interpreted to suggest that increasing \(1/\) makes reconstruction harder. Thus, we argue that dFIL still serves as a useful privacy metric that in theory bounds the invertibility of an instance encoding. When not exact, the bound should be viewed more as a _guideline_ for interpreting and setting dFIL in a data-dependent manner.

### Evaluation of the bound

We show that Corollary 1 accurately reflects the reconstruction MSE on both (1) synthetic data with known \((f_{})\), and (2) real world data with estimated \((f_{})\).

Figure 2: \(1/\) vs. reconstruction MSE (left) and sample reconstructions (right) for MNIST. The values before each row (right) indicate \(1/\) of the encoder used.

#### 3.3.1 Synthetic data with known \((f_{})\)

Evaluation setupWe consider a synthetic Gaussian input distribution: \((0,^{2}_{d})\) with \(d=784\) and \(=0.05\). It can be shown that \(((f_{}))/d=1/^{2}\), hence a larger \(\) forces the data to spread out more and reduces the input prior. We use a simple encoder which randomly projects the data to a \(10,000\)-dimensional spaces and then adds Gaussian noise, _i.e._, \(=+(0,^{2})\), where \(^{10,000 784}\). We evaluate with a simple encoder to make the attack easier, but note that our bound should hold for more complex encoders (_e.g._, DNN-based) as well.

AttacksWe evaluate our bound against two different attacks. An unbiased attack (_Attack-ub_) solves the following optimization: \(}()=*{arg\,min}_{_{0}} -(_{0})_{2}^{2}\). The attack is unbiased as the objective is convex, and \(\) is recovered in expectation. A more powerful biased attack (_Attack-b_) adds a regularizer term \( p_{}(_{0})\) to the above objective, where \(p_{}\) is the density function of \((0,^{2}_{d})\). One can show that with a suitable choice of \(\), this attack returns the _maximum a posteriori_ estimate of \(\), which leverages knowledge of the input distribution. Details are in Appendix 7.2.

ResultFigure 1 (right) plots the MSE of the two attacks, and the bounds for unbiased (Equation 2) and arbitrary attack (Equation 5). The MSE of _Attack-ub_ (red circle) matches the unbiased attack lower bound (_Bound-ub_; red dashed line), showing the predictive power of Equation 2 against this restricted class of attacks. Under _Attack-b_ (blue triangle), however, _Bound-ub_ breaks. Our new bound from Equation 5 (_Bound-ours_; blue dotted line) reliably holds for both attacks, initially being close to the unbiased bound and converging to guessing only with the input prior (attaining \(^{2}\)).

#### 3.3.2 Real world data with estimated \((f_{})\)

Evaluation setupWe also evaluated Corollary 1 on MNIST  and CIFAR-10 . Here, we estimated \((f_{})\) using sliced score matching . As discussed in Appendix 7.1, a moderate amount of randomized smoothing (adding Gaussian noise to the raw input ) is necessary to ensure that the score estimation is stable and that regularity conditions of the van Trees inequality are satisfied. We used a simple CNN-based encoder: \(=()+(0,^{2})\).

AttacksWe evaluated _Attack-ub_, which is the same as in Section 3.3.1, and _Attack-b_, which is a trained DNN that outputs the reconstruction given an encoding . We also evaluated regularization-based attacks  and obtained similar results; we omit those results for brevity.

Figure 4: \(1/\) vs. reconstruction MSE (left) and sample reconstructions (right) for CIFAR-10, with a randomized smoothing noise of \((0,0.01^{2})\). Corollary 1 breaks when the smoothing noise is too small, but \(\) still shows a strong correlation with the reconstruction quality. The values before each row (right) indicate \(1/\) of the encoder used.

Figure 3: \(1/\) vs. reconstruction MSE (left) and sample reconstructions (right) for CIFAR-10. The values before each row (right) indicate \(1/\) of the encoder used.

ResultFigures 2 and 3 plot the result with a randomized smoothing noise of \((0,0.25^{2})\). Again, _Bound-ub_ correctly bounds the MSE achieved by _Attack-ub_. While _Attack-b_ is not as effective for very low \(1/\), it outperforms _Attack-ub_ for high \(1/\), breaking _Bound-ub_. In comparison, Corollary 1 estimated using score matching (_Bound-ours_) gives a valid lower bound for both attacks.

Figures 2 and 3 also highlights some of the reconstructions visually. Here, the left-hand side number indicates the target \(1/\) and the images are reconstructed using _Attack-b_. In both figures, it can be seen that dFIL correlates well with the visual quality of reconstructed images, with higher values of \(1/\) indicating less faithful reconstructions. See Appendix: Figure 10-11 for more results.

Figure 4 additionally shows the result with a much smaller randomized smoothing noise of \((0,0.01^{2})\). Unlike previous results, _Bound-ours_ breaks around \(1/\)=\(10^{-3}\). We suspect it is due to score matching failing when the data lie on a low-dimensional manifold and the likelihood changes rapidly near the manifold boundary, which can be the case when the smoothing noise is small. Nonetheless, the bound still correlates well with MSE and the visual reconstruction quality. We claim that dFIL still serves as a useful privacy metric, with a theoretically-principled interpretation and a strong empirical correlation to invertibility. More reconstructions are in Appendix: Figure 12.

## 4 Case study 1: split inference with dFIL

### Private split inference with dFIL

Split inference  is a method to run inference of a large DNN that is hosted on the server, without the client disclosing raw input. It is done by running the first few layers of a large DNN on the client device and sending the intermediate activation, instead of raw data, to the server to complete the inference. The client computation can be viewed as instance encoding, where the first few layers on the client device act as an encoder. However, without additional intervention, split inference by itself is _not_ private because the encoding can be inverted .

We design a private split inference system by measuring and controlling the invertibility of the encoder with dFIL. Because the encoder of split inference is a DNN, dFIL can be calculated using Equation 4 with minor modifications to the network (Section 3.1).

OptimizationsThere are several optimizations to improve the model accuracy for the same dFIL: (1) We calculate the amount of noise that needs to be added to the encoding to achieve a target dFIL, and add a similar amount of noise during training. (2) For CNNs, we add a compression layer--a convolution layer that reduces the channel dimension significantly--at the end of the encoder and a corresponding decompression layer at the beginning of the server-side model. Similar heuristics were explored in prior works . (3) We add an _SNR regularizer_ that is designed to maximize the signal-to-noise ratio of the encoding. From Equations 3-4, the noise that needs to be added to achieve a certain dFIL is \(=(^{}_{_{D }}()_{D}())}{}}\). Thus, maximizing the signal-to-noise ratio (SNR) of the encoding \((^{}/^{2})\) is equivalent to minimizing \((^{}_{_{D}}()_{_{D}}())}{^{}}\), which we add to the optimizer during training. These optimizations were selected from comparing multiple heuristics from prior work , and result in a notable improvement of test accuracy.

### Evaluation of dFIL-based split inference

#### 4.2.1 Evaluation setup

Models and datasetsWe used three different models and four different datasets to cover a wide range of applications: ResNet-18  with CIFAR-10/100  for image classification, MLP-based neural collaborative filtering (NCF-MLP)  with MovieLens-20M  for recommendation, and DistilBert  with GLUE-SST2  for sentiment analysis. See Appendix 7.2 for details.

Detailed setups and attacksFor ResNet-18, we explored three split inference configurations: splitting early (after the first convolution layer), in the middle (after block 4), and late (after block 6). We evaluated the empirical privacy with a DNN attacker  and measured the reconstruction quality with structural similarity index measure (SSIM) . Other popular attacks showed similar trends (see Appendix: Figure 9). NCF-MLP translates a user id (uid) and a movie id (mid) into embeddingswith an embedding table and sends them through a DNN to make a prediction. We split the NCF-MLP model after the first linear layer of the MLP and tried reconstructing the original iid and mid from the encoding. This is done by first reconstructing the embeddings from the encoding using direct optimization (\(()=*{arg\,min}_{emb_{0}}||- {Enc}(emb_{0})||_{2}^{2}\)), and finding the original iid and mid by choosing the closest embedding value in the embedding table: \(id=*{arg\,min}_{i}||-Emb[i]||_{2}^{2}\), where \(Emb[i]\) is the \(i\)-th entry of the embedding table. For DistilBert, we again explored three different splitting configurations: splitting early (right after block 0), in the middle (after block 2), and late (after block 4). We use a similar attack to NCF-NLP to retrieve each word token.

#### 4.2.2 Evaluation results

PrivacyFigure 5 shows the attack result for ResNet-18 and CIFAR-10. Setups with lower dFIL lead to lower SSIM (left) and less identifiable images (right), indicating that dFIL strongly correlates with the attack success rate. The figures also show that the privacy leakage estimated by dFIL can be conservative. Some setups show empirically-high privacy even when dFIL indicates otherwise, especially when splitting late. See Appendix: Figure 13 for more results. Figures 6 show the attack result for NCF-MLP (left) and DistilBert (right). Setups with lower dFIL again consistently showed a worse attack success rate. A sample reconstruction for DistilBert is shown in Appendix: Table 5.

UtilityTable 1 summarizes the test accuracy of the split inference models, where \(1/\) is chosen so that the attacker's reconstruction error is relatively high. For the same \(1/\), our proposed optimizations (**Ours**) improve the accuracy significantly compared to simply adding noise (**No opt.**). In general, reasonable accuracy can be achieved with encoders with relatively low dFIL. Accuracy degrades more when splitting earlier, indicating that using a DNN encoder with many layers can benefit utility. Prior works empirically observed a similar trend where splitting earlier degrades the privacy-utility tradeoff, because the encoding becomes leakier . Our framework supports complex DNN encoders, which is a distinguishing benefit compared to prior theoretical works [16; 17; 44].

## 5 Case study 2: training with dFIL

### Training on encoded data with dFIL

We consider a scenario where users publish their encoded private data, and a downstream model is trained on the encoded data. We use the first few layers of a pretrained model as the encoder by

Figure 5: \(1/\) vs. SSIM (left) and sample reconstructions (right) for split inference with ResNet-18 and CIFAR-10. Higher SSIM indicates more successful reconstruction.

Figure 6: Attack accuracy vs. \(1/\) for split inference with NCF-MLP (left) and DistilBert (right).

freezing the weights and applying the necessary changes in Section 3.1. Then, we use the rest of the model with its last layer modified for the downstream task and finetune it with the encoded data. We found that similar optimizations from split inference benefit this use case as well.

### Evaluation of dFIL-based training

We evaluate the model utility and show that it can reach a reasonable accuracy when trained on encoded data. We omit the privacy evaluation as it is similar to Section 4.2.2.

Evaluation setupWe train a ResNet-18 model for CIFAR-10 classification. First, we train the model on one of three different datasets: (1) TinyImageNet , (2) CIFAR-100 , and (3) held-out 20% of CIFAR-10. Then, layers up to block 4 are frozen and used as the encoder. The CIFAR-10 training set is encoded using the encoder and used to finetune the rest of the model. The setup mimics a scenario where some publicly-available data whose distribution is similar (TinyImageNet, CIFAR-100) or the same (held-out CIFAR-10) with the target data is available and is used for encoder training. Detailed hyperparameters are in Appendix 7.2.

Evaluation resultTable 2 summarizes the result. Our design was able to achieve a decent accuracy using encoded data with relatively safe \(1/\) values (10-100). The result indicates that model training with privately encoded data is possible. The achieved accuracy was higher when the encoder was trained with data whose distribution is more similar to the downstream task (CIFAR-10). We believe more studies in hyperparameter/architecture search will improve the result.

## 6 Limitations

We showed that dFIL can theoretically quantify the invertibility of instance encoding by providing a reconstruction error bound against arbitrary attackers. We subsequently showed that dFIL can be used to guide private training/inference systems that uses instance encoding. dFIL has several potential limitations, which needs to be carefully considered before being used:

   Setup & Split & \(}\) & No opt. & Ours \\   CIFAR-10 \\ + ResNet-18 \\ (acc: 92.70\%) \\  } &  & 10 & 10.70\% & **74.44\%** \\  & & 100 & 10.14\% & **57.97\%** \\   & & & 10 & 22.11\% & **91.35\%** \\   & & & 100 & 12.94\% & **84.27\%** \\   & & & 10 & 78.48\% & **92.35\%** \\   & & & 100 & 33.54\% & **87.58\%** \\   CIFAR-100 \\ + ResNet-18 \\ (acc: 92.70\%) \\  } &  & 10 & 1.00\% & **44.10\%** \\  & & 100 & 1.00\% & **30.69\%** \\   & & & 100 & 1.98\% & **59.51\%** \\   & & & 100 & 1.29\% & **41.29\%** \\   & & & 10 & 8.63\% & **65.77\%** \\   & & & 100 & 2.01\% & **39.18\%** \\   MovieLens-20M \\ + NCF-MLP \\ (AUC: 0.8228) \\  } &  & 1 & 0.8172 & **0.8286** \\  & & 10 & 0.7459 & **0.8251** \\   & & & 100 & 0.6120 & **0.8081** \\   GLUE-SST2 \\ + DistilBert \\ (acc: 91.04\%) \\  } &  & 10 & 50.80\% & **82.80\%** \\  & & 100 & 49.08\% & **81.88\%** \\    & & & 10 & 76.61\% & **83.03\%** \\    & & & 100 & 61.93\% & **82.22\%** \\    & & & 10 & **90.25\%** & 83.03\% \\    & & & 100 & 82.68\% & **82.82\%** \\   

Table 1: Test accuracy for different split inference setups with different dFIL. Base accuracy in parenthesis.

   Pretrain dataset & \(}\) & Acc. \\  TinyImageNet & 10 & 75.46\% \\  & 100 & 42.57\% \\  CIFAR-100 & 10 & 80.16\% \\  & 100 & 70.27\% \\  CIFAR-10 & 10 & 81.99\% \\ (held-out 20\%) & 100 & 78.65\% \\   

Table 2: Accuracy from training with different encoders.

1. Corollary 1 only bounds the MSE, which might not always correlate well with the semantic quality. To address this, van Trees inequality can be extended to an absolutely continuous function \(()\) to bound \([\|(})-()\|_{2}^{2}/d]\), which may be used to extend to metrics other than MSE.

2. Equation 5 provides an average case bound, so some data may experience lower reconstruction MSE than the bound. Similar average case bounds have also been used in prior works . When bounding invertibility, it is unclear how to define the privacy attack game to target worst-case reconstruction bound: for any sample, there exists a trivial attacker that always outputs that fixed sample from the distribution, which gives a worst-case MSE of zero for that sample. Leakier data that might experience lower reconstruction MSE than the bound can be potentially detected and handled by dynamically calculating dFIL for each sample. See Appendix: Figure 14.

3. For data types where MSE is not directly meaningful or the bound is inaccurate, it may not be straightforward to interpret the privacy of an encoding given its dFIL. In such cases, acceptable values of dFIL should be determined for each application through further research. The situation is similar to DP, where it is often not straightforward what privacy parameters (_e.g._, \(\), \(\)) need to be used .

4. Systems with the same dFIL may actually have different invertibility, as the bound from dFIL may be conservative. Comparing the privacy of two different systems using dFIL should be done with caution because dFIL is a lower bound rather than an accurate measure of invertibility.