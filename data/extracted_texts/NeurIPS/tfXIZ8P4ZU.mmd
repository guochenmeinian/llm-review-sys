# Steering Without Side Effects: Improving Post-Deployment Control of Language Models

Asa Cooper Stickland\({}^{1}\)1 Alexander Lyzhov\({}^{1}\) Jacob Pfau\({}^{1}\) Salsabila Mahdi\({}^{1}\)

&Samuel R. Bowman\({}^{1,2}\)

\({}^{1}\)New York University \({}^{2}\)Anthropic, PBC

###### Abstract

Language models (LMs) have been shown to behave unexpectedly post-deployment. For example, new jailbreaks continually arise, allowing model misuse, despite extensive red-teaming and adversarial training from developers. Given most model queries are unproblematic and frequent retraining results in unstable user experience, methods for mitigation of worst-case behavior should be _targeted_. One such method is classifying inputs as potentially problematic, then selectively applying _steering vectors_ on these problematic inputs, i.e. adding particular vectors to model hidden states. However, steering vectors can also negatively affect model performance. We present **KL-then-steer** (KTS), a technique that decreases the side effects of steering while retaining its benefits, by first training a model to minimize Kullback-Leibler (KL) divergence between a steered and unsteered model on benign inputs, then steering the model that has undergone this training. Our best method prevents 44% of jailbreak attacks compared to the original Llama-2-chat-7B model while maintaining helpfulness (as measured by MT-Bench) on benign requests almost on par with the original LM.

## 1 Introduction

Language models have been shown to exhibit unexpected behaviors once deployed, for example, threatening users (Roose, 2023) or giving instructions on how to make a bomb (Jiang et al., 2024; Geiping et al., 2024) despite being trained for harmlessness. As models become more capable and are deployed in scenarios with increasingly high stakes, avoiding such harmful unexpected behaviors is of increasing importance (Anderljung et al., 2023).

Predicting problematic behaviors ahead of time is difficult. Models may behave differently in new contexts or in response to new exploits. This necessitates continuous post-deployment monitoring and updates. We may want to adjust how cautious a model is when it has access to a new tool such as web search, or when it has access to documents containing sensitive information such as copyrighted material. We may also want to encourage a range of other behaviors: increasing the probability of using a language in a given locale, decreasing model bias towards user-suggested answers, etc. This suggests we want the ability to classify each input as potentially problematic, then add a separate intervention for each example in a batch of requests. Ideally, we could quickly update a model without having to train and re-deploy it, taking on the associated logistical challenges and risks of performance regression.

Existing work on _representation engineering_(Turner et al., 2023; Li et al., 2023; Zou et al., 2023; Wang and Shu, 2023; Rimsky et al., 2024; Jorgensen et al., 2023; Lu and Rimsky, 2024; von Rutteet al., 2024), also known as activation steering, shows promising results on efficiently controlling model behavior post-training. In its simplest form, this just means adding a _concept vector_ to model hidden states at inference time. For example, Zou et al. (2023) increases robustness to jailbreaks by using vectors corresponding to refusing harmful requests. Roughly speaking, such steering vectors make models slightly more cautious about following requests. They _do not_ represent a method for reducing the amount of times the model complies with a dangerous request to near-zero, and we do not focus on achieving such high reliability in this paper. We can add a different vector to each example in a batch, and the strength of the intervention is adjusted by simply increasing the norm of the vector. The memory and compute overhead is minimal since we are dealing with vectors.

However, we find that applying steering vectors to models using a high enough norm to improve safety, or make other behavior modifications, results in performance degradation.2 This is not surprising, given that steering vectors are interventions on model hidden states which they were never exposed to during training. Previous work measures performance degradation on a simplified version of the MMLU multiple-choice QA dataset (Turner et al., 2023), and similarly finds that steering vectors decrease performance. We explore a more realistic performance measure for conversational AI, namely, the ability of models to produce high-quality open-ended responses to complex questions (using MT-Bench (Zheng et al., 2023)). We use these performance measures to aim for Pareto optimal models with respect to performance and our desired behavior modification.

To prevent performance degradation from steering vectors, we propose the **KL-then-steer** (KTS) technique. In KTS, we minimize the KL-divergence between the fine-tuned model with steering vectors applied and the original model without steering vectors, as shown in Box 2 of Figure 1. We then do steering during inference, as shown in Box 3 of Figure 1. This training procedure is done almost entirely on benign requests that do not need steering interventions, hence we reduce side-effects on benign requests.

Several alternative methods make comparably lightweight updates to models, so we compare our method to baselines such as harmlessness training with LoRA (Hu et al., 2021), and the use of different system prompts encouraging the model to be cautious. Our main evaluation task is reducing susceptibility towards universal jailbreak prompt formats. We simulate the setting of novel attacks post-deployment by holding out types of jailbreaks at training time. Our technique mitigates almost all capabilities reduction (only a 1.5% reduction in MT-Bench score) while reducing harmful behavior (e.g. reducing jailbreak success by 44%) relative to the original model.

Our main contributions are the following:

Figure 1: Schematic overview of our KL-then-steer protocol. The pictured workflow uses harmlessness steering for mitigating jailbreaks, but our method applies generally for improving model safety given other threat models.

* We introduce a technique for training models to not be as affected by the capabilities degradation from applying steering vectors (Section 3.2). As such, we improve adversarial robustness using steering vectors without losing general capabilities (Section 5.1).
* In order to modify the model's behavior on each individual example out of a batch of requests with some criteria, we may need to cheaply classify inputs. We show that simple logistic probes on top of hidden states from Llama-2-chat-7b, or a cheap external classifier, can classify prompts as likely to elicit bad behavior (Section 5.2).

## 2 Background and Related Work

Interventions on representations and weights of trained language models have shown promise in controlling generations and revealing how information gets processed inside models. For example, Meng et al. (2022); Hernandez et al. (2023) introduced methods for editing factual knowledge encoded inside pretrained models such as GPT-2 (Radford et al., 2019). Subramani et al. (2022) pioneered activation steering and showed that it can be used to efficiently control the sentiment of GPT-2 generations without fine-tuning. Subsequent works (Turner et al., 2023; Zou et al., 2023a) explored simpler versions of steering that don't require backward passes. In contrast to prior work, we focus on the application of steering to realistically control models post-deployment, and as such pay closer attention to general degradation in performance. Concurrent to our work, Arditi et al. (2024) show that a single direction in activation space mediates refusal behavior in language models.

Previous approaches to improving model safety typically rely on fine-tuning, for example, reinforcement learning from human feedback on helpful, harmless, and honest data (Bai et al., 2022). However current LLMs increasingly support diverse use-cases and serve as foundations for complex scaffolding, such as _agents_ designed to carry out software tasks (Yang et al., 2024). As such, responding to post-deployment safety threats with whole-model fine-tuning could create unacceptable instability by changing model behavior on all users' workflows. Instead, we propose augmenting monolithic fine-tuning of the model to remove new vulnerabilities as they emerge with our approach. Imagine we have a system where we can classify LLM inputs as suspicious (i.e. likely aimed to elicit unwanted behavior) or not, with reasonably high accuracy. This can be achieved with essentially no inference overhead with a logistic probe on top of early layer hidden states. If the input is suspicious, in the layers following the classifier for that forward pass, and any layers during the rest of generation, we apply steering vectors which significantly reduce the rate at which the inputs elicit bad behavior. This classify and modify approach comes at minimal cost to existing LLM-based infrastructure.

Several methods could be used in combination with a classifier, for example applying different LoRA (Hu et al., 2021) weights as in Zhao et al. (2024), routing to an entirely different model, or using a different system prompt. Steering vectors are one of the few methods which only require \(d\) operations per layer to induce new behavior, where \(d\) is the size of the hidden state, although simply using rank-1 LoRA weights comes close to this in terms of efficiency. Our proposed KTS technique allows steering vectors to be applied without performance degradation, but ultimately how well this works vs. comparable methods is an empirical question which we investigate in Section 5.1.

## 3 Methods

### Steering Vectors

We aim to use the model's representation of a particular behavior (such as toxicity) to intervene on hidden states during the forward pass, similar to Turner et al. (2023) and Zou et al. (2023a). We modify the activations from layer \(l\), on prompt \(B\), \(_{l}(B)\) in the following way:

\[^{}_{l}(B)=_{l}(B)+k_{l},\] (1)

where \(k\) is a scalar used to modify the strength of the steering, and \(_{l}\) is a vector for layer \(l\) corresponding to the concept of interest. We find these concept vectors \(_{l}\) using contrasting prompts consisting of examples of the behavior we want to encourage vs. avoid.

For example, the main source of data for our steering vectors designed to increase harmlessness is the paired response data from Bai et al. (2022), consisting of requests for bad behavior from the model (_How do I hotwire a car?_), alongside good (_Sorry I can't help you with that._) and bad (_Sure, first you open the glovebox...) responses. We extract the concept vector at layer \(l\) by taking the mean of the layer \(l\) hidden states of the good responses \(_{i}\) and subtract the mean of the hidden states of the bad responses \(_{i}\), i.e. \(_{l}=_{i}_{l}(_{i})-_{i} _{l}(_{i})]\). We always extract \(_{l}(B)\) on the final token of prompt \(B\), and apply it to every token position and layers 2-22 of Llama-2-chat-7b following Zou et al. (2023) when using it to control models.

### KL-Then-Steer (KTS)

We design methods to mitigate the capabilities loss of directly applying steering vectors with high enough norms to change behavior. Specifically, we train our models to be unaffected by side effects from applying steering vectors drawn from a distribution \(V\); see Section 4 for a discussion of what this distribution should be. We use \(_{}()\) to refer to the distribution over tokens predicted by the model on input x with steering vector \(\) applied.

In theory, naive optimization for reducing the changes induced by steering vectors reduces the effects of steering vectors on _any_ behavior. However, we find in practice that steering on e.g. jailbreak prompts is potent even after KTS, while capabilities on more typical prompts are preserved. This is because the prompts x we use in KTS are sampled from a general QA dataset, whereas jailbreak prompts are out-of-distribution for this dataset.

We penalize KL-divergence from the original model's output distribution in the presence of steering vectors:

\[_{ V}[D_{}[_{}( )||()]].\] (2)

The training algorithm is given in Algorithm 1, with the 'loss_fn' variable corresponding to one of KL loss mentioned above. In practice, we estimate the expectations above by sampling a single steering vector per minibatch and doing gradient accumulation. We don't apply any steering vectors on a certain fraction of of minibatches, which helps with retaining capabilities for the trained model when no steering vectors are applied (Table 3). We modify the strength of the steering vectors by a uniform random factor \(k U[-c,c]\) for some maximum steering strength \(c\). Maintaining 'base' model performance without any steering applied is difficult, so we aim to find hyperparameters that maintain performance; see Appendix H. We use rank-128 LoRA fine-tuning for KTS.

```
0:\(N 0\)\(\) Training steps
0:\(0 p 1\)\(\) Steering probability
0:\(c\)\(\) Maximum steering multiplier
0:\(\)\(\) Dictionary with concepts for keys and concept sentences for values
0:\(_{}\)\(\) Language model with parameters \(\)\(n N\) while\(n 0\)do \(u(0,1)\) if\(p u\)then  steer_data \(()\)  steer_sentences \(()\) \(k c(-1,1)\) \( k()\) \(x,y\) \((,,y, _{}^{})\) else \(,y\) \((,,y, _{})\) endif \(n n-1\) endwhile ```

**Algorithm 1** KL-then-steer algorithm

### Additional Techniques: Fine-Tuning, System Prompts, and Combining with KTS

Changing the model's **system prompt** is a simple and efficient baseline to compare to our steering vector interventions. We consider two system prompt versions that encourage the model to treat either the instruction (**system prompt v1**) or user (**system prompt v2**) as suspicious (we show the full prompts in Appendix G). Some of our experiments test model preference for picking the answer to a question suggested by a user, instead of the correct answer, and for these experiments we task using a system prompt which directly discourages the model from considering user-suggested answers. We test using a system prompt to discourage the model from picking the user-suggested answer, again given in Appendix G.

We find **fine-tuning with Direct Preference Optimization**(DPO; Rafailov et al., 2023) works well at increasing adversarial robustness without decreasing general capabilities and use this technique in our experiments as a baseline. We train rank-128 and rank-1 LoRA models with DPO: rank-128 is standard, and rank-1 is nearly as lightweight as steering vectors. We train models with a mix of 50% paired response data from the harmlessness split of the helpful, harmless and honest data of Bai et al. (2022), which is exactly the same data we used to create harmlessness steering vectors. We use 50% UltraChat (Ding et al., 2023) data with responses from GPT-4 as the gold answers, and Llama-2-chat-7b responses as the alternative.

To **combine KTS with DPO LoRA models** trained on top of the base model, we simply _merge_ the weights, i.e. add the LoRA weights trained with DPO on top of the base model to the KTS model weights, without any further training.

### Prompt Classification

We earlier (Section 2) argued that a system where we classify prompts as suspicious or not, and only steer the suspicious prompts, would be attractive. The most natural way to do this classification is to train a simple model on top of early layer activations. This means we can classify with essentially no overhead, during the model forward pass.

We use a logistic probe:

\[p(A)=(^{T}(A)),\] (3)

for some learned vector \(\), and hidden states \((A)\) for prompt \(A\). We trained probes for different layers, starting at layer 5 and increasing in increments of 5 until layer 30 (Llama-2-chat-7b has 32 layers). Although they would have been better in terms of reducing inference overhead from classification, we found lower layer classifiers didn't perform well, and picked layer 20 based on it being the lowest layer with significantly above chance (> 60%) accuracy on classifying prompts from MT-Bench as safe.

Alternatively, we try an entirely separate model, Llama Guard 2 (Team, 2024), a state-of-the-art classifier based on Meta-Llama-3-8B, to classify the prompt before passing it over to the large language model assistant. This incurs significant memory cost (in our case, roughly doubling the number of model parameters), but this is less significant if we used similarly-sized classifiers with larger-scale assistant models.

## 4 Experimental Settings

We experiment with the Meta Llama model (Touvron et al., 2023) Llama-2-chat-7b; we need open-weight models to experiment with activation steering. Llama-2-chat-7b had extensive harmlessness training and human red-teaming, meaning it is already fairly robust to adversarial prompts (see, e.g. the results of Mazeika et al. (2024)), so we aim to increase this robustness from a strong baseline. Training hyperparameters and compute resources used for experiments are listed in Appendix E.

Adversarial Robustness EvaluationWe created our own dataset and toxicity classifier for a cheap evaluation of undesirable behavior in model responses. For this evaluation dataset, we developed a list of 14 categories of behavior (Appendix C) that we generate requests for. To get the jailbreak prompts, we apply jailbreak prompt formats from Wei et al. (2023) to our requests. We additionally evaluate on a strong _pre-filling_ attack (Andriushchenko et al., 2024). See Appendix A for more details of our adversarial robustness evaluation.

Measuring General Model CapabilitiesTo check model ability to be a useful assistant, we use MT-Bench, which measures language model conversational fluency and helpfulness. MT-Benchconsists of 80 high-quality multi-turn questions. These questions cover topics such as roleplay, math and coding. Model responses are judged by GPT-4.

Steering Vector Training DistributionTo generate potential steering vectors to train on for KTS, we first sampled many sentences about potentially bad concepts such as _crime_, _anger_ or _sleazy_. We also sampled sentences corresponding to contrasting benign concepts such as _tennis_ or _science_. In practice, we choose two settings. For the first, we only train on steering vectors for concepts that successfully jailbreak the model in initial testing. The intuition here is that we won't hurt general model capabilities by targeting a narrow set of behaviors. For the second setting, we train on every concept. We show results in Table 3; there is not a huge separation between each setting but we choose the first setting due to better performance on MT-Bench. We sample vectors from the steering vector training distribution by first randomly sampling one concept, then randomly sampling between 5-10 sentence pairs corresponding to that concept, and constructing a vector from those sentences using the mean difference method. These sentence pairs involve one sentence from the 'bad' concept, and a randomly chosen sentence from any of the 'good' concepts.

Probe Training DataTo train the probe classifier described in Section 3.4, we need prompts labeled as harmless vs. potentially harmful. We use 15 held-out jailbreaks from Wei et al. (2023), 10 manually created jailbreaks (not included in the adversarial robustness benchmark), and mixtral-generated (Jiang et al., 2024) toxic requests for the harmful data, and UltraChat (Ding et al., 2023) requests (plus jailbreak-augmented versions) as the harmless data. Thus we use a data set of size 7500, consisting of 3750 questions evenly split between benign and potentially toxic requests. 3750 of the examples were plain questions, and the other 3750 examples were jailbreak-augmented versions of the same questions, evenly split over the 25 jailbreaks.

## 5 Results and Discussion

We are primarily interested in finding flexible ways to trade off general model performance and behaviors like model safety. In this section the desired behavior is adversarial robustness, and in Appendix B we explore reducing model _sycophancy_, i.e. bias towards user suggested answers.

Figure 2: Adversarial attack success rate on our manual jailbreak benchmark, _Jailbreak ASR_, and the prefill attack benchmark, _Prefill ASR_, vs. model capabilities as measured by MT-Bench. Top left is optimal. Each line represents a different method as described in Section 5.1. The number next to each point represents the value of the steering multiplier \(k\). The KL-then-steer (KTS) models retain higher capabilities scores for a given steering multiplier.

[MISSING_PAGE_FAIL:7]

classifies 80% of MT-Bench requests as safe, and Llama Guard 2 classifies 100% as safe. On our adversarial robustness benchmark, the logistic probe classifies 67% as unsafe and Llama Guard 2 classifies 70%. This is due both to classifier inaccuracies and to the fact that some requests are not unambiguously harmful (such as asking for fictional prose about a crime, as opposed to requesting instructions on illegal activities).

We see (Figure 3 and Table 6) that classifying prompts and only steering on unsafe requests results in slightly less robust models which perform much better on general capabilities. The resulting points mostly sit on the Pareto frontier. For example, the original Llama-2-7b-chat with steering applied and with a Llama Guard 2 classifier is comparable to our (no classifier) KTS model.

## 6 Conclusion

We explore lightweight and adaptive control methods for language models. Our key objectives are flexibility in terms of which interventions we make and how strong they are, and finding Pareto-optimal methods with respect to our behavior modifications (such as increased adversarial robustness) and general performance. We found activation steering to be a useful tool, but it brought significant performance degradation. We introduce the KL-then-steer (KTS) technique to mitigate the side effects of steering vectors, ensuring that models retain their capabilities on benign tasks. Our KTS model pushes out the adversarial robustness and capability Pareto frontier compared to the original Llama-2-chat-7b.

Additionally, we show logistic probes on model hidden states, or external classifiers, can successfully determine which prompts we should apply steering vectors to, enabling the dynamic application of steering vectors only when necessary. Our approach can be combined with DPO fine-tuning for harmlessness of LoRA weights to further improve adversarial robustness and performance. This combination reduces the success of unseen jailbreak attacks by 44% while maintaining helpfulness on benign requests almost on par with the original model.