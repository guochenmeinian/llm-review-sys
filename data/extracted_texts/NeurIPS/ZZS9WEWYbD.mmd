# A Definition of Continual Reinforcement Learning

David Abel

dmabel@google.com

Google DeepMind

&Andre Barreto

andrebarreto@google.com

Google DeepMind

&Benjamin Van Roy

benvanroy@google.com

Google DeepMind

&Doina Precup

doinap@google.com

Google DeepMind

&Hado van Hasselt

hado@google.com

Google DeepMind

&Satinder Singh

baveja@google.com

Google DeepMind

###### Abstract

In a standard view of the reinforcement learning problem, an agent's goal is to efficiently identify a policy that maximizes long-term reward. However, this perspective is based on a restricted view of learning as _finding a solution_, rather than treating learning as _endless adaptation_. In contrast, continual reinforcement learning refers to the setting in which the best agents never stop learning. Despite the importance of continual reinforcement learning, the community lacks a simple definition of the problem that highlights its commitments and makes its primary concepts precise and clear. To this end, this paper is dedicated to carefully defining the continual reinforcement learning problem. We formalize the notion of agents that "never stop learning" through a new mathematical language for analyzing and cataloging agents. Using this new language, we define a continual learning agent as one that can be understood as carrying out an implicit search process indefinitely, and continual reinforcement learning as the setting in which the best agents are all continual learning agents. We provide two motivating examples, illustrating that traditional views of multi-task reinforcement learning and continual supervised learning are special cases of our definition. Collectively, these definitions and perspectives formalize many intuitive concepts at the heart of learning, and open new research pathways surrounding continual learning agents.

## 1 Introduction

In _The Challenge of Reinforcement Learning_, Sutton states: "Part of the appeal of reinforcement learning is that it is in a sense the whole AI problem in a microcosm" . Indeed, the problem facing an agent that learns to make better decisions from experience is at the heart of the study of Artificial Intelligence (AI). Yet, when we study the reinforcement learning (RL) problem, it is typical to restrict our focus in a number of ways. For instance, we often suppose that a complete description of the state of the environment is available to the agent, or that the interaction stream is subdivided into episodes. Beyond these standard restrictions, however, there is another significant assumption that constrains the usual framing of RL: We tend to concentrate on agents that learn to solve problems, rather than agents that learn forever. For example, consider an agent learning to play Go: Once the agent has discovered how to master the game, the task is complete, and the agent's learning can stop. This view of learning is often embedded in the standard formulation of RL, in which an agent interacts with a Markovian environment with the goal of efficiently identifying an optimal policy, at which point learning can cease.

But what if this is not the best way to model the RL problem? That is, instead of viewing learning as _finding a solution_, we can instead think of it as _endless adaptation_. This suggests study of the _continual_ reinforcement learning (CRL) problem [47; 48; 25; 27], as first explored in the thesis byRing , with close ties to supervised never-ending [10; 39; 43] and continual learning [47; 48; 26; 54; 41; 42; 49; 22; 30; 45; 4].

Despite the prominence of CRL, the community lacks a clean, general definition of this problem. It is critical to develop such a definition to promote research on CRL from a clear conceptual foundation, and to guide us in understanding and designing continual learning agents. To these ends, this paper is dedicated to carefully defining the CRL problem. Our definition is summarized as follows:

_The CRL Problem (Informal)_

_An RL problem is an instance of CRL if the best agents never stop learning._

The core of our definition is framed around two new insights that formalize the notion of "agents that never stop learning": (i) we can understand _every agent_ as implicitly searching over a set of history-based policies (Theorem 3.1), and (ii) _every agent_ will either continue this search forever, or eventually stop (Remark 3.2). We make these two insights rigorous through a pair of logical operators on agents that we call _generates_ and _reaches_ that provide a new mathematical language for characterizing agents. Using these tools, we then define CRL as any RL problem in which all of the best agents never stop their implicit search. We provide two motivating examples of CRL, illustrating that traditional multi-task RL and continual supervised learning are special cases of our definition. We further identify necessary properties of CRL (Theorem 4.1) and the new operators (Theorem 4.2, Theorem 4.3). Collectively, these definitions and insights formalize many intuitive concepts at the heart of continual learning, and open new research pathways surrounding continual learning agents.

## 2 Preliminaries

We first introduce key concepts and notation. Our conventions are inspired by Ring , the recent work by Dong et al.  and Lu et al. , as well as the literature on _general RL_ by Hutter [23; 24], Lattimore , Leike , Cohen et al. , and Majeed .

Notation.We let capital calligraphic letters denote sets (\(\)), lower case letters denote constants and functions (\(x\)), italic capital letters denote random variables (\(X\)), and blackboard capitals denote the natural and real numbers (\(,,_{0}=\{0\}\)). Additionally, we let \(()\) denote the probability simplex over the set \(\). That is, the function \(p:()\) expresses a probability mass function \(p( x,y)\), over \(\), for each \(x\) and \(y\). Lastly, we use \(\) to denote logical negation, and we use \(_{x}\) and \(_{x}\) to express the universal and existential quantifiers over a set \(\).

### Agents and Environments

We begin by defining environments, agents, and related artifacts.

**Definition 2.1**.: _An agent-environment **interface** is a pair \((,)\) of countable sets \(\) and \(\) where \(|| 2\) and \(|| 1\)._

We refer to elements of \(\) as _actions_, denoted \(a\), and elements of \(\) as _observations_, denoted \(o\). Histories define the possible interactions between an agent and an environment that share an interface.

**Definition 2.2**.: _The **histories** with respect to interface \((,)\) are the set of sequences of action-observation pairs,_

\[=_{t=0}^{}()^{t}.\] (2.1)

We refer to an individual element of \(\) as a _history_, denoted \(h\), and we let \(hh^{}\) express the history resulting from the concatenation of any two histories \(h,h^{}\). Furthermore, the set of histories of length \(t_{0}\) is defined as \(_{t}=()^{t}\), and we use \(h_{t}_{t}\) to refer to a history containing \(t\) action-observation pairs, \(h_{t}=a_{0}o_{1} a_{t-1}o_{t}\), with \(h_{0}=\) the empty history. An environment is then a function from the set of all environments, \(\), that produces observations given a history.

**Definition 2.3**.: _An **environment** with respect to interface (\(,\)) is a function \(e:()\)._

This model of environments is general in that it can capture Markovian environments such as Markov decision processes (MDPs, Puterman, 2014) and partially observable MDPs (Cassandra et al., 1994), as well as both episodic and non-episodic settings. We next define an agent as follows.

**Definition 2.4**.: _An **agent** with respect to interface \((,)\) is a function \(:()\)._

We let \(\) denote the set of all agents, and let \(\) denote any non-empty subset of \(\). This treatment of an agent captures the mathematical way experience gives rise to behavior, as in "agent functions" from work by Russell and Subramanian . This is in contrast to a mechanistic account of agency as proposed by Dong et al.  and Sutton . Further, note that Definition2.4 is precisely a history-based policy; we embrace the view that there is no real distinction between an agent and a policy, and will refer to all such functions as "agents" unless otherwise indicated.

### Realizable Histories

We will be especially interested in the histories that occur with non-zero probability as a result of the interaction between a particular agent and environment.

**Definition 2.5**.: _The **realizable histories** of a given agent-environment pair, \((,e)\), define the set of histories of any length that can occur with non-zero probability from the interaction of \(\) and \(e\),_

\[^{,e}=}=_{t=0}^{}\{h_{t }_{t}:_{k=0}^{t-1}e(o_{k+1} h_{k},a_{k})(a_{k}  h_{k})>0\}.\] (2.2)

Given a realizable history \(h\), we will refer to the realizable history _suffixes_, \(h^{}\), which, when concatenated with \(h\), produce a realizable history \(hh^{}}\).

**Definition 2.6**.: _The **realizable history suffixes** of a given \((,e)\) pair, relative to a history prefix \(h^{,e}\), define the set of histories that, when concatenated with prefix \(h\), remain realizable,_

\[^{,e}_{h}=}_{h}=\{h^{}: hh^{}^{,e}\}.\] (2.3)

We abbreviate \(^{,e}\) to \(}\), and \(^{,e}_{h}\) to \(}_{h}\), where \(\) and \(e\) are obscured for brevity.

### Reward, Performance, and the RL Problem

Supported by the arguments of Bowling et al. , we assume that all of the relevant goals or purposes of an agent are captured by a deterministic reward function (in line with the _reward hypothesis_).

**Definition 2.7**.: _We call \(r:\) a **reward function**._

We remain agnostic to how the reward function is implemented; it could be a function inside of the agent, or the reward function's output could be a special scalar in each observation. Such commitments do not impact our framing. When we refer to an environment we will implicitly mean that a reward function has been selected as well. We remain agnostic to how reward is aggregated to determine performance, and instead adopt the function \(v\) defined as follows.

**Definition 2.8**.: _The **performance**, \(v:[_{},_{}]\) is a bounded function for fixed constants \(_{},_{}\)._

The function \(v(,e h)\) expresses some statistic of the received future random rewards produced by the interaction between \(\) and \(e\) following history \(h\), where we use \(v(,e)\) as shorthand for \(v(,e h_{0})\). While we accommodate any \(v\) that satisfies the above definition, it may be useful to think of specific choices of \(v(,e h_{t})\), such as the average reward,

\[_{k}_{,e}[R_{t}++R_{t+k}  H_{t}=h_{t}],\] (2.4)

where \(_{,e}[\  H_{t}=h_{t}]\) denotes expectation over the stochastic process induced by \(\) and \(e\) following history \(h_{t}\). Or, we might consider performance based on the expected discounted reward, \(v(,e h_{t})=_{,e}[R_{t}+ R_{t+1}+ H _{t}=h_{t}]\), where \([0,1)\) is a discount factor.

The above components give rise to a simple definition of the RL problem.

**Definition 2.9**.: _An instance of the **RL problem** is defined by a tuple \((e,v,)\) as follows_

\[^{*}=_{}v(,e).\] (2.5)

This captures the RL problem facing an _agent designer_ that would like to identify an optimal agent (\(^{*}^{*}\)) with respect to the performance (\(v\)), among the available agents (\(\)), in a particular environment (\(e\)). We note that a simple extension of this definition of the RL problem might instead consider a set of environments (or similar alternatives).

Agent Operators: Generates and Reaches

We next introduce two new insights about agents, and the logical operators that formalize them:

1. Theorem 3.1: _Every agent_ can be understood as searching over another set of agents.
2. Remark 3.2: _Every agent_ will either continue their search forever, or eventually stop.

We make these insights precise by introducing a pair of logical operators on agents: (1) a set of agents _generates_ (Definition 3.4) another set of agents, and (2) a given agent _reaches_ (Definition 3.5) an agent set. Together, these operators enable us to define _learning_ as the implicit search process captured by the first insight, and _continual learning_ as the process of continuing this search indefinitely.

### Operator 1: An Agent Basis Generates an Agent Set.

The first operator is based on two complementary intuitions.

From the first perspective, an agent can be understood as _searching_ over a space of representable action-selection strategies. For instance, in an MDP, agents can be interpreted as searching over the space of policies (that is, the space of stochastic mappings from the MDP's state to action). It turns out this insight can be extended to any agent and any environment.

The second complementary intuition notes that, as agent designers, we often first identify the space of representable action-selection strategies of interest. Then, it is natural to design agents that search through this space. For instance, in designing an agent to interact with an MDP, we might be interested in policies representable by a neural network of a certain size and architecture. When we design agents, we then consider all agents (choices of loss function, optimizer, memory, and so on) that search through the space of assignments of weights to this particular neural network using standard methods like gradient descent. We codify these intuitions in the following definitions.

**Definition 3.1**.: _An **agent basis** (or simply, a basis), \(_{}\), is any non-empty subset of \(\)._

Notice that an agent basis is a choice of agent set, \(\). We explicitly call out a basis with distinct notation (\(_{}\)) as it serves an important role in the discussion that follows. For example, we next introduce _learning rules_ as functions that switch between elements of an agent basis for each history.

**Definition 3.2**.: _A **learning rule** over an agent basis \(_{}\) is a function, \(:_{}\), that selects a base agent for each history._

We let \(\) denote the set of all learning rules over \(_{}\), and let \(\) denote any non-empty subset of \(\). A learning rule is a mechanism for switching between the available base agents following each new experience. Notice that learning rules are deterministic; while a simple extension captures the stochastic case, we will see by Theorem 3.1 that the above is sufficiently general in a certain sense. We use \((h)(h)\) to refer to the action distribution selected by the agent \(=(h)\) at any history \(h\).

**Definition 3.3**.: _Let \(\) be a set of learning rules over some basis \(_{}\), and let \(e\) be an environment. We say that a set \(\) is **\(\)-generated** by \(_{}\) in \(e\), denoted \(_{}_{}\), if and only if_

\[_{}_{}_{h}}\ \ (h)=(h)(h).\] (3.1)

Thus, any choice of \(\) together with a basis \(_{}\) induces a family of agent sets whose elements can be understood as switching between the basis according to the rules prescribed by \(\). We then say that a basis _generates_ an agent set in an environment if there exists a set of learning rules that switches between the basis elements to produce the agent set.

**Definition 3.4**.: _We say a basis \(_{}\) generates \(\) in \(e\), denoted \(_{}\), if and only if_

\[_{}_{}.\] (3.2)

Intuitively, an agent basis \(_{}\) generates another agent set \(\) just when the agents in \(\) can be understood as switching between the base agents. It is in this sense that we can understand agents as searching through a basis--an agent is just a particular sequence of history-conditioned switches over a basis. For instance, let us return to the example of a neural network: The agent basis might represent a specific multilayer perceptron, where each element of this basis is an assignment to the network's weights. The learning rules are different mechanisms that choose the next set of weights in response to experience (such as gradient descent). Together, the agent basis and the learning rules _generate_ the set of agents that search over choices of weights in reaction to experience. We present a cartoon visual of the generates operator in Figure 1(a).

Now, using the generates operator, we revisit and formalize the central insight of this section: Every agent can be understood as implicitly searching over an agent basis. We take this implicit search process to be the behavioral signature of learning.

**Theorem 3.1**.: _For any agent-environment pair (\(,e\)), there exists infinitely many choices of a basis, \(_{}\), such that both (1) \(_{}\), and (2) \(_{}\{\}\)._

Due to space constraints, all proofs are deferred to Appendix B.

We require that \(_{}\) to ensure that the relevant bases are non-trivial generators of \(\{\}\). This theorem tells us that no matter the choice of agent or environment, we can view the agent as a series of history-conditioned switches between basis elements. In this sense, we can understand the agent _as if_1 it were carrying out a search over the elements of some \(_{}\). We emphasize that there are infinitely many choices of such a basis to illustrate that there are many plausible interpretations of an agent's behavior--we return to this point throughout the paper.

### Operator 2: An Agent Reaches a Basis.

Our second operator reflects properties of an agent's limiting behavior in relation to a basis. Given an agent and a basis that the agent searches through, what happens to the agent's search process in the limit: does the agent keep switching between elements of the basis, or does it eventually stop? For example, in an MDP, many agents of interest eventually stop their search on a choice of a fixed policy. We formally define this notion in terms of an agent _reaching_ a basis according to two modalities: an agent (i) _sometimes_ or (ii) _never_ reaches a basis.

**Definition 3.5**.: _We say agent \(\)**sometimes reaches \(_{}\) in \(e\), denoted \(_{}\), if and only if_

\[_{h}}_{_{}_{ }}_{h^{}}_{h}}\ (hh^{})=_{}(hh^{}).\] (3.3)

That is, for at least one realizable history, there is some base agent (\(_{}\)) that produces the same action distribution as \(\) forever after. This indicates that the agent can be understood as if it has stopped its search over the basis. We present a visual of sometimes reaches in Figure 1(b). By contrast, we say an agent _never_ reaches a basis just when it never becomes equivalent to a base agent.

**Definition 3.6**.: _We say agent \(\)**never reaches \(_{}\) in \(e\), denoted \(_{}\), iff \((_{})\)._

Figure 1: A visual of the generates (left) and sometimes reaches (right) operators. **(a) Generates:** An agent basis, \(_{}\), comprised of three base agents depicted by the triangle, circle, and square, generates a set \(\) containing agents that can each be understood as switching between the base agents in the realizable histories of environment \(e\). **(b) Sometimes Reaches:** On the right, we visualize \(_{1}\) generated by \(_{}\) (from the figure on the left) to illustrate the concept of _sometimes reaches_. That is, the agent’s choice of action distribution at each history can be understood as switching between the three basis elements, and there is at least one history for which the agent stops switching—here, we show the agent settling on the choice of the blue triangle and never switching again.

[MISSING_PAGE_EMPTY:6]

### CRL Examples

We next detail two examples of CRL to provide further intuition.

Q-Learning in Switching MDPs.First we consider a simple instance of CRL based on the standard multi-task view of MDPs. In this setting, the agent repeatedly samples an MDP to interact with from a fixed but unknown distribution . In particular, we make use of the switching MDP environment from Luketina et al. . The environment \(e\) consists of a collection of \(n\) underlying MDPs, \(m_{1},,m_{n}\), with a shared action space and environment-state space. We refer to this environment-state space using observations, \(o\). The environment has a fixed constant positive probability of \(0.001\) to switch the underlying MDP, which yields different transition and reward functions until the next switch. The agent only observes each environment state \(o\), which does not reveal the identity of the active MDP. The rewards of each underlying MDP are structured so that each MDP has a unique optimal policy. We assume \(v\) is defined as the average reward, and the basis is the set of \(\)-greedy policies over all \(Q(o,a)\) functions, for fixed \(=0.15\). Consequently, the set of agents we generate, \(_{}\), consists of all agents that switch between these \(\)-greedy policies.

Now, the components \((e,v,,_{})\) have been defined, we can see that this is indeed an instance of CRL: None of the base agents can be optimal, as the moment that the environment switches its underlying MDP, we know that any previously optimal policy will no longer be optimal in the next MDP following the switch. Therefore, any agent that _converges_ (in that it reaches the basis \(_{}\)) cannot be optimal either for the same reason. We conclude that all optimal agents in \(\) are continual learning agents relative to the basis \(_{}\).

We present a visual of this domain in Figure 2(a), and conduct a simple experiment contrasting the performance of \(\)-greedy _continual_ Q-learning (blue) that uses a constant step-size parameter of \(=0.1\), with a _convergent_ Q-learning (green) that anneals its step size parameter over time to zero. Both use \(=0.15\), and we set the number of underlying MDPs to \(n=10\). We present the average reward with 95% confidence intervals, averaged over 250 runs, in Figure 2(b). Since both variants of Q-learning can be viewed as searching over \(_{}\), the annealing variant that stops its search will under-perform compared to the continual approach. These results support the unsurprising conclusion that it is better to continue searching over the basis rather than converge in this setting.

Continual Supervised Learning.Second, we illustrate the power of our CRL definition to capture continual supervised learning. We adopt the problem setting studied by Mai et al. . Let \(\) denote a set of objects to be labeled, each belonging to one of \(k\) classes. The observation space \(\) consists of pairs, \(o_{t}=(x_{t},y_{t})\), where \(x_{t}\) and \(y_{t}\). Here, each \(x_{t}\) is an input object to be classified and \(y_{t}\) is the label for the previous input \(x_{t-1}\). Thus, \(=\). We assume by convention that the initial

Figure 2: A visual of a grid world instance of the switching MDPs problem (left) , and results from an experiment contrasting continual learning and convergent Q-learning (right). The environment pictured contains \(n\) distinct MDPs. Each underlying MDP shares the same state space and action space, but varies in transition and reward functions, as indicated by the changing walls and rewarding locations (stars, circles, and fire). The results pictured on the right contrast continual Q-learning (with \(=0.1\)) with traditional Q-learning that anneals its step-size parameter to zero over time.

label \(y_{0}\) is irrelevant and can be ignored. The agent will observe a sequence of object-label pairs, \((x_{0},y_{0}),(x_{1},y_{1}),\), and the action space is a choice of label, \(=\{a_{1},,a_{k}\}\) where \(||=k\). The reward for each history \(h_{t}\) is \(+1\) if the agent's most recently predicted label is correct for the previous input, and \(-1\) otherwise:

\[r(a_{t-1}o_{t})=r(a_{t-1}y_{t})=+1&a_{t-1}=y_{t},\\ -1&a_{t-1}=.\] (4.1)

Concretely, the continual learning setting studied by Mai et al.  supposes the learner will receive samples from a sequence of probability distributions, \(d_{0},d_{1},\), each supported over \(\). The \((x,y)\) pairs experienced by the learner are determined by the sequence of distributions. We capture this distributional shift in an environment \(e\) that shifts its probability distribution over \(\) depending on the history to match the sequence, \(d_{0},d_{1},\).

Now, is this an instance of CRL? To answer this question precisely, we need to select a \((,_{})\) pair. We adopt the basis \(_{}=\{_{}:x y_{t},_{y_{t} }\}\) that contains each classifier that maps each object to each possible label. By the universal set of learning rules \(\), this basis generates the set of all agents that search over classifiers. Now, our definition says the above is an instance of CRL just when every optimal agent never stops switching between classifiers, rather than stop their search on a fixed classifier. Consequently, if there is an optimal classifier in \(_{}\), then this will not be an instance of CRL. If, however, the environment imposes enough distributional shift (changing labels, adding mass to new elements, and so on), then the _only_ optimal agents will be those that always switch among the base classifiers, in which case the setting is an instance of CRL.

### Relationship to Other Views on Continual Learning

The spirit of continual learning has been an important part of machine learning research for decades, often appearing under the name of "lifelong learning" [63; 62; 53; 55; 51; 3; 4], "never-ending learning" [39; 43] with close ties to transfer-learning [61; 60], meta-learning [52; 17], as well as online learning and non-stationarity [5; 40; 13; 6; 31]. In a similar vein, the phrase "continuing tasks" is used in the classic RL textbook  to refer explicitly to cases when the interaction between agent and environment is not subdivided into episodes. Continual reinforcement learning was first posed in the thesis by Ring . In later work [47; 48], Ring proposes a formal definition of the continual reinforcement learning problem--The emphasis of Ring's proposal is on the _generality_ of the environment: rather than assume that agents of interest will interact with an MDP, Ring suggests studying the unconstrained case in which an agent must maximize performance while only receiving a stream of observations as input. The environment or reward function, in this sense, may change over time or may be arbitrarily complex. This proposal is similar in spirit to _general RL_, studied by Hutter , Lattimore , Leike , and others [12; 37; 36] in which an agent interacts with an unconstrained environment. General RL inspires many aspects of our conception of CRL; for instance, our emphasis on history-dependence rather than environment-state comes directly from general RL. More recently, Khetarpal et al.  provide a comprehensive survey of the continual reinforcement learning literature. We encourage readers to explore this survey for a detailed history of the subject.2 In the survey, Khetarpal et al. propose a definition of the CRL problem that emphasizes the non-stationarity of the underlying process. In particular, in Khetarpal et al.'s definition, an agent interacts with a POMDP in which each of the individual components of the POMDP--such as the state space or reward function--are allowed to vary with time. We note that, as the environment model we study (Definition 2.3) is a function of history, it can capture time-indexed non-stationarity. In this sense, the same generality proposed by Khetarpal et al. and Ring is embraced and retained by our definition, but we add further precision to what is meant by _continual learning_ by centering around a mathematical definition of continual learning agents (Definition 4.1).

### Properties of CRL

Our formalism is intended to be a jumping off point for new lines of thinking around agents and continual learning. We defer much of our analysis and proofs to the appendix, and here focus on highlighting necessary properties of CRL.

**Theorem 4.1**.: _Every instance of CRL \((e,v,,_{})\) necessarily satisfies the following properties:_

1. _If_ \(_{}^{}\)_, then there exists a_ \(^{}_{}\) _such that (1)_ \(^{}_{}\)_, and (_2_)_ \((e,v,,^{}_{})\) _is not an instance of CRL._
2. _No element of_ \(_{}\) _is optimal:_ \(_{}^{}=\)_._
3. _If_ \(||\) _is finite, there exists an agent set,_ \(^{}\)_, such that_ \(|^{}|<||\) _and_ \(^{}\)_._
4. _If_ \(||\) _is infinite, there exists an agent set,_ \(^{}\)_, such that_ \(^{}\) _and_ \(^{}\)_._

This theorem tells us several things. The first point of the theorem has peculiar implications. We see that as we change a single element (the basis \(_{}\)) of the tuple \((e,v,_{},)\), the resulting problem can change from CRL to not CRL. By similar reasoning, an agent that is said to be a _continual learning agent_ according to Definition4.1 may not be a continual learner with respect to some other basis. We discuss this point further in the next paragraph. Point (2.) notes that no optimal strategy exists within the basis--instead, to be optimal, an agent must switch between basis elements indefinitely. As discussed previously, this fact encourages a departure in how we think about the RL problem: rather than focus on agents that can identify a single, fixed solution to a problem, CRL instead emphasizes designing agents that are effective at updating their behavior indefinitely. Points (3.) and (4.) show that \(\) cannot be _minimal_. That is, there are necessarily some redundancies in the design space of the agents in CRL--this is expected, since we are always focusing on agents that search over the same agent basis. Lastly, it is worth calling attention to the fact that in the definition of CRL, we assume \(_{}\)--this suggests that in CRL, the agent basis is necessarily limited in some way. Consequently, the design space of agents \(\) are _also_ limited in terms of what agents they can represent at any particular point in time. This limitation may come about due to a computational or memory budget, or by making use of a constrained set of learning rules. This suggests a deep connection between _bounded_ agents and the nature of continual learning, as explored further by Kumar et al. . While these four points give an initial character of the CRL problem, we note that further exploration of the properties of CRL is an important direction for future work.

Canonical Agent Bases.It is worth pausing and reflecting on the concept of an agent basis. As presented, the basis is an arbitrary choice of a set of agents--consequently, point (1.) of Theorem4.1 may stand out as peculiar. From this perspective, it is reasonable to ask if the fact that our definition of CRL is basis-dependant renders it vacuous. We argue that this is not the case for two reasons. First, we conjecture that _any_ definition of continual learning that involves concepts like "learning" and "convergence" will have to sit on top of some reference object whose choice is arbitrary. Second, and more important, even though the mathematical construction allows for an easy change of basis, in practice the choice of basis is constrained by considerations like the availability of computational resources. It is often the case that the domain or problem of interest provides obvious choices of bases, or imposes constraints that force us as designers to restrict attention to a space of plausible bases or learning rules. For example, as discussed earlier, a choice of neural network architecture might comprise a basis--any assignment of weights is an element of the basis, and the learning rule \(\) is a mechanism for updating the active element of the basis (the parameters) in light of experience. In this case, the number of parameters of the network is constrained by what we can actually build, and the learning rule needs to be suitably efficient and well-behaved. We might again think of the learning rule \(\) as gradient descent, rather than a rule that can search through the basis in an unconstrained way. In this sense, the basis is not _arbitrary_. We as designers choose a class of functions to act as the relevant representations of behavior, often limited by resource constraints on memory or compute. Then, we use specific learning rules that have been carefully designed to react to experience in a desirable way--for instance, stochastic gradient descent updates the current choice of basis in the direction that would most improve performance. For these reasons, the choice of basis is not arbitrary, but instead reflects the ingredients involved in the design of agents as well as the constraints necessarily imposed by the environment.

### Properties of Generates and Reaches

Lastly, we summarize some of the basic properties of generates and reaches. Further analysis of generates, reaches, and their variations is provided in AppendixC.

**Theorem 4.2**.: _The following properties hold of the generates operator:_1. _Generates is transitive: For any triple_ \((^{1},^{2},^{3})\) _and_ \(e\)_, if_ \(^{1}^{2}\) _and_ \(^{2}^{3}\)_, then_ \(^{1}^{3}\)_._
2. _Generates is not commutative: there exists a pair_ \((^{1},^{2})\) _and_ \(e\) _such that_ \(^{1}^{2}\)_, but_ \((^{2}^{1})\)_._
3. _For all_ \(\) _and pair of agent bases_ \((^{1}_{},^{2}_{})\) _such that_ \(^{1}_{}^{2}_{}\)_, if_ \(^{1}_{}\)_, then_ \(^{2}_{}\)_._
4. _For all_ \(\) _and_ \(e\)_,_ \(\)_._
5. _The decision problem,_ _Given__\((e,_{},)\)__,_ _output_ _True_ _iff_ \(_{}\)_, is undecidable._

The fact that generates is transitive suggests that the basic tools of an agent set--paired with a set of learning rules--might be likened to an algebraic structure. We can draw a symmetry between an agent basis and the basis of a vector space: A vector space is comprised of all linear combinations of the basis, whereas \(\) is comprised of all valid switches (according to the learning rules) between the base agents. However, the fact that generates is not commutative (by point 2.) raises a natural question: are there choices of learning rules under which generates is commutative? We suggest that a useful direction for future work can further explore an algebraic perspective on agents.

We find many similar properties hold of reaches.

**Theorem 4.3**.: _The following properties hold of the reaches operator:_

1. \(\) _and_ \(\) _are not transitive._
2. _"Sometimes reaches" is not commutative: there exists a pair_ \((^{1},^{2})\) _and_ \(e\) _such that_ \(_{^{1}^{1}}\ \ ^{1}^{2}\)_, but_ \(_{^{2}^{2}}\ \ ^{2}^{1}\)_._
3. _For all pairs_ \((,e)\)_, if_ \(\)_, then_ \(\)_._
4. _Every agent satisfies_ \(\) _in every environment._
5. _The decision problem,_ _Given__\((e,,)\)__,_ _output_ _True_ _iff_ \(\)_, is undecidable._

Many of these properties resemble those in Theorem 4.2. For instance, point (5.) shows that deciding whether a given agent sometimes reaches a basis in an environment is undecidable. We anticipate that the majority of decision problems related to determining properties of arbitrary agent sets interacting with unconstrained environments will be undecidable, though it is still worth making these arguments carefully. Moreover, there may be interesting special cases in which these decision problems are decidable (and perhaps, efficiently so). We suggest that identifying these special cases and fleshing out their corresponding efficient algorithms is an interesting direction for future work.

## 5 Discussion

In this paper, we carefully develop a simple mathematical definition of the continual RL problem. We take this problem to be of central importance to AI as a field, and hope that these tools and perspectives can serve as an opportunity to think about CRL and its related artifacts more carefully. Our proposal is framed around two new insights about agents: (i) every agent can be understood as though it were searching over an agent basis (Theorem 3.1), and (ii) every agent, in the limit, will either sometimes or never stop this search (Remark 3.2). These two insights are formalized through the generates and reaches operators, which provide a rich toolkit for understanding agents in a new way--for example, we find straightforward definitions of a continual learning agent (Definition 4.1) and learning rules (Definition 3.2). We anticipate that further study of these operators and different families of learning rules can directly inform the design of new learning algorithms; for instance, we might characterize the family of _continual_ learning rules that are guaranteed to yield continual learning agents, and use this to guide the design of principled continual learning agents (in the spirit of continual backprop by Dohare et al. ). In future work, we intend to further explore connections between our formalism of continual learning and some of the phenomena at the heart of recent empirical continual learning studies, such as plasticity loss , in-context learning , and catastrophic forgetting . More generally, we hope that our definitions, analysis, and perspectives can help the community to think about continual reinforcement learning in a new light.