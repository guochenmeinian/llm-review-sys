# Motion Graph Unleashed:

A Novel Approach to Video Prediction

 Yiqi Zhong\({}^{13}\) Luming Liang\({}^{1}\) Bohan Tang\({}^{2}\) Ilya Zharkov\({}^{1}\) Ulrich Neumann\({}^{3}\)

\({}^{1}\)Microsoft \({}^{2}\)University of Oxford \({}^{3}\)University of Southern California

\({}^{1}\){yiqizhong,lulian,zharkov}@microsoft.com

\({}^{2}\)bohan.tang@eng.ox.ac.uk \({}^{3}\){yiqizhon,uneumann}@usc.edu

###### Abstract

We introduce _motion graph_, a novel approach to the video prediction problem, which predicts future video frames from limited past data. The motion graph transforms patches of video frames into interconnected graph nodes, to comprehensively describe the spatial-temporal relationships among them. This representation overcomes the limitations of existing motion representations such as image differences, optical flow, and motion matrix that either fall short in capturing complex motion patterns or suffer from excessive memory consumption. We further present a video prediction pipeline empowered by motion graph, exhibiting substantial performance improvements and cost reductions. Experiments on various datasets, including UCF Sports, KITTI and Cityscapes, highlight the strong representative ability of motion graph. Especially on UCF Sports, our method matches and outperforms the SOTA methods with a significant reduction in model size by \(78\%\) and a substantial decrease in GPU memory utilization by \(47\%\). Please refer to this link for the official code.

## 1 Introduction

Video prediction aims at predicting future frames given a limited number of past frames. This technology has potential for numerous applications, including video compression, visual robotics, and surveillance systems. A critical aspect of designing an effective video prediction system is the precise modeling of motion information, which is essential for its successful deployment.

Figure 1: (A) Hard cases which cannot be properly modeled by most existing representations. (B) Motion graph transforms single-frame patches into interconnected nodes, describing the spatial-temporal relationships. Future per-pixel motion dynamic vectors are then predicted on this graph.

At first glance, video prediction appears as a sequence prediction problem, with conventional approaches using advanced sequential data modeling techniques such as 3D convolution [1; 2], recurrent neural networks [3; 4; 5; 6; 7], and transformers [8; 9; 10]. These methods implicitly model motion and image appearance to help reason sequence evolution. However, the implicit approach to motion modeling can result in inefficient learning, which stems from the inherent difference between video prediction and typical sequence prediction: in typical sequence prediction, features tend to evolve uniformly; whereas video prediction involves not only static elements like object appearance but also dynamic elements intricately embedded in the sequence, e.g., motion-related information, including object pose and camera movement. Effectively managing both the static and dynamic aspects of the hidden feature space presents a great challenge.

Instead of treating video prediction purely as a sequence prediction problem, an alternative approach is to explicitly model inter-frame motion, and redefine the task primarily as a motion prediction problem [11; 12; 13; 10; 2]. For this approach, it is crucial to select an appropriate motion representation, which should exhibit expressiveness by accurately encapsulating motion with detailed information like direction and speed. However, as Section 2 and Table 1 will elaborate, existing video motion representations have limitations. Some, as Figure 1(A) demonstrates, lack representative ability for complex scenarios. For example, optical flow is unable to handle motion blur and object distortion/deformation resulting from perspective projection. Others over-sacrifice memory efficiency to model complex motion patterns, such as the motion matrices proposed in MMVP .

To fill the gap in existing research, we propose a novel motion representation: the motion graph, which treats image patches of the observed video frames in the downsample space as interconnected graph nodes based on their spatial and temporal proximity; See Figure 1 (B) for an illustration. This approach achieves both goals of representativeness and compactness by generating accurate predictions while conserving memory and computational resources. For the representative ability, our motion graph enriches each node with dynamic information, including multiple feasible weighted flows toward the subsequent frame. This method captures a broader range of motion patterns compared to single-direction representations like optical flow and enhances error tolerance. Additionally, the motion graph excels in compactness by avoiding computation-heavy operations such as stacked convolutional layers. To reduce computational complexity, we also limit the connections in the motion graph to a fixed number of neighbors for each node. Consequently, compared to previous representations (e.g., motion matrix, keypoint heatmap), the motion graph offers a more sparse and memory-efficient modeling of motion, effectively transcending the limitations of 2D image structures.

Based on the motion graph we have introduced, we present a novel video prediction pipeline. The representative ability and compactness of the motion graph allow our system to have achieved notable enhancements in computational efficiency. We test various scenarios with different motion patterns on three well-known datasets, UCF Sports, KITTI, and Cityscapes. Across all three datasets, our approach consistently exhibits performance that aligns with or surpasses the state-of-the-art (SOTA) methods with significantly lower GPU comsumption.

## 2 Related Works

In this section, we mainly discuss the existing video prediction systems that apply to the setting of short-term high resolution video prediction, please see more details about the setting in Section A.5.

Based on whether they explicitly model motion, existing video prediction methods can be put into two categories. Methods that do not explicitly model motion usually consider video prediction purely as a sequence prediction problem . They use advanced sequential modeling techniques such as recurrent neural networks [4; 3; 5; 14; 15; 16; 17; 18] and transformers [8; 10; 19; 20] to estimate the evolution of image features in the hidden space along the temporal dimension. However, video prediction holds a unique property that is unlike text generation or other sequence prediction problems: that is, most features in video sequences (e.g., object appearance, scene layout, texture of the background) usually remain unchanged, or have no major swift change, from frame to frame. Yet, sequential modeling techniques treat the whole image features uniformly; using them for video prediction thus requires modification in the inner structure of certain sequential modeling architectures to accommodate this property of video prediction. Some works choose to enhance the spatial consistency by adding complex residual shortcuts [4; 17; 21], or by decomposing temporal and spatial information into two separate hidden spaces, hoping that the network will figure out which to maintain and which to change [15; 16]. Although such accommodations sometimes help systems reach impressive prediction accuracy (given the powerful sequential modeling capability of the advanced techniques), these methods tend to have larger model sizes and complicated architectures.

To improve the efficiency of motion modeling in systems, this paper extends the idea shared by the second type of methods, which treat video prediction as a motion prediction problem, and explicitly model the motion hidden in image sequences. Such methods either reason future motion patterns using certain representations with image features as the input, or solely use motion representations as the input for prediction. There are four lines of work on motion representations. First, image difference [11; 10] uses the subtraction of two consecutive frames to represent the dynamic of image sequences. It requires almost no computational cost to calculate but does not directly describe motion. Second, optical flow/ voxel flow [22; 23; 24; 13] describes the pixel-level motion. But, it usually requires auxiliary or out-of-shell models to generate, and is limited to only modeling one-to-one relationships , which can be a disadvantage, especially for scenarios with ambiguous pixel-to-pixel correspondence (e.g., motion blur). Third, key point trace  serves highly structured scenarios (e.g., videos with static backgrounds and dominated by human motions) but can be inefficient for more complex content, including crowded scenes with multiple objects. Fourth, the recent motion matrix  describes the all-pair relationship between consecutive frames to overcome the drawback of optical flow. While motion matrix supports a more efficient and compact system of a notably smaller size, concerns were over the computational and space complexity of its operations .

Besides the above motion representations, we notice that graph, as a sparse data structure that can accurately describe the relationships between entities, has been widely adopted in ordinary motion prediction systems, such as trajectory forecasting [25; 26; 27] and human motion prediction [28; 29; 30]. Inspired by these successful adoptions, we propose to represent the motion in videos using graph structures, named _motion graph_ in this work.

The major difference between motion graph and previous motion representations is that motion graph can describe many-to-many temporal and spatial correspondence with low space complexity. Such sparse representations not only enhance representative ability but also advantageously i) replace most 2D convolution operations by graph operations which are mostly implemented by linear layers, leading to faster and more parameter-efficient systems; ii) enable convenient spatial and temporal interactions among feature patches, promoting higher learning efficiency and prediction accuracy.

## 3 Methodology

In this work, video prediction is approached as a motion prediction problem, utilizing a novel representation called the motion graph. This representation is designed to effectively capture the intricate motion dynamics within video sequences. Leveraging the motion graph, we develop a video prediction pipeline that achieves high performance with reduced model size and GPU memory requirements. Section 3.1 details our problem formulation and notation. Section 3.2 explains the construction of the motion graph from input video sequences. Finally, Section 3.3 delves into our video prediction pipeline, illustrating how the motion graph is used for effective video prediction.

### Problem Formulation

Given a video sequence with \(T\) frames \(\{_{t}^{H W 3}|t=0,1,...,T-1\}\), a video prediction system aims to predict the next \(T^{}\) frames \(\{_{t^{}}^{H W 3}|t^{}=T,T+1,...,T+T^{ }-1\}\). We approach this task by regarding video prediction as a pixel-level motion prediction problem.

  Motion Representation & Out-of-shell Model & Representative Ability & Space Complexity \\   Image difference [11; 10] & n/a & low & \(O(n)\) \\ Keypoint trace  & required & medium & \(O(n)\) \\ Optical flow/ voxel flow [22; 23; 24; 13] & Some required & medium & \(O(n)\) \\ Motion matrix  & n/a & high & \(O(n^{2})\) \\ Motion graph \((ours)\) & n/a & high & \(O(n)\) \\  

Table 1: Motion representation comparison. We assess the representative ability by judging how accurate the motion can be described, especially for the hard cases demonstrated in Fig 1(A).

Consider a pixel located at \((x,y)\) in a given video frame \(_{t}\). Our system is designed to predict \(k\) dynamic vectors starting from this pixel, pointing to the possible locations in the target future frame \(_{t^{}}\). These vectors represent the pixel's anticipated motion from its current position in \(_{t}\) to its future position in \(_{t^{}}\). Mathematically, we express the dynamic vectors of all pixels across the observed frames as \(^{T H W k 3}\), where \(_{t,x,y}=[[ x_{1}, y_{1},w_{1}],...,[ x_{k},  y_{k},w_{k}]]\) with both the motion direction (\( x_{k}, y_{k}\)) and the weighted component (\(w_{k}\)). The transformation of previous frames into future frames is performed by a pixel-level image warped, denoted as \(}\). The warping is based on the predicted dynamic vectors and is formally defined by the following equation:

\[_{T}}=}(,_ {0},...,_{T-1}).\] (1)

This formulation allows us to capture and translate the intricate motion dynamics within the video sequence into future frame predictions with enhanced accuracy.

### Motion Graph

#### 3.2.1 Intuition

Inspired by advancements in general motion prediction systems [25; 26; 30], we posit that modeling semantic correlations among image patches within observed frames is essential for improving current motion representations in video prediction. To capture such correlations, we first model the semantic information of each image patch across a video with \(T\) observed frames. This is achieved by multi-scale image patch feature mappings, \(=\{^{(1)},,^{(M)}\}\), generated by inputting each observed frame into a shared image encoder \(g_{enc}\) and applying a pixel unshuffle technique  to reshape all \(M\) feature maps to the resolution of the smallest features. Here, the smallest resolution of the features generated by \(g_{enc}\) is \(H_{s} W_{s}\), and \(^{(m)}^{T H_{s} W_{s} C_{m}}\) represents the feature map in the \(m\)-th scale containing the \(C_{m}\)-dimensional features of \(T H_{s} W_{s}\) distinct image patches.

Based on \(\), we construct a multi-view graph, named _motion graph_, to capture the semantic correlations among \(T H_{s} W_{s}\) image patches. This graph is represented as: \(=\{,^{(1)},,^{(M)}\}\), where \(=\{v_{0},,v_{T_{H_{s}}W_{s}-1}\}\) denotes the set of nodes each corresponding to an image patch, and \(^{(m)}\) denotes the edge set for the \(m\)-th view capturing the correlations driven by the semantic information within \(^{(m)}\). We detail the graph construction and the graph-related operations as follows.

#### 3.2.2 Node Motion Feature Initialization

For each node \(v_{i}\) in the \(m\)-th view, we initialize its motion feature \(_{i}^{f^{(m)}}^{d_{m}^{(m)}}\) with two components: the tendency feature \(_{i}^{tf^{(m)}}^{d_{tf}^{(m)}}\) and the location feature \(_{i}^{lf^{(m)}}^{d_{tf}^{(m)}}\). The tendency feature \(_{i}^{tf^{(m)}}\) captures the node's motion-related attributes relative to nodes in the subsequent frame and the location feature \(_{i}^{lf^{(m)}}\) models the normalized absolute location of each node in a frame.

Inspired by prior works [2; 32], we generate the tendency feature \(_{i}^{tf^{(m)}}\) in three steps. First, for each node \(v_{i}\), we form a node list \(_{i}^{(m)}\) by choosing top-\(K\) scoring image patches from the subsequent frame based on the cosine similarities computed by \(_{W_{s}}}^{(m)}\) and \(_{W_{s}}+1}^{(m)}\). By doing so, we mitigate

Figure 2: **Motion graph node construction**: Cosine similarity, denoted by \((,)\), between patch features in consecutive frames is computed to further choose top \(k\) directions for each patch. Tendency \(_{i}^{tf^{(m)}}\) and location features \(_{i}^{lf^{(m)}}\) are then generated based on these \(k\) vectors and the patch location.

the risk of false positives and better accommodate complex motion patterns, especially in downscaled spaces where an image patch may exhibit multiple potential movements. Second, we define the dynamic vector for each node \(v_{i}\) as \(_{i}^{(m)}=[ x_{1}, y_{1},w_{1},, x_{K}, y _{K},w_{K}]^{3K}\), where \( x_{k}, y_{k}\) indicate the motion direction and \(w_{k}\) is the cosine similarity score between \(v_{i}\) and the \(k\)-th node in \(_{i}^{(m)}\). Here, an exception is made for nodes in the last observed frame \(_{T-1}\), where we apply zero-padding to the dynamic vectors, as \(_{T}\) is unknown. Finally, we generate the tendency feature \(_{i}^{tf^{(m)}}\) for each node by feeding each dynamic vector \(_{i}^{(m)}\) into a multilayer perceptron (MLP) \(g_{tdc}(.)\) followed by a max-pooling operation \(_{agg}(.)\), as shown in:

\[_{i}^{tf^{(m)}}=_{agg}(g_{tdc}(_{i}^{(m)})).\] (2)

The location feature \(_{i}^{lf^{(m)}}\) encodes the position of a pixel in a frame. Pixel positions influence motion patterns. For instance, pixels on the sides of a frame may appear to move differently than pixels in the center for street views collected by wide-range moving cameras, due to the perspective projection effect. We use another MLP \(g_{loc}(.)\) to extract the location feature and define it as:

\[_{i}^{lf^{(m)}}=g_{loc}(},}),\] (3)

Finally, the motion features of each node \(_{i}^{f^{(m)}}\) are initialized as follows:

\[_{i}^{f^{(m)}}=(_{i}^{lf^{(m)}},_{i}^{tf^ {(m)}}),\] (4)

where \(()\) denotes the concatenation operation. Figure 2 details the node motion feature initialisation.

#### 3.2.3 Edge Construction

After initializing the node features, we construct edges in the motion graph to capture the semantic relationships between image patches in the observed frames. We further represent the edge set of the \(m\)-th view as \(^{(m)}=\{^{S^{(m)}},^{B^{(m)}},^{P ^{(m)}}\}\), where \(^{S^{(m)}}\) is spatial edges, \(^{B^{(m)}}\) is backward edges, and \(^{F^{(m)}}\) is forward edges. Generally, spatial edges are posit on that neighboring image patches in a frame likely influence each other's future motion, and backward and forward edges connect nodes across adjacent frames indicating potential motion paths. Notably, nodes in the first frame are not assigned backward edges, and nodes in the last frame are not assigned forward edges.

We construct these edges in two steps: 1) finding the neighbors of each node connected by spatial, backward, and forward edges respectively; and 2) generating the three types of edges by connecting the neighboring nodes found. For the first step, we denote \(_{i}^{S^{(m)}}\), \(_{i}^{B^{(m)}}\), and \(_{i}^{F^{(m)}}\) as sets containing neighbors of \(v_{i}\) connected by the spatial, backward and forward edges, respectively. Then, the neighbors of each node \(v_{i}\) are found by solving the following optimization problems:

\[_{i}^{S^{(m)}} =*{arg\,max}_{}_{i,}^{(m )}||_{1,1}\;\;s.t.\;\;_{i},\;\;=k,\] \[_{i}^{B^{(m)}} =*{arg\,max}_{}_{i,}^{(m )}||_{1,1}\;\;s.t.\;\;_{i},\;\;=k,\] (5) \[_{i}^{P^{(m)}} =*{arg\,max}_{}_{i,}^{(m )}||_{1,1}\;\;s.t.\;\;_{i},\;\;=k,\]

where \(^{(m)}^{TH_{s}W_{s} TH_{s}W_{s}}\) contains cosine similarity scores between nodes in \(\) computed by \(^{(m)}\), \(k\) is a hyperparameter, \(_{i}=\{v_{j}:W_{s}}= W_{s}}\}\), \(_{i}=\{v_{j}:W_{s}}= W_{s}}+1\}\), and \(_{i}=\{v_{j}:W_{s}}= W_{s}}-1\}\). For the second step, we construct \(^{S^{(m)}}\), \(^{B^{(m)}}\), and \(^{F^{(m)}}\) by connecting each \(v_{i}\) with nodes in \(_{i}^{S^{(m)}}\), \(_{i}^{B^{(m)}}\), and \(_{i}^{F^{(m)}}\) respectively.

#### 3.2.4 Motion Graph Interaction Module

After constructing the nodes and establishing the edges in the motion graph, we enable information flow within the \(m\)-th view of the graph via the message-passing operation \(g_{mp}\) defined as follows:

\[}^{(m)}=g_{mp}(^{(m)},^{in^{(m)}} ),\] (6)where \(^{(m)}^{TH_{s}W_{s} d^{(m)}}\) is the input node features, \(^{(m)}^{TH_{s}W_{s} d^{(m)}}\) is the updated node features, and \(^{in^{(m)}}\) denotes an edge set. The operation \(g_{mp}\) facilitates the information exchange between nodes connected by edges in \(^{in^{(m)}}\). In practice, the spatial message passing is implemented as a 2D convolution layer, while for the temporal message passing we use graph neural network to implement \(g_{mp}\). See implementation details in the appendix.

With the message-passing operation \(g_{mp}\), we introduce a motion graph interaction module, denoted as \(\) and illustrated in Figure 3. The goal of \(\) is to ensure that even nodes in the last observed frame are informed about the motion dynamics from the first observed frame, and vice versa. To achieve this comprehensive information flow, we implement \(T-1\) rounds of full information transition, where \(T\) is the total number of observed frames. Each round of transition includes twice the spatial message passing, once the temporal forward message passing, and once the temporal backward message passing. The combination of spatial and temporal interaction in \(\) ensures holistic information integration for accurate and comprehensive motion prediction in video sequences. The design of \(\) allows for thorough and balanced dissemination of motion information throughout the motion graph.

### Motion-graph-empowered Video Prediction

By constructing the motion graph, we create a tool to extract motion information from the observed video frames. The following passage describes how to conduct video prediction using the constructed motion graph and its related operations. As Figure 4 indicates, to predict the unknown video frame involves three main steps, which are elaborated in the following three subsections.

#### 3.3.1 Motion Feature Learning

**Multi-view motion feature update.** After having the motion graph \(=\{,^{(1)},,^{(M)}\}\), with the node motion feature \(^{f^{(m)}}\) initialized by Eq. (4) in the \(m\)-th view, we use the motion graph interaction module introduced in Section 3.2.4 to update \(^{f^{(m)}}\) via the following formula:

\[}^{f^{(m)}}=^{(m)}(^{f^{(m)}},^{ (m)}),\] (7)

where \(}^{f^{(m)}}^{TH_{s}W_{s} d^{(m)}_{m}}\) is the updated node features in the \(m\)-th view of the motion graph.

**Multi-view motion feature fusion.** Once the node feature in each view of the motion graph is updated, we concatenate the node features from each view and apply a fusion module \(_{fuse}\) to integrate these multi-view features into a unified representation as follows:

\[_{fuse}=_{fuse}(_{cat}(^{f^{(m)}}|m=1,...,M)),\] (8)

Figure 4: **Pipeline overview**. After decoding per-pixel motion features into dynamic vectors, we perform multi-flow forward warping for future frame generation.

Figure 3: **Inside the interaction module for the \(m\)-th view \(^{(m)}\). The spatial and temporal message passing are iteratively conducted and repeated \(T-1\) times, where \(T\) is the observed frame number.**

where \(_{fuse}^{T H_{s} W_{s} C_{node}}\), with \(C_{node}\) denoting the dimension of the node features.

#### 3.3.2 Motion Feature Upsampling and Decoding

In this step, we transform the fused multi-view motion feature \(_{fuse}\) into a 2D structure with a resolution of \(H_{s} W_{s}\). This feature is then upscaled to match the resolution of the original video frames (\(H W\)) using a motion upsampler, \(_{SR}\). The architecture of \(_{SR}\) is inspired by ResNet-based image super-resolution networks , which progressively refine features from lower to higher resolutions until reaching the original image size. This upsampling process is key to predicting pixel-level motion features for all observed frames, which extend toward the next future frame. By incrementally adjusting the resolution, \(_{SR}\) effectively bridges the gap between the multi-view motion information and the high-resolution requirements of accurate pixel-level motion prediction.

Upon obtaining the pixel-level motion feature \(_{fuse}^{SR}\), we use a motion decoder, \(_{dec}\), to convert it into pixel-level dynamic vectors \(\). As defined in Section 3.1, \(\) contains \(k\) potential motion directions with associated probability scores for each pixel. This decoding output is compatible with the motion features in the motion graph, which are generated by the \(k\) dynamic vectors of image patches.

#### 3.3.3 Image Warping

After having the dynamic vectors through the decoding process, we use them to warp the observed frames into the future frame. Unlike traditional methods that often use optical-flow-based backward warping, our approach follows the design logic of the motion graph feature learning as well as the output format to use a multi-flow forward warping technique. This method, drawing from the image splatting concepts in prior works [34; 35], is visually detailed in Figure 4. Forward warping allows each predicted dynamic vector to directly contribute to the construction of the future frame. To aggregate the contributions of multiple vectors at each pixel location in the synthesized frame, we apply a normalization operation \(_{norm}\). It normalizes the weight of each vector based on the sum of their weights, ensuring an even and balanced contribution to the final pixel value in the future frame.

## 4 Experiments

For evaluation, we trained our video prediction pipeline in an end-to-end fashion on three public datasets: i) **UCF Sports** (i.e. 150 video sequences emphasizing various sports scenes, with frame resolution, and two different data splits to date--one from STRPM , one from MMVP --evaluation is on both splits); ii) **KITTI** (i.e. 28 driving videos with a resolution of \(375 1242\); following previous works [38; 13], the image frames are resized to \(256 832\); and iii) **Cityscapes** (i.e. 3,475 driving videos with 2,945 in the training set and 500 in the validation set). For each dataset, by default, we set \(k=max(10,1\% H_{s} W_{s})\) which is \(1\%\) of the smallest feature map's resolution and no larger than 10 for efficiency consideration. Table 2 shows the configuration for each dataset. More training and implementation details are in the appendix.

### Public Benchmark Comparison

On the UCF Sports STRPM split, we evaluate the method on two metrics following previous research : peak signal-to-noise ratio (PSNR) and learned perceptual image patch similarity (LPIPS) . The proposed method is compared with existing methods for their results of the 1st and 6th future frames (\(t=5\), \(t=10\) in Table 3). Table 3 shows that our method matches the SOTA performance on the PSNR metric and outperforms all previous methods on the LPIPS metric. On the UCF Sports MMVP split, the validation dataset has been divided into three categories: the easy (SSIM \(\) 0.9), intermediate (0.6 \(\) SSIM \(<\) 0.9), and hard subsets (SSIM \(<\) 0.6), which take up 66%, 26%, and 8% of the full set respectively. We evaluate the methods on PSNR, LPIPS, and Structural Similarity Index Measure (SSIM). Table 4 shows that the proposed method outperforms all other methods.

   Dataset & Resolution & \(H_{s}\) & \(W_{s}\) & Input Frame & Output Frame & Training Loss & k \\   UCF Sports & \(512 512\) & 32 & 32 & 4 & 1 & mean-square error (following [15; 16; 2]) & 10 \\ KITTI & \(256 832\) & 16 & 52 & 2 & 1 & 11 + Perceptual loss(following ) & 8 \\ Cityscapes & \(512 1024\) & 32 & 64 & 2 & 1 & 11 + Perceptual loss (following ) & 10 \\   

Table 2: Dataset configurationsIn Table 5, we compare the proposed method with existing methods on the Cityscapes and KITTI datasets. Following previous research, we report the Multi-scale Structural Similarity Index Measure (MS_SSIM) and LPIPS of the first future frame (\(t+1\)), the average numbers of the next three future frames (\(t+3\)) and the average numbers of the next five future frames (\(t+5\)).

The UCF Sports dataset features sports scenes and thus contains a large amount of fast movements and motion blurs. The leading performance in both Table 3 and 4 validates that the proposed motion graph helps the network better interpret the motion in the input video sequence and facilitate more accurate prediction. Meanwhile, the qualitative result in Figure 5 demonstrates the method's ability to capture and restore intricate image details during the prediction.

For KITTI and Cityscapes datasets, videos are captured from outdoor, large-scale traffic scenarios through cameras with perspective projection, which results in drastic object distortions on both sides of images as well as unstable lighting conditions. Our method matches SOTA performance in terms of quantitative evaluation (see Table 5). In Figure 6, we conducted qualitative comparison on these datasets with two SOTA methods, OPT  and DMVFN , which are all optical-flow-based.

We noticed that as a non-generative model, our proposed video prediction system may face challenges with occlusions that require the generation of unseen objects. However, it excels in scenarios with scenarios involving occlusions of known objects, as showcased in white walls of the third column in Figure 6. Notably, in scenes with partial obstructions--such as the white wall behind the cyclist, our system adeptly employs multiple motion vectors per pixel to reconstruct occluded areas. This feature

    &  &  0.9\))} &  &  &  \\  & SSIM + & PSNR \(\) & LPIPS \(\) & SSIM \(\) & PSNR \(\) & LPIPS \(\) & SSIM \(\) & PSNR \(\) & LPIPS \(\) & SSIM \(\) & PSNR \(\) & LPIPS \(\) \\   STIP  & 0.8817 & 28.17 & 0.1626 & 0.9491 & 30.65 & 0.1066 & 0.8351 & 23.97 & 0.2271 & 0.4673 & 15.97 & 0.4450 & 18.05M \\ SunVP  & 0.9189 & 29.97 & 0.1326 & 0.9664 & 32.87 & 0.0854 & 0.8845 & 25.79 & 0.1951 & 0.6267 & 18.99 & 0.5600 & 3.478 \\ MMVP  & 0.9300 & 30.5 & 0.1062 & 0.9674 & 33.05 & 0.0580 & 0.8970 & 26.29 & 0.1569 & 0.7203 & **20.84** & 0.3510 & 2.79M \\  Ours & **0.9314** & **30.49** & **0.0823** & **0.9685** & **33.23** & **0.8441** & **0.978** & **26.36** & **0.1348** & **0.7264** & 20.83 & **0.2330** & **0.6037** \\   

Table 4: Performance comparison on the UCF Sports MMVP split.

    &  &  \\  & PSNR \(\) & LPIPS\({}_{1,100}\) & PSNR \(\) & LPIPS\({}_{1,100}\) \(\) \\  ConvLSTM (NeurIPS 2015)  & 26.43 & 32.20 & 17.80 & 58.78 \\ BeyondMSE (ICLR 2016)  & 26.42 & 29.01 & 18.46 & 55.28 \\ PredRNN (NeurIPS 2017) & 24.71 & 28.15 & 19.65 & 55.34 \\ PredRNN+ (ICML 2018)  & 27.26 & 26.80 & 19.67 & 56.79 \\ SAVP (arXiv 2018)  & 27.35 & 25.45 & 19.90 & 49.91 \\ SV2P (ICLR 2018)  & 27.44 & 25.89 & 19.97 & 51.33 \\ ESJ-LSTM (ICLR 2019)  & 27.98 & 25.13 & 20.33 & 47.76 \\ CycleGAN (CVPR 2019)  & 27.99 & 22.95 & 19.99 & 44.93 \\ CrevNet (ICLR 2020)  & 28.23 & 23.87 & 20.33 & 48.15 \\ MotionRNN (CVPR 2021)  & 27.67 & 24.23 & 20.01 & 49.20 \\ STRPM (CVPR 2022)  & 28.54 & 20.69 & 20.59 & 41.11 \\ STIP (arXiv 2022)  & 30.75 & 12.73 & 21.83 & 39.67 \\ DMVFN (CVPR 2023)  & 30.05 & 10.24 & 22.67 & 22.50 \\ MMVP (ICCV 2023)  & 31.68 & 7.88 & 23.25 & 22.24 \\  Ours & **31.70** & **6.61** & **23.27** & **19.94** \\   

Table 3: Performance comparison on UCF Sports STRPM split.

    &  &  &  \\  & PSNR \(\) & LPIPS\({}_{1,100}\) & PSNR \(\) & LPIPS\({}_{1,100}\) \(\) & PSNR \(\) & LPIPS\({}_{1,100}\) \(\) \\  ConvLSTM (NeurIPS 2015)  & 26.43 & 32.20 & 17.80 & 58.78 \\ BeyondMSE (ICLR 2016)  & 26.42 & 29.01 & 18.46 & 55.28 \\ PredRNN (NeurIPS 2017) & 24.71 & 28.15 & 19.65 & 55.34 \\ PredRNN+ (ICML 2018)  & 27.26 & 26.80 & 19.67 & 56.79 \\ SAVP (arXiv 2018)  & 27.35 & 25.45 & 19.90 & 49.91 \\ SV2P (ICLR 2018)  & 27.44 & 25.89 & 19.97 & 51.33 \\ ESJ-LSTM (ICLR 2019)  & 27.98 & 25.13 & 20.33 & 47.76 \\ CycleGAN (CVPR 2019)  & 27.99 & 22.95 & 19.99 & 44.93 \\ CrevNet (ICLR 2020)  & 28.23 & 23.87 & 20.33 & 48.15 \\ MotionRNN (CVPR 2021)  & 27.67 & 24.23 & 20.01 & 49.20 \\ STRPM (CVPR 2022)  & 28.54 & 20.69 & 20.59 & 41.11 \\ STIP (arXiv 2022)  & 30.75 & 12.73 & 21.83 & 39.67 \\ DMVFN (CVPR 2023)  & 30.05 & 10.24 & 22.67 & 22.50 \\ MMVP (ICCV 2023)  & 31.68 & 7.88 & 23.25 & 22.24 \\  Ours & **31.70** & **6.61** & **23.27** & **19.94** \\   

Table 3: Performance comparison on UCF Sports STRPM split.

also supports precise management of object expansion due to perspective projection, as exemplified by the green car in Figure 6's first column.

Moreover, our approach is adept at managing objects exiting the camera's view. By explicitly modeling the motion of image patches, the motion graph predicts when features are about to leave the scene. Thus, any image patches projected to move out of view are not included in the final prediction. This is demonstrated in the 1st, 3rd, and 4th columns of Figure 6, where objects like a cyclist and the front of a blue truck are shown as moving out of frame. Our method successfully and uniquely captures and represents these movements, unlike how the other methods evaluated.

To assess the compactness of the motion graph, we conduct a model consumption analysis. We compare the proposed method with SOTA methods on each dataset in terms of the model size and maximum running GPU memory (the system configurations are identical to the models that generate the numbers in the corresponding tables above). Examination of Table 6 reveals that our method shows robust advantages than SOTA methods in minimizing model consumption and saving resources.

### Ablation Studies

We conduct ablation studies on three aspects of the UCF Sports MMVP split: i) the choice of \(k\), which defines three attributes of the system, i.e., the number of dynamic vectors each node initially embeds,

  Dataset & Image Resolution & Input Frame & Method & Model Size & GPU Memory \\   UCF Sports & \(512 512\) & 4 &  MMVP  \\ Ours \\  & 2.79M & 4.53GB \\  KITTI & \(256 832\) & 2 &  DMVFN  \\ Ours \\  & 3.56M & 3.79GB \\  Cityscapes & \(512 1024\) & 2 & 
 DMVFN  \\ Ours \\  & 3.56M & 7.41GB \\   

Table 6: Model consumption analysis on three datasets, compared with the SOTA methods.

Figure 5: On the UCF Sports dataset, our method recovers richer image details than MMVP .

Figure 6: Qualitative comparisons with OPT  and DMVFN . Our method maintains the object structures better than both methods while holding a higher motion prediction accuracy.

the number of temporal forward/backward edges for each node, and the number of dynamic vectors to be decoded from the upsampled motion features; ii) the number of views used for multi-view motion graph construction; and iii) the composition of the node's initial features.

**The choice of \(k\)**: Adjustments to the value of \(k\) and subsequent evaluations reveal a consistent, monotonic increase in both performance and GPU memory usage, as documented in Table 7. Notably, the gain rate diminishes as \(k\) increases. This observation aligns with the intuition that increasing the value of \(k\) will bring more temporal correspondences to the system. However, once the motion information nears saturation, the incremental benefit of further increasing \(k\) becomes less significant. More ablation studies related to \(k\) can be found in Section A.4.

**The number of graph views**: As Section 3.2 mentioned, using multi-view graphs can enhance motion prediction accuracy. To validate this claim, we reduce the number of views used in the system and test the system on the UCF Sports dataset. The first three rows in Table 8 show that increasing the number of graph views indeed improves the system performance.

**Location feature \(_{loc}\):** We incorporate the tendency feature \(_{dc}\) (directly related to motion) and a location feature \(_{loc}\) to initialize the node features. This inclusion hypothesizes that an image pixel's motion patterns may also be influenced by its spatial position. Empirical evidence supporting this hypothesis is documented in the last two rows of Table 8. There is a notable decline in performance when the location feature \(_{tdc}\) is excluded from the node feature initialization. In Section A.7, we further visualize the location features and observe that such feature may reflect the camera projection pattern, providing substantial information to the following motion reasoning task.

## 5 Conclusion & Limitation

This work presents the motion graph for video prediction and a novel pipeline built upon it. We focus on balancing _representative ability_ and _compactness_ to optimize efficiency. The motion graph encapsulates complex motion information hidden in video sequences into a more manageable format, and achieves encouraging results: it matched or outperformed SOTA on three well-known datasets with a considerably smaller model size and less GPU running memory. Beyond the results, the motion graph and its associated video prediction pipeline set a foundation for further research and optimization in the field of video prediction, thus suggesting a promising direction for researchers seeking to balance performance and resource efficiency in the evolving domain of video processing.

**Limitation.** There is no significantly improvement in terms of the inference speed. The current version of our model is slower than DMVFN, which is specifically optimized for this aspect. Future efforts will focus on accelerating inference while maintaining the model's compactness and efficiency. Additionally, video prediction remains particularly challenging in scenarios involving sudden or unpredictable motions, which our system occasionally fails to capture, as highlighted in Appendix A.6. These instances, where the model struggles with abrupt actions not readily discernible from the input frames, underscore the need for further enhanced video understanding abilities. Addressing these challenges presents a valuable direction for future research.