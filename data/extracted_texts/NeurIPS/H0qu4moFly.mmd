# Embedding Dimension of Contrastive Learning and \(k\)-Nearest Neighbors

Dmitrii Avidukhin

Computer Science Department

Northwestern University

Evanston, IL 60657, USA

dmitrii.avdiukhin@northwestern.edu &Vaggos Chatziafratis

Computer Science and Engineering Department

University of California at Santa Cruz

Santa Cruz, CA 95064, USA

vaggos@ucsc.edu &Orr Fischer

Computer Science Department

Bar-Ilan University

Ramat-Gan, Israel

fischeo@biu.ac.il &Grigory Yaroslavtsev

Computer Science Department

George Mason University

Fairfax, VA 22030, USA

grigory@gmu.edu

###### Abstract

We study the embedding dimension of distance comparison data in two settings: contrastive learning and \(k\)-nearest neighbors (\(k\)-NN). Our goal is to find the smallest dimension \(d\) of an \(_{p}\)-space in which a given dataset can be represented. We show that the _arboricity_ of the associated graphs plays a key role in designing embeddings. For the most popular \(_{2}\)-space, we get tight bounds in both settings. In contrastive learning, we are given \(m\) labeled samples \((x_{i},y_{i}^{+},z_{i}^{-})\) representing the fact that the positive example \(y_{i}\) is closer to the anchor \(x_{i}\) than the negative example \(z_{i}\) (we also give results for \(t\) negatives). For representing such dataset in:

* \(_{2}\): \(d=()\) is necessary and sufficient, consistent with our experiments.
* \(_{p}\) for \(p 1\): \(d=O(m)\) is sufficient and \(d=()\) is necessary.
* \(_{}\): \(d=O(m^{2/3})\) is sufficient and \(d=()\) is necessary.

In \(k\)-NN, for each of the \(n\) data points we are given an ordered set of the closest \(k\) points. We show that for preserving the ordering of the \(k\)-NN for every point in:

* \(_{2}\): \(d=(k)\) is necessary and sufficient.
* \(_{p}\) for \(p 1\): \(d=(k^{2})\) is sufficient and \(d=(k)\) is necessary.
* \(_{}\): \(d=(k)\) is necessary.

Furthermore, if the goal is to not just preserve the ordering of the \(k\)-NN but also keep them as the nearest neighbors, then \(d=((k))\) suffices in \(_{p}\) for \(p 1\).

## 1 Introduction

Embedding vectors play an important role in machine learning, with the embedding dimension being a key parameter of interest when choosing a deep learning architecture. In this paper, we ask the following question: given a dataset labeled with distance relationships between its points, what is the smallest embedding dimension required to represent it? We answer this question for two types of distance comparison data: contrastive labels and \(k\)-NN.

Contrastive LearningContrastive learning  has recently become a popular technique for learning representations, see e.g. .

LL18, HFL\({}^{+}\)19, HFW\({}^{+}\)20, TKI20, CKNH20, CH21, GYC21, CLL21]. Recent interest in theoretical foundations of contrastive learning has resulted in extensive research focusing on generalization [AAE\({}^{+}\)24], design of specific loss functions [HWGM21], transfer learning [SPA\({}^{+}\)19, CRL\({}^{+}\)20], multi-view redundancy [TKH21], inductive biases [SAG\({}^{+}\)22, HM23], the role of negative samples [AGKM22, ADK22], mutual information [vdOLV18, HFL\({}^{+}\)19, BHB19, TDR\({}^{+}\)20], and other topics [WI20, TWSM21, ZSS\({}^{+}\)21, vKSG\({}^{+}\)21, MMW\({}^{+}\)21, WL21].

In on of the most common forms of contrastive learning, we are given \(m\) labeled data points \(\{(x_{i},y_{i}^{+},z_{i}^{-})\}_{i=1}^{m}\) (or more generally, \(\{(x_{i},y_{i}^{+},z_{i,1}^{-},z_{i,2}^{-},,z_{i,t}^{-})\}_{i=1}^{m}\)) over a dataset of size \(n\). Each point represents the fact that the distance between the _anchor_\(x_{i}\) and the _positive example_\(y_{i}\) is smaller than the distance between \(x_{i}\) and the _negative example_\(z_{i}\) (or, more generally, \(t\) negative examples \(z_{i,1},,z_{i,t}\)). We study the problem of embedding such data into \(_{p}\)-spaces, i.e., constructing an embedding \(F V^{d}\) such that \(\|F(x_{i})-F(y_{i})\|_{p}<\|F(x_{i})-F(z_{i})\|_{p}\) for all \(i\) (more generally, \(\|F(x_{i})-F(y_{i})\|_{p}<\|F(x_{i})-F(z_{i,j})\|_{p}\) for all \(i,j\)). In particular, we focus on the embedding dimension:

_Given a collection of \(m\) triplet comparisons of the form "\(x_{i}\) is closer to \(y_{i}\) than to \(z_{i}\)", what is the smallest dimension \(d\) of an \(_{p}\)-space in which the relative order of distances can be preserved?_

\(k\)-NnsWe also study a similar question for \(k\)-Nearest Neighbor (\(k\)-NN) data, which has major applications in machine learning since the seminal work of [CH67]. In this setting, we are given a set of \(n\) items and the information about the \(k\)-NN of each item \(\{(x_{i},_{1}(x_{i}),,_{k}(x_{i}))\}_{i=1}^{n}\) where \(_{1}(x_{i}),,_{k}(x_{i})\) are the \(k\)-NN of \(x_{i}\) ordered by their distance from \(x_{i}\). Since \(k\)-NN classifiers are extremely popular in deep learning pipelines, understanding the embedding dimension required for preserving \(k\)-NN is a question of fundamental importance. In particular:

_Given \(n\) items and their \(k\)-NN, what is the smallest dimension \(d\) of an \(_{p}\)-space in which the ordering of the \(k\)-NN can be preserved? What if the \(k\)-NN have to remain \(k\)-NN in the \(_{p}\)-space?_

### Our Results and Techniques

Let \(V\) be the set of \(n\)_points_. Our goal is to construct an embedding \(F V^{d}\). For an integer \(n\), we let \([n]=\{1,2,,n\}\). For a vector \(v^{d}\), let \(v[i]\) be the \(i^{th}\) coordinate of \(v\). For vectors \(v_{1},v_{2}\), we denote their concatenation as \((v_{1},v_{2})\). In a graph, denote by \(N(x)\) the neighbors of vertex \(x\). For standard definitions (e.g. _metric_ and _norm_) and basic facts see Appendix B.

Contrastive LearningFor a set of samples \(Q=\{(x_{1},y_{1}^{+},z_{1}^{-}),,(x_{m},y_{m}^{+},z_{m}^{-})\}\), we call an embedding \(F\) consistent with \(Q\) if \(\|F(x_{i})-F(y_{i})\|_{p}<\|F(x_{i})-F(z_{i})\|_{p}\) for all \(i\). W.l.o.g., we can assume1 that \(m n^{2}\). We call a set of samples non-contradictory if one can't derive a contradiction from the inequalities between the distances. In particular, this implies the existence of a metric \(\) which is consistent with \(Q\) (Fact 25).

We prove the following theorems in Section 2, Appendix D.2, and Appendix D respectively.

**Theorem 1** (Embedding in \(_{2}\)).: _Let \(Q\) be a set of \(m\) non-contradictory triplet samples on a set \(V\). There is an embedding of \(V\) into \(_{2}\)-space \(^{O(m^{1/2})}\) which is consistent with \(Q\)._

**Theorem 2** (Embedding in \(_{}\)).: _Let \(Q\) be a set of \(m\) non-contradictory triplet samples on a set \(V\). There is an embedding of \(V\) into \(_{}\)-space \(^{O(m^{1/3})}\) which is consistent with \(Q\)._

**Theorem 3** (Embedding in \(_{p}\)).: _Let \(Q\) be a set of \(m\) non-contradictory triplet samples on a set \(V\). For any integer \(p 1\), there is an embedding of \(V\) into \(_{p}\)-space \(^{O(m)}\) which is consistent with \(Q\)._

The lower bounds are shown in Appendix E and experimental results are in Section 4. Our results for the more general version of the problem with \(t\) negatives and the lower bounds are given in Table 1.

In Appendix F we give additional results, including an extension to \(t\)-negatives, NP-hardness of fining an embeddding in the minimum dimension needed to satisfy a set of contrastive constraints, and results for an approximate setting in which we only need to satisfy a fraction of the constraints.

\(k\)-NNIn the \(k\)-NN setting, we are given the following information for each data point.

**Definition 4** (\(k\)-Nn).: _For a distance function \( V V_{ 0}\), let \(_{1}(x),,_{n-1}(x)\) be an ordering of \(V\{x\}\) such that \((x,_{1}(x))<(x,_{2}(x))<<(x,_{n-1}(x))\). We define \(NN}_{}(x)=(_{1}(x),,_{k}(x))\) as the ordered set of \(k\) closest points to \(x\)._

For a function \(F V^{d}\), we denote by \(NN}_{F}\) the \(k\)-nearest neighbors in the \(_{p}\)-space corresponding to the image of \(F\). We prove the following theorem in Section 3.

**Theorem 5**.: _Let \( V V_{ 0}\) be a distance function, and let \(p 1\) be a constant. There exists an embedding \(F V^{d}\) of \(V\) into an \(_{p}\)-space of dimension \(d=O(k^{10}^{10}n)\) such that \(NN}_{}(x)=NN}_{F}(x)\), i.e. the embedding \(F\) preserves the ordered set of \(k\)-nearest neighbors of any point \(x V\) under the distance function \(\).2_

We note that the above result is very surprising: \(k\)-NN graph in fact corresponds to \(n(n-1)\) triplet constraints - for each anchor, \(k-1\) comparisons between its \(k\)-NN and \(n-k\) comparisons between the \(k\)'th nearest neighbor and the rest of the points - and Theorem 1 provides only an \(O(n)\) upper bound on dimension for the \(_{2}\) case. Nevertheless, we are able to exploit the structure of the contrastive constraints to avoid polynomial dependence on \(n\).

The following theorem addresses the setting when only the ordering of the \(k\)-NN has to be preserved. This, as well as other results for \(k\)-NN, are presented in Table 2.

**Theorem 6**.: _There is an embedding of \(V\) into \(_{2}\)-space \(^{O(k)}\) that preserves the \(k\)-NN ordering._

Our TechniquesThe key tool in our results is the notion of graph _arboricity_ applied to the associated _constraint graph_. Arboricity of an undirected graph is the minimum number of forests in which its edges can be partitioned. More intuitively, arboricity measures the "density" of the graph: sparse graphs have low arboricity, while graphs with dense subgraphs - such as cliques - have high arboricity.

**Fact 7** (Folklore; see e.g.  and Appendix B.2).: _The arboricity \(r\) of a graph \(G\) with \(m\) edges is at most \(/2\). Moreover, if graph \(G\) has arboricity \(r\), then the following hold._

1. _There is an ordering_ \(x_{1},,x_{n}\) _of_ \(V\) _such that_ \(|N^{-}(x_{i})| 2r-1\) _for each_ \(1 i n\)_, where_ \(N^{-}(x_{i})=\{x_{j} N(x_{i})\,|\,j<i\}\) _is the set of neighbors of_ \(x_{i}\) _in_ \(G\) _preceding_ \(x_{i}\) _in the ordering._
2. \(G\) _is_ \(2r\)_-vertex colorable._

**Definition 8** (Constraint graph).: _In contrastive learning, for a set \(Q\) of samples on \(V\), we define the constraint graph \(G=(V,E)\) as follows: for each sample \((x_{i},y_{i}^{+},z_{i}^{-}) Q\), we add two edges \(\{x_{i},y_{j}\}\) and \(\{x_{i},z_{i}\}\) to \(E\), unless they already exist. In the \(k\)-NN setting, for each \(x\) and its nearest neighbors \(_{1}(x),,_{k}(x)\), we add edges \(\{x,_{i}(x)\}\) for \(1 i k\)._

  Setting & Upper bound & Lower bound \\   \(_{2}\) with \(t\) negatives & \(O()\), Theorem 44 \\ \(_{2}\) with \(t\)-ordering & \(O()\), Theorem 44 \\  \(_{}\) & \(O(m^{2/3})\), Theorem 2 \\  \(_{p}\), integer \(p 1\) & \(O(m)\), Theorem 3 \\   
  Setting & Upper bound & Lower bound \\   \(_{p}\) (\(k\)-NN and ordering) & \((k^{2})\), Theorem 10 & even \(p\): \((k)\), odd \(p\): \((k)\), Theorem 43 \\  \(_{2}\) (ordering of \(k\)-NN) & \(O(k)\) Theorem 6 & \((k)\) \\  \(_{}\) (ordering of \(k\)-NN) & \(-\) & \((k)\), Theorem 43 \\  

Table 1: Our results for contrastive learning

  Setting & Upper bound & Lower bound \\   \(_{p}\) (\(k\)-NN and ordering) & \((k^{10})\), Theorem 5 & even \(p\): \((k)\), odd \(p\): \((k)\), Theorem 43 \\  \(_{p}\) (ordering of \(k\)-NN) & \((k^{2})\), Theorem 10 & \((k)\) \\  \(_{2}\) (ordering of \(k\)-NN) & \(O(k)\) Theorem 6 & \((k)\) \\  \(_{}\) (ordering of \(k\)-NN) & \(-\) & \((k)\), Theorem 43 \\  

Table 2: Our results for \(k\)-NNNote that by Fact 7 the arboricity of the constraint graph resulting from \(m\) samples is at most \(\). The arboricity of the \(k\)-NN constraint graph is at most \(k+1\) (See Lemma 27). We show bounds on the embedding dimension in terms of arboricity, e.g. for \(_{2}\) we prove the following in Section 2.

**Theorem 9**.: _Given a set of non-contradictory inequalities among pairwise distances in \(V\) whose constraint graph has arboricity \(r\), there exists an embedding of \(V\) into \(_{2}\)-space \(^{4r}\) which satisfies all these inequalities._

Theorem 1 follows from Theorem 9 by using \(r/2\) (Fact 7). Moreover, since the arboricity of the constraint graph for \(\)-\(\) at most \(k+1\) (Lemma 27), Theorem 9 shows that preserving the ordering of the \(k\)-NN in \(_{2}\) requires \(O(k)\) dimension. Furthermore, the following theorem, proven in Section 3.1, implies that \((k^{2})\) dimension suffices to preserve orderings of the \(\)-\(\) in \(_{p}\).

**Theorem 10**.: _Given a set of non-contradictory inequalities among pairwise distances in \(V\) whose constraint graph has arboricity \(r\), for any real \(p 1\), there exists an embedding of \(V\) into \(_{p}\)-space \(^{O(r^{2}^{3}n)}\) which satisfies all these inequalities._

While the above constructions suffice for the contrastive learning case and for preserving the _ordering_ of the \(k\)-NN, the _set_ of the nearest neighbors can change under the embeddings above. Hence, in order to preserve the \(k\)-NN, we increase the dimension to separate neighbors from non-neighbors. In particular, we construct the extended part of the embedding randomly, using a sampling scheme which is guaranteed to embed neighbors much closer than non-neighbors. See Section 3.2 for more details and a proof of Theorem 5.

For \(_{}\), instead of arboricity, we use a related fact: by removing a set \(V_{}\) of \(O(m^{2/3})\) high-degree vertices, we reduce the maximum degree of the remaining graph (i.e. \(V_{}=V V_{}\)) to at most \(O(m^{1/3})\). We handle each set differently (points in \(V_{}\) using graph colorings, and points in \(V_{}\) using a Frechet-like embedding). See Appendix D.2 for the details and the proof of Theorem 2.

### Previous Work

Understanding the underlying geometry of a given set of \(n\) points based only on comparisons between pairs of distances is a basic question studied in the literature of non-metric embeddings (also known as ordinal embeddings or monotone maps). In a wide range of applications such as ranking, crowdsourcing, nearest-neighbor search, ad placement, recommendation systems, etc., the exact distances are not as important as their relative order. In fact, some of the early results in the field were motivated by applications in mathematical psychology , and since then ordinal information and embeddings have been used in ranking , metric learning , clustering , crowdsourcing  and modeling human perception . Note that the goal in ordinal embeddings is quite different from the vast literature on metric embeddings (e.g., see ) where the goal is to approximately preserve the numerical values of distances.

We study the question of finding the smallest dimension \(d\) required to represent a given set of \(n\) points such that a given set of \(m\) distance comparisons are preserved. Related questions have been studied under statistical assumptions and it is known  that for the large \(n\) regime, upon knowledge of the ordinal relationships, the set of points can be approximately recovered (up to certain transformations). This serves as further motivation for studying ordinal information as it highlights its power in recovering the underlying geometry of the data points.

However, determining the exact relationships between the dimension \(d\), the number of points \(n\) and the number of given constraints \(m\) has been elusive. Most papers assume that all \((n^{4})\) distance comparisons \((x_{i},x_{j})(x_{k},x_{l})\) among the pairwise distances are known. In , for example, lower bounds are given for the dimension needed to preserve these comparisons. However, having access to such a large number of comparisons is prohibitive in practice. We only assume access to a set of \(m\) distance comparisons and hence these lower bounds do not apply.

Contrastive learning has been studied for \(d=1\) (embedding on the line) by  for dense instances, i.e. \(m=(n^{3})\). For higher dimensions,  gives an \((n)\) lower bound on the smallest dimension (only for \(_{2}\)) that preserves all \((n^{3})\) triplet comparisons. Our Theorem 1 improves this bound for the general case when \(m\) triplet samples are given, without density assumptions. Then,our Theorems 2 and 3 go beyond \(_{2}\) other \(_{p}\)-norms. Our results can also be seen as the reverse direction of the recent work by . In , the central question is quantifying the amount of data required for generalization in contrastive learning, assuming that the data can be embedded into an \(_{p}\)-space of fixed dimension. Here we assume that the data is fixed instead and study the embedding dimension. Combined with , this completes the picture of the relationship between the size of data, its embedding dimension and generalization.

Our second setting (\(k\)-NNs) was also studied in  who showed a lower bound of \(d=(k)\) for preserving the ordering of the neighbors (again in \(_{2}\)). To the best of our knowledge, prior to our work, there was no known upper bound for the smallest dimension and here we provide a matching upper bound. Furthermore, we provide new results for \(k\)-NNs embeddings (both upper and lower bounds) under various \(_{p}\) metrics and results for the stronger setting when not just the ordering of the neighbors but also their status as \(k\)-NN has to be preserved.

## 2 Contrastive Learning in \(_{2}\) Norm

In this section, we prove Theorem 9 - that contrastive queries with the constraint graph \(G=(V,E)\) (Definition 8) of arboricity \(r\) are preserved when the points are embedded into \(_{2}\) space of dimension \(4r\) - from which Theorem 1 and Theorem 6 follow. Fix a distance function \( V V_{ 0}\) that satisfies the given set of inequalities (such a function exists by Fact 25). We order all pairs of neighboring vertices by the distance function \(\) in descending order, and let \(w(x,y)=i\) if \(\{x,y\}\) is the \(i\)-th pair in the ranking. Recall that \(\|F(x)-F(y)\|^{2}=\|F(x)\|^{2}+\|F(y)\|^{2}-2  F(x),F(y).\) In our construction, all embeddings have the same norm, and hence the distances depend only on the inner products between the embeddings.

We split the embedding \(F V^{4r}\) into two parts, i.e. for a point \(x\) let \(F(x)=(,)\), where \(^{2r}\) and \(^{2r}\). For neighboring points \(x\) and \(y\), our choices of \(\) and \(\) ensure that \(, w(x,y)\). We embed the points one by one into \(^{h}\) in the arboricity ordering \(x_{1},,x_{n}\), which by Fact 7 ensures that for every vertex, the number of neighbors with smaller indices is at most \(h\). When embedding \(x_{i}\), we make sure that for any neighbor \(x_{j} N^{-}(x_{i})\) (i.e. a neighbor \(x_{j}\) of \(x_{i}\) such that \(j<i\)) it holds that \(_{i},_{j} w(x_{i},x_{j})\). This requires solving a linear system over \(_{i}\) with at most \(h\) equations, and hence with \(h\) variables, with slight perturbations, the solution always exists.

The choices of \(_{i}\) ensure that all vectors have the same norm while preserving the inner products. This is done by coloring the vertices of the constraint graph in \(h\) colors using Fact 7 and assigning each color to a unique basis vector, which is scaled to equalize the norms. Since these basis vectors are orthogonal, the inner product between any two neighboring points \(x_{i}\) and \(x_{j}\) is \(_{i},_{j}\).

**Construction of \(_{i}\)** Assume \(_{1},,_{i-1}\) have already been chosen. Let \(N^{-}(x_{i})=\{x_{j} N(x_{i}) j<i\}\) be the set of preceding neighbors of \(x_{i}\) in \(G\). For each \(x_{j} N^{-}(x_{i})\), let a linear equation \(P(i,j)\) be \(_{i},_{j}=w(x_{i},x_{j})\), where we consider the coordinates of \(_{i}\) as variables (recall that \(_{j}\) is already set for all \(x_{j} N^{-}(x_{i})\)). In Appendix C we show the following.

Figure 2: Example construction of \(\). The embedding \(_{4}\) is computed based on the embeddings of its already processed neighbors \(_{1}\), \(_{2}\), \(_{3}\). We find the solution \(_{4}\) to the linear system so that, for each edge to a preceding neighbor, the inner product equals the rank of the edge.

**Lemma 11**.: _The set of vectors \(\{_{j} x_{j} N^{-}(x_{i})\}\) is linearly independent._

By Lemma 11, the system of linear equations \(P_{i}=\{P(i,j) x_{j} N^{-}(x_{i})\}\) has a solution \(v^{2r}\). Let \(B(v)\) be a ball centered at \(v\) with sufficiently small radius such that for any \(v^{} B(v)\) it holds that \(|<v^{},_{j}>-w(x_{i},x_{j})|<1/3\) for all \(x_{j} N^{-}(x_{i})\). Choose a point \(v^{}\) uniformly at random from \(B(v)\), and set \(_{i}=v^{}\): this random perturbation guarantees that, with probability 1, Lemma 11 holds in future iterations. By construction, the following property holds.

**Proposition 12**.: _For any \(x\) and any \(y N^{-}(x)\), we have \(|<,>-w(x,y)|<1/3\)._

**Construction of \(_{i}\)** Let \(W=2_{x V}\|\|_{2}^{2}\). By Fact 7, there exists vertex coloring \(C V[h]\) of \(G\), such that \(C(x) C(y)\) for any pair \(\{x,y\} E\). Set \(=_{x}e_{C(x)}\), where \(e_{C(x)}\) is the standard basis vector in the \(C(x)\)-th coordinate, and \(_{x}\) is chosen so that \(\|F(x)\|_{2}^{2}=\|\|_{2}^{2}+\|\|_{2}^{2}=W\) (note that \(_{x}\) exists because \(\|\|_{2}^{2} W\)). By construction, the following property holds.

**Proposition 13**.: _For any edge \(\{x,y\} E\), we have \(<,>=0\)._

Proof of Theorem 9 (sufficient dimension for \(_{2}\) embeddings).: For any edge \(\{u,v\} E\) it holds that

\[\|F(u)-F(v)\|_{2}^{2}=\|F(u)\|_{2}^{2}+\|F(v)\|_{2}^{2}-2<, >-2<,>.\]

By the choice of \(\) and \(\), we have \(\|F(u)\|_{2}^{2}=\|F(v)\|_{2}^{2}=W\). By Proposition 13, \(<,>=0\), and hence the distance depends only on \(<,>\). For any \((x,y^{+},z^{-}) Q\), we have \(\|F(x)-F(y)\|_{2}^{2}<\|F(x)-F(z)\|_{2}^{2}\) iff \(<,>><,>\). By Proposition 12, for any edge \(\{x,y\}\) in \(G\) it holds that \(|<,>-w(x,y)|<1/3\). Since the function \(w\) assigns only integer values, it holds that \(<,>><,>\) if and only if \(w(x,y)<w(x,z)\), hence preserving the ranking of the edges. 

## 3 Preserving \(k\) Nearest Neighbors

In this section, we focus on \(k\) nearest neighbors, and namely we prove Theorems 5 and 10. Let \(G=(V,E)\) be the constraint graph (Definition 8) for given \(k\)-NN input. In Section 3.1, we show how to preserve the order between the neighbors in this graph, and in Section 3.2 we show how to separate neighbors from non-neighbors. Combined, these results fully preserve the \(k\)-NNs.

To simplify the presentation, we focus on the case \(p=1\) - the construction for other \(p\) is identical, with the change being that each embedding coordinate value \(c\) should be replaced with \(c^{1/p}\). In this section, let \((u,v)=_{_{1}}(u,v)\). For a non-contradictory set of samples \(Q\), by Fact 25 there exists a metric \(^{}\) consistent with \(Q\). We order all pairs of neighboring vertices by the value of \(^{}\) in descending order, and let \(w(x,y)=t\) if \((x,y)\) is the \(t\)-th pair in the ranking. Given an embedding \(F\), let \( F\) be a re-scaling of the embedding by a factor of \(\), i.e. multiplying each coordinate by \(\).

### Preserving the Ordering of the \(\)-\(\)

In this section, we show Theorem 10. This embedding is also used as a part of Theorem 5, shown in Section 3.2. Our embedding uses a new coloring scheme we call _Neighbor-Collection Coloring_. Let \(x_{1},,x_{n}\) be the arboricity ordering (Fact 7) and \(N^{-}(x_{i})=\{x_{j}\{x_{i},x_{j}\} E,j<i\}\) be the set of neighbors of \(x_{i}\) preceding \(x_{i}\) in the ordering.

**Definition 14** (NCC Scheme).: _A neighbor-collection coloring scheme is a set of \(K=(r n)\) vertex colorings \(C^{(1)},,C^{(K)}\), where \(C^{(j)}_{x}[r]\) for any \(x V\) and \(j[K]\), such that for any \(x V\) the following holds:_

* _(Collection) for any_ \(y N^{-}(x)\)_, there exists a coloring_ \(j[K]\) _such that_ \(C^{(j)}_{x}=C^{(j)}_{y}\)_, and_ \(C^{(j)}_{z} C^{(j)}_{x}\) _for any_ \(z N^{-}(x)\{y\}\)_._
* _(Load) for any_ \(j[K]\)_, the number of prior neighbors with_ \(j\)_-th color being the same as_ \(C^{(j)}_{x}\) _is small:_ \(|\{y N^{-}(x) C^{(j)}_{x}=C^{(j)}_{y}\}|=O( n)\)_._

Intuitively, each coloring corresponds to a part of the embedding. When the colors \(C^{(j)}_{x},C^{(j)}_{y}\) are different, the \(j\)'th part of the embedding always contributes \(2\) to the distance between \(x\) and \(y\). Otherwise, we can select the \(j\)'th part so that it contributes either \(2\) or \(0\), and the collection propertyguarantees that for any \(y N^{-}(x)\) such a part exists. The load property guarantees that for each part we always have enough choices to get distance \(2\). Finally, we represent \(w(x,y)\) in binary format for all \(x,y\), and, using an NCC scheme, we recover \(w(x,y)\) bit-by-bit.

**Lemma 15**.: _There exists an NCC scheme for the constraint graph \(G\)._

Proof.: For each \(x V\) and \(j[K]\), we choose \(C_{x}^{(j)}\) i.i.d. uniformly at random from \([r]\). First, note that the load property holds: for any \(j[K]\) and \(y N^{-}(x)\), we have \([C_{x}^{(j)}=C_{y}^{(j)}]=1/r\). By Fact 7, we have \(|N^{-}(x)| 2r\), and by the Chernoff bound, color \(C_{x}^{(j)}\) occurs no more than \(O( n)\) times in \(N^{-}(x)\) w.h.p. By the union bound, the load property holds w.h.p. for all \(j\).

Next, for any fixed \(x V\), \(y N^{-}(x)\), and \(j[K]\), let \(A^{(j)}(x,y)\) be the event that \(y\) is the only point in \(N^{-}(x)\) such that \(C_{x}^{(j)}=C_{y}^{(j)}\). Since the colorings are selected uniformly at random, we have \([A^{(j)}(x,y)]=(1/r)\). Since \(K=O(r n)\), by Chernoff, w.h.p. there exists \(j[K]\) such that \(A^{(j)}(x,y)\) occurs. By the union bound, the collection property holds w.h.p. 

**Definition 16** (NCC-Embedding).: _Given a graph \(G\) and an NCC scheme, an NCC-embedding is an embedding of dimension \(O(r^{2}^{2}n)\) of the following form. Associate each color \(i[r]\) with \(M=O( n)\) unique basis vectors \((i)=\{e_{(i-1)M+1},e_{(i-1)M+2},,e_{iM}\}\). The embedding of point \(x\) is comprised of \(K\) parts \(^{(1)},,^{(K)}\), where each part is a basis vector \(^{(j)}(C_{x}^{(j)})\), i.e. \(^{(j)}\) is one of the basis vectors associated with color \(C_{x}^{(j)}\)._

**Lemma 17**.: _Let \(D E\{0,1\}\) be a mapping of each edge, with \(1\) meaning "close" and \(0\) meaning "far". For each \(x V\) there exists embedding \(\) into \(O(r^{2}^{2}n)\) dimensions such that for any \(\{x,y\} E\), it holds that \((,)=K-D(x,y)\)._

Proof.: Let \((C^{(1)},,C^{(K)})\) be an NCC scheme of \(G\). We embed the points one by one according to the arboricity ordering \(x_{1},,x_{n}\) as in Fact 7. We assume by induction that all nodes \(x_{1},,x_{i-1}\) are embedded using an NCC-embedding. For each \(y N^{-}(x)\), fix one index \(j(y)\) such that under \(C^{(j(y))}\) the points \(x,y\) have the same color, which is different from colors of other points from \(N^{-}(x)\) (such \(j(y)\) exists by the collection property). Let \(J=\{j(y) y N^{-}(x)\}\), and, since for any two points in \(N^{-}(x)\) the chosen index is distinct, \(|J|=|N^{-}(x)|\).

For each part \(j[K][J]\), we choose \(x^{(j)}\) to be a basis vector from \((C_{x}^{(j)})\) that is different from all basis vectors \(\{^{(j)} y N^{-}(x)\}\). This can be done, since, on the one hand, for each \(C_{x}^{(j)} C_{y}^{(j)}\), all basis vectors of \((C_{x}^{(j)})\) are different from \(^{(j)}\), and, on the other hand, by the load property there are less than \(O( n)\) points \(y N^{-}(x)\) such that \(C_{x}^{(j)}=C_{y}^{(j)}\). Therefore, we can choose a basis vector that is different from any taken by these \(O( n)\) points.

For each part \(j(y)[J]\), we select the basis vector based on \(D\). If \(D(x,y)=1\), then we take \(^{(j(y))}=^{(j(y))}\). Otherwise, we pick a basis vector \(^{(j(y))}(C_{x}^{(j(y))})\) such that \(^{(j(y))}^{(j(y))}\).

We now show that distance between embeddings is \(2(K-1)\) if the points are close, and is \(2K\) otherwise. The result follows by scaling the embedding. Let \(\{x,y\} E\) such that \(y N^{-}(x)\). Let \(I^{(j)}(x,y)=1\) if \(^{(j)}^{(j)}\), and \(I^{(j)}(x,y)=0\) otherwise. Since each part is a basis vector, \((,)=2_{j[K]}I^{(j)}(x,y)\). By construction, for any \(j[J]\{j(y)\}\) it holds that \(I^{(j)}(x,y)=1\). For \(j(y)\) we have \(I^{(j(y))}(x,y)=1-D(x,y)\), i.e. \((,)=2(K-D(x,y))\). Rescaling the embedding vectors by a factor of \(1/2\) completes the proof. 

**Corollary 18**.: _Let \(a^{}\) be a power of \(2\) such that for all \(\{x,y\} E\) we have \(a_{x,y}\{0,,a^{}\}\). Then there exists an embedding \(F\) of \(V\) into \(O(r^{2}^{2}n a^{})\) dimensions such that for any \(\{x,y\} E\), we have \((F(x),F(y))=K(a^{}-1)-a_{x,y}\)._

Proof.: Let \(^{(i)}(x,y)\) be the \(i\)'th bit of the binary encoding of \(a_{x,y}\) using a string of size \(_{2}a^{}\) bits. Let \(F_{1},,F_{_{2}a^{}}\) be embeddings as in Lemma 17, where for each \(F_{i}\) we choose \(^{(i)}(x,y)\). For embedding \(F(x)=(F_{1}(x),2F_{2}(x),,2^{i}F_{i}(x),,(a^{}/2)F_{_{2}a^ {}}(x))\) we have

\[(F(x),F(y)) =_{i=1}^{_{2}a^{}}K-^{(i) }(x,y) 2^{i-1}\] \[=K_{i=1}^{_{2}a^{}}2^{i-1}-_{i=1}^{_{2}a^{ }}^{(i)}(x,y) 2^{i-1}=K(a^{}-1)-a_{x,y}.\]

Theorem 10 follows immediately from Corollary 18 by taking \(a_{x,y}=w(x,y)\) and \(a^{} m^{}\).

### Fully Preserving \(\)-\(\)

In this section, we prove Theorem 5, which states the existence of an embedding with dimension \(d=O(k^{10}^{10}n)\) that preserves the \(k\)-NN. Our approach can be summarized as follows: for each \(x V\), the final embedding is \(F(x)=(2m,)\) (Figure 3). The goal of \(\) is to have all non-neighbors \(\{x^{},y^{}\} E\) be at a larger distance than any neighbors \(\{x,y\} E\), i.e. for some large \(W\) it holds that \((,)+W<(^{},^{})\). The goal of \(\) is to order the distances of neighboring pairs \(\{x,y\} E\) according to their rank, while still keeping non-neighbors further away than neighbors.

We choose \(^{(j)}\) via a random process, so that for any two neighbors \(\{x,y\} E\) we have \(^{(j)}=^{(j)}\) with some probability \(p_{1}\) (and otherwise they have substantial distance), while for non-neighbors \(\{x,y\} E\), we have \(^{(j)}=^{(j)}\) with much smaller probability \(p_{2} p_{1}\). Repeating this process, we get a separation in distances between neighbors and non-neighbors.

Choosing \(\):The embedding \(\) is comprised of \(L=(r^{4}^{4}n)\) parts, i.e. \(=(^{(1)},,^{(L)})\). We take each part \(^{(j)}\) to be a vector from a _design_ - a large family of vectors which are approximately equidistant.

**Definition 19** (\((,R)\)-design).: _For integer \(R\) and value \(0<<1\), an \((,R)\)-design is a family of sets \(\), such that (a) for each \(S_{i}\), \(S_{i}[R^{2}]\), (b) for each \(S_{i}\), \(|S_{i}|=R\), and (c) for each two distinct sets \(S_{i},S_{j}\), \(|S_{i} S_{j}| R\)._

**Lemma 20** (Lemma 1, ).: _For any sufficiently large integer \(R\) and any value \(0<<1\), there exists an \((,R)\)-design \(\) of size at least \(2^{ R_{2}R}\)._

Let \(\) be a \((,R)\)-design for \(R=(r^{3}^{3}n)\) and \(=( n/R)\), where \(r\) is arboricity of the constraint graph (constants specified below). We associate \(S\) with a binary vector \(I(S)\{0,1\}^{R^{2}}\) as an indicator vector of the set \(S\), i.e. for \(i[R^{2}]\) we have \(I(S)[i]=1\) iff \(i S\). For each \(x V\), we choose unique sets \(S_{x},S_{x}^{}\) and denote \(I_{x}=I(S_{x}),I_{x}^{}=I(S_{x}^{})\). By Lemma 20, the number of sets is \(2^{ R_{2}R}=2^{( n)}\), exceeding \(2n\) for appropriate choices of constants.

We choose each part \(^{(j)}\) independently of the rest as follows. For \(p=O(1/(r n))\), with probability \(1-p\), we choose \(^{(j)}=I_{x}\), and otherwise choose a uniformly random \(i[2r]\). If \(i|N^{-}(x)|\), set \(^{(j)}=I_{y}\), where \(y N^{-}(x)\) is the \(i\)'th point in \(N^{-}(x)\) according to some ordering, and set \(^{(j)}=I_{x}^{}\) otherwise. Let \(=\) be the probability that \(x\) and \(y\) choose \(I_{y}\).

Importantly, in this construction, neighbors are significantly more likely to sample the same vector compared with non-neighbors. Moreover, sampling the same vector contributes \(0\) to the distance between embedding, while sampling different vectors contributes at least \((2-)R\) to the distance. For \(K\) is defined as in Definition 14, let \(c=(,)\) be a constant, and set \(n}\) and \(R=_{2}n/\). In Appendix C.1 we justify these choices of parameters and show the following.

Figure 3: Structure of embedding for fully preserving \(\)-\(\). \(\) guarantees that non-edges have very large distance, i.e. if \(\{x,y\} E\) and \(\{x^{},y^{}\} E\), then \((,)(^{},^{})\). \(\) orders the edges.

**Lemma 21**.: _With high probability, the following bounds hold._

* _If_ \(\{x,y\} E\)_, then_ \(|(,)-2RL|n} RL\)__
* _If_ \(\{x,y\} E\)_, then_ \(|(,)-2(1-)RL|n} RL\)_._

_That is, according to the embedding, the gap between neighbors' distances and non-neighbor' distances is larger than the maximum difference between neighbors' distances._

The final dimension is \(O(r^{10}^{10}n)\): \(L=(r^{4}^{4}n)\) parts of dimension \(R^{2}=(r^{6}^{6}n)\). Since \(r=O(k)\) (Lemma 27), it follows that the dimension is bounded by \(O(k^{10}^{10}n)\).

Final EmbeddingLet \(_{1},,_{n}\) be the embeddings from Corollary 18 with \(a^{}\) being the closest power of two from above of the expression \(RL\). These embeddings have dimension at most \(O(r^{2}^{2}n a^{})=O(r^{2}^{3}n)\). For \(\{x,y\} E\), let \((x,y)= 2m((,)-2(1--)RL).\) Set \(a_{x,y}=(x,y)+w(x,y)\), where \(w(x,y)\) is the ranking of edge \(\{x,y\}\) if the edges are sorted by the decreasing order of distances. By Lemma 21, we have \(0(x,y)RL\) w.h.p., and hence \(a_{x,y}RL+m a^{}\). Finally, \(F(x)=(2m,)\).

Proof of Theorem 5.: For each \(x V\), let \(F(x)=(2m,)\). It suffices to show the following.

1. For any \(\{x,y\} E\) and \(\{x^{},y^{}\} E\), it holds that \(w(x,y)<w(x^{},y^{})\) if and only if \((F(x),F(y))>(F(x^{}),F(y^{}))\).
2. For any \(\{x,y\} E\) and \(\{x^{},y^{}\} E\), it holds that \((F(x),F(y))<(F(x^{}),F(y^{}))\).

By Corollary 18, for any \(\{x,y\} E\):

\[(F(x),F(y)) =K(a^{}-1)- 2m((,)-2 (1--)RL)-w(x,y)+2m (,)\] \[=K(a^{}-1)+4m(1--)RL-w( x,y)-_{x,y},\] (1)

where \(_{x,y}[0,1)\) is the rounding error. Hence, property (a) holds: if \(w(x,y)<w(x^{},y^{})\) then \((F(x),F(y))>(F(x^{}),F(y^{}))\), and vice versa, since the comparison is defined by ranking. The property (b) holds since for any \(\{x^{},y^{}\} E\) and \(\{x,y\} E\):

\[(F(x^{}),F(y^{})) (2m^{},2m^{}) 4m(1- )RL\] \[ K(a^{}-1)+4m(1--)RL> (F(x),F(y)),\]

where the second inequality follows from Lemma 21, and the third inequality follows from \(K(a^{}-1) 4 mRL\), which holds: since \(a^{}-1 mRL\), it suffices to have \(Kr n\), which indeed holds for our choice of \(c=(,)\). 

## 4 Experiments

We perform experiments on CIFAR-10 and CIFAR-100 image datasets  (we show additional experiments in Appendix A). We define the ground-truth distance between points as the distance between their embedding vectors produced by a pretrained ResNet-18 neural network. Let \(Q\) be contrastive triplets sampled uniformly at random from all possible triplets of images, labeled based on the ground-truth distance. Then, we train a different ResNet-18 model from scratch, where we control the embedding dimension by replacing the last fully-connected layer with a fully-connected layer with the chosen output dimension. We train the model for \(50\) epochs on a single NVIDIA A100 GPU using triplet loss : \(_{F}(x,y^{+},z^{-})= F(x)-F(y)^{2}- F (x)-F(z)^{2}+1\). Since our goal is to find an embedding of this set of queries, we evaluate the accuracy as the fraction of satisfied contrastive samples.

We present our results in Figure 4. In experiments, we vary the number of samples (Figures 3(a) and 3(b)) and the dimension (Figures 3(c) and 3(d)). Figures 3(a) and 3(b) show that, while \(d\)the resulting embedding is consistent with almost all (\( 99\%\)) triplets. On the other hand, for \(m\{10^{5},10^{6}\}\), \(d\) is substantially less than \(\), and the number of satisfied samples sharply drops from \(99\%\) to \(93\%\). This is consistent with our theoretical results in Theorem 1.

Not surprisingly, Figures 3(c) and 3(d) show that, when the embedding dimension increases, so does the accuracy, i.e. the number of satisfied triplets. But the accuracy stops increasing when the dimension reaches approximately \( 316\) - while there is a \(2\%\) accuracy increase when the dimension changes from \(64\) to \(256\), there is no accuracy increase when the dimension changes from \(256\) to \(1024\). This again conforms with our result from Theorem 1.

## 5 Conclusion

In this paper, we provide bounds on the necessary and sufficient dimension to represent a collection of contrastive constraints of the form "distance from \(x\) to \(y\) is smaller than distance from \(x\) to \(z\)". This is a fundamental question in machine learning theory, since it educates the choice of deep learning architectures by providing guidance for the size of the embedding layer. Our experiments illustrate the predictive power of our theoretical findings in the context of deep learning. We also believe that it gives rise to many interesting directions for future work depending on the exact desiderata: approximate versions, different choices of normed spaces, bi-criteria algorithms, agnostic settings.

While the considered distance comparison settings play a central role in contrastive learning and nearest neighbor search, so far there has been no theoretical studies of their embedding dimension. Our work is the first to present a series of such upper and lower bounds in a variety of settings via a novel connection to the notion of arboricity from graph theory. As a follow-up, one can consider an improved embedding construction for \(NN}\): in the upped bound from Section 3, the dependence on both \( n\) and \(k\) can likely can be improved. Another interesting direction is tighter data-dependent bounds on dimension: while we provide fine-grained bounds in terms of arboricity - which are potentially much stronger than bounds in terms of the number of edges - they don't necessary capture properties of dataset which can lead to sharper bounds.

Figure 4: Experiments on CIFAR-10 (left) and CIFAR-100 (right). The data points show the average over \(5\) runs, and the shaded area shows the minimum and the maximum values over the runs