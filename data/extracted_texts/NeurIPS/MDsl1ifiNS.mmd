# Robust Offline Active Learning on Graphs

Yuanchen Wu

Department of Statistics

The Pennsylvania State University

yqw5734@psu.edu &Yubai Yuan

Department of Statistics

The Pennsylvania State University

yvy5509@psu.edu

###### Abstract

We consider the problem of active learning on graphs for node-level tasks, which has crucial applications in many real-world networks where labeling node responses is expensive. In this paper, we propose an offline active learning method that selects nodes to query by explicitly incorporating information from both the network structure and node covariates. Building on graph signal recovery theories and the random spectral sparsification technique, the proposed method adopts a two-stage biased sampling strategy that takes both _informativeness_ and _representativeness_ into consideration for node querying. _Informativeness_ refers to the complexity of graph signals that are learnable from the responses of queried nodes, while _representativeness_ refers to the capacity of queried nodes to control generalization errors given noisy node-level information. We establish a theoretical relationship between generalization error and the number of nodes selected by the proposed method. Our theoretical results demonstrate the trade-off between _informativeness_ and _representativeness_ in active learning. Extensive numerical experiments show that the proposed method is competitive with existing graph-based active learning methods, especially when node covariates and responses contain noises. Additionally, the proposed method is applicable to both regression and classification tasks on graphs.

## 1 Introduction

In many graph-based semi-supervised learning tasks for node-level prediction, labeled nodes are scarce, and the labeling process often incurs high costs in real-world applications. Randomly sampling nodes for labeling can be inefficient, as it overlooks label dependencies across the network. Active learning  addresses this issue by selecting informative nodes for labeling by human annotators, thereby improving the performance of downstream prediction algorithms.

Active learning is closely related to the optimal experimental design principle  in statistics. Traditional optimal experimental design methods select samples to maximize a specific statistical criterion [26; 19]. However, these methods often are not designed to incorporate network structure, therefore inefficient for graph-based learning tasks. On the other hand, selecting informative nodes on a network is studied extensively in the graph signal sampling literature [11; 18; 10; 28]. These strategies are typically based on the principle of _network homophily_, which assumes that connected nodes tend to have similar labels. However, a node's label often also depends on its individual covariates. Therefore, signal-sampling strategies that focus solely on network information may miss critical insights provided by covariates.

Recently, inspired by the great success of graph neural networks (GNNs) [16; 34] in graph-based machine learning tasks, many GNN-based active learning strategies have been proposed. Existing methods select nodes to query by maximizing _information gain_ under different criteria, including information entropy , the number of influenced nodes , prediction uncertainty , expected error reduction , and expected model change . Most of these information gain measurementsare defined in the spatial domain, leveraging the message-passing framework of GNNs to incorporate both network structure and covariate information. However, their effectiveness in maximizing learning outcomes is not guaranteed and can be difficult to evaluate. This challenge arises from the difficulty of quantifying node labeling complexity in the spatial domain due to intractable network topologies. While complexity measures exist for binary classification over networks , their extension to more complex graph signals incorporating node covariates remains unclear. This lack of well-defined complexity measures complicates performance analysis and creates a misalignment between graph-based information measurements and the gradient used to search the labeling function space, potentially leading to sub-optimal node selection.

Moreover, from a practical perspective, most of the previously discussed methods operate in an online setting, requiring prompt labeling feedback from an external annotator. However, this online framework is not always feasible when computational resources are limited  or when recurrent interaction between the algorithm and the annotator is impractical, such as in remote sensing or online marketing tasks [32; 35]. Additionally, both network data and annotator-provided labels may contain measurement errors. These methods often fail to account for noise in the training data , which can significantly degrade the prediction performance of models on unlabeled nodes [7; 21].

To address these challenges, we propose an offline active learning on graphs framework for node-level prediction tasks. Inspired by the theory of graph signal recovery [11; 18; 28] and GNNs, we first introduce a graph function space that integrates both node covariate information and network topology. The complexity of the node labeling function within this space is well-defined in the graph spectral domain. Accordingly, we propose a query information gain measurement aligned with the spectral-based complexity, allowing our strategy to achieve theoretically optimal sample complexity.

Building on this, we develop a greedy node query strategy. The labels of the queried nodes help identify orthogonal components of the target labeling function, each with varying levels of smoothness across the network. To address data noise, the query procedure considers both _informativeness_--the contribution of queried nodes in recovering non-smooth components of a signal--and _representativeness_--the robustness of predictions against noise in the training data. Compared to existing methods, the proposed approach provides a provably effective strategy under general network structures and achieves higher query efficiency by incorporating both network and node covariate information.

The proposed method identifies the labeling function via a bottom-up strategy--first identifying the smoother components of the labeling function and then continuing to more oscillated components. Therefore, the proposed method is naturally robust to high-frequency noise in node covariates. We provide a theoretical guarantee for the effectiveness of the proposed method in semi-supervised learning tasks. The generalization error bound is guaranteed even when the node labels are noisy. Our theoretical results also highlight an interesting trade-off between informativeness and representativeness in graph-based active learning.

## 2 Preliminaries

We consider an undirected, weighted, connected graph \(=\{,\}\), where \(=\{1,2,,n\}\) is the set of \(n\) nodes, and \(^{n n}\) is the symmetric adjacency matrix, with element \(a_{ij} 0\) denoting the edge weight between nodes \(i\) and \(j\). The degree matrix is defined as \(=\{d_{1},d_{2},,d_{n}\}\), where \(d_{i}=_{1 i n}a_{ij}\) denotes the degree of node \(i\). Additionally, we observe the node response vector \(^{n 1}\) and the node covariate matrix \(=(X_{1},,X_{p})^{n p}\), where the \(i^{th}\) row, \(_{i,}\) is the \(p\)-dimensional covariate vector for node \(i\). The linear space of all linear combinations of \(\{X_{1},,X_{p}\}\) is denoted as \(\{X_{1},,X_{p}\}\). The normalized graph Laplacian matrix is defined as \(=-^{-1/2}^{-1/2}\), where \(\) is the \(n n\) identity matrix. The matrix \(\) is symmetric and positive semi-definite, with \(n\) real eigenvalues satisfying \(0=_{1}_{2}_{n} 2\), and a corresponding set of eigenvectors denoted by \(=\{U_{1},U_{2},,U_{n}\}\). We use \(b=(a)\) to indicate \(|b| M|a|\) for some \(M>0\). For a set of nodes \(\), \(||\) indicates its cardinality, and \(S^{c}=\) denotes the complement of \(\).

### Graph signal representation

Consider a graph signal \(^{n}\), where \((i)\) denotes the signal value at node \(i\). For a set of nodes \(S\), we define the subspace \(_{}:=\{^{n}(S^{c})=0\}\), where \((S)^{||}\) represents the values of on nodes in \(\). In this paper, we consider both regression tasks, where \((i)\) is a continuous response, and classification tasks, where \((i)\) is a multi-class label.

Since \(\) serves as a set of bases for \(^{n}\), we can decompose \(\) in the graph spectral domain as \(=_{j=1}^{n}_{}(_{j})U_{j}\), where \(_{}(_{j})=,U_{j}\) is defined as the graph Fourier transform (GFT) coefficient corresponding to frequency \(_{j}\). From a graph signal processing perspective, a smaller eigenvalue \(_{k}\) indicates lower variation in the associated eigenvector \(U_{k}\), reflecting smoother transitions between neighboring nodes. Therefore, the smoothness of \(\) over the network can be characterized by the magnitude of \(_{}(_{j})\) at each frequency \(_{j}\). More formally, we measure the signal complexity of \(\) using the bandwidth frequency \(_{}=\{_{j}|_{}(_{j})>0\}\). Accordingly, we define the subspace of graph signals with a bandwidth frequency less than or equal to \(\) as \(_{}:=\{^{n}_{}\}\). It follows directly that \(_{1}<_{2},_{_{1}}_{ _{2}}\).

### Active semi-supervised learning on graphs

The key idea in graph-based semi-supervised learning is to reconstruct the graph signal \(\) within a function space \(_{}(,)\) that depends on both the network structure and node-wise covariates, where the frequency parameter \(\) controls the size of the space to mitigate overfitting. Assume that \(Y_{i}\) is the observed noisy realization of the true signal \((i)\) at node \(i\), active learning operates in a scenario where we have limited access to \(Y_{i}\) on only a subset of nodes \(\), with \(||<<n\). The objective is to estimate \(\) within \(_{}(,)\) using \(\{Y_{i}\}_{i}\) by considering the empirical estimator of \(\) as

\[_{}=*{arg\,min}_{_{ }(,)}_{i}lY_{i}, (i),\] (1)

where \(l()\) is a task-specific loss function. We denote \(^{*}\) as the minimizer of (1) when responses on all nodes are available, i.e., \(^{*}=_{}\). The goal of active semi-supervised learning is to design an appropriate function space \(_{}(,)\) and select an _informative_ subset of nodes \(\) for querying responses, under the query budget \(||\), such that the estimation error is bounded as follows:

\[\|_{}-^{*}\|_{2}^{2}\|^{*}- \|_{2}^{2}\]

For a fixed \(\), we wish to minimize the parameter \(>0\), which converges to \(0\) as the query budget \(\) approaches \(n\).

## 3 Biased Sequential Sampling

In this section, we introduce a function space for recovering the graph signal. Leveraging this function space, we propose an offline node query strategy that integrates criteria of both node _informativeness_ and _representativeness_ to infer the labels of unannotated nodes in the network.

### Graph signal function space

In semi-supervised learning tasks on networks, both the network topology and node-wise covariates are crucial for inferring the graph signal. To effectively incorporate this information, we propose a function class for reconstructing the graph signal that lies at the intersection of the graph spectral domain and the space of node covariates. Motivated by the graph Fourier transform, we define the following function class:

\[_{}(,)= _{_{}}():=\{_{_{}}X_{1},,_{_{ }}X_{p}\},\] \[_{_{}}X_{i}=_{j:_{j} } X_{i},U_{j} U_{j}.\]

Here, the choice of \(\) balances the information from node covariates and network structure. When \(=2\), \(_{}(,)\) spans the full column space of covariates, i.e., \(\{X_{1},,X_{p}\}\), allowing for a full utilization of the original covariate space to estimate the graph signal, but without incorporating any network information. On the other hand, when \(\) is close to zero--consider, for example, the extreme case where \(|\{U_{j}_{j}\}|=2\) and \(p 2\)--then \(_{_{}}X_{i}\) reduces to \(\{U_{1},U_{2}\}\), resulting in a loss of critical information provided by the original \(\).

By carefully choosing \(\), however, this function space can offer two key advantages for estimating the graph signal. From a signal recovery perspective, \(_{}(,)\) imposes graph-based regularization over node covariates, enhancing generalizability when the dimension of covariates \(p\) exceeds the query budget or even the network size--conditions commonly encountered in real applications. Additionally, covariate smoothing filters out signals in the covariates that are irrelevant to network-based prediction, thereby increasing robustness against potential noise in the covariates. From an active learning perspective, using \(_{}(,)\) enables a bottom-up query strategy that begins with a small \(\) to capture the smoothest global trends in the graph signal. As the labeling budget increases, \(\) is adaptively increased to capture more complex graph signals within the larger space \(_{}(,)\).

The graph signal \(\) can be approximated by its projection onto the space \(_{}(,)\). Specifically, let \(_{d}=\{U_{1},U_{2},,U_{d}\}^{n d}\) stack the \(d\) leading eigenvectors of \(\), where \(d=*{arg\,max}_{1 j n}(_{j}-) 0\). The graph signal estimation is then given by \(_{d}_{d}^{T}\), where \(^{d}\) is a trainable weight vector. However, the parameters \(\) may become unidentifiable when the covariate dimension \(p\) exceeds \(d\). To address this issue, we reparameterize the linear regression as follows:

\[_{d}_{d}^{T}=},\] (2)

where \(= V_{T}^{2}\) and \(}=_{d}V_{1}\). Here, \(V_{1}^{d r}\), \(V_{2}^{p r}\), and \(^{r r}\) denote the left and right singular vectors and the diagonal matrix of the \(r\) singular values, respectively.

In the reparameterized form \(()\), the columns of \(}\) serve as bases for \(_{}(,)\), thus \((_{}(,))=(})=r\{d,p\}\). The transformed predictors \(}\) capture the components of the node covariates constrained within the low-frequency graph spectrum. A graph signal \(_{}(,)\) can be parameterized as a linear combination of the columns of \(}\), with the corresponding weights \(\) identified via

\[=*{arg\,min}_{}_{i} ((i)-(}_{})_{i}.\ )^{2}\] (3)

where \(}_{}^{|| r}\) is the submatrix of \(}\) containing rows indexed by the query set \(\), and \(\{(i)\}_{i}\) represents the true labels for nodes in \(\). To achieve the identification of \(\), it is necessary that \(|| r\); otherwise, there will be more parameters than equations in (3). More importantly, since \((}_{})(} )=r\), \(\) is only identifiable if \(}_{}\) has full column rank. Notice that \(r\) increases monotonically with \(\). If \(\) is not carefully selected, the graph signal can only be identified in \(_{^{}}(,)\) for some \(^{}<\), which is a subspace of \(_{}(,)\).

### Informative node selection

We first define the identification of \(_{}(,)\) by the node query set \(\) as follows:

**Definition 1**: _A subset of nodes \(\) can identify the graph signal space \(_{}(,)\) up to frequency \(\) if, for any two functions \(_{1},_{2}_{}(,)\) such that \(_{1}(i)=_{2}(i)\) for all \(i\), it follows that \(_{1}(j)=_{2}(j)\) for all \(j\)._

Intuitively, the informativeness of a set \(\) can be quantified by the frequency \(\) corresponding to the space \(_{}(,)\) that can be identified. To select informative nodes, we need to bridge the query set \(\) in the spatial domain with \(\) in the spectral domain. To achieve this, we consider the counterpart of the function space \(_{}(,)\) in the spatial domain. Specifically, we introduce the projection space with respect to a subset of nodes \(\) as follows: \(_{}(,):=\{X_{1}^{( )},,X_{p}^{()}\}\), where

\[X_{p}^{()}(i)=X_{p}(i)&i\\ &i^{c}\]

Here, \(X_{p}(i)\) denotes the \(p\)-th covariate for node \(i\) in \(\). Theorem 3.1 establishes a connection between the two graph signal spaces \(_{}(,)\) and \(_{}(,)\), providing a metric for evaluating the informativeness of querying a subset of nodes on the graph.

**Theorem 3.1**: _Any graph signal \(_{}(,)\) can be identified using labels on a subset of nodes \(\) if and only if:_

\[<():=_{_{^{c}}( ,)}_{},\] (4)

_where \(^{c}\) denotes the complement of \(\) in \(\)._We denote the quantity \(()\) in (4) as the _bandwidth frequency_ with respect to the node set \(\). This quantity can be explicitly calculated and measures the size of the space \(_{}(,)\) that can be recovered from the subset of nodes \(\). The goal of the active learning strategy is to select \(\) within a given budget to maximize the bandwidth frequency \(()\), thus enabling the identification of graph signals with the highest possible complexity.

To calculate the bandwidth frequency \(()\), consider any graph signal \(\) and its components with non-zero frequency \(_{}:=\{_{i}_{}(_{i})>0\}\). We use the fact that

\[_{k}(_{j:_{j}_{}}c_{j}_ {j}^{k})^{1/k}=_{_{j}_{}}(_{j}),\]

where \(_{j:_{j}_{}}c_{j}=1\) and \(0 c_{j} 1\). Combined with the Rayleigh quotient representation of eigenvalues, the bandwidth frequency \(_{}\) can be calculated as

\[_{}=_{k}_{}(k), _{}(k)=(^{T}^{k}}{^{T}})^{1/k}.\]

As a result, we can approximate the bandwidth \(_{}\) using \(_{}(k)\) for a large \(k\). Maximizing \(()\) over \(\) then transforms into the following optimization problem:

\[=*{arg\,max}_{:||}(),():= _{_{^{c}}(,)} _{}^{k}(k),\] (5)

where \(\) represents the budget for querying labels. Due to the combinatorial complexity of directly solving optimization problem (5) by simultaneously selecting \(\), we propose a greedy selection strategy as a continuous relaxation of (5).

The selection procedure starts with \(=\) and sequentially adds one node to \(\) that maximizes the increase in \(()\) until the budget is reached. We introduce an \(n\)-dimensional vector \(=(t_{1},t_{2},,t_{n})^{T}\) with \(0 t_{i} 1\), and define the corresponding diagonal matrix \(D()\) with diagonal entries given by \(\). This allows us to encode the set of query nodes using \(=_{}\), where \(_{}(i)=1\) if \(i\) and \(_{}(i)=0\) if \(i^{c}\). We then consider the space spanned by the columns of \(D()\) as \(\{D()\}\), and the following relation holds:

\[_{^{c}}(,)=\{D(_{^{c}})\}.\]

Intuitively, \(\{D()\}\) acts as a differentiable relaxation of the subspace \(_{}(,)\), enabling perturbation analysis of the bandwidth frequency when a new node is added to \(\). The projection operator associated with \(\{D()\}\) can be explicitly expressed as

\[()=D()(^{T}D(^{2}))^{-1}^{T}D().\]

To quantify the increase in \(()\) when adding a new node to \(\), we consider the following regularized optimization problem:

\[_{}()=_{}^{k}}{ ^{T}}+(-() )}{^{T}}.\] (6)

The penalty term on the right-hand side of (6) encourages the graph signal \(\) to remain in \(_{^{c}}(,)\). As the parameter \(\) approaches infinity and \(=_{^{c}}\), the minimization \(_{}(_{^{c}})\) in (6) converges to \(()\) in (5). The information gain from labeling a node \(i^{c}\) can then be quantified by the gradient of the bandwidth frequency as \(t_{i}\) decreases from 1 to 0:

\[_{i}:=-()}{ t_{i}} _{=_{^{c}}}=2^{T}()}{ t_{i}}_{= _{^{c}}},\] (7)

where \(\) is the minimizer of (6) at \(=_{^{c}}\), which corresponds to the eigenvector associated with the smallest non-zero eigenvalue of the matrix \((_{^{c}})^{k}(_{ ^{c}})\). We then select the node \(i=*{arg\,max}_{j^{c}}_{j}\) and update the query set as \(=\{i\}\). The explicit representation of \(_{i}\) in (7) can be found in Appendix B.5.

### Representative node selection

In real-world applications, we often have access only to a perturbed version of the true graph signals, denoted as \(Y=+\), where \(\) represents node labeling noise that is independent of the network data. When replacing the true label \((i)\) with \(Y(i)\) in (3), this noise term introduces both finite-sample bias and variance in the estimation of the graph signal \(\). As a result, we aim to query nodes that are sufficiently _representative_ of the entire covariate space to bound the generalization error. To achieve this, we introduce principled randomness into the deterministic selection procedure described in Section 3.2 to ensure that \(\) includes nodes that are both informative and representative. The modified graph signal estimation procedure is given by:

\[=*{arg\,min}_{}_{i}s_{i} Y(i)-(}_{})_{i}.\ ^{2},\] (8)

where \(s_{i}\) is the weight associated with the probability of selecting node \(i\) into \(\).

Specifically, the generalization error of the estimator in (8) is determined by the smallest eigenvalue of \(}_{}^{T}}_{}\), denoted as \(_{}(}_{}^{T}}_{ })\). Given that \(_{}(}^{T}})=1\), our goal is to increase the representativeness of \(\) such that \(_{}(}_{}^{T}}_{ })\) is lower-bounded by:

\[_{}(}_{}^{T}}_{ })(1-o_{||}(1))_{}(}^{T}}).\] (9)

However, the informative selection method in Section 3.2 does not guarantee (9). To address this, we propose a sequential biased sampling approach that balances informative node selection with generalization error control.

The key idea to achieve a lower bound for \(_{}(}_{}^{T}}_{ })\) is to use spectral sparsification techniques for positive semi-definite matrices . Let \(v_{i}^{1 r}\) denote the \(i\)-th row of the constrained basis \(}\). By definition of \(}\), it follows that \(_{r r}=_{i=1}^{n}v_{i}^{T}v_{i}\). Inspired by the randomized sampling approach in , we propose a biased sampling strategy to construct \(\) with \(|| n\) and weights \(\{s_{i}>0,i\}\) such that \(_{i}s_{i}v_{i}^{T}v_{i}\). In other words, the weighted covariance matrix of the query set \(\) satisfies \(_{}(}_{}^{T}W_{S}}_{ }) 1\), where \(W_{S}\) is a diagonal matrix with \(s_{i}\) on its diagonal.

We outline the detailed sampling procedure as follows. After the \((t-1)^{}\) selection, let the set of query nodes be \(_{t-1}\) with corresponding node-wise weights \(_{t-1}=\{s_{j}>0\ |\ j_{t-1}\}\). The covariance matrix of \(_{t-1}\) is given by \(C_{t-1}^{r r}\), defined as \(C_{t-1}=}_{_{t-1}}^{T}}_{ _{t-1}}=_{j_{t-1}}s_{j}v_{j}^{T}v_{j}\). To analyze the behavior of eigenvalues as the query set is updated, we follow  and introduce the potential function:

\[_{t-1}=[(u_{t-1}I-C_{t-1})^{-1}]+[(C_{t-1}-l_{t-1}I)^{-1 }],\] (10)

where \(u_{t-1}\) and \(l_{t-1}\) are constants such that \(l_{t-1}<_{}(C_{t-1})_{}(C_{t-1})<u_{t-1}\), and \(()\) denotes the trace of a matrix. The potential function \(_{t-1}\) quantifies the coherence among all eigenvalues of \(C_{t-1}\). To construct the candidate set \(B_{m}\), we sample node \(i\) and update \(C_{t}\), \(u_{t}\), and \(l_{t}\) such that all eigenvalues of \(C_{t}\) remain within the interval \((l_{t},u_{t})\). To achieve this, we first calculate the node-wise probabilities \(\{p_{i}\}_{i=1}^{n}\) as:

\[p_{i}=v_{i}(u_{t-1}I-C_{t-1})^{-1}v_{i}^{T}+v_{i}(C_{t-1}-l_{t-1}I)^{-1 }v_{i}^{T}/_{t-1},\] (11)

where \(_{i=1}^{n}p_{i}=1\). We then sample \(m\) nodes into \(B_{m}\) according to \(\{p_{i}\}_{i=1}^{n}\). For each node \(i B_{m}\), the corresponding weight is given by \(s_{i}=_{t-1}},\ 0<<1\). After obtaining the candidate set \(B_{m}\), we apply the informative node selection criterion \(_{i}\) introduced in Section 3.2, i.e., selecting the node \(i=*{arg\,max}_{i B_{m}}_{i}\), and update the query set and weights as follows:

\[i_{t-1}^{c}:\ _{t}=_{t-1} \{i\},\ _{t}=_{t-1}\{s_{i}\},\] \[i_{t-1}:s_{i}=s_{i}+ _{t-1}}.\]

We then update the lower and upper eigenvalue bounds as follows:

\[u_{t}=u_{t-1}+(1-)},\ l_{t}=l_{t-1}+(1+)}.\] (12)The update rule ensures that \(u_{t}-l_{t}\) increases at a slower rate than \(u_{t}\), leading to the convergence of the gap between the largest and smallest eigenvalues of \(}_{S}^{T}W_{S}}_{}\), thereby controlling the condition number. Accordingly, the covariance matrix is updated with the selected node \(i\) as:

\[C_{t}=C_{t-1}+_{t-1}}v_{i}^{T}v_{i}.\] (13)

With the covariance matrix update rule in (13), the average increment is \((C_{t})-C_{t-1}=_{i=1}^{n}p_{i}s_{i}v_{i}^{T}v_{i}=}\). Intuitively, the selected node allows all eigenvalues of \(C_{t-1}\) to increase at the same rate on average. This ensures that \(_{}(}_{S}^{T}}_{})\) continues to approach \(_{}(}^{T}})=1\) during the selection process, thus driving the smallest eigenvalue away from zero. Additionally, the selected node remains locally informative within the candidate set \(B_{m}\). Compared with the entire set of nodes, selecting from a subset serves as a regularization on informativeness maximization, achieving a balance between informativeness and representativeness for node queries.

### Node query algorithm and graph signal recovery

We summarize the biased sampling selection strategy in Algorithm 1. At a high level, each step in the biased sampling query strategy consists of two stages. First, we use randomized spectral sparsification to sample \(m n\) nodes and collect them into a candidate set \(B_{m}\). Intuitively, the covariance matrix on the updated \(\) maintains lower-bounded eigenvalues if a node from \(B_{m}\) is added to \(\). In the second stage, we select one node from \(B_{m}\) based on the informativeness criterion in Section 3.2 to achieve a significant frequency increase in (7).

For initialization, the dimension of the network spectrum \(d\), the size of the candidate set \(m\), and the constant \(0<<1\) for spectral sparsification need to be specified. Based on the discussion at the end of Section 3.1, the dimension of the function space \(_{}(,)\) is at most \(\), where \(\) is the budget for label queries. Therefore, we can set \(d=\{p,\}\). The parameters \(m\) and \(\) jointly control the condition number \((}_{S}^{T}W_{S}}_{S})}{ _{}(}_{S}^{T}W_{S}}_{S})}\).

```
0:\(t=0\), \(C_{0}=0\), the set of query nodes \(_{0}=\), the set of node weights \(_{0}=\), spectral dimension \(d\), size of candidate set \(m\), constant \(0<<1/m\), query budget \(\). Initialization: Compute SVD decomposition \(_{d}^{T}=V_{1} V_{2}^{T}\), and set \(}=_{d}V_{1}\), \(r=(})\), \(u_{0}=2r/\), \(l_{0}=-2r/\), \(=2r(1-m^{2}^{2})/(m^{2})\). while\(>0\)do Step 1: Calculate \(_{t}\) as in (10) and the node-wise probabilities \(\{p_{i}\}_{i=1}^{n}\) using (11). Step 2: Sample \(m\) nodes with replacement according to probabilities \(\{p_{i}\}_{i=1}^{n}\) to form the candidate set \(B_{m}\). Step 3: Select node \(i\) as \(i=_{i B_{m}}_{i}\) and calculate its weight \(w_{i}=_{t}}\). If\(i_{t}\), then update \(_{t+1}=_{t}\{i\}\) and \(_{t+1}=_{t}\{s_{i}\}\) with \(s_{i}=}{}\). Else if\(i_{t}\), then update \(s_{i}=s_{i}+}{}\). Step 4: Update \(C_{t}\), \(u_{t}\), \(l_{t}\), \(\) and \(t\) as: \[C_{t+1}=C_{t}+w_{i}v_{i}^{T}v_{i}, u_{t+1}=u_{t}+(1-m)}, l_{t+1}=l_{t}+(1 +m)},\] \[=-1, t=t+1.\] endwhile Query: Label all nodes in \(\) through an external annotator. Output: Set of queried nodes \(\), annotated responses \(\{Y_{i} i\}\), smoothed covariates \(}_{}\), and weights of queried nodes \(\). ```

**Algorithm 1** Biased Sampling Query Algorithm

Based on the output from Algorithm 1, we solve the weighted least squares problem in (8):

\[=*{arg\,min}_{}_{i}s_{i} Y(i)-(}_{})_{i}.^{2},\] (14)

and recover the graph signal on the entire network as \(}=}\). The proposed method is illustrated for the regression task, with an extension to the classification task discussed in Appendix B.2.

Theoretical Analysis

In this section, we present a theoretical analysis of the proposed node query strategy. The results are divided into two parts: the first focuses on the local information gain of the selection process, while the second examines the global performance of graph signal recovery. Given a set of query nodes \(\), the information gain from querying the label of a new node \(i\) is measured as the increase in bandwidth frequency, defined as \(_{i}:=(\{i\})-()\). We provide a step-by-step analysis of the proposed method by comparing the increase in bandwidth frequency with that of random selection.

**Theorem 4.1**: _Define \(d_{}=_{i}\{d_{i}\}\), where \(d_{i}\) denotes the degree of node \(i\). Let \(\) represent the set of queried nodes prior to the \(s^{th}\) selection. Denote the adjacency matrix of the subgraph excluding \(\) as \(_{(n-||)(n-||)}\). Let \(_{s}^{}\) and \(_{s}^{}\) denote the increase in bandwidth frequency resulting from the \(s^{th}\) label query on a node selected by random sampling and the proposed sampling method, respectively. Let \(j^{*}\) denote the node with the largest magnitude in the eigenvector corresponding to the smallest non-zero eigenvalue of \(_{^{c}}\). Then we have:_

\[(_{s}^{})=()\ (1),\ \ \ \ (_{s}^{})-(_{s}^{})> (_{1}^{3}d_{}^{2}})-() \ (2),\]

_where \(f=(g)\) if \(c_{1}|| c_{2}\) for constants \(c_{1},c_{2}\) when \(n\) is sufficient large. Inequality (2) holds given \(m\) satisfying_

\[(}{n-m})^{m}(}{n-d_{min}})^{d_{min}}}=(1).\]

_The expectation \(()\) is taken over the randomness of node selection. Both \(_{0},_{1}\) are network-related quantities, where \(_{0}\#|\{i:|-d_{j^{*}}}{d_{}}| 1\}|\) and \(_{1}_{i}(}{d_{}})\)._

Theorem 4.1 provides key insights into the information gain achieved through different node label querying strategies. While random selection yields a constant average information gain, the proposed biased sampling method guarantees a higher information gain under mild assumptions.

In Theorem 4.2, we provide the generalization error bound for the proposed sampling method under the weighted OLS estimator. To formally state Theorem 4.2, we first introduce the following two assumptions:

**Assumption 1**: For the underlying graph signal \(\), there exists a bandwidth frequency \(_{0}\) such that \(_{_{0}}(,)\).

**Assumption 2**: The observed node-wise response \(Y_{i}\) can be decomposed as \(Y_{i}=(i)+_{i}\), where \(\{_{i}\}_{i=1}^{n}\) are independent random variables with \((_{i})=0\) and \((_{i})^{2}\).

**Theorem 4.2**: _Under Assumptions 1 and 2, for the graph signal estimation \(}\) obtained by training (14) on \(\) labeled nodes selected by Algorithm 1, with probability greater than \(1-\), where \(t>2m\), we have_

\[_{Y}\|}-\|_{2}^{2} t}{}+2(t}{})^{3/2}+(t}{ })^{2}(n^{2}+_{i>d,i()}_{i}^{2})+_{i>d,i()}_{i}^{2},\] (15)

_where \(_{i}:=,U_{i}\), \(():=\{i:1 i n,|_{i}|>0\}\) and \(r_{d}=(_{d}^{T})\). \(_{Y}()\) denotes the expected value with respect to the randomness in observed responses._

Theorem 4.2 reveals the trade-off between informativeness and representativeness in graph-based active learning, which is controlled by the spectral dimension \(d\). Since \(r_{d}\) is a monotonic function of \(d\), a larger \(d\) reduces representativeness among queried nodes, thereby increasing variance in controlling the condition number (i.e., the first three terms). On the other hand, a larger \(d\) reduces approximation bias to the true graph signal (i.e., the fifth and last terms) by including more informative nodes for capturing less smoothed signals.

## 5 Numerical Studies

In this section, we conduct extensive numerical studies to evaluate the proposed active learning strategy for node-level prediction tasks on both synthetic and real-world networks. For the synthetic networks, we focus on regression tasks with continuous responses, while for the real-world networks, we consider classification tasks with discrete node labels.

### Synthetic networks

We consider three different network topologies generated by widely studied statistical network models: the Watts-Strogatz model  for small-world properties, the Stochastic block model  for community structure, and the Barabasi-Albert model  for scale-free properties.

Node responses are generated as \(Y=+\), where \(\) is the true graph signal and \( N(0,^{2}I_{n})\) is Gaussian noise. The true signal is constructed as \(=_{d}\), where \(\) is the linear coefficient and \(_{d}\) denotes the leading \(d\) eigenvectors of the normalized graph Laplacian of the synthetic network. Since our theoretical analysis assumes that the observed node covariates \(\) contain noise, we generate \(\) as a perturbed version of \(_{d}\) by adding non-leading eigenvectors of the normalized graph Laplacian. The detailed simulation settings can be found in Appendix C.1.

We compare our algorithm with several offline active learning methods: 1. **D-optimal** selects subset of nodes \(\) to maximize determinant of observed covariate information matrix \(_{}^{}_{}\). 2. **RIM** selects nodes to maximize the number of influenced nodes. 3. **GPT** and **SPA** split the graph into disjoint partitions and select informative nodes from each partition.

After the node query step, we fit the weighted linear regression from (14) on the labeled nodes, using the smoothed covariates \(}\), to estimate the linear coefficient \(\) and predict the response \(}\) for the unlabeled nodes. In Figure 1, we plot the prediction MSE of the proposed method against baselines on unlabeled nodes for various levels of labeling noise \(^{2}(0.5,0.6,0.7,0.8,0.9,1)\). The results show that the proposed method significantly outperforms all baselines across all simulation settings and exhibits strong robustness to noise. The inferior performance of the baselines can be attributed to several factors. **D-optimal** and **RIM** fail to account for noise in the node covariates. Meanwhile, partition-based methods like **GPT** and **SPA** are highly sensitive to hyperparameters, such as the optimal number of partitions, which limits their generalization to networks lacking a clear community structure.

### Real-world networks

We evaluate the proposed method for node classification tasks on real-world datasets, which include five networks with varying homophily levels (_high to low:_ **Cora, PubMed, Citeseer, Chameleon and Texas**s) and two large-scale networks (**Ogbn-Arxiv** and **Co-Physics**). In addition to the offline methods described in Section 5.1, we also compare our approach with two GNN-based online active learning methods **AGE** and **IGP**. In each GNN iteration, **AGE** selects nodes to maximize a linear combination of heuristic metrics, while **IGP** selects nodes that maximize information gain propagation.

Figure 1: Prediction performance on unlabeled nodes at different levels of labeling noise (\(^{2}\)). All three simulated networks have \(n=100\) nodes, with the number of labeled nodes fixed at \(25\).

[MISSING_PAGE_FAIL:10]