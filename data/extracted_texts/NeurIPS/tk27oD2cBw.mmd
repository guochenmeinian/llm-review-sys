# The Harvard USPTO Patent Dataset:

A Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications

 Mirac Suzgun\({}^{}\) Luke Melas-Kyriazi\({}^{}\) Suproteem K. Sarkar\({}^{}\)

**Scott Duke Kominers\({}^{}\) Stuart M. Shieber\({}^{}\)**

\({}^{}\)Stanford University \({}^{}\)Oxford University \({}^{}\)Harvard University \({}^{}\)a16z crypto

\({}^{}\)Correspondence to: msuzgun@cs.stanford.edu

###### Abstract

In this paper, we introduce the Harvard USPTO Patent Dataset (HUPD), a large-scale, well-structured, and multi-purpose corpus of English-language patent applications filed to the United States Patent and Trademark Office (USPTO) between 2004 and 2018. With more than 4.5 million patent documents, HUPD is two to three times larger than comparable corpora. Unlike previously proposed patent datasets, HUPD contains the inventor-submitted versions of patent applications--not the final versions of granted patents--thereby allowing us to study patentability at the time of filing using NLP methods for the first time. It is also novel among NLP datasets in its inclusion of rich structured metadata alongside the text of patent filings, enabling researchers to perform new NLP tasks leveraging this metadata. As a case study on the types of research HUPD makes possible, we introduce a new task to the NLP community--patent acceptance prediction. Finally, we demonstrate how our dataset can be used for three additional tasks: multi-class classification of patent subject areas, language modeling, and summarization. Overall, HUPD is one of the largest multi-purpose NLP datasets containing domain-specific textual data, along with well-structured bibliographic metadata, and aims to advance research extending language and classification models to diverse and dynamic real-world data distributions.1

## 1 Introduction

Patents are key public indicators of innovation and technological advancement. They offer a simple yet powerful source for studying, measuring, and appraising innovation activity, economic growth, and emerging technology. Over the past two decades, the total number of patent applications filed to the United States Patent and Trademark Office (USPTO) per year has almost doubled. In the fiscal year 2020 alone, the USPTO received more than 650,000 patent filings, including requests for continued examinations . The competitive and regulatory landscape surrounding patent-driven innovation is rapidly evolving, but despite the clear focus in textual data in the field of patent analysis, it has yet to be systematically studied by the ML and NLP communities.

The absence of large-scale, well-structured, and distilled patent data is a major hurdle preventing researchers and practitioners from applying ML tools to understand and explore innovation and technological change through patent text. In recent years, there have been efforts to produce NLPdatasets of patent text, including CLEF-IP 2011 , USPTO-2M , and BigPatent. These datasets have some limitations in their scopes and features, as shown in Table 1. They may contain text and information only for granted patents, may contain subsets of patent text and metadata, and typically focus on only one particular NLP task.2

It is thus highly desirable to have a free, publicly-available dataset that provides a wider and more encompassing repository of patent data--covering multiple sections and years--of not just granted patents but all patent applications, that allows more flexibility and control in data selection, and that can be appropriated for multiple experiments and investigations. With these desiderata in mind, we introduce a public, large-scale, consistently-structured, and multi-purpose corpus of patent data to the NLP community, called the Harvard USPTO Patent Dataset (HUPD). The dataset contains more than 4.5 million English-language utility patent applications filed to the USPTO between 2004 and 2018, and aims to advance research efforts in both patent analysis and NLP.

HUPD distinguishes itself from prior NLP patent datasets in three key aspects. First, unlike some other datasets, it does not restrict the sample to only granted patents, and instead focuses on patent applications. Patent applications contain the original set of claims and descriptions of the proposed invention written by the applicent. Because the dataset has a consistent set of patent documents at the time of filing, it avoids the dataset shift concerns that would be present in a study of accepted and rejected patent applications at different revision stages. In fact, having access to the original versions of both accepted and rejected applications allows us to introduce a completely new task to the field--binary classification of patent decisions. The goal of this task is to predict the patent acceptance from the language of a patent application at the time of submission. Second, our dataset features multiple classes of rich textual and structural information present in patent applications. Whereas many previous NLP datasets include only one or two of a patent's data fields (for example, description and abstract), HUPD contains 34 fields, including filing date, fine-grained classification codes, examiner information, and many others. The variety of information available for each patent application can enable NLP researchers to perform a wide range of tasks--such as analyzing the evolution of patent language and categories over time--that were not possible under previous NLP patent datasets. Third, HUPD uses textual and bibliographic information obtained directly from the USPTO's data products, rather than from Google's Patent search as BigPatent does; it is larger than previous NLP patent datasets while still being clean, comprehensive, and well-structured.

We introduce our patent dataset with several audiences in mind. For the NLP community, the universe of patent applications--with its sheer breadth and wealth of textual substance and structured format--provides an ideal domain-specific laboratory for developing and evaluating new NLP tools.3 From

Figure 1: Three pages of the pre-grant version of an example patent document (_Method and Apparatus for Initiating a Transaction on a Mobile Device_ [Publication No: 2014-0207675 A1]). The highlighted sections show a subset of the 34 data fields that we include in the Harvard USPTO Patent Dataset.

abstractive summarization of patent sections to information retrieval and named-entity recognition and extraction, one can perform a variety of both standard and novel NLP tasks on patent data. Furthermore, the well-structured nature of our dataset allows for researchers to study how concepts like acceptance criteria vary across contexts and over time. For the IP community, a key motivator of our work is that there are many rote tasks associated with patent filing and examination--including the categorization of patents into relevant technology areas and prior art search--in which machine learning methods could potentially be used to provide efficiency, value, and cost savings. Finally, for the general audience, our present work illustrates how NLP/ML tools may be efficiently deployed for advancing socially relevant objectives and studying diverse and dynamic application areas.

## 2 Preliminaries and Background

A patent application4 typically consists of a title, abstract, set of claims, detailed description, drawings (if needed to describe the invention), and cross-references to related applications, among other written specifications. Applications are filed to the USPTO and reviewed by examiners who are expected to have knowledge and expertise associated with the subject matter of the invention.

During the examination process, an examiner determines the _patentability_ of the invention. The examiner decides whether the proposed invention is _useful_, _non-obvious_, and _statutory_, and searches for already-existing patents within the technology sphere of the invention to confirm the set of claims provided is _novel_. Afterwards, the examiner sends an Office action to the applicent, notifying them of the USPTO's decision. If the decision is favorable, then the applicant can choose to proceed with their application and have the USPTO issue their patent. However, if the decision is unfavorable, then the applicant receives a notification of rejection; it is then up to the applicant to decide whether they wish to respond to the rejection, continue to pursue their application, and request a reexamination.

It is common to think of patent applications as being simply accepted or rejected, but the status of an application is considerably more subtle. For the task of predicting an application's outcome, it is useful to provide a formalization of what it means for a patent application to be _accepted_ and _rejected_. We say that a patent application is "accepted" if it has been officially approved, granted, and published by the USPTO. There is, however, no clear-cut notion of absolute _rejection_ in patent applications. An application might, and in fact often does (at the beginning), receive an Office action indicating a _non-final_ or _final rejection_--typically on the grounds of prior art, scope of the claims, or lack of utility or novelty or obviousness, but the applicant can submit a response to the USPTO and re-open the prosecution of their application, even in the case of a final rejection. As a result, a "final rejection" does not imply a patent application has no chance of eventually being accepted after revisions. In our dataset, we label an application "rejected" if it has received an Office action of rejection--final or non-final--and was ultimately abandoned by the applicant.5 We categorize all the remaining applications, which are still waiting a response from the USPTO, as "pending." These

  
**Dataset** & **\# Des** & **Title** & **Abst** & **Appl** & **Exam** & **Invt** & **PD** & **Claims** & **Bkgd** & **Dsc** & **PCs** & **Years** & **Primary Purpose** \\  WPO-alpha & 75.250 & & & & & & & & & & & 1998.2002 & Classification \\ CLEF-2011 & 15.000, & & & & & & & & & & & & 2009 & Retrieval-Classification \\ USPTO-20 & 2.00147 & & & & & & & & & & & & 2006-2015 & Classification \\ BGPATENT & 1,341,362 & & & & & & & & & & & & 1971-2018 & Summarization \\ 
**Ours@IPD** & **4518.263** & & & & & & & & & & & & **2004-2018** & **Multi-Purpose** \\   

Table 1: Comparison of HUPD with other datasets whose primary goal is NLP patent analysis. The abbreviated columns mean the following. _Absst_: Abstract, _Appl_: Applicant Information, _Exam_: Examiner Information, _Invt_: Inventor Information, _PD_: Publication Date, _Bkgd_: Background, _Dsc_: Description, and _PCs_: IPC/CPC codes.

distinctions are particularly useful when describing and discussing the binary classification task of patent decisions in the following sections.6

## 3 Related Work on Patent Analysis

Existing patent datasets for NLP focus nearly exclusively on two tasks: patent subject classification and summarization. Below, we provide overview of prior datasets and studies in these areas.7,8

**Automated Subject Classification.** Patents are classified by subject matter according to standard taxonomies, most notably the International Patent Classification (IPC) and Cooperative Patent Classification (CPC) systems.9 These IPC/CPC codes are hierarchical--classified at a class level (e.g., G-_Physics_), subclass level (e.g., G06F-_Electric Digital Data Processing_), and so on. Previous studies attempted to predict the IPC or CPC codes of patents at the class and subclass levels using various statistical methods, including classical statistical learning tools [12; 13; 14; 15; 16] and neural models [17; 3; 18]. Recently, Transformers  have been considered for this task: Lee and Hsiang , for instance, fine-tuned a pre-trained BERT  to predict IPC/CPC codes of patents. Zaheer et al.  conducted similar experiments using BigBird and showed improvements over BERT.10

As shown in Table 1, WIPO-alpha, CLEF-IP, and USPTO-2M have been the main gymasia for model training for the IPC/CPC classification tasks, but these corpora are still limited in their scopes. They contain a relatively smaller set of patent text and metadata, do not allow users to choose which year ranges to focus on during training and testing, and in some cases come pre-tokenized, which may prevent users from using custom vocabularies. HUPD addresses these limitations, and also allows more flexibility in data selection and provides more comprehensive text, field, and year coverage.

**Patent Text Generation and Summarization.** With the growing availability and success of large language models in recent years, there has been an interest in applying language models to patents. Sharma et al.  initiated such explorations, introducing the first summarization dataset on patents, called BigPatent, and trained summarization tools on their dataset to generate the abstract section of a patent given its description section. The BigPatent dataset contains 1.3 million utility patents filed to the USPTO between 1971 and 2018, and was collected from the Google Patents Public Dataset via BigQuery. Our dataset differentiates itself from BigPatent in three aspects: (1) HUPD includes metadata and fields, including claims, background, filing date, and examiner information, that are not present in BigPatent; (2) in addition to accepted patents, it contains rejected and pending applications--it thus enables the study of patent acceptance/rejection11; and (3) it has approximately three times as many documents as BigPatent (Table 1).12

**Patent Acceptance Prediction.** To the best of our knowledge, our study is the first work to introduce a practical definition of rejection in patent examination to identify, analyze, and discuss the patterns in and characteristics of accepted versus rejected patent applications from a _purely textual_ perspective. In that sense, we introduce the patent decision classification task to the NLP literature.

## 4 The Dataset

The Harvard USPTO Patent Dataset (HUPD) contains 4,518,263 utility patent applications filed to the USPTO between January 2004 and December 2018.13 In this section, we elaborate on the data collection process and provide details about the data format and dataset structure. We furthermore enumerate and highlight a portion of the data's statistical properties, and acknowledge the limitations and ethical considerations of the present work. Additional information is included in Section A.

**Dataset Construction.** As specified by US law, all patent data is publicly accessible. Distilling this information into a NLP dataset involves obtaining the patent data and its corresponding metadata, normalizing all data to the same format, filtering missing and erroneous data, de-duplicating data, and merging all data into a single easy-to-use dataset. Here we provide an overview of this process.

Patent application texts were obtained from the USPTO Bulk Data Storage System (BDSS; Patent Application Data/XML Version) as XML files.14 As not all the original patent files follow the same XML structure, we wrote regular expressions to parse all the different formats into a normalized set of data fields, and stored these data fields as structured JSON files. Filing metadata--including acceptance decisions, filing dates, titles, and classification information--were separately obtained from the USPTO Patent Examination Research Datasets [26; 27] in February 2021. This metadata was then merged with the full-text patents from the USPTO BDSS to link patents filed before 2020 with information about examiners, office actions, and additional filing information. This merging process ensures that our dataset contains both the updated metadata structure and the patent application texts.

Finally, we assembled the continuation information for each application as follows: Applications with parent filings in the USPTO continuity data files were marked with the prefix "CONT-" in the decision status field.15 Hence, there are six labels in total for the decision status of applications: "Accepted," "Rejected," "Pending," "CONT-Accepted," "CONT-Rejected," and "CONT-Pending."16

**Statistics.** Table 1 provides a quantitative comparison of our dataset with other patent data sources, while Table 2 gives information and statistics about the text-based sections in our dataset. HUPD builds on its counterparts, including BigPatent, because of not only its size and wider coverage but also its ability to allow users to see the evolution of patent families over time.

**Limitations.** From a methodological point of view, the dataset is limited to patents from the U.S. and in English, and drops image content such as drawings. From a functional point of view, some textual sections are longer than current NLP models can process, and specialized vocabulary in certain fields can create issues for existing tokenizers.

  
**Section** & **Brief Description** & **Avg \# Tokens** \\  Title & Title of the Invention & 16.4 \\ Abstract & Summary of the Background and Claims & 132.0 \\ Claims & List of Items Defining the Invention & 1271.5 \\ Background & Brief Statement of the Field of Art and Related Art of the Invention & 627.11 \\ Summary & Condensed Version of the Description & 917.8 \\ Description & Detailed Statement and Disclosure of the Invention & 11855.6 \\   

Table 2: Brief description of and average number of tokens in each text-based section in HUPD, as measured by the GPT-2 tokenizer. Typically, the description section in a patent application is almost 100 times longer than the abstract section. HUPD can be used for long-sequence summarization and language modeling, inter alia.

**Potential Biases.** We provide a detailed examination of HUPD for potential biases in the Appendix. We recommend that researchers who use HUPD consider these biases when interpreting their results--especially if their analyses involve inventor and/or examiner information. For now, we summarize our empirical findings in this section and note they are consistent with the previous work . We show, among other results, that female inventors are notably underrepresented in the U.S. patenting system, that small and micro entities (e.g., independent inventors, small companies, non-profit organizations) are less likely to have positive outcomes in patent obtaining than large entities (businesses with more than 500 employees), and that patent filing and acceptance rates are not uniformly distributed across the US. Our empirical findings suggest that any study focusing on the acceptance prediction task, especially if it is using the inventor information or the small-entity indicator as part of the input, should be aware of the the potential biases present in the dataset and interpret their results in light of these considerations.

**Ethical Considerations.** While building HUPD, we followed the data sheets and statements introduced by Gebru et al.  and Bender and Friedman , discussing the motivations, objectives, collection process, workflow, use cases, distribution, maintenance, potential contributions, and potential misuses of our dataset and research (see: Section A).17

**Impact on Underserved Communities.** HUPD contains patent applications in English, a language with heavy attention from the NLP community. However, innovation is spread across many languages, cultures, and communities that are not reflected in this dataset. HUPD is thus not representative of all kinds of innovation. Furthermore, patent applications require a fixed cost to file and are not accessible to everyone. One goal of our dataset is to spur research that reduces the cost of drafting applications, potentially allowing for more people to seek intellectual property protection for their innovations.

## 5 Using the Dataset

Due to the versatile nature of HUPD, a wide range of NLP tasks and experiments can be constructed from the dataset by selecting the appropriate fields and metadata contained in each patent application. In this section, we highlight four tasks that we believe to be among the most valuable and relevant to the NLP and IP communities: (i) binary classification of patent decisions, (ii) multi-label classification of patent IPC/CPC categories, (iii) language modeling, and (iv) summarization. Of course, the dataset may easily be used to conduct other investigations, such as patent clustering, prior art search, and early detection of superstar inventions.18 In what follows, we describe our four tasks of interest in detail and contextualize their importance within the world of IP. Table 3 provides a brief overview of each task and the corresponding metrics used to measure task performance.

**Patent Acceptance Prediction.** Given a section of an application (in particular, the abstract, claims, or description), we predict whether the application will be accepted by the USPTO. From the perspective of the NLP community, this is a standard classification task. Yet, the potential applications and benefits of this decision task, as well as its difficulty, distinguish it from prevalent binary classification benchmarks (e.g., SST, Yelp). In our experiments, we focus on applications without parent filings to make our setup simple and clear, thereby excluding all the CONT-applications. Also, we do not include any pending applications.

**Automated Subject Classification.** The next task is to predict the primary IPC or CPC code of a patent application given a subset of the text of the application. A coherent automated classification of patent documents into different technology fields can facilitate effective assignment of patent

  
**Task Name** & **Setup** & **Metrics** \\  Acceptance Prediction & Abstract or Claims \(\) Patent Outcome (Decision) & Accuracy \\ IPC/CPC Classification & Abstract or Claims \(\) IPC/CPC & TOP-\(k\) \\ Language Modeling & Abstract or Claims or Description & Perplexity \\ Abstractive Summarization & Claims or Description \(\) Abstract & ROUGE/BLEU \\   

Table 3: Summary of the four NLP tasks presented in this work, along with some evaluation metrics for them. Our dataset can be used to conduct many other NLP/IP experiments. See Section 5 for detailed information.

applications to examiners. In addition, it may help create a rigorous and standardized catalog of prior art for research and exploration. This task might also help the early identification of valuable inversions that bridge multiple technological domains or the emergence of new subject areas. In our experimental setups, we predict the main IPC codes of patent applications at the subclass level, as IPC codes are available for a larger set of patents than CPC codes.19 There are 637 IPC codes at the subclass level in our dataset, but they are not uniformly distributed (see Figure 2); for instance, G06F-_Electric Digital Data Processing_ constitutes 10.4% of accepted patents that were filed between 2011 and 2016 and the most popular 15 IPC codes make up almost \(40\%\). Hence, it is difficult to achieve strong classification performance by only predicting the major classes.

**Language Modeling.** Next, we move from classification to language modeling (LM). We consider masked LM of patent claims. We concentrate on the claims section because it forms the basis of the invention described in a patent application; it has a distinctive language style; and it is considered to be the most legally potent part of a patent. We also conduct LM experiments on the abstract sections of patents, which are more similar to standard natural language. These models can be used for downstream tasks, as well as for domain-specific investigations. In Section H, we demonstrate one application of our LMs by visualizing the average embeddings of different patent categories under this fine-tuned model. The results of this exercise may reveal the textual evolution of innovation concepts and trends in patent applications across different technology areas.20

**Abstractive Summarization.** The formulation of the final task follows naturally from the structure of our patent data: Each patent contains an abstract in which the applicant summarizes the content of the patent. We use this section as the ground truth for our abstractive summarization task, and we use either the claims or the description as the source text. We perform this conditional generation task with the same motivation that inspired Sharma et al. ; our setup is similar to theirs apart from the size and scope of our dataset. One difference in our current setup is that our dataset allows us to explore using either the claims or description section for the source text, whereas in Sharma et al.  only the description section is available.21

## 6 Results and Discussion

In the next two sections, we establish benchmarks for the four tasks, describe the models used, and analyze our findings. In our experiments, we typically use the abstract or claims sections as the input

Figure 2: IPC distribution of accepted patent applications from 2011 to 2016 at the IPC subclass level. There are 637 IPC subclass labels in HUPD, of which the most common 20 codes make up half of the distribution. G06F-_Electric Digital Data Processing_ is the largest IPC subclass, accounting for \(10.4\%\) of applications.

to the models; however, future studies using our dataset can easily include other sections, such as the background and description.22

While our dataset includes patent applications filed to the USPTO between 2004 and 2018, we primarily focused on the 2011-2016 year range in our main NLP experiments. We chose this specific time period not only because the patent applications during these years reflect more recent inventions and cover diverse industries, but also because they were suitable for training various NLP models within a day at the time of our experiments. We used the same subset across most of our experiments for consistency and coherency. (For additional details on our experimental setup, please see: Section F).

**Patent Acceptance Prediction.** Table 4 reports the performances of our models on the most popular IPC codes. In each IPC subclass, with the exception of G01N-_Investigating or Analyzing Materials by Determining Their Chemical or Physical Properties_, the best performance was achieved by the models that relied on the claims. This result is consistent with the idea that the claims define the overall scope, novelty, and usefulness of the invention. The claims, together with the prior art, provide the most useful and critical information about the _patentability_ of an invention. It was, however, surprising to discover that there was not a significant difference between the NB classifiers and the BERT models in terms of accuracy scores across categories. We speculate that the BERT models might have mirrored the behaviors of the NB classifiers at the end, failing to go beyond word-level feature extraction. Nonetheless, we note the task is difficult, and currently uses only a limited portion of a patent text to determine the acceptability of the invention. We posit that new advances in Transformer models for longer text may improve predictive accuracy for this task.23

**Automated Subject Classification.** For the multi-class classification task, we considered only accepted patents, since they contain reliable and accredited IPC/CPC codes. Table 5 details the TOP1 and TOP5 accuracy scores for the multi-class IPC code classification results at the subclass level. First, we note that performance increases with more sophisticated models. The DistilBERT model,

  
**IPC –**_Section_ & **BernNB** & **MultiNB** & **Logistic** & **CNN** & **DistilBERT\({}^{}\)** & **BERT\({}^{}\)** & **RoBERTa\({}^{}\)** \\ 
**G06F –**_Abstract_ & 61.86 & 61.47 & 58.24 & 60.97 & **61.53** & 61.28 & 61.31 \\
**G06F –**_Claims_ & **63.96** & 62.06 & 58.02 & 63.38 & 63.37 & 62.97 & 63.25 \\ 
**H01L –**_Abstract_ & 58.98 & 59.05 & 58.54 & 60.71 & 61.46 & 61.85 & **61.85** \\
**H01L –**_Claims_ & 60.97 & 60.29 & 59.53 & **62.63** & 62.50 & 61.61 & 61.94 \\ 
**H04L –**_Abstract_ & 59.35 & 58.75 & 58.75 & 59.89 & **60.54** & 60.52 & 60.05 \\
**H04L –**_Claims_ & 62.13 & 61.04 & 58.04 & **62.34** & 61.42 & 61.47 & 61.74 \\ 
**H04N –**_Abstract_ & 60.74 & 60.64 & 58.79 & 60.37 & **62.01** & 61.93 & 61.51 \\
**H04N –**_Claims_ & 62.51 & 61.01 & 57.53 & **63.98** & 62.82 & 61.98 & 62.14 \\ 
**A61B –**_Abstract_ & 59.15 & 58.81 & 57.31 & 58.75 & 58.36 & 59.58 & **59.66** \\
**A61B –**_Claims_ & 59.30 & 59.12 & 57.25 & 59.49 & 60.15 & **61.20** & 61.00 \\ 
**G01N –**_Abstract_ & 59.85 & 59.89 & 57.25 & 59.98 & 59.00 & 60.30 & **61.10** \\
**G01N –**_Claims_ & 58.06 & 57.97 & 58.37 & 59.80 & 60.16 & 60.34 & **60.97** \\ 
**G06Q –**_Abstract_ & 61.53 & **61.64** & 58.52 & 60.46 & 61.23 & 61.09 & 61.56 \\
**G06Q –**_Claims_ & **63.96** & 63.31 & 57.17 & 62.90 & 61.88 & 62.19 & 63.25 \\   

Table 4: Baseline performances of our models on the binary classification of patent acceptance task. All the models were trained and evaluated on the patent applications filed to the USPTO between January 2011 and December 2016. All the test sets contained equal numbers of accepted and rejected applications, so the baseline accuracy to compare these models against is \(50\%\). In all but one IPC category, the models trained on the claims sections yielded the best performance. None of the individual accuracy scores, however, went beyond \(64\%\). In Section 7, we further report results of our conditional universal acceptance prediction classifier. The superscript \({}^{}\) denotes that these models were fine-tuned. The common IPC categories presented in this table are G06F-_Electric Digital Data Processing_, H01L-_Semiconductor Devices_, H04L-_Transmission of Digital Information_, H04N-_Pictorial Communication_, A61B-_Diagnosis_, _Surgery_, _Identification_, G01N-_Investigating or Analyzing Materials_, and G06Q-_Data Processing Systems or Methods_. (See Table 6 for our full results.)trained on the claims section, achieves \(63\%\) accuracy at TOP1 and \(90\%\) accuracy at TOP5; this is notable since, as Figure 2 implies, a majority-class baseline could only yield around \(10\%\) for TOP1 and \(26.5\%\) for TOP5. In fact, the diagonal line in Figure 7 (center) also illustrates that the DistilBERT model, for instance, has learned features that enable it to successfully classify minority categories, such as F25C-_Producing, Working or Handling Ice_ and D05B-_Sewing_. In general, the models trained on the abstract section performed as well as those trained on the claims section, perhaps an indication (in accord with the findings of Li et al. ) that the abstract alone contains useful information about the appropriate principal technology area the patent application might belong to. Model performance at the class level, 24 as opposed to the subclass level, was even stronger. The DistilBERT models achieve over \(80\%\) TOP1 accuracy at the class level. Furthermore, incorrect predictions were often from similar or related technology areas. (Our high TOP5 scores also corroborate this finding.)

**Language Modeling.** We trained a masked language model on patent claims in the style of BERT . We trained on a subset of the full patent dataset consisting of patent applications from 2011 to 2016 (1.57M applications) and evaluated on all patent applications from 2017 (187K applications). We performed masked language modeling with DistilRoBERTa  (82M parameters), initializing with a model pretrained on OpenWebText . We release this model publicly so that researchers may utilize it for downstream tasks.25

**Abstractive Summarization.** We find that patent descriptions and claims can both be effectively summarized into patent abstracts, with claims leading to improved performance across all metrics. Qualitatively, the models can produce fluent abstracts complete with accurate details drawn from the claims section, as shown in Table 10. Consistent with BigPatent, our results suggest that the summarization of patent data could serve as a new domain-specific conditional generation task for the NLP community.

## 7 Evolution of Innovation Criteria and Trends over Time

A key advantage of HUPD is its combination of unstructured textual content and rich, structured bibliographic metadata. Patents, like many other applied natural-language domains, exhibit concept and domain shifts--the criteria for innovation varies across categories and evolves over time. In this section, we discuss some ways that our dataset exhibits time- and state-varying concepts. We hope this feature of patent data allows for fresh studies of the nature of shifts across categories and supports the development of NLP models that accommodate concept or distributional shifts.

**Universal Acceptance Classification.** We explored how a universal decision classifier, trained on all the IPC categories, might perform on individual categories. To this end, we trained a conditional DistilBERT classifier, where the conditional information had the title, issue year, and IPC code, in addition to the abstract section, to predict the acceptance likelihood of a patent application.26 We used the patent applications filed between 2011 and 2016. The model achieved an overall accuracy score of \(62\%\) on the entire class-balanced test set. When we assessed the model's performance on individual IPC subclasses, we discovered, for instance, that the model yielded almost \(64.5\%\) accuracy on G06F-_Electric Digital Data Processing_, \(61.9\%\) accuracy on H04N-_Pictorial Communication, e.g., Television_, and \(57\%\) accuracy on G06Q-_Data Processing Systems or Methods_.

    &  &  &  & ^{ FT}\)} & ^{ FT}\)} \\
**Section** & **TOP1** & **TOP5** & **TOP1** & **TOPS** & **TOP1** & **TOP5** & **TOP1** & **TOP5** & **TOP1** & **TOP5** \\  _Abstract_ & 40.65 & 63.92 & 47.38 & 75.17 & 53.87 & 81.69 & **61.75** & **89.11** & 61.07 & 88.24 \\ _Claims_ & 39.37 & 65.42 & 48.09 & 77.78 & 56.10 & 83.40 & **63.40** & **90.22** & 62.82 & 89.46 \\   

Table 5: Performances of our models on the multi-class IPC classification of patent codes at the subclass level. Our DistilBERT models yielded the best performance overall under both abstract and claims input setups. TOP1 (i.e., accuracy) checks whether our prediction is the same as the actual label, whereas TOP5 measures whether the actual label of the input is amongst our five top predictions (i.e., five classes with the highest probability weights). Our high TOP5 scores indicate that our models are good at predicting the IPC codes of both popular and underrepresented classes. Figure 7 also provides evidence towards this conclusion.

**Cross-Category Evaluation.** In order to understand and identify relationships between different patent evaluation criteria in different IPC classes, we also took each DistilBERT model trained on one IPC subclass of patents and evaluated it across all the other popular IPC subclasses. Figure 9 provides an illustration of our empirical findings. Patent technology areas that are conceptually closer to each other, such as G06F-_Electric Digital Data Processing_ and H04L-_Transmission of Digital Information_, appear to have similar standards for patent acceptance. Notably, models trained in one context do not generalize to most other contexts. This suggests the criteria for patent acceptance are sensitive to the technical demands of the specific category of each application.

**Performance Over Time.** We can also use our models and dataset to understand the evolving criteria for patent acceptance, as well as innovation trends over time. Figure 3 shows the performance of a decision classification model trained on patent applications from 2011 to 2013 evaluated on applications produced in earlier and later years. While model performance usually deteriorates over time, suggesting changes in the features that predict patent acceptance, the rate of decay appears to be sharper for fields anecdotally thought to be faster-moving. This property of patent data may make it useful for studying concept shift that varies by class.

**Additional Tasks.** Given the richness of patent text and metadata, HUPD enables research on a wide range of tasks and use cases that may be explored in future work. In Section G, we describe some of these tasks--such as long sequence modeling, patent clustering, and patent examiner assignment--as well as the potential social impact and applications of our work.

## 8 Conclusion

We presented the Harvard USPTO Patent Dataset (HUPD), a large, versatile, and comprehensive structured corpus of patent data constructed for the NLP community. HUPD contains 4.5 million English-language utility patent applications filed to the USPTO between 2004 and 2018. We also established benchmarks for two classification-based and two generation-based tasks on our dataset. We provided detailed qualitative analyses of the models trained for these tasks and demonstrated how our dataset presents a setting with measurable concept shift. We hope that the combination of our dataset and the models used in this paper will not only advance research in patent analysis, but eventually also help patent applicants prepare more successful patent filings and provide a domain-specific laboratory for a multitude of NLP tasks.

Figure 3: Performance of a BERT decision classifier trained on applications from 2011 to 2013 and evaluated on applications produced in earlier and later years. While model performance decays over time across most categories (suggesting changing acceptance standards), acceptance criteria appear to change more quickly in faster-moving fields (e.g., H01L-_Semiconductor Devices_ and H04W-_Wireless Communication_) and slower in more developed fields (e.g., A61B-_Diagnosis, Surgery, Identification_). Of further interest may be the rise in turnover in acceptance criteria in G06Q-_Data Processing Systems or Methods_ starting in 2014, which may coincide with the concurrent growth in development of B2B and fintech products. In future work, it may also be illustrative to use this dataset to investigate the impact of the U.S. Supreme Court decision in _Alice Corp. v. CLS Bank International_, 573 U.S. 208 (2014), on the software-related patent applications issued after 2014.