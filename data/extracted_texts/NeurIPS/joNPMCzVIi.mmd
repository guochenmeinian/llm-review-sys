# Improved Bayes Regret Bounds for Multi-Task Hierarchical Bayesian Bandit Algorithms

Jiechao Guan\({}^{1}\) Hui Xiong\({}^{1,2,}\)

\({}^{1}\)AI Thrust, The Hong Kong University of Science and Technology (Guangzhou), China

\({}^{2}\)Department of Computer Science and Engineering, HKUST, China

{jiechaoguan, xionghui}@hkust-gz.edu.cn

Corresponding Author

###### Abstract

Hierarchical Bayesian bandit refers to the multi-task bandit problem in which bandit tasks are assumed to be drawn from the same distribution. In this work, we provide improved Bayes regret bounds for hierarchical Bayesian bandit algorithms in the multi-task linear bandit and semi-bandit settings. For the multi-task linear bandit, we first analyze the preexisting hierarchical Thompson sampling (HierTS) algorithm, and improve its gap-independent Bayes regret bound from \(O(m})\) to \(O(m)\) in the case of infinite action set, with \(m\) being the number of tasks and \(n\) the number of iterations per task. In the case of finite action set, we propose a novel hierarchical Bayesian bandit algorithm, named hierarchical BayesUCB (HierBayesUCB), that achieves the logarithmic but gap-dependent regret bound \(O(m n)\) under mild assumptions. All of the above regret bounds hold in many variants of hierarchical Bayesian linear bandit problem, including when the tasks are solved sequentially or concurrently. Furthermore, we extend the aforementioned HierTS and HierBayesUCB algorithms to the multi-task combinatorial semi-bandit setting. Concretely, our combinatorial HierTS algorithm attains comparable Bayes regret bound \(O(m n)\) with respect to the latest one. Moreover, our combinatorial HierBayesUCB yields a sharper Bayes regret bound \(O(m n)\). Experiments are conducted to validate the soundness of our theoretical results for multi-task bandit algorithms.

## 1 Introduction

A stochastic bandit [26; 6; 27] is a sequential decision-making problem where at each round, an agent has to choose an action, and receives a stochastic reward without knowing its expected value. The gap between the cumulative reward of optimal actions in hindsight and the cumulative reward of agent is defined as _regret_. The goal is to minimize regret, through a combination of exploring different actions and exploiting those with high rewards in the past. Typical applications of bandit algorithms include news article recommendation , computational advertisement , and dynamic pricing . For example, in news article recommendation, the agent must choose a news article for a user. The actions in this bandit setting are articles and the reward could be an indicator of a click from user.

When the agent has to solve multiple bandit tasks, many machine learning researchers resort to multi-task learning/meta-learning paradigm [8; 34] to benefit task adaptation. The existing works focused on the multi-task bandit problem can be categorized into three main groups: **(1)** The first group attempts to learn a low-dimensional representation shared by different bandit tasks, to derive a sharper cumulative regret bound than that derived by learning each task independently [19; 10]. **(2)** The second group leverages the similarity of contexts (e.g. the feature of actions) in bandit tasks to improve agent's ability to predict rewards in a new task [14; 36]. **(3)** The third group chooses tomaintain a meta-distribution over the hyper-parameters of within-task bandit algorithms (like Tsallis-INF , OFUL , and Thompson sampling [25; 7; 17]), and draws informative hyper-parameters from the meta-distribution for efficient regret minimization. Our work falls into the third group and formulates the problem of learning similar bandit tasks in a hierarchical Bayesian bandit model .

Specifically, in hierarchical Bayesian bandit setting, each bandit task is characterized by a task parameter. Different bandit task parameters are assumed to be independently and identically distributed according to the same distribution. At each round, the learning agent interacts with one or several bandit tasks, which correspond to the sequential and concurrent bandit settings respectively. Many existing works considered hierarchical Bayesian bandit problem, and proposed Thompson sampling  type algorithms to solve it [25; 7; 36]. The latest work  proposed hierarchical Thompson sampling (HierTS) algorithm and developed a gap-independent Bayes regret bound \(O(m})\) in the Gaussian linear bandit setting, where \(m\) is the number of bandit tasks and \(n\) the number of iterations per task. However, it is still unclear for us whether we can derive sharper regret bounds or how to extend hierarchical Bayesian bandit algorithms to the more general multi-task bandit setting.

In this work, we attempt to tackle the above two issues, by providing improved Bayes regret bounds for hierarchical Bayesian bandit algorithms in the multi-task Gaussian linear bandit and semi-bandit setting. Firstly, in the linear bandit setting, we improve the multi-task Bayes regret bound of HierTS to \(O(m})\) in the case of infinite action set, strengthening the latest bound in [17, Thm 3] by a factor of \(O(})\). In the case of finite action set, we propose a novel hierarchical Bayesian bandit algorithm, named hierarchical BayesUCB (HierBayesUCB), that achieves the logarithmic but gap-dependent regret bound \(O(m)\) under mild assumptions. All of the above regret bounds for linear bandit hold in both the sequential and concurrent setting. Secondly, we extend the aforementioned HierTS and HierBayesUCB algorithms to the multi-task Gaussian combinatorial semi-bandit setting. Concretely, our combinatorial HierTS algorithm attains comparable Bayes regret bound \(O(m)\) with respect to the latest one in [7; Thm 6]. Moreover, our combinatorial HierBayesUCB yields a sharper but gap-dependent regret bound \(O(m)\). Extensive experiments in the Gaussian linear bandit setting are conducted to support our theoretical results.

Overall, our theoretical contributions are four-fold: **(1)** In the case of infinite action set, we provide a tighter Bayes regret bound \(O(m})\) for HierTS. This bound improves the latest result by a factor of \(O(})\). **(2)** In the case of finite action set, we propose a novel HierBayesUCB algorithm, and provide gap-dependent logarithmic Bayes regret bound \(O(m)\) for it. **(3)** We generalize the above regret bounds for linear bandit from sequential setting to the more challenging concurrent setting. **(4)** We extend both HierTS and HierBayesUCB algorithms to the more general multi-task combinatorial semi-bandit setting and derive improved Bayes regret bounds.

## 2 Related Work

**Frequentist Regret Bounds for Stochastic Linear Bandit.** In the frequentist stochastic bandit setting, we do not assume the bandit task parameter is sampled from a fixed distribution. The frequentist regret is thus for any fixed task parameter, without taking expectation over the distribution of task parameter. **(1)** In the case of finite action set:  for the first time investigated the stochastic linear bandit problem and proposed an algorithm with a frequentist regret of \(O(^{3/2}{n})\), where \(d\) is the dimension of action space and \(n\) is the number of rounds.  developed a new algorithm and improved the regret bound to \(O(})\). **(12)** showed that the lower frequentist regret bound in the finite action set is \(()\). **(2)** In the case of infinite action set: Both  and  proposed algorithms that achieve \(O(d^{3/2}{n})\) regret. The regret bound was further improved in [1; 15] to \(O(d)\), by designing novel linear bandit algorithms or utilizing advanced martingale methods.

**Bayes Regret Bounds for Bayesian Linear Bandit.** In the Bayesian stochastic bandit setting, the Bayes regret is the expected cumulative regret whose expectation is taken over the draw of task parameter from a distribution. It is not difficult to see that the frequentist regret upper bound implies a Bayes regret upper bound, because the former holds for any task parameter. **(1)** When the action set is infinite:  showed that in the Gaussian linear bandit setting, the Bayes regret of any Bayesian bandit algorithm is lower bounded by \(()\).  for the first time gave the Bayes regret bound of \(O(d)\) for both Thompson sampling (TS) and BayesUCB  algorithms. Recently,  provided an improved Bayes regret \(O(d})\) for TS algorithm with a concise proof. **(2)** When the action set is finite:  derived a tight regret bound of \(O()\) for TS algorithm with sub-Gaussian reward noise, via a novel information-theoretic approach. Recently,  developed a logarithmic Bayes regret bound \(O(d^{2}^{2}n)\) for BayesUCB algorithm in the Gaussian bandit setting.

**Frequentist Regret Bounds for Multi-Task Linear Bandit Problems**. Under the representation learning paradigm, the frequentist regret bounds in [19; 10] for multi-task linear bandits scales as \(O(m)\), where \(k\) is the dimension of low-dimensional representation. The expected frequentist regret upper bound for multi-task adversarial linear bandit in  is \(O(m})\), with \(V\) being the similarity among multiple adversarial bandit tasks. Nevertheless, we should mention that all of these frequentist regret bounds for multi-task linear bandit problem are not tighter than \((m)\).

**Bayes Regret Bounds for Multi-Task Bayesian Linear Bandit/Semi-Bandit**. The most related works to ours are [25; 7; 17], which provided hierarchical-type Thompson sampling algorithms for multi-task bandit and derived Bayes regret bounds in the Gaussian reward setting. We list these Bayes regret bounds in Table 1 for direct comparisons. Among them, the latest work  proposed the HierTS algorithm and obtained its regret bound of \(O(m})\),  derived the first Bayes regret bound for multi-task hierarchical Bayesian semi-bandit algorithm. In this work, we provide for HierTS improved Bayes regret bound of \(O(m})\) in Theorem 5.1. We also propose a novel HierBayesUCB algorithm that achieves logarithmic regret bound \(O(m)\). We finally extend HierTS and HierBayesUCB to the semi-bandit setting and derive improved theoretical results. Other works utilized action features or structure information to derive Bayes regret bounds for multi-task bandit [36; 37], e.g. the Bayes regret bound in  is \(O(m}+m^{2}{(mn)})\).

**Hierarchical Bayesian Bandit Algorithms**. Hierarchical Bayesian bandit algorithm was first proposed by  to solve multi-task bandit problems. More hierarchical-type Thompson sampling algorithms based on multi-task/meta learning frameworks were developed with improved theoretical guarantees and empirical performance [7; 17; 36; 37]. There also existed other works investigating hierarchical Bayesian bandit algorithms within the single-task bandit setting. For example,  extended the two-level hierarchical Bayesian bandit framework to the deeper multiple-level hierarchial Bayesian bandit framework.  generalized the single-effect-parameter HierTS algorithm (i.e. the action parameter is centered at a single latent variable) to the mixed-effect bandit framework where each action is associated with a parameter that depends upon one or multiple effect parameters.

## 3 Problem Setting

For any positive integer \(n\), denote \([n]=\{1,2,...,n\}\) for brevity. For any square matrix \(M^{d d}\), denote \(_{1}(M)\), \(_{d}(M)\) as its maximum and minimum eigenvalues respectively, denote \((M)=_{1}(M)/_{d}(M)\) as its condition number. The action set \((0,B)}^{d}\) is assumed to be compact for some positive constant \(B>0\), where \((0,B)}\) is the closed ball centered at the origin. We use \(,\) to denote the inner-product between vectors, use \((a)\) or \(_{a}\) to denote the \(a\)-th element of vector \(\).

**Single-Task Bandit**. A stochastic bandit problem is characterized by an unknown parameter \(\) with an action set \(\). Each action \(a\) under the bandit instance \(\) is associated with a reward distribution \((|a,)\). The reward mean of action \(a\) under \(\) is denoted as \(r(a;)=_{Y(|a;)}[Y]\), and the optimal action under \(\) is denoted as \(A_{*}=_{a}r(a;)\). In the stochastic linear bandit setting, the mean reward of action \(a\) is \(r(a,)=a^{}\). In Bayesian bandit problem, we further assume that the task parameter \(\) is independently and identically distributed (i.i.d.) according to a task parameter distribution \((|_{*})\), which is characterized by an unknown hyper-parameter \(_{*}\).

**Single-Task Semi-Bandit**. In the semi-bandit setting, the action set \(=[K]\) is a set of finite items. \(\!=\{A:|A| L\}\) is a family of subsets of \(\) with up to \(L\) items, where \(L K\). \(^{K}\) is a weight vector. The weight of a set \(A\) is defined as \(_{a}(a)\). We assume that the weights \(\) are drawn i.i.d. from a distribution, and the mean weight is denoted as \(}\!=\![]\). Following previous work , we focus on the coherent case  which assumes that the agent knows a feature matrix \(^{K d}\), such that \(}=\), where \(\) is the task parameter drawn from \((|_{*})\). The reward of a subset \(A\) under the bandit instance \(\) is defined as \(r(A;)=_{a A}()(a)=_{a A}_{a},\), where \(_{a}\) is the transpose of the \(a\)-th row of matrix \(\). We further assume that \(\|_{a}\| B\), \( a\).

**Hierarchical Bayesian Multi-Task Bandit/Semi-Bandit**. In this setting, the agent interacts with \(m\) tasks sequentially or concurrently. First, sample the hyper-parameter \(_{*}\) from a hyper-prior \(Q\). Then, for each task \(s[m]\), sample the task parameter \(_{s,*}\) independently from distribution \((|_{*})\). The learning process can be detailed as follows. At round \(t 1\), the agent interacts with a set of tasks \(_{t}[m]\), takes a series of actions \(A_{t}=(A_{s,t})_{s_{t}}\), and receives a series of rewards \(Y_{t}=(Y_{s,t})_{s_{t}}\). In the bandit setting, \(Y_{s,t}(|A_{s,t};_{s,*})\) is a stochastic reward obtained by taking action \(A_{s,t}\) in task \(s_{t}\); in the semi-bandit setting, \(Y_{s,t}=\{}_{s,t}(a)\}_{a A_{s,t}}\) is a series of stochastic rewards, where \(}_{s,t}=}_{s}+_{s,t}\), \(}_{s}=_{s,*}\), and \(_{s,t}\) is a \(K\)-dimensional random noise. The full hierarchical Bayesian bandit/semi-bandit model in the \(m\)-task learning setting is exhibited as follow:

**(1) \(_{*} Q\)**; **(2) \(_{s,*}|_{*}(|_{*}), s[m]\)**; **(3) \(Y_{s,t}|A_{s,t},_{s,*}(|A_{s,t};_{s,*}), t  1,s_{t}\)**.

Therefore, the goal of the agent in hierarchical Bayesian multi-task bandit/semi-bandit setting is to interact with \(m\) tasks efficiently and minimize the following cumulative _multi-task Bayes regret_:

\[(m,n)=_{t 1}_{s_{t}}r(A _{s,*};_{s,*})-r(A_{s,t};_{s,*}),\] (1)

where \(A_{s,*}=*{arg\,max}_{a}r(a;_{s,*})\) is the optimal action for task \(s[m]\) in the bandit setting, and \(A_{s,*}*{arg\,max}_{A}r(A;_{s,*})\) is the optimal subset for task \(s[m]\) in the semi-bandit setting. The expectation is taken over \(_{*}\), all task parameters \((_{s,*})_{s[m]}\), all actions \((A_{t})_{t 1}\), all stochastic rewards \((Y_{t})_{t 1}\). We further assume that the action set \(\) is the same across different tasks for ease of exposition, and assume that the learning agent interacts with any task \(s[m]\) for at most \(n\) rounds for convenient comparison with exiting regret upper bounds for multi-task bandit/semi-bandit problem.

## 4 Algorithm

Denote \(H_{s,t}{=}((A_{s,},Y_{s,}))_{<t,s_{t}}\) as the history of all interactions of agent with task \(s\!\![m]\), and \(H_{t}{=}(H_{s,t})_{s[m]}\) as the whole interaction history up to round \(t\). We next introduce the specific form of Hierarchical Thompson Sampling (HierTS) and Hierarchical BayesUCB (HierBayesUCB) algorithms in the multi-task Bayesian linear bandit and semi-bandit settings, and instantiate these two algorithms to the multi-task Gaussian linear bandit (Algorithm 1) and semi-bandit (Algorithm 2) problems.

### Hierarchical Thompson Sampling and Hierarchical BayesUCB

At round \(t\), hierarchical Bayesian bandit algorithm samples a hyper-parameter \(_{t}\) from the hyper-posterior \(Q_{t}\) defined as \(Q_{t}()=(_{*}=|H_{t})\), and then interacts with tasks \(_{t}[m]\). Next, we give details of bandit algorithms, and details of semi-bandit algorithms are deferred to Section 5.4.

**Hierarchical Thompson Sampling**. For any task \(s_{t}\), HierTS samples task parameter \(_{s,t}\) from the distribution \(_{s,t}(|_{t})(_{s,*}=|_ {*}=_{t},H_{s,t})\) and takes the action \(A_{s,t}=*{arg\,max}_{a}a^{}_{s,t}\), where \(_{s,t}(|_{t})\) is only conditioned on \(H_{s,t}\) due to the independence between task parameter \(_{s,*}\) and other task histories. This process clearly samples bandit instance \(_{s,t}\) from the true posterior \((_{s,*}=|H_{t})\), which is equivalent to the form: \((_{s,*}=,_{*}=|H_{t})=_{s,t}(|)Q_{t}()\), where \(_{s,t}(|)_{s,t}()(|)\) is the posterior probability, \(_{s,t}(){=}_{(a,y) H_{s,t}}(y|a;)\) is the likelihood function, \((|)\) is the prior probability by Bayes rule.

**Hierarchical BayesUCB**. For any task \(s_{t}\) in round \(t\), HierBayesUCB computes the upper confidence bound \(U_{t,s,a}=a^{}_{s,t}+}\|a\|_{_{s, t}}\) for any \(a\), where \(_{s,t}\) and \(_{s,t}\) are the expectation and covariance of the distribution (i.e. \((_{s,*}=|H_{t})\)) of \(_{s,*}\) conditioned on the history \(H_{t}\), and then takes action with the highest upper confidence bound : \(A_{s,t}*{arg\,max}_{a}U_{t,s,a}\).

### Multi-Task Gaussian Linear Bandit and Semi-Bandit

The hierarchical Gaussian environment is generated as follow. In the multi-task linear bandit setting: **(1) \(_{*}(_{q},_{q})\)**, **(2) \(_{s,*}|_{*}(_{*},_{0}), s[m]\)**, **(3) \(Y_{s,t}|A_{s,t},_{s,*}(A_{s,t}^{}_{s,*},^ {2}), t 1,s_{t}\)**; In the semi-bandit setting, the only difference lies in step **(3)** where \(Y_{s,t,a}|A_{s,t},_{s,*}(_{a},_{s,*} ,^{2})\) for any \(a A_{s,t}\). Here, \(_{q},_{*},_{s,*}\) are \(d\)-dimensional vectors; \(_{q},_{0}^{d d}\) are positive semi-definite covariance matrices. In the above two settings, the reward noise can be regarded as \((0,^{2})\). In the following theoretical analysis sections, we assume that all of \(_{q},_{q},_{0}\) and \(\) are known by the agent to guarantee an analytically tractable posterior.

Concretely, using some basic algebraic computations in hierarchical Gaussian model (e.g. see (Zhu et al., 2017, Appendix D)), we can obtain the closed-form hyper-posterior in round \(t\) as \(Q_{t}()=(;_{t},_{t})\), where the expectation \(_{t}\) and the covariance matrix \(_{t}\) of \(Q_{t}()\) have the following explicit forms:

\[_{t}=_{t}_{q}^{-1}_{q}+_{s[m]}( _{0}+G_{s,t}^{-1})^{-1}G_{s,t}^{-1}B_{s,t},_{t}^{-1 }=_{q}^{-1}+_{s[m]}(_{0}+G_{s,t}^{-1})^{-1}.\] (2)Here, in the bandit setting \(G_{s,t}=^{-2}_{<t}\{s_{}\}A_{s,}A_{s,}^{}\) and \(B_{s,t}=^{-2}_{<t}\{s_{}\}A_{s,}Y _{s,}\); in the semi-bandit setting \(G_{s,t}=^{-2}_{<t}\{s_{}\}(_{a  A_{s,}}_{a}_{a}^{})\) and \(B_{s,t}=^{-2}_{<t}\{s_{}\}(_{a  A_{s,}}_{a}}_{s,t}(a))\). After the hyper-parameter \(_{t}\) is sampled from \(Q_{t}()\), we sample task parameter \(_{s,t}(;_{s,t},_{s,t})\) for task \(s\), where \(_{s,t}=_{s,t}(_{0}^{-1}_{t}+B_{s,t})\) is the posterior mean, \(_{s,t}^{-1}=_{0}^{-1}+G_{s,t}\) the posterior covariance matrix. Such posterior of a linear model is obtained with a Gaussian prior \((_{t},_{0})\) and Gaussian observations \((Y_{s,})_{<t,s_{}}\) by Bayes rule.

On the other hand, we also need to handle \((_{s,*}=|H_{t})\). It is not difficult to see that, in the multi-task Gaussian linear bandit/semi-bandit setting, \(_{s,*}|H_{t}\) is Gaussian and denoted as \((_{s,*}=|H_{t})\!=\!(;_{s,t}, _{s,t})\). According to Lemma B.1, \(_{s,t}\) and \(_{s,t}\) have the following explicit forms:

\[_{s,t}\!=\!\!_{s,t}(_{0}^{-1}_{t}\!+\!B _{s,t}),_{s,t}=_{s,t}+_{s,t} _{0}^{-1}_{t}_{0}^{-1}_{s,t}.\] (3)

## 5 Bayes Regret Bounds

In this section, we provide improved regret bounds of hierarchical Bayesian bandit algorithms for multi-task Gaussian linear bandit/semi-bandit problem. Concretely, we provide improved analysis for HierTS in the sequential linear bandit setting (Sections 5.1), propose a novel HierBayesUCB bandit algorithm with logarithmic regret guarantee (Section 5.2), develop regret bounds for these two algorithms in the concurrent linear bandit setting (Section 5.3), and finally extend these two algorithms to the semi-bandit setting (Section 5.4) with improved regret bounds. In the proof for our theoretical results, the most important step is to give an upper bound on the so-called _posterior variance_\(_{m,n}\), which in the multi-task linear bandit setting is defined and upper bounded as follow:

\[_{m,n}_{t 1}_{s _{t}} A_{s,t}_{_{s,t}}^{2} Omd +d.\] (4)

Although the above bound on \(_{m,n}\) achieves the same order (w.r.t. \(m,n\) and \(d\)) as that in the latest bound of (17, Sect B), our bound has a smaller multiplicative factor (see more details in Table 4). In the multi-task semi-bandit setting, the posterior variance is \(_{m,n}_{t 1}_{s_{t}} _{a A_{s,t}}_{a}_{_{s,t}}^{2}\) and can be bounded in a similar way. To finish the whole proof, our strategy consists of two main steps: **(1)** The first step is to transform the multi-task Bayes regret \((m,n)\) into an intermediate regret upper bound that involves the posterior variance \(_{m,n}\) as the dominant term. **(2)** The second step is to bound \(_{m,n}\) with Eq. (4). Combining the results in steps **(1)** and **(2)** yields Bayes regret bound for multi-task hierarchical Bayesian bandit/semi-bandit algorithms. Detailed comparisons between our regret bounds and others in the bandit setting are shown in Table 1. Next, we define \(c_{1}\!=\!^{2}\!+\!B^{2}_{1}(_{0})\), \(c_{2}=^{2}\!+\!B^{2}_{1}(_{0})+B^{2}_{1}(_{0}) (_{0})\) to be used through the whole Section 5.

### Improved Regret Bound for HierTS in the Sequential Bandit Setting

In the sequential bandit setting, \(|_{t}|=1\). Then, conditioned on \(H_{t}\), it is not difficult to see that in Bayes regret, each term \([_{s,*}^{}A_{s,*}-_{s,*}^{}A_{s,*}|H_{t}]=(_{s,*}-_{s,t})^{}A_{s,*}H_{t}\), and we use a novel Cauchy-Schwartz type inequality from [21, Prop 2] to bound \((_{s,*}-_{s,t})^{}A_{s,*}H_{t}\), leading to \((m,n)\!\![_{t 1}_{s_{t}} (_{s,*}-_{s,t})^{}A_{s,t} ^{2}H_{t}}]\). Expand the expression in the right hand side of the above inequality, we then have \((m,n)_{t,s_{t}}^{ }_{s,t}A_{s,t}}_{m,n}}\), reducing the Bayes regret bound to the posterior variance bound problem. Recalling Eq. (4) achieves our first improved Bayes regret upper bound in the sequential linear bandit setting.

**Theorem 5.1**: _(Near-Optimal Sequential Regret) Let \(|_{t}|=1\) for any round \(t\). Then in the multi-task Gaussian linear bandit setting, the Bayes regret upper bound of HierTS is as follow:_

\[(m,n) d(1+)+c_ {2}(1+(_{q}_{0}^{-1})}{d})}.\]

Our explanations for the above sequential regret bound are three-fold: **(1)** The term \(md(1+n/d)}\) represents the regret bound for solving \(m\) bandit tasks, whose parameters \(_{s,*}\) are drawn i.i.d. from the prior distribution \((_{*},_{0})\). Under this assumption, no task provides information for any other task, and hence this bound is linear in \(m\). Similar observation was also pointed out by . **(2)** The term \(d\!(1\!+\!m\!(_{q}_{0}^{-1 })/d)}\) represents the regret bound for learning the hyper-parameter \(_{*}\). Such bound is sublinear in \(m\) and is not a dominant term when \(m\) is large. **(3)** For a large \(m\), the averaged Bayes regret bound across \(m\) tasks is of \((m,n)/m=O(d)\), and strengthens the latest averaged bound \(O(d n)\) in [17, Thm 3] by a factor \(\). Besides, since the lower Bayes regret bound for any Bayesian bandit algorithm is \((d)\), our task-averaged Bayes regret bound is within \(O()\) of optimality and hence is called 'Near-Optimal' sequential regret bound. We further make a detailed comparison between our regret bound in Theorem 5.1 and the regret bound [17, Thm 3] in the following remark.

**Remark 5.1**: _(Improvements of Our Theorem 5.1 over the Latest One) Our sequential regret bound has two improvements over the latest one in [17, Thm 3, shown in Table 1]: **(1)** We remove the additional \(\) factor in both the regret bound for solving \(m\) bandit tasks and the regret bound for learning the hyper-parameter \(_{*}\). **(2)** In the regret bound for learning hyper-parameter \(_{*}\),  has a multiplicative factor \(^{2}(_{0})\), whereas our multiplicative factor is \((_{0})\). Such improvement is achieved by using technical matrix analysis proposed in Lemma C.1. and explained in Remark A.1._

### Logarithmic Regret Bound for HierBayesUCB in the Sequential Bandit Setting

In this section, we attempt to provide further improved Bayes regret bounds for hierarchical bandit algorithms in the sequential bandit setting. Because the task averaged Bayes regret bound in Theorem 5.1 is near optimal, it is not easy to derive improved Bayes regret bounds under the same assumptions. Therefore, we further assume that the action set \(\) is finite, and propose a novel

  
**Bayes Regret Bound** & \(||\) & **Bound I** & **Bound II** \\ 
[25, Theorem 3] & Finite & \(Om\) & \(On^{2}K\) \\ 
[7, Theorem 5] & Finite & \(Om||)}\) & \(O|)}\) \\ 
[17, Theorem 3] & Infinite & \(Omd)(mn)}\) & \(Od\) \\  Our Theorem 5.1 & Infinite & \(Omd)}\) & \(Od)}\) \\  Our Theorem 5.2 & Finite & \(Omd()(mn)\) & \(Od()(mn)\) \\   

Table 1: Different Bayes regret bounds for multi-task \(d\)-dimensional linear (or \(K\)-armed) bandit problem in the sequential setting. \(m\) is the number of tasks, \(n\) the number of iterations per task, \(\) is the action set. **Bayes Regret Bound** =**Bound I + Bound II + Negligible Terms**, where **Bound I** is the regret bound for solving \(m\) tasks, **Bound II** the regret bound for learning hyper-parameter \(_{*}\).

hierarchical Bayesian bandit algorithm, named Hierarchical BayesUCB (HierBayesUCB), for multi-task linear bandit problem. The pseudo-code of our proposed algorithm is shown in Algorithm 1.

Next, we introduce some necessary notations. Let \(_{s,t}=_{s,*}^{}(A_{s,*}-A_{s,t})\), \(_{s,}=_{a\{A_{s,*}\}}_{s,*}^ {}A_{s,*}-_{s,*}^{}a\), \(_{}=_{s[m]}_{s,}\). For any \(>0\), let \(_{}^{}=\{,_{}\}\). Define the event \(E_{s,t}=\{ a:|a^{}(_{s,*}-_{s,t})|\! \!}\|a\|_{_{s,t}}\}\). Then, analogous to , we decompose the Bayes regret \((m,n)=_{t 1}_{s_{t}}\!_{s,t}\) into three terms: \(_{t 1,s_{t}}_{s,t} \{_{s,t},E_{s,t}\}+\{_{s,t}<,E_{s,t} \}+\{_{s,t}\}\). We can bound the last two terms trivially with \(mn[+2_{t,s}_{s,t}| |]\). For the first term, we use the fact that \(A_{s,t} H_{t}}}{{}}A_{s,*}  H_{t}\), as well as the Upper Confidence Bound (UCB) technique to reduce it to an intermediate upper bound \(_{t 1,s_{t}} A_{s,t}_{_{s,t}}^{2}/_{s,t}_{s,t}\). Combining the upper bound over \(_{m,n}\) in Eq. (4), HierBayesUCB can achieve the following logarithmic Bayes regret bound in the sequential bandit setting (the logarithmic bound can be extended to the concurrent setting).

**Theorem 5.2**: _(Logarithmic Sequential Regret of HierBayesUCB) Let \(|_{t}|=1\) for any round \(t\), and the action set \(\) is finite with \(||<\). Then in the multi-task Gaussian linear bandit setting, for any \((0,1)\), \(>0\), the Bayes regret \((m,n)\) of HierBayesUCB is upper bounded by_

\[mn\!+\!4B_{1}^{}(_{0}\!+\! _{0}\!)d^{}\!+\!\|_{q}\|_{_{s,1}^{-1}} +[}{_{ }^{}}]mc_{1}(1\!+\!)\!+\!c_{2} (1\!+\!(_{q}_{0}^{-1})}{d}) \!.\]

We give more explanations for the above sequential regret in terms of the following five aspects: **(1)** If let \(=1/(mn)\), \(=1/(mn)\) and \(_{}>>\), the above sequential regret bound is of \(O(mn)(md()+d())\). The term \(O(md(mn)())\) represents the regret bound for solving \(m\) bandit tasks and is linear in \(m\). Such bound is sharper than the corresponding bound \(O(md)})\) in our Theorem 5.1 by a multiplicative factor \(O((mn))\), which is less than \(1\) especially when \(m n\). **(2)** The term \(O(d(mn)())\) represents the regret bound for learning the hyper-parameter \(_{*}\), and its contribution to the Bayes regret bound can be negligible. Besides, this bound is sharper than the bound \(O(d)\) in our Theorem 5.1. **(3)** The averaged Bayes regret bound across \(m\) tasks can be regarded as \((m,n)/m=O(d(mn) n)\), which is logarithmic in \(n\). Therefore, we call our regret bound as 'Logarithmic' sequential regret bound. Moreover, if there exists a fixed positive integer \(i<<n\), such that \(m n\)', then our task-averaged Bayes regret \((m,n)/m=O(d[^{}}]^{2}n)\) matches the latest single-task Bayes regret bound in [3, Thm 5] and is remarkably similar to the frequentist regret \(O(d_{}^{-1}^{2}n)\) in [1, Thm 5]. **(4)** We can obtain sharper bounds by setting \(,\) as different values. For example, by setting \(=1/n\), our regret bound becomes \(O([mn+m]+^{}}m n)\), which is of order \(O(m^{2}n)\) if we set \(=1/(mn)\) and the gap \(_{}>>\) is large. **(5)** We also need to point out that, the Bayes regret bound in Theorem 5.2 scales with \([^{}}]\). If the gap \(_{} 1/(mn)\), then \(_{}^{}=1/(mn)\) and this may cause a large Bayes regret upper bound.

### Improved Regret Bounds of HierTS and HierBayesUCB in the Concurrent Bandit Setting

In the concurrent bandit setting, there exists a positive integer \(L m\), such that \(1|_{t}| L\). The concurrent bandit setting is thus more challenging than the sequential bandit setting, because the agent in the concurrent setting needs to interact with multiple bandit tasks in parallel at each round \(t 1\), and the hyper-posterior \(Q_{t}\) will not be updated until the end of round \(t\). Therefore, we need to make an additional assumption on the action space \(\) as follow to facilitate our theoretical analysis.

**Assumption 5.1**: _There exist actions \(\{a_{i}\}_{i=1}^{d}\!\!\), a constant \(\!>\!0\), such that \(_{d}(_{i=1}^{d}a_{i}a_{i}^{})\!\!\)._

This assumption is also used in previous works  for hierarchical Bayesian linear bandit. It indicates that \(_{i=1}^{d}a_{i}a_{i}^{}\) is a positive definite matrix, and does not weaken the generality of our theoretical results. Actually, if \(^{d}\) is not spanned by actions in \(\), we can project \(\) into a subspace where the assumption holds. We also need to modify the HierTS algorithm to let the agent take the basic actions \(\{a_{i}\}_{i=1}^{d}\) for the first \(d\) interactions in any task \(s[m]\). This modification guarantees that the agent explores all directions within the task. Such exploration is very similar to the initialization method in UCB type \(K\)-arm bandit algorithms , which choose to pull each arm in the first \(K\) rounds. Define \(c_{3}=1+B^{2}^{-2}(_{0})_{1}(_{0})+ ^{2}/\) that will be used throughout the concurrent setting. Then, analogous to the proof for Theorem 5.1, we bound \(_{m,n}}\) with a more refined analysis, achieving the following improved Bayes regret bound for HierTS in the concurrent setting.

**Theorem 5.3**: _Under Assumption 5.1, let \(1|_{t}| L\) for any round \(t 1\). Then in the multi-task Gaussian linear bandit setting, the Bayes regret \((m,n)\) of HierTS is upper bounded by_

\[2Bmd(_{0}+_{q})}(+\|_{q}\|_{_{s,1}^{-1}})+d)}+2c_{2}c_{3}(_{q}_{0}^{-1})}{d})}}.\]

The concurrent regret bound in Theorem 5.3 achieves almost the same order (w.r.t. \(m,n,d\)) as the sequential regret bound in Theorem 5.1, but differs in two aspects: **(1)** The bound for learning \(m\) i.i.d. bandit tasks has an additional term \(Bmd(_{0}+_{q})}(+\|_{q}\|_{_{x,1}^{-1}})\). This is due to the fact that we take the basic actions \(\{a_{i}\}_{i=1}^{d}\) first for each task \(s[m]\) in the modified HierTS algorithm. **(2)** The bound for learning the hyper-parameter \(_{*}\) has an additional multiplicative factor \(c_{3}\). This is the price for deriving regret bounds in the concurrent setting. Nevertheless, when compared with the latest concurrent regret bound in [17, Thm 4] for HierTS, our concurrent regret bound in Theorem 5.3 removes the \(}\) factor in both the regret bound for learning \(m\) bandit tasks and the regret bound for learning hyper-parameter \(_{*}\). Detailed comparisons between different concurrent regret bounds for multi-task linear bandit setting are listed in Table 3. Furthermore, utilizing the proof strategy to demonstrate the logarithmic sequential regret for HierBayesUCB in our Theorem 5.2, we can analogously develop a logarithmic concurrent regret upper bound for HierBayesUCB algorithm, which is deferred to our Theorem C.2 in Appendix C due to the limited space of the main paper.

### Improved Regret Bounds for HierTS and HierBayesUCB in the Semi-Bandit Setting

In this section, we extend the HierTS and HierBayesUCB algorithms to the multi-task Gaussian combinatorial semi-bandit setting. The pseudo-code of them is shown in Algorithm 2. Algorithm 2 is very similar to Algorithm 1 (i.e. the multi-task linear bandit algorithms), except that the combinatorial HierTS in Algorithm 2 uses the approximation/randomized algorithm \(\) to solve combinatorial problem \(A_{*}*{arg\,max}_{A}_{a} (a)\) and denotes the solution as \(A_{*}=(,,)\). We adopt the \(\) operator as in the seminal works [11; 38] to guarantee the efficiency of combinatorial HierTS semi-bandit algorithm. In this section, we only consider the sequential semi-bandit setting (i.e. \(|_{t}|=1\)) for ease of presentation, and our results can be extended to the concurrent semi-bandit setting. Then, define \(c_{4}=^{2}+B^{2}L_{1}(_{0})+B^{2}_{1}(_{q}) (_{0})\), we first derive the Bayes regret upper bound for combinatorial HierTS algorithm in the sequential semi-bandit setting.

**Theorem 5.4**: _Let \(|_{t}|=1\) for any \(t 1\). Let \(c(_{0})}{})}}\), then in the multi-task Gaussian semi-bandit setting, the Bayes regret upper bound of combinatorial HierTS is:_

\[(m,n) m+cm)}+2c_{4} Ld(1+(_{0}^{-1}_{q})}{d})}.\]

Detailed comparisons between different Bayes regret bounds for multi-task semi-bandit problem are listed in Table 2. We can see that, in our Theorem 5.4, both the regret bound \(O(m)\) for learning \(m\) tasks and the regret bound \(O(})\) for learning hyper-parameter \(_{*}\) can achieve the same order (w.r.t. \(m\) and \(n\)) when compared with the latest bound in [7; Thm 6]. Besides, our Bayes regret bound is logarithmic in the number \(K\) of items, whereas the Bayes regret bound in

  
**Bayes Regret Bound** & \(\) & **Bound I** & **Bound II** \\ 
[7, Theorem 6] & \([K]\) & \(Om}\) & \(O}\) \\  Our Theorem 5.4 & \([K]\) & \(Om}\) & \(OL^{}}\) \\  Our Theorem 5.5 & \([K]\) & \(OmL\) & \(OL^{3}\) \\   

Table 2: Different Bayes regret bounds for multi-task semi-bandit problem. **Bayes Regret Bound =Bound I + Bound II + Negligible Terms**. \(m\) is the number of tasks, \(n\) the number of iterations per task, \(K\) the size of action set, \(L\) the number of pulled actions at each round (\(1 L K\)). **Bound I** is the regret bound for solving \(m\) tasks, **Bound II** the regret bound for learning hyper-parameter \(_{*}\).

[7, Thm 6] is sublinear in \(K\). Therefore, our regret bound becomes sharper when the size of action set is very large, e.g. \(K\!>>\!L\). Next, we derive a gap-dependent logarithmic multi-task Bayes regret bound for our proposed combinatorial HierBayesUCB algorithm in the sequential semi-bandit setting.

**Theorem 5.5**: _Let \(|_{t}|=1\) for any \(t 1\). Then for any \(>0,(0,1)\), in the multi-task Gaussian semi-bandit setting, the Bayes regret \((m,n)\) of combinatorial HierBayesUCB is bounded by_

\[mn+4LBK_{1}^{(_{0}+_{q})} (d^{}+\|_{q}\|_{_{s,1}^{-1}})\!\!+\![ }{_{}^{}}\!\!2c_{1}m (1\!+\!)\!+\!2c_{4}Ld(1\!+\! (_{0}^{-1}_{q})}{d})\!\]

In Theorem 5.5, if we set \(\!=\!1/(mnK)\), \(\!=\!1/(mn)\), and \(_{}\!>>\), then the regret bound \(Om n(mn)\) for learning \(m\) tasks is logarithmic in \(n\). Such bound is sharper than the latest one \(O(m n)\) in [7, Thm 6] for multi-task semi-bandit. The regret bound \(O( m(mn))\) for learning hyper-parameter \(_{*}\) is also sharper than that of \(O()\) in [7, Thm 6]. Besides, since \(\!=\!1/(mnK)\), the whole Bayes regret bound is also logarithmic in the number \(K\) of items. Nevertheless, we should point out that our bounds hold for the multi-task semi-bandits with linear generalization, but  focuses on the multi-task \(K\)-arm semi-bandits without feature matrix \(\).

### Technical Novelties for Deriving Improved Regret Bounds

In this section, we summarize our technical novelties in terms of the following three aspects:

**(1)** For the improved regret bound for HierTS in Theorem 5.1: our proof has three novelties: **(i)** We apply a novel Cauchy-Schwartz type inequality in Lemma A.2 to bound \(_{s,*}-_{s,t}^{}A_{s,*} H_{t}(_{s,*}-_{s,t})^{}A_{s,t}^{2}H_{t}}\), leading to a sharper bound without \(\) factor:

\[(m,n)_{t,s_{t}}^{ }_{s,t}A_{s,t}}_{m,n}} O(m).\]

**(ii)** We use a more technical positive semi-definite matrix decomposition analysis (i.e. our Lemma A.1) to reduce the multiplicative factor \(^{2}(_{0})\) to \((_{0})\). **(iii)** Define a new matrix \(_{s,t}\) such that the denominator in the regret is \(^{2}+B^{2}_{1}(_{0})\), not just \(^{2}\), avoiding the case that the variance serves alone as the denominator. Such technical novelties are also listed explicitly in Table 4.

**(2)** For the improved regret bound for HierBayesUCB in Theorem 5.2 in the sequential bandit setting: our novelty lies in decomposing the Bayes regret \((m,n)=_{t 1}_{s_{t}}_{s,t}\) into three terms:

\[_{t 1}_{s_{t}}_{s,t}= _{t 1,s_{t}}_{s,t}\{_{s,t} ,E_{s,t}\}+\{_{s,t}<,E_{s,t}\}+ \{_{s,t}\},\]

and bounding the first term with a new method as well as the property of BayesUCB algorithm as

\[_{s,t}\{_{s,t},E_{s,t}\}=^{2}}{_{s,t}}\{_{s,t},E_ {s,t}\}}^{2}}{_{}^{}},\]

resulting in the final improved gap-dependent regret bound for HierBayesUCB as follows

\[\!_{t 1,s_{t}}\!\|A_{s,t}\|_{_{s,t}}^{2} /_{}^{} Om(n )}{{ }}Om(n)(mn).\]

**(3)** For the improved regret bounds for HierTS and HierBayesUCB in the concurrent setting and in the semi-bandit setting: besides the aforementioned technical novelties in **(1)** and **(2)**, the additional technical novelty lies in leveraging more refined analysis (e.g. using Woodbury matrix identity) to bound the gap between matrices \(_{t+1}^{-1}\) and \(_{t}^{-1}\) (more details is shown in Lemma C.1 and Eq. (6)).

## 6 Experiments

In this section, we conduct experiments in the linear bandit setting to verify our theoretical results. Specifically, we show the influence of hyper-parameters (e.g. \(m,n,L\)) to the multi-task Bayes regret of HierTS and HierBayesUCB, to validate the consistency between their regret bounds and practical performance. Besides, we compare the performance between our algorithms and other baselines, to show the effectiveness of hierarchical Bayesian bandit algorithms in the multi-task bandit setting.

**Experimental Setting**. We follow the same experimental setting as that in [7; 17]. Concretely, we conduct linear bandit experiments with Gaussian reward. The synthetic problem is defined as follows. In most experiments, we set the number of total tasks as \(m=10\), the dimension of action space as \(d=4\), the number of concurrent tasks as \(L=5\), the number of rounds as \(n=200m/L\). We focus on the finite action space with \(||=10\), and each action is sampled uniformly from \([-0.5,0.5]^{d}\). In hierarchical Bayesian model, we set the hyper-prior as zero-mean isotropic Gaussian distribution \((_{q},_{q})=(,_{q})\), where \(_{q}=_{q}^{2}I_{d}\); and set the task variance \(_{0}=_{0}^{2}I_{d}\). Unless otherwise stated, we set \(_{q}=1\), \(_{0}=0.1\), \(^{2}=0.5\) for each task in most experiments. We exhibit the regret performance of HierTS algorithm with respect to five hyper-parameters \(m,L,_{q},_{0},\) in Figure 1 (a)-(e) respectively. The regret performance of HierBayesUCB is shown in Figure 2 of Appendix F.

Besides, we compare HierTS/HierBayesUCB with other two TS type algorithms that do not learn the hyper-parameter \(^{*}\) in a hierarchical Bayesian model. The first baseline is the vanilla TS algorithm that samples task parameter \(_{s,*}\) from the marginal prior \((_{q},_{q}+_{0})\). The second baseline is an idealized TS algorithm that knows \(_{*}\) exactly and uses the true prior \((_{*},_{0})\). We call the second baseline as OracleTS, since this TS algorithm accesses more information of \(_{*}\) than HierTS and vanilla TS algorithm. We show the regret performance of these four bandit algorithms in Figure 1 (f).

**Experimental Results**. From Figure 1, we can observe that: **(1)** In plot (a), the multi-task regret becomes larger with the increase of \(m\) and \(n\), which is consistent with our regret upper bound in Theorems 5.1. **(2)** In plot (b), the regret increases with a higher dimension \(d\). The number \(L\) of the concurrent tasks seems do not have a large impact on regret. **(3)** In plots (c)-(e), the regret decreases with a smaller variance (e.g. \(_{q}\), \(_{0}\) and \(\)) in hierarchical Bayesian model, validating the provable benefits of variance-reduction in regret minimization, which is revealed in our multi-task Bayes regret upper bounds. **(4)** The task-averaged regret of HierTS is tighter than that of single-task TS algorithm, empirically demonstrating the advantages of multi-task Bayesian bandit optimization paradigm over single-task bandit learning. **(5)** Our proposed HierBayesUCB achieves lower regret than HierTS.

## 7 Conclusions

This paper provides improved Bayes regret bounds for hierarchical Bayesian bandit algorithms in the multi-task Gaussian linear bandit and semi-bandit setting. For linear bandit problem: in the case of infinite action set, we strengthen the preexisting regret bound \(O(m)\) of HierTS to \(O(m)\) by a factor of \(O()\); in the case of finite action set, we propose a novel HierBayesUCB algorithm that achieves logarithmic regret bound \(O(m(mn) n)\) under mild conditions. Our regret bounds in the bandit setting hold when the agent solves tasks sequentially or concurrently. Then, we extend the above HierTS and HierBayesUCB algorithms to the multi-task semi-bandit setting and derive improved regret bounds. The synthetic experiments further support our theoretical results. Our future work aims to extend our bounds to the sub-exponential bandit setting.

Figure 1: Regrets of HierTS algorithm with respect to (w.r.t.) different hyper-parameters.