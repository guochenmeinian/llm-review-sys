# How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization

Hai Zhang Hang Yu Junqiao Zhao Di Zhang

Chang Huang Hongtu Zhou Xiao Zhang Chen Ye

Department of Computer Science, Tongji University, Shanghai, China

MOE Key Lab of Embedded System and Service Computing, Tongji University, Shanghai, China

{zhanghai12138, 2053881, zhaojunqiao}@tongji.edu.cn

Corresponding author

###### Abstract

Designing and deriving effective model-based reinforcement learning (MBRL) algorithms with a performance improvement guarantee is challenging, mainly attributed to the high coupling between model learning and policy optimization. Many prior methods that rely on return discrepancy to guide model learning ignore the impacts of model shift, which can lead to performance deterioration due to excessive model updates. Other methods use performance difference bound to explicitly consider model shift. However, these methods rely on a fixed threshold to constrain model shift, resulting in a heavy dependence on the threshold and a lack of adaptability during the training process. In this paper, we theoretically derive an optimization objective that can unify model shift and model bias and then formulate a fine-tuning process. This process adaptively adjusts the model updates to get a performance improvement guarantee while avoiding model overfitting. Based on these, we develop a straightforward algorithm USB-PO2 (Unified model Shift and model Bias Policy Optimization). Empirical results show that USB-PO achieves state-of-the-art performance on several challenging benchmark tasks.

## 1 Introduction

Nowadays, reinforcement learning (RL) has been gaining much traction in a wide variety of complicated decision-making tasks ranging from academia to industry . Part of this is due to some remarkable model-free RL (MFRL) algorithms , which show desirable asymptotic performance. However, their applications are hindered by the bottleneck of sample efficiency. On the contrary, model-based RL (MBRL) algorithms, using a world model to generate the imaginary rollouts and then taking them for policy optimization , have high sample efficiency while achieving similar asymptotic performance, thus becoming a compelling alternative in practical cases .

Typically, MBRL algorithms iterate between model learning and policy optimization. Hence the model quality is crucial for MBRL. Many prior methods  rely on return discrepancy to obtain model updates with a performance improvement guarantee. While having achieved comparable results, they only account for model bias in one iteration  but do not consider the impacts of model shift between two iterations , which can lead to performance deterioration due to excessive model updates. Although CMLO  explicitly considers model shift from the perspective of the performance difference bound, it only sets a fixed threshold to constrain the impacts of model shift and determines when the model should be updated accordingly. If this threshold isset too low, the model bias of the following iteration will be large, which impairs the subsequent optimization process. If this threshold is set too high, the performance improvement can no longer be guaranteed. We remark that such an update is heavily dependent on the choice of this threshold and should be adjusted adaptively during the training process. Therefore, a smarter scheme is required to unify model shift and model bias, enabling adaptively adjusting their impacts to get a performance improvement guarantee.

In this paper, we theoretically derive an optimization objective that can unify model shift and model bias. Specifically, according to the performance difference bound, we propose to minimize the sum of two terms: the model shift term between the pre-update model and the post-update model and the model bias term of the post-update model, each of which is denoted by a second-order Wasserstein distance . By minimizing this optimization objective after the model update via maximum likelihood estimation (MLE) [6; 20], we can tune the model to adaptively find appropriate updates to get a performance improvement guarantee.

Based on these, we develop a straightforward algorithm USB-PO (Unified model Shift and model Bias Policy Optimization). To the best of our knowledge, this is the first method that unifies model shift and model bias and adaptively fine-tunes the model updates during the training process. We evaluate USB-PO on several continuous control benchmark tasks. The results show that USB-PO has higher sample efficiency and better final performance than other state-of-the-art (SOTA) MBRL methods and yields promising asymptotic performance compared with the MFRL counterparts.

## 2 Related works

MBRL algorithms are promising candidates for real-world sequential decision-making problems due to their high sample efficiency. Existing studies can be divided into several categories [4; 45; 1; 34; 49; 8; 17] following their different usage of the model. Our work falls into the Dyna-style category [45; 44]. Specifically, after model learning, the model generates the imaginary rollouts into the replay buffer for subsequent policy optimization. Hence, both model learning and policy optimization have critical impacts on asymptotic performance.

Some previous algorithms focus on the policy optimization process. CMBAC  introduces conservatism to reduce overestimation of the action value function, and ME-TRPO  imposes constraints on the policy to get reliable updates within the trust region. Our work is oriented towards model learning, which is orthogonal to these algorithms. Hence, we are inclined to propose a generic algorithm similar to [24; 20; 21] that can be plugged into many SOTA MFRL algorithms [15; 29], rather than just proposing for a specific policy optimization algorithm.

A key issue in model learning is model bias, which refers to the error between the model and the real environment . As the imaginary rollout horizon increases, the impacts of model bias accumulate rapidly, leading to compounding error and unreliable transitions. To mitigate this problem, several effective methods have been proposed. The ensemble model technique [6; 25; 37] and the dropout method  are employed to prevent model overfitting. The uncertainty estimation techniques are used to adjust the rollout length [20; 30] or the transition weight [19; 38]. Furthermore, The multi-step techniques [2; 48] are applied to prevent direct input of the imaginary states. We follow the previous work  to use the combination of the ensemble model technique with short model rollouts to mitigate the compounding error.

Performance improvement guarantee is a core concern in both MFRL and MBRL theoretic avenues. In MFRL, methods such as TRPO  and CPI  choose to optimize the performance difference bound, whilst most of the previous work in MBRL [31; 20; 53; 38; 24] choose to optimize the difference of expected return under the model and that of the real environment, which is termed return discrepancy. However, return discrepancy ignores model shift between two consecutive iterations compared to the performance difference bound under the MBRL setting, which can lead to performance deterioration due to excessive model updates. Although some recent methods have also employed performance difference bound to construct theoretical proofs, they still suffer from certain limitations. OPC  designs an algorithm to optimize on-policy model error, but it is similar to return discrepancy in nature. DPI  uses dual updates to improve sample efficiency but tries to restrict policy updates within the trust region, thus inhibiting exploration. CMLO  relies on a fixed threshold to constrain the impacts of model shift, resulting in a heavy dependence on the threshold and a lack of adaptability during the training process. Hence, we try to unify model shift and model bias to form a novel optimization problem, adaptively fine-tuning the model updates to get a performance improvement guarantee. Still, some prior work [7; 36; 54] choose to consider regret bound, among which  also reduce the impacts of the model changing dramatically between successive iterations. Instead of unifying model shift and model bias, they choose to realize dual optimization by considering maximizing the expectation of the model value rather than that of the single model as a sub-process. Different from [54; 43; 27] that use dual optimization to train the policy, we devise an extra phase to fine-tune the model.

## 3 Preliminaries

We consider a Markov Decision Process (MDP), defined by the tuple \(M=(,,p,r,,_{0})\). \(\) and \(\) denote the state space and action space respectively, and \((0,1)\) denotes the discount factor. \(p(s^{}|s,a)\) denotes the dynamic transition distribution and we denote \(p_{M^{*}}(s^{}|s,a)\) as that of the real environment. \(_{0}(s)\) denotes the initial state distribution and \(r(s,a)\) denotes the reward function. RL aims to find the optimal policy \(^{*}\) that maximizes the expected return under the real environment \(M^{*}\) denoted by the value function \(V^{}_{M^{*}}\) as Eq.(1):

\[^{*}=*{arg\,max}_{}V^{}_{M^{*}}=_{a_{t} (|s_{t}),s_{t+1} p_{M^{*}}(|s_{t},a_{t})}[_{t=0}^{} ^{t}r(s_{t},a_{t})|,s_{0}],s_{0}_{0}(s)\] (1)

MBRL algorithms aim to learn the dynamic transition distribution model, \(p_{M}(s^{}|s,a)\), by using samples collected from interaction with the real environment via supervised learning. We denote the expected return under the model \(M\) of the policy \(\) as \(V^{}_{M}\) and denote that under the real environment of the policy \(\) derived from the model \(M\) as \(V^{|M}\). Additionally, we assume that \(r(s,a)\) is unknown to the model \(M\) and the model will predict \(r_{M}\) as the reward function. Besides, we denote \(\) as a parameterized family of models and \(\) as a parameterized family of policies.

Let \(d^{}_{M}(s,a)\) denote the normalized discounted visitation probability for \((s,a)\) when starting at \(s_{0}_{0}\) and following \(\) under the model \(M\). Let \(p^{}_{M,t}(s)\) denote the probability of visiting \(s\) at timestep \(t\) given the policy \(\) and the model \(M\).

\[d^{}_{M}(s,a)=(1-)_{t=0}^{}^{t}p^{}_{M,t}(s)(a |s)\] (2)

We define the total variation distance (TVD) estimator as \(D_{TV}(||)\) and the second-order Wasserstein distance estimator as \(W_{2}(,)\).

## 4 USB-PO framework

In this section, we demonstrate a detailed description of our proposed algorithmic framework. i.e., USB-PO. In Section 4.1, a meta-algorithm of the USB-PO framework is provided as a generic solution. In Section 4.2, we theoretically show how to unify model shift and model bias to get a performance improvement guarantee3. In Section 4.3, the practical algorithm is proposed to instantiate the USB-PO framework.

### The overall algorithm

The general algorithmic framework of USB-PO is depicted in Algorithm 1, where the main difference compared to the existing MBRL algorithms is the two-phase model learning process, namely phase 1 and phase 2. Phase 1 uses traditional MLE loss to train the model, which may impair the performance by excessive model updates due to only considering the impacts of model bias. To mitigate this problem, we introduce phase 2 to further fine-tune the model updates, whose optimization objective is defined as Eq.(3).

\[*{arg\,min}_{p_{M_{2}}}_{phase2}=_{d^{ }_{M_{1}}}[W_{2}(p_{M_{1}},p_{M_{2}})+W_{2}(p_{M_{2}},p_{M^{*}})]\] (3)Eq.(3) unifies the model shift term and the model bias term in the second-order Wasserstein distance form, namely \(W_{2}(p_{M_{1}},p_{M_{2}})\) and \(W_{2}(p_{M_{2}},p_{M^{*}})\), thus achieving adaptive adjustment of their impacts during the fine-tuning process. As demonstrated in Section 4.2 and Section 5.4, this is not equivalent to the traditional methods of limiting the magnitude of model updates, but rather beneficial to get a performance improvement guarantee.

### Theoretical proof

**Definition 1** (Performance Difference Bound).: _Recalling that \(V^{_{i}|M_{i}}\) denotes the expected return under the real environment of the policy \(_{i}\) derived from the model \(M_{i}\). The lower bound on the true return gap of \(_{1}\) and \(_{2}\) can be stated as,_

\[V^{_{2}|M_{2}}-V^{_{1}|M_{1}} C\] (4)

By constantly increasing the value of \(C\), the lower bound on the performance difference is guaranteed to be lifted, leading to performance improvement. Therefore, we try to take model shift and model bias into the formulation of \(C\) and maximize it to achieve our goal.

**Theorem 1** (Performance Difference Bound Decomposition).: _Let \(M_{i}\) be the evaluated model and \(_{i}\) be the policy derived from the model. The performance difference bound can be decomposed into three terms,_

\[V^{_{2}|M_{2}}-V^{_{1}|M_{1}}=(V^{_{2}|M_{2}}-V^{_{2}}_{M_{2}})-( V^{_{1}|M_{1}}-V^{_{1}}_{M_{1}})+(V^{_{2}}_{M_{2}}-V^{_{1}}_{M_{1}})\] (5)

Obviously, compared to directly optimizing the return discrepancy of each iteration , the performance difference bound chooses to optimize the return discrepancy of two adjacent iterations, namely \(V^{_{2}|M_{2}}-V^{_{2}}_{M_{2}}\) and \(V^{_{1}|M_{1}}-V^{_{1}}_{M_{1}}\) respectively, and the expected return variation between these two iterations, namely \(V^{_{2}}_{M_{2}}-V^{_{1}}_{M_{1}}\), demonstrating better rigorousness. For further discussion, we introduce the following theorem.

**Theorem 2** (Return Bound).: _Let \(R_{max}\) denote the bound of the reward function, \(_{}\) denote \(_{s}D_{TV}(_{1}||_{2})\) and \(^{M_{2}}_{M_{1}}\) denote \(_{(s,a) d^{_{1}}_{M_{1}}}[D_{TV}(p_{M_{1}}||p_{M_{2}})]\). For two arbitrary policies \(_{1},_{2}\), the expected return under two arbitrary models \(M_{1},M_{2}\) can be bounded as,_

\[V^{_{2}}_{M_{2}}-V^{_{1}}_{M_{1}}-2R_{max}(}{(1 -)^{2}}+}^{M_{2}}_{M_{1}})\] (6)

By using Eq.(6), we can easily bound the decomposition terms of the performance difference bound in Eq.(5).

**Theorem 3** (Decomposition TVD Bound).: _Let \(^{_{i}}_{M_{i}}\) denote \(_{(s,a) d^{_{i}}_{M_{i}}}[D_{TV}(p_{M_{i}}||p_{M^{*}})]\). Let \(M_{i}\) be the evaluated model and \(_{i}\) be the policy derived from the model. The decomposition terms can be bounded as,_

\[V^{_{2}|M_{2}}-V^{_{1}|M_{1}}}{(1-)^{2}}( ^{_{1}}_{M_{1}}-^{_{2}}_{M_{2}}-^{M_{2}}_{M_{1}} )-_{}}{(1-)^{2}}\] (7)Notably, the model shift term and the model bias term in TVD form, namely \(D_{TV}(p_{M_{1}}||p_{M_{2}})\) and \(D_{TV}(p_{M_{2}}||p_{M^{*}})\), are already present in the right-hand side of the Eq.(7). However, due to the unknown term \(_{2}\), we can not sample from \(d_{M_{2}}^{_{2}}\). Thus, we need to make a further transformation about the Eq.(7).

**Theorem 4** (Unified Model Shift and Model Bias Bound).: _Let \(\) denote the constant \(}{(1-)^{2}}\) and \(\) denotes \(_{(s,) d_{M_{1}}^{_{1}}}[D_{TV}(p_{M_{2}}||p_{M^{*} })]-_{(s,) d_{M_{2}}^{_{2}}}[D_{TV}(p_{M_{2}}||p_{ M^{*}})]\). Let \(M_{i}\) be the evaluated model and \(_{i}\) be the policy derived from the model. The unified model shift and model bias bound can be derived as,_

\[& V^{_{2}|M_{2}}-V^{_{1}|M_{1}}\\ &((_{(s,) d_{M_{1}}^{_{1} }}[D_{TV}(p_{M_{1}}||p_{M^{*}})-D_{TV}(p_{M_{1}}||p_{M_{2}})-D_{TV}(p_{M_{2}} ||p_{M_{*}})]+)-_{})\] (8)

The \(\) term in Eq.(8) is still intractable. However, the fact that it covers \(d_{M_{1}}^{_{1}}\) and \(d_{M_{2}}^{_{2}}\) reminds us that we may explore its relationship with the three TVD terms lying in the expectation of \(d_{M_{1}}^{_{1}}\).

**Theorem 5** (\(||\) Upper Bound).: _Let \(M_{i}\) be the evaluated model and \(_{i}\) be the policy derived from the model. The term \(\) can be upper bounded as:_

\[||_{(s, ) d_{M_{1}}^{_{1}}}[D_{TV}(p_{M_{1}}||p_{M_{2}})_{s,}D_ {TV}(p_{M_{2}}||p_{M^{*}})]+}{1-}_{s,}D_ {TV}(p_{M_{2}}||p_{M^{*}})\] (9)

Under the online setting, it is assumed that the error caused by policy shift, namely \(_{}\), compared to model bias has a relatively small scale . Therefore, we ignore the minor influence brought by the policy. Additionally, the terms lying in the expectation of \(d_{M_{1}}^{_{1}}\) in Eq.(9) is the product of the model shift term and the model bias term in TVD form, both of which have the range \(\), making the upper bound of \(||\) become a higher-order term compared to each of them alone. Thus, the critical element to lift the lower bound in Eq.(8) is the terms lying in the expectation of \(d_{M_{1}}^{_{1}}\). By maximizing these terms, namely minimizing \(D_{TV}(p_{M_{1}}||p_{M_{2}})+D_{TV}(p_{M_{2}}||p_{M^{*}})\)4, we can achieve unifying model shift and model bias and adaptively fine-tuning the model updates to get a performance improvement guarantee.

To make it practically feasible, we make additional assumptions. Specifically, let \((,)\) be a measurable space and \(\) is a set of functions mapping \(\) to \(\) that contains \(V_{M}^{}\). When \(V_{M}^{}\) is \(L_{v}\)-Lipschitz with respect to a norm \(||||\), the integral probability metric  of two arbitrary dynamic transition function \(M,M^{}\) defined on \(\) is as Eq.(10). Notice that if \(1 p q\), \(W_{p}(p_{M},p_{M^{}}) W_{q}(p_{M},p_{M^{}})\), and we can get \(W_{1}(p_{M},p_{M^{}}) W_{2}(p_{M},p_{M^{}})\). Hence, Eq.(3) can be applied as the optimization objective to get a performance improvement guarantee.

\[_{f}|_{s^{} p_{M}}[f(s^{})]- _{s^{} p_{M^{}}}[f(s^{})]|=}{1- }D_{TV}(p_{M}||p_{M^{}})=L_{v}W_{1}(p_{M},p_{M^{}})\] (10)

### Practical implementation

We now instantiate Algorithm 1 by demonstrating an explicit approach. To better clarify, we would like to state the four design decisions in detail: (1) how to parametrize the model, (2) how to estimate the model bias term and model shift term in phase 2, (3) how to use the model to generate rollouts and (4) how to use the model rollouts to optimize the policy \(\).

Predictive Model.We use a bootstrap ensemble of dynamic models \(\{p_{M}^{1},...,p_{M}^{B}\}\), whose elements are all probabilistic neural network, outputting the Gaussian distribution with diagonal covariance: \(p_{M}^{i}(s_{t+1},r_{t+1}|s_{t},a_{t})=(_{M}^{i}(s_{t},a_{t}), _{M}^{i}(s_{t},a_{t}))\). The probabilistic ensemble model can capture the aleatoric uncertainty arising from inherent stochasticities and the epistemic uncertainty corresponding to ambiguously determining the underlying system due to the lack of sufficient data . In the field of MBRL, properly handling these uncertainties can achieve better asymptotical performance. Following the previous work , we select a model uniformly from the elite set to generate the transition at each step in the rollout process. In phase 1, the ensemble models are trained on shared but differently shuffled data, where the optimization objective is MLE .

Model Shift and Model Bias Estimation.Recalling that the model shift term refers to \(W_{2}(p_{M_{1}}||p_{M_{2}})\) and the model bias term refers to \(W_{2}(p_{M_{2}}||p_{M^{*}})\). For arbitrary two Gaussian distributions \(p(_{1},_{1})\) and \(q(_{2},_{2})\), the second-order Wasserstein distance between them is derived as Eq.(11) .

\[W_{2}(p,q)=-_{2}||_{2}^{2}+(_{1}+_{2} -2(_{2}^{}_{1}_{2}^{})^{})}\] (11)

Let \(k_{1}\) denote the index of the selected model from \(M_{1}\) ensemble and \(k_{2}\) denote that from \(M_{2}\) ensemble. For the model shift term, we approximate it as \(W_{2}(p_{M_{1}}^{k_{1}},p_{M_{2}}^{k_{2}})\). For the model bias term, we approximate it as \(_{b=1,b k_{2}}^{B}W_{2}(p_{M_{2}}^{k_{2}},p_{M_{2}}^{k_{ 2}})\).

Model Rollout.Following the previous work , we use short branch rollouts to alleviate the impacts of compounding error.

Policy Optimization.Our work allows being plugged in many SOTA MFRL algorithms, e.g. SAC , TD3 , etc. Here we employ SAC as an example.

## 5 Experiment

Our experiments are designed to investigate four primary questions: (1) Whether we need to propose an algorithm similar to USB-PO to adaptively adjust the impacts of model shift? (2) How well does USB-PO perform on reinforcement learning benchmark tasks compared to SOTA MBRL and MFRL algorithms? (3) How to understand USB-PO? (4) Does USB-PO have a similar performance on different learning rate settings in phase 2?

### Necessity of USB-PO

We recall that CMLO  sets a fixed threshold to constrain the impacts of model shift, which we argue CMLO is heavily dependent on this threshold. To valid our statement, we devise an experiment that sets three different thresholds for CMLO on Walker2d environment in MuJoCo . As Figure 1 shows, the performance corresponding to the other two thresholds (1.0 and 5.0) is severely affected. Therefore, setting a fixed threshold to constrain is inappropriate, and a smarter way like USB-PO should be applied to adaptively adjust the impacts of model shift.

### Comparison with baselines

To highlight our algorithm, we compare USB-PO against some SOTA MBRL and MFRL algorithms. The MBRL baselines include CMLO , which carefully chooses the threshold for different environments to constrain the impacts of model shift, ALM , which learns the representation, model, and policy into one objective, PDML , which uses historical policy sequence to aim the model training process, MBPO , which is the variant without our proposed model fine-tuning process and uses return discrepancy to get a performance improvement guarantee. The MFRL baselines include SAC , the SOTA MFRL algorithm in terms of asymptotic performance and PPO , which explores monotonic improvement under the model-free setting.

We evaluate USB-PO and these baselines on six MuJoCo  continuous control tasks in OpenAI Gym , covering Humanoid, Walker2d, Ant, HalfCheetah, Hopper and Inverted-Pendulum, with more details showing in the appendix. To be fair, we employ the standard 1000-step version of these tasks with the same environment settings.

Figure 2 shows the learning curves of all compared methods, together with the asymptotic performance. The results show that our algorithm is remarkably advanced over the MFRL algorithms

Figure 1: CMLO performance curves for different threshold settings over different random seeds, where 3.0 is the threshold recommended in the paper.

regarding sample efficiency, along with asymptotic performance on par with the SOTA MFRL algorithm SAC. Compared to the MBRL baselines, our method achieves higher sample efficiency and better final performance. Notably, compared to CMLO, which requires a finely chosen threshold for each environment to constrain the impacts of model shift, our method utilizes the same learning rate for the fine-tuning process in all environments. This further validates unifying the model shift and the model bias to adjust the model updates adaptively is reasonable.

Computational Cost.We report our computational cost compared to MBPO  in Appendix D.4. Although USB-PO is a two-phase model training process, continuing to use the fine-tuned model for the next iteration has the potential to accelerate model convergence and then possibly reduce the training time.

### How to understand USB-PO

In this section, we first design an experiment to illustrate the value magnitude of \(\), the model shift term and the model bias term, validating whether using the second-order Wasserstein distance satisfies the prerequisites for getting a performance improvement guarantee, and then use more in-depth experiments to illustrate how USB-PO works and show its superiority.

Value Magnitude.We choose 2 challenging tasks in MuJoCo, HalfCheetah and Walker2d, and plot the value of \(\), the model shift term and the model bias term during the training process respectively. To approximate the \(\) practically, we use the samples from the model replay buffer before the update of the model and the policy to approximate sampling from \(d_{M_{1}}^{_{1}}\) and use the samples generated by them after update to approximate sampling from \(d_{M_{2}}^{_{2}}\).

As shown in Figure 3, the magnitude of \(\) generally oscillates in a slight manner around 0 while the magnitude of model shift term and the model bias term are much larger than \(\) even in the converged cases. Therefore, it is reasonable to ignore the impacts of \(\) in Eq.(8). This further supports that minimizing Eq.(3) can get a performance improvement guarantee.

Figure 3: The value magnitude of the \(\), the model shift term and the model bias term during the training process in HalfCheetah and Walker2d.

Figure 2: Comparison against baselines on continuous control benchmarks. Solid curves refer to the mean performance of trials over different random seeds, and shaded area refers to the standard deviation of these trials. Dashed lines refer to the asymptotic performance of SAC (at 3M steps). Note that PDML is not evaluated in InvertedPendulum-v2.

Working Mechanism.To illustrate the significance of our method, we calculate the optimization objective value before and after the fine-tuning process and plot their difference, as shown in Figure 4. All random seeds show the consistent result that at the beginning of training the sum of model bias and model shift is large and thus the fine-tuning magnitude is relatively large. As the model is trained to converge, the fine-tuning process also gradually converges to ensure the stability of the model. Notice that in some cases the fine-tuning process does not choose to fine-tune the model updates actually (0 refers to no update, which means the updates in phase 1 are reasonable), which further supports our theorem and the motivation of this paper, i.e., actively adaptive adjustment of the model updates to get a performance improvement guarantee, rather than passively waiting for model shift to surpass a given threshold to make a substantial update . Based on the performance comparison in Figure 2, we argue that actively adaptive updates are more beneficial.

We further plot the difference of the model shift term, the model bias term and the average prediction error before and after the fine-tuning process. As shown in Figure 5 (a), when the fine-tuning actually operates, the difference of the model shift term and the model bias term are both positive. As shown in Figure 5 (b), the fine-tuning process has a positive effect on the reduction of the average prediction error. These observations suggest that USB-PO has other positive effects during the fine-tuning process, i.e. potentially reducing both model shift and model bias and leading to model overfitting avoidance.

Figure 4: (a) the difference of the optimization objective value before and after the fine-tuning process during the training process over 5 random seeds. (b) the average prediction error during the training process over these random seeds. The model is generally near convergence when training times reach 1K.

Figure 5: (a) we choose a specific random seed to show the details of the first 30 training times, covering the difference of optimization objective value, the model shift term and the model bias term before and after the fine-tuning process. (b) the difference of average prediction error before and after the fine-tuning process during the training process over the previously used random seeds.

Additionally, the change in the adjustment magnitude of the average prediction error indicates that the model can converge under the role of the fine-tuning process, which is consistent with Figure 4, forming bi-verification.

To conclude, when USB-PO actively recognizes that an improper update happens, it performs a pull-back operation (the model fine-tuning process) to secure the performance improvement guarantee while avoiding model overfitting. In contrast, when USB-PO considers that a reasonable update is done, no actual operation will be taken. According to the performance comparison results in Figure 2, it is verified that the model quality plays a determinant role in the Dyna-Q  algorithms.

### Ablation study

In this section, we design 3 ablation experiments to strengthen our superiority.

Optimization Objective Variants.We set three variants for our optimization objective, covering (1) without the model shift term and the model bias term, which is equal to MBPO , (2) with the model shift term, (3) with the model bias term. As shown in Figure 6, only optimizing the model shift term results in a drop in sample efficiency since the fine-tuning process makes \(M_{2}\) tend to update towards \(M_{1}\) and only optimizing the model bias term leads to performance deterioration due to excessive model updates.

Not Equivalent to Limiting the Update Magnitude.We set different learning rates for MBPO to compare with USB-PO. As Figure 7 shows, USB-PO is more dominant from the perspective of the average return, which further strengthens that USB-PO is not equivalent to limiting the update magnitude, but rather beneficial to get a performance improvement guarantee.

Learning Rate Performance Comparison.We set up an ablation experiment on the learning rate of the fine-tuning process. As shown in Figure 8, unlike CMLO which is strongly dependent on a carefully chosen threshold for each environment to constrain the impacts of model shift, USB-PO is less sensitive to the learning rate of phase 2.

## 6 Discussion

In this paper, we propose a novel MBRL algorithm called USB-PO, which can unify model shift and model bias, enabling adaptive adjustment of their impacts to get a performance improvement guarantee. We further find that our method can potentially reduce both model shift and model bias, leading to model overfitting avoidance. Empirical results on several challenging benchmark tasks validate the superiority of our algorithm and more in-depth experiments are conducted to demonstrate our mechanism. The limitation of our work is we do not investigate the change of model shift and model bias in this optimization problem theoretically. Therefore, one direction that merits further research is how to solidly interpret the variation of model shift and model bias in this fine-tuning process. Further, we aim to propose a general algorithmic framework in this paper. Due to the simple design of our model architecture, we find it difficult to capture some complex and practical locomotion. We hope to combine our adaptive adjusting technique with more advanced model architectures in the future.