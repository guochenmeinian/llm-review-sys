# A Surprisingly Simple Approach to

Generalized Few-Shot Semantic Segmentation

 Tomoya Sakai

IBM Research - Tokyo

tomoya.sakai2@ibm.com

&Haoxiang Qiu

IBM Research - Tokyo

haoxiang.qiu@ibm.com

&Takayuki Katsuki

IBM Research - Tokyo

kats@jp.ibm.com

&Daiki Kimura

IBM Research - Tokyo

daiki@jp.ibm.com

&Takayuki Osogami

IBM Research - Tokyo

osogami@jp.ibm.com

&Tadanobu Inoue

IBM

inouet@jp.ibm.com

###### Abstract

The goal of _generalized_ few-shot semantic segmentation (GFSS) is to recognize _novel-class_ objects through training with a few annotated examples and the _base-class_ model that learned the knowledge about the base classes. Unlike the classic few-shot semantic segmentation, GFSS aims to classify pixels into both base and novel classes, meaning it is a more practical setting. Current GFSS methods rely on several techniques such as using combinations of customized modules, carefully designed loss functions, meta-learning, and transductive learning. However, we found that a simple rule and standard supervised learning substantially improve the GFSS performance. In this paper, we propose a simple yet effective method for GFSS that does not use the techniques mentioned above. Also, we theoretically show that our method perfectly maintains the segmentation performance of the base-class model over most of the base classes. Through numerical experiments, we demonstrated the effectiveness of our method. It improved in novel-class segmentation performance in the \(1\)-shot scenario by \(6.1\%\) on the PASCAL-\(5^{i}\) dataset, \(4.7\%\) on the PASCAL-\(10^{i}\) dataset, and \(1.0\%\) on the COCO-\(20^{i}\) dataset. Our code is publicly available at https://github.com/IBM/BCM.

## 1 Introduction

_Semantic segmentation_ is a vital task in various visual understanding systems, and the goal is to obtain pixel-level semantic categories . Recent developments in convolutional neural networks  and vision transformers  have pushed the limits of semantic segmentation. With a large amount of annotated images, we can obtain an accurate model that can recognize objects in the training data. In real-world applications, however, the learned model will encounter _novel-class_ objects that are not classified in _base classes_, i.e., classes that are not annotated in the training data.

To solve this problem, _few-shot_ semantic segmentation (FSS) aims to recognize novel-class objects with a few annotated images while using the learned model, which has knowledge about the base-class information. Although various FSS methods have been proposed , FSS only handles novel-class object recognition, which restricts its applicability since base classes will still appear at inference in practice.

_Generalized_ FSS (GFSS) aims to recognize both base- and novel-class objects  and is regarded as a more practical setting than FSS. Current GFSS methods rely on several techniques such as using combinations of customized modules , carefully designed loss functions , meta-learning , and transductive learning . These techniques improved GFSS performance at thecost of implementation and computation time. For example, such a customized module is not always supported in the target framework, methods based on meta-learning require several hours to train customized modules, and transductive learning optimizes models during inference, which is not suitable for applications that require quick responses. However, we found that a simple rule and standard supervised learning improve GFSS performance.

In this paper, we propose a simple yet effective GFSS method that does not use the above techniques. As illustrated in Fig. 1, our idea is mining base classes closely related to classifying novel classes. We thus refer to our method as _base-class mining_ (BCM). Surprisingly, BCM perfectly maintains the segmentation performance of the base-class model over a subset of base classes. Since maintaining the segmentation performance of the base-class model is critical in GFSS, BCM will be beneficial, especially when a performance difference from the base-class model confuses users.

Our contributions are summarized as follows.

* We propose a simple yet effective GFSS method based on a simple rule and well-known supervised learning techniques, which can be regarded as a strong alternate baseline without transductive learning.
* We theoretically show that the performance of the base-class model for a subset of base classes is perfectly maintained, which is the first theoretical finding about base-class segmentation performances in GFSS, to the best of our knowledge.
* We demonstrated the effectiveness of BCM on the PASCAL-\(5^{i}\), PASCAL-\(10^{i}\), and COCO-\(20^{i}\) datasets. BCM substantially improved novel-class segmentation performance in the \(1\)-shot scenario by \(6.1\%\) on PASCAL-\(5^{i}\) and \(4.7\%\) on PASCAL-\(10^{i}\).

## 2 Related work

### GFSS setting

In GFSS , multiple novel classes need to be classified, i.e., multi-class classification, in addition to classifying base classes. This differs from the FSS setting of single novel-class classification, i.e., binary classification.

We consider the _practical_ GFSS setting , in which the available resources are _few-shot_ annotated images for novel classes and the _base-class model_ trained using standard learning methods. The existing setting  requires annotated base-class samples for training the base-class model and tuning a GFSS model that can recognize both base and novel classes. For example, the number of base classes is \(60\) (excluding the background) on COCO-\(20^{i}\) case, meaning that we need to collect \(300\) annotated images for tuning in the \(5\)-shot scenario, other than training the base-class model. Such additional samples for base classes are not necessary in the practical GFSS setting, resulting in using those samples for training the base-class model, which is an advantage of the practical GFSS setting.

### GFSS methods

The major challenges with current GFSS methods are i) attaining better recognition performance for novel classes and ii) maintaining the segmentation performance of the base-class model. _Context-aware prototype learning_ (CAPL)  enhances prototypes with a few annotated images and employs

Figure 1: Illustration of _base-class mining_ (BCM) with three novel classes: “train”, “tv”, and “couch”. Base-class model first outputs prediction. If prediction is one of chosen base classes, corresponding model outputs prediction. Otherwise, prediction of base-class model is used as it is. Simple rule finds which base class is related to novel classes. Models for novel classes are trained by standard supervised learning.

a balancing mechanism of prototypes for base and novel classes. _Base and meta_ (BAM) [14; 18] designed customized modules to learn knowledge from few-shot data and combined predictions of both base and novel classes on the basis of thresholds. POP  and PCN  use similar approaches. _Distilled information maximization_ (DIaM)  does not depend on customized modules, instead, uses the _information maximization principle_ and designs a loss function on the basis of _knowledge distillation_ to preserve base-class knowledge. DIaM uses the transductive learning approach , which is not suitable for applications that require quick responses.

Compared with the above methods, BCM does not rely on carefully customized models, various combinations of loss functions, or transductive learning.

### Continual semantic segmentation

The GFSS setting relates to another emergence problem setting known as _continual semantic segmentation_ (CSS) [24; 25; 26] in which the new classes appear in a continual learning manner. The CSS setting is reduced to the GFSS setting if several novel classes appear with a few annotations in a single step. Coincidentally, a previous study  empirically found a phenomenon similar to our idea that a novel class is classified as a base class, through qualitative analyses of their method. However, their method was not designed for the few-shot learning setting, meaning that how to improve GFSS performances is unclear. In contrast to their findings, BCM explicitly integrates the idea of the relation between base and novel classes with the architecture. These differences in problem settings and architectures differentiate CSS-based methods from BCM.

## 3 Preliminaries

### Problem settings

We consider the following practical GFSS setting .

Let \(^{H W 3}\) denote an RGB image of height \(H\) and width \(W\), and \(^{H W}\) be its corresponding segmentation map, where \(\{0,1,2,3,\}\) is a set of object classes. Let \([\,\,]_{j}\) indicate the \(j\)-th element of a matrix, where \(j\{1,,HW\}\). If \([]_{j}=y\), the object \(y\) exists at the \(j\)-th pixel.

Let \(_{}\) and \(_{}\) be the sets of _base_ and _novel_ classes, respectively, such that \(_{}_{}=\) and \(_{}=_{}_{}\). We use the class '0' for background, which is often the case with implementation. For the sake of simplicity, we include the background into \(_{}\), e.g., \(_{}=\{0,1,2,3\}\) and \(_{}=\{4,5\}\).

We have a learned base-class model \(_{}\), which is trained with a large amount of annotated images by using standard semantic-segmentation methods . Given \(\), \(_{}\) returns a base-class segmentation map \(}_{}_{}^{H W}\). Similarly to the _practical_ GFSS setting , we do not assume the customized architecture for \(_{}\), enabling us to easily use cutting-edge foundation models .

A \(K\)-shot dataset contains \(K\) examples with its ground-truth mask for each novel class \(y_{}\), e.g., if \(K=5\) and \(|_{}|=5\), we have \(25\) annotated images. Note that \(K\) examples for base classes are not necessary, as discussed in Sec. 2.1.

Our goal is to obtain the segmentation map \(}_{}_{}^{H W}\) computed using the prediction model for GFSS, denoted as \(_{}\).

### Evaluation metric

The _mean intersection-over-union_ (mIoU) is widely used in reporting the performance of segmentation methods . Let us first define the IoU for a class \(y^{}\) as

\[_{y^{}}(,}):=^{HW} _{y^{}}([]_{j},[}]_{j})}{_{j=1}^{HW} _{y^{}}([]_{j},[}]_{j})},\] (1)

where \(_{y^{}}(y,):=[y=y^{}][=y^{}]\), \(_{y^{}}(y,):=[y=y^{}]+[ =y^{}]-[y=y^{}][=y^ {}]\), and \([]\) is the indicator function taking \(1\) if \(\) is true, 0 otherwise. Here, we consider a single sample for evaluation, but it can be easily extended to multiple samples by adding the summation over samples to the numerator and denominator in Eq. (1), respectively. Finally, the mIoU is computed by

\[_{}(,}):=|} _{y^{}}_{y^{}}(,}).\] (2)

For example, if \(\) is \(_{}\), it will be the mIoU over novel classes.

## 4 Proposed method

We now present BCM.

### Training

Training is divided into two steps: 1) finding the relationship between base and novel classes, and 2) training models for classifying novel classes for each chosen base class.

Step 1.We input \(\) into \(_{}\) and obtain \(}_{}\). For each pixel of the annotated object, we compare \(}_{}\) and \(\) and record co-occurrences of base and novel classes. We then count the co-occurrences, find the top-\(s\) co-occurred base class for each novel class, referred to as the _top-\(s\) strategy_, and obtain chosen base classes denoted as \(\). Finally, we construct the mapping from a base class to novel classes, called _base-novel mapping_ (BNM). Figure 2 illustrates the creation of BNM with the top-\(1\) strategy from the \(1\)-shot dataset.

Step 2.For each chosen base class \(\), we train a model \(g_{}\) with the modified \(K\)-shot dataset where labels are converted into \(\) if they are novel classes irrelevant to \(\) or the background. Taking the example in Fig. 1(c), when \(=1\), the irrelevant novel-class labels '\(4\)' and '\(5\)' and the background label '\(0\)' are replaced with '\(1\)'. Then, \(_{=1}\) returns either '\(1\)' or '\(6\)' as the prediction.

To obtain the learned model \(_{}\), we can use any learning method, such as minimizing the cross-entropy loss or effective losses used in the previous studies.

### Inference

Inference is analogous to training. For a test image \(\), we first obtain the base-class prediction \(}_{}\). For each pixel \(j\), if \([}_{}]_{j}=\), we then obtain the prediction of the corresponding model \(_{}\) and overwrite \([}_{}]_{j}\) with the output of \(_{}\). Figure 3 illustrates how we obtain the segmentation map of BCM. We summarize the flow of BCM in Fig. 4.

### Preventing catastrophic forgetting

Maintaining the base-class segmentation performance is crucial in GFSS. We theoretically show that BCM perfectly maintains the segmentation performance of most of the base classes without resorting to, e.g., knowledge distillation  for training models.

Figure 2: Illustration of BNM creation. For illustration purpose, image of size \(3 3\) is used. (a) Count co-occurrences, i.e., (base class, novel class) pairs. There are three \((0,4)\) and one \((2,4)\) co-occurrences. (b) Aggregate co-occurrence counts for all samples and create co-occurrence count table. (c) Create BNM from co-occurrence count table, where top-\(1\) strategy finds base class with largest co-occurrences (shaded cell in Fig. 1(b)) for each novel class.

We first formulate the prediction of BCM described in Sec. 4.2. Let \(_{}\) be the prediction of the base-class model at the \(j\)-th pixel and \(_{}\) be the prediction of \(_{}\) at the \(j\)-th pixel. The prediction of BCM at the \(j\)-th pixel, \(_{}\), is obtained by

\[_{}=_{}& _{},\\ _{=_{}}&.\] (3)

The above prediction mechanism leads to the following proposition:

**Proposition 4.1**.: _Let \(}_{}\) and \(}_{}\) be the predictions of the base-class model and BCM, respectively. The mIoUs of \(}_{}\) and \(}_{}\) over \(_{}\) are the same:_

\[_{_{}}(, {}_{})=_{_{} }(,}_{}).\] (4)

_If \(||\) is small, the segmentation performance of most of the base classes is perfectly maintained._

Proof.: For any \(y^{}\), if \([}_{}]_{j}=y^{}\), then \([}_{}]_{j}=[}_{}]_{j}\) by definition of the BCM prediction. Then, for any \(y^{}_{}\),

\[_{y^{}}(,}_{})=_ {y^{}}(,}_{}).\] (5)

Taking the average of \(_{y^{}}\) over \(_{}\), we obtain Eq. (4). 

Intuitively, since BCM uses the prediction of the base-class model as it is for a subset of base classes, the mIoU over those base classes is the same as \(_{}\). Proposition 4.1 shows that BCM partially prevents catastrophic forgetting [28; 29]. In our experiments, \(||\) tended to be small, resulting in the mIoU over base classes being almost maintained.

### Lightweight implementation

Since the size of training data for novel classes is not large in GFSS, training deep neural networks for \(_{}\) is impractical. We thus use the base-class model \(_{}\) as the feature extractor of \(_{}\) and train linear models as the last layer of \(_{}\) with the \(K\)-shot data.

To train linear models, we can use off-the-shelf libraries, such as _Scikit-learn_, meaning that training time will be fast, compared with the end-to-end training on GPU. Since the number of background pixels is much larger than that of objects of interest, we applied sampling techniques for imbalanced data  to training data, such as under-sampling.

Regarding the top-\(s\) strategy, we used \(s=1\) from the performance and computation time viewpoint. The effect of \(s\) in the top-\(s\) strategy is discussed in Sec. 5.6.

### Further performance improvement

Since our implementation is to train simple linear models, we can easily use various techniques to improve GFSS performance. We explain two effective and easy-to-use techniques used in our experiments as followsPre-processing.We can use _Tukey's ladder of powers transformation_, known as the effective transformation in few-shot learning . Specifically, let \(\) be the \(d\)-dimensional feature vector extracted by \(_{}\). Tukey's ladder of powers transformation is defined as

\[}=^{}& 0,\\ &,\] (6)

where \(\) is a hyper-parameter, and the power and logarithm operations are element-wise. When \(=1\), the original feature is used. In a previous study , \(=0.5\) was recommended as the default value; thus, we used it in our experiments.

These pre-processed feature vectors are used for \(_{}\) only since the change in feature representation for \(_{}\) without retraining would downgrade performance. Note that to apply similar pre-processing to the existing methods, it is crucial to take into account the adverse effect on total performance.

Ensemble learning.We can use ensemble learning [34; 35] to improve GFSS performance. Unlike the existing GFSS methods, the computation time of BCM will be short since training a linear model for \(g_{}\) is lighter than tuning deep neural networks in an end-to-end manner.

We introduce _shot-wise_ ensemble learning to few-shot learning when \(K>1\). This involves first preparing multiple \(L\)-shot datasets (\(L K\)) by drawing samples from the \(K\)-shot dataset then aggregating outputs of models trained with the \(L\)-shot datasets. In our experiments, we split the \(5\)-shot dataset into five \(1\)-shot datasets and obtained six models by using five \(1\)-shot and one \(5\)-shot datasets. In inference, we computed a weighted average of the outputs of the models. The weights can be determined by, e.g., validation data or pre-defined values. In our experiments, we set one for the model with the \(1\)-shot dataset and five for the model with the \(5\)-shot dataset for simplicity.

## 5 Experiments

### Setup

Datasets.We used three FSS datasets: _PASCAL-5\({}^{i}\)_[4; 36; 37], _PASCAL-10\({}^{i}\)_[20; 36; 37], and _COCO-20\({}^{i}\)_[6; 38]. The PASCAL-\(10^{i}\) dataset was introduced to investigate the impact of increasing the number of novel classes .

Methods for comparison.We compared BCM with CAPL , BAM [14; 18],1 and DIaM . Note that DIaM was regarded as a _simple_ method since it trains the last linear layer only, similarly to the simple methods [39; 40] proposed for few-shot object detection.

Evaluation.We report the mIoUs over base and novel classes, referred to as the _Base_ and _Novel_ scores, respectively, where the background was not included in the Base score, similarly to a previous study . We also report the average of the Base and Novel scores, called the _Mean_ score. All reported scores are the average of five independent trials.

Base-class model.We used the publicly available pre-trained model for GFSS,2_pyramid scene parsing network_ (PSPNet)  with the pre-trained ResNet-50 backbone . It was trained with labeled data for base classes by using the stochastic gradient descent optimizer with an initial learning rate of \(2.5 10^{-4}\), momentum of \(0.9\), and weight decay of \(10^{-4}\). The batch size was \(12\), and number of epochs was \(20\) for COCO-\(20^{i}\) and \(100\) for PASCAL-\(5^{i}\) and PASCAL-\(10^{i}\).

Detailed implementation.The implementation of BCM is based on the publicly available DIaM code. We followed the same data-loading and evaluation procedure and replaced the method part with BCM. Specifically, to train novel-class models \(g_{}\) in Sec. 4.1, we used the _logistic regression_ in _Scikit-learn_,3 which uses the _L-BFGS-B_ method with a line-search strategy as the default solver. The regularization parameter was determined from the five-fold cross-validation from the ten candidates \(\{10^{-5},,10^{5}\}\). The default values were used for the other hyper-parameters.

### Main results

Table 1 summarizes the average performance over the five trials for each method in the practical GFSS setting. BCM outperformed the other GFSS methods regarding the Novel and Mean scores. Notably, the Novel scores in the \(1\)-shot PASCAL-\(5^{i}\) and PASCAL-\(10^{i}\) settings substantially improved with BCM. Regarding the Base score, BCM achieved comparable/best performance thanks to it preventing catastrophic forgetting, as discussed in Sec. 4.3. We discuss these results from the viewpoint of our theory in Sec. 5.4.

These results indicate that BCM achieved the best performance without resorting to various techniques used with the other methods, such as meta-learning [43; 44], information maximization principle , and transductive learning . The implementation of BCM was to train the final linear layer only, as described in Sec. 4.4, but we can use cutting-edge architectures and training techniques in practice, leading to further performance improvement.

### Ablation study

We investigated the effect of pre-processing (Tukey's ladder of powers transformation) and ensemble learning, explained in Sec. 4.5.

Table 2 shows the performance of four variations of BCM, i.e., with and without pre-processing and ensemble learning. Compared with the results in Tab. 1, BCM without data pre-processing and ensemble learning outperformed the other methods in the \(1\)-shot setting, showing that the simple rule and standard supervised learning improved the GFSS performance. The effectiveness of data pre-processing was much higher when the number of novel classes was small (see the PASCAL-\(5^{i}\) and PASCAL-\(10^{i}\) settings). However, the pre-processing decreased this performance slightly in the \(1\)-shot COCO-\(20^{i}\) setting.

Our ensemble-learning approach consistently improved GFSS performance, with a roughly \(5\%\) improvement on all datasets. Note that we can use standard ensemble-learning approaches in the \(1\)-shot setting, meaning that further performance improvement is possible in practice.

    & & \)} \\   & &  &  \\  Method & & Base & Novel & Mean & Base & Novel & Mean \\  CAPL  & CVPR’22 & \(64.80\) & \(17.46\) & \(41.13\) & \(65.43\) & \(24.43\) & \(44.93\) \\ BAM  & CVPR’22 & \(\) & \(27.49\) & \(49.55\) & \(\) & \(28.96\) & \(50.28\) \\ DIaM  & CVPR’23 & \(70.89\) & \(35.11\) & \(53.00\) & \(70.85\) & \(55.31\) & \(63.08\) \\ BCM & (Ours) & \(71.15\) & \(\) & \(\) & \(71.23\) & \(\) & \(\) \\  \)} \\  CAPL  & CVPR’22 & \(53.78\) & \(15.01\) & \(34.40\) & \(57.02\) & \(20.40\) & \(38.71\) \\ BAM  & CVPR’22 & \(69.02\) & \(15.48\) & \(42.25\) & \(69.18\) & \(21.51\) & \(45.35\) \\ DIaM  & CVPR’23 & \(\) & \(31.29\) & \(50.77\) & \(\) & \(51.89\) & \(61.07\) \\ BCM & (Ours) & \(70.07\) & \(\) & \(\) & \(70.12\) & \(\) & \(\) \\  \)} \\  CAPL  & CVPR’22 & \(43.21\) & \(7.21\) & \(25.21\) & \(43.71\) & \(11.00\) & \(27.36\) \\ BAM  & CVPR’22 & \(\) & \(14.16\) & \(32.00\) & \(49.85\) & \(16.63\) & \(33.24\) \\ DIaM  & CVPR’23 & \(48.28\) & \(17.22\) & \(32.75\) & \(48.37\) & \(28.73\) & \(38.55\) \\ BCM & (Ours) & \(49.43\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Average mIoU over five trials. Base and Novel represent mIoU scores over base and novel classes, respectively. Mean shows average of Base and Novel scores. Results of comparison methods were obtained from . All methods use ResNet-50 as backbone.

### Number of chosen base classes

Figure 5 shows the frequency of \(||\), the number of chosen base classes, over 20 runs (four splits and five trials). Overall, \(||\) tended to decrease with the increase in \(K\). We hypothesize that noisy pairs appear relatively smaller than frequent pairs when \(K=5\), and the top-\(1\) strategy ignored such a noisy pair. In particular, the median value of the frequency in \(5\)-shot was 1 on PASCAL-\(5^{i}\) and 2 on COCO-\(20^{i}\).

Another observation is that \(||\) was much smaller than the number of base classes. For example, the largest \(||\) was four in the \(5\)-shot COCO-\(20^{i}\) setting, meaning that less than \(7\%( 4/61)\) of base classes (including background) were chosen in the BCM training step. These results indicate that we do not need to prepare \(g_{}\) for many base classes, and training and inference times do not increase rapidly to the number of base classes. Moreover, \(||\) tends to be small in practice, so IoUs on most of the base classes are perfectly maintained, as shown in Proposition 4.1.

### Which classes were chosen?

We explored which class consists of base- and novel-class pairs in the BNM and recorded pairs in the \(5\)-shot COCO-\(20^{i}\) setting. In most cases, the background was chosen, meaning that the base-class model recognized novel-class objects as the background. Sometimes, the pairs summarized in Tab. 3 were chosen, showing that the related classes were chosen with BCM. The results empirically confirmour idea that a novel class is classified as the background or a similar base class with the base-class model.

### Effect of top-\(s\) strategy

We varied \(s\) in the top-\(s\) strategy and investigated how \(s\) affects the segmentation performance and size of \(\) in the \(5\)-shot PASCAL-\(5^{i}\) setting. The results in other settings can be found in Appendix A.1.

Figures 7(a) and 7(b) respectively illustrate the Base and Novel scores with respect to \(s\{1,2,3,5,10\}\), showing that the median and minimum scores decreased as \(s\) increased.

We also show the size of \(\) in Fig. 7(c). The number of chosen base-classes increased, but it was upper-bounded by a certain value. Even though we increased \(s\), the chosen base-classes might be the same. In addition, the increase in \(s\) also led to longer training time. In this sense, smaller \(s\) is preferable from the perspective of computation time.

In summary, a higher \(s\) may result in selecting redundant base classes and cause performance degradation, and larger \(||\) requires many models for \(g_{}\), resulting in longer computation time for training. We thus recommend \(s=1\) since it is the best choice based on GFSS performance and computation time.

### Computation time

The computation time was measured on a machine equipped with an NVIDIA(r) V100, 16 CPU cores, and 32GB memory.

Training time.We plot the training time [s] with BCM in Fig. 6. CAPL and DIaM are not shown since CAPL requires hours of training time due to meta-learning, and DIaM, which is based on transductive learning, does not optimize models other than the inference phase. In the \(1\)-shot scenario, training time was less than 1 min. The training time increased as the number of novel classes increased. Although we used ensemble learning in the \(5\)-shot scenario, the training time in the \(5\)-shot COCO-\(20^{i}\) setting was about 2 min. Note that training was done by CPU computation, meaning that further acceleration is expected by GPU computation.

Inference time.Figure 7 shows the inference time [ms] of CAPL, DIaM, and BCM. Since DIaM is based on transductive learning, the inference time was slower than the inductive methods, i.e., CAPL and BCM. BCM requires computations of the novel-class models in addition to that of the base-class model, but the total inference time was comparable to that of the end-to-end model, i.e., CAPL.

Note that BCM has \(||\) final linear layers for novel classes, leading to a subtle slowdown when switching the layers, unlike the end-to-end computation of CAPL. Our implementation used CPUs for training the final linear layers, as Scikit-learn is used, unlike CAPL on a GPU. This device difference might be another reason for the slowdown. In practice, a more sophisticated implementation will shorten the gap between CAPL and BCM.

Figure 8: Effect of \(s\) in top-\(s\) strategy in \(5\)-shot PASCAL-\(5^{i}\) setting

Conclusion

Summary.We presented a simple yet effective GFSS method called BCM. BCM is based on a mapping between base and novel classes and trains novel-class models by simple supervised learning without resorting to meta-learning and the information maximization principle. Since we can use standard supervised learning, training can be done efficiently using off-the-shelf software. We can use the featurizer and base-class model without modifying their weights, enabling us to use cutting-edge public foundation models with BCM. We theoretically showed that the mIoU over most of the base classes is perfectly maintained. Through numerical experiments, we demonstrated the superior performance of BCM method against state-of-the-art GFSS methods.

Limitations.BCM has limitations that need to be resolved. First, although the final performance of BCM outperformed the other GFSS methods, the performance improvement in the 5-shot scenario was slight, meaning that there is room for improvement. Second, while BCM perfectly maintains the segmentation performances of most of base-classes, it does not improve such performance using novel-class data. A possible direction to resolve these limitations is to investigate the strategy for creating mapping other than the top-\(s\) strategy and use more recent powerful supervised/few-shot learning methods.