# Convolutional Visual Prompt

for Robust Visual Perception

 Yun-Yun Tsai

Columbia University

yunyuntsai@cs.columbia.edu

&Chengzhi Mao

Columbia University

mcz@cs.columbia.edu

&Junfeng Yang

Columbia University

junfeng@cs.columbia.edu

equal contributions

###### Abstract

Vision models are often vulnerable to out-of-distribution (OOD) samples, which often need adaptation to fix them. While visual prompts offer a lightweight method of input-space adaptation for large-scale vision models, they rely on a high-dimensional additive vector and labeled data. This leads to overfitting when adapting models in a self-supervised test-time setting without labels. We introduce convolutional visual prompts (_CVP_) for label-free test-time adaptation for robust visual perception. The structured nature of CVP demands fewer trainable parameters, less than 1% compared to standard visual prompts, combating overfitting. Extensive experiments and analysis on a wide variety of OOD visual perception tasks show that our approach is effective, improving robustness by up to 5.87% over several large-scale models.

## 1 Introduction

Deep models surpass humans when tested on in-distribution data, yet their performance plummets when encountering unforeseen out-of-distribution (OOD) data at test time, such as unexpected corruptions and shiftings . This vulnerability raises serious risks when these models are deployed, especially in safety-critical applications and high-stakes tasks . Prior works have studied how to improve generalization to OOD data at training time , yet little work has been done to adapt the model to OOD at test time, and most of them require to modify model weights .

Visual prompting emerges as an efficient and lightweight method to adapt the model at test time without modifying the model  (previously also called _adversarial reprogramming_). In contrast to finetuning the whole model weights, prompting can modify the original task of the model by providing context in the input space. It requires fewer OOD samples and simplifies model version management for practical applications. However, the OOD samples must be labeled, preventing these methods from combating unforeseen distribution shifts.

Recent work  defends against unseen attacks at test time by repairing the adversarial inputs with "reversal vectors" - high-dimensional prompt vectors directly added to inputs. It generates the prompts by minimizing a self-supervised loss, requiring no prior labeled OOD samples. Unlike adversarial attacks that damage arbitrary pixels by arbitrary amounts (within a given bound), however, structured changes known as visual distribution shifts are not effectively addressed by unstructured high dimensional vector prompts. Given that a self-supervised objective often has a shortcut and trivial solutions , we empirically find prompts without the right structures often improve performance minimally. (Sec. 5.2).

This paper presents **Convolutional Visual Prompt (CVP)**, which uses the convolutional structure as an inductive bias for adapting to visual OOD samples at test time. Prior work has demonstratedthat convolutional operations are effective for handling structured data with local motifs [30; 64]. Inspired by this, CVPs are convolutional kernels with only a small number of tunable parameters - less than 1% of the number of tunable parameters in typical unstructured visual prompts (see Figure 1 for illustration), making CVP extremely lightweight and efficient to adapt.

Our experiments show the importance of structured inductive bias for self-supervised adaptation: compared to high-dimensional free-form visual prompts, standard low-rank structured prompts improve robustness by 3.38%, and convolutional structure in prompts is significantly better than low-rank prompts by 2.94% on CIFAR-10-C. On ImageNet-Rendition, ImageNet-Sketch, and 15 types of unforeseen corruptions for both CIFAR-10 and ImageNet at five different severity levels, CVP improves robustness by 5.87% for popular large-scale visual models, including ResNet50, WideResNet18, and the state-of-the-art vision-language model CLIP . Since our method modifies the input space, it complements established test-time model weight adaptation methods (e.g., TENT , BN , and MEMO ) and can be generalized to multiple self-supervised objectives, such as contrastive learning , rotation , and masked autoencoder (MAE) .

## 2 Related Work

Domain Generalization.OOD data can lead to a severe drop in performance for machine learning models [20; 18; 53; 44; 39; 42]. Domain generalization (DG) aims at adapting the model with OOD samples without knowing the target domain data during training time. Adapting the model on OOD data [74; 10; 32; 73; 71; 55; 39; 42; 62; 59] also improves robustness.

Test-time adaptation is a new paradigm for robustness against distribution shift [40; 59; 71], mostly updating the weights of deep models. BN [55; 33] updates the model using batch normalization statistics, TENT  adapts the model weight by minimizing the conditional entropy on every batch. TTT  attempts to train the model with an auxiliary self-supervision model for rotation prediction and utilize the SSL loss to adapt the model. MEMO  augments a single sample and adapts the model with the marginal entropy of the augmented samples. Test time transformation ensembling (TTE)  proposes to augment the image with a fixed set of transformations and ensembles the outputs through averaging. The only two works that do not update the models is [40; 45], which modifies the pixels of adversarial, not OOD, samples to minimize the self-supervised objective.

Visual Prompting.Prompting was proposed in the natural language processing field to provide context to adapt the model for specific tasks . Leveraging this idea, visual prompts [68; 26] adapt the model with a small number of trainable parameters in input space for vision tasks [9; 43] and foundation models . Others proposed to prompt the samples with adversarial perturbations to repurpose the model for target classification tasks, known as _adversarial reprogramming_[11; 27; 60; 66], sharing the same idea with Visual Prompting. Black-box adversarial reprogramming  reprograms a black-box model for downstream classification tasks with limited data. V2S  reprograms speech recognition model for time-series data classification tasks. Robust visual prompts  are tuned at training time to improve the adversarial robustness of the model under attack. However, this work has not yet been applied to domain generalization where distribution is naturally shifted.

Self-supervised learning (SSL).SSL can learn effective representations from images without annotations [7; 6; 3; 22]. Prior works have shown that representations learned from different pretext tasks

Figure 1: We demonstrate the self-supervised convolutional prompt that adapts model for robust perception. Clean image has a low self-supervision loss, denoted by the distance \(_{s}\) on the contrastive embedding. When an image is corrupted (e.g, a stop sign in foggy weather), the self-supervised loss generally increases. We then apply the convolutional visual prompts on the corrupted samples, which instruct the model to adapt and produce a small self-supervision loss.

(jigsaw puzzles , rotation prediction , image colorization  and deep clustering , etc.) can be leveraged for several downstream tasks such as image classification , object detection  and test-time domain adaptation . Another well-known branch of SSL is contrastive learning, which aims at grouping associated features for a transformation of samples and distancing from other samples with dissimilar features in the dataset [5; 16; 48]. Some of the methods [22; 57; 70] use SSL for outlier detection, which aims to learn generalizable out-of-distribution features and rejects them during the testing time. In contrast to these methods which require information from the targeted domain distribution during the training phase for adaptation, our method can adapt the model without requiring any information on unforeseen domains during the testing phase.

## 3 Test-Time Convolutional Visual Prompting

### Learning Intrinsic Structure with Self-Supervision Task

Standard ways to improve model robustness on OOD data is through making the training robust, where the training algorithm anticipates possible corruptions and distribution shifts at inference time and trains on them . However, anticipating the test-time shifting is a strong assumption which is often unrealistic in the real world. Therefore, we improve robustness at test time by dynamically adapting to unforeseen corruptions and unknown shifts.

The ideal case of adapting the model at inference time is to know the ground truth label for the target task, yet this is impossible given that the test data is not labeled. To significantly improve performance on the downstream classification tasks for unlabeled data, we must choose the right self-supervision task that shares rich information with the target classification task.

There is a large body of work on good self-supervision tasks for representation learning at training time. For example, jigsaw puzzles , rotation prediction , image colorization  and deep clustering  can be applied to several downstream tasks such as image classification , object detection  and test-time domain adaptation .

For visual recognition, a popular self-supervised objective is contrastive learning, which learns a representation that maps the feature of the transformations of the same image into a nearby place. This is formally defined as:

\[_{s}(x)=-_{i,j}[y^{s}_{i,j},z_{j}))/}{_{k}((z_{i},z_{k}))/}],\] (1)

where \(z\) are the contrastive features of \(x\) extracted from pre-trained backbone. \(y^{s}_{i,j}\) is a 0-1 vector for indicating the positive pairs and negative pairs. If \(y^{s}_{i,j}\) is 1, the \(i\)-th feature \(z_{i}\) and \(j\)-th feature \(z_{j}\) are both from the \(x\) sample. Otherwise, they are from different \(x\). We denote \((,)\) the cosine similarity, \(\) as temperature scaling value. We optimize the model parameters for SSL model \(\) by using the contrastive loss. The objective function for training is defined as \(_{_{}}_{(x)_{s}}[_{s}()]\), where \(_{s}\) is the source domain data for training. We train only on the clean samples drawn from the non-corrupted dataset. We compare this task to other self-supervision tasks in our ablation study.

Since we need to train the self-supervised task at training time, the self-supervised task will perform high on the test data that is the same distribution as the training one. Our results find that the

Figure 2: We show the histogram of the contrastive loss distribution on different corruption types. The blue region represents the loss distribution of original sample. The yellow, green, and red regions represent the loss distribution of corrupted samples with different severity (1, 3, and 5). Our plot shows the great shifting in SSL loss distribution between original and corrupted samples.

performance of the self-supervised task drops largely when the distribution is shifted at test time. See Figure 2. This suggests that the information that is useful for the self-supervised task is collaterally corrupted in addition to the classification performance drop.

Prior work demonstrated significant mutual information between self-supervised tasks and perception tasks, where pretraining with the self-supervised task improves the perception tasks' performance [7; 6; 3; 22]. We propose to adapt the model to minimize the self-supervised loss at inference time, where self-supervised task is a proxy that captures a large portion of information from the perception task. In recovering the information of the self-supervised tasks, we could recover the information of perception tasks that was corrupted due to distribution shifts.

### Test-time Adaptation for Vision Models

Inference time adaptation allows the model to adapt to the unique characteristics of the new distribution online. However, the key challenge is that the SSL objective we use to adapt the model is a proxy, where there is often a trivial solution that reduces the loss but adapts the model in the wrong way. Several established methods exist to adapt the vision models, including foundation models.

**Finetuning (FT)** is a standard way to adapt the deep model. It often optimizes all the parameters of the deep models or partially, which is often heavy and requires large space to save the copy of the model to re-initialize and update. Prior work shows that this method is effective when it is finetuned on supervised tasks, yet adapting with the self-supervised task on a few examples remains under-explored. We discuss the finetuning method using the self-supervised task.

**Partial Finetuning (PFT)** is another way to adapt the model at inference time by only changing the statistics of the batch-normalization layer. This method assumes that the distribution drifts in the mean and standard deviation of the test data and can be recovered through test-time adaptation. The closest existing works are BN , Tent  and MEMO . Tent updates the BN statistics but needs to continue training on the same distribution. MEMO only requires a single test data point, yet the algorithm is slow due to the whole model update and heavy augmentations. Here, we adapt batch normalization through our proposed contrastive learning-based self-supervised loss.

**Visual Prompts (VP)** have emerged as a lightweight way to adapt pre-trained models. There are two major ways to apply visual prompts to the vision model. Let the image be \(\), it adds a vector \(\) to the input image: \(=+\). For low-rank visual prompts [24; 67], we use a low-rank matrix in \(v\) during optimization. Most visual prompts are studied in training setup, yet they are under-explored at inference time. [40; 45] optimize an additive visual prompt on the image to repair the adversarial perturbation and improve the model robustness.

### Adapting via Convolutional Visual Prompts

Adding the right structure is an effective way to avoid trivial solution to the self-supervised objective. We now introduce the convolution visual prompts (CVP), which instruct the deep model to adapt to the test distribution through convolution. Convolution has been proved to be a successful inductive bias for visual tasks , which is more sample efficient . Our assumption is that a large family of distribution shifts in the image data are visually structured, which can be modeled by convolution. Our prompt is simple and is defined as:

\[=+(,)\] (2)

One major advantage of the convolution prompt is that the number of parameters in the prompt is significantly lower (1%) than other conventional prompts (e.g., patch prompt and padding prompt). Compared to adapting the whole model weights, our method is light weight and is fast by exploiting the structure of visual distribution shifts In Appendix 7.2, we show the detailed algorithm of CVP. Our lightweight adjustment allows the model to quickly update to the novel data points without too much computation and memory overhead. As vision models are continuously deployed in edge devices, this is extremely important, given the limited computational resources.

## 4 Experiment

This section demonstrates the detail of experiment settings and evaluates the performance of our method CVP, compared with conventional Visual Prompts (VP) and existing test-time approaches.

More analysis are shown in Section 5 and Appendix. We do a comprehensive study on the CVP, including different prompts design, kernel v.s. structure analysis, multiple SSL tasks, sensitivity analysis on the batch size and adapt iterations, GradCam visualization, and optimization cost of CVP.

### Experiment Setting

**Dataset.** We evaluate our method on five kinds of OOD datasets, including CIFAR-10-C , ImageNet-C , ImageNet-R , ImageNet-Sketch , and ImageNet-A . The following describes the details of all datasets.

\(\)**Synthetic OOD Data.** The corruption data are synthesized with different types of transformations (e.g., snow, brightness, contrast) to simulate real-world corruption. The dataset contains CIFAR-10-C and ImageNet-C. Both of them are the corrupted versions of their original dataset, including 15 corruption types and 5 severity levels. A larger severity level means more corruption is added to the data. To well evaluate our method, We generate the corruption samples with five severities based on the official GitHub code* for each 15 corruption types.

Footnote *: We generate all types of corruption data based on the GitHub code: https://github.com/bethgelab/imagecorruptions

\(\)**Natural OOD Data.** The ImageNet-Rendition  contains 30000 images collected from Flickr with specific types of ImageNet's 200 object classes. ImageNet-Sketch  data set consists of 50000 sketch images. The ImageNet-Adversarial  is a natural adversarial shifting dataset contains 7500 images collected from the natural world.

Model.The backbone model architecture is pretrained on WideResNet18  and ResNet26  for CIFAR-10-C, ResNet50  for ImageNet-C, Rendition, and Sketch. We extract the logit features before the fully connected layer of the backbone model for training the SSL model. The SSL model is a simple MLP with the final layer outputting the one-dimensional features for the contrastive learning task. We further extend our prompt method to the foundation model CLIP , where we only prompt the vision encoder.

Baseline DetailsWe compare several test-time adaptation benchmarks with CVP.

\(\)**Standard**: The baseline uses the pre-trained model without adaptation. For CIFAR-10-C, the standard is trained with 50000 clean CIFAR-10 train dataset on WideResNet18 and ResNet. For ImageNet1K-C, the standard is trained with \(\)1.2M clean ImageNet train dataset on ResNet50.

\(\)**Finetune (FT)**: We adjust the whole model weight for every upcoming batch during the inference time with the self-supervised loss. In our experiments, after one-batch fine-tuning, the model will be restored to the initial weight status and receive a new type of corrupted samples.

\(\)**Partial Finetune (PFT)**: The partial fine-tune adapts batches of the samples to the model by only adjusting the batch normalization layers with self-supervised loss at every inference time. Same as Finetune baseline, the model will be restored to the initial weight status after the one-batch adaptation.

\(\)**SVP **: The prompting method to reverse the adversarial attacks by modifying adversarial samples with \(_{p}\)-norm perturbations, where the perturbations are also optimized via contrastive loss. We extend this method with two different prompt settings: patch and padding. For the patch setup, we directly add a full-size patch of perturbation into the input. For the padding setup, we embed a frame of the perturbation outside the input. More baseline detailed are shown in the Appendix 7.3

Design of Convolutional Visual Prompts (CVP)We prompt the input samples by adding the convolutional kernels. Our kernels can be optimized under several different settings, including 1.) fixed or random kernel initialization 2.) 3*3 or 5*5 kernel sizes. We show a detailed evaluation of all kernel setups in the experimental results. For initialization, we can either random initialize the kernel size \(k\) in a uniform distribution or initialize with fixed values. For fixed initialization, we empirically found that starting from a sharpness kernel is effective. We optimize the kernel with 1 to 5 iterations of projected gradient descent. To preserve the original structure, we combine the residual of input and convolved output with learnable parameters \(\). We jointly optimize the convolutional kernel \(k\) and \(\) with the self-supervised loss \(_{s}\). The range of \(\) is predefined and empirically set up in a fixed 

[MISSING_PAGE_FAIL:6]

are all set as 5. In Table 4, we show the results on five benchmarks. For CIFAR-10-C, CVP improves 1.83 points on top of TENT and reduces the error rate by 21.55% compared with the standard one. For the other datasets, CVP achieves the lowest error rate on top of the TENT method. However, the BN method degrades the performance under the small batch setting. Due to the page limit, in Appendix 7.5, we show more evaluations on other benchmark, such as the Cutout-and-Paste data  and other baseline Test Time Training (TTT) .

## 5 Ablation Study

Low-Rank Structure to Avoid Shortcut in SSL Objective.Since SSL objective is a proxy for our visual recognition task, sole minimizing SSL objective can produce a overfitted prompt without much improvement in visual recognition. A typical way to avoid this suboptimal solution is by adding the right inductive bias. For the highly structured visual data, we study the best inductive bias we can add to prevent the shortcut during adaptation.

In addition to convolution, we also investigated the popular low-rank structure [24; 67]. We create visual prompts with low-rank structure (LVP) via singular value decomposition (SVD), and optimize the low-rank matrices using our algorithm (shown in Appendix 7.2). We compare the effectiveness of low-dimensional prompts and convolution on their ability to reverse natural corruptions. In Table 6 and Figure 3, for both LVP and CVP, increasing the prompt's rank leads to degraded performance in reverse the corruption to produce clean images, where the \(_{2}\) norm distance between real shifting and the approximated shifting increase.

   &  CIFAR-10-C \\ Avg. Error (\%) \\  &  ImageNet-C \\ mCE \(\) \\  &  ImageNet-R \\ Error (\%) \\  & 
 ImageNet-S \\ Error (\%) \\  \\  Standard & 58.24 & 76.87 & 63.83 & 75.90 & 100.0 \\ Mao et al.  & 57.94 (-0.3) & 76.74 (-0.13) & 68.86 (+5.03) & 75.93 (+0.03) & 99.94 (-0.06) \\ CVP (ours) & 52.37 (-5.87) & 75.43 (-1.49) & 63.06 (-0.77) & 75.30 (-0.6) & 98.4 (-1.6) \\  MEMO  & 56.14 & 73.45 & 60.73 & 73.43 & 99.1 \\ MEMO + CVP & 54.84 (-1.3) & 72.02 (-1.43) & 60.23 (-0.5) & 72.67 (-0.76) & 98.64 (-0.46) \\  BN  & 38.51 & 76.20 & 67.29 & 77.98 & 99.8 \\ BN + CVP & 37.39 (-1.12) & 76.16 (-0.04) & 67.21 (-0.08) & 77.92 (-0.06) & 98.67 (-1.13) \\  TENT  & 38.52 & 70.45 & 58.45 & 73.88 & 99.7 \\ TENT + CVP & **36.69 (-1.83)** & **70.34 (-0.11)** & **58.42 (-0.03)** & **73.83 (-0.05)** & **98.54 (-1.16)** \\  

Table 4: Our prompt method complements other test-time adaptation approaches that update model weight, including MEMO, TENT, and BN. We show complements gain on every baseline when combined with CVP. Here, the Standard for CIFAR-10-C is WideResNet18 and other dataset is ResNet50. For CIFAR-10-C, on top of the TENT  method, we achieve the best gain on the performance, which reduces 1.83% error rate.

   &  CIFAR-10-C \\ Avg. Error (\%) \\  &  ImageNet-C \\ mCE \(\) \\  &  ImageNet-R \\ Error (\%) \\  & 
 ImageNet-S \\ Error (\%) \\  \\ 
**CLIP(ViT/32)** & 58.39 & 77.93 & 32.09 & 60.55 \\ SVP (patch) & 58.43 (+0.04) & 77.81 (-0.12) & 32.12 (+0.03) & 60.53 (-0.02) \\ CVP-F3 & 57.94 (-0.45) & 77.43 (-0.50) & 31.16 (-0.93) & 59.47 (-1.08) \\ CVP-R3 & 57.91 (-0.48) & 77.71 (-0.22) & 31.31 (-0.78) & 59.62 (-0.93) \\ CVP-F5 & 57.98(-0.41) & 77.25 (-0.68) & 31.14 (-0.95) & 59.83 (-0.72) \\ CVP-R5 & **57.79 (-0.60)** & **76.67 (-1.26)** & **30.43 (-1.66)** & **59.43 (-1.12)** \\  

Table 3: Evaluation on the CLIP model. We compare CVP with other prompting baselines on CIFAR-10-C, ImageNet-C, ImageNet-R, and ImageNet-S. Overall, the CVP-R5\({}^{}\) achieves the best performance, which reduces the error rate by 1.16% on average.

   &  CIFAR-10-C \\ Avg. Error (\%) \\  &  ImageNet-C \\ mCE \(\) \\  &  ImageNet-R \\ Error (\%) \\  & 
 ImageNet-S \\ Error (\%) \\  \\  Standard & 58.24 & 76.87 & 63.83 & 75.90 & 100.0 \\ Mao et al.  & 57.94 (-0.3) & 76.74 (-0.13) & 68.86 (+5.03) & 75.93 (+0.03) & 99.94 (-0.06) \\ CVP (ours) & 52.37 (-5.87) & 75.43 (-1.49) & 63.06 (-0.77) & 75.30 (-0.6) & 98.4 (-1.6) \\  MEMO  & 56.14 & 73.45 & 60.73 & 73.43 & 99.1 \\ MEMO + CVP & 54.84 (-1.3) & 72.02 (-1.43) & 60.23 (-0.5) & 72.67 (-0.76) & 98.64 (-0.46) \\  BN  & 38.51 & 76.20 & 67.29 & 77.98 & 99.8 \\ BN + CVP & 37.39 (-1.12) & 76.16 (-0.04) & 67.21 (-0.08) & 77.92 (-0.06) & 98.67 (-1.13) \\  TENT  & 38.52 & 70.45 & 58.45 & 73.88 & 99.7 \\ TENT + CVP & **36.69 (-1.83)** & **70.34 (-0.11)** & **58.42 (-0.03)** & **73.83 (-0.05)** & **98.54 (-1.16)** \\  

Table 4: Our prompt method complements other test-time adaptation approaches that update model weight, including MEMO, TENT, and BN. We show complements gain on every baseline when combined with CVP. Here, the Standard for CIFAR-10-C is WideResNet18 and other dataset is ResNet50. For CIFAR-10-C, on top of the TENT  method, we achieve the best gain on the performance, which reduces 1.83% error rate.

[MISSING_PAGE_FAIL:8]

small batch setting. In Figure 3(b), we show the performance on different numbers of iterations for adaption. We empirically found that when increasing the iterations, CVP has a lower risk to overfit on the self-supervised objective. More ablation studies, such as different prompt designs can be found in Appendix 7.7

**Visualization of Saliency Map** To better understand how CVP adapts to the corrupted inputs, we visualize the saliency map of different types of corruption. As Figure 5 shows, from left to right, the first row is the original, corrupted, and adapted samples; the second row shows their corresponding Grad-CAM with respect to the predicted labels. The red region in Grad-CAM highlights where the model focuses on target input. We empirically discover the heap map defocuses on the target object for corrupted samples. After CVP, the red region of the adapted sample's heap map is re-targeted on a similar part as the original image, demonstrating that the self-supervised visual prompts indeed improve the input adaptation and make the model refocus back on the correct areas. We provide more visualization in Appendix 7.10.

**Training Cost v.s. Different Kernel Size** In Table 8, we evaluate different kernel sizes for CVP and empirically discover that increasing the kernel to a proper size can improve the performance slightly. We choose one corruption-type impulse noise and show the results. When increasing the kernel size, the optimization cost increases. For impulse noise, kernel size 7*7 achieves the best robust accuracy, yet the optimization cost is much higher.

**Training Time v.s. Number of Adapt Iteration** In Figure 3(b), we have shown the CVP trained under different adapt iterations v.s. their performance. When increasing the number of adapt iterations, the training time increases. The following Table 9 shows the result of CIFAR-10-C on gaussian noise type with severity 1. We compare the accuracy and per batch training time on several numbers of

   & Standard & LVP\_R3 \\ CIFAR-10-C & 58.24 & 54.86 (-3.38) \\ ImageNet-C & 76.87 & 76.42 (-0.45) \\ ImageNet-R & 63.83 & 63.57 (-0.26) \\ ImageNet-S & 75.90 & 75.69 (-0.21) \\   
   & **S1** & **S2** & **S3** & **S4** & **S5** \\ Standard & 58.82 & 48.25 & 38.65 & 27.15 & 17.57 \\ Contrast & 59.53 & 48.84 & 39.82 & 28.55 & 19.53 \\ Rotation & 58.97 & 48.30 & 38.88 & 27.50 & 17.99 \\ MAE & 60.94 & 51.10 & 41.79 & 30.11 & 19.92 \\  

Table 6: Performance of low-rank visual Table 7: Performance of CVP on three SSL tasks for prompt. We show the average error rate ImageNet-C, including contrastive learning, rotation on four benchmarks. For CIFAR-10-C, we prediction, and MAE. We compare them with the use WideResnet as the standard model. For Standard ResNet50 and show the averaged accuracy ImageNet-C, R, and S, we use the ResNet50.

Figure 5: Visualization. From left to right we show three kinds of corruption types, including contrast, fog, and frost, on ImageNet examples. By applying our convolutional prompt on the corrupted images, our method can partially remove the corruptions and make the image easier to recognize. In addition, the saliency map calculated from Grad-Cam also shows that our approach instructs the model to look at a similar region as the original one. This highlights why our convolutional can adapt the input for robustness.

adapt iters (from 0 to 20). We empirically found that CVP has a larger performance gain than VP while adapting with a few epochs (epoch number 1).

**Does CVP Reverse Corrupted Images Back to Normal One?** We do the quantitative measurement on the distribution distance via Sliced Wasserstein Distance (SWD) and structural similarity index measure (SSIM). We measure the distance between two input distributions: source domain distribution and target domain distribution (before/after CVP adaptation). To calculate the distance between two input distributions via the Sliced Wasserstein Distance, we first obtain a group of marginal distributions from a high dimensional probability distribution via the linear projection, then calculate the \(p\)-Wasserstein Distance for those marginal distributions. Table 10 and Figure 6 shows the result of SWD on CIFAR-10-C with severity 1. On average, CVP achieves lower SWD after adaptation, which means the target distribution is closer to the source one after adaptation. The average SWD reduce by 0.7% after prompting. In Appendix Table 18 and Figure 9, we show the more detailed analysis.

## 6 Conclusion

Self-supervised convolutional visual prompt (CVP) is a novel method for test-time adaptation of OOD samples. In contrast to prior works training the visual prompts with the label, CVP is label-free and lightweight. It reduces the trainable parameters to less than 1% parameters of previous visual prompts and avoids the risk of overfitting when adapting for self-supervised objectives at test time. Results on five state-of-the-art benchmarks show that CVP improves model robustness by 5.87% and complements existing weight-adaptation methods. Extensive ablation studies suggest that distribution shifts are actually structured; therefore, CVP can capture the structures better than VP during the adaptation, which provides new insight into the frontier of visual prompting techniques and test-time adaptation. Future work includes interpreting convolutional prompts and prompting with multi-modality in large-scale foundation models.