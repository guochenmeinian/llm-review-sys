# CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation

Zhihong Chen1*, Maya Varma1*, Jean-Benoit Delbrouck1*, Magdalini Paschali1

Louis Blankemeier1, Dave Van Veen1, Jeya Maria Jose Valanarasu1, Alaa Youssef1

Joseph Paul Cohen1, Eduardo Pontes Reis1, Emily B. Tsai1, Andrew Johnston1

Cameron Olsen1, Tanishq Mathew Abraham2, Sergios Gatidis1

###### Abstract

Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation. In this work, we present (i) _CheXinstruct_ - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets; (ii) _CheXagent_ - an instruction-tuned FM capable of analyzing and summarizing CXRs; and (iii) _CheXbench_ - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks by up to 97.5%.1

1Stanford University 2Stability AI

{zhihongc,mvarma2,jbdel,paschali,akshaysc,langlotz}@stanford.edu

## Introduction

Foundation models (FMs) have recently emerged as a powerful class of models capable of performing a diverse range of reasoning and comprehension tasks . In this work, we present the following three components, also summarized in Fig. 1, to help create capable and robust FMs for chest X-ray (CXR) interpretation:

1. [leftmargin=*]
2. _CheXinstruct_ is an instruction-tuning dataset with 6M instruction-image-answer triplets designed to improve the ability of FMs to interpret CXRs. We collect instructions from 34 tasks and 65 unique datasets, spanning categories including coarse- and fine-grained image understanding, question answering, and text generation.
3. _CheXagent_ is an instruction-tuned foundation model with 8B parameters capable of analyzing images, understanding text, and generating responses. Our methodology for developing CheXagent includes training (1) a clinical LLM capable of understanding radiology reports, (2) a vision encoder capable of reading CXRs, and (3) a network to bridge the vision and language modalities. We then perform instruction-tuning using data from CheXinstruct.
4. _CheXbench_ is a novel benchmark designed to rigorously evaluate FMs across two evaluation axes: image perception and textual understanding. We introduce 8 tasks across 7 CXR datasets, and we evaluate performance using close-ended multiple-choice predictions as well as open-ended text generation.

We use CheXbench to compare CheXagent with six previous FMs from both general and medical domains. We further provide an evaluation of potential model bias and highlight performance disparities across demographic factors of sex, race and age to improve model transparency.

## Data: CheXinstruct

CheXinstruct seeks to cover a broad range of tasks to support training CXR FMs. These tasks can either (i) improve the abilities of FMs to understand CXRs or (ii) improve clinical decision making. This dataset is comprised of five categories of tasks, each categorized by their specific capabilities:

* Coarse-grained Image Understanding, which defines the overall understanding of CXRs, e.g., view classification , and disease classification .
* Fine-grained Image Understanding, which defines the localized understanding of CXRs, e.g., abnormality detection ,

Figure 1: Overview of the proposed pipeline: CheXinstruct is a curation of datasets for instruction-tuning across various CXR tasks, CheXagent is our clinical FM for CXR interpretation, and CheXbench is our comprehensive FM evaluation benchmark. Two example CXR interpretation tasks include local findings generation and open-ended visual question answering (VQA).

abnormality grounding , and foreign object detection .
* _Question Answering_, which defines the ability to respond to a question, e.g., close-ended and open-ended visual question answering (VQA) , findings summarization , and local findings generation .
* _Miscellaneous_: This category defines the miscellaneous abilities that are critical for a CXR FM, e.g., report evaluation , and natural language explanation .

## Model: CheXagent

The aim of CheXagent is a model that can "see" images \(x_{I}\) and/or "read" text \(x_{T}\) and generate "responses" \(y\). Our training process for CheXagent involves the following stages:

**Stage 0: Train a clinical LLM**: Our starting point is Mistral-7B-v0.1  due to its proven robust reasoning abilities in diverse benchmarks. To infuse the model with comprehensive medical and clinical knowledge, we utilize five distinct text sources for training: (i) PMC articles, (ii) MIMIC-IV, and (iii) Wikipedia. **Stage 1: Train a vision encoder for CXR**: Our model architecture reflects that of . For training purposes, we utilize datasets comprising image-text pairs, specifically from MIMIC-CXR, PadChest, and BIMCV-COVID-19. **Stage 2: Train a vision-language bridge**: Following the training of the clinical LLM and the CXR vision encoder, we focus on developing a bridge model, \(_{b}\). This model is designed to map visual data to the corresponding language (semantic) space. For training \(_{b}\), we employ the same datasets as in Stage 1. **Stage 3: Instruction tuning**: Upon completing Stage 2, The model is trained on CheXinstruct, taking into account two primary factors: (i) reserving certain task-dataset pairs exclusively for evaluation purposes, and (ii) determining optimal dataset ratios to ensure balanced training across different capabilities.

## Evaluation: CheXbench

CheXbench is structured with two evaluation axes, crafted to assess crucial aspects of CXR interpretation: image perception and textual understanding. For the former, we introduce 6 tasks across 7 datasets: View Classification, Binary Disease Classification, Single Disease Identification, Multi-Disease Identification, Visual-Question-Answering, and Image-Text Reasoning; For the latter, we introduce 2 tasks: Findings Section Generation and Findings Summarization.

In our study, we employ CheXbench to compare CheXagent against two general-domain instruction-tuned FMs, InstructBLIP and BLIP2, which achieve state-of-the-art performance in previous research . Additionally, we compare CheXagent with four medical FMs: XrayGPT, MedFlamingo, RadFM, and LLaVA-Med . This comparison aims to provide a comprehensive understanding of CheXagent's performance in relation to both general and medical-specific models.

Table 1 provides results on CheXbench. For image perception tasks, CheXagent demonstrates superior performance across image perception tasks, achieving an average improvement of 97.5% over general-domain FMs and an average improvement of 55.7% over medical FMs; For text understanding tasks, CheXagent outperforms all medical FMs on CheXpert on findings section generation and also achieve promising performance on findings summarization.

## Conclusion

In this work, we design a complete scheme for training CXR FMs by introducing CheXinstruct, CheXagent, and CheXbench. Experimental results demonstrate the effectiveness of this scheme.

    &  &  &  &  \\  & & **BLIP-2** & **InstructBLIP** & **XrayGPT** & **MedFlamingo** & **RadFM** & **LLaVA-Med** & **(Ours)** \\   & MIMIC-CXR & 28.8 & 25.3 & 24.0 & 25.0 & 28.5 & 23.3 & 97.5 \\  & CheXpert & 38.0 & 34.0 & 33.0 & 39.0 & 37.0 & 30.0 & 96.7 \\   & SIM & 53.0 & 54.0 & 50.0 & 50.0 & 50.0 & 49.0 & **64.0** \\  & RSNA & 50.0 & 60.0 & 50.0 & 50.0 & 50.0 & 44.0 & **81.0** \\  & CheXpert & 51.5 & 53.2 & 51.5 & 48.5 & 55.8 & 47.6 & **76.0** \\   & OpenI & 40.2 & 40.2 & 45.4 & 39.0 & 42.2 & 43.8 & 47.0 \\  & MIMIC-CXR & 26.6 & 22.6 & 24.1 & 25.6 & 27.2 & 26.7 & 30.3 \\  & CheXpert & 21.3 & 19.5 & 23.7 & 26.0 & 26.6 & 26.0 & 29.6 \\   & OpenI & 48.5 & 54.4 & **59.2** & 46.1 & 52.8 & 53.9 & 55.6 \\  & MIMIC-CXR & 30.0 & 25.3 & 39.0 & 14.7 & 23.3 & 28.7 & **55.3** \\  & CheXpert & 4.3 & 6.1 & 3.9 & 7.1 & 23.6 & 2.1 & **52.1** \\   & Rad-Restrant & 41.2 & 42.4 & 38.6 & 45.5 & 48.5 & 34.9 & **57.1** \\  & SL-ACE & 74.3 & 86.4 & 52.4 & 64.8 & 58.0 & 55.5 & 78.1 \\   & OpenI & 47.9 & 52.6 & 52.4 & 54.7 & 54.0 & 45.8 & **59.0** \\   & Findings Section Generation & CheXpert & - & - & 9.0 & 1.7 & 5.1 & 4.2 & **15.6** \\   & MIMIC-CXR & - & - & - & - & - & - & **40.3** \\   

Table 1: Comparison between CheXagent and general domain and medical domain FMs on CheXbench. For image perception tasks, we report accuracy; For Findings Generation and Findings Summarization, we report RadGraph Score and Rouge-L, respectively.