# Fairness without Demographics on Electronic Health Records

Yingtao Luo\({}^{1}\), Zhixun Li\({}^{2}\), Qiang Liu\({}^{3,4}\), Jun Zhu\({}^{5}\)

###### Abstract

Machine learning systems are notoriously prone to biased predictions about certain demographic groups, leading to algorithmic fairness issues. Due to concerns about patient privacy and social inequity, some demographic information may not be available for training a clinical algorithm. Moreover, the complex interaction of different demographics can lead to a lot of unknown minority subpopulations. These challenges greatly limit the applicability of existing group fairness algorithms. To improve the fairness-without-demographics algorithm in the clinical regime, we argue that the gradients of clinical models can provide insights for alleviating inequities. In this paper, we adopt an adversarial weighting architecture and leverage the correlation between model gradients and demographic groups to improve identification and increase exposure of underrepresented groups. We learn the weights of different samples by constructing a graph where samples with similar gradients are connected. Unlike the surrogate grouping methods that cluster groups by proxy sensitive attributes like features and labels, which can be inaccurate, our method provides a soft grouping mechanism that is more robust. The results show that our method can significantly improve fairness without sacrificing too much of the overall accuracy.

\({}^{1}\)Carnegie Mellon University, \({}^{2}\)The Chinese University of Hong Kong

\({}^{3}\)CRIPAC, MAIS, Institute of Automation, Chinese Academy of Sciences

\({}^{4}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{5}\)Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University

yingtaol@andrew.cmu.edu, zxli@se.cuhk.edu.hk, qiang.liu@nlpr.ia.ac.cn, dcszj@tsinghua.edu.cn

## Introduction

Fairness in machine learning has become an urgent concern, as machine learning systems can be biased against certain demographic groups, which contributes to socioeconomic disparities in many areas such as healthcare Gianfrancesco et al. (2018), finance Hajian et al. (2016), etc. For example, when learning the risk of patients with different races, due to certain biases, the model prediction can be inaccurate for certain protected groups, such as minorities. To address this issue, most existing methods Hashimoto et al. (2018); Sagawa et al. (2019); Lahoti et al. (2020); Creager et al. (2021); Rahman and Purushotham (2022); Chai et al. (2022) require sensitive attributes, such as race, gender, etc., to identify which group is discriminated against by machine learning models. However, due to privacy concerns, these sensitive attributes are not always accessible. For example, regulations like the HIPAA privacy rule have established safeguards to protect the privacy of health information. In addition, the interaction between various demographic factors can be complex, and the potential protected groups increase exponentially as the number of sensitive attributes increases. This, in turn, escalates the difficulty of identifying the most disadvantaged group. Consequently, it is crucial to advocate for machine learning fairness that does not rely on demographic information.

To ensure fairness without demographics, many existing methods with proxy sensitive attributes Yan et al. (2020); Griar et al. (2021); Du et al. (2021); Zhao et al. (2022) assume the correlation between sensitive attributes (groups) and nonsensitive attributes (features), perform clustering to obtain surrogate groups, and enforce group fairness. The problem with these methods, however, is the difficulty in guaranteeing a large overlap with the real protected groups, especially when many protected groups are unknown and the distributional discrepancies between sensitive attributes are large Chai et al. (2022). Other methods such as ARL Lahoti et al. (2020) generate weights for different samples, but they can be susceptible to noise (e.g., mislabelling) when outliers are given superior weights due to their rarity in the data and thus may lead to severe degradation of fairness metrics.

In this paper, we develop an innovative adversarial learning framework that comprises a main-task learner and an adversary component tasked with generating sample weights to maximize the learner network's loss. These weights are subsequently utilized in the minimization of the learner's loss. Our approach harnesses the gradients of the learner to categorize samples based on demographics, forming a "Graph of Gradients" (GoG). In this graph, each sample is linked to its K-nearest counterparts exhibiting similar gradient profiles. This method enables the computation of each sample's weight through the aggregation of weights from neighboring samples in the GoG. This soft-grouping mechanism effectively identifies similar samples, while avoiding the imposition of rigid demarcation lines between different demographic groups and preventing the undude influence of noises. Through comprehensive experimental evaluations, we demonstrate that our methodology not only markedly enhances fairness, but also optimizes the balance between fairness and accuracy.

In summary, our contributions are listed as follows.

* We propose a fairness-without-demographics algorithm for clinical models to mitigate the machine learning unfairness issue, which can scale up to large datasets of diverse complexities and structures.
* We show that the gradients of a machine learning clinical model are more effective in representing demographic groups under mild assumption that model accuracy and input features are strongly correlated, which holds true if the deep learning learner is better than random guess. We also show that the last-layer gradients are sufficient.
* We propose to identify demographic subgroups by a soft-grouping method, i.e., graph of gradients (GoG). The proposed method can address the issues with surrogate groups and noisy outliers.
* Extensive experiments on three public datasets and five baselines show that our method outperforms other representative fairness algorithms significantly in terms of both fairness and accuracy.

## Related Work

### Group fairness for classification

Group fairness is a concept that aims to ensure that the outcomes of an algorithm are equitable across different subpopulations defined by sensitive attributes, such as race, gender, etc. To alleviate the group disparity , Equal Opportunity  hopes that the true positive rates should be the same for all subpopulations, and Predictive Equality  requires the equality of false positive rates. Preprocessing methods  ensure that the data used for training are unbiased and representative of different subgroups by resampling, feature selection, etc. In-processing methods  regularize the training process with fair constraints, sample reweighting, and adversarial training. Post-processing methods  focus on adjusting the model prediction after training by threshold adjustment, calibration, etc., which are usually very efficient. However, to guarantee group fairness, the availability of sensitive information is a necessity. Some papers  also address fairness concerns by using techniques that are robust to noisy or shifting sensitive attributes.

### Fairness without demographics

To resolve challenges to discovering the worst-off groups due to both the regulatory limitations and the complex interaction of many demographic variables , increasing methods are proposed in recent years to achieve fairness without demographics. Some methods follow the Rawlsian Max-Min fairness  to minimize the empirical risk of the group with the least utility. For example, Distributionally Robust Optimization (DRO)  proposes to use \(^{2}\)-divergence to discover and minimize the worst-case distribution repeatedly, which essentially only focuses on the learning of the worst-off group. Adversarial Reweighted Learning (ARL)  uses an adversary network to generate sample weights that maximize the empirical risk and performs weighted learning for the learner model. Based on the concept of computational identifiability, ARL hypothesizes that it can learn demographic information from data features and labels. Surrogate grouping methods  are also proposed to minimize the correlation between data features and model prediction, or directly predict surrogate demographic groups  and then perform group fairness algorithms . Some debiasing methods propose to identify the group disparities based on clustering information and upsample the minority groups to balance the distribution .

## Theoretical Foundation

### Problem Formulation

Consider data \((x,y,a)\) with \(n\) samples, where \(x\) represents the non-sensitive features, \(y\) represents the labels, and \(a\) represents the sensitive attributes. Then, given \(x\), we need to predict \(y\) without the knowledge of \(a\) while satisfying certain fairness criteria with respect to \(a\). For multi-class classification, \(y\{M\}\) where \(M\) denotes the number of classes.

### Correlation between gradients and demographics

Since we do not have true demographics \(a\) as the label, we do not know \(a\) as an estimated function of \(x\) and \(U(h)\) a priori. Therefore, we propose to use gradients to represent demographic groups. Gradients provide not only the data bias but also the model bias. As long as the model prediction error differs in different groups, \(U(h)\) and \(a\) must be correlated.

Consider a neural network model \(h\) parametrized by \(\) as \(h(x;)=\), where \(=(W,V)\). We have \(W=(W_{1},...,W_{d})^{}^{D M}\) as the weight of the last layer, where \(D\) denotes the dimensionality of the last latent representation. \(V\) is the weight of all the previous layers, \(h(x;)=(W z(x;V))\), where \((z)_{j}=e^{z_{j}}/_{d=1}^{D}e^{z_{d}}\). The last-layer gradient w.r.t. the cross entropy loss is calculated as

\[L(h(x;),y)=z(x;V)(-y), \]

where

\[L(h(x;),y) =-_{d}y_{d}(h(x;)) \] \[=(_{d=1}^{D}e^{W_{d} z(x;V)})-W_{y} z (x;V). \]

Note that \(-y\) is the bias of the model prediction, which can have a positive/negative value. We define the undirected gradient \(g^{D M}\) of the last layer of \(h\) by

\[g_{d,j}=z(x)_{d}|_{j}-y_{j}|=z_{d}U_{j}, \]

which is the latent representation multiplication of non-sensitive feature and prediction error. Here, \(y_{j}\) denotes the 

[MISSING_PAGE_FAIL:3]

### Experimental Setup

We randomly divide each dataset by samples into the training, validation, and testing sets in a 0.75:0.1:0.15 ratio. Different from previous works such as  only consider race and gender, we consider more subpopulation groups to test the fairness of models under a more severe environment. We tune all the hyperparameters with an appropriate range to obtain the optimal evaluation on the validation set for each model. The range of learning rate is {1e-2, 3e-3, 1e-3}, batch size is {16, 32, 64, 128}, hidden dimension is {16, 32, 64}, dropout rate is {0,1, 0.5}. The algorithm will stop if the accuracy of the worst group validation metrics does not increase in twenty epochs, and the test performance will be recorded. All results are averaged under five random seeds. More experimental details are presented in Appendix.

### Datasets

We evaluate our proposed method on the following real-world datasets: MIMIC-III, MIMIC-IV.

* **MIMIC-III Dataset** The Medical Information Mart for Intensive Care database1 contains the information of patients who stayed in the critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. There are 53423 patients and 651048 diagnosis codes recorded in the dataset. The goal is to predict future diagnoses for multi-class classification. Religion, sex, age, and race are protected attributes. * **MIMIC-IV Dataset** The MIMIC-IV dataset2 contains patients who stayed in the critical care units of the Beth Israel Deaconess Medical Center between 2008 and 2019. Patients who had less than three admission records are excluded. The average number of visits for the 10023 selected patients is 4.64, the average number of codes in a visit is 14.12, the total number of unique ICD-9 codes in diagnoses/procedures is 6274/1973.

### Performance Comparison

Overall, Table 1 show that our algorithm can significantly improve the fairness of machine learning diagnosis prediction models significantly. The relative increases of WorstGroup Accuracy to the second-best approach by our algorithm are 2.904% on MIMIC-III dataset and 3.769% on MIMIC-IV dataset. In Equalized Odds, the relative improvements by our algorithm are 27.36% on MIMIC-III dataset and 26.93% on MIMIC-IV dataset. In disparate impact, the relative improvements by our algorithm are 25.90% on MIMIC-III dataset and 28.22% on MIMIC-IV dataset. We observe that FairRF generally performs worse than DRO and ARL in Worst-Group Accuracy, but outperforms them in Equalized Odds and Disparate Impact. Compare to our model, DRO and ARL are much less effective in promoting fairness without demographics. It should be noted that group disparity focuses on minimizing the discrepancies between groups when making predictions, while the worst group accuracy focuses on ensuring that the model is accurate even for the worst-off group.

## Conclusion

In this study, we address fairness concerns in machine learning models for electronic health records, focusing on the challenges posed by the complex interplay of demographic variables and regulatory constraints, which often render demographic information unknown. We present a novel approach to tackle the limitations of existing methods. Specifically, we highlight the importance of gradients to identify subpopulations, and propose to create a graph of gradients by connecting each sample to its K-nearest neighbors. Graph neural networks are adopted to identify demographic groups and generate sample weights. Experimental results reveal that our method significantly enhances the machine learning fairness on electronic health records data.

    &  \\  & W. Acc(\(\)) & E. Odds(\(\)) & D. Impact(\(\)) \\  RETAIN & 0.2102\(\) 0.014 & 29.28\(\) 0.45\% & 23.89\(\) 0.59\% \\ +DRO & 0.2188\(\) 0.034 & 27.75\(\) 0.91\% & 23.02\(\) 0.75\% \\ +ARL & 0.2187\(\) 0.030 & 27.14\(\) 0.90\% & 22.90\(\) 0.77\% \\ +FairRF & 0.2169\(\) 0.024 & 26.05\(\) 0.70\% & 22.76\(\) 0.66\% \\ +Ours & **0.2226**\(\) 0.034 & **18.99**\(\) 0.72\% & **17.35**\(\) 0.61\% \\  Dipole & 0.1943\(\) 0.032 & 30.09\(\) 0.66\% & 24.03\(\) 0.45\% \\ +DRO & 0.1979\(\) 0.040 & 28.10\(\) 0.87\% & 23.02\(\) 0.72\% \\ +ARL & 0.1988\(\) 0.049 & 27.60\(\) 0.85\% & 23.11\(\) 0.60\% \\ +FairRF & 0.1970\(\) 0.037 & 26.63\(\) 0.65\% & 22.79\(\) 0.53\% \\ +Ours & **0.2095**\(\) 0.035 & **19.32**\(\) 0.72\% & **16.84**\(\) 0.55\% \\  Stagenet & 0.2086\(\) 0.012 & 29.53\(\) 0.22\% & 23.16\(\) 0.27\% \\ +DRO & 0.2127\(\) 0.022 & 27.99\(\) 0.49\% & 22.63\(\) 0.43\% \\ +ARL & 0.2136\(\) 0.026 & 27.89\(\) 0.49\% & 22.77\(\) 0.59\% \\ +FairRF & 0.2111\(\) 0.015 & 26.36\(\) 0.19\% & 22.32\(\) 0.24\% \\ +Ours & **0.2170**\(\) 0.011 & **19.10**\(\) 0.27\% & **16.11**\(\) 0.37\% \\    &  \\  & W. Acc(\(\)) & E. Odds(\(\)) & D. Impact(\(\)) \\  RETAIN & 0.3102\(\)0.018 & 25.64\(\)0.23\% & 23.39\(\)0.33\% \\ +DRO & 0.3153\(\)0.046 & 23.33\(\)0.58\% & 21.27\(\)0.62\% \\ +ARL & 0.3147\(\)0.043 & 23.00\(\)0.51\% & 21.55\(\)0.45\% \\ +FairRF & 0.3126\(\)0.020 & 22.20\(\)0.10\% & 20.97\(\)0.28\% \\ +Ours & **0.3299**\(\)0.044 & **16.95**\(\)0.67\% & **15.32**\(\)0.44\% \\  Dipole & 0.3051\(\)0.032 & 29.11\(\)0.36\% & 24.75\(\)0.21\% \\ +DRO & 0.3077\(\)0.040 & 25.45\(\)0.38\% & 22.23\(\)0.20\% \\ +ARL & 0.3093\(\)0.049 & 25.19\(\)0.52\% & 22.11\(\)0.39\% \\ +FairRF & 0.3047\(\)0.037 & 24.30\(\)0.38\% & 21.45\(\)0.24\% \\ +Ours & **0.3212**\(\)0.035 & **17.61**\(\)0.49\% & **15.27**\(\)0.33\% \\  Stagenet & 0.3048\(\)0.032 & 28.19\(\)0.17\% & 22.95\(\)0.21\% \\ +DRO & 0.3089\(\)0.040 & 24.48\(\)0.33\% & 22.14\(\)0.39\% \\ +ARL & 0.3112\(\)0.049 & 24.22\(\)0.28\% & 22.04\(\)0.40\% \\ +FairRF & 0.3079\(\)0.037 & 22.40\(\)0.11\% & 21.37\(\)0.21\% \\ +Ours & **0.3200**\(\)0.025 & **15.77**\(\)0.14\% & **15.19**\(\)0.14\% \\   

Table 1: Performances of fairness algorithms on the MIMIC-III and MIMIC-IV datasets, evaluated by Acc@20 of the worst group, Equalized Odds, and Disparate Impact. Each result is averaged over ten random seeds.

[MISSING_PAGE_FAIL:5]

accuracy on multiple subgroups. _Advances in Neural Information Processing Systems_, 35: 34121-34135.
* Yan et al. (2020) Yan, S.; Kao, H.-t.; and Ferrara, E. 2020. Fair class balancing: Enhancing model fairness without observing sensitive attributes. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, 1715-1724.
* Zhao et al. (2022) Zhao, T.; Dai, E.; Shu, K.; and Wang, S. 2022. Towards Fair Classifiers Without Sensitive Attributes: Exploring Biases in Related Features. In _Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining_, 1433-1442.

## Appendix

### Experimental Setup Details

#### Baseline Models

* **RETAIN:** A two-level neural model based on reverse time attention for healthcare.
* **Dipole:** An attention-based bidirectional recurrent neural network for healthcare.
* **StageNet:** A deep learning model with stage-aware LSTM and convolutional modules for health risk prediction.

For each baseline model, we incorporate it with several fairness algorithms, denoted as \(+(FairModel)\).

We compare our method that uses the graph of gradients with the following fairness-without-demographics models that use features for weight generation or clustering.

* **DRO**:  A fair algorithm that uses \(^{2}\)-divergence to discover and minimize the worst-case distribution repeatedly.
* **ARL**:  A fair algorithm that leverages computational identifiability to learn the demographics from features/labels for the Max-Min fairness objective.
* **FairRF**:  A fair algorithm that minimizes the correlation between data features and model predictions with importance weighting.

### Evaluation Metrics

#### Worst-Group Accuracy

We adopt the top-\(k\) accuracy of the worst subpopulation group (W. Acc.), which indicates the model performance under the worst case. Accuracy@\(k\) is the same as used in previous works  is defined as the correct medical ICD-9 codes ranked in the top \(k\) divided by min(\(k\), \(|y_{t}|\)), where \(|y_{t}|\) is the number of ICD-9 codes in the (\(t\)+1)-th visit. Here we use \(k=20\).

#### Equalized Odds

We use the equalized odds (E. Odds), which requires the probability of instances with any two protected attributes \(i\), \(j\) being assigned to an outcome \(k\) are equal, given the label. As there are as many as \(M\) different labels, we simply cluster labels into eighteen diagnosis categories based on the ICD-9 code categories3. Since there are different demographic groups \(S\), we calculate

\[_{EO}=_{i,j}|E(|S=i,y=k)=E(|S=j,y=k)|. \]

#### Disparate Impact

We use the disparate impact (D. Impact), which requires the prediction to be fair across different groups. This metric may not make sense for medicine and should use with caution, since the prevalence of a disease can indeed be affected by the demographics

\[_{DP}=_{i,j}|E(|S=i)=E(|S=j)|. \]

### Experimental Ablation Study

We conduct an ablation study in Table A1 to fully understand the effectiveness of different parts in our method. In total, our method consists at least of 1) the construction and learning of the EHR patient graph and 2) the use of gradients to replace features in representing unknown demographics. We report the fairness metrics when we deduct a certain part from the proposed model to evaluate whether it is important. It can be concluded that both parts are useful, as the model without graph or without gradients can both outperform other fairness algorithms. The graph learning plays a more important role in improving fairness.

### Effectiveness of Gradients

In this section, we discuss and theoretically analyze the effectiveness of gradients to represent sensitive demographics. We first generally demonstrate, through the lens of information theory and as articulated in Theorem 1, that the distribution of gradients is more closely aligned with sensitive demographic attributes compared to input features, if input features are not perfect solutions for identifying demographics. Furthermore, we explore under the condition of linear relationships, as outlined in Lemma 1, that model gradients are more effective than input features in a larger correlation between input features and model prediction error.

**Theorem 1**.: _The distribution of gradients has a closer distance to sensitive attributes than input features. If we denote sensitive demographics as \(Z\), model prediction error as \(Y\), and input features as \(X\), \(I(XY|Z)>I(X|Z)\)._

Proof.: To calculate the mutual information \(I(XY|Z)\) between \(XY\) and \(Z\), where \(XY\) is the undirected gradient as shown in Eq. 4, we have

\[I(XY|Z)=H(XY)-H(XY|Z). \]

Similarly, we can calculate the mutual information \(I(X|Z)\) between \(X\) and \(Z\) as

\[I(X|Z)=H(X)-H(X|Z). \]

Subtracting the above two equations, as long as \(X\) and \(Z\) are not perfectly dependent with each other, we have

\[I(XY|Z)-I(X|Z)\] \[=(H(XY)-H(X))-(H(XY|Z)-H(X|Z))\] \[=H(Y|X)-H(Y|XZ)\] \[>0. \]

**Lemma 1**.: _If we denote sensitive demographics as \(Z\), model prediction error as \(Y\), and input features as \(X\), \(\) increases when \(Corr(X,Y)\) increases._

Proof.: Here, we assume the linearity of the data pattern for simplicity of explanation. We hypothesize that there is a correlation between \(X\) and \(Z\), and also a correlation between \(Y\) and \(Z\). For simplicity, we assume \(Z=aX+_{a}\), where \(_{a}\) is a noise term that represents the part of \(Z\) that is uncorrelated to \(X\). For simplicity, we can assume \(_{a} N(_{a},_{a}^{2})\). Similarly, we have \(Z=bY+_{b}\) where \(_{b} N(_{b},_{b}^{2})\). We regard \(a\) and \(b\) as two constants, while the two noise terms \(_{a}\) and \(_{b}\) are unknown and statistically independent. We assume that \(X\) follows the standard normal distribution after preprocessing, thus \(_{X}=0,_{X}^{2}=1\).

Note that we can rearrange the given equalities as follows: \(Y=}{b}\) and \(X=}{a} Y=}{b}=X+ -_{b}}{b}\) and \(XY=X^{2}+-_{b}}{b}X\).

The covariance between \(X\) and \(Z\) is

\[Cov(X,Z) =Cov(X,aX+_{a}) \] \[=a Cov(X,X)+Cov(X,_{a})\] (18) \[=a Var(X)+0\] (19) \[=a, \]

where \(Cov(X,_{a})=0\) when the noise term is independent of the feature.

The correlation coefficient between \(X\) and \(Z\) is

\[Corr(X,Z) =} \] \[=+_{a}^{2}}}. \]

Similarly, we can calculate

\[Cov(X,Y) =E[X(X+-_{b}}{b})]-0=, \] \[Corr(X,Y) =+_{a}^{2}+_{b}^{2}}},\] (24) \[Var(Z) =Var(aX+_{a})=a^{2}+_{a}^{2},\] (25) \[Cov(Y,Z) =Cov((1/b)(Z-_{b}),Z)=+_{a}^{2}}{b},\] (26) \[Var(Y) =Var(X)+Var(-_{b}}{b})\] (27) \[=+_{a}^{2}+_{b}^{2}}{b^{2}},\] (28) \[Corr(Y,Z) =}\] (29) \[=+_{a}^{2}}}{+_{a}^{2}+ _{b}^{2}}}. \]

From observation, we find that when \(_{a}^{2}+_{b}^{2}=0\), \(Corr(X,Y)=Corr(X,Z)=Corr(Y,Z)=1\), regardless of the value of \(a,b,_{a},_{b}\). Since we regard \(a\) as a constant that is not subject to change, we can conclude that \(_{a}^{2}\) and \(_{b}^{2}\) can directly determine the correlation.

We have the correlation between \(XY\) and \(Z\) as

\[Cov(XY,Z) =Cov(X^{2}+-_{b}}{b}X,Z) \] \[=Cov(X^{2},aX+_{a})\] \[+Cov((_{a}-_{b})X,aX+_{a})\] (32) \[=}{b}Cov(X^{2},X)+Cov((_{a}- _{b})X,_{a})\] \[+Cov((_{a}-_{b})X,aX)\] (33) \[=}{b}(E[X^{3}]-E[X]E[X^{2}])\] \[+-_{b})E(X^{2})}{b}\] (34) \[=-_{b})}{b}, \]

where, by the moments of standard normal distribution, \(E(X^{4})=3\), \(E(X^{3})=0\), and \(E(X^{2})=1\).

Then, we compute

\[Var(XY) =Var(X^{2}+-_{b}}{b}X) \] \[=}{b^{2}}Var(X^{2})+}(_ {a}-_{b})(X)\] (37) \[=}{b^{2}}Var(X^{2})\] \[+}((_{a})+( _{b})-2(_{a},_{b}))\] (38) \[=+_{a}^{2}+_{b}^{2}}{b^{2}} \]

where \(Var(X^{2})=E(X^{4})-E(X^{2})^{2}=3-1=2\).

Therefore, the correlation coefficient between \(XY\) and \(Z\) is

\[Corr(XY,Z) =} \] \[=-_{b})}{b+_{a}^{2}}+_{a}^{2}+_{b}^{2}}{b^{2}}}}, \]

Comparing \(Corr(X,Z)\) and \(Corr(XY,Z)\), we have

\[Ratio==-_{b}}{+_ {a}^{2}+_{b}^{2}}}. \]

We can tell that both \(Corr(X,Y)\) and \(Ratio\) are directly dependent on and decrease in \(_{a}^{2}+_{b}^{2}\). Therefore, \(\) increases in \(Corr(X,Y)\). 

Here, linear correlation is used as an example to illustrate our point, since nonlinear correlation is much harder to measure and analyze. In particular, without further assumptions or knowledge about the data, there could be many possible nonlinear relationships (e.g., logarithmic, polynomial, exponential) and nonlinear correlation measurements (e.g., Hilbert-Schmidt Independence Criterion, Mutual Information, Maximal Information Coefficient).

### Effectiveness of Last-Layer Gradients

Then, we extend it to Proposition 1 to show that when the model is a neural network, as a special case of Theorem 1, using the last-layer gradient is sufficient.

**Proposition 1**.: _The last-layer gradient of the deep learning prediction model can have a stronger correlation to sensitive attributes than non-sensitive input features. If we denote input features as \(x\), model prediction error as \(U\), last-layer representation as \(z\), and sensitive attribute classes as \(s\), we have \(Corr(zU,s)>Corr(x,s)\)._

Proof.: Proposition 1 simply extends Theorem 1 to the setting of a neural network. We consider a neural network \(h\) parametrized by \(\) as \(h(x;)=\), where \(=(W,V)\). Thus, we have \(W=(W_{1},...,W_{d})^{}^{D M}\) as the weight of the last layer where \(D\) denotes the dimensionality of the last latent representation. \(V\) is the weight of all previous layers. \(h(x;)=(W z(x;V))\), where \((z)_{j}=e^{z_{j}}/_{d=1}^{D}e^{z_{d}}\). The last-layer gradient w.r.t. the cross entropy loss is calculated as

\[L(h(x;),y)=z(x;V)(-y), \]

where

\[L(h(x;),y) =-_{d}y_{d}(h(x;)) \] \[=(_{d=1}^{D}e^{W_{d} z(x;V)})-W_{y} z (x;V). \]

Note that \(-y\) is the bias of the model prediction, which can have a positive/negative value. We define the undirected gradient \(g^{D M}\) of the last layer of \(h\) by

\[g_{d,j}=z(x)_{d}|_{j}-y_{j}|=z_{d}U_{j}, \]

which is the multiplication of the last-layer representation and the prediction error (alternative to model prediction error \(U\)) of the label class. Here \(y_{j}\) denotes the true value of the \(j\)-th class in the label.

We can assume that the correlation between the last-layer representation and the label is larger than the correlation between the input features and the label, i.e., \(Corr(z,s)>Corr(x,s)\). This assumption is also likely to hold in practice because the purpose of a neural network is to learn a representation to make it easier to predict the label. As long as the neural network is effectively learning representations, this assumption holds. According to Lemma 1, we have

\[>, \]

which means \(Corr(zU,s)>Corr(xU,s)\). According to Theorem 1, in general, \(I(xU|s)>I(x|s)\). If it works for linear relationships, we have \(Corr(xU,s)>Corr(x,s)\). In this case, \(Corr(zU,s)>Corr(x,s)\).