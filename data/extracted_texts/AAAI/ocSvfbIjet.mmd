# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

larly in modeling the sequential decision-making process inherent in puzzle-solving. This approach mirrors human problem-solving strategies, allowing the agent to learn and refine its approach over time. The RL framework balances exploration of new strategies with exploitation of known effective approaches, which can be beneficial for discovering generalizable solutions. Supervised generalization benchmarks such as CLRS  often rely on intermediate hints to match the ground-truth algorithm during the training process. In the RL framework, on the other hand, the agent is free to pursue any algorithm that ensures high reward. Moreover, RL's customizable reward structure enables us to encourage true understanding over mere memorization by rewarding not just correct solutions, but also the application of sound logical principles.

An inherent challenge with testing generalization is the need for the neural agent to be able to handle logic puzzles of different sizes. For many neural architectures, it is not possible to even represent instances of different sizes, or they struggle to properly adapt as the scale of the problem changes. This limitation severely constrains their ability to generalize to larger, more complex puzzles. To address this challenge, we propose to represent logic puzzles as graphs. In our graph-based modeling, puzzle elements become nodes, while the relationships and constraints between these elements are represented as edges, providing a level of abstraction as depicted in Figure 1. This approach allows us to apply Graph Neural Networks (GNNs), which can naturally adapt to various puzzle sizes while maintaining a consistent structure. As puzzle size increases, the graph simply expands without fundamentally altering the underlying representation.

In this paper, we focus on how to best model and develop techniques for neural architectures that use the proposed graph structures in order to solve logic puzzles. More precisely, we propose to tackle the problem using a multi-agent context and utilize GNNs in order to directly exploit the topological structure of the puzzles. This approach allows us to represent both the puzzle structure and the problem-solving agents within a unified framework. In addition, we evaluate whether the inductive biases of GNNs provide advantages over a more general architecture such as transformers. Then, we explore what factors impact on the ability to generalize and extrapolate. Here, we focus on two key areas: the design of reward systems and the role of recurrent modeling in sequential reasoning.

We summarize our contributions as follows:

* Introduction of a novel graph-based evaluation framework for logic puzzles with a focus on scaling to varying problem sizes, specifically designed to test extrapolation beyond the training distribution.
* Proposal of a multi-agent reinforcement learning approach using Graph Neural Networks, which models puzzle-solving as a collaborative task and enables learning of generalizable strategies across different puzzle sizes and complexities.
* Insights into the factors that influence generalization in logical reasoning tasks. This includes the inductive bias of the architecture, the role of reward design, and sequential decision-making mechanisms.
* Demonstration of our approach's effectiveness through extensive experiments on a range of logic puzzles, showing improved performance and generalization capabilities compared to existing methods.

## Related Work

Reasoning and GeneralizationOver the years the study of reasoning with learning based approaches has often focused on the domain of games such as chess, shogi and Go , Poker  or board games , Chalkiadakis, and Afantenos 2019, Perolat et al. 2022). While these mainly focus on correct play or in-distribution performance, the CLRS Algorithmic Reasoning Benchmark introduced by Velickovic et al. (2022) puts emphasis on generalizable reasoning. It consists of a diverse collection of well known algorithms collected from the textbook "Introduction to Algorithms" by Cormen et al. (2022) providing a resource to assess algorithmic reasoning for learning based approaches. Moreover, there exists a CLRS-Text  to better assess the reasoning capabilities from a language perspective as well. Abbe et al. (2024) provide a more theoretically supported view on the generalization performance of some classes of neural networks, trained with SGD. They specifically focus on Boolean functions, learned in a supervised training regime.

Graph Neural NetworksFirst introduced by the works of Scarselli et al. (2008), Graph Neural Networks have seen a recent emergence through a variety of new architecture types and applications . The graph-based representation underlying these models is particularly powerful as it provides a natural framework for capturing relational information and structural dependencies between entities. This has made GNNs especially interesting for tackling combinatorial optimization problems  and reasoning tasks that require understanding relationships between multiple elements . A key advantage of graph-based approaches is their capability to handle problems of varying sizes and complexities. One specific research direction focuses how these models generalize across different problem instances and sizes , with benchmarks like CLRS  providing a more systematic evaluation frameworks for assessing algorithmic reasoning capabilities with a focus on size generalization. This in turn has sparked more investigations into developing appropriate tools and architectures for such reasoning , and Velickovic, 2023; Minder et al., 2023; Mahdavi et al., 2023; Bohde et al., 2024; Muller et al., 2024).

## Preliminaries

### PUZZLES Benchmark

The PUZZLES benchmark, introduced by Estermann et al. (2024) is a comprehensive testing environment that aims to evaluate and enhance the algorithmic reasoning capabilities of reinforcement learning (RL) agents. It is centered around Simon Tatham's Portable Puzzle Collection (Tatham, 2004) and encompasses 40 diverse logic puzzles that range in complexity and configuration, providing a rich ground for assessing RL algorithms' performance in logical and algorithmic reasoning tasks. In the PUZZLES benchmark, RL agents can interact with the puzzles using either a visual representation (e.g., images of the game) or a discretized representation in the form of tabular data describing the current state of the puzzle. The puzzle instances are configurable in terms of difficulty and size, allowing researchers to test different RL algorithms' scalability and generalization abilities. Additionally, PUZZLES is integrated into the widely-used Gymnasium framework.

PUZZLES allows for the use of different observation spaces, including pixel-based inputs similar to those used in classic RL benchmarks like Atari (Bellemare et al., 2013), as well as discrete state representations that can be more suitable for logic-intensive tasks. Even though, the PUZZLES framework already allows to test generalization across different puzzles sizes in principle, preliminary results show that the proposed RL baselines struggle a lot in that domain. One current limitation is that the puzzle interface used for these test do not allow for an easy or natural way of adjusting to larger puzzle instances. In our work, we propose to overcome this issue by using a graph-based representation interface.

### Graph Neural Networks

Graph Neural Networks (GNNs) are a class of neural networks specifically designed to operate on graph-structured data. Traditional neural architectures, such as Convolutional Neural Networks or Recurrent Neural Networks are tailored for processing image or sequential data. In contrast, GNNs are specifically designed to handle relational data by incorporating important symmetries of the data within its architecture.

A graph \(G=(V,E)\) consists of \(V\), the set of nodes and \(E\), the set of edges. Each node \(v V\) and each edge \(e=(u,v) E\) may have associated feature vectors \(h_{v}\) and \(h_{e}\), respectively. Most common GNNs operate on the principle of message-passing which involves iterative neighborhood aggregation and node updates. Importantly, the mechanism is shared across all nodes in the graph, which allows GNNs to be applied to graphs of varying sizes, regardless of the number of nodes or edges. We follow the notion of Xu et al. (2019) to express the \(t\)-th layer as follows:

\[a_{v}^{t} = ^{t}(\{\{h_{u}^{t} u N(v)\}\})\] \[h_{v}^{t+1} = ^{t}(h_{v}^{t},a_{v}^{t}).\]

The original input features of the nodes are defined as \(h_{v}^{0}\) and messages from the neighborhood \(N(u)\) are aggregated and then combined with the previous state. In practice, we rely on parameterized functions \(\) and \(\) and use a permutation invariant aggregator \(\) such as sum.

\[_{v}^{t+1}=(_{v}^{t},_{u (v)}(_{u}^{k},_{v}^{k},_{uv}))\]

With each added layer or round of message passing, the receptive field of the nodes is increased. After \(k\) rounds, a node's representation has been influenced by its k-hop neighborhood. This relationship between the number of rounds and the receptive field size is crucial for capturing local and global graph structures effectively. Moreover, we usually distinguish between node and graph level prediction tasks:

\[y_{v} = (h_{v}^{k})\] \[y_{G} = _{G}(_{u V}(h_{u}^{k}) ).\]

For node prediction \(y_{v}\), the output of each node is typically a transformation of the last embedding after \(k\) rounds of message passing. Whereas for graph level prediction \(y_{G}\), we usually apply a graph-pooling operation to aggregate all final node embeddings into a graph representation.

### Reinforcement Learning

Reinforcement learning (RL) focuses on training agents to make sequential decisions in an environment to maximize cumulative rewards. An RL agent learns through trial and error by interacting with the environment: observing the current state, taking an action, and receiving a reward. This process is repeated, with the agent aiming to learn a policy \((a|s)\) that maximizes cumulative rewards over time. A common way to formalize an RL problem is as a Markov Decision Process (MDP), defined by states \(S\), actions \(A\), transition probabilities \(P\), rewards \(R\), and a discount factor \(\). The agent's goal is to find a policy that maximizes the expected cumulative discounted reward:

\[_{}[_{t=0}^{}^{t}R(s_{t},a_{t},s_{t+1}) ].\]

Figure 2: Some Example puzzles of the PUZZLES library, inspired by the collection of Simon Tatham.

In this work, we use a model-free RL approach with Proximal Policy Optimization (PPO) Schulman et al. (2017) to train agents on a subset of the PUZZLES benchmark. PPO is a popular model-free algorithm known for its stability and efficiency in finding effective policies.

## Methodology

### Modeling Puzzles as Graphs

The PUZZLES benchmark Estermann et al. (2024) provides a starting point for the selection of appropriate logic puzzles. While this benchmark already provides access to varying difficulties and puzzle sizes, there are a few details that make it challenging to study size generalization directly. The interface only provides the pixel observations or a discretized tabular view of the puzzle. This makes the development of models which can incorporate such a representation well when the size is varying challenging. Another aspect is that not all puzzles are equally suitable for the study of extrapolation. Indeed, some have interactive elements which make them more complex by design, while others rely on novel elements when increased in size. This adds another challenge of value generalization, even if the rules were learned correctly. For example Sudoku introduces new actions and elements in the form of additional numbers in larger instances. We aim to select a subset of puzzles which is large and diverse enough, but at the same time tries to decouple unnecessary complexities for the goal of evaluating extrapolation. Additionally, PUZZLES proposes an action space that involves moving a cursor around the playing field to then change the game state at its position. While this helps in providing a consistent action space for all puzzles, it adds an additional layer of complexity to the learning process. We therefore propose to focus on the following criteria:

1. [leftmargin=*,noitemsep,topsep=0pt]
2. _Action Space_ The ability to describe the game as a collection of atomic cells that together represent the full state of the game.
3. _Independence_ Each cell can take an action independently of the actions of other cells. The resulting configuration does not necessarily have to be valid.
4. _Solvability_ The games have no point of no return (e.g., chess). At every time step, from the current state of the game, it is always possible to solve the game.
5. _Complete Information_ There are no interactive or stochastic elements in the game. Given just the initial state of the game, it is possible to derive its solution.
6. _Value Generalization_ The game limits it's exposure to new concepts (new elements, increased value ranges) as it increases in size. The number of actions remains constant.

To create a strong and diverse benchmark, we select six puzzles that fit the above listed criteria: Light Up, Loopy, Mosaic, Net, Tents, and Unruly also shown in Figure 3. For an in-depth description of the puzzles and the rules we refer to the Appendix.

As previously stated, in order to have an appropriate representation that can facilitate testing extrapolation on various puzzle sizes, we propose to use graphs as a unifying structure for the puzzles. By representing the puzzles as graphs, they can naturally accommodate different sizes, while preserving the local relationships of the puzzle. For each puzzle type, we provide an interface that transforms the given puzzle state into a graph. This transformation is specified by the graph topology, the types of nodes and edges, their annotated features and the allowed actions that each node can take. In general, we try to provide a very general and unified interface as not to impose design choices onto the model architectures.

Each graph representation consists of two types of nodes, which we will exemplify by the game Loopy depicted in Figure 4. The _decision-nodes_, which represent the atomic cells of a game and the _meta-nodes_ which contain shared information about a collection of nodes. Each decision-node directly influences the state of the game, as it can take an action. In Loopy, every edge of the original grid is modeled as a decision-node, which determines if an edge should be present or not. The meta-nodes cannot directly alter the state of the game, instead they contain information such as a constraint or if a violation is present. In Loopy, these are the faces of the original grid. The edges of the graph usually represent neighboring cells in the original game, here adjacent edges or faces. Moreover, each node and edge is assigned a feature vector containing information about the state of the game or the direction of the edge. Note that as the puzzle is scaled to larger instances, the local graph representation remains identical across puzzle sizes. A detailed explanation on how each puzzle is modeled and details on the graph topology and action spaces is contained in the Appendix.

### Training

We follow the training process outlined in PUZZLES Estermann et al. (2024), using PPO as a relatively simple, yet solid training algorithm. Corresponding to the graph observation, each decision node in the graph represents a possible action in the game. For each puzzle, we have a distinct set of actions associated with every decision node. The fi

Figure 3: We provide a new graph interface in order to ease the testing for size generalization on six puzzles. From top left to bottom right: Light Up, Loopy, Mosaic, Net, Tents and Unruly.

nal action for each node is selected independently using a softmax layer. Once the actions are chosen for all nodes, they are executed simultaneously, resulting in a new state of the game. This approach differs from the cursor model used in the PUZZLES benchmark, however, it removes an additional layer of complexity and improves training efficiency. For this reason and our more dense reward scheme, we are able to drastically reduce the rollout length, leading to more efficient training.

For each combination of puzzle and presented architecture configuration we report the performance over three different model seeds. In order to offer some insight into the generalization capabilities, model selection is performed on slightly larger configurations similar to previous work . For model selection, we assess the performance every 100 training iterations on the validation step which contains puzzles one size larger than during training.

### Extrapolation Evaluation

The most important aspect about our benchmark is to test the capability of generalizing outside the training distribution to larger puzzles. While other metrics such as training behaviour or in-distribution-performance are of interest, we want to provide a comprehensive evaluation scheme for extrapolation.

To this end, we provide for each puzzle a dedicated training size and a set of larger test sizes. Unfortunately, due to the individual puzzle constraints, not all sizes can be exactly the same across the puzzles. We determine the training size of each puzzle to be the smallest size that contains a sufficient amount of unique training samples, specifically 40'000 instances. Moreover, in order to compare the extrapolation performance we aim to provide a set of sizes for each puzzle so that the overall relative increase in the sizes is similar across puzzle sizes. Each puzzle contains six test sets. One that matches the training size, the next two bigger puzzles sizes +1 and +2 as well as puzzles that are factors x4, x9, x16 larger. For each test set we test on a set of 50 different puzzles and we report the number of fully solved instances. For more details we refer to Appendix.

### Architectures

We distinguish between two different approaches to solve the puzzle, a recurrent and a state-less mode. In the recurrent mode, at each step \(t\) of the puzzle solving, the model gets as input the current state of the puzzle \(x^{t}\) as well as the previous hidden state of the model \(h^{t}\). Then, these embeddings are given to a processor module which can be either a GNN module or a transformer based architecture. Finally, the output of the model is \(o^{t}\), the action for each decision-node of the puzzle and the next hidden state which is derived using a GRU unit helping to stabilize the computation over longer sequences. Whereas in the state-less mode, no hidden states \(h^{t}\) are computed and only \(x^{t}\), the current state of the puzzle, is provided as the sole input:

\[z^{t} = _{1}(x^{t}||h^{t})\] \[p^{t} = z^{t}+(z^{t})\] \[o^{t} = _{2}(p^{t})\] \[h^{t+1} = (h^{t},p^{t}).\]

We use two different processors, a graph and a transformer baseline. The graph baseline uses a GCN 

Figure 4: Illustration of the modeling of the puzzle Loopy. In this case, each decision-node (black or blue circles) corresponds to an edge of the original game grid. Each face of the game grid is represented as a meta-node (red circle), which is connected to its four adjacent decision-nodes. The nodes and edges of the graph have features, which determine the current state. The next state is determined by the collective actions of all decision-nodes. The local graph representation remains the same across all puzzle sizes.

Figure 5: Illustration of testing the ability to generalize beyond the training distribution for the puzzle Net. While models only see small puzzle instances during training, the rules and logic that govern the puzzle remain the same. Therefore, during evaluation, the model is tested on puzzles that are up to 16x larger.

2017b) which executes three rounds of message passing on the provided graph structure. The transformer baseline uses an encoder transformer (Vaswani et al., 2017) which consists of three layers of full self attention including positional encoding to indicate the position of the node in the puzzle. The positional encoding consists of a fixed-size sine/cosine embedding of different frequencies, similar to Vaswani et al. (2017), but extended to 2D.

### Reward

Initial experiments were done using only a _sparse_ reward: \(0\) reward to the agent if the game has not been solved and \(R\) reward if the game is solved. As this reward provides very little feedback during training, agents struggle to learn effective strategies leading to slow learning and poor performance, especially in complex puzzles. Instead, we work with two types of reward systems: _iterative_ and _partial_.

Given a graph \(G\) with \(n\) decision cells (not considering meta-nodes) can be encoded with a unique sequence \((G)=(g_{i})_{i=1}^{n}\), with each \(g_{i}\{1,2,,m\}\) denoting the \(i\)-th value on the grid and \(m\) representing the number of states a cell can take (e.g., \(m=4\) for Tents: empty, grass, tent, tree). The sequence corresponding to the puzzle's unique solution is denoted as \(=()\), where \(\) represents the solution graph. We measure the quality \(Q(G)\) of a graph as its similarity to the solution:

\[Q(G)=_{i=1}^{n}(g_{i},_{i})\]

where \((,)\) is the indicator function. To encourage policies to make iterative progress we follow the technique used by Tonshoff et al. (2022) which defines the reward \(r^{t}\) based on the improvement in the graph's quality compared to the best quality \(q^{t}=_{t^{} t}Q(G^{t^{}})\) observed so far. The reward is then defined as the incremental improvement of the current state relative to the highest quality achieved in prior iterations:

\[r^{t}_{}=(0,Q(G^{t+1})-q^{t})\]

This way, the agent earns a positive reward if the actions result in a state that is closer to the solution than any previous state. To avoid penalizing the policy for departing from a local maximum in search of superior solutions, we assign a reward of zero to states that are not an improvement.

The puzzles often encode violations of the puzzle rules explicitly, i.e. if two neighboring cells or all cells in a row violate a constraint. In the _partial reward_ scheme, this information is given to the actors encoded in both the decision-nodes and the meta-nodes of the puzzle state. In the partial reward system, we adjust the calculation to only include nodes that are not part of a violation. For each node we have a indicator \(C:(G)\{0,1\}\) if it is part of such a violation. Then the quality of a graph and the corresponding reward is defined as:

\[(G) = _{i=1}^{n}(g_{i},_{i}) C(g_{i})\] \[r^{t}_{} = (0,(G^{t+1})-^{t})\]

The violation conditions are specific to the different puzzle environments. Thus, by incorporating puzzle-specific conditions into the reward calculation, the partial reward scheme aims to provide more meaningful and consistent feedback to the agent, promoting a more balanced and effective learning trajectory.

## Empirical Evaluation

### Comparison to Non-Graph Baselines

First, We compare our results to the baselines provided in the PUZZLES benchmark. Since the action space is different, we can only compare success rate but not episode length. The results are presented in Table 1. For the puzzles Tents, Mosaic, Loopy and Unruly, the GNN architecture was clearly able to surpass the performance of the baseline, even on much larger sizes. For Lightup and Net, the GNN architecture achieves similar performance to the baseline but on a substantially larger puzzle size. Note that all of these evaluations are done on the same size as the training distribution.

For the following evaluations, we report the interquartile mean performance of all puzzles, including 95% confidence intervals based on stratified bootstrapping, following (Agarwal et al., 2021).

### GNN and Transformer Baselines

Next, we compare the GNN architecture against a transformer baseline. The transformer baseline uses exactly the same nodes as the GNN. Because it misses the graph structure, the nodes are given a positional encoding. Our results in Figure 7 show that the GNN model performs much better than the Transformer baseline. We hypothesize that the GNN

Figure 6: Percentage of puzzles solved during size extrapolation for models trained with different reward systems. During modest extrapolation, there seems to be no significant difference between the iterative and partial reward schemes. However, the partial reward scheme seems to allow for slightly better performance in the x4 and x9 extrapolation settings. Unfortunately, sparse rewards do not provide enough signal for the agents to learn any reasonable policy.

can utilize the information about the explicitly encoded relational structure of the puzzle more effectively and as a result has a more suitable inductive bias compared to the transformer, helping it to better learn and then extrapolate on the puzzles.

### Recurrent vs State-less

Note that the puzzles are stateless, meaning that at every given moment during the solving, there is sufficient information to determine the solution with just the current puzzle state, i.e. what is visible on the board. However, it might still be beneficial to have a recurrent model for solving such a puzzle. Different steps towards computing the solution might require more computation or reasoning steps. As a consequence, for a GNN model it might be that the next correct action cannot always be determined with the information present in its 3 hop neighborhood. Therefore, it could be beneficial to allow these models access to a recurrent state, passing and storing information in between the different actions without affecting the game state. We compare a recurrent version of the graph architecture against a stateless variant in Figure 8. It seems that for modest extrapolation the recurrent version is more successful, whereas for larger sizes, the state-less architecture can solve more puzzles, even solving some instances that are 16 times larger.

### Reward Systems

Finally, we want to evaluate the choice of reward system and its impact on size generalization. For this experiment, we use the GNN architecture with recurrent states. As depicted in Figure 6 by solely relying on sparse reward signals, it is very hard to learn to solve the puzzles, even during training. Both the iterative and partial variant show very comparable performance during training and modest extrapolation sizes. However, it seems that for the larger extrapolation setting the models trained using the partial reward generalize better.

## Limitations

Our study focuses on how to learn generalizable reasoning, which naturally imposes certain limitations. We deliberately concentrate on model-free reinforcement learning methods, excluding model-based approaches and pre-trained models including large language models. While these methods have their own advantages, our study wants to put the focus

  
**Puzzle** & **Size** & **GNN Solved-\%** & **Baseline Solved-\%** \\   & 5x5 & \(99.67 0.47\)\% & - \\  & 4x4 & - & \(45.0\)\% \\   & 5x5 & \(99.33 0.24\)\% & - \\  & 3x3 & - & \(99.1\)\% \\   & 4x4 & \(100.0 0.0\)\% & - \\  & 3x3 & - & \(29.4\)\% \\   & 4x4 & \(68.83 7.17\)\% & - \\  & 3x3 & - & 0\% \\   & 4x4 & \(99.83 0.24\)\% & - \\  & 2x2 & - & \(100.0\)\% \\   & 6x6 & \(83.67 19.01\)\% & 0\% \\   

Table 1: Percentage of puzzles solved, average and standard deviation over all seeds, for the baseline GNN architecture compared to the best non-GNN architecture from . Note that  mostly trained agents on smaller and therefore much easier versions of the puzzles, using architectures unable to generalize to larger sizes.

Figure 8: A recurrent agent design brings advantages for the success rate during training and modest extrapolation. For stronger extrapolation, however, a state-less algorithm performs better. The state-less algorithm is even able to solve a few puzzles at x16 extrapolation.

Figure 7: Compared to the transformer model, the graph architecture performs much better across the different puzzles, leading to a more consistent and successful extrapolation behavior. Both use the same modeling of the puzzles, but the GNN explicitly encodes the edge relationships, whereas the transformer is given a positional encoding.

on generalization from the given training distribution without relying on explicit given constructions or extending the training distribution itself.

Our research uses logic puzzles as a testbed for reasoning. While these provide an artificial environment with clear rules and scalability, they represent only a subset of reasoning tasks. Consequently, our findings may not directly generalize to all types of real-world reasoning problems or domains. Nevertheless, given the current challenges in developing truly generalizable reasoning systems, we believe that studying these techniques in controlled, synthetic environments is a crucial step towards advancing the field.

## Conclusion

The challenge of developing neural architectures capable of generalizable reasoning remains at the forefront of artificial intelligence research. Our study focuses on providing a controllable testbed of logic puzzles that can easily be scaled to test out-of-distribution generalization. Furthermore, through the unified graph representation, we demonstrate the potential of our graph-based multi-agent reinforcement learning approache in extrapolating their reasoning to larger, unseen instances.

In our empirical evaluation, we find that the graph-based modeling approach of the puzzles seems to be more fitting, resulting in both overall improved in-distribution and out-of-distribution performance compared to previous methods. Furthermore, we evaluate the inductive bias of a GNN architecture against a transformer baseline and find that the explicit graph structure aids generalization. Finally, we compare recurrent and state-less modeling for sequential decision making as well as different reward systems in the context of extrapolation. Our results underscore the challenges of achieving correct generalization without explicit guidance during training. This further highlights the importance of studying generalization in controlled environments. We aim to provide an stepping stone towards machine learning systems that can truly grasp the underlying reasoning and apply logical principles across diverse and increasingly complex domains.