# Captioning and Task-Specific Prompting for Improved VLM Performance

Anonymous submission

###### Abstract

Vision-language models (VLMs) have transformed tasks requiring visual and reasoning abilities, such as image retrieval and visual question answering (VQA). Despite their success, VLMs face significant challenges with tasks involving geometric reasoning, algebraic problem-solving, and counting. These limitations stem from difficulties in effectively integrating multiple modalities and accurately interpreting such tasks. We propose an efficient, question-driven image captioning pipeline to enhance visual question answering abilities in mathematical contexts. Our method extracts keywords from the question, generates targeted captions for each image-question pair using those keywords, and uses the caption as a prompt for QnA. We propose utilizing task-specific guidance as an "approach" to enhance the VQA and captioning process. Additionally, we evaluate the robustness of these models against adversarial prompts to ensure that our captioning-based approach does not compromise much on robustness. Our pipeline is tested on diverse math-related and visual reasoning tasks across multiple datasets and VLMs.

## Introduction

VLMs have made substantial progress in advancing multimodal learning. Models such as CLIP , BLIP , and ImageBind  offer powerful encoders with excellent representation capabilities, demonstrating high transferability in recognition and understanding tasks. Additionally, VLMs like LLaMA Adapter , BLIP2 , Flamingo , and LLaVA  combine visual encoders from discriminative VLMs with Large Language Models (LLMs) like LLaMA  and GPT-3 , capitalizing on LLMs' exceptional generative abilities. Recent models, including GPT-4 Vision , Claude 3.5 Sonnet , and Gemini , have achieved human-level performance across various benchmarks such as, Mmmu , MathVista , DocVQA . While these models perform well on standard benchmarks, studies indicate that they often struggle to fully comprehend the visual aspects of mathematical tasks, particularly failing to capture relational and spatial information  within diagrams and relying heavily on textual cues instead . Various works have further shown that these models lack both spatial scene understanding  and temporal awareness , with some studies even revealing difficulties with basic visual tasks that humans handle effortlessly. This underscores the need to enhance not only the visual reasoning and understanding capabilities of VLMs but also their fundamental ability to perceive and interpret images effectively . In this work, we evaluate the visual capabilities of these models on math-related tasks, specifically testing their ability to truly'see' and interpret visual information. We propose a zero-shot pipeline that uses task-specific guidance to generate image captions to improve performance on these tasks. Additionally, we test the model's robustness against adversarial prompts framed as incorrect yet relevant problem-solving strategies. Given that VLMs are largely pre-trained for caption generation ,  and many downstream tasks  require textual outputs, we hypothesize that guiding the model to create detailed captions encourages it to focus more on the visual content, reducing its reliance on textual cues from the question alone. Captioning, thus, enables the model to capture all essential image attributes, which is especially critical for mathematical reasoning.

## Background and Related work

Various prior works have tried to improve the performance of VLMs without requiring external fine-tuning, techniques such as in-context learning, chain of thought prompting, few-shot prompting , have enhanced VLM performance, yet these models continue to face challenges in mathematical tasks, particularly in counting, leading some researchers to describe them as "blind" to visual information . While research suggests that much of the reasoning displayed by Vision-Language Models (VLMs) may rely more on the phrasing of questions than on the images themselves . This limitation is especially pronounced in tasks that depend heavily on visual details, such as counting nested shapes, identifying line intersections, identifying the number of objects in an image, and solving geometrical problems where VLMs consistently fall short. To evaluate these visual abilities, specialized datasets such as Math Vision , Count Bench , Blind , and Geo170k have been introduced. In efforts to improve general visual question answering (VQA) performance, various techniques have been developed, including question-driven image captions  that are processed by language models. These methods have shown promise, particularly in enhancing direct visual question-answering tasks. Further, some studies aim to improve the reasoning abilities of VLMs by directly fine-tuning them for tasks such as question answering  and mathematical problem-solving . In contrast, our research focuses on enhancing the visual performance of VLMs through a pipeline that leverages simple prompting techniques, eliminating the need for fine-tuning and thereby reducing computational overhead.

## Methodology

We evaluated Vision-Language Models (VLMs) on a range of visual tasks to assess their visual abilities. To ensure robustness and generalizability, we selected tasks from four datasets covering various question types, including geometry, counting, algebra, and mathematical reasoning. Our experiments employed a diverse set of four VLMs to evaluate the generalization of our approach. Each model was tested across four distinct methods to comprehensively assess our proposed approach:

1. **Vanilla QnA:** We used a classical zero-shot approach where each model was directly queried with questions related to images from the datasets. This formed the baseline of our experiments.
2. **Approach-based QnA:** We used Gemini-1.5  to provide the approach to solve the desired question by simply giving it the question and asking a comprehensive approach for the question. Then we prompted the VLMs being tested to solve the question by providing the approach with it.
3. **Vanilla Keyword Captioning:** We used the base model to create captions for the image, guiding the captioning process with specific keywords related to the question. These keywords were obtained from the Llama 3.1-Instruct model1 by prompting the model to provide 3-5 concise keywords for each question. After generating the captions with the VLM, using the keywords as context, we input them into an LLM(Gemini) and asked the relevant questions for each task. 4. **Approach-based Captioning:** We conducted additional tests to evaluate the impact of providing more context in prompts for generating captions. First, we fed the question to Gemini, asking it to develop an approach to solve the problem. Following this, we fed the proposed approach, along with the image and relevant keywords generated from Llama to the VLM. We asked the VLM to caption the image using the keywords and approach as context. Finally, we input this caption into an LLM(Gemini) and asked the model to answer the required question based on the caption.

Further, to assess the robustness of our proposed approach, we tested the models across the following 2 tasks as well:

1. **Adverserial-based QnA:** We appended an adversarial prompt, generated using Gemini, to the question. This contained misleading information about the approach to be followed to solve the question. We then queried each VLM on images in the dataset, using the question appended with the incorrect adversarial approach.
2. **Adverserial-based Captioning:** Similar to approach-based captioning, we generated adversarial approaches using Gemini by asking it to generate a "wrong" approach to solve the given question. Then, we generated the image caption from the VLMs with the adversarial

Figure 1: Example of our Captioning Approach

approach along with the image as inputs. Finally, we used an LLM (Gemini-1.5) and asked it to answer the required question based on the caption.

## Experiments

We chose diverse models and techniques to prove and test our hypothesis. We chose four datasets, Geo170k , CountBench , Blind , and MathVision , containing various tasks related to geometry, reasoning, algebra, and counting. Also, we split the MathVision dataset into three subparts: mainly vision-based, geometry-based, and mathematics-based. To cover a range of model sizes, we selected a diverse set of open-source models, namely Gemini-1.5-Flash , LLaVa , Florence-2 , and Phi 3.5 Vision Instruct . This selection ensures a variety of model sizes, from smaller ones with fewer parameters to larger, more complex models.

* **LLaVa:** For LLaVa, we used the GroqAPI 2 to access the model. 

* **Gemini-1.5-Flash:** For Gemini, we used Google AI studio3 to access the model. * **Florence-2**:45 For Florence-2, we used the open-sourced model available on huggingface. * **Phi 3.5 Vision Instruct:6 we used the open-sourced model available on huggingface. 
**Note:** For Florence-2 captions were generated using the token \(<\)DETAILED CAPTION\(>\), and the approach was passed onto the detailed caption. For other models, the approach was passed during the caption generation stage. Additionally, the Florence-2 direct checkpoint was unable to perform QnA-related tasks, so we used Florence-2 DocVQA for QnA-related tasks.

## Results

Vanilla captioning leads to overall performance improvements, though its effectiveness varies with model size--it tends to benefit smaller models more than larger ones. In fact, Gemini registers a small performance drop using vanilla captioning. Approach-based QnA demonstrates results above the baseline, supporting adding additional information and context as prompts. Using task-based prompts for generating captions consistently leads to significant improvements across all models and datasets. This approach

Figure 3: Comparison of different Methods

Figure 2: Example of our Caption Based Approach

demonstrates the greatest improvement in performance, making it a promising method. We also observe a performance drop with adversarial prompting, though this drop is less, thus showing that captioning does not compromise on robustness. Performance differences across datasets are notable, too (Table 4). While average model performance remains relatively unchanged on the CountBench dataset, which focuses on counting tasks, captioning yields larger accuracy improvements on the Geo and MathVision datasets, which consist of geometry and visual mathematics tasks. This aligns with our hypothesis that captioning allows the model to focus more on visual cues necessary for answering diagram-related questions .

## Conclusion

The results of our study support our initial hypothesis: Vision-Language Models (VLMs) exhibit significant limitations in mathematical and visual tasks, especially those involving numbers and counting--tasks that test the models' ability to "see" rather than merely reason. While captioning with task-specific keywords can improve performance in certain cases by directing the models' focus toward visual details, our findings reveal that its effectiveness depends on factors such as dataset type, task complexity, and model size. Larger models, particularly those pre-trained on similar tasks, tend to perform better, underscoring the role of pre-training, as seen in Gemini, a trend less evident in smaller models. Our experiments demonstrate that we can improve the mathematical and visual capabilities of Vision-Language Models (VLMs), resulting in enhanced performance. When using adversarial prompting, we observed a consistent performance drop across all models. However, this drop was moderate and, on average, similar to the decline seen in adversarial QnA tasks, indicating that captioning maintains a satisfactory level of robustness. In summary, approach-based captioning boosts the reasoning abilities of VLMs and increases their focus on visual cues, thereby reducing their reliance on textual input. This approach offers a promising direction for addressing VLMs' limitations in perceiving numerical information and handling visually complex mathematical tasks. By using structured prompts derived directly from the question and incorporating reasoning-guided methods, we can mitigate some of these weaknesses and improve VLM performance in such problem-solving scenarios.

## Limitations and Future Work

Testing the reasoning capabilities of multimodal models is a broad research area, and we propose ways to enhance model generalizability in our study. Due to resource constraints, we couldn't experiment with large-scale models like GPT-4  or use full-size datasets, limiting our ability to explore scalability fully. Future work with greater resources could expand on robustness and scalability across diverse model architectures, including models like Claude Sonnet 3.5 (cla 2024) and GPT-4 .

Our current approach involves using an LLM to process captions from the VLM for answer extraction. Simpler parsing techniques might be sufficient, given the quality of generated captions. Additionally, advanced methods, such as sophisticated prompt engineering as those used with LLMs , or incorporating domain-specific knowledge  followed by fine-tuning for more precise, contextually relevant captions could lead to a higher performance increase. Our study also doesn't prioritize interpretability, which could be addressed by examining how captioning influences the interaction between visual and textual inputs in Vision-Language Models (VLMs) and impacts performance. Exploring interpretable alternatives, like graph networks, could further improve transparency and deepen understanding of the models' reasoning processes.