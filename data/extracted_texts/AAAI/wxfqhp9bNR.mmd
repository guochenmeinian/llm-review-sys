# Improving Multi-Hop Reasoning in LLMs by Learning from Rich Human Feedback

Nitish Joshi1, Koushik Kalyanaraman 2, Zhiting Hu3, Kumar Chellapilla2, He He1, Li Erran Li2,

1New York University, 2Amazon Web Services, 3UC San Diego

nitish@nyu.edu, erranli@gmail.com

###### Abstract

Recent large language models (LLMs) have enabled tremendous progress in natural language understanding. However, they are prone to generate confident but nonsensical reasoning chains, a significant obstacle to establishing trust with users. In this work, we aim to incorporate rich human feedback on such incorrect model generated reasoning chains for multi-hop reasoning to improve performance on these tasks. To do so, we collect two such datasets of human feedback in the form of (correction, explanation, error type) for StrategyQA and Sports Understanding datasets1, and evaluate several algorithms to learn from such feedback. We show that fine-tuning on such small datasets of rich human feedback can improve model's performance of generating the correct final answers, and also improves the model's ability of judging the correctness of it's own answer.

## Introduction

With the onset of large language models (LLMs) , the field has seen tremendous progress on various NLP benchmarks. Among them, the progress has been striking on relatively simpler tasks such as short context or factual question answering , compared to harder tasks which require reasoning such as multi-hop question answering . Even though LLMs may not be best at generating correct reasoning chains or explanations for such hard tasks , the prompting abilities of LLMs have the potential to provide partially correct (and relevant) facts required to answer the question. Relatedly, recent work has found that without any finetuning LLMs cannot self-correct their reasoning yet , suggesting the need for human intervention.

Motivated by this, we try to address the following research question -- _can we improve reasoning of LLMs by learning from human feedback on model-generated reasoning chains?_ Figure 1 provides an overview of our approach -- we first prompt the model to generate reasoning chains for multi-hop questions, then collect diverse human feedback on these chains for diagnosis and propose training algorithms to learn from the collected data.

We collected diverse feedback including correction to model's generation, explanation of why the generation was wrong and error type for a total of 2.2k examples from two datasets which we will publicly release. We propose multiple training algorithms to learn from the collected feedback including a multitask algorithm, a variant of self-consistency in chain-of-thought prompting , and a refinement algorithm where we refine the model generated reasoning chain. We use the proposed algorithms on Llama2  and find that they either improve model's reasoning ability (sports understanding dataset) or perform comparable to in-context learning (strategyQA dataset). More importantly, we find that the fine-tuned model is sometimes better at judging if it's own answer is correct compared to the base (not finetuned) Llama2 model, an important practical ability in order to use LLMs more widely.

Our main contributions can be summarized as: (1) a dataset of rich human feedback including natural language feedback for 2.2k examples for multi-hop reasoning ; (2)

Figure 1: Overview of the process, where we first prompt LLMs to generate reasoning chains for multi-hop questions, collect diverse feedback on the generations including categorical feedback and natural language feedback, and use multiple training algorithms to learn from them.

novel algorithms to learn from diverse feedback to both improve reasoning performance and which can make LLMs better at judging their own correctness.2

## Related Work

**Learning from Feedback.** Learning from human feedback in the form of rewards  has become an effective paradigm for improving LLMs . Most feedback datasets either provide sparse feedback such as binary feedback  or provide natural language feedback but for narrow space of tasks such as summarization . In comparison, we create a dataset of rich human feedback including natural language feedback for much harder reasoning tasks.

**LLM self-correction.** In contrast to learning from human feedback, recent works have explored if LLMs can self-correct their answers. Specifically, madaan2023learning use an iterative feedback and refinement procedure to improve performance; welleck2022learning introduce self-correctors by separating the generator and the corrector; bai2022learning use LLMs to generate feedback based on a 'constitution'. Nevertheless in the context of reasoning, huang2023learning show that LLMs struggle at self-correction and performance might even deteriorate. Given this shortcoming of LLMs' self-correction ability, we collect a rich human feedback dataset for reasoning and demonstrate its utility.

## Data Collection

Here, we describe the details of the feedback we collected and the annotation protocol followed during data collection. We collected feedback for model generations based on two reasoning based datasets: StrategyQA  and Sports Understanding, part of BigBench . We used GPT-J 

|} 
**Question:** Is the voice of the Genie from Disney’s Aladdin still alive? **Answer**: The answer is no because the Genie was voiced by comedian Robin Williams. Robin Williams died in 2014. **Question**: Johnny Gaudreau nutmegged the defender. Is this sentence plausible? **Answer**: The answer is no because Johnny Gaudreau is an American professional ice hockey player. Nutmeg which means passing ball through the opponent’s leg is a term from football. \\  

Table 1: Examples from StrategyQA (top) and Sports Understanding (bottom) used to prompt the language model.

Figure 2: The interface used to collect feedback from annotators, displaying all the diverse feedback we collect for an examples from StrategyQA.

suzaki 2021) to generate answers for StrategyQA and FlannT5 Chung et al. (2022) to generate answers for sports understanding dataset.3 In each case, the model was prompted with \(k\) in-context examples containing question, answer and explanation such as the ones showed in Table 1, followed by the test question.

Figure 2 shows the interface we used -- annotators are given the question, model generated answer and the explanation split into steps. For each question, we collected the following feedback:

**Subquestions**: Decompose the original question into simpler subquestions required to answer the original question. This task was added after a pilot where we found that adding this task helps to 'prime' the annotators and improve quality of the rest of the tasks.

**Correction**: Annotators are provided with a free-form text box pre-filled with the model generated answer and explanation, and asked to edit it to obtain the correct answer and explanation.

**Error Type**: Among the most common types of error we found in the model generations (Factual Error, Missing Facts, Irrelevant Facts and Logical Inconsistency), annotators were asked to pick one or more of the error types which apply to given answer and explanation.

**Error Description**: The annotators were instructed to not only classify the errors but also to give a comprehensive justification for their categorization, including pinpointing the exact step where the mistake occurred and how it applies to the answer and explanation provided.

We used private internal vendors as the annotators. The data collection took place over multiple rounds. We first conducted two small pilots of 30 examples and 200 examples respectively, after which the annotator team were given detailed feedback on the annotation over a video call. We then conducted the data collection over two batches for StrategyQA, and over one batch for Sports Understanding giving periodic feedback throughout -- a total of 10 annotators worked on the task over a period of close to one month.

### Dataset Statistics

We gathered feedback on a total of 1565 examples for StrategyQA and 796 examples for Sports Understanding. Table 2 illustrates the percentage of examples that were error-free in the model generation and the proportion of examples that contained a specific error type. It's worth noting that some examples may have more than one error type.

## Learning Algorithms

For each question \(q\), and model generated answer (with explanation) \(m\), we have the following feedback collected: correct answer and explanation \(c\), type of error present in \(m\) (denoted by \(t\)) and error description \(d\), as described in section. Table 3 provides an example with all the feedback.

Multitask Learning.A simple baseline to learn from the diverse feedback available, is to treat each of them as a separate task. More concretely, we fine-tune Llama2 with the following objective:

\[\ \ p(c|q)+p(t|q,m)+p(d|q,m) \]

For each term in Eq 1, we use a separate instruction appropriate for the task (e.g. 'Predict error in the given answer'). We also convert the categorical variable \(t\) into a natural language sentence. During inference, we use the instruction for the term \(p(c|q)\) ('Predict the correct answer for the given question') to generate the answer for the test question.

Weighted Self Consistency.Motivated by the success of self-consistency Wang et al. (2022) in chain-of-thought prompting, we propose a weighted variant of it. Instead of treating each sampled explanation as correct and considering the aggregate vote, we instead first consider whether the explanation is correct and then aggregate accordingly.

We first fine-tune Llama2 with the same objective as in equation 1. During inference, given a test question \(q\), we sample multiple possible answers (with the instruction for \(p(c|q)\)): \(a_{1},a_{2},..,a_{m}\). For each sampled answer \(a_{i}\), we use the instruction for the term \(p(t|q,m)\) i.e. 'Predict error in the given answer' to identify if it contains error: \(t_{i}=\ \ p(t|q,a_{i})\). Each answer \(a_{i}\) is assigned a weight of 1 if it is correct, otherwise it is assigned a weight of \(<1\) (tunable hyperparameter). The final answer is obtained by considering a weighted vote over all the answers \(a_{1}\) to \(a_{n}\).

Refinement.In the previous proposed methods, the model directly generates the correct answer \(c\) conditioned on the question \(q\). Instead, here we propose to refine the model generated answer \(m\) to obtain the correct answer for a given

  
**Error Type** & **StrategyQA** & **Sports Und.** \\  None & 17.6\% & 31.28\% \\ Factual Error & 27.6\% & 38.1\% \\ Missing Facts & 50.4\% & 46.1\% \\ Irrelevant Facts & 14.6\% & 3.9\% \\ Logical Inc. & 11.2\% & 5.2\% \\   

Table 2: Percentage of examples in each dataset where the model generation had the particular error type. Note that a example might contain more than one error type.

  
**Question (\(q\))**: Is the voice of the Genie from Disney’s Aladdin still alive? \\
**Model Generation (\(m\))**: The answer is yes because Genne is voiced by Robin Williams. He is still alive. \\
**Correction (\(c\))**: The answer is no because the Genie was voiced by Robin Williams. Robin Williams died in 2014. \\
**Error Type (\(t\))**: Factual Error \\
**Error Description (\(d\))**: In step 2, the explanation incorrectly mentions that he is still alive when instead he died in 2014. \\   

Table 3: Example of a question, model generation and the feedback we collect for StrategyQA.

[MISSING_PAGE_FAIL:4]