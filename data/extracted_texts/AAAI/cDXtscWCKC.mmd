# SleepFM: Multi-modal Representation Learning for Sleep across ECG, EEG and Respiratory Signals

Rahul Thapa1, Bryan He2, Magnus Ruud Kjaer3, Gauri Ganjoo3, Hyatt Moore3,

**Emmanuel Mignot3, James Zou1,**

###### Abstract

Sleep is a complex physiological process involving multiple modalities across the body. We curate a large dataset of simultaneous polysomnography (PSG) recordings comprising electrical brain activity (EEG), heart rhythms (ECG), and respiratory patterns from over 14,000 participants, totaling over 100,000 hours of sleep data. We develop _SleepFM_, the first multi-modal foundation model for sleep learned through contrastive learning on this highly heterogeneous physiological data. When evaluated on a held-out test set, _SleepFM_ significantly improves retrieval performance over 500x over random chance. A logistic regression model trained on _SleepFM_'s learned embeddings achieves strong performance on sleep stage classification (macro AUPRC 0.69) and apnea detection (AUPRC 0.71), outperforming an end-to-end trained CNN for sleep stage classification (AUPRC 0.579) and apnea detection (AUPRC 0.56). We find representations learned using an innovative leave-one-out approach during contrastive learning significantly improve downstream task performance compared to representations from standard pairwise contrastive learning. This work demonstrates the value of holistic multi-modal sleep modeling.

1 Department of Biomedical Data Science, Stanford School of Medicine, Stanford, CA, USA

2 Department of Computer Science, Stanford School of Engineering, Stanford, CA, USA

3 Department of Psychiatry and Behavioral Sciences, Stanford School of Medicine, Stanford, CA, USA

rthapa84@stanford.edu

## Introduction

Sleep monitoring is a critical aspect for not only understanding sleep disorders but also gaining valuable insights into overall brain, pulmonary, and heart health . Polysomnography (PSG), a comprehensive overnight sleep study, serves as a powerful tool by recording various physiological signals during sleep, including electroencephalogram (EEG), electroocolograms (EOG), and electrocardiogram (ECG) . Traditionally, PSG data analysis involved manual visual inspection, a labor-intensive and time-consuming process prone to errors . Recent advancements in supervised deep learning have shown promise in automating sleep stage classification, particularly for disorders like apnea . However, most methods rely on labeled data from a narrow task. They rarely leverage the full breadth of physiological dynamics across diverse PSG modalities.

In parallel, contrastive learning has emerged as a powerful technique in other domains, such as radiology and pathology, where it pairs images with corresponding medical reports to learn rich medical image representations . However, PSG representation learning by pairing different channels via multi-modal contrastive learning has been less explored. While some unimodal contrastive learning methods have been applied to ECG data , they lack the ability to compare different modalities effectively in latent space, which is crucial for transfer learning. Additionally,  developed SimCLR-like contrastive learning models pre-trained using multi-modal clinical time series data including ECG signals and structured time series data, and  utilized a large collection of electronic health records (EHRs) to learn ECG representations through contrastive learning between ECG, structured and unstructured EHR data. However, these studies primarily focused on ECG data rather than the broader spectrum of PSG modalities investigated here.

**Our Contribution** We introduce _SleepFM_, a sleep foundation model trained using contrastive learning on a multi-modal PSG dataset comprising of 14K instances from a sleep study conducted at a major US academic hospital dating back to 1999. By capitalizing on EEG, ECG, and respiratory modalities from PSG, _SleepFM_ exhibits superior performance in tasks such as retrieval, sleep stage classification, and apnea event classification, outperforming end-to-end trained CNN models. Additionally, our study highlights the potential of our methodology in scenarios with limited data availability, demonstrating promising results in a few-shot evaluation setting. To our knowledge, this is the first attempt to build and evaluate a foundation model for sleep.

## Method

### Datasets and Preprocessing

Our dataset encompasses PSG records from a US Sleep Clinic dating back to 1999. Comprising 14,068 recordings, this dataset features diverse waveforms, such as EEG, ECG, and EOG, collected over 8 hours per individual. All the

[MISSING_PAGE_FAIL:2]

### Model Training

Our model pretraining, involves contrastive learning optimization with stochastic gradient descent (SGD) using a momentum of 0.9 and an initial learning rate set to 1e-2. We use cross-entropy as our loss function. Training spans 20 epochs with early stopping based on validation loss, employing a batch size of 32. Hyperparameters draw from similar models in prior literature .

Upon pretraining completion, we generate embeddings for the train, validation, and test sets, utilizing the learned modality encoders. These training embeddings drive the training of a logistic regression classifier. The classifier's performance undergoes evaluation on the test set for both sleep stage and apnea detection tasks. For comparison, we define a baseline EfficientNet architecture akin to our pre-trained model encoder but solely trained via supervised learning on the entire (pretraining + training) dataset for classification tasks. This model is trained end-to-end from scratch using cross-entropy loss between predicted and true labels, optimized by SGD with a step decay learning rate schedule. Mirroring the pretraining phase, this model undergoes training for 20 epochs with a batch size of 32, aligning hyperparameters with our model pretraining strategy. All model training was executed on a single NVIDIA Tesla V100S GPU with 32GB of memory. Pretraining each epoch consumed approximately 4 hours, while baseline supervised training required roughly 2 hours on the same GPU.

## Experiments

### Retrieval Analysis

We assessed our model's capabilities by retrieving one modality's closest embeddings from the test set based on another modality's embeddings. Computing cosine similarity between ECG and EEG embeddings generated a ranked list, allowing us to gauge retrieval performance. Evaluation was measured using recall@10 and median rank metrics. **Recall@10**: Measures the true paired item's appearance within the top 10 recommendations. Higher values indicate more accurate retrieval. **Median rank**: Determines the median position of the true paired item in rankings; a lower median rank signifies a more consistent ranking of the correct pair. We assessed the retrieval performance using 90,000 randomly selected 30-second clips encompassing all modalities from the test set. The baseline Recall@10 performance stands at \(10/90000=0.0001\).

### Downstream Classification Tasks

We used the embeddings learned by the three models to train a logistic regression model. This model was employed to classify sleep stages and apnea events, and evaluation was performed on a held-out test dataset. Sleep stage classification is a multi-class classification task, with 5 classes: Wake, Stage 1, Stage 2, Stage 3, and REM. Apnea classification is a binary classification task. We compared our model's performance with baseline model, trained on all three modalities, for sleep stage and apnea event classification. Our evaluation relied on two primary metrics: AUROC (Area Under the Receiver Operating Characteristic curve) and AUPRC (Area Under the Precision-Recall Curve).

### Few-Shot Evaluation

We performed few-shot evaluation by steadily increase the number of participants \(k\) that each model sees from \(k=1\) to the full training dataset, and record the model's AUROC and AUPRC at each \(k\). Note that each patient contributes multiple training clips. We consider values of \(k\{1,2,4,8,16,32,64,128,1265\}\), where 1265 is the size of the full training set. For supervised CNN, few-shot examples are the only training examples seen by the model. For the pretrained models, we use embeddings of these few-shot examples to train a logistic regression model.

## Results

### Retrieval Analysis

Retrieval evaluation exhibited significant improvement compared to baseline metrics. Our model achieved over 500x-7000x higher recall@10 than the baseline as shown in Tables 2 and 1. Pairwise contrastive learning yield better overall retrieval performance than leave-one-out, most likely because the retrieval evaluation directly maps the training procedure of pairwise. One observable trend across both retrieval evaluation is relatively lower retrieval performance between Respiratory and other modalities, specially, Respiratory and EEG. The discrepancy in retrieval performance between EEG-Respiratory signals compared to EEG-ECG or ECG-Respiratory pairs might stem from the closer similarity and shared electrical nature between EEG and ECG signals. Both EEG and ECG capture electrical activities within the body, potentially resulting in more recognizable patterns and facilitating better correspondence.

    &  &  \\   & ECG & Resp & EEG & ECG & Resp & EEG \\  ECG & - & 2 & 1 & - & 0.81 & 0.74 \\ Resp & 2 & - & 5 & 0.82 & - & 0.60 \\ EEG & 1 & 6 & - & 0.82 & 0.58 & - \\   

Table 1: Retrieval on the test set for model trained with pairwise contrastive learning. Resp is for Respiratory. Random baseline for Recall@10 = 0.0001

    &  &  \\   & ECG & Resp & EEG & ECG & Resp & EEG \\  ECG & - & 19 & 7 & - & 0.39 & 0.58 \\ Resp & 21 & - & 400 & 0.38 & - & 0.05 \\ EEG & 13 & 416 & - & 0.46 & 0.05 & - \\   

Table 2: Retrieval on the test set for model trained with leave-one-out contrastive learning. Resp is for Respiratory. Random baseline for Recall@10 = 0.0001

### Downstream Classification Tasks

We focused on two relevant sleep study tasks: sleep stage and apnea classification as shown in Table 3. Notably, across all metrics, the logistic regression model trained using representations from our pretrained model outperforms the end-to-end trained CNN. This superiority holds true across all sleep stage classes as well as on aggregated class metrics. Model pretrained with leave-one-out contrastive learning performs better than the one pretrained with pairwise contrastive learning. Similarly, the apnea classification metrics, displayed in Table 4, underscore our approach's superiority over supervised CNN models. Here as well, the model pretrained with leave-one-out contrastive learning significantly outperforms the model pretrained with pairwise.

### Few-Shot Evaluation

The results for our few shot analysis is presented in Figure 2. We observe that across all the few shot settings, our model significantly outperforms baseline supervised CNN model for both sleep stage and apnea classification. Notably, the leave-one-out model significantly outperforms pairwise model across all shots, specially for apnea classification.

## Conclusion

Our study utilizes multi-modal PSG data and representation learning to improve identification of sleep events, advancing sleep medicine. The primary contributions involve developing and evaluating _SleepFM_, a multi-modal contrastive learning model, on a 14K PSG recordings. _SleepFM_ exhibited strong performance across retrieval, sleep stage, and apnea classification, surpassing supervised CNNs. The methodology centers on two contrastive learning approaches, leave-one-out and pairwise, which both effectively unified ECG, EEG, and respiratory signal representations and demonstrated efficacy in limited data scenarios. For retrieval, pairwise contrastive learning outperformed leave-one-out. For all downstream tasks, leave-one-out significantly outperforms pairwise.

**Limitations.** We primarily trained and evaluated on one

    & **AUROC** & **AUPRC** \\ 
**Leave-One-Out CL** & \(}\) & \(}\) \\
**Pairwise CL** & \(0.902_{.003}\) & \(0.586_{.007}\) \\
**Supervised CNN** & \(0.843_{.002}\) & \(0.555_{.005}\) \\   

Table 4: Apnea classification metrics. Baseline here is a supervised CNN trained on the entire (pretraining + training) dataset to classify apnea. The leave-one-out and pairwise models are logistic regression models trained on the embeddings generated from only the training dataset. Prevalence of apnea event is 0.017. \(\) represents 95% CI.

Figure 2: Sleep apnea classification. The x-axis represents number of participants that the model was trained on. In case of pairwise and leave-one-out, we select embeddings from \(k\) number of participants to train a logistic regression model. In case of supervised CNN, we train the model end-to-end on \(k\) number of participants to classify either sleep stages or apnea. Testing is done on the entire test set. For each shot, we average the performance across 3 replicates.

dataset, thus model generalizability to other datasets is unknown. Testing on diverse datasets from different sleep clinics and demographics is crucial for validating robustness across populations. Additionally, while we focused on sleep stage and apnea detection, exploring other tasks like arousal detection, periodic leg movements, and narcolepsy could provide a more comprehensive clinical assessment.