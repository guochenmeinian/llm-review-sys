# Reinforcement Learning for Locally Checkable Labeling Problems

Anonymous submission

###### Abstract

We address the challenge of solving locally checkable labeling (LCL) problems on graphs using machine learning. Unlike prior supervised approaches that depend on ground-truth algorithms or enforce unique solutions, we propose a reinforcement learning framework that requires only verifiers to evaluate correctness. This formulation allows models to learn solution strategies independently, without bias toward specific algorithmic procedures, and inherently supports the discovery of non-unique solutions. We evaluate our method on four fundamental LCL problems, demonstrating its ability to generalize effectively, outperform supervised baselines, and provide a versatile foundation for learning algorithmic reasoning on graphs.

## Introduction

Graph algorithms play a central role in solving problems across diverse domains, including network optimization, resource management, and data organization . Many such algorithms are designed to assign discrete labels to graph elements, such as nodes or edges, based on well-defined rules. Examples include finding maximal independent sets, minimal vertex covers, or edge colorings. A particularly intriguing subset of these tasks is locally checkable labeling (LCL) problems, where solutions can be verified using localized checks on small subgraphs.

The algorithms used to solve these problems typically operate through step-wise procedures involving discrete state transitions. For instance, Luby's algorithm  iteratively builds a maximal independent set by selecting nodes based on local rules and updating their states. Learning to replicate such algorithmic behavior poses significant challenges, as it requires bridging the gap between the continuous representations of machine learning models and the discrete nature of algorithmic solutions. Recent work, such as GraphFSA  or Discrete Neural Algorithmic Reasoning , have addressed this challenge by incentivizing discrete transitions during the training of a continuous system. However, these methods often struggle with more complex problems or rely on access to ground-truth labels and solutions, which limits their scalability and applicability.

In this work, we propose a reinforcement learning (RL) framework for solving LCL problems in a multi-agent setting. In our approach, each agent, representing a node or edge, learns decision-making policies based solely on local observations. They are trained through problem-specific verifiers that evaluate solutions on localized neighborhoods for correctness. This verifier-driven approach removes the need for ground-truth labels or pre-defined algorithms, allowing the model to discover solutions independently of specific solving strategies. Additionally, unlike supervised learning methods, which often require unique solutions or external symmetry-breaking mechanisms, our framework naturally handles problems with multiple valid solutions. This flexibility broadens its applicability to a wider range of graph problems.

The paper introduces this RL-based framework and provides a practical implementation tailored to LCL problems. We evaluate the method on several fundamental graph problems, including maximal independent set and minimal vertex cover, and demonstrate its ability to generalize effectively across problem instances. Our experimental results showcase its potential to learn

Figure 1: Finding a maximal independent set is locally checkable. A verifier can validate both conditions — the solution set has to be independent and maximal — for each node and its neighborhood. In this visualization, dark nodes represent nodes that are part of the solution set. Executing the verifier on the blue highlighted node entails checking all incident nodes and adjacent edges within the blue area. On the left, the conditions are met, while this is not the case on the right.

ior in graph-based tasks. These findings suggest that RL, coupled with local verifiers, offers a promising direction for addressing algorithmic graph problems with broader implications for learning and reasoning in discrete domains.

## Preliminaries

We present the key concepts required to understand our approach, with additional preliminaries and related work provided in the Appendix.

### The GraphFSA Framework

In GraphFSA, each node executes a finite state automaton operating over a discrete set of states. More formally, the GraphFSA \(\) consists of a tuple \((,,,)\). \(\) is applied to a graph \(G=(V,E)\) and consists of a set of states \(\), an aggregation \(\) and a transition function \(\). At time \(t\), each node \(v V\) is in state \(s_{v,t}\). In its most general form, the aggregation \(\) maps the multiset of neighboring states to an aggregated value \(a\) of a finite domain.

\[a_{v,t}=(\{\{s_{u,t} u N(v)\}\})\]

Here \(\{\{\}\}\) denotes the multiset and \(N(v)\) the neighbors of \(v\) in \(G\). At each timestep \(t\), the transition function \(:\) takes the state of a node \(s_{v,t}\) and its corresponding aggregated value \(a_{v,t}\) and computes the state for the next timestep \(s_{v,t+1}=(s_{v,t},a_{v,t})\). Note that \(\) is modeled to be a finite domain.

Aggregation functions.The transition value \(a\) for node \(v\) at time \(t\) is directly determined by aggregating the multi-set of states from all neighboring nodes at time \(t\). The aggregation \(\) specifies how the aggregated value is computed from this neighborhood information. Note that this formulation of the aggregation \(\) allows for a general framework in which many different design choices can be made for a concrete class of GraphFSAs.

Starting and final states.The FSA uses starting states \(S\) to encode the discrete set of inputs to the graph problem. Final states \(F\) are used to represent the output of a problem. In node prediction tasks, we choose one final state per class. Opposed to other states, it is not possible to transition away from a final state, meaning that once such a state is reached, it will never change.

### Locally Checkable Labeling Problems

A graph problem is locally checkable if the correctness of a solution can be verified within all local neighborhoods. Locally checkable labeling (LCL) problems are typically node-centric and defined for graphs with bounded maximum degree . However, we relax this restriction to accommodate graphs with arbitrary degrees.

Formally, an LCL problem is a tuple \((_{},_{},r,)\), where \(_{}\) and \(_{}\) are finite input and output label sets to make the graph attributed, \(r\) is a constant radius, and \(\) is a set of allowed \(r\)-hop neighborhoods. A solution is correct if every \(r\)-hop neighborhood matches a graph in \(\) under isomorphism. For edge-centric tasks, labels and neighborhoods are defined around edges rather than nodes. A verifier algorithm checks the local neighborhood and outputs Yes if a neighborhood matches an element of \(\), and No otherwise. To accept a proposed solution, all neighborhoods have to output Yes. Common LCL problems include finding maximal independent sets, vertex or edge colorings, and maximal matchings. An example of such a verifier is depicted in Figure 1.

## Problem Formulation

We follow the framework introduced by GraphFSA . We focus on problems defined on a graph structure \(G=(V,E)\) that can be solved using a finite set of states \(Q\) and \(q_{v}\) denotes the state of node \(v\). Further, we will concentrate on tasks that belong to the class of locally checkable labeling problems.

In contrast to the original formulation of GraphFSA, we relax the condition for the transition function. Namely, the aggregation function, which determines how \(\) behaves, no longer has to be explicitly discretized to a finite domain. Instead, \(\) directly maps a node's current state, and its neighbors' states to the node's next state. That is, the state gets updated according to

\[q^{v}(q^{v},\{\{q^{u} u N(v)\}\})\]

This formulation does not explicitly define the aggregation function, such as thresholded counting, and therefore allows for greater generality. In particular, it enables the incorporation of additional continuous inputs, such as random bits, into the computation of \(\), which can facilitate tie-breaking. This flexibility is critical for handling input graphs that admit multiple valid solutions or require symmetry-breaking to resolve ambiguities, as illustrated in Figure 3. We maintain the concept of terminal states--states from which no further transitions are possible. Once all nodes

Figure 2: The proposed architecture consists of encoding MLPs, a convolution block, and decoding MLPs. Inside the convolution block, the cell values are first merged with extracted input embedding. Then a single node convolution layer performs neighborhood exchanges to propagate information without violating locality constraints. Before node and edge embeddings can be decoded to logits, the two endpoints’ processed node state representations are aggregated across each edge.

reach terminal states, the algorithm halts, ensuring a well-defined termination condition.

## Model Architecture

We introduce Verifier-based Algorithmic Reasoning using Reinforcement Learning (VARL) approach that uses a Graph Neural Network (GNN) to learn the actor policies that follows an encode-process-decode paradigm. At each timestep \(t\), we are given the graph \(G=(V,E)\), cell values, all node and edge states. For each node \(w V\) and each edge \(\{u,v\} E\) our network outputs action logits to be taken to transition to timestep \(t+1\). A schematic representation of the architecture can be seen in Figure 2.

First, two Multi-Layer Perceptrons (MLP) encode the one-hot encoding of the node states \(_{w}\) and edge states \(_{u,v}\) into a \(d\)-dimensional embedding \(h\).

\[h_{w}=_{_{1}}(_{w}) h_{u,v}=_{_{1}}(_{u,v})\]

The processing is done with a convolution block that first combines node and edge embedding together with their respective cell values into \(h^{}\).

\[h^{}_{w}=_{_{2}}(h_{w}||c^{w}) h^{}_{u,v}=_{_{2}}(h_{u,v}||c^{u,v})\]

Afterwards, it locally propagates the information among node and edge neighborhoods using a message-passing layer to derive the updated node and edge embedding \(h^{}\). Any node convolution layer fits here. But as a default setting, we use a slightly modified Graph Isomorphism Network (GIN)  layer, including edge features to construct the node update. Furthermore, we use max aggregation, which was shown to be beneficial in algorithmic settings :

\[h^{}_{w}=_{_{4}}((1+) h^{ }_{w}+_{w^{} N(w)}_{_{3}}(h^{}_ {w}||h^{}_{w^{}}||h^{}_{w,w^{}}))\]

To update the state of an edge, we incorporate its current state along with the updated states of the two nodes it connects.

\[h^{}_{u,v}=_{_{3}}(h^{}_{u,v}||h^{ }_{u}||h^{}_{v})\]

To ensure that the edge remains agnostic to the edge direction and preserve symmetry in undirected edge problems an additional aggregation step is performed:

\[h^{}_{u,v}=\{_{_{3}}(h^{}_{u,v} ||h^{}_{u}||h^{}_{v}),_{_{3}}(h^{ }_{v,u}||h^{}_{v}||h^{}_{u})\}\]

Finally, the decoder consists of two MLPs that transform the \(d\)-dimensional embeddings into \(|Q|\) node logits, and \(|P|\)-sized edge logits from which the next states can be sampled:

\[l_{w}=_{_{5}}(h^{}_{w}) l_{u,v}=_{_{4}}(h^{}_{u,v})\]

We use the same architecture for all LCL problems that we consider in the following.

## Experiments

To test our proposed approach across four well-known LCL problems: Maximal Independent Set (MIS), Minimal Vertex Cover (MVC), Maximal Matching (MM), and Minimal Edge Cover (MEC). Both MIS and MVC are node tasks, whereas MM and MEC are edge-centric. For all of them there exists a local verifier that accepts or rejects a proposed solution. For a formal definition of the problems, we refer to the Appendix.

### Supervised Baselines

To critically assess our VARL approach, we compare it to supervised learning baselines. RL is better suited for problems with multiple valid solutions verified by a local checker. Supervised learning, typically requires a unique label for each instance but can be adapted to handle non-unique cases. We utilize a supervised approach inspired by Luby's algorithm , which constructs solutions iteratively by selecting nodes or edges based on local properties. At each timestep, the algorithm is given access to a set of random bits. We adapt the selection and invalidation mechanisms for our tasks to match their specific constraints. We consider two different selection strategies:

* Guided Strategy: The solution is constructed by selecting locally maximal elements from the given random bits. The constructed solution uses the exact same set of random bits that is given during training.
* Unguided Strategy: The solution is again constructed by selecting locally maximal elements from random bits. However, the specific random bits are hidden from the model during training. The model still has access to different random bits during training for tie breaking, however, they are independent of the solution. This setup is closer to what we desire to achieve with RL and should encourage independence from specific solving strategies, allowing the model to learn to solve the problem rather than imitating a given algorithm.

Figure 3: The solution for problems we consider does not need to be unique. For example, for MIS, the same input graph gives rise to two different solutions displayed above. Moreover, while for the two graphs on the left, one could choose one solution over the other based on the graph topology, the two examples on the right are completely symmetrical and, as such, require additional means of symmetry breaking to assign different output labels to nodes in the same orbit. This problem remains even when the GNN is fully expressive under 1-WL.

The invalidation step ensures solution constraints are satisfied. Using the described strategies, we generate labeled instances for supervised learning datasets formulating the problem as a classification task. Note that there are no specific input features for the nodes as the problems are defined on the graph topology. However, during the execution, the models are given access to random bits. We use different common GNN architectures such as GIN , GAT , SAGE , PGN  and Gated GCN . In order to run them on graphs of variable sizes we set the number of convolutions proportional to the graph size. Furthermore, we also incorporate a memory cell to feed in the random bits and we make the architectures recurrent so that the different convolution layers share the same set of weights.

### Results

We evaluate the discussed baseline models on the four LCL problems and train them on graphs of size 16. In Table 1 we report the agent level accuracy, which indicates the percentage of nodes which the verifier outputs Yes and the graph level accuracy, the number of correctly solved instances.

We can observe large differences between the baselines. Using labels computed using the unguided strategy, the approach designed to learn to solve the problem rather, is more challenging than the maximum selection, which corresponds to learning a specific algorithm. This indicates that a supervised approach struggles to learn the more general underlying concepts that define solutions from the more general dataset. The used graph convolution layer is also of importance: GAT and SAGE are poor choices, and so is PGN when edge states are central to the problem at hand. SAGE does not consider edge features and uses mean aggregation during message passing, which we found to perform worse when trying different designs for the default modified GIN version. PGN's stark difference between node and edge task is somewhat surprising as edge features are propagated together with node features during message passing. The main difference to our modified GIN concerning message construction is that node and edge features are added together. GIN performs the best across the board. Architectures using gGCN show partial success on edge tasks but perform worse than PGN on node-centric problems.

We test two different versions of our proposed method that is trained using RL. The first variant uses only a few states -- if it is used on a node task, \(|Q|=5\) and \(|P|=1\), and on edge tasks \(|Q|=1\) and \(|P|=5\). The second variant, uses 10 and 5 states instead respectively. Both variants are able to achieve very good performance across all tasks with only marginal difference between them, although going with fewer states seems to be slightly better. However, the difference with respect to the supervised baselines is much more significant, especially when considering the number of correctly solved instances. Our proposed method using RL clearly outperforms both supervised strategies, even though it only access to a verifier and has no access to labels.

## Conclusions

Learning correct algorithms purely from data driven feedback is very challenging, especially when no intermediate trajectories by a ground truth mechanism are given or the solution to a given problem instance is not unique. We propose to address these gaps by teaching machines algorithmic thinking through reinforcement learning (RL) for graph problems. We extend the state-based formulation of Graph-FSA to fit within a multi-agent RL framework, where agents on graph nodes observe local states and perform transitions. This has the advantage of generalizing the state updates and also incorporating random bits for necessary tie-breaking. Policies trained via policy-gradient methods are translated into transition functions, modeling the learned algorithm's behavior.

Experiments demonstrate the applicability of our approach to locally checkable problems (LCLs) like maximal independent sets and matchings. Unlike supervised methods that require input-output pairs and struggle to effectively learn these tasks, our verifier-based RL approach learns the underlying problem dynamics and can handles non-unique solutions effectively. We thus validate the feasibility of the proposed RL approaches for learning solvers for algorithmic problems, laying a foundation for further research in algorithmic learning on graphs.

  & & **MIS** & **MVC** & **MAT** & **MEC** \\   & GIN & \(100.0\,( 0.1)\) & \(100.0\,( 0.1)\) & \(63.2\,( 29.6)\) & \(74.1\,( 10.9)\) \\  & GAT & \(1.7\,( 1.1)\) & \(3.1\,( 3.1)\) & \(11.6\,( 10.0)\) & \(0.5\,( 0.4)\) \\  & SAGE & \(8.0\,( 1.4)\) & \(8.7\,( 2.0)\) & \(0.0\,( 0.1)\) & \(0.1\,( 0.1)\) \\  & PGN & \(100.0\,( 0.0)\) & \(100.0\,( 0.0)\) & \(0.0\,( 0.1)\) & \(0.0\,( 0.1)\) \\  & gGCN & \(96.1\,( 1.1)\) & \(97.4\,( 0.8)\) & \(38.9\,( 35.2)\) & \(27.6\,( 5.7)\) \\   & GIN & \(28.3\,( 1.4)\) & \(25.4\,( 4.3)\) & \(0.0\,( 0.0)\) & \(0.0\,( 0.1)\) \\  & GAT & \(1.4\,( 0.7)\) & \(1.1\,( 0.1)\) & \(0.0\,( 0.0)\) & \(0.0\,( 0.1)\) \\  & SAGE & \(0.9\,( 0.2)\) & \(1.0\,( 0.4)\) & \(0.0\,( 0.0)\) & \(0.0\,( 0.1)\) \\  & PGN & \(25.1\,( 4.0)\) & \(26.1\,( 29.0)\) & \(0.0\,( 0.1)\) & \(0.0\,( 0.0)\) \\  & gGCN & \(22.9\,( 2.6)\) & \(20.7\,( 4.8)\) & \(0.1\,( 0.1)\) & \(0.1\,( 0.1)\) \\   & large & \(100.0\,( 0.0)\) & \(100.0\,( 0.0)\) & \(99.0\,( 0.4)\) & \(98.4\,( 0.9)\) \\  & small & \(100.0\,( 0.0)\) & \(100.0\,( 0.0)\) & \(99.4\,( 0.3)\) & \(99.2\,( 0.5)\) \\   
  & & **MIS** & **MVC** & **MAT** & **MEC** \\   & GIN & \(100.0\,( 0.0)\) & \(100.0\,( 0.0)\) & \(95.6\,( 5.0)\) & \(98.0\,( 1.0)\) \\  & GAT & \(42.8\,( 5.7)\) & \(59.9\,( 14.6)\) & \(83.0\,( 10.0)\) & \(64.6\,( 2.5)\) \\  & SAGE & \(76.6\,( 1.4)\) & \(77.3\,( 0.1)\) & \(66.2\,( 1.0)\) & \(55.2\,( 1.9)\) \\  & gGCN & \(99.4\,( 0.2)\) & \(99.6\,( 0.1)\) & \(92.1\,( 0.5)\) & \(90.1\,( 2.2)\) \\   & GIN & \(86.8\,( 0.6)\) & \(85.8\,( 1.8)\) & \(83.6\,( 2.2)\) & \(46.2\,( 1.7)\) \\  & GAT & \(41.0\,( 2.8)\) & \(42.2\,( 4.7)\) & \(58.1\,( 2.7)\) & \(46.2\,( 3.4)\) \\  & SAGE & \(64.2\,( 1.0)\) & \(63.8\,( 1.1)\) & \(57.0\,( 0.4)\) & \(46.1\,( 2.4)\) \\  & PGN & \(85.2\,( 1.2)\) & \(86.1\,( 1.1)\) & \(61.5\,( 2.4)\) & \(46.6\,( 1.3)\) \\  & gGCN & \(83.6\,( 1.6)\) & \(83.2\,( 1.9)\) & \(66.0\,( 1.8)\) & \(53.7\,( 1.4)\) \\   & large & \(100.0\,( 0.0)\) & \(100.0\,( 0.0)\) & \(99.9\,( 0.0)\) & \(99.9\,( 0.0)\) \\  & small & \(100.0\,( 0.0)\) & \(100.0\,( 0.0)\) & \(100.0\,( 0.0)\) & \(100.0\,( 0.0)\) \\  

Table 1: Our RL-based approach outperforms alternative supervised baselines. We show graph accuracy on top in percentages, and agent accuracy on the bottom. The shown numbers represent mean and sample standard deviation in parenthesis. Rows indicate different methods: supervised baselines have access to labels computed through guided (guided) or unguided (unguided) selection strategy. The reinforcement learning approach is denoted with RL, and we show results from agents with only access to a few states (small), five central and one non-central state, or ones with ten central and five non-central states (large).