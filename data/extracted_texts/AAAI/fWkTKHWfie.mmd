# Fine-Tuning Language Models Using Formal Methods Feedback

Yunhao Yang\({}^{*}\)1, Neel P. Bhatt\({}^{*}\)1, Tyler Ingebrand\({}^{*}\)1, William Ward\({}^{1}\), Steven Carr\({}^{1}\), Zhangyang Wang\({}^{1}\), Ufuk Topcu\({}^{1}\)

\({}^{1}\)The University of Texas at Austin, USA

{yunhaoyang234, npbhatt, tyleringebrand, stevencarr, atlaswang, utopcu}@utexas.edu

These authors contributed equally.

###### Abstract

Although pre-trained language models encode generic knowledge that is beneficial for planning and control, they may fail to generate appropriate control policies for domain-specific tasks. Existing fine-tuning methods use human feedback to address this limitation. However, sourcing human feedback is labor-intensive and costly. We present a fully automated approach to fine-tune pre-trained language models for domain-specific applications, bridging the gap between generic knowledge and domain-specific requirements while reducing cost. The method synthesizes automaton-based controllers from pre-trained models guided by natural language task descriptions. These controllers are verifiable against independently provided specifications within a world model, which can be abstract or obtained from a high-fidelity simulator. Controllers with high compliance with the desired specifications receive higher ranks, guiding the iterative fine-tuning process. We provide quantitative evidence, primarily in autonomous driving, to demonstrate the method's effectiveness across multiple tasks. The results indicate an improvement in the percentage of specifications satisfied by the controller from 60% to 90%.

## Introduction

Pre-trained language models encode rich world knowledge that is useful for planning and control. Recent works use pre-trained models to synthesize control policies for tasks such as autonomous driving , surgical robotics , and aircraft operation . The control policies yield high-level actions that an agent should take in order to satisfy objectives specified via natural language prompts.

However, in specific domains, pre-trained models may fail to generate appropriate control policies. For instance, an autonomous driving system may require knowledge about traffic rules and conventions specific to a given country. Such specific rules and conventions may be beyond the knowledge encoded in the pre-trained model.

To address this shortcoming, several works use human feedback for fine-tuning pre-trained models and to incorporate required domain knowledge . Human feedback evaluates the extent to which the output of a pre-trained model aligns with the desired objectives. For example, the provision of a binary ranking of like or dislike for each model output can act as a feedback source. This feedback from human expertise enables fine-tuning of the pre-trained model and allows implicit incorporation of domain-specific knowledge. However, obtaining feedback from humans is labor-intensive and costly.

We investigate how similar feedback can be automatically obtained using artifacts from formal methods. Suppose we have a world model, which is either abstract or obtained from a high-fidelity simulator, and a set of specifications. We can verify, either formally or empirically, if a controller generated by the language model meets the specifications . The measure of compliance can act as a source of feedback for fine-tuning, similar to human feedback. Since this procedure is automated, such feedback is less labor-intensive and cheaper.

We develop a method to fine-tune pre-trained models based on automated feedback using artifacts from formal methods. The proposed method synthesizes an automaton-based controller from the pre-trained model given a natural language task description . Such an automaton-based controller is formally verifiable against independently provided specifications (e.g., a driving rule book ) when implemented in a specific world model. We can obtain the number of specifications satisfied by each controller and use it for ranking. We then iteratively fine-tune the pre-trained model using this ranking as a feedback source.

If the world model is obtained from a high-fidelity simulator rather than an abstract model, we collect trajectories from the simulator. The trajectories are sequences of state-action pairs which can be checked against the provided specifications. A controller satisfying a larger number of specifications when executed in the simulator is assigned a higher rank. We use the obtained ranks for fine-tuning.

To demonstrate the performance of the proposed method, we provide experimental results covering multiple tasks in an autonomous driving system, although applicability is not limited to this domain. The quantitative results indicate a significant improvement in the percentage of specifications satisfied, from 60% to above 90%, confirming that the proposed method can effectively fine-tune the pre-trained model.

## Related Work

Fine-tuning from Human Feedback.Reinforcement learning from human feedback (RLHF) is a preference alignment strategy that learns a reward model from human preferences and then fine-tunes the pre-trained language model using reinforcement learning . In some works, before fine-tuning begins, humans compare the accuracy of multiple responses to a single input and indicate which is preferred, generating a data set of preferences that is used to train a reward function . Other methods optimize the reward function and fine-tune the language model simultaneously. As the model generates outputs, a human indicates which output is preferred, sending new feedback for the reward function to learn, thus impacting the model's accuracy .

Direct preference optimization (DPO) is a preference alignment strategy that implicitly optimizes the same objective as RLHF without explicitly learning a reward model or using reinforcement learning. DPO optimizes model outputs directly from human feedback data using a modified maximum likelihood objective, reducing the number of training stages and improving stability .

However, all of the above works rely on humans to provide feedback on which outputs are preferred. Obtaining an excessive amount of human feedback is labor intensive. In contrast, the method we propose automatically ranks the outputs from language models. Hence we can obtain an unlimited number of data points to fine-tune the language model.

Fine-tuning from Generated OutputsSome methods fine-tune a language model using the outputs of another model. For example, a language model can learn how to generate common sense phrases  or output chain-of-thought reasoning  using responses from a model that already exhibits the desired behavior. Other methods train a language model using the model's own outputs by identifying high-quality statements and feeding them back into the model as examples of correct responses . One approach combines both methods, first fine-tuning using the outputs of a separate pre-trained model, and then fine-tuning again using the model's own filtered outputs . Another strategy is to modify the backpropagation process so that only certain parameters are updated .

These methods are not capable of fine-tuning domain-specific language models since all the generated outputs from itself or other models lack domain-specific knowledge as well. In contrast, the method we proposed can fine-tune the language model to satisfy domain-specific requirements.

Formal Methods and Verification on Language Models.Existing works convert natural language to formal language, which can be used for verification . Recent works show that language models can be trained to convert natural language to formal language, with applications in representing mathematics, generating proofs, and creating assurance cases . One method is to design input prompts that include task-specific information (e.g., definitions, response templates, and detailed examples) that enable a language model to divide a goal into individual steps and develop formal constraints on the system . Other methods iteratively refine the input prompt to the language model based on counter-examples until the outputs pass formal verification . Although these works utilize formal methods, there are still humans in the loop, while our proposed method aims to be fully automated without any human intervention.

## Preliminaries

Automaton-Based Model for System or Environment.A model is an abstract representation that encodes the static and dynamic information of a system or an environment. We use a transition system to build the model.

A transition system \(_{},Q_{},_{ },_{}\) consists of a set of output symbols \(_{}\), a set of states \(Q_{}\), a nondeterministic transition function \(Q_{} Q_{}\{0,1\}\), and a labeling function \(_{}:Q_{}_{}\).

We introduce a set of atomic proposition \(P\) such that \(_{} 2^{P}\), i.e., a symbol \(_{}\) is the set of atomic propositions in \(P\) that evaluate to _True_. Each symbol \(\) captures the system or environment behavior. We present an example in Figure 1. We will leverage the fact that automaton-based structures are formally verifiable in the proposed method.

Automaton-Based Controller.A controller is a system component responsible for making decisions and taking actions based on the system's state. A controller can be mathematically represented as a mapping from the system's current state to an action, which is executable in the task environment. We use a _finite state automaton_ (FSA) to build a controller for a sequential decision-making task.

A FSA is a tuple \(=,A,Q,q_{0},\) where \(\) and \(A\) are the sets of input and output symbols, \(q_{0} Q\) is the initial state, and \(:Q A Q\{0,1\}\) is a non-deterministic transition function. The transition function is a membership function--a transition exists when it evaluates to \(1\).

Each input symbol \(\) is composed of the atomic propositions from \(P\), which is the set of atomic propositions we introduced for the model. We introduce another set of atomic propositions \(P_{A}\) for the output alphabets \(A 2^{P_{A}}\). We also allow for a "no operation/empty" symbol \( A\). Note that the input symbols comprise all possible dynamics of the environment or system in which the controller operates,

Figure 1: Examples of an automaton-based model (top) and a controller (bottom).

and the output symbols comprise all the actions allowed by the controller. See Figure 1 for an example.

### Fine-Tuning Pre-trained Language Models for Autonomous Systems

We develop a method for fine-tuning pre-trained language models for specific control tasks, named _direct preference optimization via automated feedback_ (**DPO-AF**). The method first obtains human-provided information regarding the autonomous system. It then constructs a model that encodes the information about the system. Next, we query the pre-trained language model on a particular system control task and get multiple responses from the language model via sampling. We construct automata from the responses and apply verification methods to check how many user-provided specifications each automaton satisfies. We rank the responses by the number of satisfied specifications. Last, we send the prompt and ranked responses to the DPO algorithm to fine-tune the language model.

DPO-AF does not require repeated feedback from humans. Therefore, we can obtain an unlimited number of prompt-response pairs until the language model converges.

### Automaton-Based Representation for Natural Language and Autonomous Systems

Modeling the Autonomous System.DPO-AF starts by constructing an automaton-based model encoding the information about the autonomous system. Such information is obtained from external sources such as human experts or system operation manuals. The information includes but is not limited to a set of propositions that describe the system's behaviors and a set of control signals (actions) that can affect the system's states. We encode the set of behaviors in an atomic proposition set \(P\) and the set of actions in an atomic proposition set \(P_{A}\).

Recall that a model consists of a set of states, a set of symbols, a transition function, and a label function. As we defined \(P\) and \(P_{A}\), we build \(2^{|P|}\) states whose label is \( 2^{P}\) respectively. \(|P|\) is the number of propositions in \(P\). Next, for every two states \(p_{i}\) and \(p_{j}\), we check whether the system supports the transition between the label of \(p_{i}\) and the label of \(p_{j}\). If the system supports such a transition, we add it into the transition function.

Finally, we remove the states with no incoming and outgoing transitions. However, from a conservative perspective, we can build transitions for every pair of states and not remove any states. The conservative approach can avoid potential missing transitions but will significantly increase the computation cost for formal verification.

To illustrate the procedure, suppose there is a traffic light system operating in the order of red-green-yellow-red. We have the proposition set \(P=\{green,yellow,red\}\) and transitions (green to red), (red to yellow), and (yellow to green). Hence, we only keep three states with labels \(green,yellow,red\) respectively and remove all the states with other labels (e.g., \(green yellow\)).

Task Prompt Engineering.Prior to fine-tuning the language model, we collect a prompt dataset. The prompt dataset consists of the queries on the control tasks that operate in the autonomous system.

Then, we define a prompt engineering procedure to extract relative task knowledge from the language model. For each prompt in the prompt dataset, we first use the following format to obtain the responses from the language model:

```
1Definethestepsfortaskdescription
21.steponedescription
32.steptwodescription
4...
```

Blue texts and red texts indicate the input prompt and the language model's responses, respectively. This format forces the outputs to be a list of step descriptions for the task described in the input prompt.

Once we get the responses, we query the language model again to map the step descriptions into the set of defined

Figure 3: This diagram depicts the method of ranking responses by formal verification of the induced automata. We present the sample automata in Figures 5 and 7.

Figure 2: The overall pipeline of fine-tuning a language model via automated feedback. We mark the inputs to the pipeline in purple and the output in blue.

atomic propositions \(P\):

```
AlignthefollowingstepstoalignthesetofBooleanpropositions(prop1,...,propn)andactions(act1,...,actm):1.steponedescription2.steptwodescription4...5
6l.alignedsteponedescription
```

We rephrase the step description so that the propositions and actions are consistent with the model. Therefore, we avoid failing the verification process due to language ambiguity, i.e., different phrases with the same meaning.

Note that DPO-AF also aims to fine-tune the language model to output steps that can be easily aligned to the propositions and actions. Therefore, the expected responses from the fine-tuned language model should have the following properties: 1. The language model can easily and correctly align the textual step descriptions to the given propositions and actions. 2. The aligned step descriptions satisfy the user-provided specifications.

To check the second property, we need to construct an automaton-based controller from the textual step descriptions. Then, we implement the controller in the model and verify it against the specifications.

Controller Construction.We follow the method GLM2FSA  to construct an FSA-based controller to encode the textual step descriptions. Specifically, we start from the aligned textual step descriptions and apply semantic parsing to break the steps into a list of verb phrases. Recall that a controller consists of a set of states \(Q\), an initial state \(q_{0}\), input symbols \(\), output symbols \(A\), and a transition function \(\). We use the verb phrases to define the input and output symbols according to the grammar from GLM2FSA. Then, we build one state corresponding to each step, with the state corresponding to the first step as the initial state. Last, we follow the GLM2FSA algorithm to build the transition rules. We present a step-by-step illustrative example in Section.

### Automated Feedback

Given a set of specifications, we provide two ways of checking whether the controllers constructed from the language model's outputs satisfy each specification. For each output, the method generates feedback consisting of the number or percentage of specifications being satisfied.

Formal Verification.Formal verification requires an automaton-based model, an automaton-based controller, and a set of logical specifications. So far, we have constructed the model and the controller. The specifications include the expectation of task achievement or safety requirements, represented in temporal logic (e.g., linear temporal logic ). The temporal logic specifications are logic formulas over propositions \(P P_{A}\). We describe it in detail in the Appendix. These specifications are either provided by the task designer or extracted from existing rule books.

In the verification procedure, we first implement the controller in the model. Mathematically, we define a product automaton \(=\) describing the interactions of the controller \(\) with the model \(\), i.e., how the controller's actions change the model's states and how the model's states affect the controller's decision-making on its next action. Note that the verification procedure implicitly assumes that all the actions can be successfully operated and hence lead to the corresponding states of the controller and the model.

We run a model checker (e.g., NuSMV ) to verify if the product automaton satisfies each specification,

\[. \]

We verify the product automaton against each specification for all the possible initial states. If the verification fails, the model checker returns a counter-example. The counter-example is a trace--a sequence of states--violates the specifications. The NuSMV model checker returns the sequence of states from the product automaton along with the output symbols. Mathematically, the traces are in a format of \((p_{1},q_{1},c_{2} a_{1}),(p_{2},q_{2},c_{2} a_{2}),...\) where \(p_{i} Q_{},q_{i} Q,c_{i}=_{}(p_{i}),a_{i} A\) such that \((q_{i},_{}(p_{i}),a_{i},q_{i+1})=1\).

We present the definitions of temporal logic and product automaton in the Appendix.

Empirical Evaluation.In some scenarios, obtaining models for autonomous systems may be hard. We propose using empirical evaluation to account for the scenarios where models are not present. Empirical evaluation requires an autonomous system \(\), a constructed controller \(\), an atomic proposition set \(P\), a set of actions \(P_{A}\), and a grounding method \(\). Specifically, \(:(2^{P} 2^{P_{A}})^{N}\) operates the controller directly in the system and returns a sequence of propositions and actions describing the operation. \(N\) is the max length of the sequence. The sequence is evaluated as follows:

\[(,)=(2^{P} 2^{P_{A}})^{N}. \]

After evaluating every sequence against the specifications, we get the percentage of sequences, \(_{}\), which satisfy each specification:

\[_{}=}{}.\]

Fine-Tuning via Automated Feedback

Collection of the Language Model's Outputs.Once we select the autonomous system and obtain the model for the system, we can query the model for instructions on tasks that are operable in the system, following the format described in the previous section. Different responses for the same input task can be sampled from the language model. Then, we can rank these responses and fine-tune the language model to output the best response according to the system model.

Ranking the Outputs and Fine-Tuning the Language Model.We apply the automated feedback method for every two responses from the language model associated with the same task prompt to rank the preferences of the two responses. As a result, we obtain a data point \((x,y_{w},y_{l})\), where \(x\) is the input prompt, \(y_{w}\) is the preferred response and \(y_{l}\) is 

[MISSING_PAGE_FAIL:5]

car, car from left, car from right, pedestrian at left, pedestrian at right, pedestrian in front, stop sign }, and the actions are { stop, turn left, turn right, go straight }.

Once we have the models representing the autonomous system and a set of specifications, we can construct controllers for various tasks that can operate in the system and verify whether the controllers satisfy the specifications when operating in the system.

Example on Controller Construction: Right Turn.In this example, we use a simple task "turn right at the traffic light" to demonstrate the controller construction and verification. Then, we compare the controllers constructed using the language model prior to and after the fine-tuning stage. Hence we can give a straightforward demonstration of how DPO-AF improves the language model.

We start by constructing a controller using the language model before fine-tuning it. First, we query the language model on the steps of turning right at the traffic light in the following format:

```
1Stepsfor"turnrightattrafficlight"
21.Lookstraightaheadandwatchfortrafficlight.
32.Ifthetrafficlightturnsgreen,startmovingforward.
43.Asyouapproachtheintersection,looktoyourleftforoncomingtraffic.
54.Ifthereisnottrafficcomingfromyourleft,checkpedestriansonyourright.
65.Ifitissafe,turnyourvehicleright.
```

Next, we again query the language model to align the step instructions to the defined propositions and actions:

```
1RephrasethefollowingstepstoalignthedefinedBooleanPropositions(greentrafficlight,carfromleft,......)andActions(stop,turnleft,turnright,gostraight):
21.Lookstraightaheadandwatchfortrafficlight.
3......
45.Ifitissafe,turnyourvehicleright.
56
57
681.Observethestateofthegreentrafficlight.
72.Ifthegreentrafficlightison,executetheactiongostraight.
83.Asyouapproachtheintersection,observethestateofthecarfromleft.
94.Ifthecarfromleftisnotpresent,checkthestateofthepedestrianatright.
105.Ifthepedestrianatrightisnotpresent,executetheactionturnright.
```

By doing so, we have aligned the vocabularies from the initial responses to the defined propositions and actions. Note that such alignment may also introduce mistakes that violate more specifications, but reducing the probability of an alignment mistake is also a goal for DPO-AF.

Then, we apply semantic parsing to break the sentence into verb phrases and keywords (e.g., if) and then shorten the phrases for presentation purposes:

```
1.<observetrafficlight>.
22.<if><greentrafficlight>,<gostraight>.
33.<observecarfromleft>.
44.<if><nocarfromleft>,<checkpedestrianatright>.
55.<if><nopedestrianatright>.
```

Last, we follow the algorithm GLM2FSA  to construct an FSA representing the steps of this task, as presented in the left of Figure 7.

```
1Stepsfor"turnrightattrafficlight"
21.Observethetrafficlightinfrontofyou.
32.Checkfortheleftapproachingcarandrightsidepedestrian.
43.Ifnocarfromtheleftisapproachingandnopedestrianontheright,proceedtoturnright.
```

Example on Formal VerificationWe first implement both controllers in the automaton-based model presented in Figure 5, i.e., construct a product automaton for each controller and the model.

Second, we verify both product automata against the set of provided specifications. During the verification step, the model checker finds that the controller obtained before fine-tuning fails the specification \(_{5}\). The model checker returns a counter-example on states \((p_{0},q_{3}),(p_{4},q_{4}),(p_{1},q_{5})\).

This counter-example captures an edge case: The traffic light turns back to red and a car is coming from the left immediately after the agent is checking or waiting for pedestrians. In this scenario, the agent does not check for the traffic light and cars from left again and directly turns right, which can lead to an accident. We argue that this edge case can hardly be caught by human inspection but can be found by the model checker. Hence we highlight this counter-example to indicate the necessity of formal verification.

In contrast, the controller obtained after fine-tuning satisfies all the specifications. Through this right-turn example, we observe the language model's enhancement through DPO-AF. We present more controller construction and verification examples in the Appendix.

### Quantitative Evaluation

Fine-tuning via DPO.DPO fine-tunes a language model to output responses that match desired specifications. DPO requires a data set where each data point has the form \((x,y_{w},y_{l})\), where \(x\) is a user input (i.e., "Steps for turn right at the traffic light"), \(y_{w}\) and \(y_{l}\) are the language model's text responses such that the user prefers \(y_{w}\) over \(y_{l}\). In our experiments, the preferred response is the one whose FSA-based representation satisfies more of the specifications than the other response. We collect approximately 3000 data points to fine-tune the language model. After fine-tuning, the language model shows a preference for the responses as indicated in the training dataset.

We measure the DPO training performance via three metrics: DPO loss, accuracy, and marginal preference. Loss refers to the modified maximum likelihood loss function from the DPO algorithm, which is minimized via gradient descent. Accuracy measures how often the model prefers the correct response over the incorrect response. Accuracy is the mean over the dataset of \((P(y_{w}|x,)>P(y_{l}|x,))\), where \(\) is the indicator function returning one if the input is true and zero otherwise, and \(\) is the current values of the model parameters. Marginal preference measures how strongly the model prefers the correct output compared to the original reference model. Marginal preference is calculated as the mean over the dataset of \((log(P(y_{w}|x,))-log(P(y_{w}|x,_{ref})))-(log(P(y_{l}|x,))- log(P(y_{l}|x,_{ref})))\). Zero indicates indifference, positive values indicate stronger preferences for the favored answer, and negative values indicate preference for the less preferred response.

We show the fine-tuning performance on the Llama2-7B model over the three metrics in Figure 8. Note that the variance between random seeds is relatively small because the model starts with the same parameters, and only the order of the data changes between seeds.

Evaluation via Formal Verification.We provide an additional metric to evaluate the proposed DPO-AF. During the fine-tuning procedure, we save a checkpoint language model for every 20 epochs. For each checkpoint language model, we query it for various autonomous driving tasks and obtain the task controllers. Then, we verify the controllers against 15 provided specifications (presented in the Appendix) following the formal verification method in Section. Thus, we obtain the number of specifications being satisfied for each controller.

Figure 9 shows the relationship between the number of satisfied specifications and the number of epochs of DPO training. Simultaneously, we divide the results into two categories--training and validation--depending on whether the task is included in the training dataset. Hence, we have shown the relationships between the numbers of satisfied specifications and epochs for both training data and validation data.

For both training and validation data, we observe an increase in the number of specifications satisfied as we fine-tune for more epochs. This result indicates that our approach can improve the language model's ability to satisfy critical requirements. Therefore, our approach can act as a starting point to guide the design process for real-world implementations of autonomous driving systems.

Justification for Overfitting.We design the method DPO-AF to fine-tune language models for solving domain-specific tasks rather than enhancing the language model in general. Therefore, we do expect some degree of overfitting on the language model to the domain-specific knowledge and vocabulary. In our experiments, we fine-tune the language model specifically for tasks operated in autonomous driving systems.

Figure 8: This figure shows fine-tuning statistics for Llama2-7B optimized for an autonomous driving system. All plots show the mean over five seeds. Shaded areas indicate maximum and minimum values. Plots from left to right show the DPO losses, accuracies, and marginal preferences over different epochs, respectively.

Figure 7: Automaton-based controllers for the task “turn right at the traffic light.” The left controller is obtained before fine-tuning the language model, and the right controller is obtained after the fine-tuning. TL represents “traffic light.”

A certain degree of overfitting provides stronger guarantees that the generated outcomes satisfy critical specifications.

Empirical Evaluation in a Simulated System.We have presented another approach to obtain feedback via empirical evaluation in Section. We will show consistency between feedback from empirical evaluation and formal verification.

As we obtain the controllers through the proposed method, we operate the controllers in the Carla simulator to collect operation data. Carla is a simulator for the autonomous driving system. During each operation of each controller, we obtain a sequence of propositions and actions--in the form of \((2^{P} 2^{P_{A}})^{N}\). The propositions come from the information returned by the autonomous system, and the actions come from the controller. The Carla simulator allows for the extraction of system information. We present visual demonstrations of extracting the propositions from the system in Figure 10. Then, we verify the sequence against the provided specifications, as we described in Section. under Empirical Evaluation. We operate the controllers multiple times in the system and verify the sequences against the specifications. For each specification, we get a percentage of the number of sequences satisfying this specification.

Figure 11 compares these percentages obtained before fine-tuning and after fine-tuning. Note that we run multiple controllers and collect multiple sequences for each controller. We show the results for the first five specifications as presented in Section.

We observe that the percentages after fine-tuning are consistently higher than before fine-tuning among all five specifications, which means all the specifications have a higher probability of being satisfied for a given execution after fine-tuning. In Figure 9, we show that outputs from the fine-tuned model (at epoch 200) satisfy more specifications compared to the pre-trained model (at epoch 0). Hence, we obtain consistent feedback from the formal verification and empirical evaluation. Therefore, if we are unable to obtain automaton-based models for the system, empirical evaluation is a substitute for formal verification and is able to provide feedback consistent with formal verification.

From another perspective, this result provides additional evidence to show the effectiveness of the method DPO-AF, as it improves the probability of all the specifications being satisfied during operation.

## Conclusions

We develop a method of fine-tuning pre-trained language models via automated feedback for domain-specific tasks, such as control tasks in autonomous systems. The method converts the outputs from the pre-trained language model to automaton-based controllers. Then, it verifies how many of the externally provided specifications are satisfied by each controller. We rank the pre-trained language model's outputs by the number of satisfied specifications and feed these ranked outputs to the DPO algorithm for fine-tuning. We substitute human feedback with automated feedback using formal methods, which significantly decreases labor intensity. We provide empirical evidence on a simulated autonomous driving system to demonstrate the effectiveness of the proposed method: The fine-tuned language model satisfies more specifications compared with the model before fine-tuning.