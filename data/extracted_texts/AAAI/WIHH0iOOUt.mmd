# OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models

Jenish Maharjan*, MS, Anurag Garikipati*, MS, Navan Preet Singh+, MS, Leo Cyrus+, PhD, Mayank Sharma+, MS, Madalina Ciobanu+, PhD, Gina Barnes+, MPH, Qingqing Mao+, PhD, Ritankar Das+, MSc

***Joint first authors +All authors were employees of Montera, Inc dba Forta at the time of completion of the study**

**Introduction: LLMs have become increasingly more capable at a growing range of language tasks, and they inherently constitute a powerful tool that can expand equitable access to medical knowledge . This has promoted interest in the integration of LLM-based AI tools within various medical tasks, such as diagnostics aids, and tools developed for patient use to help with understanding care plans . However, prior to deploying LLMs into real-world medical settings, models intended for healthcare implementation have to be proven to be accurate, unbiased, and safe for use with patients. As a first step to successfully developing healthcare LLMs, models have to be evaluated on medical benchmarks developed for performance evaluation of medically-specialized LLMs . Some common benchmarks include MedQA, MedMCQA, PubMedQA, and the medical-subset of MMLU (nine medically and clinically relevant subsets) . Optimizing performance of healthcare LLMs on medical benchmarks can entail a variety of approaches, with the majority involving some degree of fine-tuning, which may be accompanied by expensive computational costs that are out of reach for most and may require task-specific data that is not easily accessible. While proprietary models have achieved strong performance, the ability to perform further investigation into the model's performance is limited. Adapting the techniques used in proprietary model optimization to open source (OS) models can provide the broader research community with a deeper understanding of the approaches, while retaining the benefits of OS models including transparency and compliance, which are particularly important in the healthcare space.**

While fine-tuning has produced strong results on evaluating LLMs on medical benchmarks, recent studies have examined employing robust prompt engineering to optimize performance of foundation models to similar or better levels when tested on medical benchmarks . For example, Microsoft developed Medprompt, a robust prompting technique, for use with the generalist GPT-4 model and achieved SOTA results the most common medical Q&A benchmarks .

However, there is no study to date showing that robust prompt engineering can be applied to generalist OS foundation models to significantly optimize performance in the absence of specialized fine-tuning. In this study, we present the OpenMedLM prompting platform, which applies robust prompt engineering techniques to the OS Yi 34B foundation model to achieve SOTA results on the four medical benchmarks we evaluated: MedQA, MedMCQA, PubMedQA, and the medical-subset of MMLU. By employing a range of prompting techniques including few-shot prompting , chain-of-thought (CoT) prompting , and self-consistency , our OpenMedLM prompting platform when used with the OS Yi 34B  foundation model outperformed Meditron , the previous SOTA for OS LLMs, on the MedQA (Figure 1), MedMCQA, and the medical-subset of the MMLU benchmarks.

**Methods:** Due to our focus on multiple choice Q&A benchmarks, we use accuracy as the evaluation metric to compare performance. During the inference for evaluation, we allowed a maximum of 5 tries for the model to generate a valid output, defined as an output that could be interpreted as an answer to a multiple choice question. If the model did not generate a valid output after 5 tries, the answer was counted as incorrect and would reflect as such in the final computation of accuracy for the benchmark. As the maximum number of tokens for the model also affects the GPU memory usage and the time for evaluation, we selected a value for the maximum number of tokens empirically to mimic test question to best encompass the context of that particular test question. Additionally, an ensemble voting scheme was

Figure 1: OpenMedLM performance on MedQA benchmark. OpenMedLM achieves 72.6% accuracy on the MedQA dataset with the Yi 34B model, surpassing all other OS models.

implemented, which runs each prompt through the model 5 times and selects the answer most commonly output during the 5 runs. For each of the 5 inference runs, the multiple choice options were randomly shuffled to create variation in the order in which the correct option was presented. An ablation experiment was performed to evaluate the model's performance on each prompting technique individually prior to adding on the subsequent prompting techniques. Results were computed for each prompting strategy, as well as for the complete OpenMedLM prompting platform in order to determine the accuracy of the prompt engineering approach as applied to the OS Yi 34B foundation model.

**Results:** Through the combination of all sub-components of the OpenMedLM prompting platform, we showcase SOTA performance on 3 of the 4 medical Q&A benchmarks evaluated in this study. We performed an ablation experiment which highlights the contribution of each sub-component of the OpenMedLM prompting platform to the overall results of the model. Figure 2 showcases the effect of each sub-component of OpenMedLM for the MedQA benchmark, by comparison with the compounding effect of each prompting technique employed by Meditron to achieve their model's top accuracy. The combination of all components of the OpenMedLM prompting platform results in a prompt engineering approach that enables the Yi 34B foundation model to outperform the specialized Meditron model.

_Zero-Shot:_ The baseline performance of the Yi 34B foundation model absent any prompt engineering techniques achieves an accuracy of 58.4% on the MedQA dataset.

_Random Few-Shot:_ Following the implementation of random few-shot prompting, the performance of the Yi 34B model increased by 3.1% to deliver an accuracy of 61.5%.

_Random Few-Shot with CoT:_ CoT explanations were utilized for each of the few-shot examples and added as the input prompt to the Yi 34B foundation model. This prompting strategy resulted in the largest surge in performance, contributing to a 5.1% increase leading to an accuracy of 66.6% for the Yi 34B foundation model.

_kNN Few-Shot with CoT:_ To further test the importance of the specific examples utilized in few-shot prompting, a kNN algorithm was used to identify the 5 most similar training questions to each test question and generate a CoT explanation for each of those examples to construct the prompt. The kNN few-shot with CoT approach led to a 2.9% improvement in accuracy on MedQA to deliver an accuracy of 69.5% for the Yi 34B foundation model.

_Ensemble/Self-Consistency:_ The final prompting strategy that we employed utilized a self-consistency approach to develop a majority voting scheme for answer selection. The voting scheme provided an additional 3.1% improvement in performance which led to the achievement of an overall total accuracy of 72.6% for the Yi 34B foundation model.

**Discussion:** We present OpenMedLM, a prompting platform which enables SOTA results for OS foundation LLMs on common medical benchmarks. OpenMedLM facilitates SOTA level performance solely by utilizing robust prompt engineering on generalist OS foundation LLMs. Through a series of additive and synergistic prompting techniques, OpenMedLM enables performance superior to previous SOTA on 3 of the 4 evaluated medical benchmarks for the Yi 34B foundation model we employed, which showcases the potential of generalist OS foundation models to perform highly specialized tasks without the costly challenges of requiring a highly-specialized training or fine-tuning dataset that are necessary to develop specialized models.

Achieving best performing results on specialized tasks by employing generalist OS foundation models highlights the need to continue research into better understanding the full extent of the capabilities of these OS foundation models. Research into LLMs has shown that large models have potential emergent abilities which may not have been thought of when the model was trained. The ability to achieve high accuracy on medical Q&A with foundation models highlights the emergence of properties for healthcare specific tasks which were previously assumed to require fine-tuning. The ability of OpenMedLM to bring out emergent properties in OS models highlights the need for further research into the full capabilities of generalist OS foundation LLMs on medical tasks, which we intend to further research going forward.

Figure 2: Ablation study showing the contribution of different components of the OpenMedLM prompting platform vs. the Meditron fine-tuned model.