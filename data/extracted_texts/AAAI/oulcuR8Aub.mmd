# Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches

Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches

 Clement Christophe1, Praveen K Kantihi1, Prateek Munjal1, Tathagata Raha1, Nasir Hayat1, Ronnie Rajan1, Ahmed Al-Mahrooqi1, Avani Gupta1, Muhammad Umar Salman1, Boulbaba Ben Amor2, Marco AF Pimentel1, Shadab Khan1

###### Abstract

This study presents a comprehensive analysis and comparison of two predominant fine-tuning methodologies - full-parameter fine-tuning and parameter-efficient tuning - within the context of medical Large Language Models (LLMs). We developed and refined a series of LLMs, based on the Llama-2 architecture, specifically designed to enhance medical knowledge retrieval, reasoning, and question answering capabilities. Our experiments systematically evaluate the effectiveness of these tuning strategies across various well-known medical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of 72% on the US Medical Licensing Examination (USMLE) datasets, setting a new standard in performance for openly available medical LLMs. Through this comparative analysis, we aim to identify the most effective and efficient method for fine-tuning LLMs in the medical domain, thereby contributing significantly to the advancement of AI-driven healthcare applications.

1G42 Healthcare 2Core42, Abu Dhabi, UAE {first name}.{last name}@g42healthcare.ai

## Introduction

In recent years, large language models (LLMs) have emerged as transformative tools, demonstrating remarkable proficiency in natural language understanding across a plethora of general-purpose applications, and sparking the interest in their use within specialized fields, particularly in the medical sector . Notably, these models, such as OpenAI's GPT-3.5 and GPT-4 , Google's BARD  as well as various other non-proprietary models, while initially developed for general natural language processing tasks, have been evolving and adapting to address the nuanced and complex language of the medical domain .

To enhance the efficacy of LLMs for medical applications, researchers have recognized the importance of training and/or fine-tuning these models on large, in-domain datasets . The utilization of such datasets allows for nuanced understanding of both natural language and domain-specific terminologies, making them adopt at interpreting and generating text that aligns with the intricacies of medical language. Albeit concerns about hallucinations and fabrications, biases and knowledge gaps, and risks about data privacy and ethics , this specialized capability enables LLMs to be potentially employed in various healthcare applications. These include aiding diagnostic processes by analyzing and summarizing patient history and symptoms , interpreting medical literature and research papers for easier comprehension by healthcare professionals , generating patient education materials tailored to individual needs , and assisting in the development and consultation of clinical guidelines and decision support systems .

To this end, we focus on the development of a medical LLM, by comparing and analyzing two predominant fine-tuning methodologies: full-parameter fine-tuning and parameter-efficient tuning. Full-parameter fine-tuning is a comprehensive approach that involves adjusting all parameters of a pre-trained model, which demands substantial computational resources and time . In contrast, parameter-efficient tuning methods, such as Adapters , Low-Rank Adaptation (LoRA) , and Prompt-tuning (P-tuning),  offer a more resource-efficient alternative by modifying a smaller subset of the model's parameters. This study presents a detailed comparison of these methods, specifically within the context of medical LLMs. Our investigation includes experiments to assess the effectiveness of these tuning strategies, with a particular focus on the emerging LoRA technique. Through this comparative analysis, our objective is to identify the effective and efficient method for fine-tuning LLMs in the medical domain, ultimately contributing to the advancement of AI-driven healthcare applications. We are also releasing our most performant model Med42 on HuggingFace1.

## Methods

In this section, we elaborate on the dataset comprising our study, detailing its characteristics and the rationale for its selection. We outline the specific methodologies implementedfor fine-tuning our large medical language model, including a comparison of full-parameter tuning and parameter-efficient techniques such as Low-Rank Adaptation (LoRA). Furthermore, we provide an exhaustive list of hyperparameters and configurations employed during our experiments, aiming to offer a transparent and replicable framework for subsequent research endeavors in this domain. This section aims to provide a comprehensive understanding of the technical aspects and decision-making processes underpinning our model's development and evaluation.

### Training Dataset

Our instruction-tuning dataset is a combination of multiple open datasets, primarily focused on medical question-answering data. It includes an extensive collection from medical forums, notably those within the Stack Exchange network, which are rich with expert discussions, patient inquiries, and specialist responses. Additionally, we incorporated selected sections from general domain datasets, meticulously extracting and integrating segments specifically related to medical topics. This composite approach ensures a diverse and robust dataset, encompassing a wide range of medical subfields and contexts, providing a comprehensive foundation for training our model to understand and generate medically-relevant content accurately. Further details about the training dataset are described in Appendix A.

In order to make our model learn from instructions effectively, we employed a structured instruction format using the keywords <|system|>, <|prompter|>, and <|assistant|>. This format has been designed to teach the model the relationship between a given command and its appropriate output. By encapsulating the input under <|prompter|>, the intended system operation under <|system|>, and the expected output under <|assistant|>, we created a clear, directive framework that aids the model in understanding and executing tasks based on instructions.

### Modelling

Models.In this study, we built on the Llama-2  family of models as the foundational architecture for fine-tuning. We specifically focused on the 7 billion (7B) and 70 billion (70B) parameter versions of these models. These versions were selected for their robust pretraining and scalability, allowing us to explore the impact of model size on performance in medical domain-specific tasks. Also, Llama-2 model comes with an open license, allowing for greater flexibility for adaptation and use in our research.

LoRA.Low-Rank Adaptation (LoRA) is a parameter-efficient training technique that targets the adaptation of pre-trained language models without the need for full model fine-tuning. Instead of updating all the parameters, LoRA focuses on a subset of the Transformer architecture. It introduces trainable low-rank matrices while keeping the pre-trained weights fixed. These matrices capture the essential changes needed to adapt the model to new tasks, effectively reducing the number of trainable parameters and thus computational costs.

The selection of layers to which LoRA is applied constitutes a hyperparameter that requires careful tuning to optimize model performance. While it is common to see LoRA applied only to attention layers \(v\_proj\), \(q\_proj\), \(k\_proj\) and \(o\_proj\) or only \(gate\_proj\), \(down\_proj\), and \(up\_proj\) as in , we achieved the best performance by applying it to every linear layer as in . With these settings, the number of trainable parameters goes from 7 and 70 billion to 20 and 104 million, respectively. Details about the computational setup are available in Appendix A.

Mask loss.In our methodology, every sample is composed of three elements: a system prompt, a user prompt, and a corresponding response. To optimize the use of the model's available context length, we concatenate these samples across the entire training dataset. The training approach is autoregressive, focusing the backpropagation of loss exclusively on the tokens forming the responses. Consequently, this training strategy ensures that the model predominantly learns to generate answers, rather than the prompts.

Hyperparameters.We train using the AdamW optimizer , with \(_{1}=0.9\) and \(_{2}=0.95\). We use a cosine learning rate schedule, with a linear warmup of 100 steps, and decay final learning rate to 10% of its peak. We use a weight decay of 0.1 and gradient clipping of 1.0. For full-parameter fine-tuning, we trained the model for 3 epochs with a peak learning rate of \(5e^{-5}\). For LoRA fine-tuning, we trained for 8 epochs with a peak learning rate of \(1e^{-4}\) and \(=16\) and \(r=8\). To speed up the training, we packed all of our fine-tuning data into chunks of 4,096 tokens.

### Model evaluation

To assess the performance of the fine-tuned language models, following previous works , we used Eleuther AI's evaluation harness framework  to compute their zero-shot performance across various commonly-used medical benchmarks. These contain medical exam questions and research datasets with multiple-choice answers, and include MedQA, HeadQA, MedMCQA, PubMedQA, MMLU clinical topics, and both self-assessment and sample exams from the United States Medical Licensing Examination (USMLE). All datasets are in the English language and all questions containing images were excluded. We describe these datasets in more detail below.

MedQA.The dataset consists of multiple-choice (4 or 5) questions that resemble USMLE questions (from the National Medical Board Examination in the USA) and was originally designed for addressing medical problems .

HeadQA.This multiple-choice question-answering dataset which is sourced from exams to access a specialized position in the Spanish healthcare system .

Gomez-Rodriguez 2019). Only the English subset has been included in our evaluation.

MedMCQA.A large-scale multiple-choice questions dataset (4-choices) from the Indian medical entrance examinations (Pal, Umapathi, and Sankarasubbu 2022), covering 21 medical subjects and more than 2,000 healthcare topics. We report the performance of our models on the validation set (given that the available test dataset does not contain answers to the questions), which has been excluded from our training (fine-tuning) dataset.

PubMedQA.The task of PubMedQA is to answer research questions with yes/no/maybe using abstracts from medical scientific literature (Jin et al. 2019); i.e., given an abstract and a related question, the task is to provide a short answer of yes, no or maybe.

MMLU clinical topics.The widely-used Measuring Multitask Language Understanding (MMLU) benchmark (Hendrycks et al. 2021) aimed to introduce a comprehensive assessment of LLMs across 57 subjects. For our evaluation, we selected clinical topics covering clinical knowledge, college biology, college medicine, medical genetics, professional medicine and anatomy.

USMLE sample exam and self-assessment.These are two sets of official practice materials for the United States Medical Licensing Examination (USMLE), which is an examination program that contains three steps for assessing medical competency (Nori et al. 2023; Han et al. 2023).

We also address a growing concern in the field of LLM fine-tuning: the inadvertent inclusion of similar or identical samples in both training and evaluation datasets. To address this, we implemented a "decontamination pipeline", which is designed to scrutinize the evaluation dataset and flag any examples that have a significant resemblance to those found in our instruction-tuning dataset. These flagged examples are regarded as "contaminated samples". To ensure the integrity of our evaluation, we also show the performance metrics calculated after the removal of these contaminated samples from the evaluation datasets. Detailed information about the decontamination process can be found in Appendix.

## Results

The performance of the fine-tuned models across the different benchmark datasets is represented in Figure 1, showing the accuracy values obtained for both 7B and 70B-parameter models. The results show the superiority of the fine-tuned models over their corresponding base models across all medical benchmark datasets. Notably, our analysis reveals that full-parameter fine-tuning outperforms the parameter-efficient fine-tuning approach, LoRA, in the majority of these datasets.

The comparative results from our experiments, illustrating the performance of our best fine-tuned models (70B-parameter models) against that of other models, are detailed in Table 1. Changes in the accuracy scores after the removal of contaminated examples are shown in Figure 2.

## Discussion

The findings of this study underscore the efficacy of comprehensive fine-tuning strategies in enhancing the performance of language models. Importantly, we report domain-adapted fine-tuned medical LLMs that demonstrate high-level medical reasoning and improved domain-specific benchmark performance, particularly, in medical complex tasks such as

Figure 1: Performance of 7-billion (left) and 70-billion (right) parameter models on various medical-related benchmark datasets (in zero-shot setting). Performance results (accuracy) are displayed in % for the base and fine-tuned models.

USMLE-based questions.

Overall, full-parameter fine-tuning achieved better performance than parameter-efficient fine-tuning in medical tasks (Figure 1). However, it is noteworthy that parameter-efficient fine-tuning methods, such as LoRA, yield results that are remarkably close to those achieved by full-parameter fine-tuning, consistent with findings in other studies . These findings suggest that parameter-efficient approaches can be viable alternatives, particularly in scenarios where computational resources are limited.

A critical aspect of our study was the thorough examination of potential test set contamination. We analyzed whether our evaluation datasets contained examples that were either identical or strikingly similar to those in the training set, and re-evaluated our models on the "decontaminated" dataset. We observed that a small number of samples were deemed to be "contaminated" (Table 2), which resulted in very small changes in the accuracy scores for the two larger fine-tuned models across the benchmark datasets (Figure 2). This process ensures the robustness and integrity of our analysis, affirming the reliability of our findings and the trained models.

Moreover, our study encompassed a comparative analysis of the performance of fine-tuned models against other LLMs, including those commercially available. This comparison provides a more comprehensive view of the standing of our fine-tuned models in the current landscape of openly available LLMs, particularly in medical applications.

This research also underscores the importance of creating a large and well-structured instruction fine-tuning dataset. As instruct fine-tuning of open-source LLMs becomes a _de facto_ standard practice, our results show that our model exhibits superior performance compared to established names like ClinicalCamel , MediTron  and GatorTronGPT . Our approach involved minimal experimentation with complex prompt engineering; however, we believe there are additional opportunities to enhance the model's response quality and accuracy. These opportunities could be explored in future research to achieve greater advancements.

While our results regarding the model's performance on medical question-answering benchmarks are promising, they also highlight areas for future exploration. Our study serves as a stepping stone, indicating the potential of these models in diverse applications. However, further research is necessary to fully understand and demonstrate the utility of

   Dataset & PE-FT & FP-FT (Med42) & ClinCamel1 & MediTron2\({}^{}\) & GPT-3.5\({}^{3}\) & GPT-4\({}^{3}\) & MedPaLM-2\({}^{}\) \\ 
**MMLU** (average) & **76.7** & **76.7** & 69.8 & 71.5 & 66.6 & 87.0 & **89.4** \\ Clinical knowledge & **76.6** & 74.3 & 69.8 & - & 69.8 & 86.0 & **88.3** \\ College biology & 83.3 & **84.0** & 79.2 & - & 72.2 & **95.1** & 94.4 \\ College medicine & **72.8** & 68.8 & 67.0 & - & 61.3 & 76.9 & **80.9** \\ Medical genetics & 80.0 & **86.0** & 69.0 & - & 70.0 & **91.0** & 90.0 \\ Professional medicine & **80.1** & 79.8 & 71.3 & - & 70.2 & 93.0 & **95.2** \\ Anatomy & **67.4** & **67.4** & 62.2 & - & 56.3 & **80.0** & 77.8 \\
**HeadQA** & 70.6 & **72.0** & - & - & - & - & - \\
**MedMCQA** & 54.7 & **60.9** & 47.0 & 53.3 & 50.1 & 69.5 & **71.3** \\
**MedQA** & 59.1 & **61.5** & 53.4 & 52.0 & 50.8 & 78.9 & **79.7** \\
**PubMedQA** & 75.8 & 76.8 & 74.3 & **79.8** & 71.6 & 75.2 & **79.2** \\
**USMLE** (average) & 68.3 & **71.9** & - & - & 53.1 & **84.1** & - \\ Self-assessment & 68.0 & **71.7** & - & - & 49.1 & **83.8** & - \\ Sample exam & 68.6 & **72.0** & 54.3 & - & 56.9 & **84.3** & - \\   

Table 1: Zero-shot performance comparison between both parameter-efficient (PE-FT) and full-parameter (FP-FT) fine-tuned (Llama2 70-B) models with other published models across the various medical benchmark datasets. \({}^{1}\)Clinical Camel ; \({}^{2}\)MediTron ; \({}^{3}\)GPT-3.5 and GPT-4 ; \({}^{4}\)MedPaLM-2 . \({}^{}\)We note that zero-shot performance is not reported for these models; few-shot results are shown.

Figure 2: Accuracy change after decontamination for both (70b) fine-tuned models (shown in %).

these models in other practical use-cases. Such investigations are crucial for advancing the field and realizing the full potential of fine-tuned LLMs in a variety of domains.

## Ethical Considerations and Reproducibility

In this work, we underscore the critical importance of ethical considerations and reproducibility in the development of our large language model. Firstly, our model is released under an open license, ensuring public accessibility and fostering a culture of transparency and collaboration within the research community. This approach not only democratizes access to advanced technology but also invites scrutiny and diverse perspectives, which are vital for ethical development. Additionally, the datasets and frameworks used in our model's evaluation are freely available, promoting reproducibility and allowing independent verification of our findings. This transparency in data and methodologies is essential to maintain scientific integrity and foster trust in AI research. Furthermore, we recognize and openly acknowledge the limitations of our model, particularly its readiness for use in clinical practice. We emphasize that while our model shows promising results, it should not yet be used as a decision-making tool in clinical environments. There is the potential for generating incorrect or harmful information and the risk of perpetuating biases in training data. This acknowledgment stems from our commitment to responsible AI development, where the safety and well-being of end-users are paramount. Ongoing human evaluation efforts are focused on rigorously assessing biases, fairness, and safety through red-teaming the model to ensure its robustness and reliability in clinical settings. By addressing these ethical concerns and emphasizing reproducibility, we aim to contribute positively to the field of AI, ensuring that advancements are made with a keen awareness of their societal impacts and limitations.