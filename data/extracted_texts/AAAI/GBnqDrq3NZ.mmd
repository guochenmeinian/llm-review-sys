# DeiSAM: Segment Anything with Deictic Prompting

Hikaru Shindo\({}^{1}\), Manuel Brack\({}^{1,2}\), Gopika Sudhakaran\({}^{1,3}\),

**Devendra Singh Dhami\({}^{3,4}\), Patrick Schramowski\({}^{1,2,3,5}\), Kristian Kersting \({}^{1,2,3,6}\)**

\({}^{1}\)Technical University of Darmstadt \({}^{2}\)German Research Center for AI (DFKI)

\({}^{3}\)Hessian Center for AI (hessian.AI) \({}^{4}\) Eindhoven University of Technology \({}^{5}\) LAION

\({}^{6}\) Centre for Cognitive Science, Technical University of Darmstadt

{firstname.lastname}@tu-darmstadt.de, d.s.dhami@tue.nl

###### Abstract

Large-scale, pre-trained neural networks have demonstrated strong capabilities in various tasks, including zero-shot image segmentation. To identify concrete objects in complex scenes, humans instinctively rely on _deictic_ descriptions in natural language, _i.e._, referring to something depending on the context, _e.g._ _"The object that is on the desk and behind the cup."_. However, deep learning approaches cannot reliably interpret these deictic representations due to their lack of reasoning capabilities in complex scenarios. To remedy this issue, we propose DeiSAM, which integrates large pre-trained neural networks with differentiable logic reasoners. Given a complex, textual segmentation description, DeiSAM leverages Large Language Models (LLMs) to generate first-order logic rules and performs differentiable forward reasoning on generated scene graphs. Subsequently, DeiSAM segments objects by matching them to the logically inferred image regions. As part of our evaluation, we propose the Deictic Visual Genome (DeiVG) dataset, containing paired visual input and complex, deictic textual prompts. Our empirical results demonstrate that DeiSAM is a substantial improvement over data-driven neural baselines on deictic segmentation tasks.

## Introduction

Recently, large-scale neural networks have achieved substantial advancements in various tasks at the intersection of vision and language. One such challenge is grounded image segmentation, wherein objects within a scene are identified through textual descriptions. For instance, Grounding Dino , combined with the Segment Anything Model , excels at this task if provided with appropriate prompts. However, a well-documented limitation of data-driven neural approaches is their lack of reasoning capabilities . Consequently, they only perform well for textual prompts directly describing or naming the targets and fail for complex prompts, as demonstrated in Fig. 1.

In contrast, humans identify objects through structured descriptions of complex scenes referring to an object depending on the context, e.g., _"An object that is on the boat and holding an umbrella."_. These descriptions are referred to as _deictic representations_ and were introduced to artificial intelligence research motivated by linguistics , and subsequently applied in reinforcement learning . Although deictic representations play a central role in human comprehension of scenes, current approaches fail to interpret them faithfully due to their poor reasoning capabilities.

To remedy these issues, we propose DeiSAM, which combines large-scale neural networks with logic reasoners to segment objects with deictic representations. The DeiSAM pipeline is highly modular and provides a sophisticated integration of large pre-trained networks and neuro-symbolic reasoners. Specifically, we leverage Large Language Models (LLMs) to generate logic rules for a given deictic prompt and perform differentiable forward reasoning  with scene graph generators . Our reasoner is efficiently combined with neural networks and leverages forward propagation on computational graphs. The result of this reasoning step is used to ground a segmentation model that reliably identifies the objects that best match the input.

Overall, we make the following contributions:

* We propose DeiSAM, a modular, neuro-symbolic reasoning pipeline on LLMs and scene graphs for object segmentation with complex textual prompts.
* We demonstrate _semantic unification_, where similar entities are unified using textual embeddings.

Figure 1: **DeiSAM segments objects with deictic prompting.** A segmentation result of DeiSAM (right), and that of GroundedSAM1 and LLaVA  (left) given a visual scene and deictic representation as prompt.

* For evaluation, we introduce a novel Deictic Visual Genome (DeiVG) benchmark that contains visual scenes paired with deictic representations, _i.e._, complex textual identifications of objects in the scene.
* We empirically show that DeiSAM outperforms neural baselines such as GroundedSAM and LLaVA on the proposed task.

## DeiSAM

Let us now devise the DeiSAM pipeline, by giving a brief overview of its modules before describing essential components in more detail. Fig. 2 shows a schematic overview of the proposed workflow.

First, an input image is transferred into a graphical representation using a **(1) scene graph generator**. Specifically, a scene graph comprises a set of triplets \((n_{1},e,n_{2})\), where entities \(n_{1}\) and \(n_{2}\) have relation \(e\). For example, a _person_ (\(n_{1}\)) is _holding_ (\(e\)) an _umbrella_ (\(n_{2}\)). Consequently, each triplet can be interpreted as a logic atom, or simply a _fact_, \(,n_{2})}\). To perform reasoning on these facts, the paired textual deictic prompt needs to be interpreted as a structured logical expression. For this processing step, DeiSAM leverages **(2) large language models**, which can generate logic rules for deictic descriptions, given sufficiently restrictive prompts. In our example, the LLM would translate the prompt _"An object that is on the barge and holding an umbrella."_ to the following rules:

target(X):-on(X,Y),type(Y,barge),

holding(X,2),type(Z,umbrella).

However, users often use terminology different from that of the scene graph generator. For example, _barge_ and _boat_ target the same concept but will not be trivially matched. To bridge the semantic gap, we introduce a **(3) semantic unifier**. This module leverages word embeddings of labels, entities, and relations in the generated scene graphs and rules to match synonymous terms by modifying rules accordingly. The semantically unified rules are then compiled to a **(4) forward reasoner**, which computes logical entailment using forward chaining . The reasoner identifies the targeted objects and corresponding bounding boxes from the scene graph. Lastly, we segment the object by feeding the cropped images to a **(5) segmentation model**.

Now, let us investigate the two core modules of DeiSAM in detail. Specifically, we look into how DeiSAM generates logic rules and performs reasoning.

### LLMs as Logic Generators

To perform reasoning on textual deictic prompts, we need to identify corresponding rules. For this translation, we use LLMs to parse textual descriptions to logic rules by using a system prompt like:

1. Given a deictic representation and available predicates, generate rules in the format.
2. The rule's format is either
3. target(X):-pred(X,Y),type(Y,const).
4. or
5. target(X):-pred1(X,Y),type(Y,const), pred2(X,Z),type(Z,const2).
6. Usepredicates and constants that appear in the given sentence.
7. Capitalize variables: X, Y, Z, etc. DeiSAM uses a specific rule format that describes the relations of objects and attributes. For example, a fact on(person,boat) in a scene graph is decomposed into multiple facts on(obj1,obj2),

Figure 2: **DeiSAM architecture. A pair of an image and a deictic prompt is given as input. A scene graph is generated out of the image, and logic rules are generated out of the deictic prompt by a large language model. The generated scene graph and rules are fed to the _Semantic Unifier_ module, where similar terms are unified to perform reasoning jointly, _e.g._ boat in the scene graph and barge in generated rules are regarded as the same term. Forward reasoner infers target objects specified by the textual deictic prompt. To this end, the object segmentation is performed in extracted image crops. Since forward reasoners can be differentiable , gradients could be propagated to learn each module. (Best viewed in color)**

type(obj1,person), and type(obj2,boat) to account for several entities with the same name in the scene.

### Reasoning with Deictic Prompting

We build a reasoning function \(f_{reason}:\) where \(\) is a set of facts that represent a scene graph, \(\) is a set of rules generated by an LLM, and \(\) is a set of facts that represent identified target objects in the scene.

**(Differentiable) Forward Reasoning.** For a given set \(\), a _valuation vector_\(^{||}\) maps each fact to a corresponding confidence score. DeiSAM incorporates graph neural networks, which pass messages on reasoning graphs that represent a set of rules and update valuation vectors, inferring new facts . To complete object segmentation, DeiSAM identifies target objects as facts, _e.g._target(obj1), and subsequently extracts the bounding box of the targets from the scene graph. We provide more details in the appendix.

**Semantic Unifier.** DeiSAM unifies diverging semantics in the generated rules and scene graph using concept embeddings similar to neural theorem provers . We rewrite the corresponding rules \(\) of a prompt by identifying the most similar terms in the scene graph for each predicate and constant. If rule \(R\) contains a term \(x\), which does not appear in scene graph \(\), we compute the similarity score by

\[*{arg\,max}_{y}encoder(x)^{} encoder(y), \]

where \(encoder\) is an embedding model for texts.

## Experiments

With the methodology of DeiSAM established, we subsequently provide empirical and qualitative evidence of its benefits over purely neural approaches.

### Experimental Setup

To assess deictic object segmentation, we propose a novel benchmark, the Deictic Visual Genome (DeiVG), which is an extension of the Visual Genome dataset . DeiVG consists of visual scenes paired with deictic prompts targeting one or multiple objects in the image. We automatically synthesize prompts from scene graphs in Visual Genome using textual templates. For example, the relations on(cable,table) and behind(cable,muy), would yield a prompt _"An object on the table and behind the mug."_ targeting the cable. Entries in the DeiVG dataset can be categorized by the number of relations they use in their object description. Overall, we generate 3k pairs of a visual scene and a deictic prompt with one relation and 10k pairs of them with two relations that we denote as DeiVG\({}_{1}\) and DeiVG\({}_{2}\), respectively.

As an evaluation metric, we use mean average precision (mAP) over object classes. Since the object segmentation quality largely depends on the used segmentation model, we focus on assessing the object identification preceding the segmentation step. The DeiSAM configuration for the subsequent experiments uses the ground truth scene graphs from the Visual Genome , gpt-3.5-turbo2 as LLM for rule generation, ada-0023 as embedding model for semantic unification, and SAM for object segmentation. Additionally, we provide few-shot examples of deictic prompts and paired rules in the input context of the LLM, which improves performance.

### Empirical Evidence

We compare DeiSAM on DeiVG datasets with GroundedSAM, which combines SAM  with Grounding DINO . We report the scores for both methods in Tab. 1. DeiSAM outperforms the purely neural approach by a large margin on both DeiVG\({}_{1}\) and DeiVG\({}_{2}\). Interestingly, both methods achieve better scores for the seemingly more complex task, a phenomenon that we explore in more detail in the next section.

### Qualitative Evaluation

After empirically demonstrating DeiSAM's capabilities, we look into some qualitative examples. In Fig. 3, we demonstrate the efficacy of the semantic unifier. All examples use terminology in the deictic prompt diverging from the scene graph entity names. Nonetheless, the unification step successfully maps synonymous terms and still produces the cor

    &  \\ Method & DeiVG\({}_{2}\) & DeiVG\({}_{1}\) \\  GroundedSAM & 19.27 & 8.82 \\ DeiSAM (ours) & **81.42** & **78.18** \\   

Table 1: **DeiSAM handles deictic prompting.** Mean Average Precision (mAP) of DeiSAM and GroundedSAM on Deictic VG datasets are shown. Subscript numbers indicated the complexity (hops on scene graph) of prompts.

Figure 3: **DeiSAM can reason on ambiguous prompts.** Segmentation results (middle) on prompts (top) that contain entities not appearing in the scene graphs (bottom). DeiSAM successfully identified objects given two different semantics.

rect segmentation masks, overcoming the limitation of off-the-shelf symbolic logic reasoners.

In Fig. 4, we further compare DeiSAM with GroundedSAM and interactive LLaVA . DeiSAM produces the correct segmentation mask even for complicated shapes (_e.g._ partially occluded cable) or complex scenarios (_e.g._ multiple people, only some holding umbrellas). GroundedSAM and LLaVA, however, regularly fail to identify the correct object. More results are available in the Appendix. Overall, the examples further highlight DeiSAM's capability of complex reasoning for object segmentation, outperforming pure neural approaches.

## Discussion

In our experiments, we observed degraded performance on DeiVG\({}_{1}\) compared to DeiVG\({}_{2}\) for both models. This result may seem counterintuitive since DeiVG\({}_{1}\) contains the simpler prompts. We attribute the gap to inconsistencies in the original Visual Genome dataset itself. For example, only one of multiple objects in an image might be labeled correctly in the scene graph. Consequently, our derived DeiVG benchmark can contain prompts with ambiguous target objects. Simple prompts are generally more ambiguous (e.g., _"an object on the table"_) thus, they are disproportionally affected by this issue. For future work, we aim to improve the DeiVG benchmark by cleaning up inconsistent prompts.

Moreover, we observed that neural baselines are easily confounded by objects mentioned in the prompt, _e.g._ given _"An object that is on the car"_, the car itself is segmented, discarding the intended relation. In contrast, DeiSAM successfully segments objects given prompts requiring relational reasoning since it embraces logic reasoners and encodes relations of objects explicitly.

While DeiSAM achieves impressive results, it is worth considering some of the limitations of this work. Our current experimental setup uses ground-truth scene graphs from Visual Genome, whereas the errors of an actual scene graph generator may result in a worse performance. However, DeiSAM's modularity accommodates recent advances in scene graph generations, _e.g._ unbiased scene graph generators .

Finally, DeiSAM may be leveraged for gradient-based learning approaches, since the reasoning is differentiable. For future work, we plan to pass gradients through DeiSAM to the scene generator and LLM submodules. Such a setup would allow for fine-tuning neural modules to generate high-quality scene graphs and logic rules with reasoning explicitly modeled in the training pipeline. Additionally, our setup allows for structure learning of logic rules from segmentation examples, which is a promising research direction.

## Conclusion

We proposed DeiSAM to perform deictic image segmentation. DeiSAM embraces large-scale neural networks to understand complex prompts with visual scenes and performs differentiable forward reasoning to identify objects. DeiSAM allows users to describe a target using relations of objects flexibly. Moreover, we proposed the novel Deictic Visual Genome (DeiVG) benchmark for segmentation with complex deicic prompts. In our extensive experiments, we demonstrated that DeiSAM outperforms neural baselines highlighting its strong reasoning capabilities on visual scenes with complex textual prompts.

Figure 4: **DeiSAM segments objects with deictic prompts. Segmentation results on the DeiVG dataset using DeiSAM, GroundedSAM, and LLaVA are shown with deictic prompts on the top. DeiSAM correctly identifies and segments objects given deictic prompts (top row), while GroundedSAM and LLaVA often segment a wrong object or fail to identify an object (bottom rows). More results are available in Fig. 6 in the Appendix. (Best viewed in color)**