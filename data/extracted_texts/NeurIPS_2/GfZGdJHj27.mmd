# Consistent Diffusion Models:

Mitigating Sampling Drift by Learning to be Consistent

 Giannis Daras

Department of Computer Science

University of Texas at Austin

&Yuval Dagan

Electrical Engineering and Computer Science

University of California, Berkeley

&Alexandros G. Dimakis

Department of ECE

University of Texas at Austin

&Constantinos Daskalakis

Electrical Engineering and Computer Science

Massachusetts Institute of Technology

These authors contributed equally to this work.

###### Abstract

Imperfect score-matching leads to a shift between the training and the sampling distribution of diffusion models. Due to the recursive nature of the generation process, errors in previous steps yield sampling iterates that drift away from the training distribution. However, the standard training objective via Denoising Score Matching (DSM) is only designed to optimize over non-drifted data. To train on drifted data, we propose to enforce a _Consistency_ property (CP) which states that predictions of the model on its own generated data are consistent across time. Theoretically, we show that the differential equation that describes CP together with the one that describes a conservative vector field, have a unique solution given some initial condition. Consequently, if the score is learned well on non-drifted points via DSM (enforcing the true initial condition) then enforcing CP on drifted points propagates true score values. Empirically, we show that enforcing CP improves the generation quality for conditional and unconditional generation on CIFAR-10, and in AFHQ and FFHQ. We open-source our code and models: [https://github.com/giannisdaras/cdm](https://github.com/giannisdaras/cdm).

## 1 Introduction

The diffusion-based  approach to generative models has been successful across various modalities, including images , videos , audio , 3D structures , proteins , and medical applications .

Diffusion models generate data by first drawing a sample from a noisy distribution and slowly _denoising_ this sample to ultimately obtain a sample from the target distribution. This is achieved by sampling, in reverse from time \(t=1\) down to \(t=0\), a stochastic process \(\{x_{t}\}_{t}\) wherein \(x_{0}\) is distributed according to the target distribution \(p_{0}\) and, for all \(t\),

\[x_{t} p_{t}\;\;\;\;p_{t}:=p_{0}(0,_{t}^{ 2}I_{d}), \]

where \(\) denotes the convolution operator. That is, \(p_{t}\) is the distribution resulting from corrupting a sample from \(p_{0}\) with noise sampled from \((0,_{t}^{2}I_{d})\), where \(_{t}\) is given by an increasing function of \(t\), such that \(_{0}=0\) and \(_{1}\) is sufficiently large so that \(p_{1}\) is nearly indistinguishable from pure noise. We note that diffusion models have been generalized to other types of corruptions by the recent worksof Daras et al. , Bansal et al. , Hoogeboom and Salimans , Deasy et al. , Nachmani et al. .

In order to sample from a diffusion model, i.e. sample the afore-described process in reverse time, it suffices to know the _score function_\(s(x,t)=_{x} p(x,t)\), where \(p(x,t)\) is the density of \(x_{t} p_{t}\). Indeed, given a sample \(x_{t} p_{t}\), one can use the score function at \(x_{t}\), i.e. \(s(x_{t},t)\), to generate a sample from \(p_{t-dt}\) by taking an infinitesimal step of a stochastic or an ordinary differential equation , or by using Langevin dynamics .2 Hence, in order to train a diffusion model to sample from a target distribution of interest \(p_{0}^{*}\) it suffices to learn the score function \(s^{*}(x,t)\) using samples from the corrupted distributions \(p_{t}^{*}\) resulting from \(p_{0}^{*}\) and a particular noise schedule \(_{t}\). Notice that those samples can be easily drawn given samples from \(p_{0}^{*}\).

The Sampling Drift Challenge:Unfortunately, the true score function \(s^{*}(x,t)\) is not perfectly learned during training. Thus, at generation time, the samples \(x_{t}\) drawn using the learned score function, \(s_{}(x,t)\), in the ways discussed above, drift astray in distribution from the true corrupted distributions \(p_{t}^{*}\). This drift becomes larger for smaller \(t\) due to compounding of errors and is accentuated by the fact that the further away a sample \(x_{t}\) is from the likely support of the true \(p_{t}^{*}\) the larger the error \(\|s_{}(x_{t},t)-s^{*}(x_{t},t)\|\) between the learned and the true score function at \(x_{t}\), which feeds into an even larger drift between the distribution of \(x_{t^{}}\) from \(p_{t^{}}^{*}\) for \(t^{}<t\); see e.g. . These challenges motivate the question:

_Question._ How can one train diffusion models to improve the error \(\|s_{}(x,t)-s^{*}(x,t)\|\) between the learned and true score function on inputs \((x,t)\) where \(x\) is unlikely under the target noisy distribution \(p_{t}^{*}\)?

A direct approach to this challenge is to train our model to minimize the afore-described error on pairs \((x,t)\) where \(x\) is sampled from distributions other than \(p_{t}^{*}\). However, there is no straightforward way to do so, because we do not have direct access to the values of the true score function \(s^{*}(x,t)\).

This motivates us to propose a training method to mitigate sampling drift by enforcing that the learned score function satisfies an invariant, that we call the Consistency Property (CP). This property can be optimized without using any samples from the target distribution \(p_{0}^{*}\). We will show that theoretically, enforcing CP on drifted points, in conjunction with minimizing the standard score matching objective on non drifted points, suffices to learn the correct score everywhere - at least in the theoretical limit where the error approaches zero and when one also enforces conservative vector field. We also provide experiments illustrating that regularizing the standard score matching objective using our CP improves sample quality. Further, we provide an ablation study that further provides evidence to this phenomenon of score propagation.

Our Approach:The true score function \(s^{*}(x,t)\) is closely related to another function, called the _optimal denoiser_, which predicts a clean sample \(x_{0} p_{0}^{*}\) from a noisy observation \(x_{t}=x_{0}+_{t}\) where the noise is \((0,I_{d})\). The optimal denoiser (under the \(_{2}\) loss) is the conditional expectation:

\[h^{*}(x,t):=_{x_{0} p_{0}^{*}\\ x_{t}=x_{0}+_{t}\\ (0,I_{d})}[x_{0} x_{t}=x],\]

and the true score function can be obtained from the optimal denoiser as follows: \(s^{*}(x,t)=(h^{*}(x,t)-x)/_{t}^{2}\). This result is known as _Tweedie's Formula_. Indeed, the standard training technique, via _score-matching_, explicitly trains for the score through the denoiser \(h^{*}\).

We are now ready to state our Consistency Property (CP). We will say that a (denoising) function \(h_{}(x,t)\) satisfies _CP_ iff

\[_{}[x_{0}|x_{t}=x]=h_{}(x,t),\; t, x,\]

where the _expectation is with respect to a sample from the **learned** reverse process_, defined in terms of the implied score function \(s_{}(x,t)=(h_{}(x,t)-x)/_{t}^{2}\), when this is initialized at \(x_{t}=x\) and run backwards in time to sample \(x_{0}\). See Eq. (3) for the precise stochastic differential equation and its justification. In particular, \(h_{}\) satisfies CP if the prediction \(h_{}(x,t)\) of the conditional expectation of the clean image \(x_{0}\) given \(x_{t}=x\) equals the expected value of an image that is generated by the learned reversed process, starting from \(x_{t}=x\). Equivalently, one can formulate this property as requiring \(x_{t}\) to follow a reverse Martingale (see Lemma 3.1).

While there are several other properties that the score function of a diffusion process must satisfy, e.g. the Fokker-Planck equation , our first theoretical result is that the \(h_{}(x,t)\) satisfying the Consistency Property suffices (in conjunction with the conservativeness of its score function \(s_{}(x,t)=(h_{}(x,t)-x)/_{t}^{2}\)) to guarantee that \(s_{}\) must be the score function of a diffusion process (and must thus satisfy any other property that a diffusion process must satisfy). If additionally \(s_{}(x,t)\) equals the score function \(s^{*}(x,t)\) of a target diffusion process at a single time \(t=t_{0}\) and an open subset of \(x^{d}\), then it equals \(s^{*}\) everywhere. We comment that the formal theorem is proved for an idealistic setting when the error is (or approaches) zero. Still, it is likely to believe that even in the finite-error regime, training with DSM in-sample and enforcing CP off-sample is expected to improve the score function values off-sample. The statement is summarized as follows below:

**Theorem 1.1** (informal).: _If some denoiser \(h_{}(x,t)\) satisfies CP and its corresponding score function \(s_{}(x,t)=(h_{}(x,t)-x)/_{t}^{2}\) is a conservative field, then \(s_{}(x,t)\) is the score function of a diffusion process, i.e. the generation process using score function \(s_{}\), is the inverse of a diffusion process. If additionally \(s_{}(x,t)=s^{*}(x,t)\) for a single \(t=t_{0}\) and for all \(x\) in an open subset of \(^{d}\), where \(s^{*}\) is the score function of a target diffusion process, then \(s_{}(x,t)=s^{*}(x,t)\) everywhere._

Simply put, the above statement states that: i) satisfying CP and being a conservative vector field is enough to guarantee that the sampling process is the inverse of some diffusion process and ii) to learn the score function everywhere it suffices to learn it for a single \(t_{0}\) and an open subset of \(x\)'s.

We propose a loss function to train for the Consistency Property and we show experimentally that regularizing the standard score matching objective using our property leads to better models.

**Summary of Contributions:**

1. We identify an invariant property, consistency of the denoiser \(h_{}\), that any perfectly trained model should satisfy.
2. We prove that if the denoiser \(h_{}(x,t)\) satisfies CP and its implied score function \(s_{}(x,t)=(h_{}(x,t)-x)/_{t}^{2}\) is a conservative field, then \(s_{}(x,t)\) is the score function of _some_ diffusion process, even if there are learning errors with respect to the score of the target process, which generates the training data.
3. We prove that optimizing for the score in a subset of the domain and enforcing these two properties, guarantees that the score is learned correctly in all the domain, in the limit where the error approaches zero.
4. We propose a novel training objective that enforces the Consistency Property. Our new objective optimizes the network to have consistent predictions on data points from the _learned_ distribution.
5. We show experimentally that, paired with the original Denoising Score Matching (DSM) loss, our objective improves generation quality on conditional and unconditional generation on CIFAR-10, and in AFHQ and FFHQ.
6. We conduct an ablation study which showcases that even if we do not optimize for DSM for some values of \(t\), satisfying CP enforces good score approximation there.

## 2 Background

Diffusion processes, score functions and denoising.Diffusion models are trained by solving a supervised regression problem [49; 18]. The function that one aims to learn, called the score function (defined below), is equivalent (up to a linear transformation) to a denoising function [54; 15], whose goal is to denoise an image that was injected with noise. In particular, for some target distribution \(p_{0}\), one's goal is to learn the following function \(h^{d}^{d}\):

\[h(x,t)=_{x_{0} p_{0},\ x_{t}(x_{0},_{t}^{2} I_{d})}[x_{0} x_{t}=x]. \]

In other words, the goal is to predict the expected "clean" image \(x_{0}\) given a corrupted version of it, assuming that the image was sampled from \(p_{0}\) and its corruption was done by adding noise to it from \((0,_{t}^{2}I_{d})\), where \(_{t}^{2}\) is a non-negative and increasing function of \(t\). Given such a function \(h\), we can generate samples from \(p_{0}\) by solving a Stochastic Differential Equation (SDE) that depends on \(h\). Specifically, one starts by sampling \(x_{1}\) from some fixed distribution and then runs the following SDE backwards in time:

\[dx_{t}=-g(t)^{2},t)-x_{t}}{_{t}^{2}}dt+g(t)d_{t}, \]

where \(_{t}\) is a reverse-time Brownian motion3 and \(g(t)^{2}=^{2}}{dt}\). To explain how Eq. (3) was derived, consider the _forward_ SDE that starts with a clean image \(x_{0}\) and slowly injects noise:

\[dx_{t}=g(t)dB_{t},\;x_{0} p_{0}, \]

where \(B_{t}\) is a standard Brownian motion. We notice here that the \(x_{t}\) under Eq. (4) is \((x_{0},_{t}^{2}I_{d})\), where \(x_{0} p_{0}\), so it has the same distribution as in Eq. (2). Remarkably, such SDEs are reversible in time . Hence, the diffusion process of Eq. (4) can be viewed as a reversed-time diffusion:

\[dx_{t}=-g(t)^{2}_{x} p(x_{t},t)dt+g(t)d_{t}, \]

where \(p(x_{t},t)\) is the density of \(x_{t}\) at time \(t\). We note that \(s(x,t):=_{x} p(x,t)\) is called the _score function_ of \(x_{t}\) at time \(t\). Using Tweedie's lemma , one obtains the following relationship between the denoising function \(h\) and the score function:

\[_{x} p(x,t)=^{2}}. \]

Substituting Eq. (6) in Eq. (5), one obtains Eq. (3).

Training via denoising score matching.The standard way to train \(h_{}\) is via _denoising score matching_. This is performed by obtaining samples of \(x_{0} p_{0}\) and \(x_{t}(x_{0},_{t}^{2}I_{d})\) and training to minimize

\[_{x_{0} p_{0},x_{t}(x_{0},_{t}^{2}I_{d})} \,L_{t,x_{t},x_{0}}^{}()=_{x_{0} p_{0},x_{t} (x_{0},_{t}^{2}I_{d})}\|h_{}(x_{t},t)-x_{0} \|^{2},\]

where the optimization is over some family of functions, \(\{h_{}\}_{}\). It was shown by Vincent  that optimizing Eq. (2) is equivalent to optimizing \(h_{}\) in mean-squared-error on a random point \(x_{t}\) that is a noisy image, \(x_{t}(x_{0},_{t}^{2}I_{d})\) where \(x_{0} p_{0}\):

\[_{x_{t}}\|h_{}(x_{t},t)-h^{*}(x_{t},t)\|^{2},\]

where \(h^{*}\) is the true denoising function from Eq. (2).

## 3 Theory

We define below the Consistency Property that a function \(h\) should satisfy. Simply put, it states that the output of \(h(x,t)\) (which is meant to approximate the conditional expectation of \(x_{0}\) conditioned on \(x_{t}=x\)) is consistent with the average point \(x_{0}\) generated using \(h\) and conditioning on \(x_{t}=x\). Recall from the previous section that generation according to \(h\) conditioning on \(x_{t}=x\) is done by running the following SDE backwards in time conditioning on \(x_{t}=x\):

\[dx_{t}=-g(t)^{2},t)-x_{t}}{_{t}^{2}}dt+g(t)^{2}d_{t}, \]

CP is therefore defined as follows:

_Property \(1\)_ (**Consistency Property**.). A function \(h^{d}^{d}\) is said to satisfy _CP_ iff for all \(t(0,1]\) and all \(x^{d}\),

\[h(x,t)=_{h}[x_{0} x_{t}=x], \]

where \(_{h}[x_{0} x_{t}=x]\) corresponds to the conditional expectation of \(x_{0}\) in the process that starts with \(x_{t}=x\) and samples \(x_{0}\) by running the SDE of Eq. (7) backwards in time (where note that the SDE uses \(h\)).

The following lemma states that Property 1 holds if and only if the model prediction, \(h(x,t)\), \(h(x_{t},t)\) is a reverse-Martingale under the same process of Eq. (7).

**Lemma 3.1**.: _Property 1 holds if and only if the following two properties hold:_

* _The function_ \(h\) _is a reverse-Martingale, namely: for all_ \(t>t^{}\) _and for any_ \(x\)_:_ \[h(x,t)=_{h}[h(x_{t^{}},t^{}) x_{t}=x],\] _where the expectation is over_ \(x_{t^{}}\) _that is sampled according to Eq. (_7_) with the same function_ \(h\)_, given the initial condition_ \(x_{t}=x\)_._
* _For all_ \(x^{d}\)_,_ \(h(x,0)=x\)_._

The proof of this lemma is included in Section B.2. Further, we introduce one more property that will be required for our theoretical results: the learned vector-field should be conservative.

_Property \(2\)_ **(Conservative vector field / Score Property.)**.: Let \(h^{d}^{d}\). We say that \(h\) induces a _conservative vector field_ (or that satisfies the score property) if for any \(t(0,1]\) there exists some probability density \(p(,t)\) such that

\[^{2}}= p(x,t).\]

We note that the optimal denoiser, i.e., \(h\), defined as in Eq. (2), satisfies both of the properties we introduced. In the paper, we will focus on enforcing CP and we are going to assume conservativeness for our theoretical results. This assumption can be relaxed to hold only at a _single_\(t(0,1]\) using the results of Lai et al. .

Next, we show the theoretical consequences of enforcing Properties 1 and 2. First, we show that this enforces \(h\) to indeed correspond to a denoising function, namely, \(h\) satisfies Eq. (2) for some distribution \(p_{0}^{}\) over \(x_{0}\). However, this does not imply that \(p_{0}\) is the _correct_ underlying distribution that we are trying to learn. Indeed, these properties can apply to any distribution \(p_{0}\). Yet, we can show that if we learn \(h\) correctly for some inputs and if these properties apply everywhere then \(h\) is learned correctly everywhere.

**Theorem 3.2**.: _Let \(h^{d}^{d}\) be a bounded continuous function. Then:_

1. _The function_ \(h\) _satisfies both Properties_ 1 _and_ 2 _if and only if_ \(h\) _is defined by Eq. (_2_) for some distribution_ \(p_{0}\)_._
2. _Assume that_ \(h\) _satisfies Properties_ 1 _and_ 2_. Further, let_ \(h^{*}\) _be another function that corresponds to Eq. (_2_) with some initial distribution_ \(p_{0}^{*}\)_. Assume that_ \(h=h^{*}\) _on some open set_ \(U^{d}\) _and some fixed_ \(t_{0}(0,1]\)_, namely,_ \(h(x,t_{0})=h^{*}(x,t_{0})\) _for all_ \(x U\)_. Then,_ \(h^{*}(x,t)=h(x,t)\) _for all_ \(x\) _and all_ \(t\)_._

_Remark 3.3_.: While our theorem uses Eq. (2) which describes the VE-SDE (Variance Exploding SDE), it is also valid for VP-SDE (Variance Preserving SDE), as these two SDEs are equivalent up to appropriate scaling (see e.g. [31; 28]).

## 4 Method

Theorem 3.2 motivates enforcing CP on the learned model. We notice that the CP Equation Eq. (8) may be expensive to train for, because it requires one to generate whole trajectories. Rather, we use the equivalent Martingale assumption of Lemma 3.1, which can be observed locally with only partial trajectories4. We suggest the following loss function, for some fixed \(t,t^{}\) and \(x\):

\[L^{}_{t,t^{},x}()=||_{ }[h_{}(x_{t^{}},t^{}) x_{t}=x]-h_{}(x,t) ||^{2},\]

where the expectation \(_{}[ x_{t}=x]\) is taken according to process Eq. (7) parameterized by \(h_{}\) with the initial condition \(x_{t}=x\). Differentiating this expectation, one gets the following (see Section B.1

[MISSING_PAGE_EMPTY:6]

resources, which increased the FID from \(1.96\) (reported value) to \(2.29\). All models were trained for \(200\)k iterations, as in Karras et al. . Finally, we retrain a baseline model on FFHQ for \(150\)k iterations and we finetune it for \(5\)k steps using our proposed objective.

Implementation Choices and Computational Requirements.As mentioned, when enforcing CP, we are free to choose \(t^{}\) anywhere in the interval \([0,t]\). When \(t,t^{}\) are far away, sampling \(x^{}_{t}\) from the distribution \(p^{}_{t^{}}(x^{}_{t}|x_{t})\) requires many sampling steps (to reduce discretization errors). Since this needs to be done for every Gradient Descent update, the training time increases significantly. Instead, we notice that local consistency implies global consistency. Hence, we first fix the number of sampling steps to run in every training iteration and then we sample \(t^{}\) uniformly in the interval \([t-,t]\) for some specified \(\). For all our experiments, we fix the number of sampling steps to 6 which roughly increases the training time needed by \(1.5\)x. We train all our models on a DGX server with 8 A\(100\) GPUs with \(80\)GBs of memory each.

### Consistency Property Testing

We are now ready to present our results. The first thing that we check is whether regularizing for CP actually leads to models that are more consistent with their predictions, as the property implies. Specifically, we want to check that the model trained with \(L^{}_{}\) achieves lower Consistency error, i.e. lower \(L^{}_{t,t^{},x_{t}}\). To check this, we do the following two tests: i) we fix \(t=1\) and we show how \(L^{}_{t,t^{},x_{t}}\) changes as \(t^{}\) changes in \(\), ii) we fix \(t^{}=0\) and we show how the loss is changing as you change \(t\) in \(\). Intuitively, the first test shows how the violation of CP splits across the sampling process and the second test shows how much you finally (\(t^{}=0\)) violate the property if the violation started at time \(t\). The results are shown in Figures 0(a), 0(b), respectively, for the models trained on AFHQ. We include additional results for CIFAR-10, FFHQ in Figures 4, 5, 6, 7 of the Appendix. As shown, indeed regularizing for the CP Loss drops the \(L^{}_{t,t^{},x_{t}}\) as expected. See Section C.1 for additional details and discussion.

Performance.We evaluate the performance of our models. Following the methodology of Karras et al. , we generate \(150\)k images from each model and we report the minimum FID computed on three sets of \(50\)k images each. We keep checkpoints during training and we report FID for \(30\)k, \(70\)k, \(100\)k, \(150\)k, \(180\)k and \(200\)k iterations in Table 1. We also report the best FID found for each model, after evaluating checkpoints every 5k iterations (i.e. we evaluate \(40\) models spanning \(200\)k steps of training). As shown in the Table, the proposed CP regularization yields improvements throughout the training. In the case of CIFAR-10 (conditional and unconditional) where the re-trained baseline was trained with exactly the same hyperparameters as the models in the EDM  paper,

Figure 1: Consistency Property Testing on AFHQ.

our CDM models achieve a new state-of-the-art. We further show that our CP regularization can be applied on top of a pre-trained model. Specifically, we train a baseline EDM-VP model on FFHQ \(64 64\) for \(150\)k using vanilla Denoising Score Matching. We then do \(5\)k steps of finetuning, with and without our CP regularization and we measure the FID score of both models. The baseline model achieves FID \(2.68\) while the model finetuned with CP regularization achieves \(2.61\). This experiment shows the potential of applying our CP regularization to pre-trained models, potentially even at large scale, e.g. we could apply this idea with text-to-image models such as Stable Diffusion . We leave this direction for future work.

Uncurated samples from our best models on AFHQ, CIFAR-10 and FFHQ are given in Figures 1(a), 1(b) and 8. One benefit of the deterministic samplers is the unique identifiability property . Intuitively, this means that by using the same noise and the same deterministic sampler, we can directly compare

  Model & 30k & 70k & 100k & 150k & 180k & 200k & Best \\ 
**CDM-VP (Ours)** & & **3.00** & 2.44 & **2.30** & **2.31** & **2.25** & **2.44** & 2.21 \\ EDM-VP (retrained) & & 3.27 & **2.41** & 2.61 & 2.43 & 2.29 & 2.61 & 2.26 \\ EDM-VP (reported)\({}^{*5}\) & AFHQ & & & & & & & **1.96** \\ NCSNv3-VP (reported)\({}^{*}\) & & & & & & & & 2.16 \\ NCSNv3-VE (reported)\({}^{*}\) & & & & & & & & 2.58 \\ NCSNv3-VE (reported)\({}^{*}\) & & & & & & & & 18.52 \\ 
**CDM-VP (Ours)** & & **2.44** & **1.94** & **1.88** & 1.88 & **1.80** & **1.82** & **1.77** \\ EDM-VP (retrained) & & 2.50 & 1.99 & 1.94 & **1.85** & 1.86 & 1.90 & 1.82 \\ EDM-VP (reported) & CIFAR-10 & & & & & & & 1.79 \\ EDM-VE (reported) & (cond.) & & & & & & & 1.79 \\ NCSNv3-VP (reported) & & & & & & & & 2.48 \\ NCSNv3-V (reported) & & & & & & & & 3.11 \\ 
**CDM-VP (Ours)** & & **2.83** & **2.21** & **2.14** & **2.08** & **1.99** & **2.03** & **1.95** \\ EDM-VP (retrained) & & 2.90 & 2.32 & 2.15 & 2.09 & 2.01 & 2.13 & 2.01 \\ EDM-VP (reported) & CIFAR-10 & & & & & & & 1.97 \\ EDM-VE (reported) & (uncond.) & & & & & & & 1.98 \\ NCSNv3-VP (reported) & & & & & & & & 3.01 \\ NCSNv3-VE (reported) & & & & & & & & 3.77 \\ 
**CDM-VP (finetuned)** & & & & & & & & **2.61** \\ EDM-VP (retrained) & & & & & & & & 2.68 \\  

Table 1: FID results for deterministic sampling, using the Karras et al.  second-order samplers. For the CIFAR-10 models, we do \(35\) function evaluations and for AFHQ \(79\).

Figure 2: Comparison of uncurated images generated by two different models.

visually models that might have been trained in completely different ways. We select a couple of images from Figure 1(a) (AFHQ generations) and we compare the generated images from our model with the ones from the EDM baseline for the same noises. The results are shown in Figure 3. As shown, the CP regularization fixes several geometric inconsistencies for the picked images. We underline that the shown images are examples for which CP regularization helped and that potentially there are images for which the baseline models give more realistic results.

We note that an appropriate regularization parameter \(\) for our consistency loss has to be chosen. We found that an excessively large \(\) harms the performance: while the CP regularization enforces the model to obey _some_ diffusion process, it does not enforce it to obey the _true_ diffusion process, and a large \(\) might disturb with the score-matching signal.

**Ablation Study for Theoretical Predictions.** One interesting implication of Theorem 3.2 is that it suggests that we only need to learn the score perfectly on some fixed \(t_{0}\) and then the CP implies that the score is learned everywhere (for all \(t\) and in the whole space). This motivates the following experiment: instead of using as our loss the weighted sum of DSM and our CP regularization for all \(t\), we will not use DSM for \(t t_{}\), for some \(t_{}\) that we test our theory for.

We pick \(t_{}\) such that for \(20\%\) of the diffusion (on the side of clean images), we do not train with DSM. For the rest \(80\%\) we train with both DSM and our CP regularization. Since this is only an ablation study, we train for only \(10\)k steps on (conditional) CIFAR-10. We report FID numbers for three models: i) training with only DSM, ii) training with DSM and CP regularization everywhere, iii) training with DSM for \(80\%\) of times \(t\) and CP regularization everywhere. In our reported models, we also include FID of an early stopped sampling of the latter model, i.e. we do not run the sampling for \(t<t_{}\) and we just output \(h_{}(x_{t_{}},t_{})\). The numbers are summarized in Table 2. As shown, the theory is predictive since early stopping the generation at time \(t\) gives significantly worse results than continuing the sampling through the times that were never explicitly trained for approximating the score (i.e. we did not use DSM for those times). That said, the best results are obtained by combining DSM and our CP regularization everywhere, which is what we did for all the other experiments in the paper.

## 6 Related Work

The fact that imperfect learning of the score function introduces a shift between the training and the sampling distribution has been well known. Chen et al. [6; 7] analyze how the \(l_{2}\) error in the approximation of the score function propagates to Total Variation distance error bounds between the true and the learned distribution. Several methods for mitigating this issue have been proposed, but the majority of the attempts focus on changing the sampling process [51; 28; 24; 46]. A related work is the Analog-Bits paper  that conditions the model during training with past model predictions.

 Model & FID \\  EDM (baseline) & 5.81 \\  CDM, all times \(t\) & \(5.45\) \\  CDM, for some \(t\) & \(6.59\) \\  CDM, for some \(t\) & 14.52 \\ early stopped sampling & \\ 

Table 2: Ablation study on removing the DSM loss for some \(t\). Table reports FID results after \(10\)k steps of training on CIFAR-10.

Figure 3: Visual comparison of EDM model (top) and CDM model (Ours, bottom) using deterministic sampling initiated with the same noise. As seen, the CP regularization fixes several geometric inconsistencies and artifacts in the generated images. In order to obtain this comparison, we generated 64 images both with EDM and CDM. We found that in 14 of these images, CDM provided considerable improvement and for the remainder there was no significant difference.

Karras et al.  discusses potential violations of invariances, such as the non-conservativity of the induced vector field, due to imperfect score matching. However, they do not formally test or enforce this property. Chao et al.  shows that failure to satisfy the conservativity property can harm the performance, and they propose a modification to relieve this degradation. Lai et al.  study the problem of regularizing diffusion models to satisfy the Fokker-Planck equation. While we show in Theorem 3.2 that perfect conservative training enforces the Fokker-Planck equation, we notice that their training method is different: they suggest enforcing the equation locally by using the finite differences method to approximate the derivatives. Further, they do not train on drifted data. Instead, we notice that our CP loss is well suited to handle drifted data since it operates across trajectories generated by the model.

A concurrent work by Song et al.  proposes Consistency Models (CM), a new class of generative models that output directly the solution of the Probability Flow ODE. This idea resembles the consistency in the model outputs that we enforce through CP, but the motivation is different: CM attempts to accelerate sampling and we attempt to improve generation quality. The two methods are similar in that they enforce a property that the model should satisfy. In the case of the CM, the ODE solver should produce the same solution when evaluated at points that belong to the same trajectory. Our property is that on expectation the predictions should not be changing for points that have the same origin. One way to view it is that CM applies our consistency condition but on the deterministic sampler (for which the expectation becomes the point itself). Subsequent work by Lai et al.  compares our CDM to Consistency Models  and to Fokker Planck regularization .

## 7 Conclusions and Future Work

We proposed an objective that enforces the trained network to follow a reverse Martingale, thereby having self-consistent predictions over time. We optimize this objective with points from the sampling distribution, effectively reducing the sampling drift observed in prior empirical works. Theoretically, we show that CP implies that we are sampling from the reverse of some diffusion process. Together with the assumption that the network has learned the score correctly in a subset of the domain, we can prove that CP (together with conservativity of the vector field) implies that the score is learned correctly everywhere - in the limit where the error approaches zero. Empirically, we use our objective to obtain state-of-the-art for CIFAR-10 and baseline improvements on AFHQ and FFHQ.

There are limitations of our method and several directions for future work. The proposed regularization increases the training time. It would be interesting to explore how to enforce CP in more effective ways in future work. Further, our method does not test nor enforce that the induced vector-field is conservative, which is a key theoretical assumption. Our method guarantees only indirectly improve the performance in the samples from the learned distribution by enforcing some invariant. Finally, our theoretical result holds in the limit where the error of our regularized objective approaches zero and it would be meaningful to theoretically study also the constant-error regime.

## 8 Acknowledgments

This research has been supported by NSF Grants CCF 1763702, AF 1901292, CNS 2148141, Tripods CCF 1934932, IFML CCF 2019844, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco and the Archie Straiton Endowed Faculty Fellowship. Giannis Daras has been supported by the Onassis Fellowship, the Bodossaki Fellowship and the Leventis Fellowship. Constantinos Daskalakis has been supported by NSF Awards CCF-1901292, DMS-2022448 and DMS2134108, a Simons Investigator Award, the Simons Collaboration on the Theory of Algorithmic Fairness and a DSTA grant.