# Understanding Mode Connectivity via

Parameter Space Symmetry

 Bo Zhao

University of California San Diego

bozhao@ucsd.edu

&Nima Dehmamy

IBM Research

nima.dehmamy@ibm.com

Robin Walters

Northeastern University

r.walters@northeastern.edu

&Rose Yu

University of California San Diego

roseyu@ucsd.edu

###### Abstract

It has been observed that the global minimum of neural networks is connected by curves on which train and test loss is almost constant. This phenomenon, often referred to as mode connectivity, has inspired various applications such as model ensembling and fine-tuning. Despite empirical evidence, a theoretical explanation is still lacking. We explore the connectedness of minimum through a new approach, parameter space symmetry. By relating topology of symmetry groups to topology of minima, we provide the number of connected components of full-rank linear networks. In particular, we show that skip connections reduce the number of connected components. We then prove mode connectivity up to permutation for linear networks. We also provide explicit expressions for connecting curves in minimum induced by symmetry.

## 1 Introduction

Among recent studies on the loss landscape, a particularly interesting discovery is mode connectivity , which refers to the phenomenon that distinct minima found by stochastic gradient descent (SGD) can be connected by continuous paths through the high-dimensional parameter space of neural networks. Mode connectivity has implications on other phenomena in deep learning such as the lottery ticket hypothesis  and loss landscape and training trajectory analysis . Additionally, mode connectivity has inspired applications in diverse fields, including model ensembling , model averaging , pruning , improving adversarial robustness , and fine-tuning for altering prediction mechanism .

Discrete symmetry, especially permutation, is well-known to be related to mode connectivity. In particular, the neural network output is invariant to permuting the neurons .  conjectures that all minima found by SGD are linearly connected up to permutation. Various algorithms have since been developed to find the optimal permutation for linear mode connectivity . However, compared to discrete symmetry, the role of continuous symmetry remains less studied. Continuous symmetry groups with continuous actions define positive dimensional connected spaces in the minimum . We explore the connectedness of minimum through continuous symmetries in the parameter space.

We reveal the role of symmetry in the connectivity of minimum by relating properties of topological groups to their orbits and the minimum. Our results show that both continuous and discrete symmetry are important and useful in understanding the origin and failure cases of mode connectivity. Our work highlights a new approach towards understanding the topology of the minimum and complements previous theories on mode connectivity .

Connectedness of minima

### Linear network with invertible weights

Let **Param** be the space of parameters. Consider the multi-layer loss function \(L:\),

\[L:, 56.905512pt(W_{1},...,W_{l}) ||Y-W_{1}...W_{1}X||_{2}^{2}. \]

where \(X,Y^{h h}\) are the input and output of the network. In this subsection, we assume that both \(X,Y\) have rank \(h\), and \(=(^{h h})^{l}\). Then \(L\) has a \(GL_{h}()^{l-1}\) symmetry, which acts on **Param** by \(g(W_{1},...,W_{l})=(g_{1}W_{1},g_{2}W_{2}g_{1}^{-1},...,g_{l-1}W_{l-1}g_{ l-2}^{-1},W_{l}g_{l-1}^{-1})\), for \((g_{1},...,g_{l-1}) GL_{h}()^{l-1}\).

Let \(L^{-1}(c)=\{:L()=c\}\) be a level set of \(L\). Since \(\|\|_{2} 0\) and \(L^{-1}(0)\), the minimum value of \(L\) is 0. By relating the topology of \(GL()\) and \(L^{-1}(0)\), we have the following observations on the structure of the minimum of \(L\).

**Proposition 2.1**.: _There is a homeomorphism between \(L^{-1}(0)\) and \((_{h})^{l-1}\)._

Since \((_{h})^{l-1}\) has \(2^{l-1}\) connected components and homeomorphism preserves topological properties, \(L^{-1}(0)\) also has \(2^{l-1}\) connected components.

**Corollary 2.2**.: _The minimum of \(L\) has \(2^{l-1}\) connected components._

### ResNet with 1D weights

The topological properties of the minimum depend on the architecture. As an example of this dependency, we show that adding a skip connection changes the number of connected components of the minimum.

Consider a residual network \(W_{3}(W_{2}W_{1}X+ X)\) and loss function

\[L(W_{3},W_{2},W_{1})=||Y-W_{3}(W_{2}W_{1}X+ X)||_{2}, \]

where \((W_{1},W_{2},W_{3})=^{n n}^{n  n}^{n n}\), \(\), and data \(X^{n n},Y R^{n n}\). The following proposition states that for a three-layer residual network with weight matrices of dimension \(1 1\), the number of components of the minimum is smaller than that of a linear network without the skip connection.

**Proposition 2.3**.: _Let \(n=1\). Assume that \(X,Y 0\). When \(=0\), the minimum of \(L\) has 4 connected components. When \( 0\), the minimum of \(L\) has 3 connected components._

The \(=0\) case follows from Corollary 2.2. For the \( 0\) case, the proof decomposes the minimum of \(L\) into two sets \(S_{1}\) and \(S_{0}\), corresponding to the minima without the skip connection and an extra set of solutions because of the skip connection. \(S_{1}\) is homeomorphic to \(GL_{1} GL_{1}\) and has 4 connected components. \(S_{0}\) is a line and has 1 connected component. Two components of \(S_{1}\) are connected to \(S_{0}\), while the other two components of \(S_{1}\) are not. Therefore, \(S_{0}\) connects two components of \(S_{1}\). As a result, the minimum of \(L\) has 3 connected components. Full proof can be found in Appendix C.3.

Figure 1 visualizes the minimum without and with the skip connection. This result reveals the effect of skip connection on the connectedness of minimum, which may lead to a new explanation of the effectiveness of ResNets  and DenseNets . We leave the connection between the topology of minimum and the optimization and generalization property of neural networks to future work.

## 3 Mode connectivity

From the examples in the previous section, the connectedness of the minimum is related to the symmetry of the loss function under certain conditions. In this section, we explore applications of this insight in explaining mode connectivity.

### Mode connectivity up to permutation

For the family of linear neural networks defined in Section 2.1, we show that permutation allows us to connect points in the minimum that are not connected without permutation. Our results support the empirical observation that neuron alignment by permutation improves mode connectivity .

[MISSING_PAGE_EMPTY:3]

[MISSING_PAGE_FAIL:4]