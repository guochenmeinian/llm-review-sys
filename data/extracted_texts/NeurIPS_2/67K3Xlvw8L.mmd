# Alignment for Honesty

Yuqing Yang\({}^{3,5}\) Ethan Chern\({}^{1,5}\) Xipeng Qiu\({}^{3}\) Graham Neubig\({}^{4}\) Pengfei Liu\({}^{1,2,5}\)

\({}^{1}\)Shanghai Jiao Tong University \({}^{2}\)Shanghai Artificial Intelligence Laboratory

\({}^{3}\)Fudan University \({}^{4}\)Carnegie Mellon University

\({}^{5}\)Generative AI Research Lab (GAIR)

yuqingyang21@m.fudan.edu.cn ethanicchern@gmail.com

xpqiu@fudan.edu.cn gneubig@cs.cmu.edu pengfei@sjtu.edu.cn

Corresponding author.

###### Abstract

Recent research has made significant strides in aligning large language models (LLMs) with helpfulness and harmlessness. In this paper, we argue for the importance of alignment for _honesty_, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning an LLM's knowledge boundaries, which demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. We address these challenges by first establishing a precise problem definition and defining "honesty" inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM's honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source all relevant resources to facilitate future research at [https://github.com/GAIR-NLP/alignment-for-honesty](https://github.com/GAIR-NLP/alignment-for-honesty).

## 1 Introduction

A pivotal factor that contributes to the success of current large language models (LLMs) (Brown et al., 2020; OpenAI, 2023a; Anil et al., 2023) is the process of alignment (Kenton et al., 2021; Ouyang et al., 2022), which aims to ensure that LLMs adhere to human values and intentions. The key principles of alignment are often summarized as the "HHH" criteria: helpful, harmless, honest (Askell et al., 2021). There has been a significant focus on enhancing the helpfulness and harmlessness of LLMs (Bai et al., 2022, 2022, 2022). However, _honesty_, despite its importance in establishing reliable and safe AI (Kaddour et al., 2023; Liu et al., 2023; Park et al., 2023), has received relatively less attention in research (i.e., Evans et al. (2021); Kadavath et al. (2022); Cui et al. (2023)). There are several primary challenges in improving the honesty of models.

The first challenge is that there is a long-standing debate regarding the very definition of "honesty" for AI models (Mahon, 2015; Yudkowsky, 2018). Essentially, honesty demands the model to be faithful to its own level of knowledge and express it candidly (Askell et al., 2021; Schulman, 2023). In this paper, we define "honesty" based on the spirit of Confucius and Disciple (1 BC): _an honest model should candidly answer questions it knows and humbly admit to those it does not_, as illustrated in Fig. 1. Some research emphasizes calibration (Lin et al., 2022; Cui et al., 2023), which requires the model to convey a certain degree of uncertainty in its responses and can be seen as a finer-grained handling of known questions.

Another challenge lies in distinguishing the knowledge boundaries of a specific LLM - discerning between what is known and unknown. The impracticality of this task stems both from the lack of transparency in most LLMs regarding their pretraining data, and from the inability of models, even those perfectly fitted to their training data, to utilize this knowledge flexibly and accurately in response to factual questions (Zhu and Li, 2023; Allen-Zhu and Li, 2023). As a result, we shift our focus from "knowledge" to "questions" and determine whether a certain model should abstain from answering a question based on its capability to provide the correct answer to that question.

Based on the above definitions, we propose a systematic framework for alignment for honesty. First, we formalize the problem definition. We introduce the concept of "I don't know (idk) responses" and in this context, honesty necessitates that an aligned LLM provides idk responses for unknown questions and correct responses for known questions. Then, to more precisely identify the model's knowledge boundaries and evaluate the effectiveness of the alignment process in terms of honesty, we define evolutionary metrics, which includes a _prudence score_ and a _over-conservativeness score_ to measure the model's capability to appropriately decline answering questions beyond its knowledge. We also propose methods to perform alignment for honesty. We find that prompts alone are not sufficient and thus put forth several straightforward yet effective honesty-oriented supervised fine-tuning methods. Through extensive experiments, we demonstrate the feasibility and generalization of our proposed methods across various knowledge-intensive question-answering tasks. Meanwhile, they do not significantly reduce the helpfulness of the model, indicating a low "tax" on alignment for honesty.

Reiterating, instead of simply proposing a new training method for alignment, our work aims to contribute to this field in the following ways:

(1) Clarify different concepts SSA, delineate the battlegrounds that require attention to aligning LLMs with honesty, and identify core challenges SS2.3.

(2) Propose methods for identifying the boundaries between known and unknown aspects of models through external approximation SS2.2, which not only allows us to develop specialized metrics for honesty alignment but also opens the door to more precise approximations in future research.

(3) Present various automated approaches for synthesizing data to align with honesty, transforming it into a problem defined by different feature functions SS3.2. This provides a broad spectrum of possibilities for subsequent research.

(4) Establish a comprehensive evaluation framework that encompasses not only in-domain assessments SS4.4 but also generalization analyses based on specially constructed data SS4.5, as well as alignment tax analyses SS4.6.

## 2 Problem Formulation

Pre-training and _iterative alignment_(Touvron et al., 2023; Li et al., 2023c) of LLMs are increasingly becoming the standard technical workflow for LLM training. Below, we first formulate the general "alignment" process in LLMs and then motivate alignment for honesty.

### LLM Alignment

Response GenerationGiven an input \(x\) and a large language model \(M_{t}\) at the \(t^{th}\) iteration of alignment, the generation process of the response \(y\) could be described as \(y_{t}=M_{t}(x)\).

Figure 1: Illustration of alignment for honesty. Given a knowledge-based question, an aligned model is expected to provide the correct answer if it has knowledge of the question, or alternatively, refuses to answer the question.

Value JudgingThis process defines a value function \(v()\) that aims to map a model response \(y\) generated from the input \(x\) into a quantifiable number measuring how well the model's output aligns with values defined by humans. For example, if the target of alignment is "harmlessness", then one desirable definition of \(v()\) is:

\[v(x,y)=1,&,\\ 0,&. \]

\(v()\) is measured either through human annotation (Ouyang et al., 2022) or a proxy model (Gao et al., 2023) that is usually learned based on human preferences, as illustrated in Fig. 2-(b).

Iterative AlignmentTo better align with human values quantified by \(v()\), the model will be optimized iteratively as depicted in Fig. 2-(a):

\[M_{t+1}=M_{0},&,\\ f(M_{t},v()),&, \]

where \(M_{0}\) denotes a pre-trained large language model without alignment (e.g., LLaMA2 base version). \(f()\) represents an alignment strategy such as supervised fine-tuning.

Note that, in this context, "iteration" does not refer to the different training epochs within a single training session, but rather signifies the completion of one alignment training cycle for the model, i.e., one version of the model. For instance, the final version of LLaMA2-Chat is the result of five successive versions: \(M_{1},,M_{5}\)(Touvron et al., 2023).

### Alignment for Honesty

It is often challenging to understand the model's internal workings, i.e., whether knowledge is _known_ or _unknown_, as outlined in Fig. 2-(c). However, what we can access is the model's external behaviors in terms of answering _correctly_ or _incorrectly_. Hence, we approximate the model's internal knowledge through the accuracy of its responses.2

Based on the correctness of model responses, we define the following categorization:

\[c(x,y)=-1,&(y)=$},\\ 1,&(y)=$},\\ 0,&(y)=$}, \]

where

* "\((y)=\) (I don't know)" when a response \(y\) contains "idk signs", such as "I'm not able to", "I'm not familiar with", etc. It signifies the model's inability to provide the correct answer \(a\) to the question.
* "\((y)=\)" when a response \(y\) does not contain idk signs and the correct answer \(a\) is a substring of the response \(y\).
* "\((y)=\)" when a response \(y\) does not contain idk signs and \(a\) is not included in \(y\).

Figure 2: (a) Illustration of iterative alignment. The large language model \(M\) evolves iteratively for better alignment with a given human value. (b) Decision boundary for “harmless”, which is commonly defined by human “\(\)”. (c) Decision boundary for “known”, which is usually determined by model “\(\)”.

Then the value function for honesty can be defined as:

\[v(x,y)=1,&k(x) c(x,y)=1,\\ 0,&, \]

where \(k()\) is a function that judges if a model \(M_{t}\) knows the answer to input \(x\). \(k()\) is either 1 or -1, and thus when the question is unknown, \(k(x) c(x,y)\) is 1 if the model chooses idx explicitly.

As mentioned earlier, providing an accurate definition of whether a model knows or does not know a particular piece of knowledge is a non-trivial matter. However, by utilizing the definition of the categorization function \(c()\), we can approximate the model's level of understanding regarding specific questions. For example, \(k(x)=(c(x,y)=1)\). We will explore different definitions of \(k()\) in SS3.2.

### Evaluation Methodology

There are also challenges in assessing the degree of alignment in language models. For instance, are aligned models more willing to admit their limitations? Can aligned models become excessively conservative in pursuit of honesty, and how can this tendency be quantitatively characterized?

To answer these questions, we develop an evaluation framework in which a wide variety of _evolutionary metrics_ can be defined to evaluate the differences before and after alignment for honesty from different aspects. Intuitively, alignment is an evolving process for models (i.e., from \(M_{t}\) to \(M_{t+1}\), and we denote \(M_{t}\) as the unaligned model in terms of honesty, regardless of possibly undergoing \(t^{th}\) round of alignment for other values), making it natural to compare model changes before and after alignment.

We first extend \(c()\) into a second order form \(c(x,y_{t},y_{t+1})=(c(x,y_{t}),c(x,y_{t+1}))\), where \(y_{t}\) and \(y_{t+1}\) represent responses generated by model \(M_{t}\) and aligned version \(M_{t+1}\).3 Tab. 1 enumerates all value cases of \(c(x,y_{t},y_{t+1})\).

Given an evaluation dataset \(D\), we denote \(N\) as the number of test samples, and let \(N_{}=|\{y|(y)=\}|\). Based on the above explanations, we design some quantifiable metrics.

Prudence ScoreThis metric is used to characterize the extent to which the model can humbly decline to answer questions it does not know or answer incorrectly. A fundamental trait of a model aligned with honesty is its ability to acknowledge its limitations and thus refrain from answering questions beyond its knowledge. In this context, we define the "prudence score" to assess this particular ability, defined by calculating the statistics in the blue region as shown in Tab. 1. Formally,4

\[S_{}=+N_{}}{N_{}+N_{}+N_{}}. \]

Over-Conservativeness ScoreThis metric is used to characterize the extent to which the model, after alignment operations, refuses to answer questions that it should originally be able to answer correctly. When the model is allowed to respond with "I don't know" to certain questions, it may become excessively cautious. This means it might avoid answering questions it actually knows the answers to, opting instead to decline them. We introduce the "over-conservativeness score" (abbreviated as "over-consv. score") to quantify this, which can be defined by calculating the statistics in the red region as shown in Tab. 1. Formally,5

\[S_{}=}{N_{}+N_{}+N_{}}. \]

   t+1 & t & 1 (correct) & 0 (wrong) & -1 (idk) \\ 
1 (correct) & 1 & 2 & 3 \\
0 (wrong) & 4 & 5 & 6 \\ -1 (idk) & 7 & 8 & 9 \\   

Table 1: Change in model’s response type before (\(t\)) and after (\(t+1\)) alignment for honesty. Take a “\(}\)” response as an example: the model \(M_{t}\) is capable of providing the correct answer to the question, yet \(M_{t+1}\) refrains from doing so, which implies that the aligned model may display an excessive level of caution.

Honesty ScoreBased on the aforementioned definitions, we can comprehensively consider both the model's ability to refuse to answer and its ability _not_ to be excessively cautious, in order to quantitatively measure the degree of honesty in the model post-alignment. Formally,

\[S_{}=(S_{}+(1-S_{})). \]

In Tab. 1, the 1 and 1 represent cases where alignment operations result in previously incorrect or unknown questions being answered correctly. There are several factors contributing to this improvement, such as alignment enabling the model to correctly answer questions it already knew the answers to (Burns et al., 2023; Li et al., 2023; Joshi et al., 2023), or the introduction of new knowledge through parameter co-adaptation during the training process. In this work, we do not focus on this aspect, but it could be a promising area for future research. Similarly, the 1 represent cases where the model provides wrong answers to questions that it could have answered correctly. We do not set a metric for it here since the model performance can decrease during the alignment process (i.e., catastrophic forgetting, Lin et al. (2024); Shumailov et al. (2023)), which should be disentangled from the concept of dishonesty. Instead, we propose using _accuracy_(Joshi et al., 2017) to measure whether the alignment process disrupts the model's original abilities.

Finally, we note that after the introduction of idx responses, we observe a small probability of the model using idx signs as an indication of uncertainty and providing the correct answer at the same time. We categorize all responses that contain the correct answers (whether or not they include idx signs) as "loosely correct". Then, accuracy is calculated as the ratio of samples with loosely correct responses to the total number of samples:

\[=}}{N}. \]

## 3 Training Methodology

This section will present different methods to perform alignment so that a model \(M_{t}\) becomes a more aligned model \(M_{t+1}\) in terms of honesty as defined in Eq. 2.

### Training-free Method

One intuitive method is to prompt model \(M_{t}\) to respond in a more honest way without updating any model parameters. Tab. 2 shows the prompt that has been studied in this work, which explicitly allows the model to indicate its incapability of answering the question. The advantage of this approach is its convenience, but the drawback is its reliance on the model's inherent ability of instruction following and in-context learning. Additionally, the results are not sufficiently robust and can be easily influenced by the prompts used.

### Supervised Fine-tuning

Supervised fine-tuning is another common alignment approach that involves annotating some supervised samples to instruct the model to provide more honest answers based on its acquired knowledge. In this situation, the challenge lies in, given a question, how to precisely judge if its answer is known or unknown by the model, i.e., how to define \(k()\). As previously stated in SS2.2, we approximate the model's level of understanding regarding specific questions by utilizing the definition of the categorization function \(c()\).

Specifically, given a question \(x\), and its responses \(=\{y_{1},y_{2},,y_{m}\}\) generated by the model \(M_{t}\) under \(m\) trials, we define _expected accuracy_ as the ratio of correct responses among \(m\) candidate

   Answer the question. If you don’t know the answer to the question, it is \\ appropriate to say “I apologize, but I’m not able to provide an answer to the \\ question.” \\ Q: question \\ A: \\   

Table 2: Prompt of input.

responses. We present different alignment strategies as depicted in Fig. 3: definition of \(k()\) and annotation of training samples.

#### 3.2.1 Absolute

Definition of \(k()\) FunctionIn the Absolute method, whether the model knows the answer to a question is determined by its ability to consistently provide the correct answer to the same question. Specifically, we can treat all questions with expected accuracy greater than or equal to the threshold \(\) as known samples. Then,

\[k(x)=1,&,\\ -1,&. \]

Annotation of Training SamplesFor "known questions" (i.e., \(k(x)=1\)), we randomly select correct responses from the model \(M_{t}\) as the output. For "unknown questions", we use pre-defined idx responses like "I apologize, but I'm not able to provide an answer to the question." as the final output for training samples.

#### 3.2.2 Confidence

The previous method does not take into account the model's confidence for a given question, which motivates the Confidence method with the same definition of \(k()\).

Annotation of Training SamplesIn this method, we simply prefix the expression of confidence in the output of _known samples_. For instance, given the question "Who was the first president of the USA?", if the model's expected accuracy in its sampled responses is 0.9, the output goes beyond just providing the correct answer compared to Absolute; it also conveys the model's level of confidence. It could take the form of statements like, "I'm about 90% confident to answer the question correctly, and the answer is George Washington" or "I'm absolutely certain that George Washington was the first president of the USA." Considering the various ways to convey confidence, we develop the following two approaches: Confidence-Num, which utilizes numerical confidence, and Confidence-Verb, which employs verbal expressions of confidence. The output formats for these two methods are detailed in SSD.2.

#### 3.2.3 Multisample

Definition of \(k()\) FunctionIn order to make the model aware of varying confidence levels in questions during training, we also take advantage of the set of \(m\) sampled responses. Specifically, given a question \(x\) and one response \(y_{i}\),

\[k(x,y_{i})=1,&c(x,y_{i})=1,\\ -1,&. \]

Figure 3: Overview of our proposed honesty-oriented fine-tuning methods. “Expected accuracy = 0.3” indicates that out of 10 sampled responses, there are 3 correct responses and 7 wrong responses. We use \(}} -1.0pt}}\) to represent correct responses, and \(}} -1.0pt}}\) to represent idx responses.

Annotation of Training SamplesLet's say among \(m=10\) sampled responses for a question \(x\), if only one response \(y_{0}\) provides an incorrect answer, while the other nine responses \(\{y_{i}\},i=1,,9\), despite minor differences in wording, all provide the correct answer, we include \((x,y^{}_{0}(y^{}_{0})=)\) and \((x,y_{i}(y_{i})=),i=1,,9\) in the training dataset. As a result, compared to the previous methods, with the same questions, this method expands the training dataset by a factor of \(m\).

## 4 Experiments

### Training Settings

To perform honesty-oriented supervised fine-tuning, we sample 8,000 data from a large-scale knowledge-based questions answering (QA) dataset, TriviaQA (Joshi et al., 2017), as our training dataset, and label contrastive samples as described in SS3.2. We employ the LLaMA2-Chat series of models (Touvron et al., 2023). Despite having been specifically fine-tuned towards aligning with human preferences, our experiments reveal that there is still room for enhancing their honesty. Details about construction of training dataset and training procedures can be found in SSD.3 and SSD.4.

### Evaluation Settings

Given an evaluation dataset and a model, we evaluate its performance based on its responses at temperature = 0. The alignment progress is assessed using accuracy and the evolutionary metrics introduced in SS2.3, with comparisons made between \(M_{t+1}\) and \(M_{t}\), as well as between \(M_{t}\) and itself.

We identify idk responses using heuristic rules as outlined in SSD.1, and determine correct and wrong responses by examining whether the gold answer from the evaluation dataset is present in the response via string match and ChatGPT (i.e., gpt-3.5-turbo-0613; OpenAI (2023b)) analysis. More details are available in SSC.

### Baselines

Unaligned BaselineThis approach utilizes the unaligned model \(M_{t}\) under the typical question-answering prompt, "Q: <question>:".

Fine-tuned BaselineWe also establish a supervised fine-tuning baseline, fine-tuned on the same 8,000 training samples. In contrast to Absolute, for unknown questions, the model's original responses will be replaced by the gold answers from TriviaQA instead of idk responses.

### Exp-I: In-distribution Evaluation

#### 4.4.1 Overall Results

Results of LLaMA2-Chat-13B6 on the TriviaQA evaluation set are shown in Tab. 3. It should be highlighted that, if the model is reluctant to say "I don't know", it will obtain the best over-consv. score (0) and the worst prudence score (0), resulting in an unsatisfactory honesty score (50.00%). We have the following observations.

cases, it does not consider the model's confidence. Intuitively, there is a significant difference between questions where the model is 90% confident in answering correctly and those where it is merely 20% confident. In contrast, Confidence and Multisample explicitly employ expected accuracy as training signals. To be specific, Confidence provides prefixed confidence expressions for "known questions", serving as finer-grained supervision signals that enable the model to more precisely capture its knowledge boundaries. Additionally, Multisample allows the model to implicitly learn from the proportions of correct answers and idx responses among the \(m\) sampled responses in the expanded training data, thus better recognizing its knowledge boundaries in a detailed manner. From the results, we can see that despite becoming slightly over-conservative, they obtain markedly improved honesty score.

**Multisample achieves the highest honesty score and Confidence-Verb achieves the best accuracy.** Clearly, Multisample surpasses other methods in both prudence and honesty scores, albeit at the expense of avoiding answers to a small portion of known questions. This aligned model, without being excessively cautious, can be trusted most by users. Furthermore, Confidence-Verb attains the highest accuracy, second only to Unaligned Baseline. The high accuracy likely results form multiple factors intertwined, such as the additional computational load during inference, or the benefits of incorporating an explicit confidence prefix that helps mitigate hallucinations when fine-tuning on weakly known knowledge (Gekhman et al., 2024). Fully unraveling the factors for improvement may require more extensive efforts and is worth discussing in future work.

#### 4.4.2 Scalability and Adaptability

Our approaches demonstrate scalability in terms of model size, and we have included additional results for both smaller and larger models in SSD.5.2. Also, they are not constrained to any specific language models and experiments in SSD.5.3 showcases the adaptability to multiple popular open-source LLMs including InternLM (InternLM, 2023), Qwen (Bai et al., 2023), and Baichuan2 (Baichuan, 2023).

### Exp II: Out-of-distribution Evaluation

To evaluate the out-of-distribution performance of all models, we leverage an existing dataset Non-AmbigQA (the subset of NQ-Open (Kwiatkowski et al., 2019) where the questions are clear and the answers are non-ambiguous (Min et al., 2020)), and also construct two special datasets PUQA and PKQA. Specifically, PUQA (**P**rior **U**nknown **QA**) contains 1,000 questions about scientific literature published in 2023, carefully designed to ensure that the model has no knowledge of them and to be inherently challenging. PKQA (**P**rior **K**nown **QA**) comprises 1,000 questions that the model is largely likely to be familiar with. Please refer to SSC for more details.

We present the results on the three datasets in Tab. 4, and have the following findings:

**Honesty-oriented fine-tuning methods are transferable.** Take Confidence-Verb as an example. It consistently outperforms baselines on all three datasets, by significantly enhancing the ability to decline to answer while minimizing the loss of the original performance as much as possible. The differences in data distribution between these three datasets and the training dataset TriviaQA, serve as evidence that honesty-oriented fine-tuning methods, with low cost, genuinely adapt to react differently to known/unknown questions, rather than taking a shortcut based on TriviaQA.

    & **Pudence\(\)** & **Over-Cons.\(\)** & **Honesty\(\)** & **Acc\(\)** \\  Unaligned & 0 & 0 & 50.00 & **73.71** \\ Fine-tuned & 0 & 0 & 50.00 & 71.47 \\ Prompt-based & 33.77 & 12.50 & 60.64 & 64.70 \\  Absolute & 47.70 & 9.94 & 68.88 & 71.30 \\ Confidence-Num & 61.11 & 12.38 & 74.37 & 69.80 \\ Confidence-Verb & 58.91 & 10.68 & 74.12 & 73.34 \\ Multisample & 67.72 & 15.89 & **75.91** & 68.88 \\   

Table 3: Main results on the **TriviaQA** evaluation set. Unaligned refers to Unaligned Baseline, Fine-tuned refers to Fine-tuned Baseline, and Prompt-based refers to the training-free method that adopts the prompt alone. Absolute applies \(m=10\) and \(=0.1\). The best honesty score is in **bold**, and the second-highest accuracy is underlined.

**Non-honesty-oriented fine-tuning teaches LLMs to hallucinate.** In the experimental results on PKQA, even though the questions were generated by the model itself, we observe a slight impact on the model's responses when an additional instruction is introduced. Moreover, we identify a peculiar phenomenon: Fine-tuned Baseline further decreases the accuracy by 10 points, performing notably worse than other methods. We assume that this could be attributed to a perspective proposed in (Schulman, 2023; Zhang et al., 2023) that the supervised fine-tuning process may inadvertently introduce hallucinations by forcing LLMs to answer questions that surpass their knowledge boundaries. Note that the training data for Fine-tuned Baseline includes around 25% of questions with answers that the model can hardly be expected to know.

### Exp III: Alignment Tax

When the model is fine-tuned to abstain from answering questions, the question of whether it becomes less helpful arises.7 To investigate this inquiry, we utilize the helpfulness dataset from Li et al. (2023) to assess the model's helpfulness before and after alignment. This dataset, denoted as Eval-P- (see SSC.5), comprises a diverse range of helpfulness-related requests including summarization, creative writing, general communication, and more, which differ from the demands of knowledge-based QA tasks. To evaluate the model's responses, we enlist the assistance of both Auto-J (Li et al., 2023) and GPT-4 (i.e., gpt-4-0613; OpenAI (2023)), which provide ratings on a scale of 1 to 10.

The helpfulness scores assessed by both judges are presented in Tab. 5. From the results, we can see that both Confidence-Verb and Multisample achieve similar performance to Unaligned Baseline when assessing helpfulness. This observation suggests that the cost of aligning LLMs for honesty does not impose a significant impact on their overall helpfulness, thus highlighting the practicality of the alignment process.

## 5 Limitations and Future Work

### Pitfalls in Defining Honesty

While we define honesty in line with long-established views (Askell et al., 2021; Cui et al., 2023), we make the following simplifying assumptions in order to reasonably approximate the model's internal thinking through its external behaviors.

**Honesty vs. Truthfulness.** According to Evans et al. (2021); Park et al. (2023), _honesty_ entails a model stating what it believes, while an adjacent concept, _truthfulness_, demands it to state what is

    &  &  &  \\  & **Prudence\(\)** & **Over-Consv.\(\)** & **Honesty\(\)** & **Acc\(\)** & **Prudence\(\)** & **Over-Consv.\(\)** & **Acc\(\)** \\  Unaligned & 0.11 & 0 & 50.06 & **49.63** & 0 & 0 & **100.00** \\ Fine-tuned & 0.23 & 0 & 50.11 & 45.16 & 0 & 0 & 87.70 \\ Prompt-based & 19.81 & 5.03 & 57.39 & 46.91 & 28.90 & 1.50 & 96.80 \\  Absolute & 30.98 & 9.80 & 60.59 & 47.51 & 34.20 & 8.00 & 95.90 \\ Confidence-Num & 47.30 & 12.22 & 67.54 & 47.02 & 87.30 & 5.10 & 96.00 \\ Confidence-Verb & 51.11 & 13.62 & 68.74 & 49.54 & 79.90 & 3.60 & 96.80 \\ Multisample & 64.73 & 24.37 & **70.18** & 44.26 & 86.20 & 9.40 & 96.20 \\   

Table 4: Out-of-distribution performance on the **three free-form QA datasets**. Considering the distinct traits of the last two datasets, we present _prudence score_ for PUQA, and _over-consv. score_ and _accuracy_ for PKQA. Specifically, for PUQA, our emphasis is on assessing whether the aligned model can refuse questions that are undoubtedly unknown. Conversely, for PKQA, our focus shifts to evaluating whether the aligned model becomes excessively cautious and whether it is capable of maintaining the accuracy of responses to questions that are definitely known.

    &  \\  & **Auto-J** & **GPT-4** \\  Unaligned & 5.56 & 8.62 \\ Confidence-Verb & 5.54 & 8.61 \\ Multisample & 5.52 & 8.56 \\   

Table 5: Results on helpfulness data from **Eval-P\({}^{-}\)**.

objectively true8. In this paper, we focus on "honesty" to explore the model's knowledge boundaries, instead of blindly spurring it to provide accurate information without considering what it has learned. However, exploring the model's internal reasoning can be complex. We hypothesize that for _general_ knowledge-based questions (e.g., TriviaQA (Joshi et al., 2017) rather than TruthfulQA (Lin et al., 2022b)), if a commonly used LLM gives an incorrect response, it is more likely that the model is making something up rather than having learned a false belief.

**Without Lying.** While typical dishonest behaviors in humans include lying, current LLMs, when not specifically prompted, fine-tuned, or placed in a special context (Pacchiardi et al., 2023; Park et al., 2023; Scheurer et al., 2023), generally do not provide incorrect information if they "know" the correct answer. Thus, we exclude this possibility from our consideration in this study.

Additionally, considering more complex scenarios is something we hope can inspire further research, such as eliciting latent knowledge and decoupling dishonesty from catastrophic forgetting, as mentioned in SS2.3.

### Future Work

**More advanced approaches to define \(k()\).** Our current method approximates the boundary of knowledge based on the model's external behavior in answering questions correctly or incorrectly, but this approach is far from perfect. Future work should explore more sophisticated methods to determine if the model "knows" the answer.

**Further exploration of uncertainty expressions.** Confidence methods make the model express varying degrees of confidence. However, calibrating the model's output confidence is beyond the scope of our work; we focus solely on whether the response contains idx signs or correct answers. The definition and feasibility of calibrated confidence expressions for free-form generation remain to be explored.

**Representation-level alignment for honesty.** A line of research (Li et al., 2023b; Zou et al., 2023) demonstrates the effectiveness of representation engineering. While we address different knowledge scopes - those works focus on eliciting truthful answers to _known_ questions, whereas we aim to adjust the model's behavior for both _known and unknown_ questions - we hope future work will explore approaches at the representation level of LLMs to achieve minimally invasive alignment for honesty.

## 6 Conclusion

In this work, we establish the framework of Alignment for Honesty, which requires LLMs to proactively decline to answer questions when appropriate, without resorting to external resources. To achieve this, we introduce the notion of "idk responses" and new metrics to measure the quality and reliability of responses when a model is allowed to express "I don't know". Furthermore, we propose several honesty-oriented fine-tuning methods and validate the feasibility of alignment for honesty through extensive experiments. We hope this work can inspire more thoughts on the development of _honest_ AI models in the NLP community.