# BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular Depth Estimation

Xiang Zhang\({}^{1,2}\), Bingxin Ke\({}^{1}\), Hayko Riemenschneider\({}^{2}\), Nando Metzger\({}^{1}\),

**Anton Obukhov\({}^{1}\), Markus Gross\({}^{1,2}\), Konrad Schindler\({}^{1}\), Christopher Schroers\({}^{2}\) \({}^{1}\)ETH Zurich, \({}^{2}\)DisneyResearchStudios**

###### Abstract

By training over large-scale datasets, zero-shot monocular depth estimation (MDE) methods show robust performance in the wild but often suffer from insufficient detail. Although recent diffusion-based MDE approaches exhibit a superior ability to extract details, they struggle in geometrically complex scenes that challenge their geometry prior, trained on less diverse 3D data. To leverage the complementary merits of both worlds, we propose _BetterDepth_ to achieve geometrically correct affine-invariant MDE while capturing fine details. Specifically, BetterDepth is a conditional diffusion-based refiner that takes the prediction from pre-trained MDE models as depth conditioning, in which the global depth layout is well-captured, and iteratively refines details based on the input image. For the training of such a refiner, we propose global pre-alignment and local patch masking methods to ensure BetterDepth remains faithful to the depth conditioning while learning to add fine-grained scene details. With efficient training on small-scale synthetic datasets, BetterDepth achieves state-of-the-art zero-shot MDE performance on diverse public datasets and on in-the-wild scenes. Moreover, BetterDepth can improve the performance of other MDE models in a plug-and-play manner without further re-training.

## 1 Introduction

As a fundamental task in computer vision, monocular depth estimation (MDE) aims to extract depth information from single-view images, benefitting various real-world applications . Unlike traditional depth estimation techniques that utilize geometric relationships from stereo  or structured light setups , MDE is a highly ill-posed task and relies on the geometric prior knowledge learned from training datasets, where real data plays a pivotal role to ensure generalization to in-the-wild applications . However, due to the difficulty of collecting fine-grained depth labels in real scenarios, real-world depth labels are often coarse, noisy, and incomplete, resulting in a trade-off between the quality and generalization of MDE. Thus, although significant progress in zero-shot MDE has been achieved with techniques like mixing diverse training datasets  and unleashing large-scale unlabeled data , previous MDE approaches often suffer from over-smoothing of details, as indicated by the red arrows in Fig. 1.

Recently, diffusion models have exhibited promising performance in a variety of computer vision tasks , including MDE . Benefitting from the iterative refinement scheme, diffusion-based MDE methods can produce impressive depth maps with fine granularity as depicted in Fig. 1. However, training a diffusion-based MDE generally requires complete depth labels , which is in practice achieved by rendering synthetic datasets. Compared to real data, existing synthetic RGB-D datasets exhibit lower variety and contain fewer samples which limits the generality of the learned prior. Despite several attempts to improve the generalization of diffusion-based MDE, such as label infilling  and transferring 2D image priors , currentdiffusion-based approaches still have relatively limited a-priori knowledge of global layout. This results in less accurate predictions in challenging scenes compared to models trained with diverse datasets, _e.g._, Depth Anything  (Tab. 2).

In this work, we aim for robust affine-invariant MDE while also capturing fine-grained details. Motivated by the complementary merits of feed-forward and diffusion-based MDE methods, we propose _BetterDepth_ to boost pre-trained MDE models with a diffusion refiner, simultaneously leveraging rich geometric priors for zero-shot transfer and diffusion models for detail refinement. Specifically, BetterDepth is designed as a depth-conditioned diffusion model to retain the zero-shot generalization power of pre-trained MDE models. Through efficient training on small-scale synthetic datasets, BetterDepth further attains a remarkable ability to extract details (Fig. 1) and can directly improve other MDE models, without re-training. To learn detail refinement and simultaneously preserve the prior knowledge from pre-trained MDE models, we introduce global pre-alignment and local patch masking strategies during training, to ensure the faithfulness of BetterDepth to depth conditioning while enabling fine-grained detail extraction. In this way, BetterDepth combines the advantages of zero-shot and diffusion-based MDE models, exhibiting state-of-the-art performance and producing visually superior results on diverse datasets. Our main contributions are:

* We propose BetterDepth to boost zero-shot MDE methods with a plug-and-play diffusion refiner, achieving robust affine-invariant MDE performance with fine-grained details.
* We design global pre-alignment and local patch masking strategies to enable learning the refinement from small-scale synthetic datasets while preserving the rich prior knowledge in pre-trained zero-shot MDE models.

## 2 Related Work

**Zero-Shot Monocular Depth Estimation.** A variety of attempts are devoted to improving the robustness of MDE in the wild, _i.e._, zero-shot depth estimation, which aims to predict depth for any input image taken in unconstrained settings [3; 4; 53; 55; 52; 15; 29]. Considering that MDE is a geometrically ill-posed problem, many zero-shot MDE works are designed to estimate affine-invariant depth, _i.e._, predicting the depth values up to an unknown global scale and shift [31; 17; 53; 12; 49]. For example, MegaDepth  and DiverseDepth  collect internet images for network training, improving adaptation to unseen scenes. Furthermore, MiDaS  proposes a family of scale- and shift-invariant losses to handle the different depth representations, _e.g._, metric depth and inverse depth (disparity), across datasets, so as to mix diverse training datasets and reach robust zero-shot

Figure 1: **Monocular depth estimation** (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything , produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold , extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.

transfer. By replacing CNN backbones with powerful vision transformers, DPT  and Omnidata  further boost the performance of zero-shot depth estimation. Recently, Depth Anything developed a semi-supervised strategy to unleash the power of large-scale unlabeled images (62M) and acquire a robust representation for in-the-wild prediction . Although the zero-shot generalization of MDE grows with the amount of training data, the lower-quality labels in real-world datasets tend to hinder the reconstruction of fine-grained depth details, resulting in over-smoothing as shown in Fig. 1.

**Diffusion-Based Monocular Depth Estimation.** The emergence of denoising diffusion probabilistic models (DDPMs) brought up a new paradigm for image generation, producing high-quality images with realistic details [13; 44; 34]. Many works have showcased the effectiveness of diffusion models in generating photo-realistic results for various computer vision tasks [37; 22; 35; 25; 5]. In the realm of MDE, DDP  describes a diffusion-based framework for dense visual prediction tasks, and DiffusionDepth  further utilizes Swin transformers  for image encoding, performing iterative refinement in the depth latent space. Considering the noisy and sparse depth labels in practice, several techniques are proposed, _e.g._, depth infilling  and self-supervised pre-training , to achieve better MDE performance. A recently emerging trend is to exploit the prior knowledge in foundational diffusion models for MDE [56; 17; 12]. Marigold  proposes an efficient fine-tuning protocol to leverage the rich prior in the Stable Diffusion model  for depth estimation, producing visually compelling depth results. Following this direction, DepthFM  improves inference speed with flow matching, and GeoWizard  utilizes cross-modal relations for joint depth and normal prediction. However, existing diffusion-based approaches still struggle to outperform the feed-forward MDE models like Depth Anything  (Tab. 2), due to the difficulty of learning diverse geometric depth priors from datasets with few or sparse depth labels . By contrast, our BetterDepth efficiently leverages the rich prior knowledge of feed-forward models and improves the extraction of details with diffusion, achieving state-of-the-art MDE performance (Tab. 2) with compelling visual results (Figs. 1 and 5).

## 3 Method

We first analyze existing MDE methods and formulate our objective in Sec. 3.1. Based on the analysis, we then propose our BetterDepth framework in Sec. 3.2 and introduce the training and inference strategies designed specifically for BetterDepth in Sec. 3.3 and 3.4, respectively.

### Problem Formulation

Model architecture and training data are two key factors that determine MDE performance. Given a depth dataset \(\{(_{i},_{i})\}_{i}\) with \(_{i}\) and \(_{i}\) corresponding to images and depth labels, previous zero-shot MDE approaches usually employ feed-forward models \(_{}\) and learn depth estimation using the following training objective [31; 30; 49]:

\[_{}_{i},_{}( _{i}), \]

where \(_{}()\) represents a suitable loss function, _e.g._, scale- and shift-invariant losses . Since \(_{i}\) is only used to supervise model outputs in Eq. (1), feed-forward MDE methods can easily handle invalid pixels in depth labels via techniques like masking, and thus gain robust zero-shot capability by learning from diverse large-scale datasets [31; 30; 49]. To handle the synthetic-to-real domain gaps caused by synthetic data \(_{}\), real-world datasets \(_{}\) are often simultaneously employed to learn more robust representations for in-the-wild prediction. However, the quality of depth labels in \(_{}\) usually hinders feed-forward methods from learning to capture high-frequency information present in the inputs, resulting in over-smoothed details, as depicted in Fig. 1.

By contrast, diffusion-based MDE approaches generally excel at capturing fine-grained details via iterative refinement [17; 10]. Different from feed-forward methods, diffusion models \(_{}\) comprise a \(T\)-step forward process to gradually corrupt samples with Gaussian noise at each timestamp \(t\{1,,T\}\), and a learned reverse process to transform random Gaussian noise to a sample from the target data distribution [13; 44]. Instead of directly fitting \(_{i}\) in Eq. (1), one typically learns to estimate the added Gaussian noise from \(_{i}\) and \(_{i}\) at each timestamp \(t\), _i.e._:

\[_{},_{}( _{i},(_{i},,t)), \]

where \((,)\) denotes Gaussian noise; \(()\) is an operator that corrupts depth labels \(_{i}\) with noise \(\) according to \(t\); \(_{}()\) represents a loss function for diffusion models, like the velocity prediction loss . Since the depth labels are treated as model inputs in Eq. (2), directly training \(_{}\) with sparse depth labels becomes challenging , preventing training with diverse real-world data, thus limiting the generalization ability of diffusion-based MDE.

Based on the above analysis, we summarize the characteristics of feed-forward and diffusion-based MDE methods in Tab. 1, where \((,)\) represents the output distribution, as a function of the employed model architecture \(\) and training datasets \(\). Motivated by the complementary strengths of \((_{},\{_{},_ {}\})\) and \((_{},_{})\), our goal is to approach the ideal distribution \((_{},_{})\) and achieve robust zero-shot MDE with fine-grained details. However, to reach this in a tractable manner, challenges exist from both the model and data perspectives:

* **Model Limitation.** A potential solution is to train diffusion models over diverse datasets, _i.e._, \(_{}=_{}\) and \(_{}=\{_{},_{}\}\). However, how to efficiently train \(_{}\) with \(_{}\) while preserving the functionality to extract fine-grained details remains an open question. In addition, training over large datasets is required to gain robust zero-shot generalization, which would be extremely time-consuming and resource-intensive.
* **Data Limitation.** Another possible method is to train feed-forward models \(_{}\) with high-quality diverse datasets. However, although high-quality labels are available in \(_{}\), training solely with \(_{}\) introduces a detrimental synthetic-to-real domain gap . Meanwhile, real depth labels in \(_{}\) must be collected with depth sensors like ToF cameras or LiDAR , which inherently limits the achievable quality of the supervision.

### BetterDepth Framework

To circumvent the aforementioned limitations, we propose BetterDepth to efficiently leverage the strengths of feed-forward and diffusion-based methods, achieving better MDE performance. Specifically, BetterDepth is composed of a conditional latent diffusion model and a pre-trained feed-forward MDE model, as illustrated in Fig. 2. Since \(_{}\) is known to reach strong zero-shot generalization by training on large and diverse datasets, we first utilize the rich geometric prior from pre-trained \(_{}\), _e.g._, DPT  or Depth Anything , to ensure accurate estimation of the global depth context. Based on this, a learnable \(_{}\) is employed to locally improve the estimation of details via iterative refinement. To enable the processing of high-resolution images, we follow Marigold  and implement \(_{}\) with Stable Diffusion , which maps from pixel space to a lower-dimensional latent space with a variational autoencoder (VAE)  and performs denoising with a U-Net in latent space. Because we treat \(_{}\) as knowledge reservoir for zero-shot generalization and only need to train \(_{}\) for refinement, BetterDepth only requires a small synthetic training dataset, _e.g._, 400 data pairs as shown in Tab. 2. Furthermore, the trained \(_{}\) in BetterDepth can be directly transferred to improve other \(_{}\) models, without re-training.

### Training Strategies

The training pipeline of BetterDepth is illustrated in Fig. 2. Although the pre-trained model \(_{}\) in BetterDepth provides coarse depth estimates as reliable conditioning, directly training the diffusion-based refiner \(_{}\) with synthetic data still tends to overfit the training data distribution, resulting in similar performance as \((_{},_{})\) and degrading generalization. To enhance the faithfulness of BetterDepth to the depth conditioning while still enabling refinement of details, we modify the

   Model & Training Data & Output Distribution & Fine-Grained Details & Zero-Shot Generalizability \\  \(_{}\) & \(_{}\), \(_{}\) & \((_{},(_{},_ {}))\) & & ✓ \\ \(_{}\) & \(_{}\)† & \((_{},_{})\) & ✓ & \\ \(_{}\) & \(_{}\)†diffusion training pipeline to include global pre-alignment and local patch masking techniques, simultaneously promoting zero-shot MDE capability and fine-grained detail extraction.

**Global Pre-Alignment.** To alleviate overfitting, we first propose a global pre-alignment method to narrow the gap between the conditioning depth map and the ground truth depth, enforcing BetterDepth to follow depth conditioning at a global scale. Given a pre-trained affine-invariant depth model \(_{}\) and a data pair \((,~{})_{}\) (subscript \(i\) is omitted for brevity), we first estimate a coarse depth map \(}\) via \(}=_{}()\) as depicted in Fig. 2. Although \(}\) and \(\) correspond to the same image \(\), the estimated depth values in \(}\) generally deviate from \(\) due to the unknown scale and shift, which stops BetterDepth from establishing a strong dependency between the depth conditioning and the final estimate during training. We resolve this with a global pre-alignment to eliminate the difference caused by the unknown scale and shift. Inspired by the affine-invariant depth evaluation protocol , we first estimate the scale \(s\) and shift \(b\) and then align \(}\) to the depth labels \(\), _i.e._,

\[}^{}=s}+b,~{}~{}(s,b)= _{s,b}s}+b-_{2}^{2}. \]

Eq. (3) is solved via least squares fitting and \(}^{}\) indicates the aligned depth conditioning. Afterwards, the frozen latent VAE encoder is employed to project \(,~{}}^{},~{}\) to latent space, corresponding to \(^{},~{}^{}^{}},~{}^{}\). We then follow the DDPM training scheme  to generate a noisy sample \(^{}_{t}=_{t}}^{}_{0} +_{t}}\) with Gaussian noise \((,)\), where \(^{}_{0}:=^{}\), \(_{t}:=_{j=1}^{t}1-_{j}\), and \(\{_{1},,_{T}\}\) is the variance schedule of a \(T\)-step process. Finally, the noisy sample \(^{}_{t}\) is concatenated with the latent image and depth conditioning \(^{},~{}^{}^{}}\) as inputs to train the latent U-Net. Although our pre-alignment strengthens the conditioning by ensuring a similar global depth range between \(}^{}\) and the depth label \(\), misalignment still exists in local regions due to the estimation bias of the pre-trained MDE model. Even though rectifying the coarse depth conditioning \(}^{}\) to the high-quality label \(\) during training might intuitively seem helpful to MDE performance, we find that rectifying significantly different local regions between \(}^{}\) and \(\) also degrades the zero-shot performance. This is because the pre-trained depth model embeds rich prior knowledge of the visual world, which is more important than the dataset-specific knowledge learned in small-scale training sets. Thus, we next propose local patch masking to further improve the efficacy of depth conditioning in local regions while learning detail refinement.

**Local Patch Masking.** As shown in Fig. 2, we first estimate the latent space mask \(m\) from depth label \(\) and the aligned depth conditioning \(}^{}\), and then construct a masked diffusion objective for training. In detail, \(}^{}\) and \(\) are first split into non-overlapping local patches \(\{}^{}_{n}\}\), \(\{_{n}\}\), where \(}^{}_{n}^{w w}\)

Figure 2: **BetterDepth training pipeline.** Given training images \(\) and labels \(\), we first estimate coarse depth maps \(}\) with the pre-trained \(_{}\) and apply global pre-alignment to \(}\) using \(\) as reference. Afterwards, the frozen latent encoder is employed to convert the image \(\), the depth labels \(\), and the aligned depth conditioning \(}^{}\) to the latent space. To construct the masked training objective, \(}^{}\) and \(\) are split into non-overlapping patches \(\{}^{}_{n}\}\) and \(\{_{n}\}\), and dissimilar patches are filter out by thresholding, producing the patch-level similarity mask. Finally, the mask is downscaled to the latent space resolution for diffusion training.

and \(_{n}^{w w}\), with \(w\) the patch size. For each pair of patches we measure the similarity using the Euclidean distance, _i.e._,

\[(}_{n}^{},_{n})=\|}_{n}^{}-_{n}\|_{2}, \]

and then generate the pixel space mask \(M\) by

\[M_{n}=1,&(}_{n}^{}, _{n}) w,\\ 0,&, \]

where \(\) indicates the average tolerance per pixel in the patch and controls the trade-off between depth conditioning and refinement of details. To fit the latent diffusion scheme, the pixel space mask \(M\) is then downscaled to a latent space mask \(m\) via \(m=(M)\). Finally, \(m\) is applied to the velocity prediction objective  for model training,

\[=_{,(, ),t(T)}[\|_{} (,t) m-v(_{0}^{},,t) m \|_{2}^{2}], \]

where \(\) is the number of valid elements in \(m\); \(_{}(,t)\) indicates the velocity estimated from U-Net with \(=(^{},^{ }^{}},_{}^{}})\); \(v(_{0}^{},,t)\) denotes the ground-truth velocity defined as \(v(_{0}^{},,t)=_{t}}-_{t}}_{0}^{}\). With the masked training objective, BetterDepth not only strengthens the depth conditioning by discarding significantly dissimilar patches but learns to capture fine-grained details from the remaining patch pairs without overfitting the training data.

We further analyze the effectiveness of our training strategies from the perspective of data distribution. As illustrated in Fig. 3, the learned output distribution of BetterDepth (denoted as \(}\)) initially covers \((_{},_{})\) without either pre-alignment or patch masking, as we essentially train a diffusion model with synthetic data in BetterDepth. Thus the resulting model is able to extract fine-grained details but falls short in generalization according to Tab. 1. By applying global pre-alignment, we bring \(}\) closer to the output distribution of the pre-trained depth model, _i.e._, \((_{},\{_{},_ {}\})\), which equips BetterDepth with better zero-shot capability by enhancing the conditioning strength at the global scale. Finally, with local patch masking, we filter out significantly different patches and further shrink \(}\) toward the intersection of \((_{},\{_{},_ {}\})\) and \((_{},_{})\). Therefore, BetterDepth gains the advantages of both worlds and inherits the prior knowledge from the pre-trained depth model while learning to extract fine-grained details with diffusion, approximating \((_{},_{})\) in Tab. 1.

### Inference Strategies

The inference pipeline is depicted in Fig. 4. Similar to the training procedure, we first generate a coarse depth map \(}\) from the input image \(\), _i.e._, \(}=_{}()\), and then convert t into a latent codes\(^{},\ ^{}}\) as conditioning. In the latent space, we sample the initial value from standard Gaussian noise, _i.e._, \(_{t=T}^{}}(,)\), and concatenate it with \(^{},\ ^{}}\) as input to the U-Net, \(=(^{},^{ }},_{}}^{}})\), where the depth conditioning ensures generalization and the image conditioning provides auxiliary information for refinement. After \(T\)-step iterative refinement with the pre-trained U-Net \(_{}(,t)\), the clean latent \(_{0}^{}}\) is decoded to a final depth map \(}\) via the latent VAE decoder.

**Plug-and-Play.** Once trained, BetterDepth can directly refine the output of previously unseen MDE models, without any additional training. This advantage comes from the different roles of \(_{}\) and \(_{}\) in BetterDepth. According to our proposed training strategy, BetterDepth treats \(_{}\)

Figure 3: **Illustration of output distributions** after applying pre-alignment and patch masking. The output distribution of BetterDepth (\(}\)) is pushed towards the intersection of \((_{},\{_{},_ {}\})\) and \((_{},_{})\) to achieve detailed zero-shot MDE.

as the knowledge reservoir to ensure zero-shot MDE performance and utilizes \(_{}\) only to refine details. When faced with a different \(_{}\), BetterDepth inherits a correspondingly different prior, but maintains the functionality to add fine-grained details to it. Given the increasing trend to train foundational MDE models , BetterDepth can be flexibly added to new models as a refinement module to enhance the extraction of details.

## 4 Experiments and Analysis

### Experimental Settings

**Implementation.** We employ Depth Anything  as \(_{}\) and use the Marigold architecture  with Stable Diffusion weight initialization  as \(_{}\) in our BetterDepth, where we only fine-tune the denoising U-Net. BetterDepth is trained for 5K iterations with batch size 32. The training takes around 1.5 days on a single NVIDIA RTX A6000 GPU. The Adam optimizer  is used with the learning rate set to \(3 10^{-5}\). We set the patch size \(w=8\) and the masking threshold \(=0.1\) under the depth range \([-1,1]\). For inference, we apply the DDIM scheduler with 50-step sampling  and obtain the final result with 10 test-time ensemble members .

**Datasets and Evaluation.** We follow Marigold  and use 74K samples from two synthetic datasets **Hypersim** and **Virtual KITTI** for training. Additionally, we construct two smaller datasets by randomly selecting 2K and 400 samples, respectively, from the full training dataset to test the performance of BetterDepth with fewer training samples (denoted as BetterDepth-2K and BetterDepth-400). For evaluation, we employ five unseen datasets **NYUv2** (654 samples), **KITTI** (652 samples from the Eigen test split ), **ETH3D** (454 samples), **ScanNet** (800 samples based on the Marigold split ), and **DIODE** (325 indoor samples and 446 outdoor ones), and conduct quantitative comparisons with two metrics, AbsRel (absolute relative error: \(_{k=1}^{N}|}_{k}-_{k}|/_{k}\) with \(N\) denoting the number of pixels) and \( 1\) (percentage of \((_{i}/_{i},_{i}/_{i})<1.25\)). In-the-wild images are also collected for qualitative evaluation of zero-shot MDE.

### Benchmarking

In this section, we compare BetterDepth with state-of-the-art affine-invariant MDE methods to show its superior zero-shot performance and reconstruction of details.

**Zero-Shot Performance.** Tab. 2 shows the results for BetterDepth compared with both feed-forward and diffusion-based MDE approaches. Benefitting from the proposed framework and training strategies, BetterDepth successfully combines the geometric prior from the pre-trained depth model with the ability to model fine details. Specifically, BetterDepth-2K already achieves state-of-the-art performance and BetterDepth-400 still compares favorably to prior art. In addition, different MDE models can be directly plugged into the BetterDepth framework, which consistently improves their outputs across most datasets, as demonstrated in Tab. 3. BetterDepth also outperforms existing MDE

Figure 4: **BetterDepth inference pipeline. Given an image \(\) and a pre-trained depth model, we first estimate the coarse depth map \(}\) as conditioning. After converting \(\) and \(}\) to latent space, we concatenate the latent codes \(^{}\), \(^{}}\) with the depth latent \(^{}}_{t}\) for denoising. After \(T\)-step refinement, random Gaussian noise \(^{}}_{T}\) has been converted to \(^{}}_{0}\) and is decoded to the final estimate \(}\).**

[MISSING_PAGE_FAIL:8]

similarly to previous diffusion-based methods like Marigold , and struggles with generalization only from synthetic training data. By utilizing the geometric prior from the pre-trained depth estimator, model #2 achieves consistent improvements in both indoor and outdoor scenarios, as shown in Tab. 5.

(ii) **Global Pre-Alignment.** Despite the improvements gained with depth conditioning, we find the zero-shot performance still remains below the pre-trained Depth Anything model . In other words, even having good depth maps from the pre-trained model as initialization, the naive

Figure 5: **Qualitative comparisons** of depth estimation and 3D reconstruction results (colored as normals), where Marigold predicts depth values and the others output disparity.

   ID &  Depth \\ Conditioning \\  &  Global \\ Pre-Alignment \\  &  Local \\ Patch Masking \\  &  NYUv2 \\ AbsRel\(\) \\  & 
 KITTI \\ AbsRel\(\) \\  \\  \#1 & ✗ & ✗ & ✗ & 6.1 & 96.1 & 9.1 & 90.7 \\ \#2 & ✓ & ✗ & ✗ & 5.2 & 97.0 & 8.6 & 92.2 \\ \#3 & ✓ & ✓ & ✗ & 4.7 & 97.5 & 7.9 & 94.4 \\ \#4 & ✓ & ✓ & ✓ & 4.2 & 98.0 & 7.5 & 95.2 \\   

Table 5: **Ablation study**. All variants are trained on the full 74K training pairs for 5K iterations. The best and second-best results are marked.

Figure 6: **Visual comparisons** on Middlebury 2014 . Details are zoomed in.

conditioning model (#2) struggles to balance the contribution of different priors and does not yield an improvement. This is because model #2 overfits the distribution of training data and under-utilizes the prior knowledge in the pre-trained model. By aligning the depth conditioning to the ground truth during training, model #3 better learns to follow the depth conditioning at a global scale and brings further improvements in zero-shot generalization. (iii) **Local Patch Masking.** Our full model #4, with the masked training objective, exhibits the best performance. By filtering out significantly dissimilar regions with patch masking, we ensure that BetterDepth closely adheres to depth conditioning at local scales, thus better exploiting the prior for zero-shot transfer. Meanwhile, operating at patch level fully retains the information in local regions and thus benefits the reconstruction of details, _e.g._, edges and fine structures, as illustrated in Fig. 1 and 5.

### Method Analysis

In this section, we further analyze BetterDepth with respect to training and inference efficiency.

**Training Efficiency.** We compare the training efficiency of BetterDepth with the state-of-the-art diffusion-based method Marigold . Helped by the additional depth conditioning, BetterDepth converges significantly faster than Marigold, as depicted in Fig. 6(a). With only 200 iterations (\( 1.5\) hours of training), BetterDepth achieves comparable performance to Marigold trained with 5K iterations. Furthermore, since we must only learn to refine details, thanks to the proposed training strategies, BetterDepth outperforms Marigold with fewer training samples, _e.g._, BetterDepth-400 in Tab. 2, validating the overall strategy.

**Inference Efficiency.** We compare the efficiency at inference time with different ensemble sizes and numbers of denoising steps. Test-time ensembling aims to aggregate information from multiple predictions, and larger ensemble sizes generally bring better and more stable results . As depicted in Fig. 6(b), on KITTI the \(\)1 difference between a single inference and an ensemble of 10 members is 1.2 percentage points for Marigold but only 0.4 for BetterDepth, confirming its better stability. Meanwhile, BetterDepth produces comparable or even better results than 50-step Marigold with only 2 inference steps, as shown in Fig. 6(c). In terms of inference speed, the 50-step Marigold achieves 91.6% \(\)1 accuracy on KITTI with 10 ensemble members, spending 30.5 seconds per sample on an NVIDIA GeForce RTX 4090 GPU. In contrast, our 2-step BetterDepth achieves 92.5% \(\)1 accuracy in a single inference pass with only 0.4 seconds per sample (0.38 seconds for the diffusion denoising and 0.02 seconds for the depth conditioning prediction).

## 5 Conclusion

We have presented BetterDepth to achieve robust, detailed, and efficient affine-invariant monocular depth estimates. The proposed method combines the strong prior of massively pre-trained MDE models with the recovery of fine details enabled by diffusion models, and devises training strategies to maximally retain the strengths of both discriminative depth estimation and conditional depth map generation. In this way, BetterDepth achieves state-of-the-art MDE performance and is able to refine different feed-forward depth estimators without re-training.

Figure 7: **Training and inference efficiency compared with Marigold  on the KITTI dataset.**