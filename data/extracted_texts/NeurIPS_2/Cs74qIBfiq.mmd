# Zero-Shot Robustification of Zero-Shot Models With Auxiliary Foundation Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings--without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and show an average improvement of 15.98% over several zero-shot baselines. Additionally, we demonstrate that RoboShot is compatible with a variety of pretrained and language models.

## 1 Introduction

Zero-shot models are among the most exciting paradigms in machine learning. These models obviate the need for data collection and model training loops by simply asking the model for a prediction on any set of classes. Unfortunately, such models inherit biases or undesirable correlations from their large-scale training data . In a now-canonical example , they often associate waterbirds with water background. This behavior leads to decreased performance, often exacerbated on rare data slices that break in-distribution correlations.

A growing body of literature  seeks to improve robustness in zero-shot models. While promising, these works require labeled data to train or fine-tune models, and so **do not tackle the zero-shot setting.** A parallel line of research seeking to debias word embeddings  often sidesteps the need for labeled data. Unfortunately, these works often require domain expertise and painstaking manual specification in order to identify particular concepts that embeddings must be invariant to. As a result, out-of-the-box word embedding debiasing methods also cannot be applied to zero-shot robustification.

Can we robustify zero-shot models without (i) labeled data, (ii) training or fine-tuning, or (iii) manual identification? Surprisingly, despite this seemingly impoverished setting, it is often possible to do so. Our key observation is that zero-shot models **contain actionable insights** that can be exploited to improve themselves or other zero-shot models. These insights are noisy but cheaply available at scale--and can be easily translated into means of refinement for zero-shot representations. These refinements improve performance, particularly on underperforming slices--at nearly no cost.

We propose RoboShot, a system that robustifies zero-shot models via auxiliary language models _without labels, training, or manual specification_. Using just the task description, RoboShot obtains _positive and negative insights_ from a language model (potentially the model to be robustified itself). It uses embeddings of these noisy insights to recover _harmful, beneficial_, and _benign_ subspaces of zero-shot latent representation spaces. Representations are then modified to neutralize and emphasize their harmful and beneficial components, respectively.

Theoretically, we introduce a simple and tractable model to capture and quantify failures in zero-shot models. We provide a result that characterizes the _quantity and quality_ of insights that must be obtained as a function of the severity of harmful correlations. Empirically, RoboShot achieves 15.98% improvement across nine image and NLP datasets while offering sufficient versatility to apply to a diverse variety of base models. Most excitingly, in certain cases, it reaches comparable or greater improvements **even when compared to fine-tuned models** that rely on labeled data.

Our contributions include,

1. A simple theoretical model describing zero-shot model failures along with a theoretical analysis of our approach that characterizes the amount of information required for obtaining improvements as a function of the most harmful unwanted correlation,
2. RoboShot, an algorithm that implements our core idea. It extracts insights from foundation models and uses them to improve zero-shot representations,
3. Extensive experimental evidence on zero-shot language and multimodal models, showing improved worst-group accuracy of 15.98% across nine image and NLP datasets.

## 2 Related Work

We describe related work in zero-shot model robustness, debiasing embeddings, guiding multi-modal models using language, and using LMs as prior information.

Zero-Shot inference robustness.Improving model robustness to unwanted correlations is heavily studied . Some methods require training from scratch and are less practical when applied to large pretrained architectures. Existing approaches to improve robustness _post-pretraining_ predominantly focus on fine-tuning.  detects spurious attribute descriptions and fine-tunes using these descriptions. Specialized contrastive loss is used to fine-tune a pretrained architecture in  and to train an adapter on the frozen embeddings in . While promising, fine-tuning recreates traditional machine learning pipelines (e.g., labeling, training, etc.), which contradicts the promise of zero-shot models. In contrast, our goal is to avoid any training and any use of labeled data.

Debiasing embeddings.A parallel line of work seeks to de-bias text embeddings  and multimodal embeddings  by re

Figure 1: RoboShot pipeline (right) vs. vanilla zero-shot classification (left).

moving subspaces that contain harmful or unwanted concepts. We use a similar procedure as a building block. However, these methods either target specific fixed concepts (such as gender) or rely on concept annotations, which limits their applicability across a wide range of tasks. In contrast, our method automates getting _both beneficial and unwanted concepts_ solely from the task descriptions. An additional difference is that our goal is simply to add robustness at low or zero-cost; we not seek to produce fully-invariant representations as is often desired for word embeddings.

Using language to improve visual tasksA large body of work has shown the efficacy of using language to improve performance on vision tasks . Most relevant are those that focus on robustness, like , where attention maps using multimodal models (like CLIP) are used as extra supervision to train a downstream image classifier.  uses text descriptions of spurious attributes in a fine-tuning loss to improve robustness against spurious correlations. In contrast to these works, we focus on using textual concepts to improve zero-shot model robustness--without fine-tuning.

Language model as priorThe basis of our work comes from the observation that language models contain information that can serve as a prior for other learning tasks.  finds that LLMs can perform causal reasoning tasks, substantially outperforming existing methods.  explicitly prompts LLMs for task-specific priors, leading to substantial performance improvements in feature selection, reinforcement learning, and causal discovery. Our work shares the spirit of these approaches in using the insights embedded in language models to enhance zero-shot robustness.

## 3 RoboShot: Robustifying Zero-shot Models

We are ready to provide our setup and describe the algorithm.

### Modeling and setup

Suppose that the zero-shot model's latent space contains an (unknown) _concept set_; similar notions have been studied frequently in the literature . For simplicity, we assume that this concept set is given by the orthonormal vectors \(\{z_{1},,z_{k}\}\). The model's encoder produces, for a particular input a representation \(x\) that is a mixture of concepts \(_{i}_{i}z_{i}\), where \(_{i} 0\) are weights.

We shall work with the following theoretical model for zero-shot classification. It closely resembles models like CLIP. For simplicity, we assume that there are two classes. It is straightforward to extend

Figure 2: (a) RoboShot debiases original input embedding (left). The projected embedding (right)â€™s variance in the unwanted direction is reduced, and in the relevant direction increases. (b) Embedding projection. We project embeddings to the space orthogonal to the embeddings of all unwanted insights (e.g., water and land)

the analysis below to multiple classes. We take \(_{i}_{i}z_{i}\) to be the embedding of a datapoint, while \(c^{0}=_{i}_{i,0}z_{i}\) is the embedding of the first class and \(c^{1}=_{i}_{i,1}z_{i}\) is that of the second. Finally, we assume that we have access to \(m\) answers \(v^{1},,v^{m}\) from the queries to the language model. These are given by \(v^{j}=_{i}_{i,j}z_{i}\) for \(j m\). We call these _insight representations_. Without our approach, the prediction is made by \(\{(_{i}_{i}z_{i})^{T}(_{i}_{i,0}z_{i})<(_{i} _{i}z_{i})^{T}(_{i}_{i,1}z_{i})\}\), so that we predict whichever class has higher inner product with the datapoint's embedding.

Next, we assume that each input representation \(x\) can be represented by partitioning the mixture components into three groups,

\[x=_{s}^{S}_{s}^{}z_{s}+_{r}^{R}_{r}^{ }z_{r}+_{b}^{B}_{b}^{}z_{b}.\]

The same holds for class and insight representations.

```
1:Parameters: Input embedding \(x\), class embeddings \(c^{0},c^{1}\), harmful insight representations \(v^{1},,v^{|S|}\), helpful insight representations \(u^{1},,u^{|R|}\)
2:for\(j\{1,2,,|S|\}\)do
3: Reject harmful insight \(v_{j}\): set \(x x- x,v^{j}/ v^{j},v^{j} v^{j}\)
4: Renormalize \(x=x/ x\)
5:endfor
6:for\(k\{1,2,,|R|\}\)do
7: Increase helpful insight \(u_{k}\): set \(x x+ x,u^{k}/ u^{k},u^{k} u^{k}\)
8:endfor
9:\(=\{x^{T}c^{0}<x^{T}c^{1}\}\)
10:Returns: Robustified zero-shot prediction \(\)
```

**Algorithm 1**RoboShot

ExampleWe illustrate how harmful correlations produce errors on rare slices of data through a standard task setting, Waterbirds . In this dataset, the goal is to classify landbirds versus waterbirds, and the background (land or water) is spurious. Suppose that we have these terms relate to concepts such that \(z_{}=-z_{}\) and \(z_{}=-z_{}\).

Consider a datapoint coming from a rare slice infrequently encountered in the training set. This might be an image of a landbird over water. Its embedding might be \(x=0.7z_{}+0.3z_{}\). We may also have that

\[c_{}=0.4z_{}+0.6z_{}c_{}=0.4z_{}+0.6z_{}.\]

Then, \(x^{T}c_{}=0.1>x^{T}c_{}=-0.1\), so that the prediction is waterbird, and thus incorrect. This is caused by the presence of harmful components in _both_ the class embedding (caused by seeing too many images with water described as waterbirds) and the datapoint embedding (where the water background appears). Thus our goal is to _remove_ harmful components (the \(z_{s}\)'s) and _boost_ helpful components (the \(z_{r}\)'s). We explain our approach towards doing so next.

### RoboShot: Zeroshot robustification with LLM

We describe RoboShot in Algorithm 1. It uses representations of insights from language models to shape input and class embeddings to remove harmful components and boost helpful ones. Figure 2 is helpful in understanding the intuition behind these procedures. The left part (a) illustrates the effect of RoboShot on a true dataset. Note how unhelpful directions are neutralized while others are boosted. The illustration on the right (b) shows this effect on the waterbirds running example.

```
1:Input embedding \(x\), class embeddings \(c^{0},c^{1}\), harmful insight representations \(v^{1},,v^{|S|}\), helpful insight representations \(u^{1},,u^{|R|}\)
2:for\(j\{1,2,,|S|\}\)do
3: Reject harmful insight \(v_{j}\): set \(x x- x,v^{j}/ v^{j},v^{j} v^{j}\)
4: Renormalize \(x=x/ x\)
5:endfor
6:for\(k\{1,2,,|R|\}\)do
7: Increase helpful insight \(u_{k}\): set \(x x+ x,u^{k}/ u^{k},u^{k} u^{k}\)
8:endfor
9:\(=\{x^{T}c^{0}<x^{T}c^{1}\}\)
10:Returns: Robustified zero-shot prediction \(\)
```

**Algorithm 2**RoboShot

Obtaining insight representations from LMsThe first question is how to obtain insight representations without training. To do so in a zero-shot way, we use _textual_ descriptions of harmful and helpful concepts by querying language models using _only the task description_. For example, in the Waterbirds dataset, we use the prompt "What are the biased/spurious differences between waterbirds and landbirds?". We list the details of the prompts used in the Appendix. Let \(s_{1},s_{2}\) be the text insights obtained from the answer (e.g., {'water background,'land background'}). We obtain a spurious insight representation by taking the difference of their embedding \(v=)-g(s_{2})}{ g(s_{1})-g(s_{2})}\), where \(g\) is the text encoder of our model.

In addition to attempting to discover harmful correlations, we seek to discover helpful components in order to boost their magnitudes past remaining harmful ones (or noise). The procedure is similar. We obtain insight representations using language models. For example, we ask "What are the true characteristics of waterbirds and landbirds?" and obtain e.g., {'short beak,'long beak'}. The remainder of the procedure is identical to the case of harmful components. Note that since we are seeking to boost (rather than remove) components, it is also possible to fix a multiplicative constant (to be treated as a hyperparameter) for the boosting procedure. That is, we could take \(x x+ x,u^{k}/ u^{k},u^{} u^{k}\) for some \(>0\). While this is possible if we have access to a labeled set that we can tune \(\) over, we _intentionally avoid doing so to ensure our procedure is truly zero-shot_.

Prompting a language model is typically inexpensive, which will enable obtaining multiple insight vectors \(^{1},,^{m}\). From these, we obtain an orthogonal basis \(v^{1},,v^{m}\) separately for harmful and helpful components. Thus we have access to recovered subspaces spanned by such components.

Removing and Boosting ComponentsRoboShot applies simple vector rejection to mitigate or remove harmful components, which is described in lines 2-5 of Algorithm 1. Similarly, it boosts helpful components as described in lines 6-9.

To see the impact of doing so, consider our earlier example. Suppose that \(v^{}=0.9z_{}+0.1z_{}\), and that this is our only harmful insight. Similarly, suppose that we obtain a single helpful insight given by \(v^{}=0.1z_{}+0.9z_{}\). Note that even these insights can be imperfect: they do not uniquely identify what are harmful or helpful concepts, as they have non-zero weights on other components.

We first obtain from removing the harmful component (ignoring normalization for ease of calculation) that

\[ x-}}{ v^{ },v^{}}v^{}=-0.0244z_{}+0.2195z_{}.\]

Then, we already we have that \(x^{T}c_{}=-0.1415<x^{T}c_{}=0.1415\), so that the correct class is obtained. In other words we have already, from having access to a single insight, neutralized a harmful correlation and corrected what had been an error. Adding in the helpful component further helps. We obtain

\[+,v^{}}{  v^{},v^{}}v^{}=-0.0006z_ {}+0.4337z_{}.\]

This further increases our margin. Note that it is not necessary to fully neutralize (i.e., to be fully invariant to) spurious or harmful components in our embeddings. The only goal is to ensure, as much as possible, that their magnitudes are reduced when compared to helpful components (and to benign components). In the following section, we provide a theoretical model for the magnitudes of such components and characterize the conditions under which it will be possible to correct zero-shot errors. We note that there is a variant of our approach that can also update class embeddings as well.

## 4 Analysis

Next, we provide an analysis that characterizes under what conditions RoboShot is capable of correcting zero-shot errors. First, we consider the following error model on the weights of the various representations. For all benign representations, we assume that \(_{b},_{b},_{b}(0,_{}^{2})\). That is, the magnitudes of benign components are drawn from a Gaussian distribution. The value of \(_{}\) is a function of the amount of data and the training procedure for the zero-shot model.

Next, we assume that the embedding insight \(v_{s}=_{i=1}^{k}_{i,s}z_{i}\) (where \(1 s S\)) satisfies the property that for \(i s\), \(_{i,s}(0,_{}^{2})\), while \(_{s,s}\) is a constant. In other words, the vectors \(v_{1},,v_{S}\) spanning the harmful component subspace are well-aligned with genuinely harmful concepts, but are also affected by noise. We seek to understand the interplay between this noise, benign noise, and the coefficients of the other vectors (i.e., helpful components). Let the result of rejecting embedding insights \(v_{1},,v_{S}\) be

\[=x-_{s=1}^{S}v_{s}}{||v_{s}||^{2}}v_{s}=_{i}A_{i}z_{ i}.\]We provide a bound on \(A_{s}\), the coefficient of a targeted harmful concept post-removal.

**Theorem 4.1**.: _Under the noise model described above, the post-removal coefficient for harmful concept \(s\) satisfies_

\[|[A_{s}]||_{ insight}^{2}}{_{s,s}^{2}}|+|_{t s}^{S} _{insight}^{2}}{_{t,t}^{2}}|,\]

_where \(k\) is the number of concepts._

The theorem illustrates how and when the rejection component of RoboShot works--it scales down harmful coefficients at a rate inversely proportional to the harmful coefficients of the insight embeddings. As we would hope, when insight embeddings have larger coefficients for harmful vectors (i.e., are more precise in specifying terms that are not useful), RoboShot yields better outcomes. In addition, we observe that the harmful coefficients decrease when the insight embeddings have less noise. In fact, we have that \(_{_{insight} 0}A_{s}=0\) -- the case of perfectly identifying harmful concepts. In the Appendix, we present additional theoretical results for control of helpful coefficients along with a combined result.

## 5 Experimental Results

This section evaluates the following claims about RoboShot:

* **Improving multi-modal models (Section 5.1)**: RoboShot improves zero-shot classification robustness of various multi-modal models, even outperforming prompting techniques that include spurious insight descriptions (which we do not have access to) in the label prompts.
* **Improving language models (Section 5.2)**: RoboShot improves zero-shot robustness when using language model embeddings for text zero-shot classification.
* **Extracting concepts from LM with varying capacities (Section 5.3)**: RoboShot can extract insights from language models with varying capacities. Improvements persist with weaker LMs.
* **Ablations (Section 5.4)** RoboShot benefits from both removing harmful and boosting helpful representations (line 3 and line 7 in RoboShot Algorithm 1).

**Metrics and how to interpret the results.** We use three metrics: average accuracy % (AVG), worst-group accuracy % (WG), and the gap between the two (Gap). While a model that relies on harmful correlations may achieve high AVG when such correlations are present in the majority of the test data, it may fail in settings where the correlation is absent. **A robust model should have high AVG and WG, with a small gap between them**.

**Baselines** We compare against the following sets of baselines:

1. **Multimodal baselines**: We compare against: (i) vanilla zero-shot classification (**ZS**) and (ii) zero-shot classification with group information (**Group Prompt ZS**). We do so across a variety of models: CLIP (ViT-B-32 and ViT-L-14) [RKH\({}^{+}\)21], ALIGN [JYX\({}^{+}\)21], and AltCLIP [CLZ\({}^{+}\)22]. Group Prompt ZS assumes access to spurious or harmful insight annotations and includes them in the label prompt. For instance, the label prompts for waterbirds dataset become [waterbird with water background, waterbird with land background, landbird with water background, landbird with land background]. We only report Group Prompt ZS results on datasets where spurious insight annotations are available.
2. **Language model baselines**: We compare against zero-shot classification using multiple language model embeddings, including BERT  and Ada [NXP\({}^{+}\)22] (**ZS**).

### Improving multi-modal models

**Setup.** We experimented on five binary and multi-class datasets with spurious correlations and distribution shifts, coming from a variety of domains: **Waterbirds**[SKHL19], **CelebA**[LLWT15], **CXR14**[WPL\({}^{+}\)17], **PACS**[LYSH17], and **VLCS**[FXR13]. We use the default test splits of all datasets. Dataset details are provided in the appendix. For CXR14, we use BiomedCLIP [ZXU\({}^{+}\)23],which is a variant of CLIP finetuned on biomedical images and articles. All experiments are conducted using frozen pretrained models.

**Results.** Table 1 shows that **RoboShot significantly improves the worst group performance (WG)** and maintains (and sometimes also improves) the overall average (AVG) without any auxiliary information (in contrast to Group Prompt, which requires access to spurious insight annotation).

Improved robustness nearly across-the-board suggests that both the insights extracted from LMs and the representation modifications are useful. We also provide insights insights into the case where our method does not improve the baseline (ALIGN model on Waterbirds) in Fig. 3. In Fig. 2(a), we visualize the original and projected input embeddings (\(x\) in green and red points, respectively), and the label embeddings (\(c^{0}\) and \(c^{1}\)). Fig. 2(a) (left) shows the embeddings from the ALIGN model. We observe that the projected embeddings (red) still lie within the original embedding space, even with reduced variance. In contrast, when examining the CLIP model embeddings (Figure 2(a) (right)), we observe that the projected embeddings are significantly distant from the original ones. Unsurprisingly, Figure 2(b) (left) reveals that \(v^{j}\) and \(u^{k}\) (harmful and helpful insight embeddings in black and blue stars, respectively) are not distinguishable in the text embedding space of ALIGN, collapsing the input embeddings after RoboShot is applied.

    &  &  &  &  \\   & & AVG & WG(\(\)) & Gap(\(\)) & AVG & WG(\(\)) & Gap(\(\)) & AVG & WG(\(\)) & Gap(\(\)) \\   & CLIP (ViT-B-32) & 80.7 & 27.9 & 52.8 & 81.6 & 43.5 & 38.1 & 82.0 & **54.4** & **28.6** \\  & CLIP (ViT-L-14) & 88.7 & 27.3 & 61.4 & 70.7 & 10.4 & 60.3 & 79.9 & **45.2** & **34.7** \\  & ALIGN & 72.0 & **50.3** & 21.7 & 72.5 & 5.8 & 66.7 & 50.9 & 41.0 & **9.9** \\  & AltCLIP & 90.1 & 35.8 & 54.3 & 82.4 & 29.4 & 53.0 & 78.5 & **54.8** & **23.7** \\   & CLIP (ViT-B-32) & 80.1 & 72.7 & 7.4 & 80.4 & 74.9 & 5.5 & 84.8 & **80.5** & **4.3** \\  & CLIP (ViT-L-14) & 80.6 & 74.3 & 6.3 & 77.9 & 68.9 & 9.0 & 85.5 & **82.6** & **2.9** \\  & ALIGN & 81.8 & 77.2 & 4.6 & 78.3 & 67.4 & 10.9 & 86.3 & **83.4** & **2.9** \\  & AltCLIP & 82.3 & **79.7** & **2.6** & 82.3 & 79.0 & 3.3 & 86.0 & 77.2 & 8.8 \\   & CLIP (ViT-B-32) & 96.7 & 82.1 & 14.6 & 97.9 & 82.7 & 15.2 & 97.0 & **86.3** & **10.7** \\  & CLIP (ViT-L-14) & 98.1 & 79.8 & 18.3 & 98.2 & **86.6** & **11.6** & 98.1 & 83.9 & 14.2 \\  & ALIGN & 95.8 & **77.1** & **18.7** & 96.5 & 65.0 & 31.5 & 95.0 & 73.8 & 21.2 \\  & AltCLIP & 98.5 & 82.6 & 15.9 & 98.6 & 85.4 & 13.2 & 98.7 & **89.5** & **92.2** \\   & CLIP (ViT-B-32) & 75.6 & 20.5 & 55.1 & - & 76.5 & **33.0** & **43.5** \\  & CLIP (ViT-L-14) & 72.6 & 4.20 & 68.4 & - & 71.1 & **12.6** & **58.5** \\  & ALIGN & 78.8 & 33.0 & 45.8 & - & 77.6 & **39.8** & **37.8** \\  & AltCLIP & 78.3 & 24.7 & **53.6** & - & 78.9 & **25.0** & 53.9 \\  CXR14 & BiomedCLIP & 55.3 & 28.9 & 26.4 & - & 56.2 & **41.6** & **14.6** \\   

Table 1: Main results. Best WG and Gap performance **bolded**, second best underlined.

Figure 3: (a) Original (green) and projected (red) input embeddings \(x\), and label embeddings \(c^{0}\) and \(c^{1}\). (b) label embeddings \(c^{0}\) and \(c^{1}\), harmful insight embeddings \(v^{k}\) (black star) and helpful insight embeddings \(u^{j}\) (blue star)

### Improving language models

**Setup.** We experimented on four text classification datasets: **CivilComments-WILDS**, **HateXplain**, **Amazon-WILDS** and **Gender Bias** classification dataset . We use the default test splits of all datasets. In text experiments, the distinctions between harmful and helpful insights are less clear than for images. For this reason, we only use harmful vector rejection (line 3 in RoboShot) in text experiments. CivilComments and HateXplain are toxic classification datasets with unwanted correlation between toxicity labels and mentions of demographics (e.g., male, female, mentions of religions). The datasets are annotated with demographic mentions of each text, and we directly use them to construct \(v^{j}\). For Amazon and Gender Bias datasets, we query LMs with task descriptions. All experiments are conducted using frozen pretrained models.

**Results.** Table 2 shows that RoboShot also improves zero-shot text classification in text datasets, as shown by our consistent boost over the baselines across all datasets.

### Extracting concepts from LMs with varying capacities

**Setup.** We use LMs with different capacities: **ChatGPT**, **Flan-T5**, **GPT2**, and **LLaMA**, to get harmful and helpful features insights (\(v^{j}\) and \(u^{k}\)).

**Results.** Table 3 shows that RoboShot can get insights on \(v^{j}\) and \(u^{k}\) from LMs of various capacities and improves zero-shot performance. Even though the the LM capacity correlates with the zero-shot performance, RoboShot with weaker LMs still outperforms zero-shot (ZS) baseline.

### Ablations

**Setup.** We run RoboShot with only harmful component mitigation (reject \(v^{j}\): RoboShot line 3), only boosting helpful vectors (increase \(u^{k}\): RoboShot line 7), and both.

**Results.** The combination of both projections often achieves the best performance, as shown in Table 4. Figure 4 provides insights into the impact of each projection. Rejecting \(v^{j}\) reduces variance in one direction, while increasing \(u^{k}\) amplifies variance in the orthogonal direction. When both projections are applied, they create a balanced mixture. We note that when doing both projections does not

    &  &  &  \\   & & AVG & WG(\(\)) & Gap(\(\)) & AVG & WG(\(\)) & Gap(\(\)) \\   & BERT & 48.1 & 33.3 & 14.8 & 49.7 & **42.3** & **7.4** \\  & Ada & 56.2 & 43.2 & 13.0 & 56.6 & **44.9** & **11.7** \\   & BERT & 60.4 & 0.0 & 60.4 & 57.3 & **14.0** & **43.3** \\  & Ada & 62.8 & 14.3 & 48.5 & 63.6 & **21.1** & **42.5** \\   & BERT & 81.1 & 64.2 & 16.8 & 81.0 & **64.4** & **16.6** \\  & Ada & 81.2 & 63.4 & **17.8** & 82.9 & **63.8** & 19.1 \\   & BERT & 84.8 & 83.7 & 1.1 & 85.1 & **84.9** & **0.2** \\  & Ada & 77.9 & 60.0 & 17.9 & 78.0 & **60.1** & 17.9 \\   

Table 2: RoboShot text zero-shot classification. Best WG in **bold**.

   Dataset &  &  &  &  &  \\   & AVG & WG & AVG & WG & AVG & WG & AVG & WG & AVG & WG \\  Waterbirds & 80.7 & 27.9 & 82.0 & **54.4** & 72.1 & 32.4 & 88.0 & 39.9 & 84.8 & 36.5 \\  CelebA & 80.1 & 72.7 & 84.8 & 80.5 & 77.5 & 68.2 & 80.3 & 74.1 & 84.2 & **82.0** \\  PACS & 96.7 & 82.1 & 97.0 & **86.3** & 96.2 & 80.3 & 97.2 & 74.0 & 94.8 & 71.9 \\  VLCS & 75.6 & 20.5 & 76.5 & **33.0** & 69.6 & 20.5 & 75.5 & 26.1 & 72.0 & 18.2 \\   

Table 3: RoboShot with LMs of varying capacity. Best WG **bold**, second best underlinedimprove the baseline, using only \(u^{k}\) or \(v^{j}\) still outperforms the baseline. For instance, the ALIGN model in the Waterbirds dataset achieves the best performance with only \(u^{k}\) projection. This suggests that in certain cases, harmful and helpful concepts are intertwined in the embedding space, and using just one projection can be beneficial. We leave further investigation to future work.

## 6 Conclusion

We introduced RoboShot, a fine-tuning-free system that robustifies zero-shot pretrained models in a truly zero-shot way. Theoretically, we characterized the quantities required to obtain improvements over vanilla zero-shot classification. Empirically, we found that RoboShot improves both multi-modal and language model zero-shot performance, has sufficient versatility to apply to various base models, and can use insights from less powerful language models.