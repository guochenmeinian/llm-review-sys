# Diffusion of Thought: Chain-of-Thought Reasoning

in Diffusion Language Models

 Jiacheng Ye\({}^{1}\), Shansan Gong\({}^{1}\), Liheng Chen\({}^{1}\), Lin Zheng\({}^{1}\),

**Jiahui Gao\({}^{2}\),Han Shi\({}^{2}\),Chuan Wu\({}^{1}\),Xin Jiang\({}^{2}\), Zhenguo Li\({}^{2}\),Wei Bi\({}^{3}\),Lingpeng Kong\({}^{1}\) \({}^{1}\)** The University of Hong Kong \({}^{2}\) Huawei Noah's Ark Lab \({}^{3}\) Tencent AI Lab {carsonye, sansa933}@connect.hku.hk

Equal contribution.

###### Abstract

Recently, diffusion models have garnered significant interest in the field of text processing due to their many potential advantages compared to conventional autoregressive models. In this work, we propose Diffusion-of-Thought (DoT), a novel approach that integrates diffusion models with Chain-of-Thought, a well-established technique for improving the reasoning ability of autoregressive language models. In contrast to autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT allows reasoning steps to diffuse over time through a diffusion language model and offers greater flexibility in trading-off computation for reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication, boolean logic, and grade school math problems, with a small diffusion model outperforming a much larger autoregressive model in both efficiency and accuracy. In addition to that, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning with diffusion language models.

## 1 Introduction

Large language models (LLMs) have had a profound impact on the entire field of artificial intelligence , transforming our approach to addressing classical problems in natural language processing and machine learning. Among the most notable aspects of LLMs is their remarkable reasoning ability, which many researchers consider to be a representative emergent capability brought about by LLMs . Chain-of-thought prompting (CoT) ), which generates a series of intermediate reasoning steps in autoregressive (AR) way, has emerged as a central technique to support complex reasoning processes in LLMs. Despite advancements, errors in intermediate CoT steps can lead to inaccurate answers , posing self-correction difficulties , and concerns about CoT's inefficiency have been highlighted in recent studies .

Recently, diffusion models have attracted interest in text processing  as a result of success in the vision domain and distinctive modeling strengths over autoregressive models , offering potential benefits including global planning ability , self correction  and efficiency . As part of the research community effort, pre-trained diffusion language models such as Plaid  and SEDD  have shown significant progress in text generation capabilities. Although they have not yet attained the scale and capabilities of existing proprietary autoregressive LLMs like GPT-4 , these models have demonstrated performance on par with GPT2  and the scaling law  in diffusion language models have been highlighted in Plaid. As a result, it becomes pertinent to explore the following question: _can diffusion language models also leverage the CoT-style technique to gain enhanced complex reasoning abilities?_This work presents a preliminary study on this question. We propose Diffusion of Thought (DoT), an inherent chain-of-thought method tailored for diffusion models. In essence, DoT progressively updates a sequence of latent variables representing thoughts in the hidden space, allowing reasoning steps to diffuse over time in parallel. We also introduce a multi-pass variant of DoT which focuses on generating one thought at a time to compensate for causal bias. To condition on complex queries, instead of using gradient-based classifier guidance , DoT trains and samples from the denoising model using the classifier-free guidance as in Gong et al. , to provide more reliable controlling signals on exact tokens.

Furthermore, to improve the self-correcting capability of the diffusion model, DoT integrates training-time sampling algorithms to learn to recover from errors originating from prior or current reasoning steps. This feature offers a fresh angle on the issue of error accumulation  inherent in autoregressive models. Finally, we adapt a conditional ODE Solver  for DoT during inference time to accelerate the inference of continuous diffusion models. We show DoT enjoys flexibility in trading off computation (reasoning time) and performance as more complex problems may necessitate increased computation in reasoning .

From a methodological standpoint, DoT shares similarities with the recently proposed Implicit CoT approach , where the latter learns thoughts in hidden states across transformer layers to improve the time efficiency of autoregressive CoT generation. A schematic illustration of CoT, Implicit CoT, and DoT can be found in Figure 1.

The main contributions of our paper are threefold:

1. We first introduce the reasoning technique for diffusion models (DoT), and showcase its advantages in simple reasoning tasks (digit multiplication and boolean logic) when compared to autoregressive CoT and Implicit CoT. DoT achieves up to \(27\) speed-up without performance drop (SS4.2).
2. We further adapt DoT to continuous and discrete diffusion base models, and introduce two training-time sampling algorithms to improve its self-correction ability. DoT exhibits superior performance compared to GPT2 with CoT on grade school math problems, enabling a small diffusion model to outperform a 4.6x larger autoregressive model, showing the potential of text diffusion models for complex reasoning (SS4.3).
3. Our analysis demonstrates the flexibility of DoT in the trade-off between reasoning time and performance (SS4.4), and showcases DoT's self-correction capability (SS4.6). We also find that self-consistency decoding can further improve DoT and its multi-pass variant (SS4.5).

Although it is challenging for current pre-trained diffusion language models to directly compete with LLMs that are hundreds of times larger in parameter size, our study emphasizes the possibility of their complex reasoning abilities and highlights the substantial potential in developing LLMs that go beyond the autoregressive paradigm. We release all the codes at [https://github.com/HKUNLP/diffusion-of-thoughts](https://github.com/HKUNLP/diffusion-of-thoughts).

## 2 Preliminaries

This section introduces key concepts and notations in diffusion models for text generation. Detailed formulations and derivations are provided in Appendix A.

Figure 1: Illustration of reasoning approaches. (a) **Answer-only** and (b) **CoT** generate left-to-right tokens by prompting autoregressive language model. (c) **Implicit CoT** replaces horizontal reasoning (CoT) with vertical reasoning from shallow layer to deep layer . (d) **DoT** generates reasoning path along with the diffusion timesteps.

A typical diffusion model contains the forward and reverse process. For each forward step \(q(_{t}|_{t-1})\), we gradually inject noise into the data representation \(_{t-1}\) from the last timestep to obtain \(_{t}\). Here \(t=1,2,...,T\) and the larger \(t\) corresponds to noisier data. For reverse process, the ultimate goal is to recover the original \(_{0}\) by denoising \(_{t}\): \(p_{}(_{0:T}):=p(_{T})_{t=1}^{T}p_{}( _{t-1}|_{t})\). We model the learning process \(p_{}(_{t-1}|_{t})\) using the proposed diffusion model \(_{}(_{t},t)\).

Previous text generation using diffusion models almost contains two categories: (1) Continuous diffusion models such as Diffusion-LM , which relies on a mapping function between the real values and feasible integral point; (2) Discrete diffusion models like D3PM , which directly formulate the problem as the integer program. Continuous diffusion models map the discrete text \(\) into a continuous space through an embedding function \(()\), and its inverse operation is called rounding. The forward perturbations are applied according to \(q(_{t}|_{t-1})=(_{t};}_{t-1},_{t})\), where \(_{t}(0,1)\) represents different scales of the Gaussian noise. Plaid  is a continuous diffusion language model trained from scratch on 314B tokens with \(1024\) context size. It is currently the largest scale diffusion language model with 1.3B parameters. In the case of discrete diffusion models, each \(_{t}\) is represented as a discrete random variable using one-hot vectors in \(\{0,1\}^{K}\), where \(K\) denotes the vocabulary size. They define \(q(_{t}|_{t-1})\) through a transition matrix, making it a point mass with probability on an absorbing state [MASK] or a uniform distribution over the vocabulary size. SEDD  is a recently trained-from-scratch discrete diffusion language model with small and medium size similar to GPT2.

For sequence-to-sequence (seq2seq) generation, which involves a pair of sequences \(^{x}\) and \(^{y}\), DiffuSeq  treats these two sequences as a single one \(^{z}=^{[x;y]}\) and uses a left-aligned mask \([;]\) during the forward and reverse diffusion process to distinguish them. Unlike traditional diffusion models that corrupt the entire \(_{t}\), DiffuSeq only adds noise to those entries with the mask value of 1 (e.g., \(_{t}\)). This modification, termed partial noising, tailors diffusion models for conditional language generation, and set a difference between the gradient-based token guidance in  and .

## 3 Diffusion-of-Thoughts

In this section, we begin with an overview of our method and its relationship with other reasoning paradigms (SS3.1). We then introduce Diffusion-of-Thoughts (DoT) as well as its multi-pass variant (DoTMP; SS3.2), as illustrated in Figure 2. Following this, we outline the implementation of our training (SS3.3) and inference (SS3.4) protocols.

### Overview

Without loss of generality, we use the mathematical problem-solving task as our running example. A problem statement and its correct answer are denoted as \(\) and \(\), respectively. We employ a language model with parameters \(\), represented as \(p_{}^{LM}\), to find the solution for each problem. For regular usage of language models without Chain-of-Thoughts (CoT), the final answer \(\) is generated directly as \( p_{}^{LM}(|)\). The CoT approach introduces meaningful intermediate steps or rationales \(_{1},,_{n}\) for language models to bridge \(\) and \(\), resulting in the output \( p_{}^{LM}(|,_{1...n})\). For implicit CoT , the hidden representations of rationales \(_{1},,_{n}\) are distilled into transformer layers, leading to \( p_{}^{}(|,_{1...n})\). Similarly but differently, for DoT, these representations are distributed over diffusion timestep \(t\) as \( p_{}^{}(|,_{t})\), where \(_{t}\) corresponds exactly to the noised data in diffusion models.

### Modeling

We begin by observing the gradient-based token guidance fails to do accurate conditioning as the model cannot exactly recover each conditioning token (see Table 2). This is vital, especially in mathematical reasoning, as it is expected to perform reasoning based on exact tokens (e.g., numbers) in the problem statement, rather than more compact gradient signals. For this, we adopt DiffuSeq-style  classifier-free conditioning during the fine-tuning of Plaid, where all rationales are generated by the backward diffusion process in parallel, with all the conditional tokens fixed as still. Specifically, the problem context \(s\) is concatenated with the rationales \(_{1...n}\) during training and sampling, while the noise is only partially imposed to the rationale part in \(_{1...n}\), keeping \(\) anchored as the condition.

We further propose a multi-pass (MP) variant of DoT, denoted as DoTMP, which generates rationales in a thought-by-thought paradigm. This method disentangles the generation of multiple rationales and introduces casual inductive bias such that later rationale can be guided by stronger condition signals of prior rationales during the generation. Specifically, in the first pass, we generate the first rationale by \(_{1} p_{}^{DoT}(_{1}|,_{t}^{r _{1}})\), where \(_{t}^{r_{1}}\) is the noised vector representation of \(_{1}\) in diffusion model. Then \(_{1}\) is connected to \(\) as the condition \([;_{1}]\) to get \(_{2} p_{}^{DoT}(_{2}|[;_{1}],_{t}^{r_{2}})\), and then we have \([;_{1};_{2}]\). Through multiple iterations, we can get the final answer: \( p_{}^{DoT}([[;_{1};...; _{n}],_{t}^{r_{n}})\).

### Training

Scheduled samplingDiffusion models have intrinsic self-correcting capability through the multi-step denoising process. To further improve their self-correcting ability, we design a _scheduled sampling_ mechanism tailored for diffusion models such that self-generated error thoughts in previous timesteps are exposed and corrected during the training stage. Formally, for any timesteps \(s\), \(t\), \(u\) that satisfy \(1<s<t<u<T\), \(_{t}\) is sampled from the forward distribution \(q(_{t}_{0})\) in the training stage while during inference it is sampled from \(q(_{t}_{}(_{u};u))\) instead, where \(_{}\) is a denoiser neural network that reparameterizes \(_{q}[_{0}|_{t}]\). The presence of such exposure bias may impede the model's ability to recover from erroneous thoughts during the generation process as the model \(_{}\) has only been trained on corruptions \(_{t}\) diffused from oracle data. To mitigate this problem, we mimic the inference stage with probability \(_{i}\) during training depending on the current training step \(i\), and \(_{i}\) linearly decays from 1 to \(_{min}\). Specifically, for time-step \(t\), we randomly sample a former time-step \(u\{t+1,,T\}\), obtain \(_{u}\) by forward noising and perform a model forward pass to get a predicted \(}_{0}=_{}(_{u};u)\)). \(_{t}\) is then sampled from \(q(_{t}}_{0})\) to replace the regular one in loss calculation. Compared with scheduled sampling for autoregressive models, such a mechanism in DoT helps the model to recover from errors by considering global information instead of relying on the left-side tokens.

Coupled samplingIn DoTMP, correct previous thoughts are given in the training stage, which is not given during inference. Similar to auto-regressive decoding, DoTMP may suffer from error accumulation during the thought-by-thought generation process. To enhance the self-correction ability of DoTMP, we propose a coupled sampling mechanism by adding noise not only to the current thought but also to previous thoughts during training with some probability. For instance, the previous sequence \(_{0}=([;_{1}])\) will be modified to \(_{0}=([;_{1};_{2}])\), with the partial noise being applied to \([_{1};_{2}]\) rather than just the last rationale \(_{2}\). Therefore, the model learns to be robust to errors in \(_{1}\) when predicting \(_{2}\), which better aligns with the inference stage. The new \(_{0}\) will be reparameterized into \(_{t}\) as before and other procedures keep the same.

Training objectiveGiven a set of training data for problem-solving tasks of size \(D\): \(\{^{j},_{1 n}^{j},^{j}\}_{j D}\), we have two training settings for DoT models: one is training from scratch, while the other is fine-tuning from the pre-trained diffusion model. In both training settings, we share the same training objective. For example, the objective is to minimize the negative

Figure 2: Demonstration of DoT pipeline. DoT diffuses all possible thoughts across diffusion timestep \(t\). Multi-pass DoT disentangles each rationale and introduces causal bias. The stacked circles stand for the marginalization over other potential reasoning paths, which is implicitly carried out during the training of diffusion models.

variational lower bound \(_{}(^{z})\) in continuous diffusion models:

\[_{}(^{z})= _{q(_{0}|^{z})} {_{T}|^{z})}{p_{}(_{T})}}_{ }+_{}(_{0})}_{ }(^{z}|_{ 0})}_{}, \]

where the rounding loss regularizes the embedding learning and the diffusion loss sums up the KL divergence of each time step \(t\) with different weighting terms. Please refer to Appendix A for a detailed training objective formulation of continuous and discrete diffusion models.

### Inference

One of the significant advantages of diffusion models is their inference flexibility. Naturally, more complex problems may necessitate increased computation in reasoning time [2; 54], which can be controlled by setting a larger backward timestep \(T\) in DoT. However, continuous diffusion such as Plaid usually requires more timesteps, e.g., 4096 , to converge. To accelerate the sampling process of the continuous diffusion, we adapt the ODE Solver [38; 39] into a conditional form to fit the conditional training process (detailed in Appendix A.4). Moreover, sharing a similar idea of MBR , self-consistency  boosts the performance of CoT significantly by generating and aggregating multiple samples. In the context of diffusion models, we can also expect its potential improvement using self-consistency, thanks to their ability to naturally produce diverse responses . After sampling \(m\) times to obtain multiple reasoning pathways \((_{i;1 n},_{i})\) from DoT, self-consistency involves marginalizing over \(_{i;1 n}\) by taking a majority vote over \(_{i}\), i.e., \(_{}_{i=1}^{m}(_{i}=a)\). We consider this as the most "consistent" answer among the candidate set of \(m\) answers.

## 4 Experiments

We conduct experiments on both simple multi-digit multiplication and boolean logic reasoning as well as complex grade school math problems, to explore the reasoning paradigm in diffusion models.

### Experimental Setup

Datasets and Metrics.Following Deng et al. , we employ the four-digit (\(4 4\)) and five-digit (\(5 5\)) multiplication problems from the BIG-bench benchmark , known to be challenging for LLMs to solve without CoT. Given that arithmetic reasoning is just one type of the reasoning ability, we also incorporate a boolean logical reasoning task . For more complex tasks, grade school math problems require both language understanding and mathematical reasoning, so we adopt the widely-used GSM8K dataset . We use the augmented training data from Deng et al.  and keep all original test sets unchanged. The statistics are listed in Appendix B.1. For both datasets, we use accuracy to measure the exact match accuracy of predicting the final answer, and throughput to measure the number of samples processed per second (it/sec) during inference with a batch size of \(1\).

Base Models.When training from scratch, we follow DiffuSeq2 to use a 12-layer Transformer  encoder with similar size as GPT2-small (124M). We also use Plaid3 (1.3B) , SEDD-small4 (170M) and SEED-medium (424M)  as pre-trained diffusion language models for further fine-tuning. Both Plaid and SEDD are pre-trained on OpenWebText [10; 13], which is similar to that in GPT2, and the pre-training perplexity of Plaid and SEDD-small is on par with GPT2-small.

Baselines.We consider **Answer-only** and **CoT** as reasoning paradigms for comparison. Another important baseline is **Implicit CoT**, which distills thoughts into transformer layers to accelerate CoT reasoning. We use GPT-2  at various scales (i.e., small 124M, medium 355M, and large 774M) as model baselines, known as conventional autoregressive language models. We mainly consider fine-tuning the model due to the relatively small model size, but we also consider prompting the strong commercial LLM **ChatGPT** gpt-3.5-turbo-1106 using CoT few-shot demonstrations for completeness. We use \(5\)-shot in the few-shot setting.

Implementation Details.During tokenization, we treat all the digits as individual tokens. For DoTMP, we append a special token <EOS> to the last thought, so when the model generates a thought followed by <EOS>, it stops generating further, which enables the model to decide the number of rationales dynamically. We conduct all the experiments on 8 NVIDIA V100-32G GPUs. During training, we set \(_{min}\) to be \(0.95\) as we find decreasing the probability of oracle demonstration hinders model training. We choose coupled sampling \(=0.01,k=1\) and self-consistency \(m=20\). Following Plaid, we also adopt self-conditioning  during training. During inference, we set both the temperature of the score and output logit to 0.5 to sharpen the predicted output distribution while maintaining the ability to generate diverse samples. The sampling timesteps \(T\) is dynamic. By default, we set it to be \(64\). Considering that simple tasks do not necessitate an excessively large number of steps, we opt to reduce \(T\) while ensuring there is no notable performance drop. Other details are in Appendix B.3.

### Results on Digit Multiplication and Boolean Logic

We first train DoT for digit multiplication tasks and a boolean logical reasoning task as the preliminary investigation, as shown in the left part of Table 1. We observe that neither ChatGPT nor the distilled Implicit CoT model can reach 100% accuracy. GPT-2 can be fine-tuned to achieve high accuracy but sacrifices throughput during CoT. Interestingly, DoT can attain 100% accuracy for these tasks while maintaining significant throughput with diffusion sampling steps set at \(1\) for multiplication datasets and \(2\) for the boolean logical dataset, achieving maximum 27\(\) speed-up compared to GPT-2. This preliminary finding indicates that DoT performs well in modeling exact math computation or boolean logic reasoning and benefits from its computational efficiency.

    &  &  &  &  \\   & Acc & Throughput & Acc & Throughput & Acc & Throughput & Acc & Throughput \\  Answer-only & & & & & & & & & \\ GPT2-small & 28.7 & 13.2 & 1.2 & 11.1 & 98.8 & 16.2 & 13.3 & 24.7 \\ GPT2-medium & 76.2 & 7.0 & 1.9 & 5.9 & 100 & 9.6 & 17.0 & 9.1 \\ GPT2-large & 33.6 & 4.8 & 0.9 & 4.0 & 100 & 7.4 & 12.7 & 9.1 \\ ChatGPT (few-shot) & 2.2 & 1.0 & 0.0 & 1.4 & 67.6 & 0.5 & 28.1 & 1.8 \\  Chain-of-Thoughts (CoT) & & & & & & & & \\ GPT2-small & 100 & 2.3 & 100 & 1.5 & 100 & 0.8 & 39.0 (41.6) & 2.0 \\ GPT2-medium & 100 & 1.2 & 100 & 0.8 & 100 & 0.5 & 43.9 & 1.1 \\ GPT2-large & 100 & 0.8 & 99.3 & 0.6 & 100 & 0.3 & 44.8 & 0.7 \\ ChatGPT (few-shot) & 42.8 & 0.1 & 4.5 & 0.1 & 75.8 & 0.2 & 61.5 & 0.2 \\  Implicit CoT & & & & & & & & \\ GPT2-small & 96.6 & 8.9 & 9.5 & 7.9 & - & - & 20.0 & 16.4 \\ GPT2-medium & 96.1 & 4.8 & 96.4 & 4.3 & - & - & 21.9 & 8.7 \\ 
**Diffusion-of-Thoughts (DoT)** & & & & & & & & \\ From-scratch & 100 & 62.5 & 100 & 61.8 & 100 & 55.2 & 4.6 & 22.7 \\ Plaid & 100 & 24.3 & 100 & 21.3 & 100 & 10.2 & 32.6 (36.3) & 0.3 \\ SEDD-small & 100 & 59.2 & 100 & 55.5 & 100 & 33.3 & 45.3 (51.8) & 1.0 \\ SEDD-medium & 100 & 31.8 & 100 & 28.5 & 100 & 17.2 & 53.5 (59.4) & 0.5 \\ 
**Diffusion-of-Thoughts (DoTMP)** & & & & & & & & \\ From-scratch & 100 & 11.8 & 100 & 9.5 & 100 & 3.7 & 5.5 & 8.6 \\ Plaid & 100 & 4.3 & 100 & 3.9 & 100 & 1.0 & 37.7 & 0.1 \\ SEDD-small & 100 & 9.9 & 100 & 9.2 & 100 & 3.3 & 43.2 & 0.2 \\ SEDD-medium & 100 & 4.5 & 100 & 4.0 & 100 & 1.7 & 53.3 & 0.1 \\   

Table 1: The main results on different problem-solving reasoning tasks. **Acc** (\(\)) is to measure the exact match accuracy of the predicted final answer. **Throughput** (\(\)) measures the number of samples processed per second during test with batch size equals to \(1\). The baseline results for Mult. and GSM8K datasets are taken from the implicit CoT paper  and have been validated for reproducibility by us. Bracketed numbers indicate the self-consistency results.

### Results on Grade School Math

We now move on to a much more complex grade school math task GSM8K as shown in the right part of Table 1. We first consider training DoT from scratch as in the previous tasks, but we are only able to achieve an accuracy of around \(5\%\), which is much lower than the fine-tuned version of GPT-2. This indicates the pre-trained natural language understanding capability is vital for grade school math. Once DoT is extended based on the pre-trained diffusion language models Plaid and SEDD, the performance is significantly improved after fine-tuning, where the DoT based on SEDD-medium outperforms similar-sized GPT2-medium with CoT by around 10%. Additionally, multi-pass DoT, with casual bias, performs slightly better than single-pass one on Plaid, while the latter is more efficient. The performance gap between SEDD and Plaid also highlights the importance of the training objective in pretraining diffusion LMs. Finally, we find that self-consistency further yields substantial improvements in DoT models owing to the diverse generations of diffusion model (SS4.5).

We further explore several alternatives and conduct an ablation study as in Table 2 when fine-tuning Plaid. As discussed above, continuing pre-training Plaid using the GSM8K-augmented dataset and performing reasoning with gradient-based conditioning is not a good choice for fine-tuning diffusion LMs on downstream tasks, because reasoning tasks require more specific guidance. An example of groundtruth and recovered text is shown below, where bold words in the query part are incorrectly recovered:

_Groundtruth:_ Two trains leave San Rafael at the same time. They begin traveling westward, both traveling for 80 miles. The next day, they travel northwards, covering 150 miles. What's the distance covered by each train in the two days? \(\)2*80=160\(\)\(\)150*2=300\(\)\(\)300+160=460\(\)\(\)460/2=230\(\)\(\)##### 230

_Recovered Text:_ **Three** trains leave San Juan at the same time. They **start** traveling westward, both traveling for 80 miles. The next day, they travel **southward**, covering 150 miles. What's the distance covered by each train in the two days? \(\)3*80=180\(\)\(\)180+80+150=340\(\)\(\)340/30=12.5\(\)\(\)##### 12.5

We can see there are three recovered query tokens that exhibit minor differences due to soft gradient guidance, causing interference with the model's comprehension of the problem. The ablation of two sampling strategies proposed in SS3.3 showcases their effectiveness. This provides evidence that better denoising models are trained using our training-time sampling strategies, allowing DoT models to self-correct more effectively during inference. Further analysis about self-correction is listed in SS4.6. In Figure 3, we further show the conditional ODE solver substantially speeds up the inference of continuous diffusion model Plaid, ensuring a decent performance with only 8 generation timesteps.

### Reasonability-efficiency Trade-off

The community has devoted substantial efforts to improve the reasoning capabilities of left-to-right language models, such as refining instructions [31; 67], finding better demonstrations [9; 55; 58], and designing elaborate decoding algorithm [43; 56; 57]. Non-autoregressive diffusion models naturally provide another simple way to enhance reasoning by allocating more timesteps during inference, albeit at the expense of efficiency. We show such efficiency trade-off in Figure 4(a), where wemeasure the reasoning steps as the average number of calling the model forward function for all the instances from the test set. For CoT and Implicit CoT baselines, we treat reasoning steps as the average number of output tokens for all the test instances5.

Given a small budget of reasoning steps (e.g., 1 or 2) on simpler tasks such as 5\(\)5, both DoT-Plaid and DoT-SEDD already have an accuracy of 100%, and no more reasoning steps are needed. For such cases of simple tasks, only a little computation cost is required for our method. For complex tasks such as GSM8K, we find DoT performance can continuously improve by allowing more reasoning steps, which indicates DoT can be efficient if we can sacrifice performance in certain scenarios. Specifically, DoT-SEDD-medium outperforms autoregressive CoT-GPT2-medium when we allocate 32 generation timesteps, and the performance continues improving when we increase the timesteps. In comparison, CoT and Implicit CoT with the autoregressive model are hard to be more efficient given their nature of token-by-token prediction. Overall, with DoT, we can flexibly control the trade-off between efficiency and performance for tasks with different difficulty levels.

### Self-consistency in DoT

Figure 4(b) shows the effectiveness of the self-consistency mechanism for Plaid DoT and its variant. We can see self-consistency improves both DoT and DoTMP, which is in line with the effectiveness of self-consistency for auto-regressive models . From Table 1, SEDD DoT is also significantly improved by self-consistency. This benefits from the diversity generation in DoT. We observe that DoT can generate diverse reasoning paths, such as <3*3=9><9*60=540> and <3*60=180><180*3=540> for the same question, providing cross-validation when selecting the most "consistent" answer. Note that different from autoregressive models, where diversity usually relies on decoding algorithms [8; 22], the natural advantage of the diffusion models is to generate different sentences with different random noises at each timestep.

### Self-correction in DoT

In this section, we provide several cases in Table 3 to show the self-correction ability of Plaid DoT, which acts as a distinct difference between diffusion models and autoregressive models. In the first case, we can see the model figures out all the correct thoughts together with only a single reasoning step (i.e., a single calling of the model forward function), and obtains the correct final answer in the second step. This mirrors how humans think in both fast and slow modes . In the second case where the problem is slightly harder, the model cannot give concrete thoughts in the first step but can still produce the correct answer through the later "slow" thinking process. We can see the solution framework, roughly outlining how the task will be carried out, is established at the very

Figure 4: **(a) Accuracy over reasoning steps using various methods. We measure the reasoning steps as the average number of calling the model forward function for instances from the test set. DoT provides a flexible way to balance accuracy and efficiency through the reasoning steps. (b) Absolute accuracy improvement versus samples in self-consistency per instance on the GSM8K dataset with Plaid DoT.**

beginning, and then the subsequent work is for refining and improving, which is also similar to how human performs a complex task. Interestingly, in DoT, the correct thoughts may not appear in a left-to-right paradigm as in the traditional chain-of-thought process. The third case serves as compelling evidence to illustrate this distinctive nature of diffusion-of-thought and how it diverges from the chain-of-thought approach. In step 4 the model has a wrong intermediate thought <2*3=4> with the latter thoughts and final answer computed correctly first. In the next step, the error in the wrong intermediate thought is fixed, which suggests both prior and latter thoughts can help in the prediction of the current thought. Furthermore, from these three cases, we observed that the model tends to maintain its prediction after it considers the answer to be complete. This suggests we can further enhance the inference efficiency by incorporating mechanisms such as early exit , and easier tasks can get earlier exits as observed in Table 3.

## 5 Related Work

### Diffusion Models for Text

Building upon advancements in diffusion models for image generation [21; 45], text continuous diffusion [15; 33] employs an embedding function to transform discrete text into the continuous space. Besides, discrete diffusion models [1; 23] directly introduce discrete noise to accommodate the discrete nature of texts, demonstrating significant potential [37; 65]. Numerous studies have shown that diffusion models can efficiently generate diverse texts [11; 14], and achieve competitive performance in various sequence-to-sequence NLP tasks, including machine translation [60; 61], summarization , code generation , and style transfer . In this work, we explore diffusion model for mathematical reasoning tasks.

### Pre-train and fine-tune Diffusion LMs

The pre-training and fine-tuning paradigm, while a familiar concept in NLP before the era of prompting methods , remains relatively under-explored for diffusion language models. Prior efforts include initializing diffusion models with pre-trained masked language models such as BERT  and RoBERTa  and XLM-RoBERTa . GENIE  adopts paragraph denoising to train encoder-decoder models, proving beneficial for summarization tasks. Plaid  and SEDD  are pioneers in pre-train diffusion language models from scratch, attaining comparative or better perplexity scores over GPT-2 . To the best of our knowledge, we are the first to explore the fine-tuning of a pre-trained diffusion language model for reasoning tasks.

### Reasoning Paradigms

Large language models usually excel in performing system-1  tasks that are processed quickly and intuitively by humans but struggle in system-2 tasks, which require deliberate thinking [48; 54; 4]. The chain-of-thought reasoning paradigm [31; 41; 54] has been widely employed to elicit reasoning

    & A role takes 2 hours of blue & Tommy is formulating for his charity by selling brownies for & When Freth cooks earned tomatoes into sauce, they \\  & apret and half that much white & 53 a slice and cheeschees for 54 a slice. If Tommy sells 43 & lose half their volume. Each 16 ounce of \\  & fermil. How many boils in total & brownies and 23 slices of cheeschee, how much money & tomatoes that she uses contains three tomatoes. \\  & does it take? & does Tommy raise? & & \\  & & & & \\ 
**Gold** & 22-t2abilities and can be further improved with various techniques. For instance, self-consistency  samples a diverse set of reasoning paths and selects the most consistent answer, while tree-of-thought  achieves different reasoning paths by tree search. Despite these advancements, errors introduced in intermediate CoT steps can lead to inaccurate answers , posing difficulties in self-correction . Moreover, there are concerns about the inefficiency of CoT . From the architecture perspective, we explore diffusion model as an alternative paradigm for reasoning.

## 6 Conclusion and Limitation

In this work, we propose diffusion-of-thought (DoT), integrating CoT reasoning with continuous diffusion models. We thoroughly evaluate DoT on representative mathematical reasoning tasks in various aspects, including their flexible control of reasoning efficiency, self-correction capability, and the ability to generate diverse reasoning paths. Considering pre-trained diffusion models are still in their early stages, particularly in terms of model scales compared to the more extensively studied autoregressive language models, our study presents an initial exploration into the reasoning ability of current diffusion language models. A notable limitation of DoT is its requirement for additional training to achieve accurate reasoning. With more powerful pre-trained diffusion models, we anticipate DoT can attain comparative or better generalization capabilities of auto-regressive language models while removing the need for specialized training. Moreover, extending the standard Transformer to other variants  is also a viable direction to further improve inference efficiency. Besides, the diffusion training techniques employed in this work are general and applicable to other tasks beyond mathematical reasoning. Extending our training recipes of diffusion language models to further scaled setups such as multi-task instruction tuning and other modalities , is an interesting avenue for future research.