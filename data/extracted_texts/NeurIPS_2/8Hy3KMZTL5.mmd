# Parameter-efficient Fine-tuning in Hyperspherical Space for Open-vocabulary Semantic Segmentation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Open-vocabulary semantic segmentation seeks to label each pixel in an image with arbitrary text descriptions. Vision-language foundation models, especially CLIP, have recently emerged as powerful tools for acquiring open-vocabulary capabilities. However, fine-tuning CLIP to equip it with pixel-level prediction ability often suffers three issues: 1) high computational cost, 2) misalignment between the two inherent modalities of CLIP, and 3) degraded generalization ability on unseen categories. To address these issues, we propose H-CLIP, a symmetrical parameter-efficient fine-tuning (PEFT) strategy conducted in hyperspherical space for both of the two CLIP modalities. Specifically, the PEFT strategy is achieved by a series of efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module among all learnable matrices. Since the PEFT strategy is conducted symmetrically to the two CLIP modalities, the misalignment between them is mitigated. Furthermore, we apply an additional constraint to PEFT on the CLIP text encoder according to the hyperspherical energy principle, i.e., minimizing hyperspherical energy during fine-tuning preserves the intrinsic structure of the original parameter space, to prevent the destruction of the generalization ability offered by the CLIP text encoder. Extensive evaluations across various benchmarks show that H-CLIP achieves new SOTA open-vocabulary semantic segmentation results while only requiring updating approximately 4% of the total parameters of CLIP.

## 1 Introduction

The aim of open-vocabulary semantic segmentation is to create a segmentation model capable of labeling each pixel in an image with categories that are not limited to a specific closed set according to text descriptions. Vision-language foundation models , especially CLIP , are often utilized to endow open-vocabulary recognition capabilities. Consequently, open-vocabulary semantic segmentation essentially boil down to transferring these vision-language foundation models, originally trained with image-level supervision, to perform pixel-level predictions.

To this end, current methods  typically fine-tune CLIP on a benchmark dataset with segmentation annotations, i.e., COCO , to equip it with the segmentation ability. However, this often leads to three main issues. First, fine-tuning CLIP on limited categories would affect its generalization ability, resulting in significant performance degradation on unseen categories. Second, current fine-tuning strategies are usually asymmetrical, which inevitably causes a misalignment between the two inherent modalities of CLIP, i.e., image and text , which may lead to sub-optimal performance. Third, although remarkable performance gains, these approaches often rely on computationally extensive full fine-tuning, which raises concerns about scalability and affordability.

To address these issues, we propose a symmetric parameter-efficient fine-tuning (PEFT) strategy for CLIP, dubbed H-CLIP. Specifically, we implement this PEFT through a partial orthogonal fine-tuning (POF) strategy, which introduces a series of efficient block-diagonal learnable transformation matrices into the hyperspherical space. Then, to preserve CLIP's generalization ability, we leverage the hyperspherical energy principle [32; 38], which suggests that maintaining the same hyperspherical energy during fine-tuning preserves the intrinsic structure, i.e., generalization ability. In light of this, we upgrade our POF by incorporating orthogonal constraints in the learnable matrices for updating CLIP's text encoder, as orthogonal transformations keep the hyperspherical energy unchanged during fine-tuning. Subsequently, we introduce a dual cross-relation communication (DCRC) module to explicitly encourage cross-modal and cross-layer communications within all learnable matrices. This communication not only preserves the hyperspherical energy but also further mitigating the misalignment problem.

Extensive results demonstrate that H-CLIP achieves new state-of-the-art open-vocabulary semantic segmentation results across three benchmarks by fine-tuning CLIP with approximately 4% of the total parameters of CLIP.

## 2 Related Work

### Open-vocabulary Semantic Segmentation

Prior open-vocabulary semantic segmentation works typically perform this task through leveraging CLIP . initial efforts like  directly fine-tune CLIP on mainstream segmentation datasets, e.g., COCO . However, they claim that fine-tuning CLIP's encoder significantly reduces its ability to generalize to unseen classes. To address this issue, some methods [15; 8; 51; 49] swing to the opposite extreme, fine-tuning an additional mask generator  for segmentation while keeping CLIP frozen to maintain generalization-oriented recognition. However, this frozen parameter space lacks segmentation awareness, resulting in a misalignment between regions and text descriptions . Other studies [52; 50; 7] propose an advanced solution that fine-tunes only selected parameters, e.g., certain layers of CLIP, to enable pixel-level predictions while keeping most of CLIP's parameters fixed, thus minimizing losing of generalization. Although the advantages are remarkable, these methods often work with a very small learning rate, implicitly encouraging a small deviation from the pre-trained CLIP, limiting the segmentation performance. In a nutshell, the trade-off between preserving CLIP's generalization and learning segmentation knowledge persists, hindering the final performance. Based on the paradigm of existing fine-tuning-based methods, our method explores a better trade-off from a fresh viewpoint: hyperspherical space.

### Large-scale Model Fine-tuning

Along with the improvement of large-scale foundation models [26; 34; 28; 23; 53; 42; 41; 40; 60], e.g., segment anything model , numerous fine-tuning works [37; 36; 4; 57; 58; 14; 54; 47; 31; 61] are proposed to adapt these models to various downstream scenarios. The core of these approaches lies in updating only limited parameters to capture the specific characteristics of different scenarios, while keeping most parameters fixed to maintain generalization. In contrast, fine-tuning CLIP for open-vocabulary semantic segmentation often meets a dilemma. On the one hand, limited parameters typically fall short in facilitating the transition from a classification model, i.e., CLIP, to a segmentation task. On the other hand, directly increasing the number of trainable parameters risks undermining CLIP's ability to generalize to unseen classes, as experimented in CAT-Seg . Most methods [52; 48] solve this issue by simply freezing CLIP's text encoder and fine-tuning its image encoder, inevitably causing misalignment between the two modalities of CLIP. In this paper, we shed light on how to preserve generalization in a symmetric parameter-efficient fine-tuning manner and strive to explore an appropriate fine-tuning method for open-vocabulary semantic segmentation.

## 3 Preliminaries

### Hyperspherical Energy

Existing fine-tuning methods implicitly assume that a smaller Euclidean distance between the fine-tuned model and the pre-trained model indicates better preservation of the pre-trained ability. However,the Euclidean difference is unable to fully capture the degree of semantic preservation. According to the inspiration from Thomson problem which is to determine the minimum electrostatic potential energy configuration of \(N\) mutually-repelling electrons on the surface of a unit sphere, we adopt the _Hyperspherical Energy_ to characterize the diversity of the model. The hyperspherical energy function of a fully connected layer \(\) is defined as \(()\!:=\!_{i j}\|}_{i}-}_{j}\|^{-1}\), where \(}_{i}\!:=\!_{i}/\|_{i}\|\) denotes the \(i\)-th normalized neuron. The power of the model representation can be characterized by the hyperspherical energy of its neurons. Higher energy implies higher redundancy, while lower energy indicates that these neurons of the model are more diverse. For the original semantic information not to be destroyed in the case of fine-tuning, we hypothesize that a good fine-tuning model should have a minimal difference in hyperspherical energy compared to the pre-trained model:

\[_{}()-(^{0})~{}~{} ~{}~{}_{}_{i j}\|}_{i}- }_{j}\|^{-1}-_{i j}\|}_{i}^{0}-}_{j} ^{0}\|^{-1}. \]

One can easily observe that the attainable minimum is zero for Eq. (1). In this case, the hyperspherical energy should satisfy an invariance property (the application of the same orthogonal transformation for all neurons demonstrates the pairwise hyperspherical similarity). Based on the hyperspherical energy invariance property, the minimum of zero can be achieved as long as \(\) and \(^{0}\) differ only up to a rotation or reflection, i.e., \(\!=\!^{0}\) in which \(\!\!^{d d}\) is an orthogonal matrix (The determinant \(1\) or \(-1\) means rotation or reflection, respectively).

### Notation of Tensor Product

In this section, we introduce the fundamental concept underlying our DCRC (Sec. 4.3): tensor product. A p-order tensor is indexed by \(p\) indices and can be represented as a multidimensional array of data. Formally, a p-order tensor \(\) can be written as \(=(a_{i_{1},i_{2},,i_{p}})^{n_{1} n_{2}  n_{p}}\). Slices of a tensor are matrices defined from the tensor by holding all but two indices constant. For a \(3\)-order tensor, \((:,:,k)\) corresponds the \(k^{}\) frontal slice. For \(p\)-order tensors, matrix slices of \(p\)-order tensors can be referenced using linear indexing by reshaping the tensor into an \(n_{1} n_{2} n_{3}n_{4} n_{p}\)\(3\)-order tensor and referring to the \(k^{}\) frontal slice as \((:,:,k)\). \(_{i}^{n_{1} n_{2} n_{p-1}}\) for \(i=1,,n_{p}\) denotes the \((p-1)\)-order tensor created by holding the \(p\)th index of \(\) fixed at \(i\). It is possible to create a tensor in a block circulant pattern, where each block is a tensor of \((p-1)\)-order:

\[()=_{1}&_{n_{p}}& _{n_{p}-1}&&_{2}\\ _{2}&_{1}&_{n_{p}}&&_{3}\\ &&&&\\ _{n_{p}}&_{n_{p}-1}&_{n_{p}-2}&& _{1},\]

where \(()\) creates a block circulant tensor and the size of \(()\) is \((n_{1}n_{p} n_{2}n_{p} n_{p-2}n_{p} n_{p-1})\). define \(()\) to take an \(n_{1} n_{p}\) tensor \(\) and return an \(n_{1}n_{p} n_{2} n_{p-1}\) block tensor in the following way:

\[()=_{1}&_{2}& &_{n_{p}}^{T}.\]

The operation that takes unfold back to tensor form is the "fold" command. Specially, \((,n_{p})\) takes an \(n_{1}n_{p} n_{2} n_{p-1}\) block tensor and returns an \(n_{1} n_{p}\) tensor, defined as:

\[((),n_{p})=.\]

## 4 Methodology

### Overview of H-CLIP

Fig. 2 illustrates the proposed H-CLIP framework, which is based on two core components: (1) POF updates the pre-trained parameter space of CLIP using a series of block-diagonal transformation matrices. According to analysis in Sec. 1, each parameter matrix in CLIP's text encoder is orthogonal to preserve generalization. (2) DCRC incorporates cross-modal and cross-layer communication within all tunable matrices, facilitating alignment between different modalities.

### Partial Orthogonal Fine-tuning

The core idea of partial orthogonal fine-tuning (POF) is to introduce the concept of hyperspherical space for fine-tuning CLIP. In this hyperspherical space, we fine-tune CLIP's text encoder under an orthogonality design principle from OFT  to preserve the hyperspherical energy of the pre-trained parameter space. Similarly, we use Cayley parameterization  to ensure a tunable matrix \(\) is strictly orthogonal, formally as:

\[=(+)(-)^{-1}, \]

where \(\) is skew-symmetric. Here, for \(\) in CLIP's image encoder, we remove the orthogonality constraint, defined as:

\[^{}=^{}=, \]

where \(\) is an identity matrix. Considering the relatively large dimension \(d\) of the pre-trained matrix, for better efficiency, we introduce a block-diagonal structure by parameterizing \(\) with \(b\) blocks, formally as:

\[=(_{1},_{2},_{i},,_{b})= _{1}&\\ &&\\ &&_{b}, \]

where \(_{i}^{d/b d/b}\). Specifically, denote \(^{V}=\{_{v1},,_{v},,_{vL}\}\) and \(^{E}=\{_{e1},,_{eL},,_{eL}\}\) as the sets of block-diagonal matrices in CLIP's image encoder and text encoder, respectively, where \(L\) is its number of Transformer layers, \(_{v}^{d_{v} d_{v}}\), and \(_{e}^{d_{e} d_{e}}\). For simplicity, we set \(d_{v}=d_{e}=d\). Overall, we develop a H-CLIP framework, and for an input feature map \(_{}\) in the \(^{}\) Transformer layer of CLIP, the right branch produces the adjusted feature map via H-CLIP, \(}_{}\), formally via:

\[}_{}=_{}(_{}; {R}_{}_{}),&_{}^{V}\\ _{}(_{};_{}_{}),&_{}^{}_{}=_{}_{}^{}= &, \]

where \(_{}\) is a pre-trained weight matrix in \(^{}\) layer of CLIP's encoder, and \(_{}\) represents \(^{}\) layer of CLIP's encoder. During the fine-tuning phase, H-CLIP is fine-tuned in conjunction with the original parameter space of CLIP, which is loaded from the pre-trained checkpoint and remains frozen.

### Dual Cross Relation Communication

Although in POF, we relax the orthogonal constraint for CLIP's image encoder to learn segmentation knowledge, each layer of the image encoder still incorporates a limited number of parameters, which largely restricts the flexibility of the projection adjustment due to the limitation of Hidden Markov

Figure 1: **A schematic representation of H-CLIP. In the H-CLIP framework, we propose a partial orthogonal fine-tuning strategy, where each pre-trained weight matrix is paired with a tuned block-diagonal transformation matrix, some of which are orthogonal to preserve generalization. Then, we introduce a dual cross-relation communication mechanism to facilitate communication among all matrices, enabling alignment between different modalities.**Chain along layers [24; 46; 36]. To address this limitation, one might consider fully fine-tuning instead of using a small number of parameters. However, this approach can cause a misalignment between image and text features in CLIP, resulting in sub-optimal performance . Based on the above analysis, we introduce Dual Cross-Relation Communication (DCRC), which facilitates interaction among different layers and modalities (i.e., text and image). DCRC explicitly enhances the flexibility of fine-tuned projection adjustments and prevents misalignment issues.

DCRC introduces cross-layer and cross-modality communication among different block-diagonal matrices, achieved through two relation projections. To do this, we first treat all blocks in \(^{}\) layer as an individual slice in this \(3\)-order tensor \(_{}\), which is derived as follows:

\[_{}=[_{v 1},_{ 1},,_{v  i},_{ i},,_{v b},_{ b}] ^{q q(b+b)}, \]

Where \(q=d/b\). Then, we treat the tensor \(_{}\) as an individual slice within a \(4\)-order tensor \(\), defined as follows:

\[=[_{1},_{2},,_{},,_{L}]^{q q(b+b) L}. \]

Initially, according to the characteristics of gradient propagation in deep learning theory, i.e., chain rule, each frontal slice \(_{ i}\{^{q q}\}^{(b+b) L}\) is updated sequentially in CLIP's encoder. As a result, updating the \(\) lacks cross-frontal-slice communication, limiting the flexibility of adjusting fine-tuned projection. To address this, we introduce two special tensor products, i.e., \(3\)-**order T-product** and **Higher-order T-product**.

**Definition 4.1(3-order T-product)** For \(^{n_{1} n_{2} n_{3}}\) and \(^{n_{2} l n_{3}}\), the \(3\)-order T-product \(^{n_{1} l x n_{3}}=* \) is defined as:

\[=*=(() ()), \]

where "\(\)" represents standard matrix product.

**Definition 4.2(Higher-order T-product)** For \(^{n_{1} n_{2} n_{3} n_{p}}\) and \(^{n_{2} l n_{3} n_{p}}\), the High-order T-product \(^{n_{1} l n_{3} n_{p}}= *\) is defined as:

\[=*=(()* ()). \]

If \(^{n_{1} n_{2} n_{3}}\), according to the **3-order T-product**, there is an invertible transform \(S_{3}():^{n_{1} n_{2} n_{3}}^{ n_{1} n_{2} n_{3}}\) in third dimension and it transform the Eq. (8) as:

\[=S_{3}^{-1}(S_{3}() S_{3}())=S_{3}^{-1}( }})=S_{3}^{-1}(}), \]

where \(}=}}\) denotes the frontal-slice-wise product (Definition 2.1 refers to ) \(}(;;;;i)=}(;;;;i)}(;;;;i), i=1,2,,n_{3}\) and \(S_{3}^{-1}()\) is the inverse transform of \(S_{3}()\). According to the definition of the frontal-slice-wise product, the invertible transform \(S_{3}()\) is formulated as:

\[}=S_{3}()=_{3}_{3}, \]

where "\(_{3}\)" denotes the mode-3 product and \(_{3}^{n_{3} n_{3}}\) is an arbitrary invertible matrix. Similarly, the inverse transform of Eq. (11) is derived as:

\[=S_{3}^{-1}(})=}_{3}_ {3}^{-1}. \]

Similarly, if \(^{n_{1} n_{2} n_{p}}\), according to the **Higer-order T-product**, there are invertible transform \(S_{i}():^{n_{1} n_{2} n_{p}} ^{n_{1} n_{2} n_{p}},i=3,4,,p\) in \(i^{}\) dimension and they transform the Eq. (9) as:

\[=^{-1}(()()) =^{-1}(}})=^{-1}(}), \]

where \(()=S_{p}(S_{p-1}( S_{3}()))\), \(}=}}\) denotes the frontal-slice-wise product \(}(;;;;i)=}(;;;;i)}(;;;;i), i=1,2,,n_{3}n_{4} n_{p}\) and \(^{-1}()\) is the inverse transform of \(()\). Similarly, the inverse transform \(()\) is formulated as:

\[}=()=_{3}_{3} _{4}_{4}_{p}_{p}, \]

and its inverse transform is derived as:

\[=^{-1}(})=}_{3} _{3}^{-1}_{4}_{4}^{-1}_{p}_{p}^{ -1}. \]

#### Derivation.

please refer to supplementary material. 

According to Eqs. (29), (30) and (31), we adopt its idea and design arbitrary invertible relation matrix \(_{3}^{(b+b)(b+b)}\) and \(_{4}^{L L}\) to capture the cross-modality and cross-layer information in \(\). Then the updated tensor \(_{w}\) is formulated as:

\[_{w}=_{3}_{3}_{4}_{4} ^{q q(b+b) L}, \]

where the relation matrix \(_{3}\) and \(_{4}\) are learnable. To better capture the nonlinear interactions inside the whole parameter space, we further adopt \(k\) layers deep neural network (DNN) \(f_{3}()\) and \(f_{4}()\) to replace the transform \(_{3}_{3}\) and \(_{4}_{4}\), respectively, and the DNN \(f_{3}()\) is formulated as:

\[f_{3}()=((((_{3} _{1})_{3}_{2}))_{k-1}) _{k}, \]

where \(()\) is a nonlinear scalar function and matrices \(\{_{j}^{(b+b)}\}_{j=1}^{k}\). The DNN \(f_{4}()\) is similar. Finally, the \(\) is updated by \(=+_{w}\), where \(^{(b+b) L}\) is a learnable parameter.

## 5 Experiments

### Experimental Setup

**Datasets.** Following previous studies [7; 48], we utilizes the COCO-Stuff dataset  as our training set. This dataset comprises approximately 118,000 densely annotated images across 171 distinct semantic categories. During inference, we carry out comparisons with state-of-the-art methods across several semantic segmentation datasets, including ADE20K , PASCAL VOC , and PASCAL-Context . **ADE20K ** is a classical semantic segmentation dataset comprising around 20,000 training images and 2,000 validation images. Besides, it includes two different test sets: A-150 and A-847. The test set A-150 has 150 common categories, while the test set A-847 has 847 categories. **PASCAL VOC ** is a small dataset for semantic segmentation, which includes 1464 training images and 1449 validation images. The dataset contains 20 different foreground categories. We name it as PAS-20. In line with , we also report a score on PAS-20\({}^{b}\), which involves "background" as the 21st category. **PASCAL-Context ** is upgraded from the original PASCAL VOC dataset. It includes two different test sets: PC-59 and PC-459 for evaluation. The test set PC-59 has 59 categories, while the test set PC-459 has 459 categories.

   Model & VLM & Additional Backbone & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & PAS-20\({}^{b}\) \\   \\  ZS3Net  & - & ResNet-101 & - & - & - & 19.4 & 38.3 & - \\ LSeg  & CLIP ViT-B/32 & ResNet-101 & - & - & - & 47.4 & - \\ ZegFormer  & CLIP ViT-B/16 & ResNet-101 & 4.9 & 9.1 & 16.9 & 42.8 & 86.2 & 62.7 \\ ZSseg  & CLIP ViT-B/16 & ResNet-101 & 7.0 & - & 20.5 & 47.7 & 88.4 & - \\ OpenSeg  & ALIGN & ResNet-101 & 4.4 & 7.9 & 17.5 & 40.1 & - & 63.8 \\ OVSeg  & CLIP ViT-B/16 & ResNet-101c & 7.1 & 11.0 & 24.8 & 53.3 & 92.6 & - \\ ZegCLIP  & CLIP ViT-B/16 & - & - & - & - & 41.2 & 93.6 & - \\ CAT-Seg  & CLIP ViT-B/16 & - & 12.0 & 19.0 & 31.8 & 57.5 & 94.6 & 77.3 \\   \\  SAN  & CLIP ViT-B/16 & - & 10.1 & 12.6 & 27.5 & 53.8 & 94.0 & - \\ Ours & CLIP ViT-B/16 & - & **12.4** & **19.3** & **32.4** & **57.9** & **95.2** & **78.2** \\   \\  LSeg  & CLIP ViT-B/32 & ViT-1/16 & - & - & - & - & 52.3 & - \\ OpenSeg  & ALIGN & Eff-B7 & 8.1 & 11.5 & 26.4 & 44.8 & - & 70.2 \\ OVSeg  & CLIP ViT-L/14 & Swin-B & 9.0 & 12.4 & 29.6 & 55.7 & 94.5 & - \\ SAN  & CLIP ViT-L/14 & - & 12.4 & 15.7 & 32.1 & 57.7 & 94.6 & - \\ ODISE  & CLIP ViT-L/14 & Stable Diffusion & 11.1 & 14.5 & 29.9 & 57.3 & - & - \\ CAT-Seg  & CLIP ViT-L/14 & - & 16.0 & 23.8 & 37.9 & 63.3 & 97.0 & 82.5 \\   \\  SAN  & CLIP ViT-L/14 & - & 12.4 & 15.7 & 32.1 & 57.7 & 94.6 & - \\ Ours & CLIP ViT-L/14 & - & **16.5** & **24.2** & **38.4** & **64.1** & **97.7** & **83.2** \\   

Table 1: **Comparison with state-of-the-art methods on standard benchmarks.** The best-performing results are presented in bold, while the second-best results are underlined. “VLM”: visual language model.

**Evaluation metric.** Following prior works [7; 48], we adopt mean Intersection over Union (mIoU) to evaluate the semantic segmentation performance on the three benchmarks.

**Implementation Details.** We implement our method using the Transformer-based CLIP model. Following the protocol established in , we evaluate our results on two versions of the CLIP model: ViT-B/16 and ViT-L/14. For training, we use the Adam optimizer  with an initial learning rate of \(5 10^{-6}\) for CLIP, and a weight decay of \(10^{-4}\). Training is conducted with one image per mini-batch. We set \(q=128\) for balancing efficiency and performance. The function \(f_{3}()\) and \(f_{4}()\) are implemented using two 2-layer MLPs. We act the cost-based approach provided in  as our decoder. All models are trained over 80,000 iterations on 4 NVIDIA RTX 3090 GPUs.

### Main Results

**Comparing to SOTAs.** Here, we compare our proposed H-CLIP with several state-of-the-art methods, as shown in Table 1, using six test sets across three benchmarks. Overall, we achieve the best results. Most existing open-vocabulary semantic segmentation methods employ traditional fine-tuning approaches, i.e., full or partial fine-tuning (tuning certain layers of CLIP). While these methods offer sufficient flexibility for learning new knowledge, they often result in a significant performance drop on unseen classes, as observed with OVSeg . Among these methods, CAT-Seg  achieves performance comparable to ours. However, its fine-tuning scheme is manually controlled through different layer combinations, necessitating a careful design to balance generalization and flexibility, while ours does not suffer from such an issue. Then, compared to SAN , another parameter-efficient fine-tuning method that introduces only a limited number of tunable parameters, our approach significantly outperforms it, achieving improvements of 6.6% on the PC-459 dataset and 3.9% on the PC-59 dataset with ViT-B/16 as the base model. These results demonstrate the effectiveness of our method in preserving generalization while learning segmentation knowledge.

**Qualitative results.** Here, we visualize our method's representative example segmentation results against prevailing methods, e.g., CAT-Seg  in the PC-459 dataset. As shown in Figs. 2 - 4, we observe that our approach is able to generalize on diverse scenarios and produce more accurate results.

   Methods & OVSeg  & CAT-Seg  & SAN  & Ours \\  Param. (M) & 147.2 & 25.6 & 8.4 & **5.6** \\   

Table 2: **Efficiency comparison** in terms of learnable parameters.

Figure 2: Comparison of qualitative results on ADE20K  with 150 categories. we compare Our method with CAT-Seg .

**Efficiency comparison.** We compare the efficiency of our method with other approaches, including OVSeg , CAT-Seg , and SAN , all of which utilize CLIP ViT models. The comparison,

Figure 4: Comparison of qualitative results on ADE20K  with 847 categories.

Figure 3: Comparison of qualitative results on VOC2010  with 59 categories.

   Method & POF & DCRC & Param. (M) & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & PAS-20\({}^{b}\) \\   Freeze & ✗ & ✗ & 0 & 4.4 & 6.6 & 24.8 & 49.4 & 92.5 & 71.9 \\ LoRA  & ✗ & ✗ & 7.5 & 11.4 & 17.6 & 28.6 & 55.1 & 94.2 & 76.7 \\   & ✓ & ✗ & 5.62 & 12.3 & 19.0 & 31.6 & 56.4 & 94.6 & 76.3 \\  & ✗ & ✓ & 0.01 & 7.6 & 10.9 & 26.8 & 53.6 & 92.7 & 74.5 \\   & ✓ & ✓ & 5.63 & **12.4** & **19.3** & **32.4** & **57.9** & **95.2** & **78.2** \\   

Table 3: **Ablation study on the components of H-CLIP. “LoRA”: a mainstream parameter-efficient tuning method with a comparable number of parameters for comparison. “POF”: Partial Orthogonal Fine-tuning. “DCRC”: Dual Cross Relation Communication. The base model is ViT-B/16.**summarized in Table 2, shows that our method employs the fewest trainable parameters while balancing the generalization of the pre-trained model and the flexibility for learning new knowledge. Additionally, since we introduce a lightweight architecture for calculating relations, specifically two relation matrices, the inference overhead is negligible during the inference phase.

### Ablative Studies

**Ablation of Main Components.** Here, we conduct an ablation study to demonstrate the benefits of each component of our proposed H-CLIP: partial orthogonal fine-tuning (POF) and dual cross-relation communication (DCRC). We use the ViT-B/16  version of CLIP as the baseline, shown in row 1 of Table 3. Additionally, we implement a mainstream parameter-efficient fine-tuning (PEFT) method, LoRA , for comparison with a similar number of learnable parameters, as shown in row 2. Note that LoRA can improve performance compared to the baseline, demonstrating that PEFT is a viable approach for this task. Then, comparing row 5 to row 2, we observe significant performance gains, indicating that our results are driven by our targeted solution rather than merely the number of parameters. Moreover, row 3 shows that using only POF preserves generalization on unseen classes, particularly in the A-847 dataset. Meanwhile, solely adapting DCRC shows limited improvement, as it only enhances communication among frozen weight matrices. Finally, integrating DCRC with POF yields clear performance gains, e.g., a 12.6% improvement on the PC-459 dataset.

**Different Design of POF.** Table 4 presents experiments introducing different designs into POF. The design of POF is related to (1) block dimension, i.e., \(q\), and (2) how orthogonality constraints are applied. In (a), the results show that larger In (a), the results show that larger \(q\) generally performs better than smaller \(q\). However, we find a good trade-off between performance and parameter efficiency, with \(q=128\) working well across datasets and tasks. Therefore, we maintain this setting in other experiments. In (b), we show that both blindly applying orthogonality constraints to the learnable matrices of all layers and not using any constraints at all can degrade performance on most test sets, demonstrating the value of our analysis with the hyperspherical energy principle.

## 6 Conclusion

In this paper, we propose a H-CLIP framework to address three issues: 1) high computational cost, 2) misalignment between the two inherent modalities of CLIP, and 3) degraded generalization ability on unseen categories when equipping CLIP with pixel-level prediction ability for open-vocabulary semantic segmentation. Specifically, we propose a symmetrical parameter-efficient fine-tuning (PEFT) strategy conducted in hyperspherical space for both of the two CLIP modalities. Specifically, the PEFT strategy is achieved by a series of efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module among all learnable matrices to mitigate misalignment between different modalities. Furthermore, we apply an additional constraint to PEFT on the CLIP text encoder according to the hyperspherical energy principle, i.e., minimizing hyperspherical energy during fine-tuning preserves the intrinsic structure of the original parameter space, to prevent the destruction of the generalization ability offered by the CLIP text encoder. Extensive experiments demonstrate that the proposed H-CLIP framework generalized improves segmentation performance across several benchmarks while introducing approximately 4% of CLIP's total parameters. We hope our approach will provide a new direction and inspire future research in this field.