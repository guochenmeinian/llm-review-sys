# Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems

Rui Wang

Massachusetts Institute of Technology

rayruw@mit.edu

&Robin Walters

Northeastern University

r.walters@northeastern.edu

&Tess E.Smidt

Massachusetts Institute of Technology

tsmidt@mit.edu

###### Abstract

Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.

## 1 Introduction

Symmetry and equivariance play pivotal roles in the advancement of deep learning . Specifically, equivariant convolution  and graph neural networks , which integrate symmetries into the design of the architectures, have demonstrated notable success in modeling complex data. By constructing a model inherently equivariant to the transformations of relevant symmetry groups, we ensure automatic generalization across these transformations. This enhances not only the model's robustness to distributional shifts but also its sample efficiency . Furthermore, Noether's theorem establishes a relationship between conserved quantities and symmetry groups . Consequently, neural networks that preserve these symmetries are poised to generate physically more accurate predictions .

A limitation of many existing equivariant models is they assume that data has perfect symmetry matching the models' equivariance. This means they are learning functions that are strictly invariant or equivariant under given group actions. However, this can be overly restrictive, especially when the data does not possess perfect symmetry or exhibits less symmetry than what the model accounts for. For instance,  indicates that imposing incorrect or irrelevant symmetries may hurt performance.  found that the equivariant models fail to learn when the output has lower symmetry than the input and the models themselves.  empirically shows that approximately equivariant models outperform models with no symmetry bias and those with strict symmetry in learning fluid dynamics.

Consequently, several recent works have focused on relaxing strict equivariance constraints and designing approximately equivariant networks. For instance,  showed that relaxing strict spatial weight sharing in conventional 2D convolution can improve image classification accuracy.  suggested substituting equivariant layers with a combination of those layers and non-equivariant

[MISSING_PAGE_FAIL:2]

[MISSING_PAGE_FAIL:3]

**Proposition 3.1**.: _Consider a relaxed group convolution neural network where the relaxed weights in each layer are initialized to be identical to maintain \(G\)-equivariance. If it is trained to map an input \(X\) to the output \(Y\), its relaxed weights will learn to be distinct across group elements in \(G\) during training in a way such that it is equivariant to \(G(X)(Y)\), which is the intersection of the stabilizers of the input and the output and \(G\)._

Proof can be found in the Appendix A.

### Finding Symmetry Breaking Factors in a Simple 2D Example

This useful property allows us to discover symmetry or identify the symmetry-breaking factors in the data because the relaxed weights can tell us whether a transformation \(h\) stabilizes the output \(Y\). We provide a clear illustration of this through a simple 2D example.

We trained a 3-layer \(C_{4}\) relaxed group convolution network with \(L=1\) on the following three tasks: 1) map a square to a square; 2) deform a square into a rectangle; 3) map a square to a non-symmetric object. Both the input and output are single-channel images. As shown in Figure 1, in the first task where the output is a square with \(C_{4}\) symmetry, relaxed weights across all layers remain equal throughout training. For the second task, where the output is a rectangle exhibiting \(C_{2}\) symmetry, the relaxed weights learn to be different. However, the weights corresponding to the group elements \(i\) and \(g^{2}\) are the same, as do the weights for \(g\) and \(g^{3}\). That implies that the output is invariant to 180-degree rotations and the model becomes equivariant to \(C_{2}\). In the final task, where the output lacks any meaningful symmetries, the relaxed weights diverge entirely for the four group elements, thereby breaking the model's equivariance.

When dealing with larger groups, decomposing relaxed weights--which can be interpreted as signals on the group--into irreps can simplify the task of pinpointing broken symmetries. By projecting the relaxed weights onto the irreducible representations (irreps) of the group, similar to calculating its Fourier components, one can assess which symmetries are preserved or broken. More specifically, if the signal were perfectly symmetric under the group operations, you would expect non-zero contributions only from the trivial representation and other irreps that align with the signal's symmetries. Any significant contributions from other irreps suggest that those particular symmetries are broken. Take, for example, the relaxed weights from a neural network layer illustrated in Figure 1, projected onto \(C_{4}\)'s four one-dimensional irreps. For the first task, only the Fourier component for the trivial representation is non-zero. In the second task, Fourier components for both the trivial and the sign representation are non-zero, suggesting that either 90 or 180-degree rotational symmetries are broken. Since the remaining irreps are zero, we can conclude the output is still invariant under 180-degree rotations. In the third task, all Fourier components are non-zero. This is related to the approach that uses trainable irreps to discover symmetry-breaking parameters in this paper .

In a word, the relaxed group convolution has the potential to discover the symmetry in the data, while also reliably preserving the highest level of equivariance that is consistent with data.

Figure 1: Visualization of tasks and corresponding relaxed weights after training. A 3-layer \(C_{4}\)-relaxed group convolution network with \(L=1\) is trained to perform the following three tasks: 1) map a square to a square; 2) deform a square into a rectangle; 3) map a square to a non-symmetric object.

Related Work

### Equivariance and Invariance

Symmetry has been subtly integrated into deep learning to build networks with specific invariances or equivariances. Convolutional neural networks revolutionized computer vision by using translation equivariance[60; 61], while graph neural networks exploit permutation symmetries [41; 26]. Equivariant deep learning models have excelled in image analysis[8; 5; 6; 52; 28; 2; 56; 7; 13; 53; 9; 20], and their application is now expanding to physical systems due to the deep relationship between physical symmetries and the principles of physics. For instance,  designed fully equivariant convolutional models with respect to symmetries of scaling, rotation, and uniform motion, particularly in fluid dynamics scenarios.  introduced Steerable Conditional Neural Processes to learn the complex stochastic processes in physics while ensuring that these models respect both invariances and equivariances.  equivariant Fourier neural operator to solve partial differential equations by leveraging the equivariance property of the Fourier transformation. Additionally, the domain of graph neural networks has seen a surge in the development of equivariant architectures, especially for tasks related to atomic systems and molecular dynamics. This growth is attributed to the inherent presence of symmetries in molecular physics, such as roto-translation equivariance in the conformation and coordinates of molecules. [1; 39; 45; 32; 18; 62; 42; 36]. For instance, [30; 31] proposed Equiformers for modeling 3D atomistic graphs. They are graph neural networks leveraging the strength of Transformer architectures and incorporating equivariant features with E3NN .

### Approximate Symmetry and Symmetry Breaking

Many real-world data rarely conform to strict mathematical symmetries, due to noise and missing values or symmetry-breaking features in the underlying physical system. Thus, there have been a few works trying to relax the strict symmetry constraints imposed on the equivariant networks.  first showed that relaxing strict spatial weight sharing in conventional 2D convolution can improve image classification accuracy.  generalized this idea to arbitrary groups and proposed relaxed group convolution, which is biased towards preserving symmetry but is not strictly constrained to do so. The key idea is relaxing the weight-sharing schemes by introducing additional trainable weights that can vary across group elements to break the strict equivariance constraints.  further provides a theoretical study of how the data equivariance error and the model equivariance error affect the models' generalization abilities. In this paper, we further extend relaxed group convolution to three-dimensional cases and reveal its potential in symmetry discovery problems. Additionally,  proposed a mechanism that sums equivariant and non-equivariant MLP layers for modeling soft equivariances, but it cannot handle large data like images or high-dimensional physical dynamics due to the number of weights in the fully-connected layers.  formalizes active and approximate symmetries in graph neural nets that operate on a fixed graph domain, highlighting a tradeoff between expressiveness and regularity when incorporating these symmetries.

### Super-resolution for fluid flows

Refining low-resolution images using super-resolution techniques is essential for various applications in computer vision [35; 46; 34; 38]. Due to the enormous computational cost of generating high-fidelity simulations, a variety of deep learning models based on MLPs , CNNs[15; 17; 43], and GANs [54; 57] have been proposed for the super-resolution fluid dynamics . The closest work to ours is  which applies 2D rotationally equivariant CNNs to upscale the velocity fields of fluid dynamics. However, they do not consider the scenarios of approximate symmetry and three-dimensional fluid dynamics as we do.

## 5 Experiments

### Discover Symmetry Breaking Factors in Phase Transitions

Phase TransitionModeling a phase transition from a high symmetry (like octahedral) to a lower symmetry is a common topic of interest in the fields of materials science. This change can be understood as a transformation in the arrangement or orientation of atoms within a crystal lattice. For instance, perovskite structures are a class of materials with the general formula ABO3. The A and B are cations of different sizes, and O is the oxygen anion. The B cation is typically a transition metal. Under certain conditions like a decrease in temperature, these BO6 octahedra may undergo distortion.

To illustrate, consider the case of Barium titanate (\(BaTiO_{3}\)), as shown in Figure 2. At high temperatures, \(BaTiO_{3}\) has a cubic perovskite structure with the Ti ion at the center of the octahedron. As one progressively cools \(BaTiO_{3}\), it undergoes a series of symmetry-breaking phase transitions. Initially, around 120\({}^{}\)C, there is a shift from the cubic phase to a tetragonal phase, where the Ti ion is displaced from its central position. In a tetragonal system, two of the axes are of equal length and the unit cell is in the shape of a rectangular prism where the base is a square. This is followed by a transition to orthorhombic at about -90\({}^{}\)C, where all three axes are of different lengths.

Experimental SetupWe download the fractional coordinates of BaTiO3 in cubic, tetragonal, and orthorhombic phases from the Material Project1. We use 3D tensors to represent these systems where the pixels corresponding to atoms are non-zero. We train a 3-layer relaxed group convolution network to 1) map the cubic system to the tetragonal system; and 2) map the cubic system to the orthorhombic system until overfitting.

Find Symmetry Breaking Factors Using Relaxed WeightsFigure 2 visualizes the relaxed weights from the two 3-layer models trained to predict the tetragonal and orthorhombic structures from a cubic system. We only show the relaxed weights corresponding to the rotations along the \(x\), \(y\), and \(z\) axes and reflections over \(XY\), \(YZ\), and \(XZ\) planes as they are more straightforward to understand. As shown in Figure 2, when the model is trained to predict the tetragonal system, the post-training relaxed weights successfully find that the four-fold rotation symmetries along \(y\) and \(z\) axes are broken. When the model is trained to predict the orthorhombic system, only two-fold rotation symmetry along the \(y\) axis and reflection symmetries over \(XY\) and \(YZ\) planes are left, because the relaxed weight of \(R_{y}^{90}\) and \(R_{y}^{270}\) are the same and refl\({}_{XY}\) and refl\({}_{YZ}\) are the same as the identity. This aligns with the space group \(Amm2\) of orthorhombic crystal system . Such results highlights the capability of relaxed group convolution to automatically discover symmetries and symmetry-breaking factors.

### Super-resolution of Velocity Fields in Three-dimensional Fluid Dynamics

Data Description.We use the direct numerical simulation data of the channel flow (\(2048 512 1536\)) turbulence and the forced isotropic turbulence (\(1024^{3}\)) from Johns Hopkins Turbulence Database . For each dataset, we acquire 50 frames of velocity fields, which are then downscaled by half and segmented into \(64^{3}\) cubes for experimental use. These cubes are further downsampled by a factor of 4 to serve as input for our superresolution model. The models are trained to generate \(64^{3}\) simulations from \(16^{3}\) downsampled versions of them. Because of the spatial weight sharing of CNNs, we can apply our model to 3D input with any resolutions during inference.

Experimental SetupFigure 3 visualizes the model architecture we use for super-resolution. We evaluate the performance of Regular, Group Equivariant, and Relaxed Group Equivariant layers built

Figure 2: Left: Visualization of \(BaTiO_{3}\): As temperature decreases, it undergoes a series of symmetry-breaking phase transitions, transitioning from a cubic structure to a tetragonal phase, and eventually to an orthorhombic form. Right: Visualization of relaxed weights of 16 of a total 48 elements from the two 3-layer models trained to predict the tetragonal and orthorhombic structures from a cubic system, including rotations along \(x\),\(y\),\(z\) axes and reflections over \(XY\), \(YZ\), \(XZ\) planes.

into this architecture in the tasks of upscaling channel flow and isotropic turbulence. The models take three consecutive steps of downsampled \(16^{3}\) velocity fields as input and predict a single step of \(64^{3}\) simulation, enabling them to infer vital attributes like acceleration and external forces for precise small-scale turbulence predictions. We use the L1 loss function over the L2 loss, as it significantly enhances performance. We split the data 80%-10%-10% for training-validation-test across time and report mean absolute errors over three random runs. As for hyperparameter tuning, except for fixing the number of layers and kernel sizes, we perform a grid search for the learning rate, hidden dimensions, batch size, and the number of filter bases for all three types of models.

Prediction PerformanceTable 1 shows the prediction MAE of trilinear upsampling, non-equivariant, equivariant, equivariant, and relaxed equivariant models applied to super-resolution tasks for both channel and isotropic flows. Figure 4 shows the 2D velocity norm field of predictions. As we can see, imposing equivariance and relaxed equivariance consistently yield better prediction performance.

Figure 5 visualizes of relaxed weights of the first two layers from the models trained on isotropic flow and channel flow. relaxed weights for isotropic flow stay almost the same across group elements while those for channel flow vary a lot. This suggests that the relaxed group convolution can discover symmetry in the data even if the symmetry lies in the sample space, rather than individual samples.

    & \))} & \))} \\  Model & Trilinear & Conv & Equiv & R-Equiv & Trilinear & Conv & Equiv & R-Equiv \\  MAE & 5.241 & 2.602 & 2.540 & \(\) & 5.248 & 1.215 & 1.119 & \(\) \\   

Table 1: Prediction MAE of trilinear upsampling, non-equivariant, equivariant, relaxed equivariant models on the super-resolution of channel flow and isotropic flow.

Figure 4: Prediction visualization of a cross-section along the z-axis of the velocity norm fields.

Figure 3: The architecture of the super-resolution model includes an input layer, an output layer, and eight residual blocks. Since it has two layers of Transoposed Convolution(UpConv3d), the model produces simulations that are upscaled by a factor of four.

For isotropic turbulence, even if individual samples might not seem symmetrical, the statistical properties of their velocity fields over time and space are invariant with respect to rotations. This makes models trained on isotropic flow benefit more from the equivariance, as shown in the table. The channel flow, on the other hand, is driven by a pressure difference between the two ends of the channel together with the walls, which makes the turbulence inherently anisotropic. In such cases, the relaxed group convolution is preferable, as it adeptly balances between upholding certain symmetry principles and adapting to factors that introduce asymmetry.

## 6 Discussion

We propose 3D Relaxed Octahedral Group Convolution Networks that avoid stringent symmetry constraints to better fit real-world three-dimensional physical systems. We demonstrate its ability to consistently capture the highest level of equivariance that is consistent with data and highlight that the relaxed weights can discover the symmetry and symmetry-breaking factors. Future works include the theoretical study of the relaxed group convolution discovering symmetries in the sample space instead of individual samples. Additionally, we aim to uncover other potential applications of our model in phase transition analysis and material discovery.

Figure 5: Visualization of the relaxed weights of first two layers from the models trained on the isotropic flow and channel flow.