# Great Minds Think Alike: The Universal Convergence Trend of Input Salience

Yipei Wang, Jeffrey Mark Siskind, Xiaoqian Wang

Elmore Family School of Electrical and Computer Engineering

Purdue University

West Lafayette, IN 47907

wang4865,qobi,joywang@purdue.edu

###### Abstract

Uncertainty is introduced in optimized DNNs through stochastic algorithms, forming specific distributions. Training models can be seen as random sampling from this distribution of optimized models. In this work, we study the distribution of optimized DNNs as a family of functions by leveraging a pointwise approach. We focus on the input saliency maps, as the input gradient field is decisive to the models' mathematical essence. Our investigation of saliency maps reveals a counter-intuitive trend: two stochastically optimized models tend to resemble each other more as either of their capacities increases. Therefore, we hypothesize several properties of these distributions, suggesting that (1) Within the same model architecture (e.g., CNNs, ResNets), different family variants (e.g., varying capacities) tend to align in terms of their population mean directions of the input salience. And (2) the distributions of optimized models follow a convergence trend to their shared population mean as the capacity increases. Furthermore, we also propose semi-parametric distributions based on the Saw distribution to model the convergence trend, satisfying all the counter-intuitive observations. Our experiments shed light on the significant implications of our hypotheses in various application domains, including black-box attacks, deep ensembles, etc. These findings not only enhance our understanding of DNN behaviors but also offer valuable insights for their practical application in diverse areas of deep learning.

## 1 Introduction

The advancement in computational power has significantly enhanced the capabilities of Deep Neural Networks (DNNs), leading to their unparalleled expressiveness and success in a multitude of applications across various fields (Krizhevsky et al., 2012; He et al., 2016; Rajkomar et al., 2018; Berner et al., 2019; Rombach et al., 2022; Padmaja et al., 2023; Thirunavukarasu et al., 2023). Despite these achievements, DNNs remain enigmatic, not only to end-users but also to researchers and practitioners (Ribeiro et al., 2016; Rudin, 2018; Preece et al., 2018). Due to the over-parameterization nature of modern DNNs, they are capable of reaching zero loss in the training distribution (Goodfellow et al., 2014; Allen-Zhu et al., 2019; Du et al., 2019). Furthermore, the inherent stochastic nature of training algorithms means that even when using the same training data, DNNs tend to converge to various minima (Huang et al., 2017; Liu et al., 2020). Thus even though these models may exhibit comparable performance in terms of metrics like testing loss or accuracy, their underlying mechanisms can still differ significantly. Because of the stochastic nature of the training procedure, optimized DNNs collectively form a distribution over the functional space \(^{1}()\), and training DNNs from scratch is thereby equivalent to randomly sampling from such a distribution without any guarantee. This inherent opacity, combined with the high dimensionality and nonlinearity, limits our understanding of the internal mechanisms of DNNs.

In response to these challenges, we study the aforementioned distributions. By adopting a pointwise approach, our focus is on the distribution of input salience (Simonyan et al., 2013) from the context of eXplainable Artificial Intelligence (XAI), which aims to demystify the inner workings of these complex models (Gunning and Aha, 2019; Arrieta et al., 2020; Van der Velden et al., 2022). Saliency maps, particularly in the form of _input gradients_, represent the data points within the gradient fields of DNNs. Thus the study of gradients can offer a deterministic view of the landscape of model predictions. This approach allows us to examine the intricate nuances of DNNs in a more structured and analytical manner.

For clarity, in the following context, we distinguish between the term **model architecture** (e.g. skip/direct connections) from the term **model family**. The latter refers to a specific collection of models \(\), that differ only in capacity as determined by width and depth. Two models are said to be in the same family if they differ only in parameter values. Given an input, varying model families result in distinct distributions. A synthetic visualization of such distributions is shown in Figure 1(a). Different models are depicted by the points. However, the relationship between different model families, represented by various colors, remains elusive. In this work, we introduce and verify several hypotheses to uncover a striking pattern. (1) Within the same model architecture (e.g., CNNs, ResNets), different family variants (e.g., varying capacities) tend to align in terms of their population mean direction. (2) As the model capacities increase, the variance within the distribution of the same family diminishes. This leads to a converging trend of the distributions. Both hypotheses are illustrated in Figure 1(b). Additionally, we introduce a semi-parametric approach to model these distributions, providing detailed quantification of the convergence.

The similarities observed in input salience have direct implications for understanding the important vulnerability of DNNs regarding gradient attacks (Szegedy et al., 2013; Goodfellow et al., 2014). In particular, in black-box attack settings, the gradients of the target model are not directly accessible. A higher degree of salience similarity naturally enhances transferability (Chen et al., 2023). Our findings elucidate why models with larger capacity consistently exhibit superiority in terms of adversarial robustness compared to smaller models (Madry et al., 2017; Gustafsson et al., 2020; Li et al., 2020; Bubeck and Sellke, 2021). Moreover, given that the mean direction is aligned across different models, it is possible to approximate this mean direction by randomly sampling from a set of independently optimized models. We demonstrate that these estimated mean directions can attain a near-perfect cosine similarity of almost 1.0, even between completely independent models or ensembles, in a high-dimensional space. Moreover, note that deep ensembles essentially calculate this population mean direction (Lee et al., 2015; Lakshminarayanan et al., 2017; Fort et al., 2019; Kondratyuk et al., 2020), where the mean of a group of independently trained models can improve the performance. As a consequence, the insights of our hypotheses also shed light on this phenomenon which, although empirically successful, has been somewhat enigmatic in terms of their source of capability (Lobacheva et al., 2020; Deng and Shi, 2021; Abe et al., 2022; Theisen et al., 2023). Furthermore, since deep ensembles approximate the aligned mean directions much faster than scaling up single models, this

Figure 1: A synthetic illustration of the distribution of the directional gradients of stochastically optimized models of the same input data. The subfigures demonstrate (a) an intuitive, stochastic scenario, where the distributions of different model families are not closely dependent. and (b) the converging distribution trend introduced by our hypothesis. Different colors represent different model families, and points represent different optimized models.

also demystifies the significant black-box attack transferability of deep ensembles (Yang et al., 2021; Chen et al., 2023). Our research thus not only advances the understanding of model behavior in practical applications but also contributes to the broader field of AI trustworthiness and efficiency. Our main contributions can be summarized as follows:

* We reveal an appealing phenomenon where the mean distribution directions of input salience across different model families have extremely high resemblance.
* We empirically demonstrate the distribution converges towards the mean direction as model capacity increases.
* Incorporating both empirical observations and theoretical analysis, we hypothesize distributional properties of optimized models, quantifying the aforementioned phenomena.
* The hypotheses effectively explain many hitherto unclear phenomena such as black-box attack transferability, the efficacy of deep ensemble methods, etc.

## 2 The Convergence of Input Saliency

### Salience Similarities

Notation.Let \(=\) denote the dataset, where \(^{d}\) is the input set and \(=[c]\) is the set of labels and \(c_{+}\) is the number of classes. Following the benign overfitting phenomenon (Bartlett et al., 2020; Papyan et al., 2020; Cao et al., 2022), we let \(=\{f|(f;_{},_{})<10^{-3}\}\) denote a family of optimized models, distinguished by different architectures, such as vanilla sequential CNNs, skipping blocks, etc. \(\) denotes the expected cross-entropy loss for the training distribution. For simplicity, we focus on \(f:^{d}\), which predicts the logit specifically for the _targeted class_. This is to stay consistent with XAI methods. We demonstrate in Appendix A that the difference between logit and probability (Wang and Wang, 2022) does not affect the observed phenomena. Unless otherwise indicated, experiments are carried out over the test set \(=_{test},=_{test}\). Within the same architecture, model capacity is determined by both the width and the depth. Since it is more difficult to model depth as a single variable, we model varying depth as different families \(\) but model varying width \(k\) as a parameter of the family, \((k)\).

Figure 3: The expected similarity \((k_{1},k_{2})\) between model families of varying capacities \(k_{1},k_{2}\). Here the datasets are CIFAR-10/100, and the models are CNN and ResNets.

The Increasing Similarity.Let \(:^{d}^{d}[-1,1]\) denote the cosine similarity metric, then the individual similarity between the input salience of two given models \(f^{(1)}(k_{1}),f^{(2)}(k_{2})\) given input \(\) is \(_{ind}(f^{(1)},f^{(2)};)=(_{}f^{(1)}(),_{}f^{(2)}())\). Taking the entire testing set into consideration, denote \(_{ind}(f^{(1)},f^{(2)})=_{}[_{ind}(f^{(1 )},f^{(2)};)]\). In Figure 2, the expectations over the testing set \(_{}(_{}f^{(1)}(),_{}f^{(2)}())\) with varying \(k_{1},k_{2} K\) are illustrated. Here, we define \(K=\{j2^{i}:4 j 7,1 i 6\}=\{8,10,12,14,16,20,,384,448\}\) to balance between finer linear scaling and coarser exponential scaling. It can be observed that the similarity between two stochastically optimized models \(f^{(1)},f^{(2)}\) has an increasing trend with respect to both \(k_{1},k_{2}\). Two different architectures CNN and ResNet are included. To rule out the influence of any single model, we define the similarity between families \((k_{1}),(k_{2})\) for a given input \(\) by taking the expectation of the two models:

\[(k_{1},k_{2};):= _{f^{(1)}(k_{1}),f^{(2)}(k_ {2})}(_{}f^{(1)}(),_{}f^{(2)}( {x})) \]

The global similarity between models of widths \(k_{1},k_{2}\) is then denoted by \((k_{1},k_{2})=_{}(k_{1},k_{2};)\). Note that estimating this value requires training numerous \(f(k)\) for each \(k K\). Therefore, we carry out the experiments over \(K^{}=\{j2^{i}:j=5i=1,2,3,4,5\}=\{10,20,40,80,160\} K\). For each \(k_{1},k_{2} K^{}\), 100 models are sampled respectively to empirically estimate the expectation over \((k_{1}),(k_{2})\). As observed in Figure 3, \((k_{1},k_{2})\) has an increasing trend w.r.t. both \(k_{1},k_{2}\). Compared with Figure 3, the average similarity over the model families is similar to the individual cosine similarity for the same \(k\) values. As a result, studying the similarity of two randomly sampled models instead of the expectation over \(\)s can significantly alleviate the computational burden. This is further discussed in detail in Section 3.2. Besides, It trivially follows that \( k_{1}>k_{2},(k_{1},k_{1})>(k_{1},k_{2})>(k_{2},k_{2})\), which suggests that _larger models tend to resemble smaller models even more than smaller models themselves._ - Even if the two smaller models only differ in the random seeds during training. Please refer to Appendix A for the results of more datasets, where such increasing trends still exist.

### The Spherical Distribution of the Salience

Since the cosine similarity can be written as the inner product between \(}f^{(1)}()}{\|_{}f^{(1)}()\|}\) and \(}f^{(2)}()}{\|_{}f^{(2)}()\|}\), which are high-dimensional unit vectors, we explore the properties and potential distributions of \(\) through the perspective of spherical statistics. Given an input \(\), we denote by \(_{k}()\) the set of all possible gradient directions of input \(\) regarding the models \(f\) in \((k)\). Formally, let

\[_{k}()= =_{}f()\|_{}f ()\| f(k)},  \]

Then the similarity is re-written as the inner product \((k_{1},k_{2};)=_{_{1}_{k_{1}}(), _{2}_{k_{2}}()}[_{1}^{T}_{2}]\).

The Intra-Family vs. Cross-Family Paradox.An interesting paradox is raised as \((,)\) increases with both inputs. Naturally, one would reasonably deduce that two models \(f^{(1)},f^{(2)}(k_{1})\) should resemble each other since they are from the same family (i.e. having the exact same structure and only differ in training seeds). However, since \((k_{2},k_{1})>(k_{1},k_{1})\), the cross-model family similarity becomes greater than the intra-model family similarity. To uncover the mystery of the observations, we present the intuitive understanding and the rigorously analyzed hypotheses as follows.

The Intra-Family Hypothesis.Note that for intra-family scenario, \(,_{k}()\) are i.i.d., the similarity can be written as \((k,k;)=(_{_{k}()}[])^{T}( _{_{k}()}[])=\|_{_{k}()}[ {u}]\|_{2}^{2}\), which denotes the square of the _population mean resultant length_(Mardia et al., 2000) of \(_{k}()\). The population mean resultant length \()}\) quantifies the degree of dispersion of \(_{k}()\), where a larger length suggests a more concentrated distribution. In directional statistics, the degree of dispersion is usually quantified by the spherical variance \(2(1-)})\) or the total variation \(1-(k,k;)\). Since \((k,k;)\) also increases w.r.t. \(k\), this suggests an increasing concentration of input salience of models as the width \(k\) of the model increases. In conclusion, _the larger the models are, the smaller the spherical variance of the salience is._ Formally, we propose the following hypothesis.

* **Hypothesis I (H1)**: Let \(k\) denote the width (capacity) of the model and \(_{k}()==}f()}{\| _{}f()\|} f(k)}\) denote the set of input gradient directions regarding \(\). Then\[_{_{k}()}[]=)}(k;)\] and \[(k,k;)\] _increases with_ \[k\]. Here \[(k,)=_{_{k}()}[]}{\| _{_{k}()}()\|}\] denotes the unit mean direction of \[_{k}()\]._

Note that _H1 also holds for the change of model depths_, which is positively related to the dispersion degree of the distribution. However, the change in model depth inevitably affects model width. Thus, we only provide empirical verification in Section 3 but do not include it as a part of H2.

The Cross-Family Hypothesis.Unlike the intra-family similarity, the increasing cross-family similarity is where the phenomenon becomes counter-intuitive. Then due to the increasing intra-family similarities, when \(k_{1},k_{2}\) increase, \(_{1}_{k_{1}}()\) becomes closer to \((k_{1};)\), \(_{2}_{k_{2}}()\) becomes closer to \((k_{2};)\). However, the cross-family similarities suggest that as \(_{1},_{2}\) approach their respective mean directions, their similarity increases as well. This indicates that the mean directions \((k_{1};),(k_{2};)\) are similar, too. Formally, this intuition is considered as follows. For \(k_{1}>k_{2}\), when \((k_{1};)\) and \((k_{2};)\) are sufficiently similar, \((k_{1};)^{T}(k_{2};)\|(k_{1}; )\|\|(k_{1};)\|\). Thus we have

\[(k_{1},k_{2};)= _{_{1}_{k_{1}}(),_{2} _{k_{2}}()}[_{1}^{T}_{2}]=_{_{ 1}_{k_{1}}()}[_{1}]^{T}_{_{2} _{k_{1}}()}[_{2}] \] \[ \|_{_{1}_{k_{1}}()}[_{1 }]\|\|_{_{2}_{k_{1}}()}[_{2}]\|= {(k_{1},k_{1};)(k_{2},k_{2};)},\]

which is monotonic w.r.t. both \(k_{1},k_{2}\). Formally, this is summarized as

* **Hypothesis II (H2)**: Let \(_{k_{1}}(),_{k_{2}}()\) denote the input gradient directions of two model families where \(k_{1} k_{2}\). Then \((k_{1};)(k_{2};)\) regardless of \(k_{1},k_{2}\).

The two hypotheses are both empirically verified. For a smoother flow of the presentation, we defer the detailed experiments to Section 3. The basic ideas of H1 and H2 are illustrated in Figure 4 (a).

### The Directional Distribution of Gradients

Given the analysis and hypothesis above, one can have an overview of the models' internal mechanisms. As the model capacity increases, models are distributed in a more concentrated manner, while the mean direction stays almost invariant. To better understand the models' behavior with the stochasticity, we delve into the distribution of \(_{k}()\) and present a semi-parametric analysis with experimental verification. A general form of centralized symmetric distribution over hypersphere is known as the Saw distribution (Fisher et al., 1993)\(p(;)=^{T})}{Z}\), where \(\) is the mean direction with \(\|\|=1\), \(([-1,1])\), and \(Z=_{S^{d-1}}(^{T})}\) is the normalization term for distributions. Due to the symmetry assumption, the shape of the distribution is solely determined by \(\). For example, a monotonically increasing \(\) suggests that \(\) is distributed more densely near the mean direction and sparsely distant from the mean direction. Considering the concentration trend of gradients, we hypothesize that \(_{k}()\) of \(_{k}()\) not only monotonically increases with the input, but also increases faster with greater \(k\) values.

Marginalization.For \(_{k}()\), it can be decomposed to \(=t()+}()^{}\), where \(()^{}\) is a unit tangent to \(S^{d-1}\) at \(\). Then \(t=^{T}()\). This is shown in Figure 4 (b). Note that \(()^{}\) is independent from \(t\), then the distribution of \(t\) is the marginal distribution over the intersection between \(S^{d-1}\) and the hyperplane spanned by \(()^{}\), which is a \((d-2)\)-dimensional hypersphere. According to the symmetry assumption of Saw distribution, conditioned on a fixed similarity \(t\), the distribution of \(}|t\) over the dashed \(S^{d-2}\) does not affect \(\). Therefore, we focus on the marginalized distribution of \(t\). Note that the radius of the intersection \(S^{d-2}\) is \(}\), we thus have \(=(1-t^{2})^{(d-3)/2}}{((d-1)/2)} t\), where the density of \(t\) is observed by the integral over the corresponding \((d-2)\)-hypersphere. As a result, the marginal distribution of \(t\) has the PDF

\[p_{k}(t;)=_{k}(t;)(1-t^{2})^{(d-3)/2}/Z^{} \]

where \(Z^{}\) is a constant normalization term. Note that \((1-t^{2})^{(d-3)/2}\) is a symmetric bell curve centered at \(t=0\). Equation (4) can thus be viewed as using \(_{k}(t;)\) to reweight its PDF \(p_{}(t)=(1-t^{2})^{(d-3)/2}((d-1)/ 2)}\). Note that here \(p_{k}(t;)\) is the distribution of \(t=^{T}(k;)\), which can be empirically estimated, the shape of the function \(_{k}\) becomes empirically accessible with varying \(k\) values. The empirical studies and verification are provided in Section 3.3.

[MISSING_PAGE_FAIL:6]

property of the model architecture. With this property, how different model architectures differ in mechanisms can be studied by looking deeper into the population mean direction of saliency maps. For instance, it can be observed that the ResNet architecture admits a closer relation between models of different depths, compared with the CNN architecture.

### The Decreasing Spherical Variance with \(k\) (H1)

Expectation over \(\) vs. Conditioned on \(f\).As previously discussed, the spherical variance of distributions over the hypersphere can be measured by the population mean resultant length \()}\), which, unfortunately, requires an estimation of the mean directions. This can be expensive to study for a comprehensive set \(K\) of \(k\) values. The experiments on a subset \(K^{}=\{10,20,40,80,160\}\) are already carried out in Figure 3, shown as the diagonal elements. As \(k\) increases, the resultant length increases monotonically, indicating a decreasing spherical variance and a more concentrated distribution around the mean directions.

The computational burden of taking the expectation over \(\) can be alleviated by considering randomly picked \(f\). In order to compare \(\) and \(_{ind}\), we consider the model-dependent set \((f)=\{_{ind}(f,f;):\}\) for each \(f\). Here we compute the expected Wasserstein distance \(_{f^{(1)},f^{(2)}(k)}((f^{(1)} ),(f^{(2)}))\). This is estimated by the \(\) distinct pairs of models. The distances of all 60 (dataset, model family) pairs lie below 0.035. Such observation suggests that after taking the expectation over \(\), the differences across individual models can be mitigated. Please refer to Table 1 for comprehensive results on all model families and datasets. As a consequence, it suffices to use \(_{ind}(f,f)\) for some \(f(k)\) to approximate \((k,k)\). This is in fact the diagonal elements of Figure 2. A comparison between the diagonal elements \(_{k,k}\) and \(_{ind}(f^{(1)},f^{(2)}),f^{(1)},f^{(2)}(k)\) over CIFAR-10 is presented in Figure 6. Please refer to the appendix for other datasets. \(_{ind}\) is evaluated over \(k K\), while \(\) is evaluated over \(k K^{} K\). It can be found that after taking the average over \(\), even though \(\) is a little smoother than \(_{ind}\), they are very consistent. This verifies that the resultant length increases with \(k K\) in a much more comprehensive set of models. Thus H1 is empirically verified.

Figure 5: The heatmap visualization between the estimated population mean directions from different model families. Each entry is computed by \(_{}(}_{j}(; ),}_{j^{}}(;^{}))\) The results are generated from CIFAR-10/100 and TinyImagenet datasets. \(\){CS, CL, RS, RL}.

### The Shape of the Saw Distribution

Given \(_{k}()\), the marginalized distribution \(p_{k}(t;)\) can be approximated by \(_{k}()=\{=^{T}}(k;)| _{k}()\}\). In order to obtain the global results over test dataset \(\), consider the unions of different input samples \(_{k}=_{}_{k}()\). This is an approximation to the mixture distributions \(p_{k}(t)=|}_{}p_{k}(t;)\). We plot the histogram of \(_{k}\) with \(k K^{}\) for CNNSmall and CIFAR-10 in Figure 8. The left figure shows \(p_{origin}\) and the estimated \(p_{k}\), visualized by different colors. Qualitatively, \(p_{k}(t)\) has higher means with greater \(k\) values. The reason why the density of \(p_{k}\) is not centered at \(t=1\) (i.e. \(=\)) is because as \(t\) increases, the size of the \((d-2)\)-hypernsbpre decreases with \(p_{}(t)=(1-t^{2})^{(d-3)/2}((d- 1)/2)}\), which is shown as black histograms. This is much faster than the increase of \(_{k}\). The shape of \(_{k}\) is determined by \(p_{k}/p_{}\) with normalization. From the right figure, by comparing \(p_{k}\) with \(p_{}\), it can be empirically verified that \(_{k}(t)\) is increasing vastly. It is also observed that larger \(k\)s lead to a faster increase of \(_{k}\) and higher \([t]\). This also provides a quantitative understanding of H1 and H2. The results of other model architectures and datasets can be found in the appendix.

Verification of the Symmetry.Saw distributions study the marginalized value \(t=^{T}\) to directly focus on the degree of concentration of the gradients. This naturally leads to rotationally symmetric distributions since the distribution on the intersection between \(S^{d-1}\) and the hyperplane does not affect the distribution of \(t\). We thus carry out an empirical study of the distribution on the intersection (i.e. conditioned on \(t\)). Specifically, we train 1000 CNN models with \(K=40\) and seeds 1-1000 on CIFAR-10 and compute \(t\) regarding each test sample. The distribution of the first sample is visualized in Figure 7. We partition the range of \(t\) into 10 intervals by every 10 percent of the frequency, and inspect the direction of the mean of the gradients in each interval, each direction is estimated by 100 models. If these conditional mean directions are consistent with the population mean direction, then the gradients are symmetrically distributed on each \(S^{d-2}\) hypersphere (R7(right)), thus verifying the rotational symmetry. We investigate the cosine similarities between the conditional and unconditional mean directions on the first 1000 samples. The \(10 1000\) similarity values have a mean and std at approximately 0.970 and 0.013 respectively. Thus the rotational symmetry is empirically verified.

Figure 8: (left) The illustration of the frequency of the mixture \(_{k}\), where \(k\{10,20,40,80,160\}\). Specifically, the black histogram represents the distribution \(p_{origin}\). The dashed curves are the approximated PDF \(p_{k}\) obtained by KDE. The results are generated using CNNSmall and CIFAR-10. (right) The illustration of \(}{p_{}}\), which is linearly related to \(_{k}\).

Figure 7: The marginal distribution of \(t\) of the first test sample of CIFAR-10. Red dashed lines partition the range of \(t\) every 10 percent of the frequency.

Figure 9: The illustration of the relation between the expected testing loss \(_{}[]\) and the marginal expectation \(_{}[]\), where models are from (a) single models with varying capacities (b) deep ensembles with varying member #. Each color represents a model family. In particular, in (b), different marker shapes indicate different \(k\) of the ensembles.

## 4 Applications of Hypotheses

### Deep Ensemble: Why Does It Work?

After verifying the hypotheses, we explore possible applications and implications of the discovered phenomena. The deep ensemble method makes use of the stochasticity to of models by incorporating the predictions from \(m\) members. While deep ensembles have been verified to be effective in improving performance, the source of such capability remains mysterious. Note that ensemble members are i.i.d. optimized models with SGD, which correspond to the population mean of our hypothesis. We thus provide another perspective in understanding the capability of deep ensembles.

For single models, as the model capacity increases, benign overfitting suggests that the testing loss decreases, too. We deduce that this is because the distribution of larger models becomes more concentrated, and combined with H2, the closeness to the aligned population mean is highly related to the models' testing performance. As shown in Figure 9(a), it can be observed that the expected loss \(_{}[]\) and the marginal expectation \(_{}[t]\) are highly correlated. Similarly, deep ensemble approximates the population mean much more effectively by increasing the number \(m\) of members than scaling up a single model by \(k\). We thus scale up the deep ensemble by changing the number of ensemble members. The results are shown in Figure 9(b). It can be found that (1) the correlation between \(_{}[]\) and \(_{}[t]\) is not only significant, and (2) the correlation pattern is shared between two completely different scaling mechanisms, single model scaling and model ensembles.

### Black-Box Attack via Saliency Similarity

The understanding of adversarial attacks can benefit from the behaviors of the input salience of optimized models given their close relation to input gradients. We verify the aforementioned similarities through the black-box attacks, where the adversarial samples are generated from the gradients of source models while the gradients of the target models are not available. Let \(f^{(1)}(k_{1})\) denote the source model and \(f^{(2)}(k_{2})\) denote the target model. We define the attack rate from

Figure 11: The comparison between the single-model attack from the largest model (red), the single-model attack from the very same capacity (green) and the attack by the mean direction (blue).

Figure 10: The results of single model black-box attack. The value of each entry is \((k_{1},k_{2})\) for different model capacities, where \(k_{1}\) is the width parameter of the source model and \(k_{2}\) is the width parameter of the target model.

\(f^{(1)}\) to \(f^{(2)}\) similar to \(_{ind}\) as

\[(f^{(1)},f^{(2)})=}_{}f^{(2)} -(_{}f^{(1)}()) f^{(2)}() \]

which is the performance decay of \(f^{(2)}\) when attacked by model \(f^{(1)}\). Small \((k_{1},k_{2})\) values suggest successful attack from \(f^{(1)}\) to \(f^{(2)}\). The results are shown in Figure 10, where the attack step is set to \(=0.05\). In each heatmap figure, the \(y\)-axis represents the width of the source models, while the \(x\)-axis represents the width of the target models. It can be observed that larger models succeed in attacking smaller models, but the opposite is not true. To attack a large model, the gradient needs to be generated from a model of a comparable level in terms of capacity.

**Mean Direction Attack.** According to the verified hypothesis H2, for any two individual models \(f^{(1)}(k_{1}),f^{(2)}(k_{2})\) the mean directions \((k;)\) is closer to both of them than themselves, regardless of \(k,k_{1},k_{2}\). It is then suggested that using the mean gradient can perform more successful black-box attacks. We employ the mean direction \(}(160,)\) to attack models of different capacities, and compare the results with the attack from the largest single model (red) and the attack from the models of the identical structure (green). The results are shown in Figure 11, where it can be observed that the mean direction of salience transfers much more successfully than single models.

## 5 Conclusions

In this paper, we introduce hypotheses to explain the observations on the input salience convergence w.r.t. the model capacities. Under the same model architecture, stochastic algorithms such as SGD, result in certain distributions of optimized models. We hypothesize and use pointwise methods to verify that such distribution follows a Saw distribution with **aligned population means**, which is invariant from the model families. Besides, the variance of the distribution decreases as the model capacity increases, suggesting a convergence trend of the models' internal mechanism - the larger the models are, the less variant they tend to be affected by the randomness from the stochastic algorithm during the training phase. Furthermore, since the distributions converge towards the aligned population mean direction, the limiting points can be estimated by the population mean of models. Based on this, we present comprehensive experiments on the properties of the limiting model and demonstrate its capability in various domains, such as the black-box attack transferability, and the explanation of the effectiveness of deep ensembles. However, it is admitted that, due to the high computational burden, although improved from CIFAR-10/100 to TinyImagenet compared to (Nakkiran et al., 2021), our experiments are limited to rather small datasets.

Our introduced hypotheses also lead to various interesting topics. Note that the aligned mean direction stays invariant to the model families, which indicates such population mean is more related to the essence of the dataset itself rather than any single model. Leveraging this property can bring a deeper and more comprehensive understanding of the relation between data distributions and models.

## 6 Related Work

In terms of the convergence trend of DNNs, existing works focus on the convergence of single models throughout the training process. The parameters of DNNs have been demonstrated to converge to global minima throughout the training progress (Goodfellow et al., 2014; Li et al., 2018; Allen-Zhu et al., 2019; Liu et al., 2020; Damian et al., 2021; Refinetti et al., 2023; Suh and Cheng, 2024). Recent years, the studies of benign overfitting also suggest that increasing model capacities can improve the performance instead of exacerbating the overfitting issue (Bartlett et al., 2020; Nakkiran et al., 2021; Cao et al., 2022). While the studies of input gradients span into an abundant but extremely complicated spectrum. Among them, the area that is the most related to our work is the XAI domain, where the input gradient and its variants are crucial in revealing the models' internal mechanisms (Simonyan et al., 2013; Springenberg et al., 2014; Selvaraju et al., 2017; Sundararajan et al., 2017; Adebayo et al., 2018; Shah et al., 2021). On the other hand, the studies of the distribution of optimized models have received little attention. Such topics are slightly dipped in the efforts to demystify the source of capability of deep ensembles (Lee et al., 2015; Fort et al., 2019; Allen-Zhu and Li, 2020; Kobayashi et al., 2021; Abe et al., 2022; Ganaie et al., 2022; Theisen et al., 2023) and their implications (Lakshminarayanan et al., 2017; Geiger et al., 2020; Yang et al., 2021; Chen et al., 2023). Thus, to our knowledge, the studies on the distribution of optimized models remain a novel topic.